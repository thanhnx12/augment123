#############params############
cuda
Task=FewRel, 5-shot
Encoding model: bert
pattern=hybridprompt
mem=1, margin=0.3, gen=1, gen_num=2
#############params############
--------Round  0
seed:  100
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hybridprompt_12token/train.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hybridprompt_12token/valid.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hybridprompt_12token/test.pkl
Task_order: [7 3 0 5 4 1 6 2]
prepared data!
Clustering into  4  clusters
Clusters:  [0 3 2 0 1 1 2 0 3 3]
#############params############
cuda
Task=FewRel, 5-shot
Encoding model: bert
pattern=hybridprompt
mem=1, margin=0.3, gen=1, gen_num=2
#############params############
--------Round  0
seed:  100
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hybridprompt_12token/train.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hybridprompt_12token/valid.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hybridprompt_12token/test.pkl
Task_order: [7 3 0 5 4 1 6 2]
prepared data!
Clustering into  4  clusters
Clusters:  [0 3 2 0 1 1 2 0 3 3]
#############params############
cuda
Task=FewRel, 5-shot
Encoding model: bert
pattern=hybridprompt
mem=1, margin=0.3, gen=1, gen_num=2
#############params############
--------Round  0
seed:  100
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hybridprompt_12token/train.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hybridprompt_12token/valid.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hybridprompt_12token/test.pkl
Task_order: [7 3 0 5 4 1 6 2]
prepared data!
Clustering into  4  clusters
Clusters:  [0 3 2 0 1 1 2 0 3 3]
Losses:  18.05565643310547 3.954390525817871 1.2517082691192627
CurrentTrain: epoch  0, batch     0 | loss: 18.0556564Losses:  19.840600967407227 5.942726135253906 1.1852174997329712
CurrentTrain: epoch  0, batch     1 | loss: 19.8406010Losses:  18.791484832763672 4.904645919799805 1.1385606527328491
CurrentTrain: epoch  0, batch     2 | loss: 18.7914848Losses:  19.86072540283203 5.966850280761719 1.1662479639053345
CurrentTrain: epoch  0, batch     3 | loss: 19.8607254Losses:  20.74845314025879 7.332503318786621 1.090196132659912
CurrentTrain: epoch  0, batch     4 | loss: 20.7484531Losses:  24.18556022644043 11.11424446105957 1.0374085903167725
CurrentTrain: epoch  0, batch     5 | loss: 24.1855602Losses:  20.936132431030273 7.961305618286133 0.9684990048408508
CurrentTrain: epoch  0, batch     6 | loss: 20.9361324Losses:  19.653478622436523 6.648677349090576 0.9946036338806152
CurrentTrain: epoch  0, batch     7 | loss: 19.6534786Losses:  16.458478927612305 3.6782279014587402 0.9916484951972961
CurrentTrain: epoch  0, batch     8 | loss: 16.4584789Losses:  16.684768676757812 4.226747035980225 0.9133108854293823
CurrentTrain: epoch  0, batch     9 | loss: 16.6847687Losses:  16.083101272583008 3.7331347465515137 0.9355766773223877
CurrentTrain: epoch  0, batch    10 | loss: 16.0831013Losses:  17.570865631103516 5.68967342376709 0.7837113738059998
CurrentTrain: epoch  0, batch    11 | loss: 17.5708656Losses:  16.86347007751465 4.8212409019470215 0.8108647465705872
CurrentTrain: epoch  0, batch    12 | loss: 16.8634701Losses:  17.756752014160156 5.92067289352417 0.7906646132469177
CurrentTrain: epoch  0, batch    13 | loss: 17.7567520Losses:  16.92249870300293 5.593482971191406 0.7269817590713501
CurrentTrain: epoch  0, batch    14 | loss: 16.9224987Losses:  16.942184448242188 5.363801002502441 0.7354592084884644
CurrentTrain: epoch  0, batch    15 | loss: 16.9421844Losses:  20.26289939880371 8.824090957641602 0.5293682813644409
CurrentTrain: epoch  0, batch    16 | loss: 20.2628994Losses:  17.618690490722656 6.086968898773193 0.6551016569137573
CurrentTrain: epoch  0, batch    17 | loss: 17.6186905Losses:  14.447784423828125 3.5462446212768555 0.6313069462776184
CurrentTrain: epoch  0, batch    18 | loss: 14.4477844Losses:  15.900558471679688 4.860014915466309 0.6191061735153198
CurrentTrain: epoch  0, batch    19 | loss: 15.9005585Losses:  16.842205047607422 5.679265975952148 0.5821693539619446
CurrentTrain: epoch  0, batch    20 | loss: 16.8422050Losses:  16.722043991088867 5.477838039398193 0.5780974626541138
CurrentTrain: epoch  0, batch    21 | loss: 16.7220440Losses:  16.741044998168945 5.754072189331055 0.5815764665603638
CurrentTrain: epoch  0, batch    22 | loss: 16.7410450Losses:  16.834447860717773 5.183246612548828 0.550274133682251
CurrentTrain: epoch  0, batch    23 | loss: 16.8344479Losses:  16.345462799072266 5.241796970367432 0.5498508214950562
CurrentTrain: epoch  0, batch    24 | loss: 16.3454628Losses:  15.182644844055176 4.247592926025391 0.5831515192985535
CurrentTrain: epoch  0, batch    25 | loss: 15.1826448Losses:  16.317703247070312 4.894707202911377 0.5554420351982117
CurrentTrain: epoch  0, batch    26 | loss: 16.3177032Losses:  15.721659660339355 5.187131881713867 0.527880072593689
CurrentTrain: epoch  0, batch    27 | loss: 15.7216597Losses:  14.944753646850586 4.324740409851074 0.47412383556365967
CurrentTrain: epoch  0, batch    28 | loss: 14.9447536Losses:  17.715801239013672 6.9432268142700195 0.5657763481140137
CurrentTrain: epoch  0, batch    29 | loss: 17.7158012Losses:  15.347006797790527 5.075684070587158 0.4706566333770752
CurrentTrain: epoch  0, batch    30 | loss: 15.3470068Losses:  15.529281616210938 5.109592437744141 0.5145713686943054
CurrentTrain: epoch  0, batch    31 | loss: 15.5292816Losses:  14.449108123779297 3.58577036857605 0.5514494180679321
CurrentTrain: epoch  0, batch    32 | loss: 14.4491081Losses:  14.788524627685547 4.912840843200684 0.44878458976745605
CurrentTrain: epoch  0, batch    33 | loss: 14.7885246Losses:  19.351394653320312 8.980777740478516 0.5235037803649902
CurrentTrain: epoch  0, batch    34 | loss: 19.3513947Losses:  14.906949996948242 4.812798023223877 0.49957263469696045
CurrentTrain: epoch  0, batch    35 | loss: 14.9069500Losses:  13.524665832519531 3.755087375640869 0.5073087811470032
CurrentTrain: epoch  0, batch    36 | loss: 13.5246658Losses:  14.919900894165039 4.305388927459717 0.5270096063613892
CurrentTrain: epoch  0, batch    37 | loss: 14.9199009Losses:  13.894047737121582 4.3472723960876465 0.4991423487663269
CurrentTrain: epoch  0, batch    38 | loss: 13.8940477Losses:  14.183494567871094 4.184627056121826 0.4864307641983032
CurrentTrain: epoch  0, batch    39 | loss: 14.1834946Losses:  16.525402069091797 6.634921550750732 0.49615490436553955
CurrentTrain: epoch  0, batch    40 | loss: 16.5254021Losses:  16.128562927246094 6.398826599121094 0.4683496654033661
CurrentTrain: epoch  0, batch    41 | loss: 16.1285629Losses:  14.094996452331543 4.028508186340332 0.4771576523780823
CurrentTrain: epoch  0, batch    42 | loss: 14.0949965Losses:  14.328564643859863 4.8764495849609375 0.4511657655239105
CurrentTrain: epoch  0, batch    43 | loss: 14.3285646Losses:  14.22475814819336 4.511984825134277 0.4744505286216736
CurrentTrain: epoch  0, batch    44 | loss: 14.2247581Losses:  12.447259902954102 3.1957786083221436 0.4581652581691742
CurrentTrain: epoch  0, batch    45 | loss: 12.4472599Losses:  13.092209815979004 3.9044644832611084 0.44211333990097046
CurrentTrain: epoch  0, batch    46 | loss: 13.0922098Losses:  12.864480018615723 3.7196860313415527 0.4230199158191681
CurrentTrain: epoch  0, batch    47 | loss: 12.8644800Losses:  13.818281173706055 4.322075843811035 0.4369809031486511
CurrentTrain: epoch  0, batch    48 | loss: 13.8182812Losses:  11.717428207397461 2.7337443828582764 0.416020929813385
CurrentTrain: epoch  0, batch    49 | loss: 11.7174282Losses:  11.702716827392578 3.160010814666748 0.3995134234428406
CurrentTrain: epoch  0, batch    50 | loss: 11.7027168Losses:  13.641990661621094 4.764345645904541 0.43573546409606934
CurrentTrain: epoch  0, batch    51 | loss: 13.6419907Losses:  13.845843315124512 5.2093000411987305 0.41099458932876587
CurrentTrain: epoch  0, batch    52 | loss: 13.8458433Losses:  15.744937896728516 6.084953308105469 0.282726526260376
CurrentTrain: epoch  0, batch    53 | loss: 15.7449379Losses:  12.534147262573242 3.7526745796203613 0.40299761295318604
CurrentTrain: epoch  0, batch    54 | loss: 12.5341473Losses:  13.81438159942627 4.6356306076049805 0.41571828722953796
CurrentTrain: epoch  0, batch    55 | loss: 13.8143816Losses:  13.91964054107666 5.091861724853516 0.3923419415950775
CurrentTrain: epoch  0, batch    56 | loss: 13.9196405Losses:  10.462418556213379 2.126812219619751 0.39577358961105347
CurrentTrain: epoch  0, batch    57 | loss: 10.4624186Losses:  12.159643173217773 3.25441312789917 0.3752116858959198
CurrentTrain: epoch  0, batch    58 | loss: 12.1596432Losses:  13.389841079711914 4.996017932891846 0.3639904260635376
CurrentTrain: epoch  0, batch    59 | loss: 13.3898411Losses:  11.83028793334961 3.6925673484802246 0.3793452978134155
CurrentTrain: epoch  0, batch    60 | loss: 11.8302879Losses:  12.01400089263916 3.524538278579712 0.38832753896713257
CurrentTrain: epoch  0, batch    61 | loss: 12.0140009Losses:  8.625536918640137 2.0342178344726562 0.21327100694179535
CurrentTrain: epoch  0, batch    62 | loss: 8.6255369Losses:  13.390934944152832 4.901991844177246 0.3741875886917114
CurrentTrain: epoch  1, batch     0 | loss: 13.3909349Losses:  10.692917823791504 2.9286317825317383 0.35909831523895264
CurrentTrain: epoch  1, batch     1 | loss: 10.6929178Losses:  13.933050155639648 5.486928939819336 0.42007604241371155
CurrentTrain: epoch  1, batch     2 | loss: 13.9330502Losses:  13.005988121032715 5.266496658325195 0.36152184009552
CurrentTrain: epoch  1, batch     3 | loss: 13.0059881Losses:  10.951292991638184 2.9215407371520996 0.35714614391326904
CurrentTrain: epoch  1, batch     4 | loss: 10.9512930Losses:  10.946918487548828 3.0978174209594727 0.37395286560058594
CurrentTrain: epoch  1, batch     5 | loss: 10.9469185Losses:  10.65574836730957 3.079350471496582 0.34592780470848083
CurrentTrain: epoch  1, batch     6 | loss: 10.6557484Losses:  19.530107498168945 12.14589786529541 0.37156787514686584
CurrentTrain: epoch  1, batch     7 | loss: 19.5301075Losses:  15.12357234954834 7.164176940917969 0.4078711271286011
CurrentTrain: epoch  1, batch     8 | loss: 15.1235723Losses:  11.537735939025879 4.0868988037109375 0.362192839384079
CurrentTrain: epoch  1, batch     9 | loss: 11.5377359Losses:  13.497538566589355 5.083099365234375 0.26263266801834106
CurrentTrain: epoch  1, batch    10 | loss: 13.4975386Losses:  15.120135307312012 6.403343200683594 0.29955336451530457
CurrentTrain: epoch  1, batch    11 | loss: 15.1201353Losses:  13.003473281860352 4.685625076293945 0.3650991916656494
CurrentTrain: epoch  1, batch    12 | loss: 13.0034733Losses:  11.318224906921387 3.5515191555023193 0.3905722200870514
CurrentTrain: epoch  1, batch    13 | loss: 11.3182249Losses:  12.96396541595459 5.095099925994873 0.34971123933792114
CurrentTrain: epoch  1, batch    14 | loss: 12.9639654Losses:  14.244179725646973 5.0546464920043945 0.28853997588157654
CurrentTrain: epoch  1, batch    15 | loss: 14.2441797Losses:  11.381749153137207 3.770123243331909 0.374495267868042
CurrentTrain: epoch  1, batch    16 | loss: 11.3817492Losses:  12.249104499816895 4.484167575836182 0.34312936663627625
CurrentTrain: epoch  1, batch    17 | loss: 12.2491045Losses:  12.235405921936035 4.4176106452941895 0.3476744294166565
CurrentTrain: epoch  1, batch    18 | loss: 12.2354059Losses:  12.372736930847168 4.883211135864258 0.38066643476486206
CurrentTrain: epoch  1, batch    19 | loss: 12.3727369Losses:  11.758617401123047 3.5202646255493164 0.3575664460659027
CurrentTrain: epoch  1, batch    20 | loss: 11.7586174Losses:  11.612332344055176 3.8895816802978516 0.33561116456985474
CurrentTrain: epoch  1, batch    21 | loss: 11.6123323Losses:  10.351113319396973 2.5386080741882324 0.3461597263813019
CurrentTrain: epoch  1, batch    22 | loss: 10.3511133Losses:  11.287415504455566 3.8546080589294434 0.36222538352012634
CurrentTrain: epoch  1, batch    23 | loss: 11.2874155Losses:  11.530862808227539 4.0442795753479 0.3503984212875366
CurrentTrain: epoch  1, batch    24 | loss: 11.5308628Losses:  14.783525466918945 7.039511203765869 0.3316877484321594
CurrentTrain: epoch  1, batch    25 | loss: 14.7835255Losses:  13.856697082519531 6.046417236328125 0.24158696830272675
CurrentTrain: epoch  1, batch    26 | loss: 13.8566971Losses:  10.133402824401855 2.6244864463806152 0.3418954014778137
CurrentTrain: epoch  1, batch    27 | loss: 10.1334028Losses:  9.598034858703613 2.381924629211426 0.3478962481021881
CurrentTrain: epoch  1, batch    28 | loss: 9.5980349Losses:  10.637125968933105 3.4645509719848633 0.35720932483673096
CurrentTrain: epoch  1, batch    29 | loss: 10.6371260Losses:  10.881745338439941 3.394033193588257 0.3150085210800171
CurrentTrain: epoch  1, batch    30 | loss: 10.8817453Losses:  10.899420738220215 3.5329904556274414 0.36046916246414185
CurrentTrain: epoch  1, batch    31 | loss: 10.8994207Losses:  11.133832931518555 4.631799221038818 0.3110272288322449
CurrentTrain: epoch  1, batch    32 | loss: 11.1338329Losses:  10.831731796264648 3.7263131141662598 0.3165227770805359
CurrentTrain: epoch  1, batch    33 | loss: 10.8317318Losses:  10.448527336120605 2.8351948261260986 0.34548720717430115
CurrentTrain: epoch  1, batch    34 | loss: 10.4485273Losses:  12.676469802856445 4.060534477233887 0.3609464764595032
CurrentTrain: epoch  1, batch    35 | loss: 12.6764698Losses:  11.602148056030273 4.563503265380859 0.3313794434070587
CurrentTrain: epoch  1, batch    36 | loss: 11.6021481Losses:  10.66096019744873 3.5973615646362305 0.32014405727386475
CurrentTrain: epoch  1, batch    37 | loss: 10.6609602Losses:  13.508003234863281 6.075915336608887 0.2820066213607788
CurrentTrain: epoch  1, batch    38 | loss: 13.5080032Losses:  11.126378059387207 3.9530181884765625 0.31930866837501526
CurrentTrain: epoch  1, batch    39 | loss: 11.1263781Losses:  10.405496597290039 3.305013656616211 0.3599892556667328
CurrentTrain: epoch  1, batch    40 | loss: 10.4054966Losses:  8.91991901397705 2.055246353149414 0.31736087799072266
CurrentTrain: epoch  1, batch    41 | loss: 8.9199190Losses:  12.088906288146973 5.244719505310059 0.3166181743144989
CurrentTrain: epoch  1, batch    42 | loss: 12.0889063Losses:  10.004539489746094 2.7977218627929688 0.32948458194732666
CurrentTrain: epoch  1, batch    43 | loss: 10.0045395Losses:  13.082381248474121 4.997838497161865 0.32690751552581787
CurrentTrain: epoch  1, batch    44 | loss: 13.0823812Losses:  13.447881698608398 5.93748664855957 0.314361035823822
CurrentTrain: epoch  1, batch    45 | loss: 13.4478817Losses:  8.805232048034668 2.475771427154541 0.3133057951927185
CurrentTrain: epoch  1, batch    46 | loss: 8.8052320Losses:  11.34009075164795 5.047955513000488 0.29721927642822266
CurrentTrain: epoch  1, batch    47 | loss: 11.3400908Losses:  10.009668350219727 3.0894978046417236 0.31306490302085876
CurrentTrain: epoch  1, batch    48 | loss: 10.0096684Losses:  11.821269989013672 4.889798164367676 0.31255728006362915
CurrentTrain: epoch  1, batch    49 | loss: 11.8212700Losses:  11.455153465270996 4.662269592285156 0.32306763529777527
CurrentTrain: epoch  1, batch    50 | loss: 11.4551535Losses:  9.843701362609863 3.0297088623046875 0.31357109546661377
CurrentTrain: epoch  1, batch    51 | loss: 9.8437014Losses:  11.610215187072754 4.179100513458252 0.31145527958869934
CurrentTrain: epoch  1, batch    52 | loss: 11.6102152Losses:  14.734128952026367 6.910172462463379 0.3333267271518707
CurrentTrain: epoch  1, batch    53 | loss: 14.7341290Losses:  10.687994956970215 3.699089527130127 0.2876841425895691
CurrentTrain: epoch  1, batch    54 | loss: 10.6879950Losses:  10.818699836730957 3.5884809494018555 0.2981586456298828
CurrentTrain: epoch  1, batch    55 | loss: 10.8186998Losses:  9.557745933532715 3.2123630046844482 0.2916635572910309
CurrentTrain: epoch  1, batch    56 | loss: 9.5577459Losses:  12.336512565612793 4.814808368682861 0.32088783383369446
CurrentTrain: epoch  1, batch    57 | loss: 12.3365126Losses:  9.31208610534668 2.9905853271484375 0.30781090259552
CurrentTrain: epoch  1, batch    58 | loss: 9.3120861Losses:  11.04507827758789 3.6563775539398193 0.23431715369224548
CurrentTrain: epoch  1, batch    59 | loss: 11.0450783Losses:  12.262353897094727 5.661705493927002 0.30832457542419434
CurrentTrain: epoch  1, batch    60 | loss: 12.2623539Losses:  8.455020904541016 3.1359808444976807 0.2981734573841095
CurrentTrain: epoch  1, batch    61 | loss: 8.4550209Losses:  5.647775173187256 0.312038391828537 0.2758840024471283
CurrentTrain: epoch  1, batch    62 | loss: 5.6477752Losses:  9.425322532653809 2.724081516265869 0.28233209252357483
CurrentTrain: epoch  2, batch     0 | loss: 9.4253225Losses:  11.059261322021484 6.125279426574707 0.196634441614151
CurrentTrain: epoch  2, batch     1 | loss: 11.0592613Losses:  9.483811378479004 3.16463303565979 0.28825071454048157
CurrentTrain: epoch  2, batch     2 | loss: 9.4838114Losses:  12.603262901306152 5.1553239822387695 0.23696176707744598
CurrentTrain: epoch  2, batch     3 | loss: 12.6032629Losses:  10.597084045410156 3.7826452255249023 0.2030077427625656
CurrentTrain: epoch  2, batch     4 | loss: 10.5970840Losses:  9.203935623168945 3.095956325531006 0.300216943025589
CurrentTrain: epoch  2, batch     5 | loss: 9.2039356Losses:  9.286136627197266 2.8473997116088867 0.290670245885849
CurrentTrain: epoch  2, batch     6 | loss: 9.2861366Losses:  11.00507640838623 4.501906871795654 0.32829177379608154
CurrentTrain: epoch  2, batch     7 | loss: 11.0050764Losses:  8.763952255249023 2.953695058822632 0.28304243087768555
CurrentTrain: epoch  2, batch     8 | loss: 8.7639523Losses:  8.405610084533691 2.501262664794922 0.29566359519958496
CurrentTrain: epoch  2, batch     9 | loss: 8.4056101Losses:  9.255729675292969 3.0953757762908936 0.30743205547332764
CurrentTrain: epoch  2, batch    10 | loss: 9.2557297Losses:  11.213643074035645 5.280467987060547 0.29592353105545044
CurrentTrain: epoch  2, batch    11 | loss: 11.2136431Losses:  9.179168701171875 3.3757972717285156 0.28578007221221924
CurrentTrain: epoch  2, batch    12 | loss: 9.1791687Losses:  9.117902755737305 3.3617517948150635 0.31769636273384094
CurrentTrain: epoch  2, batch    13 | loss: 9.1179028Losses:  11.698936462402344 5.257525444030762 0.3139244318008423
CurrentTrain: epoch  2, batch    14 | loss: 11.6989365Losses:  9.774189949035645 3.546933650970459 0.2866273522377014
CurrentTrain: epoch  2, batch    15 | loss: 9.7741899Losses:  8.521427154541016 2.873904228210449 0.27231264114379883
CurrentTrain: epoch  2, batch    16 | loss: 8.5214272Losses:  9.814095497131348 4.259330749511719 0.2836984395980835
CurrentTrain: epoch  2, batch    17 | loss: 9.8140955Losses:  12.928837776184082 5.679064750671387 0.2787139117717743
CurrentTrain: epoch  2, batch    18 | loss: 12.9288378Losses:  10.69393539428711 4.909784317016602 0.276846319437027
CurrentTrain: epoch  2, batch    19 | loss: 10.6939354Losses:  9.936068534851074 4.104526519775391 0.28040778636932373
CurrentTrain: epoch  2, batch    20 | loss: 9.9360685Losses:  11.250113487243652 5.035378932952881 0.299945205450058
CurrentTrain: epoch  2, batch    21 | loss: 11.2501135Losses:  10.537131309509277 4.380205154418945 0.3316887617111206
CurrentTrain: epoch  2, batch    22 | loss: 10.5371313Losses:  8.023085594177246 2.3115196228027344 0.2758325934410095
CurrentTrain: epoch  2, batch    23 | loss: 8.0230856Losses:  10.575960159301758 4.326345443725586 0.2788657248020172
CurrentTrain: epoch  2, batch    24 | loss: 10.5759602Losses:  8.768264770507812 2.5864884853363037 0.26976561546325684
CurrentTrain: epoch  2, batch    25 | loss: 8.7682648Losses:  8.287980079650879 2.279960870742798 0.2751680612564087
CurrentTrain: epoch  2, batch    26 | loss: 8.2879801Losses:  11.624621391296387 5.258459091186523 0.2737889587879181
CurrentTrain: epoch  2, batch    27 | loss: 11.6246214Losses:  12.088713645935059 5.492206573486328 0.30812522768974304
CurrentTrain: epoch  2, batch    28 | loss: 12.0887136Losses:  9.379855155944824 3.103188991546631 0.27457624673843384
CurrentTrain: epoch  2, batch    29 | loss: 9.3798552Losses:  9.475505828857422 2.9486966133117676 0.2869269847869873
CurrentTrain: epoch  2, batch    30 | loss: 9.4755058Losses:  8.546126365661621 2.5655736923217773 0.2907685339450836
CurrentTrain: epoch  2, batch    31 | loss: 8.5461264Losses:  8.95437240600586 2.9053025245666504 0.2713940739631653
CurrentTrain: epoch  2, batch    32 | loss: 8.9543724Losses:  7.544178485870361 2.1201202869415283 0.26552823185920715
CurrentTrain: epoch  2, batch    33 | loss: 7.5441785Losses:  8.244817733764648 2.742363929748535 0.27481797337532043
CurrentTrain: epoch  2, batch    34 | loss: 8.2448177Losses:  9.60440731048584 3.571964740753174 0.27975594997406006
CurrentTrain: epoch  2, batch    35 | loss: 9.6044073Losses:  9.569501876831055 2.9921679496765137 0.29095935821533203
CurrentTrain: epoch  2, batch    36 | loss: 9.5695019Losses:  12.412542343139648 5.777660846710205 0.28593605756759644
CurrentTrain: epoch  2, batch    37 | loss: 12.4125423Losses:  11.064291000366211 4.923569679260254 0.18716609477996826
CurrentTrain: epoch  2, batch    38 | loss: 11.0642910Losses:  8.624131202697754 2.894369125366211 0.28799569606781006
CurrentTrain: epoch  2, batch    39 | loss: 8.6241312Losses:  8.76270580291748 3.4782094955444336 0.2729051113128662
CurrentTrain: epoch  2, batch    40 | loss: 8.7627058Losses:  9.722145080566406 3.555586099624634 0.25998932123184204
CurrentTrain: epoch  2, batch    41 | loss: 9.7221451Losses:  8.486417770385742 2.9928066730499268 0.2688678503036499
CurrentTrain: epoch  2, batch    42 | loss: 8.4864178Losses:  8.549138069152832 2.5509278774261475 0.2831155061721802
CurrentTrain: epoch  2, batch    43 | loss: 8.5491381Losses:  9.097951889038086 2.537109375 0.28628867864608765
CurrentTrain: epoch  2, batch    44 | loss: 9.0979519Losses:  8.631983757019043 2.469651222229004 0.30967751145362854
CurrentTrain: epoch  2, batch    45 | loss: 8.6319838Losses:  8.030292510986328 2.464995861053467 0.26447993516921997
CurrentTrain: epoch  2, batch    46 | loss: 8.0302925Losses:  11.49835205078125 5.605391025543213 0.2023223638534546
CurrentTrain: epoch  2, batch    47 | loss: 11.4983521Losses:  10.332199096679688 3.725283622741699 0.26431000232696533
CurrentTrain: epoch  2, batch    48 | loss: 10.3321991Losses:  9.212198257446289 2.871868371963501 0.2889196276664734
CurrentTrain: epoch  2, batch    49 | loss: 9.2121983Losses:  7.945549964904785 2.4789068698883057 0.2901962399482727
CurrentTrain: epoch  2, batch    50 | loss: 7.9455500Losses:  10.619074821472168 5.195928573608398 0.29746073484420776
CurrentTrain: epoch  2, batch    51 | loss: 10.6190748Losses:  9.80255126953125 4.500540733337402 0.27068325877189636
CurrentTrain: epoch  2, batch    52 | loss: 9.8025513Losses:  7.554180145263672 2.276217460632324 0.27500247955322266
CurrentTrain: epoch  2, batch    53 | loss: 7.5541801Losses:  9.853412628173828 3.400991439819336 0.28259992599487305
CurrentTrain: epoch  2, batch    54 | loss: 9.8534126Losses:  7.667462348937988 2.4185028076171875 0.2606956362724304
CurrentTrain: epoch  2, batch    55 | loss: 7.6674623Losses:  8.057870864868164 2.6504557132720947 0.27757173776626587
CurrentTrain: epoch  2, batch    56 | loss: 8.0578709Losses:  8.89154052734375 3.4506258964538574 0.2897590398788452
CurrentTrain: epoch  2, batch    57 | loss: 8.8915405Losses:  7.775676727294922 2.7725534439086914 0.2674144506454468
CurrentTrain: epoch  2, batch    58 | loss: 7.7756767Losses:  8.609477043151855 2.784978151321411 0.2793142795562744
CurrentTrain: epoch  2, batch    59 | loss: 8.6094770Losses:  9.392634391784668 4.47862434387207 0.2921210825443268
CurrentTrain: epoch  2, batch    60 | loss: 9.3926344Losses:  9.497150421142578 4.39640474319458 0.2665504515171051
CurrentTrain: epoch  2, batch    61 | loss: 9.4971504Losses:  7.999617099761963 3.1763463020324707 0.31891128420829773
CurrentTrain: epoch  2, batch    62 | loss: 7.9996171Losses:  7.920961856842041 2.9614689350128174 0.2670714259147644
CurrentTrain: epoch  3, batch     0 | loss: 7.9209619Losses:  8.20043659210205 2.9361252784729004 0.2623433768749237
CurrentTrain: epoch  3, batch     1 | loss: 8.2004366Losses:  10.192178726196289 4.880774974822998 0.2753720283508301
CurrentTrain: epoch  3, batch     2 | loss: 10.1921787Losses:  7.716468811035156 2.4132938385009766 0.26665198802948
CurrentTrain: epoch  3, batch     3 | loss: 7.7164688Losses:  12.262310981750488 6.936686038970947 0.1781342327594757
CurrentTrain: epoch  3, batch     4 | loss: 12.2623110Losses:  7.958322048187256 2.801870822906494 0.25682884454727173
CurrentTrain: epoch  3, batch     5 | loss: 7.9583220Losses:  8.30501651763916 3.2587432861328125 0.26371878385543823
CurrentTrain: epoch  3, batch     6 | loss: 8.3050165Losses:  8.211118698120117 3.2847933769226074 0.26426732540130615
CurrentTrain: epoch  3, batch     7 | loss: 8.2111187Losses:  9.050280570983887 3.794691801071167 0.2635831832885742
CurrentTrain: epoch  3, batch     8 | loss: 9.0502806Losses:  8.298121452331543 2.6638667583465576 0.2775754928588867
CurrentTrain: epoch  3, batch     9 | loss: 8.2981215Losses:  8.303970336914062 2.93571400642395 0.2689487040042877
CurrentTrain: epoch  3, batch    10 | loss: 8.3039703Losses:  9.609076499938965 3.8380255699157715 0.29607266187667847
CurrentTrain: epoch  3, batch    11 | loss: 9.6090765Losses:  9.826047897338867 4.671196937561035 0.26158392429351807
CurrentTrain: epoch  3, batch    12 | loss: 9.8260479Losses:  6.928624629974365 2.0206692218780518 0.24458187818527222
CurrentTrain: epoch  3, batch    13 | loss: 6.9286246Losses:  12.166232109069824 7.178184509277344 0.2894282639026642
CurrentTrain: epoch  3, batch    14 | loss: 12.1662321Losses:  9.276881217956543 3.471835136413574 0.279456228017807
CurrentTrain: epoch  3, batch    15 | loss: 9.2768812Losses:  7.96932315826416 3.1747336387634277 0.25918376445770264
CurrentTrain: epoch  3, batch    16 | loss: 7.9693232Losses:  10.085054397583008 4.615042686462402 0.18676650524139404
CurrentTrain: epoch  3, batch    17 | loss: 10.0850544Losses:  7.876948356628418 2.4921910762786865 0.24860092997550964
CurrentTrain: epoch  3, batch    18 | loss: 7.8769484Losses:  11.193325996398926 5.26271915435791 0.26692479848861694
CurrentTrain: epoch  3, batch    19 | loss: 11.1933260Losses:  8.085081100463867 2.825277328491211 0.2587537169456482
CurrentTrain: epoch  3, batch    20 | loss: 8.0850811Losses:  7.815696716308594 2.5827527046203613 0.26937052607536316
CurrentTrain: epoch  3, batch    21 | loss: 7.8156967Losses:  7.305779457092285 1.8512965440750122 0.25877171754837036
CurrentTrain: epoch  3, batch    22 | loss: 7.3057795Losses:  9.396713256835938 4.448691368103027 0.24925260245800018
CurrentTrain: epoch  3, batch    23 | loss: 9.3967133Losses:  7.607126235961914 2.858628749847412 0.24975994229316711
CurrentTrain: epoch  3, batch    24 | loss: 7.6071262Losses:  8.864683151245117 3.5724880695343018 0.2544521391391754
CurrentTrain: epoch  3, batch    25 | loss: 8.8646832Losses:  9.764374732971191 4.416604042053223 0.25700199604034424
CurrentTrain: epoch  3, batch    26 | loss: 9.7643747Losses:  11.135671615600586 6.1511549949646 0.30011796951293945
CurrentTrain: epoch  3, batch    27 | loss: 11.1356716Losses:  9.292617797851562 2.7767016887664795 0.24985061585903168
CurrentTrain: epoch  3, batch    28 | loss: 9.2926178Losses:  8.197755813598633 2.7380504608154297 0.24561229348182678
CurrentTrain: epoch  3, batch    29 | loss: 8.1977558Losses:  7.716095924377441 2.7686283588409424 0.24664101004600525
CurrentTrain: epoch  3, batch    30 | loss: 7.7160959Losses:  9.60718822479248 3.963318347930908 0.26364967226982117
CurrentTrain: epoch  3, batch    31 | loss: 9.6071882Losses:  9.091198921203613 2.8314943313598633 0.2519843876361847
CurrentTrain: epoch  3, batch    32 | loss: 9.0911989Losses:  8.099130630493164 2.9487123489379883 0.25097477436065674
CurrentTrain: epoch  3, batch    33 | loss: 8.0991306Losses:  7.999680519104004 2.954850673675537 0.25132906436920166
CurrentTrain: epoch  3, batch    34 | loss: 7.9996805Losses:  7.802492618560791 2.7583816051483154 0.23598036170005798
CurrentTrain: epoch  3, batch    35 | loss: 7.8024926Losses:  7.2650909423828125 2.5251383781433105 0.2394254207611084
CurrentTrain: epoch  3, batch    36 | loss: 7.2650909Losses:  7.910919189453125 2.2968552112579346 0.2449694275856018
CurrentTrain: epoch  3, batch    37 | loss: 7.9109192Losses:  8.006985664367676 3.0879874229431152 0.2594420909881592
CurrentTrain: epoch  3, batch    38 | loss: 8.0069857Losses:  8.676770210266113 3.760718822479248 0.250643789768219
CurrentTrain: epoch  3, batch    39 | loss: 8.6767702Losses:  7.551182746887207 2.971560001373291 0.2575014531612396
CurrentTrain: epoch  3, batch    40 | loss: 7.5511827Losses:  6.694580554962158 1.4614362716674805 0.23517093062400818
CurrentTrain: epoch  3, batch    41 | loss: 6.6945806Losses:  9.301605224609375 3.95271372795105 0.2736216187477112
CurrentTrain: epoch  3, batch    42 | loss: 9.3016052Losses:  10.167824745178223 5.222002983093262 0.24262449145317078
CurrentTrain: epoch  3, batch    43 | loss: 10.1678247Losses:  7.853436470031738 2.4947710037231445 0.25295960903167725
CurrentTrain: epoch  3, batch    44 | loss: 7.8534365Losses:  8.84117603302002 3.1828534603118896 0.2497754842042923
CurrentTrain: epoch  3, batch    45 | loss: 8.8411760Losses:  10.15905475616455 3.792752742767334 0.24968639016151428
CurrentTrain: epoch  3, batch    46 | loss: 10.1590548Losses:  6.422684192657471 1.4038057327270508 0.23624855279922485
CurrentTrain: epoch  3, batch    47 | loss: 6.4226842Losses:  9.629194259643555 4.488803863525391 0.2572367191314697
CurrentTrain: epoch  3, batch    48 | loss: 9.6291943Losses:  9.04118537902832 4.007529258728027 0.2694302797317505
CurrentTrain: epoch  3, batch    49 | loss: 9.0411854Losses:  7.8646039962768555 2.573274612426758 0.2582498788833618
CurrentTrain: epoch  3, batch    50 | loss: 7.8646040Losses:  8.482440948486328 3.1124072074890137 0.26211392879486084
CurrentTrain: epoch  3, batch    51 | loss: 8.4824409Losses:  7.460841655731201 2.4817285537719727 0.2521907389163971
CurrentTrain: epoch  3, batch    52 | loss: 7.4608417Losses:  8.706679344177246 3.134486675262451 0.25846096873283386
CurrentTrain: epoch  3, batch    53 | loss: 8.7066793Losses:  7.9636054039001465 2.9159786701202393 0.23777931928634644
CurrentTrain: epoch  3, batch    54 | loss: 7.9636054Losses:  8.154122352600098 2.79649019241333 0.26250773668289185
CurrentTrain: epoch  3, batch    55 | loss: 8.1541224Losses:  10.533994674682617 4.793350696563721 0.2465572953224182
CurrentTrain: epoch  3, batch    56 | loss: 10.5339947Losses:  9.191943168640137 3.6466591358184814 0.24229134619235992
CurrentTrain: epoch  3, batch    57 | loss: 9.1919432Losses:  8.611635208129883 3.352626323699951 0.15854990482330322
CurrentTrain: epoch  3, batch    58 | loss: 8.6116352Losses:  12.919037818908691 8.221729278564453 0.2873103618621826
CurrentTrain: epoch  3, batch    59 | loss: 12.9190378Losses:  14.2465238571167 9.317098617553711 0.18275707960128784
CurrentTrain: epoch  3, batch    60 | loss: 14.2465239Losses:  7.527853012084961 2.5512585639953613 0.2565785348415375
CurrentTrain: epoch  3, batch    61 | loss: 7.5278530Losses:  7.224390029907227 1.5984127521514893 0.27127936482429504
CurrentTrain: epoch  3, batch    62 | loss: 7.2243900Losses:  8.772940635681152 3.4703867435455322 0.268379271030426
CurrentTrain: epoch  4, batch     0 | loss: 8.7729406Losses:  6.759002685546875 1.9944242238998413 0.2502082288265228
CurrentTrain: epoch  4, batch     1 | loss: 6.7590027Losses:  7.768886566162109 2.511974334716797 0.24892327189445496
CurrentTrain: epoch  4, batch     2 | loss: 7.7688866Losses:  7.984659194946289 3.4245057106018066 0.2593892514705658
CurrentTrain: epoch  4, batch     3 | loss: 7.9846592Losses:  8.180346488952637 3.478517532348633 0.24931202828884125
CurrentTrain: epoch  4, batch     4 | loss: 8.1803465Losses:  8.75210189819336 3.530640125274658 0.25473570823669434
CurrentTrain: epoch  4, batch     5 | loss: 8.7521019Losses:  9.722753524780273 4.826878547668457 0.2510809600353241
CurrentTrain: epoch  4, batch     6 | loss: 9.7227535Losses:  8.988014221191406 3.314333915710449 0.25784969329833984
CurrentTrain: epoch  4, batch     7 | loss: 8.9880142Losses:  7.949725151062012 3.0971078872680664 0.2619049549102783
CurrentTrain: epoch  4, batch     8 | loss: 7.9497252Losses:  8.146688461303711 3.306589126586914 0.24483908712863922
CurrentTrain: epoch  4, batch     9 | loss: 8.1466885Losses:  8.6259183883667 3.4660873413085938 0.27747076749801636
CurrentTrain: epoch  4, batch    10 | loss: 8.6259184Losses:  7.728598117828369 2.573244571685791 0.24914853274822235
CurrentTrain: epoch  4, batch    11 | loss: 7.7285981Losses:  6.5145721435546875 1.7980402708053589 0.23587152361869812
CurrentTrain: epoch  4, batch    12 | loss: 6.5145721Losses:  7.505375862121582 2.3611013889312744 0.25223520398139954
CurrentTrain: epoch  4, batch    13 | loss: 7.5053759Losses:  7.676176071166992 2.96120023727417 0.24186046421527863
CurrentTrain: epoch  4, batch    14 | loss: 7.6761761Losses:  10.024116516113281 4.696984767913818 0.16159102320671082
CurrentTrain: epoch  4, batch    15 | loss: 10.0241165Losses:  9.686192512512207 5.200831413269043 0.17704078555107117
CurrentTrain: epoch  4, batch    16 | loss: 9.6861925Losses:  7.787142753601074 3.271845817565918 0.2556886076927185
CurrentTrain: epoch  4, batch    17 | loss: 7.7871428Losses:  7.282235622406006 2.5475428104400635 0.2448926717042923
CurrentTrain: epoch  4, batch    18 | loss: 7.2822356Losses:  6.1910786628723145 1.3910512924194336 0.23396211862564087
CurrentTrain: epoch  4, batch    19 | loss: 6.1910787Losses:  7.588500022888184 2.922844409942627 0.25570252537727356
CurrentTrain: epoch  4, batch    20 | loss: 7.5885000Losses:  6.519560813903809 1.7476924657821655 0.2471100389957428
CurrentTrain: epoch  4, batch    21 | loss: 6.5195608Losses:  7.487027645111084 2.7548680305480957 0.244477316737175
CurrentTrain: epoch  4, batch    22 | loss: 7.4870276Losses:  7.616837501525879 2.6347904205322266 0.2507355213165283
CurrentTrain: epoch  4, batch    23 | loss: 7.6168375Losses:  8.599165916442871 2.481916904449463 0.23558150231838226
CurrentTrain: epoch  4, batch    24 | loss: 8.5991659Losses:  7.336076736450195 2.6805148124694824 0.24796368181705475
CurrentTrain: epoch  4, batch    25 | loss: 7.3360767Losses:  6.610196590423584 2.0326576232910156 0.23249509930610657
CurrentTrain: epoch  4, batch    26 | loss: 6.6101966Losses:  10.41596508026123 5.743296146392822 0.1600579470396042
CurrentTrain: epoch  4, batch    27 | loss: 10.4159651Losses:  7.292211532592773 2.663088321685791 0.24051928520202637
CurrentTrain: epoch  4, batch    28 | loss: 7.2922115Losses:  9.101841926574707 4.545555114746094 0.23783373832702637
CurrentTrain: epoch  4, batch    29 | loss: 9.1018419Losses:  12.83462905883789 7.979026794433594 0.19902193546295166
CurrentTrain: epoch  4, batch    30 | loss: 12.8346291Losses:  8.630361557006836 4.175297737121582 0.23815953731536865
CurrentTrain: epoch  4, batch    31 | loss: 8.6303616Losses:  6.879528522491455 2.3326454162597656 0.23548263311386108
CurrentTrain: epoch  4, batch    32 | loss: 6.8795285Losses:  10.55222225189209 5.661803722381592 0.18847763538360596
CurrentTrain: epoch  4, batch    33 | loss: 10.5522223Losses:  9.42071533203125 4.5192108154296875 0.24549701809883118
CurrentTrain: epoch  4, batch    34 | loss: 9.4207153Losses:  9.89538860321045 4.233133316040039 0.24432414770126343
CurrentTrain: epoch  4, batch    35 | loss: 9.8953886Losses:  8.737101554870605 4.0677714347839355 0.26083624362945557
CurrentTrain: epoch  4, batch    36 | loss: 8.7371016Losses:  7.65491247177124 2.972550868988037 0.2488536387681961
CurrentTrain: epoch  4, batch    37 | loss: 7.6549125Losses:  8.627272605895996 3.9927456378936768 0.2475750744342804
CurrentTrain: epoch  4, batch    38 | loss: 8.6272726Losses:  6.533752918243408 1.9921826124191284 0.2313465178012848
CurrentTrain: epoch  4, batch    39 | loss: 6.5337529Losses:  10.282848358154297 4.99477481842041 0.2420097291469574
CurrentTrain: epoch  4, batch    40 | loss: 10.2828484Losses:  8.962493896484375 4.58271598815918 0.24347925186157227
CurrentTrain: epoch  4, batch    41 | loss: 8.9624939Losses:  6.868406772613525 2.1333470344543457 0.2487470954656601
CurrentTrain: epoch  4, batch    42 | loss: 6.8684068Losses:  7.942883014678955 3.0084967613220215 0.24870656430721283
CurrentTrain: epoch  4, batch    43 | loss: 7.9428830Losses:  6.795774936676025 2.29817533493042 0.23520666360855103
CurrentTrain: epoch  4, batch    44 | loss: 6.7957749Losses:  7.530431270599365 2.901484727859497 0.23558956384658813
CurrentTrain: epoch  4, batch    45 | loss: 7.5304313Losses:  7.257638931274414 2.458266258239746 0.24096666276454926
CurrentTrain: epoch  4, batch    46 | loss: 7.2576389Losses:  7.2011566162109375 2.2664248943328857 0.228170245885849
CurrentTrain: epoch  4, batch    47 | loss: 7.2011566Losses:  10.53359317779541 6.05872917175293 0.27736204862594604
CurrentTrain: epoch  4, batch    48 | loss: 10.5335932Losses:  8.449965476989746 3.6259052753448486 0.24643607437610626
CurrentTrain: epoch  4, batch    49 | loss: 8.4499655Losses:  8.8849515914917 4.174269676208496 0.2423274666070938
CurrentTrain: epoch  4, batch    50 | loss: 8.8849516Losses:  7.953251361846924 2.9182448387145996 0.24628299474716187
CurrentTrain: epoch  4, batch    51 | loss: 7.9532514Losses:  10.323533058166504 5.790197372436523 0.26419514417648315
CurrentTrain: epoch  4, batch    52 | loss: 10.3235331Losses:  7.70005989074707 3.003530979156494 0.25271838903427124
CurrentTrain: epoch  4, batch    53 | loss: 7.7000599Losses:  8.033902168273926 3.3303306102752686 0.2491239607334137
CurrentTrain: epoch  4, batch    54 | loss: 8.0339022Losses:  6.910429954528809 2.3460381031036377 0.2532541751861572
CurrentTrain: epoch  4, batch    55 | loss: 6.9104300Losses:  9.163541793823242 3.9970626831054688 0.23929329216480255
CurrentTrain: epoch  4, batch    56 | loss: 9.1635418Losses:  7.708582878112793 2.5950112342834473 0.23585262894630432
CurrentTrain: epoch  4, batch    57 | loss: 7.7085829Losses:  9.565133094787598 4.9367594718933105 0.26949793100357056
CurrentTrain: epoch  4, batch    58 | loss: 9.5651331Losses:  9.140718460083008 4.23283576965332 0.2551150918006897
CurrentTrain: epoch  4, batch    59 | loss: 9.1407185Losses:  7.791704177856445 3.5247011184692383 0.15219157934188843
CurrentTrain: epoch  4, batch    60 | loss: 7.7917042Losses:  9.23807144165039 4.586687088012695 0.25164225697517395
CurrentTrain: epoch  4, batch    61 | loss: 9.2380714Losses:  4.800027370452881 0.25139835476875305 0.2489965558052063
CurrentTrain: epoch  4, batch    62 | loss: 4.8000274Losses:  13.208906173706055 8.775975227355957 0.25731828808784485
CurrentTrain: epoch  5, batch     0 | loss: 13.2089062Losses:  8.38998031616211 3.997440814971924 0.23421865701675415
CurrentTrain: epoch  5, batch     1 | loss: 8.3899803Losses:  6.295895576477051 1.861586570739746 0.23220723867416382
CurrentTrain: epoch  5, batch     2 | loss: 6.2958956Losses:  6.895591735839844 2.424081325531006 0.2366662174463272
CurrentTrain: epoch  5, batch     3 | loss: 6.8955917Losses:  6.901224136352539 2.465944766998291 0.2310636043548584
CurrentTrain: epoch  5, batch     4 | loss: 6.9012241Losses:  7.370429039001465 2.644960403442383 0.24290865659713745
CurrentTrain: epoch  5, batch     5 | loss: 7.3704290Losses:  8.46493911743164 3.122741937637329 0.2369770109653473
CurrentTrain: epoch  5, batch     6 | loss: 8.4649391Losses:  8.31838607788086 3.8486080169677734 0.24661076068878174
CurrentTrain: epoch  5, batch     7 | loss: 8.3183861Losses:  7.936347961425781 3.4833855628967285 0.2637021243572235
CurrentTrain: epoch  5, batch     8 | loss: 7.9363480Losses:  8.840391159057617 4.401730537414551 0.14987611770629883
CurrentTrain: epoch  5, batch     9 | loss: 8.8403912Losses:  8.331567764282227 3.8232851028442383 0.25367116928100586
CurrentTrain: epoch  5, batch    10 | loss: 8.3315678Losses:  7.56313943862915 3.087961196899414 0.2440706342458725
CurrentTrain: epoch  5, batch    11 | loss: 7.5631394Losses:  7.983784198760986 3.1770801544189453 0.25252866744995117
CurrentTrain: epoch  5, batch    12 | loss: 7.9837842Losses:  7.1247382164001465 2.6914892196655273 0.22453701496124268
CurrentTrain: epoch  5, batch    13 | loss: 7.1247382Losses:  7.304622173309326 2.9013724327087402 0.2552588880062103
CurrentTrain: epoch  5, batch    14 | loss: 7.3046222Losses:  8.136673927307129 3.158174991607666 0.24749678373336792
CurrentTrain: epoch  5, batch    15 | loss: 8.1366739Losses:  8.135160446166992 3.6660962104797363 0.2311679720878601
CurrentTrain: epoch  5, batch    16 | loss: 8.1351604Losses:  13.111480712890625 8.52273178100586 0.25272366404533386
CurrentTrain: epoch  5, batch    17 | loss: 13.1114807Losses:  6.632153511047363 2.004443883895874 0.24319350719451904
CurrentTrain: epoch  5, batch    18 | loss: 6.6321535Losses:  7.204261779785156 2.66907000541687 0.2324458807706833
CurrentTrain: epoch  5, batch    19 | loss: 7.2042618Losses:  6.890812873840332 2.5028676986694336 0.22717183828353882
CurrentTrain: epoch  5, batch    20 | loss: 6.8908129Losses:  7.251247882843018 2.466554880142212 0.24430230259895325
CurrentTrain: epoch  5, batch    21 | loss: 7.2512479Losses:  6.956927299499512 2.212378740310669 0.24171265959739685
CurrentTrain: epoch  5, batch    22 | loss: 6.9569273Losses:  6.661983966827393 2.195955276489258 0.23408840596675873
CurrentTrain: epoch  5, batch    23 | loss: 6.6619840Losses:  9.638670921325684 5.1344451904296875 0.2553290128707886
CurrentTrain: epoch  5, batch    24 | loss: 9.6386709Losses:  9.369771957397461 4.9776201248168945 0.25556135177612305
CurrentTrain: epoch  5, batch    25 | loss: 9.3697720Losses:  7.3766374588012695 2.9141578674316406 0.2385072410106659
CurrentTrain: epoch  5, batch    26 | loss: 7.3766375Losses:  8.825944900512695 4.287574768066406 0.25239020586013794
CurrentTrain: epoch  5, batch    27 | loss: 8.8259449Losses:  7.091130256652832 2.521622657775879 0.22690176963806152
CurrentTrain: epoch  5, batch    28 | loss: 7.0911303Losses:  6.685434818267822 2.2844161987304688 0.22319352626800537
CurrentTrain: epoch  5, batch    29 | loss: 6.6854348Losses:  7.4778289794921875 3.056006908416748 0.2570520341396332
CurrentTrain: epoch  5, batch    30 | loss: 7.4778290Losses:  9.745344161987305 5.327836036682129 0.2511126399040222
CurrentTrain: epoch  5, batch    31 | loss: 9.7453442Losses:  6.651228904724121 2.1770122051239014 0.22838880121707916
CurrentTrain: epoch  5, batch    32 | loss: 6.6512289Losses:  8.71699333190918 3.6424601078033447 0.23648323118686676
CurrentTrain: epoch  5, batch    33 | loss: 8.7169933Losses:  9.706148147583008 4.969261169433594 0.2519163489341736
CurrentTrain: epoch  5, batch    34 | loss: 9.7061481Losses:  10.038992881774902 5.474562644958496 0.26339781284332275
CurrentTrain: epoch  5, batch    35 | loss: 10.0389929Losses:  8.493388175964355 4.081335067749023 0.252513587474823
CurrentTrain: epoch  5, batch    36 | loss: 8.4933882Losses:  8.481741905212402 3.9349584579467773 0.24861212074756622
CurrentTrain: epoch  5, batch    37 | loss: 8.4817419Losses:  8.411786079406738 4.119972229003906 0.15280139446258545
CurrentTrain: epoch  5, batch    38 | loss: 8.4117861Losses:  6.716157913208008 2.2719554901123047 0.2260178178548813
CurrentTrain: epoch  5, batch    39 | loss: 6.7161579Losses:  6.800960540771484 2.2477715015411377 0.23616503179073334
CurrentTrain: epoch  5, batch    40 | loss: 6.8009605Losses:  7.3960065841674805 2.8105993270874023 0.23303233087062836
CurrentTrain: epoch  5, batch    41 | loss: 7.3960066Losses:  7.602996826171875 3.2769291400909424 0.23739446699619293
CurrentTrain: epoch  5, batch    42 | loss: 7.6029968Losses:  6.28916597366333 1.5577976703643799 0.22972716391086578
CurrentTrain: epoch  5, batch    43 | loss: 6.2891660Losses:  9.965179443359375 5.550844192504883 0.2534142732620239
CurrentTrain: epoch  5, batch    44 | loss: 9.9651794Losses:  9.72408390045166 5.360199928283691 0.1529485136270523
CurrentTrain: epoch  5, batch    45 | loss: 9.7240839Losses:  7.422055244445801 3.018280506134033 0.22244957089424133
CurrentTrain: epoch  5, batch    46 | loss: 7.4220552Losses:  6.08246374130249 1.6561065912246704 0.22006887197494507
CurrentTrain: epoch  5, batch    47 | loss: 6.0824637Losses:  7.613430976867676 3.250941276550293 0.235286146402359
CurrentTrain: epoch  5, batch    48 | loss: 7.6134310Losses:  7.461465835571289 1.9551249742507935 0.21602988243103027
CurrentTrain: epoch  5, batch    49 | loss: 7.4614658Losses:  7.454220294952393 2.810779094696045 0.22186461091041565
CurrentTrain: epoch  5, batch    50 | loss: 7.4542203Losses:  7.2269816398620605 2.169796943664551 0.23128758370876312
CurrentTrain: epoch  5, batch    51 | loss: 7.2269816Losses:  6.489232540130615 2.0190885066986084 0.21619579195976257
CurrentTrain: epoch  5, batch    52 | loss: 6.4892325Losses:  8.395750999450684 3.519735097885132 0.23850923776626587
CurrentTrain: epoch  5, batch    53 | loss: 8.3957510Losses:  10.235973358154297 5.872988700866699 0.25606364011764526
CurrentTrain: epoch  5, batch    54 | loss: 10.2359734Losses:  6.9262590408325195 2.0516717433929443 0.22525039315223694
CurrentTrain: epoch  5, batch    55 | loss: 6.9262590Losses:  9.153050422668457 4.750604152679443 0.26366931200027466
CurrentTrain: epoch  5, batch    56 | loss: 9.1530504Losses:  6.355401515960693 1.9417588710784912 0.23167827725410461
CurrentTrain: epoch  5, batch    57 | loss: 6.3554015Losses:  8.524957656860352 3.2277722358703613 0.2529950439929962
CurrentTrain: epoch  5, batch    58 | loss: 8.5249577Losses:  9.051444053649902 3.709195137023926 0.2455413043498993
CurrentTrain: epoch  5, batch    59 | loss: 9.0514441Losses:  11.641342163085938 6.727254867553711 0.25198712944984436
CurrentTrain: epoch  5, batch    60 | loss: 11.6413422Losses:  8.607233047485352 4.294856071472168 0.23573468625545502
CurrentTrain: epoch  5, batch    61 | loss: 8.6072330Losses:  4.892742156982422 0.4916926622390747 0.16111551225185394
CurrentTrain: epoch  5, batch    62 | loss: 4.8927422Losses:  6.586861610412598 2.188831329345703 0.22414343059062958
CurrentTrain: epoch  6, batch     0 | loss: 6.5868616Losses:  7.648778438568115 2.5831007957458496 0.23132625222206116
CurrentTrain: epoch  6, batch     1 | loss: 7.6487784Losses:  11.523750305175781 6.969918251037598 0.26406481862068176
CurrentTrain: epoch  6, batch     2 | loss: 11.5237503Losses:  9.768440246582031 4.916332244873047 0.23347130417823792
CurrentTrain: epoch  6, batch     3 | loss: 9.7684402Losses:  7.7555155754089355 2.7118630409240723 0.2436804324388504
CurrentTrain: epoch  6, batch     4 | loss: 7.7555156Losses:  9.55187702178955 5.228945732116699 0.2507118284702301
CurrentTrain: epoch  6, batch     5 | loss: 9.5518770Losses:  6.982315540313721 2.5946431159973145 0.23483482003211975
CurrentTrain: epoch  6, batch     6 | loss: 6.9823155Losses:  8.66431999206543 3.688079595565796 0.24996505677700043
CurrentTrain: epoch  6, batch     7 | loss: 8.6643200Losses:  8.611922264099121 3.8474321365356445 0.2587129771709442
CurrentTrain: epoch  6, batch     8 | loss: 8.6119223Losses:  6.971110820770264 2.467292308807373 0.2453230768442154
CurrentTrain: epoch  6, batch     9 | loss: 6.9711108Losses:  7.91615104675293 3.587949275970459 0.17426347732543945
CurrentTrain: epoch  6, batch    10 | loss: 7.9161510Losses:  8.473841667175293 3.8077480792999268 0.2545235753059387
CurrentTrain: epoch  6, batch    11 | loss: 8.4738417Losses:  6.66506290435791 2.3026280403137207 0.22765523195266724
CurrentTrain: epoch  6, batch    12 | loss: 6.6650629Losses:  8.60090160369873 4.284965515136719 0.2589351534843445
CurrentTrain: epoch  6, batch    13 | loss: 8.6009016Losses:  7.530338764190674 2.818406105041504 0.24686463177204132
CurrentTrain: epoch  6, batch    14 | loss: 7.5303388Losses:  7.530410289764404 3.2081408500671387 0.24626079201698303
CurrentTrain: epoch  6, batch    15 | loss: 7.5304103Losses:  7.482839107513428 3.0033411979675293 0.23399078845977783
CurrentTrain: epoch  6, batch    16 | loss: 7.4828391Losses:  10.341700553894043 6.040156364440918 0.25799497961997986
CurrentTrain: epoch  6, batch    17 | loss: 10.3417006Losses:  7.542992115020752 3.2971205711364746 0.23444248735904694
CurrentTrain: epoch  6, batch    18 | loss: 7.5429921Losses:  9.20203971862793 4.800942420959473 0.26244351267814636
CurrentTrain: epoch  6, batch    19 | loss: 9.2020397Losses:  8.615151405334473 4.357773780822754 0.2392323911190033
CurrentTrain: epoch  6, batch    20 | loss: 8.6151514Losses:  6.813488960266113 2.439448118209839 0.23602469265460968
CurrentTrain: epoch  6, batch    21 | loss: 6.8134890Losses:  8.74973201751709 4.409811019897461 0.23185378313064575
CurrentTrain: epoch  6, batch    22 | loss: 8.7497320Losses:  5.916617393493652 1.609367847442627 0.21546311676502228
CurrentTrain: epoch  6, batch    23 | loss: 5.9166174Losses:  7.132862567901611 2.8308424949645996 0.14772410690784454
CurrentTrain: epoch  6, batch    24 | loss: 7.1328626Losses:  8.09355640411377 3.469977855682373 0.25435537099838257
CurrentTrain: epoch  6, batch    25 | loss: 8.0935564Losses:  6.9074201583862305 2.4697964191436768 0.21945500373840332
CurrentTrain: epoch  6, batch    26 | loss: 6.9074202Losses:  10.739679336547852 6.267294883728027 0.17658478021621704
CurrentTrain: epoch  6, batch    27 | loss: 10.7396793Losses:  6.122245788574219 1.8399319648742676 0.22634130716323853
CurrentTrain: epoch  6, batch    28 | loss: 6.1222458Losses:  6.791927337646484 2.430881977081299 0.23532015085220337
CurrentTrain: epoch  6, batch    29 | loss: 6.7919273Losses:  7.802116394042969 3.225527048110962 0.2456103265285492
CurrentTrain: epoch  6, batch    30 | loss: 7.8021164Losses:  8.590383529663086 3.9906229972839355 0.16269519925117493
CurrentTrain: epoch  6, batch    31 | loss: 8.5903835Losses:  7.327775478363037 3.005013942718506 0.231381356716156
CurrentTrain: epoch  6, batch    32 | loss: 7.3277755Losses:  8.469755172729492 3.9935426712036133 0.16264232993125916
CurrentTrain: epoch  6, batch    33 | loss: 8.4697552Losses:  7.680968761444092 3.323274850845337 0.22958523035049438
CurrentTrain: epoch  6, batch    34 | loss: 7.6809688Losses:  6.296874046325684 1.9526574611663818 0.21632687747478485
CurrentTrain: epoch  6, batch    35 | loss: 6.2968740Losses:  6.783374309539795 2.483905792236328 0.24058271944522858
CurrentTrain: epoch  6, batch    36 | loss: 6.7833743Losses:  6.864493370056152 2.5175766944885254 0.24848084151744843
CurrentTrain: epoch  6, batch    37 | loss: 6.8644934Losses:  6.43463134765625 2.223783016204834 0.2215918004512787
CurrentTrain: epoch  6, batch    38 | loss: 6.4346313Losses:  7.5398783683776855 3.0639636516571045 0.24233104288578033
CurrentTrain: epoch  6, batch    39 | loss: 7.5398784Losses:  6.684295177459717 2.3998241424560547 0.23260194063186646
CurrentTrain: epoch  6, batch    40 | loss: 6.6842952Losses:  6.616941928863525 2.226651191711426 0.2214619368314743
CurrentTrain: epoch  6, batch    41 | loss: 6.6169419Losses:  7.006903648376465 2.6759567260742188 0.22421061992645264
CurrentTrain: epoch  6, batch    42 | loss: 7.0069036Losses:  9.545736312866211 4.2121124267578125 0.2349502146244049
CurrentTrain: epoch  6, batch    43 | loss: 9.5457363Losses:  6.512643814086914 1.9551242589950562 0.22066865861415863
CurrentTrain: epoch  6, batch    44 | loss: 6.5126438Losses:  6.735648155212402 2.4449217319488525 0.22533205151557922
CurrentTrain: epoch  6, batch    45 | loss: 6.7356482Losses:  6.269936561584473 1.596698522567749 0.2164270579814911
CurrentTrain: epoch  6, batch    46 | loss: 6.2699366Losses:  7.419645309448242 3.1429409980773926 0.2402888834476471
CurrentTrain: epoch  6, batch    47 | loss: 7.4196453Losses:  6.064341068267822 1.8091144561767578 0.21956408023834229
CurrentTrain: epoch  6, batch    48 | loss: 6.0643411Losses:  7.23158597946167 2.2766385078430176 0.2304191142320633
CurrentTrain: epoch  6, batch    49 | loss: 7.2315860Losses:  6.102384090423584 1.844474196434021 0.21756714582443237
CurrentTrain: epoch  6, batch    50 | loss: 6.1023841Losses:  7.521081924438477 3.2299017906188965 0.24613124132156372
CurrentTrain: epoch  6, batch    51 | loss: 7.5210819Losses:  6.604344367980957 1.8835563659667969 0.22732454538345337
CurrentTrain: epoch  6, batch    52 | loss: 6.6043444Losses:  7.9544219970703125 3.1940488815307617 0.22740447521209717
CurrentTrain: epoch  6, batch    53 | loss: 7.9544220Losses:  8.623750686645508 4.385281562805176 0.2366018295288086
CurrentTrain: epoch  6, batch    54 | loss: 8.6237507Losses:  7.207622528076172 2.981043577194214 0.2153170108795166
CurrentTrain: epoch  6, batch    55 | loss: 7.2076225Losses:  6.984684944152832 2.209514856338501 0.22934076189994812
CurrentTrain: epoch  6, batch    56 | loss: 6.9846849Losses:  7.630138874053955 2.7996890544891357 0.2261548638343811
CurrentTrain: epoch  6, batch    57 | loss: 7.6301389Losses:  6.645741939544678 2.221485137939453 0.2249084711074829
CurrentTrain: epoch  6, batch    58 | loss: 6.6457419Losses:  8.8360013961792 4.541856288909912 0.24093538522720337
CurrentTrain: epoch  6, batch    59 | loss: 8.8360014Losses:  6.684408187866211 2.2240383625030518 0.2182961106300354
CurrentTrain: epoch  6, batch    60 | loss: 6.6844082Losses:  6.803587436676025 2.5126519203186035 0.23185306787490845
CurrentTrain: epoch  6, batch    61 | loss: 6.8035874Losses:  4.720740795135498 0.4503958225250244 0.24911817908287048
CurrentTrain: epoch  6, batch    62 | loss: 4.7207408Losses:  6.786643981933594 2.346067428588867 0.22648262977600098
CurrentTrain: epoch  7, batch     0 | loss: 6.7866440Losses:  6.630246639251709 2.214505672454834 0.22463499009609222
CurrentTrain: epoch  7, batch     1 | loss: 6.6302466Losses:  7.315202713012695 3.031174659729004 0.2431107461452484
CurrentTrain: epoch  7, batch     2 | loss: 7.3152027Losses:  7.971565246582031 3.5651373863220215 0.2331395298242569
CurrentTrain: epoch  7, batch     3 | loss: 7.9715652Losses:  6.480006694793701 2.1862449645996094 0.2244822084903717
CurrentTrain: epoch  7, batch     4 | loss: 6.4800067Losses:  6.992979526519775 2.7628650665283203 0.22558733820915222
CurrentTrain: epoch  7, batch     5 | loss: 6.9929795Losses:  6.181901454925537 1.8185386657714844 0.22156375646591187
CurrentTrain: epoch  7, batch     6 | loss: 6.1819015Losses:  8.74508285522461 4.461965560913086 0.2412414848804474
CurrentTrain: epoch  7, batch     7 | loss: 8.7450829Losses:  7.341598987579346 3.0385305881500244 0.22637417912483215
CurrentTrain: epoch  7, batch     8 | loss: 7.3415990Losses:  10.0278902053833 5.787823677062988 0.25099462270736694
CurrentTrain: epoch  7, batch     9 | loss: 10.0278902Losses:  7.8015570640563965 3.473536968231201 0.2247357815504074
CurrentTrain: epoch  7, batch    10 | loss: 7.8015571Losses:  7.723148345947266 3.4201858043670654 0.24553225934505463
CurrentTrain: epoch  7, batch    11 | loss: 7.7231483Losses:  7.575705528259277 3.2414047718048096 0.24590373039245605
CurrentTrain: epoch  7, batch    12 | loss: 7.5757055Losses:  6.045590877532959 1.6032979488372803 0.21649213135242462
CurrentTrain: epoch  7, batch    13 | loss: 6.0455909Losses:  6.488296985626221 2.1893866062164307 0.22917978465557098
CurrentTrain: epoch  7, batch    14 | loss: 6.4882970Losses:  7.070501804351807 2.7554333209991455 0.2363048642873764
CurrentTrain: epoch  7, batch    15 | loss: 7.0705018Losses:  8.734458923339844 4.396112442016602 0.2453296184539795
CurrentTrain: epoch  7, batch    16 | loss: 8.7344589Losses:  7.917058944702148 3.303464412689209 0.2422507107257843
CurrentTrain: epoch  7, batch    17 | loss: 7.9170589Losses:  7.312597751617432 2.792985200881958 0.22746513783931732
CurrentTrain: epoch  7, batch    18 | loss: 7.3125978Losses:  6.8882269859313965 2.613168716430664 0.21828602254390717
CurrentTrain: epoch  7, batch    19 | loss: 6.8882270Losses:  7.289235591888428 2.6548256874084473 0.21896548569202423
CurrentTrain: epoch  7, batch    20 | loss: 7.2892356Losses:  7.067048072814941 2.7539563179016113 0.23422861099243164
CurrentTrain: epoch  7, batch    21 | loss: 7.0670481Losses:  7.668802261352539 3.205930233001709 0.23455697298049927
CurrentTrain: epoch  7, batch    22 | loss: 7.6688023Losses:  6.466973304748535 2.173030376434326 0.2225758135318756
CurrentTrain: epoch  7, batch    23 | loss: 6.4669733Losses:  9.994213104248047 5.504890441894531 0.26006877422332764
CurrentTrain: epoch  7, batch    24 | loss: 9.9942131Losses:  7.720322132110596 3.445887565612793 0.23820491135120392
CurrentTrain: epoch  7, batch    25 | loss: 7.7203221Losses:  8.967334747314453 4.72573184967041 0.15752005577087402
CurrentTrain: epoch  7, batch    26 | loss: 8.9673347Losses:  8.582773208618164 4.3582048416137695 0.23403999209403992
CurrentTrain: epoch  7, batch    27 | loss: 8.5827732Losses:  6.963294982910156 2.5734686851501465 0.23361074924468994
CurrentTrain: epoch  7, batch    28 | loss: 6.9632950Losses:  7.794462203979492 3.488114356994629 0.242652028799057
CurrentTrain: epoch  7, batch    29 | loss: 7.7944622Losses:  7.506725788116455 3.090088129043579 0.2166239619255066
CurrentTrain: epoch  7, batch    30 | loss: 7.5067258Losses:  6.027135372161865 1.8093464374542236 0.22122181951999664
CurrentTrain: epoch  7, batch    31 | loss: 6.0271354Losses:  6.979916095733643 2.732198715209961 0.22408443689346313
CurrentTrain: epoch  7, batch    32 | loss: 6.9799161Losses:  6.4644775390625 2.1666064262390137 0.21292582154273987
CurrentTrain: epoch  7, batch    33 | loss: 6.4644775Losses:  7.135073184967041 2.704164505004883 0.223279669880867
CurrentTrain: epoch  7, batch    34 | loss: 7.1350732Losses:  6.175482273101807 1.946529507637024 0.21875321865081787
CurrentTrain: epoch  7, batch    35 | loss: 6.1754823Losses:  11.108785629272461 6.682111740112305 0.2605302333831787
CurrentTrain: epoch  7, batch    36 | loss: 11.1087856Losses:  7.478273391723633 3.123350143432617 0.23765665292739868
CurrentTrain: epoch  7, batch    37 | loss: 7.4782734Losses:  7.162076950073242 2.7841124534606934 0.24421347677707672
CurrentTrain: epoch  7, batch    38 | loss: 7.1620770Losses:  6.055224895477295 1.7990710735321045 0.22601260244846344
CurrentTrain: epoch  7, batch    39 | loss: 6.0552249Losses:  7.948757648468018 3.753829002380371 0.2309320569038391
CurrentTrain: epoch  7, batch    40 | loss: 7.9487576Losses:  7.416678428649902 2.973862409591675 0.2400244176387787
CurrentTrain: epoch  7, batch    41 | loss: 7.4166784Losses:  6.49597692489624 2.2201507091522217 0.21919217705726624
CurrentTrain: epoch  7, batch    42 | loss: 6.4959769Losses:  7.5200300216674805 3.1815176010131836 0.24442453682422638
CurrentTrain: epoch  7, batch    43 | loss: 7.5200300Losses:  6.969125270843506 2.671949863433838 0.23053082823753357
CurrentTrain: epoch  7, batch    44 | loss: 6.9691253Losses:  6.42572546005249 2.099794864654541 0.14925740659236908
CurrentTrain: epoch  7, batch    45 | loss: 6.4257255Losses:  6.511998176574707 2.243826150894165 0.22651077806949615
CurrentTrain: epoch  7, batch    46 | loss: 6.5119982Losses:  6.613119602203369 2.4212284088134766 0.2154230773448944
CurrentTrain: epoch  7, batch    47 | loss: 6.6131196Losses:  7.962958335876465 3.5634608268737793 0.24267038702964783
CurrentTrain: epoch  7, batch    48 | loss: 7.9629583Losses:  6.83181095123291 2.5456953048706055 0.14401261508464813
CurrentTrain: epoch  7, batch    49 | loss: 6.8318110Losses:  7.4232964515686035 3.1564271450042725 0.23228411376476288
CurrentTrain: epoch  7, batch    50 | loss: 7.4232965Losses:  7.506242275238037 3.196377992630005 0.22978636622428894
CurrentTrain: epoch  7, batch    51 | loss: 7.5062423Losses:  6.238492965698242 2.0056328773498535 0.22132320702075958
CurrentTrain: epoch  7, batch    52 | loss: 6.2384930Losses:  6.8796868324279785 2.6332247257232666 0.21608301997184753
CurrentTrain: epoch  7, batch    53 | loss: 6.8796868Losses:  7.128005027770996 2.7893967628479004 0.23471477627754211
CurrentTrain: epoch  7, batch    54 | loss: 7.1280050Losses:  6.459712505340576 2.1393496990203857 0.22716867923736572
CurrentTrain: epoch  7, batch    55 | loss: 6.4597125Losses:  7.021213054656982 2.796036720275879 0.2251560539007187
CurrentTrain: epoch  7, batch    56 | loss: 7.0212131Losses:  8.045172691345215 2.5654053688049316 0.22442248463630676
CurrentTrain: epoch  7, batch    57 | loss: 8.0451727Losses:  7.566189289093018 3.358271598815918 0.23377934098243713
CurrentTrain: epoch  7, batch    58 | loss: 7.5661893Losses:  6.37180233001709 2.164539337158203 0.22460296750068665
CurrentTrain: epoch  7, batch    59 | loss: 6.3718023Losses:  9.180895805358887 4.735347747802734 0.2643752694129944
CurrentTrain: epoch  7, batch    60 | loss: 9.1808958Losses:  7.587215900421143 3.3172266483306885 0.24002599716186523
CurrentTrain: epoch  7, batch    61 | loss: 7.5872159Losses:  4.542929649353027 0.2345227748155594 0.2536771297454834
CurrentTrain: epoch  7, batch    62 | loss: 4.5429296Losses:  7.844629287719727 3.3688275814056396 0.23372915387153625
CurrentTrain: epoch  8, batch     0 | loss: 7.8446293Losses:  6.1891303062438965 1.9600954055786133 0.2142501324415207
CurrentTrain: epoch  8, batch     1 | loss: 6.1891303Losses:  10.235757827758789 5.106328964233398 0.235859215259552
CurrentTrain: epoch  8, batch     2 | loss: 10.2357578Losses:  7.28138542175293 2.6132566928863525 0.22453801333904266
CurrentTrain: epoch  8, batch     3 | loss: 7.2813854Losses:  8.108078002929688 3.1577794551849365 0.25723913311958313
CurrentTrain: epoch  8, batch     4 | loss: 8.1080780Losses:  8.208252906799316 3.930558204650879 0.24646581709384918
CurrentTrain: epoch  8, batch     5 | loss: 8.2082529Losses:  6.648734092712402 2.387906074523926 0.2316630333662033
CurrentTrain: epoch  8, batch     6 | loss: 6.6487341Losses:  6.439763069152832 2.1835615634918213 0.2202998399734497
CurrentTrain: epoch  8, batch     7 | loss: 6.4397631Losses:  7.805426120758057 3.5431203842163086 0.22679361701011658
CurrentTrain: epoch  8, batch     8 | loss: 7.8054261Losses:  7.1420745849609375 2.913722276687622 0.21854877471923828
CurrentTrain: epoch  8, batch     9 | loss: 7.1420746Losses:  8.682876586914062 4.052164077758789 0.14245443046092987
CurrentTrain: epoch  8, batch    10 | loss: 8.6828766Losses:  6.463708877563477 2.2457683086395264 0.218681201338768
CurrentTrain: epoch  8, batch    11 | loss: 6.4637089Losses:  6.397202014923096 2.1802520751953125 0.22772055864334106
CurrentTrain: epoch  8, batch    12 | loss: 6.3972020Losses:  8.872797012329102 4.607950210571289 0.24520991742610931
CurrentTrain: epoch  8, batch    13 | loss: 8.8727970Losses:  8.88733196258545 4.622621536254883 0.22995975613594055
CurrentTrain: epoch  8, batch    14 | loss: 8.8873320Losses:  6.168500900268555 1.7696752548217773 0.21978776156902313
CurrentTrain: epoch  8, batch    15 | loss: 6.1685009Losses:  6.73321533203125 2.430072069168091 0.2327602505683899
CurrentTrain: epoch  8, batch    16 | loss: 6.7332153Losses:  8.745026588439941 4.368647575378418 0.2334558069705963
CurrentTrain: epoch  8, batch    17 | loss: 8.7450266Losses:  6.879461765289307 2.3353195190429688 0.22601079940795898
CurrentTrain: epoch  8, batch    18 | loss: 6.8794618Losses:  6.028920650482178 1.8292884826660156 0.21620461344718933
CurrentTrain: epoch  8, batch    19 | loss: 6.0289207Losses:  6.137667655944824 1.9217820167541504 0.22204312682151794
CurrentTrain: epoch  8, batch    20 | loss: 6.1376677Losses:  7.101381301879883 2.722161054611206 0.23770995438098907
CurrentTrain: epoch  8, batch    21 | loss: 7.1013813Losses:  6.404509544372559 2.168142318725586 0.22710691392421722
CurrentTrain: epoch  8, batch    22 | loss: 6.4045095Losses:  6.823896884918213 2.645404815673828 0.2265334129333496
CurrentTrain: epoch  8, batch    23 | loss: 6.8238969Losses:  12.907366752624512 8.634312629699707 0.24368882179260254
CurrentTrain: epoch  8, batch    24 | loss: 12.9073668Losses:  6.578514099121094 2.348613977432251 0.23327088356018066
CurrentTrain: epoch  8, batch    25 | loss: 6.5785141Losses:  9.38913345336914 4.128868103027344 0.242219015955925
CurrentTrain: epoch  8, batch    26 | loss: 9.3891335Losses:  6.709829330444336 2.47725772857666 0.22875401377677917
CurrentTrain: epoch  8, batch    27 | loss: 6.7098293Losses:  6.445963382720947 1.9986720085144043 0.22008870542049408
CurrentTrain: epoch  8, batch    28 | loss: 6.4459634Losses:  7.404636383056641 2.7780725955963135 0.13457494974136353
CurrentTrain: epoch  8, batch    29 | loss: 7.4046364Losses:  7.235832214355469 2.8949053287506104 0.2302904576063156
CurrentTrain: epoch  8, batch    30 | loss: 7.2358322Losses:  6.062867164611816 1.9036720991134644 0.21563270688056946
CurrentTrain: epoch  8, batch    31 | loss: 6.0628672Losses:  6.406472682952881 2.1886725425720215 0.2325848489999771
CurrentTrain: epoch  8, batch    32 | loss: 6.4064727Losses:  7.7544074058532715 3.533653974533081 0.23274660110473633
CurrentTrain: epoch  8, batch    33 | loss: 7.7544074Losses:  9.429852485656738 5.172303199768066 0.24394108355045319
CurrentTrain: epoch  8, batch    34 | loss: 9.4298525Losses:  6.556070804595947 2.377958297729492 0.20987695455551147
CurrentTrain: epoch  8, batch    35 | loss: 6.5560708Losses:  6.2565836906433105 2.022099494934082 0.22832532227039337
CurrentTrain: epoch  8, batch    36 | loss: 6.2565837Losses:  7.099081039428711 2.9013724327087402 0.22576572000980377
CurrentTrain: epoch  8, batch    37 | loss: 7.0990810Losses:  7.443295001983643 3.2608704566955566 0.22561243176460266
CurrentTrain: epoch  8, batch    38 | loss: 7.4432950Losses:  7.371883869171143 3.1295034885406494 0.23666813969612122
CurrentTrain: epoch  8, batch    39 | loss: 7.3718839Losses:  5.979255199432373 1.7771586179733276 0.2204855978488922
CurrentTrain: epoch  8, batch    40 | loss: 5.9792552Losses:  6.592874526977539 2.2501564025878906 0.2281283289194107
CurrentTrain: epoch  8, batch    41 | loss: 6.5928745Losses:  7.29154634475708 3.0899810791015625 0.2292502224445343
CurrentTrain: epoch  8, batch    42 | loss: 7.2915463Losses:  9.407625198364258 5.210862636566162 0.23507344722747803
CurrentTrain: epoch  8, batch    43 | loss: 9.4076252Losses:  6.411294937133789 1.99326491355896 0.21865928173065186
CurrentTrain: epoch  8, batch    44 | loss: 6.4112949Losses:  6.8635640144348145 2.189427375793457 0.2149253785610199
CurrentTrain: epoch  8, batch    45 | loss: 6.8635640Losses:  6.071837425231934 1.6267521381378174 0.212080717086792
CurrentTrain: epoch  8, batch    46 | loss: 6.0718374Losses:  6.870182037353516 2.617508888244629 0.2253313660621643
CurrentTrain: epoch  8, batch    47 | loss: 6.8701820Losses:  7.7421345710754395 3.2963123321533203 0.23506075143814087
CurrentTrain: epoch  8, batch    48 | loss: 7.7421346Losses:  6.526947975158691 2.324828624725342 0.2294362485408783
CurrentTrain: epoch  8, batch    49 | loss: 6.5269480Losses:  7.178816795349121 2.873274326324463 0.235201895236969
CurrentTrain: epoch  8, batch    50 | loss: 7.1788168Losses:  7.2469000816345215 3.0120632648468018 0.214044988155365
CurrentTrain: epoch  8, batch    51 | loss: 7.2469001Losses:  6.880526065826416 2.674067974090576 0.23000812530517578
CurrentTrain: epoch  8, batch    52 | loss: 6.8805261Losses:  7.311111927032471 3.0881781578063965 0.22846664488315582
CurrentTrain: epoch  8, batch    53 | loss: 7.3111119Losses:  7.1703643798828125 2.959467649459839 0.2174166738986969
CurrentTrain: epoch  8, batch    54 | loss: 7.1703644Losses:  6.768101215362549 2.5716938972473145 0.22515389323234558
CurrentTrain: epoch  8, batch    55 | loss: 6.7681012Losses:  7.0215349197387695 2.683718681335449 0.23988988995552063
CurrentTrain: epoch  8, batch    56 | loss: 7.0215349Losses:  8.379018783569336 4.163344383239746 0.23962759971618652
CurrentTrain: epoch  8, batch    57 | loss: 8.3790188Losses:  7.98879337310791 3.725627899169922 0.2351050078868866
CurrentTrain: epoch  8, batch    58 | loss: 7.9887934Losses:  7.8909196853637695 3.634045124053955 0.2406248152256012
CurrentTrain: epoch  8, batch    59 | loss: 7.8909197Losses:  6.374656677246094 2.1749584674835205 0.21421250700950623
CurrentTrain: epoch  8, batch    60 | loss: 6.3746567Losses:  6.941145420074463 2.695969581604004 0.2315848171710968
CurrentTrain: epoch  8, batch    61 | loss: 6.9411454Losses:  7.086854457855225 2.8836679458618164 0.18569032847881317
CurrentTrain: epoch  8, batch    62 | loss: 7.0868545Losses:  7.253565788269043 3.002030372619629 0.22348028421401978
CurrentTrain: epoch  9, batch     0 | loss: 7.2535658Losses:  5.4759440422058105 1.2762001752853394 0.2127380669116974
CurrentTrain: epoch  9, batch     1 | loss: 5.4759440Losses:  7.411570072174072 3.2152132987976074 0.23386022448539734
CurrentTrain: epoch  9, batch     2 | loss: 7.4115701Losses:  6.255494594573975 2.1000335216522217 0.21992263197898865
CurrentTrain: epoch  9, batch     3 | loss: 6.2554946Losses:  7.071600437164307 2.8706703186035156 0.22017553448677063
CurrentTrain: epoch  9, batch     4 | loss: 7.0716004Losses:  8.128985404968262 3.888082265853882 0.2367229461669922
CurrentTrain: epoch  9, batch     5 | loss: 8.1289854Losses:  5.771003723144531 1.5522409677505493 0.21306203305721283
CurrentTrain: epoch  9, batch     6 | loss: 5.7710037Losses:  7.822518825531006 3.436098098754883 0.2379612922668457
CurrentTrain: epoch  9, batch     7 | loss: 7.8225188Losses:  6.9026875495910645 2.653587818145752 0.2399149388074875
CurrentTrain: epoch  9, batch     8 | loss: 6.9026875Losses:  7.149779796600342 2.910038948059082 0.24314850568771362
CurrentTrain: epoch  9, batch     9 | loss: 7.1497798Losses:  7.632585525512695 3.417219877243042 0.23425176739692688
CurrentTrain: epoch  9, batch    10 | loss: 7.6325855Losses:  8.334820747375488 4.149209022521973 0.2240636646747589
CurrentTrain: epoch  9, batch    11 | loss: 8.3348207Losses:  7.249997615814209 2.891928195953369 0.23961614072322845
CurrentTrain: epoch  9, batch    12 | loss: 7.2499976Losses:  8.408486366271973 4.141002655029297 0.24341605603694916
CurrentTrain: epoch  9, batch    13 | loss: 8.4084864Losses:  7.5433149337768555 3.289431571960449 0.23732484877109528
CurrentTrain: epoch  9, batch    14 | loss: 7.5433149Losses:  7.847987174987793 3.6525821685791016 0.22864317893981934
CurrentTrain: epoch  9, batch    15 | loss: 7.8479872Losses:  7.472012996673584 3.1736395359039307 0.22428451478481293
CurrentTrain: epoch  9, batch    16 | loss: 7.4720130Losses:  7.432674884796143 3.1665139198303223 0.22759310901165009
CurrentTrain: epoch  9, batch    17 | loss: 7.4326749Losses:  7.97299861907959 3.7408437728881836 0.23821529746055603
CurrentTrain: epoch  9, batch    18 | loss: 7.9729986Losses:  6.9965033531188965 2.6879351139068604 0.2429136484861374
CurrentTrain: epoch  9, batch    19 | loss: 6.9965034Losses:  6.794323921203613 2.477693557739258 0.23068052530288696
CurrentTrain: epoch  9, batch    20 | loss: 6.7943239Losses:  6.749263286590576 2.548668384552002 0.21564418077468872
CurrentTrain: epoch  9, batch    21 | loss: 6.7492633Losses:  7.9632415771484375 3.703467845916748 0.24567630887031555
CurrentTrain: epoch  9, batch    22 | loss: 7.9632416Losses:  7.038485527038574 2.7387845516204834 0.23100638389587402
CurrentTrain: epoch  9, batch    23 | loss: 7.0384855Losses:  8.711113929748535 4.4827423095703125 0.23752813041210175
CurrentTrain: epoch  9, batch    24 | loss: 8.7111139Losses:  8.006941795349121 3.763072967529297 0.23622021079063416
CurrentTrain: epoch  9, batch    25 | loss: 8.0069418Losses:  8.763571739196777 4.5739006996154785 0.24284300208091736
CurrentTrain: epoch  9, batch    26 | loss: 8.7635717Losses:  6.167221546173096 2.0264813899993896 0.13453061878681183
CurrentTrain: epoch  9, batch    27 | loss: 6.1672215Losses:  5.793467998504639 1.5621209144592285 0.21088175475597382
CurrentTrain: epoch  9, batch    28 | loss: 5.7934680Losses:  6.873992443084717 2.6836137771606445 0.22187775373458862
CurrentTrain: epoch  9, batch    29 | loss: 6.8739924Losses:  6.309091091156006 2.1210007667541504 0.21774277091026306
CurrentTrain: epoch  9, batch    30 | loss: 6.3090911Losses:  7.192935466766357 2.9833788871765137 0.21978139877319336
CurrentTrain: epoch  9, batch    31 | loss: 7.1929355Losses:  7.023982048034668 2.7932701110839844 0.23059016466140747
CurrentTrain: epoch  9, batch    32 | loss: 7.0239820Losses:  6.579092025756836 2.2954320907592773 0.2331482172012329
CurrentTrain: epoch  9, batch    33 | loss: 6.5790920Losses:  7.75676155090332 3.478097438812256 0.2422744333744049
CurrentTrain: epoch  9, batch    34 | loss: 7.7567616Losses:  7.287507057189941 3.1098546981811523 0.22426308691501617
CurrentTrain: epoch  9, batch    35 | loss: 7.2875071Losses:  6.140043258666992 2.032320976257324 0.1345033049583435
CurrentTrain: epoch  9, batch    36 | loss: 6.1400433Losses:  8.19863510131836 3.954306125640869 0.2283516228199005
CurrentTrain: epoch  9, batch    37 | loss: 8.1986351Losses:  7.966499328613281 3.828970432281494 0.2349872589111328
CurrentTrain: epoch  9, batch    38 | loss: 7.9664993Losses:  6.6322550773620605 2.4829208850860596 0.22945469617843628
CurrentTrain: epoch  9, batch    39 | loss: 6.6322551Losses:  7.4008917808532715 3.1689882278442383 0.23718252778053284
CurrentTrain: epoch  9, batch    40 | loss: 7.4008918Losses:  8.869532585144043 4.64193058013916 0.24608661234378815
CurrentTrain: epoch  9, batch    41 | loss: 8.8695326Losses:  7.699184894561768 3.452699661254883 0.23028844594955444
CurrentTrain: epoch  9, batch    42 | loss: 7.6991849Losses:  7.887511730194092 3.7667107582092285 0.1451287865638733
CurrentTrain: epoch  9, batch    43 | loss: 7.8875117Losses:  7.97722864151001 3.776289463043213 0.22767122089862823
CurrentTrain: epoch  9, batch    44 | loss: 7.9772286Losses:  7.214615821838379 3.011411666870117 0.22349359095096588
CurrentTrain: epoch  9, batch    45 | loss: 7.2146158Losses:  9.425811767578125 5.213373184204102 0.23543210327625275
CurrentTrain: epoch  9, batch    46 | loss: 9.4258118Losses:  7.62961483001709 3.4182891845703125 0.22096902132034302
CurrentTrain: epoch  9, batch    47 | loss: 7.6296148Losses:  7.084933280944824 2.888166904449463 0.22371336817741394
CurrentTrain: epoch  9, batch    48 | loss: 7.0849333Losses:  7.131278991699219 2.9350123405456543 0.21431896090507507
CurrentTrain: epoch  9, batch    49 | loss: 7.1312790Losses:  7.5036725997924805 3.2818872928619385 0.2492743879556656
CurrentTrain: epoch  9, batch    50 | loss: 7.5036726Losses:  6.56488561630249 2.372188091278076 0.22575625777244568
CurrentTrain: epoch  9, batch    51 | loss: 6.5648856Losses:  6.399744510650635 2.217900276184082 0.2200046181678772
CurrentTrain: epoch  9, batch    52 | loss: 6.3997445Losses:  9.951464653015137 5.6924896240234375 0.26522141695022583
CurrentTrain: epoch  9, batch    53 | loss: 9.9514647Losses:  7.2653069496154785 3.0710268020629883 0.2392769753932953
CurrentTrain: epoch  9, batch    54 | loss: 7.2653069Losses:  8.378944396972656 4.1518168449401855 0.25595784187316895
CurrentTrain: epoch  9, batch    55 | loss: 8.3789444Losses:  8.616341590881348 4.377009868621826 0.2544485926628113
CurrentTrain: epoch  9, batch    56 | loss: 8.6163416Losses:  7.275106430053711 3.0884008407592773 0.2318744659423828
CurrentTrain: epoch  9, batch    57 | loss: 7.2751064Losses:  8.051487922668457 3.841604709625244 0.24289584159851074
CurrentTrain: epoch  9, batch    58 | loss: 8.0514879Losses:  7.931814670562744 3.811243772506714 0.15603691339492798
CurrentTrain: epoch  9, batch    59 | loss: 7.9318147Losses:  6.188068389892578 1.9822254180908203 0.2292705774307251
CurrentTrain: epoch  9, batch    60 | loss: 6.1880684Losses:  7.302828788757324 3.1103310585021973 0.2279461920261383
CurrentTrain: epoch  9, batch    61 | loss: 7.3028288Losses:  4.700133323669434 0.4611726999282837 0.25393199920654297
CurrentTrain: epoch  9, batch    62 | loss: 4.7001333
#### movement ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "movement" means literary, artistic, scientific or philosophical movement or scene associated with this person or work.
Here is an example:
Relation: movement
Context: géricault and delacroix were the most important painters of the romanticism .
Head Entity: delacroix
Tail Entity: romanticism
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "movement":
Relation: movement  
Context: the surrealism movement was significantly influenced by the works of salvador dalí and max ernst.  
Head Entity: dalí  
Tail Entity: surrealism  

Relation: movement  
Context: the cubism movement, pioneered by pablo picasso and georges braque, revolutionized modern art.  
Head Entity: picasso  
Tail Entity: cubism  
#### located in the administrative territorial entity ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "located in the administrative territorial entity" means the item is located on the territory of the following administrative entity.
Here is an example:
Relation: located in the administrative territorial entity
Context: it is located on fm 409 , northeast of bonham in fannin county in the u.s. state of texas .
Head Entity: bonham
Tail Entity: fannin county
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "located in the administrative territorial entity":
Relation: located in the administrative territorial entity  
Context: the historic site is situated in the heart of downtown nashville, within davidson county, tennessee.  
Head Entity: nashville  
Tail Entity: davidson county  

Relation: located in the administrative territorial entity  
Context: the university campus is found in the city of berkeley, which is part of alameda county in california.  
Head Entity: berkeley  
Tail Entity: alameda county  
#### head of government ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "head of government" means head of the executive power of this town, city, municipality, state, country, or other governmental body.
Here is an example:
Relation: head of government
Context: her role as chief of protocol began as a volunteer for then - san francisco mayor jack shelley , spanning over seven mayoral administrations until mayor frank jordan 's term .
Head Entity: san francisco
Tail Entity: frank jordan
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "head of government":
Relation: head of government  
Context: during his tenure as the mayor of new york city, bill de blasio implemented several progressive policies aimed at improving the lives of residents across the five boroughs.  
Head Entity: new york city  
Tail Entity: bill de blasio  

Relation: head of government  
Context: the prime minister of canada, justin trudeau, announced new initiatives to combat climate change during his address at the global summit.  
Head Entity: canada  
Tail Entity: justin trudeau  
#### member of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "member of" means organization, club or musical group to which the subject belongs.
Here is an example:
Relation: member of
Context: the completed song was originally released on the 1971 album " the cry of love"the first posthumous album of original material compiled by the jimi hendrix experience drummer mitch mitchell and engineer eddie kramer .
Head Entity: mitch mitchell
Tail Entity: the jimi hendrix experience
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "member of":
Relation: member of  
Context: After years of hard work and dedication, Sarah was finally accepted into the prestigious National Art Society, where she could collaborate with other talented artists.  
Head Entity: Sarah  
Tail Entity: National Art Society  

Relation: member of  
Context: The renowned pianist has been a member of the International Music Association for over a decade, participating in various concerts and workshops around the world.  
Head Entity: the renowned pianist  
Tail Entity: International Music Association  
#### crosses ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "crosses" means obstacle which this bridge crosses over or this tunnel goes under.
Here is an example:
Relation: crosses
Context: the solkan bridge ( , ) is a arch bridge over the soča river near nova gorica in western slovenia ( by railway terminology it is a viaduct ) .
Head Entity: solkan bridge
Tail Entity: soča river
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "crosses":
Relation: crosses  
Context: the golden gate bridge ( , ) spans the golden gate strait, the entrance to san francisco bay from the pacific ocean.  
Head Entity: golden gate bridge  
Tail Entity: golden gate strait  

Relation: crosses  
Context: the channel tunnel ( , ) runs beneath the english channel, connecting folkestone in the uk with coqelles near calais in france.  
Head Entity: channel tunnel  
Tail Entity: english channel  
#### tributary ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "tributary" means watercourse that flows into an other one.
Here is an example:
Relation: tributary
Context: the city is bisected by the iset river , which flows from the urals into the tobol river .
Head Entity: tobol river
Tail Entity: iset river
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "tributary":
Relation: tributary  
Context: The Amazon River, known for its vast network of tributaries, receives water from the Madeira River, which is one of its largest tributaries.  
Head Entity: Amazon River  
Tail Entity: Madeira River  

Relation: tributary  
Context: The Mississippi River is fed by numerous smaller rivers, including the Missouri River, which serves as a significant tributary.  
Head Entity: Mississippi River  
Tail Entity: Missouri River  
#### position held ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "position held" means subject currently or formerly holds the object position or public office.
Here is an example:
Relation: position held
Context: the result was widely interpreted as a personal rebuke to then chief minister shane stone .
Head Entity: shane stone
Tail Entity: chief minister
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "position held":
Relation: position held  
Context: After serving as the mayor for over a decade, John Smith announced his retirement from the position.  
Head Entity: John Smith  
Tail Entity: mayor  

Relation: position held  
Context: The former president of the organization, Maria Lopez, was recognized for her contributions during her tenure.  
Head Entity: Maria Lopez  
Tail Entity: president
#### developer ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "developer" means organization or person that developed the item.
Here is an example:
Relation: developer
Context: it was developed by syn sophia , who also developed the other games in the " style savvy " series .
Head Entity: style savvy
Tail Entity: syn sophia
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "developer":
Relation: developer  
Context: the popular game was created by epic games, known for their innovative approach to game design.  
Head Entity: popular game  
Tail Entity: epic games  

Relation: developer  
Context: the groundbreaking software was launched by tech innovations inc., a leader in the tech industry.  
Head Entity: groundbreaking software  
Tail Entity: tech innovations inc.  
#### location of formation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "location of formation" means location where a group or organization was formed.
Here is an example:
Relation: location of formation
Context: fuzzy duck is the self - titled album by london - based progressive rock band fuzzy duck .
Head Entity: fuzzy duck
Tail Entity: london
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "location of formation":
Relation: location of formation  
Context: the tech startup was founded in silicon valley by a group of innovative engineers.  
Head Entity: tech startup  
Tail Entity: silicon valley  

Relation: location of formation  
Context: the famous painting was created by the artist in his studio located in paris.  
Head Entity: artist  
Tail Entity: paris  
#### country of origin ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "country of origin" means country of origin of this item (creative work, food, phrase, product, etc.).
Here is an example:
Relation: country of origin
Context: " shining time station " won a number of awards and significantly increased the popularity of the " thomas " media franchise in the united states .
Head Entity: shining time station
Tail Entity: united states
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "country of origin":
Relation: country of origin  
Context: The famous dish "sushi" is traditionally associated with Japan and has gained immense popularity worldwide.  
Head Entity: sushi  
Tail Entity: Japan  

Relation: country of origin  
Context: The iconic brand "Guinness" is known for its rich stout beer, which originated in Ireland and is now enjoyed globally.  
Head Entity: Guinness  
Tail Entity: Ireland  
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 82.81%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 82.50%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 85.71%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 88.89%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 90.00%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 90.91%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 91.67%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 91.83%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 91.96%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 92.08%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 92.58%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 93.01%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 93.06%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 93.42%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 94.05%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 94.03%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 94.29%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 94.27%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 94.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 94.23%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 94.21%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 94.20%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 94.18%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 94.38%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 94.56%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 94.73%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 94.89%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 95.04%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 95.00%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 95.14%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 95.27%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 95.23%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 95.35%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 95.47%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 95.58%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 95.68%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 95.78%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 95.88%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 95.97%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 96.06%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 96.01%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 95.96%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 96.05%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 96.12%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 96.08%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 95.91%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 95.87%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 95.95%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 95.68%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 95.65%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 95.61%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 95.58%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 95.66%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 95.62%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 95.70%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 95.67%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 94.94%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 82.81%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 82.50%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 85.71%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 88.89%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 90.00%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 90.91%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 91.67%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 91.83%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 91.96%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 92.08%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 92.58%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 93.01%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 93.06%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 93.42%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 94.05%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 94.03%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 94.29%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 94.27%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 94.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 94.23%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 94.21%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 94.20%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 94.18%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 94.38%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 94.56%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 94.73%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 94.89%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 95.04%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 95.00%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 95.14%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 95.27%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 95.23%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 95.35%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 95.47%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 95.58%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 95.68%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 95.78%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 95.88%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 95.97%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 96.06%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 96.01%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 95.96%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 96.05%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 96.12%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 96.08%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 95.91%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 95.87%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 95.95%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 95.68%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 95.65%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 95.61%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 95.58%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 95.66%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 95.62%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 95.70%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 95.67%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 94.94%   
cur_acc:  ['0.9494']
his_acc:  ['0.9494']
Clustering into  9  clusters
Clusters:  [5 1 3 0 2 2 3 6 1 1 7 6 6 2 0 1 5 2 8 4]
Losses:  12.493231773376465 4.835947036743164 0.7749698162078857
CurrentTrain: epoch  0, batch     0 | loss: 12.4932318Losses:  11.893280029296875 3.79072904586792 0.7637791037559509
CurrentTrain: epoch  0, batch     1 | loss: 11.8932800Losses:  12.064728736877441 4.774773597717285 0.703964114189148
CurrentTrain: epoch  0, batch     2 | loss: 12.0647287Losses:  6.468757629394531 -0.0 0.07589320093393326
CurrentTrain: epoch  0, batch     3 | loss: 6.4687576Losses:  11.611157417297363 3.7603461742401123 0.6982134580612183
CurrentTrain: epoch  1, batch     0 | loss: 11.6111574Losses:  10.087088584899902 3.5994820594787598 0.6301488280296326
CurrentTrain: epoch  1, batch     1 | loss: 10.0870886Losses:  8.609795570373535 2.429064989089966 0.7756990194320679
CurrentTrain: epoch  1, batch     2 | loss: 8.6097956Losses:  3.2017998695373535 -0.0 0.09547155350446701
CurrentTrain: epoch  1, batch     3 | loss: 3.2017999Losses:  9.78273868560791 3.42753267288208 0.6422607898712158
CurrentTrain: epoch  2, batch     0 | loss: 9.7827387Losses:  9.906972885131836 3.7014102935791016 0.6222679615020752
CurrentTrain: epoch  2, batch     1 | loss: 9.9069729Losses:  8.540541648864746 3.8671228885650635 0.7343335747718811
CurrentTrain: epoch  2, batch     2 | loss: 8.5405416Losses:  10.231264114379883 -0.0 -0.0
CurrentTrain: epoch  2, batch     3 | loss: 10.2312641Losses:  10.565091133117676 4.196501731872559 0.6059379577636719
CurrentTrain: epoch  3, batch     0 | loss: 10.5650911Losses:  9.996684074401855 5.2275919914245605 0.6677141785621643
CurrentTrain: epoch  3, batch     1 | loss: 9.9966841Losses:  10.384014129638672 4.332820892333984 0.5453359484672546
CurrentTrain: epoch  3, batch     2 | loss: 10.3840141Losses:  3.129621982574463 -0.0 0.0901971086859703
CurrentTrain: epoch  3, batch     3 | loss: 3.1296220Losses:  7.861936569213867 3.4147725105285645 0.6284171938896179
CurrentTrain: epoch  4, batch     0 | loss: 7.8619366Losses:  6.7710747718811035 2.711975336074829 0.6786190271377563
CurrentTrain: epoch  4, batch     1 | loss: 6.7710748Losses:  8.008492469787598 3.106567144393921 0.6734015941619873
CurrentTrain: epoch  4, batch     2 | loss: 8.0084925Losses:  2.822352409362793 -0.0 0.11106167733669281
CurrentTrain: epoch  4, batch     3 | loss: 2.8223524Losses:  7.862797260284424 3.1693472862243652 0.6103841662406921
CurrentTrain: epoch  5, batch     0 | loss: 7.8627973Losses:  6.509603977203369 2.3713574409484863 0.6847594976425171
CurrentTrain: epoch  5, batch     1 | loss: 6.5096040Losses:  7.238319396972656 3.735750198364258 0.5980092883110046
CurrentTrain: epoch  5, batch     2 | loss: 7.2383194Losses:  4.272180080413818 -0.0 0.11089737713336945
CurrentTrain: epoch  5, batch     3 | loss: 4.2721801Losses:  6.08816385269165 2.329090118408203 0.6122709512710571
CurrentTrain: epoch  6, batch     0 | loss: 6.0881639Losses:  5.210411548614502 1.8839904069900513 0.6723633408546448
CurrentTrain: epoch  6, batch     1 | loss: 5.2104115Losses:  5.29474401473999 2.125040292739868 0.5926147699356079
CurrentTrain: epoch  6, batch     2 | loss: 5.2947440Losses:  3.753542423248291 -0.0 0.09159982204437256
CurrentTrain: epoch  6, batch     3 | loss: 3.7535424Losses:  6.003245830535889 3.179781913757324 0.605269730091095
CurrentTrain: epoch  7, batch     0 | loss: 6.0032458Losses:  7.214492321014404 3.9985098838806152 0.4247992932796478
CurrentTrain: epoch  7, batch     1 | loss: 7.2144923Losses:  5.813045978546143 2.8064959049224854 0.5177198052406311
CurrentTrain: epoch  7, batch     2 | loss: 5.8130460Losses:  2.37190580368042 -0.0 0.17954698204994202
CurrentTrain: epoch  7, batch     3 | loss: 2.3719058Losses:  6.130331993103027 2.8543472290039062 0.6861176490783691
CurrentTrain: epoch  8, batch     0 | loss: 6.1303320Losses:  5.893681049346924 3.012084484100342 0.5062041878700256
CurrentTrain: epoch  8, batch     1 | loss: 5.8936810Losses:  7.131014823913574 4.390717029571533 0.5835821628570557
CurrentTrain: epoch  8, batch     2 | loss: 7.1310148Losses:  2.108403444290161 -0.0 0.10582324862480164
CurrentTrain: epoch  8, batch     3 | loss: 2.1084034Losses:  4.877972602844238 1.870652675628662 0.6650391817092896
CurrentTrain: epoch  9, batch     0 | loss: 4.8779726Losses:  5.609313011169434 2.591038227081299 0.5669987201690674
CurrentTrain: epoch  9, batch     1 | loss: 5.6093130Losses:  5.332792282104492 2.649095296859741 0.6035876274108887
CurrentTrain: epoch  9, batch     2 | loss: 5.3327923Losses:  1.6240973472595215 -0.0 -0.0
CurrentTrain: epoch  9, batch     3 | loss: 1.6240973
#### spouse ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "spouse" means the subject has the object as their spouse (husband, wife, partner, etc.).
Here is an example:
Relation: spouse
Context: rohan is the son of narayana murthy , founder of infosys , and his wife sudha murthy , an engineer and author of note .
Head Entity: narayana murthy
Tail Entity: sudha murthy
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "spouse":
Relation: spouse  
Context: elon musk, the CEO of spacex and tesla, was previously married to talulah riley, a talented actress and writer.  
Head Entity: elon musk  
Tail Entity: talulah riley  

Relation: spouse  
Context: barack obama, the 44th president of the united states, has been married to michelle obama since 1992, and they have two daughters together.  
Head Entity: barack obama  
Tail Entity: michelle obama  
#### record label ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "record label" means brand and trademark associated with the marketing of subject music recordings and music videos.
Here is an example:
Relation: record label
Context: " i 'm a cuckoo " was belle & sebastian 's second single from " dear catastrophe waitress " , released on rough trade records in 2004 .
Head Entity: belle & sebastian
Tail Entity: rough trade records
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "record label":
Relation: record label  
Context: The album "Random Access Memories" by Daft Punk was released under the Columbia Records label in 2013.  
Head Entity: Daft Punk  
Tail Entity: Columbia Records  

Relation: record label  
Context: Taylor Swift's latest album "Evermore" was produced by her own label, Republic Records, and released in 2020.  
Head Entity: Taylor Swift  
Tail Entity: Republic Records  
#### distributor ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "distributor" means distributor of a creative work; distributor for a record label; news agency; film distributor.
Here is an example:
Relation: distributor
Context: " the many adventures of winnie the pooh " is a 1977 american animated buddy musical comedy film produced by walt disney productions and distributed by buena vista distribution .
Head Entity: the many adventures of winnie the pooh
Tail Entity: buena vista distribution
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "distributor":
Relation: distributor  
Context: "Inception" is a 2010 science fiction film directed by Christopher Nolan and distributed by Warner Bros. Pictures.  
Head Entity: Inception  
Tail Entity: Warner Bros. Pictures  

Relation: distributor  
Context: "The Dark Knight" is a 2008 superhero film directed by Christopher Nolan and distributed by Warner Bros. Pictures.  
Head Entity: The Dark Knight  
Tail Entity: Warner Bros. Pictures  
#### located in or next to body of water ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "located in or next to body of water" means body of water on or next to which a place is located.
Here is an example:
Relation: located in or next to body of water
Context: one account notes , alston and his son , peter also , practiced their counterfeiting operation , at stack island , in the lower mississippi river , about 170 miles upriver from natchez .
Head Entity: stack island
Tail Entity: mississippi river
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "located in or next to body of water":
Relation: located in or next to body of water  
Context: the small town of lakeview is situated right beside the shimmering waters of lake erie, providing residents with stunning views and recreational opportunities.  
Head Entity: lakeview  
Tail Entity: lake erie  

Relation: located in or next to body of water  
Context: during the summer, tourists flock to the charming village of seaside, which lies adjacent to the beautiful pacific ocean, making it a popular destination for beachgoers.  
Head Entity: seaside  
Tail Entity: pacific ocean  
#### subsidiary ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "subsidiary" means subsidiary of a company or organization; generally a fully owned separate corporation.
Here is an example:
Relation: subsidiary
Context: cbs and viacom split again in 2005 , but both are still owned by national amusements .
Head Entity: national amusements
Tail Entity: viacom
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "subsidiary":
Relation: subsidiary  
Context: In 2018, Facebook acquired WhatsApp, making it a subsidiary of the social media giant.  
Head Entity: Facebook  
Tail Entity: WhatsApp  

Relation: subsidiary  
Context: Toyota has several subsidiaries, including Lexus, which focuses on luxury vehicles.  
Head Entity: Toyota  
Tail Entity: Lexus  
#### country of citizenship ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "country of citizenship" means the object is a country that recognizes the subject as its citizen.
Here is an example:
Relation: country of citizenship
Context: anders morelius from sweden placed third in the individual race .
Head Entity: anders morelius
Tail Entity: sweden
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "country of citizenship":
Relation: country of citizenship  
Context: maria gonzalez was born in mexico but later moved to the united states.  
Head Entity: maria gonzalez  
Tail Entity: mexico  

Relation: country of citizenship  
Context: after living in canada for several years, liu wei decided to apply for citizenship.  
Head Entity: liu wei  
Tail Entity: canada  
#### genre ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "genre" means creative work's genre or an artist's field of work.
Here is an example:
Relation: genre
Context: he is also a part of the atlanta - based hip - hop / r&b;/soul musical collective the dungeon family .
Head Entity: the dungeon family
Tail Entity: hip - hop
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "genre":
Relation: genre  
Context: the band is known for their unique blend of rock and electronic music, often categorized under the genre of synth-pop.  
Head Entity: the band  
Tail Entity: synth-pop  

Relation: genre  
Context: she has made significant contributions to the world of classical music, particularly in the genre of opera.  
Head Entity: she  
Tail Entity: opera  
#### mouth of the watercourse ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "mouth of the watercourse" means the body of water to which the watercourse drains.
Here is an example:
Relation: mouth of the watercourse
Context: san lázaro is 660 km far from asunción and is located at the exact site of the confluence of the apa river with paraguay river , bordering brazil - chaco .
Head Entity: apa river
Tail Entity: paraguay river
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "mouth of the watercourse":
Relation: mouth of the watercourse  
Context: the mississippi river flows into the gulf of mexico, creating a rich delta ecosystem that supports diverse wildlife and plant species.  
Head Entity: mississippi river  
Tail Entity: gulf of mexico  

Relation: mouth of the watercourse  
Context: the rhine river, which runs through several countries in europe, ultimately empties into the north sea, providing a vital shipping route for international trade.  
Head Entity: rhine river  
Tail Entity: north sea  
#### position played on team / speciality ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "position played on team / speciality" means position or specialism of a player on a team.
Here is an example:
Relation: position played on team / speciality
Context: benjamin van leer ( born 9 april 1992 in houten ) is a dutch professional footballer who currently plays as a goalkeeper for ajax in the dutch eredivisie .
Head Entity: benjamin van leer
Tail Entity: goalkeeper
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "position played on team / speciality":
Relation: position played on team / speciality  
Context: lebron james, born on december 30, 1984, is an american professional basketball player who currently plays as a forward for the los angeles lakers in the nba.  
Head Entity: lebron james  
Tail Entity: forward  

Relation: position played on team / speciality  
Context: serena williams, born on september 26, 1981, is an american professional tennis player who is known for her powerful serve and currently competes in singles and doubles events.  
Head Entity: serena williams  
Tail Entity: tennis player  
#### sports season of league or competition ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "sports season of league or competition" means property that shows the competition of which the item is a season.
Here is an example:
Relation: sports season of league or competition
Context: the stade louis ii in monaco was the venue for the uefa super cup every year since 1998 .
Head Entity: 1998
Tail Entity: uefa super cup
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "sports season of league or competition":
Relation: sports season of league or competition  
Context: The 2020 NBA season was significantly impacted by the COVID-19 pandemic, leading to a unique playoff format.  
Head Entity: 2020  
Tail Entity: NBA  

Relation: sports season of league or competition  
Context: The 2019 Rugby World Cup took place in Japan, showcasing teams from around the globe.  
Head Entity: 2019  
Tail Entity: Rugby World Cup  
Losses:  7.3163933753967285 0.9980207085609436 0.6649284958839417
MemoryTrain:  epoch  0, batch     0 | loss: 7.3163934Losses:  6.582855224609375 1.2792021036148071 0.690507173538208
MemoryTrain:  epoch  0, batch     1 | loss: 6.5828552Losses:  6.169562816619873 1.5333583354949951 0.46897467970848083
MemoryTrain:  epoch  0, batch     2 | loss: 6.1695628Losses:  4.434081077575684 0.5231698155403137 0.5845701098442078
MemoryTrain:  epoch  0, batch     3 | loss: 4.4340811Losses:  5.174984455108643 0.8051894903182983 0.5697882771492004
MemoryTrain:  epoch  1, batch     0 | loss: 5.1749845Losses:  5.1655402183532715 1.1099181175231934 0.6096140742301941
MemoryTrain:  epoch  1, batch     1 | loss: 5.1655402Losses:  6.554633140563965 0.8897451758384705 0.5944355726242065
MemoryTrain:  epoch  1, batch     2 | loss: 6.5546331Losses:  5.567225933074951 0.7747820615768433 0.6277650594711304
MemoryTrain:  epoch  1, batch     3 | loss: 5.5672259Losses:  5.508406639099121 1.264939546585083 0.680762767791748
MemoryTrain:  epoch  2, batch     0 | loss: 5.5084066Losses:  4.338339805603027 0.8174416422843933 0.6727129817008972
MemoryTrain:  epoch  2, batch     1 | loss: 4.3383398Losses:  4.433344841003418 0.9703036546707153 0.645643949508667
MemoryTrain:  epoch  2, batch     2 | loss: 4.4333448Losses:  3.8586039543151855 -0.0 0.5639786720275879
MemoryTrain:  epoch  2, batch     3 | loss: 3.8586040Losses:  5.812600135803223 2.033531665802002 0.7024638652801514
MemoryTrain:  epoch  3, batch     0 | loss: 5.8126001Losses:  3.9013845920562744 1.2734907865524292 0.5954296588897705
MemoryTrain:  epoch  3, batch     1 | loss: 3.9013846Losses:  3.7400453090667725 0.6450439691543579 0.6482875943183899
MemoryTrain:  epoch  3, batch     2 | loss: 3.7400453Losses:  4.796083450317383 0.5186180472373962 0.5791576504707336
MemoryTrain:  epoch  3, batch     3 | loss: 4.7960835Losses:  3.8178439140319824 0.9215982556343079 0.580098032951355
MemoryTrain:  epoch  4, batch     0 | loss: 3.8178439Losses:  3.951389789581299 1.068881630897522 0.5923895239830017
MemoryTrain:  epoch  4, batch     1 | loss: 3.9513898Losses:  3.7947540283203125 0.47993314266204834 0.6582751274108887
MemoryTrain:  epoch  4, batch     2 | loss: 3.7947540Losses:  3.90850830078125 0.2759024500846863 0.512395977973938
MemoryTrain:  epoch  4, batch     3 | loss: 3.9085083Losses:  4.9594340324401855 1.4036664962768555 0.667444109916687
MemoryTrain:  epoch  5, batch     0 | loss: 4.9594340Losses:  4.185640811920166 1.6191458702087402 0.6548474431037903
MemoryTrain:  epoch  5, batch     1 | loss: 4.1856408Losses:  4.058948516845703 1.9946717023849487 0.4881797432899475
MemoryTrain:  epoch  5, batch     2 | loss: 4.0589485Losses:  4.488268852233887 1.5172162055969238 0.6332598924636841
MemoryTrain:  epoch  5, batch     3 | loss: 4.4882689Losses:  4.182196617126465 1.5755460262298584 0.5186802744865417
MemoryTrain:  epoch  6, batch     0 | loss: 4.1821966Losses:  2.6392650604248047 0.49675700068473816 0.6101759672164917
MemoryTrain:  epoch  6, batch     1 | loss: 2.6392651Losses:  2.992429256439209 0.4543319344520569 0.5514206886291504
MemoryTrain:  epoch  6, batch     2 | loss: 2.9924293Losses:  3.2885184288024902 0.35235318541526794 0.7303808927536011
MemoryTrain:  epoch  6, batch     3 | loss: 3.2885184Losses:  3.8764264583587646 1.1779896020889282 0.5983660817146301
MemoryTrain:  epoch  7, batch     0 | loss: 3.8764265Losses:  2.963440179824829 0.9007342457771301 0.4549342095851898
MemoryTrain:  epoch  7, batch     1 | loss: 2.9634402Losses:  3.0613136291503906 0.8891336917877197 0.6380043029785156
MemoryTrain:  epoch  7, batch     2 | loss: 3.0613136Losses:  2.837451457977295 0.26978132128715515 0.6694120764732361
MemoryTrain:  epoch  7, batch     3 | loss: 2.8374515Losses:  2.954376697540283 0.7762828469276428 0.5836886167526245
MemoryTrain:  epoch  8, batch     0 | loss: 2.9543767Losses:  2.9338812828063965 0.7282611131668091 0.659633994102478
MemoryTrain:  epoch  8, batch     1 | loss: 2.9338813Losses:  2.774487018585205 0.9067676067352295 0.5439397096633911
MemoryTrain:  epoch  8, batch     2 | loss: 2.7744870Losses:  2.545290946960449 0.5590405464172363 0.5413569808006287
MemoryTrain:  epoch  8, batch     3 | loss: 2.5452909Losses:  2.8972644805908203 0.9272441864013672 0.39169687032699585
MemoryTrain:  epoch  9, batch     0 | loss: 2.8972645Losses:  2.8327410221099854 0.5185056924819946 0.5695071220397949
MemoryTrain:  epoch  9, batch     1 | loss: 2.8327410Losses:  2.702141523361206 0.5240395665168762 0.675633430480957
MemoryTrain:  epoch  9, batch     2 | loss: 2.7021415Losses:  2.7327802181243896 0.5300104022026062 0.6913542747497559
MemoryTrain:  epoch  9, batch     3 | loss: 2.7327802
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 100.00%,  total acc: 91.67%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 89.06%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 90.00%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 91.67%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 92.86%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 92.97%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 92.36%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 90.62%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 89.77%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 89.42%   [EVAL] batch:   13 | acc: 56.25%,  total acc: 87.05%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 85.83%   [EVAL] batch:   15 | acc: 93.75%,  total acc: 86.33%   [EVAL] batch:   16 | acc: 68.75%,  total acc: 85.29%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 84.38%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 84.87%   [EVAL] batch:   19 | acc: 68.75%,  total acc: 84.06%   [EVAL] batch:   20 | acc: 75.00%,  total acc: 83.63%   [EVAL] batch:   21 | acc: 68.75%,  total acc: 82.95%   [EVAL] batch:   22 | acc: 75.00%,  total acc: 82.61%   [EVAL] batch:   23 | acc: 75.00%,  total acc: 82.29%   [EVAL] batch:   24 | acc: 75.00%,  total acc: 82.00%   [EVAL] batch:   25 | acc: 68.75%,  total acc: 81.49%   [EVAL] batch:   26 | acc: 81.25%,  total acc: 81.48%   [EVAL] batch:   27 | acc: 56.25%,  total acc: 80.58%   [EVAL] batch:   28 | acc: 87.50%,  total acc: 80.82%   [EVAL] batch:   29 | acc: 43.75%,  total acc: 79.58%   [EVAL] batch:   30 | acc: 43.75%,  total acc: 78.43%   [EVAL] batch:   31 | acc: 56.25%,  total acc: 77.73%   [EVAL] batch:   32 | acc: 62.50%,  total acc: 77.27%   [EVAL] batch:   33 | acc: 62.50%,  total acc: 76.84%   [EVAL] batch:   34 | acc: 50.00%,  total acc: 76.07%   [EVAL] batch:   35 | acc: 62.50%,  total acc: 75.69%   [EVAL] batch:   36 | acc: 56.25%,  total acc: 75.17%   [EVAL] batch:   37 | acc: 62.50%,  total acc: 74.84%   [EVAL] batch:   38 | acc: 62.50%,  total acc: 74.52%   [EVAL] batch:   39 | acc: 68.75%,  total acc: 74.38%   [EVAL] batch:   40 | acc: 56.25%,  total acc: 73.93%   [EVAL] batch:   41 | acc: 37.50%,  total acc: 73.07%   [EVAL] batch:   42 | acc: 75.00%,  total acc: 73.11%   [EVAL] batch:   43 | acc: 43.75%,  total acc: 72.44%   [EVAL] batch:   44 | acc: 12.50%,  total acc: 71.11%   [EVAL] batch:   45 | acc: 37.50%,  total acc: 70.38%   [EVAL] batch:   46 | acc: 31.25%,  total acc: 69.55%   [EVAL] batch:   47 | acc: 37.50%,  total acc: 68.88%   [EVAL] batch:   48 | acc: 25.00%,  total acc: 67.98%   [EVAL] batch:   49 | acc: 37.50%,  total acc: 67.38%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 68.01%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 68.63%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 69.22%   [EVAL] batch:   53 | acc: 93.75%,  total acc: 69.68%   [EVAL] batch:   54 | acc: 100.00%,  total acc: 70.23%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 70.76%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 71.27%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 71.66%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 72.03%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 72.50%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 72.95%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 73.39%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 73.02%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 82.81%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 82.50%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 83.04%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 85.16%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 86.11%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 86.88%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 88.54%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 88.94%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 89.29%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 90.23%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 90.44%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 91.12%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 91.56%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 91.19%   [EVAL] batch:   22 | acc: 87.50%,  total acc: 91.03%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 91.15%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 91.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 91.35%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 91.44%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 91.52%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 91.59%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 91.88%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 92.14%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 92.38%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 92.61%   [EVAL] batch:   33 | acc: 68.75%,  total acc: 91.91%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 91.43%   [EVAL] batch:   35 | acc: 81.25%,  total acc: 91.15%   [EVAL] batch:   36 | acc: 93.75%,  total acc: 91.22%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 91.28%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 91.51%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 91.72%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 91.92%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 92.11%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 92.30%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 92.47%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 92.36%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 92.53%   [EVAL] batch:   46 | acc: 87.50%,  total acc: 92.42%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 92.45%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 92.60%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 92.75%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 92.77%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 92.79%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 92.81%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 92.94%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 92.73%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 92.75%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 92.76%   [EVAL] batch:   57 | acc: 81.25%,  total acc: 92.56%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 92.69%   [EVAL] batch:   59 | acc: 87.50%,  total acc: 92.60%   [EVAL] batch:   60 | acc: 93.75%,  total acc: 92.62%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 92.54%   [EVAL] batch:   62 | acc: 93.75%,  total acc: 92.56%   [EVAL] batch:   63 | acc: 81.25%,  total acc: 92.38%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 92.50%   [EVAL] batch:   65 | acc: 100.00%,  total acc: 92.61%   [EVAL] batch:   66 | acc: 81.25%,  total acc: 92.44%   [EVAL] batch:   67 | acc: 93.75%,  total acc: 92.46%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 92.57%   [EVAL] batch:   69 | acc: 93.75%,  total acc: 92.59%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 92.69%   [EVAL] batch:   71 | acc: 75.00%,  total acc: 92.45%   [EVAL] batch:   72 | acc: 87.50%,  total acc: 92.38%   [EVAL] batch:   73 | acc: 81.25%,  total acc: 92.23%   [EVAL] batch:   74 | acc: 81.25%,  total acc: 92.08%   [EVAL] batch:   75 | acc: 75.00%,  total acc: 91.86%   [EVAL] batch:   76 | acc: 62.50%,  total acc: 91.48%   [EVAL] batch:   77 | acc: 81.25%,  total acc: 91.35%   [EVAL] batch:   78 | acc: 81.25%,  total acc: 91.22%   [EVAL] batch:   79 | acc: 68.75%,  total acc: 90.94%   [EVAL] batch:   80 | acc: 81.25%,  total acc: 90.82%   [EVAL] batch:   81 | acc: 87.50%,  total acc: 90.78%   [EVAL] batch:   82 | acc: 62.50%,  total acc: 90.44%   [EVAL] batch:   83 | acc: 68.75%,  total acc: 90.18%   [EVAL] batch:   84 | acc: 75.00%,  total acc: 90.00%   [EVAL] batch:   85 | acc: 75.00%,  total acc: 89.83%   [EVAL] batch:   86 | acc: 68.75%,  total acc: 89.58%   [EVAL] batch:   87 | acc: 81.25%,  total acc: 89.49%   [EVAL] batch:   88 | acc: 75.00%,  total acc: 89.33%   [EVAL] batch:   89 | acc: 62.50%,  total acc: 89.03%   [EVAL] batch:   90 | acc: 75.00%,  total acc: 88.87%   [EVAL] batch:   91 | acc: 56.25%,  total acc: 88.52%   [EVAL] batch:   92 | acc: 56.25%,  total acc: 88.17%   [EVAL] batch:   93 | acc: 43.75%,  total acc: 87.70%   [EVAL] batch:   94 | acc: 62.50%,  total acc: 87.43%   [EVAL] batch:   95 | acc: 62.50%,  total acc: 87.17%   [EVAL] batch:   96 | acc: 50.00%,  total acc: 86.79%   [EVAL] batch:   97 | acc: 56.25%,  total acc: 86.48%   [EVAL] batch:   98 | acc: 68.75%,  total acc: 86.30%   [EVAL] batch:   99 | acc: 56.25%,  total acc: 86.00%   [EVAL] batch:  100 | acc: 56.25%,  total acc: 85.71%   [EVAL] batch:  101 | acc: 75.00%,  total acc: 85.60%   [EVAL] batch:  102 | acc: 50.00%,  total acc: 85.25%   [EVAL] batch:  103 | acc: 56.25%,  total acc: 84.98%   [EVAL] batch:  104 | acc: 50.00%,  total acc: 84.64%   [EVAL] batch:  105 | acc: 68.75%,  total acc: 84.49%   [EVAL] batch:  106 | acc: 25.00%,  total acc: 83.94%   [EVAL] batch:  107 | acc: 25.00%,  total acc: 83.39%   [EVAL] batch:  108 | acc: 31.25%,  total acc: 82.91%   [EVAL] batch:  109 | acc: 37.50%,  total acc: 82.50%   [EVAL] batch:  110 | acc: 25.00%,  total acc: 81.98%   [EVAL] batch:  111 | acc: 50.00%,  total acc: 81.70%   [EVAL] batch:  112 | acc: 56.25%,  total acc: 81.47%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 81.63%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 81.79%   [EVAL] batch:  115 | acc: 93.75%,  total acc: 81.90%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 82.05%   [EVAL] batch:  117 | acc: 100.00%,  total acc: 82.20%   [EVAL] batch:  118 | acc: 100.00%,  total acc: 82.35%   [EVAL] batch:  119 | acc: 100.00%,  total acc: 82.50%   [EVAL] batch:  120 | acc: 87.50%,  total acc: 82.54%   [EVAL] batch:  121 | acc: 100.00%,  total acc: 82.68%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 82.83%   [EVAL] batch:  123 | acc: 100.00%,  total acc: 82.96%   [EVAL] batch:  124 | acc: 100.00%,  total acc: 83.10%   
cur_acc:  ['0.9494', '0.7302']
his_acc:  ['0.9494', '0.8310']
Clustering into  14  clusters
Clusters:  [ 7  0  5  3  4  4  5  1  0  9 12  1  1  4 11  9  7  4  2 13  7  2  6 10
  6  0  8  7  1  3]
Losses:  13.616943359375 5.447763442993164 0.6982362270355225
CurrentTrain: epoch  0, batch     0 | loss: 13.6169434Losses:  11.321348190307617 3.5050346851348877 0.7459492683410645
CurrentTrain: epoch  0, batch     1 | loss: 11.3213482Losses:  12.027888298034668 4.366089820861816 0.6413359045982361
CurrentTrain: epoch  0, batch     2 | loss: 12.0278883Losses:  5.953999996185303 -0.0 0.18195869028568268
CurrentTrain: epoch  0, batch     3 | loss: 5.9540000Losses:  10.180338859558105 2.914867401123047 0.7381210923194885
CurrentTrain: epoch  1, batch     0 | loss: 10.1803389Losses:  12.295275688171387 4.908770561218262 0.7431410551071167
CurrentTrain: epoch  1, batch     1 | loss: 12.2952757Losses:  11.608480453491211 4.6497297286987305 0.6975691914558411
CurrentTrain: epoch  1, batch     2 | loss: 11.6084805Losses:  4.7478132247924805 -0.0 0.1463463008403778
CurrentTrain: epoch  1, batch     3 | loss: 4.7478132Losses:  9.208259582519531 3.2208945751190186 0.7320247888565063
CurrentTrain: epoch  2, batch     0 | loss: 9.2082596Losses:  9.86758804321289 3.3974006175994873 0.7003895044326782
CurrentTrain: epoch  2, batch     1 | loss: 9.8675880Losses:  9.396790504455566 3.2768986225128174 0.6340121030807495
CurrentTrain: epoch  2, batch     2 | loss: 9.3967905Losses:  8.371559143066406 -0.0 0.10347050428390503
CurrentTrain: epoch  2, batch     3 | loss: 8.3715591Losses:  8.551504135131836 3.7864818572998047 0.5593931674957275
CurrentTrain: epoch  3, batch     0 | loss: 8.5515041Losses:  9.095232963562012 3.0759260654449463 0.5496912002563477
CurrentTrain: epoch  3, batch     1 | loss: 9.0952330Losses:  9.103849411010742 3.0666680335998535 0.6648600697517395
CurrentTrain: epoch  3, batch     2 | loss: 9.1038494Losses:  4.923259258270264 -0.0 -0.0
CurrentTrain: epoch  3, batch     3 | loss: 4.9232593Losses:  7.708245277404785 2.7492148876190186 0.7226848602294922
CurrentTrain: epoch  4, batch     0 | loss: 7.7082453Losses:  8.930191993713379 3.2590012550354004 0.7021612524986267
CurrentTrain: epoch  4, batch     1 | loss: 8.9301920Losses:  9.395894050598145 3.5189478397369385 0.6258086562156677
CurrentTrain: epoch  4, batch     2 | loss: 9.3958941Losses:  6.116365432739258 -0.0 0.12940111756324768
CurrentTrain: epoch  4, batch     3 | loss: 6.1163654Losses:  8.447200775146484 3.61332631111145 0.7288330793380737
CurrentTrain: epoch  5, batch     0 | loss: 8.4472008Losses:  8.400812149047852 3.4572441577911377 0.6347631216049194
CurrentTrain: epoch  5, batch     1 | loss: 8.4008121Losses:  9.662930488586426 3.7985620498657227 0.6792771816253662
CurrentTrain: epoch  5, batch     2 | loss: 9.6629305Losses:  5.494598388671875 -0.0 0.14028066396713257
CurrentTrain: epoch  5, batch     3 | loss: 5.4945984Losses:  9.816816329956055 4.067835807800293 0.6485432982444763
CurrentTrain: epoch  6, batch     0 | loss: 9.8168163Losses:  6.686589241027832 2.3449809551239014 0.677810549736023
CurrentTrain: epoch  6, batch     1 | loss: 6.6865892Losses:  9.06920337677002 3.784695863723755 0.6391934156417847
CurrentTrain: epoch  6, batch     2 | loss: 9.0692034Losses:  2.2516207695007324 -0.0 0.0942709743976593
CurrentTrain: epoch  6, batch     3 | loss: 2.2516208Losses:  8.533367156982422 3.806633472442627 0.6317415237426758
CurrentTrain: epoch  7, batch     0 | loss: 8.5333672Losses:  9.444574356079102 4.971442222595215 0.6022170186042786
CurrentTrain: epoch  7, batch     1 | loss: 9.4445744Losses:  8.109578132629395 3.3254194259643555 0.6098806262016296
CurrentTrain: epoch  7, batch     2 | loss: 8.1095781Losses:  5.627283573150635 -0.0 0.16435274481773376
CurrentTrain: epoch  7, batch     3 | loss: 5.6272836Losses:  9.179566383361816 3.8878562450408936 0.7258589267730713
CurrentTrain: epoch  8, batch     0 | loss: 9.1795664Losses:  6.890713214874268 2.487955093383789 0.6874594688415527
CurrentTrain: epoch  8, batch     1 | loss: 6.8907132Losses:  6.324952125549316 1.8254852294921875 0.7034041881561279
CurrentTrain: epoch  8, batch     2 | loss: 6.3249521Losses:  3.4685933589935303 -0.0 0.1432984620332718
CurrentTrain: epoch  8, batch     3 | loss: 3.4685934Losses:  11.500715255737305 6.4355573654174805 0.5552083849906921
CurrentTrain: epoch  9, batch     0 | loss: 11.5007153Losses:  8.903841972351074 5.078209400177002 0.6246763467788696
CurrentTrain: epoch  9, batch     1 | loss: 8.9038420Losses:  9.57951831817627 4.605125427246094 0.6535032987594604
CurrentTrain: epoch  9, batch     2 | loss: 9.5795183Losses:  2.1751363277435303 -0.0 0.1525743156671524
CurrentTrain: epoch  9, batch     3 | loss: 2.1751363
#### main subject ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "main subject" means primary topic of a work.
Here is an example:
Relation: main subject
Context: auerbach is prominently featured in the documentary film , " the first basket " , about jewish basketball history .
Head Entity: the first basket
Tail Entity: basketball
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "main subject":
Relation: main subject  
Context: the novel "Pride and Prejudice" explores the themes of love and social class in early 19th century England.  
Head Entity: Pride and Prejudice  
Tail Entity: love  

Relation: main subject  
Context: the documentary "Our Planet" highlights the impact of climate change on various ecosystems around the world.  
Head Entity: Our Planet  
Tail Entity: climate change  
#### participating team ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "participating team" means teams that actively takes/took part in an event or process.
Here is an example:
Relation: participating team
Context: group b of uefa euro 2016 contained england , russia , wales and slovakia .
Head Entity: uefa euro 2016
Tail Entity: slovakia
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "participating team":
Relation: participating team  
Context: the 2020 summer olympics featured countries like the united states, china, and japan competing in various sports.  
Head Entity: 2020 summer olympics  
Tail Entity: japan  

Relation: participating team  
Context: the fifa world cup 2018 saw teams such as france, croatia, and belgium battling for the championship title.  
Head Entity: fifa world cup 2018  
Tail Entity: croatia  
#### has part ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "has part" means part of this subject.
Here is an example:
Relation: has part
Context: among the competing artists was previous eurovision song contest entrant robin juhkental who represented estonia in 2010 as part of the group malcolm lincoln .
Head Entity: malcolm lincoln
Tail Entity: robin juhkental
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "has part":
Relation: has part  
Context: The famous painting "The Last Supper" features several apostles, including Peter, who is depicted prominently on the left side of the artwork.  
Head Entity: The Last Supper  
Tail Entity: Peter  

Relation: has part  
Context: The smartphone model Galaxy S21 comes with a high-resolution camera that includes a wide-angle lens, which is essential for capturing stunning landscapes.  
Head Entity: Galaxy S21  
Tail Entity: wide-angle lens  
#### nominated for ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "nominated for" means award nomination received by a person, organisation or creative work.
Here is an example:
Relation: nominated for
Context: he was nominated for the academy award for best picture , along with steven spielberg , ian bryce , mark gordon for the film " saving private ryan " .
Head Entity: ian bryce
Tail Entity: academy award for best picture
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "nominated for":
Relation: nominated for  
Context: The film "The Shape of Water" received multiple nominations, including a nod for best director for Guillermo del Toro at the Academy Awards.  
Head Entity: Guillermo del Toro  
Tail Entity: Academy Awards  

Relation: nominated for  
Context: In 2020, the popular series "Succession" was nominated for several Emmy Awards, showcasing its critical acclaim and audience popularity.  
Head Entity: Succession  
Tail Entity: Emmy Awards  
#### instance of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "instance of" means that class of which this subject is a particular example and member.
Here is an example:
Relation: instance of
Context: on march 2015 , cube entertainment launched the multi - national girl group clc including original members seunghee , yujin , seungyeon , sorn and yeeun .
Head Entity: clc
Tail Entity: girl group
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "instance of":
Relation: instance of  
Context: The smartphone iPhone 13 was released by Apple in September 2021, showcasing advanced technology and features.  
Head Entity: iPhone 13  
Tail Entity: smartphone  

Relation: instance of  
Context: The Great Wall of China is a historic fortification that stretches across northern China, attracting millions of tourists each year.  
Head Entity: Great Wall of China  
Tail Entity: fortification  
#### mountain range ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "mountain range" means range or subrange to which the geographical item belongs.
Here is an example:
Relation: mountain range
Context: abus valley ( ) is an ice - free valley southeast of turnstile ridge at the north end of britannia range .
Head Entity: turnstile ridge
Tail Entity: britannia range
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "mountain range":
Relation: mountain range  
Context: the sierra nevada ( ) is a mountain range in the western united states, primarily in the state of california, and it includes the famous yosemite national park, which is located within the range.  
Head Entity: sierra nevada  
Tail Entity: yosemite national park  

Relation: mountain range  
Context: the appalachian mountains ( ) extend from the canadian province of quebec down to alabama, making it one of the longest mountain ranges in north america.  
Head Entity: appalachian mountains  
Tail Entity: alabama  
#### screenwriter ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "screenwriter" means person who wrote the script for subject item.
Here is an example:
Relation: screenwriter
Context: future films include " mcqueen " , ian bonhote ’s documentary about the fashion designer alexander mcqueen , brad anderson ’s thriller " beirut " , and mark pellington ’s drama " nostalgia " .
Head Entity: nostalgia
Tail Entity: mark pellington
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "screenwriter":
Relation: screenwriter  
Context: the upcoming movie "the great adventure" is penned by renowned screenwriter jessica taylor, who has previously won multiple awards for her work in the industry.  
Head Entity: the great adventure  
Tail Entity: jessica taylor  

Relation: screenwriter  
Context: in the latest film "shadows of the past," the script was crafted by the talented screenwriter robert lang, known for his gripping storytelling and character development.  
Head Entity: shadows of the past  
Tail Entity: robert lang  
#### language of work or name ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "language of work or name" means language associated with this creative work (such as books, shows, songs, broadcasts or websites) or a name.
Here is an example:
Relation: language of work or name
Context: the earliest written mention of the order is found in " tirant lo blanch " , a chivalric romance written in catalan mainly by valencian joanot martorell .
Head Entity: tirant lo blanch
Tail Entity: catalan
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "language of work or name":
Relation: language of work or name  
Context: The famous novel "One Hundred Years of Solitude" was originally published in Spanish and has been translated into many languages.  
Head Entity: One Hundred Years of Solitude  
Tail Entity: Spanish  

Relation: language of work or name  
Context: The animated series "Naruto" is primarily produced in Japanese and has gained a massive following worldwide.  
Head Entity: Naruto  
Tail Entity: Japanese  
#### operator ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "operator" means person, profession, or organization that operates the equipment, facility, or service.
Here is an example:
Relation: operator
Context: the aircraft was a boeing 707 - 321c that first entered service with pan american world airways ( pan am ) in 1963 .
Head Entity: boeing 707 - 321c
Tail Entity: pan american world airways
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "operator":
Relation: operator  
Context: the new electric bus fleet is managed by the city transportation authority, which ensures efficient public transit services.  
Head Entity: electric bus fleet  
Tail Entity: city transportation authority  

Relation: operator  
Context: the state-of-the-art data center is operated by tech solutions inc., providing cloud services to various businesses.  
Head Entity: data center  
Tail Entity: tech solutions inc.  
#### religion ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "religion" means religion of a person, organization or religious building, or associated with this subject.
Here is an example:
Relation: religion
Context: gogukwon 's successor , sosurim , adopted a foreign policy of appeasement and reconciliation with baekje , and concentrated on domestic policies to spread buddhism throughout goguryeo 's social and political systems .
Head Entity: goguryeo
Tail Entity: buddhism
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "religion":
Relation: religion  
Context: The ancient Greeks worshipped a pantheon of gods and goddesses, with Zeus being the king of the gods and a central figure in their religious practices.  
Head Entity: ancient Greeks  
Tail Entity: Zeus  

Relation: religion  
Context: The construction of the grand cathedral was a significant event for the local community, as it represented their dedication to Christianity and served as a place of worship for generations.  
Head Entity: local community  
Tail Entity: Christianity  
Losses:  6.581328868865967 1.095445156097412 0.9527229070663452
MemoryTrain:  epoch  0, batch     0 | loss: 6.5813289Losses:  5.060869216918945 0.7716071009635925 0.8437857031822205
MemoryTrain:  epoch  0, batch     1 | loss: 5.0608692Losses:  5.5100483894348145 1.1561360359191895 0.8037102818489075
MemoryTrain:  epoch  0, batch     2 | loss: 5.5100484Losses:  5.758516311645508 0.9225527048110962 0.8834064602851868
MemoryTrain:  epoch  0, batch     3 | loss: 5.7585163Losses:  4.8607354164123535 0.5196104049682617 0.7302573323249817
MemoryTrain:  epoch  0, batch     4 | loss: 4.8607354Losses:  4.134527206420898 0.6102379560470581 0.5103722214698792
MemoryTrain:  epoch  0, batch     5 | loss: 4.1345272Losses:  4.534850120544434 0.6384809017181396 0.738825798034668
MemoryTrain:  epoch  1, batch     0 | loss: 4.5348501Losses:  6.570375442504883 1.362856149673462 0.8629361987113953
MemoryTrain:  epoch  1, batch     1 | loss: 6.5703754Losses:  4.060457706451416 0.2199007272720337 0.7053216695785522
MemoryTrain:  epoch  1, batch     2 | loss: 4.0604577Losses:  3.730182409286499 -0.0 0.9573462009429932
MemoryTrain:  epoch  1, batch     3 | loss: 3.7301824Losses:  4.613914966583252 1.4296578168869019 0.7660534977912903
MemoryTrain:  epoch  1, batch     4 | loss: 4.6139150Losses:  5.487972736358643 0.5852940678596497 0.3751750886440277
MemoryTrain:  epoch  1, batch     5 | loss: 5.4879727Losses:  4.2772393226623535 0.9819297194480896 0.7638359069824219
MemoryTrain:  epoch  2, batch     0 | loss: 4.2772393Losses:  5.670605182647705 1.65022873878479 0.7833369970321655
MemoryTrain:  epoch  2, batch     1 | loss: 5.6706052Losses:  4.140499114990234 1.2875494956970215 0.7065837383270264
MemoryTrain:  epoch  2, batch     2 | loss: 4.1404991Losses:  4.764780521392822 0.9109723567962646 0.9233018755912781
MemoryTrain:  epoch  2, batch     3 | loss: 4.7647805Losses:  3.524179220199585 0.48046058416366577 0.7266848683357239
MemoryTrain:  epoch  2, batch     4 | loss: 3.5241792Losses:  3.6595513820648193 0.32718154788017273 0.6985924243927002
MemoryTrain:  epoch  2, batch     5 | loss: 3.6595514Losses:  5.149897575378418 1.8932557106018066 0.7351338863372803
MemoryTrain:  epoch  3, batch     0 | loss: 5.1498976Losses:  3.969785451889038 1.3335185050964355 0.6939523220062256
MemoryTrain:  epoch  3, batch     1 | loss: 3.9697855Losses:  3.3540101051330566 0.7226698994636536 0.7433926463127136
MemoryTrain:  epoch  3, batch     2 | loss: 3.3540101Losses:  4.912752628326416 1.0724326372146606 0.7735118269920349
MemoryTrain:  epoch  3, batch     3 | loss: 4.9127526Losses:  4.180591583251953 1.1945103406906128 0.4945509433746338
MemoryTrain:  epoch  3, batch     4 | loss: 4.1805916Losses:  3.1142373085021973 -0.0 0.7371479272842407
MemoryTrain:  epoch  3, batch     5 | loss: 3.1142373Losses:  4.19528865814209 0.7918931245803833 0.8760210871696472
MemoryTrain:  epoch  4, batch     0 | loss: 4.1952887Losses:  3.18741512298584 -0.0 0.8648527264595032
MemoryTrain:  epoch  4, batch     1 | loss: 3.1874151Losses:  3.931424617767334 0.7759377956390381 0.8468635678291321
MemoryTrain:  epoch  4, batch     2 | loss: 3.9314246Losses:  4.027535915374756 0.821597695350647 0.8982918858528137
MemoryTrain:  epoch  4, batch     3 | loss: 4.0275359Losses:  3.3523313999176025 0.7909456491470337 0.8375043272972107
MemoryTrain:  epoch  4, batch     4 | loss: 3.3523314Losses:  2.2970268726348877 0.2255387306213379 0.5957950949668884
MemoryTrain:  epoch  4, batch     5 | loss: 2.2970269Losses:  3.7442197799682617 1.0612539052963257 0.8761309385299683
MemoryTrain:  epoch  5, batch     0 | loss: 3.7442198Losses:  3.4702506065368652 0.6957212686538696 0.6849590539932251
MemoryTrain:  epoch  5, batch     1 | loss: 3.4702506Losses:  3.0038840770721436 0.5170996785163879 0.8319376111030579
MemoryTrain:  epoch  5, batch     2 | loss: 3.0038841Losses:  4.31962776184082 1.16232430934906 0.7057416439056396
MemoryTrain:  epoch  5, batch     3 | loss: 4.3196278Losses:  3.8946661949157715 1.1756620407104492 0.7751538157463074
MemoryTrain:  epoch  5, batch     4 | loss: 3.8946662Losses:  2.7052195072174072 0.3757948577404022 0.483572393655777
MemoryTrain:  epoch  5, batch     5 | loss: 2.7052195Losses:  3.398989677429199 0.7981853485107422 0.8375734686851501
MemoryTrain:  epoch  6, batch     0 | loss: 3.3989897Losses:  2.922072172164917 0.2603773772716522 0.9401654005050659
MemoryTrain:  epoch  6, batch     1 | loss: 2.9220722Losses:  4.330743312835693 1.6543304920196533 0.6965842247009277
MemoryTrain:  epoch  6, batch     2 | loss: 4.3307433Losses:  3.839116096496582 0.8249862790107727 0.8371250033378601
MemoryTrain:  epoch  6, batch     3 | loss: 3.8391161Losses:  2.5216822624206543 0.47022387385368347 0.6810674071311951
MemoryTrain:  epoch  6, batch     4 | loss: 2.5216823Losses:  3.1739659309387207 0.3554976284503937 0.6872162818908691
MemoryTrain:  epoch  6, batch     5 | loss: 3.1739659Losses:  3.437363862991333 0.6861129403114319 0.7222349643707275
MemoryTrain:  epoch  7, batch     0 | loss: 3.4373639Losses:  3.4965529441833496 0.759545087814331 0.6859624981880188
MemoryTrain:  epoch  7, batch     1 | loss: 3.4965529Losses:  3.3192481994628906 0.5133631825447083 0.8206389546394348
MemoryTrain:  epoch  7, batch     2 | loss: 3.3192482Losses:  3.898245096206665 1.6948193311691284 0.6569907069206238
MemoryTrain:  epoch  7, batch     3 | loss: 3.8982451Losses:  3.3893470764160156 1.3812679052352905 0.6841700673103333
MemoryTrain:  epoch  7, batch     4 | loss: 3.3893471Losses:  2.4656929969787598 0.255264014005661 0.6238400936126709
MemoryTrain:  epoch  7, batch     5 | loss: 2.4656930Losses:  3.386089563369751 0.7944680452346802 0.894659698009491
MemoryTrain:  epoch  8, batch     0 | loss: 3.3860896Losses:  3.5299627780914307 0.8598061800003052 0.7962610721588135
MemoryTrain:  epoch  8, batch     1 | loss: 3.5299628Losses:  3.3370001316070557 1.0516592264175415 0.6948564648628235
MemoryTrain:  epoch  8, batch     2 | loss: 3.3370001Losses:  3.001544952392578 0.655524492263794 0.551252543926239
MemoryTrain:  epoch  8, batch     3 | loss: 3.0015450Losses:  3.530470371246338 0.8637121915817261 0.9488617181777954
MemoryTrain:  epoch  8, batch     4 | loss: 3.5304704Losses:  2.371215343475342 0.31025025248527527 0.5408181548118591
MemoryTrain:  epoch  8, batch     5 | loss: 2.3712153Losses:  2.3055903911590576 0.24482041597366333 0.7099025845527649
MemoryTrain:  epoch  9, batch     0 | loss: 2.3055904Losses:  3.139739990234375 0.5377674102783203 0.7498749494552612
MemoryTrain:  epoch  9, batch     1 | loss: 3.1397400Losses:  3.918057441711426 0.9919790625572205 0.8233656287193298
MemoryTrain:  epoch  9, batch     2 | loss: 3.9180574Losses:  2.9383833408355713 0.5015485882759094 0.808107316493988
MemoryTrain:  epoch  9, batch     3 | loss: 2.9383833Losses:  2.726559638977051 0.482059121131897 0.7228103280067444
MemoryTrain:  epoch  9, batch     4 | loss: 2.7265596Losses:  2.1295294761657715 0.3131219446659088 0.45383262634277344
MemoryTrain:  epoch  9, batch     5 | loss: 2.1295295
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 68.75%,  total acc: 71.88%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 72.92%   [EVAL] batch:    3 | acc: 93.75%,  total acc: 78.12%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 80.21%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 82.81%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 82.64%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 83.12%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 84.66%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 85.42%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 84.62%   [EVAL] batch:   13 | acc: 18.75%,  total acc: 79.91%   [EVAL] batch:   14 | acc: 31.25%,  total acc: 76.67%   [EVAL] batch:   15 | acc: 18.75%,  total acc: 73.05%   [EVAL] batch:   16 | acc: 37.50%,  total acc: 70.96%   [EVAL] batch:   17 | acc: 37.50%,  total acc: 69.10%   [EVAL] batch:   18 | acc: 31.25%,  total acc: 67.11%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 68.44%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 69.94%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 71.02%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 72.28%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 73.44%   [EVAL] batch:   24 | acc: 93.75%,  total acc: 74.25%   [EVAL] batch:   25 | acc: 75.00%,  total acc: 74.28%   [EVAL] batch:   26 | acc: 43.75%,  total acc: 73.15%   [EVAL] batch:   27 | acc: 43.75%,  total acc: 72.10%   [EVAL] batch:   28 | acc: 50.00%,  total acc: 71.34%   [EVAL] batch:   29 | acc: 50.00%,  total acc: 70.62%   [EVAL] batch:   30 | acc: 25.00%,  total acc: 69.15%   [EVAL] batch:   31 | acc: 75.00%,  total acc: 69.34%   [EVAL] batch:   32 | acc: 87.50%,  total acc: 69.89%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 70.59%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 71.25%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 71.70%   [EVAL] batch:   36 | acc: 93.75%,  total acc: 72.30%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 73.03%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 73.72%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 74.38%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 75.45%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 76.02%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 76.42%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 76.67%   [EVAL] batch:   45 | acc: 87.50%,  total acc: 76.90%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 77.26%   [EVAL] batch:   47 | acc: 68.75%,  total acc: 77.08%   [EVAL] batch:   48 | acc: 75.00%,  total acc: 77.04%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 77.25%   [EVAL] batch:   50 | acc: 68.75%,  total acc: 77.08%   [EVAL] batch:   51 | acc: 75.00%,  total acc: 77.04%   [EVAL] batch:   52 | acc: 68.75%,  total acc: 76.89%   [EVAL] batch:   53 | acc: 75.00%,  total acc: 76.85%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 77.16%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 77.34%   [EVAL] batch:   56 | acc: 62.50%,  total acc: 77.08%   [EVAL] batch:   57 | acc: 56.25%,  total acc: 76.72%   [EVAL] batch:   58 | acc: 75.00%,  total acc: 76.69%   [EVAL] batch:   59 | acc: 62.50%,  total acc: 76.46%   [EVAL] batch:   60 | acc: 56.25%,  total acc: 76.13%   [EVAL] batch:   61 | acc: 50.00%,  total acc: 75.71%   [EVAL] batch:   62 | acc: 31.25%,  total acc: 75.00%   
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 75.00%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 77.08%   [EVAL] batch:    3 | acc: 56.25%,  total acc: 71.88%   [EVAL] batch:    4 | acc: 62.50%,  total acc: 70.00%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 71.88%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 71.43%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 72.66%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 73.61%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 75.62%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 76.14%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 77.08%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 77.40%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 78.57%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 79.58%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 80.86%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 81.62%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 82.29%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 83.22%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 83.75%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 83.93%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 83.81%   [EVAL] batch:   22 | acc: 81.25%,  total acc: 83.70%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 84.11%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 84.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 84.86%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 85.42%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 85.94%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 86.21%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 86.67%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 87.10%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 87.88%   [EVAL] batch:   33 | acc: 68.75%,  total acc: 87.32%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 86.96%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 86.98%   [EVAL] batch:   36 | acc: 87.50%,  total acc: 86.99%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 87.17%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 87.81%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 88.11%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 88.39%   [EVAL] batch:   42 | acc: 87.50%,  total acc: 88.37%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 88.64%   [EVAL] batch:   44 | acc: 81.25%,  total acc: 88.47%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 88.72%   [EVAL] batch:   46 | acc: 87.50%,  total acc: 88.70%   [EVAL] batch:   47 | acc: 87.50%,  total acc: 88.67%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 88.90%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 89.00%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 89.09%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 89.18%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 89.39%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 89.58%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 89.43%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 89.51%   [EVAL] batch:   56 | acc: 75.00%,  total acc: 89.25%   [EVAL] batch:   57 | acc: 50.00%,  total acc: 88.58%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 88.56%   [EVAL] batch:   59 | acc: 87.50%,  total acc: 88.54%   [EVAL] batch:   60 | acc: 68.75%,  total acc: 88.22%   [EVAL] batch:   61 | acc: 62.50%,  total acc: 87.80%   [EVAL] batch:   62 | acc: 81.25%,  total acc: 87.70%   [EVAL] batch:   63 | acc: 75.00%,  total acc: 87.50%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 87.69%   [EVAL] batch:   65 | acc: 93.75%,  total acc: 87.78%   [EVAL] batch:   66 | acc: 75.00%,  total acc: 87.59%   [EVAL] batch:   67 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:   68 | acc: 93.75%,  total acc: 87.59%   [EVAL] batch:   69 | acc: 93.75%,  total acc: 87.68%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 87.85%   [EVAL] batch:   71 | acc: 75.00%,  total acc: 87.67%   [EVAL] batch:   72 | acc: 87.50%,  total acc: 87.67%   [EVAL] batch:   73 | acc: 81.25%,  total acc: 87.58%   [EVAL] batch:   74 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:   75 | acc: 100.00%,  total acc: 87.66%   [EVAL] batch:   76 | acc: 93.75%,  total acc: 87.74%   [EVAL] batch:   77 | acc: 87.50%,  total acc: 87.74%   [EVAL] batch:   78 | acc: 81.25%,  total acc: 87.66%   [EVAL] batch:   79 | acc: 81.25%,  total acc: 87.58%   [EVAL] batch:   80 | acc: 93.75%,  total acc: 87.65%   [EVAL] batch:   81 | acc: 87.50%,  total acc: 87.65%   [EVAL] batch:   82 | acc: 62.50%,  total acc: 87.35%   [EVAL] batch:   83 | acc: 87.50%,  total acc: 87.35%   [EVAL] batch:   84 | acc: 75.00%,  total acc: 87.21%   [EVAL] batch:   85 | acc: 81.25%,  total acc: 87.14%   [EVAL] batch:   86 | acc: 62.50%,  total acc: 86.85%   [EVAL] batch:   87 | acc: 93.75%,  total acc: 86.93%   [EVAL] batch:   88 | acc: 87.50%,  total acc: 86.94%   [EVAL] batch:   89 | acc: 81.25%,  total acc: 86.88%   [EVAL] batch:   90 | acc: 100.00%,  total acc: 87.02%   [EVAL] batch:   91 | acc: 62.50%,  total acc: 86.75%   [EVAL] batch:   92 | acc: 93.75%,  total acc: 86.83%   [EVAL] batch:   93 | acc: 62.50%,  total acc: 86.57%   [EVAL] batch:   94 | acc: 75.00%,  total acc: 86.45%   [EVAL] batch:   95 | acc: 50.00%,  total acc: 86.07%   [EVAL] batch:   96 | acc: 50.00%,  total acc: 85.70%   [EVAL] batch:   97 | acc: 56.25%,  total acc: 85.40%   [EVAL] batch:   98 | acc: 81.25%,  total acc: 85.35%   [EVAL] batch:   99 | acc: 37.50%,  total acc: 84.88%   [EVAL] batch:  100 | acc: 50.00%,  total acc: 84.53%   [EVAL] batch:  101 | acc: 62.50%,  total acc: 84.31%   [EVAL] batch:  102 | acc: 50.00%,  total acc: 83.98%   [EVAL] batch:  103 | acc: 68.75%,  total acc: 83.83%   [EVAL] batch:  104 | acc: 68.75%,  total acc: 83.69%   [EVAL] batch:  105 | acc: 68.75%,  total acc: 83.55%   [EVAL] batch:  106 | acc: 37.50%,  total acc: 83.12%   [EVAL] batch:  107 | acc: 25.00%,  total acc: 82.58%   [EVAL] batch:  108 | acc: 31.25%,  total acc: 82.11%   [EVAL] batch:  109 | acc: 37.50%,  total acc: 81.70%   [EVAL] batch:  110 | acc: 25.00%,  total acc: 81.19%   [EVAL] batch:  111 | acc: 50.00%,  total acc: 80.92%   [EVAL] batch:  112 | acc: 62.50%,  total acc: 80.75%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 80.92%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 81.09%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 81.41%   [EVAL] batch:  117 | acc: 100.00%,  total acc: 81.57%   [EVAL] batch:  118 | acc: 100.00%,  total acc: 81.72%   [EVAL] batch:  119 | acc: 87.50%,  total acc: 81.77%   [EVAL] batch:  120 | acc: 81.25%,  total acc: 81.77%   [EVAL] batch:  121 | acc: 93.75%,  total acc: 81.86%   [EVAL] batch:  122 | acc: 87.50%,  total acc: 81.91%   [EVAL] batch:  123 | acc: 93.75%,  total acc: 82.01%   [EVAL] batch:  124 | acc: 87.50%,  total acc: 82.05%   [EVAL] batch:  125 | acc: 75.00%,  total acc: 81.99%   [EVAL] batch:  126 | acc: 68.75%,  total acc: 81.89%   [EVAL] batch:  127 | acc: 75.00%,  total acc: 81.84%   [EVAL] batch:  128 | acc: 93.75%,  total acc: 81.93%   [EVAL] batch:  129 | acc: 93.75%,  total acc: 82.02%   [EVAL] batch:  130 | acc: 75.00%,  total acc: 81.97%   [EVAL] batch:  131 | acc: 87.50%,  total acc: 82.01%   [EVAL] batch:  132 | acc: 93.75%,  total acc: 82.10%   [EVAL] batch:  133 | acc: 81.25%,  total acc: 82.09%   [EVAL] batch:  134 | acc: 87.50%,  total acc: 82.13%   [EVAL] batch:  135 | acc: 100.00%,  total acc: 82.26%   [EVAL] batch:  136 | acc: 93.75%,  total acc: 82.34%   [EVAL] batch:  137 | acc: 75.00%,  total acc: 82.29%   [EVAL] batch:  138 | acc: 18.75%,  total acc: 81.83%   [EVAL] batch:  139 | acc: 31.25%,  total acc: 81.47%   [EVAL] batch:  140 | acc: 18.75%,  total acc: 81.03%   [EVAL] batch:  141 | acc: 37.50%,  total acc: 80.72%   [EVAL] batch:  142 | acc: 37.50%,  total acc: 80.42%   [EVAL] batch:  143 | acc: 31.25%,  total acc: 80.08%   [EVAL] batch:  144 | acc: 93.75%,  total acc: 80.17%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 80.31%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 80.40%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 80.53%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 80.66%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 80.75%   [EVAL] batch:  150 | acc: 75.00%,  total acc: 80.71%   [EVAL] batch:  151 | acc: 43.75%,  total acc: 80.47%   [EVAL] batch:  152 | acc: 43.75%,  total acc: 80.23%   [EVAL] batch:  153 | acc: 50.00%,  total acc: 80.03%   [EVAL] batch:  154 | acc: 50.00%,  total acc: 79.84%   [EVAL] batch:  155 | acc: 25.00%,  total acc: 79.49%   [EVAL] batch:  156 | acc: 75.00%,  total acc: 79.46%   [EVAL] batch:  157 | acc: 87.50%,  total acc: 79.51%   [EVAL] batch:  158 | acc: 93.75%,  total acc: 79.60%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 79.69%   [EVAL] batch:  160 | acc: 87.50%,  total acc: 79.74%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 79.82%   [EVAL] batch:  162 | acc: 100.00%,  total acc: 79.95%   [EVAL] batch:  163 | acc: 100.00%,  total acc: 80.07%   [EVAL] batch:  164 | acc: 100.00%,  total acc: 80.19%   [EVAL] batch:  165 | acc: 100.00%,  total acc: 80.31%   [EVAL] batch:  166 | acc: 93.75%,  total acc: 80.39%   [EVAL] batch:  167 | acc: 100.00%,  total acc: 80.51%   [EVAL] batch:  168 | acc: 93.75%,  total acc: 80.58%   [EVAL] batch:  169 | acc: 87.50%,  total acc: 80.62%   [EVAL] batch:  170 | acc: 87.50%,  total acc: 80.67%   [EVAL] batch:  171 | acc: 93.75%,  total acc: 80.74%   [EVAL] batch:  172 | acc: 68.75%,  total acc: 80.67%   [EVAL] batch:  173 | acc: 75.00%,  total acc: 80.64%   [EVAL] batch:  174 | acc: 87.50%,  total acc: 80.68%   [EVAL] batch:  175 | acc: 68.75%,  total acc: 80.61%   [EVAL] batch:  176 | acc: 75.00%,  total acc: 80.58%   [EVAL] batch:  177 | acc: 68.75%,  total acc: 80.51%   [EVAL] batch:  178 | acc: 75.00%,  total acc: 80.48%   [EVAL] batch:  179 | acc: 93.75%,  total acc: 80.56%   [EVAL] batch:  180 | acc: 87.50%,  total acc: 80.59%   [EVAL] batch:  181 | acc: 62.50%,  total acc: 80.49%   [EVAL] batch:  182 | acc: 56.25%,  total acc: 80.36%   [EVAL] batch:  183 | acc: 75.00%,  total acc: 80.33%   [EVAL] batch:  184 | acc: 62.50%,  total acc: 80.24%   [EVAL] batch:  185 | acc: 56.25%,  total acc: 80.11%   [EVAL] batch:  186 | acc: 50.00%,  total acc: 79.95%   [EVAL] batch:  187 | acc: 31.25%,  total acc: 79.69%   
cur_acc:  ['0.9494', '0.7302', '0.7500']
his_acc:  ['0.9494', '0.8310', '0.7969']
Clustering into  19  clusters
Clusters:  [ 1  2  8 10  6  6  8  4  2  9 15 18  4  6 16  9  0  6  5 17  0 11  3  0
  3 12 14  0  4 10 10  1  0  2  7  5  2 12 13  2]
Losses:  10.944406509399414 4.376521110534668 0.43427014350891113
CurrentTrain: epoch  0, batch     0 | loss: 10.9444065Losses:  9.691850662231445 3.764402389526367 0.6731627583503723
CurrentTrain: epoch  0, batch     1 | loss: 9.6918507Losses:  7.667273998260498 2.3733747005462646 0.6547092795372009
CurrentTrain: epoch  0, batch     2 | loss: 7.6672740Losses:  3.5264029502868652 -0.0 0.11133714020252228
CurrentTrain: epoch  0, batch     3 | loss: 3.5264030Losses:  7.225599765777588 2.227470874786377 0.693414568901062
CurrentTrain: epoch  1, batch     0 | loss: 7.2255998Losses:  7.167909145355225 2.8369626998901367 0.5896248817443848
CurrentTrain: epoch  1, batch     1 | loss: 7.1679091Losses:  8.612375259399414 3.531595468521118 0.6057407855987549
CurrentTrain: epoch  1, batch     2 | loss: 8.6123753Losses:  2.549882411956787 -0.0 0.09697578847408295
CurrentTrain: epoch  1, batch     3 | loss: 2.5498824Losses:  6.570429801940918 3.2773795127868652 0.6104766726493835
CurrentTrain: epoch  2, batch     0 | loss: 6.5704298Losses:  9.202312469482422 4.430675029754639 0.5358807444572449
CurrentTrain: epoch  2, batch     1 | loss: 9.2023125Losses:  5.724174499511719 2.8497745990753174 0.4893770217895508
CurrentTrain: epoch  2, batch     2 | loss: 5.7241745Losses:  3.778704881668091 -0.0 0.11032025516033173
CurrentTrain: epoch  2, batch     3 | loss: 3.7787049Losses:  8.127167701721191 4.521828651428223 0.6306039690971375
CurrentTrain: epoch  3, batch     0 | loss: 8.1271677Losses:  5.920130729675293 2.8400180339813232 0.48314404487609863
CurrentTrain: epoch  3, batch     1 | loss: 5.9201307Losses:  5.957037925720215 2.1886565685272217 0.6615102887153625
CurrentTrain: epoch  3, batch     2 | loss: 5.9570379Losses:  2.0929012298583984 -0.0 0.11157695949077606
CurrentTrain: epoch  3, batch     3 | loss: 2.0929012Losses:  5.695258617401123 2.408430576324463 0.5905674695968628
CurrentTrain: epoch  4, batch     0 | loss: 5.6952586Losses:  5.212642669677734 2.146505832672119 0.6120254993438721
CurrentTrain: epoch  4, batch     1 | loss: 5.2126427Losses:  5.4776482582092285 2.273103713989258 0.6622892618179321
CurrentTrain: epoch  4, batch     2 | loss: 5.4776483Losses:  2.8950088024139404 -0.0 0.10132197290658951
CurrentTrain: epoch  4, batch     3 | loss: 2.8950088Losses:  7.902413368225098 4.634410858154297 0.6086488962173462
CurrentTrain: epoch  5, batch     0 | loss: 7.9024134Losses:  6.128284454345703 3.3640589714050293 0.5726708173751831
CurrentTrain: epoch  5, batch     1 | loss: 6.1282845Losses:  5.231201171875 2.312878131866455 0.657008171081543
CurrentTrain: epoch  5, batch     2 | loss: 5.2312012Losses:  4.341045379638672 -0.0 -0.0
CurrentTrain: epoch  5, batch     3 | loss: 4.3410454Losses:  5.936603546142578 2.9246737957000732 0.5302412509918213
CurrentTrain: epoch  6, batch     0 | loss: 5.9366035Losses:  5.482008934020996 2.6735622882843018 0.5540826320648193
CurrentTrain: epoch  6, batch     1 | loss: 5.4820089Losses:  5.655299186706543 3.0446174144744873 0.5950753688812256
CurrentTrain: epoch  6, batch     2 | loss: 5.6552992Losses:  1.6847211122512817 -0.0 -0.0
CurrentTrain: epoch  6, batch     3 | loss: 1.6847211Losses:  5.094634056091309 2.281736373901367 0.5852424502372742
CurrentTrain: epoch  7, batch     0 | loss: 5.0946341Losses:  4.770525932312012 2.120178461074829 0.6338378190994263
CurrentTrain: epoch  7, batch     1 | loss: 4.7705259Losses:  4.538188934326172 2.0828254222869873 0.6240682601928711
CurrentTrain: epoch  7, batch     2 | loss: 4.5381889Losses:  1.8019248247146606 -0.0 0.14663183689117432
CurrentTrain: epoch  7, batch     3 | loss: 1.8019248Losses:  4.782649040222168 2.2799994945526123 0.5644824504852295
CurrentTrain: epoch  8, batch     0 | loss: 4.7826490Losses:  4.636300563812256 1.9721853733062744 0.6304057836532593
CurrentTrain: epoch  8, batch     1 | loss: 4.6363006Losses:  4.586126327514648 2.1621322631835938 0.5682336688041687
CurrentTrain: epoch  8, batch     2 | loss: 4.5861263Losses:  1.9537434577941895 -0.0 0.12801294028759003
CurrentTrain: epoch  8, batch     3 | loss: 1.9537435Losses:  5.3785858154296875 2.8889870643615723 0.5495119094848633
CurrentTrain: epoch  9, batch     0 | loss: 5.3785858Losses:  4.562110424041748 2.070725679397583 0.6188200116157532
CurrentTrain: epoch  9, batch     1 | loss: 4.5621104Losses:  5.284561634063721 2.846787929534912 0.4998859763145447
CurrentTrain: epoch  9, batch     2 | loss: 5.2845616Losses:  1.8189170360565186 -0.0 0.07865236699581146
CurrentTrain: epoch  9, batch     3 | loss: 1.8189170
#### member of political party ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "member of political party" means the political party of which a person is or has been a member or otherwise affiliated.
Here is an example:
Relation: member of political party
Context: alongside evan durbin and hugh gaitskell , he brought the thinking of john maynard keynes to the labour party , especially in relation to price determination .
Head Entity: hugh gaitskell
Tail Entity: labour party
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "member of political party":
Relation: member of political party  
Context: After serving as the mayor of Chicago, he became a prominent figure in the Democratic Party, advocating for social justice and economic reform.  
Head Entity: mayor of Chicago  
Tail Entity: Democratic Party  

Relation: member of political party  
Context: Throughout her career, she has been a staunch advocate for environmental policies as a member of the Green Party, pushing for sustainable practices at both local and national levels.  
Head Entity: she  
Tail Entity: Green Party  
#### after a work by ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "after a work by" means artist whose work strongly inspired/ was copied in this item.
Here is an example:
Relation: after a work by
Context: the original story of real life escape of betty mahmoody is depicted in the movie " not without my daughter " which itself was based on betty mahmoody 's book of the same name .
Head Entity: not without my daughter
Tail Entity: betty mahmoody
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "after a work by":
Relation: after a work by  
Context: the film "the great gatsby" draws heavily from f. scott fitzgerald's classic novel, capturing the essence of the roaring twenties and the tragic love story within.  
Head Entity: the great gatsby  
Tail Entity: f. scott fitzgerald  

Relation: after a work by  
Context: the animated feature "the lion king" was inspired by shakespeare's play "hamlet," incorporating themes of betrayal and redemption in a unique setting.  
Head Entity: the lion king  
Tail Entity: shakespeare
#### notable work ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "notable work" means notable scientific, artistic or literary work, or other work of significance among subject's works.
Here is an example:
Relation: notable work
Context: the influential victorian art critic john ruskin expounded theories about social justice in " unto this last " ( 1860 ) .
Head Entity: john ruskin
Tail Entity: unto this last
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "notable work":
Relation: notable work  
Context: the renowned physicist albert einstein published his groundbreaking theory of relativity in "on the electrodynamics of moving bodies" (1905).  
Head Entity: albert einstein  
Tail Entity: on the electrodynamics of moving bodies  

Relation: notable work  
Context: the celebrated author j.k. rowling captivated readers with her first book in the harry potter series, "harry potter and the philosopher's stone" (1997).  
Head Entity: j.k. rowling  
Tail Entity: harry potter and the philosopher's stone  
#### headquarters location ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "headquarters location" means city, where an organization's headquarters is or has been situated.
Here is an example:
Relation: headquarters location
Context: in 2011 , zhigarev was elected as a deputy in the state duma , the lower house of the federal assembly of russia , as a representative for the shchelkovsky constituency in moscow .
Head Entity: state duma
Tail Entity: moscow
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "headquarters location":
Relation: headquarters location  
Context: The global technology company, headquartered in Cupertino, California, has been a leader in innovation for over a decade.  
Head Entity: global technology company  
Tail Entity: Cupertino  

Relation: headquarters location  
Context: The non-profit organization moved its headquarters to Austin, Texas, to better serve its growing community of volunteers and supporters.  
Head Entity: non-profit organization  
Tail Entity: Austin  
#### taxon rank ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "taxon rank" means level in a taxonomic hierarchy.
Here is an example:
Relation: taxon rank
Context: a number of species that were placed in the family phyllophoridae by thander in 1989 and 1990 are now included in cucumariidae .
Head Entity: phyllophoridae
Tail Entity: family
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "taxon rank":
Relation: taxon rank  
Context: The genus Canis includes several species, with Canis lupus being the most recognized as the wolf, which belongs to the family Canidae.  
Head Entity: Canis  
Tail Entity: genus  

Relation: taxon rank  
Context: In the classification of plants, the family Rosaceae encompasses a variety of species, including roses and strawberries, which are categorized under the order Rosales.  
Head Entity: Rosaceae  
Tail Entity: family  
#### field of work ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "field of work" means specialization of a person or organization.
Here is an example:
Relation: field of work
Context: elspeth frances garman is professor of molecular biophysics at the university of oxford and a former president of the british crystallographic association .
Head Entity: elspeth frances garman
Tail Entity: molecular biophysics
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "field of work":
Relation: field of work  
Context: dr. amanda jones is a leading researcher in artificial intelligence and currently works at the mit media lab, focusing on machine learning applications.  
Head Entity: dr. amanda jones  
Tail Entity: artificial intelligence  

Relation: field of work  
Context: the world health organization has appointed dr. robert smith as the new director of global health initiatives, where he will concentrate on infectious disease control.  
Head Entity: dr. robert smith  
Tail Entity: global health initiatives  
#### licensed to broadcast to ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "licensed to broadcast to" means place that a radio/TV station is licensed/required to broadcast to.
Here is an example:
Relation: licensed to broadcast to
Context: wjmi ( 99.7 fm ) is a radio station licensed to jackson , mississippi , united states , with a mainstream urban musical format .
Head Entity: wjmi
Tail Entity: jackson , mississippi
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "licensed to broadcast to":
Relation: licensed to broadcast to  
Context: kxyz ( 101.5 fm ) is a radio station licensed to transmit to the city of austin , texas , providing a mix of pop and rock music.  
Head Entity: kxyz  
Tail Entity: austin , texas  

Relation: licensed to broadcast to  
Context: wqrs ( 88.3 fm ) is a community radio station licensed to serve the area of boulder , colorado , focusing on local news and music.  
Head Entity: wqrs  
Tail Entity: boulder , colorado  
#### constellation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "constellation" means the area of the celestial sphere of which the subject is a part.
Here is an example:
Relation: constellation
Context: ngc 192 is a type sba spiral galaxy located in the constellation cetus .
Head Entity: ngc 192
Tail Entity: cetus
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "constellation":
Relation: constellation  
Context: the orion nebula is located in the constellation orion and is one of the brightest nebulae visible to the naked eye.  
Head Entity: orion nebula  
Tail Entity: orion  

Relation: constellation  
Context: the star sirius is part of the constellation canis major, which is known for its prominence in the night sky.  
Head Entity: sirius  
Tail Entity: canis major  
#### platform ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "platform" means platform for which a work was developed or released, or the specific platform version of a software product.
Here is an example:
Relation: platform
Context: " modnation racers " for playstation 3 was announced and first shown publicly at e3 2009 .
Head Entity: modnation racers
Tail Entity: playstation 3
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "platform":
Relation: platform  
Context: "The Legend of Zelda: Breath of the Wild" was released for the Nintendo Switch and Wii U in March 2017.  
Head Entity: The Legend of Zelda: Breath of the Wild  
Tail Entity: Nintendo Switch  

Relation: platform  
Context: "Call of Duty: Warzone" is available on multiple platforms including PlayStation 4, Xbox One, and PC.  
Head Entity: Call of Duty: Warzone  
Tail Entity: PlayStation 4  
#### place served by transport hub ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "place served by transport hub" means territorial entity or entities served by this transport hub (airport, train station, etc.).
Here is an example:
Relation: place served by transport hub
Context: services to the island run from san juan 's luis muñoz marín international airport , ceiba airport or isla grande airport ( 20- to 30-minute flight ) .
Head Entity: isla grande airport
Tail Entity: san juan
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "place served by transport hub":
Relation: place served by transport hub  
Context: The city of Barcelona is well-connected to the surrounding regions through the El Prat Airport, which offers numerous flights to various destinations.  
Head Entity: El Prat Airport  
Tail Entity: Barcelona  

Relation: place served by transport hub  
Context: The central train station in Zurich provides access to multiple Swiss cities and international destinations, making it a key transport hub for travelers.  
Head Entity: Zurich central train station  
Tail Entity: Zurich  
Losses:  4.017210483551025 0.2840459644794464 0.869315505027771
MemoryTrain:  epoch  0, batch     0 | loss: 4.0172105Losses:  4.552181720733643 -0.0 0.9641188979148865
MemoryTrain:  epoch  0, batch     1 | loss: 4.5521817Losses:  4.123465538024902 0.5124040842056274 0.9153884649276733
MemoryTrain:  epoch  0, batch     2 | loss: 4.1234655Losses:  5.068195343017578 0.7225474119186401 0.6719334721565247
MemoryTrain:  epoch  0, batch     3 | loss: 5.0681953Losses:  4.514598846435547 0.848145604133606 0.8449172973632812
MemoryTrain:  epoch  0, batch     4 | loss: 4.5145988Losses:  3.9938805103302 0.20068150758743286 0.799558162689209
MemoryTrain:  epoch  0, batch     5 | loss: 3.9938805Losses:  5.127622127532959 0.5317225456237793 0.9740559458732605
MemoryTrain:  epoch  0, batch     6 | loss: 5.1276221Losses:  4.162761688232422 0.4497155547142029 0.5833001136779785
MemoryTrain:  epoch  0, batch     7 | loss: 4.1627617Losses:  4.3070878982543945 0.5181176662445068 0.8624064326286316
MemoryTrain:  epoch  1, batch     0 | loss: 4.3070879Losses:  4.522617816925049 0.7686994075775146 0.8761910200119019
MemoryTrain:  epoch  1, batch     1 | loss: 4.5226178Losses:  4.603318691253662 0.5236195921897888 0.9187551140785217
MemoryTrain:  epoch  1, batch     2 | loss: 4.6033187Losses:  3.143233060836792 0.2511790096759796 0.9498183727264404
MemoryTrain:  epoch  1, batch     3 | loss: 3.1432331Losses:  4.719860553741455 0.691290020942688 0.6512819528579712
MemoryTrain:  epoch  1, batch     4 | loss: 4.7198606Losses:  3.9693593978881836 0.8718934059143066 0.9238299131393433
MemoryTrain:  epoch  1, batch     5 | loss: 3.9693594Losses:  4.135224342346191 0.25826495885849 1.0218045711517334
MemoryTrain:  epoch  1, batch     6 | loss: 4.1352243Losses:  2.893479347229004 -0.0 0.6161689162254333
MemoryTrain:  epoch  1, batch     7 | loss: 2.8934793Losses:  3.888092517852783 0.39809685945510864 0.9854798316955566
MemoryTrain:  epoch  2, batch     0 | loss: 3.8880925Losses:  3.55922532081604 -0.0 0.8167192339897156
MemoryTrain:  epoch  2, batch     1 | loss: 3.5592253Losses:  3.444836139678955 0.25726011395454407 0.9919959902763367
MemoryTrain:  epoch  2, batch     2 | loss: 3.4448361Losses:  2.9499034881591797 0.29969125986099243 0.8357657194137573
MemoryTrain:  epoch  2, batch     3 | loss: 2.9499035Losses:  3.9351229667663574 1.0340466499328613 0.6420256495475769
MemoryTrain:  epoch  2, batch     4 | loss: 3.9351230Losses:  3.038605213165283 0.2934393286705017 0.8265928626060486
MemoryTrain:  epoch  2, batch     5 | loss: 3.0386052Losses:  3.5623927116394043 -0.0 0.8826603889465332
MemoryTrain:  epoch  2, batch     6 | loss: 3.5623927Losses:  2.113330364227295 -0.0 0.6830008029937744
MemoryTrain:  epoch  2, batch     7 | loss: 2.1133304Losses:  3.8873486518859863 0.5085289478302002 0.9034959077835083
MemoryTrain:  epoch  3, batch     0 | loss: 3.8873487Losses:  4.315216541290283 1.2128419876098633 0.8082894682884216
MemoryTrain:  epoch  3, batch     1 | loss: 4.3152165Losses:  4.049149990081787 0.5774838328361511 0.7563835978507996
MemoryTrain:  epoch  3, batch     2 | loss: 4.0491500Losses:  3.587827444076538 0.4818378984928131 0.7944685816764832
MemoryTrain:  epoch  3, batch     3 | loss: 3.5878274Losses:  3.438406467437744 0.7467136383056641 0.8704038262367249
MemoryTrain:  epoch  3, batch     4 | loss: 3.4384065Losses:  3.1204090118408203 0.5163893699645996 0.8479032516479492
MemoryTrain:  epoch  3, batch     5 | loss: 3.1204090Losses:  3.3654115200042725 0.3027176856994629 0.8138823509216309
MemoryTrain:  epoch  3, batch     6 | loss: 3.3654115Losses:  1.7809197902679443 -0.0 0.4871961772441864
MemoryTrain:  epoch  3, batch     7 | loss: 1.7809198Losses:  3.03997540473938 -0.0 0.8485352396965027
MemoryTrain:  epoch  4, batch     0 | loss: 3.0399754Losses:  3.526989221572876 0.7905454039573669 0.786062479019165
MemoryTrain:  epoch  4, batch     1 | loss: 3.5269892Losses:  3.173190116882324 0.5239517092704773 0.9041887521743774
MemoryTrain:  epoch  4, batch     2 | loss: 3.1731901Losses:  5.035161972045898 1.0307910442352295 0.9072952270507812
MemoryTrain:  epoch  4, batch     3 | loss: 5.0351620Losses:  2.747459888458252 0.28864896297454834 0.8744816780090332
MemoryTrain:  epoch  4, batch     4 | loss: 2.7474599Losses:  3.2897777557373047 0.8372488021850586 0.7348480820655823
MemoryTrain:  epoch  4, batch     5 | loss: 3.2897778Losses:  2.915823221206665 0.4779176712036133 0.8173989653587341
MemoryTrain:  epoch  4, batch     6 | loss: 2.9158232Losses:  3.1989595890045166 -0.0 0.525227963924408
MemoryTrain:  epoch  4, batch     7 | loss: 3.1989596Losses:  3.3319008350372314 0.3066929578781128 0.8688265681266785
MemoryTrain:  epoch  5, batch     0 | loss: 3.3319008Losses:  2.73997163772583 0.47177204489707947 0.9209942817687988
MemoryTrain:  epoch  5, batch     1 | loss: 2.7399716Losses:  4.387327671051025 0.6674206256866455 0.955013632774353
MemoryTrain:  epoch  5, batch     2 | loss: 4.3873277Losses:  2.669487476348877 0.2767203450202942 0.7599844336509705
MemoryTrain:  epoch  5, batch     3 | loss: 2.6694875Losses:  3.5330705642700195 0.7270675897598267 0.7319145202636719
MemoryTrain:  epoch  5, batch     4 | loss: 3.5330706Losses:  2.9854071140289307 0.5629570484161377 0.8131001591682434
MemoryTrain:  epoch  5, batch     5 | loss: 2.9854071Losses:  3.6474995613098145 1.0373400449752808 0.8367896676063538
MemoryTrain:  epoch  5, batch     6 | loss: 3.6474996Losses:  1.8803703784942627 -0.0 0.5886772274971008
MemoryTrain:  epoch  5, batch     7 | loss: 1.8803704Losses:  2.8285374641418457 -0.0 0.8788837194442749
MemoryTrain:  epoch  6, batch     0 | loss: 2.8285375Losses:  4.260064601898193 1.096771478652954 0.7826425433158875
MemoryTrain:  epoch  6, batch     1 | loss: 4.2600646Losses:  3.093167781829834 0.4919697046279907 0.808597207069397
MemoryTrain:  epoch  6, batch     2 | loss: 3.0931678Losses:  3.332155227661133 0.9948984980583191 0.8142586946487427
MemoryTrain:  epoch  6, batch     3 | loss: 3.3321552Losses:  2.5902652740478516 0.49933305382728577 0.7584435939788818
MemoryTrain:  epoch  6, batch     4 | loss: 2.5902653Losses:  3.528773307800293 0.5387574434280396 0.9302549362182617
MemoryTrain:  epoch  6, batch     5 | loss: 3.5287733Losses:  2.818300247192383 -0.0 0.8681142330169678
MemoryTrain:  epoch  6, batch     6 | loss: 2.8183002Losses:  2.4069364070892334 -0.0 0.6797200441360474
MemoryTrain:  epoch  6, batch     7 | loss: 2.4069364Losses:  3.3154313564300537 0.5155720114707947 0.7391383051872253
MemoryTrain:  epoch  7, batch     0 | loss: 3.3154314Losses:  2.9542741775512695 0.7473393678665161 0.8640275001525879
MemoryTrain:  epoch  7, batch     1 | loss: 2.9542742Losses:  2.816500663757324 0.22057071328163147 0.8277539610862732
MemoryTrain:  epoch  7, batch     2 | loss: 2.8165007Losses:  2.2712395191192627 0.2438517063856125 0.7196547389030457
MemoryTrain:  epoch  7, batch     3 | loss: 2.2712395Losses:  2.9536118507385254 0.8595393896102905 0.7982864379882812
MemoryTrain:  epoch  7, batch     4 | loss: 2.9536119Losses:  3.1497232913970947 0.4721405506134033 0.8415033221244812
MemoryTrain:  epoch  7, batch     5 | loss: 3.1497233Losses:  3.2406303882598877 0.2735179662704468 0.9569103121757507
MemoryTrain:  epoch  7, batch     6 | loss: 3.2406304Losses:  2.2500176429748535 -0.0 0.6149864196777344
MemoryTrain:  epoch  7, batch     7 | loss: 2.2500176Losses:  2.6070022583007812 0.23731885850429535 0.7974211573600769
MemoryTrain:  epoch  8, batch     0 | loss: 2.6070023Losses:  2.960597515106201 0.5439056158065796 0.9361280202865601
MemoryTrain:  epoch  8, batch     1 | loss: 2.9605975Losses:  2.856505870819092 0.5088889598846436 0.8866991996765137
MemoryTrain:  epoch  8, batch     2 | loss: 2.8565059Losses:  2.820315361022949 -0.0 0.8966791033744812
MemoryTrain:  epoch  8, batch     3 | loss: 2.8203154Losses:  2.496859550476074 0.23953822255134583 0.7330224514007568
MemoryTrain:  epoch  8, batch     4 | loss: 2.4968596Losses:  2.9873316287994385 0.9826948046684265 0.7350034713745117
MemoryTrain:  epoch  8, batch     5 | loss: 2.9873316Losses:  2.8357720375061035 0.4743782579898834 0.8204174041748047
MemoryTrain:  epoch  8, batch     6 | loss: 2.8357720Losses:  1.869765281677246 -0.0 0.6476597785949707
MemoryTrain:  epoch  8, batch     7 | loss: 1.8697653Losses:  3.591501474380493 0.8830517530441284 0.8026394248008728
MemoryTrain:  epoch  9, batch     0 | loss: 3.5915015Losses:  3.0644173622131348 0.5155467987060547 1.0388449430465698
MemoryTrain:  epoch  9, batch     1 | loss: 3.0644174Losses:  3.0348939895629883 0.4746846556663513 0.8630937337875366
MemoryTrain:  epoch  9, batch     2 | loss: 3.0348940Losses:  2.9088919162750244 0.4914933443069458 0.8753631114959717
MemoryTrain:  epoch  9, batch     3 | loss: 2.9088919Losses:  3.13100266456604 1.0293917655944824 0.7371376156806946
MemoryTrain:  epoch  9, batch     4 | loss: 3.1310027Losses:  2.4028003215789795 -0.0 0.8533074259757996
MemoryTrain:  epoch  9, batch     5 | loss: 2.4028003Losses:  2.845881700515747 0.715114176273346 0.7921985983848572
MemoryTrain:  epoch  9, batch     6 | loss: 2.8458817Losses:  2.1862740516662598 0.3195755183696747 0.6211473345756531
MemoryTrain:  epoch  9, batch     7 | loss: 2.1862741
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 90.62%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 89.58%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 88.75%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:    6 | acc: 56.25%,  total acc: 85.71%   [EVAL] batch:    7 | acc: 43.75%,  total acc: 80.47%   [EVAL] batch:    8 | acc: 37.50%,  total acc: 75.69%   [EVAL] batch:    9 | acc: 31.25%,  total acc: 71.25%   [EVAL] batch:   10 | acc: 37.50%,  total acc: 68.18%   [EVAL] batch:   11 | acc: 31.25%,  total acc: 65.10%   [EVAL] batch:   12 | acc: 50.00%,  total acc: 63.94%   [EVAL] batch:   13 | acc: 81.25%,  total acc: 65.18%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 65.83%   [EVAL] batch:   15 | acc: 62.50%,  total acc: 65.62%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 66.18%   [EVAL] batch:   17 | acc: 75.00%,  total acc: 66.67%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 66.45%   [EVAL] batch:   19 | acc: 43.75%,  total acc: 65.31%   [EVAL] batch:   20 | acc: 37.50%,  total acc: 63.99%   [EVAL] batch:   21 | acc: 56.25%,  total acc: 63.64%   [EVAL] batch:   22 | acc: 50.00%,  total acc: 63.04%   [EVAL] batch:   23 | acc: 62.50%,  total acc: 63.02%   [EVAL] batch:   24 | acc: 37.50%,  total acc: 62.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 63.46%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 64.81%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 66.07%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 67.24%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 68.33%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 69.35%   [EVAL] batch:   31 | acc: 75.00%,  total acc: 69.53%   [EVAL] batch:   32 | acc: 68.75%,  total acc: 69.51%   [EVAL] batch:   33 | acc: 62.50%,  total acc: 69.30%   [EVAL] batch:   34 | acc: 68.75%,  total acc: 69.29%   [EVAL] batch:   35 | acc: 68.75%,  total acc: 69.27%   [EVAL] batch:   36 | acc: 62.50%,  total acc: 69.09%   [EVAL] batch:   37 | acc: 62.50%,  total acc: 68.91%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 69.71%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 70.47%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 71.19%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 71.88%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 72.53%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 73.01%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 73.61%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 74.18%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 74.73%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 75.26%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 75.77%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 76.25%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 76.59%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 77.04%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 77.36%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 77.78%   [EVAL] batch:   54 | acc: 100.00%,  total acc: 78.18%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 78.57%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 78.84%   [EVAL] batch:   57 | acc: 81.25%,  total acc: 78.88%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 79.13%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 79.48%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 79.82%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 80.04%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 79.56%   
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 71.88%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 72.92%   [EVAL] batch:    3 | acc: 56.25%,  total acc: 68.75%   [EVAL] batch:    4 | acc: 62.50%,  total acc: 67.50%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 68.75%   [EVAL] batch:    6 | acc: 50.00%,  total acc: 66.07%   [EVAL] batch:    7 | acc: 68.75%,  total acc: 66.41%   [EVAL] batch:    8 | acc: 75.00%,  total acc: 67.36%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 70.00%   [EVAL] batch:   10 | acc: 50.00%,  total acc: 68.18%   [EVAL] batch:   11 | acc: 68.75%,  total acc: 68.23%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 69.23%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 70.98%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 72.50%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 74.22%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 75.37%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 76.39%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 77.63%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 78.12%   [EVAL] batch:   20 | acc: 81.25%,  total acc: 78.27%   [EVAL] batch:   21 | acc: 68.75%,  total acc: 77.84%   [EVAL] batch:   22 | acc: 68.75%,  total acc: 77.45%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 77.86%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 78.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 79.09%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 79.86%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 80.58%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 81.03%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 81.67%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 82.26%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 82.81%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 83.33%   [EVAL] batch:   33 | acc: 75.00%,  total acc: 83.09%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 82.86%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 82.99%   [EVAL] batch:   36 | acc: 93.75%,  total acc: 83.28%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 83.55%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 83.97%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 84.38%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 84.76%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 85.12%   [EVAL] batch:   42 | acc: 87.50%,  total acc: 85.17%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 85.51%   [EVAL] batch:   44 | acc: 68.75%,  total acc: 85.14%   [EVAL] batch:   45 | acc: 87.50%,  total acc: 85.19%   [EVAL] batch:   46 | acc: 75.00%,  total acc: 84.97%   [EVAL] batch:   47 | acc: 81.25%,  total acc: 84.90%   [EVAL] batch:   48 | acc: 87.50%,  total acc: 84.95%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 85.00%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 85.17%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 85.22%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 85.38%   [EVAL] batch:   53 | acc: 87.50%,  total acc: 85.42%   [EVAL] batch:   54 | acc: 68.75%,  total acc: 85.11%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 85.16%   [EVAL] batch:   56 | acc: 75.00%,  total acc: 84.98%   [EVAL] batch:   57 | acc: 50.00%,  total acc: 84.38%   [EVAL] batch:   58 | acc: 81.25%,  total acc: 84.32%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 84.48%   [EVAL] batch:   60 | acc: 75.00%,  total acc: 84.32%   [EVAL] batch:   61 | acc: 75.00%,  total acc: 84.17%   [EVAL] batch:   62 | acc: 81.25%,  total acc: 84.13%   [EVAL] batch:   63 | acc: 75.00%,  total acc: 83.98%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 84.23%   [EVAL] batch:   65 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:   66 | acc: 81.25%,  total acc: 84.33%   [EVAL] batch:   67 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:   68 | acc: 93.75%,  total acc: 84.51%   [EVAL] batch:   69 | acc: 93.75%,  total acc: 84.64%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 84.86%   [EVAL] batch:   71 | acc: 81.25%,  total acc: 84.81%   [EVAL] batch:   72 | acc: 87.50%,  total acc: 84.85%   [EVAL] batch:   73 | acc: 81.25%,  total acc: 84.80%   [EVAL] batch:   74 | acc: 75.00%,  total acc: 84.67%   [EVAL] batch:   75 | acc: 81.25%,  total acc: 84.62%   [EVAL] batch:   76 | acc: 68.75%,  total acc: 84.42%   [EVAL] batch:   77 | acc: 87.50%,  total acc: 84.46%   [EVAL] batch:   78 | acc: 75.00%,  total acc: 84.34%   [EVAL] batch:   79 | acc: 81.25%,  total acc: 84.30%   [EVAL] batch:   80 | acc: 87.50%,  total acc: 84.34%   [EVAL] batch:   81 | acc: 81.25%,  total acc: 84.30%   [EVAL] batch:   82 | acc: 56.25%,  total acc: 83.96%   [EVAL] batch:   83 | acc: 87.50%,  total acc: 84.00%   [EVAL] batch:   84 | acc: 62.50%,  total acc: 83.75%   [EVAL] batch:   85 | acc: 75.00%,  total acc: 83.65%   [EVAL] batch:   86 | acc: 56.25%,  total acc: 83.33%   [EVAL] batch:   87 | acc: 93.75%,  total acc: 83.45%   [EVAL] batch:   88 | acc: 87.50%,  total acc: 83.50%   [EVAL] batch:   89 | acc: 87.50%,  total acc: 83.54%   [EVAL] batch:   90 | acc: 100.00%,  total acc: 83.72%   [EVAL] batch:   91 | acc: 68.75%,  total acc: 83.56%   [EVAL] batch:   92 | acc: 87.50%,  total acc: 83.60%   [EVAL] batch:   93 | acc: 68.75%,  total acc: 83.44%   [EVAL] batch:   94 | acc: 68.75%,  total acc: 83.29%   [EVAL] batch:   95 | acc: 56.25%,  total acc: 83.01%   [EVAL] batch:   96 | acc: 50.00%,  total acc: 82.67%   [EVAL] batch:   97 | acc: 56.25%,  total acc: 82.40%   [EVAL] batch:   98 | acc: 75.00%,  total acc: 82.32%   [EVAL] batch:   99 | acc: 50.00%,  total acc: 82.00%   [EVAL] batch:  100 | acc: 56.25%,  total acc: 81.75%   [EVAL] batch:  101 | acc: 62.50%,  total acc: 81.56%   [EVAL] batch:  102 | acc: 62.50%,  total acc: 81.37%   [EVAL] batch:  103 | acc: 68.75%,  total acc: 81.25%   [EVAL] batch:  104 | acc: 68.75%,  total acc: 81.13%   [EVAL] batch:  105 | acc: 62.50%,  total acc: 80.96%   [EVAL] batch:  106 | acc: 37.50%,  total acc: 80.55%   [EVAL] batch:  107 | acc: 12.50%,  total acc: 79.92%   [EVAL] batch:  108 | acc: 31.25%,  total acc: 79.47%   [EVAL] batch:  109 | acc: 37.50%,  total acc: 79.09%   [EVAL] batch:  110 | acc: 31.25%,  total acc: 78.66%   [EVAL] batch:  111 | acc: 50.00%,  total acc: 78.40%   [EVAL] batch:  112 | acc: 62.50%,  total acc: 78.26%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 78.45%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 78.64%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 78.83%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 79.01%   [EVAL] batch:  117 | acc: 100.00%,  total acc: 79.18%   [EVAL] batch:  118 | acc: 100.00%,  total acc: 79.36%   [EVAL] batch:  119 | acc: 87.50%,  total acc: 79.43%   [EVAL] batch:  120 | acc: 81.25%,  total acc: 79.44%   [EVAL] batch:  121 | acc: 93.75%,  total acc: 79.56%   [EVAL] batch:  122 | acc: 87.50%,  total acc: 79.62%   [EVAL] batch:  123 | acc: 93.75%,  total acc: 79.74%   [EVAL] batch:  124 | acc: 81.25%,  total acc: 79.75%   [EVAL] batch:  125 | acc: 75.00%,  total acc: 79.71%   [EVAL] batch:  126 | acc: 81.25%,  total acc: 79.72%   [EVAL] batch:  127 | acc: 68.75%,  total acc: 79.64%   [EVAL] batch:  128 | acc: 81.25%,  total acc: 79.65%   [EVAL] batch:  129 | acc: 87.50%,  total acc: 79.71%   [EVAL] batch:  130 | acc: 68.75%,  total acc: 79.63%   [EVAL] batch:  131 | acc: 93.75%,  total acc: 79.73%   [EVAL] batch:  132 | acc: 93.75%,  total acc: 79.84%   [EVAL] batch:  133 | acc: 81.25%,  total acc: 79.85%   [EVAL] batch:  134 | acc: 87.50%,  total acc: 79.91%   [EVAL] batch:  135 | acc: 100.00%,  total acc: 80.06%   [EVAL] batch:  136 | acc: 93.75%,  total acc: 80.16%   [EVAL] batch:  137 | acc: 75.00%,  total acc: 80.12%   [EVAL] batch:  138 | acc: 18.75%,  total acc: 79.68%   [EVAL] batch:  139 | acc: 25.00%,  total acc: 79.29%   [EVAL] batch:  140 | acc: 18.75%,  total acc: 78.86%   [EVAL] batch:  141 | acc: 37.50%,  total acc: 78.57%   [EVAL] batch:  142 | acc: 37.50%,  total acc: 78.28%   [EVAL] batch:  143 | acc: 25.00%,  total acc: 77.91%   [EVAL] batch:  144 | acc: 93.75%,  total acc: 78.02%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 78.17%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 78.27%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 78.42%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 78.57%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 78.67%   [EVAL] batch:  150 | acc: 75.00%,  total acc: 78.64%   [EVAL] batch:  151 | acc: 43.75%,  total acc: 78.41%   [EVAL] batch:  152 | acc: 62.50%,  total acc: 78.31%   [EVAL] batch:  153 | acc: 62.50%,  total acc: 78.21%   [EVAL] batch:  154 | acc: 43.75%,  total acc: 77.98%   [EVAL] batch:  155 | acc: 43.75%,  total acc: 77.76%   [EVAL] batch:  156 | acc: 87.50%,  total acc: 77.83%   [EVAL] batch:  157 | acc: 87.50%,  total acc: 77.89%   [EVAL] batch:  158 | acc: 93.75%,  total acc: 77.99%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 78.09%   [EVAL] batch:  160 | acc: 87.50%,  total acc: 78.14%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 78.24%   [EVAL] batch:  162 | acc: 87.50%,  total acc: 78.30%   [EVAL] batch:  163 | acc: 81.25%,  total acc: 78.32%   [EVAL] batch:  164 | acc: 93.75%,  total acc: 78.41%   [EVAL] batch:  165 | acc: 75.00%,  total acc: 78.39%   [EVAL] batch:  166 | acc: 93.75%,  total acc: 78.48%   [EVAL] batch:  167 | acc: 93.75%,  total acc: 78.57%   [EVAL] batch:  168 | acc: 93.75%,  total acc: 78.66%   [EVAL] batch:  169 | acc: 81.25%,  total acc: 78.68%   [EVAL] batch:  170 | acc: 87.50%,  total acc: 78.73%   [EVAL] batch:  171 | acc: 93.75%,  total acc: 78.82%   [EVAL] batch:  172 | acc: 75.00%,  total acc: 78.79%   [EVAL] batch:  173 | acc: 81.25%,  total acc: 78.81%   [EVAL] batch:  174 | acc: 93.75%,  total acc: 78.89%   [EVAL] batch:  175 | acc: 68.75%,  total acc: 78.84%   [EVAL] batch:  176 | acc: 62.50%,  total acc: 78.74%   [EVAL] batch:  177 | acc: 81.25%,  total acc: 78.76%   [EVAL] batch:  178 | acc: 75.00%,  total acc: 78.74%   [EVAL] batch:  179 | acc: 81.25%,  total acc: 78.75%   [EVAL] batch:  180 | acc: 81.25%,  total acc: 78.76%   [EVAL] batch:  181 | acc: 56.25%,  total acc: 78.64%   [EVAL] batch:  182 | acc: 56.25%,  total acc: 78.52%   [EVAL] batch:  183 | acc: 62.50%,  total acc: 78.43%   [EVAL] batch:  184 | acc: 50.00%,  total acc: 78.28%   [EVAL] batch:  185 | acc: 50.00%,  total acc: 78.12%   [EVAL] batch:  186 | acc: 25.00%,  total acc: 77.84%   [EVAL] batch:  187 | acc: 62.50%,  total acc: 77.76%   [EVAL] batch:  188 | acc: 87.50%,  total acc: 77.81%   [EVAL] batch:  189 | acc: 93.75%,  total acc: 77.89%   [EVAL] batch:  190 | acc: 87.50%,  total acc: 77.95%   [EVAL] batch:  191 | acc: 81.25%,  total acc: 77.96%   [EVAL] batch:  192 | acc: 100.00%,  total acc: 78.08%   [EVAL] batch:  193 | acc: 81.25%,  total acc: 78.09%   [EVAL] batch:  194 | acc: 43.75%,  total acc: 77.92%   [EVAL] batch:  195 | acc: 50.00%,  total acc: 77.77%   [EVAL] batch:  196 | acc: 37.50%,  total acc: 77.57%   [EVAL] batch:  197 | acc: 25.00%,  total acc: 77.30%   [EVAL] batch:  198 | acc: 37.50%,  total acc: 77.10%   [EVAL] batch:  199 | acc: 25.00%,  total acc: 76.84%   [EVAL] batch:  200 | acc: 75.00%,  total acc: 76.83%   [EVAL] batch:  201 | acc: 81.25%,  total acc: 76.86%   [EVAL] batch:  202 | acc: 75.00%,  total acc: 76.85%   [EVAL] batch:  203 | acc: 56.25%,  total acc: 76.75%   [EVAL] batch:  204 | acc: 81.25%,  total acc: 76.77%   [EVAL] batch:  205 | acc: 68.75%,  total acc: 76.73%   [EVAL] batch:  206 | acc: 56.25%,  total acc: 76.63%   [EVAL] batch:  207 | acc: 37.50%,  total acc: 76.44%   [EVAL] batch:  208 | acc: 43.75%,  total acc: 76.29%   [EVAL] batch:  209 | acc: 56.25%,  total acc: 76.19%   [EVAL] batch:  210 | acc: 62.50%,  total acc: 76.13%   [EVAL] batch:  211 | acc: 37.50%,  total acc: 75.94%   [EVAL] batch:  212 | acc: 75.00%,  total acc: 75.94%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 76.05%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 76.16%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 76.27%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 76.38%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 76.49%   [EVAL] batch:  218 | acc: 93.75%,  total acc: 76.57%   [EVAL] batch:  219 | acc: 62.50%,  total acc: 76.51%   [EVAL] batch:  220 | acc: 75.00%,  total acc: 76.50%   [EVAL] batch:  221 | acc: 62.50%,  total acc: 76.44%   [EVAL] batch:  222 | acc: 56.25%,  total acc: 76.35%   [EVAL] batch:  223 | acc: 81.25%,  total acc: 76.37%   [EVAL] batch:  224 | acc: 37.50%,  total acc: 76.19%   [EVAL] batch:  225 | acc: 100.00%,  total acc: 76.30%   [EVAL] batch:  226 | acc: 100.00%,  total acc: 76.40%   [EVAL] batch:  227 | acc: 100.00%,  total acc: 76.51%   [EVAL] batch:  228 | acc: 100.00%,  total acc: 76.61%   [EVAL] batch:  229 | acc: 100.00%,  total acc: 76.71%   [EVAL] batch:  230 | acc: 93.75%,  total acc: 76.79%   [EVAL] batch:  231 | acc: 100.00%,  total acc: 76.89%   [EVAL] batch:  232 | acc: 100.00%,  total acc: 76.98%   [EVAL] batch:  233 | acc: 100.00%,  total acc: 77.08%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 77.18%   [EVAL] batch:  235 | acc: 100.00%,  total acc: 77.28%   [EVAL] batch:  236 | acc: 100.00%,  total acc: 77.37%   [EVAL] batch:  237 | acc: 100.00%,  total acc: 77.47%   [EVAL] batch:  238 | acc: 93.75%,  total acc: 77.54%   [EVAL] batch:  239 | acc: 93.75%,  total acc: 77.60%   [EVAL] batch:  240 | acc: 100.00%,  total acc: 77.70%   [EVAL] batch:  241 | acc: 100.00%,  total acc: 77.79%   [EVAL] batch:  242 | acc: 100.00%,  total acc: 77.88%   [EVAL] batch:  243 | acc: 93.75%,  total acc: 77.95%   [EVAL] batch:  244 | acc: 100.00%,  total acc: 78.04%   [EVAL] batch:  245 | acc: 75.00%,  total acc: 78.02%   [EVAL] batch:  246 | acc: 100.00%,  total acc: 78.11%   [EVAL] batch:  247 | acc: 100.00%,  total acc: 78.20%   [EVAL] batch:  248 | acc: 100.00%,  total acc: 78.29%   [EVAL] batch:  249 | acc: 93.75%,  total acc: 78.35%   
cur_acc:  ['0.9494', '0.7302', '0.7500', '0.7956']
his_acc:  ['0.9494', '0.8310', '0.7969', '0.7835']
Clustering into  24  clusters
Clusters:  [ 2  5 18  3  1  1 18 11  0  6 19 11 11  1 15  6 23  1 12 21 23 20 17  4
 17  5 13 23  9  3  3  2  4  0  8 12  0  5 16  0  5  7 14 18 11 22  0 12
 10  8]
Losses:  9.13122844696045 2.4831430912017822 0.869245707988739
CurrentTrain: epoch  0, batch     0 | loss: 9.1312284Losses:  11.606128692626953 3.7247276306152344 0.8062126636505127
CurrentTrain: epoch  0, batch     1 | loss: 11.6061287Losses:  9.206467628479004 3.1070919036865234 0.7621214389801025
CurrentTrain: epoch  0, batch     2 | loss: 9.2064676Losses:  6.920363426208496 -0.0 -0.0
CurrentTrain: epoch  0, batch     3 | loss: 6.9203634Losses:  8.62929916381836 2.5047640800476074 0.7716235518455505
CurrentTrain: epoch  1, batch     0 | loss: 8.6292992Losses:  7.290854454040527 2.2008252143859863 0.8448163866996765
CurrentTrain: epoch  1, batch     1 | loss: 7.2908545Losses:  9.204160690307617 3.3940725326538086 0.717870831489563
CurrentTrain: epoch  1, batch     2 | loss: 9.2041607Losses:  4.8801984786987305 -0.0 0.12402422726154327
CurrentTrain: epoch  1, batch     3 | loss: 4.8801985Losses:  9.345152854919434 4.266303539276123 0.7145742177963257
CurrentTrain: epoch  2, batch     0 | loss: 9.3451529Losses:  8.105541229248047 3.907714605331421 0.713259220123291
CurrentTrain: epoch  2, batch     1 | loss: 8.1055412Losses:  8.389225006103516 2.5430545806884766 0.814341127872467
CurrentTrain: epoch  2, batch     2 | loss: 8.3892250Losses:  4.9284348487854 -0.0 0.08501492440700531
CurrentTrain: epoch  2, batch     3 | loss: 4.9284348Losses:  9.025579452514648 3.951519012451172 0.7608803510665894
CurrentTrain: epoch  3, batch     0 | loss: 9.0255795Losses:  7.215036392211914 2.2113170623779297 0.8408681750297546
CurrentTrain: epoch  3, batch     1 | loss: 7.2150364Losses:  7.58890962600708 3.070082187652588 0.725657045841217
CurrentTrain: epoch  3, batch     2 | loss: 7.5889096Losses:  5.449193954467773 -0.0 0.07999256253242493
CurrentTrain: epoch  3, batch     3 | loss: 5.4491940Losses:  7.873445510864258 3.786553144454956 0.6956689357757568
CurrentTrain: epoch  4, batch     0 | loss: 7.8734455Losses:  8.149409294128418 3.754396677017212 0.7328063249588013
CurrentTrain: epoch  4, batch     1 | loss: 8.1494093Losses:  8.811166763305664 3.5380661487579346 0.6870062351226807
CurrentTrain: epoch  4, batch     2 | loss: 8.8111668Losses:  3.435520648956299 -0.0 0.13013163208961487
CurrentTrain: epoch  4, batch     3 | loss: 3.4355206Losses:  8.65505313873291 3.179715394973755 0.7925956845283508
CurrentTrain: epoch  5, batch     0 | loss: 8.6550531Losses:  6.330536365509033 1.9364104270935059 0.8425261378288269
CurrentTrain: epoch  5, batch     1 | loss: 6.3305364Losses:  7.105511665344238 3.5732529163360596 0.6610138416290283
CurrentTrain: epoch  5, batch     2 | loss: 7.1055117Losses:  5.56453275680542 -0.0 -0.0
CurrentTrain: epoch  5, batch     3 | loss: 5.5645328Losses:  7.342160701751709 2.962195873260498 0.6929693222045898
CurrentTrain: epoch  6, batch     0 | loss: 7.3421607Losses:  5.594107627868652 1.9437719583511353 0.8182581067085266
CurrentTrain: epoch  6, batch     1 | loss: 5.5941076Losses:  8.623680114746094 3.8439245223999023 0.7042727470397949
CurrentTrain: epoch  6, batch     2 | loss: 8.6236801Losses:  2.617713689804077 -0.0 0.1040409579873085
CurrentTrain: epoch  6, batch     3 | loss: 2.6177137Losses:  7.691056728363037 4.420435905456543 0.6824012994766235
CurrentTrain: epoch  7, batch     0 | loss: 7.6910567Losses:  8.4591064453125 3.646796703338623 0.6236862540245056
CurrentTrain: epoch  7, batch     1 | loss: 8.4591064Losses:  5.848593711853027 2.1088085174560547 0.8162299394607544
CurrentTrain: epoch  7, batch     2 | loss: 5.8485937Losses:  2.8672280311584473 -0.0 0.11620175838470459
CurrentTrain: epoch  7, batch     3 | loss: 2.8672280Losses:  6.089657783508301 2.823960304260254 0.6934875249862671
CurrentTrain: epoch  8, batch     0 | loss: 6.0896578Losses:  6.0600080490112305 2.1057398319244385 0.8025273680686951
CurrentTrain: epoch  8, batch     1 | loss: 6.0600080Losses:  6.572068214416504 2.622666120529175 0.7984564304351807
CurrentTrain: epoch  8, batch     2 | loss: 6.5720682Losses:  3.298029661178589 -0.0 0.10624192655086517
CurrentTrain: epoch  8, batch     3 | loss: 3.2980297Losses:  6.475916862487793 3.2441818714141846 0.6642864942550659
CurrentTrain: epoch  9, batch     0 | loss: 6.4759169Losses:  6.803731441497803 3.064220428466797 0.720209538936615
CurrentTrain: epoch  9, batch     1 | loss: 6.8037314Losses:  5.71208381652832 2.208644390106201 0.744235098361969
CurrentTrain: epoch  9, batch     2 | loss: 5.7120838Losses:  5.501442909240723 -0.0 0.2758494019508362
CurrentTrain: epoch  9, batch     3 | loss: 5.5014429
#### contains administrative territorial entity ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "contains administrative territorial entity" means direct subdivisions of an administrative territorial entity.
Here is an example:
Relation: contains administrative territorial entity
Context: the cantons of créteil are administrative divisions of the val - de - marne department , île - de - france region , northern france .
Head Entity: île - de - france
Tail Entity: val - de - marne
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "contains administrative territorial entity":
Relation: contains administrative territorial entity  
Context: The provinces of Canada are the primary administrative divisions, with Ontario being one of the largest provinces in the country.  
Head Entity: Canada  
Tail Entity: Ontario  

Relation: contains administrative territorial entity  
Context: The state of California is divided into several counties, with Los Angeles County being the most populous.  
Head Entity: California  
Tail Entity: Los Angeles County  
#### said to be the same as ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "said to be the same as" means this item is said to be the same as that item, but it's uncertain or disputed.
Here is an example:
Relation: said to be the same as
Context: enzo is an italian given name derivative of heinz , a form of henry .
Head Entity: henry
Tail Entity: heinz
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "said to be the same as":
Relation: said to be the same as  
Context: The two species, Canis lupus and Canis familiaris, are often said to be the same as each other due to their close genetic relationship.  
Head Entity: Canis lupus  
Tail Entity: Canis familiaris  

Relation: said to be the same as  
Context: Many historians argue that the ancient city of Byzantium is said to be the same as modern-day Istanbul, although the transition involved significant changes over time.  
Head Entity: Byzantium  
Tail Entity: Istanbul  
#### winner ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "winner" means winner of a competition or similar event.
Here is an example:
Relation: winner
Context: he considers his father to be biggest influence on his career as his brother javier castellano recipient of four eclipse award for outstanding jockey in the row ( 2013 , 2014,2015 and 2016 ) .
Head Entity: eclipse award for outstanding jockey
Tail Entity: javier castellano
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "winner":
Relation: winner  
Context: In the thrilling finale of the championship, Sarah Thompson emerged victorious, claiming the title of best player in the tournament, while her teammate, Mark Johnson, was awarded the runner-up position.  
Head Entity: best player in the tournament  
Tail Entity: Sarah Thompson  

Relation: winner  
Context: The annual science fair concluded with Emily Chen taking home the grand prize for her innovative project on renewable energy, while her classmate, David Lee, received an honorable mention for his work on robotics.  
Head Entity: grand prize  
Tail Entity: Emily Chen  
#### military rank ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "military rank" means military rank achieved by a person, or military rank associated with a position.
Here is an example:
Relation: military rank
Context: air commodore arthur kellam tylee obe ( 24 april 1887 – 13 april 1961 ) was canadian officer who served in the royal flying corps during world war i.
Head Entity: arthur kellam tylee
Tail Entity: air commodore
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "military rank":
Relation: military rank  
Context: general john smith was appointed as the chief of staff of the united states army in 2020, overseeing all operations and strategic planning.  
Head Entity: john smith  
Tail Entity: chief of staff  

Relation: military rank  
Context: colonel sarah jones led her battalion with distinction during the peacekeeping mission in the middle east, earning her a commendation for her leadership.  
Head Entity: sarah jones  
Tail Entity: colonel  
#### publisher ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "publisher" means organization or person responsible for publishing books, periodicals, printed music, podcasts, games or software.
Here is an example:
Relation: publisher
Context: there are seven different nations that are allied or in conflict : prior to its north american release , " vanguard bandits " was titled " detonator gauntlet " by working designs .
Head Entity: vanguard bandits
Tail Entity: working designs
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "publisher":
Relation: publisher  
Context: The latest novel by the acclaimed author was released by Penguin Random House, a well-known publishing house in the industry.  
Head Entity: latest novel  
Tail Entity: Penguin Random House  

Relation: publisher  
Context: After years of hard work, the game developer finally secured a deal with Electronic Arts to publish their new sports game.  
Head Entity: new sports game  
Tail Entity: Electronic Arts  
#### followed by ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "followed by" means immediately following item in a series of which the subject is a part.
Here is an example:
Relation: followed by
Context: " muzai moratorium " and " shōso strip " were released on vinyl in 2008 , as a part of her 10-year anniversary celebrations .
Head Entity: muzai moratorium
Tail Entity: shōso strip
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "followed by":
Relation: followed by  
Context: The first chapter of the book introduces the main character, while the second chapter follows with a detailed backstory.  
Head Entity: first chapter  
Tail Entity: second chapter  

Relation: followed by  
Context: In the concert lineup, the opening act performed a lively set, followed by the headliner who captivated the audience with their greatest hits.  
Head Entity: opening act  
Tail Entity: headliner  
#### work location ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "work location" means location where persons or organisations were actively participating in employment, business or other work.
Here is an example:
Relation: work location
Context: there is a memoir of his life and work by gustav queck ( jena , 1852 ) .
Head Entity: gustav queck
Tail Entity: jena
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "work location":
Relation: work location  
Context: the headquarters of the company is situated in san francisco, where they have been operating since 2010.  
Head Entity: the company  
Tail Entity: san francisco  

Relation: work location  
Context: during her time at the university, she conducted research in various labs located in boston.  
Head Entity: she  
Tail Entity: boston  
#### occupation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "occupation" means occupation of a person.
Here is an example:
Relation: occupation
Context: andrei droznin ( russian : андрей дрознин ) is a russian theatre director and movement coach .
Head Entity: andrei droznin
Tail Entity: theatre director
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "occupation":
Relation: occupation  
Context: dr. emily thompson is a renowned biochemist known for her groundbreaking research in cancer treatment.  
Head Entity: dr. emily thompson  
Tail Entity: biochemist  

Relation: occupation  
Context: john smith, a talented software engineer, has been working at tech innovations for over five years.  
Head Entity: john smith  
Tail Entity: software engineer  
#### heritage designation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "heritage designation" means heritage designation of a cultural or natural site.
Here is an example:
Relation: heritage designation
Context: altamaha town , bluffton historic district , church of the cross , and rose hill plantation house are listed on the national register of historic places .
Head Entity: altamaha town
Tail Entity: national register of historic places
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "heritage designation":
Relation: heritage designation  
Context: The Great Wall of China is recognized as a UNESCO World Heritage Site due to its historical significance and architectural grandeur.  
Head Entity: Great Wall of China  
Tail Entity: UNESCO World Heritage Site  

Relation: heritage designation  
Context: The ancient city of Petra in Jordan is designated as a World Heritage Site, attracting tourists from around the globe.  
Head Entity: Petra  
Tail Entity: World Heritage Site  
#### voice type ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "voice type" means person's voice type.
Here is an example:
Relation: voice type
Context: beverly bower ( september 30 , 1925 – march 24 , 2002 ) was an american operatic soprano who had an active international opera career from the mid-1950s through the early 1970s .
Head Entity: beverly bower
Tail Entity: soprano
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "voice type":
Relation: voice type  
Context: luciano pavarotti ( october 12 , 1935 – september 6 , 2007 ) was an italian tenor who gained worldwide fame for his televised concerts and media appearances.  
Head Entity: luciano pavarotti  
Tail Entity: tenor  

Relation: voice type  
Context: mariah carey ( born march 27 , 1969 ) is an american singer, songwriter, and actress known for her five-octave vocal range and her use of the whistle register.  
Head Entity: mariah carey  
Tail Entity: singer  
Losses:  5.0384063720703125 0.48059704899787903 0.7906076908111572
MemoryTrain:  epoch  0, batch     0 | loss: 5.0384064Losses:  3.9230098724365234 -0.0 0.9824467301368713
MemoryTrain:  epoch  0, batch     1 | loss: 3.9230099Losses:  4.524635314941406 0.7657986283302307 0.8152559399604797
MemoryTrain:  epoch  0, batch     2 | loss: 4.5246353Losses:  4.312894344329834 0.23797577619552612 0.8896307945251465
MemoryTrain:  epoch  0, batch     3 | loss: 4.3128943Losses:  3.6462759971618652 0.2610763907432556 0.914669394493103
MemoryTrain:  epoch  0, batch     4 | loss: 3.6462760Losses:  5.281360626220703 0.5213733315467834 0.8925590515136719
MemoryTrain:  epoch  0, batch     5 | loss: 5.2813606Losses:  5.795722007751465 0.27518463134765625 0.9274191856384277
MemoryTrain:  epoch  0, batch     6 | loss: 5.7957220Losses:  5.0623321533203125 0.554497480392456 0.7308494448661804
MemoryTrain:  epoch  0, batch     7 | loss: 5.0623322Losses:  3.6773035526275635 0.5665848851203918 0.910889208316803
MemoryTrain:  epoch  0, batch     8 | loss: 3.6773036Losses:  2.901413917541504 -0.0 0.4751449227333069
MemoryTrain:  epoch  0, batch     9 | loss: 2.9014139Losses:  5.826536655426025 1.200318455696106 0.8120490312576294
MemoryTrain:  epoch  1, batch     0 | loss: 5.8265367Losses:  4.1003851890563965 -0.0 0.8824785947799683
MemoryTrain:  epoch  1, batch     1 | loss: 4.1003852Losses:  3.90509033203125 0.5290757417678833 1.0133991241455078
MemoryTrain:  epoch  1, batch     2 | loss: 3.9050903Losses:  3.7091004848480225 0.2602047324180603 0.9725064635276794
MemoryTrain:  epoch  1, batch     3 | loss: 3.7091005Losses:  4.286873817443848 0.32316702604293823 1.0036437511444092
MemoryTrain:  epoch  1, batch     4 | loss: 4.2868738Losses:  3.4553146362304688 0.2366211712360382 0.9285510778427124
MemoryTrain:  epoch  1, batch     5 | loss: 3.4553146Losses:  3.0859720706939697 0.26487496495246887 0.9513530731201172
MemoryTrain:  epoch  1, batch     6 | loss: 3.0859721Losses:  4.250721454620361 -0.0 0.9090480804443359
MemoryTrain:  epoch  1, batch     7 | loss: 4.2507215Losses:  4.204818248748779 0.23858483135700226 0.8298504948616028
MemoryTrain:  epoch  1, batch     8 | loss: 4.2048182Losses:  2.868255853652954 -0.0 0.5213139057159424
MemoryTrain:  epoch  1, batch     9 | loss: 2.8682559Losses:  3.4280519485473633 0.26921457052230835 0.935263991355896
MemoryTrain:  epoch  2, batch     0 | loss: 3.4280519Losses:  4.112574100494385 0.5032874941825867 0.8626372814178467
MemoryTrain:  epoch  2, batch     1 | loss: 4.1125741Losses:  2.8179283142089844 0.2524377703666687 0.8425054550170898
MemoryTrain:  epoch  2, batch     2 | loss: 2.8179283Losses:  2.760218858718872 -0.0 0.9906191229820251
MemoryTrain:  epoch  2, batch     3 | loss: 2.7602189Losses:  3.0590968132019043 0.25831180810928345 0.8752360343933105
MemoryTrain:  epoch  2, batch     4 | loss: 3.0590968Losses:  3.862550735473633 0.8331117033958435 0.854871392250061
MemoryTrain:  epoch  2, batch     5 | loss: 3.8625507Losses:  3.7184813022613525 0.5705283880233765 0.8074348568916321
MemoryTrain:  epoch  2, batch     6 | loss: 3.7184813Losses:  4.493187427520752 0.42110157012939453 1.0515416860580444
MemoryTrain:  epoch  2, batch     7 | loss: 4.4931874Losses:  4.18292236328125 0.3474404811859131 0.8862031102180481
MemoryTrain:  epoch  2, batch     8 | loss: 4.1829224Losses:  4.56241512298584 -0.0 0.5241195559501648
MemoryTrain:  epoch  2, batch     9 | loss: 4.5624151Losses:  3.642352342605591 0.5271074175834656 0.9857292771339417
MemoryTrain:  epoch  3, batch     0 | loss: 3.6423523Losses:  3.2786834239959717 0.5132752656936646 0.9767518043518066
MemoryTrain:  epoch  3, batch     1 | loss: 3.2786834Losses:  3.7720603942871094 0.5448548197746277 0.9257539510726929
MemoryTrain:  epoch  3, batch     2 | loss: 3.7720604Losses:  3.435311794281006 0.3078853487968445 0.8898177742958069
MemoryTrain:  epoch  3, batch     3 | loss: 3.4353118Losses:  3.6633501052856445 0.5838541984558105 0.7961930632591248
MemoryTrain:  epoch  3, batch     4 | loss: 3.6633501Losses:  3.4566922187805176 0.75700843334198 0.7528117299079895
MemoryTrain:  epoch  3, batch     5 | loss: 3.4566922Losses:  3.2387661933898926 0.8010555505752563 0.8355633616447449
MemoryTrain:  epoch  3, batch     6 | loss: 3.2387662Losses:  3.0595703125 0.23831626772880554 1.0350805521011353
MemoryTrain:  epoch  3, batch     7 | loss: 3.0595703Losses:  3.832106590270996 0.6238905787467957 0.8772486448287964
MemoryTrain:  epoch  3, batch     8 | loss: 3.8321066Losses:  2.4694857597351074 -0.0 0.5253587961196899
MemoryTrain:  epoch  3, batch     9 | loss: 2.4694858Losses:  3.758737802505493 0.5589452385902405 0.8042132258415222
MemoryTrain:  epoch  4, batch     0 | loss: 3.7587378Losses:  3.126818895339966 0.5889239311218262 0.8167321085929871
MemoryTrain:  epoch  4, batch     1 | loss: 3.1268189Losses:  3.403553009033203 0.6607338190078735 0.7556970715522766
MemoryTrain:  epoch  4, batch     2 | loss: 3.4035530Losses:  2.6017611026763916 -0.0 0.9300229549407959
MemoryTrain:  epoch  4, batch     3 | loss: 2.6017611Losses:  2.7515101432800293 -0.0 0.9047834277153015
MemoryTrain:  epoch  4, batch     4 | loss: 2.7515101Losses:  3.23402738571167 0.2510959506034851 0.7853418588638306
MemoryTrain:  epoch  4, batch     5 | loss: 3.2340274Losses:  3.253204345703125 0.7740195989608765 0.8777785897254944
MemoryTrain:  epoch  4, batch     6 | loss: 3.2532043Losses:  3.0999832153320312 0.2954912781715393 1.0419118404388428
MemoryTrain:  epoch  4, batch     7 | loss: 3.0999832Losses:  2.8384623527526855 0.5267611742019653 0.7366204857826233
MemoryTrain:  epoch  4, batch     8 | loss: 2.8384624Losses:  1.7024084329605103 -0.0 0.3175898492336273
MemoryTrain:  epoch  4, batch     9 | loss: 1.7024084Losses:  2.9274909496307373 0.273044615983963 0.9492239952087402
MemoryTrain:  epoch  5, batch     0 | loss: 2.9274909Losses:  3.1730308532714844 0.7937829494476318 0.8231741189956665
MemoryTrain:  epoch  5, batch     1 | loss: 3.1730309Losses:  2.9949707984924316 0.2576461136341095 1.0485570430755615
MemoryTrain:  epoch  5, batch     2 | loss: 2.9949708Losses:  2.8301210403442383 -0.0 0.938085675239563
MemoryTrain:  epoch  5, batch     3 | loss: 2.8301210Losses:  3.213366985321045 0.7969022989273071 0.7628287076950073
MemoryTrain:  epoch  5, batch     4 | loss: 3.2133670Losses:  3.5873265266418457 0.8121004700660706 0.7878333926200867
MemoryTrain:  epoch  5, batch     5 | loss: 3.5873265Losses:  2.4601950645446777 0.2626414895057678 0.8600748777389526
MemoryTrain:  epoch  5, batch     6 | loss: 2.4601951Losses:  3.0071442127227783 0.5199496150016785 0.9229254126548767
MemoryTrain:  epoch  5, batch     7 | loss: 3.0071442Losses:  3.5110301971435547 1.0571839809417725 0.9277925491333008
MemoryTrain:  epoch  5, batch     8 | loss: 3.5110302Losses:  2.518486738204956 -0.0 0.5385293960571289
MemoryTrain:  epoch  5, batch     9 | loss: 2.5184867Losses:  2.715312957763672 -0.0 0.9402316808700562
MemoryTrain:  epoch  6, batch     0 | loss: 2.7153130Losses:  3.538139581680298 0.8260318040847778 0.8537864089012146
MemoryTrain:  epoch  6, batch     1 | loss: 3.5381396Losses:  2.89060115814209 0.49880293011665344 0.8069270253181458
MemoryTrain:  epoch  6, batch     2 | loss: 2.8906012Losses:  2.505525588989258 -0.0 0.9579720497131348
MemoryTrain:  epoch  6, batch     3 | loss: 2.5055256Losses:  2.9966204166412354 0.571760892868042 0.9504324793815613
MemoryTrain:  epoch  6, batch     4 | loss: 2.9966204Losses:  3.71677303314209 1.6388864517211914 0.7187919616699219
MemoryTrain:  epoch  6, batch     5 | loss: 3.7167730Losses:  2.3920235633850098 -0.0 0.954421877861023
MemoryTrain:  epoch  6, batch     6 | loss: 2.3920236Losses:  3.0166406631469727 0.5280025601387024 0.8776503801345825
MemoryTrain:  epoch  6, batch     7 | loss: 3.0166407Losses:  3.0785820484161377 0.7369669079780579 0.7232463955879211
MemoryTrain:  epoch  6, batch     8 | loss: 3.0785820Losses:  1.934269905090332 0.2861254811286926 0.4266187846660614
MemoryTrain:  epoch  6, batch     9 | loss: 1.9342699Losses:  2.694495439529419 0.24424311518669128 0.9698986411094666
MemoryTrain:  epoch  7, batch     0 | loss: 2.6944954Losses:  3.1627583503723145 0.5161576867103577 0.9051269888877869
MemoryTrain:  epoch  7, batch     1 | loss: 3.1627584Losses:  3.1087260246276855 0.2571418583393097 0.797782301902771
MemoryTrain:  epoch  7, batch     2 | loss: 3.1087260Losses:  2.557523012161255 -0.0 1.0273871421813965
MemoryTrain:  epoch  7, batch     3 | loss: 2.5575230Losses:  2.3566994667053223 0.24594566226005554 0.7791672348976135
MemoryTrain:  epoch  7, batch     4 | loss: 2.3566995Losses:  2.2696471214294434 -0.0 0.818916916847229
MemoryTrain:  epoch  7, batch     5 | loss: 2.2696471Losses:  2.3225157260894775 -0.0 1.013946533203125
MemoryTrain:  epoch  7, batch     6 | loss: 2.3225157Losses:  2.716686487197876 0.49681636691093445 0.8416438698768616
MemoryTrain:  epoch  7, batch     7 | loss: 2.7166865Losses:  2.678544521331787 0.25880950689315796 0.9298304915428162
MemoryTrain:  epoch  7, batch     8 | loss: 2.6785445Losses:  1.824982762336731 -0.0 0.4974052608013153
MemoryTrain:  epoch  7, batch     9 | loss: 1.8249828Losses:  2.6306021213531494 -0.0 0.9260605573654175
MemoryTrain:  epoch  8, batch     0 | loss: 2.6306021Losses:  2.516420841217041 0.24343806505203247 0.9549567103385925
MemoryTrain:  epoch  8, batch     1 | loss: 2.5164208Losses:  2.403716564178467 0.23363108932971954 0.8494789004325867
MemoryTrain:  epoch  8, batch     2 | loss: 2.4037166Losses:  2.494006872177124 -0.0 1.0270744562149048
MemoryTrain:  epoch  8, batch     3 | loss: 2.4940069Losses:  2.617048740386963 0.28667885065078735 1.0209169387817383
MemoryTrain:  epoch  8, batch     4 | loss: 2.6170487Losses:  2.509446859359741 0.25991836190223694 0.7892183661460876
MemoryTrain:  epoch  8, batch     5 | loss: 2.5094469Losses:  3.093872308731079 0.5567803382873535 0.9253189563751221
MemoryTrain:  epoch  8, batch     6 | loss: 3.0938723Losses:  3.837094306945801 1.2331851720809937 0.8429965972900391
MemoryTrain:  epoch  8, batch     7 | loss: 3.8370943Losses:  2.796480894088745 0.29054731130599976 0.985028088092804
MemoryTrain:  epoch  8, batch     8 | loss: 2.7964809Losses:  1.7300970554351807 -0.0 0.4949285387992859
MemoryTrain:  epoch  8, batch     9 | loss: 1.7300971Losses:  2.744915723800659 0.5102077722549438 0.9492384791374207
MemoryTrain:  epoch  9, batch     0 | loss: 2.7449157Losses:  2.6515696048736572 0.4657752811908722 0.7880763411521912
MemoryTrain:  epoch  9, batch     1 | loss: 2.6515696Losses:  2.7771849632263184 0.5398310422897339 0.7116956114768982
MemoryTrain:  epoch  9, batch     2 | loss: 2.7771850Losses:  2.967629909515381 0.4956880509853363 0.9159213900566101
MemoryTrain:  epoch  9, batch     3 | loss: 2.9676299Losses:  3.3069121837615967 0.6492194533348083 0.8423957228660583
MemoryTrain:  epoch  9, batch     4 | loss: 3.3069122Losses:  2.6813747882843018 0.4931415319442749 0.9652050137519836
MemoryTrain:  epoch  9, batch     5 | loss: 2.6813748Losses:  3.035806894302368 0.7505878210067749 0.9084100127220154
MemoryTrain:  epoch  9, batch     6 | loss: 3.0358069Losses:  2.766707420349121 0.7661269307136536 0.779876172542572
MemoryTrain:  epoch  9, batch     7 | loss: 2.7667074Losses:  3.0479161739349365 0.9761937260627747 0.7709881663322449
MemoryTrain:  epoch  9, batch     8 | loss: 3.0479162Losses:  1.6810981035232544 -0.0 0.44036197662353516
MemoryTrain:  epoch  9, batch     9 | loss: 1.6810981
[EVAL] batch:    0 | acc: 12.50%,  total acc: 12.50%   [EVAL] batch:    1 | acc: 25.00%,  total acc: 18.75%   [EVAL] batch:    2 | acc: 18.75%,  total acc: 18.75%   [EVAL] batch:    3 | acc: 12.50%,  total acc: 17.19%   [EVAL] batch:    4 | acc: 31.25%,  total acc: 20.00%   [EVAL] batch:    5 | acc: 25.00%,  total acc: 20.83%   [EVAL] batch:    6 | acc: 50.00%,  total acc: 25.00%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 32.03%   [EVAL] batch:    8 | acc: 75.00%,  total acc: 36.81%   [EVAL] batch:    9 | acc: 56.25%,  total acc: 38.75%   [EVAL] batch:   10 | acc: 50.00%,  total acc: 39.77%   [EVAL] batch:   11 | acc: 68.75%,  total acc: 42.19%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 44.71%   [EVAL] batch:   13 | acc: 50.00%,  total acc: 45.09%   [EVAL] batch:   14 | acc: 56.25%,  total acc: 45.83%   [EVAL] batch:   15 | acc: 81.25%,  total acc: 48.05%   [EVAL] batch:   16 | acc: 68.75%,  total acc: 49.26%   [EVAL] batch:   17 | acc: 75.00%,  total acc: 50.69%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 52.96%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 54.69%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 56.85%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 58.81%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 60.60%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 61.98%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 63.50%   [EVAL] batch:   25 | acc: 56.25%,  total acc: 63.22%   [EVAL] batch:   26 | acc: 31.25%,  total acc: 62.04%   [EVAL] batch:   27 | acc: 43.75%,  total acc: 61.38%   [EVAL] batch:   28 | acc: 43.75%,  total acc: 60.78%   [EVAL] batch:   29 | acc: 50.00%,  total acc: 60.42%   [EVAL] batch:   30 | acc: 56.25%,  total acc: 60.28%   [EVAL] batch:   31 | acc: 62.50%,  total acc: 60.35%   [EVAL] batch:   32 | acc: 75.00%,  total acc: 60.80%   [EVAL] batch:   33 | acc: 31.25%,  total acc: 59.93%   [EVAL] batch:   34 | acc: 56.25%,  total acc: 59.82%   [EVAL] batch:   35 | acc: 37.50%,  total acc: 59.20%   [EVAL] batch:   36 | acc: 31.25%,  total acc: 58.45%   [EVAL] batch:   37 | acc: 75.00%,  total acc: 58.88%   [EVAL] batch:   38 | acc: 62.50%,  total acc: 58.97%   [EVAL] batch:   39 | acc: 81.25%,  total acc: 59.53%   [EVAL] batch:   40 | acc: 68.75%,  total acc: 59.76%   [EVAL] batch:   41 | acc: 68.75%,  total acc: 59.97%   [EVAL] batch:   42 | acc: 75.00%,  total acc: 60.32%   [EVAL] batch:   43 | acc: 75.00%,  total acc: 60.65%   [EVAL] batch:   44 | acc: 68.75%,  total acc: 60.83%   [EVAL] batch:   45 | acc: 75.00%,  total acc: 61.14%   [EVAL] batch:   46 | acc: 75.00%,  total acc: 61.44%   [EVAL] batch:   47 | acc: 68.75%,  total acc: 61.59%   [EVAL] batch:   48 | acc: 81.25%,  total acc: 61.99%   [EVAL] batch:   49 | acc: 81.25%,  total acc: 62.38%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 63.11%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 63.82%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 64.50%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 65.16%   [EVAL] batch:   54 | acc: 100.00%,  total acc: 65.80%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 66.41%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 67.00%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 67.46%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 68.01%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 68.54%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 69.06%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 69.56%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 69.25%   
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 75.00%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 79.17%   [EVAL] batch:    3 | acc: 62.50%,  total acc: 75.00%   [EVAL] batch:    4 | acc: 62.50%,  total acc: 72.50%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 75.00%   [EVAL] batch:    6 | acc: 43.75%,  total acc: 70.54%   [EVAL] batch:    7 | acc: 68.75%,  total acc: 70.31%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 70.14%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 71.88%   [EVAL] batch:   10 | acc: 50.00%,  total acc: 69.89%   [EVAL] batch:   11 | acc: 68.75%,  total acc: 69.79%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 70.19%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 71.88%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 73.33%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 76.47%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 77.43%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 78.62%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 79.06%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 79.46%   [EVAL] batch:   21 | acc: 75.00%,  total acc: 79.26%   [EVAL] batch:   22 | acc: 68.75%,  total acc: 78.80%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 79.17%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 79.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 80.29%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 81.02%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 81.70%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 82.11%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 82.71%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 83.27%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 83.79%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 84.28%   [EVAL] batch:   33 | acc: 75.00%,  total acc: 84.01%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 83.75%   [EVAL] batch:   35 | acc: 81.25%,  total acc: 83.68%   [EVAL] batch:   36 | acc: 87.50%,  total acc: 83.78%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 83.88%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 84.29%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 84.69%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 84.91%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 84.97%   [EVAL] batch:   42 | acc: 81.25%,  total acc: 84.88%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 85.09%   [EVAL] batch:   44 | acc: 75.00%,  total acc: 84.86%   [EVAL] batch:   45 | acc: 81.25%,  total acc: 84.78%   [EVAL] batch:   46 | acc: 68.75%,  total acc: 84.44%   [EVAL] batch:   47 | acc: 81.25%,  total acc: 84.38%   [EVAL] batch:   48 | acc: 75.00%,  total acc: 84.18%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 84.25%   [EVAL] batch:   50 | acc: 75.00%,  total acc: 84.07%   [EVAL] batch:   51 | acc: 81.25%,  total acc: 84.01%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 84.20%   [EVAL] batch:   53 | acc: 68.75%,  total acc: 83.91%   [EVAL] batch:   54 | acc: 62.50%,  total acc: 83.52%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 83.59%   [EVAL] batch:   56 | acc: 75.00%,  total acc: 83.44%   [EVAL] batch:   57 | acc: 56.25%,  total acc: 82.97%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 83.05%   [EVAL] batch:   59 | acc: 87.50%,  total acc: 83.12%   [EVAL] batch:   60 | acc: 87.50%,  total acc: 83.20%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 82.96%   [EVAL] batch:   62 | acc: 87.50%,  total acc: 83.04%   [EVAL] batch:   63 | acc: 87.50%,  total acc: 83.11%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 83.37%   [EVAL] batch:   65 | acc: 100.00%,  total acc: 83.62%   [EVAL] batch:   66 | acc: 100.00%,  total acc: 83.86%   [EVAL] batch:   67 | acc: 93.75%,  total acc: 84.01%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 84.24%   [EVAL] batch:   69 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 84.60%   [EVAL] batch:   71 | acc: 75.00%,  total acc: 84.46%   [EVAL] batch:   72 | acc: 81.25%,  total acc: 84.42%   [EVAL] batch:   73 | acc: 81.25%,  total acc: 84.38%   [EVAL] batch:   74 | acc: 75.00%,  total acc: 84.25%   [EVAL] batch:   75 | acc: 81.25%,  total acc: 84.21%   [EVAL] batch:   76 | acc: 62.50%,  total acc: 83.93%   [EVAL] batch:   77 | acc: 62.50%,  total acc: 83.65%   [EVAL] batch:   78 | acc: 68.75%,  total acc: 83.47%   [EVAL] batch:   79 | acc: 68.75%,  total acc: 83.28%   [EVAL] batch:   80 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:   81 | acc: 68.75%,  total acc: 83.16%   [EVAL] batch:   82 | acc: 56.25%,  total acc: 82.83%   [EVAL] batch:   83 | acc: 81.25%,  total acc: 82.81%   [EVAL] batch:   84 | acc: 62.50%,  total acc: 82.57%   [EVAL] batch:   85 | acc: 75.00%,  total acc: 82.49%   [EVAL] batch:   86 | acc: 50.00%,  total acc: 82.11%   [EVAL] batch:   87 | acc: 75.00%,  total acc: 82.03%   [EVAL] batch:   88 | acc: 87.50%,  total acc: 82.09%   [EVAL] batch:   89 | acc: 93.75%,  total acc: 82.22%   [EVAL] batch:   90 | acc: 93.75%,  total acc: 82.35%   [EVAL] batch:   91 | acc: 75.00%,  total acc: 82.27%   [EVAL] batch:   92 | acc: 75.00%,  total acc: 82.19%   [EVAL] batch:   93 | acc: 75.00%,  total acc: 82.11%   [EVAL] batch:   94 | acc: 68.75%,  total acc: 81.97%   [EVAL] batch:   95 | acc: 50.00%,  total acc: 81.64%   [EVAL] batch:   96 | acc: 56.25%,  total acc: 81.38%   [EVAL] batch:   97 | acc: 56.25%,  total acc: 81.12%   [EVAL] batch:   98 | acc: 62.50%,  total acc: 80.93%   [EVAL] batch:   99 | acc: 62.50%,  total acc: 80.75%   [EVAL] batch:  100 | acc: 56.25%,  total acc: 80.51%   [EVAL] batch:  101 | acc: 62.50%,  total acc: 80.33%   [EVAL] batch:  102 | acc: 50.00%,  total acc: 80.04%   [EVAL] batch:  103 | acc: 68.75%,  total acc: 79.93%   [EVAL] batch:  104 | acc: 68.75%,  total acc: 79.82%   [EVAL] batch:  105 | acc: 62.50%,  total acc: 79.66%   [EVAL] batch:  106 | acc: 43.75%,  total acc: 79.32%   [EVAL] batch:  107 | acc: 18.75%,  total acc: 78.76%   [EVAL] batch:  108 | acc: 31.25%,  total acc: 78.33%   [EVAL] batch:  109 | acc: 31.25%,  total acc: 77.90%   [EVAL] batch:  110 | acc: 31.25%,  total acc: 77.48%   [EVAL] batch:  111 | acc: 50.00%,  total acc: 77.23%   [EVAL] batch:  112 | acc: 62.50%,  total acc: 77.10%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 77.30%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 77.50%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 77.69%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 77.88%   [EVAL] batch:  117 | acc: 100.00%,  total acc: 78.07%   [EVAL] batch:  118 | acc: 100.00%,  total acc: 78.26%   [EVAL] batch:  119 | acc: 93.75%,  total acc: 78.39%   [EVAL] batch:  120 | acc: 87.50%,  total acc: 78.46%   [EVAL] batch:  121 | acc: 93.75%,  total acc: 78.59%   [EVAL] batch:  122 | acc: 87.50%,  total acc: 78.66%   [EVAL] batch:  123 | acc: 100.00%,  total acc: 78.83%   [EVAL] batch:  124 | acc: 87.50%,  total acc: 78.90%   [EVAL] batch:  125 | acc: 75.00%,  total acc: 78.87%   [EVAL] batch:  126 | acc: 68.75%,  total acc: 78.79%   [EVAL] batch:  127 | acc: 62.50%,  total acc: 78.66%   [EVAL] batch:  128 | acc: 81.25%,  total acc: 78.68%   [EVAL] batch:  129 | acc: 81.25%,  total acc: 78.70%   [EVAL] batch:  130 | acc: 62.50%,  total acc: 78.58%   [EVAL] batch:  131 | acc: 87.50%,  total acc: 78.65%   [EVAL] batch:  132 | acc: 93.75%,  total acc: 78.76%   [EVAL] batch:  133 | acc: 81.25%,  total acc: 78.78%   [EVAL] batch:  134 | acc: 81.25%,  total acc: 78.80%   [EVAL] batch:  135 | acc: 100.00%,  total acc: 78.95%   [EVAL] batch:  136 | acc: 100.00%,  total acc: 79.11%   [EVAL] batch:  137 | acc: 68.75%,  total acc: 79.03%   [EVAL] batch:  138 | acc: 18.75%,  total acc: 78.60%   [EVAL] batch:  139 | acc: 37.50%,  total acc: 78.30%   [EVAL] batch:  140 | acc: 18.75%,  total acc: 77.88%   [EVAL] batch:  141 | acc: 43.75%,  total acc: 77.64%   [EVAL] batch:  142 | acc: 43.75%,  total acc: 77.40%   [EVAL] batch:  143 | acc: 37.50%,  total acc: 77.13%   [EVAL] batch:  144 | acc: 93.75%,  total acc: 77.24%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 77.40%   [EVAL] batch:  146 | acc: 100.00%,  total acc: 77.55%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 77.70%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 77.85%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 77.96%   [EVAL] batch:  150 | acc: 62.50%,  total acc: 77.86%   [EVAL] batch:  151 | acc: 37.50%,  total acc: 77.59%   [EVAL] batch:  152 | acc: 50.00%,  total acc: 77.41%   [EVAL] batch:  153 | acc: 50.00%,  total acc: 77.23%   [EVAL] batch:  154 | acc: 37.50%,  total acc: 76.98%   [EVAL] batch:  155 | acc: 25.00%,  total acc: 76.64%   [EVAL] batch:  156 | acc: 81.25%,  total acc: 76.67%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 76.78%   [EVAL] batch:  158 | acc: 100.00%,  total acc: 76.93%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 77.03%   [EVAL] batch:  160 | acc: 87.50%,  total acc: 77.10%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 77.20%   [EVAL] batch:  162 | acc: 87.50%,  total acc: 77.26%   [EVAL] batch:  163 | acc: 81.25%,  total acc: 77.29%   [EVAL] batch:  164 | acc: 93.75%,  total acc: 77.39%   [EVAL] batch:  165 | acc: 75.00%,  total acc: 77.37%   [EVAL] batch:  166 | acc: 100.00%,  total acc: 77.51%   [EVAL] batch:  167 | acc: 93.75%,  total acc: 77.60%   [EVAL] batch:  168 | acc: 100.00%,  total acc: 77.74%   [EVAL] batch:  169 | acc: 87.50%,  total acc: 77.79%   [EVAL] batch:  170 | acc: 93.75%,  total acc: 77.89%   [EVAL] batch:  171 | acc: 100.00%,  total acc: 78.02%   [EVAL] batch:  172 | acc: 75.00%,  total acc: 78.00%   [EVAL] batch:  173 | acc: 100.00%,  total acc: 78.12%   [EVAL] batch:  174 | acc: 100.00%,  total acc: 78.25%   [EVAL] batch:  175 | acc: 68.75%,  total acc: 78.20%   [EVAL] batch:  176 | acc: 75.00%,  total acc: 78.18%   [EVAL] batch:  177 | acc: 81.25%,  total acc: 78.20%   [EVAL] batch:  178 | acc: 81.25%,  total acc: 78.21%   [EVAL] batch:  179 | acc: 100.00%,  total acc: 78.33%   [EVAL] batch:  180 | acc: 81.25%,  total acc: 78.35%   [EVAL] batch:  181 | acc: 50.00%,  total acc: 78.19%   [EVAL] batch:  182 | acc: 56.25%,  total acc: 78.07%   [EVAL] batch:  183 | acc: 81.25%,  total acc: 78.09%   [EVAL] batch:  184 | acc: 56.25%,  total acc: 77.97%   [EVAL] batch:  185 | acc: 56.25%,  total acc: 77.86%   [EVAL] batch:  186 | acc: 43.75%,  total acc: 77.67%   [EVAL] batch:  187 | acc: 81.25%,  total acc: 77.69%   [EVAL] batch:  188 | acc: 93.75%,  total acc: 77.78%   [EVAL] batch:  189 | acc: 87.50%,  total acc: 77.83%   [EVAL] batch:  190 | acc: 93.75%,  total acc: 77.91%   [EVAL] batch:  191 | acc: 81.25%,  total acc: 77.93%   [EVAL] batch:  192 | acc: 100.00%,  total acc: 78.04%   [EVAL] batch:  193 | acc: 87.50%,  total acc: 78.09%   [EVAL] batch:  194 | acc: 50.00%,  total acc: 77.95%   [EVAL] batch:  195 | acc: 56.25%,  total acc: 77.84%   [EVAL] batch:  196 | acc: 37.50%,  total acc: 77.63%   [EVAL] batch:  197 | acc: 31.25%,  total acc: 77.40%   [EVAL] batch:  198 | acc: 37.50%,  total acc: 77.20%   [EVAL] batch:  199 | acc: 18.75%,  total acc: 76.91%   [EVAL] batch:  200 | acc: 68.75%,  total acc: 76.87%   [EVAL] batch:  201 | acc: 81.25%,  total acc: 76.89%   [EVAL] batch:  202 | acc: 93.75%,  total acc: 76.97%   [EVAL] batch:  203 | acc: 68.75%,  total acc: 76.93%   [EVAL] batch:  204 | acc: 81.25%,  total acc: 76.95%   [EVAL] batch:  205 | acc: 68.75%,  total acc: 76.91%   [EVAL] batch:  206 | acc: 56.25%,  total acc: 76.81%   [EVAL] batch:  207 | acc: 18.75%,  total acc: 76.53%   [EVAL] batch:  208 | acc: 12.50%,  total acc: 76.23%   [EVAL] batch:  209 | acc: 25.00%,  total acc: 75.98%   [EVAL] batch:  210 | acc: 25.00%,  total acc: 75.74%   [EVAL] batch:  211 | acc: 12.50%,  total acc: 75.44%   [EVAL] batch:  212 | acc: 56.25%,  total acc: 75.35%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 75.47%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 75.58%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 75.69%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 75.81%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 75.92%   [EVAL] batch:  218 | acc: 93.75%,  total acc: 76.00%   [EVAL] batch:  219 | acc: 56.25%,  total acc: 75.91%   [EVAL] batch:  220 | acc: 75.00%,  total acc: 75.90%   [EVAL] batch:  221 | acc: 56.25%,  total acc: 75.82%   [EVAL] batch:  222 | acc: 56.25%,  total acc: 75.73%   [EVAL] batch:  223 | acc: 75.00%,  total acc: 75.73%   [EVAL] batch:  224 | acc: 31.25%,  total acc: 75.53%   [EVAL] batch:  225 | acc: 100.00%,  total acc: 75.64%   [EVAL] batch:  226 | acc: 100.00%,  total acc: 75.74%   [EVAL] batch:  227 | acc: 100.00%,  total acc: 75.85%   [EVAL] batch:  228 | acc: 100.00%,  total acc: 75.96%   [EVAL] batch:  229 | acc: 100.00%,  total acc: 76.06%   [EVAL] batch:  230 | acc: 87.50%,  total acc: 76.11%   [EVAL] batch:  231 | acc: 93.75%,  total acc: 76.19%   [EVAL] batch:  232 | acc: 100.00%,  total acc: 76.29%   [EVAL] batch:  233 | acc: 100.00%,  total acc: 76.39%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 76.49%   [EVAL] batch:  235 | acc: 100.00%,  total acc: 76.59%   [EVAL] batch:  236 | acc: 93.75%,  total acc: 76.66%   [EVAL] batch:  237 | acc: 100.00%,  total acc: 76.76%   [EVAL] batch:  238 | acc: 87.50%,  total acc: 76.80%   [EVAL] batch:  239 | acc: 93.75%,  total acc: 76.88%   [EVAL] batch:  240 | acc: 100.00%,  total acc: 76.97%   [EVAL] batch:  241 | acc: 93.75%,  total acc: 77.04%   [EVAL] batch:  242 | acc: 93.75%,  total acc: 77.11%   [EVAL] batch:  243 | acc: 93.75%,  total acc: 77.18%   [EVAL] batch:  244 | acc: 100.00%,  total acc: 77.27%   [EVAL] batch:  245 | acc: 75.00%,  total acc: 77.26%   [EVAL] batch:  246 | acc: 100.00%,  total acc: 77.35%   [EVAL] batch:  247 | acc: 100.00%,  total acc: 77.44%   [EVAL] batch:  248 | acc: 100.00%,  total acc: 77.54%   [EVAL] batch:  249 | acc: 93.75%,  total acc: 77.60%   [EVAL] batch:  250 | acc: 12.50%,  total acc: 77.34%   [EVAL] batch:  251 | acc: 25.00%,  total acc: 77.13%   [EVAL] batch:  252 | acc: 18.75%,  total acc: 76.90%   [EVAL] batch:  253 | acc: 12.50%,  total acc: 76.65%   [EVAL] batch:  254 | acc: 31.25%,  total acc: 76.47%   [EVAL] batch:  255 | acc: 25.00%,  total acc: 76.27%   [EVAL] batch:  256 | acc: 50.00%,  total acc: 76.17%   [EVAL] batch:  257 | acc: 81.25%,  total acc: 76.19%   [EVAL] batch:  258 | acc: 75.00%,  total acc: 76.18%   [EVAL] batch:  259 | acc: 56.25%,  total acc: 76.11%   [EVAL] batch:  260 | acc: 50.00%,  total acc: 76.01%   [EVAL] batch:  261 | acc: 68.75%,  total acc: 75.98%   [EVAL] batch:  262 | acc: 75.00%,  total acc: 75.97%   [EVAL] batch:  263 | acc: 50.00%,  total acc: 75.88%   [EVAL] batch:  264 | acc: 56.25%,  total acc: 75.80%   [EVAL] batch:  265 | acc: 81.25%,  total acc: 75.82%   [EVAL] batch:  266 | acc: 68.75%,  total acc: 75.80%   [EVAL] batch:  267 | acc: 75.00%,  total acc: 75.79%   [EVAL] batch:  268 | acc: 93.75%,  total acc: 75.86%   [EVAL] batch:  269 | acc: 87.50%,  total acc: 75.90%   [EVAL] batch:  270 | acc: 100.00%,  total acc: 75.99%   [EVAL] batch:  271 | acc: 100.00%,  total acc: 76.08%   [EVAL] batch:  272 | acc: 100.00%,  total acc: 76.17%   [EVAL] batch:  273 | acc: 93.75%,  total acc: 76.23%   [EVAL] batch:  274 | acc: 100.00%,  total acc: 76.32%   [EVAL] batch:  275 | acc: 56.25%,  total acc: 76.25%   [EVAL] batch:  276 | acc: 31.25%,  total acc: 76.08%   [EVAL] batch:  277 | acc: 43.75%,  total acc: 75.97%   [EVAL] batch:  278 | acc: 43.75%,  total acc: 75.85%   [EVAL] batch:  279 | acc: 50.00%,  total acc: 75.76%   [EVAL] batch:  280 | acc: 56.25%,  total acc: 75.69%   [EVAL] batch:  281 | acc: 62.50%,  total acc: 75.64%   [EVAL] batch:  282 | acc: 75.00%,  total acc: 75.64%   [EVAL] batch:  283 | acc: 31.25%,  total acc: 75.48%   [EVAL] batch:  284 | acc: 56.25%,  total acc: 75.42%   [EVAL] batch:  285 | acc: 37.50%,  total acc: 75.28%   [EVAL] batch:  286 | acc: 31.25%,  total acc: 75.13%   [EVAL] batch:  287 | acc: 75.00%,  total acc: 75.13%   [EVAL] batch:  288 | acc: 62.50%,  total acc: 75.09%   [EVAL] batch:  289 | acc: 81.25%,  total acc: 75.11%   [EVAL] batch:  290 | acc: 68.75%,  total acc: 75.09%   [EVAL] batch:  291 | acc: 68.75%,  total acc: 75.06%   [EVAL] batch:  292 | acc: 75.00%,  total acc: 75.06%   [EVAL] batch:  293 | acc: 75.00%,  total acc: 75.06%   [EVAL] batch:  294 | acc: 68.75%,  total acc: 75.04%   [EVAL] batch:  295 | acc: 75.00%,  total acc: 75.04%   [EVAL] batch:  296 | acc: 75.00%,  total acc: 75.04%   [EVAL] batch:  297 | acc: 68.75%,  total acc: 75.02%   [EVAL] batch:  298 | acc: 81.25%,  total acc: 75.04%   [EVAL] batch:  299 | acc: 81.25%,  total acc: 75.06%   [EVAL] batch:  300 | acc: 100.00%,  total acc: 75.15%   [EVAL] batch:  301 | acc: 100.00%,  total acc: 75.23%   [EVAL] batch:  302 | acc: 100.00%,  total acc: 75.31%   [EVAL] batch:  303 | acc: 100.00%,  total acc: 75.39%   [EVAL] batch:  304 | acc: 100.00%,  total acc: 75.47%   [EVAL] batch:  305 | acc: 100.00%,  total acc: 75.55%   [EVAL] batch:  306 | acc: 100.00%,  total acc: 75.63%   [EVAL] batch:  307 | acc: 93.75%,  total acc: 75.69%   [EVAL] batch:  308 | acc: 100.00%,  total acc: 75.77%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 75.85%   [EVAL] batch:  310 | acc: 100.00%,  total acc: 75.92%   [EVAL] batch:  311 | acc: 100.00%,  total acc: 76.00%   [EVAL] batch:  312 | acc: 50.00%,  total acc: 75.92%   
cur_acc:  ['0.9494', '0.7302', '0.7500', '0.7956', '0.6925']
his_acc:  ['0.9494', '0.8310', '0.7969', '0.7835', '0.7592']
Clustering into  29  clusters
Clusters:  [27  3  9  8  6  6  9  0 12 17  4 11  0  6 20 17  1  6  2 14  1 23 18 10
 18  5 15 21  0  8  8 28 10 12 24  2 22  5 13 22  3 25 16  9 11 19 12  2
  7  1 19 14  4 26 21 14  4  1 17  1]
Losses:  11.073685646057129 3.6751952171325684 0.5171178579330444
CurrentTrain: epoch  0, batch     0 | loss: 11.0736856Losses:  10.600624084472656 3.557919502258301 0.6719192862510681
CurrentTrain: epoch  0, batch     1 | loss: 10.6006241Losses:  10.370888710021973 4.362555503845215 0.6337589025497437
CurrentTrain: epoch  0, batch     2 | loss: 10.3708887Losses:  7.45207405090332 -0.0 0.11036791652441025
CurrentTrain: epoch  0, batch     3 | loss: 7.4520741Losses:  7.524908542633057 2.14245343208313 0.6148322820663452
CurrentTrain: epoch  1, batch     0 | loss: 7.5249085Losses:  9.267473220825195 3.606353759765625 0.5303921699523926
CurrentTrain: epoch  1, batch     1 | loss: 9.2674732Losses:  8.51416301727295 3.5938820838928223 0.6293766498565674
CurrentTrain: epoch  1, batch     2 | loss: 8.5141630Losses:  6.740279197692871 -0.0 0.09839826077222824
CurrentTrain: epoch  1, batch     3 | loss: 6.7402792Losses:  9.069967269897461 3.5841615200042725 0.5480728149414062
CurrentTrain: epoch  2, batch     0 | loss: 9.0699673Losses:  7.006295204162598 2.6021320819854736 0.623071551322937
CurrentTrain: epoch  2, batch     1 | loss: 7.0062952Losses:  6.072845935821533 1.9101505279541016 0.6308341026306152
CurrentTrain: epoch  2, batch     2 | loss: 6.0728459Losses:  5.155510902404785 -0.0 0.13507848978042603
CurrentTrain: epoch  2, batch     3 | loss: 5.1555109Losses:  8.381071090698242 3.466820001602173 0.535322904586792
CurrentTrain: epoch  3, batch     0 | loss: 8.3810711Losses:  6.480105876922607 2.563098192214966 0.6322987675666809
CurrentTrain: epoch  3, batch     1 | loss: 6.4801059Losses:  6.689940929412842 2.5985257625579834 0.6148913502693176
CurrentTrain: epoch  3, batch     2 | loss: 6.6899409Losses:  2.115412950515747 -0.0 0.1559603363275528
CurrentTrain: epoch  3, batch     3 | loss: 2.1154130Losses:  8.380424499511719 4.364532947540283 0.5392152070999146
CurrentTrain: epoch  4, batch     0 | loss: 8.3804245Losses:  7.576261043548584 3.464332103729248 0.6004423499107361
CurrentTrain: epoch  4, batch     1 | loss: 7.5762610Losses:  6.381031513214111 2.5373475551605225 0.6086174249649048
CurrentTrain: epoch  4, batch     2 | loss: 6.3810315Losses:  3.4231491088867188 -0.0 0.11238089948892593
CurrentTrain: epoch  4, batch     3 | loss: 3.4231491Losses:  6.053924560546875 2.357043743133545 0.5875120162963867
CurrentTrain: epoch  5, batch     0 | loss: 6.0539246Losses:  6.363208770751953 2.5144259929656982 0.5869426727294922
CurrentTrain: epoch  5, batch     1 | loss: 6.3632088Losses:  5.463770389556885 2.3471784591674805 0.5889797210693359
CurrentTrain: epoch  5, batch     2 | loss: 5.4637704Losses:  2.592623233795166 -0.0 0.10188019275665283
CurrentTrain: epoch  5, batch     3 | loss: 2.5926232Losses:  6.72316837310791 3.186089038848877 0.512181282043457
CurrentTrain: epoch  6, batch     0 | loss: 6.7231684Losses:  5.73224401473999 2.4414546489715576 0.5772272944450378
CurrentTrain: epoch  6, batch     1 | loss: 5.7322440Losses:  5.903505325317383 3.004826068878174 0.5068387985229492
CurrentTrain: epoch  6, batch     2 | loss: 5.9035053Losses:  2.610976457595825 -0.0 0.1159750372171402
CurrentTrain: epoch  6, batch     3 | loss: 2.6109765Losses:  6.823554992675781 3.424663543701172 0.5138309001922607
CurrentTrain: epoch  7, batch     0 | loss: 6.8235550Losses:  5.790929794311523 2.924222230911255 0.5631865859031677
CurrentTrain: epoch  7, batch     1 | loss: 5.7909298Losses:  4.638172626495361 1.6605193614959717 0.5702040791511536
CurrentTrain: epoch  7, batch     2 | loss: 4.6381726Losses:  3.4753189086914062 -0.0 0.14596521854400635
CurrentTrain: epoch  7, batch     3 | loss: 3.4753189Losses:  5.646848201751709 2.7038161754608154 0.5637116432189941
CurrentTrain: epoch  8, batch     0 | loss: 5.6468482Losses:  6.488420486450195 3.523319721221924 0.48949599266052246
CurrentTrain: epoch  8, batch     1 | loss: 6.4884205Losses:  5.278243064880371 2.2971954345703125 0.5796300172805786
CurrentTrain: epoch  8, batch     2 | loss: 5.2782431Losses:  2.1760201454162598 -0.0 0.10623130947351456
CurrentTrain: epoch  8, batch     3 | loss: 2.1760201Losses:  5.4085469245910645 2.4195377826690674 0.5656769871711731
CurrentTrain: epoch  9, batch     0 | loss: 5.4085469Losses:  5.239650249481201 2.6096057891845703 0.5535615682601929
CurrentTrain: epoch  9, batch     1 | loss: 5.2396502Losses:  6.176997661590576 3.3660738468170166 0.481563925743103
CurrentTrain: epoch  9, batch     2 | loss: 6.1769977Losses:  3.0247716903686523 -0.0 -0.0
CurrentTrain: epoch  9, batch     3 | loss: 3.0247717
#### follows ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "follows" means immediately prior item in a series of which the subject is a part.
Here is an example:
Relation: follows
Context: one of the young authors discovered by nekrasov was leo tolstoy who debuted in " sovremennik " with his trilogy " childhood " , " boyhood " and " youth " .
Head Entity: youth
Tail Entity: boyhood
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "follows":
Relation: follows  
Context: In the famous painting series, the artist depicted a progression of seasons, where winter is followed by spring, showcasing the beauty of nature's cycle.  
Head Entity: spring  
Tail Entity: winter  

Relation: follows  
Context: The historical timeline of the Roman Empire shows that the reign of Augustus was followed by the rule of Tiberius, marking a significant transition in leadership.  
Head Entity: Tiberius  
Tail Entity: Augustus  
#### sport ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "sport" means sport that the subject participates or participated in or is associated with.
Here is an example:
Relation: sport
Context: it is primarily used for baseball and serves as the home stadium for yaquis de obregón .
Head Entity: yaquis de obregón
Tail Entity: baseball
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "sport":
Relation: sport  
Context: she has been a professional athlete in tennis since she was a teenager, competing in various international tournaments.  
Head Entity: she  
Tail Entity: tennis  

Relation: sport  
Context: the city is known for its vibrant soccer culture, with several teams competing in the national league.  
Head Entity: the city  
Tail Entity: soccer  
#### father ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "father" means male parent of the subject.
Here is an example:
Relation: father
Context: after the crowning of his nephew radoslav , the son of stefan , sava left the serbian maritime in 1229 for a trip to palestine .
Head Entity: radoslav
Tail Entity: stefan
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "father":
Relation: father  
Context: in the historical records, it is noted that king henry was the father of prince edward, who later became a significant figure in the kingdom.  
Head Entity: prince edward  
Tail Entity: king henry  

Relation: father  
Context: during the family reunion, it was revealed that michael, the youngest son, had always looked up to his father, robert, for guidance and support.  
Head Entity: michael  
Tail Entity: robert  
#### original network ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "original network" means network or service that originally broadcast a radio or television program.
Here is an example:
Relation: original network
Context: since 2009 , culshaw has starred in the bbc one comedy sketch show " the impressions show " alongside debra stephenson .
Head Entity: the impressions show
Tail Entity: bbc one
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "original network":
Relation: original network  
Context: The popular series "Breaking Bad" first aired on AMC, captivating audiences with its intense storytelling and character development.  
Head Entity: Breaking Bad  
Tail Entity: AMC  

Relation: original network  
Context: "Friends" became a cultural phenomenon when it premiered on NBC, showcasing the lives of six friends living in New York City.  
Head Entity: Friends  
Tail Entity: NBC  
#### original language of film or TV show ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "original language of film or TV show" means language in which a film or a performance work was originally created.
Here is an example:
Relation: original language of film or TV show
Context: he was also nominated twice for the filmfare best telugu actor award , for the films bhale bhale magadivoy and " gentleman " .
Head Entity: gentleman
Tail Entity: telugu
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "original language of film or TV show":
Relation: original language of film or TV show  
Context: The movie "Parasite" received critical acclaim and was originally produced in Korean, showcasing the talents of its director Bong Joon-ho.  
Head Entity: Parasite  
Tail Entity: Korean  

Relation: original language of film or TV show  
Context: The animated series "Doraemon" has been dubbed in multiple languages, but it was originally created in Japanese, captivating audiences worldwide.  
Head Entity: Doraemon  
Tail Entity: Japanese  
#### league ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "league" means league in which team or player plays or has played in.
Here is an example:
Relation: league
Context: the league champions for the fifth time in their history ( and the second season running ) were taunton town , but runners - up mangotsfield united took promotion to the southern league .
Head Entity: mangotsfield united
Tail Entity: southern league
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "league":
Relation: league  
Context: After a successful season, the team secured their spot in the premier league, showcasing their talent and determination throughout the matches, while their rivals struggled to keep up.  
Head Entity: the team  
Tail Entity: premier league  

Relation: league  
Context: The young athlete made headlines when he signed with the local club, aiming to make a name for himself in the national league after years of hard work and dedication.  
Head Entity: the local club  
Tail Entity: national league  
#### mother ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "mother" means female parent of the subject.
Here is an example:
Relation: mother
Context: retrieved 29 september 2010 . the daughter of actress xenia desni , tamara desni was born in berlin .
Head Entity: tamara desni
Tail Entity: xenia desni
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "mother":
Relation: mother  
Context: on 15th march 2015, the famous singer and actress, jennifer lopez, celebrated her birthday with her children, emme and max, who are her pride and joy.  
Head Entity: emme  
Tail Entity: jennifer lopez  

Relation: mother  
Context: during the family reunion, it was heartwarming to see how much the children adored their mother, elena, as they shared stories and laughter together.  
Head Entity: elena  
Tail Entity: children  
#### instrument ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "instrument" means musical instrument that a person plays or teaches or used in a music occupation.
Here is an example:
Relation: instrument
Context: peter bastian ( 25 august 1943 in copenhagen – 27 march 2017 ) was a danish musician . peter bastian bio and cv he was educated in theoretical physics and classical bassoon .
Head Entity: peter bastian
Tail Entity: bassoon
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "instrument":
Relation: instrument  
Context: johann sebastian bach (31 march 1685 – 28 july 1750) was a german composer and musician of the baroque period. he was known for his exceptional skill in playing the organ and the harpsichord.  
Head Entity: johann sebastian bach  
Tail Entity: organ  

Relation: instrument  
Context: yo-yo ma, born on october 7, 1955, is a world-renowned cellist who has performed with numerous orchestras and in various genres. he is particularly celebrated for his interpretations of classical cello concertos.  
Head Entity: yo-yo ma  
Tail Entity: cello  
#### country ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "country" means sovereign state of this item (not to be used for human beings).
Here is an example:
Relation: country
Context: ploegsteert is a village in belgium located in the municipality of comines - warneton in the hainaut province and is the most westerly settlement of the walloon region .
Head Entity: hainaut
Tail Entity: belgium
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "country":
Relation: country  
Context: the city of prague is the capital of the czech republic and is known for its beautiful architecture and rich history.  
Head Entity: prague  
Tail Entity: czech republic  

Relation: country  
Context: the great barrier reef is located off the coast of australia and is the largest coral reef system in the world.  
Head Entity: great barrier reef  
Tail Entity: australia  
#### characters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "characters" means characters which appear in this item (like plays, operas, operettas, books, comics, films, TV series, video games).
Here is an example:
Relation: characters
Context: klaus kinski , who would play dracula himself nine years later in " nosferatu the vampyre " , is also featured in the film as renfield .
Head Entity: nosferatu the vampyre
Tail Entity: renfield
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "characters":
Relation: characters  
Context: in the animated series "avatar: the last airbender," zuko is a complex character who struggles with his identity and family legacy.  
Head Entity: avatar: the last airbender  
Tail Entity: zuko  

Relation: characters  
Context: the novel "pride and prejudice" features elizabeth bennet, who navigates societal expectations and her feelings for mr. darcy.  
Head Entity: pride and prejudice  
Tail Entity: elizabeth bennet  
Losses:  3.3164713382720947 -0.0 1.0330162048339844
MemoryTrain:  epoch  0, batch     0 | loss: 3.3164713Losses:  4.861471652984619 0.4930747151374817 0.9124268293380737
MemoryTrain:  epoch  0, batch     1 | loss: 4.8614717Losses:  4.308722019195557 0.47802770137786865 0.9294392466545105
MemoryTrain:  epoch  0, batch     2 | loss: 4.3087220Losses:  3.811002492904663 0.2503075897693634 0.9646760821342468
MemoryTrain:  epoch  0, batch     3 | loss: 3.8110025Losses:  3.9473795890808105 0.2693210244178772 1.007306456565857
MemoryTrain:  epoch  0, batch     4 | loss: 3.9473796Losses:  4.286352157592773 0.2698681056499481 0.985061526298523
MemoryTrain:  epoch  0, batch     5 | loss: 4.2863522Losses:  4.172131061553955 0.7522259950637817 0.9759994745254517
MemoryTrain:  epoch  0, batch     6 | loss: 4.1721311Losses:  4.534405708312988 0.6298149824142456 1.0394740104675293
MemoryTrain:  epoch  0, batch     7 | loss: 4.5344057Losses:  4.929637908935547 0.4434542655944824 0.8937890529632568
MemoryTrain:  epoch  0, batch     8 | loss: 4.9296379Losses:  3.465085029602051 0.46932244300842285 0.9121774435043335
MemoryTrain:  epoch  0, batch     9 | loss: 3.4650850Losses:  3.760089874267578 -0.0 0.9478119611740112
MemoryTrain:  epoch  0, batch    10 | loss: 3.7600899Losses:  1.942262887954712 -0.0 0.3597753643989563
MemoryTrain:  epoch  0, batch    11 | loss: 1.9422629Losses:  2.9728240966796875 0.23467408120632172 0.9565086960792542
MemoryTrain:  epoch  1, batch     0 | loss: 2.9728241Losses:  4.267836570739746 0.349714070558548 0.9388839602470398
MemoryTrain:  epoch  1, batch     1 | loss: 4.2678366Losses:  4.532092571258545 0.8292567729949951 0.8571385741233826
MemoryTrain:  epoch  1, batch     2 | loss: 4.5320926Losses:  3.6869170665740967 -0.0 1.0716698169708252
MemoryTrain:  epoch  1, batch     3 | loss: 3.6869171Losses:  3.9135308265686035 0.3115788698196411 1.0013846158981323
MemoryTrain:  epoch  1, batch     4 | loss: 3.9135308Losses:  3.4138920307159424 0.5145737528800964 0.7978182435035706
MemoryTrain:  epoch  1, batch     5 | loss: 3.4138920Losses:  3.4372737407684326 -0.0 1.0148146152496338
MemoryTrain:  epoch  1, batch     6 | loss: 3.4372737Losses:  3.4568307399749756 0.5319823026657104 0.7370575070381165
MemoryTrain:  epoch  1, batch     7 | loss: 3.4568307Losses:  5.263669967651367 0.544480562210083 0.9206740856170654
MemoryTrain:  epoch  1, batch     8 | loss: 5.2636700Losses:  3.1558995246887207 0.4948537349700928 0.9402148127555847
MemoryTrain:  epoch  1, batch     9 | loss: 3.1558995Losses:  4.6017255783081055 0.38404136896133423 0.9260362982749939
MemoryTrain:  epoch  1, batch    10 | loss: 4.6017256Losses:  1.4149973392486572 -0.0 0.22364884614944458
MemoryTrain:  epoch  1, batch    11 | loss: 1.4149973Losses:  2.9608893394470215 0.24815109372138977 0.9005192518234253
MemoryTrain:  epoch  2, batch     0 | loss: 2.9608893Losses:  3.1516993045806885 0.2646819055080414 0.9520431160926819
MemoryTrain:  epoch  2, batch     1 | loss: 3.1516993Losses:  2.9672350883483887 -0.0 0.9008201956748962
MemoryTrain:  epoch  2, batch     2 | loss: 2.9672351Losses:  3.4021568298339844 0.5193632245063782 0.8970462679862976
MemoryTrain:  epoch  2, batch     3 | loss: 3.4021568Losses:  5.011486053466797 1.2519155740737915 0.9676440954208374
MemoryTrain:  epoch  2, batch     4 | loss: 5.0114861Losses:  2.7015271186828613 -0.0 0.8567550182342529
MemoryTrain:  epoch  2, batch     5 | loss: 2.7015271Losses:  4.303473949432373 0.2910383343696594 0.9319339990615845
MemoryTrain:  epoch  2, batch     6 | loss: 4.3034739Losses:  2.897578001022339 -0.0 0.9922944903373718
MemoryTrain:  epoch  2, batch     7 | loss: 2.8975780Losses:  4.100882053375244 0.2934724986553192 0.8649214506149292
MemoryTrain:  epoch  2, batch     8 | loss: 4.1008821Losses:  3.4095771312713623 -0.0 0.9609699845314026
MemoryTrain:  epoch  2, batch     9 | loss: 3.4095771Losses:  2.853147506713867 0.2645914554595947 0.8437694311141968
MemoryTrain:  epoch  2, batch    10 | loss: 2.8531475Losses:  2.9578588008880615 -0.0 0.3373015224933624
MemoryTrain:  epoch  2, batch    11 | loss: 2.9578588Losses:  3.957583427429199 0.6263405084609985 0.8070926666259766
MemoryTrain:  epoch  3, batch     0 | loss: 3.9575834Losses:  2.7109222412109375 -0.0 0.9681628942489624
MemoryTrain:  epoch  3, batch     1 | loss: 2.7109222Losses:  3.0613667964935303 -0.0 0.9856259822845459
MemoryTrain:  epoch  3, batch     2 | loss: 3.0613668Losses:  3.406430244445801 -0.0 1.0332555770874023
MemoryTrain:  epoch  3, batch     3 | loss: 3.4064302Losses:  3.5111680030822754 0.7389694452285767 0.9047006368637085
MemoryTrain:  epoch  3, batch     4 | loss: 3.5111680Losses:  3.1774978637695312 0.4831795394420624 0.8632750511169434
MemoryTrain:  epoch  3, batch     5 | loss: 3.1774979Losses:  3.926626682281494 0.7648841738700867 0.9047714471817017
MemoryTrain:  epoch  3, batch     6 | loss: 3.9266267Losses:  3.3239099979400635 0.5016105771064758 1.0115699768066406
MemoryTrain:  epoch  3, batch     7 | loss: 3.3239100Losses:  2.8465824127197266 0.2567135691642761 0.8432320356369019
MemoryTrain:  epoch  3, batch     8 | loss: 2.8465824Losses:  2.319060802459717 -0.0 0.9618357419967651
MemoryTrain:  epoch  3, batch     9 | loss: 2.3190608Losses:  2.9539051055908203 -0.0 1.130772590637207
MemoryTrain:  epoch  3, batch    10 | loss: 2.9539051Losses:  1.643099308013916 -0.0 0.42696040868759155
MemoryTrain:  epoch  3, batch    11 | loss: 1.6430993Losses:  3.908362627029419 0.6442967653274536 0.9214579463005066
MemoryTrain:  epoch  4, batch     0 | loss: 3.9083626Losses:  2.985727071762085 0.2558799982070923 0.9620821475982666
MemoryTrain:  epoch  4, batch     1 | loss: 2.9857271Losses:  3.0166566371917725 0.28165048360824585 0.9152576923370361
MemoryTrain:  epoch  4, batch     2 | loss: 3.0166566Losses:  3.251404047012329 0.3167583644390106 0.956310510635376
MemoryTrain:  epoch  4, batch     3 | loss: 3.2514040Losses:  2.7533586025238037 0.2367793321609497 0.7351509928703308
MemoryTrain:  epoch  4, batch     4 | loss: 2.7533586Losses:  2.678964853286743 0.5027410387992859 0.9047810435295105
MemoryTrain:  epoch  4, batch     5 | loss: 2.6789649Losses:  2.575733184814453 -0.0 0.9722181558609009
MemoryTrain:  epoch  4, batch     6 | loss: 2.5757332Losses:  3.2506227493286133 0.5077012777328491 0.9299167394638062
MemoryTrain:  epoch  4, batch     7 | loss: 3.2506227Losses:  2.4685797691345215 -0.0 1.0249279737472534
MemoryTrain:  epoch  4, batch     8 | loss: 2.4685798Losses:  3.236764907836914 0.2608691453933716 0.8957762718200684
MemoryTrain:  epoch  4, batch     9 | loss: 3.2367649Losses:  2.809246301651001 0.48556679487228394 0.9274210929870605
MemoryTrain:  epoch  4, batch    10 | loss: 2.8092463Losses:  3.7128536701202393 -0.0 0.3540513813495636
MemoryTrain:  epoch  4, batch    11 | loss: 3.7128537Losses:  2.549814224243164 0.256097674369812 0.8257964253425598
MemoryTrain:  epoch  5, batch     0 | loss: 2.5498142Losses:  3.2678699493408203 0.2275136113166809 0.9996578693389893
MemoryTrain:  epoch  5, batch     1 | loss: 3.2678699Losses:  2.3058605194091797 -0.0 0.8896188139915466
MemoryTrain:  epoch  5, batch     2 | loss: 2.3058605Losses:  3.2602758407592773 0.5329875946044922 0.8879765272140503
MemoryTrain:  epoch  5, batch     3 | loss: 3.2602758Losses:  3.1632533073425293 0.4669213593006134 0.9320296049118042
MemoryTrain:  epoch  5, batch     4 | loss: 3.1632533Losses:  3.2618918418884277 0.2676020860671997 0.9009528160095215
MemoryTrain:  epoch  5, batch     5 | loss: 3.2618918Losses:  3.463015556335449 0.2891976535320282 0.8464583158493042
MemoryTrain:  epoch  5, batch     6 | loss: 3.4630156Losses:  2.3113045692443848 -0.0 0.9607849717140198
MemoryTrain:  epoch  5, batch     7 | loss: 2.3113046Losses:  2.922504186630249 0.24905668199062347 0.9629789590835571
MemoryTrain:  epoch  5, batch     8 | loss: 2.9225042Losses:  2.512321949005127 0.28465843200683594 0.8459041118621826
MemoryTrain:  epoch  5, batch     9 | loss: 2.5123219Losses:  2.366163969039917 0.23714566230773926 0.8359783887863159
MemoryTrain:  epoch  5, batch    10 | loss: 2.3661640Losses:  1.723653793334961 -0.0 0.296798974275589
MemoryTrain:  epoch  5, batch    11 | loss: 1.7236538Losses:  2.9800968170166016 0.3093723654747009 0.9653297662734985
MemoryTrain:  epoch  6, batch     0 | loss: 2.9800968Losses:  2.705667734146118 0.24385909736156464 1.0210628509521484
MemoryTrain:  epoch  6, batch     1 | loss: 2.7056677Losses:  2.703721046447754 0.25337982177734375 0.9579164981842041
MemoryTrain:  epoch  6, batch     2 | loss: 2.7037210Losses:  3.0319228172302246 0.5274380445480347 0.8639918565750122
MemoryTrain:  epoch  6, batch     3 | loss: 3.0319228Losses:  3.0907890796661377 0.565884530544281 0.9212140440940857
MemoryTrain:  epoch  6, batch     4 | loss: 3.0907891Losses:  2.4669442176818848 0.2516179382801056 0.847183346748352
MemoryTrain:  epoch  6, batch     5 | loss: 2.4669442Losses:  2.706012725830078 -0.0 1.0106738805770874
MemoryTrain:  epoch  6, batch     6 | loss: 2.7060127Losses:  2.992464542388916 0.7504612803459167 0.9126899242401123
MemoryTrain:  epoch  6, batch     7 | loss: 2.9924645Losses:  2.809086322784424 0.2798762321472168 1.010985255241394
MemoryTrain:  epoch  6, batch     8 | loss: 2.8090863Losses:  2.7152369022369385 0.23615676164627075 1.0071759223937988
MemoryTrain:  epoch  6, batch     9 | loss: 2.7152369Losses:  3.104870557785034 0.23986361920833588 0.8378972411155701
MemoryTrain:  epoch  6, batch    10 | loss: 3.1048706Losses:  2.703998327255249 -0.0 0.3131638169288635
MemoryTrain:  epoch  6, batch    11 | loss: 2.7039983Losses:  2.8450887203216553 0.46485966444015503 0.9458938241004944
MemoryTrain:  epoch  7, batch     0 | loss: 2.8450887Losses:  2.485581159591675 -0.0 0.9167343974113464
MemoryTrain:  epoch  7, batch     1 | loss: 2.4855812Losses:  2.608194351196289 0.26746660470962524 0.9644821286201477
MemoryTrain:  epoch  7, batch     2 | loss: 2.6081944Losses:  2.7386810779571533 0.2662752866744995 0.8564800024032593
MemoryTrain:  epoch  7, batch     3 | loss: 2.7386811Losses:  2.7478320598602295 0.4871230125427246 0.8373101353645325
MemoryTrain:  epoch  7, batch     4 | loss: 2.7478321Losses:  3.3010525703430176 1.1005547046661377 0.7503268122673035
MemoryTrain:  epoch  7, batch     5 | loss: 3.3010526Losses:  2.6045451164245605 0.24534891545772552 0.8386310338973999
MemoryTrain:  epoch  7, batch     6 | loss: 2.6045451Losses:  3.6632590293884277 -0.0 0.9302325248718262
MemoryTrain:  epoch  7, batch     7 | loss: 3.6632590Losses:  2.4684767723083496 0.2599394917488098 0.8570379018783569
MemoryTrain:  epoch  7, batch     8 | loss: 2.4684768Losses:  3.437939167022705 0.28353211283683777 1.022046685218811
MemoryTrain:  epoch  7, batch     9 | loss: 3.4379392Losses:  2.6290855407714844 0.25531888008117676 0.9023561477661133
MemoryTrain:  epoch  7, batch    10 | loss: 2.6290855Losses:  1.9695409536361694 0.24596843123435974 0.21892908215522766
MemoryTrain:  epoch  7, batch    11 | loss: 1.9695410Losses:  2.3207106590270996 -0.0 0.8335837125778198
MemoryTrain:  epoch  8, batch     0 | loss: 2.3207107Losses:  2.7476325035095215 0.2601207494735718 0.9836628437042236
MemoryTrain:  epoch  8, batch     1 | loss: 2.7476325Losses:  2.7462449073791504 0.26379096508026123 1.0024465322494507
MemoryTrain:  epoch  8, batch     2 | loss: 2.7462449Losses:  2.50506591796875 -0.0 0.9684902429580688
MemoryTrain:  epoch  8, batch     3 | loss: 2.5050659Losses:  3.1492490768432617 0.5529416799545288 0.952277660369873
MemoryTrain:  epoch  8, batch     4 | loss: 3.1492491Losses:  2.605576753616333 0.24896183609962463 0.9185568690299988
MemoryTrain:  epoch  8, batch     5 | loss: 2.6055768Losses:  2.776111364364624 0.264525830745697 0.9625751972198486
MemoryTrain:  epoch  8, batch     6 | loss: 2.7761114Losses:  2.8542380332946777 0.5022615194320679 0.8512381911277771
MemoryTrain:  epoch  8, batch     7 | loss: 2.8542380Losses:  2.6217830181121826 -0.0 0.9810895323753357
MemoryTrain:  epoch  8, batch     8 | loss: 2.6217830Losses:  2.651735305786133 0.26419103145599365 0.9072321653366089
MemoryTrain:  epoch  8, batch     9 | loss: 2.6517353Losses:  2.77551007270813 0.4811677932739258 0.8976325988769531
MemoryTrain:  epoch  8, batch    10 | loss: 2.7755101Losses:  1.7388566732406616 -0.0 0.35067448019981384
MemoryTrain:  epoch  8, batch    11 | loss: 1.7388567Losses:  2.744367837905884 0.2523100972175598 1.0137276649475098
MemoryTrain:  epoch  9, batch     0 | loss: 2.7443678Losses:  2.2846193313598633 -0.0 0.9437078833580017
MemoryTrain:  epoch  9, batch     1 | loss: 2.2846193Losses:  2.8346400260925293 0.708302915096283 0.834208607673645
MemoryTrain:  epoch  9, batch     2 | loss: 2.8346400Losses:  2.3871469497680664 0.2289198487997055 0.907525897026062
MemoryTrain:  epoch  9, batch     3 | loss: 2.3871469Losses:  2.7527718544006348 0.49881428480148315 0.959263026714325
MemoryTrain:  epoch  9, batch     4 | loss: 2.7527719Losses:  2.7422103881835938 0.5071125626564026 0.8778449296951294
MemoryTrain:  epoch  9, batch     5 | loss: 2.7422104Losses:  2.6758556365966797 0.2640019655227661 0.9859330654144287
MemoryTrain:  epoch  9, batch     6 | loss: 2.6758556Losses:  2.673152208328247 0.29887568950653076 0.9051432609558105
MemoryTrain:  epoch  9, batch     7 | loss: 2.6731522Losses:  2.495490074157715 0.2558812201023102 0.8659479022026062
MemoryTrain:  epoch  9, batch     8 | loss: 2.4954901Losses:  2.5921852588653564 0.5330527424812317 0.796018660068512
MemoryTrain:  epoch  9, batch     9 | loss: 2.5921853Losses:  2.309959888458252 -0.0 1.010906457901001
MemoryTrain:  epoch  9, batch    10 | loss: 2.3099599Losses:  1.6124379634857178 -0.0 0.39102551341056824
MemoryTrain:  epoch  9, batch    11 | loss: 1.6124380
[EVAL] batch:    0 | acc: 25.00%,  total acc: 25.00%   [EVAL] batch:    1 | acc: 25.00%,  total acc: 25.00%   [EVAL] batch:    2 | acc: 31.25%,  total acc: 27.08%   [EVAL] batch:    3 | acc: 25.00%,  total acc: 26.56%   [EVAL] batch:    4 | acc: 31.25%,  total acc: 27.50%   [EVAL] batch:    5 | acc: 6.25%,  total acc: 23.96%   [EVAL] batch:    6 | acc: 50.00%,  total acc: 27.68%   [EVAL] batch:    7 | acc: 68.75%,  total acc: 32.81%   [EVAL] batch:    8 | acc: 75.00%,  total acc: 37.50%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 41.88%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 46.59%   [EVAL] batch:   11 | acc: 68.75%,  total acc: 48.44%   [EVAL] batch:   12 | acc: 62.50%,  total acc: 49.52%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 50.89%   [EVAL] batch:   14 | acc: 62.50%,  total acc: 51.67%   [EVAL] batch:   15 | acc: 43.75%,  total acc: 51.17%   [EVAL] batch:   16 | acc: 68.75%,  total acc: 52.21%   [EVAL] batch:   17 | acc: 87.50%,  total acc: 54.17%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 54.61%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 56.88%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 58.93%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 60.51%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 62.23%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 63.80%   [EVAL] batch:   24 | acc: 81.25%,  total acc: 64.50%   [EVAL] batch:   25 | acc: 93.75%,  total acc: 65.62%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 66.90%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 67.86%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 68.75%   [EVAL] batch:   29 | acc: 93.75%,  total acc: 69.58%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 70.16%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 71.09%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 71.97%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 72.61%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 73.21%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 73.78%   [EVAL] batch:   36 | acc: 93.75%,  total acc: 74.32%   [EVAL] batch:   37 | acc: 56.25%,  total acc: 73.85%   [EVAL] batch:   38 | acc: 25.00%,  total acc: 72.60%   [EVAL] batch:   39 | acc: 12.50%,  total acc: 71.09%   [EVAL] batch:   40 | acc: 12.50%,  total acc: 69.66%   [EVAL] batch:   41 | acc: 31.25%,  total acc: 68.75%   [EVAL] batch:   42 | acc: 18.75%,  total acc: 67.59%   [EVAL] batch:   43 | acc: 50.00%,  total acc: 67.19%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 67.92%   [EVAL] batch:   45 | acc: 93.75%,  total acc: 68.48%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 69.15%   [EVAL] batch:   47 | acc: 87.50%,  total acc: 69.53%   [EVAL] batch:   48 | acc: 87.50%,  total acc: 69.90%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 70.25%   [EVAL] batch:   50 | acc: 62.50%,  total acc: 70.10%   [EVAL] batch:   51 | acc: 68.75%,  total acc: 70.07%   [EVAL] batch:   52 | acc: 56.25%,  total acc: 69.81%   [EVAL] batch:   53 | acc: 56.25%,  total acc: 69.56%   [EVAL] batch:   54 | acc: 68.75%,  total acc: 69.55%   [EVAL] batch:   55 | acc: 37.50%,  total acc: 68.97%   [EVAL] batch:   56 | acc: 56.25%,  total acc: 68.75%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 69.07%   [EVAL] batch:   58 | acc: 68.75%,  total acc: 69.07%   [EVAL] batch:   59 | acc: 75.00%,  total acc: 69.17%   [EVAL] batch:   60 | acc: 68.75%,  total acc: 69.16%   [EVAL] batch:   61 | acc: 75.00%,  total acc: 69.25%   [EVAL] batch:   62 | acc: 37.50%,  total acc: 68.75%   
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 75.00%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 79.17%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 76.56%   [EVAL] batch:    4 | acc: 62.50%,  total acc: 73.75%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 76.04%   [EVAL] batch:    6 | acc: 50.00%,  total acc: 72.32%   [EVAL] batch:    7 | acc: 68.75%,  total acc: 71.88%   [EVAL] batch:    8 | acc: 56.25%,  total acc: 70.14%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 69.38%   [EVAL] batch:   10 | acc: 50.00%,  total acc: 67.61%   [EVAL] batch:   11 | acc: 50.00%,  total acc: 66.15%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 66.35%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 68.30%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 70.00%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 71.88%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 73.16%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 74.31%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 75.66%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 76.25%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 77.08%   [EVAL] batch:   21 | acc: 75.00%,  total acc: 76.99%   [EVAL] batch:   22 | acc: 81.25%,  total acc: 77.17%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 77.60%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 78.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 78.85%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 79.63%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 80.36%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 80.82%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 81.46%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 82.06%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 82.62%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 82.95%   [EVAL] batch:   33 | acc: 68.75%,  total acc: 82.54%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 82.32%   [EVAL] batch:   35 | acc: 75.00%,  total acc: 82.12%   [EVAL] batch:   36 | acc: 87.50%,  total acc: 82.26%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 82.40%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 82.85%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 83.28%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 83.54%   [EVAL] batch:   41 | acc: 81.25%,  total acc: 83.48%   [EVAL] batch:   42 | acc: 75.00%,  total acc: 83.28%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 83.52%   [EVAL] batch:   44 | acc: 81.25%,  total acc: 83.47%   [EVAL] batch:   45 | acc: 75.00%,  total acc: 83.29%   [EVAL] batch:   46 | acc: 68.75%,  total acc: 82.98%   [EVAL] batch:   47 | acc: 81.25%,  total acc: 82.94%   [EVAL] batch:   48 | acc: 87.50%,  total acc: 83.04%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 83.12%   [EVAL] batch:   50 | acc: 75.00%,  total acc: 82.97%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 83.05%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 83.25%   [EVAL] batch:   53 | acc: 68.75%,  total acc: 82.99%   [EVAL] batch:   54 | acc: 62.50%,  total acc: 82.61%   [EVAL] batch:   55 | acc: 81.25%,  total acc: 82.59%   [EVAL] batch:   56 | acc: 56.25%,  total acc: 82.13%   [EVAL] batch:   57 | acc: 31.25%,  total acc: 81.25%   [EVAL] batch:   58 | acc: 68.75%,  total acc: 81.04%   [EVAL] batch:   59 | acc: 68.75%,  total acc: 80.83%   [EVAL] batch:   60 | acc: 37.50%,  total acc: 80.12%   [EVAL] batch:   61 | acc: 37.50%,  total acc: 79.44%   [EVAL] batch:   62 | acc: 43.75%,  total acc: 78.87%   [EVAL] batch:   63 | acc: 75.00%,  total acc: 78.81%   [EVAL] batch:   64 | acc: 62.50%,  total acc: 78.56%   [EVAL] batch:   65 | acc: 50.00%,  total acc: 78.12%   [EVAL] batch:   66 | acc: 37.50%,  total acc: 77.52%   [EVAL] batch:   67 | acc: 50.00%,  total acc: 77.11%   [EVAL] batch:   68 | acc: 75.00%,  total acc: 77.08%   [EVAL] batch:   69 | acc: 93.75%,  total acc: 77.32%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 77.64%   [EVAL] batch:   71 | acc: 75.00%,  total acc: 77.60%   [EVAL] batch:   72 | acc: 81.25%,  total acc: 77.65%   [EVAL] batch:   73 | acc: 87.50%,  total acc: 77.79%   [EVAL] batch:   74 | acc: 68.75%,  total acc: 77.67%   [EVAL] batch:   75 | acc: 68.75%,  total acc: 77.55%   [EVAL] batch:   76 | acc: 43.75%,  total acc: 77.11%   [EVAL] batch:   77 | acc: 56.25%,  total acc: 76.84%   [EVAL] batch:   78 | acc: 75.00%,  total acc: 76.82%   [EVAL] batch:   79 | acc: 75.00%,  total acc: 76.80%   [EVAL] batch:   80 | acc: 68.75%,  total acc: 76.70%   [EVAL] batch:   81 | acc: 68.75%,  total acc: 76.60%   [EVAL] batch:   82 | acc: 56.25%,  total acc: 76.36%   [EVAL] batch:   83 | acc: 81.25%,  total acc: 76.41%   [EVAL] batch:   84 | acc: 62.50%,  total acc: 76.25%   [EVAL] batch:   85 | acc: 75.00%,  total acc: 76.24%   [EVAL] batch:   86 | acc: 50.00%,  total acc: 75.93%   [EVAL] batch:   87 | acc: 75.00%,  total acc: 75.92%   [EVAL] batch:   88 | acc: 87.50%,  total acc: 76.05%   [EVAL] batch:   89 | acc: 87.50%,  total acc: 76.18%   [EVAL] batch:   90 | acc: 93.75%,  total acc: 76.37%   [EVAL] batch:   91 | acc: 75.00%,  total acc: 76.36%   [EVAL] batch:   92 | acc: 68.75%,  total acc: 76.28%   [EVAL] batch:   93 | acc: 68.75%,  total acc: 76.20%   [EVAL] batch:   94 | acc: 75.00%,  total acc: 76.18%   [EVAL] batch:   95 | acc: 43.75%,  total acc: 75.85%   [EVAL] batch:   96 | acc: 62.50%,  total acc: 75.71%   [EVAL] batch:   97 | acc: 62.50%,  total acc: 75.57%   [EVAL] batch:   98 | acc: 75.00%,  total acc: 75.57%   [EVAL] batch:   99 | acc: 68.75%,  total acc: 75.50%   [EVAL] batch:  100 | acc: 50.00%,  total acc: 75.25%   [EVAL] batch:  101 | acc: 62.50%,  total acc: 75.12%   [EVAL] batch:  102 | acc: 50.00%,  total acc: 74.88%   [EVAL] batch:  103 | acc: 62.50%,  total acc: 74.76%   [EVAL] batch:  104 | acc: 68.75%,  total acc: 74.70%   [EVAL] batch:  105 | acc: 62.50%,  total acc: 74.59%   [EVAL] batch:  106 | acc: 50.00%,  total acc: 74.36%   [EVAL] batch:  107 | acc: 12.50%,  total acc: 73.78%   [EVAL] batch:  108 | acc: 31.25%,  total acc: 73.39%   [EVAL] batch:  109 | acc: 31.25%,  total acc: 73.01%   [EVAL] batch:  110 | acc: 31.25%,  total acc: 72.64%   [EVAL] batch:  111 | acc: 50.00%,  total acc: 72.43%   [EVAL] batch:  112 | acc: 68.75%,  total acc: 72.40%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 72.64%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 72.88%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 73.11%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 73.34%   [EVAL] batch:  117 | acc: 100.00%,  total acc: 73.57%   [EVAL] batch:  118 | acc: 87.50%,  total acc: 73.69%   [EVAL] batch:  119 | acc: 68.75%,  total acc: 73.65%   [EVAL] batch:  120 | acc: 43.75%,  total acc: 73.40%   [EVAL] batch:  121 | acc: 87.50%,  total acc: 73.51%   [EVAL] batch:  122 | acc: 75.00%,  total acc: 73.53%   [EVAL] batch:  123 | acc: 68.75%,  total acc: 73.49%   [EVAL] batch:  124 | acc: 68.75%,  total acc: 73.45%   [EVAL] batch:  125 | acc: 68.75%,  total acc: 73.41%   [EVAL] batch:  126 | acc: 68.75%,  total acc: 73.38%   [EVAL] batch:  127 | acc: 68.75%,  total acc: 73.34%   [EVAL] batch:  128 | acc: 81.25%,  total acc: 73.40%   [EVAL] batch:  129 | acc: 87.50%,  total acc: 73.51%   [EVAL] batch:  130 | acc: 75.00%,  total acc: 73.52%   [EVAL] batch:  131 | acc: 87.50%,  total acc: 73.63%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 73.83%   [EVAL] batch:  133 | acc: 75.00%,  total acc: 73.83%   [EVAL] batch:  134 | acc: 81.25%,  total acc: 73.89%   [EVAL] batch:  135 | acc: 100.00%,  total acc: 74.08%   [EVAL] batch:  136 | acc: 100.00%,  total acc: 74.27%   [EVAL] batch:  137 | acc: 68.75%,  total acc: 74.23%   [EVAL] batch:  138 | acc: 18.75%,  total acc: 73.83%   [EVAL] batch:  139 | acc: 31.25%,  total acc: 73.53%   [EVAL] batch:  140 | acc: 25.00%,  total acc: 73.18%   [EVAL] batch:  141 | acc: 37.50%,  total acc: 72.93%   [EVAL] batch:  142 | acc: 31.25%,  total acc: 72.64%   [EVAL] batch:  143 | acc: 31.25%,  total acc: 72.35%   [EVAL] batch:  144 | acc: 93.75%,  total acc: 72.50%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 72.69%   [EVAL] batch:  146 | acc: 100.00%,  total acc: 72.87%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 73.06%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 73.24%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 73.38%   [EVAL] batch:  150 | acc: 56.25%,  total acc: 73.26%   [EVAL] batch:  151 | acc: 25.00%,  total acc: 72.94%   [EVAL] batch:  152 | acc: 43.75%,  total acc: 72.75%   [EVAL] batch:  153 | acc: 43.75%,  total acc: 72.56%   [EVAL] batch:  154 | acc: 25.00%,  total acc: 72.26%   [EVAL] batch:  155 | acc: 25.00%,  total acc: 71.96%   [EVAL] batch:  156 | acc: 87.50%,  total acc: 72.05%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 72.19%   [EVAL] batch:  158 | acc: 100.00%,  total acc: 72.37%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 72.50%   [EVAL] batch:  160 | acc: 93.75%,  total acc: 72.63%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 72.76%   [EVAL] batch:  162 | acc: 81.25%,  total acc: 72.81%   [EVAL] batch:  163 | acc: 62.50%,  total acc: 72.75%   [EVAL] batch:  164 | acc: 87.50%,  total acc: 72.84%   [EVAL] batch:  165 | acc: 62.50%,  total acc: 72.78%   [EVAL] batch:  166 | acc: 100.00%,  total acc: 72.94%   [EVAL] batch:  167 | acc: 75.00%,  total acc: 72.95%   [EVAL] batch:  168 | acc: 75.00%,  total acc: 72.97%   [EVAL] batch:  169 | acc: 31.25%,  total acc: 72.72%   [EVAL] batch:  170 | acc: 43.75%,  total acc: 72.55%   [EVAL] batch:  171 | acc: 31.25%,  total acc: 72.31%   [EVAL] batch:  172 | acc: 25.00%,  total acc: 72.04%   [EVAL] batch:  173 | acc: 37.50%,  total acc: 71.84%   [EVAL] batch:  174 | acc: 43.75%,  total acc: 71.68%   [EVAL] batch:  175 | acc: 75.00%,  total acc: 71.70%   [EVAL] batch:  176 | acc: 81.25%,  total acc: 71.75%   [EVAL] batch:  177 | acc: 81.25%,  total acc: 71.80%   [EVAL] batch:  178 | acc: 81.25%,  total acc: 71.86%   [EVAL] batch:  179 | acc: 100.00%,  total acc: 72.01%   [EVAL] batch:  180 | acc: 75.00%,  total acc: 72.03%   [EVAL] batch:  181 | acc: 50.00%,  total acc: 71.91%   [EVAL] batch:  182 | acc: 56.25%,  total acc: 71.82%   [EVAL] batch:  183 | acc: 81.25%,  total acc: 71.88%   [EVAL] batch:  184 | acc: 62.50%,  total acc: 71.82%   [EVAL] batch:  185 | acc: 56.25%,  total acc: 71.74%   [EVAL] batch:  186 | acc: 37.50%,  total acc: 71.56%   [EVAL] batch:  187 | acc: 81.25%,  total acc: 71.61%   [EVAL] batch:  188 | acc: 93.75%,  total acc: 71.73%   [EVAL] batch:  189 | acc: 87.50%,  total acc: 71.81%   [EVAL] batch:  190 | acc: 100.00%,  total acc: 71.96%   [EVAL] batch:  191 | acc: 81.25%,  total acc: 72.01%   [EVAL] batch:  192 | acc: 100.00%,  total acc: 72.15%   [EVAL] batch:  193 | acc: 81.25%,  total acc: 72.20%   [EVAL] batch:  194 | acc: 31.25%,  total acc: 71.99%   [EVAL] batch:  195 | acc: 25.00%,  total acc: 71.75%   [EVAL] batch:  196 | acc: 25.00%,  total acc: 71.51%   [EVAL] batch:  197 | acc: 25.00%,  total acc: 71.28%   [EVAL] batch:  198 | acc: 25.00%,  total acc: 71.04%   [EVAL] batch:  199 | acc: 18.75%,  total acc: 70.78%   [EVAL] batch:  200 | acc: 75.00%,  total acc: 70.80%   [EVAL] batch:  201 | acc: 87.50%,  total acc: 70.88%   [EVAL] batch:  202 | acc: 81.25%,  total acc: 70.94%   [EVAL] batch:  203 | acc: 56.25%,  total acc: 70.86%   [EVAL] batch:  204 | acc: 75.00%,  total acc: 70.88%   [EVAL] batch:  205 | acc: 68.75%,  total acc: 70.87%   [EVAL] batch:  206 | acc: 56.25%,  total acc: 70.80%   [EVAL] batch:  207 | acc: 25.00%,  total acc: 70.58%   [EVAL] batch:  208 | acc: 18.75%,  total acc: 70.33%   [EVAL] batch:  209 | acc: 25.00%,  total acc: 70.12%   [EVAL] batch:  210 | acc: 43.75%,  total acc: 69.99%   [EVAL] batch:  211 | acc: 18.75%,  total acc: 69.75%   [EVAL] batch:  212 | acc: 62.50%,  total acc: 69.72%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 69.86%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 70.00%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 70.14%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 70.28%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 70.41%   [EVAL] batch:  218 | acc: 93.75%,  total acc: 70.52%   [EVAL] batch:  219 | acc: 50.00%,  total acc: 70.43%   [EVAL] batch:  220 | acc: 68.75%,  total acc: 70.42%   [EVAL] batch:  221 | acc: 56.25%,  total acc: 70.35%   [EVAL] batch:  222 | acc: 56.25%,  total acc: 70.29%   [EVAL] batch:  223 | acc: 81.25%,  total acc: 70.34%   [EVAL] batch:  224 | acc: 31.25%,  total acc: 70.17%   [EVAL] batch:  225 | acc: 100.00%,  total acc: 70.30%   [EVAL] batch:  226 | acc: 100.00%,  total acc: 70.43%   [EVAL] batch:  227 | acc: 100.00%,  total acc: 70.56%   [EVAL] batch:  228 | acc: 100.00%,  total acc: 70.69%   [EVAL] batch:  229 | acc: 100.00%,  total acc: 70.82%   [EVAL] batch:  230 | acc: 87.50%,  total acc: 70.89%   [EVAL] batch:  231 | acc: 100.00%,  total acc: 71.01%   [EVAL] batch:  232 | acc: 100.00%,  total acc: 71.14%   [EVAL] batch:  233 | acc: 100.00%,  total acc: 71.26%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 71.38%   [EVAL] batch:  235 | acc: 100.00%,  total acc: 71.50%   [EVAL] batch:  236 | acc: 93.75%,  total acc: 71.60%   [EVAL] batch:  237 | acc: 100.00%,  total acc: 71.72%   [EVAL] batch:  238 | acc: 87.50%,  total acc: 71.78%   [EVAL] batch:  239 | acc: 93.75%,  total acc: 71.88%   [EVAL] batch:  240 | acc: 100.00%,  total acc: 71.99%   [EVAL] batch:  241 | acc: 93.75%,  total acc: 72.08%   [EVAL] batch:  242 | acc: 93.75%,  total acc: 72.17%   [EVAL] batch:  243 | acc: 87.50%,  total acc: 72.23%   [EVAL] batch:  244 | acc: 100.00%,  total acc: 72.35%   [EVAL] batch:  245 | acc: 62.50%,  total acc: 72.31%   [EVAL] batch:  246 | acc: 87.50%,  total acc: 72.37%   [EVAL] batch:  247 | acc: 87.50%,  total acc: 72.43%   [EVAL] batch:  248 | acc: 93.75%,  total acc: 72.52%   [EVAL] batch:  249 | acc: 87.50%,  total acc: 72.58%   [EVAL] batch:  250 | acc: 25.00%,  total acc: 72.39%   [EVAL] batch:  251 | acc: 37.50%,  total acc: 72.25%   [EVAL] batch:  252 | acc: 37.50%,  total acc: 72.11%   [EVAL] batch:  253 | acc: 18.75%,  total acc: 71.90%   [EVAL] batch:  254 | acc: 37.50%,  total acc: 71.76%   [EVAL] batch:  255 | acc: 37.50%,  total acc: 71.63%   [EVAL] batch:  256 | acc: 50.00%,  total acc: 71.55%   [EVAL] batch:  257 | acc: 81.25%,  total acc: 71.58%   [EVAL] batch:  258 | acc: 68.75%,  total acc: 71.57%   [EVAL] batch:  259 | acc: 62.50%,  total acc: 71.54%   [EVAL] batch:  260 | acc: 43.75%,  total acc: 71.43%   [EVAL] batch:  261 | acc: 68.75%,  total acc: 71.42%   [EVAL] batch:  262 | acc: 87.50%,  total acc: 71.48%   [EVAL] batch:  263 | acc: 50.00%,  total acc: 71.40%   [EVAL] batch:  264 | acc: 62.50%,  total acc: 71.37%   [EVAL] batch:  265 | acc: 75.00%,  total acc: 71.38%   [EVAL] batch:  266 | acc: 75.00%,  total acc: 71.40%   [EVAL] batch:  267 | acc: 68.75%,  total acc: 71.39%   [EVAL] batch:  268 | acc: 93.75%,  total acc: 71.47%   [EVAL] batch:  269 | acc: 93.75%,  total acc: 71.55%   [EVAL] batch:  270 | acc: 100.00%,  total acc: 71.66%   [EVAL] batch:  271 | acc: 100.00%,  total acc: 71.76%   [EVAL] batch:  272 | acc: 100.00%,  total acc: 71.86%   [EVAL] batch:  273 | acc: 93.75%,  total acc: 71.94%   [EVAL] batch:  274 | acc: 100.00%,  total acc: 72.05%   [EVAL] batch:  275 | acc: 50.00%,  total acc: 71.97%   [EVAL] batch:  276 | acc: 37.50%,  total acc: 71.84%   [EVAL] batch:  277 | acc: 56.25%,  total acc: 71.79%   [EVAL] batch:  278 | acc: 50.00%,  total acc: 71.71%   [EVAL] batch:  279 | acc: 50.00%,  total acc: 71.63%   [EVAL] batch:  280 | acc: 50.00%,  total acc: 71.55%   [EVAL] batch:  281 | acc: 56.25%,  total acc: 71.50%   [EVAL] batch:  282 | acc: 62.50%,  total acc: 71.47%   [EVAL] batch:  283 | acc: 25.00%,  total acc: 71.30%   [EVAL] batch:  284 | acc: 37.50%,  total acc: 71.18%   [EVAL] batch:  285 | acc: 25.00%,  total acc: 71.02%   [EVAL] batch:  286 | acc: 18.75%,  total acc: 70.84%   [EVAL] batch:  287 | acc: 62.50%,  total acc: 70.81%   [EVAL] batch:  288 | acc: 62.50%,  total acc: 70.78%   [EVAL] batch:  289 | acc: 81.25%,  total acc: 70.82%   [EVAL] batch:  290 | acc: 75.00%,  total acc: 70.83%   [EVAL] batch:  291 | acc: 81.25%,  total acc: 70.87%   [EVAL] batch:  292 | acc: 75.00%,  total acc: 70.88%   [EVAL] batch:  293 | acc: 81.25%,  total acc: 70.92%   [EVAL] batch:  294 | acc: 68.75%,  total acc: 70.91%   [EVAL] batch:  295 | acc: 81.25%,  total acc: 70.95%   [EVAL] batch:  296 | acc: 75.00%,  total acc: 70.96%   [EVAL] batch:  297 | acc: 75.00%,  total acc: 70.97%   [EVAL] batch:  298 | acc: 81.25%,  total acc: 71.01%   [EVAL] batch:  299 | acc: 81.25%,  total acc: 71.04%   [EVAL] batch:  300 | acc: 100.00%,  total acc: 71.14%   [EVAL] batch:  301 | acc: 100.00%,  total acc: 71.23%   [EVAL] batch:  302 | acc: 100.00%,  total acc: 71.33%   [EVAL] batch:  303 | acc: 100.00%,  total acc: 71.42%   [EVAL] batch:  304 | acc: 100.00%,  total acc: 71.52%   [EVAL] batch:  305 | acc: 100.00%,  total acc: 71.61%   [EVAL] batch:  306 | acc: 100.00%,  total acc: 71.70%   [EVAL] batch:  307 | acc: 87.50%,  total acc: 71.75%   [EVAL] batch:  308 | acc: 93.75%,  total acc: 71.82%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 71.92%   [EVAL] batch:  310 | acc: 93.75%,  total acc: 71.99%   [EVAL] batch:  311 | acc: 100.00%,  total acc: 72.08%   [EVAL] batch:  312 | acc: 68.75%,  total acc: 72.06%   [EVAL] batch:  313 | acc: 12.50%,  total acc: 71.88%   [EVAL] batch:  314 | acc: 31.25%,  total acc: 71.75%   [EVAL] batch:  315 | acc: 31.25%,  total acc: 71.62%   [EVAL] batch:  316 | acc: 31.25%,  total acc: 71.49%   [EVAL] batch:  317 | acc: 18.75%,  total acc: 71.32%   [EVAL] batch:  318 | acc: 25.00%,  total acc: 71.18%   [EVAL] batch:  319 | acc: 75.00%,  total acc: 71.19%   [EVAL] batch:  320 | acc: 50.00%,  total acc: 71.13%   [EVAL] batch:  321 | acc: 81.25%,  total acc: 71.16%   [EVAL] batch:  322 | acc: 93.75%,  total acc: 71.23%   [EVAL] batch:  323 | acc: 75.00%,  total acc: 71.24%   [EVAL] batch:  324 | acc: 75.00%,  total acc: 71.25%   [EVAL] batch:  325 | acc: 75.00%,  total acc: 71.26%   [EVAL] batch:  326 | acc: 50.00%,  total acc: 71.20%   [EVAL] batch:  327 | acc: 50.00%,  total acc: 71.13%   [EVAL] batch:  328 | acc: 68.75%,  total acc: 71.12%   [EVAL] batch:  329 | acc: 62.50%,  total acc: 71.10%   [EVAL] batch:  330 | acc: 75.00%,  total acc: 71.11%   [EVAL] batch:  331 | acc: 87.50%,  total acc: 71.16%   [EVAL] batch:  332 | acc: 100.00%,  total acc: 71.25%   [EVAL] batch:  333 | acc: 100.00%,  total acc: 71.33%   [EVAL] batch:  334 | acc: 93.75%,  total acc: 71.40%   [EVAL] batch:  335 | acc: 100.00%,  total acc: 71.48%   [EVAL] batch:  336 | acc: 100.00%,  total acc: 71.57%   [EVAL] batch:  337 | acc: 75.00%,  total acc: 71.58%   [EVAL] batch:  338 | acc: 100.00%,  total acc: 71.66%   [EVAL] batch:  339 | acc: 93.75%,  total acc: 71.73%   [EVAL] batch:  340 | acc: 100.00%,  total acc: 71.81%   [EVAL] batch:  341 | acc: 87.50%,  total acc: 71.86%   [EVAL] batch:  342 | acc: 100.00%,  total acc: 71.94%   [EVAL] batch:  343 | acc: 87.50%,  total acc: 71.98%   [EVAL] batch:  344 | acc: 100.00%,  total acc: 72.07%   [EVAL] batch:  345 | acc: 100.00%,  total acc: 72.15%   [EVAL] batch:  346 | acc: 87.50%,  total acc: 72.19%   [EVAL] batch:  347 | acc: 93.75%,  total acc: 72.25%   [EVAL] batch:  348 | acc: 93.75%,  total acc: 72.31%   [EVAL] batch:  349 | acc: 100.00%,  total acc: 72.39%   [EVAL] batch:  350 | acc: 25.00%,  total acc: 72.26%   [EVAL] batch:  351 | acc: 12.50%,  total acc: 72.09%   [EVAL] batch:  352 | acc: 6.25%,  total acc: 71.90%   [EVAL] batch:  353 | acc: 25.00%,  total acc: 71.77%   [EVAL] batch:  354 | acc: 25.00%,  total acc: 71.64%   [EVAL] batch:  355 | acc: 25.00%,  total acc: 71.51%   [EVAL] batch:  356 | acc: 87.50%,  total acc: 71.55%   [EVAL] batch:  357 | acc: 93.75%,  total acc: 71.61%   [EVAL] batch:  358 | acc: 100.00%,  total acc: 71.69%   [EVAL] batch:  359 | acc: 93.75%,  total acc: 71.75%   [EVAL] batch:  360 | acc: 87.50%,  total acc: 71.80%   [EVAL] batch:  361 | acc: 87.50%,  total acc: 71.84%   [EVAL] batch:  362 | acc: 68.75%,  total acc: 71.83%   [EVAL] batch:  363 | acc: 68.75%,  total acc: 71.82%   [EVAL] batch:  364 | acc: 62.50%,  total acc: 71.80%   [EVAL] batch:  365 | acc: 56.25%,  total acc: 71.76%   [EVAL] batch:  366 | acc: 68.75%,  total acc: 71.75%   [EVAL] batch:  367 | acc: 56.25%,  total acc: 71.71%   [EVAL] batch:  368 | acc: 37.50%,  total acc: 71.61%   [EVAL] batch:  369 | acc: 81.25%,  total acc: 71.64%   [EVAL] batch:  370 | acc: 75.00%,  total acc: 71.65%   [EVAL] batch:  371 | acc: 62.50%,  total acc: 71.62%   [EVAL] batch:  372 | acc: 81.25%,  total acc: 71.65%   [EVAL] batch:  373 | acc: 81.25%,  total acc: 71.67%   [EVAL] batch:  374 | acc: 62.50%,  total acc: 71.65%   
cur_acc:  ['0.9494', '0.7302', '0.7500', '0.7956', '0.6925', '0.6875']
his_acc:  ['0.9494', '0.8310', '0.7969', '0.7835', '0.7592', '0.7165']
Clustering into  34  clusters
Clusters:  [29  5  8  1 20 13  8  4  5 16  6 19 32 20 33 16  0 13  7 31  0 25  2  3
  2  9  4 11 22  1  1 15  3  5 23 26 12  9 30 12 28 24 27  8 19 17  5 26
 18  0 17  7  6 14 11  7  6  0 16  0 20 25  4 21  2 10  4  5  4  5]
Losses:  14.117819786071777 6.388128757476807 0.4916221797466278
CurrentTrain: epoch  0, batch     0 | loss: 14.1178198Losses:  10.239487648010254 3.161738395690918 0.6722503900527954
CurrentTrain: epoch  0, batch     1 | loss: 10.2394876Losses:  11.892251014709473 5.459240913391113 0.47707685828208923
CurrentTrain: epoch  0, batch     2 | loss: 11.8922510Losses:  6.553762912750244 -0.0 0.22094392776489258
CurrentTrain: epoch  0, batch     3 | loss: 6.5537629Losses:  9.811002731323242 4.5998077392578125 0.3090376853942871
CurrentTrain: epoch  1, batch     0 | loss: 9.8110027Losses:  11.433084487915039 4.815863132476807 0.6418567895889282
CurrentTrain: epoch  1, batch     1 | loss: 11.4330845Losses:  8.895545959472656 2.9553186893463135 0.6212266087532043
CurrentTrain: epoch  1, batch     2 | loss: 8.8955460Losses:  6.930980682373047 -0.0 0.11237125843763351
CurrentTrain: epoch  1, batch     3 | loss: 6.9309807Losses:  7.671422481536865 2.70170259475708 0.6020406484603882
CurrentTrain: epoch  2, batch     0 | loss: 7.6714225Losses:  8.976584434509277 3.2028884887695312 0.6462680101394653
CurrentTrain: epoch  2, batch     1 | loss: 8.9765844Losses:  8.32924747467041 3.2351646423339844 0.6100255846977234
CurrentTrain: epoch  2, batch     2 | loss: 8.3292475Losses:  3.9611141681671143 -0.0 0.10895629972219467
CurrentTrain: epoch  2, batch     3 | loss: 3.9611142Losses:  7.646093845367432 2.797731637954712 0.5750671625137329
CurrentTrain: epoch  3, batch     0 | loss: 7.6460938Losses:  8.06820011138916 3.7940378189086914 0.5105831623077393
CurrentTrain: epoch  3, batch     1 | loss: 8.0682001Losses:  8.942296981811523 4.031440734863281 0.5283488631248474
CurrentTrain: epoch  3, batch     2 | loss: 8.9422970Losses:  2.939237356185913 -0.0 0.1374809741973877
CurrentTrain: epoch  3, batch     3 | loss: 2.9392374Losses:  9.483797073364258 5.785678863525391 0.4240799844264984
CurrentTrain: epoch  4, batch     0 | loss: 9.4837971Losses:  8.69326400756836 4.696291923522949 0.5390856862068176
CurrentTrain: epoch  4, batch     1 | loss: 8.6932640Losses:  9.289506912231445 4.1850690841674805 0.49733781814575195
CurrentTrain: epoch  4, batch     2 | loss: 9.2895069Losses:  3.48982834815979 -0.0 0.12953734397888184
CurrentTrain: epoch  4, batch     3 | loss: 3.4898283Losses:  8.4146089553833 4.179671764373779 0.6076838970184326
CurrentTrain: epoch  5, batch     0 | loss: 8.4146090Losses:  7.033265113830566 3.2473976612091064 0.6387642621994019
CurrentTrain: epoch  5, batch     1 | loss: 7.0332651Losses:  7.4592390060424805 3.4867396354675293 0.42949414253234863
CurrentTrain: epoch  5, batch     2 | loss: 7.4592390Losses:  2.5941073894500732 -0.0 -0.0
CurrentTrain: epoch  5, batch     3 | loss: 2.5941074Losses:  8.93556022644043 4.217511177062988 0.43283912539482117
CurrentTrain: epoch  6, batch     0 | loss: 8.9355602Losses:  5.957960605621338 2.9844584465026855 0.479120671749115
CurrentTrain: epoch  6, batch     1 | loss: 5.9579606Losses:  6.747739315032959 2.928049087524414 0.549502968788147
CurrentTrain: epoch  6, batch     2 | loss: 6.7477393Losses:  2.5594115257263184 -0.0 0.10408984124660492
CurrentTrain: epoch  6, batch     3 | loss: 2.5594115Losses:  7.978582382202148 4.290545463562012 0.5491696000099182
CurrentTrain: epoch  7, batch     0 | loss: 7.9785824Losses:  6.504901885986328 2.9095540046691895 0.6161050200462341
CurrentTrain: epoch  7, batch     1 | loss: 6.5049019Losses:  7.323589324951172 3.6346495151519775 0.4928380846977234
CurrentTrain: epoch  7, batch     2 | loss: 7.3235893Losses:  2.6443819999694824 -0.0 0.1001385897397995
CurrentTrain: epoch  7, batch     3 | loss: 2.6443820Losses:  6.007957458496094 3.3840832710266113 0.39589864015579224
CurrentTrain: epoch  8, batch     0 | loss: 6.0079575Losses:  6.3490214347839355 2.855196475982666 0.5778878331184387
CurrentTrain: epoch  8, batch     1 | loss: 6.3490214Losses:  6.3560943603515625 2.950841188430786 0.5671684145927429
CurrentTrain: epoch  8, batch     2 | loss: 6.3560944Losses:  4.328064441680908 -0.0 0.11338595300912857
CurrentTrain: epoch  8, batch     3 | loss: 4.3280644Losses:  5.866669654846191 2.495903968811035 0.5821951627731323
CurrentTrain: epoch  9, batch     0 | loss: 5.8666697Losses:  5.674734592437744 3.082695960998535 0.47799190878868103
CurrentTrain: epoch  9, batch     1 | loss: 5.6747346Losses:  5.204616069793701 2.033330202102661 0.5151323080062866
CurrentTrain: epoch  9, batch     2 | loss: 5.2046161Losses:  3.8005950450897217 -0.0 0.11451608687639236
CurrentTrain: epoch  9, batch     3 | loss: 3.8005950
#### located on terrain feature ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "located on terrain feature" means located on the specified physical/geophysical feature.
Here is an example:
Relation: located on terrain feature
Context: it is situated in the historic lower lusatia region , about south of beeskow , and north of cottbus .
Head Entity: cottbus
Tail Entity: lower lusatia
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "located on terrain feature":
Relation: located on terrain feature  
Context: The ancient ruins are found on the slopes of the majestic Andes mountains, providing a breathtaking view of the valley below.  
Head Entity: Andes mountains  
Tail Entity: ancient ruins  

Relation: located on terrain feature  
Context: The village is nestled in the heart of the lush Black Forest, surrounded by towering trees and serene lakes.  
Head Entity: Black Forest  
Tail Entity: village  
#### participant of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "participant of" means event in which a person or organization was/is a participant.
Here is an example:
Relation: participant of
Context: tiago venâncio ( born august 19 , 1987 , setúbal ) is a freestyle swimmer from portugal , who represented his home country at the 2004 summer olympics in athens , greece .
Head Entity: tiago venâncio
Tail Entity: 2004 summer olympics
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "participant of":
Relation: participant of  
Context: the united nations climate change conference, held in paris in 2015, saw numerous world leaders and activists come together to discuss global warming.  
Head Entity: united nations climate change conference  
Tail Entity: paris agreement  

Relation: participant of  
Context: the 2022 fifa world cup in qatar featured teams from around the globe competing for the prestigious trophy.  
Head Entity: 2022 fifa world cup  
Tail Entity: qatar
#### director ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "director" means director of film, TV-series, stageplay, video game or similar.
Here is an example:
Relation: director
Context: the festival opened with " whiplash " directed by damien chazelle and closed with musical drama " rudderless " directed by william h. macy .
Head Entity: whiplash
Tail Entity: damien chazelle
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "director":
Relation: director  
Context: the critically acclaimed series "breaking bad" was brought to life by director vince gilligan, while the spin-off "better call saul" was directed by peter gould.  
Head Entity: breaking bad  
Tail Entity: vince gilligan  

Relation: director  
Context: the animated feature "spider-man: into the spider-verse" was directed by bob persichetti, peter ramsey, and rodney rothman, showcasing a unique visual style.  
Head Entity: spider-man: into the spider-verse  
Tail Entity: bob persichetti  
#### owned by ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "owned by" means owner of the subject.
Here is an example:
Relation: owned by
Context: as of april 2013 , sony isp , known as so - net , released a new fibre service to tokyo for small businesses and homes and also made it available to six surrounding prefectures .
Head Entity: so - net
Tail Entity: sony
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "owned by":
Relation: owned by  
Context: In 2020, the popular video game developer, Bungie, announced that it had become an independent studio, having previously been owned by Microsoft for several years.  
Head Entity: Bungie  
Tail Entity: Microsoft  

Relation: owned by  
Context: The luxury fashion brand Gucci, known for its high-end products, is owned by the French conglomerate Kering, which also owns several other prestigious labels.  
Head Entity: Gucci  
Tail Entity: Kering  
#### part of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "part of" means object of which the subject is a part.
Here is an example:
Relation: part of
Context: thessaly was home to extensive neolithic and chalcolithic cultures around 6000 bc–2500 bc ( see cardium pottery , dimini and sesklo ) .
Head Entity: cardium pottery
Tail Entity: neolithic
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "part of":
Relation: part of  
Context: The engine is a crucial part of the car's overall functionality, providing the necessary power to drive.  
Head Entity: engine  
Tail Entity: car  

Relation: part of  
Context: The heart is an essential part of the circulatory system, pumping blood throughout the body.  
Head Entity: heart  
Tail Entity: circulatory system  
#### occupant ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "occupant" means person or organization occupying property.
Here is an example:
Relation: occupant
Context: it served as the home of k league side gyeongnam fc and national league side changwon city fc prior to the construction of the changwon football center in 2009 .
Head Entity: changwon football center
Tail Entity: gyeongnam fc
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "occupant":
Relation: occupant  
Context: The historic building was once the residence of the famous artist, and now it serves as a gallery showcasing his works, attracting visitors from all over the world.  
Head Entity: historic building  
Tail Entity: famous artist  

Relation: occupant  
Context: After the renovation, the old factory was transformed into a vibrant co-working space, where numerous startups and freelancers now thrive.  
Head Entity: old factory  
Tail Entity: startups
#### architect ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "architect" means person or architectural firm responsible for designing this building.
Here is an example:
Relation: architect
Context: reading town hall was built in several phases between 1786 and 1897 , although the principal facade was designed by alfred waterhouse in 1875 .
Head Entity: reading town hall
Tail Entity: alfred waterhouse
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "architect":
Relation: architect  
Context: the new art museum was designed by renowned architect zaha hadid, whose innovative style has transformed modern architecture.  
Head Entity: new art museum  
Tail Entity: zaha hadid  

Relation: architect  
Context: the iconic sydney opera house was the brainchild of architect jørn utzon, who won the design competition in 1957.  
Head Entity: sydney opera house  
Tail Entity: jørn utzon  
#### residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "residence" means the place where the person is or has been, resident.
Here is an example:
Relation: residence
Context: in 2001 brian haw set up the parliament square peace campaign outside the houses of parliament in london .
Head Entity: brian haw
Tail Entity: london
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "residence":
Relation: residence  
Context: After moving to New York City in 2010, Sarah began her career as a graphic designer while living in Brooklyn.  
Head Entity: Sarah  
Tail Entity: Brooklyn  

Relation: residence  
Context: The famous author spent many years living in a quaint cottage in the countryside of England, where he found inspiration for his novels.  
Head Entity: The famous author  
Tail Entity: England  
#### composer ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "composer" means person who wrote the music.
Here is an example:
Relation: composer
Context: at a young age he was an accomplished cellist , performing at age seventeen the solo part of richard strauss 's " don quixote " .
Head Entity: don quixote
Tail Entity: richard strauss
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "composer":
Relation: composer  
Context: The renowned composer Ludwig van Beethoven created some of the most celebrated symphonies in classical music history.  
Head Entity: symphonies  
Tail Entity: Ludwig van Beethoven  

Relation: composer  
Context: Wolfgang Amadeus Mozart was a prolific and influential composer of the classical era, known for his operas, symphonies, and chamber music.  
Head Entity: operas  
Tail Entity: Wolfgang Amadeus Mozart  
#### location ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "location" means location of the object, structure or event.
Here is an example:
Relation: location
Context: one polish archer qualified for the women 's individual recurve by obtaining one of the eight olympic places available from the 2015 world archery championships in copenhagen , denmark .
Head Entity: 2015 world archery championships
Tail Entity: copenhagen
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "location":
Relation: location  
Context: The annual music festival will take place in the vibrant city of Austin, Texas, attracting thousands of fans from around the world.  
Head Entity: annual music festival  
Tail Entity: Austin, Texas  

Relation: location  
Context: The historic battle was fought near the small town of Gettysburg, Pennsylvania, which is now a popular tourist destination.  
Head Entity: historic battle  
Tail Entity: Gettysburg, Pennsylvania  
Losses:  3.4037656784057617 0.2488093376159668 0.9100503921508789
MemoryTrain:  epoch  0, batch     0 | loss: 3.4037657Losses:  4.116605758666992 0.2698541283607483 1.064731478691101
MemoryTrain:  epoch  0, batch     1 | loss: 4.1166058Losses:  3.318979263305664 0.24105575680732727 0.9149984121322632
MemoryTrain:  epoch  0, batch     2 | loss: 3.3189793Losses:  4.297855854034424 0.5295612812042236 0.9378904700279236
MemoryTrain:  epoch  0, batch     3 | loss: 4.2978559Losses:  3.503873348236084 -0.0 1.0681531429290771
MemoryTrain:  epoch  0, batch     4 | loss: 3.5038733Losses:  3.5729942321777344 -0.0 0.997884213924408
MemoryTrain:  epoch  0, batch     5 | loss: 3.5729942Losses:  4.053315162658691 -0.0 0.9096109867095947
MemoryTrain:  epoch  0, batch     6 | loss: 4.0533152Losses:  3.8376851081848145 0.25470995903015137 0.9733554124832153
MemoryTrain:  epoch  0, batch     7 | loss: 3.8376851Losses:  3.6241464614868164 -0.0 0.9160753488540649
MemoryTrain:  epoch  0, batch     8 | loss: 3.6241465Losses:  4.249786376953125 -0.0 1.0861295461654663
MemoryTrain:  epoch  0, batch     9 | loss: 4.2497864Losses:  3.866863250732422 0.2584139108657837 0.9842420220375061
MemoryTrain:  epoch  0, batch    10 | loss: 3.8668633Losses:  4.446982383728027 0.2461438775062561 0.9088742733001709
MemoryTrain:  epoch  0, batch    11 | loss: 4.4469824Losses:  3.7048697471618652 0.7710279226303101 0.7773037552833557
MemoryTrain:  epoch  0, batch    12 | loss: 3.7048697Losses:  6.869817733764648 -0.0 0.1535196453332901
MemoryTrain:  epoch  0, batch    13 | loss: 6.8698177Losses:  3.9577531814575195 0.3784095048904419 0.9961997270584106
MemoryTrain:  epoch  1, batch     0 | loss: 3.9577532Losses:  2.983654499053955 -0.0 0.9735032916069031
MemoryTrain:  epoch  1, batch     1 | loss: 2.9836545Losses:  3.6475911140441895 0.24822326004505157 0.9883185625076294
MemoryTrain:  epoch  1, batch     2 | loss: 3.6475911Losses:  3.3031604290008545 -0.0 1.1451404094696045
MemoryTrain:  epoch  1, batch     3 | loss: 3.3031604Losses:  4.137911796569824 0.2381710410118103 0.8977551460266113
MemoryTrain:  epoch  1, batch     4 | loss: 4.1379118Losses:  3.143343925476074 -0.0 0.9871859550476074
MemoryTrain:  epoch  1, batch     5 | loss: 3.1433439Losses:  3.659421920776367 0.46999692916870117 0.9005822539329529
MemoryTrain:  epoch  1, batch     6 | loss: 3.6594219Losses:  4.198868274688721 0.25605201721191406 0.8097431063652039
MemoryTrain:  epoch  1, batch     7 | loss: 4.1988683Losses:  3.543541431427002 0.24717977643013 1.0302964448928833
MemoryTrain:  epoch  1, batch     8 | loss: 3.5435414Losses:  2.5291640758514404 -0.0 0.880519449710846
MemoryTrain:  epoch  1, batch     9 | loss: 2.5291641Losses:  3.1299962997436523 0.24822917580604553 0.9620258808135986
MemoryTrain:  epoch  1, batch    10 | loss: 3.1299963Losses:  3.986639976501465 0.28532612323760986 0.9776211380958557
MemoryTrain:  epoch  1, batch    11 | loss: 3.9866400Losses:  2.7164549827575684 0.22547009587287903 0.9043752551078796
MemoryTrain:  epoch  1, batch    12 | loss: 2.7164550Losses:  1.5540225505828857 -0.0 0.1123524010181427
MemoryTrain:  epoch  1, batch    13 | loss: 1.5540226Losses:  3.5254321098327637 0.2718973755836487 0.9281822443008423
MemoryTrain:  epoch  2, batch     0 | loss: 3.5254321Losses:  3.010714054107666 -0.0 1.000649094581604
MemoryTrain:  epoch  2, batch     1 | loss: 3.0107141Losses:  3.2330822944641113 0.4966621398925781 1.0159833431243896
MemoryTrain:  epoch  2, batch     2 | loss: 3.2330823Losses:  3.0360963344573975 -0.0 0.9123556613922119
MemoryTrain:  epoch  2, batch     3 | loss: 3.0360963Losses:  3.4638354778289795 0.5480263233184814 0.9246509075164795
MemoryTrain:  epoch  2, batch     4 | loss: 3.4638355Losses:  3.6708803176879883 -0.0 1.0816432237625122
MemoryTrain:  epoch  2, batch     5 | loss: 3.6708803Losses:  2.96848726272583 -0.0 1.0997419357299805
MemoryTrain:  epoch  2, batch     6 | loss: 2.9684873Losses:  2.984312057495117 0.7500201463699341 0.5752118229866028
MemoryTrain:  epoch  2, batch     7 | loss: 2.9843121Losses:  3.0326945781707764 0.24295775592327118 0.7844576239585876
MemoryTrain:  epoch  2, batch     8 | loss: 3.0326946Losses:  3.387568473815918 0.22645819187164307 0.9200429916381836
MemoryTrain:  epoch  2, batch     9 | loss: 3.3875685Losses:  3.0854578018188477 -0.0 0.9793302416801453
MemoryTrain:  epoch  2, batch    10 | loss: 3.0854578Losses:  2.7060933113098145 -0.0 0.9257326126098633
MemoryTrain:  epoch  2, batch    11 | loss: 2.7060933Losses:  3.6216444969177246 0.28429847955703735 0.8382295370101929
MemoryTrain:  epoch  2, batch    12 | loss: 3.6216445Losses:  2.5977251529693604 -0.0 0.12554530799388885
MemoryTrain:  epoch  2, batch    13 | loss: 2.5977252Losses:  3.5313377380371094 0.5194770097732544 0.9109501242637634
MemoryTrain:  epoch  3, batch     0 | loss: 3.5313377Losses:  3.208019495010376 0.5324188470840454 0.8978459239006042
MemoryTrain:  epoch  3, batch     1 | loss: 3.2080195Losses:  3.7439792156219482 0.5288052558898926 0.9820196032524109
MemoryTrain:  epoch  3, batch     2 | loss: 3.7439792Losses:  3.163149356842041 -0.0 0.9912477731704712
MemoryTrain:  epoch  3, batch     3 | loss: 3.1631494Losses:  2.751211166381836 0.5093180537223816 0.9216526746749878
MemoryTrain:  epoch  3, batch     4 | loss: 2.7512112Losses:  2.8019309043884277 0.2390553057193756 0.9008899331092834
MemoryTrain:  epoch  3, batch     5 | loss: 2.8019309Losses:  2.834482192993164 -0.0 0.8932579755783081
MemoryTrain:  epoch  3, batch     6 | loss: 2.8344822Losses:  2.5127224922180176 0.23947036266326904 0.9725200533866882
MemoryTrain:  epoch  3, batch     7 | loss: 2.5127225Losses:  3.266103744506836 0.2814551591873169 1.034206748008728
MemoryTrain:  epoch  3, batch     8 | loss: 3.2661037Losses:  2.8252506256103516 0.5217316150665283 0.8521461486816406
MemoryTrain:  epoch  3, batch     9 | loss: 2.8252506Losses:  3.7305455207824707 0.2556626498699188 0.9833629727363586
MemoryTrain:  epoch  3, batch    10 | loss: 3.7305455Losses:  3.5198001861572266 0.26188501715660095 1.0135058164596558
MemoryTrain:  epoch  3, batch    11 | loss: 3.5198002Losses:  2.963153123855591 0.2620270252227783 0.9569684863090515
MemoryTrain:  epoch  3, batch    12 | loss: 2.9631531Losses:  2.595003843307495 -0.0 0.10411277413368225
MemoryTrain:  epoch  3, batch    13 | loss: 2.5950038Losses:  3.7839879989624023 0.26539063453674316 0.9049032926559448
MemoryTrain:  epoch  4, batch     0 | loss: 3.7839880Losses:  3.088465452194214 0.2625821828842163 0.9280974864959717
MemoryTrain:  epoch  4, batch     1 | loss: 3.0884655Losses:  2.9583544731140137 0.28406190872192383 0.9312883615493774
MemoryTrain:  epoch  4, batch     2 | loss: 2.9583545Losses:  2.640934944152832 0.47387176752090454 0.8358350396156311
MemoryTrain:  epoch  4, batch     3 | loss: 2.6409349Losses:  3.3792669773101807 0.5394574403762817 0.9307243227958679
MemoryTrain:  epoch  4, batch     4 | loss: 3.3792670Losses:  2.8149921894073486 0.24175137281417847 1.0361511707305908
MemoryTrain:  epoch  4, batch     5 | loss: 2.8149922Losses:  2.8719706535339355 0.708964467048645 0.9589359760284424
MemoryTrain:  epoch  4, batch     6 | loss: 2.8719707Losses:  3.9610373973846436 0.7522549033164978 0.8576169013977051
MemoryTrain:  epoch  4, batch     7 | loss: 3.9610374Losses:  3.0196614265441895 0.25637003779411316 1.0875046253204346
MemoryTrain:  epoch  4, batch     8 | loss: 3.0196614Losses:  3.4553403854370117 0.2648623585700989 0.917698085308075
MemoryTrain:  epoch  4, batch     9 | loss: 3.4553404Losses:  2.858726739883423 -0.0 1.0107548236846924
MemoryTrain:  epoch  4, batch    10 | loss: 2.8587267Losses:  2.974886417388916 0.506977915763855 0.8273534774780273
MemoryTrain:  epoch  4, batch    11 | loss: 2.9748864Losses:  2.558748960494995 0.2325153946876526 0.9848348498344421
MemoryTrain:  epoch  4, batch    12 | loss: 2.5587490Losses:  1.5550155639648438 -0.0 0.1730196326971054
MemoryTrain:  epoch  4, batch    13 | loss: 1.5550156Losses:  2.2917518615722656 -0.0 0.9724385738372803
MemoryTrain:  epoch  5, batch     0 | loss: 2.2917519Losses:  2.848597526550293 -0.0 0.8820613622665405
MemoryTrain:  epoch  5, batch     1 | loss: 2.8485975Losses:  2.8701372146606445 0.4939335584640503 0.9142769575119019
MemoryTrain:  epoch  5, batch     2 | loss: 2.8701372Losses:  2.9198198318481445 0.2588997185230255 0.956487774848938
MemoryTrain:  epoch  5, batch     3 | loss: 2.9198198Losses:  2.5970096588134766 0.2376243770122528 0.9043959379196167
MemoryTrain:  epoch  5, batch     4 | loss: 2.5970097Losses:  2.7171623706817627 0.2666892111301422 0.9253839254379272
MemoryTrain:  epoch  5, batch     5 | loss: 2.7171624Losses:  2.797410249710083 0.5086995959281921 0.9844911098480225
MemoryTrain:  epoch  5, batch     6 | loss: 2.7974102Losses:  3.3456523418426514 0.2463688850402832 0.9549496173858643
MemoryTrain:  epoch  5, batch     7 | loss: 3.3456523Losses:  3.0792179107666016 0.5578685402870178 0.9668976068496704
MemoryTrain:  epoch  5, batch     8 | loss: 3.0792179Losses:  2.55474853515625 0.2512536346912384 0.931541919708252
MemoryTrain:  epoch  5, batch     9 | loss: 2.5547485Losses:  3.4531219005584717 0.4795595109462738 0.8353086113929749
MemoryTrain:  epoch  5, batch    10 | loss: 3.4531219Losses:  3.6173291206359863 0.25869765877723694 0.9452177882194519
MemoryTrain:  epoch  5, batch    11 | loss: 3.6173291Losses:  3.4516658782958984 -0.0 0.9962300062179565
MemoryTrain:  epoch  5, batch    12 | loss: 3.4516659Losses:  1.2449618577957153 -0.0 0.10484163463115692
MemoryTrain:  epoch  5, batch    13 | loss: 1.2449619Losses:  3.0562474727630615 0.47917282581329346 0.8099595904350281
MemoryTrain:  epoch  6, batch     0 | loss: 3.0562475Losses:  3.413853645324707 0.25255194306373596 1.0452715158462524
MemoryTrain:  epoch  6, batch     1 | loss: 3.4138536Losses:  3.45340895652771 0.552993893623352 0.8398911356925964
MemoryTrain:  epoch  6, batch     2 | loss: 3.4534090Losses:  2.659785747528076 0.4745110869407654 0.9498358964920044
MemoryTrain:  epoch  6, batch     3 | loss: 2.6597857Losses:  2.6181812286376953 -0.0 0.9315816760063171
MemoryTrain:  epoch  6, batch     4 | loss: 2.6181812Losses:  2.9624571800231934 0.7241405248641968 0.8474287986755371
MemoryTrain:  epoch  6, batch     5 | loss: 2.9624572Losses:  2.4707465171813965 0.26276448369026184 0.8100758790969849
MemoryTrain:  epoch  6, batch     6 | loss: 2.4707465Losses:  2.964148759841919 0.2649264335632324 0.9659451246261597
MemoryTrain:  epoch  6, batch     7 | loss: 2.9641488Losses:  2.869234085083008 0.262241929769516 0.9525878429412842
MemoryTrain:  epoch  6, batch     8 | loss: 2.8692341Losses:  2.9907212257385254 -0.0 0.9160716533660889
MemoryTrain:  epoch  6, batch     9 | loss: 2.9907212Losses:  2.439535140991211 0.24195948243141174 0.9504384398460388
MemoryTrain:  epoch  6, batch    10 | loss: 2.4395351Losses:  2.605668067932129 0.24433466792106628 0.981537938117981
MemoryTrain:  epoch  6, batch    11 | loss: 2.6056681Losses:  2.741438388824463 -0.0 1.0161434412002563
MemoryTrain:  epoch  6, batch    12 | loss: 2.7414384Losses:  1.2791993618011475 -0.0 0.11911940574645996
MemoryTrain:  epoch  6, batch    13 | loss: 1.2791994Losses:  2.9110450744628906 0.2493220567703247 0.780449390411377
MemoryTrain:  epoch  7, batch     0 | loss: 2.9110451Losses:  2.459620475769043 -0.0 1.003111720085144
MemoryTrain:  epoch  7, batch     1 | loss: 2.4596205Losses:  2.491218328475952 0.25195518136024475 0.8353732824325562
MemoryTrain:  epoch  7, batch     2 | loss: 2.4912183Losses:  2.978048801422119 0.5104435682296753 0.9841099381446838
MemoryTrain:  epoch  7, batch     3 | loss: 2.9780488Losses:  2.6514129638671875 0.4660557508468628 0.9327396154403687
MemoryTrain:  epoch  7, batch     4 | loss: 2.6514130Losses:  2.2551026344299316 -0.0 0.9547544717788696
MemoryTrain:  epoch  7, batch     5 | loss: 2.2551026Losses:  2.6888837814331055 0.5025070905685425 0.8698351979255676
MemoryTrain:  epoch  7, batch     6 | loss: 2.6888838Losses:  2.529261589050293 0.2390684336423874 0.9125165939331055
MemoryTrain:  epoch  7, batch     7 | loss: 2.5292616Losses:  3.206904888153076 0.5166743397712708 0.9218112826347351
MemoryTrain:  epoch  7, batch     8 | loss: 3.2069049Losses:  2.527254581451416 -0.0 0.8562735319137573
MemoryTrain:  epoch  7, batch     9 | loss: 2.5272546Losses:  2.917667865753174 -0.0 1.0361698865890503
MemoryTrain:  epoch  7, batch    10 | loss: 2.9176679Losses:  3.0210561752319336 0.8223568201065063 0.7963669896125793
MemoryTrain:  epoch  7, batch    11 | loss: 3.0210562Losses:  4.196813106536865 1.247840404510498 0.9759655594825745
MemoryTrain:  epoch  7, batch    12 | loss: 4.1968131Losses:  1.3988516330718994 -0.0 0.13082505762577057
MemoryTrain:  epoch  7, batch    13 | loss: 1.3988516Losses:  2.764930248260498 0.26021426916122437 0.9216765761375427
MemoryTrain:  epoch  8, batch     0 | loss: 2.7649302Losses:  2.71934175491333 0.2145828902721405 0.970790684223175
MemoryTrain:  epoch  8, batch     1 | loss: 2.7193418Losses:  2.734050750732422 0.27289333939552307 1.0236833095550537
MemoryTrain:  epoch  8, batch     2 | loss: 2.7340508Losses:  2.558820962905884 0.25801026821136475 0.8570131063461304
MemoryTrain:  epoch  8, batch     3 | loss: 2.5588210Losses:  3.0407140254974365 0.23895230889320374 1.0652168989181519
MemoryTrain:  epoch  8, batch     4 | loss: 3.0407140Losses:  2.594691753387451 0.23149311542510986 1.0573934316635132
MemoryTrain:  epoch  8, batch     5 | loss: 2.5946918Losses:  3.661670207977295 0.6509203910827637 0.9126211404800415
MemoryTrain:  epoch  8, batch     6 | loss: 3.6616702Losses:  2.2801437377929688 -0.0 0.9646350145339966
MemoryTrain:  epoch  8, batch     7 | loss: 2.2801437Losses:  2.495265007019043 -0.0 1.0054476261138916
MemoryTrain:  epoch  8, batch     8 | loss: 2.4952650Losses:  2.530827760696411 0.48922938108444214 0.7896487712860107
MemoryTrain:  epoch  8, batch     9 | loss: 2.5308278Losses:  2.86865234375 0.5404284000396729 0.9743888974189758
MemoryTrain:  epoch  8, batch    10 | loss: 2.8686523Losses:  2.8122620582580566 0.26209303736686707 0.9152411222457886
MemoryTrain:  epoch  8, batch    11 | loss: 2.8122621Losses:  2.632012367248535 0.504849374294281 0.8894848823547363
MemoryTrain:  epoch  8, batch    12 | loss: 2.6320124Losses:  1.5377527475357056 -0.0 0.09679661691188812
MemoryTrain:  epoch  8, batch    13 | loss: 1.5377527Losses:  3.1691315174102783 0.5016481280326843 0.9237964153289795
MemoryTrain:  epoch  9, batch     0 | loss: 3.1691315Losses:  3.868450880050659 1.2021324634552002 0.9294425845146179
MemoryTrain:  epoch  9, batch     1 | loss: 3.8684509Losses:  2.5169386863708496 -0.0 1.0744409561157227
MemoryTrain:  epoch  9, batch     2 | loss: 2.5169387Losses:  2.7940335273742676 0.5046765208244324 0.8756622076034546
MemoryTrain:  epoch  9, batch     3 | loss: 2.7940335Losses:  2.9200267791748047 0.383023738861084 0.8757827281951904
MemoryTrain:  epoch  9, batch     4 | loss: 2.9200268Losses:  2.2970640659332275 -0.0 0.9657054543495178
MemoryTrain:  epoch  9, batch     5 | loss: 2.2970641Losses:  2.293814182281494 -0.0 1.01008141040802
MemoryTrain:  epoch  9, batch     6 | loss: 2.2938142Losses:  2.5752387046813965 0.47490763664245605 0.9059715270996094
MemoryTrain:  epoch  9, batch     7 | loss: 2.5752387Losses:  2.4244930744171143 0.2898332476615906 0.7784598469734192
MemoryTrain:  epoch  9, batch     8 | loss: 2.4244931Losses:  2.676609516143799 0.27201372385025024 1.0945038795471191
MemoryTrain:  epoch  9, batch     9 | loss: 2.6766095Losses:  2.2453970909118652 -0.0 0.9780706167221069
MemoryTrain:  epoch  9, batch    10 | loss: 2.2453971Losses:  2.7435059547424316 -0.0 0.9954936504364014
MemoryTrain:  epoch  9, batch    11 | loss: 2.7435060Losses:  2.556736707687378 0.2676382064819336 0.8046676516532898
MemoryTrain:  epoch  9, batch    12 | loss: 2.5567367Losses:  1.517767310142517 -0.0 0.14756226539611816
MemoryTrain:  epoch  9, batch    13 | loss: 1.5177673
[EVAL] batch:    0 | acc: 37.50%,  total acc: 37.50%   [EVAL] batch:    1 | acc: 50.00%,  total acc: 43.75%   [EVAL] batch:    2 | acc: 18.75%,  total acc: 35.42%   [EVAL] batch:    3 | acc: 37.50%,  total acc: 35.94%   [EVAL] batch:    4 | acc: 18.75%,  total acc: 32.50%   [EVAL] batch:    5 | acc: 25.00%,  total acc: 31.25%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 36.61%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 41.41%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 47.92%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 51.25%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 54.55%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 56.25%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 58.65%   [EVAL] batch:   13 | acc: 62.50%,  total acc: 58.93%   [EVAL] batch:   14 | acc: 81.25%,  total acc: 60.42%   [EVAL] batch:   15 | acc: 75.00%,  total acc: 61.33%   [EVAL] batch:   16 | acc: 87.50%,  total acc: 62.87%   [EVAL] batch:   17 | acc: 87.50%,  total acc: 64.24%   [EVAL] batch:   18 | acc: 50.00%,  total acc: 63.49%   [EVAL] batch:   19 | acc: 31.25%,  total acc: 61.88%   [EVAL] batch:   20 | acc: 31.25%,  total acc: 60.42%   [EVAL] batch:   21 | acc: 37.50%,  total acc: 59.38%   [EVAL] batch:   22 | acc: 62.50%,  total acc: 59.51%   [EVAL] batch:   23 | acc: 31.25%,  total acc: 58.33%   [EVAL] batch:   24 | acc: 56.25%,  total acc: 58.25%   [EVAL] batch:   25 | acc: 12.50%,  total acc: 56.49%   [EVAL] batch:   26 | acc: 12.50%,  total acc: 54.86%   [EVAL] batch:   27 | acc: 6.25%,  total acc: 53.12%   [EVAL] batch:   28 | acc: 0.00%,  total acc: 51.29%   [EVAL] batch:   29 | acc: 6.25%,  total acc: 49.79%   [EVAL] batch:   30 | acc: 6.25%,  total acc: 48.39%   [EVAL] batch:   31 | acc: 62.50%,  total acc: 48.83%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 50.19%   [EVAL] batch:   33 | acc: 87.50%,  total acc: 51.29%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 52.68%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 53.82%   [EVAL] batch:   36 | acc: 81.25%,  total acc: 54.56%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 55.76%   [EVAL] batch:   38 | acc: 87.50%,  total acc: 56.57%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 57.66%   [EVAL] batch:   40 | acc: 87.50%,  total acc: 58.38%   [EVAL] batch:   41 | acc: 81.25%,  total acc: 58.93%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 59.74%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 60.51%   [EVAL] batch:   44 | acc: 37.50%,  total acc: 60.00%   [EVAL] batch:   45 | acc: 12.50%,  total acc: 58.97%   [EVAL] batch:   46 | acc: 31.25%,  total acc: 58.38%   [EVAL] batch:   47 | acc: 25.00%,  total acc: 57.68%   [EVAL] batch:   48 | acc: 43.75%,  total acc: 57.40%   [EVAL] batch:   49 | acc: 50.00%,  total acc: 57.25%   [EVAL] batch:   50 | acc: 68.75%,  total acc: 57.48%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 58.05%   [EVAL] batch:   52 | acc: 75.00%,  total acc: 58.37%   [EVAL] batch:   53 | acc: 87.50%,  total acc: 58.91%   [EVAL] batch:   54 | acc: 75.00%,  total acc: 59.20%   [EVAL] batch:   55 | acc: 81.25%,  total acc: 59.60%   [EVAL] batch:   56 | acc: 50.00%,  total acc: 59.43%   [EVAL] batch:   57 | acc: 56.25%,  total acc: 59.38%   [EVAL] batch:   58 | acc: 37.50%,  total acc: 59.00%   [EVAL] batch:   59 | acc: 43.75%,  total acc: 58.75%   [EVAL] batch:   60 | acc: 43.75%,  total acc: 58.50%   [EVAL] batch:   61 | acc: 81.25%,  total acc: 58.87%   [EVAL] batch:   62 | acc: 18.75%,  total acc: 58.23%   
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    1 | acc: 75.00%,  total acc: 68.75%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 72.92%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 71.88%   [EVAL] batch:    4 | acc: 62.50%,  total acc: 70.00%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 70.83%   [EVAL] batch:    6 | acc: 37.50%,  total acc: 66.07%   [EVAL] batch:    7 | acc: 62.50%,  total acc: 65.62%   [EVAL] batch:    8 | acc: 37.50%,  total acc: 62.50%   [EVAL] batch:    9 | acc: 56.25%,  total acc: 61.88%   [EVAL] batch:   10 | acc: 37.50%,  total acc: 59.66%   [EVAL] batch:   11 | acc: 31.25%,  total acc: 57.29%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 58.17%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 60.71%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 62.92%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 65.23%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 67.28%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 68.75%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 70.39%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 70.94%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 72.02%   [EVAL] batch:   21 | acc: 68.75%,  total acc: 71.88%   [EVAL] batch:   22 | acc: 87.50%,  total acc: 72.55%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 73.18%   [EVAL] batch:   24 | acc: 81.25%,  total acc: 73.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 74.52%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 75.46%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 76.34%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 76.94%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 77.71%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 78.43%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 79.10%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 79.55%   [EVAL] batch:   33 | acc: 62.50%,  total acc: 79.04%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 78.93%   [EVAL] batch:   35 | acc: 81.25%,  total acc: 78.99%   [EVAL] batch:   36 | acc: 81.25%,  total acc: 79.05%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 79.28%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 79.81%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 80.31%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 80.64%   [EVAL] batch:   41 | acc: 81.25%,  total acc: 80.65%   [EVAL] batch:   42 | acc: 75.00%,  total acc: 80.52%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 80.68%   [EVAL] batch:   44 | acc: 81.25%,  total acc: 80.69%   [EVAL] batch:   45 | acc: 87.50%,  total acc: 80.84%   [EVAL] batch:   46 | acc: 68.75%,  total acc: 80.59%   [EVAL] batch:   47 | acc: 87.50%,  total acc: 80.73%   [EVAL] batch:   48 | acc: 81.25%,  total acc: 80.74%   [EVAL] batch:   49 | acc: 75.00%,  total acc: 80.62%   [EVAL] batch:   50 | acc: 75.00%,  total acc: 80.51%   [EVAL] batch:   51 | acc: 81.25%,  total acc: 80.53%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 80.78%   [EVAL] batch:   53 | acc: 68.75%,  total acc: 80.56%   [EVAL] batch:   54 | acc: 50.00%,  total acc: 80.00%   [EVAL] batch:   55 | acc: 81.25%,  total acc: 80.02%   [EVAL] batch:   56 | acc: 56.25%,  total acc: 79.61%   [EVAL] batch:   57 | acc: 37.50%,  total acc: 78.88%   [EVAL] batch:   58 | acc: 62.50%,  total acc: 78.60%   [EVAL] batch:   59 | acc: 68.75%,  total acc: 78.44%   [EVAL] batch:   60 | acc: 37.50%,  total acc: 77.77%   [EVAL] batch:   61 | acc: 50.00%,  total acc: 77.32%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 76.88%   [EVAL] batch:   63 | acc: 75.00%,  total acc: 76.86%   [EVAL] batch:   64 | acc: 81.25%,  total acc: 76.92%   [EVAL] batch:   65 | acc: 56.25%,  total acc: 76.61%   [EVAL] batch:   66 | acc: 50.00%,  total acc: 76.21%   [EVAL] batch:   67 | acc: 68.75%,  total acc: 76.10%   [EVAL] batch:   68 | acc: 68.75%,  total acc: 76.00%   [EVAL] batch:   69 | acc: 93.75%,  total acc: 76.25%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 76.58%   [EVAL] batch:   71 | acc: 62.50%,  total acc: 76.39%   [EVAL] batch:   72 | acc: 81.25%,  total acc: 76.46%   [EVAL] batch:   73 | acc: 81.25%,  total acc: 76.52%   [EVAL] batch:   74 | acc: 75.00%,  total acc: 76.50%   [EVAL] batch:   75 | acc: 62.50%,  total acc: 76.32%   [EVAL] batch:   76 | acc: 25.00%,  total acc: 75.65%   [EVAL] batch:   77 | acc: 50.00%,  total acc: 75.32%   [EVAL] batch:   78 | acc: 62.50%,  total acc: 75.16%   [EVAL] batch:   79 | acc: 50.00%,  total acc: 74.84%   [EVAL] batch:   80 | acc: 62.50%,  total acc: 74.69%   [EVAL] batch:   81 | acc: 62.50%,  total acc: 74.54%   [EVAL] batch:   82 | acc: 62.50%,  total acc: 74.40%   [EVAL] batch:   83 | acc: 87.50%,  total acc: 74.55%   [EVAL] batch:   84 | acc: 68.75%,  total acc: 74.49%   [EVAL] batch:   85 | acc: 75.00%,  total acc: 74.49%   [EVAL] batch:   86 | acc: 50.00%,  total acc: 74.21%   [EVAL] batch:   87 | acc: 87.50%,  total acc: 74.36%   [EVAL] batch:   88 | acc: 87.50%,  total acc: 74.51%   [EVAL] batch:   89 | acc: 68.75%,  total acc: 74.44%   [EVAL] batch:   90 | acc: 81.25%,  total acc: 74.52%   [EVAL] batch:   91 | acc: 81.25%,  total acc: 74.59%   [EVAL] batch:   92 | acc: 75.00%,  total acc: 74.60%   [EVAL] batch:   93 | acc: 68.75%,  total acc: 74.53%   [EVAL] batch:   94 | acc: 75.00%,  total acc: 74.54%   [EVAL] batch:   95 | acc: 43.75%,  total acc: 74.22%   [EVAL] batch:   96 | acc: 62.50%,  total acc: 74.10%   [EVAL] batch:   97 | acc: 50.00%,  total acc: 73.85%   [EVAL] batch:   98 | acc: 68.75%,  total acc: 73.80%   [EVAL] batch:   99 | acc: 50.00%,  total acc: 73.56%   [EVAL] batch:  100 | acc: 50.00%,  total acc: 73.33%   [EVAL] batch:  101 | acc: 62.50%,  total acc: 73.22%   [EVAL] batch:  102 | acc: 50.00%,  total acc: 73.00%   [EVAL] batch:  103 | acc: 62.50%,  total acc: 72.90%   [EVAL] batch:  104 | acc: 68.75%,  total acc: 72.86%   [EVAL] batch:  105 | acc: 62.50%,  total acc: 72.76%   [EVAL] batch:  106 | acc: 43.75%,  total acc: 72.49%   [EVAL] batch:  107 | acc: 25.00%,  total acc: 72.05%   [EVAL] batch:  108 | acc: 37.50%,  total acc: 71.73%   [EVAL] batch:  109 | acc: 37.50%,  total acc: 71.42%   [EVAL] batch:  110 | acc: 31.25%,  total acc: 71.06%   [EVAL] batch:  111 | acc: 50.00%,  total acc: 70.87%   [EVAL] batch:  112 | acc: 68.75%,  total acc: 70.85%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 71.11%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 71.36%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 71.61%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 71.85%   [EVAL] batch:  117 | acc: 100.00%,  total acc: 72.09%   [EVAL] batch:  118 | acc: 93.75%,  total acc: 72.27%   [EVAL] batch:  119 | acc: 75.00%,  total acc: 72.29%   [EVAL] batch:  120 | acc: 43.75%,  total acc: 72.06%   [EVAL] batch:  121 | acc: 81.25%,  total acc: 72.13%   [EVAL] batch:  122 | acc: 68.75%,  total acc: 72.10%   [EVAL] batch:  123 | acc: 68.75%,  total acc: 72.08%   [EVAL] batch:  124 | acc: 75.00%,  total acc: 72.10%   [EVAL] batch:  125 | acc: 68.75%,  total acc: 72.07%   [EVAL] batch:  126 | acc: 68.75%,  total acc: 72.05%   [EVAL] batch:  127 | acc: 62.50%,  total acc: 71.97%   [EVAL] batch:  128 | acc: 75.00%,  total acc: 72.00%   [EVAL] batch:  129 | acc: 87.50%,  total acc: 72.12%   [EVAL] batch:  130 | acc: 68.75%,  total acc: 72.09%   [EVAL] batch:  131 | acc: 81.25%,  total acc: 72.16%   [EVAL] batch:  132 | acc: 87.50%,  total acc: 72.27%   [EVAL] batch:  133 | acc: 68.75%,  total acc: 72.25%   [EVAL] batch:  134 | acc: 81.25%,  total acc: 72.31%   [EVAL] batch:  135 | acc: 93.75%,  total acc: 72.47%   [EVAL] batch:  136 | acc: 81.25%,  total acc: 72.54%   [EVAL] batch:  137 | acc: 50.00%,  total acc: 72.37%   [EVAL] batch:  138 | acc: 18.75%,  total acc: 71.99%   [EVAL] batch:  139 | acc: 25.00%,  total acc: 71.65%   [EVAL] batch:  140 | acc: 25.00%,  total acc: 71.32%   [EVAL] batch:  141 | acc: 43.75%,  total acc: 71.13%   [EVAL] batch:  142 | acc: 31.25%,  total acc: 70.85%   [EVAL] batch:  143 | acc: 31.25%,  total acc: 70.57%   [EVAL] batch:  144 | acc: 93.75%,  total acc: 70.73%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 70.93%   [EVAL] batch:  146 | acc: 100.00%,  total acc: 71.13%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 71.33%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 71.52%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 71.67%   [EVAL] batch:  150 | acc: 50.00%,  total acc: 71.52%   [EVAL] batch:  151 | acc: 31.25%,  total acc: 71.26%   [EVAL] batch:  152 | acc: 31.25%,  total acc: 71.00%   [EVAL] batch:  153 | acc: 50.00%,  total acc: 70.86%   [EVAL] batch:  154 | acc: 37.50%,  total acc: 70.65%   [EVAL] batch:  155 | acc: 37.50%,  total acc: 70.43%   [EVAL] batch:  156 | acc: 81.25%,  total acc: 70.50%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 70.65%   [EVAL] batch:  158 | acc: 93.75%,  total acc: 70.79%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 70.94%   [EVAL] batch:  160 | acc: 87.50%,  total acc: 71.04%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 71.18%   [EVAL] batch:  162 | acc: 56.25%,  total acc: 71.09%   [EVAL] batch:  163 | acc: 18.75%,  total acc: 70.77%   [EVAL] batch:  164 | acc: 25.00%,  total acc: 70.49%   [EVAL] batch:  165 | acc: 0.00%,  total acc: 70.07%   [EVAL] batch:  166 | acc: 6.25%,  total acc: 69.69%   [EVAL] batch:  167 | acc: 18.75%,  total acc: 69.38%   [EVAL] batch:  168 | acc: 37.50%,  total acc: 69.19%   [EVAL] batch:  169 | acc: 37.50%,  total acc: 69.01%   [EVAL] batch:  170 | acc: 37.50%,  total acc: 68.82%   [EVAL] batch:  171 | acc: 43.75%,  total acc: 68.68%   [EVAL] batch:  172 | acc: 31.25%,  total acc: 68.46%   [EVAL] batch:  173 | acc: 37.50%,  total acc: 68.28%   [EVAL] batch:  174 | acc: 56.25%,  total acc: 68.21%   [EVAL] batch:  175 | acc: 75.00%,  total acc: 68.25%   [EVAL] batch:  176 | acc: 68.75%,  total acc: 68.26%   [EVAL] batch:  177 | acc: 75.00%,  total acc: 68.29%   [EVAL] batch:  178 | acc: 81.25%,  total acc: 68.37%   [EVAL] batch:  179 | acc: 81.25%,  total acc: 68.44%   [EVAL] batch:  180 | acc: 87.50%,  total acc: 68.54%   [EVAL] batch:  181 | acc: 43.75%,  total acc: 68.41%   [EVAL] batch:  182 | acc: 62.50%,  total acc: 68.37%   [EVAL] batch:  183 | acc: 75.00%,  total acc: 68.41%   [EVAL] batch:  184 | acc: 50.00%,  total acc: 68.31%   [EVAL] batch:  185 | acc: 56.25%,  total acc: 68.25%   [EVAL] batch:  186 | acc: 43.75%,  total acc: 68.11%   [EVAL] batch:  187 | acc: 81.25%,  total acc: 68.18%   [EVAL] batch:  188 | acc: 87.50%,  total acc: 68.29%   [EVAL] batch:  189 | acc: 87.50%,  total acc: 68.39%   [EVAL] batch:  190 | acc: 87.50%,  total acc: 68.49%   [EVAL] batch:  191 | acc: 75.00%,  total acc: 68.52%   [EVAL] batch:  192 | acc: 100.00%,  total acc: 68.69%   [EVAL] batch:  193 | acc: 81.25%,  total acc: 68.75%   [EVAL] batch:  194 | acc: 31.25%,  total acc: 68.56%   [EVAL] batch:  195 | acc: 25.00%,  total acc: 68.34%   [EVAL] batch:  196 | acc: 18.75%,  total acc: 68.08%   [EVAL] batch:  197 | acc: 25.00%,  total acc: 67.87%   [EVAL] batch:  198 | acc: 25.00%,  total acc: 67.65%   [EVAL] batch:  199 | acc: 18.75%,  total acc: 67.41%   [EVAL] batch:  200 | acc: 81.25%,  total acc: 67.48%   [EVAL] batch:  201 | acc: 75.00%,  total acc: 67.51%   [EVAL] batch:  202 | acc: 81.25%,  total acc: 67.58%   [EVAL] batch:  203 | acc: 50.00%,  total acc: 67.49%   [EVAL] batch:  204 | acc: 75.00%,  total acc: 67.53%   [EVAL] batch:  205 | acc: 75.00%,  total acc: 67.57%   [EVAL] batch:  206 | acc: 56.25%,  total acc: 67.51%   [EVAL] batch:  207 | acc: 18.75%,  total acc: 67.28%   [EVAL] batch:  208 | acc: 6.25%,  total acc: 66.99%   [EVAL] batch:  209 | acc: 43.75%,  total acc: 66.88%   [EVAL] batch:  210 | acc: 37.50%,  total acc: 66.74%   [EVAL] batch:  211 | acc: 18.75%,  total acc: 66.51%   [EVAL] batch:  212 | acc: 56.25%,  total acc: 66.46%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 66.62%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 66.77%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 66.93%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 67.08%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 67.23%   [EVAL] batch:  218 | acc: 93.75%,  total acc: 67.35%   [EVAL] batch:  219 | acc: 68.75%,  total acc: 67.36%   [EVAL] batch:  220 | acc: 87.50%,  total acc: 67.45%   [EVAL] batch:  221 | acc: 50.00%,  total acc: 67.37%   [EVAL] batch:  222 | acc: 56.25%,  total acc: 67.32%   [EVAL] batch:  223 | acc: 81.25%,  total acc: 67.38%   [EVAL] batch:  224 | acc: 37.50%,  total acc: 67.25%   [EVAL] batch:  225 | acc: 100.00%,  total acc: 67.39%   [EVAL] batch:  226 | acc: 100.00%,  total acc: 67.54%   [EVAL] batch:  227 | acc: 100.00%,  total acc: 67.68%   [EVAL] batch:  228 | acc: 100.00%,  total acc: 67.82%   [EVAL] batch:  229 | acc: 100.00%,  total acc: 67.96%   [EVAL] batch:  230 | acc: 87.50%,  total acc: 68.05%   [EVAL] batch:  231 | acc: 93.75%,  total acc: 68.16%   [EVAL] batch:  232 | acc: 100.00%,  total acc: 68.29%   [EVAL] batch:  233 | acc: 100.00%,  total acc: 68.43%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 68.56%   [EVAL] batch:  235 | acc: 100.00%,  total acc: 68.70%   [EVAL] batch:  236 | acc: 93.75%,  total acc: 68.80%   [EVAL] batch:  237 | acc: 100.00%,  total acc: 68.93%   [EVAL] batch:  238 | acc: 87.50%,  total acc: 69.01%   [EVAL] batch:  239 | acc: 93.75%,  total acc: 69.11%   [EVAL] batch:  240 | acc: 100.00%,  total acc: 69.24%   [EVAL] batch:  241 | acc: 93.75%,  total acc: 69.34%   [EVAL] batch:  242 | acc: 93.75%,  total acc: 69.44%   [EVAL] batch:  243 | acc: 93.75%,  total acc: 69.54%   [EVAL] batch:  244 | acc: 100.00%,  total acc: 69.67%   [EVAL] batch:  245 | acc: 87.50%,  total acc: 69.74%   [EVAL] batch:  246 | acc: 100.00%,  total acc: 69.86%   [EVAL] batch:  247 | acc: 93.75%,  total acc: 69.96%   [EVAL] batch:  248 | acc: 93.75%,  total acc: 70.06%   [EVAL] batch:  249 | acc: 87.50%,  total acc: 70.12%   [EVAL] batch:  250 | acc: 25.00%,  total acc: 69.95%   [EVAL] batch:  251 | acc: 68.75%,  total acc: 69.94%   [EVAL] batch:  252 | acc: 37.50%,  total acc: 69.81%   [EVAL] batch:  253 | acc: 25.00%,  total acc: 69.64%   [EVAL] batch:  254 | acc: 50.00%,  total acc: 69.56%   [EVAL] batch:  255 | acc: 50.00%,  total acc: 69.48%   [EVAL] batch:  256 | acc: 56.25%,  total acc: 69.43%   [EVAL] batch:  257 | acc: 75.00%,  total acc: 69.45%   [EVAL] batch:  258 | acc: 68.75%,  total acc: 69.45%   [EVAL] batch:  259 | acc: 62.50%,  total acc: 69.42%   [EVAL] batch:  260 | acc: 50.00%,  total acc: 69.35%   [EVAL] batch:  261 | acc: 68.75%,  total acc: 69.35%   [EVAL] batch:  262 | acc: 81.25%,  total acc: 69.39%   [EVAL] batch:  263 | acc: 62.50%,  total acc: 69.37%   [EVAL] batch:  264 | acc: 62.50%,  total acc: 69.34%   [EVAL] batch:  265 | acc: 62.50%,  total acc: 69.31%   [EVAL] batch:  266 | acc: 68.75%,  total acc: 69.31%   [EVAL] batch:  267 | acc: 75.00%,  total acc: 69.33%   [EVAL] batch:  268 | acc: 100.00%,  total acc: 69.45%   [EVAL] batch:  269 | acc: 87.50%,  total acc: 69.51%   [EVAL] batch:  270 | acc: 100.00%,  total acc: 69.63%   [EVAL] batch:  271 | acc: 100.00%,  total acc: 69.74%   [EVAL] batch:  272 | acc: 100.00%,  total acc: 69.85%   [EVAL] batch:  273 | acc: 93.75%,  total acc: 69.94%   [EVAL] batch:  274 | acc: 100.00%,  total acc: 70.05%   [EVAL] batch:  275 | acc: 50.00%,  total acc: 69.97%   [EVAL] batch:  276 | acc: 43.75%,  total acc: 69.88%   [EVAL] batch:  277 | acc: 62.50%,  total acc: 69.85%   [EVAL] batch:  278 | acc: 43.75%,  total acc: 69.76%   [EVAL] batch:  279 | acc: 56.25%,  total acc: 69.71%   [EVAL] batch:  280 | acc: 50.00%,  total acc: 69.64%   [EVAL] batch:  281 | acc: 56.25%,  total acc: 69.59%   [EVAL] batch:  282 | acc: 43.75%,  total acc: 69.50%   [EVAL] batch:  283 | acc: 18.75%,  total acc: 69.32%   [EVAL] batch:  284 | acc: 25.00%,  total acc: 69.17%   [EVAL] batch:  285 | acc: 25.00%,  total acc: 69.01%   [EVAL] batch:  286 | acc: 12.50%,  total acc: 68.82%   [EVAL] batch:  287 | acc: 43.75%,  total acc: 68.73%   [EVAL] batch:  288 | acc: 31.25%,  total acc: 68.60%   [EVAL] batch:  289 | acc: 37.50%,  total acc: 68.49%   [EVAL] batch:  290 | acc: 62.50%,  total acc: 68.47%   [EVAL] batch:  291 | acc: 50.00%,  total acc: 68.41%   [EVAL] batch:  292 | acc: 62.50%,  total acc: 68.39%   [EVAL] batch:  293 | acc: 68.75%,  total acc: 68.39%   [EVAL] batch:  294 | acc: 68.75%,  total acc: 68.39%   [EVAL] batch:  295 | acc: 75.00%,  total acc: 68.41%   [EVAL] batch:  296 | acc: 62.50%,  total acc: 68.39%   [EVAL] batch:  297 | acc: 62.50%,  total acc: 68.37%   [EVAL] batch:  298 | acc: 81.25%,  total acc: 68.42%   [EVAL] batch:  299 | acc: 81.25%,  total acc: 68.46%   [EVAL] batch:  300 | acc: 100.00%,  total acc: 68.56%   [EVAL] batch:  301 | acc: 100.00%,  total acc: 68.67%   [EVAL] batch:  302 | acc: 100.00%,  total acc: 68.77%   [EVAL] batch:  303 | acc: 100.00%,  total acc: 68.87%   [EVAL] batch:  304 | acc: 100.00%,  total acc: 68.98%   [EVAL] batch:  305 | acc: 100.00%,  total acc: 69.08%   [EVAL] batch:  306 | acc: 100.00%,  total acc: 69.18%   [EVAL] batch:  307 | acc: 87.50%,  total acc: 69.24%   [EVAL] batch:  308 | acc: 87.50%,  total acc: 69.30%   [EVAL] batch:  309 | acc: 93.75%,  total acc: 69.38%   [EVAL] batch:  310 | acc: 93.75%,  total acc: 69.45%   [EVAL] batch:  311 | acc: 100.00%,  total acc: 69.55%   [EVAL] batch:  312 | acc: 81.25%,  total acc: 69.59%   [EVAL] batch:  313 | acc: 37.50%,  total acc: 69.49%   [EVAL] batch:  314 | acc: 43.75%,  total acc: 69.40%   [EVAL] batch:  315 | acc: 31.25%,  total acc: 69.28%   [EVAL] batch:  316 | acc: 37.50%,  total acc: 69.18%   [EVAL] batch:  317 | acc: 37.50%,  total acc: 69.08%   [EVAL] batch:  318 | acc: 43.75%,  total acc: 69.00%   [EVAL] batch:  319 | acc: 75.00%,  total acc: 69.02%   [EVAL] batch:  320 | acc: 50.00%,  total acc: 68.96%   [EVAL] batch:  321 | acc: 81.25%,  total acc: 69.00%   [EVAL] batch:  322 | acc: 93.75%,  total acc: 69.08%   [EVAL] batch:  323 | acc: 75.00%,  total acc: 69.10%   [EVAL] batch:  324 | acc: 75.00%,  total acc: 69.12%   [EVAL] batch:  325 | acc: 68.75%,  total acc: 69.11%   [EVAL] batch:  326 | acc: 43.75%,  total acc: 69.04%   [EVAL] batch:  327 | acc: 43.75%,  total acc: 68.96%   [EVAL] batch:  328 | acc: 75.00%,  total acc: 68.98%   [EVAL] batch:  329 | acc: 68.75%,  total acc: 68.98%   [EVAL] batch:  330 | acc: 75.00%,  total acc: 69.00%   [EVAL] batch:  331 | acc: 81.25%,  total acc: 69.03%   [EVAL] batch:  332 | acc: 100.00%,  total acc: 69.13%   [EVAL] batch:  333 | acc: 100.00%,  total acc: 69.22%   [EVAL] batch:  334 | acc: 87.50%,  total acc: 69.27%   [EVAL] batch:  335 | acc: 100.00%,  total acc: 69.36%   [EVAL] batch:  336 | acc: 100.00%,  total acc: 69.45%   [EVAL] batch:  337 | acc: 75.00%,  total acc: 69.47%   [EVAL] batch:  338 | acc: 100.00%,  total acc: 69.56%   [EVAL] batch:  339 | acc: 93.75%,  total acc: 69.63%   [EVAL] batch:  340 | acc: 100.00%,  total acc: 69.72%   [EVAL] batch:  341 | acc: 93.75%,  total acc: 69.79%   [EVAL] batch:  342 | acc: 100.00%,  total acc: 69.88%   [EVAL] batch:  343 | acc: 87.50%,  total acc: 69.93%   [EVAL] batch:  344 | acc: 100.00%,  total acc: 70.02%   [EVAL] batch:  345 | acc: 100.00%,  total acc: 70.10%   [EVAL] batch:  346 | acc: 87.50%,  total acc: 70.15%   [EVAL] batch:  347 | acc: 93.75%,  total acc: 70.22%   [EVAL] batch:  348 | acc: 93.75%,  total acc: 70.29%   [EVAL] batch:  349 | acc: 100.00%,  total acc: 70.38%   [EVAL] batch:  350 | acc: 25.00%,  total acc: 70.25%   [EVAL] batch:  351 | acc: 12.50%,  total acc: 70.08%   [EVAL] batch:  352 | acc: 12.50%,  total acc: 69.92%   [EVAL] batch:  353 | acc: 18.75%,  total acc: 69.77%   [EVAL] batch:  354 | acc: 18.75%,  total acc: 69.63%   [EVAL] batch:  355 | acc: 25.00%,  total acc: 69.50%   [EVAL] batch:  356 | acc: 87.50%,  total acc: 69.56%   [EVAL] batch:  357 | acc: 87.50%,  total acc: 69.61%   [EVAL] batch:  358 | acc: 100.00%,  total acc: 69.69%   [EVAL] batch:  359 | acc: 93.75%,  total acc: 69.76%   [EVAL] batch:  360 | acc: 81.25%,  total acc: 69.79%   [EVAL] batch:  361 | acc: 75.00%,  total acc: 69.80%   [EVAL] batch:  362 | acc: 62.50%,  total acc: 69.78%   [EVAL] batch:  363 | acc: 68.75%,  total acc: 69.78%   [EVAL] batch:  364 | acc: 68.75%,  total acc: 69.78%   [EVAL] batch:  365 | acc: 68.75%,  total acc: 69.77%   [EVAL] batch:  366 | acc: 56.25%,  total acc: 69.74%   [EVAL] batch:  367 | acc: 50.00%,  total acc: 69.68%   [EVAL] batch:  368 | acc: 37.50%,  total acc: 69.60%   [EVAL] batch:  369 | acc: 75.00%,  total acc: 69.61%   [EVAL] batch:  370 | acc: 75.00%,  total acc: 69.63%   [EVAL] batch:  371 | acc: 62.50%,  total acc: 69.61%   [EVAL] batch:  372 | acc: 81.25%,  total acc: 69.64%   [EVAL] batch:  373 | acc: 93.75%,  total acc: 69.70%   [EVAL] batch:  374 | acc: 81.25%,  total acc: 69.73%   [EVAL] batch:  375 | acc: 37.50%,  total acc: 69.65%   [EVAL] batch:  376 | acc: 50.00%,  total acc: 69.60%   [EVAL] batch:  377 | acc: 18.75%,  total acc: 69.46%   [EVAL] batch:  378 | acc: 37.50%,  total acc: 69.38%   [EVAL] batch:  379 | acc: 18.75%,  total acc: 69.24%   [EVAL] batch:  380 | acc: 25.00%,  total acc: 69.13%   [EVAL] batch:  381 | acc: 68.75%,  total acc: 69.13%   [EVAL] batch:  382 | acc: 75.00%,  total acc: 69.14%   [EVAL] batch:  383 | acc: 100.00%,  total acc: 69.22%   [EVAL] batch:  384 | acc: 81.25%,  total acc: 69.25%   [EVAL] batch:  385 | acc: 87.50%,  total acc: 69.30%   [EVAL] batch:  386 | acc: 75.00%,  total acc: 69.32%   [EVAL] batch:  387 | acc: 87.50%,  total acc: 69.36%   [EVAL] batch:  388 | acc: 62.50%,  total acc: 69.34%   [EVAL] batch:  389 | acc: 81.25%,  total acc: 69.38%   [EVAL] batch:  390 | acc: 75.00%,  total acc: 69.39%   [EVAL] batch:  391 | acc: 87.50%,  total acc: 69.44%   [EVAL] batch:  392 | acc: 87.50%,  total acc: 69.48%   [EVAL] batch:  393 | acc: 50.00%,  total acc: 69.43%   [EVAL] batch:  394 | acc: 31.25%,  total acc: 69.34%   [EVAL] batch:  395 | acc: 31.25%,  total acc: 69.24%   [EVAL] batch:  396 | acc: 37.50%,  total acc: 69.16%   [EVAL] batch:  397 | acc: 62.50%,  total acc: 69.14%   [EVAL] batch:  398 | acc: 31.25%,  total acc: 69.05%   [EVAL] batch:  399 | acc: 56.25%,  total acc: 69.02%   [EVAL] batch:  400 | acc: 12.50%,  total acc: 68.87%   [EVAL] batch:  401 | acc: 12.50%,  total acc: 68.73%   [EVAL] batch:  402 | acc: 6.25%,  total acc: 68.58%   [EVAL] batch:  403 | acc: 0.00%,  total acc: 68.41%   [EVAL] batch:  404 | acc: 6.25%,  total acc: 68.26%   [EVAL] batch:  405 | acc: 6.25%,  total acc: 68.10%   [EVAL] batch:  406 | acc: 62.50%,  total acc: 68.09%   [EVAL] batch:  407 | acc: 93.75%,  total acc: 68.15%   [EVAL] batch:  408 | acc: 87.50%,  total acc: 68.20%   [EVAL] batch:  409 | acc: 100.00%,  total acc: 68.28%   [EVAL] batch:  410 | acc: 93.75%,  total acc: 68.34%   [EVAL] batch:  411 | acc: 81.25%,  total acc: 68.37%   [EVAL] batch:  412 | acc: 100.00%,  total acc: 68.45%   [EVAL] batch:  413 | acc: 87.50%,  total acc: 68.49%   [EVAL] batch:  414 | acc: 100.00%,  total acc: 68.57%   [EVAL] batch:  415 | acc: 87.50%,  total acc: 68.61%   [EVAL] batch:  416 | acc: 81.25%,  total acc: 68.65%   [EVAL] batch:  417 | acc: 93.75%,  total acc: 68.71%   [EVAL] batch:  418 | acc: 93.75%,  total acc: 68.76%   [EVAL] batch:  419 | acc: 37.50%,  total acc: 68.69%   [EVAL] batch:  420 | acc: 12.50%,  total acc: 68.56%   [EVAL] batch:  421 | acc: 31.25%,  total acc: 68.47%   [EVAL] batch:  422 | acc: 25.00%,  total acc: 68.37%   [EVAL] batch:  423 | acc: 43.75%,  total acc: 68.31%   [EVAL] batch:  424 | acc: 50.00%,  total acc: 68.26%   [EVAL] batch:  425 | acc: 68.75%,  total acc: 68.27%   [EVAL] batch:  426 | acc: 87.50%,  total acc: 68.31%   [EVAL] batch:  427 | acc: 75.00%,  total acc: 68.33%   [EVAL] batch:  428 | acc: 87.50%,  total acc: 68.37%   [EVAL] batch:  429 | acc: 75.00%,  total acc: 68.39%   [EVAL] batch:  430 | acc: 81.25%,  total acc: 68.42%   [EVAL] batch:  431 | acc: 50.00%,  total acc: 68.37%   [EVAL] batch:  432 | acc: 56.25%,  total acc: 68.35%   [EVAL] batch:  433 | acc: 37.50%,  total acc: 68.27%   [EVAL] batch:  434 | acc: 43.75%,  total acc: 68.22%   [EVAL] batch:  435 | acc: 43.75%,  total acc: 68.16%   [EVAL] batch:  436 | acc: 81.25%,  total acc: 68.19%   [EVAL] batch:  437 | acc: 18.75%,  total acc: 68.08%   
cur_acc:  ['0.9494', '0.7302', '0.7500', '0.7956', '0.6925', '0.6875', '0.5823']
his_acc:  ['0.9494', '0.8310', '0.7969', '0.7835', '0.7592', '0.7165', '0.6808']
Clustering into  39  clusters
Clusters:  [31  1  9 25 20 28  9 18  1  8 36 11 18 20 34  8 22 28 13 32 22 16  3  4
  3  6 10 22 30 38 25 23  4  1 27 12  5  6 33  5 29 19  2  9 11 17  1 12
 14  7 17 13 26  0  0 13 26  7  8 35 20 16 10 24  3 21 10  1 10  1  5 10
 18 16 33 15  2 26 37 26]
Losses:  13.035796165466309 4.2021942138671875 0.663282036781311
CurrentTrain: epoch  0, batch     0 | loss: 13.0357962Losses:  10.765246391296387 4.55098295211792 0.6050394177436829
CurrentTrain: epoch  0, batch     1 | loss: 10.7652464Losses:  11.3836030960083 4.384679794311523 0.7518811821937561
CurrentTrain: epoch  0, batch     2 | loss: 11.3836031Losses:  6.61684513092041 -0.0 0.12910021841526031
CurrentTrain: epoch  0, batch     3 | loss: 6.6168451Losses:  8.824366569519043 2.8966386318206787 0.6835377216339111
CurrentTrain: epoch  1, batch     0 | loss: 8.8243666Losses:  9.591461181640625 3.204437732696533 0.6603543162345886
CurrentTrain: epoch  1, batch     1 | loss: 9.5914612Losses:  8.545401573181152 2.5482177734375 0.7523606419563293
CurrentTrain: epoch  1, batch     2 | loss: 8.5454016Losses:  8.267374038696289 -0.0 0.14090511202812195
CurrentTrain: epoch  1, batch     3 | loss: 8.2673740Losses:  9.390679359436035 4.425703048706055 0.6830693483352661
CurrentTrain: epoch  2, batch     0 | loss: 9.3906794Losses:  8.256245613098145 2.9761853218078613 0.6690388917922974
CurrentTrain: epoch  2, batch     1 | loss: 8.2562456Losses:  11.634092330932617 5.506711959838867 0.6021366119384766
CurrentTrain: epoch  2, batch     2 | loss: 11.6340923Losses:  4.164174556732178 -0.0 0.15813404321670532
CurrentTrain: epoch  2, batch     3 | loss: 4.1641746Losses:  8.254549026489258 2.590683937072754 0.7271161675453186
CurrentTrain: epoch  3, batch     0 | loss: 8.2545490Losses:  8.092265129089355 2.6315431594848633 0.6795097589492798
CurrentTrain: epoch  3, batch     1 | loss: 8.0922651Losses:  7.285075664520264 2.052652359008789 0.7561381459236145
CurrentTrain: epoch  3, batch     2 | loss: 7.2850757Losses:  2.376072645187378 -0.0 0.10341133177280426
CurrentTrain: epoch  3, batch     3 | loss: 2.3760726Losses:  7.857594966888428 3.629580497741699 0.6435419321060181
CurrentTrain: epoch  4, batch     0 | loss: 7.8575950Losses:  7.359756946563721 2.3509812355041504 0.744937539100647
CurrentTrain: epoch  4, batch     1 | loss: 7.3597569Losses:  8.55538558959961 3.1086602210998535 0.6885560154914856
CurrentTrain: epoch  4, batch     2 | loss: 8.5553856Losses:  4.369281768798828 -0.0 0.11173379421234131
CurrentTrain: epoch  4, batch     3 | loss: 4.3692818Losses:  7.003574371337891 3.276578903198242 0.6453410983085632
CurrentTrain: epoch  5, batch     0 | loss: 7.0035744Losses:  7.431281566619873 3.1886067390441895 0.5989660620689392
CurrentTrain: epoch  5, batch     1 | loss: 7.4312816Losses:  8.88499641418457 3.806577205657959 0.578615665435791
CurrentTrain: epoch  5, batch     2 | loss: 8.8849964Losses:  4.423673152923584 -0.0 0.10182522982358932
CurrentTrain: epoch  5, batch     3 | loss: 4.4236732Losses:  6.406476974487305 2.324395179748535 0.7256559133529663
CurrentTrain: epoch  6, batch     0 | loss: 6.4064770Losses:  7.508134841918945 2.6509172916412354 0.7147209048271179
CurrentTrain: epoch  6, batch     1 | loss: 7.5081348Losses:  7.092394828796387 2.601799964904785 0.6884214878082275
CurrentTrain: epoch  6, batch     2 | loss: 7.0923948Losses:  3.921980857849121 -0.0 0.14340871572494507
CurrentTrain: epoch  6, batch     3 | loss: 3.9219809Losses:  6.005433559417725 2.3223867416381836 0.6667045950889587
CurrentTrain: epoch  7, batch     0 | loss: 6.0054336Losses:  6.888210296630859 3.238178253173828 0.6855027079582214
CurrentTrain: epoch  7, batch     1 | loss: 6.8882103Losses:  9.726235389709473 4.433305740356445 0.6175709962844849
CurrentTrain: epoch  7, batch     2 | loss: 9.7262354Losses:  2.6498641967773438 -0.0 0.14059872925281525
CurrentTrain: epoch  7, batch     3 | loss: 2.6498642Losses:  6.431673526763916 2.7045559883117676 0.6521368622779846
CurrentTrain: epoch  8, batch     0 | loss: 6.4316735Losses:  7.567871570587158 3.559568405151367 0.7514871954917908
CurrentTrain: epoch  8, batch     1 | loss: 7.5678716Losses:  6.148287773132324 2.4308853149414062 0.6455605030059814
CurrentTrain: epoch  8, batch     2 | loss: 6.1482878Losses:  2.5387089252471924 -0.0 0.09998413175344467
CurrentTrain: epoch  8, batch     3 | loss: 2.5387089Losses:  6.511899471282959 2.9277987480163574 0.6012064218521118
CurrentTrain: epoch  9, batch     0 | loss: 6.5118995Losses:  5.997941970825195 2.6480019092559814 0.6756508350372314
CurrentTrain: epoch  9, batch     1 | loss: 5.9979420Losses:  6.228923797607422 2.878594160079956 0.6383750438690186
CurrentTrain: epoch  9, batch     2 | loss: 6.2289238Losses:  4.4024434089660645 -0.0 0.0900987982749939
CurrentTrain: epoch  9, batch     3 | loss: 4.4024434
#### applies to jurisdiction ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "applies to jurisdiction" means the item (institution, law, public office, public register...) or statement belongs to or has power over or applies to the value (a territorial jurisdiction: a country, state, municipality, ...).
Here is an example:
Relation: applies to jurisdiction
Context: george lawson sheldon ( may 31 , 1870 – april 4 , 1960 ) was 14th governor of nebraska with service from 1907 until 1909 .
Head Entity: governor
Tail Entity: nebraska
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "applies to jurisdiction":
Relation: applies to jurisdiction  
Context: The Environmental Protection Agency (EPA) is responsible for enforcing regulations related to environmental protection in the United States.  
Head Entity: Environmental Protection Agency  
Tail Entity: United States  

Relation: applies to jurisdiction  
Context: The California Consumer Privacy Act (CCPA) provides residents of California with specific rights regarding their personal information.  
Head Entity: California Consumer Privacy Act  
Tail Entity: California  
#### performer ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "performer" means actor, musician, band or other performer associated with this role or musical work.
Here is an example:
Relation: performer
Context: it is prominently featured on jean michel jarre 's albums " oxygène " ( 1977 ) and " équinoxe " ( 1978 ) .
Head Entity: équinoxe
Tail Entity: jean michel jarre
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "performer":
Relation: performer  
Context: The legendary band Queen is known for their iconic song "Bohemian Rhapsody," which showcases Freddie Mercury's extraordinary vocal talent.  
Head Entity: Bohemian Rhapsody  
Tail Entity: Queen  

Relation: performer  
Context: Taylor Swift captivated audiences with her heartfelt performance of "Love Story" during the music festival last summer.  
Head Entity: Love Story  
Tail Entity: Taylor Swift  
#### manufacturer ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "manufacturer" means manufacturer or producer of this product.
Here is an example:
Relation: manufacturer
Context: the next full - frame digital slrs were the canon eos-1ds of late 2002 , followed by kodak 's dcs pro 14n in 2003 .
Head Entity: dcs pro 14n
Tail Entity: kodak
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "manufacturer":
Relation: manufacturer  
Context: The latest smartphone model, the Galaxy S21, was developed by Samsung and released in early 2021.  
Head Entity: Galaxy S21  
Tail Entity: Samsung  

Relation: manufacturer  
Context: The iconic Model T was produced by Ford Motor Company and revolutionized the automotive industry in the early 20th century.  
Head Entity: Model T  
Tail Entity: Ford Motor Company  
#### participant ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "participant" means person, group of people or organization (object) that actively takes/took part in an event or process.
Here is an example:
Relation: participant
Context: norway was represented in the eurovision song contest 2005 by the song " in my dreams " performed by wig wam .
Head Entity: eurovision song contest 2005
Tail Entity: wig wam
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "participant":
Relation: participant  
Context: the united states was represented in the 2020 summer olympics by a team of athletes competing in various sports.  
Head Entity: 2020 summer olympics  
Tail Entity: team of athletes  

Relation: participant  
Context: the annual tech conference featured several startups, including a presentation by innovative solutions inc.  
Head Entity: annual tech conference  
Tail Entity: innovative solutions inc.  
#### operating system ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "operating system" means operating system (OS) on which a software works or the OS installed on hardware.
Here is an example:
Relation: operating system
Context: " robowar " is a similar game that was released later on the macintosh , and is now available for both mac os and windows .
Head Entity: robowar
Tail Entity: windows
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "operating system":
Relation: operating system  
Context: "The latest version of Adobe Photoshop is compatible with both Windows 10 and macOS Monterey."  
Head Entity: Adobe Photoshop  
Tail Entity: Windows 10  

Relation: operating system  
Context: "Minecraft runs smoothly on various platforms, including Linux and the latest version of Windows."  
Head Entity: Minecraft  
Tail Entity: Linux  
#### competition class ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "competition class" means official classification by a regulating body under which the subject (events, teams, participants, or equipment) qualifies for inclusion.
Here is an example:
Relation: competition class
Context: he placed second three times in a row at the cuban national championships from 2005- 2007 at light heavyweight , each time behind yusiel nápoles .
Head Entity: yusiel nápoles
Tail Entity: light heavyweight
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "competition class":
Relation: competition class  
Context: The team qualified for the finals in the under-18 category of the national soccer tournament, showcasing their skills against various opponents throughout the season.  
Head Entity: under-18 category  
Tail Entity: national soccer tournament  

Relation: competition class  
Context: She competed in the women's marathon event, which is classified under the elite category for international competitions, achieving a personal best time.  
Head Entity: elite category  
Tail Entity: women's marathon event  
#### successful candidate ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "successful candidate" means person elected after the election.
Here is an example:
Relation: successful candidate
Context: the qup gained 3 seats from its previous election result , and won a 10 % swing to it in primary votes , but failed to defeat the incumbent labor government led by ted theodore .
Head Entity: previous election
Tail Entity: ted theodore
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "successful candidate":
Relation: successful candidate  
Context: After a closely contested campaign, the voters chose Sarah Johnson as their representative, marking a significant shift in the political landscape of the district.  
Head Entity: closely contested campaign  
Tail Entity: Sarah Johnson  

Relation: successful candidate  
Context: In the recent mayoral election, the community rallied behind Mark Thompson, who promised to bring new initiatives to the city, ultimately leading to his victory.  
Head Entity: recent mayoral election  
Tail Entity: Mark Thompson  
#### sibling ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "sibling" means the subject and the object have the same parents.
Here is an example:
Relation: sibling
Context: the deaths of his brothers wenceslaus ii ( 1487 ) , casimir ii ( 1490 ) and władysław ( 1494 ) allowed jan v to reunificated the whole duchy of zator .
Head Entity: casimir ii
Tail Entity: wenceslaus ii
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "sibling":
Relation: sibling  
Context: both elizabeth and her brother, charles, were known for their contributions to the arts and culture of their time.  
Head Entity: elizabeth  
Tail Entity: charles  

Relation: sibling  
Context: during the family reunion, it was heartwarming to see how much john and his sister, sarah, resembled each other in both looks and personality.  
Head Entity: john  
Tail Entity: sarah  
#### military branch ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "military branch" means branch to which this military unit, award, office, or person belongs.
Here is an example:
Relation: military branch
Context: dennis chalker is a retired navy seal , inventor and author who has written six books about the united states navy seals .
Head Entity: dennis chalker
Tail Entity: united states navy
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "military branch":
Relation: military branch  
Context: general john smith served in the air force for over twenty years, leading numerous missions and earning several commendations.  
Head Entity: general john smith  
Tail Entity: air force  

Relation: military branch  
Context: the distinguished service cross was awarded to captain elizabeth jones for her bravery while serving in the army during a critical operation.  
Head Entity: captain elizabeth jones  
Tail Entity: army  
#### child ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "child" means subject has object as child.
Here is an example:
Relation: child
Context: in 1406 adolf married marie of burgundy , daughter of john the fearless and margaret of bavaria .
Head Entity: john the fearless
Tail Entity: marie of burgundy
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "child":
Relation: child  
Context: in 1980, michael and sarah welcomed their first child, a daughter named emily, into the world.  
Head Entity: michael  
Tail Entity: emily  

Relation: child  
Context: during the family reunion, it was revealed that elizabeth is the proud mother of three children, including her youngest, a son named alex.  
Head Entity: elizabeth  
Tail Entity: alex  
Losses:  3.398487091064453 -0.0 0.9897419214248657
MemoryTrain:  epoch  0, batch     0 | loss: 3.3984871Losses:  3.4501471519470215 -0.0 1.0380239486694336
MemoryTrain:  epoch  0, batch     1 | loss: 3.4501472Losses:  4.343150615692139 0.6535048484802246 0.8222053647041321
MemoryTrain:  epoch  0, batch     2 | loss: 4.3431506Losses:  3.460859775543213 0.2538979947566986 1.0178601741790771
MemoryTrain:  epoch  0, batch     3 | loss: 3.4608598Losses:  4.195550441741943 0.2574825584888458 0.9268087148666382
MemoryTrain:  epoch  0, batch     4 | loss: 4.1955504Losses:  3.666198968887329 0.24517254531383514 0.9007399082183838
MemoryTrain:  epoch  0, batch     5 | loss: 3.6661990Losses:  3.969618082046509 -0.0 0.9453312754631042
MemoryTrain:  epoch  0, batch     6 | loss: 3.9696181Losses:  3.782052993774414 0.2364000827074051 1.0072630643844604
MemoryTrain:  epoch  0, batch     7 | loss: 3.7820530Losses:  3.646028518676758 0.25538599491119385 0.966302752494812
MemoryTrain:  epoch  0, batch     8 | loss: 3.6460285Losses:  5.0489630699157715 0.5181825160980225 0.983202338218689
MemoryTrain:  epoch  0, batch     9 | loss: 5.0489631Losses:  3.8537447452545166 0.2346244603395462 1.0240309238433838
MemoryTrain:  epoch  0, batch    10 | loss: 3.8537447Losses:  4.594447135925293 0.3271870017051697 0.931975781917572
MemoryTrain:  epoch  0, batch    11 | loss: 4.5944471Losses:  3.342787504196167 0.2619239091873169 1.0276439189910889
MemoryTrain:  epoch  0, batch    12 | loss: 3.3427875Losses:  3.6434340476989746 0.24021530151367188 1.0213472843170166
MemoryTrain:  epoch  0, batch    13 | loss: 3.6434340Losses:  3.5572705268859863 -0.0 0.977996289730072
MemoryTrain:  epoch  0, batch    14 | loss: 3.5572705Losses:  3.8865795135498047 0.4685957729816437 0.8337916135787964
MemoryTrain:  epoch  1, batch     0 | loss: 3.8865795Losses:  3.4872305393218994 0.24257150292396545 1.0229730606079102
MemoryTrain:  epoch  1, batch     1 | loss: 3.4872305Losses:  4.13966703414917 0.593287467956543 0.9177300930023193
MemoryTrain:  epoch  1, batch     2 | loss: 4.1396670Losses:  3.7397050857543945 0.5251338481903076 0.8521935939788818
MemoryTrain:  epoch  1, batch     3 | loss: 3.7397051Losses:  2.943150758743286 0.2691747546195984 0.8166249394416809
MemoryTrain:  epoch  1, batch     4 | loss: 2.9431508Losses:  4.0442423820495605 0.25190696120262146 0.9636476635932922
MemoryTrain:  epoch  1, batch     5 | loss: 4.0442424Losses:  3.4060521125793457 0.25139516592025757 0.9588367938995361
MemoryTrain:  epoch  1, batch     6 | loss: 3.4060521Losses:  3.679574966430664 0.2353462278842926 1.007162094116211
MemoryTrain:  epoch  1, batch     7 | loss: 3.6795750Losses:  2.8499138355255127 -0.0 0.9543954730033875
MemoryTrain:  epoch  1, batch     8 | loss: 2.8499138Losses:  3.643892288208008 0.49079281091690063 0.9746846556663513
MemoryTrain:  epoch  1, batch     9 | loss: 3.6438923Losses:  3.567431688308716 -0.0 1.0655982494354248
MemoryTrain:  epoch  1, batch    10 | loss: 3.5674317Losses:  3.9045443534851074 0.5631892085075378 0.9594357013702393
MemoryTrain:  epoch  1, batch    11 | loss: 3.9045444Losses:  3.585463285446167 -0.0 1.0554687976837158
MemoryTrain:  epoch  1, batch    12 | loss: 3.5854633Losses:  3.408951759338379 0.5800464749336243 0.9751471281051636
MemoryTrain:  epoch  1, batch    13 | loss: 3.4089518Losses:  3.2347822189331055 -0.0 1.1333575248718262
MemoryTrain:  epoch  1, batch    14 | loss: 3.2347822Losses:  3.5514426231384277 0.4624060094356537 0.9424216151237488
MemoryTrain:  epoch  2, batch     0 | loss: 3.5514426Losses:  3.166206121444702 0.5039528608322144 0.9203321933746338
MemoryTrain:  epoch  2, batch     1 | loss: 3.1662061Losses:  2.6456806659698486 -0.0 0.9903764128684998
MemoryTrain:  epoch  2, batch     2 | loss: 2.6456807Losses:  3.4147958755493164 0.2593083679676056 1.0119481086730957
MemoryTrain:  epoch  2, batch     3 | loss: 3.4147959Losses:  3.0971808433532715 0.23853528499603271 1.0298982858657837
MemoryTrain:  epoch  2, batch     4 | loss: 3.0971808Losses:  3.3277692794799805 0.2648133635520935 1.0296149253845215
MemoryTrain:  epoch  2, batch     5 | loss: 3.3277693Losses:  3.0583865642547607 0.2528631091117859 0.9591230750083923
MemoryTrain:  epoch  2, batch     6 | loss: 3.0583866Losses:  3.8613312244415283 0.5725538730621338 0.8475245833396912
MemoryTrain:  epoch  2, batch     7 | loss: 3.8613312Losses:  2.7833919525146484 -0.0 0.9788339138031006
MemoryTrain:  epoch  2, batch     8 | loss: 2.7833920Losses:  4.6954474449157715 1.2737579345703125 0.7163603901863098
MemoryTrain:  epoch  2, batch     9 | loss: 4.6954474Losses:  3.0636138916015625 0.2559257447719574 0.9659243822097778
MemoryTrain:  epoch  2, batch    10 | loss: 3.0636139Losses:  2.835237503051758 0.25250911712646484 1.024955153465271
MemoryTrain:  epoch  2, batch    11 | loss: 2.8352375Losses:  2.5785372257232666 -0.0 1.033278465270996
MemoryTrain:  epoch  2, batch    12 | loss: 2.5785372Losses:  2.8706917762756348 -0.0 1.0115197896957397
MemoryTrain:  epoch  2, batch    13 | loss: 2.8706918Losses:  3.100778579711914 -0.0 1.0271615982055664
MemoryTrain:  epoch  2, batch    14 | loss: 3.1007786Losses:  3.2044601440429688 -0.0 1.1126233339309692
MemoryTrain:  epoch  3, batch     0 | loss: 3.2044601Losses:  2.8784728050231934 0.2443854808807373 0.9632407426834106
MemoryTrain:  epoch  3, batch     1 | loss: 2.8784728Losses:  2.9997363090515137 0.24689318239688873 0.9511775374412537
MemoryTrain:  epoch  3, batch     2 | loss: 2.9997363Losses:  2.639253854751587 0.26693931221961975 0.9597883820533752
MemoryTrain:  epoch  3, batch     3 | loss: 2.6392539Losses:  3.222202777862549 0.46979278326034546 0.9047083258628845
MemoryTrain:  epoch  3, batch     4 | loss: 3.2222028Losses:  3.0553536415100098 -0.0 1.0473569631576538
MemoryTrain:  epoch  3, batch     5 | loss: 3.0553536Losses:  3.7011914253234863 0.5580747723579407 0.9365986585617065
MemoryTrain:  epoch  3, batch     6 | loss: 3.7011914Losses:  3.1796560287475586 0.8192840218544006 0.8693369626998901
MemoryTrain:  epoch  3, batch     7 | loss: 3.1796560Losses:  2.94387149810791 0.49474549293518066 0.9744105935096741
MemoryTrain:  epoch  3, batch     8 | loss: 2.9438715Losses:  2.8595995903015137 -0.0 1.0635030269622803
MemoryTrain:  epoch  3, batch     9 | loss: 2.8595996Losses:  2.6130733489990234 -0.0 0.9609454870223999
MemoryTrain:  epoch  3, batch    10 | loss: 2.6130733Losses:  2.816985607147217 -0.0 1.0330810546875
MemoryTrain:  epoch  3, batch    11 | loss: 2.8169856Losses:  2.5548155307769775 -0.0 0.9590949416160583
MemoryTrain:  epoch  3, batch    12 | loss: 2.5548155Losses:  2.5719451904296875 0.2487606406211853 0.8920073509216309
MemoryTrain:  epoch  3, batch    13 | loss: 2.5719452Losses:  2.9901742935180664 0.2225976437330246 0.9911141395568848
MemoryTrain:  epoch  3, batch    14 | loss: 2.9901743Losses:  2.838198661804199 0.2700830101966858 0.9510155320167542
MemoryTrain:  epoch  4, batch     0 | loss: 2.8381987Losses:  2.687174081802368 0.2414572834968567 1.009124994277954
MemoryTrain:  epoch  4, batch     1 | loss: 2.6871741Losses:  2.6481595039367676 0.2587354779243469 1.0020233392715454
MemoryTrain:  epoch  4, batch     2 | loss: 2.6481595Losses:  2.8624229431152344 0.4803735613822937 0.9614558219909668
MemoryTrain:  epoch  4, batch     3 | loss: 2.8624229Losses:  3.217343807220459 0.2563708424568176 1.035406470298767
MemoryTrain:  epoch  4, batch     4 | loss: 3.2173438Losses:  3.4360976219177246 0.23095980286598206 0.9617316722869873
MemoryTrain:  epoch  4, batch     5 | loss: 3.4360976Losses:  2.6941542625427246 -0.0 1.0671577453613281
MemoryTrain:  epoch  4, batch     6 | loss: 2.6941543Losses:  3.4138989448547363 0.5046576857566833 0.8599397540092468
MemoryTrain:  epoch  4, batch     7 | loss: 3.4138989Losses:  2.983879566192627 0.5165882110595703 0.9279955625534058
MemoryTrain:  epoch  4, batch     8 | loss: 2.9838796Losses:  2.479478359222412 -0.0 0.9636115431785583
MemoryTrain:  epoch  4, batch     9 | loss: 2.4794784Losses:  2.2092339992523193 -0.0 0.9018034934997559
MemoryTrain:  epoch  4, batch    10 | loss: 2.2092340Losses:  2.3316402435302734 -0.0 0.9744254350662231
MemoryTrain:  epoch  4, batch    11 | loss: 2.3316402Losses:  2.3113014698028564 -0.0 0.9074437022209167
MemoryTrain:  epoch  4, batch    12 | loss: 2.3113015Losses:  2.9689955711364746 0.26141178607940674 0.8176854252815247
MemoryTrain:  epoch  4, batch    13 | loss: 2.9689956Losses:  2.667116641998291 -0.0 1.0292466878890991
MemoryTrain:  epoch  4, batch    14 | loss: 2.6671166Losses:  2.3806610107421875 -0.0 0.9038630723953247
MemoryTrain:  epoch  5, batch     0 | loss: 2.3806610Losses:  2.4296212196350098 0.2374342679977417 0.897240161895752
MemoryTrain:  epoch  5, batch     1 | loss: 2.4296212Losses:  2.372135877609253 -0.0 0.9723408222198486
MemoryTrain:  epoch  5, batch     2 | loss: 2.3721359Losses:  2.8936715126037598 0.4614727795124054 0.9188224077224731
MemoryTrain:  epoch  5, batch     3 | loss: 2.8936715Losses:  2.7769112586975098 0.2541816234588623 1.0072021484375
MemoryTrain:  epoch  5, batch     4 | loss: 2.7769113Losses:  2.3249335289001465 -0.0 0.9091756343841553
MemoryTrain:  epoch  5, batch     5 | loss: 2.3249335Losses:  2.998715877532959 -0.0 1.0739401578903198
MemoryTrain:  epoch  5, batch     6 | loss: 2.9987159Losses:  3.1346192359924316 0.2735276520252228 0.9733018279075623
MemoryTrain:  epoch  5, batch     7 | loss: 3.1346192Losses:  2.8225107192993164 0.25584670901298523 1.032120704650879
MemoryTrain:  epoch  5, batch     8 | loss: 2.8225107Losses:  2.9288346767425537 0.2625194787979126 0.8201139569282532
MemoryTrain:  epoch  5, batch     9 | loss: 2.9288347Losses:  2.820478677749634 0.2481294572353363 0.9649326205253601
MemoryTrain:  epoch  5, batch    10 | loss: 2.8204787Losses:  2.688803195953369 0.24452278017997742 0.9172747135162354
MemoryTrain:  epoch  5, batch    11 | loss: 2.6888032Losses:  2.8075406551361084 0.48213469982147217 0.825758159160614
MemoryTrain:  epoch  5, batch    12 | loss: 2.8075407Losses:  2.7336015701293945 0.23848526179790497 0.954235315322876
MemoryTrain:  epoch  5, batch    13 | loss: 2.7336016Losses:  2.7666444778442383 0.2446531057357788 0.9627048969268799
MemoryTrain:  epoch  5, batch    14 | loss: 2.7666445Losses:  2.679131507873535 -0.0 1.0190105438232422
MemoryTrain:  epoch  6, batch     0 | loss: 2.6791315Losses:  2.4909558296203613 0.2546330690383911 0.8501750826835632
MemoryTrain:  epoch  6, batch     1 | loss: 2.4909558Losses:  3.132432460784912 0.5220391750335693 0.9852496385574341
MemoryTrain:  epoch  6, batch     2 | loss: 3.1324325Losses:  2.6244916915893555 -0.0 0.958699643611908
MemoryTrain:  epoch  6, batch     3 | loss: 2.6244917Losses:  2.5495169162750244 0.2545071840286255 0.9734398126602173
MemoryTrain:  epoch  6, batch     4 | loss: 2.5495169Losses:  2.534707546234131 -0.0 1.07209050655365
MemoryTrain:  epoch  6, batch     5 | loss: 2.5347075Losses:  2.438307762145996 0.23357698321342468 0.9471704363822937
MemoryTrain:  epoch  6, batch     6 | loss: 2.4383078Losses:  2.451723575592041 -0.0 1.0072873830795288
MemoryTrain:  epoch  6, batch     7 | loss: 2.4517236Losses:  2.8670613765716553 0.5178666710853577 0.9719238877296448
MemoryTrain:  epoch  6, batch     8 | loss: 2.8670614Losses:  2.7475786209106445 0.2813683748245239 0.9153008460998535
MemoryTrain:  epoch  6, batch     9 | loss: 2.7475786Losses:  2.508991241455078 -0.0 0.8561276197433472
MemoryTrain:  epoch  6, batch    10 | loss: 2.5089912Losses:  2.707296371459961 -0.0 1.018571376800537
MemoryTrain:  epoch  6, batch    11 | loss: 2.7072964Losses:  3.430464506149292 0.6147096753120422 0.9214181900024414
MemoryTrain:  epoch  6, batch    12 | loss: 3.4304645Losses:  2.391533374786377 -0.0 1.0776128768920898
MemoryTrain:  epoch  6, batch    13 | loss: 2.3915334Losses:  2.644912004470825 0.5068944692611694 0.8650490641593933
MemoryTrain:  epoch  6, batch    14 | loss: 2.6449120Losses:  2.6856985092163086 0.5119866728782654 0.9131843447685242
MemoryTrain:  epoch  7, batch     0 | loss: 2.6856985Losses:  2.478990077972412 0.26488184928894043 0.9851613640785217
MemoryTrain:  epoch  7, batch     1 | loss: 2.4789901Losses:  2.897397994995117 -0.0 0.9882460832595825
MemoryTrain:  epoch  7, batch     2 | loss: 2.8973980Losses:  2.5600991249084473 0.2572038471698761 1.065666913986206
MemoryTrain:  epoch  7, batch     3 | loss: 2.5600991Losses:  2.943842649459839 0.5462543964385986 0.9504972696304321
MemoryTrain:  epoch  7, batch     4 | loss: 2.9438426Losses:  2.4234824180603027 -0.0 0.9692232608795166
MemoryTrain:  epoch  7, batch     5 | loss: 2.4234824Losses:  2.7256789207458496 0.4997999668121338 0.8973387479782104
MemoryTrain:  epoch  7, batch     6 | loss: 2.7256789Losses:  2.806793689727783 0.4776029586791992 0.8273112773895264
MemoryTrain:  epoch  7, batch     7 | loss: 2.8067937Losses:  2.3218493461608887 -0.0 1.0523003339767456
MemoryTrain:  epoch  7, batch     8 | loss: 2.3218493Losses:  2.5720715522766113 0.2689053416252136 0.9195312261581421
MemoryTrain:  epoch  7, batch     9 | loss: 2.5720716Losses:  3.367357015609741 0.5969939827919006 0.9819527268409729
MemoryTrain:  epoch  7, batch    10 | loss: 3.3673570Losses:  2.8043782711029053 0.27479082345962524 0.9674830436706543
MemoryTrain:  epoch  7, batch    11 | loss: 2.8043783Losses:  2.568385601043701 0.25766488909721375 1.0207566022872925
MemoryTrain:  epoch  7, batch    12 | loss: 2.5683856Losses:  2.2609589099884033 -0.0 0.9990830421447754
MemoryTrain:  epoch  7, batch    13 | loss: 2.2609589Losses:  2.6549763679504395 0.47283899784088135 0.9240531325340271
MemoryTrain:  epoch  7, batch    14 | loss: 2.6549764Losses:  2.8408117294311523 0.5249925255775452 0.8207917213439941
MemoryTrain:  epoch  8, batch     0 | loss: 2.8408117Losses:  2.627293825149536 0.5325478315353394 0.8000485301017761
MemoryTrain:  epoch  8, batch     1 | loss: 2.6272938Losses:  2.218749523162842 -0.0 0.9561662077903748
MemoryTrain:  epoch  8, batch     2 | loss: 2.2187495Losses:  2.6770691871643066 0.495161235332489 0.8466340899467468
MemoryTrain:  epoch  8, batch     3 | loss: 2.6770692Losses:  2.296544313430786 -0.0 1.055550217628479
MemoryTrain:  epoch  8, batch     4 | loss: 2.2965443Losses:  2.685955047607422 0.24833771586418152 1.0623725652694702
MemoryTrain:  epoch  8, batch     5 | loss: 2.6859550Losses:  2.8931050300598145 0.5568941235542297 0.8670741319656372
MemoryTrain:  epoch  8, batch     6 | loss: 2.8931050Losses:  2.8097546100616455 0.26698052883148193 0.951727569103241
MemoryTrain:  epoch  8, batch     7 | loss: 2.8097546Losses:  2.4881982803344727 -0.0 1.0483267307281494
MemoryTrain:  epoch  8, batch     8 | loss: 2.4881983Losses:  2.414475202560425 -0.0 0.9773684144020081
MemoryTrain:  epoch  8, batch     9 | loss: 2.4144752Losses:  2.5401973724365234 0.2491580694913864 0.9683405160903931
MemoryTrain:  epoch  8, batch    10 | loss: 2.5401974Losses:  2.950133800506592 0.5000535249710083 1.0318453311920166
MemoryTrain:  epoch  8, batch    11 | loss: 2.9501338Losses:  2.9741406440734863 0.8428941369056702 0.8961113691329956
MemoryTrain:  epoch  8, batch    12 | loss: 2.9741406Losses:  2.869568109512329 0.5072704553604126 0.9614443182945251
MemoryTrain:  epoch  8, batch    13 | loss: 2.8695681Losses:  2.3844263553619385 -0.0 1.086557149887085
MemoryTrain:  epoch  8, batch    14 | loss: 2.3844264Losses:  2.5913944244384766 -0.0 1.0377434492111206
MemoryTrain:  epoch  9, batch     0 | loss: 2.5913944Losses:  3.0123097896575928 0.9904348254203796 0.7834784388542175
MemoryTrain:  epoch  9, batch     1 | loss: 3.0123098Losses:  2.2780537605285645 -0.0 1.0089410543441772
MemoryTrain:  epoch  9, batch     2 | loss: 2.2780538Losses:  2.6290385723114014 0.2457786500453949 1.0155779123306274
MemoryTrain:  epoch  9, batch     3 | loss: 2.6290386Losses:  2.5686445236206055 0.24392858147621155 0.9818912148475647
MemoryTrain:  epoch  9, batch     4 | loss: 2.5686445Losses:  2.5836341381073 0.26879966259002686 0.9572645425796509
MemoryTrain:  epoch  9, batch     5 | loss: 2.5836341Losses:  2.1708099842071533 -0.0 0.9132892489433289
MemoryTrain:  epoch  9, batch     6 | loss: 2.1708100Losses:  2.7870545387268066 0.5245548486709595 0.9028041362762451
MemoryTrain:  epoch  9, batch     7 | loss: 2.7870545Losses:  2.3974711894989014 0.23131294548511505 0.8306609988212585
MemoryTrain:  epoch  9, batch     8 | loss: 2.3974712Losses:  2.4259161949157715 0.25621575117111206 0.9467229247093201
MemoryTrain:  epoch  9, batch     9 | loss: 2.4259162Losses:  2.700772285461426 0.24081775546073914 1.009325623512268
MemoryTrain:  epoch  9, batch    10 | loss: 2.7007723Losses:  2.603668212890625 0.24057519435882568 1.0522739887237549
MemoryTrain:  epoch  9, batch    11 | loss: 2.6036682Losses:  2.431342363357544 0.2287386655807495 0.8060263991355896
MemoryTrain:  epoch  9, batch    12 | loss: 2.4313424Losses:  2.5587446689605713 0.23506750166416168 0.9452680945396423
MemoryTrain:  epoch  9, batch    13 | loss: 2.5587447Losses:  2.426248550415039 -0.0 0.9623943567276001
MemoryTrain:  epoch  9, batch    14 | loss: 2.4262486
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    1 | acc: 68.75%,  total acc: 65.62%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 72.92%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 71.88%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 71.25%   [EVAL] batch:    5 | acc: 62.50%,  total acc: 69.79%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 71.43%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 72.66%   [EVAL] batch:    8 | acc: 75.00%,  total acc: 72.92%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 73.75%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 74.43%   [EVAL] batch:   11 | acc: 68.75%,  total acc: 73.96%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 74.52%   [EVAL] batch:   13 | acc: 56.25%,  total acc: 73.21%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 73.33%   [EVAL] batch:   15 | acc: 75.00%,  total acc: 73.44%   [EVAL] batch:   16 | acc: 25.00%,  total acc: 70.59%   [EVAL] batch:   17 | acc: 75.00%,  total acc: 70.83%   [EVAL] batch:   18 | acc: 75.00%,  total acc: 71.05%   [EVAL] batch:   19 | acc: 43.75%,  total acc: 69.69%   [EVAL] batch:   20 | acc: 62.50%,  total acc: 69.35%   [EVAL] batch:   21 | acc: 62.50%,  total acc: 69.03%   [EVAL] batch:   22 | acc: 62.50%,  total acc: 68.75%   [EVAL] batch:   23 | acc: 75.00%,  total acc: 69.01%   [EVAL] batch:   24 | acc: 62.50%,  total acc: 68.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 69.95%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 71.06%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 72.10%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 72.84%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 73.75%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 74.60%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 75.20%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 75.95%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 76.65%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 77.32%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 77.95%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 78.55%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 78.78%   [EVAL] batch:   38 | acc: 81.25%,  total acc: 78.85%   [EVAL] batch:   39 | acc: 81.25%,  total acc: 78.91%   [EVAL] batch:   40 | acc: 75.00%,  total acc: 78.81%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 79.02%   [EVAL] batch:   42 | acc: 87.50%,  total acc: 79.22%   [EVAL] batch:   43 | acc: 68.75%,  total acc: 78.98%   [EVAL] batch:   44 | acc: 56.25%,  total acc: 78.47%   [EVAL] batch:   45 | acc: 37.50%,  total acc: 77.58%   [EVAL] batch:   46 | acc: 50.00%,  total acc: 76.99%   [EVAL] batch:   47 | acc: 75.00%,  total acc: 76.95%   [EVAL] batch:   48 | acc: 37.50%,  total acc: 76.15%   [EVAL] batch:   49 | acc: 81.25%,  total acc: 76.25%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 76.59%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 76.92%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 77.24%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 77.31%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 77.61%   [EVAL] batch:   55 | acc: 81.25%,  total acc: 77.68%   [EVAL] batch:   56 | acc: 37.50%,  total acc: 76.97%   [EVAL] batch:   57 | acc: 25.00%,  total acc: 76.08%   [EVAL] batch:   58 | acc: 25.00%,  total acc: 75.21%   [EVAL] batch:   59 | acc: 25.00%,  total acc: 74.38%   [EVAL] batch:   60 | acc: 31.25%,  total acc: 73.67%   [EVAL] batch:   61 | acc: 18.75%,  total acc: 72.78%   [EVAL] batch:   62 | acc: 18.75%,  total acc: 71.92%   
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    1 | acc: 68.75%,  total acc: 65.62%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 70.83%   [EVAL] batch:    3 | acc: 62.50%,  total acc: 68.75%   [EVAL] batch:    4 | acc: 62.50%,  total acc: 67.50%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 69.79%   [EVAL] batch:    6 | acc: 37.50%,  total acc: 65.18%   [EVAL] batch:    7 | acc: 56.25%,  total acc: 64.06%   [EVAL] batch:    8 | acc: 37.50%,  total acc: 61.11%   [EVAL] batch:    9 | acc: 56.25%,  total acc: 60.62%   [EVAL] batch:   10 | acc: 37.50%,  total acc: 58.52%   [EVAL] batch:   11 | acc: 37.50%,  total acc: 56.77%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 57.69%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 60.27%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 62.50%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 64.84%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 66.54%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 68.06%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 69.74%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 70.31%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 71.43%   [EVAL] batch:   21 | acc: 75.00%,  total acc: 71.59%   [EVAL] batch:   22 | acc: 75.00%,  total acc: 71.74%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 72.40%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 73.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 74.04%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 75.89%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 76.51%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 77.29%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 78.02%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 78.71%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 79.17%   [EVAL] batch:   33 | acc: 62.50%,  total acc: 78.68%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 78.57%   [EVAL] batch:   35 | acc: 75.00%,  total acc: 78.47%   [EVAL] batch:   36 | acc: 81.25%,  total acc: 78.55%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 78.78%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 79.33%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 79.84%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 80.18%   [EVAL] batch:   41 | acc: 81.25%,  total acc: 80.21%   [EVAL] batch:   42 | acc: 62.50%,  total acc: 79.80%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 79.97%   [EVAL] batch:   44 | acc: 81.25%,  total acc: 80.00%   [EVAL] batch:   45 | acc: 81.25%,  total acc: 80.03%   [EVAL] batch:   46 | acc: 68.75%,  total acc: 79.79%   [EVAL] batch:   47 | acc: 81.25%,  total acc: 79.82%   [EVAL] batch:   48 | acc: 81.25%,  total acc: 79.85%   [EVAL] batch:   49 | acc: 75.00%,  total acc: 79.75%   [EVAL] batch:   50 | acc: 75.00%,  total acc: 79.66%   [EVAL] batch:   51 | acc: 81.25%,  total acc: 79.69%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 79.95%   [EVAL] batch:   53 | acc: 68.75%,  total acc: 79.75%   [EVAL] batch:   54 | acc: 50.00%,  total acc: 79.20%   [EVAL] batch:   55 | acc: 81.25%,  total acc: 79.24%   [EVAL] batch:   56 | acc: 50.00%,  total acc: 78.73%   [EVAL] batch:   57 | acc: 25.00%,  total acc: 77.80%   [EVAL] batch:   58 | acc: 56.25%,  total acc: 77.44%   [EVAL] batch:   59 | acc: 68.75%,  total acc: 77.29%   [EVAL] batch:   60 | acc: 31.25%,  total acc: 76.54%   [EVAL] batch:   61 | acc: 37.50%,  total acc: 75.91%   [EVAL] batch:   62 | acc: 37.50%,  total acc: 75.30%   [EVAL] batch:   63 | acc: 56.25%,  total acc: 75.00%   [EVAL] batch:   64 | acc: 50.00%,  total acc: 74.62%   [EVAL] batch:   65 | acc: 18.75%,  total acc: 73.77%   [EVAL] batch:   66 | acc: 25.00%,  total acc: 73.04%   [EVAL] batch:   67 | acc: 62.50%,  total acc: 72.89%   [EVAL] batch:   68 | acc: 56.25%,  total acc: 72.64%   [EVAL] batch:   69 | acc: 93.75%,  total acc: 72.95%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 73.33%   [EVAL] batch:   71 | acc: 62.50%,  total acc: 73.18%   [EVAL] batch:   72 | acc: 81.25%,  total acc: 73.29%   [EVAL] batch:   73 | acc: 87.50%,  total acc: 73.48%   [EVAL] batch:   74 | acc: 75.00%,  total acc: 73.50%   [EVAL] batch:   75 | acc: 62.50%,  total acc: 73.36%   [EVAL] batch:   76 | acc: 25.00%,  total acc: 72.73%   [EVAL] batch:   77 | acc: 43.75%,  total acc: 72.36%   [EVAL] batch:   78 | acc: 62.50%,  total acc: 72.23%   [EVAL] batch:   79 | acc: 50.00%,  total acc: 71.95%   [EVAL] batch:   80 | acc: 56.25%,  total acc: 71.76%   [EVAL] batch:   81 | acc: 68.75%,  total acc: 71.72%   [EVAL] batch:   82 | acc: 62.50%,  total acc: 71.61%   [EVAL] batch:   83 | acc: 81.25%,  total acc: 71.73%   [EVAL] batch:   84 | acc: 68.75%,  total acc: 71.69%   [EVAL] batch:   85 | acc: 68.75%,  total acc: 71.66%   [EVAL] batch:   86 | acc: 56.25%,  total acc: 71.48%   [EVAL] batch:   87 | acc: 81.25%,  total acc: 71.59%   [EVAL] batch:   88 | acc: 81.25%,  total acc: 71.70%   [EVAL] batch:   89 | acc: 56.25%,  total acc: 71.53%   [EVAL] batch:   90 | acc: 56.25%,  total acc: 71.36%   [EVAL] batch:   91 | acc: 62.50%,  total acc: 71.26%   [EVAL] batch:   92 | acc: 62.50%,  total acc: 71.17%   [EVAL] batch:   93 | acc: 75.00%,  total acc: 71.21%   [EVAL] batch:   94 | acc: 68.75%,  total acc: 71.18%   [EVAL] batch:   95 | acc: 43.75%,  total acc: 70.90%   [EVAL] batch:   96 | acc: 56.25%,  total acc: 70.75%   [EVAL] batch:   97 | acc: 62.50%,  total acc: 70.66%   [EVAL] batch:   98 | acc: 75.00%,  total acc: 70.71%   [EVAL] batch:   99 | acc: 56.25%,  total acc: 70.56%   [EVAL] batch:  100 | acc: 50.00%,  total acc: 70.36%   [EVAL] batch:  101 | acc: 62.50%,  total acc: 70.28%   [EVAL] batch:  102 | acc: 56.25%,  total acc: 70.15%   [EVAL] batch:  103 | acc: 62.50%,  total acc: 70.07%   [EVAL] batch:  104 | acc: 68.75%,  total acc: 70.06%   [EVAL] batch:  105 | acc: 56.25%,  total acc: 69.93%   [EVAL] batch:  106 | acc: 43.75%,  total acc: 69.68%   [EVAL] batch:  107 | acc: 18.75%,  total acc: 69.21%   [EVAL] batch:  108 | acc: 37.50%,  total acc: 68.92%   [EVAL] batch:  109 | acc: 37.50%,  total acc: 68.64%   [EVAL] batch:  110 | acc: 31.25%,  total acc: 68.30%   [EVAL] batch:  111 | acc: 50.00%,  total acc: 68.14%   [EVAL] batch:  112 | acc: 68.75%,  total acc: 68.14%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 68.42%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 68.70%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 68.97%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 69.23%   [EVAL] batch:  117 | acc: 100.00%,  total acc: 69.49%   [EVAL] batch:  118 | acc: 93.75%,  total acc: 69.70%   [EVAL] batch:  119 | acc: 56.25%,  total acc: 69.58%   [EVAL] batch:  120 | acc: 37.50%,  total acc: 69.32%   [EVAL] batch:  121 | acc: 75.00%,  total acc: 69.36%   [EVAL] batch:  122 | acc: 68.75%,  total acc: 69.36%   [EVAL] batch:  123 | acc: 62.50%,  total acc: 69.30%   [EVAL] batch:  124 | acc: 62.50%,  total acc: 69.25%   [EVAL] batch:  125 | acc: 68.75%,  total acc: 69.25%   [EVAL] batch:  126 | acc: 62.50%,  total acc: 69.19%   [EVAL] batch:  127 | acc: 62.50%,  total acc: 69.14%   [EVAL] batch:  128 | acc: 68.75%,  total acc: 69.14%   [EVAL] batch:  129 | acc: 75.00%,  total acc: 69.18%   [EVAL] batch:  130 | acc: 68.75%,  total acc: 69.18%   [EVAL] batch:  131 | acc: 62.50%,  total acc: 69.13%   [EVAL] batch:  132 | acc: 25.00%,  total acc: 68.80%   [EVAL] batch:  133 | acc: 43.75%,  total acc: 68.61%   [EVAL] batch:  134 | acc: 50.00%,  total acc: 68.47%   [EVAL] batch:  135 | acc: 50.00%,  total acc: 68.34%   [EVAL] batch:  136 | acc: 43.75%,  total acc: 68.16%   [EVAL] batch:  137 | acc: 31.25%,  total acc: 67.89%   [EVAL] batch:  138 | acc: 12.50%,  total acc: 67.49%   [EVAL] batch:  139 | acc: 18.75%,  total acc: 67.14%   [EVAL] batch:  140 | acc: 25.00%,  total acc: 66.84%   [EVAL] batch:  141 | acc: 37.50%,  total acc: 66.64%   [EVAL] batch:  142 | acc: 37.50%,  total acc: 66.43%   [EVAL] batch:  143 | acc: 25.00%,  total acc: 66.15%   [EVAL] batch:  144 | acc: 93.75%,  total acc: 66.34%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 66.57%   [EVAL] batch:  146 | acc: 100.00%,  total acc: 66.79%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 67.02%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 67.24%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 67.42%   [EVAL] batch:  150 | acc: 37.50%,  total acc: 67.22%   [EVAL] batch:  151 | acc: 25.00%,  total acc: 66.94%   [EVAL] batch:  152 | acc: 25.00%,  total acc: 66.67%   [EVAL] batch:  153 | acc: 50.00%,  total acc: 66.56%   [EVAL] batch:  154 | acc: 31.25%,  total acc: 66.33%   [EVAL] batch:  155 | acc: 18.75%,  total acc: 66.03%   [EVAL] batch:  156 | acc: 81.25%,  total acc: 66.12%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 66.30%   [EVAL] batch:  158 | acc: 93.75%,  total acc: 66.47%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 66.64%   [EVAL] batch:  160 | acc: 87.50%,  total acc: 66.77%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 66.94%   [EVAL] batch:  162 | acc: 50.00%,  total acc: 66.83%   [EVAL] batch:  163 | acc: 18.75%,  total acc: 66.54%   [EVAL] batch:  164 | acc: 12.50%,  total acc: 66.21%   [EVAL] batch:  165 | acc: 6.25%,  total acc: 65.85%   [EVAL] batch:  166 | acc: 6.25%,  total acc: 65.49%   [EVAL] batch:  167 | acc: 18.75%,  total acc: 65.22%   [EVAL] batch:  168 | acc: 31.25%,  total acc: 65.01%   [EVAL] batch:  169 | acc: 37.50%,  total acc: 64.85%   [EVAL] batch:  170 | acc: 37.50%,  total acc: 64.69%   [EVAL] batch:  171 | acc: 43.75%,  total acc: 64.57%   [EVAL] batch:  172 | acc: 25.00%,  total acc: 64.34%   [EVAL] batch:  173 | acc: 43.75%,  total acc: 64.22%   [EVAL] batch:  174 | acc: 50.00%,  total acc: 64.14%   [EVAL] batch:  175 | acc: 37.50%,  total acc: 63.99%   [EVAL] batch:  176 | acc: 50.00%,  total acc: 63.91%   [EVAL] batch:  177 | acc: 68.75%,  total acc: 63.94%   [EVAL] batch:  178 | acc: 56.25%,  total acc: 63.90%   [EVAL] batch:  179 | acc: 43.75%,  total acc: 63.78%   [EVAL] batch:  180 | acc: 62.50%,  total acc: 63.78%   [EVAL] batch:  181 | acc: 37.50%,  total acc: 63.63%   [EVAL] batch:  182 | acc: 56.25%,  total acc: 63.59%   [EVAL] batch:  183 | acc: 75.00%,  total acc: 63.65%   [EVAL] batch:  184 | acc: 56.25%,  total acc: 63.61%   [EVAL] batch:  185 | acc: 56.25%,  total acc: 63.58%   [EVAL] batch:  186 | acc: 43.75%,  total acc: 63.47%   [EVAL] batch:  187 | acc: 81.25%,  total acc: 63.56%   [EVAL] batch:  188 | acc: 93.75%,  total acc: 63.72%   [EVAL] batch:  189 | acc: 93.75%,  total acc: 63.88%   [EVAL] batch:  190 | acc: 100.00%,  total acc: 64.07%   [EVAL] batch:  191 | acc: 81.25%,  total acc: 64.16%   [EVAL] batch:  192 | acc: 100.00%,  total acc: 64.35%   [EVAL] batch:  193 | acc: 87.50%,  total acc: 64.47%   [EVAL] batch:  194 | acc: 31.25%,  total acc: 64.29%   [EVAL] batch:  195 | acc: 31.25%,  total acc: 64.13%   [EVAL] batch:  196 | acc: 18.75%,  total acc: 63.90%   [EVAL] batch:  197 | acc: 25.00%,  total acc: 63.70%   [EVAL] batch:  198 | acc: 37.50%,  total acc: 63.57%   [EVAL] batch:  199 | acc: 18.75%,  total acc: 63.34%   [EVAL] batch:  200 | acc: 68.75%,  total acc: 63.37%   [EVAL] batch:  201 | acc: 68.75%,  total acc: 63.40%   [EVAL] batch:  202 | acc: 75.00%,  total acc: 63.45%   [EVAL] batch:  203 | acc: 56.25%,  total acc: 63.42%   [EVAL] batch:  204 | acc: 75.00%,  total acc: 63.48%   [EVAL] batch:  205 | acc: 68.75%,  total acc: 63.50%   [EVAL] batch:  206 | acc: 50.00%,  total acc: 63.44%   [EVAL] batch:  207 | acc: 12.50%,  total acc: 63.19%   [EVAL] batch:  208 | acc: 6.25%,  total acc: 62.92%   [EVAL] batch:  209 | acc: 25.00%,  total acc: 62.74%   [EVAL] batch:  210 | acc: 25.00%,  total acc: 62.56%   [EVAL] batch:  211 | acc: 12.50%,  total acc: 62.32%   [EVAL] batch:  212 | acc: 56.25%,  total acc: 62.29%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 62.47%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 62.65%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 62.82%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 62.99%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 63.16%   [EVAL] batch:  218 | acc: 93.75%,  total acc: 63.30%   [EVAL] batch:  219 | acc: 62.50%,  total acc: 63.30%   [EVAL] batch:  220 | acc: 81.25%,  total acc: 63.38%   [EVAL] batch:  221 | acc: 50.00%,  total acc: 63.32%   [EVAL] batch:  222 | acc: 56.25%,  total acc: 63.28%   [EVAL] batch:  223 | acc: 81.25%,  total acc: 63.36%   [EVAL] batch:  224 | acc: 37.50%,  total acc: 63.25%   [EVAL] batch:  225 | acc: 100.00%,  total acc: 63.41%   [EVAL] batch:  226 | acc: 100.00%,  total acc: 63.57%   [EVAL] batch:  227 | acc: 100.00%,  total acc: 63.73%   [EVAL] batch:  228 | acc: 93.75%,  total acc: 63.86%   [EVAL] batch:  229 | acc: 100.00%,  total acc: 64.02%   [EVAL] batch:  230 | acc: 81.25%,  total acc: 64.10%   [EVAL] batch:  231 | acc: 93.75%,  total acc: 64.22%   [EVAL] batch:  232 | acc: 100.00%,  total acc: 64.38%   [EVAL] batch:  233 | acc: 100.00%,  total acc: 64.53%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 64.68%   [EVAL] batch:  235 | acc: 100.00%,  total acc: 64.83%   [EVAL] batch:  236 | acc: 93.75%,  total acc: 64.95%   [EVAL] batch:  237 | acc: 75.00%,  total acc: 64.99%   [EVAL] batch:  238 | acc: 62.50%,  total acc: 64.98%   [EVAL] batch:  239 | acc: 87.50%,  total acc: 65.08%   [EVAL] batch:  240 | acc: 81.25%,  total acc: 65.15%   [EVAL] batch:  241 | acc: 68.75%,  total acc: 65.16%   [EVAL] batch:  242 | acc: 81.25%,  total acc: 65.23%   [EVAL] batch:  243 | acc: 81.25%,  total acc: 65.29%   [EVAL] batch:  244 | acc: 100.00%,  total acc: 65.43%   [EVAL] batch:  245 | acc: 87.50%,  total acc: 65.52%   [EVAL] batch:  246 | acc: 100.00%,  total acc: 65.66%   [EVAL] batch:  247 | acc: 100.00%,  total acc: 65.80%   [EVAL] batch:  248 | acc: 93.75%,  total acc: 65.91%   [EVAL] batch:  249 | acc: 87.50%,  total acc: 66.00%   [EVAL] batch:  250 | acc: 31.25%,  total acc: 65.86%   [EVAL] batch:  251 | acc: 62.50%,  total acc: 65.85%   [EVAL] batch:  252 | acc: 43.75%,  total acc: 65.76%   [EVAL] batch:  253 | acc: 25.00%,  total acc: 65.60%   [EVAL] batch:  254 | acc: 56.25%,  total acc: 65.56%   [EVAL] batch:  255 | acc: 50.00%,  total acc: 65.50%   [EVAL] batch:  256 | acc: 50.00%,  total acc: 65.44%   [EVAL] batch:  257 | acc: 75.00%,  total acc: 65.48%   [EVAL] batch:  258 | acc: 68.75%,  total acc: 65.49%   [EVAL] batch:  259 | acc: 50.00%,  total acc: 65.43%   [EVAL] batch:  260 | acc: 50.00%,  total acc: 65.37%   [EVAL] batch:  261 | acc: 68.75%,  total acc: 65.39%   [EVAL] batch:  262 | acc: 62.50%,  total acc: 65.38%   [EVAL] batch:  263 | acc: 43.75%,  total acc: 65.29%   [EVAL] batch:  264 | acc: 43.75%,  total acc: 65.21%   [EVAL] batch:  265 | acc: 37.50%,  total acc: 65.11%   [EVAL] batch:  266 | acc: 50.00%,  total acc: 65.05%   [EVAL] batch:  267 | acc: 43.75%,  total acc: 64.97%   [EVAL] batch:  268 | acc: 87.50%,  total acc: 65.06%   [EVAL] batch:  269 | acc: 93.75%,  total acc: 65.16%   [EVAL] batch:  270 | acc: 100.00%,  total acc: 65.29%   [EVAL] batch:  271 | acc: 100.00%,  total acc: 65.42%   [EVAL] batch:  272 | acc: 100.00%,  total acc: 65.54%   [EVAL] batch:  273 | acc: 93.75%,  total acc: 65.65%   [EVAL] batch:  274 | acc: 100.00%,  total acc: 65.77%   [EVAL] batch:  275 | acc: 81.25%,  total acc: 65.83%   [EVAL] batch:  276 | acc: 43.75%,  total acc: 65.75%   [EVAL] batch:  277 | acc: 62.50%,  total acc: 65.74%   [EVAL] batch:  278 | acc: 50.00%,  total acc: 65.68%   [EVAL] batch:  279 | acc: 62.50%,  total acc: 65.67%   [EVAL] batch:  280 | acc: 50.00%,  total acc: 65.61%   [EVAL] batch:  281 | acc: 56.25%,  total acc: 65.58%   [EVAL] batch:  282 | acc: 50.00%,  total acc: 65.53%   [EVAL] batch:  283 | acc: 31.25%,  total acc: 65.40%   [EVAL] batch:  284 | acc: 43.75%,  total acc: 65.33%   [EVAL] batch:  285 | acc: 25.00%,  total acc: 65.19%   [EVAL] batch:  286 | acc: 18.75%,  total acc: 65.03%   [EVAL] batch:  287 | acc: 50.00%,  total acc: 64.97%   [EVAL] batch:  288 | acc: 43.75%,  total acc: 64.90%   [EVAL] batch:  289 | acc: 43.75%,  total acc: 64.83%   [EVAL] batch:  290 | acc: 62.50%,  total acc: 64.82%   [EVAL] batch:  291 | acc: 56.25%,  total acc: 64.79%   [EVAL] batch:  292 | acc: 68.75%,  total acc: 64.80%   [EVAL] batch:  293 | acc: 62.50%,  total acc: 64.80%   [EVAL] batch:  294 | acc: 68.75%,  total acc: 64.81%   [EVAL] batch:  295 | acc: 68.75%,  total acc: 64.82%   [EVAL] batch:  296 | acc: 56.25%,  total acc: 64.79%   [EVAL] batch:  297 | acc: 56.25%,  total acc: 64.77%   [EVAL] batch:  298 | acc: 62.50%,  total acc: 64.76%   [EVAL] batch:  299 | acc: 75.00%,  total acc: 64.79%   [EVAL] batch:  300 | acc: 100.00%,  total acc: 64.91%   [EVAL] batch:  301 | acc: 100.00%,  total acc: 65.02%   [EVAL] batch:  302 | acc: 100.00%,  total acc: 65.14%   [EVAL] batch:  303 | acc: 100.00%,  total acc: 65.25%   [EVAL] batch:  304 | acc: 100.00%,  total acc: 65.37%   [EVAL] batch:  305 | acc: 100.00%,  total acc: 65.48%   [EVAL] batch:  306 | acc: 100.00%,  total acc: 65.59%   [EVAL] batch:  307 | acc: 87.50%,  total acc: 65.67%   [EVAL] batch:  308 | acc: 87.50%,  total acc: 65.74%   [EVAL] batch:  309 | acc: 93.75%,  total acc: 65.83%   [EVAL] batch:  310 | acc: 93.75%,  total acc: 65.92%   [EVAL] batch:  311 | acc: 100.00%,  total acc: 66.03%   [EVAL] batch:  312 | acc: 68.75%,  total acc: 66.03%   [EVAL] batch:  313 | acc: 18.75%,  total acc: 65.88%   [EVAL] batch:  314 | acc: 25.00%,  total acc: 65.75%   [EVAL] batch:  315 | acc: 25.00%,  total acc: 65.62%   [EVAL] batch:  316 | acc: 25.00%,  total acc: 65.50%   [EVAL] batch:  317 | acc: 18.75%,  total acc: 65.35%   [EVAL] batch:  318 | acc: 25.00%,  total acc: 65.22%   [EVAL] batch:  319 | acc: 75.00%,  total acc: 65.25%   [EVAL] batch:  320 | acc: 50.00%,  total acc: 65.21%   [EVAL] batch:  321 | acc: 75.00%,  total acc: 65.24%   [EVAL] batch:  322 | acc: 87.50%,  total acc: 65.31%   [EVAL] batch:  323 | acc: 68.75%,  total acc: 65.32%   [EVAL] batch:  324 | acc: 75.00%,  total acc: 65.35%   [EVAL] batch:  325 | acc: 25.00%,  total acc: 65.22%   [EVAL] batch:  326 | acc: 25.00%,  total acc: 65.10%   [EVAL] batch:  327 | acc: 25.00%,  total acc: 64.98%   [EVAL] batch:  328 | acc: 37.50%,  total acc: 64.89%   [EVAL] batch:  329 | acc: 50.00%,  total acc: 64.85%   [EVAL] batch:  330 | acc: 25.00%,  total acc: 64.73%   [EVAL] batch:  331 | acc: 81.25%,  total acc: 64.78%   [EVAL] batch:  332 | acc: 100.00%,  total acc: 64.88%   [EVAL] batch:  333 | acc: 100.00%,  total acc: 64.99%   [EVAL] batch:  334 | acc: 93.75%,  total acc: 65.07%   [EVAL] batch:  335 | acc: 100.00%,  total acc: 65.18%   [EVAL] batch:  336 | acc: 100.00%,  total acc: 65.28%   [EVAL] batch:  337 | acc: 75.00%,  total acc: 65.31%   [EVAL] batch:  338 | acc: 93.75%,  total acc: 65.39%   [EVAL] batch:  339 | acc: 93.75%,  total acc: 65.48%   [EVAL] batch:  340 | acc: 100.00%,  total acc: 65.58%   [EVAL] batch:  341 | acc: 87.50%,  total acc: 65.64%   [EVAL] batch:  342 | acc: 100.00%,  total acc: 65.74%   [EVAL] batch:  343 | acc: 87.50%,  total acc: 65.81%   [EVAL] batch:  344 | acc: 100.00%,  total acc: 65.91%   [EVAL] batch:  345 | acc: 100.00%,  total acc: 66.00%   [EVAL] batch:  346 | acc: 87.50%,  total acc: 66.07%   [EVAL] batch:  347 | acc: 93.75%,  total acc: 66.15%   [EVAL] batch:  348 | acc: 93.75%,  total acc: 66.22%   [EVAL] batch:  349 | acc: 100.00%,  total acc: 66.32%   [EVAL] batch:  350 | acc: 18.75%,  total acc: 66.19%   [EVAL] batch:  351 | acc: 0.00%,  total acc: 66.00%   [EVAL] batch:  352 | acc: 0.00%,  total acc: 65.81%   [EVAL] batch:  353 | acc: 6.25%,  total acc: 65.64%   [EVAL] batch:  354 | acc: 0.00%,  total acc: 65.46%   [EVAL] batch:  355 | acc: 18.75%,  total acc: 65.33%   [EVAL] batch:  356 | acc: 75.00%,  total acc: 65.35%   [EVAL] batch:  357 | acc: 81.25%,  total acc: 65.40%   [EVAL] batch:  358 | acc: 100.00%,  total acc: 65.49%   [EVAL] batch:  359 | acc: 93.75%,  total acc: 65.57%   [EVAL] batch:  360 | acc: 68.75%,  total acc: 65.58%   [EVAL] batch:  361 | acc: 68.75%,  total acc: 65.59%   [EVAL] batch:  362 | acc: 56.25%,  total acc: 65.56%   [EVAL] batch:  363 | acc: 68.75%,  total acc: 65.57%   [EVAL] batch:  364 | acc: 56.25%,  total acc: 65.55%   [EVAL] batch:  365 | acc: 56.25%,  total acc: 65.52%   [EVAL] batch:  366 | acc: 62.50%,  total acc: 65.51%   [EVAL] batch:  367 | acc: 62.50%,  total acc: 65.51%   [EVAL] batch:  368 | acc: 37.50%,  total acc: 65.43%   [EVAL] batch:  369 | acc: 75.00%,  total acc: 65.46%   [EVAL] batch:  370 | acc: 62.50%,  total acc: 65.45%   [EVAL] batch:  371 | acc: 43.75%,  total acc: 65.39%   [EVAL] batch:  372 | acc: 81.25%,  total acc: 65.43%   [EVAL] batch:  373 | acc: 93.75%,  total acc: 65.51%   [EVAL] batch:  374 | acc: 68.75%,  total acc: 65.52%   [EVAL] batch:  375 | acc: 25.00%,  total acc: 65.41%   [EVAL] batch:  376 | acc: 37.50%,  total acc: 65.33%   [EVAL] batch:  377 | acc: 6.25%,  total acc: 65.18%   [EVAL] batch:  378 | acc: 31.25%,  total acc: 65.09%   [EVAL] batch:  379 | acc: 6.25%,  total acc: 64.93%   [EVAL] batch:  380 | acc: 18.75%,  total acc: 64.81%   [EVAL] batch:  381 | acc: 68.75%,  total acc: 64.82%   [EVAL] batch:  382 | acc: 81.25%,  total acc: 64.87%   [EVAL] batch:  383 | acc: 100.00%,  total acc: 64.96%   [EVAL] batch:  384 | acc: 87.50%,  total acc: 65.02%   [EVAL] batch:  385 | acc: 81.25%,  total acc: 65.06%   [EVAL] batch:  386 | acc: 81.25%,  total acc: 65.10%   [EVAL] batch:  387 | acc: 87.50%,  total acc: 65.16%   [EVAL] batch:  388 | acc: 68.75%,  total acc: 65.17%   [EVAL] batch:  389 | acc: 68.75%,  total acc: 65.18%   [EVAL] batch:  390 | acc: 75.00%,  total acc: 65.20%   [EVAL] batch:  391 | acc: 87.50%,  total acc: 65.26%   [EVAL] batch:  392 | acc: 93.75%,  total acc: 65.33%   [EVAL] batch:  393 | acc: 50.00%,  total acc: 65.29%   [EVAL] batch:  394 | acc: 31.25%,  total acc: 65.21%   [EVAL] batch:  395 | acc: 31.25%,  total acc: 65.12%   [EVAL] batch:  396 | acc: 50.00%,  total acc: 65.08%   [EVAL] batch:  397 | acc: 56.25%,  total acc: 65.06%   [EVAL] batch:  398 | acc: 31.25%,  total acc: 64.97%   [EVAL] batch:  399 | acc: 43.75%,  total acc: 64.92%   [EVAL] batch:  400 | acc: 0.00%,  total acc: 64.76%   [EVAL] batch:  401 | acc: 12.50%,  total acc: 64.63%   [EVAL] batch:  402 | acc: 12.50%,  total acc: 64.50%   [EVAL] batch:  403 | acc: 0.00%,  total acc: 64.34%   [EVAL] batch:  404 | acc: 6.25%,  total acc: 64.20%   [EVAL] batch:  405 | acc: 6.25%,  total acc: 64.05%   [EVAL] batch:  406 | acc: 62.50%,  total acc: 64.05%   [EVAL] batch:  407 | acc: 93.75%,  total acc: 64.12%   [EVAL] batch:  408 | acc: 81.25%,  total acc: 64.17%   [EVAL] batch:  409 | acc: 93.75%,  total acc: 64.24%   [EVAL] batch:  410 | acc: 93.75%,  total acc: 64.31%   [EVAL] batch:  411 | acc: 81.25%,  total acc: 64.35%   [EVAL] batch:  412 | acc: 100.00%,  total acc: 64.44%   [EVAL] batch:  413 | acc: 87.50%,  total acc: 64.49%   [EVAL] batch:  414 | acc: 100.00%,  total acc: 64.58%   [EVAL] batch:  415 | acc: 87.50%,  total acc: 64.63%   [EVAL] batch:  416 | acc: 81.25%,  total acc: 64.67%   [EVAL] batch:  417 | acc: 87.50%,  total acc: 64.73%   [EVAL] batch:  418 | acc: 93.75%,  total acc: 64.80%   [EVAL] batch:  419 | acc: 43.75%,  total acc: 64.75%   [EVAL] batch:  420 | acc: 18.75%,  total acc: 64.64%   [EVAL] batch:  421 | acc: 31.25%,  total acc: 64.56%   [EVAL] batch:  422 | acc: 18.75%,  total acc: 64.45%   [EVAL] batch:  423 | acc: 43.75%,  total acc: 64.40%   [EVAL] batch:  424 | acc: 37.50%,  total acc: 64.34%   [EVAL] batch:  425 | acc: 25.00%,  total acc: 64.25%   [EVAL] batch:  426 | acc: 62.50%,  total acc: 64.24%   [EVAL] batch:  427 | acc: 50.00%,  total acc: 64.21%   [EVAL] batch:  428 | acc: 68.75%,  total acc: 64.22%   [EVAL] batch:  429 | acc: 56.25%,  total acc: 64.20%   [EVAL] batch:  430 | acc: 68.75%,  total acc: 64.21%   [EVAL] batch:  431 | acc: 37.50%,  total acc: 64.15%   [EVAL] batch:  432 | acc: 56.25%,  total acc: 64.13%   [EVAL] batch:  433 | acc: 37.50%,  total acc: 64.07%   [EVAL] batch:  434 | acc: 56.25%,  total acc: 64.05%   [EVAL] batch:  435 | acc: 50.00%,  total acc: 64.02%   [EVAL] batch:  436 | acc: 81.25%,  total acc: 64.06%   [EVAL] batch:  437 | acc: 43.75%,  total acc: 64.01%   [EVAL] batch:  438 | acc: 68.75%,  total acc: 64.02%   [EVAL] batch:  439 | acc: 75.00%,  total acc: 64.05%   [EVAL] batch:  440 | acc: 87.50%,  total acc: 64.10%   [EVAL] batch:  441 | acc: 62.50%,  total acc: 64.10%   [EVAL] batch:  442 | acc: 75.00%,  total acc: 64.12%   [EVAL] batch:  443 | acc: 68.75%,  total acc: 64.13%   [EVAL] batch:  444 | acc: 81.25%,  total acc: 64.17%   [EVAL] batch:  445 | acc: 75.00%,  total acc: 64.20%   [EVAL] batch:  446 | acc: 68.75%,  total acc: 64.21%   [EVAL] batch:  447 | acc: 93.75%,  total acc: 64.27%   [EVAL] batch:  448 | acc: 75.00%,  total acc: 64.30%   [EVAL] batch:  449 | acc: 81.25%,  total acc: 64.33%   [EVAL] batch:  450 | acc: 62.50%,  total acc: 64.33%   [EVAL] batch:  451 | acc: 75.00%,  total acc: 64.35%   [EVAL] batch:  452 | acc: 62.50%,  total acc: 64.35%   [EVAL] batch:  453 | acc: 50.00%,  total acc: 64.32%   [EVAL] batch:  454 | acc: 50.00%,  total acc: 64.29%   [EVAL] batch:  455 | acc: 75.00%,  total acc: 64.31%   [EVAL] batch:  456 | acc: 50.00%,  total acc: 64.28%   [EVAL] batch:  457 | acc: 62.50%,  total acc: 64.27%   [EVAL] batch:  458 | acc: 56.25%,  total acc: 64.26%   [EVAL] batch:  459 | acc: 68.75%,  total acc: 64.27%   [EVAL] batch:  460 | acc: 68.75%,  total acc: 64.28%   [EVAL] batch:  461 | acc: 68.75%,  total acc: 64.29%   [EVAL] batch:  462 | acc: 81.25%,  total acc: 64.32%   [EVAL] batch:  463 | acc: 100.00%,  total acc: 64.40%   [EVAL] batch:  464 | acc: 100.00%,  total acc: 64.48%   [EVAL] batch:  465 | acc: 93.75%,  total acc: 64.54%   [EVAL] batch:  466 | acc: 100.00%,  total acc: 64.61%   [EVAL] batch:  467 | acc: 100.00%,  total acc: 64.69%   [EVAL] batch:  468 | acc: 93.75%,  total acc: 64.75%   [EVAL] batch:  469 | acc: 100.00%,  total acc: 64.83%   [EVAL] batch:  470 | acc: 100.00%,  total acc: 64.90%   [EVAL] batch:  471 | acc: 100.00%,  total acc: 64.98%   [EVAL] batch:  472 | acc: 100.00%,  total acc: 65.05%   [EVAL] batch:  473 | acc: 100.00%,  total acc: 65.12%   [EVAL] batch:  474 | acc: 100.00%,  total acc: 65.20%   [EVAL] batch:  475 | acc: 75.00%,  total acc: 65.22%   [EVAL] batch:  476 | acc: 81.25%,  total acc: 65.25%   [EVAL] batch:  477 | acc: 81.25%,  total acc: 65.29%   [EVAL] batch:  478 | acc: 75.00%,  total acc: 65.31%   [EVAL] batch:  479 | acc: 100.00%,  total acc: 65.38%   [EVAL] batch:  480 | acc: 68.75%,  total acc: 65.38%   [EVAL] batch:  481 | acc: 75.00%,  total acc: 65.40%   [EVAL] batch:  482 | acc: 25.00%,  total acc: 65.32%   [EVAL] batch:  483 | acc: 68.75%,  total acc: 65.33%   [EVAL] batch:  484 | acc: 43.75%,  total acc: 65.28%   [EVAL] batch:  485 | acc: 62.50%,  total acc: 65.28%   [EVAL] batch:  486 | acc: 56.25%,  total acc: 65.26%   [EVAL] batch:  487 | acc: 93.75%,  total acc: 65.32%   [EVAL] batch:  488 | acc: 87.50%,  total acc: 65.36%   [EVAL] batch:  489 | acc: 100.00%,  total acc: 65.43%   [EVAL] batch:  490 | acc: 87.50%,  total acc: 65.48%   [EVAL] batch:  491 | acc: 87.50%,  total acc: 65.52%   [EVAL] batch:  492 | acc: 87.50%,  total acc: 65.57%   [EVAL] batch:  493 | acc: 68.75%,  total acc: 65.57%   [EVAL] batch:  494 | acc: 25.00%,  total acc: 65.49%   [EVAL] batch:  495 | acc: 18.75%,  total acc: 65.40%   [EVAL] batch:  496 | acc: 25.00%,  total acc: 65.32%   [EVAL] batch:  497 | acc: 31.25%,  total acc: 65.25%   [EVAL] batch:  498 | acc: 18.75%,  total acc: 65.16%   [EVAL] batch:  499 | acc: 31.25%,  total acc: 65.09%   
cur_acc:  ['0.9494', '0.7302', '0.7500', '0.7956', '0.6925', '0.6875', '0.5823', '0.7192']
his_acc:  ['0.9494', '0.8310', '0.7969', '0.7835', '0.7592', '0.7165', '0.6808', '0.6509']
--------Round  1
seed:  200
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hybridprompt_12token/train.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hybridprompt_12token/valid.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hybridprompt_12token/test.pkl
Task_order: [7 6 3 2 4 0 5 1]
prepared data!
Clustering into  4  clusters
Clusters:  [0 3 2 0 1 1 2 0 3 3]
Losses:  19.73458480834961 5.4493231773376465 1.2393338680267334
CurrentTrain: epoch  0, batch     0 | loss: 19.7345848Losses:  20.853212356567383 6.997264862060547 1.171019434928894
CurrentTrain: epoch  0, batch     1 | loss: 20.8532124Losses:  17.6872615814209 3.968393325805664 1.192124843597412
CurrentTrain: epoch  0, batch     2 | loss: 17.6872616Losses:  19.309551239013672 5.827575206756592 1.0655245780944824
CurrentTrain: epoch  0, batch     3 | loss: 19.3095512Losses:  21.024032592773438 7.788888931274414 1.0532697439193726
CurrentTrain: epoch  0, batch     4 | loss: 21.0240326Losses:  20.480552673339844 7.272534370422363 1.0827386379241943
CurrentTrain: epoch  0, batch     5 | loss: 20.4805527Losses:  21.32733726501465 7.942826747894287 0.9220609068870544
CurrentTrain: epoch  0, batch     6 | loss: 21.3273373Losses:  19.902233123779297 7.0141191482543945 0.9551997780799866
CurrentTrain: epoch  0, batch     7 | loss: 19.9022331Losses:  16.769495010375977 4.410144805908203 0.9305459260940552
CurrentTrain: epoch  0, batch     8 | loss: 16.7694950Losses:  18.388391494750977 6.020556449890137 0.744046151638031
CurrentTrain: epoch  0, batch     9 | loss: 18.3883915Losses:  19.69575309753418 7.44028377532959 0.694528341293335
CurrentTrain: epoch  0, batch    10 | loss: 19.6957531Losses:  17.459909439086914 5.292811870574951 0.8757960796356201
CurrentTrain: epoch  0, batch    11 | loss: 17.4599094Losses:  17.70639991760254 6.175081729888916 0.7602161169052124
CurrentTrain: epoch  0, batch    12 | loss: 17.7063999Losses:  16.441570281982422 4.699718475341797 0.7402058839797974
CurrentTrain: epoch  0, batch    13 | loss: 16.4415703Losses:  19.33187484741211 7.563366889953613 0.7080034613609314
CurrentTrain: epoch  0, batch    14 | loss: 19.3318748Losses:  16.38982582092285 5.002190589904785 0.668249249458313
CurrentTrain: epoch  0, batch    15 | loss: 16.3898258Losses:  15.645146369934082 4.787038803100586 0.6372890472412109
CurrentTrain: epoch  0, batch    16 | loss: 15.6451464Losses:  16.62141227722168 5.03706693649292 0.6457260847091675
CurrentTrain: epoch  0, batch    17 | loss: 16.6214123Losses:  14.958609580993652 4.143280029296875 0.5967693328857422
CurrentTrain: epoch  0, batch    18 | loss: 14.9586096Losses:  14.525544166564941 3.562681198120117 0.5779004693031311
CurrentTrain: epoch  0, batch    19 | loss: 14.5255442Losses:  20.494556427001953 8.97147274017334 0.6480748653411865
CurrentTrain: epoch  0, batch    20 | loss: 20.4945564Losses:  15.902290344238281 4.99485969543457 0.5541006326675415
CurrentTrain: epoch  0, batch    21 | loss: 15.9022903Losses:  17.48379898071289 6.406434535980225 0.5815973281860352
CurrentTrain: epoch  0, batch    22 | loss: 17.4837990Losses:  16.648773193359375 5.645573616027832 0.5257903337478638
CurrentTrain: epoch  0, batch    23 | loss: 16.6487732Losses:  15.23292350769043 4.154087066650391 0.5619280934333801
CurrentTrain: epoch  0, batch    24 | loss: 15.2329235Losses:  17.472320556640625 5.9198527336120605 0.5411983728408813
CurrentTrain: epoch  0, batch    25 | loss: 17.4723206Losses:  16.70499038696289 5.20070743560791 0.5012404918670654
CurrentTrain: epoch  0, batch    26 | loss: 16.7049904Losses:  14.755820274353027 4.745161056518555 0.5041841268539429
CurrentTrain: epoch  0, batch    27 | loss: 14.7558203Losses:  14.597959518432617 3.9904260635375977 0.532475471496582
CurrentTrain: epoch  0, batch    28 | loss: 14.5979595Losses:  16.298004150390625 5.237903118133545 0.5334856510162354
CurrentTrain: epoch  0, batch    29 | loss: 16.2980042Losses:  15.175704956054688 4.837563991546631 0.5221997499465942
CurrentTrain: epoch  0, batch    30 | loss: 15.1757050Losses:  18.22746467590332 7.733196258544922 0.517697811126709
CurrentTrain: epoch  0, batch    31 | loss: 18.2274647Losses:  18.42522621154785 7.964362144470215 0.3457275629043579
CurrentTrain: epoch  0, batch    32 | loss: 18.4252262Losses:  15.181391716003418 4.410909652709961 0.5253533720970154
CurrentTrain: epoch  0, batch    33 | loss: 15.1813917Losses:  14.102866172790527 3.3943233489990234 0.49286597967147827
CurrentTrain: epoch  0, batch    34 | loss: 14.1028662Losses:  15.154257774353027 4.506902694702148 0.5874494910240173
CurrentTrain: epoch  0, batch    35 | loss: 15.1542578Losses:  16.13595199584961 6.077713489532471 0.4821423590183258
CurrentTrain: epoch  0, batch    36 | loss: 16.1359520Losses:  13.210916519165039 3.938109874725342 0.46276330947875977
CurrentTrain: epoch  0, batch    37 | loss: 13.2109165Losses:  15.511014938354492 5.203335285186768 0.5349235534667969
CurrentTrain: epoch  0, batch    38 | loss: 15.5110149Losses:  13.641918182373047 3.737607955932617 0.4649120271205902
CurrentTrain: epoch  0, batch    39 | loss: 13.6419182Losses:  16.1529483795166 6.726139068603516 0.48100194334983826
CurrentTrain: epoch  0, batch    40 | loss: 16.1529484Losses:  12.242056846618652 2.6424312591552734 0.5116569399833679
CurrentTrain: epoch  0, batch    41 | loss: 12.2420568Losses:  14.319218635559082 4.643400192260742 0.4574517011642456
CurrentTrain: epoch  0, batch    42 | loss: 14.3192186Losses:  15.230649948120117 5.527923583984375 0.5022557973861694
CurrentTrain: epoch  0, batch    43 | loss: 15.2306499Losses:  13.871442794799805 4.507922649383545 0.4615626931190491
CurrentTrain: epoch  0, batch    44 | loss: 13.8714428Losses:  14.199012756347656 5.3693389892578125 0.41006892919540405
CurrentTrain: epoch  0, batch    45 | loss: 14.1990128Losses:  15.097989082336426 5.226927757263184 0.4769412577152252
CurrentTrain: epoch  0, batch    46 | loss: 15.0979891Losses:  13.289247512817383 4.442366600036621 0.4249706566333771
CurrentTrain: epoch  0, batch    47 | loss: 13.2892475Losses:  14.960884094238281 6.000391960144043 0.3997681438922882
CurrentTrain: epoch  0, batch    48 | loss: 14.9608841Losses:  13.511457443237305 4.263354301452637 0.4337153136730194
CurrentTrain: epoch  0, batch    49 | loss: 13.5114574Losses:  11.778351783752441 2.670069456100464 0.40993958711624146
CurrentTrain: epoch  0, batch    50 | loss: 11.7783518Losses:  13.762324333190918 4.492632865905762 0.31967923045158386
CurrentTrain: epoch  0, batch    51 | loss: 13.7623243Losses:  14.564637184143066 5.429318904876709 0.38845330476760864
CurrentTrain: epoch  0, batch    52 | loss: 14.5646372Losses:  13.184168815612793 4.945281028747559 0.4289136230945587
CurrentTrain: epoch  0, batch    53 | loss: 13.1841688Losses:  12.758544921875 4.30843448638916 0.3894246220588684
CurrentTrain: epoch  0, batch    54 | loss: 12.7585449Losses:  12.812274932861328 3.4184465408325195 0.448880672454834
CurrentTrain: epoch  0, batch    55 | loss: 12.8122749Losses:  13.193626403808594 3.991969108581543 0.39450445771217346
CurrentTrain: epoch  0, batch    56 | loss: 13.1936264Losses:  14.022577285766602 5.281106948852539 0.38991302251815796
CurrentTrain: epoch  0, batch    57 | loss: 14.0225773Losses:  16.18893814086914 7.815629005432129 0.26964664459228516
CurrentTrain: epoch  0, batch    58 | loss: 16.1889381Losses:  12.750560760498047 4.494771957397461 0.4034513533115387
CurrentTrain: epoch  0, batch    59 | loss: 12.7505608Losses:  14.296048164367676 5.083861351013184 0.41752299666404724
CurrentTrain: epoch  0, batch    60 | loss: 14.2960482Losses:  16.01735496520996 6.686805725097656 0.39243361353874207
CurrentTrain: epoch  0, batch    61 | loss: 16.0173550Losses:  9.13589096069336 0.8340451717376709 0.3875572979450226
CurrentTrain: epoch  0, batch    62 | loss: 9.1358910Losses:  14.999427795410156 5.973860263824463 0.40679553151130676
CurrentTrain: epoch  1, batch     0 | loss: 14.9994278Losses:  11.986614227294922 3.2364068031311035 0.3705998659133911
CurrentTrain: epoch  1, batch     1 | loss: 11.9866142Losses:  11.748444557189941 4.69017219543457 0.35313543677330017
CurrentTrain: epoch  1, batch     2 | loss: 11.7484446Losses:  13.126004219055176 5.331625461578369 0.3526526689529419
CurrentTrain: epoch  1, batch     3 | loss: 13.1260042Losses:  13.041399002075195 4.9123640060424805 0.4072880446910858
CurrentTrain: epoch  1, batch     4 | loss: 13.0413990Losses:  14.74563980102539 5.8960371017456055 0.37487727403640747
CurrentTrain: epoch  1, batch     5 | loss: 14.7456398Losses:  12.068568229675293 3.928100109100342 0.39100515842437744
CurrentTrain: epoch  1, batch     6 | loss: 12.0685682Losses:  12.08774185180664 3.9867329597473145 0.39033904671669006
CurrentTrain: epoch  1, batch     7 | loss: 12.0877419Losses:  15.77660083770752 7.467587471008301 0.3606172800064087
CurrentTrain: epoch  1, batch     8 | loss: 15.7766008Losses:  10.560808181762695 3.0710015296936035 0.3418733477592468
CurrentTrain: epoch  1, batch     9 | loss: 10.5608082Losses:  13.117447853088379 4.742945671081543 0.38068607449531555
CurrentTrain: epoch  1, batch    10 | loss: 13.1174479Losses:  13.203316688537598 4.832845687866211 0.36878883838653564
CurrentTrain: epoch  1, batch    11 | loss: 13.2033167Losses:  11.157448768615723 3.2629802227020264 0.34372642636299133
CurrentTrain: epoch  1, batch    12 | loss: 11.1574488Losses:  10.796486854553223 2.853764057159424 0.40615785121917725
CurrentTrain: epoch  1, batch    13 | loss: 10.7964869Losses:  13.368688583374023 5.175529479980469 0.34644365310668945
CurrentTrain: epoch  1, batch    14 | loss: 13.3686886Losses:  12.905709266662598 4.481532096862793 0.3601454794406891
CurrentTrain: epoch  1, batch    15 | loss: 12.9057093Losses:  10.421693801879883 2.922032117843628 0.3489740192890167
CurrentTrain: epoch  1, batch    16 | loss: 10.4216938Losses:  12.812088966369629 6.1397271156311035 0.33433324098587036
CurrentTrain: epoch  1, batch    17 | loss: 12.8120890Losses:  12.882183074951172 4.902756690979004 0.3716107904911041
CurrentTrain: epoch  1, batch    18 | loss: 12.8821831Losses:  11.655012130737305 4.241064071655273 0.35559868812561035
CurrentTrain: epoch  1, batch    19 | loss: 11.6550121Losses:  10.561060905456543 3.510735034942627 0.33169856667518616
CurrentTrain: epoch  1, batch    20 | loss: 10.5610609Losses:  13.561511039733887 5.676628112792969 0.3963264226913452
CurrentTrain: epoch  1, batch    21 | loss: 13.5615110Losses:  11.005912780761719 3.0898020267486572 0.3650653064250946
CurrentTrain: epoch  1, batch    22 | loss: 11.0059128Losses:  10.326512336730957 3.07914400100708 0.35800814628601074
CurrentTrain: epoch  1, batch    23 | loss: 10.3265123Losses:  13.137295722961426 5.263499736785889 0.3693113327026367
CurrentTrain: epoch  1, batch    24 | loss: 13.1372957Losses:  9.90445613861084 2.1953065395355225 0.3430432677268982
CurrentTrain: epoch  1, batch    25 | loss: 9.9044561Losses:  11.423487663269043 3.688108444213867 0.3399263024330139
CurrentTrain: epoch  1, batch    26 | loss: 11.4234877Losses:  15.146798133850098 7.44992733001709 0.32846885919570923
CurrentTrain: epoch  1, batch    27 | loss: 15.1467981Losses:  10.579697608947754 3.1854021549224854 0.3720034956932068
CurrentTrain: epoch  1, batch    28 | loss: 10.5796976Losses:  11.786511421203613 3.891533136367798 0.34927481412887573
CurrentTrain: epoch  1, batch    29 | loss: 11.7865114Losses:  9.353477478027344 2.1435108184814453 0.3314477801322937
CurrentTrain: epoch  1, batch    30 | loss: 9.3534775Losses:  10.047591209411621 2.9706311225891113 0.33585745096206665
CurrentTrain: epoch  1, batch    31 | loss: 10.0475912Losses:  10.508870124816895 3.5284016132354736 0.3280797600746155
CurrentTrain: epoch  1, batch    32 | loss: 10.5088701Losses:  10.425833702087402 2.9286844730377197 0.3142004609107971
CurrentTrain: epoch  1, batch    33 | loss: 10.4258337Losses:  11.745392799377441 5.196540832519531 0.3458977937698364
CurrentTrain: epoch  1, batch    34 | loss: 11.7453928Losses:  10.253643035888672 3.3440942764282227 0.31940558552742004
CurrentTrain: epoch  1, batch    35 | loss: 10.2536430Losses:  10.3803129196167 2.4028995037078857 0.3362826704978943
CurrentTrain: epoch  1, batch    36 | loss: 10.3803129Losses:  10.740555763244629 4.250251770019531 0.3578045666217804
CurrentTrain: epoch  1, batch    37 | loss: 10.7405558Losses:  11.422775268554688 3.5904688835144043 0.3431244492530823
CurrentTrain: epoch  1, batch    38 | loss: 11.4227753Losses:  9.582473754882812 2.454164743423462 0.32307395339012146
CurrentTrain: epoch  1, batch    39 | loss: 9.5824738Losses:  11.535775184631348 4.1760711669921875 0.3284711241722107
CurrentTrain: epoch  1, batch    40 | loss: 11.5357752Losses:  11.759400367736816 3.3227272033691406 0.3395644724369049
CurrentTrain: epoch  1, batch    41 | loss: 11.7594004Losses:  9.755047798156738 3.04909086227417 0.34678319096565247
CurrentTrain: epoch  1, batch    42 | loss: 9.7550478Losses:  14.282265663146973 7.325417518615723 0.34600943326950073
CurrentTrain: epoch  1, batch    43 | loss: 14.2822657Losses:  10.40070915222168 3.4211971759796143 0.34272944927215576
CurrentTrain: epoch  1, batch    44 | loss: 10.4007092Losses:  9.367298126220703 2.765665054321289 0.305348664522171
CurrentTrain: epoch  1, batch    45 | loss: 9.3672981Losses:  10.486350059509277 3.2502083778381348 0.33648186922073364
CurrentTrain: epoch  1, batch    46 | loss: 10.4863501Losses:  9.746881484985352 3.3405368328094482 0.298198938369751
CurrentTrain: epoch  1, batch    47 | loss: 9.7468815Losses:  11.317777633666992 3.8511242866516113 0.31665271520614624
CurrentTrain: epoch  1, batch    48 | loss: 11.3177776Losses:  11.33913516998291 4.85036039352417 0.3113604784011841
CurrentTrain: epoch  1, batch    49 | loss: 11.3391352Losses:  11.161757469177246 3.4556126594543457 0.31079357862472534
CurrentTrain: epoch  1, batch    50 | loss: 11.1617575Losses:  13.234668731689453 5.6817474365234375 0.2482549399137497
CurrentTrain: epoch  1, batch    51 | loss: 13.2346687Losses:  10.184617042541504 3.507742404937744 0.2991001605987549
CurrentTrain: epoch  1, batch    52 | loss: 10.1846170Losses:  10.18640422821045 3.4443202018737793 0.3562173545360565
CurrentTrain: epoch  1, batch    53 | loss: 10.1864042Losses:  10.359716415405273 3.703038215637207 0.296446830034256
CurrentTrain: epoch  1, batch    54 | loss: 10.3597164Losses:  10.049365997314453 2.9134998321533203 0.32555055618286133
CurrentTrain: epoch  1, batch    55 | loss: 10.0493660Losses:  11.239778518676758 4.2304511070251465 0.3157081604003906
CurrentTrain: epoch  1, batch    56 | loss: 11.2397785Losses:  12.99695873260498 5.162313938140869 0.3218965530395508
CurrentTrain: epoch  1, batch    57 | loss: 12.9969587Losses:  12.439055442810059 5.143364906311035 0.33619755506515503
CurrentTrain: epoch  1, batch    58 | loss: 12.4390554Losses:  9.278463363647461 2.5187501907348633 0.30984681844711304
CurrentTrain: epoch  1, batch    59 | loss: 9.2784634Losses:  10.916677474975586 4.65679931640625 0.33769530057907104
CurrentTrain: epoch  1, batch    60 | loss: 10.9166775Losses:  10.64653205871582 4.354558944702148 0.35097819566726685
CurrentTrain: epoch  1, batch    61 | loss: 10.6465321Losses:  10.238348007202148 1.1074411869049072 0.3379332423210144
CurrentTrain: epoch  1, batch    62 | loss: 10.2383480Losses:  8.823735237121582 2.54938006401062 0.31252533197402954
CurrentTrain: epoch  2, batch     0 | loss: 8.8237352Losses:  10.701061248779297 4.207402229309082 0.20316001772880554
CurrentTrain: epoch  2, batch     1 | loss: 10.7010612Losses:  8.478364944458008 2.5000767707824707 0.29282933473587036
CurrentTrain: epoch  2, batch     2 | loss: 8.4783649Losses:  12.28065013885498 5.10338020324707 0.23487578332424164
CurrentTrain: epoch  2, batch     3 | loss: 12.2806501Losses:  9.263089179992676 3.4571471214294434 0.29085996747016907
CurrentTrain: epoch  2, batch     4 | loss: 9.2630892Losses:  10.088310241699219 3.6395556926727295 0.30724960565567017
CurrentTrain: epoch  2, batch     5 | loss: 10.0883102Losses:  10.043071746826172 3.5109715461730957 0.2979699373245239
CurrentTrain: epoch  2, batch     6 | loss: 10.0430717Losses:  9.180257797241211 3.8719241619110107 0.28999096155166626
CurrentTrain: epoch  2, batch     7 | loss: 9.1802578Losses:  10.540447235107422 3.437685251235962 0.2858049273490906
CurrentTrain: epoch  2, batch     8 | loss: 10.5404472Losses:  12.201815605163574 5.437869071960449 0.35183650255203247
CurrentTrain: epoch  2, batch     9 | loss: 12.2018156Losses:  8.43189525604248 2.7685203552246094 0.298089861869812
CurrentTrain: epoch  2, batch    10 | loss: 8.4318953Losses:  10.06065845489502 3.854306221008301 0.3330236077308655
CurrentTrain: epoch  2, batch    11 | loss: 10.0606585Losses:  11.159210205078125 4.891175746917725 0.30610835552215576
CurrentTrain: epoch  2, batch    12 | loss: 11.1592102Losses:  11.045500755310059 4.795459747314453 0.30090412497520447
CurrentTrain: epoch  2, batch    13 | loss: 11.0455008Losses:  8.488612174987793 2.1674318313598633 0.2948850393295288
CurrentTrain: epoch  2, batch    14 | loss: 8.4886122Losses:  9.0048828125 3.2468857765197754 0.28118687868118286
CurrentTrain: epoch  2, batch    15 | loss: 9.0048828Losses:  10.206815719604492 3.7851052284240723 0.29651644825935364
CurrentTrain: epoch  2, batch    16 | loss: 10.2068157Losses:  11.667987823486328 4.441517353057861 0.29523801803588867
CurrentTrain: epoch  2, batch    17 | loss: 11.6679878Losses:  8.756586074829102 2.5667433738708496 0.2700181305408478
CurrentTrain: epoch  2, batch    18 | loss: 8.7565861Losses:  8.751755714416504 2.7022457122802734 0.28035056591033936
CurrentTrain: epoch  2, batch    19 | loss: 8.7517557Losses:  9.201871871948242 3.278306007385254 0.29680871963500977
CurrentTrain: epoch  2, batch    20 | loss: 9.2018719Losses:  12.857665061950684 6.056381702423096 0.2855287194252014
CurrentTrain: epoch  2, batch    21 | loss: 12.8576651Losses:  8.607545852661133 2.624312400817871 0.29389798641204834
CurrentTrain: epoch  2, batch    22 | loss: 8.6075459Losses:  10.493935585021973 4.461715221405029 0.29325515031814575
CurrentTrain: epoch  2, batch    23 | loss: 10.4939356Losses:  8.199355125427246 2.234307289123535 0.2754053771495819
CurrentTrain: epoch  2, batch    24 | loss: 8.1993551Losses:  8.495433807373047 2.2886202335357666 0.28330957889556885
CurrentTrain: epoch  2, batch    25 | loss: 8.4954338Losses:  11.409173965454102 4.276866912841797 0.29377835988998413
CurrentTrain: epoch  2, batch    26 | loss: 11.4091740Losses:  9.200824737548828 2.958217144012451 0.29331761598587036
CurrentTrain: epoch  2, batch    27 | loss: 9.2008247Losses:  9.734931945800781 3.2922792434692383 0.27307599782943726
CurrentTrain: epoch  2, batch    28 | loss: 9.7349319Losses:  9.073517799377441 3.574742317199707 0.2857338786125183
CurrentTrain: epoch  2, batch    29 | loss: 9.0735178Losses:  8.290943145751953 2.528581380844116 0.26628848910331726
CurrentTrain: epoch  2, batch    30 | loss: 8.2909431Losses:  10.126218795776367 3.7791800498962402 0.28854912519454956
CurrentTrain: epoch  2, batch    31 | loss: 10.1262188Losses:  9.19601058959961 3.4579262733459473 0.2745552957057953
CurrentTrain: epoch  2, batch    32 | loss: 9.1960106Losses:  9.008269309997559 2.4934611320495605 0.28600260615348816
CurrentTrain: epoch  2, batch    33 | loss: 9.0082693Losses:  11.079830169677734 4.793763637542725 0.2832643985748291
CurrentTrain: epoch  2, batch    34 | loss: 11.0798302Losses:  10.28227710723877 3.9258055686950684 0.29193374514579773
CurrentTrain: epoch  2, batch    35 | loss: 10.2822771Losses:  8.724861145019531 2.6155624389648438 0.3083696961402893
CurrentTrain: epoch  2, batch    36 | loss: 8.7248611Losses:  9.289535522460938 2.917463779449463 0.2904183566570282
CurrentTrain: epoch  2, batch    37 | loss: 9.2895355Losses:  9.564229965209961 3.3574471473693848 0.2821773886680603
CurrentTrain: epoch  2, batch    38 | loss: 9.5642300Losses:  10.623225212097168 4.7890167236328125 0.2666047215461731
CurrentTrain: epoch  2, batch    39 | loss: 10.6232252Losses:  8.304388046264648 2.800229549407959 0.2907237708568573
CurrentTrain: epoch  2, batch    40 | loss: 8.3043880Losses:  9.399979591369629 4.008262634277344 0.29319214820861816
CurrentTrain: epoch  2, batch    41 | loss: 9.3999796Losses:  8.158090591430664 2.549872875213623 0.28192955255508423
CurrentTrain: epoch  2, batch    42 | loss: 8.1580906Losses:  9.372029304504395 3.432680130004883 0.29620033502578735
CurrentTrain: epoch  2, batch    43 | loss: 9.3720293Losses:  8.818256378173828 2.910208225250244 0.28644049167633057
CurrentTrain: epoch  2, batch    44 | loss: 8.8182564Losses:  8.85519027709961 3.091874599456787 0.30759215354919434
CurrentTrain: epoch  2, batch    45 | loss: 8.8551903Losses:  8.835389137268066 3.27386212348938 0.29956328868865967
CurrentTrain: epoch  2, batch    46 | loss: 8.8353891Losses:  8.63979721069336 3.021545171737671 0.27242887020111084
CurrentTrain: epoch  2, batch    47 | loss: 8.6397972Losses:  10.083932876586914 4.097483158111572 0.2949470281600952
CurrentTrain: epoch  2, batch    48 | loss: 10.0839329Losses:  9.859768867492676 3.4857425689697266 0.28852808475494385
CurrentTrain: epoch  2, batch    49 | loss: 9.8597689Losses:  8.326619148254395 3.0828330516815186 0.2723242938518524
CurrentTrain: epoch  2, batch    50 | loss: 8.3266191Losses:  7.679427623748779 2.514455556869507 0.28406763076782227
CurrentTrain: epoch  2, batch    51 | loss: 7.6794276Losses:  8.765960693359375 3.0916757583618164 0.30570051074028015
CurrentTrain: epoch  2, batch    52 | loss: 8.7659607Losses:  8.525430679321289 3.1187214851379395 0.26340627670288086
CurrentTrain: epoch  2, batch    53 | loss: 8.5254307Losses:  10.135757446289062 4.285764694213867 0.26931172609329224
CurrentTrain: epoch  2, batch    54 | loss: 10.1357574Losses:  8.953054428100586 3.6909525394439697 0.29154929518699646
CurrentTrain: epoch  2, batch    55 | loss: 8.9530544Losses:  9.687520027160645 3.831000804901123 0.27386415004730225
CurrentTrain: epoch  2, batch    56 | loss: 9.6875200Losses:  8.603177070617676 2.7654662132263184 0.27066296339035034
CurrentTrain: epoch  2, batch    57 | loss: 8.6031771Losses:  8.14676284790039 2.6701645851135254 0.27897629141807556
CurrentTrain: epoch  2, batch    58 | loss: 8.1467628Losses:  9.816006660461426 4.359104156494141 0.27733927965164185
CurrentTrain: epoch  2, batch    59 | loss: 9.8160067Losses:  7.401432037353516 2.1935617923736572 0.26292526721954346
CurrentTrain: epoch  2, batch    60 | loss: 7.4014320Losses:  7.171357154846191 1.948425531387329 0.26134955883026123
CurrentTrain: epoch  2, batch    61 | loss: 7.1713572Losses:  6.310781002044678 0.2532452344894409 0.26982325315475464
CurrentTrain: epoch  2, batch    62 | loss: 6.3107810Losses:  8.647330284118652 3.2265968322753906 0.28849321603775024
CurrentTrain: epoch  3, batch     0 | loss: 8.6473303Losses:  10.13369083404541 4.313748359680176 0.28145885467529297
CurrentTrain: epoch  3, batch     1 | loss: 10.1336908Losses:  8.898579597473145 3.2947099208831787 0.2834101617336273
CurrentTrain: epoch  3, batch     2 | loss: 8.8985796Losses:  7.576189041137695 2.542584180831909 0.25671881437301636
CurrentTrain: epoch  3, batch     3 | loss: 7.5761890Losses:  8.35269546508789 3.034306049346924 0.27521181106567383
CurrentTrain: epoch  3, batch     4 | loss: 8.3526955Losses:  8.131587982177734 2.3162474632263184 0.2746541500091553
CurrentTrain: epoch  3, batch     5 | loss: 8.1315880Losses:  9.4627046585083 3.1274843215942383 0.27824920415878296
CurrentTrain: epoch  3, batch     6 | loss: 9.4627047Losses:  8.607911109924316 2.7521114349365234 0.19844184815883636
CurrentTrain: epoch  3, batch     7 | loss: 8.6079111Losses:  11.969789505004883 5.678407669067383 0.28008154034614563
CurrentTrain: epoch  3, batch     8 | loss: 11.9697895Losses:  9.945684432983398 4.910892009735107 0.2591281235218048
CurrentTrain: epoch  3, batch     9 | loss: 9.9456844Losses:  11.067832946777344 5.815334320068359 0.2733287215232849
CurrentTrain: epoch  3, batch    10 | loss: 11.0678329Losses:  9.33635139465332 4.14451789855957 0.2753438949584961
CurrentTrain: epoch  3, batch    11 | loss: 9.3363514Losses:  6.680838584899902 1.419129490852356 0.2481689155101776
CurrentTrain: epoch  3, batch    12 | loss: 6.6808386Losses:  8.957372665405273 4.086366653442383 0.26023098826408386
CurrentTrain: epoch  3, batch    13 | loss: 8.9573727Losses:  9.948198318481445 3.9665863513946533 0.2695002555847168
CurrentTrain: epoch  3, batch    14 | loss: 9.9481983Losses:  8.38707447052002 3.10969877243042 0.2590128183364868
CurrentTrain: epoch  3, batch    15 | loss: 8.3870745Losses:  11.048102378845215 4.794008255004883 0.2782514691352844
CurrentTrain: epoch  3, batch    16 | loss: 11.0481024Losses:  8.794404029846191 3.148508071899414 0.25479963421821594
CurrentTrain: epoch  3, batch    17 | loss: 8.7944040Losses:  7.883216381072998 3.078303337097168 0.2565562129020691
CurrentTrain: epoch  3, batch    18 | loss: 7.8832164Losses:  7.92482852935791 2.3466124534606934 0.27327901124954224
CurrentTrain: epoch  3, batch    19 | loss: 7.9248285Losses:  7.860329627990723 2.8789756298065186 0.2632277011871338
CurrentTrain: epoch  3, batch    20 | loss: 7.8603296Losses:  9.63930892944336 3.7210404872894287 0.26716044545173645
CurrentTrain: epoch  3, batch    21 | loss: 9.6393089Losses:  7.313216686248779 1.7907075881958008 0.2545267343521118
CurrentTrain: epoch  3, batch    22 | loss: 7.3132167Losses:  11.127248764038086 4.749020099639893 0.26433202624320984
CurrentTrain: epoch  3, batch    23 | loss: 11.1272488Losses:  7.4523701667785645 1.9733365774154663 0.2740135192871094
CurrentTrain: epoch  3, batch    24 | loss: 7.4523702Losses:  11.567364692687988 6.569215297698975 0.20410826802253723
CurrentTrain: epoch  3, batch    25 | loss: 11.5673647Losses:  8.594633102416992 3.3505682945251465 0.2759133577346802
CurrentTrain: epoch  3, batch    26 | loss: 8.5946331Losses:  9.741596221923828 4.113735198974609 0.28246092796325684
CurrentTrain: epoch  3, batch    27 | loss: 9.7415962Losses:  10.514183044433594 5.073044776916504 0.26536524295806885
CurrentTrain: epoch  3, batch    28 | loss: 10.5141830Losses:  8.677513122558594 3.750941276550293 0.2696743905544281
CurrentTrain: epoch  3, batch    29 | loss: 8.6775131Losses:  9.10685920715332 3.7420105934143066 0.25667208433151245
CurrentTrain: epoch  3, batch    30 | loss: 9.1068592Losses:  8.613609313964844 2.98239803314209 0.25825297832489014
CurrentTrain: epoch  3, batch    31 | loss: 8.6136093Losses:  7.23726749420166 2.341874837875366 0.24884016811847687
CurrentTrain: epoch  3, batch    32 | loss: 7.2372675Losses:  9.671653747558594 4.019162178039551 0.17366242408752441
CurrentTrain: epoch  3, batch    33 | loss: 9.6716537Losses:  7.934427261352539 2.940769672393799 0.26269596815109253
CurrentTrain: epoch  3, batch    34 | loss: 7.9344273Losses:  7.571450710296631 2.746570587158203 0.24812942743301392
CurrentTrain: epoch  3, batch    35 | loss: 7.5714507Losses:  8.637592315673828 2.737199306488037 0.24876049160957336
CurrentTrain: epoch  3, batch    36 | loss: 8.6375923Losses:  7.90831995010376 2.5634665489196777 0.2686530351638794
CurrentTrain: epoch  3, batch    37 | loss: 7.9083200Losses:  7.66800594329834 2.9529523849487305 0.25891876220703125
CurrentTrain: epoch  3, batch    38 | loss: 7.6680059Losses:  7.535942077636719 2.697185516357422 0.2608521580696106
CurrentTrain: epoch  3, batch    39 | loss: 7.5359421Losses:  7.776321887969971 3.0151896476745605 0.2543163001537323
CurrentTrain: epoch  3, batch    40 | loss: 7.7763219Losses:  7.917559623718262 2.6638693809509277 0.2713667154312134
CurrentTrain: epoch  3, batch    41 | loss: 7.9175596Losses:  7.958193302154541 3.018090009689331 0.263062983751297
CurrentTrain: epoch  3, batch    42 | loss: 7.9581933Losses:  6.937338829040527 2.257140636444092 0.2471640408039093
CurrentTrain: epoch  3, batch    43 | loss: 6.9373388Losses:  8.016661643981934 2.2745039463043213 0.2375119924545288
CurrentTrain: epoch  3, batch    44 | loss: 8.0166616Losses:  8.468908309936523 3.403568744659424 0.24749146401882172
CurrentTrain: epoch  3, batch    45 | loss: 8.4689083Losses:  8.2122220993042 2.5608134269714355 0.26078665256500244
CurrentTrain: epoch  3, batch    46 | loss: 8.2122221Losses:  10.188262939453125 4.940596580505371 0.2683286964893341
CurrentTrain: epoch  3, batch    47 | loss: 10.1882629Losses:  7.403497695922852 2.2898287773132324 0.2451821267604828
CurrentTrain: epoch  3, batch    48 | loss: 7.4034977Losses:  10.352282524108887 4.945719242095947 0.27272123098373413
CurrentTrain: epoch  3, batch    49 | loss: 10.3522825Losses:  7.913417339324951 3.034658432006836 0.2575293183326721
CurrentTrain: epoch  3, batch    50 | loss: 7.9134173Losses:  7.362797737121582 2.035554885864258 0.2556249499320984
CurrentTrain: epoch  3, batch    51 | loss: 7.3627977Losses:  7.6685943603515625 2.8663430213928223 0.24570509791374207
CurrentTrain: epoch  3, batch    52 | loss: 7.6685944Losses:  8.204462051391602 3.282869577407837 0.2572399973869324
CurrentTrain: epoch  3, batch    53 | loss: 8.2044621Losses:  7.923447132110596 2.9891302585601807 0.26604244112968445
CurrentTrain: epoch  3, batch    54 | loss: 7.9234471Losses:  10.044761657714844 4.125189304351807 0.25438830256462097
CurrentTrain: epoch  3, batch    55 | loss: 10.0447617Losses:  8.758461952209473 3.964437961578369 0.24844445288181305
CurrentTrain: epoch  3, batch    56 | loss: 8.7584620Losses:  8.101800918579102 2.8223142623901367 0.24985991418361664
CurrentTrain: epoch  3, batch    57 | loss: 8.1018009Losses:  11.945615768432617 6.226654052734375 0.2708016037940979
CurrentTrain: epoch  3, batch    58 | loss: 11.9456158Losses:  8.382587432861328 3.6532387733459473 0.24666829407215118
CurrentTrain: epoch  3, batch    59 | loss: 8.3825874Losses:  8.965385437011719 4.032176971435547 0.2579631805419922
CurrentTrain: epoch  3, batch    60 | loss: 8.9653854Losses:  10.162817001342773 4.083828926086426 0.2537420690059662
CurrentTrain: epoch  3, batch    61 | loss: 10.1628170Losses:  9.170063972473145 2.346940040588379 0.29690712690353394
CurrentTrain: epoch  3, batch    62 | loss: 9.1700640Losses:  6.976720809936523 2.3270680904388428 0.24591748416423798
CurrentTrain: epoch  4, batch     0 | loss: 6.9767208Losses:  8.694615364074707 2.700364112854004 0.2585170567035675
CurrentTrain: epoch  4, batch     1 | loss: 8.6946154Losses:  8.373661041259766 3.290921449661255 0.26338401436805725
CurrentTrain: epoch  4, batch     2 | loss: 8.3736610Losses:  8.147512435913086 2.6583199501037598 0.25103747844696045
CurrentTrain: epoch  4, batch     3 | loss: 8.1475124Losses:  8.819097518920898 3.946079969406128 0.2555686831474304
CurrentTrain: epoch  4, batch     4 | loss: 8.8190975Losses:  7.824149131774902 2.5424532890319824 0.24412494897842407
CurrentTrain: epoch  4, batch     5 | loss: 7.8241491Losses:  7.905614852905273 2.7978241443634033 0.2428056001663208
CurrentTrain: epoch  4, batch     6 | loss: 7.9056149Losses:  7.153228282928467 2.3482041358947754 0.2450849562883377
CurrentTrain: epoch  4, batch     7 | loss: 7.1532283Losses:  7.081957817077637 1.9607493877410889 0.24509119987487793
CurrentTrain: epoch  4, batch     8 | loss: 7.0819578Losses:  7.25319766998291 2.4826745986938477 0.23950088024139404
CurrentTrain: epoch  4, batch     9 | loss: 7.2531977Losses:  10.566913604736328 5.814517974853516 0.2715287208557129
CurrentTrain: epoch  4, batch    10 | loss: 10.5669136Losses:  8.926046371459961 4.161380767822266 0.2661464810371399
CurrentTrain: epoch  4, batch    11 | loss: 8.9260464Losses:  10.280265808105469 4.301671028137207 0.2840477228164673
CurrentTrain: epoch  4, batch    12 | loss: 10.2802658Losses:  7.497663497924805 2.4613311290740967 0.2485789656639099
CurrentTrain: epoch  4, batch    13 | loss: 7.4976635Losses:  7.882213115692139 3.036869764328003 0.2413124442100525
CurrentTrain: epoch  4, batch    14 | loss: 7.8822131Losses:  9.295552253723145 4.700981140136719 0.2491689920425415
CurrentTrain: epoch  4, batch    15 | loss: 9.2955523Losses:  9.687300682067871 4.7911200523376465 0.25470489263534546
CurrentTrain: epoch  4, batch    16 | loss: 9.6873007Losses:  11.10258960723877 5.7935051918029785 0.2751748263835907
CurrentTrain: epoch  4, batch    17 | loss: 11.1025896Losses:  6.987725734710693 2.224626064300537 0.2379569262266159
CurrentTrain: epoch  4, batch    18 | loss: 6.9877257Losses:  13.221068382263184 7.383521556854248 0.26928675174713135
CurrentTrain: epoch  4, batch    19 | loss: 13.2210684Losses:  7.776127815246582 3.0769519805908203 0.2459823191165924
CurrentTrain: epoch  4, batch    20 | loss: 7.7761278Losses:  8.576558113098145 3.5597119331359863 0.27112889289855957
CurrentTrain: epoch  4, batch    21 | loss: 8.5765581Losses:  7.6758832931518555 3.0475761890411377 0.262207955121994
CurrentTrain: epoch  4, batch    22 | loss: 7.6758833Losses:  9.670284271240234 4.922144412994385 0.2601930499076843
CurrentTrain: epoch  4, batch    23 | loss: 9.6702843Losses:  6.656341075897217 2.0732669830322266 0.23213228583335876
CurrentTrain: epoch  4, batch    24 | loss: 6.6563411Losses:  7.527780532836914 2.7071471214294434 0.25354820489883423
CurrentTrain: epoch  4, batch    25 | loss: 7.5277805Losses:  8.585735321044922 3.945232629776001 0.25231441855430603
CurrentTrain: epoch  4, batch    26 | loss: 8.5857353Losses:  9.218035697937012 4.369296073913574 0.28417646884918213
CurrentTrain: epoch  4, batch    27 | loss: 9.2180357Losses:  7.180080890655518 2.32135009765625 0.23935988545417786
CurrentTrain: epoch  4, batch    28 | loss: 7.1800809Losses:  9.117318153381348 4.223604202270508 0.2684936225414276
CurrentTrain: epoch  4, batch    29 | loss: 9.1173182Losses:  8.304744720458984 3.4257497787475586 0.2523338496685028
CurrentTrain: epoch  4, batch    30 | loss: 8.3047447Losses:  7.473471164703369 2.772190570831299 0.25880610942840576
CurrentTrain: epoch  4, batch    31 | loss: 7.4734712Losses:  7.886970520019531 2.8065571784973145 0.23959705233573914
CurrentTrain: epoch  4, batch    32 | loss: 7.8869705Losses:  8.028190612792969 2.9738850593566895 0.24391496181488037
CurrentTrain: epoch  4, batch    33 | loss: 8.0281906Losses:  9.581878662109375 4.404572010040283 0.26016664505004883
CurrentTrain: epoch  4, batch    34 | loss: 9.5818787Losses:  10.11252212524414 5.5672149658203125 0.2508193850517273
CurrentTrain: epoch  4, batch    35 | loss: 10.1125221Losses:  9.11500358581543 3.324781894683838 0.2439408153295517
CurrentTrain: epoch  4, batch    36 | loss: 9.1150036Losses:  8.60184383392334 3.9450392723083496 0.25639063119888306
CurrentTrain: epoch  4, batch    37 | loss: 8.6018438Losses:  9.266695976257324 4.262707233428955 0.2531646192073822
CurrentTrain: epoch  4, batch    38 | loss: 9.2666960Losses:  8.642382621765137 3.321645975112915 0.24093788862228394
CurrentTrain: epoch  4, batch    39 | loss: 8.6423826Losses:  7.852987289428711 2.7336068153381348 0.24424734711647034
CurrentTrain: epoch  4, batch    40 | loss: 7.8529873Losses:  7.4065752029418945 2.1969666481018066 0.23990359902381897
CurrentTrain: epoch  4, batch    41 | loss: 7.4065752Losses:  8.993365287780762 4.116043567657471 0.23268237709999084
CurrentTrain: epoch  4, batch    42 | loss: 8.9933653Losses:  8.647442817687988 3.892963409423828 0.25292903184890747
CurrentTrain: epoch  4, batch    43 | loss: 8.6474428Losses:  8.316875457763672 2.6494250297546387 0.24175876379013062
CurrentTrain: epoch  4, batch    44 | loss: 8.3168755Losses:  6.284790992736816 1.723081111907959 0.22701792418956757
CurrentTrain: epoch  4, batch    45 | loss: 6.2847910Losses:  10.527563095092773 5.167829513549805 0.24812421202659607
CurrentTrain: epoch  4, batch    46 | loss: 10.5275631Losses:  9.808835983276367 5.345221519470215 0.2501911520957947
CurrentTrain: epoch  4, batch    47 | loss: 9.8088360Losses:  8.865447044372559 4.015312194824219 0.2520102262496948
CurrentTrain: epoch  4, batch    48 | loss: 8.8654470Losses:  6.380661487579346 1.750372052192688 0.2350422441959381
CurrentTrain: epoch  4, batch    49 | loss: 6.3806615Losses:  6.413573265075684 1.8444108963012695 0.23306924104690552
CurrentTrain: epoch  4, batch    50 | loss: 6.4135733Losses:  7.2725348472595215 2.8259129524230957 0.25174152851104736
CurrentTrain: epoch  4, batch    51 | loss: 7.2725348Losses:  9.049609184265137 3.9713501930236816 0.18377575278282166
CurrentTrain: epoch  4, batch    52 | loss: 9.0496092Losses:  10.322638511657715 5.101813793182373 0.2667016088962555
CurrentTrain: epoch  4, batch    53 | loss: 10.3226385Losses:  7.495092391967773 2.8968281745910645 0.24383394420146942
CurrentTrain: epoch  4, batch    54 | loss: 7.4950924Losses:  8.255908966064453 2.8305702209472656 0.24785315990447998
CurrentTrain: epoch  4, batch    55 | loss: 8.2559090Losses:  7.66140079498291 2.99609375 0.25405097007751465
CurrentTrain: epoch  4, batch    56 | loss: 7.6614008Losses:  6.549769878387451 1.9179139137268066 0.23730596899986267
CurrentTrain: epoch  4, batch    57 | loss: 6.5497699Losses:  8.959497451782227 4.543735027313232 0.1830430030822754
CurrentTrain: epoch  4, batch    58 | loss: 8.9594975Losses:  7.619379043579102 3.0418033599853516 0.24792344868183136
CurrentTrain: epoch  4, batch    59 | loss: 7.6193790Losses:  7.939121246337891 3.3508100509643555 0.2521495223045349
CurrentTrain: epoch  4, batch    60 | loss: 7.9391212Losses:  6.7937726974487305 2.4380135536193848 0.23446230590343475
CurrentTrain: epoch  4, batch    61 | loss: 6.7937727Losses:  5.001151084899902 0.24042204022407532 0.16697882115840912
CurrentTrain: epoch  4, batch    62 | loss: 5.0011511Losses:  8.880048751831055 3.768505334854126 0.23857976496219635
CurrentTrain: epoch  5, batch     0 | loss: 8.8800488Losses:  8.292732238769531 3.7923033237457275 0.16365714371204376
CurrentTrain: epoch  5, batch     1 | loss: 8.2927322Losses:  7.086587429046631 2.5228586196899414 0.23521769046783447
CurrentTrain: epoch  5, batch     2 | loss: 7.0865874Losses:  8.20529842376709 2.5515098571777344 0.2464188039302826
CurrentTrain: epoch  5, batch     3 | loss: 8.2052984Losses:  7.644692420959473 3.2155981063842773 0.2248445749282837
CurrentTrain: epoch  5, batch     4 | loss: 7.6446924Losses:  10.150558471679688 4.526541233062744 0.2536751627922058
CurrentTrain: epoch  5, batch     5 | loss: 10.1505585Losses:  7.670015335083008 2.5282533168792725 0.24298971891403198
CurrentTrain: epoch  5, batch     6 | loss: 7.6700153Losses:  7.7971601486206055 3.3150367736816406 0.2490769326686859
CurrentTrain: epoch  5, batch     7 | loss: 7.7971601Losses:  6.6836066246032715 2.277909517288208 0.23150962591171265
CurrentTrain: epoch  5, batch     8 | loss: 6.6836066Losses:  7.459911823272705 2.5464959144592285 0.24599778652191162
CurrentTrain: epoch  5, batch     9 | loss: 7.4599118Losses:  7.751253604888916 3.3290696144104004 0.24474051594734192
CurrentTrain: epoch  5, batch    10 | loss: 7.7512536Losses:  8.213167190551758 3.5855906009674072 0.23606061935424805
CurrentTrain: epoch  5, batch    11 | loss: 8.2131672Losses:  6.259345054626465 1.7579107284545898 0.2208254337310791
CurrentTrain: epoch  5, batch    12 | loss: 6.2593451Losses:  8.6026611328125 4.18152379989624 0.23697586357593536
CurrentTrain: epoch  5, batch    13 | loss: 8.6026611Losses:  6.708341598510742 2.285398006439209 0.23140877485275269
CurrentTrain: epoch  5, batch    14 | loss: 6.7083416Losses:  6.253118515014648 1.4180357456207275 0.22666476666927338
CurrentTrain: epoch  5, batch    15 | loss: 6.2531185Losses:  6.750400066375732 2.279116153717041 0.23425894975662231
CurrentTrain: epoch  5, batch    16 | loss: 6.7504001Losses:  8.564362525939941 3.416865110397339 0.23549243807792664
CurrentTrain: epoch  5, batch    17 | loss: 8.5643625Losses:  9.843046188354492 5.412624835968018 0.2486272007226944
CurrentTrain: epoch  5, batch    18 | loss: 9.8430462Losses:  11.542501449584961 7.041292190551758 0.2587664723396301
CurrentTrain: epoch  5, batch    19 | loss: 11.5425014Losses:  9.024232864379883 4.308248996734619 0.22818583250045776
CurrentTrain: epoch  5, batch    20 | loss: 9.0242329Losses:  8.754841804504395 4.2559614181518555 0.15697190165519714
CurrentTrain: epoch  5, batch    21 | loss: 8.7548418Losses:  9.847298622131348 4.823962211608887 0.2557581663131714
CurrentTrain: epoch  5, batch    22 | loss: 9.8472986Losses:  15.251485824584961 10.648726463317871 0.194271057844162
CurrentTrain: epoch  5, batch    23 | loss: 15.2514858Losses:  9.429409980773926 4.471290588378906 0.2408701777458191
CurrentTrain: epoch  5, batch    24 | loss: 9.4294100Losses:  7.695026874542236 2.9608678817749023 0.2354801595211029
CurrentTrain: epoch  5, batch    25 | loss: 7.6950269Losses:  7.554875373840332 3.0163190364837646 0.23435594141483307
CurrentTrain: epoch  5, batch    26 | loss: 7.5548754Losses:  8.599283218383789 3.1819562911987305 0.2490871250629425
CurrentTrain: epoch  5, batch    27 | loss: 8.5992832Losses:  7.837554454803467 3.240194797515869 0.2438574731349945
CurrentTrain: epoch  5, batch    28 | loss: 7.8375545Losses:  6.4467997550964355 2.065568447113037 0.2313545048236847
CurrentTrain: epoch  5, batch    29 | loss: 6.4467998Losses:  7.3275837898254395 2.638278007507324 0.24201716482639313
CurrentTrain: epoch  5, batch    30 | loss: 7.3275838Losses:  8.613832473754883 2.8322713375091553 0.2349715232849121
CurrentTrain: epoch  5, batch    31 | loss: 8.6138325Losses:  7.205305099487305 2.6169683933258057 0.22146694362163544
CurrentTrain: epoch  5, batch    32 | loss: 7.2053051Losses:  7.649021148681641 2.6203482151031494 0.22686490416526794
CurrentTrain: epoch  5, batch    33 | loss: 7.6490211Losses:  8.135872840881348 2.96118426322937 0.23534174263477325
CurrentTrain: epoch  5, batch    34 | loss: 8.1358728Losses:  7.204847812652588 2.5560152530670166 0.2501349449157715
CurrentTrain: epoch  5, batch    35 | loss: 7.2048478Losses:  7.323003768920898 2.7697789669036865 0.23899944126605988
CurrentTrain: epoch  5, batch    36 | loss: 7.3230038Losses:  7.911227703094482 3.139697313308716 0.24577464163303375
CurrentTrain: epoch  5, batch    37 | loss: 7.9112277Losses:  9.553916931152344 4.567400932312012 0.24576601386070251
CurrentTrain: epoch  5, batch    38 | loss: 9.5539169Losses:  8.066936492919922 3.4191925525665283 0.25698643922805786
CurrentTrain: epoch  5, batch    39 | loss: 8.0669365Losses:  8.44148063659668 3.703291893005371 0.24639739096164703
CurrentTrain: epoch  5, batch    40 | loss: 8.4414806Losses:  8.988105773925781 4.193507671356201 0.24428434669971466
CurrentTrain: epoch  5, batch    41 | loss: 8.9881058Losses:  6.316859722137451 1.9570157527923584 0.22253826260566711
CurrentTrain: epoch  5, batch    42 | loss: 6.3168597Losses:  8.981962203979492 4.458984375 0.25545769929885864
CurrentTrain: epoch  5, batch    43 | loss: 8.9819622Losses:  9.22497272491455 4.257458209991455 0.1706761121749878
CurrentTrain: epoch  5, batch    44 | loss: 9.2249727Losses:  7.876258850097656 3.178936243057251 0.2557944655418396
CurrentTrain: epoch  5, batch    45 | loss: 7.8762589Losses:  9.491419792175293 4.680573463439941 0.25096389651298523
CurrentTrain: epoch  5, batch    46 | loss: 9.4914198Losses:  8.726116180419922 4.295915126800537 0.25646042823791504
CurrentTrain: epoch  5, batch    47 | loss: 8.7261162Losses:  7.735779285430908 3.2870335578918457 0.24238398671150208
CurrentTrain: epoch  5, batch    48 | loss: 7.7357793Losses:  9.373882293701172 5.091514587402344 0.14938288927078247
CurrentTrain: epoch  5, batch    49 | loss: 9.3738823Losses:  9.435702323913574 4.898874759674072 0.28061026334762573
CurrentTrain: epoch  5, batch    50 | loss: 9.4357023Losses:  10.066106796264648 5.284132957458496 0.26336565613746643
CurrentTrain: epoch  5, batch    51 | loss: 10.0661068Losses:  7.205328941345215 2.314012050628662 0.24407434463500977
CurrentTrain: epoch  5, batch    52 | loss: 7.2053289Losses:  9.568696975708008 4.621176719665527 0.25247424840927124
CurrentTrain: epoch  5, batch    53 | loss: 9.5686970Losses:  10.856176376342773 6.432821750640869 0.24308304488658905
CurrentTrain: epoch  5, batch    54 | loss: 10.8561764Losses:  7.877062797546387 3.5820109844207764 0.23249348998069763
CurrentTrain: epoch  5, batch    55 | loss: 7.8770628Losses:  6.999589920043945 2.3160858154296875 0.23367488384246826
CurrentTrain: epoch  5, batch    56 | loss: 6.9995899Losses:  8.435081481933594 3.6080074310302734 0.14778247475624084
CurrentTrain: epoch  5, batch    57 | loss: 8.4350815Losses:  13.134100914001465 8.728341102600098 0.24251936376094818
CurrentTrain: epoch  5, batch    58 | loss: 13.1341009Losses:  7.004050254821777 2.242049217224121 0.22844845056533813
CurrentTrain: epoch  5, batch    59 | loss: 7.0040503Losses:  6.865665435791016 2.2737884521484375 0.22683720290660858
CurrentTrain: epoch  5, batch    60 | loss: 6.8656654Losses:  7.346494674682617 2.30619478225708 0.22739681601524353
CurrentTrain: epoch  5, batch    61 | loss: 7.3464947Losses:  6.002650737762451 1.4486266374588013 0.20037907361984253
CurrentTrain: epoch  5, batch    62 | loss: 6.0026507Losses:  8.93458366394043 4.361685752868652 0.24313798546791077
CurrentTrain: epoch  6, batch     0 | loss: 8.9345837Losses:  7.541909217834473 2.982177972793579 0.23057855665683746
CurrentTrain: epoch  6, batch     1 | loss: 7.5419092Losses:  8.461006164550781 4.0572662353515625 0.2635931968688965
CurrentTrain: epoch  6, batch     2 | loss: 8.4610062Losses:  7.417377471923828 3.0273706912994385 0.24355614185333252
CurrentTrain: epoch  6, batch     3 | loss: 7.4173775Losses:  6.759143352508545 2.373884677886963 0.23385962843894958
CurrentTrain: epoch  6, batch     4 | loss: 6.7591434Losses:  6.685887336730957 2.1810338497161865 0.23316460847854614
CurrentTrain: epoch  6, batch     5 | loss: 6.6858873Losses:  12.859220504760742 8.544005393981934 0.24649888277053833
CurrentTrain: epoch  6, batch     6 | loss: 12.8592205Losses:  8.201545715332031 3.6438565254211426 0.24231767654418945
CurrentTrain: epoch  6, batch     7 | loss: 8.2015457Losses:  7.297028064727783 2.839329242706299 0.2387504130601883
CurrentTrain: epoch  6, batch     8 | loss: 7.2970281Losses:  7.247581958770752 2.6156368255615234 0.23319199681282043
CurrentTrain: epoch  6, batch     9 | loss: 7.2475820Losses:  7.078034400939941 2.7301454544067383 0.22419743239879608
CurrentTrain: epoch  6, batch    10 | loss: 7.0780344Losses:  7.253130912780762 2.790574550628662 0.24218964576721191
CurrentTrain: epoch  6, batch    11 | loss: 7.2531309Losses:  7.7624592781066895 3.2949180603027344 0.2613920569419861
CurrentTrain: epoch  6, batch    12 | loss: 7.7624593Losses:  7.431405544281006 3.1087536811828613 0.22808866202831268
CurrentTrain: epoch  6, batch    13 | loss: 7.4314055Losses:  9.446046829223633 4.904019355773926 0.2456567883491516
CurrentTrain: epoch  6, batch    14 | loss: 9.4460468Losses:  6.83376932144165 2.4110097885131836 0.24083185195922852
CurrentTrain: epoch  6, batch    15 | loss: 6.8337693Losses:  8.339366912841797 3.734565258026123 0.15605321526527405
CurrentTrain: epoch  6, batch    16 | loss: 8.3393669Losses:  7.498057842254639 3.1044301986694336 0.23050183057785034
CurrentTrain: epoch  6, batch    17 | loss: 7.4980578Losses:  6.037783622741699 1.634704351425171 0.21565741300582886
CurrentTrain: epoch  6, batch    18 | loss: 6.0377836Losses:  7.734653949737549 3.0727906227111816 0.23985230922698975
CurrentTrain: epoch  6, batch    19 | loss: 7.7346539Losses:  7.219397068023682 2.6410117149353027 0.23472759127616882
CurrentTrain: epoch  6, batch    20 | loss: 7.2193971Losses:  6.668797016143799 2.2566938400268555 0.2367824912071228
CurrentTrain: epoch  6, batch    21 | loss: 6.6687970Losses:  10.353453636169434 5.9800896644592285 0.24729269742965698
CurrentTrain: epoch  6, batch    22 | loss: 10.3534536Losses:  6.498988628387451 2.1654176712036133 0.22777479887008667
CurrentTrain: epoch  6, batch    23 | loss: 6.4989886Losses:  7.381348133087158 3.0329089164733887 0.22827735543251038
CurrentTrain: epoch  6, batch    24 | loss: 7.3813481Losses:  7.059916973114014 2.688502311706543 0.23397797346115112
CurrentTrain: epoch  6, batch    25 | loss: 7.0599170Losses:  9.574387550354004 4.936614990234375 0.2672666013240814
CurrentTrain: epoch  6, batch    26 | loss: 9.5743876Losses:  11.486328125 7.059502124786377 0.2659493088722229
CurrentTrain: epoch  6, batch    27 | loss: 11.4863281Losses:  6.47291374206543 2.0708963871002197 0.213581845164299
CurrentTrain: epoch  6, batch    28 | loss: 6.4729137Losses:  7.619268417358398 3.0962624549865723 0.23099683225154877
CurrentTrain: epoch  6, batch    29 | loss: 7.6192684Losses:  6.856273174285889 2.1550889015197754 0.22725559771060944
CurrentTrain: epoch  6, batch    30 | loss: 6.8562732Losses:  11.421910285949707 7.185831069946289 0.1566893458366394
CurrentTrain: epoch  6, batch    31 | loss: 11.4219103Losses:  7.66048526763916 3.1677894592285156 0.2615787386894226
CurrentTrain: epoch  6, batch    32 | loss: 7.6604853Losses:  7.471981525421143 3.0428857803344727 0.22972039878368378
CurrentTrain: epoch  6, batch    33 | loss: 7.4719815Losses:  7.618080139160156 3.219179630279541 0.2513590455055237
CurrentTrain: epoch  6, batch    34 | loss: 7.6180801Losses:  8.172229766845703 3.78920841217041 0.26182395219802856
CurrentTrain: epoch  6, batch    35 | loss: 8.1722298Losses:  7.188806533813477 2.8590402603149414 0.23139940202236176
CurrentTrain: epoch  6, batch    36 | loss: 7.1888065Losses:  8.127128601074219 3.8426730632781982 0.23081658780574799
CurrentTrain: epoch  6, batch    37 | loss: 8.1271286Losses:  7.103090286254883 2.673020362854004 0.22782063484191895
CurrentTrain: epoch  6, batch    38 | loss: 7.1030903Losses:  7.596710205078125 3.1930313110351562 0.23924069106578827
CurrentTrain: epoch  6, batch    39 | loss: 7.5967102Losses:  7.180117607116699 2.700917959213257 0.23364558815956116
CurrentTrain: epoch  6, batch    40 | loss: 7.1801176Losses:  13.381582260131836 8.912458419799805 0.21146658062934875
CurrentTrain: epoch  6, batch    41 | loss: 13.3815823Losses:  6.600378513336182 2.198624849319458 0.226175919175148
CurrentTrain: epoch  6, batch    42 | loss: 6.6003785Losses:  11.798442840576172 6.174639701843262 0.2496742606163025
CurrentTrain: epoch  6, batch    43 | loss: 11.7984428Losses:  9.720569610595703 5.396291732788086 0.2586336135864258
CurrentTrain: epoch  6, batch    44 | loss: 9.7205696Losses:  6.714553356170654 2.1745214462280273 0.23532629013061523
CurrentTrain: epoch  6, batch    45 | loss: 6.7145534Losses:  7.478331089019775 3.1966781616210938 0.23008480668067932
CurrentTrain: epoch  6, batch    46 | loss: 7.4783311Losses:  6.697652339935303 2.226339817047119 0.22656506299972534
CurrentTrain: epoch  6, batch    47 | loss: 6.6976523Losses:  8.66666316986084 4.437479019165039 0.23767751455307007
CurrentTrain: epoch  6, batch    48 | loss: 8.6666632Losses:  8.586119651794434 4.382096290588379 0.1544993370771408
CurrentTrain: epoch  6, batch    49 | loss: 8.5861197Losses:  8.4248046875 4.136255741119385 0.23338082432746887
CurrentTrain: epoch  6, batch    50 | loss: 8.4248047Losses:  8.048666954040527 3.743924856185913 0.17456744611263275
CurrentTrain: epoch  6, batch    51 | loss: 8.0486670Losses:  7.973941802978516 3.1737427711486816 0.23391492664813995
CurrentTrain: epoch  6, batch    52 | loss: 7.9739418Losses:  7.461200714111328 3.1375374794006348 0.24218273162841797
CurrentTrain: epoch  6, batch    53 | loss: 7.4612007Losses:  8.821319580078125 4.172157287597656 0.24365916848182678
CurrentTrain: epoch  6, batch    54 | loss: 8.8213196Losses:  6.165192127227783 1.8454480171203613 0.21920277178287506
CurrentTrain: epoch  6, batch    55 | loss: 6.1651921Losses:  7.857468605041504 3.5716898441314697 0.23075413703918457
CurrentTrain: epoch  6, batch    56 | loss: 7.8574686Losses:  8.514961242675781 4.16053581237793 0.23147639632225037
CurrentTrain: epoch  6, batch    57 | loss: 8.5149612Losses:  6.8672895431518555 2.5634872913360596 0.23190008103847504
CurrentTrain: epoch  6, batch    58 | loss: 6.8672895Losses:  8.349695205688477 3.9734079837799072 0.2505040764808655
CurrentTrain: epoch  6, batch    59 | loss: 8.3496952Losses:  7.789600849151611 3.1621899604797363 0.24057215452194214
CurrentTrain: epoch  6, batch    60 | loss: 7.7896008Losses:  9.734294891357422 4.541604042053223 0.24075663089752197
CurrentTrain: epoch  6, batch    61 | loss: 9.7342949Losses:  4.544118881225586 0.24805134534835815 0.244492769241333
CurrentTrain: epoch  6, batch    62 | loss: 4.5441189Losses:  7.045028209686279 2.7791290283203125 0.2360762059688568
CurrentTrain: epoch  7, batch     0 | loss: 7.0450282Losses:  7.633919715881348 3.3570332527160645 0.23784519731998444
CurrentTrain: epoch  7, batch     1 | loss: 7.6339197Losses:  8.195429801940918 3.3704566955566406 0.14916889369487762
CurrentTrain: epoch  7, batch     2 | loss: 8.1954298Losses:  7.912553310394287 3.563199520111084 0.23242641985416412
CurrentTrain: epoch  7, batch     3 | loss: 7.9125533Losses:  6.3852009773254395 2.096782684326172 0.23724189400672913
CurrentTrain: epoch  7, batch     4 | loss: 6.3852010Losses:  7.315615177154541 3.018517017364502 0.23636658489704132
CurrentTrain: epoch  7, batch     5 | loss: 7.3156152Losses:  7.6657795906066895 2.9558708667755127 0.15304701030254364
CurrentTrain: epoch  7, batch     6 | loss: 7.6657796Losses:  9.951407432556152 5.614856719970703 0.2542545199394226
CurrentTrain: epoch  7, batch     7 | loss: 9.9514074Losses:  7.286827564239502 2.948845148086548 0.23966839909553528
CurrentTrain: epoch  7, batch     8 | loss: 7.2868276Losses:  7.740940570831299 3.418837070465088 0.23391833901405334
CurrentTrain: epoch  7, batch     9 | loss: 7.7409406Losses:  7.592442512512207 3.2078640460968018 0.2330072820186615
CurrentTrain: epoch  7, batch    10 | loss: 7.5924425Losses:  7.1036529541015625 2.750304937362671 0.20762419700622559
CurrentTrain: epoch  7, batch    11 | loss: 7.1036530Losses:  8.72651481628418 4.549603462219238 0.1459815502166748
CurrentTrain: epoch  7, batch    12 | loss: 8.7265148Losses:  5.896110534667969 1.6261816024780273 0.21260274946689606
CurrentTrain: epoch  7, batch    13 | loss: 5.8961105Losses:  7.01515531539917 2.748478412628174 0.23160924017429352
CurrentTrain: epoch  7, batch    14 | loss: 7.0151553Losses:  6.360286235809326 2.030585527420044 0.22911299765110016
CurrentTrain: epoch  7, batch    15 | loss: 6.3602862Losses:  7.058065414428711 2.5958411693573 0.23626399040222168
CurrentTrain: epoch  7, batch    16 | loss: 7.0580654Losses:  7.070616245269775 2.7159500122070312 0.23400968313217163
CurrentTrain: epoch  7, batch    17 | loss: 7.0706162Losses:  7.910249710083008 3.356107234954834 0.25362712144851685
CurrentTrain: epoch  7, batch    18 | loss: 7.9102497Losses:  8.047516822814941 3.80572247505188 0.22265344858169556
CurrentTrain: epoch  7, batch    19 | loss: 8.0475168Losses:  6.986468315124512 2.681741952896118 0.2317369282245636
CurrentTrain: epoch  7, batch    20 | loss: 6.9864683Losses:  7.024669647216797 2.7349188327789307 0.23855598270893097
CurrentTrain: epoch  7, batch    21 | loss: 7.0246696Losses:  6.197534084320068 1.8948150873184204 0.2321549654006958
CurrentTrain: epoch  7, batch    22 | loss: 6.1975341Losses:  10.40611457824707 6.183036804199219 0.2433912456035614
CurrentTrain: epoch  7, batch    23 | loss: 10.4061146Losses:  9.022664070129395 4.742814540863037 0.24338781833648682
CurrentTrain: epoch  7, batch    24 | loss: 9.0226641Losses:  6.9575114250183105 2.634922981262207 0.2246602177619934
CurrentTrain: epoch  7, batch    25 | loss: 6.9575114Losses:  6.210516929626465 2.0429787635803223 0.1394936889410019
CurrentTrain: epoch  7, batch    26 | loss: 6.2105169Losses:  8.322273254394531 4.041242599487305 0.2312890589237213
CurrentTrain: epoch  7, batch    27 | loss: 8.3222733Losses:  5.8520989418029785 1.6087086200714111 0.21137768030166626
CurrentTrain: epoch  7, batch    28 | loss: 5.8520989Losses:  6.076280117034912 1.799593448638916 0.2252873331308365
CurrentTrain: epoch  7, batch    29 | loss: 6.0762801Losses:  7.54899787902832 3.181593418121338 0.24571561813354492
CurrentTrain: epoch  7, batch    30 | loss: 7.5489979Losses:  7.4508466720581055 3.1390035152435303 0.23530396819114685
CurrentTrain: epoch  7, batch    31 | loss: 7.4508467Losses:  7.2134599685668945 2.9252984523773193 0.22719180583953857
CurrentTrain: epoch  7, batch    32 | loss: 7.2134600Losses:  6.354197978973389 2.1284656524658203 0.2210891842842102
CurrentTrain: epoch  7, batch    33 | loss: 6.3541980Losses:  6.754740238189697 2.4615330696105957 0.22680774331092834
CurrentTrain: epoch  7, batch    34 | loss: 6.7547402Losses:  7.092283248901367 2.733670949935913 0.23898208141326904
CurrentTrain: epoch  7, batch    35 | loss: 7.0922832Losses:  6.825952053070068 2.587409734725952 0.21852722764015198
CurrentTrain: epoch  7, batch    36 | loss: 6.8259521Losses:  8.266572952270508 4.088801383972168 0.22982372343540192
CurrentTrain: epoch  7, batch    37 | loss: 8.2665730Losses:  7.304047584533691 2.974764823913574 0.2524913549423218
CurrentTrain: epoch  7, batch    38 | loss: 7.3040476Losses:  7.187562942504883 2.953852653503418 0.22829470038414001
CurrentTrain: epoch  7, batch    39 | loss: 7.1875629Losses:  7.133275985717773 2.863518238067627 0.22979578375816345
CurrentTrain: epoch  7, batch    40 | loss: 7.1332760Losses:  7.5519819259643555 3.103349208831787 0.24191316962242126
CurrentTrain: epoch  7, batch    41 | loss: 7.5519819Losses:  6.091622352600098 1.8318980932235718 0.2217479795217514
CurrentTrain: epoch  7, batch    42 | loss: 6.0916224Losses:  8.89612102508545 4.449333667755127 0.24884919822216034
CurrentTrain: epoch  7, batch    43 | loss: 8.8961210Losses:  8.576593399047852 4.272432327270508 0.25220346450805664
CurrentTrain: epoch  7, batch    44 | loss: 8.5765934Losses:  6.9374470710754395 2.598700761795044 0.22812320291996002
CurrentTrain: epoch  7, batch    45 | loss: 6.9374471Losses:  7.158089637756348 2.925426721572876 0.22115537524223328
CurrentTrain: epoch  7, batch    46 | loss: 7.1580896Losses:  6.908770561218262 2.631300449371338 0.2322182059288025
CurrentTrain: epoch  7, batch    47 | loss: 6.9087706Losses:  6.685208797454834 2.391439914703369 0.23074716329574585
CurrentTrain: epoch  7, batch    48 | loss: 6.6852088Losses:  9.127490043640137 4.915546894073486 0.23769748210906982
CurrentTrain: epoch  7, batch    49 | loss: 9.1274900Losses:  7.077779769897461 2.893528461456299 0.24428853392601013
CurrentTrain: epoch  7, batch    50 | loss: 7.0777798Losses:  6.508829593658447 2.1419625282287598 0.2220371812582016
CurrentTrain: epoch  7, batch    51 | loss: 6.5088296Losses:  7.311418056488037 2.9382424354553223 0.23180106282234192
CurrentTrain: epoch  7, batch    52 | loss: 7.3114181Losses:  9.413691520690918 5.08514404296875 0.26271021366119385
CurrentTrain: epoch  7, batch    53 | loss: 9.4136915Losses:  8.720193862915039 4.466334342956543 0.2321435809135437
CurrentTrain: epoch  7, batch    54 | loss: 8.7201939Losses:  6.703521728515625 2.4693188667297363 0.23171862959861755
CurrentTrain: epoch  7, batch    55 | loss: 6.7035217Losses:  6.247651100158691 1.9830868244171143 0.22415360808372498
CurrentTrain: epoch  7, batch    56 | loss: 6.2476511Losses:  8.407683372497559 4.196840286254883 0.2214895337820053
CurrentTrain: epoch  7, batch    57 | loss: 8.4076834Losses:  7.371140003204346 3.1244447231292725 0.23680055141448975
CurrentTrain: epoch  7, batch    58 | loss: 7.3711400Losses:  8.339459419250488 4.082453727722168 0.23271587491035461
CurrentTrain: epoch  7, batch    59 | loss: 8.3394594Losses:  9.02706241607666 4.756945610046387 0.2465890794992447
CurrentTrain: epoch  7, batch    60 | loss: 9.0270624Losses:  11.224228858947754 7.050392150878906 0.23644967377185822
CurrentTrain: epoch  7, batch    61 | loss: 11.2242289Losses:  4.786423206329346 0.5193611979484558 0.26922890543937683
CurrentTrain: epoch  7, batch    62 | loss: 4.7864232Losses:  7.805477142333984 3.556363821029663 0.25180089473724365
CurrentTrain: epoch  8, batch     0 | loss: 7.8054771Losses:  15.689879417419434 11.430177688598633 0.24338647723197937
CurrentTrain: epoch  8, batch     1 | loss: 15.6898794Losses:  9.2347412109375 4.948938846588135 0.25902578234672546
CurrentTrain: epoch  8, batch     2 | loss: 9.2347412Losses:  12.676539421081543 8.45797061920166 0.2548940181732178
CurrentTrain: epoch  8, batch     3 | loss: 12.6765394Losses:  6.872795104980469 2.7178726196289062 0.22526443004608154
CurrentTrain: epoch  8, batch     4 | loss: 6.8727951Losses:  6.560668468475342 2.3299102783203125 0.22923603653907776
CurrentTrain: epoch  8, batch     5 | loss: 6.5606685Losses:  8.906966209411621 4.578388214111328 0.2563822269439697
CurrentTrain: epoch  8, batch     6 | loss: 8.9069662Losses:  8.692968368530273 4.468491554260254 0.24567961692810059
CurrentTrain: epoch  8, batch     7 | loss: 8.6929684Losses:  9.745078086853027 5.562326908111572 0.2528904378414154
CurrentTrain: epoch  8, batch     8 | loss: 9.7450781Losses:  5.82488489151001 1.6027833223342896 0.2092975676059723
CurrentTrain: epoch  8, batch     9 | loss: 5.8248849Losses:  6.74609375 2.5281758308410645 0.2257116436958313
CurrentTrain: epoch  8, batch    10 | loss: 6.7460938Losses:  7.657203197479248 3.3722660541534424 0.24592158198356628
CurrentTrain: epoch  8, batch    11 | loss: 7.6572032Losses:  7.106997013092041 2.9362525939941406 0.2252049297094345
CurrentTrain: epoch  8, batch    12 | loss: 7.1069970Losses:  7.068237781524658 2.834585666656494 0.23305638134479523
CurrentTrain: epoch  8, batch    13 | loss: 7.0682378Losses:  8.926507949829102 4.651351451873779 0.25524163246154785
CurrentTrain: epoch  8, batch    14 | loss: 8.9265079Losses:  7.623582363128662 3.5002052783966064 0.14585618674755096
CurrentTrain: epoch  8, batch    15 | loss: 7.6235824Losses:  7.292763710021973 3.0681052207946777 0.22288212180137634
CurrentTrain: epoch  8, batch    16 | loss: 7.2927637Losses:  7.1637091636657715 2.939887285232544 0.23700305819511414
CurrentTrain: epoch  8, batch    17 | loss: 7.1637092Losses:  7.308953285217285 3.0605525970458984 0.2371845543384552
CurrentTrain: epoch  8, batch    18 | loss: 7.3089533Losses:  6.167765140533447 1.9930341243743896 0.2243761122226715
CurrentTrain: epoch  8, batch    19 | loss: 6.1677651Losses:  6.558032035827637 2.306007146835327 0.23017792403697968
CurrentTrain: epoch  8, batch    20 | loss: 6.5580320Losses:  8.554317474365234 4.351366996765137 0.23093563318252563
CurrentTrain: epoch  8, batch    21 | loss: 8.5543175Losses:  7.422765731811523 3.178628444671631 0.23802700638771057
CurrentTrain: epoch  8, batch    22 | loss: 7.4227657Losses:  7.416873455047607 3.179593801498413 0.22932776808738708
CurrentTrain: epoch  8, batch    23 | loss: 7.4168735Losses:  5.8100738525390625 1.5857130289077759 0.21398130059242249
CurrentTrain: epoch  8, batch    24 | loss: 5.8100739Losses:  8.39752197265625 4.108948230743408 0.23916828632354736
CurrentTrain: epoch  8, batch    25 | loss: 8.3975220Losses:  5.8180832862854 1.606290340423584 0.2143801748752594
CurrentTrain: epoch  8, batch    26 | loss: 5.8180833Losses:  6.746452808380127 2.523402690887451 0.23431384563446045
CurrentTrain: epoch  8, batch    27 | loss: 6.7464528Losses:  7.951446533203125 3.7658371925354004 0.21751967072486877
CurrentTrain: epoch  8, batch    28 | loss: 7.9514465Losses:  7.117308616638184 2.9297986030578613 0.222175732254982
CurrentTrain: epoch  8, batch    29 | loss: 7.1173086Losses:  7.440965175628662 3.1673974990844727 0.24572807550430298
CurrentTrain: epoch  8, batch    30 | loss: 7.4409652Losses:  6.847427845001221 2.6517128944396973 0.2250962108373642
CurrentTrain: epoch  8, batch    31 | loss: 6.8474278Losses:  7.903546333312988 3.673051118850708 0.2513291835784912
CurrentTrain: epoch  8, batch    32 | loss: 7.9035463Losses:  8.915139198303223 4.699440956115723 0.23454684019088745
CurrentTrain: epoch  8, batch    33 | loss: 8.9151392Losses:  5.781708717346191 1.5629767179489136 0.21589401364326477
CurrentTrain: epoch  8, batch    34 | loss: 5.7817087Losses:  6.942418098449707 2.683593988418579 0.22358226776123047
CurrentTrain: epoch  8, batch    35 | loss: 6.9424181Losses:  6.389772891998291 2.1707587242126465 0.2281697690486908
CurrentTrain: epoch  8, batch    36 | loss: 6.3897729Losses:  7.7697272300720215 3.532188892364502 0.2368852198123932
CurrentTrain: epoch  8, batch    37 | loss: 7.7697272Losses:  7.326817989349365 3.1353559494018555 0.2266169935464859
CurrentTrain: epoch  8, batch    38 | loss: 7.3268180Losses:  5.81522274017334 1.6021119356155396 0.20968809723854065
CurrentTrain: epoch  8, batch    39 | loss: 5.8152227Losses:  6.346644401550293 2.1341042518615723 0.2238263040781021
CurrentTrain: epoch  8, batch    40 | loss: 6.3466444Losses:  6.519136428833008 2.3455824851989746 0.23330581188201904
CurrentTrain: epoch  8, batch    41 | loss: 6.5191364Losses:  10.068682670593262 5.8426923751831055 0.24364779889583588
CurrentTrain: epoch  8, batch    42 | loss: 10.0686827Losses:  8.434393882751465 4.2160210609436035 0.23664149641990662
CurrentTrain: epoch  8, batch    43 | loss: 8.4343939Losses:  7.833380222320557 3.6451995372772217 0.24625156819820404
CurrentTrain: epoch  8, batch    44 | loss: 7.8333802Losses:  7.300853729248047 3.055619239807129 0.23185262084007263
CurrentTrain: epoch  8, batch    45 | loss: 7.3008537Losses:  6.542778968811035 2.3147194385528564 0.2297385185956955
CurrentTrain: epoch  8, batch    46 | loss: 6.5427790Losses:  8.867456436157227 4.611840724945068 0.24686944484710693
CurrentTrain: epoch  8, batch    47 | loss: 8.8674564Losses:  6.892895221710205 2.703671932220459 0.23175083100795746
CurrentTrain: epoch  8, batch    48 | loss: 6.8928952Losses:  7.158373832702637 2.9649131298065186 0.22432799637317657
CurrentTrain: epoch  8, batch    49 | loss: 7.1583738Losses:  8.549056053161621 4.366055488586426 0.24194681644439697
CurrentTrain: epoch  8, batch    50 | loss: 8.5490561Losses:  5.747716903686523 1.579301118850708 0.21032872796058655
CurrentTrain: epoch  8, batch    51 | loss: 5.7477169Losses:  7.8899126052856445 3.740713119506836 0.1515485644340515
CurrentTrain: epoch  8, batch    52 | loss: 7.8899126Losses:  5.785444736480713 1.6093168258666992 0.21197162568569183
CurrentTrain: epoch  8, batch    53 | loss: 5.7854447Losses:  6.869358062744141 2.7420969009399414 0.14626702666282654
CurrentTrain: epoch  8, batch    54 | loss: 6.8693581Losses:  8.832330703735352 4.6191205978393555 0.23779883980751038
CurrentTrain: epoch  8, batch    55 | loss: 8.8323307Losses:  7.283395767211914 3.090822458267212 0.22607235610485077
CurrentTrain: epoch  8, batch    56 | loss: 7.2833958Losses:  7.358944416046143 3.123136043548584 0.23645351827144623
CurrentTrain: epoch  8, batch    57 | loss: 7.3589444Losses:  10.33949089050293 6.055685520172119 0.2440931648015976
CurrentTrain: epoch  8, batch    58 | loss: 10.3394909Losses:  9.152743339538574 4.884077072143555 0.2460644394159317
CurrentTrain: epoch  8, batch    59 | loss: 9.1527433Losses:  6.883569717407227 2.6958742141723633 0.22298642992973328
CurrentTrain: epoch  8, batch    60 | loss: 6.8835697Losses:  7.137990951538086 2.933310031890869 0.2356444001197815
CurrentTrain: epoch  8, batch    61 | loss: 7.1379910Losses:  4.966562271118164 0.7631698846817017 0.1885240077972412
CurrentTrain: epoch  8, batch    62 | loss: 4.9665623Losses:  6.253649711608887 2.096036434173584 0.22368201613426208
CurrentTrain: epoch  9, batch     0 | loss: 6.2536497Losses:  7.1604180335998535 2.9313859939575195 0.23787279427051544
CurrentTrain: epoch  9, batch     1 | loss: 7.1604180Losses:  6.340791702270508 2.104295492172241 0.21794214844703674
CurrentTrain: epoch  9, batch     2 | loss: 6.3407917Losses:  7.313943862915039 3.083648920059204 0.23012426495552063
CurrentTrain: epoch  9, batch     3 | loss: 7.3139439Losses:  6.92772912979126 2.6968932151794434 0.23685869574546814
CurrentTrain: epoch  9, batch     4 | loss: 6.9277291Losses:  7.2705979347229 3.0574893951416016 0.2324371337890625
CurrentTrain: epoch  9, batch     5 | loss: 7.2705979Losses:  7.087527751922607 2.8572864532470703 0.22444678843021393
CurrentTrain: epoch  9, batch     6 | loss: 7.0875278Losses:  6.395225524902344 2.1864285469055176 0.21945488452911377
CurrentTrain: epoch  9, batch     7 | loss: 6.3952255Losses:  7.859317302703857 3.6574783325195312 0.2403194010257721
CurrentTrain: epoch  9, batch     8 | loss: 7.8593173Losses:  5.768304347991943 1.5722084045410156 0.2081056386232376
CurrentTrain: epoch  9, batch     9 | loss: 5.7683043Losses:  6.339571475982666 2.1604223251342773 0.21922166645526886
CurrentTrain: epoch  9, batch    10 | loss: 6.3395715Losses:  6.68020486831665 2.4900646209716797 0.2269999235868454
CurrentTrain: epoch  9, batch    11 | loss: 6.6802049Losses:  6.744448661804199 2.5471343994140625 0.217146098613739
CurrentTrain: epoch  9, batch    12 | loss: 6.7444487Losses:  8.077260971069336 3.8028321266174316 0.24639469385147095
CurrentTrain: epoch  9, batch    13 | loss: 8.0772610Losses:  6.18796443939209 1.9968552589416504 0.2226761281490326
CurrentTrain: epoch  9, batch    14 | loss: 6.1879644Losses:  6.285680770874023 2.1057512760162354 0.220270037651062
CurrentTrain: epoch  9, batch    15 | loss: 6.2856808Losses:  7.877112865447998 3.6693310737609863 0.24023663997650146
CurrentTrain: epoch  9, batch    16 | loss: 7.8771129Losses:  8.201044082641602 3.9948368072509766 0.23474730551242828
CurrentTrain: epoch  9, batch    17 | loss: 8.2010441Losses:  8.03797721862793 3.8369829654693604 0.2248157113790512
CurrentTrain: epoch  9, batch    18 | loss: 8.0379772Losses:  7.094995498657227 2.9000377655029297 0.22389096021652222
CurrentTrain: epoch  9, batch    19 | loss: 7.0949955Losses:  7.0555315017700195 2.7888412475585938 0.2364140897989273
CurrentTrain: epoch  9, batch    20 | loss: 7.0555315Losses:  6.61066198348999 2.3870272636413574 0.2369455099105835
CurrentTrain: epoch  9, batch    21 | loss: 6.6106620Losses:  6.650398254394531 2.456624984741211 0.22330495715141296
CurrentTrain: epoch  9, batch    22 | loss: 6.6503983Losses:  7.231313228607178 2.986431837081909 0.23308885097503662
CurrentTrain: epoch  9, batch    23 | loss: 7.2313132Losses:  8.403486251831055 4.1736578941345215 0.22655141353607178
CurrentTrain: epoch  9, batch    24 | loss: 8.4034863Losses:  6.741456031799316 2.5385866165161133 0.22166821360588074
CurrentTrain: epoch  9, batch    25 | loss: 6.7414560Losses:  6.287444591522217 2.1017119884490967 0.2212902307510376
CurrentTrain: epoch  9, batch    26 | loss: 6.2874446Losses:  9.964402198791504 5.7802414894104 0.23397400975227356
CurrentTrain: epoch  9, batch    27 | loss: 9.9644022Losses:  7.374034404754639 3.2235350608825684 0.227920264005661
CurrentTrain: epoch  9, batch    28 | loss: 7.3740344Losses:  7.28570556640625 3.048922538757324 0.2284279763698578
CurrentTrain: epoch  9, batch    29 | loss: 7.2857056Losses:  11.016105651855469 6.769783020019531 0.24387970566749573
CurrentTrain: epoch  9, batch    30 | loss: 11.0161057Losses:  7.121209621429443 2.9478306770324707 0.21973735094070435
CurrentTrain: epoch  9, batch    31 | loss: 7.1212096Losses:  7.324506759643555 3.1548686027526855 0.23555892705917358
CurrentTrain: epoch  9, batch    32 | loss: 7.3245068Losses:  9.062688827514648 4.8970489501953125 0.2401200234889984
CurrentTrain: epoch  9, batch    33 | loss: 9.0626888Losses:  7.219784259796143 3.0503225326538086 0.2236742526292801
CurrentTrain: epoch  9, batch    34 | loss: 7.2197843Losses:  7.167759418487549 2.957484722137451 0.22500449419021606
CurrentTrain: epoch  9, batch    35 | loss: 7.1677594Losses:  6.3938117027282715 2.195335865020752 0.21853190660476685
CurrentTrain: epoch  9, batch    36 | loss: 6.3938117Losses:  8.736152648925781 4.600102424621582 0.15511982142925262
CurrentTrain: epoch  9, batch    37 | loss: 8.7361526Losses:  7.087092876434326 2.905815601348877 0.22786390781402588
CurrentTrain: epoch  9, batch    38 | loss: 7.0870929Losses:  5.967794895172119 1.7726433277130127 0.22121402621269226
CurrentTrain: epoch  9, batch    39 | loss: 5.9677949Losses:  8.323968887329102 4.136159896850586 0.22011522948741913
CurrentTrain: epoch  9, batch    40 | loss: 8.3239689Losses:  6.524689197540283 2.3233110904693604 0.21851982176303864
CurrentTrain: epoch  9, batch    41 | loss: 6.5246892Losses:  6.332653999328613 2.144747257232666 0.22507476806640625
CurrentTrain: epoch  9, batch    42 | loss: 6.3326540Losses:  8.759986877441406 4.527034759521484 0.24836193025112152
CurrentTrain: epoch  9, batch    43 | loss: 8.7599869Losses:  5.767568111419678 1.5980618000030518 0.20751523971557617
CurrentTrain: epoch  9, batch    44 | loss: 5.7675681Losses:  6.513286590576172 2.3848278522491455 0.14176476001739502
CurrentTrain: epoch  9, batch    45 | loss: 6.5132866Losses:  6.519715785980225 2.3416810035705566 0.21290452778339386
CurrentTrain: epoch  9, batch    46 | loss: 6.5197158Losses:  9.704970359802246 5.558681488037109 0.16966521739959717
CurrentTrain: epoch  9, batch    47 | loss: 9.7049704Losses:  7.1080145835876465 2.870730400085449 0.22170069813728333
CurrentTrain: epoch  9, batch    48 | loss: 7.1080146Losses:  7.315772533416748 3.133411407470703 0.23389561474323273
CurrentTrain: epoch  9, batch    49 | loss: 7.3157725Losses:  7.1092023849487305 2.934501886367798 0.21873077750205994
CurrentTrain: epoch  9, batch    50 | loss: 7.1092024Losses:  6.9974212646484375 2.897397756576538 0.24178045988082886
CurrentTrain: epoch  9, batch    51 | loss: 6.9974213Losses:  7.930455207824707 3.6931424140930176 0.24773384630680084
CurrentTrain: epoch  9, batch    52 | loss: 7.9304552Losses:  6.916027069091797 2.6789541244506836 0.23174870014190674
CurrentTrain: epoch  9, batch    53 | loss: 6.9160271Losses:  6.274236679077148 2.1270666122436523 0.21989792585372925
CurrentTrain: epoch  9, batch    54 | loss: 6.2742367Losses:  6.324430465698242 2.119948387145996 0.22612789273262024
CurrentTrain: epoch  9, batch    55 | loss: 6.3244305Losses:  10.427563667297363 6.225437164306641 0.2425193190574646
CurrentTrain: epoch  9, batch    56 | loss: 10.4275637Losses:  11.438143730163574 7.338066101074219 0.16209714114665985
CurrentTrain: epoch  9, batch    57 | loss: 11.4381437Losses:  8.281301498413086 4.116483211517334 0.25168436765670776
CurrentTrain: epoch  9, batch    58 | loss: 8.2813015Losses:  8.04299259185791 3.8616762161254883 0.2509961724281311
CurrentTrain: epoch  9, batch    59 | loss: 8.0429926Losses:  7.1929426193237305 3.0590195655822754 0.23371939361095428
CurrentTrain: epoch  9, batch    60 | loss: 7.1929426Losses:  6.50131368637085 2.3355581760406494 0.20584939420223236
CurrentTrain: epoch  9, batch    61 | loss: 6.5013137Losses:  4.913437843322754 0.7137844562530518 0.25738805532455444
CurrentTrain: epoch  9, batch    62 | loss: 4.9134378
#### movement ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "movement" means literary, artistic, scientific or philosophical movement or scene associated with this person or work.
Here is an example:
Relation: movement
Context: born in copenhagen , as a young man bang became interested in surrealism as practised by giorgio de chirico , max ernst and yves tanguy .
Head Entity: yves tanguy
Tail Entity: surrealism
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "movement":
Relation: movement  
Context: during the early 20th century, the artist was heavily influenced by the cubist movement, which was pioneered by pablo picasso and georges braque.  
Head Entity: pablo picasso  
Tail Entity: cubist movement  

Relation: movement  
Context: the philosopher's ideas were deeply rooted in the existentialist movement, which gained prominence in the mid-20th century through the works of thinkers like jean-paul sartre.  
Head Entity: jean-paul sartre  
Tail Entity: existentialist movement  
#### located in the administrative territorial entity ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "located in the administrative territorial entity" means the item is located on the territory of the following administrative entity.
Here is an example:
Relation: located in the administrative territorial entity
Context: it linked bundoran and ballyshannon on the atlantic coast of donegal with the londonderry and enniskillen railway ( l&er ; ) at in fermanagh .
Head Entity: ballyshannon
Tail Entity: donegal
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "located in the administrative territorial entity":
Relation: located in the administrative territorial entity  
Context: the city of bristol is known for its vibrant culture and is situated in the southwest region of england.  
Head Entity: bristol  
Tail Entity: england  

Relation: located in the administrative territorial entity  
Context: the famous statue of liberty stands proudly on liberty island, which is part of new york harbor.  
Head Entity: liberty island  
Tail Entity: new york
#### head of government ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "head of government" means head of the executive power of this town, city, municipality, state, country, or other governmental body.
Here is an example:
Relation: head of government
Context: on february 10 , 2010 , stefan mappus was elected minister - president of baden - württemberg , and asked that all ministers continue their roles .
Head Entity: baden - württemberg
Tail Entity: stefan mappus
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "head of government":
Relation: head of government  
Context: in 2021, jacinda ardern was re-elected as the prime minister of new zealand, leading the labor party to a significant victory in the general elections.  
Head Entity: new zealand  
Tail Entity: jacinda ardern  

Relation: head of government  
Context: during the annual summit, justin trudeau addressed the nation as the prime minister of canada, outlining his government's plans for the upcoming year.  
Head Entity: canada  
Tail Entity: justin trudeau  
#### member of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "member of" means organization, club or musical group to which the subject belongs.
Here is an example:
Relation: member of
Context: rage against the machine bandmates , tom morello , brad wilk , and de la rocha , as well as audioslave bandmate , chris cornell , came to support commerford .
Head Entity: chris cornell
Tail Entity: audioslave
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "member of":
Relation: member of  
Context: The famous actor and musician, Jared Leto, is not only known for his role in "Requiem for a Dream" but also as the lead vocalist of the band Thirty Seconds to Mars.  
Head Entity: Jared Leto  
Tail Entity: Thirty Seconds to Mars  

Relation: member of  
Context: The renowned physicist Stephen Hawking was a prominent member of the Cambridge University community, contributing significantly to its research and academic life.  
Head Entity: Stephen Hawking  
Tail Entity: Cambridge University  
#### crosses ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "crosses" means obstacle which this bridge crosses over or this tunnel goes under.
Here is an example:
Relation: crosses
Context: on july 2 , 1935 , the rip van winkle bridge over the hudson river between catskill and greenport was opened to traffic .
Head Entity: rip van winkle bridge
Tail Entity: hudson river
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "crosses":
Relation: crosses  
Context: The Golden Gate Bridge spans the Golden Gate Strait, connecting San Francisco to Marin County, and is a vital link for traffic crossing over the water.  
Head Entity: Golden Gate Bridge  
Tail Entity: Golden Gate Strait  

Relation: crosses  
Context: The Channel Tunnel, also known as the Chunnel, runs beneath the English Channel, providing a rail link between the UK and mainland Europe.  
Head Entity: Channel Tunnel  
Tail Entity: English Channel  
#### tributary ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "tributary" means watercourse that flows into an other one.
Here is an example:
Relation: tributary
Context: the municipality contains the lower reaches of the maracanã river , which is joined by the caripi river before widening into maracanã bay and emptying into the atlantic ocean beside maiandeua island .
Head Entity: maracanã river
Tail Entity: caripi river
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "tributary":
Relation: tributary  
Context: The Amazon River, one of the largest rivers in the world, receives numerous tributaries, including the Madeira River, which significantly contributes to its flow.  
Head Entity: Amazon River  
Tail Entity: Madeira River  

Relation: tributary  
Context: The Mississippi River is fed by several smaller rivers, with the Missouri River being one of its major tributaries that enhances its water volume.  
Head Entity: Mississippi River  
Tail Entity: Missouri River  
#### position held ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "position held" means subject currently or formerly holds the object position or public office.
Here is an example:
Relation: position held
Context: after a review was conducted by former first minister of scotland henry mcleish , the sfa board structure was streamlined considerably in 2011 .
Head Entity: henry mcleish
Tail Entity: first minister of scotland
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "position held":
Relation: position held  
Context: In 2020, the renowned scientist Dr. Jane Smith was appointed as the Chief Research Officer at the National Institute of Health.  
Head Entity: Dr. Jane Smith  
Tail Entity: Chief Research Officer  

Relation: position held  
Context: After serving two terms, Mayor John Doe announced his retirement from the position of Mayor of Springfield in 2022.  
Head Entity: Mayor John Doe  
Tail Entity: Mayor of Springfield  
#### developer ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "developer" means organization or person that developed the item.
Here is an example:
Relation: developer
Context: it was developed by syn sophia , who also developed the other games in the " style savvy " series .
Head Entity: style savvy
Tail Entity: syn sophia
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "developer":
Relation: developer  
Context: the groundbreaking software was created by tech innovations inc., a leader in the field of artificial intelligence.  
Head Entity: artificial intelligence  
Tail Entity: tech innovations inc.  

Relation: developer  
Context: the popular mobile app was designed by creative minds ltd., which specializes in user-friendly applications.  
Head Entity: mobile app  
Tail Entity: creative minds ltd.  
#### location of formation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "location of formation" means location where a group or organization was formed.
Here is an example:
Relation: location of formation
Context: in 2012 , anderson lent his keyboard talent to the seattle supergroup walking papers .
Head Entity: walking papers
Tail Entity: seattle
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "location of formation":
Relation: location of formation  
Context: The famous band was established in London in the early 1960s, quickly gaining popularity across the globe.  
Head Entity: The famous band  
Tail Entity: London  

Relation: location of formation  
Context: In 1995, the tech startup was founded in Silicon Valley, becoming a leader in innovation.  
Head Entity: the tech startup  
Tail Entity: Silicon Valley  
#### country of origin ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "country of origin" means country of origin of this item (creative work, food, phrase, product, etc.).
Here is an example:
Relation: country of origin
Context: " shining time station " won a number of awards and significantly increased the popularity of the " thomas " media franchise in the united states .
Head Entity: shining time station
Tail Entity: united states
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "country of origin":
Relation: country of origin  
Context: The famous dish "sushi" is traditionally associated with Japan and has gained immense popularity worldwide.  
Head Entity: sushi  
Tail Entity: Japan  

Relation: country of origin  
Context: The iconic brand "Guinness" is known for its rich stout beer, which originated in Ireland and is now enjoyed globally.  
Head Entity: Guinness  
Tail Entity: Ireland  
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 85.94%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 86.25%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 88.39%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 89.84%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 90.97%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 91.88%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 92.61%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 93.23%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 93.27%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 93.30%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 93.33%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 94.12%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 94.10%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 94.41%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 94.69%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 94.94%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 95.17%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 95.38%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 95.31%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 95.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 95.19%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 95.14%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 95.09%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 95.04%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 95.21%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 95.36%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 95.51%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 95.64%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 95.77%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 95.71%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 95.66%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 95.78%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 95.72%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 95.83%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 95.94%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 96.04%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 96.13%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 96.22%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 96.31%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 96.25%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 96.33%   [EVAL] batch:   46 | acc: 87.50%,  total acc: 96.14%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 96.09%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 96.17%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 96.25%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 96.20%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 96.03%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 95.99%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 96.06%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 95.80%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 95.65%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 95.61%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 95.58%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 95.66%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 95.62%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 95.70%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 95.67%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 94.94%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 85.94%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 86.25%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 88.39%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 89.84%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 90.97%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 91.88%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 92.61%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 93.23%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 93.27%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 93.30%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 93.33%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 94.12%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 94.10%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 94.41%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 94.69%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 94.94%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 95.17%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 95.38%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 95.31%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 95.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 95.19%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 95.14%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 95.09%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 95.04%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 95.21%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 95.36%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 95.51%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 95.64%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 95.77%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 95.71%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 95.66%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 95.78%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 95.72%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 95.83%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 95.94%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 96.04%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 96.13%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 96.22%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 96.31%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 96.25%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 96.33%   [EVAL] batch:   46 | acc: 87.50%,  total acc: 96.14%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 96.09%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 96.17%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 96.25%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 96.20%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 96.03%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 95.99%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 96.06%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 95.80%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 95.65%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 95.61%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 95.58%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 95.66%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 95.62%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 95.70%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 95.67%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 94.94%   
cur_acc:  ['0.9494']
his_acc:  ['0.9494']
Clustering into  9  clusters
Clusters:  [8 2 7 3 1 1 7 4 2 6 2 3 4 0 5 0 4 2 4 2]
Losses:  13.796442985534668 3.859431743621826 0.4790849983692169
CurrentTrain: epoch  0, batch     0 | loss: 13.7964430Losses:  10.953760147094727 3.103928327560425 0.478649765253067
CurrentTrain: epoch  0, batch     1 | loss: 10.9537601Losses:  12.908706665039062 4.31768798828125 0.6144812703132629
CurrentTrain: epoch  0, batch     2 | loss: 12.9087067Losses:  9.223347663879395 -0.0 0.1632116138935089
CurrentTrain: epoch  0, batch     3 | loss: 9.2233477Losses:  10.974295616149902 2.9696786403656006 0.46903249621391296
CurrentTrain: epoch  1, batch     0 | loss: 10.9742956Losses:  14.338905334472656 5.715264320373535 0.4672214090824127
CurrentTrain: epoch  1, batch     1 | loss: 14.3389053Losses:  11.486595153808594 3.416323661804199 0.5060707926750183
CurrentTrain: epoch  1, batch     2 | loss: 11.4865952Losses:  6.532611846923828 -0.0 0.11279114335775375
CurrentTrain: epoch  1, batch     3 | loss: 6.5326118Losses:  12.054949760437012 4.527072906494141 0.5435642600059509
CurrentTrain: epoch  2, batch     0 | loss: 12.0549498Losses:  11.061500549316406 3.6504945755004883 0.5317367911338806
CurrentTrain: epoch  2, batch     1 | loss: 11.0615005Losses:  10.90070629119873 4.039344787597656 0.41533809900283813
CurrentTrain: epoch  2, batch     2 | loss: 10.9007063Losses:  5.217300891876221 -0.0 0.08154255896806717
CurrentTrain: epoch  2, batch     3 | loss: 5.2173009Losses:  10.457724571228027 3.8333382606506348 0.47650226950645447
CurrentTrain: epoch  3, batch     0 | loss: 10.4577246Losses:  9.589140892028809 3.147671699523926 0.37224072217941284
CurrentTrain: epoch  3, batch     1 | loss: 9.5891409Losses:  9.899354934692383 3.051245927810669 0.49878254532814026
CurrentTrain: epoch  3, batch     2 | loss: 9.8993549Losses:  6.497745990753174 -0.0 0.16188369691371918
CurrentTrain: epoch  3, batch     3 | loss: 6.4977460Losses:  9.268723487854004 3.118492603302002 0.5513342618942261
CurrentTrain: epoch  4, batch     0 | loss: 9.2687235Losses:  10.459406852722168 4.575836181640625 0.3736420273780823
CurrentTrain: epoch  4, batch     1 | loss: 10.4594069Losses:  8.324875831604004 2.5820400714874268 0.47934451699256897
CurrentTrain: epoch  4, batch     2 | loss: 8.3248758Losses:  3.134340286254883 -0.0 0.13061988353729248
CurrentTrain: epoch  4, batch     3 | loss: 3.1343403Losses:  8.266336441040039 2.9723167419433594 0.5067448616027832
CurrentTrain: epoch  5, batch     0 | loss: 8.2663364Losses:  9.99445915222168 4.631859302520752 0.5077892541885376
CurrentTrain: epoch  5, batch     1 | loss: 9.9944592Losses:  8.467039108276367 2.610457181930542 0.47225356101989746
CurrentTrain: epoch  5, batch     2 | loss: 8.4670391Losses:  5.982004165649414 -0.0 -0.0
CurrentTrain: epoch  5, batch     3 | loss: 5.9820042Losses:  7.347968578338623 2.393692970275879 0.4348464012145996
CurrentTrain: epoch  6, batch     0 | loss: 7.3479686Losses:  8.227082252502441 3.4379730224609375 0.5088117122650146
CurrentTrain: epoch  6, batch     1 | loss: 8.2270823Losses:  8.837652206420898 3.5327534675598145 0.45013830065727234
CurrentTrain: epoch  6, batch     2 | loss: 8.8376522Losses:  3.917781114578247 -0.0 0.1084253266453743
CurrentTrain: epoch  6, batch     3 | loss: 3.9177811Losses:  7.559175491333008 3.2829177379608154 0.4911021292209625
CurrentTrain: epoch  7, batch     0 | loss: 7.5591755Losses:  6.296445369720459 2.103365898132324 0.4708436131477356
CurrentTrain: epoch  7, batch     1 | loss: 6.2964454Losses:  9.444707870483398 4.089841842651367 0.4440549314022064
CurrentTrain: epoch  7, batch     2 | loss: 9.4447079Losses:  2.179281234741211 -0.0 0.14409422874450684
CurrentTrain: epoch  7, batch     3 | loss: 2.1792812Losses:  6.539737701416016 2.2963778972625732 0.4618252217769623
CurrentTrain: epoch  8, batch     0 | loss: 6.5397377Losses:  6.9354166984558105 2.3525376319885254 0.4446896016597748
CurrentTrain: epoch  8, batch     1 | loss: 6.9354167Losses:  6.245217800140381 2.147437572479248 0.44619423151016235
CurrentTrain: epoch  8, batch     2 | loss: 6.2452178Losses:  3.0260729789733887 -0.0 -0.0
CurrentTrain: epoch  8, batch     3 | loss: 3.0260730Losses:  6.05527400970459 2.55580735206604 0.41988396644592285
CurrentTrain: epoch  9, batch     0 | loss: 6.0552740Losses:  6.518457412719727 2.200871229171753 0.45229125022888184
CurrentTrain: epoch  9, batch     1 | loss: 6.5184574Losses:  6.988275527954102 2.6295127868652344 0.4541109502315521
CurrentTrain: epoch  9, batch     2 | loss: 6.9882755Losses:  3.2311861515045166 -0.0 0.1220356822013855
CurrentTrain: epoch  9, batch     3 | loss: 3.2311862
#### located on terrain feature ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "located on terrain feature" means located on the specified physical/geophysical feature.
Here is an example:
Relation: located on terrain feature
Context: the andesite line , a zone of intense volcanic and seismic activity , is a major regional distinction in the pacific .
Head Entity: andesite line
Tail Entity: pacific
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "located on terrain feature":
Relation: located on terrain feature  
Context: The Great Barrier Reef, a stunning natural wonder, stretches along the northeastern coast of Australia.  
Head Entity: Great Barrier Reef  
Tail Entity: Australia  

Relation: located on terrain feature  
Context: Mount Everest, the highest peak in the world, is part of the Himalayas and attracts climbers from around the globe.  
Head Entity: Mount Everest  
Tail Entity: Himalayas  
#### participant of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "participant of" means event in which a person or organization was/is a participant.
Here is an example:
Relation: participant of
Context: tiago venâncio ( born august 19 , 1987 , setúbal ) is a freestyle swimmer from portugal , who represented his home country at the 2004 summer olympics in athens , greece .
Head Entity: tiago venâncio
Tail Entity: 2004 summer olympics
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "participant of":
Relation: participant of  
Context: the united nations climate change conference, held in paris in 2015, saw numerous world leaders and activists come together to discuss global warming.  
Head Entity: united nations climate change conference  
Tail Entity: paris agreement  

Relation: participant of  
Context: the 2022 fifa world cup in qatar featured teams from around the globe competing for the prestigious trophy.  
Head Entity: 2022 fifa world cup  
Tail Entity: qatar
#### director ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "director" means director of film, TV-series, stageplay, video game or similar.
Here is an example:
Relation: director
Context: " runaway reptar " was written by ali marie matheson and jon cooksey and directed by john holmquist and jim duffy .
Head Entity: runaway reptar
Tail Entity: john holmquist
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "director":
Relation: director  
Context: "Inception" was directed by Christopher Nolan, who is known for his innovative storytelling and visual effects.  
Head Entity: Inception  
Tail Entity: Christopher Nolan  

Relation: director  
Context: The acclaimed stage play "Hamilton" was directed by Thomas Kail, bringing Lin-Manuel Miranda's vision to life on Broadway.  
Head Entity: Hamilton  
Tail Entity: Thomas Kail  
#### owned by ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "owned by" means owner of the subject.
Here is an example:
Relation: owned by
Context: as of april 2013 , sony isp , known as so - net , released a new fibre service to tokyo for small businesses and homes and also made it available to six surrounding prefectures .
Head Entity: so - net
Tail Entity: sony
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "owned by":
Relation: owned by  
Context: In 2020, the popular video game company, Bungie, announced that it had become an independent studio, having previously been owned by Microsoft for several years.  
Head Entity: Bungie  
Tail Entity: Microsoft  

Relation: owned by  
Context: The luxury fashion brand Gucci, known for its high-end products, is owned by the French conglomerate Kering, which also owns several other prestigious labels.  
Head Entity: Gucci  
Tail Entity: Kering  
#### part of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "part of" means object of which the subject is a part.
Here is an example:
Relation: part of
Context: remains of taxa that can be confidently assigned to istiodactylidae have been found in the uk and china , in rocks dating from the early cretaceous period ( barremian to aptian stage ) .
Head Entity: barremian
Tail Entity: early cretaceous
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "part of":
Relation: part of  
Context: The Great Wall of China is a series of fortifications made of various materials, and it is part of the UNESCO World Heritage Sites.  
Head Entity: Great Wall of China  
Tail Entity: UNESCO World Heritage Sites  

Relation: part of  
Context: The human heart is a vital organ that is part of the circulatory system, responsible for pumping blood throughout the body.  
Head Entity: human heart  
Tail Entity: circulatory system  
#### occupant ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "occupant" means person or organization occupying property.
Here is an example:
Relation: occupant
Context: it served as the home of k league side gyeongnam fc and national league side changwon city fc prior to the construction of the changwon football center in 2009 .
Head Entity: changwon football center
Tail Entity: gyeongnam fc
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "occupant":
Relation: occupant  
Context: The historic building was once the residence of the famous artist Pablo Picasso before it was converted into a museum.  
Head Entity: museum  
Tail Entity: Pablo Picasso  

Relation: occupant  
Context: The office space was leased to a tech startup that specializes in artificial intelligence solutions, making it a hub for innovation.  
Head Entity: office space  
Tail Entity: tech startup  
#### architect ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "architect" means person or architectural firm responsible for designing this building.
Here is an example:
Relation: architect
Context: the art nouveau style was brought to italy by figures such as giuseppe sommaruga and ernesto basile ( who respectively designed the palazzo castiglioni and expanded the palazzo montecitorio in rome ) .
Head Entity: palazzo castiglioni
Tail Entity: giuseppe sommaruga
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "architect":
Relation: architect  
Context: The iconic Sydney Opera House was designed by Danish architect Jørn Utzon, who won the design competition in 1957.  
Head Entity: Sydney Opera House  
Tail Entity: Jørn Utzon  

Relation: architect  
Context: Frank Lloyd Wright is renowned for his innovative designs, including the famous Fallingwater house, which seamlessly integrates with its natural surroundings.  
Head Entity: Fallingwater  
Tail Entity: Frank Lloyd Wright  
#### residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "residence" means the place where the person is or has been, resident.
Here is an example:
Relation: residence
Context: cyrus alexander was born in pennsylvania , and his family soon moved to illinois .
Head Entity: cyrus alexander
Tail Entity: illinois
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "residence":
Relation: residence  
Context: after living in new york for several years, emily decided to relocate to california for a fresh start.  
Head Entity: emily  
Tail Entity: california  

Relation: residence  
Context: during his childhood, michael spent most of his time in texas before moving to florida as a teenager.  
Head Entity: michael  
Tail Entity: florida  
#### composer ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "composer" means person who wrote the music.
Here is an example:
Relation: composer
Context: he also took part in recordings of several gilbert and sullivan operattas as well as edward german 's " merrie england " under the direction of joe batten .
Head Entity: merrie england
Tail Entity: edward german
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "composer":
Relation: composer  
Context: Ludwig van Beethoven is widely regarded as one of the greatest composers in the history of Western music, known for his symphonies and sonatas.  
Head Entity: Ludwig van Beethoven  
Tail Entity: symphonies  

Relation: composer  
Context: The famous opera "Carmen" was composed by Georges Bizet, who is celebrated for his contributions to the world of classical music.  
Head Entity: Carmen  
Tail Entity: Georges Bizet  
#### location ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "location" means location of the object, structure or event.
Here is an example:
Relation: location
Context: the postseason tournament concluded with the san francisco 49ers defeating the cincinnati bengals in super bowl xvi , 26–21 , on january 24 , 1982 , at the pontiac silverdome in pontiac , michigan .
Head Entity: super bowl xvi
Tail Entity: pontiac silverdome
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "location":
Relation: location  
Context: the annual music festival took place in the vibrant city of new orleans, attracting thousands of visitors from around the world.  
Head Entity: music festival  
Tail Entity: new orleans  

Relation: location  
Context: the historic battle was fought at gettysburg, a site that has become a symbol of american history and resilience.  
Head Entity: battle  
Tail Entity: gettysburg  
Losses:  6.931879043579102 0.8274140954017639 0.474314421415329
MemoryTrain:  epoch  0, batch     0 | loss: 6.9318790Losses:  8.24412727355957 1.3704569339752197 0.7012157440185547
MemoryTrain:  epoch  0, batch     1 | loss: 8.2441273Losses:  8.005175590515137 1.6061198711395264 0.5688533782958984
MemoryTrain:  epoch  0, batch     2 | loss: 8.0051756Losses:  7.179734706878662 0.7400082945823669 0.639284610748291
MemoryTrain:  epoch  0, batch     3 | loss: 7.1797347Losses:  7.927323818206787 1.4421039819717407 0.6584740877151489
MemoryTrain:  epoch  1, batch     0 | loss: 7.9273238Losses:  7.329055309295654 0.9361196756362915 0.651412844657898
MemoryTrain:  epoch  1, batch     1 | loss: 7.3290553Losses:  5.878582954406738 1.0983585119247437 0.5877413153648376
MemoryTrain:  epoch  1, batch     2 | loss: 5.8785830Losses:  5.700404644012451 0.6134337782859802 0.6635411977767944
MemoryTrain:  epoch  1, batch     3 | loss: 5.7004046Losses:  6.1814775466918945 1.4409252405166626 0.5300376415252686
MemoryTrain:  epoch  2, batch     0 | loss: 6.1814775Losses:  6.53287935256958 1.5022164583206177 0.6351487040519714
MemoryTrain:  epoch  2, batch     1 | loss: 6.5328794Losses:  5.489607810974121 0.9997716546058655 0.5660531520843506
MemoryTrain:  epoch  2, batch     2 | loss: 5.4896078Losses:  4.6605000495910645 0.560356616973877 0.7045586705207825
MemoryTrain:  epoch  2, batch     3 | loss: 4.6605000Losses:  5.082337379455566 0.4866141080856323 0.6581189632415771
MemoryTrain:  epoch  3, batch     0 | loss: 5.0823374Losses:  4.753842830657959 1.1126585006713867 0.6759733557701111
MemoryTrain:  epoch  3, batch     1 | loss: 4.7538428Losses:  5.903038024902344 1.3049514293670654 0.6040341854095459
MemoryTrain:  epoch  3, batch     2 | loss: 5.9030380Losses:  4.5176005363464355 0.6603474617004395 0.4992806613445282
MemoryTrain:  epoch  3, batch     3 | loss: 4.5176005Losses:  6.2850022315979 1.5587941408157349 0.664422333240509
MemoryTrain:  epoch  4, batch     0 | loss: 6.2850022Losses:  4.414025783538818 0.6799367666244507 0.6517278552055359
MemoryTrain:  epoch  4, batch     1 | loss: 4.4140258Losses:  4.610379219055176 1.0839653015136719 0.28671735525131226
MemoryTrain:  epoch  4, batch     2 | loss: 4.6103792Losses:  3.8372488021850586 0.6218047738075256 0.5884947776794434
MemoryTrain:  epoch  4, batch     3 | loss: 3.8372488Losses:  4.855528831481934 1.1466251611709595 0.5118744373321533
MemoryTrain:  epoch  5, batch     0 | loss: 4.8555288Losses:  3.413031816482544 0.48545289039611816 0.6695213317871094
MemoryTrain:  epoch  5, batch     1 | loss: 3.4130318Losses:  4.05830192565918 0.3312762379646301 0.6144717931747437
MemoryTrain:  epoch  5, batch     2 | loss: 4.0583019Losses:  4.000582695007324 0.3667733669281006 0.6777682304382324
MemoryTrain:  epoch  5, batch     3 | loss: 4.0005827Losses:  4.698416709899902 1.4117152690887451 0.6716973185539246
MemoryTrain:  epoch  6, batch     0 | loss: 4.6984167Losses:  4.784857749938965 1.8358162641525269 0.49097001552581787
MemoryTrain:  epoch  6, batch     1 | loss: 4.7848577Losses:  5.653855323791504 1.998085856437683 0.6744099855422974
MemoryTrain:  epoch  6, batch     2 | loss: 5.6538553Losses:  3.0972468852996826 0.2749853730201721 0.39610299468040466
MemoryTrain:  epoch  6, batch     3 | loss: 3.0972469Losses:  3.12685227394104 0.7778500318527222 0.5094982981681824
MemoryTrain:  epoch  7, batch     0 | loss: 3.1268523Losses:  3.3613195419311523 0.5416394472122192 0.6784356832504272
MemoryTrain:  epoch  7, batch     1 | loss: 3.3613195Losses:  4.62086820602417 0.828112006187439 0.7454046607017517
MemoryTrain:  epoch  7, batch     2 | loss: 4.6208682Losses:  2.8582444190979004 0.4975605010986328 0.6060894131660461
MemoryTrain:  epoch  7, batch     3 | loss: 2.8582444Losses:  3.2216920852661133 0.7130938768386841 0.6461968421936035
MemoryTrain:  epoch  8, batch     0 | loss: 3.2216921Losses:  5.158092021942139 1.9193283319473267 0.6384389996528625
MemoryTrain:  epoch  8, batch     1 | loss: 5.1580920Losses:  3.014023542404175 0.9299273490905762 0.5660306811332703
MemoryTrain:  epoch  8, batch     2 | loss: 3.0140235Losses:  2.8895063400268555 0.2630018889904022 0.6248771548271179
MemoryTrain:  epoch  8, batch     3 | loss: 2.8895063Losses:  2.4383749961853027 0.45510852336883545 0.5291476845741272
MemoryTrain:  epoch  9, batch     0 | loss: 2.4383750Losses:  4.228434085845947 1.2025797367095947 0.6029869318008423
MemoryTrain:  epoch  9, batch     1 | loss: 4.2284341Losses:  3.7911922931671143 0.7058125734329224 0.683498203754425
MemoryTrain:  epoch  9, batch     2 | loss: 3.7911923Losses:  2.1333718299865723 -0.0 0.5783869028091431
MemoryTrain:  epoch  9, batch     3 | loss: 2.1333718
[EVAL] batch:    0 | acc: 18.75%,  total acc: 18.75%   [EVAL] batch:    1 | acc: 18.75%,  total acc: 18.75%   [EVAL] batch:    2 | acc: 18.75%,  total acc: 18.75%   [EVAL] batch:    3 | acc: 25.00%,  total acc: 20.31%   [EVAL] batch:    4 | acc: 31.25%,  total acc: 22.50%   [EVAL] batch:    5 | acc: 6.25%,  total acc: 19.79%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 27.68%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 35.16%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 42.36%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 48.12%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 52.27%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 55.21%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 58.17%   [EVAL] batch:   13 | acc: 81.25%,  total acc: 59.82%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 62.08%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 64.45%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 66.54%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 68.06%   [EVAL] batch:   18 | acc: 75.00%,  total acc: 68.42%   [EVAL] batch:   19 | acc: 56.25%,  total acc: 67.81%   [EVAL] batch:   20 | acc: 37.50%,  total acc: 66.37%   [EVAL] batch:   21 | acc: 68.75%,  total acc: 66.48%   [EVAL] batch:   22 | acc: 62.50%,  total acc: 66.30%   [EVAL] batch:   23 | acc: 37.50%,  total acc: 65.10%   [EVAL] batch:   24 | acc: 62.50%,  total acc: 65.00%   [EVAL] batch:   25 | acc: 6.25%,  total acc: 62.74%   [EVAL] batch:   26 | acc: 0.00%,  total acc: 60.42%   [EVAL] batch:   27 | acc: 6.25%,  total acc: 58.48%   [EVAL] batch:   28 | acc: 0.00%,  total acc: 56.47%   [EVAL] batch:   29 | acc: 12.50%,  total acc: 55.00%   [EVAL] batch:   30 | acc: 0.00%,  total acc: 53.23%   [EVAL] batch:   31 | acc: 62.50%,  total acc: 53.52%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 54.73%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 56.07%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 57.32%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 58.33%   [EVAL] batch:   36 | acc: 81.25%,  total acc: 58.95%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 59.70%   [EVAL] batch:   38 | acc: 81.25%,  total acc: 60.26%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 61.25%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 62.04%   [EVAL] batch:   41 | acc: 81.25%,  total acc: 62.50%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 63.23%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 64.06%   [EVAL] batch:   44 | acc: 62.50%,  total acc: 64.03%   [EVAL] batch:   45 | acc: 43.75%,  total acc: 63.59%   [EVAL] batch:   46 | acc: 43.75%,  total acc: 63.16%   [EVAL] batch:   47 | acc: 50.00%,  total acc: 62.89%   [EVAL] batch:   48 | acc: 50.00%,  total acc: 62.63%   [EVAL] batch:   49 | acc: 62.50%,  total acc: 62.62%   [EVAL] batch:   50 | acc: 43.75%,  total acc: 62.25%   [EVAL] batch:   51 | acc: 75.00%,  total acc: 62.50%   [EVAL] batch:   52 | acc: 56.25%,  total acc: 62.38%   [EVAL] batch:   53 | acc: 75.00%,  total acc: 62.62%   [EVAL] batch:   54 | acc: 68.75%,  total acc: 62.73%   [EVAL] batch:   55 | acc: 81.25%,  total acc: 63.06%   [EVAL] batch:   56 | acc: 43.75%,  total acc: 62.72%   [EVAL] batch:   57 | acc: 43.75%,  total acc: 62.39%   [EVAL] batch:   58 | acc: 37.50%,  total acc: 61.97%   [EVAL] batch:   59 | acc: 68.75%,  total acc: 62.08%   [EVAL] batch:   60 | acc: 62.50%,  total acc: 62.09%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 62.20%   [EVAL] batch:   62 | acc: 18.75%,  total acc: 61.51%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 82.81%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 82.50%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 82.14%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 82.03%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 81.94%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 82.50%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 82.95%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 83.65%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 85.00%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 85.94%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 86.76%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 87.15%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 87.83%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 88.44%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 88.99%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 89.20%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 89.40%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 89.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 89.90%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 90.28%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 90.73%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 91.04%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 91.33%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 91.60%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 91.86%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 92.10%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 92.14%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 92.36%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 92.57%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 92.76%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 92.95%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 93.12%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 93.29%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 93.45%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 93.60%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 93.89%   [EVAL] batch:   46 | acc: 87.50%,  total acc: 93.75%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   48 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 93.88%   [EVAL] batch:   50 | acc: 87.50%,  total acc: 93.75%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 93.63%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 93.63%   [EVAL] batch:   53 | acc: 93.75%,  total acc: 93.63%   [EVAL] batch:   54 | acc: 75.00%,  total acc: 93.30%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 93.19%   [EVAL] batch:   56 | acc: 87.50%,  total acc: 93.09%   [EVAL] batch:   57 | acc: 81.25%,  total acc: 92.89%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 93.01%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 93.12%   [EVAL] batch:   60 | acc: 93.75%,  total acc: 93.14%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 93.15%   [EVAL] batch:   62 | acc: 62.50%,  total acc: 92.66%   [EVAL] batch:   63 | acc: 18.75%,  total acc: 91.50%   [EVAL] batch:   64 | acc: 12.50%,  total acc: 90.29%   [EVAL] batch:   65 | acc: 12.50%,  total acc: 89.11%   [EVAL] batch:   66 | acc: 37.50%,  total acc: 88.34%   [EVAL] batch:   67 | acc: 18.75%,  total acc: 87.32%   [EVAL] batch:   68 | acc: 37.50%,  total acc: 86.59%   [EVAL] batch:   69 | acc: 87.50%,  total acc: 86.61%   [EVAL] batch:   70 | acc: 93.75%,  total acc: 86.71%   [EVAL] batch:   71 | acc: 100.00%,  total acc: 86.89%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 87.07%   [EVAL] batch:   73 | acc: 87.50%,  total acc: 87.08%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 87.08%   [EVAL] batch:   75 | acc: 100.00%,  total acc: 87.25%   [EVAL] batch:   76 | acc: 81.25%,  total acc: 87.18%   [EVAL] batch:   77 | acc: 93.75%,  total acc: 87.26%   [EVAL] batch:   78 | acc: 100.00%,  total acc: 87.42%   [EVAL] batch:   79 | acc: 100.00%,  total acc: 87.58%   [EVAL] batch:   80 | acc: 93.75%,  total acc: 87.65%   [EVAL] batch:   81 | acc: 43.75%,  total acc: 87.12%   [EVAL] batch:   82 | acc: 56.25%,  total acc: 86.75%   [EVAL] batch:   83 | acc: 43.75%,  total acc: 86.24%   [EVAL] batch:   84 | acc: 81.25%,  total acc: 86.18%   [EVAL] batch:   85 | acc: 50.00%,  total acc: 85.76%   [EVAL] batch:   86 | acc: 37.50%,  total acc: 85.20%   [EVAL] batch:   87 | acc: 43.75%,  total acc: 84.73%   [EVAL] batch:   88 | acc: 0.00%,  total acc: 83.78%   [EVAL] batch:   89 | acc: 0.00%,  total acc: 82.85%   [EVAL] batch:   90 | acc: 6.25%,  total acc: 82.01%   [EVAL] batch:   91 | acc: 6.25%,  total acc: 81.18%   [EVAL] batch:   92 | acc: 6.25%,  total acc: 80.38%   [EVAL] batch:   93 | acc: 18.75%,  total acc: 79.72%   [EVAL] batch:   94 | acc: 87.50%,  total acc: 79.80%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 80.01%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 80.22%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 80.42%   [EVAL] batch:   98 | acc: 87.50%,  total acc: 80.49%   [EVAL] batch:   99 | acc: 87.50%,  total acc: 80.56%   [EVAL] batch:  100 | acc: 75.00%,  total acc: 80.51%   [EVAL] batch:  101 | acc: 93.75%,  total acc: 80.64%   [EVAL] batch:  102 | acc: 93.75%,  total acc: 80.76%   [EVAL] batch:  103 | acc: 93.75%,  total acc: 80.89%   [EVAL] batch:  104 | acc: 87.50%,  total acc: 80.95%   [EVAL] batch:  105 | acc: 93.75%,  total acc: 81.07%   [EVAL] batch:  106 | acc: 68.75%,  total acc: 80.96%   [EVAL] batch:  107 | acc: 68.75%,  total acc: 80.84%   [EVAL] batch:  108 | acc: 37.50%,  total acc: 80.45%   [EVAL] batch:  109 | acc: 56.25%,  total acc: 80.23%   [EVAL] batch:  110 | acc: 37.50%,  total acc: 79.84%   [EVAL] batch:  111 | acc: 68.75%,  total acc: 79.74%   [EVAL] batch:  112 | acc: 43.75%,  total acc: 79.42%   [EVAL] batch:  113 | acc: 62.50%,  total acc: 79.28%   [EVAL] batch:  114 | acc: 56.25%,  total acc: 79.08%   [EVAL] batch:  115 | acc: 81.25%,  total acc: 79.09%   [EVAL] batch:  116 | acc: 62.50%,  total acc: 78.95%   [EVAL] batch:  117 | acc: 81.25%,  total acc: 78.97%   [EVAL] batch:  118 | acc: 62.50%,  total acc: 78.83%   [EVAL] batch:  119 | acc: 37.50%,  total acc: 78.49%   [EVAL] batch:  120 | acc: 43.75%,  total acc: 78.20%   [EVAL] batch:  121 | acc: 50.00%,  total acc: 77.97%   [EVAL] batch:  122 | acc: 68.75%,  total acc: 77.90%   [EVAL] batch:  123 | acc: 68.75%,  total acc: 77.82%   [EVAL] batch:  124 | acc: 50.00%,  total acc: 77.60%   
cur_acc:  ['0.9494', '0.6151']
his_acc:  ['0.9494', '0.7760']
Clustering into  14  clusters
Clusters:  [ 4  5  9  0  2 11  9  8  5  7  2  0  8  3  1 13  8  5  8  5 10 12 12  2
  3  7  4 11  6  1]
Losses:  9.125252723693848 2.6582207679748535 0.8478265404701233
CurrentTrain: epoch  0, batch     0 | loss: 9.1252527Losses:  8.22708797454834 2.284776210784912 0.8755674362182617
CurrentTrain: epoch  0, batch     1 | loss: 8.2270880Losses:  9.500621795654297 2.7610738277435303 0.8051683306694031
CurrentTrain: epoch  0, batch     2 | loss: 9.5006218Losses:  8.637418746948242 -0.0 0.15086057782173157
CurrentTrain: epoch  0, batch     3 | loss: 8.6374187Losses:  10.602838516235352 4.084519386291504 0.6859992146492004
CurrentTrain: epoch  1, batch     0 | loss: 10.6028385Losses:  7.3861894607543945 2.407163143157959 0.7972872257232666
CurrentTrain: epoch  1, batch     1 | loss: 7.3861895Losses:  7.693511962890625 3.072033405303955 0.7148339748382568
CurrentTrain: epoch  1, batch     2 | loss: 7.6935120Losses:  6.184577941894531 -0.0 0.08953586220741272
CurrentTrain: epoch  1, batch     3 | loss: 6.1845779Losses:  7.351906776428223 2.7644195556640625 0.6816623210906982
CurrentTrain: epoch  2, batch     0 | loss: 7.3519068Losses:  6.952096462249756 2.2454521656036377 0.7716754078865051
CurrentTrain: epoch  2, batch     1 | loss: 6.9520965Losses:  7.624355792999268 3.0878021717071533 0.621802806854248
CurrentTrain: epoch  2, batch     2 | loss: 7.6243558Losses:  2.573256731033325 -0.0 0.09365317225456238
CurrentTrain: epoch  2, batch     3 | loss: 2.5732567Losses:  8.015172004699707 4.048296928405762 0.5147695541381836
CurrentTrain: epoch  3, batch     0 | loss: 8.0151720Losses:  7.1903486251831055 2.8491415977478027 0.7467892169952393
CurrentTrain: epoch  3, batch     1 | loss: 7.1903486Losses:  6.180974960327148 2.4539694786071777 0.66762375831604
CurrentTrain: epoch  3, batch     2 | loss: 6.1809750Losses:  2.3948311805725098 -0.0 0.08748527616262436
CurrentTrain: epoch  3, batch     3 | loss: 2.3948312Losses:  5.421627044677734 2.065932273864746 0.6473516225814819
CurrentTrain: epoch  4, batch     0 | loss: 5.4216270Losses:  6.338788032531738 2.515725612640381 0.6995095610618591
CurrentTrain: epoch  4, batch     1 | loss: 6.3387880Losses:  6.5076470375061035 2.9777493476867676 0.5924491286277771
CurrentTrain: epoch  4, batch     2 | loss: 6.5076470Losses:  3.017493724822998 -0.0 0.10509279370307922
CurrentTrain: epoch  4, batch     3 | loss: 3.0174937Losses:  5.457550048828125 2.555504322052002 0.7215203642845154
CurrentTrain: epoch  5, batch     0 | loss: 5.4575500Losses:  6.1245856285095215 2.378077983856201 0.6791901588439941
CurrentTrain: epoch  5, batch     1 | loss: 6.1245856Losses:  5.433567047119141 2.3481392860412598 0.6416566371917725
CurrentTrain: epoch  5, batch     2 | loss: 5.4335670Losses:  2.891024112701416 -0.0 0.11827511340379715
CurrentTrain: epoch  5, batch     3 | loss: 2.8910241Losses:  4.9716997146606445 1.601142168045044 0.7196663022041321
CurrentTrain: epoch  6, batch     0 | loss: 4.9716997Losses:  5.461223602294922 2.450624465942383 0.6589441299438477
CurrentTrain: epoch  6, batch     1 | loss: 5.4612236Losses:  5.728861331939697 2.6178300380706787 0.6616117358207703
CurrentTrain: epoch  6, batch     2 | loss: 5.7288613Losses:  2.1606502532958984 -0.0 0.08309140801429749
CurrentTrain: epoch  6, batch     3 | loss: 2.1606503Losses:  5.175256252288818 2.0070369243621826 0.648448646068573
CurrentTrain: epoch  7, batch     0 | loss: 5.1752563Losses:  5.5128984451293945 3.002918243408203 0.6164748072624207
CurrentTrain: epoch  7, batch     1 | loss: 5.5128984Losses:  5.5370774269104 2.536078691482544 0.6330679655075073
CurrentTrain: epoch  7, batch     2 | loss: 5.5370774Losses:  1.9899487495422363 -0.0 0.11966117471456528
CurrentTrain: epoch  7, batch     3 | loss: 1.9899487Losses:  4.968871116638184 2.3772308826446533 0.6976481676101685
CurrentTrain: epoch  8, batch     0 | loss: 4.9688711Losses:  5.640817165374756 3.0873489379882812 0.5979899168014526
CurrentTrain: epoch  8, batch     1 | loss: 5.6408172Losses:  8.437466621398926 5.109379291534424 0.5658454895019531
CurrentTrain: epoch  8, batch     2 | loss: 8.4374666Losses:  1.8411548137664795 -0.0 0.08928360044956207
CurrentTrain: epoch  8, batch     3 | loss: 1.8411548Losses:  4.755660533905029 2.0733582973480225 0.7006165385246277
CurrentTrain: epoch  9, batch     0 | loss: 4.7556605Losses:  4.4113545417785645 1.6261169910430908 0.7043253779411316
CurrentTrain: epoch  9, batch     1 | loss: 4.4113545Losses:  5.234949111938477 2.5631356239318848 0.6418629288673401
CurrentTrain: epoch  9, batch     2 | loss: 5.2349491Losses:  2.59297776222229 -0.0 0.09670105576515198
CurrentTrain: epoch  9, batch     3 | loss: 2.5929778
#### spouse ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "spouse" means the subject has the object as their spouse (husband, wife, partner, etc.).
Here is an example:
Relation: spouse
Context: rohan is the son of narayana murthy , founder of infosys , and his wife sudha murthy , an engineer and author of note .
Head Entity: narayana murthy
Tail Entity: sudha murthy
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "spouse":
Relation: spouse  
Context: elena is a renowned scientist who often collaborates with her husband, dr. mark thompson, on various research projects.  
Head Entity: dr. mark thompson  
Tail Entity: elena  

Relation: spouse  
Context: during the award ceremony, it was revealed that the famous actor, john doe, had been married to his long-time partner, jane smith, for over a decade.  
Head Entity: john doe  
Tail Entity: jane smith  
#### record label ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "record label" means brand and trademark associated with the marketing of subject music recordings and music videos.
Here is an example:
Relation: record label
Context: they briefly reformed in 1992 , when sub pop re - released " primal rock therapy " on cd with five additional unreleased tracks .
Head Entity: primal rock therapy
Tail Entity: sub pop
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "record label":
Relation: record label  
Context: The album "Future Nostalgia" was released under the Warner Records label, showcasing Dua Lipa's unique style and sound.  
Head Entity: Future Nostalgia  
Tail Entity: Warner Records  

Relation: record label  
Context: After signing with Atlantic Records, the band released their highly anticipated debut album, which quickly climbed the charts.  
Head Entity: debut album  
Tail Entity: Atlantic Records  
#### distributor ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "distributor" means distributor of a creative work; distributor for a record label; news agency; film distributor.
Here is an example:
Relation: distributor
Context: distributed by paramount pictures , " the italian job " was theatrically released in the united states on may 30 , 2003 , and grossed over $ 176 million worldwide .
Head Entity: the italian job
Tail Entity: paramount pictures
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "distributor":
Relation: distributor  
Context: the film was distributed by universal pictures and received critical acclaim upon its release in 2015.  
Head Entity: the film  
Tail Entity: universal pictures  

Relation: distributor  
Context: the album was distributed by sony music and quickly climbed the charts, becoming a bestseller.  
Head Entity: the album  
Tail Entity: sony music  
#### located in or next to body of water ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "located in or next to body of water" means body of water on or next to which a place is located.
Here is an example:
Relation: located in or next to body of water
Context: it then crossed the indian ocean , passing st. pierre island , providence atoll and farquhar atoll of seychelles before making landfall in southeastern australia .
Head Entity: farquhar atoll
Tail Entity: indian ocean
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "located in or next to body of water":
Relation: located in or next to body of water  
Context: The city of Venice is famous for its canals, which are situated right next to the Adriatic Sea, providing a unique maritime experience.  
Head Entity: Venice  
Tail Entity: Adriatic Sea  

Relation: located in or next to body of water  
Context: The village of Hvar is nestled on the shores of the Aegean Sea, attracting tourists with its stunning views and vibrant culture.  
Head Entity: Hvar  
Tail Entity: Aegean Sea  
#### subsidiary ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "subsidiary" means subsidiary of a company or organization; generally a fully owned separate corporation.
Here is an example:
Relation: subsidiary
Context: following the sale of amstrad plc to rupert murdoch 's bskyb , viglen is now lord sugar 's sole it establishment .
Head Entity: bskyb
Tail Entity: amstrad
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "subsidiary":
Relation: subsidiary  
Context: In 2018, Facebook acquired WhatsApp, making it a subsidiary of the social media giant.  
Head Entity: Facebook  
Tail Entity: WhatsApp  

Relation: subsidiary  
Context: After the merger, the new company announced that its subsidiary, XYZ Corp, would continue to operate independently.  
Head Entity: new company  
Tail Entity: XYZ Corp  
#### country of citizenship ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "country of citizenship" means the object is a country that recognizes the subject as its citizen.
Here is an example:
Relation: country of citizenship
Context: he challenged seven other swimmers in heat four including ukraine 's rostyslav svanidze , a top 16 finalist in atlanta four years earlier .
Head Entity: rostyslav svanidze
Tail Entity: ukraine
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "country of citizenship":
Relation: country of citizenship  
Context: after moving to the united states, she became a naturalized citizen and proudly represents her country of citizenship, which is canada, in international competitions.  
Head Entity: she  
Tail Entity: canada  

Relation: country of citizenship  
Context: during the conference, the speaker highlighted the contributions of various scientists, including dr. emilio garcia, who is originally from spain and holds dual citizenship with his country of citizenship being spain.  
Head Entity: dr. emilio garcia  
Tail Entity: spain  
#### genre ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "genre" means creative work's genre or an artist's field of work.
Here is an example:
Relation: genre
Context: nina cites " duffy , muse , and other pop and alternative artists " as her major influences .
Head Entity: duffy
Tail Entity: pop
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "genre":
Relation: genre  
Context: the film "inception" is often categorized as a science fiction thriller that explores complex themes of dreams and reality.  
Head Entity: inception  
Tail Entity: science fiction  

Relation: genre  
Context: the band "coldplay" is known for their melodic rock sound, blending elements of alternative and pop music in their songs.  
Head Entity: coldplay  
Tail Entity: melodic rock  
#### mouth of the watercourse ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "mouth of the watercourse" means the body of water to which the watercourse drains.
Here is an example:
Relation: mouth of the watercourse
Context: in the west , the rivers rib , ash and stort flow south from the hundred parishes to meet the lea and then the thames .
Head Entity: rib
Tail Entity: lea
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "mouth of the watercourse":
Relation: mouth of the watercourse  
Context: The river Seine flows through Paris and eventually empties into the English Channel, marking its mouth.  
Head Entity: Seine  
Tail Entity: English Channel  

Relation: mouth of the watercourse  
Context: The Mississippi River travels a long distance before reaching its mouth at the Gulf of Mexico, where it meets the sea.  
Head Entity: Mississippi River  
Tail Entity: Gulf of Mexico  
#### position played on team / speciality ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "position played on team / speciality" means position or specialism of a player on a team.
Here is an example:
Relation: position played on team / speciality
Context: benjamin van leer ( born 9 april 1992 in houten ) is a dutch professional footballer who currently plays as a goalkeeper for ajax in the dutch eredivisie .
Head Entity: benjamin van leer
Tail Entity: goalkeeper
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "position played on team / speciality":
Relation: position played on team / speciality  
Context: lebron james, born on december 30, 1984, is an american professional basketball player who currently plays as a forward for the los angeles lakers in the nba.  
Head Entity: lebron james  
Tail Entity: forward  

Relation: position played on team / speciality  
Context: serena williams, born on september 26, 1981, is an american professional tennis player who is known for her powerful serve and currently competes in singles and doubles events.  
Head Entity: serena williams  
Tail Entity: tennis player  
#### sports season of league or competition ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "sports season of league or competition" means property that shows the competition of which the item is a season.
Here is an example:
Relation: sports season of league or competition
Context: the stade louis ii in monaco was the venue for the uefa super cup every year since 1998 .
Head Entity: 1998
Tail Entity: uefa super cup
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "sports season of league or competition":
Relation: sports season of league or competition  
Context: The 2020 NBA season was significantly impacted by the COVID-19 pandemic, leading to a unique playoff format.  
Head Entity: 2020  
Tail Entity: NBA  

Relation: sports season of league or competition  
Context: The 2019 Rugby World Cup took place in Japan, showcasing teams from around the globe competing for the title.  
Head Entity: 2019  
Tail Entity: Rugby World Cup  
Losses:  6.986702919006348 0.7570042610168457 0.9224491119384766
MemoryTrain:  epoch  0, batch     0 | loss: 6.9867029Losses:  5.9543657302856445 0.5603147745132446 0.9536637663841248
MemoryTrain:  epoch  0, batch     1 | loss: 5.9543657Losses:  5.751098155975342 0.2627739906311035 0.7121941447257996
MemoryTrain:  epoch  0, batch     2 | loss: 5.7510982Losses:  6.733922958374023 1.2112698554992676 0.855004072189331
MemoryTrain:  epoch  0, batch     3 | loss: 6.7339230Losses:  6.538841724395752 0.5218812227249146 0.8325096368789673
MemoryTrain:  epoch  0, batch     4 | loss: 6.5388417Losses:  5.03484582901001 -0.0 0.6060957312583923
MemoryTrain:  epoch  0, batch     5 | loss: 5.0348458Losses:  6.4566545486450195 0.8814444541931152 0.7574441432952881
MemoryTrain:  epoch  1, batch     0 | loss: 6.4566545Losses:  6.189457893371582 1.2306010723114014 0.736233651638031
MemoryTrain:  epoch  1, batch     1 | loss: 6.1894579Losses:  7.269256591796875 1.594765067100525 0.8450175523757935
MemoryTrain:  epoch  1, batch     2 | loss: 7.2692566Losses:  7.307004451751709 1.2157995700836182 0.7524458169937134
MemoryTrain:  epoch  1, batch     3 | loss: 7.3070045Losses:  5.059693813323975 0.5760617256164551 0.7035334706306458
MemoryTrain:  epoch  1, batch     4 | loss: 5.0596938Losses:  4.699211597442627 0.38742199540138245 0.7133313417434692
MemoryTrain:  epoch  1, batch     5 | loss: 4.6992116Losses:  4.98663854598999 0.5178031325340271 0.7792900204658508
MemoryTrain:  epoch  2, batch     0 | loss: 4.9866385Losses:  5.190283298492432 0.47274237871170044 0.8610337376594543
MemoryTrain:  epoch  2, batch     1 | loss: 5.1902833Losses:  5.52653694152832 0.8029022216796875 0.5859963893890381
MemoryTrain:  epoch  2, batch     2 | loss: 5.5265369Losses:  5.691978931427002 0.9917450547218323 0.7899090647697449
MemoryTrain:  epoch  2, batch     3 | loss: 5.6919789Losses:  4.663744926452637 1.3461589813232422 0.7798445224761963
MemoryTrain:  epoch  2, batch     4 | loss: 4.6637449Losses:  6.263130187988281 0.6329169869422913 0.48503005504608154
MemoryTrain:  epoch  2, batch     5 | loss: 6.2631302Losses:  6.268648624420166 1.0657315254211426 0.746049702167511
MemoryTrain:  epoch  3, batch     0 | loss: 6.2686486Losses:  4.790399551391602 0.7630548477172852 0.7476234436035156
MemoryTrain:  epoch  3, batch     1 | loss: 4.7903996Losses:  4.684298515319824 0.7320060729980469 0.8556305766105652
MemoryTrain:  epoch  3, batch     2 | loss: 4.6842985Losses:  5.356050968170166 0.2560115158557892 0.8015812039375305
MemoryTrain:  epoch  3, batch     3 | loss: 5.3560510Losses:  3.7153491973876953 0.7493953108787537 0.6699599027633667
MemoryTrain:  epoch  3, batch     4 | loss: 3.7153492Losses:  3.9999756813049316 0.2711673676967621 0.4691751003265381
MemoryTrain:  epoch  3, batch     5 | loss: 3.9999757Losses:  5.710791110992432 1.1262967586517334 0.7944206595420837
MemoryTrain:  epoch  4, batch     0 | loss: 5.7107911Losses:  4.907087802886963 0.8077841997146606 0.7715644836425781
MemoryTrain:  epoch  4, batch     1 | loss: 4.9070878Losses:  4.182312965393066 1.11444890499115 0.7011551260948181
MemoryTrain:  epoch  4, batch     2 | loss: 4.1823130Losses:  4.229732036590576 0.29425477981567383 0.8522508144378662
MemoryTrain:  epoch  4, batch     3 | loss: 4.2297320Losses:  4.278433322906494 0.6504803895950317 0.864995539188385
MemoryTrain:  epoch  4, batch     4 | loss: 4.2784333Losses:  4.477409362792969 0.32722678780555725 0.5229129791259766
MemoryTrain:  epoch  4, batch     5 | loss: 4.4774094Losses:  3.976996898651123 1.0130743980407715 0.8231087923049927
MemoryTrain:  epoch  5, batch     0 | loss: 3.9769969Losses:  4.895730495452881 0.48431962728500366 0.8254769444465637
MemoryTrain:  epoch  5, batch     1 | loss: 4.8957305Losses:  3.507315158843994 0.7583996057510376 0.8280634880065918
MemoryTrain:  epoch  5, batch     2 | loss: 3.5073152Losses:  4.950174808502197 1.489823341369629 0.8166418075561523
MemoryTrain:  epoch  5, batch     3 | loss: 4.9501748Losses:  4.644234657287598 0.278406023979187 0.7122411727905273
MemoryTrain:  epoch  5, batch     4 | loss: 4.6442347Losses:  4.505478382110596 0.2401895970106125 0.6897510886192322
MemoryTrain:  epoch  5, batch     5 | loss: 4.5054784Losses:  4.355875492095947 0.8561473488807678 0.9030399322509766
MemoryTrain:  epoch  6, batch     0 | loss: 4.3558755Losses:  4.330881595611572 0.9977737665176392 0.7673323750495911
MemoryTrain:  epoch  6, batch     1 | loss: 4.3308816Losses:  3.8387417793273926 0.25568461418151855 0.741951048374176
MemoryTrain:  epoch  6, batch     2 | loss: 3.8387418Losses:  3.6965737342834473 0.5666408538818359 0.833229660987854
MemoryTrain:  epoch  6, batch     3 | loss: 3.6965737Losses:  4.131142616271973 0.26842200756073 0.7861430644989014
MemoryTrain:  epoch  6, batch     4 | loss: 4.1311426Losses:  3.8868398666381836 0.22904253005981445 0.6425365209579468
MemoryTrain:  epoch  6, batch     5 | loss: 3.8868399Losses:  4.383613586425781 0.24690967798233032 0.9106351733207703
MemoryTrain:  epoch  7, batch     0 | loss: 4.3836136Losses:  3.051488161087036 0.25106653571128845 0.6712074875831604
MemoryTrain:  epoch  7, batch     1 | loss: 3.0514882Losses:  4.619823455810547 0.8359030485153198 0.8261911273002625
MemoryTrain:  epoch  7, batch     2 | loss: 4.6198235Losses:  3.261258125305176 0.35154399275779724 0.8113189935684204
MemoryTrain:  epoch  7, batch     3 | loss: 3.2612581Losses:  3.471771478652954 0.4791910946369171 0.7304063439369202
MemoryTrain:  epoch  7, batch     4 | loss: 3.4717715Losses:  2.303187847137451 0.22105370461940765 0.5587042570114136
MemoryTrain:  epoch  7, batch     5 | loss: 2.3031878Losses:  3.7615342140197754 0.8165972232818604 0.8128564953804016
MemoryTrain:  epoch  8, batch     0 | loss: 3.7615342Losses:  3.8397932052612305 0.4766938090324402 0.7859876155853271
MemoryTrain:  epoch  8, batch     1 | loss: 3.8397932Losses:  4.071505546569824 0.8004088997840881 0.6121076345443726
MemoryTrain:  epoch  8, batch     2 | loss: 4.0715055Losses:  3.7290868759155273 0.8031165599822998 0.8501279950141907
MemoryTrain:  epoch  8, batch     3 | loss: 3.7290869Losses:  3.409508228302002 1.3090572357177734 0.5881761908531189
MemoryTrain:  epoch  8, batch     4 | loss: 3.4095082Losses:  3.7252092361450195 0.5604909062385559 0.5485343337059021
MemoryTrain:  epoch  8, batch     5 | loss: 3.7252092Losses:  3.1400954723358154 0.5353813171386719 0.7193678021430969
MemoryTrain:  epoch  9, batch     0 | loss: 3.1400955Losses:  3.4272255897521973 0.4784279465675354 0.6983636617660522
MemoryTrain:  epoch  9, batch     1 | loss: 3.4272256Losses:  2.693241834640503 0.2464088499546051 0.7854916453361511
MemoryTrain:  epoch  9, batch     2 | loss: 2.6932418Losses:  3.6009891033172607 0.5576823949813843 0.8313234448432922
MemoryTrain:  epoch  9, batch     3 | loss: 3.6009891Losses:  4.010462284088135 1.2145655155181885 0.8302200436592102
MemoryTrain:  epoch  9, batch     4 | loss: 4.0104623Losses:  2.9438788890838623 0.3484029769897461 0.6209080815315247
MemoryTrain:  epoch  9, batch     5 | loss: 2.9438789
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 89.58%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 91.07%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 90.62%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 90.28%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 87.50%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 86.93%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 85.94%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 85.58%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 86.16%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 85.42%   [EVAL] batch:   15 | acc: 93.75%,  total acc: 85.94%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 85.29%   [EVAL] batch:   17 | acc: 87.50%,  total acc: 85.42%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 85.86%   [EVAL] batch:   19 | acc: 68.75%,  total acc: 85.00%   [EVAL] batch:   20 | acc: 68.75%,  total acc: 84.23%   [EVAL] batch:   21 | acc: 68.75%,  total acc: 83.52%   [EVAL] batch:   22 | acc: 75.00%,  total acc: 83.15%   [EVAL] batch:   23 | acc: 75.00%,  total acc: 82.81%   [EVAL] batch:   24 | acc: 81.25%,  total acc: 82.75%   [EVAL] batch:   25 | acc: 18.75%,  total acc: 80.29%   [EVAL] batch:   26 | acc: 62.50%,  total acc: 79.63%   [EVAL] batch:   27 | acc: 31.25%,  total acc: 77.90%   [EVAL] batch:   28 | acc: 31.25%,  total acc: 76.29%   [EVAL] batch:   29 | acc: 25.00%,  total acc: 74.58%   [EVAL] batch:   30 | acc: 25.00%,  total acc: 72.98%   [EVAL] batch:   31 | acc: 56.25%,  total acc: 72.46%   [EVAL] batch:   32 | acc: 56.25%,  total acc: 71.97%   [EVAL] batch:   33 | acc: 56.25%,  total acc: 71.51%   [EVAL] batch:   34 | acc: 50.00%,  total acc: 70.89%   [EVAL] batch:   35 | acc: 62.50%,  total acc: 70.66%   [EVAL] batch:   36 | acc: 62.50%,  total acc: 70.44%   [EVAL] batch:   37 | acc: 56.25%,  total acc: 70.07%   [EVAL] batch:   38 | acc: 68.75%,  total acc: 70.03%   [EVAL] batch:   39 | acc: 75.00%,  total acc: 70.16%   [EVAL] batch:   40 | acc: 75.00%,  total acc: 70.27%   [EVAL] batch:   41 | acc: 68.75%,  total acc: 70.24%   [EVAL] batch:   42 | acc: 87.50%,  total acc: 70.64%   [EVAL] batch:   43 | acc: 56.25%,  total acc: 70.31%   [EVAL] batch:   44 | acc: 37.50%,  total acc: 69.58%   [EVAL] batch:   45 | acc: 31.25%,  total acc: 68.75%   [EVAL] batch:   46 | acc: 18.75%,  total acc: 67.69%   [EVAL] batch:   47 | acc: 37.50%,  total acc: 67.06%   [EVAL] batch:   48 | acc: 18.75%,  total acc: 66.07%   [EVAL] batch:   49 | acc: 31.25%,  total acc: 65.38%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 66.05%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 66.71%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 67.33%   [EVAL] batch:   53 | acc: 93.75%,  total acc: 67.82%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 68.30%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 68.86%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 69.41%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 69.83%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 70.34%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 70.83%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 71.31%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 71.77%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 71.43%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 84.38%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 85.00%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 86.46%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 85.71%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 86.72%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 86.81%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 86.88%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 86.93%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 87.95%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 88.33%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 89.06%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 89.71%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 89.93%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 90.46%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 90.94%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 90.77%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 90.34%   [EVAL] batch:   22 | acc: 81.25%,  total acc: 89.95%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 90.10%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 90.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 90.38%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 90.51%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 90.85%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 90.95%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 91.25%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 91.53%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 91.80%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 92.05%   [EVAL] batch:   33 | acc: 75.00%,  total acc: 91.54%   [EVAL] batch:   34 | acc: 87.50%,  total acc: 91.43%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 91.49%   [EVAL] batch:   36 | acc: 87.50%,  total acc: 91.39%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 91.28%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 91.51%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 91.72%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 91.92%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 92.11%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 92.15%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 92.33%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 92.36%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 92.53%   [EVAL] batch:   46 | acc: 87.50%,  total acc: 92.42%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 92.45%   [EVAL] batch:   48 | acc: 93.75%,  total acc: 92.47%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 92.50%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 92.52%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 92.43%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 92.45%   [EVAL] batch:   53 | acc: 93.75%,  total acc: 92.48%   [EVAL] batch:   54 | acc: 75.00%,  total acc: 92.16%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 92.19%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 92.21%   [EVAL] batch:   57 | acc: 81.25%,  total acc: 92.03%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 92.16%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 92.29%   [EVAL] batch:   60 | acc: 93.75%,  total acc: 92.32%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 92.24%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 91.57%   [EVAL] batch:   63 | acc: 0.00%,  total acc: 90.14%   [EVAL] batch:   64 | acc: 6.25%,  total acc: 88.85%   [EVAL] batch:   65 | acc: 12.50%,  total acc: 87.69%   [EVAL] batch:   66 | acc: 12.50%,  total acc: 86.57%   [EVAL] batch:   67 | acc: 0.00%,  total acc: 85.29%   [EVAL] batch:   68 | acc: 43.75%,  total acc: 84.69%   [EVAL] batch:   69 | acc: 81.25%,  total acc: 84.64%   [EVAL] batch:   70 | acc: 87.50%,  total acc: 84.68%   [EVAL] batch:   71 | acc: 93.75%,  total acc: 84.81%   [EVAL] batch:   72 | acc: 87.50%,  total acc: 84.85%   [EVAL] batch:   73 | acc: 81.25%,  total acc: 84.80%   [EVAL] batch:   74 | acc: 81.25%,  total acc: 84.75%   [EVAL] batch:   75 | acc: 100.00%,  total acc: 84.95%   [EVAL] batch:   76 | acc: 87.50%,  total acc: 84.98%   [EVAL] batch:   77 | acc: 93.75%,  total acc: 85.10%   [EVAL] batch:   78 | acc: 93.75%,  total acc: 85.21%   [EVAL] batch:   79 | acc: 93.75%,  total acc: 85.31%   [EVAL] batch:   80 | acc: 100.00%,  total acc: 85.49%   [EVAL] batch:   81 | acc: 43.75%,  total acc: 84.98%   [EVAL] batch:   82 | acc: 31.25%,  total acc: 84.34%   [EVAL] batch:   83 | acc: 25.00%,  total acc: 83.63%   [EVAL] batch:   84 | acc: 50.00%,  total acc: 83.24%   [EVAL] batch:   85 | acc: 37.50%,  total acc: 82.70%   [EVAL] batch:   86 | acc: 31.25%,  total acc: 82.11%   [EVAL] batch:   87 | acc: 25.00%,  total acc: 81.46%   [EVAL] batch:   88 | acc: 12.50%,  total acc: 80.69%   [EVAL] batch:   89 | acc: 0.00%,  total acc: 79.79%   [EVAL] batch:   90 | acc: 6.25%,  total acc: 78.98%   [EVAL] batch:   91 | acc: 18.75%,  total acc: 78.33%   [EVAL] batch:   92 | acc: 18.75%,  total acc: 77.69%   [EVAL] batch:   93 | acc: 25.00%,  total acc: 77.13%   [EVAL] batch:   94 | acc: 81.25%,  total acc: 77.17%   [EVAL] batch:   95 | acc: 93.75%,  total acc: 77.34%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 77.58%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 77.81%   [EVAL] batch:   98 | acc: 87.50%,  total acc: 77.90%   [EVAL] batch:   99 | acc: 81.25%,  total acc: 77.94%   [EVAL] batch:  100 | acc: 93.75%,  total acc: 78.09%   [EVAL] batch:  101 | acc: 93.75%,  total acc: 78.25%   [EVAL] batch:  102 | acc: 93.75%,  total acc: 78.40%   [EVAL] batch:  103 | acc: 93.75%,  total acc: 78.55%   [EVAL] batch:  104 | acc: 93.75%,  total acc: 78.69%   [EVAL] batch:  105 | acc: 93.75%,  total acc: 78.83%   [EVAL] batch:  106 | acc: 87.50%,  total acc: 78.91%   [EVAL] batch:  107 | acc: 68.75%,  total acc: 78.82%   [EVAL] batch:  108 | acc: 43.75%,  total acc: 78.50%   [EVAL] batch:  109 | acc: 56.25%,  total acc: 78.30%   [EVAL] batch:  110 | acc: 68.75%,  total acc: 78.21%   [EVAL] batch:  111 | acc: 75.00%,  total acc: 78.18%   [EVAL] batch:  112 | acc: 50.00%,  total acc: 77.93%   [EVAL] batch:  113 | acc: 62.50%,  total acc: 77.80%   [EVAL] batch:  114 | acc: 68.75%,  total acc: 77.72%   [EVAL] batch:  115 | acc: 87.50%,  total acc: 77.80%   [EVAL] batch:  116 | acc: 62.50%,  total acc: 77.67%   [EVAL] batch:  117 | acc: 75.00%,  total acc: 77.65%   [EVAL] batch:  118 | acc: 62.50%,  total acc: 77.52%   [EVAL] batch:  119 | acc: 50.00%,  total acc: 77.29%   [EVAL] batch:  120 | acc: 50.00%,  total acc: 77.07%   [EVAL] batch:  121 | acc: 56.25%,  total acc: 76.90%   [EVAL] batch:  122 | acc: 68.75%,  total acc: 76.83%   [EVAL] batch:  123 | acc: 81.25%,  total acc: 76.86%   [EVAL] batch:  124 | acc: 68.75%,  total acc: 76.80%   [EVAL] batch:  125 | acc: 93.75%,  total acc: 76.93%   [EVAL] batch:  126 | acc: 81.25%,  total acc: 76.97%   [EVAL] batch:  127 | acc: 87.50%,  total acc: 77.05%   [EVAL] batch:  128 | acc: 87.50%,  total acc: 77.13%   [EVAL] batch:  129 | acc: 87.50%,  total acc: 77.21%   [EVAL] batch:  130 | acc: 100.00%,  total acc: 77.39%   [EVAL] batch:  131 | acc: 100.00%,  total acc: 77.56%   [EVAL] batch:  132 | acc: 87.50%,  total acc: 77.63%   [EVAL] batch:  133 | acc: 87.50%,  total acc: 77.71%   [EVAL] batch:  134 | acc: 62.50%,  total acc: 77.59%   [EVAL] batch:  135 | acc: 81.25%,  total acc: 77.62%   [EVAL] batch:  136 | acc: 75.00%,  total acc: 77.60%   [EVAL] batch:  137 | acc: 81.25%,  total acc: 77.63%   [EVAL] batch:  138 | acc: 93.75%,  total acc: 77.74%   [EVAL] batch:  139 | acc: 75.00%,  total acc: 77.72%   [EVAL] batch:  140 | acc: 93.75%,  total acc: 77.84%   [EVAL] batch:  141 | acc: 75.00%,  total acc: 77.82%   [EVAL] batch:  142 | acc: 87.50%,  total acc: 77.88%   [EVAL] batch:  143 | acc: 93.75%,  total acc: 77.99%   [EVAL] batch:  144 | acc: 68.75%,  total acc: 77.93%   [EVAL] batch:  145 | acc: 68.75%,  total acc: 77.87%   [EVAL] batch:  146 | acc: 68.75%,  total acc: 77.81%   [EVAL] batch:  147 | acc: 75.00%,  total acc: 77.79%   [EVAL] batch:  148 | acc: 75.00%,  total acc: 77.77%   [EVAL] batch:  149 | acc: 81.25%,  total acc: 77.79%   [EVAL] batch:  150 | acc: 18.75%,  total acc: 77.40%   [EVAL] batch:  151 | acc: 62.50%,  total acc: 77.30%   [EVAL] batch:  152 | acc: 31.25%,  total acc: 77.00%   [EVAL] batch:  153 | acc: 31.25%,  total acc: 76.70%   [EVAL] batch:  154 | acc: 25.00%,  total acc: 76.37%   [EVAL] batch:  155 | acc: 25.00%,  total acc: 76.04%   [EVAL] batch:  156 | acc: 56.25%,  total acc: 75.92%   [EVAL] batch:  157 | acc: 56.25%,  total acc: 75.79%   [EVAL] batch:  158 | acc: 56.25%,  total acc: 75.67%   [EVAL] batch:  159 | acc: 50.00%,  total acc: 75.51%   [EVAL] batch:  160 | acc: 62.50%,  total acc: 75.43%   [EVAL] batch:  161 | acc: 62.50%,  total acc: 75.35%   [EVAL] batch:  162 | acc: 56.25%,  total acc: 75.23%   [EVAL] batch:  163 | acc: 68.75%,  total acc: 75.19%   [EVAL] batch:  164 | acc: 75.00%,  total acc: 75.19%   [EVAL] batch:  165 | acc: 75.00%,  total acc: 75.19%   [EVAL] batch:  166 | acc: 68.75%,  total acc: 75.15%   [EVAL] batch:  167 | acc: 87.50%,  total acc: 75.22%   [EVAL] batch:  168 | acc: 56.25%,  total acc: 75.11%   [EVAL] batch:  169 | acc: 37.50%,  total acc: 74.89%   [EVAL] batch:  170 | acc: 31.25%,  total acc: 74.63%   [EVAL] batch:  171 | acc: 18.75%,  total acc: 74.31%   [EVAL] batch:  172 | acc: 37.50%,  total acc: 74.10%   [EVAL] batch:  173 | acc: 18.75%,  total acc: 73.78%   [EVAL] batch:  174 | acc: 31.25%,  total acc: 73.54%   [EVAL] batch:  175 | acc: 100.00%,  total acc: 73.69%   [EVAL] batch:  176 | acc: 100.00%,  total acc: 73.83%   [EVAL] batch:  177 | acc: 100.00%,  total acc: 73.98%   [EVAL] batch:  178 | acc: 93.75%,  total acc: 74.09%   [EVAL] batch:  179 | acc: 93.75%,  total acc: 74.20%   [EVAL] batch:  180 | acc: 100.00%,  total acc: 74.34%   [EVAL] batch:  181 | acc: 100.00%,  total acc: 74.48%   [EVAL] batch:  182 | acc: 93.75%,  total acc: 74.59%   [EVAL] batch:  183 | acc: 100.00%,  total acc: 74.73%   [EVAL] batch:  184 | acc: 100.00%,  total acc: 74.86%   [EVAL] batch:  185 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:  186 | acc: 100.00%,  total acc: 75.13%   [EVAL] batch:  187 | acc: 50.00%,  total acc: 75.00%   
cur_acc:  ['0.9494', '0.6151', '0.7143']
his_acc:  ['0.9494', '0.7760', '0.7500']
Clustering into  19  clusters
Clusters:  [12  5 10  2  3  7 10 14  5  4  3 15  8  6  0 17  8  5  8  5  1 14 14  3
  6  4 12  7 16  0 18  8 14 15  9 11 13  1  2  1]
Losses:  9.984111785888672 2.4357728958129883 0.8641633987426758
CurrentTrain: epoch  0, batch     0 | loss: 9.9841118Losses:  9.599392890930176 2.7587504386901855 0.8245941400527954
CurrentTrain: epoch  0, batch     1 | loss: 9.5993929Losses:  11.451295852661133 4.013365745544434 0.7866688370704651
CurrentTrain: epoch  0, batch     2 | loss: 11.4512959Losses:  6.092223644256592 -0.0 0.10082928836345673
CurrentTrain: epoch  0, batch     3 | loss: 6.0922236Losses:  13.431538581848145 6.238894462585449 0.5750268697738647
CurrentTrain: epoch  1, batch     0 | loss: 13.4315386Losses:  10.70619010925293 4.061560153961182 0.7734411954879761
CurrentTrain: epoch  1, batch     1 | loss: 10.7061901Losses:  7.978793144226074 2.5134353637695312 0.7790791988372803
CurrentTrain: epoch  1, batch     2 | loss: 7.9787931Losses:  2.4028091430664062 -0.0 -0.0
CurrentTrain: epoch  1, batch     3 | loss: 2.4028091Losses:  8.209835052490234 2.220543146133423 0.7337454557418823
CurrentTrain: epoch  2, batch     0 | loss: 8.2098351Losses:  7.301083564758301 2.289799213409424 0.687384843826294
CurrentTrain: epoch  2, batch     1 | loss: 7.3010836Losses:  7.8895978927612305 2.4750967025756836 0.741267204284668
CurrentTrain: epoch  2, batch     2 | loss: 7.8895979Losses:  4.608885288238525 -0.0 0.1343437135219574
CurrentTrain: epoch  2, batch     3 | loss: 4.6088853Losses:  7.7658162117004395 2.8793931007385254 0.5881682634353638
CurrentTrain: epoch  3, batch     0 | loss: 7.7658162Losses:  7.716747760772705 2.9376533031463623 0.6861844062805176
CurrentTrain: epoch  3, batch     1 | loss: 7.7167478Losses:  10.29187297821045 4.458661079406738 0.6228131055831909
CurrentTrain: epoch  3, batch     2 | loss: 10.2918730Losses:  1.87200129032135 -0.0 0.10678353905677795
CurrentTrain: epoch  3, batch     3 | loss: 1.8720013Losses:  7.825100421905518 4.028440952301025 0.6894659399986267
CurrentTrain: epoch  4, batch     0 | loss: 7.8251004Losses:  7.426762580871582 2.760468006134033 0.6948785185813904
CurrentTrain: epoch  4, batch     1 | loss: 7.4267626Losses:  8.757857322692871 3.4109673500061035 0.6150716543197632
CurrentTrain: epoch  4, batch     2 | loss: 8.7578573Losses:  6.2487053871154785 -0.0 0.2186410129070282
CurrentTrain: epoch  4, batch     3 | loss: 6.2487054Losses:  9.327943801879883 3.940288543701172 0.6482371091842651
CurrentTrain: epoch  5, batch     0 | loss: 9.3279438Losses:  7.105755805969238 3.178345203399658 0.5751651525497437
CurrentTrain: epoch  5, batch     1 | loss: 7.1057558Losses:  6.726154327392578 2.7849602699279785 0.6766294240951538
CurrentTrain: epoch  5, batch     2 | loss: 6.7261543Losses:  1.848874568939209 -0.0 0.09524007886648178
CurrentTrain: epoch  5, batch     3 | loss: 1.8488746Losses:  6.610224723815918 2.3728151321411133 0.7932236790657043
CurrentTrain: epoch  6, batch     0 | loss: 6.6102247Losses:  8.065645217895508 4.0314531326293945 0.6224629282951355
CurrentTrain: epoch  6, batch     1 | loss: 8.0656452Losses:  7.781256675720215 3.2459964752197266 0.6455506682395935
CurrentTrain: epoch  6, batch     2 | loss: 7.7812567Losses:  4.122786045074463 -0.0 0.18999439477920532
CurrentTrain: epoch  6, batch     3 | loss: 4.1227860Losses:  7.767662525177002 3.628568410873413 0.5170087218284607
CurrentTrain: epoch  7, batch     0 | loss: 7.7676625Losses:  6.941398620605469 3.4334588050842285 0.7476302981376648
CurrentTrain: epoch  7, batch     1 | loss: 6.9413986Losses:  7.3057050704956055 3.3732235431671143 0.7377527952194214
CurrentTrain: epoch  7, batch     2 | loss: 7.3057051Losses:  5.6172637939453125 -0.0 -0.0
CurrentTrain: epoch  7, batch     3 | loss: 5.6172638Losses:  8.275357246398926 4.096068382263184 0.6010284423828125
CurrentTrain: epoch  8, batch     0 | loss: 8.2753572Losses:  5.288497447967529 1.6806690692901611 0.7514394521713257
CurrentTrain: epoch  8, batch     1 | loss: 5.2884974Losses:  5.953807830810547 2.6409010887145996 0.7520282864570618
CurrentTrain: epoch  8, batch     2 | loss: 5.9538078Losses:  2.0285606384277344 -0.0 0.12536028027534485
CurrentTrain: epoch  8, batch     3 | loss: 2.0285606Losses:  6.11625862121582 2.628337860107422 0.722737193107605
CurrentTrain: epoch  9, batch     0 | loss: 6.1162586Losses:  6.764169692993164 2.462193012237549 0.6948717832565308
CurrentTrain: epoch  9, batch     1 | loss: 6.7641697Losses:  5.539618968963623 1.9289630651474 0.7268559336662292
CurrentTrain: epoch  9, batch     2 | loss: 5.5396190Losses:  2.677875518798828 -0.0 0.0965239405632019
CurrentTrain: epoch  9, batch     3 | loss: 2.6778755
#### applies to jurisdiction ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "applies to jurisdiction" means the item (institution, law, public office, public register...) or statement belongs to or has power over or applies to the value (a territorial jurisdiction: a country, state, municipality, ...).
Here is an example:
Relation: applies to jurisdiction
Context: in 2004 the catalan government gave him the george cross .
Head Entity: george cross
Tail Entity: catalan
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "applies to jurisdiction":
Relation: applies to jurisdiction  
Context: The new environmental regulations established by the European Union will be enforced across all member states starting next year.  
Head Entity: environmental regulations  
Tail Entity: European Union  

Relation: applies to jurisdiction  
Context: The Supreme Court's ruling on the matter will have implications for all states within the United States.  
Head Entity: Supreme Court  
Tail Entity: United States  
#### performer ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "performer" means actor, musician, band or other performer associated with this role or musical work.
Here is an example:
Relation: performer
Context: " tenth avenue freeze - out " is the second song on bruce springsteen 's breakthrough album " born to run " , released in 1975 .
Head Entity: tenth avenue freeze - out
Tail Entity: bruce springsteen
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "performer":
Relation: performer  
Context: "Shape of You" is a popular song by Ed Sheeran that topped charts worldwide after its release in 2017.  
Head Entity: Shape of You  
Tail Entity: Ed Sheeran  

Relation: performer  
Context: The Beatles are widely regarded as one of the most influential bands in the history of music, with hits like "Hey Jude" and "Let It Be."  
Head Entity: The Beatles  
Tail Entity: Hey Jude  
#### manufacturer ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "manufacturer" means manufacturer or producer of this product.
Here is an example:
Relation: manufacturer
Context: an emd gp49 is a 4-axle diesel locomotive built by general motors electro - motive division .
Head Entity: emd gp49
Tail Entity: general motors electro - motive division
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "manufacturer":
Relation: manufacturer  
Context: the iphone 13 is a smartphone designed and marketed by apple inc.  
Head Entity: iphone 13  
Tail Entity: apple inc.  

Relation: manufacturer  
Context: the model s is an all-electric sedan produced by tesla, inc.  
Head Entity: model s  
Tail Entity: tesla, inc.  
#### participant ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "participant" means person, group of people or organization (object) that actively takes/took part in an event or process.
Here is an example:
Relation: participant
Context: in contrast to gwladys and roger 's allegiance to the house of lancaster , their three sons were staunch yorkists during the wars of the roses .
Head Entity: wars of the roses
Tail Entity: house of lancaster
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "participant":
Relation: participant  
Context: During the annual tech conference, several startups showcased their innovative products, while industry leaders discussed the future of technology.  
Head Entity: tech conference  
Tail Entity: startups  

Relation: participant  
Context: The local community organized a charity run to raise funds for the homeless, with many volunteers helping to make the event a success.  
Head Entity: charity run  
Tail Entity: volunteers  
#### operating system ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "operating system" means operating system (OS) on which a software works or the OS installed on hardware.
Here is an example:
Relation: operating system
Context: the software is written in python and works on all the platforms supported by blender : windows , osx and linux .
Head Entity: blender
Tail Entity: windows
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "operating system":
Relation: operating system  
Context: The latest version of the application is compatible with various operating systems, including macOS, Windows, and Ubuntu.  
Head Entity: application  
Tail Entity: macOS  

Relation: operating system  
Context: This game runs smoothly on multiple operating systems, such as Linux, Windows 10, and the latest version of macOS.  
Head Entity: game  
Tail Entity: Windows 10  
#### competition class ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "competition class" means official classification by a regulating body under which the subject (events, teams, participants, or equipment) qualifies for inclusion.
Here is an example:
Relation: competition class
Context: in his next bout , he won the wbc light welterweight title by defeating roger mayweather for a second time .
Head Entity: roger mayweather
Tail Entity: light welterweight
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "competition class":
Relation: competition class  
Context: The team secured their place in the tournament by qualifying under the UEFA Champions League regulations.  
Head Entity: UEFA Champions League  
Tail Entity: tournament  

Relation: competition class  
Context: The athlete competed in the 100 meters event, which is classified under the Olympic Games standards.  
Head Entity: Olympic Games  
Tail Entity: 100 meters event  
#### successful candidate ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "successful candidate" means person elected after the election.
Here is an example:
Relation: successful candidate
Context: swartz served as swedish prime minister for only seven months before being forced from power after the swedish elections of 1917 and replaced by the liberal government of nils edén .
Head Entity: swedish elections of 1917
Tail Entity: nils edén
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "successful candidate":
Relation: successful candidate  
Context: in the recent municipal elections, the voters chose maria gonzalez as the new mayor of the city, marking a significant shift in local governance.  
Head Entity: municipal elections  
Tail Entity: maria gonzalez  

Relation: successful candidate  
Context: after a closely contested race, the citizens of springfield elected john doe as their new governor, succeeding the long-serving incumbent.  
Head Entity: springfield elections  
Tail Entity: john doe  
#### sibling ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "sibling" means the subject and the object have the same parents.
Here is an example:
Relation: sibling
Context: although the latter 's eldest son , lǫgmaðr , appears to have succeeded to the kingship , he was soon forced to contend with factions supporting his younger brothers : haraldr , and óláfr .
Head Entity: lǫgmaðr
Tail Entity: óláfr
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "sibling":
Relation: sibling  
Context: In the royal family of the ancient kingdom, the two brothers, Eirik and Sigurd, were known for their fierce rivalry, yet they shared a bond that only siblings could understand.  
Head Entity: Eirik  
Tail Entity: Sigurd  

Relation: sibling  
Context: During the family reunion, it became clear that the two sisters, Maria and Elena, had grown apart over the years, but their shared childhood memories reminded them of their strong sibling connection.  
Head Entity: Maria  
Tail Entity: Elena  
#### military branch ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "military branch" means branch to which this military unit, award, office, or person belongs.
Here is an example:
Relation: military branch
Context: iain harrison is a competitive shooter and former british army captain .
Head Entity: iain harrison
Tail Entity: british army
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "military branch":
Relation: military branch  
Context: sergeant johnson served in the united states marine corps during his military career.  
Head Entity: sergeant johnson  
Tail Entity: united states marine corps  

Relation: military branch  
Context: general smith was a prominent figure in the royal air force, leading several key missions.  
Head Entity: general smith  
Tail Entity: royal air force  
#### child ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "child" means subject has object as child.
Here is an example:
Relation: child
Context: ziad rahbani is the son of the lebanese famous composer assi rahbani and nouhad haddad , the famous lebanese female singer known as fairuz .
Head Entity: assi rahbani
Tail Entity: ziad rahbani
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "child":
Relation: child  
Context: in the animated series, the character of simon is the son of the adventurous couple, marie and tom.  
Head Entity: marie  
Tail Entity: simon  

Relation: child  
Context: the renowned physicist albert einstein had a daughter named lieserl, who was born in 1902.  
Head Entity: albert einstein  
Tail Entity: lieserl  
Losses:  5.755472183227539 0.5773992538452148 0.8756478428840637
MemoryTrain:  epoch  0, batch     0 | loss: 5.7554722Losses:  5.857130527496338 0.31372085213661194 0.886351466178894
MemoryTrain:  epoch  0, batch     1 | loss: 5.8571305Losses:  5.495755672454834 1.3137857913970947 0.7757192254066467
MemoryTrain:  epoch  0, batch     2 | loss: 5.4957557Losses:  5.508505821228027 0.8516223430633545 0.9049977660179138
MemoryTrain:  epoch  0, batch     3 | loss: 5.5085058Losses:  4.562330722808838 0.5123329162597656 0.8477701544761658
MemoryTrain:  epoch  0, batch     4 | loss: 4.5623307Losses:  5.393918037414551 0.5260359048843384 0.8235183954238892
MemoryTrain:  epoch  0, batch     5 | loss: 5.3939180Losses:  5.971256256103516 1.4725048542022705 0.8020551800727844
MemoryTrain:  epoch  0, batch     6 | loss: 5.9712563Losses:  4.004941463470459 -0.0 0.6611703634262085
MemoryTrain:  epoch  0, batch     7 | loss: 4.0049415Losses:  5.041686534881592 0.8346202373504639 0.8435242772102356
MemoryTrain:  epoch  1, batch     0 | loss: 5.0416865Losses:  5.681604862213135 0.8581734895706177 0.8131048083305359
MemoryTrain:  epoch  1, batch     1 | loss: 5.6816049Losses:  4.47853946685791 0.3329383134841919 0.8640348315238953
MemoryTrain:  epoch  1, batch     2 | loss: 4.4785395Losses:  4.890233039855957 0.5544974207878113 0.9387015104293823
MemoryTrain:  epoch  1, batch     3 | loss: 4.8902330Losses:  4.977856636047363 0.5999060273170471 0.7476032376289368
MemoryTrain:  epoch  1, batch     4 | loss: 4.9778566Losses:  5.6764116287231445 0.5377954244613647 0.7400935888290405
MemoryTrain:  epoch  1, batch     5 | loss: 5.6764116Losses:  5.167664527893066 0.4997076392173767 0.9227496385574341
MemoryTrain:  epoch  1, batch     6 | loss: 5.1676645Losses:  3.0610263347625732 -0.0 0.39313533902168274
MemoryTrain:  epoch  1, batch     7 | loss: 3.0610263Losses:  4.074014663696289 0.3110847473144531 0.7756065130233765
MemoryTrain:  epoch  2, batch     0 | loss: 4.0740147Losses:  5.800370693206787 1.4051779508590698 0.8731417059898376
MemoryTrain:  epoch  2, batch     1 | loss: 5.8003707Losses:  3.4702494144439697 0.499955415725708 0.7897443175315857
MemoryTrain:  epoch  2, batch     2 | loss: 3.4702494Losses:  5.399386405944824 1.5019845962524414 0.7143746018409729
MemoryTrain:  epoch  2, batch     3 | loss: 5.3993864Losses:  5.187747478485107 1.1105743646621704 0.928243100643158
MemoryTrain:  epoch  2, batch     4 | loss: 5.1877475Losses:  5.204931735992432 1.174607515335083 0.9374037981033325
MemoryTrain:  epoch  2, batch     5 | loss: 5.2049317Losses:  3.7200450897216797 -0.0 0.9613794088363647
MemoryTrain:  epoch  2, batch     6 | loss: 3.7200451Losses:  3.201223373413086 -0.0 0.5406986474990845
MemoryTrain:  epoch  2, batch     7 | loss: 3.2012234Losses:  4.771750450134277 1.1067850589752197 0.8499565124511719
MemoryTrain:  epoch  3, batch     0 | loss: 4.7717505Losses:  3.5535473823547363 0.5036104917526245 0.814908504486084
MemoryTrain:  epoch  3, batch     1 | loss: 3.5535474Losses:  3.522583484649658 -0.0 0.8216356635093689
MemoryTrain:  epoch  3, batch     2 | loss: 3.5225835Losses:  4.99471378326416 0.9260514974594116 0.8784370422363281
MemoryTrain:  epoch  3, batch     3 | loss: 4.9947138Losses:  3.8919425010681152 0.32524561882019043 0.7968502044677734
MemoryTrain:  epoch  3, batch     4 | loss: 3.8919425Losses:  3.249624490737915 0.28422898054122925 0.9123713970184326
MemoryTrain:  epoch  3, batch     5 | loss: 3.2496245Losses:  3.054105043411255 -0.0 0.9686853289604187
MemoryTrain:  epoch  3, batch     6 | loss: 3.0541050Losses:  2.7324209213256836 -0.0 0.6702786087989807
MemoryTrain:  epoch  3, batch     7 | loss: 2.7324209Losses:  4.085527420043945 -0.0 1.034740686416626
MemoryTrain:  epoch  4, batch     0 | loss: 4.0855274Losses:  3.5295050144195557 0.7794079184532166 0.8028222918510437
MemoryTrain:  epoch  4, batch     1 | loss: 3.5295050Losses:  3.417853355407715 0.28811848163604736 0.9104890823364258
MemoryTrain:  epoch  4, batch     2 | loss: 3.4178534Losses:  3.3192694187164307 0.5943678617477417 0.8041431307792664
MemoryTrain:  epoch  4, batch     3 | loss: 3.3192694Losses:  4.262669563293457 0.7352659106254578 0.9084595441818237
MemoryTrain:  epoch  4, batch     4 | loss: 4.2626696Losses:  3.654710292816162 0.5562426447868347 0.9079423546791077
MemoryTrain:  epoch  4, batch     5 | loss: 3.6547103Losses:  2.66512393951416 -0.0 0.8484878540039062
MemoryTrain:  epoch  4, batch     6 | loss: 2.6651239Losses:  2.4587249755859375 -0.0 0.5755308866500854
MemoryTrain:  epoch  4, batch     7 | loss: 2.4587250Losses:  3.2548742294311523 0.5275992751121521 0.8172260522842407
MemoryTrain:  epoch  5, batch     0 | loss: 3.2548742Losses:  2.7269139289855957 0.25304293632507324 0.7905609607696533
MemoryTrain:  epoch  5, batch     1 | loss: 2.7269139Losses:  3.0942652225494385 0.5034966468811035 0.8518587946891785
MemoryTrain:  epoch  5, batch     2 | loss: 3.0942652Losses:  3.026148557662964 0.2584892511367798 0.8215788006782532
MemoryTrain:  epoch  5, batch     3 | loss: 3.0261486Losses:  2.990370512008667 0.4765329658985138 0.9145479202270508
MemoryTrain:  epoch  5, batch     4 | loss: 2.9903705Losses:  2.9465222358703613 0.2253466248512268 0.9206595420837402
MemoryTrain:  epoch  5, batch     5 | loss: 2.9465222Losses:  4.100320816040039 0.7232545018196106 0.9456827044487
MemoryTrain:  epoch  5, batch     6 | loss: 4.1003208Losses:  2.0656511783599854 -0.0 0.6578519940376282
MemoryTrain:  epoch  5, batch     7 | loss: 2.0656512Losses:  3.3730437755584717 0.5396514534950256 0.8778348565101624
MemoryTrain:  epoch  6, batch     0 | loss: 3.3730438Losses:  2.6981282234191895 0.47811269760131836 0.7542679309844971
MemoryTrain:  epoch  6, batch     1 | loss: 2.6981282Losses:  4.395777225494385 1.4787898063659668 0.808498203754425
MemoryTrain:  epoch  6, batch     2 | loss: 4.3957772Losses:  3.8866918087005615 1.4850939512252808 0.7559936046600342
MemoryTrain:  epoch  6, batch     3 | loss: 3.8866918Losses:  3.3747751712799072 0.6073881387710571 0.8733037114143372
MemoryTrain:  epoch  6, batch     4 | loss: 3.3747752Losses:  2.858503818511963 0.530113935470581 0.7767942547798157
MemoryTrain:  epoch  6, batch     5 | loss: 2.8585038Losses:  2.5056753158569336 -0.0 0.8105687499046326
MemoryTrain:  epoch  6, batch     6 | loss: 2.5056753Losses:  1.910246729850769 -0.0 0.5074654817581177
MemoryTrain:  epoch  6, batch     7 | loss: 1.9102467Losses:  2.852998733520508 0.5369191765785217 0.8227996826171875
MemoryTrain:  epoch  7, batch     0 | loss: 2.8529987Losses:  2.5767033100128174 0.2582321763038635 0.8294425010681152
MemoryTrain:  epoch  7, batch     1 | loss: 2.5767033Losses:  2.5637736320495605 0.2522468864917755 0.7182714939117432
MemoryTrain:  epoch  7, batch     2 | loss: 2.5637736Losses:  3.071812391281128 0.7269699573516846 0.7666172981262207
MemoryTrain:  epoch  7, batch     3 | loss: 3.0718124Losses:  3.4715256690979004 0.6885409355163574 0.9202178716659546
MemoryTrain:  epoch  7, batch     4 | loss: 3.4715257Losses:  3.039217710494995 0.2256692349910736 0.8096269965171814
MemoryTrain:  epoch  7, batch     5 | loss: 3.0392177Losses:  3.8949146270751953 1.1734437942504883 0.883081316947937
MemoryTrain:  epoch  7, batch     6 | loss: 3.8949146Losses:  2.574256181716919 0.35745638608932495 0.6276907920837402
MemoryTrain:  epoch  7, batch     7 | loss: 2.5742562Losses:  3.011354446411133 0.4790257215499878 0.8958749175071716
MemoryTrain:  epoch  8, batch     0 | loss: 3.0113544Losses:  3.0487289428710938 0.5370211601257324 0.8624380826950073
MemoryTrain:  epoch  8, batch     1 | loss: 3.0487289Losses:  3.409235954284668 0.4936690926551819 0.985880434513092
MemoryTrain:  epoch  8, batch     2 | loss: 3.4092360Losses:  2.4627017974853516 -0.0 0.9176485538482666
MemoryTrain:  epoch  8, batch     3 | loss: 2.4627018Losses:  2.225426435470581 0.2639630436897278 0.574551522731781
MemoryTrain:  epoch  8, batch     4 | loss: 2.2254264Losses:  2.619199752807617 0.417047381401062 0.7652974724769592
MemoryTrain:  epoch  8, batch     5 | loss: 2.6191998Losses:  2.802363634109497 0.4973602890968323 0.8569234013557434
MemoryTrain:  epoch  8, batch     6 | loss: 2.8023636Losses:  2.971818685531616 -0.0 0.6145957112312317
MemoryTrain:  epoch  8, batch     7 | loss: 2.9718187Losses:  2.309901237487793 -0.0 0.9580804109573364
MemoryTrain:  epoch  9, batch     0 | loss: 2.3099012Losses:  3.7472805976867676 1.1351728439331055 0.8760667443275452
MemoryTrain:  epoch  9, batch     1 | loss: 3.7472806Losses:  2.7229485511779785 0.258266419172287 0.9173208475112915
MemoryTrain:  epoch  9, batch     2 | loss: 2.7229486Losses:  2.811934471130371 0.7038556337356567 0.837190568447113
MemoryTrain:  epoch  9, batch     3 | loss: 2.8119345Losses:  3.2986748218536377 0.5967822670936584 0.8906567692756653
MemoryTrain:  epoch  9, batch     4 | loss: 3.2986748Losses:  2.9397361278533936 0.539620041847229 0.931584358215332
MemoryTrain:  epoch  9, batch     5 | loss: 2.9397361Losses:  2.4069323539733887 0.2442241758108139 0.797547459602356
MemoryTrain:  epoch  9, batch     6 | loss: 2.4069324Losses:  2.163595199584961 -0.0 0.533061146736145
MemoryTrain:  epoch  9, batch     7 | loss: 2.1635952
[EVAL] batch:    0 | acc: 31.25%,  total acc: 31.25%   [EVAL] batch:    1 | acc: 37.50%,  total acc: 34.38%   [EVAL] batch:    2 | acc: 25.00%,  total acc: 31.25%   [EVAL] batch:    3 | acc: 31.25%,  total acc: 31.25%   [EVAL] batch:    4 | acc: 37.50%,  total acc: 32.50%   [EVAL] batch:    5 | acc: 50.00%,  total acc: 35.42%   [EVAL] batch:    6 | acc: 56.25%,  total acc: 38.39%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 43.75%   [EVAL] batch:    8 | acc: 75.00%,  total acc: 47.22%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 50.62%   [EVAL] batch:   10 | acc: 62.50%,  total acc: 51.70%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 54.17%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 57.21%   [EVAL] batch:   13 | acc: 81.25%,  total acc: 58.93%   [EVAL] batch:   14 | acc: 87.50%,  total acc: 60.83%   [EVAL] batch:   15 | acc: 75.00%,  total acc: 61.72%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 62.50%   [EVAL] batch:   17 | acc: 87.50%,  total acc: 63.89%   [EVAL] batch:   18 | acc: 81.25%,  total acc: 64.80%   [EVAL] batch:   19 | acc: 25.00%,  total acc: 62.81%   [EVAL] batch:   20 | acc: 62.50%,  total acc: 62.80%   [EVAL] batch:   21 | acc: 31.25%,  total acc: 61.36%   [EVAL] batch:   22 | acc: 56.25%,  total acc: 61.14%   [EVAL] batch:   23 | acc: 56.25%,  total acc: 60.94%   [EVAL] batch:   24 | acc: 18.75%,  total acc: 59.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 60.82%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 62.27%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 63.39%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 64.66%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 65.83%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 66.94%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 67.77%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 68.75%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 69.67%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 70.54%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 71.35%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 72.13%   [EVAL] batch:   37 | acc: 75.00%,  total acc: 72.20%   [EVAL] batch:   38 | acc: 56.25%,  total acc: 71.79%   [EVAL] batch:   39 | acc: 62.50%,  total acc: 71.56%   [EVAL] batch:   40 | acc: 62.50%,  total acc: 71.34%   [EVAL] batch:   41 | acc: 62.50%,  total acc: 71.13%   [EVAL] batch:   42 | acc: 81.25%,  total acc: 71.37%   [EVAL] batch:   43 | acc: 68.75%,  total acc: 71.31%   [EVAL] batch:   44 | acc: 50.00%,  total acc: 70.83%   [EVAL] batch:   45 | acc: 43.75%,  total acc: 70.24%   [EVAL] batch:   46 | acc: 62.50%,  total acc: 70.08%   [EVAL] batch:   47 | acc: 81.25%,  total acc: 70.31%   [EVAL] batch:   48 | acc: 50.00%,  total acc: 69.90%   [EVAL] batch:   49 | acc: 62.50%,  total acc: 69.75%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 70.22%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 70.67%   [EVAL] batch:   52 | acc: 87.50%,  total acc: 70.99%   [EVAL] batch:   53 | acc: 75.00%,  total acc: 71.06%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 71.48%   [EVAL] batch:   55 | acc: 68.75%,  total acc: 71.43%   [EVAL] batch:   56 | acc: 81.25%,  total acc: 71.60%   [EVAL] batch:   57 | acc: 56.25%,  total acc: 71.34%   [EVAL] batch:   58 | acc: 37.50%,  total acc: 70.76%   [EVAL] batch:   59 | acc: 50.00%,  total acc: 70.42%   [EVAL] batch:   60 | acc: 37.50%,  total acc: 69.88%   [EVAL] batch:   61 | acc: 56.25%,  total acc: 69.66%   [EVAL] batch:   62 | acc: 18.75%,  total acc: 68.85%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 84.38%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 85.00%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 86.46%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 83.93%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 83.59%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 84.03%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 84.09%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 84.62%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 85.27%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 85.83%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 86.72%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 87.85%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 88.49%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 88.75%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 88.69%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 88.35%   [EVAL] batch:   22 | acc: 87.50%,  total acc: 88.32%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 88.54%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 88.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 88.94%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 89.35%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 89.73%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 89.87%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 90.21%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 90.52%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 90.91%   [EVAL] batch:   33 | acc: 68.75%,  total acc: 90.26%   [EVAL] batch:   34 | acc: 87.50%,  total acc: 90.18%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 90.10%   [EVAL] batch:   36 | acc: 81.25%,  total acc: 89.86%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 89.80%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 90.06%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 90.31%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 90.55%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 90.77%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 90.84%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 90.91%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 90.83%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 91.03%   [EVAL] batch:   46 | acc: 68.75%,  total acc: 90.56%   [EVAL] batch:   47 | acc: 87.50%,  total acc: 90.49%   [EVAL] batch:   48 | acc: 93.75%,  total acc: 90.56%   [EVAL] batch:   49 | acc: 81.25%,  total acc: 90.38%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 90.44%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 90.38%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 90.45%   [EVAL] batch:   53 | acc: 93.75%,  total acc: 90.51%   [EVAL] batch:   54 | acc: 75.00%,  total acc: 90.23%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 90.29%   [EVAL] batch:   56 | acc: 87.50%,  total acc: 90.24%   [EVAL] batch:   57 | acc: 75.00%,  total acc: 89.98%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 90.15%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 90.21%   [EVAL] batch:   60 | acc: 87.50%,  total acc: 90.16%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 90.12%   [EVAL] batch:   62 | acc: 56.25%,  total acc: 89.58%   [EVAL] batch:   63 | acc: 6.25%,  total acc: 88.28%   [EVAL] batch:   64 | acc: 6.25%,  total acc: 87.02%   [EVAL] batch:   65 | acc: 18.75%,  total acc: 85.98%   [EVAL] batch:   66 | acc: 18.75%,  total acc: 84.98%   [EVAL] batch:   67 | acc: 12.50%,  total acc: 83.92%   [EVAL] batch:   68 | acc: 43.75%,  total acc: 83.33%   [EVAL] batch:   69 | acc: 75.00%,  total acc: 83.21%   [EVAL] batch:   70 | acc: 93.75%,  total acc: 83.36%   [EVAL] batch:   71 | acc: 93.75%,  total acc: 83.51%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 83.73%   [EVAL] batch:   73 | acc: 75.00%,  total acc: 83.61%   [EVAL] batch:   74 | acc: 81.25%,  total acc: 83.58%   [EVAL] batch:   75 | acc: 100.00%,  total acc: 83.80%   [EVAL] batch:   76 | acc: 75.00%,  total acc: 83.69%   [EVAL] batch:   77 | acc: 93.75%,  total acc: 83.81%   [EVAL] batch:   78 | acc: 87.50%,  total acc: 83.86%   [EVAL] batch:   79 | acc: 100.00%,  total acc: 84.06%   [EVAL] batch:   80 | acc: 93.75%,  total acc: 84.18%   [EVAL] batch:   81 | acc: 31.25%,  total acc: 83.54%   [EVAL] batch:   82 | acc: 25.00%,  total acc: 82.83%   [EVAL] batch:   83 | acc: 18.75%,  total acc: 82.07%   [EVAL] batch:   84 | acc: 50.00%,  total acc: 81.69%   [EVAL] batch:   85 | acc: 43.75%,  total acc: 81.25%   [EVAL] batch:   86 | acc: 37.50%,  total acc: 80.75%   [EVAL] batch:   87 | acc: 25.00%,  total acc: 80.11%   [EVAL] batch:   88 | acc: 12.50%,  total acc: 79.35%   [EVAL] batch:   89 | acc: 25.00%,  total acc: 78.75%   [EVAL] batch:   90 | acc: 0.00%,  total acc: 77.88%   [EVAL] batch:   91 | acc: 25.00%,  total acc: 77.31%   [EVAL] batch:   92 | acc: 12.50%,  total acc: 76.61%   [EVAL] batch:   93 | acc: 18.75%,  total acc: 76.00%   [EVAL] batch:   94 | acc: 81.25%,  total acc: 76.05%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 76.30%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 76.55%   [EVAL] batch:   97 | acc: 93.75%,  total acc: 76.72%   [EVAL] batch:   98 | acc: 93.75%,  total acc: 76.89%   [EVAL] batch:   99 | acc: 87.50%,  total acc: 77.00%   [EVAL] batch:  100 | acc: 93.75%,  total acc: 77.17%   [EVAL] batch:  101 | acc: 93.75%,  total acc: 77.33%   [EVAL] batch:  102 | acc: 93.75%,  total acc: 77.49%   [EVAL] batch:  103 | acc: 93.75%,  total acc: 77.64%   [EVAL] batch:  104 | acc: 93.75%,  total acc: 77.80%   [EVAL] batch:  105 | acc: 87.50%,  total acc: 77.89%   [EVAL] batch:  106 | acc: 62.50%,  total acc: 77.75%   [EVAL] batch:  107 | acc: 62.50%,  total acc: 77.60%   [EVAL] batch:  108 | acc: 43.75%,  total acc: 77.29%   [EVAL] batch:  109 | acc: 50.00%,  total acc: 77.05%   [EVAL] batch:  110 | acc: 62.50%,  total acc: 76.91%   [EVAL] batch:  111 | acc: 56.25%,  total acc: 76.73%   [EVAL] batch:  112 | acc: 37.50%,  total acc: 76.38%   [EVAL] batch:  113 | acc: 50.00%,  total acc: 76.15%   [EVAL] batch:  114 | acc: 50.00%,  total acc: 75.92%   [EVAL] batch:  115 | acc: 93.75%,  total acc: 76.08%   [EVAL] batch:  116 | acc: 62.50%,  total acc: 75.96%   [EVAL] batch:  117 | acc: 62.50%,  total acc: 75.85%   [EVAL] batch:  118 | acc: 62.50%,  total acc: 75.74%   [EVAL] batch:  119 | acc: 50.00%,  total acc: 75.52%   [EVAL] batch:  120 | acc: 43.75%,  total acc: 75.26%   [EVAL] batch:  121 | acc: 50.00%,  total acc: 75.05%   [EVAL] batch:  122 | acc: 68.75%,  total acc: 75.00%   [EVAL] batch:  123 | acc: 81.25%,  total acc: 75.05%   [EVAL] batch:  124 | acc: 56.25%,  total acc: 74.90%   [EVAL] batch:  125 | acc: 93.75%,  total acc: 75.05%   [EVAL] batch:  126 | acc: 75.00%,  total acc: 75.05%   [EVAL] batch:  127 | acc: 68.75%,  total acc: 75.00%   [EVAL] batch:  128 | acc: 62.50%,  total acc: 74.90%   [EVAL] batch:  129 | acc: 62.50%,  total acc: 74.81%   [EVAL] batch:  130 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:  131 | acc: 93.75%,  total acc: 75.14%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 75.33%   [EVAL] batch:  133 | acc: 87.50%,  total acc: 75.42%   [EVAL] batch:  134 | acc: 75.00%,  total acc: 75.42%   [EVAL] batch:  135 | acc: 93.75%,  total acc: 75.55%   [EVAL] batch:  136 | acc: 93.75%,  total acc: 75.68%   [EVAL] batch:  137 | acc: 87.50%,  total acc: 75.77%   [EVAL] batch:  138 | acc: 81.25%,  total acc: 75.81%   [EVAL] batch:  139 | acc: 81.25%,  total acc: 75.85%   [EVAL] batch:  140 | acc: 87.50%,  total acc: 75.93%   [EVAL] batch:  141 | acc: 81.25%,  total acc: 75.97%   [EVAL] batch:  142 | acc: 68.75%,  total acc: 75.92%   [EVAL] batch:  143 | acc: 93.75%,  total acc: 76.04%   [EVAL] batch:  144 | acc: 62.50%,  total acc: 75.95%   [EVAL] batch:  145 | acc: 68.75%,  total acc: 75.90%   [EVAL] batch:  146 | acc: 68.75%,  total acc: 75.85%   [EVAL] batch:  147 | acc: 75.00%,  total acc: 75.84%   [EVAL] batch:  148 | acc: 68.75%,  total acc: 75.80%   [EVAL] batch:  149 | acc: 68.75%,  total acc: 75.75%   [EVAL] batch:  150 | acc: 43.75%,  total acc: 75.54%   [EVAL] batch:  151 | acc: 56.25%,  total acc: 75.41%   [EVAL] batch:  152 | acc: 50.00%,  total acc: 75.25%   [EVAL] batch:  153 | acc: 50.00%,  total acc: 75.08%   [EVAL] batch:  154 | acc: 50.00%,  total acc: 74.92%   [EVAL] batch:  155 | acc: 68.75%,  total acc: 74.88%   [EVAL] batch:  156 | acc: 62.50%,  total acc: 74.80%   [EVAL] batch:  157 | acc: 50.00%,  total acc: 74.64%   [EVAL] batch:  158 | acc: 56.25%,  total acc: 74.53%   [EVAL] batch:  159 | acc: 43.75%,  total acc: 74.34%   [EVAL] batch:  160 | acc: 62.50%,  total acc: 74.26%   [EVAL] batch:  161 | acc: 56.25%,  total acc: 74.15%   [EVAL] batch:  162 | acc: 68.75%,  total acc: 74.12%   [EVAL] batch:  163 | acc: 75.00%,  total acc: 74.12%   [EVAL] batch:  164 | acc: 81.25%,  total acc: 74.17%   [EVAL] batch:  165 | acc: 81.25%,  total acc: 74.21%   [EVAL] batch:  166 | acc: 75.00%,  total acc: 74.21%   [EVAL] batch:  167 | acc: 87.50%,  total acc: 74.29%   [EVAL] batch:  168 | acc: 75.00%,  total acc: 74.30%   [EVAL] batch:  169 | acc: 37.50%,  total acc: 74.08%   [EVAL] batch:  170 | acc: 31.25%,  total acc: 73.83%   [EVAL] batch:  171 | acc: 18.75%,  total acc: 73.51%   [EVAL] batch:  172 | acc: 56.25%,  total acc: 73.41%   [EVAL] batch:  173 | acc: 25.00%,  total acc: 73.13%   [EVAL] batch:  174 | acc: 43.75%,  total acc: 72.96%   [EVAL] batch:  175 | acc: 100.00%,  total acc: 73.12%   [EVAL] batch:  176 | acc: 100.00%,  total acc: 73.27%   [EVAL] batch:  177 | acc: 100.00%,  total acc: 73.42%   [EVAL] batch:  178 | acc: 100.00%,  total acc: 73.57%   [EVAL] batch:  179 | acc: 100.00%,  total acc: 73.72%   [EVAL] batch:  180 | acc: 100.00%,  total acc: 73.86%   [EVAL] batch:  181 | acc: 100.00%,  total acc: 74.00%   [EVAL] batch:  182 | acc: 93.75%,  total acc: 74.11%   [EVAL] batch:  183 | acc: 100.00%,  total acc: 74.25%   [EVAL] batch:  184 | acc: 93.75%,  total acc: 74.36%   [EVAL] batch:  185 | acc: 100.00%,  total acc: 74.50%   [EVAL] batch:  186 | acc: 93.75%,  total acc: 74.60%   [EVAL] batch:  187 | acc: 56.25%,  total acc: 74.50%   [EVAL] batch:  188 | acc: 31.25%,  total acc: 74.27%   [EVAL] batch:  189 | acc: 31.25%,  total acc: 74.05%   [EVAL] batch:  190 | acc: 31.25%,  total acc: 73.82%   [EVAL] batch:  191 | acc: 31.25%,  total acc: 73.60%   [EVAL] batch:  192 | acc: 50.00%,  total acc: 73.48%   [EVAL] batch:  193 | acc: 50.00%,  total acc: 73.36%   [EVAL] batch:  194 | acc: 81.25%,  total acc: 73.40%   [EVAL] batch:  195 | acc: 75.00%,  total acc: 73.41%   [EVAL] batch:  196 | acc: 62.50%,  total acc: 73.35%   [EVAL] batch:  197 | acc: 81.25%,  total acc: 73.39%   [EVAL] batch:  198 | acc: 75.00%,  total acc: 73.40%   [EVAL] batch:  199 | acc: 87.50%,  total acc: 73.47%   [EVAL] batch:  200 | acc: 87.50%,  total acc: 73.54%   [EVAL] batch:  201 | acc: 81.25%,  total acc: 73.58%   [EVAL] batch:  202 | acc: 75.00%,  total acc: 73.58%   [EVAL] batch:  203 | acc: 87.50%,  total acc: 73.65%   [EVAL] batch:  204 | acc: 68.75%,  total acc: 73.63%   [EVAL] batch:  205 | acc: 100.00%,  total acc: 73.76%   [EVAL] batch:  206 | acc: 31.25%,  total acc: 73.55%   [EVAL] batch:  207 | acc: 56.25%,  total acc: 73.47%   [EVAL] batch:  208 | acc: 50.00%,  total acc: 73.36%   [EVAL] batch:  209 | acc: 31.25%,  total acc: 73.15%   [EVAL] batch:  210 | acc: 62.50%,  total acc: 73.10%   [EVAL] batch:  211 | acc: 43.75%,  total acc: 72.97%   [EVAL] batch:  212 | acc: 56.25%,  total acc: 72.89%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 73.01%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 73.14%   [EVAL] batch:  215 | acc: 93.75%,  total acc: 73.23%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 73.36%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 73.48%   [EVAL] batch:  218 | acc: 93.75%,  total acc: 73.57%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 73.69%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 73.81%   [EVAL] batch:  221 | acc: 100.00%,  total acc: 73.93%   [EVAL] batch:  222 | acc: 100.00%,  total acc: 74.05%   [EVAL] batch:  223 | acc: 100.00%,  total acc: 74.16%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 74.28%   [EVAL] batch:  225 | acc: 50.00%,  total acc: 74.17%   [EVAL] batch:  226 | acc: 62.50%,  total acc: 74.12%   [EVAL] batch:  227 | acc: 62.50%,  total acc: 74.07%   [EVAL] batch:  228 | acc: 62.50%,  total acc: 74.02%   [EVAL] batch:  229 | acc: 81.25%,  total acc: 74.05%   [EVAL] batch:  230 | acc: 62.50%,  total acc: 74.00%   [EVAL] batch:  231 | acc: 62.50%,  total acc: 73.95%   [EVAL] batch:  232 | acc: 43.75%,  total acc: 73.82%   [EVAL] batch:  233 | acc: 62.50%,  total acc: 73.77%   [EVAL] batch:  234 | acc: 62.50%,  total acc: 73.72%   [EVAL] batch:  235 | acc: 68.75%,  total acc: 73.70%   [EVAL] batch:  236 | acc: 56.25%,  total acc: 73.63%   [EVAL] batch:  237 | acc: 81.25%,  total acc: 73.66%   [EVAL] batch:  238 | acc: 87.50%,  total acc: 73.72%   [EVAL] batch:  239 | acc: 100.00%,  total acc: 73.83%   [EVAL] batch:  240 | acc: 75.00%,  total acc: 73.83%   [EVAL] batch:  241 | acc: 87.50%,  total acc: 73.89%   [EVAL] batch:  242 | acc: 81.25%,  total acc: 73.92%   [EVAL] batch:  243 | acc: 75.00%,  total acc: 73.92%   [EVAL] batch:  244 | acc: 68.75%,  total acc: 73.90%   [EVAL] batch:  245 | acc: 50.00%,  total acc: 73.81%   [EVAL] batch:  246 | acc: 31.25%,  total acc: 73.63%   [EVAL] batch:  247 | acc: 50.00%,  total acc: 73.54%   [EVAL] batch:  248 | acc: 37.50%,  total acc: 73.39%   [EVAL] batch:  249 | acc: 56.25%,  total acc: 73.32%   
cur_acc:  ['0.9494', '0.6151', '0.7143', '0.6885']
his_acc:  ['0.9494', '0.7760', '0.7500', '0.7332']
Clustering into  24  clusters
Clusters:  [22 11  3  0 14  9  3  6 10  5 14 13  8  1 12 21  8 10  8 10  2  6  6 14
  1  5  4  9 23 19 11  8  6 13 15 16  7  2  0  2 12 17  7  3  6 20 10  3
 18  4]
Losses:  10.255964279174805 3.7657299041748047 0.7401162385940552
CurrentTrain: epoch  0, batch     0 | loss: 10.2559643Losses:  9.660083770751953 3.4203662872314453 0.6864337921142578
CurrentTrain: epoch  0, batch     1 | loss: 9.6600838Losses:  10.180886268615723 3.7998604774475098 0.6224703192710876
CurrentTrain: epoch  0, batch     2 | loss: 10.1808863Losses:  7.825270652770996 -0.0 0.08347820490598679
CurrentTrain: epoch  0, batch     3 | loss: 7.8252707Losses:  9.462662696838379 4.2771711349487305 0.6710177063941956
CurrentTrain: epoch  1, batch     0 | loss: 9.4626627Losses:  8.378451347351074 3.687706470489502 0.618092954158783
CurrentTrain: epoch  1, batch     1 | loss: 8.3784513Losses:  9.838479042053223 3.618074893951416 0.6803366541862488
CurrentTrain: epoch  1, batch     2 | loss: 9.8384790Losses:  3.3600282669067383 -0.0 0.09827131032943726
CurrentTrain: epoch  1, batch     3 | loss: 3.3600283Losses:  8.324310302734375 3.0245347023010254 0.5965026021003723
CurrentTrain: epoch  2, batch     0 | loss: 8.3243103Losses:  7.547423839569092 2.7721381187438965 0.6847949028015137
CurrentTrain: epoch  2, batch     1 | loss: 7.5474238Losses:  6.743997573852539 3.150850772857666 0.6718509197235107
CurrentTrain: epoch  2, batch     2 | loss: 6.7439976Losses:  3.855433940887451 -0.0 0.1451425850391388
CurrentTrain: epoch  2, batch     3 | loss: 3.8554339Losses:  7.899908542633057 3.8716776371002197 0.5970589518547058
CurrentTrain: epoch  3, batch     0 | loss: 7.8999085Losses:  7.31797456741333 2.8319878578186035 0.651192307472229
CurrentTrain: epoch  3, batch     1 | loss: 7.3179746Losses:  6.735956192016602 2.2210323810577393 0.7447040677070618
CurrentTrain: epoch  3, batch     2 | loss: 6.7359562Losses:  2.2226758003234863 -0.0 -0.0
CurrentTrain: epoch  3, batch     3 | loss: 2.2226758Losses:  6.434815883636475 2.8142499923706055 0.6904932260513306
CurrentTrain: epoch  4, batch     0 | loss: 6.4348159Losses:  6.714016914367676 3.058894634246826 0.6961289644241333
CurrentTrain: epoch  4, batch     1 | loss: 6.7140169Losses:  8.44801139831543 3.6854658126831055 0.5887549519538879
CurrentTrain: epoch  4, batch     2 | loss: 8.4480114Losses:  4.214006423950195 -0.0 0.08374475687742233
CurrentTrain: epoch  4, batch     3 | loss: 4.2140064Losses:  6.892238140106201 3.7028861045837402 0.6725034713745117
CurrentTrain: epoch  5, batch     0 | loss: 6.8922381Losses:  6.324149131774902 2.5784761905670166 0.6565167903900146
CurrentTrain: epoch  5, batch     1 | loss: 6.3241491Losses:  8.212921142578125 3.864468812942505 0.6065788269042969
CurrentTrain: epoch  5, batch     2 | loss: 8.2129211Losses:  2.014174461364746 -0.0 0.10293219983577728
CurrentTrain: epoch  5, batch     3 | loss: 2.0141745Losses:  6.441280364990234 2.4947547912597656 0.6715719699859619
CurrentTrain: epoch  6, batch     0 | loss: 6.4412804Losses:  4.787477970123291 1.5277248620986938 0.7292003035545349
CurrentTrain: epoch  6, batch     1 | loss: 4.7874780Losses:  6.191967964172363 2.708651542663574 0.733850359916687
CurrentTrain: epoch  6, batch     2 | loss: 6.1919680Losses:  3.919776678085327 -0.0 0.07968378067016602
CurrentTrain: epoch  6, batch     3 | loss: 3.9197767Losses:  5.257360935211182 1.8609790802001953 0.6412751078605652
CurrentTrain: epoch  7, batch     0 | loss: 5.2573609Losses:  6.805723667144775 3.3750057220458984 0.7227463126182556
CurrentTrain: epoch  7, batch     1 | loss: 6.8057237Losses:  7.0714216232299805 3.546860456466675 0.6734895706176758
CurrentTrain: epoch  7, batch     2 | loss: 7.0714216Losses:  2.020296096801758 -0.0 0.09252642095088959
CurrentTrain: epoch  7, batch     3 | loss: 2.0202961Losses:  6.927074432373047 3.3149943351745605 0.5827319622039795
CurrentTrain: epoch  8, batch     0 | loss: 6.9270744Losses:  5.062377452850342 2.0221750736236572 0.7190670371055603
CurrentTrain: epoch  8, batch     1 | loss: 5.0623775Losses:  5.824012756347656 2.8547301292419434 0.6443527340888977
CurrentTrain: epoch  8, batch     2 | loss: 5.8240128Losses:  2.214783191680908 -0.0 -0.0
CurrentTrain: epoch  8, batch     3 | loss: 2.2147832Losses:  4.951793670654297 1.9077789783477783 0.7061030864715576
CurrentTrain: epoch  9, batch     0 | loss: 4.9517937Losses:  5.956842422485352 2.9485511779785156 0.6435115337371826
CurrentTrain: epoch  9, batch     1 | loss: 5.9568424Losses:  5.379152774810791 1.9684247970581055 0.7032489776611328
CurrentTrain: epoch  9, batch     2 | loss: 5.3791528Losses:  3.0303666591644287 -0.0 0.12670458853244781
CurrentTrain: epoch  9, batch     3 | loss: 3.0303667
#### contains administrative territorial entity ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "contains administrative territorial entity" means direct subdivisions of an administrative territorial entity.
Here is an example:
Relation: contains administrative territorial entity
Context: the cantons of créteil are administrative divisions of the val - de - marne department , île - de - france region , northern france .
Head Entity: île - de - france
Tail Entity: val - de - marne
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "contains administrative territorial entity":
Relation: contains administrative territorial entity  
Context: The provinces of Canada are the primary administrative divisions, with Ontario being one of the largest provinces in the country.  
Head Entity: Canada  
Tail Entity: Ontario  

Relation: contains administrative territorial entity  
Context: The state of California is divided into several counties, including Los Angeles County, which is known for its entertainment industry.  
Head Entity: California  
Tail Entity: Los Angeles County  
#### said to be the same as ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "said to be the same as" means this item is said to be the same as that item, but it's uncertain or disputed.
Here is an example:
Relation: said to be the same as
Context: heinz , is a german given name , a diminutive of heinrich and cognate of the given name henry .
Head Entity: heinrich
Tail Entity: henry
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "said to be the same as":
Relation: said to be the same as  
Context: The two species, Canis lupus and Canis familiaris, are often said to be the same as they share a common ancestor and many genetic traits.  
Head Entity: Canis lupus  
Tail Entity: Canis familiaris  

Relation: said to be the same as  
Context: In some cultures, the terms "soda" and "pop" are said to be the same as they refer to the same type of carbonated beverage, though preferences vary by region.  
Head Entity: soda  
Tail Entity: pop  
#### winner ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "winner" means winner of a competition or similar event.
Here is an example:
Relation: winner
Context: in 2010 , ty burrell received a nomination for the television critics association award for individual achievement in comedy , winning in 2011 .
Head Entity: television critics association award for individual achievement in comedy
Tail Entity: ty burrell
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "winner":
Relation: winner  
Context: In 2022, the Boston Red Sox triumphed in the World Series, marking their fourth championship in the last two decades.  
Head Entity: World Series  
Tail Entity: Boston Red Sox  

Relation: winner  
Context: At the annual film festival, the documentary "Voices of Change" was awarded the Best Documentary prize, highlighting its impactful storytelling.  
Head Entity: Best Documentary prize  
Tail Entity: "Voices of Change"  
#### military rank ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "military rank" means military rank achieved by a person, or military rank associated with a position.
Here is an example:
Relation: military rank
Context: air commodore arthur kellam tylee obe ( 24 april 1887 – 13 april 1961 ) was canadian officer who served in the royal flying corps during world war i.
Head Entity: arthur kellam tylee
Tail Entity: air commodore
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "military rank":
Relation: military rank  
Context: general john smith was appointed as the chief of staff of the united states army in 2020, overseeing all operations and strategic planning.  
Head Entity: john smith  
Tail Entity: chief of staff  

Relation: military rank  
Context: colonel sarah jones led her battalion with distinction during the peacekeeping mission in the middle east, earning her a commendation for her leadership.  
Head Entity: sarah jones  
Tail Entity: colonel  
#### publisher ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "publisher" means organization or person responsible for publishing books, periodicals, printed music, podcasts, games or software.
Here is an example:
Relation: publisher
Context: tokyopop also released a collection of karin short stories under the title : " chibi vampire : airmail " on august 31 , 2010 .
Head Entity: karin
Tail Entity: tokyopop
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "publisher":
Relation: publisher  
Context: Penguin Random House announced the release of a new novel by the acclaimed author John Grisham, set to hit the shelves on June 15, 2021.  
Head Entity: John Grisham  
Tail Entity: Penguin Random House  

Relation: publisher  
Context: The popular online platform Wattpad has been instrumental in launching the careers of many writers, including Anna Todd, whose series "After" gained immense popularity.  
Head Entity: Anna Todd  
Tail Entity: Wattpad  
#### followed by ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "followed by" means immediately following item in a series of which the subject is a part.
Here is an example:
Relation: followed by
Context: " choose you " and " homesick " were released as the album 's second and third singles , respectively , and each attained moderate chart success .
Head Entity: choose you
Tail Entity: homesick
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "followed by":
Relation: followed by  
Context: The first chapter of the book introduces the main character, while the second chapter delves into the backstory.  
Head Entity: first chapter  
Tail Entity: second chapter  

Relation: followed by  
Context: The opening act of the concert was a local band, followed by a well-known pop artist who energized the crowd.  
Head Entity: local band  
Tail Entity: well-known pop artist  
#### work location ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "work location" means location where persons or organisations were actively participating in employment, business or other work.
Here is an example:
Relation: work location
Context: there is a memoir of his life and work by gustav queck ( jena , 1852 ) .
Head Entity: gustav queck
Tail Entity: jena
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "work location":
Relation: work location  
Context: The headquarters of the company is situated in San Francisco, where they have been operating since 2010.  
Head Entity: the company  
Tail Entity: San Francisco  

Relation: work location  
Context: During her time at the university, she conducted research in various labs located in Cambridge.  
Head Entity: she  
Tail Entity: Cambridge  
#### occupation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "occupation" means occupation of a person.
Here is an example:
Relation: occupation
Context: andrei droznin ( russian : андрей дрознин ) is a russian theatre director and movement coach .
Head Entity: andrei droznin
Tail Entity: theatre director
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "occupation":
Relation: occupation  
Context: dr. emily thompson is a renowned biochemist known for her groundbreaking research in cancer treatment.  
Head Entity: dr. emily thompson  
Tail Entity: biochemist  

Relation: occupation  
Context: john smith, a talented software engineer, has been working at tech innovations for over five years.  
Head Entity: john smith  
Tail Entity: software engineer  
#### heritage designation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "heritage designation" means heritage designation of a cultural or natural site.
Here is an example:
Relation: heritage designation
Context: altamaha town , bluffton historic district , church of the cross , and rose hill plantation house are listed on the national register of historic places .
Head Entity: altamaha town
Tail Entity: national register of historic places
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "heritage designation":
Relation: heritage designation  
Context: The Great Wall of China is recognized as a UNESCO World Heritage Site due to its historical significance and architectural grandeur.  
Head Entity: Great Wall of China  
Tail Entity: UNESCO World Heritage Site  

Relation: heritage designation  
Context: The ancient city of Petra in Jordan is designated as a World Heritage Site, attracting tourists from around the globe.  
Head Entity: ancient city of Petra  
Tail Entity: World Heritage Site  
#### voice type ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "voice type" means person's voice type.
Here is an example:
Relation: voice type
Context: beverly bower ( september 30 , 1925 – march 24 , 2002 ) was an american operatic soprano who had an active international opera career from the mid-1950s through the early 1970s .
Head Entity: beverly bower
Tail Entity: soprano
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "voice type":
Relation: voice type  
Context: luciano pavarotti ( october 12 , 1935 – september 6 , 2007 ) was an italian operatic tenor who gained worldwide fame for his televised concerts and media appearances.  
Head Entity: luciano pavarotti  
Tail Entity: tenor  

Relation: voice type  
Context: mariah carey ( born march 27 , 1969 ) is an american singer, songwriter, and actress known for her five-octave vocal range and her use of the whistle register.  
Head Entity: mariah carey  
Tail Entity: singer  
Losses:  4.031795024871826 0.3603883683681488 0.9561509490013123
MemoryTrain:  epoch  0, batch     0 | loss: 4.0317950Losses:  4.484328746795654 0.22857554256916046 0.8180108070373535
MemoryTrain:  epoch  0, batch     1 | loss: 4.4843287Losses:  4.589530944824219 0.3201051950454712 0.9722778797149658
MemoryTrain:  epoch  0, batch     2 | loss: 4.5895309Losses:  5.458035945892334 0.23307840526103973 1.041821002960205
MemoryTrain:  epoch  0, batch     3 | loss: 5.4580359Losses:  5.297441482543945 0.5439841747283936 1.0387628078460693
MemoryTrain:  epoch  0, batch     4 | loss: 5.2974415Losses:  4.4663543701171875 0.5191496014595032 0.8231124877929688
MemoryTrain:  epoch  0, batch     5 | loss: 4.4663544Losses:  4.676074504852295 0.26177889108657837 0.9556178450584412
MemoryTrain:  epoch  0, batch     6 | loss: 4.6760745Losses:  5.006516456604004 0.22638756036758423 0.9184713363647461
MemoryTrain:  epoch  0, batch     7 | loss: 5.0065165Losses:  5.352185249328613 0.6773914098739624 0.8650492429733276
MemoryTrain:  epoch  0, batch     8 | loss: 5.3521852Losses:  3.198676109313965 -0.0 0.4978804588317871
MemoryTrain:  epoch  0, batch     9 | loss: 3.1986761Losses:  5.643044471740723 0.9989047646522522 0.9358839392662048
MemoryTrain:  epoch  1, batch     0 | loss: 5.6430445Losses:  5.107676029205322 0.4202001392841339 1.0638751983642578
MemoryTrain:  epoch  1, batch     1 | loss: 5.1076760Losses:  4.041135311126709 -0.0 0.9972122311592102
MemoryTrain:  epoch  1, batch     2 | loss: 4.0411353Losses:  3.0684237480163574 0.232765331864357 0.8112689852714539
MemoryTrain:  epoch  1, batch     3 | loss: 3.0684237Losses:  3.942298412322998 0.5227447152137756 0.7488674521446228
MemoryTrain:  epoch  1, batch     4 | loss: 3.9422984Losses:  4.0486626625061035 0.4577888250350952 0.7387405633926392
MemoryTrain:  epoch  1, batch     5 | loss: 4.0486627Losses:  4.4209160804748535 0.300916463136673 0.8808589577674866
MemoryTrain:  epoch  1, batch     6 | loss: 4.4209161Losses:  3.6451029777526855 -0.0 1.0118062496185303
MemoryTrain:  epoch  1, batch     7 | loss: 3.6451030Losses:  3.842799425125122 -0.0 0.9388176798820496
MemoryTrain:  epoch  1, batch     8 | loss: 3.8427994Losses:  3.0423355102539062 -0.0 0.5146552324295044
MemoryTrain:  epoch  1, batch     9 | loss: 3.0423355Losses:  4.054260730743408 0.5248419046401978 0.9124512672424316
MemoryTrain:  epoch  2, batch     0 | loss: 4.0542607Losses:  4.213705062866211 0.9591856598854065 0.8571106791496277
MemoryTrain:  epoch  2, batch     1 | loss: 4.2137051Losses:  4.301898002624512 0.32309702038764954 0.8690018653869629
MemoryTrain:  epoch  2, batch     2 | loss: 4.3018980Losses:  4.469326019287109 0.8940993547439575 0.8030929565429688
MemoryTrain:  epoch  2, batch     3 | loss: 4.4693260Losses:  3.7071545124053955 0.5631518363952637 0.9585749506950378
MemoryTrain:  epoch  2, batch     4 | loss: 3.7071545Losses:  4.786709785461426 0.7891796827316284 0.8592232465744019
MemoryTrain:  epoch  2, batch     5 | loss: 4.7867098Losses:  3.958242177963257 0.7207140922546387 0.867394208908081
MemoryTrain:  epoch  2, batch     6 | loss: 3.9582422Losses:  3.754047393798828 0.5481895208358765 0.8445495367050171
MemoryTrain:  epoch  2, batch     7 | loss: 3.7540474Losses:  3.5355710983276367 0.2947003245353699 0.8494104146957397
MemoryTrain:  epoch  2, batch     8 | loss: 3.5355711Losses:  2.668354034423828 -0.0 0.506772518157959
MemoryTrain:  epoch  2, batch     9 | loss: 2.6683540Losses:  3.1495919227600098 0.2599896192550659 0.8571692705154419
MemoryTrain:  epoch  3, batch     0 | loss: 3.1495919Losses:  5.024677753448486 0.9501057267189026 0.9383913278579712
MemoryTrain:  epoch  3, batch     1 | loss: 5.0246778Losses:  3.502387285232544 0.7811972498893738 0.8729948997497559
MemoryTrain:  epoch  3, batch     2 | loss: 3.5023873Losses:  3.643345832824707 -0.0 0.9394906759262085
MemoryTrain:  epoch  3, batch     3 | loss: 3.6433458Losses:  3.3397507667541504 0.5840151906013489 0.8663440346717834
MemoryTrain:  epoch  3, batch     4 | loss: 3.3397508Losses:  4.5866570472717285 0.9468101263046265 0.9080997109413147
MemoryTrain:  epoch  3, batch     5 | loss: 4.5866570Losses:  3.3557605743408203 1.010505199432373 0.722221851348877
MemoryTrain:  epoch  3, batch     6 | loss: 3.3557606Losses:  3.5391972064971924 0.5415188670158386 0.8760087490081787
MemoryTrain:  epoch  3, batch     7 | loss: 3.5391972Losses:  3.0863943099975586 -0.0 1.0340429544448853
MemoryTrain:  epoch  3, batch     8 | loss: 3.0863943Losses:  3.4394748210906982 -0.0 0.5771427154541016
MemoryTrain:  epoch  3, batch     9 | loss: 3.4394748Losses:  3.6689441204071045 0.24643489718437195 1.0236778259277344
MemoryTrain:  epoch  4, batch     0 | loss: 3.6689441Losses:  3.209132194519043 0.25797298550605774 0.9232786893844604
MemoryTrain:  epoch  4, batch     1 | loss: 3.2091322Losses:  3.3318021297454834 0.27989599108695984 0.9702107906341553
MemoryTrain:  epoch  4, batch     2 | loss: 3.3318021Losses:  3.4472837448120117 0.8082647323608398 0.6350053548812866
MemoryTrain:  epoch  4, batch     3 | loss: 3.4472837Losses:  3.424947500228882 0.822960376739502 0.8418191075325012
MemoryTrain:  epoch  4, batch     4 | loss: 3.4249475Losses:  3.095938205718994 -0.0 1.0590423345565796
MemoryTrain:  epoch  4, batch     5 | loss: 3.0959382Losses:  2.7880117893218994 -0.0 1.0982434749603271
MemoryTrain:  epoch  4, batch     6 | loss: 2.7880118Losses:  2.654470443725586 -0.0 0.8719472885131836
MemoryTrain:  epoch  4, batch     7 | loss: 2.6544704Losses:  3.109947681427002 -0.0 1.0122251510620117
MemoryTrain:  epoch  4, batch     8 | loss: 3.1099477Losses:  2.463913917541504 -0.0 0.48375484347343445
MemoryTrain:  epoch  4, batch     9 | loss: 2.4639139Losses:  2.9920356273651123 0.2779029607772827 0.835292637348175
MemoryTrain:  epoch  5, batch     0 | loss: 2.9920356Losses:  3.6123924255371094 0.4862763583660126 0.9950164556503296
MemoryTrain:  epoch  5, batch     1 | loss: 3.6123924Losses:  3.078911781311035 0.2962879538536072 0.8585323095321655
MemoryTrain:  epoch  5, batch     2 | loss: 3.0789118Losses:  2.9177448749542236 0.2629490792751312 0.9446304440498352
MemoryTrain:  epoch  5, batch     3 | loss: 2.9177449Losses:  2.9450104236602783 0.5267552137374878 0.9240329265594482
MemoryTrain:  epoch  5, batch     4 | loss: 2.9450104Losses:  2.620342254638672 -0.0 0.9850935935974121
MemoryTrain:  epoch  5, batch     5 | loss: 2.6203423Losses:  4.364986896514893 1.7583106756210327 0.8268373608589172
MemoryTrain:  epoch  5, batch     6 | loss: 4.3649869Losses:  4.852908134460449 1.536948323249817 0.7045294642448425
MemoryTrain:  epoch  5, batch     7 | loss: 4.8529081Losses:  3.3531126976013184 0.5253375768661499 0.9630802869796753
MemoryTrain:  epoch  5, batch     8 | loss: 3.3531127Losses:  2.0685575008392334 -0.0 0.5267232656478882
MemoryTrain:  epoch  5, batch     9 | loss: 2.0685575Losses:  2.6483614444732666 0.27268755435943604 0.770546555519104
MemoryTrain:  epoch  6, batch     0 | loss: 2.6483614Losses:  2.8810839653015137 0.2892228960990906 0.9323339462280273
MemoryTrain:  epoch  6, batch     1 | loss: 2.8810840Losses:  2.8294262886047363 0.2595027685165405 0.9039608836174011
MemoryTrain:  epoch  6, batch     2 | loss: 2.8294263Losses:  3.033573865890503 0.28692543506622314 0.7389618754386902
MemoryTrain:  epoch  6, batch     3 | loss: 3.0335739Losses:  2.8348164558410645 0.5120223760604858 0.967280924320221
MemoryTrain:  epoch  6, batch     4 | loss: 2.8348165Losses:  3.6562633514404297 0.9379053115844727 0.8525009155273438
MemoryTrain:  epoch  6, batch     5 | loss: 3.6562634Losses:  2.625476837158203 0.2565935254096985 0.8368242979049683
MemoryTrain:  epoch  6, batch     6 | loss: 2.6254768Losses:  2.7852096557617188 0.28544068336486816 0.933370053768158
MemoryTrain:  epoch  6, batch     7 | loss: 2.7852097Losses:  3.3245205879211426 1.0863072872161865 0.8693915605545044
MemoryTrain:  epoch  6, batch     8 | loss: 3.3245206Losses:  1.8075110912322998 -0.0 0.43526360392570496
MemoryTrain:  epoch  6, batch     9 | loss: 1.8075111Losses:  3.439650535583496 0.5820850133895874 0.9582051634788513
MemoryTrain:  epoch  7, batch     0 | loss: 3.4396505Losses:  2.733536720275879 0.48271745443344116 0.7210284471511841
MemoryTrain:  epoch  7, batch     1 | loss: 2.7335367Losses:  2.2504520416259766 -0.0 0.8551939725875854
MemoryTrain:  epoch  7, batch     2 | loss: 2.2504520Losses:  3.5564255714416504 0.5831983089447021 0.8224981427192688
MemoryTrain:  epoch  7, batch     3 | loss: 3.5564256Losses:  2.783881425857544 0.5301629900932312 0.8676111102104187
MemoryTrain:  epoch  7, batch     4 | loss: 2.7838814Losses:  3.0114572048187256 0.513931393623352 0.9822118282318115
MemoryTrain:  epoch  7, batch     5 | loss: 3.0114572Losses:  2.5442090034484863 0.23216284811496735 0.9308359026908875
MemoryTrain:  epoch  7, batch     6 | loss: 2.5442090Losses:  3.37324857711792 1.1361533403396606 0.7952584624290466
MemoryTrain:  epoch  7, batch     7 | loss: 3.3732486Losses:  2.809373378753662 0.5342987775802612 0.81646728515625
MemoryTrain:  epoch  7, batch     8 | loss: 2.8093734Losses:  1.8373433351516724 -0.0 0.4443628787994385
MemoryTrain:  epoch  7, batch     9 | loss: 1.8373433Losses:  2.987718105316162 0.5476933717727661 0.9707893133163452
MemoryTrain:  epoch  8, batch     0 | loss: 2.9877181Losses:  2.625828742980957 0.530449628829956 0.7123399376869202
MemoryTrain:  epoch  8, batch     1 | loss: 2.6258287Losses:  2.374768018722534 -0.0 0.9709076285362244
MemoryTrain:  epoch  8, batch     2 | loss: 2.3747680Losses:  2.5817148685455322 0.4750649929046631 0.8438081741333008
MemoryTrain:  epoch  8, batch     3 | loss: 2.5817149Losses:  2.4939262866973877 -0.0 0.8556153774261475
MemoryTrain:  epoch  8, batch     4 | loss: 2.4939263Losses:  3.173307418823242 0.4922979474067688 1.01083242893219
MemoryTrain:  epoch  8, batch     5 | loss: 3.1733074Losses:  2.9382338523864746 0.8523957133293152 0.7816258072853088
MemoryTrain:  epoch  8, batch     6 | loss: 2.9382339Losses:  2.4593396186828613 0.27069705724716187 0.7994410395622253
MemoryTrain:  epoch  8, batch     7 | loss: 2.4593396Losses:  2.647991895675659 0.2774442732334137 0.7722834944725037
MemoryTrain:  epoch  8, batch     8 | loss: 2.6479919Losses:  2.161715269088745 -0.0 0.5289942622184753
MemoryTrain:  epoch  8, batch     9 | loss: 2.1617153Losses:  2.985454559326172 0.4819963872432709 0.9081451296806335
MemoryTrain:  epoch  9, batch     0 | loss: 2.9854546Losses:  2.747251510620117 0.48618319630622864 0.9465473890304565
MemoryTrain:  epoch  9, batch     1 | loss: 2.7472515Losses:  3.0388665199279785 0.5351879596710205 0.9651588201522827
MemoryTrain:  epoch  9, batch     2 | loss: 3.0388665Losses:  2.8899760246276855 0.5507908463478088 0.9148058891296387
MemoryTrain:  epoch  9, batch     3 | loss: 2.8899760Losses:  2.54379940032959 -0.0 1.0015202760696411
MemoryTrain:  epoch  9, batch     4 | loss: 2.5437994Losses:  2.58617901802063 0.5062440037727356 0.7185317873954773
MemoryTrain:  epoch  9, batch     5 | loss: 2.5861790Losses:  2.2427291870117188 -0.0 0.894385039806366
MemoryTrain:  epoch  9, batch     6 | loss: 2.2427292Losses:  2.3480114936828613 0.25113946199417114 0.8019329309463501
MemoryTrain:  epoch  9, batch     7 | loss: 2.3480115Losses:  2.591270923614502 0.5144997239112854 0.7132471203804016
MemoryTrain:  epoch  9, batch     8 | loss: 2.5912709Losses:  1.5650041103363037 -0.0 0.3156387507915497
MemoryTrain:  epoch  9, batch     9 | loss: 1.5650041
[EVAL] batch:    0 | acc: 50.00%,  total acc: 50.00%   [EVAL] batch:    1 | acc: 75.00%,  total acc: 62.50%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 66.67%   [EVAL] batch:    3 | acc: 62.50%,  total acc: 65.62%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 67.50%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 68.75%   [EVAL] batch:    6 | acc: 62.50%,  total acc: 67.86%   [EVAL] batch:    7 | acc: 62.50%,  total acc: 67.19%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 67.36%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 67.50%   [EVAL] batch:   10 | acc: 50.00%,  total acc: 65.91%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 66.67%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 68.75%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 70.54%   [EVAL] batch:   14 | acc: 87.50%,  total acc: 71.67%   [EVAL] batch:   15 | acc: 87.50%,  total acc: 72.66%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 74.26%   [EVAL] batch:   17 | acc: 100.00%,  total acc: 75.69%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 76.64%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 77.19%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 78.27%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 79.26%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 80.16%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 80.47%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:   25 | acc: 62.50%,  total acc: 80.53%   [EVAL] batch:   26 | acc: 50.00%,  total acc: 79.40%   [EVAL] batch:   27 | acc: 68.75%,  total acc: 79.02%   [EVAL] batch:   28 | acc: 68.75%,  total acc: 78.66%   [EVAL] batch:   29 | acc: 50.00%,  total acc: 77.71%   [EVAL] batch:   30 | acc: 56.25%,  total acc: 77.02%   [EVAL] batch:   31 | acc: 62.50%,  total acc: 76.56%   [EVAL] batch:   32 | acc: 62.50%,  total acc: 76.14%   [EVAL] batch:   33 | acc: 12.50%,  total acc: 74.26%   [EVAL] batch:   34 | acc: 37.50%,  total acc: 73.21%   [EVAL] batch:   35 | acc: 12.50%,  total acc: 71.53%   [EVAL] batch:   36 | acc: 12.50%,  total acc: 69.93%   [EVAL] batch:   37 | acc: 68.75%,  total acc: 69.90%   [EVAL] batch:   38 | acc: 68.75%,  total acc: 69.87%   [EVAL] batch:   39 | acc: 87.50%,  total acc: 70.31%   [EVAL] batch:   40 | acc: 56.25%,  total acc: 69.97%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 70.39%   [EVAL] batch:   42 | acc: 81.25%,  total acc: 70.64%   [EVAL] batch:   43 | acc: 75.00%,  total acc: 70.74%   [EVAL] batch:   44 | acc: 56.25%,  total acc: 70.42%   [EVAL] batch:   45 | acc: 68.75%,  total acc: 70.38%   [EVAL] batch:   46 | acc: 68.75%,  total acc: 70.35%   [EVAL] batch:   47 | acc: 56.25%,  total acc: 70.05%   [EVAL] batch:   48 | acc: 62.50%,  total acc: 69.90%   [EVAL] batch:   49 | acc: 75.00%,  total acc: 70.00%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 70.59%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 71.15%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 71.58%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 72.11%   [EVAL] batch:   54 | acc: 100.00%,  total acc: 72.61%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 73.10%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 73.57%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 73.92%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 74.26%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 74.69%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 75.10%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 75.50%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 75.10%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 85.94%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 85.00%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 86.46%   [EVAL] batch:    6 | acc: 62.50%,  total acc: 83.04%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 82.81%   [EVAL] batch:    8 | acc: 75.00%,  total acc: 81.94%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 82.50%   [EVAL] batch:   10 | acc: 75.00%,  total acc: 81.82%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 81.77%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:   13 | acc: 87.50%,  total acc: 81.70%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 82.50%   [EVAL] batch:   15 | acc: 93.75%,  total acc: 83.20%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 84.19%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 84.72%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 85.20%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 85.00%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 85.12%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 85.23%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 85.60%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 85.94%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 86.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 86.54%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 87.04%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 87.72%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 88.12%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 88.51%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 88.67%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 89.02%   [EVAL] batch:   33 | acc: 68.75%,  total acc: 88.42%   [EVAL] batch:   34 | acc: 81.25%,  total acc: 88.21%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 88.19%   [EVAL] batch:   36 | acc: 75.00%,  total acc: 87.84%   [EVAL] batch:   37 | acc: 81.25%,  total acc: 87.66%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 87.98%   [EVAL] batch:   39 | acc: 93.75%,  total acc: 88.12%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 88.41%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 88.39%   [EVAL] batch:   42 | acc: 87.50%,  total acc: 88.37%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 88.49%   [EVAL] batch:   44 | acc: 75.00%,  total acc: 88.19%   [EVAL] batch:   45 | acc: 93.75%,  total acc: 88.32%   [EVAL] batch:   46 | acc: 68.75%,  total acc: 87.90%   [EVAL] batch:   47 | acc: 81.25%,  total acc: 87.76%   [EVAL] batch:   48 | acc: 81.25%,  total acc: 87.63%   [EVAL] batch:   49 | acc: 75.00%,  total acc: 87.38%   [EVAL] batch:   50 | acc: 81.25%,  total acc: 87.25%   [EVAL] batch:   51 | acc: 81.25%,  total acc: 87.14%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 87.26%   [EVAL] batch:   53 | acc: 75.00%,  total acc: 87.04%   [EVAL] batch:   54 | acc: 56.25%,  total acc: 86.48%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 86.50%   [EVAL] batch:   56 | acc: 81.25%,  total acc: 86.40%   [EVAL] batch:   57 | acc: 75.00%,  total acc: 86.21%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 86.33%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 86.46%   [EVAL] batch:   60 | acc: 93.75%,  total acc: 86.58%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 86.59%   [EVAL] batch:   62 | acc: 56.25%,  total acc: 86.11%   [EVAL] batch:   63 | acc: 6.25%,  total acc: 84.86%   [EVAL] batch:   64 | acc: 6.25%,  total acc: 83.65%   [EVAL] batch:   65 | acc: 12.50%,  total acc: 82.58%   [EVAL] batch:   66 | acc: 18.75%,  total acc: 81.62%   [EVAL] batch:   67 | acc: 12.50%,  total acc: 80.61%   [EVAL] batch:   68 | acc: 43.75%,  total acc: 80.07%   [EVAL] batch:   69 | acc: 75.00%,  total acc: 80.00%   [EVAL] batch:   70 | acc: 93.75%,  total acc: 80.19%   [EVAL] batch:   71 | acc: 93.75%,  total acc: 80.38%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 80.65%   [EVAL] batch:   73 | acc: 81.25%,  total acc: 80.66%   [EVAL] batch:   74 | acc: 81.25%,  total acc: 80.67%   [EVAL] batch:   75 | acc: 100.00%,  total acc: 80.92%   [EVAL] batch:   76 | acc: 75.00%,  total acc: 80.84%   [EVAL] batch:   77 | acc: 100.00%,  total acc: 81.09%   [EVAL] batch:   78 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:   79 | acc: 100.00%,  total acc: 81.48%   [EVAL] batch:   80 | acc: 100.00%,  total acc: 81.71%   [EVAL] batch:   81 | acc: 31.25%,  total acc: 81.10%   [EVAL] batch:   82 | acc: 18.75%,  total acc: 80.35%   [EVAL] batch:   83 | acc: 12.50%,  total acc: 79.54%   [EVAL] batch:   84 | acc: 37.50%,  total acc: 79.04%   [EVAL] batch:   85 | acc: 25.00%,  total acc: 78.42%   [EVAL] batch:   86 | acc: 37.50%,  total acc: 77.95%   [EVAL] batch:   87 | acc: 18.75%,  total acc: 77.27%   [EVAL] batch:   88 | acc: 18.75%,  total acc: 76.62%   [EVAL] batch:   89 | acc: 18.75%,  total acc: 75.97%   [EVAL] batch:   90 | acc: 6.25%,  total acc: 75.21%   [EVAL] batch:   91 | acc: 18.75%,  total acc: 74.59%   [EVAL] batch:   92 | acc: 12.50%,  total acc: 73.92%   [EVAL] batch:   93 | acc: 25.00%,  total acc: 73.40%   [EVAL] batch:   94 | acc: 75.00%,  total acc: 73.42%   [EVAL] batch:   95 | acc: 93.75%,  total acc: 73.63%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 73.90%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 74.17%   [EVAL] batch:   98 | acc: 87.50%,  total acc: 74.31%   [EVAL] batch:   99 | acc: 75.00%,  total acc: 74.31%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 74.57%   [EVAL] batch:  101 | acc: 93.75%,  total acc: 74.75%   [EVAL] batch:  102 | acc: 87.50%,  total acc: 74.88%   [EVAL] batch:  103 | acc: 87.50%,  total acc: 75.00%   [EVAL] batch:  104 | acc: 87.50%,  total acc: 75.12%   [EVAL] batch:  105 | acc: 87.50%,  total acc: 75.24%   [EVAL] batch:  106 | acc: 37.50%,  total acc: 74.88%   [EVAL] batch:  107 | acc: 25.00%,  total acc: 74.42%   [EVAL] batch:  108 | acc: 6.25%,  total acc: 73.80%   [EVAL] batch:  109 | acc: 18.75%,  total acc: 73.30%   [EVAL] batch:  110 | acc: 18.75%,  total acc: 72.80%   [EVAL] batch:  111 | acc: 6.25%,  total acc: 72.21%   [EVAL] batch:  112 | acc: 25.00%,  total acc: 71.79%   [EVAL] batch:  113 | acc: 56.25%,  total acc: 71.66%   [EVAL] batch:  114 | acc: 50.00%,  total acc: 71.47%   [EVAL] batch:  115 | acc: 87.50%,  total acc: 71.61%   [EVAL] batch:  116 | acc: 62.50%,  total acc: 71.53%   [EVAL] batch:  117 | acc: 62.50%,  total acc: 71.45%   [EVAL] batch:  118 | acc: 56.25%,  total acc: 71.32%   [EVAL] batch:  119 | acc: 43.75%,  total acc: 71.09%   [EVAL] batch:  120 | acc: 37.50%,  total acc: 70.82%   [EVAL] batch:  121 | acc: 43.75%,  total acc: 70.59%   [EVAL] batch:  122 | acc: 62.50%,  total acc: 70.53%   [EVAL] batch:  123 | acc: 81.25%,  total acc: 70.61%   [EVAL] batch:  124 | acc: 43.75%,  total acc: 70.40%   [EVAL] batch:  125 | acc: 81.25%,  total acc: 70.49%   [EVAL] batch:  126 | acc: 62.50%,  total acc: 70.42%   [EVAL] batch:  127 | acc: 56.25%,  total acc: 70.31%   [EVAL] batch:  128 | acc: 43.75%,  total acc: 70.11%   [EVAL] batch:  129 | acc: 50.00%,  total acc: 69.95%   [EVAL] batch:  130 | acc: 68.75%,  total acc: 69.94%   [EVAL] batch:  131 | acc: 93.75%,  total acc: 70.12%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 70.35%   [EVAL] batch:  133 | acc: 87.50%,  total acc: 70.48%   [EVAL] batch:  134 | acc: 81.25%,  total acc: 70.56%   [EVAL] batch:  135 | acc: 93.75%,  total acc: 70.73%   [EVAL] batch:  136 | acc: 100.00%,  total acc: 70.94%   [EVAL] batch:  137 | acc: 87.50%,  total acc: 71.06%   [EVAL] batch:  138 | acc: 81.25%,  total acc: 71.13%   [EVAL] batch:  139 | acc: 68.75%,  total acc: 71.12%   [EVAL] batch:  140 | acc: 75.00%,  total acc: 71.14%   [EVAL] batch:  141 | acc: 75.00%,  total acc: 71.17%   [EVAL] batch:  142 | acc: 75.00%,  total acc: 71.20%   [EVAL] batch:  143 | acc: 87.50%,  total acc: 71.31%   [EVAL] batch:  144 | acc: 62.50%,  total acc: 71.25%   [EVAL] batch:  145 | acc: 81.25%,  total acc: 71.32%   [EVAL] batch:  146 | acc: 68.75%,  total acc: 71.30%   [EVAL] batch:  147 | acc: 81.25%,  total acc: 71.37%   [EVAL] batch:  148 | acc: 75.00%,  total acc: 71.39%   [EVAL] batch:  149 | acc: 81.25%,  total acc: 71.46%   [EVAL] batch:  150 | acc: 81.25%,  total acc: 71.52%   [EVAL] batch:  151 | acc: 87.50%,  total acc: 71.63%   [EVAL] batch:  152 | acc: 81.25%,  total acc: 71.69%   [EVAL] batch:  153 | acc: 93.75%,  total acc: 71.83%   [EVAL] batch:  154 | acc: 68.75%,  total acc: 71.81%   [EVAL] batch:  155 | acc: 62.50%,  total acc: 71.75%   [EVAL] batch:  156 | acc: 43.75%,  total acc: 71.58%   [EVAL] batch:  157 | acc: 50.00%,  total acc: 71.44%   [EVAL] batch:  158 | acc: 50.00%,  total acc: 71.31%   [EVAL] batch:  159 | acc: 43.75%,  total acc: 71.13%   [EVAL] batch:  160 | acc: 56.25%,  total acc: 71.04%   [EVAL] batch:  161 | acc: 56.25%,  total acc: 70.95%   [EVAL] batch:  162 | acc: 62.50%,  total acc: 70.90%   [EVAL] batch:  163 | acc: 68.75%,  total acc: 70.88%   [EVAL] batch:  164 | acc: 81.25%,  total acc: 70.95%   [EVAL] batch:  165 | acc: 81.25%,  total acc: 71.01%   [EVAL] batch:  166 | acc: 75.00%,  total acc: 71.03%   [EVAL] batch:  167 | acc: 87.50%,  total acc: 71.13%   [EVAL] batch:  168 | acc: 81.25%,  total acc: 71.19%   [EVAL] batch:  169 | acc: 43.75%,  total acc: 71.03%   [EVAL] batch:  170 | acc: 31.25%,  total acc: 70.80%   [EVAL] batch:  171 | acc: 18.75%,  total acc: 70.49%   [EVAL] batch:  172 | acc: 56.25%,  total acc: 70.41%   [EVAL] batch:  173 | acc: 31.25%,  total acc: 70.19%   [EVAL] batch:  174 | acc: 43.75%,  total acc: 70.04%   [EVAL] batch:  175 | acc: 100.00%,  total acc: 70.21%   [EVAL] batch:  176 | acc: 100.00%,  total acc: 70.37%   [EVAL] batch:  177 | acc: 100.00%,  total acc: 70.54%   [EVAL] batch:  178 | acc: 100.00%,  total acc: 70.71%   [EVAL] batch:  179 | acc: 100.00%,  total acc: 70.87%   [EVAL] batch:  180 | acc: 100.00%,  total acc: 71.03%   [EVAL] batch:  181 | acc: 100.00%,  total acc: 71.19%   [EVAL] batch:  182 | acc: 75.00%,  total acc: 71.21%   [EVAL] batch:  183 | acc: 100.00%,  total acc: 71.37%   [EVAL] batch:  184 | acc: 93.75%,  total acc: 71.49%   [EVAL] batch:  185 | acc: 100.00%,  total acc: 71.64%   [EVAL] batch:  186 | acc: 87.50%,  total acc: 71.72%   [EVAL] batch:  187 | acc: 62.50%,  total acc: 71.68%   [EVAL] batch:  188 | acc: 31.25%,  total acc: 71.46%   [EVAL] batch:  189 | acc: 37.50%,  total acc: 71.28%   [EVAL] batch:  190 | acc: 25.00%,  total acc: 71.04%   [EVAL] batch:  191 | acc: 50.00%,  total acc: 70.93%   [EVAL] batch:  192 | acc: 56.25%,  total acc: 70.85%   [EVAL] batch:  193 | acc: 37.50%,  total acc: 70.68%   [EVAL] batch:  194 | acc: 68.75%,  total acc: 70.67%   [EVAL] batch:  195 | acc: 68.75%,  total acc: 70.66%   [EVAL] batch:  196 | acc: 43.75%,  total acc: 70.53%   [EVAL] batch:  197 | acc: 56.25%,  total acc: 70.45%   [EVAL] batch:  198 | acc: 43.75%,  total acc: 70.32%   [EVAL] batch:  199 | acc: 75.00%,  total acc: 70.34%   [EVAL] batch:  200 | acc: 87.50%,  total acc: 70.43%   [EVAL] batch:  201 | acc: 75.00%,  total acc: 70.45%   [EVAL] batch:  202 | acc: 62.50%,  total acc: 70.41%   [EVAL] batch:  203 | acc: 75.00%,  total acc: 70.44%   [EVAL] batch:  204 | acc: 68.75%,  total acc: 70.43%   [EVAL] batch:  205 | acc: 100.00%,  total acc: 70.57%   [EVAL] batch:  206 | acc: 31.25%,  total acc: 70.38%   [EVAL] batch:  207 | acc: 25.00%,  total acc: 70.16%   [EVAL] batch:  208 | acc: 43.75%,  total acc: 70.04%   [EVAL] batch:  209 | acc: 6.25%,  total acc: 69.73%   [EVAL] batch:  210 | acc: 18.75%,  total acc: 69.49%   [EVAL] batch:  211 | acc: 37.50%,  total acc: 69.34%   [EVAL] batch:  212 | acc: 43.75%,  total acc: 69.22%   [EVAL] batch:  213 | acc: 93.75%,  total acc: 69.33%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 69.48%   [EVAL] batch:  215 | acc: 75.00%,  total acc: 69.50%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 69.64%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 69.78%   [EVAL] batch:  218 | acc: 93.75%,  total acc: 69.89%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 70.03%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 70.16%   [EVAL] batch:  221 | acc: 93.75%,  total acc: 70.27%   [EVAL] batch:  222 | acc: 100.00%,  total acc: 70.40%   [EVAL] batch:  223 | acc: 100.00%,  total acc: 70.54%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 70.67%   [EVAL] batch:  225 | acc: 68.75%,  total acc: 70.66%   [EVAL] batch:  226 | acc: 75.00%,  total acc: 70.68%   [EVAL] batch:  227 | acc: 75.00%,  total acc: 70.70%   [EVAL] batch:  228 | acc: 62.50%,  total acc: 70.66%   [EVAL] batch:  229 | acc: 93.75%,  total acc: 70.76%   [EVAL] batch:  230 | acc: 68.75%,  total acc: 70.75%   [EVAL] batch:  231 | acc: 56.25%,  total acc: 70.69%   [EVAL] batch:  232 | acc: 31.25%,  total acc: 70.52%   [EVAL] batch:  233 | acc: 50.00%,  total acc: 70.43%   [EVAL] batch:  234 | acc: 18.75%,  total acc: 70.21%   [EVAL] batch:  235 | acc: 56.25%,  total acc: 70.15%   [EVAL] batch:  236 | acc: 37.50%,  total acc: 70.02%   [EVAL] batch:  237 | acc: 68.75%,  total acc: 70.01%   [EVAL] batch:  238 | acc: 81.25%,  total acc: 70.06%   [EVAL] batch:  239 | acc: 100.00%,  total acc: 70.18%   [EVAL] batch:  240 | acc: 81.25%,  total acc: 70.23%   [EVAL] batch:  241 | acc: 87.50%,  total acc: 70.30%   [EVAL] batch:  242 | acc: 81.25%,  total acc: 70.34%   [EVAL] batch:  243 | acc: 81.25%,  total acc: 70.39%   [EVAL] batch:  244 | acc: 87.50%,  total acc: 70.46%   [EVAL] batch:  245 | acc: 87.50%,  total acc: 70.53%   [EVAL] batch:  246 | acc: 56.25%,  total acc: 70.47%   [EVAL] batch:  247 | acc: 75.00%,  total acc: 70.49%   [EVAL] batch:  248 | acc: 68.75%,  total acc: 70.48%   [EVAL] batch:  249 | acc: 81.25%,  total acc: 70.53%   [EVAL] batch:  250 | acc: 50.00%,  total acc: 70.44%   [EVAL] batch:  251 | acc: 75.00%,  total acc: 70.46%   [EVAL] batch:  252 | acc: 75.00%,  total acc: 70.48%   [EVAL] batch:  253 | acc: 62.50%,  total acc: 70.45%   [EVAL] batch:  254 | acc: 75.00%,  total acc: 70.47%   [EVAL] batch:  255 | acc: 75.00%,  total acc: 70.48%   [EVAL] batch:  256 | acc: 62.50%,  total acc: 70.45%   [EVAL] batch:  257 | acc: 62.50%,  total acc: 70.42%   [EVAL] batch:  258 | acc: 68.75%,  total acc: 70.42%   [EVAL] batch:  259 | acc: 68.75%,  total acc: 70.41%   [EVAL] batch:  260 | acc: 50.00%,  total acc: 70.33%   [EVAL] batch:  261 | acc: 75.00%,  total acc: 70.35%   [EVAL] batch:  262 | acc: 93.75%,  total acc: 70.44%   [EVAL] batch:  263 | acc: 93.75%,  total acc: 70.53%   [EVAL] batch:  264 | acc: 87.50%,  total acc: 70.59%   [EVAL] batch:  265 | acc: 87.50%,  total acc: 70.65%   [EVAL] batch:  266 | acc: 100.00%,  total acc: 70.76%   [EVAL] batch:  267 | acc: 100.00%,  total acc: 70.87%   [EVAL] batch:  268 | acc: 93.75%,  total acc: 70.96%   [EVAL] batch:  269 | acc: 87.50%,  total acc: 71.02%   [EVAL] batch:  270 | acc: 100.00%,  total acc: 71.13%   [EVAL] batch:  271 | acc: 100.00%,  total acc: 71.23%   [EVAL] batch:  272 | acc: 100.00%,  total acc: 71.34%   [EVAL] batch:  273 | acc: 87.50%,  total acc: 71.40%   [EVAL] batch:  274 | acc: 100.00%,  total acc: 71.50%   [EVAL] batch:  275 | acc: 62.50%,  total acc: 71.47%   [EVAL] batch:  276 | acc: 50.00%,  total acc: 71.39%   [EVAL] batch:  277 | acc: 68.75%,  total acc: 71.38%   [EVAL] batch:  278 | acc: 68.75%,  total acc: 71.37%   [EVAL] batch:  279 | acc: 50.00%,  total acc: 71.29%   [EVAL] batch:  280 | acc: 56.25%,  total acc: 71.24%   [EVAL] batch:  281 | acc: 62.50%,  total acc: 71.21%   [EVAL] batch:  282 | acc: 62.50%,  total acc: 71.18%   [EVAL] batch:  283 | acc: 12.50%,  total acc: 70.97%   [EVAL] batch:  284 | acc: 37.50%,  total acc: 70.86%   [EVAL] batch:  285 | acc: 12.50%,  total acc: 70.65%   [EVAL] batch:  286 | acc: 12.50%,  total acc: 70.45%   [EVAL] batch:  287 | acc: 68.75%,  total acc: 70.44%   [EVAL] batch:  288 | acc: 68.75%,  total acc: 70.44%   [EVAL] batch:  289 | acc: 87.50%,  total acc: 70.50%   [EVAL] batch:  290 | acc: 56.25%,  total acc: 70.45%   [EVAL] batch:  291 | acc: 87.50%,  total acc: 70.51%   [EVAL] batch:  292 | acc: 81.25%,  total acc: 70.54%   [EVAL] batch:  293 | acc: 75.00%,  total acc: 70.56%   [EVAL] batch:  294 | acc: 56.25%,  total acc: 70.51%   [EVAL] batch:  295 | acc: 68.75%,  total acc: 70.50%   [EVAL] batch:  296 | acc: 68.75%,  total acc: 70.50%   [EVAL] batch:  297 | acc: 56.25%,  total acc: 70.45%   [EVAL] batch:  298 | acc: 62.50%,  total acc: 70.42%   [EVAL] batch:  299 | acc: 75.00%,  total acc: 70.44%   [EVAL] batch:  300 | acc: 100.00%,  total acc: 70.54%   [EVAL] batch:  301 | acc: 100.00%,  total acc: 70.63%   [EVAL] batch:  302 | acc: 93.75%,  total acc: 70.71%   [EVAL] batch:  303 | acc: 100.00%,  total acc: 70.81%   [EVAL] batch:  304 | acc: 100.00%,  total acc: 70.90%   [EVAL] batch:  305 | acc: 100.00%,  total acc: 71.00%   [EVAL] batch:  306 | acc: 100.00%,  total acc: 71.09%   [EVAL] batch:  307 | acc: 93.75%,  total acc: 71.16%   [EVAL] batch:  308 | acc: 93.75%,  total acc: 71.24%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 71.33%   [EVAL] batch:  310 | acc: 100.00%,  total acc: 71.42%   [EVAL] batch:  311 | acc: 100.00%,  total acc: 71.51%   [EVAL] batch:  312 | acc: 50.00%,  total acc: 71.45%   
cur_acc:  ['0.9494', '0.6151', '0.7143', '0.6885', '0.7510']
his_acc:  ['0.9494', '0.7760', '0.7500', '0.7332', '0.7145']
Clustering into  29  clusters
Clusters:  [25 10 12  0 18 22 12 17 10 16 18 26  3 27  4 23  3 10  3 10  2  6 17 18
 21 16  7 22  1 20 24  3 17 26 15 14  5  2  0  2  8 28  5 12  6 19 10  1
  9  7  7 26  4  9  4  8  3  7 13 11]
Losses:  10.482998847961426 2.8842639923095703 0.6028146147727966
CurrentTrain: epoch  0, batch     0 | loss: 10.4829988Losses:  10.350621223449707 3.280076026916504 0.6297920942306519
CurrentTrain: epoch  0, batch     1 | loss: 10.3506212Losses:  10.564905166625977 3.870370626449585 0.6570059657096863
CurrentTrain: epoch  0, batch     2 | loss: 10.5649052Losses:  7.366638660430908 -0.0 0.11640386283397675
CurrentTrain: epoch  0, batch     3 | loss: 7.3666387Losses:  12.599874496459961 4.796733856201172 0.6610040664672852
CurrentTrain: epoch  1, batch     0 | loss: 12.5998745Losses:  8.452130317687988 2.4578511714935303 0.6821898221969604
CurrentTrain: epoch  1, batch     1 | loss: 8.4521303Losses:  8.331111907958984 3.2768301963806152 0.5453873872756958
CurrentTrain: epoch  1, batch     2 | loss: 8.3311119Losses:  3.9714999198913574 -0.0 -0.0
CurrentTrain: epoch  1, batch     3 | loss: 3.9714999Losses:  8.207170486450195 2.714776039123535 0.6587817668914795
CurrentTrain: epoch  2, batch     0 | loss: 8.2071705Losses:  8.9378662109375 3.382204055786133 0.670265793800354
CurrentTrain: epoch  2, batch     1 | loss: 8.9378662Losses:  11.819086074829102 5.3326005935668945 0.6396920680999756
CurrentTrain: epoch  2, batch     2 | loss: 11.8190861Losses:  3.615539073944092 -0.0 0.10067687928676605
CurrentTrain: epoch  2, batch     3 | loss: 3.6155391Losses:  7.59883975982666 2.6658987998962402 0.6467458009719849
CurrentTrain: epoch  3, batch     0 | loss: 7.5988398Losses:  7.932482719421387 2.4552226066589355 0.6546379327774048
CurrentTrain: epoch  3, batch     1 | loss: 7.9324827Losses:  8.457639694213867 3.1000518798828125 0.6029837727546692
CurrentTrain: epoch  3, batch     2 | loss: 8.4576397Losses:  7.42976713180542 -0.0 0.12944668531417847
CurrentTrain: epoch  3, batch     3 | loss: 7.4297671Losses:  8.265740394592285 3.250311851501465 0.6475130319595337
CurrentTrain: epoch  4, batch     0 | loss: 8.2657404Losses:  8.289799690246582 3.393460273742676 0.4983404874801636
CurrentTrain: epoch  4, batch     1 | loss: 8.2897997Losses:  8.456010818481445 3.0397701263427734 0.6856772899627686
CurrentTrain: epoch  4, batch     2 | loss: 8.4560108Losses:  3.7706382274627686 -0.0 0.12425150722265244
CurrentTrain: epoch  4, batch     3 | loss: 3.7706382Losses:  7.731500625610352 2.283900737762451 0.6602441072463989
CurrentTrain: epoch  5, batch     0 | loss: 7.7315006Losses:  7.388925552368164 2.99101185798645 0.5679116249084473
CurrentTrain: epoch  5, batch     1 | loss: 7.3889256Losses:  8.602144241333008 4.28297233581543 0.5183770060539246
CurrentTrain: epoch  5, batch     2 | loss: 8.6021442Losses:  2.8029699325561523 -0.0 0.14435170590877533
CurrentTrain: epoch  5, batch     3 | loss: 2.8029699Losses:  6.738543510437012 2.303464889526367 0.6574288606643677
CurrentTrain: epoch  6, batch     0 | loss: 6.7385435Losses:  6.894257545471191 2.5279390811920166 0.644460916519165
CurrentTrain: epoch  6, batch     1 | loss: 6.8942575Losses:  7.623928070068359 3.6094107627868652 0.49185141921043396
CurrentTrain: epoch  6, batch     2 | loss: 7.6239281Losses:  4.510274410247803 -0.0 0.13640911877155304
CurrentTrain: epoch  6, batch     3 | loss: 4.5102744Losses:  6.357731819152832 2.7525038719177246 0.5835368633270264
CurrentTrain: epoch  7, batch     0 | loss: 6.3577318Losses:  7.0741119384765625 3.2539048194885254 0.5742795467376709
CurrentTrain: epoch  7, batch     1 | loss: 7.0741119Losses:  8.238828659057617 3.6709890365600586 0.5270915031433105
CurrentTrain: epoch  7, batch     2 | loss: 8.2388287Losses:  4.93389892578125 -0.0 0.1329309642314911
CurrentTrain: epoch  7, batch     3 | loss: 4.9338989Losses:  7.653555870056152 3.4077444076538086 0.5846506953239441
CurrentTrain: epoch  8, batch     0 | loss: 7.6535559Losses:  5.801632881164551 1.9815441370010376 0.6318143010139465
CurrentTrain: epoch  8, batch     1 | loss: 5.8016329Losses:  6.271272659301758 2.4804277420043945 0.5782749056816101
CurrentTrain: epoch  8, batch     2 | loss: 6.2712727Losses:  3.3093273639678955 -0.0 0.08820076286792755
CurrentTrain: epoch  8, batch     3 | loss: 3.3093274Losses:  8.155964851379395 5.534976959228516 0.5554089546203613
CurrentTrain: epoch  9, batch     0 | loss: 8.1559649Losses:  7.150156021118164 3.1451830863952637 0.508358895778656
CurrentTrain: epoch  9, batch     1 | loss: 7.1501560Losses:  7.77815055847168 3.4372806549072266 0.5932385921478271
CurrentTrain: epoch  9, batch     2 | loss: 7.7781506Losses:  3.774151563644409 -0.0 0.15693829953670502
CurrentTrain: epoch  9, batch     3 | loss: 3.7741516
#### main subject ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "main subject" means primary topic of a work.
Here is an example:
Relation: main subject
Context: the " tetrabiblos " was largely responsible for laying down the basic precepts of renaissance astrology , webster ( 1979 ) p.276 .
Head Entity: tetrabiblos
Tail Entity: astrology
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "main subject":
Relation: main subject  
Context: In her groundbreaking book, "The Second Sex," Simone de Beauvoir explores the role of women in society and the concept of femininity, which has influenced feminist theory significantly.  
Head Entity: The Second Sex  
Tail Entity: feminist theory  

Relation: main subject  
Context: The documentary "Planet Earth" showcases the beauty and diversity of the natural world, focusing on various ecosystems and wildlife, which has sparked interest in environmental conservation.  
Head Entity: Planet Earth  
Tail Entity: environmental conservation  
#### participating team ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "participating team" means teams that actively takes/took part in an event or process.
Here is an example:
Relation: participating team
Context: he represented romania at euro 2000 , his first major tournament where they beat england 3–2 , and drew 1–1 against germany .
Head Entity: euro 2000
Tail Entity: germany
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "participating team":
Relation: participating team  
Context: The United States women's national soccer team has consistently performed well in international competitions, winning the FIFA Women's World Cup multiple times, including in 2019, where they faced the Netherlands in the final.  
Head Entity: FIFA Women's World Cup  
Tail Entity: Netherlands  

Relation: participating team  
Context: During the 2022 Winter Olympics, the Canadian ice hockey team showcased their skills, ultimately defeating the Russian Olympic Committee in a thrilling final match.  
Head Entity: 2022 Winter Olympics  
Tail Entity: Russian Olympic Committee  
#### has part ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "has part" means part of this subject.
Here is an example:
Relation: has part
Context: among the competing artists was previous eurovision song contest entrant robin juhkental who represented estonia in 2010 as part of the group malcolm lincoln .
Head Entity: malcolm lincoln
Tail Entity: robin juhkental
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "has part":
Relation: has part  
Context: The famous painting "The Last Supper" features several apostles, including Peter, who is depicted prominently on the left side of the artwork.  
Head Entity: The Last Supper  
Tail Entity: Peter  

Relation: has part  
Context: The smartphone model Galaxy S21 comes with a high-resolution camera that includes a wide-angle lens, allowing users to capture stunning landscapes.  
Head Entity: Galaxy S21  
Tail Entity: wide-angle lens  
#### nominated for ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "nominated for" means award nomination received by a person, organisation or creative work.
Here is an example:
Relation: nominated for
Context: for her performance in the musical - drama " crazy heart " ( 2009 ) , she was nominated for the academy award for best supporting actress .
Head Entity: crazy heart
Tail Entity: academy award for best supporting actress
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "nominated for":
Relation: nominated for  
Context: The film "The Shape of Water" received critical acclaim and was nominated for several prestigious awards, including the Academy Award for Best Picture.  
Head Entity: The Shape of Water  
Tail Entity: Academy Award for Best Picture  

Relation: nominated for  
Context: In 2021, the popular series "The Crown" was nominated for multiple Emmy Awards, showcasing its excellence in television production.  
Head Entity: The Crown  
Tail Entity: Emmy Awards  
#### instance of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "instance of" means that class of which this subject is a particular example and member.
Here is an example:
Relation: instance of
Context: in 2007 , the county municipality created the transit authority called " skyss " that would market public transport while is would be operated by private companies based on public service obligation contracts .
Head Entity: skyss
Tail Entity: transit authority
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "instance of":
Relation: instance of  
Context: The smartphone known as the iPhone was first released by Apple Inc. in 2007, revolutionizing the mobile phone industry and setting new standards for design and functionality.  
Head Entity: iPhone  
Tail Entity: smartphone  

Relation: instance of  
Context: The Great Wall of China is a series of fortifications made of various materials, built to protect the Chinese states from invasions and raids, and is considered one of the most iconic structures in the world.  
Head Entity: Great Wall of China  
Tail Entity: fortification
#### mountain range ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "mountain range" means range or subrange to which the geographical item belongs.
Here is an example:
Relation: mountain range
Context: the mount kapaz or kepez ( ) is a mountain in lesser caucasus near ganja city in central azerbaijan .
Head Entity: mount kapaz
Tail Entity: lesser caucasus
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "mountain range":
Relation: mountain range  
Context: the sierra nevada is a mountain range in the western united states, primarily located in the state of california.  
Head Entity: sierra nevada  
Tail Entity: california  

Relation: mountain range  
Context: the alps are a major mountain range in europe, stretching across eight countries including france, germany, and italy.  
Head Entity: alps  
Tail Entity: europe  
#### screenwriter ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "screenwriter" means person who wrote the script for subject item.
Here is an example:
Relation: screenwriter
Context: later that year he had a minor hit film with " landru " , written by françoise sagan and starring charles denner , michèle morgan , danielle darrieux and hildegard knef .
Head Entity: " landru "
Tail Entity: françoise sagan
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "screenwriter":
Relation: screenwriter  
Context: The acclaimed film "Inception," directed by Christopher Nolan, features a complex narrative crafted by the talented screenwriter, who is none other than Nolan himself.  
Head Entity: "Inception"  
Tail Entity: Christopher Nolan  

Relation: screenwriter  
Context: The beloved animated movie "Toy Story" was brought to life through the imaginative script penned by the brilliant screenwriter, Joss Whedon, along with his team.  
Head Entity: "Toy Story"  
Tail Entity: Joss Whedon  
#### language of work or name ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "language of work or name" means language associated with this creative work (such as books, shows, songs, broadcasts or websites) or a name.
Here is an example:
Relation: language of work or name
Context: the publisher found success in 1993 with tarō gomi 's " everyone poops " , a japanese language import that sold over one million copies and is now their best - known title .
Head Entity: everyone poops
Tail Entity: japanese language
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "language of work or name":
Relation: language of work or name  
Context: The animated series "Avatar: The Last Airbender" features a rich narrative and is primarily produced in English, appealing to a global audience.  
Head Entity: Avatar: The Last Airbender  
Tail Entity: English  

Relation: language of work or name  
Context: The famous novel "Cien años de soledad" by Gabriel García Márquez is celebrated for its magical realism and is originally written in Spanish, captivating readers worldwide.  
Head Entity: Cien años de soledad  
Tail Entity: Spanish  
#### operator ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "operator" means person, profession, or organization that operates the equipment, facility, or service.
Here is an example:
Relation: operator
Context: it hosts both freight traffic and metra 's union pacific / northwest line commuter rail service .
Head Entity: union pacific / northwest line
Tail Entity: metra
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "operator":
Relation: operator  
Context: The city council has contracted with GreenTech Solutions to manage the waste disposal services in the area.  
Head Entity: GreenTech Solutions  
Tail Entity: city council  

Relation: operator  
Context: The airline has partnered with SkyHigh Services to provide ground handling at the international airport.  
Head Entity: SkyHigh Services  
Tail Entity: airline  
#### religion ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "religion" means religion of a person, organization or religious building, or associated with this subject.
Here is an example:
Relation: religion
Context: the most important lama of this series was the third changkya , rolpai dorje , who was preceptor to the qianlong emperor of china , and chief representative of tibetan buddhism at the qing court .
Head Entity: rolpai dorje
Tail Entity: tibetan buddhism
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "religion":
Relation: religion  
Context: the cathedral of notre-dame is a famous example of french gothic architecture and serves as a major center for the roman catholic faith in paris.  
Head Entity: cathedral of notre-dame  
Tail Entity: roman catholic faith  

Relation: religion  
Context: the dalai lama is a prominent figure in the gelug school of tibetan buddhism, advocating for peace and compassion worldwide.  
Head Entity: dalai lama  
Tail Entity: tibetan buddhism  
Losses:  4.3538360595703125 -0.0 0.9536164999008179
MemoryTrain:  epoch  0, batch     0 | loss: 4.3538361Losses:  3.444861888885498 0.565475344657898 0.929084300994873
MemoryTrain:  epoch  0, batch     1 | loss: 3.4448619Losses:  4.524027347564697 0.9202272891998291 0.9086670279502869
MemoryTrain:  epoch  0, batch     2 | loss: 4.5240273Losses:  4.2083024978637695 0.2395121455192566 0.7861738801002502
MemoryTrain:  epoch  0, batch     3 | loss: 4.2083025Losses:  4.495420932769775 0.2665208578109741 0.8544096946716309
MemoryTrain:  epoch  0, batch     4 | loss: 4.4954209Losses:  4.678830146789551 0.7685117721557617 0.8341111540794373
MemoryTrain:  epoch  0, batch     5 | loss: 4.6788301Losses:  4.439879894256592 0.25905850529670715 0.9737699031829834
MemoryTrain:  epoch  0, batch     6 | loss: 4.4398799Losses:  4.83210563659668 0.5220575332641602 0.9340462684631348
MemoryTrain:  epoch  0, batch     7 | loss: 4.8321056Losses:  4.583573341369629 0.5905522704124451 0.8814897537231445
MemoryTrain:  epoch  0, batch     8 | loss: 4.5835733Losses:  4.255332946777344 0.4947948753833771 0.9550255537033081
MemoryTrain:  epoch  0, batch     9 | loss: 4.2553329Losses:  4.899631023406982 0.2478456199169159 0.9451013803482056
MemoryTrain:  epoch  0, batch    10 | loss: 4.8996310Losses:  3.832294225692749 -0.0 0.20573703944683075
MemoryTrain:  epoch  0, batch    11 | loss: 3.8322942Losses:  4.0589776039123535 0.7466483116149902 0.8865399360656738
MemoryTrain:  epoch  1, batch     0 | loss: 4.0589776Losses:  4.303014278411865 -0.0 0.9820237755775452
MemoryTrain:  epoch  1, batch     1 | loss: 4.3030143Losses:  4.063114643096924 -0.0 0.9901466965675354
MemoryTrain:  epoch  1, batch     2 | loss: 4.0631146Losses:  4.1393351554870605 0.47273725271224976 0.8795170783996582
MemoryTrain:  epoch  1, batch     3 | loss: 4.1393352Losses:  3.7780497074127197 0.5519469976425171 0.9796969890594482
MemoryTrain:  epoch  1, batch     4 | loss: 3.7780497Losses:  4.047422885894775 0.5140869617462158 0.9609493017196655
MemoryTrain:  epoch  1, batch     5 | loss: 4.0474229Losses:  3.5727109909057617 -0.0 0.9147168397903442
MemoryTrain:  epoch  1, batch     6 | loss: 3.5727110Losses:  3.807617425918579 0.22854381799697876 0.7973573207855225
MemoryTrain:  epoch  1, batch     7 | loss: 3.8076174Losses:  3.9755301475524902 0.25124260783195496 0.8525427579879761
MemoryTrain:  epoch  1, batch     8 | loss: 3.9755301Losses:  3.506016731262207 -0.0 0.9593650102615356
MemoryTrain:  epoch  1, batch     9 | loss: 3.5060167Losses:  3.418734073638916 0.49600160121917725 0.8816594481468201
MemoryTrain:  epoch  1, batch    10 | loss: 3.4187341Losses:  2.4285762310028076 -0.0 0.33118104934692383
MemoryTrain:  epoch  1, batch    11 | loss: 2.4285762Losses:  3.368586540222168 0.22882241010665894 1.0298303365707397
MemoryTrain:  epoch  2, batch     0 | loss: 3.3685865Losses:  3.9366962909698486 0.4841079115867615 1.0025990009307861
MemoryTrain:  epoch  2, batch     1 | loss: 3.9366963Losses:  3.851755380630493 0.4926804304122925 0.971447765827179
MemoryTrain:  epoch  2, batch     2 | loss: 3.8517554Losses:  3.605489730834961 0.2852790653705597 0.8692282438278198
MemoryTrain:  epoch  2, batch     3 | loss: 3.6054897Losses:  3.3166775703430176 -0.0 0.9444860816001892
MemoryTrain:  epoch  2, batch     4 | loss: 3.3166776Losses:  3.9642891883850098 0.28739064931869507 0.9715673327445984
MemoryTrain:  epoch  2, batch     5 | loss: 3.9642892Losses:  2.9281373023986816 0.25517040491104126 0.9545893669128418
MemoryTrain:  epoch  2, batch     6 | loss: 2.9281373Losses:  3.5086019039154053 0.47054868936538696 0.9028207659721375
MemoryTrain:  epoch  2, batch     7 | loss: 3.5086019Losses:  3.314713954925537 0.2629167437553406 0.9435638189315796
MemoryTrain:  epoch  2, batch     8 | loss: 3.3147140Losses:  3.2560012340545654 0.2620497941970825 0.9098812937736511
MemoryTrain:  epoch  2, batch     9 | loss: 3.2560012Losses:  3.1727333068847656 0.24310702085494995 0.8624602556228638
MemoryTrain:  epoch  2, batch    10 | loss: 3.1727333Losses:  2.879436492919922 -0.0 0.3485884368419647
MemoryTrain:  epoch  2, batch    11 | loss: 2.8794365Losses:  3.2530581951141357 0.2690374255180359 0.9584150910377502
MemoryTrain:  epoch  3, batch     0 | loss: 3.2530582Losses:  3.114708423614502 0.27040213346481323 0.9224269390106201
MemoryTrain:  epoch  3, batch     1 | loss: 3.1147084Losses:  2.896247625350952 0.27344560623168945 1.0150933265686035
MemoryTrain:  epoch  3, batch     2 | loss: 2.8962476Losses:  3.8886921405792236 0.322475790977478 0.9269315600395203
MemoryTrain:  epoch  3, batch     3 | loss: 3.8886921Losses:  3.2553870677948 0.5260872840881348 0.8641629219055176
MemoryTrain:  epoch  3, batch     4 | loss: 3.2553871Losses:  3.1926162242889404 -0.0 0.9246088862419128
MemoryTrain:  epoch  3, batch     5 | loss: 3.1926162Losses:  3.735520839691162 1.1457231044769287 0.7735277414321899
MemoryTrain:  epoch  3, batch     6 | loss: 3.7355208Losses:  3.7123804092407227 0.22702831029891968 0.9211864471435547
MemoryTrain:  epoch  3, batch     7 | loss: 3.7123804Losses:  2.7467453479766846 -0.0 0.9830272793769836
MemoryTrain:  epoch  3, batch     8 | loss: 2.7467453Losses:  3.5441150665283203 0.2871227264404297 0.9330496788024902
MemoryTrain:  epoch  3, batch     9 | loss: 3.5441151Losses:  2.7622716426849365 0.23658521473407745 0.8794211149215698
MemoryTrain:  epoch  3, batch    10 | loss: 2.7622716Losses:  1.9363431930541992 -0.0 0.38629692792892456
MemoryTrain:  epoch  3, batch    11 | loss: 1.9363432Losses:  3.246338367462158 0.4965485632419586 0.9111830592155457
MemoryTrain:  epoch  4, batch     0 | loss: 3.2463384Losses:  3.9206223487854004 0.578932523727417 0.9091222286224365
MemoryTrain:  epoch  4, batch     1 | loss: 3.9206223Losses:  3.282104253768921 0.25247371196746826 0.9969842433929443
MemoryTrain:  epoch  4, batch     2 | loss: 3.2821043Losses:  3.2658557891845703 0.5244741439819336 0.7971063852310181
MemoryTrain:  epoch  4, batch     3 | loss: 3.2658558Losses:  3.9993927478790283 0.8959746360778809 0.8332025408744812
MemoryTrain:  epoch  4, batch     4 | loss: 3.9993927Losses:  3.093801259994507 -0.0 0.7934123873710632
MemoryTrain:  epoch  4, batch     5 | loss: 3.0938013Losses:  2.9916024208068848 0.2732950747013092 0.9791512489318848
MemoryTrain:  epoch  4, batch     6 | loss: 2.9916024Losses:  2.7372474670410156 -0.0 1.0823132991790771
MemoryTrain:  epoch  4, batch     7 | loss: 2.7372475Losses:  2.2917520999908447 -0.0 0.8558372259140015
MemoryTrain:  epoch  4, batch     8 | loss: 2.2917521Losses:  2.5143942832946777 -0.0 1.0112541913986206
MemoryTrain:  epoch  4, batch     9 | loss: 2.5143943Losses:  3.7419471740722656 0.8876898884773254 0.9599406719207764
MemoryTrain:  epoch  4, batch    10 | loss: 3.7419472Losses:  1.5716521739959717 -0.0 0.3684383034706116
MemoryTrain:  epoch  4, batch    11 | loss: 1.5716522Losses:  2.439199447631836 -0.0 0.8467809557914734
MemoryTrain:  epoch  5, batch     0 | loss: 2.4391994Losses:  2.9159045219421387 -0.0 0.9626664519309998
MemoryTrain:  epoch  5, batch     1 | loss: 2.9159045Losses:  3.250223159790039 -0.0 1.0305529832839966
MemoryTrain:  epoch  5, batch     2 | loss: 3.2502232Losses:  2.9783992767333984 0.24585489928722382 0.9378772974014282
MemoryTrain:  epoch  5, batch     3 | loss: 2.9783993Losses:  2.907236099243164 0.26979488134384155 0.9160014986991882
MemoryTrain:  epoch  5, batch     4 | loss: 2.9072361Losses:  2.8677175045013428 -0.0 1.0248465538024902
MemoryTrain:  epoch  5, batch     5 | loss: 2.8677175Losses:  3.064939498901367 -0.0 1.021216869354248
MemoryTrain:  epoch  5, batch     6 | loss: 3.0649395Losses:  2.5918421745300293 -0.0 0.9099227786064148
MemoryTrain:  epoch  5, batch     7 | loss: 2.5918422Losses:  2.7285919189453125 0.29307159781455994 0.9700887203216553
MemoryTrain:  epoch  5, batch     8 | loss: 2.7285919Losses:  2.233675718307495 -0.0 0.8538849949836731
MemoryTrain:  epoch  5, batch     9 | loss: 2.2336757Losses:  2.7487151622772217 0.49993041157722473 0.9238947629928589
MemoryTrain:  epoch  5, batch    10 | loss: 2.7487152Losses:  2.0705575942993164 -0.0 0.33070582151412964
MemoryTrain:  epoch  5, batch    11 | loss: 2.0705576Losses:  2.2140321731567383 -0.0 0.945463240146637
MemoryTrain:  epoch  6, batch     0 | loss: 2.2140322Losses:  2.757495403289795 -0.0 0.9318269491195679
MemoryTrain:  epoch  6, batch     1 | loss: 2.7574954Losses:  2.989119529724121 0.7581493854522705 0.9145036935806274
MemoryTrain:  epoch  6, batch     2 | loss: 2.9891195Losses:  2.4877800941467285 0.2593437433242798 0.8761981129646301
MemoryTrain:  epoch  6, batch     3 | loss: 2.4877801Losses:  3.316202402114868 0.7387586236000061 0.8556551933288574
MemoryTrain:  epoch  6, batch     4 | loss: 3.3162024Losses:  2.7969932556152344 -0.0 1.0636241436004639
MemoryTrain:  epoch  6, batch     5 | loss: 2.7969933Losses:  3.6448991298675537 0.6498445868492126 0.8665385842323303
MemoryTrain:  epoch  6, batch     6 | loss: 3.6448991Losses:  3.0683369636535645 0.2722247242927551 0.961235523223877
MemoryTrain:  epoch  6, batch     7 | loss: 3.0683370Losses:  2.6416947841644287 -0.0 0.9642907381057739
MemoryTrain:  epoch  6, batch     8 | loss: 2.6416948Losses:  3.178720474243164 0.8132179975509644 0.8392496109008789
MemoryTrain:  epoch  6, batch     9 | loss: 3.1787205Losses:  2.7441654205322266 0.24541640281677246 0.9640353918075562
MemoryTrain:  epoch  6, batch    10 | loss: 2.7441654Losses:  1.621497392654419 -0.0 0.3399682641029358
MemoryTrain:  epoch  6, batch    11 | loss: 1.6214974Losses:  2.585725784301758 -0.0 0.9078812599182129
MemoryTrain:  epoch  7, batch     0 | loss: 2.5857258Losses:  2.751404047012329 0.2628154456615448 0.9637967944145203
MemoryTrain:  epoch  7, batch     1 | loss: 2.7514040Losses:  2.922640562057495 0.2566302418708801 0.8988332152366638
MemoryTrain:  epoch  7, batch     2 | loss: 2.9226406Losses:  2.514707088470459 0.2535557150840759 0.8411210775375366
MemoryTrain:  epoch  7, batch     3 | loss: 2.5147071Losses:  2.890974998474121 0.24755749106407166 0.9076730608940125
MemoryTrain:  epoch  7, batch     4 | loss: 2.8909750Losses:  2.6775572299957275 -0.0 0.8473016619682312
MemoryTrain:  epoch  7, batch     5 | loss: 2.6775572Losses:  2.8057994842529297 0.25693759322166443 1.0231035947799683
MemoryTrain:  epoch  7, batch     6 | loss: 2.8057995Losses:  2.9507768154144287 0.24369990825653076 1.0110145807266235
MemoryTrain:  epoch  7, batch     7 | loss: 2.9507768Losses:  2.966370105743408 0.5054610371589661 0.9858323931694031
MemoryTrain:  epoch  7, batch     8 | loss: 2.9663701Losses:  2.8512802124023438 0.5037293434143066 1.0083584785461426
MemoryTrain:  epoch  7, batch     9 | loss: 2.8512802Losses:  2.6056675910949707 0.2682102918624878 0.9356731176376343
MemoryTrain:  epoch  7, batch    10 | loss: 2.6056676Losses:  1.4850023984909058 -0.0 0.21839170157909393
MemoryTrain:  epoch  7, batch    11 | loss: 1.4850024Losses:  3.7099485397338867 0.7357796430587769 0.9069947004318237
MemoryTrain:  epoch  8, batch     0 | loss: 3.7099485Losses:  2.608564853668213 0.5275208950042725 0.7051180005073547
MemoryTrain:  epoch  8, batch     1 | loss: 2.6085649Losses:  2.2253050804138184 -0.0 0.9444609880447388
MemoryTrain:  epoch  8, batch     2 | loss: 2.2253051Losses:  3.256460666656494 0.9021874666213989 0.8711427450180054
MemoryTrain:  epoch  8, batch     3 | loss: 3.2564607Losses:  2.187302589416504 -0.0 0.9547166228294373
MemoryTrain:  epoch  8, batch     4 | loss: 2.1873026Losses:  2.8973886966705322 0.2653745710849762 1.0175989866256714
MemoryTrain:  epoch  8, batch     5 | loss: 2.8973887Losses:  2.8366734981536865 0.7713932991027832 0.7308829426765442
MemoryTrain:  epoch  8, batch     6 | loss: 2.8366735Losses:  2.232942581176758 -0.0 0.8544176816940308
MemoryTrain:  epoch  8, batch     7 | loss: 2.2329426Losses:  3.4136860370635986 1.1297657489776611 0.9186286330223083
MemoryTrain:  epoch  8, batch     8 | loss: 3.4136860Losses:  3.0954320430755615 0.2554730474948883 0.9737493395805359
MemoryTrain:  epoch  8, batch     9 | loss: 3.0954320Losses:  2.6611781120300293 0.26995450258255005 0.932336688041687
MemoryTrain:  epoch  8, batch    10 | loss: 2.6611781Losses:  1.6242762804031372 0.2658482789993286 0.11167211830615997
MemoryTrain:  epoch  8, batch    11 | loss: 1.6242763Losses:  2.327126979827881 -0.0 0.8669829368591309
MemoryTrain:  epoch  9, batch     0 | loss: 2.3271270Losses:  2.0907230377197266 -0.0 0.8350207805633545
MemoryTrain:  epoch  9, batch     1 | loss: 2.0907230Losses:  2.8179519176483154 0.532071590423584 0.8871647715568542
MemoryTrain:  epoch  9, batch     2 | loss: 2.8179519Losses:  2.868368148803711 0.7172427177429199 0.8894401788711548
MemoryTrain:  epoch  9, batch     3 | loss: 2.8683681Losses:  2.8985631465911865 0.5236630439758301 0.8880059719085693
MemoryTrain:  epoch  9, batch     4 | loss: 2.8985631Losses:  2.3980422019958496 -0.0 0.987540066242218
MemoryTrain:  epoch  9, batch     5 | loss: 2.3980422Losses:  3.181300163269043 1.120537281036377 0.7909342050552368
MemoryTrain:  epoch  9, batch     6 | loss: 3.1813002Losses:  2.8886466026306152 0.5576968193054199 0.9398664832115173
MemoryTrain:  epoch  9, batch     7 | loss: 2.8886466Losses:  3.4115819931030273 0.7052297592163086 0.7665735483169556
MemoryTrain:  epoch  9, batch     8 | loss: 3.4115820Losses:  2.4829213619232178 0.25751635432243347 0.9019660353660583
MemoryTrain:  epoch  9, batch     9 | loss: 2.4829214Losses:  3.1468429565429688 0.7295925617218018 0.8393934965133667
MemoryTrain:  epoch  9, batch    10 | loss: 3.1468430Losses:  1.525862693786621 -0.0 0.31802719831466675
MemoryTrain:  epoch  9, batch    11 | loss: 1.5258627
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 56.25%,  total acc: 65.62%   [EVAL] batch:    2 | acc: 68.75%,  total acc: 66.67%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 70.31%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 73.75%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 73.96%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 75.89%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 78.12%   [EVAL] batch:    8 | acc: 75.00%,  total acc: 77.78%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 76.88%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 78.41%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 78.12%   [EVAL] batch:   12 | acc: 50.00%,  total acc: 75.96%   [EVAL] batch:   13 | acc: 12.50%,  total acc: 71.43%   [EVAL] batch:   14 | acc: 25.00%,  total acc: 68.33%   [EVAL] batch:   15 | acc: 12.50%,  total acc: 64.84%   [EVAL] batch:   16 | acc: 25.00%,  total acc: 62.50%   [EVAL] batch:   17 | acc: 31.25%,  total acc: 60.76%   [EVAL] batch:   18 | acc: 31.25%,  total acc: 59.21%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 60.94%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 62.80%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 64.20%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 65.76%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 67.19%   [EVAL] batch:   24 | acc: 93.75%,  total acc: 68.25%   [EVAL] batch:   25 | acc: 12.50%,  total acc: 66.11%   [EVAL] batch:   26 | acc: 12.50%,  total acc: 64.12%   [EVAL] batch:   27 | acc: 6.25%,  total acc: 62.05%   [EVAL] batch:   28 | acc: 25.00%,  total acc: 60.78%   [EVAL] batch:   29 | acc: 6.25%,  total acc: 58.96%   [EVAL] batch:   30 | acc: 6.25%,  total acc: 57.26%   [EVAL] batch:   31 | acc: 68.75%,  total acc: 57.62%   [EVAL] batch:   32 | acc: 81.25%,  total acc: 58.33%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 59.38%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 60.36%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 61.11%   [EVAL] batch:   36 | acc: 93.75%,  total acc: 61.99%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 62.83%   [EVAL] batch:   38 | acc: 62.50%,  total acc: 62.82%   [EVAL] batch:   39 | acc: 81.25%,  total acc: 63.28%   [EVAL] batch:   40 | acc: 62.50%,  total acc: 63.26%   [EVAL] batch:   41 | acc: 50.00%,  total acc: 62.95%   [EVAL] batch:   42 | acc: 81.25%,  total acc: 63.37%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 63.78%   [EVAL] batch:   44 | acc: 81.25%,  total acc: 64.17%   [EVAL] batch:   45 | acc: 81.25%,  total acc: 64.54%   [EVAL] batch:   46 | acc: 75.00%,  total acc: 64.76%   [EVAL] batch:   47 | acc: 62.50%,  total acc: 64.71%   [EVAL] batch:   48 | acc: 68.75%,  total acc: 64.80%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 65.38%   [EVAL] batch:   50 | acc: 37.50%,  total acc: 64.83%   [EVAL] batch:   51 | acc: 37.50%,  total acc: 64.30%   [EVAL] batch:   52 | acc: 62.50%,  total acc: 64.27%   [EVAL] batch:   53 | acc: 37.50%,  total acc: 63.77%   [EVAL] batch:   54 | acc: 56.25%,  total acc: 63.64%   [EVAL] batch:   55 | acc: 50.00%,  total acc: 63.39%   [EVAL] batch:   56 | acc: 81.25%,  total acc: 63.71%   [EVAL] batch:   57 | acc: 81.25%,  total acc: 64.01%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 64.51%   [EVAL] batch:   59 | acc: 75.00%,  total acc: 64.69%   [EVAL] batch:   60 | acc: 87.50%,  total acc: 65.06%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 65.12%   [EVAL] batch:   62 | acc: 43.75%,  total acc: 64.78%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 78.75%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:    6 | acc: 62.50%,  total acc: 78.57%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 78.91%   [EVAL] batch:    8 | acc: 75.00%,  total acc: 78.47%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 78.75%   [EVAL] batch:   10 | acc: 75.00%,  total acc: 78.41%   [EVAL] batch:   11 | acc: 68.75%,  total acc: 77.60%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 76.92%   [EVAL] batch:   13 | acc: 87.50%,  total acc: 77.68%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 78.75%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 80.08%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 81.94%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 82.57%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 82.50%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 82.74%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 82.95%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 83.42%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 83.85%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 84.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 84.62%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 85.19%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 85.49%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 85.78%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 86.25%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 86.69%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 87.11%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   33 | acc: 68.75%,  total acc: 86.95%   [EVAL] batch:   34 | acc: 87.50%,  total acc: 86.96%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 86.98%   [EVAL] batch:   36 | acc: 75.00%,  total acc: 86.66%   [EVAL] batch:   37 | acc: 81.25%,  total acc: 86.51%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 86.86%   [EVAL] batch:   39 | acc: 93.75%,  total acc: 87.03%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 87.20%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 87.20%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 87.35%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 87.36%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 87.36%   [EVAL] batch:   45 | acc: 87.50%,  total acc: 87.36%   [EVAL] batch:   46 | acc: 62.50%,  total acc: 86.84%   [EVAL] batch:   47 | acc: 68.75%,  total acc: 86.46%   [EVAL] batch:   48 | acc: 81.25%,  total acc: 86.35%   [EVAL] batch:   49 | acc: 68.75%,  total acc: 86.00%   [EVAL] batch:   50 | acc: 81.25%,  total acc: 85.91%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 85.94%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 86.08%   [EVAL] batch:   53 | acc: 75.00%,  total acc: 85.88%   [EVAL] batch:   54 | acc: 62.50%,  total acc: 85.45%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 85.49%   [EVAL] batch:   56 | acc: 81.25%,  total acc: 85.42%   [EVAL] batch:   57 | acc: 68.75%,  total acc: 85.13%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 85.17%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 85.31%   [EVAL] batch:   60 | acc: 81.25%,  total acc: 85.25%   [EVAL] batch:   61 | acc: 75.00%,  total acc: 85.08%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 84.52%   [EVAL] batch:   63 | acc: 0.00%,  total acc: 83.20%   [EVAL] batch:   64 | acc: 0.00%,  total acc: 81.92%   [EVAL] batch:   65 | acc: 0.00%,  total acc: 80.68%   [EVAL] batch:   66 | acc: 0.00%,  total acc: 79.48%   [EVAL] batch:   67 | acc: 0.00%,  total acc: 78.31%   [EVAL] batch:   68 | acc: 31.25%,  total acc: 77.63%   [EVAL] batch:   69 | acc: 75.00%,  total acc: 77.59%   [EVAL] batch:   70 | acc: 93.75%,  total acc: 77.82%   [EVAL] batch:   71 | acc: 93.75%,  total acc: 78.04%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 78.34%   [EVAL] batch:   73 | acc: 75.00%,  total acc: 78.29%   [EVAL] batch:   74 | acc: 81.25%,  total acc: 78.33%   [EVAL] batch:   75 | acc: 75.00%,  total acc: 78.29%   [EVAL] batch:   76 | acc: 50.00%,  total acc: 77.92%   [EVAL] batch:   77 | acc: 75.00%,  total acc: 77.88%   [EVAL] batch:   78 | acc: 62.50%,  total acc: 77.69%   [EVAL] batch:   79 | acc: 56.25%,  total acc: 77.42%   [EVAL] batch:   80 | acc: 62.50%,  total acc: 77.24%   [EVAL] batch:   81 | acc: 12.50%,  total acc: 76.45%   [EVAL] batch:   82 | acc: 25.00%,  total acc: 75.83%   [EVAL] batch:   83 | acc: 12.50%,  total acc: 75.07%   [EVAL] batch:   84 | acc: 31.25%,  total acc: 74.56%   [EVAL] batch:   85 | acc: 31.25%,  total acc: 74.06%   [EVAL] batch:   86 | acc: 37.50%,  total acc: 73.64%   [EVAL] batch:   87 | acc: 12.50%,  total acc: 72.94%   [EVAL] batch:   88 | acc: 25.00%,  total acc: 72.40%   [EVAL] batch:   89 | acc: 12.50%,  total acc: 71.74%   [EVAL] batch:   90 | acc: 6.25%,  total acc: 71.02%   [EVAL] batch:   91 | acc: 12.50%,  total acc: 70.38%   [EVAL] batch:   92 | acc: 6.25%,  total acc: 69.69%   [EVAL] batch:   93 | acc: 18.75%,  total acc: 69.15%   [EVAL] batch:   94 | acc: 75.00%,  total acc: 69.21%   [EVAL] batch:   95 | acc: 87.50%,  total acc: 69.40%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 69.72%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 70.03%   [EVAL] batch:   98 | acc: 81.25%,  total acc: 70.14%   [EVAL] batch:   99 | acc: 68.75%,  total acc: 70.12%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 70.42%   [EVAL] batch:  101 | acc: 93.75%,  total acc: 70.65%   [EVAL] batch:  102 | acc: 87.50%,  total acc: 70.81%   [EVAL] batch:  103 | acc: 93.75%,  total acc: 71.03%   [EVAL] batch:  104 | acc: 87.50%,  total acc: 71.19%   [EVAL] batch:  105 | acc: 93.75%,  total acc: 71.40%   [EVAL] batch:  106 | acc: 31.25%,  total acc: 71.03%   [EVAL] batch:  107 | acc: 18.75%,  total acc: 70.54%   [EVAL] batch:  108 | acc: 6.25%,  total acc: 69.95%   [EVAL] batch:  109 | acc: 12.50%,  total acc: 69.43%   [EVAL] batch:  110 | acc: 12.50%,  total acc: 68.92%   [EVAL] batch:  111 | acc: 6.25%,  total acc: 68.36%   [EVAL] batch:  112 | acc: 25.00%,  total acc: 67.98%   [EVAL] batch:  113 | acc: 50.00%,  total acc: 67.82%   [EVAL] batch:  114 | acc: 56.25%,  total acc: 67.72%   [EVAL] batch:  115 | acc: 87.50%,  total acc: 67.89%   [EVAL] batch:  116 | acc: 56.25%,  total acc: 67.79%   [EVAL] batch:  117 | acc: 68.75%,  total acc: 67.80%   [EVAL] batch:  118 | acc: 62.50%,  total acc: 67.75%   [EVAL] batch:  119 | acc: 50.00%,  total acc: 67.60%   [EVAL] batch:  120 | acc: 37.50%,  total acc: 67.36%   [EVAL] batch:  121 | acc: 50.00%,  total acc: 67.21%   [EVAL] batch:  122 | acc: 62.50%,  total acc: 67.17%   [EVAL] batch:  123 | acc: 81.25%,  total acc: 67.29%   [EVAL] batch:  124 | acc: 68.75%,  total acc: 67.30%   [EVAL] batch:  125 | acc: 81.25%,  total acc: 67.41%   [EVAL] batch:  126 | acc: 56.25%,  total acc: 67.32%   [EVAL] batch:  127 | acc: 56.25%,  total acc: 67.24%   [EVAL] batch:  128 | acc: 37.50%,  total acc: 67.01%   [EVAL] batch:  129 | acc: 56.25%,  total acc: 66.92%   [EVAL] batch:  130 | acc: 87.50%,  total acc: 67.08%   [EVAL] batch:  131 | acc: 93.75%,  total acc: 67.28%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 67.53%   [EVAL] batch:  133 | acc: 93.75%,  total acc: 67.72%   [EVAL] batch:  134 | acc: 81.25%,  total acc: 67.82%   [EVAL] batch:  135 | acc: 93.75%,  total acc: 68.01%   [EVAL] batch:  136 | acc: 100.00%,  total acc: 68.25%   [EVAL] batch:  137 | acc: 100.00%,  total acc: 68.48%   [EVAL] batch:  138 | acc: 81.25%,  total acc: 68.57%   [EVAL] batch:  139 | acc: 68.75%,  total acc: 68.57%   [EVAL] batch:  140 | acc: 75.00%,  total acc: 68.62%   [EVAL] batch:  141 | acc: 75.00%,  total acc: 68.66%   [EVAL] batch:  142 | acc: 75.00%,  total acc: 68.71%   [EVAL] batch:  143 | acc: 81.25%,  total acc: 68.79%   [EVAL] batch:  144 | acc: 62.50%,  total acc: 68.75%   [EVAL] batch:  145 | acc: 81.25%,  total acc: 68.84%   [EVAL] batch:  146 | acc: 50.00%,  total acc: 68.71%   [EVAL] batch:  147 | acc: 81.25%,  total acc: 68.79%   [EVAL] batch:  148 | acc: 68.75%,  total acc: 68.79%   [EVAL] batch:  149 | acc: 68.75%,  total acc: 68.79%   [EVAL] batch:  150 | acc: 75.00%,  total acc: 68.83%   [EVAL] batch:  151 | acc: 68.75%,  total acc: 68.83%   [EVAL] batch:  152 | acc: 68.75%,  total acc: 68.83%   [EVAL] batch:  153 | acc: 81.25%,  total acc: 68.91%   [EVAL] batch:  154 | acc: 75.00%,  total acc: 68.95%   [EVAL] batch:  155 | acc: 68.75%,  total acc: 68.95%   [EVAL] batch:  156 | acc: 56.25%,  total acc: 68.87%   [EVAL] batch:  157 | acc: 50.00%,  total acc: 68.75%   [EVAL] batch:  158 | acc: 50.00%,  total acc: 68.63%   [EVAL] batch:  159 | acc: 50.00%,  total acc: 68.52%   [EVAL] batch:  160 | acc: 50.00%,  total acc: 68.40%   [EVAL] batch:  161 | acc: 56.25%,  total acc: 68.33%   [EVAL] batch:  162 | acc: 62.50%,  total acc: 68.29%   [EVAL] batch:  163 | acc: 62.50%,  total acc: 68.25%   [EVAL] batch:  164 | acc: 75.00%,  total acc: 68.30%   [EVAL] batch:  165 | acc: 68.75%,  total acc: 68.30%   [EVAL] batch:  166 | acc: 75.00%,  total acc: 68.34%   [EVAL] batch:  167 | acc: 68.75%,  total acc: 68.34%   [EVAL] batch:  168 | acc: 81.25%,  total acc: 68.42%   [EVAL] batch:  169 | acc: 43.75%,  total acc: 68.27%   [EVAL] batch:  170 | acc: 43.75%,  total acc: 68.13%   [EVAL] batch:  171 | acc: 25.00%,  total acc: 67.88%   [EVAL] batch:  172 | acc: 56.25%,  total acc: 67.81%   [EVAL] batch:  173 | acc: 31.25%,  total acc: 67.60%   [EVAL] batch:  174 | acc: 43.75%,  total acc: 67.46%   [EVAL] batch:  175 | acc: 100.00%,  total acc: 67.65%   [EVAL] batch:  176 | acc: 100.00%,  total acc: 67.83%   [EVAL] batch:  177 | acc: 100.00%,  total acc: 68.01%   [EVAL] batch:  178 | acc: 100.00%,  total acc: 68.19%   [EVAL] batch:  179 | acc: 100.00%,  total acc: 68.37%   [EVAL] batch:  180 | acc: 100.00%,  total acc: 68.54%   [EVAL] batch:  181 | acc: 100.00%,  total acc: 68.72%   [EVAL] batch:  182 | acc: 68.75%,  total acc: 68.72%   [EVAL] batch:  183 | acc: 87.50%,  total acc: 68.82%   [EVAL] batch:  184 | acc: 93.75%,  total acc: 68.95%   [EVAL] batch:  185 | acc: 93.75%,  total acc: 69.09%   [EVAL] batch:  186 | acc: 81.25%,  total acc: 69.15%   [EVAL] batch:  187 | acc: 43.75%,  total acc: 69.02%   [EVAL] batch:  188 | acc: 18.75%,  total acc: 68.75%   [EVAL] batch:  189 | acc: 18.75%,  total acc: 68.49%   [EVAL] batch:  190 | acc: 25.00%,  total acc: 68.26%   [EVAL] batch:  191 | acc: 50.00%,  total acc: 68.16%   [EVAL] batch:  192 | acc: 50.00%,  total acc: 68.07%   [EVAL] batch:  193 | acc: 31.25%,  total acc: 67.88%   [EVAL] batch:  194 | acc: 68.75%,  total acc: 67.88%   [EVAL] batch:  195 | acc: 56.25%,  total acc: 67.83%   [EVAL] batch:  196 | acc: 43.75%,  total acc: 67.70%   [EVAL] batch:  197 | acc: 37.50%,  total acc: 67.55%   [EVAL] batch:  198 | acc: 50.00%,  total acc: 67.46%   [EVAL] batch:  199 | acc: 75.00%,  total acc: 67.50%   [EVAL] batch:  200 | acc: 75.00%,  total acc: 67.54%   [EVAL] batch:  201 | acc: 75.00%,  total acc: 67.57%   [EVAL] batch:  202 | acc: 62.50%,  total acc: 67.55%   [EVAL] batch:  203 | acc: 68.75%,  total acc: 67.56%   [EVAL] batch:  204 | acc: 62.50%,  total acc: 67.53%   [EVAL] batch:  205 | acc: 93.75%,  total acc: 67.66%   [EVAL] batch:  206 | acc: 25.00%,  total acc: 67.45%   [EVAL] batch:  207 | acc: 18.75%,  total acc: 67.22%   [EVAL] batch:  208 | acc: 37.50%,  total acc: 67.08%   [EVAL] batch:  209 | acc: 6.25%,  total acc: 66.79%   [EVAL] batch:  210 | acc: 25.00%,  total acc: 66.59%   [EVAL] batch:  211 | acc: 31.25%,  total acc: 66.42%   [EVAL] batch:  212 | acc: 37.50%,  total acc: 66.29%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 66.44%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 66.60%   [EVAL] batch:  215 | acc: 81.25%,  total acc: 66.67%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 66.82%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 66.97%   [EVAL] batch:  218 | acc: 93.75%,  total acc: 67.09%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 67.24%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 67.39%   [EVAL] batch:  221 | acc: 100.00%,  total acc: 67.54%   [EVAL] batch:  222 | acc: 100.00%,  total acc: 67.68%   [EVAL] batch:  223 | acc: 100.00%,  total acc: 67.83%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 67.97%   [EVAL] batch:  225 | acc: 68.75%,  total acc: 67.98%   [EVAL] batch:  226 | acc: 62.50%,  total acc: 67.95%   [EVAL] batch:  227 | acc: 75.00%,  total acc: 67.98%   [EVAL] batch:  228 | acc: 62.50%,  total acc: 67.96%   [EVAL] batch:  229 | acc: 93.75%,  total acc: 68.07%   [EVAL] batch:  230 | acc: 68.75%,  total acc: 68.07%   [EVAL] batch:  231 | acc: 56.25%,  total acc: 68.02%   [EVAL] batch:  232 | acc: 43.75%,  total acc: 67.92%   [EVAL] batch:  233 | acc: 56.25%,  total acc: 67.87%   [EVAL] batch:  234 | acc: 25.00%,  total acc: 67.69%   [EVAL] batch:  235 | acc: 68.75%,  total acc: 67.69%   [EVAL] batch:  236 | acc: 43.75%,  total acc: 67.59%   [EVAL] batch:  237 | acc: 68.75%,  total acc: 67.59%   [EVAL] batch:  238 | acc: 81.25%,  total acc: 67.65%   [EVAL] batch:  239 | acc: 100.00%,  total acc: 67.79%   [EVAL] batch:  240 | acc: 87.50%,  total acc: 67.87%   [EVAL] batch:  241 | acc: 87.50%,  total acc: 67.95%   [EVAL] batch:  242 | acc: 81.25%,  total acc: 68.00%   [EVAL] batch:  243 | acc: 81.25%,  total acc: 68.06%   [EVAL] batch:  244 | acc: 81.25%,  total acc: 68.11%   [EVAL] batch:  245 | acc: 87.50%,  total acc: 68.19%   [EVAL] batch:  246 | acc: 56.25%,  total acc: 68.14%   [EVAL] batch:  247 | acc: 75.00%,  total acc: 68.17%   [EVAL] batch:  248 | acc: 75.00%,  total acc: 68.20%   [EVAL] batch:  249 | acc: 75.00%,  total acc: 68.23%   [EVAL] batch:  250 | acc: 50.00%,  total acc: 68.15%   [EVAL] batch:  251 | acc: 62.50%,  total acc: 68.13%   [EVAL] batch:  252 | acc: 75.00%,  total acc: 68.16%   [EVAL] batch:  253 | acc: 62.50%,  total acc: 68.13%   [EVAL] batch:  254 | acc: 68.75%,  total acc: 68.14%   [EVAL] batch:  255 | acc: 75.00%,  total acc: 68.16%   [EVAL] batch:  256 | acc: 62.50%,  total acc: 68.14%   [EVAL] batch:  257 | acc: 81.25%,  total acc: 68.19%   [EVAL] batch:  258 | acc: 68.75%,  total acc: 68.19%   [EVAL] batch:  259 | acc: 75.00%,  total acc: 68.22%   [EVAL] batch:  260 | acc: 50.00%,  total acc: 68.15%   [EVAL] batch:  261 | acc: 75.00%,  total acc: 68.18%   [EVAL] batch:  262 | acc: 62.50%,  total acc: 68.16%   [EVAL] batch:  263 | acc: 81.25%,  total acc: 68.21%   [EVAL] batch:  264 | acc: 62.50%,  total acc: 68.18%   [EVAL] batch:  265 | acc: 62.50%,  total acc: 68.16%   [EVAL] batch:  266 | acc: 62.50%,  total acc: 68.14%   [EVAL] batch:  267 | acc: 62.50%,  total acc: 68.12%   [EVAL] batch:  268 | acc: 93.75%,  total acc: 68.22%   [EVAL] batch:  269 | acc: 87.50%,  total acc: 68.29%   [EVAL] batch:  270 | acc: 100.00%,  total acc: 68.40%   [EVAL] batch:  271 | acc: 100.00%,  total acc: 68.52%   [EVAL] batch:  272 | acc: 100.00%,  total acc: 68.64%   [EVAL] batch:  273 | acc: 93.75%,  total acc: 68.73%   [EVAL] batch:  274 | acc: 100.00%,  total acc: 68.84%   [EVAL] batch:  275 | acc: 75.00%,  total acc: 68.86%   [EVAL] batch:  276 | acc: 37.50%,  total acc: 68.75%   [EVAL] batch:  277 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:  278 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:  279 | acc: 50.00%,  total acc: 68.68%   [EVAL] batch:  280 | acc: 68.75%,  total acc: 68.68%   [EVAL] batch:  281 | acc: 68.75%,  total acc: 68.68%   [EVAL] batch:  282 | acc: 62.50%,  total acc: 68.66%   [EVAL] batch:  283 | acc: 18.75%,  total acc: 68.49%   [EVAL] batch:  284 | acc: 37.50%,  total acc: 68.38%   [EVAL] batch:  285 | acc: 18.75%,  total acc: 68.20%   [EVAL] batch:  286 | acc: 25.00%,  total acc: 68.05%   [EVAL] batch:  287 | acc: 81.25%,  total acc: 68.10%   [EVAL] batch:  288 | acc: 75.00%,  total acc: 68.12%   [EVAL] batch:  289 | acc: 87.50%,  total acc: 68.19%   [EVAL] batch:  290 | acc: 68.75%,  total acc: 68.19%   [EVAL] batch:  291 | acc: 87.50%,  total acc: 68.26%   [EVAL] batch:  292 | acc: 81.25%,  total acc: 68.30%   [EVAL] batch:  293 | acc: 81.25%,  total acc: 68.35%   [EVAL] batch:  294 | acc: 56.25%,  total acc: 68.31%   [EVAL] batch:  295 | acc: 68.75%,  total acc: 68.31%   [EVAL] batch:  296 | acc: 62.50%,  total acc: 68.29%   [EVAL] batch:  297 | acc: 50.00%,  total acc: 68.23%   [EVAL] batch:  298 | acc: 62.50%,  total acc: 68.21%   [EVAL] batch:  299 | acc: 75.00%,  total acc: 68.23%   [EVAL] batch:  300 | acc: 100.00%,  total acc: 68.33%   [EVAL] batch:  301 | acc: 100.00%,  total acc: 68.44%   [EVAL] batch:  302 | acc: 100.00%,  total acc: 68.54%   [EVAL] batch:  303 | acc: 93.75%,  total acc: 68.63%   [EVAL] batch:  304 | acc: 100.00%,  total acc: 68.73%   [EVAL] batch:  305 | acc: 100.00%,  total acc: 68.83%   [EVAL] batch:  306 | acc: 100.00%,  total acc: 68.93%   [EVAL] batch:  307 | acc: 93.75%,  total acc: 69.01%   [EVAL] batch:  308 | acc: 93.75%,  total acc: 69.09%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 69.19%   [EVAL] batch:  310 | acc: 100.00%,  total acc: 69.29%   [EVAL] batch:  311 | acc: 100.00%,  total acc: 69.39%   [EVAL] batch:  312 | acc: 93.75%,  total acc: 69.47%   [EVAL] batch:  313 | acc: 62.50%,  total acc: 69.45%   [EVAL] batch:  314 | acc: 62.50%,  total acc: 69.42%   [EVAL] batch:  315 | acc: 75.00%,  total acc: 69.44%   [EVAL] batch:  316 | acc: 81.25%,  total acc: 69.48%   [EVAL] batch:  317 | acc: 87.50%,  total acc: 69.54%   [EVAL] batch:  318 | acc: 75.00%,  total acc: 69.55%   [EVAL] batch:  319 | acc: 93.75%,  total acc: 69.63%   [EVAL] batch:  320 | acc: 75.00%,  total acc: 69.65%   [EVAL] batch:  321 | acc: 81.25%,  total acc: 69.68%   [EVAL] batch:  322 | acc: 81.25%,  total acc: 69.72%   [EVAL] batch:  323 | acc: 87.50%,  total acc: 69.77%   [EVAL] batch:  324 | acc: 62.50%,  total acc: 69.75%   [EVAL] batch:  325 | acc: 25.00%,  total acc: 69.61%   [EVAL] batch:  326 | acc: 18.75%,  total acc: 69.46%   [EVAL] batch:  327 | acc: 12.50%,  total acc: 69.28%   [EVAL] batch:  328 | acc: 25.00%,  total acc: 69.15%   [EVAL] batch:  329 | acc: 31.25%,  total acc: 69.03%   [EVAL] batch:  330 | acc: 18.75%,  total acc: 68.88%   [EVAL] batch:  331 | acc: 68.75%,  total acc: 68.88%   [EVAL] batch:  332 | acc: 100.00%,  total acc: 68.98%   [EVAL] batch:  333 | acc: 100.00%,  total acc: 69.07%   [EVAL] batch:  334 | acc: 93.75%,  total acc: 69.14%   [EVAL] batch:  335 | acc: 100.00%,  total acc: 69.23%   [EVAL] batch:  336 | acc: 93.75%,  total acc: 69.31%   [EVAL] batch:  337 | acc: 56.25%,  total acc: 69.27%   [EVAL] batch:  338 | acc: 6.25%,  total acc: 69.08%   [EVAL] batch:  339 | acc: 18.75%,  total acc: 68.93%   [EVAL] batch:  340 | acc: 18.75%,  total acc: 68.79%   [EVAL] batch:  341 | acc: 6.25%,  total acc: 68.60%   [EVAL] batch:  342 | acc: 12.50%,  total acc: 68.44%   [EVAL] batch:  343 | acc: 25.00%,  total acc: 68.31%   [EVAL] batch:  344 | acc: 81.25%,  total acc: 68.35%   [EVAL] batch:  345 | acc: 93.75%,  total acc: 68.42%   [EVAL] batch:  346 | acc: 87.50%,  total acc: 68.48%   [EVAL] batch:  347 | acc: 87.50%,  total acc: 68.53%   [EVAL] batch:  348 | acc: 93.75%,  total acc: 68.61%   [EVAL] batch:  349 | acc: 100.00%,  total acc: 68.70%   [EVAL] batch:  350 | acc: 68.75%,  total acc: 68.70%   [EVAL] batch:  351 | acc: 81.25%,  total acc: 68.73%   [EVAL] batch:  352 | acc: 68.75%,  total acc: 68.73%   [EVAL] batch:  353 | acc: 50.00%,  total acc: 68.68%   [EVAL] batch:  354 | acc: 81.25%,  total acc: 68.71%   [EVAL] batch:  355 | acc: 68.75%,  total acc: 68.71%   [EVAL] batch:  356 | acc: 81.25%,  total acc: 68.75%   [EVAL] batch:  357 | acc: 81.25%,  total acc: 68.78%   [EVAL] batch:  358 | acc: 81.25%,  total acc: 68.82%   [EVAL] batch:  359 | acc: 68.75%,  total acc: 68.82%   [EVAL] batch:  360 | acc: 68.75%,  total acc: 68.82%   [EVAL] batch:  361 | acc: 75.00%,  total acc: 68.84%   [EVAL] batch:  362 | acc: 68.75%,  total acc: 68.84%   [EVAL] batch:  363 | acc: 31.25%,  total acc: 68.73%   [EVAL] batch:  364 | acc: 62.50%,  total acc: 68.72%   [EVAL] batch:  365 | acc: 37.50%,  total acc: 68.63%   [EVAL] batch:  366 | acc: 56.25%,  total acc: 68.60%   [EVAL] batch:  367 | acc: 43.75%,  total acc: 68.53%   [EVAL] batch:  368 | acc: 62.50%,  total acc: 68.51%   [EVAL] batch:  369 | acc: 93.75%,  total acc: 68.58%   [EVAL] batch:  370 | acc: 87.50%,  total acc: 68.63%   [EVAL] batch:  371 | acc: 81.25%,  total acc: 68.67%   [EVAL] batch:  372 | acc: 75.00%,  total acc: 68.68%   [EVAL] batch:  373 | acc: 81.25%,  total acc: 68.72%   [EVAL] batch:  374 | acc: 81.25%,  total acc: 68.75%   
cur_acc:  ['0.9494', '0.6151', '0.7143', '0.6885', '0.7510', '0.6478']
his_acc:  ['0.9494', '0.7760', '0.7500', '0.7332', '0.7145', '0.6875']
Clustering into  34  clusters
Clusters:  [30  3 21 18  8 13 21 12  3  7  8 24  9 31  4 32  9  3  9  3  1 29 12  8
 23  7  2 13 33 22 11  9 12 24 14 19  5  1 15  1 28 17  5 21 29 27  3 26
  0  2  2 24  4 10  4  6  9  2 20  0 18 16 10  3 25 26 11  6 14 11]
Losses:  8.753683090209961 2.920809745788574 0.7633882164955139
CurrentTrain: epoch  0, batch     0 | loss: 8.7536831Losses:  9.16821002960205 3.2232489585876465 0.7277594208717346
CurrentTrain: epoch  0, batch     1 | loss: 9.1682100Losses:  8.519899368286133 2.540550947189331 0.7063356637954712
CurrentTrain: epoch  0, batch     2 | loss: 8.5198994Losses:  8.79269027709961 -0.0 0.2567671239376068
CurrentTrain: epoch  0, batch     3 | loss: 8.7926903Losses:  8.091803550720215 3.336111545562744 0.6277521252632141
CurrentTrain: epoch  1, batch     0 | loss: 8.0918036Losses:  8.251145362854004 2.663390636444092 0.7688280344009399
CurrentTrain: epoch  1, batch     1 | loss: 8.2511454Losses:  8.051891326904297 3.2106685638427734 0.6752680540084839
CurrentTrain: epoch  1, batch     2 | loss: 8.0518913Losses:  2.9339189529418945 -0.0 0.10306770354509354
CurrentTrain: epoch  1, batch     3 | loss: 2.9339190Losses:  6.65079402923584 2.5495100021362305 0.6710096597671509
CurrentTrain: epoch  2, batch     0 | loss: 6.6507940Losses:  8.220781326293945 4.457196235656738 0.5017949342727661
CurrentTrain: epoch  2, batch     1 | loss: 8.2207813Losses:  7.73835563659668 3.3402516841888428 0.6730194091796875
CurrentTrain: epoch  2, batch     2 | loss: 7.7383556Losses:  4.021907329559326 -0.0 0.10471168160438538
CurrentTrain: epoch  2, batch     3 | loss: 4.0219073Losses:  8.270707130432129 3.481733798980713 0.7055018544197083
CurrentTrain: epoch  3, batch     0 | loss: 8.2707071Losses:  6.407336235046387 3.025747776031494 0.6641272306442261
CurrentTrain: epoch  3, batch     1 | loss: 6.4073362Losses:  6.405642509460449 2.9647722244262695 0.6344080567359924
CurrentTrain: epoch  3, batch     2 | loss: 6.4056425Losses:  2.4929697513580322 -0.0 0.10947383940219879
CurrentTrain: epoch  3, batch     3 | loss: 2.4929698Losses:  5.861156463623047 2.5473570823669434 0.6613142490386963
CurrentTrain: epoch  4, batch     0 | loss: 5.8611565Losses:  8.077825546264648 4.2192816734313965 0.6567131280899048
CurrentTrain: epoch  4, batch     1 | loss: 8.0778255Losses:  6.774967670440674 3.703217029571533 0.4962458610534668
CurrentTrain: epoch  4, batch     2 | loss: 6.7749677Losses:  1.8072412014007568 -0.0 0.1303805410861969
CurrentTrain: epoch  4, batch     3 | loss: 1.8072412Losses:  6.158437728881836 3.293128728866577 0.5936143398284912
CurrentTrain: epoch  5, batch     0 | loss: 6.1584377Losses:  6.0206122398376465 2.7123618125915527 0.6559324264526367
CurrentTrain: epoch  5, batch     1 | loss: 6.0206122Losses:  8.694500923156738 5.362007141113281 0.6001376509666443
CurrentTrain: epoch  5, batch     2 | loss: 8.6945009Losses:  2.041940689086914 -0.0 0.14032235741615295
CurrentTrain: epoch  5, batch     3 | loss: 2.0419407Losses:  5.491729736328125 2.5616164207458496 0.593043327331543
CurrentTrain: epoch  6, batch     0 | loss: 5.4917297Losses:  6.255500793457031 3.0884180068969727 0.7118056416511536
CurrentTrain: epoch  6, batch     1 | loss: 6.2555008Losses:  5.230579853057861 2.509068489074707 0.6378355026245117
CurrentTrain: epoch  6, batch     2 | loss: 5.2305799Losses:  2.611631393432617 -0.0 0.09268485754728317
CurrentTrain: epoch  6, batch     3 | loss: 2.6116314Losses:  5.338397979736328 2.7960410118103027 0.5653636455535889
CurrentTrain: epoch  7, batch     0 | loss: 5.3383980Losses:  5.039424419403076 2.054764986038208 0.7249501347541809
CurrentTrain: epoch  7, batch     1 | loss: 5.0394244Losses:  4.534275054931641 1.8560423851013184 0.7136170268058777
CurrentTrain: epoch  7, batch     2 | loss: 4.5342751Losses:  2.015882968902588 -0.0 0.10744792968034744
CurrentTrain: epoch  7, batch     3 | loss: 2.0158830Losses:  5.576241493225098 2.9877023696899414 0.6405644416809082
CurrentTrain: epoch  8, batch     0 | loss: 5.5762415Losses:  4.737194061279297 2.134953737258911 0.6294785737991333
CurrentTrain: epoch  8, batch     1 | loss: 4.7371941Losses:  5.031031608581543 2.5656180381774902 0.6270196437835693
CurrentTrain: epoch  8, batch     2 | loss: 5.0310316Losses:  1.9245860576629639 -0.0 0.1139238104224205
CurrentTrain: epoch  8, batch     3 | loss: 1.9245861Losses:  6.430267810821533 3.904430866241455 0.6366759538650513
CurrentTrain: epoch  9, batch     0 | loss: 6.4302678Losses:  5.982883930206299 3.452197551727295 0.6358773112297058
CurrentTrain: epoch  9, batch     1 | loss: 5.9828839Losses:  4.230102062225342 1.7869607210159302 0.6966115236282349
CurrentTrain: epoch  9, batch     2 | loss: 4.2301021Losses:  1.7688188552856445 -0.0 0.09306932240724564
CurrentTrain: epoch  9, batch     3 | loss: 1.7688189
#### member of political party ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "member of political party" means the political party of which a person is or has been a member or otherwise affiliated.
Here is an example:
Relation: member of political party
Context: karl johan aarønes ( 8 may 1900 – 12 august 1969 ) was a norwegian politician for the labour party .
Head Entity: karl johan aarønes
Tail Entity: labour party
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "member of political party":
Relation: member of political party  
Context: barack obama served as a member of the democratic party during his presidency from 2009 to 2017.  
Head Entity: barack obama  
Tail Entity: democratic party  

Relation: member of political party  
Context: angela merkel was a prominent member of the christian democratic union throughout her political career in germany.  
Head Entity: angela merkel  
Tail Entity: christian democratic union  
#### after a work by ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "after a work by" means artist whose work strongly inspired/ was copied in this item.
Here is an example:
Relation: after a work by
Context: the script for " the great santini " was adapted by carlino from the 1976 novel by pat conroy , with assistance from an un - credited herman raucher .
Head Entity: the great santini
Tail Entity: pat conroy
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "after a work by":
Relation: after a work by  
Context: the film "inception" draws heavily from the themes and concepts presented in the 1982 novel "neuromancer" by william gibson, influencing its narrative structure and visual style.  
Head Entity: inception  
Tail Entity: william gibson  

Relation: after a work by  
Context: the musical "hamilton" is inspired by the biography of alexander hamilton written by ron chernow, which provided the foundation for its storytelling and character development.  
Head Entity: hamilton  
Tail Entity: ron chernow  
#### notable work ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "notable work" means notable scientific, artistic or literary work, or other work of significance among subject's works.
Here is an example:
Relation: notable work
Context: several paintings are also mentioned , including rousseau 's " the dream " and van gogh 's " bedroom in arles " .
Head Entity: rousseau
Tail Entity: the dream
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "notable work":
Relation: notable work  
Context: Among his many contributions to literature, one of his most acclaimed novels is "The Great Gatsby," which explores themes of wealth and social change in 1920s America.  
Head Entity: F. Scott Fitzgerald  
Tail Entity: The Great Gatsby  

Relation: notable work  
Context: The artist is best known for his sculpture "The Thinker," which has become a symbol of philosophy and contemplation.  
Head Entity: Auguste Rodin  
Tail Entity: The Thinker  
#### headquarters location ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "headquarters location" means city, where an organization's headquarters is or has been situated.
Here is an example:
Relation: headquarters location
Context: at the request of the stockholm institute for bible translation , mirza khazar translated the bible – the new testament and old testament – into the azerbaijani language .
Head Entity: institute for bible translation
Tail Entity: stockholm
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "headquarters location":
Relation: headquarters location  
Context: the global technology company, headquartered in san francisco, has been a leader in software development for over two decades.  
Head Entity: global technology company  
Tail Entity: san francisco  

Relation: headquarters location  
Context: after relocating its main office to new york city, the fashion brand saw a significant increase in sales and brand recognition.  
Head Entity: fashion brand  
Tail Entity: new york city  
#### taxon rank ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "taxon rank" means level in a taxonomic hierarchy.
Here is an example:
Relation: taxon rank
Context: gyaclavator is an extinct genus of lace bug in the family tingidae known from a fossil found in north america .
Head Entity: tingidae
Tail Entity: family
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "taxon rank":
Relation: taxon rank  
Context: The genus Panthera includes several large cats, such as lions and tigers, which are classified under the family Felidae.  
Head Entity: Panthera  
Tail Entity: family  

Relation: taxon rank  
Context: The species Homo sapiens is part of the genus Homo, which falls under the family Hominidae, known for its members' advanced cognitive abilities.  
Head Entity: Homo  
Tail Entity: family  
#### field of work ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "field of work" means specialization of a person or organization.
Here is an example:
Relation: field of work
Context: elspeth frances garman is professor of molecular biophysics at the university of oxford and a former president of the british crystallographic association .
Head Entity: elspeth frances garman
Tail Entity: molecular biophysics
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "field of work":
Relation: field of work  
Context: dr. amanda jones is a leading researcher in the field of artificial intelligence and currently works at the tech innovation lab in silicon valley.  
Head Entity: dr. amanda jones  
Tail Entity: artificial intelligence  

Relation: field of work  
Context: the world health organization has appointed dr. rajesh kumar as the new director of global health initiatives, focusing on infectious diseases.  
Head Entity: dr. rajesh kumar  
Tail Entity: infectious diseases  
#### licensed to broadcast to ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "licensed to broadcast to" means place that a radio/TV station is licensed/required to broadcast to.
Here is an example:
Relation: licensed to broadcast to
Context: wjet erie , pennsylvania was his first official radio job outside of duties performed in the u.s. air force .
Head Entity: wjet
Tail Entity: erie , pennsylvania
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "licensed to broadcast to":
Relation: licensed to broadcast to  
Context: wxyz television is the primary news station serving the city of springfield, known for its local coverage.  
Head Entity: wxyz  
Tail Entity: springfield  

Relation: licensed to broadcast to  
Context: kqed is a public television station that provides educational programming to the residents of san francisco.  
Head Entity: kqed  
Tail Entity: san francisco  
#### constellation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "constellation" means the area of the celestial sphere of which the subject is a part.
Here is an example:
Relation: constellation
Context: hd 32518 b is an extrasolar planet which orbits the k - type giant star hd 32518 , located approximately 383 light years away in the constellation camelopardalis .
Head Entity: hd 32518
Tail Entity: camelopardalis
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "constellation":
Relation: constellation  
Context: the star betelgeuse is located in the constellation orion and is one of the brightest stars in the night sky.  
Head Entity: betelgeuse  
Tail Entity: orion  

Relation: constellation  
Context: the andromeda galaxy is visible in the constellation andromeda, which is named after a princess in Greek mythology.  
Head Entity: andromeda galaxy  
Tail Entity: andromeda  
#### platform ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "platform" means platform for which a work was developed or released, or the specific platform version of a software product.
Here is an example:
Relation: platform
Context: " toy soldiers : war chest " was released on august 11 , 2015 for windows , playstation 4 and xbox one .
Head Entity: toy soldiers : war chest
Tail Entity: xbox one
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "platform":
Relation: platform  
Context: "The Legend of Zelda: Breath of the Wild" was launched on March 3, 2017 for the Nintendo Switch and Wii U.  
Head Entity: The Legend of Zelda: Breath of the Wild  
Tail Entity: Nintendo Switch  

Relation: platform  
Context: "Cyberpunk 2077" was made available on December 10, 2020 for PC, PlayStation 4, and Xbox One.  
Head Entity: Cyberpunk 2077  
Tail Entity: PlayStation 4  
#### place served by transport hub ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "place served by transport hub" means territorial entity or entities served by this transport hub (airport, train station, etc.).
Here is an example:
Relation: place served by transport hub
Context: services to the island run from san juan 's luis muñoz marín international airport , ceiba airport or isla grande airport ( 20- to 30-minute flight ) .
Head Entity: isla grande airport
Tail Entity: san juan
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "place served by transport hub":
Relation: place served by transport hub  
Context: The city of Barcelona is well-connected to the surrounding regions through the El Prat Airport, which offers numerous flights to various destinations.  
Head Entity: El Prat Airport  
Tail Entity: Barcelona  

Relation: place served by transport hub  
Context: The central train station in Zurich provides access to multiple Swiss cities and international destinations, making it a key transport hub for travelers.  
Head Entity: Zurich central train station  
Tail Entity: Zurich  
Losses:  3.8434951305389404 0.5093231797218323 0.9376046061515808
MemoryTrain:  epoch  0, batch     0 | loss: 3.8434951Losses:  3.79719614982605 0.7429561018943787 0.9330937266349792
MemoryTrain:  epoch  0, batch     1 | loss: 3.7971961Losses:  3.8522794246673584 -0.0 0.9093866348266602
MemoryTrain:  epoch  0, batch     2 | loss: 3.8522794Losses:  3.742666244506836 0.25003212690353394 0.9947855472564697
MemoryTrain:  epoch  0, batch     3 | loss: 3.7426662Losses:  3.5438318252563477 0.2861250042915344 1.0199034214019775
MemoryTrain:  epoch  0, batch     4 | loss: 3.5438318Losses:  4.027729034423828 0.7465837597846985 0.9113951921463013
MemoryTrain:  epoch  0, batch     5 | loss: 4.0277290Losses:  3.0240869522094727 0.22652950882911682 0.7936517596244812
MemoryTrain:  epoch  0, batch     6 | loss: 3.0240870Losses:  3.25958514213562 0.22503666579723358 0.7996034026145935
MemoryTrain:  epoch  0, batch     7 | loss: 3.2595851Losses:  4.316065311431885 0.27612531185150146 0.8595895171165466
MemoryTrain:  epoch  0, batch     8 | loss: 4.3160653Losses:  4.40575647354126 0.3257436752319336 0.9778425097465515
MemoryTrain:  epoch  0, batch     9 | loss: 4.4057565Losses:  4.323370933532715 0.3504059910774231 0.9259918928146362
MemoryTrain:  epoch  0, batch    10 | loss: 4.3233709Losses:  3.0120999813079834 0.2641087472438812 0.8644471764564514
MemoryTrain:  epoch  0, batch    11 | loss: 3.0121000Losses:  3.7721822261810303 0.23394471406936646 0.9742345213890076
MemoryTrain:  epoch  0, batch    12 | loss: 3.7721822Losses:  3.766561269760132 -0.0 0.09809624403715134
MemoryTrain:  epoch  0, batch    13 | loss: 3.7665613Losses:  3.59631085395813 0.48961079120635986 0.7875559329986572
MemoryTrain:  epoch  1, batch     0 | loss: 3.5963109Losses:  4.106687068939209 0.48624980449676514 0.8795932531356812
MemoryTrain:  epoch  1, batch     1 | loss: 4.1066871Losses:  3.718595027923584 -0.0 0.9601078629493713
MemoryTrain:  epoch  1, batch     2 | loss: 3.7185950Losses:  2.9630239009857178 0.5336557030677795 0.8530349135398865
MemoryTrain:  epoch  1, batch     3 | loss: 2.9630239Losses:  3.0445313453674316 -0.0 1.016254186630249
MemoryTrain:  epoch  1, batch     4 | loss: 3.0445313Losses:  3.7482705116271973 -0.0 0.9149107933044434
MemoryTrain:  epoch  1, batch     5 | loss: 3.7482705Losses:  3.5552597045898438 0.7716226577758789 0.8537768125534058
MemoryTrain:  epoch  1, batch     6 | loss: 3.5552597Losses:  2.9856839179992676 -0.0 1.0485913753509521
MemoryTrain:  epoch  1, batch     7 | loss: 2.9856839Losses:  3.906224250793457 0.5148788690567017 0.9732909202575684
MemoryTrain:  epoch  1, batch     8 | loss: 3.9062243Losses:  4.059058666229248 0.8466043472290039 0.939588725566864
MemoryTrain:  epoch  1, batch     9 | loss: 4.0590587Losses:  3.3958725929260254 0.510589599609375 0.9147741198539734
MemoryTrain:  epoch  1, batch    10 | loss: 3.3958726Losses:  3.0562171936035156 0.5262073278427124 1.0181726217269897
MemoryTrain:  epoch  1, batch    11 | loss: 3.0562172Losses:  3.1723241806030273 0.2577277719974518 1.038051724433899
MemoryTrain:  epoch  1, batch    12 | loss: 3.1723242Losses:  1.677261233329773 -0.0 0.10891444236040115
MemoryTrain:  epoch  1, batch    13 | loss: 1.6772612Losses:  3.2500720024108887 -0.0 1.0620081424713135
MemoryTrain:  epoch  2, batch     0 | loss: 3.2500720Losses:  3.3152904510498047 0.5291436314582825 0.9398857355117798
MemoryTrain:  epoch  2, batch     1 | loss: 3.3152905Losses:  2.9786250591278076 0.24892230331897736 1.0028960704803467
MemoryTrain:  epoch  2, batch     2 | loss: 2.9786251Losses:  2.9869613647460938 0.5105241537094116 0.9793182611465454
MemoryTrain:  epoch  2, batch     3 | loss: 2.9869614Losses:  2.6935489177703857 0.228990837931633 1.0304206609725952
MemoryTrain:  epoch  2, batch     4 | loss: 2.6935489Losses:  3.669835090637207 0.29177922010421753 0.9677261710166931
MemoryTrain:  epoch  2, batch     5 | loss: 3.6698351Losses:  3.0781025886535645 0.5219027400016785 0.9679388403892517
MemoryTrain:  epoch  2, batch     6 | loss: 3.0781026Losses:  3.4239344596862793 0.7767599821090698 0.918670654296875
MemoryTrain:  epoch  2, batch     7 | loss: 3.4239345Losses:  3.23496675491333 0.7348566055297852 0.9151028394699097
MemoryTrain:  epoch  2, batch     8 | loss: 3.2349668Losses:  2.7204957008361816 -0.0 0.9126355051994324
MemoryTrain:  epoch  2, batch     9 | loss: 2.7204957Losses:  3.0007376670837402 0.24434660375118256 0.9110615253448486
MemoryTrain:  epoch  2, batch    10 | loss: 3.0007377Losses:  2.7182042598724365 0.2762562036514282 0.9644738435745239
MemoryTrain:  epoch  2, batch    11 | loss: 2.7182043Losses:  3.176236152648926 0.23492327332496643 0.9659599661827087
MemoryTrain:  epoch  2, batch    12 | loss: 3.1762362Losses:  1.390896201133728 -0.0 0.10875748842954636
MemoryTrain:  epoch  2, batch    13 | loss: 1.3908962Losses:  3.0772054195404053 0.5398449897766113 0.8805503845214844
MemoryTrain:  epoch  3, batch     0 | loss: 3.0772054Losses:  2.670719623565674 0.23801104724407196 1.0060932636260986
MemoryTrain:  epoch  3, batch     1 | loss: 2.6707196Losses:  3.337552785873413 0.5303707122802734 0.9673780798912048
MemoryTrain:  epoch  3, batch     2 | loss: 3.3375528Losses:  2.4186744689941406 0.2423611581325531 0.8628888130187988
MemoryTrain:  epoch  3, batch     3 | loss: 2.4186745Losses:  2.598010540008545 -0.0 1.0137608051300049
MemoryTrain:  epoch  3, batch     4 | loss: 2.5980105Losses:  2.848202705383301 0.5029924511909485 1.0288959741592407
MemoryTrain:  epoch  3, batch     5 | loss: 2.8482027Losses:  3.4703009128570557 0.7514380216598511 0.9139940142631531
MemoryTrain:  epoch  3, batch     6 | loss: 3.4703009Losses:  2.9221408367156982 0.24204404652118683 0.961357057094574
MemoryTrain:  epoch  3, batch     7 | loss: 2.9221408Losses:  2.6638360023498535 0.5008580684661865 0.8360387682914734
MemoryTrain:  epoch  3, batch     8 | loss: 2.6638360Losses:  3.3196957111358643 0.5150867104530334 0.9366776347160339
MemoryTrain:  epoch  3, batch     9 | loss: 3.3196957Losses:  2.6191885471343994 0.2404709756374359 1.0685995817184448
MemoryTrain:  epoch  3, batch    10 | loss: 2.6191885Losses:  2.468151569366455 -0.0 1.029765248298645
MemoryTrain:  epoch  3, batch    11 | loss: 2.4681516Losses:  2.7947113513946533 0.2662942111492157 0.9024145007133484
MemoryTrain:  epoch  3, batch    12 | loss: 2.7947114Losses:  1.4480420351028442 -0.0 0.10030876100063324
MemoryTrain:  epoch  3, batch    13 | loss: 1.4480420Losses:  2.56593656539917 0.23323045670986176 0.8621071577072144
MemoryTrain:  epoch  4, batch     0 | loss: 2.5659366Losses:  3.358947277069092 0.34452664852142334 0.955787718296051
MemoryTrain:  epoch  4, batch     1 | loss: 3.3589473Losses:  2.6876187324523926 -0.0 1.0515713691711426
MemoryTrain:  epoch  4, batch     2 | loss: 2.6876187Losses:  2.180665969848633 -0.0 0.8239458203315735
MemoryTrain:  epoch  4, batch     3 | loss: 2.1806660Losses:  2.3742828369140625 -0.0 1.0008422136306763
MemoryTrain:  epoch  4, batch     4 | loss: 2.3742828Losses:  2.862362861633301 0.2451639622449875 0.9704722166061401
MemoryTrain:  epoch  4, batch     5 | loss: 2.8623629Losses:  2.619232654571533 0.2397913783788681 1.0073341131210327
MemoryTrain:  epoch  4, batch     6 | loss: 2.6192327Losses:  2.517139434814453 0.24191254377365112 0.8910506963729858
MemoryTrain:  epoch  4, batch     7 | loss: 2.5171394Losses:  2.579401969909668 0.2583468556404114 0.9796706438064575
MemoryTrain:  epoch  4, batch     8 | loss: 2.5794020Losses:  2.525146245956421 0.2491861879825592 0.9158123731613159
MemoryTrain:  epoch  4, batch     9 | loss: 2.5251462Losses:  2.897047996520996 0.30108898878097534 0.9847732782363892
MemoryTrain:  epoch  4, batch    10 | loss: 2.8970480Losses:  2.7317633628845215 0.2664144039154053 0.9569375514984131
MemoryTrain:  epoch  4, batch    11 | loss: 2.7317634Losses:  2.797863245010376 0.5018945932388306 0.9084731936454773
MemoryTrain:  epoch  4, batch    12 | loss: 2.7978632Losses:  1.3051469326019287 -0.0 0.09073849767446518
MemoryTrain:  epoch  4, batch    13 | loss: 1.3051469Losses:  2.526855945587158 0.26059412956237793 0.9068581461906433
MemoryTrain:  epoch  5, batch     0 | loss: 2.5268559Losses:  2.526435375213623 0.24104547500610352 0.9770551323890686
MemoryTrain:  epoch  5, batch     1 | loss: 2.5264354Losses:  2.772134304046631 0.24786047637462616 0.8445653319358826
MemoryTrain:  epoch  5, batch     2 | loss: 2.7721343Losses:  2.7951290607452393 0.236354261636734 0.9523810148239136
MemoryTrain:  epoch  5, batch     3 | loss: 2.7951291Losses:  2.708202600479126 0.4834545850753784 0.927965521812439
MemoryTrain:  epoch  5, batch     4 | loss: 2.7082026Losses:  2.3997607231140137 -0.0 1.003859281539917
MemoryTrain:  epoch  5, batch     5 | loss: 2.3997607Losses:  2.7707440853118896 0.49205881357192993 0.9858734607696533
MemoryTrain:  epoch  5, batch     6 | loss: 2.7707441Losses:  2.822296142578125 0.5226479172706604 0.9333011507987976
MemoryTrain:  epoch  5, batch     7 | loss: 2.8222961Losses:  2.775223731994629 0.5067412853240967 0.9647611975669861
MemoryTrain:  epoch  5, batch     8 | loss: 2.7752237Losses:  2.3360533714294434 -0.0 0.9650547504425049
MemoryTrain:  epoch  5, batch     9 | loss: 2.3360534Losses:  2.5664799213409424 0.24024009704589844 0.8911845088005066
MemoryTrain:  epoch  5, batch    10 | loss: 2.5664799Losses:  3.0903477668762207 0.5412291288375854 0.9092459678649902
MemoryTrain:  epoch  5, batch    11 | loss: 3.0903478Losses:  2.1471152305603027 -0.0 0.8947776556015015
MemoryTrain:  epoch  5, batch    12 | loss: 2.1471152Losses:  1.709064245223999 -0.0 0.08936209976673126
MemoryTrain:  epoch  5, batch    13 | loss: 1.7090642Losses:  2.623290538787842 0.22546830773353577 0.9973886013031006
MemoryTrain:  epoch  6, batch     0 | loss: 2.6232905Losses:  2.5579721927642822 0.4861017167568207 0.7947329878807068
MemoryTrain:  epoch  6, batch     1 | loss: 2.5579722Losses:  2.9988675117492676 0.7757266759872437 0.9061939716339111
MemoryTrain:  epoch  6, batch     2 | loss: 2.9988675Losses:  2.4983301162719727 0.22338101267814636 0.9525419473648071
MemoryTrain:  epoch  6, batch     3 | loss: 2.4983301Losses:  2.460775852203369 -0.0 0.9729435443878174
MemoryTrain:  epoch  6, batch     4 | loss: 2.4607759Losses:  2.794368267059326 0.5287115573883057 0.8904544711112976
MemoryTrain:  epoch  6, batch     5 | loss: 2.7943683Losses:  2.714254856109619 0.49805116653442383 0.9593199491500854
MemoryTrain:  epoch  6, batch     6 | loss: 2.7142549Losses:  2.3113222122192383 -0.0 1.0296730995178223
MemoryTrain:  epoch  6, batch     7 | loss: 2.3113222Losses:  3.3077778816223145 0.8393363952636719 0.9759997129440308
MemoryTrain:  epoch  6, batch     8 | loss: 3.3077779Losses:  2.748831272125244 0.2400461733341217 1.0622936487197876
MemoryTrain:  epoch  6, batch     9 | loss: 2.7488313Losses:  2.1143250465393066 -0.0 0.7986848950386047
MemoryTrain:  epoch  6, batch    10 | loss: 2.1143250Losses:  2.6733644008636475 0.2662813067436218 0.996901273727417
MemoryTrain:  epoch  6, batch    11 | loss: 2.6733644Losses:  2.2907557487487793 -0.0 1.0058159828186035
MemoryTrain:  epoch  6, batch    12 | loss: 2.2907557Losses:  1.3608310222625732 -0.0 0.13187569379806519
MemoryTrain:  epoch  6, batch    13 | loss: 1.3608310Losses:  2.139533519744873 -0.0 0.8871632814407349
MemoryTrain:  epoch  7, batch     0 | loss: 2.1395335Losses:  2.651918649673462 0.4924176037311554 0.840617835521698
MemoryTrain:  epoch  7, batch     1 | loss: 2.6519186Losses:  2.634798526763916 0.24445441365242004 1.0235956907272339
MemoryTrain:  epoch  7, batch     2 | loss: 2.6347985Losses:  2.8568923473358154 0.5216903686523438 0.9225878119468689
MemoryTrain:  epoch  7, batch     3 | loss: 2.8568923Losses:  2.272706985473633 -0.0 0.9361512064933777
MemoryTrain:  epoch  7, batch     4 | loss: 2.2727070Losses:  2.449672222137451 0.2342841774225235 0.9210277795791626
MemoryTrain:  epoch  7, batch     5 | loss: 2.4496722Losses:  2.450484037399292 0.276512086391449 0.8566250801086426
MemoryTrain:  epoch  7, batch     6 | loss: 2.4504840Losses:  2.7644777297973633 0.7316523194313049 0.8114839792251587
MemoryTrain:  epoch  7, batch     7 | loss: 2.7644777Losses:  2.5726611614227295 0.23401235044002533 1.0755946636199951
MemoryTrain:  epoch  7, batch     8 | loss: 2.5726612Losses:  2.610196590423584 0.23820823431015015 0.8967427611351013
MemoryTrain:  epoch  7, batch     9 | loss: 2.6101966Losses:  2.2131659984588623 -0.0 0.9176969528198242
MemoryTrain:  epoch  7, batch    10 | loss: 2.2131660Losses:  2.3142218589782715 -0.0 1.0107851028442383
MemoryTrain:  epoch  7, batch    11 | loss: 2.3142219Losses:  2.433619737625122 0.22744804620742798 0.8234012126922607
MemoryTrain:  epoch  7, batch    12 | loss: 2.4336197Losses:  1.2676838636398315 -0.0 0.10058854520320892
MemoryTrain:  epoch  7, batch    13 | loss: 1.2676839Losses:  3.2167768478393555 0.9421499371528625 0.9077838659286499
MemoryTrain:  epoch  8, batch     0 | loss: 3.2167768Losses:  2.489591360092163 0.23742777109146118 0.9754247069358826
MemoryTrain:  epoch  8, batch     1 | loss: 2.4895914Losses:  2.1832940578460693 -0.0 0.9146000742912292
MemoryTrain:  epoch  8, batch     2 | loss: 2.1832941Losses:  2.5437443256378174 0.269755482673645 0.9071909189224243
MemoryTrain:  epoch  8, batch     3 | loss: 2.5437443Losses:  2.7203660011291504 0.5218598246574402 0.903383731842041
MemoryTrain:  epoch  8, batch     4 | loss: 2.7203660Losses:  2.909703254699707 0.5227569341659546 0.9689428210258484
MemoryTrain:  epoch  8, batch     5 | loss: 2.9097033Losses:  2.4363346099853516 0.23857152462005615 0.9550302624702454
MemoryTrain:  epoch  8, batch     6 | loss: 2.4363346Losses:  2.878016471862793 0.5305604934692383 0.8776358366012573
MemoryTrain:  epoch  8, batch     7 | loss: 2.8780165Losses:  2.7432985305786133 0.5379142761230469 0.9560830593109131
MemoryTrain:  epoch  8, batch     8 | loss: 2.7432985Losses:  2.7472996711730957 0.4791562557220459 0.967359185218811
MemoryTrain:  epoch  8, batch     9 | loss: 2.7472997Losses:  2.162862777709961 -0.0 0.9133169054985046
MemoryTrain:  epoch  8, batch    10 | loss: 2.1628628Losses:  2.537668228149414 0.2616913318634033 0.9225564002990723
MemoryTrain:  epoch  8, batch    11 | loss: 2.5376682Losses:  2.5281219482421875 0.24715226888656616 0.9652701616287231
MemoryTrain:  epoch  8, batch    12 | loss: 2.5281219Losses:  1.347590684890747 -0.0 0.13868147134780884
MemoryTrain:  epoch  8, batch    13 | loss: 1.3475907Losses:  2.6502182483673096 0.48527681827545166 0.9449185132980347
MemoryTrain:  epoch  9, batch     0 | loss: 2.6502182Losses:  2.436368942260742 -0.0 1.033052682876587
MemoryTrain:  epoch  9, batch     1 | loss: 2.4363689Losses:  2.4576501846313477 0.25318872928619385 0.898869514465332
MemoryTrain:  epoch  9, batch     2 | loss: 2.4576502Losses:  2.3441643714904785 -0.0 1.013916254043579
MemoryTrain:  epoch  9, batch     3 | loss: 2.3441644Losses:  2.438282012939453 0.2512396574020386 0.8993553519248962
MemoryTrain:  epoch  9, batch     4 | loss: 2.4382820Losses:  2.5870442390441895 0.24376901984214783 1.0810973644256592
MemoryTrain:  epoch  9, batch     5 | loss: 2.5870442Losses:  2.6446266174316406 0.4897712469100952 0.9234441518783569
MemoryTrain:  epoch  9, batch     6 | loss: 2.6446266Losses:  2.542205810546875 0.2713814675807953 0.9633844494819641
MemoryTrain:  epoch  9, batch     7 | loss: 2.5422058Losses:  2.354128122329712 0.23018679022789001 0.9002041220664978
MemoryTrain:  epoch  9, batch     8 | loss: 2.3541281Losses:  2.314389705657959 -0.0 0.9015371203422546
MemoryTrain:  epoch  9, batch     9 | loss: 2.3143897Losses:  2.678718328475952 0.5093746781349182 0.8505387902259827
MemoryTrain:  epoch  9, batch    10 | loss: 2.6787183Losses:  2.437992572784424 0.2645517885684967 0.9124325513839722
MemoryTrain:  epoch  9, batch    11 | loss: 2.4379926Losses:  2.245490550994873 -0.0 1.0210131406784058
MemoryTrain:  epoch  9, batch    12 | loss: 2.2454906Losses:  1.3983806371688843 -0.0 0.12803612649440765
MemoryTrain:  epoch  9, batch    13 | loss: 1.3983806
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 90.62%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 85.94%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 89.58%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 89.29%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 91.67%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 91.88%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 91.48%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 91.15%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 90.38%   [EVAL] batch:   13 | acc: 75.00%,  total acc: 89.29%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 88.33%   [EVAL] batch:   15 | acc: 62.50%,  total acc: 86.72%   [EVAL] batch:   16 | acc: 62.50%,  total acc: 85.29%   [EVAL] batch:   17 | acc: 56.25%,  total acc: 83.68%   [EVAL] batch:   18 | acc: 56.25%,  total acc: 82.24%   [EVAL] batch:   19 | acc: 62.50%,  total acc: 81.25%   [EVAL] batch:   20 | acc: 43.75%,  total acc: 79.46%   [EVAL] batch:   21 | acc: 56.25%,  total acc: 78.41%   [EVAL] batch:   22 | acc: 68.75%,  total acc: 77.99%   [EVAL] batch:   23 | acc: 75.00%,  total acc: 77.86%   [EVAL] batch:   24 | acc: 62.50%,  total acc: 77.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 78.12%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 78.94%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 79.69%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 80.39%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 81.04%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 81.65%   [EVAL] batch:   31 | acc: 75.00%,  total acc: 81.45%   [EVAL] batch:   32 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:   33 | acc: 62.50%,  total acc: 80.70%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 80.54%   [EVAL] batch:   35 | acc: 68.75%,  total acc: 80.21%   [EVAL] batch:   36 | acc: 62.50%,  total acc: 79.73%   [EVAL] batch:   37 | acc: 81.25%,  total acc: 79.77%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 80.29%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 80.78%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 81.70%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 82.12%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 82.39%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 82.78%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 83.15%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 83.51%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 83.85%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 84.18%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 84.50%   [EVAL] batch:   50 | acc: 87.50%,  total acc: 84.56%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 84.74%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 84.91%   [EVAL] batch:   53 | acc: 93.75%,  total acc: 85.07%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 85.23%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 85.49%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 85.64%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 85.78%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 85.81%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 86.04%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 86.27%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 86.39%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 85.81%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 85.94%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 86.25%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    6 | acc: 43.75%,  total acc: 81.25%   [EVAL] batch:    7 | acc: 68.75%,  total acc: 79.69%   [EVAL] batch:    8 | acc: 50.00%,  total acc: 76.39%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 76.25%   [EVAL] batch:   10 | acc: 37.50%,  total acc: 72.73%   [EVAL] batch:   11 | acc: 43.75%,  total acc: 70.31%   [EVAL] batch:   12 | acc: 62.50%,  total acc: 69.71%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 71.43%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 72.92%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 74.61%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 76.10%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 77.08%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 78.29%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 78.44%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 78.87%   [EVAL] batch:   21 | acc: 75.00%,  total acc: 78.69%   [EVAL] batch:   22 | acc: 75.00%,  total acc: 78.53%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 79.17%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 79.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 80.29%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 81.02%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 81.47%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 81.90%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 82.50%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 83.06%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 83.59%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 84.09%   [EVAL] batch:   33 | acc: 62.50%,  total acc: 83.46%   [EVAL] batch:   34 | acc: 81.25%,  total acc: 83.39%   [EVAL] batch:   35 | acc: 81.25%,  total acc: 83.33%   [EVAL] batch:   36 | acc: 75.00%,  total acc: 83.11%   [EVAL] batch:   37 | acc: 81.25%,  total acc: 83.06%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 83.49%   [EVAL] batch:   39 | acc: 93.75%,  total acc: 83.75%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 83.99%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 84.08%   [EVAL] batch:   42 | acc: 87.50%,  total acc: 84.16%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 84.23%   [EVAL] batch:   44 | acc: 75.00%,  total acc: 84.03%   [EVAL] batch:   45 | acc: 75.00%,  total acc: 83.83%   [EVAL] batch:   46 | acc: 50.00%,  total acc: 83.11%   [EVAL] batch:   47 | acc: 56.25%,  total acc: 82.55%   [EVAL] batch:   48 | acc: 75.00%,  total acc: 82.40%   [EVAL] batch:   49 | acc: 62.50%,  total acc: 82.00%   [EVAL] batch:   50 | acc: 75.00%,  total acc: 81.86%   [EVAL] batch:   51 | acc: 75.00%,  total acc: 81.73%   [EVAL] batch:   52 | acc: 75.00%,  total acc: 81.60%   [EVAL] batch:   53 | acc: 56.25%,  total acc: 81.13%   [EVAL] batch:   54 | acc: 56.25%,  total acc: 80.68%   [EVAL] batch:   55 | acc: 75.00%,  total acc: 80.58%   [EVAL] batch:   56 | acc: 81.25%,  total acc: 80.59%   [EVAL] batch:   57 | acc: 68.75%,  total acc: 80.39%   [EVAL] batch:   58 | acc: 81.25%,  total acc: 80.40%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 80.62%   [EVAL] batch:   60 | acc: 81.25%,  total acc: 80.64%   [EVAL] batch:   61 | acc: 75.00%,  total acc: 80.54%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 80.06%   [EVAL] batch:   63 | acc: 0.00%,  total acc: 78.81%   [EVAL] batch:   64 | acc: 6.25%,  total acc: 77.69%   [EVAL] batch:   65 | acc: 0.00%,  total acc: 76.52%   [EVAL] batch:   66 | acc: 0.00%,  total acc: 75.37%   [EVAL] batch:   67 | acc: 0.00%,  total acc: 74.26%   [EVAL] batch:   68 | acc: 31.25%,  total acc: 73.64%   [EVAL] batch:   69 | acc: 75.00%,  total acc: 73.66%   [EVAL] batch:   70 | acc: 93.75%,  total acc: 73.94%   [EVAL] batch:   71 | acc: 93.75%,  total acc: 74.22%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 74.57%   [EVAL] batch:   73 | acc: 81.25%,  total acc: 74.66%   [EVAL] batch:   74 | acc: 81.25%,  total acc: 74.75%   [EVAL] batch:   75 | acc: 68.75%,  total acc: 74.67%   [EVAL] batch:   76 | acc: 50.00%,  total acc: 74.35%   [EVAL] batch:   77 | acc: 68.75%,  total acc: 74.28%   [EVAL] batch:   78 | acc: 68.75%,  total acc: 74.21%   [EVAL] batch:   79 | acc: 37.50%,  total acc: 73.75%   [EVAL] batch:   80 | acc: 50.00%,  total acc: 73.46%   [EVAL] batch:   81 | acc: 12.50%,  total acc: 72.71%   [EVAL] batch:   82 | acc: 25.00%,  total acc: 72.14%   [EVAL] batch:   83 | acc: 12.50%,  total acc: 71.43%   [EVAL] batch:   84 | acc: 31.25%,  total acc: 70.96%   [EVAL] batch:   85 | acc: 25.00%,  total acc: 70.42%   [EVAL] batch:   86 | acc: 31.25%,  total acc: 69.97%   [EVAL] batch:   87 | acc: 12.50%,  total acc: 69.32%   [EVAL] batch:   88 | acc: 18.75%,  total acc: 68.75%   [EVAL] batch:   89 | acc: 25.00%,  total acc: 68.26%   [EVAL] batch:   90 | acc: 6.25%,  total acc: 67.58%   [EVAL] batch:   91 | acc: 6.25%,  total acc: 66.92%   [EVAL] batch:   92 | acc: 12.50%,  total acc: 66.33%   [EVAL] batch:   93 | acc: 31.25%,  total acc: 65.96%   [EVAL] batch:   94 | acc: 75.00%,  total acc: 66.05%   [EVAL] batch:   95 | acc: 87.50%,  total acc: 66.28%   [EVAL] batch:   96 | acc: 93.75%,  total acc: 66.56%   [EVAL] batch:   97 | acc: 93.75%,  total acc: 66.84%   [EVAL] batch:   98 | acc: 87.50%,  total acc: 67.05%   [EVAL] batch:   99 | acc: 75.00%,  total acc: 67.12%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 67.45%   [EVAL] batch:  101 | acc: 93.75%,  total acc: 67.71%   [EVAL] batch:  102 | acc: 87.50%,  total acc: 67.90%   [EVAL] batch:  103 | acc: 93.75%,  total acc: 68.15%   [EVAL] batch:  104 | acc: 93.75%,  total acc: 68.39%   [EVAL] batch:  105 | acc: 93.75%,  total acc: 68.63%   [EVAL] batch:  106 | acc: 43.75%,  total acc: 68.40%   [EVAL] batch:  107 | acc: 31.25%,  total acc: 68.06%   [EVAL] batch:  108 | acc: 12.50%,  total acc: 67.55%   [EVAL] batch:  109 | acc: 25.00%,  total acc: 67.16%   [EVAL] batch:  110 | acc: 18.75%,  total acc: 66.72%   [EVAL] batch:  111 | acc: 12.50%,  total acc: 66.24%   [EVAL] batch:  112 | acc: 25.00%,  total acc: 65.87%   [EVAL] batch:  113 | acc: 37.50%,  total acc: 65.62%   [EVAL] batch:  114 | acc: 56.25%,  total acc: 65.54%   [EVAL] batch:  115 | acc: 68.75%,  total acc: 65.57%   [EVAL] batch:  116 | acc: 56.25%,  total acc: 65.49%   [EVAL] batch:  117 | acc: 62.50%,  total acc: 65.47%   [EVAL] batch:  118 | acc: 56.25%,  total acc: 65.39%   [EVAL] batch:  119 | acc: 50.00%,  total acc: 65.26%   [EVAL] batch:  120 | acc: 43.75%,  total acc: 65.08%   [EVAL] batch:  121 | acc: 50.00%,  total acc: 64.96%   [EVAL] batch:  122 | acc: 62.50%,  total acc: 64.94%   [EVAL] batch:  123 | acc: 75.00%,  total acc: 65.02%   [EVAL] batch:  124 | acc: 56.25%,  total acc: 64.95%   [EVAL] batch:  125 | acc: 81.25%,  total acc: 65.08%   [EVAL] batch:  126 | acc: 62.50%,  total acc: 65.06%   [EVAL] batch:  127 | acc: 56.25%,  total acc: 64.99%   [EVAL] batch:  128 | acc: 56.25%,  total acc: 64.92%   [EVAL] batch:  129 | acc: 62.50%,  total acc: 64.90%   [EVAL] batch:  130 | acc: 81.25%,  total acc: 65.03%   [EVAL] batch:  131 | acc: 93.75%,  total acc: 65.25%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 65.51%   [EVAL] batch:  133 | acc: 87.50%,  total acc: 65.67%   [EVAL] batch:  134 | acc: 81.25%,  total acc: 65.79%   [EVAL] batch:  135 | acc: 93.75%,  total acc: 65.99%   [EVAL] batch:  136 | acc: 100.00%,  total acc: 66.24%   [EVAL] batch:  137 | acc: 93.75%,  total acc: 66.44%   [EVAL] batch:  138 | acc: 75.00%,  total acc: 66.50%   [EVAL] batch:  139 | acc: 68.75%,  total acc: 66.52%   [EVAL] batch:  140 | acc: 81.25%,  total acc: 66.62%   [EVAL] batch:  141 | acc: 81.25%,  total acc: 66.73%   [EVAL] batch:  142 | acc: 75.00%,  total acc: 66.78%   [EVAL] batch:  143 | acc: 75.00%,  total acc: 66.84%   [EVAL] batch:  144 | acc: 62.50%,  total acc: 66.81%   [EVAL] batch:  145 | acc: 75.00%,  total acc: 66.87%   [EVAL] batch:  146 | acc: 62.50%,  total acc: 66.84%   [EVAL] batch:  147 | acc: 81.25%,  total acc: 66.93%   [EVAL] batch:  148 | acc: 62.50%,  total acc: 66.90%   [EVAL] batch:  149 | acc: 43.75%,  total acc: 66.75%   [EVAL] batch:  150 | acc: 68.75%,  total acc: 66.76%   [EVAL] batch:  151 | acc: 75.00%,  total acc: 66.82%   [EVAL] batch:  152 | acc: 68.75%,  total acc: 66.83%   [EVAL] batch:  153 | acc: 87.50%,  total acc: 66.96%   [EVAL] batch:  154 | acc: 68.75%,  total acc: 66.98%   [EVAL] batch:  155 | acc: 56.25%,  total acc: 66.91%   [EVAL] batch:  156 | acc: 62.50%,  total acc: 66.88%   [EVAL] batch:  157 | acc: 50.00%,  total acc: 66.77%   [EVAL] batch:  158 | acc: 50.00%,  total acc: 66.67%   [EVAL] batch:  159 | acc: 50.00%,  total acc: 66.56%   [EVAL] batch:  160 | acc: 75.00%,  total acc: 66.61%   [EVAL] batch:  161 | acc: 50.00%,  total acc: 66.51%   [EVAL] batch:  162 | acc: 68.75%,  total acc: 66.53%   [EVAL] batch:  163 | acc: 62.50%,  total acc: 66.50%   [EVAL] batch:  164 | acc: 75.00%,  total acc: 66.55%   [EVAL] batch:  165 | acc: 75.00%,  total acc: 66.60%   [EVAL] batch:  166 | acc: 75.00%,  total acc: 66.65%   [EVAL] batch:  167 | acc: 75.00%,  total acc: 66.70%   [EVAL] batch:  168 | acc: 81.25%,  total acc: 66.79%   [EVAL] batch:  169 | acc: 43.75%,  total acc: 66.65%   [EVAL] batch:  170 | acc: 50.00%,  total acc: 66.56%   [EVAL] batch:  171 | acc: 25.00%,  total acc: 66.32%   [EVAL] batch:  172 | acc: 56.25%,  total acc: 66.26%   [EVAL] batch:  173 | acc: 31.25%,  total acc: 66.06%   [EVAL] batch:  174 | acc: 43.75%,  total acc: 65.93%   [EVAL] batch:  175 | acc: 100.00%,  total acc: 66.12%   [EVAL] batch:  176 | acc: 100.00%,  total acc: 66.31%   [EVAL] batch:  177 | acc: 100.00%,  total acc: 66.50%   [EVAL] batch:  178 | acc: 100.00%,  total acc: 66.69%   [EVAL] batch:  179 | acc: 100.00%,  total acc: 66.88%   [EVAL] batch:  180 | acc: 100.00%,  total acc: 67.06%   [EVAL] batch:  181 | acc: 100.00%,  total acc: 67.24%   [EVAL] batch:  182 | acc: 62.50%,  total acc: 67.21%   [EVAL] batch:  183 | acc: 93.75%,  total acc: 67.36%   [EVAL] batch:  184 | acc: 93.75%,  total acc: 67.50%   [EVAL] batch:  185 | acc: 87.50%,  total acc: 67.61%   [EVAL] batch:  186 | acc: 81.25%,  total acc: 67.68%   [EVAL] batch:  187 | acc: 37.50%,  total acc: 67.52%   [EVAL] batch:  188 | acc: 31.25%,  total acc: 67.33%   [EVAL] batch:  189 | acc: 43.75%,  total acc: 67.20%   [EVAL] batch:  190 | acc: 25.00%,  total acc: 66.98%   [EVAL] batch:  191 | acc: 50.00%,  total acc: 66.89%   [EVAL] batch:  192 | acc: 43.75%,  total acc: 66.77%   [EVAL] batch:  193 | acc: 31.25%,  total acc: 66.59%   [EVAL] batch:  194 | acc: 68.75%,  total acc: 66.60%   [EVAL] batch:  195 | acc: 62.50%,  total acc: 66.58%   [EVAL] batch:  196 | acc: 37.50%,  total acc: 66.43%   [EVAL] batch:  197 | acc: 56.25%,  total acc: 66.38%   [EVAL] batch:  198 | acc: 50.00%,  total acc: 66.30%   [EVAL] batch:  199 | acc: 75.00%,  total acc: 66.34%   [EVAL] batch:  200 | acc: 75.00%,  total acc: 66.39%   [EVAL] batch:  201 | acc: 75.00%,  total acc: 66.43%   [EVAL] batch:  202 | acc: 62.50%,  total acc: 66.41%   [EVAL] batch:  203 | acc: 75.00%,  total acc: 66.45%   [EVAL] batch:  204 | acc: 75.00%,  total acc: 66.49%   [EVAL] batch:  205 | acc: 93.75%,  total acc: 66.63%   [EVAL] batch:  206 | acc: 25.00%,  total acc: 66.43%   [EVAL] batch:  207 | acc: 18.75%,  total acc: 66.20%   [EVAL] batch:  208 | acc: 18.75%,  total acc: 65.97%   [EVAL] batch:  209 | acc: 6.25%,  total acc: 65.68%   [EVAL] batch:  210 | acc: 25.00%,  total acc: 65.49%   [EVAL] batch:  211 | acc: 31.25%,  total acc: 65.33%   [EVAL] batch:  212 | acc: 31.25%,  total acc: 65.17%   [EVAL] batch:  213 | acc: 87.50%,  total acc: 65.27%   [EVAL] batch:  214 | acc: 81.25%,  total acc: 65.35%   [EVAL] batch:  215 | acc: 56.25%,  total acc: 65.31%   [EVAL] batch:  216 | acc: 75.00%,  total acc: 65.35%   [EVAL] batch:  217 | acc: 75.00%,  total acc: 65.40%   [EVAL] batch:  218 | acc: 75.00%,  total acc: 65.44%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 65.60%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 65.75%   [EVAL] batch:  221 | acc: 100.00%,  total acc: 65.91%   [EVAL] batch:  222 | acc: 100.00%,  total acc: 66.06%   [EVAL] batch:  223 | acc: 100.00%,  total acc: 66.21%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 66.36%   [EVAL] batch:  225 | acc: 75.00%,  total acc: 66.40%   [EVAL] batch:  226 | acc: 68.75%,  total acc: 66.41%   [EVAL] batch:  227 | acc: 75.00%,  total acc: 66.45%   [EVAL] batch:  228 | acc: 62.50%,  total acc: 66.43%   [EVAL] batch:  229 | acc: 93.75%,  total acc: 66.55%   [EVAL] batch:  230 | acc: 68.75%,  total acc: 66.56%   [EVAL] batch:  231 | acc: 43.75%,  total acc: 66.46%   [EVAL] batch:  232 | acc: 43.75%,  total acc: 66.36%   [EVAL] batch:  233 | acc: 56.25%,  total acc: 66.32%   [EVAL] batch:  234 | acc: 31.25%,  total acc: 66.17%   [EVAL] batch:  235 | acc: 68.75%,  total acc: 66.18%   [EVAL] batch:  236 | acc: 43.75%,  total acc: 66.09%   [EVAL] batch:  237 | acc: 62.50%,  total acc: 66.07%   [EVAL] batch:  238 | acc: 87.50%,  total acc: 66.16%   [EVAL] batch:  239 | acc: 100.00%,  total acc: 66.30%   [EVAL] batch:  240 | acc: 81.25%,  total acc: 66.36%   [EVAL] batch:  241 | acc: 87.50%,  total acc: 66.45%   [EVAL] batch:  242 | acc: 81.25%,  total acc: 66.51%   [EVAL] batch:  243 | acc: 81.25%,  total acc: 66.57%   [EVAL] batch:  244 | acc: 75.00%,  total acc: 66.61%   [EVAL] batch:  245 | acc: 87.50%,  total acc: 66.69%   [EVAL] batch:  246 | acc: 62.50%,  total acc: 66.68%   [EVAL] batch:  247 | acc: 75.00%,  total acc: 66.71%   [EVAL] batch:  248 | acc: 68.75%,  total acc: 66.72%   [EVAL] batch:  249 | acc: 75.00%,  total acc: 66.75%   [EVAL] batch:  250 | acc: 62.50%,  total acc: 66.73%   [EVAL] batch:  251 | acc: 68.75%,  total acc: 66.74%   [EVAL] batch:  252 | acc: 75.00%,  total acc: 66.77%   [EVAL] batch:  253 | acc: 56.25%,  total acc: 66.73%   [EVAL] batch:  254 | acc: 75.00%,  total acc: 66.76%   [EVAL] batch:  255 | acc: 75.00%,  total acc: 66.80%   [EVAL] batch:  256 | acc: 62.50%,  total acc: 66.78%   [EVAL] batch:  257 | acc: 68.75%,  total acc: 66.79%   [EVAL] batch:  258 | acc: 68.75%,  total acc: 66.80%   [EVAL] batch:  259 | acc: 68.75%,  total acc: 66.80%   [EVAL] batch:  260 | acc: 50.00%,  total acc: 66.74%   [EVAL] batch:  261 | acc: 75.00%,  total acc: 66.77%   [EVAL] batch:  262 | acc: 62.50%,  total acc: 66.75%   [EVAL] batch:  263 | acc: 75.00%,  total acc: 66.79%   [EVAL] batch:  264 | acc: 62.50%,  total acc: 66.77%   [EVAL] batch:  265 | acc: 56.25%,  total acc: 66.73%   [EVAL] batch:  266 | acc: 56.25%,  total acc: 66.69%   [EVAL] batch:  267 | acc: 62.50%,  total acc: 66.67%   [EVAL] batch:  268 | acc: 93.75%,  total acc: 66.78%   [EVAL] batch:  269 | acc: 87.50%,  total acc: 66.85%   [EVAL] batch:  270 | acc: 100.00%,  total acc: 66.97%   [EVAL] batch:  271 | acc: 100.00%,  total acc: 67.10%   [EVAL] batch:  272 | acc: 100.00%,  total acc: 67.22%   [EVAL] batch:  273 | acc: 93.75%,  total acc: 67.31%   [EVAL] batch:  274 | acc: 100.00%,  total acc: 67.43%   [EVAL] batch:  275 | acc: 62.50%,  total acc: 67.41%   [EVAL] batch:  276 | acc: 37.50%,  total acc: 67.31%   [EVAL] batch:  277 | acc: 56.25%,  total acc: 67.27%   [EVAL] batch:  278 | acc: 68.75%,  total acc: 67.27%   [EVAL] batch:  279 | acc: 50.00%,  total acc: 67.21%   [EVAL] batch:  280 | acc: 56.25%,  total acc: 67.17%   [EVAL] batch:  281 | acc: 68.75%,  total acc: 67.18%   [EVAL] batch:  282 | acc: 62.50%,  total acc: 67.16%   [EVAL] batch:  283 | acc: 18.75%,  total acc: 66.99%   [EVAL] batch:  284 | acc: 37.50%,  total acc: 66.89%   [EVAL] batch:  285 | acc: 18.75%,  total acc: 66.72%   [EVAL] batch:  286 | acc: 18.75%,  total acc: 66.55%   [EVAL] batch:  287 | acc: 68.75%,  total acc: 66.56%   [EVAL] batch:  288 | acc: 62.50%,  total acc: 66.54%   [EVAL] batch:  289 | acc: 87.50%,  total acc: 66.62%   [EVAL] batch:  290 | acc: 62.50%,  total acc: 66.60%   [EVAL] batch:  291 | acc: 87.50%,  total acc: 66.67%   [EVAL] batch:  292 | acc: 75.00%,  total acc: 66.70%   [EVAL] batch:  293 | acc: 62.50%,  total acc: 66.69%   [EVAL] batch:  294 | acc: 68.75%,  total acc: 66.69%   [EVAL] batch:  295 | acc: 68.75%,  total acc: 66.70%   [EVAL] batch:  296 | acc: 62.50%,  total acc: 66.69%   [EVAL] batch:  297 | acc: 56.25%,  total acc: 66.65%   [EVAL] batch:  298 | acc: 62.50%,  total acc: 66.64%   [EVAL] batch:  299 | acc: 81.25%,  total acc: 66.69%   [EVAL] batch:  300 | acc: 100.00%,  total acc: 66.80%   [EVAL] batch:  301 | acc: 100.00%,  total acc: 66.91%   [EVAL] batch:  302 | acc: 100.00%,  total acc: 67.02%   [EVAL] batch:  303 | acc: 93.75%,  total acc: 67.11%   [EVAL] batch:  304 | acc: 100.00%,  total acc: 67.21%   [EVAL] batch:  305 | acc: 100.00%,  total acc: 67.32%   [EVAL] batch:  306 | acc: 100.00%,  total acc: 67.43%   [EVAL] batch:  307 | acc: 100.00%,  total acc: 67.53%   [EVAL] batch:  308 | acc: 93.75%,  total acc: 67.62%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 67.72%   [EVAL] batch:  310 | acc: 100.00%,  total acc: 67.83%   [EVAL] batch:  311 | acc: 100.00%,  total acc: 67.93%   [EVAL] batch:  312 | acc: 87.50%,  total acc: 67.99%   [EVAL] batch:  313 | acc: 62.50%,  total acc: 67.97%   [EVAL] batch:  314 | acc: 56.25%,  total acc: 67.94%   [EVAL] batch:  315 | acc: 87.50%,  total acc: 68.00%   [EVAL] batch:  316 | acc: 87.50%,  total acc: 68.06%   [EVAL] batch:  317 | acc: 93.75%,  total acc: 68.14%   [EVAL] batch:  318 | acc: 75.00%,  total acc: 68.16%   [EVAL] batch:  319 | acc: 93.75%,  total acc: 68.24%   [EVAL] batch:  320 | acc: 81.25%,  total acc: 68.28%   [EVAL] batch:  321 | acc: 81.25%,  total acc: 68.32%   [EVAL] batch:  322 | acc: 81.25%,  total acc: 68.36%   [EVAL] batch:  323 | acc: 87.50%,  total acc: 68.42%   [EVAL] batch:  324 | acc: 62.50%,  total acc: 68.40%   [EVAL] batch:  325 | acc: 18.75%,  total acc: 68.25%   [EVAL] batch:  326 | acc: 18.75%,  total acc: 68.10%   [EVAL] batch:  327 | acc: 12.50%,  total acc: 67.93%   [EVAL] batch:  328 | acc: 25.00%,  total acc: 67.80%   [EVAL] batch:  329 | acc: 37.50%,  total acc: 67.71%   [EVAL] batch:  330 | acc: 25.00%,  total acc: 67.58%   [EVAL] batch:  331 | acc: 68.75%,  total acc: 67.58%   [EVAL] batch:  332 | acc: 100.00%,  total acc: 67.68%   [EVAL] batch:  333 | acc: 100.00%,  total acc: 67.78%   [EVAL] batch:  334 | acc: 93.75%,  total acc: 67.85%   [EVAL] batch:  335 | acc: 100.00%,  total acc: 67.95%   [EVAL] batch:  336 | acc: 93.75%,  total acc: 68.03%   [EVAL] batch:  337 | acc: 56.25%,  total acc: 67.99%   [EVAL] batch:  338 | acc: 6.25%,  total acc: 67.81%   [EVAL] batch:  339 | acc: 12.50%,  total acc: 67.65%   [EVAL] batch:  340 | acc: 12.50%,  total acc: 67.49%   [EVAL] batch:  341 | acc: 12.50%,  total acc: 67.32%   [EVAL] batch:  342 | acc: 6.25%,  total acc: 67.15%   [EVAL] batch:  343 | acc: 31.25%,  total acc: 67.04%   [EVAL] batch:  344 | acc: 87.50%,  total acc: 67.10%   [EVAL] batch:  345 | acc: 93.75%,  total acc: 67.18%   [EVAL] batch:  346 | acc: 93.75%,  total acc: 67.26%   [EVAL] batch:  347 | acc: 87.50%,  total acc: 67.31%   [EVAL] batch:  348 | acc: 93.75%,  total acc: 67.39%   [EVAL] batch:  349 | acc: 100.00%,  total acc: 67.48%   [EVAL] batch:  350 | acc: 0.00%,  total acc: 67.29%   [EVAL] batch:  351 | acc: 0.00%,  total acc: 67.10%   [EVAL] batch:  352 | acc: 0.00%,  total acc: 66.91%   [EVAL] batch:  353 | acc: 0.00%,  total acc: 66.72%   [EVAL] batch:  354 | acc: 0.00%,  total acc: 66.53%   [EVAL] batch:  355 | acc: 0.00%,  total acc: 66.34%   [EVAL] batch:  356 | acc: 56.25%,  total acc: 66.32%   [EVAL] batch:  357 | acc: 81.25%,  total acc: 66.36%   [EVAL] batch:  358 | acc: 81.25%,  total acc: 66.40%   [EVAL] batch:  359 | acc: 68.75%,  total acc: 66.41%   [EVAL] batch:  360 | acc: 75.00%,  total acc: 66.43%   [EVAL] batch:  361 | acc: 75.00%,  total acc: 66.45%   [EVAL] batch:  362 | acc: 62.50%,  total acc: 66.44%   [EVAL] batch:  363 | acc: 25.00%,  total acc: 66.33%   [EVAL] batch:  364 | acc: 43.75%,  total acc: 66.27%   [EVAL] batch:  365 | acc: 25.00%,  total acc: 66.15%   [EVAL] batch:  366 | acc: 43.75%,  total acc: 66.09%   [EVAL] batch:  367 | acc: 43.75%,  total acc: 66.03%   [EVAL] batch:  368 | acc: 62.50%,  total acc: 66.02%   [EVAL] batch:  369 | acc: 93.75%,  total acc: 66.10%   [EVAL] batch:  370 | acc: 93.75%,  total acc: 66.17%   [EVAL] batch:  371 | acc: 81.25%,  total acc: 66.21%   [EVAL] batch:  372 | acc: 68.75%,  total acc: 66.22%   [EVAL] batch:  373 | acc: 81.25%,  total acc: 66.26%   [EVAL] batch:  374 | acc: 93.75%,  total acc: 66.33%   [EVAL] batch:  375 | acc: 93.75%,  total acc: 66.41%   [EVAL] batch:  376 | acc: 87.50%,  total acc: 66.46%   [EVAL] batch:  377 | acc: 81.25%,  total acc: 66.50%   [EVAL] batch:  378 | acc: 81.25%,  total acc: 66.54%   [EVAL] batch:  379 | acc: 93.75%,  total acc: 66.61%   [EVAL] batch:  380 | acc: 100.00%,  total acc: 66.70%   [EVAL] batch:  381 | acc: 87.50%,  total acc: 66.75%   [EVAL] batch:  382 | acc: 100.00%,  total acc: 66.84%   [EVAL] batch:  383 | acc: 100.00%,  total acc: 66.93%   [EVAL] batch:  384 | acc: 93.75%,  total acc: 67.00%   [EVAL] batch:  385 | acc: 87.50%,  total acc: 67.05%   [EVAL] batch:  386 | acc: 87.50%,  total acc: 67.10%   [EVAL] batch:  387 | acc: 81.25%,  total acc: 67.14%   [EVAL] batch:  388 | acc: 75.00%,  total acc: 67.16%   [EVAL] batch:  389 | acc: 75.00%,  total acc: 67.18%   [EVAL] batch:  390 | acc: 62.50%,  total acc: 67.17%   [EVAL] batch:  391 | acc: 62.50%,  total acc: 67.16%   [EVAL] batch:  392 | acc: 56.25%,  total acc: 67.13%   [EVAL] batch:  393 | acc: 56.25%,  total acc: 67.10%   [EVAL] batch:  394 | acc: 62.50%,  total acc: 67.09%   [EVAL] batch:  395 | acc: 43.75%,  total acc: 67.03%   [EVAL] batch:  396 | acc: 56.25%,  total acc: 67.00%   [EVAL] batch:  397 | acc: 68.75%,  total acc: 67.01%   [EVAL] batch:  398 | acc: 75.00%,  total acc: 67.03%   [EVAL] batch:  399 | acc: 62.50%,  total acc: 67.02%   [EVAL] batch:  400 | acc: 100.00%,  total acc: 67.10%   [EVAL] batch:  401 | acc: 100.00%,  total acc: 67.18%   [EVAL] batch:  402 | acc: 100.00%,  total acc: 67.26%   [EVAL] batch:  403 | acc: 100.00%,  total acc: 67.34%   [EVAL] batch:  404 | acc: 100.00%,  total acc: 67.42%   [EVAL] batch:  405 | acc: 100.00%,  total acc: 67.50%   [EVAL] batch:  406 | acc: 75.00%,  total acc: 67.52%   [EVAL] batch:  407 | acc: 75.00%,  total acc: 67.54%   [EVAL] batch:  408 | acc: 62.50%,  total acc: 67.53%   [EVAL] batch:  409 | acc: 75.00%,  total acc: 67.55%   [EVAL] batch:  410 | acc: 68.75%,  total acc: 67.55%   [EVAL] batch:  411 | acc: 62.50%,  total acc: 67.54%   [EVAL] batch:  412 | acc: 81.25%,  total acc: 67.57%   [EVAL] batch:  413 | acc: 100.00%,  total acc: 67.65%   [EVAL] batch:  414 | acc: 100.00%,  total acc: 67.73%   [EVAL] batch:  415 | acc: 100.00%,  total acc: 67.80%   [EVAL] batch:  416 | acc: 100.00%,  total acc: 67.88%   [EVAL] batch:  417 | acc: 100.00%,  total acc: 67.96%   [EVAL] batch:  418 | acc: 93.75%,  total acc: 68.02%   [EVAL] batch:  419 | acc: 100.00%,  total acc: 68.10%   [EVAL] batch:  420 | acc: 100.00%,  total acc: 68.17%   [EVAL] batch:  421 | acc: 100.00%,  total acc: 68.25%   [EVAL] batch:  422 | acc: 100.00%,  total acc: 68.32%   [EVAL] batch:  423 | acc: 100.00%,  total acc: 68.40%   [EVAL] batch:  424 | acc: 100.00%,  total acc: 68.47%   [EVAL] batch:  425 | acc: 87.50%,  total acc: 68.52%   [EVAL] batch:  426 | acc: 93.75%,  total acc: 68.57%   [EVAL] batch:  427 | acc: 93.75%,  total acc: 68.63%   [EVAL] batch:  428 | acc: 93.75%,  total acc: 68.69%   [EVAL] batch:  429 | acc: 93.75%,  total acc: 68.75%   [EVAL] batch:  430 | acc: 100.00%,  total acc: 68.82%   [EVAL] batch:  431 | acc: 93.75%,  total acc: 68.88%   [EVAL] batch:  432 | acc: 93.75%,  total acc: 68.94%   [EVAL] batch:  433 | acc: 87.50%,  total acc: 68.98%   [EVAL] batch:  434 | acc: 100.00%,  total acc: 69.05%   [EVAL] batch:  435 | acc: 100.00%,  total acc: 69.12%   [EVAL] batch:  436 | acc: 93.75%,  total acc: 69.18%   [EVAL] batch:  437 | acc: 50.00%,  total acc: 69.14%   
cur_acc:  ['0.9494', '0.6151', '0.7143', '0.6885', '0.7510', '0.6478', '0.8581']
his_acc:  ['0.9494', '0.7760', '0.7500', '0.7332', '0.7145', '0.6875', '0.6914']
Clustering into  39  clusters
Clusters:  [31  1  9 25 20 28  9 18  1  8 20 16 10 34  3 19 10  1 10  1 36 11 18 20
 24  8 22 28 13 32  5 10 18 16 33 15  2 26 37 26 29 21  2  9 11 17  1 12
 38  7 22 16  3  4  3  6 10 22 30 14 25 23  4  1 27 12  5  6 33  5 17 13
 26  0  0 13 26  7  8 35]
Losses:  9.031094551086426 3.3870816230773926 0.5746867656707764
CurrentTrain: epoch  0, batch     0 | loss: 9.0310946Losses:  12.121732711791992 3.9570765495300293 0.5379120111465454
CurrentTrain: epoch  0, batch     1 | loss: 12.1217327Losses:  9.944901466369629 3.1849021911621094 0.579415500164032
CurrentTrain: epoch  0, batch     2 | loss: 9.9449015Losses:  5.5419111251831055 -0.0 0.11309827864170074
CurrentTrain: epoch  0, batch     3 | loss: 5.5419111Losses:  9.91788387298584 3.7810885906219482 0.554031491279602
CurrentTrain: epoch  1, batch     0 | loss: 9.9178839Losses:  10.576136589050293 4.1900224685668945 0.4490554928779602
CurrentTrain: epoch  1, batch     1 | loss: 10.5761366Losses:  8.283849716186523 2.7661678791046143 0.5938251614570618
CurrentTrain: epoch  1, batch     2 | loss: 8.2838497Losses:  3.0285840034484863 -0.0 0.10869675874710083
CurrentTrain: epoch  1, batch     3 | loss: 3.0285840Losses:  7.823121547698975 3.640692710876465 0.5259608030319214
CurrentTrain: epoch  2, batch     0 | loss: 7.8231215Losses:  9.234395980834961 3.637852668762207 0.6324632167816162
CurrentTrain: epoch  2, batch     1 | loss: 9.2343960Losses:  9.749948501586914 3.7103452682495117 0.594355583190918
CurrentTrain: epoch  2, batch     2 | loss: 9.7499485Losses:  5.556701183319092 -0.0 0.10529645532369614
CurrentTrain: epoch  2, batch     3 | loss: 5.5567012Losses:  9.438565254211426 3.99214243888855 0.5766929388046265
CurrentTrain: epoch  3, batch     0 | loss: 9.4385653Losses:  8.327686309814453 3.415353775024414 0.5775440335273743
CurrentTrain: epoch  3, batch     1 | loss: 8.3276863Losses:  6.8470001220703125 3.02046537399292 0.49824637174606323
CurrentTrain: epoch  3, batch     2 | loss: 6.8470001Losses:  3.0410349369049072 -0.0 0.14894583821296692
CurrentTrain: epoch  3, batch     3 | loss: 3.0410349Losses:  6.389304161071777 3.2639102935791016 0.517810583114624
CurrentTrain: epoch  4, batch     0 | loss: 6.3893042Losses:  9.543980598449707 4.403565406799316 0.518345057964325
CurrentTrain: epoch  4, batch     1 | loss: 9.5439806Losses:  7.6756415367126465 3.285144805908203 0.5614333152770996
CurrentTrain: epoch  4, batch     2 | loss: 7.6756415Losses:  4.204032897949219 -0.0 0.11303605139255524
CurrentTrain: epoch  4, batch     3 | loss: 4.2040329Losses:  6.155339241027832 2.4796061515808105 0.6117776036262512
CurrentTrain: epoch  5, batch     0 | loss: 6.1553392Losses:  6.462459564208984 2.717853546142578 0.40548524260520935
CurrentTrain: epoch  5, batch     1 | loss: 6.4624596Losses:  6.559991836547852 2.8600480556488037 0.5247241854667664
CurrentTrain: epoch  5, batch     2 | loss: 6.5599918Losses:  2.7041401863098145 -0.0 0.13795706629753113
CurrentTrain: epoch  5, batch     3 | loss: 2.7041402Losses:  8.021379470825195 3.712841510772705 0.5431827902793884
CurrentTrain: epoch  6, batch     0 | loss: 8.0213795Losses:  6.346481800079346 2.736909866333008 0.587663471698761
CurrentTrain: epoch  6, batch     1 | loss: 6.3464818Losses:  5.828549861907959 3.3229928016662598 0.5041459798812866
CurrentTrain: epoch  6, batch     2 | loss: 5.8285499Losses:  3.4973065853118896 -0.0 0.09025472402572632
CurrentTrain: epoch  6, batch     3 | loss: 3.4973066Losses:  6.968276023864746 3.480672597885132 0.5817793607711792
CurrentTrain: epoch  7, batch     0 | loss: 6.9682760Losses:  5.61582088470459 2.1392455101013184 0.5814036726951599
CurrentTrain: epoch  7, batch     1 | loss: 5.6158209Losses:  6.1643195152282715 3.4449944496154785 0.5475984215736389
CurrentTrain: epoch  7, batch     2 | loss: 6.1643195Losses:  4.4110565185546875 -0.0 0.11751740425825119
CurrentTrain: epoch  7, batch     3 | loss: 4.4110565Losses:  5.116141319274902 1.9861392974853516 0.5690140724182129
CurrentTrain: epoch  8, batch     0 | loss: 5.1161413Losses:  5.65695333480835 2.40364933013916 0.561691403388977
CurrentTrain: epoch  8, batch     1 | loss: 5.6569533Losses:  5.263280868530273 2.599856376647949 0.49946093559265137
CurrentTrain: epoch  8, batch     2 | loss: 5.2632809Losses:  2.045459508895874 -0.0 0.13307619094848633
CurrentTrain: epoch  8, batch     3 | loss: 2.0454595Losses:  5.848959922790527 3.0570688247680664 0.5508345365524292
CurrentTrain: epoch  9, batch     0 | loss: 5.8489599Losses:  5.150690078735352 2.5783040523529053 0.5629464983940125
CurrentTrain: epoch  9, batch     1 | loss: 5.1506901Losses:  4.301261901855469 1.57671058177948 0.5542056560516357
CurrentTrain: epoch  9, batch     2 | loss: 4.3012619Losses:  3.032292604446411 -0.0 0.09070752561092377
CurrentTrain: epoch  9, batch     3 | loss: 3.0322926
#### follows ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "follows" means immediately prior item in a series of which the subject is a part.
Here is an example:
Relation: follows
Context: the nikon d5500 features other improvements over its predecessor nikon d5300 .
Head Entity: nikon d5500
Tail Entity: nikon d5300
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "follows":
Relation: follows  
Context: in the series of popular smartphone models, the iphone 12 follows the iphone 11, which was released the previous year.  
Head Entity: iphone 12  
Tail Entity: iphone 11  

Relation: follows  
Context: the latest installment in the series, the harry potter and the deathly hallows, follows the previous book, harry potter and the half-blood prince.  
Head Entity: harry potter and the deathly hallows  
Tail Entity: harry potter and the half-blood prince  
#### sport ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "sport" means sport that the subject participates or participated in or is associated with.
Here is an example:
Relation: sport
Context: it is primarily used for baseball and serves as the home stadium for yaquis de obregón .
Head Entity: yaquis de obregón
Tail Entity: baseball
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "sport":
Relation: sport  
Context: She has been a professional player in the world of tennis, winning several championships throughout her career.  
Head Entity: She  
Tail Entity: tennis  

Relation: sport  
Context: The city is known for its vibrant soccer culture, with numerous teams competing in the national league.  
Head Entity: The city  
Tail Entity: soccer  
#### father ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "father" means male parent of the subject.
Here is an example:
Relation: father
Context: after the crowning of his nephew radoslav , the son of stefan , sava left the serbian maritime in 1229 for a trip to palestine .
Head Entity: radoslav
Tail Entity: stefan
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "father":
Relation: father  
Context: in the historical records, it is noted that king henry was the father of prince edward, who later became a significant figure in the kingdom.  
Head Entity: prince edward  
Tail Entity: king henry  

Relation: father  
Context: during the family reunion, it was revealed that michael was the father of lucas, who had recently graduated from university.  
Head Entity: lucas  
Tail Entity: michael  
#### original network ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "original network" means network or service that originally broadcast a radio or television program.
Here is an example:
Relation: original network
Context: since 2009 , culshaw has starred in the bbc one comedy sketch show " the impressions show " alongside debra stephenson .
Head Entity: the impressions show
Tail Entity: bbc one
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "original network":
Relation: original network  
Context: The popular series "Breaking Bad" first aired on AMC, captivating audiences with its intense storytelling and character development.  
Head Entity: Breaking Bad  
Tail Entity: AMC  

Relation: original network  
Context: "Friends" became a cultural phenomenon after it premiered on NBC, showcasing the lives of six friends living in New York City.  
Head Entity: Friends  
Tail Entity: NBC  
#### original language of film or TV show ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "original language of film or TV show" means language in which a film or a performance work was originally created.
Here is an example:
Relation: original language of film or TV show
Context: the 1954 film , directed by kadri venkata reddy , " peddamanushulu " was honoured with the first president 's silver medal for best feature film in telugu .
Head Entity: peddamanushulu
Tail Entity: telugu
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "original language of film or TV show":
Relation: original language of film or TV show  
Context: The 1994 animated film "The Lion King," produced by Walt Disney Feature Animation, was originally created in English.  
Head Entity: The Lion King  
Tail Entity: English  

Relation: original language of film or TV show  
Context: The critically acclaimed 2001 film "Amélie," directed by Jean-Pierre Jeunet, was originally filmed in French.  
Head Entity: Amélie  
Tail Entity: French  
#### league ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "league" means league in which team or player plays or has played in.
Here is an example:
Relation: league
Context: in the summer of 2013 he returned to montenegro and joined with fk rudar pljevlja , playing with the club for two seasons in the montenegrin first league .
Head Entity: fk rudar pljevlja
Tail Entity: montenegrin first league
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "league":
Relation: league  
Context: After a successful stint in the English Premier League, the player signed with FC Barcelona, competing in La Liga for the next three seasons.  
Head Entity: FC Barcelona  
Tail Entity: La Liga  

Relation: league  
Context: The young athlete made headlines when he joined the New York Yankees, showcasing his talent in Major League Baseball.  
Head Entity: New York Yankees  
Tail Entity: Major League Baseball  
#### mother ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "mother" means female parent of the subject.
Here is an example:
Relation: mother
Context: the first stewart king of scotland and son of marjorie bruce and walter stewart , robert ii , is believed to have been born in the abbey .
Head Entity: robert ii
Tail Entity: marjorie bruce
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "mother":
Relation: mother  
Context: in the famous tale, cinderella's mother passed away when she was young, leaving her in the care of her cruel stepmother.  
Head Entity: cinderella  
Tail Entity: cinderella's mother  

Relation: mother  
Context: the renowned scientist marie curie was inspired by her mother, who was a teacher and a strong advocate for education.  
Head Entity: marie curie  
Tail Entity: marie curie's mother  
#### instrument ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "instrument" means musical instrument that a person plays or teaches or used in a music occupation.
Here is an example:
Relation: instrument
Context: peter bastian ( 25 august 1943 in copenhagen – 27 march 2017 ) was a danish musician . peter bastian bio and cv he was educated in theoretical physics and classical bassoon .
Head Entity: peter bastian
Tail Entity: bassoon
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "instrument":
Relation: instrument  
Context: johann sebastian bach (31 march 1685 – 28 july 1750) was a german composer and musician of the baroque period. he was known for his exceptional skill in playing the organ and the harpsichord.  
Head Entity: johann sebastian bach  
Tail Entity: organ  

Relation: instrument  
Context: yo-yo ma, born on october 7, 1955, is a world-renowned cellist who has performed with numerous orchestras and in various genres. he has dedicated his life to music and is known for his mastery of the cello.  
Head Entity: yo-yo ma  
Tail Entity: cello  
#### country ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "country" means sovereign state of this item (not to be used for human beings).
Here is an example:
Relation: country
Context: the type locality given is " fan - si - pan mountains , tonkin , indo - china " ( mount fansipan , sa pa district , nghệ an province , northwestern vietnam ) .
Head Entity: sa pa district
Tail Entity: vietnam
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "country":
Relation: country  
Context: the capital city of the country is located in the central region, known for its rich history and cultural heritage, particularly in the city of "hanoi" which serves as the political center of the nation.  
Head Entity: hanoi  
Tail Entity: vietnam  

Relation: country  
Context: the famous landmark "eiffel tower" is situated in the heart of the city, which is the capital of a well-known european country famous for its art, fashion, and cuisine.  
Head Entity: paris  
Tail Entity: france  
#### characters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "characters" means characters which appear in this item (like plays, operas, operettas, books, comics, films, TV series, video games).
Here is an example:
Relation: characters
Context: klaus kinski , who would play dracula himself nine years later in " nosferatu the vampyre " , is also featured in the film as renfield .
Head Entity: nosferatu the vampyre
Tail Entity: renfield
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "characters":
Relation: characters  
Context: in the animated series "avatar: the last airbender," zuko is a complex character who struggles with his identity and family legacy.  
Head Entity: avatar: the last airbender  
Tail Entity: zuko  

Relation: characters  
Context: the novel "pride and prejudice" features elizabeth bennet, a strong-willed protagonist who navigates societal expectations and personal relationships.  
Head Entity: pride and prejudice  
Tail Entity: elizabeth bennet  
Losses:  4.057644367218018 0.5107575058937073 0.9906637072563171
MemoryTrain:  epoch  0, batch     0 | loss: 4.0576444Losses:  3.3217670917510986 0.24105581641197205 0.8989471793174744
MemoryTrain:  epoch  0, batch     1 | loss: 3.3217671Losses:  4.261358261108398 0.5056716799736023 0.9192328453063965
MemoryTrain:  epoch  0, batch     2 | loss: 4.2613583Losses:  3.744300365447998 -0.0 1.1230714321136475
MemoryTrain:  epoch  0, batch     3 | loss: 3.7443004Losses:  3.5886406898498535 -0.0 0.9697719812393188
MemoryTrain:  epoch  0, batch     4 | loss: 3.5886407Losses:  2.996002674102783 -0.0 1.066353678703308
MemoryTrain:  epoch  0, batch     5 | loss: 2.9960027Losses:  2.8036954402923584 -0.0 0.9731987118721008
MemoryTrain:  epoch  0, batch     6 | loss: 2.8036954Losses:  2.8394389152526855 0.5070315003395081 0.7409933805465698
MemoryTrain:  epoch  0, batch     7 | loss: 2.8394389Losses:  4.244729995727539 0.28347617387771606 0.9625344276428223
MemoryTrain:  epoch  0, batch     8 | loss: 4.2447300Losses:  3.1875908374786377 0.5316434502601624 0.9620282649993896
MemoryTrain:  epoch  0, batch     9 | loss: 3.1875908Losses:  3.621187210083008 -0.0 1.0136622190475464
MemoryTrain:  epoch  0, batch    10 | loss: 3.6211872Losses:  3.8967177867889404 0.5540949106216431 1.0574696063995361
MemoryTrain:  epoch  0, batch    11 | loss: 3.8967178Losses:  3.5604004859924316 0.577570915222168 0.9715177416801453
MemoryTrain:  epoch  0, batch    12 | loss: 3.5604005Losses:  3.8801486492156982 -0.0 0.9202591776847839
MemoryTrain:  epoch  0, batch    13 | loss: 3.8801486Losses:  3.44108247756958 0.24922458827495575 1.048327922821045
MemoryTrain:  epoch  0, batch    14 | loss: 3.4410825Losses:  2.8366804122924805 0.26636701822280884 1.0447747707366943
MemoryTrain:  epoch  1, batch     0 | loss: 2.8366804Losses:  3.3131954669952393 -0.0 1.0553853511810303
MemoryTrain:  epoch  1, batch     1 | loss: 3.3131955Losses:  3.789219379425049 0.5095138549804688 0.9601894021034241
MemoryTrain:  epoch  1, batch     2 | loss: 3.7892194Losses:  3.776169776916504 0.2523876130580902 1.0628008842468262
MemoryTrain:  epoch  1, batch     3 | loss: 3.7761698Losses:  2.985164165496826 -0.0 1.0767492055892944
MemoryTrain:  epoch  1, batch     4 | loss: 2.9851642Losses:  3.757265090942383 0.26318806409835815 1.0292022228240967
MemoryTrain:  epoch  1, batch     5 | loss: 3.7572651Losses:  3.6848204135894775 0.8352949619293213 0.8857619762420654
MemoryTrain:  epoch  1, batch     6 | loss: 3.6848204Losses:  2.7880265712738037 -0.0 0.8941555023193359
MemoryTrain:  epoch  1, batch     7 | loss: 2.7880266Losses:  3.350778579711914 0.2595967650413513 0.9689081907272339
MemoryTrain:  epoch  1, batch     8 | loss: 3.3507786Losses:  3.1441378593444824 0.22701925039291382 0.9938240647315979
MemoryTrain:  epoch  1, batch     9 | loss: 3.1441379Losses:  2.9251627922058105 -0.0 1.037937045097351
MemoryTrain:  epoch  1, batch    10 | loss: 2.9251628Losses:  2.346374034881592 -0.0 0.886093020439148
MemoryTrain:  epoch  1, batch    11 | loss: 2.3463740Losses:  2.6356945037841797 -0.0 1.0450544357299805
MemoryTrain:  epoch  1, batch    12 | loss: 2.6356945Losses:  4.364310264587402 0.5354732871055603 0.9321016669273376
MemoryTrain:  epoch  1, batch    13 | loss: 4.3643103Losses:  3.498231887817383 0.5528518557548523 0.8503867387771606
MemoryTrain:  epoch  1, batch    14 | loss: 3.4982319Losses:  3.771064519882202 0.535419225692749 0.8660283088684082
MemoryTrain:  epoch  2, batch     0 | loss: 3.7710645Losses:  2.6025471687316895 0.2508837580680847 0.9101390838623047
MemoryTrain:  epoch  2, batch     1 | loss: 2.6025472Losses:  3.02962589263916 0.5114433169364929 0.9893043637275696
MemoryTrain:  epoch  2, batch     2 | loss: 3.0296259Losses:  3.056610345840454 0.4921819865703583 0.9703946113586426
MemoryTrain:  epoch  2, batch     3 | loss: 3.0566103Losses:  3.0736303329467773 0.24306514859199524 0.9186059832572937
MemoryTrain:  epoch  2, batch     4 | loss: 3.0736303Losses:  4.288126468658447 1.3567452430725098 0.8017687201499939
MemoryTrain:  epoch  2, batch     5 | loss: 4.2881265Losses:  2.782876968383789 0.23450163006782532 1.0578155517578125
MemoryTrain:  epoch  2, batch     6 | loss: 2.7828770Losses:  2.7648367881774902 0.2760624289512634 1.0182421207427979
MemoryTrain:  epoch  2, batch     7 | loss: 2.7648368Losses:  2.6034202575683594 -0.0 1.0095852613449097
MemoryTrain:  epoch  2, batch     8 | loss: 2.6034203Losses:  3.001236915588379 0.2507140040397644 0.9324125051498413
MemoryTrain:  epoch  2, batch     9 | loss: 3.0012369Losses:  3.0777647495269775 -0.0 0.9848568439483643
MemoryTrain:  epoch  2, batch    10 | loss: 3.0777647Losses:  3.9577009677886963 1.103147268295288 0.9010998606681824
MemoryTrain:  epoch  2, batch    11 | loss: 3.9577010Losses:  2.685631036758423 -0.0 1.0357435941696167
MemoryTrain:  epoch  2, batch    12 | loss: 2.6856310Losses:  3.0877771377563477 0.4894266128540039 0.8905420303344727
MemoryTrain:  epoch  2, batch    13 | loss: 3.0877771Losses:  2.6748971939086914 0.2466694414615631 0.9525599479675293
MemoryTrain:  epoch  2, batch    14 | loss: 2.6748972Losses:  2.630911350250244 0.24414482712745667 1.0750845670700073
MemoryTrain:  epoch  3, batch     0 | loss: 2.6309114Losses:  3.9585633277893066 0.7787869572639465 0.9717317819595337
MemoryTrain:  epoch  3, batch     1 | loss: 3.9585633Losses:  3.5429697036743164 0.23778055608272552 1.022705316543579
MemoryTrain:  epoch  3, batch     2 | loss: 3.5429697Losses:  2.8449270725250244 0.22655990719795227 1.0059585571289062
MemoryTrain:  epoch  3, batch     3 | loss: 2.8449271Losses:  2.87859845161438 0.52073073387146 0.9013740420341492
MemoryTrain:  epoch  3, batch     4 | loss: 2.8785985Losses:  2.927281379699707 -0.0 1.0606788396835327
MemoryTrain:  epoch  3, batch     5 | loss: 2.9272814Losses:  3.0524749755859375 0.5040541291236877 0.959098756313324
MemoryTrain:  epoch  3, batch     6 | loss: 3.0524750Losses:  2.54726505279541 0.25746339559555054 1.0232921838760376
MemoryTrain:  epoch  3, batch     7 | loss: 2.5472651Losses:  2.975433111190796 0.5398589372634888 0.8457000255584717
MemoryTrain:  epoch  3, batch     8 | loss: 2.9754331Losses:  3.1938517093658447 0.2649073898792267 0.9321858286857605
MemoryTrain:  epoch  3, batch     9 | loss: 3.1938517Losses:  2.289246082305908 -0.0 0.8516974449157715
MemoryTrain:  epoch  3, batch    10 | loss: 2.2892461Losses:  2.475339889526367 -0.0 0.9822054505348206
MemoryTrain:  epoch  3, batch    11 | loss: 2.4753399Losses:  2.30631160736084 -0.0 0.9742312431335449
MemoryTrain:  epoch  3, batch    12 | loss: 2.3063116Losses:  3.3633482456207275 1.0962460041046143 0.9118465781211853
MemoryTrain:  epoch  3, batch    13 | loss: 3.3633482Losses:  2.472165822982788 0.24890433251857758 0.9747678637504578
MemoryTrain:  epoch  3, batch    14 | loss: 2.4721658Losses:  2.897125244140625 0.5109933614730835 0.9104814529418945
MemoryTrain:  epoch  4, batch     0 | loss: 2.8971252Losses:  2.4614882469177246 0.24326679110527039 0.9606298208236694
MemoryTrain:  epoch  4, batch     1 | loss: 2.4614882Losses:  2.6014935970306396 -0.0 0.999129056930542
MemoryTrain:  epoch  4, batch     2 | loss: 2.6014936Losses:  2.748356819152832 0.2581450343132019 0.931303858757019
MemoryTrain:  epoch  4, batch     3 | loss: 2.7483568Losses:  2.4557909965515137 -0.0 1.0582449436187744
MemoryTrain:  epoch  4, batch     4 | loss: 2.4557910Losses:  2.6490604877471924 0.5128411650657654 0.8427603840827942
MemoryTrain:  epoch  4, batch     5 | loss: 2.6490605Losses:  3.0582637786865234 0.4714038074016571 0.9774349927902222
MemoryTrain:  epoch  4, batch     6 | loss: 3.0582638Losses:  2.2587008476257324 -0.0 1.0236899852752686
MemoryTrain:  epoch  4, batch     7 | loss: 2.2587008Losses:  2.4307608604431152 -0.0 1.1155428886413574
MemoryTrain:  epoch  4, batch     8 | loss: 2.4307609Losses:  3.3503129482269287 0.27542126178741455 1.0683319568634033
MemoryTrain:  epoch  4, batch     9 | loss: 3.3503129Losses:  2.828381061553955 0.5148618817329407 0.9787989854812622
MemoryTrain:  epoch  4, batch    10 | loss: 2.8283811Losses:  4.1975932121276855 1.1512032747268677 0.8465703129768372
MemoryTrain:  epoch  4, batch    11 | loss: 4.1975932Losses:  2.730358600616455 0.5254517793655396 0.7482364773750305
MemoryTrain:  epoch  4, batch    12 | loss: 2.7303586Losses:  2.6606500148773193 0.2814030051231384 0.9674025177955627
MemoryTrain:  epoch  4, batch    13 | loss: 2.6606500Losses:  2.8735456466674805 0.553398847579956 1.0046049356460571
MemoryTrain:  epoch  4, batch    14 | loss: 2.8735456Losses:  2.747648239135742 0.4903562068939209 0.8999072909355164
MemoryTrain:  epoch  5, batch     0 | loss: 2.7476482Losses:  2.2539501190185547 -0.0 1.0039186477661133
MemoryTrain:  epoch  5, batch     1 | loss: 2.2539501Losses:  3.1665194034576416 0.2509060204029083 1.0109295845031738
MemoryTrain:  epoch  5, batch     2 | loss: 3.1665194Losses:  2.819690227508545 0.24109649658203125 1.0222153663635254
MemoryTrain:  epoch  5, batch     3 | loss: 2.8196902Losses:  2.669548988342285 0.2778729200363159 1.034767508506775
MemoryTrain:  epoch  5, batch     4 | loss: 2.6695490Losses:  2.402312755584717 0.25191211700439453 0.8490611910820007
MemoryTrain:  epoch  5, batch     5 | loss: 2.4023128Losses:  2.654539108276367 0.24124610424041748 1.0753200054168701
MemoryTrain:  epoch  5, batch     6 | loss: 2.6545391Losses:  3.188802719116211 0.27412426471710205 0.9857235550880432
MemoryTrain:  epoch  5, batch     7 | loss: 3.1888027Losses:  2.7380571365356445 0.2521965205669403 0.9040489196777344
MemoryTrain:  epoch  5, batch     8 | loss: 2.7380571Losses:  2.578092098236084 0.2586406469345093 0.9196585416793823
MemoryTrain:  epoch  5, batch     9 | loss: 2.5780921Losses:  2.7343764305114746 -0.0 1.0170716047286987
MemoryTrain:  epoch  5, batch    10 | loss: 2.7343764Losses:  2.901653289794922 0.4876711964607239 0.7848309278488159
MemoryTrain:  epoch  5, batch    11 | loss: 2.9016533Losses:  2.516305923461914 -0.0 1.0043213367462158
MemoryTrain:  epoch  5, batch    12 | loss: 2.5163059Losses:  2.5508999824523926 -0.0 1.015816330909729
MemoryTrain:  epoch  5, batch    13 | loss: 2.5509000Losses:  2.54241943359375 -0.0 1.1020952463150024
MemoryTrain:  epoch  5, batch    14 | loss: 2.5424194Losses:  2.250466823577881 -0.0 0.9755467176437378
MemoryTrain:  epoch  6, batch     0 | loss: 2.2504668Losses:  2.757434844970703 0.24004478752613068 1.005553126335144
MemoryTrain:  epoch  6, batch     1 | loss: 2.7574348Losses:  2.730822801589966 0.2741298973560333 0.9056609869003296
MemoryTrain:  epoch  6, batch     2 | loss: 2.7308228Losses:  2.332853317260742 -0.0 1.020761489868164
MemoryTrain:  epoch  6, batch     3 | loss: 2.3328533Losses:  2.639399528503418 0.24670149385929108 1.054114580154419
MemoryTrain:  epoch  6, batch     4 | loss: 2.6393995Losses:  2.343418598175049 -0.0 1.0312471389770508
MemoryTrain:  epoch  6, batch     5 | loss: 2.3434186Losses:  2.6676833629608154 0.4939362108707428 0.9167119860649109
MemoryTrain:  epoch  6, batch     6 | loss: 2.6676834Losses:  2.600898265838623 0.24248433113098145 1.0321118831634521
MemoryTrain:  epoch  6, batch     7 | loss: 2.6008983Losses:  2.9010305404663086 0.2438887655735016 0.8907198905944824
MemoryTrain:  epoch  6, batch     8 | loss: 2.9010305Losses:  2.5689616203308105 -0.0 0.956882119178772
MemoryTrain:  epoch  6, batch     9 | loss: 2.5689616Losses:  2.64333438873291 0.28539568185806274 0.9885979294776917
MemoryTrain:  epoch  6, batch    10 | loss: 2.6433344Losses:  2.3345935344696045 -0.0 0.9210230708122253
MemoryTrain:  epoch  6, batch    11 | loss: 2.3345935Losses:  2.1350698471069336 -0.0 0.9196723699569702
MemoryTrain:  epoch  6, batch    12 | loss: 2.1350698Losses:  2.412398338317871 0.24154400825500488 0.9585233926773071
MemoryTrain:  epoch  6, batch    13 | loss: 2.4123983Losses:  3.2099716663360596 0.29753613471984863 1.0266790390014648
MemoryTrain:  epoch  6, batch    14 | loss: 3.2099717Losses:  2.5182301998138428 -0.0 0.9675780534744263
MemoryTrain:  epoch  7, batch     0 | loss: 2.5182302Losses:  2.550541877746582 0.2549065053462982 1.0136514902114868
MemoryTrain:  epoch  7, batch     1 | loss: 2.5505419Losses:  2.4518229961395264 0.2502825856208801 0.9512090086936951
MemoryTrain:  epoch  7, batch     2 | loss: 2.4518230Losses:  2.2518632411956787 -0.0 0.9475767016410828
MemoryTrain:  epoch  7, batch     3 | loss: 2.2518632Losses:  2.4858455657958984 0.2491578459739685 0.9743505716323853
MemoryTrain:  epoch  7, batch     4 | loss: 2.4858456Losses:  3.4132866859436035 0.5373563766479492 0.9004312753677368
MemoryTrain:  epoch  7, batch     5 | loss: 3.4132867Losses:  2.397481679916382 0.2637215852737427 0.8681542277336121
MemoryTrain:  epoch  7, batch     6 | loss: 2.3974817Losses:  2.3744289875030518 -0.0 0.9662773609161377
MemoryTrain:  epoch  7, batch     7 | loss: 2.3744290Losses:  2.393587589263916 -0.0 0.9737263321876526
MemoryTrain:  epoch  7, batch     8 | loss: 2.3935876Losses:  2.735382318496704 0.5015425682067871 0.9234625101089478
MemoryTrain:  epoch  7, batch     9 | loss: 2.7353823Losses:  2.4522604942321777 0.24497728049755096 0.9637736082077026
MemoryTrain:  epoch  7, batch    10 | loss: 2.4522605Losses:  2.525484561920166 0.2554171085357666 1.0217084884643555
MemoryTrain:  epoch  7, batch    11 | loss: 2.5254846Losses:  2.223707914352417 -0.0 0.965714693069458
MemoryTrain:  epoch  7, batch    12 | loss: 2.2237079Losses:  2.6515696048736572 0.24927040934562683 0.8733919858932495
MemoryTrain:  epoch  7, batch    13 | loss: 2.6515696Losses:  2.6391918659210205 0.231126070022583 0.9136983752250671
MemoryTrain:  epoch  7, batch    14 | loss: 2.6391919Losses:  2.2299768924713135 -0.0 0.9508724808692932
MemoryTrain:  epoch  8, batch     0 | loss: 2.2299769Losses:  2.7913150787353516 0.5035637617111206 0.9040255546569824
MemoryTrain:  epoch  8, batch     1 | loss: 2.7913151Losses:  2.516179323196411 0.25110507011413574 1.0265331268310547
MemoryTrain:  epoch  8, batch     2 | loss: 2.5161793Losses:  2.518186092376709 0.24635466933250427 1.0179243087768555
MemoryTrain:  epoch  8, batch     3 | loss: 2.5181861Losses:  2.347581624984741 -0.0 0.9098904132843018
MemoryTrain:  epoch  8, batch     4 | loss: 2.3475816Losses:  2.189469337463379 -0.0 0.9401981830596924
MemoryTrain:  epoch  8, batch     5 | loss: 2.1894693Losses:  2.284243583679199 0.2278681993484497 0.8480318784713745
MemoryTrain:  epoch  8, batch     6 | loss: 2.2842436Losses:  2.584054946899414 0.5038065910339355 0.8367341160774231
MemoryTrain:  epoch  8, batch     7 | loss: 2.5840549Losses:  2.552590847015381 0.24334442615509033 1.072070598602295
MemoryTrain:  epoch  8, batch     8 | loss: 2.5525908Losses:  2.475236415863037 0.27076536417007446 0.8972853422164917
MemoryTrain:  epoch  8, batch     9 | loss: 2.4752364Losses:  2.5677671432495117 0.26684969663619995 0.9872240424156189
MemoryTrain:  epoch  8, batch    10 | loss: 2.5677671Losses:  2.8848724365234375 0.5147761702537537 1.031347632408142
MemoryTrain:  epoch  8, batch    11 | loss: 2.8848724Losses:  2.624319076538086 0.4642806053161621 0.9003068208694458
MemoryTrain:  epoch  8, batch    12 | loss: 2.6243191Losses:  2.555906295776367 0.271815687417984 1.041962742805481
MemoryTrain:  epoch  8, batch    13 | loss: 2.5559063Losses:  2.7521421909332275 0.7239682674407959 0.7797526717185974
MemoryTrain:  epoch  8, batch    14 | loss: 2.7521422Losses:  2.2792720794677734 -0.0 0.9747051000595093
MemoryTrain:  epoch  9, batch     0 | loss: 2.2792721Losses:  2.3091511726379395 -0.0 1.0313414335250854
MemoryTrain:  epoch  9, batch     1 | loss: 2.3091512Losses:  3.14909029006958 0.8391425013542175 1.0130361318588257
MemoryTrain:  epoch  9, batch     2 | loss: 3.1490903Losses:  2.5616660118103027 0.4899613559246063 0.8369643688201904
MemoryTrain:  epoch  9, batch     3 | loss: 2.5616660Losses:  2.4842283725738525 0.25963711738586426 0.9835814237594604
MemoryTrain:  epoch  9, batch     4 | loss: 2.4842284Losses:  2.314422130584717 -0.0 1.0550457239151
MemoryTrain:  epoch  9, batch     5 | loss: 2.3144221Losses:  2.536416530609131 0.23504678905010223 0.9687984585762024
MemoryTrain:  epoch  9, batch     6 | loss: 2.5364165Losses:  2.2648653984069824 -0.0 0.9983418583869934
MemoryTrain:  epoch  9, batch     7 | loss: 2.2648654Losses:  2.3655314445495605 0.23272934556007385 0.9155236482620239
MemoryTrain:  epoch  9, batch     8 | loss: 2.3655314Losses:  2.2384467124938965 -0.0 1.0073187351226807
MemoryTrain:  epoch  9, batch     9 | loss: 2.2384467Losses:  2.4517462253570557 0.2501234710216522 1.014822006225586
MemoryTrain:  epoch  9, batch    10 | loss: 2.4517462Losses:  2.515111207962036 0.2565055787563324 0.8349670171737671
MemoryTrain:  epoch  9, batch    11 | loss: 2.5151112Losses:  2.1233630180358887 -0.0 0.8691705465316772
MemoryTrain:  epoch  9, batch    12 | loss: 2.1233630Losses:  2.2916576862335205 -0.0 0.9610597491264343
MemoryTrain:  epoch  9, batch    13 | loss: 2.2916577Losses:  2.1628270149230957 -0.0 0.8627588152885437
MemoryTrain:  epoch  9, batch    14 | loss: 2.1628270
[EVAL] batch:    0 | acc: 25.00%,  total acc: 25.00%   [EVAL] batch:    1 | acc: 31.25%,  total acc: 28.12%   [EVAL] batch:    2 | acc: 31.25%,  total acc: 29.17%   [EVAL] batch:    3 | acc: 18.75%,  total acc: 26.56%   [EVAL] batch:    4 | acc: 31.25%,  total acc: 27.50%   [EVAL] batch:    5 | acc: 18.75%,  total acc: 26.04%   [EVAL] batch:    6 | acc: 37.50%,  total acc: 27.68%   [EVAL] batch:    7 | acc: 62.50%,  total acc: 32.03%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 36.11%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 40.62%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 44.32%   [EVAL] batch:   11 | acc: 68.75%,  total acc: 46.35%   [EVAL] batch:   12 | acc: 56.25%,  total acc: 47.12%   [EVAL] batch:   13 | acc: 62.50%,  total acc: 48.21%   [EVAL] batch:   14 | acc: 81.25%,  total acc: 50.42%   [EVAL] batch:   15 | acc: 75.00%,  total acc: 51.95%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 54.41%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 56.60%   [EVAL] batch:   18 | acc: 75.00%,  total acc: 57.57%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 59.69%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 61.61%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 63.07%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 64.67%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 65.89%   [EVAL] batch:   24 | acc: 81.25%,  total acc: 66.50%   [EVAL] batch:   25 | acc: 87.50%,  total acc: 67.31%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 68.29%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 69.20%   [EVAL] batch:   28 | acc: 87.50%,  total acc: 69.83%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 70.21%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 70.77%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 71.68%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 72.54%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 73.35%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 74.11%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 74.65%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 75.34%   [EVAL] batch:   37 | acc: 68.75%,  total acc: 75.16%   [EVAL] batch:   38 | acc: 43.75%,  total acc: 74.36%   [EVAL] batch:   39 | acc: 31.25%,  total acc: 73.28%   [EVAL] batch:   40 | acc: 18.75%,  total acc: 71.95%   [EVAL] batch:   41 | acc: 6.25%,  total acc: 70.39%   [EVAL] batch:   42 | acc: 18.75%,  total acc: 69.19%   [EVAL] batch:   43 | acc: 37.50%,  total acc: 68.47%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 69.03%   [EVAL] batch:   45 | acc: 87.50%,  total acc: 69.43%   [EVAL] batch:   46 | acc: 87.50%,  total acc: 69.81%   [EVAL] batch:   47 | acc: 87.50%,  total acc: 70.18%   [EVAL] batch:   48 | acc: 93.75%,  total acc: 70.66%   [EVAL] batch:   49 | acc: 81.25%,  total acc: 70.88%   [EVAL] batch:   50 | acc: 43.75%,  total acc: 70.34%   [EVAL] batch:   51 | acc: 31.25%,  total acc: 69.59%   [EVAL] batch:   52 | acc: 43.75%,  total acc: 69.10%   [EVAL] batch:   53 | acc: 50.00%,  total acc: 68.75%   [EVAL] batch:   54 | acc: 50.00%,  total acc: 68.41%   [EVAL] batch:   55 | acc: 31.25%,  total acc: 67.75%   [EVAL] batch:   56 | acc: 56.25%,  total acc: 67.54%   [EVAL] batch:   57 | acc: 81.25%,  total acc: 67.78%   [EVAL] batch:   58 | acc: 68.75%,  total acc: 67.80%   [EVAL] batch:   59 | acc: 56.25%,  total acc: 67.60%   [EVAL] batch:   60 | acc: 62.50%,  total acc: 67.52%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 67.94%   [EVAL] batch:   62 | acc: 37.50%,  total acc: 67.46%   
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 87.50%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 86.25%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    6 | acc: 43.75%,  total acc: 81.25%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 80.47%   [EVAL] batch:    8 | acc: 62.50%,  total acc: 78.47%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 76.88%   [EVAL] batch:   10 | acc: 37.50%,  total acc: 73.30%   [EVAL] batch:   11 | acc: 37.50%,  total acc: 70.31%   [EVAL] batch:   12 | acc: 62.50%,  total acc: 69.71%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 71.43%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 72.92%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 74.61%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 76.10%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 77.08%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 77.96%   [EVAL] batch:   19 | acc: 56.25%,  total acc: 76.88%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 77.38%   [EVAL] batch:   21 | acc: 68.75%,  total acc: 76.99%   [EVAL] batch:   22 | acc: 62.50%,  total acc: 76.36%   [EVAL] batch:   23 | acc: 81.25%,  total acc: 76.56%   [EVAL] batch:   24 | acc: 68.75%,  total acc: 76.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 77.16%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 77.78%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 78.35%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 78.88%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 79.58%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 80.24%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 80.86%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 81.44%   [EVAL] batch:   33 | acc: 68.75%,  total acc: 81.07%   [EVAL] batch:   34 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:   35 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:   36 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:   37 | acc: 75.00%,  total acc: 81.09%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 81.57%   [EVAL] batch:   39 | acc: 93.75%,  total acc: 81.88%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 82.16%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 82.29%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 82.56%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 82.81%   [EVAL] batch:   44 | acc: 81.25%,  total acc: 82.78%   [EVAL] batch:   45 | acc: 75.00%,  total acc: 82.61%   [EVAL] batch:   46 | acc: 68.75%,  total acc: 82.31%   [EVAL] batch:   47 | acc: 62.50%,  total acc: 81.90%   [EVAL] batch:   48 | acc: 81.25%,  total acc: 81.89%   [EVAL] batch:   49 | acc: 68.75%,  total acc: 81.62%   [EVAL] batch:   50 | acc: 75.00%,  total acc: 81.50%   [EVAL] batch:   51 | acc: 75.00%,  total acc: 81.37%   [EVAL] batch:   52 | acc: 68.75%,  total acc: 81.13%   [EVAL] batch:   53 | acc: 56.25%,  total acc: 80.67%   [EVAL] batch:   54 | acc: 50.00%,  total acc: 80.11%   [EVAL] batch:   55 | acc: 68.75%,  total acc: 79.91%   [EVAL] batch:   56 | acc: 50.00%,  total acc: 79.39%   [EVAL] batch:   57 | acc: 25.00%,  total acc: 78.45%   [EVAL] batch:   58 | acc: 50.00%,  total acc: 77.97%   [EVAL] batch:   59 | acc: 68.75%,  total acc: 77.81%   [EVAL] batch:   60 | acc: 31.25%,  total acc: 77.05%   [EVAL] batch:   61 | acc: 43.75%,  total acc: 76.51%   [EVAL] batch:   62 | acc: 31.25%,  total acc: 75.79%   [EVAL] batch:   63 | acc: 0.00%,  total acc: 74.61%   [EVAL] batch:   64 | acc: 0.00%,  total acc: 73.46%   [EVAL] batch:   65 | acc: 0.00%,  total acc: 72.35%   [EVAL] batch:   66 | acc: 0.00%,  total acc: 71.27%   [EVAL] batch:   67 | acc: 0.00%,  total acc: 70.22%   [EVAL] batch:   68 | acc: 31.25%,  total acc: 69.66%   [EVAL] batch:   69 | acc: 75.00%,  total acc: 69.73%   [EVAL] batch:   70 | acc: 81.25%,  total acc: 69.89%   [EVAL] batch:   71 | acc: 87.50%,  total acc: 70.14%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 70.55%   [EVAL] batch:   73 | acc: 81.25%,  total acc: 70.69%   [EVAL] batch:   74 | acc: 81.25%,  total acc: 70.83%   [EVAL] batch:   75 | acc: 68.75%,  total acc: 70.81%   [EVAL] batch:   76 | acc: 50.00%,  total acc: 70.54%   [EVAL] batch:   77 | acc: 68.75%,  total acc: 70.51%   [EVAL] batch:   78 | acc: 62.50%,  total acc: 70.41%   [EVAL] batch:   79 | acc: 56.25%,  total acc: 70.23%   [EVAL] batch:   80 | acc: 56.25%,  total acc: 70.06%   [EVAL] batch:   81 | acc: 6.25%,  total acc: 69.28%   [EVAL] batch:   82 | acc: 18.75%,  total acc: 68.67%   [EVAL] batch:   83 | acc: 12.50%,  total acc: 68.01%   [EVAL] batch:   84 | acc: 31.25%,  total acc: 67.57%   [EVAL] batch:   85 | acc: 25.00%,  total acc: 67.08%   [EVAL] batch:   86 | acc: 12.50%,  total acc: 66.45%   [EVAL] batch:   87 | acc: 12.50%,  total acc: 65.84%   [EVAL] batch:   88 | acc: 25.00%,  total acc: 65.38%   [EVAL] batch:   89 | acc: 25.00%,  total acc: 64.93%   [EVAL] batch:   90 | acc: 6.25%,  total acc: 64.29%   [EVAL] batch:   91 | acc: 6.25%,  total acc: 63.65%   [EVAL] batch:   92 | acc: 12.50%,  total acc: 63.10%   [EVAL] batch:   93 | acc: 25.00%,  total acc: 62.70%   [EVAL] batch:   94 | acc: 81.25%,  total acc: 62.89%   [EVAL] batch:   95 | acc: 87.50%,  total acc: 63.15%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 63.53%   [EVAL] batch:   97 | acc: 93.75%,  total acc: 63.84%   [EVAL] batch:   98 | acc: 87.50%,  total acc: 64.08%   [EVAL] batch:   99 | acc: 62.50%,  total acc: 64.06%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 64.42%   [EVAL] batch:  101 | acc: 93.75%,  total acc: 64.71%   [EVAL] batch:  102 | acc: 87.50%,  total acc: 64.93%   [EVAL] batch:  103 | acc: 93.75%,  total acc: 65.20%   [EVAL] batch:  104 | acc: 87.50%,  total acc: 65.42%   [EVAL] batch:  105 | acc: 87.50%,  total acc: 65.62%   [EVAL] batch:  106 | acc: 43.75%,  total acc: 65.42%   [EVAL] batch:  107 | acc: 25.00%,  total acc: 65.05%   [EVAL] batch:  108 | acc: 12.50%,  total acc: 64.56%   [EVAL] batch:  109 | acc: 12.50%,  total acc: 64.09%   [EVAL] batch:  110 | acc: 12.50%,  total acc: 63.63%   [EVAL] batch:  111 | acc: 6.25%,  total acc: 63.11%   [EVAL] batch:  112 | acc: 25.00%,  total acc: 62.78%   [EVAL] batch:  113 | acc: 56.25%,  total acc: 62.72%   [EVAL] batch:  114 | acc: 56.25%,  total acc: 62.66%   [EVAL] batch:  115 | acc: 68.75%,  total acc: 62.72%   [EVAL] batch:  116 | acc: 62.50%,  total acc: 62.71%   [EVAL] batch:  117 | acc: 68.75%,  total acc: 62.76%   [EVAL] batch:  118 | acc: 56.25%,  total acc: 62.71%   [EVAL] batch:  119 | acc: 50.00%,  total acc: 62.60%   [EVAL] batch:  120 | acc: 37.50%,  total acc: 62.40%   [EVAL] batch:  121 | acc: 50.00%,  total acc: 62.30%   [EVAL] batch:  122 | acc: 62.50%,  total acc: 62.30%   [EVAL] batch:  123 | acc: 75.00%,  total acc: 62.40%   [EVAL] batch:  124 | acc: 62.50%,  total acc: 62.40%   [EVAL] batch:  125 | acc: 50.00%,  total acc: 62.30%   [EVAL] batch:  126 | acc: 43.75%,  total acc: 62.16%   [EVAL] batch:  127 | acc: 25.00%,  total acc: 61.87%   [EVAL] batch:  128 | acc: 31.25%,  total acc: 61.63%   [EVAL] batch:  129 | acc: 31.25%,  total acc: 61.39%   [EVAL] batch:  130 | acc: 25.00%,  total acc: 61.12%   [EVAL] batch:  131 | acc: 87.50%,  total acc: 61.32%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 61.61%   [EVAL] batch:  133 | acc: 87.50%,  total acc: 61.80%   [EVAL] batch:  134 | acc: 81.25%,  total acc: 61.94%   [EVAL] batch:  135 | acc: 93.75%,  total acc: 62.18%   [EVAL] batch:  136 | acc: 93.75%,  total acc: 62.41%   [EVAL] batch:  137 | acc: 93.75%,  total acc: 62.64%   [EVAL] batch:  138 | acc: 43.75%,  total acc: 62.50%   [EVAL] batch:  139 | acc: 56.25%,  total acc: 62.46%   [EVAL] batch:  140 | acc: 81.25%,  total acc: 62.59%   [EVAL] batch:  141 | acc: 75.00%,  total acc: 62.68%   [EVAL] batch:  142 | acc: 50.00%,  total acc: 62.59%   [EVAL] batch:  143 | acc: 62.50%,  total acc: 62.59%   [EVAL] batch:  144 | acc: 62.50%,  total acc: 62.59%   [EVAL] batch:  145 | acc: 75.00%,  total acc: 62.67%   [EVAL] batch:  146 | acc: 62.50%,  total acc: 62.67%   [EVAL] batch:  147 | acc: 75.00%,  total acc: 62.75%   [EVAL] batch:  148 | acc: 50.00%,  total acc: 62.67%   [EVAL] batch:  149 | acc: 56.25%,  total acc: 62.62%   [EVAL] batch:  150 | acc: 75.00%,  total acc: 62.71%   [EVAL] batch:  151 | acc: 75.00%,  total acc: 62.79%   [EVAL] batch:  152 | acc: 75.00%,  total acc: 62.87%   [EVAL] batch:  153 | acc: 81.25%,  total acc: 62.99%   [EVAL] batch:  154 | acc: 62.50%,  total acc: 62.98%   [EVAL] batch:  155 | acc: 62.50%,  total acc: 62.98%   [EVAL] batch:  156 | acc: 62.50%,  total acc: 62.98%   [EVAL] batch:  157 | acc: 50.00%,  total acc: 62.90%   [EVAL] batch:  158 | acc: 50.00%,  total acc: 62.81%   [EVAL] batch:  159 | acc: 50.00%,  total acc: 62.73%   [EVAL] batch:  160 | acc: 75.00%,  total acc: 62.81%   [EVAL] batch:  161 | acc: 50.00%,  total acc: 62.73%   [EVAL] batch:  162 | acc: 68.75%,  total acc: 62.77%   [EVAL] batch:  163 | acc: 62.50%,  total acc: 62.77%   [EVAL] batch:  164 | acc: 75.00%,  total acc: 62.84%   [EVAL] batch:  165 | acc: 81.25%,  total acc: 62.95%   [EVAL] batch:  166 | acc: 75.00%,  total acc: 63.02%   [EVAL] batch:  167 | acc: 87.50%,  total acc: 63.17%   [EVAL] batch:  168 | acc: 87.50%,  total acc: 63.31%   [EVAL] batch:  169 | acc: 50.00%,  total acc: 63.24%   [EVAL] batch:  170 | acc: 43.75%,  total acc: 63.12%   [EVAL] batch:  171 | acc: 25.00%,  total acc: 62.90%   [EVAL] batch:  172 | acc: 56.25%,  total acc: 62.86%   [EVAL] batch:  173 | acc: 31.25%,  total acc: 62.68%   [EVAL] batch:  174 | acc: 43.75%,  total acc: 62.57%   [EVAL] batch:  175 | acc: 100.00%,  total acc: 62.78%   [EVAL] batch:  176 | acc: 100.00%,  total acc: 62.99%   [EVAL] batch:  177 | acc: 100.00%,  total acc: 63.20%   [EVAL] batch:  178 | acc: 100.00%,  total acc: 63.41%   [EVAL] batch:  179 | acc: 100.00%,  total acc: 63.61%   [EVAL] batch:  180 | acc: 100.00%,  total acc: 63.81%   [EVAL] batch:  181 | acc: 75.00%,  total acc: 63.87%   [EVAL] batch:  182 | acc: 50.00%,  total acc: 63.80%   [EVAL] batch:  183 | acc: 62.50%,  total acc: 63.79%   [EVAL] batch:  184 | acc: 87.50%,  total acc: 63.92%   [EVAL] batch:  185 | acc: 56.25%,  total acc: 63.88%   [EVAL] batch:  186 | acc: 56.25%,  total acc: 63.84%   [EVAL] batch:  187 | acc: 43.75%,  total acc: 63.73%   [EVAL] batch:  188 | acc: 31.25%,  total acc: 63.56%   [EVAL] batch:  189 | acc: 43.75%,  total acc: 63.45%   [EVAL] batch:  190 | acc: 18.75%,  total acc: 63.22%   [EVAL] batch:  191 | acc: 43.75%,  total acc: 63.12%   [EVAL] batch:  192 | acc: 37.50%,  total acc: 62.99%   [EVAL] batch:  193 | acc: 25.00%,  total acc: 62.79%   [EVAL] batch:  194 | acc: 75.00%,  total acc: 62.85%   [EVAL] batch:  195 | acc: 62.50%,  total acc: 62.85%   [EVAL] batch:  196 | acc: 43.75%,  total acc: 62.75%   [EVAL] batch:  197 | acc: 62.50%,  total acc: 62.75%   [EVAL] batch:  198 | acc: 50.00%,  total acc: 62.69%   [EVAL] batch:  199 | acc: 81.25%,  total acc: 62.78%   [EVAL] batch:  200 | acc: 81.25%,  total acc: 62.87%   [EVAL] batch:  201 | acc: 75.00%,  total acc: 62.93%   [EVAL] batch:  202 | acc: 62.50%,  total acc: 62.93%   [EVAL] batch:  203 | acc: 81.25%,  total acc: 63.02%   [EVAL] batch:  204 | acc: 75.00%,  total acc: 63.08%   [EVAL] batch:  205 | acc: 93.75%,  total acc: 63.23%   [EVAL] batch:  206 | acc: 31.25%,  total acc: 63.07%   [EVAL] batch:  207 | acc: 18.75%,  total acc: 62.86%   [EVAL] batch:  208 | acc: 18.75%,  total acc: 62.65%   [EVAL] batch:  209 | acc: 12.50%,  total acc: 62.41%   [EVAL] batch:  210 | acc: 25.00%,  total acc: 62.23%   [EVAL] batch:  211 | acc: 25.00%,  total acc: 62.06%   [EVAL] batch:  212 | acc: 31.25%,  total acc: 61.91%   [EVAL] batch:  213 | acc: 87.50%,  total acc: 62.03%   [EVAL] batch:  214 | acc: 93.75%,  total acc: 62.18%   [EVAL] batch:  215 | acc: 68.75%,  total acc: 62.21%   [EVAL] batch:  216 | acc: 81.25%,  total acc: 62.30%   [EVAL] batch:  217 | acc: 87.50%,  total acc: 62.41%   [EVAL] batch:  218 | acc: 81.25%,  total acc: 62.50%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 62.67%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 62.84%   [EVAL] batch:  221 | acc: 100.00%,  total acc: 63.01%   [EVAL] batch:  222 | acc: 100.00%,  total acc: 63.17%   [EVAL] batch:  223 | acc: 100.00%,  total acc: 63.34%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 63.50%   [EVAL] batch:  225 | acc: 68.75%,  total acc: 63.52%   [EVAL] batch:  226 | acc: 75.00%,  total acc: 63.57%   [EVAL] batch:  227 | acc: 75.00%,  total acc: 63.62%   [EVAL] batch:  228 | acc: 62.50%,  total acc: 63.62%   [EVAL] batch:  229 | acc: 87.50%,  total acc: 63.72%   [EVAL] batch:  230 | acc: 68.75%,  total acc: 63.74%   [EVAL] batch:  231 | acc: 31.25%,  total acc: 63.60%   [EVAL] batch:  232 | acc: 31.25%,  total acc: 63.47%   [EVAL] batch:  233 | acc: 43.75%,  total acc: 63.38%   [EVAL] batch:  234 | acc: 6.25%,  total acc: 63.14%   [EVAL] batch:  235 | acc: 50.00%,  total acc: 63.08%   [EVAL] batch:  236 | acc: 18.75%,  total acc: 62.90%   [EVAL] batch:  237 | acc: 62.50%,  total acc: 62.89%   [EVAL] batch:  238 | acc: 87.50%,  total acc: 63.00%   [EVAL] batch:  239 | acc: 100.00%,  total acc: 63.15%   [EVAL] batch:  240 | acc: 87.50%,  total acc: 63.25%   [EVAL] batch:  241 | acc: 87.50%,  total acc: 63.35%   [EVAL] batch:  242 | acc: 87.50%,  total acc: 63.45%   [EVAL] batch:  243 | acc: 75.00%,  total acc: 63.50%   [EVAL] batch:  244 | acc: 12.50%,  total acc: 63.29%   [EVAL] batch:  245 | acc: 31.25%,  total acc: 63.16%   [EVAL] batch:  246 | acc: 18.75%,  total acc: 62.98%   [EVAL] batch:  247 | acc: 12.50%,  total acc: 62.78%   [EVAL] batch:  248 | acc: 12.50%,  total acc: 62.58%   [EVAL] batch:  249 | acc: 12.50%,  total acc: 62.38%   [EVAL] batch:  250 | acc: 62.50%,  total acc: 62.38%   [EVAL] batch:  251 | acc: 56.25%,  total acc: 62.35%   [EVAL] batch:  252 | acc: 75.00%,  total acc: 62.40%   [EVAL] batch:  253 | acc: 56.25%,  total acc: 62.38%   [EVAL] batch:  254 | acc: 62.50%,  total acc: 62.38%   [EVAL] batch:  255 | acc: 75.00%,  total acc: 62.43%   [EVAL] batch:  256 | acc: 62.50%,  total acc: 62.43%   [EVAL] batch:  257 | acc: 62.50%,  total acc: 62.43%   [EVAL] batch:  258 | acc: 68.75%,  total acc: 62.45%   [EVAL] batch:  259 | acc: 68.75%,  total acc: 62.48%   [EVAL] batch:  260 | acc: 56.25%,  total acc: 62.45%   [EVAL] batch:  261 | acc: 68.75%,  total acc: 62.48%   [EVAL] batch:  262 | acc: 62.50%,  total acc: 62.48%   [EVAL] batch:  263 | acc: 81.25%,  total acc: 62.55%   [EVAL] batch:  264 | acc: 68.75%,  total acc: 62.57%   [EVAL] batch:  265 | acc: 62.50%,  total acc: 62.57%   [EVAL] batch:  266 | acc: 68.75%,  total acc: 62.59%   [EVAL] batch:  267 | acc: 68.75%,  total acc: 62.62%   [EVAL] batch:  268 | acc: 93.75%,  total acc: 62.73%   [EVAL] batch:  269 | acc: 81.25%,  total acc: 62.80%   [EVAL] batch:  270 | acc: 93.75%,  total acc: 62.92%   [EVAL] batch:  271 | acc: 93.75%,  total acc: 63.03%   [EVAL] batch:  272 | acc: 100.00%,  total acc: 63.16%   [EVAL] batch:  273 | acc: 93.75%,  total acc: 63.28%   [EVAL] batch:  274 | acc: 100.00%,  total acc: 63.41%   [EVAL] batch:  275 | acc: 62.50%,  total acc: 63.41%   [EVAL] batch:  276 | acc: 31.25%,  total acc: 63.29%   [EVAL] batch:  277 | acc: 62.50%,  total acc: 63.29%   [EVAL] batch:  278 | acc: 68.75%,  total acc: 63.31%   [EVAL] batch:  279 | acc: 43.75%,  total acc: 63.24%   [EVAL] batch:  280 | acc: 56.25%,  total acc: 63.21%   [EVAL] batch:  281 | acc: 68.75%,  total acc: 63.23%   [EVAL] batch:  282 | acc: 62.50%,  total acc: 63.23%   [EVAL] batch:  283 | acc: 43.75%,  total acc: 63.16%   [EVAL] batch:  284 | acc: 43.75%,  total acc: 63.09%   [EVAL] batch:  285 | acc: 43.75%,  total acc: 63.02%   [EVAL] batch:  286 | acc: 31.25%,  total acc: 62.91%   [EVAL] batch:  287 | acc: 56.25%,  total acc: 62.89%   [EVAL] batch:  288 | acc: 62.50%,  total acc: 62.89%   [EVAL] batch:  289 | acc: 81.25%,  total acc: 62.95%   [EVAL] batch:  290 | acc: 56.25%,  total acc: 62.93%   [EVAL] batch:  291 | acc: 87.50%,  total acc: 63.01%   [EVAL] batch:  292 | acc: 68.75%,  total acc: 63.03%   [EVAL] batch:  293 | acc: 68.75%,  total acc: 63.05%   [EVAL] batch:  294 | acc: 62.50%,  total acc: 63.05%   [EVAL] batch:  295 | acc: 68.75%,  total acc: 63.07%   [EVAL] batch:  296 | acc: 56.25%,  total acc: 63.05%   [EVAL] batch:  297 | acc: 50.00%,  total acc: 63.00%   [EVAL] batch:  298 | acc: 62.50%,  total acc: 63.00%   [EVAL] batch:  299 | acc: 75.00%,  total acc: 63.04%   [EVAL] batch:  300 | acc: 100.00%,  total acc: 63.16%   [EVAL] batch:  301 | acc: 100.00%,  total acc: 63.29%   [EVAL] batch:  302 | acc: 100.00%,  total acc: 63.41%   [EVAL] batch:  303 | acc: 93.75%,  total acc: 63.51%   [EVAL] batch:  304 | acc: 100.00%,  total acc: 63.63%   [EVAL] batch:  305 | acc: 100.00%,  total acc: 63.75%   [EVAL] batch:  306 | acc: 93.75%,  total acc: 63.84%   [EVAL] batch:  307 | acc: 87.50%,  total acc: 63.92%   [EVAL] batch:  308 | acc: 93.75%,  total acc: 64.02%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 64.13%   [EVAL] batch:  310 | acc: 87.50%,  total acc: 64.21%   [EVAL] batch:  311 | acc: 100.00%,  total acc: 64.32%   [EVAL] batch:  312 | acc: 87.50%,  total acc: 64.40%   [EVAL] batch:  313 | acc: 50.00%,  total acc: 64.35%   [EVAL] batch:  314 | acc: 43.75%,  total acc: 64.29%   [EVAL] batch:  315 | acc: 75.00%,  total acc: 64.32%   [EVAL] batch:  316 | acc: 75.00%,  total acc: 64.35%   [EVAL] batch:  317 | acc: 75.00%,  total acc: 64.39%   [EVAL] batch:  318 | acc: 68.75%,  total acc: 64.40%   [EVAL] batch:  319 | acc: 93.75%,  total acc: 64.49%   [EVAL] batch:  320 | acc: 75.00%,  total acc: 64.52%   [EVAL] batch:  321 | acc: 75.00%,  total acc: 64.56%   [EVAL] batch:  322 | acc: 75.00%,  total acc: 64.59%   [EVAL] batch:  323 | acc: 81.25%,  total acc: 64.64%   [EVAL] batch:  324 | acc: 56.25%,  total acc: 64.62%   [EVAL] batch:  325 | acc: 12.50%,  total acc: 64.46%   [EVAL] batch:  326 | acc: 12.50%,  total acc: 64.30%   [EVAL] batch:  327 | acc: 12.50%,  total acc: 64.14%   [EVAL] batch:  328 | acc: 18.75%,  total acc: 64.00%   [EVAL] batch:  329 | acc: 12.50%,  total acc: 63.84%   [EVAL] batch:  330 | acc: 6.25%,  total acc: 63.67%   [EVAL] batch:  331 | acc: 68.75%,  total acc: 63.69%   [EVAL] batch:  332 | acc: 100.00%,  total acc: 63.80%   [EVAL] batch:  333 | acc: 100.00%,  total acc: 63.90%   [EVAL] batch:  334 | acc: 93.75%,  total acc: 63.99%   [EVAL] batch:  335 | acc: 100.00%,  total acc: 64.10%   [EVAL] batch:  336 | acc: 93.75%,  total acc: 64.19%   [EVAL] batch:  337 | acc: 56.25%,  total acc: 64.16%   [EVAL] batch:  338 | acc: 0.00%,  total acc: 63.97%   [EVAL] batch:  339 | acc: 12.50%,  total acc: 63.82%   [EVAL] batch:  340 | acc: 6.25%,  total acc: 63.65%   [EVAL] batch:  341 | acc: 12.50%,  total acc: 63.51%   [EVAL] batch:  342 | acc: 6.25%,  total acc: 63.34%   [EVAL] batch:  343 | acc: 31.25%,  total acc: 63.24%   [EVAL] batch:  344 | acc: 87.50%,  total acc: 63.32%   [EVAL] batch:  345 | acc: 93.75%,  total acc: 63.40%   [EVAL] batch:  346 | acc: 93.75%,  total acc: 63.49%   [EVAL] batch:  347 | acc: 93.75%,  total acc: 63.58%   [EVAL] batch:  348 | acc: 93.75%,  total acc: 63.66%   [EVAL] batch:  349 | acc: 100.00%,  total acc: 63.77%   [EVAL] batch:  350 | acc: 0.00%,  total acc: 63.59%   [EVAL] batch:  351 | acc: 0.00%,  total acc: 63.41%   [EVAL] batch:  352 | acc: 0.00%,  total acc: 63.23%   [EVAL] batch:  353 | acc: 0.00%,  total acc: 63.05%   [EVAL] batch:  354 | acc: 0.00%,  total acc: 62.87%   [EVAL] batch:  355 | acc: 0.00%,  total acc: 62.69%   [EVAL] batch:  356 | acc: 31.25%,  total acc: 62.61%   [EVAL] batch:  357 | acc: 37.50%,  total acc: 62.53%   [EVAL] batch:  358 | acc: 62.50%,  total acc: 62.53%   [EVAL] batch:  359 | acc: 37.50%,  total acc: 62.47%   [EVAL] batch:  360 | acc: 56.25%,  total acc: 62.45%   [EVAL] batch:  361 | acc: 37.50%,  total acc: 62.38%   [EVAL] batch:  362 | acc: 62.50%,  total acc: 62.38%   [EVAL] batch:  363 | acc: 37.50%,  total acc: 62.31%   [EVAL] batch:  364 | acc: 68.75%,  total acc: 62.33%   [EVAL] batch:  365 | acc: 56.25%,  total acc: 62.31%   [EVAL] batch:  366 | acc: 43.75%,  total acc: 62.26%   [EVAL] batch:  367 | acc: 50.00%,  total acc: 62.23%   [EVAL] batch:  368 | acc: 62.50%,  total acc: 62.23%   [EVAL] batch:  369 | acc: 93.75%,  total acc: 62.31%   [EVAL] batch:  370 | acc: 93.75%,  total acc: 62.40%   [EVAL] batch:  371 | acc: 81.25%,  total acc: 62.45%   [EVAL] batch:  372 | acc: 75.00%,  total acc: 62.48%   [EVAL] batch:  373 | acc: 81.25%,  total acc: 62.53%   [EVAL] batch:  374 | acc: 87.50%,  total acc: 62.60%   [EVAL] batch:  375 | acc: 93.75%,  total acc: 62.68%   [EVAL] batch:  376 | acc: 93.75%,  total acc: 62.77%   [EVAL] batch:  377 | acc: 81.25%,  total acc: 62.81%   [EVAL] batch:  378 | acc: 87.50%,  total acc: 62.88%   [EVAL] batch:  379 | acc: 93.75%,  total acc: 62.96%   [EVAL] batch:  380 | acc: 100.00%,  total acc: 63.06%   [EVAL] batch:  381 | acc: 68.75%,  total acc: 63.07%   [EVAL] batch:  382 | acc: 75.00%,  total acc: 63.10%   [EVAL] batch:  383 | acc: 87.50%,  total acc: 63.17%   [EVAL] batch:  384 | acc: 75.00%,  total acc: 63.20%   [EVAL] batch:  385 | acc: 56.25%,  total acc: 63.18%   [EVAL] batch:  386 | acc: 75.00%,  total acc: 63.21%   [EVAL] batch:  387 | acc: 68.75%,  total acc: 63.22%   [EVAL] batch:  388 | acc: 75.00%,  total acc: 63.26%   [EVAL] batch:  389 | acc: 75.00%,  total acc: 63.29%   [EVAL] batch:  390 | acc: 62.50%,  total acc: 63.28%   [EVAL] batch:  391 | acc: 62.50%,  total acc: 63.28%   [EVAL] batch:  392 | acc: 50.00%,  total acc: 63.25%   [EVAL] batch:  393 | acc: 56.25%,  total acc: 63.23%   [EVAL] batch:  394 | acc: 68.75%,  total acc: 63.24%   [EVAL] batch:  395 | acc: 50.00%,  total acc: 63.21%   [EVAL] batch:  396 | acc: 56.25%,  total acc: 63.19%   [EVAL] batch:  397 | acc: 75.00%,  total acc: 63.22%   [EVAL] batch:  398 | acc: 68.75%,  total acc: 63.24%   [EVAL] batch:  399 | acc: 62.50%,  total acc: 63.23%   [EVAL] batch:  400 | acc: 100.00%,  total acc: 63.33%   [EVAL] batch:  401 | acc: 100.00%,  total acc: 63.42%   [EVAL] batch:  402 | acc: 100.00%,  total acc: 63.51%   [EVAL] batch:  403 | acc: 100.00%,  total acc: 63.60%   [EVAL] batch:  404 | acc: 100.00%,  total acc: 63.69%   [EVAL] batch:  405 | acc: 100.00%,  total acc: 63.78%   [EVAL] batch:  406 | acc: 75.00%,  total acc: 63.81%   [EVAL] batch:  407 | acc: 81.25%,  total acc: 63.85%   [EVAL] batch:  408 | acc: 62.50%,  total acc: 63.84%   [EVAL] batch:  409 | acc: 68.75%,  total acc: 63.86%   [EVAL] batch:  410 | acc: 68.75%,  total acc: 63.87%   [EVAL] batch:  411 | acc: 62.50%,  total acc: 63.87%   [EVAL] batch:  412 | acc: 68.75%,  total acc: 63.88%   [EVAL] batch:  413 | acc: 100.00%,  total acc: 63.96%   [EVAL] batch:  414 | acc: 100.00%,  total acc: 64.05%   [EVAL] batch:  415 | acc: 100.00%,  total acc: 64.14%   [EVAL] batch:  416 | acc: 100.00%,  total acc: 64.22%   [EVAL] batch:  417 | acc: 100.00%,  total acc: 64.31%   [EVAL] batch:  418 | acc: 87.50%,  total acc: 64.36%   [EVAL] batch:  419 | acc: 100.00%,  total acc: 64.45%   [EVAL] batch:  420 | acc: 100.00%,  total acc: 64.53%   [EVAL] batch:  421 | acc: 100.00%,  total acc: 64.62%   [EVAL] batch:  422 | acc: 100.00%,  total acc: 64.70%   [EVAL] batch:  423 | acc: 100.00%,  total acc: 64.78%   [EVAL] batch:  424 | acc: 93.75%,  total acc: 64.85%   [EVAL] batch:  425 | acc: 81.25%,  total acc: 64.89%   [EVAL] batch:  426 | acc: 93.75%,  total acc: 64.96%   [EVAL] batch:  427 | acc: 93.75%,  total acc: 65.03%   [EVAL] batch:  428 | acc: 93.75%,  total acc: 65.09%   [EVAL] batch:  429 | acc: 93.75%,  total acc: 65.16%   [EVAL] batch:  430 | acc: 87.50%,  total acc: 65.21%   [EVAL] batch:  431 | acc: 93.75%,  total acc: 65.28%   [EVAL] batch:  432 | acc: 93.75%,  total acc: 65.34%   [EVAL] batch:  433 | acc: 87.50%,  total acc: 65.39%   [EVAL] batch:  434 | acc: 100.00%,  total acc: 65.47%   [EVAL] batch:  435 | acc: 100.00%,  total acc: 65.55%   [EVAL] batch:  436 | acc: 93.75%,  total acc: 65.62%   [EVAL] batch:  437 | acc: 50.00%,  total acc: 65.58%   [EVAL] batch:  438 | acc: 43.75%,  total acc: 65.53%   [EVAL] batch:  439 | acc: 31.25%,  total acc: 65.45%   [EVAL] batch:  440 | acc: 18.75%,  total acc: 65.35%   [EVAL] batch:  441 | acc: 25.00%,  total acc: 65.26%   [EVAL] batch:  442 | acc: 25.00%,  total acc: 65.17%   [EVAL] batch:  443 | acc: 31.25%,  total acc: 65.09%   [EVAL] batch:  444 | acc: 62.50%,  total acc: 65.08%   [EVAL] batch:  445 | acc: 50.00%,  total acc: 65.05%   [EVAL] batch:  446 | acc: 75.00%,  total acc: 65.07%   [EVAL] batch:  447 | acc: 81.25%,  total acc: 65.11%   [EVAL] batch:  448 | acc: 75.00%,  total acc: 65.13%   [EVAL] batch:  449 | acc: 62.50%,  total acc: 65.12%   [EVAL] batch:  450 | acc: 62.50%,  total acc: 65.12%   [EVAL] batch:  451 | acc: 68.75%,  total acc: 65.13%   [EVAL] batch:  452 | acc: 75.00%,  total acc: 65.15%   [EVAL] batch:  453 | acc: 93.75%,  total acc: 65.21%   [EVAL] batch:  454 | acc: 87.50%,  total acc: 65.26%   [EVAL] batch:  455 | acc: 87.50%,  total acc: 65.31%   [EVAL] batch:  456 | acc: 87.50%,  total acc: 65.36%   [EVAL] batch:  457 | acc: 100.00%,  total acc: 65.43%   [EVAL] batch:  458 | acc: 100.00%,  total acc: 65.51%   [EVAL] batch:  459 | acc: 93.75%,  total acc: 65.57%   [EVAL] batch:  460 | acc: 93.75%,  total acc: 65.63%   [EVAL] batch:  461 | acc: 100.00%,  total acc: 65.71%   [EVAL] batch:  462 | acc: 75.00%,  total acc: 65.73%   [EVAL] batch:  463 | acc: 87.50%,  total acc: 65.77%   [EVAL] batch:  464 | acc: 93.75%,  total acc: 65.83%   [EVAL] batch:  465 | acc: 93.75%,  total acc: 65.89%   [EVAL] batch:  466 | acc: 75.00%,  total acc: 65.91%   [EVAL] batch:  467 | acc: 100.00%,  total acc: 65.99%   [EVAL] batch:  468 | acc: 87.50%,  total acc: 66.03%   [EVAL] batch:  469 | acc: 100.00%,  total acc: 66.10%   [EVAL] batch:  470 | acc: 100.00%,  total acc: 66.18%   [EVAL] batch:  471 | acc: 100.00%,  total acc: 66.25%   [EVAL] batch:  472 | acc: 93.75%,  total acc: 66.31%   [EVAL] batch:  473 | acc: 100.00%,  total acc: 66.38%   [EVAL] batch:  474 | acc: 100.00%,  total acc: 66.45%   [EVAL] batch:  475 | acc: 25.00%,  total acc: 66.36%   [EVAL] batch:  476 | acc: 56.25%,  total acc: 66.34%   [EVAL] batch:  477 | acc: 25.00%,  total acc: 66.25%   [EVAL] batch:  478 | acc: 12.50%,  total acc: 66.14%   [EVAL] batch:  479 | acc: 6.25%,  total acc: 66.02%   [EVAL] batch:  480 | acc: 25.00%,  total acc: 65.93%   [EVAL] batch:  481 | acc: 75.00%,  total acc: 65.95%   [EVAL] batch:  482 | acc: 87.50%,  total acc: 65.99%   [EVAL] batch:  483 | acc: 81.25%,  total acc: 66.03%   [EVAL] batch:  484 | acc: 100.00%,  total acc: 66.10%   [EVAL] batch:  485 | acc: 87.50%,  total acc: 66.14%   [EVAL] batch:  486 | acc: 81.25%,  total acc: 66.17%   [EVAL] batch:  487 | acc: 56.25%,  total acc: 66.15%   [EVAL] batch:  488 | acc: 50.00%,  total acc: 66.12%   [EVAL] batch:  489 | acc: 31.25%,  total acc: 66.05%   [EVAL] batch:  490 | acc: 43.75%,  total acc: 66.00%   [EVAL] batch:  491 | acc: 56.25%,  total acc: 65.98%   [EVAL] batch:  492 | acc: 43.75%,  total acc: 65.94%   [EVAL] batch:  493 | acc: 37.50%,  total acc: 65.88%   [EVAL] batch:  494 | acc: 81.25%,  total acc: 65.91%   [EVAL] batch:  495 | acc: 75.00%,  total acc: 65.93%   [EVAL] batch:  496 | acc: 43.75%,  total acc: 65.88%   [EVAL] batch:  497 | acc: 62.50%,  total acc: 65.88%   [EVAL] batch:  498 | acc: 87.50%,  total acc: 65.92%   [EVAL] batch:  499 | acc: 81.25%,  total acc: 65.95%   
cur_acc:  ['0.9494', '0.6151', '0.7143', '0.6885', '0.7510', '0.6478', '0.8581', '0.6746']
his_acc:  ['0.9494', '0.7760', '0.7500', '0.7332', '0.7145', '0.6875', '0.6914', '0.6595']
--------Round  2
seed:  300
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hybridprompt_12token/train.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hybridprompt_12token/valid.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hybridprompt_12token/test.pkl
Task_order: [7 2 3 1 5 6 0 4]
prepared data!
Clustering into  4  clusters
Clusters:  [0 3 2 0 1 1 2 0 3 3]
Losses:  19.632648468017578 5.460475921630859 1.244140386581421
CurrentTrain: epoch  0, batch     0 | loss: 19.6326485Losses:  24.01803970336914 9.95770263671875 1.186988115310669
CurrentTrain: epoch  0, batch     1 | loss: 24.0180397Losses:  18.417640686035156 4.853786468505859 1.1645574569702148
CurrentTrain: epoch  0, batch     2 | loss: 18.4176407Losses:  20.050798416137695 6.4832987785339355 1.126124620437622
CurrentTrain: epoch  0, batch     3 | loss: 20.0507984Losses:  17.823881149291992 4.466606140136719 1.091184139251709
CurrentTrain: epoch  0, batch     4 | loss: 17.8238811Losses:  16.44411849975586 3.6552488803863525 1.0392253398895264
CurrentTrain: epoch  0, batch     5 | loss: 16.4441185Losses:  16.597700119018555 4.030026435852051 0.951630711555481
CurrentTrain: epoch  0, batch     6 | loss: 16.5977001Losses:  17.29530906677246 4.6606621742248535 0.9579636454582214
CurrentTrain: epoch  0, batch     7 | loss: 17.2953091Losses:  17.50653648376465 5.424857139587402 0.7886793613433838
CurrentTrain: epoch  0, batch     8 | loss: 17.5065365Losses:  16.263959884643555 4.108605861663818 0.8768149614334106
CurrentTrain: epoch  0, batch     9 | loss: 16.2639599Losses:  18.205469131469727 5.816980838775635 0.7459172010421753
CurrentTrain: epoch  0, batch    10 | loss: 18.2054691Losses:  14.966691017150879 2.982877731323242 0.836232602596283
CurrentTrain: epoch  0, batch    11 | loss: 14.9666910Losses:  16.5125732421875 4.891071796417236 0.7223455905914307
CurrentTrain: epoch  0, batch    12 | loss: 16.5125732Losses:  17.891820907592773 6.131148815155029 0.7603774666786194
CurrentTrain: epoch  0, batch    13 | loss: 17.8918209Losses:  16.600589752197266 5.086050987243652 0.7482132315635681
CurrentTrain: epoch  0, batch    14 | loss: 16.6005898Losses:  15.051729202270508 3.7025442123413086 0.6866785287857056
CurrentTrain: epoch  0, batch    15 | loss: 15.0517292Losses:  15.666031837463379 4.432415008544922 0.6139194965362549
CurrentTrain: epoch  0, batch    16 | loss: 15.6660318Losses:  18.376972198486328 7.081350326538086 0.656909167766571
CurrentTrain: epoch  0, batch    17 | loss: 18.3769722Losses:  15.618637084960938 4.486198425292969 0.6058595180511475
CurrentTrain: epoch  0, batch    18 | loss: 15.6186371Losses:  14.553502082824707 4.180209636688232 0.5964630842208862
CurrentTrain: epoch  0, batch    19 | loss: 14.5535021Losses:  16.53040313720703 5.696717739105225 0.6012223958969116
CurrentTrain: epoch  0, batch    20 | loss: 16.5304031Losses:  17.839336395263672 7.079746723175049 0.4407247304916382
CurrentTrain: epoch  0, batch    21 | loss: 17.8393364Losses:  18.377410888671875 7.537626266479492 0.5764474272727966
CurrentTrain: epoch  0, batch    22 | loss: 18.3774109Losses:  15.38834285736084 4.734339237213135 0.5469715595245361
CurrentTrain: epoch  0, batch    23 | loss: 15.3883429Losses:  14.178436279296875 3.6926276683807373 0.5452963709831238
CurrentTrain: epoch  0, batch    24 | loss: 14.1784363Losses:  15.561887741088867 4.890714645385742 0.5382263660430908
CurrentTrain: epoch  0, batch    25 | loss: 15.5618877Losses:  14.967638969421387 4.180120468139648 0.5591630935668945
CurrentTrain: epoch  0, batch    26 | loss: 14.9676390Losses:  18.057147979736328 7.163242340087891 0.5110888481140137
CurrentTrain: epoch  0, batch    27 | loss: 18.0571480Losses:  15.357582092285156 4.868781089782715 0.508345365524292
CurrentTrain: epoch  0, batch    28 | loss: 15.3575821Losses:  18.068958282470703 7.495182037353516 0.5479660034179688
CurrentTrain: epoch  0, batch    29 | loss: 18.0689583Losses:  20.875978469848633 9.527007102966309 0.46848076581954956
CurrentTrain: epoch  0, batch    30 | loss: 20.8759785Losses:  18.7044734954834 7.904880523681641 0.45522767305374146
CurrentTrain: epoch  0, batch    31 | loss: 18.7044735Losses:  14.85659408569336 5.151578903198242 0.4829692244529724
CurrentTrain: epoch  0, batch    32 | loss: 14.8565941Losses:  14.664003372192383 4.26136589050293 0.49079620838165283
CurrentTrain: epoch  0, batch    33 | loss: 14.6640034Losses:  14.74052619934082 4.612462997436523 0.5055720806121826
CurrentTrain: epoch  0, batch    34 | loss: 14.7405262Losses:  13.33331298828125 3.5457911491394043 0.4854409694671631
CurrentTrain: epoch  0, batch    35 | loss: 13.3333130Losses:  16.443410873413086 6.229235649108887 0.5343834757804871
CurrentTrain: epoch  0, batch    36 | loss: 16.4434109Losses:  14.97994613647461 4.982637405395508 0.5255953073501587
CurrentTrain: epoch  0, batch    37 | loss: 14.9799461Losses:  14.991182327270508 4.604855537414551 0.547783374786377
CurrentTrain: epoch  0, batch    38 | loss: 14.9911823Losses:  12.653072357177734 3.153456211090088 0.5173393487930298
CurrentTrain: epoch  0, batch    39 | loss: 12.6530724Losses:  16.5814151763916 6.856009006500244 0.5000762939453125
CurrentTrain: epoch  0, batch    40 | loss: 16.5814152Losses:  15.648164749145508 6.721113204956055 0.4419676661491394
CurrentTrain: epoch  0, batch    41 | loss: 15.6481647Losses:  20.437849044799805 10.79212760925293 0.4750715494155884
CurrentTrain: epoch  0, batch    42 | loss: 20.4378490Losses:  13.996432304382324 4.551270961761475 0.4506351351737976
CurrentTrain: epoch  0, batch    43 | loss: 13.9964323Losses:  15.155301094055176 5.211118221282959 0.47980937361717224
CurrentTrain: epoch  0, batch    44 | loss: 15.1553011Losses:  14.743687629699707 5.767446994781494 0.4461846947669983
CurrentTrain: epoch  0, batch    45 | loss: 14.7436876Losses:  14.44322681427002 4.7858686447143555 0.43909114599227905
CurrentTrain: epoch  0, batch    46 | loss: 14.4432268Losses:  13.59062385559082 4.958930492401123 0.38917502760887146
CurrentTrain: epoch  0, batch    47 | loss: 13.5906239Losses:  14.485528945922852 5.564515113830566 0.4350213408470154
CurrentTrain: epoch  0, batch    48 | loss: 14.4855289Losses:  14.706446647644043 5.474755764007568 0.410156786441803
CurrentTrain: epoch  0, batch    49 | loss: 14.7064466Losses:  11.903203010559082 2.957582473754883 0.44476452469825745
CurrentTrain: epoch  0, batch    50 | loss: 11.9032030Losses:  13.106108665466309 4.297861099243164 0.38081449270248413
CurrentTrain: epoch  0, batch    51 | loss: 13.1061087Losses:  13.828853607177734 4.869645118713379 0.4179830551147461
CurrentTrain: epoch  0, batch    52 | loss: 13.8288536Losses:  16.65679168701172 7.578142166137695 0.4153859615325928
CurrentTrain: epoch  0, batch    53 | loss: 16.6567917Losses:  10.594687461853027 2.3977737426757812 0.4180322587490082
CurrentTrain: epoch  0, batch    54 | loss: 10.5946875Losses:  12.211443901062012 3.788468837738037 0.41196000576019287
CurrentTrain: epoch  0, batch    55 | loss: 12.2114439Losses:  12.102193832397461 3.9980618953704834 0.3803436756134033
CurrentTrain: epoch  0, batch    56 | loss: 12.1021938Losses:  10.532662391662598 2.8658056259155273 0.3896295428276062
CurrentTrain: epoch  0, batch    57 | loss: 10.5326624Losses:  15.164983749389648 6.422573089599609 0.39510899782180786
CurrentTrain: epoch  0, batch    58 | loss: 15.1649837Losses:  9.911476135253906 2.241612434387207 0.3901960849761963
CurrentTrain: epoch  0, batch    59 | loss: 9.9114761Losses:  13.715887069702148 5.1166229248046875 0.44658327102661133
CurrentTrain: epoch  0, batch    60 | loss: 13.7158871Losses:  14.920740127563477 5.996453285217285 0.38654762506484985
CurrentTrain: epoch  0, batch    61 | loss: 14.9207401Losses:  10.829026222229004 2.4292991161346436 0.27238041162490845
CurrentTrain: epoch  0, batch    62 | loss: 10.8290262Losses:  11.637190818786621 3.8711938858032227 0.3751433491706848
CurrentTrain: epoch  1, batch     0 | loss: 11.6371908Losses:  10.838789939880371 3.1940243244171143 0.3728657364845276
CurrentTrain: epoch  1, batch     1 | loss: 10.8387899Losses:  12.06814956665039 4.231291770935059 0.36885976791381836
CurrentTrain: epoch  1, batch     2 | loss: 12.0681496Losses:  13.038895606994629 5.358392715454102 0.35359686613082886
CurrentTrain: epoch  1, batch     3 | loss: 13.0388956Losses:  9.6470308303833 2.4290239810943604 0.3592301309108734
CurrentTrain: epoch  1, batch     4 | loss: 9.6470308Losses:  12.107621192932129 4.222522735595703 0.3676159977912903
CurrentTrain: epoch  1, batch     5 | loss: 12.1076212Losses:  11.396991729736328 3.7220873832702637 0.3690284490585327
CurrentTrain: epoch  1, batch     6 | loss: 11.3969917Losses:  10.882412910461426 3.2615742683410645 0.3496124744415283
CurrentTrain: epoch  1, batch     7 | loss: 10.8824129Losses:  11.325222969055176 3.552354335784912 0.3586476445198059
CurrentTrain: epoch  1, batch     8 | loss: 11.3252230Losses:  14.082904815673828 5.1181321144104 0.36353904008865356
CurrentTrain: epoch  1, batch     9 | loss: 14.0829048Losses:  11.861654281616211 5.139090538024902 0.23463968932628632
CurrentTrain: epoch  1, batch    10 | loss: 11.8616543Losses:  17.181827545166016 8.542322158813477 0.34738627076148987
CurrentTrain: epoch  1, batch    11 | loss: 17.1818275Losses:  12.611059188842773 4.404150009155273 0.35382193326950073
CurrentTrain: epoch  1, batch    12 | loss: 12.6110592Losses:  11.118870735168457 2.8300981521606445 0.36780762672424316
CurrentTrain: epoch  1, batch    13 | loss: 11.1188707Losses:  11.447505950927734 2.9317378997802734 0.3576923608779907
CurrentTrain: epoch  1, batch    14 | loss: 11.4475060Losses:  14.720585823059082 6.365848541259766 0.39038148522377014
CurrentTrain: epoch  1, batch    15 | loss: 14.7205858Losses:  13.200544357299805 5.253706932067871 0.35427671670913696
CurrentTrain: epoch  1, batch    16 | loss: 13.2005444Losses:  12.95179557800293 4.775987148284912 0.34209302067756653
CurrentTrain: epoch  1, batch    17 | loss: 12.9517956Losses:  9.6633939743042 1.8956263065338135 0.33869805932044983
CurrentTrain: epoch  1, batch    18 | loss: 9.6633940Losses:  11.086502075195312 3.306297779083252 0.3416210412979126
CurrentTrain: epoch  1, batch    19 | loss: 11.0865021Losses:  10.594919204711914 3.151203155517578 0.31573888659477234
CurrentTrain: epoch  1, batch    20 | loss: 10.5949192Losses:  12.095335006713867 4.354875087738037 0.334223210811615
CurrentTrain: epoch  1, batch    21 | loss: 12.0953350Losses:  14.59782886505127 7.099458694458008 0.3463800251483917
CurrentTrain: epoch  1, batch    22 | loss: 14.5978289Losses:  11.371525764465332 3.7875306606292725 0.3758588433265686
CurrentTrain: epoch  1, batch    23 | loss: 11.3715258Losses:  9.986891746520996 2.9423561096191406 0.3206670880317688
CurrentTrain: epoch  1, batch    24 | loss: 9.9868917Losses:  12.860655784606934 5.100377082824707 0.3337072432041168
CurrentTrain: epoch  1, batch    25 | loss: 12.8606558Losses:  11.307940483093262 3.670391082763672 0.3295525312423706
CurrentTrain: epoch  1, batch    26 | loss: 11.3079405Losses:  11.00058650970459 3.7076401710510254 0.3321659564971924
CurrentTrain: epoch  1, batch    27 | loss: 11.0005865Losses:  11.397079467773438 3.0931739807128906 0.3308371901512146
CurrentTrain: epoch  1, batch    28 | loss: 11.3970795Losses:  11.223562240600586 3.6523609161376953 0.31255945563316345
CurrentTrain: epoch  1, batch    29 | loss: 11.2235622Losses:  12.387292861938477 4.378534317016602 0.34007012844085693
CurrentTrain: epoch  1, batch    30 | loss: 12.3872929Losses:  14.58707046508789 7.0325188636779785 0.34011074900627136
CurrentTrain: epoch  1, batch    31 | loss: 14.5870705Losses:  11.66120719909668 4.350553035736084 0.34003105759620667
CurrentTrain: epoch  1, batch    32 | loss: 11.6612072Losses:  12.671649932861328 4.499381065368652 0.3727211356163025
CurrentTrain: epoch  1, batch    33 | loss: 12.6716499Losses:  11.95409107208252 4.663057327270508 0.3507348299026489
CurrentTrain: epoch  1, batch    34 | loss: 11.9540911Losses:  12.463735580444336 4.780832290649414 0.3482174873352051
CurrentTrain: epoch  1, batch    35 | loss: 12.4637356Losses:  13.663002967834473 5.9097490310668945 0.35938572883605957
CurrentTrain: epoch  1, batch    36 | loss: 13.6630030Losses:  9.960856437683105 3.2275376319885254 0.3279677629470825
CurrentTrain: epoch  1, batch    37 | loss: 9.9608564Losses:  12.587559700012207 5.762233734130859 0.34840908646583557
CurrentTrain: epoch  1, batch    38 | loss: 12.5875597Losses:  10.261240005493164 2.9203901290893555 0.33402347564697266
CurrentTrain: epoch  1, batch    39 | loss: 10.2612400Losses:  11.816781044006348 5.505757808685303 0.3287016749382019
CurrentTrain: epoch  1, batch    40 | loss: 11.8167810Losses:  10.748772621154785 3.452913284301758 0.32978731393814087
CurrentTrain: epoch  1, batch    41 | loss: 10.7487726Losses:  10.78127384185791 3.440971851348877 0.3415176272392273
CurrentTrain: epoch  1, batch    42 | loss: 10.7812738Losses:  12.183605194091797 5.063027381896973 0.3324214518070221
CurrentTrain: epoch  1, batch    43 | loss: 12.1836052Losses:  11.694269180297852 3.9594650268554688 0.33142969012260437
CurrentTrain: epoch  1, batch    44 | loss: 11.6942692Losses:  9.481239318847656 3.2887766361236572 0.31567519903182983
CurrentTrain: epoch  1, batch    45 | loss: 9.4812393Losses:  9.168702125549316 2.6100826263427734 0.30450713634490967
CurrentTrain: epoch  1, batch    46 | loss: 9.1687021Losses:  12.711325645446777 5.221897602081299 0.3360461890697479
CurrentTrain: epoch  1, batch    47 | loss: 12.7113256Losses:  13.671683311462402 5.963974475860596 0.2527282238006592
CurrentTrain: epoch  1, batch    48 | loss: 13.6716833Losses:  11.05892562866211 3.916001319885254 0.3095044493675232
CurrentTrain: epoch  1, batch    49 | loss: 11.0589256Losses:  14.146278381347656 5.7236714363098145 0.3168601393699646
CurrentTrain: epoch  1, batch    50 | loss: 14.1462784Losses:  11.998053550720215 5.422747611999512 0.32269132137298584
CurrentTrain: epoch  1, batch    51 | loss: 11.9980536Losses:  10.821873664855957 3.7815966606140137 0.30589982867240906
CurrentTrain: epoch  1, batch    52 | loss: 10.8218737Losses:  10.240765571594238 3.348963737487793 0.30820325016975403
CurrentTrain: epoch  1, batch    53 | loss: 10.2407656Losses:  11.018914222717285 4.499699592590332 0.32318422198295593
CurrentTrain: epoch  1, batch    54 | loss: 11.0189142Losses:  9.486886024475098 3.349233388900757 0.3011550009250641
CurrentTrain: epoch  1, batch    55 | loss: 9.4868860Losses:  9.633428573608398 3.1611385345458984 0.3203444480895996
CurrentTrain: epoch  1, batch    56 | loss: 9.6334286Losses:  8.38461971282959 2.04681134223938 0.28112471103668213
CurrentTrain: epoch  1, batch    57 | loss: 8.3846197Losses:  12.037678718566895 6.727705955505371 0.29163432121276855
CurrentTrain: epoch  1, batch    58 | loss: 12.0376787Losses:  9.110405921936035 3.018533229827881 0.3047855496406555
CurrentTrain: epoch  1, batch    59 | loss: 9.1104059Losses:  8.622103691101074 2.6161112785339355 0.30660873651504517
CurrentTrain: epoch  1, batch    60 | loss: 8.6221037Losses:  10.13969612121582 4.886807918548584 0.2932925224304199
CurrentTrain: epoch  1, batch    61 | loss: 10.1396961Losses:  7.2019805908203125 1.1925159692764282 0.28119128942489624
CurrentTrain: epoch  1, batch    62 | loss: 7.2019806Losses:  12.035873413085938 5.5961785316467285 0.31104281544685364
CurrentTrain: epoch  2, batch     0 | loss: 12.0358734Losses:  10.686984062194824 4.1678690910339355 0.3101491928100586
CurrentTrain: epoch  2, batch     1 | loss: 10.6869841Losses:  9.536107063293457 3.2046098709106445 0.29587459564208984
CurrentTrain: epoch  2, batch     2 | loss: 9.5361071Losses:  11.594955444335938 4.908347129821777 0.25754353404045105
CurrentTrain: epoch  2, batch     3 | loss: 11.5949554Losses:  9.117280960083008 3.052797317504883 0.2932945489883423
CurrentTrain: epoch  2, batch     4 | loss: 9.1172810Losses:  12.546651840209961 5.227941513061523 0.3050205707550049
CurrentTrain: epoch  2, batch     5 | loss: 12.5466518Losses:  12.277088165283203 6.568936347961426 0.29573822021484375
CurrentTrain: epoch  2, batch     6 | loss: 12.2770882Losses:  9.801384925842285 3.97761869430542 0.28961098194122314
CurrentTrain: epoch  2, batch     7 | loss: 9.8013849Losses:  8.200583457946777 2.4432756900787354 0.29681140184402466
CurrentTrain: epoch  2, batch     8 | loss: 8.2005835Losses:  8.651700019836426 2.78316593170166 0.3081032633781433
CurrentTrain: epoch  2, batch     9 | loss: 8.6517000Losses:  10.017036437988281 3.6622681617736816 0.30791059136390686
CurrentTrain: epoch  2, batch    10 | loss: 10.0170364Losses:  8.293041229248047 2.2853806018829346 0.28021740913391113
CurrentTrain: epoch  2, batch    11 | loss: 8.2930412Losses:  10.478751182556152 4.27559757232666 0.323201447725296
CurrentTrain: epoch  2, batch    12 | loss: 10.4787512Losses:  13.243438720703125 6.84324836730957 0.34062159061431885
CurrentTrain: epoch  2, batch    13 | loss: 13.2434387Losses:  9.606608390808105 3.613248825073242 0.279180645942688
CurrentTrain: epoch  2, batch    14 | loss: 9.6066084Losses:  11.555547714233398 4.6899871826171875 0.30027732253074646
CurrentTrain: epoch  2, batch    15 | loss: 11.5555477Losses:  12.047389030456543 6.448544025421143 0.17296366393566132
CurrentTrain: epoch  2, batch    16 | loss: 12.0473890Losses:  8.275507926940918 2.3105721473693848 0.30266398191452026
CurrentTrain: epoch  2, batch    17 | loss: 8.2755079Losses:  11.003830909729004 4.216041564941406 0.30414679646492004
CurrentTrain: epoch  2, batch    18 | loss: 11.0038309Losses:  11.276989936828613 4.205655097961426 0.2973898649215698
CurrentTrain: epoch  2, batch    19 | loss: 11.2769899Losses:  10.267109870910645 4.773007392883301 0.2851633131504059
CurrentTrain: epoch  2, batch    20 | loss: 10.2671099Losses:  11.910656929016113 5.881650924682617 0.3160886764526367
CurrentTrain: epoch  2, batch    21 | loss: 11.9106569Losses:  10.91316032409668 4.740301132202148 0.31043386459350586
CurrentTrain: epoch  2, batch    22 | loss: 10.9131603Losses:  11.766397476196289 6.447254180908203 0.28185170888900757
CurrentTrain: epoch  2, batch    23 | loss: 11.7663975Losses:  11.526674270629883 3.9594368934631348 0.2981949746608734
CurrentTrain: epoch  2, batch    24 | loss: 11.5266743Losses:  8.744392395019531 2.97509503364563 0.29676181077957153
CurrentTrain: epoch  2, batch    25 | loss: 8.7443924Losses:  9.05686092376709 3.6048102378845215 0.28481030464172363
CurrentTrain: epoch  2, batch    26 | loss: 9.0568609Losses:  8.740906715393066 3.027501344680786 0.2744542360305786
CurrentTrain: epoch  2, batch    27 | loss: 8.7409067Losses:  9.573891639709473 4.434088706970215 0.27232810854911804
CurrentTrain: epoch  2, batch    28 | loss: 9.5738916Losses:  10.262064933776855 4.102889060974121 0.2724984884262085
CurrentTrain: epoch  2, batch    29 | loss: 10.2620649Losses:  13.685961723327637 7.568558692932129 0.2902504801750183
CurrentTrain: epoch  2, batch    30 | loss: 13.6859617Losses:  9.450959205627441 3.4688663482666016 0.26016494631767273
CurrentTrain: epoch  2, batch    31 | loss: 9.4509592Losses:  8.338924407958984 2.4772372245788574 0.26423609256744385
CurrentTrain: epoch  2, batch    32 | loss: 8.3389244Losses:  8.913394927978516 3.2305703163146973 0.2753638029098511
CurrentTrain: epoch  2, batch    33 | loss: 8.9133949Losses:  8.198819160461426 2.6574292182922363 0.26688751578330994
CurrentTrain: epoch  2, batch    34 | loss: 8.1988192Losses:  8.701169967651367 2.681692600250244 0.28698110580444336
CurrentTrain: epoch  2, batch    35 | loss: 8.7011700Losses:  10.242889404296875 3.4370927810668945 0.290144681930542
CurrentTrain: epoch  2, batch    36 | loss: 10.2428894Losses:  8.7485990524292 3.023682117462158 0.28181368112564087
CurrentTrain: epoch  2, batch    37 | loss: 8.7485991Losses:  10.014262199401855 3.422464370727539 0.3116358518600464
CurrentTrain: epoch  2, batch    38 | loss: 10.0142622Losses:  8.343411445617676 2.4938714504241943 0.27450984716415405
CurrentTrain: epoch  2, batch    39 | loss: 8.3434114Losses:  13.64941120147705 7.294582366943359 0.18747112154960632
CurrentTrain: epoch  2, batch    40 | loss: 13.6494112Losses:  7.911477088928223 2.333625316619873 0.26280543208122253
CurrentTrain: epoch  2, batch    41 | loss: 7.9114771Losses:  7.828347206115723 2.3914618492126465 0.2610471248626709
CurrentTrain: epoch  2, batch    42 | loss: 7.8283472Losses:  11.877708435058594 5.493152618408203 0.3221953511238098
CurrentTrain: epoch  2, batch    43 | loss: 11.8777084Losses:  9.262519836425781 3.4985244274139404 0.27912676334381104
CurrentTrain: epoch  2, batch    44 | loss: 9.2625198Losses:  14.233658790588379 7.9308061599731445 0.20539727807044983
CurrentTrain: epoch  2, batch    45 | loss: 14.2336588Losses:  9.707490921020508 4.686599254608154 0.2553499639034271
CurrentTrain: epoch  2, batch    46 | loss: 9.7074909Losses:  10.029415130615234 3.4647674560546875 0.2642778754234314
CurrentTrain: epoch  2, batch    47 | loss: 10.0294151Losses:  9.903709411621094 3.150932788848877 0.30055296421051025
CurrentTrain: epoch  2, batch    48 | loss: 9.9037094Losses:  8.582645416259766 2.88653826713562 0.2819366157054901
CurrentTrain: epoch  2, batch    49 | loss: 8.5826454Losses:  9.251373291015625 3.362847328186035 0.2553502917289734
CurrentTrain: epoch  2, batch    50 | loss: 9.2513733Losses:  7.934613227844238 2.8097376823425293 0.2618536949157715
CurrentTrain: epoch  2, batch    51 | loss: 7.9346132Losses:  9.14048957824707 2.9680325984954834 0.2754005193710327
CurrentTrain: epoch  2, batch    52 | loss: 9.1404896Losses:  8.131400108337402 2.3850948810577393 0.25712499022483826
CurrentTrain: epoch  2, batch    53 | loss: 8.1314001Losses:  9.341241836547852 3.781440496444702 0.26831260323524475
CurrentTrain: epoch  2, batch    54 | loss: 9.3412418Losses:  9.149336814880371 3.5413622856140137 0.17799639701843262
CurrentTrain: epoch  2, batch    55 | loss: 9.1493368Losses:  7.458171367645264 2.1718711853027344 0.2685462236404419
CurrentTrain: epoch  2, batch    56 | loss: 7.4581714Losses:  8.630974769592285 3.0137779712677 0.2773638069629669
CurrentTrain: epoch  2, batch    57 | loss: 8.6309748Losses:  9.86232852935791 4.445444583892822 0.28010833263397217
CurrentTrain: epoch  2, batch    58 | loss: 9.8623285Losses:  7.723288536071777 2.5791544914245605 0.28060364723205566
CurrentTrain: epoch  2, batch    59 | loss: 7.7232885Losses:  9.717337608337402 3.706613302230835 0.28526318073272705
CurrentTrain: epoch  2, batch    60 | loss: 9.7173376Losses:  9.916679382324219 4.515735149383545 0.27097663283348083
CurrentTrain: epoch  2, batch    61 | loss: 9.9166794Losses:  5.687134742736816 0.27400845289230347 0.3066301941871643
CurrentTrain: epoch  2, batch    62 | loss: 5.6871347Losses:  10.33364200592041 4.257224082946777 0.26360607147216797
CurrentTrain: epoch  3, batch     0 | loss: 10.3336420Losses:  8.208757400512695 3.286996603012085 0.25029677152633667
CurrentTrain: epoch  3, batch     1 | loss: 8.2087574Losses:  7.9347243309021 2.4777865409851074 0.2602955102920532
CurrentTrain: epoch  3, batch     2 | loss: 7.9347243Losses:  10.00483512878418 4.680780410766602 0.27338582277297974
CurrentTrain: epoch  3, batch     3 | loss: 10.0048351Losses:  9.19810676574707 3.941131353378296 0.28053730726242065
CurrentTrain: epoch  3, batch     4 | loss: 9.1981068Losses:  7.3676652908325195 1.5294126272201538 0.2801794707775116
CurrentTrain: epoch  3, batch     5 | loss: 7.3676653Losses:  8.234354019165039 3.213406562805176 0.2884127199649811
CurrentTrain: epoch  3, batch     6 | loss: 8.2343540Losses:  8.135120391845703 2.8714146614074707 0.27371612191200256
CurrentTrain: epoch  3, batch     7 | loss: 8.1351204Losses:  8.283794403076172 2.7519617080688477 0.2743801474571228
CurrentTrain: epoch  3, batch     8 | loss: 8.2837944Losses:  8.607008934020996 3.211641788482666 0.27383676171302795
CurrentTrain: epoch  3, batch     9 | loss: 8.6070089Losses:  8.364049911499023 2.700188398361206 0.28074216842651367
CurrentTrain: epoch  3, batch    10 | loss: 8.3640499Losses:  7.783563137054443 2.3776164054870605 0.2684211730957031
CurrentTrain: epoch  3, batch    11 | loss: 7.7835631Losses:  7.110226631164551 2.0811355113983154 0.2412063479423523
CurrentTrain: epoch  3, batch    12 | loss: 7.1102266Losses:  9.564046859741211 4.346127033233643 0.2857416272163391
CurrentTrain: epoch  3, batch    13 | loss: 9.5640469Losses:  7.84274959564209 2.9106571674346924 0.25561222434043884
CurrentTrain: epoch  3, batch    14 | loss: 7.8427496Losses:  8.532564163208008 3.0723166465759277 0.260822057723999
CurrentTrain: epoch  3, batch    15 | loss: 8.5325642Losses:  7.27621603012085 2.147690773010254 0.2682674527168274
CurrentTrain: epoch  3, batch    16 | loss: 7.2762160Losses:  8.949989318847656 2.925074338912964 0.2808316946029663
CurrentTrain: epoch  3, batch    17 | loss: 8.9499893Losses:  8.53021240234375 3.342406749725342 0.2673817574977875
CurrentTrain: epoch  3, batch    18 | loss: 8.5302124Losses:  8.146605491638184 3.056367874145508 0.2787759304046631
CurrentTrain: epoch  3, batch    19 | loss: 8.1466055Losses:  8.088988304138184 2.6706323623657227 0.2660677433013916
CurrentTrain: epoch  3, batch    20 | loss: 8.0889883Losses:  8.645120620727539 3.7316150665283203 0.26700109243392944
CurrentTrain: epoch  3, batch    21 | loss: 8.6451206Losses:  8.180541038513184 2.4949474334716797 0.2527214288711548
CurrentTrain: epoch  3, batch    22 | loss: 8.1805410Losses:  10.488758087158203 5.639998435974121 0.28327226638793945
CurrentTrain: epoch  3, batch    23 | loss: 10.4887581Losses:  7.1018147468566895 2.1195106506347656 0.27225905656814575
CurrentTrain: epoch  3, batch    24 | loss: 7.1018147Losses:  9.452275276184082 4.844043731689453 0.2465953528881073
CurrentTrain: epoch  3, batch    25 | loss: 9.4522753Losses:  7.521523952484131 2.3162755966186523 0.24505729973316193
CurrentTrain: epoch  3, batch    26 | loss: 7.5215240Losses:  10.656064987182617 4.901493072509766 0.28311997652053833
CurrentTrain: epoch  3, batch    27 | loss: 10.6560650Losses:  7.2205705642700195 2.283224582672119 0.2548762559890747
CurrentTrain: epoch  3, batch    28 | loss: 7.2205706Losses:  10.446416854858398 5.136562824249268 0.2780759334564209
CurrentTrain: epoch  3, batch    29 | loss: 10.4464169Losses:  8.711552619934082 3.9479849338531494 0.2557228207588196
CurrentTrain: epoch  3, batch    30 | loss: 8.7115526Losses:  8.468457221984863 3.3777246475219727 0.27705198526382446
CurrentTrain: epoch  3, batch    31 | loss: 8.4684572Losses:  12.452601432800293 7.284988880157471 0.22681133449077606
CurrentTrain: epoch  3, batch    32 | loss: 12.4526014Losses:  9.781421661376953 4.817787170410156 0.2632405161857605
CurrentTrain: epoch  3, batch    33 | loss: 9.7814217Losses:  9.959502220153809 4.476282596588135 0.3007882535457611
CurrentTrain: epoch  3, batch    34 | loss: 9.9595022Losses:  7.971470355987549 2.9706361293792725 0.2796541452407837
CurrentTrain: epoch  3, batch    35 | loss: 7.9714704Losses:  8.24674129486084 3.279733657836914 0.2574698328971863
CurrentTrain: epoch  3, batch    36 | loss: 8.2467413Losses:  6.949997901916504 2.0440664291381836 0.2686641812324524
CurrentTrain: epoch  3, batch    37 | loss: 6.9499979Losses:  8.41635799407959 3.3755674362182617 0.25305822491645813
CurrentTrain: epoch  3, batch    38 | loss: 8.4163580Losses:  8.873529434204102 2.8726439476013184 0.2714197635650635
CurrentTrain: epoch  3, batch    39 | loss: 8.8735294Losses:  7.378840446472168 2.150815010070801 0.25016409158706665
CurrentTrain: epoch  3, batch    40 | loss: 7.3788404Losses:  9.89410400390625 4.130402088165283 0.2853701114654541
CurrentTrain: epoch  3, batch    41 | loss: 9.8941040Losses:  8.486434936523438 3.214803695678711 0.18528154492378235
CurrentTrain: epoch  3, batch    42 | loss: 8.4864349Losses:  9.166159629821777 3.296705484390259 0.18549031019210815
CurrentTrain: epoch  3, batch    43 | loss: 9.1661596Losses:  8.180499076843262 2.976508855819702 0.2556668221950531
CurrentTrain: epoch  3, batch    44 | loss: 8.1804991Losses:  7.765512466430664 2.5296366214752197 0.24920962750911713
CurrentTrain: epoch  3, batch    45 | loss: 7.7655125Losses:  8.843212127685547 3.7313761711120605 0.25458917021751404
CurrentTrain: epoch  3, batch    46 | loss: 8.8432121Losses:  10.165661811828613 5.368161678314209 0.267927885055542
CurrentTrain: epoch  3, batch    47 | loss: 10.1656618Losses:  8.302983283996582 3.081019878387451 0.25801903009414673
CurrentTrain: epoch  3, batch    48 | loss: 8.3029833Losses:  7.529729843139648 2.79585599899292 0.24761900305747986
CurrentTrain: epoch  3, batch    49 | loss: 7.5297298Losses:  8.160603523254395 2.868670701980591 0.25430136919021606
CurrentTrain: epoch  3, batch    50 | loss: 8.1606035Losses:  8.036587715148926 3.0820372104644775 0.26028525829315186
CurrentTrain: epoch  3, batch    51 | loss: 8.0365877Losses:  11.313447952270508 6.1492085456848145 0.26193052530288696
CurrentTrain: epoch  3, batch    52 | loss: 11.3134480Losses:  9.294017791748047 3.6931228637695312 0.26169663667678833
CurrentTrain: epoch  3, batch    53 | loss: 9.2940178Losses:  8.050666809082031 3.11529278755188 0.25061196088790894
CurrentTrain: epoch  3, batch    54 | loss: 8.0506668Losses:  10.346367835998535 5.518954277038574 0.2735435962677002
CurrentTrain: epoch  3, batch    55 | loss: 10.3463678Losses:  7.007064342498779 2.077265739440918 0.24397823214530945
CurrentTrain: epoch  3, batch    56 | loss: 7.0070643Losses:  7.3678460121154785 2.308979034423828 0.22829557955265045
CurrentTrain: epoch  3, batch    57 | loss: 7.3678460Losses:  9.148578643798828 4.215115070343018 0.2550168037414551
CurrentTrain: epoch  3, batch    58 | loss: 9.1485786Losses:  6.964719295501709 2.2454283237457275 0.24511663615703583
CurrentTrain: epoch  3, batch    59 | loss: 6.9647193Losses:  7.92073917388916 2.661562204360962 0.2518312335014343
CurrentTrain: epoch  3, batch    60 | loss: 7.9207392Losses:  11.189701080322266 6.324836730957031 0.2542526423931122
CurrentTrain: epoch  3, batch    61 | loss: 11.1897011Losses:  6.997222423553467 1.5875951051712036 0.18820993602275848
CurrentTrain: epoch  3, batch    62 | loss: 6.9972224Losses:  9.468587875366211 3.879206895828247 0.2326732873916626
CurrentTrain: epoch  4, batch     0 | loss: 9.4685879Losses:  9.046636581420898 4.488631725311279 0.1521775722503662
CurrentTrain: epoch  4, batch     1 | loss: 9.0466366Losses:  7.983479976654053 3.185375213623047 0.2536684274673462
CurrentTrain: epoch  4, batch     2 | loss: 7.9834800Losses:  7.903670310974121 3.1943392753601074 0.23379205167293549
CurrentTrain: epoch  4, batch     3 | loss: 7.9036703Losses:  6.839098930358887 2.012073516845703 0.2410033643245697
CurrentTrain: epoch  4, batch     4 | loss: 6.8390989Losses:  9.97210693359375 5.265846252441406 0.26106753945350647
CurrentTrain: epoch  4, batch     5 | loss: 9.9721069Losses:  7.311916351318359 2.5981593132019043 0.24308623373508453
CurrentTrain: epoch  4, batch     6 | loss: 7.3119164Losses:  9.075222969055176 3.8153743743896484 0.26609429717063904
CurrentTrain: epoch  4, batch     7 | loss: 9.0752230Losses:  7.1467204093933105 2.559373140335083 0.2518409490585327
CurrentTrain: epoch  4, batch     8 | loss: 7.1467204Losses:  8.088788986206055 3.1713829040527344 0.24230103194713593
CurrentTrain: epoch  4, batch     9 | loss: 8.0887890Losses:  9.014610290527344 3.62176513671875 0.255452960729599
CurrentTrain: epoch  4, batch    10 | loss: 9.0146103Losses:  6.542501926422119 1.9269397258758545 0.23411619663238525
CurrentTrain: epoch  4, batch    11 | loss: 6.5425019Losses:  9.245656967163086 4.624555587768555 0.26048219203948975
CurrentTrain: epoch  4, batch    12 | loss: 9.2456570Losses:  10.042928695678711 5.037081718444824 0.2615818977355957
CurrentTrain: epoch  4, batch    13 | loss: 10.0429287Losses:  7.498236179351807 2.7628188133239746 0.23587830364704132
CurrentTrain: epoch  4, batch    14 | loss: 7.4982362Losses:  7.878331184387207 3.2936646938323975 0.248393252491951
CurrentTrain: epoch  4, batch    15 | loss: 7.8783312Losses:  7.806653022766113 3.0534842014312744 0.22812160849571228
CurrentTrain: epoch  4, batch    16 | loss: 7.8066530Losses:  7.487649440765381 2.598322629928589 0.24964362382888794
CurrentTrain: epoch  4, batch    17 | loss: 7.4876494Losses:  8.88071060180664 3.6689469814300537 0.2337280809879303
CurrentTrain: epoch  4, batch    18 | loss: 8.8807106Losses:  6.863641262054443 2.221877098083496 0.24039079248905182
CurrentTrain: epoch  4, batch    19 | loss: 6.8636413Losses:  11.388459205627441 5.354207992553711 0.17337295413017273
CurrentTrain: epoch  4, batch    20 | loss: 11.3884592Losses:  7.499312400817871 2.856902599334717 0.24110467731952667
CurrentTrain: epoch  4, batch    21 | loss: 7.4993124Losses:  7.2441792488098145 2.312241554260254 0.2347342073917389
CurrentTrain: epoch  4, batch    22 | loss: 7.2441792Losses:  9.0218505859375 4.470522880554199 0.23713698983192444
CurrentTrain: epoch  4, batch    23 | loss: 9.0218506Losses:  7.891861915588379 2.781860589981079 0.24922561645507812
CurrentTrain: epoch  4, batch    24 | loss: 7.8918619Losses:  7.381568908691406 2.5313684940338135 0.23109059035778046
CurrentTrain: epoch  4, batch    25 | loss: 7.3815689Losses:  7.614989757537842 2.789759635925293 0.24681700766086578
CurrentTrain: epoch  4, batch    26 | loss: 7.6149898Losses:  8.559366226196289 3.447794198989868 0.2467145472764969
CurrentTrain: epoch  4, batch    27 | loss: 8.5593662Losses:  8.56104564666748 4.111353397369385 0.23735705018043518
CurrentTrain: epoch  4, batch    28 | loss: 8.5610456Losses:  8.928787231445312 3.8656177520751953 0.2534376382827759
CurrentTrain: epoch  4, batch    29 | loss: 8.9287872Losses:  9.676758766174316 4.716402053833008 0.2674783170223236
CurrentTrain: epoch  4, batch    30 | loss: 9.6767588Losses:  6.803154945373535 1.9418261051177979 0.26119884848594666
CurrentTrain: epoch  4, batch    31 | loss: 6.8031549Losses:  7.720736980438232 2.9310672283172607 0.2516236901283264
CurrentTrain: epoch  4, batch    32 | loss: 7.7207370Losses:  7.115146636962891 2.488326072692871 0.24676945805549622
CurrentTrain: epoch  4, batch    33 | loss: 7.1151466Losses:  11.468147277832031 6.901092529296875 0.25985586643218994
CurrentTrain: epoch  4, batch    34 | loss: 11.4681473Losses:  6.7934417724609375 2.253143548965454 0.24556536972522736
CurrentTrain: epoch  4, batch    35 | loss: 6.7934418Losses:  6.7332539558410645 2.1694679260253906 0.2645098865032196
CurrentTrain: epoch  4, batch    36 | loss: 6.7332540Losses:  9.250064849853516 4.640481472015381 0.25483134388923645
CurrentTrain: epoch  4, batch    37 | loss: 9.2500648Losses:  7.534286022186279 2.6737663745880127 0.23664253950119019
CurrentTrain: epoch  4, batch    38 | loss: 7.5342860Losses:  7.096128463745117 1.8061431646347046 0.2383311241865158
CurrentTrain: epoch  4, batch    39 | loss: 7.0961285Losses:  7.291902542114258 2.688316583633423 0.24298083782196045
CurrentTrain: epoch  4, batch    40 | loss: 7.2919025Losses:  8.140849113464355 3.2514452934265137 0.25861942768096924
CurrentTrain: epoch  4, batch    41 | loss: 8.1408491Losses:  8.355326652526855 3.7008583545684814 0.2525942325592041
CurrentTrain: epoch  4, batch    42 | loss: 8.3553267Losses:  8.637298583984375 4.062367916107178 0.2557294964790344
CurrentTrain: epoch  4, batch    43 | loss: 8.6372986Losses:  7.368965148925781 2.951362371444702 0.24631977081298828
CurrentTrain: epoch  4, batch    44 | loss: 7.3689651Losses:  7.808923721313477 2.697767972946167 0.2570963203907013
CurrentTrain: epoch  4, batch    45 | loss: 7.8089237Losses:  11.335488319396973 5.68808126449585 0.2585252523422241
CurrentTrain: epoch  4, batch    46 | loss: 11.3354883Losses:  7.52852201461792 2.519582748413086 0.25746119022369385
CurrentTrain: epoch  4, batch    47 | loss: 7.5285220Losses:  7.523340225219727 1.9676653146743774 0.2301669418811798
CurrentTrain: epoch  4, batch    48 | loss: 7.5233402Losses:  7.825275897979736 3.0575313568115234 0.24599570035934448
CurrentTrain: epoch  4, batch    49 | loss: 7.8252759Losses:  9.195647239685059 4.262804985046387 0.25569799542427063
CurrentTrain: epoch  4, batch    50 | loss: 9.1956472Losses:  9.406805992126465 3.675410270690918 0.26379644870758057
CurrentTrain: epoch  4, batch    51 | loss: 9.4068060Losses:  6.611652374267578 1.786219835281372 0.2207667976617813
CurrentTrain: epoch  4, batch    52 | loss: 6.6116524Losses:  7.764466285705566 3.338183879852295 0.2376941442489624
CurrentTrain: epoch  4, batch    53 | loss: 7.7644663Losses:  7.048669815063477 2.5097455978393555 0.24311959743499756
CurrentTrain: epoch  4, batch    54 | loss: 7.0486698Losses:  6.52715539932251 2.047297716140747 0.2365860939025879
CurrentTrain: epoch  4, batch    55 | loss: 6.5271554Losses:  8.20332145690918 3.2000088691711426 0.24282896518707275
CurrentTrain: epoch  4, batch    56 | loss: 8.2033215Losses:  7.607029438018799 3.1935012340545654 0.2420525848865509
CurrentTrain: epoch  4, batch    57 | loss: 7.6070294Losses:  7.658185958862305 3.000518798828125 0.23585212230682373
CurrentTrain: epoch  4, batch    58 | loss: 7.6581860Losses:  8.41305923461914 4.0513129234313965 0.15806594491004944
CurrentTrain: epoch  4, batch    59 | loss: 8.4130592Losses:  6.358689308166504 1.9396196603775024 0.23604455590248108
CurrentTrain: epoch  4, batch    60 | loss: 6.3586893Losses:  10.130241394042969 4.923273086547852 0.2592809200286865
CurrentTrain: epoch  4, batch    61 | loss: 10.1302414Losses:  7.561054229736328 1.1471420526504517 0.33148515224456787
CurrentTrain: epoch  4, batch    62 | loss: 7.5610542Losses:  8.757099151611328 4.170580863952637 0.2376696765422821
CurrentTrain: epoch  5, batch     0 | loss: 8.7570992Losses:  7.8055291175842285 2.618868827819824 0.24777822196483612
CurrentTrain: epoch  5, batch     1 | loss: 7.8055291Losses:  7.549439430236816 2.591160774230957 0.22939521074295044
CurrentTrain: epoch  5, batch     2 | loss: 7.5494394Losses:  10.617055892944336 6.1620073318481445 0.24868032336235046
CurrentTrain: epoch  5, batch     3 | loss: 10.6170559Losses:  7.6098737716674805 2.5487537384033203 0.24663609266281128
CurrentTrain: epoch  5, batch     4 | loss: 7.6098738Losses:  9.564979553222656 4.727419853210449 0.24985358119010925
CurrentTrain: epoch  5, batch     5 | loss: 9.5649796Losses:  9.190519332885742 4.489075183868408 0.25042927265167236
CurrentTrain: epoch  5, batch     6 | loss: 9.1905193Losses:  7.488056182861328 3.0200603008270264 0.254798948764801
CurrentTrain: epoch  5, batch     7 | loss: 7.4880562Losses:  8.588666915893555 4.184700012207031 0.25140583515167236
CurrentTrain: epoch  5, batch     8 | loss: 8.5886669Losses:  7.900028228759766 3.2902960777282715 0.2433502972126007
CurrentTrain: epoch  5, batch     9 | loss: 7.9000282Losses:  10.044204711914062 4.511556625366211 0.2407776117324829
CurrentTrain: epoch  5, batch    10 | loss: 10.0442047Losses:  6.425563812255859 1.8596068620681763 0.22821396589279175
CurrentTrain: epoch  5, batch    11 | loss: 6.4255638Losses:  7.435885429382324 2.2185988426208496 0.2288820743560791
CurrentTrain: epoch  5, batch    12 | loss: 7.4358854Losses:  7.879302501678467 3.382685899734497 0.2556679844856262
CurrentTrain: epoch  5, batch    13 | loss: 7.8793025Losses:  8.782336235046387 4.451901435852051 0.15683522820472717
CurrentTrain: epoch  5, batch    14 | loss: 8.7823362Losses:  7.618381023406982 3.2510993480682373 0.23139438033103943
CurrentTrain: epoch  5, batch    15 | loss: 7.6183810Losses:  7.605688571929932 3.195517063140869 0.24384158849716187
CurrentTrain: epoch  5, batch    16 | loss: 7.6056886Losses:  9.274728775024414 4.472044467926025 0.157194584608078
CurrentTrain: epoch  5, batch    17 | loss: 9.2747288Losses:  7.858609676361084 3.184530735015869 0.2329830378293991
CurrentTrain: epoch  5, batch    18 | loss: 7.8586097Losses:  6.86236572265625 2.3033933639526367 0.2546231746673584
CurrentTrain: epoch  5, batch    19 | loss: 6.8623657Losses:  6.801046371459961 2.507284641265869 0.22828620672225952
CurrentTrain: epoch  5, batch    20 | loss: 6.8010464Losses:  7.617913722991943 2.8645124435424805 0.23547551035881042
CurrentTrain: epoch  5, batch    21 | loss: 7.6179137Losses:  7.845550537109375 3.2363924980163574 0.2450263500213623
CurrentTrain: epoch  5, batch    22 | loss: 7.8455505Losses:  7.749884605407715 3.279719829559326 0.23873083293437958
CurrentTrain: epoch  5, batch    23 | loss: 7.7498846Losses:  7.84609317779541 3.2623629570007324 0.2294798195362091
CurrentTrain: epoch  5, batch    24 | loss: 7.8460932Losses:  8.125629425048828 3.26557993888855 0.2380731701850891
CurrentTrain: epoch  5, batch    25 | loss: 8.1256294Losses:  7.497983455657959 3.0515666007995605 0.23459726572036743
CurrentTrain: epoch  5, batch    26 | loss: 7.4979835Losses:  8.226990699768066 3.799379348754883 0.23205414414405823
CurrentTrain: epoch  5, batch    27 | loss: 8.2269907Losses:  7.528765678405762 3.0692148208618164 0.23981192708015442
CurrentTrain: epoch  5, batch    28 | loss: 7.5287657Losses:  7.405559539794922 3.006746292114258 0.23985357582569122
CurrentTrain: epoch  5, batch    29 | loss: 7.4055595Losses:  9.781128883361816 4.266950607299805 0.2569113075733185
CurrentTrain: epoch  5, batch    30 | loss: 9.7811289Losses:  7.3927459716796875 3.026871681213379 0.2352524846792221
CurrentTrain: epoch  5, batch    31 | loss: 7.3927460Losses:  9.013626098632812 4.698211669921875 0.23030757904052734
CurrentTrain: epoch  5, batch    32 | loss: 9.0136261Losses:  8.154699325561523 3.0236830711364746 0.2396065592765808
CurrentTrain: epoch  5, batch    33 | loss: 8.1546993Losses:  7.922605037689209 3.143460988998413 0.24977634847164154
CurrentTrain: epoch  5, batch    34 | loss: 7.9226050Losses:  11.812455177307129 7.385232448577881 0.3091861605644226
CurrentTrain: epoch  5, batch    35 | loss: 11.8124552Losses:  9.958637237548828 5.286437034606934 0.2647700905799866
CurrentTrain: epoch  5, batch    36 | loss: 9.9586372Losses:  6.970142364501953 2.538844585418701 0.2565418481826782
CurrentTrain: epoch  5, batch    37 | loss: 6.9701424Losses:  7.415842056274414 2.9808545112609863 0.2350183129310608
CurrentTrain: epoch  5, batch    38 | loss: 7.4158421Losses:  7.253471374511719 2.5914969444274902 0.24517948925495148
CurrentTrain: epoch  5, batch    39 | loss: 7.2534714Losses:  8.969683647155762 4.241837978363037 0.254628449678421
CurrentTrain: epoch  5, batch    40 | loss: 8.9696836Losses:  8.477849960327148 4.148097991943359 0.24839872121810913
CurrentTrain: epoch  5, batch    41 | loss: 8.4778500Losses:  7.483532428741455 2.988999366760254 0.23548272252082825
CurrentTrain: epoch  5, batch    42 | loss: 7.4835324Losses:  7.135570526123047 2.764453887939453 0.22924527525901794
CurrentTrain: epoch  5, batch    43 | loss: 7.1355705Losses:  7.814091205596924 3.2314300537109375 0.22940510511398315
CurrentTrain: epoch  5, batch    44 | loss: 7.8140912Losses:  7.294830322265625 2.9620707035064697 0.22737397253513336
CurrentTrain: epoch  5, batch    45 | loss: 7.2948303Losses:  6.9882893562316895 2.61728572845459 0.2264360785484314
CurrentTrain: epoch  5, batch    46 | loss: 6.9882894Losses:  7.385627746582031 3.0320749282836914 0.24728792905807495
CurrentTrain: epoch  5, batch    47 | loss: 7.3856277Losses:  6.8731369972229 2.473176956176758 0.23951968550682068
CurrentTrain: epoch  5, batch    48 | loss: 6.8731370Losses:  6.796268939971924 2.238466262817383 0.22916126251220703
CurrentTrain: epoch  5, batch    49 | loss: 6.7962689Losses:  6.529719352722168 1.921609878540039 0.21907265484333038
CurrentTrain: epoch  5, batch    50 | loss: 6.5297194Losses:  6.790359973907471 2.2639384269714355 0.2269626259803772
CurrentTrain: epoch  5, batch    51 | loss: 6.7903600Losses:  8.251529693603516 3.7067131996154785 0.26713240146636963
CurrentTrain: epoch  5, batch    52 | loss: 8.2515297Losses:  7.007503032684326 2.3843894004821777 0.2304747998714447
CurrentTrain: epoch  5, batch    53 | loss: 7.0075030Losses:  7.232641696929932 2.6964588165283203 0.2289767563343048
CurrentTrain: epoch  5, batch    54 | loss: 7.2326417Losses:  8.168754577636719 3.7984437942504883 0.16919919848442078
CurrentTrain: epoch  5, batch    55 | loss: 8.1687546Losses:  6.56887674331665 2.2138164043426514 0.23472338914871216
CurrentTrain: epoch  5, batch    56 | loss: 6.5688767Losses:  7.294931888580322 2.7522122859954834 0.1456093043088913
CurrentTrain: epoch  5, batch    57 | loss: 7.2949319Losses:  6.7430949211120605 2.313091993331909 0.2327139675617218
CurrentTrain: epoch  5, batch    58 | loss: 6.7430949Losses:  8.274784088134766 3.8574507236480713 0.25028330087661743
CurrentTrain: epoch  5, batch    59 | loss: 8.2747841Losses:  6.462747097015381 2.2281436920166016 0.23056963086128235
CurrentTrain: epoch  5, batch    60 | loss: 6.4627471Losses:  7.539377689361572 2.9910221099853516 0.23926109075546265
CurrentTrain: epoch  5, batch    61 | loss: 7.5393777Losses:  6.027279376983643 1.577906847000122 0.09784084558486938
CurrentTrain: epoch  5, batch    62 | loss: 6.0272794Losses:  7.964030742645264 3.585268020629883 0.26567402482032776
CurrentTrain: epoch  6, batch     0 | loss: 7.9640307Losses:  7.673051357269287 3.298602819442749 0.2288125604391098
CurrentTrain: epoch  6, batch     1 | loss: 7.6730514Losses:  8.561247825622559 4.100924491882324 0.23813468217849731
CurrentTrain: epoch  6, batch     2 | loss: 8.5612478Losses:  10.277628898620605 5.9211530685424805 0.23441985249519348
CurrentTrain: epoch  6, batch     3 | loss: 10.2776289Losses:  9.077494621276855 4.708159446716309 0.25255218148231506
CurrentTrain: epoch  6, batch     4 | loss: 9.0774946Losses:  6.298518657684326 1.9316270351409912 0.23471111059188843
CurrentTrain: epoch  6, batch     5 | loss: 6.2985187Losses:  6.763396739959717 2.3931422233581543 0.2348238080739975
CurrentTrain: epoch  6, batch     6 | loss: 6.7633967Losses:  7.160970211029053 2.684866428375244 0.24922242760658264
CurrentTrain: epoch  6, batch     7 | loss: 7.1609702Losses:  9.6328763961792 5.2191596031188965 0.25002938508987427
CurrentTrain: epoch  6, batch     8 | loss: 9.6328764Losses:  8.959030151367188 4.588118076324463 0.26312798261642456
CurrentTrain: epoch  6, batch     9 | loss: 8.9590302Losses:  6.610270023345947 2.267869234085083 0.22492548823356628
CurrentTrain: epoch  6, batch    10 | loss: 6.6102700Losses:  6.990853786468506 2.510528802871704 0.2368590086698532
CurrentTrain: epoch  6, batch    11 | loss: 6.9908538Losses:  9.018299102783203 4.537779808044434 0.25774824619293213
CurrentTrain: epoch  6, batch    12 | loss: 9.0182991Losses:  8.689614295959473 4.157317161560059 0.2518126666545868
CurrentTrain: epoch  6, batch    13 | loss: 8.6896143Losses:  9.24410343170166 4.845709800720215 0.25319385528564453
CurrentTrain: epoch  6, batch    14 | loss: 9.2441034Losses:  8.97182559967041 4.690829753875732 0.23721076548099518
CurrentTrain: epoch  6, batch    15 | loss: 8.9718256Losses:  6.652698516845703 2.360502004623413 0.22925394773483276
CurrentTrain: epoch  6, batch    16 | loss: 6.6526985Losses:  7.967013835906982 3.5347485542297363 0.2639363408088684
CurrentTrain: epoch  6, batch    17 | loss: 7.9670138Losses:  7.2897562980651855 2.930172920227051 0.23979000747203827
CurrentTrain: epoch  6, batch    18 | loss: 7.2897563Losses:  6.123785018920898 1.8217798471450806 0.21998989582061768
CurrentTrain: epoch  6, batch    19 | loss: 6.1237850Losses:  8.964414596557617 4.633182525634766 0.24140846729278564
CurrentTrain: epoch  6, batch    20 | loss: 8.9644146Losses:  6.372072219848633 1.9501862525939941 0.229360431432724
CurrentTrain: epoch  6, batch    21 | loss: 6.3720722Losses:  6.484663963317871 2.1186060905456543 0.23146778345108032
CurrentTrain: epoch  6, batch    22 | loss: 6.4846640Losses:  7.078278541564941 2.7101786136627197 0.239019513130188
CurrentTrain: epoch  6, batch    23 | loss: 7.0782785Losses:  7.206605911254883 2.8143882751464844 0.244283989071846
CurrentTrain: epoch  6, batch    24 | loss: 7.2066059Losses:  8.603418350219727 4.2923736572265625 0.2559646964073181
CurrentTrain: epoch  6, batch    25 | loss: 8.6034184Losses:  6.23037576675415 1.9400033950805664 0.23328928649425507
CurrentTrain: epoch  6, batch    26 | loss: 6.2303758Losses:  7.183220386505127 2.742668390274048 0.2345244288444519
CurrentTrain: epoch  6, batch    27 | loss: 7.1832204Losses:  6.6931233406066895 2.4084153175354004 0.22765257954597473
CurrentTrain: epoch  6, batch    28 | loss: 6.6931233Losses:  6.905924320220947 2.4971280097961426 0.23476094007492065
CurrentTrain: epoch  6, batch    29 | loss: 6.9059243Losses:  8.595283508300781 4.248722553253174 0.23155055940151215
CurrentTrain: epoch  6, batch    30 | loss: 8.5952835Losses:  6.260064125061035 1.8459093570709229 0.22023475170135498
CurrentTrain: epoch  6, batch    31 | loss: 6.2600641Losses:  7.56740665435791 3.1557421684265137 0.24602854251861572
CurrentTrain: epoch  6, batch    32 | loss: 7.5674067Losses:  7.543890476226807 3.211501359939575 0.2432999312877655
CurrentTrain: epoch  6, batch    33 | loss: 7.5438905Losses:  8.471792221069336 3.1174235343933105 0.22882625460624695
CurrentTrain: epoch  6, batch    34 | loss: 8.4717922Losses:  6.788705825805664 2.0826401710510254 0.22266525030136108
CurrentTrain: epoch  6, batch    35 | loss: 6.7887058Losses:  7.279448509216309 2.477111339569092 0.23493146896362305
CurrentTrain: epoch  6, batch    36 | loss: 7.2794485Losses:  7.26820182800293 2.944709300994873 0.22944365441799164
CurrentTrain: epoch  6, batch    37 | loss: 7.2682018Losses:  10.798806190490723 6.290226459503174 0.17310774326324463
CurrentTrain: epoch  6, batch    38 | loss: 10.7988062Losses:  7.545308589935303 3.128880739212036 0.23672647774219513
CurrentTrain: epoch  6, batch    39 | loss: 7.5453086Losses:  8.407636642456055 4.117571830749512 0.23500801622867584
CurrentTrain: epoch  6, batch    40 | loss: 8.4076366Losses:  7.069559097290039 2.553435802459717 0.23080500960350037
CurrentTrain: epoch  6, batch    41 | loss: 7.0695591Losses:  6.167443752288818 1.8628281354904175 0.21919631958007812
CurrentTrain: epoch  6, batch    42 | loss: 6.1674438Losses:  7.9022216796875 3.001218318939209 0.22684697806835175
CurrentTrain: epoch  6, batch    43 | loss: 7.9022217Losses:  7.453169822692871 2.8027522563934326 0.22714433073997498
CurrentTrain: epoch  6, batch    44 | loss: 7.4531698Losses:  8.034134864807129 3.185253143310547 0.24147942662239075
CurrentTrain: epoch  6, batch    45 | loss: 8.0341349Losses:  9.68892765045166 5.389429092407227 0.25406062602996826
CurrentTrain: epoch  6, batch    46 | loss: 9.6889277Losses:  7.853330135345459 3.3175225257873535 0.24658839404582977
CurrentTrain: epoch  6, batch    47 | loss: 7.8533301Losses:  7.592660903930664 2.958653450012207 0.2504284977912903
CurrentTrain: epoch  6, batch    48 | loss: 7.5926609Losses:  8.997671127319336 4.767940521240234 0.16822449862957
CurrentTrain: epoch  6, batch    49 | loss: 8.9976711Losses:  6.620594501495361 2.367309093475342 0.2302943468093872
CurrentTrain: epoch  6, batch    50 | loss: 6.6205945Losses:  7.890684604644775 3.0307464599609375 0.24060580134391785
CurrentTrain: epoch  6, batch    51 | loss: 7.8906846Losses:  7.613167762756348 3.268570899963379 0.23237141966819763
CurrentTrain: epoch  6, batch    52 | loss: 7.6131678Losses:  6.601169586181641 2.1480655670166016 0.2306138277053833
CurrentTrain: epoch  6, batch    53 | loss: 6.6011696Losses:  6.948150634765625 2.570236921310425 0.23219963908195496
CurrentTrain: epoch  6, batch    54 | loss: 6.9481506Losses:  6.424805164337158 2.138761520385742 0.22892242670059204
CurrentTrain: epoch  6, batch    55 | loss: 6.4248052Losses:  8.778000831604004 4.156311988830566 0.23433822393417358
CurrentTrain: epoch  6, batch    56 | loss: 8.7780008Losses:  6.617796421051025 2.172515392303467 0.22192007303237915
CurrentTrain: epoch  6, batch    57 | loss: 6.6177964Losses:  7.580934047698975 2.995781421661377 0.22898194193840027
CurrentTrain: epoch  6, batch    58 | loss: 7.5809340Losses:  6.624512672424316 2.361785411834717 0.23160195350646973
CurrentTrain: epoch  6, batch    59 | loss: 6.6245127Losses:  7.491875648498535 3.2419605255126953 0.23278996348381042
CurrentTrain: epoch  6, batch    60 | loss: 7.4918756Losses:  6.452447891235352 2.200151205062866 0.2283189594745636
CurrentTrain: epoch  6, batch    61 | loss: 6.4524479Losses:  5.574842929840088 1.3430371284484863 0.16661453247070312
CurrentTrain: epoch  6, batch    62 | loss: 5.5748429Losses:  7.47620964050293 3.1136207580566406 0.24014584720134735
CurrentTrain: epoch  7, batch     0 | loss: 7.4762096Losses:  7.4103217124938965 3.1401801109313965 0.23033930361270905
CurrentTrain: epoch  7, batch     1 | loss: 7.4103217Losses:  9.286422729492188 3.7958717346191406 0.22987331449985504
CurrentTrain: epoch  7, batch     2 | loss: 9.2864227Losses:  7.537001132965088 3.151998519897461 0.1434539258480072
CurrentTrain: epoch  7, batch     3 | loss: 7.5370011Losses:  7.297003269195557 2.690019369125366 0.2316240668296814
CurrentTrain: epoch  7, batch     4 | loss: 7.2970033Losses:  6.382678985595703 2.142606735229492 0.22566798329353333
CurrentTrain: epoch  7, batch     5 | loss: 6.3826790Losses:  6.827240467071533 2.5667707920074463 0.22284084558486938
CurrentTrain: epoch  7, batch     6 | loss: 6.8272405Losses:  7.268120288848877 2.9335379600524902 0.2261420041322708
CurrentTrain: epoch  7, batch     7 | loss: 7.2681203Losses:  8.058496475219727 3.7356138229370117 0.15978002548217773
CurrentTrain: epoch  7, batch     8 | loss: 8.0584965Losses:  7.783834934234619 2.9811413288116455 0.23271247744560242
CurrentTrain: epoch  7, batch     9 | loss: 7.7838349Losses:  5.90217399597168 1.621688961982727 0.21323618292808533
CurrentTrain: epoch  7, batch    10 | loss: 5.9021740Losses:  5.912545204162598 1.588708758354187 0.21617341041564941
CurrentTrain: epoch  7, batch    11 | loss: 5.9125452Losses:  7.997994422912598 3.4536330699920654 0.23431585729122162
CurrentTrain: epoch  7, batch    12 | loss: 7.9979944Losses:  7.509199142456055 3.2116124629974365 0.23737068474292755
CurrentTrain: epoch  7, batch    13 | loss: 7.5091991Losses:  9.681556701660156 4.924218654632568 0.15801827609539032
CurrentTrain: epoch  7, batch    14 | loss: 9.6815567Losses:  9.111298561096191 4.495236396789551 0.24667857587337494
CurrentTrain: epoch  7, batch    15 | loss: 9.1112986Losses:  7.862998008728027 3.3978705406188965 0.1473650336265564
CurrentTrain: epoch  7, batch    16 | loss: 7.8629980Losses:  11.765602111816406 7.258694648742676 0.2571478486061096
CurrentTrain: epoch  7, batch    17 | loss: 11.7656021Losses:  6.687020301818848 2.3373210430145264 0.23462983965873718
CurrentTrain: epoch  7, batch    18 | loss: 6.6870203Losses:  6.986286163330078 2.563969135284424 0.23663592338562012
CurrentTrain: epoch  7, batch    19 | loss: 6.9862862Losses:  6.649857044219971 2.4007253646850586 0.23326817154884338
CurrentTrain: epoch  7, batch    20 | loss: 6.6498570Losses:  8.960336685180664 4.505926132202148 0.25557661056518555
CurrentTrain: epoch  7, batch    21 | loss: 8.9603367Losses:  8.857170104980469 4.557612419128418 0.23787403106689453
CurrentTrain: epoch  7, batch    22 | loss: 8.8571701Losses:  6.932193279266357 2.7161452770233154 0.22476893663406372
CurrentTrain: epoch  7, batch    23 | loss: 6.9321933Losses:  6.406656265258789 2.1258292198181152 0.22090551257133484
CurrentTrain: epoch  7, batch    24 | loss: 6.4066563Losses:  7.7361321449279785 3.320364475250244 0.2443467676639557
CurrentTrain: epoch  7, batch    25 | loss: 7.7361321Losses:  6.669210910797119 2.387906789779663 0.22564177215099335
CurrentTrain: epoch  7, batch    26 | loss: 6.6692109Losses:  7.515836715698242 3.236943244934082 0.23691797256469727
CurrentTrain: epoch  7, batch    27 | loss: 7.5158367Losses:  6.725773334503174 2.448188543319702 0.2432495355606079
CurrentTrain: epoch  7, batch    28 | loss: 6.7257733Losses:  7.962610721588135 3.6966683864593506 0.22958630323410034
CurrentTrain: epoch  7, batch    29 | loss: 7.9626107Losses:  7.0406036376953125 2.628931999206543 0.23540520668029785
CurrentTrain: epoch  7, batch    30 | loss: 7.0406036Losses:  6.690998554229736 2.354557752609253 0.23374328017234802
CurrentTrain: epoch  7, batch    31 | loss: 6.6909986Losses:  7.200506210327148 2.9336752891540527 0.25192248821258545
CurrentTrain: epoch  7, batch    32 | loss: 7.2005062Losses:  9.412545204162598 4.9449567794799805 0.16650685667991638
CurrentTrain: epoch  7, batch    33 | loss: 9.4125452Losses:  6.790102005004883 2.3941361904144287 0.2450389266014099
CurrentTrain: epoch  7, batch    34 | loss: 6.7901020Losses:  9.082132339477539 4.738377571105957 0.25299084186553955
CurrentTrain: epoch  7, batch    35 | loss: 9.0821323Losses:  6.036154747009277 1.793026089668274 0.22028791904449463
CurrentTrain: epoch  7, batch    36 | loss: 6.0361547Losses:  6.204341411590576 1.936207890510559 0.22081990540027618
CurrentTrain: epoch  7, batch    37 | loss: 6.2043414Losses:  8.549413681030273 4.027040004730225 0.2565751075744629
CurrentTrain: epoch  7, batch    38 | loss: 8.5494137Losses:  8.93358325958252 4.549225330352783 0.24743898212909698
CurrentTrain: epoch  7, batch    39 | loss: 8.9335833Losses:  7.001746654510498 2.704115390777588 0.22202405333518982
CurrentTrain: epoch  7, batch    40 | loss: 7.0017467Losses:  8.385464668273926 4.0919270515441895 0.1426343023777008
CurrentTrain: epoch  7, batch    41 | loss: 8.3854647Losses:  7.452861309051514 3.1765918731689453 0.24134422838687897
CurrentTrain: epoch  7, batch    42 | loss: 7.4528613Losses:  11.377724647521973 7.138970375061035 0.2561856210231781
CurrentTrain: epoch  7, batch    43 | loss: 11.3777246Losses:  7.06170129776001 2.7065939903259277 0.23774823546409607
CurrentTrain: epoch  7, batch    44 | loss: 7.0617013Losses:  6.870498180389404 2.5866518020629883 0.21897916495800018
CurrentTrain: epoch  7, batch    45 | loss: 6.8704982Losses:  5.714473724365234 1.4483368396759033 0.2161426842212677
CurrentTrain: epoch  7, batch    46 | loss: 5.7144737Losses:  6.388378143310547 2.1511621475219727 0.2236005663871765
CurrentTrain: epoch  7, batch    47 | loss: 6.3883781Losses:  6.429770469665527 2.1594676971435547 0.22364354133605957
CurrentTrain: epoch  7, batch    48 | loss: 6.4297705Losses:  7.227556228637695 2.9247288703918457 0.24079161882400513
CurrentTrain: epoch  7, batch    49 | loss: 7.2275562Losses:  7.996087551116943 3.855299949645996 0.15999841690063477
CurrentTrain: epoch  7, batch    50 | loss: 7.9960876Losses:  6.124970436096191 1.9140028953552246 0.22141411900520325
CurrentTrain: epoch  7, batch    51 | loss: 6.1249704Losses:  7.4645490646362305 3.11664080619812 0.25414538383483887
CurrentTrain: epoch  7, batch    52 | loss: 7.4645491Losses:  6.457136154174805 2.191704034805298 0.2161484807729721
CurrentTrain: epoch  7, batch    53 | loss: 6.4571362Losses:  6.739365577697754 2.57328462600708 0.14471575617790222
CurrentTrain: epoch  7, batch    54 | loss: 6.7393656Losses:  7.161643028259277 2.933015823364258 0.15293288230895996
CurrentTrain: epoch  7, batch    55 | loss: 7.1616430Losses:  6.9172210693359375 2.6653900146484375 0.23781003057956696
CurrentTrain: epoch  7, batch    56 | loss: 6.9172211Losses:  11.804405212402344 7.481754302978516 0.15974698960781097
CurrentTrain: epoch  7, batch    57 | loss: 11.8044052Losses:  5.651313304901123 1.4469376802444458 0.21335262060165405
CurrentTrain: epoch  7, batch    58 | loss: 5.6513133Losses:  8.706439018249512 4.41169548034668 0.24401551485061646
CurrentTrain: epoch  7, batch    59 | loss: 8.7064390Losses:  8.798330307006836 4.276661396026611 0.24127614498138428
CurrentTrain: epoch  7, batch    60 | loss: 8.7983303Losses:  6.4994988441467285 2.21091365814209 0.2256937175989151
CurrentTrain: epoch  7, batch    61 | loss: 6.4994988Losses:  4.9006757736206055 0.5957857966423035 0.21584008634090424
CurrentTrain: epoch  7, batch    62 | loss: 4.9006758Losses:  6.6372528076171875 2.384713888168335 0.2318250685930252
CurrentTrain: epoch  8, batch     0 | loss: 6.6372528Losses:  6.816575050354004 2.6253767013549805 0.22932672500610352
CurrentTrain: epoch  8, batch     1 | loss: 6.8165751Losses:  7.317346572875977 3.0865628719329834 0.23736463487148285
CurrentTrain: epoch  8, batch     2 | loss: 7.3173466Losses:  7.411912441253662 3.12119722366333 0.23799282312393188
CurrentTrain: epoch  8, batch     3 | loss: 7.4119124Losses:  7.031886100769043 2.767660617828369 0.23547738790512085
CurrentTrain: epoch  8, batch     4 | loss: 7.0318861Losses:  6.110659122467041 1.8213655948638916 0.22605293989181519
CurrentTrain: epoch  8, batch     5 | loss: 6.1106591Losses:  7.5230841636657715 3.2923152446746826 0.23258620500564575
CurrentTrain: epoch  8, batch     6 | loss: 7.5230842Losses:  7.111313343048096 2.6916821002960205 0.22464698553085327
CurrentTrain: epoch  8, batch     7 | loss: 7.1113133Losses:  6.547472953796387 2.3358750343322754 0.22464674711227417
CurrentTrain: epoch  8, batch     8 | loss: 6.5474730Losses:  7.183956623077393 2.9735565185546875 0.229378342628479
CurrentTrain: epoch  8, batch     9 | loss: 7.1839566Losses:  7.147510528564453 2.9345195293426514 0.2225288450717926
CurrentTrain: epoch  8, batch    10 | loss: 7.1475105Losses:  6.932018280029297 2.701794385910034 0.2461176961660385
CurrentTrain: epoch  8, batch    11 | loss: 6.9320183Losses:  9.033839225769043 4.742297172546387 0.24743475019931793
CurrentTrain: epoch  8, batch    12 | loss: 9.0338392Losses:  7.8848676681518555 3.5788519382476807 0.23761631548404694
CurrentTrain: epoch  8, batch    13 | loss: 7.8848677Losses:  6.0779643058776855 1.8236570358276367 0.22063587605953217
CurrentTrain: epoch  8, batch    14 | loss: 6.0779643Losses:  6.67304801940918 2.431400775909424 0.2221357226371765
CurrentTrain: epoch  8, batch    15 | loss: 6.6730480Losses:  7.212423801422119 2.9684910774230957 0.22041872143745422
CurrentTrain: epoch  8, batch    16 | loss: 7.2124238Losses:  7.158926963806152 2.932199001312256 0.2182934433221817
CurrentTrain: epoch  8, batch    17 | loss: 7.1589270Losses:  6.586924076080322 2.341336488723755 0.22893020510673523
CurrentTrain: epoch  8, batch    18 | loss: 6.5869241Losses:  6.964798927307129 2.6952788829803467 0.21155650913715363
CurrentTrain: epoch  8, batch    19 | loss: 6.9647989Losses:  9.430414199829102 5.194052696228027 0.14390355348587036
CurrentTrain: epoch  8, batch    20 | loss: 9.4304142Losses:  7.510578155517578 3.2441697120666504 0.24036964774131775
CurrentTrain: epoch  8, batch    21 | loss: 7.5105782Losses:  7.248724460601807 3.0336616039276123 0.23014643788337708
CurrentTrain: epoch  8, batch    22 | loss: 7.2487245Losses:  8.254745483398438 2.865452527999878 0.2253561019897461
CurrentTrain: epoch  8, batch    23 | loss: 8.2547455Losses:  7.7985005378723145 2.759725570678711 0.23485799133777618
CurrentTrain: epoch  8, batch    24 | loss: 7.7985005Losses:  8.24022102355957 3.3493199348449707 0.2365078330039978
CurrentTrain: epoch  8, batch    25 | loss: 8.2402210Losses:  8.227309226989746 3.580606460571289 0.24998320639133453
CurrentTrain: epoch  8, batch    26 | loss: 8.2273092Losses:  6.546606540679932 2.1952264308929443 0.23066076636314392
CurrentTrain: epoch  8, batch    27 | loss: 6.5466065Losses:  7.401109218597412 3.119856834411621 0.2306058704853058
CurrentTrain: epoch  8, batch    28 | loss: 7.4011092Losses:  7.640895843505859 2.696045160293579 0.23921318352222443
CurrentTrain: epoch  8, batch    29 | loss: 7.6408958Losses:  8.437870025634766 4.240653038024902 0.15918320417404175
CurrentTrain: epoch  8, batch    30 | loss: 8.4378700Losses:  6.941197872161865 2.681838035583496 0.21278829872608185
CurrentTrain: epoch  8, batch    31 | loss: 6.9411979Losses:  5.782135009765625 1.229292392730713 0.20914912223815918
CurrentTrain: epoch  8, batch    32 | loss: 5.7821350Losses:  7.092466354370117 2.915034294128418 0.22113622725009918
CurrentTrain: epoch  8, batch    33 | loss: 7.0924664Losses:  5.962771892547607 1.5383570194244385 0.21294943988323212
CurrentTrain: epoch  8, batch    34 | loss: 5.9627719Losses:  6.680172920227051 2.396832227706909 0.24161624908447266
CurrentTrain: epoch  8, batch    35 | loss: 6.6801729Losses:  7.405571460723877 2.9090116024017334 0.24194112420082092
CurrentTrain: epoch  8, batch    36 | loss: 7.4055715Losses:  6.161077499389648 1.9712247848510742 0.2204911708831787
CurrentTrain: epoch  8, batch    37 | loss: 6.1610775Losses:  6.8635406494140625 2.6640734672546387 0.22494012117385864
CurrentTrain: epoch  8, batch    38 | loss: 6.8635406Losses:  6.7413835525512695 2.35722279548645 0.2219332456588745
CurrentTrain: epoch  8, batch    39 | loss: 6.7413836Losses:  7.8679680824279785 3.5029401779174805 0.150849387049675
CurrentTrain: epoch  8, batch    40 | loss: 7.8679681Losses:  6.753365993499756 2.559619426727295 0.22266033291816711
CurrentTrain: epoch  8, batch    41 | loss: 6.7533660Losses:  10.363287925720215 5.537484169006348 0.2655988335609436
CurrentTrain: epoch  8, batch    42 | loss: 10.3632879Losses:  10.653100967407227 6.423665523529053 0.2532179355621338
CurrentTrain: epoch  8, batch    43 | loss: 10.6531010Losses:  7.320300102233887 2.858302593231201 0.23072095215320587
CurrentTrain: epoch  8, batch    44 | loss: 7.3203001Losses:  6.43213415145874 2.153748035430908 0.2306174635887146
CurrentTrain: epoch  8, batch    45 | loss: 6.4321342Losses:  6.455867767333984 2.0845999717712402 0.22527630627155304
CurrentTrain: epoch  8, batch    46 | loss: 6.4558678Losses:  6.810725688934326 2.1331472396850586 0.2240215539932251
CurrentTrain: epoch  8, batch    47 | loss: 6.8107257Losses:  6.652052402496338 2.391134262084961 0.24000045657157898
CurrentTrain: epoch  8, batch    48 | loss: 6.6520524Losses:  7.6431779861450195 3.4103989601135254 0.2241569459438324
CurrentTrain: epoch  8, batch    49 | loss: 7.6431780Losses:  7.428203582763672 2.7154197692871094 0.23856648802757263
CurrentTrain: epoch  8, batch    50 | loss: 7.4282036Losses:  6.92427921295166 2.684882402420044 0.23769959807395935
CurrentTrain: epoch  8, batch    51 | loss: 6.9242792Losses:  7.161021709442139 2.6182684898376465 0.23846572637557983
CurrentTrain: epoch  8, batch    52 | loss: 7.1610217Losses:  8.061179161071777 3.8099770545959473 0.25696808099746704
CurrentTrain: epoch  8, batch    53 | loss: 8.0611792Losses:  7.345973968505859 2.958479642868042 0.22652031481266022
CurrentTrain: epoch  8, batch    54 | loss: 7.3459740Losses:  6.965875148773193 2.724412441253662 0.21622756123542786
CurrentTrain: epoch  8, batch    55 | loss: 6.9658751Losses:  10.224689483642578 5.963138103485107 0.24144935607910156
CurrentTrain: epoch  8, batch    56 | loss: 10.2246895Losses:  7.299472808837891 2.999951124191284 0.2332211136817932
CurrentTrain: epoch  8, batch    57 | loss: 7.2994728Losses:  6.155267238616943 1.8097164630889893 0.2221732884645462
CurrentTrain: epoch  8, batch    58 | loss: 6.1552672Losses:  12.670970916748047 8.443075180053711 0.24334563314914703
CurrentTrain: epoch  8, batch    59 | loss: 12.6709709Losses:  7.870550155639648 3.694340705871582 0.1514369696378708
CurrentTrain: epoch  8, batch    60 | loss: 7.8705502Losses:  9.054240226745605 4.701958179473877 0.260163277387619
CurrentTrain: epoch  8, batch    61 | loss: 9.0542402Losses:  5.9470953941345215 1.3923771381378174 0.10022421181201935
CurrentTrain: epoch  8, batch    62 | loss: 5.9470954Losses:  6.460566520690918 2.1185030937194824 0.22761473059654236
CurrentTrain: epoch  9, batch     0 | loss: 6.4605665Losses:  7.559754848480225 2.9184730052948 0.22483403980731964
CurrentTrain: epoch  9, batch     1 | loss: 7.5597548Losses:  7.209558010101318 2.9865822792053223 0.21264797449111938
CurrentTrain: epoch  9, batch     2 | loss: 7.2095580Losses:  6.679744720458984 2.3717634677886963 0.22907361388206482
CurrentTrain: epoch  9, batch     3 | loss: 6.6797447Losses:  6.529205322265625 2.3403964042663574 0.22373321652412415
CurrentTrain: epoch  9, batch     4 | loss: 6.5292053Losses:  7.4833664894104 3.1487278938293457 0.23278149962425232
CurrentTrain: epoch  9, batch     5 | loss: 7.4833665Losses:  6.650421142578125 2.361409902572632 0.13839447498321533
CurrentTrain: epoch  9, batch     6 | loss: 6.6504211Losses:  6.426924705505371 2.1878552436828613 0.217694491147995
CurrentTrain: epoch  9, batch     7 | loss: 6.4269247Losses:  7.521414279937744 3.1732969284057617 0.24664930999279022
CurrentTrain: epoch  9, batch     8 | loss: 7.5214143Losses:  9.083990097045898 4.871076583862305 0.24106596410274506
CurrentTrain: epoch  9, batch     9 | loss: 9.0839901Losses:  7.698389053344727 3.4702255725860596 0.23346710205078125
CurrentTrain: epoch  9, batch    10 | loss: 7.6983891Losses:  7.073012351989746 2.870790481567383 0.2234601080417633
CurrentTrain: epoch  9, batch    11 | loss: 7.0730124Losses:  6.375690937042236 2.1418771743774414 0.21758614480495453
CurrentTrain: epoch  9, batch    12 | loss: 6.3756909Losses:  8.393476486206055 4.1916046142578125 0.2230667620897293
CurrentTrain: epoch  9, batch    13 | loss: 8.3934765Losses:  7.562438488006592 3.1928844451904297 0.24810001254081726
CurrentTrain: epoch  9, batch    14 | loss: 7.5624385Losses:  7.212924480438232 2.9134466648101807 0.23429712653160095
CurrentTrain: epoch  9, batch    15 | loss: 7.2129245Losses:  7.3128156661987305 3.071892738342285 0.23280052840709686
CurrentTrain: epoch  9, batch    16 | loss: 7.3128157Losses:  6.376304626464844 2.0895566940307617 0.22655263543128967
CurrentTrain: epoch  9, batch    17 | loss: 6.3763046Losses:  5.938162803649902 1.5806574821472168 0.21104587614536285
CurrentTrain: epoch  9, batch    18 | loss: 5.9381628Losses:  5.390745639801025 1.203073501586914 0.20598076283931732
CurrentTrain: epoch  9, batch    19 | loss: 5.3907456Losses:  6.783166408538818 2.503788709640503 0.22479307651519775
CurrentTrain: epoch  9, batch    20 | loss: 6.7831664Losses:  6.313945293426514 2.0966691970825195 0.22278240323066711
CurrentTrain: epoch  9, batch    21 | loss: 6.3139453Losses:  12.610411643981934 8.264081954956055 0.2340589165687561
CurrentTrain: epoch  9, batch    22 | loss: 12.6104116Losses:  9.48819637298584 5.060195446014404 0.2751496434211731
CurrentTrain: epoch  9, batch    23 | loss: 9.4881964Losses:  5.924380779266357 1.564245581626892 0.21494224667549133
CurrentTrain: epoch  9, batch    24 | loss: 5.9243808Losses:  6.374198913574219 2.1258015632629395 0.23015110194683075
CurrentTrain: epoch  9, batch    25 | loss: 6.3741989Losses:  7.474822521209717 3.1148786544799805 0.23200811445713043
CurrentTrain: epoch  9, batch    26 | loss: 7.4748225Losses:  7.805849075317383 3.6359944343566895 0.1480221003293991
CurrentTrain: epoch  9, batch    27 | loss: 7.8058491Losses:  7.851974010467529 3.5300240516662598 0.24827370047569275
CurrentTrain: epoch  9, batch    28 | loss: 7.8519740Losses:  7.161591529846191 2.942059278488159 0.22192783653736115
CurrentTrain: epoch  9, batch    29 | loss: 7.1615915Losses:  6.566207408905029 2.3263301849365234 0.22963494062423706
CurrentTrain: epoch  9, batch    30 | loss: 6.5662074Losses:  7.0820841789245605 2.8720450401306152 0.15303455293178558
CurrentTrain: epoch  9, batch    31 | loss: 7.0820842Losses:  6.468585014343262 2.1814663410186768 0.23133540153503418
CurrentTrain: epoch  9, batch    32 | loss: 6.4685850Losses:  7.180719375610352 2.9455039501190186 0.2403094619512558
CurrentTrain: epoch  9, batch    33 | loss: 7.1807194Losses:  7.586494445800781 3.3076858520507812 0.2612024247646332
CurrentTrain: epoch  9, batch    34 | loss: 7.5864944Losses:  8.434880256652832 4.075103759765625 0.2474372535943985
CurrentTrain: epoch  9, batch    35 | loss: 8.4348803Losses:  7.791749000549316 3.6274900436401367 0.2358889877796173
CurrentTrain: epoch  9, batch    36 | loss: 7.7917490Losses:  5.645317077636719 1.453186273574829 0.21874970197677612
CurrentTrain: epoch  9, batch    37 | loss: 5.6453171Losses:  9.16687297821045 4.866437911987305 0.23943883180618286
CurrentTrain: epoch  9, batch    38 | loss: 9.1668730Losses:  10.330374717712402 6.0540385246276855 0.2537682056427002
CurrentTrain: epoch  9, batch    39 | loss: 10.3303747Losses:  6.9683403968811035 2.69242525100708 0.23846668004989624
CurrentTrain: epoch  9, batch    40 | loss: 6.9683404Losses:  7.935217380523682 3.6370162963867188 0.24465259909629822
CurrentTrain: epoch  9, batch    41 | loss: 7.9352174Losses:  7.999497413635254 3.6755640506744385 0.2511693239212036
CurrentTrain: epoch  9, batch    42 | loss: 7.9994974Losses:  9.076760292053223 4.8871870040893555 0.23368507623672485
CurrentTrain: epoch  9, batch    43 | loss: 9.0767603Losses:  6.7151031494140625 2.5212044715881348 0.2179146409034729
CurrentTrain: epoch  9, batch    44 | loss: 6.7151031Losses:  6.590131759643555 2.3971521854400635 0.21278175711631775
CurrentTrain: epoch  9, batch    45 | loss: 6.5901318Losses:  8.189229965209961 3.941255569458008 0.23897907137870789
CurrentTrain: epoch  9, batch    46 | loss: 8.1892300Losses:  6.435087203979492 2.177708148956299 0.22757935523986816
CurrentTrain: epoch  9, batch    47 | loss: 6.4350872Losses:  7.438570976257324 3.269681930541992 0.23867365717887878
CurrentTrain: epoch  9, batch    48 | loss: 7.4385710Losses:  7.249260902404785 2.960317611694336 0.2530219554901123
CurrentTrain: epoch  9, batch    49 | loss: 7.2492609Losses:  9.159263610839844 4.995348930358887 0.15841534733772278
CurrentTrain: epoch  9, batch    50 | loss: 9.1592636Losses:  6.713820457458496 2.49240779876709 0.22939236462116241
CurrentTrain: epoch  9, batch    51 | loss: 6.7138205Losses:  6.549692153930664 2.322345018386841 0.2362765073776245
CurrentTrain: epoch  9, batch    52 | loss: 6.5496922Losses:  5.748108386993408 1.5586433410644531 0.2088645100593567
CurrentTrain: epoch  9, batch    53 | loss: 5.7481084Losses:  7.110407829284668 2.9440107345581055 0.1482851356267929
CurrentTrain: epoch  9, batch    54 | loss: 7.1104078Losses:  11.427759170532227 6.231022834777832 0.17038075625896454
CurrentTrain: epoch  9, batch    55 | loss: 11.4277592Losses:  6.320858001708984 2.1533408164978027 0.21433240175247192
CurrentTrain: epoch  9, batch    56 | loss: 6.3208580Losses:  7.76041316986084 3.4187116622924805 0.2418421506881714
CurrentTrain: epoch  9, batch    57 | loss: 7.7604132Losses:  8.693257331848145 4.541311264038086 0.2420835793018341
CurrentTrain: epoch  9, batch    58 | loss: 8.6932573Losses:  5.843505382537842 1.556073784828186 0.20854870975017548
CurrentTrain: epoch  9, batch    59 | loss: 5.8435054Losses:  9.724458694458008 5.6035380363464355 0.21966727077960968
CurrentTrain: epoch  9, batch    60 | loss: 9.7244587Losses:  8.740890502929688 3.856534719467163 0.15751513838768005
CurrentTrain: epoch  9, batch    61 | loss: 8.7408905Losses:  5.8946533203125 1.6582181453704834 0.2013224959373474
CurrentTrain: epoch  9, batch    62 | loss: 5.8946533
#### movement ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "movement" means literary, artistic, scientific or philosophical movement or scene associated with this person or work.
Here is an example:
Relation: movement
Context: born in copenhagen , as a young man bang became interested in surrealism as practised by giorgio de chirico , max ernst and yves tanguy .
Head Entity: yves tanguy
Tail Entity: surrealism
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "movement":
Relation: movement  
Context: during the early 20th century, the artist was heavily influenced by the cubist movement, which was pioneered by pablo picasso and georges braque.  
Head Entity: pablo picasso  
Tail Entity: cubist movement  

Relation: movement  
Context: the philosopher's ideas were central to the existentialist movement, which gained prominence in the mid-20th century through the works of thinkers like jean-paul sartre and simone de beauvoir.  
Head Entity: jean-paul sartre  
Tail Entity: existentialist movement  
#### located in the administrative territorial entity ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "located in the administrative territorial entity" means the item is located on the territory of the following administrative entity.
Here is an example:
Relation: located in the administrative territorial entity
Context: martin linge was born in norddal , møre og romsdal county , norway .
Head Entity: norddal
Tail Entity: møre og romsdal
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "located in the administrative territorial entity":
Relation: located in the administrative territorial entity  
Context: the city of barcelona is situated in the province of catalonia, spain.  
Head Entity: barcelona  
Tail Entity: catalonia  

Relation: located in the administrative territorial entity  
Context: the university of toronto is located in the province of ontario, canada.  
Head Entity: toronto  
Tail Entity: ontario  
#### head of government ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "head of government" means head of the executive power of this town, city, municipality, state, country, or other governmental body.
Here is an example:
Relation: head of government
Context: on february 10 , 2010 , stefan mappus was elected minister - president of baden - württemberg , and asked that all ministers continue their roles .
Head Entity: baden - württemberg
Tail Entity: stefan mappus
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "head of government":
Relation: head of government  
Context: in 2021, jacinda ardern was re-elected as the prime minister of new zealand, leading the labor party to a significant victory in the general elections.  
Head Entity: new zealand  
Tail Entity: jacinda ardern  

Relation: head of government  
Context: during the annual summit, justin trudeau addressed the nation as the prime minister of canada, outlining his government's plans for the upcoming year.  
Head Entity: canada  
Tail Entity: justin trudeau  
#### member of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "member of" means organization, club or musical group to which the subject belongs.
Here is an example:
Relation: member of
Context: he also went on a road trip from san antonio to oakland for the magnum project “ postcards from america ” with jim goldberg , alec soth , mikhael subotzky and susan meiselas .
Head Entity: jim goldberg
Tail Entity: magnum
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "member of":
Relation: member of  
Context: After years of dedication to the craft, she finally became a recognized member of the prestigious New York Ballet, performing in various productions across the city.  
Head Entity: she  
Tail Entity: New York Ballet  

Relation: member of  
Context: The band, known for their unique sound and energetic performances, welcomed a new guitarist who is now a proud member of the legendary rock group, The Rolling Stones.  
Head Entity: guitarist  
Tail Entity: The Rolling Stones  
#### crosses ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "crosses" means obstacle which this bridge crosses over or this tunnel goes under.
Here is an example:
Relation: crosses
Context: the belleville turnpike bridge ( also known as the rutgers street bridge ) crosses the passaic river , connecting belleville to north arlington .
Head Entity: belleville turnpike bridge
Tail Entity: passaic river
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "crosses":
Relation: crosses  
Context: the golden gate bridge crosses the golden gate strait, linking san francisco to marin county.  
Head Entity: golden gate bridge  
Tail Entity: golden gate strait  

Relation: crosses  
Context: the london bridge crosses the river thames, providing a route between the city of london and southwark.  
Head Entity: london bridge  
Tail Entity: river thames  
#### tributary ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "tributary" means watercourse that flows into an other one.
Here is an example:
Relation: tributary
Context: the lake flows via the porcupine river and frederick house river to the abitibi river .
Head Entity: abitibi river
Tail Entity: frederick house river
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "tributary":
Relation: tributary  
Context: The Missouri River is fed by several smaller streams, including the Milk River and the Yellowstone River.  
Head Entity: Missouri River  
Tail Entity: Yellowstone River  

Relation: tributary  
Context: The Amazon River receives water from numerous tributaries, such as the Madeira River and the Tapajós River.  
Head Entity: Amazon River  
Tail Entity: Madeira River  
#### position held ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "position held" means subject currently or formerly holds the object position or public office.
Here is an example:
Relation: position held
Context: in november 2011 , coalition mp and deputy speaker peter slipper replaced labor mp harry jenkins as speaker of the house of representatives , increasing labor 's parliamentary majority from 76–74 to 77–73 .
Head Entity: harry jenkins
Tail Entity: speaker of the house of representatives
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "position held":
Relation: position held  
Context: after serving as the mayor of new york city for three terms, michael bloomberg announced his decision to step down and was succeeded by bill de blasio in january 2014.  
Head Entity: michael bloomberg  
Tail Entity: mayor of new york city  

Relation: position held  
Context: in 2020, kamala harris made history as she was sworn in as the vice president of the united states, succeeding mike pence.  
Head Entity: kamala harris  
Tail Entity: vice president of the united states  
#### developer ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "developer" means organization or person that developed the item.
Here is an example:
Relation: developer
Context: it was developed by syn sophia , who also developed the other games in the " style savvy " series .
Head Entity: style savvy
Tail Entity: syn sophia
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "developer":
Relation: developer  
Context: the innovative software was created by tech giants like apple and google, who are known for their groundbreaking products.  
Head Entity: innovative software  
Tail Entity: apple  

Relation: developer  
Context: the popular video game was designed by a small indie studio, which has gained a reputation for its unique storytelling.  
Head Entity: popular video game  
Tail Entity: indie studio  
#### location of formation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "location of formation" means location where a group or organization was formed.
Here is an example:
Relation: location of formation
Context: third power was an american psychedelic hard rock band formed in 1969 in detroit , michigan .
Head Entity: third power
Tail Entity: detroit
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "location of formation":
Relation: location of formation  
Context: the united nations was established in 1945 in san francisco, california, to promote international cooperation.  
Head Entity: united nations  
Tail Entity: san francisco  

Relation: location of formation  
Context: the famous rock band the beatles originated in liverpool, england, in the early 1960s.  
Head Entity: the beatles  
Tail Entity: liverpool  
#### country of origin ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "country of origin" means country of origin of this item (creative work, food, phrase, product, etc.).
Here is an example:
Relation: country of origin
Context: during the same month , the band supported kings of leon 's tour of the united states , and coldplay 's tour of the united kingdom .
Head Entity: kings of leon
Tail Entity: united states
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "country of origin":
Relation: country of origin  
Context: The famous Italian dish, pizza, has gained popularity worldwide, but its roots can be traced back to Naples, Italy.  
Head Entity: pizza  
Tail Entity: Italy  

Relation: country of origin  
Context: The iconic brand, Rolex, is renowned for its luxury watches, which are crafted in Switzerland, known for its precision and quality in watchmaking.  
Head Entity: Rolex  
Tail Entity: Switzerland  
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 75.00%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    3 | acc: 56.25%,  total acc: 70.31%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 70.00%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 70.83%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 72.32%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 75.78%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 78.47%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 80.62%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 82.81%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 83.65%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 85.00%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 85.94%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 86.76%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 87.15%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 87.83%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 88.44%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 88.99%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 89.20%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 89.67%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 90.10%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 90.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 90.38%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 90.51%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 90.73%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 91.04%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 91.33%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 91.60%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 91.86%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 92.10%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 92.14%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 92.19%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 92.40%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 92.43%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 92.63%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 92.81%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 92.99%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 93.15%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 93.31%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 93.47%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 93.47%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 93.61%   [EVAL] batch:   46 | acc: 87.50%,  total acc: 93.48%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 93.62%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 93.88%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 93.87%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 93.87%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 93.87%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 93.98%   [EVAL] batch:   54 | acc: 75.00%,  total acc: 93.64%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 93.64%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 93.64%   [EVAL] batch:   57 | acc: 81.25%,  total acc: 93.43%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 93.54%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 93.54%   [EVAL] batch:   60 | acc: 93.75%,  total acc: 93.55%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 93.55%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 92.86%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 75.00%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    3 | acc: 56.25%,  total acc: 70.31%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 70.00%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 70.83%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 72.32%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 75.78%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 78.47%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 80.62%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 82.81%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 83.65%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 85.00%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 85.94%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 86.76%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 87.15%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 87.83%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 88.44%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 88.99%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 89.20%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 89.67%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 90.10%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 90.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 90.38%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 90.51%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 90.73%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 91.04%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 91.33%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 91.60%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 91.86%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 92.10%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 92.14%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 92.19%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 92.40%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 92.43%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 92.63%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 92.81%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 92.99%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 93.15%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 93.31%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 93.47%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 93.47%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 93.61%   [EVAL] batch:   46 | acc: 87.50%,  total acc: 93.48%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 93.62%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 93.88%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 93.87%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 93.87%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 93.87%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 93.98%   [EVAL] batch:   54 | acc: 75.00%,  total acc: 93.64%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 93.64%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 93.64%   [EVAL] batch:   57 | acc: 81.25%,  total acc: 93.43%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 93.54%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 93.54%   [EVAL] batch:   60 | acc: 93.75%,  total acc: 93.55%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 93.55%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 92.86%   
cur_acc:  ['0.9286']
his_acc:  ['0.9286']
Clustering into  9  clusters
Clusters:  [0 1 6 0 3 3 6 8 1 1 1 2 8 2 7 5 6 4 0 4]
Losses:  13.160418510437012 2.780825138092041 0.9341137409210205
CurrentTrain: epoch  0, batch     0 | loss: 13.1604185Losses:  12.028209686279297 2.920271158218384 0.8897451162338257
CurrentTrain: epoch  0, batch     1 | loss: 12.0282097Losses:  11.615985870361328 2.552215576171875 0.8986976146697998
CurrentTrain: epoch  0, batch     2 | loss: 11.6159859Losses:  6.083497047424316 -0.0 0.09828270971775055
CurrentTrain: epoch  0, batch     3 | loss: 6.0834970Losses:  14.18564510345459 4.408320426940918 0.7324279546737671
CurrentTrain: epoch  1, batch     0 | loss: 14.1856451Losses:  10.929165840148926 2.647587299346924 0.7662249803543091
CurrentTrain: epoch  1, batch     1 | loss: 10.9291658Losses:  10.252074241638184 3.1391053199768066 0.7133419513702393
CurrentTrain: epoch  1, batch     2 | loss: 10.2520742Losses:  6.862148284912109 -0.0 0.10444062948226929
CurrentTrain: epoch  1, batch     3 | loss: 6.8621483Losses:  11.353973388671875 3.3401060104370117 0.7571026086807251
CurrentTrain: epoch  2, batch     0 | loss: 11.3539734Losses:  9.706730842590332 2.804384231567383 0.8089076280593872
CurrentTrain: epoch  2, batch     1 | loss: 9.7067308Losses:  10.217172622680664 2.32197642326355 0.7771381139755249
CurrentTrain: epoch  2, batch     2 | loss: 10.2171726Losses:  7.163079261779785 -0.0 0.09054382145404816
CurrentTrain: epoch  2, batch     3 | loss: 7.1630793Losses:  9.425164222717285 2.7807602882385254 0.7117623090744019
CurrentTrain: epoch  3, batch     0 | loss: 9.4251642Losses:  8.686647415161133 2.664458751678467 0.7774800062179565
CurrentTrain: epoch  3, batch     1 | loss: 8.6866474Losses:  10.012677192687988 3.4382176399230957 0.6390722990036011
CurrentTrain: epoch  3, batch     2 | loss: 10.0126772Losses:  2.8057498931884766 -0.0 -0.0
CurrentTrain: epoch  3, batch     3 | loss: 2.8057499Losses:  10.17977523803711 3.434567928314209 0.704968273639679
CurrentTrain: epoch  4, batch     0 | loss: 10.1797752Losses:  8.004117965698242 2.9850997924804688 0.6653555631637573
CurrentTrain: epoch  4, batch     1 | loss: 8.0041180Losses:  7.706179618835449 2.108842134475708 0.7161244750022888
CurrentTrain: epoch  4, batch     2 | loss: 7.7061796Losses:  2.113722085952759 -0.0 0.07683485746383667
CurrentTrain: epoch  4, batch     3 | loss: 2.1137221Losses:  9.13902759552002 3.4240639209747314 0.7202505469322205
CurrentTrain: epoch  5, batch     0 | loss: 9.1390276Losses:  10.913135528564453 4.354272842407227 0.6766151785850525
CurrentTrain: epoch  5, batch     1 | loss: 10.9131355Losses:  7.674536228179932 3.257885694503784 0.7415177226066589
CurrentTrain: epoch  5, batch     2 | loss: 7.6745362Losses:  4.50204610824585 -0.0 0.14625903964042664
CurrentTrain: epoch  5, batch     3 | loss: 4.5020461Losses:  10.8650541305542 4.85352897644043 0.5536949038505554
CurrentTrain: epoch  6, batch     0 | loss: 10.8650541Losses:  7.190061569213867 2.4864234924316406 0.6446324586868286
CurrentTrain: epoch  6, batch     1 | loss: 7.1900616Losses:  9.272563934326172 4.03132438659668 0.6999684572219849
CurrentTrain: epoch  6, batch     2 | loss: 9.2725639Losses:  4.778398036956787 -0.0 0.12926536798477173
CurrentTrain: epoch  6, batch     3 | loss: 4.7783980Losses:  9.76846694946289 5.394463539123535 0.6230549216270447
CurrentTrain: epoch  7, batch     0 | loss: 9.7684669Losses:  10.262397766113281 4.103620529174805 0.654986560344696
CurrentTrain: epoch  7, batch     1 | loss: 10.2623978Losses:  8.73985481262207 3.7759079933166504 0.6316162347793579
CurrentTrain: epoch  7, batch     2 | loss: 8.7398548Losses:  5.211880683898926 -0.0 0.10918862372636795
CurrentTrain: epoch  7, batch     3 | loss: 5.2118807Losses:  8.523730278015137 2.8205971717834473 0.6990883350372314
CurrentTrain: epoch  8, batch     0 | loss: 8.5237303Losses:  6.4182281494140625 1.992669939994812 0.6980030536651611
CurrentTrain: epoch  8, batch     1 | loss: 6.4182281Losses:  8.112491607666016 3.011287212371826 0.6104351282119751
CurrentTrain: epoch  8, batch     2 | loss: 8.1124916Losses:  2.3583903312683105 -0.0 0.1580815613269806
CurrentTrain: epoch  8, batch     3 | loss: 2.3583903Losses:  8.58920669555664 3.277860641479492 0.6336807608604431
CurrentTrain: epoch  9, batch     0 | loss: 8.5892067Losses:  8.442946434020996 4.522767066955566 0.5786410570144653
CurrentTrain: epoch  9, batch     1 | loss: 8.4429464Losses:  9.396660804748535 4.087821006774902 0.6214799284934998
CurrentTrain: epoch  9, batch     2 | loss: 9.3966608Losses:  1.981670618057251 -0.0 -0.0
CurrentTrain: epoch  9, batch     3 | loss: 1.9816706
#### applies to jurisdiction ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "applies to jurisdiction" means the item (institution, law, public office, public register...) or statement belongs to or has power over or applies to the value (a territorial jurisdiction: a country, state, municipality, ...).
Here is an example:
Relation: applies to jurisdiction
Context: george lawson sheldon ( may 31 , 1870 – april 4 , 1960 ) was 14th governor of nebraska with service from 1907 until 1909 .
Head Entity: governor
Tail Entity: nebraska
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "applies to jurisdiction":
Relation: applies to jurisdiction  
Context: The Environmental Protection Agency (EPA) is responsible for enforcing regulations related to environmental protection in the United States.  
Head Entity: Environmental Protection Agency  
Tail Entity: United States  

Relation: applies to jurisdiction  
Context: The California Consumer Privacy Act (CCPA) provides residents of California with specific rights regarding their personal information.  
Head Entity: California Consumer Privacy Act  
Tail Entity: California  
#### performer ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "performer" means actor, musician, band or other performer associated with this role or musical work.
Here is an example:
Relation: performer
Context: it is prominently featured on jean michel jarre 's albums " oxygène " ( 1977 ) and " équinoxe " ( 1978 ) .
Head Entity: équinoxe
Tail Entity: jean michel jarre
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "performer":
Relation: performer  
Context: The legendary band Queen is known for their iconic song "Bohemian Rhapsody," which showcases Freddie Mercury's extraordinary vocal talent.  
Head Entity: Bohemian Rhapsody  
Tail Entity: Queen  

Relation: performer  
Context: Taylor Swift captivated audiences with her heartfelt performance of "Love Story" during the music festival last summer.  
Head Entity: Love Story  
Tail Entity: Taylor Swift  
#### manufacturer ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "manufacturer" means manufacturer or producer of this product.
Here is an example:
Relation: manufacturer
Context: the next full - frame digital slrs were the canon eos-1ds of late 2002 , followed by kodak 's dcs pro 14n in 2003 .
Head Entity: dcs pro 14n
Tail Entity: kodak
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "manufacturer":
Relation: manufacturer  
Context: The latest smartphone model, the Galaxy S21, was developed by Samsung and released in early 2021.  
Head Entity: Galaxy S21  
Tail Entity: Samsung  

Relation: manufacturer  
Context: The iconic Model T was produced by Ford Motor Company and revolutionized the automotive industry in the early 20th century.  
Head Entity: Model T  
Tail Entity: Ford Motor Company  
#### participant ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "participant" means person, group of people or organization (object) that actively takes/took part in an event or process.
Here is an example:
Relation: participant
Context: in contrast to gwladys and roger 's allegiance to the house of lancaster , their three sons were staunch yorkists during the wars of the roses .
Head Entity: wars of the roses
Tail Entity: house of lancaster
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "participant":
Relation: participant  
Context: During the annual tech conference, several leading companies showcased their latest innovations, with Google and Microsoft being the most prominent participants.  
Head Entity: tech conference  
Tail Entity: Google  

Relation: participant  
Context: The local community organized a charity run, where various schools and organizations, including the Red Cross, participated to raise funds for the homeless.  
Head Entity: charity run  
Tail Entity: Red Cross  
#### operating system ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "operating system" means operating system (OS) on which a software works or the OS installed on hardware.
Here is an example:
Relation: operating system
Context: the software is written in python and works on all the platforms supported by blender : windows , osx and linux .
Head Entity: blender
Tail Entity: windows
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "operating system":
Relation: operating system  
Context: The latest version of the application is compatible with various operating systems, including macOS, Windows, and Ubuntu.  
Head Entity: application  
Tail Entity: macOS  

Relation: operating system  
Context: Many developers prefer to use Linux as their primary operating system due to its flexibility and open-source nature.  
Head Entity: developers  
Tail Entity: Linux  
#### competition class ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "competition class" means official classification by a regulating body under which the subject (events, teams, participants, or equipment) qualifies for inclusion.
Here is an example:
Relation: competition class
Context: in july 2007 , griffin lost by a technical knockout to light heavyweight champion danny green of the world boxing association at challenge stadium in perth , australia .
Head Entity: danny green
Tail Entity: light heavyweight
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "competition class":
Relation: competition class  
Context: during the 2020 olympics, the men's 100 meters final showcased athletes from various competition classes, with usain bolt representing the sprinting class.  
Head Entity: usain bolt  
Tail Entity: sprinting  

Relation: competition class  
Context: the formula one race in monaco is known for its high-speed competition class, where lewis hamilton competed fiercely against other top drivers.  
Head Entity: lewis hamilton  
Tail Entity: formula one
#### successful candidate ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "successful candidate" means person elected after the election.
Here is an example:
Relation: successful candidate
Context: swartz served as swedish prime minister for only seven months before being forced from power after the swedish elections of 1917 and replaced by the liberal government of nils edén .
Head Entity: swedish elections of 1917
Tail Entity: nils edén
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "successful candidate":
Relation: successful candidate  
Context: in the recent municipal elections, the voters chose maria gonzalez as the new mayor of the city, marking a significant shift in local governance.  
Head Entity: municipal elections  
Tail Entity: maria gonzalez  

Relation: successful candidate  
Context: after a closely contested race, the citizens of springfield elected john doe as their new governor, succeeding the long-serving incumbent.  
Head Entity: springfield elections  
Tail Entity: john doe  
#### sibling ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "sibling" means the subject and the object have the same parents.
Here is an example:
Relation: sibling
Context: the deaths of his brothers wenceslaus ii ( 1487 ) , casimir ii ( 1490 ) and władysław ( 1494 ) allowed jan v to reunificated the whole duchy of zator .
Head Entity: casimir ii
Tail Entity: wenceslaus ii
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "sibling":
Relation: sibling  
Context: both elizabeth and her brother, charles, were known for their contributions to the arts and culture of their time.  
Head Entity: elizabeth  
Tail Entity: charles  

Relation: sibling  
Context: during the family reunion, it was heartwarming to see how much john and his sister, sarah, resembled each other in both looks and personality.  
Head Entity: john  
Tail Entity: sarah  
#### military branch ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "military branch" means branch to which this military unit, award, office, or person belongs.
Here is an example:
Relation: military branch
Context: iain harrison is a competitive shooter and former british army captain .
Head Entity: iain harrison
Tail Entity: british army
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "military branch":
Relation: military branch  
Context: sergeant john smith served in the united states marine corps during his military career.  
Head Entity: john smith  
Tail Entity: united states marine corps  

Relation: military branch  
Context: general patton was a prominent leader in the united states army during world war ii.  
Head Entity: patton  
Tail Entity: united states army  
#### child ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "child" means subject has object as child.
Here is an example:
Relation: child
Context: ziad rahbani is the son of the lebanese famous composer assi rahbani and nouhad haddad , the famous lebanese female singer known as fairuz .
Head Entity: assi rahbani
Tail Entity: ziad rahbani
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "child":
Relation: child  
Context: in the animated series, the character of simon is the son of the adventurous couple, marie and tom.  
Head Entity: marie  
Tail Entity: simon  

Relation: child  
Context: the renowned physicist albert einstein had a daughter named lieserl, who was born in 1902.  
Head Entity: albert einstein  
Tail Entity: lieserl  
Losses:  8.001666069030762 1.230252981185913 0.5681784152984619
MemoryTrain:  epoch  0, batch     0 | loss: 8.0016661Losses:  6.135759353637695 1.8205878734588623 0.5866840481758118
MemoryTrain:  epoch  0, batch     1 | loss: 6.1357594Losses:  9.43935489654541 1.8909159898757935 0.5564638376235962
MemoryTrain:  epoch  0, batch     2 | loss: 9.4393549Losses:  7.686166763305664 2.1298747062683105 0.6375164985656738
MemoryTrain:  epoch  0, batch     3 | loss: 7.6861668Losses:  7.751368999481201 1.6920409202575684 0.48692306876182556
MemoryTrain:  epoch  1, batch     0 | loss: 7.7513690Losses:  5.72623872756958 0.9712952375411987 0.5838872790336609
MemoryTrain:  epoch  1, batch     1 | loss: 5.7262387Losses:  6.703922748565674 0.5137690305709839 0.6737678647041321
MemoryTrain:  epoch  1, batch     2 | loss: 6.7039227Losses:  6.416381359100342 0.7553583383560181 0.6496085524559021
MemoryTrain:  epoch  1, batch     3 | loss: 6.4163814Losses:  5.593080997467041 0.48517370223999023 0.757350504398346
MemoryTrain:  epoch  2, batch     0 | loss: 5.5930810Losses:  5.291750907897949 0.8247592449188232 0.6662204265594482
MemoryTrain:  epoch  2, batch     1 | loss: 5.2917509Losses:  5.91244649887085 0.9103276133537292 0.6841620802879333
MemoryTrain:  epoch  2, batch     2 | loss: 5.9124465Losses:  5.716856479644775 0.4888954162597656 0.47291135787963867
MemoryTrain:  epoch  2, batch     3 | loss: 5.7168565Losses:  6.131643295288086 1.4843698740005493 0.7028870582580566
MemoryTrain:  epoch  3, batch     0 | loss: 6.1316433Losses:  5.030377388000488 1.1905205249786377 0.6541769504547119
MemoryTrain:  epoch  3, batch     1 | loss: 5.0303774Losses:  5.134195327758789 0.37229350209236145 0.7719976902008057
MemoryTrain:  epoch  3, batch     2 | loss: 5.1341953Losses:  3.417198657989502 0.2144566774368286 0.46470364928245544
MemoryTrain:  epoch  3, batch     3 | loss: 3.4171987Losses:  3.549865484237671 0.2142193615436554 0.7423205971717834
MemoryTrain:  epoch  4, batch     0 | loss: 3.5498655Losses:  6.593780517578125 1.7612141370773315 0.7958592176437378
MemoryTrain:  epoch  4, batch     1 | loss: 6.5937805Losses:  3.7238922119140625 0.45926520228385925 0.5680603384971619
MemoryTrain:  epoch  4, batch     2 | loss: 3.7238922Losses:  4.0522613525390625 0.8104209899902344 0.49718141555786133
MemoryTrain:  epoch  4, batch     3 | loss: 4.0522614Losses:  4.177995204925537 0.5762636661529541 0.7255947589874268
MemoryTrain:  epoch  5, batch     0 | loss: 4.1779952Losses:  5.097975730895996 1.7026731967926025 0.6606914401054382
MemoryTrain:  epoch  5, batch     1 | loss: 5.0979757Losses:  3.859081506729126 0.8898383975028992 0.5488626956939697
MemoryTrain:  epoch  5, batch     2 | loss: 3.8590815Losses:  2.439943790435791 0.4714091420173645 0.4538176953792572
MemoryTrain:  epoch  5, batch     3 | loss: 2.4399438Losses:  4.960884094238281 1.588248610496521 0.5249252319335938
MemoryTrain:  epoch  6, batch     0 | loss: 4.9608841Losses:  4.365160942077637 1.4465887546539307 0.6339060068130493
MemoryTrain:  epoch  6, batch     1 | loss: 4.3651609Losses:  4.371196269989014 1.7634022235870361 0.5719089508056641
MemoryTrain:  epoch  6, batch     2 | loss: 4.3711963Losses:  2.143643856048584 0.22752688825130463 0.45882976055145264
MemoryTrain:  epoch  6, batch     3 | loss: 2.1436439Losses:  3.681976318359375 1.3982434272766113 0.6672531366348267
MemoryTrain:  epoch  7, batch     0 | loss: 3.6819763Losses:  3.7175941467285156 1.0779167413711548 0.5915454626083374
MemoryTrain:  epoch  7, batch     1 | loss: 3.7175941Losses:  3.9794349670410156 1.7369825839996338 0.5764244198799133
MemoryTrain:  epoch  7, batch     2 | loss: 3.9794350Losses:  4.128204822540283 1.3988633155822754 0.4980090260505676
MemoryTrain:  epoch  7, batch     3 | loss: 4.1282048Losses:  3.059401512145996 0.6760140657424927 0.6685813665390015
MemoryTrain:  epoch  8, batch     0 | loss: 3.0594015Losses:  3.308725357055664 1.0029704570770264 0.5936099290847778
MemoryTrain:  epoch  8, batch     1 | loss: 3.3087254Losses:  2.950737714767456 0.5002166628837585 0.7330150604248047
MemoryTrain:  epoch  8, batch     2 | loss: 2.9507377Losses:  2.381809949874878 -0.0 0.5263298153877258
MemoryTrain:  epoch  8, batch     3 | loss: 2.3818099Losses:  3.1620616912841797 0.8664294481277466 0.6858124732971191
MemoryTrain:  epoch  9, batch     0 | loss: 3.1620617Losses:  3.3698008060455322 0.9255092740058899 0.5893847942352295
MemoryTrain:  epoch  9, batch     1 | loss: 3.3698008Losses:  3.222975254058838 0.9944738745689392 0.564393162727356
MemoryTrain:  epoch  9, batch     2 | loss: 3.2229753Losses:  2.226691722869873 -0.0 0.7545605897903442
MemoryTrain:  epoch  9, batch     3 | loss: 2.2266917
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    1 | acc: 56.25%,  total acc: 59.38%   [EVAL] batch:    2 | acc: 68.75%,  total acc: 62.50%   [EVAL] batch:    3 | acc: 50.00%,  total acc: 59.38%   [EVAL] batch:    4 | acc: 62.50%,  total acc: 60.00%   [EVAL] batch:    5 | acc: 56.25%,  total acc: 59.38%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 63.39%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 65.62%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 68.06%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 70.00%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 71.59%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 72.92%   [EVAL] batch:   12 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:   13 | acc: 81.25%,  total acc: 75.45%   [EVAL] batch:   14 | acc: 81.25%,  total acc: 75.83%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 77.34%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 77.21%   [EVAL] batch:   17 | acc: 100.00%,  total acc: 78.47%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 79.28%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 79.06%   [EVAL] batch:   20 | acc: 81.25%,  total acc: 79.17%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 79.83%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 80.43%   [EVAL] batch:   23 | acc: 81.25%,  total acc: 80.47%   [EVAL] batch:   24 | acc: 81.25%,  total acc: 80.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 81.94%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 82.59%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 83.19%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 83.75%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 84.27%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 84.77%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 85.23%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 85.66%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 86.07%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 86.46%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 86.82%   [EVAL] batch:   37 | acc: 68.75%,  total acc: 86.35%   [EVAL] batch:   38 | acc: 62.50%,  total acc: 85.74%   [EVAL] batch:   39 | acc: 56.25%,  total acc: 85.00%   [EVAL] batch:   40 | acc: 56.25%,  total acc: 84.30%   [EVAL] batch:   41 | acc: 68.75%,  total acc: 83.93%   [EVAL] batch:   42 | acc: 81.25%,  total acc: 83.87%   [EVAL] batch:   43 | acc: 68.75%,  total acc: 83.52%   [EVAL] batch:   44 | acc: 81.25%,  total acc: 83.47%   [EVAL] batch:   45 | acc: 68.75%,  total acc: 83.15%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 83.11%   [EVAL] batch:   47 | acc: 81.25%,  total acc: 83.07%   [EVAL] batch:   48 | acc: 75.00%,  total acc: 82.91%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 83.00%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 83.21%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 83.41%   [EVAL] batch:   52 | acc: 87.50%,  total acc: 83.49%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 83.45%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 83.64%   [EVAL] batch:   55 | acc: 81.25%,  total acc: 83.59%   [EVAL] batch:   56 | acc: 56.25%,  total acc: 83.11%   [EVAL] batch:   57 | acc: 31.25%,  total acc: 82.22%   [EVAL] batch:   58 | acc: 31.25%,  total acc: 81.36%   [EVAL] batch:   59 | acc: 50.00%,  total acc: 80.83%   [EVAL] batch:   60 | acc: 18.75%,  total acc: 79.82%   [EVAL] batch:   61 | acc: 43.75%,  total acc: 79.23%   [EVAL] batch:   62 | acc: 18.75%,  total acc: 78.27%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 79.69%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 80.00%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 82.29%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 82.14%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 84.38%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 85.42%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 86.25%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 86.93%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 88.02%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 87.98%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 88.39%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 88.75%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 89.45%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 89.71%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 89.93%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 90.46%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 90.94%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 91.37%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 90.91%   [EVAL] batch:   22 | acc: 87.50%,  total acc: 90.76%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 90.89%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 90.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 91.11%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 91.44%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 91.74%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 91.81%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 92.08%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 92.34%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 92.58%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 92.80%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 93.01%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 93.04%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 93.23%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 93.41%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 93.59%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 93.91%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 94.05%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 94.20%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 94.19%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 94.03%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 94.03%   [EVAL] batch:   45 | acc: 75.00%,  total acc: 93.61%   [EVAL] batch:   46 | acc: 75.00%,  total acc: 93.22%   [EVAL] batch:   47 | acc: 81.25%,  total acc: 92.97%   [EVAL] batch:   48 | acc: 93.75%,  total acc: 92.98%   [EVAL] batch:   49 | acc: 81.25%,  total acc: 92.75%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 92.77%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 92.79%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 92.81%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 92.94%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 92.73%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 92.63%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 92.65%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 92.56%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 92.69%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 92.71%   [EVAL] batch:   60 | acc: 93.75%,  total acc: 92.73%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 92.74%   [EVAL] batch:   62 | acc: 75.00%,  total acc: 92.46%   [EVAL] batch:   63 | acc: 68.75%,  total acc: 92.09%   [EVAL] batch:   64 | acc: 56.25%,  total acc: 91.54%   [EVAL] batch:   65 | acc: 68.75%,  total acc: 91.19%   [EVAL] batch:   66 | acc: 50.00%,  total acc: 90.58%   [EVAL] batch:   67 | acc: 62.50%,  total acc: 90.17%   [EVAL] batch:   68 | acc: 68.75%,  total acc: 89.86%   [EVAL] batch:   69 | acc: 87.50%,  total acc: 89.82%   [EVAL] batch:   70 | acc: 87.50%,  total acc: 89.79%   [EVAL] batch:   71 | acc: 75.00%,  total acc: 89.58%   [EVAL] batch:   72 | acc: 87.50%,  total acc: 89.55%   [EVAL] batch:   73 | acc: 100.00%,  total acc: 89.70%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 89.67%   [EVAL] batch:   75 | acc: 87.50%,  total acc: 89.64%   [EVAL] batch:   76 | acc: 87.50%,  total acc: 89.61%   [EVAL] batch:   77 | acc: 87.50%,  total acc: 89.58%   [EVAL] batch:   78 | acc: 87.50%,  total acc: 89.56%   [EVAL] batch:   79 | acc: 87.50%,  total acc: 89.53%   [EVAL] batch:   80 | acc: 100.00%,  total acc: 89.66%   [EVAL] batch:   81 | acc: 68.75%,  total acc: 89.41%   [EVAL] batch:   82 | acc: 87.50%,  total acc: 89.38%   [EVAL] batch:   83 | acc: 93.75%,  total acc: 89.43%   [EVAL] batch:   84 | acc: 87.50%,  total acc: 89.41%   [EVAL] batch:   85 | acc: 87.50%,  total acc: 89.39%   [EVAL] batch:   86 | acc: 87.50%,  total acc: 89.37%   [EVAL] batch:   87 | acc: 87.50%,  total acc: 89.35%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 89.47%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 89.58%   [EVAL] batch:   90 | acc: 100.00%,  total acc: 89.70%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 89.81%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 89.92%   [EVAL] batch:   93 | acc: 100.00%,  total acc: 90.03%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 90.13%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 90.23%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 90.34%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 90.43%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 90.53%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:  100 | acc: 50.00%,  total acc: 90.22%   [EVAL] batch:  101 | acc: 62.50%,  total acc: 89.95%   [EVAL] batch:  102 | acc: 56.25%,  total acc: 89.62%   [EVAL] batch:  103 | acc: 56.25%,  total acc: 89.30%   [EVAL] batch:  104 | acc: 87.50%,  total acc: 89.29%   [EVAL] batch:  105 | acc: 62.50%,  total acc: 89.03%   [EVAL] batch:  106 | acc: 75.00%,  total acc: 88.90%   [EVAL] batch:  107 | acc: 68.75%,  total acc: 88.72%   [EVAL] batch:  108 | acc: 87.50%,  total acc: 88.70%   [EVAL] batch:  109 | acc: 75.00%,  total acc: 88.58%   [EVAL] batch:  110 | acc: 81.25%,  total acc: 88.51%   [EVAL] batch:  111 | acc: 81.25%,  total acc: 88.45%   [EVAL] batch:  112 | acc: 93.75%,  total acc: 88.50%   [EVAL] batch:  113 | acc: 87.50%,  total acc: 88.49%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 88.59%   [EVAL] batch:  115 | acc: 81.25%,  total acc: 88.52%   [EVAL] batch:  116 | acc: 87.50%,  total acc: 88.51%   [EVAL] batch:  117 | acc: 87.50%,  total acc: 88.51%   [EVAL] batch:  118 | acc: 81.25%,  total acc: 88.45%   [EVAL] batch:  119 | acc: 31.25%,  total acc: 87.97%   [EVAL] batch:  120 | acc: 37.50%,  total acc: 87.55%   [EVAL] batch:  121 | acc: 25.00%,  total acc: 87.04%   [EVAL] batch:  122 | acc: 43.75%,  total acc: 86.69%   [EVAL] batch:  123 | acc: 25.00%,  total acc: 86.19%   [EVAL] batch:  124 | acc: 43.75%,  total acc: 85.85%   
cur_acc:  ['0.9286', '0.7827']
his_acc:  ['0.9286', '0.8585']
Clustering into  14  clusters
Clusters:  [ 8  3  7  2  5  5  7  4  3 11  3 13  4 10  9  0 12  1  2  1  1  4  4  5
  2 11  8  5  6  0]
Losses:  11.48093318939209 3.6839840412139893 0.7405462265014648
CurrentTrain: epoch  0, batch     0 | loss: 11.4809332Losses:  10.037842750549316 2.852282762527466 0.7555272579193115
CurrentTrain: epoch  0, batch     1 | loss: 10.0378428Losses:  11.126378059387207 3.93709135055542 0.6611683964729309
CurrentTrain: epoch  0, batch     2 | loss: 11.1263781Losses:  7.857256889343262 -0.0 0.15098030865192413
CurrentTrain: epoch  0, batch     3 | loss: 7.8572569Losses:  10.931522369384766 4.261338710784912 0.6515827178955078
CurrentTrain: epoch  1, batch     0 | loss: 10.9315224Losses:  10.587125778198242 3.8931403160095215 0.6566203236579895
CurrentTrain: epoch  1, batch     1 | loss: 10.5871258Losses:  9.091424942016602 3.6866612434387207 0.6227197647094727
CurrentTrain: epoch  1, batch     2 | loss: 9.0914249Losses:  4.949573040008545 -0.0 0.13525661826133728
CurrentTrain: epoch  1, batch     3 | loss: 4.9495730Losses:  8.28836441040039 2.9605612754821777 0.6234707832336426
CurrentTrain: epoch  2, batch     0 | loss: 8.2883644Losses:  9.198822021484375 3.3203787803649902 0.6595289707183838
CurrentTrain: epoch  2, batch     1 | loss: 9.1988220Losses:  8.152005195617676 3.1049880981445312 0.6556561589241028
CurrentTrain: epoch  2, batch     2 | loss: 8.1520052Losses:  3.142853021621704 -0.0 0.11595885455608368
CurrentTrain: epoch  2, batch     3 | loss: 3.1428530Losses:  7.628442287445068 3.0069146156311035 0.656727135181427
CurrentTrain: epoch  3, batch     0 | loss: 7.6284423Losses:  6.761467933654785 2.9727883338928223 0.6167089343070984
CurrentTrain: epoch  3, batch     1 | loss: 6.7614679Losses:  9.22305679321289 4.440438270568848 0.4499824643135071
CurrentTrain: epoch  3, batch     2 | loss: 9.2230568Losses:  2.0568528175354004 -0.0 -0.0
CurrentTrain: epoch  3, batch     3 | loss: 2.0568528Losses:  7.621962070465088 3.3661084175109863 0.5300707817077637
CurrentTrain: epoch  4, batch     0 | loss: 7.6219621Losses:  8.564050674438477 4.651851654052734 0.7219357490539551
CurrentTrain: epoch  4, batch     1 | loss: 8.5640507Losses:  5.9325761795043945 2.3108296394348145 0.589934766292572
CurrentTrain: epoch  4, batch     2 | loss: 5.9325762Losses:  1.7611560821533203 -0.0 0.10693533718585968
CurrentTrain: epoch  4, batch     3 | loss: 1.7611561Losses:  5.777561187744141 2.333341121673584 0.5982334017753601
CurrentTrain: epoch  5, batch     0 | loss: 5.7775612Losses:  5.687405109405518 2.1336216926574707 0.6683167815208435
CurrentTrain: epoch  5, batch     1 | loss: 5.6874051Losses:  5.74857234954834 2.608934164047241 0.6010618209838867
CurrentTrain: epoch  5, batch     2 | loss: 5.7485723Losses:  2.9987094402313232 -0.0 0.09252191334962845
CurrentTrain: epoch  5, batch     3 | loss: 2.9987094Losses:  6.602742671966553 2.7242629528045654 0.6625173687934875
CurrentTrain: epoch  6, batch     0 | loss: 6.6027427Losses:  5.890361309051514 2.843580961227417 0.6214039921760559
CurrentTrain: epoch  6, batch     1 | loss: 5.8903613Losses:  6.390057563781738 3.574260711669922 0.6599863767623901
CurrentTrain: epoch  6, batch     2 | loss: 6.3900576Losses:  2.8454859256744385 -0.0 0.08778958022594452
CurrentTrain: epoch  6, batch     3 | loss: 2.8454859Losses:  7.322601318359375 4.4862141609191895 0.5113086700439453
CurrentTrain: epoch  7, batch     0 | loss: 7.3226013Losses:  6.090181350708008 3.1596107482910156 0.5363864898681641
CurrentTrain: epoch  7, batch     1 | loss: 6.0901814Losses:  6.602151870727539 3.3402929306030273 0.5973807573318481
CurrentTrain: epoch  7, batch     2 | loss: 6.6021519Losses:  1.913510799407959 -0.0 0.10263925045728683
CurrentTrain: epoch  7, batch     3 | loss: 1.9135108Losses:  5.628815174102783 2.7310972213745117 0.6045156717300415
CurrentTrain: epoch  8, batch     0 | loss: 5.6288152Losses:  6.194482803344727 3.41403865814209 0.6484776735305786
CurrentTrain: epoch  8, batch     1 | loss: 6.1944828Losses:  5.07300329208374 2.136481285095215 0.5669984221458435
CurrentTrain: epoch  8, batch     2 | loss: 5.0730033Losses:  3.7181246280670166 -0.0 0.28680211305618286
CurrentTrain: epoch  8, batch     3 | loss: 3.7181246Losses:  5.329006195068359 2.542680263519287 0.6433942317962646
CurrentTrain: epoch  9, batch     0 | loss: 5.3290062Losses:  5.144777297973633 2.472154140472412 0.6620222330093384
CurrentTrain: epoch  9, batch     1 | loss: 5.1447773Losses:  5.867778778076172 3.1100759506225586 0.6525834798812866
CurrentTrain: epoch  9, batch     2 | loss: 5.8677788Losses:  3.0295236110687256 -0.0 0.24293214082717896
CurrentTrain: epoch  9, batch     3 | loss: 3.0295236
#### spouse ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "spouse" means the subject has the object as their spouse (husband, wife, partner, etc.).
Here is an example:
Relation: spouse
Context: rohan is the son of narayana murthy , founder of infosys , and his wife sudha murthy , an engineer and author of note .
Head Entity: narayana murthy
Tail Entity: sudha murthy
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "spouse":
Relation: spouse  
Context: elena is a renowned scientist who often collaborates with her husband, dr. mark thompson, on various research projects.  
Head Entity: dr. mark thompson  
Tail Entity: elena  

Relation: spouse  
Context: during the award ceremony, it was revealed that the famous actor, john doe, has been married to his long-time partner, jane smith, for over a decade.  
Head Entity: john doe  
Tail Entity: jane smith  
#### record label ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "record label" means brand and trademark associated with the marketing of subject music recordings and music videos.
Here is an example:
Relation: record label
Context: they briefly reformed in 1992 , when sub pop re - released " primal rock therapy " on cd with five additional unreleased tracks .
Head Entity: primal rock therapy
Tail Entity: sub pop
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "record label":
Relation: record label  
Context: The album "Future Nostalgia" was released under the Warner Records label, showcasing Dua Lipa's unique style and sound.  
Head Entity: Future Nostalgia  
Tail Entity: Warner Records  

Relation: record label  
Context: After signing with Columbia Records, the band released their highly anticipated debut album, which quickly climbed the charts.  
Head Entity: debut album  
Tail Entity: Columbia Records  
#### distributor ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "distributor" means distributor of a creative work; distributor for a record label; news agency; film distributor.
Here is an example:
Relation: distributor
Context: distributed by paramount pictures , " the italian job " was theatrically released in the united states on may 30 , 2003 , and grossed over $ 176 million worldwide .
Head Entity: the italian job
Tail Entity: paramount pictures
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "distributor":
Relation: distributor  
Context: the film was distributed by universal pictures and received critical acclaim upon its release in 2015.  
Head Entity: the film  
Tail Entity: universal pictures  

Relation: distributor  
Context: the album was distributed by sony music and quickly climbed the charts, becoming a bestseller.  
Head Entity: the album  
Tail Entity: sony music  
#### located in or next to body of water ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "located in or next to body of water" means body of water on or next to which a place is located.
Here is an example:
Relation: located in or next to body of water
Context: one account notes , alston and his son , peter also , practiced their counterfeiting operation , at stack island , in the lower mississippi river , about 170 miles upriver from natchez .
Head Entity: stack island
Tail Entity: mississippi river
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "located in or next to body of water":
Relation: located in or next to body of water  
Context: the picturesque village of portsmouth is situated along the banks of the serene thames river, offering stunning views and a tranquil atmosphere for its residents.  
Head Entity: portsmouth  
Tail Entity: thames river  

Relation: located in or next to body of water  
Context: during our trip, we discovered that the charming town of venice is built on a series of islands surrounded by the beautiful adriatic sea, making it a unique destination for tourists.  
Head Entity: venice  
Tail Entity: adriatic sea  
#### subsidiary ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "subsidiary" means subsidiary of a company or organization; generally a fully owned separate corporation.
Here is an example:
Relation: subsidiary
Context: cbs and viacom split again in 2005 , but both are still owned by national amusements .
Head Entity: national amusements
Tail Entity: viacom
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "subsidiary":
Relation: subsidiary  
Context: In 2018, Facebook acquired WhatsApp, making it a subsidiary of the social media giant.  
Head Entity: Facebook  
Tail Entity: WhatsApp  

Relation: subsidiary  
Context: Toyota has several subsidiaries, including Lexus, which focuses on luxury vehicles.  
Head Entity: Toyota  
Tail Entity: Lexus  
#### country of citizenship ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "country of citizenship" means the object is a country that recognizes the subject as its citizen.
Here is an example:
Relation: country of citizenship
Context: he challenged seven other swimmers in heat four including ukraine 's rostyslav svanidze , a top 16 finalist in atlanta four years earlier .
Head Entity: rostyslav svanidze
Tail Entity: ukraine
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "country of citizenship":
Relation: country of citizenship  
Context: after moving to the united states, she became a naturalized citizen and proudly represents her country of birth, nigeria, in international competitions.  
Head Entity: she  
Tail Entity: nigeria  

Relation: country of citizenship  
Context: during the conference, the renowned scientist from canada discussed his research while highlighting the importance of his homeland, india, in shaping his career.  
Head Entity: the renowned scientist  
Tail Entity: india  
#### genre ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "genre" means creative work's genre or an artist's field of work.
Here is an example:
Relation: genre
Context: he is also a part of the atlanta - based hip - hop / r&b;/soul musical collective the dungeon family .
Head Entity: the dungeon family
Tail Entity: hip - hop
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "genre":
Relation: genre  
Context: the band is known for their unique blend of rock and electronic music, often categorized under the genre of synth-pop.  
Head Entity: the band  
Tail Entity: synth-pop  

Relation: genre  
Context: she has made significant contributions to the world of classical music, particularly in the genre of opera.  
Head Entity: she  
Tail Entity: opera  
#### mouth of the watercourse ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "mouth of the watercourse" means the body of water to which the watercourse drains.
Here is an example:
Relation: mouth of the watercourse
Context: san lázaro is 660 km far from asunción and is located at the exact site of the confluence of the apa river with paraguay river , bordering brazil - chaco .
Head Entity: apa river
Tail Entity: paraguay river
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "mouth of the watercourse":
Relation: mouth of the watercourse  
Context: the mississippi river flows into the gulf of mexico, creating a rich delta ecosystem that supports diverse wildlife and plant species.  
Head Entity: mississippi river  
Tail Entity: gulf of mexico  

Relation: mouth of the watercourse  
Context: the rhine river, which runs through several countries in europe, empties into the north sea, providing a vital shipping route for international trade.  
Head Entity: rhine river  
Tail Entity: north sea  
#### position played on team / speciality ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "position played on team / speciality" means position or specialism of a player on a team.
Here is an example:
Relation: position played on team / speciality
Context: bradley halliday ( born 10 july 1995 ) is an english professional footballer who plays as a defender or a midfielder for league two club cambridge united .
Head Entity: bradley halliday
Tail Entity: defender
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "position played on team / speciality":
Relation: position played on team / speciality  
Context: alex morgan ( born 2 july 1989 ) is an american soccer player who plays as a forward for the national team and the club orlando pride in the national women's soccer league.  
Head Entity: alex morgan  
Tail Entity: forward  

Relation: position played on team / speciality  
Context: lebron james ( born 30 december 1984 ) is an american professional basketball player who plays as a small forward for the los angeles lakers in the nba.  
Head Entity: lebron james  
Tail Entity: small forward  
#### sports season of league or competition ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "sports season of league or competition" means property that shows the competition of which the item is a season.
Here is an example:
Relation: sports season of league or competition
Context: the stade louis ii in monaco was the venue for the uefa super cup every year since 1998 .
Head Entity: 1998
Tail Entity: uefa super cup
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "sports season of league or competition":
Relation: sports season of league or competition  
Context: The 2020 NBA season was significantly impacted by the COVID-19 pandemic, leading to a unique playoff format.  
Head Entity: 2020  
Tail Entity: NBA  

Relation: sports season of league or competition  
Context: The 2019 Rugby World Cup took place in Japan, showcasing teams from around the globe.  
Head Entity: 2019  
Tail Entity: Rugby World Cup  
Losses:  4.700750827789307 0.2957882285118103 0.8672362565994263
MemoryTrain:  epoch  0, batch     0 | loss: 4.7007508Losses:  4.27312707901001 1.1403553485870361 0.6820581555366516
MemoryTrain:  epoch  0, batch     1 | loss: 4.2731271Losses:  5.135613441467285 0.7715659737586975 0.8144176602363586
MemoryTrain:  epoch  0, batch     2 | loss: 5.1356134Losses:  5.103328227996826 0.7005023956298828 0.7813663482666016
MemoryTrain:  epoch  0, batch     3 | loss: 5.1033282Losses:  5.551937103271484 0.3426898419857025 0.7668787240982056
MemoryTrain:  epoch  0, batch     4 | loss: 5.5519371Losses:  5.635830879211426 0.21813832223415375 0.5891610980033875
MemoryTrain:  epoch  0, batch     5 | loss: 5.6358309Losses:  5.106429576873779 0.7824138402938843 0.8745325803756714
MemoryTrain:  epoch  1, batch     0 | loss: 5.1064296Losses:  5.7002949714660645 1.1112481355667114 0.8545922636985779
MemoryTrain:  epoch  1, batch     1 | loss: 5.7002950Losses:  4.565896987915039 0.24690070748329163 0.8378178477287292
MemoryTrain:  epoch  1, batch     2 | loss: 4.5658970Losses:  4.491030216217041 0.22866526246070862 0.8353807330131531
MemoryTrain:  epoch  1, batch     3 | loss: 4.4910302Losses:  5.506519794464111 0.8123642206192017 0.7904784083366394
MemoryTrain:  epoch  1, batch     4 | loss: 5.5065198Losses:  2.599658727645874 -0.0 0.6835646033287048
MemoryTrain:  epoch  1, batch     5 | loss: 2.5996587Losses:  4.551712512969971 1.3082313537597656 0.6355963945388794
MemoryTrain:  epoch  2, batch     0 | loss: 4.5517125Losses:  4.240257263183594 0.6423251628875732 0.8095468878746033
MemoryTrain:  epoch  2, batch     1 | loss: 4.2402573Losses:  3.8964149951934814 0.7716411352157593 0.7510706782341003
MemoryTrain:  epoch  2, batch     2 | loss: 3.8964150Losses:  4.798474311828613 0.7535035610198975 0.734555184841156
MemoryTrain:  epoch  2, batch     3 | loss: 4.7984743Losses:  4.492835998535156 0.7743411660194397 0.6916962265968323
MemoryTrain:  epoch  2, batch     4 | loss: 4.4928360Losses:  3.2224953174591064 0.31565386056900024 0.737621009349823
MemoryTrain:  epoch  2, batch     5 | loss: 3.2224953Losses:  4.490157127380371 0.9947112798690796 0.7443677186965942
MemoryTrain:  epoch  3, batch     0 | loss: 4.4901571Losses:  3.6875734329223633 0.5395610928535461 0.765898585319519
MemoryTrain:  epoch  3, batch     1 | loss: 3.6875734Losses:  4.018607139587402 1.1604938507080078 0.8621855974197388
MemoryTrain:  epoch  3, batch     2 | loss: 4.0186071Losses:  4.2824578285217285 0.26543399691581726 0.8773424029350281
MemoryTrain:  epoch  3, batch     3 | loss: 4.2824578Losses:  3.9589004516601562 0.7291576862335205 0.9227487444877625
MemoryTrain:  epoch  3, batch     4 | loss: 3.9589005Losses:  2.728076696395874 -0.0 0.7438828945159912
MemoryTrain:  epoch  3, batch     5 | loss: 2.7280767Losses:  4.128132343292236 0.766299843788147 0.7118776440620422
MemoryTrain:  epoch  4, batch     0 | loss: 4.1281323Losses:  3.681492328643799 0.8044567108154297 0.812203049659729
MemoryTrain:  epoch  4, batch     1 | loss: 3.6814923Losses:  3.4264519214630127 0.5498521327972412 0.7997589707374573
MemoryTrain:  epoch  4, batch     2 | loss: 3.4264519Losses:  3.684037446975708 0.8742728233337402 0.6534867882728577
MemoryTrain:  epoch  4, batch     3 | loss: 3.6840374Losses:  3.149702310562134 0.6044275760650635 0.8436252474784851
MemoryTrain:  epoch  4, batch     4 | loss: 3.1497023Losses:  3.1455984115600586 0.30924510955810547 0.5806001424789429
MemoryTrain:  epoch  4, batch     5 | loss: 3.1455984Losses:  3.7998881340026855 0.5317472219467163 0.7426748275756836
MemoryTrain:  epoch  5, batch     0 | loss: 3.7998881Losses:  2.8769969940185547 0.45084649324417114 0.8304528594017029
MemoryTrain:  epoch  5, batch     1 | loss: 2.8769970Losses:  4.028879165649414 1.098565697669983 0.7448529005050659
MemoryTrain:  epoch  5, batch     2 | loss: 4.0288792Losses:  2.8473758697509766 0.26298409700393677 0.803387463092804
MemoryTrain:  epoch  5, batch     3 | loss: 2.8473759Losses:  3.442399740219116 1.0260182619094849 0.8009269833564758
MemoryTrain:  epoch  5, batch     4 | loss: 3.4423997Losses:  2.5031886100769043 -0.0 0.6093063354492188
MemoryTrain:  epoch  5, batch     5 | loss: 2.5031886Losses:  3.3277223110198975 -0.0 0.7800911068916321
MemoryTrain:  epoch  6, batch     0 | loss: 3.3277223Losses:  3.3380539417266846 0.8057715892791748 0.5627171993255615
MemoryTrain:  epoch  6, batch     1 | loss: 3.3380539Losses:  2.551889419555664 0.28685951232910156 0.8791105151176453
MemoryTrain:  epoch  6, batch     2 | loss: 2.5518894Losses:  2.9330062866210938 0.25600335001945496 0.7991583943367004
MemoryTrain:  epoch  6, batch     3 | loss: 2.9330063Losses:  2.570188045501709 0.25282907485961914 0.914035975933075
MemoryTrain:  epoch  6, batch     4 | loss: 2.5701880Losses:  3.6552484035491943 1.229917049407959 0.5955378413200378
MemoryTrain:  epoch  6, batch     5 | loss: 3.6552484Losses:  3.616450548171997 1.2206392288208008 0.8118219375610352
MemoryTrain:  epoch  7, batch     0 | loss: 3.6164505Losses:  2.759658098220825 -0.0 0.7151922583580017
MemoryTrain:  epoch  7, batch     1 | loss: 2.7596581Losses:  3.3049545288085938 0.5156611204147339 0.8135013580322266
MemoryTrain:  epoch  7, batch     2 | loss: 3.3049545Losses:  3.0176749229431152 0.7215392589569092 0.6582428216934204
MemoryTrain:  epoch  7, batch     3 | loss: 3.0176749Losses:  2.424100637435913 0.2693852186203003 0.7742664217948914
MemoryTrain:  epoch  7, batch     4 | loss: 2.4241006Losses:  2.3778531551361084 0.26792430877685547 0.6016950011253357
MemoryTrain:  epoch  7, batch     5 | loss: 2.3778532Losses:  2.823507785797119 0.5475026369094849 0.7666860222816467
MemoryTrain:  epoch  8, batch     0 | loss: 2.8235078Losses:  2.265958070755005 -0.0 0.7724067568778992
MemoryTrain:  epoch  8, batch     1 | loss: 2.2659581Losses:  3.015815258026123 0.9692132472991943 0.44877681136131287
MemoryTrain:  epoch  8, batch     2 | loss: 3.0158153Losses:  2.6994881629943848 0.46939125657081604 0.8031889200210571
MemoryTrain:  epoch  8, batch     3 | loss: 2.6994882Losses:  2.816427707672119 0.5595736503601074 0.7794340252876282
MemoryTrain:  epoch  8, batch     4 | loss: 2.8164277Losses:  2.5460968017578125 0.5959741473197937 0.5855098962783813
MemoryTrain:  epoch  8, batch     5 | loss: 2.5460968Losses:  2.8796310424804688 0.7626112699508667 0.8623555898666382
MemoryTrain:  epoch  9, batch     0 | loss: 2.8796310Losses:  2.846872329711914 0.5132184624671936 0.8082081079483032
MemoryTrain:  epoch  9, batch     1 | loss: 2.8468723Losses:  3.577562093734741 1.6277215480804443 0.667913019657135
MemoryTrain:  epoch  9, batch     2 | loss: 3.5775621Losses:  2.8179221153259277 0.860713005065918 0.7000041604042053
MemoryTrain:  epoch  9, batch     3 | loss: 2.8179221Losses:  2.546922206878662 0.5402871966362 0.5684049725532532
MemoryTrain:  epoch  9, batch     4 | loss: 2.5469222Losses:  2.4376468658447266 0.3460787534713745 0.6716335415840149
MemoryTrain:  epoch  9, batch     5 | loss: 2.4376469
[EVAL] batch:    0 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    1 | acc: 68.75%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 62.50%,  total acc: 79.69%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 78.75%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 82.29%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 84.82%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 85.16%   [EVAL] batch:    8 | acc: 75.00%,  total acc: 84.03%   [EVAL] batch:    9 | acc: 56.25%,  total acc: 81.25%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 82.29%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 82.21%   [EVAL] batch:   13 | acc: 56.25%,  total acc: 80.36%   [EVAL] batch:   14 | acc: 62.50%,  total acc: 79.17%   [EVAL] batch:   15 | acc: 81.25%,  total acc: 79.30%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 79.04%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 78.47%   [EVAL] batch:   18 | acc: 87.50%,  total acc: 78.95%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 78.75%   [EVAL] batch:   20 | acc: 81.25%,  total acc: 78.87%   [EVAL] batch:   21 | acc: 75.00%,  total acc: 78.69%   [EVAL] batch:   22 | acc: 75.00%,  total acc: 78.53%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 78.91%   [EVAL] batch:   24 | acc: 75.00%,  total acc: 78.75%   [EVAL] batch:   25 | acc: 75.00%,  total acc: 78.61%   [EVAL] batch:   26 | acc: 68.75%,  total acc: 78.24%   [EVAL] batch:   27 | acc: 75.00%,  total acc: 78.12%   [EVAL] batch:   28 | acc: 81.25%,  total acc: 78.23%   [EVAL] batch:   29 | acc: 43.75%,  total acc: 77.08%   [EVAL] batch:   30 | acc: 56.25%,  total acc: 76.41%   [EVAL] batch:   31 | acc: 62.50%,  total acc: 75.98%   [EVAL] batch:   32 | acc: 68.75%,  total acc: 75.76%   [EVAL] batch:   33 | acc: 68.75%,  total acc: 75.55%   [EVAL] batch:   34 | acc: 62.50%,  total acc: 75.18%   [EVAL] batch:   35 | acc: 62.50%,  total acc: 74.83%   [EVAL] batch:   36 | acc: 62.50%,  total acc: 74.49%   [EVAL] batch:   37 | acc: 50.00%,  total acc: 73.85%   [EVAL] batch:   38 | acc: 62.50%,  total acc: 73.56%   [EVAL] batch:   39 | acc: 68.75%,  total acc: 73.44%   [EVAL] batch:   40 | acc: 62.50%,  total acc: 73.17%   [EVAL] batch:   41 | acc: 56.25%,  total acc: 72.77%   [EVAL] batch:   42 | acc: 62.50%,  total acc: 72.53%   [EVAL] batch:   43 | acc: 31.25%,  total acc: 71.59%   [EVAL] batch:   44 | acc: 25.00%,  total acc: 70.56%   [EVAL] batch:   45 | acc: 31.25%,  total acc: 69.70%   [EVAL] batch:   46 | acc: 31.25%,  total acc: 68.88%   [EVAL] batch:   47 | acc: 37.50%,  total acc: 68.23%   [EVAL] batch:   48 | acc: 25.00%,  total acc: 67.35%   [EVAL] batch:   49 | acc: 50.00%,  total acc: 67.00%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 67.65%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 68.27%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 68.87%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 69.44%   [EVAL] batch:   54 | acc: 100.00%,  total acc: 70.00%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 70.54%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 71.05%   [EVAL] batch:   57 | acc: 100.00%,  total acc: 71.55%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 71.82%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 72.19%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 72.64%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 72.98%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 72.62%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 83.33%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 83.93%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 85.16%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 86.11%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 86.88%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 86.93%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 88.02%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 87.98%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 88.39%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 88.75%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 89.45%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 89.71%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 89.93%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 90.46%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 90.94%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 91.37%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 90.91%   [EVAL] batch:   22 | acc: 87.50%,  total acc: 90.76%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 90.62%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 90.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 90.87%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 91.20%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 91.52%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 91.81%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 92.08%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 92.34%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 92.58%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 92.80%   [EVAL] batch:   33 | acc: 68.75%,  total acc: 92.10%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 91.61%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:   36 | acc: 87.50%,  total acc: 91.55%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 91.61%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 91.83%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 92.03%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 92.23%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 92.41%   [EVAL] batch:   42 | acc: 87.50%,  total acc: 92.30%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 92.33%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 92.22%   [EVAL] batch:   45 | acc: 87.50%,  total acc: 92.12%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 91.89%   [EVAL] batch:   47 | acc: 81.25%,  total acc: 91.67%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 91.84%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 91.75%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 91.79%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 91.71%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 91.75%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 91.90%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 91.70%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 91.74%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 91.78%   [EVAL] batch:   57 | acc: 75.00%,  total acc: 91.49%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 91.63%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 91.80%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 91.73%   [EVAL] batch:   62 | acc: 81.25%,  total acc: 91.57%   [EVAL] batch:   63 | acc: 81.25%,  total acc: 91.41%   [EVAL] batch:   64 | acc: 68.75%,  total acc: 91.06%   [EVAL] batch:   65 | acc: 81.25%,  total acc: 90.91%   [EVAL] batch:   66 | acc: 75.00%,  total acc: 90.67%   [EVAL] batch:   67 | acc: 87.50%,  total acc: 90.62%   [EVAL] batch:   68 | acc: 81.25%,  total acc: 90.49%   [EVAL] batch:   69 | acc: 87.50%,  total acc: 90.45%   [EVAL] batch:   70 | acc: 68.75%,  total acc: 90.14%   [EVAL] batch:   71 | acc: 62.50%,  total acc: 89.76%   [EVAL] batch:   72 | acc: 68.75%,  total acc: 89.47%   [EVAL] batch:   73 | acc: 87.50%,  total acc: 89.44%   [EVAL] batch:   74 | acc: 81.25%,  total acc: 89.33%   [EVAL] batch:   75 | acc: 87.50%,  total acc: 89.31%   [EVAL] batch:   76 | acc: 87.50%,  total acc: 89.29%   [EVAL] batch:   77 | acc: 75.00%,  total acc: 89.10%   [EVAL] batch:   78 | acc: 87.50%,  total acc: 89.08%   [EVAL] batch:   79 | acc: 75.00%,  total acc: 88.91%   [EVAL] batch:   80 | acc: 100.00%,  total acc: 89.04%   [EVAL] batch:   81 | acc: 43.75%,  total acc: 88.49%   [EVAL] batch:   82 | acc: 68.75%,  total acc: 88.25%   [EVAL] batch:   83 | acc: 93.75%,  total acc: 88.32%   [EVAL] batch:   84 | acc: 68.75%,  total acc: 88.09%   [EVAL] batch:   85 | acc: 87.50%,  total acc: 88.08%   [EVAL] batch:   86 | acc: 87.50%,  total acc: 88.07%   [EVAL] batch:   87 | acc: 81.25%,  total acc: 88.00%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 88.13%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 88.26%   [EVAL] batch:   90 | acc: 100.00%,  total acc: 88.39%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 88.52%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 88.64%   [EVAL] batch:   93 | acc: 93.75%,  total acc: 88.70%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 88.82%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 88.93%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 89.05%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 89.16%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 89.27%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 89.38%   [EVAL] batch:  100 | acc: 50.00%,  total acc: 88.99%   [EVAL] batch:  101 | acc: 62.50%,  total acc: 88.73%   [EVAL] batch:  102 | acc: 62.50%,  total acc: 88.47%   [EVAL] batch:  103 | acc: 56.25%,  total acc: 88.16%   [EVAL] batch:  104 | acc: 87.50%,  total acc: 88.15%   [EVAL] batch:  105 | acc: 62.50%,  total acc: 87.91%   [EVAL] batch:  106 | acc: 62.50%,  total acc: 87.68%   [EVAL] batch:  107 | acc: 37.50%,  total acc: 87.21%   [EVAL] batch:  108 | acc: 62.50%,  total acc: 86.98%   [EVAL] batch:  109 | acc: 43.75%,  total acc: 86.59%   [EVAL] batch:  110 | acc: 62.50%,  total acc: 86.37%   [EVAL] batch:  111 | acc: 62.50%,  total acc: 86.16%   [EVAL] batch:  112 | acc: 68.75%,  total acc: 86.01%   [EVAL] batch:  113 | acc: 87.50%,  total acc: 86.02%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 86.14%   [EVAL] batch:  115 | acc: 81.25%,  total acc: 86.10%   [EVAL] batch:  116 | acc: 87.50%,  total acc: 86.11%   [EVAL] batch:  117 | acc: 87.50%,  total acc: 86.12%   [EVAL] batch:  118 | acc: 81.25%,  total acc: 86.08%   [EVAL] batch:  119 | acc: 31.25%,  total acc: 85.62%   [EVAL] batch:  120 | acc: 50.00%,  total acc: 85.33%   [EVAL] batch:  121 | acc: 18.75%,  total acc: 84.78%   [EVAL] batch:  122 | acc: 37.50%,  total acc: 84.40%   [EVAL] batch:  123 | acc: 25.00%,  total acc: 83.92%   [EVAL] batch:  124 | acc: 43.75%,  total acc: 83.60%   [EVAL] batch:  125 | acc: 100.00%,  total acc: 83.73%   [EVAL] batch:  126 | acc: 68.75%,  total acc: 83.61%   [EVAL] batch:  127 | acc: 87.50%,  total acc: 83.64%   [EVAL] batch:  128 | acc: 62.50%,  total acc: 83.48%   [EVAL] batch:  129 | acc: 75.00%,  total acc: 83.41%   [EVAL] batch:  130 | acc: 100.00%,  total acc: 83.54%   [EVAL] batch:  131 | acc: 100.00%,  total acc: 83.66%   [EVAL] batch:  132 | acc: 87.50%,  total acc: 83.69%   [EVAL] batch:  133 | acc: 75.00%,  total acc: 83.63%   [EVAL] batch:  134 | acc: 56.25%,  total acc: 83.43%   [EVAL] batch:  135 | acc: 81.25%,  total acc: 83.41%   [EVAL] batch:  136 | acc: 93.75%,  total acc: 83.49%   [EVAL] batch:  137 | acc: 81.25%,  total acc: 83.47%   [EVAL] batch:  138 | acc: 56.25%,  total acc: 83.27%   [EVAL] batch:  139 | acc: 62.50%,  total acc: 83.12%   [EVAL] batch:  140 | acc: 81.25%,  total acc: 83.11%   [EVAL] batch:  141 | acc: 75.00%,  total acc: 83.05%   [EVAL] batch:  142 | acc: 68.75%,  total acc: 82.95%   [EVAL] batch:  143 | acc: 87.50%,  total acc: 82.99%   [EVAL] batch:  144 | acc: 75.00%,  total acc: 82.93%   [EVAL] batch:  145 | acc: 81.25%,  total acc: 82.92%   [EVAL] batch:  146 | acc: 75.00%,  total acc: 82.87%   [EVAL] batch:  147 | acc: 75.00%,  total acc: 82.81%   [EVAL] batch:  148 | acc: 87.50%,  total acc: 82.84%   [EVAL] batch:  149 | acc: 75.00%,  total acc: 82.79%   [EVAL] batch:  150 | acc: 75.00%,  total acc: 82.74%   [EVAL] batch:  151 | acc: 68.75%,  total acc: 82.65%   [EVAL] batch:  152 | acc: 75.00%,  total acc: 82.60%   [EVAL] batch:  153 | acc: 81.25%,  total acc: 82.59%   [EVAL] batch:  154 | acc: 43.75%,  total acc: 82.34%   [EVAL] batch:  155 | acc: 56.25%,  total acc: 82.17%   [EVAL] batch:  156 | acc: 62.50%,  total acc: 82.05%   [EVAL] batch:  157 | acc: 68.75%,  total acc: 81.96%   [EVAL] batch:  158 | acc: 68.75%,  total acc: 81.88%   [EVAL] batch:  159 | acc: 62.50%,  total acc: 81.76%   [EVAL] batch:  160 | acc: 62.50%,  total acc: 81.64%   [EVAL] batch:  161 | acc: 62.50%,  total acc: 81.52%   [EVAL] batch:  162 | acc: 50.00%,  total acc: 81.33%   [EVAL] batch:  163 | acc: 62.50%,  total acc: 81.21%   [EVAL] batch:  164 | acc: 68.75%,  total acc: 81.14%   [EVAL] batch:  165 | acc: 62.50%,  total acc: 81.02%   [EVAL] batch:  166 | acc: 56.25%,  total acc: 80.88%   [EVAL] batch:  167 | acc: 62.50%,  total acc: 80.77%   [EVAL] batch:  168 | acc: 31.25%,  total acc: 80.47%   [EVAL] batch:  169 | acc: 25.00%,  total acc: 80.15%   [EVAL] batch:  170 | acc: 31.25%,  total acc: 79.86%   [EVAL] batch:  171 | acc: 31.25%,  total acc: 79.58%   [EVAL] batch:  172 | acc: 37.50%,  total acc: 79.34%   [EVAL] batch:  173 | acc: 25.00%,  total acc: 79.02%   [EVAL] batch:  174 | acc: 50.00%,  total acc: 78.86%   [EVAL] batch:  175 | acc: 100.00%,  total acc: 78.98%   [EVAL] batch:  176 | acc: 100.00%,  total acc: 79.10%   [EVAL] batch:  177 | acc: 100.00%,  total acc: 79.21%   [EVAL] batch:  178 | acc: 100.00%,  total acc: 79.33%   [EVAL] batch:  179 | acc: 100.00%,  total acc: 79.44%   [EVAL] batch:  180 | acc: 100.00%,  total acc: 79.56%   [EVAL] batch:  181 | acc: 100.00%,  total acc: 79.67%   [EVAL] batch:  182 | acc: 100.00%,  total acc: 79.78%   [EVAL] batch:  183 | acc: 87.50%,  total acc: 79.82%   [EVAL] batch:  184 | acc: 93.75%,  total acc: 79.90%   [EVAL] batch:  185 | acc: 100.00%,  total acc: 80.01%   [EVAL] batch:  186 | acc: 93.75%,  total acc: 80.08%   [EVAL] batch:  187 | acc: 50.00%,  total acc: 79.92%   
cur_acc:  ['0.9286', '0.7827', '0.7262']
his_acc:  ['0.9286', '0.8585', '0.7992']
Clustering into  19  clusters
Clusters:  [ 3  4  7  2  0  0  7 10  4  6  4  1 10 17  9 15 14 13  2 13 18  8 10  0
 16  6  3  0  5  5 11  5 13 12 12  5 13  1  6  1]
Losses:  12.127189636230469 5.410531997680664 0.5106521844863892
CurrentTrain: epoch  0, batch     0 | loss: 12.1271896Losses:  10.977985382080078 4.307652473449707 0.577715277671814
CurrentTrain: epoch  0, batch     1 | loss: 10.9779854Losses:  11.166949272155762 3.8629345893859863 0.5064398050308228
CurrentTrain: epoch  0, batch     2 | loss: 11.1669493Losses:  8.690816879272461 -0.0 0.08834214508533478
CurrentTrain: epoch  0, batch     3 | loss: 8.6908169Losses:  13.172738075256348 6.3801116943359375 0.5713323354721069
CurrentTrain: epoch  1, batch     0 | loss: 13.1727381Losses:  9.725239753723145 3.4012978076934814 0.561659038066864
CurrentTrain: epoch  1, batch     1 | loss: 9.7252398Losses:  9.730157852172852 4.2218146324157715 0.4778599739074707
CurrentTrain: epoch  1, batch     2 | loss: 9.7301579Losses:  4.346578598022461 -0.0 0.15891286730766296
CurrentTrain: epoch  1, batch     3 | loss: 4.3465786Losses:  9.012653350830078 2.626969337463379 0.5954723358154297
CurrentTrain: epoch  2, batch     0 | loss: 9.0126534Losses:  8.110513687133789 2.8490800857543945 0.5520109534263611
CurrentTrain: epoch  2, batch     1 | loss: 8.1105137Losses:  8.931299209594727 4.066068172454834 0.47963497042655945
CurrentTrain: epoch  2, batch     2 | loss: 8.9312992Losses:  3.9926679134368896 -0.0 0.09810715913772583
CurrentTrain: epoch  2, batch     3 | loss: 3.9926679Losses:  8.380704879760742 3.0820538997650146 0.5392392873764038
CurrentTrain: epoch  3, batch     0 | loss: 8.3807049Losses:  7.695685386657715 2.8119544982910156 0.5309288501739502
CurrentTrain: epoch  3, batch     1 | loss: 7.6956854Losses:  7.297448635101318 2.4286952018737793 0.5606767535209656
CurrentTrain: epoch  3, batch     2 | loss: 7.2974486Losses:  4.7482099533081055 -0.0 0.12583425641059875
CurrentTrain: epoch  3, batch     3 | loss: 4.7482100Losses:  9.084787368774414 4.0896148681640625 0.5431351661682129
CurrentTrain: epoch  4, batch     0 | loss: 9.0847874Losses:  7.7380194664001465 3.1626834869384766 0.5381456613540649
CurrentTrain: epoch  4, batch     1 | loss: 7.7380195Losses:  8.356828689575195 4.450985908508301 0.457881897687912
CurrentTrain: epoch  4, batch     2 | loss: 8.3568287Losses:  2.1921298503875732 -0.0 0.09600543975830078
CurrentTrain: epoch  4, batch     3 | loss: 2.1921299Losses:  7.581175327301025 3.944139242172241 0.37530985474586487
CurrentTrain: epoch  5, batch     0 | loss: 7.5811753Losses:  8.038785934448242 4.099432468414307 0.4779296815395355
CurrentTrain: epoch  5, batch     1 | loss: 8.0387859Losses:  10.068315505981445 5.6529693603515625 0.4251735806465149
CurrentTrain: epoch  5, batch     2 | loss: 10.0683155Losses:  2.7522377967834473 -0.0 0.12756487727165222
CurrentTrain: epoch  5, batch     3 | loss: 2.7522378Losses:  7.182239055633545 3.648291826248169 0.5473372340202332
CurrentTrain: epoch  6, batch     0 | loss: 7.1822391Losses:  7.852311611175537 3.771476984024048 0.539908766746521
CurrentTrain: epoch  6, batch     1 | loss: 7.8523116Losses:  7.025138854980469 2.8370959758758545 0.5106444358825684
CurrentTrain: epoch  6, batch     2 | loss: 7.0251389Losses:  2.8021082878112793 -0.0 0.1172172948718071
CurrentTrain: epoch  6, batch     3 | loss: 2.8021083Losses:  7.440588474273682 3.7365808486938477 0.4162314832210541
CurrentTrain: epoch  7, batch     0 | loss: 7.4405885Losses:  7.512640953063965 3.9492599964141846 0.5197752714157104
CurrentTrain: epoch  7, batch     1 | loss: 7.5126410Losses:  6.927448272705078 3.231800079345703 0.510065495967865
CurrentTrain: epoch  7, batch     2 | loss: 6.9274483Losses:  1.8684022426605225 -0.0 0.11605311185121536
CurrentTrain: epoch  7, batch     3 | loss: 1.8684022Losses:  6.633472919464111 3.3283965587615967 0.5302873253822327
CurrentTrain: epoch  8, batch     0 | loss: 6.6334729Losses:  5.836593151092529 2.5727248191833496 0.5062679052352905
CurrentTrain: epoch  8, batch     1 | loss: 5.8365932Losses:  6.160961151123047 2.659778118133545 0.5092270374298096
CurrentTrain: epoch  8, batch     2 | loss: 6.1609612Losses:  1.89528489112854 -0.0 -0.0
CurrentTrain: epoch  8, batch     3 | loss: 1.8952849Losses:  6.668337821960449 3.5419445037841797 0.41382884979248047
CurrentTrain: epoch  9, batch     0 | loss: 6.6683378Losses:  5.266017436981201 2.1011650562286377 0.5050625801086426
CurrentTrain: epoch  9, batch     1 | loss: 5.2660174Losses:  5.894552230834961 2.9947590827941895 0.5037567615509033
CurrentTrain: epoch  9, batch     2 | loss: 5.8945522Losses:  1.9734179973602295 -0.0 0.0854959785938263
CurrentTrain: epoch  9, batch     3 | loss: 1.9734180
#### follows ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "follows" means immediately prior item in a series of which the subject is a part.
Here is an example:
Relation: follows
Context: ( 1988 ) , " night watch " ( 2004 ) , " day watch " ( 2006 ) and " legend № 17 " ( 2013 ) .
Head Entity: day watch
Tail Entity: night watch
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "follows":
Relation: follows  
Context: ( 1995 ) , " The Phantom Menace " ( 1999 ) , " Attack of the Clones " ( 2002 ) and " Revenge of the Sith " ( 2005 ) .  
Head Entity: Attack of the Clones  
Tail Entity: The Phantom Menace  

Relation: follows  
Context: ( 2010 ) , " The Hunger Games " ( 2012 ) , " Catching Fire " ( 2013 ) and " Mockingjay " ( 2014 ) .  
Head Entity: Catching Fire  
Tail Entity: The Hunger Games  
#### sport ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "sport" means sport that the subject participates or participated in or is associated with.
Here is an example:
Relation: sport
Context: it is primarily used for baseball and serves as the home stadium for yaquis de obregón .
Head Entity: yaquis de obregón
Tail Entity: baseball
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "sport":
Relation: sport  
Context: She has been a professional player in the world of tennis, winning several championships throughout her career.  
Head Entity: She  
Tail Entity: tennis  

Relation: sport  
Context: The city is known for its vibrant soccer culture, with numerous teams competing in the national league.  
Head Entity: The city  
Tail Entity: soccer  
#### father ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "father" means male parent of the subject.
Here is an example:
Relation: father
Context: after the crowning of his nephew radoslav , the son of stefan , sava left the serbian maritime in 1229 for a trip to palestine .
Head Entity: radoslav
Tail Entity: stefan
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "father":
Relation: father  
Context: in the historical records, it is noted that king henry was the father of prince edward, who later became a significant figure in the kingdom.  
Head Entity: prince edward  
Tail Entity: king henry  

Relation: father  
Context: during the family reunion, it was revealed that michael was the father of two daughters, emily and sarah, who both excelled in their studies.  
Head Entity: michael  
Tail Entity: emily  
#### original network ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "original network" means network or service that originally broadcast a radio or television program.
Here is an example:
Relation: original network
Context: since 2009 , culshaw has starred in the bbc one comedy sketch show " the impressions show " alongside debra stephenson .
Head Entity: the impressions show
Tail Entity: bbc one
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "original network":
Relation: original network  
Context: The popular series "Breaking Bad" first aired on AMC, captivating audiences with its intense storytelling and character development.  
Head Entity: Breaking Bad  
Tail Entity: AMC  

Relation: original network  
Context: "Friends" became a cultural phenomenon after it premiered on NBC, showcasing the lives of six friends living in New York City.  
Head Entity: Friends  
Tail Entity: NBC  
#### original language of film or TV show ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "original language of film or TV show" means language in which a film or a performance work was originally created.
Here is an example:
Relation: original language of film or TV show
Context: that same year saran made a special appearance in a scene in the kannada film " arasu " .
Head Entity: arasu
Tail Entity: kannada
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "original language of film or TV show":
Relation: original language of film or TV show  
Context: The critically acclaimed movie "Parasite" was primarily filmed in Korean and received international recognition.  
Head Entity: Parasite  
Tail Entity: Korean  

Relation: original language of film or TV show  
Context: The animated series "Doraemon" is originally produced in Japanese and has been dubbed in several languages worldwide.  
Head Entity: Doraemon  
Tail Entity: Japanese  
#### league ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "league" means league in which team or player plays or has played in.
Here is an example:
Relation: league
Context: in the summer of 2013 he returned to montenegro and joined with fk rudar pljevlja , playing with the club for two seasons in the montenegrin first league .
Head Entity: fk rudar pljevlja
Tail Entity: montenegrin first league
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "league":
Relation: league  
Context: After a successful stint in the English Premier League, the player signed with FC Barcelona, competing in La Liga for the next three seasons.  
Head Entity: FC Barcelona  
Tail Entity: La Liga  

Relation: league  
Context: The young athlete made headlines when he joined the New York Yankees, showcasing his talent in Major League Baseball.  
Head Entity: New York Yankees  
Tail Entity: Major League Baseball  
#### mother ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "mother" means female parent of the subject.
Here is an example:
Relation: mother
Context: other teammates included dominique leray and élisabeth riffiod , whose son boris diaw currently plays in the nba for the spurs ( 2014 ) .
Head Entity: boris diaw
Tail Entity: élisabeth riffiod
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "mother":
Relation: mother  
Context: in a recent interview, actress jennifer garner spoke fondly of her children, including violet, who is the daughter of her ex-husband, ben affleck.  
Head Entity: violet  
Tail Entity: jennifer garner  

Relation: mother  
Context: during the family reunion, it was heartwarming to see how much emily cherishes her time with her mother, who has always been her biggest supporter.  
Head Entity: emily  
Tail Entity: emily's mother  
#### instrument ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "instrument" means musical instrument that a person plays or teaches or used in a music occupation.
Here is an example:
Relation: instrument
Context: peter bastian ( 25 august 1943 in copenhagen – 27 march 2017 ) was a danish musician . peter bastian bio and cv he was educated in theoretical physics and classical bassoon .
Head Entity: peter bastian
Tail Entity: bassoon
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "instrument":
Relation: instrument  
Context: johann sebastian bach (31 march 1685 – 28 july 1750) was a german composer and musician of the baroque period. he was known for his exceptional skill in playing the organ and the harpsichord.  
Head Entity: johann sebastian bach  
Tail Entity: organ  

Relation: instrument  
Context: yo-yo ma, born on october 7, 1955, is a world-renowned cellist who has performed with numerous orchestras and in various genres. he is particularly famous for his interpretations of classical cello concertos.  
Head Entity: yo-yo ma  
Tail Entity: cello  
#### country ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "country" means sovereign state of this item (not to be used for human beings).
Here is an example:
Relation: country
Context: ploegsteert is a village in belgium located in the municipality of comines - warneton in the hainaut province and is the most westerly settlement of the walloon region .
Head Entity: hainaut
Tail Entity: belgium
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "country":
Relation: country  
Context: the city of prague is the capital of the czech republic and is known for its beautiful architecture and rich history.  
Head Entity: prague  
Tail Entity: czech republic  

Relation: country  
Context: the great barrier reef is located off the coast of australia and is the largest coral reef system in the world.  
Head Entity: great barrier reef  
Tail Entity: australia  
#### characters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "characters" means characters which appear in this item (like plays, operas, operettas, books, comics, films, TV series, video games).
Here is an example:
Relation: characters
Context: klaus kinski , who would play dracula himself nine years later in " nosferatu the vampyre " , is also featured in the film as renfield .
Head Entity: nosferatu the vampyre
Tail Entity: renfield
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "characters":
Relation: characters  
Context: in the animated series "avatar: the last airbender," zuko is a complex character who struggles with his identity and family legacy.  
Head Entity: avatar: the last airbender  
Tail Entity: zuko  

Relation: characters  
Context: the novel "pride and prejudice" features elizabeth bennet, a strong-willed protagonist who navigates societal expectations and personal relationships.  
Head Entity: pride and prejudice  
Tail Entity: elizabeth bennet  
Losses:  4.632524490356445 1.032442331314087 0.6515425443649292
MemoryTrain:  epoch  0, batch     0 | loss: 4.6325245Losses:  3.357771873474121 0.23347198963165283 0.8189334869384766
MemoryTrain:  epoch  0, batch     1 | loss: 3.3577719Losses:  3.662501811981201 0.21783287823200226 0.8507460951805115
MemoryTrain:  epoch  0, batch     2 | loss: 3.6625018Losses:  4.766790866851807 1.0393468141555786 0.7977876663208008
MemoryTrain:  epoch  0, batch     3 | loss: 4.7667909Losses:  4.261679172515869 0.529800295829773 0.8733773827552795
MemoryTrain:  epoch  0, batch     4 | loss: 4.2616792Losses:  4.7064714431762695 0.488547682762146 0.9169292449951172
MemoryTrain:  epoch  0, batch     5 | loss: 4.7064714Losses:  4.954232215881348 0.3910340666770935 0.9618595242500305
MemoryTrain:  epoch  0, batch     6 | loss: 4.9542322Losses:  3.496089458465576 -0.0 0.718241810798645
MemoryTrain:  epoch  0, batch     7 | loss: 3.4960895Losses:  3.4657230377197266 0.24535654485225677 0.7037590146064758
MemoryTrain:  epoch  1, batch     0 | loss: 3.4657230Losses:  5.394352912902832 1.3075119256973267 0.9188770651817322
MemoryTrain:  epoch  1, batch     1 | loss: 5.3943529Losses:  3.2435994148254395 0.23120221495628357 0.9926429986953735
MemoryTrain:  epoch  1, batch     2 | loss: 3.2435994Losses:  3.6456518173217773 0.8191622495651245 0.9366792440414429
MemoryTrain:  epoch  1, batch     3 | loss: 3.6456518Losses:  5.1971306800842285 1.3951703310012817 0.8077818155288696
MemoryTrain:  epoch  1, batch     4 | loss: 5.1971307Losses:  3.88982892036438 -0.0 0.7795724272727966
MemoryTrain:  epoch  1, batch     5 | loss: 3.8898289Losses:  3.480307102203369 0.5375127792358398 0.7203123569488525
MemoryTrain:  epoch  1, batch     6 | loss: 3.4803071Losses:  3.424143075942993 0.7032291889190674 0.4762917459011078
MemoryTrain:  epoch  1, batch     7 | loss: 3.4241431Losses:  3.3217077255249023 0.3138188123703003 0.9520673751831055
MemoryTrain:  epoch  2, batch     0 | loss: 3.3217077Losses:  3.1144723892211914 0.27678823471069336 0.9663318395614624
MemoryTrain:  epoch  2, batch     1 | loss: 3.1144724Losses:  3.7813963890075684 0.44863244891166687 0.8530547618865967
MemoryTrain:  epoch  2, batch     2 | loss: 3.7813964Losses:  3.1686630249023438 0.263979434967041 0.974887490272522
MemoryTrain:  epoch  2, batch     3 | loss: 3.1686630Losses:  3.6302762031555176 0.5738610029220581 0.6749266386032104
MemoryTrain:  epoch  2, batch     4 | loss: 3.6302762Losses:  3.6776928901672363 0.758054256439209 0.796634316444397
MemoryTrain:  epoch  2, batch     5 | loss: 3.6776929Losses:  3.3865163326263428 0.468280166387558 0.8774457573890686
MemoryTrain:  epoch  2, batch     6 | loss: 3.3865163Losses:  3.3227992057800293 -0.0 0.6385235786437988
MemoryTrain:  epoch  2, batch     7 | loss: 3.3227992Losses:  3.322230577468872 0.7960950136184692 0.8521208763122559
MemoryTrain:  epoch  3, batch     0 | loss: 3.3222306Losses:  3.033038377761841 -0.0 0.9399344325065613
MemoryTrain:  epoch  3, batch     1 | loss: 3.0330384Losses:  4.0027265548706055 1.162969708442688 0.7964456081390381
MemoryTrain:  epoch  3, batch     2 | loss: 4.0027266Losses:  3.207667589187622 0.570480465888977 0.6745632290840149
MemoryTrain:  epoch  3, batch     3 | loss: 3.2076676Losses:  2.443552017211914 -0.0 0.9138073325157166
MemoryTrain:  epoch  3, batch     4 | loss: 2.4435520Losses:  3.282524347305298 0.2232828587293625 0.7476617693901062
MemoryTrain:  epoch  3, batch     5 | loss: 3.2825243Losses:  3.2491250038146973 0.7920052409172058 0.868310809135437
MemoryTrain:  epoch  3, batch     6 | loss: 3.2491250Losses:  2.3825058937072754 -0.0 0.5375235080718994
MemoryTrain:  epoch  3, batch     7 | loss: 2.3825059Losses:  2.847121000289917 0.491180956363678 0.7039995789527893
MemoryTrain:  epoch  4, batch     0 | loss: 2.8471210Losses:  2.982105255126953 0.2769518494606018 0.9580819606781006
MemoryTrain:  epoch  4, batch     1 | loss: 2.9821053Losses:  3.2646656036376953 0.252027690410614 0.8689752221107483
MemoryTrain:  epoch  4, batch     2 | loss: 3.2646656Losses:  2.5438108444213867 0.21765871345996857 0.879897952079773
MemoryTrain:  epoch  4, batch     3 | loss: 2.5438108Losses:  3.6580281257629395 0.27072447538375854 0.8948155641555786
MemoryTrain:  epoch  4, batch     4 | loss: 3.6580281Losses:  2.7574710845947266 0.5137381553649902 0.724189281463623
MemoryTrain:  epoch  4, batch     5 | loss: 2.7574711Losses:  2.8054676055908203 0.7713631391525269 0.7136354446411133
MemoryTrain:  epoch  4, batch     6 | loss: 2.8054676Losses:  2.916600465774536 0.2738398313522339 0.6229847073554993
MemoryTrain:  epoch  4, batch     7 | loss: 2.9166005Losses:  2.832498550415039 0.47534939646720886 0.7385832667350769
MemoryTrain:  epoch  5, batch     0 | loss: 2.8324986Losses:  2.6547465324401855 0.4459814429283142 0.8949376344680786
MemoryTrain:  epoch  5, batch     1 | loss: 2.6547465Losses:  3.483774185180664 1.1195780038833618 0.8485783934593201
MemoryTrain:  epoch  5, batch     2 | loss: 3.4837742Losses:  2.742258310317993 0.5062530040740967 0.810204029083252
MemoryTrain:  epoch  5, batch     3 | loss: 2.7422583Losses:  2.88091778755188 0.2454361915588379 0.907078742980957
MemoryTrain:  epoch  5, batch     4 | loss: 2.8809178Losses:  2.972888469696045 0.47860637307167053 0.828996479511261
MemoryTrain:  epoch  5, batch     5 | loss: 2.9728885Losses:  3.821894645690918 1.2207114696502686 0.7137331962585449
MemoryTrain:  epoch  5, batch     6 | loss: 3.8218946Losses:  2.9067208766937256 -0.0 0.5115459561347961
MemoryTrain:  epoch  5, batch     7 | loss: 2.9067209Losses:  3.9522461891174316 1.7384792566299438 0.595443844795227
MemoryTrain:  epoch  6, batch     0 | loss: 3.9522462Losses:  2.6651194095611572 0.4951273202896118 0.8046886324882507
MemoryTrain:  epoch  6, batch     1 | loss: 2.6651194Losses:  2.9193599224090576 0.4774141311645508 0.7415409088134766
MemoryTrain:  epoch  6, batch     2 | loss: 2.9193599Losses:  2.536144495010376 0.22984880208969116 0.8837811350822449
MemoryTrain:  epoch  6, batch     3 | loss: 2.5361445Losses:  3.155264377593994 0.32918936014175415 0.9230256080627441
MemoryTrain:  epoch  6, batch     4 | loss: 3.1552644Losses:  2.4985904693603516 0.2308638095855713 0.9641880989074707
MemoryTrain:  epoch  6, batch     5 | loss: 2.4985905Losses:  2.743518114089966 0.5313912630081177 0.7421652674674988
MemoryTrain:  epoch  6, batch     6 | loss: 2.7435181Losses:  2.0102109909057617 -0.0 0.6081030964851379
MemoryTrain:  epoch  6, batch     7 | loss: 2.0102110Losses:  3.179978609085083 0.7688583731651306 0.8363051414489746
MemoryTrain:  epoch  7, batch     0 | loss: 3.1799786Losses:  2.5382344722747803 0.457838237285614 0.5401127934455872
MemoryTrain:  epoch  7, batch     1 | loss: 2.5382345Losses:  3.149604320526123 0.7644459009170532 0.8926358222961426
MemoryTrain:  epoch  7, batch     2 | loss: 3.1496043Losses:  2.5994057655334473 0.25963014364242554 0.7250587940216064
MemoryTrain:  epoch  7, batch     3 | loss: 2.5994058Losses:  2.567629814147949 0.2644030451774597 0.9736003279685974
MemoryTrain:  epoch  7, batch     4 | loss: 2.5676298Losses:  2.434194564819336 0.2608702778816223 0.8746403455734253
MemoryTrain:  epoch  7, batch     5 | loss: 2.4341946Losses:  2.6152126789093018 0.22055964171886444 0.9185458421707153
MemoryTrain:  epoch  7, batch     6 | loss: 2.6152127Losses:  2.147521734237671 -0.0 0.6010043621063232
MemoryTrain:  epoch  7, batch     7 | loss: 2.1475217Losses:  2.7824466228485107 0.7458271980285645 0.5987958312034607
MemoryTrain:  epoch  8, batch     0 | loss: 2.7824466Losses:  2.485729455947876 0.20997396111488342 0.7783038020133972
MemoryTrain:  epoch  8, batch     1 | loss: 2.4857295Losses:  3.1221933364868164 0.7727776169776917 0.8570634722709656
MemoryTrain:  epoch  8, batch     2 | loss: 3.1221933Losses:  2.4273977279663086 -0.0 0.956153154373169
MemoryTrain:  epoch  8, batch     3 | loss: 2.4273977Losses:  2.343839645385742 0.21418316662311554 0.7715346217155457
MemoryTrain:  epoch  8, batch     4 | loss: 2.3438396Losses:  2.7720279693603516 0.7572541236877441 0.6558065414428711
MemoryTrain:  epoch  8, batch     5 | loss: 2.7720280Losses:  2.5481324195861816 0.26458343863487244 0.8711415529251099
MemoryTrain:  epoch  8, batch     6 | loss: 2.5481324Losses:  1.918426513671875 -0.0 0.6015647053718567
MemoryTrain:  epoch  8, batch     7 | loss: 1.9184265Losses:  2.4179177284240723 0.22276827692985535 0.8520182967185974
MemoryTrain:  epoch  9, batch     0 | loss: 2.4179177Losses:  2.95399808883667 0.7618400454521179 0.875458300113678
MemoryTrain:  epoch  9, batch     1 | loss: 2.9539981Losses:  2.448869228363037 0.2689293324947357 0.8422136306762695
MemoryTrain:  epoch  9, batch     2 | loss: 2.4488692Losses:  2.826991319656372 0.6983240842819214 0.7663605809211731
MemoryTrain:  epoch  9, batch     3 | loss: 2.8269913Losses:  3.0330162048339844 0.8259259462356567 0.7150267362594604
MemoryTrain:  epoch  9, batch     4 | loss: 3.0330162Losses:  2.6674842834472656 0.47267282009124756 0.9552530646324158
MemoryTrain:  epoch  9, batch     5 | loss: 2.6674843Losses:  2.8934383392333984 0.7655844688415527 0.7666156888008118
MemoryTrain:  epoch  9, batch     6 | loss: 2.8934383Losses:  2.064485549926758 -0.0 0.5829699039459229
MemoryTrain:  epoch  9, batch     7 | loss: 2.0644855
[EVAL] batch:    0 | acc: 50.00%,  total acc: 50.00%   [EVAL] batch:    1 | acc: 62.50%,  total acc: 56.25%   [EVAL] batch:    2 | acc: 43.75%,  total acc: 52.08%   [EVAL] batch:    3 | acc: 50.00%,  total acc: 51.56%   [EVAL] batch:    4 | acc: 25.00%,  total acc: 46.25%   [EVAL] batch:    5 | acc: 31.25%,  total acc: 43.75%   [EVAL] batch:    6 | acc: 62.50%,  total acc: 46.43%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 50.00%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 53.47%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 56.88%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 59.66%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 61.46%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 62.50%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 62.95%   [EVAL] batch:   14 | acc: 56.25%,  total acc: 62.50%   [EVAL] batch:   15 | acc: 75.00%,  total acc: 63.28%   [EVAL] batch:   16 | acc: 50.00%,  total acc: 62.50%   [EVAL] batch:   17 | acc: 81.25%,  total acc: 63.54%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 63.49%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 65.31%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 66.96%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 68.18%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 69.57%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 70.83%   [EVAL] batch:   24 | acc: 81.25%,  total acc: 71.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 72.36%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 73.38%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 74.33%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 75.22%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 76.04%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 76.81%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 77.54%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 78.22%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 78.68%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 79.11%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 79.51%   [EVAL] batch:   36 | acc: 93.75%,  total acc: 79.90%   [EVAL] batch:   37 | acc: 68.75%,  total acc: 79.61%   [EVAL] batch:   38 | acc: 50.00%,  total acc: 78.85%   [EVAL] batch:   39 | acc: 37.50%,  total acc: 77.81%   [EVAL] batch:   40 | acc: 31.25%,  total acc: 76.68%   [EVAL] batch:   41 | acc: 12.50%,  total acc: 75.15%   [EVAL] batch:   42 | acc: 18.75%,  total acc: 73.84%   [EVAL] batch:   43 | acc: 62.50%,  total acc: 73.58%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 74.17%   [EVAL] batch:   45 | acc: 93.75%,  total acc: 74.59%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 75.13%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 75.52%   [EVAL] batch:   48 | acc: 93.75%,  total acc: 75.89%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 76.25%   [EVAL] batch:   50 | acc: 56.25%,  total acc: 75.86%   [EVAL] batch:   51 | acc: 50.00%,  total acc: 75.36%   [EVAL] batch:   52 | acc: 37.50%,  total acc: 74.65%   [EVAL] batch:   53 | acc: 68.75%,  total acc: 74.54%   [EVAL] batch:   54 | acc: 31.25%,  total acc: 73.75%   [EVAL] batch:   55 | acc: 43.75%,  total acc: 73.21%   [EVAL] batch:   56 | acc: 62.50%,  total acc: 73.03%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 73.28%   [EVAL] batch:   58 | acc: 75.00%,  total acc: 73.31%   [EVAL] batch:   59 | acc: 81.25%,  total acc: 73.44%   [EVAL] batch:   60 | acc: 75.00%,  total acc: 73.46%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 73.89%   [EVAL] batch:   62 | acc: 37.50%,  total acc: 73.31%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 83.33%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 83.04%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 83.59%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 84.03%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 84.09%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 85.42%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 85.58%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 86.16%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 86.67%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 87.87%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 88.19%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 88.82%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 89.06%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 89.29%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 88.92%   [EVAL] batch:   22 | acc: 87.50%,  total acc: 88.86%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 88.80%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 88.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 89.18%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 89.58%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 89.96%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 90.09%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 90.42%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 90.73%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 91.02%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 91.29%   [EVAL] batch:   33 | acc: 68.75%,  total acc: 90.62%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 90.18%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 90.28%   [EVAL] batch:   36 | acc: 87.50%,  total acc: 90.20%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 90.30%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 90.54%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 90.78%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 91.01%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 91.22%   [EVAL] batch:   42 | acc: 87.50%,  total acc: 91.13%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 91.19%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 91.11%   [EVAL] batch:   45 | acc: 87.50%,  total acc: 91.03%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 90.82%   [EVAL] batch:   47 | acc: 81.25%,  total acc: 90.62%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 90.82%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 90.75%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 90.81%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 90.87%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 90.92%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 91.09%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 90.91%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 90.96%   [EVAL] batch:   56 | acc: 75.00%,  total acc: 90.68%   [EVAL] batch:   57 | acc: 56.25%,  total acc: 90.09%   [EVAL] batch:   58 | acc: 62.50%,  total acc: 89.62%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 89.69%   [EVAL] batch:   60 | acc: 81.25%,  total acc: 89.55%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 89.21%   [EVAL] batch:   62 | acc: 75.00%,  total acc: 88.99%   [EVAL] batch:   63 | acc: 81.25%,  total acc: 88.87%   [EVAL] batch:   64 | acc: 75.00%,  total acc: 88.65%   [EVAL] batch:   65 | acc: 75.00%,  total acc: 88.45%   [EVAL] batch:   66 | acc: 68.75%,  total acc: 88.15%   [EVAL] batch:   67 | acc: 75.00%,  total acc: 87.96%   [EVAL] batch:   68 | acc: 81.25%,  total acc: 87.86%   [EVAL] batch:   69 | acc: 81.25%,  total acc: 87.77%   [EVAL] batch:   70 | acc: 68.75%,  total acc: 87.50%   [EVAL] batch:   71 | acc: 68.75%,  total acc: 87.24%   [EVAL] batch:   72 | acc: 75.00%,  total acc: 87.07%   [EVAL] batch:   73 | acc: 87.50%,  total acc: 87.08%   [EVAL] batch:   74 | acc: 81.25%,  total acc: 87.00%   [EVAL] batch:   75 | acc: 87.50%,  total acc: 87.01%   [EVAL] batch:   76 | acc: 87.50%,  total acc: 87.01%   [EVAL] batch:   77 | acc: 75.00%,  total acc: 86.86%   [EVAL] batch:   78 | acc: 87.50%,  total acc: 86.87%   [EVAL] batch:   79 | acc: 81.25%,  total acc: 86.80%   [EVAL] batch:   80 | acc: 93.75%,  total acc: 86.88%   [EVAL] batch:   81 | acc: 62.50%,  total acc: 86.59%   [EVAL] batch:   82 | acc: 68.75%,  total acc: 86.37%   [EVAL] batch:   83 | acc: 81.25%,  total acc: 86.31%   [EVAL] batch:   84 | acc: 81.25%,  total acc: 86.25%   [EVAL] batch:   85 | acc: 87.50%,  total acc: 86.26%   [EVAL] batch:   86 | acc: 87.50%,  total acc: 86.28%   [EVAL] batch:   87 | acc: 81.25%,  total acc: 86.22%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 86.38%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 86.53%   [EVAL] batch:   90 | acc: 100.00%,  total acc: 86.68%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 86.82%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 86.96%   [EVAL] batch:   93 | acc: 93.75%,  total acc: 87.03%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 87.17%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 87.30%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 87.44%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 87.56%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 87.69%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 87.81%   [EVAL] batch:  100 | acc: 50.00%,  total acc: 87.44%   [EVAL] batch:  101 | acc: 62.50%,  total acc: 87.19%   [EVAL] batch:  102 | acc: 62.50%,  total acc: 86.95%   [EVAL] batch:  103 | acc: 56.25%,  total acc: 86.66%   [EVAL] batch:  104 | acc: 87.50%,  total acc: 86.67%   [EVAL] batch:  105 | acc: 62.50%,  total acc: 86.44%   [EVAL] batch:  106 | acc: 31.25%,  total acc: 85.92%   [EVAL] batch:  107 | acc: 18.75%,  total acc: 85.30%   [EVAL] batch:  108 | acc: 43.75%,  total acc: 84.92%   [EVAL] batch:  109 | acc: 12.50%,  total acc: 84.26%   [EVAL] batch:  110 | acc: 37.50%,  total acc: 83.84%   [EVAL] batch:  111 | acc: 18.75%,  total acc: 83.26%   [EVAL] batch:  112 | acc: 56.25%,  total acc: 83.02%   [EVAL] batch:  113 | acc: 87.50%,  total acc: 83.06%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 83.21%   [EVAL] batch:  115 | acc: 81.25%,  total acc: 83.19%   [EVAL] batch:  116 | acc: 87.50%,  total acc: 83.23%   [EVAL] batch:  117 | acc: 87.50%,  total acc: 83.26%   [EVAL] batch:  118 | acc: 75.00%,  total acc: 83.19%   [EVAL] batch:  119 | acc: 12.50%,  total acc: 82.60%   [EVAL] batch:  120 | acc: 18.75%,  total acc: 82.08%   [EVAL] batch:  121 | acc: 6.25%,  total acc: 81.45%   [EVAL] batch:  122 | acc: 31.25%,  total acc: 81.05%   [EVAL] batch:  123 | acc: 12.50%,  total acc: 80.49%   [EVAL] batch:  124 | acc: 6.25%,  total acc: 79.90%   [EVAL] batch:  125 | acc: 75.00%,  total acc: 79.86%   [EVAL] batch:  126 | acc: 31.25%,  total acc: 79.48%   [EVAL] batch:  127 | acc: 43.75%,  total acc: 79.20%   [EVAL] batch:  128 | acc: 25.00%,  total acc: 78.78%   [EVAL] batch:  129 | acc: 68.75%,  total acc: 78.70%   [EVAL] batch:  130 | acc: 50.00%,  total acc: 78.48%   [EVAL] batch:  131 | acc: 87.50%,  total acc: 78.55%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 78.71%   [EVAL] batch:  133 | acc: 81.25%,  total acc: 78.73%   [EVAL] batch:  134 | acc: 68.75%,  total acc: 78.66%   [EVAL] batch:  135 | acc: 81.25%,  total acc: 78.68%   [EVAL] batch:  136 | acc: 93.75%,  total acc: 78.79%   [EVAL] batch:  137 | acc: 75.00%,  total acc: 78.76%   [EVAL] batch:  138 | acc: 56.25%,  total acc: 78.60%   [EVAL] batch:  139 | acc: 62.50%,  total acc: 78.48%   [EVAL] batch:  140 | acc: 81.25%,  total acc: 78.50%   [EVAL] batch:  141 | acc: 75.00%,  total acc: 78.48%   [EVAL] batch:  142 | acc: 68.75%,  total acc: 78.41%   [EVAL] batch:  143 | acc: 75.00%,  total acc: 78.39%   [EVAL] batch:  144 | acc: 75.00%,  total acc: 78.36%   [EVAL] batch:  145 | acc: 87.50%,  total acc: 78.42%   [EVAL] batch:  146 | acc: 81.25%,  total acc: 78.44%   [EVAL] batch:  147 | acc: 87.50%,  total acc: 78.51%   [EVAL] batch:  148 | acc: 87.50%,  total acc: 78.57%   [EVAL] batch:  149 | acc: 81.25%,  total acc: 78.58%   [EVAL] batch:  150 | acc: 87.50%,  total acc: 78.64%   [EVAL] batch:  151 | acc: 75.00%,  total acc: 78.62%   [EVAL] batch:  152 | acc: 81.25%,  total acc: 78.64%   [EVAL] batch:  153 | acc: 93.75%,  total acc: 78.73%   [EVAL] batch:  154 | acc: 50.00%,  total acc: 78.55%   [EVAL] batch:  155 | acc: 75.00%,  total acc: 78.53%   [EVAL] batch:  156 | acc: 75.00%,  total acc: 78.50%   [EVAL] batch:  157 | acc: 62.50%,  total acc: 78.40%   [EVAL] batch:  158 | acc: 56.25%,  total acc: 78.26%   [EVAL] batch:  159 | acc: 62.50%,  total acc: 78.16%   [EVAL] batch:  160 | acc: 56.25%,  total acc: 78.03%   [EVAL] batch:  161 | acc: 56.25%,  total acc: 77.89%   [EVAL] batch:  162 | acc: 68.75%,  total acc: 77.84%   [EVAL] batch:  163 | acc: 56.25%,  total acc: 77.71%   [EVAL] batch:  164 | acc: 62.50%,  total acc: 77.61%   [EVAL] batch:  165 | acc: 56.25%,  total acc: 77.48%   [EVAL] batch:  166 | acc: 68.75%,  total acc: 77.43%   [EVAL] batch:  167 | acc: 56.25%,  total acc: 77.31%   [EVAL] batch:  168 | acc: 56.25%,  total acc: 77.18%   [EVAL] batch:  169 | acc: 18.75%,  total acc: 76.84%   [EVAL] batch:  170 | acc: 37.50%,  total acc: 76.61%   [EVAL] batch:  171 | acc: 25.00%,  total acc: 76.31%   [EVAL] batch:  172 | acc: 43.75%,  total acc: 76.12%   [EVAL] batch:  173 | acc: 25.00%,  total acc: 75.83%   [EVAL] batch:  174 | acc: 43.75%,  total acc: 75.64%   [EVAL] batch:  175 | acc: 100.00%,  total acc: 75.78%   [EVAL] batch:  176 | acc: 100.00%,  total acc: 75.92%   [EVAL] batch:  177 | acc: 100.00%,  total acc: 76.05%   [EVAL] batch:  178 | acc: 100.00%,  total acc: 76.19%   [EVAL] batch:  179 | acc: 100.00%,  total acc: 76.32%   [EVAL] batch:  180 | acc: 100.00%,  total acc: 76.45%   [EVAL] batch:  181 | acc: 100.00%,  total acc: 76.58%   [EVAL] batch:  182 | acc: 81.25%,  total acc: 76.61%   [EVAL] batch:  183 | acc: 87.50%,  total acc: 76.66%   [EVAL] batch:  184 | acc: 93.75%,  total acc: 76.76%   [EVAL] batch:  185 | acc: 87.50%,  total acc: 76.81%   [EVAL] batch:  186 | acc: 87.50%,  total acc: 76.87%   [EVAL] batch:  187 | acc: 75.00%,  total acc: 76.86%   [EVAL] batch:  188 | acc: 43.75%,  total acc: 76.69%   [EVAL] batch:  189 | acc: 56.25%,  total acc: 76.58%   [EVAL] batch:  190 | acc: 56.25%,  total acc: 76.47%   [EVAL] batch:  191 | acc: 25.00%,  total acc: 76.20%   [EVAL] batch:  192 | acc: 43.75%,  total acc: 76.04%   [EVAL] batch:  193 | acc: 37.50%,  total acc: 75.84%   [EVAL] batch:  194 | acc: 81.25%,  total acc: 75.87%   [EVAL] batch:  195 | acc: 68.75%,  total acc: 75.83%   [EVAL] batch:  196 | acc: 81.25%,  total acc: 75.86%   [EVAL] batch:  197 | acc: 93.75%,  total acc: 75.95%   [EVAL] batch:  198 | acc: 81.25%,  total acc: 75.97%   [EVAL] batch:  199 | acc: 75.00%,  total acc: 75.97%   [EVAL] batch:  200 | acc: 68.75%,  total acc: 75.93%   [EVAL] batch:  201 | acc: 62.50%,  total acc: 75.87%   [EVAL] batch:  202 | acc: 68.75%,  total acc: 75.83%   [EVAL] batch:  203 | acc: 62.50%,  total acc: 75.77%   [EVAL] batch:  204 | acc: 68.75%,  total acc: 75.73%   [EVAL] batch:  205 | acc: 56.25%,  total acc: 75.64%   [EVAL] batch:  206 | acc: 93.75%,  total acc: 75.72%   [EVAL] batch:  207 | acc: 100.00%,  total acc: 75.84%   [EVAL] batch:  208 | acc: 100.00%,  total acc: 75.96%   [EVAL] batch:  209 | acc: 93.75%,  total acc: 76.04%   [EVAL] batch:  210 | acc: 100.00%,  total acc: 76.16%   [EVAL] batch:  211 | acc: 100.00%,  total acc: 76.27%   [EVAL] batch:  212 | acc: 81.25%,  total acc: 76.29%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 76.40%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 76.51%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 76.62%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 76.73%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 76.83%   [EVAL] batch:  218 | acc: 100.00%,  total acc: 76.94%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 77.05%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 77.15%   [EVAL] batch:  221 | acc: 87.50%,  total acc: 77.20%   [EVAL] batch:  222 | acc: 93.75%,  total acc: 77.27%   [EVAL] batch:  223 | acc: 93.75%,  total acc: 77.34%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 77.44%   [EVAL] batch:  225 | acc: 50.00%,  total acc: 77.32%   [EVAL] batch:  226 | acc: 25.00%,  total acc: 77.09%   [EVAL] batch:  227 | acc: 43.75%,  total acc: 76.95%   [EVAL] batch:  228 | acc: 31.25%,  total acc: 76.75%   [EVAL] batch:  229 | acc: 12.50%,  total acc: 76.47%   [EVAL] batch:  230 | acc: 31.25%,  total acc: 76.27%   [EVAL] batch:  231 | acc: 87.50%,  total acc: 76.32%   [EVAL] batch:  232 | acc: 93.75%,  total acc: 76.39%   [EVAL] batch:  233 | acc: 100.00%,  total acc: 76.50%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 76.60%   [EVAL] batch:  235 | acc: 87.50%,  total acc: 76.64%   [EVAL] batch:  236 | acc: 93.75%,  total acc: 76.71%   [EVAL] batch:  237 | acc: 81.25%,  total acc: 76.73%   [EVAL] batch:  238 | acc: 43.75%,  total acc: 76.60%   [EVAL] batch:  239 | acc: 50.00%,  total acc: 76.48%   [EVAL] batch:  240 | acc: 43.75%,  total acc: 76.35%   [EVAL] batch:  241 | acc: 68.75%,  total acc: 76.32%   [EVAL] batch:  242 | acc: 31.25%,  total acc: 76.13%   [EVAL] batch:  243 | acc: 43.75%,  total acc: 76.00%   [EVAL] batch:  244 | acc: 87.50%,  total acc: 76.05%   [EVAL] batch:  245 | acc: 81.25%,  total acc: 76.07%   [EVAL] batch:  246 | acc: 68.75%,  total acc: 76.04%   [EVAL] batch:  247 | acc: 81.25%,  total acc: 76.06%   [EVAL] batch:  248 | acc: 87.50%,  total acc: 76.10%   [EVAL] batch:  249 | acc: 87.50%,  total acc: 76.15%   
cur_acc:  ['0.9286', '0.7827', '0.7262', '0.7331']
his_acc:  ['0.9286', '0.8585', '0.7992', '0.7615']
Clustering into  24  clusters
Clusters:  [ 5  7 10  3  0  0 10  4 22 15  7  1  4 19  8 23 16  9  3  9 11 20  4  0
 18 15  5  0  2  2 12  2  9  6  6  2  9  1 15  1  3 13  5 22 17 21  7 14
  8  7]
Losses:  8.899269104003906 3.3316102027893066 0.6590354442596436
CurrentTrain: epoch  0, batch     0 | loss: 8.8992691Losses:  9.113572120666504 2.3854546546936035 0.7475786209106445
CurrentTrain: epoch  0, batch     1 | loss: 9.1135721Losses:  9.965110778808594 3.8986024856567383 0.6331573724746704
CurrentTrain: epoch  0, batch     2 | loss: 9.9651108Losses:  6.247896194458008 -0.0 0.10212994366884232
CurrentTrain: epoch  0, batch     3 | loss: 6.2478962Losses:  10.074621200561523 4.341968536376953 0.6232403516769409
CurrentTrain: epoch  1, batch     0 | loss: 10.0746212Losses:  7.462972640991211 3.6967873573303223 0.591895341873169
CurrentTrain: epoch  1, batch     1 | loss: 7.4629726Losses:  10.1350679397583 4.899096488952637 0.6121649146080017
CurrentTrain: epoch  1, batch     2 | loss: 10.1350679Losses:  4.775355339050293 -0.0 0.09814666211605072
CurrentTrain: epoch  1, batch     3 | loss: 4.7753553Losses:  7.025643348693848 2.6598594188690186 0.7375057339668274
CurrentTrain: epoch  2, batch     0 | loss: 7.0256433Losses:  7.296291351318359 3.645171880722046 0.6906957626342773
CurrentTrain: epoch  2, batch     1 | loss: 7.2962914Losses:  8.046101570129395 3.3380208015441895 0.6857746839523315
CurrentTrain: epoch  2, batch     2 | loss: 8.0461016Losses:  4.8383870124816895 -0.0 0.14434407651424408
CurrentTrain: epoch  2, batch     3 | loss: 4.8383870Losses:  6.677933692932129 3.000462532043457 0.6590988039970398
CurrentTrain: epoch  3, batch     0 | loss: 6.6779337Losses:  5.807679653167725 2.056547164916992 0.7445195317268372
CurrentTrain: epoch  3, batch     1 | loss: 5.8076797Losses:  7.072915077209473 2.734701633453369 0.6653491258621216
CurrentTrain: epoch  3, batch     2 | loss: 7.0729151Losses:  4.426024913787842 -0.0 0.17969095706939697
CurrentTrain: epoch  3, batch     3 | loss: 4.4260249Losses:  6.8998613357543945 3.1289782524108887 0.6630962491035461
CurrentTrain: epoch  4, batch     0 | loss: 6.8998613Losses:  7.974451065063477 3.9966437816619873 0.6844112873077393
CurrentTrain: epoch  4, batch     1 | loss: 7.9744511Losses:  7.323424339294434 3.749190092086792 0.7200862169265747
CurrentTrain: epoch  4, batch     2 | loss: 7.3234243Losses:  1.670135498046875 -0.0 -0.0
CurrentTrain: epoch  4, batch     3 | loss: 1.6701355Losses:  5.758667945861816 2.125277042388916 0.7361011505126953
CurrentTrain: epoch  5, batch     0 | loss: 5.7586679Losses:  5.625251770019531 2.5988974571228027 0.6683911085128784
CurrentTrain: epoch  5, batch     1 | loss: 5.6252518Losses:  6.558464050292969 2.8721933364868164 0.7137432098388672
CurrentTrain: epoch  5, batch     2 | loss: 6.5584641Losses:  4.860367774963379 -0.0 0.14738410711288452
CurrentTrain: epoch  5, batch     3 | loss: 4.8603678Losses:  5.098672389984131 1.936184048652649 0.7275355458259583
CurrentTrain: epoch  6, batch     0 | loss: 5.0986724Losses:  5.772007465362549 2.1132054328918457 0.6372588872909546
CurrentTrain: epoch  6, batch     1 | loss: 5.7720075Losses:  6.478530406951904 3.3161702156066895 0.6745432019233704
CurrentTrain: epoch  6, batch     2 | loss: 6.4785304Losses:  1.856428623199463 -0.0 0.12980298697948456
CurrentTrain: epoch  6, batch     3 | loss: 1.8564286Losses:  5.846379280090332 3.1578445434570312 0.6479485034942627
CurrentTrain: epoch  7, batch     0 | loss: 5.8463793Losses:  6.4150309562683105 3.4304616451263428 0.6603425145149231
CurrentTrain: epoch  7, batch     1 | loss: 6.4150310Losses:  6.848466873168945 3.676553964614868 0.5325307250022888
CurrentTrain: epoch  7, batch     2 | loss: 6.8484669Losses:  5.501979827880859 -0.0 0.12266387790441513
CurrentTrain: epoch  7, batch     3 | loss: 5.5019798Losses:  5.560669898986816 2.530590534210205 0.6473686695098877
CurrentTrain: epoch  8, batch     0 | loss: 5.5606699Losses:  5.4140167236328125 2.0934388637542725 0.7016220092773438
CurrentTrain: epoch  8, batch     1 | loss: 5.4140167Losses:  4.875715732574463 2.055361747741699 0.7013430595397949
CurrentTrain: epoch  8, batch     2 | loss: 4.8757157Losses:  2.8687281608581543 -0.0 0.10919664055109024
CurrentTrain: epoch  8, batch     3 | loss: 2.8687282Losses:  4.47952127456665 1.7835654020309448 0.698806881904602
CurrentTrain: epoch  9, batch     0 | loss: 4.4795213Losses:  5.928981304168701 2.758936882019043 0.6913884282112122
CurrentTrain: epoch  9, batch     1 | loss: 5.9289813Losses:  5.6826863288879395 2.7961559295654297 0.5796314477920532
CurrentTrain: epoch  9, batch     2 | loss: 5.6826863Losses:  2.010545015335083 -0.0 0.11212331056594849
CurrentTrain: epoch  9, batch     3 | loss: 2.0105450
#### member of political party ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "member of political party" means the political party of which a person is or has been a member or otherwise affiliated.
Here is an example:
Relation: member of political party
Context: alongside evan durbin and hugh gaitskell , he brought the thinking of john maynard keynes to the labour party , especially in relation to price determination .
Head Entity: hugh gaitskell
Tail Entity: labour party
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "member of political party":
Relation: member of political party  
Context: After serving as the mayor of Chicago, he became a prominent figure in the Democratic Party, advocating for social justice and economic reform.  
Head Entity: mayor of Chicago  
Tail Entity: Democratic Party  

Relation: member of political party  
Context: Throughout her career, she has been a staunch advocate for environmental policies as a member of the Green Party, pushing for sustainable practices in government.  
Head Entity: she  
Tail Entity: Green Party  
#### after a work by ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "after a work by" means artist whose work strongly inspired/ was copied in this item.
Here is an example:
Relation: after a work by
Context: the script for " the great santini " was adapted by carlino from the 1976 novel by pat conroy , with assistance from an un - credited herman raucher .
Head Entity: the great santini
Tail Entity: pat conroy
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "after a work by":
Relation: after a work by  
Context: the film "inception" draws heavily from the concepts presented in the 1988 novel "the mind's eye" by jeffrey a. t. h. s.  
Head Entity: inception  
Tail Entity: jeffrey a. t. h. s.  

Relation: after a work by  
Context: the musical "hamilton" was inspired by the biography "alexander hamilton" written by ron chernow, which provided a fresh perspective on the founding father.  
Head Entity: hamilton  
Tail Entity: ron chernow  
#### notable work ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "notable work" means notable scientific, artistic or literary work, or other work of significance among subject's works.
Here is an example:
Relation: notable work
Context: the influential victorian art critic john ruskin expounded theories about social justice in " unto this last " ( 1860 ) .
Head Entity: john ruskin
Tail Entity: unto this last
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "notable work":
Relation: notable work  
Context: the renowned physicist albert einstein published his groundbreaking theory of relativity in "on the electrodynamics of moving bodies" (1905).  
Head Entity: albert einstein  
Tail Entity: on the electrodynamics of moving bodies  

Relation: notable work  
Context: the celebrated author j.k. rowling captivated readers with her first book in the harry potter series, "harry potter and the philosopher's stone" (1997).  
Head Entity: j.k. rowling  
Tail Entity: harry potter and the philosopher's stone  
#### headquarters location ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "headquarters location" means city, where an organization's headquarters is or has been situated.
Here is an example:
Relation: headquarters location
Context: at the request of the stockholm institute for bible translation , mirza khazar translated the bible – the new testament and old testament – into the azerbaijani language .
Head Entity: institute for bible translation
Tail Entity: stockholm
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "headquarters location":
Relation: headquarters location  
Context: the global technology company, headquartered in san francisco, has been a leader in software development for over two decades.  
Head Entity: global technology company  
Tail Entity: san francisco  

Relation: headquarters location  
Context: after relocating its main office to new york city, the startup saw a significant increase in investment and growth opportunities.  
Head Entity: startup  
Tail Entity: new york city  
#### taxon rank ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "taxon rank" means level in a taxonomic hierarchy.
Here is an example:
Relation: taxon rank
Context: gyaclavator is an extinct genus of lace bug in the family tingidae known from a fossil found in north america .
Head Entity: tingidae
Tail Entity: family
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "taxon rank":
Relation: taxon rank  
Context: The genus Panthera includes several large cats, such as lions and tigers, which are classified under the family Felidae.  
Head Entity: Panthera  
Tail Entity: family  

Relation: taxon rank  
Context: The species Homo sapiens is part of the genus Homo, which falls under the family Hominidae, known for its members' advanced cognitive abilities.  
Head Entity: Homo  
Tail Entity: family  
#### field of work ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "field of work" means specialization of a person or organization.
Here is an example:
Relation: field of work
Context: elspeth frances garman is professor of molecular biophysics at the university of oxford and a former president of the british crystallographic association .
Head Entity: elspeth frances garman
Tail Entity: molecular biophysics
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "field of work":
Relation: field of work  
Context: dr. amanda jones is a leading researcher in the field of artificial intelligence and currently works at the tech innovation lab in silicon valley.  
Head Entity: dr. amanda jones  
Tail Entity: artificial intelligence  

Relation: field of work  
Context: the world health organization has appointed dr. rajesh kumar as the new director of global health initiatives, focusing on infectious diseases.  
Head Entity: dr. rajesh kumar  
Tail Entity: infectious diseases  
#### licensed to broadcast to ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "licensed to broadcast to" means place that a radio/TV station is licensed/required to broadcast to.
Here is an example:
Relation: licensed to broadcast to
Context: this early involvement in radio led him to buy many small , under performing radio stations beginning with wcos in columbia , south carolina , and turn them around with new formats and improved management .
Head Entity: wcos
Tail Entity: columbia , south carolina
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "licensed to broadcast to":
Relation: licensed to broadcast to  
Context: The new regulations allowed the station KXYZ to expand its reach, enabling it to serve the community of Springfield more effectively.  
Head Entity: KXYZ  
Tail Entity: Springfield  

Relation: licensed to broadcast to  
Context: After acquiring the rights, the television network was able to broadcast its popular shows to the audience in the city of Denver, significantly increasing its viewership.  
Head Entity: television network  
Tail Entity: Denver  
#### constellation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "constellation" means the area of the celestial sphere of which the subject is a part.
Here is an example:
Relation: constellation
Context: tau² eridani ( τ² eridani , abbreviated tau² eri , τ² eri ) , also named angetenar , is a star in the constellation of eridanus .
Head Entity: angetenar
Tail Entity: eridanus
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "constellation":
Relation: constellation  
Context: betelgeuse ( α orionis ) is a red supergiant star in the constellation of orion, known for its brightness and distinctive reddish hue.  
Head Entity: betelgeuse  
Tail Entity: orion  

Relation: constellation  
Context: the star deneb is located in the constellation of cygnus and is one of the brightest stars in the northern sky.  
Head Entity: deneb  
Tail Entity: cygnus  
#### platform ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "platform" means platform for which a work was developed or released, or the specific platform version of a software product.
Here is an example:
Relation: platform
Context: " toy soldiers : war chest " was released on august 11 , 2015 for windows , playstation 4 and xbox one .
Head Entity: toy soldiers : war chest
Tail Entity: xbox one
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "platform":
Relation: platform  
Context: "The Legend of Zelda: Breath of the Wild" was launched on March 3, 2017 for the Nintendo Switch and Wii U.  
Head Entity: The Legend of Zelda: Breath of the Wild  
Tail Entity: Nintendo Switch  

Relation: platform  
Context: "Cyberpunk 2077" was made available on December 10, 2020 for PC, PlayStation 4, and Xbox One.  
Head Entity: Cyberpunk 2077  
Tail Entity: PlayStation 4  
#### place served by transport hub ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "place served by transport hub" means territorial entity or entities served by this transport hub (airport, train station, etc.).
Here is an example:
Relation: place served by transport hub
Context: services to the island run from san juan 's luis muñoz marín international airport , ceiba airport or isla grande airport ( 20- to 30-minute flight ) .
Head Entity: isla grande airport
Tail Entity: san juan
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "place served by transport hub":
Relation: place served by transport hub  
Context: The city of Barcelona is well-connected to the surrounding regions through the El Prat Airport, which offers numerous flights to various destinations.  
Head Entity: El Prat Airport  
Tail Entity: Barcelona  

Relation: place served by transport hub  
Context: The central train station in Zurich provides access to multiple Swiss cities and international destinations, making it a key transport hub for travelers.  
Head Entity: Zurich central train station  
Tail Entity: Zurich  
Losses:  3.8592894077301025 0.26329338550567627 0.9599407315254211
MemoryTrain:  epoch  0, batch     0 | loss: 3.8592894Losses:  4.783989429473877 0.7381998300552368 0.8802995085716248
MemoryTrain:  epoch  0, batch     1 | loss: 4.7839894Losses:  3.464634895324707 -0.0 0.978717565536499
MemoryTrain:  epoch  0, batch     2 | loss: 3.4646349Losses:  3.4256203174591064 0.2582227885723114 0.9566776752471924
MemoryTrain:  epoch  0, batch     3 | loss: 3.4256203Losses:  4.197962760925293 0.24448521435260773 0.9565958380699158
MemoryTrain:  epoch  0, batch     4 | loss: 4.1979628Losses:  3.643942356109619 0.4819502830505371 0.9049547910690308
MemoryTrain:  epoch  0, batch     5 | loss: 3.6439424Losses:  4.1470465660095215 0.27224037051200867 0.583341121673584
MemoryTrain:  epoch  0, batch     6 | loss: 4.1470466Losses:  3.9501144886016846 0.4864545464515686 0.9040667414665222
MemoryTrain:  epoch  0, batch     7 | loss: 3.9501145Losses:  5.042003154754639 0.8810014128684998 1.0111327171325684
MemoryTrain:  epoch  0, batch     8 | loss: 5.0420032Losses:  4.41676664352417 -0.0 0.48482370376586914
MemoryTrain:  epoch  0, batch     9 | loss: 4.4167666Losses:  3.600738048553467 0.531829833984375 0.8067595362663269
MemoryTrain:  epoch  1, batch     0 | loss: 3.6007380Losses:  3.0722224712371826 0.5039029717445374 0.8015955090522766
MemoryTrain:  epoch  1, batch     1 | loss: 3.0722225Losses:  3.6466522216796875 0.24778440594673157 0.9567865133285522
MemoryTrain:  epoch  1, batch     2 | loss: 3.6466522Losses:  3.5513112545013428 0.36697056889533997 0.7845310568809509
MemoryTrain:  epoch  1, batch     3 | loss: 3.5513113Losses:  2.9387621879577637 0.2621387541294098 0.871599555015564
MemoryTrain:  epoch  1, batch     4 | loss: 2.9387622Losses:  2.6135482788085938 -0.0 0.8523811101913452
MemoryTrain:  epoch  1, batch     5 | loss: 2.6135483Losses:  4.107333183288574 0.27863672375679016 0.933006763458252
MemoryTrain:  epoch  1, batch     6 | loss: 4.1073332Losses:  3.799323320388794 0.2549407482147217 0.9935746192932129
MemoryTrain:  epoch  1, batch     7 | loss: 3.7993233Losses:  3.609163522720337 0.5514482855796814 0.799121081829071
MemoryTrain:  epoch  1, batch     8 | loss: 3.6091635Losses:  3.559788703918457 -0.0 0.48657089471817017
MemoryTrain:  epoch  1, batch     9 | loss: 3.5597887Losses:  3.131101369857788 -0.0 0.97073894739151
MemoryTrain:  epoch  2, batch     0 | loss: 3.1311014Losses:  2.7907214164733887 -0.0 1.0344817638397217
MemoryTrain:  epoch  2, batch     1 | loss: 2.7907214Losses:  4.084702491760254 0.49311524629592896 0.7449054718017578
MemoryTrain:  epoch  2, batch     2 | loss: 4.0847025Losses:  3.238449811935425 -0.0 1.0595877170562744
MemoryTrain:  epoch  2, batch     3 | loss: 3.2384498Losses:  3.248577833175659 0.25370100140571594 0.9591701626777649
MemoryTrain:  epoch  2, batch     4 | loss: 3.2485778Losses:  2.907662868499756 -0.0 0.8564092516899109
MemoryTrain:  epoch  2, batch     5 | loss: 2.9076629Losses:  3.4007697105407715 0.5062485933303833 0.9003660082817078
MemoryTrain:  epoch  2, batch     6 | loss: 3.4007697Losses:  3.3725502490997314 0.7575335502624512 0.9062373042106628
MemoryTrain:  epoch  2, batch     7 | loss: 3.3725502Losses:  2.3989968299865723 -0.0 0.8649236559867859
MemoryTrain:  epoch  2, batch     8 | loss: 2.3989968Losses:  1.7441129684448242 -0.0 0.43942269682884216
MemoryTrain:  epoch  2, batch     9 | loss: 1.7441130Losses:  2.8941586017608643 0.29275625944137573 0.8584799766540527
MemoryTrain:  epoch  3, batch     0 | loss: 2.8941586Losses:  2.5650722980499268 0.27831676602363586 0.7965363264083862
MemoryTrain:  epoch  3, batch     1 | loss: 2.5650723Losses:  3.7196898460388184 1.3709195852279663 0.7711716890335083
MemoryTrain:  epoch  3, batch     2 | loss: 3.7196898Losses:  3.174358606338501 0.2297241985797882 0.9486765265464783
MemoryTrain:  epoch  3, batch     3 | loss: 3.1743586Losses:  2.5627801418304443 -0.0 1.0159249305725098
MemoryTrain:  epoch  3, batch     4 | loss: 2.5627801Losses:  2.707111358642578 0.25182169675827026 0.9074244499206543
MemoryTrain:  epoch  3, batch     5 | loss: 2.7071114Losses:  2.1917896270751953 -0.0 0.8608590960502625
MemoryTrain:  epoch  3, batch     6 | loss: 2.1917896Losses:  2.239523410797119 -0.0 0.9208194613456726
MemoryTrain:  epoch  3, batch     7 | loss: 2.2395234Losses:  3.477834939956665 0.5323922634124756 0.9181229472160339
MemoryTrain:  epoch  3, batch     8 | loss: 3.4778349Losses:  3.7889013290405273 0.29397350549697876 0.43270358443260193
MemoryTrain:  epoch  3, batch     9 | loss: 3.7889013Losses:  3.219243288040161 0.8715014457702637 0.8037443161010742
MemoryTrain:  epoch  4, batch     0 | loss: 3.2192433Losses:  2.748220443725586 0.2728467285633087 0.8625283241271973
MemoryTrain:  epoch  4, batch     1 | loss: 2.7482204Losses:  2.9270107746124268 0.5129116773605347 0.8445733189582825
MemoryTrain:  epoch  4, batch     2 | loss: 2.9270108Losses:  2.876030683517456 0.49087005853652954 0.8937298655509949
MemoryTrain:  epoch  4, batch     3 | loss: 2.8760307Losses:  2.8064541816711426 0.46543341875076294 0.8580066561698914
MemoryTrain:  epoch  4, batch     4 | loss: 2.8064542Losses:  2.8237011432647705 0.2836070954799652 0.9234773516654968
MemoryTrain:  epoch  4, batch     5 | loss: 2.8237011Losses:  2.935009002685547 0.5122533440589905 0.8511571288108826
MemoryTrain:  epoch  4, batch     6 | loss: 2.9350090Losses:  2.5196831226348877 0.2518910765647888 0.9110638499259949
MemoryTrain:  epoch  4, batch     7 | loss: 2.5196831Losses:  2.9184224605560303 0.7884618043899536 0.8254116177558899
MemoryTrain:  epoch  4, batch     8 | loss: 2.9184225Losses:  2.0620369911193848 -0.0 0.5210554599761963
MemoryTrain:  epoch  4, batch     9 | loss: 2.0620370Losses:  2.698896884918213 0.2602464556694031 0.9595808982849121
MemoryTrain:  epoch  5, batch     0 | loss: 2.6988969Losses:  2.54641056060791 -0.0 0.8552613854408264
MemoryTrain:  epoch  5, batch     1 | loss: 2.5464106Losses:  2.812415599822998 0.5475793480873108 0.8022903800010681
MemoryTrain:  epoch  5, batch     2 | loss: 2.8124156Losses:  2.232865333557129 -0.0 0.9136515855789185
MemoryTrain:  epoch  5, batch     3 | loss: 2.2328653Losses:  2.5104198455810547 0.4838320016860962 0.7467950582504272
MemoryTrain:  epoch  5, batch     4 | loss: 2.5104198Losses:  2.3761801719665527 -0.0 0.8815877437591553
MemoryTrain:  epoch  5, batch     5 | loss: 2.3761802Losses:  2.8403306007385254 0.5003875494003296 0.9007338881492615
MemoryTrain:  epoch  5, batch     6 | loss: 2.8403306Losses:  2.9437386989593506 0.7737541198730469 0.8249217867851257
MemoryTrain:  epoch  5, batch     7 | loss: 2.9437387Losses:  2.5119223594665527 0.5129177570343018 0.7099630832672119
MemoryTrain:  epoch  5, batch     8 | loss: 2.5119224Losses:  1.6898571252822876 -0.0 0.49484166502952576
MemoryTrain:  epoch  5, batch     9 | loss: 1.6898571Losses:  2.745110511779785 0.5253536701202393 0.860649824142456
MemoryTrain:  epoch  6, batch     0 | loss: 2.7451105Losses:  2.8122305870056152 0.5004623532295227 0.9670182466506958
MemoryTrain:  epoch  6, batch     1 | loss: 2.8122306Losses:  2.605825901031494 0.25393053889274597 0.8519047498703003
MemoryTrain:  epoch  6, batch     2 | loss: 2.6058259Losses:  3.587036371231079 1.3896369934082031 0.7852839827537537
MemoryTrain:  epoch  6, batch     3 | loss: 3.5870364Losses:  2.464071750640869 0.25568073987960815 0.7523516416549683
MemoryTrain:  epoch  6, batch     4 | loss: 2.4640718Losses:  2.3831682205200195 -0.0 1.049480676651001
MemoryTrain:  epoch  6, batch     5 | loss: 2.3831682Losses:  2.824052095413208 0.6896171569824219 0.8997146487236023
MemoryTrain:  epoch  6, batch     6 | loss: 2.8240521Losses:  2.89272403717041 0.796208381652832 0.7952489256858826
MemoryTrain:  epoch  6, batch     7 | loss: 2.8927240Losses:  2.8330795764923096 0.4825340509414673 0.9768103957176208
MemoryTrain:  epoch  6, batch     8 | loss: 2.8330796Losses:  1.8519995212554932 -0.0 0.4336135983467102
MemoryTrain:  epoch  6, batch     9 | loss: 1.8519995Losses:  2.348215341567993 0.21153783798217773 0.8335475325584412
MemoryTrain:  epoch  7, batch     0 | loss: 2.3482153Losses:  2.875298023223877 0.7358862161636353 0.8815366625785828
MemoryTrain:  epoch  7, batch     1 | loss: 2.8752980Losses:  2.6459057331085205 0.24811924993991852 0.9588554501533508
MemoryTrain:  epoch  7, batch     2 | loss: 2.6459057Losses:  2.4050848484039307 0.23701956868171692 0.901563823223114
MemoryTrain:  epoch  7, batch     3 | loss: 2.4050848Losses:  3.306295394897461 1.1000587940216064 0.9430513978004456
MemoryTrain:  epoch  7, batch     4 | loss: 3.3062954Losses:  2.4110634326934814 0.2527329921722412 0.8382226824760437
MemoryTrain:  epoch  7, batch     5 | loss: 2.4110634Losses:  2.7966604232788086 0.5506919622421265 0.8134710192680359
MemoryTrain:  epoch  7, batch     6 | loss: 2.7966604Losses:  2.142930030822754 -0.0 0.9025090932846069
MemoryTrain:  epoch  7, batch     7 | loss: 2.1429300Losses:  2.6112923622131348 0.48846185207366943 0.8390969038009644
MemoryTrain:  epoch  7, batch     8 | loss: 2.6112924Losses:  1.9193601608276367 -0.0 0.4985761046409607
MemoryTrain:  epoch  7, batch     9 | loss: 1.9193602Losses:  2.735710620880127 0.5266002416610718 0.7823268175125122
MemoryTrain:  epoch  8, batch     0 | loss: 2.7357106Losses:  2.2754952907562256 -0.0 0.9571888446807861
MemoryTrain:  epoch  8, batch     1 | loss: 2.2754953Losses:  2.7395172119140625 0.6844316720962524 0.8508746027946472
MemoryTrain:  epoch  8, batch     2 | loss: 2.7395172Losses:  2.5454440116882324 0.4876864552497864 0.8540791869163513
MemoryTrain:  epoch  8, batch     3 | loss: 2.5454440Losses:  2.3902134895324707 0.23909702897071838 0.8625251054763794
MemoryTrain:  epoch  8, batch     4 | loss: 2.3902135Losses:  2.418093681335449 0.23782449960708618 0.838312029838562
MemoryTrain:  epoch  8, batch     5 | loss: 2.4180937Losses:  2.23555326461792 -0.0 0.9730993509292603
MemoryTrain:  epoch  8, batch     6 | loss: 2.2355533Losses:  2.5444397926330566 0.48554107546806335 0.7880309820175171
MemoryTrain:  epoch  8, batch     7 | loss: 2.5444398Losses:  2.378244638442993 0.24273748695850372 0.9098562598228455
MemoryTrain:  epoch  8, batch     8 | loss: 2.3782446Losses:  1.9914472103118896 0.29046958684921265 0.4640650749206543
MemoryTrain:  epoch  8, batch     9 | loss: 1.9914472Losses:  2.647794246673584 0.4703453481197357 0.909392237663269
MemoryTrain:  epoch  9, batch     0 | loss: 2.6477942Losses:  2.3572192192077637 0.2299647033214569 0.9121763110160828
MemoryTrain:  epoch  9, batch     1 | loss: 2.3572192Losses:  2.4671504497528076 0.2579384744167328 0.9164947867393494
MemoryTrain:  epoch  9, batch     2 | loss: 2.4671504Losses:  2.637859344482422 0.4752940237522125 0.9371799230575562
MemoryTrain:  epoch  9, batch     3 | loss: 2.6378593Losses:  2.2987916469573975 0.24999670684337616 0.7916796803474426
MemoryTrain:  epoch  9, batch     4 | loss: 2.2987916Losses:  2.8774194717407227 0.7182806134223938 0.912308931350708
MemoryTrain:  epoch  9, batch     5 | loss: 2.8774195Losses:  2.5707778930664062 0.4958403706550598 0.8442867994308472
MemoryTrain:  epoch  9, batch     6 | loss: 2.5707779Losses:  2.25504732131958 0.22593259811401367 0.8376803994178772
MemoryTrain:  epoch  9, batch     7 | loss: 2.2550473Losses:  3.146437644958496 0.8790614604949951 0.9177846312522888
MemoryTrain:  epoch  9, batch     8 | loss: 3.1464376Losses:  1.7896240949630737 -0.0 0.5697000026702881
MemoryTrain:  epoch  9, batch     9 | loss: 1.7896241
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 84.38%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 86.25%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 88.54%   [EVAL] batch:    6 | acc: 62.50%,  total acc: 84.82%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 84.38%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 84.03%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 83.12%   [EVAL] batch:   10 | acc: 50.00%,  total acc: 80.11%   [EVAL] batch:   11 | acc: 68.75%,  total acc: 79.17%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 79.81%   [EVAL] batch:   13 | acc: 81.25%,  total acc: 79.91%   [EVAL] batch:   14 | acc: 81.25%,  total acc: 80.00%   [EVAL] batch:   15 | acc: 81.25%,  total acc: 80.08%   [EVAL] batch:   16 | acc: 81.25%,  total acc: 80.15%   [EVAL] batch:   17 | acc: 75.00%,  total acc: 79.86%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 79.28%   [EVAL] batch:   19 | acc: 56.25%,  total acc: 78.12%   [EVAL] batch:   20 | acc: 68.75%,  total acc: 77.68%   [EVAL] batch:   21 | acc: 62.50%,  total acc: 76.99%   [EVAL] batch:   22 | acc: 43.75%,  total acc: 75.54%   [EVAL] batch:   23 | acc: 81.25%,  total acc: 75.78%   [EVAL] batch:   24 | acc: 56.25%,  total acc: 75.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 75.96%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 76.85%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 77.68%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 78.45%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 79.17%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 79.84%   [EVAL] batch:   31 | acc: 75.00%,  total acc: 79.69%   [EVAL] batch:   32 | acc: 87.50%,  total acc: 79.92%   [EVAL] batch:   33 | acc: 68.75%,  total acc: 79.60%   [EVAL] batch:   34 | acc: 87.50%,  total acc: 79.82%   [EVAL] batch:   35 | acc: 62.50%,  total acc: 79.34%   [EVAL] batch:   36 | acc: 68.75%,  total acc: 79.05%   [EVAL] batch:   37 | acc: 68.75%,  total acc: 78.78%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 79.33%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 79.84%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 80.34%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 80.80%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 81.53%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 81.94%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 82.34%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 82.71%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 83.07%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 83.42%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 83.75%   [EVAL] batch:   50 | acc: 87.50%,  total acc: 83.82%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 83.89%   [EVAL] batch:   52 | acc: 87.50%,  total acc: 83.96%   [EVAL] batch:   53 | acc: 93.75%,  total acc: 84.14%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 84.09%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 84.15%   [EVAL] batch:   56 | acc: 87.50%,  total acc: 84.21%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 84.27%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 84.43%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 84.69%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 84.94%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 85.08%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 84.52%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 83.33%   [EVAL] batch:    6 | acc: 56.25%,  total acc: 79.46%   [EVAL] batch:    7 | acc: 68.75%,  total acc: 78.12%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 78.47%   [EVAL] batch:    9 | acc: 56.25%,  total acc: 76.25%   [EVAL] batch:   10 | acc: 56.25%,  total acc: 74.43%   [EVAL] batch:   11 | acc: 62.50%,  total acc: 73.44%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 74.52%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 75.89%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 77.08%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 78.52%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 79.41%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 80.21%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 81.56%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 81.85%   [EVAL] batch:   21 | acc: 75.00%,  total acc: 81.53%   [EVAL] batch:   22 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:   23 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 81.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 82.21%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 82.87%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 83.48%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 83.84%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 84.38%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 84.88%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 85.35%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 85.80%   [EVAL] batch:   33 | acc: 68.75%,  total acc: 85.29%   [EVAL] batch:   34 | acc: 68.75%,  total acc: 84.82%   [EVAL] batch:   35 | acc: 81.25%,  total acc: 84.72%   [EVAL] batch:   36 | acc: 87.50%,  total acc: 84.80%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 85.03%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 85.42%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 85.78%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 86.13%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 86.46%   [EVAL] batch:   42 | acc: 87.50%,  total acc: 86.48%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 86.51%   [EVAL] batch:   44 | acc: 75.00%,  total acc: 86.25%   [EVAL] batch:   45 | acc: 81.25%,  total acc: 86.14%   [EVAL] batch:   46 | acc: 75.00%,  total acc: 85.90%   [EVAL] batch:   47 | acc: 75.00%,  total acc: 85.68%   [EVAL] batch:   48 | acc: 81.25%,  total acc: 85.59%   [EVAL] batch:   49 | acc: 68.75%,  total acc: 85.25%   [EVAL] batch:   50 | acc: 81.25%,  total acc: 85.17%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 85.22%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 85.38%   [EVAL] batch:   53 | acc: 75.00%,  total acc: 85.19%   [EVAL] batch:   54 | acc: 56.25%,  total acc: 84.66%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 84.71%   [EVAL] batch:   56 | acc: 68.75%,  total acc: 84.43%   [EVAL] batch:   57 | acc: 56.25%,  total acc: 83.94%   [EVAL] batch:   58 | acc: 75.00%,  total acc: 83.79%   [EVAL] batch:   59 | acc: 87.50%,  total acc: 83.85%   [EVAL] batch:   60 | acc: 87.50%,  total acc: 83.91%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 83.67%   [EVAL] batch:   62 | acc: 87.50%,  total acc: 83.73%   [EVAL] batch:   63 | acc: 81.25%,  total acc: 83.69%   [EVAL] batch:   64 | acc: 75.00%,  total acc: 83.56%   [EVAL] batch:   65 | acc: 100.00%,  total acc: 83.81%   [EVAL] batch:   66 | acc: 75.00%,  total acc: 83.68%   [EVAL] batch:   67 | acc: 87.50%,  total acc: 83.73%   [EVAL] batch:   68 | acc: 81.25%,  total acc: 83.70%   [EVAL] batch:   69 | acc: 87.50%,  total acc: 83.75%   [EVAL] batch:   70 | acc: 68.75%,  total acc: 83.54%   [EVAL] batch:   71 | acc: 62.50%,  total acc: 83.25%   [EVAL] batch:   72 | acc: 75.00%,  total acc: 83.13%   [EVAL] batch:   73 | acc: 81.25%,  total acc: 83.11%   [EVAL] batch:   74 | acc: 81.25%,  total acc: 83.08%   [EVAL] batch:   75 | acc: 81.25%,  total acc: 83.06%   [EVAL] batch:   76 | acc: 81.25%,  total acc: 83.04%   [EVAL] batch:   77 | acc: 68.75%,  total acc: 82.85%   [EVAL] batch:   78 | acc: 87.50%,  total acc: 82.91%   [EVAL] batch:   79 | acc: 75.00%,  total acc: 82.81%   [EVAL] batch:   80 | acc: 93.75%,  total acc: 82.95%   [EVAL] batch:   81 | acc: 62.50%,  total acc: 82.70%   [EVAL] batch:   82 | acc: 68.75%,  total acc: 82.53%   [EVAL] batch:   83 | acc: 75.00%,  total acc: 82.44%   [EVAL] batch:   84 | acc: 68.75%,  total acc: 82.28%   [EVAL] batch:   85 | acc: 87.50%,  total acc: 82.34%   [EVAL] batch:   86 | acc: 93.75%,  total acc: 82.47%   [EVAL] batch:   87 | acc: 87.50%,  total acc: 82.53%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 82.72%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 82.92%   [EVAL] batch:   90 | acc: 93.75%,  total acc: 83.04%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 83.22%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 83.40%   [EVAL] batch:   93 | acc: 100.00%,  total acc: 83.58%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 83.75%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 83.92%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 84.09%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 84.25%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 84.41%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 84.56%   [EVAL] batch:  100 | acc: 50.00%,  total acc: 84.22%   [EVAL] batch:  101 | acc: 62.50%,  total acc: 84.01%   [EVAL] batch:  102 | acc: 62.50%,  total acc: 83.80%   [EVAL] batch:  103 | acc: 56.25%,  total acc: 83.53%   [EVAL] batch:  104 | acc: 87.50%,  total acc: 83.57%   [EVAL] batch:  105 | acc: 62.50%,  total acc: 83.37%   [EVAL] batch:  106 | acc: 31.25%,  total acc: 82.89%   [EVAL] batch:  107 | acc: 18.75%,  total acc: 82.29%   [EVAL] batch:  108 | acc: 37.50%,  total acc: 81.88%   [EVAL] batch:  109 | acc: 6.25%,  total acc: 81.19%   [EVAL] batch:  110 | acc: 43.75%,  total acc: 80.86%   [EVAL] batch:  111 | acc: 31.25%,  total acc: 80.41%   [EVAL] batch:  112 | acc: 50.00%,  total acc: 80.14%   [EVAL] batch:  113 | acc: 87.50%,  total acc: 80.21%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 80.38%   [EVAL] batch:  115 | acc: 87.50%,  total acc: 80.44%   [EVAL] batch:  116 | acc: 87.50%,  total acc: 80.50%   [EVAL] batch:  117 | acc: 93.75%,  total acc: 80.61%   [EVAL] batch:  118 | acc: 75.00%,  total acc: 80.57%   [EVAL] batch:  119 | acc: 6.25%,  total acc: 79.95%   [EVAL] batch:  120 | acc: 18.75%,  total acc: 79.44%   [EVAL] batch:  121 | acc: 6.25%,  total acc: 78.84%   [EVAL] batch:  122 | acc: 25.00%,  total acc: 78.40%   [EVAL] batch:  123 | acc: 6.25%,  total acc: 77.82%   [EVAL] batch:  124 | acc: 6.25%,  total acc: 77.25%   [EVAL] batch:  125 | acc: 75.00%,  total acc: 77.23%   [EVAL] batch:  126 | acc: 43.75%,  total acc: 76.97%   [EVAL] batch:  127 | acc: 43.75%,  total acc: 76.71%   [EVAL] batch:  128 | acc: 37.50%,  total acc: 76.41%   [EVAL] batch:  129 | acc: 62.50%,  total acc: 76.30%   [EVAL] batch:  130 | acc: 62.50%,  total acc: 76.19%   [EVAL] batch:  131 | acc: 87.50%,  total acc: 76.28%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 76.46%   [EVAL] batch:  133 | acc: 81.25%,  total acc: 76.49%   [EVAL] batch:  134 | acc: 81.25%,  total acc: 76.53%   [EVAL] batch:  135 | acc: 81.25%,  total acc: 76.56%   [EVAL] batch:  136 | acc: 100.00%,  total acc: 76.73%   [EVAL] batch:  137 | acc: 75.00%,  total acc: 76.72%   [EVAL] batch:  138 | acc: 43.75%,  total acc: 76.48%   [EVAL] batch:  139 | acc: 68.75%,  total acc: 76.43%   [EVAL] batch:  140 | acc: 81.25%,  total acc: 76.46%   [EVAL] batch:  141 | acc: 87.50%,  total acc: 76.54%   [EVAL] batch:  142 | acc: 62.50%,  total acc: 76.44%   [EVAL] batch:  143 | acc: 75.00%,  total acc: 76.43%   [EVAL] batch:  144 | acc: 75.00%,  total acc: 76.42%   [EVAL] batch:  145 | acc: 87.50%,  total acc: 76.50%   [EVAL] batch:  146 | acc: 75.00%,  total acc: 76.49%   [EVAL] batch:  147 | acc: 75.00%,  total acc: 76.48%   [EVAL] batch:  148 | acc: 68.75%,  total acc: 76.43%   [EVAL] batch:  149 | acc: 75.00%,  total acc: 76.42%   [EVAL] batch:  150 | acc: 87.50%,  total acc: 76.49%   [EVAL] batch:  151 | acc: 81.25%,  total acc: 76.52%   [EVAL] batch:  152 | acc: 87.50%,  total acc: 76.59%   [EVAL] batch:  153 | acc: 93.75%,  total acc: 76.70%   [EVAL] batch:  154 | acc: 68.75%,  total acc: 76.65%   [EVAL] batch:  155 | acc: 56.25%,  total acc: 76.52%   [EVAL] batch:  156 | acc: 75.00%,  total acc: 76.51%   [EVAL] batch:  157 | acc: 56.25%,  total acc: 76.38%   [EVAL] batch:  158 | acc: 37.50%,  total acc: 76.14%   [EVAL] batch:  159 | acc: 62.50%,  total acc: 76.05%   [EVAL] batch:  160 | acc: 43.75%,  total acc: 75.85%   [EVAL] batch:  161 | acc: 50.00%,  total acc: 75.69%   [EVAL] batch:  162 | acc: 56.25%,  total acc: 75.58%   [EVAL] batch:  163 | acc: 56.25%,  total acc: 75.46%   [EVAL] batch:  164 | acc: 62.50%,  total acc: 75.38%   [EVAL] batch:  165 | acc: 62.50%,  total acc: 75.30%   [EVAL] batch:  166 | acc: 68.75%,  total acc: 75.26%   [EVAL] batch:  167 | acc: 62.50%,  total acc: 75.19%   [EVAL] batch:  168 | acc: 56.25%,  total acc: 75.07%   [EVAL] batch:  169 | acc: 37.50%,  total acc: 74.85%   [EVAL] batch:  170 | acc: 37.50%,  total acc: 74.63%   [EVAL] batch:  171 | acc: 37.50%,  total acc: 74.42%   [EVAL] batch:  172 | acc: 43.75%,  total acc: 74.24%   [EVAL] batch:  173 | acc: 31.25%,  total acc: 73.99%   [EVAL] batch:  174 | acc: 50.00%,  total acc: 73.86%   [EVAL] batch:  175 | acc: 100.00%,  total acc: 74.01%   [EVAL] batch:  176 | acc: 100.00%,  total acc: 74.15%   [EVAL] batch:  177 | acc: 100.00%,  total acc: 74.30%   [EVAL] batch:  178 | acc: 100.00%,  total acc: 74.44%   [EVAL] batch:  179 | acc: 100.00%,  total acc: 74.58%   [EVAL] batch:  180 | acc: 100.00%,  total acc: 74.72%   [EVAL] batch:  181 | acc: 93.75%,  total acc: 74.83%   [EVAL] batch:  182 | acc: 68.75%,  total acc: 74.80%   [EVAL] batch:  183 | acc: 75.00%,  total acc: 74.80%   [EVAL] batch:  184 | acc: 87.50%,  total acc: 74.86%   [EVAL] batch:  185 | acc: 68.75%,  total acc: 74.83%   [EVAL] batch:  186 | acc: 81.25%,  total acc: 74.87%   [EVAL] batch:  187 | acc: 75.00%,  total acc: 74.87%   [EVAL] batch:  188 | acc: 50.00%,  total acc: 74.74%   [EVAL] batch:  189 | acc: 56.25%,  total acc: 74.64%   [EVAL] batch:  190 | acc: 50.00%,  total acc: 74.51%   [EVAL] batch:  191 | acc: 31.25%,  total acc: 74.28%   [EVAL] batch:  192 | acc: 56.25%,  total acc: 74.19%   [EVAL] batch:  193 | acc: 31.25%,  total acc: 73.97%   [EVAL] batch:  194 | acc: 81.25%,  total acc: 74.01%   [EVAL] batch:  195 | acc: 81.25%,  total acc: 74.04%   [EVAL] batch:  196 | acc: 87.50%,  total acc: 74.11%   [EVAL] batch:  197 | acc: 93.75%,  total acc: 74.21%   [EVAL] batch:  198 | acc: 87.50%,  total acc: 74.28%   [EVAL] batch:  199 | acc: 81.25%,  total acc: 74.31%   [EVAL] batch:  200 | acc: 75.00%,  total acc: 74.32%   [EVAL] batch:  201 | acc: 68.75%,  total acc: 74.29%   [EVAL] batch:  202 | acc: 62.50%,  total acc: 74.23%   [EVAL] batch:  203 | acc: 62.50%,  total acc: 74.17%   [EVAL] batch:  204 | acc: 75.00%,  total acc: 74.18%   [EVAL] batch:  205 | acc: 50.00%,  total acc: 74.06%   [EVAL] batch:  206 | acc: 100.00%,  total acc: 74.18%   [EVAL] batch:  207 | acc: 100.00%,  total acc: 74.31%   [EVAL] batch:  208 | acc: 100.00%,  total acc: 74.43%   [EVAL] batch:  209 | acc: 93.75%,  total acc: 74.52%   [EVAL] batch:  210 | acc: 100.00%,  total acc: 74.64%   [EVAL] batch:  211 | acc: 100.00%,  total acc: 74.76%   [EVAL] batch:  212 | acc: 87.50%,  total acc: 74.82%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 74.94%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 75.06%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 75.17%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 75.29%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 75.40%   [EVAL] batch:  218 | acc: 100.00%,  total acc: 75.51%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 75.62%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 75.74%   [EVAL] batch:  221 | acc: 93.75%,  total acc: 75.82%   [EVAL] batch:  222 | acc: 93.75%,  total acc: 75.90%   [EVAL] batch:  223 | acc: 100.00%,  total acc: 76.00%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 76.11%   [EVAL] batch:  225 | acc: 62.50%,  total acc: 76.05%   [EVAL] batch:  226 | acc: 62.50%,  total acc: 75.99%   [EVAL] batch:  227 | acc: 50.00%,  total acc: 75.88%   [EVAL] batch:  228 | acc: 43.75%,  total acc: 75.74%   [EVAL] batch:  229 | acc: 25.00%,  total acc: 75.52%   [EVAL] batch:  230 | acc: 43.75%,  total acc: 75.38%   [EVAL] batch:  231 | acc: 93.75%,  total acc: 75.46%   [EVAL] batch:  232 | acc: 93.75%,  total acc: 75.54%   [EVAL] batch:  233 | acc: 93.75%,  total acc: 75.61%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 75.72%   [EVAL] batch:  235 | acc: 93.75%,  total acc: 75.79%   [EVAL] batch:  236 | acc: 93.75%,  total acc: 75.87%   [EVAL] batch:  237 | acc: 81.25%,  total acc: 75.89%   [EVAL] batch:  238 | acc: 68.75%,  total acc: 75.86%   [EVAL] batch:  239 | acc: 62.50%,  total acc: 75.81%   [EVAL] batch:  240 | acc: 50.00%,  total acc: 75.70%   [EVAL] batch:  241 | acc: 68.75%,  total acc: 75.67%   [EVAL] batch:  242 | acc: 50.00%,  total acc: 75.57%   [EVAL] batch:  243 | acc: 37.50%,  total acc: 75.41%   [EVAL] batch:  244 | acc: 81.25%,  total acc: 75.43%   [EVAL] batch:  245 | acc: 75.00%,  total acc: 75.43%   [EVAL] batch:  246 | acc: 68.75%,  total acc: 75.40%   [EVAL] batch:  247 | acc: 81.25%,  total acc: 75.43%   [EVAL] batch:  248 | acc: 87.50%,  total acc: 75.48%   [EVAL] batch:  249 | acc: 75.00%,  total acc: 75.48%   [EVAL] batch:  250 | acc: 93.75%,  total acc: 75.55%   [EVAL] batch:  251 | acc: 81.25%,  total acc: 75.57%   [EVAL] batch:  252 | acc: 87.50%,  total acc: 75.62%   [EVAL] batch:  253 | acc: 75.00%,  total acc: 75.62%   [EVAL] batch:  254 | acc: 93.75%,  total acc: 75.69%   [EVAL] batch:  255 | acc: 100.00%,  total acc: 75.78%   [EVAL] batch:  256 | acc: 62.50%,  total acc: 75.73%   [EVAL] batch:  257 | acc: 81.25%,  total acc: 75.75%   [EVAL] batch:  258 | acc: 81.25%,  total acc: 75.77%   [EVAL] batch:  259 | acc: 75.00%,  total acc: 75.77%   [EVAL] batch:  260 | acc: 50.00%,  total acc: 75.67%   [EVAL] batch:  261 | acc: 68.75%,  total acc: 75.64%   [EVAL] batch:  262 | acc: 87.50%,  total acc: 75.69%   [EVAL] batch:  263 | acc: 81.25%,  total acc: 75.71%   [EVAL] batch:  264 | acc: 81.25%,  total acc: 75.73%   [EVAL] batch:  265 | acc: 81.25%,  total acc: 75.75%   [EVAL] batch:  266 | acc: 81.25%,  total acc: 75.77%   [EVAL] batch:  267 | acc: 75.00%,  total acc: 75.77%   [EVAL] batch:  268 | acc: 68.75%,  total acc: 75.74%   [EVAL] batch:  269 | acc: 56.25%,  total acc: 75.67%   [EVAL] batch:  270 | acc: 68.75%,  total acc: 75.65%   [EVAL] batch:  271 | acc: 62.50%,  total acc: 75.60%   [EVAL] batch:  272 | acc: 43.75%,  total acc: 75.48%   [EVAL] batch:  273 | acc: 81.25%,  total acc: 75.50%   [EVAL] batch:  274 | acc: 56.25%,  total acc: 75.43%   [EVAL] batch:  275 | acc: 100.00%,  total acc: 75.52%   [EVAL] batch:  276 | acc: 100.00%,  total acc: 75.61%   [EVAL] batch:  277 | acc: 100.00%,  total acc: 75.70%   [EVAL] batch:  278 | acc: 100.00%,  total acc: 75.78%   [EVAL] batch:  279 | acc: 100.00%,  total acc: 75.87%   [EVAL] batch:  280 | acc: 100.00%,  total acc: 75.96%   [EVAL] batch:  281 | acc: 75.00%,  total acc: 75.95%   [EVAL] batch:  282 | acc: 87.50%,  total acc: 75.99%   [EVAL] batch:  283 | acc: 68.75%,  total acc: 75.97%   [EVAL] batch:  284 | acc: 87.50%,  total acc: 76.01%   [EVAL] batch:  285 | acc: 62.50%,  total acc: 75.96%   [EVAL] batch:  286 | acc: 68.75%,  total acc: 75.94%   [EVAL] batch:  287 | acc: 68.75%,  total acc: 75.91%   [EVAL] batch:  288 | acc: 100.00%,  total acc: 75.99%   [EVAL] batch:  289 | acc: 100.00%,  total acc: 76.08%   [EVAL] batch:  290 | acc: 100.00%,  total acc: 76.16%   [EVAL] batch:  291 | acc: 100.00%,  total acc: 76.24%   [EVAL] batch:  292 | acc: 100.00%,  total acc: 76.32%   [EVAL] batch:  293 | acc: 93.75%,  total acc: 76.38%   [EVAL] batch:  294 | acc: 100.00%,  total acc: 76.46%   [EVAL] batch:  295 | acc: 100.00%,  total acc: 76.54%   [EVAL] batch:  296 | acc: 100.00%,  total acc: 76.62%   [EVAL] batch:  297 | acc: 100.00%,  total acc: 76.70%   [EVAL] batch:  298 | acc: 100.00%,  total acc: 76.78%   [EVAL] batch:  299 | acc: 100.00%,  total acc: 76.85%   [EVAL] batch:  300 | acc: 87.50%,  total acc: 76.89%   [EVAL] batch:  301 | acc: 87.50%,  total acc: 76.92%   [EVAL] batch:  302 | acc: 87.50%,  total acc: 76.96%   [EVAL] batch:  303 | acc: 93.75%,  total acc: 77.01%   [EVAL] batch:  304 | acc: 81.25%,  total acc: 77.03%   [EVAL] batch:  305 | acc: 87.50%,  total acc: 77.06%   [EVAL] batch:  306 | acc: 87.50%,  total acc: 77.10%   [EVAL] batch:  307 | acc: 87.50%,  total acc: 77.13%   [EVAL] batch:  308 | acc: 93.75%,  total acc: 77.18%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 77.26%   [EVAL] batch:  310 | acc: 100.00%,  total acc: 77.33%   [EVAL] batch:  311 | acc: 93.75%,  total acc: 77.38%   [EVAL] batch:  312 | acc: 50.00%,  total acc: 77.30%   
cur_acc:  ['0.9286', '0.7827', '0.7262', '0.7331', '0.8452']
his_acc:  ['0.9286', '0.8585', '0.7992', '0.7615', '0.7730']
Clustering into  29  clusters
Clusters:  [10  2 12  0 14 22 12  3  2 13 11  9  3 19  7 23 18 20  0 20 26 24  3 14
  1 13 10 22  4  5 21  5 20  6  6  5 20 28 13 27  0 17 10  2 25  4 11  8
  7 11 14 19  9  1 16 15  9  2  9  2]
Losses:  12.226540565490723 4.899513244628906 0.5856598615646362
CurrentTrain: epoch  0, batch     0 | loss: 12.2265406Losses:  9.403450012207031 3.3689136505126953 0.5595817565917969
CurrentTrain: epoch  0, batch     1 | loss: 9.4034500Losses:  10.590224266052246 3.4644758701324463 0.5413196682929993
CurrentTrain: epoch  0, batch     2 | loss: 10.5902243Losses:  7.07253885269165 -0.0 0.16472715139389038
CurrentTrain: epoch  0, batch     3 | loss: 7.0725389Losses:  10.363385200500488 4.247243881225586 0.6151936650276184
CurrentTrain: epoch  1, batch     0 | loss: 10.3633852Losses:  9.58462905883789 3.665907144546509 0.6213299036026001
CurrentTrain: epoch  1, batch     1 | loss: 9.5846291Losses:  8.469440460205078 2.96091890335083 0.5708504915237427
CurrentTrain: epoch  1, batch     2 | loss: 8.4694405Losses:  7.124814033508301 -0.0 0.12325236946344376
CurrentTrain: epoch  1, batch     3 | loss: 7.1248140Losses:  9.195778846740723 3.3035504817962646 0.6063003540039062
CurrentTrain: epoch  2, batch     0 | loss: 9.1957788Losses:  9.141079902648926 4.003019332885742 0.6341102719306946
CurrentTrain: epoch  2, batch     1 | loss: 9.1410799Losses:  10.309886932373047 4.711973190307617 0.6038329005241394
CurrentTrain: epoch  2, batch     2 | loss: 10.3098869Losses:  3.734632730484009 -0.0 0.10298366099596024
CurrentTrain: epoch  2, batch     3 | loss: 3.7346327Losses:  8.534408569335938 3.0471627712249756 0.6270316243171692
CurrentTrain: epoch  3, batch     0 | loss: 8.5344086Losses:  10.929219245910645 5.024890899658203 0.6495330333709717
CurrentTrain: epoch  3, batch     1 | loss: 10.9292192Losses:  6.287687301635742 2.4561967849731445 0.5141097903251648
CurrentTrain: epoch  3, batch     2 | loss: 6.2876873Losses:  5.554662704467773 -0.0 0.11242106556892395
CurrentTrain: epoch  3, batch     3 | loss: 5.5546627Losses:  8.215388298034668 3.249411106109619 0.5262040495872498
CurrentTrain: epoch  4, batch     0 | loss: 8.2153883Losses:  8.406839370727539 3.601874351501465 0.5418194532394409
CurrentTrain: epoch  4, batch     1 | loss: 8.4068394Losses:  8.214911460876465 4.1058783531188965 0.5201868414878845
CurrentTrain: epoch  4, batch     2 | loss: 8.2149115Losses:  2.375115156173706 -0.0 0.10578940808773041
CurrentTrain: epoch  4, batch     3 | loss: 2.3751152Losses:  7.939556121826172 3.2031328678131104 0.6052369475364685
CurrentTrain: epoch  5, batch     0 | loss: 7.9395561Losses:  7.726651191711426 3.9849438667297363 0.5000074505805969
CurrentTrain: epoch  5, batch     1 | loss: 7.7266512Losses:  6.999648094177246 2.882983684539795 0.6337707042694092
CurrentTrain: epoch  5, batch     2 | loss: 6.9996481Losses:  5.769956588745117 -0.0 0.10501063615083694
CurrentTrain: epoch  5, batch     3 | loss: 5.7699566Losses:  6.450562000274658 2.4699974060058594 0.6090774536132812
CurrentTrain: epoch  6, batch     0 | loss: 6.4505620Losses:  7.883962631225586 3.3424134254455566 0.5775858759880066
CurrentTrain: epoch  6, batch     1 | loss: 7.8839626Losses:  6.110996723175049 2.206552505493164 0.6048907041549683
CurrentTrain: epoch  6, batch     2 | loss: 6.1109967Losses:  2.1536827087402344 -0.0 0.09905286133289337
CurrentTrain: epoch  6, batch     3 | loss: 2.1536827Losses:  9.617565155029297 5.848411560058594 0.44888219237327576
CurrentTrain: epoch  7, batch     0 | loss: 9.6175652Losses:  6.96456241607666 3.5147218704223633 0.4871891438961029
CurrentTrain: epoch  7, batch     1 | loss: 6.9645624Losses:  6.614500999450684 2.9595284461975098 0.4729163646697998
CurrentTrain: epoch  7, batch     2 | loss: 6.6145010Losses:  7.928923606872559 -0.0 -0.0
CurrentTrain: epoch  7, batch     3 | loss: 7.9289236Losses:  6.640238285064697 2.8793935775756836 0.5161023139953613
CurrentTrain: epoch  8, batch     0 | loss: 6.6402383Losses:  5.632628440856934 2.2447409629821777 0.6119396686553955
CurrentTrain: epoch  8, batch     1 | loss: 5.6326284Losses:  6.621562957763672 2.8624372482299805 0.4092434048652649
CurrentTrain: epoch  8, batch     2 | loss: 6.6215630Losses:  2.1821584701538086 -0.0 0.10307609289884567
CurrentTrain: epoch  8, batch     3 | loss: 2.1821585Losses:  6.178080081939697 3.7489800453186035 0.38517484068870544
CurrentTrain: epoch  9, batch     0 | loss: 6.1780801Losses:  6.5542144775390625 3.456913471221924 0.5088965892791748
CurrentTrain: epoch  9, batch     1 | loss: 6.5542145Losses:  8.9802827835083 4.648627758026123 0.5170941948890686
CurrentTrain: epoch  9, batch     2 | loss: 8.9802828Losses:  5.315607070922852 -0.0 0.12081266194581985
CurrentTrain: epoch  9, batch     3 | loss: 5.3156071
#### located on terrain feature ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "located on terrain feature" means located on the specified physical/geophysical feature.
Here is an example:
Relation: located on terrain feature
Context: it is situated in the historic lower lusatia region , about south of beeskow , and north of cottbus .
Head Entity: cottbus
Tail Entity: lower lusatia
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "located on terrain feature":
Relation: located on terrain feature  
Context: The ancient ruins are found on the slopes of the majestic Andes mountains, providing a breathtaking view of the valley below.  
Head Entity: Andes mountains  
Tail Entity: ancient ruins  

Relation: located on terrain feature  
Context: The village is nestled in the heart of the lush Black Forest, surrounded by towering trees and serene lakes.  
Head Entity: Black Forest  
Tail Entity: village  
#### participant of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "participant of" means event in which a person or organization was/is a participant.
Here is an example:
Relation: participant of
Context: tiago venâncio ( born august 19 , 1987 , setúbal ) is a freestyle swimmer from portugal , who represented his home country at the 2004 summer olympics in athens , greece .
Head Entity: tiago venâncio
Tail Entity: 2004 summer olympics
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "participant of":
Relation: participant of  
Context: the united nations climate change conference, held in paris in 2015, saw numerous world leaders and activists come together to discuss global warming and its impacts.  
Head Entity: united nations climate change conference  
Tail Entity: paris agreement  

Relation: participant of  
Context: the 2022 fifa world cup in qatar featured teams from around the globe competing for the prestigious trophy, with fans eagerly supporting their national teams.  
Head Entity: 2022 fifa world cup  
Tail Entity: qatar
#### director ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "director" means director of film, TV-series, stageplay, video game or similar.
Here is an example:
Relation: director
Context: the festival opened with " whiplash " directed by damien chazelle and closed with musical drama " rudderless " directed by william h. macy .
Head Entity: whiplash
Tail Entity: damien chazelle
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "director":
Relation: director  
Context: the critically acclaimed series "breaking bad" was brought to life by director vince gilligan, while the spin-off "better call saul" was directed by peter gould.  
Head Entity: breaking bad  
Tail Entity: vince gilligan  

Relation: director  
Context: the animated feature "spider-man: into the spider-verse" was directed by bob persichetti, peter ramsey, and rodney rothman, showcasing a unique visual style.  
Head Entity: spider-man: into the spider-verse  
Tail Entity: bob persichetti  
#### owned by ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "owned by" means owner of the subject.
Here is an example:
Relation: owned by
Context: as of april 2013 , sony isp , known as so - net , released a new fibre service to tokyo for small businesses and homes and also made it available to six surrounding prefectures .
Head Entity: so - net
Tail Entity: sony
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "owned by":
Relation: owned by  
Context: In 2020, the popular smartphone brand OnePlus was acquired by BBK Electronics, which also owns other brands like Oppo and Vivo.  
Head Entity: OnePlus  
Tail Entity: BBK Electronics  

Relation: owned by  
Context: The famous luxury fashion house Gucci is a subsidiary of the Kering Group, which manages several other high-end brands.  
Head Entity: Gucci  
Tail Entity: Kering Group  
#### part of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "part of" means object of which the subject is a part.
Here is an example:
Relation: part of
Context: remains of taxa that can be confidently assigned to istiodactylidae have been found in the uk and china , in rocks dating from the early cretaceous period ( barremian to aptian stage ) .
Head Entity: barremian
Tail Entity: early cretaceous
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "part of":
Relation: part of  
Context: The Great Wall of China is a series of fortifications made of various materials, and it is part of the UNESCO World Heritage Sites.  
Head Entity: Great Wall of China  
Tail Entity: UNESCO World Heritage Sites  

Relation: part of  
Context: The human heart is a vital organ that is part of the circulatory system, responsible for pumping blood throughout the body.  
Head Entity: human heart  
Tail Entity: circulatory system  
#### occupant ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "occupant" means person or organization occupying property.
Here is an example:
Relation: occupant
Context: the brewery closed in 1996 and is now the site of the halliwell jones stadium , home of warrington wolves , as well as a tesco store .
Head Entity: halliwell jones stadium
Tail Entity: warrington wolves
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "occupant":
Relation: occupant  
Context: The historic building was transformed into a cultural center, which now hosts the local art gallery and serves as the headquarters for the community theater group.  
Head Entity: cultural center  
Tail Entity: community theater group  

Relation: occupant  
Context: After the renovation, the old factory became a vibrant co-working space, attracting freelancers and startups from the tech industry.  
Head Entity: co-working space  
Tail Entity: tech industry
#### architect ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "architect" means person or architectural firm responsible for designing this building.
Here is an example:
Relation: architect
Context: reading town hall was built in several phases between 1786 and 1897 , although the principal facade was designed by alfred waterhouse in 1875 .
Head Entity: reading town hall
Tail Entity: alfred waterhouse
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "architect":
Relation: architect  
Context: the new art museum was designed by renowned architect zaha hadid, whose innovative style has transformed modern architecture.  
Head Entity: new art museum  
Tail Entity: zaha hadid  

Relation: architect  
Context: the iconic sydney opera house was the brainchild of architect jørn utzon, who won the design competition in 1957.  
Head Entity: sydney opera house  
Tail Entity: jørn utzon  
#### residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "residence" means the place where the person is or has been, resident.
Here is an example:
Relation: residence
Context: cyrus alexander was born in pennsylvania , and his family soon moved to illinois .
Head Entity: cyrus alexander
Tail Entity: illinois
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "residence":
Relation: residence  
Context: after living in new york for several years, emily decided to relocate to california for a fresh start.  
Head Entity: emily  
Tail Entity: california  

Relation: residence  
Context: during his childhood, michael spent most of his time in texas before moving to florida as a teenager.  
Head Entity: michael  
Tail Entity: florida  
#### composer ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "composer" means person who wrote the music.
Here is an example:
Relation: composer
Context: at a young age he was an accomplished cellist , performing at age seventeen the solo part of richard strauss 's " don quixote " .
Head Entity: don quixote
Tail Entity: richard strauss
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "composer":
Relation: composer  
Context: The renowned composer Ludwig van Beethoven created some of the most celebrated symphonies in classical music history.  
Head Entity: symphonies  
Tail Entity: Ludwig van Beethoven  

Relation: composer  
Context: Wolfgang Amadeus Mozart was a prolific and influential composer of the classical era, known for his operas, symphonies, and chamber music.  
Head Entity: operas  
Tail Entity: Wolfgang Amadeus Mozart  
#### location ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "location" means location of the object, structure or event.
Here is an example:
Relation: location
Context: one polish archer qualified for the women 's individual recurve by obtaining one of the eight olympic places available from the 2015 world archery championships in copenhagen , denmark .
Head Entity: 2015 world archery championships
Tail Entity: copenhagen
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "location":
Relation: location  
Context: The annual music festival will take place in the vibrant city of Austin, Texas, attracting thousands of fans from around the world.  
Head Entity: annual music festival  
Tail Entity: Austin, Texas  

Relation: location  
Context: The historic battle was fought near the banks of the Potomac River, a significant site in American history.  
Head Entity: historic battle  
Tail Entity: Potomac River  
Losses:  3.767685890197754 0.2671399712562561 0.9335845708847046
MemoryTrain:  epoch  0, batch     0 | loss: 3.7676859Losses:  2.8082847595214844 -0.0 0.8168798685073853
MemoryTrain:  epoch  0, batch     1 | loss: 2.8082848Losses:  3.368434190750122 0.2625790536403656 0.932574450969696
MemoryTrain:  epoch  0, batch     2 | loss: 3.3684342Losses:  3.86357045173645 -0.0 0.9039208292961121
MemoryTrain:  epoch  0, batch     3 | loss: 3.8635705Losses:  4.18914794921875 0.4231024980545044 0.8987183570861816
MemoryTrain:  epoch  0, batch     4 | loss: 4.1891479Losses:  3.7870984077453613 0.25256991386413574 0.955978274345398
MemoryTrain:  epoch  0, batch     5 | loss: 3.7870984Losses:  3.0180320739746094 -0.0 0.9160725474357605
MemoryTrain:  epoch  0, batch     6 | loss: 3.0180321Losses:  4.395143032073975 -0.0 1.0558500289916992
MemoryTrain:  epoch  0, batch     7 | loss: 4.3951430Losses:  4.931878566741943 -0.0 1.054125428199768
MemoryTrain:  epoch  0, batch     8 | loss: 4.9318786Losses:  3.065906047821045 0.5019179582595825 0.8685768842697144
MemoryTrain:  epoch  0, batch     9 | loss: 3.0659060Losses:  4.279876708984375 0.24394308030605316 1.050276279449463
MemoryTrain:  epoch  0, batch    10 | loss: 4.2798767Losses:  2.166318416595459 -0.0 0.36439698934555054
MemoryTrain:  epoch  0, batch    11 | loss: 2.1663184Losses:  3.677884340286255 0.7633057236671448 0.876780092716217
MemoryTrain:  epoch  1, batch     0 | loss: 3.6778843Losses:  3.6728899478912354 -0.0 0.9951269030570984
MemoryTrain:  epoch  1, batch     1 | loss: 3.6728899Losses:  3.6830732822418213 0.5635203123092651 0.8737252354621887
MemoryTrain:  epoch  1, batch     2 | loss: 3.6830733Losses:  3.1991376876831055 0.24529233574867249 1.0155246257781982
MemoryTrain:  epoch  1, batch     3 | loss: 3.1991377Losses:  3.3430020809173584 -0.0 0.9771287441253662
MemoryTrain:  epoch  1, batch     4 | loss: 3.3430021Losses:  3.9786887168884277 0.24964755773544312 0.9177314639091492
MemoryTrain:  epoch  1, batch     5 | loss: 3.9786887Losses:  3.329622507095337 0.509623110294342 1.00388765335083
MemoryTrain:  epoch  1, batch     6 | loss: 3.3296225Losses:  2.8422255516052246 0.5114161372184753 0.9618934392929077
MemoryTrain:  epoch  1, batch     7 | loss: 2.8422256Losses:  4.6145782470703125 1.0583136081695557 0.7896475195884705
MemoryTrain:  epoch  1, batch     8 | loss: 4.6145782Losses:  3.3097984790802 0.2561526596546173 0.8980699181556702
MemoryTrain:  epoch  1, batch     9 | loss: 3.3097985Losses:  3.521705150604248 -0.0 1.018246054649353
MemoryTrain:  epoch  1, batch    10 | loss: 3.5217052Losses:  2.7897849082946777 -0.0 0.3429260849952698
MemoryTrain:  epoch  1, batch    11 | loss: 2.7897849Losses:  2.6101040840148926 0.26421013474464417 0.7714352011680603
MemoryTrain:  epoch  2, batch     0 | loss: 2.6101041Losses:  3.0872457027435303 0.7931206226348877 0.861646831035614
MemoryTrain:  epoch  2, batch     1 | loss: 3.0872457Losses:  2.5335986614227295 0.2576104402542114 0.9693852066993713
MemoryTrain:  epoch  2, batch     2 | loss: 2.5335987Losses:  3.270172119140625 -0.0 1.0127124786376953
MemoryTrain:  epoch  2, batch     3 | loss: 3.2701721Losses:  2.944304943084717 -0.0 0.9551267027854919
MemoryTrain:  epoch  2, batch     4 | loss: 2.9443049Losses:  3.2949156761169434 0.485093355178833 0.968751072883606
MemoryTrain:  epoch  2, batch     5 | loss: 3.2949157Losses:  2.5977649688720703 0.4810701608657837 0.8518385887145996
MemoryTrain:  epoch  2, batch     6 | loss: 2.5977650Losses:  2.9163622856140137 0.23674854636192322 0.8635901212692261
MemoryTrain:  epoch  2, batch     7 | loss: 2.9163623Losses:  5.440433502197266 1.7097653150558472 0.890569269657135
MemoryTrain:  epoch  2, batch     8 | loss: 5.4404335Losses:  2.678109884262085 0.22883999347686768 0.8636338114738464
MemoryTrain:  epoch  2, batch     9 | loss: 2.6781099Losses:  3.2041163444519043 0.3162175714969635 1.0028616189956665
MemoryTrain:  epoch  2, batch    10 | loss: 3.2041163Losses:  1.5873222351074219 -0.0 0.31267112493515015
MemoryTrain:  epoch  2, batch    11 | loss: 1.5873222Losses:  2.6011624336242676 0.48578158020973206 0.8462361097335815
MemoryTrain:  epoch  3, batch     0 | loss: 2.6011624Losses:  3.7383062839508057 0.5278366208076477 0.8997316360473633
MemoryTrain:  epoch  3, batch     1 | loss: 3.7383063Losses:  2.879178762435913 -0.0 1.0704741477966309
MemoryTrain:  epoch  3, batch     2 | loss: 2.8791788Losses:  3.4188857078552246 0.534938633441925 0.8078414797782898
MemoryTrain:  epoch  3, batch     3 | loss: 3.4188857Losses:  2.921376943588257 0.5334415435791016 0.7733864784240723
MemoryTrain:  epoch  3, batch     4 | loss: 2.9213769Losses:  2.7869231700897217 0.27065011858940125 0.9539710879325867
MemoryTrain:  epoch  3, batch     5 | loss: 2.7869232Losses:  2.4774999618530273 0.2518543601036072 0.8986401557922363
MemoryTrain:  epoch  3, batch     6 | loss: 2.4775000Losses:  2.760986328125 0.5413768887519836 0.8419417142868042
MemoryTrain:  epoch  3, batch     7 | loss: 2.7609863Losses:  3.743741750717163 0.7589187622070312 0.8405343890190125
MemoryTrain:  epoch  3, batch     8 | loss: 3.7437418Losses:  2.587639808654785 0.4709354043006897 0.8793128728866577
MemoryTrain:  epoch  3, batch     9 | loss: 2.5876398Losses:  3.294948101043701 0.49200621247291565 0.8708007335662842
MemoryTrain:  epoch  3, batch    10 | loss: 3.2949481Losses:  1.6650092601776123 0.2444704920053482 0.23699361085891724
MemoryTrain:  epoch  3, batch    11 | loss: 1.6650093Losses:  2.5863254070281982 -0.0 1.030880093574524
MemoryTrain:  epoch  4, batch     0 | loss: 2.5863254Losses:  2.861360549926758 0.3017846345901489 0.8330463171005249
MemoryTrain:  epoch  4, batch     1 | loss: 2.8613605Losses:  2.579500436782837 -0.0 0.9703608751296997
MemoryTrain:  epoch  4, batch     2 | loss: 2.5795004Losses:  3.322626829147339 1.0028995275497437 0.8059279322624207
MemoryTrain:  epoch  4, batch     3 | loss: 3.3226268Losses:  3.4594850540161133 0.3624984920024872 0.9253250360488892
MemoryTrain:  epoch  4, batch     4 | loss: 3.4594851Losses:  3.0109143257141113 0.7213300466537476 0.7951564192771912
MemoryTrain:  epoch  4, batch     5 | loss: 3.0109143Losses:  2.788738250732422 0.5521526336669922 0.7892888784408569
MemoryTrain:  epoch  4, batch     6 | loss: 2.7887383Losses:  3.1169724464416504 0.7374780774116516 0.9129542112350464
MemoryTrain:  epoch  4, batch     7 | loss: 3.1169724Losses:  2.911147356033325 0.7351702451705933 0.8980423808097839
MemoryTrain:  epoch  4, batch     8 | loss: 2.9111474Losses:  3.063060998916626 0.6963005661964417 0.852449357509613
MemoryTrain:  epoch  4, batch     9 | loss: 3.0630610Losses:  2.8828229904174805 0.2527848780155182 0.9076886177062988
MemoryTrain:  epoch  4, batch    10 | loss: 2.8828230Losses:  1.4601085186004639 -0.0 0.21398401260375977
MemoryTrain:  epoch  4, batch    11 | loss: 1.4601085Losses:  2.586735248565674 0.2517361044883728 0.9020562171936035
MemoryTrain:  epoch  5, batch     0 | loss: 2.5867352Losses:  2.7525885105133057 0.5082733631134033 1.012596607208252
MemoryTrain:  epoch  5, batch     1 | loss: 2.7525885Losses:  2.484088659286499 0.24443072080612183 0.9000320434570312
MemoryTrain:  epoch  5, batch     2 | loss: 2.4840887Losses:  2.6156959533691406 0.4830387234687805 0.779300332069397
MemoryTrain:  epoch  5, batch     3 | loss: 2.6156960Losses:  2.8403842449188232 0.5073192119598389 0.7730709910392761
MemoryTrain:  epoch  5, batch     4 | loss: 2.8403842Losses:  3.7611191272735596 0.6384580135345459 1.0000123977661133
MemoryTrain:  epoch  5, batch     5 | loss: 3.7611191Losses:  2.6756060123443604 -0.0 1.0305079221725464
MemoryTrain:  epoch  5, batch     6 | loss: 2.6756060Losses:  3.991203784942627 1.070814609527588 0.8776379823684692
MemoryTrain:  epoch  5, batch     7 | loss: 3.9912038Losses:  2.5819098949432373 0.49588704109191895 0.8450619578361511
MemoryTrain:  epoch  5, batch     8 | loss: 2.5819099Losses:  2.868124008178711 0.5236558318138123 0.9688165187835693
MemoryTrain:  epoch  5, batch     9 | loss: 2.8681240Losses:  2.997868061065674 0.2496516853570938 1.0163578987121582
MemoryTrain:  epoch  5, batch    10 | loss: 2.9978681Losses:  1.570703387260437 -0.0 0.3455392122268677
MemoryTrain:  epoch  5, batch    11 | loss: 1.5707034Losses:  2.4246878623962402 0.24325689673423767 0.9095931053161621
MemoryTrain:  epoch  6, batch     0 | loss: 2.4246879Losses:  2.507711410522461 0.2593032717704773 1.0145336389541626
MemoryTrain:  epoch  6, batch     1 | loss: 2.5077114Losses:  2.428402900695801 -0.0 0.9284389019012451
MemoryTrain:  epoch  6, batch     2 | loss: 2.4284029Losses:  2.11057710647583 -0.0 0.8909522294998169
MemoryTrain:  epoch  6, batch     3 | loss: 2.1105771Losses:  2.6101861000061035 0.5268406867980957 0.8470176458358765
MemoryTrain:  epoch  6, batch     4 | loss: 2.6101861Losses:  2.6707563400268555 -0.0 0.9266778826713562
MemoryTrain:  epoch  6, batch     5 | loss: 2.6707563Losses:  2.8830158710479736 0.5376601815223694 0.905216634273529
MemoryTrain:  epoch  6, batch     6 | loss: 2.8830159Losses:  3.372889757156372 0.7542319297790527 0.8441651463508606
MemoryTrain:  epoch  6, batch     7 | loss: 3.3728898Losses:  3.141767740249634 0.7455887198448181 0.8130989074707031
MemoryTrain:  epoch  6, batch     8 | loss: 3.1417677Losses:  3.6576035022735596 1.1084659099578857 0.8591737747192383
MemoryTrain:  epoch  6, batch     9 | loss: 3.6576035Losses:  2.6972029209136963 0.23675261437892914 0.9504510760307312
MemoryTrain:  epoch  6, batch    10 | loss: 2.6972029Losses:  1.5069057941436768 -0.0 0.323625385761261
MemoryTrain:  epoch  6, batch    11 | loss: 1.5069058Losses:  2.3743135929107666 -0.0 1.0563466548919678
MemoryTrain:  epoch  7, batch     0 | loss: 2.3743136Losses:  2.4172801971435547 0.48769283294677734 0.7195190191268921
MemoryTrain:  epoch  7, batch     1 | loss: 2.4172802Losses:  2.5991454124450684 0.5417183041572571 0.8280497789382935
MemoryTrain:  epoch  7, batch     2 | loss: 2.5991454Losses:  3.5802178382873535 1.032454252243042 0.8855153322219849
MemoryTrain:  epoch  7, batch     3 | loss: 3.5802178Losses:  2.77351975440979 0.23922112584114075 0.8173601031303406
MemoryTrain:  epoch  7, batch     4 | loss: 2.7735198Losses:  2.551644802093506 0.2215431034564972 0.9533654451370239
MemoryTrain:  epoch  7, batch     5 | loss: 2.5516448Losses:  2.8185007572174072 0.28794583678245544 0.9652597308158875
MemoryTrain:  epoch  7, batch     6 | loss: 2.8185008Losses:  2.723008394241333 0.22989492118358612 1.0145223140716553
MemoryTrain:  epoch  7, batch     7 | loss: 2.7230084Losses:  2.6965553760528564 0.5262863636016846 0.9629412293434143
MemoryTrain:  epoch  7, batch     8 | loss: 2.6965554Losses:  2.8213729858398438 0.44349533319473267 0.9736922979354858
MemoryTrain:  epoch  7, batch     9 | loss: 2.8213730Losses:  2.9278032779693604 0.2511449456214905 1.0261621475219727
MemoryTrain:  epoch  7, batch    10 | loss: 2.9278033Losses:  1.7013185024261475 0.23547640442848206 0.24073469638824463
MemoryTrain:  epoch  7, batch    11 | loss: 1.7013185Losses:  2.611830472946167 0.2505168616771698 1.0018243789672852
MemoryTrain:  epoch  8, batch     0 | loss: 2.6118305Losses:  2.4355688095092773 0.24867576360702515 1.0093607902526855
MemoryTrain:  epoch  8, batch     1 | loss: 2.4355688Losses:  2.8439719676971436 0.48209989070892334 0.9017764925956726
MemoryTrain:  epoch  8, batch     2 | loss: 2.8439720Losses:  2.3697872161865234 0.25147420167922974 0.9462753534317017
MemoryTrain:  epoch  8, batch     3 | loss: 2.3697872Losses:  2.946589946746826 -0.0 1.0187852382659912
MemoryTrain:  epoch  8, batch     4 | loss: 2.9465899Losses:  2.4890151023864746 0.25175121426582336 0.9811596870422363
MemoryTrain:  epoch  8, batch     5 | loss: 2.4890151Losses:  3.0266127586364746 0.7917824387550354 0.8178186416625977
MemoryTrain:  epoch  8, batch     6 | loss: 3.0266128Losses:  2.847386121749878 0.7559658288955688 0.9169635772705078
MemoryTrain:  epoch  8, batch     7 | loss: 2.8473861Losses:  2.848025321960449 0.5138564109802246 0.8593829870223999
MemoryTrain:  epoch  8, batch     8 | loss: 2.8480253Losses:  2.216792345046997 -0.0 0.9545544981956482
MemoryTrain:  epoch  8, batch     9 | loss: 2.2167923Losses:  2.4863719940185547 -0.0 0.9721692800521851
MemoryTrain:  epoch  8, batch    10 | loss: 2.4863720Losses:  1.8188533782958984 -0.0 0.30489581823349
MemoryTrain:  epoch  8, batch    11 | loss: 1.8188534Losses:  2.609023094177246 0.49889376759529114 0.9444977045059204
MemoryTrain:  epoch  9, batch     0 | loss: 2.6090231Losses:  2.5507683753967285 0.26137712597846985 0.9096614122390747
MemoryTrain:  epoch  9, batch     1 | loss: 2.5507684Losses:  2.6094515323638916 0.23568065464496613 0.8327321410179138
MemoryTrain:  epoch  9, batch     2 | loss: 2.6094515Losses:  2.489870309829712 0.2611329257488251 0.9374834895133972
MemoryTrain:  epoch  9, batch     3 | loss: 2.4898703Losses:  2.5983500480651855 0.7198368310928345 0.7040097117424011
MemoryTrain:  epoch  9, batch     4 | loss: 2.5983500Losses:  2.6268820762634277 0.4924699366092682 0.9503000974655151
MemoryTrain:  epoch  9, batch     5 | loss: 2.6268821Losses:  2.714014768600464 0.24945499002933502 0.9113298654556274
MemoryTrain:  epoch  9, batch     6 | loss: 2.7140148Losses:  2.729616641998291 0.49403107166290283 0.7718152403831482
MemoryTrain:  epoch  9, batch     7 | loss: 2.7296166Losses:  2.279933452606201 -0.0 0.9648298621177673
MemoryTrain:  epoch  9, batch     8 | loss: 2.2799335Losses:  2.3574111461639404 0.21432559192180634 0.8534845113754272
MemoryTrain:  epoch  9, batch     9 | loss: 2.3574111Losses:  3.0387110710144043 0.6035256385803223 0.8493844866752625
MemoryTrain:  epoch  9, batch    10 | loss: 3.0387111Losses:  1.594138264656067 -0.0 0.3398700952529907
MemoryTrain:  epoch  9, batch    11 | loss: 1.5941383
[EVAL] batch:    0 | acc: 43.75%,  total acc: 43.75%   [EVAL] batch:    1 | acc: 37.50%,  total acc: 40.62%   [EVAL] batch:    2 | acc: 43.75%,  total acc: 41.67%   [EVAL] batch:    3 | acc: 37.50%,  total acc: 40.62%   [EVAL] batch:    4 | acc: 37.50%,  total acc: 40.00%   [EVAL] batch:    5 | acc: 50.00%,  total acc: 41.67%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 46.43%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 50.78%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 55.56%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 58.75%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 60.80%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 61.98%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 63.94%   [EVAL] batch:   13 | acc: 75.00%,  total acc: 64.73%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 65.42%   [EVAL] batch:   15 | acc: 87.50%,  total acc: 66.80%   [EVAL] batch:   16 | acc: 87.50%,  total acc: 68.01%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 68.06%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 67.76%   [EVAL] batch:   19 | acc: 31.25%,  total acc: 65.94%   [EVAL] batch:   20 | acc: 18.75%,  total acc: 63.69%   [EVAL] batch:   21 | acc: 6.25%,  total acc: 61.08%   [EVAL] batch:   22 | acc: 25.00%,  total acc: 59.51%   [EVAL] batch:   23 | acc: 18.75%,  total acc: 57.81%   [EVAL] batch:   24 | acc: 25.00%,  total acc: 56.50%   [EVAL] batch:   25 | acc: 6.25%,  total acc: 54.57%   [EVAL] batch:   26 | acc: 12.50%,  total acc: 53.01%   [EVAL] batch:   27 | acc: 31.25%,  total acc: 52.23%   [EVAL] batch:   28 | acc: 0.00%,  total acc: 50.43%   [EVAL] batch:   29 | acc: 12.50%,  total acc: 49.17%   [EVAL] batch:   30 | acc: 0.00%,  total acc: 47.58%   [EVAL] batch:   31 | acc: 68.75%,  total acc: 48.24%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 49.62%   [EVAL] batch:   33 | acc: 81.25%,  total acc: 50.55%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 51.96%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 52.95%   [EVAL] batch:   36 | acc: 87.50%,  total acc: 53.89%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 55.10%   [EVAL] batch:   38 | acc: 87.50%,  total acc: 55.93%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 57.03%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 58.08%   [EVAL] batch:   41 | acc: 81.25%,  total acc: 58.63%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 59.59%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 60.37%   [EVAL] batch:   44 | acc: 75.00%,  total acc: 60.69%   [EVAL] batch:   45 | acc: 50.00%,  total acc: 60.46%   [EVAL] batch:   46 | acc: 43.75%,  total acc: 60.11%   [EVAL] batch:   47 | acc: 68.75%,  total acc: 60.29%   [EVAL] batch:   48 | acc: 81.25%,  total acc: 60.71%   [EVAL] batch:   49 | acc: 56.25%,  total acc: 60.62%   [EVAL] batch:   50 | acc: 12.50%,  total acc: 59.68%   [EVAL] batch:   51 | acc: 43.75%,  total acc: 59.38%   [EVAL] batch:   52 | acc: 43.75%,  total acc: 59.08%   [EVAL] batch:   53 | acc: 50.00%,  total acc: 58.91%   [EVAL] batch:   54 | acc: 43.75%,  total acc: 58.64%   [EVAL] batch:   55 | acc: 62.50%,  total acc: 58.71%   [EVAL] batch:   56 | acc: 43.75%,  total acc: 58.44%   [EVAL] batch:   57 | acc: 50.00%,  total acc: 58.30%   [EVAL] batch:   58 | acc: 37.50%,  total acc: 57.94%   [EVAL] batch:   59 | acc: 62.50%,  total acc: 58.02%   [EVAL] batch:   60 | acc: 50.00%,  total acc: 57.89%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 58.37%   [EVAL] batch:   62 | acc: 25.00%,  total acc: 57.84%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 80.00%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 82.29%   [EVAL] batch:    6 | acc: 50.00%,  total acc: 77.68%   [EVAL] batch:    7 | acc: 62.50%,  total acc: 75.78%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 75.00%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 73.75%   [EVAL] batch:   10 | acc: 50.00%,  total acc: 71.59%   [EVAL] batch:   11 | acc: 43.75%,  total acc: 69.27%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 70.19%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 71.88%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 73.33%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 76.10%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 77.08%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 78.29%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 78.44%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 79.17%   [EVAL] batch:   21 | acc: 75.00%,  total acc: 78.98%   [EVAL] batch:   22 | acc: 75.00%,  total acc: 78.80%   [EVAL] batch:   23 | acc: 81.25%,  total acc: 78.91%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 79.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 80.05%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 80.79%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 81.47%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 81.90%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 82.50%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 83.06%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 83.59%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 83.90%   [EVAL] batch:   33 | acc: 62.50%,  total acc: 83.27%   [EVAL] batch:   34 | acc: 68.75%,  total acc: 82.86%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 82.99%   [EVAL] batch:   36 | acc: 87.50%,  total acc: 83.11%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 83.39%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 83.81%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 84.22%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 84.60%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 84.97%   [EVAL] batch:   42 | acc: 87.50%,  total acc: 85.03%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 85.23%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 85.28%   [EVAL] batch:   45 | acc: 81.25%,  total acc: 85.19%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 85.11%   [EVAL] batch:   47 | acc: 75.00%,  total acc: 84.90%   [EVAL] batch:   48 | acc: 81.25%,  total acc: 84.82%   [EVAL] batch:   49 | acc: 81.25%,  total acc: 84.75%   [EVAL] batch:   50 | acc: 81.25%,  total acc: 84.68%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 84.74%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 84.91%   [EVAL] batch:   53 | acc: 62.50%,  total acc: 84.49%   [EVAL] batch:   54 | acc: 56.25%,  total acc: 83.98%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 84.04%   [EVAL] batch:   56 | acc: 62.50%,  total acc: 83.66%   [EVAL] batch:   57 | acc: 56.25%,  total acc: 83.19%   [EVAL] batch:   58 | acc: 75.00%,  total acc: 83.05%   [EVAL] batch:   59 | acc: 87.50%,  total acc: 83.12%   [EVAL] batch:   60 | acc: 87.50%,  total acc: 83.20%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 82.96%   [EVAL] batch:   62 | acc: 87.50%,  total acc: 83.04%   [EVAL] batch:   63 | acc: 81.25%,  total acc: 83.01%   [EVAL] batch:   64 | acc: 75.00%,  total acc: 82.88%   [EVAL] batch:   65 | acc: 93.75%,  total acc: 83.05%   [EVAL] batch:   66 | acc: 75.00%,  total acc: 82.93%   [EVAL] batch:   67 | acc: 68.75%,  total acc: 82.72%   [EVAL] batch:   68 | acc: 81.25%,  total acc: 82.70%   [EVAL] batch:   69 | acc: 87.50%,  total acc: 82.77%   [EVAL] batch:   70 | acc: 68.75%,  total acc: 82.57%   [EVAL] batch:   71 | acc: 62.50%,  total acc: 82.29%   [EVAL] batch:   72 | acc: 81.25%,  total acc: 82.28%   [EVAL] batch:   73 | acc: 75.00%,  total acc: 82.18%   [EVAL] batch:   74 | acc: 81.25%,  total acc: 82.17%   [EVAL] batch:   75 | acc: 75.00%,  total acc: 82.07%   [EVAL] batch:   76 | acc: 81.25%,  total acc: 82.06%   [EVAL] batch:   77 | acc: 68.75%,  total acc: 81.89%   [EVAL] batch:   78 | acc: 75.00%,  total acc: 81.80%   [EVAL] batch:   79 | acc: 81.25%,  total acc: 81.80%   [EVAL] batch:   80 | acc: 93.75%,  total acc: 81.94%   [EVAL] batch:   81 | acc: 62.50%,  total acc: 81.71%   [EVAL] batch:   82 | acc: 68.75%,  total acc: 81.55%   [EVAL] batch:   83 | acc: 75.00%,  total acc: 81.47%   [EVAL] batch:   84 | acc: 68.75%,  total acc: 81.32%   [EVAL] batch:   85 | acc: 81.25%,  total acc: 81.32%   [EVAL] batch:   86 | acc: 87.50%,  total acc: 81.39%   [EVAL] batch:   87 | acc: 75.00%,  total acc: 81.32%   [EVAL] batch:   88 | acc: 93.75%,  total acc: 81.46%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 81.67%   [EVAL] batch:   90 | acc: 93.75%,  total acc: 81.80%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 82.00%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 82.19%   [EVAL] batch:   93 | acc: 100.00%,  total acc: 82.38%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 82.57%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 82.75%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 82.93%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 83.10%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 83.27%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 83.44%   [EVAL] batch:  100 | acc: 62.50%,  total acc: 83.23%   [EVAL] batch:  101 | acc: 68.75%,  total acc: 83.09%   [EVAL] batch:  102 | acc: 68.75%,  total acc: 82.95%   [EVAL] batch:  103 | acc: 62.50%,  total acc: 82.75%   [EVAL] batch:  104 | acc: 87.50%,  total acc: 82.80%   [EVAL] batch:  105 | acc: 62.50%,  total acc: 82.61%   [EVAL] batch:  106 | acc: 31.25%,  total acc: 82.13%   [EVAL] batch:  107 | acc: 18.75%,  total acc: 81.54%   [EVAL] batch:  108 | acc: 37.50%,  total acc: 81.14%   [EVAL] batch:  109 | acc: 12.50%,  total acc: 80.51%   [EVAL] batch:  110 | acc: 43.75%,  total acc: 80.18%   [EVAL] batch:  111 | acc: 31.25%,  total acc: 79.74%   [EVAL] batch:  112 | acc: 50.00%,  total acc: 79.48%   [EVAL] batch:  113 | acc: 87.50%,  total acc: 79.55%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 79.73%   [EVAL] batch:  115 | acc: 81.25%,  total acc: 79.74%   [EVAL] batch:  116 | acc: 87.50%,  total acc: 79.81%   [EVAL] batch:  117 | acc: 93.75%,  total acc: 79.93%   [EVAL] batch:  118 | acc: 75.00%,  total acc: 79.88%   [EVAL] batch:  119 | acc: 0.00%,  total acc: 79.22%   [EVAL] batch:  120 | acc: 12.50%,  total acc: 78.67%   [EVAL] batch:  121 | acc: 6.25%,  total acc: 78.07%   [EVAL] batch:  122 | acc: 18.75%,  total acc: 77.59%   [EVAL] batch:  123 | acc: 6.25%,  total acc: 77.02%   [EVAL] batch:  124 | acc: 6.25%,  total acc: 76.45%   [EVAL] batch:  125 | acc: 75.00%,  total acc: 76.44%   [EVAL] batch:  126 | acc: 43.75%,  total acc: 76.18%   [EVAL] batch:  127 | acc: 43.75%,  total acc: 75.93%   [EVAL] batch:  128 | acc: 31.25%,  total acc: 75.58%   [EVAL] batch:  129 | acc: 62.50%,  total acc: 75.48%   [EVAL] batch:  130 | acc: 62.50%,  total acc: 75.38%   [EVAL] batch:  131 | acc: 87.50%,  total acc: 75.47%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 75.66%   [EVAL] batch:  133 | acc: 81.25%,  total acc: 75.70%   [EVAL] batch:  134 | acc: 75.00%,  total acc: 75.69%   [EVAL] batch:  135 | acc: 81.25%,  total acc: 75.74%   [EVAL] batch:  136 | acc: 93.75%,  total acc: 75.87%   [EVAL] batch:  137 | acc: 75.00%,  total acc: 75.86%   [EVAL] batch:  138 | acc: 43.75%,  total acc: 75.63%   [EVAL] batch:  139 | acc: 56.25%,  total acc: 75.49%   [EVAL] batch:  140 | acc: 81.25%,  total acc: 75.53%   [EVAL] batch:  141 | acc: 75.00%,  total acc: 75.53%   [EVAL] batch:  142 | acc: 56.25%,  total acc: 75.39%   [EVAL] batch:  143 | acc: 62.50%,  total acc: 75.30%   [EVAL] batch:  144 | acc: 62.50%,  total acc: 75.22%   [EVAL] batch:  145 | acc: 75.00%,  total acc: 75.21%   [EVAL] batch:  146 | acc: 56.25%,  total acc: 75.09%   [EVAL] batch:  147 | acc: 75.00%,  total acc: 75.08%   [EVAL] batch:  148 | acc: 68.75%,  total acc: 75.04%   [EVAL] batch:  149 | acc: 62.50%,  total acc: 74.96%   [EVAL] batch:  150 | acc: 68.75%,  total acc: 74.92%   [EVAL] batch:  151 | acc: 75.00%,  total acc: 74.92%   [EVAL] batch:  152 | acc: 75.00%,  total acc: 74.92%   [EVAL] batch:  153 | acc: 68.75%,  total acc: 74.88%   [EVAL] batch:  154 | acc: 56.25%,  total acc: 74.76%   [EVAL] batch:  155 | acc: 75.00%,  total acc: 74.76%   [EVAL] batch:  156 | acc: 75.00%,  total acc: 74.76%   [EVAL] batch:  157 | acc: 50.00%,  total acc: 74.60%   [EVAL] batch:  158 | acc: 43.75%,  total acc: 74.41%   [EVAL] batch:  159 | acc: 62.50%,  total acc: 74.34%   [EVAL] batch:  160 | acc: 50.00%,  total acc: 74.18%   [EVAL] batch:  161 | acc: 50.00%,  total acc: 74.04%   [EVAL] batch:  162 | acc: 56.25%,  total acc: 73.93%   [EVAL] batch:  163 | acc: 62.50%,  total acc: 73.86%   [EVAL] batch:  164 | acc: 62.50%,  total acc: 73.79%   [EVAL] batch:  165 | acc: 68.75%,  total acc: 73.76%   [EVAL] batch:  166 | acc: 68.75%,  total acc: 73.73%   [EVAL] batch:  167 | acc: 62.50%,  total acc: 73.66%   [EVAL] batch:  168 | acc: 56.25%,  total acc: 73.56%   [EVAL] batch:  169 | acc: 31.25%,  total acc: 73.31%   [EVAL] batch:  170 | acc: 43.75%,  total acc: 73.14%   [EVAL] batch:  171 | acc: 43.75%,  total acc: 72.97%   [EVAL] batch:  172 | acc: 43.75%,  total acc: 72.80%   [EVAL] batch:  173 | acc: 37.50%,  total acc: 72.59%   [EVAL] batch:  174 | acc: 50.00%,  total acc: 72.46%   [EVAL] batch:  175 | acc: 100.00%,  total acc: 72.62%   [EVAL] batch:  176 | acc: 100.00%,  total acc: 72.78%   [EVAL] batch:  177 | acc: 100.00%,  total acc: 72.93%   [EVAL] batch:  178 | acc: 100.00%,  total acc: 73.08%   [EVAL] batch:  179 | acc: 100.00%,  total acc: 73.23%   [EVAL] batch:  180 | acc: 100.00%,  total acc: 73.38%   [EVAL] batch:  181 | acc: 100.00%,  total acc: 73.52%   [EVAL] batch:  182 | acc: 81.25%,  total acc: 73.57%   [EVAL] batch:  183 | acc: 93.75%,  total acc: 73.68%   [EVAL] batch:  184 | acc: 93.75%,  total acc: 73.78%   [EVAL] batch:  185 | acc: 100.00%,  total acc: 73.92%   [EVAL] batch:  186 | acc: 93.75%,  total acc: 74.03%   [EVAL] batch:  187 | acc: 81.25%,  total acc: 74.07%   [EVAL] batch:  188 | acc: 37.50%,  total acc: 73.88%   [EVAL] batch:  189 | acc: 56.25%,  total acc: 73.78%   [EVAL] batch:  190 | acc: 43.75%,  total acc: 73.63%   [EVAL] batch:  191 | acc: 25.00%,  total acc: 73.37%   [EVAL] batch:  192 | acc: 50.00%,  total acc: 73.25%   [EVAL] batch:  193 | acc: 31.25%,  total acc: 73.03%   [EVAL] batch:  194 | acc: 81.25%,  total acc: 73.08%   [EVAL] batch:  195 | acc: 87.50%,  total acc: 73.15%   [EVAL] batch:  196 | acc: 87.50%,  total acc: 73.22%   [EVAL] batch:  197 | acc: 93.75%,  total acc: 73.33%   [EVAL] batch:  198 | acc: 87.50%,  total acc: 73.40%   [EVAL] batch:  199 | acc: 81.25%,  total acc: 73.44%   [EVAL] batch:  200 | acc: 75.00%,  total acc: 73.45%   [EVAL] batch:  201 | acc: 62.50%,  total acc: 73.39%   [EVAL] batch:  202 | acc: 62.50%,  total acc: 73.34%   [EVAL] batch:  203 | acc: 68.75%,  total acc: 73.31%   [EVAL] batch:  204 | acc: 75.00%,  total acc: 73.32%   [EVAL] batch:  205 | acc: 43.75%,  total acc: 73.18%   [EVAL] batch:  206 | acc: 93.75%,  total acc: 73.28%   [EVAL] batch:  207 | acc: 100.00%,  total acc: 73.41%   [EVAL] batch:  208 | acc: 100.00%,  total acc: 73.53%   [EVAL] batch:  209 | acc: 93.75%,  total acc: 73.63%   [EVAL] batch:  210 | acc: 100.00%,  total acc: 73.76%   [EVAL] batch:  211 | acc: 100.00%,  total acc: 73.88%   [EVAL] batch:  212 | acc: 87.50%,  total acc: 73.94%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 74.07%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 74.19%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 74.31%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 74.42%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 74.54%   [EVAL] batch:  218 | acc: 100.00%,  total acc: 74.66%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 74.77%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 74.89%   [EVAL] batch:  221 | acc: 87.50%,  total acc: 74.94%   [EVAL] batch:  222 | acc: 93.75%,  total acc: 75.03%   [EVAL] batch:  223 | acc: 93.75%,  total acc: 75.11%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 75.22%   [EVAL] batch:  225 | acc: 43.75%,  total acc: 75.08%   [EVAL] batch:  226 | acc: 56.25%,  total acc: 75.00%   [EVAL] batch:  227 | acc: 50.00%,  total acc: 74.89%   [EVAL] batch:  228 | acc: 43.75%,  total acc: 74.75%   [EVAL] batch:  229 | acc: 12.50%,  total acc: 74.48%   [EVAL] batch:  230 | acc: 43.75%,  total acc: 74.35%   [EVAL] batch:  231 | acc: 93.75%,  total acc: 74.43%   [EVAL] batch:  232 | acc: 93.75%,  total acc: 74.52%   [EVAL] batch:  233 | acc: 87.50%,  total acc: 74.57%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 74.68%   [EVAL] batch:  235 | acc: 87.50%,  total acc: 74.74%   [EVAL] batch:  236 | acc: 93.75%,  total acc: 74.82%   [EVAL] batch:  237 | acc: 81.25%,  total acc: 74.84%   [EVAL] batch:  238 | acc: 68.75%,  total acc: 74.82%   [EVAL] batch:  239 | acc: 62.50%,  total acc: 74.77%   [EVAL] batch:  240 | acc: 50.00%,  total acc: 74.66%   [EVAL] batch:  241 | acc: 75.00%,  total acc: 74.66%   [EVAL] batch:  242 | acc: 50.00%,  total acc: 74.56%   [EVAL] batch:  243 | acc: 50.00%,  total acc: 74.46%   [EVAL] batch:  244 | acc: 75.00%,  total acc: 74.46%   [EVAL] batch:  245 | acc: 75.00%,  total acc: 74.47%   [EVAL] batch:  246 | acc: 56.25%,  total acc: 74.39%   [EVAL] batch:  247 | acc: 81.25%,  total acc: 74.42%   [EVAL] batch:  248 | acc: 87.50%,  total acc: 74.47%   [EVAL] batch:  249 | acc: 68.75%,  total acc: 74.45%   [EVAL] batch:  250 | acc: 87.50%,  total acc: 74.50%   [EVAL] batch:  251 | acc: 68.75%,  total acc: 74.48%   [EVAL] batch:  252 | acc: 81.25%,  total acc: 74.51%   [EVAL] batch:  253 | acc: 75.00%,  total acc: 74.51%   [EVAL] batch:  254 | acc: 93.75%,  total acc: 74.58%   [EVAL] batch:  255 | acc: 100.00%,  total acc: 74.68%   [EVAL] batch:  256 | acc: 50.00%,  total acc: 74.59%   [EVAL] batch:  257 | acc: 50.00%,  total acc: 74.49%   [EVAL] batch:  258 | acc: 50.00%,  total acc: 74.40%   [EVAL] batch:  259 | acc: 62.50%,  total acc: 74.35%   [EVAL] batch:  260 | acc: 43.75%,  total acc: 74.23%   [EVAL] batch:  261 | acc: 62.50%,  total acc: 74.19%   [EVAL] batch:  262 | acc: 62.50%,  total acc: 74.14%   [EVAL] batch:  263 | acc: 75.00%,  total acc: 74.15%   [EVAL] batch:  264 | acc: 75.00%,  total acc: 74.15%   [EVAL] batch:  265 | acc: 62.50%,  total acc: 74.11%   [EVAL] batch:  266 | acc: 68.75%,  total acc: 74.09%   [EVAL] batch:  267 | acc: 68.75%,  total acc: 74.07%   [EVAL] batch:  268 | acc: 68.75%,  total acc: 74.05%   [EVAL] batch:  269 | acc: 56.25%,  total acc: 73.98%   [EVAL] batch:  270 | acc: 68.75%,  total acc: 73.96%   [EVAL] batch:  271 | acc: 75.00%,  total acc: 73.97%   [EVAL] batch:  272 | acc: 50.00%,  total acc: 73.88%   [EVAL] batch:  273 | acc: 81.25%,  total acc: 73.91%   [EVAL] batch:  274 | acc: 50.00%,  total acc: 73.82%   [EVAL] batch:  275 | acc: 100.00%,  total acc: 73.91%   [EVAL] batch:  276 | acc: 100.00%,  total acc: 74.01%   [EVAL] batch:  277 | acc: 100.00%,  total acc: 74.10%   [EVAL] batch:  278 | acc: 100.00%,  total acc: 74.19%   [EVAL] batch:  279 | acc: 100.00%,  total acc: 74.29%   [EVAL] batch:  280 | acc: 100.00%,  total acc: 74.38%   [EVAL] batch:  281 | acc: 75.00%,  total acc: 74.38%   [EVAL] batch:  282 | acc: 81.25%,  total acc: 74.40%   [EVAL] batch:  283 | acc: 62.50%,  total acc: 74.36%   [EVAL] batch:  284 | acc: 81.25%,  total acc: 74.39%   [EVAL] batch:  285 | acc: 62.50%,  total acc: 74.34%   [EVAL] batch:  286 | acc: 68.75%,  total acc: 74.32%   [EVAL] batch:  287 | acc: 68.75%,  total acc: 74.31%   [EVAL] batch:  288 | acc: 100.00%,  total acc: 74.39%   [EVAL] batch:  289 | acc: 100.00%,  total acc: 74.48%   [EVAL] batch:  290 | acc: 100.00%,  total acc: 74.57%   [EVAL] batch:  291 | acc: 100.00%,  total acc: 74.66%   [EVAL] batch:  292 | acc: 100.00%,  total acc: 74.74%   [EVAL] batch:  293 | acc: 93.75%,  total acc: 74.81%   [EVAL] batch:  294 | acc: 100.00%,  total acc: 74.89%   [EVAL] batch:  295 | acc: 100.00%,  total acc: 74.98%   [EVAL] batch:  296 | acc: 100.00%,  total acc: 75.06%   [EVAL] batch:  297 | acc: 100.00%,  total acc: 75.15%   [EVAL] batch:  298 | acc: 100.00%,  total acc: 75.23%   [EVAL] batch:  299 | acc: 100.00%,  total acc: 75.31%   [EVAL] batch:  300 | acc: 87.50%,  total acc: 75.35%   [EVAL] batch:  301 | acc: 87.50%,  total acc: 75.39%   [EVAL] batch:  302 | acc: 93.75%,  total acc: 75.45%   [EVAL] batch:  303 | acc: 93.75%,  total acc: 75.51%   [EVAL] batch:  304 | acc: 81.25%,  total acc: 75.53%   [EVAL] batch:  305 | acc: 93.75%,  total acc: 75.59%   [EVAL] batch:  306 | acc: 87.50%,  total acc: 75.63%   [EVAL] batch:  307 | acc: 81.25%,  total acc: 75.65%   [EVAL] batch:  308 | acc: 93.75%,  total acc: 75.71%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 75.79%   [EVAL] batch:  310 | acc: 100.00%,  total acc: 75.86%   [EVAL] batch:  311 | acc: 81.25%,  total acc: 75.88%   [EVAL] batch:  312 | acc: 62.50%,  total acc: 75.84%   [EVAL] batch:  313 | acc: 37.50%,  total acc: 75.72%   [EVAL] batch:  314 | acc: 43.75%,  total acc: 75.62%   [EVAL] batch:  315 | acc: 37.50%,  total acc: 75.49%   [EVAL] batch:  316 | acc: 50.00%,  total acc: 75.41%   [EVAL] batch:  317 | acc: 18.75%,  total acc: 75.24%   [EVAL] batch:  318 | acc: 81.25%,  total acc: 75.25%   [EVAL] batch:  319 | acc: 75.00%,  total acc: 75.25%   [EVAL] batch:  320 | acc: 87.50%,  total acc: 75.29%   [EVAL] batch:  321 | acc: 87.50%,  total acc: 75.33%   [EVAL] batch:  322 | acc: 93.75%,  total acc: 75.39%   [EVAL] batch:  323 | acc: 68.75%,  total acc: 75.37%   [EVAL] batch:  324 | acc: 81.25%,  total acc: 75.38%   [EVAL] batch:  325 | acc: 93.75%,  total acc: 75.44%   [EVAL] batch:  326 | acc: 62.50%,  total acc: 75.40%   [EVAL] batch:  327 | acc: 87.50%,  total acc: 75.44%   [EVAL] batch:  328 | acc: 87.50%,  total acc: 75.47%   [EVAL] batch:  329 | acc: 68.75%,  total acc: 75.45%   [EVAL] batch:  330 | acc: 93.75%,  total acc: 75.51%   [EVAL] batch:  331 | acc: 18.75%,  total acc: 75.34%   [EVAL] batch:  332 | acc: 37.50%,  total acc: 75.23%   [EVAL] batch:  333 | acc: 12.50%,  total acc: 75.04%   [EVAL] batch:  334 | acc: 12.50%,  total acc: 74.85%   [EVAL] batch:  335 | acc: 18.75%,  total acc: 74.68%   [EVAL] batch:  336 | acc: 18.75%,  total acc: 74.52%   [EVAL] batch:  337 | acc: 25.00%,  total acc: 74.37%   [EVAL] batch:  338 | acc: 12.50%,  total acc: 74.19%   [EVAL] batch:  339 | acc: 25.00%,  total acc: 74.04%   [EVAL] batch:  340 | acc: 6.25%,  total acc: 73.85%   [EVAL] batch:  341 | acc: 6.25%,  total acc: 73.65%   [EVAL] batch:  342 | acc: 6.25%,  total acc: 73.45%   [EVAL] batch:  343 | acc: 25.00%,  total acc: 73.31%   [EVAL] batch:  344 | acc: 87.50%,  total acc: 73.35%   [EVAL] batch:  345 | acc: 87.50%,  total acc: 73.39%   [EVAL] batch:  346 | acc: 93.75%,  total acc: 73.45%   [EVAL] batch:  347 | acc: 93.75%,  total acc: 73.51%   [EVAL] batch:  348 | acc: 93.75%,  total acc: 73.57%   [EVAL] batch:  349 | acc: 87.50%,  total acc: 73.61%   [EVAL] batch:  350 | acc: 93.75%,  total acc: 73.66%   [EVAL] batch:  351 | acc: 93.75%,  total acc: 73.72%   [EVAL] batch:  352 | acc: 100.00%,  total acc: 73.80%   [EVAL] batch:  353 | acc: 87.50%,  total acc: 73.83%   [EVAL] batch:  354 | acc: 93.75%,  total acc: 73.89%   [EVAL] batch:  355 | acc: 100.00%,  total acc: 73.96%   [EVAL] batch:  356 | acc: 75.00%,  total acc: 73.97%   [EVAL] batch:  357 | acc: 68.75%,  total acc: 73.95%   [EVAL] batch:  358 | acc: 37.50%,  total acc: 73.85%   [EVAL] batch:  359 | acc: 68.75%,  total acc: 73.84%   [EVAL] batch:  360 | acc: 68.75%,  total acc: 73.82%   [EVAL] batch:  361 | acc: 75.00%,  total acc: 73.83%   [EVAL] batch:  362 | acc: 31.25%,  total acc: 73.71%   [EVAL] batch:  363 | acc: 31.25%,  total acc: 73.59%   [EVAL] batch:  364 | acc: 31.25%,  total acc: 73.48%   [EVAL] batch:  365 | acc: 62.50%,  total acc: 73.45%   [EVAL] batch:  366 | acc: 43.75%,  total acc: 73.37%   [EVAL] batch:  367 | acc: 50.00%,  total acc: 73.30%   [EVAL] batch:  368 | acc: 50.00%,  total acc: 73.24%   [EVAL] batch:  369 | acc: 50.00%,  total acc: 73.18%   [EVAL] batch:  370 | acc: 43.75%,  total acc: 73.10%   [EVAL] batch:  371 | acc: 43.75%,  total acc: 73.02%   [EVAL] batch:  372 | acc: 56.25%,  total acc: 72.97%   [EVAL] batch:  373 | acc: 81.25%,  total acc: 72.99%   [EVAL] batch:  374 | acc: 62.50%,  total acc: 72.97%   
cur_acc:  ['0.9286', '0.7827', '0.7262', '0.7331', '0.8452', '0.5784']
his_acc:  ['0.9286', '0.8585', '0.7992', '0.7615', '0.7730', '0.7297']
Clustering into  34  clusters
Clusters:  [29  0 15  2  8 23 15 21  0  3 11  1 21 24 18 27 17 16  2 16 32 22 21  8
 33  3  5 23 10 31 25 10 16 26  7 10 16  1  3  1  2 19 14  0 20  5 11  4
 18 11  8 24 13 12  6  9 13  0 13  0  5 24  6 14  6  4 13  7 28 30]
Losses:  10.776357650756836 3.864377975463867 0.6040439009666443
CurrentTrain: epoch  0, batch     0 | loss: 10.7763577Losses:  10.513097763061523 3.5348076820373535 0.6868953704833984
CurrentTrain: epoch  0, batch     1 | loss: 10.5130978Losses:  10.433960914611816 4.164150238037109 0.7053731083869934
CurrentTrain: epoch  0, batch     2 | loss: 10.4339609Losses:  6.420010089874268 -0.0 0.1159750372171402
CurrentTrain: epoch  0, batch     3 | loss: 6.4200101Losses:  11.444465637207031 5.001131057739258 0.6082260012626648
CurrentTrain: epoch  1, batch     0 | loss: 11.4444656Losses:  9.126883506774902 2.757154941558838 0.7423970699310303
CurrentTrain: epoch  1, batch     1 | loss: 9.1268835Losses:  8.7398042678833 3.8518588542938232 0.6567223072052002
CurrentTrain: epoch  1, batch     2 | loss: 8.7398043Losses:  9.112594604492188 -0.0 0.19877031445503235
CurrentTrain: epoch  1, batch     3 | loss: 9.1125946Losses:  8.25378704071045 2.435824155807495 0.7437154054641724
CurrentTrain: epoch  2, batch     0 | loss: 8.2537870Losses:  9.021947860717773 3.25756573677063 0.7645805478096008
CurrentTrain: epoch  2, batch     1 | loss: 9.0219479Losses:  8.150187492370605 3.194108486175537 0.611748993396759
CurrentTrain: epoch  2, batch     2 | loss: 8.1501875Losses:  4.849759578704834 -0.0 0.12958204746246338
CurrentTrain: epoch  2, batch     3 | loss: 4.8497596Losses:  8.398192405700684 3.2956504821777344 0.6807168126106262
CurrentTrain: epoch  3, batch     0 | loss: 8.3981924Losses:  9.336104393005371 4.04534912109375 0.6700292825698853
CurrentTrain: epoch  3, batch     1 | loss: 9.3361044Losses:  8.480204582214355 3.4737396240234375 0.659039318561554
CurrentTrain: epoch  3, batch     2 | loss: 8.4802046Losses:  3.08042311668396 -0.0 0.12325310707092285
CurrentTrain: epoch  3, batch     3 | loss: 3.0804231Losses:  10.720511436462402 5.34669303894043 0.6341003775596619
CurrentTrain: epoch  4, batch     0 | loss: 10.7205114Losses:  8.367822647094727 3.697871208190918 0.6520276069641113
CurrentTrain: epoch  4, batch     1 | loss: 8.3678226Losses:  7.758479118347168 3.0190367698669434 0.6607391834259033
CurrentTrain: epoch  4, batch     2 | loss: 7.7584791Losses:  2.341521739959717 -0.0 -0.0
CurrentTrain: epoch  4, batch     3 | loss: 2.3415217Losses:  8.474848747253418 3.3382277488708496 0.6974327564239502
CurrentTrain: epoch  5, batch     0 | loss: 8.4748487Losses:  5.971463680267334 2.331857204437256 0.7198432683944702
CurrentTrain: epoch  5, batch     1 | loss: 5.9714637Losses:  8.751503944396973 3.5545716285705566 0.703599214553833
CurrentTrain: epoch  5, batch     2 | loss: 8.7515039Losses:  4.675381660461426 -0.0 0.11891001462936401
CurrentTrain: epoch  5, batch     3 | loss: 4.6753817Losses:  7.940311431884766 2.8352270126342773 0.6937015056610107
CurrentTrain: epoch  6, batch     0 | loss: 7.9403114Losses:  6.8706231117248535 3.4120616912841797 0.6583636999130249
CurrentTrain: epoch  6, batch     1 | loss: 6.8706231Losses:  8.764928817749023 3.910238742828369 0.6793010234832764
CurrentTrain: epoch  6, batch     2 | loss: 8.7649288Losses:  4.778668403625488 -0.0 0.10916083306074142
CurrentTrain: epoch  6, batch     3 | loss: 4.7786684Losses:  8.308018684387207 4.58929443359375 0.5151907205581665
CurrentTrain: epoch  7, batch     0 | loss: 8.3080187Losses:  7.611932754516602 3.543303966522217 0.6866434216499329
CurrentTrain: epoch  7, batch     1 | loss: 7.6119328Losses:  8.298125267028809 3.998861312866211 0.5911449790000916
CurrentTrain: epoch  7, batch     2 | loss: 8.2981253Losses:  3.9016244411468506 -0.0 0.10968132317066193
CurrentTrain: epoch  7, batch     3 | loss: 3.9016244Losses:  6.4598283767700195 2.273132801055908 0.7469833493232727
CurrentTrain: epoch  8, batch     0 | loss: 6.4598284Losses:  7.085946559906006 2.463376045227051 0.7578626275062561
CurrentTrain: epoch  8, batch     1 | loss: 7.0859466Losses:  6.117980003356934 2.8523683547973633 0.6524341106414795
CurrentTrain: epoch  8, batch     2 | loss: 6.1179800Losses:  4.559024333953857 -0.0 0.141791433095932
CurrentTrain: epoch  8, batch     3 | loss: 4.5590243Losses:  6.617598056793213 3.316531181335449 0.568566620349884
CurrentTrain: epoch  9, batch     0 | loss: 6.6175981Losses:  7.347095012664795 3.0258374214172363 0.6125263571739197
CurrentTrain: epoch  9, batch     1 | loss: 7.3470950Losses:  7.292277812957764 3.5546324253082275 0.5844267010688782
CurrentTrain: epoch  9, batch     2 | loss: 7.2922778Losses:  5.028413772583008 -0.0 0.16454409062862396
CurrentTrain: epoch  9, batch     3 | loss: 5.0284138
#### main subject ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "main subject" means primary topic of a work.
Here is an example:
Relation: main subject
Context: auerbach is prominently featured in the documentary film , " the first basket " , about jewish basketball history .
Head Entity: the first basket
Tail Entity: basketball
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "main subject":
Relation: main subject  
Context: the book "sapiens: a brief history of humankind" explores the evolution of human societies and cultures.  
Head Entity: sapiens: a brief history of humankind  
Tail Entity: human societies  

Relation: main subject  
Context: the documentary "our planet" showcases the beauty of nature and the impact of climate change on wildlife.  
Head Entity: our planet  
Tail Entity: nature  
#### participating team ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "participating team" means teams that actively takes/took part in an event or process.
Here is an example:
Relation: participating team
Context: he represented romania at euro 2000 , his first major tournament where they beat england 3–2 , and drew 1–1 against germany .
Head Entity: euro 2000
Tail Entity: germany
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "participating team":
Relation: participating team  
Context: The United States women's national soccer team has consistently performed well in international competitions, winning the FIFA Women's World Cup multiple times, including in 2019, where they faced the Netherlands in the final.  
Head Entity: FIFA Women's World Cup  
Tail Entity: Netherlands  

Relation: participating team  
Context: During the 2022 Winter Olympics, the Canadian ice hockey team showcased their skills, ultimately defeating the Russian Olympic Committee in a thrilling final match to secure the gold medal.  
Head Entity: 2022 Winter Olympics  
Tail Entity: Russian Olympic Committee  
#### has part ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "has part" means part of this subject.
Here is an example:
Relation: has part
Context: among the competing artists was previous eurovision song contest entrant robin juhkental who represented estonia in 2010 as part of the group malcolm lincoln .
Head Entity: malcolm lincoln
Tail Entity: robin juhkental
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "has part":
Relation: has part  
Context: The famous painting "The Last Supper" features several apostles, including Peter, who is depicted prominently on the left side of the artwork.  
Head Entity: The Last Supper  
Tail Entity: Peter  

Relation: has part  
Context: The smartphone model Galaxy S21 comes with a high-resolution camera that includes a wide-angle lens, allowing users to capture stunning landscapes.  
Head Entity: Galaxy S21  
Tail Entity: wide-angle lens  
#### nominated for ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "nominated for" means award nomination received by a person, organisation or creative work.
Here is an example:
Relation: nominated for
Context: he was nominated for the academy award for best picture , along with steven spielberg , ian bryce , mark gordon for the film " saving private ryan " .
Head Entity: ian bryce
Tail Entity: academy award for best picture
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "nominated for":
Relation: nominated for  
Context: The film "The Shape of Water" received multiple nominations, including a nod for best director for Guillermo del Toro at the Academy Awards.  
Head Entity: Guillermo del Toro  
Tail Entity: Academy Awards  

Relation: nominated for  
Context: The popular band was nominated for the Grammy Award for Best New Artist after their debut album topped the charts.  
Head Entity: the popular band  
Tail Entity: Grammy Award for Best New Artist  
#### instance of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "instance of" means that class of which this subject is a particular example and member.
Here is an example:
Relation: instance of
Context: in 2007 , the county municipality created the transit authority called " skyss " that would market public transport while is would be operated by private companies based on public service obligation contracts .
Head Entity: skyss
Tail Entity: transit authority
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "instance of":
Relation: instance of  
Context: The smartphone known as the iPhone was first released by Apple Inc. in 2007, revolutionizing the mobile phone industry and setting new standards for design and functionality.  
Head Entity: iPhone  
Tail Entity: smartphone  

Relation: instance of  
Context: The Great Wall of China is a series of fortifications made of various materials, built to protect the Chinese states and empires from invasions and raids.  
Head Entity: Great Wall of China  
Tail Entity: fortification  
#### mountain range ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "mountain range" means range or subrange to which the geographical item belongs.
Here is an example:
Relation: mountain range
Context: brand peak is a sharp snow - covered peak located east - southeast of the eternity range and northwest of mount duemler , in palmer land .
Head Entity: mount duemler
Tail Entity: eternity range
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "mountain range":
Relation: mountain range  
Context: the rocky mountains extend from Canada through the United States and into Mexico, forming a major mountain range in North America.  
Head Entity: rocky mountains  
Tail Entity: North America  

Relation: mountain range  
Context: the appalachian mountains are one of the oldest mountain ranges in the world, stretching from Canada down to Alabama.  
Head Entity: appalachian mountains  
Tail Entity: Alabama  
#### screenwriter ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "screenwriter" means person who wrote the script for subject item.
Here is an example:
Relation: screenwriter
Context: anders matthesen has also released several cds with his radio material , in addition to the animated movie " terkel in trouble " , based on one of these .
Head Entity: terkel in trouble
Tail Entity: anders matthesen
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "screenwriter":
Relation: screenwriter  
Context: the film "inception" was directed by christopher nolan, who also wrote the screenplay that intricately weaves together dreams and reality.  
Head Entity: inception  
Tail Entity: christopher nolan  

Relation: screenwriter  
Context: the beloved animated feature "the lion king" features a screenplay penned by jeffrey katzenberg, bringing the story of simba to life.  
Head Entity: the lion king  
Tail Entity: jeffrey katzenberg  
#### language of work or name ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "language of work or name" means language associated with this creative work (such as books, shows, songs, broadcasts or websites) or a name.
Here is an example:
Relation: language of work or name
Context: the earliest written mention of the order is found in " tirant lo blanch " , a chivalric romance written in catalan mainly by valencian joanot martorell .
Head Entity: tirant lo blanch
Tail Entity: catalan
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "language of work or name":
Relation: language of work or name  
Context: The famous novel "One Hundred Years of Solitude" was originally published in Spanish and has been translated into many languages.  
Head Entity: One Hundred Years of Solitude  
Tail Entity: Spanish  

Relation: language of work or name  
Context: The animated series "Naruto" is primarily produced in Japanese and has gained a massive following worldwide.  
Head Entity: Naruto  
Tail Entity: Japanese  
#### operator ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "operator" means person, profession, or organization that operates the equipment, facility, or service.
Here is an example:
Relation: operator
Context: the aircraft was a boeing 707 - 321c that first entered service with pan american world airways ( pan am ) in 1963 .
Head Entity: boeing 707 - 321c
Tail Entity: pan american world airways
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "operator":
Relation: operator  
Context: the new electric bus fleet is managed by the city transportation authority, which ensures efficient public transit services.  
Head Entity: electric bus fleet  
Tail Entity: city transportation authority  

Relation: operator  
Context: the state-of-the-art research facility is operated by the national science foundation, providing resources for groundbreaking studies.  
Head Entity: research facility  
Tail Entity: national science foundation  
#### religion ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "religion" means religion of a person, organization or religious building, or associated with this subject.
Here is an example:
Relation: religion
Context: baianism is a term applied to the theology of catholic theologian michael baius ( 1513 - 1589 ) .
Head Entity: michael baius
Tail Entity: catholic
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "religion":
Relation: religion  
Context: the church of england is the largest christian denomination in england, with its roots tracing back to the reformation.  
Head Entity: church of england  
Tail Entity: christian  

Relation: religion  
Context: the baha'i faith emphasizes the spiritual unity of all humankind and was founded by baha'u'llah in the 19th century.  
Head Entity: baha'i faith  
Tail Entity: baha'u'llah  
Losses:  3.937981128692627 0.3009597659111023 0.9931640028953552
MemoryTrain:  epoch  0, batch     0 | loss: 3.9379811Losses:  3.715003252029419 0.2665582299232483 1.0740466117858887
MemoryTrain:  epoch  0, batch     1 | loss: 3.7150033Losses:  4.135147571563721 0.5517295598983765 0.9806489944458008
MemoryTrain:  epoch  0, batch     2 | loss: 4.1351476Losses:  4.194400310516357 -0.0 1.0423563718795776
MemoryTrain:  epoch  0, batch     3 | loss: 4.1944003Losses:  4.027220726013184 -0.0 0.9860920906066895
MemoryTrain:  epoch  0, batch     4 | loss: 4.0272207Losses:  3.6481003761291504 0.24253584444522858 0.9207762479782104
MemoryTrain:  epoch  0, batch     5 | loss: 3.6481004Losses:  3.139561414718628 0.5143871903419495 0.9199270606040955
MemoryTrain:  epoch  0, batch     6 | loss: 3.1395614Losses:  4.793074607849121 -0.0 0.9637719392776489
MemoryTrain:  epoch  0, batch     7 | loss: 4.7930746Losses:  4.441788196563721 0.28588980436325073 0.9451915621757507
MemoryTrain:  epoch  0, batch     8 | loss: 4.4417882Losses:  3.5277810096740723 -0.0 0.9906373023986816
MemoryTrain:  epoch  0, batch     9 | loss: 3.5277810Losses:  5.194166660308838 0.25795865058898926 1.0612887144088745
MemoryTrain:  epoch  0, batch    10 | loss: 5.1941667Losses:  3.199406385421753 0.23447510600090027 0.8724002838134766
MemoryTrain:  epoch  0, batch    11 | loss: 3.1994064Losses:  3.999774932861328 0.5498967170715332 0.9102234244346619
MemoryTrain:  epoch  0, batch    12 | loss: 3.9997749Losses:  1.528957486152649 -0.0 0.12336602061986923
MemoryTrain:  epoch  0, batch    13 | loss: 1.5289575Losses:  4.04906702041626 -0.0 0.9575596451759338
MemoryTrain:  epoch  1, batch     0 | loss: 4.0490670Losses:  4.971401691436768 0.5012071132659912 0.8597310781478882
MemoryTrain:  epoch  1, batch     1 | loss: 4.9714017Losses:  3.7457494735717773 0.24824051558971405 0.932968258857727
MemoryTrain:  epoch  1, batch     2 | loss: 3.7457495Losses:  4.186394691467285 0.2634623050689697 0.9577766060829163
MemoryTrain:  epoch  1, batch     3 | loss: 4.1863947Losses:  2.806692123413086 -0.0 0.9519747495651245
MemoryTrain:  epoch  1, batch     4 | loss: 2.8066921Losses:  3.2890944480895996 0.2515897750854492 0.8940829038619995
MemoryTrain:  epoch  1, batch     5 | loss: 3.2890944Losses:  3.7351386547088623 0.46142125129699707 0.8603493571281433
MemoryTrain:  epoch  1, batch     6 | loss: 3.7351387Losses:  3.270512104034424 0.5094530582427979 1.0319000482559204
MemoryTrain:  epoch  1, batch     7 | loss: 3.2705121Losses:  3.0711617469787598 0.4942479133605957 0.9664386510848999
MemoryTrain:  epoch  1, batch     8 | loss: 3.0711617Losses:  2.7728331089019775 0.2571803331375122 0.8468366265296936
MemoryTrain:  epoch  1, batch     9 | loss: 2.7728331Losses:  4.013730049133301 0.27890896797180176 0.977260947227478
MemoryTrain:  epoch  1, batch    10 | loss: 4.0137300Losses:  3.566145896911621 -0.0 1.035982608795166
MemoryTrain:  epoch  1, batch    11 | loss: 3.5661459Losses:  4.10542631149292 0.47662442922592163 0.9901813268661499
MemoryTrain:  epoch  1, batch    12 | loss: 4.1054263Losses:  2.946002960205078 -0.0 0.11716523766517639
MemoryTrain:  epoch  1, batch    13 | loss: 2.9460030Losses:  4.177975654602051 0.5588819980621338 0.9087203741073608
MemoryTrain:  epoch  2, batch     0 | loss: 4.1779757Losses:  3.8486292362213135 0.22556710243225098 1.0158679485321045
MemoryTrain:  epoch  2, batch     1 | loss: 3.8486292Losses:  3.7603201866149902 0.5966535806655884 0.8900213241577148
MemoryTrain:  epoch  2, batch     2 | loss: 3.7603202Losses:  3.6879332065582275 0.341614305973053 0.8789234161376953
MemoryTrain:  epoch  2, batch     3 | loss: 3.6879332Losses:  2.8010120391845703 0.26519453525543213 0.9757120013237
MemoryTrain:  epoch  2, batch     4 | loss: 2.8010120Losses:  2.937448501586914 0.2631304860115051 1.012449026107788
MemoryTrain:  epoch  2, batch     5 | loss: 2.9374485Losses:  2.903120994567871 -0.0 0.9647554159164429
MemoryTrain:  epoch  2, batch     6 | loss: 2.9031210Losses:  3.330331563949585 0.4740655720233917 0.913846492767334
MemoryTrain:  epoch  2, batch     7 | loss: 3.3303316Losses:  4.045001983642578 0.4676706790924072 0.8477432131767273
MemoryTrain:  epoch  2, batch     8 | loss: 4.0450020Losses:  2.5582351684570312 -0.0 1.0520288944244385
MemoryTrain:  epoch  2, batch     9 | loss: 2.5582352Losses:  2.717824935913086 -0.0 1.0167540311813354
MemoryTrain:  epoch  2, batch    10 | loss: 2.7178249Losses:  3.834503173828125 0.3019881248474121 0.9259486198425293
MemoryTrain:  epoch  2, batch    11 | loss: 3.8345032Losses:  2.880275249481201 -0.0 1.0760352611541748
MemoryTrain:  epoch  2, batch    12 | loss: 2.8802752Losses:  1.3792140483856201 -0.0 0.15292906761169434
MemoryTrain:  epoch  2, batch    13 | loss: 1.3792140Losses:  3.0075387954711914 0.23446476459503174 0.9989680051803589
MemoryTrain:  epoch  3, batch     0 | loss: 3.0075388Losses:  3.252722978591919 -0.0 1.022796630859375
MemoryTrain:  epoch  3, batch     1 | loss: 3.2527230Losses:  2.4953739643096924 -0.0 0.9645578265190125
MemoryTrain:  epoch  3, batch     2 | loss: 2.4953740Losses:  3.071765899658203 -0.0 1.0068495273590088
MemoryTrain:  epoch  3, batch     3 | loss: 3.0717659Losses:  4.143254280090332 1.223686933517456 0.6623935699462891
MemoryTrain:  epoch  3, batch     4 | loss: 4.1432543Losses:  3.6744766235351562 0.5303168892860413 0.9643627405166626
MemoryTrain:  epoch  3, batch     5 | loss: 3.6744766Losses:  4.391246795654297 0.48765718936920166 0.9450408220291138
MemoryTrain:  epoch  3, batch     6 | loss: 4.3912468Losses:  3.156871795654297 0.5281473994255066 0.965241551399231
MemoryTrain:  epoch  3, batch     7 | loss: 3.1568718Losses:  3.0344767570495605 0.27731743454933167 0.9163730144500732
MemoryTrain:  epoch  3, batch     8 | loss: 3.0344768Losses:  3.20473051071167 0.3546913266181946 1.0169694423675537
MemoryTrain:  epoch  3, batch     9 | loss: 3.2047305Losses:  3.185774564743042 -0.0 1.0157155990600586
MemoryTrain:  epoch  3, batch    10 | loss: 3.1857746Losses:  2.9795355796813965 0.4964481592178345 0.9533286690711975
MemoryTrain:  epoch  3, batch    11 | loss: 2.9795356Losses:  3.9420089721679688 0.3415437340736389 1.0269428491592407
MemoryTrain:  epoch  3, batch    12 | loss: 3.9420090Losses:  1.5778906345367432 -0.0 0.14113348722457886
MemoryTrain:  epoch  3, batch    13 | loss: 1.5778906Losses:  2.519874334335327 0.21572719514369965 0.8575946688652039
MemoryTrain:  epoch  4, batch     0 | loss: 2.5198743Losses:  2.833329677581787 -0.0 0.9745199084281921
MemoryTrain:  epoch  4, batch     1 | loss: 2.8333297Losses:  2.8175878524780273 -0.0 0.85887211561203
MemoryTrain:  epoch  4, batch     2 | loss: 2.8175879Losses:  2.6680359840393066 0.24382956326007843 0.9458720088005066
MemoryTrain:  epoch  4, batch     3 | loss: 2.6680360Losses:  3.646059513092041 1.0580527782440186 0.8189991116523743
MemoryTrain:  epoch  4, batch     4 | loss: 3.6460595Losses:  3.1032471656799316 0.2674581706523895 0.944108247756958
MemoryTrain:  epoch  4, batch     5 | loss: 3.1032472Losses:  3.0116686820983887 0.26559215784072876 0.8547881841659546
MemoryTrain:  epoch  4, batch     6 | loss: 3.0116687Losses:  3.6793675422668457 -0.0 1.0070247650146484
MemoryTrain:  epoch  4, batch     7 | loss: 3.6793675Losses:  3.5475077629089355 0.27420419454574585 0.8608306050300598
MemoryTrain:  epoch  4, batch     8 | loss: 3.5475078Losses:  2.911278009414673 0.490475594997406 0.8585166931152344
MemoryTrain:  epoch  4, batch     9 | loss: 2.9112780Losses:  3.122011661529541 0.25954023003578186 0.9530920386314392
MemoryTrain:  epoch  4, batch    10 | loss: 3.1220117Losses:  2.7657365798950195 -0.0 0.9501605033874512
MemoryTrain:  epoch  4, batch    11 | loss: 2.7657366Losses:  3.2866897583007812 0.8042385578155518 0.903225302696228
MemoryTrain:  epoch  4, batch    12 | loss: 3.2866898Losses:  1.2838078737258911 -0.0 0.11168704181909561
MemoryTrain:  epoch  4, batch    13 | loss: 1.2838079Losses:  2.4126663208007812 -0.0 1.011285662651062
MemoryTrain:  epoch  5, batch     0 | loss: 2.4126663Losses:  3.8846182823181152 0.25865501165390015 1.0134010314941406
MemoryTrain:  epoch  5, batch     1 | loss: 3.8846183Losses:  3.379737138748169 0.25990888476371765 0.9553086161613464
MemoryTrain:  epoch  5, batch     2 | loss: 3.3797371Losses:  3.6399481296539307 0.8785287141799927 1.0298244953155518
MemoryTrain:  epoch  5, batch     3 | loss: 3.6399481Losses:  2.7291431427001953 0.2667558789253235 0.9437124133110046
MemoryTrain:  epoch  5, batch     4 | loss: 2.7291431Losses:  2.8604350090026855 0.23917904496192932 0.9266175031661987
MemoryTrain:  epoch  5, batch     5 | loss: 2.8604350Losses:  3.564255714416504 0.4731009006500244 0.8986675143241882
MemoryTrain:  epoch  5, batch     6 | loss: 3.5642557Losses:  3.23492431640625 0.4893980026245117 1.0207880735397339
MemoryTrain:  epoch  5, batch     7 | loss: 3.2349243Losses:  2.7644453048706055 0.26629167795181274 0.9069811105728149
MemoryTrain:  epoch  5, batch     8 | loss: 2.7644453Losses:  3.1267924308776855 0.505124568939209 0.8525408506393433
MemoryTrain:  epoch  5, batch     9 | loss: 3.1267924Losses:  2.821554660797119 -0.0 1.029061198234558
MemoryTrain:  epoch  5, batch    10 | loss: 2.8215547Losses:  3.1052324771881104 0.5881929397583008 0.9112534523010254
MemoryTrain:  epoch  5, batch    11 | loss: 3.1052325Losses:  2.9261245727539062 0.2818957567214966 0.8741784691810608
MemoryTrain:  epoch  5, batch    12 | loss: 2.9261246Losses:  1.562221884727478 -0.0 0.11202750355005264
MemoryTrain:  epoch  5, batch    13 | loss: 1.5622219Losses:  2.811119794845581 -0.0 0.9683782458305359
MemoryTrain:  epoch  6, batch     0 | loss: 2.8111198Losses:  2.99107027053833 0.4913201630115509 0.9581367373466492
MemoryTrain:  epoch  6, batch     1 | loss: 2.9910703Losses:  3.2681517601013184 0.7664204239845276 0.9167755842208862
MemoryTrain:  epoch  6, batch     2 | loss: 3.2681518Losses:  2.7337465286254883 0.25324901938438416 1.0360862016677856
MemoryTrain:  epoch  6, batch     3 | loss: 2.7337465Losses:  2.5629444122314453 -0.0 0.8638049364089966
MemoryTrain:  epoch  6, batch     4 | loss: 2.5629444Losses:  2.555814743041992 -0.0 0.9733328223228455
MemoryTrain:  epoch  6, batch     5 | loss: 2.5558147Losses:  2.5745911598205566 -0.0 0.9048768877983093
MemoryTrain:  epoch  6, batch     6 | loss: 2.5745912Losses:  2.748483419418335 -0.0 1.0000910758972168
MemoryTrain:  epoch  6, batch     7 | loss: 2.7484834Losses:  2.4840216636657715 -0.0 1.0388305187225342
MemoryTrain:  epoch  6, batch     8 | loss: 2.4840217Losses:  2.862863540649414 0.27099162340164185 1.042237639427185
MemoryTrain:  epoch  6, batch     9 | loss: 2.8628635Losses:  2.8969566822052 0.25313228368759155 0.9623773097991943
MemoryTrain:  epoch  6, batch    10 | loss: 2.8969567Losses:  3.074387311935425 0.4817126393318176 0.852294921875
MemoryTrain:  epoch  6, batch    11 | loss: 3.0743873Losses:  3.6898298263549805 1.123031735420227 0.9074620008468628
MemoryTrain:  epoch  6, batch    12 | loss: 3.6898298Losses:  2.2672879695892334 -0.0 0.11369327455759048
MemoryTrain:  epoch  6, batch    13 | loss: 2.2672880Losses:  2.4749860763549805 0.2241297960281372 0.9436189532279968
MemoryTrain:  epoch  7, batch     0 | loss: 2.4749861Losses:  2.1485939025878906 -0.0 0.8945491313934326
MemoryTrain:  epoch  7, batch     1 | loss: 2.1485939Losses:  3.123004198074341 0.3033781349658966 0.8487303853034973
MemoryTrain:  epoch  7, batch     2 | loss: 3.1230042Losses:  3.3841452598571777 0.315931111574173 1.005527377128601
MemoryTrain:  epoch  7, batch     3 | loss: 3.3841453Losses:  2.8042404651641846 0.49399030208587646 0.8589624166488647
MemoryTrain:  epoch  7, batch     4 | loss: 2.8042405Losses:  3.0688817501068115 0.29626598954200745 0.886396586894989
MemoryTrain:  epoch  7, batch     5 | loss: 3.0688818Losses:  2.6509411334991455 0.2534313499927521 1.0134586095809937
MemoryTrain:  epoch  7, batch     6 | loss: 2.6509411Losses:  2.572202205657959 -0.0 0.9924708604812622
MemoryTrain:  epoch  7, batch     7 | loss: 2.5722022Losses:  2.9930601119995117 0.342560738325119 1.0673515796661377
MemoryTrain:  epoch  7, batch     8 | loss: 2.9930601Losses:  3.4871835708618164 0.6802905201911926 0.9282277822494507
MemoryTrain:  epoch  7, batch     9 | loss: 3.4871836Losses:  2.7526421546936035 0.4914592504501343 1.0131248235702515
MemoryTrain:  epoch  7, batch    10 | loss: 2.7526422Losses:  2.9066195487976074 0.24739378690719604 1.0299445390701294
MemoryTrain:  epoch  7, batch    11 | loss: 2.9066195Losses:  2.6558027267456055 0.49798834323883057 0.9019238948822021
MemoryTrain:  epoch  7, batch    12 | loss: 2.6558027Losses:  1.2673510313034058 -0.0 0.12848009169101715
MemoryTrain:  epoch  7, batch    13 | loss: 1.2673510Losses:  3.338744640350342 1.0588083267211914 0.8313482999801636
MemoryTrain:  epoch  8, batch     0 | loss: 3.3387446Losses:  2.366058111190796 -0.0 1.0181810855865479
MemoryTrain:  epoch  8, batch     1 | loss: 2.3660581Losses:  2.5283827781677246 -0.0 0.8478690385818481
MemoryTrain:  epoch  8, batch     2 | loss: 2.5283828Losses:  2.744192600250244 0.45564544200897217 0.9480146169662476
MemoryTrain:  epoch  8, batch     3 | loss: 2.7441926Losses:  2.6009700298309326 -0.0 0.9659901857376099
MemoryTrain:  epoch  8, batch     4 | loss: 2.6009700Losses:  3.0436620712280273 0.8164345622062683 0.8369966149330139
MemoryTrain:  epoch  8, batch     5 | loss: 3.0436621Losses:  2.380948066711426 -0.0 0.9096052050590515
MemoryTrain:  epoch  8, batch     6 | loss: 2.3809481Losses:  2.5386414527893066 0.2538587152957916 0.9850728511810303
MemoryTrain:  epoch  8, batch     7 | loss: 2.5386415Losses:  2.71440052986145 0.4581313729286194 0.9026546478271484
MemoryTrain:  epoch  8, batch     8 | loss: 2.7144005Losses:  2.7524828910827637 0.5172483921051025 0.9289113283157349
MemoryTrain:  epoch  8, batch     9 | loss: 2.7524829Losses:  2.5784406661987305 -0.0 1.0160130262374878
MemoryTrain:  epoch  8, batch    10 | loss: 2.5784407Losses:  2.3123488426208496 -0.0 1.0686308145523071
MemoryTrain:  epoch  8, batch    11 | loss: 2.3123488Losses:  2.5367183685302734 0.2771430015563965 0.9052248001098633
MemoryTrain:  epoch  8, batch    12 | loss: 2.5367184Losses:  1.773056149482727 -0.0 0.16839274764060974
MemoryTrain:  epoch  8, batch    13 | loss: 1.7730561Losses:  2.473444700241089 0.23854602873325348 0.9898425936698914
MemoryTrain:  epoch  9, batch     0 | loss: 2.4734447Losses:  2.6470580101013184 -0.0 1.1412036418914795
MemoryTrain:  epoch  9, batch     1 | loss: 2.6470580Losses:  2.356905698776245 -0.0 1.0608547925949097
MemoryTrain:  epoch  9, batch     2 | loss: 2.3569057Losses:  2.2815706729888916 -0.0 0.9174534678459167
MemoryTrain:  epoch  9, batch     3 | loss: 2.2815707Losses:  3.0423758029937744 0.5482193231582642 0.8467690348625183
MemoryTrain:  epoch  9, batch     4 | loss: 3.0423758Losses:  2.5852725505828857 0.2654852271080017 0.9498875141143799
MemoryTrain:  epoch  9, batch     5 | loss: 2.5852726Losses:  2.451831579208374 0.24700309336185455 0.8070926070213318
MemoryTrain:  epoch  9, batch     6 | loss: 2.4518316Losses:  2.7676312923431396 0.5087014436721802 0.7305510640144348
MemoryTrain:  epoch  9, batch     7 | loss: 2.7676313Losses:  2.635399341583252 0.5192558765411377 0.8769307732582092
MemoryTrain:  epoch  9, batch     8 | loss: 2.6353993Losses:  2.5992884635925293 -0.0 1.1343944072723389
MemoryTrain:  epoch  9, batch     9 | loss: 2.5992885Losses:  2.71498703956604 0.5134795904159546 0.8379780650138855
MemoryTrain:  epoch  9, batch    10 | loss: 2.7149870Losses:  2.3826637268066406 -0.0 1.0751194953918457
MemoryTrain:  epoch  9, batch    11 | loss: 2.3826637Losses:  2.401859998703003 0.23478245735168457 0.9637608528137207
MemoryTrain:  epoch  9, batch    12 | loss: 2.4018600Losses:  1.3560903072357178 -0.0 -0.0
MemoryTrain:  epoch  9, batch    13 | loss: 1.3560903
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 50.00%,  total acc: 59.38%   [EVAL] batch:    2 | acc: 62.50%,  total acc: 60.42%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 65.62%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 67.50%   [EVAL] batch:    5 | acc: 68.75%,  total acc: 67.71%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 71.43%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 74.22%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 77.08%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 78.12%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 80.11%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 80.29%   [EVAL] batch:   13 | acc: 18.75%,  total acc: 75.89%   [EVAL] batch:   14 | acc: 18.75%,  total acc: 72.08%   [EVAL] batch:   15 | acc: 6.25%,  total acc: 67.97%   [EVAL] batch:   16 | acc: 25.00%,  total acc: 65.44%   [EVAL] batch:   17 | acc: 12.50%,  total acc: 62.50%   [EVAL] batch:   18 | acc: 31.25%,  total acc: 60.86%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 62.50%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 64.29%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 65.62%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 67.12%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 68.49%   [EVAL] batch:   24 | acc: 93.75%,  total acc: 69.50%   [EVAL] batch:   25 | acc: 37.50%,  total acc: 68.27%   [EVAL] batch:   26 | acc: 25.00%,  total acc: 66.67%   [EVAL] batch:   27 | acc: 6.25%,  total acc: 64.51%   [EVAL] batch:   28 | acc: 50.00%,  total acc: 64.01%   [EVAL] batch:   29 | acc: 25.00%,  total acc: 62.71%   [EVAL] batch:   30 | acc: 12.50%,  total acc: 61.09%   [EVAL] batch:   31 | acc: 81.25%,  total acc: 61.72%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 62.69%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 63.79%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 64.82%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 65.45%   [EVAL] batch:   36 | acc: 93.75%,  total acc: 66.22%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 66.78%   [EVAL] batch:   38 | acc: 87.50%,  total acc: 67.31%   [EVAL] batch:   39 | acc: 56.25%,  total acc: 67.03%   [EVAL] batch:   40 | acc: 81.25%,  total acc: 67.38%   [EVAL] batch:   41 | acc: 68.75%,  total acc: 67.41%   [EVAL] batch:   42 | acc: 68.75%,  total acc: 67.44%   [EVAL] batch:   43 | acc: 68.75%,  total acc: 67.47%   [EVAL] batch:   44 | acc: 25.00%,  total acc: 66.53%   [EVAL] batch:   45 | acc: 50.00%,  total acc: 66.17%   [EVAL] batch:   46 | acc: 37.50%,  total acc: 65.56%   [EVAL] batch:   47 | acc: 37.50%,  total acc: 64.97%   [EVAL] batch:   48 | acc: 50.00%,  total acc: 64.67%   [EVAL] batch:   49 | acc: 37.50%,  total acc: 64.12%   [EVAL] batch:   50 | acc: 62.50%,  total acc: 64.09%   [EVAL] batch:   51 | acc: 56.25%,  total acc: 63.94%   [EVAL] batch:   52 | acc: 81.25%,  total acc: 64.27%   [EVAL] batch:   53 | acc: 68.75%,  total acc: 64.35%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 64.77%   [EVAL] batch:   55 | acc: 75.00%,  total acc: 64.96%   [EVAL] batch:   56 | acc: 75.00%,  total acc: 65.13%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 65.52%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 66.00%   [EVAL] batch:   59 | acc: 87.50%,  total acc: 66.35%   [EVAL] batch:   60 | acc: 87.50%,  total acc: 66.70%   [EVAL] batch:   61 | acc: 81.25%,  total acc: 66.94%   [EVAL] batch:   62 | acc: 43.75%,  total acc: 66.57%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 79.69%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 77.50%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 80.21%   [EVAL] batch:    6 | acc: 50.00%,  total acc: 75.89%   [EVAL] batch:    7 | acc: 50.00%,  total acc: 72.66%   [EVAL] batch:    8 | acc: 62.50%,  total acc: 71.53%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 70.62%   [EVAL] batch:   10 | acc: 56.25%,  total acc: 69.32%   [EVAL] batch:   11 | acc: 43.75%,  total acc: 67.19%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 68.27%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 70.09%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 71.67%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 73.44%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 74.63%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 75.69%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 76.97%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 76.88%   [EVAL] batch:   20 | acc: 81.25%,  total acc: 77.08%   [EVAL] batch:   21 | acc: 75.00%,  total acc: 76.99%   [EVAL] batch:   22 | acc: 68.75%,  total acc: 76.63%   [EVAL] batch:   23 | acc: 81.25%,  total acc: 76.82%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 77.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 78.12%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 78.94%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 79.69%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 80.17%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 80.83%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 81.45%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 82.03%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 82.39%   [EVAL] batch:   33 | acc: 68.75%,  total acc: 81.99%   [EVAL] batch:   34 | acc: 68.75%,  total acc: 81.61%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 81.77%   [EVAL] batch:   36 | acc: 87.50%,  total acc: 81.93%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 82.24%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 82.69%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 83.12%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 83.54%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 83.93%   [EVAL] batch:   42 | acc: 87.50%,  total acc: 84.01%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 84.23%   [EVAL] batch:   44 | acc: 81.25%,  total acc: 84.17%   [EVAL] batch:   45 | acc: 81.25%,  total acc: 84.10%   [EVAL] batch:   46 | acc: 68.75%,  total acc: 83.78%   [EVAL] batch:   47 | acc: 68.75%,  total acc: 83.46%   [EVAL] batch:   48 | acc: 81.25%,  total acc: 83.42%   [EVAL] batch:   49 | acc: 75.00%,  total acc: 83.25%   [EVAL] batch:   50 | acc: 81.25%,  total acc: 83.21%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 83.29%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 83.49%   [EVAL] batch:   53 | acc: 68.75%,  total acc: 83.22%   [EVAL] batch:   54 | acc: 56.25%,  total acc: 82.73%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 82.81%   [EVAL] batch:   56 | acc: 62.50%,  total acc: 82.46%   [EVAL] batch:   57 | acc: 43.75%,  total acc: 81.79%   [EVAL] batch:   58 | acc: 62.50%,  total acc: 81.46%   [EVAL] batch:   59 | acc: 75.00%,  total acc: 81.35%   [EVAL] batch:   60 | acc: 81.25%,  total acc: 81.35%   [EVAL] batch:   61 | acc: 62.50%,  total acc: 81.05%   [EVAL] batch:   62 | acc: 62.50%,  total acc: 80.75%   [EVAL] batch:   63 | acc: 81.25%,  total acc: 80.76%   [EVAL] batch:   64 | acc: 75.00%,  total acc: 80.67%   [EVAL] batch:   65 | acc: 87.50%,  total acc: 80.78%   [EVAL] batch:   66 | acc: 68.75%,  total acc: 80.60%   [EVAL] batch:   67 | acc: 56.25%,  total acc: 80.24%   [EVAL] batch:   68 | acc: 81.25%,  total acc: 80.25%   [EVAL] batch:   69 | acc: 81.25%,  total acc: 80.27%   [EVAL] batch:   70 | acc: 68.75%,  total acc: 80.11%   [EVAL] batch:   71 | acc: 62.50%,  total acc: 79.86%   [EVAL] batch:   72 | acc: 81.25%,  total acc: 79.88%   [EVAL] batch:   73 | acc: 68.75%,  total acc: 79.73%   [EVAL] batch:   74 | acc: 81.25%,  total acc: 79.75%   [EVAL] batch:   75 | acc: 56.25%,  total acc: 79.44%   [EVAL] batch:   76 | acc: 62.50%,  total acc: 79.22%   [EVAL] batch:   77 | acc: 56.25%,  total acc: 78.93%   [EVAL] batch:   78 | acc: 56.25%,  total acc: 78.64%   [EVAL] batch:   79 | acc: 68.75%,  total acc: 78.52%   [EVAL] batch:   80 | acc: 75.00%,  total acc: 78.47%   [EVAL] batch:   81 | acc: 25.00%,  total acc: 77.82%   [EVAL] batch:   82 | acc: 12.50%,  total acc: 77.03%   [EVAL] batch:   83 | acc: 0.00%,  total acc: 76.12%   [EVAL] batch:   84 | acc: 0.00%,  total acc: 75.22%   [EVAL] batch:   85 | acc: 12.50%,  total acc: 74.49%   [EVAL] batch:   86 | acc: 18.75%,  total acc: 73.85%   [EVAL] batch:   87 | acc: 50.00%,  total acc: 73.58%   [EVAL] batch:   88 | acc: 93.75%,  total acc: 73.81%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 74.10%   [EVAL] batch:   90 | acc: 93.75%,  total acc: 74.31%   [EVAL] batch:   91 | acc: 93.75%,  total acc: 74.52%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 74.80%   [EVAL] batch:   93 | acc: 93.75%,  total acc: 75.00%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 75.26%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 75.52%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 75.77%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 76.02%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 76.26%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 76.50%   [EVAL] batch:  100 | acc: 75.00%,  total acc: 76.49%   [EVAL] batch:  101 | acc: 75.00%,  total acc: 76.47%   [EVAL] batch:  102 | acc: 68.75%,  total acc: 76.40%   [EVAL] batch:  103 | acc: 75.00%,  total acc: 76.38%   [EVAL] batch:  104 | acc: 93.75%,  total acc: 76.55%   [EVAL] batch:  105 | acc: 62.50%,  total acc: 76.42%   [EVAL] batch:  106 | acc: 31.25%,  total acc: 75.99%   [EVAL] batch:  107 | acc: 12.50%,  total acc: 75.41%   [EVAL] batch:  108 | acc: 37.50%,  total acc: 75.06%   [EVAL] batch:  109 | acc: 12.50%,  total acc: 74.49%   [EVAL] batch:  110 | acc: 50.00%,  total acc: 74.27%   [EVAL] batch:  111 | acc: 25.00%,  total acc: 73.83%   [EVAL] batch:  112 | acc: 50.00%,  total acc: 73.62%   [EVAL] batch:  113 | acc: 87.50%,  total acc: 73.74%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 73.97%   [EVAL] batch:  115 | acc: 81.25%,  total acc: 74.03%   [EVAL] batch:  116 | acc: 87.50%,  total acc: 74.15%   [EVAL] batch:  117 | acc: 93.75%,  total acc: 74.31%   [EVAL] batch:  118 | acc: 81.25%,  total acc: 74.37%   [EVAL] batch:  119 | acc: 0.00%,  total acc: 73.75%   [EVAL] batch:  120 | acc: 18.75%,  total acc: 73.30%   [EVAL] batch:  121 | acc: 6.25%,  total acc: 72.75%   [EVAL] batch:  122 | acc: 18.75%,  total acc: 72.31%   [EVAL] batch:  123 | acc: 0.00%,  total acc: 71.72%   [EVAL] batch:  124 | acc: 6.25%,  total acc: 71.20%   [EVAL] batch:  125 | acc: 75.00%,  total acc: 71.23%   [EVAL] batch:  126 | acc: 31.25%,  total acc: 70.92%   [EVAL] batch:  127 | acc: 43.75%,  total acc: 70.70%   [EVAL] batch:  128 | acc: 25.00%,  total acc: 70.35%   [EVAL] batch:  129 | acc: 62.50%,  total acc: 70.29%   [EVAL] batch:  130 | acc: 62.50%,  total acc: 70.23%   [EVAL] batch:  131 | acc: 81.25%,  total acc: 70.31%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 70.54%   [EVAL] batch:  133 | acc: 93.75%,  total acc: 70.71%   [EVAL] batch:  134 | acc: 75.00%,  total acc: 70.74%   [EVAL] batch:  135 | acc: 81.25%,  total acc: 70.82%   [EVAL] batch:  136 | acc: 100.00%,  total acc: 71.03%   [EVAL] batch:  137 | acc: 75.00%,  total acc: 71.06%   [EVAL] batch:  138 | acc: 50.00%,  total acc: 70.91%   [EVAL] batch:  139 | acc: 56.25%,  total acc: 70.80%   [EVAL] batch:  140 | acc: 68.75%,  total acc: 70.79%   [EVAL] batch:  141 | acc: 81.25%,  total acc: 70.86%   [EVAL] batch:  142 | acc: 56.25%,  total acc: 70.76%   [EVAL] batch:  143 | acc: 62.50%,  total acc: 70.70%   [EVAL] batch:  144 | acc: 62.50%,  total acc: 70.65%   [EVAL] batch:  145 | acc: 75.00%,  total acc: 70.68%   [EVAL] batch:  146 | acc: 50.00%,  total acc: 70.54%   [EVAL] batch:  147 | acc: 81.25%,  total acc: 70.61%   [EVAL] batch:  148 | acc: 62.50%,  total acc: 70.55%   [EVAL] batch:  149 | acc: 56.25%,  total acc: 70.46%   [EVAL] batch:  150 | acc: 75.00%,  total acc: 70.49%   [EVAL] batch:  151 | acc: 93.75%,  total acc: 70.64%   [EVAL] batch:  152 | acc: 75.00%,  total acc: 70.67%   [EVAL] batch:  153 | acc: 81.25%,  total acc: 70.74%   [EVAL] batch:  154 | acc: 50.00%,  total acc: 70.60%   [EVAL] batch:  155 | acc: 68.75%,  total acc: 70.59%   [EVAL] batch:  156 | acc: 75.00%,  total acc: 70.62%   [EVAL] batch:  157 | acc: 56.25%,  total acc: 70.53%   [EVAL] batch:  158 | acc: 50.00%,  total acc: 70.40%   [EVAL] batch:  159 | acc: 56.25%,  total acc: 70.31%   [EVAL] batch:  160 | acc: 50.00%,  total acc: 70.19%   [EVAL] batch:  161 | acc: 50.00%,  total acc: 70.06%   [EVAL] batch:  162 | acc: 56.25%,  total acc: 69.98%   [EVAL] batch:  163 | acc: 56.25%,  total acc: 69.89%   [EVAL] batch:  164 | acc: 75.00%,  total acc: 69.92%   [EVAL] batch:  165 | acc: 68.75%,  total acc: 69.92%   [EVAL] batch:  166 | acc: 68.75%,  total acc: 69.91%   [EVAL] batch:  167 | acc: 62.50%,  total acc: 69.87%   [EVAL] batch:  168 | acc: 56.25%,  total acc: 69.79%   [EVAL] batch:  169 | acc: 31.25%,  total acc: 69.56%   [EVAL] batch:  170 | acc: 43.75%,  total acc: 69.41%   [EVAL] batch:  171 | acc: 43.75%,  total acc: 69.26%   [EVAL] batch:  172 | acc: 56.25%,  total acc: 69.18%   [EVAL] batch:  173 | acc: 37.50%,  total acc: 69.00%   [EVAL] batch:  174 | acc: 43.75%,  total acc: 68.86%   [EVAL] batch:  175 | acc: 100.00%,  total acc: 69.03%   [EVAL] batch:  176 | acc: 100.00%,  total acc: 69.21%   [EVAL] batch:  177 | acc: 100.00%,  total acc: 69.38%   [EVAL] batch:  178 | acc: 100.00%,  total acc: 69.55%   [EVAL] batch:  179 | acc: 100.00%,  total acc: 69.72%   [EVAL] batch:  180 | acc: 100.00%,  total acc: 69.89%   [EVAL] batch:  181 | acc: 100.00%,  total acc: 70.05%   [EVAL] batch:  182 | acc: 75.00%,  total acc: 70.08%   [EVAL] batch:  183 | acc: 93.75%,  total acc: 70.21%   [EVAL] batch:  184 | acc: 81.25%,  total acc: 70.27%   [EVAL] batch:  185 | acc: 93.75%,  total acc: 70.40%   [EVAL] batch:  186 | acc: 93.75%,  total acc: 70.52%   [EVAL] batch:  187 | acc: 75.00%,  total acc: 70.55%   [EVAL] batch:  188 | acc: 37.50%,  total acc: 70.37%   [EVAL] batch:  189 | acc: 56.25%,  total acc: 70.30%   [EVAL] batch:  190 | acc: 43.75%,  total acc: 70.16%   [EVAL] batch:  191 | acc: 25.00%,  total acc: 69.92%   [EVAL] batch:  192 | acc: 56.25%,  total acc: 69.85%   [EVAL] batch:  193 | acc: 37.50%,  total acc: 69.68%   [EVAL] batch:  194 | acc: 93.75%,  total acc: 69.81%   [EVAL] batch:  195 | acc: 87.50%,  total acc: 69.90%   [EVAL] batch:  196 | acc: 87.50%,  total acc: 69.99%   [EVAL] batch:  197 | acc: 93.75%,  total acc: 70.11%   [EVAL] batch:  198 | acc: 87.50%,  total acc: 70.19%   [EVAL] batch:  199 | acc: 81.25%,  total acc: 70.25%   [EVAL] batch:  200 | acc: 75.00%,  total acc: 70.27%   [EVAL] batch:  201 | acc: 62.50%,  total acc: 70.24%   [EVAL] batch:  202 | acc: 68.75%,  total acc: 70.23%   [EVAL] batch:  203 | acc: 68.75%,  total acc: 70.22%   [EVAL] batch:  204 | acc: 75.00%,  total acc: 70.24%   [EVAL] batch:  205 | acc: 62.50%,  total acc: 70.21%   [EVAL] batch:  206 | acc: 100.00%,  total acc: 70.35%   [EVAL] batch:  207 | acc: 100.00%,  total acc: 70.49%   [EVAL] batch:  208 | acc: 100.00%,  total acc: 70.63%   [EVAL] batch:  209 | acc: 93.75%,  total acc: 70.74%   [EVAL] batch:  210 | acc: 100.00%,  total acc: 70.88%   [EVAL] batch:  211 | acc: 100.00%,  total acc: 71.02%   [EVAL] batch:  212 | acc: 75.00%,  total acc: 71.04%   [EVAL] batch:  213 | acc: 75.00%,  total acc: 71.06%   [EVAL] batch:  214 | acc: 68.75%,  total acc: 71.05%   [EVAL] batch:  215 | acc: 75.00%,  total acc: 71.06%   [EVAL] batch:  216 | acc: 81.25%,  total acc: 71.11%   [EVAL] batch:  217 | acc: 81.25%,  total acc: 71.16%   [EVAL] batch:  218 | acc: 81.25%,  total acc: 71.20%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 71.34%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 71.46%   [EVAL] batch:  221 | acc: 87.50%,  total acc: 71.54%   [EVAL] batch:  222 | acc: 93.75%,  total acc: 71.64%   [EVAL] batch:  223 | acc: 93.75%,  total acc: 71.74%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 71.86%   [EVAL] batch:  225 | acc: 37.50%,  total acc: 71.71%   [EVAL] batch:  226 | acc: 43.75%,  total acc: 71.59%   [EVAL] batch:  227 | acc: 37.50%,  total acc: 71.44%   [EVAL] batch:  228 | acc: 43.75%,  total acc: 71.32%   [EVAL] batch:  229 | acc: 6.25%,  total acc: 71.03%   [EVAL] batch:  230 | acc: 37.50%,  total acc: 70.89%   [EVAL] batch:  231 | acc: 93.75%,  total acc: 70.99%   [EVAL] batch:  232 | acc: 93.75%,  total acc: 71.08%   [EVAL] batch:  233 | acc: 87.50%,  total acc: 71.15%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 71.28%   [EVAL] batch:  235 | acc: 93.75%,  total acc: 71.37%   [EVAL] batch:  236 | acc: 93.75%,  total acc: 71.47%   [EVAL] batch:  237 | acc: 68.75%,  total acc: 71.45%   [EVAL] batch:  238 | acc: 75.00%,  total acc: 71.47%   [EVAL] batch:  239 | acc: 62.50%,  total acc: 71.43%   [EVAL] batch:  240 | acc: 56.25%,  total acc: 71.37%   [EVAL] batch:  241 | acc: 81.25%,  total acc: 71.41%   [EVAL] batch:  242 | acc: 56.25%,  total acc: 71.35%   [EVAL] batch:  243 | acc: 43.75%,  total acc: 71.23%   [EVAL] batch:  244 | acc: 68.75%,  total acc: 71.22%   [EVAL] batch:  245 | acc: 62.50%,  total acc: 71.19%   [EVAL] batch:  246 | acc: 56.25%,  total acc: 71.13%   [EVAL] batch:  247 | acc: 68.75%,  total acc: 71.12%   [EVAL] batch:  248 | acc: 87.50%,  total acc: 71.18%   [EVAL] batch:  249 | acc: 68.75%,  total acc: 71.17%   [EVAL] batch:  250 | acc: 93.75%,  total acc: 71.26%   [EVAL] batch:  251 | acc: 81.25%,  total acc: 71.30%   [EVAL] batch:  252 | acc: 81.25%,  total acc: 71.34%   [EVAL] batch:  253 | acc: 75.00%,  total acc: 71.36%   [EVAL] batch:  254 | acc: 93.75%,  total acc: 71.45%   [EVAL] batch:  255 | acc: 100.00%,  total acc: 71.56%   [EVAL] batch:  256 | acc: 50.00%,  total acc: 71.47%   [EVAL] batch:  257 | acc: 31.25%,  total acc: 71.32%   [EVAL] batch:  258 | acc: 43.75%,  total acc: 71.21%   [EVAL] batch:  259 | acc: 50.00%,  total acc: 71.13%   [EVAL] batch:  260 | acc: 31.25%,  total acc: 70.98%   [EVAL] batch:  261 | acc: 56.25%,  total acc: 70.92%   [EVAL] batch:  262 | acc: 56.25%,  total acc: 70.87%   [EVAL] batch:  263 | acc: 81.25%,  total acc: 70.90%   [EVAL] batch:  264 | acc: 62.50%,  total acc: 70.87%   [EVAL] batch:  265 | acc: 62.50%,  total acc: 70.84%   [EVAL] batch:  266 | acc: 68.75%,  total acc: 70.83%   [EVAL] batch:  267 | acc: 62.50%,  total acc: 70.80%   [EVAL] batch:  268 | acc: 75.00%,  total acc: 70.82%   [EVAL] batch:  269 | acc: 56.25%,  total acc: 70.76%   [EVAL] batch:  270 | acc: 68.75%,  total acc: 70.76%   [EVAL] batch:  271 | acc: 75.00%,  total acc: 70.77%   [EVAL] batch:  272 | acc: 56.25%,  total acc: 70.72%   [EVAL] batch:  273 | acc: 81.25%,  total acc: 70.76%   [EVAL] batch:  274 | acc: 62.50%,  total acc: 70.73%   [EVAL] batch:  275 | acc: 100.00%,  total acc: 70.83%   [EVAL] batch:  276 | acc: 100.00%,  total acc: 70.94%   [EVAL] batch:  277 | acc: 100.00%,  total acc: 71.04%   [EVAL] batch:  278 | acc: 100.00%,  total acc: 71.15%   [EVAL] batch:  279 | acc: 100.00%,  total acc: 71.25%   [EVAL] batch:  280 | acc: 100.00%,  total acc: 71.35%   [EVAL] batch:  281 | acc: 81.25%,  total acc: 71.39%   [EVAL] batch:  282 | acc: 81.25%,  total acc: 71.42%   [EVAL] batch:  283 | acc: 62.50%,  total acc: 71.39%   [EVAL] batch:  284 | acc: 93.75%,  total acc: 71.47%   [EVAL] batch:  285 | acc: 62.50%,  total acc: 71.44%   [EVAL] batch:  286 | acc: 68.75%,  total acc: 71.43%   [EVAL] batch:  287 | acc: 62.50%,  total acc: 71.40%   [EVAL] batch:  288 | acc: 100.00%,  total acc: 71.50%   [EVAL] batch:  289 | acc: 100.00%,  total acc: 71.59%   [EVAL] batch:  290 | acc: 100.00%,  total acc: 71.69%   [EVAL] batch:  291 | acc: 100.00%,  total acc: 71.79%   [EVAL] batch:  292 | acc: 100.00%,  total acc: 71.89%   [EVAL] batch:  293 | acc: 93.75%,  total acc: 71.96%   [EVAL] batch:  294 | acc: 100.00%,  total acc: 72.06%   [EVAL] batch:  295 | acc: 100.00%,  total acc: 72.15%   [EVAL] batch:  296 | acc: 100.00%,  total acc: 72.24%   [EVAL] batch:  297 | acc: 100.00%,  total acc: 72.34%   [EVAL] batch:  298 | acc: 100.00%,  total acc: 72.43%   [EVAL] batch:  299 | acc: 100.00%,  total acc: 72.52%   [EVAL] batch:  300 | acc: 93.75%,  total acc: 72.59%   [EVAL] batch:  301 | acc: 93.75%,  total acc: 72.66%   [EVAL] batch:  302 | acc: 93.75%,  total acc: 72.73%   [EVAL] batch:  303 | acc: 93.75%,  total acc: 72.80%   [EVAL] batch:  304 | acc: 81.25%,  total acc: 72.83%   [EVAL] batch:  305 | acc: 93.75%,  total acc: 72.90%   [EVAL] batch:  306 | acc: 81.25%,  total acc: 72.92%   [EVAL] batch:  307 | acc: 75.00%,  total acc: 72.93%   [EVAL] batch:  308 | acc: 93.75%,  total acc: 73.00%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 73.08%   [EVAL] batch:  310 | acc: 100.00%,  total acc: 73.17%   [EVAL] batch:  311 | acc: 81.25%,  total acc: 73.20%   [EVAL] batch:  312 | acc: 43.75%,  total acc: 73.10%   [EVAL] batch:  313 | acc: 0.00%,  total acc: 72.87%   [EVAL] batch:  314 | acc: 25.00%,  total acc: 72.72%   [EVAL] batch:  315 | acc: 0.00%,  total acc: 72.49%   [EVAL] batch:  316 | acc: 6.25%,  total acc: 72.28%   [EVAL] batch:  317 | acc: 0.00%,  total acc: 72.05%   [EVAL] batch:  318 | acc: 25.00%,  total acc: 71.90%   [EVAL] batch:  319 | acc: 75.00%,  total acc: 71.91%   [EVAL] batch:  320 | acc: 87.50%,  total acc: 71.96%   [EVAL] batch:  321 | acc: 87.50%,  total acc: 72.01%   [EVAL] batch:  322 | acc: 93.75%,  total acc: 72.08%   [EVAL] batch:  323 | acc: 68.75%,  total acc: 72.07%   [EVAL] batch:  324 | acc: 81.25%,  total acc: 72.10%   [EVAL] batch:  325 | acc: 75.00%,  total acc: 72.11%   [EVAL] batch:  326 | acc: 56.25%,  total acc: 72.06%   [EVAL] batch:  327 | acc: 68.75%,  total acc: 72.05%   [EVAL] batch:  328 | acc: 68.75%,  total acc: 72.04%   [EVAL] batch:  329 | acc: 43.75%,  total acc: 71.95%   [EVAL] batch:  330 | acc: 62.50%,  total acc: 71.92%   [EVAL] batch:  331 | acc: 6.25%,  total acc: 71.72%   [EVAL] batch:  332 | acc: 31.25%,  total acc: 71.60%   [EVAL] batch:  333 | acc: 12.50%,  total acc: 71.43%   [EVAL] batch:  334 | acc: 6.25%,  total acc: 71.23%   [EVAL] batch:  335 | acc: 6.25%,  total acc: 71.04%   [EVAL] batch:  336 | acc: 25.00%,  total acc: 70.90%   [EVAL] batch:  337 | acc: 25.00%,  total acc: 70.77%   [EVAL] batch:  338 | acc: 12.50%,  total acc: 70.59%   [EVAL] batch:  339 | acc: 12.50%,  total acc: 70.42%   [EVAL] batch:  340 | acc: 0.00%,  total acc: 70.22%   [EVAL] batch:  341 | acc: 0.00%,  total acc: 70.01%   [EVAL] batch:  342 | acc: 6.25%,  total acc: 69.83%   [EVAL] batch:  343 | acc: 18.75%,  total acc: 69.68%   [EVAL] batch:  344 | acc: 81.25%,  total acc: 69.71%   [EVAL] batch:  345 | acc: 87.50%,  total acc: 69.76%   [EVAL] batch:  346 | acc: 93.75%,  total acc: 69.83%   [EVAL] batch:  347 | acc: 93.75%,  total acc: 69.90%   [EVAL] batch:  348 | acc: 81.25%,  total acc: 69.93%   [EVAL] batch:  349 | acc: 87.50%,  total acc: 69.98%   [EVAL] batch:  350 | acc: 100.00%,  total acc: 70.07%   [EVAL] batch:  351 | acc: 93.75%,  total acc: 70.13%   [EVAL] batch:  352 | acc: 93.75%,  total acc: 70.20%   [EVAL] batch:  353 | acc: 87.50%,  total acc: 70.25%   [EVAL] batch:  354 | acc: 93.75%,  total acc: 70.32%   [EVAL] batch:  355 | acc: 100.00%,  total acc: 70.40%   [EVAL] batch:  356 | acc: 87.50%,  total acc: 70.45%   [EVAL] batch:  357 | acc: 68.75%,  total acc: 70.44%   [EVAL] batch:  358 | acc: 37.50%,  total acc: 70.35%   [EVAL] batch:  359 | acc: 75.00%,  total acc: 70.36%   [EVAL] batch:  360 | acc: 75.00%,  total acc: 70.38%   [EVAL] batch:  361 | acc: 81.25%,  total acc: 70.41%   [EVAL] batch:  362 | acc: 31.25%,  total acc: 70.30%   [EVAL] batch:  363 | acc: 25.00%,  total acc: 70.18%   [EVAL] batch:  364 | acc: 43.75%,  total acc: 70.10%   [EVAL] batch:  365 | acc: 62.50%,  total acc: 70.08%   [EVAL] batch:  366 | acc: 43.75%,  total acc: 70.01%   [EVAL] batch:  367 | acc: 62.50%,  total acc: 69.99%   [EVAL] batch:  368 | acc: 50.00%,  total acc: 69.94%   [EVAL] batch:  369 | acc: 50.00%,  total acc: 69.88%   [EVAL] batch:  370 | acc: 43.75%,  total acc: 69.81%   [EVAL] batch:  371 | acc: 43.75%,  total acc: 69.74%   [EVAL] batch:  372 | acc: 56.25%,  total acc: 69.71%   [EVAL] batch:  373 | acc: 81.25%,  total acc: 69.74%   [EVAL] batch:  374 | acc: 68.75%,  total acc: 69.73%   [EVAL] batch:  375 | acc: 68.75%,  total acc: 69.73%   [EVAL] batch:  376 | acc: 50.00%,  total acc: 69.68%   [EVAL] batch:  377 | acc: 62.50%,  total acc: 69.66%   [EVAL] batch:  378 | acc: 81.25%,  total acc: 69.69%   [EVAL] batch:  379 | acc: 75.00%,  total acc: 69.70%   [EVAL] batch:  380 | acc: 68.75%,  total acc: 69.70%   [EVAL] batch:  381 | acc: 93.75%,  total acc: 69.76%   [EVAL] batch:  382 | acc: 93.75%,  total acc: 69.83%   [EVAL] batch:  383 | acc: 100.00%,  total acc: 69.91%   [EVAL] batch:  384 | acc: 87.50%,  total acc: 69.95%   [EVAL] batch:  385 | acc: 100.00%,  total acc: 70.03%   [EVAL] batch:  386 | acc: 93.75%,  total acc: 70.09%   [EVAL] batch:  387 | acc: 68.75%,  total acc: 70.09%   [EVAL] batch:  388 | acc: 18.75%,  total acc: 69.96%   [EVAL] batch:  389 | acc: 18.75%,  total acc: 69.82%   [EVAL] batch:  390 | acc: 6.25%,  total acc: 69.66%   [EVAL] batch:  391 | acc: 25.00%,  total acc: 69.55%   [EVAL] batch:  392 | acc: 12.50%,  total acc: 69.40%   [EVAL] batch:  393 | acc: 31.25%,  total acc: 69.31%   [EVAL] batch:  394 | acc: 93.75%,  total acc: 69.37%   [EVAL] batch:  395 | acc: 100.00%,  total acc: 69.44%   [EVAL] batch:  396 | acc: 93.75%,  total acc: 69.51%   [EVAL] batch:  397 | acc: 100.00%,  total acc: 69.58%   [EVAL] batch:  398 | acc: 100.00%,  total acc: 69.66%   [EVAL] batch:  399 | acc: 93.75%,  total acc: 69.72%   [EVAL] batch:  400 | acc: 37.50%,  total acc: 69.64%   [EVAL] batch:  401 | acc: 25.00%,  total acc: 69.53%   [EVAL] batch:  402 | acc: 6.25%,  total acc: 69.37%   [EVAL] batch:  403 | acc: 50.00%,  total acc: 69.32%   [EVAL] batch:  404 | acc: 25.00%,  total acc: 69.21%   [EVAL] batch:  405 | acc: 12.50%,  total acc: 69.07%   [EVAL] batch:  406 | acc: 81.25%,  total acc: 69.10%   [EVAL] batch:  407 | acc: 93.75%,  total acc: 69.16%   [EVAL] batch:  408 | acc: 100.00%,  total acc: 69.24%   [EVAL] batch:  409 | acc: 100.00%,  total acc: 69.31%   [EVAL] batch:  410 | acc: 87.50%,  total acc: 69.36%   [EVAL] batch:  411 | acc: 93.75%,  total acc: 69.42%   [EVAL] batch:  412 | acc: 87.50%,  total acc: 69.46%   [EVAL] batch:  413 | acc: 87.50%,  total acc: 69.50%   [EVAL] batch:  414 | acc: 56.25%,  total acc: 69.47%   [EVAL] batch:  415 | acc: 81.25%,  total acc: 69.50%   [EVAL] batch:  416 | acc: 68.75%,  total acc: 69.50%   [EVAL] batch:  417 | acc: 68.75%,  total acc: 69.50%   [EVAL] batch:  418 | acc: 68.75%,  total acc: 69.50%   [EVAL] batch:  419 | acc: 25.00%,  total acc: 69.39%   [EVAL] batch:  420 | acc: 50.00%,  total acc: 69.34%   [EVAL] batch:  421 | acc: 37.50%,  total acc: 69.27%   [EVAL] batch:  422 | acc: 37.50%,  total acc: 69.19%   [EVAL] batch:  423 | acc: 50.00%,  total acc: 69.15%   [EVAL] batch:  424 | acc: 37.50%,  total acc: 69.07%   [EVAL] batch:  425 | acc: 62.50%,  total acc: 69.06%   [EVAL] batch:  426 | acc: 56.25%,  total acc: 69.03%   [EVAL] batch:  427 | acc: 81.25%,  total acc: 69.06%   [EVAL] batch:  428 | acc: 68.75%,  total acc: 69.06%   [EVAL] batch:  429 | acc: 87.50%,  total acc: 69.10%   [EVAL] batch:  430 | acc: 75.00%,  total acc: 69.11%   [EVAL] batch:  431 | acc: 75.00%,  total acc: 69.13%   [EVAL] batch:  432 | acc: 87.50%,  total acc: 69.17%   [EVAL] batch:  433 | acc: 93.75%,  total acc: 69.23%   [EVAL] batch:  434 | acc: 87.50%,  total acc: 69.27%   [EVAL] batch:  435 | acc: 87.50%,  total acc: 69.31%   [EVAL] batch:  436 | acc: 81.25%,  total acc: 69.34%   [EVAL] batch:  437 | acc: 43.75%,  total acc: 69.28%   
cur_acc:  ['0.9286', '0.7827', '0.7262', '0.7331', '0.8452', '0.5784', '0.6657']
his_acc:  ['0.9286', '0.8585', '0.7992', '0.7615', '0.7730', '0.7297', '0.6928']
Clustering into  39  clusters
Clusters:  [31  1  9 25 20 28  9 18  1  8  5 10 18 16 33 32  2 26 37 26 36 11 18 20
 34  8 22 28 13 15 17 13 26  0  0 13 26  7  8 35 25 23  4  1 27 12  5  6
 33  5 20 16 10 24  3 19 10  1 10  1 22 16  3  4  3  6 10 22 30 38 29 21
  2  9 11 17  1 12 14  7]
Losses:  9.390092849731445 3.451664447784424 0.6949695348739624
CurrentTrain: epoch  0, batch     0 | loss: 9.3900928Losses:  8.472569465637207 3.0011987686157227 0.6722728610038757
CurrentTrain: epoch  0, batch     1 | loss: 8.4725695Losses:  10.449422836303711 4.133916854858398 0.7358263731002808
CurrentTrain: epoch  0, batch     2 | loss: 10.4494228Losses:  8.60477066040039 -0.0 0.14767037332057953
CurrentTrain: epoch  0, batch     3 | loss: 8.6047707Losses:  8.658724784851074 3.0300207138061523 0.7362558245658875
CurrentTrain: epoch  1, batch     0 | loss: 8.6587248Losses:  9.598828315734863 5.252081394195557 0.6039779186248779
CurrentTrain: epoch  1, batch     1 | loss: 9.5988283Losses:  7.3062567710876465 2.23061466217041 0.7418249249458313
CurrentTrain: epoch  1, batch     2 | loss: 7.3062568Losses:  3.4135546684265137 -0.0 0.11871196329593658
CurrentTrain: epoch  1, batch     3 | loss: 3.4135547Losses:  7.998993396759033 3.4436428546905518 0.7479224801063538
CurrentTrain: epoch  2, batch     0 | loss: 7.9989934Losses:  9.490457534790039 5.152141094207764 0.7269322276115417
CurrentTrain: epoch  2, batch     1 | loss: 9.4904575Losses:  7.501100540161133 3.659571647644043 0.7110811471939087
CurrentTrain: epoch  2, batch     2 | loss: 7.5011005Losses:  5.152787685394287 -0.0 0.09176568686962128
CurrentTrain: epoch  2, batch     3 | loss: 5.1527877Losses:  7.5064215660095215 3.5776708126068115 0.6698513627052307
CurrentTrain: epoch  3, batch     0 | loss: 7.5064216Losses:  8.067626953125 3.731335401535034 0.6722985506057739
CurrentTrain: epoch  3, batch     1 | loss: 8.0676270Losses:  6.840384483337402 3.525588035583496 0.6817905902862549
CurrentTrain: epoch  3, batch     2 | loss: 6.8403845Losses:  4.4619269371032715 -0.0 0.1777547150850296
CurrentTrain: epoch  3, batch     3 | loss: 4.4619269Losses:  9.276348114013672 4.464364051818848 0.7271807789802551
CurrentTrain: epoch  4, batch     0 | loss: 9.2763481Losses:  7.083756923675537 3.460117816925049 0.7402827143669128
CurrentTrain: epoch  4, batch     1 | loss: 7.0837569Losses:  6.067121505737305 2.6712121963500977 0.7469264268875122
CurrentTrain: epoch  4, batch     2 | loss: 6.0671215Losses:  1.8436144590377808 -0.0 0.10160814970731735
CurrentTrain: epoch  4, batch     3 | loss: 1.8436145Losses:  5.971599578857422 2.2384681701660156 0.7768924832344055
CurrentTrain: epoch  5, batch     0 | loss: 5.9715996Losses:  5.348406791687012 2.3155531883239746 0.7326553463935852
CurrentTrain: epoch  5, batch     1 | loss: 5.3484068Losses:  6.099578857421875 2.5170392990112305 0.7155323028564453
CurrentTrain: epoch  5, batch     2 | loss: 6.0995789Losses:  1.8964344263076782 -0.0 0.0874260738492012
CurrentTrain: epoch  5, batch     3 | loss: 1.8964344Losses:  7.576422214508057 3.985206365585327 0.6455587148666382
CurrentTrain: epoch  6, batch     0 | loss: 7.5764222Losses:  5.936185836791992 2.7670459747314453 0.6687967777252197
CurrentTrain: epoch  6, batch     1 | loss: 5.9361858Losses:  7.62558126449585 4.6149420738220215 0.6679826974868774
CurrentTrain: epoch  6, batch     2 | loss: 7.6255813Losses:  2.059532642364502 -0.0 0.11609286069869995
CurrentTrain: epoch  6, batch     3 | loss: 2.0595326Losses:  6.42225456237793 3.0463085174560547 0.6225913763046265
CurrentTrain: epoch  7, batch     0 | loss: 6.4222546Losses:  6.306886196136475 3.068443775177002 0.7301069498062134
CurrentTrain: epoch  7, batch     1 | loss: 6.3068862Losses:  6.262186050415039 3.4326815605163574 0.717432975769043
CurrentTrain: epoch  7, batch     2 | loss: 6.2621861Losses:  1.8790531158447266 -0.0 0.11094555258750916
CurrentTrain: epoch  7, batch     3 | loss: 1.8790531Losses:  5.229259014129639 2.247722864151001 0.7115979790687561
CurrentTrain: epoch  8, batch     0 | loss: 5.2292590Losses:  4.582059860229492 1.5777034759521484 0.7709988355636597
CurrentTrain: epoch  8, batch     1 | loss: 4.5820599Losses:  6.045414924621582 2.783855676651001 0.6904659271240234
CurrentTrain: epoch  8, batch     2 | loss: 6.0454149Losses:  2.09959077835083 -0.0 0.0934591218829155
CurrentTrain: epoch  8, batch     3 | loss: 2.0995908Losses:  5.3730268478393555 2.5187602043151855 0.7149485945701599
CurrentTrain: epoch  9, batch     0 | loss: 5.3730268Losses:  5.030794143676758 2.0172619819641113 0.7632207274436951
CurrentTrain: epoch  9, batch     1 | loss: 5.0307941Losses:  4.793435096740723 2.075312614440918 0.7049847841262817
CurrentTrain: epoch  9, batch     2 | loss: 4.7934351Losses:  4.107193470001221 -0.0 0.10625633597373962
CurrentTrain: epoch  9, batch     3 | loss: 4.1071935
#### contains administrative territorial entity ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "contains administrative territorial entity" means direct subdivisions of an administrative territorial entity.
Here is an example:
Relation: contains administrative territorial entity
Context: the cantons of créteil are administrative divisions of the val - de - marne department , île - de - france region , northern france .
Head Entity: île - de - france
Tail Entity: val - de - marne
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "contains administrative territorial entity":
Relation: contains administrative territorial entity  
Context: The provinces of Canada are the primary administrative divisions of the country, with Ontario being one of the most populous provinces.  
Head Entity: Canada  
Tail Entity: Ontario  

Relation: contains administrative territorial entity  
Context: The states of Australia are the major political divisions, with New South Wales being the most populous state in the country.  
Head Entity: Australia  
Tail Entity: New South Wales  
#### said to be the same as ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "said to be the same as" means this item is said to be the same as that item, but it's uncertain or disputed.
Here is an example:
Relation: said to be the same as
Context: this contrasts with the common usage of harem as an english loan - word , which implies a female - only enclave or seraglio .
Head Entity: harem
Tail Entity: seraglio
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "said to be the same as":
Relation: said to be the same as  
Context: Some researchers argue that the term "artificial intelligence" is said to be the same as "machine learning," although others believe they represent different concepts.  
Head Entity: artificial intelligence  
Tail Entity: machine learning  

Relation: said to be the same as  
Context: The two species, Canis lupus and Canis familiaris, are often said to be the same as each other, but there is ongoing debate about their distinct classifications.  
Head Entity: Canis lupus  
Tail Entity: Canis familiaris  
#### winner ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "winner" means winner of a competition or similar event.
Here is an example:
Relation: winner
Context: he considers his father to be biggest influence on his career as his brother javier castellano recipient of four eclipse award for outstanding jockey in the row ( 2013 , 2014,2015 and 2016 ) .
Head Entity: eclipse award for outstanding jockey
Tail Entity: javier castellano
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "winner":
Relation: winner  
Context: In the thrilling finale of the championship, Sarah Thompson emerged victorious, claiming the title of best player in the tournament, while her teammate, Mark Johnson, was awarded the runner-up position.  
Head Entity: best player in the tournament  
Tail Entity: Sarah Thompson  

Relation: winner  
Context: The annual science fair concluded with Emily Chen taking home the grand prize for her innovative project on renewable energy, while her classmate, David Lee, received an honorable mention for his work on robotics.  
Head Entity: grand prize  
Tail Entity: Emily Chen  
#### military rank ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "military rank" means military rank achieved by a person, or military rank associated with a position.
Here is an example:
Relation: military rank
Context: air commodore arthur kellam tylee obe ( 24 april 1887 – 13 april 1961 ) was canadian officer who served in the royal flying corps during world war i.
Head Entity: arthur kellam tylee
Tail Entity: air commodore
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "military rank":
Relation: military rank  
Context: general john smith was appointed as the chief of staff of the united states army in 2020, overseeing all operations and strategic planning.  
Head Entity: john smith  
Tail Entity: general  

Relation: military rank  
Context: colonel jane doe led her battalion with distinction during the peacekeeping mission in the middle east, earning her a commendation for bravery.  
Head Entity: jane doe  
Tail Entity: colonel  
#### publisher ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "publisher" means organization or person responsible for publishing books, periodicals, printed music, podcasts, games or software.
Here is an example:
Relation: publisher
Context: she went on to the film school at san francisco state university , when she was hired as an artist for an atari game called " electrocop " .
Head Entity: electrocop
Tail Entity: atari
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "publisher":
Relation: publisher  
Context: The renowned author released her latest novel through a well-known publishing house that has been in the industry for decades.  
Head Entity: latest novel  
Tail Entity: publishing house  

Relation: publisher  
Context: After years of hard work, the independent game developer finally secured a deal with a major publisher to distribute their new game worldwide.  
Head Entity: new game  
Tail Entity: major publisher  
#### followed by ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "followed by" means immediately following item in a series of which the subject is a part.
Here is an example:
Relation: followed by
Context: following this song were the # 21 " she wants to be wanted again " , the # 2 " loved too much " and the # 17 " i have to surrender " .
Head Entity: she wants to be wanted again
Tail Entity: loved too much
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "followed by":
Relation: followed by  
Context: The top three movies of the year were "Inception," followed by "The Social Network" and "Toy Story 3."  
Head Entity: Inception  
Tail Entity: The Social Network  

Relation: followed by  
Context: The conference schedule included a keynote speech, followed by a panel discussion and then a networking session.  
Head Entity: keynote speech  
Tail Entity: panel discussion  
#### work location ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "work location" means location where persons or organisations were actively participating in employment, business or other work.
Here is an example:
Relation: work location
Context: there is a memoir of his life and work by gustav queck ( jena , 1852 ) .
Head Entity: gustav queck
Tail Entity: jena
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "work location":
Relation: work location  
Context: The headquarters of the company is situated in San Francisco, where they have been operating since 2010.  
Head Entity: the company  
Tail Entity: San Francisco  

Relation: work location  
Context: During her time at the university, she conducted research in various labs located in Cambridge.  
Head Entity: she  
Tail Entity: Cambridge  
#### occupation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "occupation" means occupation of a person.
Here is an example:
Relation: occupation
Context: andrei droznin ( russian : андрей дрознин ) is a russian theatre director and movement coach .
Head Entity: andrei droznin
Tail Entity: theatre director
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "occupation":
Relation: occupation  
Context: dr. emily thompson is a renowned biochemist known for her groundbreaking research in cancer treatment.  
Head Entity: dr. emily thompson  
Tail Entity: biochemist  

Relation: occupation  
Context: john smith, a talented software engineer, has been working at tech innovations for over five years.  
Head Entity: john smith  
Tail Entity: software engineer  
#### heritage designation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "heritage designation" means heritage designation of a cultural or natural site.
Here is an example:
Relation: heritage designation
Context: altamaha town , bluffton historic district , church of the cross , and rose hill plantation house are listed on the national register of historic places .
Head Entity: altamaha town
Tail Entity: national register of historic places
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "heritage designation":
Relation: heritage designation  
Context: The Great Wall of China is recognized as a UNESCO World Heritage Site due to its historical significance and architectural grandeur.  
Head Entity: Great Wall of China  
Tail Entity: UNESCO World Heritage Site  

Relation: heritage designation  
Context: The ancient city of Petra in Jordan is designated as a World Heritage Site, attracting tourists from around the globe.  
Head Entity: Petra  
Tail Entity: World Heritage Site  
#### voice type ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "voice type" means person's voice type.
Here is an example:
Relation: voice type
Context: ada adini ( 1855 – february 1924 ) was an american operatic soprano who had an active international career from 1876 up into the first decade of the 20th century .
Head Entity: ada adini
Tail Entity: soprano
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "voice type":
Relation: voice type  
Context: luciano pavarotti ( october 12, 1935 – september 6, 2007 ) was an italian operatic tenor who gained worldwide fame for his televised concerts and media appearances.  
Head Entity: luciano pavarotti  
Tail Entity: tenor  

Relation: voice type  
Context: mariah carey ( born march 27, 1969 ) is an american singer, songwriter, and actress known for her five-octave vocal range and her use of the whistle register.  
Head Entity: mariah carey  
Tail Entity: singer  
Losses:  3.812154769897461 0.25065240263938904 0.9269753694534302
MemoryTrain:  epoch  0, batch     0 | loss: 3.8121548Losses:  3.979120969772339 0.2732360064983368 0.8970409035682678
MemoryTrain:  epoch  0, batch     1 | loss: 3.9791210Losses:  3.773698091506958 0.7977620363235474 0.8280860781669617
MemoryTrain:  epoch  0, batch     2 | loss: 3.7736981Losses:  3.9242453575134277 0.5027632713317871 0.9686309099197388
MemoryTrain:  epoch  0, batch     3 | loss: 3.9242454Losses:  3.931100845336914 0.2702876925468445 0.908332347869873
MemoryTrain:  epoch  0, batch     4 | loss: 3.9311008Losses:  3.8157382011413574 0.2495027780532837 0.9723782539367676
MemoryTrain:  epoch  0, batch     5 | loss: 3.8157382Losses:  4.406085968017578 0.5363947153091431 0.9682514667510986
MemoryTrain:  epoch  0, batch     6 | loss: 4.4060860Losses:  3.9195594787597656 0.25467437505722046 0.9784751534461975
MemoryTrain:  epoch  0, batch     7 | loss: 3.9195595Losses:  3.456711769104004 -0.0 0.9139041900634766
MemoryTrain:  epoch  0, batch     8 | loss: 3.4567118Losses:  3.4315433502197266 -0.0 1.0672003030776978
MemoryTrain:  epoch  0, batch     9 | loss: 3.4315434Losses:  3.4448838233947754 0.25309938192367554 1.0361201763153076
MemoryTrain:  epoch  0, batch    10 | loss: 3.4448838Losses:  4.061071872711182 -0.0 0.9859083890914917
MemoryTrain:  epoch  0, batch    11 | loss: 4.0610719Losses:  2.9629762172698975 -0.0 0.9148761630058289
MemoryTrain:  epoch  0, batch    12 | loss: 2.9629762Losses:  3.8056693077087402 -0.0 1.0257680416107178
MemoryTrain:  epoch  0, batch    13 | loss: 3.8056693Losses:  3.8526344299316406 0.502637505531311 1.027077317237854
MemoryTrain:  epoch  0, batch    14 | loss: 3.8526344Losses:  3.2394213676452637 -0.0 0.959821879863739
MemoryTrain:  epoch  1, batch     0 | loss: 3.2394214Losses:  3.1891322135925293 -0.0 1.0342153310775757
MemoryTrain:  epoch  1, batch     1 | loss: 3.1891322Losses:  2.7251486778259277 -0.0 0.9616935849189758
MemoryTrain:  epoch  1, batch     2 | loss: 2.7251487Losses:  3.578691244125366 0.25842857360839844 0.8606135249137878
MemoryTrain:  epoch  1, batch     3 | loss: 3.5786912Losses:  3.4964945316314697 0.507905125617981 0.9895622134208679
MemoryTrain:  epoch  1, batch     4 | loss: 3.4964945Losses:  3.414118766784668 -0.0 1.1497594118118286
MemoryTrain:  epoch  1, batch     5 | loss: 3.4141188Losses:  3.5486621856689453 0.25057896971702576 0.9538937211036682
MemoryTrain:  epoch  1, batch     6 | loss: 3.5486622Losses:  4.186614513397217 0.508851945400238 0.8678753972053528
MemoryTrain:  epoch  1, batch     7 | loss: 4.1866145Losses:  3.5121169090270996 -0.0 1.0824440717697144
MemoryTrain:  epoch  1, batch     8 | loss: 3.5121169Losses:  4.019680023193359 0.2366637885570526 0.901473879814148
MemoryTrain:  epoch  1, batch     9 | loss: 4.0196800Losses:  3.4772863388061523 0.24527016282081604 1.0750700235366821
MemoryTrain:  epoch  1, batch    10 | loss: 3.4772863Losses:  3.2583656311035156 0.2520594596862793 1.0354571342468262
MemoryTrain:  epoch  1, batch    11 | loss: 3.2583656Losses:  2.9560627937316895 0.25154179334640503 1.0036234855651855
MemoryTrain:  epoch  1, batch    12 | loss: 2.9560628Losses:  2.583087921142578 -0.0 0.9755397439002991
MemoryTrain:  epoch  1, batch    13 | loss: 2.5830879Losses:  2.8596701622009277 0.5125223994255066 0.7369621396064758
MemoryTrain:  epoch  1, batch    14 | loss: 2.8596702Losses:  2.9674830436706543 0.2690480947494507 1.0035510063171387
MemoryTrain:  epoch  2, batch     0 | loss: 2.9674830Losses:  2.829026699066162 0.25926828384399414 1.054864764213562
MemoryTrain:  epoch  2, batch     1 | loss: 2.8290267Losses:  2.973604440689087 0.804572343826294 0.6584856510162354
MemoryTrain:  epoch  2, batch     2 | loss: 2.9736044Losses:  3.0596563816070557 0.5002102851867676 0.9639337062835693
MemoryTrain:  epoch  2, batch     3 | loss: 3.0596564Losses:  2.7064619064331055 -0.0 0.8538088798522949
MemoryTrain:  epoch  2, batch     4 | loss: 2.7064619Losses:  3.010394334793091 0.2222422957420349 1.00351881980896
MemoryTrain:  epoch  2, batch     5 | loss: 3.0103943Losses:  3.6037065982818604 0.5527155995368958 0.9662435054779053
MemoryTrain:  epoch  2, batch     6 | loss: 3.6037066Losses:  3.086571216583252 0.4541652798652649 1.0126675367355347
MemoryTrain:  epoch  2, batch     7 | loss: 3.0865712Losses:  2.784654140472412 0.2536155581474304 0.9081990718841553
MemoryTrain:  epoch  2, batch     8 | loss: 2.7846541Losses:  3.105356216430664 0.258897066116333 0.9285799264907837
MemoryTrain:  epoch  2, batch     9 | loss: 3.1053562Losses:  3.1911988258361816 0.5053747892379761 0.8527669310569763
MemoryTrain:  epoch  2, batch    10 | loss: 3.1911988Losses:  2.7354722023010254 -0.0 1.0191878080368042
MemoryTrain:  epoch  2, batch    11 | loss: 2.7354722Losses:  3.156587600708008 0.2403516322374344 1.0204553604125977
MemoryTrain:  epoch  2, batch    12 | loss: 3.1565876Losses:  2.7674691677093506 -0.0 1.0387977361679077
MemoryTrain:  epoch  2, batch    13 | loss: 2.7674692Losses:  3.9654541015625 0.6108994483947754 0.8771480321884155
MemoryTrain:  epoch  2, batch    14 | loss: 3.9654541Losses:  2.850886106491089 -0.0 1.0311923027038574
MemoryTrain:  epoch  3, batch     0 | loss: 2.8508861Losses:  2.7751882076263428 -0.0 0.9659268856048584
MemoryTrain:  epoch  3, batch     1 | loss: 2.7751882Losses:  3.4467098712921143 0.49589070677757263 0.9678854942321777
MemoryTrain:  epoch  3, batch     2 | loss: 3.4467099Losses:  3.703617811203003 0.5243676900863647 0.9095622897148132
MemoryTrain:  epoch  3, batch     3 | loss: 3.7036178Losses:  2.477283477783203 -0.0 0.9988478422164917
MemoryTrain:  epoch  3, batch     4 | loss: 2.4772835Losses:  3.226531982421875 0.7495979070663452 0.9311599731445312
MemoryTrain:  epoch  3, batch     5 | loss: 3.2265320Losses:  2.6254494190216064 0.25863027572631836 0.7847159504890442
MemoryTrain:  epoch  3, batch     6 | loss: 2.6254494Losses:  2.715818166732788 0.6862255334854126 0.7365589737892151
MemoryTrain:  epoch  3, batch     7 | loss: 2.7158182Losses:  3.463253974914551 0.8018184900283813 0.9729437232017517
MemoryTrain:  epoch  3, batch     8 | loss: 3.4632540Losses:  2.9607186317443848 0.2455577850341797 0.9211171865463257
MemoryTrain:  epoch  3, batch     9 | loss: 2.9607186Losses:  2.715245246887207 -0.0 0.9013335704803467
MemoryTrain:  epoch  3, batch    10 | loss: 2.7152452Losses:  3.112041473388672 0.9959466457366943 0.8070186972618103
MemoryTrain:  epoch  3, batch    11 | loss: 3.1120415Losses:  2.604886054992676 -0.0 0.9981042146682739
MemoryTrain:  epoch  3, batch    12 | loss: 2.6048861Losses:  2.991591453552246 0.2661556005477905 0.9156668186187744
MemoryTrain:  epoch  3, batch    13 | loss: 2.9915915Losses:  2.741201877593994 -0.0 1.0179680585861206
MemoryTrain:  epoch  3, batch    14 | loss: 2.7412019Losses:  2.491983652114868 0.2498835325241089 0.861596941947937
MemoryTrain:  epoch  4, batch     0 | loss: 2.4919837Losses:  3.5620243549346924 0.5033732652664185 0.9688137173652649
MemoryTrain:  epoch  4, batch     1 | loss: 3.5620244Losses:  3.3686752319335938 1.1476712226867676 0.8712026476860046
MemoryTrain:  epoch  4, batch     2 | loss: 3.3686752Losses:  2.8362903594970703 0.2955965995788574 0.9246751666069031
MemoryTrain:  epoch  4, batch     3 | loss: 2.8362904Losses:  2.8934078216552734 0.5211452841758728 0.9933000802993774
MemoryTrain:  epoch  4, batch     4 | loss: 2.8934078Losses:  2.518817186355591 -0.0 0.9090962409973145
MemoryTrain:  epoch  4, batch     5 | loss: 2.5188172Losses:  3.5785019397735596 0.27175724506378174 0.9801933765411377
MemoryTrain:  epoch  4, batch     6 | loss: 3.5785019Losses:  3.9306490421295166 0.6614726185798645 0.9815039038658142
MemoryTrain:  epoch  4, batch     7 | loss: 3.9306490Losses:  2.7127912044525146 0.23832491040229797 0.9133290648460388
MemoryTrain:  epoch  4, batch     8 | loss: 2.7127912Losses:  2.6021294593811035 -0.0 1.1205289363861084
MemoryTrain:  epoch  4, batch     9 | loss: 2.6021295Losses:  2.4360742568969727 -0.0 1.0129222869873047
MemoryTrain:  epoch  4, batch    10 | loss: 2.4360743Losses:  2.520995855331421 0.48451483249664307 0.7357829809188843
MemoryTrain:  epoch  4, batch    11 | loss: 2.5209959Losses:  2.494502544403076 -0.0 1.102378487586975
MemoryTrain:  epoch  4, batch    12 | loss: 2.4945025Losses:  2.4771745204925537 -0.0 0.9645114541053772
MemoryTrain:  epoch  4, batch    13 | loss: 2.4771745Losses:  2.8219571113586426 0.7352868914604187 0.7820833921432495
MemoryTrain:  epoch  4, batch    14 | loss: 2.8219571Losses:  2.426888942718506 -0.0 0.9685871601104736
MemoryTrain:  epoch  5, batch     0 | loss: 2.4268889Losses:  2.645153045654297 0.24482940137386322 1.0130088329315186
MemoryTrain:  epoch  5, batch     1 | loss: 2.6451530Losses:  2.6056437492370605 -0.0 1.034192681312561
MemoryTrain:  epoch  5, batch     2 | loss: 2.6056437Losses:  3.522031307220459 1.1533801555633545 0.8456836342811584
MemoryTrain:  epoch  5, batch     3 | loss: 3.5220313Losses:  2.659240484237671 -0.0 1.067091941833496
MemoryTrain:  epoch  5, batch     4 | loss: 2.6592405Losses:  3.0344760417938232 -0.0 1.1115241050720215
MemoryTrain:  epoch  5, batch     5 | loss: 3.0344760Losses:  3.176811695098877 0.5136688351631165 1.0111030340194702
MemoryTrain:  epoch  5, batch     6 | loss: 3.1768117Losses:  2.6007447242736816 0.24829161167144775 1.0419378280639648
MemoryTrain:  epoch  5, batch     7 | loss: 2.6007447Losses:  3.0158989429473877 0.27538108825683594 0.7851220965385437
MemoryTrain:  epoch  5, batch     8 | loss: 3.0158989Losses:  2.549896717071533 0.2441122680902481 1.0093210935592651
MemoryTrain:  epoch  5, batch     9 | loss: 2.5498967Losses:  3.310279607772827 0.5292337536811829 0.9994412064552307
MemoryTrain:  epoch  5, batch    10 | loss: 3.3102796Losses:  2.4957854747772217 -0.0 1.0227954387664795
MemoryTrain:  epoch  5, batch    11 | loss: 2.4957855Losses:  2.465771198272705 -0.0 0.9812606573104858
MemoryTrain:  epoch  5, batch    12 | loss: 2.4657712Losses:  2.202887773513794 -0.0 0.9466836452484131
MemoryTrain:  epoch  5, batch    13 | loss: 2.2028878Losses:  2.767226457595825 0.513807475566864 0.9175312519073486
MemoryTrain:  epoch  5, batch    14 | loss: 2.7672265Losses:  3.0138163566589355 0.5409812331199646 0.968934178352356
MemoryTrain:  epoch  6, batch     0 | loss: 3.0138164Losses:  2.59145188331604 -0.0 0.9587342739105225
MemoryTrain:  epoch  6, batch     1 | loss: 2.5914519Losses:  2.4797775745391846 -0.0 0.9499348998069763
MemoryTrain:  epoch  6, batch     2 | loss: 2.4797776Losses:  2.631011486053467 0.24926474690437317 1.0138899087905884
MemoryTrain:  epoch  6, batch     3 | loss: 2.6310115Losses:  2.331212043762207 -0.0 0.9779083132743835
MemoryTrain:  epoch  6, batch     4 | loss: 2.3312120Losses:  2.5786056518554688 0.2836478054523468 1.0100940465927124
MemoryTrain:  epoch  6, batch     5 | loss: 2.5786057Losses:  2.550445079803467 0.23197594285011292 1.0522788763046265
MemoryTrain:  epoch  6, batch     6 | loss: 2.5504451Losses:  2.7995619773864746 0.5095266103744507 0.984506368637085
MemoryTrain:  epoch  6, batch     7 | loss: 2.7995620Losses:  2.785461902618408 0.260296106338501 1.0615019798278809
MemoryTrain:  epoch  6, batch     8 | loss: 2.7854619Losses:  2.4667773246765137 0.26778554916381836 0.8590565919876099
MemoryTrain:  epoch  6, batch     9 | loss: 2.4667773Losses:  2.856156349182129 0.252734899520874 0.8910118937492371
MemoryTrain:  epoch  6, batch    10 | loss: 2.8561563Losses:  2.4611730575561523 -0.0 1.0103869438171387
MemoryTrain:  epoch  6, batch    11 | loss: 2.4611731Losses:  2.4067816734313965 -0.0 0.9390267729759216
MemoryTrain:  epoch  6, batch    12 | loss: 2.4067817Losses:  2.534414768218994 0.2541988492012024 0.9619261026382446
MemoryTrain:  epoch  6, batch    13 | loss: 2.5344148Losses:  2.903715133666992 0.5117559432983398 0.9615474939346313
MemoryTrain:  epoch  6, batch    14 | loss: 2.9037151Losses:  2.229660987854004 -0.0 1.0048948526382446
MemoryTrain:  epoch  7, batch     0 | loss: 2.2296610Losses:  2.9596171379089355 0.7139897346496582 0.9427329301834106
MemoryTrain:  epoch  7, batch     1 | loss: 2.9596171Losses:  2.2816977500915527 -0.0 0.9722796678543091
MemoryTrain:  epoch  7, batch     2 | loss: 2.2816978Losses:  2.5307040214538574 -0.0 1.0652540922164917
MemoryTrain:  epoch  7, batch     3 | loss: 2.5307040Losses:  2.70746111869812 0.25511592626571655 0.9551710486412048
MemoryTrain:  epoch  7, batch     4 | loss: 2.7074611Losses:  2.73166823387146 0.2949836552143097 0.8830851316452026
MemoryTrain:  epoch  7, batch     5 | loss: 2.7316682Losses:  2.3918890953063965 -0.0 1.0149765014648438
MemoryTrain:  epoch  7, batch     6 | loss: 2.3918891Losses:  3.032527208328247 0.5344240665435791 0.8765113353729248
MemoryTrain:  epoch  7, batch     7 | loss: 3.0325272Losses:  2.499352216720581 0.2507603168487549 0.9585668444633484
MemoryTrain:  epoch  7, batch     8 | loss: 2.4993522Losses:  2.465247392654419 0.2538365125656128 0.8737513422966003
MemoryTrain:  epoch  7, batch     9 | loss: 2.4652474Losses:  2.42490816116333 -0.0 1.0564937591552734
MemoryTrain:  epoch  7, batch    10 | loss: 2.4249082Losses:  2.410187005996704 0.25644996762275696 0.907338559627533
MemoryTrain:  epoch  7, batch    11 | loss: 2.4101870Losses:  2.4662699699401855 -0.0 1.0794041156768799
MemoryTrain:  epoch  7, batch    12 | loss: 2.4662700Losses:  2.421328544616699 0.2530633807182312 0.9741986989974976
MemoryTrain:  epoch  7, batch    13 | loss: 2.4213285Losses:  2.410651683807373 -0.0 1.008053183555603
MemoryTrain:  epoch  7, batch    14 | loss: 2.4106517Losses:  2.4471404552459717 0.21984006464481354 0.9613547921180725
MemoryTrain:  epoch  8, batch     0 | loss: 2.4471405Losses:  2.5828564167022705 0.2745829224586487 0.9280939102172852
MemoryTrain:  epoch  8, batch     1 | loss: 2.5828564Losses:  2.486039400100708 0.25640547275543213 1.0064482688903809
MemoryTrain:  epoch  8, batch     2 | loss: 2.4860394Losses:  2.5808770656585693 0.48861873149871826 0.8457152247428894
MemoryTrain:  epoch  8, batch     3 | loss: 2.5808771Losses:  2.68477725982666 0.49500834941864014 0.9640060067176819
MemoryTrain:  epoch  8, batch     4 | loss: 2.6847773Losses:  2.324803590774536 -0.0 0.8502278327941895
MemoryTrain:  epoch  8, batch     5 | loss: 2.3248036Losses:  2.516915798187256 0.24989533424377441 0.9292234778404236
MemoryTrain:  epoch  8, batch     6 | loss: 2.5169158Losses:  2.5627498626708984 0.25229522585868835 0.9222962260246277
MemoryTrain:  epoch  8, batch     7 | loss: 2.5627499Losses:  2.4068822860717773 -0.0 1.0099581480026245
MemoryTrain:  epoch  8, batch     8 | loss: 2.4068823Losses:  2.5809168815612793 0.22133104503154755 1.0161141157150269
MemoryTrain:  epoch  8, batch     9 | loss: 2.5809169Losses:  2.415105104446411 -0.0 0.9730887413024902
MemoryTrain:  epoch  8, batch    10 | loss: 2.4151051Losses:  2.7181949615478516 0.48783355951309204 0.7804917693138123
MemoryTrain:  epoch  8, batch    11 | loss: 2.7181950Losses:  2.592203140258789 0.2757161855697632 0.9208347201347351
MemoryTrain:  epoch  8, batch    12 | loss: 2.5922031Losses:  2.4404149055480957 0.2527506351470947 0.8867132067680359
MemoryTrain:  epoch  8, batch    13 | loss: 2.4404149Losses:  2.515559673309326 0.2429998219013214 1.0244039297103882
MemoryTrain:  epoch  8, batch    14 | loss: 2.5155597Losses:  2.1368041038513184 -0.0 0.8391371965408325
MemoryTrain:  epoch  9, batch     0 | loss: 2.1368041Losses:  2.6848342418670654 0.48785585165023804 0.9720602631568909
MemoryTrain:  epoch  9, batch     1 | loss: 2.6848342Losses:  2.6543846130371094 0.4895738959312439 0.883224368095398
MemoryTrain:  epoch  9, batch     2 | loss: 2.6543846Losses:  2.3084535598754883 -0.0 1.1015559434890747
MemoryTrain:  epoch  9, batch     3 | loss: 2.3084536Losses:  2.3402321338653564 -0.0 0.9240016937255859
MemoryTrain:  epoch  9, batch     4 | loss: 2.3402321Losses:  2.9779224395751953 0.5408253073692322 0.9700826406478882
MemoryTrain:  epoch  9, batch     5 | loss: 2.9779224Losses:  2.3223321437835693 0.24244534969329834 0.8994115591049194
MemoryTrain:  epoch  9, batch     6 | loss: 2.3223321Losses:  2.513183116912842 0.2786896526813507 0.8421149253845215
MemoryTrain:  epoch  9, batch     7 | loss: 2.5131831Losses:  2.573559284210205 0.2667360007762909 1.0383671522140503
MemoryTrain:  epoch  9, batch     8 | loss: 2.5735593Losses:  2.4094414710998535 0.23146182298660278 0.9107969999313354
MemoryTrain:  epoch  9, batch     9 | loss: 2.4094415Losses:  2.403996467590332 0.22293487191200256 0.9678465127944946
MemoryTrain:  epoch  9, batch    10 | loss: 2.4039965Losses:  2.416987180709839 -0.0 1.0664304494857788
MemoryTrain:  epoch  9, batch    11 | loss: 2.4169872Losses:  3.0188331604003906 0.8795796632766724 0.8624237775802612
MemoryTrain:  epoch  9, batch    12 | loss: 3.0188332Losses:  2.738316297531128 0.5193496346473694 0.98726886510849
MemoryTrain:  epoch  9, batch    13 | loss: 2.7383163Losses:  2.577425241470337 0.25295114517211914 0.9844571352005005
MemoryTrain:  epoch  9, batch    14 | loss: 2.5774252
[EVAL] batch:    0 | acc: 31.25%,  total acc: 31.25%   [EVAL] batch:    1 | acc: 62.50%,  total acc: 46.88%   [EVAL] batch:    2 | acc: 56.25%,  total acc: 50.00%   [EVAL] batch:    3 | acc: 43.75%,  total acc: 48.44%   [EVAL] batch:    4 | acc: 62.50%,  total acc: 51.25%   [EVAL] batch:    5 | acc: 68.75%,  total acc: 54.17%   [EVAL] batch:    6 | acc: 56.25%,  total acc: 54.46%   [EVAL] batch:    7 | acc: 68.75%,  total acc: 56.25%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 57.64%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 58.75%   [EVAL] batch:   10 | acc: 43.75%,  total acc: 57.39%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 58.85%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 59.62%   [EVAL] batch:   13 | acc: 56.25%,  total acc: 59.38%   [EVAL] batch:   14 | acc: 62.50%,  total acc: 59.58%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 59.38%   [EVAL] batch:   16 | acc: 62.50%,  total acc: 59.56%   [EVAL] batch:   17 | acc: 31.25%,  total acc: 57.99%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 59.87%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 61.25%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 63.10%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 64.77%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 66.30%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 67.45%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 68.75%   [EVAL] batch:   25 | acc: 75.00%,  total acc: 68.99%   [EVAL] batch:   26 | acc: 62.50%,  total acc: 68.75%   [EVAL] batch:   27 | acc: 62.50%,  total acc: 68.53%   [EVAL] batch:   28 | acc: 56.25%,  total acc: 68.10%   [EVAL] batch:   29 | acc: 68.75%,  total acc: 68.12%   [EVAL] batch:   30 | acc: 68.75%,  total acc: 68.15%   [EVAL] batch:   31 | acc: 37.50%,  total acc: 67.19%   [EVAL] batch:   32 | acc: 6.25%,  total acc: 65.34%   [EVAL] batch:   33 | acc: 37.50%,  total acc: 64.52%   [EVAL] batch:   34 | acc: 31.25%,  total acc: 63.57%   [EVAL] batch:   35 | acc: 37.50%,  total acc: 62.85%   [EVAL] batch:   36 | acc: 56.25%,  total acc: 62.67%   [EVAL] batch:   37 | acc: 43.75%,  total acc: 62.17%   [EVAL] batch:   38 | acc: 81.25%,  total acc: 62.66%   [EVAL] batch:   39 | acc: 87.50%,  total acc: 63.28%   [EVAL] batch:   40 | acc: 87.50%,  total acc: 63.87%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 64.58%   [EVAL] batch:   42 | acc: 81.25%,  total acc: 64.97%   [EVAL] batch:   43 | acc: 75.00%,  total acc: 65.20%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 65.69%   [EVAL] batch:   45 | acc: 75.00%,  total acc: 65.90%   [EVAL] batch:   46 | acc: 75.00%,  total acc: 66.09%   [EVAL] batch:   47 | acc: 56.25%,  total acc: 65.89%   [EVAL] batch:   48 | acc: 75.00%,  total acc: 66.07%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 66.50%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 67.16%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 67.79%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 68.40%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 68.98%   [EVAL] batch:   54 | acc: 100.00%,  total acc: 69.55%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 70.09%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 70.61%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 70.91%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 71.29%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 71.67%   [EVAL] batch:   60 | acc: 93.75%,  total acc: 72.03%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 72.48%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 72.12%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 79.17%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 78.12%   [EVAL] batch:    4 | acc: 62.50%,  total acc: 75.00%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 78.12%   [EVAL] batch:    6 | acc: 50.00%,  total acc: 74.11%   [EVAL] batch:    7 | acc: 43.75%,  total acc: 70.31%   [EVAL] batch:    8 | acc: 62.50%,  total acc: 69.44%   [EVAL] batch:    9 | acc: 56.25%,  total acc: 68.12%   [EVAL] batch:   10 | acc: 37.50%,  total acc: 65.34%   [EVAL] batch:   11 | acc: 50.00%,  total acc: 64.06%   [EVAL] batch:   12 | acc: 62.50%,  total acc: 63.94%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 66.07%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 67.92%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 69.92%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 71.32%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 72.57%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 74.01%   [EVAL] batch:   19 | acc: 68.75%,  total acc: 73.75%   [EVAL] batch:   20 | acc: 81.25%,  total acc: 74.11%   [EVAL] batch:   21 | acc: 75.00%,  total acc: 74.15%   [EVAL] batch:   22 | acc: 62.50%,  total acc: 73.64%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 74.22%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 74.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 75.72%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 76.39%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 77.23%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 77.80%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 78.54%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 79.23%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 79.88%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 80.30%   [EVAL] batch:   33 | acc: 62.50%,  total acc: 79.78%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 79.64%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 79.86%   [EVAL] batch:   36 | acc: 87.50%,  total acc: 80.07%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 80.43%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 80.93%   [EVAL] batch:   39 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 81.55%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 81.70%   [EVAL] batch:   42 | acc: 87.50%,  total acc: 81.83%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 81.96%   [EVAL] batch:   44 | acc: 81.25%,  total acc: 81.94%   [EVAL] batch:   45 | acc: 75.00%,  total acc: 81.79%   [EVAL] batch:   46 | acc: 62.50%,  total acc: 81.38%   [EVAL] batch:   47 | acc: 62.50%,  total acc: 80.99%   [EVAL] batch:   48 | acc: 81.25%,  total acc: 80.99%   [EVAL] batch:   49 | acc: 75.00%,  total acc: 80.88%   [EVAL] batch:   50 | acc: 75.00%,  total acc: 80.76%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 80.89%   [EVAL] batch:   52 | acc: 81.25%,  total acc: 80.90%   [EVAL] batch:   53 | acc: 56.25%,  total acc: 80.44%   [EVAL] batch:   54 | acc: 56.25%,  total acc: 80.00%   [EVAL] batch:   55 | acc: 68.75%,  total acc: 79.80%   [EVAL] batch:   56 | acc: 62.50%,  total acc: 79.50%   [EVAL] batch:   57 | acc: 50.00%,  total acc: 78.99%   [EVAL] batch:   58 | acc: 62.50%,  total acc: 78.71%   [EVAL] batch:   59 | acc: 81.25%,  total acc: 78.75%   [EVAL] batch:   60 | acc: 75.00%,  total acc: 78.69%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 78.53%   [EVAL] batch:   62 | acc: 68.75%,  total acc: 78.37%   [EVAL] batch:   63 | acc: 75.00%,  total acc: 78.32%   [EVAL] batch:   64 | acc: 68.75%,  total acc: 78.17%   [EVAL] batch:   65 | acc: 75.00%,  total acc: 78.12%   [EVAL] batch:   66 | acc: 62.50%,  total acc: 77.89%   [EVAL] batch:   67 | acc: 56.25%,  total acc: 77.57%   [EVAL] batch:   68 | acc: 75.00%,  total acc: 77.54%   [EVAL] batch:   69 | acc: 75.00%,  total acc: 77.50%   [EVAL] batch:   70 | acc: 62.50%,  total acc: 77.29%   [EVAL] batch:   71 | acc: 62.50%,  total acc: 77.08%   [EVAL] batch:   72 | acc: 75.00%,  total acc: 77.05%   [EVAL] batch:   73 | acc: 75.00%,  total acc: 77.03%   [EVAL] batch:   74 | acc: 75.00%,  total acc: 77.00%   [EVAL] batch:   75 | acc: 56.25%,  total acc: 76.73%   [EVAL] batch:   76 | acc: 62.50%,  total acc: 76.54%   [EVAL] batch:   77 | acc: 62.50%,  total acc: 76.36%   [EVAL] batch:   78 | acc: 62.50%,  total acc: 76.19%   [EVAL] batch:   79 | acc: 68.75%,  total acc: 76.09%   [EVAL] batch:   80 | acc: 75.00%,  total acc: 76.08%   [EVAL] batch:   81 | acc: 25.00%,  total acc: 75.46%   [EVAL] batch:   82 | acc: 6.25%,  total acc: 74.62%   [EVAL] batch:   83 | acc: 0.00%,  total acc: 73.74%   [EVAL] batch:   84 | acc: 0.00%,  total acc: 72.87%   [EVAL] batch:   85 | acc: 12.50%,  total acc: 72.17%   [EVAL] batch:   86 | acc: 12.50%,  total acc: 71.48%   [EVAL] batch:   87 | acc: 37.50%,  total acc: 71.09%   [EVAL] batch:   88 | acc: 93.75%,  total acc: 71.35%   [EVAL] batch:   89 | acc: 93.75%,  total acc: 71.60%   [EVAL] batch:   90 | acc: 75.00%,  total acc: 71.63%   [EVAL] batch:   91 | acc: 93.75%,  total acc: 71.88%   [EVAL] batch:   92 | acc: 87.50%,  total acc: 72.04%   [EVAL] batch:   93 | acc: 93.75%,  total acc: 72.27%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 72.57%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 72.85%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 73.13%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 73.41%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 73.67%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 73.94%   [EVAL] batch:  100 | acc: 75.00%,  total acc: 73.95%   [EVAL] batch:  101 | acc: 87.50%,  total acc: 74.08%   [EVAL] batch:  102 | acc: 68.75%,  total acc: 74.03%   [EVAL] batch:  103 | acc: 81.25%,  total acc: 74.10%   [EVAL] batch:  104 | acc: 93.75%,  total acc: 74.29%   [EVAL] batch:  105 | acc: 75.00%,  total acc: 74.29%   [EVAL] batch:  106 | acc: 31.25%,  total acc: 73.89%   [EVAL] batch:  107 | acc: 12.50%,  total acc: 73.32%   [EVAL] batch:  108 | acc: 31.25%,  total acc: 72.94%   [EVAL] batch:  109 | acc: 6.25%,  total acc: 72.33%   [EVAL] batch:  110 | acc: 37.50%,  total acc: 72.02%   [EVAL] batch:  111 | acc: 6.25%,  total acc: 71.43%   [EVAL] batch:  112 | acc: 50.00%,  total acc: 71.24%   [EVAL] batch:  113 | acc: 87.50%,  total acc: 71.38%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 71.63%   [EVAL] batch:  115 | acc: 81.25%,  total acc: 71.71%   [EVAL] batch:  116 | acc: 87.50%,  total acc: 71.85%   [EVAL] batch:  117 | acc: 87.50%,  total acc: 71.98%   [EVAL] batch:  118 | acc: 75.00%,  total acc: 72.01%   [EVAL] batch:  119 | acc: 0.00%,  total acc: 71.41%   [EVAL] batch:  120 | acc: 12.50%,  total acc: 70.92%   [EVAL] batch:  121 | acc: 6.25%,  total acc: 70.39%   [EVAL] batch:  122 | acc: 12.50%,  total acc: 69.92%   [EVAL] batch:  123 | acc: 0.00%,  total acc: 69.35%   [EVAL] batch:  124 | acc: 6.25%,  total acc: 68.85%   [EVAL] batch:  125 | acc: 68.75%,  total acc: 68.85%   [EVAL] batch:  126 | acc: 43.75%,  total acc: 68.65%   [EVAL] batch:  127 | acc: 43.75%,  total acc: 68.46%   [EVAL] batch:  128 | acc: 31.25%,  total acc: 68.17%   [EVAL] batch:  129 | acc: 62.50%,  total acc: 68.12%   [EVAL] batch:  130 | acc: 50.00%,  total acc: 67.99%   [EVAL] batch:  131 | acc: 81.25%,  total acc: 68.09%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 68.33%   [EVAL] batch:  133 | acc: 87.50%,  total acc: 68.47%   [EVAL] batch:  134 | acc: 75.00%,  total acc: 68.52%   [EVAL] batch:  135 | acc: 81.25%,  total acc: 68.61%   [EVAL] batch:  136 | acc: 100.00%,  total acc: 68.84%   [EVAL] batch:  137 | acc: 56.25%,  total acc: 68.75%   [EVAL] batch:  138 | acc: 31.25%,  total acc: 68.48%   [EVAL] batch:  139 | acc: 18.75%,  total acc: 68.12%   [EVAL] batch:  140 | acc: 43.75%,  total acc: 67.95%   [EVAL] batch:  141 | acc: 62.50%,  total acc: 67.91%   [EVAL] batch:  142 | acc: 37.50%,  total acc: 67.70%   [EVAL] batch:  143 | acc: 56.25%,  total acc: 67.62%   [EVAL] batch:  144 | acc: 68.75%,  total acc: 67.63%   [EVAL] batch:  145 | acc: 81.25%,  total acc: 67.72%   [EVAL] batch:  146 | acc: 43.75%,  total acc: 67.56%   [EVAL] batch:  147 | acc: 75.00%,  total acc: 67.61%   [EVAL] batch:  148 | acc: 68.75%,  total acc: 67.62%   [EVAL] batch:  149 | acc: 56.25%,  total acc: 67.54%   [EVAL] batch:  150 | acc: 75.00%,  total acc: 67.59%   [EVAL] batch:  151 | acc: 87.50%,  total acc: 67.72%   [EVAL] batch:  152 | acc: 75.00%,  total acc: 67.77%   [EVAL] batch:  153 | acc: 75.00%,  total acc: 67.82%   [EVAL] batch:  154 | acc: 56.25%,  total acc: 67.74%   [EVAL] batch:  155 | acc: 68.75%,  total acc: 67.75%   [EVAL] batch:  156 | acc: 68.75%,  total acc: 67.75%   [EVAL] batch:  157 | acc: 56.25%,  total acc: 67.68%   [EVAL] batch:  158 | acc: 50.00%,  total acc: 67.57%   [EVAL] batch:  159 | acc: 50.00%,  total acc: 67.46%   [EVAL] batch:  160 | acc: 56.25%,  total acc: 67.39%   [EVAL] batch:  161 | acc: 56.25%,  total acc: 67.32%   [EVAL] batch:  162 | acc: 56.25%,  total acc: 67.25%   [EVAL] batch:  163 | acc: 56.25%,  total acc: 67.19%   [EVAL] batch:  164 | acc: 75.00%,  total acc: 67.23%   [EVAL] batch:  165 | acc: 62.50%,  total acc: 67.21%   [EVAL] batch:  166 | acc: 68.75%,  total acc: 67.22%   [EVAL] batch:  167 | acc: 62.50%,  total acc: 67.19%   [EVAL] batch:  168 | acc: 56.25%,  total acc: 67.12%   [EVAL] batch:  169 | acc: 25.00%,  total acc: 66.88%   [EVAL] batch:  170 | acc: 37.50%,  total acc: 66.70%   [EVAL] batch:  171 | acc: 37.50%,  total acc: 66.53%   [EVAL] batch:  172 | acc: 43.75%,  total acc: 66.40%   [EVAL] batch:  173 | acc: 31.25%,  total acc: 66.20%   [EVAL] batch:  174 | acc: 43.75%,  total acc: 66.07%   [EVAL] batch:  175 | acc: 100.00%,  total acc: 66.26%   [EVAL] batch:  176 | acc: 100.00%,  total acc: 66.45%   [EVAL] batch:  177 | acc: 100.00%,  total acc: 66.64%   [EVAL] batch:  178 | acc: 100.00%,  total acc: 66.83%   [EVAL] batch:  179 | acc: 100.00%,  total acc: 67.01%   [EVAL] batch:  180 | acc: 100.00%,  total acc: 67.20%   [EVAL] batch:  181 | acc: 100.00%,  total acc: 67.38%   [EVAL] batch:  182 | acc: 68.75%,  total acc: 67.38%   [EVAL] batch:  183 | acc: 93.75%,  total acc: 67.53%   [EVAL] batch:  184 | acc: 81.25%,  total acc: 67.60%   [EVAL] batch:  185 | acc: 81.25%,  total acc: 67.67%   [EVAL] batch:  186 | acc: 81.25%,  total acc: 67.75%   [EVAL] batch:  187 | acc: 56.25%,  total acc: 67.69%   [EVAL] batch:  188 | acc: 25.00%,  total acc: 67.46%   [EVAL] batch:  189 | acc: 43.75%,  total acc: 67.34%   [EVAL] batch:  190 | acc: 25.00%,  total acc: 67.11%   [EVAL] batch:  191 | acc: 25.00%,  total acc: 66.89%   [EVAL] batch:  192 | acc: 43.75%,  total acc: 66.77%   [EVAL] batch:  193 | acc: 37.50%,  total acc: 66.62%   [EVAL] batch:  194 | acc: 75.00%,  total acc: 66.67%   [EVAL] batch:  195 | acc: 81.25%,  total acc: 66.74%   [EVAL] batch:  196 | acc: 81.25%,  total acc: 66.81%   [EVAL] batch:  197 | acc: 93.75%,  total acc: 66.95%   [EVAL] batch:  198 | acc: 75.00%,  total acc: 66.99%   [EVAL] batch:  199 | acc: 81.25%,  total acc: 67.06%   [EVAL] batch:  200 | acc: 81.25%,  total acc: 67.13%   [EVAL] batch:  201 | acc: 68.75%,  total acc: 67.14%   [EVAL] batch:  202 | acc: 62.50%,  total acc: 67.12%   [EVAL] batch:  203 | acc: 68.75%,  total acc: 67.13%   [EVAL] batch:  204 | acc: 75.00%,  total acc: 67.16%   [EVAL] batch:  205 | acc: 62.50%,  total acc: 67.14%   [EVAL] batch:  206 | acc: 100.00%,  total acc: 67.30%   [EVAL] batch:  207 | acc: 100.00%,  total acc: 67.46%   [EVAL] batch:  208 | acc: 100.00%,  total acc: 67.61%   [EVAL] batch:  209 | acc: 93.75%,  total acc: 67.74%   [EVAL] batch:  210 | acc: 93.75%,  total acc: 67.86%   [EVAL] batch:  211 | acc: 100.00%,  total acc: 68.01%   [EVAL] batch:  212 | acc: 62.50%,  total acc: 67.99%   [EVAL] batch:  213 | acc: 68.75%,  total acc: 67.99%   [EVAL] batch:  214 | acc: 62.50%,  total acc: 67.97%   [EVAL] batch:  215 | acc: 62.50%,  total acc: 67.94%   [EVAL] batch:  216 | acc: 62.50%,  total acc: 67.91%   [EVAL] batch:  217 | acc: 68.75%,  total acc: 67.92%   [EVAL] batch:  218 | acc: 75.00%,  total acc: 67.95%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 68.10%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 68.24%   [EVAL] batch:  221 | acc: 93.75%,  total acc: 68.36%   [EVAL] batch:  222 | acc: 93.75%,  total acc: 68.47%   [EVAL] batch:  223 | acc: 100.00%,  total acc: 68.61%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 68.75%   [EVAL] batch:  225 | acc: 37.50%,  total acc: 68.61%   [EVAL] batch:  226 | acc: 31.25%,  total acc: 68.45%   [EVAL] batch:  227 | acc: 43.75%,  total acc: 68.34%   [EVAL] batch:  228 | acc: 43.75%,  total acc: 68.23%   [EVAL] batch:  229 | acc: 12.50%,  total acc: 67.99%   [EVAL] batch:  230 | acc: 43.75%,  total acc: 67.88%   [EVAL] batch:  231 | acc: 93.75%,  total acc: 68.00%   [EVAL] batch:  232 | acc: 93.75%,  total acc: 68.11%   [EVAL] batch:  233 | acc: 87.50%,  total acc: 68.19%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 68.32%   [EVAL] batch:  235 | acc: 87.50%,  total acc: 68.41%   [EVAL] batch:  236 | acc: 87.50%,  total acc: 68.49%   [EVAL] batch:  237 | acc: 68.75%,  total acc: 68.49%   [EVAL] batch:  238 | acc: 68.75%,  total acc: 68.49%   [EVAL] batch:  239 | acc: 62.50%,  total acc: 68.46%   [EVAL] batch:  240 | acc: 56.25%,  total acc: 68.41%   [EVAL] batch:  241 | acc: 75.00%,  total acc: 68.44%   [EVAL] batch:  242 | acc: 56.25%,  total acc: 68.39%   [EVAL] batch:  243 | acc: 37.50%,  total acc: 68.26%   [EVAL] batch:  244 | acc: 68.75%,  total acc: 68.27%   [EVAL] batch:  245 | acc: 68.75%,  total acc: 68.27%   [EVAL] batch:  246 | acc: 56.25%,  total acc: 68.22%   [EVAL] batch:  247 | acc: 75.00%,  total acc: 68.25%   [EVAL] batch:  248 | acc: 87.50%,  total acc: 68.32%   [EVAL] batch:  249 | acc: 62.50%,  total acc: 68.30%   [EVAL] batch:  250 | acc: 93.75%,  total acc: 68.40%   [EVAL] batch:  251 | acc: 81.25%,  total acc: 68.45%   [EVAL] batch:  252 | acc: 81.25%,  total acc: 68.50%   [EVAL] batch:  253 | acc: 75.00%,  total acc: 68.53%   [EVAL] batch:  254 | acc: 87.50%,  total acc: 68.60%   [EVAL] batch:  255 | acc: 100.00%,  total acc: 68.73%   [EVAL] batch:  256 | acc: 43.75%,  total acc: 68.63%   [EVAL] batch:  257 | acc: 31.25%,  total acc: 68.48%   [EVAL] batch:  258 | acc: 43.75%,  total acc: 68.39%   [EVAL] batch:  259 | acc: 31.25%,  total acc: 68.25%   [EVAL] batch:  260 | acc: 25.00%,  total acc: 68.08%   [EVAL] batch:  261 | acc: 43.75%,  total acc: 67.99%   [EVAL] batch:  262 | acc: 50.00%,  total acc: 67.92%   [EVAL] batch:  263 | acc: 81.25%,  total acc: 67.97%   [EVAL] batch:  264 | acc: 68.75%,  total acc: 67.97%   [EVAL] batch:  265 | acc: 62.50%,  total acc: 67.95%   [EVAL] batch:  266 | acc: 75.00%,  total acc: 67.98%   [EVAL] batch:  267 | acc: 68.75%,  total acc: 67.98%   [EVAL] batch:  268 | acc: 68.75%,  total acc: 67.98%   [EVAL] batch:  269 | acc: 43.75%,  total acc: 67.89%   [EVAL] batch:  270 | acc: 75.00%,  total acc: 67.92%   [EVAL] batch:  271 | acc: 62.50%,  total acc: 67.90%   [EVAL] batch:  272 | acc: 18.75%,  total acc: 67.72%   [EVAL] batch:  273 | acc: 75.00%,  total acc: 67.75%   [EVAL] batch:  274 | acc: 50.00%,  total acc: 67.68%   [EVAL] batch:  275 | acc: 100.00%,  total acc: 67.80%   [EVAL] batch:  276 | acc: 100.00%,  total acc: 67.92%   [EVAL] batch:  277 | acc: 100.00%,  total acc: 68.03%   [EVAL] batch:  278 | acc: 100.00%,  total acc: 68.15%   [EVAL] batch:  279 | acc: 100.00%,  total acc: 68.26%   [EVAL] batch:  280 | acc: 100.00%,  total acc: 68.37%   [EVAL] batch:  281 | acc: 68.75%,  total acc: 68.37%   [EVAL] batch:  282 | acc: 68.75%,  total acc: 68.37%   [EVAL] batch:  283 | acc: 56.25%,  total acc: 68.33%   [EVAL] batch:  284 | acc: 68.75%,  total acc: 68.33%   [EVAL] batch:  285 | acc: 62.50%,  total acc: 68.31%   [EVAL] batch:  286 | acc: 56.25%,  total acc: 68.27%   [EVAL] batch:  287 | acc: 62.50%,  total acc: 68.25%   [EVAL] batch:  288 | acc: 100.00%,  total acc: 68.36%   [EVAL] batch:  289 | acc: 100.00%,  total acc: 68.47%   [EVAL] batch:  290 | acc: 100.00%,  total acc: 68.58%   [EVAL] batch:  291 | acc: 100.00%,  total acc: 68.69%   [EVAL] batch:  292 | acc: 100.00%,  total acc: 68.79%   [EVAL] batch:  293 | acc: 93.75%,  total acc: 68.88%   [EVAL] batch:  294 | acc: 100.00%,  total acc: 68.98%   [EVAL] batch:  295 | acc: 100.00%,  total acc: 69.09%   [EVAL] batch:  296 | acc: 100.00%,  total acc: 69.19%   [EVAL] batch:  297 | acc: 100.00%,  total acc: 69.30%   [EVAL] batch:  298 | acc: 100.00%,  total acc: 69.40%   [EVAL] batch:  299 | acc: 93.75%,  total acc: 69.48%   [EVAL] batch:  300 | acc: 93.75%,  total acc: 69.56%   [EVAL] batch:  301 | acc: 93.75%,  total acc: 69.64%   [EVAL] batch:  302 | acc: 93.75%,  total acc: 69.72%   [EVAL] batch:  303 | acc: 100.00%,  total acc: 69.82%   [EVAL] batch:  304 | acc: 81.25%,  total acc: 69.86%   [EVAL] batch:  305 | acc: 100.00%,  total acc: 69.96%   [EVAL] batch:  306 | acc: 87.50%,  total acc: 70.01%   [EVAL] batch:  307 | acc: 75.00%,  total acc: 70.03%   [EVAL] batch:  308 | acc: 87.50%,  total acc: 70.08%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 70.18%   [EVAL] batch:  310 | acc: 100.00%,  total acc: 70.28%   [EVAL] batch:  311 | acc: 81.25%,  total acc: 70.31%   [EVAL] batch:  312 | acc: 50.00%,  total acc: 70.25%   [EVAL] batch:  313 | acc: 0.00%,  total acc: 70.02%   [EVAL] batch:  314 | acc: 12.50%,  total acc: 69.84%   [EVAL] batch:  315 | acc: 0.00%,  total acc: 69.62%   [EVAL] batch:  316 | acc: 0.00%,  total acc: 69.40%   [EVAL] batch:  317 | acc: 0.00%,  total acc: 69.18%   [EVAL] batch:  318 | acc: 25.00%,  total acc: 69.04%   [EVAL] batch:  319 | acc: 75.00%,  total acc: 69.06%   [EVAL] batch:  320 | acc: 87.50%,  total acc: 69.12%   [EVAL] batch:  321 | acc: 87.50%,  total acc: 69.18%   [EVAL] batch:  322 | acc: 100.00%,  total acc: 69.27%   [EVAL] batch:  323 | acc: 75.00%,  total acc: 69.29%   [EVAL] batch:  324 | acc: 81.25%,  total acc: 69.33%   [EVAL] batch:  325 | acc: 75.00%,  total acc: 69.34%   [EVAL] batch:  326 | acc: 43.75%,  total acc: 69.27%   [EVAL] batch:  327 | acc: 68.75%,  total acc: 69.26%   [EVAL] batch:  328 | acc: 68.75%,  total acc: 69.26%   [EVAL] batch:  329 | acc: 43.75%,  total acc: 69.19%   [EVAL] batch:  330 | acc: 56.25%,  total acc: 69.15%   [EVAL] batch:  331 | acc: 6.25%,  total acc: 68.96%   [EVAL] batch:  332 | acc: 12.50%,  total acc: 68.79%   [EVAL] batch:  333 | acc: 18.75%,  total acc: 68.64%   [EVAL] batch:  334 | acc: 0.00%,  total acc: 68.43%   [EVAL] batch:  335 | acc: 6.25%,  total acc: 68.25%   [EVAL] batch:  336 | acc: 6.25%,  total acc: 68.06%   [EVAL] batch:  337 | acc: 25.00%,  total acc: 67.94%   [EVAL] batch:  338 | acc: 12.50%,  total acc: 67.77%   [EVAL] batch:  339 | acc: 6.25%,  total acc: 67.59%   [EVAL] batch:  340 | acc: 0.00%,  total acc: 67.39%   [EVAL] batch:  341 | acc: 0.00%,  total acc: 67.20%   [EVAL] batch:  342 | acc: 0.00%,  total acc: 67.00%   [EVAL] batch:  343 | acc: 18.75%,  total acc: 66.86%   [EVAL] batch:  344 | acc: 87.50%,  total acc: 66.92%   [EVAL] batch:  345 | acc: 87.50%,  total acc: 66.98%   [EVAL] batch:  346 | acc: 93.75%,  total acc: 67.06%   [EVAL] batch:  347 | acc: 93.75%,  total acc: 67.13%   [EVAL] batch:  348 | acc: 87.50%,  total acc: 67.19%   [EVAL] batch:  349 | acc: 87.50%,  total acc: 67.25%   [EVAL] batch:  350 | acc: 100.00%,  total acc: 67.34%   [EVAL] batch:  351 | acc: 93.75%,  total acc: 67.42%   [EVAL] batch:  352 | acc: 93.75%,  total acc: 67.49%   [EVAL] batch:  353 | acc: 93.75%,  total acc: 67.57%   [EVAL] batch:  354 | acc: 93.75%,  total acc: 67.64%   [EVAL] batch:  355 | acc: 100.00%,  total acc: 67.73%   [EVAL] batch:  356 | acc: 25.00%,  total acc: 67.61%   [EVAL] batch:  357 | acc: 6.25%,  total acc: 67.44%   [EVAL] batch:  358 | acc: 0.00%,  total acc: 67.25%   [EVAL] batch:  359 | acc: 6.25%,  total acc: 67.08%   [EVAL] batch:  360 | acc: 12.50%,  total acc: 66.93%   [EVAL] batch:  361 | acc: 0.00%,  total acc: 66.75%   [EVAL] batch:  362 | acc: 18.75%,  total acc: 66.62%   [EVAL] batch:  363 | acc: 31.25%,  total acc: 66.52%   [EVAL] batch:  364 | acc: 43.75%,  total acc: 66.46%   [EVAL] batch:  365 | acc: 56.25%,  total acc: 66.43%   [EVAL] batch:  366 | acc: 37.50%,  total acc: 66.35%   [EVAL] batch:  367 | acc: 56.25%,  total acc: 66.32%   [EVAL] batch:  368 | acc: 50.00%,  total acc: 66.28%   [EVAL] batch:  369 | acc: 50.00%,  total acc: 66.23%   [EVAL] batch:  370 | acc: 37.50%,  total acc: 66.16%   [EVAL] batch:  371 | acc: 50.00%,  total acc: 66.11%   [EVAL] batch:  372 | acc: 56.25%,  total acc: 66.09%   [EVAL] batch:  373 | acc: 81.25%,  total acc: 66.13%   [EVAL] batch:  374 | acc: 62.50%,  total acc: 66.12%   [EVAL] batch:  375 | acc: 62.50%,  total acc: 66.11%   [EVAL] batch:  376 | acc: 37.50%,  total acc: 66.03%   [EVAL] batch:  377 | acc: 62.50%,  total acc: 66.02%   [EVAL] batch:  378 | acc: 75.00%,  total acc: 66.05%   [EVAL] batch:  379 | acc: 81.25%,  total acc: 66.09%   [EVAL] batch:  380 | acc: 56.25%,  total acc: 66.06%   [EVAL] batch:  381 | acc: 93.75%,  total acc: 66.13%   [EVAL] batch:  382 | acc: 100.00%,  total acc: 66.22%   [EVAL] batch:  383 | acc: 100.00%,  total acc: 66.31%   [EVAL] batch:  384 | acc: 87.50%,  total acc: 66.36%   [EVAL] batch:  385 | acc: 100.00%,  total acc: 66.45%   [EVAL] batch:  386 | acc: 93.75%,  total acc: 66.52%   [EVAL] batch:  387 | acc: 56.25%,  total acc: 66.49%   [EVAL] batch:  388 | acc: 12.50%,  total acc: 66.36%   [EVAL] batch:  389 | acc: 12.50%,  total acc: 66.22%   [EVAL] batch:  390 | acc: 6.25%,  total acc: 66.06%   [EVAL] batch:  391 | acc: 12.50%,  total acc: 65.93%   [EVAL] batch:  392 | acc: 6.25%,  total acc: 65.78%   [EVAL] batch:  393 | acc: 31.25%,  total acc: 65.69%   [EVAL] batch:  394 | acc: 93.75%,  total acc: 65.76%   [EVAL] batch:  395 | acc: 100.00%,  total acc: 65.85%   [EVAL] batch:  396 | acc: 93.75%,  total acc: 65.92%   [EVAL] batch:  397 | acc: 100.00%,  total acc: 66.00%   [EVAL] batch:  398 | acc: 100.00%,  total acc: 66.09%   [EVAL] batch:  399 | acc: 93.75%,  total acc: 66.16%   [EVAL] batch:  400 | acc: 43.75%,  total acc: 66.10%   [EVAL] batch:  401 | acc: 6.25%,  total acc: 65.95%   [EVAL] batch:  402 | acc: 12.50%,  total acc: 65.82%   [EVAL] batch:  403 | acc: 31.25%,  total acc: 65.73%   [EVAL] batch:  404 | acc: 12.50%,  total acc: 65.60%   [EVAL] batch:  405 | acc: 0.00%,  total acc: 65.44%   [EVAL] batch:  406 | acc: 75.00%,  total acc: 65.46%   [EVAL] batch:  407 | acc: 93.75%,  total acc: 65.53%   [EVAL] batch:  408 | acc: 100.00%,  total acc: 65.62%   [EVAL] batch:  409 | acc: 100.00%,  total acc: 65.70%   [EVAL] batch:  410 | acc: 93.75%,  total acc: 65.77%   [EVAL] batch:  411 | acc: 87.50%,  total acc: 65.82%   [EVAL] batch:  412 | acc: 87.50%,  total acc: 65.87%   [EVAL] batch:  413 | acc: 93.75%,  total acc: 65.94%   [EVAL] batch:  414 | acc: 75.00%,  total acc: 65.96%   [EVAL] batch:  415 | acc: 87.50%,  total acc: 66.02%   [EVAL] batch:  416 | acc: 68.75%,  total acc: 66.02%   [EVAL] batch:  417 | acc: 68.75%,  total acc: 66.03%   [EVAL] batch:  418 | acc: 68.75%,  total acc: 66.04%   [EVAL] batch:  419 | acc: 31.25%,  total acc: 65.95%   [EVAL] batch:  420 | acc: 43.75%,  total acc: 65.90%   [EVAL] batch:  421 | acc: 43.75%,  total acc: 65.85%   [EVAL] batch:  422 | acc: 43.75%,  total acc: 65.79%   [EVAL] batch:  423 | acc: 43.75%,  total acc: 65.74%   [EVAL] batch:  424 | acc: 43.75%,  total acc: 65.69%   [EVAL] batch:  425 | acc: 50.00%,  total acc: 65.65%   [EVAL] batch:  426 | acc: 56.25%,  total acc: 65.63%   [EVAL] batch:  427 | acc: 81.25%,  total acc: 65.67%   [EVAL] batch:  428 | acc: 81.25%,  total acc: 65.71%   [EVAL] batch:  429 | acc: 87.50%,  total acc: 65.76%   [EVAL] batch:  430 | acc: 75.00%,  total acc: 65.78%   [EVAL] batch:  431 | acc: 75.00%,  total acc: 65.80%   [EVAL] batch:  432 | acc: 87.50%,  total acc: 65.85%   [EVAL] batch:  433 | acc: 93.75%,  total acc: 65.91%   [EVAL] batch:  434 | acc: 87.50%,  total acc: 65.96%   [EVAL] batch:  435 | acc: 93.75%,  total acc: 66.03%   [EVAL] batch:  436 | acc: 81.25%,  total acc: 66.06%   [EVAL] batch:  437 | acc: 56.25%,  total acc: 66.04%   [EVAL] batch:  438 | acc: 56.25%,  total acc: 66.02%   [EVAL] batch:  439 | acc: 56.25%,  total acc: 65.99%   [EVAL] batch:  440 | acc: 43.75%,  total acc: 65.94%   [EVAL] batch:  441 | acc: 68.75%,  total acc: 65.95%   [EVAL] batch:  442 | acc: 56.25%,  total acc: 65.93%   [EVAL] batch:  443 | acc: 50.00%,  total acc: 65.89%   [EVAL] batch:  444 | acc: 75.00%,  total acc: 65.91%   [EVAL] batch:  445 | acc: 62.50%,  total acc: 65.91%   [EVAL] batch:  446 | acc: 68.75%,  total acc: 65.91%   [EVAL] batch:  447 | acc: 68.75%,  total acc: 65.92%   [EVAL] batch:  448 | acc: 37.50%,  total acc: 65.85%   [EVAL] batch:  449 | acc: 100.00%,  total acc: 65.93%   [EVAL] batch:  450 | acc: 50.00%,  total acc: 65.90%   [EVAL] batch:  451 | acc: 56.25%,  total acc: 65.87%   [EVAL] batch:  452 | acc: 62.50%,  total acc: 65.87%   [EVAL] batch:  453 | acc: 56.25%,  total acc: 65.85%   [EVAL] batch:  454 | acc: 37.50%,  total acc: 65.78%   [EVAL] batch:  455 | acc: 75.00%,  total acc: 65.80%   [EVAL] batch:  456 | acc: 87.50%,  total acc: 65.85%   [EVAL] batch:  457 | acc: 93.75%,  total acc: 65.91%   [EVAL] batch:  458 | acc: 100.00%,  total acc: 65.99%   [EVAL] batch:  459 | acc: 100.00%,  total acc: 66.06%   [EVAL] batch:  460 | acc: 93.75%,  total acc: 66.12%   [EVAL] batch:  461 | acc: 100.00%,  total acc: 66.19%   [EVAL] batch:  462 | acc: 81.25%,  total acc: 66.23%   [EVAL] batch:  463 | acc: 68.75%,  total acc: 66.23%   [EVAL] batch:  464 | acc: 68.75%,  total acc: 66.24%   [EVAL] batch:  465 | acc: 56.25%,  total acc: 66.22%   [EVAL] batch:  466 | acc: 68.75%,  total acc: 66.22%   [EVAL] batch:  467 | acc: 75.00%,  total acc: 66.24%   [EVAL] batch:  468 | acc: 50.00%,  total acc: 66.20%   [EVAL] batch:  469 | acc: 18.75%,  total acc: 66.10%   [EVAL] batch:  470 | acc: 18.75%,  total acc: 66.00%   [EVAL] batch:  471 | acc: 31.25%,  total acc: 65.93%   [EVAL] batch:  472 | acc: 31.25%,  total acc: 65.86%   [EVAL] batch:  473 | acc: 56.25%,  total acc: 65.84%   [EVAL] batch:  474 | acc: 25.00%,  total acc: 65.75%   [EVAL] batch:  475 | acc: 81.25%,  total acc: 65.78%   [EVAL] batch:  476 | acc: 81.25%,  total acc: 65.81%   [EVAL] batch:  477 | acc: 87.50%,  total acc: 65.86%   [EVAL] batch:  478 | acc: 100.00%,  total acc: 65.93%   [EVAL] batch:  479 | acc: 81.25%,  total acc: 65.96%   [EVAL] batch:  480 | acc: 68.75%,  total acc: 65.97%   [EVAL] batch:  481 | acc: 93.75%,  total acc: 66.03%   [EVAL] batch:  482 | acc: 75.00%,  total acc: 66.05%   [EVAL] batch:  483 | acc: 81.25%,  total acc: 66.08%   [EVAL] batch:  484 | acc: 68.75%,  total acc: 66.08%   [EVAL] batch:  485 | acc: 62.50%,  total acc: 66.08%   [EVAL] batch:  486 | acc: 81.25%,  total acc: 66.11%   [EVAL] batch:  487 | acc: 93.75%,  total acc: 66.16%   [EVAL] batch:  488 | acc: 100.00%,  total acc: 66.23%   [EVAL] batch:  489 | acc: 100.00%,  total acc: 66.30%   [EVAL] batch:  490 | acc: 100.00%,  total acc: 66.37%   [EVAL] batch:  491 | acc: 100.00%,  total acc: 66.44%   [EVAL] batch:  492 | acc: 100.00%,  total acc: 66.51%   [EVAL] batch:  493 | acc: 100.00%,  total acc: 66.57%   [EVAL] batch:  494 | acc: 100.00%,  total acc: 66.64%   [EVAL] batch:  495 | acc: 87.50%,  total acc: 66.68%   [EVAL] batch:  496 | acc: 93.75%,  total acc: 66.74%   [EVAL] batch:  497 | acc: 87.50%,  total acc: 66.78%   [EVAL] batch:  498 | acc: 100.00%,  total acc: 66.85%   [EVAL] batch:  499 | acc: 100.00%,  total acc: 66.91%   
cur_acc:  ['0.9286', '0.7827', '0.7262', '0.7331', '0.8452', '0.5784', '0.6657', '0.7212']
his_acc:  ['0.9286', '0.8585', '0.7992', '0.7615', '0.7730', '0.7297', '0.6928', '0.6691']
--------Round  3
seed:  400
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hybridprompt_12token/train.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hybridprompt_12token/valid.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hybridprompt_12token/test.pkl
Task_order: [7 0 1 2 5 3 4 6]
prepared data!
Clustering into  4  clusters
Clusters:  [0 3 2 0 1 1 2 0 3 3]
Losses:  20.088953018188477 6.057196617126465 0.9910752773284912
CurrentTrain: epoch  0, batch     0 | loss: 20.0889530Losses:  18.019695281982422 4.007472991943359 1.2255125045776367
CurrentTrain: epoch  0, batch     1 | loss: 18.0196953Losses:  20.904451370239258 7.013500690460205 1.1639232635498047
CurrentTrain: epoch  0, batch     2 | loss: 20.9044514Losses:  19.311092376708984 5.433562278747559 1.1623311042785645
CurrentTrain: epoch  0, batch     3 | loss: 19.3110924Losses:  19.289690017700195 6.1619415283203125 1.0956532955169678
CurrentTrain: epoch  0, batch     4 | loss: 19.2896900Losses:  17.99156951904297 4.760045051574707 0.9947044849395752
CurrentTrain: epoch  0, batch     5 | loss: 17.9915695Losses:  17.186790466308594 4.249220848083496 1.0319406986236572
CurrentTrain: epoch  0, batch     6 | loss: 17.1867905Losses:  17.059642791748047 3.9212117195129395 0.9816431999206543
CurrentTrain: epoch  0, batch     7 | loss: 17.0596428Losses:  19.59913444519043 7.153836727142334 0.9393409490585327
CurrentTrain: epoch  0, batch     8 | loss: 19.5991344Losses:  17.469966888427734 4.603085041046143 0.9099172353744507
CurrentTrain: epoch  0, batch     9 | loss: 17.4699669Losses:  15.465264320373535 3.0350053310394287 0.8502978086471558
CurrentTrain: epoch  0, batch    10 | loss: 15.4652643Losses:  17.671958923339844 5.462100982666016 0.8333451151847839
CurrentTrain: epoch  0, batch    11 | loss: 17.6719589Losses:  18.4838809967041 6.447478294372559 0.6820287108421326
CurrentTrain: epoch  0, batch    12 | loss: 18.4838810Losses:  18.576154708862305 6.7055511474609375 0.7345735430717468
CurrentTrain: epoch  0, batch    13 | loss: 18.5761547Losses:  17.9632568359375 6.431609153747559 0.6646742224693298
CurrentTrain: epoch  0, batch    14 | loss: 17.9632568Losses:  14.531209945678711 3.0832180976867676 0.7236542701721191
CurrentTrain: epoch  0, batch    15 | loss: 14.5312099Losses:  17.561174392700195 6.18558406829834 0.6735175251960754
CurrentTrain: epoch  0, batch    16 | loss: 17.5611744Losses:  15.012327194213867 3.7508933544158936 0.6561304330825806
CurrentTrain: epoch  0, batch    17 | loss: 15.0123272Losses:  16.546131134033203 5.481260299682617 0.5713372826576233
CurrentTrain: epoch  0, batch    18 | loss: 16.5461311Losses:  19.53382682800293 8.158452987670898 0.469205379486084
CurrentTrain: epoch  0, batch    19 | loss: 19.5338268Losses:  15.621885299682617 4.409568786621094 0.5866820812225342
CurrentTrain: epoch  0, batch    20 | loss: 15.6218853Losses:  15.802017211914062 5.045585632324219 0.5524978637695312
CurrentTrain: epoch  0, batch    21 | loss: 15.8020172Losses:  15.43027114868164 4.236808776855469 0.6222869157791138
CurrentTrain: epoch  0, batch    22 | loss: 15.4302711Losses:  15.548028945922852 4.30148983001709 0.5780743360519409
CurrentTrain: epoch  0, batch    23 | loss: 15.5480289Losses:  18.387474060058594 7.406563758850098 0.5539135336875916
CurrentTrain: epoch  0, batch    24 | loss: 18.3874741Losses:  19.377405166625977 7.821474075317383 0.5492063760757446
CurrentTrain: epoch  0, batch    25 | loss: 19.3774052Losses:  15.065605163574219 4.152237892150879 0.5411379337310791
CurrentTrain: epoch  0, batch    26 | loss: 15.0656052Losses:  17.426267623901367 6.5897111892700195 0.5431962013244629
CurrentTrain: epoch  0, batch    27 | loss: 17.4262676Losses:  15.506980895996094 4.868825912475586 0.5113062858581543
CurrentTrain: epoch  0, batch    28 | loss: 15.5069809Losses:  15.158807754516602 4.590594291687012 0.47111326456069946
CurrentTrain: epoch  0, batch    29 | loss: 15.1588078Losses:  15.297115325927734 4.664294242858887 0.552609920501709
CurrentTrain: epoch  0, batch    30 | loss: 15.2971153Losses:  15.93626880645752 5.779074668884277 0.46051275730133057
CurrentTrain: epoch  0, batch    31 | loss: 15.9362688Losses:  20.077301025390625 9.968408584594727 0.3899533450603485
CurrentTrain: epoch  0, batch    32 | loss: 20.0773010Losses:  14.078064918518066 3.410930871963501 0.598382294178009
CurrentTrain: epoch  0, batch    33 | loss: 14.0780649Losses:  15.663207054138184 5.534373760223389 0.523589551448822
CurrentTrain: epoch  0, batch    34 | loss: 15.6632071Losses:  15.031388282775879 4.455791473388672 0.5574264526367188
CurrentTrain: epoch  0, batch    35 | loss: 15.0313883Losses:  15.506487846374512 5.39622688293457 0.526802659034729
CurrentTrain: epoch  0, batch    36 | loss: 15.5064878Losses:  16.30621910095215 5.879135608673096 0.5098178386688232
CurrentTrain: epoch  0, batch    37 | loss: 16.3062191Losses:  14.897364616394043 4.436533451080322 0.5320746898651123
CurrentTrain: epoch  0, batch    38 | loss: 14.8973646Losses:  16.168657302856445 6.245415687561035 0.4542621076107025
CurrentTrain: epoch  0, batch    39 | loss: 16.1686573Losses:  13.571861267089844 4.0515031814575195 0.4851379990577698
CurrentTrain: epoch  0, batch    40 | loss: 13.5718613Losses:  14.169081687927246 5.212497234344482 0.4628608524799347
CurrentTrain: epoch  0, batch    41 | loss: 14.1690817Losses:  16.34841537475586 6.093636989593506 0.5248581171035767
CurrentTrain: epoch  0, batch    42 | loss: 16.3484154Losses:  14.459565162658691 4.8797807693481445 0.5063968896865845
CurrentTrain: epoch  0, batch    43 | loss: 14.4595652Losses:  12.466999053955078 3.5033109188079834 0.45338794589042664
CurrentTrain: epoch  0, batch    44 | loss: 12.4669991Losses:  12.08526611328125 3.092130422592163 0.45890307426452637
CurrentTrain: epoch  0, batch    45 | loss: 12.0852661Losses:  11.873032569885254 2.62127685546875 0.443976491689682
CurrentTrain: epoch  0, batch    46 | loss: 11.8730326Losses:  14.38431167602539 4.560723304748535 0.42516881227493286
CurrentTrain: epoch  0, batch    47 | loss: 14.3843117Losses:  14.99412727355957 5.308880805969238 0.46162277460098267
CurrentTrain: epoch  0, batch    48 | loss: 14.9941273Losses:  12.281017303466797 2.599839687347412 0.392680823802948
CurrentTrain: epoch  0, batch    49 | loss: 12.2810173Losses:  14.557605743408203 5.864963054656982 0.398088276386261
CurrentTrain: epoch  0, batch    50 | loss: 14.5576057Losses:  13.360971450805664 3.9117214679718018 0.40917742252349854
CurrentTrain: epoch  0, batch    51 | loss: 13.3609715Losses:  13.674001693725586 4.1076812744140625 0.40726304054260254
CurrentTrain: epoch  0, batch    52 | loss: 13.6740017Losses:  12.889704704284668 4.059966564178467 0.42626455426216125
CurrentTrain: epoch  0, batch    53 | loss: 12.8897047Losses:  12.827221870422363 3.2314765453338623 0.40332967042922974
CurrentTrain: epoch  0, batch    54 | loss: 12.8272219Losses:  13.13258171081543 4.110160827636719 0.4173399806022644
CurrentTrain: epoch  0, batch    55 | loss: 13.1325817Losses:  13.937081336975098 5.286290168762207 0.3820788860321045
CurrentTrain: epoch  0, batch    56 | loss: 13.9370813Losses:  12.643641471862793 3.914255380630493 0.4070805013179779
CurrentTrain: epoch  0, batch    57 | loss: 12.6436415Losses:  12.037700653076172 3.103424072265625 0.38442757725715637
CurrentTrain: epoch  0, batch    58 | loss: 12.0377007Losses:  12.638229370117188 4.68095588684082 0.36633506417274475
CurrentTrain: epoch  0, batch    59 | loss: 12.6382294Losses:  11.250426292419434 3.30599308013916 0.3609936535358429
CurrentTrain: epoch  0, batch    60 | loss: 11.2504263Losses:  15.03860855102539 5.906953811645508 0.3852912187576294
CurrentTrain: epoch  0, batch    61 | loss: 15.0386086Losses:  8.136446952819824 1.145721435546875 0.22863344848155975
CurrentTrain: epoch  0, batch    62 | loss: 8.1364470Losses:  13.445087432861328 5.209290981292725 0.38687658309936523
CurrentTrain: epoch  1, batch     0 | loss: 13.4450874Losses:  12.347234725952148 4.349294662475586 0.2808838486671448
CurrentTrain: epoch  1, batch     1 | loss: 12.3472347Losses:  13.62524127960205 5.219545364379883 0.3943106234073639
CurrentTrain: epoch  1, batch     2 | loss: 13.6252413Losses:  12.955912590026855 4.689009666442871 0.3832727074623108
CurrentTrain: epoch  1, batch     3 | loss: 12.9559126Losses:  16.387718200683594 8.580997467041016 0.3521502614021301
CurrentTrain: epoch  1, batch     4 | loss: 16.3877182Losses:  13.23080062866211 4.449901580810547 0.3742135167121887
CurrentTrain: epoch  1, batch     5 | loss: 13.2308006Losses:  10.764058113098145 3.2959091663360596 0.3607247769832611
CurrentTrain: epoch  1, batch     6 | loss: 10.7640581Losses:  16.67705535888672 8.510819435119629 0.3559918701648712
CurrentTrain: epoch  1, batch     7 | loss: 16.6770554Losses:  10.841501235961914 2.6978254318237305 0.372683584690094
CurrentTrain: epoch  1, batch     8 | loss: 10.8415012Losses:  10.003216743469238 2.1586251258850098 0.36943697929382324
CurrentTrain: epoch  1, batch     9 | loss: 10.0032167Losses:  11.187745094299316 3.510197639465332 0.34535515308380127
CurrentTrain: epoch  1, batch    10 | loss: 11.1877451Losses:  12.343530654907227 4.088088035583496 0.33984389901161194
CurrentTrain: epoch  1, batch    11 | loss: 12.3435307Losses:  12.947527885437012 4.695564270019531 0.34199634194374084
CurrentTrain: epoch  1, batch    12 | loss: 12.9475279Losses:  11.182823181152344 2.8705642223358154 0.34469524025917053
CurrentTrain: epoch  1, batch    13 | loss: 11.1828232Losses:  11.252909660339355 3.7182459831237793 0.3660190999507904
CurrentTrain: epoch  1, batch    14 | loss: 11.2529097Losses:  11.140397071838379 3.3300728797912598 0.35526174306869507
CurrentTrain: epoch  1, batch    15 | loss: 11.1403971Losses:  13.267684936523438 4.824897289276123 0.360767662525177
CurrentTrain: epoch  1, batch    16 | loss: 13.2676849Losses:  9.990647315979004 2.6797280311584473 0.32606640458106995
CurrentTrain: epoch  1, batch    17 | loss: 9.9906473Losses:  12.607322692871094 4.525959491729736 0.35395002365112305
CurrentTrain: epoch  1, batch    18 | loss: 12.6073227Losses:  13.682168006896973 5.759607315063477 0.3034072518348694
CurrentTrain: epoch  1, batch    19 | loss: 13.6821680Losses:  9.915448188781738 2.816518545150757 0.3610341548919678
CurrentTrain: epoch  1, batch    20 | loss: 9.9154482Losses:  10.070488929748535 2.0212173461914062 0.3466094136238098
CurrentTrain: epoch  1, batch    21 | loss: 10.0704889Losses:  10.837727546691895 2.6027309894561768 0.31990498304367065
CurrentTrain: epoch  1, batch    22 | loss: 10.8377275Losses:  13.85303020477295 4.992391586303711 0.33846575021743774
CurrentTrain: epoch  1, batch    23 | loss: 13.8530302Losses:  12.770333290100098 6.039653778076172 0.3111054003238678
CurrentTrain: epoch  1, batch    24 | loss: 12.7703333Losses:  10.022274017333984 2.7442498207092285 0.32702088356018066
CurrentTrain: epoch  1, batch    25 | loss: 10.0222740Losses:  11.798372268676758 3.739367961883545 0.3244054913520813
CurrentTrain: epoch  1, batch    26 | loss: 11.7983723Losses:  11.708026885986328 4.075231552124023 0.3301015794277191
CurrentTrain: epoch  1, batch    27 | loss: 11.7080269Losses:  11.130059242248535 3.861462116241455 0.3358853757381439
CurrentTrain: epoch  1, batch    28 | loss: 11.1300592Losses:  10.85519027709961 3.8023688793182373 0.3227829337120056
CurrentTrain: epoch  1, batch    29 | loss: 10.8551903Losses:  10.708252906799316 3.475546360015869 0.3261007070541382
CurrentTrain: epoch  1, batch    30 | loss: 10.7082529Losses:  11.260337829589844 3.2990503311157227 0.35048848390579224
CurrentTrain: epoch  1, batch    31 | loss: 11.2603378Losses:  11.696267127990723 4.402726650238037 0.34012454748153687
CurrentTrain: epoch  1, batch    32 | loss: 11.6962671Losses:  11.309667587280273 4.394265174865723 0.3682211935520172
CurrentTrain: epoch  1, batch    33 | loss: 11.3096676Losses:  10.917973518371582 4.332009315490723 0.31801319122314453
CurrentTrain: epoch  1, batch    34 | loss: 10.9179735Losses:  10.897689819335938 4.233372688293457 0.31406378746032715
CurrentTrain: epoch  1, batch    35 | loss: 10.8976898Losses:  12.299332618713379 5.113491058349609 0.2238665223121643
CurrentTrain: epoch  1, batch    36 | loss: 12.2993326Losses:  8.954310417175293 2.467985153198242 0.3177027106285095
CurrentTrain: epoch  1, batch    37 | loss: 8.9543104Losses:  9.466602325439453 2.6639580726623535 0.33353254199028015
CurrentTrain: epoch  1, batch    38 | loss: 9.4666023Losses:  12.580179214477539 4.8135480880737305 0.32129746675491333
CurrentTrain: epoch  1, batch    39 | loss: 12.5801792Losses:  12.678168296813965 5.660001754760742 0.3350217938423157
CurrentTrain: epoch  1, batch    40 | loss: 12.6781683Losses:  12.197882652282715 4.225765228271484 0.3656049370765686
CurrentTrain: epoch  1, batch    41 | loss: 12.1978827Losses:  9.040567398071289 2.1617321968078613 0.35239577293395996
CurrentTrain: epoch  1, batch    42 | loss: 9.0405674Losses:  12.885047912597656 5.262810707092285 0.36992692947387695
CurrentTrain: epoch  1, batch    43 | loss: 12.8850479Losses:  10.451033592224121 3.4906606674194336 0.33301886916160583
CurrentTrain: epoch  1, batch    44 | loss: 10.4510336Losses:  11.808534622192383 4.824647903442383 0.21820960938930511
CurrentTrain: epoch  1, batch    45 | loss: 11.8085346Losses:  11.192892074584961 3.300368547439575 0.35450565814971924
CurrentTrain: epoch  1, batch    46 | loss: 11.1928921Losses:  11.50355052947998 3.7617006301879883 0.32286128401756287
CurrentTrain: epoch  1, batch    47 | loss: 11.5035505Losses:  11.480592727661133 3.7082624435424805 0.34800615906715393
CurrentTrain: epoch  1, batch    48 | loss: 11.4805927Losses:  11.74540901184082 4.413959503173828 0.3184092342853546
CurrentTrain: epoch  1, batch    49 | loss: 11.7454090Losses:  11.21656608581543 3.624638557434082 0.33888232707977295
CurrentTrain: epoch  1, batch    50 | loss: 11.2165661Losses:  13.282941818237305 6.16923189163208 0.33558839559555054
CurrentTrain: epoch  1, batch    51 | loss: 13.2829418Losses:  13.396126747131348 5.950808525085449 0.33409708738327026
CurrentTrain: epoch  1, batch    52 | loss: 13.3961267Losses:  11.370565414428711 4.266233921051025 0.31799161434173584
CurrentTrain: epoch  1, batch    53 | loss: 11.3705654Losses:  14.347504615783691 7.599358558654785 0.3084174394607544
CurrentTrain: epoch  1, batch    54 | loss: 14.3475046Losses:  8.479776382446289 2.0438241958618164 0.30681541562080383
CurrentTrain: epoch  1, batch    55 | loss: 8.4797764Losses:  9.849695205688477 3.051813840866089 0.29828619956970215
CurrentTrain: epoch  1, batch    56 | loss: 9.8496952Losses:  10.30620002746582 3.740736246109009 0.3027503490447998
CurrentTrain: epoch  1, batch    57 | loss: 10.3062000Losses:  11.710199356079102 4.975076198577881 0.31472599506378174
CurrentTrain: epoch  1, batch    58 | loss: 11.7101994Losses:  10.08160400390625 2.944103479385376 0.311055451631546
CurrentTrain: epoch  1, batch    59 | loss: 10.0816040Losses:  9.303217887878418 3.4299488067626953 0.3212525248527527
CurrentTrain: epoch  1, batch    60 | loss: 9.3032179Losses:  12.32569694519043 5.655796051025391 0.32702726125717163
CurrentTrain: epoch  1, batch    61 | loss: 12.3256969Losses:  8.917060852050781 1.8396910429000854 0.30753058195114136
CurrentTrain: epoch  1, batch    62 | loss: 8.9170609Losses:  10.070348739624023 3.70792555809021 0.3205025792121887
CurrentTrain: epoch  2, batch     0 | loss: 10.0703487Losses:  10.799174308776855 4.494972229003906 0.30569565296173096
CurrentTrain: epoch  2, batch     1 | loss: 10.7991743Losses:  12.826623916625977 6.349762439727783 0.3040871322154999
CurrentTrain: epoch  2, batch     2 | loss: 12.8266239Losses:  8.909994125366211 2.439751148223877 0.30700477957725525
CurrentTrain: epoch  2, batch     3 | loss: 8.9099941Losses:  9.531624794006348 3.1829833984375 0.3031286597251892
CurrentTrain: epoch  2, batch     4 | loss: 9.5316248Losses:  10.529211044311523 4.570159912109375 0.29881060123443604
CurrentTrain: epoch  2, batch     5 | loss: 10.5292110Losses:  14.239933967590332 7.7668609619140625 0.28849273920059204
CurrentTrain: epoch  2, batch     6 | loss: 14.2399340Losses:  9.322105407714844 3.6191318035125732 0.29642340540885925
CurrentTrain: epoch  2, batch     7 | loss: 9.3221054Losses:  10.77026081085205 4.7438883781433105 0.24147526919841766
CurrentTrain: epoch  2, batch     8 | loss: 10.7702608Losses:  12.858603477478027 6.917497158050537 0.3056741952896118
CurrentTrain: epoch  2, batch     9 | loss: 12.8586035Losses:  12.025028228759766 6.19083833694458 0.30596742033958435
CurrentTrain: epoch  2, batch    10 | loss: 12.0250282Losses:  9.221375465393066 2.837028741836548 0.285076379776001
CurrentTrain: epoch  2, batch    11 | loss: 9.2213755Losses:  12.037846565246582 5.656082630157471 0.38294488191604614
CurrentTrain: epoch  2, batch    12 | loss: 12.0378466Losses:  9.037074089050293 2.8850574493408203 0.31074923276901245
CurrentTrain: epoch  2, batch    13 | loss: 9.0370741Losses:  7.646674156188965 1.9474767446517944 0.27874910831451416
CurrentTrain: epoch  2, batch    14 | loss: 7.6466742Losses:  9.953483581542969 3.5655593872070312 0.2999432384967804
CurrentTrain: epoch  2, batch    15 | loss: 9.9534836Losses:  9.196063995361328 3.059337854385376 0.29969650506973267
CurrentTrain: epoch  2, batch    16 | loss: 9.1960640Losses:  12.260514259338379 5.747583866119385 0.20566074550151825
CurrentTrain: epoch  2, batch    17 | loss: 12.2605143Losses:  9.677960395812988 3.269256114959717 0.296034574508667
CurrentTrain: epoch  2, batch    18 | loss: 9.6779604Losses:  10.772547721862793 4.557852745056152 0.28023627400398254
CurrentTrain: epoch  2, batch    19 | loss: 10.7725477Losses:  10.056145668029785 3.581956624984741 0.2666921019554138
CurrentTrain: epoch  2, batch    20 | loss: 10.0561457Losses:  10.660149574279785 4.426219940185547 0.3189844489097595
CurrentTrain: epoch  2, batch    21 | loss: 10.6601496Losses:  10.814508438110352 4.6618499755859375 0.3031001389026642
CurrentTrain: epoch  2, batch    22 | loss: 10.8145084Losses:  7.795966148376465 2.113083839416504 0.27552297711372375
CurrentTrain: epoch  2, batch    23 | loss: 7.7959661Losses:  12.597012519836426 6.11913537979126 0.31618431210517883
CurrentTrain: epoch  2, batch    24 | loss: 12.5970125Losses:  9.822662353515625 4.0341997146606445 0.28635087609291077
CurrentTrain: epoch  2, batch    25 | loss: 9.8226624Losses:  9.52980899810791 3.7011477947235107 0.28340208530426025
CurrentTrain: epoch  2, batch    26 | loss: 9.5298090Losses:  10.109254837036133 4.437277793884277 0.30591994524002075
CurrentTrain: epoch  2, batch    27 | loss: 10.1092548Losses:  8.310365676879883 2.296466827392578 0.2692773640155792
CurrentTrain: epoch  2, batch    28 | loss: 8.3103657Losses:  8.41101360321045 2.3844151496887207 0.2774689495563507
CurrentTrain: epoch  2, batch    29 | loss: 8.4110136Losses:  12.373519897460938 4.899993419647217 0.31130754947662354
CurrentTrain: epoch  2, batch    30 | loss: 12.3735199Losses:  8.194924354553223 2.8591833114624023 0.265643835067749
CurrentTrain: epoch  2, batch    31 | loss: 8.1949244Losses:  8.953949928283691 2.990809917449951 0.29545146226882935
CurrentTrain: epoch  2, batch    32 | loss: 8.9539499Losses:  8.101304054260254 2.7421391010284424 0.28256088495254517
CurrentTrain: epoch  2, batch    33 | loss: 8.1013041Losses:  8.68673038482666 2.435474395751953 0.2732473313808441
CurrentTrain: epoch  2, batch    34 | loss: 8.6867304Losses:  8.431466102600098 2.6388416290283203 0.258242666721344
CurrentTrain: epoch  2, batch    35 | loss: 8.4314661Losses:  8.196813583374023 2.4965715408325195 0.277360737323761
CurrentTrain: epoch  2, batch    36 | loss: 8.1968136Losses:  9.3038911819458 3.0247364044189453 0.2724565863609314
CurrentTrain: epoch  2, batch    37 | loss: 9.3038912Losses:  11.172261238098145 4.827207565307617 0.2776086628437042
CurrentTrain: epoch  2, batch    38 | loss: 11.1722612Losses:  9.201272010803223 3.2900028228759766 0.2685841917991638
CurrentTrain: epoch  2, batch    39 | loss: 9.2012720Losses:  11.020829200744629 5.235414505004883 0.26186585426330566
CurrentTrain: epoch  2, batch    40 | loss: 11.0208292Losses:  8.363235473632812 2.291398525238037 0.2596622407436371
CurrentTrain: epoch  2, batch    41 | loss: 8.3632355Losses:  10.798018455505371 3.3608145713806152 0.25927841663360596
CurrentTrain: epoch  2, batch    42 | loss: 10.7980185Losses:  11.736799240112305 6.410515308380127 0.2725442051887512
CurrentTrain: epoch  2, batch    43 | loss: 11.7367992Losses:  8.96496868133545 3.0600790977478027 0.2750077545642853
CurrentTrain: epoch  2, batch    44 | loss: 8.9649687Losses:  7.180440425872803 1.5274574756622314 0.26761019229888916
CurrentTrain: epoch  2, batch    45 | loss: 7.1804404Losses:  7.914876937866211 2.0016512870788574 0.2730586528778076
CurrentTrain: epoch  2, batch    46 | loss: 7.9148769Losses:  11.129213333129883 5.5604352951049805 0.2834228277206421
CurrentTrain: epoch  2, batch    47 | loss: 11.1292133Losses:  8.972794532775879 3.10325288772583 0.272845596075058
CurrentTrain: epoch  2, batch    48 | loss: 8.9727945Losses:  8.72166919708252 2.1661617755889893 0.2871343493461609
CurrentTrain: epoch  2, batch    49 | loss: 8.7216692Losses:  8.69874095916748 3.3306403160095215 0.2673385739326477
CurrentTrain: epoch  2, batch    50 | loss: 8.6987410Losses:  11.42729377746582 6.1146559715271 0.2879120111465454
CurrentTrain: epoch  2, batch    51 | loss: 11.4272938Losses:  7.704120635986328 2.0815494060516357 0.27390193939208984
CurrentTrain: epoch  2, batch    52 | loss: 7.7041206Losses:  9.983016967773438 4.016661167144775 0.2669902443885803
CurrentTrain: epoch  2, batch    53 | loss: 9.9830170Losses:  11.751261711120605 5.910665988922119 0.2880197763442993
CurrentTrain: epoch  2, batch    54 | loss: 11.7512617Losses:  8.566878318786621 2.726655960083008 0.2781944274902344
CurrentTrain: epoch  2, batch    55 | loss: 8.5668783Losses:  8.562605857849121 3.3342247009277344 0.26470398902893066
CurrentTrain: epoch  2, batch    56 | loss: 8.5626059Losses:  10.708623886108398 4.654409885406494 0.28485527634620667
CurrentTrain: epoch  2, batch    57 | loss: 10.7086239Losses:  11.675056457519531 5.715823173522949 0.28329232335090637
CurrentTrain: epoch  2, batch    58 | loss: 11.6750565Losses:  8.659780502319336 3.580843687057495 0.28374621272087097
CurrentTrain: epoch  2, batch    59 | loss: 8.6597805Losses:  9.849112510681152 3.9041991233825684 0.285263329744339
CurrentTrain: epoch  2, batch    60 | loss: 9.8491125Losses:  9.78620719909668 4.082602500915527 0.2763403058052063
CurrentTrain: epoch  2, batch    61 | loss: 9.7862072Losses:  6.1361870765686035 0.5345583558082581 0.29495084285736084
CurrentTrain: epoch  2, batch    62 | loss: 6.1361871Losses:  10.447394371032715 4.553536415100098 0.17475315928459167
CurrentTrain: epoch  3, batch     0 | loss: 10.4473944Losses:  7.742328643798828 2.176269054412842 0.26219314336776733
CurrentTrain: epoch  3, batch     1 | loss: 7.7423286Losses:  7.468442916870117 2.551701068878174 0.2703627645969391
CurrentTrain: epoch  3, batch     2 | loss: 7.4684429Losses:  9.105843544006348 3.478659152984619 0.2513945996761322
CurrentTrain: epoch  3, batch     3 | loss: 9.1058435Losses:  8.056992530822754 3.0629217624664307 0.2680000960826874
CurrentTrain: epoch  3, batch     4 | loss: 8.0569925Losses:  9.425837516784668 3.897899866104126 0.2975693345069885
CurrentTrain: epoch  3, batch     5 | loss: 9.4258375Losses:  9.796217918395996 4.557709217071533 0.29060935974121094
CurrentTrain: epoch  3, batch     6 | loss: 9.7962179Losses:  8.034415245056152 2.810307025909424 0.269275963306427
CurrentTrain: epoch  3, batch     7 | loss: 8.0344152Losses:  8.79343032836914 3.8389434814453125 0.28084737062454224
CurrentTrain: epoch  3, batch     8 | loss: 8.7934303Losses:  8.171198844909668 2.809857130050659 0.272477924823761
CurrentTrain: epoch  3, batch     9 | loss: 8.1711988Losses:  7.812360763549805 2.189134120941162 0.2614225149154663
CurrentTrain: epoch  3, batch    10 | loss: 7.8123608Losses:  8.288418769836426 2.863708019256592 0.17221204936504364
CurrentTrain: epoch  3, batch    11 | loss: 8.2884188Losses:  10.324040412902832 5.163236141204834 0.28295189142227173
CurrentTrain: epoch  3, batch    12 | loss: 10.3240404Losses:  7.345777988433838 2.154097080230713 0.26315706968307495
CurrentTrain: epoch  3, batch    13 | loss: 7.3457780Losses:  8.599471092224121 3.513272762298584 0.25060850381851196
CurrentTrain: epoch  3, batch    14 | loss: 8.5994711Losses:  8.06056022644043 3.3367295265197754 0.25048983097076416
CurrentTrain: epoch  3, batch    15 | loss: 8.0605602Losses:  6.82522439956665 1.769587755203247 0.24836289882659912
CurrentTrain: epoch  3, batch    16 | loss: 6.8252244Losses:  10.150833129882812 4.877572536468506 0.28236979246139526
CurrentTrain: epoch  3, batch    17 | loss: 10.1508331Losses:  8.209145545959473 3.215444326400757 0.25842007994651794
CurrentTrain: epoch  3, batch    18 | loss: 8.2091455Losses:  10.236274719238281 5.052944660186768 0.1557024121284485
CurrentTrain: epoch  3, batch    19 | loss: 10.2362747Losses:  7.936655521392822 2.841946601867676 0.2540651559829712
CurrentTrain: epoch  3, batch    20 | loss: 7.9366555Losses:  8.101028442382812 3.3243157863616943 0.2591618001461029
CurrentTrain: epoch  3, batch    21 | loss: 8.1010284Losses:  8.602180480957031 3.269458770751953 0.2508428692817688
CurrentTrain: epoch  3, batch    22 | loss: 8.6021805Losses:  7.77970027923584 3.005122184753418 0.241926908493042
CurrentTrain: epoch  3, batch    23 | loss: 7.7797003Losses:  6.7090325355529785 1.791874647140503 0.2586739659309387
CurrentTrain: epoch  3, batch    24 | loss: 6.7090325Losses:  8.952420234680176 3.0484209060668945 0.2814764678478241
CurrentTrain: epoch  3, batch    25 | loss: 8.9524202Losses:  11.103588104248047 5.50382137298584 0.30550217628479004
CurrentTrain: epoch  3, batch    26 | loss: 11.1035881Losses:  7.630812644958496 2.479954957962036 0.2520914077758789
CurrentTrain: epoch  3, batch    27 | loss: 7.6308126Losses:  9.188592910766602 4.214372634887695 0.24966052174568176
CurrentTrain: epoch  3, batch    28 | loss: 9.1885929Losses:  7.934512138366699 3.0221457481384277 0.27339136600494385
CurrentTrain: epoch  3, batch    29 | loss: 7.9345121Losses:  9.13427734375 4.199633598327637 0.21209995448589325
CurrentTrain: epoch  3, batch    30 | loss: 9.1342773Losses:  9.33008861541748 4.480262756347656 0.26384514570236206
CurrentTrain: epoch  3, batch    31 | loss: 9.3300886Losses:  9.298101425170898 3.6535463333129883 0.2619929909706116
CurrentTrain: epoch  3, batch    32 | loss: 9.2981014Losses:  8.615961074829102 3.777961254119873 0.2652340233325958
CurrentTrain: epoch  3, batch    33 | loss: 8.6159611Losses:  8.628321647644043 3.205660343170166 0.24649707973003387
CurrentTrain: epoch  3, batch    34 | loss: 8.6283216Losses:  7.723610877990723 2.870818853378296 0.25913241505622864
CurrentTrain: epoch  3, batch    35 | loss: 7.7236109Losses:  10.038674354553223 4.340271949768066 0.25887805223464966
CurrentTrain: epoch  3, batch    36 | loss: 10.0386744Losses:  6.945685863494873 2.2605173587799072 0.24108827114105225
CurrentTrain: epoch  3, batch    37 | loss: 6.9456859Losses:  8.550203323364258 3.8370370864868164 0.25123074650764465
CurrentTrain: epoch  3, batch    38 | loss: 8.5502033Losses:  9.110456466674805 3.8652899265289307 0.2705170214176178
CurrentTrain: epoch  3, batch    39 | loss: 9.1104565Losses:  10.54926586151123 5.221079349517822 0.1865304708480835
CurrentTrain: epoch  3, batch    40 | loss: 10.5492659Losses:  8.297369003295898 2.668787956237793 0.2556452751159668
CurrentTrain: epoch  3, batch    41 | loss: 8.2973690Losses:  8.493173599243164 3.097166061401367 0.24922409653663635
CurrentTrain: epoch  3, batch    42 | loss: 8.4931736Losses:  7.714560031890869 2.1709702014923096 0.2529083490371704
CurrentTrain: epoch  3, batch    43 | loss: 7.7145600Losses:  10.829339981079102 4.931009292602539 0.2598751485347748
CurrentTrain: epoch  3, batch    44 | loss: 10.8293400Losses:  11.776375770568848 5.669564247131348 0.2730559706687927
CurrentTrain: epoch  3, batch    45 | loss: 11.7763758Losses:  9.326008796691895 3.9489784240722656 0.2693088948726654
CurrentTrain: epoch  3, batch    46 | loss: 9.3260088Losses:  8.189093589782715 3.222818374633789 0.26562806963920593
CurrentTrain: epoch  3, batch    47 | loss: 8.1890936Losses:  7.792250633239746 2.9566867351531982 0.26720523834228516
CurrentTrain: epoch  3, batch    48 | loss: 7.7922506Losses:  8.337078094482422 2.9973113536834717 0.2502328157424927
CurrentTrain: epoch  3, batch    49 | loss: 8.3370781Losses:  6.980433464050293 2.2178759574890137 0.2580440044403076
CurrentTrain: epoch  3, batch    50 | loss: 6.9804335Losses:  7.712133884429932 2.6852965354919434 0.25257909297943115
CurrentTrain: epoch  3, batch    51 | loss: 7.7121339Losses:  8.273813247680664 3.3853793144226074 0.26206299662590027
CurrentTrain: epoch  3, batch    52 | loss: 8.2738132Losses:  8.00696849822998 1.7892107963562012 0.2409513294696808
CurrentTrain: epoch  3, batch    53 | loss: 8.0069685Losses:  7.6032209396362305 2.6884102821350098 0.2517277002334595
CurrentTrain: epoch  3, batch    54 | loss: 7.6032209Losses:  6.8098368644714355 2.077582597732544 0.24942940473556519
CurrentTrain: epoch  3, batch    55 | loss: 6.8098369Losses:  7.922270774841309 2.5688514709472656 0.2442474216222763
CurrentTrain: epoch  3, batch    56 | loss: 7.9222708Losses:  10.786142349243164 5.028855800628662 0.2822437286376953
CurrentTrain: epoch  3, batch    57 | loss: 10.7861423Losses:  9.66203498840332 4.673724174499512 0.2551458775997162
CurrentTrain: epoch  3, batch    58 | loss: 9.6620350Losses:  7.657591342926025 2.598881483078003 0.25469064712524414
CurrentTrain: epoch  3, batch    59 | loss: 7.6575913Losses:  9.475275039672852 4.663447380065918 0.2668936252593994
CurrentTrain: epoch  3, batch    60 | loss: 9.4752750Losses:  7.26120138168335 2.0959157943725586 0.23711413145065308
CurrentTrain: epoch  3, batch    61 | loss: 7.2612014Losses:  5.489058017730713 1.1595280170440674 0.16469430923461914
CurrentTrain: epoch  3, batch    62 | loss: 5.4890580Losses:  6.804314136505127 1.9752800464630127 0.2412029206752777
CurrentTrain: epoch  4, batch     0 | loss: 6.8043141Losses:  10.880440711975098 5.354901313781738 0.2774607539176941
CurrentTrain: epoch  4, batch     1 | loss: 10.8804407Losses:  9.13507080078125 4.636510848999023 0.2487107366323471
CurrentTrain: epoch  4, batch     2 | loss: 9.1350708Losses:  8.035506248474121 3.3257994651794434 0.24429552257061005
CurrentTrain: epoch  4, batch     3 | loss: 8.0355062Losses:  11.01919174194336 6.267854690551758 0.1759674996137619
CurrentTrain: epoch  4, batch     4 | loss: 11.0191917Losses:  7.670793533325195 2.7673795223236084 0.24531935155391693
CurrentTrain: epoch  4, batch     5 | loss: 7.6707935Losses:  6.709216117858887 2.0569686889648438 0.23610901832580566
CurrentTrain: epoch  4, batch     6 | loss: 6.7092161Losses:  8.504071235656738 3.8478753566741943 0.2528316080570221
CurrentTrain: epoch  4, batch     7 | loss: 8.5040712Losses:  7.559256076812744 2.9893784523010254 0.2560950517654419
CurrentTrain: epoch  4, batch     8 | loss: 7.5592561Losses:  8.258485794067383 3.415834903717041 0.2703852951526642
CurrentTrain: epoch  4, batch     9 | loss: 8.2584858Losses:  7.846632957458496 3.0718016624450684 0.2470400333404541
CurrentTrain: epoch  4, batch    10 | loss: 7.8466330Losses:  10.031919479370117 5.565094470977783 0.16659389436244965
CurrentTrain: epoch  4, batch    11 | loss: 10.0319195Losses:  8.781122207641602 3.6696224212646484 0.2682889997959137
CurrentTrain: epoch  4, batch    12 | loss: 8.7811222Losses:  6.844015121459961 1.8374042510986328 0.2429053783416748
CurrentTrain: epoch  4, batch    13 | loss: 6.8440151Losses:  7.288425922393799 2.2698400020599365 0.24437497556209564
CurrentTrain: epoch  4, batch    14 | loss: 7.2884259Losses:  11.185555458068848 5.950512409210205 0.16317777335643768
CurrentTrain: epoch  4, batch    15 | loss: 11.1855555Losses:  9.000686645507812 4.142387390136719 0.2580532431602478
CurrentTrain: epoch  4, batch    16 | loss: 9.0006866Losses:  9.999076843261719 5.177911758422852 0.15102419257164001
CurrentTrain: epoch  4, batch    17 | loss: 9.9990768Losses:  9.717909812927246 4.750019550323486 0.25898149609565735
CurrentTrain: epoch  4, batch    18 | loss: 9.7179098Losses:  8.063298225402832 3.29598069190979 0.2379213273525238
CurrentTrain: epoch  4, batch    19 | loss: 8.0632982Losses:  6.088671684265137 1.36615788936615 0.2222057580947876
CurrentTrain: epoch  4, batch    20 | loss: 6.0886717Losses:  7.635694980621338 2.932746410369873 0.26203930377960205
CurrentTrain: epoch  4, batch    21 | loss: 7.6356950Losses:  11.026381492614746 6.126667022705078 0.238945871591568
CurrentTrain: epoch  4, batch    22 | loss: 11.0263815Losses:  7.525239944458008 2.7763919830322266 0.23688793182373047
CurrentTrain: epoch  4, batch    23 | loss: 7.5252399Losses:  9.214200019836426 3.8924684524536133 0.26613476872444153
CurrentTrain: epoch  4, batch    24 | loss: 9.2142000Losses:  8.132155418395996 2.9564719200134277 0.24452736973762512
CurrentTrain: epoch  4, batch    25 | loss: 8.1321554Losses:  9.067105293273926 3.5901291370391846 0.26881730556488037
CurrentTrain: epoch  4, batch    26 | loss: 9.0671053Losses:  7.171231269836426 2.4478704929351807 0.23947063088417053
CurrentTrain: epoch  4, batch    27 | loss: 7.1712313Losses:  9.11874008178711 4.593814849853516 0.25448763370513916
CurrentTrain: epoch  4, batch    28 | loss: 9.1187401Losses:  7.5030107498168945 2.1224002838134766 0.22777357697486877
CurrentTrain: epoch  4, batch    29 | loss: 7.5030107Losses:  8.92833423614502 4.209188461303711 0.24621668457984924
CurrentTrain: epoch  4, batch    30 | loss: 8.9283342Losses:  7.637369632720947 2.848684072494507 0.24396005272865295
CurrentTrain: epoch  4, batch    31 | loss: 7.6373696Losses:  8.128453254699707 2.595722198486328 0.24801191687583923
CurrentTrain: epoch  4, batch    32 | loss: 8.1284533Losses:  9.90767765045166 4.1033148765563965 0.25740742683410645
CurrentTrain: epoch  4, batch    33 | loss: 9.9076777Losses:  7.551854610443115 2.3938231468200684 0.22412241995334625
CurrentTrain: epoch  4, batch    34 | loss: 7.5518546Losses:  7.534318923950195 2.661006212234497 0.25301188230514526
CurrentTrain: epoch  4, batch    35 | loss: 7.5343189Losses:  8.206741333007812 3.3109569549560547 0.23740558326244354
CurrentTrain: epoch  4, batch    36 | loss: 8.2067413Losses:  6.148663520812988 1.3840649127960205 0.2272537648677826
CurrentTrain: epoch  4, batch    37 | loss: 6.1486635Losses:  7.793222427368164 2.6718950271606445 0.2353219836950302
CurrentTrain: epoch  4, batch    38 | loss: 7.7932224Losses:  6.3693528175354 1.9281243085861206 0.2310989499092102
CurrentTrain: epoch  4, batch    39 | loss: 6.3693528Losses:  7.083012104034424 2.5782341957092285 0.24840253591537476
CurrentTrain: epoch  4, batch    40 | loss: 7.0830121Losses:  7.8858537673950195 3.2061550617218018 0.25386542081832886
CurrentTrain: epoch  4, batch    41 | loss: 7.8858538Losses:  10.321345329284668 5.679426193237305 0.24672040343284607
CurrentTrain: epoch  4, batch    42 | loss: 10.3213453Losses:  7.38265323638916 2.734163999557495 0.24340739846229553
CurrentTrain: epoch  4, batch    43 | loss: 7.3826532Losses:  8.006404876708984 3.0832293033599854 0.24872732162475586
CurrentTrain: epoch  4, batch    44 | loss: 8.0064049Losses:  6.493338584899902 1.706916093826294 0.22606578469276428
CurrentTrain: epoch  4, batch    45 | loss: 6.4933386Losses:  9.21717357635498 4.715517520904541 0.2452249526977539
CurrentTrain: epoch  4, batch    46 | loss: 9.2171736Losses:  6.395407199859619 1.8789359331130981 0.2369113266468048
CurrentTrain: epoch  4, batch    47 | loss: 6.3954072Losses:  7.596892356872559 3.226445198059082 0.24085785448551178
CurrentTrain: epoch  4, batch    48 | loss: 7.5968924Losses:  7.558171272277832 3.018386125564575 0.2423582375049591
CurrentTrain: epoch  4, batch    49 | loss: 7.5581713Losses:  9.740206718444824 4.67570161819458 0.2564384341239929
CurrentTrain: epoch  4, batch    50 | loss: 9.7402067Losses:  6.796494007110596 2.269951343536377 0.22582730650901794
CurrentTrain: epoch  4, batch    51 | loss: 6.7964940Losses:  6.979037284851074 2.491027355194092 0.249531090259552
CurrentTrain: epoch  4, batch    52 | loss: 6.9790373Losses:  8.131800651550293 3.508131504058838 0.25731411576271057
CurrentTrain: epoch  4, batch    53 | loss: 8.1318007Losses:  7.022925853729248 2.5617897510528564 0.24150025844573975
CurrentTrain: epoch  4, batch    54 | loss: 7.0229259Losses:  6.634152889251709 2.091675281524658 0.2256908416748047
CurrentTrain: epoch  4, batch    55 | loss: 6.6341529Losses:  8.219770431518555 3.8786885738372803 0.23631548881530762
CurrentTrain: epoch  4, batch    56 | loss: 8.2197704Losses:  6.885897159576416 2.2272768020629883 0.22571425139904022
CurrentTrain: epoch  4, batch    57 | loss: 6.8858972Losses:  7.308009147644043 2.5076866149902344 0.23529483377933502
CurrentTrain: epoch  4, batch    58 | loss: 7.3080091Losses:  8.578023910522461 4.165804386138916 0.24121811985969543
CurrentTrain: epoch  4, batch    59 | loss: 8.5780239Losses:  6.733330726623535 2.261777400970459 0.2340559959411621
CurrentTrain: epoch  4, batch    60 | loss: 6.7333307Losses:  9.538346290588379 5.133241176605225 0.2582026720046997
CurrentTrain: epoch  4, batch    61 | loss: 9.5383463Losses:  5.1047234535217285 0.5684260129928589 0.2731477618217468
CurrentTrain: epoch  4, batch    62 | loss: 5.1047235Losses:  6.642039775848389 2.0048210620880127 0.24186544120311737
CurrentTrain: epoch  5, batch     0 | loss: 6.6420398Losses:  7.5865092277526855 2.8420283794403076 0.2554602026939392
CurrentTrain: epoch  5, batch     1 | loss: 7.5865092Losses:  6.417571544647217 1.896005630493164 0.24147948622703552
CurrentTrain: epoch  5, batch     2 | loss: 6.4175715Losses:  5.924088001251221 1.3153390884399414 0.21948808431625366
CurrentTrain: epoch  5, batch     3 | loss: 5.9240880Losses:  7.024165153503418 2.6055283546447754 0.14667552709579468
CurrentTrain: epoch  5, batch     4 | loss: 7.0241652Losses:  7.7189483642578125 3.2463693618774414 0.23689094185829163
CurrentTrain: epoch  5, batch     5 | loss: 7.7189484Losses:  7.1185736656188965 2.6640894412994385 0.23924681544303894
CurrentTrain: epoch  5, batch     6 | loss: 7.1185737Losses:  7.268564224243164 2.8210902214050293 0.230416938662529
CurrentTrain: epoch  5, batch     7 | loss: 7.2685642Losses:  6.114648342132568 1.6837526559829712 0.2203921377658844
CurrentTrain: epoch  5, batch     8 | loss: 6.1146483Losses:  6.927172660827637 2.395385265350342 0.2478155493736267
CurrentTrain: epoch  5, batch     9 | loss: 6.9271727Losses:  6.735355854034424 2.245375633239746 0.22887955605983734
CurrentTrain: epoch  5, batch    10 | loss: 6.7353559Losses:  8.269415855407715 3.651412010192871 0.25156092643737793
CurrentTrain: epoch  5, batch    11 | loss: 8.2694159Losses:  8.007024765014648 3.5360124111175537 0.2515634596347809
CurrentTrain: epoch  5, batch    12 | loss: 8.0070248Losses:  7.410793781280518 2.3548874855041504 0.23378527164459229
CurrentTrain: epoch  5, batch    13 | loss: 7.4107938Losses:  7.55800199508667 3.1464333534240723 0.16726522147655487
CurrentTrain: epoch  5, batch    14 | loss: 7.5580020Losses:  7.072872161865234 2.727142333984375 0.23721134662628174
CurrentTrain: epoch  5, batch    15 | loss: 7.0728722Losses:  7.506626605987549 3.0888991355895996 0.23285311460494995
CurrentTrain: epoch  5, batch    16 | loss: 7.5066266Losses:  8.021635055541992 3.606748104095459 0.24203768372535706
CurrentTrain: epoch  5, batch    17 | loss: 8.0216351Losses:  8.0596923828125 3.5469918251037598 0.24563416838645935
CurrentTrain: epoch  5, batch    18 | loss: 8.0596924Losses:  7.706419467926025 3.2523720264434814 0.24195612967014313
CurrentTrain: epoch  5, batch    19 | loss: 7.7064195Losses:  9.649969100952148 5.258975982666016 0.26121509075164795
CurrentTrain: epoch  5, batch    20 | loss: 9.6499691Losses:  7.8417863845825195 3.379303455352783 0.24164336919784546
CurrentTrain: epoch  5, batch    21 | loss: 7.8417864Losses:  9.169781684875488 4.86346435546875 0.1717076599597931
CurrentTrain: epoch  5, batch    22 | loss: 9.1697817Losses:  6.864580154418945 2.3612594604492188 0.25367626547813416
CurrentTrain: epoch  5, batch    23 | loss: 6.8645802Losses:  6.247700214385986 1.7788145542144775 0.21882100403308868
CurrentTrain: epoch  5, batch    24 | loss: 6.2477002Losses:  9.424477577209473 4.66863489151001 0.2843462824821472
CurrentTrain: epoch  5, batch    25 | loss: 9.4244776Losses:  8.232905387878418 3.6864233016967773 0.24290722608566284
CurrentTrain: epoch  5, batch    26 | loss: 8.2329054Losses:  7.680319786071777 3.264099597930908 0.23656299710273743
CurrentTrain: epoch  5, batch    27 | loss: 7.6803198Losses:  7.445499897003174 2.914950370788574 0.2357979267835617
CurrentTrain: epoch  5, batch    28 | loss: 7.4454999Losses:  9.122150421142578 4.3606157302856445 0.24469207227230072
CurrentTrain: epoch  5, batch    29 | loss: 9.1221504Losses:  7.807709217071533 2.9531564712524414 0.23843127489089966
CurrentTrain: epoch  5, batch    30 | loss: 7.8077092Losses:  8.018819808959961 3.4755754470825195 0.262210488319397
CurrentTrain: epoch  5, batch    31 | loss: 8.0188198Losses:  6.861082077026367 2.4479732513427734 0.23827636241912842
CurrentTrain: epoch  5, batch    32 | loss: 6.8610821Losses:  7.1709794998168945 2.8167569637298584 0.23842407763004303
CurrentTrain: epoch  5, batch    33 | loss: 7.1709795Losses:  6.124634742736816 1.7052438259124756 0.21888446807861328
CurrentTrain: epoch  5, batch    34 | loss: 6.1246347Losses:  7.55622673034668 3.2234392166137695 0.23291289806365967
CurrentTrain: epoch  5, batch    35 | loss: 7.5562267Losses:  7.997652053833008 3.654110908508301 0.2378663271665573
CurrentTrain: epoch  5, batch    36 | loss: 7.9976521Losses:  6.7963762283325195 2.416278839111328 0.24215802550315857
CurrentTrain: epoch  5, batch    37 | loss: 6.7963762Losses:  8.705060005187988 4.239339828491211 0.24615974724292755
CurrentTrain: epoch  5, batch    38 | loss: 8.7050600Losses:  7.6290411949157715 3.1689300537109375 0.24118249118328094
CurrentTrain: epoch  5, batch    39 | loss: 7.6290412Losses:  9.497579574584961 4.9789886474609375 0.24936094880104065
CurrentTrain: epoch  5, batch    40 | loss: 9.4975796Losses:  7.424914836883545 3.032696485519409 0.24901334941387177
CurrentTrain: epoch  5, batch    41 | loss: 7.4249148Losses:  6.810804843902588 2.2629590034484863 0.2296554446220398
CurrentTrain: epoch  5, batch    42 | loss: 6.8108048Losses:  8.223564147949219 3.874041795730591 0.23817089200019836
CurrentTrain: epoch  5, batch    43 | loss: 8.2235641Losses:  6.773193836212158 2.0675997734069824 0.22269442677497864
CurrentTrain: epoch  5, batch    44 | loss: 6.7731938Losses:  8.487757682800293 3.9804916381835938 0.24850349128246307
CurrentTrain: epoch  5, batch    45 | loss: 8.4877577Losses:  6.826022148132324 2.445235013961792 0.23458102345466614
CurrentTrain: epoch  5, batch    46 | loss: 6.8260221Losses:  6.3257155418396 1.9713211059570312 0.22009439766407013
CurrentTrain: epoch  5, batch    47 | loss: 6.3257155Losses:  7.366020679473877 2.9767799377441406 0.22764751315116882
CurrentTrain: epoch  5, batch    48 | loss: 7.3660207Losses:  7.439718723297119 3.0669105052948 0.22634656727313995
CurrentTrain: epoch  5, batch    49 | loss: 7.4397187Losses:  10.59692096710205 6.1873040199279785 0.2526963949203491
CurrentTrain: epoch  5, batch    50 | loss: 10.5969210Losses:  7.084983825683594 2.763014078140259 0.24322055280208588
CurrentTrain: epoch  5, batch    51 | loss: 7.0849838Losses:  8.258389472961426 3.9062399864196777 0.23577821254730225
CurrentTrain: epoch  5, batch    52 | loss: 8.2583895Losses:  8.997143745422363 4.498545169830322 0.2442934513092041
CurrentTrain: epoch  5, batch    53 | loss: 8.9971437Losses:  6.786640167236328 2.408210277557373 0.23160791397094727
CurrentTrain: epoch  5, batch    54 | loss: 6.7866402Losses:  8.269051551818848 3.886599063873291 0.23583652079105377
CurrentTrain: epoch  5, batch    55 | loss: 8.2690516Losses:  8.377521514892578 4.0868120193481445 0.22477498650550842
CurrentTrain: epoch  5, batch    56 | loss: 8.3775215Losses:  8.63662338256836 4.309240818023682 0.23673811554908752
CurrentTrain: epoch  5, batch    57 | loss: 8.6366234Losses:  7.418288707733154 3.0284907817840576 0.25679922103881836
CurrentTrain: epoch  5, batch    58 | loss: 7.4182887Losses:  7.776388645172119 3.3619213104248047 0.2542455792427063
CurrentTrain: epoch  5, batch    59 | loss: 7.7763886Losses:  6.9503021240234375 2.6218044757843018 0.23451723158359528
CurrentTrain: epoch  5, batch    60 | loss: 6.9503021Losses:  6.732367515563965 2.456385612487793 0.23609337210655212
CurrentTrain: epoch  5, batch    61 | loss: 6.7323675Losses:  4.901444911956787 0.680508017539978 0.1576116383075714
CurrentTrain: epoch  5, batch    62 | loss: 4.9014449Losses:  6.5050368309021 2.189429998397827 0.23267148435115814
CurrentTrain: epoch  6, batch     0 | loss: 6.5050368Losses:  6.944005966186523 2.5502307415008545 0.23586469888687134
CurrentTrain: epoch  6, batch     1 | loss: 6.9440060Losses:  6.791502475738525 2.4007534980773926 0.23595425486564636
CurrentTrain: epoch  6, batch     2 | loss: 6.7915025Losses:  12.454108238220215 8.006903648376465 0.17299388349056244
CurrentTrain: epoch  6, batch     3 | loss: 12.4541082Losses:  5.794753074645996 1.4836198091506958 0.21595361828804016
CurrentTrain: epoch  6, batch     4 | loss: 5.7947531Losses:  7.358529090881348 3.0337777137756348 0.22695453464984894
CurrentTrain: epoch  6, batch     5 | loss: 7.3585291Losses:  6.438764572143555 2.0651955604553223 0.2318054437637329
CurrentTrain: epoch  6, batch     6 | loss: 6.4387646Losses:  8.104158401489258 3.7938501834869385 0.2442561239004135
CurrentTrain: epoch  6, batch     7 | loss: 8.1041584Losses:  6.6266961097717285 2.2883379459381104 0.22409483790397644
CurrentTrain: epoch  6, batch     8 | loss: 6.6266961Losses:  6.786628723144531 2.489772319793701 0.14220526814460754
CurrentTrain: epoch  6, batch     9 | loss: 6.7866287Losses:  8.740918159484863 4.370330810546875 0.24265673756599426
CurrentTrain: epoch  6, batch    10 | loss: 8.7409182Losses:  7.550987720489502 3.2068119049072266 0.24123989045619965
CurrentTrain: epoch  6, batch    11 | loss: 7.5509877Losses:  6.669772624969482 2.354523181915283 0.2302798181772232
CurrentTrain: epoch  6, batch    12 | loss: 6.6697726Losses:  8.014481544494629 3.278163433074951 0.2605929672718048
CurrentTrain: epoch  6, batch    13 | loss: 8.0144815Losses:  8.003547668457031 3.8307886123657227 0.16264724731445312
CurrentTrain: epoch  6, batch    14 | loss: 8.0035477Losses:  7.021951675415039 2.721513271331787 0.22976434230804443
CurrentTrain: epoch  6, batch    15 | loss: 7.0219517Losses:  8.502337455749512 4.182694911956787 0.24756161868572235
CurrentTrain: epoch  6, batch    16 | loss: 8.5023375Losses:  9.613954544067383 5.223399639129639 0.24041365087032318
CurrentTrain: epoch  6, batch    17 | loss: 9.6139545Losses:  5.927417278289795 1.6313031911849976 0.21766597032546997
CurrentTrain: epoch  6, batch    18 | loss: 5.9274173Losses:  6.9833903312683105 2.6170401573181152 0.23735298216342926
CurrentTrain: epoch  6, batch    19 | loss: 6.9833903Losses:  8.509150505065918 4.200385093688965 0.2385934740304947
CurrentTrain: epoch  6, batch    20 | loss: 8.5091505Losses:  7.974629878997803 3.713183879852295 0.23900996148586273
CurrentTrain: epoch  6, batch    21 | loss: 7.9746299Losses:  6.6319580078125 2.3476524353027344 0.23026683926582336
CurrentTrain: epoch  6, batch    22 | loss: 6.6319580Losses:  7.854223728179932 3.5414164066314697 0.2426932156085968
CurrentTrain: epoch  6, batch    23 | loss: 7.8542237Losses:  7.144134998321533 2.829970598220825 0.24446538090705872
CurrentTrain: epoch  6, batch    24 | loss: 7.1441350Losses:  7.183862686157227 2.948733329772949 0.21680346131324768
CurrentTrain: epoch  6, batch    25 | loss: 7.1838627Losses:  7.581691265106201 3.244263172149658 0.24083563685417175
CurrentTrain: epoch  6, batch    26 | loss: 7.5816913Losses:  7.91541051864624 3.6856679916381836 0.24981114268302917
CurrentTrain: epoch  6, batch    27 | loss: 7.9154105Losses:  8.644868850708008 4.367590427398682 0.23902711272239685
CurrentTrain: epoch  6, batch    28 | loss: 8.6448689Losses:  7.37038516998291 3.1036293506622314 0.24188879132270813
CurrentTrain: epoch  6, batch    29 | loss: 7.3703852Losses:  6.197454929351807 1.9275827407836914 0.21060127019882202
CurrentTrain: epoch  6, batch    30 | loss: 6.1974549Losses:  8.465415000915527 4.104582786560059 0.2384960949420929
CurrentTrain: epoch  6, batch    31 | loss: 8.4654150Losses:  8.35747241973877 4.045483589172363 0.23960743844509125
CurrentTrain: epoch  6, batch    32 | loss: 8.3574724Losses:  6.098737716674805 1.851374626159668 0.22400535643100739
CurrentTrain: epoch  6, batch    33 | loss: 6.0987377Losses:  5.806921482086182 1.505326271057129 0.21803000569343567
CurrentTrain: epoch  6, batch    34 | loss: 5.8069215Losses:  7.61834192276001 3.3162221908569336 0.21983136236667633
CurrentTrain: epoch  6, batch    35 | loss: 7.6183419Losses:  6.8518524169921875 2.587230682373047 0.22998705506324768
CurrentTrain: epoch  6, batch    36 | loss: 6.8518524Losses:  7.954134941101074 3.641364574432373 0.23778772354125977
CurrentTrain: epoch  6, batch    37 | loss: 7.9541349Losses:  6.738023281097412 2.546018123626709 0.2250114530324936
CurrentTrain: epoch  6, batch    38 | loss: 6.7380233Losses:  6.226423740386963 2.00688099861145 0.2223445475101471
CurrentTrain: epoch  6, batch    39 | loss: 6.2264237Losses:  8.196398735046387 3.979527473449707 0.24037353694438934
CurrentTrain: epoch  6, batch    40 | loss: 8.1963987Losses:  6.698581695556641 2.4170637130737305 0.2331029772758484
CurrentTrain: epoch  6, batch    41 | loss: 6.6985817Losses:  6.6004252433776855 2.340670108795166 0.23193463683128357
CurrentTrain: epoch  6, batch    42 | loss: 6.6004252Losses:  8.345328330993652 4.0417609214782715 0.24038448929786682
CurrentTrain: epoch  6, batch    43 | loss: 8.3453283Losses:  7.4745378494262695 3.1842947006225586 0.24246780574321747
CurrentTrain: epoch  6, batch    44 | loss: 7.4745378Losses:  8.749918937683105 4.465635776519775 0.2428685426712036
CurrentTrain: epoch  6, batch    45 | loss: 8.7499189Losses:  5.8666510581970215 1.62986159324646 0.21311146020889282
CurrentTrain: epoch  6, batch    46 | loss: 5.8666511Losses:  6.38701057434082 2.1474151611328125 0.23228785395622253
CurrentTrain: epoch  6, batch    47 | loss: 6.3870106Losses:  10.625807762145996 6.32728910446167 0.2387586534023285
CurrentTrain: epoch  6, batch    48 | loss: 10.6258078Losses:  6.674620628356934 2.462357521057129 0.15340106189250946
CurrentTrain: epoch  6, batch    49 | loss: 6.6746206Losses:  7.021177768707275 2.739741325378418 0.22552324831485748
CurrentTrain: epoch  6, batch    50 | loss: 7.0211778Losses:  7.932538986206055 3.635443687438965 0.25711095333099365
CurrentTrain: epoch  6, batch    51 | loss: 7.9325390Losses:  7.603244304656982 3.353896141052246 0.23629158735275269
CurrentTrain: epoch  6, batch    52 | loss: 7.6032443Losses:  6.242691993713379 1.952479600906372 0.22527670860290527
CurrentTrain: epoch  6, batch    53 | loss: 6.2426920Losses:  7.022620677947998 2.751394271850586 0.24370238184928894
CurrentTrain: epoch  6, batch    54 | loss: 7.0226207Losses:  5.819854736328125 1.5693738460540771 0.21164795756340027
CurrentTrain: epoch  6, batch    55 | loss: 5.8198547Losses:  7.358837127685547 3.130793809890747 0.23336413502693176
CurrentTrain: epoch  6, batch    56 | loss: 7.3588371Losses:  8.156608581542969 3.955697536468506 0.2323586493730545
CurrentTrain: epoch  6, batch    57 | loss: 8.1566086Losses:  6.466102123260498 2.1960997581481934 0.22926363348960876
CurrentTrain: epoch  6, batch    58 | loss: 6.4661021Losses:  7.520488739013672 3.2327921390533447 0.2317427396774292
CurrentTrain: epoch  6, batch    59 | loss: 7.5204887Losses:  7.955046653747559 3.7104222774505615 0.23165974020957947
CurrentTrain: epoch  6, batch    60 | loss: 7.9550467Losses:  8.288036346435547 4.066278457641602 0.2487945258617401
CurrentTrain: epoch  6, batch    61 | loss: 8.2880363Losses:  4.648781776428223 0.4771748483181 0.15791311860084534
CurrentTrain: epoch  6, batch    62 | loss: 4.6487818Losses:  6.883456707000732 2.658177614212036 0.223918616771698
CurrentTrain: epoch  7, batch     0 | loss: 6.8834567Losses:  8.697460174560547 4.460958957672119 0.24227944016456604
CurrentTrain: epoch  7, batch     1 | loss: 8.6974602Losses:  7.184324264526367 2.971163272857666 0.22238950431346893
CurrentTrain: epoch  7, batch     2 | loss: 7.1843243Losses:  5.823751926422119 1.6131162643432617 0.21077962219715118
CurrentTrain: epoch  7, batch     3 | loss: 5.8237519Losses:  7.19043493270874 2.75492525100708 0.21485552191734314
CurrentTrain: epoch  7, batch     4 | loss: 7.1904349Losses:  7.1576995849609375 2.9382786750793457 0.2234058380126953
CurrentTrain: epoch  7, batch     5 | loss: 7.1576996Losses:  6.424287796020508 2.1963744163513184 0.21060693264007568
CurrentTrain: epoch  7, batch     6 | loss: 6.4242878Losses:  7.992437839508057 3.792738199234009 0.16581223905086517
CurrentTrain: epoch  7, batch     7 | loss: 7.9924378Losses:  7.722955703735352 3.4538967609405518 0.23504149913787842
CurrentTrain: epoch  7, batch     8 | loss: 7.7229557Losses:  6.951716899871826 2.686224937438965 0.25461024045944214
CurrentTrain: epoch  7, batch     9 | loss: 6.9517169Losses:  6.756791591644287 2.5421104431152344 0.22378987073898315
CurrentTrain: epoch  7, batch    10 | loss: 6.7567916Losses:  9.063702583312988 4.871760368347168 0.24309852719306946
CurrentTrain: epoch  7, batch    11 | loss: 9.0637026Losses:  6.489439010620117 2.3022570610046387 0.2253997027873993
CurrentTrain: epoch  7, batch    12 | loss: 6.4894390Losses:  7.142999172210693 2.9001479148864746 0.22511956095695496
CurrentTrain: epoch  7, batch    13 | loss: 7.1429992Losses:  9.001704216003418 4.667324542999268 0.25387275218963623
CurrentTrain: epoch  7, batch    14 | loss: 9.0017042Losses:  10.781341552734375 6.486728668212891 0.24133527278900146
CurrentTrain: epoch  7, batch    15 | loss: 10.7813416Losses:  7.493870735168457 3.2841649055480957 0.2333945333957672
CurrentTrain: epoch  7, batch    16 | loss: 7.4938707Losses:  6.467489242553711 2.1941585540771484 0.22563153505325317
CurrentTrain: epoch  7, batch    17 | loss: 6.4674892Losses:  6.018350124359131 1.815625548362732 0.21885456144809723
CurrentTrain: epoch  7, batch    18 | loss: 6.0183501Losses:  6.5836567878723145 2.355630874633789 0.23246526718139648
CurrentTrain: epoch  7, batch    19 | loss: 6.5836568Losses:  7.0045671463012695 2.805142879486084 0.22133192420005798
CurrentTrain: epoch  7, batch    20 | loss: 7.0045671Losses:  6.474847316741943 1.88564932346344 0.22503554821014404
CurrentTrain: epoch  7, batch    21 | loss: 6.4748473Losses:  6.663773059844971 2.435279130935669 0.22383081912994385
CurrentTrain: epoch  7, batch    22 | loss: 6.6637731Losses:  9.76584243774414 5.463785171508789 0.24914196133613586
CurrentTrain: epoch  7, batch    23 | loss: 9.7658424Losses:  6.6233015060424805 2.3467416763305664 0.23098330199718475
CurrentTrain: epoch  7, batch    24 | loss: 6.6233015Losses:  7.097095012664795 2.8085577487945557 0.23460529744625092
CurrentTrain: epoch  7, batch    25 | loss: 7.0970950Losses:  11.08952522277832 6.780733108520508 0.24747154116630554
CurrentTrain: epoch  7, batch    26 | loss: 11.0895252Losses:  6.796341896057129 2.245403289794922 0.21389026939868927
CurrentTrain: epoch  7, batch    27 | loss: 6.7963419Losses:  6.9275360107421875 2.6736326217651367 0.22402408719062805
CurrentTrain: epoch  7, batch    28 | loss: 6.9275360Losses:  5.502657413482666 1.2539937496185303 0.21131183207035065
CurrentTrain: epoch  7, batch    29 | loss: 5.5026574Losses:  7.070413589477539 2.7851858139038086 0.22561557590961456
CurrentTrain: epoch  7, batch    30 | loss: 7.0704136Losses:  7.35273551940918 3.112739324569702 0.23152080178260803
CurrentTrain: epoch  7, batch    31 | loss: 7.3527355Losses:  8.0134916305542 3.773084878921509 0.1635953187942505
CurrentTrain: epoch  7, batch    32 | loss: 8.0134916Losses:  6.3232102394104 2.0607872009277344 0.2284005880355835
CurrentTrain: epoch  7, batch    33 | loss: 6.3232102Losses:  6.996668338775635 2.714116096496582 0.2221253216266632
CurrentTrain: epoch  7, batch    34 | loss: 6.9966683Losses:  10.4869384765625 6.213906288146973 0.23460979759693146
CurrentTrain: epoch  7, batch    35 | loss: 10.4869385Losses:  6.96257209777832 2.8035573959350586 0.13622525334358215
CurrentTrain: epoch  7, batch    36 | loss: 6.9625721Losses:  10.306695938110352 6.057249069213867 0.24884435534477234
CurrentTrain: epoch  7, batch    37 | loss: 10.3066959Losses:  9.337634086608887 5.14415979385376 0.2374807745218277
CurrentTrain: epoch  7, batch    38 | loss: 9.3376341Losses:  11.317773818969727 7.0851640701293945 0.24768532812595367
CurrentTrain: epoch  7, batch    39 | loss: 11.3177738Losses:  7.971498012542725 3.7254066467285156 0.22564664483070374
CurrentTrain: epoch  7, batch    40 | loss: 7.9714980Losses:  7.028123378753662 2.8080010414123535 0.22275683283805847
CurrentTrain: epoch  7, batch    41 | loss: 7.0281234Losses:  6.068206310272217 1.8421671390533447 0.21748226881027222
CurrentTrain: epoch  7, batch    42 | loss: 6.0682063Losses:  10.604652404785156 6.4582133293151855 0.15506917238235474
CurrentTrain: epoch  7, batch    43 | loss: 10.6046524Losses:  6.93259859085083 2.6978201866149902 0.2296161651611328
CurrentTrain: epoch  7, batch    44 | loss: 6.9325986Losses:  8.96697998046875 4.7221198081970215 0.23368999361991882
CurrentTrain: epoch  7, batch    45 | loss: 8.9669800Losses:  6.687793254852295 2.41848087310791 0.2151390165090561
CurrentTrain: epoch  7, batch    46 | loss: 6.6877933Losses:  5.864552021026611 1.6210166215896606 0.21365410089492798
CurrentTrain: epoch  7, batch    47 | loss: 5.8645520Losses:  6.978339672088623 2.7321524620056152 0.24022254347801208
CurrentTrain: epoch  7, batch    48 | loss: 6.9783397Losses:  8.916659355163574 4.660768508911133 0.23543843626976013
CurrentTrain: epoch  7, batch    49 | loss: 8.9166594Losses:  6.907763957977295 2.6394450664520264 0.23696833848953247
CurrentTrain: epoch  7, batch    50 | loss: 6.9077640Losses:  7.461637020111084 3.169650077819824 0.23260146379470825
CurrentTrain: epoch  7, batch    51 | loss: 7.4616370Losses:  7.331676006317139 2.4683287143707275 0.23472824692726135
CurrentTrain: epoch  7, batch    52 | loss: 7.3316760Losses:  7.874917984008789 3.7035810947418213 0.13906025886535645
CurrentTrain: epoch  7, batch    53 | loss: 7.8749180Losses:  6.069171905517578 1.791563630104065 0.22201497852802277
CurrentTrain: epoch  7, batch    54 | loss: 6.0691719Losses:  6.165799617767334 1.958137035369873 0.2103489637374878
CurrentTrain: epoch  7, batch    55 | loss: 6.1657996Losses:  6.528508186340332 2.3131179809570312 0.22899775207042694
CurrentTrain: epoch  7, batch    56 | loss: 6.5285082Losses:  7.398651599884033 3.200958013534546 0.22569085657596588
CurrentTrain: epoch  7, batch    57 | loss: 7.3986516Losses:  7.281367301940918 3.002999782562256 0.23041796684265137
CurrentTrain: epoch  7, batch    58 | loss: 7.2813673Losses:  6.059952735900879 1.8307273387908936 0.22637450695037842
CurrentTrain: epoch  7, batch    59 | loss: 6.0599527Losses:  6.59826135635376 2.375035047531128 0.23514136672019958
CurrentTrain: epoch  7, batch    60 | loss: 6.5982614Losses:  8.794824600219727 4.575737953186035 0.2377675473690033
CurrentTrain: epoch  7, batch    61 | loss: 8.7948246Losses:  9.813175201416016 5.855111122131348 0.09562128782272339
CurrentTrain: epoch  7, batch    62 | loss: 9.8131752Losses:  6.605568885803223 2.3324029445648193 0.22923265397548676
CurrentTrain: epoch  8, batch     0 | loss: 6.6055689Losses:  7.328990459442139 3.116888999938965 0.2354198396205902
CurrentTrain: epoch  8, batch     1 | loss: 7.3289905Losses:  5.785830497741699 1.5641765594482422 0.21106329560279846
CurrentTrain: epoch  8, batch     2 | loss: 5.7858305Losses:  5.995153903961182 1.6343778371810913 0.2120800018310547
CurrentTrain: epoch  8, batch     3 | loss: 5.9951539Losses:  7.693339824676514 3.4211881160736084 0.24953432381153107
CurrentTrain: epoch  8, batch     4 | loss: 7.6933398Losses:  8.437504768371582 4.165096282958984 0.23512350022792816
CurrentTrain: epoch  8, batch     5 | loss: 8.4375048Losses:  5.598187446594238 1.2867541313171387 0.20765431225299835
CurrentTrain: epoch  8, batch     6 | loss: 5.5981874Losses:  8.054134368896484 3.619755268096924 0.2379259318113327
CurrentTrain: epoch  8, batch     7 | loss: 8.0541344Losses:  5.666478633880615 1.4539852142333984 0.21639713644981384
CurrentTrain: epoch  8, batch     8 | loss: 5.6664786Losses:  7.888522148132324 3.730475902557373 0.15498653054237366
CurrentTrain: epoch  8, batch     9 | loss: 7.8885221Losses:  5.731139183044434 1.4875339269638062 0.22189509868621826
CurrentTrain: epoch  8, batch    10 | loss: 5.7311392Losses:  6.63004207611084 2.379987955093384 0.23918843269348145
CurrentTrain: epoch  8, batch    11 | loss: 6.6300421Losses:  8.598388671875 4.337040901184082 0.23360784351825714
CurrentTrain: epoch  8, batch    12 | loss: 8.5983887Losses:  6.982699871063232 2.7358615398406982 0.23006215691566467
CurrentTrain: epoch  8, batch    13 | loss: 6.9826999Losses:  9.497058868408203 5.284712791442871 0.2353188693523407
CurrentTrain: epoch  8, batch    14 | loss: 9.4970589Losses:  7.043455600738525 2.7558999061584473 0.23380276560783386
CurrentTrain: epoch  8, batch    15 | loss: 7.0434556Losses:  6.389371871948242 2.1706156730651855 0.218613862991333
CurrentTrain: epoch  8, batch    16 | loss: 6.3893719Losses:  6.002537250518799 1.8081576824188232 0.21594759821891785
CurrentTrain: epoch  8, batch    17 | loss: 6.0025373Losses:  7.9355597496032715 3.6340184211730957 0.2503148913383484
CurrentTrain: epoch  8, batch    18 | loss: 7.9355597Losses:  8.820642471313477 4.546871185302734 0.1707310676574707
CurrentTrain: epoch  8, batch    19 | loss: 8.8206425Losses:  8.360381126403809 4.068850517272949 0.24316498637199402
CurrentTrain: epoch  8, batch    20 | loss: 8.3603811Losses:  6.057102203369141 1.8238154649734497 0.22159716486930847
CurrentTrain: epoch  8, batch    21 | loss: 6.0571022Losses:  7.1433210372924805 2.9005720615386963 0.22816191613674164
CurrentTrain: epoch  8, batch    22 | loss: 7.1433210Losses:  7.717827796936035 3.4748096466064453 0.23453807830810547
CurrentTrain: epoch  8, batch    23 | loss: 7.7178278Losses:  6.7580885887146 2.537548542022705 0.2192213088274002
CurrentTrain: epoch  8, batch    24 | loss: 6.7580886Losses:  6.37190055847168 2.1336748600006104 0.2286711186170578
CurrentTrain: epoch  8, batch    25 | loss: 6.3719006Losses:  5.7681403160095215 1.561826467514038 0.20831428468227386
CurrentTrain: epoch  8, batch    26 | loss: 5.7681403Losses:  6.41586446762085 2.1902308464050293 0.23234233260154724
CurrentTrain: epoch  8, batch    27 | loss: 6.4158645Losses:  6.208187580108643 1.9862405061721802 0.22901321947574615
CurrentTrain: epoch  8, batch    28 | loss: 6.2081876Losses:  9.159226417541504 4.956699848175049 0.2510119676589966
CurrentTrain: epoch  8, batch    29 | loss: 9.1592264Losses:  6.674688816070557 2.481914520263672 0.22989457845687866
CurrentTrain: epoch  8, batch    30 | loss: 6.6746888Losses:  7.538086414337158 2.972766399383545 0.2287338227033615
CurrentTrain: epoch  8, batch    31 | loss: 7.5380864Losses:  7.799343585968018 3.584181785583496 0.23058989644050598
CurrentTrain: epoch  8, batch    32 | loss: 7.7993436Losses:  7.003104209899902 2.7556588649749756 0.23732778429985046
CurrentTrain: epoch  8, batch    33 | loss: 7.0031042Losses:  6.356148719787598 2.1690869331359863 0.22342020273208618
CurrentTrain: epoch  8, batch    34 | loss: 6.3561487Losses:  7.152763366699219 2.8963241577148438 0.2317385971546173
CurrentTrain: epoch  8, batch    35 | loss: 7.1527634Losses:  8.588356018066406 4.373718738555908 0.24134260416030884
CurrentTrain: epoch  8, batch    36 | loss: 8.5883560Losses:  7.168021202087402 2.91326904296875 0.23627112805843353
CurrentTrain: epoch  8, batch    37 | loss: 7.1680212Losses:  7.812844276428223 3.5694403648376465 0.23538385331630707
CurrentTrain: epoch  8, batch    38 | loss: 7.8128443Losses:  6.497027397155762 2.2446608543395996 0.21921128034591675
CurrentTrain: epoch  8, batch    39 | loss: 6.4970274Losses:  7.05288028717041 2.6994872093200684 0.21961086988449097
CurrentTrain: epoch  8, batch    40 | loss: 7.0528803Losses:  6.569718360900879 2.139399528503418 0.2249314934015274
CurrentTrain: epoch  8, batch    41 | loss: 6.5697184Losses:  8.698850631713867 4.480568885803223 0.1688147783279419
CurrentTrain: epoch  8, batch    42 | loss: 8.6988506Losses:  7.5398268699646 3.271178722381592 0.24303431808948517
CurrentTrain: epoch  8, batch    43 | loss: 7.5398269Losses:  7.419577121734619 3.1741466522216797 0.22602063417434692
CurrentTrain: epoch  8, batch    44 | loss: 7.4195771Losses:  8.989336967468262 4.741779327392578 0.24841175973415375
CurrentTrain: epoch  8, batch    45 | loss: 8.9893370Losses:  7.081558704376221 2.8514771461486816 0.21971459686756134
CurrentTrain: epoch  8, batch    46 | loss: 7.0815587Losses:  8.161802291870117 3.880587339401245 0.23338893055915833
CurrentTrain: epoch  8, batch    47 | loss: 8.1618023Losses:  8.908153533935547 4.583439826965332 0.2575616240501404
CurrentTrain: epoch  8, batch    48 | loss: 8.9081535Losses:  7.025645732879639 2.7630200386047363 0.23196211457252502
CurrentTrain: epoch  8, batch    49 | loss: 7.0256457Losses:  5.761621475219727 1.5816031694412231 0.20962870121002197
CurrentTrain: epoch  8, batch    50 | loss: 5.7616215Losses:  7.172140121459961 2.951709270477295 0.2204826921224594
CurrentTrain: epoch  8, batch    51 | loss: 7.1721401Losses:  6.5554046630859375 2.4127633571624756 0.1406104564666748
CurrentTrain: epoch  8, batch    52 | loss: 6.5554047Losses:  7.295464038848877 3.0770511627197266 0.22829343378543854
CurrentTrain: epoch  8, batch    53 | loss: 7.2954640Losses:  8.413718223571777 4.189447402954102 0.2320856750011444
CurrentTrain: epoch  8, batch    54 | loss: 8.4137182Losses:  10.88675594329834 6.83012580871582 0.1497640758752823
CurrentTrain: epoch  8, batch    55 | loss: 10.8867559Losses:  6.397438049316406 2.1576414108276367 0.22327226400375366
CurrentTrain: epoch  8, batch    56 | loss: 6.3974380Losses:  7.109627723693848 2.861532211303711 0.23551519215106964
CurrentTrain: epoch  8, batch    57 | loss: 7.1096277Losses:  7.77274751663208 3.529676914215088 0.23057501018047333
CurrentTrain: epoch  8, batch    58 | loss: 7.7727475Losses:  8.122846603393555 3.8881025314331055 0.2571542263031006
CurrentTrain: epoch  8, batch    59 | loss: 8.1228466Losses:  6.765373229980469 2.5497725009918213 0.23745524883270264
CurrentTrain: epoch  8, batch    60 | loss: 6.7653732Losses:  6.9722161293029785 2.7596306800842285 0.2327929139137268
CurrentTrain: epoch  8, batch    61 | loss: 6.9722161Losses:  4.669106960296631 0.48087558150291443 0.24622979760169983
CurrentTrain: epoch  8, batch    62 | loss: 4.6691070Losses:  10.017017364501953 5.838171005249023 0.15830889344215393
CurrentTrain: epoch  9, batch     0 | loss: 10.0170174Losses:  7.346595764160156 3.1656267642974854 0.23528379201889038
CurrentTrain: epoch  9, batch     1 | loss: 7.3465958Losses:  6.726783752441406 2.5066335201263428 0.2260608971118927
CurrentTrain: epoch  9, batch     2 | loss: 6.7267838Losses:  8.193022727966309 3.887028455734253 0.24089008569717407
CurrentTrain: epoch  9, batch     3 | loss: 8.1930227Losses:  7.9263739585876465 3.710512638092041 0.23733873665332794
CurrentTrain: epoch  9, batch     4 | loss: 7.9263740Losses:  5.955623149871826 1.7778571844100952 0.21804898977279663
CurrentTrain: epoch  9, batch     5 | loss: 5.9556231Losses:  6.837458610534668 2.6498725414276123 0.22278445959091187
CurrentTrain: epoch  9, batch     6 | loss: 6.8374586Losses:  7.188077926635742 2.9084115028381348 0.24348293244838715
CurrentTrain: epoch  9, batch     7 | loss: 7.1880779Losses:  7.057811737060547 2.8660926818847656 0.21891097724437714
CurrentTrain: epoch  9, batch     8 | loss: 7.0578117Losses:  7.907601833343506 3.6662113666534424 0.24943767488002777
CurrentTrain: epoch  9, batch     9 | loss: 7.9076018Losses:  7.705474376678467 3.4371275901794434 0.23359271883964539
CurrentTrain: epoch  9, batch    10 | loss: 7.7054744Losses:  6.624823093414307 2.4134631156921387 0.22349518537521362
CurrentTrain: epoch  9, batch    11 | loss: 6.6248231Losses:  7.358165740966797 3.1192893981933594 0.24744205176830292
CurrentTrain: epoch  9, batch    12 | loss: 7.3581657Losses:  6.184093475341797 1.968567132949829 0.22680774331092834
CurrentTrain: epoch  9, batch    13 | loss: 6.1840935Losses:  9.793679237365723 5.502608299255371 0.2602371573448181
CurrentTrain: epoch  9, batch    14 | loss: 9.7936792Losses:  7.402842044830322 3.1274571418762207 0.23332901298999786
CurrentTrain: epoch  9, batch    15 | loss: 7.4028420Losses:  6.030186653137207 1.7771704196929932 0.21871954202651978
CurrentTrain: epoch  9, batch    16 | loss: 6.0301867Losses:  7.825573921203613 3.603883743286133 0.23539471626281738
CurrentTrain: epoch  9, batch    17 | loss: 7.8255739Losses:  10.088802337646484 5.884826183319092 0.24436844885349274
CurrentTrain: epoch  9, batch    18 | loss: 10.0888023Losses:  6.575196266174316 2.350644588470459 0.22599603235721588
CurrentTrain: epoch  9, batch    19 | loss: 6.5751963Losses:  5.983052730560303 1.7893706560134888 0.2206384837627411
CurrentTrain: epoch  9, batch    20 | loss: 5.9830527Losses:  7.329601287841797 3.111807107925415 0.23800018429756165
CurrentTrain: epoch  9, batch    21 | loss: 7.3296013Losses:  5.9657135009765625 1.7668633460998535 0.21780943870544434
CurrentTrain: epoch  9, batch    22 | loss: 5.9657135Losses:  6.562963485717773 2.3183023929595947 0.2271701693534851
CurrentTrain: epoch  9, batch    23 | loss: 6.5629635Losses:  7.688028812408447 3.4462103843688965 0.23280423879623413
CurrentTrain: epoch  9, batch    24 | loss: 7.6880288Losses:  9.678993225097656 5.418669700622559 0.2620367407798767
CurrentTrain: epoch  9, batch    25 | loss: 9.6789932Losses:  6.285019397735596 2.080353260040283 0.2224726527929306
CurrentTrain: epoch  9, batch    26 | loss: 6.2850194Losses:  6.487376689910889 2.320465087890625 0.22582180798053741
CurrentTrain: epoch  9, batch    27 | loss: 6.4873767Losses:  9.152054786682129 4.907330513000488 0.24573203921318054
CurrentTrain: epoch  9, batch    28 | loss: 9.1520548Losses:  7.395615577697754 3.2241148948669434 0.2255685031414032
CurrentTrain: epoch  9, batch    29 | loss: 7.3956156Losses:  6.171664237976074 1.9596242904663086 0.20774464309215546
CurrentTrain: epoch  9, batch    30 | loss: 6.1716642Losses:  7.8335137367248535 3.6523733139038086 0.23306486010551453
CurrentTrain: epoch  9, batch    31 | loss: 7.8335137Losses:  9.968255996704102 5.77546501159668 0.24651557207107544
CurrentTrain: epoch  9, batch    32 | loss: 9.9682560Losses:  8.527706146240234 4.320428848266602 0.22876518964767456
CurrentTrain: epoch  9, batch    33 | loss: 8.5277061Losses:  6.934178352355957 2.714643955230713 0.23038259148597717
CurrentTrain: epoch  9, batch    34 | loss: 6.9341784Losses:  6.718055725097656 2.568950891494751 0.21922864019870758
CurrentTrain: epoch  9, batch    35 | loss: 6.7180557Losses:  6.369329929351807 2.1689188480377197 0.22312134504318237
CurrentTrain: epoch  9, batch    36 | loss: 6.3693299Losses:  5.3718366622924805 1.1889338493347168 0.20861603319644928
CurrentTrain: epoch  9, batch    37 | loss: 5.3718367Losses:  6.490527629852295 2.3211939334869385 0.21261972188949585
CurrentTrain: epoch  9, batch    38 | loss: 6.4905276Losses:  8.573718070983887 4.373222351074219 0.23594282567501068
CurrentTrain: epoch  9, batch    39 | loss: 8.5737181Losses:  8.756988525390625 4.6769843101501465 0.15763169527053833
CurrentTrain: epoch  9, batch    40 | loss: 8.7569885Losses:  7.035987377166748 2.867663860321045 0.21771439909934998
CurrentTrain: epoch  9, batch    41 | loss: 7.0359874Losses:  7.5342698097229 3.384946823120117 0.22620263695716858
CurrentTrain: epoch  9, batch    42 | loss: 7.5342698Losses:  6.583181381225586 2.3575246334075928 0.22346198558807373
CurrentTrain: epoch  9, batch    43 | loss: 6.5831814Losses:  6.517786502838135 2.2829489707946777 0.23252759873867035
CurrentTrain: epoch  9, batch    44 | loss: 6.5177865Losses:  7.126321792602539 2.8955283164978027 0.22535082697868347
CurrentTrain: epoch  9, batch    45 | loss: 7.1263218Losses:  6.337244987487793 2.1552772521972656 0.21331200003623962
CurrentTrain: epoch  9, batch    46 | loss: 6.3372450Losses:  7.205491542816162 3.098544120788574 0.14319881796836853
CurrentTrain: epoch  9, batch    47 | loss: 7.2054915Losses:  7.531766891479492 3.348292589187622 0.2312108874320984
CurrentTrain: epoch  9, batch    48 | loss: 7.5317669Losses:  7.477248668670654 3.227336883544922 0.23114734888076782
CurrentTrain: epoch  9, batch    49 | loss: 7.4772487Losses:  5.955050468444824 1.7728257179260254 0.22074861824512482
CurrentTrain: epoch  9, batch    50 | loss: 5.9550505Losses:  10.956254005432129 6.7629923820495605 0.248867005109787
CurrentTrain: epoch  9, batch    51 | loss: 10.9562540Losses:  7.531605243682861 3.202298402786255 0.23718325793743134
CurrentTrain: epoch  9, batch    52 | loss: 7.5316052Losses:  7.074962139129639 2.879789352416992 0.23769544064998627
CurrentTrain: epoch  9, batch    53 | loss: 7.0749621Losses:  8.156573295593262 3.954016923904419 0.2551068663597107
CurrentTrain: epoch  9, batch    54 | loss: 8.1565733Losses:  7.274209976196289 3.050654411315918 0.22183609008789062
CurrentTrain: epoch  9, batch    55 | loss: 7.2742100Losses:  9.115488052368164 4.882245063781738 0.24174556136131287
CurrentTrain: epoch  9, batch    56 | loss: 9.1154881Losses:  6.901992321014404 2.6987149715423584 0.2343374341726303
CurrentTrain: epoch  9, batch    57 | loss: 6.9019923Losses:  5.993423938751221 1.8053867816925049 0.21939605474472046
CurrentTrain: epoch  9, batch    58 | loss: 5.9934239Losses:  7.359041690826416 3.1752264499664307 0.2269575148820877
CurrentTrain: epoch  9, batch    59 | loss: 7.3590417Losses:  7.571120738983154 3.35979962348938 0.24332167208194733
CurrentTrain: epoch  9, batch    60 | loss: 7.5711207Losses:  6.193826675415039 1.9801795482635498 0.22687718272209167
CurrentTrain: epoch  9, batch    61 | loss: 6.1938267Losses:  4.154721260070801 -0.0 0.22728312015533447
CurrentTrain: epoch  9, batch    62 | loss: 4.1547213
#### movement ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "movement" means literary, artistic, scientific or philosophical movement or scene associated with this person or work.
Here is an example:
Relation: movement
Context: born in copenhagen , as a young man bang became interested in surrealism as practised by giorgio de chirico , max ernst and yves tanguy .
Head Entity: yves tanguy
Tail Entity: surrealism
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "movement":
Relation: movement  
Context: during the early 20th century, the artist was heavily influenced by the cubist movement, which was pioneered by pablo picasso and georges braque.  
Head Entity: pablo picasso  
Tail Entity: cubist movement  

Relation: movement  
Context: the philosopher's ideas were deeply rooted in the existentialist movement, which gained prominence in the mid-20th century through the works of thinkers like jean-paul sartre.  
Head Entity: jean-paul sartre  
Tail Entity: existentialist movement  
#### located in the administrative territorial entity ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "located in the administrative territorial entity" means the item is located on the territory of the following administrative entity.
Here is an example:
Relation: located in the administrative territorial entity
Context: lyons hill is a restored village , and former parish with church , now part of the community of ardclough in north county kildare .
Head Entity: ardclough
Tail Entity: county kildare
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "located in the administrative territorial entity":
Relation: located in the administrative territorial entity  
Context: The city of San Francisco is known for its iconic Golden Gate Bridge and is part of the larger San Francisco County in California.  
Head Entity: San Francisco  
Tail Entity: San Francisco County  

Relation: located in the administrative territorial entity  
Context: The historic town of Bath is famous for its Roman baths and is situated within the unitary authority of Bath and North East Somerset in England.  
Head Entity: Bath  
Tail Entity: Bath and North East Somerset
#### head of government ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "head of government" means head of the executive power of this town, city, municipality, state, country, or other governmental body.
Here is an example:
Relation: head of government
Context: in 2004 , the mayor of bremen , henning scherf , repeated this act when the concert " disenchanted amazon " was presented in the bremen municipal government palace .
Head Entity: bremen
Tail Entity: henning scherf
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "head of government":
Relation: head of government  
Context: In 2021, the Prime Minister of Canada, Justin Trudeau, announced new policies during a press conference held in Ottawa.  
Head Entity: Canada  
Tail Entity: Justin Trudeau  

Relation: head of government  
Context: During the annual summit, the President of France, Emmanuel Macron, addressed the nation from the Élysée Palace in Paris.  
Head Entity: France  
Tail Entity: Emmanuel Macron  
#### member of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "member of" means organization, club or musical group to which the subject belongs.
Here is an example:
Relation: member of
Context: felim egan is a member of aosdána .
Head Entity: felim egan
Tail Entity: aosdána
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "member of":
Relation: member of  
Context: the band coldplay is a member of the british rock music scene.  
Head Entity: coldplay  
Tail Entity: british rock music scene  

Relation: member of  
Context: she has been a member of the national honor society since her sophomore year.  
Head Entity: she  
Tail Entity: national honor society  
#### crosses ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "crosses" means obstacle which this bridge crosses over or this tunnel goes under.
Here is an example:
Relation: crosses
Context: on july 2 , 1935 , the rip van winkle bridge over the hudson river between catskill and greenport was opened to traffic .
Head Entity: rip van winkle bridge
Tail Entity: hudson river
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "crosses":
Relation: crosses  
Context: The Golden Gate Bridge spans the Golden Gate Strait, connecting San Francisco to Marin County, and is a vital link for traffic crossing over the water.  
Head Entity: Golden Gate Bridge  
Tail Entity: Golden Gate Strait  

Relation: crosses  
Context: The Channel Tunnel, also known as the Chunnel, runs beneath the English Channel, providing a rail link between the UK and mainland Europe.  
Head Entity: Channel Tunnel  
Tail Entity: English Channel  
#### tributary ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "tributary" means watercourse that flows into an other one.
Here is an example:
Relation: tributary
Context: the municipality contains the lower reaches of the maracanã river , which is joined by the caripi river before widening into maracanã bay and emptying into the atlantic ocean beside maiandeua island .
Head Entity: maracanã river
Tail Entity: caripi river
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "tributary":
Relation: tributary  
Context: The Amazon River, one of the largest rivers in the world, receives numerous tributaries, including the Madeira River, which significantly contributes to its flow.  
Head Entity: Amazon River  
Tail Entity: Madeira River  

Relation: tributary  
Context: The Mississippi River is fed by many smaller rivers, with the Missouri River being one of its major tributaries that enhances its water volume.  
Head Entity: Mississippi River  
Tail Entity: Missouri River  
#### position held ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "position held" means subject currently or formerly holds the object position or public office.
Here is an example:
Relation: position held
Context: his father robert docking and grandfather george docking both served as governor of kansas .
Head Entity: george docking
Tail Entity: governor of kansas
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "position held":
Relation: position held  
Context: After a successful career in politics, Sarah Thompson was appointed as the Secretary of State for New York.  
Head Entity: Sarah Thompson  
Tail Entity: Secretary of State for New York  

Relation: position held  
Context: During his tenure, Michael Johnson was the mayor of Chicago for over a decade, implementing numerous reforms.  
Head Entity: Michael Johnson  
Tail Entity: mayor of Chicago  
#### developer ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "developer" means organization or person that developed the item.
Here is an example:
Relation: developer
Context: " jedi outcast " was developed by raven software and powered by the i d tech 3 game engine .
Head Entity: jedi outcast
Tail Entity: raven software
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "developer":
Relation: developer  
Context: "The Witcher 3: Wild Hunt" was developed by CD Projekt Red and has received numerous awards for its storytelling and gameplay.  
Head Entity: The Witcher 3: Wild Hunt  
Tail Entity: CD Projekt Red  

Relation: developer  
Context: "Minecraft" was developed by Mojang Studios and has become one of the best-selling video games of all time.  
Head Entity: Minecraft  
Tail Entity: Mojang Studios  
#### location of formation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "location of formation" means location where a group or organization was formed.
Here is an example:
Relation: location of formation
Context: in 2008 , the montreal - based band simple plan featured koivu in its video for the song " save you . "
Head Entity: simple plan
Tail Entity: montreal
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "location of formation":
Relation: location of formation  
Context: The famous tech company was established in a small garage in Cupertino, California, where it began its journey to revolutionize the industry.  
Head Entity: Apple Inc.  
Tail Entity: Cupertino  

Relation: location of formation  
Context: The renowned soccer club was founded in 1899 in the city of Munich, where it quickly gained popularity and success in the sport.  
Head Entity: Bayern Munich  
Tail Entity: Munich  
#### country of origin ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "country of origin" means country of origin of this item (creative work, food, phrase, product, etc.).
Here is an example:
Relation: country of origin
Context: " shining time station " won a number of awards and significantly increased the popularity of the " thomas " media franchise in the united states .
Head Entity: shining time station
Tail Entity: united states
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "country of origin":
Relation: country of origin  
Context: The famous dish "sushi" is traditionally associated with Japan and has gained immense popularity worldwide.  
Head Entity: sushi  
Tail Entity: Japan  

Relation: country of origin  
Context: The iconic brand "Guinness" is known for its rich stout beer, which originated in Ireland and is now enjoyed globally.  
Head Entity: Guinness  
Tail Entity: Ireland  
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 88.54%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 89.29%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 89.84%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 90.97%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 91.25%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 92.05%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 92.71%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 92.31%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 92.41%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 92.50%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 92.97%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 93.38%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 93.40%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 94.05%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 94.03%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 94.29%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 94.27%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 94.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 94.23%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 94.21%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 94.20%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 94.18%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 94.38%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 94.56%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 94.73%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 94.89%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 95.04%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 95.00%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 95.14%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 95.27%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 95.23%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 95.35%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 95.47%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 95.58%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 95.68%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 95.78%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 95.88%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 95.97%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 96.06%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 96.01%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 96.09%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 96.17%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 96.25%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 96.20%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 96.15%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 96.11%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 96.18%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 96.02%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 95.98%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 95.94%   [EVAL] batch:   57 | acc: 81.25%,  total acc: 95.69%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 95.76%   [EVAL] batch:   59 | acc: 87.50%,  total acc: 95.62%   [EVAL] batch:   60 | acc: 93.75%,  total acc: 95.59%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 95.56%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 94.84%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 88.54%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 89.29%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 89.84%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 90.97%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 91.25%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 92.05%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 92.71%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 92.31%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 92.41%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 92.50%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 92.97%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 93.38%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 93.40%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 94.05%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 94.03%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 94.29%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 94.27%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 94.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 94.23%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 94.21%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 94.20%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 94.18%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 94.38%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 94.56%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 94.73%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 94.89%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 95.04%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 95.00%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 95.14%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 95.27%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 95.23%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 95.35%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 95.47%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 95.58%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 95.68%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 95.78%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 95.88%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 95.97%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 96.06%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 96.01%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 96.09%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 96.17%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 96.25%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 96.20%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 96.15%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 96.11%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 96.18%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 96.02%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 95.98%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 95.94%   [EVAL] batch:   57 | acc: 81.25%,  total acc: 95.69%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 95.76%   [EVAL] batch:   59 | acc: 87.50%,  total acc: 95.62%   [EVAL] batch:   60 | acc: 93.75%,  total acc: 95.59%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 95.56%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 94.84%   
cur_acc:  ['0.9484']
his_acc:  ['0.9484']
Clustering into  9  clusters
Clusters:  [1 2 4 7 0 0 4 3 2 2 1 5 6 8 6 0 3 1 3 7]
Losses:  13.427550315856934 4.223880767822266 0.752287745475769
CurrentTrain: epoch  0, batch     0 | loss: 13.4275503Losses:  14.079360961914062 3.9529013633728027 0.7454336881637573
CurrentTrain: epoch  0, batch     1 | loss: 14.0793610Losses:  13.239093780517578 4.002321720123291 0.5870718955993652
CurrentTrain: epoch  0, batch     2 | loss: 13.2390938Losses:  6.663928985595703 -0.0 -0.0
CurrentTrain: epoch  0, batch     3 | loss: 6.6639290Losses:  11.73586368560791 3.420438289642334 0.680175244808197
CurrentTrain: epoch  1, batch     0 | loss: 11.7358637Losses:  12.231497764587402 3.30802583694458 0.7797614932060242
CurrentTrain: epoch  1, batch     1 | loss: 12.2314978Losses:  12.428447723388672 3.6872951984405518 0.6996355652809143
CurrentTrain: epoch  1, batch     2 | loss: 12.4284477Losses:  8.755961418151855 -0.0 0.2710829973220825
CurrentTrain: epoch  1, batch     3 | loss: 8.7559614Losses:  11.933990478515625 3.593069553375244 0.6192507743835449
CurrentTrain: epoch  2, batch     0 | loss: 11.9339905Losses:  10.00794506072998 2.8518295288085938 0.657631516456604
CurrentTrain: epoch  2, batch     1 | loss: 10.0079451Losses:  9.972208023071289 2.6666364669799805 0.6481754779815674
CurrentTrain: epoch  2, batch     2 | loss: 9.9722080Losses:  9.020078659057617 -0.0 0.1801607310771942
CurrentTrain: epoch  2, batch     3 | loss: 9.0200787Losses:  11.458962440490723 4.071927547454834 0.5825601816177368
CurrentTrain: epoch  3, batch     0 | loss: 11.4589624Losses:  11.184627532958984 3.3054986000061035 0.6980854272842407
CurrentTrain: epoch  3, batch     1 | loss: 11.1846275Losses:  8.740633964538574 2.9587111473083496 0.7253252267837524
CurrentTrain: epoch  3, batch     2 | loss: 8.7406340Losses:  5.431654453277588 -0.0 0.14663821458816528
CurrentTrain: epoch  3, batch     3 | loss: 5.4316545Losses:  9.91761589050293 3.4548192024230957 0.6370311975479126
CurrentTrain: epoch  4, batch     0 | loss: 9.9176159Losses:  13.375295639038086 6.583266735076904 0.3160610795021057
CurrentTrain: epoch  4, batch     1 | loss: 13.3752956Losses:  8.751182556152344 2.9282264709472656 0.70926433801651
CurrentTrain: epoch  4, batch     2 | loss: 8.7511826Losses:  7.633081436157227 -0.0 0.16283349692821503
CurrentTrain: epoch  4, batch     3 | loss: 7.6330814Losses:  10.598257064819336 4.912783622741699 0.6509478688240051
CurrentTrain: epoch  5, batch     0 | loss: 10.5982571Losses:  9.699097633361816 3.567337989807129 0.611657977104187
CurrentTrain: epoch  5, batch     1 | loss: 9.6990976Losses:  10.499287605285645 3.8106541633605957 0.6391174793243408
CurrentTrain: epoch  5, batch     2 | loss: 10.4992876Losses:  3.005284070968628 -0.0 0.0925176739692688
CurrentTrain: epoch  5, batch     3 | loss: 3.0052841Losses:  10.774811744689941 4.575509071350098 0.5290228724479675
CurrentTrain: epoch  6, batch     0 | loss: 10.7748117Losses:  9.484309196472168 4.637668132781982 0.6461969614028931
CurrentTrain: epoch  6, batch     1 | loss: 9.4843092Losses:  10.376664161682129 4.448211193084717 0.5429776906967163
CurrentTrain: epoch  6, batch     2 | loss: 10.3766642Losses:  4.107088565826416 -0.0 0.15617939829826355
CurrentTrain: epoch  6, batch     3 | loss: 4.1070886Losses:  9.110045433044434 3.833744764328003 0.6119410991668701
CurrentTrain: epoch  7, batch     0 | loss: 9.1100454Losses:  7.913471698760986 3.176363706588745 0.6537166833877563
CurrentTrain: epoch  7, batch     1 | loss: 7.9134717Losses:  9.58265495300293 4.3611650466918945 0.5712675452232361
CurrentTrain: epoch  7, batch     2 | loss: 9.5826550Losses:  8.483972549438477 -0.0 -0.0
CurrentTrain: epoch  7, batch     3 | loss: 8.4839725Losses:  7.706876754760742 2.504072666168213 0.5450804233551025
CurrentTrain: epoch  8, batch     0 | loss: 7.7068768Losses:  8.378612518310547 3.557243824005127 0.5772456526756287
CurrentTrain: epoch  8, batch     1 | loss: 8.3786125Losses:  9.988571166992188 5.059267044067383 0.645065426826477
CurrentTrain: epoch  8, batch     2 | loss: 9.9885712Losses:  3.3616671562194824 -0.0 0.1001376137137413
CurrentTrain: epoch  8, batch     3 | loss: 3.3616672Losses:  7.60477876663208 2.861001491546631 0.5741907358169556
CurrentTrain: epoch  9, batch     0 | loss: 7.6047788Losses:  8.80410385131836 3.617457628250122 0.5711746215820312
CurrentTrain: epoch  9, batch     1 | loss: 8.8041039Losses:  7.2962646484375 2.750272274017334 0.5526508688926697
CurrentTrain: epoch  9, batch     2 | loss: 7.2962646Losses:  5.043029308319092 -0.0 -0.0
CurrentTrain: epoch  9, batch     3 | loss: 5.0430293
#### main subject ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "main subject" means primary topic of a work.
Here is an example:
Relation: main subject
Context: for more information on the grand mosque seizure itself , see " the siege of mecca " by yaroslav trofimov .
Head Entity: the siege of mecca
Tail Entity: grand mosque seizure
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "main subject":
Relation: main subject  
Context: In her latest book, "The Immortal Life of Henrietta Lacks," Rebecca Skloot explores the story of Henrietta Lacks and the impact of her cells on medical research.  
Head Entity: The Immortal Life of Henrietta Lacks  
Tail Entity: Henrietta Lacks  

Relation: main subject  
Context: The documentary "13th" directed by Ava DuVernay examines the intersection of race, justice, and mass incarceration in the United States.  
Head Entity: 13th  
Tail Entity: mass incarceration
#### participating team ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "participating team" means teams that actively takes/took part in an event or process.
Here is an example:
Relation: participating team
Context: group b of uefa euro 2016 contained england , russia , wales and slovakia .
Head Entity: uefa euro 2016
Tail Entity: slovakia
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "participating team":
Relation: participating team  
Context: the 2020 summer olympics featured countries like the united states, china, and japan competing in various sports.  
Head Entity: 2020 summer olympics  
Tail Entity: japan  

Relation: participating team  
Context: the fifa world cup 2018 saw teams such as france, croatia, and belgium battling for the championship title.  
Head Entity: fifa world cup 2018  
Tail Entity: croatia  
#### has part ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "has part" means part of this subject.
Here is an example:
Relation: has part
Context: in 1971 , chris and pat joined ratchell with bassist howard messer and former steppenwolf guitarist larry byrom .
Head Entity: ratchell
Tail Entity: larry byrom
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "has part":
Relation: has part  
Context: The car is equipped with a powerful engine, which includes a turbocharger and a high-performance exhaust system.  
Head Entity: the car  
Tail Entity: a turbocharger  

Relation: has part  
Context: The human body consists of various organs, including the heart, lungs, and liver, each playing a crucial role in maintaining health.  
Head Entity: the human body  
Tail Entity: the heart  
#### nominated for ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "nominated for" means award nomination received by a person, organisation or creative work.
Here is an example:
Relation: nominated for
Context: gyllenhaal was nominated for an academy award for best supporting actress for her performance .
Head Entity: gyllenhaal
Tail Entity: academy award for best supporting actress
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "nominated for":
Relation: nominated for  
Context: the film was nominated for several prestigious awards, including the golden globe for best drama.  
Head Entity: the film  
Tail Entity: golden globe for best drama  

Relation: nominated for  
Context: she was nominated for the emmy award for her outstanding performance in the television series.  
Head Entity: she  
Tail Entity: emmy award for her outstanding performance in the television series  
#### instance of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "instance of" means that class of which this subject is a particular example and member.
Here is an example:
Relation: instance of
Context: in 2007 , the county municipality created the transit authority called " skyss " that would market public transport while is would be operated by private companies based on public service obligation contracts .
Head Entity: skyss
Tail Entity: transit authority
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "instance of":
Relation: instance of  
Context: The smartphone known as the iPhone has revolutionized the way we communicate and interact with technology, becoming a benchmark for mobile devices worldwide.  
Head Entity: iPhone  
Tail Entity: smartphone  

Relation: instance of  
Context: The Great Wall of China is a remarkable feat of engineering and a symbol of Chinese heritage, attracting millions of tourists each year.  
Head Entity: Great Wall of China  
Tail Entity: engineering marvel  
#### mountain range ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "mountain range" means range or subrange to which the geographical item belongs.
Here is an example:
Relation: mountain range
Context: the mount kapaz or kepez ( ) is a mountain in lesser caucasus near ganja city in central azerbaijan .
Head Entity: mount kapaz
Tail Entity: lesser caucasus
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "mountain range":
Relation: mountain range  
Context: the sierra nevada is a mountain range in the western united states, known for its stunning landscapes and diverse ecosystems.  
Head Entity: sierra nevada  
Tail Entity: western united states  

Relation: mountain range  
Context: the alps are a major mountain range in europe, stretching across eight countries and famous for their breathtaking scenery and skiing resorts.  
Head Entity: alps  
Tail Entity: europe  
#### screenwriter ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "screenwriter" means person who wrote the script for subject item.
Here is an example:
Relation: screenwriter
Context: future films include " mcqueen " , ian bonhote ’s documentary about the fashion designer alexander mcqueen , brad anderson ’s thriller " beirut " , and mark pellington ’s drama " nostalgia " .
Head Entity: nostalgia
Tail Entity: mark pellington
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "screenwriter":
Relation: screenwriter  
Context: the upcoming movie "the great adventure" is penned by renowned screenwriter jessica taylor, who has previously won multiple awards for her work in the industry.  
Head Entity: the great adventure  
Tail Entity: jessica taylor  

Relation: screenwriter  
Context: in the latest film "shadows of the past," the script was crafted by the talented screenwriter robert lang, known for his gripping storytelling and character development.  
Head Entity: shadows of the past  
Tail Entity: robert lang  
#### language of work or name ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "language of work or name" means language associated with this creative work (such as books, shows, songs, broadcasts or websites) or a name.
Here is an example:
Relation: language of work or name
Context: the publisher found success in 1993 with tarō gomi 's " everyone poops " , a japanese language import that sold over one million copies and is now their best - known title .
Head Entity: everyone poops
Tail Entity: japanese language
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "language of work or name":
Relation: language of work or name  
Context: The animated series "Doraemon" has been translated into multiple languages, but the original version is in Japanese, which has captivated audiences worldwide.  
Head Entity: Doraemon  
Tail Entity: Japanese  

Relation: language of work or name  
Context: The famous novel "One Hundred Years of Solitude" was originally written in Spanish by Gabriel García Márquez, and it has since been translated into numerous languages.  
Head Entity: One Hundred Years of Solitude  
Tail Entity: Spanish  
#### operator ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "operator" means person, profession, or organization that operates the equipment, facility, or service.
Here is an example:
Relation: operator
Context: the aircraft was a boeing 707 - 321c that first entered service with pan american world airways ( pan am ) in 1963 .
Head Entity: boeing 707 - 321c
Tail Entity: pan american world airways
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "operator":
Relation: operator  
Context: the new electric bus fleet is managed by the city transportation authority, which ensures efficient public transit services.  
Head Entity: electric bus fleet  
Tail Entity: city transportation authority  

Relation: operator  
Context: the state-of-the-art research facility is operated by the national science foundation, focusing on advanced materials and nanotechnology.  
Head Entity: research facility  
Tail Entity: national science foundation  
#### religion ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "religion" means religion of a person, organization or religious building, or associated with this subject.
Here is an example:
Relation: religion
Context: the most important lama of this series was the third changkya , rolpai dorje , who was preceptor to the qianlong emperor of china , and chief representative of tibetan buddhism at the qing court .
Head Entity: rolpai dorje
Tail Entity: tibetan buddhism
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "religion":
Relation: religion  
Context: the cathedral of notre-dame is a famous example of french gothic architecture and serves as the seat of the archbishop of paris, representing the roman catholic faith in the heart of the city.  
Head Entity: cathedral of notre-dame  
Tail Entity: roman catholic faith  

Relation: religion  
Context: the dalai lama is a prominent figure in the world, known for his teachings on compassion and non-violence, and is the spiritual leader of tibetan buddhism.  
Head Entity: dalai lama  
Tail Entity: tibetan buddhism  
Losses:  7.854832649230957 1.867883324623108 0.5896878242492676
MemoryTrain:  epoch  0, batch     0 | loss: 7.8548326Losses:  6.201070308685303 1.3204373121261597 0.5331522226333618
MemoryTrain:  epoch  0, batch     1 | loss: 6.2010703Losses:  5.787423133850098 1.112357258796692 0.7680871486663818
MemoryTrain:  epoch  0, batch     2 | loss: 5.7874231Losses:  5.414947509765625 0.5681650638580322 0.7287576198577881
MemoryTrain:  epoch  0, batch     3 | loss: 5.4149475Losses:  7.544855117797852 1.93927800655365 0.5689281225204468
MemoryTrain:  epoch  1, batch     0 | loss: 7.5448551Losses:  4.452139854431152 0.6551748514175415 0.6840755343437195
MemoryTrain:  epoch  1, batch     1 | loss: 4.4521399Losses:  6.005704402923584 1.7613115310668945 0.700008749961853
MemoryTrain:  epoch  1, batch     2 | loss: 6.0057044Losses:  6.3103132247924805 0.635794460773468 0.750078022480011
MemoryTrain:  epoch  1, batch     3 | loss: 6.3103132Losses:  4.707898139953613 1.5823012590408325 0.6006140112876892
MemoryTrain:  epoch  2, batch     0 | loss: 4.7078981Losses:  4.689504146575928 0.9227684140205383 0.7649087905883789
MemoryTrain:  epoch  2, batch     1 | loss: 4.6895041Losses:  5.9986772537231445 1.4220621585845947 0.691415548324585
MemoryTrain:  epoch  2, batch     2 | loss: 5.9986773Losses:  4.37025785446167 0.7692639827728271 0.47456949949264526
MemoryTrain:  epoch  2, batch     3 | loss: 4.3702579Losses:  4.674339294433594 1.544654369354248 0.6451235413551331
MemoryTrain:  epoch  3, batch     0 | loss: 4.6743393Losses:  4.631235599517822 1.2477381229400635 0.6328127384185791
MemoryTrain:  epoch  3, batch     1 | loss: 4.6312356Losses:  4.646783351898193 0.7087118625640869 0.653113603591919
MemoryTrain:  epoch  3, batch     2 | loss: 4.6467834Losses:  3.1959848403930664 -0.0 0.6629925966262817
MemoryTrain:  epoch  3, batch     3 | loss: 3.1959848Losses:  3.7271926403045654 1.1949717998504639 0.7465898394584656
MemoryTrain:  epoch  4, batch     0 | loss: 3.7271926Losses:  4.10018253326416 0.34777504205703735 0.5893416404724121
MemoryTrain:  epoch  4, batch     1 | loss: 4.1001825Losses:  3.844541072845459 1.092321753501892 0.5984367728233337
MemoryTrain:  epoch  4, batch     2 | loss: 3.8445411Losses:  3.7144200801849365 0.6093282699584961 0.6204574704170227
MemoryTrain:  epoch  4, batch     3 | loss: 3.7144201Losses:  2.7840983867645264 0.20739836990833282 0.7394962310791016
MemoryTrain:  epoch  5, batch     0 | loss: 2.7840984Losses:  3.848684310913086 1.3164353370666504 0.6086536645889282
MemoryTrain:  epoch  5, batch     1 | loss: 3.8486843Losses:  4.146464824676514 1.3534104824066162 0.6709127426147461
MemoryTrain:  epoch  5, batch     2 | loss: 4.1464648Losses:  4.479654312133789 1.0086795091629028 0.5356655120849609
MemoryTrain:  epoch  5, batch     3 | loss: 4.4796543Losses:  3.6106104850769043 1.5088871717453003 0.5749536752700806
MemoryTrain:  epoch  6, batch     0 | loss: 3.6106105Losses:  4.534078121185303 1.2383220195770264 0.703822910785675
MemoryTrain:  epoch  6, batch     1 | loss: 4.5340781Losses:  3.8526859283447266 0.7923212051391602 0.6623818278312683
MemoryTrain:  epoch  6, batch     2 | loss: 3.8526859Losses:  2.4144277572631836 0.7781972885131836 0.26564228534698486
MemoryTrain:  epoch  6, batch     3 | loss: 2.4144278Losses:  3.793233633041382 1.1086151599884033 0.7273958325386047
MemoryTrain:  epoch  7, batch     0 | loss: 3.7932336Losses:  4.028748989105225 1.2152420282363892 0.675979733467102
MemoryTrain:  epoch  7, batch     1 | loss: 4.0287490Losses:  3.7281651496887207 1.3290481567382812 0.5840402841567993
MemoryTrain:  epoch  7, batch     2 | loss: 3.7281651Losses:  2.832034111022949 0.5334529876708984 0.5109766125679016
MemoryTrain:  epoch  7, batch     3 | loss: 2.8320341Losses:  3.4014322757720947 0.891801655292511 0.6287963390350342
MemoryTrain:  epoch  8, batch     0 | loss: 3.4014323Losses:  3.207144021987915 0.9609999656677246 0.6959732174873352
MemoryTrain:  epoch  8, batch     1 | loss: 3.2071440Losses:  3.826930522918701 0.8868125081062317 0.7167465686798096
MemoryTrain:  epoch  8, batch     2 | loss: 3.8269305Losses:  3.355386972427368 1.070037603378296 0.4975612163543701
MemoryTrain:  epoch  8, batch     3 | loss: 3.3553870Losses:  2.694702625274658 0.4675140380859375 0.5908695459365845
MemoryTrain:  epoch  9, batch     0 | loss: 2.6947026Losses:  3.5313780307769775 1.127575397491455 0.590001106262207
MemoryTrain:  epoch  9, batch     1 | loss: 3.5313780Losses:  3.6822831630706787 1.2598005533218384 0.6603052020072937
MemoryTrain:  epoch  9, batch     2 | loss: 3.6822832Losses:  2.7518773078918457 0.3235611319541931 0.5570221543312073
MemoryTrain:  epoch  9, batch     3 | loss: 2.7518773
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    1 | acc: 68.75%,  total acc: 65.62%   [EVAL] batch:    2 | acc: 56.25%,  total acc: 62.50%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 64.06%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 68.75%   [EVAL] batch:    5 | acc: 62.50%,  total acc: 67.71%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 68.75%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 71.88%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 71.53%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 73.12%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 75.00%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 76.04%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 75.48%   [EVAL] batch:   13 | acc: 18.75%,  total acc: 71.43%   [EVAL] batch:   14 | acc: 25.00%,  total acc: 68.33%   [EVAL] batch:   15 | acc: 25.00%,  total acc: 65.62%   [EVAL] batch:   16 | acc: 43.75%,  total acc: 64.34%   [EVAL] batch:   17 | acc: 43.75%,  total acc: 63.19%   [EVAL] batch:   18 | acc: 31.25%,  total acc: 61.51%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 63.12%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 64.88%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 66.19%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 67.66%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 69.01%   [EVAL] batch:   24 | acc: 93.75%,  total acc: 70.00%   [EVAL] batch:   25 | acc: 43.75%,  total acc: 68.99%   [EVAL] batch:   26 | acc: 43.75%,  total acc: 68.06%   [EVAL] batch:   27 | acc: 31.25%,  total acc: 66.74%   [EVAL] batch:   28 | acc: 50.00%,  total acc: 66.16%   [EVAL] batch:   29 | acc: 31.25%,  total acc: 65.00%   [EVAL] batch:   30 | acc: 12.50%,  total acc: 63.31%   [EVAL] batch:   31 | acc: 87.50%,  total acc: 64.06%   [EVAL] batch:   32 | acc: 81.25%,  total acc: 64.58%   [EVAL] batch:   33 | acc: 81.25%,  total acc: 65.07%   [EVAL] batch:   34 | acc: 87.50%,  total acc: 65.71%   [EVAL] batch:   35 | acc: 62.50%,  total acc: 65.62%   [EVAL] batch:   36 | acc: 75.00%,  total acc: 65.88%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 66.61%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 67.47%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 68.28%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 69.05%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 69.79%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 70.49%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 71.16%   [EVAL] batch:   44 | acc: 81.25%,  total acc: 71.39%   [EVAL] batch:   45 | acc: 81.25%,  total acc: 71.60%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 72.07%   [EVAL] batch:   47 | acc: 68.75%,  total acc: 72.01%   [EVAL] batch:   48 | acc: 81.25%,  total acc: 72.19%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 72.62%   [EVAL] batch:   50 | acc: 68.75%,  total acc: 72.55%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 72.84%   [EVAL] batch:   52 | acc: 81.25%,  total acc: 73.00%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 73.15%   [EVAL] batch:   54 | acc: 100.00%,  total acc: 73.64%   [EVAL] batch:   55 | acc: 75.00%,  total acc: 73.66%   [EVAL] batch:   56 | acc: 75.00%,  total acc: 73.68%   [EVAL] batch:   57 | acc: 62.50%,  total acc: 73.49%   [EVAL] batch:   58 | acc: 75.00%,  total acc: 73.52%   [EVAL] batch:   59 | acc: 62.50%,  total acc: 73.33%   [EVAL] batch:   60 | acc: 68.75%,  total acc: 73.26%   [EVAL] batch:   61 | acc: 50.00%,  total acc: 72.88%   [EVAL] batch:   62 | acc: 37.50%,  total acc: 72.32%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 84.38%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 85.00%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 86.46%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 83.93%   [EVAL] batch:    7 | acc: 68.75%,  total acc: 82.03%   [EVAL] batch:    8 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 81.88%   [EVAL] batch:   10 | acc: 62.50%,  total acc: 80.11%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 79.69%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 79.81%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 80.80%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 81.67%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 82.81%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 83.82%   [EVAL] batch:   17 | acc: 100.00%,  total acc: 84.72%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 85.53%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 85.62%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 86.31%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 86.36%   [EVAL] batch:   22 | acc: 87.50%,  total acc: 86.41%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 86.46%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 86.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 87.02%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 87.72%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 87.93%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 88.33%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 88.71%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 89.06%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 89.39%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 89.71%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 89.82%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 90.10%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 90.37%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 90.87%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 91.09%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 91.31%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 91.52%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 91.57%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 91.76%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 91.94%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 92.12%   [EVAL] batch:   46 | acc: 87.50%,  total acc: 92.02%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 92.06%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 92.22%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 92.25%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 92.28%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 92.31%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 92.45%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 92.59%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 92.50%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 92.52%   [EVAL] batch:   56 | acc: 81.25%,  total acc: 92.32%   [EVAL] batch:   57 | acc: 62.50%,  total acc: 91.81%   [EVAL] batch:   58 | acc: 75.00%,  total acc: 91.53%   [EVAL] batch:   59 | acc: 81.25%,  total acc: 91.35%   [EVAL] batch:   60 | acc: 75.00%,  total acc: 91.09%   [EVAL] batch:   61 | acc: 81.25%,  total acc: 90.93%   [EVAL] batch:   62 | acc: 81.25%,  total acc: 90.77%   [EVAL] batch:   63 | acc: 56.25%,  total acc: 90.23%   [EVAL] batch:   64 | acc: 62.50%,  total acc: 89.81%   [EVAL] batch:   65 | acc: 81.25%,  total acc: 89.68%   [EVAL] batch:   66 | acc: 56.25%,  total acc: 89.18%   [EVAL] batch:   67 | acc: 75.00%,  total acc: 88.97%   [EVAL] batch:   68 | acc: 81.25%,  total acc: 88.86%   [EVAL] batch:   69 | acc: 81.25%,  total acc: 88.75%   [EVAL] batch:   70 | acc: 81.25%,  total acc: 88.64%   [EVAL] batch:   71 | acc: 75.00%,  total acc: 88.45%   [EVAL] batch:   72 | acc: 93.75%,  total acc: 88.53%   [EVAL] batch:   73 | acc: 87.50%,  total acc: 88.51%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 88.50%   [EVAL] batch:   75 | acc: 31.25%,  total acc: 87.75%   [EVAL] batch:   76 | acc: 25.00%,  total acc: 86.93%   [EVAL] batch:   77 | acc: 25.00%,  total acc: 86.14%   [EVAL] batch:   78 | acc: 37.50%,  total acc: 85.52%   [EVAL] batch:   79 | acc: 43.75%,  total acc: 85.00%   [EVAL] batch:   80 | acc: 25.00%,  total acc: 84.26%   [EVAL] batch:   81 | acc: 68.75%,  total acc: 84.07%   [EVAL] batch:   82 | acc: 100.00%,  total acc: 84.26%   [EVAL] batch:   83 | acc: 100.00%,  total acc: 84.45%   [EVAL] batch:   84 | acc: 93.75%,  total acc: 84.56%   [EVAL] batch:   85 | acc: 100.00%,  total acc: 84.74%   [EVAL] batch:   86 | acc: 93.75%,  total acc: 84.84%   [EVAL] batch:   87 | acc: 81.25%,  total acc: 84.80%   [EVAL] batch:   88 | acc: 25.00%,  total acc: 84.13%   [EVAL] batch:   89 | acc: 50.00%,  total acc: 83.75%   [EVAL] batch:   90 | acc: 37.50%,  total acc: 83.24%   [EVAL] batch:   91 | acc: 43.75%,  total acc: 82.81%   [EVAL] batch:   92 | acc: 12.50%,  total acc: 82.06%   [EVAL] batch:   93 | acc: 50.00%,  total acc: 81.72%   [EVAL] batch:   94 | acc: 87.50%,  total acc: 81.78%   [EVAL] batch:   95 | acc: 81.25%,  total acc: 81.77%   [EVAL] batch:   96 | acc: 87.50%,  total acc: 81.83%   [EVAL] batch:   97 | acc: 81.25%,  total acc: 81.82%   [EVAL] batch:   98 | acc: 62.50%,  total acc: 81.63%   [EVAL] batch:   99 | acc: 81.25%,  total acc: 81.62%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 81.81%   [EVAL] batch:  101 | acc: 100.00%,  total acc: 81.99%   [EVAL] batch:  102 | acc: 100.00%,  total acc: 82.16%   [EVAL] batch:  103 | acc: 100.00%,  total acc: 82.33%   [EVAL] batch:  104 | acc: 100.00%,  total acc: 82.50%   [EVAL] batch:  105 | acc: 100.00%,  total acc: 82.67%   [EVAL] batch:  106 | acc: 87.50%,  total acc: 82.71%   [EVAL] batch:  107 | acc: 81.25%,  total acc: 82.70%   [EVAL] batch:  108 | acc: 87.50%,  total acc: 82.74%   [EVAL] batch:  109 | acc: 87.50%,  total acc: 82.78%   [EVAL] batch:  110 | acc: 81.25%,  total acc: 82.77%   [EVAL] batch:  111 | acc: 75.00%,  total acc: 82.70%   [EVAL] batch:  112 | acc: 87.50%,  total acc: 82.74%   [EVAL] batch:  113 | acc: 68.75%,  total acc: 82.62%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 82.77%   [EVAL] batch:  115 | acc: 81.25%,  total acc: 82.76%   [EVAL] batch:  116 | acc: 81.25%,  total acc: 82.75%   [EVAL] batch:  117 | acc: 81.25%,  total acc: 82.73%   [EVAL] batch:  118 | acc: 75.00%,  total acc: 82.67%   [EVAL] batch:  119 | acc: 81.25%,  total acc: 82.66%   [EVAL] batch:  120 | acc: 62.50%,  total acc: 82.49%   [EVAL] batch:  121 | acc: 68.75%,  total acc: 82.38%   [EVAL] batch:  122 | acc: 50.00%,  total acc: 82.11%   [EVAL] batch:  123 | acc: 75.00%,  total acc: 82.06%   [EVAL] batch:  124 | acc: 62.50%,  total acc: 81.90%   
cur_acc:  ['0.9484', '0.7232']
his_acc:  ['0.9484', '0.8190']
Clustering into  14  clusters
Clusters:  [12 10  3  1 13  4  3  0 10 10  5  8  9  6  9  4  0  2  0  1 11  1  7  2
  2  1  7  5 10  5]
Losses:  11.389451026916504 5.271463871002197 0.46879273653030396
CurrentTrain: epoch  0, batch     0 | loss: 11.3894510Losses:  13.00302791595459 5.433152198791504 0.5311285853385925
CurrentTrain: epoch  0, batch     1 | loss: 13.0030279Losses:  11.088449478149414 4.5150146484375 0.5531921982765198
CurrentTrain: epoch  0, batch     2 | loss: 11.0884495Losses:  7.757948398590088 -0.0 -0.0
CurrentTrain: epoch  0, batch     3 | loss: 7.7579484Losses:  9.925436019897461 3.6402747631073 0.5696595907211304
CurrentTrain: epoch  1, batch     0 | loss: 9.9254360Losses:  9.831022262573242 4.169436454772949 0.5167320370674133
CurrentTrain: epoch  1, batch     1 | loss: 9.8310223Losses:  10.375248908996582 5.024255752563477 0.5661497116088867
CurrentTrain: epoch  1, batch     2 | loss: 10.3752489Losses:  7.876043796539307 -0.0 0.18021708726882935
CurrentTrain: epoch  1, batch     3 | loss: 7.8760438Losses:  8.313812255859375 3.165334939956665 0.5764207243919373
CurrentTrain: epoch  2, batch     0 | loss: 8.3138123Losses:  8.031379699707031 2.8740200996398926 0.6028724908828735
CurrentTrain: epoch  2, batch     1 | loss: 8.0313797Losses:  9.804636001586914 4.2537994384765625 0.5893301963806152
CurrentTrain: epoch  2, batch     2 | loss: 9.8046360Losses:  4.716734886169434 -0.0 0.12842196226119995
CurrentTrain: epoch  2, batch     3 | loss: 4.7167349Losses:  7.838056564331055 2.8390607833862305 0.622117280960083
CurrentTrain: epoch  3, batch     0 | loss: 7.8380566Losses:  7.836171627044678 2.8257827758789062 0.5813986659049988
CurrentTrain: epoch  3, batch     1 | loss: 7.8361716Losses:  9.25500202178955 4.830986499786377 0.4685218036174774
CurrentTrain: epoch  3, batch     2 | loss: 9.2550020Losses:  1.8736178874969482 -0.0 -0.0
CurrentTrain: epoch  3, batch     3 | loss: 1.8736179Losses:  8.616909980773926 3.6704630851745605 0.5795530080795288
CurrentTrain: epoch  4, batch     0 | loss: 8.6169100Losses:  6.841073036193848 2.560701847076416 0.6284009218215942
CurrentTrain: epoch  4, batch     1 | loss: 6.8410730Losses:  8.396299362182617 4.544095993041992 0.455540269613266
CurrentTrain: epoch  4, batch     2 | loss: 8.3962994Losses:  3.3126754760742188 -0.0 0.1049705445766449
CurrentTrain: epoch  4, batch     3 | loss: 3.3126755Losses:  7.208132743835449 3.05718994140625 0.5390605330467224
CurrentTrain: epoch  5, batch     0 | loss: 7.2081327Losses:  8.36347484588623 3.9864845275878906 0.5308953523635864
CurrentTrain: epoch  5, batch     1 | loss: 8.3634748Losses:  8.4862642288208 4.538792610168457 0.5719526410102844
CurrentTrain: epoch  5, batch     2 | loss: 8.4862642Losses:  3.772955894470215 -0.0 0.10373618453741074
CurrentTrain: epoch  5, batch     3 | loss: 3.7729559Losses:  7.415763854980469 3.1217894554138184 0.5327635407447815
CurrentTrain: epoch  6, batch     0 | loss: 7.4157639Losses:  8.161293983459473 3.6545987129211426 0.45020967721939087
CurrentTrain: epoch  6, batch     1 | loss: 8.1612940Losses:  6.028572082519531 2.738157272338867 0.5318633913993835
CurrentTrain: epoch  6, batch     2 | loss: 6.0285721Losses:  3.013277769088745 -0.0 0.16530394554138184
CurrentTrain: epoch  6, batch     3 | loss: 3.0132778Losses:  6.6483564376831055 3.0001702308654785 0.5767537355422974
CurrentTrain: epoch  7, batch     0 | loss: 6.6483564Losses:  6.767410755157471 2.571829080581665 0.5449473261833191
CurrentTrain: epoch  7, batch     1 | loss: 6.7674108Losses:  5.743231296539307 2.227799654006958 0.5542664527893066
CurrentTrain: epoch  7, batch     2 | loss: 5.7432313Losses:  2.2094736099243164 -0.0 0.10184353590011597
CurrentTrain: epoch  7, batch     3 | loss: 2.2094736Losses:  6.095790386199951 2.2492611408233643 0.5433415174484253
CurrentTrain: epoch  8, batch     0 | loss: 6.0957904Losses:  5.651206970214844 2.6895246505737305 0.5410356521606445
CurrentTrain: epoch  8, batch     1 | loss: 5.6512070Losses:  7.234564781188965 4.402818202972412 0.5048468112945557
CurrentTrain: epoch  8, batch     2 | loss: 7.2345648Losses:  3.618238925933838 -0.0 0.16266116499900818
CurrentTrain: epoch  8, batch     3 | loss: 3.6182389Losses:  5.4777913093566895 2.6215524673461914 0.5068952441215515
CurrentTrain: epoch  9, batch     0 | loss: 5.4777913Losses:  6.573943138122559 3.57607102394104 0.5491827130317688
CurrentTrain: epoch  9, batch     1 | loss: 6.5739431Losses:  6.174954414367676 2.666912078857422 0.5179624557495117
CurrentTrain: epoch  9, batch     2 | loss: 6.1749544Losses:  1.68929123878479 -0.0 -0.0
CurrentTrain: epoch  9, batch     3 | loss: 1.6892912
#### follows ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "follows" means immediately prior item in a series of which the subject is a part.
Here is an example:
Relation: follows
Context: peugeot took a similar step in 2010 when replacing the 407 and long - running but unpopular 607 with a single model , the 508 .
Head Entity: 508
Tail Entity: 407
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "follows":
Relation: follows  
Context: In the 2021 season, the team introduced the new model, the X5, which follows the successful launch of the X3 in 2020.  
Head Entity: X5  
Tail Entity: X3  

Relation: follows  
Context: The latest smartphone, the Galaxy S21, follows the previous model, the Galaxy S20, which was released just a year earlier.  
Head Entity: Galaxy S21  
Tail Entity: Galaxy S20  
#### sport ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "sport" means sport that the subject participates or participated in or is associated with.
Here is an example:
Relation: sport
Context: dick packer was a u.s. soccer center forward who was a member of the u.s. team at the 1956 summer olympics .
Head Entity: dick packer
Tail Entity: soccer
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "sport":
Relation: sport  
Context: serena williams is a professional tennis player who has won numerous grand slam titles throughout her career.  
Head Entity: serena williams  
Tail Entity: tennis  

Relation: sport  
Context: lebron james is known for his exceptional skills in basketball and has played for the los angeles lakers in the nba.  
Head Entity: lebron james  
Tail Entity: basketball  
#### father ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "father" means male parent of the subject.
Here is an example:
Relation: father
Context: after the crowning of his nephew radoslav , the son of stefan , sava left the serbian maritime in 1229 for a trip to palestine .
Head Entity: radoslav
Tail Entity: stefan
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "father":
Relation: father  
Context: in the historical records, it is noted that king henry was the father of prince edward, who later became a significant figure in the kingdom.  
Head Entity: prince edward  
Tail Entity: king henry  

Relation: father  
Context: during the family reunion, it was revealed that michael was the father of two daughters, emily and sarah, who both excelled in their studies.  
Head Entity: michael  
Tail Entity: emily  
#### original network ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "original network" means network or service that originally broadcast a radio or television program.
Here is an example:
Relation: original network
Context: hana yori dango returns ( ) is a japanese television drama series , broadcast on tbs in 2007 .
Head Entity: hana yori dango returns
Tail Entity: tbs
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "original network":
Relation: original network  
Context: the popular series breaking bad was originally aired on amc from 2008 to 2013.  
Head Entity: breaking bad  
Tail Entity: amc  

Relation: original network  
Context: friends is a classic sitcom that premiered on nbc in 1994 and ran for ten seasons.  
Head Entity: friends  
Tail Entity: nbc  
#### original language of film or TV show ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "original language of film or TV show" means language in which a film or a performance work was originally created.
Here is an example:
Relation: original language of film or TV show
Context: the 1954 film , directed by kadri venkata reddy , " peddamanushulu " was honoured with the first president 's silver medal for best feature film in telugu .
Head Entity: peddamanushulu
Tail Entity: telugu
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "original language of film or TV show":
Relation: original language of film or TV show  
Context: the 1994 animated film "The Lion King," produced by Walt Disney Feature Animation, was originally created in English.  
Head Entity: The Lion King  
Tail Entity: English  

Relation: original language of film or TV show  
Context: the critically acclaimed 2019 film "Parasite," directed by Bong Joon-ho, was originally filmed in Korean.  
Head Entity: Parasite  
Tail Entity: Korean  
#### league ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "league" means league in which team or player plays or has played in.
Here is an example:
Relation: league
Context: after years on loan with various lower division turkish teams , cangöz made his professional debut for antalyaspor in a 4 - 1 süper lig victory over gaziantepspor on 2 june 2017 .
Head Entity: antalyaspor
Tail Entity: süper lig
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "league":
Relation: league  
Context: in 2020, the talented midfielder signed with the club after impressing in the youth academy, making his first appearance in the premier league against manchester city.  
Head Entity: club  
Tail Entity: premier league  

Relation: league  
Context: during his time at the club, he helped them secure a spot in the championship, showcasing his skills in several key matches throughout the season.  
Head Entity: club  
Tail Entity: championship  
#### mother ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "mother" means female parent of the subject.
Here is an example:
Relation: mother
Context: other teammates included dominique leray and élisabeth riffiod , whose son boris diaw currently plays in the nba for the spurs ( 2014 ) .
Head Entity: boris diaw
Tail Entity: élisabeth riffiod
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "mother":
Relation: mother  
Context: in a recent interview, actress jennifer garner spoke fondly of her children, including violet, who is the daughter of actor ben affleck.  
Head Entity: violet  
Tail Entity: jennifer garner  

Relation: mother  
Context: during the family reunion, it was heartwarming to see how much emily cherished her time with her mother, sarah, who had always been her guiding light.  
Head Entity: emily  
Tail Entity: sarah  
#### instrument ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "instrument" means musical instrument that a person plays or teaches or used in a music occupation.
Here is an example:
Relation: instrument
Context: peter bastian ( 25 august 1943 in copenhagen – 27 march 2017 ) was a danish musician . peter bastian bio and cv he was educated in theoretical physics and classical bassoon .
Head Entity: peter bastian
Tail Entity: bassoon
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "instrument":
Relation: instrument  
Context: johann sebastian bach (31 march 1685 – 28 july 1750) was a german composer and musician of the baroque period. he was known for his exceptional skill in playing the organ and the harpsichord.  
Head Entity: johann sebastian bach  
Tail Entity: organ  

Relation: instrument  
Context: yo-yo ma, born on october 7, 1955, is a world-renowned cellist who has performed with numerous orchestras and in various genres. he is particularly celebrated for his interpretations of classical cello concertos.  
Head Entity: yo-yo ma  
Tail Entity: cello  
#### country ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "country" means sovereign state of this item (not to be used for human beings).
Here is an example:
Relation: country
Context: ploegsteert is a village in belgium located in the municipality of comines - warneton in the hainaut province and is the most westerly settlement of the walloon region .
Head Entity: hainaut
Tail Entity: belgium
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "country":
Relation: country  
Context: the city of prague is the capital of the czech republic and is known for its beautiful architecture and rich history.  
Head Entity: prague  
Tail Entity: czech republic  

Relation: country  
Context: the great barrier reef is located off the coast of australia and is the largest coral reef system in the world.  
Head Entity: great barrier reef  
Tail Entity: australia  
#### characters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "characters" means characters which appear in this item (like plays, operas, operettas, books, comics, films, TV series, video games).
Here is an example:
Relation: characters
Context: klaus kinski , who would play dracula himself nine years later in " nosferatu the vampyre " , is also featured in the film as renfield .
Head Entity: nosferatu the vampyre
Tail Entity: renfield
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "characters":
Relation: characters  
Context: in the animated series "avatar: the last airbender," zuko is a complex character who struggles with his identity and family legacy.  
Head Entity: avatar: the last airbender  
Tail Entity: zuko  

Relation: characters  
Context: the novel "pride and prejudice" features elizabeth bennet, a strong-willed protagonist who navigates societal expectations and personal relationships.  
Head Entity: pride and prejudice  
Tail Entity: elizabeth bennet  
Losses:  4.448979377746582 0.2754979729652405 0.9209423065185547
MemoryTrain:  epoch  0, batch     0 | loss: 4.4489794Losses:  4.5311784744262695 0.4734039902687073 0.7308729290962219
MemoryTrain:  epoch  0, batch     1 | loss: 4.5311785Losses:  5.530238151550293 0.6344786286354065 0.7544347047805786
MemoryTrain:  epoch  0, batch     2 | loss: 5.5302382Losses:  5.445170879364014 0.897181510925293 0.8871693015098572
MemoryTrain:  epoch  0, batch     3 | loss: 5.4451709Losses:  4.9151716232299805 0.4904203414916992 0.7781696319580078
MemoryTrain:  epoch  0, batch     4 | loss: 4.9151716Losses:  4.433018684387207 0.41223445534706116 0.5742917060852051
MemoryTrain:  epoch  0, batch     5 | loss: 4.4330187Losses:  4.538725852966309 0.6968133449554443 0.5949075818061829
MemoryTrain:  epoch  1, batch     0 | loss: 4.5387259Losses:  4.161462783813477 0.25198793411254883 0.847683846950531
MemoryTrain:  epoch  1, batch     1 | loss: 4.1614628Losses:  5.040154933929443 -0.0 0.886692225933075
MemoryTrain:  epoch  1, batch     2 | loss: 5.0401549Losses:  4.696645736694336 1.1924359798431396 0.5889818668365479
MemoryTrain:  epoch  1, batch     3 | loss: 4.6966457Losses:  4.812320232391357 0.529232919216156 0.7646374702453613
MemoryTrain:  epoch  1, batch     4 | loss: 4.8123202Losses:  3.374696969985962 -0.0 0.5949901938438416
MemoryTrain:  epoch  1, batch     5 | loss: 3.3746970Losses:  5.527257919311523 0.6799063086509705 0.8351590037345886
MemoryTrain:  epoch  2, batch     0 | loss: 5.5272579Losses:  4.404089450836182 0.8900483846664429 0.8882225751876831
MemoryTrain:  epoch  2, batch     1 | loss: 4.4040895Losses:  3.953458070755005 0.7181124687194824 0.919599711894989
MemoryTrain:  epoch  2, batch     2 | loss: 3.9534581Losses:  3.4478096961975098 -0.0 0.8133148550987244
MemoryTrain:  epoch  2, batch     3 | loss: 3.4478097Losses:  5.431672096252441 1.1221344470977783 0.7275683283805847
MemoryTrain:  epoch  2, batch     4 | loss: 5.4316721Losses:  3.1222660541534424 -0.0 0.6447489261627197
MemoryTrain:  epoch  2, batch     5 | loss: 3.1222661Losses:  4.569411754608154 0.25424301624298096 0.6576360464096069
MemoryTrain:  epoch  3, batch     0 | loss: 4.5694118Losses:  4.592764854431152 0.8103200197219849 0.8108918070793152
MemoryTrain:  epoch  3, batch     1 | loss: 4.5927649Losses:  3.859450340270996 0.7353705167770386 0.7923635244369507
MemoryTrain:  epoch  3, batch     2 | loss: 3.8594503Losses:  3.0341038703918457 0.4695892333984375 0.7853344678878784
MemoryTrain:  epoch  3, batch     3 | loss: 3.0341039Losses:  3.7923169136047363 0.2965144217014313 0.6815510392189026
MemoryTrain:  epoch  3, batch     4 | loss: 3.7923169Losses:  3.9524970054626465 0.32970955967903137 0.5448774099349976
MemoryTrain:  epoch  3, batch     5 | loss: 3.9524970Losses:  4.4531097412109375 0.6358340978622437 0.8280227780342102
MemoryTrain:  epoch  4, batch     0 | loss: 4.4531097Losses:  3.5123720169067383 0.2170368731021881 0.6919397115707397
MemoryTrain:  epoch  4, batch     1 | loss: 3.5123720Losses:  3.2327098846435547 0.25553786754608154 0.9774001836776733
MemoryTrain:  epoch  4, batch     2 | loss: 3.2327099Losses:  3.3097901344299316 0.5691989064216614 0.7716355323791504
MemoryTrain:  epoch  4, batch     3 | loss: 3.3097901Losses:  4.3090620040893555 1.5311903953552246 0.6513757705688477
MemoryTrain:  epoch  4, batch     4 | loss: 4.3090620Losses:  5.097308158874512 2.2642905712127686 0.44042783975601196
MemoryTrain:  epoch  4, batch     5 | loss: 5.0973082Losses:  4.085935115814209 1.0666184425354004 0.8273019194602966
MemoryTrain:  epoch  5, batch     0 | loss: 4.0859351Losses:  3.648838996887207 0.8087476491928101 0.7781139612197876
MemoryTrain:  epoch  5, batch     1 | loss: 3.6488390Losses:  3.074509620666504 0.7047604918479919 0.6461469531059265
MemoryTrain:  epoch  5, batch     2 | loss: 3.0745096Losses:  2.9700355529785156 0.5069760680198669 0.813406765460968
MemoryTrain:  epoch  5, batch     3 | loss: 2.9700356Losses:  3.8115310668945312 0.9953644275665283 0.6456149816513062
MemoryTrain:  epoch  5, batch     4 | loss: 3.8115311Losses:  3.530625104904175 0.40843936800956726 0.5964156985282898
MemoryTrain:  epoch  5, batch     5 | loss: 3.5306251Losses:  3.5881035327911377 1.139582633972168 0.660592794418335
MemoryTrain:  epoch  6, batch     0 | loss: 3.5881035Losses:  2.821676731109619 0.47015583515167236 0.7770086526870728
MemoryTrain:  epoch  6, batch     1 | loss: 2.8216767Losses:  3.1532135009765625 0.271358847618103 0.8162607550621033
MemoryTrain:  epoch  6, batch     2 | loss: 3.1532135Losses:  3.948566436767578 1.5530484914779663 0.6411701440811157
MemoryTrain:  epoch  6, batch     3 | loss: 3.9485664Losses:  3.391402006149292 0.30710768699645996 0.8222545981407166
MemoryTrain:  epoch  6, batch     4 | loss: 3.3914020Losses:  2.3255293369293213 -0.0 0.6663143634796143
MemoryTrain:  epoch  6, batch     5 | loss: 2.3255293Losses:  4.088391304016113 1.6409807205200195 0.7826834917068481
MemoryTrain:  epoch  7, batch     0 | loss: 4.0883913Losses:  3.7192611694335938 1.3181172609329224 0.7418031692504883
MemoryTrain:  epoch  7, batch     1 | loss: 3.7192612Losses:  3.0062057971954346 0.5180939435958862 0.6840298771858215
MemoryTrain:  epoch  7, batch     2 | loss: 3.0062058Losses:  2.8678040504455566 0.5547906160354614 0.7335913181304932
MemoryTrain:  epoch  7, batch     3 | loss: 2.8678041Losses:  3.1721272468566895 0.2730875015258789 0.8032949566841125
MemoryTrain:  epoch  7, batch     4 | loss: 3.1721272Losses:  2.012624740600586 0.20763225853443146 0.5826071500778198
MemoryTrain:  epoch  7, batch     5 | loss: 2.0126247Losses:  3.5136144161224365 1.4002819061279297 0.7384874224662781
MemoryTrain:  epoch  8, batch     0 | loss: 3.5136144Losses:  3.243070602416992 1.047937035560608 0.557640552520752
MemoryTrain:  epoch  8, batch     1 | loss: 3.2430706Losses:  3.9646146297454834 1.1944994926452637 0.752183735370636
MemoryTrain:  epoch  8, batch     2 | loss: 3.9646146Losses:  2.9636776447296143 0.7893670201301575 0.7180963158607483
MemoryTrain:  epoch  8, batch     3 | loss: 2.9636776Losses:  2.974445343017578 0.48994460701942444 0.9288346171379089
MemoryTrain:  epoch  8, batch     4 | loss: 2.9744453Losses:  2.212819814682007 0.24299733340740204 0.6515558958053589
MemoryTrain:  epoch  8, batch     5 | loss: 2.2128198Losses:  2.354034185409546 0.25951051712036133 0.6596943140029907
MemoryTrain:  epoch  9, batch     0 | loss: 2.3540342Losses:  3.3972256183624268 0.5613341927528381 0.8145057559013367
MemoryTrain:  epoch  9, batch     1 | loss: 3.3972256Losses:  3.3087568283081055 1.2309013605117798 0.6627858877182007
MemoryTrain:  epoch  9, batch     2 | loss: 3.3087568Losses:  2.593419313430786 0.45906758308410645 0.7051627039909363
MemoryTrain:  epoch  9, batch     3 | loss: 2.5934193Losses:  3.769085168838501 1.2951006889343262 0.7568711638450623
MemoryTrain:  epoch  9, batch     4 | loss: 3.7690852Losses:  2.085028886795044 -0.0 0.6673807501792908
MemoryTrain:  epoch  9, batch     5 | loss: 2.0850289
[EVAL] batch:    0 | acc: 31.25%,  total acc: 31.25%   [EVAL] batch:    1 | acc: 50.00%,  total acc: 40.62%   [EVAL] batch:    2 | acc: 62.50%,  total acc: 47.92%   [EVAL] batch:    3 | acc: 31.25%,  total acc: 43.75%   [EVAL] batch:    4 | acc: 50.00%,  total acc: 45.00%   [EVAL] batch:    5 | acc: 37.50%,  total acc: 43.75%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 48.21%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 51.56%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 55.56%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 59.38%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 61.93%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 63.54%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 64.90%   [EVAL] batch:   13 | acc: 62.50%,  total acc: 64.73%   [EVAL] batch:   14 | acc: 81.25%,  total acc: 65.83%   [EVAL] batch:   15 | acc: 68.75%,  total acc: 66.02%   [EVAL] batch:   16 | acc: 68.75%,  total acc: 66.18%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 66.32%   [EVAL] batch:   18 | acc: 75.00%,  total acc: 66.78%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 68.44%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 69.94%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 71.02%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 72.28%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 73.44%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 74.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 75.93%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 76.79%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 77.59%   [EVAL] batch:   29 | acc: 93.75%,  total acc: 78.12%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 78.43%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 78.91%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 79.36%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 79.78%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 80.18%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 80.56%   [EVAL] batch:   36 | acc: 87.50%,  total acc: 80.74%   [EVAL] batch:   37 | acc: 62.50%,  total acc: 80.26%   [EVAL] batch:   38 | acc: 37.50%,  total acc: 79.17%   [EVAL] batch:   39 | acc: 37.50%,  total acc: 78.12%   [EVAL] batch:   40 | acc: 50.00%,  total acc: 77.44%   [EVAL] batch:   41 | acc: 50.00%,  total acc: 76.79%   [EVAL] batch:   42 | acc: 37.50%,  total acc: 75.87%   [EVAL] batch:   43 | acc: 68.75%,  total acc: 75.71%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 76.25%   [EVAL] batch:   45 | acc: 87.50%,  total acc: 76.49%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 76.99%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 77.47%   [EVAL] batch:   48 | acc: 87.50%,  total acc: 77.68%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 78.00%   [EVAL] batch:   50 | acc: 50.00%,  total acc: 77.45%   [EVAL] batch:   51 | acc: 43.75%,  total acc: 76.80%   [EVAL] batch:   52 | acc: 50.00%,  total acc: 76.30%   [EVAL] batch:   53 | acc: 50.00%,  total acc: 75.81%   [EVAL] batch:   54 | acc: 50.00%,  total acc: 75.34%   [EVAL] batch:   55 | acc: 43.75%,  total acc: 74.78%   [EVAL] batch:   56 | acc: 56.25%,  total acc: 74.45%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 74.68%   [EVAL] batch:   58 | acc: 43.75%,  total acc: 74.15%   [EVAL] batch:   59 | acc: 87.50%,  total acc: 74.38%   [EVAL] batch:   60 | acc: 75.00%,  total acc: 74.39%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 74.70%   [EVAL] batch:   62 | acc: 43.75%,  total acc: 74.21%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 85.94%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 85.00%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 86.46%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 83.93%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 83.59%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 81.94%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:   10 | acc: 68.75%,  total acc: 80.11%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 80.73%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 80.77%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 81.70%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 82.50%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 83.59%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 84.56%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 85.07%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 85.86%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 85.62%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 85.71%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 85.80%   [EVAL] batch:   22 | acc: 75.00%,  total acc: 85.33%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 85.68%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 85.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 86.30%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 86.81%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 87.05%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 87.28%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 87.71%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 88.10%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 88.48%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 88.83%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 89.15%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 89.29%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 89.58%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 89.86%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 90.13%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 90.38%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 90.85%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 91.07%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 91.13%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 91.34%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 91.53%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 91.71%   [EVAL] batch:   46 | acc: 87.50%,  total acc: 91.62%   [EVAL] batch:   47 | acc: 87.50%,  total acc: 91.54%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 91.71%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 91.75%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 91.79%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 91.83%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 91.86%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 92.01%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 91.93%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 91.96%   [EVAL] batch:   56 | acc: 68.75%,  total acc: 91.56%   [EVAL] batch:   57 | acc: 50.00%,  total acc: 90.84%   [EVAL] batch:   58 | acc: 81.25%,  total acc: 90.68%   [EVAL] batch:   59 | acc: 81.25%,  total acc: 90.52%   [EVAL] batch:   60 | acc: 68.75%,  total acc: 90.16%   [EVAL] batch:   61 | acc: 75.00%,  total acc: 89.92%   [EVAL] batch:   62 | acc: 75.00%,  total acc: 89.68%   [EVAL] batch:   63 | acc: 56.25%,  total acc: 89.16%   [EVAL] batch:   64 | acc: 43.75%,  total acc: 88.46%   [EVAL] batch:   65 | acc: 81.25%,  total acc: 88.35%   [EVAL] batch:   66 | acc: 56.25%,  total acc: 87.87%   [EVAL] batch:   67 | acc: 56.25%,  total acc: 87.41%   [EVAL] batch:   68 | acc: 75.00%,  total acc: 87.23%   [EVAL] batch:   69 | acc: 87.50%,  total acc: 87.23%   [EVAL] batch:   70 | acc: 87.50%,  total acc: 87.24%   [EVAL] batch:   71 | acc: 81.25%,  total acc: 87.15%   [EVAL] batch:   72 | acc: 93.75%,  total acc: 87.24%   [EVAL] batch:   73 | acc: 93.75%,  total acc: 87.33%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 87.33%   [EVAL] batch:   75 | acc: 31.25%,  total acc: 86.60%   [EVAL] batch:   76 | acc: 25.00%,  total acc: 85.80%   [EVAL] batch:   77 | acc: 18.75%,  total acc: 84.94%   [EVAL] batch:   78 | acc: 37.50%,  total acc: 84.34%   [EVAL] batch:   79 | acc: 43.75%,  total acc: 83.83%   [EVAL] batch:   80 | acc: 25.00%,  total acc: 83.10%   [EVAL] batch:   81 | acc: 68.75%,  total acc: 82.93%   [EVAL] batch:   82 | acc: 100.00%,  total acc: 83.13%   [EVAL] batch:   83 | acc: 100.00%,  total acc: 83.33%   [EVAL] batch:   84 | acc: 93.75%,  total acc: 83.46%   [EVAL] batch:   85 | acc: 100.00%,  total acc: 83.65%   [EVAL] batch:   86 | acc: 93.75%,  total acc: 83.76%   [EVAL] batch:   87 | acc: 93.75%,  total acc: 83.88%   [EVAL] batch:   88 | acc: 56.25%,  total acc: 83.57%   [EVAL] batch:   89 | acc: 62.50%,  total acc: 83.33%   [EVAL] batch:   90 | acc: 50.00%,  total acc: 82.97%   [EVAL] batch:   91 | acc: 50.00%,  total acc: 82.61%   [EVAL] batch:   92 | acc: 12.50%,  total acc: 81.85%   [EVAL] batch:   93 | acc: 68.75%,  total acc: 81.72%   [EVAL] batch:   94 | acc: 87.50%,  total acc: 81.78%   [EVAL] batch:   95 | acc: 75.00%,  total acc: 81.71%   [EVAL] batch:   96 | acc: 87.50%,  total acc: 81.77%   [EVAL] batch:   97 | acc: 81.25%,  total acc: 81.76%   [EVAL] batch:   98 | acc: 68.75%,  total acc: 81.63%   [EVAL] batch:   99 | acc: 81.25%,  total acc: 81.62%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 81.81%   [EVAL] batch:  101 | acc: 93.75%,  total acc: 81.92%   [EVAL] batch:  102 | acc: 87.50%,  total acc: 81.98%   [EVAL] batch:  103 | acc: 93.75%,  total acc: 82.09%   [EVAL] batch:  104 | acc: 100.00%,  total acc: 82.26%   [EVAL] batch:  105 | acc: 100.00%,  total acc: 82.43%   [EVAL] batch:  106 | acc: 25.00%,  total acc: 81.89%   [EVAL] batch:  107 | acc: 12.50%,  total acc: 81.25%   [EVAL] batch:  108 | acc: 31.25%,  total acc: 80.79%   [EVAL] batch:  109 | acc: 12.50%,  total acc: 80.17%   [EVAL] batch:  110 | acc: 25.00%,  total acc: 79.67%   [EVAL] batch:  111 | acc: 31.25%,  total acc: 79.24%   [EVAL] batch:  112 | acc: 50.00%,  total acc: 78.98%   [EVAL] batch:  113 | acc: 68.75%,  total acc: 78.89%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 79.08%   [EVAL] batch:  115 | acc: 75.00%,  total acc: 79.04%   [EVAL] batch:  116 | acc: 81.25%,  total acc: 79.06%   [EVAL] batch:  117 | acc: 81.25%,  total acc: 79.08%   [EVAL] batch:  118 | acc: 75.00%,  total acc: 79.04%   [EVAL] batch:  119 | acc: 81.25%,  total acc: 79.06%   [EVAL] batch:  120 | acc: 68.75%,  total acc: 78.98%   [EVAL] batch:  121 | acc: 75.00%,  total acc: 78.94%   [EVAL] batch:  122 | acc: 50.00%,  total acc: 78.71%   [EVAL] batch:  123 | acc: 75.00%,  total acc: 78.68%   [EVAL] batch:  124 | acc: 68.75%,  total acc: 78.60%   [EVAL] batch:  125 | acc: 31.25%,  total acc: 78.22%   [EVAL] batch:  126 | acc: 50.00%,  total acc: 78.00%   [EVAL] batch:  127 | acc: 62.50%,  total acc: 77.88%   [EVAL] batch:  128 | acc: 31.25%,  total acc: 77.52%   [EVAL] batch:  129 | acc: 50.00%,  total acc: 77.31%   [EVAL] batch:  130 | acc: 37.50%,  total acc: 77.00%   [EVAL] batch:  131 | acc: 75.00%,  total acc: 76.99%   [EVAL] batch:  132 | acc: 75.00%,  total acc: 76.97%   [EVAL] batch:  133 | acc: 87.50%,  total acc: 77.05%   [EVAL] batch:  134 | acc: 93.75%,  total acc: 77.18%   [EVAL] batch:  135 | acc: 87.50%,  total acc: 77.25%   [EVAL] batch:  136 | acc: 81.25%,  total acc: 77.28%   [EVAL] batch:  137 | acc: 81.25%,  total acc: 77.31%   [EVAL] batch:  138 | acc: 62.50%,  total acc: 77.20%   [EVAL] batch:  139 | acc: 81.25%,  total acc: 77.23%   [EVAL] batch:  140 | acc: 68.75%,  total acc: 77.17%   [EVAL] batch:  141 | acc: 68.75%,  total acc: 77.11%   [EVAL] batch:  142 | acc: 68.75%,  total acc: 77.05%   [EVAL] batch:  143 | acc: 75.00%,  total acc: 77.04%   [EVAL] batch:  144 | acc: 100.00%,  total acc: 77.20%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 77.35%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 77.47%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 77.62%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 77.77%   [EVAL] batch:  149 | acc: 87.50%,  total acc: 77.83%   [EVAL] batch:  150 | acc: 100.00%,  total acc: 77.98%   [EVAL] batch:  151 | acc: 100.00%,  total acc: 78.12%   [EVAL] batch:  152 | acc: 100.00%,  total acc: 78.27%   [EVAL] batch:  153 | acc: 100.00%,  total acc: 78.41%   [EVAL] batch:  154 | acc: 93.75%,  total acc: 78.51%   [EVAL] batch:  155 | acc: 87.50%,  total acc: 78.57%   [EVAL] batch:  156 | acc: 93.75%,  total acc: 78.66%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 78.76%   [EVAL] batch:  158 | acc: 93.75%,  total acc: 78.85%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 78.95%   [EVAL] batch:  160 | acc: 93.75%,  total acc: 79.04%   [EVAL] batch:  161 | acc: 87.50%,  total acc: 79.09%   [EVAL] batch:  162 | acc: 62.50%,  total acc: 78.99%   [EVAL] batch:  163 | acc: 37.50%,  total acc: 78.73%   [EVAL] batch:  164 | acc: 37.50%,  total acc: 78.48%   [EVAL] batch:  165 | acc: 50.00%,  total acc: 78.31%   [EVAL] batch:  166 | acc: 50.00%,  total acc: 78.14%   [EVAL] batch:  167 | acc: 37.50%,  total acc: 77.90%   [EVAL] batch:  168 | acc: 68.75%,  total acc: 77.85%   [EVAL] batch:  169 | acc: 100.00%,  total acc: 77.98%   [EVAL] batch:  170 | acc: 87.50%,  total acc: 78.03%   [EVAL] batch:  171 | acc: 100.00%,  total acc: 78.16%   [EVAL] batch:  172 | acc: 100.00%,  total acc: 78.29%   [EVAL] batch:  173 | acc: 87.50%,  total acc: 78.34%   [EVAL] batch:  174 | acc: 93.75%,  total acc: 78.43%   [EVAL] batch:  175 | acc: 50.00%,  total acc: 78.27%   [EVAL] batch:  176 | acc: 43.75%,  total acc: 78.07%   [EVAL] batch:  177 | acc: 50.00%,  total acc: 77.91%   [EVAL] batch:  178 | acc: 50.00%,  total acc: 77.76%   [EVAL] batch:  179 | acc: 50.00%,  total acc: 77.60%   [EVAL] batch:  180 | acc: 43.75%,  total acc: 77.42%   [EVAL] batch:  181 | acc: 56.25%,  total acc: 77.30%   [EVAL] batch:  182 | acc: 87.50%,  total acc: 77.36%   [EVAL] batch:  183 | acc: 43.75%,  total acc: 77.17%   [EVAL] batch:  184 | acc: 87.50%,  total acc: 77.23%   [EVAL] batch:  185 | acc: 75.00%,  total acc: 77.22%   [EVAL] batch:  186 | acc: 93.75%,  total acc: 77.31%   [EVAL] batch:  187 | acc: 43.75%,  total acc: 77.13%   
cur_acc:  ['0.9484', '0.7232', '0.7421']
his_acc:  ['0.9484', '0.8190', '0.7713']
Clustering into  19  clusters
Clusters:  [ 0  3 12  2  1  1 12  6  8  8  0 10  4 14  4  3 16  5  6  2 11  7  9  5
  5  7  9 13  8 13  3 13  6 10 17 15 18  9  2  9]
Losses:  13.234155654907227 6.167872905731201 0.8396907448768616
CurrentTrain: epoch  0, batch     0 | loss: 13.2341557Losses:  10.122594833374023 4.129362106323242 0.7703608274459839
CurrentTrain: epoch  0, batch     1 | loss: 10.1225948Losses:  11.495213508605957 4.405755996704102 0.6524274349212646
CurrentTrain: epoch  0, batch     2 | loss: 11.4952135Losses:  5.406188488006592 -0.0 0.10769762843847275
CurrentTrain: epoch  0, batch     3 | loss: 5.4061885Losses:  7.880300521850586 2.0339550971984863 0.8053956627845764
CurrentTrain: epoch  1, batch     0 | loss: 7.8803005Losses:  10.227745056152344 3.7761032581329346 0.7135462164878845
CurrentTrain: epoch  1, batch     1 | loss: 10.2277451Losses:  8.265497207641602 2.5916171073913574 0.7130582928657532
CurrentTrain: epoch  1, batch     2 | loss: 8.2654972Losses:  4.533362865447998 -0.0 0.23333343863487244
CurrentTrain: epoch  1, batch     3 | loss: 4.5333629Losses:  10.323201179504395 4.523681163787842 0.7504833340644836
CurrentTrain: epoch  2, batch     0 | loss: 10.3232012Losses:  9.115448951721191 3.846172571182251 0.5126485228538513
CurrentTrain: epoch  2, batch     1 | loss: 9.1154490Losses:  8.461610794067383 3.4214658737182617 0.6585181355476379
CurrentTrain: epoch  2, batch     2 | loss: 8.4616108Losses:  5.020867347717285 -0.0 0.11637929826974869
CurrentTrain: epoch  2, batch     3 | loss: 5.0208673Losses:  7.924628734588623 3.3265881538391113 0.7670879364013672
CurrentTrain: epoch  3, batch     0 | loss: 7.9246287Losses:  9.17536735534668 3.865762233734131 0.5764792561531067
CurrentTrain: epoch  3, batch     1 | loss: 9.1753674Losses:  10.10537338256836 4.483117580413818 0.6166111826896667
CurrentTrain: epoch  3, batch     2 | loss: 10.1053734Losses:  2.550278663635254 -0.0 0.13305771350860596
CurrentTrain: epoch  3, batch     3 | loss: 2.5502787Losses:  7.583652496337891 2.743748426437378 0.6876792907714844
CurrentTrain: epoch  4, batch     0 | loss: 7.5836525Losses:  9.561607360839844 3.9408299922943115 0.6223865747451782
CurrentTrain: epoch  4, batch     1 | loss: 9.5616074Losses:  7.389012336730957 2.7105863094329834 0.7553470134735107
CurrentTrain: epoch  4, batch     2 | loss: 7.3890123Losses:  3.30293345451355 -0.0 0.21812227368354797
CurrentTrain: epoch  4, batch     3 | loss: 3.3029335Losses:  7.760831356048584 2.919647216796875 0.6012187004089355
CurrentTrain: epoch  5, batch     0 | loss: 7.7608314Losses:  6.825414180755615 2.8303284645080566 0.6768254041671753
CurrentTrain: epoch  5, batch     1 | loss: 6.8254142Losses:  8.19360637664795 3.2521631717681885 0.598555862903595
CurrentTrain: epoch  5, batch     2 | loss: 8.1936064Losses:  4.267636775970459 -0.0 0.10935304313898087
CurrentTrain: epoch  5, batch     3 | loss: 4.2676368Losses:  7.633272171020508 3.3810606002807617 0.6605744957923889
CurrentTrain: epoch  6, batch     0 | loss: 7.6332722Losses:  7.509090900421143 2.470705986022949 0.7478764653205872
CurrentTrain: epoch  6, batch     1 | loss: 7.5090909Losses:  6.554041862487793 2.4492573738098145 0.7562084794044495
CurrentTrain: epoch  6, batch     2 | loss: 6.5540419Losses:  3.2781245708465576 -0.0 0.14665010571479797
CurrentTrain: epoch  6, batch     3 | loss: 3.2781246Losses:  6.671744346618652 2.2564468383789062 0.7379184365272522
CurrentTrain: epoch  7, batch     0 | loss: 6.6717443Losses:  7.437906742095947 2.858259916305542 0.6593319177627563
CurrentTrain: epoch  7, batch     1 | loss: 7.4379067Losses:  6.67765998840332 2.702470302581787 0.7321868538856506
CurrentTrain: epoch  7, batch     2 | loss: 6.6776600Losses:  2.4887187480926514 -0.0 0.11321287602186203
CurrentTrain: epoch  7, batch     3 | loss: 2.4887187Losses:  7.886295795440674 3.861956834793091 0.5963312387466431
CurrentTrain: epoch  8, batch     0 | loss: 7.8862958Losses:  6.322033882141113 2.8177742958068848 0.6406872272491455
CurrentTrain: epoch  8, batch     1 | loss: 6.3220339Losses:  6.2697014808654785 2.14251971244812 0.6970850825309753
CurrentTrain: epoch  8, batch     2 | loss: 6.2697015Losses:  1.9885523319244385 -0.0 0.159195676445961
CurrentTrain: epoch  8, batch     3 | loss: 1.9885523Losses:  8.012316703796387 3.7840681076049805 0.6126207709312439
CurrentTrain: epoch  9, batch     0 | loss: 8.0123167Losses:  7.298756122589111 4.522762298583984 0.5722032785415649
CurrentTrain: epoch  9, batch     1 | loss: 7.2987561Losses:  7.072834014892578 3.213344097137451 0.6561638116836548
CurrentTrain: epoch  9, batch     2 | loss: 7.0728340Losses:  1.7315293550491333 -0.0 0.09361154586076736
CurrentTrain: epoch  9, batch     3 | loss: 1.7315294
#### applies to jurisdiction ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "applies to jurisdiction" means the item (institution, law, public office, public register...) or statement belongs to or has power over or applies to the value (a territorial jurisdiction: a country, state, municipality, ...).
Here is an example:
Relation: applies to jurisdiction
Context: george lawson sheldon ( may 31 , 1870 – april 4 , 1960 ) was 14th governor of nebraska with service from 1907 until 1909 .
Head Entity: governor
Tail Entity: nebraska
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "applies to jurisdiction":
Relation: applies to jurisdiction  
Context: The Environmental Protection Agency (EPA) is responsible for enforcing regulations related to environmental protection in the United States.  
Head Entity: Environmental Protection Agency  
Tail Entity: United States  

Relation: applies to jurisdiction  
Context: The California Consumer Privacy Act (CCPA) provides residents of California with specific rights regarding their personal information.  
Head Entity: California Consumer Privacy Act  
Tail Entity: California  
#### performer ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "performer" means actor, musician, band or other performer associated with this role or musical work.
Here is an example:
Relation: performer
Context: it is prominently featured on jean michel jarre 's albums " oxygène " ( 1977 ) and " équinoxe " ( 1978 ) .
Head Entity: équinoxe
Tail Entity: jean michel jarre
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "performer":
Relation: performer  
Context: The legendary band Queen is known for their iconic song "Bohemian Rhapsody," which showcases Freddie Mercury's extraordinary vocal talent.  
Head Entity: Bohemian Rhapsody  
Tail Entity: Queen  

Relation: performer  
Context: Taylor Swift captivated audiences with her heartfelt performance of "Love Story" during the concert last summer.  
Head Entity: Love Story  
Tail Entity: Taylor Swift  
#### manufacturer ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "manufacturer" means manufacturer or producer of this product.
Here is an example:
Relation: manufacturer
Context: an emd gp49 is a 4-axle diesel locomotive built by general motors electro - motive division .
Head Entity: emd gp49
Tail Entity: general motors electro - motive division
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "manufacturer":
Relation: manufacturer  
Context: the iphone 13 is a smartphone designed and marketed by apple inc.  
Head Entity: iphone 13  
Tail Entity: apple inc.  

Relation: manufacturer  
Context: the model s is an all-electric sedan produced by tesla, inc.  
Head Entity: model s  
Tail Entity: tesla, inc.  
#### participant ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "participant" means person, group of people or organization (object) that actively takes/took part in an event or process.
Here is an example:
Relation: participant
Context: in contrast to gwladys and roger 's allegiance to the house of lancaster , their three sons were staunch yorkists during the wars of the roses .
Head Entity: wars of the roses
Tail Entity: house of lancaster
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "participant":
Relation: participant  
Context: During the annual tech conference, several leading companies showcased their latest innovations, with Google and Microsoft being the most prominent participants.  
Head Entity: tech conference  
Tail Entity: Google  

Relation: participant  
Context: The local community organized a charity run, where various schools and organizations, including the Red Cross, participated to raise funds for the homeless.  
Head Entity: charity run  
Tail Entity: Red Cross  
#### operating system ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "operating system" means operating system (OS) on which a software works or the OS installed on hardware.
Here is an example:
Relation: operating system
Context: the software is written in python and works on all the platforms supported by blender : windows , osx and linux .
Head Entity: blender
Tail Entity: windows
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "operating system":
Relation: operating system  
Context: The latest version of the application is compatible with various operating systems, including macOS, Windows, and Ubuntu.  
Head Entity: application  
Tail Entity: macOS  

Relation: operating system  
Context: Many developers prefer to use Linux as their primary operating system due to its flexibility and open-source nature.  
Head Entity: developers  
Tail Entity: Linux  
#### competition class ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "competition class" means official classification by a regulating body under which the subject (events, teams, participants, or equipment) qualifies for inclusion.
Here is an example:
Relation: competition class
Context: in his next bout , he won the wbc light welterweight title by defeating roger mayweather for a second time .
Head Entity: roger mayweather
Tail Entity: light welterweight
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "competition class":
Relation: competition class  
Context: The team secured their place in the tournament by qualifying under the UEFA Champions League regulations.  
Head Entity: UEFA Champions League  
Tail Entity: tournament  

Relation: competition class  
Context: The athlete competed in the 100 meters event, which is classified under the Olympic Games standards.  
Head Entity: Olympic Games  
Tail Entity: 100 meters event  
#### successful candidate ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "successful candidate" means person elected after the election.
Here is an example:
Relation: successful candidate
Context: the qup gained 3 seats from its previous election result , and won a 10 % swing to it in primary votes , but failed to defeat the incumbent labor government led by ted theodore .
Head Entity: previous election
Tail Entity: ted theodore
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "successful candidate":
Relation: successful candidate  
Context: After a closely contested campaign, the voters chose Sarah Johnson as their representative, marking a significant shift in the local political landscape.  
Head Entity: closely contested campaign  
Tail Entity: Sarah Johnson  

Relation: successful candidate  
Context: In the recent mayoral election, the community rallied behind Mark Thompson, who promised to bring new ideas and energy to the office.  
Head Entity: recent mayoral election  
Tail Entity: Mark Thompson  
#### sibling ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "sibling" means the subject and the object have the same parents.
Here is an example:
Relation: sibling
Context: the deaths of his brothers wenceslaus ii ( 1487 ) , casimir ii ( 1490 ) and władysław ( 1494 ) allowed jan v to reunificated the whole duchy of zator .
Head Entity: casimir ii
Tail Entity: wenceslaus ii
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "sibling":
Relation: sibling  
Context: both elizabeth and her brother, charles, were known for their contributions to the arts and culture of their time.  
Head Entity: elizabeth  
Tail Entity: charles  

Relation: sibling  
Context: during the family reunion, it was heartwarming to see how much john and his sister, sarah, resembled each other in both looks and personality.  
Head Entity: john  
Tail Entity: sarah  
#### military branch ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "military branch" means branch to which this military unit, award, office, or person belongs.
Here is an example:
Relation: military branch
Context: iain harrison is a competitive shooter and former british army captain .
Head Entity: iain harrison
Tail Entity: british army
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "military branch":
Relation: military branch  
Context: sergeant johnson served in the united states marine corps during his military career.  
Head Entity: sergeant johnson  
Tail Entity: united states marine corps  

Relation: military branch  
Context: general smith was a prominent figure in the royal air force, leading several key missions.  
Head Entity: general smith  
Tail Entity: royal air force  
#### child ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "child" means subject has object as child.
Here is an example:
Relation: child
Context: ziad rahbani is the son of the lebanese famous composer assi rahbani and nouhad haddad , the famous lebanese female singer known as fairuz .
Head Entity: assi rahbani
Tail Entity: ziad rahbani
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "child":
Relation: child  
Context: in the animated series, the character of simon is the son of the adventurous couple, marie and tom.  
Head Entity: marie  
Tail Entity: simon  

Relation: child  
Context: the renowned physicist albert einstein had a daughter named lieserl, who was born in 1902.  
Head Entity: albert einstein  
Tail Entity: lieserl  
Losses:  5.5265631675720215 0.8508023023605347 0.929129958152771
MemoryTrain:  epoch  0, batch     0 | loss: 5.5265632Losses:  4.80912971496582 0.257419228553772 0.8679282069206238
MemoryTrain:  epoch  0, batch     1 | loss: 4.8091297Losses:  5.490276336669922 0.27255839109420776 0.9417600631713867
MemoryTrain:  epoch  0, batch     2 | loss: 5.4902763Losses:  6.245871067047119 1.1236040592193604 0.8333466649055481
MemoryTrain:  epoch  0, batch     3 | loss: 6.2458711Losses:  5.05539083480835 0.816242516040802 0.81648188829422
MemoryTrain:  epoch  0, batch     4 | loss: 5.0553908Losses:  5.4682722091674805 1.6440880298614502 0.6523582339286804
MemoryTrain:  epoch  0, batch     5 | loss: 5.4682722Losses:  6.279099941253662 0.8634264469146729 0.9686684608459473
MemoryTrain:  epoch  0, batch     6 | loss: 6.2790999Losses:  3.723590850830078 -0.0 0.6871361136436462
MemoryTrain:  epoch  0, batch     7 | loss: 3.7235909Losses:  3.9128270149230957 0.5484409332275391 0.7394182682037354
MemoryTrain:  epoch  1, batch     0 | loss: 3.9128270Losses:  4.1712646484375 0.23364633321762085 0.859717845916748
MemoryTrain:  epoch  1, batch     1 | loss: 4.1712646Losses:  5.422815799713135 0.4644496738910675 0.835324227809906
MemoryTrain:  epoch  1, batch     2 | loss: 5.4228158Losses:  4.53939151763916 0.500137209892273 0.9526687860488892
MemoryTrain:  epoch  1, batch     3 | loss: 4.5393915Losses:  5.700254440307617 1.088883876800537 0.8859357833862305
MemoryTrain:  epoch  1, batch     4 | loss: 5.7002544Losses:  4.335960388183594 0.28946271538734436 0.9068979024887085
MemoryTrain:  epoch  1, batch     5 | loss: 4.3359604Losses:  4.27923059463501 0.8173674941062927 0.8034361004829407
MemoryTrain:  epoch  1, batch     6 | loss: 4.2792306Losses:  4.032532691955566 -0.0 0.6547005772590637
MemoryTrain:  epoch  1, batch     7 | loss: 4.0325327Losses:  5.08675479888916 0.5523196458816528 0.9383524656295776
MemoryTrain:  epoch  2, batch     0 | loss: 5.0867548Losses:  3.682220458984375 0.8239490985870361 0.6544665098190308
MemoryTrain:  epoch  2, batch     1 | loss: 3.6822205Losses:  3.962751865386963 0.26873576641082764 0.9118014574050903
MemoryTrain:  epoch  2, batch     2 | loss: 3.9627519Losses:  3.5387637615203857 0.2927802801132202 0.789895236492157
MemoryTrain:  epoch  2, batch     3 | loss: 3.5387638Losses:  3.609219789505005 0.3332914710044861 0.8456584811210632
MemoryTrain:  epoch  2, batch     4 | loss: 3.6092198Losses:  3.4645423889160156 0.27048468589782715 0.9145957827568054
MemoryTrain:  epoch  2, batch     5 | loss: 3.4645424Losses:  4.016426086425781 0.5562354326248169 0.7927501797676086
MemoryTrain:  epoch  2, batch     6 | loss: 4.0164261Losses:  2.249354124069214 -0.0 0.4654720425605774
MemoryTrain:  epoch  2, batch     7 | loss: 2.2493541Losses:  3.5491507053375244 0.9043088555335999 0.7869866490364075
MemoryTrain:  epoch  3, batch     0 | loss: 3.5491507Losses:  3.4130074977874756 0.2495494931936264 0.9473366737365723
MemoryTrain:  epoch  3, batch     1 | loss: 3.4130075Losses:  3.7541792392730713 0.3499937057495117 0.7384000420570374
MemoryTrain:  epoch  3, batch     2 | loss: 3.7541792Losses:  3.6781227588653564 0.2646632790565491 0.925517737865448
MemoryTrain:  epoch  3, batch     3 | loss: 3.6781228Losses:  2.879497528076172 -0.0 0.9561963677406311
MemoryTrain:  epoch  3, batch     4 | loss: 2.8794975Losses:  4.453507900238037 0.631573498249054 0.9345287680625916
MemoryTrain:  epoch  3, batch     5 | loss: 4.4535079Losses:  2.8771162033081055 0.4849080741405487 0.9382195472717285
MemoryTrain:  epoch  3, batch     6 | loss: 2.8771162Losses:  2.5795812606811523 -0.0 0.6225321292877197
MemoryTrain:  epoch  3, batch     7 | loss: 2.5795813Losses:  3.8580899238586426 0.5648903846740723 0.6788341999053955
MemoryTrain:  epoch  4, batch     0 | loss: 3.8580899Losses:  3.816211223602295 0.596230149269104 0.842593789100647
MemoryTrain:  epoch  4, batch     1 | loss: 3.8162112Losses:  2.9127941131591797 0.2379077821969986 0.8934616446495056
MemoryTrain:  epoch  4, batch     2 | loss: 2.9127941Losses:  3.2850847244262695 0.775658130645752 0.8820887804031372
MemoryTrain:  epoch  4, batch     3 | loss: 3.2850847Losses:  2.7549872398376465 -0.0 0.9775581359863281
MemoryTrain:  epoch  4, batch     4 | loss: 2.7549872Losses:  3.031095266342163 0.5130499601364136 0.9170125126838684
MemoryTrain:  epoch  4, batch     5 | loss: 3.0310953Losses:  3.6604981422424316 0.5232908725738525 0.9214069247245789
MemoryTrain:  epoch  4, batch     6 | loss: 3.6604981Losses:  2.747946262359619 0.2722613215446472 0.5308929681777954
MemoryTrain:  epoch  4, batch     7 | loss: 2.7479463Losses:  2.661088705062866 0.28527313470840454 0.9330469965934753
MemoryTrain:  epoch  5, batch     0 | loss: 2.6610887Losses:  3.4505770206451416 0.6103278398513794 0.9157254695892334
MemoryTrain:  epoch  5, batch     1 | loss: 3.4505770Losses:  3.6211462020874023 0.4937387704849243 0.8444420099258423
MemoryTrain:  epoch  5, batch     2 | loss: 3.6211462Losses:  3.786325216293335 1.0836732387542725 0.7392714023590088
MemoryTrain:  epoch  5, batch     3 | loss: 3.7863252Losses:  3.1984331607818604 0.22629791498184204 0.9375255107879639
MemoryTrain:  epoch  5, batch     4 | loss: 3.1984332Losses:  3.095181941986084 0.53138267993927 0.8721039891242981
MemoryTrain:  epoch  5, batch     5 | loss: 3.0951819Losses:  3.023287057876587 0.7568875551223755 0.8111678957939148
MemoryTrain:  epoch  5, batch     6 | loss: 3.0232871Losses:  3.317765712738037 -0.0 0.544435977935791
MemoryTrain:  epoch  5, batch     7 | loss: 3.3177657Losses:  2.457254409790039 -0.0 0.9171031713485718
MemoryTrain:  epoch  6, batch     0 | loss: 2.4572544Losses:  2.927621841430664 0.3359626531600952 0.7666999101638794
MemoryTrain:  epoch  6, batch     1 | loss: 2.9276218Losses:  2.7148709297180176 -0.0 0.8832893371582031
MemoryTrain:  epoch  6, batch     2 | loss: 2.7148709Losses:  2.6078619956970215 0.4448140859603882 0.8498128056526184
MemoryTrain:  epoch  6, batch     3 | loss: 2.6078620Losses:  3.4636077880859375 0.5991083383560181 0.9381929636001587
MemoryTrain:  epoch  6, batch     4 | loss: 3.4636078Losses:  4.309776782989502 1.0058691501617432 0.9602383971214294
MemoryTrain:  epoch  6, batch     5 | loss: 4.3097768Losses:  3.2252893447875977 0.5288516283035278 0.8848551511764526
MemoryTrain:  epoch  6, batch     6 | loss: 3.2252893Losses:  2.1678309440612793 -0.0 0.679567277431488
MemoryTrain:  epoch  6, batch     7 | loss: 2.1678309Losses:  2.4419145584106445 0.23989693820476532 0.7106195092201233
MemoryTrain:  epoch  7, batch     0 | loss: 2.4419146Losses:  3.451099395751953 1.116984486579895 0.8493208885192871
MemoryTrain:  epoch  7, batch     1 | loss: 3.4510994Losses:  2.7626872062683105 -0.0 0.9208353757858276
MemoryTrain:  epoch  7, batch     2 | loss: 2.7626872Losses:  2.9717345237731934 0.5266952514648438 0.8417278528213501
MemoryTrain:  epoch  7, batch     3 | loss: 2.9717345Losses:  3.2302260398864746 0.9691951274871826 0.8025237321853638
MemoryTrain:  epoch  7, batch     4 | loss: 3.2302260Losses:  3.7086143493652344 1.2516216039657593 0.8255853652954102
MemoryTrain:  epoch  7, batch     5 | loss: 3.7086143Losses:  3.134469509124756 0.5102706551551819 0.9716346263885498
MemoryTrain:  epoch  7, batch     6 | loss: 3.1344695Losses:  2.9666762351989746 0.3252948820590973 0.42129889130592346
MemoryTrain:  epoch  7, batch     7 | loss: 2.9666762Losses:  2.750898838043213 0.26099693775177 0.7759127616882324
MemoryTrain:  epoch  8, batch     0 | loss: 2.7508988Losses:  3.1105313301086426 0.5263442993164062 0.8674544095993042
MemoryTrain:  epoch  8, batch     1 | loss: 3.1105313Losses:  3.359100818634033 0.8600271940231323 0.9193911552429199
MemoryTrain:  epoch  8, batch     2 | loss: 3.3591008Losses:  2.1855969429016113 -0.0 0.9036890268325806
MemoryTrain:  epoch  8, batch     3 | loss: 2.1855969Losses:  3.117886781692505 0.5496432781219482 1.0302822589874268
MemoryTrain:  epoch  8, batch     4 | loss: 3.1178868Losses:  2.8352859020233154 0.2795674800872803 0.931888997554779
MemoryTrain:  epoch  8, batch     5 | loss: 2.8352859Losses:  3.1494662761688232 0.5197439789772034 0.8546566963195801
MemoryTrain:  epoch  8, batch     6 | loss: 3.1494663Losses:  2.060359001159668 -0.0 0.5187140703201294
MemoryTrain:  epoch  8, batch     7 | loss: 2.0603590Losses:  3.147599220275879 0.7684617638587952 0.7532764077186584
MemoryTrain:  epoch  9, batch     0 | loss: 3.1475992Losses:  2.7014365196228027 0.5343092083930969 0.8428068161010742
MemoryTrain:  epoch  9, batch     1 | loss: 2.7014365Losses:  2.72208309173584 0.270512193441391 0.9111777544021606
MemoryTrain:  epoch  9, batch     2 | loss: 2.7220831Losses:  2.6597607135772705 0.2380479872226715 0.9026440382003784
MemoryTrain:  epoch  9, batch     3 | loss: 2.6597607Losses:  2.587178945541382 0.24977056682109833 0.8664481043815613
MemoryTrain:  epoch  9, batch     4 | loss: 2.5871789Losses:  2.4495749473571777 0.2393800914287567 0.8554925918579102
MemoryTrain:  epoch  9, batch     5 | loss: 2.4495749Losses:  3.053776979446411 0.7722963094711304 0.8724381327629089
MemoryTrain:  epoch  9, batch     6 | loss: 3.0537770Losses:  2.654371500015259 -0.0 0.6855807900428772
MemoryTrain:  epoch  9, batch     7 | loss: 2.6543715
[EVAL] batch:    0 | acc: 56.25%,  total acc: 56.25%   [EVAL] batch:    1 | acc: 75.00%,  total acc: 65.62%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 68.75%   [EVAL] batch:    3 | acc: 43.75%,  total acc: 62.50%   [EVAL] batch:    4 | acc: 50.00%,  total acc: 60.00%   [EVAL] batch:    5 | acc: 43.75%,  total acc: 57.29%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 61.61%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 63.28%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 63.89%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 65.00%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 67.05%   [EVAL] batch:   11 | acc: 68.75%,  total acc: 67.19%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 69.23%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 69.20%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 69.58%   [EVAL] batch:   15 | acc: 75.00%,  total acc: 69.92%   [EVAL] batch:   16 | acc: 68.75%,  total acc: 69.85%   [EVAL] batch:   17 | acc: 87.50%,  total acc: 70.83%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 70.72%   [EVAL] batch:   19 | acc: 12.50%,  total acc: 67.81%   [EVAL] batch:   20 | acc: 6.25%,  total acc: 64.88%   [EVAL] batch:   21 | acc: 6.25%,  total acc: 62.22%   [EVAL] batch:   22 | acc: 25.00%,  total acc: 60.60%   [EVAL] batch:   23 | acc: 25.00%,  total acc: 59.11%   [EVAL] batch:   24 | acc: 12.50%,  total acc: 57.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 58.89%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 60.42%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 61.83%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 63.15%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 64.38%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 65.52%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 66.60%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 67.61%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 68.57%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 69.46%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 70.14%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 70.95%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 71.38%   [EVAL] batch:   38 | acc: 62.50%,  total acc: 71.15%   [EVAL] batch:   39 | acc: 56.25%,  total acc: 70.78%   [EVAL] batch:   40 | acc: 68.75%,  total acc: 70.73%   [EVAL] batch:   41 | acc: 81.25%,  total acc: 70.98%   [EVAL] batch:   42 | acc: 81.25%,  total acc: 71.22%   [EVAL] batch:   43 | acc: 68.75%,  total acc: 71.16%   [EVAL] batch:   44 | acc: 62.50%,  total acc: 70.97%   [EVAL] batch:   45 | acc: 37.50%,  total acc: 70.24%   [EVAL] batch:   46 | acc: 50.00%,  total acc: 69.81%   [EVAL] batch:   47 | acc: 68.75%,  total acc: 69.79%   [EVAL] batch:   48 | acc: 56.25%,  total acc: 69.52%   [EVAL] batch:   49 | acc: 62.50%,  total acc: 69.38%   [EVAL] batch:   50 | acc: 87.50%,  total acc: 69.73%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 70.19%   [EVAL] batch:   52 | acc: 87.50%,  total acc: 70.52%   [EVAL] batch:   53 | acc: 75.00%,  total acc: 70.60%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 71.02%   [EVAL] batch:   55 | acc: 68.75%,  total acc: 70.98%   [EVAL] batch:   56 | acc: 56.25%,  total acc: 70.72%   [EVAL] batch:   57 | acc: 50.00%,  total acc: 70.37%   [EVAL] batch:   58 | acc: 25.00%,  total acc: 69.60%   [EVAL] batch:   59 | acc: 37.50%,  total acc: 69.06%   [EVAL] batch:   60 | acc: 37.50%,  total acc: 68.55%   [EVAL] batch:   61 | acc: 37.50%,  total acc: 68.04%   [EVAL] batch:   62 | acc: 25.00%,  total acc: 67.36%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 79.69%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 78.75%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 79.46%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 79.69%   [EVAL] batch:    8 | acc: 75.00%,  total acc: 79.17%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 79.38%   [EVAL] batch:   10 | acc: 62.50%,  total acc: 77.84%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 78.12%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 78.37%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 79.46%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 80.42%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 81.64%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 82.35%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 82.99%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 83.88%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 83.44%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 83.63%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 83.81%   [EVAL] batch:   22 | acc: 75.00%,  total acc: 83.42%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 83.85%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 84.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 84.62%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 85.19%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 85.71%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 85.99%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 86.46%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 86.90%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 87.30%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 87.69%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 88.05%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 88.21%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 88.54%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 88.85%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 89.14%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 89.42%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 89.69%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 89.79%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 90.03%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 90.12%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 90.20%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 90.14%   [EVAL] batch:   45 | acc: 87.50%,  total acc: 90.08%   [EVAL] batch:   46 | acc: 75.00%,  total acc: 89.76%   [EVAL] batch:   47 | acc: 75.00%,  total acc: 89.45%   [EVAL] batch:   48 | acc: 93.75%,  total acc: 89.54%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 89.62%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 89.71%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 89.78%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 89.86%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 90.05%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 90.00%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 90.07%   [EVAL] batch:   56 | acc: 81.25%,  total acc: 89.91%   [EVAL] batch:   57 | acc: 56.25%,  total acc: 89.33%   [EVAL] batch:   58 | acc: 81.25%,  total acc: 89.19%   [EVAL] batch:   59 | acc: 81.25%,  total acc: 89.06%   [EVAL] batch:   60 | acc: 75.00%,  total acc: 88.83%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 88.51%   [EVAL] batch:   62 | acc: 75.00%,  total acc: 88.29%   [EVAL] batch:   63 | acc: 56.25%,  total acc: 87.79%   [EVAL] batch:   64 | acc: 43.75%,  total acc: 87.12%   [EVAL] batch:   65 | acc: 75.00%,  total acc: 86.93%   [EVAL] batch:   66 | acc: 50.00%,  total acc: 86.38%   [EVAL] batch:   67 | acc: 62.50%,  total acc: 86.03%   [EVAL] batch:   68 | acc: 75.00%,  total acc: 85.87%   [EVAL] batch:   69 | acc: 87.50%,  total acc: 85.89%   [EVAL] batch:   70 | acc: 81.25%,  total acc: 85.83%   [EVAL] batch:   71 | acc: 75.00%,  total acc: 85.68%   [EVAL] batch:   72 | acc: 93.75%,  total acc: 85.79%   [EVAL] batch:   73 | acc: 100.00%,  total acc: 85.98%   [EVAL] batch:   74 | acc: 93.75%,  total acc: 86.08%   [EVAL] batch:   75 | acc: 31.25%,  total acc: 85.36%   [EVAL] batch:   76 | acc: 18.75%,  total acc: 84.50%   [EVAL] batch:   77 | acc: 31.25%,  total acc: 83.81%   [EVAL] batch:   78 | acc: 31.25%,  total acc: 83.15%   [EVAL] batch:   79 | acc: 50.00%,  total acc: 82.73%   [EVAL] batch:   80 | acc: 25.00%,  total acc: 82.02%   [EVAL] batch:   81 | acc: 81.25%,  total acc: 82.01%   [EVAL] batch:   82 | acc: 100.00%,  total acc: 82.23%   [EVAL] batch:   83 | acc: 100.00%,  total acc: 82.44%   [EVAL] batch:   84 | acc: 93.75%,  total acc: 82.57%   [EVAL] batch:   85 | acc: 100.00%,  total acc: 82.78%   [EVAL] batch:   86 | acc: 100.00%,  total acc: 82.97%   [EVAL] batch:   87 | acc: 93.75%,  total acc: 83.10%   [EVAL] batch:   88 | acc: 50.00%,  total acc: 82.72%   [EVAL] batch:   89 | acc: 62.50%,  total acc: 82.50%   [EVAL] batch:   90 | acc: 62.50%,  total acc: 82.28%   [EVAL] batch:   91 | acc: 50.00%,  total acc: 81.93%   [EVAL] batch:   92 | acc: 31.25%,  total acc: 81.38%   [EVAL] batch:   93 | acc: 68.75%,  total acc: 81.25%   [EVAL] batch:   94 | acc: 87.50%,  total acc: 81.32%   [EVAL] batch:   95 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:   96 | acc: 87.50%,  total acc: 81.31%   [EVAL] batch:   97 | acc: 81.25%,  total acc: 81.31%   [EVAL] batch:   98 | acc: 68.75%,  total acc: 81.19%   [EVAL] batch:   99 | acc: 81.25%,  total acc: 81.19%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 81.37%   [EVAL] batch:  101 | acc: 93.75%,  total acc: 81.50%   [EVAL] batch:  102 | acc: 81.25%,  total acc: 81.49%   [EVAL] batch:  103 | acc: 100.00%,  total acc: 81.67%   [EVAL] batch:  104 | acc: 100.00%,  total acc: 81.85%   [EVAL] batch:  105 | acc: 100.00%,  total acc: 82.02%   [EVAL] batch:  106 | acc: 31.25%,  total acc: 81.54%   [EVAL] batch:  107 | acc: 12.50%,  total acc: 80.90%   [EVAL] batch:  108 | acc: 25.00%,  total acc: 80.39%   [EVAL] batch:  109 | acc: 12.50%,  total acc: 79.77%   [EVAL] batch:  110 | acc: 25.00%,  total acc: 79.28%   [EVAL] batch:  111 | acc: 25.00%,  total acc: 78.79%   [EVAL] batch:  112 | acc: 25.00%,  total acc: 78.32%   [EVAL] batch:  113 | acc: 31.25%,  total acc: 77.91%   [EVAL] batch:  114 | acc: 62.50%,  total acc: 77.77%   [EVAL] batch:  115 | acc: 56.25%,  total acc: 77.59%   [EVAL] batch:  116 | acc: 43.75%,  total acc: 77.30%   [EVAL] batch:  117 | acc: 50.00%,  total acc: 77.07%   [EVAL] batch:  118 | acc: 87.50%,  total acc: 77.15%   [EVAL] batch:  119 | acc: 93.75%,  total acc: 77.29%   [EVAL] batch:  120 | acc: 75.00%,  total acc: 77.27%   [EVAL] batch:  121 | acc: 81.25%,  total acc: 77.31%   [EVAL] batch:  122 | acc: 62.50%,  total acc: 77.18%   [EVAL] batch:  123 | acc: 75.00%,  total acc: 77.17%   [EVAL] batch:  124 | acc: 81.25%,  total acc: 77.20%   [EVAL] batch:  125 | acc: 37.50%,  total acc: 76.88%   [EVAL] batch:  126 | acc: 25.00%,  total acc: 76.48%   [EVAL] batch:  127 | acc: 37.50%,  total acc: 76.17%   [EVAL] batch:  128 | acc: 31.25%,  total acc: 75.82%   [EVAL] batch:  129 | acc: 31.25%,  total acc: 75.48%   [EVAL] batch:  130 | acc: 18.75%,  total acc: 75.05%   [EVAL] batch:  131 | acc: 50.00%,  total acc: 74.86%   [EVAL] batch:  132 | acc: 68.75%,  total acc: 74.81%   [EVAL] batch:  133 | acc: 68.75%,  total acc: 74.77%   [EVAL] batch:  134 | acc: 87.50%,  total acc: 74.86%   [EVAL] batch:  135 | acc: 87.50%,  total acc: 74.95%   [EVAL] batch:  136 | acc: 75.00%,  total acc: 74.95%   [EVAL] batch:  137 | acc: 56.25%,  total acc: 74.82%   [EVAL] batch:  138 | acc: 18.75%,  total acc: 74.42%   [EVAL] batch:  139 | acc: 31.25%,  total acc: 74.11%   [EVAL] batch:  140 | acc: 25.00%,  total acc: 73.76%   [EVAL] batch:  141 | acc: 31.25%,  total acc: 73.46%   [EVAL] batch:  142 | acc: 37.50%,  total acc: 73.21%   [EVAL] batch:  143 | acc: 50.00%,  total acc: 73.05%   [EVAL] batch:  144 | acc: 100.00%,  total acc: 73.23%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 73.42%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 73.55%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 73.73%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 73.91%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 74.04%   [EVAL] batch:  150 | acc: 100.00%,  total acc: 74.21%   [EVAL] batch:  151 | acc: 100.00%,  total acc: 74.38%   [EVAL] batch:  152 | acc: 100.00%,  total acc: 74.55%   [EVAL] batch:  153 | acc: 100.00%,  total acc: 74.72%   [EVAL] batch:  154 | acc: 100.00%,  total acc: 74.88%   [EVAL] batch:  155 | acc: 87.50%,  total acc: 74.96%   [EVAL] batch:  156 | acc: 100.00%,  total acc: 75.12%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 75.24%   [EVAL] batch:  158 | acc: 93.75%,  total acc: 75.35%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 75.47%   [EVAL] batch:  160 | acc: 93.75%,  total acc: 75.58%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 75.69%   [EVAL] batch:  162 | acc: 50.00%,  total acc: 75.54%   [EVAL] batch:  163 | acc: 37.50%,  total acc: 75.30%   [EVAL] batch:  164 | acc: 25.00%,  total acc: 75.00%   [EVAL] batch:  165 | acc: 31.25%,  total acc: 74.74%   [EVAL] batch:  166 | acc: 37.50%,  total acc: 74.51%   [EVAL] batch:  167 | acc: 18.75%,  total acc: 74.18%   [EVAL] batch:  168 | acc: 56.25%,  total acc: 74.08%   [EVAL] batch:  169 | acc: 100.00%,  total acc: 74.23%   [EVAL] batch:  170 | acc: 87.50%,  total acc: 74.31%   [EVAL] batch:  171 | acc: 100.00%,  total acc: 74.45%   [EVAL] batch:  172 | acc: 93.75%,  total acc: 74.57%   [EVAL] batch:  173 | acc: 81.25%,  total acc: 74.60%   [EVAL] batch:  174 | acc: 93.75%,  total acc: 74.71%   [EVAL] batch:  175 | acc: 50.00%,  total acc: 74.57%   [EVAL] batch:  176 | acc: 50.00%,  total acc: 74.44%   [EVAL] batch:  177 | acc: 43.75%,  total acc: 74.26%   [EVAL] batch:  178 | acc: 50.00%,  total acc: 74.13%   [EVAL] batch:  179 | acc: 56.25%,  total acc: 74.03%   [EVAL] batch:  180 | acc: 31.25%,  total acc: 73.79%   [EVAL] batch:  181 | acc: 56.25%,  total acc: 73.70%   [EVAL] batch:  182 | acc: 68.75%,  total acc: 73.67%   [EVAL] batch:  183 | acc: 56.25%,  total acc: 73.57%   [EVAL] batch:  184 | acc: 81.25%,  total acc: 73.61%   [EVAL] batch:  185 | acc: 87.50%,  total acc: 73.69%   [EVAL] batch:  186 | acc: 93.75%,  total acc: 73.80%   [EVAL] batch:  187 | acc: 68.75%,  total acc: 73.77%   [EVAL] batch:  188 | acc: 68.75%,  total acc: 73.74%   [EVAL] batch:  189 | acc: 68.75%,  total acc: 73.72%   [EVAL] batch:  190 | acc: 75.00%,  total acc: 73.72%   [EVAL] batch:  191 | acc: 50.00%,  total acc: 73.60%   [EVAL] batch:  192 | acc: 43.75%,  total acc: 73.45%   [EVAL] batch:  193 | acc: 56.25%,  total acc: 73.36%   [EVAL] batch:  194 | acc: 87.50%,  total acc: 73.43%   [EVAL] batch:  195 | acc: 68.75%,  total acc: 73.41%   [EVAL] batch:  196 | acc: 56.25%,  total acc: 73.32%   [EVAL] batch:  197 | acc: 87.50%,  total acc: 73.39%   [EVAL] batch:  198 | acc: 87.50%,  total acc: 73.46%   [EVAL] batch:  199 | acc: 81.25%,  total acc: 73.50%   [EVAL] batch:  200 | acc: 81.25%,  total acc: 73.54%   [EVAL] batch:  201 | acc: 75.00%,  total acc: 73.55%   [EVAL] batch:  202 | acc: 68.75%,  total acc: 73.52%   [EVAL] batch:  203 | acc: 75.00%,  total acc: 73.53%   [EVAL] batch:  204 | acc: 68.75%,  total acc: 73.51%   [EVAL] batch:  205 | acc: 93.75%,  total acc: 73.60%   [EVAL] batch:  206 | acc: 25.00%,  total acc: 73.37%   [EVAL] batch:  207 | acc: 18.75%,  total acc: 73.11%   [EVAL] batch:  208 | acc: 6.25%,  total acc: 72.79%   [EVAL] batch:  209 | acc: 12.50%,  total acc: 72.50%   [EVAL] batch:  210 | acc: 18.75%,  total acc: 72.25%   [EVAL] batch:  211 | acc: 31.25%,  total acc: 72.05%   [EVAL] batch:  212 | acc: 50.00%,  total acc: 71.95%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 72.08%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 72.21%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 72.34%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 72.47%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 72.59%   [EVAL] batch:  218 | acc: 100.00%,  total acc: 72.72%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 72.84%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 72.96%   [EVAL] batch:  221 | acc: 100.00%,  total acc: 73.09%   [EVAL] batch:  222 | acc: 93.75%,  total acc: 73.18%   [EVAL] batch:  223 | acc: 100.00%,  total acc: 73.30%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 73.42%   [EVAL] batch:  225 | acc: 68.75%,  total acc: 73.40%   [EVAL] batch:  226 | acc: 62.50%,  total acc: 73.35%   [EVAL] batch:  227 | acc: 62.50%,  total acc: 73.30%   [EVAL] batch:  228 | acc: 68.75%,  total acc: 73.28%   [EVAL] batch:  229 | acc: 93.75%,  total acc: 73.37%   [EVAL] batch:  230 | acc: 62.50%,  total acc: 73.32%   [EVAL] batch:  231 | acc: 68.75%,  total acc: 73.30%   [EVAL] batch:  232 | acc: 43.75%,  total acc: 73.18%   [EVAL] batch:  233 | acc: 56.25%,  total acc: 73.10%   [EVAL] batch:  234 | acc: 50.00%,  total acc: 73.01%   [EVAL] batch:  235 | acc: 68.75%,  total acc: 72.99%   [EVAL] batch:  236 | acc: 50.00%,  total acc: 72.89%   [EVAL] batch:  237 | acc: 81.25%,  total acc: 72.93%   [EVAL] batch:  238 | acc: 87.50%,  total acc: 72.99%   [EVAL] batch:  239 | acc: 100.00%,  total acc: 73.10%   [EVAL] batch:  240 | acc: 81.25%,  total acc: 73.13%   [EVAL] batch:  241 | acc: 81.25%,  total acc: 73.17%   [EVAL] batch:  242 | acc: 81.25%,  total acc: 73.20%   [EVAL] batch:  243 | acc: 75.00%,  total acc: 73.21%   [EVAL] batch:  244 | acc: 37.50%,  total acc: 73.06%   [EVAL] batch:  245 | acc: 50.00%,  total acc: 72.97%   [EVAL] batch:  246 | acc: 6.25%,  total acc: 72.70%   [EVAL] batch:  247 | acc: 50.00%,  total acc: 72.61%   [EVAL] batch:  248 | acc: 31.25%,  total acc: 72.44%   [EVAL] batch:  249 | acc: 50.00%,  total acc: 72.35%   
cur_acc:  ['0.9484', '0.7232', '0.7421', '0.6736']
his_acc:  ['0.9484', '0.8190', '0.7713', '0.7235']
Clustering into  24  clusters
Clusters:  [23 10 13  2  0  0 13  1  5  5  9 21  6  3  6  7 18 14  1  2 11 16 20 22
 14 16 20  4  5  4 10  4  1 21  8 15 17 20  2 20  2 12  3  5 19  9 10  7
  8 10]
Losses:  10.239234924316406 4.196279048919678 0.6627157330513
CurrentTrain: epoch  0, batch     0 | loss: 10.2392349Losses:  10.098974227905273 3.939452648162842 0.6406635642051697
CurrentTrain: epoch  0, batch     1 | loss: 10.0989742Losses:  8.839876174926758 2.314833164215088 0.7939476370811462
CurrentTrain: epoch  0, batch     2 | loss: 8.8398762Losses:  3.040257453918457 -0.0 0.10070037841796875
CurrentTrain: epoch  0, batch     3 | loss: 3.0402575Losses:  8.778167724609375 3.615936040878296 0.7963312864303589
CurrentTrain: epoch  1, batch     0 | loss: 8.7781677Losses:  8.260430335998535 2.748577117919922 0.757986843585968
CurrentTrain: epoch  1, batch     1 | loss: 8.2604303Losses:  8.099390983581543 3.2096385955810547 0.6468984484672546
CurrentTrain: epoch  1, batch     2 | loss: 8.0993910Losses:  3.784435749053955 -0.0 0.10315641760826111
CurrentTrain: epoch  1, batch     3 | loss: 3.7844357Losses:  6.346824645996094 2.0756938457489014 0.6865204572677612
CurrentTrain: epoch  2, batch     0 | loss: 6.3468246Losses:  9.18039321899414 4.5377349853515625 0.6012434363365173
CurrentTrain: epoch  2, batch     1 | loss: 9.1803932Losses:  7.1521196365356445 3.1148486137390137 0.6614177227020264
CurrentTrain: epoch  2, batch     2 | loss: 7.1521196Losses:  2.2211556434631348 -0.0 0.11176545172929764
CurrentTrain: epoch  2, batch     3 | loss: 2.2211556Losses:  7.963769912719727 4.46412992477417 0.5141069889068604
CurrentTrain: epoch  3, batch     0 | loss: 7.9637699Losses:  7.041038513183594 3.323845386505127 0.650476336479187
CurrentTrain: epoch  3, batch     1 | loss: 7.0410385Losses:  6.957174301147461 2.8734207153320312 0.6717040538787842
CurrentTrain: epoch  3, batch     2 | loss: 6.9571743Losses:  1.8644427061080933 -0.0 -0.0
CurrentTrain: epoch  3, batch     3 | loss: 1.8644427Losses:  6.1820173263549805 2.4346530437469482 0.7272614240646362
CurrentTrain: epoch  4, batch     0 | loss: 6.1820173Losses:  6.228726863861084 2.450561046600342 0.6760856509208679
CurrentTrain: epoch  4, batch     1 | loss: 6.2287269Losses:  6.907134056091309 3.4565021991729736 0.6574513912200928
CurrentTrain: epoch  4, batch     2 | loss: 6.9071341Losses:  2.063655376434326 -0.0 0.1332455277442932
CurrentTrain: epoch  4, batch     3 | loss: 2.0636554Losses:  7.093583583831787 3.3297204971313477 0.7318663001060486
CurrentTrain: epoch  5, batch     0 | loss: 7.0935836Losses:  5.942662715911865 2.255065441131592 0.6453646421432495
CurrentTrain: epoch  5, batch     1 | loss: 5.9426627Losses:  5.290679931640625 2.50535249710083 0.6660701036453247
CurrentTrain: epoch  5, batch     2 | loss: 5.2906799Losses:  2.065096378326416 -0.0 0.10830263793468475
CurrentTrain: epoch  5, batch     3 | loss: 2.0650964Losses:  6.918064117431641 3.3193507194519043 0.6399253010749817
CurrentTrain: epoch  6, batch     0 | loss: 6.9180641Losses:  6.576097011566162 3.8321759700775146 0.5793737769126892
CurrentTrain: epoch  6, batch     1 | loss: 6.5760970Losses:  5.070448875427246 2.129547357559204 0.6375349164009094
CurrentTrain: epoch  6, batch     2 | loss: 5.0704489Losses:  2.993922472000122 -0.0 0.10921283066272736
CurrentTrain: epoch  6, batch     3 | loss: 2.9939225Losses:  5.365674018859863 2.3126955032348633 0.6462082266807556
CurrentTrain: epoch  7, batch     0 | loss: 5.3656740Losses:  6.164571762084961 3.280568838119507 0.6887753009796143
CurrentTrain: epoch  7, batch     1 | loss: 6.1645718Losses:  5.91766357421875 2.81927490234375 0.5815989375114441
CurrentTrain: epoch  7, batch     2 | loss: 5.9176636Losses:  1.972867488861084 -0.0 0.13814190030097961
CurrentTrain: epoch  7, batch     3 | loss: 1.9728675Losses:  5.372751235961914 2.2760939598083496 0.6948903203010559
CurrentTrain: epoch  8, batch     0 | loss: 5.3727512Losses:  4.821205139160156 1.900200366973877 0.6980732083320618
CurrentTrain: epoch  8, batch     1 | loss: 4.8212051Losses:  5.542142868041992 2.6205382347106934 0.6375722885131836
CurrentTrain: epoch  8, batch     2 | loss: 5.5421429Losses:  2.435100793838501 -0.0 0.20095227658748627
CurrentTrain: epoch  8, batch     3 | loss: 2.4351008Losses:  4.459671974182129 1.7941231727600098 0.6948087215423584
CurrentTrain: epoch  9, batch     0 | loss: 4.4596720Losses:  5.665694236755371 3.070908784866333 0.5673283934593201
CurrentTrain: epoch  9, batch     1 | loss: 5.6656942Losses:  6.514990329742432 3.425997257232666 0.6295427680015564
CurrentTrain: epoch  9, batch     2 | loss: 6.5149903Losses:  1.8821536302566528 -0.0 0.13989433646202087
CurrentTrain: epoch  9, batch     3 | loss: 1.8821536
#### member of political party ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "member of political party" means the political party of which a person is or has been a member or otherwise affiliated.
Here is an example:
Relation: member of political party
Context: karl johan aarønes ( 8 may 1900 – 12 august 1969 ) was a norwegian politician for the labour party .
Head Entity: karl johan aarønes
Tail Entity: labour party
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "member of political party":
Relation: member of political party  
Context: barack obama served as a member of the democratic party during his presidency from 2009 to 2017.  
Head Entity: barack obama  
Tail Entity: democratic party  

Relation: member of political party  
Context: angela merkel was a prominent member of the christian democratic union throughout her political career in germany.  
Head Entity: angela merkel  
Tail Entity: christian democratic union  
#### after a work by ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "after a work by" means artist whose work strongly inspired/ was copied in this item.
Here is an example:
Relation: after a work by
Context: the script for " the great santini " was adapted by carlino from the 1976 novel by pat conroy , with assistance from an un - credited herman raucher .
Head Entity: the great santini
Tail Entity: pat conroy
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "after a work by":
Relation: after a work by  
Context: the film "inception" draws heavily from the concepts presented in the 2001 novel "the dreamers" by j. k. rowling, which explores the nature of dreams and reality.  
Head Entity: inception  
Tail Entity: j. k. rowling  

Relation: after a work by  
Context: the musical "hamilton" was inspired by the biography "alexander hamilton" written by ron chernow, which details the life of the founding father.  
Head Entity: hamilton  
Tail Entity: ron chernow  
#### notable work ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "notable work" means notable scientific, artistic or literary work, or other work of significance among subject's works.
Here is an example:
Relation: notable work
Context: another was " the death of nelson " by daniel maclise , a large wall painting in the royal gallery of the palace of westminster .
Head Entity: daniel maclise
Tail Entity: the death of nelson
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "notable work":
Relation: notable work  
Context: one of his most famous pieces is "the persistence of memory," a surreal painting that features melting clocks and explores the concept of time.  
Head Entity: salvador dalí  
Tail Entity: the persistence of memory  

Relation: notable work  
Context: her acclaimed novel "to kill a mockingbird" addresses serious issues of race and injustice in the American South through the eyes of a young girl.  
Head Entity: harper lee  
Tail Entity: to kill a mockingbird  
#### headquarters location ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "headquarters location" means city, where an organization's headquarters is or has been situated.
Here is an example:
Relation: headquarters location
Context: at the request of the stockholm institute for bible translation , mirza khazar translated the bible – the new testament and old testament – into the azerbaijani language .
Head Entity: institute for bible translation
Tail Entity: stockholm
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "headquarters location":
Relation: headquarters location  
Context: The global headquarters of the tech giant is located in Silicon Valley, where it has been a major player in the industry for over two decades.  
Head Entity: tech giant  
Tail Entity: Silicon Valley  

Relation: headquarters location  
Context: After years of expansion, the non-profit organization finally established its main office in the heart of New York City, allowing it to better serve its community.  
Head Entity: non-profit organization  
Tail Entity: New York City  
#### taxon rank ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "taxon rank" means level in a taxonomic hierarchy.
Here is an example:
Relation: taxon rank
Context: a number of species that were placed in the family phyllophoridae by thander in 1989 and 1990 are now included in cucumariidae .
Head Entity: phyllophoridae
Tail Entity: family
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "taxon rank":
Relation: taxon rank  
Context: The genus Canis includes several species, with Canis lupus being the most well-known, classified under the family Canidae.  
Head Entity: Canis  
Tail Entity: genus  

Relation: taxon rank  
Context: In the classification of living organisms, the order Primates encompasses various families, including Hominidae, which contains humans and their closest relatives.  
Head Entity: Hominidae  
Tail Entity: order  
#### field of work ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "field of work" means specialization of a person or organization.
Here is an example:
Relation: field of work
Context: elspeth frances garman is professor of molecular biophysics at the university of oxford and a former president of the british crystallographic association .
Head Entity: elspeth frances garman
Tail Entity: molecular biophysics
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "field of work":
Relation: field of work  
Context: dr. amanda jones is a leading researcher in artificial intelligence and currently works at the mit media lab, focusing on machine learning applications.  
Head Entity: dr. amanda jones  
Tail Entity: artificial intelligence  

Relation: field of work  
Context: the world health organization has appointed dr. rajesh kumar as the new director of global health initiatives, where he will concentrate on infectious disease control.  
Head Entity: dr. rajesh kumar  
Tail Entity: global health initiatives  
#### licensed to broadcast to ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "licensed to broadcast to" means place that a radio/TV station is licensed/required to broadcast to.
Here is an example:
Relation: licensed to broadcast to
Context: since march 2006 , wkxp 's programming has been simulcasted on 97.3 wzad wurtsboro , new york .
Head Entity: wzad
Tail Entity: wurtsboro , new york
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "licensed to broadcast to":
Relation: licensed to broadcast to  
Context: in 2010, the station wxyz began broadcasting to the community of springfield, illinois.  
Head Entity: wxyz  
Tail Entity: springfield, illinois  

Relation: licensed to broadcast to  
Context: the radio station kqrs has been granted a license to broadcast to the area of minneapolis, minnesota since 1995.  
Head Entity: kqrs  
Tail Entity: minneapolis, minnesota  
#### constellation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "constellation" means the area of the celestial sphere of which the subject is a part.
Here is an example:
Relation: constellation
Context: tau² eridani ( τ² eridani , abbreviated tau² eri , τ² eri ) , also named angetenar , is a star in the constellation of eridanus .
Head Entity: angetenar
Tail Entity: eridanus
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "constellation":
Relation: constellation  
Context: betelgeuse ( α orionis ) is a red supergiant star in the constellation of orion, known for its brightness and distinctive reddish hue.  
Head Entity: betelgeuse  
Tail Entity: orion  

Relation: constellation  
Context: the star deneb is located in the constellation of cygnus and is one of the brightest stars in the northern sky.  
Head Entity: deneb  
Tail Entity: cygnus  
#### platform ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "platform" means platform for which a work was developed or released, or the specific platform version of a software product.
Here is an example:
Relation: platform
Context: " toy soldiers : war chest " was released on august 11 , 2015 for windows , playstation 4 and xbox one .
Head Entity: toy soldiers : war chest
Tail Entity: xbox one
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "platform":
Relation: platform  
Context: "The Legend of Zelda: Breath of the Wild" was launched on March 3, 2017 for the Nintendo Switch and Wii U.  
Head Entity: The Legend of Zelda: Breath of the Wild  
Tail Entity: Nintendo Switch  

Relation: platform  
Context: "Cyberpunk 2077" was made available on December 10, 2020 for PC, PlayStation 4, and Xbox One.  
Head Entity: Cyberpunk 2077  
Tail Entity: PlayStation 4  
#### place served by transport hub ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "place served by transport hub" means territorial entity or entities served by this transport hub (airport, train station, etc.).
Here is an example:
Relation: place served by transport hub
Context: services to the island run from san juan 's luis muñoz marín international airport , ceiba airport or isla grande airport ( 20- to 30-minute flight ) .
Head Entity: isla grande airport
Tail Entity: san juan
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "place served by transport hub":
Relation: place served by transport hub  
Context: The city of Barcelona is well-connected to the surrounding regions through the El Prat Airport, which offers numerous flights to various destinations.  
Head Entity: El Prat Airport  
Tail Entity: Barcelona  

Relation: place served by transport hub  
Context: The central train station in Zurich provides access to multiple Swiss cities and international destinations, making it a key transport hub for travelers.  
Head Entity: Zurich central train station  
Tail Entity: Zurich  
Losses:  4.9208903312683105 1.0724093914031982 0.8787015080451965
MemoryTrain:  epoch  0, batch     0 | loss: 4.9208903Losses:  4.631066799163818 0.5932065844535828 0.8037264943122864
MemoryTrain:  epoch  0, batch     1 | loss: 4.6310668Losses:  4.189759254455566 0.8592346906661987 0.9050878286361694
MemoryTrain:  epoch  0, batch     2 | loss: 4.1897593Losses:  3.3850135803222656 -0.0 0.868726909160614
MemoryTrain:  epoch  0, batch     3 | loss: 3.3850136Losses:  3.5402755737304688 -0.0 0.886102557182312
MemoryTrain:  epoch  0, batch     4 | loss: 3.5402756Losses:  4.526109218597412 0.5465058088302612 1.0455236434936523
MemoryTrain:  epoch  0, batch     5 | loss: 4.5261092Losses:  3.944415330886841 0.25189071893692017 0.8809471130371094
MemoryTrain:  epoch  0, batch     6 | loss: 3.9444153Losses:  3.8562674522399902 1.1442021131515503 0.788142204284668
MemoryTrain:  epoch  0, batch     7 | loss: 3.8562675Losses:  3.84962797164917 0.2745972275733948 1.0021800994873047
MemoryTrain:  epoch  0, batch     8 | loss: 3.8496280Losses:  3.3346707820892334 -0.0 0.419375479221344
MemoryTrain:  epoch  0, batch     9 | loss: 3.3346708Losses:  3.9993720054626465 0.7773196697235107 0.8628906011581421
MemoryTrain:  epoch  1, batch     0 | loss: 3.9993720Losses:  3.6801648139953613 0.24972829222679138 0.923864483833313
MemoryTrain:  epoch  1, batch     1 | loss: 3.6801648Losses:  3.9166488647460938 -0.0 1.049543857574463
MemoryTrain:  epoch  1, batch     2 | loss: 3.9166489Losses:  3.960240364074707 0.8379884958267212 0.851436972618103
MemoryTrain:  epoch  1, batch     3 | loss: 3.9602404Losses:  2.8705806732177734 0.5070361495018005 0.7930148243904114
MemoryTrain:  epoch  1, batch     4 | loss: 2.8705807Losses:  3.7764124870300293 0.5427582859992981 0.975380539894104
MemoryTrain:  epoch  1, batch     5 | loss: 3.7764125Losses:  3.189826488494873 0.2680230736732483 0.9247783422470093
MemoryTrain:  epoch  1, batch     6 | loss: 3.1898265Losses:  4.130276679992676 0.5620989799499512 1.0065279006958008
MemoryTrain:  epoch  1, batch     7 | loss: 4.1302767Losses:  3.0378966331481934 0.25645560026168823 0.9842086434364319
MemoryTrain:  epoch  1, batch     8 | loss: 3.0378966Losses:  2.3381848335266113 -0.0 0.5193824768066406
MemoryTrain:  epoch  1, batch     9 | loss: 2.3381848Losses:  2.5735228061676025 0.5007196664810181 0.7378274202346802
MemoryTrain:  epoch  2, batch     0 | loss: 2.5735228Losses:  3.2955691814422607 0.2189050316810608 1.0353477001190186
MemoryTrain:  epoch  2, batch     1 | loss: 3.2955692Losses:  3.1411688327789307 -0.0 0.9361326098442078
MemoryTrain:  epoch  2, batch     2 | loss: 3.1411688Losses:  3.6867377758026123 1.3943901062011719 0.8715617060661316
MemoryTrain:  epoch  2, batch     3 | loss: 3.6867378Losses:  4.186742305755615 0.5324874520301819 0.8569124937057495
MemoryTrain:  epoch  2, batch     4 | loss: 4.1867423Losses:  2.4256978034973145 -0.0 0.9193255305290222
MemoryTrain:  epoch  2, batch     5 | loss: 2.4256978Losses:  3.785609722137451 0.6508046388626099 0.8157772421836853
MemoryTrain:  epoch  2, batch     6 | loss: 3.7856097Losses:  3.790130615234375 0.7824366092681885 0.9816778302192688
MemoryTrain:  epoch  2, batch     7 | loss: 3.7901306Losses:  2.833559036254883 0.5155181288719177 0.8154040575027466
MemoryTrain:  epoch  2, batch     8 | loss: 2.8335590Losses:  1.7961022853851318 -0.0 0.508561372756958
MemoryTrain:  epoch  2, batch     9 | loss: 1.7961023Losses:  3.308407783508301 0.7795003652572632 0.7259912490844727
MemoryTrain:  epoch  3, batch     0 | loss: 3.3084078Losses:  3.2520196437835693 0.5888502597808838 0.885525643825531
MemoryTrain:  epoch  3, batch     1 | loss: 3.2520196Losses:  2.797940254211426 0.4847111105918884 0.8477843999862671
MemoryTrain:  epoch  3, batch     2 | loss: 2.7979403Losses:  3.6971075534820557 0.478161096572876 0.7966098785400391
MemoryTrain:  epoch  3, batch     3 | loss: 3.6971076Losses:  3.900123119354248 1.789182424545288 0.8084015250205994
MemoryTrain:  epoch  3, batch     4 | loss: 3.9001231Losses:  3.4669086933135986 1.0481282472610474 0.7852581143379211
MemoryTrain:  epoch  3, batch     5 | loss: 3.4669087Losses:  3.023751735687256 0.47604304552078247 0.8485755920410156
MemoryTrain:  epoch  3, batch     6 | loss: 3.0237517Losses:  2.619642496109009 0.2543439269065857 0.9094815254211426
MemoryTrain:  epoch  3, batch     7 | loss: 2.6196425Losses:  3.592122793197632 0.8123782873153687 0.8720967173576355
MemoryTrain:  epoch  3, batch     8 | loss: 3.5921228Losses:  1.8548945188522339 -0.0 0.5145574808120728
MemoryTrain:  epoch  3, batch     9 | loss: 1.8548945Losses:  2.709120273590088 -0.0 0.9192767143249512
MemoryTrain:  epoch  4, batch     0 | loss: 2.7091203Losses:  2.834132671356201 0.23508384823799133 0.9717288613319397
MemoryTrain:  epoch  4, batch     1 | loss: 2.8341327Losses:  3.3939504623413086 1.1420092582702637 0.8663834929466248
MemoryTrain:  epoch  4, batch     2 | loss: 3.3939505Losses:  2.718111276626587 0.21355296671390533 0.8565437197685242
MemoryTrain:  epoch  4, batch     3 | loss: 2.7181113Losses:  2.469275951385498 0.2524779140949249 0.9192061424255371
MemoryTrain:  epoch  4, batch     4 | loss: 2.4692760Losses:  2.413444995880127 -0.0 1.0296589136123657
MemoryTrain:  epoch  4, batch     5 | loss: 2.4134450Losses:  3.1278157234191895 0.49915605783462524 1.0276727676391602
MemoryTrain:  epoch  4, batch     6 | loss: 3.1278157Losses:  2.580625534057617 0.2365970015525818 0.9418455362319946
MemoryTrain:  epoch  4, batch     7 | loss: 2.5806255Losses:  2.777005434036255 0.28772979974746704 0.9916929602622986
MemoryTrain:  epoch  4, batch     8 | loss: 2.7770054Losses:  1.7191529273986816 -0.0 0.44534024596214294
MemoryTrain:  epoch  4, batch     9 | loss: 1.7191529Losses:  2.5642459392547607 0.27965909242630005 0.8483996987342834
MemoryTrain:  epoch  5, batch     0 | loss: 2.5642459Losses:  3.586087942123413 1.1360089778900146 0.9272529482841492
MemoryTrain:  epoch  5, batch     1 | loss: 3.5860879Losses:  2.424574613571167 -0.0 0.9316393733024597
MemoryTrain:  epoch  5, batch     2 | loss: 2.4245746Losses:  2.4717259407043457 -0.0 0.9808356761932373
MemoryTrain:  epoch  5, batch     3 | loss: 2.4717259Losses:  2.667330265045166 0.21645647287368774 0.8423856496810913
MemoryTrain:  epoch  5, batch     4 | loss: 2.6673303Losses:  2.2569000720977783 -0.0 0.9219915270805359
MemoryTrain:  epoch  5, batch     5 | loss: 2.2569001Losses:  2.6852359771728516 0.505781888961792 0.8529977798461914
MemoryTrain:  epoch  5, batch     6 | loss: 2.6852360Losses:  2.5749900341033936 0.4750809371471405 0.8160292506217957
MemoryTrain:  epoch  5, batch     7 | loss: 2.5749900Losses:  2.9672582149505615 0.7182157039642334 0.9844666123390198
MemoryTrain:  epoch  5, batch     8 | loss: 2.9672582Losses:  1.6664249897003174 -0.0 0.4465990662574768
MemoryTrain:  epoch  5, batch     9 | loss: 1.6664250Losses:  2.8037946224212646 0.29819750785827637 0.9854229092597961
MemoryTrain:  epoch  6, batch     0 | loss: 2.8037946Losses:  2.3325533866882324 -0.0 1.0460821390151978
MemoryTrain:  epoch  6, batch     1 | loss: 2.3325534Losses:  2.7438621520996094 0.4846119284629822 0.9035682678222656
MemoryTrain:  epoch  6, batch     2 | loss: 2.7438622Losses:  2.741224527359009 0.527141809463501 0.9242281317710876
MemoryTrain:  epoch  6, batch     3 | loss: 2.7412245Losses:  2.6222035884857178 0.24651716649532318 0.9092685580253601
MemoryTrain:  epoch  6, batch     4 | loss: 2.6222036Losses:  2.569692850112915 0.27583956718444824 0.847366988658905
MemoryTrain:  epoch  6, batch     5 | loss: 2.5696929Losses:  2.899505615234375 0.5045832395553589 1.0279991626739502
MemoryTrain:  epoch  6, batch     6 | loss: 2.8995056Losses:  2.973853588104248 0.790239691734314 0.816501796245575
MemoryTrain:  epoch  6, batch     7 | loss: 2.9738536Losses:  2.4252679347991943 0.24878472089767456 0.8907520174980164
MemoryTrain:  epoch  6, batch     8 | loss: 2.4252679Losses:  2.0591745376586914 -0.0 0.2418571412563324
MemoryTrain:  epoch  6, batch     9 | loss: 2.0591745Losses:  2.271838665008545 -0.0 0.9037778377532959
MemoryTrain:  epoch  7, batch     0 | loss: 2.2718387Losses:  2.430756092071533 0.22102904319763184 0.9525802731513977
MemoryTrain:  epoch  7, batch     1 | loss: 2.4307561Losses:  2.5651323795318604 0.25835198163986206 0.8967323303222656
MemoryTrain:  epoch  7, batch     2 | loss: 2.5651324Losses:  2.748138189315796 0.539372980594635 0.8093528151512146
MemoryTrain:  epoch  7, batch     3 | loss: 2.7481382Losses:  2.679410219192505 0.23839408159255981 0.9584729671478271
MemoryTrain:  epoch  7, batch     4 | loss: 2.6794102Losses:  2.4412007331848145 0.2647183835506439 0.8758090734481812
MemoryTrain:  epoch  7, batch     5 | loss: 2.4412007Losses:  2.7959847450256348 0.5341578722000122 0.972316324710846
MemoryTrain:  epoch  7, batch     6 | loss: 2.7959847Losses:  2.4122705459594727 -0.0 0.9655457139015198
MemoryTrain:  epoch  7, batch     7 | loss: 2.4122705Losses:  2.5152993202209473 0.25598204135894775 0.9720413684844971
MemoryTrain:  epoch  7, batch     8 | loss: 2.5152993Losses:  1.9931156635284424 0.30292555689811707 0.3756607174873352
MemoryTrain:  epoch  7, batch     9 | loss: 1.9931157Losses:  2.3722352981567383 0.24895982444286346 0.7906414866447449
MemoryTrain:  epoch  8, batch     0 | loss: 2.3722353Losses:  2.4220969676971436 -0.0 1.0062940120697021
MemoryTrain:  epoch  8, batch     1 | loss: 2.4220970Losses:  2.7324280738830566 0.5768756866455078 0.8801202178001404
MemoryTrain:  epoch  8, batch     2 | loss: 2.7324281Losses:  2.5996479988098145 0.5376254916191101 0.8708410263061523
MemoryTrain:  epoch  8, batch     3 | loss: 2.5996480Losses:  2.580597400665283 0.30126428604125977 0.9059140086174011
MemoryTrain:  epoch  8, batch     4 | loss: 2.5805974Losses:  2.7141506671905518 0.5443940758705139 0.8353164196014404
MemoryTrain:  epoch  8, batch     5 | loss: 2.7141507Losses:  2.5929906368255615 0.5168639421463013 0.8481311202049255
MemoryTrain:  epoch  8, batch     6 | loss: 2.5929906Losses:  2.608952045440674 0.2503812909126282 0.9055683016777039
MemoryTrain:  epoch  8, batch     7 | loss: 2.6089520Losses:  2.3977622985839844 0.25484704971313477 0.7316555976867676
MemoryTrain:  epoch  8, batch     8 | loss: 2.3977623Losses:  1.5474727153778076 -0.0 0.30459949374198914
MemoryTrain:  epoch  8, batch     9 | loss: 1.5474727Losses:  2.362420082092285 0.26151955127716064 0.8079087138175964
MemoryTrain:  epoch  9, batch     0 | loss: 2.3624201Losses:  2.3690924644470215 -0.0 1.0057464838027954
MemoryTrain:  epoch  9, batch     1 | loss: 2.3690925Losses:  2.540701150894165 0.25877562165260315 0.9608463644981384
MemoryTrain:  epoch  9, batch     2 | loss: 2.5407012Losses:  3.307380199432373 1.2054530382156372 0.8552918434143066
MemoryTrain:  epoch  9, batch     3 | loss: 3.3073802Losses:  2.64918851852417 0.5272389650344849 0.8669072985649109
MemoryTrain:  epoch  9, batch     4 | loss: 2.6491885Losses:  2.6154415607452393 0.44454628229141235 0.9544007778167725
MemoryTrain:  epoch  9, batch     5 | loss: 2.6154416Losses:  2.8641533851623535 0.51312655210495 0.9760017991065979
MemoryTrain:  epoch  9, batch     6 | loss: 2.8641534Losses:  2.6464269161224365 0.2334282547235489 0.9587077498435974
MemoryTrain:  epoch  9, batch     7 | loss: 2.6464269Losses:  2.4586758613586426 0.23271429538726807 1.0185247659683228
MemoryTrain:  epoch  9, batch     8 | loss: 2.4586759Losses:  1.5407133102416992 -0.0 0.315346360206604
MemoryTrain:  epoch  9, batch     9 | loss: 1.5407133
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 90.62%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 90.62%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 90.00%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 91.67%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 91.96%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 92.97%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 91.67%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 91.88%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 90.91%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 90.10%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 90.38%   [EVAL] batch:   13 | acc: 81.25%,  total acc: 89.73%   [EVAL] batch:   14 | acc: 81.25%,  total acc: 89.17%   [EVAL] batch:   15 | acc: 87.50%,  total acc: 89.06%   [EVAL] batch:   16 | acc: 68.75%,  total acc: 87.87%   [EVAL] batch:   17 | acc: 75.00%,  total acc: 87.15%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 86.18%   [EVAL] batch:   19 | acc: 25.00%,  total acc: 83.12%   [EVAL] batch:   20 | acc: 37.50%,  total acc: 80.95%   [EVAL] batch:   21 | acc: 25.00%,  total acc: 78.41%   [EVAL] batch:   22 | acc: 18.75%,  total acc: 75.82%   [EVAL] batch:   23 | acc: 31.25%,  total acc: 73.96%   [EVAL] batch:   24 | acc: 18.75%,  total acc: 71.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 72.84%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 73.84%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 74.78%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 75.65%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 76.46%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 77.22%   [EVAL] batch:   31 | acc: 75.00%,  total acc: 77.15%   [EVAL] batch:   32 | acc: 87.50%,  total acc: 77.46%   [EVAL] batch:   33 | acc: 62.50%,  total acc: 77.02%   [EVAL] batch:   34 | acc: 62.50%,  total acc: 76.61%   [EVAL] batch:   35 | acc: 68.75%,  total acc: 76.39%   [EVAL] batch:   36 | acc: 62.50%,  total acc: 76.01%   [EVAL] batch:   37 | acc: 68.75%,  total acc: 75.82%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 76.44%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 77.03%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 77.59%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 78.12%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 78.63%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 78.98%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 79.44%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 79.89%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 80.32%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 80.73%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 81.12%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 81.50%   [EVAL] batch:   50 | acc: 81.25%,  total acc: 81.50%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 81.73%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 81.96%   [EVAL] batch:   53 | acc: 93.75%,  total acc: 82.18%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 82.27%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 82.48%   [EVAL] batch:   56 | acc: 87.50%,  total acc: 82.57%   [EVAL] batch:   57 | acc: 75.00%,  total acc: 82.44%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 82.63%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 82.92%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 83.20%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 83.37%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 82.84%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 84.38%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 85.00%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 85.42%   [EVAL] batch:    6 | acc: 56.25%,  total acc: 81.25%   [EVAL] batch:    7 | acc: 68.75%,  total acc: 79.69%   [EVAL] batch:    8 | acc: 62.50%,  total acc: 77.78%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 76.88%   [EVAL] batch:   10 | acc: 50.00%,  total acc: 74.43%   [EVAL] batch:   11 | acc: 56.25%,  total acc: 72.92%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 73.56%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 75.00%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 76.25%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 77.73%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 78.68%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 79.51%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 80.59%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 80.62%   [EVAL] batch:   20 | acc: 81.25%,  total acc: 80.65%   [EVAL] batch:   21 | acc: 75.00%,  total acc: 80.40%   [EVAL] batch:   22 | acc: 68.75%,  total acc: 79.89%   [EVAL] batch:   23 | acc: 81.25%,  total acc: 79.95%   [EVAL] batch:   24 | acc: 81.25%,  total acc: 80.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 80.77%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 81.48%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 82.14%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 82.54%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 83.12%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 83.67%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 84.18%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 84.66%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 85.11%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 85.36%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 85.76%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 86.15%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 86.51%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 86.86%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 87.19%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 87.80%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 87.94%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 87.93%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 87.92%   [EVAL] batch:   45 | acc: 62.50%,  total acc: 87.36%   [EVAL] batch:   46 | acc: 62.50%,  total acc: 86.84%   [EVAL] batch:   47 | acc: 68.75%,  total acc: 86.46%   [EVAL] batch:   48 | acc: 81.25%,  total acc: 86.35%   [EVAL] batch:   49 | acc: 81.25%,  total acc: 86.25%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 86.40%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 86.42%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 86.56%   [EVAL] batch:   53 | acc: 87.50%,  total acc: 86.57%   [EVAL] batch:   54 | acc: 75.00%,  total acc: 86.36%   [EVAL] batch:   55 | acc: 81.25%,  total acc: 86.27%   [EVAL] batch:   56 | acc: 81.25%,  total acc: 86.18%   [EVAL] batch:   57 | acc: 56.25%,  total acc: 85.67%   [EVAL] batch:   58 | acc: 75.00%,  total acc: 85.49%   [EVAL] batch:   59 | acc: 87.50%,  total acc: 85.52%   [EVAL] batch:   60 | acc: 81.25%,  total acc: 85.45%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 85.18%   [EVAL] batch:   62 | acc: 62.50%,  total acc: 84.82%   [EVAL] batch:   63 | acc: 56.25%,  total acc: 84.38%   [EVAL] batch:   64 | acc: 25.00%,  total acc: 83.46%   [EVAL] batch:   65 | acc: 75.00%,  total acc: 83.33%   [EVAL] batch:   66 | acc: 31.25%,  total acc: 82.56%   [EVAL] batch:   67 | acc: 37.50%,  total acc: 81.89%   [EVAL] batch:   68 | acc: 75.00%,  total acc: 81.79%   [EVAL] batch:   69 | acc: 93.75%,  total acc: 81.96%   [EVAL] batch:   70 | acc: 81.25%,  total acc: 81.95%   [EVAL] batch:   71 | acc: 75.00%,  total acc: 81.86%   [EVAL] batch:   72 | acc: 93.75%,  total acc: 82.02%   [EVAL] batch:   73 | acc: 100.00%,  total acc: 82.26%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 82.33%   [EVAL] batch:   75 | acc: 31.25%,  total acc: 81.66%   [EVAL] batch:   76 | acc: 25.00%,  total acc: 80.93%   [EVAL] batch:   77 | acc: 31.25%,  total acc: 80.29%   [EVAL] batch:   78 | acc: 37.50%,  total acc: 79.75%   [EVAL] batch:   79 | acc: 43.75%,  total acc: 79.30%   [EVAL] batch:   80 | acc: 25.00%,  total acc: 78.63%   [EVAL] batch:   81 | acc: 75.00%,  total acc: 78.58%   [EVAL] batch:   82 | acc: 100.00%,  total acc: 78.84%   [EVAL] batch:   83 | acc: 100.00%,  total acc: 79.09%   [EVAL] batch:   84 | acc: 93.75%,  total acc: 79.26%   [EVAL] batch:   85 | acc: 100.00%,  total acc: 79.51%   [EVAL] batch:   86 | acc: 100.00%,  total acc: 79.74%   [EVAL] batch:   87 | acc: 87.50%,  total acc: 79.83%   [EVAL] batch:   88 | acc: 50.00%,  total acc: 79.49%   [EVAL] batch:   89 | acc: 31.25%,  total acc: 78.96%   [EVAL] batch:   90 | acc: 37.50%,  total acc: 78.50%   [EVAL] batch:   91 | acc: 56.25%,  total acc: 78.26%   [EVAL] batch:   92 | acc: 25.00%,  total acc: 77.69%   [EVAL] batch:   93 | acc: 68.75%,  total acc: 77.59%   [EVAL] batch:   94 | acc: 87.50%,  total acc: 77.70%   [EVAL] batch:   95 | acc: 81.25%,  total acc: 77.73%   [EVAL] batch:   96 | acc: 87.50%,  total acc: 77.84%   [EVAL] batch:   97 | acc: 81.25%,  total acc: 77.87%   [EVAL] batch:   98 | acc: 62.50%,  total acc: 77.71%   [EVAL] batch:   99 | acc: 93.75%,  total acc: 77.88%   [EVAL] batch:  100 | acc: 50.00%,  total acc: 77.60%   [EVAL] batch:  101 | acc: 62.50%,  total acc: 77.45%   [EVAL] batch:  102 | acc: 43.75%,  total acc: 77.12%   [EVAL] batch:  103 | acc: 62.50%,  total acc: 76.98%   [EVAL] batch:  104 | acc: 50.00%,  total acc: 76.73%   [EVAL] batch:  105 | acc: 62.50%,  total acc: 76.59%   [EVAL] batch:  106 | acc: 12.50%,  total acc: 75.99%   [EVAL] batch:  107 | acc: 12.50%,  total acc: 75.41%   [EVAL] batch:  108 | acc: 12.50%,  total acc: 74.83%   [EVAL] batch:  109 | acc: 6.25%,  total acc: 74.20%   [EVAL] batch:  110 | acc: 18.75%,  total acc: 73.70%   [EVAL] batch:  111 | acc: 25.00%,  total acc: 73.27%   [EVAL] batch:  112 | acc: 25.00%,  total acc: 72.84%   [EVAL] batch:  113 | acc: 31.25%,  total acc: 72.48%   [EVAL] batch:  114 | acc: 68.75%,  total acc: 72.45%   [EVAL] batch:  115 | acc: 37.50%,  total acc: 72.14%   [EVAL] batch:  116 | acc: 43.75%,  total acc: 71.90%   [EVAL] batch:  117 | acc: 50.00%,  total acc: 71.72%   [EVAL] batch:  118 | acc: 81.25%,  total acc: 71.80%   [EVAL] batch:  119 | acc: 93.75%,  total acc: 71.98%   [EVAL] batch:  120 | acc: 87.50%,  total acc: 72.11%   [EVAL] batch:  121 | acc: 81.25%,  total acc: 72.18%   [EVAL] batch:  122 | acc: 75.00%,  total acc: 72.21%   [EVAL] batch:  123 | acc: 75.00%,  total acc: 72.23%   [EVAL] batch:  124 | acc: 75.00%,  total acc: 72.25%   [EVAL] batch:  125 | acc: 25.00%,  total acc: 71.88%   [EVAL] batch:  126 | acc: 18.75%,  total acc: 71.46%   [EVAL] batch:  127 | acc: 6.25%,  total acc: 70.95%   [EVAL] batch:  128 | acc: 25.00%,  total acc: 70.59%   [EVAL] batch:  129 | acc: 6.25%,  total acc: 70.10%   [EVAL] batch:  130 | acc: 6.25%,  total acc: 69.61%   [EVAL] batch:  131 | acc: 43.75%,  total acc: 69.41%   [EVAL] batch:  132 | acc: 75.00%,  total acc: 69.45%   [EVAL] batch:  133 | acc: 81.25%,  total acc: 69.54%   [EVAL] batch:  134 | acc: 100.00%,  total acc: 69.77%   [EVAL] batch:  135 | acc: 81.25%,  total acc: 69.85%   [EVAL] batch:  136 | acc: 87.50%,  total acc: 69.98%   [EVAL] batch:  137 | acc: 68.75%,  total acc: 69.97%   [EVAL] batch:  138 | acc: 12.50%,  total acc: 69.56%   [EVAL] batch:  139 | acc: 18.75%,  total acc: 69.20%   [EVAL] batch:  140 | acc: 31.25%,  total acc: 68.93%   [EVAL] batch:  141 | acc: 12.50%,  total acc: 68.53%   [EVAL] batch:  142 | acc: 31.25%,  total acc: 68.27%   [EVAL] batch:  143 | acc: 56.25%,  total acc: 68.19%   [EVAL] batch:  144 | acc: 100.00%,  total acc: 68.41%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 68.62%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 68.79%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 69.00%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 69.21%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 69.38%   [EVAL] batch:  150 | acc: 100.00%,  total acc: 69.58%   [EVAL] batch:  151 | acc: 100.00%,  total acc: 69.78%   [EVAL] batch:  152 | acc: 100.00%,  total acc: 69.98%   [EVAL] batch:  153 | acc: 93.75%,  total acc: 70.13%   [EVAL] batch:  154 | acc: 100.00%,  total acc: 70.32%   [EVAL] batch:  155 | acc: 93.75%,  total acc: 70.47%   [EVAL] batch:  156 | acc: 100.00%,  total acc: 70.66%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 70.81%   [EVAL] batch:  158 | acc: 87.50%,  total acc: 70.91%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 71.05%   [EVAL] batch:  160 | acc: 93.75%,  total acc: 71.20%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 71.33%   [EVAL] batch:  162 | acc: 62.50%,  total acc: 71.28%   [EVAL] batch:  163 | acc: 43.75%,  total acc: 71.11%   [EVAL] batch:  164 | acc: 25.00%,  total acc: 70.83%   [EVAL] batch:  165 | acc: 31.25%,  total acc: 70.59%   [EVAL] batch:  166 | acc: 37.50%,  total acc: 70.40%   [EVAL] batch:  167 | acc: 18.75%,  total acc: 70.09%   [EVAL] batch:  168 | acc: 62.50%,  total acc: 70.04%   [EVAL] batch:  169 | acc: 100.00%,  total acc: 70.22%   [EVAL] batch:  170 | acc: 87.50%,  total acc: 70.32%   [EVAL] batch:  171 | acc: 100.00%,  total acc: 70.49%   [EVAL] batch:  172 | acc: 100.00%,  total acc: 70.66%   [EVAL] batch:  173 | acc: 93.75%,  total acc: 70.80%   [EVAL] batch:  174 | acc: 87.50%,  total acc: 70.89%   [EVAL] batch:  175 | acc: 56.25%,  total acc: 70.81%   [EVAL] batch:  176 | acc: 50.00%,  total acc: 70.69%   [EVAL] batch:  177 | acc: 50.00%,  total acc: 70.58%   [EVAL] batch:  178 | acc: 56.25%,  total acc: 70.50%   [EVAL] batch:  179 | acc: 75.00%,  total acc: 70.52%   [EVAL] batch:  180 | acc: 37.50%,  total acc: 70.34%   [EVAL] batch:  181 | acc: 56.25%,  total acc: 70.26%   [EVAL] batch:  182 | acc: 81.25%,  total acc: 70.32%   [EVAL] batch:  183 | acc: 75.00%,  total acc: 70.35%   [EVAL] batch:  184 | acc: 87.50%,  total acc: 70.44%   [EVAL] batch:  185 | acc: 93.75%,  total acc: 70.56%   [EVAL] batch:  186 | acc: 93.75%,  total acc: 70.69%   [EVAL] batch:  187 | acc: 81.25%,  total acc: 70.74%   [EVAL] batch:  188 | acc: 81.25%,  total acc: 70.80%   [EVAL] batch:  189 | acc: 81.25%,  total acc: 70.86%   [EVAL] batch:  190 | acc: 93.75%,  total acc: 70.98%   [EVAL] batch:  191 | acc: 81.25%,  total acc: 71.03%   [EVAL] batch:  192 | acc: 87.50%,  total acc: 71.11%   [EVAL] batch:  193 | acc: 75.00%,  total acc: 71.13%   [EVAL] batch:  194 | acc: 87.50%,  total acc: 71.22%   [EVAL] batch:  195 | acc: 81.25%,  total acc: 71.27%   [EVAL] batch:  196 | acc: 75.00%,  total acc: 71.29%   [EVAL] batch:  197 | acc: 93.75%,  total acc: 71.40%   [EVAL] batch:  198 | acc: 93.75%,  total acc: 71.51%   [EVAL] batch:  199 | acc: 81.25%,  total acc: 71.56%   [EVAL] batch:  200 | acc: 87.50%,  total acc: 71.64%   [EVAL] batch:  201 | acc: 81.25%,  total acc: 71.69%   [EVAL] batch:  202 | acc: 75.00%,  total acc: 71.71%   [EVAL] batch:  203 | acc: 87.50%,  total acc: 71.78%   [EVAL] batch:  204 | acc: 68.75%,  total acc: 71.77%   [EVAL] batch:  205 | acc: 87.50%,  total acc: 71.84%   [EVAL] batch:  206 | acc: 25.00%,  total acc: 71.62%   [EVAL] batch:  207 | acc: 18.75%,  total acc: 71.36%   [EVAL] batch:  208 | acc: 18.75%,  total acc: 71.11%   [EVAL] batch:  209 | acc: 18.75%,  total acc: 70.86%   [EVAL] batch:  210 | acc: 25.00%,  total acc: 70.65%   [EVAL] batch:  211 | acc: 37.50%,  total acc: 70.49%   [EVAL] batch:  212 | acc: 50.00%,  total acc: 70.39%   [EVAL] batch:  213 | acc: 93.75%,  total acc: 70.50%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 70.64%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 70.78%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 70.91%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 71.04%   [EVAL] batch:  218 | acc: 100.00%,  total acc: 71.18%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 71.31%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 71.44%   [EVAL] batch:  221 | acc: 100.00%,  total acc: 71.57%   [EVAL] batch:  222 | acc: 100.00%,  total acc: 71.69%   [EVAL] batch:  223 | acc: 100.00%,  total acc: 71.82%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 71.94%   [EVAL] batch:  225 | acc: 68.75%,  total acc: 71.93%   [EVAL] batch:  226 | acc: 68.75%,  total acc: 71.92%   [EVAL] batch:  227 | acc: 62.50%,  total acc: 71.88%   [EVAL] batch:  228 | acc: 75.00%,  total acc: 71.89%   [EVAL] batch:  229 | acc: 93.75%,  total acc: 71.98%   [EVAL] batch:  230 | acc: 62.50%,  total acc: 71.94%   [EVAL] batch:  231 | acc: 68.75%,  total acc: 71.93%   [EVAL] batch:  232 | acc: 31.25%,  total acc: 71.75%   [EVAL] batch:  233 | acc: 43.75%,  total acc: 71.63%   [EVAL] batch:  234 | acc: 25.00%,  total acc: 71.44%   [EVAL] batch:  235 | acc: 56.25%,  total acc: 71.37%   [EVAL] batch:  236 | acc: 31.25%,  total acc: 71.20%   [EVAL] batch:  237 | acc: 75.00%,  total acc: 71.22%   [EVAL] batch:  238 | acc: 87.50%,  total acc: 71.29%   [EVAL] batch:  239 | acc: 100.00%,  total acc: 71.41%   [EVAL] batch:  240 | acc: 81.25%,  total acc: 71.45%   [EVAL] batch:  241 | acc: 87.50%,  total acc: 71.51%   [EVAL] batch:  242 | acc: 93.75%,  total acc: 71.60%   [EVAL] batch:  243 | acc: 87.50%,  total acc: 71.67%   [EVAL] batch:  244 | acc: 43.75%,  total acc: 71.56%   [EVAL] batch:  245 | acc: 50.00%,  total acc: 71.47%   [EVAL] batch:  246 | acc: 6.25%,  total acc: 71.20%   [EVAL] batch:  247 | acc: 56.25%,  total acc: 71.14%   [EVAL] batch:  248 | acc: 31.25%,  total acc: 70.98%   [EVAL] batch:  249 | acc: 50.00%,  total acc: 70.90%   [EVAL] batch:  250 | acc: 93.75%,  total acc: 70.99%   [EVAL] batch:  251 | acc: 87.50%,  total acc: 71.06%   [EVAL] batch:  252 | acc: 93.75%,  total acc: 71.15%   [EVAL] batch:  253 | acc: 87.50%,  total acc: 71.21%   [EVAL] batch:  254 | acc: 87.50%,  total acc: 71.27%   [EVAL] batch:  255 | acc: 100.00%,  total acc: 71.39%   [EVAL] batch:  256 | acc: 93.75%,  total acc: 71.47%   [EVAL] batch:  257 | acc: 100.00%,  total acc: 71.58%   [EVAL] batch:  258 | acc: 81.25%,  total acc: 71.62%   [EVAL] batch:  259 | acc: 93.75%,  total acc: 71.71%   [EVAL] batch:  260 | acc: 81.25%,  total acc: 71.74%   [EVAL] batch:  261 | acc: 81.25%,  total acc: 71.78%   [EVAL] batch:  262 | acc: 93.75%,  total acc: 71.86%   [EVAL] batch:  263 | acc: 81.25%,  total acc: 71.90%   [EVAL] batch:  264 | acc: 81.25%,  total acc: 71.93%   [EVAL] batch:  265 | acc: 87.50%,  total acc: 71.99%   [EVAL] batch:  266 | acc: 68.75%,  total acc: 71.98%   [EVAL] batch:  267 | acc: 75.00%,  total acc: 71.99%   [EVAL] batch:  268 | acc: 68.75%,  total acc: 71.98%   [EVAL] batch:  269 | acc: 25.00%,  total acc: 71.81%   [EVAL] batch:  270 | acc: 37.50%,  total acc: 71.68%   [EVAL] batch:  271 | acc: 25.00%,  total acc: 71.51%   [EVAL] batch:  272 | acc: 18.75%,  total acc: 71.31%   [EVAL] batch:  273 | acc: 31.25%,  total acc: 71.17%   [EVAL] batch:  274 | acc: 18.75%,  total acc: 70.98%   [EVAL] batch:  275 | acc: 100.00%,  total acc: 71.08%   [EVAL] batch:  276 | acc: 100.00%,  total acc: 71.19%   [EVAL] batch:  277 | acc: 100.00%,  total acc: 71.29%   [EVAL] batch:  278 | acc: 100.00%,  total acc: 71.39%   [EVAL] batch:  279 | acc: 100.00%,  total acc: 71.50%   [EVAL] batch:  280 | acc: 100.00%,  total acc: 71.60%   [EVAL] batch:  281 | acc: 75.00%,  total acc: 71.61%   [EVAL] batch:  282 | acc: 87.50%,  total acc: 71.67%   [EVAL] batch:  283 | acc: 62.50%,  total acc: 71.63%   [EVAL] batch:  284 | acc: 62.50%,  total acc: 71.60%   [EVAL] batch:  285 | acc: 68.75%,  total acc: 71.59%   [EVAL] batch:  286 | acc: 62.50%,  total acc: 71.56%   [EVAL] batch:  287 | acc: 68.75%,  total acc: 71.55%   [EVAL] batch:  288 | acc: 100.00%,  total acc: 71.65%   [EVAL] batch:  289 | acc: 100.00%,  total acc: 71.75%   [EVAL] batch:  290 | acc: 100.00%,  total acc: 71.84%   [EVAL] batch:  291 | acc: 100.00%,  total acc: 71.94%   [EVAL] batch:  292 | acc: 100.00%,  total acc: 72.03%   [EVAL] batch:  293 | acc: 93.75%,  total acc: 72.11%   [EVAL] batch:  294 | acc: 100.00%,  total acc: 72.20%   [EVAL] batch:  295 | acc: 100.00%,  total acc: 72.30%   [EVAL] batch:  296 | acc: 100.00%,  total acc: 72.39%   [EVAL] batch:  297 | acc: 100.00%,  total acc: 72.48%   [EVAL] batch:  298 | acc: 100.00%,  total acc: 72.58%   [EVAL] batch:  299 | acc: 100.00%,  total acc: 72.67%   [EVAL] batch:  300 | acc: 81.25%,  total acc: 72.70%   [EVAL] batch:  301 | acc: 93.75%,  total acc: 72.76%   [EVAL] batch:  302 | acc: 93.75%,  total acc: 72.83%   [EVAL] batch:  303 | acc: 93.75%,  total acc: 72.90%   [EVAL] batch:  304 | acc: 87.50%,  total acc: 72.95%   [EVAL] batch:  305 | acc: 93.75%,  total acc: 73.02%   [EVAL] batch:  306 | acc: 87.50%,  total acc: 73.07%   [EVAL] batch:  307 | acc: 75.00%,  total acc: 73.07%   [EVAL] batch:  308 | acc: 93.75%,  total acc: 73.14%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 73.23%   [EVAL] batch:  310 | acc: 100.00%,  total acc: 73.31%   [EVAL] batch:  311 | acc: 93.75%,  total acc: 73.38%   [EVAL] batch:  312 | acc: 50.00%,  total acc: 73.30%   
cur_acc:  ['0.9484', '0.7232', '0.7421', '0.6736', '0.8284']
his_acc:  ['0.9484', '0.8190', '0.7713', '0.7235', '0.7330']
Clustering into  29  clusters
Clusters:  [ 6  4  9 11  1  1  9 13 27  8 12 23 14 20 14  3 16 19 25 28 26  2  0 24
 19  2  0  5  8  5  4  5 13 23  7 17 18  0 11  0 11 15  6 27 21 12  4  3
  7  4  0 22 13  1 10  8 12  1  2  2]
Losses:  9.852967262268066 4.077469825744629 0.5270271897315979
CurrentTrain: epoch  0, batch     0 | loss: 9.8529673Losses:  12.514151573181152 5.43682336807251 0.5853807926177979
CurrentTrain: epoch  0, batch     1 | loss: 12.5141516Losses:  10.080350875854492 3.631406307220459 0.6934853196144104
CurrentTrain: epoch  0, batch     2 | loss: 10.0803509Losses:  5.533821105957031 -0.0 0.15424582362174988
CurrentTrain: epoch  0, batch     3 | loss: 5.5338211Losses:  8.767912864685059 3.1765356063842773 0.6294423341751099
CurrentTrain: epoch  1, batch     0 | loss: 8.7679129Losses:  9.861902236938477 3.749476432800293 0.6241699457168579
CurrentTrain: epoch  1, batch     1 | loss: 9.8619022Losses:  7.319202423095703 2.2904157638549805 0.6693905591964722
CurrentTrain: epoch  1, batch     2 | loss: 7.3192024Losses:  2.339704990386963 -0.0 0.1313338577747345
CurrentTrain: epoch  1, batch     3 | loss: 2.3397050Losses:  8.432847023010254 3.1438679695129395 0.6777511835098267
CurrentTrain: epoch  2, batch     0 | loss: 8.4328470Losses:  8.606657028198242 3.964715003967285 0.697277307510376
CurrentTrain: epoch  2, batch     1 | loss: 8.6066570Losses:  7.182858943939209 2.803570508956909 0.5804861187934875
CurrentTrain: epoch  2, batch     2 | loss: 7.1828589Losses:  3.4404590129852295 -0.0 0.12936624884605408
CurrentTrain: epoch  2, batch     3 | loss: 3.4404590Losses:  7.4250969886779785 3.2318170070648193 0.6636587977409363
CurrentTrain: epoch  3, batch     0 | loss: 7.4250970Losses:  6.475879192352295 2.3262078762054443 0.6345587968826294
CurrentTrain: epoch  3, batch     1 | loss: 6.4758792Losses:  7.586352348327637 3.3783493041992188 0.6619683504104614
CurrentTrain: epoch  3, batch     2 | loss: 7.5863523Losses:  3.4469141960144043 -0.0 0.11219875514507294
CurrentTrain: epoch  3, batch     3 | loss: 3.4469142Losses:  5.734006881713867 1.6309881210327148 0.6583030223846436
CurrentTrain: epoch  4, batch     0 | loss: 5.7340069Losses:  6.0351457595825195 2.2803502082824707 0.5737597942352295
CurrentTrain: epoch  4, batch     1 | loss: 6.0351458Losses:  7.6292405128479 3.8561105728149414 0.6634830832481384
CurrentTrain: epoch  4, batch     2 | loss: 7.6292405Losses:  1.9717319011688232 -0.0 0.0934097170829773
CurrentTrain: epoch  4, batch     3 | loss: 1.9717319Losses:  5.515018463134766 1.9796290397644043 0.6572120189666748
CurrentTrain: epoch  5, batch     0 | loss: 5.5150185Losses:  5.267857551574707 2.2381725311279297 0.64372718334198
CurrentTrain: epoch  5, batch     1 | loss: 5.2678576Losses:  7.770417213439941 3.8106231689453125 0.5762064456939697
CurrentTrain: epoch  5, batch     2 | loss: 7.7704172Losses:  2.26601505279541 -0.0 0.11392354965209961
CurrentTrain: epoch  5, batch     3 | loss: 2.2660151Losses:  6.606818199157715 3.5319626331329346 0.653157114982605
CurrentTrain: epoch  6, batch     0 | loss: 6.6068182Losses:  5.985527992248535 2.5966708660125732 0.6442643404006958
CurrentTrain: epoch  6, batch     1 | loss: 5.9855280Losses:  6.237971782684326 2.997352123260498 0.5703312754631042
CurrentTrain: epoch  6, batch     2 | loss: 6.2379718Losses:  2.1544556617736816 -0.0 -0.0
CurrentTrain: epoch  6, batch     3 | loss: 2.1544557Losses:  5.254380226135254 2.378755569458008 0.6381096839904785
CurrentTrain: epoch  7, batch     0 | loss: 5.2543802Losses:  7.011659145355225 3.5534002780914307 0.6403223872184753
CurrentTrain: epoch  7, batch     1 | loss: 7.0116591Losses:  5.022902011871338 2.448732614517212 0.5352844595909119
CurrentTrain: epoch  7, batch     2 | loss: 5.0229020Losses:  1.8867322206497192 -0.0 0.10193310678005219
CurrentTrain: epoch  7, batch     3 | loss: 1.8867322Losses:  5.856732368469238 3.1708931922912598 0.6339842081069946
CurrentTrain: epoch  8, batch     0 | loss: 5.8567324Losses:  4.351405143737793 1.550225019454956 0.6223241090774536
CurrentTrain: epoch  8, batch     1 | loss: 4.3514051Losses:  6.409571170806885 2.8783063888549805 0.637891411781311
CurrentTrain: epoch  8, batch     2 | loss: 6.4095712Losses:  1.9808906316757202 -0.0 0.09407410770654678
CurrentTrain: epoch  8, batch     3 | loss: 1.9808906Losses:  6.682980537414551 3.4532785415649414 0.5914905667304993
CurrentTrain: epoch  9, batch     0 | loss: 6.6829805Losses:  5.134636878967285 2.4145212173461914 0.6226111650466919
CurrentTrain: epoch  9, batch     1 | loss: 5.1346369Losses:  5.073083400726318 2.850605010986328 0.3685687184333801
CurrentTrain: epoch  9, batch     2 | loss: 5.0730834Losses:  3.2002007961273193 -0.0 0.11846897006034851
CurrentTrain: epoch  9, batch     3 | loss: 3.2002008
#### spouse ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "spouse" means the subject has the object as their spouse (husband, wife, partner, etc.).
Here is an example:
Relation: spouse
Context: rohan is the son of narayana murthy , founder of infosys , and his wife sudha murthy , an engineer and author of note .
Head Entity: narayana murthy
Tail Entity: sudha murthy
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "spouse":
Relation: spouse  
Context: elon musk, the CEO of spacex and tesla, was previously married to talulah riley, a talented actress and writer.  
Head Entity: elon musk  
Tail Entity: talulah riley  

Relation: spouse  
Context: barack obama, the 44th president of the united states, has been married to michelle obama since 1992, and they have two daughters together.  
Head Entity: barack obama  
Tail Entity: michelle obama  
#### record label ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "record label" means brand and trademark associated with the marketing of subject music recordings and music videos.
Here is an example:
Relation: record label
Context: darlene released , " darlene mccoy " , on may 8 , 2007 with label emi gospel .
Head Entity: darlene mccoy
Tail Entity: emi gospel
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "record label":
Relation: record label  
Context: the band released their latest album, "echoes of the past," under the label universal music group in 2021.  
Head Entity: the band  
Tail Entity: universal music group  

Relation: record label  
Context: after signing a new contract, the artist announced that their upcoming single would be distributed by sony music entertainment.  
Head Entity: the artist  
Tail Entity: sony music entertainment  
#### distributor ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "distributor" means distributor of a creative work; distributor for a record label; news agency; film distributor.
Here is an example:
Relation: distributor
Context: distributed by paramount pictures , " the italian job " was theatrically released in the united states on may 30 , 2003 , and grossed over $ 176 million worldwide .
Head Entity: the italian job
Tail Entity: paramount pictures
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "distributor":
Relation: distributor  
Context: the film was distributed by universal pictures and received critical acclaim upon its release in 2015.  
Head Entity: the film  
Tail Entity: universal pictures  

Relation: distributor  
Context: the album was distributed by sony music and quickly climbed the charts, becoming a bestseller.  
Head Entity: the album  
Tail Entity: sony music  
#### located in or next to body of water ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "located in or next to body of water" means body of water on or next to which a place is located.
Here is an example:
Relation: located in or next to body of water
Context: cape hallett is a snow - free area ( antarctic oasis ) on the northern tip of the hallett peninsula on the ross sea coast of victoria land , east antarctica .
Head Entity: east antarctica
Tail Entity: ross sea
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "located in or next to body of water":
Relation: located in or next to body of water  
Context: the city of venice is built on a group of 118 small islands separated by canals and linked by bridges, situated in the venetian lagoon along the adriatic sea.  
Head Entity: venice  
Tail Entity: adriatic sea  

Relation: located in or next to body of water  
Context: the town of key west is located at the southernmost point of the continental united states, surrounded by the waters of the gulf of mexico.  
Head Entity: key west  
Tail Entity: gulf of mexico  
#### subsidiary ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "subsidiary" means subsidiary of a company or organization; generally a fully owned separate corporation.
Here is an example:
Relation: subsidiary
Context: cbs and viacom split again in 2005 , but both are still owned by national amusements .
Head Entity: national amusements
Tail Entity: viacom
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "subsidiary":
Relation: subsidiary  
Context: In 2018, Facebook acquired WhatsApp, making it a subsidiary of the social media giant.  
Head Entity: Facebook  
Tail Entity: WhatsApp  

Relation: subsidiary  
Context: Toyota has several subsidiaries, including Lexus, which focuses on luxury vehicles.  
Head Entity: Toyota  
Tail Entity: Lexus  
#### country of citizenship ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "country of citizenship" means the object is a country that recognizes the subject as its citizen.
Here is an example:
Relation: country of citizenship
Context: he challenged seven other swimmers in heat four including ukraine 's rostyslav svanidze , a top 16 finalist in atlanta four years earlier .
Head Entity: rostyslav svanidze
Tail Entity: ukraine
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "country of citizenship":
Relation: country of citizenship  
Context: after years of living in the united states, the renowned artist decided to apply for dual citizenship, allowing him to retain his ties to his homeland, spain.  
Head Entity: the renowned artist  
Tail Entity: spain  

Relation: country of citizenship  
Context: during the international conference, the delegate from nigeria proudly spoke about the rights and responsibilities of citizens in his country.  
Head Entity: the delegate from nigeria  
Tail Entity: nigeria  
#### genre ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "genre" means creative work's genre or an artist's field of work.
Here is an example:
Relation: genre
Context: buck moved to austin in the mid-1970s and joined the blues rock group , the fabulous thunderbirds , along with keith ferguson , jimmie vaughan , and kim wilson .
Head Entity: the fabulous thunderbirds
Tail Entity: blues rock
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "genre":
Relation: genre  
Context: the album "thriller" by michael jackson is often regarded as a landmark in pop music history, blending elements of rock, funk, and soul.  
Head Entity: michael jackson  
Tail Entity: pop  

Relation: genre  
Context: the novel "dune" by frank herbert is a seminal work in the science fiction genre, exploring themes of politics, religion, and ecology.  
Head Entity: frank herbert  
Tail Entity: science fiction  
#### mouth of the watercourse ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "mouth of the watercourse" means the body of water to which the watercourse drains.
Here is an example:
Relation: mouth of the watercourse
Context: san lázaro is 660 km far from asunción and is located at the exact site of the confluence of the apa river with paraguay river , bordering brazil - chaco .
Head Entity: apa river
Tail Entity: paraguay river
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "mouth of the watercourse":
Relation: mouth of the watercourse  
Context: the mississippi river flows into the gulf of mexico, creating a rich delta ecosystem that supports diverse wildlife and plant species.  
Head Entity: mississippi river  
Tail Entity: gulf of mexico  

Relation: mouth of the watercourse  
Context: the rhine river, which runs through several countries in europe, ultimately empties into the north sea, providing a vital shipping route for international trade.  
Head Entity: rhine river  
Tail Entity: north sea  
#### position played on team / speciality ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "position played on team / speciality" means position or specialism of a player on a team.
Here is an example:
Relation: position played on team / speciality
Context: benjamin van leer ( born 9 april 1992 in houten ) is a dutch professional footballer who currently plays as a goalkeeper for ajax in the dutch eredivisie .
Head Entity: benjamin van leer
Tail Entity: goalkeeper
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "position played on team / speciality":
Relation: position played on team / speciality  
Context: lebron james, born on december 30, 1984, is an american professional basketball player who currently plays as a forward for the los angeles lakers in the nba.  
Head Entity: lebron james  
Tail Entity: forward  

Relation: position played on team / speciality  
Context: serena williams, born on september 26, 1981, is an american professional tennis player who is known for her powerful serve and currently plays in singles and doubles competitions.  
Head Entity: serena williams  
Tail Entity: tennis player  
#### sports season of league or competition ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "sports season of league or competition" means property that shows the competition of which the item is a season.
Here is an example:
Relation: sports season of league or competition
Context: the stade louis ii in monaco was the venue for the uefa super cup every year since 1998 .
Head Entity: 1998
Tail Entity: uefa super cup
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "sports season of league or competition":
Relation: sports season of league or competition  
Context: The 2020 NBA season was significantly impacted by the COVID-19 pandemic, leading to a unique playoff format.  
Head Entity: 2020  
Tail Entity: NBA  

Relation: sports season of league or competition  
Context: The 2019 Rugby World Cup took place in Japan, showcasing teams from around the globe.  
Head Entity: 2019  
Tail Entity: Rugby World Cup  
Losses:  4.073206901550293 -0.0 1.0666046142578125
MemoryTrain:  epoch  0, batch     0 | loss: 4.0732069Losses:  3.9014859199523926 0.28461354970932007 0.8649454116821289
MemoryTrain:  epoch  0, batch     1 | loss: 3.9014859Losses:  3.325878143310547 -0.0 0.9702343344688416
MemoryTrain:  epoch  0, batch     2 | loss: 3.3258781Losses:  3.2385430335998535 -0.0 0.9519959092140198
MemoryTrain:  epoch  0, batch     3 | loss: 3.2385430Losses:  3.238084316253662 -0.0 0.9517967700958252
MemoryTrain:  epoch  0, batch     4 | loss: 3.2380843Losses:  4.691864013671875 0.8598319292068481 0.9771615862846375
MemoryTrain:  epoch  0, batch     5 | loss: 4.6918640Losses:  4.469429969787598 0.4101964235305786 0.8842843174934387
MemoryTrain:  epoch  0, batch     6 | loss: 4.4694300Losses:  4.798759460449219 0.2742674946784973 0.841749370098114
MemoryTrain:  epoch  0, batch     7 | loss: 4.7987595Losses:  3.5219779014587402 0.24590229988098145 0.90693199634552
MemoryTrain:  epoch  0, batch     8 | loss: 3.5219779Losses:  4.842190265655518 0.8051154613494873 0.8588787913322449
MemoryTrain:  epoch  0, batch     9 | loss: 4.8421903Losses:  3.9598453044891357 -0.0 0.7975648045539856
MemoryTrain:  epoch  0, batch    10 | loss: 3.9598453Losses:  2.679041862487793 -0.0 0.3832549452781677
MemoryTrain:  epoch  0, batch    11 | loss: 2.6790419Losses:  3.8455893993377686 0.8268583416938782 0.7631679177284241
MemoryTrain:  epoch  1, batch     0 | loss: 3.8455894Losses:  4.113009452819824 0.22031033039093018 1.0635499954223633
MemoryTrain:  epoch  1, batch     1 | loss: 4.1130095Losses:  4.337277412414551 0.3259481191635132 0.8726268410682678
MemoryTrain:  epoch  1, batch     2 | loss: 4.3372774Losses:  3.384838581085205 0.23898287117481232 0.8880500793457031
MemoryTrain:  epoch  1, batch     3 | loss: 3.3848386Losses:  3.3780617713928223 0.5227315425872803 0.9972324967384338
MemoryTrain:  epoch  1, batch     4 | loss: 3.3780618Losses:  3.4930129051208496 0.23130787909030914 0.9214457273483276
MemoryTrain:  epoch  1, batch     5 | loss: 3.4930129Losses:  3.985290765762329 0.2409312129020691 1.0217790603637695
MemoryTrain:  epoch  1, batch     6 | loss: 3.9852908Losses:  3.107175588607788 0.2438770830631256 0.8943859934806824
MemoryTrain:  epoch  1, batch     7 | loss: 3.1071756Losses:  3.655515670776367 1.0930248498916626 0.8299970626831055
MemoryTrain:  epoch  1, batch     8 | loss: 3.6555157Losses:  3.201538562774658 0.3348623216152191 0.8579930067062378
MemoryTrain:  epoch  1, batch     9 | loss: 3.2015386Losses:  3.384070873260498 0.25047069787979126 0.9125288724899292
MemoryTrain:  epoch  1, batch    10 | loss: 3.3840709Losses:  3.059819221496582 -0.0 0.3443490266799927
MemoryTrain:  epoch  1, batch    11 | loss: 3.0598192Losses:  3.657334327697754 0.3384687900543213 1.0518602132797241
MemoryTrain:  epoch  2, batch     0 | loss: 3.6573343Losses:  3.535219192504883 0.24133068323135376 0.7144188284873962
MemoryTrain:  epoch  2, batch     1 | loss: 3.5352192Losses:  3.9657440185546875 0.7861640453338623 0.883640468120575
MemoryTrain:  epoch  2, batch     2 | loss: 3.9657440Losses:  2.6635923385620117 -0.0 0.9994423985481262
MemoryTrain:  epoch  2, batch     3 | loss: 2.6635923Losses:  2.949474811553955 -0.0 0.8758648633956909
MemoryTrain:  epoch  2, batch     4 | loss: 2.9494748Losses:  3.882791519165039 0.49173492193222046 0.8462867736816406
MemoryTrain:  epoch  2, batch     5 | loss: 3.8827915Losses:  3.2078638076782227 -0.0 0.9714233875274658
MemoryTrain:  epoch  2, batch     6 | loss: 3.2078638Losses:  2.96103835105896 0.4910488426685333 0.9716560244560242
MemoryTrain:  epoch  2, batch     7 | loss: 2.9610384Losses:  2.7356667518615723 0.25449684262275696 1.0077732801437378
MemoryTrain:  epoch  2, batch     8 | loss: 2.7356668Losses:  2.605914831161499 -0.0 1.0301927328109741
MemoryTrain:  epoch  2, batch     9 | loss: 2.6059148Losses:  2.933863401412964 0.7826170325279236 0.7878736853599548
MemoryTrain:  epoch  2, batch    10 | loss: 2.9338634Losses:  1.8264938592910767 -0.0 0.3121849000453949
MemoryTrain:  epoch  2, batch    11 | loss: 1.8264939Losses:  2.7073917388916016 0.264485239982605 1.0234694480895996
MemoryTrain:  epoch  3, batch     0 | loss: 2.7073917Losses:  2.9054369926452637 0.2471792995929718 0.961208701133728
MemoryTrain:  epoch  3, batch     1 | loss: 2.9054370Losses:  2.7100043296813965 0.25878095626831055 0.9097084403038025
MemoryTrain:  epoch  3, batch     2 | loss: 2.7100043Losses:  2.4320015907287598 -0.0 0.74934983253479
MemoryTrain:  epoch  3, batch     3 | loss: 2.4320016Losses:  3.2316670417785645 0.31667715311050415 1.0696862936019897
MemoryTrain:  epoch  3, batch     4 | loss: 3.2316670Losses:  2.9008629322052 0.26857882738113403 0.8759778141975403
MemoryTrain:  epoch  3, batch     5 | loss: 2.9008629Losses:  3.3075551986694336 0.7440718412399292 0.7005611658096313
MemoryTrain:  epoch  3, batch     6 | loss: 3.3075552Losses:  3.212057113647461 0.5581165552139282 0.9876489043235779
MemoryTrain:  epoch  3, batch     7 | loss: 3.2120571Losses:  3.2866554260253906 0.5294120907783508 0.8551340103149414
MemoryTrain:  epoch  3, batch     8 | loss: 3.2866554Losses:  3.2028684616088867 0.2649940848350525 0.8563070297241211
MemoryTrain:  epoch  3, batch     9 | loss: 3.2028685Losses:  2.3242123126983643 -0.0 0.8661336302757263
MemoryTrain:  epoch  3, batch    10 | loss: 2.3242123Losses:  1.4882768392562866 -0.0 0.31529751420021057
MemoryTrain:  epoch  3, batch    11 | loss: 1.4882768Losses:  2.786447286605835 0.2494106888771057 0.9050347208976746
MemoryTrain:  epoch  4, batch     0 | loss: 2.7864473Losses:  2.6687326431274414 -0.0 0.9980642795562744
MemoryTrain:  epoch  4, batch     1 | loss: 2.6687326Losses:  2.6592061519622803 0.2663373053073883 0.9669018983840942
MemoryTrain:  epoch  4, batch     2 | loss: 2.6592062Losses:  2.4068713188171387 -0.0 1.0128635168075562
MemoryTrain:  epoch  4, batch     3 | loss: 2.4068713Losses:  3.307990789413452 1.1308913230895996 0.7908279299736023
MemoryTrain:  epoch  4, batch     4 | loss: 3.3079908Losses:  2.5413525104522705 0.26549604535102844 0.9540380835533142
MemoryTrain:  epoch  4, batch     5 | loss: 2.5413525Losses:  2.5720643997192383 -0.0 0.923904299736023
MemoryTrain:  epoch  4, batch     6 | loss: 2.5720644Losses:  3.20422625541687 -0.0 0.9556682109832764
MemoryTrain:  epoch  4, batch     7 | loss: 3.2042263Losses:  2.81839656829834 -0.0 1.0414959192276
MemoryTrain:  epoch  4, batch     8 | loss: 2.8183966Losses:  2.6903958320617676 0.25203096866607666 0.8610371947288513
MemoryTrain:  epoch  4, batch     9 | loss: 2.6903958Losses:  2.8716654777526855 -0.0 0.9069916605949402
MemoryTrain:  epoch  4, batch    10 | loss: 2.8716655Losses:  1.9476227760314941 -0.0 0.3099880814552307
MemoryTrain:  epoch  4, batch    11 | loss: 1.9476228Losses:  2.1702239513397217 -0.0 0.8577647805213928
MemoryTrain:  epoch  5, batch     0 | loss: 2.1702240Losses:  4.398950576782227 1.5999842882156372 0.7545008063316345
MemoryTrain:  epoch  5, batch     1 | loss: 4.3989506Losses:  2.237760305404663 -0.0 0.9891952276229858
MemoryTrain:  epoch  5, batch     2 | loss: 2.2377603Losses:  2.6212003231048584 0.499264657497406 0.8324060440063477
MemoryTrain:  epoch  5, batch     3 | loss: 2.6212003Losses:  3.1315879821777344 0.24385634064674377 1.0652258396148682
MemoryTrain:  epoch  5, batch     4 | loss: 3.1315880Losses:  2.464629650115967 -0.0 1.0538866519927979
MemoryTrain:  epoch  5, batch     5 | loss: 2.4646297Losses:  3.304882526397705 0.5194647312164307 0.8971315026283264
MemoryTrain:  epoch  5, batch     6 | loss: 3.3048825Losses:  3.5895321369171143 1.3800464868545532 0.7810694575309753
MemoryTrain:  epoch  5, batch     7 | loss: 3.5895321Losses:  3.0027778148651123 0.4843897223472595 0.8840577602386475
MemoryTrain:  epoch  5, batch     8 | loss: 3.0027778Losses:  3.3933913707733154 1.1690657138824463 0.7901840806007385
MemoryTrain:  epoch  5, batch     9 | loss: 3.3933914Losses:  2.5677130222320557 0.2669811546802521 0.8488302230834961
MemoryTrain:  epoch  5, batch    10 | loss: 2.5677130Losses:  1.7145354747772217 -0.0 0.3142085671424866
MemoryTrain:  epoch  5, batch    11 | loss: 1.7145355Losses:  3.429283618927002 1.1343672275543213 0.9249796867370605
MemoryTrain:  epoch  6, batch     0 | loss: 3.4292836Losses:  2.5808188915252686 0.25384244322776794 0.8923535346984863
MemoryTrain:  epoch  6, batch     1 | loss: 2.5808189Losses:  2.6646995544433594 0.24235579371452332 0.8490397930145264
MemoryTrain:  epoch  6, batch     2 | loss: 2.6646996Losses:  2.4580278396606445 0.24186645448207855 0.7748998999595642
MemoryTrain:  epoch  6, batch     3 | loss: 2.4580278Losses:  2.3638408184051514 -0.0 1.0354609489440918
MemoryTrain:  epoch  6, batch     4 | loss: 2.3638408Losses:  2.16170072555542 -0.0 0.8969773054122925
MemoryTrain:  epoch  6, batch     5 | loss: 2.1617007Losses:  3.1554903984069824 0.7545734643936157 0.8279939293861389
MemoryTrain:  epoch  6, batch     6 | loss: 3.1554904Losses:  2.823824405670166 0.21913766860961914 0.8336833119392395
MemoryTrain:  epoch  6, batch     7 | loss: 2.8238244Losses:  2.799851894378662 0.5101112723350525 0.9434425830841064
MemoryTrain:  epoch  6, batch     8 | loss: 2.7998519Losses:  2.3847029209136963 -0.0 0.9529122114181519
MemoryTrain:  epoch  6, batch     9 | loss: 2.3847029Losses:  2.9743266105651855 0.7433341145515442 0.9304450750350952
MemoryTrain:  epoch  6, batch    10 | loss: 2.9743266Losses:  1.7666983604431152 -0.0 0.31071096658706665
MemoryTrain:  epoch  6, batch    11 | loss: 1.7666984Losses:  2.495321750640869 0.24030980467796326 0.7794545292854309
MemoryTrain:  epoch  7, batch     0 | loss: 2.4953218Losses:  2.803229808807373 0.7462413311004639 0.7058697938919067
MemoryTrain:  epoch  7, batch     1 | loss: 2.8032298Losses:  2.7527832984924316 0.49460315704345703 0.9085133075714111
MemoryTrain:  epoch  7, batch     2 | loss: 2.7527833Losses:  2.887962818145752 0.5253181457519531 1.0147836208343506
MemoryTrain:  epoch  7, batch     3 | loss: 2.8879628Losses:  2.4158403873443604 0.22881293296813965 0.7984704375267029
MemoryTrain:  epoch  7, batch     4 | loss: 2.4158404Losses:  2.5525126457214355 -0.0 1.1122026443481445
MemoryTrain:  epoch  7, batch     5 | loss: 2.5525126Losses:  2.587006092071533 0.2528504729270935 1.055083155632019
MemoryTrain:  epoch  7, batch     6 | loss: 2.5870061Losses:  2.8449933528900146 0.523647129535675 0.7284396290779114
MemoryTrain:  epoch  7, batch     7 | loss: 2.8449934Losses:  2.3831558227539062 0.24706709384918213 0.8499539494514465
MemoryTrain:  epoch  7, batch     8 | loss: 2.3831558Losses:  2.7954771518707275 0.2646271586418152 0.8945624232292175
MemoryTrain:  epoch  7, batch     9 | loss: 2.7954772Losses:  2.7794225215911865 0.5122348070144653 1.0156358480453491
MemoryTrain:  epoch  7, batch    10 | loss: 2.7794225Losses:  1.6371042728424072 -0.0 0.35060977935791016
MemoryTrain:  epoch  7, batch    11 | loss: 1.6371043Losses:  2.77006196975708 0.5003816485404968 0.9667675495147705
MemoryTrain:  epoch  8, batch     0 | loss: 2.7700620Losses:  2.5001494884490967 0.26498880982398987 0.9138540029525757
MemoryTrain:  epoch  8, batch     1 | loss: 2.5001495Losses:  2.434123992919922 0.20923221111297607 0.8804349899291992
MemoryTrain:  epoch  8, batch     2 | loss: 2.4341240Losses:  2.6664979457855225 0.4994470477104187 0.9137219786643982
MemoryTrain:  epoch  8, batch     3 | loss: 2.6664979Losses:  2.666968822479248 0.49448931217193604 0.8999046087265015
MemoryTrain:  epoch  8, batch     4 | loss: 2.6669688Losses:  2.7485244274139404 0.4850854277610779 0.9359883666038513
MemoryTrain:  epoch  8, batch     5 | loss: 2.7485244Losses:  2.5217761993408203 0.2524312138557434 1.0098446607589722
MemoryTrain:  epoch  8, batch     6 | loss: 2.5217762Losses:  2.9420177936553955 0.7495642900466919 0.849930465221405
MemoryTrain:  epoch  8, batch     7 | loss: 2.9420178Losses:  3.2721288204193115 1.0352795124053955 0.7755255103111267
MemoryTrain:  epoch  8, batch     8 | loss: 3.2721288Losses:  2.420377731323242 0.23084904253482819 0.9065499305725098
MemoryTrain:  epoch  8, batch     9 | loss: 2.4203777Losses:  2.108246326446533 -0.0 0.9057755470275879
MemoryTrain:  epoch  8, batch    10 | loss: 2.1082463Losses:  1.708634376525879 -0.0 0.3292492628097534
MemoryTrain:  epoch  8, batch    11 | loss: 1.7086344Losses:  2.52889347076416 0.24141934514045715 0.9693859219551086
MemoryTrain:  epoch  9, batch     0 | loss: 2.5288935Losses:  2.2969253063201904 0.24238544702529907 0.8309779167175293
MemoryTrain:  epoch  9, batch     1 | loss: 2.2969253Losses:  2.170844554901123 -0.0 0.9408751130104065
MemoryTrain:  epoch  9, batch     2 | loss: 2.1708446Losses:  2.494882106781006 0.25921571254730225 0.8993209600448608
MemoryTrain:  epoch  9, batch     3 | loss: 2.4948821Losses:  2.680542230606079 0.511321485042572 0.9133898615837097
MemoryTrain:  epoch  9, batch     4 | loss: 2.6805422Losses:  2.307136058807373 -0.0 1.0572872161865234
MemoryTrain:  epoch  9, batch     5 | loss: 2.3071361Losses:  2.414121150970459 0.24609948694705963 0.9337736368179321
MemoryTrain:  epoch  9, batch     6 | loss: 2.4141212Losses:  2.7743020057678223 0.6870429515838623 0.837530791759491
MemoryTrain:  epoch  9, batch     7 | loss: 2.7743020Losses:  2.435626268386841 0.2397337257862091 0.9341275691986084
MemoryTrain:  epoch  9, batch     8 | loss: 2.4356263Losses:  2.3381333351135254 0.25108009576797485 0.8632514476776123
MemoryTrain:  epoch  9, batch     9 | loss: 2.3381333Losses:  2.731189727783203 0.48189711570739746 1.0081658363342285
MemoryTrain:  epoch  9, batch    10 | loss: 2.7311897Losses:  1.8526272773742676 0.31293076276779175 0.3351013660430908
MemoryTrain:  epoch  9, batch    11 | loss: 1.8526273
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    1 | acc: 68.75%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 68.75%,  total acc: 77.08%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 75.00%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 79.17%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 79.46%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 82.03%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 84.03%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 83.75%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 85.23%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 84.38%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 84.62%   [EVAL] batch:   13 | acc: 56.25%,  total acc: 82.59%   [EVAL] batch:   14 | acc: 81.25%,  total acc: 82.50%   [EVAL] batch:   15 | acc: 81.25%,  total acc: 82.42%   [EVAL] batch:   16 | acc: 81.25%,  total acc: 82.35%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 81.60%   [EVAL] batch:   18 | acc: 87.50%,  total acc: 81.91%   [EVAL] batch:   19 | acc: 68.75%,  total acc: 81.25%   [EVAL] batch:   20 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:   21 | acc: 75.00%,  total acc: 80.97%   [EVAL] batch:   22 | acc: 81.25%,  total acc: 80.98%   [EVAL] batch:   23 | acc: 62.50%,  total acc: 80.21%   [EVAL] batch:   24 | acc: 68.75%,  total acc: 79.75%   [EVAL] batch:   25 | acc: 81.25%,  total acc: 79.81%   [EVAL] batch:   26 | acc: 75.00%,  total acc: 79.63%   [EVAL] batch:   27 | acc: 81.25%,  total acc: 79.69%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 80.39%   [EVAL] batch:   29 | acc: 50.00%,  total acc: 79.38%   [EVAL] batch:   30 | acc: 62.50%,  total acc: 78.83%   [EVAL] batch:   31 | acc: 68.75%,  total acc: 78.52%   [EVAL] batch:   32 | acc: 50.00%,  total acc: 77.65%   [EVAL] batch:   33 | acc: 50.00%,  total acc: 76.84%   [EVAL] batch:   34 | acc: 56.25%,  total acc: 76.25%   [EVAL] batch:   35 | acc: 68.75%,  total acc: 76.04%   [EVAL] batch:   36 | acc: 43.75%,  total acc: 75.17%   [EVAL] batch:   37 | acc: 75.00%,  total acc: 75.16%   [EVAL] batch:   38 | acc: 62.50%,  total acc: 74.84%   [EVAL] batch:   39 | acc: 87.50%,  total acc: 75.16%   [EVAL] batch:   40 | acc: 68.75%,  total acc: 75.00%   [EVAL] batch:   41 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:   42 | acc: 87.50%,  total acc: 75.29%   [EVAL] batch:   43 | acc: 62.50%,  total acc: 75.00%   [EVAL] batch:   44 | acc: 25.00%,  total acc: 73.89%   [EVAL] batch:   45 | acc: 31.25%,  total acc: 72.96%   [EVAL] batch:   46 | acc: 31.25%,  total acc: 72.07%   [EVAL] batch:   47 | acc: 37.50%,  total acc: 71.35%   [EVAL] batch:   48 | acc: 18.75%,  total acc: 70.28%   [EVAL] batch:   49 | acc: 43.75%,  total acc: 69.75%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 70.34%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 70.91%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 71.46%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 71.99%   [EVAL] batch:   54 | acc: 100.00%,  total acc: 72.50%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 72.99%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 73.46%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 73.81%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 74.15%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 74.48%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 74.90%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 75.10%   [EVAL] batch:   62 | acc: 37.50%,  total acc: 74.50%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 79.17%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 78.12%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 78.75%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 80.21%   [EVAL] batch:    6 | acc: 56.25%,  total acc: 76.79%   [EVAL] batch:    7 | acc: 50.00%,  total acc: 73.44%   [EVAL] batch:    8 | acc: 56.25%,  total acc: 71.53%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 71.25%   [EVAL] batch:   10 | acc: 37.50%,  total acc: 68.18%   [EVAL] batch:   11 | acc: 43.75%,  total acc: 66.15%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 66.83%   [EVAL] batch:   13 | acc: 87.50%,  total acc: 68.30%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 70.00%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 71.88%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 73.16%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 74.31%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 75.66%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 75.62%   [EVAL] batch:   20 | acc: 81.25%,  total acc: 75.89%   [EVAL] batch:   21 | acc: 75.00%,  total acc: 75.85%   [EVAL] batch:   22 | acc: 62.50%,  total acc: 75.27%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 75.78%   [EVAL] batch:   24 | acc: 75.00%,  total acc: 75.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 76.68%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 77.55%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 78.35%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 78.88%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 79.58%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 80.24%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 80.86%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 81.44%   [EVAL] batch:   33 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 81.07%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 81.42%   [EVAL] batch:   36 | acc: 87.50%,  total acc: 81.59%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 81.91%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 82.37%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 82.81%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 83.23%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 83.63%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 83.87%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 84.09%   [EVAL] batch:   44 | acc: 75.00%,  total acc: 83.89%   [EVAL] batch:   45 | acc: 81.25%,  total acc: 83.83%   [EVAL] batch:   46 | acc: 68.75%,  total acc: 83.51%   [EVAL] batch:   47 | acc: 68.75%,  total acc: 83.20%   [EVAL] batch:   48 | acc: 87.50%,  total acc: 83.29%   [EVAL] batch:   49 | acc: 75.00%,  total acc: 83.12%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 83.33%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 83.53%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 83.73%   [EVAL] batch:   53 | acc: 93.75%,  total acc: 83.91%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 83.86%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 84.04%   [EVAL] batch:   56 | acc: 81.25%,  total acc: 83.99%   [EVAL] batch:   57 | acc: 43.75%,  total acc: 83.30%   [EVAL] batch:   58 | acc: 68.75%,  total acc: 83.05%   [EVAL] batch:   59 | acc: 87.50%,  total acc: 83.12%   [EVAL] batch:   60 | acc: 81.25%,  total acc: 83.09%   [EVAL] batch:   61 | acc: 75.00%,  total acc: 82.96%   [EVAL] batch:   62 | acc: 56.25%,  total acc: 82.54%   [EVAL] batch:   63 | acc: 50.00%,  total acc: 82.03%   [EVAL] batch:   64 | acc: 31.25%,  total acc: 81.25%   [EVAL] batch:   65 | acc: 75.00%,  total acc: 81.16%   [EVAL] batch:   66 | acc: 37.50%,  total acc: 80.50%   [EVAL] batch:   67 | acc: 37.50%,  total acc: 79.87%   [EVAL] batch:   68 | acc: 75.00%,  total acc: 79.80%   [EVAL] batch:   69 | acc: 87.50%,  total acc: 79.91%   [EVAL] batch:   70 | acc: 81.25%,  total acc: 79.93%   [EVAL] batch:   71 | acc: 75.00%,  total acc: 79.86%   [EVAL] batch:   72 | acc: 93.75%,  total acc: 80.05%   [EVAL] batch:   73 | acc: 100.00%,  total acc: 80.32%   [EVAL] batch:   74 | acc: 81.25%,  total acc: 80.33%   [EVAL] batch:   75 | acc: 25.00%,  total acc: 79.61%   [EVAL] batch:   76 | acc: 25.00%,  total acc: 78.90%   [EVAL] batch:   77 | acc: 25.00%,  total acc: 78.21%   [EVAL] batch:   78 | acc: 31.25%,  total acc: 77.61%   [EVAL] batch:   79 | acc: 43.75%,  total acc: 77.19%   [EVAL] batch:   80 | acc: 25.00%,  total acc: 76.54%   [EVAL] batch:   81 | acc: 75.00%,  total acc: 76.52%   [EVAL] batch:   82 | acc: 100.00%,  total acc: 76.81%   [EVAL] batch:   83 | acc: 100.00%,  total acc: 77.08%   [EVAL] batch:   84 | acc: 93.75%,  total acc: 77.28%   [EVAL] batch:   85 | acc: 100.00%,  total acc: 77.54%   [EVAL] batch:   86 | acc: 93.75%,  total acc: 77.73%   [EVAL] batch:   87 | acc: 87.50%,  total acc: 77.84%   [EVAL] batch:   88 | acc: 43.75%,  total acc: 77.46%   [EVAL] batch:   89 | acc: 37.50%,  total acc: 77.01%   [EVAL] batch:   90 | acc: 43.75%,  total acc: 76.65%   [EVAL] batch:   91 | acc: 56.25%,  total acc: 76.43%   [EVAL] batch:   92 | acc: 25.00%,  total acc: 75.87%   [EVAL] batch:   93 | acc: 68.75%,  total acc: 75.80%   [EVAL] batch:   94 | acc: 81.25%,  total acc: 75.86%   [EVAL] batch:   95 | acc: 75.00%,  total acc: 75.85%   [EVAL] batch:   96 | acc: 93.75%,  total acc: 76.03%   [EVAL] batch:   97 | acc: 81.25%,  total acc: 76.08%   [EVAL] batch:   98 | acc: 75.00%,  total acc: 76.07%   [EVAL] batch:   99 | acc: 93.75%,  total acc: 76.25%   [EVAL] batch:  100 | acc: 56.25%,  total acc: 76.05%   [EVAL] batch:  101 | acc: 68.75%,  total acc: 75.98%   [EVAL] batch:  102 | acc: 50.00%,  total acc: 75.73%   [EVAL] batch:  103 | acc: 62.50%,  total acc: 75.60%   [EVAL] batch:  104 | acc: 43.75%,  total acc: 75.30%   [EVAL] batch:  105 | acc: 68.75%,  total acc: 75.24%   [EVAL] batch:  106 | acc: 25.00%,  total acc: 74.77%   [EVAL] batch:  107 | acc: 12.50%,  total acc: 74.19%   [EVAL] batch:  108 | acc: 12.50%,  total acc: 73.62%   [EVAL] batch:  109 | acc: 12.50%,  total acc: 73.07%   [EVAL] batch:  110 | acc: 18.75%,  total acc: 72.58%   [EVAL] batch:  111 | acc: 25.00%,  total acc: 72.15%   [EVAL] batch:  112 | acc: 31.25%,  total acc: 71.79%   [EVAL] batch:  113 | acc: 37.50%,  total acc: 71.49%   [EVAL] batch:  114 | acc: 81.25%,  total acc: 71.58%   [EVAL] batch:  115 | acc: 62.50%,  total acc: 71.50%   [EVAL] batch:  116 | acc: 62.50%,  total acc: 71.42%   [EVAL] batch:  117 | acc: 62.50%,  total acc: 71.35%   [EVAL] batch:  118 | acc: 75.00%,  total acc: 71.38%   [EVAL] batch:  119 | acc: 87.50%,  total acc: 71.51%   [EVAL] batch:  120 | acc: 87.50%,  total acc: 71.64%   [EVAL] batch:  121 | acc: 75.00%,  total acc: 71.67%   [EVAL] batch:  122 | acc: 56.25%,  total acc: 71.54%   [EVAL] batch:  123 | acc: 75.00%,  total acc: 71.57%   [EVAL] batch:  124 | acc: 75.00%,  total acc: 71.60%   [EVAL] batch:  125 | acc: 25.00%,  total acc: 71.23%   [EVAL] batch:  126 | acc: 18.75%,  total acc: 70.82%   [EVAL] batch:  127 | acc: 6.25%,  total acc: 70.31%   [EVAL] batch:  128 | acc: 18.75%,  total acc: 69.91%   [EVAL] batch:  129 | acc: 6.25%,  total acc: 69.42%   [EVAL] batch:  130 | acc: 6.25%,  total acc: 68.94%   [EVAL] batch:  131 | acc: 31.25%,  total acc: 68.66%   [EVAL] batch:  132 | acc: 62.50%,  total acc: 68.61%   [EVAL] batch:  133 | acc: 68.75%,  total acc: 68.61%   [EVAL] batch:  134 | acc: 81.25%,  total acc: 68.70%   [EVAL] batch:  135 | acc: 87.50%,  total acc: 68.84%   [EVAL] batch:  136 | acc: 62.50%,  total acc: 68.80%   [EVAL] batch:  137 | acc: 62.50%,  total acc: 68.75%   [EVAL] batch:  138 | acc: 18.75%,  total acc: 68.39%   [EVAL] batch:  139 | acc: 25.00%,  total acc: 68.08%   [EVAL] batch:  140 | acc: 18.75%,  total acc: 67.73%   [EVAL] batch:  141 | acc: 12.50%,  total acc: 67.34%   [EVAL] batch:  142 | acc: 37.50%,  total acc: 67.13%   [EVAL] batch:  143 | acc: 56.25%,  total acc: 67.06%   [EVAL] batch:  144 | acc: 100.00%,  total acc: 67.28%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 67.51%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 67.69%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 67.91%   [EVAL] batch:  148 | acc: 93.75%,  total acc: 68.08%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 68.25%   [EVAL] batch:  150 | acc: 100.00%,  total acc: 68.46%   [EVAL] batch:  151 | acc: 100.00%,  total acc: 68.67%   [EVAL] batch:  152 | acc: 100.00%,  total acc: 68.87%   [EVAL] batch:  153 | acc: 93.75%,  total acc: 69.03%   [EVAL] batch:  154 | acc: 100.00%,  total acc: 69.23%   [EVAL] batch:  155 | acc: 93.75%,  total acc: 69.39%   [EVAL] batch:  156 | acc: 100.00%,  total acc: 69.59%   [EVAL] batch:  157 | acc: 100.00%,  total acc: 69.78%   [EVAL] batch:  158 | acc: 93.75%,  total acc: 69.93%   [EVAL] batch:  159 | acc: 87.50%,  total acc: 70.04%   [EVAL] batch:  160 | acc: 93.75%,  total acc: 70.19%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 70.33%   [EVAL] batch:  162 | acc: 50.00%,  total acc: 70.21%   [EVAL] batch:  163 | acc: 25.00%,  total acc: 69.93%   [EVAL] batch:  164 | acc: 12.50%,  total acc: 69.58%   [EVAL] batch:  165 | acc: 25.00%,  total acc: 69.31%   [EVAL] batch:  166 | acc: 12.50%,  total acc: 68.97%   [EVAL] batch:  167 | acc: 18.75%,  total acc: 68.68%   [EVAL] batch:  168 | acc: 50.00%,  total acc: 68.57%   [EVAL] batch:  169 | acc: 81.25%,  total acc: 68.64%   [EVAL] batch:  170 | acc: 81.25%,  total acc: 68.71%   [EVAL] batch:  171 | acc: 81.25%,  total acc: 68.79%   [EVAL] batch:  172 | acc: 81.25%,  total acc: 68.86%   [EVAL] batch:  173 | acc: 68.75%,  total acc: 68.86%   [EVAL] batch:  174 | acc: 68.75%,  total acc: 68.86%   [EVAL] batch:  175 | acc: 50.00%,  total acc: 68.75%   [EVAL] batch:  176 | acc: 50.00%,  total acc: 68.64%   [EVAL] batch:  177 | acc: 37.50%,  total acc: 68.47%   [EVAL] batch:  178 | acc: 56.25%,  total acc: 68.40%   [EVAL] batch:  179 | acc: 56.25%,  total acc: 68.33%   [EVAL] batch:  180 | acc: 25.00%,  total acc: 68.09%   [EVAL] batch:  181 | acc: 62.50%,  total acc: 68.06%   [EVAL] batch:  182 | acc: 87.50%,  total acc: 68.17%   [EVAL] batch:  183 | acc: 62.50%,  total acc: 68.14%   [EVAL] batch:  184 | acc: 93.75%,  total acc: 68.28%   [EVAL] batch:  185 | acc: 93.75%,  total acc: 68.41%   [EVAL] batch:  186 | acc: 93.75%,  total acc: 68.55%   [EVAL] batch:  187 | acc: 75.00%,  total acc: 68.58%   [EVAL] batch:  188 | acc: 62.50%,  total acc: 68.55%   [EVAL] batch:  189 | acc: 75.00%,  total acc: 68.59%   [EVAL] batch:  190 | acc: 87.50%,  total acc: 68.68%   [EVAL] batch:  191 | acc: 81.25%,  total acc: 68.75%   [EVAL] batch:  192 | acc: 81.25%,  total acc: 68.81%   [EVAL] batch:  193 | acc: 62.50%,  total acc: 68.78%   [EVAL] batch:  194 | acc: 87.50%,  total acc: 68.88%   [EVAL] batch:  195 | acc: 81.25%,  total acc: 68.94%   [EVAL] batch:  196 | acc: 68.75%,  total acc: 68.94%   [EVAL] batch:  197 | acc: 93.75%,  total acc: 69.07%   [EVAL] batch:  198 | acc: 87.50%,  total acc: 69.16%   [EVAL] batch:  199 | acc: 81.25%,  total acc: 69.22%   [EVAL] batch:  200 | acc: 87.50%,  total acc: 69.31%   [EVAL] batch:  201 | acc: 81.25%,  total acc: 69.37%   [EVAL] batch:  202 | acc: 68.75%,  total acc: 69.37%   [EVAL] batch:  203 | acc: 87.50%,  total acc: 69.45%   [EVAL] batch:  204 | acc: 75.00%,  total acc: 69.48%   [EVAL] batch:  205 | acc: 93.75%,  total acc: 69.60%   [EVAL] batch:  206 | acc: 31.25%,  total acc: 69.41%   [EVAL] batch:  207 | acc: 18.75%,  total acc: 69.17%   [EVAL] batch:  208 | acc: 18.75%,  total acc: 68.93%   [EVAL] batch:  209 | acc: 6.25%,  total acc: 68.63%   [EVAL] batch:  210 | acc: 18.75%,  total acc: 68.39%   [EVAL] batch:  211 | acc: 37.50%,  total acc: 68.25%   [EVAL] batch:  212 | acc: 50.00%,  total acc: 68.16%   [EVAL] batch:  213 | acc: 93.75%,  total acc: 68.28%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 68.43%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 68.58%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 68.72%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 68.86%   [EVAL] batch:  218 | acc: 93.75%,  total acc: 68.98%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 69.12%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 69.26%   [EVAL] batch:  221 | acc: 100.00%,  total acc: 69.40%   [EVAL] batch:  222 | acc: 100.00%,  total acc: 69.53%   [EVAL] batch:  223 | acc: 100.00%,  total acc: 69.67%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 69.81%   [EVAL] batch:  225 | acc: 68.75%,  total acc: 69.80%   [EVAL] batch:  226 | acc: 62.50%,  total acc: 69.77%   [EVAL] batch:  227 | acc: 62.50%,  total acc: 69.74%   [EVAL] batch:  228 | acc: 75.00%,  total acc: 69.76%   [EVAL] batch:  229 | acc: 93.75%,  total acc: 69.86%   [EVAL] batch:  230 | acc: 62.50%,  total acc: 69.83%   [EVAL] batch:  231 | acc: 68.75%,  total acc: 69.83%   [EVAL] batch:  232 | acc: 37.50%,  total acc: 69.69%   [EVAL] batch:  233 | acc: 31.25%,  total acc: 69.52%   [EVAL] batch:  234 | acc: 25.00%,  total acc: 69.34%   [EVAL] batch:  235 | acc: 50.00%,  total acc: 69.25%   [EVAL] batch:  236 | acc: 25.00%,  total acc: 69.07%   [EVAL] batch:  237 | acc: 62.50%,  total acc: 69.04%   [EVAL] batch:  238 | acc: 87.50%,  total acc: 69.12%   [EVAL] batch:  239 | acc: 100.00%,  total acc: 69.24%   [EVAL] batch:  240 | acc: 81.25%,  total acc: 69.29%   [EVAL] batch:  241 | acc: 87.50%,  total acc: 69.37%   [EVAL] batch:  242 | acc: 87.50%,  total acc: 69.44%   [EVAL] batch:  243 | acc: 81.25%,  total acc: 69.49%   [EVAL] batch:  244 | acc: 37.50%,  total acc: 69.36%   [EVAL] batch:  245 | acc: 31.25%,  total acc: 69.21%   [EVAL] batch:  246 | acc: 6.25%,  total acc: 68.95%   [EVAL] batch:  247 | acc: 50.00%,  total acc: 68.88%   [EVAL] batch:  248 | acc: 25.00%,  total acc: 68.70%   [EVAL] batch:  249 | acc: 43.75%,  total acc: 68.60%   [EVAL] batch:  250 | acc: 93.75%,  total acc: 68.70%   [EVAL] batch:  251 | acc: 81.25%,  total acc: 68.75%   [EVAL] batch:  252 | acc: 87.50%,  total acc: 68.82%   [EVAL] batch:  253 | acc: 87.50%,  total acc: 68.90%   [EVAL] batch:  254 | acc: 93.75%,  total acc: 69.00%   [EVAL] batch:  255 | acc: 100.00%,  total acc: 69.12%   [EVAL] batch:  256 | acc: 81.25%,  total acc: 69.16%   [EVAL] batch:  257 | acc: 87.50%,  total acc: 69.23%   [EVAL] batch:  258 | acc: 87.50%,  total acc: 69.31%   [EVAL] batch:  259 | acc: 93.75%,  total acc: 69.40%   [EVAL] batch:  260 | acc: 87.50%,  total acc: 69.47%   [EVAL] batch:  261 | acc: 87.50%,  total acc: 69.54%   [EVAL] batch:  262 | acc: 87.50%,  total acc: 69.61%   [EVAL] batch:  263 | acc: 81.25%,  total acc: 69.65%   [EVAL] batch:  264 | acc: 68.75%,  total acc: 69.65%   [EVAL] batch:  265 | acc: 75.00%,  total acc: 69.67%   [EVAL] batch:  266 | acc: 68.75%,  total acc: 69.66%   [EVAL] batch:  267 | acc: 75.00%,  total acc: 69.68%   [EVAL] batch:  268 | acc: 68.75%,  total acc: 69.68%   [EVAL] batch:  269 | acc: 18.75%,  total acc: 69.49%   [EVAL] batch:  270 | acc: 25.00%,  total acc: 69.33%   [EVAL] batch:  271 | acc: 12.50%,  total acc: 69.12%   [EVAL] batch:  272 | acc: 0.00%,  total acc: 68.86%   [EVAL] batch:  273 | acc: 18.75%,  total acc: 68.68%   [EVAL] batch:  274 | acc: 12.50%,  total acc: 68.48%   [EVAL] batch:  275 | acc: 100.00%,  total acc: 68.59%   [EVAL] batch:  276 | acc: 100.00%,  total acc: 68.70%   [EVAL] batch:  277 | acc: 100.00%,  total acc: 68.82%   [EVAL] batch:  278 | acc: 100.00%,  total acc: 68.93%   [EVAL] batch:  279 | acc: 100.00%,  total acc: 69.04%   [EVAL] batch:  280 | acc: 100.00%,  total acc: 69.15%   [EVAL] batch:  281 | acc: 68.75%,  total acc: 69.15%   [EVAL] batch:  282 | acc: 75.00%,  total acc: 69.17%   [EVAL] batch:  283 | acc: 62.50%,  total acc: 69.15%   [EVAL] batch:  284 | acc: 68.75%,  total acc: 69.14%   [EVAL] batch:  285 | acc: 68.75%,  total acc: 69.14%   [EVAL] batch:  286 | acc: 62.50%,  total acc: 69.12%   [EVAL] batch:  287 | acc: 68.75%,  total acc: 69.12%   [EVAL] batch:  288 | acc: 100.00%,  total acc: 69.23%   [EVAL] batch:  289 | acc: 100.00%,  total acc: 69.33%   [EVAL] batch:  290 | acc: 100.00%,  total acc: 69.44%   [EVAL] batch:  291 | acc: 100.00%,  total acc: 69.54%   [EVAL] batch:  292 | acc: 100.00%,  total acc: 69.65%   [EVAL] batch:  293 | acc: 93.75%,  total acc: 69.73%   [EVAL] batch:  294 | acc: 100.00%,  total acc: 69.83%   [EVAL] batch:  295 | acc: 100.00%,  total acc: 69.93%   [EVAL] batch:  296 | acc: 100.00%,  total acc: 70.03%   [EVAL] batch:  297 | acc: 100.00%,  total acc: 70.13%   [EVAL] batch:  298 | acc: 100.00%,  total acc: 70.23%   [EVAL] batch:  299 | acc: 100.00%,  total acc: 70.33%   [EVAL] batch:  300 | acc: 87.50%,  total acc: 70.39%   [EVAL] batch:  301 | acc: 93.75%,  total acc: 70.47%   [EVAL] batch:  302 | acc: 93.75%,  total acc: 70.54%   [EVAL] batch:  303 | acc: 93.75%,  total acc: 70.62%   [EVAL] batch:  304 | acc: 81.25%,  total acc: 70.66%   [EVAL] batch:  305 | acc: 93.75%,  total acc: 70.73%   [EVAL] batch:  306 | acc: 93.75%,  total acc: 70.81%   [EVAL] batch:  307 | acc: 93.75%,  total acc: 70.88%   [EVAL] batch:  308 | acc: 93.75%,  total acc: 70.95%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 71.05%   [EVAL] batch:  310 | acc: 100.00%,  total acc: 71.14%   [EVAL] batch:  311 | acc: 93.75%,  total acc: 71.21%   [EVAL] batch:  312 | acc: 93.75%,  total acc: 71.29%   [EVAL] batch:  313 | acc: 81.25%,  total acc: 71.32%   [EVAL] batch:  314 | acc: 75.00%,  total acc: 71.33%   [EVAL] batch:  315 | acc: 62.50%,  total acc: 71.30%   [EVAL] batch:  316 | acc: 68.75%,  total acc: 71.29%   [EVAL] batch:  317 | acc: 93.75%,  total acc: 71.36%   [EVAL] batch:  318 | acc: 81.25%,  total acc: 71.39%   [EVAL] batch:  319 | acc: 100.00%,  total acc: 71.48%   [EVAL] batch:  320 | acc: 100.00%,  total acc: 71.57%   [EVAL] batch:  321 | acc: 87.50%,  total acc: 71.62%   [EVAL] batch:  322 | acc: 93.75%,  total acc: 71.69%   [EVAL] batch:  323 | acc: 93.75%,  total acc: 71.76%   [EVAL] batch:  324 | acc: 75.00%,  total acc: 71.77%   [EVAL] batch:  325 | acc: 75.00%,  total acc: 71.78%   [EVAL] batch:  326 | acc: 68.75%,  total acc: 71.77%   [EVAL] batch:  327 | acc: 81.25%,  total acc: 71.80%   [EVAL] batch:  328 | acc: 75.00%,  total acc: 71.81%   [EVAL] batch:  329 | acc: 81.25%,  total acc: 71.84%   [EVAL] batch:  330 | acc: 75.00%,  total acc: 71.85%   [EVAL] batch:  331 | acc: 87.50%,  total acc: 71.89%   [EVAL] batch:  332 | acc: 56.25%,  total acc: 71.85%   [EVAL] batch:  333 | acc: 93.75%,  total acc: 71.91%   [EVAL] batch:  334 | acc: 68.75%,  total acc: 71.90%   [EVAL] batch:  335 | acc: 75.00%,  total acc: 71.91%   [EVAL] batch:  336 | acc: 56.25%,  total acc: 71.87%   [EVAL] batch:  337 | acc: 93.75%,  total acc: 71.93%   [EVAL] batch:  338 | acc: 75.00%,  total acc: 71.94%   [EVAL] batch:  339 | acc: 68.75%,  total acc: 71.93%   [EVAL] batch:  340 | acc: 93.75%,  total acc: 71.99%   [EVAL] batch:  341 | acc: 75.00%,  total acc: 72.00%   [EVAL] batch:  342 | acc: 50.00%,  total acc: 71.94%   [EVAL] batch:  343 | acc: 62.50%,  total acc: 71.91%   [EVAL] batch:  344 | acc: 68.75%,  total acc: 71.90%   [EVAL] batch:  345 | acc: 37.50%,  total acc: 71.80%   [EVAL] batch:  346 | acc: 56.25%,  total acc: 71.76%   [EVAL] batch:  347 | acc: 62.50%,  total acc: 71.73%   [EVAL] batch:  348 | acc: 75.00%,  total acc: 71.74%   [EVAL] batch:  349 | acc: 50.00%,  total acc: 71.68%   [EVAL] batch:  350 | acc: 62.50%,  total acc: 71.65%   [EVAL] batch:  351 | acc: 81.25%,  total acc: 71.68%   [EVAL] batch:  352 | acc: 75.00%,  total acc: 71.69%   [EVAL] batch:  353 | acc: 75.00%,  total acc: 71.70%   [EVAL] batch:  354 | acc: 75.00%,  total acc: 71.71%   [EVAL] batch:  355 | acc: 87.50%,  total acc: 71.75%   [EVAL] batch:  356 | acc: 43.75%,  total acc: 71.67%   [EVAL] batch:  357 | acc: 18.75%,  total acc: 71.53%   [EVAL] batch:  358 | acc: 31.25%,  total acc: 71.41%   [EVAL] batch:  359 | acc: 37.50%,  total acc: 71.32%   [EVAL] batch:  360 | acc: 25.00%,  total acc: 71.19%   [EVAL] batch:  361 | acc: 43.75%,  total acc: 71.12%   [EVAL] batch:  362 | acc: 62.50%,  total acc: 71.09%   [EVAL] batch:  363 | acc: 100.00%,  total acc: 71.17%   [EVAL] batch:  364 | acc: 100.00%,  total acc: 71.25%   [EVAL] batch:  365 | acc: 100.00%,  total acc: 71.33%   [EVAL] batch:  366 | acc: 100.00%,  total acc: 71.41%   [EVAL] batch:  367 | acc: 100.00%,  total acc: 71.48%   [EVAL] batch:  368 | acc: 100.00%,  total acc: 71.56%   [EVAL] batch:  369 | acc: 93.75%,  total acc: 71.62%   [EVAL] batch:  370 | acc: 100.00%,  total acc: 71.70%   [EVAL] batch:  371 | acc: 93.75%,  total acc: 71.76%   [EVAL] batch:  372 | acc: 93.75%,  total acc: 71.82%   [EVAL] batch:  373 | acc: 93.75%,  total acc: 71.88%   [EVAL] batch:  374 | acc: 81.25%,  total acc: 71.90%   
cur_acc:  ['0.9484', '0.7232', '0.7421', '0.6736', '0.8284', '0.7450']
his_acc:  ['0.9484', '0.8190', '0.7713', '0.7235', '0.7330', '0.7190']
Clustering into  34  clusters
Clusters:  [ 2  1  8  0  5  5  8  7 28  9  3 17 18  4 18 13 31 12 29 32 14 16 25 26
 12 15 25  6  9  6  1  6  7 17 20 27 11 25  0 25  0 21  2 28 22 10  1 13
 20  1 33 24  7  5 23  9  3  5 16 15 30 19 11  8 24 14 28 10  4  3]
Losses:  8.95777416229248 3.035521984100342 0.7453333139419556
CurrentTrain: epoch  0, batch     0 | loss: 8.9577742Losses:  9.679908752441406 3.45361328125 0.7604702115058899
CurrentTrain: epoch  0, batch     1 | loss: 9.6799088Losses:  9.446725845336914 2.746746301651001 0.8086199760437012
CurrentTrain: epoch  0, batch     2 | loss: 9.4467258Losses:  4.702590465545654 -0.0 0.12315298616886139
CurrentTrain: epoch  0, batch     3 | loss: 4.7025905Losses:  7.72454833984375 2.542389154434204 0.7239494323730469
CurrentTrain: epoch  1, batch     0 | loss: 7.7245483Losses:  7.92632532119751 2.6230902671813965 0.7517648339271545
CurrentTrain: epoch  1, batch     1 | loss: 7.9263253Losses:  9.392817497253418 3.9510672092437744 0.709533154964447
CurrentTrain: epoch  1, batch     2 | loss: 9.3928175Losses:  5.05399751663208 -0.0 -0.0
CurrentTrain: epoch  1, batch     3 | loss: 5.0539975Losses:  10.708196640014648 5.213823318481445 0.7082751393318176
CurrentTrain: epoch  2, batch     0 | loss: 10.7081966Losses:  8.08462905883789 3.340963363647461 0.6742125749588013
CurrentTrain: epoch  2, batch     1 | loss: 8.0846291Losses:  6.672597408294678 2.679619789123535 0.7333796620368958
CurrentTrain: epoch  2, batch     2 | loss: 6.6725974Losses:  3.6476142406463623 -0.0 0.10589396208524704
CurrentTrain: epoch  2, batch     3 | loss: 3.6476142Losses:  7.006354331970215 2.570075511932373 0.7283034920692444
CurrentTrain: epoch  3, batch     0 | loss: 7.0063543Losses:  6.891945838928223 2.6215133666992188 0.7511370778083801
CurrentTrain: epoch  3, batch     1 | loss: 6.8919458Losses:  8.062087059020996 3.44179105758667 0.7380248308181763
CurrentTrain: epoch  3, batch     2 | loss: 8.0620871Losses:  3.447092056274414 -0.0 0.1109379231929779
CurrentTrain: epoch  3, batch     3 | loss: 3.4470921Losses:  7.265958786010742 3.2359955310821533 0.6693539619445801
CurrentTrain: epoch  4, batch     0 | loss: 7.2659588Losses:  8.063785552978516 4.089409351348877 0.6514294147491455
CurrentTrain: epoch  4, batch     1 | loss: 8.0637856Losses:  7.1160478591918945 3.5489442348480225 0.662070095539093
CurrentTrain: epoch  4, batch     2 | loss: 7.1160479Losses:  6.303793430328369 -0.0 0.08417508751153946
CurrentTrain: epoch  4, batch     3 | loss: 6.3037934Losses:  5.658478736877441 2.124809980392456 0.7250224351882935
CurrentTrain: epoch  5, batch     0 | loss: 5.6584787Losses:  7.740536689758301 3.4102697372436523 0.6655807495117188
CurrentTrain: epoch  5, batch     1 | loss: 7.7405367Losses:  7.727354526519775 4.0922698974609375 0.7318096160888672
CurrentTrain: epoch  5, batch     2 | loss: 7.7273545Losses:  3.2329704761505127 -0.0 0.21955516934394836
CurrentTrain: epoch  5, batch     3 | loss: 3.2329705Losses:  8.379947662353516 4.546719551086426 0.6494393348693848
CurrentTrain: epoch  6, batch     0 | loss: 8.3799477Losses:  5.6164164543151855 2.539461612701416 0.7140350937843323
CurrentTrain: epoch  6, batch     1 | loss: 5.6164165Losses:  7.1503005027771 3.5751500129699707 0.7143434882164001
CurrentTrain: epoch  6, batch     2 | loss: 7.1503005Losses:  1.7987637519836426 -0.0 0.1302441954612732
CurrentTrain: epoch  6, batch     3 | loss: 1.7987638Losses:  5.514884948730469 2.1910758018493652 0.7279924750328064
CurrentTrain: epoch  7, batch     0 | loss: 5.5148849Losses:  6.654543876647949 3.2904629707336426 0.651074230670929
CurrentTrain: epoch  7, batch     1 | loss: 6.6545439Losses:  5.530759811401367 2.7188479900360107 0.6275489330291748
CurrentTrain: epoch  7, batch     2 | loss: 5.5307598Losses:  4.7062764167785645 -0.0 0.1139426976442337
CurrentTrain: epoch  7, batch     3 | loss: 4.7062764Losses:  5.215002059936523 2.4044294357299805 0.7101372480392456
CurrentTrain: epoch  8, batch     0 | loss: 5.2150021Losses:  4.82122802734375 1.8997722864151 0.7767063975334167
CurrentTrain: epoch  8, batch     1 | loss: 4.8212280Losses:  6.663567543029785 3.2275142669677734 0.7276155948638916
CurrentTrain: epoch  8, batch     2 | loss: 6.6635675Losses:  2.2262537479400635 -0.0 0.14095371961593628
CurrentTrain: epoch  8, batch     3 | loss: 2.2262537Losses:  5.279268264770508 2.1207022666931152 0.7774648070335388
CurrentTrain: epoch  9, batch     0 | loss: 5.2792683Losses:  4.635141849517822 1.8201899528503418 0.7675151824951172
CurrentTrain: epoch  9, batch     1 | loss: 4.6351418Losses:  5.073235988616943 2.194782257080078 0.7120212912559509
CurrentTrain: epoch  9, batch     2 | loss: 5.0732360Losses:  2.1385819911956787 -0.0 0.1239500418305397
CurrentTrain: epoch  9, batch     3 | loss: 2.1385820
#### contains administrative territorial entity ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "contains administrative territorial entity" means direct subdivisions of an administrative territorial entity.
Here is an example:
Relation: contains administrative territorial entity
Context: hettange - grande ( , lorraine franconian : " grouss - hetténgen " ) is a commune in the moselle department in grand est in north - eastern france .
Head Entity: moselle
Tail Entity: hettange
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "contains administrative territorial entity":
Relation: contains administrative territorial entity  
Context: The state of California is divided into several counties, including Los Angeles County, which is known for its entertainment industry.  
Head Entity: California  
Tail Entity: Los Angeles County  

Relation: contains administrative territorial entity  
Context: The province of Ontario is home to many cities, with Toronto being the capital and the largest city in the province.  
Head Entity: Ontario  
Tail Entity: Toronto  
#### said to be the same as ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "said to be the same as" means this item is said to be the same as that item, but it's uncertain or disputed.
Here is an example:
Relation: said to be the same as
Context: enzo is an italian given name derivative of heinz , a form of henry .
Head Entity: henry
Tail Entity: heinz
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "said to be the same as":
Relation: said to be the same as  
Context: The two species, Canis lupus and Canis familiaris, are often said to be the same as each other due to their close genetic relationship.  
Head Entity: Canis lupus  
Tail Entity: Canis familiaris  

Relation: said to be the same as  
Context: Many historians argue that the ancient city of Byzantium is said to be the same as modern-day Istanbul, although the transition involved significant changes over time.  
Head Entity: Byzantium  
Tail Entity: Istanbul  
#### winner ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "winner" means winner of a competition or similar event.
Here is an example:
Relation: winner
Context: he considers his father to be biggest influence on his career as his brother javier castellano recipient of four eclipse award for outstanding jockey in the row ( 2013 , 2014,2015 and 2016 ) .
Head Entity: eclipse award for outstanding jockey
Tail Entity: javier castellano
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "winner":
Relation: winner  
Context: In the thrilling finale of the championship, Sarah Thompson emerged victorious, claiming the title of best player in the tournament, while her teammate, Mark Johnson, was awarded the runner-up position.  
Head Entity: best player in the tournament  
Tail Entity: Sarah Thompson  

Relation: winner  
Context: The annual science fair concluded with Emily Chen taking home the grand prize for her innovative project on renewable energy, while her classmate, David Lee, received an honorable mention for his work on robotics.  
Head Entity: grand prize  
Tail Entity: Emily Chen  
#### military rank ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "military rank" means military rank achieved by a person, or military rank associated with a position.
Here is an example:
Relation: military rank
Context: the calais region was defended by the 15th army under " generaloberst " ( colonel general ) hans von salmuth , and normandy by the 7th army commanded by " generaloberst " friedrich dollmann .
Head Entity: hans von salmuth
Tail Entity: generaloberst
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "military rank":
Relation: military rank  
Context: during the battle of stalingrad, the soviet forces were led by general vasily chuikov, who held the rank of major general, while the german troops were commanded by general friedrich paulus.  
Head Entity: vasily chuikov  
Tail Entity: major general  

Relation: military rank  
Context: admiral horatio nelson was a key figure in the naval battles against the french, holding the rank of vice admiral during the napoleonic wars.  
Head Entity: horatio nelson  
Tail Entity: vice admiral  
#### publisher ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "publisher" means organization or person responsible for publishing books, periodicals, printed music, podcasts, games or software.
Here is an example:
Relation: publisher
Context: there are seven different nations that are allied or in conflict : prior to its north american release , " vanguard bandits " was titled " detonator gauntlet " by working designs .
Head Entity: vanguard bandits
Tail Entity: working designs
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "publisher":
Relation: publisher  
Context: The latest novel by the acclaimed author was released by Penguin Random House, a well-known publishing house in the industry.  
Head Entity: latest novel  
Tail Entity: Penguin Random House  

Relation: publisher  
Context: After years of hard work, the game was finally published by Electronic Arts, bringing excitement to gamers worldwide.  
Head Entity: game  
Tail Entity: Electronic Arts  
#### followed by ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "followed by" means immediately following item in a series of which the subject is a part.
Here is an example:
Relation: followed by
Context: " muzai moratorium " and " shōso strip " were released on vinyl in 2008 , as a part of her 10-year anniversary celebrations .
Head Entity: muzai moratorium
Tail Entity: shōso strip
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "followed by":
Relation: followed by  
Context: The first book in the series, "The Enchanted Forest," was published in 2015, followed by its sequel, "The Hidden Kingdom," in 2017.  
Head Entity: The Enchanted Forest  
Tail Entity: The Hidden Kingdom  

Relation: followed by  
Context: The opening act of the concert featured a local band, which was followed by the headlining artist who performed for over two hours.  
Head Entity: local band  
Tail Entity: headlining artist  
#### work location ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "work location" means location where persons or organisations were actively participating in employment, business or other work.
Here is an example:
Relation: work location
Context: since then they had been under the supervision of valerand poullain , formerly john calvin 's successor as minister of the french congregation in strasbourg .
Head Entity: john calvin
Tail Entity: strasbourg
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "work location":
Relation: work location  
Context: The headquarters of the company is situated in the bustling city of San Francisco, where many tech startups thrive.  
Head Entity: tech startups  
Tail Entity: San Francisco  

Relation: work location  
Context: After years of research, the team relocated to the prestigious Oxford University to continue their groundbreaking studies.  
Head Entity: the team  
Tail Entity: Oxford University  
#### occupation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "occupation" means occupation of a person.
Here is an example:
Relation: occupation
Context: andrei droznin ( russian : андрей дрознин ) is a russian theatre director and movement coach .
Head Entity: andrei droznin
Tail Entity: theatre director
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "occupation":
Relation: occupation  
Context: dr. emily thompson is a renowned biochemist known for her groundbreaking research in cancer treatment.  
Head Entity: dr. emily thompson  
Tail Entity: biochemist  

Relation: occupation  
Context: john smith, a talented software engineer, has been working at tech innovations for over five years.  
Head Entity: john smith  
Tail Entity: software engineer  
#### heritage designation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "heritage designation" means heritage designation of a cultural or natural site.
Here is an example:
Relation: heritage designation
Context: altamaha town , bluffton historic district , church of the cross , and rose hill plantation house are listed on the national register of historic places .
Head Entity: altamaha town
Tail Entity: national register of historic places
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "heritage designation":
Relation: heritage designation  
Context: The Great Wall of China is recognized as a UNESCO World Heritage Site due to its historical significance and architectural grandeur.  
Head Entity: Great Wall of China  
Tail Entity: UNESCO World Heritage Site  

Relation: heritage designation  
Context: The ancient city of Petra in Jordan is designated as a World Heritage Site, attracting tourists from around the globe.  
Head Entity: Petra  
Tail Entity: World Heritage Site  
#### voice type ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "voice type" means person's voice type.
Here is an example:
Relation: voice type
Context: beverly bower ( september 30 , 1925 – march 24 , 2002 ) was an american operatic soprano who had an active international opera career from the mid-1950s through the early 1970s .
Head Entity: beverly bower
Tail Entity: soprano
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "voice type":
Relation: voice type  
Context: luciano pavarotti ( october 12 , 1935 – september 6 , 2007 ) was an italian tenor who gained worldwide fame for his televised concerts and media appearances.  
Head Entity: luciano pavarotti  
Tail Entity: tenor  

Relation: voice type  
Context: mariah carey ( born march 27 , 1969 ) is an american singer, songwriter, and actress known for her five-octave vocal range and her use of the whistle register.  
Head Entity: mariah carey  
Tail Entity: singer  
Losses:  4.586976051330566 0.8087431192398071 1.017290711402893
MemoryTrain:  epoch  0, batch     0 | loss: 4.5869761Losses:  3.4727697372436523 0.502977192401886 0.909190833568573
MemoryTrain:  epoch  0, batch     1 | loss: 3.4727697Losses:  4.828059196472168 0.25937968492507935 1.0960376262664795
MemoryTrain:  epoch  0, batch     2 | loss: 4.8280592Losses:  3.897517204284668 0.5119248628616333 1.0053385496139526
MemoryTrain:  epoch  0, batch     3 | loss: 3.8975172Losses:  3.494654893875122 0.22607824206352234 0.9528260231018066
MemoryTrain:  epoch  0, batch     4 | loss: 3.4946549Losses:  4.038293838500977 0.5147913098335266 1.022172451019287
MemoryTrain:  epoch  0, batch     5 | loss: 4.0382938Losses:  3.9858546257019043 0.49947410821914673 0.9145030379295349
MemoryTrain:  epoch  0, batch     6 | loss: 3.9858546Losses:  3.7742600440979004 -0.0 1.0517324209213257
MemoryTrain:  epoch  0, batch     7 | loss: 3.7742600Losses:  4.563035011291504 0.8368292450904846 0.9328055381774902
MemoryTrain:  epoch  0, batch     8 | loss: 4.5630350Losses:  4.019092559814453 -0.0 1.0401417016983032
MemoryTrain:  epoch  0, batch     9 | loss: 4.0190926Losses:  3.4958271980285645 0.2374001145362854 0.9699837565422058
MemoryTrain:  epoch  0, batch    10 | loss: 3.4958272Losses:  3.724806308746338 0.26255038380622864 0.984065592288971
MemoryTrain:  epoch  0, batch    11 | loss: 3.7248063Losses:  3.9692580699920654 0.37495797872543335 0.8941213488578796
MemoryTrain:  epoch  0, batch    12 | loss: 3.9692581Losses:  2.56289005279541 -0.0 0.11277714371681213
MemoryTrain:  epoch  0, batch    13 | loss: 2.5628901Losses:  3.3875913619995117 0.2488187849521637 0.9850783944129944
MemoryTrain:  epoch  1, batch     0 | loss: 3.3875914Losses:  3.2774853706359863 0.24892152845859528 0.9944247007369995
MemoryTrain:  epoch  1, batch     1 | loss: 3.2774854Losses:  3.3080320358276367 0.24935516715049744 0.9547117948532104
MemoryTrain:  epoch  1, batch     2 | loss: 3.3080320Losses:  3.387594699859619 0.2678922414779663 1.0225481986999512
MemoryTrain:  epoch  1, batch     3 | loss: 3.3875947Losses:  3.510861396789551 0.2523445188999176 0.9711399674415588
MemoryTrain:  epoch  1, batch     4 | loss: 3.5108614Losses:  2.784130334854126 -0.0 0.9565609097480774
MemoryTrain:  epoch  1, batch     5 | loss: 2.7841303Losses:  3.439673662185669 0.2831186056137085 0.9229523539543152
MemoryTrain:  epoch  1, batch     6 | loss: 3.4396737Losses:  4.719307899475098 0.25749364495277405 1.057661771774292
MemoryTrain:  epoch  1, batch     7 | loss: 4.7193079Losses:  3.46177339553833 0.25595104694366455 0.9154729247093201
MemoryTrain:  epoch  1, batch     8 | loss: 3.4617734Losses:  3.5760979652404785 0.2713560461997986 0.8639678955078125
MemoryTrain:  epoch  1, batch     9 | loss: 3.5760980Losses:  2.798190116882324 0.24168099462985992 1.0114328861236572
MemoryTrain:  epoch  1, batch    10 | loss: 2.7981901Losses:  3.3468754291534424 0.23989427089691162 0.8722302913665771
MemoryTrain:  epoch  1, batch    11 | loss: 3.3468754Losses:  3.195157051086426 0.23749420046806335 0.9677032232284546
MemoryTrain:  epoch  1, batch    12 | loss: 3.1951571Losses:  1.445151925086975 -0.0 0.10077895224094391
MemoryTrain:  epoch  1, batch    13 | loss: 1.4451519Losses:  3.3419413566589355 -0.0 0.9601754546165466
MemoryTrain:  epoch  2, batch     0 | loss: 3.3419414Losses:  3.355440616607666 0.7086060047149658 0.845718264579773
MemoryTrain:  epoch  2, batch     1 | loss: 3.3554406Losses:  2.955869436264038 -0.0 1.0319697856903076
MemoryTrain:  epoch  2, batch     2 | loss: 2.9558694Losses:  3.5739850997924805 0.5369930863380432 0.9044135808944702
MemoryTrain:  epoch  2, batch     3 | loss: 3.5739851Losses:  3.1774468421936035 0.25216513872146606 0.9704747200012207
MemoryTrain:  epoch  2, batch     4 | loss: 3.1774468Losses:  2.9322686195373535 -0.0 0.9826127886772156
MemoryTrain:  epoch  2, batch     5 | loss: 2.9322686Losses:  2.834035873413086 0.24322417378425598 0.9119647741317749
MemoryTrain:  epoch  2, batch     6 | loss: 2.8340359Losses:  2.9370436668395996 0.4929192066192627 0.9978219866752625
MemoryTrain:  epoch  2, batch     7 | loss: 2.9370437Losses:  2.959831953048706 0.509032130241394 0.97862708568573
MemoryTrain:  epoch  2, batch     8 | loss: 2.9598320Losses:  3.1916613578796387 0.508868932723999 0.864733099937439
MemoryTrain:  epoch  2, batch     9 | loss: 3.1916614Losses:  2.249911308288574 -0.0 1.0266311168670654
MemoryTrain:  epoch  2, batch    10 | loss: 2.2499113Losses:  3.4761834144592285 0.2551214396953583 0.9199935793876648
MemoryTrain:  epoch  2, batch    11 | loss: 3.4761834Losses:  2.781905174255371 -0.0 1.0201796293258667
MemoryTrain:  epoch  2, batch    12 | loss: 2.7819052Losses:  1.4312443733215332 -0.0 0.11377014219760895
MemoryTrain:  epoch  2, batch    13 | loss: 1.4312444Losses:  2.488853693008423 -0.0 1.014587640762329
MemoryTrain:  epoch  3, batch     0 | loss: 2.4888537Losses:  2.5904202461242676 0.2855266034603119 0.9146226644515991
MemoryTrain:  epoch  3, batch     1 | loss: 2.5904202Losses:  2.5890955924987793 0.2422681450843811 0.9796913862228394
MemoryTrain:  epoch  3, batch     2 | loss: 2.5890956Losses:  2.868382453918457 0.23528450727462769 1.0879395008087158
MemoryTrain:  epoch  3, batch     3 | loss: 2.8683825Losses:  2.4951348304748535 -0.0 1.0287929773330688
MemoryTrain:  epoch  3, batch     4 | loss: 2.4951348Losses:  2.924212694168091 0.2617422640323639 0.9260613322257996
MemoryTrain:  epoch  3, batch     5 | loss: 2.9242127Losses:  4.81624174118042 1.4326341152191162 0.8959025740623474
MemoryTrain:  epoch  3, batch     6 | loss: 4.8162417Losses:  2.8812952041625977 0.26509344577789307 1.0026029348373413
MemoryTrain:  epoch  3, batch     7 | loss: 2.8812952Losses:  2.6834585666656494 -0.0 1.0710945129394531
MemoryTrain:  epoch  3, batch     8 | loss: 2.6834586Losses:  2.9069604873657227 -0.0 0.9725981950759888
MemoryTrain:  epoch  3, batch     9 | loss: 2.9069605Losses:  2.4812378883361816 -0.0 0.9697522521018982
MemoryTrain:  epoch  3, batch    10 | loss: 2.4812379Losses:  2.68774676322937 -0.0 1.0085957050323486
MemoryTrain:  epoch  3, batch    11 | loss: 2.6877468Losses:  2.8154351711273193 0.466773122549057 0.9022650122642517
MemoryTrain:  epoch  3, batch    12 | loss: 2.8154352Losses:  1.3702760934829712 -0.0 0.1114250123500824
MemoryTrain:  epoch  3, batch    13 | loss: 1.3702761Losses:  2.3017919063568115 -0.0 1.0026671886444092
MemoryTrain:  epoch  4, batch     0 | loss: 2.3017919Losses:  2.936269760131836 0.5204460620880127 0.9811415672302246
MemoryTrain:  epoch  4, batch     1 | loss: 2.9362698Losses:  2.2511587142944336 -0.0 1.0058262348175049
MemoryTrain:  epoch  4, batch     2 | loss: 2.2511587Losses:  3.2120542526245117 0.5191789269447327 1.0191535949707031
MemoryTrain:  epoch  4, batch     3 | loss: 3.2120543Losses:  2.1574866771698 -0.0 0.856324315071106
MemoryTrain:  epoch  4, batch     4 | loss: 2.1574867Losses:  2.689805507659912 0.2483735829591751 1.0620225667953491
MemoryTrain:  epoch  4, batch     5 | loss: 2.6898055Losses:  2.7533679008483887 -0.0 1.1272646188735962
MemoryTrain:  epoch  4, batch     6 | loss: 2.7533679Losses:  3.4804155826568604 1.0940475463867188 0.7707738280296326
MemoryTrain:  epoch  4, batch     7 | loss: 3.4804156Losses:  2.4046263694763184 0.2449193298816681 0.9097532033920288
MemoryTrain:  epoch  4, batch     8 | loss: 2.4046264Losses:  2.756500005722046 0.25343650579452515 0.9149002432823181
MemoryTrain:  epoch  4, batch     9 | loss: 2.7565000Losses:  2.490304470062256 0.25294721126556396 0.9317468404769897
MemoryTrain:  epoch  4, batch    10 | loss: 2.4903045Losses:  3.5127058029174805 0.26210981607437134 1.066489577293396
MemoryTrain:  epoch  4, batch    11 | loss: 3.5127058Losses:  3.2963781356811523 0.502260148525238 0.851433515548706
MemoryTrain:  epoch  4, batch    12 | loss: 3.2963781Losses:  2.794130325317383 -0.0 0.1379886418581009
MemoryTrain:  epoch  4, batch    13 | loss: 2.7941303Losses:  3.2592580318450928 0.5924615859985352 0.8632485270500183
MemoryTrain:  epoch  5, batch     0 | loss: 3.2592580Losses:  2.5500600337982178 0.2462523877620697 0.9801825284957886
MemoryTrain:  epoch  5, batch     1 | loss: 2.5500600Losses:  2.5334694385528564 -0.0 1.0754202604293823
MemoryTrain:  epoch  5, batch     2 | loss: 2.5334694Losses:  4.14098596572876 1.5082862377166748 0.7674530148506165
MemoryTrain:  epoch  5, batch     3 | loss: 4.1409860Losses:  2.8154749870300293 0.7232634425163269 0.8378065824508667
MemoryTrain:  epoch  5, batch     4 | loss: 2.8154750Losses:  2.4935154914855957 0.2501753866672516 1.0243377685546875
MemoryTrain:  epoch  5, batch     5 | loss: 2.4935155Losses:  2.3499908447265625 -0.0 1.0543025732040405
MemoryTrain:  epoch  5, batch     6 | loss: 2.3499908Losses:  2.844668388366699 0.23187118768692017 0.9155422449111938
MemoryTrain:  epoch  5, batch     7 | loss: 2.8446684Losses:  2.424618721008301 0.26538151502609253 0.9095596075057983
MemoryTrain:  epoch  5, batch     8 | loss: 2.4246187Losses:  2.8217902183532715 0.49027013778686523 0.956012487411499
MemoryTrain:  epoch  5, batch     9 | loss: 2.8217902Losses:  3.2426517009735107 0.8184489011764526 0.8876599669456482
MemoryTrain:  epoch  5, batch    10 | loss: 3.2426517Losses:  2.5001025199890137 0.229966938495636 1.0094709396362305
MemoryTrain:  epoch  5, batch    11 | loss: 2.5001025Losses:  2.443558692932129 -0.0 1.0568891763687134
MemoryTrain:  epoch  5, batch    12 | loss: 2.4435587Losses:  1.3418751955032349 -0.0 0.11589892208576202
MemoryTrain:  epoch  5, batch    13 | loss: 1.3418752Losses:  2.3655996322631836 -0.0 1.0051343441009521
MemoryTrain:  epoch  6, batch     0 | loss: 2.3655996Losses:  2.645540714263916 0.49968475103378296 0.9047524929046631
MemoryTrain:  epoch  6, batch     1 | loss: 2.6455407Losses:  2.7163643836975098 0.22965550422668457 1.0630449056625366
MemoryTrain:  epoch  6, batch     2 | loss: 2.7163644Losses:  2.492081880569458 0.2536928057670593 0.9117794036865234
MemoryTrain:  epoch  6, batch     3 | loss: 2.4920819Losses:  3.2362165451049805 0.6376687288284302 0.8955672979354858
MemoryTrain:  epoch  6, batch     4 | loss: 3.2362165Losses:  2.5001730918884277 0.2347615361213684 1.0615489482879639
MemoryTrain:  epoch  6, batch     5 | loss: 2.5001731Losses:  2.8375325202941895 0.5035319924354553 0.9746026396751404
MemoryTrain:  epoch  6, batch     6 | loss: 2.8375325Losses:  2.5449302196502686 0.2725617289543152 0.9757389426231384
MemoryTrain:  epoch  6, batch     7 | loss: 2.5449302Losses:  2.937338352203369 0.3173498511314392 0.9043123126029968
MemoryTrain:  epoch  6, batch     8 | loss: 2.9373384Losses:  2.4633147716522217 -0.0 0.9492565393447876
MemoryTrain:  epoch  6, batch     9 | loss: 2.4633148Losses:  2.9124674797058105 0.27506089210510254 1.0559190511703491
MemoryTrain:  epoch  6, batch    10 | loss: 2.9124675Losses:  2.2322945594787598 -0.0 0.974427342414856
MemoryTrain:  epoch  6, batch    11 | loss: 2.2322946Losses:  2.6813125610351562 0.4664404094219208 0.9150895476341248
MemoryTrain:  epoch  6, batch    12 | loss: 2.6813126Losses:  1.2837011814117432 -0.0 0.13642150163650513
MemoryTrain:  epoch  6, batch    13 | loss: 1.2837012Losses:  2.6836369037628174 0.2668459117412567 0.9730945825576782
MemoryTrain:  epoch  7, batch     0 | loss: 2.6836369Losses:  2.2245888710021973 -0.0 0.9622929096221924
MemoryTrain:  epoch  7, batch     1 | loss: 2.2245889Losses:  2.346012830734253 -0.0 1.0313572883605957
MemoryTrain:  epoch  7, batch     2 | loss: 2.3460128Losses:  2.3761467933654785 0.25069987773895264 0.9008402228355408
MemoryTrain:  epoch  7, batch     3 | loss: 2.3761468Losses:  2.5236287117004395 0.2487526386976242 0.9122163653373718
MemoryTrain:  epoch  7, batch     4 | loss: 2.5236287Losses:  2.6556055545806885 0.4979838728904724 0.9612160325050354
MemoryTrain:  epoch  7, batch     5 | loss: 2.6556056Losses:  3.0197911262512207 0.4968990087509155 0.9193590879440308
MemoryTrain:  epoch  7, batch     6 | loss: 3.0197911Losses:  2.9999465942382812 0.5234095454216003 0.9829418659210205
MemoryTrain:  epoch  7, batch     7 | loss: 2.9999466Losses:  2.6642322540283203 0.5115343332290649 0.9134508371353149
MemoryTrain:  epoch  7, batch     8 | loss: 2.6642323Losses:  2.8628029823303223 0.5527989864349365 0.8544606566429138
MemoryTrain:  epoch  7, batch     9 | loss: 2.8628030Losses:  2.6087148189544678 0.2458072304725647 1.0003242492675781
MemoryTrain:  epoch  7, batch    10 | loss: 2.6087148Losses:  2.948150634765625 0.7546144723892212 0.917609691619873
MemoryTrain:  epoch  7, batch    11 | loss: 2.9481506Losses:  2.265751838684082 -0.0 1.0249249935150146
MemoryTrain:  epoch  7, batch    12 | loss: 2.2657518Losses:  1.2684000730514526 -0.0 0.12303151190280914
MemoryTrain:  epoch  7, batch    13 | loss: 1.2684001Losses:  2.927805185317993 0.7072948813438416 0.9376838803291321
MemoryTrain:  epoch  8, batch     0 | loss: 2.9278052Losses:  2.709818124771118 0.4686221480369568 0.844588041305542
MemoryTrain:  epoch  8, batch     1 | loss: 2.7098181Losses:  2.64040470123291 0.48983365297317505 0.9139401912689209
MemoryTrain:  epoch  8, batch     2 | loss: 2.6404047Losses:  2.7945456504821777 0.4842579960823059 0.9716275930404663
MemoryTrain:  epoch  8, batch     3 | loss: 2.7945457Losses:  2.3338661193847656 0.21560059487819672 0.9081344604492188
MemoryTrain:  epoch  8, batch     4 | loss: 2.3338661Losses:  2.332473039627075 -0.0 0.9646153450012207
MemoryTrain:  epoch  8, batch     5 | loss: 2.3324730Losses:  2.5878429412841797 0.49435004591941833 0.9072879552841187
MemoryTrain:  epoch  8, batch     6 | loss: 2.5878429Losses:  2.475159168243408 0.2574605643749237 0.9752484560012817
MemoryTrain:  epoch  8, batch     7 | loss: 2.4751592Losses:  2.8065009117126465 0.7601091861724854 0.8555526733398438
MemoryTrain:  epoch  8, batch     8 | loss: 2.8065009Losses:  2.4961748123168945 -0.0 1.0260553359985352
MemoryTrain:  epoch  8, batch     9 | loss: 2.4961748Losses:  2.70499324798584 0.5108165740966797 0.9686456918716431
MemoryTrain:  epoch  8, batch    10 | loss: 2.7049932Losses:  2.304440975189209 -0.0 1.0287177562713623
MemoryTrain:  epoch  8, batch    11 | loss: 2.3044410Losses:  2.4704642295837402 0.23015201091766357 1.006162405014038
MemoryTrain:  epoch  8, batch    12 | loss: 2.4704642Losses:  1.3709214925765991 -0.0 0.1081763505935669
MemoryTrain:  epoch  8, batch    13 | loss: 1.3709215Losses:  2.7433202266693115 0.4991154670715332 1.0150424242019653
MemoryTrain:  epoch  9, batch     0 | loss: 2.7433202Losses:  2.3407034873962402 -0.0 1.0007150173187256
MemoryTrain:  epoch  9, batch     1 | loss: 2.3407035Losses:  2.3183507919311523 -0.0 1.0053017139434814
MemoryTrain:  epoch  9, batch     2 | loss: 2.3183508Losses:  2.4790337085723877 0.24225036799907684 0.9541934728622437
MemoryTrain:  epoch  9, batch     3 | loss: 2.4790337Losses:  2.284994602203369 0.22886277735233307 0.8466163277626038
MemoryTrain:  epoch  9, batch     4 | loss: 2.2849946Losses:  2.651230812072754 0.49822548031806946 0.9298315048217773
MemoryTrain:  epoch  9, batch     5 | loss: 2.6512308Losses:  2.3109331130981445 -0.0 0.9957795143127441
MemoryTrain:  epoch  9, batch     6 | loss: 2.3109331Losses:  2.5512514114379883 0.23227106034755707 1.0109699964523315
MemoryTrain:  epoch  9, batch     7 | loss: 2.5512514Losses:  2.990830898284912 0.8030739426612854 0.8892043828964233
MemoryTrain:  epoch  9, batch     8 | loss: 2.9908309Losses:  2.9992964267730713 0.8677234053611755 0.9015634059906006
MemoryTrain:  epoch  9, batch     9 | loss: 2.9992964Losses:  2.7494752407073975 0.5156964063644409 0.9677900671958923
MemoryTrain:  epoch  9, batch    10 | loss: 2.7494752Losses:  2.322226047515869 -0.0 1.0103952884674072
MemoryTrain:  epoch  9, batch    11 | loss: 2.3222260Losses:  2.843451499938965 0.7442267537117004 0.8602174520492554
MemoryTrain:  epoch  9, batch    12 | loss: 2.8434515Losses:  1.3605676889419556 -0.0 0.18175294995307922
MemoryTrain:  epoch  9, batch    13 | loss: 1.3605677
[EVAL] batch:    0 | acc: 18.75%,  total acc: 18.75%   [EVAL] batch:    1 | acc: 56.25%,  total acc: 37.50%   [EVAL] batch:    2 | acc: 37.50%,  total acc: 37.50%   [EVAL] batch:    3 | acc: 37.50%,  total acc: 37.50%   [EVAL] batch:    4 | acc: 50.00%,  total acc: 40.00%   [EVAL] batch:    5 | acc: 43.75%,  total acc: 40.62%   [EVAL] batch:    6 | acc: 62.50%,  total acc: 43.75%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 47.66%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 51.39%   [EVAL] batch:    9 | acc: 56.25%,  total acc: 51.88%   [EVAL] batch:   10 | acc: 43.75%,  total acc: 51.14%   [EVAL] batch:   11 | acc: 68.75%,  total acc: 52.60%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 53.85%   [EVAL] batch:   13 | acc: 62.50%,  total acc: 54.46%   [EVAL] batch:   14 | acc: 62.50%,  total acc: 55.00%   [EVAL] batch:   15 | acc: 50.00%,  total acc: 54.69%   [EVAL] batch:   16 | acc: 62.50%,  total acc: 55.15%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 55.90%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 57.89%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 59.69%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 61.61%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 63.35%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 64.95%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 66.15%   [EVAL] batch:   24 | acc: 93.75%,  total acc: 67.25%   [EVAL] batch:   25 | acc: 81.25%,  total acc: 67.79%   [EVAL] batch:   26 | acc: 62.50%,  total acc: 67.59%   [EVAL] batch:   27 | acc: 75.00%,  total acc: 67.86%   [EVAL] batch:   28 | acc: 62.50%,  total acc: 67.67%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 68.33%   [EVAL] batch:   30 | acc: 81.25%,  total acc: 68.75%   [EVAL] batch:   31 | acc: 75.00%,  total acc: 68.95%   [EVAL] batch:   32 | acc: 62.50%,  total acc: 68.75%   [EVAL] batch:   33 | acc: 12.50%,  total acc: 67.10%   [EVAL] batch:   34 | acc: 43.75%,  total acc: 66.43%   [EVAL] batch:   35 | acc: 18.75%,  total acc: 65.10%   [EVAL] batch:   36 | acc: 25.00%,  total acc: 64.02%   [EVAL] batch:   37 | acc: 50.00%,  total acc: 63.65%   [EVAL] batch:   38 | acc: 56.25%,  total acc: 63.46%   [EVAL] batch:   39 | acc: 12.50%,  total acc: 62.19%   [EVAL] batch:   40 | acc: 50.00%,  total acc: 61.89%   [EVAL] batch:   41 | acc: 56.25%,  total acc: 61.76%   [EVAL] batch:   42 | acc: 43.75%,  total acc: 61.34%   [EVAL] batch:   43 | acc: 62.50%,  total acc: 61.36%   [EVAL] batch:   44 | acc: 62.50%,  total acc: 61.39%   [EVAL] batch:   45 | acc: 68.75%,  total acc: 61.55%   [EVAL] batch:   46 | acc: 50.00%,  total acc: 61.30%   [EVAL] batch:   47 | acc: 56.25%,  total acc: 61.20%   [EVAL] batch:   48 | acc: 56.25%,  total acc: 61.10%   [EVAL] batch:   49 | acc: 81.25%,  total acc: 61.50%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 62.25%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 62.98%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 63.68%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 64.35%   [EVAL] batch:   54 | acc: 100.00%,  total acc: 65.00%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 65.62%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 66.23%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 66.59%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 67.16%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 67.71%   [EVAL] batch:   60 | acc: 93.75%,  total acc: 68.14%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 68.65%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 68.35%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 82.81%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 82.29%   [EVAL] batch:    6 | acc: 43.75%,  total acc: 76.79%   [EVAL] batch:    7 | acc: 37.50%,  total acc: 71.88%   [EVAL] batch:    8 | acc: 50.00%,  total acc: 69.44%   [EVAL] batch:    9 | acc: 50.00%,  total acc: 67.50%   [EVAL] batch:   10 | acc: 37.50%,  total acc: 64.77%   [EVAL] batch:   11 | acc: 37.50%,  total acc: 62.50%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 63.46%   [EVAL] batch:   13 | acc: 87.50%,  total acc: 65.18%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 67.08%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 69.14%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 70.59%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 71.88%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 73.36%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 73.75%   [EVAL] batch:   20 | acc: 81.25%,  total acc: 74.11%   [EVAL] batch:   21 | acc: 75.00%,  total acc: 74.15%   [EVAL] batch:   22 | acc: 56.25%,  total acc: 73.37%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 73.96%   [EVAL] batch:   24 | acc: 62.50%,  total acc: 73.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 74.52%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 75.23%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 75.89%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 76.51%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 77.29%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 78.02%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 78.71%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 79.36%   [EVAL] batch:   33 | acc: 75.00%,  total acc: 79.23%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 79.11%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 79.51%   [EVAL] batch:   36 | acc: 87.50%,  total acc: 79.73%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 80.26%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 80.77%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 81.55%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 81.70%   [EVAL] batch:   42 | acc: 87.50%,  total acc: 81.83%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 81.96%   [EVAL] batch:   44 | acc: 68.75%,  total acc: 81.67%   [EVAL] batch:   45 | acc: 75.00%,  total acc: 81.52%   [EVAL] batch:   46 | acc: 43.75%,  total acc: 80.72%   [EVAL] batch:   47 | acc: 50.00%,  total acc: 80.08%   [EVAL] batch:   48 | acc: 68.75%,  total acc: 79.85%   [EVAL] batch:   49 | acc: 68.75%,  total acc: 79.62%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 79.90%   [EVAL] batch:   51 | acc: 81.25%,  total acc: 79.93%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 80.19%   [EVAL] batch:   53 | acc: 68.75%,  total acc: 79.98%   [EVAL] batch:   54 | acc: 62.50%,  total acc: 79.66%   [EVAL] batch:   55 | acc: 75.00%,  total acc: 79.58%   [EVAL] batch:   56 | acc: 75.00%,  total acc: 79.50%   [EVAL] batch:   57 | acc: 50.00%,  total acc: 78.99%   [EVAL] batch:   58 | acc: 75.00%,  total acc: 78.92%   [EVAL] batch:   59 | acc: 87.50%,  total acc: 79.06%   [EVAL] batch:   60 | acc: 81.25%,  total acc: 79.10%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 78.93%   [EVAL] batch:   62 | acc: 62.50%,  total acc: 78.67%   [EVAL] batch:   63 | acc: 56.25%,  total acc: 78.32%   [EVAL] batch:   64 | acc: 31.25%,  total acc: 77.60%   [EVAL] batch:   65 | acc: 68.75%,  total acc: 77.46%   [EVAL] batch:   66 | acc: 37.50%,  total acc: 76.87%   [EVAL] batch:   67 | acc: 43.75%,  total acc: 76.38%   [EVAL] batch:   68 | acc: 75.00%,  total acc: 76.36%   [EVAL] batch:   69 | acc: 87.50%,  total acc: 76.52%   [EVAL] batch:   70 | acc: 68.75%,  total acc: 76.41%   [EVAL] batch:   71 | acc: 68.75%,  total acc: 76.30%   [EVAL] batch:   72 | acc: 93.75%,  total acc: 76.54%   [EVAL] batch:   73 | acc: 100.00%,  total acc: 76.86%   [EVAL] batch:   74 | acc: 81.25%,  total acc: 76.92%   [EVAL] batch:   75 | acc: 25.00%,  total acc: 76.23%   [EVAL] batch:   76 | acc: 18.75%,  total acc: 75.49%   [EVAL] batch:   77 | acc: 25.00%,  total acc: 74.84%   [EVAL] batch:   78 | acc: 25.00%,  total acc: 74.21%   [EVAL] batch:   79 | acc: 37.50%,  total acc: 73.75%   [EVAL] batch:   80 | acc: 18.75%,  total acc: 73.07%   [EVAL] batch:   81 | acc: 68.75%,  total acc: 73.02%   [EVAL] batch:   82 | acc: 100.00%,  total acc: 73.34%   [EVAL] batch:   83 | acc: 100.00%,  total acc: 73.66%   [EVAL] batch:   84 | acc: 100.00%,  total acc: 73.97%   [EVAL] batch:   85 | acc: 100.00%,  total acc: 74.27%   [EVAL] batch:   86 | acc: 93.75%,  total acc: 74.50%   [EVAL] batch:   87 | acc: 75.00%,  total acc: 74.50%   [EVAL] batch:   88 | acc: 43.75%,  total acc: 74.16%   [EVAL] batch:   89 | acc: 37.50%,  total acc: 73.75%   [EVAL] batch:   90 | acc: 31.25%,  total acc: 73.28%   [EVAL] batch:   91 | acc: 43.75%,  total acc: 72.96%   [EVAL] batch:   92 | acc: 6.25%,  total acc: 72.24%   [EVAL] batch:   93 | acc: 56.25%,  total acc: 72.07%   [EVAL] batch:   94 | acc: 87.50%,  total acc: 72.24%   [EVAL] batch:   95 | acc: 81.25%,  total acc: 72.33%   [EVAL] batch:   96 | acc: 93.75%,  total acc: 72.55%   [EVAL] batch:   97 | acc: 81.25%,  total acc: 72.64%   [EVAL] batch:   98 | acc: 75.00%,  total acc: 72.66%   [EVAL] batch:   99 | acc: 93.75%,  total acc: 72.88%   [EVAL] batch:  100 | acc: 56.25%,  total acc: 72.71%   [EVAL] batch:  101 | acc: 68.75%,  total acc: 72.67%   [EVAL] batch:  102 | acc: 43.75%,  total acc: 72.39%   [EVAL] batch:  103 | acc: 62.50%,  total acc: 72.30%   [EVAL] batch:  104 | acc: 43.75%,  total acc: 72.02%   [EVAL] batch:  105 | acc: 62.50%,  total acc: 71.93%   [EVAL] batch:  106 | acc: 25.00%,  total acc: 71.50%   [EVAL] batch:  107 | acc: 12.50%,  total acc: 70.95%   [EVAL] batch:  108 | acc: 12.50%,  total acc: 70.41%   [EVAL] batch:  109 | acc: 6.25%,  total acc: 69.83%   [EVAL] batch:  110 | acc: 12.50%,  total acc: 69.31%   [EVAL] batch:  111 | acc: 18.75%,  total acc: 68.86%   [EVAL] batch:  112 | acc: 25.00%,  total acc: 68.47%   [EVAL] batch:  113 | acc: 37.50%,  total acc: 68.20%   [EVAL] batch:  114 | acc: 75.00%,  total acc: 68.26%   [EVAL] batch:  115 | acc: 56.25%,  total acc: 68.16%   [EVAL] batch:  116 | acc: 56.25%,  total acc: 68.06%   [EVAL] batch:  117 | acc: 50.00%,  total acc: 67.90%   [EVAL] batch:  118 | acc: 75.00%,  total acc: 67.96%   [EVAL] batch:  119 | acc: 87.50%,  total acc: 68.12%   [EVAL] batch:  120 | acc: 81.25%,  total acc: 68.23%   [EVAL] batch:  121 | acc: 68.75%,  total acc: 68.24%   [EVAL] batch:  122 | acc: 68.75%,  total acc: 68.24%   [EVAL] batch:  123 | acc: 75.00%,  total acc: 68.30%   [EVAL] batch:  124 | acc: 75.00%,  total acc: 68.35%   [EVAL] batch:  125 | acc: 12.50%,  total acc: 67.91%   [EVAL] batch:  126 | acc: 6.25%,  total acc: 67.42%   [EVAL] batch:  127 | acc: 0.00%,  total acc: 66.89%   [EVAL] batch:  128 | acc: 6.25%,  total acc: 66.42%   [EVAL] batch:  129 | acc: 6.25%,  total acc: 65.96%   [EVAL] batch:  130 | acc: 12.50%,  total acc: 65.55%   [EVAL] batch:  131 | acc: 31.25%,  total acc: 65.29%   [EVAL] batch:  132 | acc: 56.25%,  total acc: 65.23%   [EVAL] batch:  133 | acc: 68.75%,  total acc: 65.25%   [EVAL] batch:  134 | acc: 81.25%,  total acc: 65.37%   [EVAL] batch:  135 | acc: 81.25%,  total acc: 65.49%   [EVAL] batch:  136 | acc: 62.50%,  total acc: 65.47%   [EVAL] batch:  137 | acc: 56.25%,  total acc: 65.40%   [EVAL] batch:  138 | acc: 18.75%,  total acc: 65.06%   [EVAL] batch:  139 | acc: 25.00%,  total acc: 64.78%   [EVAL] batch:  140 | acc: 25.00%,  total acc: 64.49%   [EVAL] batch:  141 | acc: 25.00%,  total acc: 64.22%   [EVAL] batch:  142 | acc: 43.75%,  total acc: 64.07%   [EVAL] batch:  143 | acc: 50.00%,  total acc: 63.98%   [EVAL] batch:  144 | acc: 100.00%,  total acc: 64.22%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 64.47%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 64.67%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 64.91%   [EVAL] batch:  148 | acc: 93.75%,  total acc: 65.10%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 65.29%   [EVAL] batch:  150 | acc: 100.00%,  total acc: 65.52%   [EVAL] batch:  151 | acc: 100.00%,  total acc: 65.75%   [EVAL] batch:  152 | acc: 100.00%,  total acc: 65.97%   [EVAL] batch:  153 | acc: 93.75%,  total acc: 66.15%   [EVAL] batch:  154 | acc: 100.00%,  total acc: 66.37%   [EVAL] batch:  155 | acc: 93.75%,  total acc: 66.55%   [EVAL] batch:  156 | acc: 100.00%,  total acc: 66.76%   [EVAL] batch:  157 | acc: 100.00%,  total acc: 66.97%   [EVAL] batch:  158 | acc: 93.75%,  total acc: 67.14%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 67.30%   [EVAL] batch:  160 | acc: 93.75%,  total acc: 67.47%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 67.63%   [EVAL] batch:  162 | acc: 62.50%,  total acc: 67.60%   [EVAL] batch:  163 | acc: 43.75%,  total acc: 67.45%   [EVAL] batch:  164 | acc: 37.50%,  total acc: 67.27%   [EVAL] batch:  165 | acc: 25.00%,  total acc: 67.02%   [EVAL] batch:  166 | acc: 18.75%,  total acc: 66.73%   [EVAL] batch:  167 | acc: 18.75%,  total acc: 66.44%   [EVAL] batch:  168 | acc: 62.50%,  total acc: 66.42%   [EVAL] batch:  169 | acc: 68.75%,  total acc: 66.43%   [EVAL] batch:  170 | acc: 81.25%,  total acc: 66.52%   [EVAL] batch:  171 | acc: 75.00%,  total acc: 66.57%   [EVAL] batch:  172 | acc: 68.75%,  total acc: 66.58%   [EVAL] batch:  173 | acc: 50.00%,  total acc: 66.49%   [EVAL] batch:  174 | acc: 68.75%,  total acc: 66.50%   [EVAL] batch:  175 | acc: 56.25%,  total acc: 66.44%   [EVAL] batch:  176 | acc: 43.75%,  total acc: 66.31%   [EVAL] batch:  177 | acc: 37.50%,  total acc: 66.15%   [EVAL] batch:  178 | acc: 50.00%,  total acc: 66.06%   [EVAL] batch:  179 | acc: 62.50%,  total acc: 66.04%   [EVAL] batch:  180 | acc: 25.00%,  total acc: 65.81%   [EVAL] batch:  181 | acc: 62.50%,  total acc: 65.80%   [EVAL] batch:  182 | acc: 75.00%,  total acc: 65.85%   [EVAL] batch:  183 | acc: 62.50%,  total acc: 65.83%   [EVAL] batch:  184 | acc: 75.00%,  total acc: 65.88%   [EVAL] batch:  185 | acc: 87.50%,  total acc: 65.99%   [EVAL] batch:  186 | acc: 81.25%,  total acc: 66.08%   [EVAL] batch:  187 | acc: 81.25%,  total acc: 66.16%   [EVAL] batch:  188 | acc: 62.50%,  total acc: 66.14%   [EVAL] batch:  189 | acc: 75.00%,  total acc: 66.18%   [EVAL] batch:  190 | acc: 81.25%,  total acc: 66.26%   [EVAL] batch:  191 | acc: 81.25%,  total acc: 66.34%   [EVAL] batch:  192 | acc: 87.50%,  total acc: 66.45%   [EVAL] batch:  193 | acc: 75.00%,  total acc: 66.49%   [EVAL] batch:  194 | acc: 87.50%,  total acc: 66.60%   [EVAL] batch:  195 | acc: 75.00%,  total acc: 66.65%   [EVAL] batch:  196 | acc: 68.75%,  total acc: 66.66%   [EVAL] batch:  197 | acc: 93.75%,  total acc: 66.79%   [EVAL] batch:  198 | acc: 81.25%,  total acc: 66.87%   [EVAL] batch:  199 | acc: 81.25%,  total acc: 66.94%   [EVAL] batch:  200 | acc: 93.75%,  total acc: 67.07%   [EVAL] batch:  201 | acc: 81.25%,  total acc: 67.14%   [EVAL] batch:  202 | acc: 68.75%,  total acc: 67.15%   [EVAL] batch:  203 | acc: 93.75%,  total acc: 67.28%   [EVAL] batch:  204 | acc: 81.25%,  total acc: 67.35%   [EVAL] batch:  205 | acc: 93.75%,  total acc: 67.48%   [EVAL] batch:  206 | acc: 31.25%,  total acc: 67.30%   [EVAL] batch:  207 | acc: 18.75%,  total acc: 67.07%   [EVAL] batch:  208 | acc: 12.50%,  total acc: 66.81%   [EVAL] batch:  209 | acc: 12.50%,  total acc: 66.55%   [EVAL] batch:  210 | acc: 18.75%,  total acc: 66.32%   [EVAL] batch:  211 | acc: 37.50%,  total acc: 66.19%   [EVAL] batch:  212 | acc: 43.75%,  total acc: 66.08%   [EVAL] batch:  213 | acc: 93.75%,  total acc: 66.21%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 66.37%   [EVAL] batch:  215 | acc: 87.50%,  total acc: 66.46%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 66.62%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 66.77%   [EVAL] batch:  218 | acc: 93.75%,  total acc: 66.89%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 67.05%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 67.19%   [EVAL] batch:  221 | acc: 100.00%,  total acc: 67.34%   [EVAL] batch:  222 | acc: 100.00%,  total acc: 67.49%   [EVAL] batch:  223 | acc: 100.00%,  total acc: 67.63%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 67.78%   [EVAL] batch:  225 | acc: 75.00%,  total acc: 67.81%   [EVAL] batch:  226 | acc: 68.75%,  total acc: 67.81%   [EVAL] batch:  227 | acc: 75.00%,  total acc: 67.85%   [EVAL] batch:  228 | acc: 75.00%,  total acc: 67.88%   [EVAL] batch:  229 | acc: 93.75%,  total acc: 67.99%   [EVAL] batch:  230 | acc: 68.75%,  total acc: 67.99%   [EVAL] batch:  231 | acc: 62.50%,  total acc: 67.97%   [EVAL] batch:  232 | acc: 31.25%,  total acc: 67.81%   [EVAL] batch:  233 | acc: 31.25%,  total acc: 67.65%   [EVAL] batch:  234 | acc: 18.75%,  total acc: 67.45%   [EVAL] batch:  235 | acc: 50.00%,  total acc: 67.37%   [EVAL] batch:  236 | acc: 18.75%,  total acc: 67.17%   [EVAL] batch:  237 | acc: 62.50%,  total acc: 67.15%   [EVAL] batch:  238 | acc: 87.50%,  total acc: 67.23%   [EVAL] batch:  239 | acc: 100.00%,  total acc: 67.37%   [EVAL] batch:  240 | acc: 87.50%,  total acc: 67.45%   [EVAL] batch:  241 | acc: 87.50%,  total acc: 67.54%   [EVAL] batch:  242 | acc: 93.75%,  total acc: 67.64%   [EVAL] batch:  243 | acc: 81.25%,  total acc: 67.70%   [EVAL] batch:  244 | acc: 37.50%,  total acc: 67.58%   [EVAL] batch:  245 | acc: 31.25%,  total acc: 67.43%   [EVAL] batch:  246 | acc: 6.25%,  total acc: 67.18%   [EVAL] batch:  247 | acc: 50.00%,  total acc: 67.11%   [EVAL] batch:  248 | acc: 31.25%,  total acc: 66.97%   [EVAL] batch:  249 | acc: 56.25%,  total acc: 66.92%   [EVAL] batch:  250 | acc: 93.75%,  total acc: 67.03%   [EVAL] batch:  251 | acc: 93.75%,  total acc: 67.14%   [EVAL] batch:  252 | acc: 87.50%,  total acc: 67.22%   [EVAL] batch:  253 | acc: 81.25%,  total acc: 67.27%   [EVAL] batch:  254 | acc: 93.75%,  total acc: 67.38%   [EVAL] batch:  255 | acc: 100.00%,  total acc: 67.50%   [EVAL] batch:  256 | acc: 87.50%,  total acc: 67.58%   [EVAL] batch:  257 | acc: 93.75%,  total acc: 67.68%   [EVAL] batch:  258 | acc: 81.25%,  total acc: 67.74%   [EVAL] batch:  259 | acc: 93.75%,  total acc: 67.84%   [EVAL] batch:  260 | acc: 81.25%,  total acc: 67.89%   [EVAL] batch:  261 | acc: 81.25%,  total acc: 67.94%   [EVAL] batch:  262 | acc: 93.75%,  total acc: 68.04%   [EVAL] batch:  263 | acc: 87.50%,  total acc: 68.11%   [EVAL] batch:  264 | acc: 81.25%,  total acc: 68.16%   [EVAL] batch:  265 | acc: 87.50%,  total acc: 68.23%   [EVAL] batch:  266 | acc: 68.75%,  total acc: 68.24%   [EVAL] batch:  267 | acc: 75.00%,  total acc: 68.26%   [EVAL] batch:  268 | acc: 62.50%,  total acc: 68.24%   [EVAL] batch:  269 | acc: 6.25%,  total acc: 68.01%   [EVAL] batch:  270 | acc: 12.50%,  total acc: 67.80%   [EVAL] batch:  271 | acc: 6.25%,  total acc: 67.58%   [EVAL] batch:  272 | acc: 0.00%,  total acc: 67.33%   [EVAL] batch:  273 | acc: 18.75%,  total acc: 67.15%   [EVAL] batch:  274 | acc: 12.50%,  total acc: 66.95%   [EVAL] batch:  275 | acc: 100.00%,  total acc: 67.07%   [EVAL] batch:  276 | acc: 100.00%,  total acc: 67.19%   [EVAL] batch:  277 | acc: 100.00%,  total acc: 67.31%   [EVAL] batch:  278 | acc: 100.00%,  total acc: 67.43%   [EVAL] batch:  279 | acc: 100.00%,  total acc: 67.54%   [EVAL] batch:  280 | acc: 100.00%,  total acc: 67.66%   [EVAL] batch:  281 | acc: 75.00%,  total acc: 67.69%   [EVAL] batch:  282 | acc: 75.00%,  total acc: 67.71%   [EVAL] batch:  283 | acc: 62.50%,  total acc: 67.69%   [EVAL] batch:  284 | acc: 75.00%,  total acc: 67.72%   [EVAL] batch:  285 | acc: 56.25%,  total acc: 67.68%   [EVAL] batch:  286 | acc: 56.25%,  total acc: 67.64%   [EVAL] batch:  287 | acc: 68.75%,  total acc: 67.64%   [EVAL] batch:  288 | acc: 100.00%,  total acc: 67.76%   [EVAL] batch:  289 | acc: 100.00%,  total acc: 67.87%   [EVAL] batch:  290 | acc: 100.00%,  total acc: 67.98%   [EVAL] batch:  291 | acc: 100.00%,  total acc: 68.09%   [EVAL] batch:  292 | acc: 100.00%,  total acc: 68.20%   [EVAL] batch:  293 | acc: 93.75%,  total acc: 68.28%   [EVAL] batch:  294 | acc: 100.00%,  total acc: 68.39%   [EVAL] batch:  295 | acc: 100.00%,  total acc: 68.50%   [EVAL] batch:  296 | acc: 100.00%,  total acc: 68.60%   [EVAL] batch:  297 | acc: 100.00%,  total acc: 68.71%   [EVAL] batch:  298 | acc: 100.00%,  total acc: 68.81%   [EVAL] batch:  299 | acc: 93.75%,  total acc: 68.90%   [EVAL] batch:  300 | acc: 93.75%,  total acc: 68.98%   [EVAL] batch:  301 | acc: 93.75%,  total acc: 69.06%   [EVAL] batch:  302 | acc: 93.75%,  total acc: 69.14%   [EVAL] batch:  303 | acc: 93.75%,  total acc: 69.22%   [EVAL] batch:  304 | acc: 87.50%,  total acc: 69.28%   [EVAL] batch:  305 | acc: 100.00%,  total acc: 69.38%   [EVAL] batch:  306 | acc: 93.75%,  total acc: 69.46%   [EVAL] batch:  307 | acc: 87.50%,  total acc: 69.52%   [EVAL] batch:  308 | acc: 93.75%,  total acc: 69.60%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 69.70%   [EVAL] batch:  310 | acc: 100.00%,  total acc: 69.80%   [EVAL] batch:  311 | acc: 93.75%,  total acc: 69.87%   [EVAL] batch:  312 | acc: 93.75%,  total acc: 69.95%   [EVAL] batch:  313 | acc: 68.75%,  total acc: 69.94%   [EVAL] batch:  314 | acc: 75.00%,  total acc: 69.96%   [EVAL] batch:  315 | acc: 56.25%,  total acc: 69.92%   [EVAL] batch:  316 | acc: 62.50%,  total acc: 69.89%   [EVAL] batch:  317 | acc: 93.75%,  total acc: 69.97%   [EVAL] batch:  318 | acc: 81.25%,  total acc: 70.00%   [EVAL] batch:  319 | acc: 100.00%,  total acc: 70.10%   [EVAL] batch:  320 | acc: 100.00%,  total acc: 70.19%   [EVAL] batch:  321 | acc: 87.50%,  total acc: 70.24%   [EVAL] batch:  322 | acc: 100.00%,  total acc: 70.34%   [EVAL] batch:  323 | acc: 100.00%,  total acc: 70.43%   [EVAL] batch:  324 | acc: 81.25%,  total acc: 70.46%   [EVAL] batch:  325 | acc: 68.75%,  total acc: 70.46%   [EVAL] batch:  326 | acc: 56.25%,  total acc: 70.41%   [EVAL] batch:  327 | acc: 68.75%,  total acc: 70.41%   [EVAL] batch:  328 | acc: 75.00%,  total acc: 70.42%   [EVAL] batch:  329 | acc: 62.50%,  total acc: 70.40%   [EVAL] batch:  330 | acc: 62.50%,  total acc: 70.37%   [EVAL] batch:  331 | acc: 81.25%,  total acc: 70.41%   [EVAL] batch:  332 | acc: 56.25%,  total acc: 70.36%   [EVAL] batch:  333 | acc: 87.50%,  total acc: 70.42%   [EVAL] batch:  334 | acc: 62.50%,  total acc: 70.39%   [EVAL] batch:  335 | acc: 68.75%,  total acc: 70.39%   [EVAL] batch:  336 | acc: 56.25%,  total acc: 70.34%   [EVAL] batch:  337 | acc: 87.50%,  total acc: 70.40%   [EVAL] batch:  338 | acc: 75.00%,  total acc: 70.41%   [EVAL] batch:  339 | acc: 62.50%,  total acc: 70.39%   [EVAL] batch:  340 | acc: 81.25%,  total acc: 70.42%   [EVAL] batch:  341 | acc: 68.75%,  total acc: 70.41%   [EVAL] batch:  342 | acc: 50.00%,  total acc: 70.35%   [EVAL] batch:  343 | acc: 50.00%,  total acc: 70.29%   [EVAL] batch:  344 | acc: 56.25%,  total acc: 70.25%   [EVAL] batch:  345 | acc: 37.50%,  total acc: 70.16%   [EVAL] batch:  346 | acc: 56.25%,  total acc: 70.12%   [EVAL] batch:  347 | acc: 62.50%,  total acc: 70.10%   [EVAL] batch:  348 | acc: 68.75%,  total acc: 70.09%   [EVAL] batch:  349 | acc: 43.75%,  total acc: 70.02%   [EVAL] batch:  350 | acc: 68.75%,  total acc: 70.01%   [EVAL] batch:  351 | acc: 81.25%,  total acc: 70.05%   [EVAL] batch:  352 | acc: 75.00%,  total acc: 70.06%   [EVAL] batch:  353 | acc: 81.25%,  total acc: 70.09%   [EVAL] batch:  354 | acc: 75.00%,  total acc: 70.11%   [EVAL] batch:  355 | acc: 87.50%,  total acc: 70.15%   [EVAL] batch:  356 | acc: 43.75%,  total acc: 70.08%   [EVAL] batch:  357 | acc: 18.75%,  total acc: 69.94%   [EVAL] batch:  358 | acc: 31.25%,  total acc: 69.83%   [EVAL] batch:  359 | acc: 43.75%,  total acc: 69.76%   [EVAL] batch:  360 | acc: 31.25%,  total acc: 69.65%   [EVAL] batch:  361 | acc: 50.00%,  total acc: 69.60%   [EVAL] batch:  362 | acc: 62.50%,  total acc: 69.58%   [EVAL] batch:  363 | acc: 100.00%,  total acc: 69.66%   [EVAL] batch:  364 | acc: 100.00%,  total acc: 69.74%   [EVAL] batch:  365 | acc: 100.00%,  total acc: 69.83%   [EVAL] batch:  366 | acc: 100.00%,  total acc: 69.91%   [EVAL] batch:  367 | acc: 100.00%,  total acc: 69.99%   [EVAL] batch:  368 | acc: 100.00%,  total acc: 70.07%   [EVAL] batch:  369 | acc: 100.00%,  total acc: 70.15%   [EVAL] batch:  370 | acc: 93.75%,  total acc: 70.22%   [EVAL] batch:  371 | acc: 93.75%,  total acc: 70.28%   [EVAL] batch:  372 | acc: 100.00%,  total acc: 70.36%   [EVAL] batch:  373 | acc: 100.00%,  total acc: 70.44%   [EVAL] batch:  374 | acc: 75.00%,  total acc: 70.45%   [EVAL] batch:  375 | acc: 18.75%,  total acc: 70.31%   [EVAL] batch:  376 | acc: 56.25%,  total acc: 70.28%   [EVAL] batch:  377 | acc: 37.50%,  total acc: 70.19%   [EVAL] batch:  378 | acc: 37.50%,  total acc: 70.10%   [EVAL] batch:  379 | acc: 50.00%,  total acc: 70.05%   [EVAL] batch:  380 | acc: 43.75%,  total acc: 69.98%   [EVAL] batch:  381 | acc: 62.50%,  total acc: 69.96%   [EVAL] batch:  382 | acc: 75.00%,  total acc: 69.97%   [EVAL] batch:  383 | acc: 81.25%,  total acc: 70.00%   [EVAL] batch:  384 | acc: 56.25%,  total acc: 69.97%   [EVAL] batch:  385 | acc: 43.75%,  total acc: 69.90%   [EVAL] batch:  386 | acc: 68.75%,  total acc: 69.90%   [EVAL] batch:  387 | acc: 68.75%,  total acc: 69.89%   [EVAL] batch:  388 | acc: 62.50%,  total acc: 69.87%   [EVAL] batch:  389 | acc: 62.50%,  total acc: 69.86%   [EVAL] batch:  390 | acc: 50.00%,  total acc: 69.80%   [EVAL] batch:  391 | acc: 62.50%,  total acc: 69.79%   [EVAL] batch:  392 | acc: 68.75%,  total acc: 69.78%   [EVAL] batch:  393 | acc: 93.75%,  total acc: 69.84%   [EVAL] batch:  394 | acc: 93.75%,  total acc: 69.91%   [EVAL] batch:  395 | acc: 100.00%,  total acc: 69.98%   [EVAL] batch:  396 | acc: 100.00%,  total acc: 70.06%   [EVAL] batch:  397 | acc: 100.00%,  total acc: 70.13%   [EVAL] batch:  398 | acc: 93.75%,  total acc: 70.19%   [EVAL] batch:  399 | acc: 93.75%,  total acc: 70.25%   [EVAL] batch:  400 | acc: 81.25%,  total acc: 70.28%   [EVAL] batch:  401 | acc: 62.50%,  total acc: 70.26%   [EVAL] batch:  402 | acc: 75.00%,  total acc: 70.27%   [EVAL] batch:  403 | acc: 62.50%,  total acc: 70.25%   [EVAL] batch:  404 | acc: 87.50%,  total acc: 70.29%   [EVAL] batch:  405 | acc: 81.25%,  total acc: 70.32%   [EVAL] batch:  406 | acc: 75.00%,  total acc: 70.33%   [EVAL] batch:  407 | acc: 62.50%,  total acc: 70.31%   [EVAL] batch:  408 | acc: 12.50%,  total acc: 70.17%   [EVAL] batch:  409 | acc: 43.75%,  total acc: 70.11%   [EVAL] batch:  410 | acc: 18.75%,  total acc: 69.98%   [EVAL] batch:  411 | acc: 25.00%,  total acc: 69.87%   [EVAL] batch:  412 | acc: 50.00%,  total acc: 69.82%   [EVAL] batch:  413 | acc: 56.25%,  total acc: 69.79%   [EVAL] batch:  414 | acc: 12.50%,  total acc: 69.65%   [EVAL] batch:  415 | acc: 50.00%,  total acc: 69.61%   [EVAL] batch:  416 | acc: 56.25%,  total acc: 69.57%   [EVAL] batch:  417 | acc: 43.75%,  total acc: 69.51%   [EVAL] batch:  418 | acc: 62.50%,  total acc: 69.50%   [EVAL] batch:  419 | acc: 62.50%,  total acc: 69.48%   [EVAL] batch:  420 | acc: 68.75%,  total acc: 69.48%   [EVAL] batch:  421 | acc: 50.00%,  total acc: 69.43%   [EVAL] batch:  422 | acc: 56.25%,  total acc: 69.40%   [EVAL] batch:  423 | acc: 56.25%,  total acc: 69.37%   [EVAL] batch:  424 | acc: 81.25%,  total acc: 69.40%   [EVAL] batch:  425 | acc: 100.00%,  total acc: 69.47%   [EVAL] batch:  426 | acc: 100.00%,  total acc: 69.54%   [EVAL] batch:  427 | acc: 100.00%,  total acc: 69.61%   [EVAL] batch:  428 | acc: 100.00%,  total acc: 69.68%   [EVAL] batch:  429 | acc: 100.00%,  total acc: 69.75%   [EVAL] batch:  430 | acc: 100.00%,  total acc: 69.82%   [EVAL] batch:  431 | acc: 100.00%,  total acc: 69.89%   [EVAL] batch:  432 | acc: 87.50%,  total acc: 69.93%   [EVAL] batch:  433 | acc: 100.00%,  total acc: 70.00%   [EVAL] batch:  434 | acc: 100.00%,  total acc: 70.07%   [EVAL] batch:  435 | acc: 93.75%,  total acc: 70.13%   [EVAL] batch:  436 | acc: 100.00%,  total acc: 70.19%   [EVAL] batch:  437 | acc: 50.00%,  total acc: 70.15%   
cur_acc:  ['0.9484', '0.7232', '0.7421', '0.6736', '0.8284', '0.7450', '0.6835']
his_acc:  ['0.9484', '0.8190', '0.7713', '0.7235', '0.7330', '0.7190', '0.7015']
Clustering into  39  clusters
Clusters:  [31  1  9 25 20 28  9 18  1  8 22 16  3  4  3  6 10 22 30 38 17 13 26  0
  0 13 26  7  8 35  5 10 18 16 33 32  2 26 37 26 25 23  4  1 27 12  5  6
 33  5 36 11 18 20 34  8 22 28 13 15 29 19  2  9 11 17  1 12 14  7 20 16
 10 24  3 21 10  1 10  1]
Losses:  10.316550254821777 3.8823275566101074 0.5896350741386414
CurrentTrain: epoch  0, batch     0 | loss: 10.3165503Losses:  13.142037391662598 5.656514644622803 0.5077067613601685
CurrentTrain: epoch  0, batch     1 | loss: 13.1420374Losses:  11.220916748046875 4.078895092010498 0.5000590085983276
CurrentTrain: epoch  0, batch     2 | loss: 11.2209167Losses:  4.884149551391602 -0.0 0.11081954091787338
CurrentTrain: epoch  0, batch     3 | loss: 4.8841496Losses:  13.223060607910156 6.07611083984375 0.647077739238739
CurrentTrain: epoch  1, batch     0 | loss: 13.2230606Losses:  10.463957786560059 4.136504173278809 0.41021308302879333
CurrentTrain: epoch  1, batch     1 | loss: 10.4639578Losses:  8.72097396850586 3.745131492614746 0.5475600957870483
CurrentTrain: epoch  1, batch     2 | loss: 8.7209740Losses:  2.664339303970337 -0.0 0.11220962554216385
CurrentTrain: epoch  1, batch     3 | loss: 2.6643393Losses:  9.124639511108398 3.763732671737671 0.4994915723800659
CurrentTrain: epoch  2, batch     0 | loss: 9.1246395Losses:  9.804439544677734 4.248561859130859 0.6098783612251282
CurrentTrain: epoch  2, batch     1 | loss: 9.8044395Losses:  7.056830406188965 2.236461639404297 0.6029729247093201
CurrentTrain: epoch  2, batch     2 | loss: 7.0568304Losses:  7.073044776916504 -0.0 -0.0
CurrentTrain: epoch  2, batch     3 | loss: 7.0730448Losses:  9.06955623626709 4.0468902587890625 0.5108518004417419
CurrentTrain: epoch  3, batch     0 | loss: 9.0695562Losses:  8.51357364654541 3.679020404815674 0.6077978610992432
CurrentTrain: epoch  3, batch     1 | loss: 8.5135736Losses:  6.904240608215332 2.3039698600769043 0.5924060344696045
CurrentTrain: epoch  3, batch     2 | loss: 6.9042406Losses:  6.170999050140381 -0.0 0.10091453790664673
CurrentTrain: epoch  3, batch     3 | loss: 6.1709991Losses:  8.39888858795166 4.2560529708862305 0.5154024362564087
CurrentTrain: epoch  4, batch     0 | loss: 8.3988886Losses:  7.885889530181885 3.338275671005249 0.5372289419174194
CurrentTrain: epoch  4, batch     1 | loss: 7.8858895Losses:  8.933284759521484 3.455648899078369 0.5879366993904114
CurrentTrain: epoch  4, batch     2 | loss: 8.9332848Losses:  2.9333763122558594 -0.0 0.08665712922811508
CurrentTrain: epoch  4, batch     3 | loss: 2.9333763Losses:  7.599049091339111 2.9772324562072754 0.6066865921020508
CurrentTrain: epoch  5, batch     0 | loss: 7.5990491Losses:  9.635306358337402 4.277811527252197 0.5201025605201721
CurrentTrain: epoch  5, batch     1 | loss: 9.6353064Losses:  7.002758026123047 3.3079004287719727 0.5100305080413818
CurrentTrain: epoch  5, batch     2 | loss: 7.0027580Losses:  4.0749053955078125 -0.0 0.12074679136276245
CurrentTrain: epoch  5, batch     3 | loss: 4.0749054Losses:  6.385281085968018 3.097332239151001 0.4419606328010559
CurrentTrain: epoch  6, batch     0 | loss: 6.3852811Losses:  7.86825704574585 3.330181121826172 0.5079789757728577
CurrentTrain: epoch  6, batch     1 | loss: 7.8682570Losses:  8.02585220336914 3.5606751441955566 0.6425115466117859
CurrentTrain: epoch  6, batch     2 | loss: 8.0258522Losses:  5.066207408905029 -0.0 0.07954370230436325
CurrentTrain: epoch  6, batch     3 | loss: 5.0662074Losses:  5.881781101226807 1.9935433864593506 0.512935996055603
CurrentTrain: epoch  7, batch     0 | loss: 5.8817811Losses:  8.47958755493164 4.628348350524902 0.6009291410446167
CurrentTrain: epoch  7, batch     1 | loss: 8.4795876Losses:  7.04338264465332 3.4443607330322266 0.5160638689994812
CurrentTrain: epoch  7, batch     2 | loss: 7.0433826Losses:  3.3606300354003906 -0.0 -0.0
CurrentTrain: epoch  7, batch     3 | loss: 3.3606300Losses:  5.853466987609863 2.502629041671753 0.49999237060546875
CurrentTrain: epoch  8, batch     0 | loss: 5.8534670Losses:  6.6516432762146 3.11820387840271 0.5847298502922058
CurrentTrain: epoch  8, batch     1 | loss: 6.6516433Losses:  6.440738201141357 3.063042640686035 0.507317841053009
CurrentTrain: epoch  8, batch     2 | loss: 6.4407382Losses:  4.506863594055176 -0.0 0.13023504614830017
CurrentTrain: epoch  8, batch     3 | loss: 4.5068636Losses:  4.984236240386963 1.7322497367858887 0.5677749514579773
CurrentTrain: epoch  9, batch     0 | loss: 4.9842362Losses:  5.637156963348389 2.3517754077911377 0.4970364272594452
CurrentTrain: epoch  9, batch     1 | loss: 5.6371570Losses:  5.171756267547607 1.932605504989624 0.5896110534667969
CurrentTrain: epoch  9, batch     2 | loss: 5.1717563Losses:  2.690582513809204 -0.0 0.08702255040407181
CurrentTrain: epoch  9, batch     3 | loss: 2.6905825
#### located on terrain feature ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "located on terrain feature" means located on the specified physical/geophysical feature.
Here is an example:
Relation: located on terrain feature
Context: the andesite line , a zone of intense volcanic and seismic activity , is a major regional distinction in the pacific .
Head Entity: andesite line
Tail Entity: pacific
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "located on terrain feature":
Relation: located on terrain feature  
Context: The Great Barrier Reef, a stunning natural wonder, stretches along the northeastern coast of Australia.  
Head Entity: Great Barrier Reef  
Tail Entity: Australia  

Relation: located on terrain feature  
Context: Mount Everest, the highest peak in the world, is part of the Himalayas and attracts climbers from around the globe.  
Head Entity: Mount Everest  
Tail Entity: Himalayas  
#### participant of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "participant of" means event in which a person or organization was/is a participant.
Here is an example:
Relation: participant of
Context: tiago venâncio ( born august 19 , 1987 , setúbal ) is a freestyle swimmer from portugal , who represented his home country at the 2004 summer olympics in athens , greece .
Head Entity: tiago venâncio
Tail Entity: 2004 summer olympics
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "participant of":
Relation: participant of  
Context: the united nations climate change conference, held in paris in 2015, saw numerous world leaders and organizations come together to discuss global warming and its impacts.  
Head Entity: united nations climate change conference  
Tail Entity: paris agreement  

Relation: participant of  
Context: the 2022 fifa world cup in qatar featured teams from around the globe competing for the prestigious trophy, showcasing the best of international football.  
Head Entity: 2022 fifa world cup  
Tail Entity: qatar
#### director ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "director" means director of film, TV-series, stageplay, video game or similar.
Here is an example:
Relation: director
Context: the festival opened with " whiplash " directed by damien chazelle and closed with musical drama " rudderless " directed by william h. macy .
Head Entity: whiplash
Tail Entity: damien chazelle
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "director":
Relation: director  
Context: the critically acclaimed series "breaking bad" was brought to life by the visionary director vince gilligan, while the spin-off "better call saul" continued his legacy under the direction of peter gould.  
Head Entity: breaking bad  
Tail Entity: vince gilligan  

Relation: director  
Context: the animated feature "spider-man: into the spider-verse" was directed by bob persichetti, peter ramsey, and rodney rothman, showcasing a unique visual style that captivated audiences.  
Head Entity: spider-man: into the spider-verse  
Tail Entity: bob persichetti  
#### owned by ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "owned by" means owner of the subject.
Here is an example:
Relation: owned by
Context: as of april 2013 , sony isp , known as so - net , released a new fibre service to tokyo for small businesses and homes and also made it available to six surrounding prefectures .
Head Entity: so - net
Tail Entity: sony
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "owned by":
Relation: owned by  
Context: In 2020, the popular video game franchise Call of Duty was acquired by Activision Blizzard, which has been a major player in the gaming industry for years.  
Head Entity: Call of Duty  
Tail Entity: Activision Blizzard  

Relation: owned by  
Context: The iconic fashion brand Gucci is a subsidiary of the French luxury group Kering, which oversees several high-end labels.  
Head Entity: Gucci  
Tail Entity: Kering  
#### part of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "part of" means object of which the subject is a part.
Here is an example:
Relation: part of
Context: thessaly was home to extensive neolithic and chalcolithic cultures around 6000 bc–2500 bc ( see cardium pottery , dimini and sesklo ) .
Head Entity: cardium pottery
Tail Entity: neolithic
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "part of":
Relation: part of  
Context: The engine is a crucial part of the car's overall functionality, providing the necessary power to drive.  
Head Entity: engine  
Tail Entity: car  

Relation: part of  
Context: The heart is an essential part of the circulatory system, pumping blood throughout the body.  
Head Entity: heart  
Tail Entity: circulatory system  
#### occupant ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "occupant" means person or organization occupying property.
Here is an example:
Relation: occupant
Context: the brewery closed in 1996 and is now the site of the halliwell jones stadium , home of warrington wolves , as well as a tesco store .
Head Entity: halliwell jones stadium
Tail Entity: warrington wolves
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "occupant":
Relation: occupant  
Context: The historic building was transformed into a luxury hotel, which now serves as the residence for many celebrities during film festivals.  
Head Entity: luxury hotel  
Tail Entity: celebrities  

Relation: occupant  
Context: After the renovation, the old factory became a vibrant art space, hosting various artists and their exhibitions throughout the year.  
Head Entity: art space  
Tail Entity: artists  
#### architect ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "architect" means person or architectural firm responsible for designing this building.
Here is an example:
Relation: architect
Context: the art nouveau style was brought to italy by figures such as giuseppe sommaruga and ernesto basile ( who respectively designed the palazzo castiglioni and expanded the palazzo montecitorio in rome ) .
Head Entity: palazzo castiglioni
Tail Entity: giuseppe sommaruga
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "architect":
Relation: architect  
Context: The iconic Sydney Opera House was designed by Danish architect Jørn Utzon, who won the design competition in 1957.  
Head Entity: Sydney Opera House  
Tail Entity: Jørn Utzon  

Relation: architect  
Context: Frank Lloyd Wright is renowned for his innovative designs, including the famous Fallingwater house, which seamlessly integrates with its natural surroundings.  
Head Entity: Fallingwater  
Tail Entity: Frank Lloyd Wright  
#### residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "residence" means the place where the person is or has been, resident.
Here is an example:
Relation: residence
Context: in 2001 brian haw set up the parliament square peace campaign outside the houses of parliament in london .
Head Entity: brian haw
Tail Entity: london
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "residence":
Relation: residence  
Context: After moving to New York City in 2010, Sarah began her career as a graphic designer.  
Head Entity: Sarah  
Tail Entity: New York City  

Relation: residence  
Context: The famous author spent many years living in a quaint cottage in the countryside of England.  
Head Entity: The famous author  
Tail Entity: England  
#### composer ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "composer" means person who wrote the music.
Here is an example:
Relation: composer
Context: at a young age he was an accomplished cellist , performing at age seventeen the solo part of richard strauss 's " don quixote " .
Head Entity: don quixote
Tail Entity: richard strauss
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "composer":
Relation: composer  
Context: The renowned composer Ludwig van Beethoven created some of the most celebrated symphonies in classical music history.  
Head Entity: symphonies  
Tail Entity: Ludwig van Beethoven  

Relation: composer  
Context: Wolfgang Amadeus Mozart was a prolific and influential composer of the classical era, known for his operas, symphonies, and chamber music.  
Head Entity: operas  
Tail Entity: Wolfgang Amadeus Mozart  
#### location ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "location" means location of the object, structure or event.
Here is an example:
Relation: location
Context: in 1972 , terrorists from the japanese red army launched an attack that led to the deaths of at least 24 people at ben gurion airport .
Head Entity: attack
Tail Entity: ben gurion airport
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "location":
Relation: location  
Context: The annual music festival will take place in the picturesque town of Coachella, attracting thousands of fans from around the world.  
Head Entity: music festival  
Tail Entity: Coachella  

Relation: location  
Context: The historic battle of Gettysburg was fought in Pennsylvania and is considered a turning point in the American Civil War.  
Head Entity: battle  
Tail Entity: Pennsylvania  
Losses:  3.1009607315063477 0.25742125511169434 0.981465220451355
MemoryTrain:  epoch  0, batch     0 | loss: 3.1009607Losses:  6.0417327880859375 0.518875777721405 0.8864453434944153
MemoryTrain:  epoch  0, batch     1 | loss: 6.0417328Losses:  4.699203014373779 0.23987247049808502 0.9890487194061279
MemoryTrain:  epoch  0, batch     2 | loss: 4.6992030Losses:  3.0023694038391113 0.26047223806381226 0.8007835745811462
MemoryTrain:  epoch  0, batch     3 | loss: 3.0023694Losses:  4.075626373291016 -0.0 1.1379971504211426
MemoryTrain:  epoch  0, batch     4 | loss: 4.0756264Losses:  3.2816922664642334 0.5670422315597534 0.8490754961967468
MemoryTrain:  epoch  0, batch     5 | loss: 3.2816923Losses:  3.6871259212493896 0.7154887914657593 0.8715073466300964
MemoryTrain:  epoch  0, batch     6 | loss: 3.6871259Losses:  3.535855293273926 0.25750911235809326 0.9784013032913208
MemoryTrain:  epoch  0, batch     7 | loss: 3.5358553Losses:  3.969954013824463 -0.0 0.9738319516181946
MemoryTrain:  epoch  0, batch     8 | loss: 3.9699540Losses:  5.031886100769043 0.26375848054885864 1.0067622661590576
MemoryTrain:  epoch  0, batch     9 | loss: 5.0318861Losses:  3.3534657955169678 0.23846003413200378 0.9244499206542969
MemoryTrain:  epoch  0, batch    10 | loss: 3.3534658Losses:  3.613037347793579 0.25014492869377136 1.0631425380706787
MemoryTrain:  epoch  0, batch    11 | loss: 3.6130373Losses:  3.9908485412597656 -0.0 0.9841012358665466
MemoryTrain:  epoch  0, batch    12 | loss: 3.9908485Losses:  3.3571743965148926 0.25811392068862915 0.9705241918563843
MemoryTrain:  epoch  0, batch    13 | loss: 3.3571744Losses:  4.7376627922058105 0.823902428150177 0.8984004259109497
MemoryTrain:  epoch  0, batch    14 | loss: 4.7376628Losses:  3.1566925048828125 0.2760063409805298 0.9796022176742554
MemoryTrain:  epoch  1, batch     0 | loss: 3.1566925Losses:  2.883432626724243 0.24439480900764465 0.9917792677879333
MemoryTrain:  epoch  1, batch     1 | loss: 2.8834326Losses:  3.2297985553741455 -0.0 1.1453096866607666
MemoryTrain:  epoch  1, batch     2 | loss: 3.2297986Losses:  3.143711566925049 0.2481851875782013 0.9620131254196167
MemoryTrain:  epoch  1, batch     3 | loss: 3.1437116Losses:  3.121919631958008 0.48574328422546387 0.961098313331604
MemoryTrain:  epoch  1, batch     4 | loss: 3.1219196Losses:  4.499561309814453 0.7525767087936401 0.975221574306488
MemoryTrain:  epoch  1, batch     5 | loss: 4.4995613Losses:  3.7120120525360107 -0.0 0.9103424549102783
MemoryTrain:  epoch  1, batch     6 | loss: 3.7120121Losses:  3.5642001628875732 0.5042164325714111 0.9222869277000427
MemoryTrain:  epoch  1, batch     7 | loss: 3.5642002Losses:  2.808781147003174 0.263779878616333 0.91953444480896
MemoryTrain:  epoch  1, batch     8 | loss: 2.8087811Losses:  3.813159227371216 0.25468185544013977 0.8658828735351562
MemoryTrain:  epoch  1, batch     9 | loss: 3.8131592Losses:  4.919427871704102 0.7583625912666321 1.0127148628234863
MemoryTrain:  epoch  1, batch    10 | loss: 4.9194279Losses:  3.3668172359466553 -0.0 1.0081779956817627
MemoryTrain:  epoch  1, batch    11 | loss: 3.3668172Losses:  3.5741963386535645 0.23819658160209656 0.7921679019927979
MemoryTrain:  epoch  1, batch    12 | loss: 3.5741963Losses:  3.46433162689209 0.5328148007392883 0.9204127192497253
MemoryTrain:  epoch  1, batch    13 | loss: 3.4643316Losses:  3.864206314086914 -0.0 0.9575563669204712
MemoryTrain:  epoch  1, batch    14 | loss: 3.8642063Losses:  3.3189094066619873 -0.0 0.9078897833824158
MemoryTrain:  epoch  2, batch     0 | loss: 3.3189094Losses:  4.237970352172852 0.49653351306915283 0.7884829044342041
MemoryTrain:  epoch  2, batch     1 | loss: 4.2379704Losses:  4.004569053649902 0.25774112343788147 0.9188992381095886
MemoryTrain:  epoch  2, batch     2 | loss: 4.0045691Losses:  2.5854244232177734 -0.0 1.0199304819107056
MemoryTrain:  epoch  2, batch     3 | loss: 2.5854244Losses:  3.2949037551879883 0.2524612247943878 0.9556167721748352
MemoryTrain:  epoch  2, batch     4 | loss: 3.2949038Losses:  2.911407232284546 0.25454217195510864 1.0367400646209717
MemoryTrain:  epoch  2, batch     5 | loss: 2.9114072Losses:  3.0618889331817627 0.5169903039932251 0.9124509692192078
MemoryTrain:  epoch  2, batch     6 | loss: 3.0618889Losses:  2.824838876724243 0.24213047325611115 1.0506479740142822
MemoryTrain:  epoch  2, batch     7 | loss: 2.8248389Losses:  2.598764657974243 -0.0 1.0148766040802002
MemoryTrain:  epoch  2, batch     8 | loss: 2.5987647Losses:  3.2286040782928467 0.25016874074935913 0.9059640765190125
MemoryTrain:  epoch  2, batch     9 | loss: 3.2286041Losses:  2.8879599571228027 -0.0 1.039586067199707
MemoryTrain:  epoch  2, batch    10 | loss: 2.8879600Losses:  3.069890022277832 0.5078128576278687 0.8990235328674316
MemoryTrain:  epoch  2, batch    11 | loss: 3.0698900Losses:  2.78353214263916 0.4883495569229126 1.045313835144043
MemoryTrain:  epoch  2, batch    12 | loss: 2.7835321Losses:  3.1159396171569824 -0.0 0.8968992233276367
MemoryTrain:  epoch  2, batch    13 | loss: 3.1159396Losses:  2.7038960456848145 0.49216562509536743 0.9027403593063354
MemoryTrain:  epoch  2, batch    14 | loss: 2.7038960Losses:  3.095754623413086 0.5240300893783569 0.9620261788368225
MemoryTrain:  epoch  3, batch     0 | loss: 3.0957546Losses:  3.490953207015991 0.7406589388847351 0.9804077744483948
MemoryTrain:  epoch  3, batch     1 | loss: 3.4909532Losses:  3.019296169281006 0.4984702467918396 0.9387311339378357
MemoryTrain:  epoch  3, batch     2 | loss: 3.0192962Losses:  3.0370607376098633 0.7544468641281128 0.9645504951477051
MemoryTrain:  epoch  3, batch     3 | loss: 3.0370607Losses:  3.5012049674987793 0.23749101161956787 1.004417896270752
MemoryTrain:  epoch  3, batch     4 | loss: 3.5012050Losses:  3.0533595085144043 0.7457420229911804 0.9743791818618774
MemoryTrain:  epoch  3, batch     5 | loss: 3.0533595Losses:  2.693483591079712 0.2581215798854828 1.0418142080307007
MemoryTrain:  epoch  3, batch     6 | loss: 2.6934836Losses:  3.974189043045044 0.48317283391952515 0.9674283862113953
MemoryTrain:  epoch  3, batch     7 | loss: 3.9741890Losses:  2.5467066764831543 -0.0 1.0111092329025269
MemoryTrain:  epoch  3, batch     8 | loss: 2.5467067Losses:  2.3258063793182373 -0.0 1.0084304809570312
MemoryTrain:  epoch  3, batch     9 | loss: 2.3258064Losses:  2.3539507389068604 -0.0 1.0174400806427002
MemoryTrain:  epoch  3, batch    10 | loss: 2.3539507Losses:  3.2936625480651855 0.24845585227012634 0.9758435487747192
MemoryTrain:  epoch  3, batch    11 | loss: 3.2936625Losses:  2.800469398498535 -0.0 0.9126261472702026
MemoryTrain:  epoch  3, batch    12 | loss: 2.8004694Losses:  2.6012845039367676 0.25309282541275024 0.8640283346176147
MemoryTrain:  epoch  3, batch    13 | loss: 2.6012845Losses:  2.4692912101745605 -0.0 0.8215925097465515
MemoryTrain:  epoch  3, batch    14 | loss: 2.4692912Losses:  2.2362546920776367 -0.0 0.8972947597503662
MemoryTrain:  epoch  4, batch     0 | loss: 2.2362547Losses:  2.9268147945404053 0.2376197874546051 0.957267701625824
MemoryTrain:  epoch  4, batch     1 | loss: 2.9268148Losses:  2.6554813385009766 0.24099822342395782 1.0246542692184448
MemoryTrain:  epoch  4, batch     2 | loss: 2.6554813Losses:  2.7504982948303223 0.30523449182510376 0.9142827391624451
MemoryTrain:  epoch  4, batch     3 | loss: 2.7504983Losses:  2.5460593700408936 -0.0 0.9535457491874695
MemoryTrain:  epoch  4, batch     4 | loss: 2.5460594Losses:  2.9211249351501465 0.49531805515289307 0.9757884740829468
MemoryTrain:  epoch  4, batch     5 | loss: 2.9211249Losses:  2.5206847190856934 -0.0 1.0076072216033936
MemoryTrain:  epoch  4, batch     6 | loss: 2.5206847Losses:  3.4194698333740234 0.5321676731109619 0.918156623840332
MemoryTrain:  epoch  4, batch     7 | loss: 3.4194698Losses:  2.490255355834961 -0.0 1.0049500465393066
MemoryTrain:  epoch  4, batch     8 | loss: 2.4902554Losses:  2.8549091815948486 0.7350257635116577 0.8496799468994141
MemoryTrain:  epoch  4, batch     9 | loss: 2.8549092Losses:  3.220763683319092 0.5670918226242065 1.0031821727752686
MemoryTrain:  epoch  4, batch    10 | loss: 3.2207637Losses:  3.4132285118103027 0.6080466508865356 0.9762539267539978
MemoryTrain:  epoch  4, batch    11 | loss: 3.4132285Losses:  2.397657632827759 -0.0 1.016366720199585
MemoryTrain:  epoch  4, batch    12 | loss: 2.3976576Losses:  3.1138765811920166 0.2422696352005005 0.984719455242157
MemoryTrain:  epoch  4, batch    13 | loss: 3.1138766Losses:  2.8316612243652344 0.2521369457244873 1.009331226348877
MemoryTrain:  epoch  4, batch    14 | loss: 2.8316612Losses:  2.6628613471984863 0.22285708785057068 0.9359599947929382
MemoryTrain:  epoch  5, batch     0 | loss: 2.6628613Losses:  2.7593255043029785 0.482524037361145 0.9702746868133545
MemoryTrain:  epoch  5, batch     1 | loss: 2.7593255Losses:  3.0859029293060303 0.523821234703064 0.9325778484344482
MemoryTrain:  epoch  5, batch     2 | loss: 3.0859029Losses:  2.1485562324523926 -0.0 0.9125207662582397
MemoryTrain:  epoch  5, batch     3 | loss: 2.1485562Losses:  2.9039814472198486 0.2616846263408661 1.0116491317749023
MemoryTrain:  epoch  5, batch     4 | loss: 2.9039814Losses:  3.076641082763672 0.4880011975765228 0.909975528717041
MemoryTrain:  epoch  5, batch     5 | loss: 3.0766411Losses:  2.9138851165771484 0.2377568781375885 1.0276809930801392
MemoryTrain:  epoch  5, batch     6 | loss: 2.9138851Losses:  2.6392476558685303 0.2513160705566406 0.8990190029144287
MemoryTrain:  epoch  5, batch     7 | loss: 2.6392477Losses:  2.9835450649261475 0.2473691999912262 0.9859862327575684
MemoryTrain:  epoch  5, batch     8 | loss: 2.9835451Losses:  2.615067720413208 -0.0 0.9706231951713562
MemoryTrain:  epoch  5, batch     9 | loss: 2.6150677Losses:  2.735278606414795 0.5259159207344055 0.9313162565231323
MemoryTrain:  epoch  5, batch    10 | loss: 2.7352786Losses:  2.8964409828186035 -0.0 1.069482684135437
MemoryTrain:  epoch  5, batch    11 | loss: 2.8964410Losses:  2.8683173656463623 0.7541773915290833 0.8366847038269043
MemoryTrain:  epoch  5, batch    12 | loss: 2.8683174Losses:  2.568225383758545 0.24187904596328735 0.9539230465888977
MemoryTrain:  epoch  5, batch    13 | loss: 2.5682254Losses:  2.666961193084717 0.24863864481449127 1.0194170475006104
MemoryTrain:  epoch  5, batch    14 | loss: 2.6669612Losses:  2.9870519638061523 0.26097071170806885 1.0182138681411743
MemoryTrain:  epoch  6, batch     0 | loss: 2.9870520Losses:  2.30501389503479 -0.0 0.9765525460243225
MemoryTrain:  epoch  6, batch     1 | loss: 2.3050139Losses:  2.2776618003845215 -0.0 0.9603326320648193
MemoryTrain:  epoch  6, batch     2 | loss: 2.2776618Losses:  2.413410186767578 -0.0 0.951611340045929
MemoryTrain:  epoch  6, batch     3 | loss: 2.4134102Losses:  2.2092392444610596 -0.0 0.9478301405906677
MemoryTrain:  epoch  6, batch     4 | loss: 2.2092392Losses:  2.5468130111694336 0.2386295199394226 1.0666277408599854
MemoryTrain:  epoch  6, batch     5 | loss: 2.5468130Losses:  2.9032034873962402 0.5406221747398376 0.8969199061393738
MemoryTrain:  epoch  6, batch     6 | loss: 2.9032035Losses:  2.822619915008545 0.5447613000869751 0.9677954912185669
MemoryTrain:  epoch  6, batch     7 | loss: 2.8226199Losses:  2.28421688079834 -0.0 1.068006992340088
MemoryTrain:  epoch  6, batch     8 | loss: 2.2842169Losses:  2.635765552520752 0.24109616875648499 0.9047834277153015
MemoryTrain:  epoch  6, batch     9 | loss: 2.6357656Losses:  3.219956874847412 0.4715573787689209 0.963551938533783
MemoryTrain:  epoch  6, batch    10 | loss: 3.2199569Losses:  2.623088836669922 0.24808049201965332 1.0600444078445435
MemoryTrain:  epoch  6, batch    11 | loss: 2.6230888Losses:  2.764138698577881 0.24481835961341858 0.9074284434318542
MemoryTrain:  epoch  6, batch    12 | loss: 2.7641387Losses:  3.760486125946045 0.3216351568698883 0.9784289598464966
MemoryTrain:  epoch  6, batch    13 | loss: 3.7604861Losses:  2.465456247329712 -0.0 1.0377000570297241
MemoryTrain:  epoch  6, batch    14 | loss: 2.4654562Losses:  2.472050905227661 0.24640393257141113 0.9091987609863281
MemoryTrain:  epoch  7, batch     0 | loss: 2.4720509Losses:  2.5922651290893555 0.47610050439834595 0.8824262022972107
MemoryTrain:  epoch  7, batch     1 | loss: 2.5922651Losses:  2.974846124649048 0.3179633617401123 1.0162014961242676
MemoryTrain:  epoch  7, batch     2 | loss: 2.9748461Losses:  2.7273688316345215 -0.0 1.0565868616104126
MemoryTrain:  epoch  7, batch     3 | loss: 2.7273688Losses:  2.7108678817749023 0.514072060585022 0.970633864402771
MemoryTrain:  epoch  7, batch     4 | loss: 2.7108679Losses:  2.6335978507995605 -0.0 0.877977728843689
MemoryTrain:  epoch  7, batch     5 | loss: 2.6335979Losses:  2.3127310276031494 -0.0 1.0109002590179443
MemoryTrain:  epoch  7, batch     6 | loss: 2.3127310Losses:  2.575601577758789 0.25449132919311523 0.970934271812439
MemoryTrain:  epoch  7, batch     7 | loss: 2.5756016Losses:  2.523578643798828 -0.0 1.0673184394836426
MemoryTrain:  epoch  7, batch     8 | loss: 2.5235786Losses:  2.7398152351379395 0.4858066141605377 0.9596359729766846
MemoryTrain:  epoch  7, batch     9 | loss: 2.7398152Losses:  3.0680577754974365 0.512987494468689 0.8617612719535828
MemoryTrain:  epoch  7, batch    10 | loss: 3.0680578Losses:  2.7377877235412598 0.3447491526603699 0.9754372835159302
MemoryTrain:  epoch  7, batch    11 | loss: 2.7377877Losses:  2.1832170486450195 -0.0 1.0064053535461426
MemoryTrain:  epoch  7, batch    12 | loss: 2.1832170Losses:  2.606290578842163 0.4942144751548767 0.8641719818115234
MemoryTrain:  epoch  7, batch    13 | loss: 2.6062906Losses:  2.918517589569092 0.24670815467834473 0.9552597999572754
MemoryTrain:  epoch  7, batch    14 | loss: 2.9185176Losses:  2.4449944496154785 0.25629961490631104 0.8961868286132812
MemoryTrain:  epoch  8, batch     0 | loss: 2.4449944Losses:  2.4908883571624756 -0.0 1.010326862335205
MemoryTrain:  epoch  8, batch     1 | loss: 2.4908884Losses:  2.853179693222046 0.5372800827026367 0.905159592628479
MemoryTrain:  epoch  8, batch     2 | loss: 2.8531797Losses:  2.5295326709747314 0.26116040349006653 0.924921989440918
MemoryTrain:  epoch  8, batch     3 | loss: 2.5295327Losses:  2.627039909362793 -0.0 0.8662915825843811
MemoryTrain:  epoch  8, batch     4 | loss: 2.6270399Losses:  2.506892681121826 0.23105071485042572 0.8791728019714355
MemoryTrain:  epoch  8, batch     5 | loss: 2.5068927Losses:  2.4292092323303223 0.235444575548172 0.9643088579177856
MemoryTrain:  epoch  8, batch     6 | loss: 2.4292092Losses:  2.1916298866271973 -0.0 0.9838081002235413
MemoryTrain:  epoch  8, batch     7 | loss: 2.1916299Losses:  2.694093704223633 -0.0 1.010836124420166
MemoryTrain:  epoch  8, batch     8 | loss: 2.6940937Losses:  2.889133930206299 0.5497990846633911 0.9104938507080078
MemoryTrain:  epoch  8, batch     9 | loss: 2.8891339Losses:  2.799891233444214 0.49042898416519165 0.8923777937889099
MemoryTrain:  epoch  8, batch    10 | loss: 2.7998912Losses:  3.2684109210968018 1.3025250434875488 0.7797477841377258
MemoryTrain:  epoch  8, batch    11 | loss: 3.2684109Losses:  2.3338470458984375 -0.0 1.0703827142715454
MemoryTrain:  epoch  8, batch    12 | loss: 2.3338470Losses:  2.57112455368042 0.23909582197666168 1.0154438018798828
MemoryTrain:  epoch  8, batch    13 | loss: 2.5711246Losses:  2.5668509006500244 0.24373674392700195 1.0277152061462402
MemoryTrain:  epoch  8, batch    14 | loss: 2.5668509Losses:  2.7358014583587646 -0.0 0.9608139991760254
MemoryTrain:  epoch  9, batch     0 | loss: 2.7358015Losses:  2.4720237255096436 0.2334723025560379 0.9767521023750305
MemoryTrain:  epoch  9, batch     1 | loss: 2.4720237Losses:  2.5789430141448975 0.49547719955444336 0.8458277583122253
MemoryTrain:  epoch  9, batch     2 | loss: 2.5789430Losses:  2.510854721069336 0.25561392307281494 0.9702268838882446
MemoryTrain:  epoch  9, batch     3 | loss: 2.5108547Losses:  2.6653146743774414 0.4758918285369873 0.944342851638794
MemoryTrain:  epoch  9, batch     4 | loss: 2.6653147Losses:  2.607056140899658 0.24553221464157104 0.971451997756958
MemoryTrain:  epoch  9, batch     5 | loss: 2.6070561Losses:  2.7111120223999023 0.48637717962265015 0.9772731065750122
MemoryTrain:  epoch  9, batch     6 | loss: 2.7111120Losses:  2.808105945587158 0.24300144612789154 0.9481194019317627
MemoryTrain:  epoch  9, batch     7 | loss: 2.8081059Losses:  2.447582960128784 0.2529311180114746 0.9634248614311218
MemoryTrain:  epoch  9, batch     8 | loss: 2.4475830Losses:  2.534832000732422 0.23495206236839294 0.8500868678092957
MemoryTrain:  epoch  9, batch     9 | loss: 2.5348320Losses:  2.2759311199188232 -0.0 0.9280397295951843
MemoryTrain:  epoch  9, batch    10 | loss: 2.2759311Losses:  2.8148231506347656 0.5278853178024292 0.9050365686416626
MemoryTrain:  epoch  9, batch    11 | loss: 2.8148232Losses:  2.4495890140533447 0.2262236326932907 1.0139437913894653
MemoryTrain:  epoch  9, batch    12 | loss: 2.4495890Losses:  2.433725357055664 0.24492716789245605 0.9620757699012756
MemoryTrain:  epoch  9, batch    13 | loss: 2.4337254Losses:  3.079819917678833 0.511893093585968 0.8046054840087891
MemoryTrain:  epoch  9, batch    14 | loss: 3.0798199
[EVAL] batch:    0 | acc: 6.25%,  total acc: 6.25%   [EVAL] batch:    1 | acc: 31.25%,  total acc: 18.75%   [EVAL] batch:    2 | acc: 25.00%,  total acc: 20.83%   [EVAL] batch:    3 | acc: 18.75%,  total acc: 20.31%   [EVAL] batch:    4 | acc: 25.00%,  total acc: 21.25%   [EVAL] batch:    5 | acc: 12.50%,  total acc: 19.79%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 26.79%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 34.38%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 40.28%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 44.38%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 47.73%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 50.00%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 52.88%   [EVAL] batch:   13 | acc: 75.00%,  total acc: 54.46%   [EVAL] batch:   14 | acc: 87.50%,  total acc: 56.67%   [EVAL] batch:   15 | acc: 87.50%,  total acc: 58.59%   [EVAL] batch:   16 | acc: 87.50%,  total acc: 60.29%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 62.15%   [EVAL] batch:   18 | acc: 75.00%,  total acc: 62.83%   [EVAL] batch:   19 | acc: 43.75%,  total acc: 61.88%   [EVAL] batch:   20 | acc: 37.50%,  total acc: 60.71%   [EVAL] batch:   21 | acc: 43.75%,  total acc: 59.94%   [EVAL] batch:   22 | acc: 43.75%,  total acc: 59.24%   [EVAL] batch:   23 | acc: 25.00%,  total acc: 57.81%   [EVAL] batch:   24 | acc: 50.00%,  total acc: 57.50%   [EVAL] batch:   25 | acc: 0.00%,  total acc: 55.29%   [EVAL] batch:   26 | acc: 6.25%,  total acc: 53.47%   [EVAL] batch:   27 | acc: 12.50%,  total acc: 52.01%   [EVAL] batch:   28 | acc: 6.25%,  total acc: 50.43%   [EVAL] batch:   29 | acc: 18.75%,  total acc: 49.38%   [EVAL] batch:   30 | acc: 0.00%,  total acc: 47.78%   [EVAL] batch:   31 | acc: 56.25%,  total acc: 48.05%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 49.43%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 50.74%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 52.14%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 53.30%   [EVAL] batch:   36 | acc: 75.00%,  total acc: 53.89%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 54.77%   [EVAL] batch:   38 | acc: 87.50%,  total acc: 55.61%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 56.72%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 57.62%   [EVAL] batch:   41 | acc: 75.00%,  total acc: 58.04%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 59.01%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 59.66%   [EVAL] batch:   44 | acc: 68.75%,  total acc: 59.86%   [EVAL] batch:   45 | acc: 81.25%,  total acc: 60.33%   [EVAL] batch:   46 | acc: 56.25%,  total acc: 60.24%   [EVAL] batch:   47 | acc: 62.50%,  total acc: 60.29%   [EVAL] batch:   48 | acc: 81.25%,  total acc: 60.71%   [EVAL] batch:   49 | acc: 68.75%,  total acc: 60.88%   [EVAL] batch:   50 | acc: 31.25%,  total acc: 60.29%   [EVAL] batch:   51 | acc: 56.25%,  total acc: 60.22%   [EVAL] batch:   52 | acc: 50.00%,  total acc: 60.02%   [EVAL] batch:   53 | acc: 62.50%,  total acc: 60.07%   [EVAL] batch:   54 | acc: 50.00%,  total acc: 59.89%   [EVAL] batch:   55 | acc: 62.50%,  total acc: 59.93%   [EVAL] batch:   56 | acc: 43.75%,  total acc: 59.65%   [EVAL] batch:   57 | acc: 56.25%,  total acc: 59.59%   [EVAL] batch:   58 | acc: 37.50%,  total acc: 59.22%   [EVAL] batch:   59 | acc: 62.50%,  total acc: 59.27%   [EVAL] batch:   60 | acc: 50.00%,  total acc: 59.12%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 59.58%   [EVAL] batch:   62 | acc: 25.00%,  total acc: 59.03%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 79.69%   [EVAL] batch:    4 | acc: 62.50%,  total acc: 76.25%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 78.12%   [EVAL] batch:    6 | acc: 43.75%,  total acc: 73.21%   [EVAL] batch:    7 | acc: 43.75%,  total acc: 69.53%   [EVAL] batch:    8 | acc: 43.75%,  total acc: 66.67%   [EVAL] batch:    9 | acc: 50.00%,  total acc: 65.00%   [EVAL] batch:   10 | acc: 31.25%,  total acc: 61.93%   [EVAL] batch:   11 | acc: 50.00%,  total acc: 60.94%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 61.54%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 63.84%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 65.83%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 67.97%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 69.49%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 70.83%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 72.37%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 72.81%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 73.51%   [EVAL] batch:   21 | acc: 68.75%,  total acc: 73.30%   [EVAL] batch:   22 | acc: 62.50%,  total acc: 72.83%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 73.44%   [EVAL] batch:   24 | acc: 68.75%,  total acc: 73.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 74.28%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 75.23%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 76.12%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 76.72%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 77.50%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 78.23%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 78.91%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 79.55%   [EVAL] batch:   33 | acc: 62.50%,  total acc: 79.04%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 78.93%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 79.17%   [EVAL] batch:   36 | acc: 87.50%,  total acc: 79.39%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 79.77%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 80.29%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 80.78%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 81.10%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 81.40%   [EVAL] batch:   42 | acc: 87.50%,  total acc: 81.54%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 81.68%   [EVAL] batch:   44 | acc: 68.75%,  total acc: 81.39%   [EVAL] batch:   45 | acc: 68.75%,  total acc: 81.11%   [EVAL] batch:   46 | acc: 43.75%,  total acc: 80.32%   [EVAL] batch:   47 | acc: 56.25%,  total acc: 79.82%   [EVAL] batch:   48 | acc: 75.00%,  total acc: 79.72%   [EVAL] batch:   49 | acc: 56.25%,  total acc: 79.25%   [EVAL] batch:   50 | acc: 87.50%,  total acc: 79.41%   [EVAL] batch:   51 | acc: 68.75%,  total acc: 79.21%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 79.48%   [EVAL] batch:   53 | acc: 62.50%,  total acc: 79.17%   [EVAL] batch:   54 | acc: 56.25%,  total acc: 78.75%   [EVAL] batch:   55 | acc: 81.25%,  total acc: 78.79%   [EVAL] batch:   56 | acc: 62.50%,  total acc: 78.51%   [EVAL] batch:   57 | acc: 37.50%,  total acc: 77.80%   [EVAL] batch:   58 | acc: 62.50%,  total acc: 77.54%   [EVAL] batch:   59 | acc: 87.50%,  total acc: 77.71%   [EVAL] batch:   60 | acc: 81.25%,  total acc: 77.77%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 77.62%   [EVAL] batch:   62 | acc: 68.75%,  total acc: 77.48%   [EVAL] batch:   63 | acc: 50.00%,  total acc: 77.05%   [EVAL] batch:   64 | acc: 37.50%,  total acc: 76.44%   [EVAL] batch:   65 | acc: 62.50%,  total acc: 76.23%   [EVAL] batch:   66 | acc: 37.50%,  total acc: 75.65%   [EVAL] batch:   67 | acc: 37.50%,  total acc: 75.09%   [EVAL] batch:   68 | acc: 75.00%,  total acc: 75.09%   [EVAL] batch:   69 | acc: 81.25%,  total acc: 75.18%   [EVAL] batch:   70 | acc: 62.50%,  total acc: 75.00%   [EVAL] batch:   71 | acc: 68.75%,  total acc: 74.91%   [EVAL] batch:   72 | acc: 81.25%,  total acc: 75.00%   [EVAL] batch:   73 | acc: 87.50%,  total acc: 75.17%   [EVAL] batch:   74 | acc: 75.00%,  total acc: 75.17%   [EVAL] batch:   75 | acc: 18.75%,  total acc: 74.42%   [EVAL] batch:   76 | acc: 12.50%,  total acc: 73.62%   [EVAL] batch:   77 | acc: 25.00%,  total acc: 73.00%   [EVAL] batch:   78 | acc: 31.25%,  total acc: 72.47%   [EVAL] batch:   79 | acc: 43.75%,  total acc: 72.11%   [EVAL] batch:   80 | acc: 18.75%,  total acc: 71.45%   [EVAL] batch:   81 | acc: 68.75%,  total acc: 71.42%   [EVAL] batch:   82 | acc: 100.00%,  total acc: 71.76%   [EVAL] batch:   83 | acc: 100.00%,  total acc: 72.10%   [EVAL] batch:   84 | acc: 93.75%,  total acc: 72.35%   [EVAL] batch:   85 | acc: 100.00%,  total acc: 72.67%   [EVAL] batch:   86 | acc: 93.75%,  total acc: 72.92%   [EVAL] batch:   87 | acc: 75.00%,  total acc: 72.94%   [EVAL] batch:   88 | acc: 25.00%,  total acc: 72.40%   [EVAL] batch:   89 | acc: 25.00%,  total acc: 71.88%   [EVAL] batch:   90 | acc: 25.00%,  total acc: 71.36%   [EVAL] batch:   91 | acc: 37.50%,  total acc: 70.99%   [EVAL] batch:   92 | acc: 6.25%,  total acc: 70.30%   [EVAL] batch:   93 | acc: 50.00%,  total acc: 70.08%   [EVAL] batch:   94 | acc: 25.00%,  total acc: 69.61%   [EVAL] batch:   95 | acc: 25.00%,  total acc: 69.14%   [EVAL] batch:   96 | acc: 37.50%,  total acc: 68.81%   [EVAL] batch:   97 | acc: 31.25%,  total acc: 68.43%   [EVAL] batch:   98 | acc: 18.75%,  total acc: 67.93%   [EVAL] batch:   99 | acc: 50.00%,  total acc: 67.75%   [EVAL] batch:  100 | acc: 25.00%,  total acc: 67.33%   [EVAL] batch:  101 | acc: 12.50%,  total acc: 66.79%   [EVAL] batch:  102 | acc: 6.25%,  total acc: 66.20%   [EVAL] batch:  103 | acc: 6.25%,  total acc: 65.62%   [EVAL] batch:  104 | acc: 12.50%,  total acc: 65.12%   [EVAL] batch:  105 | acc: 12.50%,  total acc: 64.62%   [EVAL] batch:  106 | acc: 31.25%,  total acc: 64.31%   [EVAL] batch:  107 | acc: 12.50%,  total acc: 63.83%   [EVAL] batch:  108 | acc: 18.75%,  total acc: 63.42%   [EVAL] batch:  109 | acc: 25.00%,  total acc: 63.07%   [EVAL] batch:  110 | acc: 12.50%,  total acc: 62.61%   [EVAL] batch:  111 | acc: 25.00%,  total acc: 62.28%   [EVAL] batch:  112 | acc: 37.50%,  total acc: 62.06%   [EVAL] batch:  113 | acc: 50.00%,  total acc: 61.95%   [EVAL] batch:  114 | acc: 75.00%,  total acc: 62.07%   [EVAL] batch:  115 | acc: 62.50%,  total acc: 62.07%   [EVAL] batch:  116 | acc: 75.00%,  total acc: 62.18%   [EVAL] batch:  117 | acc: 68.75%,  total acc: 62.24%   [EVAL] batch:  118 | acc: 62.50%,  total acc: 62.24%   [EVAL] batch:  119 | acc: 87.50%,  total acc: 62.45%   [EVAL] batch:  120 | acc: 81.25%,  total acc: 62.60%   [EVAL] batch:  121 | acc: 75.00%,  total acc: 62.70%   [EVAL] batch:  122 | acc: 62.50%,  total acc: 62.70%   [EVAL] batch:  123 | acc: 81.25%,  total acc: 62.85%   [EVAL] batch:  124 | acc: 75.00%,  total acc: 62.95%   [EVAL] batch:  125 | acc: 12.50%,  total acc: 62.55%   [EVAL] batch:  126 | acc: 6.25%,  total acc: 62.11%   [EVAL] batch:  127 | acc: 6.25%,  total acc: 61.67%   [EVAL] batch:  128 | acc: 6.25%,  total acc: 61.24%   [EVAL] batch:  129 | acc: 6.25%,  total acc: 60.82%   [EVAL] batch:  130 | acc: 6.25%,  total acc: 60.40%   [EVAL] batch:  131 | acc: 31.25%,  total acc: 60.18%   [EVAL] batch:  132 | acc: 56.25%,  total acc: 60.15%   [EVAL] batch:  133 | acc: 68.75%,  total acc: 60.21%   [EVAL] batch:  134 | acc: 75.00%,  total acc: 60.32%   [EVAL] batch:  135 | acc: 75.00%,  total acc: 60.43%   [EVAL] batch:  136 | acc: 56.25%,  total acc: 60.40%   [EVAL] batch:  137 | acc: 56.25%,  total acc: 60.37%   [EVAL] batch:  138 | acc: 25.00%,  total acc: 60.12%   [EVAL] batch:  139 | acc: 25.00%,  total acc: 59.87%   [EVAL] batch:  140 | acc: 25.00%,  total acc: 59.62%   [EVAL] batch:  141 | acc: 31.25%,  total acc: 59.42%   [EVAL] batch:  142 | acc: 43.75%,  total acc: 59.31%   [EVAL] batch:  143 | acc: 43.75%,  total acc: 59.20%   [EVAL] batch:  144 | acc: 100.00%,  total acc: 59.48%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 59.76%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 59.99%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 60.26%   [EVAL] batch:  148 | acc: 93.75%,  total acc: 60.49%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 60.71%   [EVAL] batch:  150 | acc: 100.00%,  total acc: 60.97%   [EVAL] batch:  151 | acc: 100.00%,  total acc: 61.23%   [EVAL] batch:  152 | acc: 100.00%,  total acc: 61.48%   [EVAL] batch:  153 | acc: 93.75%,  total acc: 61.69%   [EVAL] batch:  154 | acc: 100.00%,  total acc: 61.94%   [EVAL] batch:  155 | acc: 93.75%,  total acc: 62.14%   [EVAL] batch:  156 | acc: 100.00%,  total acc: 62.38%   [EVAL] batch:  157 | acc: 100.00%,  total acc: 62.62%   [EVAL] batch:  158 | acc: 93.75%,  total acc: 62.81%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 63.01%   [EVAL] batch:  160 | acc: 93.75%,  total acc: 63.20%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 63.39%   [EVAL] batch:  162 | acc: 56.25%,  total acc: 63.34%   [EVAL] batch:  163 | acc: 43.75%,  total acc: 63.22%   [EVAL] batch:  164 | acc: 18.75%,  total acc: 62.95%   [EVAL] batch:  165 | acc: 18.75%,  total acc: 62.69%   [EVAL] batch:  166 | acc: 12.50%,  total acc: 62.39%   [EVAL] batch:  167 | acc: 12.50%,  total acc: 62.09%   [EVAL] batch:  168 | acc: 56.25%,  total acc: 62.06%   [EVAL] batch:  169 | acc: 75.00%,  total acc: 62.13%   [EVAL] batch:  170 | acc: 81.25%,  total acc: 62.24%   [EVAL] batch:  171 | acc: 81.25%,  total acc: 62.35%   [EVAL] batch:  172 | acc: 68.75%,  total acc: 62.39%   [EVAL] batch:  173 | acc: 56.25%,  total acc: 62.36%   [EVAL] batch:  174 | acc: 68.75%,  total acc: 62.39%   [EVAL] batch:  175 | acc: 25.00%,  total acc: 62.18%   [EVAL] batch:  176 | acc: 31.25%,  total acc: 62.01%   [EVAL] batch:  177 | acc: 12.50%,  total acc: 61.73%   [EVAL] batch:  178 | acc: 43.75%,  total acc: 61.63%   [EVAL] batch:  179 | acc: 43.75%,  total acc: 61.53%   [EVAL] batch:  180 | acc: 18.75%,  total acc: 61.29%   [EVAL] batch:  181 | acc: 56.25%,  total acc: 61.26%   [EVAL] batch:  182 | acc: 68.75%,  total acc: 61.30%   [EVAL] batch:  183 | acc: 50.00%,  total acc: 61.24%   [EVAL] batch:  184 | acc: 75.00%,  total acc: 61.32%   [EVAL] batch:  185 | acc: 87.50%,  total acc: 61.46%   [EVAL] batch:  186 | acc: 87.50%,  total acc: 61.60%   [EVAL] batch:  187 | acc: 68.75%,  total acc: 61.64%   [EVAL] batch:  188 | acc: 50.00%,  total acc: 61.57%   [EVAL] batch:  189 | acc: 68.75%,  total acc: 61.61%   [EVAL] batch:  190 | acc: 81.25%,  total acc: 61.71%   [EVAL] batch:  191 | acc: 81.25%,  total acc: 61.82%   [EVAL] batch:  192 | acc: 81.25%,  total acc: 61.92%   [EVAL] batch:  193 | acc: 62.50%,  total acc: 61.92%   [EVAL] batch:  194 | acc: 87.50%,  total acc: 62.05%   [EVAL] batch:  195 | acc: 62.50%,  total acc: 62.05%   [EVAL] batch:  196 | acc: 62.50%,  total acc: 62.06%   [EVAL] batch:  197 | acc: 87.50%,  total acc: 62.18%   [EVAL] batch:  198 | acc: 75.00%,  total acc: 62.25%   [EVAL] batch:  199 | acc: 75.00%,  total acc: 62.31%   [EVAL] batch:  200 | acc: 87.50%,  total acc: 62.44%   [EVAL] batch:  201 | acc: 75.00%,  total acc: 62.50%   [EVAL] batch:  202 | acc: 56.25%,  total acc: 62.47%   [EVAL] batch:  203 | acc: 68.75%,  total acc: 62.50%   [EVAL] batch:  204 | acc: 81.25%,  total acc: 62.59%   [EVAL] batch:  205 | acc: 87.50%,  total acc: 62.71%   [EVAL] batch:  206 | acc: 31.25%,  total acc: 62.56%   [EVAL] batch:  207 | acc: 25.00%,  total acc: 62.38%   [EVAL] batch:  208 | acc: 25.00%,  total acc: 62.20%   [EVAL] batch:  209 | acc: 25.00%,  total acc: 62.02%   [EVAL] batch:  210 | acc: 31.25%,  total acc: 61.88%   [EVAL] batch:  211 | acc: 37.50%,  total acc: 61.76%   [EVAL] batch:  212 | acc: 56.25%,  total acc: 61.74%   [EVAL] batch:  213 | acc: 93.75%,  total acc: 61.89%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 62.06%   [EVAL] batch:  215 | acc: 87.50%,  total acc: 62.18%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 62.36%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 62.53%   [EVAL] batch:  218 | acc: 93.75%,  total acc: 62.67%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 62.84%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 63.01%   [EVAL] batch:  221 | acc: 100.00%,  total acc: 63.18%   [EVAL] batch:  222 | acc: 100.00%,  total acc: 63.34%   [EVAL] batch:  223 | acc: 100.00%,  total acc: 63.50%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 63.67%   [EVAL] batch:  225 | acc: 68.75%,  total acc: 63.69%   [EVAL] batch:  226 | acc: 68.75%,  total acc: 63.71%   [EVAL] batch:  227 | acc: 62.50%,  total acc: 63.71%   [EVAL] batch:  228 | acc: 75.00%,  total acc: 63.76%   [EVAL] batch:  229 | acc: 93.75%,  total acc: 63.89%   [EVAL] batch:  230 | acc: 62.50%,  total acc: 63.88%   [EVAL] batch:  231 | acc: 68.75%,  total acc: 63.90%   [EVAL] batch:  232 | acc: 37.50%,  total acc: 63.79%   [EVAL] batch:  233 | acc: 37.50%,  total acc: 63.68%   [EVAL] batch:  234 | acc: 25.00%,  total acc: 63.51%   [EVAL] batch:  235 | acc: 50.00%,  total acc: 63.45%   [EVAL] batch:  236 | acc: 31.25%,  total acc: 63.32%   [EVAL] batch:  237 | acc: 62.50%,  total acc: 63.31%   [EVAL] batch:  238 | acc: 81.25%,  total acc: 63.39%   [EVAL] batch:  239 | acc: 100.00%,  total acc: 63.54%   [EVAL] batch:  240 | acc: 81.25%,  total acc: 63.62%   [EVAL] batch:  241 | acc: 87.50%,  total acc: 63.71%   [EVAL] batch:  242 | acc: 93.75%,  total acc: 63.84%   [EVAL] batch:  243 | acc: 81.25%,  total acc: 63.91%   [EVAL] batch:  244 | acc: 25.00%,  total acc: 63.75%   [EVAL] batch:  245 | acc: 18.75%,  total acc: 63.57%   [EVAL] batch:  246 | acc: 6.25%,  total acc: 63.34%   [EVAL] batch:  247 | acc: 50.00%,  total acc: 63.28%   [EVAL] batch:  248 | acc: 25.00%,  total acc: 63.13%   [EVAL] batch:  249 | acc: 43.75%,  total acc: 63.05%   [EVAL] batch:  250 | acc: 93.75%,  total acc: 63.17%   [EVAL] batch:  251 | acc: 93.75%,  total acc: 63.29%   [EVAL] batch:  252 | acc: 87.50%,  total acc: 63.39%   [EVAL] batch:  253 | acc: 81.25%,  total acc: 63.46%   [EVAL] batch:  254 | acc: 93.75%,  total acc: 63.58%   [EVAL] batch:  255 | acc: 100.00%,  total acc: 63.72%   [EVAL] batch:  256 | acc: 56.25%,  total acc: 63.69%   [EVAL] batch:  257 | acc: 75.00%,  total acc: 63.74%   [EVAL] batch:  258 | acc: 56.25%,  total acc: 63.71%   [EVAL] batch:  259 | acc: 87.50%,  total acc: 63.80%   [EVAL] batch:  260 | acc: 62.50%,  total acc: 63.79%   [EVAL] batch:  261 | acc: 62.50%,  total acc: 63.79%   [EVAL] batch:  262 | acc: 62.50%,  total acc: 63.78%   [EVAL] batch:  263 | acc: 81.25%,  total acc: 63.85%   [EVAL] batch:  264 | acc: 75.00%,  total acc: 63.89%   [EVAL] batch:  265 | acc: 75.00%,  total acc: 63.93%   [EVAL] batch:  266 | acc: 62.50%,  total acc: 63.93%   [EVAL] batch:  267 | acc: 62.50%,  total acc: 63.92%   [EVAL] batch:  268 | acc: 62.50%,  total acc: 63.92%   [EVAL] batch:  269 | acc: 18.75%,  total acc: 63.75%   [EVAL] batch:  270 | acc: 12.50%,  total acc: 63.56%   [EVAL] batch:  271 | acc: 18.75%,  total acc: 63.40%   [EVAL] batch:  272 | acc: 0.00%,  total acc: 63.16%   [EVAL] batch:  273 | acc: 25.00%,  total acc: 63.02%   [EVAL] batch:  274 | acc: 12.50%,  total acc: 62.84%   [EVAL] batch:  275 | acc: 100.00%,  total acc: 62.98%   [EVAL] batch:  276 | acc: 100.00%,  total acc: 63.11%   [EVAL] batch:  277 | acc: 100.00%,  total acc: 63.24%   [EVAL] batch:  278 | acc: 100.00%,  total acc: 63.37%   [EVAL] batch:  279 | acc: 100.00%,  total acc: 63.50%   [EVAL] batch:  280 | acc: 100.00%,  total acc: 63.63%   [EVAL] batch:  281 | acc: 75.00%,  total acc: 63.67%   [EVAL] batch:  282 | acc: 81.25%,  total acc: 63.74%   [EVAL] batch:  283 | acc: 62.50%,  total acc: 63.73%   [EVAL] batch:  284 | acc: 62.50%,  total acc: 63.73%   [EVAL] batch:  285 | acc: 62.50%,  total acc: 63.72%   [EVAL] batch:  286 | acc: 62.50%,  total acc: 63.72%   [EVAL] batch:  287 | acc: 68.75%,  total acc: 63.74%   [EVAL] batch:  288 | acc: 100.00%,  total acc: 63.86%   [EVAL] batch:  289 | acc: 100.00%,  total acc: 63.99%   [EVAL] batch:  290 | acc: 100.00%,  total acc: 64.11%   [EVAL] batch:  291 | acc: 100.00%,  total acc: 64.23%   [EVAL] batch:  292 | acc: 100.00%,  total acc: 64.36%   [EVAL] batch:  293 | acc: 93.75%,  total acc: 64.46%   [EVAL] batch:  294 | acc: 100.00%,  total acc: 64.58%   [EVAL] batch:  295 | acc: 100.00%,  total acc: 64.70%   [EVAL] batch:  296 | acc: 100.00%,  total acc: 64.81%   [EVAL] batch:  297 | acc: 100.00%,  total acc: 64.93%   [EVAL] batch:  298 | acc: 100.00%,  total acc: 65.05%   [EVAL] batch:  299 | acc: 93.75%,  total acc: 65.15%   [EVAL] batch:  300 | acc: 93.75%,  total acc: 65.24%   [EVAL] batch:  301 | acc: 93.75%,  total acc: 65.34%   [EVAL] batch:  302 | acc: 93.75%,  total acc: 65.43%   [EVAL] batch:  303 | acc: 93.75%,  total acc: 65.52%   [EVAL] batch:  304 | acc: 87.50%,  total acc: 65.59%   [EVAL] batch:  305 | acc: 93.75%,  total acc: 65.69%   [EVAL] batch:  306 | acc: 93.75%,  total acc: 65.78%   [EVAL] batch:  307 | acc: 87.50%,  total acc: 65.85%   [EVAL] batch:  308 | acc: 93.75%,  total acc: 65.94%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 66.05%   [EVAL] batch:  310 | acc: 100.00%,  total acc: 66.16%   [EVAL] batch:  311 | acc: 93.75%,  total acc: 66.25%   [EVAL] batch:  312 | acc: 93.75%,  total acc: 66.33%   [EVAL] batch:  313 | acc: 81.25%,  total acc: 66.38%   [EVAL] batch:  314 | acc: 75.00%,  total acc: 66.41%   [EVAL] batch:  315 | acc: 62.50%,  total acc: 66.40%   [EVAL] batch:  316 | acc: 68.75%,  total acc: 66.40%   [EVAL] batch:  317 | acc: 100.00%,  total acc: 66.51%   [EVAL] batch:  318 | acc: 87.50%,  total acc: 66.58%   [EVAL] batch:  319 | acc: 100.00%,  total acc: 66.68%   [EVAL] batch:  320 | acc: 100.00%,  total acc: 66.78%   [EVAL] batch:  321 | acc: 75.00%,  total acc: 66.81%   [EVAL] batch:  322 | acc: 100.00%,  total acc: 66.91%   [EVAL] batch:  323 | acc: 93.75%,  total acc: 66.99%   [EVAL] batch:  324 | acc: 75.00%,  total acc: 67.02%   [EVAL] batch:  325 | acc: 56.25%,  total acc: 66.99%   [EVAL] batch:  326 | acc: 56.25%,  total acc: 66.95%   [EVAL] batch:  327 | acc: 68.75%,  total acc: 66.96%   [EVAL] batch:  328 | acc: 75.00%,  total acc: 66.98%   [EVAL] batch:  329 | acc: 56.25%,  total acc: 66.95%   [EVAL] batch:  330 | acc: 62.50%,  total acc: 66.94%   [EVAL] batch:  331 | acc: 68.75%,  total acc: 66.94%   [EVAL] batch:  332 | acc: 56.25%,  total acc: 66.91%   [EVAL] batch:  333 | acc: 81.25%,  total acc: 66.95%   [EVAL] batch:  334 | acc: 56.25%,  total acc: 66.92%   [EVAL] batch:  335 | acc: 75.00%,  total acc: 66.95%   [EVAL] batch:  336 | acc: 43.75%,  total acc: 66.88%   [EVAL] batch:  337 | acc: 81.25%,  total acc: 66.92%   [EVAL] batch:  338 | acc: 37.50%,  total acc: 66.83%   [EVAL] batch:  339 | acc: 56.25%,  total acc: 66.80%   [EVAL] batch:  340 | acc: 43.75%,  total acc: 66.73%   [EVAL] batch:  341 | acc: 56.25%,  total acc: 66.70%   [EVAL] batch:  342 | acc: 43.75%,  total acc: 66.64%   [EVAL] batch:  343 | acc: 31.25%,  total acc: 66.53%   [EVAL] batch:  344 | acc: 68.75%,  total acc: 66.54%   [EVAL] batch:  345 | acc: 31.25%,  total acc: 66.44%   [EVAL] batch:  346 | acc: 50.00%,  total acc: 66.39%   [EVAL] batch:  347 | acc: 62.50%,  total acc: 66.38%   [EVAL] batch:  348 | acc: 75.00%,  total acc: 66.40%   [EVAL] batch:  349 | acc: 43.75%,  total acc: 66.34%   [EVAL] batch:  350 | acc: 75.00%,  total acc: 66.36%   [EVAL] batch:  351 | acc: 81.25%,  total acc: 66.41%   [EVAL] batch:  352 | acc: 87.50%,  total acc: 66.47%   [EVAL] batch:  353 | acc: 81.25%,  total acc: 66.51%   [EVAL] batch:  354 | acc: 75.00%,  total acc: 66.53%   [EVAL] batch:  355 | acc: 81.25%,  total acc: 66.57%   [EVAL] batch:  356 | acc: 56.25%,  total acc: 66.54%   [EVAL] batch:  357 | acc: 25.00%,  total acc: 66.43%   [EVAL] batch:  358 | acc: 37.50%,  total acc: 66.35%   [EVAL] batch:  359 | acc: 43.75%,  total acc: 66.28%   [EVAL] batch:  360 | acc: 37.50%,  total acc: 66.20%   [EVAL] batch:  361 | acc: 50.00%,  total acc: 66.16%   [EVAL] batch:  362 | acc: 62.50%,  total acc: 66.15%   [EVAL] batch:  363 | acc: 100.00%,  total acc: 66.24%   [EVAL] batch:  364 | acc: 100.00%,  total acc: 66.34%   [EVAL] batch:  365 | acc: 100.00%,  total acc: 66.43%   [EVAL] batch:  366 | acc: 100.00%,  total acc: 66.52%   [EVAL] batch:  367 | acc: 100.00%,  total acc: 66.61%   [EVAL] batch:  368 | acc: 100.00%,  total acc: 66.70%   [EVAL] batch:  369 | acc: 100.00%,  total acc: 66.79%   [EVAL] batch:  370 | acc: 87.50%,  total acc: 66.85%   [EVAL] batch:  371 | acc: 87.50%,  total acc: 66.90%   [EVAL] batch:  372 | acc: 100.00%,  total acc: 66.99%   [EVAL] batch:  373 | acc: 93.75%,  total acc: 67.06%   [EVAL] batch:  374 | acc: 75.00%,  total acc: 67.08%   [EVAL] batch:  375 | acc: 31.25%,  total acc: 66.99%   [EVAL] batch:  376 | acc: 37.50%,  total acc: 66.91%   [EVAL] batch:  377 | acc: 37.50%,  total acc: 66.83%   [EVAL] batch:  378 | acc: 37.50%,  total acc: 66.75%   [EVAL] batch:  379 | acc: 62.50%,  total acc: 66.74%   [EVAL] batch:  380 | acc: 43.75%,  total acc: 66.68%   [EVAL] batch:  381 | acc: 62.50%,  total acc: 66.67%   [EVAL] batch:  382 | acc: 62.50%,  total acc: 66.66%   [EVAL] batch:  383 | acc: 75.00%,  total acc: 66.68%   [EVAL] batch:  384 | acc: 56.25%,  total acc: 66.66%   [EVAL] batch:  385 | acc: 43.75%,  total acc: 66.60%   [EVAL] batch:  386 | acc: 68.75%,  total acc: 66.60%   [EVAL] batch:  387 | acc: 75.00%,  total acc: 66.62%   [EVAL] batch:  388 | acc: 56.25%,  total acc: 66.60%   [EVAL] batch:  389 | acc: 62.50%,  total acc: 66.59%   [EVAL] batch:  390 | acc: 68.75%,  total acc: 66.59%   [EVAL] batch:  391 | acc: 62.50%,  total acc: 66.58%   [EVAL] batch:  392 | acc: 75.00%,  total acc: 66.60%   [EVAL] batch:  393 | acc: 93.75%,  total acc: 66.67%   [EVAL] batch:  394 | acc: 93.75%,  total acc: 66.74%   [EVAL] batch:  395 | acc: 100.00%,  total acc: 66.82%   [EVAL] batch:  396 | acc: 100.00%,  total acc: 66.91%   [EVAL] batch:  397 | acc: 100.00%,  total acc: 66.99%   [EVAL] batch:  398 | acc: 93.75%,  total acc: 67.06%   [EVAL] batch:  399 | acc: 93.75%,  total acc: 67.12%   [EVAL] batch:  400 | acc: 68.75%,  total acc: 67.13%   [EVAL] batch:  401 | acc: 50.00%,  total acc: 67.09%   [EVAL] batch:  402 | acc: 68.75%,  total acc: 67.09%   [EVAL] batch:  403 | acc: 43.75%,  total acc: 67.03%   [EVAL] batch:  404 | acc: 81.25%,  total acc: 67.07%   [EVAL] batch:  405 | acc: 75.00%,  total acc: 67.09%   [EVAL] batch:  406 | acc: 75.00%,  total acc: 67.11%   [EVAL] batch:  407 | acc: 62.50%,  total acc: 67.10%   [EVAL] batch:  408 | acc: 37.50%,  total acc: 67.02%   [EVAL] batch:  409 | acc: 43.75%,  total acc: 66.97%   [EVAL] batch:  410 | acc: 18.75%,  total acc: 66.85%   [EVAL] batch:  411 | acc: 43.75%,  total acc: 66.79%   [EVAL] batch:  412 | acc: 43.75%,  total acc: 66.74%   [EVAL] batch:  413 | acc: 12.50%,  total acc: 66.61%   [EVAL] batch:  414 | acc: 12.50%,  total acc: 66.48%   [EVAL] batch:  415 | acc: 0.00%,  total acc: 66.32%   [EVAL] batch:  416 | acc: 18.75%,  total acc: 66.20%   [EVAL] batch:  417 | acc: 18.75%,  total acc: 66.09%   [EVAL] batch:  418 | acc: 37.50%,  total acc: 66.02%   [EVAL] batch:  419 | acc: 62.50%,  total acc: 66.01%   [EVAL] batch:  420 | acc: 68.75%,  total acc: 66.02%   [EVAL] batch:  421 | acc: 50.00%,  total acc: 65.98%   [EVAL] batch:  422 | acc: 50.00%,  total acc: 65.94%   [EVAL] batch:  423 | acc: 56.25%,  total acc: 65.92%   [EVAL] batch:  424 | acc: 81.25%,  total acc: 65.96%   [EVAL] batch:  425 | acc: 100.00%,  total acc: 66.04%   [EVAL] batch:  426 | acc: 100.00%,  total acc: 66.12%   [EVAL] batch:  427 | acc: 100.00%,  total acc: 66.19%   [EVAL] batch:  428 | acc: 100.00%,  total acc: 66.27%   [EVAL] batch:  429 | acc: 100.00%,  total acc: 66.35%   [EVAL] batch:  430 | acc: 100.00%,  total acc: 66.43%   [EVAL] batch:  431 | acc: 100.00%,  total acc: 66.51%   [EVAL] batch:  432 | acc: 87.50%,  total acc: 66.56%   [EVAL] batch:  433 | acc: 93.75%,  total acc: 66.62%   [EVAL] batch:  434 | acc: 100.00%,  total acc: 66.70%   [EVAL] batch:  435 | acc: 93.75%,  total acc: 66.76%   [EVAL] batch:  436 | acc: 100.00%,  total acc: 66.83%   [EVAL] batch:  437 | acc: 56.25%,  total acc: 66.81%   [EVAL] batch:  438 | acc: 12.50%,  total acc: 66.69%   [EVAL] batch:  439 | acc: 31.25%,  total acc: 66.61%   [EVAL] batch:  440 | acc: 25.00%,  total acc: 66.51%   [EVAL] batch:  441 | acc: 18.75%,  total acc: 66.40%   [EVAL] batch:  442 | acc: 12.50%,  total acc: 66.28%   [EVAL] batch:  443 | acc: 43.75%,  total acc: 66.23%   [EVAL] batch:  444 | acc: 81.25%,  total acc: 66.26%   [EVAL] batch:  445 | acc: 81.25%,  total acc: 66.30%   [EVAL] batch:  446 | acc: 87.50%,  total acc: 66.35%   [EVAL] batch:  447 | acc: 87.50%,  total acc: 66.39%   [EVAL] batch:  448 | acc: 75.00%,  total acc: 66.41%   [EVAL] batch:  449 | acc: 75.00%,  total acc: 66.43%   [EVAL] batch:  450 | acc: 87.50%,  total acc: 66.48%   [EVAL] batch:  451 | acc: 81.25%,  total acc: 66.51%   [EVAL] batch:  452 | acc: 93.75%,  total acc: 66.57%   [EVAL] batch:  453 | acc: 87.50%,  total acc: 66.62%   [EVAL] batch:  454 | acc: 87.50%,  total acc: 66.66%   [EVAL] batch:  455 | acc: 93.75%,  total acc: 66.72%   [EVAL] batch:  456 | acc: 37.50%,  total acc: 66.66%   [EVAL] batch:  457 | acc: 50.00%,  total acc: 66.62%   [EVAL] batch:  458 | acc: 43.75%,  total acc: 66.57%   [EVAL] batch:  459 | acc: 43.75%,  total acc: 66.52%   [EVAL] batch:  460 | acc: 31.25%,  total acc: 66.45%   [EVAL] batch:  461 | acc: 31.25%,  total acc: 66.37%   [EVAL] batch:  462 | acc: 31.25%,  total acc: 66.29%   [EVAL] batch:  463 | acc: 0.00%,  total acc: 66.15%   [EVAL] batch:  464 | acc: 18.75%,  total acc: 66.05%   [EVAL] batch:  465 | acc: 0.00%,  total acc: 65.91%   [EVAL] batch:  466 | acc: 18.75%,  total acc: 65.81%   [EVAL] batch:  467 | acc: 6.25%,  total acc: 65.68%   [EVAL] batch:  468 | acc: 12.50%,  total acc: 65.57%   [EVAL] batch:  469 | acc: 87.50%,  total acc: 65.61%   [EVAL] batch:  470 | acc: 100.00%,  total acc: 65.68%   [EVAL] batch:  471 | acc: 93.75%,  total acc: 65.74%   [EVAL] batch:  472 | acc: 100.00%,  total acc: 65.82%   [EVAL] batch:  473 | acc: 87.50%,  total acc: 65.86%   [EVAL] batch:  474 | acc: 75.00%,  total acc: 65.88%   [EVAL] batch:  475 | acc: 87.50%,  total acc: 65.93%   [EVAL] batch:  476 | acc: 93.75%,  total acc: 65.99%   [EVAL] batch:  477 | acc: 93.75%,  total acc: 66.04%   [EVAL] batch:  478 | acc: 87.50%,  total acc: 66.09%   [EVAL] batch:  479 | acc: 87.50%,  total acc: 66.13%   [EVAL] batch:  480 | acc: 100.00%,  total acc: 66.20%   [EVAL] batch:  481 | acc: 68.75%,  total acc: 66.21%   [EVAL] batch:  482 | acc: 81.25%,  total acc: 66.24%   [EVAL] batch:  483 | acc: 56.25%,  total acc: 66.22%   [EVAL] batch:  484 | acc: 75.00%,  total acc: 66.24%   [EVAL] batch:  485 | acc: 68.75%,  total acc: 66.24%   [EVAL] batch:  486 | acc: 81.25%,  total acc: 66.27%   [EVAL] batch:  487 | acc: 43.75%,  total acc: 66.23%   [EVAL] batch:  488 | acc: 37.50%,  total acc: 66.17%   [EVAL] batch:  489 | acc: 50.00%,  total acc: 66.14%   [EVAL] batch:  490 | acc: 68.75%,  total acc: 66.14%   [EVAL] batch:  491 | acc: 43.75%,  total acc: 66.10%   [EVAL] batch:  492 | acc: 56.25%,  total acc: 66.08%   [EVAL] batch:  493 | acc: 56.25%,  total acc: 66.06%   [EVAL] batch:  494 | acc: 50.00%,  total acc: 66.02%   [EVAL] batch:  495 | acc: 50.00%,  total acc: 65.99%   [EVAL] batch:  496 | acc: 50.00%,  total acc: 65.96%   [EVAL] batch:  497 | acc: 50.00%,  total acc: 65.93%   [EVAL] batch:  498 | acc: 81.25%,  total acc: 65.96%   [EVAL] batch:  499 | acc: 62.50%,  total acc: 65.95%   
cur_acc:  ['0.9484', '0.7232', '0.7421', '0.6736', '0.8284', '0.7450', '0.6835', '0.5903']
his_acc:  ['0.9484', '0.8190', '0.7713', '0.7235', '0.7330', '0.7190', '0.7015', '0.6595']
--------Round  4
seed:  500
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hybridprompt_12token/train.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hybridprompt_12token/valid.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hybridprompt_12token/test.pkl
Task_order: [7 5 6 4 2 1 3 0]
prepared data!
Clustering into  4  clusters
Clusters:  [0 3 2 0 1 1 2 0 3 3]
Losses:  18.148780822753906 3.9411065578460693 1.2505851984024048
CurrentTrain: epoch  0, batch     0 | loss: 18.1487808Losses:  21.345245361328125 7.495120525360107 0.9757005572319031
CurrentTrain: epoch  0, batch     1 | loss: 21.3452454Losses:  21.612842559814453 8.26106071472168 1.110926866531372
CurrentTrain: epoch  0, batch     2 | loss: 21.6128426Losses:  17.260704040527344 3.9096031188964844 1.0666744709014893
CurrentTrain: epoch  0, batch     3 | loss: 17.2607040Losses:  17.773040771484375 4.727388381958008 1.0090636014938354
CurrentTrain: epoch  0, batch     4 | loss: 17.7730408Losses:  19.417882919311523 6.782268047332764 1.0073397159576416
CurrentTrain: epoch  0, batch     5 | loss: 19.4178829Losses:  16.855119705200195 4.547698497772217 0.9252574443817139
CurrentTrain: epoch  0, batch     6 | loss: 16.8551197Losses:  16.560970306396484 4.226375102996826 0.9751157760620117
CurrentTrain: epoch  0, batch     7 | loss: 16.5609703Losses:  16.123764038085938 3.7495651245117188 0.931338906288147
CurrentTrain: epoch  0, batch     8 | loss: 16.1237640Losses:  18.064491271972656 5.581602096557617 0.8452430367469788
CurrentTrain: epoch  0, batch     9 | loss: 18.0644913Losses:  17.589752197265625 5.5578413009643555 0.796843945980072
CurrentTrain: epoch  0, batch    10 | loss: 17.5897522Losses:  17.758440017700195 6.257946968078613 0.7557231187820435
CurrentTrain: epoch  0, batch    11 | loss: 17.7584400Losses:  15.721394538879395 4.310871124267578 0.752562403678894
CurrentTrain: epoch  0, batch    12 | loss: 15.7213945Losses:  15.123254776000977 3.488055944442749 0.8113051056861877
CurrentTrain: epoch  0, batch    13 | loss: 15.1232548Losses:  14.795462608337402 3.12016224861145 0.7493135929107666
CurrentTrain: epoch  0, batch    14 | loss: 14.7954626Losses:  15.090951919555664 3.944680690765381 0.7261029481887817
CurrentTrain: epoch  0, batch    15 | loss: 15.0909519Losses:  14.484220504760742 3.400512933731079 0.6513442993164062
CurrentTrain: epoch  0, batch    16 | loss: 14.4842205Losses:  16.39728546142578 5.446824073791504 0.620740532875061
CurrentTrain: epoch  0, batch    17 | loss: 16.3972855Losses:  16.006982803344727 5.259889602661133 0.633472204208374
CurrentTrain: epoch  0, batch    18 | loss: 16.0069828Losses:  14.836470603942871 3.9697012901306152 0.6611167192459106
CurrentTrain: epoch  0, batch    19 | loss: 14.8364706Losses:  15.155993461608887 4.465081214904785 0.6070939898490906
CurrentTrain: epoch  0, batch    20 | loss: 15.1559935Losses:  14.645157814025879 3.6044249534606934 0.6606646180152893
CurrentTrain: epoch  0, batch    21 | loss: 14.6451578Losses:  14.682689666748047 3.9048166275024414 0.5806326866149902
CurrentTrain: epoch  0, batch    22 | loss: 14.6826897Losses:  17.584199905395508 6.96322774887085 0.38442462682724
CurrentTrain: epoch  0, batch    23 | loss: 17.5841999Losses:  17.44930648803711 6.509112358093262 0.5312185883522034
CurrentTrain: epoch  0, batch    24 | loss: 17.4493065Losses:  15.59814453125 4.894672870635986 0.590246319770813
CurrentTrain: epoch  0, batch    25 | loss: 15.5981445Losses:  14.55588150024414 3.862781047821045 0.5287455320358276
CurrentTrain: epoch  0, batch    26 | loss: 14.5558815Losses:  16.616493225097656 6.372990131378174 0.4536641240119934
CurrentTrain: epoch  0, batch    27 | loss: 16.6164932Losses:  20.212900161743164 9.365216255187988 0.5144320726394653
CurrentTrain: epoch  0, batch    28 | loss: 20.2129002Losses:  15.71300983428955 4.958582878112793 0.5242093801498413
CurrentTrain: epoch  0, batch    29 | loss: 15.7130098Losses:  13.640995979309082 3.151463031768799 0.5374536514282227
CurrentTrain: epoch  0, batch    30 | loss: 13.6409960Losses:  12.34365463256836 2.4531397819519043 0.5208563804626465
CurrentTrain: epoch  0, batch    31 | loss: 12.3436546Losses:  15.749462127685547 5.506345748901367 0.46650320291519165
CurrentTrain: epoch  0, batch    32 | loss: 15.7494621Losses:  14.41102123260498 4.746335983276367 0.5005347728729248
CurrentTrain: epoch  0, batch    33 | loss: 14.4110212Losses:  17.075483322143555 7.8408403396606445 0.4389011263847351
CurrentTrain: epoch  0, batch    34 | loss: 17.0754833Losses:  14.637845039367676 4.806238174438477 0.457281231880188
CurrentTrain: epoch  0, batch    35 | loss: 14.6378450Losses:  13.314723014831543 3.173128128051758 0.5164209604263306
CurrentTrain: epoch  0, batch    36 | loss: 13.3147230Losses:  13.610820770263672 3.885681629180908 0.4776234030723572
CurrentTrain: epoch  0, batch    37 | loss: 13.6108208Losses:  13.721179962158203 4.577592849731445 0.4494265913963318
CurrentTrain: epoch  0, batch    38 | loss: 13.7211800Losses:  16.26926040649414 6.918022155761719 0.46684369444847107
CurrentTrain: epoch  0, batch    39 | loss: 16.2692604Losses:  16.692167282104492 7.663994789123535 0.4294542074203491
CurrentTrain: epoch  0, batch    40 | loss: 16.6921673Losses:  12.110491752624512 2.80499267578125 0.4663645625114441
CurrentTrain: epoch  0, batch    41 | loss: 12.1104918Losses:  12.764437675476074 3.8551621437072754 0.45133113861083984
CurrentTrain: epoch  0, batch    42 | loss: 12.7644377Losses:  16.13899040222168 7.068414688110352 0.4234093725681305
CurrentTrain: epoch  0, batch    43 | loss: 16.1389904Losses:  17.25408172607422 6.97286319732666 0.5092952847480774
CurrentTrain: epoch  0, batch    44 | loss: 17.2540817Losses:  15.86001968383789 5.975401401519775 0.5108796954154968
CurrentTrain: epoch  0, batch    45 | loss: 15.8600197Losses:  13.114394187927246 3.457766532897949 0.48607349395751953
CurrentTrain: epoch  0, batch    46 | loss: 13.1143942Losses:  14.033480644226074 5.026074409484863 0.4326331615447998
CurrentTrain: epoch  0, batch    47 | loss: 14.0334806Losses:  15.93609619140625 6.495295524597168 0.32635408639907837
CurrentTrain: epoch  0, batch    48 | loss: 15.9360962Losses:  11.708182334899902 2.5822038650512695 0.43996283411979675
CurrentTrain: epoch  0, batch    49 | loss: 11.7081823Losses:  13.9309720993042 5.043087005615234 0.4503815174102783
CurrentTrain: epoch  0, batch    50 | loss: 13.9309721Losses:  13.316793441772461 3.970472812652588 0.39275163412094116
CurrentTrain: epoch  0, batch    51 | loss: 13.3167934Losses:  11.832101821899414 3.1208157539367676 0.4362415671348572
CurrentTrain: epoch  0, batch    52 | loss: 11.8321018Losses:  10.863718032836914 2.5801405906677246 0.4055277109146118
CurrentTrain: epoch  0, batch    53 | loss: 10.8637180Losses:  11.060745239257812 2.7190394401550293 0.4213848114013672
CurrentTrain: epoch  0, batch    54 | loss: 11.0607452Losses:  12.872164726257324 4.370482444763184 0.39063146710395813
CurrentTrain: epoch  0, batch    55 | loss: 12.8721647Losses:  13.224691390991211 4.862313747406006 0.40425124764442444
CurrentTrain: epoch  0, batch    56 | loss: 13.2246914Losses:  12.318077087402344 3.541045665740967 0.4248366951942444
CurrentTrain: epoch  0, batch    57 | loss: 12.3180771Losses:  15.994351387023926 6.933896541595459 0.25903618335723877
CurrentTrain: epoch  0, batch    58 | loss: 15.9943514Losses:  12.218180656433105 4.242748260498047 0.3770645260810852
CurrentTrain: epoch  0, batch    59 | loss: 12.2181807Losses:  12.49508285522461 3.659783363342285 0.4155053496360779
CurrentTrain: epoch  0, batch    60 | loss: 12.4950829Losses:  13.309378623962402 5.340594291687012 0.37018388509750366
CurrentTrain: epoch  0, batch    61 | loss: 13.3093786Losses:  9.540729522705078 0.9217749834060669 0.4026361107826233
CurrentTrain: epoch  0, batch    62 | loss: 9.5407295Losses:  11.759626388549805 3.3485352993011475 0.3660222291946411
CurrentTrain: epoch  1, batch     0 | loss: 11.7596264Losses:  11.172066688537598 3.2620887756347656 0.3765352666378021
CurrentTrain: epoch  1, batch     1 | loss: 11.1720667Losses:  10.8563871383667 3.4385826587677 0.36274629831314087
CurrentTrain: epoch  1, batch     2 | loss: 10.8563871Losses:  12.028730392456055 3.904566764831543 0.38702392578125
CurrentTrain: epoch  1, batch     3 | loss: 12.0287304Losses:  11.516708374023438 3.4653124809265137 0.37144845724105835
CurrentTrain: epoch  1, batch     4 | loss: 11.5167084Losses:  11.21966552734375 3.569854259490967 0.3581107258796692
CurrentTrain: epoch  1, batch     5 | loss: 11.2196655Losses:  9.500783920288086 2.3500452041625977 0.34773629903793335
CurrentTrain: epoch  1, batch     6 | loss: 9.5007839Losses:  10.673507690429688 2.9878416061401367 0.3443519175052643
CurrentTrain: epoch  1, batch     7 | loss: 10.6735077Losses:  11.264213562011719 3.5222413539886475 0.3759758472442627
CurrentTrain: epoch  1, batch     8 | loss: 11.2642136Losses:  13.447367668151855 5.508809566497803 0.36387500166893005
CurrentTrain: epoch  1, batch     9 | loss: 13.4473677Losses:  12.289945602416992 3.3860137462615967 0.38057976961135864
CurrentTrain: epoch  1, batch    10 | loss: 12.2899456Losses:  12.865119934082031 4.256034851074219 0.3673928380012512
CurrentTrain: epoch  1, batch    11 | loss: 12.8651199Losses:  17.617294311523438 10.117445945739746 0.33592742681503296
CurrentTrain: epoch  1, batch    12 | loss: 17.6172943Losses:  10.628893852233887 3.5668370723724365 0.3539643883705139
CurrentTrain: epoch  1, batch    13 | loss: 10.6288939Losses:  13.116106986999512 5.595590591430664 0.3868163526058197
CurrentTrain: epoch  1, batch    14 | loss: 13.1161070Losses:  10.351232528686523 3.160135507583618 0.3456644117832184
CurrentTrain: epoch  1, batch    15 | loss: 10.3512325Losses:  11.52724552154541 3.8372750282287598 0.36045703291893005
CurrentTrain: epoch  1, batch    16 | loss: 11.5272455Losses:  10.378104209899902 3.5808615684509277 0.32022789120674133
CurrentTrain: epoch  1, batch    17 | loss: 10.3781042Losses:  9.73276424407959 2.6791138648986816 0.33866608142852783
CurrentTrain: epoch  1, batch    18 | loss: 9.7327642Losses:  12.087543487548828 4.040981292724609 0.35064011812210083
CurrentTrain: epoch  1, batch    19 | loss: 12.0875435Losses:  13.254958152770996 5.156103134155273 0.2876289486885071
CurrentTrain: epoch  1, batch    20 | loss: 13.2549582Losses:  12.278952598571777 4.704188823699951 0.3215671479701996
CurrentTrain: epoch  1, batch    21 | loss: 12.2789526Losses:  13.071582794189453 5.18369197845459 0.3430165648460388
CurrentTrain: epoch  1, batch    22 | loss: 13.0715828Losses:  9.826180458068848 2.932739734649658 0.3314712643623352
CurrentTrain: epoch  1, batch    23 | loss: 9.8261805Losses:  11.667925834655762 4.532514572143555 0.32562217116355896
CurrentTrain: epoch  1, batch    24 | loss: 11.6679258Losses:  11.805295944213867 4.4324798583984375 0.328075110912323
CurrentTrain: epoch  1, batch    25 | loss: 11.8052959Losses:  11.82551097869873 4.8535003662109375 0.3384523391723633
CurrentTrain: epoch  1, batch    26 | loss: 11.8255110Losses:  13.736059188842773 5.583988189697266 0.3353564143180847
CurrentTrain: epoch  1, batch    27 | loss: 13.7360592Losses:  11.46711540222168 4.749623775482178 0.235108882188797
CurrentTrain: epoch  1, batch    28 | loss: 11.4671154Losses:  12.295584678649902 5.489249229431152 0.3391605019569397
CurrentTrain: epoch  1, batch    29 | loss: 12.2955847Losses:  10.409287452697754 2.7501492500305176 0.3358278274536133
CurrentTrain: epoch  1, batch    30 | loss: 10.4092875Losses:  11.330392837524414 4.342053413391113 0.3442626893520355
CurrentTrain: epoch  1, batch    31 | loss: 11.3303928Losses:  10.240636825561523 3.1448187828063965 0.3333946168422699
CurrentTrain: epoch  1, batch    32 | loss: 10.2406368Losses:  10.148092269897461 2.693166971206665 0.33366328477859497
CurrentTrain: epoch  1, batch    33 | loss: 10.1480923Losses:  9.879291534423828 2.9296116828918457 0.3409937620162964
CurrentTrain: epoch  1, batch    34 | loss: 9.8792915Losses:  11.137605667114258 4.151293754577637 0.36305510997772217
CurrentTrain: epoch  1, batch    35 | loss: 11.1376057Losses:  15.256019592285156 7.170035362243652 0.2837408185005188
CurrentTrain: epoch  1, batch    36 | loss: 15.2560196Losses:  11.360032081604004 4.938533782958984 0.328906387090683
CurrentTrain: epoch  1, batch    37 | loss: 11.3600321Losses:  10.254197120666504 3.3219141960144043 0.33215203881263733
CurrentTrain: epoch  1, batch    38 | loss: 10.2541971Losses:  9.26501750946045 2.4698128700256348 0.3441251516342163
CurrentTrain: epoch  1, batch    39 | loss: 9.2650175Losses:  9.059046745300293 2.1624560356140137 0.3400769531726837
CurrentTrain: epoch  1, batch    40 | loss: 9.0590467Losses:  8.813396453857422 2.2755014896392822 0.334587037563324
CurrentTrain: epoch  1, batch    41 | loss: 8.8133965Losses:  14.358113288879395 6.4383015632629395 0.31764543056488037
CurrentTrain: epoch  1, batch    42 | loss: 14.3581133Losses:  9.782286643981934 3.2674570083618164 0.3508402705192566
CurrentTrain: epoch  1, batch    43 | loss: 9.7822866Losses:  11.077191352844238 3.424710750579834 0.3568832576274872
CurrentTrain: epoch  1, batch    44 | loss: 11.0771914Losses:  13.548938751220703 6.06563138961792 0.3109646141529083
CurrentTrain: epoch  1, batch    45 | loss: 13.5489388Losses:  9.582479476928711 2.5560455322265625 0.32825034856796265
CurrentTrain: epoch  1, batch    46 | loss: 9.5824795Losses:  11.027549743652344 4.1451416015625 0.31032150983810425
CurrentTrain: epoch  1, batch    47 | loss: 11.0275497Losses:  13.95682430267334 7.054939270019531 0.32499250769615173
CurrentTrain: epoch  1, batch    48 | loss: 13.9568243Losses:  9.391016960144043 2.63039493560791 0.30785059928894043
CurrentTrain: epoch  1, batch    49 | loss: 9.3910170Losses:  10.20910358428955 3.1479244232177734 0.3429100513458252
CurrentTrain: epoch  1, batch    50 | loss: 10.2091036Losses:  10.793573379516602 3.7423880100250244 0.3074533939361572
CurrentTrain: epoch  1, batch    51 | loss: 10.7935734Losses:  12.207399368286133 5.623604774475098 0.3110736012458801
CurrentTrain: epoch  1, batch    52 | loss: 12.2073994Losses:  11.41745376586914 4.414955139160156 0.3432542085647583
CurrentTrain: epoch  1, batch    53 | loss: 11.4174538Losses:  9.825719833374023 3.5625853538513184 0.3054155111312866
CurrentTrain: epoch  1, batch    54 | loss: 9.8257198Losses:  16.18927764892578 7.7728142738342285 0.30464810132980347
CurrentTrain: epoch  1, batch    55 | loss: 16.1892776Losses:  13.54134750366211 6.994678974151611 0.3212520480155945
CurrentTrain: epoch  1, batch    56 | loss: 13.5413475Losses:  9.683466911315918 3.215627431869507 0.31745314598083496
CurrentTrain: epoch  1, batch    57 | loss: 9.6834669Losses:  12.571223258972168 5.351957321166992 0.31074059009552
CurrentTrain: epoch  1, batch    58 | loss: 12.5712233Losses:  12.847851753234863 5.070199012756348 0.30903998017311096
CurrentTrain: epoch  1, batch    59 | loss: 12.8478518Losses:  14.0562744140625 7.020149230957031 0.29876142740249634
CurrentTrain: epoch  1, batch    60 | loss: 14.0562744Losses:  9.61975383758545 3.3223280906677246 0.3122395873069763
CurrentTrain: epoch  1, batch    61 | loss: 9.6197538Losses:  7.798500061035156 0.766860842704773 0.32026222348213196
CurrentTrain: epoch  1, batch    62 | loss: 7.7985001Losses:  12.40593147277832 5.313440322875977 0.31393003463745117
CurrentTrain: epoch  2, batch     0 | loss: 12.4059315Losses:  12.907835960388184 5.686553001403809 0.2560844421386719
CurrentTrain: epoch  2, batch     1 | loss: 12.9078360Losses:  10.949888229370117 5.266250133514404 0.28854215145111084
CurrentTrain: epoch  2, batch     2 | loss: 10.9498882Losses:  8.058847427368164 1.7109503746032715 0.2956404983997345
CurrentTrain: epoch  2, batch     3 | loss: 8.0588474Losses:  9.884942054748535 3.3734447956085205 0.28429272770881653
CurrentTrain: epoch  2, batch     4 | loss: 9.8849421Losses:  12.987051010131836 6.515709400177002 0.3223491907119751
CurrentTrain: epoch  2, batch     5 | loss: 12.9870510Losses:  8.8922119140625 2.775866985321045 0.28763067722320557
CurrentTrain: epoch  2, batch     6 | loss: 8.8922119Losses:  12.126664161682129 6.044681072235107 0.29436200857162476
CurrentTrain: epoch  2, batch     7 | loss: 12.1266642Losses:  9.074468612670898 3.536142349243164 0.2891314923763275
CurrentTrain: epoch  2, batch     8 | loss: 9.0744686Losses:  12.59571647644043 5.425561428070068 0.30965983867645264
CurrentTrain: epoch  2, batch     9 | loss: 12.5957165Losses:  8.39920711517334 2.0781898498535156 0.27919286489486694
CurrentTrain: epoch  2, batch    10 | loss: 8.3992071Losses:  8.368752479553223 2.190654754638672 0.2915993928909302
CurrentTrain: epoch  2, batch    11 | loss: 8.3687525Losses:  8.582261085510254 2.557462692260742 0.2938521206378937
CurrentTrain: epoch  2, batch    12 | loss: 8.5822611Losses:  10.341097831726074 3.1528406143188477 0.278124064207077
CurrentTrain: epoch  2, batch    13 | loss: 10.3410978Losses:  15.646017074584961 9.129892349243164 0.3122867941856384
CurrentTrain: epoch  2, batch    14 | loss: 15.6460171Losses:  9.693496704101562 3.56630802154541 0.30395662784576416
CurrentTrain: epoch  2, batch    15 | loss: 9.6934967Losses:  9.527962684631348 3.811941623687744 0.29986876249313354
CurrentTrain: epoch  2, batch    16 | loss: 9.5279627Losses:  8.914170265197754 2.6922595500946045 0.272921621799469
CurrentTrain: epoch  2, batch    17 | loss: 8.9141703Losses:  8.95505142211914 3.073850631713867 0.290294885635376
CurrentTrain: epoch  2, batch    18 | loss: 8.9550514Losses:  9.823902130126953 3.9946818351745605 0.28541994094848633
CurrentTrain: epoch  2, batch    19 | loss: 9.8239021Losses:  10.692599296569824 4.89552116394043 0.28193387389183044
CurrentTrain: epoch  2, batch    20 | loss: 10.6925993Losses:  7.9292378425598145 2.4906749725341797 0.2957621216773987
CurrentTrain: epoch  2, batch    21 | loss: 7.9292378Losses:  12.161270141601562 6.034970283508301 0.21982038021087646
CurrentTrain: epoch  2, batch    22 | loss: 12.1612701Losses:  10.002260208129883 3.433320999145508 0.2953410744667053
CurrentTrain: epoch  2, batch    23 | loss: 10.0022602Losses:  9.964126586914062 4.332475185394287 0.30523836612701416
CurrentTrain: epoch  2, batch    24 | loss: 9.9641266Losses:  11.372979164123535 5.728344917297363 0.2869187891483307
CurrentTrain: epoch  2, batch    25 | loss: 11.3729792Losses:  9.616086959838867 3.7320404052734375 0.29030799865722656
CurrentTrain: epoch  2, batch    26 | loss: 9.6160870Losses:  10.522137641906738 4.646875381469727 0.2811445891857147
CurrentTrain: epoch  2, batch    27 | loss: 10.5221376Losses:  11.069154739379883 5.08107852935791 0.30364298820495605
CurrentTrain: epoch  2, batch    28 | loss: 11.0691547Losses:  8.336554527282715 2.569692373275757 0.3061025142669678
CurrentTrain: epoch  2, batch    29 | loss: 8.3365545Losses:  9.365913391113281 3.5708694458007812 0.2832508087158203
CurrentTrain: epoch  2, batch    30 | loss: 9.3659134Losses:  11.015144348144531 4.4370880126953125 0.30005714297294617
CurrentTrain: epoch  2, batch    31 | loss: 11.0151443Losses:  9.771313667297363 3.0565297603607178 0.305724173784256
CurrentTrain: epoch  2, batch    32 | loss: 9.7713137Losses:  8.7903470993042 2.9873199462890625 0.29853570461273193
CurrentTrain: epoch  2, batch    33 | loss: 8.7903471Losses:  7.790544033050537 2.3587801456451416 0.28336381912231445
CurrentTrain: epoch  2, batch    34 | loss: 7.7905440Losses:  7.708243370056152 2.425337314605713 0.2832273840904236
CurrentTrain: epoch  2, batch    35 | loss: 7.7082434Losses:  10.929952621459961 4.736449241638184 0.2978106141090393
CurrentTrain: epoch  2, batch    36 | loss: 10.9299526Losses:  10.594849586486816 3.5141754150390625 0.294096440076828
CurrentTrain: epoch  2, batch    37 | loss: 10.5948496Losses:  8.682127952575684 2.9099230766296387 0.278319776058197
CurrentTrain: epoch  2, batch    38 | loss: 8.6821280Losses:  9.587409019470215 4.040182113647461 0.27904385328292847
CurrentTrain: epoch  2, batch    39 | loss: 9.5874090Losses:  10.462158203125 4.135157585144043 0.2828488051891327
CurrentTrain: epoch  2, batch    40 | loss: 10.4621582Losses:  8.783520698547363 2.724391460418701 0.26470866799354553
CurrentTrain: epoch  2, batch    41 | loss: 8.7835207Losses:  8.258177757263184 2.6730854511260986 0.27750152349472046
CurrentTrain: epoch  2, batch    42 | loss: 8.2581778Losses:  9.302404403686523 3.686476230621338 0.27386289834976196
CurrentTrain: epoch  2, batch    43 | loss: 9.3024044Losses:  10.747435569763184 4.5636186599731445 0.307115375995636
CurrentTrain: epoch  2, batch    44 | loss: 10.7474356Losses:  8.997366905212402 3.477037191390991 0.26970604062080383
CurrentTrain: epoch  2, batch    45 | loss: 8.9973669Losses:  11.420207977294922 4.528687000274658 0.2949431538581848
CurrentTrain: epoch  2, batch    46 | loss: 11.4202080Losses:  9.785699844360352 4.391855716705322 0.2752898931503296
CurrentTrain: epoch  2, batch    47 | loss: 9.7856998Losses:  8.716541290283203 3.4230942726135254 0.2662490904331207
CurrentTrain: epoch  2, batch    48 | loss: 8.7165413Losses:  7.75767183303833 2.0157554149627686 0.2825571298599243
CurrentTrain: epoch  2, batch    49 | loss: 7.7576718Losses:  9.183256149291992 3.4136948585510254 0.2777334451675415
CurrentTrain: epoch  2, batch    50 | loss: 9.1832561Losses:  9.041500091552734 3.2371907234191895 0.2757413983345032
CurrentTrain: epoch  2, batch    51 | loss: 9.0415001Losses:  8.575578689575195 2.9522223472595215 0.2702869176864624
CurrentTrain: epoch  2, batch    52 | loss: 8.5755787Losses:  10.455546379089355 4.921109199523926 0.2893226146697998
CurrentTrain: epoch  2, batch    53 | loss: 10.4555464Losses:  8.669848442077637 3.354241371154785 0.28414082527160645
CurrentTrain: epoch  2, batch    54 | loss: 8.6698484Losses:  9.491093635559082 3.64516019821167 0.29288187623023987
CurrentTrain: epoch  2, batch    55 | loss: 9.4910936Losses:  10.899513244628906 5.00875997543335 0.28288763761520386
CurrentTrain: epoch  2, batch    56 | loss: 10.8995132Losses:  10.14082145690918 4.029366493225098 0.2580810785293579
CurrentTrain: epoch  2, batch    57 | loss: 10.1408215Losses:  11.160298347473145 5.0099406242370605 0.277353972196579
CurrentTrain: epoch  2, batch    58 | loss: 11.1602983Losses:  8.126479148864746 2.1002278327941895 0.2650166451931
CurrentTrain: epoch  2, batch    59 | loss: 8.1264791Losses:  9.988265991210938 3.9078025817871094 0.27327507734298706
CurrentTrain: epoch  2, batch    60 | loss: 9.9882660Losses:  7.471096038818359 2.166079521179199 0.26442354917526245
CurrentTrain: epoch  2, batch    61 | loss: 7.4710960Losses:  5.828076362609863 0.8204935789108276 0.28501957654953003
CurrentTrain: epoch  2, batch    62 | loss: 5.8280764Losses:  10.840494155883789 5.2275166511535645 0.2842060923576355
CurrentTrain: epoch  3, batch     0 | loss: 10.8404942Losses:  8.498312950134277 3.673290729522705 0.2533814311027527
CurrentTrain: epoch  3, batch     1 | loss: 8.4983130Losses:  8.699173927307129 3.314314842224121 0.2768462598323822
CurrentTrain: epoch  3, batch     2 | loss: 8.6991739Losses:  9.12973690032959 3.191635847091675 0.27623918652534485
CurrentTrain: epoch  3, batch     3 | loss: 9.1297369Losses:  8.9276123046875 4.0656938552856445 0.2560581564903259
CurrentTrain: epoch  3, batch     4 | loss: 8.9276123Losses:  9.097813606262207 4.106208801269531 0.1703234761953354
CurrentTrain: epoch  3, batch     5 | loss: 9.0978136Losses:  7.995942115783691 2.0798494815826416 0.24847328662872314
CurrentTrain: epoch  3, batch     6 | loss: 7.9959421Losses:  8.59216594696045 3.2798991203308105 0.26711562275886536
CurrentTrain: epoch  3, batch     7 | loss: 8.5921659Losses:  8.356483459472656 3.2032551765441895 0.2688673734664917
CurrentTrain: epoch  3, batch     8 | loss: 8.3564835Losses:  11.445874214172363 5.845170497894287 0.26723429560661316
CurrentTrain: epoch  3, batch     9 | loss: 11.4458742Losses:  7.230335712432861 2.1058781147003174 0.2440774291753769
CurrentTrain: epoch  3, batch    10 | loss: 7.2303357Losses:  8.230074882507324 2.90297269821167 0.26821112632751465
CurrentTrain: epoch  3, batch    11 | loss: 8.2300749Losses:  8.166614532470703 2.663914203643799 0.2708622217178345
CurrentTrain: epoch  3, batch    12 | loss: 8.1666145Losses:  6.800573348999023 1.7642040252685547 0.2467299848794937
CurrentTrain: epoch  3, batch    13 | loss: 6.8005733Losses:  10.832747459411621 5.526810646057129 0.2670815587043762
CurrentTrain: epoch  3, batch    14 | loss: 10.8327475Losses:  9.898470878601074 4.4283127784729 0.28422296047210693
CurrentTrain: epoch  3, batch    15 | loss: 9.8984709Losses:  8.488560676574707 2.9657797813415527 0.2559354305267334
CurrentTrain: epoch  3, batch    16 | loss: 8.4885607Losses:  7.089966773986816 2.120161533355713 0.24599143862724304
CurrentTrain: epoch  3, batch    17 | loss: 7.0899668Losses:  7.631074905395508 1.9759984016418457 0.2541361451148987
CurrentTrain: epoch  3, batch    18 | loss: 7.6310749Losses:  7.388572692871094 2.321751594543457 0.26060405373573303
CurrentTrain: epoch  3, batch    19 | loss: 7.3885727Losses:  9.17198657989502 4.138607978820801 0.2711040675640106
CurrentTrain: epoch  3, batch    20 | loss: 9.1719866Losses:  9.620085716247559 4.247579574584961 0.25904038548469543
CurrentTrain: epoch  3, batch    21 | loss: 9.6200857Losses:  8.327892303466797 3.095815658569336 0.2716107666492462
CurrentTrain: epoch  3, batch    22 | loss: 8.3278923Losses:  9.867176055908203 5.112301826477051 0.2599644064903259
CurrentTrain: epoch  3, batch    23 | loss: 9.8671761Losses:  7.813220024108887 2.848404884338379 0.24362263083457947
CurrentTrain: epoch  3, batch    24 | loss: 7.8132200Losses:  7.971935749053955 2.8309712409973145 0.24983641505241394
CurrentTrain: epoch  3, batch    25 | loss: 7.9719357Losses:  7.87551736831665 3.1455984115600586 0.2529708743095398
CurrentTrain: epoch  3, batch    26 | loss: 7.8755174Losses:  8.763736724853516 3.678640365600586 0.24841949343681335
CurrentTrain: epoch  3, batch    27 | loss: 8.7637367Losses:  8.56270694732666 2.910055637359619 0.24291546642780304
CurrentTrain: epoch  3, batch    28 | loss: 8.5627069Losses:  8.418224334716797 2.7367119789123535 0.2579767107963562
CurrentTrain: epoch  3, batch    29 | loss: 8.4182243Losses:  9.344778060913086 3.686985969543457 0.263021856546402
CurrentTrain: epoch  3, batch    30 | loss: 9.3447781Losses:  8.730013847351074 2.650071144104004 0.24583113193511963
CurrentTrain: epoch  3, batch    31 | loss: 8.7300138Losses:  9.47429084777832 4.206460952758789 0.2717055678367615
CurrentTrain: epoch  3, batch    32 | loss: 9.4742908Losses:  8.836182594299316 3.65330171585083 0.27197593450546265
CurrentTrain: epoch  3, batch    33 | loss: 8.8361826Losses:  7.324455738067627 2.352851152420044 0.23937654495239258
CurrentTrain: epoch  3, batch    34 | loss: 7.3244557Losses:  8.662908554077148 2.897284984588623 0.24455887079238892
CurrentTrain: epoch  3, batch    35 | loss: 8.6629086Losses:  7.640562057495117 2.2607760429382324 0.25247693061828613
CurrentTrain: epoch  3, batch    36 | loss: 7.6405621Losses:  8.546382904052734 3.341176986694336 0.23923279345035553
CurrentTrain: epoch  3, batch    37 | loss: 8.5463829Losses:  8.60374927520752 3.405970573425293 0.2717819809913635
CurrentTrain: epoch  3, batch    38 | loss: 8.6037493Losses:  7.265655040740967 2.0193138122558594 0.25457820296287537
CurrentTrain: epoch  3, batch    39 | loss: 7.2656550Losses:  8.330280303955078 2.6688499450683594 0.25726908445358276
CurrentTrain: epoch  3, batch    40 | loss: 8.3302803Losses:  8.054758071899414 3.2125794887542725 0.27401119470596313
CurrentTrain: epoch  3, batch    41 | loss: 8.0547581Losses:  10.869380950927734 4.843508720397949 0.2785607576370239
CurrentTrain: epoch  3, batch    42 | loss: 10.8693810Losses:  8.312407493591309 3.164926528930664 0.2577053904533386
CurrentTrain: epoch  3, batch    43 | loss: 8.3124075Losses:  9.680669784545898 4.600769519805908 0.27783921360969543
CurrentTrain: epoch  3, batch    44 | loss: 9.6806698Losses:  7.844006061553955 2.70139217376709 0.2606987953186035
CurrentTrain: epoch  3, batch    45 | loss: 7.8440061Losses:  7.92657995223999 2.8045639991760254 0.25178617238998413
CurrentTrain: epoch  3, batch    46 | loss: 7.9265800Losses:  8.740445137023926 3.9602532386779785 0.24454939365386963
CurrentTrain: epoch  3, batch    47 | loss: 8.7404451Losses:  9.836084365844727 4.343967914581299 0.2575557827949524
CurrentTrain: epoch  3, batch    48 | loss: 9.8360844Losses:  7.780117988586426 2.334254026412964 0.2517908215522766
CurrentTrain: epoch  3, batch    49 | loss: 7.7801180Losses:  8.827820777893066 3.5200371742248535 0.259120374917984
CurrentTrain: epoch  3, batch    50 | loss: 8.8278208Losses:  6.361690998077393 1.7275550365447998 0.24234044551849365
CurrentTrain: epoch  3, batch    51 | loss: 6.3616910Losses:  8.204286575317383 3.6126480102539062 0.26067957282066345
CurrentTrain: epoch  3, batch    52 | loss: 8.2042866Losses:  9.834137916564941 4.592972278594971 0.28650742769241333
CurrentTrain: epoch  3, batch    53 | loss: 9.8341379Losses:  8.454100608825684 3.614387273788452 0.2600826919078827
CurrentTrain: epoch  3, batch    54 | loss: 8.4541006Losses:  12.741891860961914 6.835162162780762 0.2762436866760254
CurrentTrain: epoch  3, batch    55 | loss: 12.7418919Losses:  7.878157615661621 2.818376064300537 0.2772756814956665
CurrentTrain: epoch  3, batch    56 | loss: 7.8781576Losses:  11.082832336425781 4.773514747619629 0.273496150970459
CurrentTrain: epoch  3, batch    57 | loss: 11.0828323Losses:  9.812387466430664 4.132997035980225 0.263189435005188
CurrentTrain: epoch  3, batch    58 | loss: 9.8123875Losses:  8.785761833190918 3.9639134407043457 0.25740376114845276
CurrentTrain: epoch  3, batch    59 | loss: 8.7857618Losses:  10.716421127319336 5.571200370788574 0.28266334533691406
CurrentTrain: epoch  3, batch    60 | loss: 10.7164211Losses:  7.183588027954102 2.2942774295806885 0.2506963014602661
CurrentTrain: epoch  3, batch    61 | loss: 7.1835880Losses:  5.362714767456055 0.54435133934021 0.17553822696208954
CurrentTrain: epoch  3, batch    62 | loss: 5.3627148Losses:  10.036765098571777 4.884787559509277 0.24613192677497864
CurrentTrain: epoch  4, batch     0 | loss: 10.0367651Losses:  8.969249725341797 4.019043922424316 0.2840404510498047
CurrentTrain: epoch  4, batch     1 | loss: 8.9692497Losses:  7.31947660446167 2.089845895767212 0.2497599720954895
CurrentTrain: epoch  4, batch     2 | loss: 7.3194766Losses:  8.6520357131958 3.89391827583313 0.2745845317840576
CurrentTrain: epoch  4, batch     3 | loss: 8.6520357Losses:  12.0338716506958 7.722494125366211 0.26316702365875244
CurrentTrain: epoch  4, batch     4 | loss: 12.0338717Losses:  9.516982078552246 4.563097953796387 0.2679647207260132
CurrentTrain: epoch  4, batch     5 | loss: 9.5169821Losses:  8.76789665222168 3.6621856689453125 0.2598658800125122
CurrentTrain: epoch  4, batch     6 | loss: 8.7678967Losses:  9.352513313293457 4.595644950866699 0.27353549003601074
CurrentTrain: epoch  4, batch     7 | loss: 9.3525133Losses:  7.430637359619141 2.7302231788635254 0.2539266347885132
CurrentTrain: epoch  4, batch     8 | loss: 7.4306374Losses:  6.774425983428955 1.9620139598846436 0.24119627475738525
CurrentTrain: epoch  4, batch     9 | loss: 6.7744260Losses:  7.710460186004639 2.906158447265625 0.2397909164428711
CurrentTrain: epoch  4, batch    10 | loss: 7.7104602Losses:  8.273782730102539 3.2245795726776123 0.2548079192638397
CurrentTrain: epoch  4, batch    11 | loss: 8.2737827Losses:  7.429185390472412 2.4510552883148193 0.24682553112506866
CurrentTrain: epoch  4, batch    12 | loss: 7.4291854Losses:  7.308680057525635 2.6556787490844727 0.25192156434059143
CurrentTrain: epoch  4, batch    13 | loss: 7.3086801Losses:  8.146484375 3.449943780899048 0.24373678863048553
CurrentTrain: epoch  4, batch    14 | loss: 8.1464844Losses:  7.360698223114014 2.7919390201568604 0.24718141555786133
CurrentTrain: epoch  4, batch    15 | loss: 7.3606982Losses:  11.340493202209473 6.524364471435547 0.26491889357566833
CurrentTrain: epoch  4, batch    16 | loss: 11.3404932Losses:  9.210227012634277 3.853219985961914 0.2529144883155823
CurrentTrain: epoch  4, batch    17 | loss: 9.2102270Losses:  8.015044212341309 3.388040781021118 0.2578708529472351
CurrentTrain: epoch  4, batch    18 | loss: 8.0150442Losses:  7.846258640289307 3.1460509300231934 0.2526542544364929
CurrentTrain: epoch  4, batch    19 | loss: 7.8462586Losses:  8.073408126831055 3.436277151107788 0.24320155382156372
CurrentTrain: epoch  4, batch    20 | loss: 8.0734081Losses:  6.283498764038086 1.7401025295257568 0.23585882782936096
CurrentTrain: epoch  4, batch    21 | loss: 6.2834988Losses:  6.718840599060059 1.9392200708389282 0.2436707317829132
CurrentTrain: epoch  4, batch    22 | loss: 6.7188406Losses:  8.656152725219727 3.619424343109131 0.2496490180492401
CurrentTrain: epoch  4, batch    23 | loss: 8.6561527Losses:  8.73685073852539 4.039974212646484 0.17126694321632385
CurrentTrain: epoch  4, batch    24 | loss: 8.7368507Losses:  7.66753625869751 2.8782758712768555 0.2439909726381302
CurrentTrain: epoch  4, batch    25 | loss: 7.6675363Losses:  9.374368667602539 4.609042644500732 0.26970475912094116
CurrentTrain: epoch  4, batch    26 | loss: 9.3743687Losses:  6.190311431884766 1.6494274139404297 0.22826454043388367
CurrentTrain: epoch  4, batch    27 | loss: 6.1903114Losses:  8.323127746582031 3.2709121704101562 0.25863194465637207
CurrentTrain: epoch  4, batch    28 | loss: 8.3231277Losses:  7.84071159362793 3.225890636444092 0.2640336751937866
CurrentTrain: epoch  4, batch    29 | loss: 7.8407116Losses:  10.722332954406738 6.215692520141602 0.2740679979324341
CurrentTrain: epoch  4, batch    30 | loss: 10.7223330Losses:  7.370960712432861 2.878865957260132 0.24112290143966675
CurrentTrain: epoch  4, batch    31 | loss: 7.3709607Losses:  6.7123308181762695 2.200640916824341 0.23580093681812286
CurrentTrain: epoch  4, batch    32 | loss: 6.7123308Losses:  8.14510440826416 2.98085880279541 0.23558640480041504
CurrentTrain: epoch  4, batch    33 | loss: 8.1451044Losses:  7.488612651824951 2.9873502254486084 0.2401529848575592
CurrentTrain: epoch  4, batch    34 | loss: 7.4886127Losses:  6.950663089752197 2.089738368988037 0.24897804856300354
CurrentTrain: epoch  4, batch    35 | loss: 6.9506631Losses:  7.541189193725586 3.032956838607788 0.2676212787628174
CurrentTrain: epoch  4, batch    36 | loss: 7.5411892Losses:  9.756667137145996 5.020857810974121 0.1733618825674057
CurrentTrain: epoch  4, batch    37 | loss: 9.7566671Losses:  7.637560844421387 3.0866782665252686 0.23462308943271637
CurrentTrain: epoch  4, batch    38 | loss: 7.6375608Losses:  6.834660530090332 2.275567054748535 0.24077019095420837
CurrentTrain: epoch  4, batch    39 | loss: 6.8346605Losses:  10.760468482971191 5.128543853759766 0.2614547610282898
CurrentTrain: epoch  4, batch    40 | loss: 10.7604685Losses:  7.122084140777588 2.6799638271331787 0.23970863223075867
CurrentTrain: epoch  4, batch    41 | loss: 7.1220841Losses:  7.599674224853516 2.995588779449463 0.24600167572498322
CurrentTrain: epoch  4, batch    42 | loss: 7.5996742Losses:  9.031243324279785 4.485610485076904 0.25224390625953674
CurrentTrain: epoch  4, batch    43 | loss: 9.0312433Losses:  10.773366928100586 5.170503616333008 0.15446308255195618
CurrentTrain: epoch  4, batch    44 | loss: 10.7733669Losses:  7.774104118347168 3.2444753646850586 0.2465076595544815
CurrentTrain: epoch  4, batch    45 | loss: 7.7741041Losses:  7.446481704711914 2.6405529975891113 0.23122185468673706
CurrentTrain: epoch  4, batch    46 | loss: 7.4464817Losses:  7.664834022521973 2.5452260971069336 0.23604756593704224
CurrentTrain: epoch  4, batch    47 | loss: 7.6648340Losses:  6.673549175262451 1.6264057159423828 0.2390238642692566
CurrentTrain: epoch  4, batch    48 | loss: 6.6735492Losses:  10.076081275939941 5.134406089782715 0.24475790560245514
CurrentTrain: epoch  4, batch    49 | loss: 10.0760813Losses:  7.884662628173828 3.204706907272339 0.24255961179733276
CurrentTrain: epoch  4, batch    50 | loss: 7.8846626Losses:  9.276747703552246 4.100492477416992 0.2627021074295044
CurrentTrain: epoch  4, batch    51 | loss: 9.2767477Losses:  6.86934757232666 2.2167224884033203 0.2330002337694168
CurrentTrain: epoch  4, batch    52 | loss: 6.8693476Losses:  7.818208694458008 3.203404426574707 0.24222008883953094
CurrentTrain: epoch  4, batch    53 | loss: 7.8182087Losses:  9.221336364746094 4.204470157623291 0.22662760317325592
CurrentTrain: epoch  4, batch    54 | loss: 9.2213364Losses:  6.908377647399902 2.4422249794006348 0.24271686375141144
CurrentTrain: epoch  4, batch    55 | loss: 6.9083776Losses:  6.953287124633789 2.3933451175689697 0.2518918514251709
CurrentTrain: epoch  4, batch    56 | loss: 6.9532871Losses:  7.056926727294922 2.60650634765625 0.24473214149475098
CurrentTrain: epoch  4, batch    57 | loss: 7.0569267Losses:  7.727917194366455 2.7781050205230713 0.2363802194595337
CurrentTrain: epoch  4, batch    58 | loss: 7.7279172Losses:  8.904987335205078 3.353428840637207 0.17351286113262177
CurrentTrain: epoch  4, batch    59 | loss: 8.9049873Losses:  7.705537796020508 3.1325204372406006 0.22584998607635498
CurrentTrain: epoch  4, batch    60 | loss: 7.7055378Losses:  7.340059757232666 2.660961151123047 0.24170547723770142
CurrentTrain: epoch  4, batch    61 | loss: 7.3400598Losses:  7.876428604125977 2.1899242401123047 0.20784831047058105
CurrentTrain: epoch  4, batch    62 | loss: 7.8764286Losses:  7.9116950035095215 2.2848896980285645 0.234130859375
CurrentTrain: epoch  5, batch     0 | loss: 7.9116950Losses:  7.027137279510498 2.5427615642547607 0.2291828989982605
CurrentTrain: epoch  5, batch     1 | loss: 7.0271373Losses:  8.477460861206055 3.739569664001465 0.24267762899398804
CurrentTrain: epoch  5, batch     2 | loss: 8.4774609Losses:  7.543277740478516 2.593510866165161 0.23130816221237183
CurrentTrain: epoch  5, batch     3 | loss: 7.5432777Losses:  9.08272933959961 4.370543003082275 0.2378944307565689
CurrentTrain: epoch  5, batch     4 | loss: 9.0827293Losses:  10.739521980285645 6.292788982391357 0.23827609419822693
CurrentTrain: epoch  5, batch     5 | loss: 10.7395220Losses:  7.372824192047119 2.8154568672180176 0.24541175365447998
CurrentTrain: epoch  5, batch     6 | loss: 7.3728242Losses:  8.992110252380371 3.8506836891174316 0.25529953837394714
CurrentTrain: epoch  5, batch     7 | loss: 8.9921103Losses:  8.22030258178711 3.7903451919555664 0.2512272000312805
CurrentTrain: epoch  5, batch     8 | loss: 8.2203026Losses:  8.536580085754395 3.8800461292266846 0.25549447536468506
CurrentTrain: epoch  5, batch     9 | loss: 8.5365801Losses:  6.9164628982543945 2.2913999557495117 0.23026540875434875
CurrentTrain: epoch  5, batch    10 | loss: 6.9164629Losses:  9.58320426940918 4.949119567871094 0.25068235397338867
CurrentTrain: epoch  5, batch    11 | loss: 9.5832043Losses:  6.253419399261475 1.8767461776733398 0.2219170331954956
CurrentTrain: epoch  5, batch    12 | loss: 6.2534194Losses:  8.002656936645508 3.3970844745635986 0.2495782971382141
CurrentTrain: epoch  5, batch    13 | loss: 8.0026569Losses:  9.522250175476074 4.9261980056762695 0.16427287459373474
CurrentTrain: epoch  5, batch    14 | loss: 9.5222502Losses:  10.358232498168945 5.807552814483643 0.2551788091659546
CurrentTrain: epoch  5, batch    15 | loss: 10.3582325Losses:  6.609953880310059 2.200303554534912 0.23270012438297272
CurrentTrain: epoch  5, batch    16 | loss: 6.6099539Losses:  10.740983963012695 6.204587936401367 0.24145102500915527
CurrentTrain: epoch  5, batch    17 | loss: 10.7409840Losses:  7.362936496734619 2.7763352394104004 0.22979043424129486
CurrentTrain: epoch  5, batch    18 | loss: 7.3629365Losses:  7.884102821350098 3.240863800048828 0.24565620720386505
CurrentTrain: epoch  5, batch    19 | loss: 7.8841028Losses:  6.594401836395264 2.121387004852295 0.23308196663856506
CurrentTrain: epoch  5, batch    20 | loss: 6.5944018Losses:  9.031851768493652 4.416950225830078 0.2571902871131897
CurrentTrain: epoch  5, batch    21 | loss: 9.0318518Losses:  10.228849411010742 5.311352729797363 0.16466747224330902
CurrentTrain: epoch  5, batch    22 | loss: 10.2288494Losses:  8.02302074432373 3.491753101348877 0.25053173303604126
CurrentTrain: epoch  5, batch    23 | loss: 8.0230207Losses:  6.286011219024658 1.704982042312622 0.23311012983322144
CurrentTrain: epoch  5, batch    24 | loss: 6.2860112Losses:  6.9690260887146 2.1989290714263916 0.2340300977230072
CurrentTrain: epoch  5, batch    25 | loss: 6.9690261Losses:  7.665592193603516 3.2519516944885254 0.23795387148857117
CurrentTrain: epoch  5, batch    26 | loss: 7.6655922Losses:  10.698273658752441 6.061985015869141 0.2501879930496216
CurrentTrain: epoch  5, batch    27 | loss: 10.6982737Losses:  6.788990497589111 2.278287410736084 0.23477323353290558
CurrentTrain: epoch  5, batch    28 | loss: 6.7889905Losses:  7.367481231689453 2.8558855056762695 0.2532234191894531
CurrentTrain: epoch  5, batch    29 | loss: 7.3674812Losses:  8.217069625854492 3.6714956760406494 0.24592682719230652
CurrentTrain: epoch  5, batch    30 | loss: 8.2170696Losses:  6.748751640319824 2.246283531188965 0.2323140949010849
CurrentTrain: epoch  5, batch    31 | loss: 6.7487516Losses:  6.460108757019043 1.9721083641052246 0.22952806949615479
CurrentTrain: epoch  5, batch    32 | loss: 6.4601088Losses:  6.175515174865723 1.806520938873291 0.22195184230804443
CurrentTrain: epoch  5, batch    33 | loss: 6.1755152Losses:  7.319363117218018 2.824406147003174 0.24338199198246002
CurrentTrain: epoch  5, batch    34 | loss: 7.3193631Losses:  8.678935050964355 4.21741247177124 0.2339448630809784
CurrentTrain: epoch  5, batch    35 | loss: 8.6789351Losses:  9.266648292541504 4.7198805809021 0.2550438642501831
CurrentTrain: epoch  5, batch    36 | loss: 9.2666483Losses:  8.360157012939453 3.944866418838501 0.26016151905059814
CurrentTrain: epoch  5, batch    37 | loss: 8.3601570Losses:  6.045695781707764 1.6515034437179565 0.21542519330978394
CurrentTrain: epoch  5, batch    38 | loss: 6.0456958Losses:  6.995645999908447 2.512404441833496 0.251322478055954
CurrentTrain: epoch  5, batch    39 | loss: 6.9956460Losses:  7.957996845245361 3.5665969848632812 0.2534320056438446
CurrentTrain: epoch  5, batch    40 | loss: 7.9579968Losses:  6.960415363311768 2.6537671089172363 0.22779430449008942
CurrentTrain: epoch  5, batch    41 | loss: 6.9604154Losses:  7.590799808502197 3.1369309425354004 0.23954302072525024
CurrentTrain: epoch  5, batch    42 | loss: 7.5907998Losses:  10.529684066772461 6.075850486755371 0.24647921323776245
CurrentTrain: epoch  5, batch    43 | loss: 10.5296841Losses:  6.589829444885254 2.045041561126709 0.2306637018918991
CurrentTrain: epoch  5, batch    44 | loss: 6.5898294Losses:  7.029783725738525 2.5227246284484863 0.23989544808864594
CurrentTrain: epoch  5, batch    45 | loss: 7.0297837Losses:  7.169248580932617 2.6905250549316406 0.22885258495807648
CurrentTrain: epoch  5, batch    46 | loss: 7.1692486Losses:  8.178170204162598 3.875319004058838 0.22712162137031555
CurrentTrain: epoch  5, batch    47 | loss: 8.1781702Losses:  6.829349040985107 2.45271897315979 0.2459787279367447
CurrentTrain: epoch  5, batch    48 | loss: 6.8293490Losses:  6.587380886077881 2.209354877471924 0.23236539959907532
CurrentTrain: epoch  5, batch    49 | loss: 6.5873809Losses:  6.650778293609619 2.2550835609436035 0.23264604806900024
CurrentTrain: epoch  5, batch    50 | loss: 6.6507783Losses:  8.182319641113281 3.8091306686401367 0.1687047928571701
CurrentTrain: epoch  5, batch    51 | loss: 8.1823196Losses:  8.251959800720215 3.8141860961914062 0.2274368703365326
CurrentTrain: epoch  5, batch    52 | loss: 8.2519598Losses:  7.513600826263428 3.136481761932373 0.15136688947677612
CurrentTrain: epoch  5, batch    53 | loss: 7.5136008Losses:  6.925155162811279 2.5888805389404297 0.23879826068878174
CurrentTrain: epoch  5, batch    54 | loss: 6.9251552Losses:  7.40115213394165 3.1322600841522217 0.23527370393276215
CurrentTrain: epoch  5, batch    55 | loss: 7.4011521Losses:  7.09947395324707 2.709873676300049 0.23755860328674316
CurrentTrain: epoch  5, batch    56 | loss: 7.0994740Losses:  8.851411819458008 4.580409049987793 0.23419930040836334
CurrentTrain: epoch  5, batch    57 | loss: 8.8514118Losses:  8.064677238464355 3.7055277824401855 0.21425531804561615
CurrentTrain: epoch  5, batch    58 | loss: 8.0646772Losses:  6.762109279632568 2.395951509475708 0.23505434393882751
CurrentTrain: epoch  5, batch    59 | loss: 6.7621093Losses:  7.285247802734375 2.9294919967651367 0.24001216888427734
CurrentTrain: epoch  5, batch    60 | loss: 7.2852478Losses:  10.42239761352539 5.938290596008301 0.2482069730758667
CurrentTrain: epoch  5, batch    61 | loss: 10.4223976Losses:  4.883416175842285 0.5416547060012817 0.27946168184280396
CurrentTrain: epoch  5, batch    62 | loss: 4.8834162Losses:  6.804683685302734 2.386955738067627 0.2352348268032074
CurrentTrain: epoch  6, batch     0 | loss: 6.8046837Losses:  9.166098594665527 4.805675029754639 0.2358461320400238
CurrentTrain: epoch  6, batch     1 | loss: 9.1660986Losses:  10.040189743041992 5.629408359527588 0.2529107332229614
CurrentTrain: epoch  6, batch     2 | loss: 10.0401897Losses:  6.673613548278809 2.3580644130706787 0.2193659543991089
CurrentTrain: epoch  6, batch     3 | loss: 6.6736135Losses:  8.386148452758789 4.0114240646362305 0.26294463872909546
CurrentTrain: epoch  6, batch     4 | loss: 8.3861485Losses:  6.5851240158081055 2.2735934257507324 0.22770819067955017
CurrentTrain: epoch  6, batch     5 | loss: 6.5851240Losses:  7.271944999694824 2.9430065155029297 0.23912714421749115
CurrentTrain: epoch  6, batch     6 | loss: 7.2719450Losses:  6.735246658325195 2.423013687133789 0.23381352424621582
CurrentTrain: epoch  6, batch     7 | loss: 6.7352467Losses:  6.847393035888672 2.454812526702881 0.2343415915966034
CurrentTrain: epoch  6, batch     8 | loss: 6.8473930Losses:  8.770353317260742 4.333517074584961 0.2516944706439972
CurrentTrain: epoch  6, batch     9 | loss: 8.7703533Losses:  7.52897834777832 3.166539430618286 0.24083344638347626
CurrentTrain: epoch  6, batch    10 | loss: 7.5289783Losses:  8.363452911376953 3.7651314735412598 0.25175386667251587
CurrentTrain: epoch  6, batch    11 | loss: 8.3634529Losses:  6.1565937995910645 1.7460205554962158 0.2217440903186798
CurrentTrain: epoch  6, batch    12 | loss: 6.1565938Losses:  7.99776029586792 3.6787257194519043 0.25000983476638794
CurrentTrain: epoch  6, batch    13 | loss: 7.9977603Losses:  7.088179588317871 2.7621331214904785 0.22524023056030273
CurrentTrain: epoch  6, batch    14 | loss: 7.0881796Losses:  6.688299655914307 2.3617987632751465 0.2339688241481781
CurrentTrain: epoch  6, batch    15 | loss: 6.6882997Losses:  7.249564170837402 2.90553879737854 0.23660558462142944
CurrentTrain: epoch  6, batch    16 | loss: 7.2495642Losses:  6.817193031311035 2.5701346397399902 0.24305760860443115
CurrentTrain: epoch  6, batch    17 | loss: 6.8171930Losses:  8.278369903564453 3.9544551372528076 0.23843088746070862
CurrentTrain: epoch  6, batch    18 | loss: 8.2783699Losses:  6.6581034660339355 2.3838024139404297 0.23146218061447144
CurrentTrain: epoch  6, batch    19 | loss: 6.6581035Losses:  8.353752136230469 4.003051280975342 0.23518602550029755
CurrentTrain: epoch  6, batch    20 | loss: 8.3537521Losses:  7.083890914916992 2.7470366954803467 0.22652825713157654
CurrentTrain: epoch  6, batch    21 | loss: 7.0838909Losses:  9.554019927978516 5.315349578857422 0.23558282852172852
CurrentTrain: epoch  6, batch    22 | loss: 9.5540199Losses:  8.976183891296387 4.727931022644043 0.14608895778656006
CurrentTrain: epoch  6, batch    23 | loss: 8.9761839Losses:  7.532395362854004 3.195570468902588 0.24331170320510864
CurrentTrain: epoch  6, batch    24 | loss: 7.5323954Losses:  7.316019058227539 2.9994473457336426 0.26517200469970703
CurrentTrain: epoch  6, batch    25 | loss: 7.3160191Losses:  7.473205089569092 3.11187744140625 0.23316052556037903
CurrentTrain: epoch  6, batch    26 | loss: 7.4732051Losses:  6.979850769042969 2.665222644805908 0.2400205284357071
CurrentTrain: epoch  6, batch    27 | loss: 6.9798508Losses:  10.641993522644043 6.347009658813477 0.24146972596645355
CurrentTrain: epoch  6, batch    28 | loss: 10.6419935Losses:  8.852420806884766 4.471245765686035 0.23726877570152283
CurrentTrain: epoch  6, batch    29 | loss: 8.8524208Losses:  5.773671627044678 1.4859389066696167 0.2175372838973999
CurrentTrain: epoch  6, batch    30 | loss: 5.7736716Losses:  7.189807891845703 2.8095338344573975 0.22769926488399506
CurrentTrain: epoch  6, batch    31 | loss: 7.1898079Losses:  8.387323379516602 4.148868560791016 0.2286398559808731
CurrentTrain: epoch  6, batch    32 | loss: 8.3873234Losses:  6.317424297332764 2.0294618606567383 0.23376983404159546
CurrentTrain: epoch  6, batch    33 | loss: 6.3174243Losses:  6.814346790313721 2.5020787715911865 0.23950235545635223
CurrentTrain: epoch  6, batch    34 | loss: 6.8143468Losses:  9.374199867248535 5.099020957946777 0.19156377017498016
CurrentTrain: epoch  6, batch    35 | loss: 9.3741999Losses:  7.413632392883301 3.1949329376220703 0.2333134114742279
CurrentTrain: epoch  6, batch    36 | loss: 7.4136324Losses:  7.910968780517578 3.58396053314209 0.23674596846103668
CurrentTrain: epoch  6, batch    37 | loss: 7.9109688Losses:  7.321245193481445 2.983347177505493 0.21789073944091797
CurrentTrain: epoch  6, batch    38 | loss: 7.3212452Losses:  7.793520927429199 3.51670241355896 0.1535884439945221
CurrentTrain: epoch  6, batch    39 | loss: 7.7935209Losses:  7.830538272857666 3.547328472137451 0.23546281456947327
CurrentTrain: epoch  6, batch    40 | loss: 7.8305383Losses:  7.215040683746338 2.9431872367858887 0.23955054581165314
CurrentTrain: epoch  6, batch    41 | loss: 7.2150407Losses:  6.627108573913574 2.2779245376586914 0.21689772605895996
CurrentTrain: epoch  6, batch    42 | loss: 6.6271086Losses:  6.458728790283203 2.1781394481658936 0.23017047345638275
CurrentTrain: epoch  6, batch    43 | loss: 6.4587288Losses:  7.418028354644775 3.1620349884033203 0.23702938854694366
CurrentTrain: epoch  6, batch    44 | loss: 7.4180284Losses:  6.703671932220459 2.3828678131103516 0.23153367638587952
CurrentTrain: epoch  6, batch    45 | loss: 6.7036719Losses:  8.963173866271973 4.592563629150391 0.25009024143218994
CurrentTrain: epoch  6, batch    46 | loss: 8.9631739Losses:  7.087191104888916 2.8047666549682617 0.22759681940078735
CurrentTrain: epoch  6, batch    47 | loss: 7.0871911Losses:  6.15753698348999 1.854985237121582 0.22264930605888367
CurrentTrain: epoch  6, batch    48 | loss: 6.1575370Losses:  7.923882484436035 3.6742167472839355 0.23544248938560486
CurrentTrain: epoch  6, batch    49 | loss: 7.9238825Losses:  7.518804550170898 3.2168643474578857 0.23613065481185913
CurrentTrain: epoch  6, batch    50 | loss: 7.5188046Losses:  7.257582187652588 2.968322277069092 0.22511471807956696
CurrentTrain: epoch  6, batch    51 | loss: 7.2575822Losses:  6.461452007293701 2.205077648162842 0.2313879281282425
CurrentTrain: epoch  6, batch    52 | loss: 6.4614520Losses:  9.040558815002441 4.77406120300293 0.16316352784633636
CurrentTrain: epoch  6, batch    53 | loss: 9.0405588Losses:  8.063501358032227 3.9221973419189453 0.22978776693344116
CurrentTrain: epoch  6, batch    54 | loss: 8.0635014Losses:  10.372445106506348 6.013731002807617 0.15981581807136536
CurrentTrain: epoch  6, batch    55 | loss: 10.3724451Losses:  5.847076892852783 1.6095930337905884 0.2136358618736267
CurrentTrain: epoch  6, batch    56 | loss: 5.8470769Losses:  9.19999885559082 4.902570724487305 0.2463664710521698
CurrentTrain: epoch  6, batch    57 | loss: 9.1999989Losses:  9.432168960571289 5.137638092041016 0.2499087154865265
CurrentTrain: epoch  6, batch    58 | loss: 9.4321690Losses:  5.617595195770264 1.368844747543335 0.21099244058132172
CurrentTrain: epoch  6, batch    59 | loss: 5.6175952Losses:  6.898859977722168 2.6478238105773926 0.22592204809188843
CurrentTrain: epoch  6, batch    60 | loss: 6.8988600Losses:  6.442605018615723 2.195359230041504 0.21314740180969238
CurrentTrain: epoch  6, batch    61 | loss: 6.4426050Losses:  5.322447299957275 1.0785129070281982 0.2452179491519928
CurrentTrain: epoch  6, batch    62 | loss: 5.3224473Losses:  5.483059883117676 1.2653415203094482 0.2121870517730713
CurrentTrain: epoch  7, batch     0 | loss: 5.4830599Losses:  11.013537406921387 6.763726234436035 0.1862289011478424
CurrentTrain: epoch  7, batch     1 | loss: 11.0135374Losses:  7.155787944793701 2.92562198638916 0.23017945885658264
CurrentTrain: epoch  7, batch     2 | loss: 7.1557879Losses:  7.2510666847229 3.0003111362457275 0.24400772154331207
CurrentTrain: epoch  7, batch     3 | loss: 7.2510667Losses:  8.606882095336914 4.346122741699219 0.2576245665550232
CurrentTrain: epoch  7, batch     4 | loss: 8.6068821Losses:  10.873157501220703 6.596152305603027 0.2810779809951782
CurrentTrain: epoch  7, batch     5 | loss: 10.8731575Losses:  7.087111473083496 2.805694103240967 0.22101329267024994
CurrentTrain: epoch  7, batch     6 | loss: 7.0871115Losses:  6.078557014465332 1.8271257877349854 0.2213716208934784
CurrentTrain: epoch  7, batch     7 | loss: 6.0785570Losses:  7.89081335067749 3.566962718963623 0.2515615224838257
CurrentTrain: epoch  7, batch     8 | loss: 7.8908134Losses:  8.377311706542969 4.149408340454102 0.2451271414756775
CurrentTrain: epoch  7, batch     9 | loss: 8.3773117Losses:  8.177895545959473 3.9052438735961914 0.25383225083351135
CurrentTrain: epoch  7, batch    10 | loss: 8.1778955Losses:  6.899007320404053 2.615293025970459 0.23798570036888123
CurrentTrain: epoch  7, batch    11 | loss: 6.8990073Losses:  6.999152183532715 2.722888708114624 0.2298385202884674
CurrentTrain: epoch  7, batch    12 | loss: 6.9991522Losses:  6.0201191902160645 1.8383253812789917 0.2188343107700348
CurrentTrain: epoch  7, batch    13 | loss: 6.0201192Losses:  8.872769355773926 4.610451698303223 0.25323930382728577
CurrentTrain: epoch  7, batch    14 | loss: 8.8727694Losses:  7.4434356689453125 3.189995050430298 0.2377299964427948
CurrentTrain: epoch  7, batch    15 | loss: 7.4434357Losses:  6.584770679473877 2.4134628772735596 0.14816050231456757
CurrentTrain: epoch  7, batch    16 | loss: 6.5847707Losses:  6.818882465362549 2.5665340423583984 0.23664671182632446
CurrentTrain: epoch  7, batch    17 | loss: 6.8188825Losses:  6.9981842041015625 2.778130531311035 0.23767314851284027
CurrentTrain: epoch  7, batch    18 | loss: 6.9981842Losses:  9.771513938903809 5.584693908691406 0.24942097067832947
CurrentTrain: epoch  7, batch    19 | loss: 9.7715139Losses:  7.376515865325928 3.1232388019561768 0.23065605759620667
CurrentTrain: epoch  7, batch    20 | loss: 7.3765159Losses:  9.75628662109375 5.4784135818481445 0.2467098832130432
CurrentTrain: epoch  7, batch    21 | loss: 9.7562866Losses:  6.828400611877441 2.608076333999634 0.2190179079771042
CurrentTrain: epoch  7, batch    22 | loss: 6.8284006Losses:  7.114020347595215 2.8418068885803223 0.23737996816635132
CurrentTrain: epoch  7, batch    23 | loss: 7.1140203Losses:  7.370593547821045 3.2267203330993652 0.14766614139080048
CurrentTrain: epoch  7, batch    24 | loss: 7.3705935Losses:  7.425435543060303 3.224752426147461 0.22973394393920898
CurrentTrain: epoch  7, batch    25 | loss: 7.4254355Losses:  8.43449592590332 4.172091007232666 0.24259914457798004
CurrentTrain: epoch  7, batch    26 | loss: 8.4344959Losses:  6.8996992111206055 2.649338722229004 0.22091460227966309
CurrentTrain: epoch  7, batch    27 | loss: 6.8996992Losses:  6.635269641876221 2.4256813526153564 0.22488757967948914
CurrentTrain: epoch  7, batch    28 | loss: 6.6352696Losses:  6.1744232177734375 1.971074104309082 0.21121364831924438
CurrentTrain: epoch  7, batch    29 | loss: 6.1744232Losses:  6.537677764892578 2.30230975151062 0.23226721584796906
CurrentTrain: epoch  7, batch    30 | loss: 6.5376778Losses:  7.733395099639893 3.5637893676757812 0.13770601153373718
CurrentTrain: epoch  7, batch    31 | loss: 7.7333951Losses:  6.622534275054932 2.4001309871673584 0.22780752182006836
CurrentTrain: epoch  7, batch    32 | loss: 6.6225343Losses:  5.9667229652404785 1.7877116203308105 0.21818968653678894
CurrentTrain: epoch  7, batch    33 | loss: 5.9667230Losses:  6.510844707489014 2.2451343536376953 0.22493694722652435
CurrentTrain: epoch  7, batch    34 | loss: 6.5108447Losses:  8.636595726013184 4.4021806716918945 0.23211166262626648
CurrentTrain: epoch  7, batch    35 | loss: 8.6365957Losses:  10.01032829284668 5.766200065612793 0.25346195697784424
CurrentTrain: epoch  7, batch    36 | loss: 10.0103283Losses:  7.734809398651123 3.495032787322998 0.25689929723739624
CurrentTrain: epoch  7, batch    37 | loss: 7.7348094Losses:  7.91624641418457 3.671311140060425 0.23520168662071228
CurrentTrain: epoch  7, batch    38 | loss: 7.9162464Losses:  9.025514602661133 4.920933723449707 0.14557704329490662
CurrentTrain: epoch  7, batch    39 | loss: 9.0255146Losses:  6.744318962097168 2.498082160949707 0.22490403056144714
CurrentTrain: epoch  7, batch    40 | loss: 6.7443190Losses:  6.167862415313721 1.9589431285858154 0.21090763807296753
CurrentTrain: epoch  7, batch    41 | loss: 6.1678624Losses:  7.351273059844971 3.105282783508301 0.23750688135623932
CurrentTrain: epoch  7, batch    42 | loss: 7.3512731Losses:  6.933244228363037 2.6825499534606934 0.23144057393074036
CurrentTrain: epoch  7, batch    43 | loss: 6.9332442Losses:  8.70807933807373 4.469448089599609 0.23947222530841827
CurrentTrain: epoch  7, batch    44 | loss: 8.7080793Losses:  6.381816864013672 2.153946876525879 0.22694534063339233
CurrentTrain: epoch  7, batch    45 | loss: 6.3818169Losses:  10.773682594299316 6.615888595581055 0.18129783868789673
CurrentTrain: epoch  7, batch    46 | loss: 10.7736826Losses:  7.435710430145264 3.1854472160339355 0.2357465773820877
CurrentTrain: epoch  7, batch    47 | loss: 7.4357104Losses:  7.2804155349731445 3.035065174102783 0.22721326351165771
CurrentTrain: epoch  7, batch    48 | loss: 7.2804155Losses:  6.557587146759033 2.3707571029663086 0.2333202064037323
CurrentTrain: epoch  7, batch    49 | loss: 6.5575871Losses:  8.76559829711914 4.450075149536133 0.2391553521156311
CurrentTrain: epoch  7, batch    50 | loss: 8.7655983Losses:  5.856479644775391 1.6138654947280884 0.21622410416603088
CurrentTrain: epoch  7, batch    51 | loss: 5.8564796Losses:  7.494442939758301 3.3267881870269775 0.24558629095554352
CurrentTrain: epoch  7, batch    52 | loss: 7.4944429Losses:  6.373002052307129 2.192229747772217 0.224627286195755
CurrentTrain: epoch  7, batch    53 | loss: 6.3730021Losses:  6.0564422607421875 1.8324896097183228 0.22054246068000793
CurrentTrain: epoch  7, batch    54 | loss: 6.0564423Losses:  6.949250221252441 2.695253372192383 0.2312624454498291
CurrentTrain: epoch  7, batch    55 | loss: 6.9492502Losses:  6.975762844085693 2.81284236907959 0.22136171162128448
CurrentTrain: epoch  7, batch    56 | loss: 6.9757628Losses:  9.306585311889648 5.064239501953125 0.24125826358795166
CurrentTrain: epoch  7, batch    57 | loss: 9.3065853Losses:  8.369894981384277 4.131629943847656 0.24611936509609222
CurrentTrain: epoch  7, batch    58 | loss: 8.3698950Losses:  6.029452800750732 1.7864291667938232 0.22113169729709625
CurrentTrain: epoch  7, batch    59 | loss: 6.0294528Losses:  8.544195175170898 4.330323696136475 0.23857492208480835
CurrentTrain: epoch  7, batch    60 | loss: 8.5441952Losses:  6.997897624969482 2.7447116374969482 0.22960630059242249
CurrentTrain: epoch  7, batch    61 | loss: 6.9978976Losses:  6.998924732208252 2.8182554244995117 0.17877750098705292
CurrentTrain: epoch  7, batch    62 | loss: 6.9989247Losses:  6.319201946258545 2.1362688541412354 0.22040513157844543
CurrentTrain: epoch  8, batch     0 | loss: 6.3192019Losses:  7.107872009277344 2.884406089782715 0.23257820308208466
CurrentTrain: epoch  8, batch     1 | loss: 7.1078720Losses:  11.102205276489258 6.836605072021484 0.24464532732963562
CurrentTrain: epoch  8, batch     2 | loss: 11.1022053Losses:  6.411832332611084 2.170356273651123 0.2334933876991272
CurrentTrain: epoch  8, batch     3 | loss: 6.4118323Losses:  6.549962997436523 2.3385050296783447 0.2279851734638214
CurrentTrain: epoch  8, batch     4 | loss: 6.5499630Losses:  7.093691349029541 2.8689768314361572 0.2292216718196869
CurrentTrain: epoch  8, batch     5 | loss: 7.0936913Losses:  7.945310115814209 3.6962482929229736 0.22516487538814545
CurrentTrain: epoch  8, batch     6 | loss: 7.9453101Losses:  7.012819290161133 2.782982587814331 0.22362706065177917
CurrentTrain: epoch  8, batch     7 | loss: 7.0128193Losses:  6.968522548675537 2.732053756713867 0.23260469734668732
CurrentTrain: epoch  8, batch     8 | loss: 6.9685225Losses:  6.479613304138184 2.286776542663574 0.2180940806865692
CurrentTrain: epoch  8, batch     9 | loss: 6.4796133Losses:  6.884554862976074 2.6539835929870605 0.2276051938533783
CurrentTrain: epoch  8, batch    10 | loss: 6.8845549Losses:  7.964506149291992 3.7912561893463135 0.22300243377685547
CurrentTrain: epoch  8, batch    11 | loss: 7.9645061Losses:  6.831684589385986 2.6695728302001953 0.2329067587852478
CurrentTrain: epoch  8, batch    12 | loss: 6.8316846Losses:  6.862508773803711 2.6380958557128906 0.23004809021949768
CurrentTrain: epoch  8, batch    13 | loss: 6.8625088Losses:  7.432840824127197 3.181725025177002 0.24592715501785278
CurrentTrain: epoch  8, batch    14 | loss: 7.4328408Losses:  6.556037902832031 2.3364338874816895 0.22723156213760376
CurrentTrain: epoch  8, batch    15 | loss: 6.5560379Losses:  6.744446754455566 2.5288772583007812 0.21622440218925476
CurrentTrain: epoch  8, batch    16 | loss: 6.7444468Losses:  7.417030334472656 3.1420960426330566 0.2401711791753769
CurrentTrain: epoch  8, batch    17 | loss: 7.4170303Losses:  7.027179718017578 2.795311450958252 0.23622816801071167
CurrentTrain: epoch  8, batch    18 | loss: 7.0271797Losses:  6.037293910980225 1.8190611600875854 0.21844333410263062
CurrentTrain: epoch  8, batch    19 | loss: 6.0372939Losses:  6.230828762054443 1.9984047412872314 0.22762075066566467
CurrentTrain: epoch  8, batch    20 | loss: 6.2308288Losses:  7.385670185089111 3.16379714012146 0.23105737566947937
CurrentTrain: epoch  8, batch    21 | loss: 7.3856702Losses:  6.276679039001465 2.0253982543945312 0.2363986372947693
CurrentTrain: epoch  8, batch    22 | loss: 6.2766790Losses:  6.427063941955566 2.261395215988159 0.22087514400482178
CurrentTrain: epoch  8, batch    23 | loss: 6.4270639Losses:  8.192497253417969 3.969482898712158 0.2243398129940033
CurrentTrain: epoch  8, batch    24 | loss: 8.1924973Losses:  6.310319423675537 2.06535005569458 0.23603807389736176
CurrentTrain: epoch  8, batch    25 | loss: 6.3103194Losses:  8.18042278289795 3.956286907196045 0.2489320933818817
CurrentTrain: epoch  8, batch    26 | loss: 8.1804228Losses:  6.352519989013672 2.1392953395843506 0.22216814756393433
CurrentTrain: epoch  8, batch    27 | loss: 6.3525200Losses:  8.393710136413574 4.185215950012207 0.2412644624710083
CurrentTrain: epoch  8, batch    28 | loss: 8.3937101Losses:  8.02491283416748 3.859813928604126 0.22341257333755493
CurrentTrain: epoch  8, batch    29 | loss: 8.0249128Losses:  7.321860313415527 3.069530487060547 0.2341340035200119
CurrentTrain: epoch  8, batch    30 | loss: 7.3218603Losses:  6.751172065734863 2.5373642444610596 0.2280062437057495
CurrentTrain: epoch  8, batch    31 | loss: 6.7511721Losses:  5.780082702636719 1.614393711090088 0.20797665417194366
CurrentTrain: epoch  8, batch    32 | loss: 5.7800827Losses:  6.942579746246338 2.7008814811706543 0.23067842423915863
CurrentTrain: epoch  8, batch    33 | loss: 6.9425797Losses:  8.30737018585205 4.137729644775391 0.14494627714157104
CurrentTrain: epoch  8, batch    34 | loss: 8.3073702Losses:  10.027853965759277 5.8545074462890625 0.2370660901069641
CurrentTrain: epoch  8, batch    35 | loss: 10.0278540Losses:  7.479378700256348 3.208029270172119 0.24677926301956177
CurrentTrain: epoch  8, batch    36 | loss: 7.4793787Losses:  6.553277015686035 2.380354404449463 0.22606602311134338
CurrentTrain: epoch  8, batch    37 | loss: 6.5532770Losses:  7.24395751953125 3.033849000930786 0.22559237480163574
CurrentTrain: epoch  8, batch    38 | loss: 7.2439575Losses:  6.392148494720459 2.1857991218566895 0.2177327424287796
CurrentTrain: epoch  8, batch    39 | loss: 6.3921485Losses:  6.964073181152344 2.7075953483581543 0.2426956295967102
CurrentTrain: epoch  8, batch    40 | loss: 6.9640732Losses:  5.985227108001709 1.78273344039917 0.22009538114070892
CurrentTrain: epoch  8, batch    41 | loss: 5.9852271Losses:  6.905837535858154 2.730827808380127 0.21169811487197876
CurrentTrain: epoch  8, batch    42 | loss: 6.9058375Losses:  7.035002708435059 2.792135238647461 0.2341691255569458
CurrentTrain: epoch  8, batch    43 | loss: 7.0350027Losses:  9.444324493408203 5.179075717926025 0.2521609663963318
CurrentTrain: epoch  8, batch    44 | loss: 9.4443245Losses:  8.40881633758545 4.217996597290039 0.23570357263088226
CurrentTrain: epoch  8, batch    45 | loss: 8.4088163Losses:  5.98317813873291 1.7984941005706787 0.21467378735542297
CurrentTrain: epoch  8, batch    46 | loss: 5.9831781Losses:  5.9726948738098145 1.796218752861023 0.2221391797065735
CurrentTrain: epoch  8, batch    47 | loss: 5.9726949Losses:  6.534974575042725 2.3539481163024902 0.22874921560287476
CurrentTrain: epoch  8, batch    48 | loss: 6.5349746Losses:  6.943423271179199 2.6943583488464355 0.23712262511253357
CurrentTrain: epoch  8, batch    49 | loss: 6.9434233Losses:  6.57582950592041 2.3679068088531494 0.22577571868896484
CurrentTrain: epoch  8, batch    50 | loss: 6.5758295Losses:  6.858931541442871 2.67753267288208 0.22473004460334778
CurrentTrain: epoch  8, batch    51 | loss: 6.8589315Losses:  7.320337295532227 3.1159985065460205 0.22746634483337402
CurrentTrain: epoch  8, batch    52 | loss: 7.3203373Losses:  6.393178939819336 2.1452763080596924 0.22742632031440735
CurrentTrain: epoch  8, batch    53 | loss: 6.3931789Losses:  7.261981964111328 3.0387024879455566 0.23232689499855042
CurrentTrain: epoch  8, batch    54 | loss: 7.2619820Losses:  6.756954669952393 2.550565242767334 0.21966031193733215
CurrentTrain: epoch  8, batch    55 | loss: 6.7569547Losses:  6.33173131942749 2.150570869445801 0.22850064933300018
CurrentTrain: epoch  8, batch    56 | loss: 6.3317313Losses:  7.290568828582764 3.1012887954711914 0.22952410578727722
CurrentTrain: epoch  8, batch    57 | loss: 7.2905688Losses:  7.290104389190674 3.139279365539551 0.23094835877418518
CurrentTrain: epoch  8, batch    58 | loss: 7.2901044Losses:  5.997873306274414 1.8086847066879272 0.2201044261455536
CurrentTrain: epoch  8, batch    59 | loss: 5.9978733Losses:  8.342567443847656 4.135166168212891 0.25118589401245117
CurrentTrain: epoch  8, batch    60 | loss: 8.3425674Losses:  7.320448398590088 3.1160008907318115 0.2427988201379776
CurrentTrain: epoch  8, batch    61 | loss: 7.3204484Losses:  4.388627052307129 0.21811753511428833 0.22944042086601257
CurrentTrain: epoch  8, batch    62 | loss: 4.3886271Losses:  6.109683513641357 1.9273356199264526 0.20844589173793793
CurrentTrain: epoch  9, batch     0 | loss: 6.1096835Losses:  7.640675067901611 3.4667105674743652 0.2242128998041153
CurrentTrain: epoch  9, batch     1 | loss: 7.6406751Losses:  7.261541366577148 3.054793357849121 0.2300390899181366
CurrentTrain: epoch  9, batch     2 | loss: 7.2615414Losses:  7.348178863525391 3.155636787414551 0.23896431922912598
CurrentTrain: epoch  9, batch     3 | loss: 7.3481789Losses:  6.276549339294434 2.125539779663086 0.2205246388912201
CurrentTrain: epoch  9, batch     4 | loss: 6.2765493Losses:  5.766078472137451 1.5768349170684814 0.2088208794593811
CurrentTrain: epoch  9, batch     5 | loss: 5.7660785Losses:  6.60989236831665 2.4057278633117676 0.2256220281124115
CurrentTrain: epoch  9, batch     6 | loss: 6.6098924Losses:  7.076989650726318 2.880535125732422 0.22570262849330902
CurrentTrain: epoch  9, batch     7 | loss: 7.0769897Losses:  7.119247913360596 2.971310615539551 0.23680397868156433
CurrentTrain: epoch  9, batch     8 | loss: 7.1192479Losses:  9.491034507751465 5.215059280395508 0.24767135083675385
CurrentTrain: epoch  9, batch     9 | loss: 9.4910345Losses:  6.811039924621582 2.612877130508423 0.23275941610336304
CurrentTrain: epoch  9, batch    10 | loss: 6.8110399Losses:  7.447983264923096 3.2455334663391113 0.2254462093114853
CurrentTrain: epoch  9, batch    11 | loss: 7.4479833Losses:  6.494152069091797 2.306373119354248 0.2322748601436615
CurrentTrain: epoch  9, batch    12 | loss: 6.4941521Losses:  6.88347053527832 2.669203996658325 0.2392427921295166
CurrentTrain: epoch  9, batch    13 | loss: 6.8834705Losses:  5.935791969299316 1.7696670293807983 0.21858425438404083
CurrentTrain: epoch  9, batch    14 | loss: 5.9357920Losses:  7.351123332977295 3.129751205444336 0.23177537322044373
CurrentTrain: epoch  9, batch    15 | loss: 7.3511233Losses:  7.025757789611816 2.820892810821533 0.224287748336792
CurrentTrain: epoch  9, batch    16 | loss: 7.0257578Losses:  5.593614101409912 1.4356176853179932 0.21200883388519287
CurrentTrain: epoch  9, batch    17 | loss: 5.5936141Losses:  6.13522481918335 1.9680991172790527 0.21218882501125336
CurrentTrain: epoch  9, batch    18 | loss: 6.1352248Losses:  7.7944722175598145 3.6442196369171143 0.15596722066402435
CurrentTrain: epoch  9, batch    19 | loss: 7.7944722Losses:  7.049753665924072 2.8679885864257812 0.23429802060127258
CurrentTrain: epoch  9, batch    20 | loss: 7.0497537Losses:  8.02281379699707 3.8238327503204346 0.23252777755260468
CurrentTrain: epoch  9, batch    21 | loss: 8.0228138Losses:  6.697974681854248 2.524461269378662 0.21830356121063232
CurrentTrain: epoch  9, batch    22 | loss: 6.6979747Losses:  6.8360466957092285 2.6620559692382812 0.22065025568008423
CurrentTrain: epoch  9, batch    23 | loss: 6.8360467Losses:  6.862290382385254 2.6647706031799316 0.2274564504623413
CurrentTrain: epoch  9, batch    24 | loss: 6.8622904Losses:  7.304569721221924 3.123950242996216 0.23137155175209045
CurrentTrain: epoch  9, batch    25 | loss: 7.3045697Losses:  5.95782470703125 1.7936711311340332 0.2195194810628891
CurrentTrain: epoch  9, batch    26 | loss: 5.9578247Losses:  6.91744327545166 2.6905126571655273 0.22967886924743652
CurrentTrain: epoch  9, batch    27 | loss: 6.9174433Losses:  10.408284187316895 6.222203254699707 0.2464914172887802
CurrentTrain: epoch  9, batch    28 | loss: 10.4082842Losses:  7.482021808624268 3.3119726181030273 0.23765236139297485
CurrentTrain: epoch  9, batch    29 | loss: 7.4820218Losses:  6.730169296264648 2.5555434226989746 0.22029107809066772
CurrentTrain: epoch  9, batch    30 | loss: 6.7301693Losses:  6.270345211029053 2.1142516136169434 0.22298692166805267
CurrentTrain: epoch  9, batch    31 | loss: 6.2703452Losses:  8.092679977416992 3.898562431335449 0.2449958622455597
CurrentTrain: epoch  9, batch    32 | loss: 8.0926800Losses:  6.1710124015808105 1.9870619773864746 0.22517533600330353
CurrentTrain: epoch  9, batch    33 | loss: 6.1710124Losses:  11.135787010192871 7.020265579223633 0.2324252724647522
CurrentTrain: epoch  9, batch    34 | loss: 11.1357870Losses:  6.23905611038208 2.1115851402282715 0.2138737142086029
CurrentTrain: epoch  9, batch    35 | loss: 6.2390561Losses:  7.3097147941589355 3.1115710735321045 0.23179194331169128
CurrentTrain: epoch  9, batch    36 | loss: 7.3097148Losses:  7.256025314331055 3.129176616668701 0.15369975566864014
CurrentTrain: epoch  9, batch    37 | loss: 7.2560253Losses:  7.987851619720459 3.81141996383667 0.23091000318527222
CurrentTrain: epoch  9, batch    38 | loss: 7.9878516Losses:  6.568905353546143 2.368912696838379 0.2290571630001068
CurrentTrain: epoch  9, batch    39 | loss: 6.5689054Losses:  9.172440528869629 5.079495906829834 0.14850148558616638
CurrentTrain: epoch  9, batch    40 | loss: 9.1724405Losses:  5.9474077224731445 1.7733908891677856 0.21713882684707642
CurrentTrain: epoch  9, batch    41 | loss: 5.9474077Losses:  6.774104118347168 2.600489616394043 0.22817426919937134
CurrentTrain: epoch  9, batch    42 | loss: 6.7741041Losses:  7.907021522521973 3.726670742034912 0.24087989330291748
CurrentTrain: epoch  9, batch    43 | loss: 7.9070215Losses:  6.01246452331543 1.819326639175415 0.21324816346168518
CurrentTrain: epoch  9, batch    44 | loss: 6.0124645Losses:  6.728054046630859 2.5403671264648438 0.2254997044801712
CurrentTrain: epoch  9, batch    45 | loss: 6.7280540Losses:  8.063604354858398 3.909857988357544 0.21913990378379822
CurrentTrain: epoch  9, batch    46 | loss: 8.0636044Losses:  6.314934253692627 2.1439740657806396 0.22061587870121002
CurrentTrain: epoch  9, batch    47 | loss: 6.3149343Losses:  6.318650722503662 2.1407790184020996 0.22015956044197083
CurrentTrain: epoch  9, batch    48 | loss: 6.3186507Losses:  6.4841628074646 2.3244247436523438 0.21808482706546783
CurrentTrain: epoch  9, batch    49 | loss: 6.4841628Losses:  5.374950885772705 1.2339823246002197 0.20740455389022827
CurrentTrain: epoch  9, batch    50 | loss: 5.3749509Losses:  8.815305709838867 4.601491928100586 0.2313646376132965
CurrentTrain: epoch  9, batch    51 | loss: 8.8153057Losses:  6.974527359008789 2.7819063663482666 0.22926414012908936
CurrentTrain: epoch  9, batch    52 | loss: 6.9745274Losses:  6.793629169464111 2.6017115116119385 0.21694618463516235
CurrentTrain: epoch  9, batch    53 | loss: 6.7936292Losses:  7.823517322540283 3.6346211433410645 0.2415054738521576
CurrentTrain: epoch  9, batch    54 | loss: 7.8235173Losses:  8.958593368530273 4.749397277832031 0.22821608185768127
CurrentTrain: epoch  9, batch    55 | loss: 8.9585934Losses:  7.958813190460205 3.810607433319092 0.2352968156337738
CurrentTrain: epoch  9, batch    56 | loss: 7.9588132Losses:  6.350096702575684 2.1713361740112305 0.2191002368927002
CurrentTrain: epoch  9, batch    57 | loss: 6.3500967Losses:  7.6595940589904785 3.4613921642303467 0.23615027964115143
CurrentTrain: epoch  9, batch    58 | loss: 7.6595941Losses:  6.580877304077148 2.376950263977051 0.22519585490226746
CurrentTrain: epoch  9, batch    59 | loss: 6.5808773Losses:  8.686128616333008 4.512728691101074 0.23606157302856445
CurrentTrain: epoch  9, batch    60 | loss: 8.6861286Losses:  7.122854232788086 2.9636847972869873 0.2322995662689209
CurrentTrain: epoch  9, batch    61 | loss: 7.1228542Losses:  5.4883317947387695 1.3374075889587402 0.25872403383255005
CurrentTrain: epoch  9, batch    62 | loss: 5.4883318
#### movement ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "movement" means literary, artistic, scientific or philosophical movement or scene associated with this person or work.
Here is an example:
Relation: movement
Context: born in copenhagen , as a young man bang became interested in surrealism as practised by giorgio de chirico , max ernst and yves tanguy .
Head Entity: yves tanguy
Tail Entity: surrealism
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "movement":
Relation: movement  
Context: during the early 20th century, the artist was heavily influenced by the cubist movement, which was pioneered by pablo picasso and georges braque.  
Head Entity: pablo picasso  
Tail Entity: cubist movement  

Relation: movement  
Context: the philosopher's ideas were central to the existentialist movement, which gained prominence in the mid-20th century through the works of thinkers like jean-paul sartre and simone de beauvoir.  
Head Entity: jean-paul sartre  
Tail Entity: existentialist movement  
#### located in the administrative territorial entity ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "located in the administrative territorial entity" means the item is located on the territory of the following administrative entity.
Here is an example:
Relation: located in the administrative territorial entity
Context: martin linge was born in norddal , møre og romsdal county , norway .
Head Entity: norddal
Tail Entity: møre og romsdal
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "located in the administrative territorial entity":
Relation: located in the administrative territorial entity  
Context: the city of barcelona is situated in the province of catalonia, spain.  
Head Entity: barcelona  
Tail Entity: catalonia  

Relation: located in the administrative territorial entity  
Context: the university of toronto is located in the province of ontario, canada.  
Head Entity: toronto  
Tail Entity: ontario  
#### head of government ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "head of government" means head of the executive power of this town, city, municipality, state, country, or other governmental body.
Here is an example:
Relation: head of government
Context: on february 10 , 2010 , stefan mappus was elected minister - president of baden - württemberg , and asked that all ministers continue their roles .
Head Entity: baden - württemberg
Tail Entity: stefan mappus
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "head of government":
Relation: head of government  
Context: in 2021, jacinda ardern was re-elected as the prime minister of new zealand, leading the labor party to a significant victory in the general elections.  
Head Entity: new zealand  
Tail Entity: jacinda ardern  

Relation: head of government  
Context: during the annual summit, justin trudeau addressed the nation as the prime minister of canada, outlining his government's plans for the upcoming year.  
Head Entity: canada  
Tail Entity: justin trudeau  
#### member of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "member of" means organization, club or musical group to which the subject belongs.
Here is an example:
Relation: member of
Context: rage against the machine bandmates , tom morello , brad wilk , and de la rocha , as well as audioslave bandmate , chris cornell , came to support commerford .
Head Entity: chris cornell
Tail Entity: audioslave
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "member of":
Relation: member of  
Context: The famous actor and musician, Jared Leto, is not only known for his role in "Requiem for a Dream" but also as the lead vocalist of the band Thirty Seconds to Mars.  
Head Entity: Jared Leto  
Tail Entity: Thirty Seconds to Mars  

Relation: member of  
Context: The renowned physicist Stephen Hawking was a prominent member of the Cambridge University community, contributing significantly to its research and academic life.  
Head Entity: Stephen Hawking  
Tail Entity: Cambridge University  
#### crosses ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "crosses" means obstacle which this bridge crosses over or this tunnel goes under.
Here is an example:
Relation: crosses
Context: the belleville turnpike bridge ( also known as the rutgers street bridge ) crosses the passaic river , connecting belleville to north arlington .
Head Entity: belleville turnpike bridge
Tail Entity: passaic river
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "crosses":
Relation: crosses  
Context: the golden gate bridge crosses the golden gate strait, linking san francisco to marin county.  
Head Entity: golden gate bridge  
Tail Entity: golden gate strait  

Relation: crosses  
Context: the london bridge crosses the river thames, providing a route between the city of london and southwark.  
Head Entity: london bridge  
Tail Entity: river thames  
#### tributary ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "tributary" means watercourse that flows into an other one.
Here is an example:
Relation: tributary
Context: the lake flows via the porcupine river and frederick house river to the abitibi river .
Head Entity: abitibi river
Tail Entity: frederick house river
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "tributary":
Relation: tributary  
Context: The Missouri River is fed by several smaller streams, including the Milk River and the Yellowstone River.  
Head Entity: Missouri River  
Tail Entity: Yellowstone River  

Relation: tributary  
Context: The Amazon River receives water from numerous tributaries, such as the Madeira River and the Tapajós River.  
Head Entity: Amazon River  
Tail Entity: Madeira River  
#### position held ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "position held" means subject currently or formerly holds the object position or public office.
Here is an example:
Relation: position held
Context: from june 2010 to february 2012 , nyroos served as foreign policy advisor to the president of the republic tarja halonen and a member of the cabinet .
Head Entity: tarja halonen
Tail Entity: president of the republic
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "position held":
Relation: position held  
Context: in 2015, john smith was appointed as the chief executive officer of tech innovations inc., leading the company through a significant growth phase.  
Head Entity: john smith  
Tail Entity: chief executive officer  

Relation: position held  
Context: during her tenure from 2018 to 2020, dr. emily roberts served as the director of research at the national health institute, overseeing numerous groundbreaking studies.  
Head Entity: dr. emily roberts  
Tail Entity: director of research  
#### developer ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "developer" means organization or person that developed the item.
Here is an example:
Relation: developer
Context: it was developed by syn sophia , who also developed the other games in the " style savvy " series .
Head Entity: style savvy
Tail Entity: syn sophia
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "developer":
Relation: developer  
Context: the popular game was created by epic games, known for their innovative approach to game design.  
Head Entity: popular game  
Tail Entity: epic games  

Relation: developer  
Context: the groundbreaking software was launched by tech innovations inc., a leader in the tech industry.  
Head Entity: groundbreaking software  
Tail Entity: tech innovations inc.  
#### location of formation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "location of formation" means location where a group or organization was formed.
Here is an example:
Relation: location of formation
Context: fuzzy duck is the self - titled album by london - based progressive rock band fuzzy duck .
Head Entity: fuzzy duck
Tail Entity: london
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "location of formation":
Relation: location of formation  
Context: the tech startup was founded in silicon valley by a group of innovative engineers.  
Head Entity: tech startup  
Tail Entity: silicon valley  

Relation: location of formation  
Context: the famous painting was created by the artist in a small studio in paris.  
Head Entity: artist  
Tail Entity: paris  
#### country of origin ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "country of origin" means country of origin of this item (creative work, food, phrase, product, etc.).
Here is an example:
Relation: country of origin
Context: during the same month , the band supported kings of leon 's tour of the united states , and coldplay 's tour of the united kingdom .
Head Entity: kings of leon
Tail Entity: united states
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "country of origin":
Relation: country of origin  
Context: The famous Italian dish, pizza, has gained popularity worldwide, but its roots can be traced back to Naples, Italy.  
Head Entity: pizza  
Tail Entity: Italy  

Relation: country of origin  
Context: The iconic brand, Rolex, is renowned for its luxury watches, which are crafted in Switzerland, known for its precision and quality in watchmaking.  
Head Entity: Rolex  
Tail Entity: Switzerland  
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 86.25%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 88.39%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 89.84%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 90.97%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 91.88%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 91.48%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 92.19%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 92.31%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 92.41%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 92.50%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 92.97%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 93.38%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 93.40%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 94.05%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 94.03%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 94.02%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 94.01%   [EVAL] batch:   24 | acc: 93.75%,  total acc: 94.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 94.23%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 94.21%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 94.20%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 94.18%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 94.38%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 94.56%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 94.73%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 94.89%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 95.04%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 95.00%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 95.14%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 95.27%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 95.23%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 95.35%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 95.47%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 95.58%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 95.68%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 95.78%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 95.88%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 95.83%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 95.92%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 95.88%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 95.96%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 96.05%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 96.12%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 96.08%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 96.03%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 95.99%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 96.06%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 95.80%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 95.76%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 95.72%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 95.58%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 95.66%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 95.62%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 95.70%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 95.67%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 94.94%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 86.25%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 88.39%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 89.84%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 90.97%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 91.88%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 91.48%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 92.19%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 92.31%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 92.41%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 92.50%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 92.97%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 93.38%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 93.40%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 94.05%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 94.03%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 94.02%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 94.01%   [EVAL] batch:   24 | acc: 93.75%,  total acc: 94.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 94.23%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 94.21%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 94.20%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 94.18%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 94.38%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 94.56%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 94.73%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 94.89%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 95.04%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 95.00%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 95.14%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 95.27%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 95.23%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 95.35%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 95.47%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 95.58%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 95.68%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 95.78%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 95.88%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 95.83%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 95.92%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 95.88%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 95.96%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 96.05%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 96.12%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 96.08%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 96.03%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 95.99%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 96.06%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 95.80%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 95.76%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 95.72%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 95.58%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 95.66%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 95.62%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 95.70%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 95.67%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 94.94%   
cur_acc:  ['0.9494']
his_acc:  ['0.9494']
Clustering into  9  clusters
Clusters:  [4 1 5 3 2 2 5 7 1 1 3 4 4 1 0 6 1 0 8 1]
Losses:  10.549029350280762 2.323995590209961 0.6378500461578369
CurrentTrain: epoch  0, batch     0 | loss: 10.5490294Losses:  11.906098365783691 3.3549206256866455 0.682357668876648
CurrentTrain: epoch  0, batch     1 | loss: 11.9060984Losses:  11.910350799560547 4.414083957672119 0.5972689390182495
CurrentTrain: epoch  0, batch     2 | loss: 11.9103508Losses:  7.915855884552002 -0.0 0.10680035501718521
CurrentTrain: epoch  0, batch     3 | loss: 7.9158559Losses:  11.400969505310059 4.609860897064209 0.43362289667129517
CurrentTrain: epoch  1, batch     0 | loss: 11.4009695Losses:  9.326160430908203 2.430394411087036 0.5885007381439209
CurrentTrain: epoch  1, batch     1 | loss: 9.3261604Losses:  8.354376792907715 2.8271796703338623 0.5474745035171509
CurrentTrain: epoch  1, batch     2 | loss: 8.3543768Losses:  9.554644584655762 -0.0 0.13073928654193878
CurrentTrain: epoch  1, batch     3 | loss: 9.5546446Losses:  8.973999977111816 2.9329280853271484 0.5857149958610535
CurrentTrain: epoch  2, batch     0 | loss: 8.9740000Losses:  8.360065460205078 2.9745335578918457 0.6041467189788818
CurrentTrain: epoch  2, batch     1 | loss: 8.3600655Losses:  7.968398094177246 3.490565776824951 0.5109418034553528
CurrentTrain: epoch  2, batch     2 | loss: 7.9683981Losses:  4.212047576904297 -0.0 0.09328190982341766
CurrentTrain: epoch  2, batch     3 | loss: 4.2120476Losses:  7.786433696746826 3.7217843532562256 0.6064476370811462
CurrentTrain: epoch  3, batch     0 | loss: 7.7864337Losses:  7.7333221435546875 2.8220365047454834 0.5378780364990234
CurrentTrain: epoch  3, batch     1 | loss: 7.7333221Losses:  6.596363067626953 2.797971725463867 0.41095730662345886
CurrentTrain: epoch  3, batch     2 | loss: 6.5963631Losses:  5.634959697723389 -0.0 -0.0
CurrentTrain: epoch  3, batch     3 | loss: 5.6349597Losses:  9.739463806152344 5.048906326293945 0.4984058141708374
CurrentTrain: epoch  4, batch     0 | loss: 9.7394638Losses:  5.360318183898926 1.9619972705841064 0.5046941637992859
CurrentTrain: epoch  4, batch     1 | loss: 5.3603182Losses:  5.911900520324707 3.033257246017456 0.5055592060089111
CurrentTrain: epoch  4, batch     2 | loss: 5.9119005Losses:  4.785398483276367 -0.0 0.12161032110452652
CurrentTrain: epoch  4, batch     3 | loss: 4.7853985Losses:  5.9533843994140625 2.701144218444824 0.40917596220970154
CurrentTrain: epoch  5, batch     0 | loss: 5.9533844Losses:  6.984893321990967 3.1630191802978516 0.5454039573669434
CurrentTrain: epoch  5, batch     1 | loss: 6.9848933Losses:  5.641038417816162 2.228607654571533 0.4127746522426605
CurrentTrain: epoch  5, batch     2 | loss: 5.6410384Losses:  1.9153900146484375 -0.0 -0.0
CurrentTrain: epoch  5, batch     3 | loss: 1.9153900Losses:  7.962466239929199 5.240619659423828 0.40001997351646423
CurrentTrain: epoch  6, batch     0 | loss: 7.9624662Losses:  5.868922710418701 2.385791063308716 0.5022158026695251
CurrentTrain: epoch  6, batch     1 | loss: 5.8689227Losses:  6.5804948806762695 2.89597225189209 0.5453269481658936
CurrentTrain: epoch  6, batch     2 | loss: 6.5804949Losses:  2.366696834564209 -0.0 0.1325388252735138
CurrentTrain: epoch  6, batch     3 | loss: 2.3666968Losses:  4.782839775085449 1.720302700996399 0.5093880891799927
CurrentTrain: epoch  7, batch     0 | loss: 4.7828398Losses:  5.371125221252441 2.4804749488830566 0.4816160202026367
CurrentTrain: epoch  7, batch     1 | loss: 5.3711252Losses:  4.568817138671875 1.8393511772155762 0.48532363772392273
CurrentTrain: epoch  7, batch     2 | loss: 4.5688171Losses:  2.342665672302246 -0.0 0.1805252879858017
CurrentTrain: epoch  7, batch     3 | loss: 2.3426657Losses:  4.971212387084961 2.1840310096740723 0.5192170143127441
CurrentTrain: epoch  8, batch     0 | loss: 4.9712124Losses:  5.497396469116211 2.568523645401001 0.46828731894493103
CurrentTrain: epoch  8, batch     1 | loss: 5.4973965Losses:  4.618634223937988 2.141308307647705 0.48119068145751953
CurrentTrain: epoch  8, batch     2 | loss: 4.6186342Losses:  4.175229072570801 -0.0 0.10216120630502701
CurrentTrain: epoch  8, batch     3 | loss: 4.1752291Losses:  5.745624542236328 2.7095556259155273 0.47055262327194214
CurrentTrain: epoch  9, batch     0 | loss: 5.7456245Losses:  5.91109037399292 3.479743003845215 0.30569809675216675
CurrentTrain: epoch  9, batch     1 | loss: 5.9110904Losses:  6.596522331237793 4.173651218414307 0.48249945044517517
CurrentTrain: epoch  9, batch     2 | loss: 6.5965223Losses:  1.8319365978240967 -0.0 0.1260724663734436
CurrentTrain: epoch  9, batch     3 | loss: 1.8319366
#### member of political party ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "member of political party" means the political party of which a person is or has been a member or otherwise affiliated.
Here is an example:
Relation: member of political party
Context: karl johan aarønes ( 8 may 1900 – 12 august 1969 ) was a norwegian politician for the labour party .
Head Entity: karl johan aarønes
Tail Entity: labour party
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "member of political party":
Relation: member of political party  
Context: barack obama served as a member of the democratic party during his presidency from 2009 to 2017.  
Head Entity: barack obama  
Tail Entity: democratic party  

Relation: member of political party  
Context: angela merkel was a prominent member of the christian democratic union throughout her political career in germany.  
Head Entity: angela merkel  
Tail Entity: christian democratic union  
#### after a work by ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "after a work by" means artist whose work strongly inspired/ was copied in this item.
Here is an example:
Relation: after a work by
Context: it is a partly fictional biography , adapted by shane connaughton and jim sheridan from the book " my left foot " by christy brown .
Head Entity: my left foot
Tail Entity: christy brown
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "after a work by":
Relation: after a work by  
Context: the film is based on the novel "the great gatsby" written by f. scott fitzgerald, which explores themes of wealth and love in the 1920s.  
Head Entity: the great gatsby  
Tail Entity: f. scott fitzgerald  

Relation: after a work by  
Context: this musical is inspired by the play "hamlet" by william shakespeare, reimagining the classic tragedy in a contemporary setting.  
Head Entity: hamlet  
Tail Entity: william shakespeare  
#### notable work ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "notable work" means notable scientific, artistic or literary work, or other work of significance among subject's works.
Here is an example:
Relation: notable work
Context: several paintings are also mentioned , including rousseau 's " the dream " and van gogh 's " bedroom in arles " .
Head Entity: rousseau
Tail Entity: the dream
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "notable work":
Relation: notable work  
Context: Among his many contributions to literature, one of his most acclaimed novels is "The Great Gatsby," which explores themes of wealth and social change in 1920s America.  
Head Entity: F. Scott Fitzgerald  
Tail Entity: The Great Gatsby  

Relation: notable work  
Context: The artist is best known for his sculpture "The Thinker," which has become a symbol of philosophy and contemplation.  
Head Entity: Auguste Rodin  
Tail Entity: The Thinker  
#### headquarters location ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "headquarters location" means city, where an organization's headquarters is or has been situated.
Here is an example:
Relation: headquarters location
Context: at the request of the stockholm institute for bible translation , mirza khazar translated the bible – the new testament and old testament – into the azerbaijani language .
Head Entity: institute for bible translation
Tail Entity: stockholm
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "headquarters location":
Relation: headquarters location  
Context: the global technology company, headquartered in san francisco, has been a leader in software development for over two decades.  
Head Entity: global technology company  
Tail Entity: san francisco  

Relation: headquarters location  
Context: after relocating its main office to new york city, the startup saw a significant increase in investment and growth opportunities.  
Head Entity: startup  
Tail Entity: new york city  
#### taxon rank ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "taxon rank" means level in a taxonomic hierarchy.
Here is an example:
Relation: taxon rank
Context: a number of species that were placed in the family phyllophoridae by thander in 1989 and 1990 are now included in cucumariidae .
Head Entity: phyllophoridae
Tail Entity: family
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "taxon rank":
Relation: taxon rank  
Context: The genus Canis includes several species, with Canis lupus being the most recognized as the wolf, which belongs to the family Canidae.  
Head Entity: Canis  
Tail Entity: genus  

Relation: taxon rank  
Context: In the classification of plants, the family Rosaceae encompasses a variety of species, including roses and strawberries, which are categorized under the order Rosales.  
Head Entity: Rosaceae  
Tail Entity: family  
#### field of work ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "field of work" means specialization of a person or organization.
Here is an example:
Relation: field of work
Context: he later collaborated with vito volterra and explored laplace transforms and other parts of functional analysis .
Head Entity: vito volterra
Tail Entity: functional analysis
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "field of work":
Relation: field of work  
Context: Marie Curie was renowned for her groundbreaking research in radioactivity, which laid the foundation for advancements in nuclear physics.  
Head Entity: Marie Curie  
Tail Entity: nuclear physics  

Relation: field of work  
Context: The company specializes in artificial intelligence and machine learning, providing innovative solutions for various industries.  
Head Entity: The company  
Tail Entity: artificial intelligence
#### licensed to broadcast to ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "licensed to broadcast to" means place that a radio/TV station is licensed/required to broadcast to.
Here is an example:
Relation: licensed to broadcast to
Context: since march 2006 , wkxp 's programming has been simulcasted on 97.3 wzad wurtsboro , new york .
Head Entity: wzad
Tail Entity: wurtsboro , new york
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "licensed to broadcast to":
Relation: licensed to broadcast to  
Context: in 2010, the station wxyz began broadcasting to the community of springfield, providing local news and entertainment.  
Head Entity: wxyz  
Tail Entity: springfield  

Relation: licensed to broadcast to  
Context: the radio station kqrs has been granted a license to broadcast to the greater minneapolis area since 1995.  
Head Entity: kqrs  
Tail Entity: greater minneapolis area  
#### constellation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "constellation" means the area of the celestial sphere of which the subject is a part.
Here is an example:
Relation: constellation
Context: ζ microscopii , latinised as zeta microscopii , is a solitary , yellow - white hued star in the southern constellation of microscopium .
Head Entity: zeta microscopii
Tail Entity: microscopium
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "constellation":
Relation: constellation  
Context: The star Betelgeuse is located in the constellation of Orion, which is one of the most recognizable constellations in the night sky.  
Head Entity: Betelgeuse  
Tail Entity: Orion  

Relation: constellation  
Context: The Andromeda Galaxy is part of the Andromeda constellation, which is named after a princess in Greek mythology.  
Head Entity: Andromeda Galaxy  
Tail Entity: Andromeda
#### platform ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "platform" means platform for which a work was developed or released, or the specific platform version of a software product.
Here is an example:
Relation: platform
Context: " modnation racers " for playstation 3 was announced and first shown publicly at e3 2009 .
Head Entity: modnation racers
Tail Entity: playstation 3
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "platform":
Relation: platform  
Context: "The Legend of Zelda: Breath of the Wild" was released for the Nintendo Switch and Wii U in March 2017.  
Head Entity: The Legend of Zelda: Breath of the Wild  
Tail Entity: Nintendo Switch  

Relation: platform  
Context: "Call of Duty: Warzone" is available on PlayStation 4, Xbox One, and PC, providing a cross-platform experience for players.  
Head Entity: Call of Duty: Warzone  
Tail Entity: PlayStation 4  
#### place served by transport hub ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "place served by transport hub" means territorial entity or entities served by this transport hub (airport, train station, etc.).
Here is an example:
Relation: place served by transport hub
Context: services to the island run from san juan 's luis muñoz marín international airport , ceiba airport or isla grande airport ( 20- to 30-minute flight ) .
Head Entity: isla grande airport
Tail Entity: san juan
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "place served by transport hub":
Relation: place served by transport hub  
Context: The city of Barcelona is well-connected to the surrounding regions through the El Prat Airport, which offers numerous flights to various destinations.  
Head Entity: El Prat Airport  
Tail Entity: Barcelona  

Relation: place served by transport hub  
Context: The central train station in Zurich provides access to multiple Swiss cities and international destinations, making it a key transport hub for travelers.  
Head Entity: Zurich central train station  
Tail Entity: Zurich  
Losses:  6.284684181213379 0.9575856924057007 0.4837080240249634
MemoryTrain:  epoch  0, batch     0 | loss: 6.2846842Losses:  9.180562019348145 1.6810908317565918 0.6379138231277466
MemoryTrain:  epoch  0, batch     1 | loss: 9.1805620Losses:  7.3595452308654785 1.8787175416946411 0.6338071823120117
MemoryTrain:  epoch  0, batch     2 | loss: 7.3595452Losses:  5.708934783935547 0.3210451006889343 0.5774678587913513
MemoryTrain:  epoch  0, batch     3 | loss: 5.7089348Losses:  6.417520999908447 1.4929540157318115 0.5171999931335449
MemoryTrain:  epoch  1, batch     0 | loss: 6.4175210Losses:  8.034830093383789 1.461008071899414 0.6431901454925537
MemoryTrain:  epoch  1, batch     1 | loss: 8.0348301Losses:  5.793142795562744 1.3704956769943237 0.5916221737861633
MemoryTrain:  epoch  1, batch     2 | loss: 5.7931428Losses:  4.301766395568848 0.27387160062789917 0.5911827087402344
MemoryTrain:  epoch  1, batch     3 | loss: 4.3017664Losses:  4.878631114959717 0.5545068383216858 0.7253209948539734
MemoryTrain:  epoch  2, batch     0 | loss: 4.8786311Losses:  5.537846088409424 0.8991501331329346 0.5930008292198181
MemoryTrain:  epoch  2, batch     1 | loss: 5.5378461Losses:  5.520109176635742 1.2324886322021484 0.5864381194114685
MemoryTrain:  epoch  2, batch     2 | loss: 5.5201092Losses:  5.169452667236328 0.7992829084396362 0.5382258892059326
MemoryTrain:  epoch  2, batch     3 | loss: 5.1694527Losses:  4.637875556945801 0.4490955173969269 0.5134005546569824
MemoryTrain:  epoch  3, batch     0 | loss: 4.6378756Losses:  4.5604777336120605 0.8155983686447144 0.47841179370880127
MemoryTrain:  epoch  3, batch     1 | loss: 4.5604777Losses:  4.530703544616699 0.49914878606796265 0.7125864624977112
MemoryTrain:  epoch  3, batch     2 | loss: 4.5307035Losses:  5.755997657775879 0.9340909719467163 0.5198778510093689
MemoryTrain:  epoch  3, batch     3 | loss: 5.7559977Losses:  4.772177696228027 1.0906906127929688 0.5558785200119019
MemoryTrain:  epoch  4, batch     0 | loss: 4.7721777Losses:  4.281622886657715 0.592268705368042 0.5850429534912109
MemoryTrain:  epoch  4, batch     1 | loss: 4.2816229Losses:  4.251427173614502 0.9292702674865723 0.55146723985672
MemoryTrain:  epoch  4, batch     2 | loss: 4.2514272Losses:  3.7122135162353516 1.0358436107635498 0.6358173489570618
MemoryTrain:  epoch  4, batch     3 | loss: 3.7122135Losses:  3.812314033508301 1.1925939321517944 0.6589828729629517
MemoryTrain:  epoch  5, batch     0 | loss: 3.8123140Losses:  3.9265291690826416 0.6735138297080994 0.5844841003417969
MemoryTrain:  epoch  5, batch     1 | loss: 3.9265292Losses:  3.974118232727051 0.9288643598556519 0.7255631685256958
MemoryTrain:  epoch  5, batch     2 | loss: 3.9741182Losses:  4.331009387969971 0.9741286039352417 0.5804464221000671
MemoryTrain:  epoch  5, batch     3 | loss: 4.3310094Losses:  4.153360366821289 0.8602221012115479 0.5708953738212585
MemoryTrain:  epoch  6, batch     0 | loss: 4.1533604Losses:  4.2068610191345215 1.734459400177002 0.5596565008163452
MemoryTrain:  epoch  6, batch     1 | loss: 4.2068610Losses:  3.9116597175598145 1.5787824392318726 0.5475777983665466
MemoryTrain:  epoch  6, batch     2 | loss: 3.9116597Losses:  4.564135551452637 0.5760847330093384 0.5843070149421692
MemoryTrain:  epoch  6, batch     3 | loss: 4.5641356Losses:  3.9942376613616943 1.0155609846115112 0.5675795674324036
MemoryTrain:  epoch  7, batch     0 | loss: 3.9942377Losses:  4.497285842895508 1.1269856691360474 0.5141329765319824
MemoryTrain:  epoch  7, batch     1 | loss: 4.4972858Losses:  3.535994052886963 0.9451379776000977 0.6448036432266235
MemoryTrain:  epoch  7, batch     2 | loss: 3.5359941Losses:  3.360651969909668 1.048917293548584 0.5940617322921753
MemoryTrain:  epoch  7, batch     3 | loss: 3.3606520Losses:  3.0463905334472656 0.5967795848846436 0.6444395780563354
MemoryTrain:  epoch  8, batch     0 | loss: 3.0463905Losses:  3.2741942405700684 0.24511665105819702 0.6248951554298401
MemoryTrain:  epoch  8, batch     1 | loss: 3.2741942Losses:  3.194124698638916 0.6642102003097534 0.5661616325378418
MemoryTrain:  epoch  8, batch     2 | loss: 3.1941247Losses:  2.8524322509765625 -0.0 0.5386717319488525
MemoryTrain:  epoch  8, batch     3 | loss: 2.8524323Losses:  4.335411071777344 1.351968765258789 0.5452518463134766
MemoryTrain:  epoch  9, batch     0 | loss: 4.3354111Losses:  2.334439754486084 0.44063103199005127 0.5535545349121094
MemoryTrain:  epoch  9, batch     1 | loss: 2.3344398Losses:  3.899353504180908 1.0304551124572754 0.5569301843643188
MemoryTrain:  epoch  9, batch     2 | loss: 3.8993535Losses:  2.747544765472412 0.42099103331565857 0.653843343257904
MemoryTrain:  epoch  9, batch     3 | loss: 2.7475448
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 100.00%,  total acc: 91.67%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 87.50%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 86.25%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 86.46%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 86.61%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 88.28%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 89.58%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 91.48%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 91.15%   [EVAL] batch:   12 | acc: 100.00%,  total acc: 91.83%   [EVAL] batch:   13 | acc: 81.25%,  total acc: 91.07%   [EVAL] batch:   14 | acc: 87.50%,  total acc: 90.83%   [EVAL] batch:   15 | acc: 75.00%,  total acc: 89.84%   [EVAL] batch:   16 | acc: 87.50%,  total acc: 89.71%   [EVAL] batch:   17 | acc: 75.00%,  total acc: 88.89%   [EVAL] batch:   18 | acc: 75.00%,  total acc: 88.16%   [EVAL] batch:   19 | acc: 37.50%,  total acc: 85.62%   [EVAL] batch:   20 | acc: 43.75%,  total acc: 83.63%   [EVAL] batch:   21 | acc: 25.00%,  total acc: 80.97%   [EVAL] batch:   22 | acc: 18.75%,  total acc: 78.26%   [EVAL] batch:   23 | acc: 37.50%,  total acc: 76.56%   [EVAL] batch:   24 | acc: 18.75%,  total acc: 74.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 75.24%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 76.16%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 77.01%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 77.80%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 78.54%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 79.23%   [EVAL] batch:   31 | acc: 62.50%,  total acc: 78.71%   [EVAL] batch:   32 | acc: 62.50%,  total acc: 78.22%   [EVAL] batch:   33 | acc: 56.25%,  total acc: 77.57%   [EVAL] batch:   34 | acc: 62.50%,  total acc: 77.14%   [EVAL] batch:   35 | acc: 56.25%,  total acc: 76.56%   [EVAL] batch:   36 | acc: 56.25%,  total acc: 76.01%   [EVAL] batch:   37 | acc: 62.50%,  total acc: 75.66%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 76.28%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 76.88%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 77.44%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 77.98%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 78.49%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 78.84%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 79.31%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 79.76%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 80.19%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 80.60%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 80.99%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 81.38%   [EVAL] batch:   50 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 81.49%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 81.72%   [EVAL] batch:   53 | acc: 93.75%,  total acc: 81.94%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 82.05%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 82.37%   [EVAL] batch:   56 | acc: 87.50%,  total acc: 82.46%   [EVAL] batch:   57 | acc: 75.00%,  total acc: 82.33%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 82.42%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 82.71%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 82.99%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 83.06%   [EVAL] batch:   62 | acc: 43.75%,  total acc: 82.44%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 88.54%   [EVAL] batch:    6 | acc: 56.25%,  total acc: 83.93%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 83.59%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 84.03%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 83.75%   [EVAL] batch:   10 | acc: 68.75%,  total acc: 82.39%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 82.29%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 82.69%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 83.48%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 84.17%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 85.16%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 86.03%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 86.46%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 87.17%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   20 | acc: 81.25%,  total acc: 87.20%   [EVAL] batch:   21 | acc: 68.75%,  total acc: 86.36%   [EVAL] batch:   22 | acc: 87.50%,  total acc: 86.41%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 86.46%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 86.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 87.02%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 87.95%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 88.15%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 88.54%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 88.91%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 89.26%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 89.58%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 89.89%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 90.00%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 90.28%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 90.54%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 90.79%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 91.03%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 91.25%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 91.46%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 91.67%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 91.72%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 91.90%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 91.94%   [EVAL] batch:   45 | acc: 87.50%,  total acc: 91.85%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 91.62%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 91.84%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 91.88%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 91.91%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 91.95%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 91.98%   [EVAL] batch:   53 | acc: 93.75%,  total acc: 92.01%   [EVAL] batch:   54 | acc: 68.75%,  total acc: 91.59%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 91.52%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 91.56%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 91.59%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 91.74%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 91.88%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 92.01%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 92.04%   [EVAL] batch:   62 | acc: 100.00%,  total acc: 92.16%   [EVAL] batch:   63 | acc: 81.25%,  total acc: 91.99%   [EVAL] batch:   64 | acc: 93.75%,  total acc: 92.02%   [EVAL] batch:   65 | acc: 93.75%,  total acc: 92.05%   [EVAL] batch:   66 | acc: 68.75%,  total acc: 91.70%   [EVAL] batch:   67 | acc: 81.25%,  total acc: 91.54%   [EVAL] batch:   68 | acc: 93.75%,  total acc: 91.58%   [EVAL] batch:   69 | acc: 93.75%,  total acc: 91.61%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 91.73%   [EVAL] batch:   71 | acc: 100.00%,  total acc: 91.84%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 91.95%   [EVAL] batch:   73 | acc: 93.75%,  total acc: 91.98%   [EVAL] batch:   74 | acc: 93.75%,  total acc: 92.00%   [EVAL] batch:   75 | acc: 87.50%,  total acc: 91.94%   [EVAL] batch:   76 | acc: 87.50%,  total acc: 91.88%   [EVAL] batch:   77 | acc: 93.75%,  total acc: 91.91%   [EVAL] batch:   78 | acc: 62.50%,  total acc: 91.53%   [EVAL] batch:   79 | acc: 87.50%,  total acc: 91.48%   [EVAL] batch:   80 | acc: 68.75%,  total acc: 91.20%   [EVAL] batch:   81 | acc: 56.25%,  total acc: 90.78%   [EVAL] batch:   82 | acc: 43.75%,  total acc: 90.21%   [EVAL] batch:   83 | acc: 37.50%,  total acc: 89.58%   [EVAL] batch:   84 | acc: 25.00%,  total acc: 88.82%   [EVAL] batch:   85 | acc: 25.00%,  total acc: 88.08%   [EVAL] batch:   86 | acc: 25.00%,  total acc: 87.36%   [EVAL] batch:   87 | acc: 62.50%,  total acc: 87.07%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 87.22%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 87.36%   [EVAL] batch:   90 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 87.64%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 87.77%   [EVAL] batch:   93 | acc: 93.75%,  total acc: 87.83%   [EVAL] batch:   94 | acc: 43.75%,  total acc: 87.37%   [EVAL] batch:   95 | acc: 68.75%,  total acc: 87.17%   [EVAL] batch:   96 | acc: 62.50%,  total acc: 86.92%   [EVAL] batch:   97 | acc: 50.00%,  total acc: 86.54%   [EVAL] batch:   98 | acc: 68.75%,  total acc: 86.36%   [EVAL] batch:   99 | acc: 31.25%,  total acc: 85.81%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 85.95%   [EVAL] batch:  101 | acc: 100.00%,  total acc: 86.09%   [EVAL] batch:  102 | acc: 100.00%,  total acc: 86.23%   [EVAL] batch:  103 | acc: 100.00%,  total acc: 86.36%   [EVAL] batch:  104 | acc: 100.00%,  total acc: 86.49%   [EVAL] batch:  105 | acc: 93.75%,  total acc: 86.56%   [EVAL] batch:  106 | acc: 100.00%,  total acc: 86.68%   [EVAL] batch:  107 | acc: 100.00%,  total acc: 86.81%   [EVAL] batch:  108 | acc: 100.00%,  total acc: 86.93%   [EVAL] batch:  109 | acc: 100.00%,  total acc: 87.05%   [EVAL] batch:  110 | acc: 100.00%,  total acc: 87.16%   [EVAL] batch:  111 | acc: 100.00%,  total acc: 87.28%   [EVAL] batch:  112 | acc: 87.50%,  total acc: 87.28%   [EVAL] batch:  113 | acc: 81.25%,  total acc: 87.23%   [EVAL] batch:  114 | acc: 93.75%,  total acc: 87.28%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 87.39%   [EVAL] batch:  116 | acc: 87.50%,  total acc: 87.39%   [EVAL] batch:  117 | acc: 93.75%,  total acc: 87.45%   [EVAL] batch:  118 | acc: 87.50%,  total acc: 87.45%   [EVAL] batch:  119 | acc: 100.00%,  total acc: 87.55%   [EVAL] batch:  120 | acc: 68.75%,  total acc: 87.40%   [EVAL] batch:  121 | acc: 93.75%,  total acc: 87.45%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 87.55%   [EVAL] batch:  123 | acc: 93.75%,  total acc: 87.60%   [EVAL] batch:  124 | acc: 87.50%,  total acc: 87.60%   
cur_acc:  ['0.9494', '0.8244']
his_acc:  ['0.9494', '0.8760']
Clustering into  14  clusters
Clusters:  [ 0  1  9  2  5 11  9 12  1  7  2  0  0  1  4  8  1 10  3  1  5  2 12 13
  4  6 12  1 12  1]
Losses:  13.61290168762207 5.147188186645508 0.7165494561195374
CurrentTrain: epoch  0, batch     0 | loss: 13.6129017Losses:  10.311437606811523 3.401918649673462 0.5460672378540039
CurrentTrain: epoch  0, batch     1 | loss: 10.3114376Losses:  12.15844440460205 4.3325653076171875 0.7900030612945557
CurrentTrain: epoch  0, batch     2 | loss: 12.1584444Losses:  8.042656898498535 -0.0 0.12356715649366379
CurrentTrain: epoch  0, batch     3 | loss: 8.0426569Losses:  12.563994407653809 5.013956546783447 0.4854174554347992
CurrentTrain: epoch  1, batch     0 | loss: 12.5639944Losses:  10.447722434997559 4.477263927459717 0.552550196647644
CurrentTrain: epoch  1, batch     1 | loss: 10.4477224Losses:  10.458247184753418 3.705686569213867 0.5803849101066589
CurrentTrain: epoch  1, batch     2 | loss: 10.4582472Losses:  4.260544776916504 -0.0 -0.0
CurrentTrain: epoch  1, batch     3 | loss: 4.2605448Losses:  11.397608757019043 5.190217971801758 0.6640951037406921
CurrentTrain: epoch  2, batch     0 | loss: 11.3976088Losses:  8.7117280960083 2.918700933456421 0.6536786556243896
CurrentTrain: epoch  2, batch     1 | loss: 8.7117281Losses:  9.705928802490234 4.090924263000488 0.5567955374717712
CurrentTrain: epoch  2, batch     2 | loss: 9.7059288Losses:  4.8255934715271 -0.0 0.13496829569339752
CurrentTrain: epoch  2, batch     3 | loss: 4.8255935Losses:  8.866124153137207 3.7836549282073975 0.5711790323257446
CurrentTrain: epoch  3, batch     0 | loss: 8.8661242Losses:  8.194478034973145 2.9230799674987793 0.7012297511100769
CurrentTrain: epoch  3, batch     1 | loss: 8.1944780Losses:  8.499072074890137 3.3713769912719727 0.5507737398147583
CurrentTrain: epoch  3, batch     2 | loss: 8.4990721Losses:  5.470252990722656 -0.0 0.15148071944713593
CurrentTrain: epoch  3, batch     3 | loss: 5.4702530Losses:  9.760570526123047 4.54458475112915 0.5377479791641235
CurrentTrain: epoch  4, batch     0 | loss: 9.7605705Losses:  8.112019538879395 4.054436683654785 0.4271591305732727
CurrentTrain: epoch  4, batch     1 | loss: 8.1120195Losses:  7.964563846588135 3.2669966220855713 0.6271851658821106
CurrentTrain: epoch  4, batch     2 | loss: 7.9645638Losses:  2.9373857975006104 -0.0 0.12922458350658417
CurrentTrain: epoch  4, batch     3 | loss: 2.9373858Losses:  7.538694858551025 3.0774118900299072 0.6749807596206665
CurrentTrain: epoch  5, batch     0 | loss: 7.5386949Losses:  7.3690996170043945 3.5848517417907715 0.3614884614944458
CurrentTrain: epoch  5, batch     1 | loss: 7.3690996Losses:  8.80487060546875 3.8716282844543457 0.5517326593399048
CurrentTrain: epoch  5, batch     2 | loss: 8.8048706Losses:  3.8924520015716553 -0.0 0.13588258624076843
CurrentTrain: epoch  5, batch     3 | loss: 3.8924520Losses:  7.164245128631592 2.590622901916504 0.6533176302909851
CurrentTrain: epoch  6, batch     0 | loss: 7.1642451Losses:  7.883975982666016 3.783022880554199 0.5641164779663086
CurrentTrain: epoch  6, batch     1 | loss: 7.8839760Losses:  6.061098098754883 2.0555291175842285 0.6107799410820007
CurrentTrain: epoch  6, batch     2 | loss: 6.0610981Losses:  1.9878979921340942 -0.0 -0.0
CurrentTrain: epoch  6, batch     3 | loss: 1.9878980Losses:  6.033312797546387 2.5607943534851074 0.5533218383789062
CurrentTrain: epoch  7, batch     0 | loss: 6.0333128Losses:  8.2738037109375 3.892176628112793 0.5490505695343018
CurrentTrain: epoch  7, batch     1 | loss: 8.2738037Losses:  7.7145185470581055 3.9221584796905518 0.6079439520835876
CurrentTrain: epoch  7, batch     2 | loss: 7.7145185Losses:  3.282437324523926 -0.0 0.11858739703893661
CurrentTrain: epoch  7, batch     3 | loss: 3.2824373Losses:  8.741802215576172 4.084640026092529 0.5181462168693542
CurrentTrain: epoch  8, batch     0 | loss: 8.7418022Losses:  5.922545433044434 2.4531869888305664 0.6096649169921875
CurrentTrain: epoch  8, batch     1 | loss: 5.9225454Losses:  4.827202320098877 1.865483283996582 0.521419882774353
CurrentTrain: epoch  8, batch     2 | loss: 4.8272023Losses:  2.920750141143799 -0.0 0.09907035529613495
CurrentTrain: epoch  8, batch     3 | loss: 2.9207501Losses:  7.0056281089782715 3.439483165740967 0.6056036949157715
CurrentTrain: epoch  9, batch     0 | loss: 7.0056281Losses:  6.64453125 3.314809560775757 0.5478914380073547
CurrentTrain: epoch  9, batch     1 | loss: 6.6445312Losses:  7.68795919418335 3.9810214042663574 0.5333200693130493
CurrentTrain: epoch  9, batch     2 | loss: 7.6879592Losses:  2.1097729206085205 -0.0 -0.0
CurrentTrain: epoch  9, batch     3 | loss: 2.1097729
#### located on terrain feature ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "located on terrain feature" means located on the specified physical/geophysical feature.
Here is an example:
Relation: located on terrain feature
Context: the andesite line , a zone of intense volcanic and seismic activity , is a major regional distinction in the pacific .
Head Entity: andesite line
Tail Entity: pacific
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "located on terrain feature":
Relation: located on terrain feature  
Context: The Great Barrier Reef, a stunning natural wonder, stretches along the northeastern coast of Australia.  
Head Entity: Great Barrier Reef  
Tail Entity: Australia  

Relation: located on terrain feature  
Context: Mount Everest, the highest peak in the world, is part of the Himalayas and attracts climbers from around the globe.  
Head Entity: Mount Everest  
Tail Entity: Himalayas  
#### participant of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "participant of" means event in which a person or organization was/is a participant.
Here is an example:
Relation: participant of
Context: tiago venâncio ( born august 19 , 1987 , setúbal ) is a freestyle swimmer from portugal , who represented his home country at the 2004 summer olympics in athens , greece .
Head Entity: tiago venâncio
Tail Entity: 2004 summer olympics
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "participant of":
Relation: participant of  
Context: the united nations climate change conference, held in paris in 2015, saw numerous world leaders and organizations come together to discuss global warming and its impacts.  
Head Entity: united nations climate change conference  
Tail Entity: paris agreement  

Relation: participant of  
Context: the 2022 fifa world cup in qatar featured teams from around the globe competing for the prestigious trophy, showcasing the best of international football.  
Head Entity: 2022 fifa world cup  
Tail Entity: qatar
#### director ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "director" means director of film, TV-series, stageplay, video game or similar.
Here is an example:
Relation: director
Context: the festival opened with " whiplash " directed by damien chazelle and closed with musical drama " rudderless " directed by william h. macy .
Head Entity: whiplash
Tail Entity: damien chazelle
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "director":
Relation: director  
Context: the critically acclaimed series "breaking bad" was brought to life by director vince gilligan, while the spin-off "better call saul" was directed by peter gould.  
Head Entity: breaking bad  
Tail Entity: vince gilligan  

Relation: director  
Context: the animated feature "spider-man: into the spider-verse" was directed by bob persichetti, peter ramsey, and rodney rothman, showcasing a unique visual style.  
Head Entity: spider-man: into the spider-verse  
Tail Entity: bob persichetti  
#### owned by ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "owned by" means owner of the subject.
Here is an example:
Relation: owned by
Context: as of april 2013 , sony isp , known as so - net , released a new fibre service to tokyo for small businesses and homes and also made it available to six surrounding prefectures .
Head Entity: so - net
Tail Entity: sony
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "owned by":
Relation: owned by  
Context: In 2020, the popular video game company Activision Blizzard announced that it had acquired the rights to several franchises, including the beloved "Crash Bandicoot" series, which was originally developed by Naughty Dog.  
Head Entity: Activision Blizzard  
Tail Entity: Naughty Dog  

Relation: owned by  
Context: The luxury fashion brand Gucci, known for its high-end products and distinctive designs, is a subsidiary of the Kering Group, which also owns several other prestigious labels.  
Head Entity: Gucci  
Tail Entity: Kering Group  
#### part of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "part of" means object of which the subject is a part.
Here is an example:
Relation: part of
Context: thessaly was home to extensive neolithic and chalcolithic cultures around 6000 bc–2500 bc ( see cardium pottery , dimini and sesklo ) .
Head Entity: cardium pottery
Tail Entity: neolithic
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "part of":
Relation: part of  
Context: The engine is a crucial part of the car's overall functionality, providing the necessary power to drive.  
Head Entity: engine  
Tail Entity: car  

Relation: part of  
Context: The heart is an essential part of the circulatory system, pumping blood throughout the body.  
Head Entity: heart  
Tail Entity: circulatory system  
#### occupant ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "occupant" means person or organization occupying property.
Here is an example:
Relation: occupant
Context: the brewery closed in 1996 and is now the site of the halliwell jones stadium , home of warrington wolves , as well as a tesco store .
Head Entity: halliwell jones stadium
Tail Entity: warrington wolves
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "occupant":
Relation: occupant  
Context: The historic building was transformed into a cultural center, which now hosts the local art gallery and serves as the headquarters for the community theater group.  
Head Entity: cultural center  
Tail Entity: community theater group  

Relation: occupant  
Context: After the renovation, the old factory became a vibrant co-working space, attracting freelancers and startups, including a popular tech incubator.  
Head Entity: co-working space  
Tail Entity: tech incubator  
#### architect ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "architect" means person or architectural firm responsible for designing this building.
Here is an example:
Relation: architect
Context: the art nouveau style was brought to italy by figures such as giuseppe sommaruga and ernesto basile ( who respectively designed the palazzo castiglioni and expanded the palazzo montecitorio in rome ) .
Head Entity: palazzo castiglioni
Tail Entity: giuseppe sommaruga
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "architect":
Relation: architect  
Context: The iconic Sydney Opera House was designed by Danish architect Jørn Utzon, who won the design competition in 1957.  
Head Entity: Sydney Opera House  
Tail Entity: Jørn Utzon  

Relation: architect  
Context: Frank Lloyd Wright is renowned for his innovative designs, including the famous Fallingwater house, which harmonizes beautifully with its natural surroundings.  
Head Entity: Fallingwater  
Tail Entity: Frank Lloyd Wright  
#### residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "residence" means the place where the person is or has been, resident.
Here is an example:
Relation: residence
Context: fantcha was born as francelina durão almeida in mindelo on são vicente island , cape verde .
Head Entity: fantcha
Tail Entity: são vicente island
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "residence":
Relation: residence  
Context: after living in new york city for several years, she decided to move back to her hometown in los angeles.  
Head Entity: she  
Tail Entity: los angeles  

Relation: residence  
Context: the famous author spent most of his life in a quaint cottage located in the picturesque village of windsor.  
Head Entity: the famous author  
Tail Entity: windsor  
#### composer ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "composer" means person who wrote the music.
Here is an example:
Relation: composer
Context: at a young age he was an accomplished cellist , performing at age seventeen the solo part of richard strauss 's " don quixote " .
Head Entity: don quixote
Tail Entity: richard strauss
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "composer":
Relation: composer  
Context: The renowned composer Ludwig van Beethoven created some of the most celebrated symphonies in classical music history.  
Head Entity: symphonies  
Tail Entity: Ludwig van Beethoven  

Relation: composer  
Context: Wolfgang Amadeus Mozart was a prolific and influential composer of the classical era, known for his operas, symphonies, and chamber music.  
Head Entity: operas  
Tail Entity: Wolfgang Amadeus Mozart  
#### location ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "location" means location of the object, structure or event.
Here is an example:
Relation: location
Context: another attraction is engelsberg ironworks ( ) , an ironworks in ängelsberg .
Head Entity: engelsberg ironworks
Tail Entity: ängelsberg
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "location":
Relation: location  
Context: the famous landmark is the eiffel tower ( ) , located in the heart of paris .  
Head Entity: eiffel tower  
Tail Entity: paris  

Relation: location  
Context: the annual festival takes place at the grand central park ( ) , a large public park in new york city .  
Head Entity: grand central park  
Tail Entity: new york city  
Losses:  5.881253242492676 1.0704052448272705 0.5195494294166565
MemoryTrain:  epoch  0, batch     0 | loss: 5.8812532Losses:  7.146834850311279 1.1621698141098022 0.73284512758255
MemoryTrain:  epoch  0, batch     1 | loss: 7.1468349Losses:  6.86813497543335 1.6293110847473145 0.8580225110054016
MemoryTrain:  epoch  0, batch     2 | loss: 6.8681350Losses:  4.556386470794678 0.2561180591583252 0.7375558018684387
MemoryTrain:  epoch  0, batch     3 | loss: 4.5563865Losses:  6.234830379486084 1.105789303779602 0.7031372785568237
MemoryTrain:  epoch  0, batch     4 | loss: 6.2348304Losses:  6.652309417724609 0.5467099547386169 0.5006687045097351
MemoryTrain:  epoch  0, batch     5 | loss: 6.6523094Losses:  7.379151344299316 0.9412819147109985 0.8402094841003418
MemoryTrain:  epoch  1, batch     0 | loss: 7.3791513Losses:  5.758284568786621 0.7300629615783691 0.605971097946167
MemoryTrain:  epoch  1, batch     1 | loss: 5.7582846Losses:  4.66711950302124 0.5096688270568848 0.7462696433067322
MemoryTrain:  epoch  1, batch     2 | loss: 4.6671195Losses:  4.735446453094482 -0.0 0.8784761428833008
MemoryTrain:  epoch  1, batch     3 | loss: 4.7354465Losses:  4.465250015258789 1.244340419769287 0.6705045700073242
MemoryTrain:  epoch  1, batch     4 | loss: 4.4652500Losses:  6.755690574645996 1.5485848188400269 0.5647528171539307
MemoryTrain:  epoch  1, batch     5 | loss: 6.7556906Losses:  5.020636081695557 0.5793522596359253 0.5530710816383362
MemoryTrain:  epoch  2, batch     0 | loss: 5.0206361Losses:  4.7935075759887695 0.2788335978984833 0.7190102338790894
MemoryTrain:  epoch  2, batch     1 | loss: 4.7935076Losses:  4.24625301361084 0.28123337030410767 0.8458136916160583
MemoryTrain:  epoch  2, batch     2 | loss: 4.2462530Losses:  3.7669265270233154 0.49823057651519775 0.9139096736907959
MemoryTrain:  epoch  2, batch     3 | loss: 3.7669265Losses:  5.2048139572143555 0.6445173025131226 0.7661097645759583
MemoryTrain:  epoch  2, batch     4 | loss: 5.2048140Losses:  4.335458755493164 -0.0 0.5716638565063477
MemoryTrain:  epoch  2, batch     5 | loss: 4.3354588Losses:  4.691110610961914 0.7776573896408081 0.4906017482280731
MemoryTrain:  epoch  3, batch     0 | loss: 4.6911106Losses:  3.93703031539917 -0.0 0.8844413757324219
MemoryTrain:  epoch  3, batch     1 | loss: 3.9370303Losses:  4.731968879699707 1.0556174516677856 0.5914402008056641
MemoryTrain:  epoch  3, batch     2 | loss: 4.7319689Losses:  5.528393268585205 0.8329282402992249 0.8265318274497986
MemoryTrain:  epoch  3, batch     3 | loss: 5.5283933Losses:  3.3659989833831787 -0.0 0.8008554577827454
MemoryTrain:  epoch  3, batch     4 | loss: 3.3659990Losses:  3.223299026489258 0.266633003950119 0.625557541847229
MemoryTrain:  epoch  3, batch     5 | loss: 3.2232990Losses:  4.481064319610596 0.4994868338108063 0.8985719680786133
MemoryTrain:  epoch  4, batch     0 | loss: 4.4810643Losses:  4.590247631072998 0.5703954100608826 0.6715123057365417
MemoryTrain:  epoch  4, batch     1 | loss: 4.5902476Losses:  3.6428511142730713 0.29859185218811035 0.7477880120277405
MemoryTrain:  epoch  4, batch     2 | loss: 3.6428511Losses:  4.135417938232422 1.4100487232208252 0.6099647879600525
MemoryTrain:  epoch  4, batch     3 | loss: 4.1354179Losses:  3.75654935836792 0.5080505013465881 0.7604128122329712
MemoryTrain:  epoch  4, batch     4 | loss: 3.7565494Losses:  2.8243494033813477 0.23452119529247284 0.5985729098320007
MemoryTrain:  epoch  4, batch     5 | loss: 2.8243494Losses:  3.108581781387329 0.2508334517478943 0.5857608318328857
MemoryTrain:  epoch  5, batch     0 | loss: 3.1085818Losses:  3.2350540161132812 0.26009607315063477 0.649077296257019
MemoryTrain:  epoch  5, batch     1 | loss: 3.2350540Losses:  3.5888776779174805 0.280767023563385 0.8202372789382935
MemoryTrain:  epoch  5, batch     2 | loss: 3.5888777Losses:  2.9501237869262695 0.4929051399230957 0.7399903535842896
MemoryTrain:  epoch  5, batch     3 | loss: 2.9501238Losses:  3.6938843727111816 1.0883936882019043 0.7703870534896851
MemoryTrain:  epoch  5, batch     4 | loss: 3.6938844Losses:  3.2417564392089844 -0.0 0.6972435712814331
MemoryTrain:  epoch  5, batch     5 | loss: 3.2417564Losses:  3.3361027240753174 0.5528100728988647 0.88755202293396
MemoryTrain:  epoch  6, batch     0 | loss: 3.3361027Losses:  3.4170351028442383 1.0179858207702637 0.6660770773887634
MemoryTrain:  epoch  6, batch     1 | loss: 3.4170351Losses:  3.495239019393921 0.5093317627906799 0.8190234303474426
MemoryTrain:  epoch  6, batch     2 | loss: 3.4952390Losses:  2.8728809356689453 0.2616612911224365 0.8035076260566711
MemoryTrain:  epoch  6, batch     3 | loss: 2.8728809Losses:  4.59415340423584 1.769730806350708 0.5791903734207153
MemoryTrain:  epoch  6, batch     4 | loss: 4.5941534Losses:  2.8845651149749756 -0.0 0.5250115990638733
MemoryTrain:  epoch  6, batch     5 | loss: 2.8845651Losses:  3.2792468070983887 0.47728195786476135 0.6617841720581055
MemoryTrain:  epoch  7, batch     0 | loss: 3.2792468Losses:  3.0488240718841553 0.7430021166801453 0.7625183463096619
MemoryTrain:  epoch  7, batch     1 | loss: 3.0488241Losses:  3.1287312507629395 0.2551939785480499 0.7842556834220886
MemoryTrain:  epoch  7, batch     2 | loss: 3.1287313Losses:  3.1748342514038086 0.5243374109268188 0.8847819566726685
MemoryTrain:  epoch  7, batch     3 | loss: 3.1748343Losses:  3.193465232849121 0.21821081638336182 0.8941830992698669
MemoryTrain:  epoch  7, batch     4 | loss: 3.1934652Losses:  3.767672061920166 1.5262125730514526 0.2939787209033966
MemoryTrain:  epoch  7, batch     5 | loss: 3.7676721Losses:  2.8255550861358643 0.20862020552158356 0.8271661996841431
MemoryTrain:  epoch  8, batch     0 | loss: 2.8255551Losses:  3.1828441619873047 1.0052289962768555 0.7407252192497253
MemoryTrain:  epoch  8, batch     1 | loss: 3.1828442Losses:  4.703593730926514 1.3826878070831299 0.7490841746330261
MemoryTrain:  epoch  8, batch     2 | loss: 4.7035937Losses:  3.4372053146362305 1.0628864765167236 0.698165774345398
MemoryTrain:  epoch  8, batch     3 | loss: 3.4372053Losses:  3.496095895767212 0.9538620710372925 0.6727907061576843
MemoryTrain:  epoch  8, batch     4 | loss: 3.4960959Losses:  2.331672430038452 0.3320091664791107 0.5504459142684937
MemoryTrain:  epoch  8, batch     5 | loss: 2.3316724Losses:  2.9436428546905518 0.712777853012085 0.787588894367218
MemoryTrain:  epoch  9, batch     0 | loss: 2.9436429Losses:  2.8427371978759766 0.6952558755874634 0.7266295552253723
MemoryTrain:  epoch  9, batch     1 | loss: 2.8427372Losses:  2.7940833568573 0.2577216625213623 0.7650410532951355
MemoryTrain:  epoch  9, batch     2 | loss: 2.7940834Losses:  3.5656394958496094 0.7461678981781006 0.7357299327850342
MemoryTrain:  epoch  9, batch     3 | loss: 3.5656395Losses:  2.7968931198120117 0.522223949432373 0.5025618076324463
MemoryTrain:  epoch  9, batch     4 | loss: 2.7968931Losses:  2.257843017578125 0.2278815060853958 0.6486724019050598
MemoryTrain:  epoch  9, batch     5 | loss: 2.2578430
[EVAL] batch:    0 | acc: 18.75%,  total acc: 18.75%   [EVAL] batch:    1 | acc: 37.50%,  total acc: 28.12%   [EVAL] batch:    2 | acc: 25.00%,  total acc: 27.08%   [EVAL] batch:    3 | acc: 37.50%,  total acc: 29.69%   [EVAL] batch:    4 | acc: 37.50%,  total acc: 31.25%   [EVAL] batch:    5 | acc: 37.50%,  total acc: 32.29%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 39.29%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 44.53%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 50.00%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 55.00%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 57.95%   [EVAL] batch:   11 | acc: 68.75%,  total acc: 58.85%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 61.54%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 63.84%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 65.83%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 67.97%   [EVAL] batch:   16 | acc: 87.50%,  total acc: 69.12%   [EVAL] batch:   17 | acc: 81.25%,  total acc: 69.79%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 69.74%   [EVAL] batch:   19 | acc: 56.25%,  total acc: 69.06%   [EVAL] batch:   20 | acc: 31.25%,  total acc: 67.26%   [EVAL] batch:   21 | acc: 75.00%,  total acc: 67.61%   [EVAL] batch:   22 | acc: 62.50%,  total acc: 67.39%   [EVAL] batch:   23 | acc: 43.75%,  total acc: 66.41%   [EVAL] batch:   24 | acc: 56.25%,  total acc: 66.00%   [EVAL] batch:   25 | acc: 6.25%,  total acc: 63.70%   [EVAL] batch:   26 | acc: 6.25%,  total acc: 61.57%   [EVAL] batch:   27 | acc: 0.00%,  total acc: 59.38%   [EVAL] batch:   28 | acc: 0.00%,  total acc: 57.33%   [EVAL] batch:   29 | acc: 6.25%,  total acc: 55.62%   [EVAL] batch:   30 | acc: 0.00%,  total acc: 53.83%   [EVAL] batch:   31 | acc: 62.50%,  total acc: 54.10%   [EVAL] batch:   32 | acc: 87.50%,  total acc: 55.11%   [EVAL] batch:   33 | acc: 87.50%,  total acc: 56.07%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 57.32%   [EVAL] batch:   35 | acc: 81.25%,  total acc: 57.99%   [EVAL] batch:   36 | acc: 81.25%,  total acc: 58.61%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 59.54%   [EVAL] batch:   38 | acc: 81.25%,  total acc: 60.10%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 61.09%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 61.89%   [EVAL] batch:   41 | acc: 75.00%,  total acc: 62.20%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 63.08%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 63.64%   [EVAL] batch:   44 | acc: 75.00%,  total acc: 63.89%   [EVAL] batch:   45 | acc: 81.25%,  total acc: 64.27%   [EVAL] batch:   46 | acc: 62.50%,  total acc: 64.23%   [EVAL] batch:   47 | acc: 56.25%,  total acc: 64.06%   [EVAL] batch:   48 | acc: 75.00%,  total acc: 64.29%   [EVAL] batch:   49 | acc: 75.00%,  total acc: 64.50%   [EVAL] batch:   50 | acc: 25.00%,  total acc: 63.73%   [EVAL] batch:   51 | acc: 50.00%,  total acc: 63.46%   [EVAL] batch:   52 | acc: 37.50%,  total acc: 62.97%   [EVAL] batch:   53 | acc: 50.00%,  total acc: 62.73%   [EVAL] batch:   54 | acc: 56.25%,  total acc: 62.61%   [EVAL] batch:   55 | acc: 68.75%,  total acc: 62.72%   [EVAL] batch:   56 | acc: 25.00%,  total acc: 62.06%   [EVAL] batch:   57 | acc: 31.25%,  total acc: 61.53%   [EVAL] batch:   58 | acc: 50.00%,  total acc: 61.33%   [EVAL] batch:   59 | acc: 43.75%,  total acc: 61.04%   [EVAL] batch:   60 | acc: 31.25%,  total acc: 60.55%   [EVAL] batch:   61 | acc: 37.50%,  total acc: 60.18%   [EVAL] batch:   62 | acc: 25.00%,  total acc: 59.62%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 85.94%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 86.25%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    6 | acc: 50.00%,  total acc: 82.14%   [EVAL] batch:    7 | acc: 68.75%,  total acc: 80.47%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 79.17%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 78.75%   [EVAL] batch:   10 | acc: 43.75%,  total acc: 75.57%   [EVAL] batch:   11 | acc: 50.00%,  total acc: 73.44%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 73.08%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 74.55%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 75.83%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 77.34%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 78.31%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 79.17%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 80.26%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 80.62%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 80.95%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 80.97%   [EVAL] batch:   22 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 81.51%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 81.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 82.45%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 83.10%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 83.71%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 84.05%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 84.58%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 85.08%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 85.55%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 85.98%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 86.40%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 86.79%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 87.15%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 87.83%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 88.14%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 88.44%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 88.72%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 88.99%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 89.10%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 89.35%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 89.44%   [EVAL] batch:   45 | acc: 87.50%,  total acc: 89.40%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 89.23%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 89.32%   [EVAL] batch:   48 | acc: 93.75%,  total acc: 89.41%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 89.62%   [EVAL] batch:   50 | acc: 81.25%,  total acc: 89.46%   [EVAL] batch:   51 | acc: 81.25%,  total acc: 89.30%   [EVAL] batch:   52 | acc: 81.25%,  total acc: 89.15%   [EVAL] batch:   53 | acc: 75.00%,  total acc: 88.89%   [EVAL] batch:   54 | acc: 56.25%,  total acc: 88.30%   [EVAL] batch:   55 | acc: 81.25%,  total acc: 88.17%   [EVAL] batch:   56 | acc: 87.50%,  total acc: 88.16%   [EVAL] batch:   57 | acc: 81.25%,  total acc: 88.04%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 88.24%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 88.44%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 88.63%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 88.71%   [EVAL] batch:   62 | acc: 93.75%,  total acc: 88.79%   [EVAL] batch:   63 | acc: 81.25%,  total acc: 88.67%   [EVAL] batch:   64 | acc: 93.75%,  total acc: 88.75%   [EVAL] batch:   65 | acc: 100.00%,  total acc: 88.92%   [EVAL] batch:   66 | acc: 81.25%,  total acc: 88.81%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 88.97%   [EVAL] batch:   68 | acc: 87.50%,  total acc: 88.95%   [EVAL] batch:   69 | acc: 81.25%,  total acc: 88.84%   [EVAL] batch:   70 | acc: 87.50%,  total acc: 88.82%   [EVAL] batch:   71 | acc: 81.25%,  total acc: 88.72%   [EVAL] batch:   72 | acc: 81.25%,  total acc: 88.61%   [EVAL] batch:   73 | acc: 75.00%,  total acc: 88.43%   [EVAL] batch:   74 | acc: 68.75%,  total acc: 88.17%   [EVAL] batch:   75 | acc: 81.25%,  total acc: 88.08%   [EVAL] batch:   76 | acc: 81.25%,  total acc: 87.99%   [EVAL] batch:   77 | acc: 68.75%,  total acc: 87.74%   [EVAL] batch:   78 | acc: 43.75%,  total acc: 87.18%   [EVAL] batch:   79 | acc: 81.25%,  total acc: 87.11%   [EVAL] batch:   80 | acc: 68.75%,  total acc: 86.88%   [EVAL] batch:   81 | acc: 68.75%,  total acc: 86.66%   [EVAL] batch:   82 | acc: 75.00%,  total acc: 86.52%   [EVAL] batch:   83 | acc: 56.25%,  total acc: 86.16%   [EVAL] batch:   84 | acc: 50.00%,  total acc: 85.74%   [EVAL] batch:   85 | acc: 68.75%,  total acc: 85.54%   [EVAL] batch:   86 | acc: 56.25%,  total acc: 85.20%   [EVAL] batch:   87 | acc: 81.25%,  total acc: 85.16%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 85.32%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 85.49%   [EVAL] batch:   90 | acc: 100.00%,  total acc: 85.65%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 85.80%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 85.95%   [EVAL] batch:   93 | acc: 93.75%,  total acc: 86.04%   [EVAL] batch:   94 | acc: 56.25%,  total acc: 85.72%   [EVAL] batch:   95 | acc: 68.75%,  total acc: 85.55%   [EVAL] batch:   96 | acc: 68.75%,  total acc: 85.37%   [EVAL] batch:   97 | acc: 62.50%,  total acc: 85.14%   [EVAL] batch:   98 | acc: 75.00%,  total acc: 85.04%   [EVAL] batch:   99 | acc: 50.00%,  total acc: 84.69%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 84.84%   [EVAL] batch:  101 | acc: 100.00%,  total acc: 84.99%   [EVAL] batch:  102 | acc: 100.00%,  total acc: 85.13%   [EVAL] batch:  103 | acc: 100.00%,  total acc: 85.28%   [EVAL] batch:  104 | acc: 100.00%,  total acc: 85.42%   [EVAL] batch:  105 | acc: 93.75%,  total acc: 85.50%   [EVAL] batch:  106 | acc: 100.00%,  total acc: 85.63%   [EVAL] batch:  107 | acc: 100.00%,  total acc: 85.76%   [EVAL] batch:  108 | acc: 100.00%,  total acc: 85.89%   [EVAL] batch:  109 | acc: 100.00%,  total acc: 86.02%   [EVAL] batch:  110 | acc: 100.00%,  total acc: 86.15%   [EVAL] batch:  111 | acc: 100.00%,  total acc: 86.27%   [EVAL] batch:  112 | acc: 93.75%,  total acc: 86.34%   [EVAL] batch:  113 | acc: 81.25%,  total acc: 86.29%   [EVAL] batch:  114 | acc: 93.75%,  total acc: 86.36%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 86.48%   [EVAL] batch:  116 | acc: 87.50%,  total acc: 86.49%   [EVAL] batch:  117 | acc: 93.75%,  total acc: 86.55%   [EVAL] batch:  118 | acc: 81.25%,  total acc: 86.50%   [EVAL] batch:  119 | acc: 87.50%,  total acc: 86.51%   [EVAL] batch:  120 | acc: 62.50%,  total acc: 86.31%   [EVAL] batch:  121 | acc: 93.75%,  total acc: 86.37%   [EVAL] batch:  122 | acc: 93.75%,  total acc: 86.43%   [EVAL] batch:  123 | acc: 81.25%,  total acc: 86.39%   [EVAL] batch:  124 | acc: 93.75%,  total acc: 86.45%   [EVAL] batch:  125 | acc: 18.75%,  total acc: 85.91%   [EVAL] batch:  126 | acc: 37.50%,  total acc: 85.53%   [EVAL] batch:  127 | acc: 25.00%,  total acc: 85.06%   [EVAL] batch:  128 | acc: 37.50%,  total acc: 84.69%   [EVAL] batch:  129 | acc: 37.50%,  total acc: 84.33%   [EVAL] batch:  130 | acc: 37.50%,  total acc: 83.97%   [EVAL] batch:  131 | acc: 81.25%,  total acc: 83.95%   [EVAL] batch:  132 | acc: 81.25%,  total acc: 83.93%   [EVAL] batch:  133 | acc: 93.75%,  total acc: 84.00%   [EVAL] batch:  134 | acc: 100.00%,  total acc: 84.12%   [EVAL] batch:  135 | acc: 87.50%,  total acc: 84.15%   [EVAL] batch:  136 | acc: 68.75%,  total acc: 84.03%   [EVAL] batch:  137 | acc: 93.75%,  total acc: 84.10%   [EVAL] batch:  138 | acc: 93.75%,  total acc: 84.17%   [EVAL] batch:  139 | acc: 93.75%,  total acc: 84.24%   [EVAL] batch:  140 | acc: 100.00%,  total acc: 84.35%   [EVAL] batch:  141 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:  142 | acc: 81.25%,  total acc: 84.35%   [EVAL] batch:  143 | acc: 68.75%,  total acc: 84.24%   [EVAL] batch:  144 | acc: 56.25%,  total acc: 84.05%   [EVAL] batch:  145 | acc: 31.25%,  total acc: 83.69%   [EVAL] batch:  146 | acc: 75.00%,  total acc: 83.63%   [EVAL] batch:  147 | acc: 62.50%,  total acc: 83.49%   [EVAL] batch:  148 | acc: 43.75%,  total acc: 83.22%   [EVAL] batch:  149 | acc: 56.25%,  total acc: 83.04%   [EVAL] batch:  150 | acc: 6.25%,  total acc: 82.53%   [EVAL] batch:  151 | acc: 6.25%,  total acc: 82.03%   [EVAL] batch:  152 | acc: 0.00%,  total acc: 81.50%   [EVAL] batch:  153 | acc: 0.00%,  total acc: 80.97%   [EVAL] batch:  154 | acc: 6.25%,  total acc: 80.48%   [EVAL] batch:  155 | acc: 0.00%,  total acc: 79.97%   [EVAL] batch:  156 | acc: 62.50%,  total acc: 79.86%   [EVAL] batch:  157 | acc: 87.50%,  total acc: 79.91%   [EVAL] batch:  158 | acc: 87.50%,  total acc: 79.95%   [EVAL] batch:  159 | acc: 100.00%,  total acc: 80.08%   [EVAL] batch:  160 | acc: 81.25%,  total acc: 80.09%   [EVAL] batch:  161 | acc: 81.25%,  total acc: 80.09%   [EVAL] batch:  162 | acc: 93.75%,  total acc: 80.18%   [EVAL] batch:  163 | acc: 81.25%,  total acc: 80.18%   [EVAL] batch:  164 | acc: 100.00%,  total acc: 80.30%   [EVAL] batch:  165 | acc: 93.75%,  total acc: 80.38%   [EVAL] batch:  166 | acc: 75.00%,  total acc: 80.35%   [EVAL] batch:  167 | acc: 100.00%,  total acc: 80.47%   [EVAL] batch:  168 | acc: 87.50%,  total acc: 80.51%   [EVAL] batch:  169 | acc: 75.00%,  total acc: 80.48%   [EVAL] batch:  170 | acc: 81.25%,  total acc: 80.48%   [EVAL] batch:  171 | acc: 62.50%,  total acc: 80.38%   [EVAL] batch:  172 | acc: 56.25%,  total acc: 80.24%   [EVAL] batch:  173 | acc: 75.00%,  total acc: 80.21%   [EVAL] batch:  174 | acc: 75.00%,  total acc: 80.18%   [EVAL] batch:  175 | acc: 25.00%,  total acc: 79.87%   [EVAL] batch:  176 | acc: 50.00%,  total acc: 79.70%   [EVAL] batch:  177 | acc: 37.50%,  total acc: 79.46%   [EVAL] batch:  178 | acc: 50.00%,  total acc: 79.29%   [EVAL] batch:  179 | acc: 56.25%,  total acc: 79.17%   [EVAL] batch:  180 | acc: 68.75%,  total acc: 79.11%   [EVAL] batch:  181 | acc: 25.00%,  total acc: 78.81%   [EVAL] batch:  182 | acc: 31.25%,  total acc: 78.55%   [EVAL] batch:  183 | acc: 50.00%,  total acc: 78.40%   [EVAL] batch:  184 | acc: 43.75%,  total acc: 78.21%   [EVAL] batch:  185 | acc: 31.25%,  total acc: 77.96%   [EVAL] batch:  186 | acc: 37.50%,  total acc: 77.74%   [EVAL] batch:  187 | acc: 25.00%,  total acc: 77.46%   
cur_acc:  ['0.9494', '0.8244', '0.5962']
his_acc:  ['0.9494', '0.8760', '0.7746']
Clustering into  19  clusters
Clusters:  [ 5  6  4  1  2 18  4  3  6 11  1 14  5  6  0  7  2 15  9  2  2  1  3 17
 12  8  3  6  3  6 12 16 13  4  3 10  6  7  0  0]
Losses:  9.995646476745605 3.1955513954162598 0.7353045344352722
CurrentTrain: epoch  0, batch     0 | loss: 9.9956465Losses:  11.160953521728516 4.497170448303223 0.7819195985794067
CurrentTrain: epoch  0, batch     1 | loss: 11.1609535Losses:  9.977823257446289 3.169909715652466 0.6938095688819885
CurrentTrain: epoch  0, batch     2 | loss: 9.9778233Losses:  8.896899223327637 -0.0 0.1490033119916916
CurrentTrain: epoch  0, batch     3 | loss: 8.8968992Losses:  10.00118637084961 3.970350742340088 0.6616747379302979
CurrentTrain: epoch  1, batch     0 | loss: 10.0011864Losses:  11.048664093017578 4.564593315124512 0.7794783115386963
CurrentTrain: epoch  1, batch     1 | loss: 11.0486641Losses:  9.501619338989258 4.333059310913086 0.6683260202407837
CurrentTrain: epoch  1, batch     2 | loss: 9.5016193Losses:  4.859896659851074 -0.0 0.09792710840702057
CurrentTrain: epoch  1, batch     3 | loss: 4.8598967Losses:  9.562037467956543 4.604823589324951 0.6613909602165222
CurrentTrain: epoch  2, batch     0 | loss: 9.5620375Losses:  7.8609232902526855 2.294341802597046 0.8000980019569397
CurrentTrain: epoch  2, batch     1 | loss: 7.8609233Losses:  9.553010940551758 4.131765365600586 0.6715778708457947
CurrentTrain: epoch  2, batch     2 | loss: 9.5530109Losses:  5.224459648132324 -0.0 0.13760200142860413
CurrentTrain: epoch  2, batch     3 | loss: 5.2244596Losses:  7.912463188171387 3.268287181854248 0.7298141717910767
CurrentTrain: epoch  3, batch     0 | loss: 7.9124632Losses:  11.255252838134766 5.080609321594238 0.7144620418548584
CurrentTrain: epoch  3, batch     1 | loss: 11.2552528Losses:  6.880553722381592 3.4086549282073975 0.6387812495231628
CurrentTrain: epoch  3, batch     2 | loss: 6.8805537Losses:  2.7106027603149414 -0.0 0.119412861764431
CurrentTrain: epoch  3, batch     3 | loss: 2.7106028Losses:  6.666875839233398 2.3961446285247803 0.7156573534011841
CurrentTrain: epoch  4, batch     0 | loss: 6.6668758Losses:  8.023809432983398 3.6322920322418213 0.6942873001098633
CurrentTrain: epoch  4, batch     1 | loss: 8.0238094Losses:  9.177732467651367 4.369063377380371 0.7166880369186401
CurrentTrain: epoch  4, batch     2 | loss: 9.1777325Losses:  1.9583903551101685 -0.0 0.14818313717842102
CurrentTrain: epoch  4, batch     3 | loss: 1.9583904Losses:  7.187453269958496 3.5053606033325195 0.7346664071083069
CurrentTrain: epoch  5, batch     0 | loss: 7.1874533Losses:  7.43730354309082 3.1033341884613037 0.7211521863937378
CurrentTrain: epoch  5, batch     1 | loss: 7.4373035Losses:  7.598886966705322 2.8246006965637207 0.7793003916740417
CurrentTrain: epoch  5, batch     2 | loss: 7.5988870Losses:  2.260387420654297 -0.0 0.1375170201063156
CurrentTrain: epoch  5, batch     3 | loss: 2.2603874Losses:  7.75518274307251 3.4386491775512695 0.7042614221572876
CurrentTrain: epoch  6, batch     0 | loss: 7.7551827Losses:  7.006426811218262 4.031253814697266 0.503618061542511
CurrentTrain: epoch  6, batch     1 | loss: 7.0064268Losses:  6.759861469268799 2.31795597076416 0.76490718126297
CurrentTrain: epoch  6, batch     2 | loss: 6.7598615Losses:  2.531510353088379 -0.0 0.10053528845310211
CurrentTrain: epoch  6, batch     3 | loss: 2.5315104Losses:  6.80338191986084 2.961277961730957 0.7095528841018677
CurrentTrain: epoch  7, batch     0 | loss: 6.8033819Losses:  7.0413818359375 3.4477555751800537 0.6247312426567078
CurrentTrain: epoch  7, batch     1 | loss: 7.0413818Losses:  7.11445426940918 3.6448254585266113 0.6352946162223816
CurrentTrain: epoch  7, batch     2 | loss: 7.1144543Losses:  3.6675450801849365 -0.0 0.1630828082561493
CurrentTrain: epoch  7, batch     3 | loss: 3.6675451Losses:  5.811424255371094 3.3533248901367188 0.5205098390579224
CurrentTrain: epoch  8, batch     0 | loss: 5.8114243Losses:  8.333830833435059 3.9315719604492188 0.6158874034881592
CurrentTrain: epoch  8, batch     1 | loss: 8.3338308Losses:  7.220327377319336 3.656567096710205 0.6139955520629883
CurrentTrain: epoch  8, batch     2 | loss: 7.2203274Losses:  1.9670079946517944 -0.0 0.10784474015235901
CurrentTrain: epoch  8, batch     3 | loss: 1.9670080Losses:  7.383413314819336 4.214031219482422 0.5945374369621277
CurrentTrain: epoch  9, batch     0 | loss: 7.3834133Losses:  7.028615474700928 3.3664722442626953 0.6578030586242676
CurrentTrain: epoch  9, batch     1 | loss: 7.0286155Losses:  5.552762031555176 2.31654953956604 0.6804302930831909
CurrentTrain: epoch  9, batch     2 | loss: 5.5527620Losses:  2.488416910171509 -0.0 0.14388814568519592
CurrentTrain: epoch  9, batch     3 | loss: 2.4884169
#### contains administrative territorial entity ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "contains administrative territorial entity" means direct subdivisions of an administrative territorial entity.
Here is an example:
Relation: contains administrative territorial entity
Context: the cantons of créteil are administrative divisions of the val - de - marne department , île - de - france region , northern france .
Head Entity: île - de - france
Tail Entity: val - de - marne
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "contains administrative territorial entity":
Relation: contains administrative territorial entity  
Context: The provinces of Canada are the primary administrative divisions of the country, with Ontario being one of the most populous provinces.  
Head Entity: Canada  
Tail Entity: Ontario  

Relation: contains administrative territorial entity  
Context: The states of the United States are divided into various counties, with Los Angeles County being one of the largest in California.  
Head Entity: California  
Tail Entity: Los Angeles County  
#### said to be the same as ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "said to be the same as" means this item is said to be the same as that item, but it's uncertain or disputed.
Here is an example:
Relation: said to be the same as
Context: enzo is an italian given name derivative of heinz , a form of henry .
Head Entity: henry
Tail Entity: heinz
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "said to be the same as":
Relation: said to be the same as  
Context: The two species, Canis lupus and Canis familiaris, are often said to be the same as each other due to their close genetic relationship.  
Head Entity: Canis lupus  
Tail Entity: Canis familiaris  

Relation: said to be the same as  
Context: In some circles, the terms "artificial intelligence" and "machine learning" are said to be the same as each other, although experts argue there are key differences.  
Head Entity: artificial intelligence  
Tail Entity: machine learning  
#### winner ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "winner" means winner of a competition or similar event.
Here is an example:
Relation: winner
Context: the filming was primarily held at pollachi . it fetched pawan kalyan the filmfare award for best actor – telugu award at 59th filmfare awards south .
Head Entity: filmfare award for best actor – telugu
Tail Entity: pawan kalyan
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "winner":
Relation: winner  
Context: in the thrilling finale of the championship, the underdog team triumphed, securing the title and making history as they celebrated their victory at the national sports arena. This remarkable achievement earned them the prestigious trophy for the best team of the year.  
Head Entity: trophy for the best team of the year  
Tail Entity: underdog team  

Relation: winner  
Context: during the annual music awards, the talented singer captivated the audience with her stunning performance, ultimately leading her to win the coveted title of best new artist. The event was a celebration of emerging talent in the music industry.  
Head Entity: title of best new artist  
Tail Entity: talented singer  
#### military rank ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "military rank" means military rank achieved by a person, or military rank associated with a position.
Here is an example:
Relation: military rank
Context: air commodore arthur kellam tylee obe ( 24 april 1887 – 13 april 1961 ) was canadian officer who served in the royal flying corps during world war i.
Head Entity: arthur kellam tylee
Tail Entity: air commodore
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "military rank":
Relation: military rank  
Context: general john smith was appointed as the chief of staff of the army in 2020, overseeing all military operations.  
Head Entity: john smith  
Tail Entity: general  

Relation: military rank  
Context: colonel jane doe led her battalion with distinction during the peacekeeping mission in the region.  
Head Entity: jane doe  
Tail Entity: colonel  
#### publisher ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "publisher" means organization or person responsible for publishing books, periodicals, printed music, podcasts, games or software.
Here is an example:
Relation: publisher
Context: there are seven different nations that are allied or in conflict : prior to its north american release , " vanguard bandits " was titled " detonator gauntlet " by working designs .
Head Entity: vanguard bandits
Tail Entity: working designs
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "publisher":
Relation: publisher  
Context: The latest novel by the acclaimed author was released by Penguin Random House, a well-known publishing house in the industry.  
Head Entity: latest novel  
Tail Entity: Penguin Random House  

Relation: publisher  
Context: The popular science magazine was launched by National Geographic Society, which is renowned for its educational content.  
Head Entity: popular science magazine  
Tail Entity: National Geographic Society  
#### followed by ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "followed by" means immediately following item in a series of which the subject is a part.
Here is an example:
Relation: followed by
Context: " choose you " and " homesick " were released as the album 's second and third singles , respectively , and each attained moderate chart success .
Head Entity: choose you
Tail Entity: homesick
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "followed by":
Relation: followed by  
Context: The first chapter of the book introduces the main character, while the second chapter delves into the backstory.  
Head Entity: first chapter  
Tail Entity: second chapter  

Relation: followed by  
Context: The opening act of the concert was electrifying, followed by a stunning performance from the headliner.  
Head Entity: opening act  
Tail Entity: headliner  
#### work location ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "work location" means location where persons or organisations were actively participating in employment, business or other work.
Here is an example:
Relation: work location
Context: he was also associated with robert wilkinson in producing " londina illustrata " , an illustrated account of ancient buildings in london and westminster in two volumes ( 1819–25 ) .
Head Entity: robert wilkinson
Tail Entity: london
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "work location":
Relation: work location  
Context: The headquarters of the company is situated in the bustling city of San Francisco, where it has been operating since its inception in 2010.  
Head Entity: the company  
Tail Entity: San Francisco  

Relation: work location  
Context: During her tenure at the university, she conducted groundbreaking research in the field of neuroscience, primarily based in the labs at Cambridge.  
Head Entity: she  
Tail Entity: Cambridge  
#### occupation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "occupation" means occupation of a person.
Here is an example:
Relation: occupation
Context: andrei droznin ( russian : андрей дрознин ) is a russian theatre director and movement coach .
Head Entity: andrei droznin
Tail Entity: theatre director
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "occupation":
Relation: occupation  
Context: dr. emily thompson is a renowned biochemist known for her groundbreaking research in enzyme activity.  
Head Entity: dr. emily thompson  
Tail Entity: biochemist  

Relation: occupation  
Context: john smith, a talented software engineer, has been working on innovative artificial intelligence projects for several years.  
Head Entity: john smith  
Tail Entity: software engineer  
#### heritage designation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "heritage designation" means heritage designation of a cultural or natural site.
Here is an example:
Relation: heritage designation
Context: altamaha town , bluffton historic district , church of the cross , and rose hill plantation house are listed on the national register of historic places .
Head Entity: altamaha town
Tail Entity: national register of historic places
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "heritage designation":
Relation: heritage designation  
Context: The Great Wall of China is recognized as a UNESCO World Heritage Site due to its historical significance and architectural grandeur.  
Head Entity: Great Wall of China  
Tail Entity: UNESCO World Heritage Site  

Relation: heritage designation  
Context: The ancient city of Petra in Jordan is designated as a World Heritage Site, attracting tourists from around the globe.  
Head Entity: ancient city of Petra  
Tail Entity: World Heritage Site  
#### voice type ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "voice type" means person's voice type.
Here is an example:
Relation: voice type
Context: beverly bower ( september 30 , 1925 – march 24 , 2002 ) was an american operatic soprano who had an active international opera career from the mid-1950s through the early 1970s .
Head Entity: beverly bower
Tail Entity: soprano
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "voice type":
Relation: voice type  
Context: luciano pavarotti ( october 12 , 1935 – september 6 , 2007 ) was an italian operatic tenor who gained worldwide fame for his televised concerts and media appearances.  
Head Entity: luciano pavarotti  
Tail Entity: tenor  

Relation: voice type  
Context: mariah carey ( born march 27 , 1969 ) is an american singer, songwriter, and actress known for her five-octave vocal range and her use of the whistle register.  
Head Entity: mariah carey  
Tail Entity: singer  
Losses:  4.652433395385742 0.587860107421875 0.9107532501220703
MemoryTrain:  epoch  0, batch     0 | loss: 4.6524334Losses:  4.974100589752197 0.605972409248352 0.8413843512535095
MemoryTrain:  epoch  0, batch     1 | loss: 4.9741006Losses:  4.7488017082214355 0.5643519163131714 0.9349591135978699
MemoryTrain:  epoch  0, batch     2 | loss: 4.7488017Losses:  4.838665962219238 -0.0 0.8102750182151794
MemoryTrain:  epoch  0, batch     3 | loss: 4.8386660Losses:  3.8068625926971436 0.26135003566741943 0.8728777766227722
MemoryTrain:  epoch  0, batch     4 | loss: 3.8068626Losses:  4.777982711791992 0.8436546325683594 0.8365209698677063
MemoryTrain:  epoch  0, batch     5 | loss: 4.7779827Losses:  4.633541107177734 0.47583746910095215 0.6283080577850342
MemoryTrain:  epoch  0, batch     6 | loss: 4.6335411Losses:  3.143573045730591 -0.0 0.3879011571407318
MemoryTrain:  epoch  0, batch     7 | loss: 3.1435730Losses:  4.151891708374023 -0.0 0.9601604342460632
MemoryTrain:  epoch  1, batch     0 | loss: 4.1518917Losses:  5.16702127456665 0.5166992545127869 0.8798852562904358
MemoryTrain:  epoch  1, batch     1 | loss: 5.1670213Losses:  4.3081159591674805 0.8696748614311218 0.9106923341751099
MemoryTrain:  epoch  1, batch     2 | loss: 4.3081160Losses:  4.186295032501221 0.4964450001716614 0.7510699033737183
MemoryTrain:  epoch  1, batch     3 | loss: 4.1862950Losses:  3.3799240589141846 0.7622265815734863 0.7830192446708679
MemoryTrain:  epoch  1, batch     4 | loss: 3.3799241Losses:  4.484461784362793 0.5992190837860107 0.8437082171440125
MemoryTrain:  epoch  1, batch     5 | loss: 4.4844618Losses:  4.316705703735352 0.22956857085227966 0.8069424629211426
MemoryTrain:  epoch  1, batch     6 | loss: 4.3167057Losses:  3.1950743198394775 -0.0 0.5970289707183838
MemoryTrain:  epoch  1, batch     7 | loss: 3.1950743Losses:  4.375298023223877 0.751389741897583 0.9007071256637573
MemoryTrain:  epoch  2, batch     0 | loss: 4.3752980Losses:  4.49341344833374 0.9292339086532593 0.785504937171936
MemoryTrain:  epoch  2, batch     1 | loss: 4.4934134Losses:  3.0781729221343994 0.28388190269470215 0.8393890261650085
MemoryTrain:  epoch  2, batch     2 | loss: 3.0781729Losses:  4.303384780883789 0.48482078313827515 0.7483512163162231
MemoryTrain:  epoch  2, batch     3 | loss: 4.3033848Losses:  3.662310838699341 0.24481205642223358 0.8016937375068665
MemoryTrain:  epoch  2, batch     4 | loss: 3.6623108Losses:  4.627394199371338 0.6530020236968994 0.6875112056732178
MemoryTrain:  epoch  2, batch     5 | loss: 4.6273942Losses:  3.8248848915100098 0.6838367581367493 0.8298321962356567
MemoryTrain:  epoch  2, batch     6 | loss: 3.8248849Losses:  2.71403169631958 -0.0 0.6854985952377319
MemoryTrain:  epoch  2, batch     7 | loss: 2.7140317Losses:  3.1664600372314453 0.4963285028934479 0.7464103698730469
MemoryTrain:  epoch  3, batch     0 | loss: 3.1664600Losses:  3.0521671772003174 0.2936944365501404 0.9843217730522156
MemoryTrain:  epoch  3, batch     1 | loss: 3.0521672Losses:  2.6475088596343994 -0.0 0.8100118637084961
MemoryTrain:  epoch  3, batch     2 | loss: 2.6475089Losses:  4.29647970199585 0.4707495868206024 0.9390324354171753
MemoryTrain:  epoch  3, batch     3 | loss: 4.2964797Losses:  3.9393959045410156 0.5659092664718628 0.8190723657608032
MemoryTrain:  epoch  3, batch     4 | loss: 3.9393959Losses:  3.1431949138641357 0.7322665452957153 0.7926852703094482
MemoryTrain:  epoch  3, batch     5 | loss: 3.1431949Losses:  4.273803234100342 0.3307865262031555 0.86505526304245
MemoryTrain:  epoch  3, batch     6 | loss: 4.2738032Losses:  2.7509407997131348 -0.0 0.659502387046814
MemoryTrain:  epoch  3, batch     7 | loss: 2.7509408Losses:  3.4064483642578125 0.24262823164463043 0.6871213316917419
MemoryTrain:  epoch  4, batch     0 | loss: 3.4064484Losses:  3.0368123054504395 0.5380197763442993 0.6738621592521667
MemoryTrain:  epoch  4, batch     1 | loss: 3.0368123Losses:  3.2587778568267822 0.24205848574638367 0.9380068778991699
MemoryTrain:  epoch  4, batch     2 | loss: 3.2587779Losses:  3.138702392578125 0.5672222971916199 0.9212613105773926
MemoryTrain:  epoch  4, batch     3 | loss: 3.1387024Losses:  3.6012792587280273 0.5099127292633057 0.8152371644973755
MemoryTrain:  epoch  4, batch     4 | loss: 3.6012793Losses:  3.3209633827209473 0.5228113532066345 0.7694816589355469
MemoryTrain:  epoch  4, batch     5 | loss: 3.3209634Losses:  4.0183563232421875 0.6848189234733582 0.9493228793144226
MemoryTrain:  epoch  4, batch     6 | loss: 4.0183563Losses:  2.7750754356384277 0.33720460534095764 0.3676922917366028
MemoryTrain:  epoch  4, batch     7 | loss: 2.7750754Losses:  3.3876538276672363 0.8385540246963501 0.7180262804031372
MemoryTrain:  epoch  5, batch     0 | loss: 3.3876538Losses:  3.3076179027557373 0.2730679512023926 0.755072832107544
MemoryTrain:  epoch  5, batch     1 | loss: 3.3076179Losses:  3.7563207149505615 0.686461329460144 0.8093646168708801
MemoryTrain:  epoch  5, batch     2 | loss: 3.7563207Losses:  3.4527621269226074 1.0058698654174805 0.7894145250320435
MemoryTrain:  epoch  5, batch     3 | loss: 3.4527621Losses:  3.0724384784698486 0.7153615355491638 0.8332818150520325
MemoryTrain:  epoch  5, batch     4 | loss: 3.0724385Losses:  3.534276008605957 0.2902124226093292 0.8577782511711121
MemoryTrain:  epoch  5, batch     5 | loss: 3.5342760Losses:  3.067401647567749 0.5217673182487488 0.8968183398246765
MemoryTrain:  epoch  5, batch     6 | loss: 3.0674016Losses:  3.010009765625 0.6513234376907349 0.5049479007720947
MemoryTrain:  epoch  5, batch     7 | loss: 3.0100098Losses:  2.7953312397003174 0.2180684208869934 0.9315933585166931
MemoryTrain:  epoch  6, batch     0 | loss: 2.7953312Losses:  3.1965503692626953 0.9503514170646667 0.4853537976741791
MemoryTrain:  epoch  6, batch     1 | loss: 3.1965504Losses:  2.7585182189941406 -0.0 0.9401248693466187
MemoryTrain:  epoch  6, batch     2 | loss: 2.7585182Losses:  2.8364057540893555 0.25240999460220337 0.9714361429214478
MemoryTrain:  epoch  6, batch     3 | loss: 2.8364058Losses:  2.562134265899658 -0.0 0.7459659576416016
MemoryTrain:  epoch  6, batch     4 | loss: 2.5621343Losses:  2.4992353916168213 0.2700733542442322 0.8295401930809021
MemoryTrain:  epoch  6, batch     5 | loss: 2.4992354Losses:  3.336763858795166 0.2535877227783203 0.8654478788375854
MemoryTrain:  epoch  6, batch     6 | loss: 3.3367639Losses:  2.7322235107421875 -0.0 0.6879507303237915
MemoryTrain:  epoch  6, batch     7 | loss: 2.7322235Losses:  3.3703689575195312 0.8311043977737427 0.7364739179611206
MemoryTrain:  epoch  7, batch     0 | loss: 3.3703690Losses:  3.274839162826538 0.6415181159973145 0.8175151944160461
MemoryTrain:  epoch  7, batch     1 | loss: 3.2748392Losses:  2.5143752098083496 0.2809358239173889 0.7188776731491089
MemoryTrain:  epoch  7, batch     2 | loss: 2.5143752Losses:  2.973330497741699 0.7415082454681396 0.7165384292602539
MemoryTrain:  epoch  7, batch     3 | loss: 2.9733305Losses:  3.2374250888824463 0.7476014494895935 0.8598842620849609
MemoryTrain:  epoch  7, batch     4 | loss: 3.2374251Losses:  3.071402072906494 0.5195671319961548 0.8088594675064087
MemoryTrain:  epoch  7, batch     5 | loss: 3.0714021Losses:  3.5473880767822266 0.9975271224975586 0.797350287437439
MemoryTrain:  epoch  7, batch     6 | loss: 3.5473881Losses:  2.025088310241699 -0.0 0.621785044670105
MemoryTrain:  epoch  7, batch     7 | loss: 2.0250883Losses:  2.984520196914673 0.48313388228416443 0.7309191226959229
MemoryTrain:  epoch  8, batch     0 | loss: 2.9845202Losses:  3.436955451965332 0.831877589225769 0.9259605407714844
MemoryTrain:  epoch  8, batch     1 | loss: 3.4369555Losses:  3.1340858936309814 0.5444259643554688 0.7429032325744629
MemoryTrain:  epoch  8, batch     2 | loss: 3.1340859Losses:  2.7442352771759033 0.7017579674720764 0.7179114818572998
MemoryTrain:  epoch  8, batch     3 | loss: 2.7442353Losses:  3.0358951091766357 0.7165251970291138 0.7834489941596985
MemoryTrain:  epoch  8, batch     4 | loss: 3.0358951Losses:  3.0585834980010986 0.5370571613311768 0.8780459761619568
MemoryTrain:  epoch  8, batch     5 | loss: 3.0585835Losses:  2.248960256576538 -0.0 0.7188214659690857
MemoryTrain:  epoch  8, batch     6 | loss: 2.2489603Losses:  2.8197507858276367 0.2901611924171448 0.6068909764289856
MemoryTrain:  epoch  8, batch     7 | loss: 2.8197508Losses:  2.5424253940582275 0.24990996718406677 0.7178075909614563
MemoryTrain:  epoch  9, batch     0 | loss: 2.5424254Losses:  2.714080333709717 0.5110631585121155 0.8595222234725952
MemoryTrain:  epoch  9, batch     1 | loss: 2.7140803Losses:  2.156733512878418 0.2690659165382385 0.6542055010795593
MemoryTrain:  epoch  9, batch     2 | loss: 2.1567335Losses:  2.9561946392059326 0.4717693626880646 0.9720162749290466
MemoryTrain:  epoch  9, batch     3 | loss: 2.9561946Losses:  3.41147780418396 0.6544278264045715 0.8302772641181946
MemoryTrain:  epoch  9, batch     4 | loss: 3.4114778Losses:  2.61759877204895 0.23915347456932068 0.8092749714851379
MemoryTrain:  epoch  9, batch     5 | loss: 2.6175988Losses:  2.8887619972229004 0.2596115469932556 0.9173115491867065
MemoryTrain:  epoch  9, batch     6 | loss: 2.8887620Losses:  1.798887848854065 -0.0 0.5205928087234497
MemoryTrain:  epoch  9, batch     7 | loss: 1.7988878
[EVAL] batch:    0 | acc: 12.50%,  total acc: 12.50%   [EVAL] batch:    1 | acc: 31.25%,  total acc: 21.88%   [EVAL] batch:    2 | acc: 25.00%,  total acc: 22.92%   [EVAL] batch:    3 | acc: 31.25%,  total acc: 25.00%   [EVAL] batch:    4 | acc: 31.25%,  total acc: 26.25%   [EVAL] batch:    5 | acc: 37.50%,  total acc: 28.12%   [EVAL] batch:    6 | acc: 50.00%,  total acc: 31.25%   [EVAL] batch:    7 | acc: 68.75%,  total acc: 35.94%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 39.58%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 42.50%   [EVAL] batch:   10 | acc: 56.25%,  total acc: 43.75%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 46.35%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 49.52%   [EVAL] batch:   13 | acc: 81.25%,  total acc: 51.79%   [EVAL] batch:   14 | acc: 81.25%,  total acc: 53.75%   [EVAL] batch:   15 | acc: 81.25%,  total acc: 55.47%   [EVAL] batch:   16 | acc: 81.25%,  total acc: 56.99%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 59.03%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 61.18%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 62.50%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 64.29%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 65.91%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 67.39%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 68.49%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 69.75%   [EVAL] batch:   25 | acc: 62.50%,  total acc: 69.47%   [EVAL] batch:   26 | acc: 56.25%,  total acc: 68.98%   [EVAL] batch:   27 | acc: 68.75%,  total acc: 68.97%   [EVAL] batch:   28 | acc: 56.25%,  total acc: 68.53%   [EVAL] batch:   29 | acc: 62.50%,  total acc: 68.33%   [EVAL] batch:   30 | acc: 62.50%,  total acc: 68.15%   [EVAL] batch:   31 | acc: 68.75%,  total acc: 68.16%   [EVAL] batch:   32 | acc: 62.50%,  total acc: 67.99%   [EVAL] batch:   33 | acc: 18.75%,  total acc: 66.54%   [EVAL] batch:   34 | acc: 31.25%,  total acc: 65.54%   [EVAL] batch:   35 | acc: 18.75%,  total acc: 64.24%   [EVAL] batch:   36 | acc: 12.50%,  total acc: 62.84%   [EVAL] batch:   37 | acc: 68.75%,  total acc: 62.99%   [EVAL] batch:   38 | acc: 75.00%,  total acc: 63.30%   [EVAL] batch:   39 | acc: 68.75%,  total acc: 63.44%   [EVAL] batch:   40 | acc: 62.50%,  total acc: 63.41%   [EVAL] batch:   41 | acc: 68.75%,  total acc: 63.54%   [EVAL] batch:   42 | acc: 62.50%,  total acc: 63.52%   [EVAL] batch:   43 | acc: 56.25%,  total acc: 63.35%   [EVAL] batch:   44 | acc: 75.00%,  total acc: 63.61%   [EVAL] batch:   45 | acc: 75.00%,  total acc: 63.86%   [EVAL] batch:   46 | acc: 87.50%,  total acc: 64.36%   [EVAL] batch:   47 | acc: 81.25%,  total acc: 64.71%   [EVAL] batch:   48 | acc: 75.00%,  total acc: 64.92%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 65.38%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 66.05%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 66.71%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 67.33%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 67.94%   [EVAL] batch:   54 | acc: 100.00%,  total acc: 68.52%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 69.08%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 69.63%   [EVAL] batch:   57 | acc: 100.00%,  total acc: 70.15%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 70.66%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 71.15%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 71.62%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 72.08%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 71.73%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 82.81%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 82.50%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:    6 | acc: 43.75%,  total acc: 78.57%   [EVAL] batch:    7 | acc: 62.50%,  total acc: 76.56%   [EVAL] batch:    8 | acc: 50.00%,  total acc: 73.61%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 73.75%   [EVAL] batch:   10 | acc: 43.75%,  total acc: 71.02%   [EVAL] batch:   11 | acc: 43.75%,  total acc: 68.75%   [EVAL] batch:   12 | acc: 62.50%,  total acc: 68.27%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 70.09%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 71.67%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 73.44%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 76.04%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 77.30%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 77.81%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 78.57%   [EVAL] batch:   21 | acc: 68.75%,  total acc: 78.12%   [EVAL] batch:   22 | acc: 81.25%,  total acc: 78.26%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 78.91%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 79.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 80.05%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 80.79%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 81.47%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 81.90%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 82.50%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 83.06%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 83.59%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 84.09%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 84.56%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 84.82%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 85.24%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 85.64%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 86.02%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 86.38%   [EVAL] batch:   39 | acc: 93.75%,  total acc: 86.56%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 86.89%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 87.05%   [EVAL] batch:   42 | acc: 87.50%,  total acc: 87.06%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 87.22%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 87.22%   [EVAL] batch:   45 | acc: 87.50%,  total acc: 87.23%   [EVAL] batch:   46 | acc: 75.00%,  total acc: 86.97%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 87.11%   [EVAL] batch:   48 | acc: 87.50%,  total acc: 87.12%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 87.12%   [EVAL] batch:   50 | acc: 81.25%,  total acc: 87.01%   [EVAL] batch:   51 | acc: 81.25%,  total acc: 86.90%   [EVAL] batch:   52 | acc: 75.00%,  total acc: 86.67%   [EVAL] batch:   53 | acc: 75.00%,  total acc: 86.46%   [EVAL] batch:   54 | acc: 56.25%,  total acc: 85.91%   [EVAL] batch:   55 | acc: 81.25%,  total acc: 85.83%   [EVAL] batch:   56 | acc: 87.50%,  total acc: 85.86%   [EVAL] batch:   57 | acc: 81.25%,  total acc: 85.78%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 86.02%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 86.15%   [EVAL] batch:   60 | acc: 81.25%,  total acc: 86.07%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 86.19%   [EVAL] batch:   62 | acc: 93.75%,  total acc: 86.31%   [EVAL] batch:   63 | acc: 87.50%,  total acc: 86.33%   [EVAL] batch:   64 | acc: 87.50%,  total acc: 86.35%   [EVAL] batch:   65 | acc: 93.75%,  total acc: 86.46%   [EVAL] batch:   66 | acc: 81.25%,  total acc: 86.38%   [EVAL] batch:   67 | acc: 93.75%,  total acc: 86.49%   [EVAL] batch:   68 | acc: 87.50%,  total acc: 86.50%   [EVAL] batch:   69 | acc: 75.00%,  total acc: 86.34%   [EVAL] batch:   70 | acc: 81.25%,  total acc: 86.27%   [EVAL] batch:   71 | acc: 75.00%,  total acc: 86.11%   [EVAL] batch:   72 | acc: 81.25%,  total acc: 86.04%   [EVAL] batch:   73 | acc: 68.75%,  total acc: 85.81%   [EVAL] batch:   74 | acc: 75.00%,  total acc: 85.67%   [EVAL] batch:   75 | acc: 75.00%,  total acc: 85.53%   [EVAL] batch:   76 | acc: 81.25%,  total acc: 85.47%   [EVAL] batch:   77 | acc: 68.75%,  total acc: 85.26%   [EVAL] batch:   78 | acc: 43.75%,  total acc: 84.73%   [EVAL] batch:   79 | acc: 81.25%,  total acc: 84.69%   [EVAL] batch:   80 | acc: 68.75%,  total acc: 84.49%   [EVAL] batch:   81 | acc: 56.25%,  total acc: 84.15%   [EVAL] batch:   82 | acc: 50.00%,  total acc: 83.73%   [EVAL] batch:   83 | acc: 50.00%,  total acc: 83.33%   [EVAL] batch:   84 | acc: 50.00%,  total acc: 82.94%   [EVAL] batch:   85 | acc: 37.50%,  total acc: 82.41%   [EVAL] batch:   86 | acc: 43.75%,  total acc: 81.97%   [EVAL] batch:   87 | acc: 62.50%,  total acc: 81.75%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 81.95%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 82.15%   [EVAL] batch:   90 | acc: 93.75%,  total acc: 82.28%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 82.47%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 82.66%   [EVAL] batch:   93 | acc: 93.75%,  total acc: 82.78%   [EVAL] batch:   94 | acc: 56.25%,  total acc: 82.50%   [EVAL] batch:   95 | acc: 68.75%,  total acc: 82.36%   [EVAL] batch:   96 | acc: 56.25%,  total acc: 82.09%   [EVAL] batch:   97 | acc: 50.00%,  total acc: 81.76%   [EVAL] batch:   98 | acc: 75.00%,  total acc: 81.69%   [EVAL] batch:   99 | acc: 37.50%,  total acc: 81.25%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 81.44%   [EVAL] batch:  101 | acc: 100.00%,  total acc: 81.62%   [EVAL] batch:  102 | acc: 100.00%,  total acc: 81.80%   [EVAL] batch:  103 | acc: 100.00%,  total acc: 81.97%   [EVAL] batch:  104 | acc: 100.00%,  total acc: 82.14%   [EVAL] batch:  105 | acc: 93.75%,  total acc: 82.25%   [EVAL] batch:  106 | acc: 100.00%,  total acc: 82.42%   [EVAL] batch:  107 | acc: 100.00%,  total acc: 82.58%   [EVAL] batch:  108 | acc: 100.00%,  total acc: 82.74%   [EVAL] batch:  109 | acc: 100.00%,  total acc: 82.90%   [EVAL] batch:  110 | acc: 100.00%,  total acc: 83.05%   [EVAL] batch:  111 | acc: 100.00%,  total acc: 83.20%   [EVAL] batch:  112 | acc: 100.00%,  total acc: 83.35%   [EVAL] batch:  113 | acc: 87.50%,  total acc: 83.39%   [EVAL] batch:  114 | acc: 93.75%,  total acc: 83.48%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 83.62%   [EVAL] batch:  116 | acc: 93.75%,  total acc: 83.71%   [EVAL] batch:  117 | acc: 93.75%,  total acc: 83.79%   [EVAL] batch:  118 | acc: 81.25%,  total acc: 83.77%   [EVAL] batch:  119 | acc: 93.75%,  total acc: 83.85%   [EVAL] batch:  120 | acc: 62.50%,  total acc: 83.68%   [EVAL] batch:  121 | acc: 93.75%,  total acc: 83.76%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 83.89%   [EVAL] batch:  123 | acc: 81.25%,  total acc: 83.87%   [EVAL] batch:  124 | acc: 81.25%,  total acc: 83.85%   [EVAL] batch:  125 | acc: 25.00%,  total acc: 83.38%   [EVAL] batch:  126 | acc: 31.25%,  total acc: 82.97%   [EVAL] batch:  127 | acc: 25.00%,  total acc: 82.52%   [EVAL] batch:  128 | acc: 12.50%,  total acc: 81.98%   [EVAL] batch:  129 | acc: 43.75%,  total acc: 81.68%   [EVAL] batch:  130 | acc: 31.25%,  total acc: 81.30%   [EVAL] batch:  131 | acc: 68.75%,  total acc: 81.20%   [EVAL] batch:  132 | acc: 81.25%,  total acc: 81.20%   [EVAL] batch:  133 | acc: 93.75%,  total acc: 81.30%   [EVAL] batch:  134 | acc: 93.75%,  total acc: 81.39%   [EVAL] batch:  135 | acc: 81.25%,  total acc: 81.39%   [EVAL] batch:  136 | acc: 56.25%,  total acc: 81.20%   [EVAL] batch:  137 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:  138 | acc: 87.50%,  total acc: 81.29%   [EVAL] batch:  139 | acc: 100.00%,  total acc: 81.43%   [EVAL] batch:  140 | acc: 100.00%,  total acc: 81.56%   [EVAL] batch:  141 | acc: 93.75%,  total acc: 81.65%   [EVAL] batch:  142 | acc: 87.50%,  total acc: 81.69%   [EVAL] batch:  143 | acc: 68.75%,  total acc: 81.60%   [EVAL] batch:  144 | acc: 62.50%,  total acc: 81.47%   [EVAL] batch:  145 | acc: 37.50%,  total acc: 81.16%   [EVAL] batch:  146 | acc: 68.75%,  total acc: 81.08%   [EVAL] batch:  147 | acc: 68.75%,  total acc: 81.00%   [EVAL] batch:  148 | acc: 43.75%,  total acc: 80.75%   [EVAL] batch:  149 | acc: 56.25%,  total acc: 80.58%   [EVAL] batch:  150 | acc: 6.25%,  total acc: 80.09%   [EVAL] batch:  151 | acc: 6.25%,  total acc: 79.61%   [EVAL] batch:  152 | acc: 6.25%,  total acc: 79.13%   [EVAL] batch:  153 | acc: 0.00%,  total acc: 78.61%   [EVAL] batch:  154 | acc: 6.25%,  total acc: 78.15%   [EVAL] batch:  155 | acc: 0.00%,  total acc: 77.64%   [EVAL] batch:  156 | acc: 56.25%,  total acc: 77.51%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 77.61%   [EVAL] batch:  158 | acc: 87.50%,  total acc: 77.67%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 77.77%   [EVAL] batch:  160 | acc: 93.75%,  total acc: 77.87%   [EVAL] batch:  161 | acc: 81.25%,  total acc: 77.89%   [EVAL] batch:  162 | acc: 100.00%,  total acc: 78.03%   [EVAL] batch:  163 | acc: 81.25%,  total acc: 78.05%   [EVAL] batch:  164 | acc: 100.00%,  total acc: 78.18%   [EVAL] batch:  165 | acc: 93.75%,  total acc: 78.28%   [EVAL] batch:  166 | acc: 81.25%,  total acc: 78.29%   [EVAL] batch:  167 | acc: 93.75%,  total acc: 78.39%   [EVAL] batch:  168 | acc: 87.50%,  total acc: 78.44%   [EVAL] batch:  169 | acc: 37.50%,  total acc: 78.20%   [EVAL] batch:  170 | acc: 25.00%,  total acc: 77.89%   [EVAL] batch:  171 | acc: 25.00%,  total acc: 77.58%   [EVAL] batch:  172 | acc: 50.00%,  total acc: 77.42%   [EVAL] batch:  173 | acc: 56.25%,  total acc: 77.30%   [EVAL] batch:  174 | acc: 37.50%,  total acc: 77.07%   [EVAL] batch:  175 | acc: 56.25%,  total acc: 76.95%   [EVAL] batch:  176 | acc: 62.50%,  total acc: 76.87%   [EVAL] batch:  177 | acc: 37.50%,  total acc: 76.65%   [EVAL] batch:  178 | acc: 75.00%,  total acc: 76.64%   [EVAL] batch:  179 | acc: 56.25%,  total acc: 76.53%   [EVAL] batch:  180 | acc: 75.00%,  total acc: 76.52%   [EVAL] batch:  181 | acc: 25.00%,  total acc: 76.24%   [EVAL] batch:  182 | acc: 31.25%,  total acc: 75.99%   [EVAL] batch:  183 | acc: 43.75%,  total acc: 75.82%   [EVAL] batch:  184 | acc: 31.25%,  total acc: 75.57%   [EVAL] batch:  185 | acc: 43.75%,  total acc: 75.40%   [EVAL] batch:  186 | acc: 25.00%,  total acc: 75.13%   [EVAL] batch:  187 | acc: 18.75%,  total acc: 74.83%   [EVAL] batch:  188 | acc: 25.00%,  total acc: 74.57%   [EVAL] batch:  189 | acc: 31.25%,  total acc: 74.34%   [EVAL] batch:  190 | acc: 37.50%,  total acc: 74.15%   [EVAL] batch:  191 | acc: 25.00%,  total acc: 73.89%   [EVAL] batch:  192 | acc: 31.25%,  total acc: 73.67%   [EVAL] batch:  193 | acc: 37.50%,  total acc: 73.49%   [EVAL] batch:  194 | acc: 62.50%,  total acc: 73.43%   [EVAL] batch:  195 | acc: 68.75%,  total acc: 73.41%   [EVAL] batch:  196 | acc: 68.75%,  total acc: 73.38%   [EVAL] batch:  197 | acc: 81.25%,  total acc: 73.42%   [EVAL] batch:  198 | acc: 37.50%,  total acc: 73.24%   [EVAL] batch:  199 | acc: 100.00%,  total acc: 73.38%   [EVAL] batch:  200 | acc: 75.00%,  total acc: 73.38%   [EVAL] batch:  201 | acc: 81.25%,  total acc: 73.42%   [EVAL] batch:  202 | acc: 87.50%,  total acc: 73.49%   [EVAL] batch:  203 | acc: 75.00%,  total acc: 73.50%   [EVAL] batch:  204 | acc: 93.75%,  total acc: 73.60%   [EVAL] batch:  205 | acc: 93.75%,  total acc: 73.70%   [EVAL] batch:  206 | acc: 93.75%,  total acc: 73.79%   [EVAL] batch:  207 | acc: 93.75%,  total acc: 73.89%   [EVAL] batch:  208 | acc: 100.00%,  total acc: 74.01%   [EVAL] batch:  209 | acc: 100.00%,  total acc: 74.14%   [EVAL] batch:  210 | acc: 93.75%,  total acc: 74.23%   [EVAL] batch:  211 | acc: 100.00%,  total acc: 74.35%   [EVAL] batch:  212 | acc: 75.00%,  total acc: 74.35%   [EVAL] batch:  213 | acc: 68.75%,  total acc: 74.33%   [EVAL] batch:  214 | acc: 56.25%,  total acc: 74.24%   [EVAL] batch:  215 | acc: 68.75%,  total acc: 74.22%   [EVAL] batch:  216 | acc: 50.00%,  total acc: 74.11%   [EVAL] batch:  217 | acc: 68.75%,  total acc: 74.08%   [EVAL] batch:  218 | acc: 62.50%,  total acc: 74.03%   [EVAL] batch:  219 | acc: 68.75%,  total acc: 74.01%   [EVAL] batch:  220 | acc: 31.25%,  total acc: 73.81%   [EVAL] batch:  221 | acc: 31.25%,  total acc: 73.62%   [EVAL] batch:  222 | acc: 25.00%,  total acc: 73.40%   [EVAL] batch:  223 | acc: 18.75%,  total acc: 73.16%   [EVAL] batch:  224 | acc: 43.75%,  total acc: 73.03%   [EVAL] batch:  225 | acc: 62.50%,  total acc: 72.98%   [EVAL] batch:  226 | acc: 68.75%,  total acc: 72.96%   [EVAL] batch:  227 | acc: 68.75%,  total acc: 72.94%   [EVAL] batch:  228 | acc: 68.75%,  total acc: 72.93%   [EVAL] batch:  229 | acc: 68.75%,  total acc: 72.91%   [EVAL] batch:  230 | acc: 50.00%,  total acc: 72.81%   [EVAL] batch:  231 | acc: 75.00%,  total acc: 72.82%   [EVAL] batch:  232 | acc: 68.75%,  total acc: 72.80%   [EVAL] batch:  233 | acc: 87.50%,  total acc: 72.86%   [EVAL] batch:  234 | acc: 81.25%,  total acc: 72.90%   [EVAL] batch:  235 | acc: 75.00%,  total acc: 72.91%   [EVAL] batch:  236 | acc: 87.50%,  total acc: 72.97%   [EVAL] batch:  237 | acc: 93.75%,  total acc: 73.06%   [EVAL] batch:  238 | acc: 100.00%,  total acc: 73.17%   [EVAL] batch:  239 | acc: 100.00%,  total acc: 73.28%   [EVAL] batch:  240 | acc: 100.00%,  total acc: 73.39%   [EVAL] batch:  241 | acc: 100.00%,  total acc: 73.50%   [EVAL] batch:  242 | acc: 100.00%,  total acc: 73.61%   [EVAL] batch:  243 | acc: 100.00%,  total acc: 73.72%   [EVAL] batch:  244 | acc: 100.00%,  total acc: 73.83%   [EVAL] batch:  245 | acc: 100.00%,  total acc: 73.93%   [EVAL] batch:  246 | acc: 100.00%,  total acc: 74.04%   [EVAL] batch:  247 | acc: 100.00%,  total acc: 74.14%   [EVAL] batch:  248 | acc: 100.00%,  total acc: 74.25%   [EVAL] batch:  249 | acc: 100.00%,  total acc: 74.35%   
cur_acc:  ['0.9494', '0.8244', '0.5962', '0.7173']
his_acc:  ['0.9494', '0.8760', '0.7746', '0.7435']
Clustering into  24  clusters
Clusters:  [ 3  5 10 16  4 21 10  2  8 13 16 19  3  8  1 11  5 14  6  5  4 17  2 20
 18 23  2  8  2  8 18  9  7  1  2 15  8 11  0  0  5  2  2 17  6 12  7 22
  1 22]
Losses:  10.958109855651855 3.911088705062866 0.7918274998664856
CurrentTrain: epoch  0, batch     0 | loss: 10.9581099Losses:  12.577001571655273 5.1741228103637695 0.6470109224319458
CurrentTrain: epoch  0, batch     1 | loss: 12.5770016Losses:  8.282914161682129 2.521923542022705 0.7426393032073975
CurrentTrain: epoch  0, batch     2 | loss: 8.2829142Losses:  5.2206807136535645 -0.0 0.14163997769355774
CurrentTrain: epoch  0, batch     3 | loss: 5.2206807Losses:  8.605376243591309 3.614022970199585 0.6567577719688416
CurrentTrain: epoch  1, batch     0 | loss: 8.6053762Losses:  9.90198040008545 4.388011932373047 0.6227177977561951
CurrentTrain: epoch  1, batch     1 | loss: 9.9019804Losses:  10.270624160766602 3.5342459678649902 0.7036052942276001
CurrentTrain: epoch  1, batch     2 | loss: 10.2706242Losses:  7.758770942687988 -0.0 0.0984555184841156
CurrentTrain: epoch  1, batch     3 | loss: 7.7587709Losses:  8.18444538116455 2.402493953704834 0.6851586699485779
CurrentTrain: epoch  2, batch     0 | loss: 8.1844454Losses:  7.512337684631348 2.8935580253601074 0.6867747902870178
CurrentTrain: epoch  2, batch     1 | loss: 7.5123377Losses:  9.469898223876953 3.951396942138672 0.7122592329978943
CurrentTrain: epoch  2, batch     2 | loss: 9.4698982Losses:  5.203692436218262 -0.0 0.10739091783761978
CurrentTrain: epoch  2, batch     3 | loss: 5.2036924Losses:  6.942227363586426 2.1270499229431152 0.6639364957809448
CurrentTrain: epoch  3, batch     0 | loss: 6.9422274Losses:  7.899947643280029 3.1444735527038574 0.6719598770141602
CurrentTrain: epoch  3, batch     1 | loss: 7.8999476Losses:  8.90566635131836 3.213517189025879 0.602075457572937
CurrentTrain: epoch  3, batch     2 | loss: 8.9056664Losses:  1.9941182136535645 -0.0 0.10714837163686752
CurrentTrain: epoch  3, batch     3 | loss: 1.9941182Losses:  8.797207832336426 3.7167809009552 0.6211625337600708
CurrentTrain: epoch  4, batch     0 | loss: 8.7972078Losses:  6.6504998207092285 2.8427224159240723 0.6716179251670837
CurrentTrain: epoch  4, batch     1 | loss: 6.6504998Losses:  10.06507682800293 4.779966831207275 0.4800991714000702
CurrentTrain: epoch  4, batch     2 | loss: 10.0650768Losses:  3.721013307571411 -0.0 0.12125447392463684
CurrentTrain: epoch  4, batch     3 | loss: 3.7210133Losses:  8.51130485534668 3.5490798950195312 0.5372476577758789
CurrentTrain: epoch  5, batch     0 | loss: 8.5113049Losses:  8.514724731445312 3.5351314544677734 0.6792910099029541
CurrentTrain: epoch  5, batch     1 | loss: 8.5147247Losses:  7.221714973449707 3.7070603370666504 0.6038200259208679
CurrentTrain: epoch  5, batch     2 | loss: 7.2217150Losses:  2.4145379066467285 -0.0 0.10239264369010925
CurrentTrain: epoch  5, batch     3 | loss: 2.4145379Losses:  8.339140892028809 3.9648585319519043 0.6234514117240906
CurrentTrain: epoch  6, batch     0 | loss: 8.3391409Losses:  7.62917423248291 3.6832072734832764 0.55897057056427
CurrentTrain: epoch  6, batch     1 | loss: 7.6291742Losses:  8.272911071777344 3.9866554737091064 0.4718497693538666
CurrentTrain: epoch  6, batch     2 | loss: 8.2729111Losses:  2.977461576461792 -0.0 0.10107972472906113
CurrentTrain: epoch  6, batch     3 | loss: 2.9774616Losses:  6.438755035400391 2.618079662322998 0.5986192226409912
CurrentTrain: epoch  7, batch     0 | loss: 6.4387550Losses:  7.27186918258667 2.8830747604370117 0.5867725610733032
CurrentTrain: epoch  7, batch     1 | loss: 7.2718692Losses:  5.949589729309082 2.3239338397979736 0.6600644588470459
CurrentTrain: epoch  7, batch     2 | loss: 5.9495897Losses:  4.530208587646484 -0.0 0.12135235220193863
CurrentTrain: epoch  7, batch     3 | loss: 4.5302086Losses:  6.2006025314331055 2.6857175827026367 0.6490070819854736
CurrentTrain: epoch  8, batch     0 | loss: 6.2006025Losses:  6.7707719802856445 2.606224536895752 0.6465612053871155
CurrentTrain: epoch  8, batch     1 | loss: 6.7707720Losses:  5.884433269500732 2.6521413326263428 0.5687717795372009
CurrentTrain: epoch  8, batch     2 | loss: 5.8844333Losses:  1.807166337966919 -0.0 0.13349968194961548
CurrentTrain: epoch  8, batch     3 | loss: 1.8071663Losses:  6.54985237121582 3.1629996299743652 0.547651469707489
CurrentTrain: epoch  9, batch     0 | loss: 6.5498524Losses:  5.432229042053223 2.496464729309082 0.640294075012207
CurrentTrain: epoch  9, batch     1 | loss: 5.4322290Losses:  5.3031511306762695 2.192835807800293 0.575982928276062
CurrentTrain: epoch  9, batch     2 | loss: 5.3031511Losses:  2.8524980545043945 -0.0 0.1013125479221344
CurrentTrain: epoch  9, batch     3 | loss: 2.8524981
#### applies to jurisdiction ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "applies to jurisdiction" means the item (institution, law, public office, public register...) or statement belongs to or has power over or applies to the value (a territorial jurisdiction: a country, state, municipality, ...).
Here is an example:
Relation: applies to jurisdiction
Context: george lawson sheldon ( may 31 , 1870 – april 4 , 1960 ) was 14th governor of nebraska with service from 1907 until 1909 .
Head Entity: governor
Tail Entity: nebraska
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "applies to jurisdiction":
Relation: applies to jurisdiction  
Context: The Environmental Protection Agency (EPA) is responsible for enforcing regulations related to environmental protection in the United States.  
Head Entity: Environmental Protection Agency  
Tail Entity: United States  

Relation: applies to jurisdiction  
Context: The California Consumer Privacy Act (CCPA) provides residents of California with specific rights regarding their personal information.  
Head Entity: California Consumer Privacy Act  
Tail Entity: California  
#### performer ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "performer" means actor, musician, band or other performer associated with this role or musical work.
Here is an example:
Relation: performer
Context: " tenth avenue freeze - out " is the second song on bruce springsteen 's breakthrough album " born to run " , released in 1975 .
Head Entity: tenth avenue freeze - out
Tail Entity: bruce springsteen
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "performer":
Relation: performer  
Context: "Shape of You" is a popular song by Ed Sheeran that topped charts worldwide after its release in 2017.  
Head Entity: Shape of You  
Tail Entity: Ed Sheeran  

Relation: performer  
Context: The Beatles are widely regarded as one of the most influential bands in the history of music, with hits like "Hey Jude" and "Let It Be."  
Head Entity: The Beatles  
Tail Entity: Hey Jude  
#### manufacturer ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "manufacturer" means manufacturer or producer of this product.
Here is an example:
Relation: manufacturer
Context: the next full - frame digital slrs were the canon eos-1ds of late 2002 , followed by kodak 's dcs pro 14n in 2003 .
Head Entity: dcs pro 14n
Tail Entity: kodak
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "manufacturer":
Relation: manufacturer  
Context: The latest smartphone model, the Galaxy S21, was developed by Samsung and released in early 2021.  
Head Entity: Galaxy S21  
Tail Entity: Samsung  

Relation: manufacturer  
Context: The iconic Model T was produced by Ford Motor Company and revolutionized the automotive industry in the early 20th century.  
Head Entity: Model T  
Tail Entity: Ford Motor Company  
#### participant ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "participant" means person, group of people or organization (object) that actively takes/took part in an event or process.
Here is an example:
Relation: participant
Context: at the 2016 summer olympics , heath and schofield won the silver k-2 200 event , finishing behind spain 's saúl craviotto and cristian toro .
Head Entity: 2016 summer olympics
Tail Entity: spain
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "participant":
Relation: participant  
Context: during the 2020 presidential election, the democratic party nominated joe biden as their candidate, while the republican party supported donald trump.  
Head Entity: 2020 presidential election  
Tail Entity: democratic party  

Relation: participant  
Context: in the 2022 world cup, france emerged as the champion, defeating argentina in a thrilling final match.  
Head Entity: 2022 world cup  
Tail Entity: france  
#### operating system ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "operating system" means operating system (OS) on which a software works or the OS installed on hardware.
Here is an example:
Relation: operating system
Context: microsoft has positioned the mail app ( bundled with windows 10 , available in windows store ) as a replacement for it .
Head Entity: mail
Tail Entity: windows 10
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "operating system":
Relation: operating system  
Context: The latest version of the software is compatible with macOS Monterey, ensuring a smooth user experience for Apple users.  
Head Entity: software  
Tail Entity: macOS Monterey  

Relation: operating system  
Context: Many applications are designed specifically for Ubuntu, making it a popular choice among developers and tech enthusiasts.  
Head Entity: applications  
Tail Entity: Ubuntu  
#### competition class ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "competition class" means official classification by a regulating body under which the subject (events, teams, participants, or equipment) qualifies for inclusion.
Here is an example:
Relation: competition class
Context: in july 2007 , griffin lost by a technical knockout to light heavyweight champion danny green of the world boxing association at challenge stadium in perth , australia .
Head Entity: danny green
Tail Entity: light heavyweight
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "competition class":
Relation: competition class  
Context: during the 2020 olympics, the men's 100 meters final showcased athletes from various competition classes, with usain bolt representing the sprinting class.  
Head Entity: usain bolt  
Tail Entity: sprinting  

Relation: competition class  
Context: the formula one race in monaco is known for its high-speed competition class, where lewis hamilton competed fiercely against other top drivers.  
Head Entity: lewis hamilton  
Tail Entity: formula one
#### successful candidate ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "successful candidate" means person elected after the election.
Here is an example:
Relation: successful candidate
Context: the qup gained 3 seats from its previous election result , and won a 10 % swing to it in primary votes , but failed to defeat the incumbent labor government led by ted theodore .
Head Entity: previous election
Tail Entity: ted theodore
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "successful candidate":
Relation: successful candidate  
Context: After a closely contested campaign, the voters chose Sarah Johnson as their representative, marking a significant shift in the political landscape of the district.  
Head Entity: closely contested campaign  
Tail Entity: Sarah Johnson  

Relation: successful candidate  
Context: In the recent mayoral election, the community rallied behind Mark Thompson, who promised to bring new initiatives to the city, ultimately leading to his victory.  
Head Entity: recent mayoral election  
Tail Entity: Mark Thompson  
#### sibling ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "sibling" means the subject and the object have the same parents.
Here is an example:
Relation: sibling
Context: although the latter 's eldest son , lǫgmaðr , appears to have succeeded to the kingship , he was soon forced to contend with factions supporting his younger brothers : haraldr , and óláfr .
Head Entity: lǫgmaðr
Tail Entity: óláfr
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "sibling":
Relation: sibling  
Context: In the royal family of the ancient kingdom, the two brothers, Eirik and Sigurd, were known for their fierce rivalry, yet they shared a bond that only siblings could understand.  
Head Entity: Eirik  
Tail Entity: Sigurd  

Relation: sibling  
Context: During the family reunion, it became clear that the two sisters, Maria and Elena, had grown apart over the years, but their shared childhood memories reminded them of their strong sibling connection.  
Head Entity: Maria  
Tail Entity: Elena  
#### military branch ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "military branch" means branch to which this military unit, award, office, or person belongs.
Here is an example:
Relation: military branch
Context: dennis chalker is a retired navy seal , inventor and author who has written six books about the united states navy seals .
Head Entity: dennis chalker
Tail Entity: united states navy
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "military branch":
Relation: military branch  
Context: general john smith served in the air force for over twenty years, leading numerous missions and earning several commendations.  
Head Entity: general john smith  
Tail Entity: air force  

Relation: military branch  
Context: the distinguished service cross was awarded to captain elizabeth jones for her bravery while serving in the army during a critical operation.  
Head Entity: captain elizabeth jones  
Tail Entity: army  
#### child ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "child" means subject has object as child.
Here is an example:
Relation: child
Context: ziad rahbani is the son of the lebanese famous composer assi rahbani and nouhad haddad , the famous lebanese female singer known as fairuz .
Head Entity: assi rahbani
Tail Entity: ziad rahbani
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "child":
Relation: child  
Context: in the animated series, the character of simon is the son of the adventurous couple, marie and tom.  
Head Entity: marie  
Tail Entity: simon  

Relation: child  
Context: the renowned physicist albert einstein had a daughter named lieserl, who was born in 1902.  
Head Entity: albert einstein  
Tail Entity: lieserl  
Losses:  4.974179744720459 0.2703406512737274 0.8899855613708496
MemoryTrain:  epoch  0, batch     0 | loss: 4.9741797Losses:  5.785573959350586 1.3547999858856201 0.838053822517395
MemoryTrain:  epoch  0, batch     1 | loss: 5.7855740Losses:  4.637278079986572 0.2799111604690552 0.9261106252670288
MemoryTrain:  epoch  0, batch     2 | loss: 4.6372781Losses:  3.859753370285034 0.5343149900436401 0.9521247744560242
MemoryTrain:  epoch  0, batch     3 | loss: 3.8597534Losses:  4.318443298339844 0.5256513357162476 0.9369462728500366
MemoryTrain:  epoch  0, batch     4 | loss: 4.3184433Losses:  6.126053810119629 0.918563961982727 0.9401125907897949
MemoryTrain:  epoch  0, batch     5 | loss: 6.1260538Losses:  5.011388778686523 0.6074488759040833 0.8876738548278809
MemoryTrain:  epoch  0, batch     6 | loss: 5.0113888Losses:  5.111835956573486 0.8656874895095825 0.9099640846252441
MemoryTrain:  epoch  0, batch     7 | loss: 5.1118360Losses:  3.6017839908599854 0.28689122200012207 0.7987999320030212
MemoryTrain:  epoch  0, batch     8 | loss: 3.6017840Losses:  3.377811908721924 -0.0 0.4661993682384491
MemoryTrain:  epoch  0, batch     9 | loss: 3.3778119Losses:  4.1476922035217285 0.2718474864959717 0.7885367274284363
MemoryTrain:  epoch  1, batch     0 | loss: 4.1476922Losses:  5.817946434020996 1.2858110666275024 0.8093533515930176
MemoryTrain:  epoch  1, batch     1 | loss: 5.8179464Losses:  4.023962020874023 0.39612191915512085 0.9193985462188721
MemoryTrain:  epoch  1, batch     2 | loss: 4.0239620Losses:  3.062501907348633 0.260336697101593 0.8093584179878235
MemoryTrain:  epoch  1, batch     3 | loss: 3.0625019Losses:  4.3529839515686035 0.9361242651939392 0.9050289988517761
MemoryTrain:  epoch  1, batch     4 | loss: 4.3529840Losses:  3.9839863777160645 0.7618201971054077 0.8730143308639526
MemoryTrain:  epoch  1, batch     5 | loss: 3.9839864Losses:  5.0250749588012695 0.63116055727005 0.9148523807525635
MemoryTrain:  epoch  1, batch     6 | loss: 5.0250750Losses:  3.3420016765594482 0.24068193137645721 0.9169468283653259
MemoryTrain:  epoch  1, batch     7 | loss: 3.3420017Losses:  3.747136116027832 0.581874430179596 0.7841586470603943
MemoryTrain:  epoch  1, batch     8 | loss: 3.7471361Losses:  2.658250093460083 -0.0 0.49916762113571167
MemoryTrain:  epoch  1, batch     9 | loss: 2.6582501Losses:  3.078047752380371 0.23037058115005493 0.9470838308334351
MemoryTrain:  epoch  2, batch     0 | loss: 3.0780478Losses:  3.755971908569336 0.5228161811828613 0.8261527419090271
MemoryTrain:  epoch  2, batch     1 | loss: 3.7559719Losses:  3.570291519165039 0.2521839737892151 0.8693379163742065
MemoryTrain:  epoch  2, batch     2 | loss: 3.5702915Losses:  3.133889675140381 0.24966922402381897 0.8768208622932434
MemoryTrain:  epoch  2, batch     3 | loss: 3.1338897Losses:  4.074819564819336 0.8060284852981567 0.85999596118927
MemoryTrain:  epoch  2, batch     4 | loss: 4.0748196Losses:  3.753617763519287 0.29587429761886597 1.0278122425079346
MemoryTrain:  epoch  2, batch     5 | loss: 3.7536178Losses:  3.136406898498535 0.4511435031890869 0.7616980671882629
MemoryTrain:  epoch  2, batch     6 | loss: 3.1364069Losses:  3.029038906097412 0.23933851718902588 0.9197214841842651
MemoryTrain:  epoch  2, batch     7 | loss: 3.0290389Losses:  3.2507083415985107 0.2395695149898529 0.8939318656921387
MemoryTrain:  epoch  2, batch     8 | loss: 3.2507083Losses:  2.1890649795532227 -0.0 0.4381704330444336
MemoryTrain:  epoch  2, batch     9 | loss: 2.1890650Losses:  3.5182363986968994 0.5689719915390015 0.8816463351249695
MemoryTrain:  epoch  3, batch     0 | loss: 3.5182364Losses:  3.2597334384918213 0.543592631816864 0.8203973770141602
MemoryTrain:  epoch  3, batch     1 | loss: 3.2597334Losses:  3.256124973297119 0.7823425531387329 0.6885972619056702
MemoryTrain:  epoch  3, batch     2 | loss: 3.2561250Losses:  3.1609487533569336 -0.0 1.0383894443511963
MemoryTrain:  epoch  3, batch     3 | loss: 3.1609488Losses:  3.7157235145568848 0.6315414905548096 0.9498456716537476
MemoryTrain:  epoch  3, batch     4 | loss: 3.7157235Losses:  3.5755207538604736 0.5192928910255432 0.867954432964325
MemoryTrain:  epoch  3, batch     5 | loss: 3.5755208Losses:  3.950885057449341 1.1226763725280762 0.8114809393882751
MemoryTrain:  epoch  3, batch     6 | loss: 3.9508851Losses:  3.168614149093628 0.3192518353462219 0.8440570831298828
MemoryTrain:  epoch  3, batch     7 | loss: 3.1686141Losses:  2.560755729675293 0.25428521633148193 0.8431682586669922
MemoryTrain:  epoch  3, batch     8 | loss: 2.5607557Losses:  1.9457098245620728 -0.0 0.44638529419898987
MemoryTrain:  epoch  3, batch     9 | loss: 1.9457098Losses:  3.2226624488830566 0.5274420380592346 0.8284210562705994
MemoryTrain:  epoch  4, batch     0 | loss: 3.2226624Losses:  2.6944432258605957 0.23668667674064636 0.9074820280075073
MemoryTrain:  epoch  4, batch     1 | loss: 2.6944432Losses:  3.4808671474456787 0.8085288405418396 0.8639640212059021
MemoryTrain:  epoch  4, batch     2 | loss: 3.4808671Losses:  2.8551254272460938 0.2541608214378357 1.0327905416488647
MemoryTrain:  epoch  4, batch     3 | loss: 2.8551254Losses:  2.757143020629883 0.2639232873916626 1.0383753776550293
MemoryTrain:  epoch  4, batch     4 | loss: 2.7571430Losses:  3.030302047729492 0.4971884787082672 0.9419498443603516
MemoryTrain:  epoch  4, batch     5 | loss: 3.0303020Losses:  3.511898994445801 0.8735171556472778 0.7745755910873413
MemoryTrain:  epoch  4, batch     6 | loss: 3.5118990Losses:  2.8163557052612305 -0.0 0.9791402220726013
MemoryTrain:  epoch  4, batch     7 | loss: 2.8163557Losses:  3.5529019832611084 0.5988850593566895 0.9073955416679382
MemoryTrain:  epoch  4, batch     8 | loss: 3.5529020Losses:  2.952075958251953 -0.0 0.5215097665786743
MemoryTrain:  epoch  4, batch     9 | loss: 2.9520760Losses:  3.188798427581787 0.5218403935432434 0.7919077277183533
MemoryTrain:  epoch  5, batch     0 | loss: 3.1887984Losses:  2.782172918319702 0.24970310926437378 0.9222745895385742
MemoryTrain:  epoch  5, batch     1 | loss: 2.7821729Losses:  3.116713523864746 0.54152512550354 0.8678321838378906
MemoryTrain:  epoch  5, batch     2 | loss: 3.1167135Losses:  3.4381144046783447 0.2673685550689697 0.9702767729759216
MemoryTrain:  epoch  5, batch     3 | loss: 3.4381144Losses:  2.863217830657959 0.7135964035987854 0.8677352666854858
MemoryTrain:  epoch  5, batch     4 | loss: 2.8632178Losses:  2.641407012939453 0.4747021794319153 0.8571547269821167
MemoryTrain:  epoch  5, batch     5 | loss: 2.6414070Losses:  3.6987924575805664 1.264661192893982 0.791347086429596
MemoryTrain:  epoch  5, batch     6 | loss: 3.6987925Losses:  2.588916778564453 0.2627519965171814 0.879906952381134
MemoryTrain:  epoch  5, batch     7 | loss: 2.5889168Losses:  3.1949729919433594 0.5631555318832397 0.8109294176101685
MemoryTrain:  epoch  5, batch     8 | loss: 3.1949730Losses:  2.202597141265869 -0.0 0.5063093900680542
MemoryTrain:  epoch  5, batch     9 | loss: 2.2025971Losses:  2.7900688648223877 0.3424786925315857 0.853509247303009
MemoryTrain:  epoch  6, batch     0 | loss: 2.7900689Losses:  3.0580368041992188 0.5350244045257568 0.7931097149848938
MemoryTrain:  epoch  6, batch     1 | loss: 3.0580368Losses:  2.5647165775299072 0.25061511993408203 0.8718960881233215
MemoryTrain:  epoch  6, batch     2 | loss: 2.5647166Losses:  2.8237786293029785 0.29730886220932007 0.9500621557235718
MemoryTrain:  epoch  6, batch     3 | loss: 2.8237786Losses:  3.0626182556152344 0.5928506255149841 0.933066725730896
MemoryTrain:  epoch  6, batch     4 | loss: 3.0626183Losses:  2.741344690322876 0.22923369705677032 1.0404211282730103
MemoryTrain:  epoch  6, batch     5 | loss: 2.7413447Losses:  2.90633487701416 0.5171090364456177 0.8586271405220032
MemoryTrain:  epoch  6, batch     6 | loss: 2.9063349Losses:  2.4934329986572266 0.23130685091018677 0.8655437231063843
MemoryTrain:  epoch  6, batch     7 | loss: 2.4934330Losses:  4.141809940338135 1.724571704864502 0.8162940144538879
MemoryTrain:  epoch  6, batch     8 | loss: 4.1418099Losses:  1.8927721977233887 -0.0 0.49480003118515015
MemoryTrain:  epoch  6, batch     9 | loss: 1.8927722Losses:  2.5856781005859375 0.23745284974575043 0.8817880153656006
MemoryTrain:  epoch  7, batch     0 | loss: 2.5856781Losses:  2.9684627056121826 0.5252876281738281 0.9190189242362976
MemoryTrain:  epoch  7, batch     1 | loss: 2.9684627Losses:  2.7797787189483643 0.6846340298652649 0.8902313113212585
MemoryTrain:  epoch  7, batch     2 | loss: 2.7797787Losses:  2.7451999187469482 0.2774428725242615 0.9122409820556641
MemoryTrain:  epoch  7, batch     3 | loss: 2.7451999Losses:  3.179821252822876 1.1155476570129395 0.653444230556488
MemoryTrain:  epoch  7, batch     4 | loss: 3.1798213Losses:  3.0555789470672607 0.49269843101501465 0.9348876476287842
MemoryTrain:  epoch  7, batch     5 | loss: 3.0555789Losses:  2.7594244480133057 0.30715641379356384 0.7833474278450012
MemoryTrain:  epoch  7, batch     6 | loss: 2.7594244Losses:  2.7710938453674316 0.2505533993244171 0.9111940264701843
MemoryTrain:  epoch  7, batch     7 | loss: 2.7710938Losses:  3.6075594425201416 1.011432409286499 0.8676654696464539
MemoryTrain:  epoch  7, batch     8 | loss: 3.6075594Losses:  1.7474584579467773 -0.0 0.4763495624065399
MemoryTrain:  epoch  7, batch     9 | loss: 1.7474585Losses:  2.76039457321167 0.6051396131515503 0.7581624984741211
MemoryTrain:  epoch  8, batch     0 | loss: 2.7603946Losses:  2.9323673248291016 0.47293126583099365 0.9100488424301147
MemoryTrain:  epoch  8, batch     1 | loss: 2.9323673Losses:  3.307234525680542 0.8424779772758484 0.8643842339515686
MemoryTrain:  epoch  8, batch     2 | loss: 3.3072345Losses:  2.55169677734375 0.2540389895439148 0.8602198958396912
MemoryTrain:  epoch  8, batch     3 | loss: 2.5516968Losses:  2.7427289485931396 0.49761661887168884 0.8639230132102966
MemoryTrain:  epoch  8, batch     4 | loss: 2.7427289Losses:  2.615743398666382 0.25192344188690186 0.9853973984718323
MemoryTrain:  epoch  8, batch     5 | loss: 2.6157434Losses:  2.8893368244171143 0.5618008971214294 0.8483845591545105
MemoryTrain:  epoch  8, batch     6 | loss: 2.8893368Losses:  2.485736846923828 0.2844422161579132 0.8469237089157104
MemoryTrain:  epoch  8, batch     7 | loss: 2.4857368Losses:  2.5735270977020264 0.2788918614387512 0.8388726711273193
MemoryTrain:  epoch  8, batch     8 | loss: 2.5735271Losses:  1.9260907173156738 -0.0 0.5388076305389404
MemoryTrain:  epoch  8, batch     9 | loss: 1.9260907Losses:  2.742380380630493 0.500017523765564 0.7864186763763428
MemoryTrain:  epoch  9, batch     0 | loss: 2.7423804Losses:  3.5662858486175537 1.190299153327942 0.844923734664917
MemoryTrain:  epoch  9, batch     1 | loss: 3.5662858Losses:  3.093888759613037 0.7553061246871948 0.9496631026268005
MemoryTrain:  epoch  9, batch     2 | loss: 3.0938888Losses:  2.9000940322875977 0.5381718277931213 0.8787650465965271
MemoryTrain:  epoch  9, batch     3 | loss: 2.9000940Losses:  2.501251220703125 0.24692240357398987 0.9619486331939697
MemoryTrain:  epoch  9, batch     4 | loss: 2.5012512Losses:  2.5586700439453125 0.26025813817977905 0.8351384997367859
MemoryTrain:  epoch  9, batch     5 | loss: 2.5586700Losses:  2.745222568511963 0.5320320129394531 0.8524487614631653
MemoryTrain:  epoch  9, batch     6 | loss: 2.7452226Losses:  2.736499309539795 0.5236174464225769 0.8982117176055908
MemoryTrain:  epoch  9, batch     7 | loss: 2.7364993Losses:  2.7388272285461426 0.23238341510295868 1.0322153568267822
MemoryTrain:  epoch  9, batch     8 | loss: 2.7388272Losses:  1.9062423706054688 -0.0 0.34341931343078613
MemoryTrain:  epoch  9, batch     9 | loss: 1.9062424
[EVAL] batch:    0 | acc: 56.25%,  total acc: 56.25%   [EVAL] batch:    1 | acc: 43.75%,  total acc: 50.00%   [EVAL] batch:    2 | acc: 56.25%,  total acc: 52.08%   [EVAL] batch:    3 | acc: 37.50%,  total acc: 48.44%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 52.50%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 57.29%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 61.61%   [EVAL] batch:    7 | acc: 68.75%,  total acc: 62.50%   [EVAL] batch:    8 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 63.12%   [EVAL] batch:   10 | acc: 56.25%,  total acc: 62.50%   [EVAL] batch:   11 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:   12 | acc: 100.00%,  total acc: 65.38%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 65.62%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 66.25%   [EVAL] batch:   15 | acc: 75.00%,  total acc: 66.80%   [EVAL] batch:   16 | acc: 81.25%,  total acc: 67.65%   [EVAL] batch:   17 | acc: 81.25%,  total acc: 68.40%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 69.74%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 70.62%   [EVAL] batch:   20 | acc: 68.75%,  total acc: 70.54%   [EVAL] batch:   21 | acc: 75.00%,  total acc: 70.74%   [EVAL] batch:   22 | acc: 75.00%,  total acc: 70.92%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 71.88%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 72.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 73.56%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 74.54%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 75.45%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 76.08%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 76.88%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 77.62%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 78.32%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 78.98%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 79.60%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 80.18%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 80.73%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 81.41%   [EVAL] batch:   38 | acc: 56.25%,  total acc: 80.77%   [EVAL] batch:   39 | acc: 62.50%,  total acc: 80.31%   [EVAL] batch:   40 | acc: 56.25%,  total acc: 79.73%   [EVAL] batch:   41 | acc: 75.00%,  total acc: 79.61%   [EVAL] batch:   42 | acc: 81.25%,  total acc: 79.65%   [EVAL] batch:   43 | acc: 62.50%,  total acc: 79.26%   [EVAL] batch:   44 | acc: 50.00%,  total acc: 78.61%   [EVAL] batch:   45 | acc: 31.25%,  total acc: 77.58%   [EVAL] batch:   46 | acc: 43.75%,  total acc: 76.86%   [EVAL] batch:   47 | acc: 56.25%,  total acc: 76.43%   [EVAL] batch:   48 | acc: 56.25%,  total acc: 76.02%   [EVAL] batch:   49 | acc: 50.00%,  total acc: 75.50%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 75.86%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 76.20%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 76.53%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 76.62%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 76.93%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 77.12%   [EVAL] batch:   56 | acc: 87.50%,  total acc: 77.30%   [EVAL] batch:   57 | acc: 75.00%,  total acc: 77.26%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 77.54%   [EVAL] batch:   59 | acc: 68.75%,  total acc: 77.40%   [EVAL] batch:   60 | acc: 81.25%,  total acc: 77.46%   [EVAL] batch:   61 | acc: 81.25%,  total acc: 77.52%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 77.08%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 82.81%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 83.33%   [EVAL] batch:    6 | acc: 43.75%,  total acc: 77.68%   [EVAL] batch:    7 | acc: 62.50%,  total acc: 75.78%   [EVAL] batch:    8 | acc: 50.00%,  total acc: 72.92%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 73.12%   [EVAL] batch:   10 | acc: 43.75%,  total acc: 70.45%   [EVAL] batch:   11 | acc: 37.50%,  total acc: 67.71%   [EVAL] batch:   12 | acc: 62.50%,  total acc: 67.31%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 69.20%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 70.83%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 72.66%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 73.90%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 75.00%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 76.32%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 76.56%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 77.08%   [EVAL] batch:   21 | acc: 75.00%,  total acc: 76.99%   [EVAL] batch:   22 | acc: 75.00%,  total acc: 76.90%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 77.60%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 78.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 78.85%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 79.63%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 80.36%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 80.82%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 81.46%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 82.06%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 82.62%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 83.14%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 83.64%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 83.93%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 84.38%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 84.80%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 85.20%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 85.58%   [EVAL] batch:   39 | acc: 93.75%,  total acc: 85.78%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 86.13%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 86.31%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 86.48%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 86.51%   [EVAL] batch:   44 | acc: 75.00%,  total acc: 86.25%   [EVAL] batch:   45 | acc: 75.00%,  total acc: 86.01%   [EVAL] batch:   46 | acc: 75.00%,  total acc: 85.77%   [EVAL] batch:   47 | acc: 75.00%,  total acc: 85.55%   [EVAL] batch:   48 | acc: 87.50%,  total acc: 85.59%   [EVAL] batch:   49 | acc: 75.00%,  total acc: 85.38%   [EVAL] batch:   50 | acc: 62.50%,  total acc: 84.93%   [EVAL] batch:   51 | acc: 62.50%,  total acc: 84.50%   [EVAL] batch:   52 | acc: 62.50%,  total acc: 84.08%   [EVAL] batch:   53 | acc: 68.75%,  total acc: 83.80%   [EVAL] batch:   54 | acc: 37.50%,  total acc: 82.95%   [EVAL] batch:   55 | acc: 81.25%,  total acc: 82.92%   [EVAL] batch:   56 | acc: 87.50%,  total acc: 83.00%   [EVAL] batch:   57 | acc: 75.00%,  total acc: 82.87%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 83.16%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 83.33%   [EVAL] batch:   60 | acc: 81.25%,  total acc: 83.30%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 83.47%   [EVAL] batch:   62 | acc: 93.75%,  total acc: 83.63%   [EVAL] batch:   63 | acc: 87.50%,  total acc: 83.69%   [EVAL] batch:   64 | acc: 81.25%,  total acc: 83.65%   [EVAL] batch:   65 | acc: 93.75%,  total acc: 83.81%   [EVAL] batch:   66 | acc: 81.25%,  total acc: 83.77%   [EVAL] batch:   67 | acc: 93.75%,  total acc: 83.92%   [EVAL] batch:   68 | acc: 87.50%,  total acc: 83.97%   [EVAL] batch:   69 | acc: 75.00%,  total acc: 83.84%   [EVAL] batch:   70 | acc: 81.25%,  total acc: 83.80%   [EVAL] batch:   71 | acc: 75.00%,  total acc: 83.68%   [EVAL] batch:   72 | acc: 75.00%,  total acc: 83.56%   [EVAL] batch:   73 | acc: 68.75%,  total acc: 83.36%   [EVAL] batch:   74 | acc: 68.75%,  total acc: 83.17%   [EVAL] batch:   75 | acc: 75.00%,  total acc: 83.06%   [EVAL] batch:   76 | acc: 75.00%,  total acc: 82.95%   [EVAL] batch:   77 | acc: 68.75%,  total acc: 82.77%   [EVAL] batch:   78 | acc: 50.00%,  total acc: 82.36%   [EVAL] batch:   79 | acc: 81.25%,  total acc: 82.34%   [EVAL] batch:   80 | acc: 62.50%,  total acc: 82.10%   [EVAL] batch:   81 | acc: 62.50%,  total acc: 81.86%   [EVAL] batch:   82 | acc: 56.25%,  total acc: 81.55%   [EVAL] batch:   83 | acc: 56.25%,  total acc: 81.25%   [EVAL] batch:   84 | acc: 50.00%,  total acc: 80.88%   [EVAL] batch:   85 | acc: 37.50%,  total acc: 80.38%   [EVAL] batch:   86 | acc: 37.50%,  total acc: 79.89%   [EVAL] batch:   87 | acc: 68.75%,  total acc: 79.76%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 79.99%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 80.21%   [EVAL] batch:   90 | acc: 93.75%,  total acc: 80.36%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 80.57%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 80.78%   [EVAL] batch:   93 | acc: 93.75%,  total acc: 80.92%   [EVAL] batch:   94 | acc: 56.25%,  total acc: 80.66%   [EVAL] batch:   95 | acc: 68.75%,  total acc: 80.53%   [EVAL] batch:   96 | acc: 62.50%,  total acc: 80.35%   [EVAL] batch:   97 | acc: 62.50%,  total acc: 80.17%   [EVAL] batch:   98 | acc: 75.00%,  total acc: 80.11%   [EVAL] batch:   99 | acc: 43.75%,  total acc: 79.75%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 79.95%   [EVAL] batch:  101 | acc: 100.00%,  total acc: 80.15%   [EVAL] batch:  102 | acc: 100.00%,  total acc: 80.34%   [EVAL] batch:  103 | acc: 93.75%,  total acc: 80.47%   [EVAL] batch:  104 | acc: 100.00%,  total acc: 80.65%   [EVAL] batch:  105 | acc: 93.75%,  total acc: 80.78%   [EVAL] batch:  106 | acc: 100.00%,  total acc: 80.96%   [EVAL] batch:  107 | acc: 100.00%,  total acc: 81.13%   [EVAL] batch:  108 | acc: 100.00%,  total acc: 81.31%   [EVAL] batch:  109 | acc: 100.00%,  total acc: 81.48%   [EVAL] batch:  110 | acc: 100.00%,  total acc: 81.64%   [EVAL] batch:  111 | acc: 100.00%,  total acc: 81.81%   [EVAL] batch:  112 | acc: 87.50%,  total acc: 81.86%   [EVAL] batch:  113 | acc: 62.50%,  total acc: 81.69%   [EVAL] batch:  114 | acc: 93.75%,  total acc: 81.79%   [EVAL] batch:  115 | acc: 87.50%,  total acc: 81.84%   [EVAL] batch:  116 | acc: 81.25%,  total acc: 81.84%   [EVAL] batch:  117 | acc: 93.75%,  total acc: 81.94%   [EVAL] batch:  118 | acc: 75.00%,  total acc: 81.88%   [EVAL] batch:  119 | acc: 87.50%,  total acc: 81.93%   [EVAL] batch:  120 | acc: 62.50%,  total acc: 81.77%   [EVAL] batch:  121 | acc: 93.75%,  total acc: 81.86%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 82.01%   [EVAL] batch:  123 | acc: 81.25%,  total acc: 82.01%   [EVAL] batch:  124 | acc: 81.25%,  total acc: 82.00%   [EVAL] batch:  125 | acc: 37.50%,  total acc: 81.65%   [EVAL] batch:  126 | acc: 37.50%,  total acc: 81.30%   [EVAL] batch:  127 | acc: 31.25%,  total acc: 80.91%   [EVAL] batch:  128 | acc: 31.25%,  total acc: 80.52%   [EVAL] batch:  129 | acc: 43.75%,  total acc: 80.24%   [EVAL] batch:  130 | acc: 37.50%,  total acc: 79.91%   [EVAL] batch:  131 | acc: 68.75%,  total acc: 79.83%   [EVAL] batch:  132 | acc: 81.25%,  total acc: 79.84%   [EVAL] batch:  133 | acc: 100.00%,  total acc: 79.99%   [EVAL] batch:  134 | acc: 93.75%,  total acc: 80.09%   [EVAL] batch:  135 | acc: 81.25%,  total acc: 80.10%   [EVAL] batch:  136 | acc: 62.50%,  total acc: 79.97%   [EVAL] batch:  137 | acc: 87.50%,  total acc: 80.03%   [EVAL] batch:  138 | acc: 87.50%,  total acc: 80.08%   [EVAL] batch:  139 | acc: 93.75%,  total acc: 80.18%   [EVAL] batch:  140 | acc: 93.75%,  total acc: 80.27%   [EVAL] batch:  141 | acc: 87.50%,  total acc: 80.33%   [EVAL] batch:  142 | acc: 93.75%,  total acc: 80.42%   [EVAL] batch:  143 | acc: 68.75%,  total acc: 80.34%   [EVAL] batch:  144 | acc: 62.50%,  total acc: 80.22%   [EVAL] batch:  145 | acc: 31.25%,  total acc: 79.88%   [EVAL] batch:  146 | acc: 75.00%,  total acc: 79.85%   [EVAL] batch:  147 | acc: 62.50%,  total acc: 79.73%   [EVAL] batch:  148 | acc: 50.00%,  total acc: 79.53%   [EVAL] batch:  149 | acc: 62.50%,  total acc: 79.42%   [EVAL] batch:  150 | acc: 6.25%,  total acc: 78.93%   [EVAL] batch:  151 | acc: 6.25%,  total acc: 78.45%   [EVAL] batch:  152 | acc: 0.00%,  total acc: 77.94%   [EVAL] batch:  153 | acc: 0.00%,  total acc: 77.44%   [EVAL] batch:  154 | acc: 12.50%,  total acc: 77.02%   [EVAL] batch:  155 | acc: 0.00%,  total acc: 76.52%   [EVAL] batch:  156 | acc: 56.25%,  total acc: 76.39%   [EVAL] batch:  157 | acc: 100.00%,  total acc: 76.54%   [EVAL] batch:  158 | acc: 93.75%,  total acc: 76.65%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 76.76%   [EVAL] batch:  160 | acc: 93.75%,  total acc: 76.86%   [EVAL] batch:  161 | acc: 81.25%,  total acc: 76.89%   [EVAL] batch:  162 | acc: 93.75%,  total acc: 76.99%   [EVAL] batch:  163 | acc: 81.25%,  total acc: 77.02%   [EVAL] batch:  164 | acc: 100.00%,  total acc: 77.16%   [EVAL] batch:  165 | acc: 93.75%,  total acc: 77.26%   [EVAL] batch:  166 | acc: 93.75%,  total acc: 77.36%   [EVAL] batch:  167 | acc: 100.00%,  total acc: 77.49%   [EVAL] batch:  168 | acc: 93.75%,  total acc: 77.59%   [EVAL] batch:  169 | acc: 37.50%,  total acc: 77.35%   [EVAL] batch:  170 | acc: 37.50%,  total acc: 77.12%   [EVAL] batch:  171 | acc: 12.50%,  total acc: 76.74%   [EVAL] batch:  172 | acc: 31.25%,  total acc: 76.48%   [EVAL] batch:  173 | acc: 43.75%,  total acc: 76.29%   [EVAL] batch:  174 | acc: 31.25%,  total acc: 76.04%   [EVAL] batch:  175 | acc: 12.50%,  total acc: 75.67%   [EVAL] batch:  176 | acc: 37.50%,  total acc: 75.46%   [EVAL] batch:  177 | acc: 37.50%,  total acc: 75.25%   [EVAL] batch:  178 | acc: 56.25%,  total acc: 75.14%   [EVAL] batch:  179 | acc: 43.75%,  total acc: 74.97%   [EVAL] batch:  180 | acc: 62.50%,  total acc: 74.90%   [EVAL] batch:  181 | acc: 18.75%,  total acc: 74.59%   [EVAL] batch:  182 | acc: 25.00%,  total acc: 74.32%   [EVAL] batch:  183 | acc: 43.75%,  total acc: 74.15%   [EVAL] batch:  184 | acc: 31.25%,  total acc: 73.92%   [EVAL] batch:  185 | acc: 37.50%,  total acc: 73.72%   [EVAL] batch:  186 | acc: 18.75%,  total acc: 73.43%   [EVAL] batch:  187 | acc: 18.75%,  total acc: 73.14%   [EVAL] batch:  188 | acc: 31.25%,  total acc: 72.92%   [EVAL] batch:  189 | acc: 37.50%,  total acc: 72.73%   [EVAL] batch:  190 | acc: 31.25%,  total acc: 72.51%   [EVAL] batch:  191 | acc: 37.50%,  total acc: 72.33%   [EVAL] batch:  192 | acc: 37.50%,  total acc: 72.15%   [EVAL] batch:  193 | acc: 37.50%,  total acc: 71.97%   [EVAL] batch:  194 | acc: 68.75%,  total acc: 71.96%   [EVAL] batch:  195 | acc: 68.75%,  total acc: 71.94%   [EVAL] batch:  196 | acc: 75.00%,  total acc: 71.95%   [EVAL] batch:  197 | acc: 68.75%,  total acc: 71.94%   [EVAL] batch:  198 | acc: 37.50%,  total acc: 71.77%   [EVAL] batch:  199 | acc: 100.00%,  total acc: 71.91%   [EVAL] batch:  200 | acc: 37.50%,  total acc: 71.74%   [EVAL] batch:  201 | acc: 43.75%,  total acc: 71.60%   [EVAL] batch:  202 | acc: 50.00%,  total acc: 71.49%   [EVAL] batch:  203 | acc: 43.75%,  total acc: 71.35%   [EVAL] batch:  204 | acc: 50.00%,  total acc: 71.25%   [EVAL] batch:  205 | acc: 75.00%,  total acc: 71.27%   [EVAL] batch:  206 | acc: 87.50%,  total acc: 71.35%   [EVAL] batch:  207 | acc: 100.00%,  total acc: 71.48%   [EVAL] batch:  208 | acc: 100.00%,  total acc: 71.62%   [EVAL] batch:  209 | acc: 100.00%,  total acc: 71.76%   [EVAL] batch:  210 | acc: 93.75%,  total acc: 71.86%   [EVAL] batch:  211 | acc: 100.00%,  total acc: 71.99%   [EVAL] batch:  212 | acc: 75.00%,  total acc: 72.01%   [EVAL] batch:  213 | acc: 68.75%,  total acc: 71.99%   [EVAL] batch:  214 | acc: 56.25%,  total acc: 71.92%   [EVAL] batch:  215 | acc: 68.75%,  total acc: 71.90%   [EVAL] batch:  216 | acc: 56.25%,  total acc: 71.83%   [EVAL] batch:  217 | acc: 75.00%,  total acc: 71.85%   [EVAL] batch:  218 | acc: 68.75%,  total acc: 71.83%   [EVAL] batch:  219 | acc: 68.75%,  total acc: 71.82%   [EVAL] batch:  220 | acc: 31.25%,  total acc: 71.63%   [EVAL] batch:  221 | acc: 31.25%,  total acc: 71.45%   [EVAL] batch:  222 | acc: 25.00%,  total acc: 71.24%   [EVAL] batch:  223 | acc: 12.50%,  total acc: 70.98%   [EVAL] batch:  224 | acc: 37.50%,  total acc: 70.83%   [EVAL] batch:  225 | acc: 62.50%,  total acc: 70.80%   [EVAL] batch:  226 | acc: 81.25%,  total acc: 70.84%   [EVAL] batch:  227 | acc: 75.00%,  total acc: 70.86%   [EVAL] batch:  228 | acc: 87.50%,  total acc: 70.93%   [EVAL] batch:  229 | acc: 68.75%,  total acc: 70.92%   [EVAL] batch:  230 | acc: 62.50%,  total acc: 70.89%   [EVAL] batch:  231 | acc: 87.50%,  total acc: 70.96%   [EVAL] batch:  232 | acc: 62.50%,  total acc: 70.92%   [EVAL] batch:  233 | acc: 62.50%,  total acc: 70.89%   [EVAL] batch:  234 | acc: 62.50%,  total acc: 70.85%   [EVAL] batch:  235 | acc: 50.00%,  total acc: 70.76%   [EVAL] batch:  236 | acc: 81.25%,  total acc: 70.81%   [EVAL] batch:  237 | acc: 87.50%,  total acc: 70.88%   [EVAL] batch:  238 | acc: 100.00%,  total acc: 71.00%   [EVAL] batch:  239 | acc: 100.00%,  total acc: 71.12%   [EVAL] batch:  240 | acc: 100.00%,  total acc: 71.24%   [EVAL] batch:  241 | acc: 100.00%,  total acc: 71.36%   [EVAL] batch:  242 | acc: 100.00%,  total acc: 71.48%   [EVAL] batch:  243 | acc: 100.00%,  total acc: 71.59%   [EVAL] batch:  244 | acc: 100.00%,  total acc: 71.71%   [EVAL] batch:  245 | acc: 100.00%,  total acc: 71.82%   [EVAL] batch:  246 | acc: 100.00%,  total acc: 71.94%   [EVAL] batch:  247 | acc: 100.00%,  total acc: 72.05%   [EVAL] batch:  248 | acc: 100.00%,  total acc: 72.16%   [EVAL] batch:  249 | acc: 100.00%,  total acc: 72.28%   [EVAL] batch:  250 | acc: 56.25%,  total acc: 72.21%   [EVAL] batch:  251 | acc: 43.75%,  total acc: 72.10%   [EVAL] batch:  252 | acc: 56.25%,  total acc: 72.04%   [EVAL] batch:  253 | acc: 37.50%,  total acc: 71.90%   [EVAL] batch:  254 | acc: 68.75%,  total acc: 71.89%   [EVAL] batch:  255 | acc: 81.25%,  total acc: 71.92%   [EVAL] batch:  256 | acc: 87.50%,  total acc: 71.98%   [EVAL] batch:  257 | acc: 68.75%,  total acc: 71.97%   [EVAL] batch:  258 | acc: 62.50%,  total acc: 71.94%   [EVAL] batch:  259 | acc: 68.75%,  total acc: 71.92%   [EVAL] batch:  260 | acc: 56.25%,  total acc: 71.86%   [EVAL] batch:  261 | acc: 62.50%,  total acc: 71.83%   [EVAL] batch:  262 | acc: 100.00%,  total acc: 71.93%   [EVAL] batch:  263 | acc: 68.75%,  total acc: 71.92%   [EVAL] batch:  264 | acc: 75.00%,  total acc: 71.93%   [EVAL] batch:  265 | acc: 75.00%,  total acc: 71.95%   [EVAL] batch:  266 | acc: 81.25%,  total acc: 71.98%   [EVAL] batch:  267 | acc: 81.25%,  total acc: 72.01%   [EVAL] batch:  268 | acc: 93.75%,  total acc: 72.10%   [EVAL] batch:  269 | acc: 87.50%,  total acc: 72.15%   [EVAL] batch:  270 | acc: 68.75%,  total acc: 72.14%   [EVAL] batch:  271 | acc: 75.00%,  total acc: 72.15%   [EVAL] batch:  272 | acc: 75.00%,  total acc: 72.16%   [EVAL] batch:  273 | acc: 93.75%,  total acc: 72.24%   [EVAL] batch:  274 | acc: 87.50%,  total acc: 72.30%   [EVAL] batch:  275 | acc: 100.00%,  total acc: 72.40%   [EVAL] batch:  276 | acc: 100.00%,  total acc: 72.50%   [EVAL] batch:  277 | acc: 100.00%,  total acc: 72.59%   [EVAL] batch:  278 | acc: 93.75%,  total acc: 72.67%   [EVAL] batch:  279 | acc: 100.00%,  total acc: 72.77%   [EVAL] batch:  280 | acc: 100.00%,  total acc: 72.86%   [EVAL] batch:  281 | acc: 100.00%,  total acc: 72.96%   [EVAL] batch:  282 | acc: 100.00%,  total acc: 73.06%   [EVAL] batch:  283 | acc: 100.00%,  total acc: 73.15%   [EVAL] batch:  284 | acc: 100.00%,  total acc: 73.25%   [EVAL] batch:  285 | acc: 100.00%,  total acc: 73.34%   [EVAL] batch:  286 | acc: 100.00%,  total acc: 73.43%   [EVAL] batch:  287 | acc: 87.50%,  total acc: 73.48%   [EVAL] batch:  288 | acc: 56.25%,  total acc: 73.42%   [EVAL] batch:  289 | acc: 62.50%,  total acc: 73.38%   [EVAL] batch:  290 | acc: 56.25%,  total acc: 73.32%   [EVAL] batch:  291 | acc: 75.00%,  total acc: 73.33%   [EVAL] batch:  292 | acc: 81.25%,  total acc: 73.36%   [EVAL] batch:  293 | acc: 62.50%,  total acc: 73.32%   [EVAL] batch:  294 | acc: 50.00%,  total acc: 73.24%   [EVAL] batch:  295 | acc: 31.25%,  total acc: 73.10%   [EVAL] batch:  296 | acc: 43.75%,  total acc: 73.00%   [EVAL] batch:  297 | acc: 56.25%,  total acc: 72.94%   [EVAL] batch:  298 | acc: 56.25%,  total acc: 72.89%   [EVAL] batch:  299 | acc: 50.00%,  total acc: 72.81%   [EVAL] batch:  300 | acc: 93.75%,  total acc: 72.88%   [EVAL] batch:  301 | acc: 93.75%,  total acc: 72.95%   [EVAL] batch:  302 | acc: 93.75%,  total acc: 73.02%   [EVAL] batch:  303 | acc: 81.25%,  total acc: 73.05%   [EVAL] batch:  304 | acc: 93.75%,  total acc: 73.11%   [EVAL] batch:  305 | acc: 87.50%,  total acc: 73.16%   [EVAL] batch:  306 | acc: 87.50%,  total acc: 73.21%   [EVAL] batch:  307 | acc: 75.00%,  total acc: 73.21%   [EVAL] batch:  308 | acc: 93.75%,  total acc: 73.28%   [EVAL] batch:  309 | acc: 68.75%,  total acc: 73.27%   [EVAL] batch:  310 | acc: 81.25%,  total acc: 73.29%   [EVAL] batch:  311 | acc: 81.25%,  total acc: 73.32%   [EVAL] batch:  312 | acc: 50.00%,  total acc: 73.24%   
cur_acc:  ['0.9494', '0.8244', '0.5962', '0.7173', '0.7708']
his_acc:  ['0.9494', '0.8760', '0.7746', '0.7435', '0.7324']
Clustering into  29  clusters
Clusters:  [ 4 28  8 22 14 24  8  2 10  9 22 19  4 10  1 26 12 23  7 12 14 17  0 27
 21 11  0 10  0 10 21 20  3  1  2 15 10 26 16  5 12  0  2 17  7 25  3 13
  1 13 15 18 13  6  6 18 13  5  9  0]
Losses:  12.122180938720703 4.738576889038086 0.5980374813079834
CurrentTrain: epoch  0, batch     0 | loss: 12.1221809Losses:  9.387003898620605 3.1041455268859863 0.5274344682693481
CurrentTrain: epoch  0, batch     1 | loss: 9.3870039Losses:  11.833389282226562 4.527011394500732 0.6356828212738037
CurrentTrain: epoch  0, batch     2 | loss: 11.8333893Losses:  3.458794355392456 -0.0 -0.0
CurrentTrain: epoch  0, batch     3 | loss: 3.4587944Losses:  9.146369934082031 3.097334861755371 0.6064891815185547
CurrentTrain: epoch  1, batch     0 | loss: 9.1463699Losses:  9.509967803955078 3.664823055267334 0.5742189884185791
CurrentTrain: epoch  1, batch     1 | loss: 9.5099678Losses:  9.413111686706543 2.9639008045196533 0.5871641039848328
CurrentTrain: epoch  1, batch     2 | loss: 9.4131117Losses:  4.622960567474365 -0.0 0.15395572781562805
CurrentTrain: epoch  1, batch     3 | loss: 4.6229606Losses:  9.728411674499512 3.8132197856903076 0.5828678607940674
CurrentTrain: epoch  2, batch     0 | loss: 9.7284117Losses:  7.589325428009033 2.5943093299865723 0.5020383596420288
CurrentTrain: epoch  2, batch     1 | loss: 7.5893254Losses:  6.9925127029418945 1.9517827033996582 0.5783689022064209
CurrentTrain: epoch  2, batch     2 | loss: 6.9925127Losses:  7.535845756530762 -0.0 0.1097555086016655
CurrentTrain: epoch  2, batch     3 | loss: 7.5358458Losses:  7.909762382507324 3.0167479515075684 0.5918974280357361
CurrentTrain: epoch  3, batch     0 | loss: 7.9097624Losses:  7.325884819030762 2.7014412879943848 0.5168130397796631
CurrentTrain: epoch  3, batch     1 | loss: 7.3258848Losses:  7.721996307373047 2.929145336151123 0.5230157375335693
CurrentTrain: epoch  3, batch     2 | loss: 7.7219963Losses:  3.324985980987549 -0.0 0.10270583629608154
CurrentTrain: epoch  3, batch     3 | loss: 3.3249860Losses:  7.360238552093506 3.3305344581604004 0.6107036471366882
CurrentTrain: epoch  4, batch     0 | loss: 7.3602386Losses:  7.5436835289001465 2.718294143676758 0.5206590294837952
CurrentTrain: epoch  4, batch     1 | loss: 7.5436835Losses:  6.759340763092041 3.098629951477051 0.4921059012413025
CurrentTrain: epoch  4, batch     2 | loss: 6.7593408Losses:  6.639287948608398 -0.0 -0.0
CurrentTrain: epoch  4, batch     3 | loss: 6.6392879Losses:  6.627020359039307 2.8480148315429688 0.5816985964775085
CurrentTrain: epoch  5, batch     0 | loss: 6.6270204Losses:  6.593236446380615 2.7355074882507324 0.5068784952163696
CurrentTrain: epoch  5, batch     1 | loss: 6.5932364Losses:  5.794938564300537 2.6629796028137207 0.48386043310165405
CurrentTrain: epoch  5, batch     2 | loss: 5.7949386Losses:  2.867453098297119 -0.0 0.10297274589538574
CurrentTrain: epoch  5, batch     3 | loss: 2.8674531Losses:  7.1290283203125 3.4617807865142822 0.5750598907470703
CurrentTrain: epoch  6, batch     0 | loss: 7.1290283Losses:  7.2129621505737305 3.7720041275024414 0.4999523162841797
CurrentTrain: epoch  6, batch     1 | loss: 7.2129622Losses:  6.197086811065674 3.314314126968384 0.492243230342865
CurrentTrain: epoch  6, batch     2 | loss: 6.1970868Losses:  2.108597993850708 -0.0 0.09751114249229431
CurrentTrain: epoch  6, batch     3 | loss: 2.1085980Losses:  6.340246200561523 3.2711422443389893 0.47905898094177246
CurrentTrain: epoch  7, batch     0 | loss: 6.3402462Losses:  6.404487133026123 3.295238494873047 0.5582712888717651
CurrentTrain: epoch  7, batch     1 | loss: 6.4044871Losses:  5.315027713775635 2.3302083015441895 0.5649656057357788
CurrentTrain: epoch  7, batch     2 | loss: 5.3150277Losses:  2.2068288326263428 -0.0 0.1365860253572464
CurrentTrain: epoch  7, batch     3 | loss: 2.2068288Losses:  5.693231582641602 2.8768725395202637 0.4829399585723877
CurrentTrain: epoch  8, batch     0 | loss: 5.6932316Losses:  5.266984939575195 2.320986270904541 0.5542024374008179
CurrentTrain: epoch  8, batch     1 | loss: 5.2669849Losses:  5.697718620300293 3.2424890995025635 0.4037148952484131
CurrentTrain: epoch  8, batch     2 | loss: 5.6977186Losses:  2.599050283432007 -0.0 0.10106486082077026
CurrentTrain: epoch  8, batch     3 | loss: 2.5990503Losses:  5.807182312011719 3.2372946739196777 0.37452125549316406
CurrentTrain: epoch  9, batch     0 | loss: 5.8071823Losses:  5.313030242919922 2.4715356826782227 0.5518676042556763
CurrentTrain: epoch  9, batch     1 | loss: 5.3130302Losses:  6.855149745941162 4.229706764221191 0.5487345457077026
CurrentTrain: epoch  9, batch     2 | loss: 6.8551497Losses:  1.872233510017395 -0.0 0.14872966706752777
CurrentTrain: epoch  9, batch     3 | loss: 1.8722335
#### follows ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "follows" means immediately prior item in a series of which the subject is a part.
Here is an example:
Relation: follows
Context: peugeot took a similar step in 2010 when replacing the 407 and long - running but unpopular 607 with a single model , the 508 .
Head Entity: 508
Tail Entity: 407
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "follows":
Relation: follows  
Context: In the 2021 season, the team introduced the new model, the X5, which follows the successful launch of the X3 in 2020.  
Head Entity: X5  
Tail Entity: X3  

Relation: follows  
Context: The latest smartphone, the Galaxy S21, follows the previous model, the Galaxy S20, which was released just a year earlier.  
Head Entity: Galaxy S21  
Tail Entity: Galaxy S20  
#### sport ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "sport" means sport that the subject participates or participated in or is associated with.
Here is an example:
Relation: sport
Context: it is primarily used for baseball and serves as the home stadium for yaquis de obregón .
Head Entity: yaquis de obregón
Tail Entity: baseball
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "sport":
Relation: sport  
Context: She has been a professional player in the world of tennis, winning several championships throughout her career.  
Head Entity: She  
Tail Entity: tennis  

Relation: sport  
Context: The city is known for its vibrant soccer culture, with numerous teams competing in the national league.  
Head Entity: The city  
Tail Entity: soccer  
#### father ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "father" means male parent of the subject.
Here is an example:
Relation: father
Context: after the crowning of his nephew radoslav , the son of stefan , sava left the serbian maritime in 1229 for a trip to palestine .
Head Entity: radoslav
Tail Entity: stefan
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "father":
Relation: father  
Context: in the historical records, it is noted that king henry was the father of prince edward, who later became a significant figure in the kingdom.  
Head Entity: prince edward  
Tail Entity: king henry  

Relation: father  
Context: during the family reunion, it was revealed that michael was the father of two daughters, emily and sarah, who both excelled in their studies.  
Head Entity: michael  
Tail Entity: emily  
#### original network ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "original network" means network or service that originally broadcast a radio or television program.
Here is an example:
Relation: original network
Context: he also made a special appearance in kbs ' romantic comedy drama " fight for my way " .
Head Entity: fight for my way
Tail Entity: kbs
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "original network":
Relation: original network  
Context: the popular series "breaking bad" was first aired on amc, gaining a massive following.  
Head Entity: breaking bad  
Tail Entity: amc  

Relation: original network  
Context: the hit show "stranger things" premiered on netflix and quickly became a cultural phenomenon.  
Head Entity: stranger things  
Tail Entity: netflix  
#### original language of film or TV show ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "original language of film or TV show" means language in which a film or a performance work was originally created.
Here is an example:
Relation: original language of film or TV show
Context: that same year saran made a special appearance in a scene in the kannada film " arasu " .
Head Entity: arasu
Tail Entity: kannada
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "original language of film or TV show":
Relation: original language of film or TV show  
Context: The critically acclaimed movie "Parasite" was primarily filmed in Korean and received numerous awards for its storytelling.  
Head Entity: Parasite  
Tail Entity: Korean  

Relation: original language of film or TV show  
Context: The animated series "Doraemon" is originally produced in Japanese and has been dubbed into several other languages for international audiences.  
Head Entity: Doraemon  
Tail Entity: Japanese  
#### league ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "league" means league in which team or player plays or has played in.
Here is an example:
Relation: league
Context: the league champions for the fifth time in their history ( and the second season running ) were taunton town , but runners - up mangotsfield united took promotion to the southern league .
Head Entity: mangotsfield united
Tail Entity: southern league
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "league":
Relation: league  
Context: After an impressive season, the team secured a spot in the prestigious league, where they will compete against top clubs, including their rivals, the city wanderers.  
Head Entity: city wanderers  
Tail Entity: prestigious league  

Relation: league  
Context: The young athlete showcased his skills in the national league, where he played alongside some of the best players in the country, including his teammate, the star striker.  
Head Entity: star striker  
Tail Entity: national league  
#### mother ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "mother" means female parent of the subject.
Here is an example:
Relation: mother
Context: the first stewart king of scotland and son of marjorie bruce and walter stewart , robert ii , is believed to have been born in the abbey .
Head Entity: robert ii
Tail Entity: marjorie bruce
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "mother":
Relation: mother  
Context: in the historical records, it is noted that queen elizabeth i was the daughter of anne boleyn, who served as her mother and played a crucial role in her early life.  
Head Entity: queen elizabeth i  
Tail Entity: anne boleyn  

Relation: mother  
Context: the famous artist pablo picasso often spoke fondly of his mother, maría ruiz, who greatly influenced his artistic journey from a young age.  
Head Entity: pablo picasso  
Tail Entity: maría ruiz  
#### instrument ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "instrument" means musical instrument that a person plays or teaches or used in a music occupation.
Here is an example:
Relation: instrument
Context: peter bastian ( 25 august 1943 in copenhagen – 27 march 2017 ) was a danish musician . peter bastian bio and cv he was educated in theoretical physics and classical bassoon .
Head Entity: peter bastian
Tail Entity: bassoon
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "instrument":
Relation: instrument  
Context: johann sebastian bach (31 march 1685 – 28 july 1750) was a german composer and musician of the baroque period. he was known for his exceptional skill in playing the organ and the harpsichord.  
Head Entity: johann sebastian bach  
Tail Entity: organ  

Relation: instrument  
Context: yo-yo ma, born on october 7, 1955, is a world-renowned cellist who has performed with numerous orchestras and in various genres. he is particularly famous for his interpretations of classical cello concertos.  
Head Entity: yo-yo ma  
Tail Entity: cello  
#### country ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "country" means sovereign state of this item (not to be used for human beings).
Here is an example:
Relation: country
Context: ploegsteert is a village in belgium located in the municipality of comines - warneton in the hainaut province and is the most westerly settlement of the walloon region .
Head Entity: hainaut
Tail Entity: belgium
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "country":
Relation: country  
Context: the city of prague is the capital of the czech republic and is known for its beautiful architecture and rich history.  
Head Entity: prague  
Tail Entity: czech republic  

Relation: country  
Context: the great barrier reef is located off the coast of australia and is the largest coral reef system in the world.  
Head Entity: great barrier reef  
Tail Entity: australia  
#### characters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "characters" means characters which appear in this item (like plays, operas, operettas, books, comics, films, TV series, video games).
Here is an example:
Relation: characters
Context: klaus kinski , who would play dracula himself nine years later in " nosferatu the vampyre " , is also featured in the film as renfield .
Head Entity: nosferatu the vampyre
Tail Entity: renfield
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "characters":
Relation: characters  
Context: in the animated series "avatar: the last airbender," zuko is a complex character who struggles with his identity and family legacy.  
Head Entity: avatar: the last airbender  
Tail Entity: zuko  

Relation: characters  
Context: the novel "pride and prejudice" features elizabeth bennet, a strong-willed protagonist who navigates societal expectations and personal relationships.  
Head Entity: pride and prejudice  
Tail Entity: elizabeth bennet  
Losses:  4.164752960205078 0.8742345571517944 0.8606313467025757
MemoryTrain:  epoch  0, batch     0 | loss: 4.1647530Losses:  4.711524963378906 0.5892875790596008 0.877784252166748
MemoryTrain:  epoch  0, batch     1 | loss: 4.7115250Losses:  3.7144856452941895 -0.0 0.9920273423194885
MemoryTrain:  epoch  0, batch     2 | loss: 3.7144856Losses:  4.290286064147949 0.31131473183631897 1.0832574367523193
MemoryTrain:  epoch  0, batch     3 | loss: 4.2902861Losses:  4.349465847015381 0.21880578994750977 0.914631724357605
MemoryTrain:  epoch  0, batch     4 | loss: 4.3494658Losses:  3.84066104888916 0.7763881087303162 0.7736445069313049
MemoryTrain:  epoch  0, batch     5 | loss: 3.8406610Losses:  3.7435669898986816 0.5231760144233704 0.7970439195632935
MemoryTrain:  epoch  0, batch     6 | loss: 3.7435670Losses:  3.8031156063079834 0.6199477910995483 0.9119941592216492
MemoryTrain:  epoch  0, batch     7 | loss: 3.8031156Losses:  5.405810832977295 -0.0 0.9894846677780151
MemoryTrain:  epoch  0, batch     8 | loss: 5.4058108Losses:  4.764345645904541 0.5558310747146606 0.9261603355407715
MemoryTrain:  epoch  0, batch     9 | loss: 4.7643456Losses:  4.541450023651123 0.2635806202888489 0.9960693120956421
MemoryTrain:  epoch  0, batch    10 | loss: 4.5414500Losses:  4.596351146697998 -0.0 0.31342729926109314
MemoryTrain:  epoch  0, batch    11 | loss: 4.5963511Losses:  3.746560573577881 0.5212754607200623 0.9195586442947388
MemoryTrain:  epoch  1, batch     0 | loss: 3.7465606Losses:  3.9732916355133057 0.2669324576854706 1.0185282230377197
MemoryTrain:  epoch  1, batch     1 | loss: 3.9732916Losses:  4.206960201263428 -0.0 1.024182915687561
MemoryTrain:  epoch  1, batch     2 | loss: 4.2069602Losses:  3.728778123855591 0.2691751718521118 0.9875423908233643
MemoryTrain:  epoch  1, batch     3 | loss: 3.7287781Losses:  3.1719534397125244 -0.0 1.0244948863983154
MemoryTrain:  epoch  1, batch     4 | loss: 3.1719534Losses:  4.2216644287109375 -0.0 0.9758076667785645
MemoryTrain:  epoch  1, batch     5 | loss: 4.2216644Losses:  3.4996800422668457 -0.0 0.8965494632720947
MemoryTrain:  epoch  1, batch     6 | loss: 3.4996800Losses:  3.874565601348877 0.2596636414527893 1.0309213399887085
MemoryTrain:  epoch  1, batch     7 | loss: 3.8745656Losses:  3.5741894245147705 -0.0 0.9554741382598877
MemoryTrain:  epoch  1, batch     8 | loss: 3.5741894Losses:  3.528815269470215 0.4674181342124939 0.9043928384780884
MemoryTrain:  epoch  1, batch     9 | loss: 3.5288153Losses:  3.1673290729522705 0.2451300024986267 0.9557859301567078
MemoryTrain:  epoch  1, batch    10 | loss: 3.1673291Losses:  3.903411388397217 -0.0 0.3628591299057007
MemoryTrain:  epoch  1, batch    11 | loss: 3.9034114Losses:  3.2798304557800293 0.2662719488143921 1.0690616369247437
MemoryTrain:  epoch  2, batch     0 | loss: 3.2798305Losses:  3.1216089725494385 0.4893808364868164 0.9059523940086365
MemoryTrain:  epoch  2, batch     1 | loss: 3.1216090Losses:  2.8203012943267822 0.22123271226882935 0.9031712412834167
MemoryTrain:  epoch  2, batch     2 | loss: 2.8203013Losses:  4.556563854217529 0.6287117600440979 0.9654383063316345
MemoryTrain:  epoch  2, batch     3 | loss: 4.5565639Losses:  2.371776580810547 -0.0 0.9766430258750916
MemoryTrain:  epoch  2, batch     4 | loss: 2.3717766Losses:  4.347860813140869 0.48108863830566406 0.8899690508842468
MemoryTrain:  epoch  2, batch     5 | loss: 4.3478608Losses:  3.298391819000244 -0.0 0.9417723417282104
MemoryTrain:  epoch  2, batch     6 | loss: 3.2983918Losses:  3.480211019515991 0.2344752997159958 0.8500344157218933
MemoryTrain:  epoch  2, batch     7 | loss: 3.4802110Losses:  2.9140944480895996 0.2991276979446411 0.9885406494140625
MemoryTrain:  epoch  2, batch     8 | loss: 2.9140944Losses:  3.0383591651916504 -0.0 1.0106772184371948
MemoryTrain:  epoch  2, batch     9 | loss: 3.0383592Losses:  2.9797468185424805 0.49636760354042053 0.9083842039108276
MemoryTrain:  epoch  2, batch    10 | loss: 2.9797468Losses:  2.754781484603882 -0.0 0.3343536853790283
MemoryTrain:  epoch  2, batch    11 | loss: 2.7547815Losses:  2.4801278114318848 -0.0 0.9878125190734863
MemoryTrain:  epoch  3, batch     0 | loss: 2.4801278Losses:  2.992398500442505 -0.0 1.0230908393859863
MemoryTrain:  epoch  3, batch     1 | loss: 2.9923985Losses:  2.6060574054718018 -0.0 0.9585756659507751
MemoryTrain:  epoch  3, batch     2 | loss: 2.6060574Losses:  3.477356195449829 -0.0 0.9904091358184814
MemoryTrain:  epoch  3, batch     3 | loss: 3.4773562Losses:  2.9252448081970215 0.21414721012115479 0.9551955461502075
MemoryTrain:  epoch  3, batch     4 | loss: 2.9252448Losses:  2.96621036529541 -0.0 1.0382994413375854
MemoryTrain:  epoch  3, batch     5 | loss: 2.9662104Losses:  2.8175597190856934 0.2765869200229645 0.9930815696716309
MemoryTrain:  epoch  3, batch     6 | loss: 2.8175597Losses:  2.6730072498321533 0.23829235136508942 0.7186253666877747
MemoryTrain:  epoch  3, batch     7 | loss: 2.6730072Losses:  3.0329720973968506 0.5267957448959351 0.7432244420051575
MemoryTrain:  epoch  3, batch     8 | loss: 3.0329721Losses:  2.753896951675415 -0.0 0.8468780517578125
MemoryTrain:  epoch  3, batch     9 | loss: 2.7538970Losses:  2.9771902561187744 0.5104297399520874 0.9357110857963562
MemoryTrain:  epoch  3, batch    10 | loss: 2.9771903Losses:  1.8292803764343262 -0.0 0.3136167526245117
MemoryTrain:  epoch  3, batch    11 | loss: 1.8292804Losses:  3.2420566082000732 0.2888216972351074 0.9012279510498047
MemoryTrain:  epoch  4, batch     0 | loss: 3.2420566Losses:  2.812227964401245 0.23841744661331177 0.8320690989494324
MemoryTrain:  epoch  4, batch     1 | loss: 2.8122280Losses:  3.002682685852051 0.4663088619709015 0.9866638779640198
MemoryTrain:  epoch  4, batch     2 | loss: 3.0026827Losses:  2.9555511474609375 0.2515503168106079 1.0670273303985596
MemoryTrain:  epoch  4, batch     3 | loss: 2.9555511Losses:  2.743242025375366 0.5505046844482422 0.8572294116020203
MemoryTrain:  epoch  4, batch     4 | loss: 2.7432420Losses:  2.908885955810547 0.23440566658973694 0.9184777736663818
MemoryTrain:  epoch  4, batch     5 | loss: 2.9088860Losses:  2.9625632762908936 0.7551789283752441 0.8655116558074951
MemoryTrain:  epoch  4, batch     6 | loss: 2.9625633Losses:  2.9897189140319824 0.5642228126525879 0.9651952981948853
MemoryTrain:  epoch  4, batch     7 | loss: 2.9897189Losses:  2.883755922317505 0.2575419545173645 0.8515030741691589
MemoryTrain:  epoch  4, batch     8 | loss: 2.8837559Losses:  3.562612533569336 0.5151228904724121 0.8914451599121094
MemoryTrain:  epoch  4, batch     9 | loss: 3.5626125Losses:  2.914097785949707 -0.0 1.0104900598526
MemoryTrain:  epoch  4, batch    10 | loss: 2.9140978Losses:  1.8599966764450073 -0.0 0.36184847354888916
MemoryTrain:  epoch  4, batch    11 | loss: 1.8599967Losses:  2.66680908203125 0.46245166659355164 0.9068280458450317
MemoryTrain:  epoch  5, batch     0 | loss: 2.6668091Losses:  3.5915117263793945 0.7860474586486816 0.8161665201187134
MemoryTrain:  epoch  5, batch     1 | loss: 3.5915117Losses:  3.140507221221924 0.5591835379600525 0.8679121732711792
MemoryTrain:  epoch  5, batch     2 | loss: 3.1405072Losses:  3.4585795402526855 0.905686616897583 0.9200979471206665
MemoryTrain:  epoch  5, batch     3 | loss: 3.4585795Losses:  2.5984716415405273 0.5011909008026123 0.8001877069473267
MemoryTrain:  epoch  5, batch     4 | loss: 2.5984716Losses:  2.7644972801208496 0.5185779333114624 0.8510932922363281
MemoryTrain:  epoch  5, batch     5 | loss: 2.7644973Losses:  2.9686996936798096 0.26563340425491333 0.9301717281341553
MemoryTrain:  epoch  5, batch     6 | loss: 2.9686997Losses:  2.856804132461548 0.545780599117279 0.8601979613304138
MemoryTrain:  epoch  5, batch     7 | loss: 2.8568041Losses:  2.6930603981018066 0.2509419023990631 0.9111639261245728
MemoryTrain:  epoch  5, batch     8 | loss: 2.6930604Losses:  2.4294517040252686 -0.0 0.8500058650970459
MemoryTrain:  epoch  5, batch     9 | loss: 2.4294517Losses:  3.186960220336914 -0.0 1.011719822883606
MemoryTrain:  epoch  5, batch    10 | loss: 3.1869602Losses:  1.7363512516021729 -0.0 0.3530270457267761
MemoryTrain:  epoch  5, batch    11 | loss: 1.7363513Losses:  2.4325852394104004 -0.0 1.0370721817016602
MemoryTrain:  epoch  6, batch     0 | loss: 2.4325852Losses:  2.885450839996338 -0.0 1.0184515714645386
MemoryTrain:  epoch  6, batch     1 | loss: 2.8854508Losses:  3.040757656097412 0.26663458347320557 0.9559831619262695
MemoryTrain:  epoch  6, batch     2 | loss: 3.0407577Losses:  2.3437843322753906 -0.0 0.9719257950782776
MemoryTrain:  epoch  6, batch     3 | loss: 2.3437843Losses:  2.570643663406372 0.24501951038837433 0.920170783996582
MemoryTrain:  epoch  6, batch     4 | loss: 2.5706437Losses:  2.433157444000244 -0.0 0.8484889268875122
MemoryTrain:  epoch  6, batch     5 | loss: 2.4331574Losses:  2.559055805206299 0.23835304379463196 0.9739822149276733
MemoryTrain:  epoch  6, batch     6 | loss: 2.5590558Losses:  3.116655111312866 0.2508854866027832 0.836806058883667
MemoryTrain:  epoch  6, batch     7 | loss: 3.1166551Losses:  2.6385111808776855 0.2688549757003784 1.0124434232711792
MemoryTrain:  epoch  6, batch     8 | loss: 2.6385112Losses:  2.754218578338623 0.246262326836586 1.0095133781433105
MemoryTrain:  epoch  6, batch     9 | loss: 2.7542186Losses:  2.803311824798584 0.5087498426437378 0.9520997405052185
MemoryTrain:  epoch  6, batch    10 | loss: 2.8033118Losses:  1.6372790336608887 -0.0 0.33507680892944336
MemoryTrain:  epoch  6, batch    11 | loss: 1.6372790Losses:  2.9192309379577637 0.49010169506073 0.9611475467681885
MemoryTrain:  epoch  7, batch     0 | loss: 2.9192309Losses:  2.6063756942749023 0.24527597427368164 0.8309375047683716
MemoryTrain:  epoch  7, batch     1 | loss: 2.6063757Losses:  2.404069423675537 -0.0 0.9838968515396118
MemoryTrain:  epoch  7, batch     2 | loss: 2.4040694Losses:  3.585294723510742 0.66404128074646 0.9764081835746765
MemoryTrain:  epoch  7, batch     3 | loss: 3.5852947Losses:  2.573915958404541 0.2534283399581909 0.9632847309112549
MemoryTrain:  epoch  7, batch     4 | loss: 2.5739160Losses:  2.3523740768432617 -0.0 0.9730305671691895
MemoryTrain:  epoch  7, batch     5 | loss: 2.3523741Losses:  2.308572292327881 -0.0 0.9684504270553589
MemoryTrain:  epoch  7, batch     6 | loss: 2.3085723Losses:  3.026445150375366 0.7204396724700928 0.8512112498283386
MemoryTrain:  epoch  7, batch     7 | loss: 3.0264452Losses:  2.750431776046753 0.5180472731590271 0.8506940007209778
MemoryTrain:  epoch  7, batch     8 | loss: 2.7504318Losses:  3.047792673110962 0.5281585454940796 0.9057938456535339
MemoryTrain:  epoch  7, batch     9 | loss: 3.0477927Losses:  2.5846309661865234 -0.0 1.0443024635314941
MemoryTrain:  epoch  7, batch    10 | loss: 2.5846310Losses:  1.531190276145935 -0.0 0.3294795751571655
MemoryTrain:  epoch  7, batch    11 | loss: 1.5311903Losses:  3.733386993408203 1.1125504970550537 0.8961974382400513
MemoryTrain:  epoch  8, batch     0 | loss: 3.7333870Losses:  2.944861888885498 0.7673950791358948 0.8215652704238892
MemoryTrain:  epoch  8, batch     1 | loss: 2.9448619Losses:  2.6100921630859375 0.23934179544448853 0.7907009720802307
MemoryTrain:  epoch  8, batch     2 | loss: 2.6100922Losses:  3.3098461627960205 1.1267013549804688 0.7564104199409485
MemoryTrain:  epoch  8, batch     3 | loss: 3.3098462Losses:  2.7928378582000732 0.543628454208374 0.8490118980407715
MemoryTrain:  epoch  8, batch     4 | loss: 2.7928379Losses:  2.758044719696045 0.5115466117858887 0.9167512655258179
MemoryTrain:  epoch  8, batch     5 | loss: 2.7580447Losses:  2.3499715328216553 -0.0 1.0841563940048218
MemoryTrain:  epoch  8, batch     6 | loss: 2.3499715Losses:  2.544290781021118 0.2642781138420105 0.9516797065734863
MemoryTrain:  epoch  8, batch     7 | loss: 2.5442908Losses:  2.824399948120117 0.5248328447341919 0.9207355976104736
MemoryTrain:  epoch  8, batch     8 | loss: 2.8243999Losses:  3.2470319271087646 0.8399473428726196 0.9599254131317139
MemoryTrain:  epoch  8, batch     9 | loss: 3.2470319Losses:  2.8144164085388184 0.5229523181915283 0.8837795257568359
MemoryTrain:  epoch  8, batch    10 | loss: 2.8144164Losses:  1.6374857425689697 -0.0 0.34372198581695557
MemoryTrain:  epoch  8, batch    11 | loss: 1.6374857Losses:  2.418151378631592 0.2391355037689209 0.9616876840591431
MemoryTrain:  epoch  9, batch     0 | loss: 2.4181514Losses:  2.400128126144409 0.23478583991527557 0.8390567302703857
MemoryTrain:  epoch  9, batch     1 | loss: 2.4001281Losses:  2.6742019653320312 0.2906285524368286 0.9540494680404663
MemoryTrain:  epoch  9, batch     2 | loss: 2.6742020Losses:  2.6956119537353516 0.47823891043663025 0.8898061513900757
MemoryTrain:  epoch  9, batch     3 | loss: 2.6956120Losses:  2.693098306655884 0.47041574120521545 0.9245823621749878
MemoryTrain:  epoch  9, batch     4 | loss: 2.6930983Losses:  2.438697338104248 0.43595772981643677 0.7720544934272766
MemoryTrain:  epoch  9, batch     5 | loss: 2.4386973Losses:  3.1627302169799805 0.6634300947189331 0.7821741104125977
MemoryTrain:  epoch  9, batch     6 | loss: 3.1627302Losses:  2.6545207500457764 0.54157555103302 0.8636773228645325
MemoryTrain:  epoch  9, batch     7 | loss: 2.6545208Losses:  2.5984747409820557 0.28699541091918945 0.9747411608695984
MemoryTrain:  epoch  9, batch     8 | loss: 2.5984747Losses:  2.693258762359619 0.2687249183654785 0.9784009456634521
MemoryTrain:  epoch  9, batch     9 | loss: 2.6932588Losses:  2.8192243576049805 0.4773232042789459 0.9224414825439453
MemoryTrain:  epoch  9, batch    10 | loss: 2.8192244Losses:  1.5558868646621704 -0.0 0.31329023838043213
MemoryTrain:  epoch  9, batch    11 | loss: 1.5558869
[EVAL] batch:    0 | acc: 6.25%,  total acc: 6.25%   [EVAL] batch:    1 | acc: 18.75%,  total acc: 12.50%   [EVAL] batch:    2 | acc: 0.00%,  total acc: 8.33%   [EVAL] batch:    3 | acc: 12.50%,  total acc: 9.38%   [EVAL] batch:    4 | acc: 12.50%,  total acc: 10.00%   [EVAL] batch:    5 | acc: 12.50%,  total acc: 10.42%   [EVAL] batch:    6 | acc: 50.00%,  total acc: 16.07%   [EVAL] batch:    7 | acc: 68.75%,  total acc: 22.66%   [EVAL] batch:    8 | acc: 75.00%,  total acc: 28.47%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 33.75%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 38.64%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 42.19%   [EVAL] batch:   12 | acc: 62.50%,  total acc: 43.75%   [EVAL] batch:   13 | acc: 18.75%,  total acc: 41.96%   [EVAL] batch:   14 | acc: 50.00%,  total acc: 42.50%   [EVAL] batch:   15 | acc: 43.75%,  total acc: 42.58%   [EVAL] batch:   16 | acc: 37.50%,  total acc: 42.28%   [EVAL] batch:   17 | acc: 50.00%,  total acc: 42.71%   [EVAL] batch:   18 | acc: 56.25%,  total acc: 43.42%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 46.25%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 48.81%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 50.85%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 52.72%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 54.69%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 56.00%   [EVAL] batch:   25 | acc: 81.25%,  total acc: 56.97%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 58.56%   [EVAL] batch:   27 | acc: 87.50%,  total acc: 59.60%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 60.99%   [EVAL] batch:   29 | acc: 93.75%,  total acc: 62.08%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 63.10%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 64.06%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 65.15%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 66.18%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 66.96%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 67.71%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 68.58%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 69.08%   [EVAL] batch:   38 | acc: 62.50%,  total acc: 68.91%   [EVAL] batch:   39 | acc: 56.25%,  total acc: 68.59%   [EVAL] batch:   40 | acc: 31.25%,  total acc: 67.68%   [EVAL] batch:   41 | acc: 68.75%,  total acc: 67.71%   [EVAL] batch:   42 | acc: 43.75%,  total acc: 67.15%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 67.47%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 68.06%   [EVAL] batch:   45 | acc: 87.50%,  total acc: 68.48%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 69.15%   [EVAL] batch:   47 | acc: 68.75%,  total acc: 69.14%   [EVAL] batch:   48 | acc: 56.25%,  total acc: 68.88%   [EVAL] batch:   49 | acc: 81.25%,  total acc: 69.12%   [EVAL] batch:   50 | acc: 56.25%,  total acc: 68.87%   [EVAL] batch:   51 | acc: 37.50%,  total acc: 68.27%   [EVAL] batch:   52 | acc: 50.00%,  total acc: 67.92%   [EVAL] batch:   53 | acc: 50.00%,  total acc: 67.59%   [EVAL] batch:   54 | acc: 37.50%,  total acc: 67.05%   [EVAL] batch:   55 | acc: 31.25%,  total acc: 66.41%   [EVAL] batch:   56 | acc: 75.00%,  total acc: 66.56%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 67.03%   [EVAL] batch:   58 | acc: 68.75%,  total acc: 67.06%   [EVAL] batch:   59 | acc: 68.75%,  total acc: 67.08%   [EVAL] batch:   60 | acc: 81.25%,  total acc: 67.32%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 67.74%   [EVAL] batch:   62 | acc: 37.50%,  total acc: 67.26%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 82.81%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 82.50%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:    6 | acc: 50.00%,  total acc: 79.46%   [EVAL] batch:    7 | acc: 62.50%,  total acc: 77.34%   [EVAL] batch:    8 | acc: 43.75%,  total acc: 73.61%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 72.50%   [EVAL] batch:   10 | acc: 37.50%,  total acc: 69.32%   [EVAL] batch:   11 | acc: 37.50%,  total acc: 66.67%   [EVAL] batch:   12 | acc: 62.50%,  total acc: 66.35%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 68.30%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 70.00%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 71.88%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 73.16%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 74.31%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 75.66%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 75.94%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 76.79%   [EVAL] batch:   21 | acc: 75.00%,  total acc: 76.70%   [EVAL] batch:   22 | acc: 68.75%,  total acc: 76.36%   [EVAL] batch:   23 | acc: 81.25%,  total acc: 76.56%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 77.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 77.88%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 78.70%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 79.46%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 79.96%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 80.62%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 81.84%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 82.39%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 82.90%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 83.21%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 83.68%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 84.12%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 84.54%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 84.94%   [EVAL] batch:   39 | acc: 93.75%,  total acc: 85.16%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 85.52%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 85.71%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 85.90%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 85.94%   [EVAL] batch:   44 | acc: 81.25%,  total acc: 85.83%   [EVAL] batch:   45 | acc: 81.25%,  total acc: 85.73%   [EVAL] batch:   46 | acc: 75.00%,  total acc: 85.51%   [EVAL] batch:   47 | acc: 81.25%,  total acc: 85.42%   [EVAL] batch:   48 | acc: 87.50%,  total acc: 85.46%   [EVAL] batch:   49 | acc: 75.00%,  total acc: 85.25%   [EVAL] batch:   50 | acc: 37.50%,  total acc: 84.31%   [EVAL] batch:   51 | acc: 56.25%,  total acc: 83.77%   [EVAL] batch:   52 | acc: 56.25%,  total acc: 83.25%   [EVAL] batch:   53 | acc: 43.75%,  total acc: 82.52%   [EVAL] batch:   54 | acc: 18.75%,  total acc: 81.36%   [EVAL] batch:   55 | acc: 50.00%,  total acc: 80.80%   [EVAL] batch:   56 | acc: 81.25%,  total acc: 80.81%   [EVAL] batch:   57 | acc: 81.25%,  total acc: 80.82%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 81.04%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:   60 | acc: 87.50%,  total acc: 81.35%   [EVAL] batch:   61 | acc: 81.25%,  total acc: 81.35%   [EVAL] batch:   62 | acc: 93.75%,  total acc: 81.55%   [EVAL] batch:   63 | acc: 87.50%,  total acc: 81.64%   [EVAL] batch:   64 | acc: 81.25%,  total acc: 81.63%   [EVAL] batch:   65 | acc: 100.00%,  total acc: 81.91%   [EVAL] batch:   66 | acc: 81.25%,  total acc: 81.90%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 82.17%   [EVAL] batch:   68 | acc: 87.50%,  total acc: 82.25%   [EVAL] batch:   69 | acc: 68.75%,  total acc: 82.05%   [EVAL] batch:   70 | acc: 75.00%,  total acc: 81.95%   [EVAL] batch:   71 | acc: 75.00%,  total acc: 81.86%   [EVAL] batch:   72 | acc: 56.25%,  total acc: 81.51%   [EVAL] batch:   73 | acc: 62.50%,  total acc: 81.25%   [EVAL] batch:   74 | acc: 62.50%,  total acc: 81.00%   [EVAL] batch:   75 | acc: 75.00%,  total acc: 80.92%   [EVAL] batch:   76 | acc: 75.00%,  total acc: 80.84%   [EVAL] batch:   77 | acc: 68.75%,  total acc: 80.69%   [EVAL] batch:   78 | acc: 43.75%,  total acc: 80.22%   [EVAL] batch:   79 | acc: 68.75%,  total acc: 80.08%   [EVAL] batch:   80 | acc: 56.25%,  total acc: 79.78%   [EVAL] batch:   81 | acc: 50.00%,  total acc: 79.42%   [EVAL] batch:   82 | acc: 56.25%,  total acc: 79.14%   [EVAL] batch:   83 | acc: 56.25%,  total acc: 78.87%   [EVAL] batch:   84 | acc: 50.00%,  total acc: 78.53%   [EVAL] batch:   85 | acc: 37.50%,  total acc: 78.05%   [EVAL] batch:   86 | acc: 37.50%,  total acc: 77.59%   [EVAL] batch:   87 | acc: 68.75%,  total acc: 77.49%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 77.74%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 77.99%   [EVAL] batch:   90 | acc: 93.75%,  total acc: 78.16%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 78.40%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 78.63%   [EVAL] batch:   93 | acc: 93.75%,  total acc: 78.79%   [EVAL] batch:   94 | acc: 56.25%,  total acc: 78.55%   [EVAL] batch:   95 | acc: 81.25%,  total acc: 78.58%   [EVAL] batch:   96 | acc: 68.75%,  total acc: 78.48%   [EVAL] batch:   97 | acc: 62.50%,  total acc: 78.32%   [EVAL] batch:   98 | acc: 75.00%,  total acc: 78.28%   [EVAL] batch:   99 | acc: 43.75%,  total acc: 77.94%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 78.16%   [EVAL] batch:  101 | acc: 100.00%,  total acc: 78.37%   [EVAL] batch:  102 | acc: 100.00%,  total acc: 78.58%   [EVAL] batch:  103 | acc: 100.00%,  total acc: 78.79%   [EVAL] batch:  104 | acc: 100.00%,  total acc: 78.99%   [EVAL] batch:  105 | acc: 93.75%,  total acc: 79.13%   [EVAL] batch:  106 | acc: 100.00%,  total acc: 79.32%   [EVAL] batch:  107 | acc: 100.00%,  total acc: 79.51%   [EVAL] batch:  108 | acc: 100.00%,  total acc: 79.70%   [EVAL] batch:  109 | acc: 100.00%,  total acc: 79.89%   [EVAL] batch:  110 | acc: 100.00%,  total acc: 80.07%   [EVAL] batch:  111 | acc: 100.00%,  total acc: 80.25%   [EVAL] batch:  112 | acc: 81.25%,  total acc: 80.25%   [EVAL] batch:  113 | acc: 62.50%,  total acc: 80.10%   [EVAL] batch:  114 | acc: 93.75%,  total acc: 80.22%   [EVAL] batch:  115 | acc: 93.75%,  total acc: 80.33%   [EVAL] batch:  116 | acc: 75.00%,  total acc: 80.29%   [EVAL] batch:  117 | acc: 93.75%,  total acc: 80.40%   [EVAL] batch:  118 | acc: 75.00%,  total acc: 80.36%   [EVAL] batch:  119 | acc: 87.50%,  total acc: 80.42%   [EVAL] batch:  120 | acc: 68.75%,  total acc: 80.32%   [EVAL] batch:  121 | acc: 87.50%,  total acc: 80.38%   [EVAL] batch:  122 | acc: 87.50%,  total acc: 80.44%   [EVAL] batch:  123 | acc: 81.25%,  total acc: 80.44%   [EVAL] batch:  124 | acc: 87.50%,  total acc: 80.50%   [EVAL] batch:  125 | acc: 25.00%,  total acc: 80.06%   [EVAL] batch:  126 | acc: 18.75%,  total acc: 79.58%   [EVAL] batch:  127 | acc: 37.50%,  total acc: 79.25%   [EVAL] batch:  128 | acc: 31.25%,  total acc: 78.88%   [EVAL] batch:  129 | acc: 43.75%,  total acc: 78.61%   [EVAL] batch:  130 | acc: 37.50%,  total acc: 78.29%   [EVAL] batch:  131 | acc: 62.50%,  total acc: 78.17%   [EVAL] batch:  132 | acc: 81.25%,  total acc: 78.20%   [EVAL] batch:  133 | acc: 100.00%,  total acc: 78.36%   [EVAL] batch:  134 | acc: 87.50%,  total acc: 78.43%   [EVAL] batch:  135 | acc: 75.00%,  total acc: 78.40%   [EVAL] batch:  136 | acc: 56.25%,  total acc: 78.24%   [EVAL] batch:  137 | acc: 87.50%,  total acc: 78.31%   [EVAL] batch:  138 | acc: 75.00%,  total acc: 78.28%   [EVAL] batch:  139 | acc: 75.00%,  total acc: 78.26%   [EVAL] batch:  140 | acc: 93.75%,  total acc: 78.37%   [EVAL] batch:  141 | acc: 81.25%,  total acc: 78.39%   [EVAL] batch:  142 | acc: 75.00%,  total acc: 78.37%   [EVAL] batch:  143 | acc: 50.00%,  total acc: 78.17%   [EVAL] batch:  144 | acc: 62.50%,  total acc: 78.06%   [EVAL] batch:  145 | acc: 31.25%,  total acc: 77.74%   [EVAL] batch:  146 | acc: 62.50%,  total acc: 77.64%   [EVAL] batch:  147 | acc: 50.00%,  total acc: 77.45%   [EVAL] batch:  148 | acc: 43.75%,  total acc: 77.22%   [EVAL] batch:  149 | acc: 56.25%,  total acc: 77.08%   [EVAL] batch:  150 | acc: 6.25%,  total acc: 76.61%   [EVAL] batch:  151 | acc: 6.25%,  total acc: 76.15%   [EVAL] batch:  152 | acc: 6.25%,  total acc: 75.69%   [EVAL] batch:  153 | acc: 0.00%,  total acc: 75.20%   [EVAL] batch:  154 | acc: 12.50%,  total acc: 74.80%   [EVAL] batch:  155 | acc: 0.00%,  total acc: 74.32%   [EVAL] batch:  156 | acc: 56.25%,  total acc: 74.20%   [EVAL] batch:  157 | acc: 100.00%,  total acc: 74.37%   [EVAL] batch:  158 | acc: 87.50%,  total acc: 74.45%   [EVAL] batch:  159 | acc: 100.00%,  total acc: 74.61%   [EVAL] batch:  160 | acc: 93.75%,  total acc: 74.73%   [EVAL] batch:  161 | acc: 87.50%,  total acc: 74.81%   [EVAL] batch:  162 | acc: 93.75%,  total acc: 74.92%   [EVAL] batch:  163 | acc: 87.50%,  total acc: 75.00%   [EVAL] batch:  164 | acc: 100.00%,  total acc: 75.15%   [EVAL] batch:  165 | acc: 93.75%,  total acc: 75.26%   [EVAL] batch:  166 | acc: 93.75%,  total acc: 75.37%   [EVAL] batch:  167 | acc: 100.00%,  total acc: 75.52%   [EVAL] batch:  168 | acc: 93.75%,  total acc: 75.63%   [EVAL] batch:  169 | acc: 37.50%,  total acc: 75.40%   [EVAL] batch:  170 | acc: 37.50%,  total acc: 75.18%   [EVAL] batch:  171 | acc: 6.25%,  total acc: 74.78%   [EVAL] batch:  172 | acc: 25.00%,  total acc: 74.49%   [EVAL] batch:  173 | acc: 31.25%,  total acc: 74.25%   [EVAL] batch:  174 | acc: 31.25%,  total acc: 74.00%   [EVAL] batch:  175 | acc: 12.50%,  total acc: 73.65%   [EVAL] batch:  176 | acc: 50.00%,  total acc: 73.52%   [EVAL] batch:  177 | acc: 37.50%,  total acc: 73.31%   [EVAL] batch:  178 | acc: 56.25%,  total acc: 73.22%   [EVAL] batch:  179 | acc: 43.75%,  total acc: 73.06%   [EVAL] batch:  180 | acc: 56.25%,  total acc: 72.96%   [EVAL] batch:  181 | acc: 18.75%,  total acc: 72.66%   [EVAL] batch:  182 | acc: 25.00%,  total acc: 72.40%   [EVAL] batch:  183 | acc: 37.50%,  total acc: 72.21%   [EVAL] batch:  184 | acc: 31.25%,  total acc: 71.99%   [EVAL] batch:  185 | acc: 31.25%,  total acc: 71.77%   [EVAL] batch:  186 | acc: 18.75%,  total acc: 71.49%   [EVAL] batch:  187 | acc: 18.75%,  total acc: 71.21%   [EVAL] batch:  188 | acc: 31.25%,  total acc: 71.00%   [EVAL] batch:  189 | acc: 43.75%,  total acc: 70.86%   [EVAL] batch:  190 | acc: 43.75%,  total acc: 70.71%   [EVAL] batch:  191 | acc: 37.50%,  total acc: 70.54%   [EVAL] batch:  192 | acc: 37.50%,  total acc: 70.37%   [EVAL] batch:  193 | acc: 37.50%,  total acc: 70.20%   [EVAL] batch:  194 | acc: 62.50%,  total acc: 70.16%   [EVAL] batch:  195 | acc: 68.75%,  total acc: 70.15%   [EVAL] batch:  196 | acc: 68.75%,  total acc: 70.15%   [EVAL] batch:  197 | acc: 62.50%,  total acc: 70.11%   [EVAL] batch:  198 | acc: 25.00%,  total acc: 69.88%   [EVAL] batch:  199 | acc: 100.00%,  total acc: 70.03%   [EVAL] batch:  200 | acc: 43.75%,  total acc: 69.90%   [EVAL] batch:  201 | acc: 43.75%,  total acc: 69.77%   [EVAL] batch:  202 | acc: 56.25%,  total acc: 69.70%   [EVAL] batch:  203 | acc: 43.75%,  total acc: 69.58%   [EVAL] batch:  204 | acc: 50.00%,  total acc: 69.48%   [EVAL] batch:  205 | acc: 75.00%,  total acc: 69.51%   [EVAL] batch:  206 | acc: 87.50%,  total acc: 69.60%   [EVAL] batch:  207 | acc: 93.75%,  total acc: 69.71%   [EVAL] batch:  208 | acc: 100.00%,  total acc: 69.86%   [EVAL] batch:  209 | acc: 100.00%,  total acc: 70.00%   [EVAL] batch:  210 | acc: 93.75%,  total acc: 70.11%   [EVAL] batch:  211 | acc: 100.00%,  total acc: 70.25%   [EVAL] batch:  212 | acc: 75.00%,  total acc: 70.28%   [EVAL] batch:  213 | acc: 56.25%,  total acc: 70.21%   [EVAL] batch:  214 | acc: 56.25%,  total acc: 70.15%   [EVAL] batch:  215 | acc: 68.75%,  total acc: 70.14%   [EVAL] batch:  216 | acc: 37.50%,  total acc: 69.99%   [EVAL] batch:  217 | acc: 62.50%,  total acc: 69.95%   [EVAL] batch:  218 | acc: 56.25%,  total acc: 69.89%   [EVAL] batch:  219 | acc: 50.00%,  total acc: 69.80%   [EVAL] batch:  220 | acc: 25.00%,  total acc: 69.60%   [EVAL] batch:  221 | acc: 25.00%,  total acc: 69.40%   [EVAL] batch:  222 | acc: 12.50%,  total acc: 69.14%   [EVAL] batch:  223 | acc: 18.75%,  total acc: 68.92%   [EVAL] batch:  224 | acc: 31.25%,  total acc: 68.75%   [EVAL] batch:  225 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:  226 | acc: 81.25%,  total acc: 68.81%   [EVAL] batch:  227 | acc: 75.00%,  total acc: 68.83%   [EVAL] batch:  228 | acc: 93.75%,  total acc: 68.94%   [EVAL] batch:  229 | acc: 68.75%,  total acc: 68.94%   [EVAL] batch:  230 | acc: 75.00%,  total acc: 68.97%   [EVAL] batch:  231 | acc: 68.75%,  total acc: 68.97%   [EVAL] batch:  232 | acc: 56.25%,  total acc: 68.91%   [EVAL] batch:  233 | acc: 68.75%,  total acc: 68.91%   [EVAL] batch:  234 | acc: 68.75%,  total acc: 68.91%   [EVAL] batch:  235 | acc: 56.25%,  total acc: 68.86%   [EVAL] batch:  236 | acc: 81.25%,  total acc: 68.91%   [EVAL] batch:  237 | acc: 93.75%,  total acc: 69.01%   [EVAL] batch:  238 | acc: 100.00%,  total acc: 69.14%   [EVAL] batch:  239 | acc: 100.00%,  total acc: 69.27%   [EVAL] batch:  240 | acc: 100.00%,  total acc: 69.40%   [EVAL] batch:  241 | acc: 100.00%,  total acc: 69.52%   [EVAL] batch:  242 | acc: 100.00%,  total acc: 69.65%   [EVAL] batch:  243 | acc: 100.00%,  total acc: 69.77%   [EVAL] batch:  244 | acc: 100.00%,  total acc: 69.90%   [EVAL] batch:  245 | acc: 87.50%,  total acc: 69.97%   [EVAL] batch:  246 | acc: 93.75%,  total acc: 70.07%   [EVAL] batch:  247 | acc: 93.75%,  total acc: 70.16%   [EVAL] batch:  248 | acc: 100.00%,  total acc: 70.28%   [EVAL] batch:  249 | acc: 100.00%,  total acc: 70.40%   [EVAL] batch:  250 | acc: 75.00%,  total acc: 70.42%   [EVAL] batch:  251 | acc: 62.50%,  total acc: 70.39%   [EVAL] batch:  252 | acc: 68.75%,  total acc: 70.38%   [EVAL] batch:  253 | acc: 62.50%,  total acc: 70.35%   [EVAL] batch:  254 | acc: 75.00%,  total acc: 70.37%   [EVAL] batch:  255 | acc: 75.00%,  total acc: 70.39%   [EVAL] batch:  256 | acc: 81.25%,  total acc: 70.43%   [EVAL] batch:  257 | acc: 75.00%,  total acc: 70.45%   [EVAL] batch:  258 | acc: 68.75%,  total acc: 70.44%   [EVAL] batch:  259 | acc: 75.00%,  total acc: 70.46%   [EVAL] batch:  260 | acc: 56.25%,  total acc: 70.40%   [EVAL] batch:  261 | acc: 62.50%,  total acc: 70.37%   [EVAL] batch:  262 | acc: 100.00%,  total acc: 70.48%   [EVAL] batch:  263 | acc: 68.75%,  total acc: 70.48%   [EVAL] batch:  264 | acc: 75.00%,  total acc: 70.50%   [EVAL] batch:  265 | acc: 68.75%,  total acc: 70.49%   [EVAL] batch:  266 | acc: 87.50%,  total acc: 70.55%   [EVAL] batch:  267 | acc: 81.25%,  total acc: 70.59%   [EVAL] batch:  268 | acc: 93.75%,  total acc: 70.68%   [EVAL] batch:  269 | acc: 81.25%,  total acc: 70.72%   [EVAL] batch:  270 | acc: 62.50%,  total acc: 70.69%   [EVAL] batch:  271 | acc: 68.75%,  total acc: 70.68%   [EVAL] batch:  272 | acc: 75.00%,  total acc: 70.70%   [EVAL] batch:  273 | acc: 87.50%,  total acc: 70.76%   [EVAL] batch:  274 | acc: 87.50%,  total acc: 70.82%   [EVAL] batch:  275 | acc: 100.00%,  total acc: 70.92%   [EVAL] batch:  276 | acc: 100.00%,  total acc: 71.03%   [EVAL] batch:  277 | acc: 100.00%,  total acc: 71.13%   [EVAL] batch:  278 | acc: 93.75%,  total acc: 71.21%   [EVAL] batch:  279 | acc: 100.00%,  total acc: 71.32%   [EVAL] batch:  280 | acc: 100.00%,  total acc: 71.42%   [EVAL] batch:  281 | acc: 100.00%,  total acc: 71.52%   [EVAL] batch:  282 | acc: 100.00%,  total acc: 71.62%   [EVAL] batch:  283 | acc: 100.00%,  total acc: 71.72%   [EVAL] batch:  284 | acc: 100.00%,  total acc: 71.82%   [EVAL] batch:  285 | acc: 100.00%,  total acc: 71.92%   [EVAL] batch:  286 | acc: 100.00%,  total acc: 72.02%   [EVAL] batch:  287 | acc: 87.50%,  total acc: 72.07%   [EVAL] batch:  288 | acc: 56.25%,  total acc: 72.02%   [EVAL] batch:  289 | acc: 62.50%,  total acc: 71.98%   [EVAL] batch:  290 | acc: 68.75%,  total acc: 71.97%   [EVAL] batch:  291 | acc: 81.25%,  total acc: 72.00%   [EVAL] batch:  292 | acc: 81.25%,  total acc: 72.03%   [EVAL] batch:  293 | acc: 56.25%,  total acc: 71.98%   [EVAL] batch:  294 | acc: 37.50%,  total acc: 71.86%   [EVAL] batch:  295 | acc: 31.25%,  total acc: 71.73%   [EVAL] batch:  296 | acc: 31.25%,  total acc: 71.59%   [EVAL] batch:  297 | acc: 50.00%,  total acc: 71.52%   [EVAL] batch:  298 | acc: 37.50%,  total acc: 71.40%   [EVAL] batch:  299 | acc: 31.25%,  total acc: 71.27%   [EVAL] batch:  300 | acc: 93.75%,  total acc: 71.35%   [EVAL] batch:  301 | acc: 93.75%,  total acc: 71.42%   [EVAL] batch:  302 | acc: 93.75%,  total acc: 71.49%   [EVAL] batch:  303 | acc: 81.25%,  total acc: 71.53%   [EVAL] batch:  304 | acc: 93.75%,  total acc: 71.60%   [EVAL] batch:  305 | acc: 87.50%,  total acc: 71.65%   [EVAL] batch:  306 | acc: 50.00%,  total acc: 71.58%   [EVAL] batch:  307 | acc: 25.00%,  total acc: 71.43%   [EVAL] batch:  308 | acc: 18.75%,  total acc: 71.26%   [EVAL] batch:  309 | acc: 25.00%,  total acc: 71.11%   [EVAL] batch:  310 | acc: 31.25%,  total acc: 70.98%   [EVAL] batch:  311 | acc: 18.75%,  total acc: 70.81%   [EVAL] batch:  312 | acc: 25.00%,  total acc: 70.67%   [EVAL] batch:  313 | acc: 18.75%,  total acc: 70.50%   [EVAL] batch:  314 | acc: 6.25%,  total acc: 70.30%   [EVAL] batch:  315 | acc: 0.00%,  total acc: 70.08%   [EVAL] batch:  316 | acc: 12.50%,  total acc: 69.89%   [EVAL] batch:  317 | acc: 18.75%,  total acc: 69.73%   [EVAL] batch:  318 | acc: 25.00%,  total acc: 69.59%   [EVAL] batch:  319 | acc: 81.25%,  total acc: 69.63%   [EVAL] batch:  320 | acc: 56.25%,  total acc: 69.59%   [EVAL] batch:  321 | acc: 81.25%,  total acc: 69.62%   [EVAL] batch:  322 | acc: 87.50%,  total acc: 69.68%   [EVAL] batch:  323 | acc: 75.00%,  total acc: 69.70%   [EVAL] batch:  324 | acc: 81.25%,  total acc: 69.73%   [EVAL] batch:  325 | acc: 43.75%,  total acc: 69.65%   [EVAL] batch:  326 | acc: 18.75%,  total acc: 69.50%   [EVAL] batch:  327 | acc: 43.75%,  total acc: 69.42%   [EVAL] batch:  328 | acc: 50.00%,  total acc: 69.36%   [EVAL] batch:  329 | acc: 50.00%,  total acc: 69.30%   [EVAL] batch:  330 | acc: 43.75%,  total acc: 69.22%   [EVAL] batch:  331 | acc: 81.25%,  total acc: 69.26%   [EVAL] batch:  332 | acc: 100.00%,  total acc: 69.35%   [EVAL] batch:  333 | acc: 100.00%,  total acc: 69.44%   [EVAL] batch:  334 | acc: 87.50%,  total acc: 69.50%   [EVAL] batch:  335 | acc: 100.00%,  total acc: 69.59%   [EVAL] batch:  336 | acc: 100.00%,  total acc: 69.68%   [EVAL] batch:  337 | acc: 75.00%,  total acc: 69.69%   [EVAL] batch:  338 | acc: 93.75%,  total acc: 69.76%   [EVAL] batch:  339 | acc: 93.75%,  total acc: 69.83%   [EVAL] batch:  340 | acc: 93.75%,  total acc: 69.90%   [EVAL] batch:  341 | acc: 93.75%,  total acc: 69.97%   [EVAL] batch:  342 | acc: 100.00%,  total acc: 70.06%   [EVAL] batch:  343 | acc: 87.50%,  total acc: 70.11%   [EVAL] batch:  344 | acc: 100.00%,  total acc: 70.20%   [EVAL] batch:  345 | acc: 100.00%,  total acc: 70.29%   [EVAL] batch:  346 | acc: 93.75%,  total acc: 70.35%   [EVAL] batch:  347 | acc: 93.75%,  total acc: 70.42%   [EVAL] batch:  348 | acc: 100.00%,  total acc: 70.51%   [EVAL] batch:  349 | acc: 100.00%,  total acc: 70.59%   [EVAL] batch:  350 | acc: 62.50%,  total acc: 70.57%   [EVAL] batch:  351 | acc: 62.50%,  total acc: 70.54%   [EVAL] batch:  352 | acc: 37.50%,  total acc: 70.45%   [EVAL] batch:  353 | acc: 62.50%,  total acc: 70.43%   [EVAL] batch:  354 | acc: 56.25%,  total acc: 70.39%   [EVAL] batch:  355 | acc: 56.25%,  total acc: 70.35%   [EVAL] batch:  356 | acc: 93.75%,  total acc: 70.41%   [EVAL] batch:  357 | acc: 87.50%,  total acc: 70.46%   [EVAL] batch:  358 | acc: 93.75%,  total acc: 70.53%   [EVAL] batch:  359 | acc: 87.50%,  total acc: 70.57%   [EVAL] batch:  360 | acc: 56.25%,  total acc: 70.53%   [EVAL] batch:  361 | acc: 68.75%,  total acc: 70.53%   [EVAL] batch:  362 | acc: 62.50%,  total acc: 70.51%   [EVAL] batch:  363 | acc: 56.25%,  total acc: 70.47%   [EVAL] batch:  364 | acc: 37.50%,  total acc: 70.38%   [EVAL] batch:  365 | acc: 56.25%,  total acc: 70.34%   [EVAL] batch:  366 | acc: 50.00%,  total acc: 70.28%   [EVAL] batch:  367 | acc: 31.25%,  total acc: 70.18%   [EVAL] batch:  368 | acc: 37.50%,  total acc: 70.09%   [EVAL] batch:  369 | acc: 100.00%,  total acc: 70.17%   [EVAL] batch:  370 | acc: 87.50%,  total acc: 70.22%   [EVAL] batch:  371 | acc: 56.25%,  total acc: 70.18%   [EVAL] batch:  372 | acc: 75.00%,  total acc: 70.19%   [EVAL] batch:  373 | acc: 93.75%,  total acc: 70.25%   [EVAL] batch:  374 | acc: 81.25%,  total acc: 70.28%   
cur_acc:  ['0.9494', '0.8244', '0.5962', '0.7173', '0.7708', '0.6726']
his_acc:  ['0.9494', '0.8760', '0.7746', '0.7435', '0.7324', '0.7028']
Clustering into  34  clusters
Clusters:  [ 2  0  9 11  8 12  9  6  0 16 11 23  2  0 33 13 14 24 18 14  8 17 10 31
  5 19 10  0 10  0  5 25  7  4 28 21  0 13 20  1 14 10  6 17 18 27  7 15
  4 15 21 22 15  3  3 22 15  1 16 32 30 28  6  8 26 16  2 12 22 29]
Losses:  9.592751502990723 3.1500349044799805 0.7450398206710815
CurrentTrain: epoch  0, batch     0 | loss: 9.5927515Losses:  8.544266700744629 1.9628660678863525 0.8291439414024353
CurrentTrain: epoch  0, batch     1 | loss: 8.5442667Losses:  11.075921058654785 3.8820338249206543 0.7968800663948059
CurrentTrain: epoch  0, batch     2 | loss: 11.0759211Losses:  4.815943241119385 -0.0 0.10938264429569244
CurrentTrain: epoch  0, batch     3 | loss: 4.8159432Losses:  9.129470825195312 3.483696937561035 0.7030279636383057
CurrentTrain: epoch  1, batch     0 | loss: 9.1294708Losses:  8.611946105957031 2.689157247543335 0.8197354674339294
CurrentTrain: epoch  1, batch     1 | loss: 8.6119461Losses:  8.49045467376709 3.6454601287841797 0.7087170481681824
CurrentTrain: epoch  1, batch     2 | loss: 8.4904547Losses:  2.9684898853302 -0.0 0.11337200552225113
CurrentTrain: epoch  1, batch     3 | loss: 2.9684899Losses:  8.6981201171875 3.4440767765045166 0.7393513917922974
CurrentTrain: epoch  2, batch     0 | loss: 8.6981201Losses:  7.195810317993164 3.057007074356079 0.7292139530181885
CurrentTrain: epoch  2, batch     1 | loss: 7.1958103Losses:  6.399176597595215 1.9488517045974731 0.7930235862731934
CurrentTrain: epoch  2, batch     2 | loss: 6.3991766Losses:  3.4094693660736084 -0.0 0.10003827512264252
CurrentTrain: epoch  2, batch     3 | loss: 3.4094694Losses:  6.978777885437012 2.87300968170166 0.7793047428131104
CurrentTrain: epoch  3, batch     0 | loss: 6.9787779Losses:  7.315246105194092 3.1983842849731445 0.7472066879272461
CurrentTrain: epoch  3, batch     1 | loss: 7.3152461Losses:  6.7592597007751465 2.6793630123138428 0.7405775189399719
CurrentTrain: epoch  3, batch     2 | loss: 6.7592597Losses:  2.7824788093566895 -0.0 0.1827673465013504
CurrentTrain: epoch  3, batch     3 | loss: 2.7824788Losses:  6.672849178314209 3.6242196559906006 0.6459964513778687
CurrentTrain: epoch  4, batch     0 | loss: 6.6728492Losses:  8.512585639953613 4.377621650695801 0.649059534072876
CurrentTrain: epoch  4, batch     1 | loss: 8.5125856Losses:  6.492143630981445 3.2107887268066406 0.572913408279419
CurrentTrain: epoch  4, batch     2 | loss: 6.4921436Losses:  2.4998364448547363 -0.0 0.10128071159124374
CurrentTrain: epoch  4, batch     3 | loss: 2.4998364Losses:  6.590221881866455 3.1322126388549805 0.7716063261032104
CurrentTrain: epoch  5, batch     0 | loss: 6.5902219Losses:  6.365007400512695 2.848959445953369 0.7278902530670166
CurrentTrain: epoch  5, batch     1 | loss: 6.3650074Losses:  5.603994846343994 2.477876663208008 0.7039211392402649
CurrentTrain: epoch  5, batch     2 | loss: 5.6039948Losses:  1.953771710395813 -0.0 0.09310842305421829
CurrentTrain: epoch  5, batch     3 | loss: 1.9537717Losses:  6.178438663482666 2.9659643173217773 0.704258382320404
CurrentTrain: epoch  6, batch     0 | loss: 6.1784387Losses:  8.092105865478516 4.396546840667725 0.5870906710624695
CurrentTrain: epoch  6, batch     1 | loss: 8.0921059Losses:  5.382072448730469 2.5305800437927246 0.6884210705757141
CurrentTrain: epoch  6, batch     2 | loss: 5.3820724Losses:  2.4690194129943848 -0.0 0.1533917784690857
CurrentTrain: epoch  6, batch     3 | loss: 2.4690194Losses:  6.168145656585693 3.353980302810669 0.6311823129653931
CurrentTrain: epoch  7, batch     0 | loss: 6.1681457Losses:  5.987506866455078 2.844759464263916 0.7141420841217041
CurrentTrain: epoch  7, batch     1 | loss: 5.9875069Losses:  5.709651947021484 2.786094903945923 0.7516724467277527
CurrentTrain: epoch  7, batch     2 | loss: 5.7096519Losses:  3.075991630554199 -0.0 0.12411116063594818
CurrentTrain: epoch  7, batch     3 | loss: 3.0759916Losses:  6.727697849273682 3.8247156143188477 0.6427373290061951
CurrentTrain: epoch  8, batch     0 | loss: 6.7276978Losses:  4.841856956481934 1.8328180313110352 0.7512274384498596
CurrentTrain: epoch  8, batch     1 | loss: 4.8418570Losses:  6.664278984069824 4.088114261627197 0.6197295188903809
CurrentTrain: epoch  8, batch     2 | loss: 6.6642790Losses:  1.7992627620697021 -0.0 0.10787706077098846
CurrentTrain: epoch  8, batch     3 | loss: 1.7992628Losses:  5.2882914543151855 2.668248176574707 0.6319466233253479
CurrentTrain: epoch  9, batch     0 | loss: 5.2882915Losses:  5.82230806350708 2.816021203994751 0.6954646706581116
CurrentTrain: epoch  9, batch     1 | loss: 5.8223081Losses:  4.831634044647217 2.0883700847625732 0.7430718541145325
CurrentTrain: epoch  9, batch     2 | loss: 4.8316340Losses:  2.327890157699585 -0.0 0.07747729867696762
CurrentTrain: epoch  9, batch     3 | loss: 2.3278902
#### spouse ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "spouse" means the subject has the object as their spouse (husband, wife, partner, etc.).
Here is an example:
Relation: spouse
Context: rohan is the son of narayana murthy , founder of infosys , and his wife sudha murthy , an engineer and author of note .
Head Entity: narayana murthy
Tail Entity: sudha murthy
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "spouse":
Relation: spouse  
Context: elena is a renowned scientist who often collaborates with her husband, dr. mark thompson, on various research projects.  
Head Entity: dr. mark thompson  
Tail Entity: elena  

Relation: spouse  
Context: during the award ceremony, it was revealed that the famous actor, john doe, had been married to his long-time partner, jane smith, for over a decade.  
Head Entity: john doe  
Tail Entity: jane smith  
#### record label ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "record label" means brand and trademark associated with the marketing of subject music recordings and music videos.
Here is an example:
Relation: record label
Context: they briefly reformed in 1992 , when sub pop re - released " primal rock therapy " on cd with five additional unreleased tracks .
Head Entity: primal rock therapy
Tail Entity: sub pop
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "record label":
Relation: record label  
Context: The album "Future Nostalgia" was released under the Warner Records label, showcasing Dua Lipa's unique pop sound.  
Head Entity: Future Nostalgia  
Tail Entity: Warner Records  

Relation: record label  
Context: After signing with Columbia Records, the band released their highly anticipated debut album, which quickly climbed the charts.  
Head Entity: debut album  
Tail Entity: Columbia Records  
#### distributor ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "distributor" means distributor of a creative work; distributor for a record label; news agency; film distributor.
Here is an example:
Relation: distributor
Context: distributed by paramount pictures , " the italian job " was theatrically released in the united states on may 30 , 2003 , and grossed over $ 176 million worldwide .
Head Entity: the italian job
Tail Entity: paramount pictures
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "distributor":
Relation: distributor  
Context: the film was distributed by universal pictures and received critical acclaim upon its release in 2015.  
Head Entity: the film  
Tail Entity: universal pictures  

Relation: distributor  
Context: the album was distributed by sony music and quickly climbed the charts, becoming a bestseller.  
Head Entity: the album  
Tail Entity: sony music  
#### located in or next to body of water ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "located in or next to body of water" means body of water on or next to which a place is located.
Here is an example:
Relation: located in or next to body of water
Context: it then crossed the indian ocean , passing st. pierre island , providence atoll and farquhar atoll of seychelles before making landfall in southeastern australia .
Head Entity: farquhar atoll
Tail Entity: indian ocean
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "located in or next to body of water":
Relation: located in or next to body of water  
Context: The city of Venice is famous for its canals, which are situated right next to the Adriatic Sea, making it a unique destination for tourists.  
Head Entity: Venice  
Tail Entity: Adriatic Sea  

Relation: located in or next to body of water  
Context: The village of Hvar is nestled on the shores of the Aegean Sea, attracting visitors with its stunning views and vibrant culture.  
Head Entity: Hvar  
Tail Entity: Aegean Sea  
#### subsidiary ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "subsidiary" means subsidiary of a company or organization; generally a fully owned separate corporation.
Here is an example:
Relation: subsidiary
Context: cbs and viacom split again in 2005 , but both are still owned by national amusements .
Head Entity: national amusements
Tail Entity: viacom
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "subsidiary":
Relation: subsidiary  
Context: In 2018, Facebook acquired WhatsApp, making it a subsidiary of the social media giant.  
Head Entity: Facebook  
Tail Entity: WhatsApp  

Relation: subsidiary  
Context: Toyota has several subsidiaries, including Lexus, which focuses on luxury vehicles.  
Head Entity: Toyota  
Tail Entity: Lexus  
#### country of citizenship ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "country of citizenship" means the object is a country that recognizes the subject as its citizen.
Here is an example:
Relation: country of citizenship
Context: anders morelius from sweden placed third in the individual race .
Head Entity: anders morelius
Tail Entity: sweden
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "country of citizenship":
Relation: country of citizenship  
Context: maria gonzalez was born in mexico but later moved to the united states.  
Head Entity: maria gonzalez  
Tail Entity: mexico  

Relation: country of citizenship  
Context: after living in canada for several years, li wei decided to apply for citizenship there.  
Head Entity: li wei  
Tail Entity: canada  
#### genre ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "genre" means creative work's genre or an artist's field of work.
Here is an example:
Relation: genre
Context: he is also a part of the atlanta - based hip - hop / r&b;/soul musical collective the dungeon family .
Head Entity: the dungeon family
Tail Entity: hip - hop
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "genre":
Relation: genre  
Context: the band is known for their unique blend of rock and electronic music, often categorized under the genre of synth-pop.  
Head Entity: the band  
Tail Entity: synth-pop  

Relation: genre  
Context: she has made a significant impact in the world of classical music, particularly as a renowned pianist and composer in the genre of contemporary classical.  
Head Entity: she  
Tail Entity: contemporary classical  
#### mouth of the watercourse ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "mouth of the watercourse" means the body of water to which the watercourse drains.
Here is an example:
Relation: mouth of the watercourse
Context: san lázaro is 660 km far from asunción and is located at the exact site of the confluence of the apa river with paraguay river , bordering brazil - chaco .
Head Entity: apa river
Tail Entity: paraguay river
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "mouth of the watercourse":
Relation: mouth of the watercourse  
Context: the mississippi river flows into the gulf of mexico, creating a rich delta ecosystem that supports diverse wildlife and plant species.  
Head Entity: mississippi river  
Tail Entity: gulf of mexico  

Relation: mouth of the watercourse  
Context: the rhine river, which runs through several countries in europe, ultimately empties into the north sea, providing a vital shipping route for international trade.  
Head Entity: rhine river  
Tail Entity: north sea  
#### position played on team / speciality ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "position played on team / speciality" means position or specialism of a player on a team.
Here is an example:
Relation: position played on team / speciality
Context: bradley halliday ( born 10 july 1995 ) is an english professional footballer who plays as a defender or a midfielder for league two club cambridge united .
Head Entity: bradley halliday
Tail Entity: defender
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "position played on team / speciality":
Relation: position played on team / speciality  
Context: alex morgan ( born 2 july 1989 ) is an american soccer player who plays as a forward for the national team and the club san diego wave fc in the nwsl.  
Head Entity: alex morgan  
Tail Entity: forward  

Relation: position played on team / speciality  
Context: lebron james ( born 30 december 1984 ) is an american professional basketball player who plays as a small forward for the los angeles lakers in the nba.  
Head Entity: lebron james  
Tail Entity: small forward  
#### sports season of league or competition ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "sports season of league or competition" means property that shows the competition of which the item is a season.
Here is an example:
Relation: sports season of league or competition
Context: hammami made his international debut for tunisia in 2007 , and represented them at the africa cup of nations in 2010 and 2013 .
Head Entity: 2010
Tail Entity: africa cup of nations
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "sports season of league or competition":
Relation: sports season of league or competition  
Context: The 2021 UEFA European Championship was held in various cities across Europe, showcasing top national teams competing for the title.  
Head Entity: 2021  
Tail Entity: UEFA European Championship  

Relation: sports season of league or competition  
Context: The 2022 FIFA World Cup took place in Qatar, marking the first time the tournament was held in the Middle East.  
Head Entity: 2022  
Tail Entity: FIFA World Cup  
Losses:  4.757978439331055 0.5684486627578735 0.971412718296051
MemoryTrain:  epoch  0, batch     0 | loss: 4.7579784Losses:  4.452425003051758 0.8639230132102966 0.9760624766349792
MemoryTrain:  epoch  0, batch     1 | loss: 4.4524250Losses:  3.0874476432800293 0.24630442261695862 0.9013440012931824
MemoryTrain:  epoch  0, batch     2 | loss: 3.0874476Losses:  3.883838176727295 0.755262553691864 0.9283987879753113
MemoryTrain:  epoch  0, batch     3 | loss: 3.8838382Losses:  3.622551918029785 -0.0 1.0350435972213745
MemoryTrain:  epoch  0, batch     4 | loss: 3.6225519Losses:  4.796926975250244 0.5270314812660217 0.944854736328125
MemoryTrain:  epoch  0, batch     5 | loss: 4.7969270Losses:  3.2266156673431396 0.2534431219100952 0.9284254908561707
MemoryTrain:  epoch  0, batch     6 | loss: 3.2266157Losses:  4.502416133880615 0.24827587604522705 0.9505847692489624
MemoryTrain:  epoch  0, batch     7 | loss: 4.5024161Losses:  3.412670373916626 -0.0 1.0238618850708008
MemoryTrain:  epoch  0, batch     8 | loss: 3.4126704Losses:  4.476563453674316 0.2910577654838562 0.9094429016113281
MemoryTrain:  epoch  0, batch     9 | loss: 4.4765635Losses:  4.51478910446167 0.2865860164165497 1.035911202430725
MemoryTrain:  epoch  0, batch    10 | loss: 4.5147891Losses:  4.39664888381958 0.2641664147377014 0.9435833096504211
MemoryTrain:  epoch  0, batch    11 | loss: 4.3966489Losses:  4.844666481018066 0.2990732192993164 1.033537745475769
MemoryTrain:  epoch  0, batch    12 | loss: 4.8446665Losses:  3.244281053543091 -0.0 0.114059217274189
MemoryTrain:  epoch  0, batch    13 | loss: 3.2442811Losses:  4.355166912078857 0.28464728593826294 0.997057318687439
MemoryTrain:  epoch  1, batch     0 | loss: 4.3551669Losses:  4.913225173950195 0.49067553877830505 0.9390122890472412
MemoryTrain:  epoch  1, batch     1 | loss: 4.9132252Losses:  4.491451263427734 0.8205438256263733 0.8958282470703125
MemoryTrain:  epoch  1, batch     2 | loss: 4.4914513Losses:  4.167701721191406 0.2672364115715027 0.9662203192710876
MemoryTrain:  epoch  1, batch     3 | loss: 4.1677017Losses:  4.015299320220947 -0.0 0.9678462743759155
MemoryTrain:  epoch  1, batch     4 | loss: 4.0152993Losses:  3.8829872608184814 0.4542008638381958 0.8964827060699463
MemoryTrain:  epoch  1, batch     5 | loss: 3.8829873Losses:  3.395968198776245 0.23939238488674164 0.8940935134887695
MemoryTrain:  epoch  1, batch     6 | loss: 3.3959682Losses:  4.068418979644775 0.2888386845588684 0.912143886089325
MemoryTrain:  epoch  1, batch     7 | loss: 4.0684190Losses:  3.4391818046569824 0.26928597688674927 1.0077484846115112
MemoryTrain:  epoch  1, batch     8 | loss: 3.4391818Losses:  3.0859732627868652 0.2343832403421402 1.0096851587295532
MemoryTrain:  epoch  1, batch     9 | loss: 3.0859733Losses:  2.822213649749756 -0.0 0.9631109833717346
MemoryTrain:  epoch  1, batch    10 | loss: 2.8222136Losses:  3.337502956390381 0.24006524682044983 0.950724720954895
MemoryTrain:  epoch  1, batch    11 | loss: 3.3375030Losses:  2.981748104095459 0.2602267861366272 0.9294154047966003
MemoryTrain:  epoch  1, batch    12 | loss: 2.9817481Losses:  4.437921524047852 -0.0 0.1786731481552124
MemoryTrain:  epoch  1, batch    13 | loss: 4.4379215Losses:  3.4039535522460938 0.5273388028144836 0.9951832294464111
MemoryTrain:  epoch  2, batch     0 | loss: 3.4039536Losses:  3.308321237564087 0.27367666363716125 0.9590782523155212
MemoryTrain:  epoch  2, batch     1 | loss: 3.3083212Losses:  4.019415855407715 0.2837486267089844 0.9535847306251526
MemoryTrain:  epoch  2, batch     2 | loss: 4.0194159Losses:  4.365192890167236 0.4887841045856476 0.9626879096031189
MemoryTrain:  epoch  2, batch     3 | loss: 4.3651929Losses:  2.94950795173645 0.23459696769714355 0.9815012216567993
MemoryTrain:  epoch  2, batch     4 | loss: 2.9495080Losses:  2.425475835800171 -0.0 0.8679736256599426
MemoryTrain:  epoch  2, batch     5 | loss: 2.4254758Losses:  3.2052252292633057 0.8555465936660767 0.808066189289093
MemoryTrain:  epoch  2, batch     6 | loss: 3.2052252Losses:  3.326162338256836 -0.0 0.8736523389816284
MemoryTrain:  epoch  2, batch     7 | loss: 3.3261623Losses:  3.330742835998535 0.2692916691303253 0.9715349078178406
MemoryTrain:  epoch  2, batch     8 | loss: 3.3307428Losses:  3.8002448081970215 0.742719292640686 0.917742133140564
MemoryTrain:  epoch  2, batch     9 | loss: 3.8002448Losses:  3.4792027473449707 0.6085032224655151 0.8609580993652344
MemoryTrain:  epoch  2, batch    10 | loss: 3.4792027Losses:  2.912078857421875 0.24978765845298767 0.927176833152771
MemoryTrain:  epoch  2, batch    11 | loss: 2.9120789Losses:  3.599618434906006 0.27861571311950684 0.9534667730331421
MemoryTrain:  epoch  2, batch    12 | loss: 3.5996184Losses:  1.4621970653533936 -0.0 0.10852265357971191
MemoryTrain:  epoch  2, batch    13 | loss: 1.4621971Losses:  3.656162738800049 0.47303372621536255 0.9446967840194702
MemoryTrain:  epoch  3, batch     0 | loss: 3.6561627Losses:  3.188046455383301 0.3512418270111084 0.985230565071106
MemoryTrain:  epoch  3, batch     1 | loss: 3.1880465Losses:  3.394731283187866 0.26507192850112915 1.0175740718841553
MemoryTrain:  epoch  3, batch     2 | loss: 3.3947313Losses:  2.5189852714538574 0.23922374844551086 0.8768569827079773
MemoryTrain:  epoch  3, batch     3 | loss: 2.5189853Losses:  2.4991114139556885 0.2471717894077301 0.910184383392334
MemoryTrain:  epoch  3, batch     4 | loss: 2.4991114Losses:  2.7470507621765137 0.2598688006401062 0.9256619215011597
MemoryTrain:  epoch  3, batch     5 | loss: 2.7470508Losses:  3.5183448791503906 0.2508944272994995 1.0183086395263672
MemoryTrain:  epoch  3, batch     6 | loss: 3.5183449Losses:  3.468757390975952 0.8062864542007446 0.9168054461479187
MemoryTrain:  epoch  3, batch     7 | loss: 3.4687574Losses:  3.0905208587646484 0.478046715259552 0.9078347086906433
MemoryTrain:  epoch  3, batch     8 | loss: 3.0905209Losses:  3.075087547302246 0.2741104066371918 0.9012755155563354
MemoryTrain:  epoch  3, batch     9 | loss: 3.0750875Losses:  3.2461295127868652 0.27903568744659424 0.9679380059242249
MemoryTrain:  epoch  3, batch    10 | loss: 3.2461295Losses:  3.3791584968566895 0.2803511321544647 0.9703962206840515
MemoryTrain:  epoch  3, batch    11 | loss: 3.3791585Losses:  2.9692883491516113 -0.0 1.0285508632659912
MemoryTrain:  epoch  3, batch    12 | loss: 2.9692883Losses:  1.5905930995941162 -0.0 0.10577847063541412
MemoryTrain:  epoch  3, batch    13 | loss: 1.5905931Losses:  3.0028915405273438 0.2521660625934601 1.0101696252822876
MemoryTrain:  epoch  4, batch     0 | loss: 3.0028915Losses:  2.9889039993286133 0.2908158004283905 0.9768154621124268
MemoryTrain:  epoch  4, batch     1 | loss: 2.9889040Losses:  3.068279266357422 0.4922676086425781 0.975037693977356
MemoryTrain:  epoch  4, batch     2 | loss: 3.0682793Losses:  2.313032627105713 -0.0 0.9273515939712524
MemoryTrain:  epoch  4, batch     3 | loss: 2.3130326Losses:  4.450254440307617 0.25920605659484863 0.9680514335632324
MemoryTrain:  epoch  4, batch     4 | loss: 4.4502544Losses:  2.962437152862549 0.5675299167633057 0.8299393653869629
MemoryTrain:  epoch  4, batch     5 | loss: 2.9624372Losses:  2.8920836448669434 -0.0 1.0104502439498901
MemoryTrain:  epoch  4, batch     6 | loss: 2.8920836Losses:  3.0384488105773926 0.26112109422683716 1.0386966466903687
MemoryTrain:  epoch  4, batch     7 | loss: 3.0384488Losses:  2.5908894538879395 0.49389708042144775 0.7968717217445374
MemoryTrain:  epoch  4, batch     8 | loss: 2.5908895Losses:  3.673295497894287 0.48958465456962585 0.9033357501029968
MemoryTrain:  epoch  4, batch     9 | loss: 3.6732955Losses:  3.36415433883667 0.9888718128204346 0.899833083152771
MemoryTrain:  epoch  4, batch    10 | loss: 3.3641543Losses:  2.8694560527801514 0.4730016589164734 1.037003517150879
MemoryTrain:  epoch  4, batch    11 | loss: 2.8694561Losses:  2.6323025226593018 -0.0 1.0154435634613037
MemoryTrain:  epoch  4, batch    12 | loss: 2.6323025Losses:  1.9161285161972046 -0.0 0.12919628620147705
MemoryTrain:  epoch  4, batch    13 | loss: 1.9161285Losses:  2.5727880001068115 0.244074746966362 1.0104378461837769
MemoryTrain:  epoch  5, batch     0 | loss: 2.5727880Losses:  3.183504819869995 -0.0 1.0213592052459717
MemoryTrain:  epoch  5, batch     1 | loss: 3.1835048Losses:  2.835468292236328 -0.0 1.0667293071746826
MemoryTrain:  epoch  5, batch     2 | loss: 2.8354683Losses:  2.9076437950134277 0.25787150859832764 0.8964771032333374
MemoryTrain:  epoch  5, batch     3 | loss: 2.9076438Losses:  3.5701019763946533 0.2960367202758789 0.9186007380485535
MemoryTrain:  epoch  5, batch     4 | loss: 3.5701020Losses:  2.3707220554351807 -0.0 0.8463166356086731
MemoryTrain:  epoch  5, batch     5 | loss: 2.3707221Losses:  2.539458990097046 -0.0 0.9076263308525085
MemoryTrain:  epoch  5, batch     6 | loss: 2.5394590Losses:  2.3747599124908447 -0.0 0.9164513945579529
MemoryTrain:  epoch  5, batch     7 | loss: 2.3747599Losses:  2.2453887462615967 -0.0 0.9297005534172058
MemoryTrain:  epoch  5, batch     8 | loss: 2.2453887Losses:  2.9220967292785645 0.23294971883296967 1.040540099143982
MemoryTrain:  epoch  5, batch     9 | loss: 2.9220967Losses:  3.0014522075653076 0.26095691323280334 1.015225887298584
MemoryTrain:  epoch  5, batch    10 | loss: 3.0014522Losses:  2.5919201374053955 0.2587299346923828 0.9186062216758728
MemoryTrain:  epoch  5, batch    11 | loss: 2.5919201Losses:  3.1256399154663086 0.5378406047821045 0.9668067693710327
MemoryTrain:  epoch  5, batch    12 | loss: 3.1256399Losses:  1.4696788787841797 -0.0 0.16228026151657104
MemoryTrain:  epoch  5, batch    13 | loss: 1.4696789Losses:  3.2975714206695557 0.5147173404693604 1.015716552734375
MemoryTrain:  epoch  6, batch     0 | loss: 3.2975714Losses:  2.8535287380218506 0.5120998024940491 0.8448998332023621
MemoryTrain:  epoch  6, batch     1 | loss: 2.8535287Losses:  2.5786900520324707 -0.0 1.0126174688339233
MemoryTrain:  epoch  6, batch     2 | loss: 2.5786901Losses:  2.818876266479492 0.6979697346687317 0.7823691368103027
MemoryTrain:  epoch  6, batch     3 | loss: 2.8188763Losses:  2.579268455505371 -0.0 0.895018458366394
MemoryTrain:  epoch  6, batch     4 | loss: 2.5792685Losses:  2.686516284942627 0.24389691650867462 0.9806404709815979
MemoryTrain:  epoch  6, batch     5 | loss: 2.6865163Losses:  2.720167398452759 0.5076777935028076 0.7951685190200806
MemoryTrain:  epoch  6, batch     6 | loss: 2.7201674Losses:  2.703662872314453 0.5005838871002197 0.8973381519317627
MemoryTrain:  epoch  6, batch     7 | loss: 2.7036629Losses:  2.6394834518432617 0.23371362686157227 0.9561504125595093
MemoryTrain:  epoch  6, batch     8 | loss: 2.6394835Losses:  2.3292455673217773 -0.0 1.0466020107269287
MemoryTrain:  epoch  6, batch     9 | loss: 2.3292456Losses:  2.4831485748291016 0.24417568743228912 0.8215875029563904
MemoryTrain:  epoch  6, batch    10 | loss: 2.4831486Losses:  2.4067704677581787 -0.0 1.0611937046051025
MemoryTrain:  epoch  6, batch    11 | loss: 2.4067705Losses:  2.5388598442077637 -0.0 1.0710475444793701
MemoryTrain:  epoch  6, batch    12 | loss: 2.5388598Losses:  1.4263709783554077 -0.0 0.17935529351234436
MemoryTrain:  epoch  6, batch    13 | loss: 1.4263710Losses:  2.596191883087158 0.2636902332305908 0.8912897109985352
MemoryTrain:  epoch  7, batch     0 | loss: 2.5961919Losses:  2.5293405055999756 0.48323583602905273 0.7994815707206726
MemoryTrain:  epoch  7, batch     1 | loss: 2.5293405Losses:  2.6122796535491943 0.4838603436946869 0.8347565531730652
MemoryTrain:  epoch  7, batch     2 | loss: 2.6122797Losses:  3.1489758491516113 0.7667300701141357 0.7275618314743042
MemoryTrain:  epoch  7, batch     3 | loss: 3.1489758Losses:  2.7408814430236816 0.5188286304473877 0.904667317867279
MemoryTrain:  epoch  7, batch     4 | loss: 2.7408814Losses:  2.530266284942627 0.255988746881485 0.9225987792015076
MemoryTrain:  epoch  7, batch     5 | loss: 2.5302663Losses:  2.401676654815674 -0.0 1.0084705352783203
MemoryTrain:  epoch  7, batch     6 | loss: 2.4016767Losses:  2.581103801727295 0.24677571654319763 0.9441744089126587
MemoryTrain:  epoch  7, batch     7 | loss: 2.5811038Losses:  2.819387912750244 0.25604838132858276 0.9880977272987366
MemoryTrain:  epoch  7, batch     8 | loss: 2.8193879Losses:  2.3927011489868164 0.2605046033859253 0.8278550505638123
MemoryTrain:  epoch  7, batch     9 | loss: 2.3927011Losses:  3.2201409339904785 1.0407946109771729 0.7377328872680664
MemoryTrain:  epoch  7, batch    10 | loss: 3.2201409Losses:  2.6368017196655273 0.24598345160484314 1.022965431213379
MemoryTrain:  epoch  7, batch    11 | loss: 2.6368017Losses:  2.851860284805298 0.2592582404613495 0.9633083939552307
MemoryTrain:  epoch  7, batch    12 | loss: 2.8518603Losses:  1.4685959815979004 -0.0 0.15738517045974731
MemoryTrain:  epoch  7, batch    13 | loss: 1.4685960Losses:  2.5590171813964844 0.23875629901885986 1.0106879472732544
MemoryTrain:  epoch  8, batch     0 | loss: 2.5590172Losses:  2.629368543624878 0.24892470240592957 0.9902707934379578
MemoryTrain:  epoch  8, batch     1 | loss: 2.6293685Losses:  2.264436721801758 -0.0 0.9665086269378662
MemoryTrain:  epoch  8, batch     2 | loss: 2.2644367Losses:  2.6337904930114746 0.2197381556034088 0.8882479667663574
MemoryTrain:  epoch  8, batch     3 | loss: 2.6337905Losses:  2.8898541927337646 0.7770474553108215 0.7917139530181885
MemoryTrain:  epoch  8, batch     4 | loss: 2.8898542Losses:  2.3792777061462402 -0.0 1.0250494480133057
MemoryTrain:  epoch  8, batch     5 | loss: 2.3792777Losses:  2.7156991958618164 0.48816096782684326 0.9358201026916504
MemoryTrain:  epoch  8, batch     6 | loss: 2.7156992Losses:  2.8058371543884277 0.5110411643981934 0.8953856229782104
MemoryTrain:  epoch  8, batch     7 | loss: 2.8058372Losses:  2.7663395404815674 0.5168076157569885 0.9701326489448547
MemoryTrain:  epoch  8, batch     8 | loss: 2.7663395Losses:  3.2064898014068604 0.800571084022522 0.95802903175354
MemoryTrain:  epoch  8, batch     9 | loss: 3.2064898Losses:  2.381859302520752 -0.0 1.0311217308044434
MemoryTrain:  epoch  8, batch    10 | loss: 2.3818593Losses:  2.8544678688049316 0.5434331893920898 0.9203672409057617
MemoryTrain:  epoch  8, batch    11 | loss: 2.8544679Losses:  2.27675724029541 -0.0 0.9035574197769165
MemoryTrain:  epoch  8, batch    12 | loss: 2.2767572Losses:  1.3130854368209839 -0.0 0.11202803999185562
MemoryTrain:  epoch  8, batch    13 | loss: 1.3130854Losses:  2.6271629333496094 0.28822433948516846 1.0441491603851318
MemoryTrain:  epoch  9, batch     0 | loss: 2.6271629Losses:  2.226921796798706 -0.0 0.9546118378639221
MemoryTrain:  epoch  9, batch     1 | loss: 2.2269218Losses:  2.2158992290496826 -0.0 0.9583878517150879
MemoryTrain:  epoch  9, batch     2 | loss: 2.2158992Losses:  2.3112974166870117 -0.0 0.9582206010818481
MemoryTrain:  epoch  9, batch     3 | loss: 2.3112974Losses:  2.384828805923462 -0.0 1.0791181325912476
MemoryTrain:  epoch  9, batch     4 | loss: 2.3848288Losses:  2.2610604763031006 -0.0 0.9575760364532471
MemoryTrain:  epoch  9, batch     5 | loss: 2.2610605Losses:  2.4310190677642822 0.24408847093582153 0.8518771529197693
MemoryTrain:  epoch  9, batch     6 | loss: 2.4310191Losses:  2.267019271850586 -0.0 1.0141140222549438
MemoryTrain:  epoch  9, batch     7 | loss: 2.2670193Losses:  3.1674957275390625 0.7373626232147217 0.95063316822052
MemoryTrain:  epoch  9, batch     8 | loss: 3.1674957Losses:  2.278005599975586 -0.0 1.008254885673523
MemoryTrain:  epoch  9, batch     9 | loss: 2.2780056Losses:  2.4349277019500732 0.2514648735523224 0.9153355360031128
MemoryTrain:  epoch  9, batch    10 | loss: 2.4349277Losses:  2.4852988719940186 0.23595021665096283 0.9479847550392151
MemoryTrain:  epoch  9, batch    11 | loss: 2.4852989Losses:  2.357698917388916 -0.0 1.006119966506958
MemoryTrain:  epoch  9, batch    12 | loss: 2.3576989Losses:  1.4737058877944946 -0.0 0.1875838339328766
MemoryTrain:  epoch  9, batch    13 | loss: 1.4737059
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 56.25%,  total acc: 62.50%   [EVAL] batch:    2 | acc: 43.75%,  total acc: 56.25%   [EVAL] batch:    3 | acc: 37.50%,  total acc: 51.56%   [EVAL] batch:    4 | acc: 50.00%,  total acc: 51.25%   [EVAL] batch:    5 | acc: 56.25%,  total acc: 52.08%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 58.04%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 62.50%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 65.28%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 66.25%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 67.61%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 69.79%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 70.19%   [EVAL] batch:   13 | acc: 56.25%,  total acc: 69.20%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 69.17%   [EVAL] batch:   15 | acc: 75.00%,  total acc: 69.53%   [EVAL] batch:   16 | acc: 81.25%,  total acc: 70.22%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 69.79%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 69.74%   [EVAL] batch:   19 | acc: 62.50%,  total acc: 69.38%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 70.24%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 70.74%   [EVAL] batch:   22 | acc: 68.75%,  total acc: 70.65%   [EVAL] batch:   23 | acc: 68.75%,  total acc: 70.57%   [EVAL] batch:   24 | acc: 62.50%,  total acc: 70.25%   [EVAL] batch:   25 | acc: 25.00%,  total acc: 68.51%   [EVAL] batch:   26 | acc: 68.75%,  total acc: 68.52%   [EVAL] batch:   27 | acc: 31.25%,  total acc: 67.19%   [EVAL] batch:   28 | acc: 43.75%,  total acc: 66.38%   [EVAL] batch:   29 | acc: 43.75%,  total acc: 65.62%   [EVAL] batch:   30 | acc: 31.25%,  total acc: 64.52%   [EVAL] batch:   31 | acc: 62.50%,  total acc: 64.45%   [EVAL] batch:   32 | acc: 50.00%,  total acc: 64.02%   [EVAL] batch:   33 | acc: 50.00%,  total acc: 63.60%   [EVAL] batch:   34 | acc: 50.00%,  total acc: 63.21%   [EVAL] batch:   35 | acc: 50.00%,  total acc: 62.85%   [EVAL] batch:   36 | acc: 56.25%,  total acc: 62.67%   [EVAL] batch:   37 | acc: 56.25%,  total acc: 62.50%   [EVAL] batch:   38 | acc: 68.75%,  total acc: 62.66%   [EVAL] batch:   39 | acc: 81.25%,  total acc: 63.12%   [EVAL] batch:   40 | acc: 87.50%,  total acc: 63.72%   [EVAL] batch:   41 | acc: 81.25%,  total acc: 64.14%   [EVAL] batch:   42 | acc: 87.50%,  total acc: 64.68%   [EVAL] batch:   43 | acc: 62.50%,  total acc: 64.63%   [EVAL] batch:   44 | acc: 25.00%,  total acc: 63.75%   [EVAL] batch:   45 | acc: 31.25%,  total acc: 63.04%   [EVAL] batch:   46 | acc: 25.00%,  total acc: 62.23%   [EVAL] batch:   47 | acc: 37.50%,  total acc: 61.72%   [EVAL] batch:   48 | acc: 31.25%,  total acc: 61.10%   [EVAL] batch:   49 | acc: 50.00%,  total acc: 60.88%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 61.64%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 62.38%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 63.09%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 63.77%   [EVAL] batch:   54 | acc: 100.00%,  total acc: 64.43%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 65.07%   [EVAL] batch:   56 | acc: 56.25%,  total acc: 64.91%   [EVAL] batch:   57 | acc: 50.00%,  total acc: 64.66%   [EVAL] batch:   58 | acc: 37.50%,  total acc: 64.19%   [EVAL] batch:   59 | acc: 68.75%,  total acc: 64.27%   [EVAL] batch:   60 | acc: 56.25%,  total acc: 64.14%   [EVAL] batch:   61 | acc: 62.50%,  total acc: 64.11%   [EVAL] batch:   62 | acc: 37.50%,  total acc: 63.69%   
[EVAL] batch:    0 | acc: 56.25%,  total acc: 56.25%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 71.88%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 75.00%   [EVAL] batch:    3 | acc: 56.25%,  total acc: 70.31%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 70.00%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 70.83%   [EVAL] batch:    6 | acc: 56.25%,  total acc: 68.75%   [EVAL] batch:    7 | acc: 62.50%,  total acc: 67.97%   [EVAL] batch:    8 | acc: 62.50%,  total acc: 67.36%   [EVAL] batch:    9 | acc: 56.25%,  total acc: 66.25%   [EVAL] batch:   10 | acc: 37.50%,  total acc: 63.64%   [EVAL] batch:   11 | acc: 37.50%,  total acc: 61.46%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 62.50%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 64.73%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 66.67%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 68.75%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 70.22%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 71.53%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 73.03%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 73.44%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 74.40%   [EVAL] batch:   21 | acc: 68.75%,  total acc: 74.15%   [EVAL] batch:   22 | acc: 68.75%,  total acc: 73.91%   [EVAL] batch:   23 | acc: 81.25%,  total acc: 74.22%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 74.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 75.72%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 76.62%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 77.46%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 78.02%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 78.75%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 79.44%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 80.08%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 80.68%   [EVAL] batch:   33 | acc: 62.50%,  total acc: 80.15%   [EVAL] batch:   34 | acc: 68.75%,  total acc: 79.82%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 80.21%   [EVAL] batch:   36 | acc: 87.50%,  total acc: 80.41%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 80.76%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:   39 | acc: 93.75%,  total acc: 81.56%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 82.01%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 82.29%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 82.56%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 82.53%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 82.64%   [EVAL] batch:   45 | acc: 68.75%,  total acc: 82.34%   [EVAL] batch:   46 | acc: 75.00%,  total acc: 82.18%   [EVAL] batch:   47 | acc: 75.00%,  total acc: 82.03%   [EVAL] batch:   48 | acc: 87.50%,  total acc: 82.14%   [EVAL] batch:   49 | acc: 68.75%,  total acc: 81.88%   [EVAL] batch:   50 | acc: 56.25%,  total acc: 81.37%   [EVAL] batch:   51 | acc: 62.50%,  total acc: 81.01%   [EVAL] batch:   52 | acc: 62.50%,  total acc: 80.66%   [EVAL] batch:   53 | acc: 50.00%,  total acc: 80.09%   [EVAL] batch:   54 | acc: 31.25%,  total acc: 79.20%   [EVAL] batch:   55 | acc: 56.25%,  total acc: 78.79%   [EVAL] batch:   56 | acc: 81.25%,  total acc: 78.84%   [EVAL] batch:   57 | acc: 75.00%,  total acc: 78.77%   [EVAL] batch:   58 | acc: 68.75%,  total acc: 78.60%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 78.85%   [EVAL] batch:   60 | acc: 75.00%,  total acc: 78.79%   [EVAL] batch:   61 | acc: 75.00%,  total acc: 78.73%   [EVAL] batch:   62 | acc: 81.25%,  total acc: 78.77%   [EVAL] batch:   63 | acc: 93.75%,  total acc: 79.00%   [EVAL] batch:   64 | acc: 87.50%,  total acc: 79.13%   [EVAL] batch:   65 | acc: 100.00%,  total acc: 79.45%   [EVAL] batch:   66 | acc: 81.25%,  total acc: 79.48%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 79.78%   [EVAL] batch:   68 | acc: 93.75%,  total acc: 79.98%   [EVAL] batch:   69 | acc: 75.00%,  total acc: 79.91%   [EVAL] batch:   70 | acc: 75.00%,  total acc: 79.84%   [EVAL] batch:   71 | acc: 87.50%,  total acc: 79.95%   [EVAL] batch:   72 | acc: 87.50%,  total acc: 80.05%   [EVAL] batch:   73 | acc: 62.50%,  total acc: 79.81%   [EVAL] batch:   74 | acc: 81.25%,  total acc: 79.83%   [EVAL] batch:   75 | acc: 75.00%,  total acc: 79.77%   [EVAL] batch:   76 | acc: 75.00%,  total acc: 79.71%   [EVAL] batch:   77 | acc: 75.00%,  total acc: 79.65%   [EVAL] batch:   78 | acc: 50.00%,  total acc: 79.27%   [EVAL] batch:   79 | acc: 62.50%,  total acc: 79.06%   [EVAL] batch:   80 | acc: 62.50%,  total acc: 78.86%   [EVAL] batch:   81 | acc: 50.00%,  total acc: 78.51%   [EVAL] batch:   82 | acc: 25.00%,  total acc: 77.86%   [EVAL] batch:   83 | acc: 37.50%,  total acc: 77.38%   [EVAL] batch:   84 | acc: 37.50%,  total acc: 76.91%   [EVAL] batch:   85 | acc: 43.75%,  total acc: 76.53%   [EVAL] batch:   86 | acc: 25.00%,  total acc: 75.93%   [EVAL] batch:   87 | acc: 62.50%,  total acc: 75.78%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 76.05%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 76.32%   [EVAL] batch:   90 | acc: 93.75%,  total acc: 76.51%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 76.77%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 77.02%   [EVAL] batch:   93 | acc: 93.75%,  total acc: 77.19%   [EVAL] batch:   94 | acc: 56.25%,  total acc: 76.97%   [EVAL] batch:   95 | acc: 81.25%,  total acc: 77.02%   [EVAL] batch:   96 | acc: 68.75%,  total acc: 76.93%   [EVAL] batch:   97 | acc: 62.50%,  total acc: 76.79%   [EVAL] batch:   98 | acc: 75.00%,  total acc: 76.77%   [EVAL] batch:   99 | acc: 43.75%,  total acc: 76.44%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 76.67%   [EVAL] batch:  101 | acc: 100.00%,  total acc: 76.90%   [EVAL] batch:  102 | acc: 100.00%,  total acc: 77.12%   [EVAL] batch:  103 | acc: 100.00%,  total acc: 77.34%   [EVAL] batch:  104 | acc: 100.00%,  total acc: 77.56%   [EVAL] batch:  105 | acc: 93.75%,  total acc: 77.71%   [EVAL] batch:  106 | acc: 100.00%,  total acc: 77.92%   [EVAL] batch:  107 | acc: 100.00%,  total acc: 78.12%   [EVAL] batch:  108 | acc: 100.00%,  total acc: 78.33%   [EVAL] batch:  109 | acc: 100.00%,  total acc: 78.52%   [EVAL] batch:  110 | acc: 100.00%,  total acc: 78.72%   [EVAL] batch:  111 | acc: 100.00%,  total acc: 78.91%   [EVAL] batch:  112 | acc: 75.00%,  total acc: 78.87%   [EVAL] batch:  113 | acc: 56.25%,  total acc: 78.67%   [EVAL] batch:  114 | acc: 93.75%,  total acc: 78.80%   [EVAL] batch:  115 | acc: 87.50%,  total acc: 78.88%   [EVAL] batch:  116 | acc: 75.00%,  total acc: 78.85%   [EVAL] batch:  117 | acc: 87.50%,  total acc: 78.92%   [EVAL] batch:  118 | acc: 75.00%,  total acc: 78.89%   [EVAL] batch:  119 | acc: 87.50%,  total acc: 78.96%   [EVAL] batch:  120 | acc: 62.50%,  total acc: 78.82%   [EVAL] batch:  121 | acc: 87.50%,  total acc: 78.89%   [EVAL] batch:  122 | acc: 87.50%,  total acc: 78.96%   [EVAL] batch:  123 | acc: 75.00%,  total acc: 78.93%   [EVAL] batch:  124 | acc: 81.25%,  total acc: 78.95%   [EVAL] batch:  125 | acc: 6.25%,  total acc: 78.37%   [EVAL] batch:  126 | acc: 12.50%,  total acc: 77.85%   [EVAL] batch:  127 | acc: 18.75%,  total acc: 77.39%   [EVAL] batch:  128 | acc: 0.00%,  total acc: 76.79%   [EVAL] batch:  129 | acc: 25.00%,  total acc: 76.39%   [EVAL] batch:  130 | acc: 18.75%,  total acc: 75.95%   [EVAL] batch:  131 | acc: 62.50%,  total acc: 75.85%   [EVAL] batch:  132 | acc: 68.75%,  total acc: 75.80%   [EVAL] batch:  133 | acc: 81.25%,  total acc: 75.84%   [EVAL] batch:  134 | acc: 68.75%,  total acc: 75.79%   [EVAL] batch:  135 | acc: 62.50%,  total acc: 75.69%   [EVAL] batch:  136 | acc: 50.00%,  total acc: 75.50%   [EVAL] batch:  137 | acc: 87.50%,  total acc: 75.59%   [EVAL] batch:  138 | acc: 75.00%,  total acc: 75.58%   [EVAL] batch:  139 | acc: 81.25%,  total acc: 75.62%   [EVAL] batch:  140 | acc: 100.00%,  total acc: 75.80%   [EVAL] batch:  141 | acc: 87.50%,  total acc: 75.88%   [EVAL] batch:  142 | acc: 75.00%,  total acc: 75.87%   [EVAL] batch:  143 | acc: 56.25%,  total acc: 75.74%   [EVAL] batch:  144 | acc: 25.00%,  total acc: 75.39%   [EVAL] batch:  145 | acc: 25.00%,  total acc: 75.04%   [EVAL] batch:  146 | acc: 37.50%,  total acc: 74.79%   [EVAL] batch:  147 | acc: 12.50%,  total acc: 74.37%   [EVAL] batch:  148 | acc: 12.50%,  total acc: 73.95%   [EVAL] batch:  149 | acc: 25.00%,  total acc: 73.62%   [EVAL] batch:  150 | acc: 6.25%,  total acc: 73.18%   [EVAL] batch:  151 | acc: 6.25%,  total acc: 72.74%   [EVAL] batch:  152 | acc: 6.25%,  total acc: 72.30%   [EVAL] batch:  153 | acc: 6.25%,  total acc: 71.88%   [EVAL] batch:  154 | acc: 12.50%,  total acc: 71.49%   [EVAL] batch:  155 | acc: 0.00%,  total acc: 71.03%   [EVAL] batch:  156 | acc: 43.75%,  total acc: 70.86%   [EVAL] batch:  157 | acc: 100.00%,  total acc: 71.04%   [EVAL] batch:  158 | acc: 93.75%,  total acc: 71.19%   [EVAL] batch:  159 | acc: 87.50%,  total acc: 71.29%   [EVAL] batch:  160 | acc: 87.50%,  total acc: 71.39%   [EVAL] batch:  161 | acc: 75.00%,  total acc: 71.41%   [EVAL] batch:  162 | acc: 100.00%,  total acc: 71.59%   [EVAL] batch:  163 | acc: 87.50%,  total acc: 71.68%   [EVAL] batch:  164 | acc: 100.00%,  total acc: 71.86%   [EVAL] batch:  165 | acc: 93.75%,  total acc: 71.99%   [EVAL] batch:  166 | acc: 93.75%,  total acc: 72.12%   [EVAL] batch:  167 | acc: 100.00%,  total acc: 72.28%   [EVAL] batch:  168 | acc: 93.75%,  total acc: 72.41%   [EVAL] batch:  169 | acc: 31.25%,  total acc: 72.17%   [EVAL] batch:  170 | acc: 43.75%,  total acc: 72.00%   [EVAL] batch:  171 | acc: 12.50%,  total acc: 71.66%   [EVAL] batch:  172 | acc: 31.25%,  total acc: 71.42%   [EVAL] batch:  173 | acc: 43.75%,  total acc: 71.26%   [EVAL] batch:  174 | acc: 37.50%,  total acc: 71.07%   [EVAL] batch:  175 | acc: 31.25%,  total acc: 70.85%   [EVAL] batch:  176 | acc: 62.50%,  total acc: 70.80%   [EVAL] batch:  177 | acc: 37.50%,  total acc: 70.61%   [EVAL] batch:  178 | acc: 50.00%,  total acc: 70.50%   [EVAL] batch:  179 | acc: 43.75%,  total acc: 70.35%   [EVAL] batch:  180 | acc: 56.25%,  total acc: 70.27%   [EVAL] batch:  181 | acc: 18.75%,  total acc: 69.99%   [EVAL] batch:  182 | acc: 25.00%,  total acc: 69.74%   [EVAL] batch:  183 | acc: 43.75%,  total acc: 69.60%   [EVAL] batch:  184 | acc: 31.25%,  total acc: 69.39%   [EVAL] batch:  185 | acc: 31.25%,  total acc: 69.19%   [EVAL] batch:  186 | acc: 18.75%,  total acc: 68.92%   [EVAL] batch:  187 | acc: 18.75%,  total acc: 68.65%   [EVAL] batch:  188 | acc: 31.25%,  total acc: 68.45%   [EVAL] batch:  189 | acc: 25.00%,  total acc: 68.22%   [EVAL] batch:  190 | acc: 37.50%,  total acc: 68.06%   [EVAL] batch:  191 | acc: 37.50%,  total acc: 67.90%   [EVAL] batch:  192 | acc: 37.50%,  total acc: 67.75%   [EVAL] batch:  193 | acc: 37.50%,  total acc: 67.59%   [EVAL] batch:  194 | acc: 62.50%,  total acc: 67.56%   [EVAL] batch:  195 | acc: 68.75%,  total acc: 67.57%   [EVAL] batch:  196 | acc: 68.75%,  total acc: 67.58%   [EVAL] batch:  197 | acc: 62.50%,  total acc: 67.55%   [EVAL] batch:  198 | acc: 25.00%,  total acc: 67.34%   [EVAL] batch:  199 | acc: 100.00%,  total acc: 67.50%   [EVAL] batch:  200 | acc: 50.00%,  total acc: 67.41%   [EVAL] batch:  201 | acc: 56.25%,  total acc: 67.36%   [EVAL] batch:  202 | acc: 56.25%,  total acc: 67.30%   [EVAL] batch:  203 | acc: 50.00%,  total acc: 67.22%   [EVAL] batch:  204 | acc: 50.00%,  total acc: 67.13%   [EVAL] batch:  205 | acc: 81.25%,  total acc: 67.20%   [EVAL] batch:  206 | acc: 87.50%,  total acc: 67.30%   [EVAL] batch:  207 | acc: 93.75%,  total acc: 67.43%   [EVAL] batch:  208 | acc: 100.00%,  total acc: 67.58%   [EVAL] batch:  209 | acc: 100.00%,  total acc: 67.74%   [EVAL] batch:  210 | acc: 93.75%,  total acc: 67.86%   [EVAL] batch:  211 | acc: 100.00%,  total acc: 68.01%   [EVAL] batch:  212 | acc: 62.50%,  total acc: 67.99%   [EVAL] batch:  213 | acc: 37.50%,  total acc: 67.84%   [EVAL] batch:  214 | acc: 43.75%,  total acc: 67.73%   [EVAL] batch:  215 | acc: 62.50%,  total acc: 67.71%   [EVAL] batch:  216 | acc: 18.75%,  total acc: 67.48%   [EVAL] batch:  217 | acc: 43.75%,  total acc: 67.37%   [EVAL] batch:  218 | acc: 50.00%,  total acc: 67.29%   [EVAL] batch:  219 | acc: 43.75%,  total acc: 67.19%   [EVAL] batch:  220 | acc: 18.75%,  total acc: 66.97%   [EVAL] batch:  221 | acc: 25.00%,  total acc: 66.78%   [EVAL] batch:  222 | acc: 12.50%,  total acc: 66.54%   [EVAL] batch:  223 | acc: 12.50%,  total acc: 66.29%   [EVAL] batch:  224 | acc: 31.25%,  total acc: 66.14%   [EVAL] batch:  225 | acc: 75.00%,  total acc: 66.18%   [EVAL] batch:  226 | acc: 81.25%,  total acc: 66.24%   [EVAL] batch:  227 | acc: 75.00%,  total acc: 66.28%   [EVAL] batch:  228 | acc: 87.50%,  total acc: 66.38%   [EVAL] batch:  229 | acc: 68.75%,  total acc: 66.39%   [EVAL] batch:  230 | acc: 68.75%,  total acc: 66.40%   [EVAL] batch:  231 | acc: 68.75%,  total acc: 66.41%   [EVAL] batch:  232 | acc: 56.25%,  total acc: 66.36%   [EVAL] batch:  233 | acc: 68.75%,  total acc: 66.37%   [EVAL] batch:  234 | acc: 68.75%,  total acc: 66.38%   [EVAL] batch:  235 | acc: 50.00%,  total acc: 66.31%   [EVAL] batch:  236 | acc: 75.00%,  total acc: 66.35%   [EVAL] batch:  237 | acc: 93.75%,  total acc: 66.47%   [EVAL] batch:  238 | acc: 100.00%,  total acc: 66.61%   [EVAL] batch:  239 | acc: 100.00%,  total acc: 66.74%   [EVAL] batch:  240 | acc: 100.00%,  total acc: 66.88%   [EVAL] batch:  241 | acc: 100.00%,  total acc: 67.02%   [EVAL] batch:  242 | acc: 100.00%,  total acc: 67.16%   [EVAL] batch:  243 | acc: 100.00%,  total acc: 67.29%   [EVAL] batch:  244 | acc: 100.00%,  total acc: 67.42%   [EVAL] batch:  245 | acc: 87.50%,  total acc: 67.51%   [EVAL] batch:  246 | acc: 93.75%,  total acc: 67.61%   [EVAL] batch:  247 | acc: 100.00%,  total acc: 67.74%   [EVAL] batch:  248 | acc: 100.00%,  total acc: 67.87%   [EVAL] batch:  249 | acc: 100.00%,  total acc: 68.00%   [EVAL] batch:  250 | acc: 75.00%,  total acc: 68.03%   [EVAL] batch:  251 | acc: 62.50%,  total acc: 68.01%   [EVAL] batch:  252 | acc: 68.75%,  total acc: 68.01%   [EVAL] batch:  253 | acc: 62.50%,  total acc: 67.99%   [EVAL] batch:  254 | acc: 75.00%,  total acc: 68.01%   [EVAL] batch:  255 | acc: 75.00%,  total acc: 68.04%   [EVAL] batch:  256 | acc: 75.00%,  total acc: 68.07%   [EVAL] batch:  257 | acc: 75.00%,  total acc: 68.10%   [EVAL] batch:  258 | acc: 68.75%,  total acc: 68.10%   [EVAL] batch:  259 | acc: 56.25%,  total acc: 68.05%   [EVAL] batch:  260 | acc: 62.50%,  total acc: 68.03%   [EVAL] batch:  261 | acc: 62.50%,  total acc: 68.01%   [EVAL] batch:  262 | acc: 100.00%,  total acc: 68.13%   [EVAL] batch:  263 | acc: 68.75%,  total acc: 68.13%   [EVAL] batch:  264 | acc: 75.00%,  total acc: 68.16%   [EVAL] batch:  265 | acc: 62.50%,  total acc: 68.14%   [EVAL] batch:  266 | acc: 81.25%,  total acc: 68.19%   [EVAL] batch:  267 | acc: 75.00%,  total acc: 68.21%   [EVAL] batch:  268 | acc: 81.25%,  total acc: 68.26%   [EVAL] batch:  269 | acc: 43.75%,  total acc: 68.17%   [EVAL] batch:  270 | acc: 50.00%,  total acc: 68.10%   [EVAL] batch:  271 | acc: 68.75%,  total acc: 68.11%   [EVAL] batch:  272 | acc: 81.25%,  total acc: 68.15%   [EVAL] batch:  273 | acc: 75.00%,  total acc: 68.18%   [EVAL] batch:  274 | acc: 56.25%,  total acc: 68.14%   [EVAL] batch:  275 | acc: 100.00%,  total acc: 68.25%   [EVAL] batch:  276 | acc: 100.00%,  total acc: 68.37%   [EVAL] batch:  277 | acc: 100.00%,  total acc: 68.48%   [EVAL] batch:  278 | acc: 100.00%,  total acc: 68.59%   [EVAL] batch:  279 | acc: 100.00%,  total acc: 68.71%   [EVAL] batch:  280 | acc: 100.00%,  total acc: 68.82%   [EVAL] batch:  281 | acc: 100.00%,  total acc: 68.93%   [EVAL] batch:  282 | acc: 100.00%,  total acc: 69.04%   [EVAL] batch:  283 | acc: 100.00%,  total acc: 69.15%   [EVAL] batch:  284 | acc: 100.00%,  total acc: 69.25%   [EVAL] batch:  285 | acc: 100.00%,  total acc: 69.36%   [EVAL] batch:  286 | acc: 100.00%,  total acc: 69.47%   [EVAL] batch:  287 | acc: 87.50%,  total acc: 69.53%   [EVAL] batch:  288 | acc: 56.25%,  total acc: 69.49%   [EVAL] batch:  289 | acc: 62.50%,  total acc: 69.46%   [EVAL] batch:  290 | acc: 62.50%,  total acc: 69.44%   [EVAL] batch:  291 | acc: 81.25%,  total acc: 69.48%   [EVAL] batch:  292 | acc: 81.25%,  total acc: 69.52%   [EVAL] batch:  293 | acc: 62.50%,  total acc: 69.49%   [EVAL] batch:  294 | acc: 31.25%,  total acc: 69.36%   [EVAL] batch:  295 | acc: 25.00%,  total acc: 69.21%   [EVAL] batch:  296 | acc: 25.00%,  total acc: 69.07%   [EVAL] batch:  297 | acc: 43.75%,  total acc: 68.98%   [EVAL] batch:  298 | acc: 37.50%,  total acc: 68.88%   [EVAL] batch:  299 | acc: 25.00%,  total acc: 68.73%   [EVAL] batch:  300 | acc: 93.75%,  total acc: 68.81%   [EVAL] batch:  301 | acc: 93.75%,  total acc: 68.89%   [EVAL] batch:  302 | acc: 93.75%,  total acc: 68.98%   [EVAL] batch:  303 | acc: 81.25%,  total acc: 69.02%   [EVAL] batch:  304 | acc: 93.75%,  total acc: 69.10%   [EVAL] batch:  305 | acc: 81.25%,  total acc: 69.14%   [EVAL] batch:  306 | acc: 31.25%,  total acc: 69.01%   [EVAL] batch:  307 | acc: 12.50%,  total acc: 68.83%   [EVAL] batch:  308 | acc: 6.25%,  total acc: 68.63%   [EVAL] batch:  309 | acc: 6.25%,  total acc: 68.43%   [EVAL] batch:  310 | acc: 6.25%,  total acc: 68.23%   [EVAL] batch:  311 | acc: 18.75%,  total acc: 68.07%   [EVAL] batch:  312 | acc: 0.00%,  total acc: 67.85%   [EVAL] batch:  313 | acc: 18.75%,  total acc: 67.70%   [EVAL] batch:  314 | acc: 0.00%,  total acc: 67.48%   [EVAL] batch:  315 | acc: 6.25%,  total acc: 67.29%   [EVAL] batch:  316 | acc: 12.50%,  total acc: 67.11%   [EVAL] batch:  317 | acc: 18.75%,  total acc: 66.96%   [EVAL] batch:  318 | acc: 31.25%,  total acc: 66.85%   [EVAL] batch:  319 | acc: 81.25%,  total acc: 66.89%   [EVAL] batch:  320 | acc: 56.25%,  total acc: 66.86%   [EVAL] batch:  321 | acc: 81.25%,  total acc: 66.91%   [EVAL] batch:  322 | acc: 87.50%,  total acc: 66.97%   [EVAL] batch:  323 | acc: 75.00%,  total acc: 66.99%   [EVAL] batch:  324 | acc: 75.00%,  total acc: 67.02%   [EVAL] batch:  325 | acc: 56.25%,  total acc: 66.99%   [EVAL] batch:  326 | acc: 31.25%,  total acc: 66.88%   [EVAL] batch:  327 | acc: 56.25%,  total acc: 66.84%   [EVAL] batch:  328 | acc: 50.00%,  total acc: 66.79%   [EVAL] batch:  329 | acc: 62.50%,  total acc: 66.78%   [EVAL] batch:  330 | acc: 43.75%,  total acc: 66.71%   [EVAL] batch:  331 | acc: 87.50%,  total acc: 66.77%   [EVAL] batch:  332 | acc: 100.00%,  total acc: 66.87%   [EVAL] batch:  333 | acc: 100.00%,  total acc: 66.97%   [EVAL] batch:  334 | acc: 93.75%,  total acc: 67.05%   [EVAL] batch:  335 | acc: 100.00%,  total acc: 67.15%   [EVAL] batch:  336 | acc: 100.00%,  total acc: 67.25%   [EVAL] batch:  337 | acc: 81.25%,  total acc: 67.29%   [EVAL] batch:  338 | acc: 100.00%,  total acc: 67.39%   [EVAL] batch:  339 | acc: 100.00%,  total acc: 67.48%   [EVAL] batch:  340 | acc: 93.75%,  total acc: 67.56%   [EVAL] batch:  341 | acc: 93.75%,  total acc: 67.64%   [EVAL] batch:  342 | acc: 100.00%,  total acc: 67.73%   [EVAL] batch:  343 | acc: 87.50%,  total acc: 67.79%   [EVAL] batch:  344 | acc: 100.00%,  total acc: 67.88%   [EVAL] batch:  345 | acc: 100.00%,  total acc: 67.97%   [EVAL] batch:  346 | acc: 93.75%,  total acc: 68.05%   [EVAL] batch:  347 | acc: 93.75%,  total acc: 68.12%   [EVAL] batch:  348 | acc: 100.00%,  total acc: 68.21%   [EVAL] batch:  349 | acc: 100.00%,  total acc: 68.30%   [EVAL] batch:  350 | acc: 56.25%,  total acc: 68.27%   [EVAL] batch:  351 | acc: 43.75%,  total acc: 68.20%   [EVAL] batch:  352 | acc: 31.25%,  total acc: 68.09%   [EVAL] batch:  353 | acc: 31.25%,  total acc: 67.99%   [EVAL] batch:  354 | acc: 37.50%,  total acc: 67.90%   [EVAL] batch:  355 | acc: 43.75%,  total acc: 67.84%   [EVAL] batch:  356 | acc: 81.25%,  total acc: 67.87%   [EVAL] batch:  357 | acc: 87.50%,  total acc: 67.93%   [EVAL] batch:  358 | acc: 93.75%,  total acc: 68.00%   [EVAL] batch:  359 | acc: 87.50%,  total acc: 68.06%   [EVAL] batch:  360 | acc: 56.25%,  total acc: 68.02%   [EVAL] batch:  361 | acc: 56.25%,  total acc: 67.99%   [EVAL] batch:  362 | acc: 62.50%,  total acc: 67.98%   [EVAL] batch:  363 | acc: 68.75%,  total acc: 67.98%   [EVAL] batch:  364 | acc: 50.00%,  total acc: 67.93%   [EVAL] batch:  365 | acc: 50.00%,  total acc: 67.88%   [EVAL] batch:  366 | acc: 62.50%,  total acc: 67.86%   [EVAL] batch:  367 | acc: 43.75%,  total acc: 67.80%   [EVAL] batch:  368 | acc: 37.50%,  total acc: 67.72%   [EVAL] batch:  369 | acc: 100.00%,  total acc: 67.80%   [EVAL] batch:  370 | acc: 87.50%,  total acc: 67.86%   [EVAL] batch:  371 | acc: 62.50%,  total acc: 67.84%   [EVAL] batch:  372 | acc: 81.25%,  total acc: 67.88%   [EVAL] batch:  373 | acc: 87.50%,  total acc: 67.93%   [EVAL] batch:  374 | acc: 87.50%,  total acc: 67.98%   [EVAL] batch:  375 | acc: 68.75%,  total acc: 67.99%   [EVAL] batch:  376 | acc: 56.25%,  total acc: 67.95%   [EVAL] batch:  377 | acc: 43.75%,  total acc: 67.89%   [EVAL] batch:  378 | acc: 37.50%,  total acc: 67.81%   [EVAL] batch:  379 | acc: 50.00%,  total acc: 67.76%   [EVAL] batch:  380 | acc: 56.25%,  total acc: 67.73%   [EVAL] batch:  381 | acc: 93.75%,  total acc: 67.80%   [EVAL] batch:  382 | acc: 93.75%,  total acc: 67.87%   [EVAL] batch:  383 | acc: 87.50%,  total acc: 67.92%   [EVAL] batch:  384 | acc: 75.00%,  total acc: 67.94%   [EVAL] batch:  385 | acc: 81.25%,  total acc: 67.97%   [EVAL] batch:  386 | acc: 93.75%,  total acc: 68.04%   [EVAL] batch:  387 | acc: 75.00%,  total acc: 68.06%   [EVAL] batch:  388 | acc: 56.25%,  total acc: 68.03%   [EVAL] batch:  389 | acc: 68.75%,  total acc: 68.03%   [EVAL] batch:  390 | acc: 75.00%,  total acc: 68.05%   [EVAL] batch:  391 | acc: 81.25%,  total acc: 68.08%   [EVAL] batch:  392 | acc: 62.50%,  total acc: 68.07%   [EVAL] batch:  393 | acc: 68.75%,  total acc: 68.07%   [EVAL] batch:  394 | acc: 62.50%,  total acc: 68.05%   [EVAL] batch:  395 | acc: 87.50%,  total acc: 68.10%   [EVAL] batch:  396 | acc: 81.25%,  total acc: 68.14%   [EVAL] batch:  397 | acc: 68.75%,  total acc: 68.14%   [EVAL] batch:  398 | acc: 68.75%,  total acc: 68.14%   [EVAL] batch:  399 | acc: 62.50%,  total acc: 68.12%   [EVAL] batch:  400 | acc: 25.00%,  total acc: 68.02%   [EVAL] batch:  401 | acc: 68.75%,  total acc: 68.02%   [EVAL] batch:  402 | acc: 31.25%,  total acc: 67.93%   [EVAL] batch:  403 | acc: 43.75%,  total acc: 67.87%   [EVAL] batch:  404 | acc: 43.75%,  total acc: 67.81%   [EVAL] batch:  405 | acc: 31.25%,  total acc: 67.72%   [EVAL] batch:  406 | acc: 62.50%,  total acc: 67.71%   [EVAL] batch:  407 | acc: 50.00%,  total acc: 67.66%   [EVAL] batch:  408 | acc: 50.00%,  total acc: 67.62%   [EVAL] batch:  409 | acc: 50.00%,  total acc: 67.58%   [EVAL] batch:  410 | acc: 50.00%,  total acc: 67.53%   [EVAL] batch:  411 | acc: 56.25%,  total acc: 67.51%   [EVAL] batch:  412 | acc: 56.25%,  total acc: 67.48%   [EVAL] batch:  413 | acc: 68.75%,  total acc: 67.48%   [EVAL] batch:  414 | acc: 81.25%,  total acc: 67.52%   [EVAL] batch:  415 | acc: 87.50%,  total acc: 67.56%   [EVAL] batch:  416 | acc: 81.25%,  total acc: 67.60%   [EVAL] batch:  417 | acc: 87.50%,  total acc: 67.64%   [EVAL] batch:  418 | acc: 62.50%,  total acc: 67.63%   [EVAL] batch:  419 | acc: 25.00%,  total acc: 67.53%   [EVAL] batch:  420 | acc: 31.25%,  total acc: 67.44%   [EVAL] batch:  421 | acc: 25.00%,  total acc: 67.34%   [EVAL] batch:  422 | acc: 37.50%,  total acc: 67.27%   [EVAL] batch:  423 | acc: 31.25%,  total acc: 67.19%   [EVAL] batch:  424 | acc: 50.00%,  total acc: 67.15%   [EVAL] batch:  425 | acc: 100.00%,  total acc: 67.22%   [EVAL] batch:  426 | acc: 100.00%,  total acc: 67.30%   [EVAL] batch:  427 | acc: 100.00%,  total acc: 67.38%   [EVAL] batch:  428 | acc: 100.00%,  total acc: 67.45%   [EVAL] batch:  429 | acc: 100.00%,  total acc: 67.53%   [EVAL] batch:  430 | acc: 100.00%,  total acc: 67.60%   [EVAL] batch:  431 | acc: 56.25%,  total acc: 67.58%   [EVAL] batch:  432 | acc: 50.00%,  total acc: 67.54%   [EVAL] batch:  433 | acc: 37.50%,  total acc: 67.47%   [EVAL] batch:  434 | acc: 68.75%,  total acc: 67.47%   [EVAL] batch:  435 | acc: 56.25%,  total acc: 67.45%   [EVAL] batch:  436 | acc: 62.50%,  total acc: 67.43%   [EVAL] batch:  437 | acc: 37.50%,  total acc: 67.37%   
cur_acc:  ['0.9494', '0.8244', '0.5962', '0.7173', '0.7708', '0.6726', '0.6369']
his_acc:  ['0.9494', '0.8760', '0.7746', '0.7435', '0.7324', '0.7028', '0.6737']
Clustering into  39  clusters
Clusters:  [31  1  9 25 20 28  9 18  1  8 25 23  4  1 27 12  5  6 33  5 20 16 10 34
  3 19 10  1 10  1 29 21  2  9 11 17  1 12 38  7  5 10 18 16 33 32  2 26
 37 26 17 13 26  0  0 13 26  7  8 35 36 11 18 20 24  8 22 28 13 15 22 16
  3  4  3  6 10 22 30 14]
Losses:  9.718750953674316 2.7867462635040283 0.6521786451339722
CurrentTrain: epoch  0, batch     0 | loss: 9.7187510Losses:  11.653099060058594 4.050668716430664 0.6015456318855286
CurrentTrain: epoch  0, batch     1 | loss: 11.6530991Losses:  10.079668045043945 3.3208014965057373 0.6767375469207764
CurrentTrain: epoch  0, batch     2 | loss: 10.0796680Losses:  5.9010772705078125 -0.0 0.10398031026124954
CurrentTrain: epoch  0, batch     3 | loss: 5.9010773Losses:  10.518441200256348 4.718785285949707 0.5596519112586975
CurrentTrain: epoch  1, batch     0 | loss: 10.5184412Losses:  8.811180114746094 3.134263038635254 0.5901381969451904
CurrentTrain: epoch  1, batch     1 | loss: 8.8111801Losses:  10.69707202911377 4.235329627990723 0.6541985869407654
CurrentTrain: epoch  1, batch     2 | loss: 10.6970720Losses:  5.854098796844482 -0.0 0.09508991241455078
CurrentTrain: epoch  1, batch     3 | loss: 5.8540988Losses:  7.180555820465088 3.090972423553467 0.548771321773529
CurrentTrain: epoch  2, batch     0 | loss: 7.1805558Losses:  9.356441497802734 3.5280251502990723 0.5787584781646729
CurrentTrain: epoch  2, batch     1 | loss: 9.3564415Losses:  11.055587768554688 4.7551469802856445 0.5695779323577881
CurrentTrain: epoch  2, batch     2 | loss: 11.0555878Losses:  2.212318181991577 -0.0 0.12484311312437057
CurrentTrain: epoch  2, batch     3 | loss: 2.2123182Losses:  8.901334762573242 3.719993829727173 0.6403999328613281
CurrentTrain: epoch  3, batch     0 | loss: 8.9013348Losses:  8.444743156433105 3.6012444496154785 0.5943552851676941
CurrentTrain: epoch  3, batch     1 | loss: 8.4447432Losses:  12.023551940917969 6.357188701629639 0.5787791609764099
CurrentTrain: epoch  3, batch     2 | loss: 12.0235519Losses:  2.237583875656128 -0.0 0.11155291646718979
CurrentTrain: epoch  3, batch     3 | loss: 2.2375839Losses:  7.885416030883789 2.6760287284851074 0.5655267238616943
CurrentTrain: epoch  4, batch     0 | loss: 7.8854160Losses:  7.056912899017334 2.0776638984680176 0.6370881795883179
CurrentTrain: epoch  4, batch     1 | loss: 7.0569129Losses:  6.996897220611572 2.6858081817626953 0.6380181908607483
CurrentTrain: epoch  4, batch     2 | loss: 6.9968972Losses:  4.730607509613037 -0.0 -0.0
CurrentTrain: epoch  4, batch     3 | loss: 4.7306075Losses:  7.608523368835449 2.499847888946533 0.6560804843902588
CurrentTrain: epoch  5, batch     0 | loss: 7.6085234Losses:  7.27830171585083 3.5311622619628906 0.6360864639282227
CurrentTrain: epoch  5, batch     1 | loss: 7.2783017Losses:  7.471478462219238 2.593151092529297 0.5596926212310791
CurrentTrain: epoch  5, batch     2 | loss: 7.4714785Losses:  4.777658462524414 -0.0 0.1166493222117424
CurrentTrain: epoch  5, batch     3 | loss: 4.7776585Losses:  6.932895183563232 3.338869571685791 0.5676674842834473
CurrentTrain: epoch  6, batch     0 | loss: 6.9328952Losses:  6.774453163146973 2.4268155097961426 0.5558756589889526
CurrentTrain: epoch  6, batch     1 | loss: 6.7744532Losses:  11.228038787841797 6.521017074584961 0.5835283994674683
CurrentTrain: epoch  6, batch     2 | loss: 11.2280388Losses:  8.97288703918457 -0.0 -0.0
CurrentTrain: epoch  6, batch     3 | loss: 8.9728870Losses:  6.5983567237854 2.8640289306640625 0.5617233514785767
CurrentTrain: epoch  7, batch     0 | loss: 6.5983567Losses:  7.14491081237793 2.8641440868377686 0.5877029299736023
CurrentTrain: epoch  7, batch     1 | loss: 7.1449108Losses:  8.202756881713867 3.6493537425994873 0.6531708836555481
CurrentTrain: epoch  7, batch     2 | loss: 8.2027569Losses:  5.001871109008789 -0.0 0.13230924308300018
CurrentTrain: epoch  7, batch     3 | loss: 5.0018711Losses:  8.389810562133789 3.922821521759033 0.4820704162120819
CurrentTrain: epoch  8, batch     0 | loss: 8.3898106Losses:  7.185044288635254 3.474135637283325 0.5590853691101074
CurrentTrain: epoch  8, batch     1 | loss: 7.1850443Losses:  6.7567949295043945 3.272099494934082 0.5943173766136169
CurrentTrain: epoch  8, batch     2 | loss: 6.7567949Losses:  7.014230251312256 -0.0 0.3027978837490082
CurrentTrain: epoch  8, batch     3 | loss: 7.0142303Losses:  8.111851692199707 4.436280250549316 0.4593624174594879
CurrentTrain: epoch  9, batch     0 | loss: 8.1118517Losses:  8.182169914245605 4.064358234405518 0.5637512803077698
CurrentTrain: epoch  9, batch     1 | loss: 8.1821699Losses:  7.927709579467773 4.324894905090332 0.5833929777145386
CurrentTrain: epoch  9, batch     2 | loss: 7.9277096Losses:  3.6366689205169678 -0.0 0.17995095252990723
CurrentTrain: epoch  9, batch     3 | loss: 3.6366689
#### main subject ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "main subject" means primary topic of a work.
Here is an example:
Relation: main subject
Context: auerbach is prominently featured in the documentary film , " the first basket " , about jewish basketball history .
Head Entity: the first basket
Tail Entity: basketball
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "main subject":
Relation: main subject  
Context: the book "sapiens" explores the history of humankind from the stone age to the modern era.  
Head Entity: sapiens  
Tail Entity: humankind  

Relation: main subject  
Context: the exhibition "van gogh: the immersive experience" showcases the life and works of the famous painter.  
Head Entity: van gogh: the immersive experience  
Tail Entity: painter  
#### participating team ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "participating team" means teams that actively takes/took part in an event or process.
Here is an example:
Relation: participating team
Context: he represented romania at euro 2000 , his first major tournament where they beat england 3–2 , and drew 1–1 against germany .
Head Entity: euro 2000
Tail Entity: germany
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "participating team":
Relation: participating team  
Context: The United States women's national soccer team has consistently performed well in international competitions, winning the FIFA Women's World Cup multiple times, including in 2019, where they faced the Netherlands in the final.  
Head Entity: FIFA Women's World Cup  
Tail Entity: Netherlands  

Relation: participating team  
Context: During the 2022 Winter Olympics, the Canadian ice hockey team showcased their skills, ultimately defeating the Russian Olympic Committee in a thrilling final match.  
Head Entity: 2022 Winter Olympics  
Tail Entity: Russian Olympic Committee  
#### has part ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "has part" means part of this subject.
Here is an example:
Relation: has part
Context: among the competing artists was previous eurovision song contest entrant robin juhkental who represented estonia in 2010 as part of the group malcolm lincoln .
Head Entity: malcolm lincoln
Tail Entity: robin juhkental
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "has part":
Relation: has part  
Context: The famous painting "The Last Supper" features several apostles, including Judas Iscariot, who is depicted as part of the group gathered around Jesus.  
Head Entity: The Last Supper  
Tail Entity: Judas Iscariot  

Relation: has part  
Context: The smartphone model Galaxy S21 includes various components, such as the Exynos 2100 processor, which is an integral part of its performance.  
Head Entity: Galaxy S21  
Tail Entity: Exynos 2100 processor  
#### nominated for ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "nominated for" means award nomination received by a person, organisation or creative work.
Here is an example:
Relation: nominated for
Context: he was nominated for the academy award for best picture , along with steven spielberg , ian bryce , mark gordon for the film " saving private ryan " .
Head Entity: ian bryce
Tail Entity: academy award for best picture
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "nominated for":
Relation: nominated for  
Context: The film "The Shape of Water" received multiple nominations, including a nod for best director for Guillermo del Toro at the Academy Awards.  
Head Entity: Guillermo del Toro  
Tail Entity: Academy Awards  

Relation: nominated for  
Context: The popular band was nominated for the Grammy Award for Best New Artist after their debut album topped the charts.  
Head Entity: the popular band  
Tail Entity: Grammy Award for Best New Artist  
#### instance of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "instance of" means that class of which this subject is a particular example and member.
Here is an example:
Relation: instance of
Context: in 2007 , the county municipality created the transit authority called " skyss " that would market public transport while is would be operated by private companies based on public service obligation contracts .
Head Entity: skyss
Tail Entity: transit authority
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "instance of":
Relation: instance of  
Context: The smartphone known as the iPhone was first released by Apple Inc. in 2007, revolutionizing the mobile phone industry and setting new standards for design and functionality.  
Head Entity: iPhone  
Tail Entity: smartphone  

Relation: instance of  
Context: The Great Wall of China is a series of fortifications made of various materials, built to protect the Chinese states and empires from invasions and raids.  
Head Entity: Great Wall of China  
Tail Entity: fortification
#### mountain range ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "mountain range" means range or subrange to which the geographical item belongs.
Here is an example:
Relation: mountain range
Context: abus valley ( ) is an ice - free valley southeast of turnstile ridge at the north end of britannia range .
Head Entity: turnstile ridge
Tail Entity: britannia range
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "mountain range":
Relation: mountain range  
Context: the sierra nevada ( ) is a mountain range in the western united states, primarily in california, that includes the famous yosemite national park and is part of the larger sierra mountain system.  
Head Entity: sierra nevada  
Tail Entity: sierra mountain system  

Relation: mountain range  
Context: the appalachian mountains ( ) are a vast system of mountains in eastern north america, stretching from canada down to alabama, and are known for their rich biodiversity and scenic beauty.  
Head Entity: appalachian mountains  
Tail Entity: north america  
#### screenwriter ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "screenwriter" means person who wrote the script for subject item.
Here is an example:
Relation: screenwriter
Context: anders matthesen has also released several cds with his radio material , in addition to the animated movie " terkel in trouble " , based on one of these .
Head Entity: terkel in trouble
Tail Entity: anders matthesen
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "screenwriter":
Relation: screenwriter  
Context: the acclaimed film "inception" features a complex narrative that was intricately crafted by its talented screenwriter, who is known for his unique storytelling style.  
Head Entity: inception  
Tail Entity: christopher nolan  

Relation: screenwriter  
Context: the beloved animated feature "finding nemo" was brought to life through the creative vision of its screenwriter, whose work has captivated audiences of all ages.  
Head Entity: finding nemo  
Tail Entity: andrew stanton  
#### language of work or name ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "language of work or name" means language associated with this creative work (such as books, shows, songs, broadcasts or websites) or a name.
Here is an example:
Relation: language of work or name
Context: " aamulehti " was founded in 1881 to “ improve the position of the finnish people and the finnish language ” during russia ’s rule over finland .
Head Entity: aamulehti
Tail Entity: finnish language
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "language of work or name":
Relation: language of work or name  
Context: The novel "Cien años de soledad" is celebrated for its rich narrative style and is written in Spanish, reflecting the culture of Latin America.  
Head Entity: Cien años de soledad  
Tail Entity: Spanish  

Relation: language of work or name  
Context: The popular animated series "Naruto" is originally produced in Japanese, showcasing the unique storytelling and art style of Japanese anime.  
Head Entity: Naruto  
Tail Entity: Japanese  
#### operator ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "operator" means person, profession, or organization that operates the equipment, facility, or service.
Here is an example:
Relation: operator
Context: the aircraft was a boeing 707 - 321c that first entered service with pan american world airways ( pan am ) in 1963 .
Head Entity: boeing 707 - 321c
Tail Entity: pan american world airways
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "operator":
Relation: operator  
Context: the new electric bus fleet is managed by the city transportation authority, which ensures efficient public transit services.  
Head Entity: electric bus fleet  
Tail Entity: city transportation authority  

Relation: operator  
Context: the state-of-the-art research facility is operated by the national science foundation, providing resources for groundbreaking studies.  
Head Entity: research facility  
Tail Entity: national science foundation  
#### religion ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "religion" means religion of a person, organization or religious building, or associated with this subject.
Here is an example:
Relation: religion
Context: udasi has been an early sect based on the teachings of sri chand ( 1494–1643 ) , the son of guru nanak , the founder and the first guru of sikhism .
Head Entity: guru nanak
Tail Entity: sikhism
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "religion":
Relation: religion  
Context: the baha'i faith is a religion founded by baha'u'llah in the 19th century, emphasizing the spiritual unity of all humankind.  
Head Entity: baha'u'llah  
Tail Entity: baha'i faith  

Relation: religion  
Context: the ancient greeks practiced a polytheistic religion that included a pantheon of gods and goddesses, such as zeus and athena.  
Head Entity: ancient greeks  
Tail Entity: polytheistic religion  
Losses:  3.9513635635375977 0.24880069494247437 1.049975037574768
MemoryTrain:  epoch  0, batch     0 | loss: 3.9513636Losses:  4.11998176574707 0.5548423528671265 0.9250171184539795
MemoryTrain:  epoch  0, batch     1 | loss: 4.1199818Losses:  3.116422653198242 0.23989152908325195 1.0438991785049438
MemoryTrain:  epoch  0, batch     2 | loss: 3.1164227Losses:  3.8774752616882324 0.5813527703285217 0.8864411115646362
MemoryTrain:  epoch  0, batch     3 | loss: 3.8774753Losses:  4.269285202026367 0.5138711333274841 0.9973528385162354
MemoryTrain:  epoch  0, batch     4 | loss: 4.2692852Losses:  4.4240498542785645 -0.0 0.9743517637252808
MemoryTrain:  epoch  0, batch     5 | loss: 4.4240499Losses:  4.322101593017578 0.24260088801383972 1.0407419204711914
MemoryTrain:  epoch  0, batch     6 | loss: 4.3221016Losses:  3.299201488494873 0.2562648057937622 0.9685559272766113
MemoryTrain:  epoch  0, batch     7 | loss: 3.2992015Losses:  4.0839619636535645 0.25835272669792175 0.9937015175819397
MemoryTrain:  epoch  0, batch     8 | loss: 4.0839620Losses:  3.487624406814575 0.5370920896530151 0.861771285533905
MemoryTrain:  epoch  0, batch     9 | loss: 3.4876244Losses:  4.923139572143555 0.4796421229839325 1.0832555294036865
MemoryTrain:  epoch  0, batch    10 | loss: 4.9231396Losses:  4.002624988555908 0.5435012578964233 0.9434908628463745
MemoryTrain:  epoch  0, batch    11 | loss: 4.0026250Losses:  4.869617938995361 0.5090131759643555 0.9432287812232971
MemoryTrain:  epoch  0, batch    12 | loss: 4.8696179Losses:  4.1060471534729 0.26288604736328125 0.957423746585846
MemoryTrain:  epoch  0, batch    13 | loss: 4.1060472Losses:  2.939310073852539 -0.0 0.9578128457069397
MemoryTrain:  epoch  0, batch    14 | loss: 2.9393101Losses:  3.264388084411621 -0.0 1.0073751211166382
MemoryTrain:  epoch  1, batch     0 | loss: 3.2643881Losses:  3.0540823936462402 0.4955841302871704 1.0433149337768555
MemoryTrain:  epoch  1, batch     1 | loss: 3.0540824Losses:  3.896061658859253 0.2723960280418396 1.0138304233551025
MemoryTrain:  epoch  1, batch     2 | loss: 3.8960617Losses:  2.7579991817474365 0.4918583631515503 0.7183021903038025
MemoryTrain:  epoch  1, batch     3 | loss: 2.7579992Losses:  2.57381272315979 0.26586106419563293 0.9634028077125549
MemoryTrain:  epoch  1, batch     4 | loss: 2.5738127Losses:  4.211990833282471 0.5216066837310791 0.9297746419906616
MemoryTrain:  epoch  1, batch     5 | loss: 4.2119908Losses:  4.189316272735596 0.49825066328048706 0.9382749795913696
MemoryTrain:  epoch  1, batch     6 | loss: 4.1893163Losses:  3.5125885009765625 0.2640143930912018 1.016737699508667
MemoryTrain:  epoch  1, batch     7 | loss: 3.5125885Losses:  4.607486724853516 0.2616572678089142 0.9871958494186401
MemoryTrain:  epoch  1, batch     8 | loss: 4.6074867Losses:  4.638537883758545 0.26682591438293457 0.9297407865524292
MemoryTrain:  epoch  1, batch     9 | loss: 4.6385379Losses:  3.716762065887451 0.8540043234825134 0.9488564729690552
MemoryTrain:  epoch  1, batch    10 | loss: 3.7167621Losses:  3.469492197036743 0.7628311514854431 0.9842133522033691
MemoryTrain:  epoch  1, batch    11 | loss: 3.4694922Losses:  4.090557098388672 -0.0 1.0158241987228394
MemoryTrain:  epoch  1, batch    12 | loss: 4.0905571Losses:  3.985879898071289 0.28702643513679504 1.054125189781189
MemoryTrain:  epoch  1, batch    13 | loss: 3.9858799Losses:  3.9291200637817383 0.26830554008483887 0.8832111358642578
MemoryTrain:  epoch  1, batch    14 | loss: 3.9291201Losses:  3.189791202545166 0.49101632833480835 0.8445164561271667
MemoryTrain:  epoch  2, batch     0 | loss: 3.1897912Losses:  3.4582180976867676 0.28950852155685425 0.8512701988220215
MemoryTrain:  epoch  2, batch     1 | loss: 3.4582181Losses:  2.4541382789611816 -0.0 1.0655550956726074
MemoryTrain:  epoch  2, batch     2 | loss: 2.4541383Losses:  3.650968551635742 -0.0 0.9963175058364868
MemoryTrain:  epoch  2, batch     3 | loss: 3.6509686Losses:  2.2067465782165527 -0.0 0.9095889925956726
MemoryTrain:  epoch  2, batch     4 | loss: 2.2067466Losses:  4.3392252922058105 0.4796580672264099 0.8428216576576233
MemoryTrain:  epoch  2, batch     5 | loss: 4.3392253Losses:  3.0486438274383545 -0.0 1.0624778270721436
MemoryTrain:  epoch  2, batch     6 | loss: 3.0486438Losses:  2.9689817428588867 -0.0 0.9654825925827026
MemoryTrain:  epoch  2, batch     7 | loss: 2.9689817Losses:  3.597317695617676 0.2501596510410309 0.955528736114502
MemoryTrain:  epoch  2, batch     8 | loss: 3.5973177Losses:  2.7400286197662354 -0.0 0.9730458855628967
MemoryTrain:  epoch  2, batch     9 | loss: 2.7400286Losses:  2.7867631912231445 0.2698879539966583 0.9610075354576111
MemoryTrain:  epoch  2, batch    10 | loss: 2.7867632Losses:  3.7485642433166504 -0.0 1.0313061475753784
MemoryTrain:  epoch  2, batch    11 | loss: 3.7485642Losses:  4.988879680633545 0.3820875883102417 0.9720876812934875
MemoryTrain:  epoch  2, batch    12 | loss: 4.9888797Losses:  4.016040802001953 0.49803128838539124 1.0523157119750977
MemoryTrain:  epoch  2, batch    13 | loss: 4.0160408Losses:  2.9100229740142822 0.5417085289955139 0.9268019199371338
MemoryTrain:  epoch  2, batch    14 | loss: 2.9100230Losses:  3.3296213150024414 -0.0 1.0096138715744019
MemoryTrain:  epoch  3, batch     0 | loss: 3.3296213Losses:  2.548802614212036 0.27468520402908325 0.8538476824760437
MemoryTrain:  epoch  3, batch     1 | loss: 2.5488026Losses:  3.5481815338134766 0.251575231552124 0.8668116331100464
MemoryTrain:  epoch  3, batch     2 | loss: 3.5481815Losses:  3.237372398376465 -0.0 0.9086757898330688
MemoryTrain:  epoch  3, batch     3 | loss: 3.2373724Losses:  3.1698081493377686 0.5156217813491821 0.9621853828430176
MemoryTrain:  epoch  3, batch     4 | loss: 3.1698081Losses:  3.2930150032043457 0.23656892776489258 1.011704683303833
MemoryTrain:  epoch  3, batch     5 | loss: 3.2930150Losses:  2.3908681869506836 -0.0 1.0782850980758667
MemoryTrain:  epoch  3, batch     6 | loss: 2.3908682Losses:  2.877514362335205 -0.0 1.069075345993042
MemoryTrain:  epoch  3, batch     7 | loss: 2.8775144Losses:  3.1451706886291504 -0.0 0.9467629790306091
MemoryTrain:  epoch  3, batch     8 | loss: 3.1451707Losses:  3.5626816749572754 0.2716744840145111 0.9750577211380005
MemoryTrain:  epoch  3, batch     9 | loss: 3.5626817Losses:  2.8818893432617188 -0.0 1.1120942831039429
MemoryTrain:  epoch  3, batch    10 | loss: 2.8818893Losses:  2.707746744155884 0.2657639980316162 0.9455949664115906
MemoryTrain:  epoch  3, batch    11 | loss: 2.7077467Losses:  3.46863055229187 -0.0 0.909712016582489
MemoryTrain:  epoch  3, batch    12 | loss: 3.4686306Losses:  3.4265074729919434 -0.0 1.0953243970870972
MemoryTrain:  epoch  3, batch    13 | loss: 3.4265075Losses:  2.773820161819458 -0.0 1.005603551864624
MemoryTrain:  epoch  3, batch    14 | loss: 2.7738202Losses:  3.6602489948272705 -0.0 1.014646291732788
MemoryTrain:  epoch  4, batch     0 | loss: 3.6602490Losses:  4.846662521362305 1.134337306022644 0.9030398726463318
MemoryTrain:  epoch  4, batch     1 | loss: 4.8466625Losses:  3.0288100242614746 -0.0 1.0127449035644531
MemoryTrain:  epoch  4, batch     2 | loss: 3.0288100Losses:  3.0760934352874756 0.24649780988693237 1.0164484977722168
MemoryTrain:  epoch  4, batch     3 | loss: 3.0760934Losses:  2.537402391433716 -0.0 1.0571298599243164
MemoryTrain:  epoch  4, batch     4 | loss: 2.5374024Losses:  3.591865301132202 0.24533678591251373 0.8534532189369202
MemoryTrain:  epoch  4, batch     5 | loss: 3.5918653Losses:  3.052518367767334 -0.0 0.9563073515892029
MemoryTrain:  epoch  4, batch     6 | loss: 3.0525184Losses:  2.6597652435302734 -0.0 1.0289690494537354
MemoryTrain:  epoch  4, batch     7 | loss: 2.6597652Losses:  3.0821399688720703 0.2591763734817505 1.018886685371399
MemoryTrain:  epoch  4, batch     8 | loss: 3.0821400Losses:  2.608593225479126 -0.0 0.9117600321769714
MemoryTrain:  epoch  4, batch     9 | loss: 2.6085932Losses:  3.1458163261413574 0.25263628363609314 0.9779089689254761
MemoryTrain:  epoch  4, batch    10 | loss: 3.1458163Losses:  2.86173677444458 0.5334672927856445 0.8735398054122925
MemoryTrain:  epoch  4, batch    11 | loss: 2.8617368Losses:  2.4936437606811523 -0.0 0.8987334966659546
MemoryTrain:  epoch  4, batch    12 | loss: 2.4936438Losses:  2.516913890838623 -0.0 1.024338960647583
MemoryTrain:  epoch  4, batch    13 | loss: 2.5169139Losses:  2.8184213638305664 0.2651265263557434 0.9113437533378601
MemoryTrain:  epoch  4, batch    14 | loss: 2.8184214Losses:  2.2659802436828613 -0.0 0.9010790586471558
MemoryTrain:  epoch  5, batch     0 | loss: 2.2659802Losses:  2.8360493183135986 0.5454303026199341 0.8453870415687561
MemoryTrain:  epoch  5, batch     1 | loss: 2.8360493Losses:  2.9784021377563477 -0.0 1.0870158672332764
MemoryTrain:  epoch  5, batch     2 | loss: 2.9784021Losses:  3.16001033782959 -0.0 0.916786789894104
MemoryTrain:  epoch  5, batch     3 | loss: 3.1600103Losses:  2.2756402492523193 -0.0 0.9573622941970825
MemoryTrain:  epoch  5, batch     4 | loss: 2.2756402Losses:  2.6523265838623047 -0.0 0.9135941863059998
MemoryTrain:  epoch  5, batch     5 | loss: 2.6523266Losses:  2.8678746223449707 -0.0 1.071325659751892
MemoryTrain:  epoch  5, batch     6 | loss: 2.8678746Losses:  3.3837242126464844 0.4927774965763092 0.8523564338684082
MemoryTrain:  epoch  5, batch     7 | loss: 3.3837242Losses:  3.186795473098755 -0.0 1.0698645114898682
MemoryTrain:  epoch  5, batch     8 | loss: 3.1867955Losses:  2.929760217666626 -0.0 1.0168702602386475
MemoryTrain:  epoch  5, batch     9 | loss: 2.9297602Losses:  3.2925541400909424 0.550413191318512 0.8994762897491455
MemoryTrain:  epoch  5, batch    10 | loss: 3.2925541Losses:  3.6931161880493164 0.5501824617385864 1.0390925407409668
MemoryTrain:  epoch  5, batch    11 | loss: 3.6931162Losses:  3.174938678741455 0.2538864016532898 0.9685238599777222
MemoryTrain:  epoch  5, batch    12 | loss: 3.1749387Losses:  3.0462331771850586 0.7648029923439026 0.9743026494979858
MemoryTrain:  epoch  5, batch    13 | loss: 3.0462332Losses:  2.1431527137756348 -0.0 0.9130560755729675
MemoryTrain:  epoch  5, batch    14 | loss: 2.1431527Losses:  2.996063232421875 0.5213655233383179 0.898274302482605
MemoryTrain:  epoch  6, batch     0 | loss: 2.9960632Losses:  3.042860984802246 0.35868799686431885 0.9972163438796997
MemoryTrain:  epoch  6, batch     1 | loss: 3.0428610Losses:  2.629192590713501 -0.0 0.9689385890960693
MemoryTrain:  epoch  6, batch     2 | loss: 2.6291926Losses:  2.3546392917633057 0.24473628401756287 0.9116739630699158
MemoryTrain:  epoch  6, batch     3 | loss: 2.3546393Losses:  3.2723228931427 0.7847028970718384 0.9435393214225769
MemoryTrain:  epoch  6, batch     4 | loss: 3.2723229Losses:  2.772045612335205 -0.0 1.0116156339645386
MemoryTrain:  epoch  6, batch     5 | loss: 2.7720456Losses:  3.047173500061035 -0.0 1.0562537908554077
MemoryTrain:  epoch  6, batch     6 | loss: 3.0471735Losses:  3.247800588607788 0.2829476594924927 0.860152006149292
MemoryTrain:  epoch  6, batch     7 | loss: 3.2478006Losses:  2.645535945892334 -0.0 1.011762022972107
MemoryTrain:  epoch  6, batch     8 | loss: 2.6455359Losses:  2.862452507019043 -0.0 1.02982497215271
MemoryTrain:  epoch  6, batch     9 | loss: 2.8624525Losses:  3.0798213481903076 0.27123379707336426 0.7509159445762634
MemoryTrain:  epoch  6, batch    10 | loss: 3.0798213Losses:  2.4381277561187744 -0.0 0.8986112475395203
MemoryTrain:  epoch  6, batch    11 | loss: 2.4381278Losses:  2.784929037094116 0.5092017650604248 0.9664164781570435
MemoryTrain:  epoch  6, batch    12 | loss: 2.7849290Losses:  3.011763095855713 0.2772260010242462 0.9313110709190369
MemoryTrain:  epoch  6, batch    13 | loss: 3.0117631Losses:  2.762951374053955 0.5417587757110596 0.8534112572669983
MemoryTrain:  epoch  6, batch    14 | loss: 2.7629514Losses:  2.812032699584961 0.2707251310348511 1.0206055641174316
MemoryTrain:  epoch  7, batch     0 | loss: 2.8120327Losses:  2.9127964973449707 -0.0 1.0742688179016113
MemoryTrain:  epoch  7, batch     1 | loss: 2.9127965Losses:  3.140082836151123 0.31709063053131104 0.9066051244735718
MemoryTrain:  epoch  7, batch     2 | loss: 3.1400828Losses:  3.4506592750549316 0.2742460072040558 0.9555938243865967
MemoryTrain:  epoch  7, batch     3 | loss: 3.4506593Losses:  2.7178685665130615 0.249156653881073 0.9472308158874512
MemoryTrain:  epoch  7, batch     4 | loss: 2.7178686Losses:  2.637847423553467 0.2224632352590561 0.8591756224632263
MemoryTrain:  epoch  7, batch     5 | loss: 2.6378474Losses:  3.335078716278076 0.7586264610290527 0.9749100208282471
MemoryTrain:  epoch  7, batch     6 | loss: 3.3350787Losses:  2.806849241256714 0.25766420364379883 1.023662805557251
MemoryTrain:  epoch  7, batch     7 | loss: 2.8068492Losses:  2.5947489738464355 0.2344510406255722 1.0248621702194214
MemoryTrain:  epoch  7, batch     8 | loss: 2.5947490Losses:  3.3917431831359863 0.2570285201072693 1.0600324869155884
MemoryTrain:  epoch  7, batch     9 | loss: 3.3917432Losses:  2.5578792095184326 0.21682769060134888 0.9057378768920898
MemoryTrain:  epoch  7, batch    10 | loss: 2.5578792Losses:  2.847022294998169 0.248049795627594 0.9375936388969421
MemoryTrain:  epoch  7, batch    11 | loss: 2.8470223Losses:  2.8371710777282715 -0.0 0.9003027677536011
MemoryTrain:  epoch  7, batch    12 | loss: 2.8371711Losses:  2.702892780303955 -0.0 1.0165218114852905
MemoryTrain:  epoch  7, batch    13 | loss: 2.7028928Losses:  2.368060350418091 0.2574426829814911 0.8716496825218201
MemoryTrain:  epoch  7, batch    14 | loss: 2.3680604Losses:  2.9456636905670166 0.2455296367406845 0.9598483443260193
MemoryTrain:  epoch  8, batch     0 | loss: 2.9456637Losses:  2.96773624420166 0.4925980567932129 0.9627817869186401
MemoryTrain:  epoch  8, batch     1 | loss: 2.9677362Losses:  3.3321027755737305 0.7860969305038452 0.9106218814849854
MemoryTrain:  epoch  8, batch     2 | loss: 3.3321028Losses:  2.5112576484680176 -0.0 1.0262391567230225
MemoryTrain:  epoch  8, batch     3 | loss: 2.5112576Losses:  2.741352081298828 0.24050948023796082 0.8915390968322754
MemoryTrain:  epoch  8, batch     4 | loss: 2.7413521Losses:  2.632107734680176 0.2534826993942261 1.0162636041641235
MemoryTrain:  epoch  8, batch     5 | loss: 2.6321077Losses:  2.7368035316467285 0.5081621408462524 0.9200115203857422
MemoryTrain:  epoch  8, batch     6 | loss: 2.7368035Losses:  2.498974084854126 0.25402411818504333 0.9584562182426453
MemoryTrain:  epoch  8, batch     7 | loss: 2.4989741Losses:  2.834657669067383 0.4862040579319 1.0262624025344849
MemoryTrain:  epoch  8, batch     8 | loss: 2.8346577Losses:  2.751979351043701 -0.0 1.0743775367736816
MemoryTrain:  epoch  8, batch     9 | loss: 2.7519794Losses:  2.7174274921417236 0.5188394784927368 0.9672344326972961
MemoryTrain:  epoch  8, batch    10 | loss: 2.7174275Losses:  2.774667978286743 0.5029665231704712 0.9413592219352722
MemoryTrain:  epoch  8, batch    11 | loss: 2.7746680Losses:  2.755232334136963 -0.0 0.933680534362793
MemoryTrain:  epoch  8, batch    12 | loss: 2.7552323Losses:  2.821516990661621 0.31729018688201904 0.9268989562988281
MemoryTrain:  epoch  8, batch    13 | loss: 2.8215170Losses:  2.8358991146087646 0.7456952333450317 0.8642184138298035
MemoryTrain:  epoch  8, batch    14 | loss: 2.8358991Losses:  2.578587532043457 0.23908273875713348 0.9718917608261108
MemoryTrain:  epoch  9, batch     0 | loss: 2.5785875Losses:  2.442458152770996 -0.0 1.1127647161483765
MemoryTrain:  epoch  9, batch     1 | loss: 2.4424582Losses:  2.8813371658325195 0.2456924170255661 1.0779685974121094
MemoryTrain:  epoch  9, batch     2 | loss: 2.8813372Losses:  2.865823268890381 0.49234768748283386 1.007325530052185
MemoryTrain:  epoch  9, batch     3 | loss: 2.8658233Losses:  3.036980628967285 0.79767906665802 0.8457395434379578
MemoryTrain:  epoch  9, batch     4 | loss: 3.0369806Losses:  2.7558207511901855 0.48596516251564026 0.9966570138931274
MemoryTrain:  epoch  9, batch     5 | loss: 2.7558208Losses:  2.9904181957244873 0.5200926661491394 0.925193727016449
MemoryTrain:  epoch  9, batch     6 | loss: 2.9904182Losses:  2.5299863815307617 0.26670998334884644 0.898568332195282
MemoryTrain:  epoch  9, batch     7 | loss: 2.5299864Losses:  2.6453018188476562 0.22949782013893127 0.9606063365936279
MemoryTrain:  epoch  9, batch     8 | loss: 2.6453018Losses:  2.328381299972534 -0.0 1.062157154083252
MemoryTrain:  epoch  9, batch     9 | loss: 2.3283813Losses:  2.300541639328003 -0.0 0.9475753903388977
MemoryTrain:  epoch  9, batch    10 | loss: 2.3005416Losses:  2.263232946395874 -0.0 0.911338746547699
MemoryTrain:  epoch  9, batch    11 | loss: 2.2632329Losses:  2.6651272773742676 0.2857418656349182 0.8990749716758728
MemoryTrain:  epoch  9, batch    12 | loss: 2.6651273Losses:  2.728846311569214 0.25289520621299744 1.0169838666915894
MemoryTrain:  epoch  9, batch    13 | loss: 2.7288463Losses:  2.235812187194824 -0.0 0.9591537117958069
MemoryTrain:  epoch  9, batch    14 | loss: 2.2358122
[EVAL] batch:    0 | acc: 56.25%,  total acc: 56.25%   [EVAL] batch:    1 | acc: 68.75%,  total acc: 62.50%   [EVAL] batch:    2 | acc: 56.25%,  total acc: 60.42%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 65.62%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 67.50%   [EVAL] batch:    5 | acc: 56.25%,  total acc: 65.62%   [EVAL] batch:    6 | acc: 50.00%,  total acc: 63.39%   [EVAL] batch:    7 | acc: 68.75%,  total acc: 64.06%   [EVAL] batch:    8 | acc: 50.00%,  total acc: 62.50%   [EVAL] batch:    9 | acc: 43.75%,  total acc: 60.62%   [EVAL] batch:   10 | acc: 68.75%,  total acc: 61.36%   [EVAL] batch:   11 | acc: 43.75%,  total acc: 59.90%   [EVAL] batch:   12 | acc: 62.50%,  total acc: 60.10%   [EVAL] batch:   13 | acc: 18.75%,  total acc: 57.14%   [EVAL] batch:   14 | acc: 25.00%,  total acc: 55.00%   [EVAL] batch:   15 | acc: 12.50%,  total acc: 52.34%   [EVAL] batch:   16 | acc: 43.75%,  total acc: 51.84%   [EVAL] batch:   17 | acc: 43.75%,  total acc: 51.39%   [EVAL] batch:   18 | acc: 37.50%,  total acc: 50.66%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 52.81%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 55.06%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 57.10%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 58.97%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 60.68%   [EVAL] batch:   24 | acc: 93.75%,  total acc: 62.00%   [EVAL] batch:   25 | acc: 6.25%,  total acc: 59.86%   [EVAL] batch:   26 | acc: 6.25%,  total acc: 57.87%   [EVAL] batch:   27 | acc: 6.25%,  total acc: 56.03%   [EVAL] batch:   28 | acc: 31.25%,  total acc: 55.17%   [EVAL] batch:   29 | acc: 12.50%,  total acc: 53.75%   [EVAL] batch:   30 | acc: 6.25%,  total acc: 52.22%   [EVAL] batch:   31 | acc: 75.00%,  total acc: 52.93%   [EVAL] batch:   32 | acc: 87.50%,  total acc: 53.98%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 55.33%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 56.43%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 57.29%   [EVAL] batch:   36 | acc: 93.75%,  total acc: 58.28%   [EVAL] batch:   37 | acc: 75.00%,  total acc: 58.72%   [EVAL] batch:   38 | acc: 68.75%,  total acc: 58.97%   [EVAL] batch:   39 | acc: 62.50%,  total acc: 59.06%   [EVAL] batch:   40 | acc: 62.50%,  total acc: 59.15%   [EVAL] batch:   41 | acc: 43.75%,  total acc: 58.78%   [EVAL] batch:   42 | acc: 50.00%,  total acc: 58.58%   [EVAL] batch:   43 | acc: 56.25%,  total acc: 58.52%   [EVAL] batch:   44 | acc: 62.50%,  total acc: 58.61%   [EVAL] batch:   45 | acc: 93.75%,  total acc: 59.38%   [EVAL] batch:   46 | acc: 68.75%,  total acc: 59.57%   [EVAL] batch:   47 | acc: 62.50%,  total acc: 59.64%   [EVAL] batch:   48 | acc: 75.00%,  total acc: 59.95%   [EVAL] batch:   49 | acc: 62.50%,  total acc: 60.00%   [EVAL] batch:   50 | acc: 37.50%,  total acc: 59.56%   [EVAL] batch:   51 | acc: 50.00%,  total acc: 59.38%   [EVAL] batch:   52 | acc: 56.25%,  total acc: 59.32%   [EVAL] batch:   53 | acc: 62.50%,  total acc: 59.38%   [EVAL] batch:   54 | acc: 75.00%,  total acc: 59.66%   [EVAL] batch:   55 | acc: 62.50%,  total acc: 59.71%   [EVAL] batch:   56 | acc: 75.00%,  total acc: 59.98%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 60.45%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 61.02%   [EVAL] batch:   59 | acc: 75.00%,  total acc: 61.25%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 61.89%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 62.00%   [EVAL] batch:   62 | acc: 43.75%,  total acc: 61.71%   
[EVAL] batch:    0 | acc: 56.25%,  total acc: 56.25%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 75.00%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 77.08%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 75.00%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 73.75%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 73.96%   [EVAL] batch:    6 | acc: 56.25%,  total acc: 71.43%   [EVAL] batch:    7 | acc: 62.50%,  total acc: 70.31%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 70.14%   [EVAL] batch:    9 | acc: 56.25%,  total acc: 68.75%   [EVAL] batch:   10 | acc: 37.50%,  total acc: 65.91%   [EVAL] batch:   11 | acc: 50.00%,  total acc: 64.58%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 65.38%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 67.41%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 69.17%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 71.09%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 72.43%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 73.61%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:   19 | acc: 68.75%,  total acc: 74.69%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 75.60%   [EVAL] batch:   21 | acc: 68.75%,  total acc: 75.28%   [EVAL] batch:   22 | acc: 56.25%,  total acc: 74.46%   [EVAL] batch:   23 | acc: 81.25%,  total acc: 74.74%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 75.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 76.20%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 77.08%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 77.90%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 78.66%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 79.38%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 80.04%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 80.66%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:   33 | acc: 56.25%,  total acc: 80.51%   [EVAL] batch:   34 | acc: 68.75%,  total acc: 80.18%   [EVAL] batch:   35 | acc: 81.25%,  total acc: 80.21%   [EVAL] batch:   36 | acc: 81.25%,  total acc: 80.24%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 80.59%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 81.09%   [EVAL] batch:   39 | acc: 93.75%,  total acc: 81.41%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 81.71%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 81.99%   [EVAL] batch:   42 | acc: 87.50%,  total acc: 82.12%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 82.39%   [EVAL] batch:   44 | acc: 81.25%,  total acc: 82.36%   [EVAL] batch:   45 | acc: 75.00%,  total acc: 82.20%   [EVAL] batch:   46 | acc: 75.00%,  total acc: 82.05%   [EVAL] batch:   47 | acc: 75.00%,  total acc: 81.90%   [EVAL] batch:   48 | acc: 81.25%,  total acc: 81.89%   [EVAL] batch:   49 | acc: 75.00%,  total acc: 81.75%   [EVAL] batch:   50 | acc: 37.50%,  total acc: 80.88%   [EVAL] batch:   51 | acc: 50.00%,  total acc: 80.29%   [EVAL] batch:   52 | acc: 50.00%,  total acc: 79.72%   [EVAL] batch:   53 | acc: 37.50%,  total acc: 78.94%   [EVAL] batch:   54 | acc: 18.75%,  total acc: 77.84%   [EVAL] batch:   55 | acc: 43.75%,  total acc: 77.23%   [EVAL] batch:   56 | acc: 56.25%,  total acc: 76.86%   [EVAL] batch:   57 | acc: 50.00%,  total acc: 76.40%   [EVAL] batch:   58 | acc: 56.25%,  total acc: 76.06%   [EVAL] batch:   59 | acc: 81.25%,  total acc: 76.15%   [EVAL] batch:   60 | acc: 68.75%,  total acc: 76.02%   [EVAL] batch:   61 | acc: 62.50%,  total acc: 75.81%   [EVAL] batch:   62 | acc: 81.25%,  total acc: 75.89%   [EVAL] batch:   63 | acc: 93.75%,  total acc: 76.17%   [EVAL] batch:   64 | acc: 81.25%,  total acc: 76.25%   [EVAL] batch:   65 | acc: 100.00%,  total acc: 76.61%   [EVAL] batch:   66 | acc: 75.00%,  total acc: 76.59%   [EVAL] batch:   67 | acc: 93.75%,  total acc: 76.84%   [EVAL] batch:   68 | acc: 81.25%,  total acc: 76.90%   [EVAL] batch:   69 | acc: 50.00%,  total acc: 76.52%   [EVAL] batch:   70 | acc: 62.50%,  total acc: 76.32%   [EVAL] batch:   71 | acc: 62.50%,  total acc: 76.13%   [EVAL] batch:   72 | acc: 37.50%,  total acc: 75.60%   [EVAL] batch:   73 | acc: 62.50%,  total acc: 75.42%   [EVAL] batch:   74 | acc: 43.75%,  total acc: 75.00%   [EVAL] batch:   75 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:   76 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:   77 | acc: 81.25%,  total acc: 75.08%   [EVAL] batch:   78 | acc: 50.00%,  total acc: 74.76%   [EVAL] batch:   79 | acc: 62.50%,  total acc: 74.61%   [EVAL] batch:   80 | acc: 56.25%,  total acc: 74.38%   [EVAL] batch:   81 | acc: 56.25%,  total acc: 74.16%   [EVAL] batch:   82 | acc: 31.25%,  total acc: 73.64%   [EVAL] batch:   83 | acc: 43.75%,  total acc: 73.29%   [EVAL] batch:   84 | acc: 37.50%,  total acc: 72.87%   [EVAL] batch:   85 | acc: 50.00%,  total acc: 72.60%   [EVAL] batch:   86 | acc: 25.00%,  total acc: 72.05%   [EVAL] batch:   87 | acc: 62.50%,  total acc: 71.95%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 72.26%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 72.57%   [EVAL] batch:   90 | acc: 100.00%,  total acc: 72.87%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 73.17%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 73.45%   [EVAL] batch:   93 | acc: 93.75%,  total acc: 73.67%   [EVAL] batch:   94 | acc: 56.25%,  total acc: 73.49%   [EVAL] batch:   95 | acc: 81.25%,  total acc: 73.57%   [EVAL] batch:   96 | acc: 68.75%,  total acc: 73.52%   [EVAL] batch:   97 | acc: 50.00%,  total acc: 73.28%   [EVAL] batch:   98 | acc: 75.00%,  total acc: 73.30%   [EVAL] batch:   99 | acc: 43.75%,  total acc: 73.00%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 73.27%   [EVAL] batch:  101 | acc: 100.00%,  total acc: 73.53%   [EVAL] batch:  102 | acc: 100.00%,  total acc: 73.79%   [EVAL] batch:  103 | acc: 93.75%,  total acc: 73.98%   [EVAL] batch:  104 | acc: 100.00%,  total acc: 74.23%   [EVAL] batch:  105 | acc: 93.75%,  total acc: 74.41%   [EVAL] batch:  106 | acc: 100.00%,  total acc: 74.65%   [EVAL] batch:  107 | acc: 100.00%,  total acc: 74.88%   [EVAL] batch:  108 | acc: 100.00%,  total acc: 75.11%   [EVAL] batch:  109 | acc: 100.00%,  total acc: 75.34%   [EVAL] batch:  110 | acc: 100.00%,  total acc: 75.56%   [EVAL] batch:  111 | acc: 100.00%,  total acc: 75.78%   [EVAL] batch:  112 | acc: 75.00%,  total acc: 75.77%   [EVAL] batch:  113 | acc: 56.25%,  total acc: 75.60%   [EVAL] batch:  114 | acc: 87.50%,  total acc: 75.71%   [EVAL] batch:  115 | acc: 87.50%,  total acc: 75.81%   [EVAL] batch:  116 | acc: 68.75%,  total acc: 75.75%   [EVAL] batch:  117 | acc: 87.50%,  total acc: 75.85%   [EVAL] batch:  118 | acc: 75.00%,  total acc: 75.84%   [EVAL] batch:  119 | acc: 87.50%,  total acc: 75.94%   [EVAL] batch:  120 | acc: 62.50%,  total acc: 75.83%   [EVAL] batch:  121 | acc: 93.75%,  total acc: 75.97%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 76.17%   [EVAL] batch:  123 | acc: 81.25%,  total acc: 76.21%   [EVAL] batch:  124 | acc: 87.50%,  total acc: 76.30%   [EVAL] batch:  125 | acc: 0.00%,  total acc: 75.69%   [EVAL] batch:  126 | acc: 0.00%,  total acc: 75.10%   [EVAL] batch:  127 | acc: 0.00%,  total acc: 74.51%   [EVAL] batch:  128 | acc: 0.00%,  total acc: 73.93%   [EVAL] batch:  129 | acc: 0.00%,  total acc: 73.37%   [EVAL] batch:  130 | acc: 6.25%,  total acc: 72.85%   [EVAL] batch:  131 | acc: 56.25%,  total acc: 72.73%   [EVAL] batch:  132 | acc: 68.75%,  total acc: 72.70%   [EVAL] batch:  133 | acc: 75.00%,  total acc: 72.71%   [EVAL] batch:  134 | acc: 68.75%,  total acc: 72.69%   [EVAL] batch:  135 | acc: 56.25%,  total acc: 72.56%   [EVAL] batch:  136 | acc: 50.00%,  total acc: 72.40%   [EVAL] batch:  137 | acc: 81.25%,  total acc: 72.46%   [EVAL] batch:  138 | acc: 50.00%,  total acc: 72.30%   [EVAL] batch:  139 | acc: 56.25%,  total acc: 72.19%   [EVAL] batch:  140 | acc: 75.00%,  total acc: 72.21%   [EVAL] batch:  141 | acc: 56.25%,  total acc: 72.10%   [EVAL] batch:  142 | acc: 62.50%,  total acc: 72.03%   [EVAL] batch:  143 | acc: 25.00%,  total acc: 71.70%   [EVAL] batch:  144 | acc: 25.00%,  total acc: 71.38%   [EVAL] batch:  145 | acc: 18.75%,  total acc: 71.02%   [EVAL] batch:  146 | acc: 31.25%,  total acc: 70.75%   [EVAL] batch:  147 | acc: 6.25%,  total acc: 70.31%   [EVAL] batch:  148 | acc: 25.00%,  total acc: 70.01%   [EVAL] batch:  149 | acc: 25.00%,  total acc: 69.71%   [EVAL] batch:  150 | acc: 6.25%,  total acc: 69.29%   [EVAL] batch:  151 | acc: 6.25%,  total acc: 68.87%   [EVAL] batch:  152 | acc: 0.00%,  total acc: 68.42%   [EVAL] batch:  153 | acc: 6.25%,  total acc: 68.02%   [EVAL] batch:  154 | acc: 12.50%,  total acc: 67.66%   [EVAL] batch:  155 | acc: 0.00%,  total acc: 67.23%   [EVAL] batch:  156 | acc: 43.75%,  total acc: 67.08%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 67.25%   [EVAL] batch:  158 | acc: 93.75%,  total acc: 67.41%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 67.58%   [EVAL] batch:  160 | acc: 93.75%,  total acc: 67.74%   [EVAL] batch:  161 | acc: 81.25%,  total acc: 67.82%   [EVAL] batch:  162 | acc: 93.75%,  total acc: 67.98%   [EVAL] batch:  163 | acc: 87.50%,  total acc: 68.10%   [EVAL] batch:  164 | acc: 100.00%,  total acc: 68.30%   [EVAL] batch:  165 | acc: 100.00%,  total acc: 68.49%   [EVAL] batch:  166 | acc: 93.75%,  total acc: 68.64%   [EVAL] batch:  167 | acc: 100.00%,  total acc: 68.82%   [EVAL] batch:  168 | acc: 93.75%,  total acc: 68.97%   [EVAL] batch:  169 | acc: 31.25%,  total acc: 68.75%   [EVAL] batch:  170 | acc: 37.50%,  total acc: 68.57%   [EVAL] batch:  171 | acc: 18.75%,  total acc: 68.28%   [EVAL] batch:  172 | acc: 31.25%,  total acc: 68.06%   [EVAL] batch:  173 | acc: 43.75%,  total acc: 67.92%   [EVAL] batch:  174 | acc: 43.75%,  total acc: 67.79%   [EVAL] batch:  175 | acc: 31.25%,  total acc: 67.58%   [EVAL] batch:  176 | acc: 43.75%,  total acc: 67.44%   [EVAL] batch:  177 | acc: 37.50%,  total acc: 67.28%   [EVAL] batch:  178 | acc: 50.00%,  total acc: 67.18%   [EVAL] batch:  179 | acc: 43.75%,  total acc: 67.05%   [EVAL] batch:  180 | acc: 56.25%,  total acc: 66.99%   [EVAL] batch:  181 | acc: 18.75%,  total acc: 66.72%   [EVAL] batch:  182 | acc: 25.00%,  total acc: 66.50%   [EVAL] batch:  183 | acc: 56.25%,  total acc: 66.44%   [EVAL] batch:  184 | acc: 37.50%,  total acc: 66.28%   [EVAL] batch:  185 | acc: 25.00%,  total acc: 66.06%   [EVAL] batch:  186 | acc: 18.75%,  total acc: 65.81%   [EVAL] batch:  187 | acc: 25.00%,  total acc: 65.59%   [EVAL] batch:  188 | acc: 56.25%,  total acc: 65.54%   [EVAL] batch:  189 | acc: 37.50%,  total acc: 65.39%   [EVAL] batch:  190 | acc: 37.50%,  total acc: 65.25%   [EVAL] batch:  191 | acc: 37.50%,  total acc: 65.10%   [EVAL] batch:  192 | acc: 37.50%,  total acc: 64.96%   [EVAL] batch:  193 | acc: 37.50%,  total acc: 64.82%   [EVAL] batch:  194 | acc: 62.50%,  total acc: 64.81%   [EVAL] batch:  195 | acc: 62.50%,  total acc: 64.80%   [EVAL] batch:  196 | acc: 68.75%,  total acc: 64.82%   [EVAL] batch:  197 | acc: 62.50%,  total acc: 64.80%   [EVAL] batch:  198 | acc: 37.50%,  total acc: 64.67%   [EVAL] batch:  199 | acc: 100.00%,  total acc: 64.84%   [EVAL] batch:  200 | acc: 50.00%,  total acc: 64.77%   [EVAL] batch:  201 | acc: 56.25%,  total acc: 64.73%   [EVAL] batch:  202 | acc: 56.25%,  total acc: 64.69%   [EVAL] batch:  203 | acc: 50.00%,  total acc: 64.61%   [EVAL] batch:  204 | acc: 43.75%,  total acc: 64.51%   [EVAL] batch:  205 | acc: 68.75%,  total acc: 64.53%   [EVAL] batch:  206 | acc: 87.50%,  total acc: 64.64%   [EVAL] batch:  207 | acc: 93.75%,  total acc: 64.78%   [EVAL] batch:  208 | acc: 100.00%,  total acc: 64.95%   [EVAL] batch:  209 | acc: 100.00%,  total acc: 65.12%   [EVAL] batch:  210 | acc: 93.75%,  total acc: 65.25%   [EVAL] batch:  211 | acc: 100.00%,  total acc: 65.42%   [EVAL] batch:  212 | acc: 62.50%,  total acc: 65.40%   [EVAL] batch:  213 | acc: 31.25%,  total acc: 65.25%   [EVAL] batch:  214 | acc: 43.75%,  total acc: 65.15%   [EVAL] batch:  215 | acc: 68.75%,  total acc: 65.16%   [EVAL] batch:  216 | acc: 31.25%,  total acc: 65.01%   [EVAL] batch:  217 | acc: 50.00%,  total acc: 64.94%   [EVAL] batch:  218 | acc: 50.00%,  total acc: 64.87%   [EVAL] batch:  219 | acc: 43.75%,  total acc: 64.77%   [EVAL] batch:  220 | acc: 25.00%,  total acc: 64.59%   [EVAL] batch:  221 | acc: 31.25%,  total acc: 64.44%   [EVAL] batch:  222 | acc: 25.00%,  total acc: 64.27%   [EVAL] batch:  223 | acc: 18.75%,  total acc: 64.06%   [EVAL] batch:  224 | acc: 37.50%,  total acc: 63.94%   [EVAL] batch:  225 | acc: 75.00%,  total acc: 63.99%   [EVAL] batch:  226 | acc: 81.25%,  total acc: 64.07%   [EVAL] batch:  227 | acc: 75.00%,  total acc: 64.12%   [EVAL] batch:  228 | acc: 93.75%,  total acc: 64.25%   [EVAL] batch:  229 | acc: 68.75%,  total acc: 64.27%   [EVAL] batch:  230 | acc: 75.00%,  total acc: 64.31%   [EVAL] batch:  231 | acc: 68.75%,  total acc: 64.33%   [EVAL] batch:  232 | acc: 50.00%,  total acc: 64.27%   [EVAL] batch:  233 | acc: 68.75%,  total acc: 64.29%   [EVAL] batch:  234 | acc: 62.50%,  total acc: 64.28%   [EVAL] batch:  235 | acc: 43.75%,  total acc: 64.19%   [EVAL] batch:  236 | acc: 75.00%,  total acc: 64.24%   [EVAL] batch:  237 | acc: 93.75%,  total acc: 64.36%   [EVAL] batch:  238 | acc: 100.00%,  total acc: 64.51%   [EVAL] batch:  239 | acc: 100.00%,  total acc: 64.66%   [EVAL] batch:  240 | acc: 100.00%,  total acc: 64.81%   [EVAL] batch:  241 | acc: 100.00%,  total acc: 64.95%   [EVAL] batch:  242 | acc: 100.00%,  total acc: 65.10%   [EVAL] batch:  243 | acc: 100.00%,  total acc: 65.24%   [EVAL] batch:  244 | acc: 93.75%,  total acc: 65.36%   [EVAL] batch:  245 | acc: 87.50%,  total acc: 65.45%   [EVAL] batch:  246 | acc: 87.50%,  total acc: 65.54%   [EVAL] batch:  247 | acc: 93.75%,  total acc: 65.65%   [EVAL] batch:  248 | acc: 100.00%,  total acc: 65.79%   [EVAL] batch:  249 | acc: 100.00%,  total acc: 65.92%   [EVAL] batch:  250 | acc: 81.25%,  total acc: 65.99%   [EVAL] batch:  251 | acc: 68.75%,  total acc: 66.00%   [EVAL] batch:  252 | acc: 75.00%,  total acc: 66.03%   [EVAL] batch:  253 | acc: 56.25%,  total acc: 65.99%   [EVAL] batch:  254 | acc: 81.25%,  total acc: 66.05%   [EVAL] batch:  255 | acc: 81.25%,  total acc: 66.11%   [EVAL] batch:  256 | acc: 75.00%,  total acc: 66.15%   [EVAL] batch:  257 | acc: 68.75%,  total acc: 66.16%   [EVAL] batch:  258 | acc: 62.50%,  total acc: 66.14%   [EVAL] batch:  259 | acc: 62.50%,  total acc: 66.13%   [EVAL] batch:  260 | acc: 68.75%,  total acc: 66.14%   [EVAL] batch:  261 | acc: 62.50%,  total acc: 66.13%   [EVAL] batch:  262 | acc: 87.50%,  total acc: 66.21%   [EVAL] batch:  263 | acc: 56.25%,  total acc: 66.17%   [EVAL] batch:  264 | acc: 75.00%,  total acc: 66.20%   [EVAL] batch:  265 | acc: 56.25%,  total acc: 66.17%   [EVAL] batch:  266 | acc: 81.25%,  total acc: 66.22%   [EVAL] batch:  267 | acc: 81.25%,  total acc: 66.28%   [EVAL] batch:  268 | acc: 68.75%,  total acc: 66.29%   [EVAL] batch:  269 | acc: 18.75%,  total acc: 66.11%   [EVAL] batch:  270 | acc: 37.50%,  total acc: 66.01%   [EVAL] batch:  271 | acc: 12.50%,  total acc: 65.81%   [EVAL] batch:  272 | acc: 37.50%,  total acc: 65.71%   [EVAL] batch:  273 | acc: 50.00%,  total acc: 65.65%   [EVAL] batch:  274 | acc: 31.25%,  total acc: 65.52%   [EVAL] batch:  275 | acc: 100.00%,  total acc: 65.65%   [EVAL] batch:  276 | acc: 100.00%,  total acc: 65.77%   [EVAL] batch:  277 | acc: 100.00%,  total acc: 65.89%   [EVAL] batch:  278 | acc: 100.00%,  total acc: 66.02%   [EVAL] batch:  279 | acc: 100.00%,  total acc: 66.14%   [EVAL] batch:  280 | acc: 100.00%,  total acc: 66.26%   [EVAL] batch:  281 | acc: 100.00%,  total acc: 66.38%   [EVAL] batch:  282 | acc: 100.00%,  total acc: 66.50%   [EVAL] batch:  283 | acc: 100.00%,  total acc: 66.62%   [EVAL] batch:  284 | acc: 100.00%,  total acc: 66.73%   [EVAL] batch:  285 | acc: 100.00%,  total acc: 66.85%   [EVAL] batch:  286 | acc: 100.00%,  total acc: 66.96%   [EVAL] batch:  287 | acc: 93.75%,  total acc: 67.06%   [EVAL] batch:  288 | acc: 68.75%,  total acc: 67.06%   [EVAL] batch:  289 | acc: 62.50%,  total acc: 67.05%   [EVAL] batch:  290 | acc: 68.75%,  total acc: 67.05%   [EVAL] batch:  291 | acc: 81.25%,  total acc: 67.10%   [EVAL] batch:  292 | acc: 81.25%,  total acc: 67.15%   [EVAL] batch:  293 | acc: 62.50%,  total acc: 67.13%   [EVAL] batch:  294 | acc: 37.50%,  total acc: 67.03%   [EVAL] batch:  295 | acc: 31.25%,  total acc: 66.91%   [EVAL] batch:  296 | acc: 18.75%,  total acc: 66.75%   [EVAL] batch:  297 | acc: 50.00%,  total acc: 66.69%   [EVAL] batch:  298 | acc: 43.75%,  total acc: 66.62%   [EVAL] batch:  299 | acc: 50.00%,  total acc: 66.56%   [EVAL] batch:  300 | acc: 93.75%,  total acc: 66.65%   [EVAL] batch:  301 | acc: 93.75%,  total acc: 66.74%   [EVAL] batch:  302 | acc: 93.75%,  total acc: 66.83%   [EVAL] batch:  303 | acc: 81.25%,  total acc: 66.88%   [EVAL] batch:  304 | acc: 93.75%,  total acc: 66.97%   [EVAL] batch:  305 | acc: 87.50%,  total acc: 67.03%   [EVAL] batch:  306 | acc: 43.75%,  total acc: 66.96%   [EVAL] batch:  307 | acc: 12.50%,  total acc: 66.78%   [EVAL] batch:  308 | acc: 6.25%,  total acc: 66.59%   [EVAL] batch:  309 | acc: 18.75%,  total acc: 66.43%   [EVAL] batch:  310 | acc: 6.25%,  total acc: 66.24%   [EVAL] batch:  311 | acc: 18.75%,  total acc: 66.09%   [EVAL] batch:  312 | acc: 0.00%,  total acc: 65.87%   [EVAL] batch:  313 | acc: 18.75%,  total acc: 65.72%   [EVAL] batch:  314 | acc: 0.00%,  total acc: 65.52%   [EVAL] batch:  315 | acc: 12.50%,  total acc: 65.35%   [EVAL] batch:  316 | acc: 12.50%,  total acc: 65.18%   [EVAL] batch:  317 | acc: 18.75%,  total acc: 65.04%   [EVAL] batch:  318 | acc: 31.25%,  total acc: 64.93%   [EVAL] batch:  319 | acc: 81.25%,  total acc: 64.98%   [EVAL] batch:  320 | acc: 56.25%,  total acc: 64.95%   [EVAL] batch:  321 | acc: 81.25%,  total acc: 65.00%   [EVAL] batch:  322 | acc: 81.25%,  total acc: 65.05%   [EVAL] batch:  323 | acc: 75.00%,  total acc: 65.08%   [EVAL] batch:  324 | acc: 75.00%,  total acc: 65.12%   [EVAL] batch:  325 | acc: 50.00%,  total acc: 65.07%   [EVAL] batch:  326 | acc: 31.25%,  total acc: 64.97%   [EVAL] batch:  327 | acc: 68.75%,  total acc: 64.98%   [EVAL] batch:  328 | acc: 56.25%,  total acc: 64.95%   [EVAL] batch:  329 | acc: 62.50%,  total acc: 64.94%   [EVAL] batch:  330 | acc: 37.50%,  total acc: 64.86%   [EVAL] batch:  331 | acc: 93.75%,  total acc: 64.95%   [EVAL] batch:  332 | acc: 100.00%,  total acc: 65.05%   [EVAL] batch:  333 | acc: 100.00%,  total acc: 65.16%   [EVAL] batch:  334 | acc: 93.75%,  total acc: 65.24%   [EVAL] batch:  335 | acc: 93.75%,  total acc: 65.33%   [EVAL] batch:  336 | acc: 100.00%,  total acc: 65.43%   [EVAL] batch:  337 | acc: 50.00%,  total acc: 65.38%   [EVAL] batch:  338 | acc: 50.00%,  total acc: 65.34%   [EVAL] batch:  339 | acc: 43.75%,  total acc: 65.28%   [EVAL] batch:  340 | acc: 62.50%,  total acc: 65.27%   [EVAL] batch:  341 | acc: 75.00%,  total acc: 65.30%   [EVAL] batch:  342 | acc: 75.00%,  total acc: 65.32%   [EVAL] batch:  343 | acc: 75.00%,  total acc: 65.35%   [EVAL] batch:  344 | acc: 100.00%,  total acc: 65.45%   [EVAL] batch:  345 | acc: 100.00%,  total acc: 65.55%   [EVAL] batch:  346 | acc: 93.75%,  total acc: 65.63%   [EVAL] batch:  347 | acc: 93.75%,  total acc: 65.71%   [EVAL] batch:  348 | acc: 100.00%,  total acc: 65.81%   [EVAL] batch:  349 | acc: 100.00%,  total acc: 65.91%   [EVAL] batch:  350 | acc: 50.00%,  total acc: 65.87%   [EVAL] batch:  351 | acc: 50.00%,  total acc: 65.82%   [EVAL] batch:  352 | acc: 31.25%,  total acc: 65.72%   [EVAL] batch:  353 | acc: 31.25%,  total acc: 65.62%   [EVAL] batch:  354 | acc: 37.50%,  total acc: 65.55%   [EVAL] batch:  355 | acc: 50.00%,  total acc: 65.50%   [EVAL] batch:  356 | acc: 87.50%,  total acc: 65.56%   [EVAL] batch:  357 | acc: 87.50%,  total acc: 65.62%   [EVAL] batch:  358 | acc: 93.75%,  total acc: 65.70%   [EVAL] batch:  359 | acc: 87.50%,  total acc: 65.76%   [EVAL] batch:  360 | acc: 62.50%,  total acc: 65.75%   [EVAL] batch:  361 | acc: 62.50%,  total acc: 65.75%   [EVAL] batch:  362 | acc: 68.75%,  total acc: 65.75%   [EVAL] batch:  363 | acc: 68.75%,  total acc: 65.76%   [EVAL] batch:  364 | acc: 62.50%,  total acc: 65.75%   [EVAL] batch:  365 | acc: 50.00%,  total acc: 65.71%   [EVAL] batch:  366 | acc: 62.50%,  total acc: 65.70%   [EVAL] batch:  367 | acc: 56.25%,  total acc: 65.68%   [EVAL] batch:  368 | acc: 31.25%,  total acc: 65.58%   [EVAL] batch:  369 | acc: 81.25%,  total acc: 65.62%   [EVAL] batch:  370 | acc: 50.00%,  total acc: 65.58%   [EVAL] batch:  371 | acc: 62.50%,  total acc: 65.57%   [EVAL] batch:  372 | acc: 81.25%,  total acc: 65.62%   [EVAL] batch:  373 | acc: 87.50%,  total acc: 65.68%   [EVAL] batch:  374 | acc: 81.25%,  total acc: 65.72%   [EVAL] batch:  375 | acc: 68.75%,  total acc: 65.72%   [EVAL] batch:  376 | acc: 50.00%,  total acc: 65.68%   [EVAL] batch:  377 | acc: 37.50%,  total acc: 65.61%   [EVAL] batch:  378 | acc: 37.50%,  total acc: 65.53%   [EVAL] batch:  379 | acc: 56.25%,  total acc: 65.51%   [EVAL] batch:  380 | acc: 56.25%,  total acc: 65.49%   [EVAL] batch:  381 | acc: 93.75%,  total acc: 65.56%   [EVAL] batch:  382 | acc: 93.75%,  total acc: 65.63%   [EVAL] batch:  383 | acc: 87.50%,  total acc: 65.69%   [EVAL] batch:  384 | acc: 87.50%,  total acc: 65.75%   [EVAL] batch:  385 | acc: 87.50%,  total acc: 65.80%   [EVAL] batch:  386 | acc: 100.00%,  total acc: 65.89%   [EVAL] batch:  387 | acc: 81.25%,  total acc: 65.93%   [EVAL] batch:  388 | acc: 56.25%,  total acc: 65.91%   [EVAL] batch:  389 | acc: 68.75%,  total acc: 65.91%   [EVAL] batch:  390 | acc: 68.75%,  total acc: 65.92%   [EVAL] batch:  391 | acc: 81.25%,  total acc: 65.96%   [EVAL] batch:  392 | acc: 62.50%,  total acc: 65.95%   [EVAL] batch:  393 | acc: 75.00%,  total acc: 65.97%   [EVAL] batch:  394 | acc: 68.75%,  total acc: 65.98%   [EVAL] batch:  395 | acc: 81.25%,  total acc: 66.02%   [EVAL] batch:  396 | acc: 75.00%,  total acc: 66.04%   [EVAL] batch:  397 | acc: 75.00%,  total acc: 66.06%   [EVAL] batch:  398 | acc: 62.50%,  total acc: 66.06%   [EVAL] batch:  399 | acc: 56.25%,  total acc: 66.03%   [EVAL] batch:  400 | acc: 37.50%,  total acc: 65.96%   [EVAL] batch:  401 | acc: 81.25%,  total acc: 66.00%   [EVAL] batch:  402 | acc: 43.75%,  total acc: 65.94%   [EVAL] batch:  403 | acc: 56.25%,  total acc: 65.92%   [EVAL] batch:  404 | acc: 37.50%,  total acc: 65.85%   [EVAL] batch:  405 | acc: 31.25%,  total acc: 65.76%   [EVAL] batch:  406 | acc: 56.25%,  total acc: 65.74%   [EVAL] batch:  407 | acc: 50.00%,  total acc: 65.70%   [EVAL] batch:  408 | acc: 43.75%,  total acc: 65.65%   [EVAL] batch:  409 | acc: 50.00%,  total acc: 65.61%   [EVAL] batch:  410 | acc: 62.50%,  total acc: 65.60%   [EVAL] batch:  411 | acc: 50.00%,  total acc: 65.56%   [EVAL] batch:  412 | acc: 62.50%,  total acc: 65.56%   [EVAL] batch:  413 | acc: 81.25%,  total acc: 65.59%   [EVAL] batch:  414 | acc: 81.25%,  total acc: 65.63%   [EVAL] batch:  415 | acc: 87.50%,  total acc: 65.69%   [EVAL] batch:  416 | acc: 81.25%,  total acc: 65.72%   [EVAL] batch:  417 | acc: 87.50%,  total acc: 65.77%   [EVAL] batch:  418 | acc: 56.25%,  total acc: 65.75%   [EVAL] batch:  419 | acc: 25.00%,  total acc: 65.65%   [EVAL] batch:  420 | acc: 37.50%,  total acc: 65.59%   [EVAL] batch:  421 | acc: 25.00%,  total acc: 65.49%   [EVAL] batch:  422 | acc: 43.75%,  total acc: 65.44%   [EVAL] batch:  423 | acc: 31.25%,  total acc: 65.36%   [EVAL] batch:  424 | acc: 50.00%,  total acc: 65.32%   [EVAL] batch:  425 | acc: 100.00%,  total acc: 65.40%   [EVAL] batch:  426 | acc: 100.00%,  total acc: 65.49%   [EVAL] batch:  427 | acc: 100.00%,  total acc: 65.57%   [EVAL] batch:  428 | acc: 100.00%,  total acc: 65.65%   [EVAL] batch:  429 | acc: 100.00%,  total acc: 65.73%   [EVAL] batch:  430 | acc: 100.00%,  total acc: 65.81%   [EVAL] batch:  431 | acc: 62.50%,  total acc: 65.80%   [EVAL] batch:  432 | acc: 43.75%,  total acc: 65.75%   [EVAL] batch:  433 | acc: 50.00%,  total acc: 65.71%   [EVAL] batch:  434 | acc: 68.75%,  total acc: 65.72%   [EVAL] batch:  435 | acc: 68.75%,  total acc: 65.73%   [EVAL] batch:  436 | acc: 56.25%,  total acc: 65.70%   [EVAL] batch:  437 | acc: 68.75%,  total acc: 65.71%   [EVAL] batch:  438 | acc: 62.50%,  total acc: 65.70%   [EVAL] batch:  439 | acc: 62.50%,  total acc: 65.70%   [EVAL] batch:  440 | acc: 75.00%,  total acc: 65.72%   [EVAL] batch:  441 | acc: 75.00%,  total acc: 65.74%   [EVAL] batch:  442 | acc: 62.50%,  total acc: 65.73%   [EVAL] batch:  443 | acc: 56.25%,  total acc: 65.71%   [EVAL] batch:  444 | acc: 50.00%,  total acc: 65.67%   [EVAL] batch:  445 | acc: 68.75%,  total acc: 65.68%   [EVAL] batch:  446 | acc: 37.50%,  total acc: 65.62%   [EVAL] batch:  447 | acc: 62.50%,  total acc: 65.61%   [EVAL] batch:  448 | acc: 43.75%,  total acc: 65.56%   [EVAL] batch:  449 | acc: 68.75%,  total acc: 65.57%   [EVAL] batch:  450 | acc: 37.50%,  total acc: 65.51%   [EVAL] batch:  451 | acc: 18.75%,  total acc: 65.40%   [EVAL] batch:  452 | acc: 12.50%,  total acc: 65.29%   [EVAL] batch:  453 | acc: 31.25%,  total acc: 65.21%   [EVAL] batch:  454 | acc: 50.00%,  total acc: 65.18%   [EVAL] batch:  455 | acc: 25.00%,  total acc: 65.09%   [EVAL] batch:  456 | acc: 75.00%,  total acc: 65.11%   [EVAL] batch:  457 | acc: 100.00%,  total acc: 65.19%   [EVAL] batch:  458 | acc: 100.00%,  total acc: 65.26%   [EVAL] batch:  459 | acc: 100.00%,  total acc: 65.34%   [EVAL] batch:  460 | acc: 100.00%,  total acc: 65.41%   [EVAL] batch:  461 | acc: 93.75%,  total acc: 65.48%   [EVAL] batch:  462 | acc: 56.25%,  total acc: 65.46%   [EVAL] batch:  463 | acc: 0.00%,  total acc: 65.32%   [EVAL] batch:  464 | acc: 12.50%,  total acc: 65.20%   [EVAL] batch:  465 | acc: 18.75%,  total acc: 65.10%   [EVAL] batch:  466 | acc: 18.75%,  total acc: 65.00%   [EVAL] batch:  467 | acc: 6.25%,  total acc: 64.88%   [EVAL] batch:  468 | acc: 31.25%,  total acc: 64.81%   [EVAL] batch:  469 | acc: 93.75%,  total acc: 64.87%   [EVAL] batch:  470 | acc: 93.75%,  total acc: 64.93%   [EVAL] batch:  471 | acc: 93.75%,  total acc: 64.99%   [EVAL] batch:  472 | acc: 87.50%,  total acc: 65.04%   [EVAL] batch:  473 | acc: 93.75%,  total acc: 65.10%   [EVAL] batch:  474 | acc: 100.00%,  total acc: 65.17%   [EVAL] batch:  475 | acc: 50.00%,  total acc: 65.14%   [EVAL] batch:  476 | acc: 87.50%,  total acc: 65.19%   [EVAL] batch:  477 | acc: 50.00%,  total acc: 65.15%   [EVAL] batch:  478 | acc: 43.75%,  total acc: 65.11%   [EVAL] batch:  479 | acc: 62.50%,  total acc: 65.10%   [EVAL] batch:  480 | acc: 50.00%,  total acc: 65.07%   [EVAL] batch:  481 | acc: 50.00%,  total acc: 65.04%   [EVAL] batch:  482 | acc: 81.25%,  total acc: 65.08%   [EVAL] batch:  483 | acc: 81.25%,  total acc: 65.11%   [EVAL] batch:  484 | acc: 75.00%,  total acc: 65.13%   [EVAL] batch:  485 | acc: 62.50%,  total acc: 65.12%   [EVAL] batch:  486 | acc: 75.00%,  total acc: 65.14%   [EVAL] batch:  487 | acc: 43.75%,  total acc: 65.10%   [EVAL] batch:  488 | acc: 37.50%,  total acc: 65.04%   [EVAL] batch:  489 | acc: 68.75%,  total acc: 65.05%   [EVAL] batch:  490 | acc: 56.25%,  total acc: 65.03%   [EVAL] batch:  491 | acc: 68.75%,  total acc: 65.04%   [EVAL] batch:  492 | acc: 50.00%,  total acc: 65.01%   [EVAL] batch:  493 | acc: 68.75%,  total acc: 65.02%   [EVAL] batch:  494 | acc: 93.75%,  total acc: 65.08%   [EVAL] batch:  495 | acc: 93.75%,  total acc: 65.13%   [EVAL] batch:  496 | acc: 81.25%,  total acc: 65.17%   [EVAL] batch:  497 | acc: 87.50%,  total acc: 65.21%   [EVAL] batch:  498 | acc: 81.25%,  total acc: 65.24%   [EVAL] batch:  499 | acc: 81.25%,  total acc: 65.28%   
cur_acc:  ['0.9494', '0.8244', '0.5962', '0.7173', '0.7708', '0.6726', '0.6369', '0.6171']
his_acc:  ['0.9494', '0.8760', '0.7746', '0.7435', '0.7324', '0.7028', '0.6737', '0.6528']
--------Round  5
seed:  600
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hybridprompt_12token/train.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hybridprompt_12token/valid.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hybridprompt_12token/test.pkl
Task_order: [7 2 0 1 6 3 4 5]
prepared data!
Clustering into  4  clusters
Clusters:  [0 3 2 0 1 1 2 0 3 3]
Losses:  18.191686630249023 3.9093708992004395 1.2311686277389526
CurrentTrain: epoch  0, batch     0 | loss: 18.1916866Losses:  19.133758544921875 5.368856430053711 0.9722613096237183
CurrentTrain: epoch  0, batch     1 | loss: 19.1337585Losses:  19.64826202392578 5.558585166931152 1.2100872993469238
CurrentTrain: epoch  0, batch     2 | loss: 19.6482620Losses:  18.444961547851562 4.485507011413574 1.1457191705703735
CurrentTrain: epoch  0, batch     3 | loss: 18.4449615Losses:  19.935779571533203 6.393089294433594 1.0827069282531738
CurrentTrain: epoch  0, batch     4 | loss: 19.9357796Losses:  17.991743087768555 4.79251766204834 1.0189945697784424
CurrentTrain: epoch  0, batch     5 | loss: 17.9917431Losses:  19.051607131958008 6.178459167480469 0.9725342988967896
CurrentTrain: epoch  0, batch     6 | loss: 19.0516071Losses:  19.785696029663086 7.0547003746032715 0.9279911518096924
CurrentTrain: epoch  0, batch     7 | loss: 19.7856960Losses:  17.889930725097656 5.251070022583008 0.8897184133529663
CurrentTrain: epoch  0, batch     8 | loss: 17.8899307Losses:  18.827877044677734 6.366794586181641 0.8713865876197815
CurrentTrain: epoch  0, batch     9 | loss: 18.8278770Losses:  17.866527557373047 5.942898273468018 0.8259394764900208
CurrentTrain: epoch  0, batch    10 | loss: 17.8665276Losses:  17.932723999023438 5.954112529754639 0.8722162246704102
CurrentTrain: epoch  0, batch    11 | loss: 17.9327240Losses:  15.725098609924316 3.693716526031494 0.8012130260467529
CurrentTrain: epoch  0, batch    12 | loss: 15.7250986Losses:  16.347326278686523 4.9637651443481445 0.7517184019088745
CurrentTrain: epoch  0, batch    13 | loss: 16.3473263Losses:  15.938163757324219 4.060240268707275 0.77529376745224
CurrentTrain: epoch  0, batch    14 | loss: 15.9381638Losses:  16.704483032226562 5.219937801361084 0.6964958310127258
CurrentTrain: epoch  0, batch    15 | loss: 16.7044830Losses:  16.922895431518555 5.602421760559082 0.6491862535476685
CurrentTrain: epoch  0, batch    16 | loss: 16.9228954Losses:  15.635274887084961 4.533978462219238 0.6698832511901855
CurrentTrain: epoch  0, batch    17 | loss: 15.6352749Losses:  16.814043045043945 5.968944072723389 0.6519508957862854
CurrentTrain: epoch  0, batch    18 | loss: 16.8140430Losses:  16.310495376586914 5.88295841217041 0.5510684251785278
CurrentTrain: epoch  0, batch    19 | loss: 16.3104954Losses:  16.437517166137695 5.1253156661987305 0.6354577541351318
CurrentTrain: epoch  0, batch    20 | loss: 16.4375172Losses:  17.246028900146484 5.894294738769531 0.6167349815368652
CurrentTrain: epoch  0, batch    21 | loss: 17.2460289Losses:  14.457148551940918 3.2899210453033447 0.5729116201400757
CurrentTrain: epoch  0, batch    22 | loss: 14.4571486Losses:  16.542377471923828 5.199751377105713 0.5623180866241455
CurrentTrain: epoch  0, batch    23 | loss: 16.5423775Losses:  18.74199104309082 7.375507831573486 0.5093607902526855
CurrentTrain: epoch  0, batch    24 | loss: 18.7419910Losses:  14.579160690307617 3.591676950454712 0.5178924798965454
CurrentTrain: epoch  0, batch    25 | loss: 14.5791607Losses:  15.441750526428223 4.636494159698486 0.565537691116333
CurrentTrain: epoch  0, batch    26 | loss: 15.4417505Losses:  16.431344985961914 5.984414100646973 0.513936460018158
CurrentTrain: epoch  0, batch    27 | loss: 16.4313450Losses:  15.084352493286133 4.357143878936768 0.5607967972755432
CurrentTrain: epoch  0, batch    28 | loss: 15.0843525Losses:  14.162199974060059 3.650250196456909 0.5459483861923218
CurrentTrain: epoch  0, batch    29 | loss: 14.1622000Losses:  14.829737663269043 4.056807518005371 0.5330312252044678
CurrentTrain: epoch  0, batch    30 | loss: 14.8297377Losses:  14.72270393371582 4.054058074951172 0.5395733118057251
CurrentTrain: epoch  0, batch    31 | loss: 14.7227039Losses:  15.965777397155762 6.170612335205078 0.5151090621948242
CurrentTrain: epoch  0, batch    32 | loss: 15.9657774Losses:  15.373046875 5.402972221374512 0.5016923546791077
CurrentTrain: epoch  0, batch    33 | loss: 15.3730469Losses:  13.454233169555664 3.5306029319763184 0.5088123679161072
CurrentTrain: epoch  0, batch    34 | loss: 13.4542332Losses:  14.801017761230469 4.637718677520752 0.492801308631897
CurrentTrain: epoch  0, batch    35 | loss: 14.8010178Losses:  15.760828018188477 5.854743003845215 0.4842240810394287
CurrentTrain: epoch  0, batch    36 | loss: 15.7608280Losses:  13.563345909118652 3.7800722122192383 0.5036238431930542
CurrentTrain: epoch  0, batch    37 | loss: 13.5633459Losses:  15.572216033935547 5.920535087585449 0.46181976795196533
CurrentTrain: epoch  0, batch    38 | loss: 15.5722160Losses:  14.395218849182129 5.2181901931762695 0.4916042983531952
CurrentTrain: epoch  0, batch    39 | loss: 14.3952188Losses:  15.975564002990723 6.597788333892822 0.46786585450172424
CurrentTrain: epoch  0, batch    40 | loss: 15.9755640Losses:  12.375877380371094 2.967522144317627 0.4757100045681
CurrentTrain: epoch  0, batch    41 | loss: 12.3758774Losses:  13.640275955200195 4.29763650894165 0.4755667448043823
CurrentTrain: epoch  0, batch    42 | loss: 13.6402760Losses:  14.228034019470215 4.3342437744140625 0.4761350154876709
CurrentTrain: epoch  0, batch    43 | loss: 14.2280340Losses:  15.286616325378418 5.519040107727051 0.460560142993927
CurrentTrain: epoch  0, batch    44 | loss: 15.2866163Losses:  13.192618370056152 3.859576940536499 0.4628227949142456
CurrentTrain: epoch  0, batch    45 | loss: 13.1926184Losses:  14.961859703063965 4.967767715454102 0.4601592421531677
CurrentTrain: epoch  0, batch    46 | loss: 14.9618597Losses:  15.40279769897461 5.9753265380859375 0.47472715377807617
CurrentTrain: epoch  0, batch    47 | loss: 15.4027977Losses:  12.203917503356934 3.6057016849517822 0.42175614833831787
CurrentTrain: epoch  0, batch    48 | loss: 12.2039175Losses:  13.797760009765625 4.201122283935547 0.45708101987838745
CurrentTrain: epoch  0, batch    49 | loss: 13.7977600Losses:  13.557016372680664 4.861051082611084 0.4024145007133484
CurrentTrain: epoch  0, batch    50 | loss: 13.5570164Losses:  14.245240211486816 4.916429042816162 0.35361167788505554
CurrentTrain: epoch  0, batch    51 | loss: 14.2452402Losses:  13.115610122680664 4.429728984832764 0.4326133131980896
CurrentTrain: epoch  0, batch    52 | loss: 13.1156101Losses:  12.736363410949707 3.789841890335083 0.4008689224720001
CurrentTrain: epoch  0, batch    53 | loss: 12.7363634Losses:  16.081829071044922 6.535491466522217 0.4282785654067993
CurrentTrain: epoch  0, batch    54 | loss: 16.0818291Losses:  11.212126731872559 2.8684513568878174 0.4222937226295471
CurrentTrain: epoch  0, batch    55 | loss: 11.2121267Losses:  14.246317863464355 5.303250312805176 0.2956487536430359
CurrentTrain: epoch  0, batch    56 | loss: 14.2463179Losses:  13.878866195678711 5.421564102172852 0.2875671088695526
CurrentTrain: epoch  0, batch    57 | loss: 13.8788662Losses:  11.457212448120117 3.7463135719299316 0.3845714330673218
CurrentTrain: epoch  0, batch    58 | loss: 11.4572124Losses:  12.42898941040039 4.305041313171387 0.38672298192977905
CurrentTrain: epoch  0, batch    59 | loss: 12.4289894Losses:  12.024133682250977 3.608323335647583 0.39770424365997314
CurrentTrain: epoch  0, batch    60 | loss: 12.0241337Losses:  13.380847930908203 5.721939563751221 0.3766366243362427
CurrentTrain: epoch  0, batch    61 | loss: 13.3808479Losses:  8.889734268188477 0.994418740272522 0.38150492310523987
CurrentTrain: epoch  0, batch    62 | loss: 8.8897343Losses:  14.249900817871094 6.337785720825195 0.34425193071365356
CurrentTrain: epoch  1, batch     0 | loss: 14.2499008Losses:  10.749585151672363 2.6121606826782227 0.3968046307563782
CurrentTrain: epoch  1, batch     1 | loss: 10.7495852Losses:  11.22136116027832 3.6163251399993896 0.35134419798851013
CurrentTrain: epoch  1, batch     2 | loss: 11.2213612Losses:  11.297872543334961 3.6790926456451416 0.364481657743454
CurrentTrain: epoch  1, batch     3 | loss: 11.2978725Losses:  11.101161003112793 3.190101385116577 0.359914094209671
CurrentTrain: epoch  1, batch     4 | loss: 11.1011610Losses:  13.887375831604004 5.253264427185059 0.39082127809524536
CurrentTrain: epoch  1, batch     5 | loss: 13.8873758Losses:  11.890551567077637 3.2819266319274902 0.41437241435050964
CurrentTrain: epoch  1, batch     6 | loss: 11.8905516Losses:  14.576347351074219 7.307720184326172 0.37712204456329346
CurrentTrain: epoch  1, batch     7 | loss: 14.5763474Losses:  14.04749870300293 5.395566940307617 0.4054201543331146
CurrentTrain: epoch  1, batch     8 | loss: 14.0474987Losses:  11.666162490844727 4.083691120147705 0.34186556935310364
CurrentTrain: epoch  1, batch     9 | loss: 11.6661625Losses:  12.111720085144043 3.7916343212127686 0.39095181226730347
CurrentTrain: epoch  1, batch    10 | loss: 12.1117201Losses:  11.3248872756958 3.4739551544189453 0.36510559916496277
CurrentTrain: epoch  1, batch    11 | loss: 11.3248873Losses:  11.493902206420898 3.3187484741210938 0.3954123854637146
CurrentTrain: epoch  1, batch    12 | loss: 11.4939022Losses:  10.938605308532715 2.8246471881866455 0.3893006443977356
CurrentTrain: epoch  1, batch    13 | loss: 10.9386053Losses:  11.69996452331543 3.4245917797088623 0.36368393898010254
CurrentTrain: epoch  1, batch    14 | loss: 11.6999645Losses:  10.217975616455078 2.5648770332336426 0.33961915969848633
CurrentTrain: epoch  1, batch    15 | loss: 10.2179756Losses:  9.93105411529541 2.2908639907836914 0.3662470281124115
CurrentTrain: epoch  1, batch    16 | loss: 9.9310541Losses:  10.099869728088379 2.1623787879943848 0.36683180928230286
CurrentTrain: epoch  1, batch    17 | loss: 10.0998697Losses:  12.161933898925781 4.677145481109619 0.3475880026817322
CurrentTrain: epoch  1, batch    18 | loss: 12.1619339Losses:  13.520383834838867 4.333106994628906 0.36506399512290955
CurrentTrain: epoch  1, batch    19 | loss: 13.5203838Losses:  11.285989761352539 3.2247791290283203 0.3637875020503998
CurrentTrain: epoch  1, batch    20 | loss: 11.2859898Losses:  12.378366470336914 5.216975212097168 0.3392142951488495
CurrentTrain: epoch  1, batch    21 | loss: 12.3783665Losses:  10.155688285827637 2.6287732124328613 0.3444734513759613
CurrentTrain: epoch  1, batch    22 | loss: 10.1556883Losses:  9.94192123413086 2.708615779876709 0.324905663728714
CurrentTrain: epoch  1, batch    23 | loss: 9.9419212Losses:  13.862100601196289 5.983994483947754 0.34211266040802
CurrentTrain: epoch  1, batch    24 | loss: 13.8621006Losses:  11.82490348815918 4.164188861846924 0.2712612748146057
CurrentTrain: epoch  1, batch    25 | loss: 11.8249035Losses:  11.64645004272461 3.3552772998809814 0.345659077167511
CurrentTrain: epoch  1, batch    26 | loss: 11.6464500Losses:  10.834031105041504 3.8368663787841797 0.3461484909057617
CurrentTrain: epoch  1, batch    27 | loss: 10.8340311Losses:  10.84561824798584 3.2643256187438965 0.3546034097671509
CurrentTrain: epoch  1, batch    28 | loss: 10.8456182Losses:  9.4119291305542 2.098094940185547 0.32919302582740784
CurrentTrain: epoch  1, batch    29 | loss: 9.4119291Losses:  10.801013946533203 3.4472222328186035 0.34663572907447815
CurrentTrain: epoch  1, batch    30 | loss: 10.8010139Losses:  13.251537322998047 5.463253498077393 0.3440154194831848
CurrentTrain: epoch  1, batch    31 | loss: 13.2515373Losses:  14.207174301147461 5.8015570640563965 0.3487420082092285
CurrentTrain: epoch  1, batch    32 | loss: 14.2071743Losses:  11.616969108581543 4.103771209716797 0.34985801577568054
CurrentTrain: epoch  1, batch    33 | loss: 11.6169691Losses:  11.507729530334473 3.5264976024627686 0.3631044626235962
CurrentTrain: epoch  1, batch    34 | loss: 11.5077295Losses:  9.907825469970703 2.984229564666748 0.32759732007980347
CurrentTrain: epoch  1, batch    35 | loss: 9.9078255Losses:  9.33674430847168 2.2054290771484375 0.3323667049407959
CurrentTrain: epoch  1, batch    36 | loss: 9.3367443Losses:  11.420454025268555 4.657068252563477 0.3439479470252991
CurrentTrain: epoch  1, batch    37 | loss: 11.4204540Losses:  16.217641830444336 8.863560676574707 0.35102200508117676
CurrentTrain: epoch  1, batch    38 | loss: 16.2176418Losses:  11.918177604675293 4.396416664123535 0.3443787693977356
CurrentTrain: epoch  1, batch    39 | loss: 11.9181776Losses:  10.422357559204102 3.394012212753296 0.35483816266059875
CurrentTrain: epoch  1, batch    40 | loss: 10.4223576Losses:  9.112815856933594 1.9852359294891357 0.330797016620636
CurrentTrain: epoch  1, batch    41 | loss: 9.1128159Losses:  13.74995231628418 5.647453784942627 0.2658999562263489
CurrentTrain: epoch  1, batch    42 | loss: 13.7499523Losses:  12.425477981567383 4.831301689147949 0.3646910786628723
CurrentTrain: epoch  1, batch    43 | loss: 12.4254780Losses:  9.82443618774414 2.5201797485351562 0.32754361629486084
CurrentTrain: epoch  1, batch    44 | loss: 9.8244362Losses:  12.646910667419434 5.440221786499023 0.3624069094657898
CurrentTrain: epoch  1, batch    45 | loss: 12.6469107Losses:  8.514137268066406 2.229860305786133 0.3220614194869995
CurrentTrain: epoch  1, batch    46 | loss: 8.5141373Losses:  9.779984474182129 3.191819190979004 0.35345038771629333
CurrentTrain: epoch  1, batch    47 | loss: 9.7799845Losses:  10.662698745727539 3.5091428756713867 0.3667929172515869
CurrentTrain: epoch  1, batch    48 | loss: 10.6626987Losses:  14.667394638061523 7.407319068908691 0.323307067155838
CurrentTrain: epoch  1, batch    49 | loss: 14.6673946Losses:  8.898026466369629 2.6165246963500977 0.31504693627357483
CurrentTrain: epoch  1, batch    50 | loss: 8.8980265Losses:  12.397359848022461 6.023298740386963 0.3209911286830902
CurrentTrain: epoch  1, batch    51 | loss: 12.3973598Losses:  9.780550956726074 3.4098281860351562 0.3336430788040161
CurrentTrain: epoch  1, batch    52 | loss: 9.7805510Losses:  10.847921371459961 3.5742688179016113 0.3281577527523041
CurrentTrain: epoch  1, batch    53 | loss: 10.8479214Losses:  9.648092269897461 3.117938995361328 0.3101619482040405
CurrentTrain: epoch  1, batch    54 | loss: 9.6480923Losses:  9.381808280944824 3.3919663429260254 0.30341631174087524
CurrentTrain: epoch  1, batch    55 | loss: 9.3818083Losses:  10.355451583862305 3.659062385559082 0.34308725595474243
CurrentTrain: epoch  1, batch    56 | loss: 10.3554516Losses:  9.07933235168457 2.4224696159362793 0.32802796363830566
CurrentTrain: epoch  1, batch    57 | loss: 9.0793324Losses:  9.797659873962402 3.347550868988037 0.33023715019226074
CurrentTrain: epoch  1, batch    58 | loss: 9.7976599Losses:  9.506564140319824 3.508146047592163 0.2993074655532837
CurrentTrain: epoch  1, batch    59 | loss: 9.5065641Losses:  10.394508361816406 3.767751455307007 0.3293169140815735
CurrentTrain: epoch  1, batch    60 | loss: 10.3945084Losses:  10.53927230834961 3.741283416748047 0.32013845443725586
CurrentTrain: epoch  1, batch    61 | loss: 10.5392723Losses:  6.687053203582764 0.47136715054512024 0.2992897629737854
CurrentTrain: epoch  1, batch    62 | loss: 6.6870532Losses:  9.633528709411621 3.4482221603393555 0.30133193731307983
CurrentTrain: epoch  2, batch     0 | loss: 9.6335287Losses:  12.079839706420898 4.332568168640137 0.3452390432357788
CurrentTrain: epoch  2, batch     1 | loss: 12.0798397Losses:  11.810247421264648 5.050928115844727 0.3048323690891266
CurrentTrain: epoch  2, batch     2 | loss: 11.8102474Losses:  9.082262992858887 2.915487289428711 0.29113197326660156
CurrentTrain: epoch  2, batch     3 | loss: 9.0822630Losses:  8.976055145263672 3.182555675506592 0.2915041148662567
CurrentTrain: epoch  2, batch     4 | loss: 8.9760551Losses:  10.19261360168457 3.148259162902832 0.2984340786933899
CurrentTrain: epoch  2, batch     5 | loss: 10.1926136Losses:  8.336813926696777 2.481625556945801 0.30323660373687744
CurrentTrain: epoch  2, batch     6 | loss: 8.3368139Losses:  8.498841285705566 2.1953611373901367 0.29711058735847473
CurrentTrain: epoch  2, batch     7 | loss: 8.4988413Losses:  9.331302642822266 3.200005054473877 0.2974015474319458
CurrentTrain: epoch  2, batch     8 | loss: 9.3313026Losses:  10.189962387084961 4.694034576416016 0.29869550466537476
CurrentTrain: epoch  2, batch     9 | loss: 10.1899624Losses:  9.91221809387207 3.5557150840759277 0.2805424630641937
CurrentTrain: epoch  2, batch    10 | loss: 9.9122181Losses:  8.788573265075684 2.7403066158294678 0.2836804687976837
CurrentTrain: epoch  2, batch    11 | loss: 8.7885733Losses:  9.76694393157959 3.8391969203948975 0.32762640714645386
CurrentTrain: epoch  2, batch    12 | loss: 9.7669439Losses:  11.85575008392334 3.898303985595703 0.30263063311576843
CurrentTrain: epoch  2, batch    13 | loss: 11.8557501Losses:  8.08317756652832 2.019954204559326 0.2956851124763489
CurrentTrain: epoch  2, batch    14 | loss: 8.0831776Losses:  10.27631950378418 4.116177558898926 0.2945409417152405
CurrentTrain: epoch  2, batch    15 | loss: 10.2763195Losses:  10.855672836303711 4.23166561126709 0.31183701753616333
CurrentTrain: epoch  2, batch    16 | loss: 10.8556728Losses:  9.198060989379883 2.9502360820770264 0.2849031686782837
CurrentTrain: epoch  2, batch    17 | loss: 9.1980610Losses:  8.708145141601562 2.231971502304077 0.28412872552871704
CurrentTrain: epoch  2, batch    18 | loss: 8.7081451Losses:  14.846541404724121 8.032722473144531 0.31687015295028687
CurrentTrain: epoch  2, batch    19 | loss: 14.8465414Losses:  9.998641967773438 3.6743109226226807 0.2790835201740265
CurrentTrain: epoch  2, batch    20 | loss: 9.9986420Losses:  10.719078063964844 4.284140110015869 0.2764008045196533
CurrentTrain: epoch  2, batch    21 | loss: 10.7190781Losses:  8.327486038208008 2.123253583908081 0.2765679359436035
CurrentTrain: epoch  2, batch    22 | loss: 8.3274860Losses:  8.093894004821777 2.433255672454834 0.26446646451950073
CurrentTrain: epoch  2, batch    23 | loss: 8.0938940Losses:  13.251260757446289 7.15875768661499 0.28373482823371887
CurrentTrain: epoch  2, batch    24 | loss: 13.2512608Losses:  9.77556037902832 3.333407402038574 0.29420995712280273
CurrentTrain: epoch  2, batch    25 | loss: 9.7755604Losses:  9.972088813781738 4.570273399353027 0.29637178778648376
CurrentTrain: epoch  2, batch    26 | loss: 9.9720888Losses:  10.778965950012207 4.936896324157715 0.29828354716300964
CurrentTrain: epoch  2, batch    27 | loss: 10.7789660Losses:  9.322675704956055 2.1095316410064697 0.2741848826408386
CurrentTrain: epoch  2, batch    28 | loss: 9.3226757Losses:  13.827425956726074 8.739099502563477 0.2772282660007477
CurrentTrain: epoch  2, batch    29 | loss: 13.8274260Losses:  10.083608627319336 4.419867992401123 0.2788558304309845
CurrentTrain: epoch  2, batch    30 | loss: 10.0836086Losses:  10.9561767578125 5.030774116516113 0.3004738688468933
CurrentTrain: epoch  2, batch    31 | loss: 10.9561768Losses:  10.042866706848145 4.388901710510254 0.2866191565990448
CurrentTrain: epoch  2, batch    32 | loss: 10.0428667Losses:  11.000584602355957 5.569736480712891 0.2944628596305847
CurrentTrain: epoch  2, batch    33 | loss: 11.0005846Losses:  11.998478889465332 5.830532550811768 0.2889814078807831
CurrentTrain: epoch  2, batch    34 | loss: 11.9984789Losses:  7.388376712799072 1.649390459060669 0.26595818996429443
CurrentTrain: epoch  2, batch    35 | loss: 7.3883767Losses:  10.697141647338867 4.858353137969971 0.28520190715789795
CurrentTrain: epoch  2, batch    36 | loss: 10.6971416Losses:  12.347926139831543 5.706859588623047 0.2710243761539459
CurrentTrain: epoch  2, batch    37 | loss: 12.3479261Losses:  10.217705726623535 4.254088401794434 0.29992610216140747
CurrentTrain: epoch  2, batch    38 | loss: 10.2177057Losses:  12.144657135009766 6.02849006652832 0.2819288372993469
CurrentTrain: epoch  2, batch    39 | loss: 12.1446571Losses:  8.292952537536621 2.536807060241699 0.27163398265838623
CurrentTrain: epoch  2, batch    40 | loss: 8.2929525Losses:  8.9649019241333 3.5251643657684326 0.29271167516708374
CurrentTrain: epoch  2, batch    41 | loss: 8.9649019Losses:  9.228453636169434 3.28671932220459 0.30626845359802246
CurrentTrain: epoch  2, batch    42 | loss: 9.2284536Losses:  10.331267356872559 4.906433582305908 0.2834348976612091
CurrentTrain: epoch  2, batch    43 | loss: 10.3312674Losses:  7.405350685119629 2.1801934242248535 0.2676827311515808
CurrentTrain: epoch  2, batch    44 | loss: 7.4053507Losses:  9.922075271606445 3.5292181968688965 0.27727484703063965
CurrentTrain: epoch  2, batch    45 | loss: 9.9220753Losses:  8.099420547485352 2.154989242553711 0.2596794366836548
CurrentTrain: epoch  2, batch    46 | loss: 8.0994205Losses:  9.507473945617676 3.443584442138672 0.28945109248161316
CurrentTrain: epoch  2, batch    47 | loss: 9.5074739Losses:  9.525806427001953 3.420523166656494 0.26548898220062256
CurrentTrain: epoch  2, batch    48 | loss: 9.5258064Losses:  8.520792007446289 2.8063228130340576 0.2742895483970642
CurrentTrain: epoch  2, batch    49 | loss: 8.5207920Losses:  10.299910545349121 4.232050895690918 0.29222527146339417
CurrentTrain: epoch  2, batch    50 | loss: 10.2999105Losses:  10.488743782043457 5.002350807189941 0.28155332803726196
CurrentTrain: epoch  2, batch    51 | loss: 10.4887438Losses:  10.398055076599121 4.827244758605957 0.27224746346473694
CurrentTrain: epoch  2, batch    52 | loss: 10.3980551Losses:  11.880338668823242 7.105377197265625 0.2698710560798645
CurrentTrain: epoch  2, batch    53 | loss: 11.8803387Losses:  8.183111190795898 2.5753707885742188 0.2956559956073761
CurrentTrain: epoch  2, batch    54 | loss: 8.1831112Losses:  9.613977432250977 3.5613973140716553 0.2729277014732361
CurrentTrain: epoch  2, batch    55 | loss: 9.6139774Losses:  9.548759460449219 3.7715976238250732 0.28047266602516174
CurrentTrain: epoch  2, batch    56 | loss: 9.5487595Losses:  7.691082954406738 2.5030651092529297 0.25729358196258545
CurrentTrain: epoch  2, batch    57 | loss: 7.6910830Losses:  9.028497695922852 3.2429606914520264 0.2738291025161743
CurrentTrain: epoch  2, batch    58 | loss: 9.0284977Losses:  8.197423934936523 2.7290163040161133 0.27345186471939087
CurrentTrain: epoch  2, batch    59 | loss: 8.1974239Losses:  7.564176559448242 2.590296745300293 0.26092463731765747
CurrentTrain: epoch  2, batch    60 | loss: 7.5641766Losses:  9.693541526794434 3.963057041168213 0.30857568979263306
CurrentTrain: epoch  2, batch    61 | loss: 9.6935415Losses:  7.545809745788574 0.3082183003425598 0.29399171471595764
CurrentTrain: epoch  2, batch    62 | loss: 7.5458097Losses:  10.216663360595703 5.432820796966553 0.19505023956298828
CurrentTrain: epoch  3, batch     0 | loss: 10.2166634Losses:  10.387228965759277 4.998818397521973 0.28423428535461426
CurrentTrain: epoch  3, batch     1 | loss: 10.3872290Losses:  7.454371929168701 2.032350540161133 0.2574332356452942
CurrentTrain: epoch  3, batch     2 | loss: 7.4543719Losses:  8.455750465393066 3.3860740661621094 0.27963337302207947
CurrentTrain: epoch  3, batch     3 | loss: 8.4557505Losses:  7.982424736022949 2.5044872760772705 0.2468670904636383
CurrentTrain: epoch  3, batch     4 | loss: 7.9824247Losses:  7.898897171020508 2.9635958671569824 0.25223928689956665
CurrentTrain: epoch  3, batch     5 | loss: 7.8988972Losses:  7.671576499938965 2.540571689605713 0.26417720317840576
CurrentTrain: epoch  3, batch     6 | loss: 7.6715765Losses:  7.9905104637146 2.958238363265991 0.2549206018447876
CurrentTrain: epoch  3, batch     7 | loss: 7.9905105Losses:  11.088988304138184 4.012667179107666 0.255083829164505
CurrentTrain: epoch  3, batch     8 | loss: 11.0889883Losses:  7.474331855773926 2.605041742324829 0.2634563148021698
CurrentTrain: epoch  3, batch     9 | loss: 7.4743319Losses:  8.692404747009277 2.8268420696258545 0.2679072320461273
CurrentTrain: epoch  3, batch    10 | loss: 8.6924047Losses:  10.20617389678955 4.903939247131348 0.2635311782360077
CurrentTrain: epoch  3, batch    11 | loss: 10.2061739Losses:  7.605480194091797 1.8689491748809814 0.2574654519557953
CurrentTrain: epoch  3, batch    12 | loss: 7.6054802Losses:  9.629902839660645 4.283867359161377 0.2751529812812805
CurrentTrain: epoch  3, batch    13 | loss: 9.6299028Losses:  7.984886169433594 2.807554244995117 0.26655566692352295
CurrentTrain: epoch  3, batch    14 | loss: 7.9848862Losses:  10.463903427124023 5.219470977783203 0.2736685872077942
CurrentTrain: epoch  3, batch    15 | loss: 10.4639034Losses:  7.561053276062012 1.8788976669311523 0.25319093465805054
CurrentTrain: epoch  3, batch    16 | loss: 7.5610533Losses:  7.179571628570557 2.042088270187378 0.24843916296958923
CurrentTrain: epoch  3, batch    17 | loss: 7.1795716Losses:  9.917596817016602 4.015129566192627 0.2684251368045807
CurrentTrain: epoch  3, batch    18 | loss: 9.9175968Losses:  7.585602760314941 2.682915449142456 0.2624233365058899
CurrentTrain: epoch  3, batch    19 | loss: 7.5856028Losses:  9.075652122497559 2.9928739070892334 0.276183545589447
CurrentTrain: epoch  3, batch    20 | loss: 9.0756521Losses:  9.08111572265625 4.193719863891602 0.17262610793113708
CurrentTrain: epoch  3, batch    21 | loss: 9.0811157Losses:  13.670980453491211 8.601335525512695 0.26712167263031006
CurrentTrain: epoch  3, batch    22 | loss: 13.6709805Losses:  8.0912504196167 2.863252639770508 0.29970014095306396
CurrentTrain: epoch  3, batch    23 | loss: 8.0912504Losses:  7.386446475982666 2.116818428039551 0.2544110417366028
CurrentTrain: epoch  3, batch    24 | loss: 7.3864465Losses:  7.909580707550049 2.7267355918884277 0.26725223660469055
CurrentTrain: epoch  3, batch    25 | loss: 7.9095807Losses:  7.4774861335754395 2.0243117809295654 0.2551018297672272
CurrentTrain: epoch  3, batch    26 | loss: 7.4774861Losses:  9.868785858154297 4.245362281799316 0.3141908645629883
CurrentTrain: epoch  3, batch    27 | loss: 9.8687859Losses:  9.994709014892578 4.752215385437012 0.29769352078437805
CurrentTrain: epoch  3, batch    28 | loss: 9.9947090Losses:  7.917335510253906 2.7884271144866943 0.27279675006866455
CurrentTrain: epoch  3, batch    29 | loss: 7.9173355Losses:  11.026957511901855 5.875000953674316 0.2835931181907654
CurrentTrain: epoch  3, batch    30 | loss: 11.0269575Losses:  7.974392414093018 2.919612169265747 0.24196398258209229
CurrentTrain: epoch  3, batch    31 | loss: 7.9743924Losses:  10.861932754516602 5.246724605560303 0.26986587047576904
CurrentTrain: epoch  3, batch    32 | loss: 10.8619328Losses:  9.579547882080078 4.116723537445068 0.29192307591438293
CurrentTrain: epoch  3, batch    33 | loss: 9.5795479Losses:  6.639550685882568 1.8082114458084106 0.2462727129459381
CurrentTrain: epoch  3, batch    34 | loss: 6.6395507Losses:  8.06709098815918 2.5603175163269043 0.2725422978401184
CurrentTrain: epoch  3, batch    35 | loss: 8.0670910Losses:  7.390264511108398 2.6427268981933594 0.24761295318603516
CurrentTrain: epoch  3, batch    36 | loss: 7.3902645Losses:  11.62037467956543 6.388885021209717 0.2867891490459442
CurrentTrain: epoch  3, batch    37 | loss: 11.6203747Losses:  9.006178855895996 3.7267234325408936 0.27665579319000244
CurrentTrain: epoch  3, batch    38 | loss: 9.0061789Losses:  10.094531059265137 5.032900810241699 0.26888197660446167
CurrentTrain: epoch  3, batch    39 | loss: 10.0945311Losses:  8.882275581359863 4.131255149841309 0.2519388794898987
CurrentTrain: epoch  3, batch    40 | loss: 8.8822756Losses:  7.553613185882568 2.3981759548187256 0.24561163783073425
CurrentTrain: epoch  3, batch    41 | loss: 7.5536132Losses:  7.788111209869385 2.6422760486602783 0.27298396825790405
CurrentTrain: epoch  3, batch    42 | loss: 7.7881112Losses:  9.252398490905762 4.234742164611816 0.27203232049942017
CurrentTrain: epoch  3, batch    43 | loss: 9.2523985Losses:  7.664976119995117 2.265523672103882 0.25704967975616455
CurrentTrain: epoch  3, batch    44 | loss: 7.6649761Losses:  6.9951605796813965 2.037370204925537 0.24591857194900513
CurrentTrain: epoch  3, batch    45 | loss: 6.9951606Losses:  8.526737213134766 3.384469509124756 0.2797892987728119
CurrentTrain: epoch  3, batch    46 | loss: 8.5267372Losses:  8.065448760986328 3.3073177337646484 0.2524108588695526
CurrentTrain: epoch  3, batch    47 | loss: 8.0654488Losses:  9.008491516113281 4.185235500335693 0.26111263036727905
CurrentTrain: epoch  3, batch    48 | loss: 9.0084915Losses:  6.995919227600098 2.1426455974578857 0.2513189911842346
CurrentTrain: epoch  3, batch    49 | loss: 6.9959192Losses:  7.004258155822754 2.207784414291382 0.2538090944290161
CurrentTrain: epoch  3, batch    50 | loss: 7.0042582Losses:  9.354517936706543 4.107975006103516 0.25091227889060974
CurrentTrain: epoch  3, batch    51 | loss: 9.3545179Losses:  8.20506763458252 2.3774826526641846 0.2480844259262085
CurrentTrain: epoch  3, batch    52 | loss: 8.2050676Losses:  11.329142570495605 5.970147609710693 0.28076452016830444
CurrentTrain: epoch  3, batch    53 | loss: 11.3291426Losses:  7.276435852050781 1.640197992324829 0.2414177805185318
CurrentTrain: epoch  3, batch    54 | loss: 7.2764359Losses:  10.300503730773926 4.850322246551514 0.27084654569625854
CurrentTrain: epoch  3, batch    55 | loss: 10.3005037Losses:  7.712272644042969 2.7166733741760254 0.23377281427383423
CurrentTrain: epoch  3, batch    56 | loss: 7.7122726Losses:  8.47524642944336 3.5861387252807617 0.2682204246520996
CurrentTrain: epoch  3, batch    57 | loss: 8.4752464Losses:  8.960041046142578 3.066922664642334 0.26184362173080444
CurrentTrain: epoch  3, batch    58 | loss: 8.9600410Losses:  10.805421829223633 4.660119533538818 0.24750861525535583
CurrentTrain: epoch  3, batch    59 | loss: 10.8054218Losses:  10.842784881591797 5.484772682189941 0.27935993671417236
CurrentTrain: epoch  3, batch    60 | loss: 10.8427849Losses:  7.178277015686035 2.0361759662628174 0.23646730184555054
CurrentTrain: epoch  3, batch    61 | loss: 7.1782770Losses:  5.313479900360107 0.8201413750648499 0.17340755462646484
CurrentTrain: epoch  3, batch    62 | loss: 5.3134799Losses:  9.925074577331543 4.785472869873047 0.1491069346666336
CurrentTrain: epoch  4, batch     0 | loss: 9.9250746Losses:  7.187894821166992 2.2178955078125 0.239793598651886
CurrentTrain: epoch  4, batch     1 | loss: 7.1878948Losses:  9.108444213867188 4.261659145355225 0.24876432120800018
CurrentTrain: epoch  4, batch     2 | loss: 9.1084442Losses:  7.234492301940918 1.9881988763809204 0.24258898198604584
CurrentTrain: epoch  4, batch     3 | loss: 7.2344923Losses:  10.738801002502441 4.505277156829834 0.24642130732536316
CurrentTrain: epoch  4, batch     4 | loss: 10.7388010Losses:  7.463890075683594 2.4147298336029053 0.24383170902729034
CurrentTrain: epoch  4, batch     5 | loss: 7.4638901Losses:  10.444852828979492 5.001331329345703 0.3266884684562683
CurrentTrain: epoch  4, batch     6 | loss: 10.4448528Losses:  7.918972492218018 3.1610708236694336 0.23943674564361572
CurrentTrain: epoch  4, batch     7 | loss: 7.9189725Losses:  7.031816482543945 2.18998384475708 0.24210408329963684
CurrentTrain: epoch  4, batch     8 | loss: 7.0318165Losses:  8.952009201049805 3.6014564037323 0.17575329542160034
CurrentTrain: epoch  4, batch     9 | loss: 8.9520092Losses:  11.276697158813477 6.29104471206665 0.253772497177124
CurrentTrain: epoch  4, batch    10 | loss: 11.2766972Losses:  9.725687980651855 4.8515472412109375 0.2544734477996826
CurrentTrain: epoch  4, batch    11 | loss: 9.7256880Losses:  8.805757522583008 3.492875576019287 0.24535733461380005
CurrentTrain: epoch  4, batch    12 | loss: 8.8057575Losses:  8.645598411560059 3.4941601753234863 0.2570871114730835
CurrentTrain: epoch  4, batch    13 | loss: 8.6455984Losses:  7.1475396156311035 2.250420570373535 0.15575727820396423
CurrentTrain: epoch  4, batch    14 | loss: 7.1475396Losses:  11.034886360168457 5.053711891174316 0.26059257984161377
CurrentTrain: epoch  4, batch    15 | loss: 11.0348864Losses:  8.077287673950195 2.869666337966919 0.2524101734161377
CurrentTrain: epoch  4, batch    16 | loss: 8.0772877Losses:  7.129438400268555 2.4160494804382324 0.27168601751327515
CurrentTrain: epoch  4, batch    17 | loss: 7.1294384Losses:  7.694037914276123 2.679443597793579 0.2479119598865509
CurrentTrain: epoch  4, batch    18 | loss: 7.6940379Losses:  7.086033821105957 2.0401813983917236 0.23244482278823853
CurrentTrain: epoch  4, batch    19 | loss: 7.0860338Losses:  7.29838228225708 2.418306589126587 0.24165482819080353
CurrentTrain: epoch  4, batch    20 | loss: 7.2983823Losses:  7.756137371063232 3.0242199897766113 0.25663456320762634
CurrentTrain: epoch  4, batch    21 | loss: 7.7561374Losses:  10.534415245056152 5.6562910079956055 0.28153514862060547
CurrentTrain: epoch  4, batch    22 | loss: 10.5344152Losses:  8.615066528320312 2.7440943717956543 0.2672847509384155
CurrentTrain: epoch  4, batch    23 | loss: 8.6150665Losses:  8.095867156982422 2.8540024757385254 0.2613639533519745
CurrentTrain: epoch  4, batch    24 | loss: 8.0958672Losses:  8.570568084716797 3.9891772270202637 0.25767087936401367
CurrentTrain: epoch  4, batch    25 | loss: 8.5705681Losses:  9.185160636901855 4.531605243682861 0.25348708033561707
CurrentTrain: epoch  4, batch    26 | loss: 9.1851606Losses:  7.665746688842773 2.8927242755889893 0.24348565936088562
CurrentTrain: epoch  4, batch    27 | loss: 7.6657467Losses:  8.093387603759766 2.7179017066955566 0.24891245365142822
CurrentTrain: epoch  4, batch    28 | loss: 8.0933876Losses:  6.992257595062256 1.9225447177886963 0.24241022765636444
CurrentTrain: epoch  4, batch    29 | loss: 6.9922576Losses:  7.328005790710449 2.6141085624694824 0.23319858312606812
CurrentTrain: epoch  4, batch    30 | loss: 7.3280058Losses:  7.39890193939209 2.3229618072509766 0.25049853324890137
CurrentTrain: epoch  4, batch    31 | loss: 7.3989019Losses:  7.3447980880737305 2.378457546234131 0.25022703409194946
CurrentTrain: epoch  4, batch    32 | loss: 7.3447981Losses:  9.441056251525879 3.8711047172546387 0.25181296467781067
CurrentTrain: epoch  4, batch    33 | loss: 9.4410563Losses:  7.442870140075684 2.4018630981445312 0.2491714060306549
CurrentTrain: epoch  4, batch    34 | loss: 7.4428701Losses:  9.801576614379883 4.48137092590332 0.22371047735214233
CurrentTrain: epoch  4, batch    35 | loss: 9.8015766Losses:  7.073550701141357 2.2945377826690674 0.2449105679988861
CurrentTrain: epoch  4, batch    36 | loss: 7.0735507Losses:  7.307727813720703 2.2686514854431152 0.23858368396759033
CurrentTrain: epoch  4, batch    37 | loss: 7.3077278Losses:  9.638459205627441 5.065721035003662 0.26173847913742065
CurrentTrain: epoch  4, batch    38 | loss: 9.6384592Losses:  8.845291137695312 3.255039930343628 0.2320556491613388
CurrentTrain: epoch  4, batch    39 | loss: 8.8452911Losses:  7.168646812438965 2.4833359718322754 0.2255469560623169
CurrentTrain: epoch  4, batch    40 | loss: 7.1686468Losses:  10.029461860656738 5.375972747802734 0.2527264356613159
CurrentTrain: epoch  4, batch    41 | loss: 10.0294619Losses:  7.559809684753418 2.5285468101501465 0.2413991093635559
CurrentTrain: epoch  4, batch    42 | loss: 7.5598097Losses:  6.498623847961426 2.0026021003723145 0.22669997811317444
CurrentTrain: epoch  4, batch    43 | loss: 6.4986238Losses:  8.024999618530273 3.118709087371826 0.2533445358276367
CurrentTrain: epoch  4, batch    44 | loss: 8.0249996Losses:  10.011034965515137 5.163911819458008 0.2814561724662781
CurrentTrain: epoch  4, batch    45 | loss: 10.0110350Losses:  8.200020790100098 3.538330554962158 0.2555962800979614
CurrentTrain: epoch  4, batch    46 | loss: 8.2000208Losses:  9.044644355773926 4.532693862915039 0.2500322461128235
CurrentTrain: epoch  4, batch    47 | loss: 9.0446444Losses:  8.485495567321777 3.862008571624756 0.23169346153736115
CurrentTrain: epoch  4, batch    48 | loss: 8.4854956Losses:  7.489683628082275 2.587092399597168 0.15061521530151367
CurrentTrain: epoch  4, batch    49 | loss: 7.4896836Losses:  7.449028015136719 2.873995780944824 0.247248575091362
CurrentTrain: epoch  4, batch    50 | loss: 7.4490280Losses:  8.509807586669922 3.690478801727295 0.22657239437103271
CurrentTrain: epoch  4, batch    51 | loss: 8.5098076Losses:  7.669246196746826 2.5239338874816895 0.23196467757225037
CurrentTrain: epoch  4, batch    52 | loss: 7.6692462Losses:  6.927036762237549 2.3161661624908447 0.2289835810661316
CurrentTrain: epoch  4, batch    53 | loss: 6.9270368Losses:  7.827017784118652 3.060030460357666 0.23070663213729858
CurrentTrain: epoch  4, batch    54 | loss: 7.8270178Losses:  9.164946556091309 4.572776794433594 0.24325767159461975
CurrentTrain: epoch  4, batch    55 | loss: 9.1649466Losses:  7.308931827545166 2.68064546585083 0.24104031920433044
CurrentTrain: epoch  4, batch    56 | loss: 7.3089318Losses:  8.61676025390625 4.0850019454956055 0.24293872714042664
CurrentTrain: epoch  4, batch    57 | loss: 8.6167603Losses:  7.321905136108398 2.6829476356506348 0.23963677883148193
CurrentTrain: epoch  4, batch    58 | loss: 7.3219051Losses:  9.614322662353516 5.019454002380371 0.24895146489143372
CurrentTrain: epoch  4, batch    59 | loss: 9.6143227Losses:  10.114078521728516 4.583207607269287 0.24776247143745422
CurrentTrain: epoch  4, batch    60 | loss: 10.1140785Losses:  7.691384315490723 3.208993911743164 0.24731478095054626
CurrentTrain: epoch  4, batch    61 | loss: 7.6913843Losses:  5.11129093170166 0.5128830671310425 0.28350529074668884
CurrentTrain: epoch  4, batch    62 | loss: 5.1112909Losses:  9.301918029785156 4.580975532531738 0.2587321400642395
CurrentTrain: epoch  5, batch     0 | loss: 9.3019180Losses:  10.194002151489258 5.638158798217773 0.2603906989097595
CurrentTrain: epoch  5, batch     1 | loss: 10.1940022Losses:  6.787399768829346 2.3266043663024902 0.24440428614616394
CurrentTrain: epoch  5, batch     2 | loss: 6.7873998Losses:  8.195229530334473 3.2887587547302246 0.24059699475765228
CurrentTrain: epoch  5, batch     3 | loss: 8.1952295Losses:  8.018922805786133 3.6561942100524902 0.25373560190200806
CurrentTrain: epoch  5, batch     4 | loss: 8.0189228Losses:  7.612296104431152 3.1175591945648193 0.23837369680404663
CurrentTrain: epoch  5, batch     5 | loss: 7.6122961Losses:  8.24750804901123 2.622732162475586 0.23013578355312347
CurrentTrain: epoch  5, batch     6 | loss: 8.2475080Losses:  8.733467102050781 3.062466859817505 0.23806138336658478
CurrentTrain: epoch  5, batch     7 | loss: 8.7334671Losses:  9.745397567749023 5.382359504699707 0.17733804881572723
CurrentTrain: epoch  5, batch     8 | loss: 9.7453976Losses:  6.700433731079102 2.2104780673980713 0.23341858386993408
CurrentTrain: epoch  5, batch     9 | loss: 6.7004337Losses:  9.224876403808594 4.535147666931152 0.16143572330474854
CurrentTrain: epoch  5, batch    10 | loss: 9.2248764Losses:  6.295307636260986 1.57387113571167 0.23005063831806183
CurrentTrain: epoch  5, batch    11 | loss: 6.2953076Losses:  5.9853515625 1.5809167623519897 0.22349673509597778
CurrentTrain: epoch  5, batch    12 | loss: 5.9853516Losses:  9.824281692504883 4.905957221984863 0.25731199979782104
CurrentTrain: epoch  5, batch    13 | loss: 9.8242817Losses:  7.383909225463867 2.95563006401062 0.23128312826156616
CurrentTrain: epoch  5, batch    14 | loss: 7.3839092Losses:  7.894630432128906 3.123640537261963 0.2539762556552887
CurrentTrain: epoch  5, batch    15 | loss: 7.8946304Losses:  7.010725975036621 2.596111536026001 0.24066339433193207
CurrentTrain: epoch  5, batch    16 | loss: 7.0107260Losses:  10.675445556640625 5.088150978088379 0.2522956132888794
CurrentTrain: epoch  5, batch    17 | loss: 10.6754456Losses:  8.461891174316406 3.85703182220459 0.24430084228515625
CurrentTrain: epoch  5, batch    18 | loss: 8.4618912Losses:  9.5495023727417 5.100454330444336 0.23852290213108063
CurrentTrain: epoch  5, batch    19 | loss: 9.5495024Losses:  8.925771713256836 3.995415210723877 0.2619004249572754
CurrentTrain: epoch  5, batch    20 | loss: 8.9257717Losses:  11.552392959594727 7.144326686859131 0.24487850069999695
CurrentTrain: epoch  5, batch    21 | loss: 11.5523930Losses:  8.376054763793945 4.077177047729492 0.16975390911102295
CurrentTrain: epoch  5, batch    22 | loss: 8.3760548Losses:  6.944052696228027 2.5257387161254883 0.24052676558494568
CurrentTrain: epoch  5, batch    23 | loss: 6.9440527Losses:  14.730896949768066 9.854154586791992 0.18177065253257751
CurrentTrain: epoch  5, batch    24 | loss: 14.7308969Losses:  8.914417266845703 3.8714406490325928 0.25990408658981323
CurrentTrain: epoch  5, batch    25 | loss: 8.9144173Losses:  8.474818229675293 3.9935784339904785 0.24675709009170532
CurrentTrain: epoch  5, batch    26 | loss: 8.4748182Losses:  6.565845966339111 2.1175127029418945 0.23938554525375366
CurrentTrain: epoch  5, batch    27 | loss: 6.5658460Losses:  6.840615272521973 2.358640432357788 0.2320602536201477
CurrentTrain: epoch  5, batch    28 | loss: 6.8406153Losses:  6.934050559997559 1.9759204387664795 0.22117340564727783
CurrentTrain: epoch  5, batch    29 | loss: 6.9340506Losses:  7.833707809448242 3.382249593734741 0.246354341506958
CurrentTrain: epoch  5, batch    30 | loss: 7.8337078Losses:  8.487160682678223 4.071014881134033 0.24746763706207275
CurrentTrain: epoch  5, batch    31 | loss: 8.4871607Losses:  11.140495300292969 6.697957515716553 0.26698657870292664
CurrentTrain: epoch  5, batch    32 | loss: 11.1404953Losses:  8.402251243591309 3.5458602905273438 0.25216901302337646
CurrentTrain: epoch  5, batch    33 | loss: 8.4022512Losses:  10.303400039672852 5.784635543823242 0.1579992175102234
CurrentTrain: epoch  5, batch    34 | loss: 10.3034000Losses:  8.196269989013672 3.3447320461273193 0.22947418689727783
CurrentTrain: epoch  5, batch    35 | loss: 8.1962700Losses:  11.03404426574707 6.523962020874023 0.23740923404693604
CurrentTrain: epoch  5, batch    36 | loss: 11.0340443Losses:  8.57691764831543 4.024596691131592 0.25461405515670776
CurrentTrain: epoch  5, batch    37 | loss: 8.5769176Losses:  7.160829067230225 2.7339224815368652 0.25770294666290283
CurrentTrain: epoch  5, batch    38 | loss: 7.1608291Losses:  8.770925521850586 3.5226645469665527 0.2383728325366974
CurrentTrain: epoch  5, batch    39 | loss: 8.7709255Losses:  6.295741081237793 1.879288673400879 0.2266782522201538
CurrentTrain: epoch  5, batch    40 | loss: 6.2957411Losses:  6.634191036224365 2.226348876953125 0.2285524308681488
CurrentTrain: epoch  5, batch    41 | loss: 6.6341910Losses:  9.29177474975586 4.194311141967773 0.2537054419517517
CurrentTrain: epoch  5, batch    42 | loss: 9.2917747Losses:  8.71572494506836 4.20719575881958 0.22746089100837708
CurrentTrain: epoch  5, batch    43 | loss: 8.7157249Losses:  9.084393501281738 4.434506416320801 0.22989004850387573
CurrentTrain: epoch  5, batch    44 | loss: 9.0843935Losses:  7.244808673858643 2.7714593410491943 0.22969874739646912
CurrentTrain: epoch  5, batch    45 | loss: 7.2448087Losses:  7.142095565795898 2.672945022583008 0.2275020182132721
CurrentTrain: epoch  5, batch    46 | loss: 7.1420956Losses:  6.78842306137085 2.494927406311035 0.23891887068748474
CurrentTrain: epoch  5, batch    47 | loss: 6.7884231Losses:  6.979974269866943 2.197880268096924 0.2362174689769745
CurrentTrain: epoch  5, batch    48 | loss: 6.9799743Losses:  6.991400241851807 2.6213538646698 0.22236508131027222
CurrentTrain: epoch  5, batch    49 | loss: 6.9914002Losses:  9.704097747802734 5.248463153839111 0.26007145643234253
CurrentTrain: epoch  5, batch    50 | loss: 9.7040977Losses:  6.382155418395996 2.0659537315368652 0.2241886407136917
CurrentTrain: epoch  5, batch    51 | loss: 6.3821554Losses:  10.864013671875 6.44054651260376 0.2566586434841156
CurrentTrain: epoch  5, batch    52 | loss: 10.8640137Losses:  6.16306734085083 1.7442071437835693 0.21865686774253845
CurrentTrain: epoch  5, batch    53 | loss: 6.1630673Losses:  7.567477226257324 2.9622933864593506 0.24329471588134766
CurrentTrain: epoch  5, batch    54 | loss: 7.5674772Losses:  7.880955219268799 3.4369053840637207 0.22906294465065002
CurrentTrain: epoch  5, batch    55 | loss: 7.8809552Losses:  6.867999076843262 2.5387580394744873 0.22805190086364746
CurrentTrain: epoch  5, batch    56 | loss: 6.8679991Losses:  6.315886497497559 1.9171535968780518 0.22841285169124603
CurrentTrain: epoch  5, batch    57 | loss: 6.3158865Losses:  7.085069179534912 2.7564196586608887 0.22920134663581848
CurrentTrain: epoch  5, batch    58 | loss: 7.0850692Losses:  6.528429985046387 2.161104917526245 0.23202303051948547
CurrentTrain: epoch  5, batch    59 | loss: 6.5284300Losses:  9.0130615234375 4.639105796813965 0.25693464279174805
CurrentTrain: epoch  5, batch    60 | loss: 9.0130615Losses:  7.498395919799805 3.114236354827881 0.23176027834415436
CurrentTrain: epoch  5, batch    61 | loss: 7.4983959Losses:  5.201864242553711 0.7191851139068604 0.18065916001796722
CurrentTrain: epoch  5, batch    62 | loss: 5.2018642Losses:  10.317848205566406 5.923418998718262 0.2352408468723297
CurrentTrain: epoch  6, batch     0 | loss: 10.3178482Losses:  6.5534443855285645 2.2302398681640625 0.2378067672252655
CurrentTrain: epoch  6, batch     1 | loss: 6.5534444Losses:  6.814577579498291 2.291510581970215 0.23602375388145447
CurrentTrain: epoch  6, batch     2 | loss: 6.8145776Losses:  7.421264171600342 3.1106834411621094 0.2314087450504303
CurrentTrain: epoch  6, batch     3 | loss: 7.4212642Losses:  6.640875816345215 2.226119041442871 0.23576302826404572
CurrentTrain: epoch  6, batch     4 | loss: 6.6408758Losses:  7.17336893081665 2.781735420227051 0.2393007129430771
CurrentTrain: epoch  6, batch     5 | loss: 7.1733689Losses:  8.140578269958496 3.8104751110076904 0.24348528683185577
CurrentTrain: epoch  6, batch     6 | loss: 8.1405783Losses:  7.634971618652344 3.2056338787078857 0.24147725105285645
CurrentTrain: epoch  6, batch     7 | loss: 7.6349716Losses:  7.627092361450195 3.32098388671875 0.24723055958747864
CurrentTrain: epoch  6, batch     8 | loss: 7.6270924Losses:  6.682565212249756 2.2931389808654785 0.22363556921482086
CurrentTrain: epoch  6, batch     9 | loss: 6.6825652Losses:  8.311206817626953 3.0849828720092773 0.239275723695755
CurrentTrain: epoch  6, batch    10 | loss: 8.3112068Losses:  8.65622329711914 4.222721099853516 0.23043766617774963
CurrentTrain: epoch  6, batch    11 | loss: 8.6562233Losses:  6.176024436950684 1.873741865158081 0.22228331863880157
CurrentTrain: epoch  6, batch    12 | loss: 6.1760244Losses:  9.332289695739746 4.109238624572754 0.22483977675437927
CurrentTrain: epoch  6, batch    13 | loss: 9.3322897Losses:  7.809259414672852 3.3684144020080566 0.2423921674489975
CurrentTrain: epoch  6, batch    14 | loss: 7.8092594Losses:  7.9295172691345215 3.07761287689209 0.23023538291454315
CurrentTrain: epoch  6, batch    15 | loss: 7.9295173Losses:  6.621633052825928 2.163520097732544 0.22152739763259888
CurrentTrain: epoch  6, batch    16 | loss: 6.6216331Losses:  6.4186906814575195 2.011331558227539 0.2147637903690338
CurrentTrain: epoch  6, batch    17 | loss: 6.4186907Losses:  7.091952323913574 2.7400197982788086 0.24093171954154968
CurrentTrain: epoch  6, batch    18 | loss: 7.0919523Losses:  10.875378608703613 6.489993095397949 0.25137683749198914
CurrentTrain: epoch  6, batch    19 | loss: 10.8753786Losses:  7.00698184967041 2.3916051387786865 0.21645236015319824
CurrentTrain: epoch  6, batch    20 | loss: 7.0069818Losses:  8.055925369262695 3.362123966217041 0.25747665762901306
CurrentTrain: epoch  6, batch    21 | loss: 8.0559254Losses:  7.615385055541992 2.8923707008361816 0.2381439357995987
CurrentTrain: epoch  6, batch    22 | loss: 7.6153851Losses:  8.413689613342285 3.7566895484924316 0.24555131793022156
CurrentTrain: epoch  6, batch    23 | loss: 8.4136896Losses:  6.833512306213379 2.189128875732422 0.2220870852470398
CurrentTrain: epoch  6, batch    24 | loss: 6.8335123Losses:  7.751276969909668 3.202085494995117 0.23172996938228607
CurrentTrain: epoch  6, batch    25 | loss: 7.7512770Losses:  7.938673973083496 3.588210344314575 0.227145254611969
CurrentTrain: epoch  6, batch    26 | loss: 7.9386740Losses:  7.075528144836426 2.427809238433838 0.223170667886734
CurrentTrain: epoch  6, batch    27 | loss: 7.0755281Losses:  7.333558559417725 2.6602275371551514 0.23481324315071106
CurrentTrain: epoch  6, batch    28 | loss: 7.3335586Losses:  7.957825660705566 3.2729954719543457 0.234923854470253
CurrentTrain: epoch  6, batch    29 | loss: 7.9578257Losses:  7.479933738708496 3.0536136627197266 0.22779260575771332
CurrentTrain: epoch  6, batch    30 | loss: 7.4799337Losses:  6.669921875 2.265097141265869 0.22487598657608032
CurrentTrain: epoch  6, batch    31 | loss: 6.6699219Losses:  6.745349407196045 2.2564687728881836 0.22043287754058838
CurrentTrain: epoch  6, batch    32 | loss: 6.7453494Losses:  7.894957542419434 3.5090789794921875 0.2436513900756836
CurrentTrain: epoch  6, batch    33 | loss: 7.8949575Losses:  7.6857476234436035 3.1621923446655273 0.24116244912147522
CurrentTrain: epoch  6, batch    34 | loss: 7.6857476Losses:  6.5333638191223145 2.1878867149353027 0.23761005699634552
CurrentTrain: epoch  6, batch    35 | loss: 6.5333638Losses:  6.477046489715576 2.0721969604492188 0.22693687677383423
CurrentTrain: epoch  6, batch    36 | loss: 6.4770465Losses:  10.312617301940918 5.878476619720459 0.24440206587314606
CurrentTrain: epoch  6, batch    37 | loss: 10.3126173Losses:  8.950909614562988 4.375609397888184 0.2446943074464798
CurrentTrain: epoch  6, batch    38 | loss: 8.9509096Losses:  7.087732791900635 2.6079463958740234 0.24506989121437073
CurrentTrain: epoch  6, batch    39 | loss: 7.0877328Losses:  7.520906925201416 2.1216018199920654 0.21199855208396912
CurrentTrain: epoch  6, batch    40 | loss: 7.5209069Losses:  6.708720684051514 2.4211652278900146 0.23273319005966187
CurrentTrain: epoch  6, batch    41 | loss: 6.7087207Losses:  8.169609069824219 3.8856494426727295 0.2404724806547165
CurrentTrain: epoch  6, batch    42 | loss: 8.1696091Losses:  7.7246413230896 3.2430858612060547 0.24117699265480042
CurrentTrain: epoch  6, batch    43 | loss: 7.7246413Losses:  8.154786109924316 3.0597925186157227 0.22441630065441132
CurrentTrain: epoch  6, batch    44 | loss: 8.1547861Losses:  6.915133476257324 2.585688352584839 0.22055132687091827
CurrentTrain: epoch  6, batch    45 | loss: 6.9151335Losses:  9.56051254272461 4.62722110748291 0.25393185019493103
CurrentTrain: epoch  6, batch    46 | loss: 9.5605125Losses:  8.568920135498047 4.197939395904541 0.24706855416297913
CurrentTrain: epoch  6, batch    47 | loss: 8.5689201Losses:  8.183343887329102 3.2906675338745117 0.24006402492523193
CurrentTrain: epoch  6, batch    48 | loss: 8.1833439Losses:  8.285530090332031 3.211432456970215 0.23465599119663239
CurrentTrain: epoch  6, batch    49 | loss: 8.2855301Losses:  7.833285808563232 3.105288505554199 0.15078124403953552
CurrentTrain: epoch  6, batch    50 | loss: 7.8332858Losses:  7.3390889167785645 3.1522257328033447 0.22990216314792633
CurrentTrain: epoch  6, batch    51 | loss: 7.3390889Losses:  7.797369480133057 3.533740997314453 0.2568061947822571
CurrentTrain: epoch  6, batch    52 | loss: 7.7973695Losses:  6.952418327331543 2.3252947330474854 0.22872070968151093
CurrentTrain: epoch  6, batch    53 | loss: 6.9524183Losses:  8.229822158813477 3.0538692474365234 0.2208293378353119
CurrentTrain: epoch  6, batch    54 | loss: 8.2298222Losses:  8.77011489868164 4.456863880157471 0.24569721519947052
CurrentTrain: epoch  6, batch    55 | loss: 8.7701149Losses:  6.8597493171691895 2.4995791912078857 0.2360750138759613
CurrentTrain: epoch  6, batch    56 | loss: 6.8597493Losses:  7.686614990234375 2.9167542457580566 0.24511364102363586
CurrentTrain: epoch  6, batch    57 | loss: 7.6866150Losses:  6.529585838317871 2.14385724067688 0.21817393600940704
CurrentTrain: epoch  6, batch    58 | loss: 6.5295858Losses:  9.531671524047852 4.974748134613037 0.25709518790245056
CurrentTrain: epoch  6, batch    59 | loss: 9.5316715Losses:  6.379110813140869 1.898900032043457 0.2310892641544342
CurrentTrain: epoch  6, batch    60 | loss: 6.3791108Losses:  6.339139938354492 1.861431360244751 0.22901621460914612
CurrentTrain: epoch  6, batch    61 | loss: 6.3391399Losses:  4.7453293800354 0.21492820978164673 0.1563969999551773
CurrentTrain: epoch  6, batch    62 | loss: 4.7453294Losses:  8.969764709472656 4.519937515258789 0.2526986300945282
CurrentTrain: epoch  7, batch     0 | loss: 8.9697647Losses:  7.640215873718262 3.1343021392822266 0.22911150753498077
CurrentTrain: epoch  7, batch     1 | loss: 7.6402159Losses:  8.369251251220703 3.426344871520996 0.24435777962207794
CurrentTrain: epoch  7, batch     2 | loss: 8.3692513Losses:  10.8377046585083 6.607374668121338 0.17580276727676392
CurrentTrain: epoch  7, batch     3 | loss: 10.8377047Losses:  7.5903215408325195 3.2773046493530273 0.22465142607688904
CurrentTrain: epoch  7, batch     4 | loss: 7.5903215Losses:  6.130168914794922 1.8028745651245117 0.22059617936611176
CurrentTrain: epoch  7, batch     5 | loss: 6.1301689Losses:  7.573222637176514 2.80462646484375 0.23666732013225555
CurrentTrain: epoch  7, batch     6 | loss: 7.5732226Losses:  7.480035781860352 3.0117743015289307 0.22683313488960266
CurrentTrain: epoch  7, batch     7 | loss: 7.4800358Losses:  7.176672458648682 2.8609509468078613 0.23659417033195496
CurrentTrain: epoch  7, batch     8 | loss: 7.1766725Losses:  8.09133529663086 3.771780252456665 0.2540685832500458
CurrentTrain: epoch  7, batch     9 | loss: 8.0913353Losses:  7.514147758483887 3.051223039627075 0.2260606437921524
CurrentTrain: epoch  7, batch    10 | loss: 7.5141478Losses:  8.14525032043457 3.851806163787842 0.15575425326824188
CurrentTrain: epoch  7, batch    11 | loss: 8.1452503Losses:  7.210461139678955 2.8627634048461914 0.24137341976165771
CurrentTrain: epoch  7, batch    12 | loss: 7.2104611Losses:  6.217743873596191 1.8613173961639404 0.23122891783714294
CurrentTrain: epoch  7, batch    13 | loss: 6.2177439Losses:  7.853230953216553 3.5335159301757812 0.23318515717983246
CurrentTrain: epoch  7, batch    14 | loss: 7.8532310Losses:  8.637369155883789 4.399965763092041 0.23904046416282654
CurrentTrain: epoch  7, batch    15 | loss: 8.6373692Losses:  9.004039764404297 4.659409523010254 0.24608206748962402
CurrentTrain: epoch  7, batch    16 | loss: 9.0040398Losses:  6.479227066040039 2.1503140926361084 0.23031409084796906
CurrentTrain: epoch  7, batch    17 | loss: 6.4792271Losses:  9.343472480773926 3.7912213802337646 0.25050675868988037
CurrentTrain: epoch  7, batch    18 | loss: 9.3434725Losses:  6.985324382781982 2.6910126209259033 0.23715174198150635
CurrentTrain: epoch  7, batch    19 | loss: 6.9853244Losses:  6.571070671081543 2.1770615577697754 0.2312791347503662
CurrentTrain: epoch  7, batch    20 | loss: 6.5710707Losses:  8.623373985290527 4.440887451171875 0.2368820309638977
CurrentTrain: epoch  7, batch    21 | loss: 8.6233740Losses:  7.077444553375244 2.741337776184082 0.2273031622171402
CurrentTrain: epoch  7, batch    22 | loss: 7.0774446Losses:  7.3054118156433105 2.649055004119873 0.23140749335289001
CurrentTrain: epoch  7, batch    23 | loss: 7.3054118Losses:  8.154632568359375 3.891737937927246 0.24026323854923248
CurrentTrain: epoch  7, batch    24 | loss: 8.1546326Losses:  7.6819586753845215 3.1952242851257324 0.24596340954303741
CurrentTrain: epoch  7, batch    25 | loss: 7.6819587Losses:  7.008858680725098 2.5926437377929688 0.22980688512325287
CurrentTrain: epoch  7, batch    26 | loss: 7.0088587Losses:  8.111159324645996 3.7039642333984375 0.24111174046993256
CurrentTrain: epoch  7, batch    27 | loss: 8.1111593Losses:  6.590244770050049 2.1731674671173096 0.22134622931480408
CurrentTrain: epoch  7, batch    28 | loss: 6.5902448Losses:  7.115865230560303 2.788883686065674 0.2403489351272583
CurrentTrain: epoch  7, batch    29 | loss: 7.1158652Losses:  10.037951469421387 5.185604095458984 0.24018044769763947
CurrentTrain: epoch  7, batch    30 | loss: 10.0379515Losses:  7.798831939697266 3.481612205505371 0.23767313361167908
CurrentTrain: epoch  7, batch    31 | loss: 7.7988319Losses:  6.6996307373046875 2.4479994773864746 0.23215961456298828
CurrentTrain: epoch  7, batch    32 | loss: 6.6996307Losses:  6.835257530212402 2.508774518966675 0.23316025733947754
CurrentTrain: epoch  7, batch    33 | loss: 6.8352575Losses:  6.253541946411133 1.9425281286239624 0.21948370337486267
CurrentTrain: epoch  7, batch    34 | loss: 6.2535419Losses:  7.662563323974609 3.3354134559631348 0.2306222915649414
CurrentTrain: epoch  7, batch    35 | loss: 7.6625633Losses:  7.89333438873291 3.520036458969116 0.23603244125843048
CurrentTrain: epoch  7, batch    36 | loss: 7.8933344Losses:  7.516672611236572 3.242799758911133 0.23255056142807007
CurrentTrain: epoch  7, batch    37 | loss: 7.5166726Losses:  8.244890213012695 3.982346296310425 0.2331421673297882
CurrentTrain: epoch  7, batch    38 | loss: 8.2448902Losses:  6.311456203460693 1.9009760618209839 0.22255359590053558
CurrentTrain: epoch  7, batch    39 | loss: 6.3114562Losses:  6.488997936248779 2.225236415863037 0.22138965129852295
CurrentTrain: epoch  7, batch    40 | loss: 6.4889979Losses:  10.101253509521484 5.913345813751221 0.2581976056098938
CurrentTrain: epoch  7, batch    41 | loss: 10.1012535Losses:  7.0946807861328125 2.7301673889160156 0.2275402545928955
CurrentTrain: epoch  7, batch    42 | loss: 7.0946808Losses:  8.539743423461914 4.247817039489746 0.25093600153923035
CurrentTrain: epoch  7, batch    43 | loss: 8.5397434Losses:  7.404993057250977 2.086165428161621 0.21974892914295197
CurrentTrain: epoch  7, batch    44 | loss: 7.4049931Losses:  6.907042026519775 2.203800678253174 0.2146892249584198
CurrentTrain: epoch  7, batch    45 | loss: 6.9070420Losses:  7.029560089111328 2.6235594749450684 0.22111369669437408
CurrentTrain: epoch  7, batch    46 | loss: 7.0295601Losses:  6.504645347595215 2.133424997329712 0.22425483167171478
CurrentTrain: epoch  7, batch    47 | loss: 6.5046453Losses:  8.233531951904297 3.9924724102020264 0.24476414918899536
CurrentTrain: epoch  7, batch    48 | loss: 8.2335320Losses:  6.498680591583252 2.2262814044952393 0.22281961143016815
CurrentTrain: epoch  7, batch    49 | loss: 6.4986806Losses:  7.535122394561768 2.686906337738037 0.223796546459198
CurrentTrain: epoch  7, batch    50 | loss: 7.5351224Losses:  6.440307140350342 2.1264944076538086 0.22387826442718506
CurrentTrain: epoch  7, batch    51 | loss: 6.4403071Losses:  6.918022632598877 2.153799533843994 0.22372521460056305
CurrentTrain: epoch  7, batch    52 | loss: 6.9180226Losses:  9.409126281738281 5.137351036071777 0.23665273189544678
CurrentTrain: epoch  7, batch    53 | loss: 9.4091263Losses:  7.289872169494629 2.957662343978882 0.23506459593772888
CurrentTrain: epoch  7, batch    54 | loss: 7.2898722Losses:  6.640482425689697 1.986398696899414 0.2090713232755661
CurrentTrain: epoch  7, batch    55 | loss: 6.6404824Losses:  10.149894714355469 5.747623920440674 0.24086344242095947
CurrentTrain: epoch  7, batch    56 | loss: 10.1498947Losses:  6.587749004364014 2.170212745666504 0.23423893749713898
CurrentTrain: epoch  7, batch    57 | loss: 6.5877490Losses:  6.473793029785156 1.815106987953186 0.22659069299697876
CurrentTrain: epoch  7, batch    58 | loss: 6.4737930Losses:  6.4196062088012695 2.1846156120300293 0.21480710804462433
CurrentTrain: epoch  7, batch    59 | loss: 6.4196062Losses:  6.6963934898376465 2.3154165744781494 0.22547870874404907
CurrentTrain: epoch  7, batch    60 | loss: 6.6963935Losses:  7.056440353393555 2.7489843368530273 0.25764936208724976
CurrentTrain: epoch  7, batch    61 | loss: 7.0564404Losses:  4.784910202026367 0.4567083418369293 0.26592111587524414
CurrentTrain: epoch  7, batch    62 | loss: 4.7849102Losses:  6.112143516540527 1.8378820419311523 0.21777990460395813
CurrentTrain: epoch  8, batch     0 | loss: 6.1121435Losses:  6.6937360763549805 2.3602304458618164 0.23587697744369507
CurrentTrain: epoch  8, batch     1 | loss: 6.6937361Losses:  9.949990272521973 5.376081466674805 0.1525014340877533
CurrentTrain: epoch  8, batch     2 | loss: 9.9499903Losses:  8.867341995239258 4.657451629638672 0.23001950979232788
CurrentTrain: epoch  8, batch     3 | loss: 8.8673420Losses:  6.372020244598389 2.112224817276001 0.2244841456413269
CurrentTrain: epoch  8, batch     4 | loss: 6.3720202Losses:  9.404426574707031 5.059497833251953 0.14824479818344116
CurrentTrain: epoch  8, batch     5 | loss: 9.4044266Losses:  7.428099155426025 3.1691884994506836 0.23782502114772797
CurrentTrain: epoch  8, batch     6 | loss: 7.4280992Losses:  6.502353668212891 2.2817258834838867 0.22153477370738983
CurrentTrain: epoch  8, batch     7 | loss: 6.5023537Losses:  6.955373764038086 2.6118552684783936 0.23844599723815918
CurrentTrain: epoch  8, batch     8 | loss: 6.9553738Losses:  8.354342460632324 4.079105854034424 0.23443280160427094
CurrentTrain: epoch  8, batch     9 | loss: 8.3543425Losses:  7.135116100311279 2.902493476867676 0.2244504988193512
CurrentTrain: epoch  8, batch    10 | loss: 7.1351161Losses:  7.224618911743164 2.9252588748931885 0.2234725058078766
CurrentTrain: epoch  8, batch    11 | loss: 7.2246189Losses:  7.944150924682617 3.5071470737457275 0.24098604917526245
CurrentTrain: epoch  8, batch    12 | loss: 7.9441509Losses:  7.578554153442383 3.3081045150756836 0.23371915519237518
CurrentTrain: epoch  8, batch    13 | loss: 7.5785542Losses:  8.000082015991211 3.6856889724731445 0.25419044494628906
CurrentTrain: epoch  8, batch    14 | loss: 8.0000820Losses:  7.786285877227783 3.5179097652435303 0.24739620089530945
CurrentTrain: epoch  8, batch    15 | loss: 7.7862859Losses:  10.755819320678711 6.390596389770508 0.25856977701187134
CurrentTrain: epoch  8, batch    16 | loss: 10.7558193Losses:  6.481395244598389 2.1736631393432617 0.22865363955497742
CurrentTrain: epoch  8, batch    17 | loss: 6.4813952Losses:  8.005986213684082 3.6946704387664795 0.2258894145488739
CurrentTrain: epoch  8, batch    18 | loss: 8.0059862Losses:  7.357519149780273 3.157870054244995 0.22255448997020721
CurrentTrain: epoch  8, batch    19 | loss: 7.3575191Losses:  7.55540657043457 3.274526357650757 0.2430942952632904
CurrentTrain: epoch  8, batch    20 | loss: 7.5554066Losses:  6.529505252838135 2.2406609058380127 0.2211262583732605
CurrentTrain: epoch  8, batch    21 | loss: 6.5295053Losses:  9.799466133117676 5.620835781097412 0.2440091371536255
CurrentTrain: epoch  8, batch    22 | loss: 9.7994661Losses:  8.859089851379395 4.626428604125977 0.25179365277290344
CurrentTrain: epoch  8, batch    23 | loss: 8.8590899Losses:  7.2288312911987305 2.966028928756714 0.2279961109161377
CurrentTrain: epoch  8, batch    24 | loss: 7.2288313Losses:  6.56437349319458 2.308443069458008 0.22694620490074158
CurrentTrain: epoch  8, batch    25 | loss: 6.5643735Losses:  6.35737419128418 2.148655414581299 0.22462652623653412
CurrentTrain: epoch  8, batch    26 | loss: 6.3573742Losses:  8.200206756591797 3.821643829345703 0.22674430906772614
CurrentTrain: epoch  8, batch    27 | loss: 8.2002068Losses:  6.48071813583374 2.22414493560791 0.22758431732654572
CurrentTrain: epoch  8, batch    28 | loss: 6.4807181Losses:  8.37008285522461 4.068912029266357 0.23246583342552185
CurrentTrain: epoch  8, batch    29 | loss: 8.3700829Losses:  6.446876525878906 2.1771140098571777 0.22460997104644775
CurrentTrain: epoch  8, batch    30 | loss: 6.4468765Losses:  6.712543964385986 2.4276299476623535 0.23552125692367554
CurrentTrain: epoch  8, batch    31 | loss: 6.7125440Losses:  7.187525749206543 2.8998987674713135 0.22797396779060364
CurrentTrain: epoch  8, batch    32 | loss: 7.1875257Losses:  8.272411346435547 4.091980457305908 0.15691399574279785
CurrentTrain: epoch  8, batch    33 | loss: 8.2724113Losses:  9.070557594299316 4.815451622009277 0.22755950689315796
CurrentTrain: epoch  8, batch    34 | loss: 9.0705576Losses:  7.485128402709961 3.2212395668029785 0.23496118187904358
CurrentTrain: epoch  8, batch    35 | loss: 7.4851284Losses:  6.891798496246338 2.721280336380005 0.23000004887580872
CurrentTrain: epoch  8, batch    36 | loss: 6.8917985Losses:  9.543354988098145 5.428796768188477 0.14598338305950165
CurrentTrain: epoch  8, batch    37 | loss: 9.5433550Losses:  6.724600315093994 2.370887279510498 0.23122164607048035
CurrentTrain: epoch  8, batch    38 | loss: 6.7246003Losses:  6.209567070007324 1.8407728672027588 0.2273167073726654
CurrentTrain: epoch  8, batch    39 | loss: 6.2095671Losses:  6.816397190093994 2.6076064109802246 0.22094827890396118
CurrentTrain: epoch  8, batch    40 | loss: 6.8163972Losses:  6.971322536468506 2.7744736671447754 0.22817935049533844
CurrentTrain: epoch  8, batch    41 | loss: 6.9713225Losses:  7.128718376159668 2.8675436973571777 0.23091772198677063
CurrentTrain: epoch  8, batch    42 | loss: 7.1287184Losses:  7.697579383850098 3.3866167068481445 0.23603177070617676
CurrentTrain: epoch  8, batch    43 | loss: 7.6975794Losses:  6.962510585784912 2.740938663482666 0.23938268423080444
CurrentTrain: epoch  8, batch    44 | loss: 6.9625106Losses:  8.24401569366455 3.9967756271362305 0.23219715058803558
CurrentTrain: epoch  8, batch    45 | loss: 8.2440157Losses:  7.179084777832031 2.9376142024993896 0.2302408516407013
CurrentTrain: epoch  8, batch    46 | loss: 7.1790848Losses:  8.251110076904297 3.9750216007232666 0.23732037842273712
CurrentTrain: epoch  8, batch    47 | loss: 8.2511101Losses:  6.994518756866455 2.7593839168548584 0.22853785753250122
CurrentTrain: epoch  8, batch    48 | loss: 6.9945188Losses:  6.9911346435546875 2.717895030975342 0.22746455669403076
CurrentTrain: epoch  8, batch    49 | loss: 6.9911346Losses:  7.394732475280762 3.221280574798584 0.22885474562644958
CurrentTrain: epoch  8, batch    50 | loss: 7.3947325Losses:  6.30550479888916 2.084880828857422 0.22400802373886108
CurrentTrain: epoch  8, batch    51 | loss: 6.3055048Losses:  6.60701322555542 2.3938066959381104 0.23377159237861633
CurrentTrain: epoch  8, batch    52 | loss: 6.6070132Losses:  6.254728317260742 2.0121757984161377 0.22927848994731903
CurrentTrain: epoch  8, batch    53 | loss: 6.2547283Losses:  7.095806121826172 2.904362678527832 0.22787465155124664
CurrentTrain: epoch  8, batch    54 | loss: 7.0958061Losses:  9.714061737060547 5.510061264038086 0.21648432314395905
CurrentTrain: epoch  8, batch    55 | loss: 9.7140617Losses:  6.23898458480835 2.0091347694396973 0.22640497982501984
CurrentTrain: epoch  8, batch    56 | loss: 6.2389846Losses:  8.371292114257812 4.181089401245117 0.2352008819580078
CurrentTrain: epoch  8, batch    57 | loss: 8.3712921Losses:  6.932239055633545 2.7339932918548584 0.22483304142951965
CurrentTrain: epoch  8, batch    58 | loss: 6.9322391Losses:  6.816436767578125 2.5907673835754395 0.22339816391468048
CurrentTrain: epoch  8, batch    59 | loss: 6.8164368Losses:  6.850646018981934 2.665231227874756 0.22503304481506348
CurrentTrain: epoch  8, batch    60 | loss: 6.8506460Losses:  6.981234550476074 2.7703542709350586 0.22658729553222656
CurrentTrain: epoch  8, batch    61 | loss: 6.9812346Losses:  4.555062770843506 0.4651999771595001 0.16163502633571625
CurrentTrain: epoch  8, batch    62 | loss: 4.5550628Losses:  6.293641090393066 2.1237683296203613 0.2183074951171875
CurrentTrain: epoch  9, batch     0 | loss: 6.2936411Losses:  7.912055015563965 3.6844429969787598 0.237742617726326
CurrentTrain: epoch  9, batch     1 | loss: 7.9120550Losses:  5.402519226074219 1.2136316299438477 0.2053905874490738
CurrentTrain: epoch  9, batch     2 | loss: 5.4025192Losses:  8.092350006103516 3.872084617614746 0.22969681024551392
CurrentTrain: epoch  9, batch     3 | loss: 8.0923500Losses:  6.349758148193359 2.1754441261291504 0.2190535068511963
CurrentTrain: epoch  9, batch     4 | loss: 6.3497581Losses:  6.926171779632568 2.6964221000671387 0.22096294164657593
CurrentTrain: epoch  9, batch     5 | loss: 6.9261718Losses:  6.529402256011963 2.215461254119873 0.2141483724117279
CurrentTrain: epoch  9, batch     6 | loss: 6.5294023Losses:  7.374113082885742 3.1359076499938965 0.2260458618402481
CurrentTrain: epoch  9, batch     7 | loss: 7.3741131Losses:  6.554942607879639 2.3544044494628906 0.22384977340698242
CurrentTrain: epoch  9, batch     8 | loss: 6.5549426Losses:  9.128935813903809 5.007941246032715 0.1523871272802353
CurrentTrain: epoch  9, batch     9 | loss: 9.1289358Losses:  9.377703666687012 5.156439781188965 0.24596844613552094
CurrentTrain: epoch  9, batch    10 | loss: 9.3777037Losses:  10.322670936584473 5.586175918579102 0.15776985883712769
CurrentTrain: epoch  9, batch    11 | loss: 10.3226709Losses:  11.09379768371582 6.8201775550842285 0.2430671900510788
CurrentTrain: epoch  9, batch    12 | loss: 11.0937977Losses:  5.428888320922852 1.2387115955352783 0.20668776333332062
CurrentTrain: epoch  9, batch    13 | loss: 5.4288883Losses:  6.879091262817383 2.686196804046631 0.23047493398189545
CurrentTrain: epoch  9, batch    14 | loss: 6.8790913Losses:  6.738631248474121 2.545511245727539 0.2228165715932846
CurrentTrain: epoch  9, batch    15 | loss: 6.7386312Losses:  6.93596887588501 2.6895172595977783 0.22082394361495972
CurrentTrain: epoch  9, batch    16 | loss: 6.9359689Losses:  6.6428117752075195 2.45285701751709 0.2186654657125473
CurrentTrain: epoch  9, batch    17 | loss: 6.6428118Losses:  7.441413879394531 3.242003917694092 0.23229342699050903
CurrentTrain: epoch  9, batch    18 | loss: 7.4414139Losses:  6.983664512634277 2.705993175506592 0.23187677562236786
CurrentTrain: epoch  9, batch    19 | loss: 6.9836645Losses:  5.770456790924072 1.5958868265151978 0.20662647485733032
CurrentTrain: epoch  9, batch    20 | loss: 5.7704568Losses:  7.666608810424805 3.4795589447021484 0.2407372146844864
CurrentTrain: epoch  9, batch    21 | loss: 7.6666088Losses:  6.449217319488525 2.157435417175293 0.2213960886001587
CurrentTrain: epoch  9, batch    22 | loss: 6.4492173Losses:  8.148811340332031 3.891169786453247 0.23924997448921204
CurrentTrain: epoch  9, batch    23 | loss: 8.1488113Losses:  6.906111240386963 2.700350284576416 0.22222024202346802
CurrentTrain: epoch  9, batch    24 | loss: 6.9061112Losses:  5.999526023864746 1.8264451026916504 0.21858561038970947
CurrentTrain: epoch  9, batch    25 | loss: 5.9995260Losses:  8.347173690795898 4.161737442016602 0.24129286408424377
CurrentTrain: epoch  9, batch    26 | loss: 8.3471737Losses:  8.184966087341309 4.104313850402832 0.15036827325820923
CurrentTrain: epoch  9, batch    27 | loss: 8.1849661Losses:  8.160235404968262 3.924680471420288 0.2185412496328354
CurrentTrain: epoch  9, batch    28 | loss: 8.1602354Losses:  6.284850120544434 2.0856809616088867 0.22243690490722656
CurrentTrain: epoch  9, batch    29 | loss: 6.2848501Losses:  6.5142717361450195 2.345320463180542 0.2307855188846588
CurrentTrain: epoch  9, batch    30 | loss: 6.5142717Losses:  6.626004695892334 2.3632829189300537 0.23788690567016602
CurrentTrain: epoch  9, batch    31 | loss: 6.6260047Losses:  8.779624938964844 4.597135543823242 0.24143531918525696
CurrentTrain: epoch  9, batch    32 | loss: 8.7796249Losses:  7.340287208557129 2.8474366664886475 0.22629152238368988
CurrentTrain: epoch  9, batch    33 | loss: 7.3402872Losses:  7.184798717498779 3.0687198638916016 0.1356484293937683
CurrentTrain: epoch  9, batch    34 | loss: 7.1847987Losses:  7.408078193664551 2.906492233276367 0.24907098710536957
CurrentTrain: epoch  9, batch    35 | loss: 7.4080782Losses:  6.6530537605285645 2.41519832611084 0.23421260714530945
CurrentTrain: epoch  9, batch    36 | loss: 6.6530538Losses:  7.338851451873779 3.126030445098877 0.22607089579105377
CurrentTrain: epoch  9, batch    37 | loss: 7.3388515Losses:  6.966591835021973 2.713817834854126 0.2269585132598877
CurrentTrain: epoch  9, batch    38 | loss: 6.9665918Losses:  8.328710556030273 4.150257110595703 0.23393088579177856
CurrentTrain: epoch  9, batch    39 | loss: 8.3287106Losses:  7.357091426849365 3.146641731262207 0.24152792990207672
CurrentTrain: epoch  9, batch    40 | loss: 7.3570914Losses:  6.418788433074951 2.191333532333374 0.22382768988609314
CurrentTrain: epoch  9, batch    41 | loss: 6.4187884Losses:  6.532349586486816 2.338994264602661 0.21581366658210754
CurrentTrain: epoch  9, batch    42 | loss: 6.5323496Losses:  5.734019756317139 1.554261565208435 0.2045021951198578
CurrentTrain: epoch  9, batch    43 | loss: 5.7340198Losses:  7.622181415557861 3.4303956031799316 0.22238506376743317
CurrentTrain: epoch  9, batch    44 | loss: 7.6221814Losses:  7.114963054656982 2.9258556365966797 0.21788254380226135
CurrentTrain: epoch  9, batch    45 | loss: 7.1149631Losses:  7.952348232269287 2.6469714641571045 0.22795143723487854
CurrentTrain: epoch  9, batch    46 | loss: 7.9523482Losses:  8.645214080810547 4.044110298156738 0.22783605754375458
CurrentTrain: epoch  9, batch    47 | loss: 8.6452141Losses:  8.88561725616455 4.704054355621338 0.26091301441192627
CurrentTrain: epoch  9, batch    48 | loss: 8.8856173Losses:  7.010986804962158 2.7802112102508545 0.22266130149364471
CurrentTrain: epoch  9, batch    49 | loss: 7.0109868Losses:  7.475040912628174 3.0380544662475586 0.22900830209255219
CurrentTrain: epoch  9, batch    50 | loss: 7.4750409Losses:  6.414214134216309 2.1627278327941895 0.22819113731384277
CurrentTrain: epoch  9, batch    51 | loss: 6.4142141Losses:  6.146218299865723 1.8933184146881104 0.21147342026233673
CurrentTrain: epoch  9, batch    52 | loss: 6.1462183Losses:  6.907345294952393 2.5658485889434814 0.21391326189041138
CurrentTrain: epoch  9, batch    53 | loss: 6.9073453Losses:  7.052188873291016 2.8113842010498047 0.2322026491165161
CurrentTrain: epoch  9, batch    54 | loss: 7.0521889Losses:  6.875795364379883 2.6604318618774414 0.21599793434143066
CurrentTrain: epoch  9, batch    55 | loss: 6.8757954Losses:  7.616549968719482 3.0793261528015137 0.24451448023319244
CurrentTrain: epoch  9, batch    56 | loss: 7.6165500Losses:  8.69469928741455 4.411252021789551 0.24595610797405243
CurrentTrain: epoch  9, batch    57 | loss: 8.6946993Losses:  8.033819198608398 3.7628769874572754 0.25523024797439575
CurrentTrain: epoch  9, batch    58 | loss: 8.0338192Losses:  9.809407234191895 5.538270950317383 0.254861056804657
CurrentTrain: epoch  9, batch    59 | loss: 9.8094072Losses:  8.254518508911133 4.033064365386963 0.2354903221130371
CurrentTrain: epoch  9, batch    60 | loss: 8.2545185Losses:  6.712007522583008 2.5278916358947754 0.22104185819625854
CurrentTrain: epoch  9, batch    61 | loss: 6.7120075Losses:  7.249454021453857 2.891561269760132 0.27447840571403503
CurrentTrain: epoch  9, batch    62 | loss: 7.2494540
#### movement ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "movement" means literary, artistic, scientific or philosophical movement or scene associated with this person or work.
Here is an example:
Relation: movement
Context: the theme of the loss of identity in crowds in this painting can be seen as a precursor to expressionist painting , particularly the work of james ensor .
Head Entity: james ensor
Tail Entity: expressionist
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "movement":
Relation: movement  
Context: The surrealist movement, which sought to challenge the conventional perception of reality, was significantly influenced by the works of Salvador Dalí.  
Head Entity: Salvador Dalí  
Tail Entity: surrealist  

Relation: movement  
Context: The emergence of the romantic movement in literature was marked by the profound impact of poets like William Wordsworth and Samuel Taylor Coleridge.  
Head Entity: William Wordsworth  
Tail Entity: romantic  
#### located in the administrative territorial entity ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "located in the administrative territorial entity" means the item is located on the territory of the following administrative entity.
Here is an example:
Relation: located in the administrative territorial entity
Context: lake koshkonong is a census - designated place ( cdp ) in the town of sumner , jefferson county , wisconsin , united states .
Head Entity: sumner
Tail Entity: jefferson county
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "located in the administrative territorial entity":
Relation: located in the administrative territorial entity  
Context: the city of barcelona is the capital of catalonia, an autonomous community in northeastern spain.  
Head Entity: barcelona  
Tail Entity: catalonia  

Relation: located in the administrative territorial entity  
Context: mount fuji is situated in shizuoka and yamanashi prefectures, which are located on honshu island in japan.  
Head Entity: mount fuji  
Tail Entity: shizuoka and yamanashi prefectures  
#### head of government ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "head of government" means head of the executive power of this town, city, municipality, state, country, or other governmental body.
Here is an example:
Relation: head of government
Context: on february 10 , 2010 , stefan mappus was elected minister - president of baden - württemberg , and asked that all ministers continue their roles .
Head Entity: baden - württemberg
Tail Entity: stefan mappus
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "head of government":
Relation: head of government  
Context: in 2021, jacinda ardern was re-elected as the prime minister of new zealand, leading the labor party to a significant victory in the general elections.  
Head Entity: new zealand  
Tail Entity: jacinda ardern  

Relation: head of government  
Context: on march 15, 2019, justin trudeau announced his intention to run for re-election as the prime minister of canada, emphasizing his government's achievements over the past four years.  
Head Entity: canada  
Tail Entity: justin trudeau  
#### member of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "member of" means organization, club or musical group to which the subject belongs.
Here is an example:
Relation: member of
Context: rage against the machine bandmates , tom morello , brad wilk , and de la rocha , as well as audioslave bandmate , chris cornell , came to support commerford .
Head Entity: chris cornell
Tail Entity: audioslave
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "member of":
Relation: member of  
Context: The famous actor and musician, Jared Leto, is not only the lead vocalist of Thirty Seconds to Mars but also a member of the Academy Award-winning film community.  
Head Entity: Jared Leto  
Tail Entity: Thirty Seconds to Mars  

Relation: member of  
Context: During the annual conference, several prominent figures from the tech industry, including Sundar Pichai, who is a member of the Google team, shared their insights on innovation.  
Head Entity: Sundar Pichai  
Tail Entity: Google  
#### crosses ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "crosses" means obstacle which this bridge crosses over or this tunnel goes under.
Here is an example:
Relation: crosses
Context: on july 2 , 1935 , the rip van winkle bridge over the hudson river between catskill and greenport was opened to traffic .
Head Entity: rip van winkle bridge
Tail Entity: hudson river
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "crosses":
Relation: crosses  
Context: The Golden Gate Bridge spans the Golden Gate Strait, connecting San Francisco to Marin County, and is a vital link for traffic crossing over the water.  
Head Entity: Golden Gate Bridge  
Tail Entity: Golden Gate Strait  

Relation: crosses  
Context: The Channel Tunnel, also known as the Chunnel, runs beneath the English Channel, providing a rail link between the UK and mainland Europe.  
Head Entity: Channel Tunnel  
Tail Entity: English Channel  
#### tributary ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "tributary" means watercourse that flows into an other one.
Here is an example:
Relation: tributary
Context: it is on the panj river just downstream from the mouth of the bartang river at the point where the panj briefly turns west before resuming its northerly course .
Head Entity: panj river
Tail Entity: bartang river
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "tributary":
Relation: tributary  
Context: The Missouri River, which is a major tributary of the Mississippi River, flows through several states before joining the larger watercourse.  
Head Entity: Missouri River  
Tail Entity: Mississippi River  

Relation: tributary  
Context: The Rio Grande serves as a natural border between the United States and Mexico, and it has several tributaries, including the Pecos River.  
Head Entity: Rio Grande  
Tail Entity: Pecos River  
#### position held ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "position held" means subject currently or formerly holds the object position or public office.
Here is an example:
Relation: position held
Context: in september 2001 , mimica became minister of european integration under prime minister ivica račan , position he held until december 2003 .
Head Entity: ivica račan
Tail Entity: prime minister
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "position held":
Relation: position held  
Context: after serving as the mayor of new york city for eight years, michael bloomberg announced his decision to run for a third term, a position he held from 2002 to 2013.  
Head Entity: michael bloomberg  
Tail Entity: mayor of new york city  

Relation: position held  
Context: in 2015, angela merkel was re-elected as chancellor of germany, a position she has held since 2005, making her one of the longest-serving leaders in the country’s history.  
Head Entity: angela merkel  
Tail Entity: chancellor of germany  
#### developer ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "developer" means organization or person that developed the item.
Here is an example:
Relation: developer
Context: it was developed by syn sophia , who also developed the other games in the " style savvy " series .
Head Entity: style savvy
Tail Entity: syn sophia
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "developer":
Relation: developer  
Context: the popular game was created by epic games, known for their innovative approach to game design.  
Head Entity: popular game  
Tail Entity: epic games  

Relation: developer  
Context: the groundbreaking software was launched by tech innovations inc., a leader in the tech industry.  
Head Entity: groundbreaking software  
Tail Entity: tech innovations inc.  
#### location of formation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "location of formation" means location where a group or organization was formed.
Here is an example:
Relation: location of formation
Context: happy refugees are a british post - punk band from london , formed in 1981 .
Head Entity: happy refugees
Tail Entity: london
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "location of formation":
Relation: location of formation  
Context: the tech startup was established in silicon valley in 2015, quickly gaining recognition for its innovative solutions.  
Head Entity: the tech startup  
Tail Entity: silicon valley  

Relation: location of formation  
Context: the renowned art collective originated in new york city during the late 1990s, bringing together diverse artists from various backgrounds.  
Head Entity: the renowned art collective  
Tail Entity: new york city  
#### country of origin ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "country of origin" means country of origin of this item (creative work, food, phrase, product, etc.).
Here is an example:
Relation: country of origin
Context: during the same month , the band supported kings of leon 's tour of the united states , and coldplay 's tour of the united kingdom .
Head Entity: kings of leon
Tail Entity: united states
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "country of origin":
Relation: country of origin  
Context: The famous Italian dish, pizza, has gained popularity worldwide, but its roots can be traced back to Naples, Italy.  
Head Entity: pizza  
Tail Entity: Italy  

Relation: country of origin  
Context: The iconic brand, Rolex, is renowned for its luxury watches, which are crafted in Switzerland, known for its precision and quality in watchmaking.  
Head Entity: Rolex  
Tail Entity: Switzerland  
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:    3 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 90.00%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 90.18%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 91.41%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 92.36%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 93.12%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 92.61%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 93.23%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 93.27%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 93.30%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 93.33%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 94.12%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 94.10%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 94.41%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 94.69%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 94.94%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 94.89%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 95.11%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 95.05%   [EVAL] batch:   24 | acc: 81.25%,  total acc: 94.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 94.71%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 94.68%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 94.64%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 94.61%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 94.79%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 94.96%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 95.12%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 95.27%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 95.40%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 95.36%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 95.49%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 95.61%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 95.56%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 95.67%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 95.78%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 95.88%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 95.98%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 96.08%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 96.16%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 96.25%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 96.33%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 96.28%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 96.22%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 96.30%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 96.38%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 96.32%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 96.27%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 96.23%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 96.30%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 96.02%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 95.98%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 95.94%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 95.91%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 95.97%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 95.94%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 96.00%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 95.97%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 95.24%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:    3 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 90.00%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 90.18%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 91.41%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 92.36%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 93.12%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 92.61%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 93.23%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 93.27%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 93.30%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 93.33%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 94.12%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 94.10%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 94.41%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 94.69%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 94.94%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 94.89%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 95.11%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 95.05%   [EVAL] batch:   24 | acc: 81.25%,  total acc: 94.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 94.71%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 94.68%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 94.64%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 94.61%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 94.79%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 94.96%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 95.12%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 95.27%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 95.40%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 95.36%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 95.49%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 95.61%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 95.56%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 95.67%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 95.78%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 95.88%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 95.98%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 96.08%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 96.16%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 96.25%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 96.33%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 96.28%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 96.22%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 96.30%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 96.38%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 96.32%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 96.27%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 96.23%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 96.30%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 96.02%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 95.98%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 95.94%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 95.91%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 95.97%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 95.94%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 96.00%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 95.97%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 95.24%   
cur_acc:  ['0.9524']
his_acc:  ['0.9524']
Clustering into  9  clusters
Clusters:  [0 1 6 0 3 3 6 8 1 1 1 2 8 2 7 5 6 4 0 4]
Losses:  12.292929649353027 4.297410488128662 0.7145065665245056
CurrentTrain: epoch  0, batch     0 | loss: 12.2929296Losses:  12.255475044250488 4.561153888702393 0.7221635580062866
CurrentTrain: epoch  0, batch     1 | loss: 12.2554750Losses:  12.25263786315918 3.384650707244873 0.8618885278701782
CurrentTrain: epoch  0, batch     2 | loss: 12.2526379Losses:  7.623457908630371 -0.0 0.10504733771085739
CurrentTrain: epoch  0, batch     3 | loss: 7.6234579Losses:  10.414581298828125 2.9708876609802246 0.7799913287162781
CurrentTrain: epoch  1, batch     0 | loss: 10.4145813Losses:  11.158402442932129 4.905355453491211 0.6039131879806519
CurrentTrain: epoch  1, batch     1 | loss: 11.1584024Losses:  12.656444549560547 4.227359771728516 0.7384702563285828
CurrentTrain: epoch  1, batch     2 | loss: 12.6564445Losses:  7.4766106605529785 -0.0 0.12736894190311432
CurrentTrain: epoch  1, batch     3 | loss: 7.4766107Losses:  10.35450267791748 3.4071531295776367 0.6845484972000122
CurrentTrain: epoch  2, batch     0 | loss: 10.3545027Losses:  9.803995132446289 3.5245015621185303 0.671811580657959
CurrentTrain: epoch  2, batch     1 | loss: 9.8039951Losses:  8.78707504272461 2.488917827606201 0.740854024887085
CurrentTrain: epoch  2, batch     2 | loss: 8.7870750Losses:  4.910583019256592 -0.0 0.2328244000673294
CurrentTrain: epoch  2, batch     3 | loss: 4.9105830Losses:  8.69638729095459 2.255227565765381 0.7041895985603333
CurrentTrain: epoch  3, batch     0 | loss: 8.6963873Losses:  8.737439155578613 2.237898349761963 0.7368209362030029
CurrentTrain: epoch  3, batch     1 | loss: 8.7374392Losses:  9.158409118652344 3.432215690612793 0.6586382985115051
CurrentTrain: epoch  3, batch     2 | loss: 9.1584091Losses:  4.089583873748779 -0.0 0.11171326786279678
CurrentTrain: epoch  3, batch     3 | loss: 4.0895839Losses:  9.91075325012207 5.476616859436035 0.5697714686393738
CurrentTrain: epoch  4, batch     0 | loss: 9.9107533Losses:  11.707388877868652 5.316439628601074 0.45894336700439453
CurrentTrain: epoch  4, batch     1 | loss: 11.7073889Losses:  8.777984619140625 2.795682191848755 0.6611754894256592
CurrentTrain: epoch  4, batch     2 | loss: 8.7779846Losses:  2.284618377685547 -0.0 -0.0
CurrentTrain: epoch  4, batch     3 | loss: 2.2846184Losses:  8.04328727722168 2.692974090576172 0.7237234115600586
CurrentTrain: epoch  5, batch     0 | loss: 8.0432873Losses:  7.977966785430908 2.5169243812561035 0.7124292254447937
CurrentTrain: epoch  5, batch     1 | loss: 7.9779668Losses:  7.611782550811768 2.647094964981079 0.6790575981140137
CurrentTrain: epoch  5, batch     2 | loss: 7.6117826Losses:  8.045300483703613 -0.0 0.10568065941333771
CurrentTrain: epoch  5, batch     3 | loss: 8.0453005Losses:  9.767434120178223 4.332886219024658 0.5222272872924805
CurrentTrain: epoch  6, batch     0 | loss: 9.7674341Losses:  8.279382705688477 2.8257713317871094 0.6332495212554932
CurrentTrain: epoch  6, batch     1 | loss: 8.2793827Losses:  6.3086700439453125 1.9884467124938965 0.6853551864624023
CurrentTrain: epoch  6, batch     2 | loss: 6.3086700Losses:  2.5510318279266357 -0.0 -0.0
CurrentTrain: epoch  6, batch     3 | loss: 2.5510318Losses:  9.482329368591309 3.5569944381713867 0.5583029985427856
CurrentTrain: epoch  7, batch     0 | loss: 9.4823294Losses:  7.474658966064453 3.1369733810424805 0.6843986511230469
CurrentTrain: epoch  7, batch     1 | loss: 7.4746590Losses:  7.390893936157227 2.9162256717681885 0.6117055416107178
CurrentTrain: epoch  7, batch     2 | loss: 7.3908939Losses:  5.701275825500488 -0.0 0.11409564316272736
CurrentTrain: epoch  7, batch     3 | loss: 5.7012758Losses:  8.721485137939453 3.5295169353485107 0.6204437017440796
CurrentTrain: epoch  8, batch     0 | loss: 8.7214851Losses:  7.833472728729248 2.761013984680176 0.6109739542007446
CurrentTrain: epoch  8, batch     1 | loss: 7.8334727Losses:  6.297290802001953 1.980454921722412 0.6512281894683838
CurrentTrain: epoch  8, batch     2 | loss: 6.2972908Losses:  1.8219807147979736 -0.0 0.10767430067062378
CurrentTrain: epoch  8, batch     3 | loss: 1.8219807Losses:  7.972774505615234 3.29180908203125 0.5772783756256104
CurrentTrain: epoch  9, batch     0 | loss: 7.9727745Losses:  9.086931228637695 4.78056526184082 0.6162514686584473
CurrentTrain: epoch  9, batch     1 | loss: 9.0869312Losses:  8.1886568069458 3.0578441619873047 0.647278904914856
CurrentTrain: epoch  9, batch     2 | loss: 8.1886568Losses:  3.6723616123199463 -0.0 0.1100219339132309
CurrentTrain: epoch  9, batch     3 | loss: 3.6723616
#### applies to jurisdiction ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "applies to jurisdiction" means the item (institution, law, public office, public register...) or statement belongs to or has power over or applies to the value (a territorial jurisdiction: a country, state, municipality, ...).
Here is an example:
Relation: applies to jurisdiction
Context: in 2004 the catalan government gave him the george cross .
Head Entity: george cross
Tail Entity: catalan
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "applies to jurisdiction":
Relation: applies to jurisdiction  
Context: The new environmental regulations introduced by the European Union will affect all member states.  
Head Entity: environmental regulations  
Tail Entity: European Union  

Relation: applies to jurisdiction  
Context: The Supreme Court's ruling has significant implications for the state of California's water rights.  
Head Entity: Supreme Court  
Tail Entity: California  
#### performer ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "performer" means actor, musician, band or other performer associated with this role or musical work.
Here is an example:
Relation: performer
Context: it is prominently featured on jean michel jarre 's albums " oxygène " ( 1977 ) and " équinoxe " ( 1978 ) .
Head Entity: équinoxe
Tail Entity: jean michel jarre
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "performer":
Relation: performer  
Context: The legendary band Queen is known for their iconic song "Bohemian Rhapsody," which showcases Freddie Mercury's extraordinary vocal talent.  
Head Entity: Bohemian Rhapsody  
Tail Entity: Queen  

Relation: performer  
Context: Taylor Swift captivated audiences with her heartfelt performance of "Love Story" during the concert last summer.  
Head Entity: Love Story  
Tail Entity: Taylor Swift  
#### manufacturer ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "manufacturer" means manufacturer or producer of this product.
Here is an example:
Relation: manufacturer
Context: the glasflügel h-201 standard " libelle " ( german : " dragonfly " ) is an early composite standard class single - seat sailplane produced by glasflügel from 1967 .
Head Entity: dragonfly
Tail Entity: glasflügel
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "manufacturer":
Relation: manufacturer  
Context: The iPhone 13, a smartphone developed by Apple, was released in September 2021 and features advanced camera technology and improved battery life.  
Head Entity: iPhone 13  
Tail Entity: Apple  

Relation: manufacturer  
Context: The Tesla Model S is an all-electric luxury sedan that was first introduced by Tesla, Inc. in 2012, showcasing innovative technology and performance.  
Head Entity: Tesla Model S  
Tail Entity: Tesla, Inc.  
#### participant ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "participant" means person, group of people or organization (object) that actively takes/took part in an event or process.
Here is an example:
Relation: participant
Context: norway was represented in the eurovision song contest 2005 by the song " in my dreams " performed by wig wam .
Head Entity: eurovision song contest 2005
Tail Entity: wig wam
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "participant":
Relation: participant  
Context: the united states was represented in the 2020 summer olympics by a team of over 600 athletes competing in various sports.  
Head Entity: 2020 summer olympics  
Tail Entity: united states  

Relation: participant  
Context: the annual tech conference featured several startups, including a presentation by innovative solutions, which showcased their latest product.  
Head Entity: annual tech conference  
Tail Entity: innovative solutions  
#### operating system ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "operating system" means operating system (OS) on which a software works or the OS installed on hardware.
Here is an example:
Relation: operating system
Context: the software is written in python and works on all the platforms supported by blender : windows , osx and linux .
Head Entity: blender
Tail Entity: windows
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "operating system":
Relation: operating system  
Context: The latest version of the application is compatible with various operating systems, including macOS, Windows, and Ubuntu.  
Head Entity: application  
Tail Entity: macOS  

Relation: operating system  
Context: Many developers prefer to use Linux as their primary operating system due to its flexibility and open-source nature.  
Head Entity: developers  
Tail Entity: Linux  
#### competition class ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "competition class" means official classification by a regulating body under which the subject (events, teams, participants, or equipment) qualifies for inclusion.
Here is an example:
Relation: competition class
Context: in his next bout , he won the wbc light welterweight title by defeating roger mayweather for a second time .
Head Entity: roger mayweather
Tail Entity: light welterweight
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "competition class":
Relation: competition class  
Context: The team secured their place in the tournament by qualifying under the UEFA Champions League regulations.  
Head Entity: UEFA Champions League  
Tail Entity: tournament  

Relation: competition class  
Context: The athlete competed in the 100 meters event, which is classified under the Olympic Games standards.  
Head Entity: Olympic Games  
Tail Entity: 100 meters event  
#### successful candidate ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "successful candidate" means person elected after the election.
Here is an example:
Relation: successful candidate
Context: swartz served as swedish prime minister for only seven months before being forced from power after the swedish elections of 1917 and replaced by the liberal government of nils edén .
Head Entity: swedish elections of 1917
Tail Entity: nils edén
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "successful candidate":
Relation: successful candidate  
Context: in the recent municipal elections, the voters chose maria gonzalez as the new mayor of the city, marking a significant shift in local governance.  
Head Entity: municipal elections  
Tail Entity: maria gonzalez  

Relation: successful candidate  
Context: after a closely contested race, the citizens of springfield elected john doe as their new governor, succeeding the long-serving incumbent.  
Head Entity: springfield elections  
Tail Entity: john doe  
#### sibling ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "sibling" means the subject and the object have the same parents.
Here is an example:
Relation: sibling
Context: although the latter 's eldest son , lǫgmaðr , appears to have succeeded to the kingship , he was soon forced to contend with factions supporting his younger brothers : haraldr , and óláfr .
Head Entity: lǫgmaðr
Tail Entity: óláfr
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "sibling":
Relation: sibling  
Context: In the royal family of the ancient kingdom, the two brothers, Eirik and Sigurd, were known for their fierce rivalry, yet they shared a bond that only siblings could understand.  
Head Entity: Eirik  
Tail Entity: Sigurd  

Relation: sibling  
Context: During the family reunion, it became clear that the two sisters, Maria and Elena, had grown apart over the years, but their shared childhood memories reminded them of their strong sibling connection.  
Head Entity: Maria  
Tail Entity: Elena  
#### military branch ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "military branch" means branch to which this military unit, award, office, or person belongs.
Here is an example:
Relation: military branch
Context: iain harrison is a competitive shooter and former british army captain .
Head Entity: iain harrison
Tail Entity: british army
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "military branch":
Relation: military branch  
Context: the united states navy is known for its naval operations and maritime security.  
Head Entity: united states navy  
Tail Entity: naval operations  

Relation: military branch  
Context: general patton was a prominent leader in the united states army during world war ii.  
Head Entity: general patton  
Tail Entity: united states army  
#### child ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "child" means subject has object as child.
Here is an example:
Relation: child
Context: ziad rahbani is the son of the lebanese famous composer assi rahbani and nouhad haddad , the famous lebanese female singer known as fairuz .
Head Entity: assi rahbani
Tail Entity: ziad rahbani
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "child":
Relation: child  
Context: in the animated series, the character of simon is the son of the adventurous couple, marie and tom.  
Head Entity: marie  
Tail Entity: simon  

Relation: child  
Context: the renowned physicist albert einstein had a daughter named lieserl, who was born in 1902.  
Head Entity: albert einstein  
Tail Entity: lieserl  
Losses:  7.099592208862305 0.8445554971694946 0.6803176403045654
MemoryTrain:  epoch  0, batch     0 | loss: 7.0995922Losses:  6.44034481048584 0.45489978790283203 0.7737653851509094
MemoryTrain:  epoch  0, batch     1 | loss: 6.4403448Losses:  9.5936861038208 2.418600559234619 0.7183945178985596
MemoryTrain:  epoch  0, batch     2 | loss: 9.5936861Losses:  6.62403678894043 0.7151593565940857 0.605944812297821
MemoryTrain:  epoch  0, batch     3 | loss: 6.6240368Losses:  6.7540812492370605 1.1798573732376099 0.5785308480262756
MemoryTrain:  epoch  1, batch     0 | loss: 6.7540812Losses:  7.725953578948975 1.1694703102111816 0.7332954406738281
MemoryTrain:  epoch  1, batch     1 | loss: 7.7259536Losses:  6.999285697937012 1.4827899932861328 0.7085081338882446
MemoryTrain:  epoch  1, batch     2 | loss: 6.9992857Losses:  5.480573654174805 0.4930656850337982 0.7078629732131958
MemoryTrain:  epoch  1, batch     3 | loss: 5.4805737Losses:  6.91985559463501 1.4074410200119019 0.7187866568565369
MemoryTrain:  epoch  2, batch     0 | loss: 6.9198556Losses:  6.583881378173828 1.1690764427185059 0.6838651895523071
MemoryTrain:  epoch  2, batch     1 | loss: 6.5838814Losses:  4.585902214050293 0.8993160724639893 0.563774824142456
MemoryTrain:  epoch  2, batch     2 | loss: 4.5859022Losses:  5.047372341156006 1.2723928689956665 0.5208526849746704
MemoryTrain:  epoch  2, batch     3 | loss: 5.0473723Losses:  5.56049108505249 1.881847858428955 0.5872154235839844
MemoryTrain:  epoch  3, batch     0 | loss: 5.5604911Losses:  6.567481994628906 2.2295689582824707 0.5640105605125427
MemoryTrain:  epoch  3, batch     1 | loss: 6.5674820Losses:  5.887772083282471 1.1969292163848877 0.7177377343177795
MemoryTrain:  epoch  3, batch     2 | loss: 5.8877721Losses:  4.63940954208374 0.7492362856864929 0.262677401304245
MemoryTrain:  epoch  3, batch     3 | loss: 4.6394095Losses:  5.021713733673096 1.0055068731307983 0.6116675138473511
MemoryTrain:  epoch  4, batch     0 | loss: 5.0217137Losses:  3.863572120666504 0.23471704125404358 0.6507269144058228
MemoryTrain:  epoch  4, batch     1 | loss: 3.8635721Losses:  4.880249500274658 0.9283337593078613 0.5810419321060181
MemoryTrain:  epoch  4, batch     2 | loss: 4.8802495Losses:  4.3734636306762695 0.9321907758712769 0.5627932548522949
MemoryTrain:  epoch  4, batch     3 | loss: 4.3734636Losses:  4.344703197479248 1.3171061277389526 0.6612508893013
MemoryTrain:  epoch  5, batch     0 | loss: 4.3447032Losses:  5.415292263031006 1.5584664344787598 0.6330379843711853
MemoryTrain:  epoch  5, batch     1 | loss: 5.4152923Losses:  4.298316478729248 1.5397727489471436 0.7038553953170776
MemoryTrain:  epoch  5, batch     2 | loss: 4.2983165Losses:  5.226384162902832 1.3290166854858398 0.6347872614860535
MemoryTrain:  epoch  5, batch     3 | loss: 5.2263842Losses:  5.443930149078369 1.3978544473648071 0.7699015736579895
MemoryTrain:  epoch  6, batch     0 | loss: 5.4439301Losses:  4.689249515533447 2.3168601989746094 0.6210121512413025
MemoryTrain:  epoch  6, batch     1 | loss: 4.6892495Losses:  3.8934082984924316 0.48291927576065063 0.6056997776031494
MemoryTrain:  epoch  6, batch     2 | loss: 3.8934083Losses:  2.809659242630005 0.25189995765686035 0.5671567320823669
MemoryTrain:  epoch  6, batch     3 | loss: 2.8096592Losses:  3.8417062759399414 1.185720443725586 0.679733395576477
MemoryTrain:  epoch  7, batch     0 | loss: 3.8417063Losses:  3.781069278717041 0.9265762567520142 0.6257342100143433
MemoryTrain:  epoch  7, batch     1 | loss: 3.7810693Losses:  2.8804237842559814 0.6409976482391357 0.582015335559845
MemoryTrain:  epoch  7, batch     2 | loss: 2.8804238Losses:  4.936858654022217 1.1870434284210205 0.6743943095207214
MemoryTrain:  epoch  7, batch     3 | loss: 4.9368587Losses:  3.507784843444824 1.028315782546997 0.6689054369926453
MemoryTrain:  epoch  8, batch     0 | loss: 3.5077848Losses:  3.9874396324157715 1.4187897443771362 0.5568599700927734
MemoryTrain:  epoch  8, batch     1 | loss: 3.9874396Losses:  3.94073748588562 0.8650778532028198 0.706305205821991
MemoryTrain:  epoch  8, batch     2 | loss: 3.9407375Losses:  3.1920931339263916 0.6366896629333496 0.6470000743865967
MemoryTrain:  epoch  8, batch     3 | loss: 3.1920931Losses:  4.214481353759766 1.2037954330444336 0.7551818490028381
MemoryTrain:  epoch  9, batch     0 | loss: 4.2144814Losses:  2.947441339492798 0.6799618005752563 0.597568690776825
MemoryTrain:  epoch  9, batch     1 | loss: 2.9474413Losses:  4.984915733337402 2.1642258167266846 0.6313107013702393
MemoryTrain:  epoch  9, batch     2 | loss: 4.9849157Losses:  2.9840521812438965 0.5939933061599731 0.5389672517776489
MemoryTrain:  epoch  9, batch     3 | loss: 2.9840522
[EVAL] batch:    0 | acc: 0.00%,  total acc: 0.00%   [EVAL] batch:    1 | acc: 6.25%,  total acc: 3.12%   [EVAL] batch:    2 | acc: 0.00%,  total acc: 2.08%   [EVAL] batch:    3 | acc: 12.50%,  total acc: 4.69%   [EVAL] batch:    4 | acc: 6.25%,  total acc: 5.00%   [EVAL] batch:    5 | acc: 6.25%,  total acc: 5.21%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 16.07%   [EVAL] batch:    7 | acc: 68.75%,  total acc: 22.66%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 30.56%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 36.25%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 40.34%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 44.27%   [EVAL] batch:   12 | acc: 100.00%,  total acc: 48.56%   [EVAL] batch:   13 | acc: 87.50%,  total acc: 51.34%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 54.17%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 57.03%   [EVAL] batch:   16 | acc: 87.50%,  total acc: 58.82%   [EVAL] batch:   17 | acc: 100.00%,  total acc: 61.11%   [EVAL] batch:   18 | acc: 87.50%,  total acc: 62.50%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 63.12%   [EVAL] batch:   20 | acc: 56.25%,  total acc: 62.80%   [EVAL] batch:   21 | acc: 68.75%,  total acc: 63.07%   [EVAL] batch:   22 | acc: 62.50%,  total acc: 63.04%   [EVAL] batch:   23 | acc: 68.75%,  total acc: 63.28%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 64.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 65.62%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 66.90%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 68.08%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 69.18%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 70.21%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 71.17%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 71.88%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 72.73%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 73.53%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 74.29%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 75.68%   [EVAL] batch:   37 | acc: 75.00%,  total acc: 75.66%   [EVAL] batch:   38 | acc: 62.50%,  total acc: 75.32%   [EVAL] batch:   39 | acc: 56.25%,  total acc: 74.84%   [EVAL] batch:   40 | acc: 56.25%,  total acc: 74.39%   [EVAL] batch:   41 | acc: 68.75%,  total acc: 74.26%   [EVAL] batch:   42 | acc: 81.25%,  total acc: 74.42%   [EVAL] batch:   43 | acc: 68.75%,  total acc: 74.29%   [EVAL] batch:   44 | acc: 62.50%,  total acc: 74.03%   [EVAL] batch:   45 | acc: 56.25%,  total acc: 73.64%   [EVAL] batch:   46 | acc: 43.75%,  total acc: 73.01%   [EVAL] batch:   47 | acc: 75.00%,  total acc: 73.05%   [EVAL] batch:   48 | acc: 68.75%,  total acc: 72.96%   [EVAL] batch:   49 | acc: 43.75%,  total acc: 72.38%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 72.79%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 73.20%   [EVAL] batch:   52 | acc: 87.50%,  total acc: 73.47%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 73.61%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 73.98%   [EVAL] batch:   55 | acc: 81.25%,  total acc: 74.11%   [EVAL] batch:   56 | acc: 56.25%,  total acc: 73.79%   [EVAL] batch:   57 | acc: 37.50%,  total acc: 73.17%   [EVAL] batch:   58 | acc: 31.25%,  total acc: 72.46%   [EVAL] batch:   59 | acc: 50.00%,  total acc: 72.08%   [EVAL] batch:   60 | acc: 18.75%,  total acc: 71.21%   [EVAL] batch:   61 | acc: 43.75%,  total acc: 70.77%   [EVAL] batch:   62 | acc: 18.75%,  total acc: 69.94%   
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:    3 | acc: 93.75%,  total acc: 92.19%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 91.25%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 91.07%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 92.19%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 93.06%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 93.18%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 94.14%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 94.12%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 94.10%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 94.41%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 94.69%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 94.94%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 94.60%   [EVAL] batch:   22 | acc: 75.00%,  total acc: 93.75%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   24 | acc: 81.25%,  total acc: 93.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 93.51%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 93.52%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 93.53%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 93.53%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 93.95%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 94.14%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 94.32%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 94.49%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 94.46%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 94.62%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 94.76%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 94.74%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 94.87%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 95.00%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 95.12%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 95.24%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 95.20%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 95.17%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 95.14%   [EVAL] batch:   45 | acc: 81.25%,  total acc: 94.84%   [EVAL] batch:   46 | acc: 75.00%,  total acc: 94.41%   [EVAL] batch:   47 | acc: 81.25%,  total acc: 94.14%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 94.26%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 94.12%   [EVAL] batch:   50 | acc: 87.50%,  total acc: 94.00%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 93.99%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 93.99%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 94.10%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 93.86%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 93.75%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 93.64%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 93.85%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 93.85%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 93.15%   [EVAL] batch:   63 | acc: 6.25%,  total acc: 91.80%   [EVAL] batch:   64 | acc: 0.00%,  total acc: 90.38%   [EVAL] batch:   65 | acc: 6.25%,  total acc: 89.11%   [EVAL] batch:   66 | acc: 6.25%,  total acc: 87.87%   [EVAL] batch:   67 | acc: 12.50%,  total acc: 86.76%   [EVAL] batch:   68 | acc: 37.50%,  total acc: 86.05%   [EVAL] batch:   69 | acc: 87.50%,  total acc: 86.07%   [EVAL] batch:   70 | acc: 75.00%,  total acc: 85.92%   [EVAL] batch:   71 | acc: 81.25%,  total acc: 85.85%   [EVAL] batch:   72 | acc: 87.50%,  total acc: 85.87%   [EVAL] batch:   73 | acc: 93.75%,  total acc: 85.98%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 86.00%   [EVAL] batch:   75 | acc: 93.75%,  total acc: 86.10%   [EVAL] batch:   76 | acc: 93.75%,  total acc: 86.20%   [EVAL] batch:   77 | acc: 93.75%,  total acc: 86.30%   [EVAL] batch:   78 | acc: 93.75%,  total acc: 86.39%   [EVAL] batch:   79 | acc: 93.75%,  total acc: 86.48%   [EVAL] batch:   80 | acc: 100.00%,  total acc: 86.65%   [EVAL] batch:   81 | acc: 81.25%,  total acc: 86.59%   [EVAL] batch:   82 | acc: 62.50%,  total acc: 86.30%   [EVAL] batch:   83 | acc: 50.00%,  total acc: 85.86%   [EVAL] batch:   84 | acc: 75.00%,  total acc: 85.74%   [EVAL] batch:   85 | acc: 68.75%,  total acc: 85.54%   [EVAL] batch:   86 | acc: 75.00%,  total acc: 85.42%   [EVAL] batch:   87 | acc: 93.75%,  total acc: 85.51%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 85.67%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 85.83%   [EVAL] batch:   90 | acc: 100.00%,  total acc: 85.99%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 86.14%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 86.29%   [EVAL] batch:   93 | acc: 93.75%,  total acc: 86.37%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 86.51%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 86.65%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 86.79%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 86.93%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 87.06%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 87.19%   [EVAL] batch:  100 | acc: 56.25%,  total acc: 86.88%   [EVAL] batch:  101 | acc: 62.50%,  total acc: 86.64%   [EVAL] batch:  102 | acc: 56.25%,  total acc: 86.35%   [EVAL] batch:  103 | acc: 56.25%,  total acc: 86.06%   [EVAL] batch:  104 | acc: 87.50%,  total acc: 86.07%   [EVAL] batch:  105 | acc: 62.50%,  total acc: 85.85%   [EVAL] batch:  106 | acc: 75.00%,  total acc: 85.75%   [EVAL] batch:  107 | acc: 50.00%,  total acc: 85.42%   [EVAL] batch:  108 | acc: 62.50%,  total acc: 85.21%   [EVAL] batch:  109 | acc: 50.00%,  total acc: 84.89%   [EVAL] batch:  110 | acc: 75.00%,  total acc: 84.80%   [EVAL] batch:  111 | acc: 50.00%,  total acc: 84.49%   [EVAL] batch:  112 | acc: 75.00%,  total acc: 84.40%   [EVAL] batch:  113 | acc: 87.50%,  total acc: 84.43%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 84.57%   [EVAL] batch:  115 | acc: 81.25%,  total acc: 84.54%   [EVAL] batch:  116 | acc: 87.50%,  total acc: 84.56%   [EVAL] batch:  117 | acc: 93.75%,  total acc: 84.64%   [EVAL] batch:  118 | acc: 75.00%,  total acc: 84.56%   [EVAL] batch:  119 | acc: 31.25%,  total acc: 84.11%   [EVAL] batch:  120 | acc: 43.75%,  total acc: 83.78%   [EVAL] batch:  121 | acc: 25.00%,  total acc: 83.30%   [EVAL] batch:  122 | acc: 43.75%,  total acc: 82.98%   [EVAL] batch:  123 | acc: 12.50%,  total acc: 82.41%   [EVAL] batch:  124 | acc: 56.25%,  total acc: 82.20%   
cur_acc:  ['0.9524', '0.6994']
his_acc:  ['0.9524', '0.8220']
Clustering into  14  clusters
Clusters:  [ 1  9  0  3  4  4  0  8  6  6  9  2  8  7 11 13  0  5  3  5  1  7 10 12
 10  9  2  1  8  3]
Losses:  12.030277252197266 4.482997894287109 0.6914640665054321
CurrentTrain: epoch  0, batch     0 | loss: 12.0302773Losses:  11.254053115844727 3.75006365776062 0.7892376780509949
CurrentTrain: epoch  0, batch     1 | loss: 11.2540531Losses:  11.040504455566406 2.9422755241394043 0.7901955842971802
CurrentTrain: epoch  0, batch     2 | loss: 11.0405045Losses:  6.562776565551758 -0.0 0.17095772922039032
CurrentTrain: epoch  0, batch     3 | loss: 6.5627766Losses:  9.48880672454834 2.8541579246520996 0.7235609292984009
CurrentTrain: epoch  1, batch     0 | loss: 9.4888067Losses:  11.591615676879883 4.667717933654785 0.7453328967094421
CurrentTrain: epoch  1, batch     1 | loss: 11.5916157Losses:  12.147127151489258 5.036107063293457 0.7471407055854797
CurrentTrain: epoch  1, batch     2 | loss: 12.1471272Losses:  5.346148490905762 -0.0 0.2483510673046112
CurrentTrain: epoch  1, batch     3 | loss: 5.3461485Losses:  11.009230613708496 3.959193468093872 0.6772087812423706
CurrentTrain: epoch  2, batch     0 | loss: 11.0092306Losses:  9.777799606323242 3.627379894256592 0.6747227311134338
CurrentTrain: epoch  2, batch     1 | loss: 9.7777996Losses:  8.922056198120117 2.8456101417541504 0.6787101030349731
CurrentTrain: epoch  2, batch     2 | loss: 8.9220562Losses:  2.8876540660858154 -0.0 0.15810507535934448
CurrentTrain: epoch  2, batch     3 | loss: 2.8876541Losses:  7.337390422821045 2.3872156143188477 0.760945737361908
CurrentTrain: epoch  3, batch     0 | loss: 7.3373904Losses:  9.786906242370605 3.436047077178955 0.7504186630249023
CurrentTrain: epoch  3, batch     1 | loss: 9.7869062Losses:  8.756793975830078 2.861081600189209 0.7245609760284424
CurrentTrain: epoch  3, batch     2 | loss: 8.7567940Losses:  3.6760125160217285 -0.0 -0.0
CurrentTrain: epoch  3, batch     3 | loss: 3.6760125Losses:  7.499259948730469 2.3593695163726807 0.7349674701690674
CurrentTrain: epoch  4, batch     0 | loss: 7.4992599Losses:  8.294614791870117 2.900430917739868 0.7280821204185486
CurrentTrain: epoch  4, batch     1 | loss: 8.2946148Losses:  10.474170684814453 3.927685260772705 0.6892106533050537
CurrentTrain: epoch  4, batch     2 | loss: 10.4741707Losses:  2.7114148139953613 -0.0 0.12739524245262146
CurrentTrain: epoch  4, batch     3 | loss: 2.7114148Losses:  8.6691312789917 3.867480516433716 0.5529145002365112
CurrentTrain: epoch  5, batch     0 | loss: 8.6691313Losses:  9.907114028930664 4.6588897705078125 0.6170013546943665
CurrentTrain: epoch  5, batch     1 | loss: 9.9071140Losses:  9.38231086730957 3.9522008895874023 0.5518333911895752
CurrentTrain: epoch  5, batch     2 | loss: 9.3823109Losses:  3.1305737495422363 -0.0 -0.0
CurrentTrain: epoch  5, batch     3 | loss: 3.1305737Losses:  8.429288864135742 3.6864776611328125 0.7238879203796387
CurrentTrain: epoch  6, batch     0 | loss: 8.4292889Losses:  8.625021934509277 4.136264324188232 0.6447928547859192
CurrentTrain: epoch  6, batch     1 | loss: 8.6250219Losses:  8.976040840148926 3.6016969680786133 0.640778660774231
CurrentTrain: epoch  6, batch     2 | loss: 8.9760408Losses:  5.455538272857666 -0.0 0.10418716073036194
CurrentTrain: epoch  6, batch     3 | loss: 5.4555383Losses:  6.53822660446167 2.326190710067749 0.607964813709259
CurrentTrain: epoch  7, batch     0 | loss: 6.5382266Losses:  9.512208938598633 4.261666297912598 0.6339144110679626
CurrentTrain: epoch  7, batch     1 | loss: 9.5122089Losses:  7.318909168243408 2.794851779937744 0.6232563257217407
CurrentTrain: epoch  7, batch     2 | loss: 7.3189092Losses:  1.9738425016403198 -0.0 0.1838497370481491
CurrentTrain: epoch  7, batch     3 | loss: 1.9738425Losses:  9.160623550415039 3.9171714782714844 0.6050865650177002
CurrentTrain: epoch  8, batch     0 | loss: 9.1606236Losses:  6.351504325866699 2.260714054107666 0.6872622966766357
CurrentTrain: epoch  8, batch     1 | loss: 6.3515043Losses:  7.7175493240356445 3.61366605758667 0.6855018138885498
CurrentTrain: epoch  8, batch     2 | loss: 7.7175493Losses:  2.056955575942993 -0.0 0.12239125370979309
CurrentTrain: epoch  8, batch     3 | loss: 2.0569556Losses:  8.360637664794922 3.7389156818389893 0.5987375974655151
CurrentTrain: epoch  9, batch     0 | loss: 8.3606377Losses:  6.400071144104004 2.200474739074707 0.5955066680908203
CurrentTrain: epoch  9, batch     1 | loss: 6.4000711Losses:  7.945836544036865 3.8274965286254883 0.6777738332748413
CurrentTrain: epoch  9, batch     2 | loss: 7.9458365Losses:  1.8923307657241821 -0.0 0.11406788229942322
CurrentTrain: epoch  9, batch     3 | loss: 1.8923308
#### main subject ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "main subject" means primary topic of a work.
Here is an example:
Relation: main subject
Context: auerbach is prominently featured in the documentary film , " the first basket " , about jewish basketball history .
Head Entity: the first basket
Tail Entity: basketball
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "main subject":
Relation: main subject  
Context: the book "sapiens: a brief history of humankind" explores the evolution of human societies and cultures.  
Head Entity: sapiens: a brief history of humankind  
Tail Entity: human societies  

Relation: main subject  
Context: the article discusses the impact of climate change on global agriculture and food security.  
Head Entity: the impact of climate change  
Tail Entity: global agriculture  
#### participating team ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "participating team" means teams that actively takes/took part in an event or process.
Here is an example:
Relation: participating team
Context: he represented romania at euro 2000 , his first major tournament where they beat england 3–2 , and drew 1–1 against germany .
Head Entity: euro 2000
Tail Entity: germany
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "participating team":
Relation: participating team  
Context: The United States women's national soccer team has consistently performed well in international competitions, winning the FIFA Women's World Cup multiple times, including in 2019, where they faced the Netherlands in the final.  
Head Entity: FIFA Women's World Cup  
Tail Entity: Netherlands  

Relation: participating team  
Context: During the 2022 Winter Olympics, the Canadian ice hockey team showcased their skills, ultimately defeating the Russian Olympic Committee in a thrilling final match.  
Head Entity: 2022 Winter Olympics  
Tail Entity: Russian Olympic Committee  
#### has part ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "has part" means part of this subject.
Here is an example:
Relation: has part
Context: among the competing artists was previous eurovision song contest entrant robin juhkental who represented estonia in 2010 as part of the group malcolm lincoln .
Head Entity: malcolm lincoln
Tail Entity: robin juhkental
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "has part":
Relation: has part  
Context: The famous painting "The Last Supper" features several apostles, including Peter, who is depicted prominently on the left side of the artwork.  
Head Entity: The Last Supper  
Tail Entity: Peter  

Relation: has part  
Context: The smartphone model Galaxy S21 comes with a high-resolution camera that includes a wide-angle lens, allowing users to capture stunning landscapes.  
Head Entity: Galaxy S21  
Tail Entity: wide-angle lens  
#### nominated for ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "nominated for" means award nomination received by a person, organisation or creative work.
Here is an example:
Relation: nominated for
Context: gyllenhaal was nominated for an academy award for best supporting actress for her performance .
Head Entity: gyllenhaal
Tail Entity: academy award for best supporting actress
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "nominated for":
Relation: nominated for  
Context: the film was nominated for several prestigious awards, including the golden globe for best drama.  
Head Entity: the film  
Tail Entity: golden globe for best drama  

Relation: nominated for  
Context: she was nominated for the emmy award for her outstanding performance in the television series.  
Head Entity: she  
Tail Entity: emmy award for her outstanding performance in the television series  
#### instance of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "instance of" means that class of which this subject is a particular example and member.
Here is an example:
Relation: instance of
Context: on march 2015 , cube entertainment launched the multi - national girl group clc including original members seunghee , yujin , seungyeon , sorn and yeeun .
Head Entity: clc
Tail Entity: girl group
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "instance of":
Relation: instance of  
Context: The smartphone iPhone 13 was released by Apple in September 2021, showcasing advanced technology and features.  
Head Entity: iPhone 13  
Tail Entity: smartphone  

Relation: instance of  
Context: The Great Wall of China is a historic fortification that stretches across northern China, attracting millions of tourists each year.  
Head Entity: Great Wall of China  
Tail Entity: fortification  
#### mountain range ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "mountain range" means range or subrange to which the geographical item belongs.
Here is an example:
Relation: mountain range
Context: brand peak is a sharp snow - covered peak located east - southeast of the eternity range and northwest of mount duemler , in palmer land .
Head Entity: mount duemler
Tail Entity: eternity range
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "mountain range":
Relation: mountain range  
Context: the rocky mountains extend from Canada through the United States and into Mexico, forming a major mountain range in North America.  
Head Entity: rocky mountains  
Tail Entity: North America  

Relation: mountain range  
Context: the appalachian mountains are one of the oldest mountain ranges in the world, stretching from Canada down to Alabama.  
Head Entity: appalachian mountains  
Tail Entity: Alabama  
#### screenwriter ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "screenwriter" means person who wrote the script for subject item.
Here is an example:
Relation: screenwriter
Context: future films include " mcqueen " , ian bonhote ’s documentary about the fashion designer alexander mcqueen , brad anderson ’s thriller " beirut " , and mark pellington ’s drama " nostalgia " .
Head Entity: nostalgia
Tail Entity: mark pellington
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "screenwriter":
Relation: screenwriter  
Context: the upcoming movie "the great adventure" is penned by renowned screenwriter jessica taylor, who has previously won multiple awards for her work in the industry.  
Head Entity: the great adventure  
Tail Entity: jessica taylor  

Relation: screenwriter  
Context: in the latest film "shadows of the past," the script was crafted by the talented screenwriter robert lang, known for his gripping storytelling and character development.  
Head Entity: shadows of the past  
Tail Entity: robert lang  
#### language of work or name ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "language of work or name" means language associated with this creative work (such as books, shows, songs, broadcasts or websites) or a name.
Here is an example:
Relation: language of work or name
Context: the publisher found success in 1993 with tarō gomi 's " everyone poops " , a japanese language import that sold over one million copies and is now their best - known title .
Head Entity: everyone poops
Tail Entity: japanese language
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "language of work or name":
Relation: language of work or name  
Context: The animated series "Avatar: The Last Airbender" features a rich narrative and is primarily produced in English, appealing to a global audience.  
Head Entity: Avatar: The Last Airbender  
Tail Entity: English  

Relation: language of work or name  
Context: The famous novel "Cien años de soledad" by Gabriel García Márquez is celebrated for its magical realism and is originally written in Spanish, reflecting the culture of Latin America.  
Head Entity: Cien años de soledad  
Tail Entity: Spanish  
#### operator ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "operator" means person, profession, or organization that operates the equipment, facility, or service.
Here is an example:
Relation: operator
Context: it hosts both freight traffic and metra 's union pacific / northwest line commuter rail service .
Head Entity: union pacific / northwest line
Tail Entity: metra
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "operator":
Relation: operator  
Context: The city council has contracted with GreenTech Solutions to manage the waste disposal services in the area.  
Head Entity: GreenTech Solutions  
Tail Entity: city council  

Relation: operator  
Context: The airline has partnered with SkyHigh Services to provide ground handling at the international airport.  
Head Entity: SkyHigh Services  
Tail Entity: airline  
#### religion ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "religion" means religion of a person, organization or religious building, or associated with this subject.
Here is an example:
Relation: religion
Context: the most important lama of this series was the third changkya , rolpai dorje , who was preceptor to the qianlong emperor of china , and chief representative of tibetan buddhism at the qing court .
Head Entity: rolpai dorje
Tail Entity: tibetan buddhism
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "religion":
Relation: religion  
Context: the cathedral of notre-dame is a famous example of french gothic architecture and serves as the seat of the archbishop of paris, representing the roman catholic faith in the heart of the city.  
Head Entity: cathedral of notre-dame  
Tail Entity: roman catholic faith  

Relation: religion  
Context: the dalai lama is a prominent figure in the world, known for his teachings on compassion and non-violence, and is the spiritual leader of tibetan buddhism.  
Head Entity: dalai lama  
Tail Entity: tibetan buddhism  
Losses:  5.506096839904785 0.6350106000900269 0.9124228954315186
MemoryTrain:  epoch  0, batch     0 | loss: 5.5060968Losses:  6.175601482391357 0.463650643825531 0.8491393327713013
MemoryTrain:  epoch  0, batch     1 | loss: 6.1756015Losses:  5.830028057098389 0.9244054555892944 0.8805767893791199
MemoryTrain:  epoch  0, batch     2 | loss: 5.8300281Losses:  7.176651954650879 0.6938191652297974 0.7640878558158875
MemoryTrain:  epoch  0, batch     3 | loss: 7.1766520Losses:  5.808182716369629 0.8137273788452148 0.6763421297073364
MemoryTrain:  epoch  0, batch     4 | loss: 5.8081827Losses:  5.622584819793701 1.0973986387252808 0.5980600118637085
MemoryTrain:  epoch  0, batch     5 | loss: 5.6225848Losses:  5.203375816345215 1.0040737390518188 0.7522541880607605
MemoryTrain:  epoch  1, batch     0 | loss: 5.2033758Losses:  7.518486976623535 1.066176414489746 0.9160338640213013
MemoryTrain:  epoch  1, batch     1 | loss: 7.5184870Losses:  5.2007012367248535 0.31721794605255127 0.8921763300895691
MemoryTrain:  epoch  1, batch     2 | loss: 5.2007012Losses:  5.4690961837768555 0.7363121509552002 0.7941492199897766
MemoryTrain:  epoch  1, batch     3 | loss: 5.4690962Losses:  4.336215972900391 0.22153276205062866 0.797081708908081
MemoryTrain:  epoch  1, batch     4 | loss: 4.3362160Losses:  4.506540298461914 -0.0 0.6075006723403931
MemoryTrain:  epoch  1, batch     5 | loss: 4.5065403Losses:  4.764003753662109 0.5204930305480957 0.8308134078979492
MemoryTrain:  epoch  2, batch     0 | loss: 4.7640038Losses:  5.1479597091674805 0.38490986824035645 0.8890010714530945
MemoryTrain:  epoch  2, batch     1 | loss: 5.1479597Losses:  5.3618316650390625 0.6711575984954834 0.9120715856552124
MemoryTrain:  epoch  2, batch     2 | loss: 5.3618317Losses:  5.480494976043701 0.8243668079376221 0.7824204564094543
MemoryTrain:  epoch  2, batch     3 | loss: 5.4804950Losses:  4.146414756774902 0.28204935789108276 0.8729079961776733
MemoryTrain:  epoch  2, batch     4 | loss: 4.1464148Losses:  4.785248756408691 0.6536800265312195 0.5984638333320618
MemoryTrain:  epoch  2, batch     5 | loss: 4.7852488Losses:  4.891012668609619 0.5990360975265503 0.8089828491210938
MemoryTrain:  epoch  3, batch     0 | loss: 4.8910127Losses:  5.0303955078125 0.6036854982376099 0.8147012591362
MemoryTrain:  epoch  3, batch     1 | loss: 5.0303955Losses:  4.559350490570068 0.7808094024658203 0.7713879346847534
MemoryTrain:  epoch  3, batch     2 | loss: 4.5593505Losses:  5.914065837860107 0.8193694353103638 0.8006971478462219
MemoryTrain:  epoch  3, batch     3 | loss: 5.9140658Losses:  4.675918102264404 1.258819580078125 0.7572934031486511
MemoryTrain:  epoch  3, batch     4 | loss: 4.6759181Losses:  3.026252269744873 -0.0 0.5746694803237915
MemoryTrain:  epoch  3, batch     5 | loss: 3.0262523Losses:  4.6523356437683105 0.9258227944374084 0.7907889485359192
MemoryTrain:  epoch  4, batch     0 | loss: 4.6523356Losses:  4.291107177734375 0.6095996499061584 0.8210657238960266
MemoryTrain:  epoch  4, batch     1 | loss: 4.2911072Losses:  4.081634044647217 0.855746865272522 0.7767955660820007
MemoryTrain:  epoch  4, batch     2 | loss: 4.0816340Losses:  5.6689581871032715 1.161956787109375 0.8831591606140137
MemoryTrain:  epoch  4, batch     3 | loss: 5.6689582Losses:  3.2418878078460693 0.4807766377925873 0.7290491461753845
MemoryTrain:  epoch  4, batch     4 | loss: 3.2418878Losses:  3.703054428100586 -0.0 0.6222603917121887
MemoryTrain:  epoch  4, batch     5 | loss: 3.7030544Losses:  4.526031494140625 0.883183479309082 0.8437830209732056
MemoryTrain:  epoch  5, batch     0 | loss: 4.5260315Losses:  4.367224216461182 0.8687642216682434 0.7723711133003235
MemoryTrain:  epoch  5, batch     1 | loss: 4.3672242Losses:  3.613987445831299 0.4664985239505768 0.8603571057319641
MemoryTrain:  epoch  5, batch     2 | loss: 3.6139874Losses:  4.288429260253906 1.01006281375885 0.8246100544929504
MemoryTrain:  epoch  5, batch     3 | loss: 4.2884293Losses:  3.4892306327819824 0.5573673844337463 0.8431083559989929
MemoryTrain:  epoch  5, batch     4 | loss: 3.4892306Losses:  4.428556442260742 0.8163862228393555 0.611583948135376
MemoryTrain:  epoch  5, batch     5 | loss: 4.4285564Losses:  4.198700904846191 0.8546446561813354 0.7609236836433411
MemoryTrain:  epoch  6, batch     0 | loss: 4.1987009Losses:  3.1289548873901367 -0.0 0.8914963603019714
MemoryTrain:  epoch  6, batch     1 | loss: 3.1289549Losses:  2.943589687347412 0.2575543522834778 0.9245623350143433
MemoryTrain:  epoch  6, batch     2 | loss: 2.9435897Losses:  5.076528549194336 1.7921404838562012 0.813580334186554
MemoryTrain:  epoch  6, batch     3 | loss: 5.0765285Losses:  4.084673881530762 0.8693326115608215 0.8287235498428345
MemoryTrain:  epoch  6, batch     4 | loss: 4.0846739Losses:  3.157071590423584 0.3965766727924347 0.5963956117630005
MemoryTrain:  epoch  6, batch     5 | loss: 3.1570716Losses:  3.9320485591888428 0.8533092737197876 0.8909794688224792
MemoryTrain:  epoch  7, batch     0 | loss: 3.9320486Losses:  3.1386518478393555 0.5052213668823242 0.8271604776382446
MemoryTrain:  epoch  7, batch     1 | loss: 3.1386518Losses:  4.587275505065918 1.2625343799591064 0.770461916923523
MemoryTrain:  epoch  7, batch     2 | loss: 4.5872755Losses:  3.997469902038574 1.0316557884216309 0.6861363649368286
MemoryTrain:  epoch  7, batch     3 | loss: 3.9974699Losses:  2.974161148071289 0.7433550357818604 0.7180829644203186
MemoryTrain:  epoch  7, batch     4 | loss: 2.9741611Losses:  3.7327213287353516 0.24897988140583038 0.5562514066696167
MemoryTrain:  epoch  7, batch     5 | loss: 3.7327213Losses:  3.257014751434326 0.7805730700492859 0.8030475974082947
MemoryTrain:  epoch  8, batch     0 | loss: 3.2570148Losses:  4.128626823425293 0.5969480276107788 0.9519195556640625
MemoryTrain:  epoch  8, batch     1 | loss: 4.1286268Losses:  3.4913368225097656 1.0179036855697632 0.6888937950134277
MemoryTrain:  epoch  8, batch     2 | loss: 3.4913368Losses:  3.900818109512329 0.6261270046234131 0.8562150001525879
MemoryTrain:  epoch  8, batch     3 | loss: 3.9008181Losses:  3.449155330657959 1.010381817817688 0.6491559743881226
MemoryTrain:  epoch  8, batch     4 | loss: 3.4491553Losses:  3.0877022743225098 0.40485239028930664 0.7019278407096863
MemoryTrain:  epoch  8, batch     5 | loss: 3.0877023Losses:  3.0812273025512695 0.7513055205345154 0.7430582642555237
MemoryTrain:  epoch  9, batch     0 | loss: 3.0812273Losses:  3.0658130645751953 0.3057640790939331 0.8589910268783569
MemoryTrain:  epoch  9, batch     1 | loss: 3.0658131Losses:  3.080615758895874 0.8286548852920532 0.7200129628181458
MemoryTrain:  epoch  9, batch     2 | loss: 3.0806158Losses:  3.696793794631958 0.5566723942756653 0.8458197712898254
MemoryTrain:  epoch  9, batch     3 | loss: 3.6967938Losses:  3.6291892528533936 1.0594911575317383 0.7780258059501648
MemoryTrain:  epoch  9, batch     4 | loss: 3.6291893Losses:  3.000256299972534 -0.0 0.6301591396331787
MemoryTrain:  epoch  9, batch     5 | loss: 3.0002563
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    1 | acc: 68.75%,  total acc: 65.62%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 68.75%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 70.31%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 70.00%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 73.96%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 77.68%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 80.47%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 81.94%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 82.50%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 84.09%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 84.90%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 84.13%   [EVAL] batch:   13 | acc: 25.00%,  total acc: 79.91%   [EVAL] batch:   14 | acc: 31.25%,  total acc: 76.67%   [EVAL] batch:   15 | acc: 12.50%,  total acc: 72.66%   [EVAL] batch:   16 | acc: 37.50%,  total acc: 70.59%   [EVAL] batch:   17 | acc: 43.75%,  total acc: 69.10%   [EVAL] batch:   18 | acc: 37.50%,  total acc: 67.43%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 68.75%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 70.24%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 71.31%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 72.55%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 73.70%   [EVAL] batch:   24 | acc: 93.75%,  total acc: 74.50%   [EVAL] batch:   25 | acc: 50.00%,  total acc: 73.56%   [EVAL] batch:   26 | acc: 50.00%,  total acc: 72.69%   [EVAL] batch:   27 | acc: 56.25%,  total acc: 72.10%   [EVAL] batch:   28 | acc: 50.00%,  total acc: 71.34%   [EVAL] batch:   29 | acc: 43.75%,  total acc: 70.42%   [EVAL] batch:   30 | acc: 31.25%,  total acc: 69.15%   [EVAL] batch:   31 | acc: 81.25%,  total acc: 69.53%   [EVAL] batch:   32 | acc: 81.25%,  total acc: 69.89%   [EVAL] batch:   33 | acc: 87.50%,  total acc: 70.40%   [EVAL] batch:   34 | acc: 87.50%,  total acc: 70.89%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 71.53%   [EVAL] batch:   36 | acc: 87.50%,  total acc: 71.96%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 72.70%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 73.40%   [EVAL] batch:   39 | acc: 87.50%,  total acc: 73.75%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 74.39%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 75.58%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 76.14%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 76.53%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 77.04%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 77.53%   [EVAL] batch:   47 | acc: 81.25%,  total acc: 77.60%   [EVAL] batch:   48 | acc: 93.75%,  total acc: 77.93%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 78.25%   [EVAL] batch:   50 | acc: 43.75%,  total acc: 77.57%   [EVAL] batch:   51 | acc: 50.00%,  total acc: 77.04%   [EVAL] batch:   52 | acc: 62.50%,  total acc: 76.77%   [EVAL] batch:   53 | acc: 62.50%,  total acc: 76.50%   [EVAL] batch:   54 | acc: 62.50%,  total acc: 76.25%   [EVAL] batch:   55 | acc: 68.75%,  total acc: 76.12%   [EVAL] batch:   56 | acc: 81.25%,  total acc: 76.21%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 76.40%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 76.69%   [EVAL] batch:   59 | acc: 81.25%,  total acc: 76.77%   [EVAL] batch:   60 | acc: 93.75%,  total acc: 77.05%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 76.92%   [EVAL] batch:   62 | acc: 43.75%,  total acc: 76.39%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 89.06%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 88.75%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 87.50%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 88.28%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 88.12%   [EVAL] batch:   10 | acc: 75.00%,  total acc: 86.93%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 86.46%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 86.06%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 86.61%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 87.08%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 87.89%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 88.24%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 88.54%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 89.14%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 89.38%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 89.49%   [EVAL] batch:   22 | acc: 81.25%,  total acc: 89.13%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 89.32%   [EVAL] batch:   24 | acc: 81.25%,  total acc: 89.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 89.42%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 89.81%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 90.18%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 90.30%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 90.93%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 91.21%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 91.48%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 91.73%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 91.79%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 92.01%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 92.23%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 92.43%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 92.63%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 92.81%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 92.99%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 93.15%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 93.17%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 93.18%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 93.19%   [EVAL] batch:   45 | acc: 81.25%,  total acc: 92.93%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 92.69%   [EVAL] batch:   47 | acc: 81.25%,  total acc: 92.45%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 92.60%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 92.62%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 92.65%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 92.67%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 92.69%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 92.82%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 92.73%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 92.75%   [EVAL] batch:   56 | acc: 81.25%,  total acc: 92.54%   [EVAL] batch:   57 | acc: 68.75%,  total acc: 92.13%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 92.06%   [EVAL] batch:   59 | acc: 87.50%,  total acc: 91.98%   [EVAL] batch:   60 | acc: 87.50%,  total acc: 91.91%   [EVAL] batch:   61 | acc: 81.25%,  total acc: 91.73%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 91.07%   [EVAL] batch:   63 | acc: 25.00%,  total acc: 90.04%   [EVAL] batch:   64 | acc: 31.25%,  total acc: 89.13%   [EVAL] batch:   65 | acc: 18.75%,  total acc: 88.07%   [EVAL] batch:   66 | acc: 31.25%,  total acc: 87.22%   [EVAL] batch:   67 | acc: 18.75%,  total acc: 86.21%   [EVAL] batch:   68 | acc: 56.25%,  total acc: 85.78%   [EVAL] batch:   69 | acc: 93.75%,  total acc: 85.89%   [EVAL] batch:   70 | acc: 75.00%,  total acc: 85.74%   [EVAL] batch:   71 | acc: 68.75%,  total acc: 85.50%   [EVAL] batch:   72 | acc: 87.50%,  total acc: 85.53%   [EVAL] batch:   73 | acc: 87.50%,  total acc: 85.56%   [EVAL] batch:   74 | acc: 81.25%,  total acc: 85.50%   [EVAL] batch:   75 | acc: 100.00%,  total acc: 85.69%   [EVAL] batch:   76 | acc: 93.75%,  total acc: 85.80%   [EVAL] batch:   77 | acc: 81.25%,  total acc: 85.74%   [EVAL] batch:   78 | acc: 87.50%,  total acc: 85.76%   [EVAL] batch:   79 | acc: 87.50%,  total acc: 85.78%   [EVAL] batch:   80 | acc: 93.75%,  total acc: 85.88%   [EVAL] batch:   81 | acc: 68.75%,  total acc: 85.67%   [EVAL] batch:   82 | acc: 37.50%,  total acc: 85.09%   [EVAL] batch:   83 | acc: 25.00%,  total acc: 84.38%   [EVAL] batch:   84 | acc: 43.75%,  total acc: 83.90%   [EVAL] batch:   85 | acc: 18.75%,  total acc: 83.14%   [EVAL] batch:   86 | acc: 50.00%,  total acc: 82.76%   [EVAL] batch:   87 | acc: 87.50%,  total acc: 82.81%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 83.01%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 83.19%   [EVAL] batch:   90 | acc: 100.00%,  total acc: 83.38%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 83.56%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 83.74%   [EVAL] batch:   93 | acc: 93.75%,  total acc: 83.84%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 84.01%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 84.18%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 84.34%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 84.50%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 84.66%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 84.81%   [EVAL] batch:  100 | acc: 56.25%,  total acc: 84.53%   [EVAL] batch:  101 | acc: 62.50%,  total acc: 84.31%   [EVAL] batch:  102 | acc: 56.25%,  total acc: 84.04%   [EVAL] batch:  103 | acc: 56.25%,  total acc: 83.77%   [EVAL] batch:  104 | acc: 87.50%,  total acc: 83.81%   [EVAL] batch:  105 | acc: 62.50%,  total acc: 83.61%   [EVAL] batch:  106 | acc: 81.25%,  total acc: 83.59%   [EVAL] batch:  107 | acc: 50.00%,  total acc: 83.28%   [EVAL] batch:  108 | acc: 62.50%,  total acc: 83.08%   [EVAL] batch:  109 | acc: 56.25%,  total acc: 82.84%   [EVAL] batch:  110 | acc: 75.00%,  total acc: 82.77%   [EVAL] batch:  111 | acc: 50.00%,  total acc: 82.48%   [EVAL] batch:  112 | acc: 75.00%,  total acc: 82.41%   [EVAL] batch:  113 | acc: 87.50%,  total acc: 82.46%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 82.61%   [EVAL] batch:  115 | acc: 87.50%,  total acc: 82.65%   [EVAL] batch:  116 | acc: 87.50%,  total acc: 82.69%   [EVAL] batch:  117 | acc: 87.50%,  total acc: 82.73%   [EVAL] batch:  118 | acc: 87.50%,  total acc: 82.77%   [EVAL] batch:  119 | acc: 37.50%,  total acc: 82.40%   [EVAL] batch:  120 | acc: 50.00%,  total acc: 82.13%   [EVAL] batch:  121 | acc: 43.75%,  total acc: 81.81%   [EVAL] batch:  122 | acc: 56.25%,  total acc: 81.61%   [EVAL] batch:  123 | acc: 31.25%,  total acc: 81.20%   [EVAL] batch:  124 | acc: 62.50%,  total acc: 81.05%   [EVAL] batch:  125 | acc: 62.50%,  total acc: 80.90%   [EVAL] batch:  126 | acc: 68.75%,  total acc: 80.81%   [EVAL] batch:  127 | acc: 75.00%,  total acc: 80.76%   [EVAL] batch:  128 | acc: 75.00%,  total acc: 80.72%   [EVAL] batch:  129 | acc: 68.75%,  total acc: 80.62%   [EVAL] batch:  130 | acc: 93.75%,  total acc: 80.73%   [EVAL] batch:  131 | acc: 100.00%,  total acc: 80.87%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 81.02%   [EVAL] batch:  133 | acc: 93.75%,  total acc: 81.11%   [EVAL] batch:  134 | acc: 87.50%,  total acc: 81.16%   [EVAL] batch:  135 | acc: 100.00%,  total acc: 81.30%   [EVAL] batch:  136 | acc: 93.75%,  total acc: 81.39%   [EVAL] batch:  137 | acc: 75.00%,  total acc: 81.34%   [EVAL] batch:  138 | acc: 25.00%,  total acc: 80.94%   [EVAL] batch:  139 | acc: 31.25%,  total acc: 80.58%   [EVAL] batch:  140 | acc: 12.50%,  total acc: 80.10%   [EVAL] batch:  141 | acc: 37.50%,  total acc: 79.80%   [EVAL] batch:  142 | acc: 43.75%,  total acc: 79.55%   [EVAL] batch:  143 | acc: 37.50%,  total acc: 79.25%   [EVAL] batch:  144 | acc: 93.75%,  total acc: 79.35%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 79.49%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 79.59%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 79.73%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 79.87%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 79.96%   [EVAL] batch:  150 | acc: 50.00%,  total acc: 79.76%   [EVAL] batch:  151 | acc: 50.00%,  total acc: 79.56%   [EVAL] batch:  152 | acc: 56.25%,  total acc: 79.41%   [EVAL] batch:  153 | acc: 50.00%,  total acc: 79.22%   [EVAL] batch:  154 | acc: 43.75%,  total acc: 78.99%   [EVAL] batch:  155 | acc: 31.25%,  total acc: 78.69%   [EVAL] batch:  156 | acc: 81.25%,  total acc: 78.70%   [EVAL] batch:  157 | acc: 81.25%,  total acc: 78.72%   [EVAL] batch:  158 | acc: 87.50%,  total acc: 78.77%   [EVAL] batch:  159 | acc: 87.50%,  total acc: 78.83%   [EVAL] batch:  160 | acc: 93.75%,  total acc: 78.92%   [EVAL] batch:  161 | acc: 87.50%,  total acc: 78.97%   [EVAL] batch:  162 | acc: 100.00%,  total acc: 79.10%   [EVAL] batch:  163 | acc: 100.00%,  total acc: 79.23%   [EVAL] batch:  164 | acc: 87.50%,  total acc: 79.28%   [EVAL] batch:  165 | acc: 100.00%,  total acc: 79.41%   [EVAL] batch:  166 | acc: 100.00%,  total acc: 79.53%   [EVAL] batch:  167 | acc: 100.00%,  total acc: 79.65%   [EVAL] batch:  168 | acc: 100.00%,  total acc: 79.77%   [EVAL] batch:  169 | acc: 93.75%,  total acc: 79.85%   [EVAL] batch:  170 | acc: 100.00%,  total acc: 79.97%   [EVAL] batch:  171 | acc: 100.00%,  total acc: 80.09%   [EVAL] batch:  172 | acc: 81.25%,  total acc: 80.09%   [EVAL] batch:  173 | acc: 93.75%,  total acc: 80.17%   [EVAL] batch:  174 | acc: 93.75%,  total acc: 80.25%   [EVAL] batch:  175 | acc: 43.75%,  total acc: 80.04%   [EVAL] batch:  176 | acc: 50.00%,  total acc: 79.87%   [EVAL] batch:  177 | acc: 62.50%,  total acc: 79.78%   [EVAL] batch:  178 | acc: 62.50%,  total acc: 79.68%   [EVAL] batch:  179 | acc: 62.50%,  total acc: 79.58%   [EVAL] batch:  180 | acc: 68.75%,  total acc: 79.52%   [EVAL] batch:  181 | acc: 81.25%,  total acc: 79.53%   [EVAL] batch:  182 | acc: 87.50%,  total acc: 79.58%   [EVAL] batch:  183 | acc: 93.75%,  total acc: 79.65%   [EVAL] batch:  184 | acc: 81.25%,  total acc: 79.66%   [EVAL] batch:  185 | acc: 93.75%,  total acc: 79.74%   [EVAL] batch:  186 | acc: 68.75%,  total acc: 79.68%   [EVAL] batch:  187 | acc: 43.75%,  total acc: 79.49%   
cur_acc:  ['0.9524', '0.6994', '0.7639']
his_acc:  ['0.9524', '0.8220', '0.7949']
Clustering into  19  clusters
Clusters:  [ 0  3 12  2  1  1 12  6  8  8  3 13  6 10 17 15 18  9  2  9  0 10  4 14
  4  3 16  5  6  2 11  7  9  5  5  7  9 13  8 13]
Losses:  11.199630737304688 3.7505297660827637 0.5087794661521912
CurrentTrain: epoch  0, batch     0 | loss: 11.1996307Losses:  10.047395706176758 3.916234254837036 0.6139355897903442
CurrentTrain: epoch  0, batch     1 | loss: 10.0473957Losses:  11.992661476135254 4.333399295806885 0.5924139618873596
CurrentTrain: epoch  0, batch     2 | loss: 11.9926615Losses:  6.4207563400268555 -0.0 0.13491418957710266
CurrentTrain: epoch  0, batch     3 | loss: 6.4207563Losses:  10.137948036193848 4.358493328094482 0.47498342394828796
CurrentTrain: epoch  1, batch     0 | loss: 10.1379480Losses:  10.476770401000977 3.875380277633667 0.553382158279419
CurrentTrain: epoch  1, batch     1 | loss: 10.4767704Losses:  9.106559753417969 3.3507137298583984 0.5641497373580933
CurrentTrain: epoch  1, batch     2 | loss: 9.1065598Losses:  5.033748626708984 -0.0 -0.0
CurrentTrain: epoch  1, batch     3 | loss: 5.0337486Losses:  9.669614791870117 3.31882381439209 0.5525872111320496
CurrentTrain: epoch  2, batch     0 | loss: 9.6696148Losses:  10.185190200805664 5.52675199508667 0.5619370937347412
CurrentTrain: epoch  2, batch     1 | loss: 10.1851902Losses:  8.976194381713867 4.40667724609375 0.4317968487739563
CurrentTrain: epoch  2, batch     2 | loss: 8.9761944Losses:  7.971611022949219 -0.0 0.14021015167236328
CurrentTrain: epoch  2, batch     3 | loss: 7.9716110Losses:  7.70327615737915 2.914877414703369 0.5795826315879822
CurrentTrain: epoch  3, batch     0 | loss: 7.7032762Losses:  8.370017051696777 3.359633445739746 0.4638432562351227
CurrentTrain: epoch  3, batch     1 | loss: 8.3700171Losses:  7.691683292388916 3.526555061340332 0.5714568495750427
CurrentTrain: epoch  3, batch     2 | loss: 7.6916833Losses:  7.606533050537109 -0.0 0.15322402119636536
CurrentTrain: epoch  3, batch     3 | loss: 7.6065331Losses:  7.6037678718566895 2.9322686195373535 0.5228301882743835
CurrentTrain: epoch  4, batch     0 | loss: 7.6037679Losses:  9.481094360351562 5.168787956237793 0.3369704484939575
CurrentTrain: epoch  4, batch     1 | loss: 9.4810944Losses:  6.851555824279785 2.374418020248413 0.5622051358222961
CurrentTrain: epoch  4, batch     2 | loss: 6.8515558Losses:  3.435995101928711 -0.0 0.23402681946754456
CurrentTrain: epoch  4, batch     3 | loss: 3.4359951Losses:  6.866192817687988 2.9588847160339355 0.5709439516067505
CurrentTrain: epoch  5, batch     0 | loss: 6.8661928Losses:  7.515698432922363 3.965125560760498 0.4419102668762207
CurrentTrain: epoch  5, batch     1 | loss: 7.5156984Losses:  7.335989952087402 3.3313469886779785 0.572152316570282
CurrentTrain: epoch  5, batch     2 | loss: 7.3359900Losses:  7.735430717468262 -0.0 -0.0
CurrentTrain: epoch  5, batch     3 | loss: 7.7354307Losses:  8.237750053405762 3.831319808959961 0.5533899664878845
CurrentTrain: epoch  6, batch     0 | loss: 8.2377501Losses:  6.402120590209961 2.9321377277374268 0.4600144922733307
CurrentTrain: epoch  6, batch     1 | loss: 6.4021206Losses:  6.489680290222168 3.2263526916503906 0.3357018232345581
CurrentTrain: epoch  6, batch     2 | loss: 6.4896803Losses:  5.325503826141357 -0.0 0.1513485461473465
CurrentTrain: epoch  6, batch     3 | loss: 5.3255038Losses:  6.946419715881348 2.643420696258545 0.5580850839614868
CurrentTrain: epoch  7, batch     0 | loss: 6.9464197Losses:  6.25337028503418 3.5403599739074707 0.48568135499954224
CurrentTrain: epoch  7, batch     1 | loss: 6.2533703Losses:  7.938968658447266 3.7435269355773926 0.5297228097915649
CurrentTrain: epoch  7, batch     2 | loss: 7.9389687Losses:  4.934778213500977 -0.0 0.09988084435462952
CurrentTrain: epoch  7, batch     3 | loss: 4.9347782Losses:  6.692432880401611 3.4783108234405518 0.4587806165218353
CurrentTrain: epoch  8, batch     0 | loss: 6.6924329Losses:  6.166811466217041 2.7687644958496094 0.5175347328186035
CurrentTrain: epoch  8, batch     1 | loss: 6.1668115Losses:  5.526320934295654 1.8369882106781006 0.5116933584213257
CurrentTrain: epoch  8, batch     2 | loss: 5.5263209Losses:  1.966899037361145 -0.0 0.13416996598243713
CurrentTrain: epoch  8, batch     3 | loss: 1.9668990Losses:  6.6502227783203125 3.6037275791168213 0.35287803411483765
CurrentTrain: epoch  9, batch     0 | loss: 6.6502228Losses:  7.328851699829102 3.7480549812316895 0.5201302170753479
CurrentTrain: epoch  9, batch     1 | loss: 7.3288517Losses:  6.002942085266113 3.3787784576416016 0.4260122776031494
CurrentTrain: epoch  9, batch     2 | loss: 6.0029421Losses:  2.871340036392212 -0.0 0.14375847578048706
CurrentTrain: epoch  9, batch     3 | loss: 2.8713400
#### follows ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "follows" means immediately prior item in a series of which the subject is a part.
Here is an example:
Relation: follows
Context: peugeot took a similar step in 2010 when replacing the 407 and long - running but unpopular 607 with a single model , the 508 .
Head Entity: 508
Tail Entity: 407
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "follows":
Relation: follows  
Context: In the 2021 season, the team introduced the new model, the X5, which follows the successful launch of the X3 in 2020.  
Head Entity: X5  
Tail Entity: X3  

Relation: follows  
Context: The latest smartphone, the Galaxy S21, follows the previous model, the Galaxy S20, which was released just a year earlier.  
Head Entity: Galaxy S21  
Tail Entity: Galaxy S20  
#### sport ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "sport" means sport that the subject participates or participated in or is associated with.
Here is an example:
Relation: sport
Context: dick packer was a u.s. soccer center forward who was a member of the u.s. team at the 1956 summer olympics .
Head Entity: dick packer
Tail Entity: soccer
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "sport":
Relation: sport  
Context: serena williams is a professional tennis player who has won numerous grand slam titles throughout her career.  
Head Entity: serena williams  
Tail Entity: tennis  

Relation: sport  
Context: lebron james is known for his exceptional skills in basketball and has played for the los angeles lakers in the nba.  
Head Entity: lebron james  
Tail Entity: basketball  
#### father ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "father" means male parent of the subject.
Here is an example:
Relation: father
Context: after the crowning of his nephew radoslav , the son of stefan , sava left the serbian maritime in 1229 for a trip to palestine .
Head Entity: radoslav
Tail Entity: stefan
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "father":
Relation: father  
Context: in the historical records, it is noted that king henry was the father of prince edward, who later became a significant figure in the kingdom.  
Head Entity: prince edward  
Tail Entity: king henry  

Relation: father  
Context: during the family reunion, it was revealed that michael is the father of emily, who has recently graduated from university.  
Head Entity: emily  
Tail Entity: michael  
#### original network ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "original network" means network or service that originally broadcast a radio or television program.
Here is an example:
Relation: original network
Context: on november 2 , 2016 msnbc 's " rachel maddow show " featured an expose on yahweh ben yahweh and his followers connection to the 2016 donald trump presidential campaign .
Head Entity: rachel maddow show
Tail Entity: msnbc
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "original network":
Relation: original network  
Context: the popular series "breaking bad" premiered on january 20, 2008, on the amc network, quickly gaining a massive following and critical acclaim.  
Head Entity: breaking bad  
Tail Entity: amc  

Relation: original network  
Context: "the office," a beloved mockumentary-style sitcom, first aired on march 24, 2005, on nbc, becoming a cultural phenomenon over its nine-season run.  
Head Entity: the office  
Tail Entity: nbc  
#### original language of film or TV show ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "original language of film or TV show" means language in which a film or a performance work was originally created.
Here is an example:
Relation: original language of film or TV show
Context: he was also nominated twice for the filmfare best telugu actor award , for the films bhale bhale magadivoy and " gentleman " .
Head Entity: gentleman
Tail Entity: telugu
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "original language of film or TV show":
Relation: original language of film or TV show  
Context: The movie "Parasite" received critical acclaim and was originally produced in Korean, showcasing the talents of its director Bong Joon-ho.  
Head Entity: Parasite  
Tail Entity: Korean  

Relation: original language of film or TV show  
Context: The animated series "Avatar: The Last Airbender" was created in English and has gained a massive following since its release.  
Head Entity: Avatar: The Last Airbender  
Tail Entity: English  
#### league ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "league" means league in which team or player plays or has played in.
Here is an example:
Relation: league
Context: in the summer of 2013 he returned to montenegro and joined with fk rudar pljevlja , playing with the club for two seasons in the montenegrin first league .
Head Entity: fk rudar pljevlja
Tail Entity: montenegrin first league
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "league":
Relation: league  
Context: After a successful stint in the English Premier League, the player signed with FC Barcelona, competing in La Liga for the next three seasons.  
Head Entity: FC Barcelona  
Tail Entity: La Liga  

Relation: league  
Context: The young athlete made headlines when he joined the New York Yankees, showcasing his talent in Major League Baseball.  
Head Entity: New York Yankees  
Tail Entity: Major League Baseball  
#### mother ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "mother" means female parent of the subject.
Here is an example:
Relation: mother
Context: other teammates included dominique leray and élisabeth riffiod , whose son boris diaw currently plays in the nba for the spurs ( 2014 ) .
Head Entity: boris diaw
Tail Entity: élisabeth riffiod
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "mother":
Relation: mother  
Context: in a recent interview, actress jennifer garner spoke fondly of her children, including violet, who is the daughter of her ex-husband, ben affleck.  
Head Entity: violet  
Tail Entity: jennifer garner  

Relation: mother  
Context: during the family reunion, it was heartwarming to see how much emily cherished her time with her mother, who had always supported her dreams.  
Head Entity: emily  
Tail Entity: emily's mother  
#### instrument ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "instrument" means musical instrument that a person plays or teaches or used in a music occupation.
Here is an example:
Relation: instrument
Context: peter bastian ( 25 august 1943 in copenhagen – 27 march 2017 ) was a danish musician . peter bastian bio and cv he was educated in theoretical physics and classical bassoon .
Head Entity: peter bastian
Tail Entity: bassoon
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "instrument":
Relation: instrument  
Context: johann sebastian bach (31 march 1685 – 28 july 1750) was a german composer and musician of the baroque period. he was known for his exceptional skill in playing the organ and the harpsichord.  
Head Entity: johann sebastian bach  
Tail Entity: organ  

Relation: instrument  
Context: yo-yo ma, born on october 7, 1955, is a world-renowned cellist who has performed with numerous orchestras and in various genres. he is particularly famous for his interpretations of classical cello concertos.  
Head Entity: yo-yo ma  
Tail Entity: cello  
#### country ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "country" means sovereign state of this item (not to be used for human beings).
Here is an example:
Relation: country
Context: the type locality given is " fan - si - pan mountains , tonkin , indo - china " ( mount fansipan , sa pa district , nghệ an province , northwestern vietnam ) .
Head Entity: sa pa district
Tail Entity: vietnam
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "country":
Relation: country  
Context: the capital city of the country is located in the central region, known for its rich history and cultural heritage, particularly in the city of "hanoi" which serves as the political center of the nation.  
Head Entity: hanoi  
Tail Entity: vietnam  

Relation: country  
Context: the famous landmark "eiffel tower" is situated in the heart of the city, which is the capital of a country renowned for its art, fashion, and cuisine, specifically in "paris".  
Head Entity: paris  
Tail Entity: france  
#### characters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "characters" means characters which appear in this item (like plays, operas, operettas, books, comics, films, TV series, video games).
Here is an example:
Relation: characters
Context: klaus kinski , who would play dracula himself nine years later in " nosferatu the vampyre " , is also featured in the film as renfield .
Head Entity: nosferatu the vampyre
Tail Entity: renfield
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "characters":
Relation: characters  
Context: in the animated series "avatar: the last airbender," zuko is a complex character who struggles with his identity and family legacy.  
Head Entity: avatar: the last airbender  
Tail Entity: zuko  

Relation: characters  
Context: the novel "pride and prejudice" features elizabeth bennet, a strong-willed protagonist who navigates societal expectations and personal relationships.  
Head Entity: pride and prejudice  
Tail Entity: elizabeth bennet  
Losses:  4.715080738067627 0.759253740310669 0.8549717664718628
MemoryTrain:  epoch  0, batch     0 | loss: 4.7150807Losses:  5.042744159698486 1.7406129837036133 0.7154455184936523
MemoryTrain:  epoch  0, batch     1 | loss: 5.0427442Losses:  4.36331844329834 0.5193045139312744 0.9002962112426758
MemoryTrain:  epoch  0, batch     2 | loss: 4.3633184Losses:  5.900163173675537 1.7026865482330322 0.6892695426940918
MemoryTrain:  epoch  0, batch     3 | loss: 5.9001632Losses:  6.674875259399414 1.154215693473816 0.8587770462036133
MemoryTrain:  epoch  0, batch     4 | loss: 6.6748753Losses:  5.293145179748535 1.2862595319747925 0.7105101943016052
MemoryTrain:  epoch  0, batch     5 | loss: 5.2931452Losses:  5.221278190612793 0.9728266000747681 0.9439479112625122
MemoryTrain:  epoch  0, batch     6 | loss: 5.2212782Losses:  4.2456746101379395 -0.0 0.7281786203384399
MemoryTrain:  epoch  0, batch     7 | loss: 4.2456746Losses:  4.609302520751953 0.8343484997749329 0.8843032121658325
MemoryTrain:  epoch  1, batch     0 | loss: 4.6093025Losses:  5.625827312469482 0.2987867593765259 0.9253758192062378
MemoryTrain:  epoch  1, batch     1 | loss: 5.6258273Losses:  4.1088175773620605 0.741840660572052 0.8523004055023193
MemoryTrain:  epoch  1, batch     2 | loss: 4.1088176Losses:  4.535131931304932 0.8869963884353638 0.8938069939613342
MemoryTrain:  epoch  1, batch     3 | loss: 4.5351319Losses:  4.864502906799316 0.5210599899291992 0.8484020233154297
MemoryTrain:  epoch  1, batch     4 | loss: 4.8645029Losses:  5.000808238983154 0.7374368906021118 0.7975279688835144
MemoryTrain:  epoch  1, batch     5 | loss: 5.0008082Losses:  4.515769004821777 0.5136547088623047 0.7845935225486755
MemoryTrain:  epoch  1, batch     6 | loss: 4.5157690Losses:  4.054198265075684 0.4883744716644287 0.6292641758918762
MemoryTrain:  epoch  1, batch     7 | loss: 4.0541983Losses:  4.130593776702881 0.8611597418785095 0.799379289150238
MemoryTrain:  epoch  2, batch     0 | loss: 4.1305938Losses:  4.363492012023926 1.138136386871338 1.0077059268951416
MemoryTrain:  epoch  2, batch     1 | loss: 4.3634920Losses:  3.6277225017547607 0.5051400661468506 0.8176975250244141
MemoryTrain:  epoch  2, batch     2 | loss: 3.6277225Losses:  4.76918888092041 0.9448859691619873 0.8288938403129578
MemoryTrain:  epoch  2, batch     3 | loss: 4.7691889Losses:  5.210071086883545 0.9290004968643188 0.8546003103256226
MemoryTrain:  epoch  2, batch     4 | loss: 5.2100711Losses:  4.26393461227417 0.5645651817321777 0.9144173860549927
MemoryTrain:  epoch  2, batch     5 | loss: 4.2639346Losses:  3.495914936065674 0.5578029155731201 0.9421449899673462
MemoryTrain:  epoch  2, batch     6 | loss: 3.4959149Losses:  3.9646592140197754 0.36202722787857056 0.45507699251174927
MemoryTrain:  epoch  2, batch     7 | loss: 3.9646592Losses:  3.9522948265075684 0.3760087788105011 0.9866510629653931
MemoryTrain:  epoch  3, batch     0 | loss: 3.9522948Losses:  3.7991652488708496 0.2884635329246521 0.8136696219444275
MemoryTrain:  epoch  3, batch     1 | loss: 3.7991652Losses:  4.093705177307129 1.0150303840637207 0.9108899831771851
MemoryTrain:  epoch  3, batch     2 | loss: 4.0937052Losses:  3.599925994873047 0.4869263172149658 0.9436581134796143
MemoryTrain:  epoch  3, batch     3 | loss: 3.5999260Losses:  3.2171075344085693 0.21206381916999817 0.8022386431694031
MemoryTrain:  epoch  3, batch     4 | loss: 3.2171075Losses:  3.7282676696777344 0.32952532172203064 1.031738519668579
MemoryTrain:  epoch  3, batch     5 | loss: 3.7282677Losses:  3.3503596782684326 -0.0 1.043379545211792
MemoryTrain:  epoch  3, batch     6 | loss: 3.3503597Losses:  2.9360437393188477 -0.0 0.5797409415245056
MemoryTrain:  epoch  3, batch     7 | loss: 2.9360437Losses:  3.9215738773345947 0.9537946581840515 0.6908061504364014
MemoryTrain:  epoch  4, batch     0 | loss: 3.9215739Losses:  2.9608376026153564 0.22491250932216644 0.7350048422813416
MemoryTrain:  epoch  4, batch     1 | loss: 2.9608376Losses:  3.7322158813476562 0.2396838665008545 0.8936985731124878
MemoryTrain:  epoch  4, batch     2 | loss: 3.7322159Losses:  3.4001851081848145 -0.0 0.9807113409042358
MemoryTrain:  epoch  4, batch     3 | loss: 3.4001851Losses:  3.7353105545043945 0.4701206684112549 0.8534530401229858
MemoryTrain:  epoch  4, batch     4 | loss: 3.7353106Losses:  3.8311805725097656 0.5151925683021545 0.8733755946159363
MemoryTrain:  epoch  4, batch     5 | loss: 3.8311806Losses:  3.180234909057617 0.28868675231933594 0.86624675989151
MemoryTrain:  epoch  4, batch     6 | loss: 3.1802349Losses:  2.743373394012451 -0.0 0.6452217102050781
MemoryTrain:  epoch  4, batch     7 | loss: 2.7433734Losses:  3.5102035999298096 0.25785771012306213 0.8242632746696472
MemoryTrain:  epoch  5, batch     0 | loss: 3.5102036Losses:  4.431178092956543 1.163875937461853 0.7935255169868469
MemoryTrain:  epoch  5, batch     1 | loss: 4.4311781Losses:  3.066582679748535 -0.0 0.9188565015792847
MemoryTrain:  epoch  5, batch     2 | loss: 3.0665827Losses:  3.312628746032715 -0.0 0.9384845495223999
MemoryTrain:  epoch  5, batch     3 | loss: 3.3126287Losses:  3.5857667922973633 0.4466237425804138 0.920872688293457
MemoryTrain:  epoch  5, batch     4 | loss: 3.5857668Losses:  3.4313981533050537 0.767923891544342 0.9044699668884277
MemoryTrain:  epoch  5, batch     5 | loss: 3.4313982Losses:  3.227325439453125 0.5356934070587158 0.9661822319030762
MemoryTrain:  epoch  5, batch     6 | loss: 3.2273254Losses:  3.4029626846313477 0.712824285030365 0.5364352464675903
MemoryTrain:  epoch  5, batch     7 | loss: 3.4029627Losses:  3.834315776824951 1.0893051624298096 0.777923047542572
MemoryTrain:  epoch  6, batch     0 | loss: 3.8343158Losses:  3.4621129035949707 0.5043030381202698 0.8772305250167847
MemoryTrain:  epoch  6, batch     1 | loss: 3.4621129Losses:  4.584221839904785 1.128303050994873 0.8653859496116638
MemoryTrain:  epoch  6, batch     2 | loss: 4.5842218Losses:  3.422801971435547 1.051682949066162 0.7463824152946472
MemoryTrain:  epoch  6, batch     3 | loss: 3.4228020Losses:  4.074328899383545 0.9874262809753418 0.8486565947532654
MemoryTrain:  epoch  6, batch     4 | loss: 4.0743289Losses:  3.0119221210479736 0.2574348449707031 0.8855263590812683
MemoryTrain:  epoch  6, batch     5 | loss: 3.0119221Losses:  3.5338878631591797 0.6272092461585999 0.9238283038139343
MemoryTrain:  epoch  6, batch     6 | loss: 3.5338879Losses:  2.8984534740448 -0.0 0.677469789981842
MemoryTrain:  epoch  6, batch     7 | loss: 2.8984535Losses:  3.7153611183166504 0.4714549779891968 0.8424844741821289
MemoryTrain:  epoch  7, batch     0 | loss: 3.7153611Losses:  3.4925904273986816 0.28003108501434326 0.9646931886672974
MemoryTrain:  epoch  7, batch     1 | loss: 3.4925904Losses:  3.830562114715576 1.60542893409729 0.7107831835746765
MemoryTrain:  epoch  7, batch     2 | loss: 3.8305621Losses:  3.6413192749023438 0.8131874799728394 0.8239666223526001
MemoryTrain:  epoch  7, batch     3 | loss: 3.6413193Losses:  2.963944435119629 0.2809799015522003 0.9145832061767578
MemoryTrain:  epoch  7, batch     4 | loss: 2.9639444Losses:  4.407441139221191 1.4340647459030151 0.814293384552002
MemoryTrain:  epoch  7, batch     5 | loss: 4.4074411Losses:  3.0558624267578125 0.546401858329773 0.7445718050003052
MemoryTrain:  epoch  7, batch     6 | loss: 3.0558624Losses:  2.537601947784424 -0.0 0.5961371660232544
MemoryTrain:  epoch  7, batch     7 | loss: 2.5376019Losses:  3.1603338718414307 0.5125218629837036 0.782486617565155
MemoryTrain:  epoch  8, batch     0 | loss: 3.1603339Losses:  3.4098308086395264 0.30162134766578674 0.8662562966346741
MemoryTrain:  epoch  8, batch     1 | loss: 3.4098308Losses:  2.5935895442962646 -0.0 0.9590890407562256
MemoryTrain:  epoch  8, batch     2 | loss: 2.5935895Losses:  4.1094794273376465 0.5989824533462524 0.9972972273826599
MemoryTrain:  epoch  8, batch     3 | loss: 4.1094794Losses:  3.0326263904571533 -0.0 0.909031093120575
MemoryTrain:  epoch  8, batch     4 | loss: 3.0326264Losses:  3.289487361907959 1.1000924110412598 0.8413896560668945
MemoryTrain:  epoch  8, batch     5 | loss: 3.2894874Losses:  3.1606812477111816 0.5385197401046753 0.9500097036361694
MemoryTrain:  epoch  8, batch     6 | loss: 3.1606812Losses:  1.914423942565918 -0.0 0.5879648327827454
MemoryTrain:  epoch  8, batch     7 | loss: 1.9144239Losses:  2.8577585220336914 0.2772390842437744 0.6546781063079834
MemoryTrain:  epoch  9, batch     0 | loss: 2.8577585Losses:  2.889150381088257 0.2200615406036377 0.9747596383094788
MemoryTrain:  epoch  9, batch     1 | loss: 2.8891504Losses:  3.1578142642974854 0.22972644865512848 0.916022777557373
MemoryTrain:  epoch  9, batch     2 | loss: 3.1578143Losses:  2.764747381210327 0.5286538004875183 0.8465076088905334
MemoryTrain:  epoch  9, batch     3 | loss: 2.7647474Losses:  3.90629243850708 1.1438546180725098 0.750732421875
MemoryTrain:  epoch  9, batch     4 | loss: 3.9062924Losses:  3.134697914123535 0.5996630191802979 0.7935371994972229
MemoryTrain:  epoch  9, batch     5 | loss: 3.1346979Losses:  2.7962942123413086 0.21981734037399292 0.8408478498458862
MemoryTrain:  epoch  9, batch     6 | loss: 2.7962942Losses:  2.6421687602996826 0.347247838973999 0.5172286033630371
MemoryTrain:  epoch  9, batch     7 | loss: 2.6421688
[EVAL] batch:    0 | acc: 31.25%,  total acc: 31.25%   [EVAL] batch:    1 | acc: 31.25%,  total acc: 31.25%   [EVAL] batch:    2 | acc: 31.25%,  total acc: 31.25%   [EVAL] batch:    3 | acc: 43.75%,  total acc: 34.38%   [EVAL] batch:    4 | acc: 31.25%,  total acc: 33.75%   [EVAL] batch:    5 | acc: 18.75%,  total acc: 31.25%   [EVAL] batch:    6 | acc: 50.00%,  total acc: 33.93%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 39.06%   [EVAL] batch:    8 | acc: 75.00%,  total acc: 43.06%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 46.88%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 50.57%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 52.60%   [EVAL] batch:   12 | acc: 50.00%,  total acc: 52.40%   [EVAL] batch:   13 | acc: 81.25%,  total acc: 54.46%   [EVAL] batch:   14 | acc: 62.50%,  total acc: 55.00%   [EVAL] batch:   15 | acc: 68.75%,  total acc: 55.86%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 56.99%   [EVAL] batch:   17 | acc: 75.00%,  total acc: 57.99%   [EVAL] batch:   18 | acc: 75.00%,  total acc: 58.88%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 60.94%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 62.80%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 64.20%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 65.76%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 67.19%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 68.50%   [EVAL] batch:   25 | acc: 93.75%,  total acc: 69.47%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 70.37%   [EVAL] batch:   27 | acc: 87.50%,  total acc: 70.98%   [EVAL] batch:   28 | acc: 87.50%,  total acc: 71.55%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 72.50%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 72.98%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 73.83%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 74.62%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 75.18%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 75.71%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 76.22%   [EVAL] batch:   36 | acc: 93.75%,  total acc: 76.69%   [EVAL] batch:   37 | acc: 62.50%,  total acc: 76.32%   [EVAL] batch:   38 | acc: 25.00%,  total acc: 75.00%   [EVAL] batch:   39 | acc: 31.25%,  total acc: 73.91%   [EVAL] batch:   40 | acc: 31.25%,  total acc: 72.87%   [EVAL] batch:   41 | acc: 18.75%,  total acc: 71.58%   [EVAL] batch:   42 | acc: 25.00%,  total acc: 70.49%   [EVAL] batch:   43 | acc: 31.25%,  total acc: 69.60%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 70.14%   [EVAL] batch:   45 | acc: 87.50%,  total acc: 70.52%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 71.01%   [EVAL] batch:   47 | acc: 81.25%,  total acc: 71.22%   [EVAL] batch:   48 | acc: 81.25%,  total acc: 71.43%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 71.88%   [EVAL] batch:   50 | acc: 31.25%,  total acc: 71.08%   [EVAL] batch:   51 | acc: 43.75%,  total acc: 70.55%   [EVAL] batch:   52 | acc: 50.00%,  total acc: 70.17%   [EVAL] batch:   53 | acc: 56.25%,  total acc: 69.91%   [EVAL] batch:   54 | acc: 37.50%,  total acc: 69.32%   [EVAL] batch:   55 | acc: 43.75%,  total acc: 68.86%   [EVAL] batch:   56 | acc: 43.75%,  total acc: 68.42%   [EVAL] batch:   57 | acc: 75.00%,  total acc: 68.53%   [EVAL] batch:   58 | acc: 81.25%,  total acc: 68.75%   [EVAL] batch:   59 | acc: 81.25%,  total acc: 68.96%   [EVAL] batch:   60 | acc: 68.75%,  total acc: 68.95%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 69.35%   [EVAL] batch:   62 | acc: 43.75%,  total acc: 68.95%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 82.81%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 83.75%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 85.42%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 83.04%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 83.59%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 83.33%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 83.75%   [EVAL] batch:   10 | acc: 75.00%,  total acc: 82.95%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 83.65%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 85.00%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 85.94%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 86.40%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 86.81%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 87.19%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 87.20%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 87.22%   [EVAL] batch:   22 | acc: 81.25%,  total acc: 86.96%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 86.98%   [EVAL] batch:   24 | acc: 81.25%,  total acc: 86.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 87.26%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 87.72%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 87.93%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 88.33%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 88.71%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 89.06%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 89.39%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 89.71%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 89.82%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 90.10%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 90.37%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 90.87%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 91.09%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 91.31%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 91.52%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 91.57%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 91.62%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 91.53%   [EVAL] batch:   45 | acc: 81.25%,  total acc: 91.30%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 91.09%   [EVAL] batch:   47 | acc: 81.25%,  total acc: 90.89%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 91.07%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 91.12%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 91.18%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 91.23%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 91.27%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 91.44%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 91.36%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 91.41%   [EVAL] batch:   56 | acc: 75.00%,  total acc: 91.12%   [EVAL] batch:   57 | acc: 43.75%,  total acc: 90.30%   [EVAL] batch:   58 | acc: 56.25%,  total acc: 89.72%   [EVAL] batch:   59 | acc: 81.25%,  total acc: 89.58%   [EVAL] batch:   60 | acc: 75.00%,  total acc: 89.34%   [EVAL] batch:   61 | acc: 50.00%,  total acc: 88.71%   [EVAL] batch:   62 | acc: 43.75%,  total acc: 88.00%   [EVAL] batch:   63 | acc: 25.00%,  total acc: 87.01%   [EVAL] batch:   64 | acc: 37.50%,  total acc: 86.25%   [EVAL] batch:   65 | acc: 31.25%,  total acc: 85.42%   [EVAL] batch:   66 | acc: 37.50%,  total acc: 84.70%   [EVAL] batch:   67 | acc: 25.00%,  total acc: 83.82%   [EVAL] batch:   68 | acc: 37.50%,  total acc: 83.15%   [EVAL] batch:   69 | acc: 87.50%,  total acc: 83.21%   [EVAL] batch:   70 | acc: 93.75%,  total acc: 83.36%   [EVAL] batch:   71 | acc: 68.75%,  total acc: 83.16%   [EVAL] batch:   72 | acc: 93.75%,  total acc: 83.30%   [EVAL] batch:   73 | acc: 93.75%,  total acc: 83.45%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 83.50%   [EVAL] batch:   75 | acc: 93.75%,  total acc: 83.63%   [EVAL] batch:   76 | acc: 93.75%,  total acc: 83.77%   [EVAL] batch:   77 | acc: 81.25%,  total acc: 83.73%   [EVAL] batch:   78 | acc: 87.50%,  total acc: 83.78%   [EVAL] batch:   79 | acc: 87.50%,  total acc: 83.83%   [EVAL] batch:   80 | acc: 93.75%,  total acc: 83.95%   [EVAL] batch:   81 | acc: 68.75%,  total acc: 83.77%   [EVAL] batch:   82 | acc: 43.75%,  total acc: 83.28%   [EVAL] batch:   83 | acc: 25.00%,  total acc: 82.59%   [EVAL] batch:   84 | acc: 43.75%,  total acc: 82.13%   [EVAL] batch:   85 | acc: 18.75%,  total acc: 81.40%   [EVAL] batch:   86 | acc: 56.25%,  total acc: 81.11%   [EVAL] batch:   87 | acc: 87.50%,  total acc: 81.18%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 81.39%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 81.60%   [EVAL] batch:   90 | acc: 93.75%,  total acc: 81.73%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 81.93%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 82.12%   [EVAL] batch:   93 | acc: 87.50%,  total acc: 82.18%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 82.37%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 82.55%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 82.73%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 82.91%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 83.08%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 83.25%   [EVAL] batch:  100 | acc: 56.25%,  total acc: 82.98%   [EVAL] batch:  101 | acc: 62.50%,  total acc: 82.78%   [EVAL] batch:  102 | acc: 56.25%,  total acc: 82.52%   [EVAL] batch:  103 | acc: 56.25%,  total acc: 82.27%   [EVAL] batch:  104 | acc: 81.25%,  total acc: 82.26%   [EVAL] batch:  105 | acc: 62.50%,  total acc: 82.08%   [EVAL] batch:  106 | acc: 50.00%,  total acc: 81.78%   [EVAL] batch:  107 | acc: 31.25%,  total acc: 81.31%   [EVAL] batch:  108 | acc: 37.50%,  total acc: 80.91%   [EVAL] batch:  109 | acc: 25.00%,  total acc: 80.40%   [EVAL] batch:  110 | acc: 56.25%,  total acc: 80.18%   [EVAL] batch:  111 | acc: 31.25%,  total acc: 79.74%   [EVAL] batch:  112 | acc: 68.75%,  total acc: 79.65%   [EVAL] batch:  113 | acc: 81.25%,  total acc: 79.66%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 79.84%   [EVAL] batch:  115 | acc: 87.50%,  total acc: 79.90%   [EVAL] batch:  116 | acc: 87.50%,  total acc: 79.97%   [EVAL] batch:  117 | acc: 87.50%,  total acc: 80.03%   [EVAL] batch:  118 | acc: 75.00%,  total acc: 79.99%   [EVAL] batch:  119 | acc: 18.75%,  total acc: 79.48%   [EVAL] batch:  120 | acc: 25.00%,  total acc: 79.03%   [EVAL] batch:  121 | acc: 6.25%,  total acc: 78.43%   [EVAL] batch:  122 | acc: 31.25%,  total acc: 78.05%   [EVAL] batch:  123 | acc: 12.50%,  total acc: 77.52%   [EVAL] batch:  124 | acc: 31.25%,  total acc: 77.15%   [EVAL] batch:  125 | acc: 50.00%,  total acc: 76.93%   [EVAL] batch:  126 | acc: 56.25%,  total acc: 76.77%   [EVAL] batch:  127 | acc: 62.50%,  total acc: 76.66%   [EVAL] batch:  128 | acc: 56.25%,  total acc: 76.50%   [EVAL] batch:  129 | acc: 62.50%,  total acc: 76.39%   [EVAL] batch:  130 | acc: 75.00%,  total acc: 76.38%   [EVAL] batch:  131 | acc: 93.75%,  total acc: 76.52%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 76.69%   [EVAL] batch:  133 | acc: 81.25%,  total acc: 76.73%   [EVAL] batch:  134 | acc: 93.75%,  total acc: 76.85%   [EVAL] batch:  135 | acc: 93.75%,  total acc: 76.98%   [EVAL] batch:  136 | acc: 93.75%,  total acc: 77.10%   [EVAL] batch:  137 | acc: 75.00%,  total acc: 77.08%   [EVAL] batch:  138 | acc: 18.75%,  total acc: 76.66%   [EVAL] batch:  139 | acc: 37.50%,  total acc: 76.38%   [EVAL] batch:  140 | acc: 12.50%,  total acc: 75.93%   [EVAL] batch:  141 | acc: 31.25%,  total acc: 75.62%   [EVAL] batch:  142 | acc: 43.75%,  total acc: 75.39%   [EVAL] batch:  143 | acc: 31.25%,  total acc: 75.09%   [EVAL] batch:  144 | acc: 93.75%,  total acc: 75.22%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 75.39%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 75.51%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 75.68%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 75.84%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 75.96%   [EVAL] batch:  150 | acc: 62.50%,  total acc: 75.87%   [EVAL] batch:  151 | acc: 37.50%,  total acc: 75.62%   [EVAL] batch:  152 | acc: 56.25%,  total acc: 75.49%   [EVAL] batch:  153 | acc: 43.75%,  total acc: 75.28%   [EVAL] batch:  154 | acc: 37.50%,  total acc: 75.04%   [EVAL] batch:  155 | acc: 31.25%,  total acc: 74.76%   [EVAL] batch:  156 | acc: 87.50%,  total acc: 74.84%   [EVAL] batch:  157 | acc: 87.50%,  total acc: 74.92%   [EVAL] batch:  158 | acc: 93.75%,  total acc: 75.04%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 75.16%   [EVAL] batch:  160 | acc: 93.75%,  total acc: 75.27%   [EVAL] batch:  161 | acc: 100.00%,  total acc: 75.42%   [EVAL] batch:  162 | acc: 100.00%,  total acc: 75.58%   [EVAL] batch:  163 | acc: 93.75%,  total acc: 75.69%   [EVAL] batch:  164 | acc: 81.25%,  total acc: 75.72%   [EVAL] batch:  165 | acc: 81.25%,  total acc: 75.75%   [EVAL] batch:  166 | acc: 93.75%,  total acc: 75.86%   [EVAL] batch:  167 | acc: 93.75%,  total acc: 75.97%   [EVAL] batch:  168 | acc: 87.50%,  total acc: 76.04%   [EVAL] batch:  169 | acc: 31.25%,  total acc: 75.77%   [EVAL] batch:  170 | acc: 62.50%,  total acc: 75.69%   [EVAL] batch:  171 | acc: 31.25%,  total acc: 75.44%   [EVAL] batch:  172 | acc: 25.00%,  total acc: 75.14%   [EVAL] batch:  173 | acc: 50.00%,  total acc: 75.00%   [EVAL] batch:  174 | acc: 62.50%,  total acc: 74.93%   [EVAL] batch:  175 | acc: 37.50%,  total acc: 74.72%   [EVAL] batch:  176 | acc: 50.00%,  total acc: 74.58%   [EVAL] batch:  177 | acc: 62.50%,  total acc: 74.51%   [EVAL] batch:  178 | acc: 62.50%,  total acc: 74.44%   [EVAL] batch:  179 | acc: 68.75%,  total acc: 74.41%   [EVAL] batch:  180 | acc: 75.00%,  total acc: 74.41%   [EVAL] batch:  181 | acc: 87.50%,  total acc: 74.48%   [EVAL] batch:  182 | acc: 87.50%,  total acc: 74.56%   [EVAL] batch:  183 | acc: 93.75%,  total acc: 74.66%   [EVAL] batch:  184 | acc: 75.00%,  total acc: 74.66%   [EVAL] batch:  185 | acc: 87.50%,  total acc: 74.73%   [EVAL] batch:  186 | acc: 68.75%,  total acc: 74.70%   [EVAL] batch:  187 | acc: 50.00%,  total acc: 74.57%   [EVAL] batch:  188 | acc: 37.50%,  total acc: 74.37%   [EVAL] batch:  189 | acc: 31.25%,  total acc: 74.14%   [EVAL] batch:  190 | acc: 37.50%,  total acc: 73.95%   [EVAL] batch:  191 | acc: 31.25%,  total acc: 73.73%   [EVAL] batch:  192 | acc: 31.25%,  total acc: 73.51%   [EVAL] batch:  193 | acc: 37.50%,  total acc: 73.32%   [EVAL] batch:  194 | acc: 75.00%,  total acc: 73.33%   [EVAL] batch:  195 | acc: 62.50%,  total acc: 73.28%   [EVAL] batch:  196 | acc: 81.25%,  total acc: 73.32%   [EVAL] batch:  197 | acc: 87.50%,  total acc: 73.39%   [EVAL] batch:  198 | acc: 75.00%,  total acc: 73.40%   [EVAL] batch:  199 | acc: 68.75%,  total acc: 73.38%   [EVAL] batch:  200 | acc: 62.50%,  total acc: 73.32%   [EVAL] batch:  201 | acc: 75.00%,  total acc: 73.33%   [EVAL] batch:  202 | acc: 50.00%,  total acc: 73.21%   [EVAL] batch:  203 | acc: 87.50%,  total acc: 73.28%   [EVAL] batch:  204 | acc: 62.50%,  total acc: 73.23%   [EVAL] batch:  205 | acc: 81.25%,  total acc: 73.27%   [EVAL] batch:  206 | acc: 87.50%,  total acc: 73.34%   [EVAL] batch:  207 | acc: 100.00%,  total acc: 73.47%   [EVAL] batch:  208 | acc: 100.00%,  total acc: 73.59%   [EVAL] batch:  209 | acc: 93.75%,  total acc: 73.69%   [EVAL] batch:  210 | acc: 100.00%,  total acc: 73.82%   [EVAL] batch:  211 | acc: 100.00%,  total acc: 73.94%   [EVAL] batch:  212 | acc: 100.00%,  total acc: 74.06%   [EVAL] batch:  213 | acc: 87.50%,  total acc: 74.12%   [EVAL] batch:  214 | acc: 87.50%,  total acc: 74.19%   [EVAL] batch:  215 | acc: 93.75%,  total acc: 74.28%   [EVAL] batch:  216 | acc: 93.75%,  total acc: 74.37%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 74.48%   [EVAL] batch:  218 | acc: 87.50%,  total acc: 74.54%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 74.66%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 74.77%   [EVAL] batch:  221 | acc: 87.50%,  total acc: 74.83%   [EVAL] batch:  222 | acc: 93.75%,  total acc: 74.92%   [EVAL] batch:  223 | acc: 93.75%,  total acc: 75.00%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 75.11%   [EVAL] batch:  225 | acc: 25.00%,  total acc: 74.89%   [EVAL] batch:  226 | acc: 31.25%,  total acc: 74.70%   [EVAL] batch:  227 | acc: 25.00%,  total acc: 74.48%   [EVAL] batch:  228 | acc: 31.25%,  total acc: 74.29%   [EVAL] batch:  229 | acc: 18.75%,  total acc: 74.05%   [EVAL] batch:  230 | acc: 18.75%,  total acc: 73.81%   [EVAL] batch:  231 | acc: 68.75%,  total acc: 73.79%   [EVAL] batch:  232 | acc: 93.75%,  total acc: 73.87%   [EVAL] batch:  233 | acc: 87.50%,  total acc: 73.93%   [EVAL] batch:  234 | acc: 93.75%,  total acc: 74.02%   [EVAL] batch:  235 | acc: 75.00%,  total acc: 74.02%   [EVAL] batch:  236 | acc: 87.50%,  total acc: 74.08%   [EVAL] batch:  237 | acc: 62.50%,  total acc: 74.03%   [EVAL] batch:  238 | acc: 37.50%,  total acc: 73.88%   [EVAL] batch:  239 | acc: 43.75%,  total acc: 73.75%   [EVAL] batch:  240 | acc: 50.00%,  total acc: 73.65%   [EVAL] batch:  241 | acc: 62.50%,  total acc: 73.61%   [EVAL] batch:  242 | acc: 37.50%,  total acc: 73.46%   [EVAL] batch:  243 | acc: 37.50%,  total acc: 73.31%   [EVAL] batch:  244 | acc: 75.00%,  total acc: 73.32%   [EVAL] batch:  245 | acc: 75.00%,  total acc: 73.32%   [EVAL] batch:  246 | acc: 68.75%,  total acc: 73.30%   [EVAL] batch:  247 | acc: 81.25%,  total acc: 73.34%   [EVAL] batch:  248 | acc: 81.25%,  total acc: 73.37%   [EVAL] batch:  249 | acc: 87.50%,  total acc: 73.42%   
cur_acc:  ['0.9524', '0.6994', '0.7639', '0.6895']
his_acc:  ['0.9524', '0.8220', '0.7949', '0.7342']
Clustering into  24  clusters
Clusters:  [21  1 14  2  9 23 14 18  1 12  1  8 18  6 19 22 20 11  2 11  0  6  5 17
  5 16 10  4  3  0 15  7 11  4  4  7 11  8 12  8  9  6 10  3  5 13 10  1
 10  1]
Losses:  9.437705039978027 2.769221782684326 0.692159116268158
CurrentTrain: epoch  0, batch     0 | loss: 9.4377050Losses:  11.09108829498291 3.2260184288024902 0.6674779057502747
CurrentTrain: epoch  0, batch     1 | loss: 11.0910883Losses:  10.787872314453125 3.469460964202881 0.5977202653884888
CurrentTrain: epoch  0, batch     2 | loss: 10.7878723Losses:  7.502158164978027 -0.0 0.1234980970621109
CurrentTrain: epoch  0, batch     3 | loss: 7.5021582Losses:  10.319662094116211 3.894392490386963 0.6567844152450562
CurrentTrain: epoch  1, batch     0 | loss: 10.3196621Losses:  10.384868621826172 4.0127854347229 0.44722890853881836
CurrentTrain: epoch  1, batch     1 | loss: 10.3848686Losses:  8.597107887268066 2.5155248641967773 0.6926878690719604
CurrentTrain: epoch  1, batch     2 | loss: 8.5971079Losses:  6.749915599822998 -0.0 0.12645569443702698
CurrentTrain: epoch  1, batch     3 | loss: 6.7499156Losses:  8.426679611206055 3.541059970855713 0.5242677927017212
CurrentTrain: epoch  2, batch     0 | loss: 8.4266796Losses:  10.849284172058105 4.555208206176758 0.5545243620872498
CurrentTrain: epoch  2, batch     1 | loss: 10.8492842Losses:  8.324033737182617 3.2585983276367188 0.6260461211204529
CurrentTrain: epoch  2, batch     2 | loss: 8.3240337Losses:  6.531545162200928 -0.0 0.1281888782978058
CurrentTrain: epoch  2, batch     3 | loss: 6.5315452Losses:  10.352373123168945 5.365849018096924 0.6211227178573608
CurrentTrain: epoch  3, batch     0 | loss: 10.3523731Losses:  9.023828506469727 3.806877613067627 0.455858051776886
CurrentTrain: epoch  3, batch     1 | loss: 9.0238285Losses:  8.19408130645752 3.878450393676758 0.5107954740524292
CurrentTrain: epoch  3, batch     2 | loss: 8.1940813Losses:  5.4785614013671875 -0.0 0.19252637028694153
CurrentTrain: epoch  3, batch     3 | loss: 5.4785614Losses:  6.944612979888916 2.2572944164276123 0.6566786170005798
CurrentTrain: epoch  4, batch     0 | loss: 6.9446130Losses:  7.490744113922119 3.4519906044006348 0.5163396596908569
CurrentTrain: epoch  4, batch     1 | loss: 7.4907441Losses:  8.03660774230957 3.3515615463256836 0.6019079685211182
CurrentTrain: epoch  4, batch     2 | loss: 8.0366077Losses:  7.661516189575195 -0.0 0.1486387401819229
CurrentTrain: epoch  4, batch     3 | loss: 7.6615162Losses:  6.99524450302124 2.652116298675537 0.5207370519638062
CurrentTrain: epoch  5, batch     0 | loss: 6.9952445Losses:  7.570921897888184 3.266294479370117 0.615130603313446
CurrentTrain: epoch  5, batch     1 | loss: 7.5709219Losses:  7.71879768371582 3.1949563026428223 0.5169575214385986
CurrentTrain: epoch  5, batch     2 | loss: 7.7187977Losses:  2.17403244972229 -0.0 0.08621808141469955
CurrentTrain: epoch  5, batch     3 | loss: 2.1740324Losses:  7.040952205657959 2.951869010925293 0.6255021095275879
CurrentTrain: epoch  6, batch     0 | loss: 7.0409522Losses:  7.876340866088867 3.3741111755371094 0.5073411464691162
CurrentTrain: epoch  6, batch     1 | loss: 7.8763409Losses:  6.071105003356934 2.1551547050476074 0.5888081789016724
CurrentTrain: epoch  6, batch     2 | loss: 6.0711050Losses:  3.900009870529175 -0.0 0.1770603358745575
CurrentTrain: epoch  6, batch     3 | loss: 3.9000099Losses:  6.510307312011719 2.2669639587402344 0.625263512134552
CurrentTrain: epoch  7, batch     0 | loss: 6.5103073Losses:  6.866493225097656 3.2655696868896484 0.5030051469802856
CurrentTrain: epoch  7, batch     1 | loss: 6.8664932Losses:  7.043942928314209 3.209048271179199 0.6089205145835876
CurrentTrain: epoch  7, batch     2 | loss: 7.0439429Losses:  2.761507511138916 -0.0 0.10372044146060944
CurrentTrain: epoch  7, batch     3 | loss: 2.7615075Losses:  8.343733787536621 4.581958770751953 0.53952956199646
CurrentTrain: epoch  8, batch     0 | loss: 8.3437338Losses:  7.113435745239258 2.880349636077881 0.5957816243171692
CurrentTrain: epoch  8, batch     1 | loss: 7.1134357Losses:  6.782649040222168 3.523380756378174 0.535966157913208
CurrentTrain: epoch  8, batch     2 | loss: 6.7826490Losses:  1.9890333414077759 -0.0 0.11042081564664841
CurrentTrain: epoch  8, batch     3 | loss: 1.9890333Losses:  7.052218437194824 3.046994209289551 0.5972141623497009
CurrentTrain: epoch  9, batch     0 | loss: 7.0522184Losses:  5.852417469024658 1.9486085176467896 0.6175064444541931
CurrentTrain: epoch  9, batch     1 | loss: 5.8524175Losses:  6.131120204925537 3.407151699066162 0.3909781277179718
CurrentTrain: epoch  9, batch     2 | loss: 6.1311202Losses:  2.054518222808838 -0.0 0.1462906002998352
CurrentTrain: epoch  9, batch     3 | loss: 2.0545182
#### located on terrain feature ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "located on terrain feature" means located on the specified physical/geophysical feature.
Here is an example:
Relation: located on terrain feature
Context: it is situated in the historic lower lusatia region , about south of beeskow , and north of cottbus .
Head Entity: cottbus
Tail Entity: lower lusatia
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "located on terrain feature":
Relation: located on terrain feature  
Context: The ancient ruins are found on the slopes of the majestic Andes mountains, providing a breathtaking view of the valley below.  
Head Entity: Andes mountains  
Tail Entity: ancient ruins  

Relation: located on terrain feature  
Context: The village is nestled in the heart of the lush Black Forest, surrounded by towering trees and serene lakes.  
Head Entity: Black Forest  
Tail Entity: village  
#### participant of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "participant of" means event in which a person or organization was/is a participant.
Here is an example:
Relation: participant of
Context: tiago venâncio ( born august 19 , 1987 , setúbal ) is a freestyle swimmer from portugal , who represented his home country at the 2004 summer olympics in athens , greece .
Head Entity: tiago venâncio
Tail Entity: 2004 summer olympics
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "participant of":
Relation: participant of  
Context: the united nations climate change conference, held in paris in 2015, saw numerous world leaders and activists come together to discuss global warming.  
Head Entity: united nations climate change conference  
Tail Entity: paris agreement  

Relation: participant of  
Context: the 2022 fifa world cup in qatar featured teams from around the globe competing for the prestigious trophy.  
Head Entity: 2022 fifa world cup  
Tail Entity: qatar
#### director ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "director" means director of film, TV-series, stageplay, video game or similar.
Here is an example:
Relation: director
Context: luna subsequently had a supporting role in the comedy film " gravy " , directed by james roday , which was released in october 2015 .
Head Entity: gravy
Tail Entity: james roday
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "director":
Relation: director  
Context: the acclaimed drama series "the crown" was brought to life by the visionary director peter morgan, who skillfully navigated the complexities of the royal family.  
Head Entity: the crown  
Tail Entity: peter morgan  

Relation: director  
Context: in the thrilling action movie "inception," the brilliant director christopher nolan crafted a mind-bending narrative that captivated audiences worldwide.  
Head Entity: inception  
Tail Entity: christopher nolan  
#### owned by ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "owned by" means owner of the subject.
Here is an example:
Relation: owned by
Context: as of april 2013 , sony isp , known as so - net , released a new fibre service to tokyo for small businesses and homes and also made it available to six surrounding prefectures .
Head Entity: so - net
Tail Entity: sony
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "owned by":
Relation: owned by  
Context: In 2020, the popular video game company Activision Blizzard announced that it had acquired the rights to several franchises, including the beloved "Crash Bandicoot" series, which was originally developed by Naughty Dog.  
Head Entity: Activision Blizzard  
Tail Entity: Naughty Dog  

Relation: owned by  
Context: The luxury fashion brand Gucci, known for its high-end products and iconic designs, is a subsidiary of the Kering Group, which oversees several other prestigious labels in the fashion industry.  
Head Entity: Gucci  
Tail Entity: Kering Group  
#### part of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "part of" means object of which the subject is a part.
Here is an example:
Relation: part of
Context: thessaly was home to extensive neolithic and chalcolithic cultures around 6000 bc–2500 bc ( see cardium pottery , dimini and sesklo ) .
Head Entity: cardium pottery
Tail Entity: neolithic
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "part of":
Relation: part of  
Context: The engine is a crucial part of the car's overall functionality, providing the necessary power to drive.  
Head Entity: engine  
Tail Entity: car  

Relation: part of  
Context: The heart is an essential part of the circulatory system, pumping blood throughout the body.  
Head Entity: heart  
Tail Entity: circulatory system  
#### occupant ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "occupant" means person or organization occupying property.
Here is an example:
Relation: occupant
Context: the brewery closed in 1996 and is now the site of the halliwell jones stadium , home of warrington wolves , as well as a tesco store .
Head Entity: halliwell jones stadium
Tail Entity: warrington wolves
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "occupant":
Relation: occupant  
Context: The historic building was transformed into a cultural center, which now hosts various events and is home to the local art gallery.  
Head Entity: cultural center  
Tail Entity: local art gallery  

Relation: occupant  
Context: After the renovation, the old factory became a vibrant community hub, serving as the headquarters for several non-profit organizations.  
Head Entity: community hub  
Tail Entity: non-profit organizations  
#### architect ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "architect" means person or architectural firm responsible for designing this building.
Here is an example:
Relation: architect
Context: the art nouveau style was brought to italy by figures such as giuseppe sommaruga and ernesto basile ( who respectively designed the palazzo castiglioni and expanded the palazzo montecitorio in rome ) .
Head Entity: palazzo castiglioni
Tail Entity: giuseppe sommaruga
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "architect":
Relation: architect  
Context: The iconic Sydney Opera House was designed by Danish architect Jørn Utzon, who won the design competition in 1957.  
Head Entity: Sydney Opera House  
Tail Entity: Jørn Utzon  

Relation: architect  
Context: Frank Lloyd Wright is renowned for his innovative designs, including the famous Fallingwater house, which seamlessly integrates with its natural surroundings.  
Head Entity: Fallingwater  
Tail Entity: Frank Lloyd Wright  
#### residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "residence" means the place where the person is or has been, resident.
Here is an example:
Relation: residence
Context: in 2001 brian haw set up the parliament square peace campaign outside the houses of parliament in london .
Head Entity: brian haw
Tail Entity: london
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "residence":
Relation: residence  
Context: After moving to New York City in 2010, Sarah began her career as a graphic designer.  
Head Entity: Sarah  
Tail Entity: New York City  

Relation: residence  
Context: The famous author spent many years living in a quaint cottage in the countryside of England.  
Head Entity: The famous author  
Tail Entity: England  
#### composer ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "composer" means person who wrote the music.
Here is an example:
Relation: composer
Context: at a young age he was an accomplished cellist , performing at age seventeen the solo part of richard strauss 's " don quixote " .
Head Entity: don quixote
Tail Entity: richard strauss
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "composer":
Relation: composer  
Context: The renowned composer Ludwig van Beethoven created some of the most celebrated symphonies in classical music history.  
Head Entity: symphonies  
Tail Entity: Ludwig van Beethoven  

Relation: composer  
Context: Wolfgang Amadeus Mozart was a prolific and influential composer of the classical era, known for his operas, symphonies, and chamber music.  
Head Entity: operas  
Tail Entity: Wolfgang Amadeus Mozart  
#### location ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "location" means location of the object, structure or event.
Here is an example:
Relation: location
Context: in 1972 , terrorists from the japanese red army launched an attack that led to the deaths of at least 24 people at ben gurion airport .
Head Entity: attack
Tail Entity: ben gurion airport
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "location":
Relation: location  
Context: The annual music festival took place in the picturesque town of Coachella, attracting thousands of fans from around the world.  
Head Entity: music festival  
Tail Entity: Coachella  

Relation: location  
Context: The historic battle was fought in the fields of Gettysburg, which later became a national park.  
Head Entity: battle  
Tail Entity: Gettysburg  
Losses:  4.735356330871582 0.5620419979095459 0.9060721397399902
MemoryTrain:  epoch  0, batch     0 | loss: 4.7353563Losses:  3.9519052505493164 0.8044358491897583 0.9161109924316406
MemoryTrain:  epoch  0, batch     1 | loss: 3.9519053Losses:  5.434115409851074 1.0320699214935303 0.7124253511428833
MemoryTrain:  epoch  0, batch     2 | loss: 5.4341154Losses:  4.635009765625 -0.0 0.9876904487609863
MemoryTrain:  epoch  0, batch     3 | loss: 4.6350098Losses:  4.7071146965026855 0.23648186028003693 0.9502033591270447
MemoryTrain:  epoch  0, batch     4 | loss: 4.7071147Losses:  6.336413383483887 0.4110063314437866 1.0311954021453857
MemoryTrain:  epoch  0, batch     5 | loss: 6.3364134Losses:  4.691661357879639 0.3170871436595917 0.7967543005943298
MemoryTrain:  epoch  0, batch     6 | loss: 4.6916614Losses:  5.291447639465332 0.46610087156295776 0.9223377704620361
MemoryTrain:  epoch  0, batch     7 | loss: 5.2914476Losses:  5.495820999145508 -0.0 0.8528069853782654
MemoryTrain:  epoch  0, batch     8 | loss: 5.4958210Losses:  3.4138097763061523 -0.0 0.5266441106796265
MemoryTrain:  epoch  0, batch     9 | loss: 3.4138098Losses:  5.687871932983398 0.33628833293914795 0.916201114654541
MemoryTrain:  epoch  1, batch     0 | loss: 5.6878719Losses:  4.839673042297363 0.50838303565979 0.8878971934318542
MemoryTrain:  epoch  1, batch     1 | loss: 4.8396730Losses:  5.654829502105713 0.27208149433135986 0.8839019536972046
MemoryTrain:  epoch  1, batch     2 | loss: 5.6548295Losses:  4.205082416534424 0.2726184129714966 0.9729700088500977
MemoryTrain:  epoch  1, batch     3 | loss: 4.2050824Losses:  4.708266735076904 0.2788560390472412 1.051486849784851
MemoryTrain:  epoch  1, batch     4 | loss: 4.7082667Losses:  4.508219242095947 0.25668853521347046 0.9101373553276062
MemoryTrain:  epoch  1, batch     5 | loss: 4.5082192Losses:  3.6031599044799805 0.25113433599472046 0.8669143319129944
MemoryTrain:  epoch  1, batch     6 | loss: 3.6031599Losses:  3.5272538661956787 -0.0 0.9079887270927429
MemoryTrain:  epoch  1, batch     7 | loss: 3.5272539Losses:  2.6872289180755615 0.21763482689857483 0.9115585088729858
MemoryTrain:  epoch  1, batch     8 | loss: 2.6872289Losses:  3.7349436283111572 -0.0 0.4944024085998535
MemoryTrain:  epoch  1, batch     9 | loss: 3.7349436Losses:  4.8442888259887695 0.6386146545410156 0.9264935255050659
MemoryTrain:  epoch  2, batch     0 | loss: 4.8442888Losses:  4.003022193908691 0.8066582679748535 0.8793540000915527
MemoryTrain:  epoch  2, batch     1 | loss: 4.0030222Losses:  4.000586986541748 0.2837640047073364 0.9093190431594849
MemoryTrain:  epoch  2, batch     2 | loss: 4.0005870Losses:  2.8811213970184326 -0.0 0.974616289138794
MemoryTrain:  epoch  2, batch     3 | loss: 2.8811214Losses:  3.376469612121582 -0.0 0.8763881325721741
MemoryTrain:  epoch  2, batch     4 | loss: 3.3764696Losses:  4.440369129180908 0.24131377041339874 0.9866333603858948
MemoryTrain:  epoch  2, batch     5 | loss: 4.4403691Losses:  3.531313419342041 0.31895750761032104 0.9357353448867798
MemoryTrain:  epoch  2, batch     6 | loss: 3.5313134Losses:  4.410829067230225 0.3693353831768036 1.0047575235366821
MemoryTrain:  epoch  2, batch     7 | loss: 4.4108291Losses:  4.558144569396973 0.3411061763763428 0.9181593656539917
MemoryTrain:  epoch  2, batch     8 | loss: 4.5581446Losses:  2.878295421600342 -0.0 0.5710371732711792
MemoryTrain:  epoch  2, batch     9 | loss: 2.8782954Losses:  6.6714091300964355 1.4057320356369019 0.7274971008300781
MemoryTrain:  epoch  3, batch     0 | loss: 6.6714091Losses:  2.8685152530670166 -0.0 0.9707425832748413
MemoryTrain:  epoch  3, batch     1 | loss: 2.8685153Losses:  2.3034987449645996 -0.0 0.7782019972801208
MemoryTrain:  epoch  3, batch     2 | loss: 2.3034987Losses:  4.225864410400391 -0.0 1.1645900011062622
MemoryTrain:  epoch  3, batch     3 | loss: 4.2258644Losses:  3.8513216972351074 0.2509532570838928 0.8872753381729126
MemoryTrain:  epoch  3, batch     4 | loss: 3.8513217Losses:  3.325864791870117 -0.0 1.0242599248886108
MemoryTrain:  epoch  3, batch     5 | loss: 3.3258648Losses:  3.8616371154785156 0.554189145565033 0.8185182809829712
MemoryTrain:  epoch  3, batch     6 | loss: 3.8616371Losses:  2.977832317352295 0.255704402923584 0.9763950109481812
MemoryTrain:  epoch  3, batch     7 | loss: 2.9778323Losses:  3.5477969646453857 -0.0 0.9482828974723816
MemoryTrain:  epoch  3, batch     8 | loss: 3.5477970Losses:  1.941267967224121 -0.0 0.44949275255203247
MemoryTrain:  epoch  3, batch     9 | loss: 1.9412680Losses:  3.202967643737793 -0.0 1.0419923067092896
MemoryTrain:  epoch  4, batch     0 | loss: 3.2029676Losses:  4.018659591674805 0.5999467372894287 0.9056634306907654
MemoryTrain:  epoch  4, batch     1 | loss: 4.0186596Losses:  4.7529754638671875 0.8387780785560608 0.9185104370117188
MemoryTrain:  epoch  4, batch     2 | loss: 4.7529755Losses:  3.688098907470703 0.48766499757766724 0.9198569059371948
MemoryTrain:  epoch  4, batch     3 | loss: 3.6880989Losses:  3.3510022163391113 0.8254092931747437 0.8523268699645996
MemoryTrain:  epoch  4, batch     4 | loss: 3.3510022Losses:  3.409294843673706 0.24638038873672485 0.855732262134552
MemoryTrain:  epoch  4, batch     5 | loss: 3.4092948Losses:  3.140632152557373 0.2908416986465454 0.9428896307945251
MemoryTrain:  epoch  4, batch     6 | loss: 3.1406322Losses:  4.214485168457031 0.27816903591156006 0.9748895764350891
MemoryTrain:  epoch  4, batch     7 | loss: 4.2144852Losses:  4.793374061584473 0.8349212408065796 0.9287989139556885
MemoryTrain:  epoch  4, batch     8 | loss: 4.7933741Losses:  3.4354288578033447 -0.0 0.4605568051338196
MemoryTrain:  epoch  4, batch     9 | loss: 3.4354289Losses:  3.7484278678894043 0.32661962509155273 0.8708487749099731
MemoryTrain:  epoch  5, batch     0 | loss: 3.7484279Losses:  3.976625680923462 0.5945396423339844 0.8884047865867615
MemoryTrain:  epoch  5, batch     1 | loss: 3.9766257Losses:  3.4927992820739746 0.27818381786346436 0.809573769569397
MemoryTrain:  epoch  5, batch     2 | loss: 3.4927993Losses:  3.8881194591522217 0.5293865203857422 0.977802038192749
MemoryTrain:  epoch  5, batch     3 | loss: 3.8881195Losses:  4.075837135314941 0.9236671328544617 0.8061419725418091
MemoryTrain:  epoch  5, batch     4 | loss: 4.0758371Losses:  3.9080400466918945 0.2832806408405304 0.877189040184021
MemoryTrain:  epoch  5, batch     5 | loss: 3.9080400Losses:  3.6062839031219482 0.5278888940811157 0.819935142993927
MemoryTrain:  epoch  5, batch     6 | loss: 3.6062839Losses:  2.822202682495117 0.28108271956443787 0.9731453657150269
MemoryTrain:  epoch  5, batch     7 | loss: 2.8222027Losses:  2.8545761108398438 0.22224180400371552 1.0731981992721558
MemoryTrain:  epoch  5, batch     8 | loss: 2.8545761Losses:  1.8280479907989502 -0.0 0.42153316736221313
MemoryTrain:  epoch  5, batch     9 | loss: 1.8280480Losses:  2.8596277236938477 0.23138375580310822 0.7757521867752075
MemoryTrain:  epoch  6, batch     0 | loss: 2.8596277Losses:  3.5866148471832275 0.5539460182189941 0.876063346862793
MemoryTrain:  epoch  6, batch     1 | loss: 3.5866148Losses:  3.4836902618408203 0.5771703124046326 0.9054850339889526
MemoryTrain:  epoch  6, batch     2 | loss: 3.4836903Losses:  3.881169080734253 0.7613142728805542 0.8020513653755188
MemoryTrain:  epoch  6, batch     3 | loss: 3.8811691Losses:  3.419674873352051 0.5751660466194153 0.9921796321868896
MemoryTrain:  epoch  6, batch     4 | loss: 3.4196749Losses:  3.0040030479431152 0.24649521708488464 0.9167717695236206
MemoryTrain:  epoch  6, batch     5 | loss: 3.0040030Losses:  3.412370204925537 0.850411057472229 0.8619834780693054
MemoryTrain:  epoch  6, batch     6 | loss: 3.4123702Losses:  3.5287156105041504 0.5227078795433044 0.8922070860862732
MemoryTrain:  epoch  6, batch     7 | loss: 3.5287156Losses:  4.006570816040039 0.8454696536064148 0.9007241129875183
MemoryTrain:  epoch  6, batch     8 | loss: 4.0065708Losses:  2.328784942626953 -0.0 0.47390857338905334
MemoryTrain:  epoch  6, batch     9 | loss: 2.3287849Losses:  2.8787598609924316 -0.0 0.928162693977356
MemoryTrain:  epoch  7, batch     0 | loss: 2.8787599Losses:  2.374718189239502 -0.0 0.920689582824707
MemoryTrain:  epoch  7, batch     1 | loss: 2.3747182Losses:  2.8211617469787598 -0.0 0.9240220785140991
MemoryTrain:  epoch  7, batch     2 | loss: 2.8211617Losses:  3.060670852661133 0.7944364547729492 0.8956913352012634
MemoryTrain:  epoch  7, batch     3 | loss: 3.0606709Losses:  3.7931599617004395 0.9974714517593384 0.8223481178283691
MemoryTrain:  epoch  7, batch     4 | loss: 3.7931600Losses:  3.5701143741607666 0.48996058106422424 0.9348482489585876
MemoryTrain:  epoch  7, batch     5 | loss: 3.5701144Losses:  2.626387119293213 -0.0 0.9149008989334106
MemoryTrain:  epoch  7, batch     6 | loss: 2.6263871Losses:  3.6094818115234375 0.765701413154602 1.0001137256622314
MemoryTrain:  epoch  7, batch     7 | loss: 3.6094818Losses:  4.076211929321289 0.5383579730987549 0.8709191083908081
MemoryTrain:  epoch  7, batch     8 | loss: 4.0762119Losses:  1.850976586341858 -0.0 0.41519519686698914
MemoryTrain:  epoch  7, batch     9 | loss: 1.8509766Losses:  3.6508078575134277 0.4541279375553131 0.8094474673271179
MemoryTrain:  epoch  8, batch     0 | loss: 3.6508079Losses:  3.0030031204223633 0.5597202777862549 0.9344879388809204
MemoryTrain:  epoch  8, batch     1 | loss: 3.0030031Losses:  2.614842414855957 -0.0 0.9634827971458435
MemoryTrain:  epoch  8, batch     2 | loss: 2.6148424Losses:  3.30526065826416 0.48108911514282227 0.8917803764343262
MemoryTrain:  epoch  8, batch     3 | loss: 3.3052607Losses:  2.569101095199585 0.2613181471824646 0.8878114223480225
MemoryTrain:  epoch  8, batch     4 | loss: 2.5691011Losses:  3.8585166931152344 0.7085932493209839 0.8828787803649902
MemoryTrain:  epoch  8, batch     5 | loss: 3.8585167Losses:  2.7838945388793945 0.5456912517547607 0.8383130431175232
MemoryTrain:  epoch  8, batch     6 | loss: 2.7838945Losses:  2.951359272003174 0.2556449770927429 0.8714070916175842
MemoryTrain:  epoch  8, batch     7 | loss: 2.9513593Losses:  2.5105834007263184 -0.0 0.8755868673324585
MemoryTrain:  epoch  8, batch     8 | loss: 2.5105834Losses:  2.070706844329834 -0.0 0.5089282989501953
MemoryTrain:  epoch  8, batch     9 | loss: 2.0707068Losses:  3.2858150005340576 0.35503512620925903 1.0214734077453613
MemoryTrain:  epoch  9, batch     0 | loss: 3.2858150Losses:  2.65463924407959 0.2860354483127594 0.8343048095703125
MemoryTrain:  epoch  9, batch     1 | loss: 2.6546392Losses:  3.0963387489318848 0.8020904064178467 0.9637736678123474
MemoryTrain:  epoch  9, batch     2 | loss: 3.0963387Losses:  3.301142930984497 0.5103864073753357 0.9024100303649902
MemoryTrain:  epoch  9, batch     3 | loss: 3.3011429Losses:  3.4412405490875244 0.4815387725830078 0.9115681648254395
MemoryTrain:  epoch  9, batch     4 | loss: 3.4412405Losses:  3.8005402088165283 1.1031497716903687 0.8267796635627747
MemoryTrain:  epoch  9, batch     5 | loss: 3.8005402Losses:  3.100325107574463 0.7608447670936584 0.9449328184127808
MemoryTrain:  epoch  9, batch     6 | loss: 3.1003251Losses:  2.7564473152160645 0.25767529010772705 0.9635517001152039
MemoryTrain:  epoch  9, batch     7 | loss: 2.7564473Losses:  3.927790403366089 1.2665328979492188 0.9294421672821045
MemoryTrain:  epoch  9, batch     8 | loss: 3.9277904Losses:  2.20973539352417 0.3725973963737488 0.3317829370498657
MemoryTrain:  epoch  9, batch     9 | loss: 2.2097354
[EVAL] batch:    0 | acc: 6.25%,  total acc: 6.25%   [EVAL] batch:    1 | acc: 31.25%,  total acc: 18.75%   [EVAL] batch:    2 | acc: 12.50%,  total acc: 16.67%   [EVAL] batch:    3 | acc: 25.00%,  total acc: 18.75%   [EVAL] batch:    4 | acc: 6.25%,  total acc: 16.25%   [EVAL] batch:    5 | acc: 12.50%,  total acc: 15.62%   [EVAL] batch:    6 | acc: 62.50%,  total acc: 22.32%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 30.47%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 38.19%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 43.75%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 47.73%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 50.52%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 52.40%   [EVAL] batch:   13 | acc: 50.00%,  total acc: 52.23%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 53.75%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 53.91%   [EVAL] batch:   16 | acc: 43.75%,  total acc: 53.31%   [EVAL] batch:   17 | acc: 81.25%,  total acc: 54.86%   [EVAL] batch:   18 | acc: 37.50%,  total acc: 53.95%   [EVAL] batch:   19 | acc: 56.25%,  total acc: 54.06%   [EVAL] batch:   20 | acc: 25.00%,  total acc: 52.68%   [EVAL] batch:   21 | acc: 68.75%,  total acc: 53.41%   [EVAL] batch:   22 | acc: 62.50%,  total acc: 53.80%   [EVAL] batch:   23 | acc: 31.25%,  total acc: 52.86%   [EVAL] batch:   24 | acc: 50.00%,  total acc: 52.75%   [EVAL] batch:   25 | acc: 6.25%,  total acc: 50.96%   [EVAL] batch:   26 | acc: 0.00%,  total acc: 49.07%   [EVAL] batch:   27 | acc: 0.00%,  total acc: 47.32%   [EVAL] batch:   28 | acc: 6.25%,  total acc: 45.91%   [EVAL] batch:   29 | acc: 18.75%,  total acc: 45.00%   [EVAL] batch:   30 | acc: 0.00%,  total acc: 43.55%   [EVAL] batch:   31 | acc: 56.25%,  total acc: 43.95%   [EVAL] batch:   32 | acc: 81.25%,  total acc: 45.08%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 46.51%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 47.86%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 48.96%   [EVAL] batch:   36 | acc: 81.25%,  total acc: 49.83%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 50.82%   [EVAL] batch:   38 | acc: 87.50%,  total acc: 51.76%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 52.97%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 54.12%   [EVAL] batch:   41 | acc: 75.00%,  total acc: 54.61%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 55.52%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 56.39%   [EVAL] batch:   44 | acc: 81.25%,  total acc: 56.94%   [EVAL] batch:   45 | acc: 75.00%,  total acc: 57.34%   [EVAL] batch:   46 | acc: 75.00%,  total acc: 57.71%   [EVAL] batch:   47 | acc: 68.75%,  total acc: 57.94%   [EVAL] batch:   48 | acc: 81.25%,  total acc: 58.42%   [EVAL] batch:   49 | acc: 81.25%,  total acc: 58.88%   [EVAL] batch:   50 | acc: 37.50%,  total acc: 58.46%   [EVAL] batch:   51 | acc: 68.75%,  total acc: 58.65%   [EVAL] batch:   52 | acc: 62.50%,  total acc: 58.73%   [EVAL] batch:   53 | acc: 68.75%,  total acc: 58.91%   [EVAL] batch:   54 | acc: 56.25%,  total acc: 58.86%   [EVAL] batch:   55 | acc: 68.75%,  total acc: 59.04%   [EVAL] batch:   56 | acc: 43.75%,  total acc: 58.77%   [EVAL] batch:   57 | acc: 50.00%,  total acc: 58.62%   [EVAL] batch:   58 | acc: 43.75%,  total acc: 58.37%   [EVAL] batch:   59 | acc: 81.25%,  total acc: 58.75%   [EVAL] batch:   60 | acc: 56.25%,  total acc: 58.71%   [EVAL] batch:   61 | acc: 75.00%,  total acc: 58.97%   [EVAL] batch:   62 | acc: 25.00%,  total acc: 58.43%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 79.17%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 78.12%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 76.25%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 79.17%   [EVAL] batch:    6 | acc: 62.50%,  total acc: 76.79%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 78.12%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 78.47%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 77.50%   [EVAL] batch:   10 | acc: 68.75%,  total acc: 76.70%   [EVAL] batch:   11 | acc: 68.75%,  total acc: 76.04%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 76.92%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 78.12%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 79.17%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 80.47%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 81.94%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 82.89%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 82.81%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 83.33%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 83.52%   [EVAL] batch:   22 | acc: 87.50%,  total acc: 83.70%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 83.85%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 84.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 84.62%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 85.19%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 85.49%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 85.78%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 86.25%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 86.69%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 87.11%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 87.87%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 88.04%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 88.37%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 88.68%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 88.98%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 89.26%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 89.53%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 89.79%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 90.03%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 90.12%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 90.06%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 90.00%   [EVAL] batch:   45 | acc: 81.25%,  total acc: 89.81%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 89.63%   [EVAL] batch:   47 | acc: 81.25%,  total acc: 89.45%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 89.67%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 89.75%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 89.83%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 89.90%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 89.98%   [EVAL] batch:   53 | acc: 93.75%,  total acc: 90.05%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 89.89%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 89.96%   [EVAL] batch:   56 | acc: 75.00%,  total acc: 89.69%   [EVAL] batch:   57 | acc: 50.00%,  total acc: 89.01%   [EVAL] batch:   58 | acc: 62.50%,  total acc: 88.56%   [EVAL] batch:   59 | acc: 75.00%,  total acc: 88.33%   [EVAL] batch:   60 | acc: 81.25%,  total acc: 88.22%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 87.90%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 87.30%   [EVAL] batch:   63 | acc: 43.75%,  total acc: 86.62%   [EVAL] batch:   64 | acc: 68.75%,  total acc: 86.35%   [EVAL] batch:   65 | acc: 50.00%,  total acc: 85.80%   [EVAL] batch:   66 | acc: 56.25%,  total acc: 85.35%   [EVAL] batch:   67 | acc: 56.25%,  total acc: 84.93%   [EVAL] batch:   68 | acc: 43.75%,  total acc: 84.33%   [EVAL] batch:   69 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:   70 | acc: 87.50%,  total acc: 84.42%   [EVAL] batch:   71 | acc: 68.75%,  total acc: 84.20%   [EVAL] batch:   72 | acc: 81.25%,  total acc: 84.16%   [EVAL] batch:   73 | acc: 81.25%,  total acc: 84.12%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 84.17%   [EVAL] batch:   75 | acc: 87.50%,  total acc: 84.21%   [EVAL] batch:   76 | acc: 81.25%,  total acc: 84.17%   [EVAL] batch:   77 | acc: 75.00%,  total acc: 84.05%   [EVAL] batch:   78 | acc: 75.00%,  total acc: 83.94%   [EVAL] batch:   79 | acc: 81.25%,  total acc: 83.91%   [EVAL] batch:   80 | acc: 100.00%,  total acc: 84.10%   [EVAL] batch:   81 | acc: 68.75%,  total acc: 83.92%   [EVAL] batch:   82 | acc: 43.75%,  total acc: 83.43%   [EVAL] batch:   83 | acc: 31.25%,  total acc: 82.81%   [EVAL] batch:   84 | acc: 43.75%,  total acc: 82.35%   [EVAL] batch:   85 | acc: 18.75%,  total acc: 81.61%   [EVAL] batch:   86 | acc: 43.75%,  total acc: 81.18%   [EVAL] batch:   87 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 81.46%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 81.67%   [EVAL] batch:   90 | acc: 100.00%,  total acc: 81.87%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 82.07%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 82.26%   [EVAL] batch:   93 | acc: 100.00%,  total acc: 82.45%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 82.63%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 82.81%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 82.99%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 83.16%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 83.33%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 83.50%   [EVAL] batch:  100 | acc: 50.00%,  total acc: 83.17%   [EVAL] batch:  101 | acc: 68.75%,  total acc: 83.03%   [EVAL] batch:  102 | acc: 56.25%,  total acc: 82.77%   [EVAL] batch:  103 | acc: 56.25%,  total acc: 82.51%   [EVAL] batch:  104 | acc: 87.50%,  total acc: 82.56%   [EVAL] batch:  105 | acc: 62.50%,  total acc: 82.37%   [EVAL] batch:  106 | acc: 43.75%,  total acc: 82.01%   [EVAL] batch:  107 | acc: 25.00%,  total acc: 81.48%   [EVAL] batch:  108 | acc: 43.75%,  total acc: 81.14%   [EVAL] batch:  109 | acc: 25.00%,  total acc: 80.62%   [EVAL] batch:  110 | acc: 50.00%,  total acc: 80.35%   [EVAL] batch:  111 | acc: 25.00%,  total acc: 79.85%   [EVAL] batch:  112 | acc: 68.75%,  total acc: 79.76%   [EVAL] batch:  113 | acc: 87.50%,  total acc: 79.82%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 80.00%   [EVAL] batch:  115 | acc: 87.50%,  total acc: 80.06%   [EVAL] batch:  116 | acc: 87.50%,  total acc: 80.13%   [EVAL] batch:  117 | acc: 87.50%,  total acc: 80.19%   [EVAL] batch:  118 | acc: 75.00%,  total acc: 80.15%   [EVAL] batch:  119 | acc: 25.00%,  total acc: 79.69%   [EVAL] batch:  120 | acc: 31.25%,  total acc: 79.29%   [EVAL] batch:  121 | acc: 6.25%,  total acc: 78.69%   [EVAL] batch:  122 | acc: 37.50%,  total acc: 78.35%   [EVAL] batch:  123 | acc: 12.50%,  total acc: 77.82%   [EVAL] batch:  124 | acc: 31.25%,  total acc: 77.45%   [EVAL] batch:  125 | acc: 62.50%,  total acc: 77.33%   [EVAL] batch:  126 | acc: 56.25%,  total acc: 77.17%   [EVAL] batch:  127 | acc: 68.75%,  total acc: 77.10%   [EVAL] batch:  128 | acc: 75.00%,  total acc: 77.08%   [EVAL] batch:  129 | acc: 75.00%,  total acc: 77.07%   [EVAL] batch:  130 | acc: 68.75%,  total acc: 77.00%   [EVAL] batch:  131 | acc: 93.75%,  total acc: 77.13%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 77.30%   [EVAL] batch:  133 | acc: 81.25%,  total acc: 77.33%   [EVAL] batch:  134 | acc: 87.50%,  total acc: 77.41%   [EVAL] batch:  135 | acc: 93.75%,  total acc: 77.53%   [EVAL] batch:  136 | acc: 93.75%,  total acc: 77.65%   [EVAL] batch:  137 | acc: 75.00%,  total acc: 77.63%   [EVAL] batch:  138 | acc: 25.00%,  total acc: 77.25%   [EVAL] batch:  139 | acc: 37.50%,  total acc: 76.96%   [EVAL] batch:  140 | acc: 18.75%,  total acc: 76.55%   [EVAL] batch:  141 | acc: 25.00%,  total acc: 76.19%   [EVAL] batch:  142 | acc: 25.00%,  total acc: 75.83%   [EVAL] batch:  143 | acc: 31.25%,  total acc: 75.52%   [EVAL] batch:  144 | acc: 100.00%,  total acc: 75.69%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 75.86%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 75.98%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 76.14%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 76.30%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 76.42%   [EVAL] batch:  150 | acc: 37.50%,  total acc: 76.16%   [EVAL] batch:  151 | acc: 25.00%,  total acc: 75.82%   [EVAL] batch:  152 | acc: 37.50%,  total acc: 75.57%   [EVAL] batch:  153 | acc: 37.50%,  total acc: 75.32%   [EVAL] batch:  154 | acc: 31.25%,  total acc: 75.04%   [EVAL] batch:  155 | acc: 12.50%,  total acc: 74.64%   [EVAL] batch:  156 | acc: 81.25%,  total acc: 74.68%   [EVAL] batch:  157 | acc: 87.50%,  total acc: 74.76%   [EVAL] batch:  158 | acc: 93.75%,  total acc: 74.88%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 75.00%   [EVAL] batch:  160 | acc: 87.50%,  total acc: 75.08%   [EVAL] batch:  161 | acc: 100.00%,  total acc: 75.23%   [EVAL] batch:  162 | acc: 68.75%,  total acc: 75.19%   [EVAL] batch:  163 | acc: 56.25%,  total acc: 75.08%   [EVAL] batch:  164 | acc: 62.50%,  total acc: 75.00%   [EVAL] batch:  165 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:  166 | acc: 62.50%,  total acc: 74.93%   [EVAL] batch:  167 | acc: 68.75%,  total acc: 74.89%   [EVAL] batch:  168 | acc: 75.00%,  total acc: 74.89%   [EVAL] batch:  169 | acc: 56.25%,  total acc: 74.78%   [EVAL] batch:  170 | acc: 75.00%,  total acc: 74.78%   [EVAL] batch:  171 | acc: 62.50%,  total acc: 74.71%   [EVAL] batch:  172 | acc: 31.25%,  total acc: 74.46%   [EVAL] batch:  173 | acc: 56.25%,  total acc: 74.35%   [EVAL] batch:  174 | acc: 75.00%,  total acc: 74.36%   [EVAL] batch:  175 | acc: 43.75%,  total acc: 74.18%   [EVAL] batch:  176 | acc: 43.75%,  total acc: 74.01%   [EVAL] batch:  177 | acc: 62.50%,  total acc: 73.95%   [EVAL] batch:  178 | acc: 68.75%,  total acc: 73.92%   [EVAL] batch:  179 | acc: 62.50%,  total acc: 73.85%   [EVAL] batch:  180 | acc: 68.75%,  total acc: 73.83%   [EVAL] batch:  181 | acc: 75.00%,  total acc: 73.83%   [EVAL] batch:  182 | acc: 87.50%,  total acc: 73.91%   [EVAL] batch:  183 | acc: 93.75%,  total acc: 74.01%   [EVAL] batch:  184 | acc: 81.25%,  total acc: 74.05%   [EVAL] batch:  185 | acc: 87.50%,  total acc: 74.13%   [EVAL] batch:  186 | acc: 68.75%,  total acc: 74.10%   [EVAL] batch:  187 | acc: 62.50%,  total acc: 74.04%   [EVAL] batch:  188 | acc: 31.25%,  total acc: 73.81%   [EVAL] batch:  189 | acc: 37.50%,  total acc: 73.62%   [EVAL] batch:  190 | acc: 50.00%,  total acc: 73.49%   [EVAL] batch:  191 | acc: 31.25%,  total acc: 73.27%   [EVAL] batch:  192 | acc: 43.75%,  total acc: 73.12%   [EVAL] batch:  193 | acc: 50.00%,  total acc: 73.00%   [EVAL] batch:  194 | acc: 81.25%,  total acc: 73.04%   [EVAL] batch:  195 | acc: 62.50%,  total acc: 72.99%   [EVAL] batch:  196 | acc: 81.25%,  total acc: 73.03%   [EVAL] batch:  197 | acc: 93.75%,  total acc: 73.14%   [EVAL] batch:  198 | acc: 75.00%,  total acc: 73.15%   [EVAL] batch:  199 | acc: 62.50%,  total acc: 73.09%   [EVAL] batch:  200 | acc: 75.00%,  total acc: 73.10%   [EVAL] batch:  201 | acc: 68.75%,  total acc: 73.08%   [EVAL] batch:  202 | acc: 56.25%,  total acc: 73.00%   [EVAL] batch:  203 | acc: 75.00%,  total acc: 73.01%   [EVAL] batch:  204 | acc: 81.25%,  total acc: 73.05%   [EVAL] batch:  205 | acc: 81.25%,  total acc: 73.09%   [EVAL] batch:  206 | acc: 93.75%,  total acc: 73.19%   [EVAL] batch:  207 | acc: 100.00%,  total acc: 73.32%   [EVAL] batch:  208 | acc: 93.75%,  total acc: 73.42%   [EVAL] batch:  209 | acc: 93.75%,  total acc: 73.51%   [EVAL] batch:  210 | acc: 100.00%,  total acc: 73.64%   [EVAL] batch:  211 | acc: 100.00%,  total acc: 73.76%   [EVAL] batch:  212 | acc: 87.50%,  total acc: 73.83%   [EVAL] batch:  213 | acc: 87.50%,  total acc: 73.89%   [EVAL] batch:  214 | acc: 87.50%,  total acc: 73.95%   [EVAL] batch:  215 | acc: 87.50%,  total acc: 74.02%   [EVAL] batch:  216 | acc: 87.50%,  total acc: 74.08%   [EVAL] batch:  217 | acc: 93.75%,  total acc: 74.17%   [EVAL] batch:  218 | acc: 81.25%,  total acc: 74.20%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 74.32%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 74.43%   [EVAL] batch:  221 | acc: 87.50%,  total acc: 74.49%   [EVAL] batch:  222 | acc: 93.75%,  total acc: 74.58%   [EVAL] batch:  223 | acc: 93.75%,  total acc: 74.67%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 74.78%   [EVAL] batch:  225 | acc: 31.25%,  total acc: 74.59%   [EVAL] batch:  226 | acc: 25.00%,  total acc: 74.37%   [EVAL] batch:  227 | acc: 25.00%,  total acc: 74.15%   [EVAL] batch:  228 | acc: 31.25%,  total acc: 73.96%   [EVAL] batch:  229 | acc: 25.00%,  total acc: 73.75%   [EVAL] batch:  230 | acc: 25.00%,  total acc: 73.54%   [EVAL] batch:  231 | acc: 68.75%,  total acc: 73.52%   [EVAL] batch:  232 | acc: 93.75%,  total acc: 73.61%   [EVAL] batch:  233 | acc: 93.75%,  total acc: 73.69%   [EVAL] batch:  234 | acc: 93.75%,  total acc: 73.78%   [EVAL] batch:  235 | acc: 75.00%,  total acc: 73.78%   [EVAL] batch:  236 | acc: 87.50%,  total acc: 73.84%   [EVAL] batch:  237 | acc: 68.75%,  total acc: 73.82%   [EVAL] batch:  238 | acc: 50.00%,  total acc: 73.72%   [EVAL] batch:  239 | acc: 50.00%,  total acc: 73.62%   [EVAL] batch:  240 | acc: 56.25%,  total acc: 73.55%   [EVAL] batch:  241 | acc: 62.50%,  total acc: 73.50%   [EVAL] batch:  242 | acc: 43.75%,  total acc: 73.38%   [EVAL] batch:  243 | acc: 25.00%,  total acc: 73.18%   [EVAL] batch:  244 | acc: 81.25%,  total acc: 73.21%   [EVAL] batch:  245 | acc: 81.25%,  total acc: 73.25%   [EVAL] batch:  246 | acc: 62.50%,  total acc: 73.20%   [EVAL] batch:  247 | acc: 81.25%,  total acc: 73.24%   [EVAL] batch:  248 | acc: 87.50%,  total acc: 73.29%   [EVAL] batch:  249 | acc: 93.75%,  total acc: 73.38%   [EVAL] batch:  250 | acc: 6.25%,  total acc: 73.11%   [EVAL] batch:  251 | acc: 31.25%,  total acc: 72.94%   [EVAL] batch:  252 | acc: 12.50%,  total acc: 72.70%   [EVAL] batch:  253 | acc: 25.00%,  total acc: 72.51%   [EVAL] batch:  254 | acc: 6.25%,  total acc: 72.25%   [EVAL] batch:  255 | acc: 12.50%,  total acc: 72.02%   [EVAL] batch:  256 | acc: 62.50%,  total acc: 71.98%   [EVAL] batch:  257 | acc: 87.50%,  total acc: 72.04%   [EVAL] batch:  258 | acc: 100.00%,  total acc: 72.15%   [EVAL] batch:  259 | acc: 93.75%,  total acc: 72.24%   [EVAL] batch:  260 | acc: 87.50%,  total acc: 72.29%   [EVAL] batch:  261 | acc: 81.25%,  total acc: 72.33%   [EVAL] batch:  262 | acc: 75.00%,  total acc: 72.34%   [EVAL] batch:  263 | acc: 50.00%,  total acc: 72.25%   [EVAL] batch:  264 | acc: 75.00%,  total acc: 72.26%   [EVAL] batch:  265 | acc: 56.25%,  total acc: 72.20%   [EVAL] batch:  266 | acc: 43.75%,  total acc: 72.10%   [EVAL] batch:  267 | acc: 81.25%,  total acc: 72.13%   [EVAL] batch:  268 | acc: 37.50%,  total acc: 72.00%   [EVAL] batch:  269 | acc: 56.25%,  total acc: 71.94%   [EVAL] batch:  270 | acc: 25.00%,  total acc: 71.77%   [EVAL] batch:  271 | acc: 68.75%,  total acc: 71.76%   [EVAL] batch:  272 | acc: 62.50%,  total acc: 71.73%   [EVAL] batch:  273 | acc: 31.25%,  total acc: 71.58%   [EVAL] batch:  274 | acc: 50.00%,  total acc: 71.50%   [EVAL] batch:  275 | acc: 6.25%,  total acc: 71.26%   [EVAL] batch:  276 | acc: 0.00%,  total acc: 71.01%   [EVAL] batch:  277 | acc: 0.00%,  total acc: 70.75%   [EVAL] batch:  278 | acc: 6.25%,  total acc: 70.52%   [EVAL] batch:  279 | acc: 18.75%,  total acc: 70.33%   [EVAL] batch:  280 | acc: 0.00%,  total acc: 70.08%   [EVAL] batch:  281 | acc: 56.25%,  total acc: 70.04%   [EVAL] batch:  282 | acc: 81.25%,  total acc: 70.08%   [EVAL] batch:  283 | acc: 93.75%,  total acc: 70.16%   [EVAL] batch:  284 | acc: 93.75%,  total acc: 70.24%   [EVAL] batch:  285 | acc: 87.50%,  total acc: 70.30%   [EVAL] batch:  286 | acc: 81.25%,  total acc: 70.34%   [EVAL] batch:  287 | acc: 87.50%,  total acc: 70.40%   [EVAL] batch:  288 | acc: 87.50%,  total acc: 70.46%   [EVAL] batch:  289 | acc: 100.00%,  total acc: 70.56%   [EVAL] batch:  290 | acc: 100.00%,  total acc: 70.66%   [EVAL] batch:  291 | acc: 75.00%,  total acc: 70.68%   [EVAL] batch:  292 | acc: 93.75%,  total acc: 70.76%   [EVAL] batch:  293 | acc: 93.75%,  total acc: 70.83%   [EVAL] batch:  294 | acc: 81.25%,  total acc: 70.87%   [EVAL] batch:  295 | acc: 75.00%,  total acc: 70.88%   [EVAL] batch:  296 | acc: 75.00%,  total acc: 70.90%   [EVAL] batch:  297 | acc: 68.75%,  total acc: 70.89%   [EVAL] batch:  298 | acc: 81.25%,  total acc: 70.92%   [EVAL] batch:  299 | acc: 81.25%,  total acc: 70.96%   [EVAL] batch:  300 | acc: 37.50%,  total acc: 70.85%   [EVAL] batch:  301 | acc: 68.75%,  total acc: 70.84%   [EVAL] batch:  302 | acc: 62.50%,  total acc: 70.81%   [EVAL] batch:  303 | acc: 68.75%,  total acc: 70.81%   [EVAL] batch:  304 | acc: 56.25%,  total acc: 70.76%   [EVAL] batch:  305 | acc: 68.75%,  total acc: 70.75%   [EVAL] batch:  306 | acc: 43.75%,  total acc: 70.66%   [EVAL] batch:  307 | acc: 50.00%,  total acc: 70.60%   [EVAL] batch:  308 | acc: 43.75%,  total acc: 70.51%   [EVAL] batch:  309 | acc: 81.25%,  total acc: 70.54%   [EVAL] batch:  310 | acc: 56.25%,  total acc: 70.50%   [EVAL] batch:  311 | acc: 75.00%,  total acc: 70.51%   [EVAL] batch:  312 | acc: 25.00%,  total acc: 70.37%   
cur_acc:  ['0.9524', '0.6994', '0.7639', '0.6895', '0.5843']
his_acc:  ['0.9524', '0.8220', '0.7949', '0.7342', '0.7037']
Clustering into  29  clusters
Clusters:  [ 3 15  9  8  5 10  9 11 15  6 27  7 11  4 21 28 20 12  8 12  3  4 17 19
 17 16  2 18 26 23 24  0 12 25 18  0 12  7  6  7  5  4  2  1 17 14  2 15
  2 15 13 22 11  5  1  6  3 10  0  0]
Losses:  13.7241792678833 6.269272804260254 0.7962701916694641
CurrentTrain: epoch  0, batch     0 | loss: 13.7241793Losses:  11.50401782989502 4.853418350219727 0.6188053488731384
CurrentTrain: epoch  0, batch     1 | loss: 11.5040178Losses:  11.192144393920898 4.349335193634033 0.6439208984375
CurrentTrain: epoch  0, batch     2 | loss: 11.1921444Losses:  5.752127647399902 -0.0 0.10979470610618591
CurrentTrain: epoch  0, batch     3 | loss: 5.7521276Losses:  8.710959434509277 2.839871883392334 0.7483967542648315
CurrentTrain: epoch  1, batch     0 | loss: 8.7109594Losses:  8.661225318908691 2.727853536605835 0.7181118726730347
CurrentTrain: epoch  1, batch     1 | loss: 8.6612253Losses:  7.7617597579956055 2.848101854324341 0.6776447892189026
CurrentTrain: epoch  1, batch     2 | loss: 7.7617598Losses:  3.5737617015838623 -0.0 0.11363794654607773
CurrentTrain: epoch  1, batch     3 | loss: 3.5737617Losses:  8.123291015625 3.14774751663208 0.7290022373199463
CurrentTrain: epoch  2, batch     0 | loss: 8.1232910Losses:  8.969834327697754 4.2169599533081055 0.6010245084762573
CurrentTrain: epoch  2, batch     1 | loss: 8.9698343Losses:  6.497068405151367 2.1935274600982666 0.7602434754371643
CurrentTrain: epoch  2, batch     2 | loss: 6.4970684Losses:  2.1056106090545654 -0.0 0.12562616169452667
CurrentTrain: epoch  2, batch     3 | loss: 2.1056106Losses:  7.415022850036621 3.398273468017578 0.6604411602020264
CurrentTrain: epoch  3, batch     0 | loss: 7.4150229Losses:  7.188266754150391 3.1137852668762207 0.69178307056427
CurrentTrain: epoch  3, batch     1 | loss: 7.1882668Losses:  6.25068998336792 2.7987756729125977 0.7299941182136536
CurrentTrain: epoch  3, batch     2 | loss: 6.2506900Losses:  5.518260478973389 -0.0 0.0746903270483017
CurrentTrain: epoch  3, batch     3 | loss: 5.5182605Losses:  7.183678150177002 3.470133066177368 0.6787066459655762
CurrentTrain: epoch  4, batch     0 | loss: 7.1836782Losses:  6.86499547958374 3.4114696979522705 0.6420121192932129
CurrentTrain: epoch  4, batch     1 | loss: 6.8649955Losses:  6.322792053222656 3.0620787143707275 0.6540176868438721
CurrentTrain: epoch  4, batch     2 | loss: 6.3227921Losses:  3.0011839866638184 -0.0 0.12378253042697906
CurrentTrain: epoch  4, batch     3 | loss: 3.0011840Losses:  5.539275169372559 2.0980892181396484 0.7201744914054871
CurrentTrain: epoch  5, batch     0 | loss: 5.5392752Losses:  6.087671756744385 3.2638378143310547 0.6203921437263489
CurrentTrain: epoch  5, batch     1 | loss: 6.0876718Losses:  8.145390510559082 4.395559310913086 0.7497637271881104
CurrentTrain: epoch  5, batch     2 | loss: 8.1453905Losses:  1.8475207090377808 -0.0 0.09340201318264008
CurrentTrain: epoch  5, batch     3 | loss: 1.8475207Losses:  5.2638020515441895 2.4884581565856934 0.6409417986869812
CurrentTrain: epoch  6, batch     0 | loss: 5.2638021Losses:  7.095902919769287 3.6899771690368652 0.6634524464607239
CurrentTrain: epoch  6, batch     1 | loss: 7.0959029Losses:  5.365680694580078 2.3191635608673096 0.7092016935348511
CurrentTrain: epoch  6, batch     2 | loss: 5.3656807Losses:  2.0662479400634766 -0.0 0.10064804553985596
CurrentTrain: epoch  6, batch     3 | loss: 2.0662479Losses:  5.882063865661621 2.499530076980591 0.7150330543518066
CurrentTrain: epoch  7, batch     0 | loss: 5.8820639Losses:  4.99892520904541 1.9474046230316162 0.7065287828445435
CurrentTrain: epoch  7, batch     1 | loss: 4.9989252Losses:  5.905196189880371 3.0847878456115723 0.5414162874221802
CurrentTrain: epoch  7, batch     2 | loss: 5.9051962Losses:  1.995204210281372 -0.0 0.10011841356754303
CurrentTrain: epoch  7, batch     3 | loss: 1.9952042Losses:  5.694994926452637 2.794006824493408 0.5714671611785889
CurrentTrain: epoch  8, batch     0 | loss: 5.6949949Losses:  5.94151496887207 3.1195194721221924 0.6422210335731506
CurrentTrain: epoch  8, batch     1 | loss: 5.9415150Losses:  6.082941055297852 3.7481629848480225 0.4759727418422699
CurrentTrain: epoch  8, batch     2 | loss: 6.0829411Losses:  1.9072530269622803 -0.0 0.10825107991695404
CurrentTrain: epoch  8, batch     3 | loss: 1.9072530Losses:  5.508752346038818 2.9973952770233154 0.6425722241401672
CurrentTrain: epoch  9, batch     0 | loss: 5.5087523Losses:  4.650978088378906 1.9298193454742432 0.6850612163543701
CurrentTrain: epoch  9, batch     1 | loss: 4.6509781Losses:  4.66574239730835 1.8564167022705078 0.7020098567008972
CurrentTrain: epoch  9, batch     2 | loss: 4.6657424Losses:  1.7414765357971191 -0.0 -0.0
CurrentTrain: epoch  9, batch     3 | loss: 1.7414765
#### spouse ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "spouse" means the subject has the object as their spouse (husband, wife, partner, etc.).
Here is an example:
Relation: spouse
Context: flavius timasius ( died 396 ) was a general of the roman empire , a relative of the empress aelia flaccilla , wife of emperor theodosius i ( r. 379–395 ) .
Head Entity: theodosius i
Tail Entity: aelia flaccilla
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "spouse":
Relation: spouse  
Context: in 1992, the famous actor brad pitt married actress jennifer aniston, marking one of the most talked-about celebrity unions of the decade.  
Head Entity: brad pitt  
Tail Entity: jennifer aniston  

Relation: spouse  
Context: after years of dating, the renowned musician john legend finally tied the knot with model chrissy teigen in a lavish ceremony in italy.  
Head Entity: john legend  
Tail Entity: chrissy teigen  
#### record label ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "record label" means brand and trademark associated with the marketing of subject music recordings and music videos.
Here is an example:
Relation: record label
Context: " i 'm a cuckoo " was belle & sebastian 's second single from " dear catastrophe waitress " , released on rough trade records in 2004 .
Head Entity: belle & sebastian
Tail Entity: rough trade records
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "record label":
Relation: record label  
Context: The album "Random Access Memories" by Daft Punk was released under the Columbia Records label in 2013.  
Head Entity: Daft Punk  
Tail Entity: Columbia Records  

Relation: record label  
Context: Taylor Swift's latest album "Evermore" was produced by her own label, Republic Records, and released in 2020.  
Head Entity: Taylor Swift  
Tail Entity: Republic Records  
#### distributor ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "distributor" means distributor of a creative work; distributor for a record label; news agency; film distributor.
Here is an example:
Relation: distributor
Context: distributed by paramount pictures , " the italian job " was theatrically released in the united states on may 30 , 2003 , and grossed over $ 176 million worldwide .
Head Entity: the italian job
Tail Entity: paramount pictures
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "distributor":
Relation: distributor  
Context: the film was distributed by universal pictures and received critical acclaim upon its release in 2015.  
Head Entity: the film  
Tail Entity: universal pictures  

Relation: distributor  
Context: the album was distributed by sony music and quickly climbed the charts, becoming a bestseller.  
Head Entity: the album  
Tail Entity: sony music  
#### located in or next to body of water ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "located in or next to body of water" means body of water on or next to which a place is located.
Here is an example:
Relation: located in or next to body of water
Context: in 1796 carondelet commissioned pintado to conduct a survey of lands granted to ambrosio longue - épée in iberville county on the west bank of the mississippi river opposite baton rouge .
Head Entity: baton rouge
Tail Entity: mississippi river
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "located in or next to body of water":
Relation: located in or next to body of water  
Context: the city of new orleans is situated on the banks of the mississippi river, making it a vibrant hub for trade and culture.  
Head Entity: new orleans  
Tail Entity: mississippi river  

Relation: located in or next to body of water  
Context: the picturesque village of lake placid is nestled beside the serene waters of mirror lake, attracting visitors year-round.  
Head Entity: lake placid  
Tail Entity: mirror lake  
#### subsidiary ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "subsidiary" means subsidiary of a company or organization; generally a fully owned separate corporation.
Here is an example:
Relation: subsidiary
Context: cbs and viacom split again in 2005 , but both are still owned by national amusements .
Head Entity: national amusements
Tail Entity: viacom
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "subsidiary":
Relation: subsidiary  
Context: In 2018, Facebook acquired WhatsApp, making it a subsidiary of the social media giant.  
Head Entity: Facebook  
Tail Entity: WhatsApp  

Relation: subsidiary  
Context: Toyota has several subsidiaries, including Lexus, which focuses on luxury vehicles.  
Head Entity: Toyota  
Tail Entity: Lexus  
#### country of citizenship ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "country of citizenship" means the object is a country that recognizes the subject as its citizen.
Here is an example:
Relation: country of citizenship
Context: he challenged seven other swimmers in heat four including ukraine 's rostyslav svanidze , a top 16 finalist in atlanta four years earlier .
Head Entity: rostyslav svanidze
Tail Entity: ukraine
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "country of citizenship":
Relation: country of citizenship  
Context: after years of living in the united states, the renowned artist decided to apply for dual citizenship, allowing him to retain his ties to his homeland, italy.  
Head Entity: the renowned artist  
Tail Entity: italy  

Relation: country of citizenship  
Context: during the international conference, the speaker proudly mentioned that she had recently obtained her citizenship in canada, where she had been residing for over a decade.  
Head Entity: the speaker  
Tail Entity: canada  
#### genre ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "genre" means creative work's genre or an artist's field of work.
Here is an example:
Relation: genre
Context: he is also a part of the atlanta - based hip - hop / r&b;/soul musical collective the dungeon family .
Head Entity: the dungeon family
Tail Entity: hip - hop
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "genre":
Relation: genre  
Context: the band is known for their unique blend of rock and electronic music, often categorized under the genre of synth-pop.  
Head Entity: the band  
Tail Entity: synth-pop  

Relation: genre  
Context: she has made a significant impact in the world of classical music, particularly as a renowned pianist and composer in the genre of contemporary classical.  
Head Entity: she  
Tail Entity: contemporary classical  
#### mouth of the watercourse ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "mouth of the watercourse" means the body of water to which the watercourse drains.
Here is an example:
Relation: mouth of the watercourse
Context: the club is named after the ave river , which flows through the town and into the atlantic ocean .
Head Entity: ave river
Tail Entity: atlantic ocean
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "mouth of the watercourse":
Relation: mouth of the watercourse  
Context: the mississippi river empties into the gulf of mexico, creating a rich delta ecosystem.  
Head Entity: mississippi river  
Tail Entity: gulf of mexico  

Relation: mouth of the watercourse  
Context: the danube river flows through several countries before reaching the black sea, where it finally discharges its waters.  
Head Entity: danube river  
Tail Entity: black sea  
#### position played on team / speciality ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "position played on team / speciality" means position or specialism of a player on a team.
Here is an example:
Relation: position played on team / speciality
Context: bradley halliday ( born 10 july 1995 ) is an english professional footballer who plays as a defender or a midfielder for league two club cambridge united .
Head Entity: bradley halliday
Tail Entity: defender
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "position played on team / speciality":
Relation: position played on team / speciality  
Context: alex morgan ( born 2 july 1989 ) is an american soccer player who plays as a forward for the national team and club orlando pride in the national women's soccer league.  
Head Entity: alex morgan  
Tail Entity: forward  

Relation: position played on team / speciality  
Context: lebron james ( born 30 december 1984 ) is an american professional basketball player who plays as a small forward for the los angeles lakers in the nba.  
Head Entity: lebron james  
Tail Entity: small forward  
#### sports season of league or competition ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "sports season of league or competition" means property that shows the competition of which the item is a season.
Here is an example:
Relation: sports season of league or competition
Context: chetroiu 's best finish at the fil world luge championships was 17th in the doubles event twice ( 2008 , 2009 ) .
Head Entity: 2008
Tail Entity: fil world luge championships
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "sports season of league or competition":
Relation: sports season of league or competition  
Context: The 2020 UEFA European Championship was postponed to 2021 due to the COVID-19 pandemic, marking a significant change in the tournament's schedule.  
Head Entity: 2021  
Tail Entity: UEFA European Championship  

Relation: sports season of league or competition  
Context: The NBA season of 2019-2020 was interrupted by the pandemic, leading to a unique playoff format that took place in a bubble environment.  
Head Entity: 2019-2020  
Tail Entity: NBA
Losses:  4.841509819030762 0.20811034739017487 0.9675033092498779
MemoryTrain:  epoch  0, batch     0 | loss: 4.8415098Losses:  3.9011447429656982 0.8999776244163513 0.9469802379608154
MemoryTrain:  epoch  0, batch     1 | loss: 3.9011447Losses:  3.641559600830078 0.28478747606277466 0.9214675426483154
MemoryTrain:  epoch  0, batch     2 | loss: 3.6415596Losses:  4.137432098388672 -0.0 0.9923434853553772
MemoryTrain:  epoch  0, batch     3 | loss: 4.1374321Losses:  4.566007137298584 0.5795745253562927 0.8916791081428528
MemoryTrain:  epoch  0, batch     4 | loss: 4.5660071Losses:  4.434326648712158 0.28911149501800537 0.9489316940307617
MemoryTrain:  epoch  0, batch     5 | loss: 4.4343266Losses:  4.337373733520508 -0.0 0.9379755854606628
MemoryTrain:  epoch  0, batch     6 | loss: 4.3373737Losses:  4.845363616943359 0.7281489372253418 0.8528652191162109
MemoryTrain:  epoch  0, batch     7 | loss: 4.8453636Losses:  5.115306854248047 0.37001702189445496 0.9321982264518738
MemoryTrain:  epoch  0, batch     8 | loss: 5.1153069Losses:  4.1298346519470215 0.2683200240135193 0.9674229621887207
MemoryTrain:  epoch  0, batch     9 | loss: 4.1298347Losses:  3.7826061248779297 0.2549608647823334 0.9613302946090698
MemoryTrain:  epoch  0, batch    10 | loss: 3.7826061Losses:  5.773504257202148 -0.0 0.4379264712333679
MemoryTrain:  epoch  0, batch    11 | loss: 5.7735043Losses:  3.3819971084594727 0.8010940551757812 0.8314609527587891
MemoryTrain:  epoch  1, batch     0 | loss: 3.3819971Losses:  5.034966468811035 0.32253390550613403 0.9366011023521423
MemoryTrain:  epoch  1, batch     1 | loss: 5.0349665Losses:  3.7068943977355957 0.2695598602294922 0.8629783391952515
MemoryTrain:  epoch  1, batch     2 | loss: 3.7068944Losses:  4.316267967224121 0.544991135597229 0.9681527018547058
MemoryTrain:  epoch  1, batch     3 | loss: 4.3162680Losses:  4.342967510223389 1.0242449045181274 0.794070303440094
MemoryTrain:  epoch  1, batch     4 | loss: 4.3429675Losses:  3.583864212036133 0.29192912578582764 0.9409412145614624
MemoryTrain:  epoch  1, batch     5 | loss: 3.5838642Losses:  4.92417049407959 0.5608062744140625 0.9876163601875305
MemoryTrain:  epoch  1, batch     6 | loss: 4.9241705Losses:  4.457701683044434 0.763047456741333 0.9003666639328003
MemoryTrain:  epoch  1, batch     7 | loss: 4.4577017Losses:  4.3024187088012695 0.5589282512664795 0.9251667857170105
MemoryTrain:  epoch  1, batch     8 | loss: 4.3024187Losses:  4.173434257507324 0.5822899341583252 0.9766087532043457
MemoryTrain:  epoch  1, batch     9 | loss: 4.1734343Losses:  4.408951759338379 0.5335730314254761 1.0368752479553223
MemoryTrain:  epoch  1, batch    10 | loss: 4.4089518Losses:  3.632214069366455 -0.0 0.3083983063697815
MemoryTrain:  epoch  1, batch    11 | loss: 3.6322141Losses:  3.548837184906006 0.26199567317962646 0.8873244524002075
MemoryTrain:  epoch  2, batch     0 | loss: 3.5488372Losses:  3.9059393405914307 0.22676005959510803 0.9709944128990173
MemoryTrain:  epoch  2, batch     1 | loss: 3.9059393Losses:  4.390655040740967 0.9002170562744141 0.9262832999229431
MemoryTrain:  epoch  2, batch     2 | loss: 4.3906550Losses:  3.084003210067749 -0.0 0.9233543276786804
MemoryTrain:  epoch  2, batch     3 | loss: 3.0840032Losses:  2.7843170166015625 -0.0 0.9652078747749329
MemoryTrain:  epoch  2, batch     4 | loss: 2.7843170Losses:  3.4865918159484863 0.25736111402511597 0.9890236854553223
MemoryTrain:  epoch  2, batch     5 | loss: 3.4865918Losses:  3.1474127769470215 0.2514146566390991 0.888262152671814
MemoryTrain:  epoch  2, batch     6 | loss: 3.1474128Losses:  3.8914308547973633 0.5397218465805054 0.9526824951171875
MemoryTrain:  epoch  2, batch     7 | loss: 3.8914309Losses:  4.548101425170898 0.5244539976119995 0.9823716878890991
MemoryTrain:  epoch  2, batch     8 | loss: 4.5481014Losses:  4.6802544593811035 1.4892256259918213 0.7617743015289307
MemoryTrain:  epoch  2, batch     9 | loss: 4.6802545Losses:  3.1638433933258057 0.26678895950317383 0.9884588122367859
MemoryTrain:  epoch  2, batch    10 | loss: 3.1638434Losses:  4.339264392852783 -0.0 0.33741527795791626
MemoryTrain:  epoch  2, batch    11 | loss: 4.3392644Losses:  3.674372911453247 -0.0 1.0020155906677246
MemoryTrain:  epoch  3, batch     0 | loss: 3.6743729Losses:  3.8240106105804443 0.29604703187942505 0.9868801236152649
MemoryTrain:  epoch  3, batch     1 | loss: 3.8240106Losses:  2.564393997192383 -0.0 1.021212100982666
MemoryTrain:  epoch  3, batch     2 | loss: 2.5643940Losses:  3.337491035461426 0.4924897849559784 0.9140169620513916
MemoryTrain:  epoch  3, batch     3 | loss: 3.3374910Losses:  3.380087375640869 -0.0 0.8484883308410645
MemoryTrain:  epoch  3, batch     4 | loss: 3.3800874Losses:  3.000410318374634 0.5482085943222046 0.9333631992340088
MemoryTrain:  epoch  3, batch     5 | loss: 3.0004103Losses:  3.235706329345703 -0.0 0.9174312949180603
MemoryTrain:  epoch  3, batch     6 | loss: 3.2357063Losses:  4.29639196395874 0.821482241153717 0.8653474450111389
MemoryTrain:  epoch  3, batch     7 | loss: 4.2963920Losses:  3.368333578109741 0.26999932527542114 0.9664792418479919
MemoryTrain:  epoch  3, batch     8 | loss: 3.3683336Losses:  3.877838134765625 0.5961542725563049 1.0013645887374878
MemoryTrain:  epoch  3, batch     9 | loss: 3.8778381Losses:  3.434816837310791 -0.0 1.1081699132919312
MemoryTrain:  epoch  3, batch    10 | loss: 3.4348168Losses:  2.5592784881591797 -0.0 0.3255225121974945
MemoryTrain:  epoch  3, batch    11 | loss: 2.5592785Losses:  3.5775012969970703 0.26515859365463257 1.0086723566055298
MemoryTrain:  epoch  4, batch     0 | loss: 3.5775013Losses:  3.747191905975342 0.5326568484306335 0.9047484397888184
MemoryTrain:  epoch  4, batch     1 | loss: 3.7471919Losses:  3.587101459503174 -0.0 0.9330568313598633
MemoryTrain:  epoch  4, batch     2 | loss: 3.5871015Losses:  2.673153877258301 -0.0 1.0368578433990479
MemoryTrain:  epoch  4, batch     3 | loss: 2.6731539Losses:  3.030970335006714 -0.0 0.9274699091911316
MemoryTrain:  epoch  4, batch     4 | loss: 3.0309703Losses:  3.639738082885742 0.5290831327438354 0.9773816466331482
MemoryTrain:  epoch  4, batch     5 | loss: 3.6397381Losses:  2.95843243598938 0.43559038639068604 0.892207145690918
MemoryTrain:  epoch  4, batch     6 | loss: 2.9584324Losses:  3.3197569847106934 0.5454070568084717 0.9324604272842407
MemoryTrain:  epoch  4, batch     7 | loss: 3.3197570Losses:  3.30496883392334 0.3098442256450653 0.9108487367630005
MemoryTrain:  epoch  4, batch     8 | loss: 3.3049688Losses:  2.413787364959717 -0.0 0.9584488868713379
MemoryTrain:  epoch  4, batch     9 | loss: 2.4137874Losses:  3.15181303024292 0.491380512714386 0.9152789115905762
MemoryTrain:  epoch  4, batch    10 | loss: 3.1518130Losses:  1.919879674911499 -0.0 0.31953689455986023
MemoryTrain:  epoch  4, batch    11 | loss: 1.9198797Losses:  2.661266326904297 -0.0 0.9685591459274292
MemoryTrain:  epoch  5, batch     0 | loss: 2.6612663Losses:  2.946462631225586 0.2677396237850189 1.0250353813171387
MemoryTrain:  epoch  5, batch     1 | loss: 2.9464626Losses:  3.1150527000427246 0.26554015278816223 0.9509271383285522
MemoryTrain:  epoch  5, batch     2 | loss: 3.1150527Losses:  2.9590554237365723 -0.0 0.9731929302215576
MemoryTrain:  epoch  5, batch     3 | loss: 2.9590554Losses:  2.7285799980163574 0.23518690466880798 0.8683077692985535
MemoryTrain:  epoch  5, batch     4 | loss: 2.7285800Losses:  2.9132814407348633 -0.0 0.9819682836532593
MemoryTrain:  epoch  5, batch     5 | loss: 2.9132814Losses:  2.939979076385498 0.2505137622356415 1.0312436819076538
MemoryTrain:  epoch  5, batch     6 | loss: 2.9399791Losses:  2.4318959712982178 -0.0 1.0128873586654663
MemoryTrain:  epoch  5, batch     7 | loss: 2.4318960Losses:  3.165834903717041 0.25484853982925415 0.8556452989578247
MemoryTrain:  epoch  5, batch     8 | loss: 3.1658349Losses:  3.257188081741333 -0.0 0.9745690822601318
MemoryTrain:  epoch  5, batch     9 | loss: 3.2571881Losses:  3.0551557540893555 0.5292202234268188 0.8579760789871216
MemoryTrain:  epoch  5, batch    10 | loss: 3.0551558Losses:  1.8075599670410156 -0.0 0.24449339509010315
MemoryTrain:  epoch  5, batch    11 | loss: 1.8075600Losses:  3.1671111583709717 0.2797136902809143 1.0087335109710693
MemoryTrain:  epoch  6, batch     0 | loss: 3.1671112Losses:  2.7440385818481445 -0.0 0.8786005973815918
MemoryTrain:  epoch  6, batch     1 | loss: 2.7440386Losses:  2.8014113903045654 0.47348350286483765 0.9020662307739258
MemoryTrain:  epoch  6, batch     2 | loss: 2.8014114Losses:  2.4025025367736816 -0.0 1.0309494733810425
MemoryTrain:  epoch  6, batch     3 | loss: 2.4025025Losses:  3.1188621520996094 -0.0 0.9695533514022827
MemoryTrain:  epoch  6, batch     4 | loss: 3.1188622Losses:  2.9188661575317383 0.23452940583229065 0.9092568755149841
MemoryTrain:  epoch  6, batch     5 | loss: 2.9188662Losses:  2.971238136291504 0.2557455599308014 1.051842212677002
MemoryTrain:  epoch  6, batch     6 | loss: 2.9712381Losses:  3.0266337394714355 0.4722989797592163 0.9132961630821228
MemoryTrain:  epoch  6, batch     7 | loss: 3.0266337Losses:  2.5108118057250977 0.2663120627403259 0.737655520439148
MemoryTrain:  epoch  6, batch     8 | loss: 2.5108118Losses:  4.0923004150390625 1.1828973293304443 0.8574090600013733
MemoryTrain:  epoch  6, batch     9 | loss: 4.0923004Losses:  2.972944974899292 0.25892749428749084 0.9428085684776306
MemoryTrain:  epoch  6, batch    10 | loss: 2.9729450Losses:  2.170711040496826 -0.0 0.32867246866226196
MemoryTrain:  epoch  6, batch    11 | loss: 2.1707110Losses:  2.9457242488861084 0.591423749923706 0.6861063838005066
MemoryTrain:  epoch  7, batch     0 | loss: 2.9457242Losses:  3.384290933609009 0.8271322250366211 1.0080015659332275
MemoryTrain:  epoch  7, batch     1 | loss: 3.3842909Losses:  3.0229251384735107 0.8162563443183899 0.8900351524353027
MemoryTrain:  epoch  7, batch     2 | loss: 3.0229251Losses:  2.766496419906616 -0.0 0.8989908695220947
MemoryTrain:  epoch  7, batch     3 | loss: 2.7664964Losses:  2.3977978229522705 0.22576791048049927 0.8267843127250671
MemoryTrain:  epoch  7, batch     4 | loss: 2.3977978Losses:  2.9296040534973145 0.5375797748565674 0.810120701789856
MemoryTrain:  epoch  7, batch     5 | loss: 2.9296041Losses:  2.9172286987304688 0.2738996148109436 0.930284857749939
MemoryTrain:  epoch  7, batch     6 | loss: 2.9172287Losses:  3.0614044666290283 0.26694104075431824 0.875529944896698
MemoryTrain:  epoch  7, batch     7 | loss: 3.0614045Losses:  2.808720827102661 0.24582011997699738 0.9261406660079956
MemoryTrain:  epoch  7, batch     8 | loss: 2.8087208Losses:  2.881093978881836 0.5140630006790161 0.7880607843399048
MemoryTrain:  epoch  7, batch     9 | loss: 2.8810940Losses:  2.878390312194824 0.2640647292137146 0.9521521925926208
MemoryTrain:  epoch  7, batch    10 | loss: 2.8783903Losses:  1.8507490158081055 -0.0 0.33819371461868286
MemoryTrain:  epoch  7, batch    11 | loss: 1.8507490Losses:  2.689042091369629 0.24785256385803223 0.949370801448822
MemoryTrain:  epoch  8, batch     0 | loss: 2.6890421Losses:  2.3094797134399414 -0.0 0.8711991906166077
MemoryTrain:  epoch  8, batch     1 | loss: 2.3094797Losses:  2.890824556350708 0.5017523765563965 0.9726881384849548
MemoryTrain:  epoch  8, batch     2 | loss: 2.8908246Losses:  2.6733813285827637 0.2886262536048889 0.9926784038543701
MemoryTrain:  epoch  8, batch     3 | loss: 2.6733813Losses:  3.2630066871643066 0.886100172996521 0.8634791374206543
MemoryTrain:  epoch  8, batch     4 | loss: 3.2630067Losses:  2.3048152923583984 -0.0 0.8988277316093445
MemoryTrain:  epoch  8, batch     5 | loss: 2.3048153Losses:  3.320603847503662 0.318548321723938 1.0193275213241577
MemoryTrain:  epoch  8, batch     6 | loss: 3.3206038Losses:  2.5695040225982666 0.25437283515930176 1.0286052227020264
MemoryTrain:  epoch  8, batch     7 | loss: 2.5695040Losses:  2.6809020042419434 0.2541172504425049 1.0337804555892944
MemoryTrain:  epoch  8, batch     8 | loss: 2.6809020Losses:  2.4330525398254395 -0.0 0.9377859234809875
MemoryTrain:  epoch  8, batch     9 | loss: 2.4330525Losses:  3.0470597743988037 0.544535219669342 0.9663261771202087
MemoryTrain:  epoch  8, batch    10 | loss: 3.0470598Losses:  2.671954870223999 -0.0 0.3207624852657318
MemoryTrain:  epoch  8, batch    11 | loss: 2.6719549Losses:  2.792020559310913 0.293416827917099 0.9685088396072388
MemoryTrain:  epoch  9, batch     0 | loss: 2.7920206Losses:  2.696810245513916 0.267741322517395 0.9554225206375122
MemoryTrain:  epoch  9, batch     1 | loss: 2.6968102Losses:  2.3375563621520996 -0.0 1.0147483348846436
MemoryTrain:  epoch  9, batch     2 | loss: 2.3375564Losses:  3.275204658508301 0.8163710236549377 0.7784817814826965
MemoryTrain:  epoch  9, batch     3 | loss: 3.2752047Losses:  3.188601016998291 0.7824646234512329 0.8014798164367676
MemoryTrain:  epoch  9, batch     4 | loss: 3.1886010Losses:  2.5567679405212402 0.31312862038612366 0.8466739654541016
MemoryTrain:  epoch  9, batch     5 | loss: 2.5567679Losses:  2.5192792415618896 0.26552286744117737 0.9961523413658142
MemoryTrain:  epoch  9, batch     6 | loss: 2.5192792Losses:  2.6469569206237793 0.23373821377754211 0.9389601945877075
MemoryTrain:  epoch  9, batch     7 | loss: 2.6469569Losses:  2.640582323074341 0.25139176845550537 0.990969717502594
MemoryTrain:  epoch  9, batch     8 | loss: 2.6405823Losses:  2.4473044872283936 -0.0 1.0105702877044678
MemoryTrain:  epoch  9, batch     9 | loss: 2.4473045Losses:  2.3732006549835205 -0.0 0.9680924415588379
MemoryTrain:  epoch  9, batch    10 | loss: 2.3732007Losses:  1.6225382089614868 -0.0 0.24290810525417328
MemoryTrain:  epoch  9, batch    11 | loss: 1.6225382
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    1 | acc: 56.25%,  total acc: 59.38%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 64.58%   [EVAL] batch:    3 | acc: 43.75%,  total acc: 59.38%   [EVAL] batch:    4 | acc: 50.00%,  total acc: 57.50%   [EVAL] batch:    5 | acc: 62.50%,  total acc: 58.33%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 61.61%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 66.41%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 69.44%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 71.25%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 73.30%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 73.96%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 74.52%   [EVAL] batch:   13 | acc: 56.25%,  total acc: 73.21%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 73.33%   [EVAL] batch:   15 | acc: 81.25%,  total acc: 73.83%   [EVAL] batch:   16 | acc: 81.25%,  total acc: 74.26%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 73.96%   [EVAL] batch:   18 | acc: 87.50%,  total acc: 74.67%   [EVAL] batch:   19 | acc: 68.75%,  total acc: 74.38%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 75.00%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 75.57%   [EVAL] batch:   22 | acc: 81.25%,  total acc: 75.82%   [EVAL] batch:   23 | acc: 81.25%,  total acc: 76.04%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 76.50%   [EVAL] batch:   25 | acc: 75.00%,  total acc: 76.44%   [EVAL] batch:   26 | acc: 68.75%,  total acc: 76.16%   [EVAL] batch:   27 | acc: 56.25%,  total acc: 75.45%   [EVAL] batch:   28 | acc: 75.00%,  total acc: 75.43%   [EVAL] batch:   29 | acc: 50.00%,  total acc: 74.58%   [EVAL] batch:   30 | acc: 50.00%,  total acc: 73.79%   [EVAL] batch:   31 | acc: 68.75%,  total acc: 73.63%   [EVAL] batch:   32 | acc: 43.75%,  total acc: 72.73%   [EVAL] batch:   33 | acc: 50.00%,  total acc: 72.06%   [EVAL] batch:   34 | acc: 56.25%,  total acc: 71.61%   [EVAL] batch:   35 | acc: 62.50%,  total acc: 71.35%   [EVAL] batch:   36 | acc: 62.50%,  total acc: 71.11%   [EVAL] batch:   37 | acc: 62.50%,  total acc: 70.89%   [EVAL] batch:   38 | acc: 50.00%,  total acc: 70.35%   [EVAL] batch:   39 | acc: 62.50%,  total acc: 70.16%   [EVAL] batch:   40 | acc: 75.00%,  total acc: 70.27%   [EVAL] batch:   41 | acc: 68.75%,  total acc: 70.24%   [EVAL] batch:   42 | acc: 75.00%,  total acc: 70.35%   [EVAL] batch:   43 | acc: 50.00%,  total acc: 69.89%   [EVAL] batch:   44 | acc: 37.50%,  total acc: 69.17%   [EVAL] batch:   45 | acc: 31.25%,  total acc: 68.34%   [EVAL] batch:   46 | acc: 18.75%,  total acc: 67.29%   [EVAL] batch:   47 | acc: 43.75%,  total acc: 66.80%   [EVAL] batch:   48 | acc: 31.25%,  total acc: 66.07%   [EVAL] batch:   49 | acc: 37.50%,  total acc: 65.50%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 66.18%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 66.83%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 67.45%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 68.06%   [EVAL] batch:   54 | acc: 100.00%,  total acc: 68.64%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 69.20%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 69.74%   [EVAL] batch:   57 | acc: 75.00%,  total acc: 69.83%   [EVAL] batch:   58 | acc: 81.25%,  total acc: 70.02%   [EVAL] batch:   59 | acc: 87.50%,  total acc: 70.31%   [EVAL] batch:   60 | acc: 68.75%,  total acc: 70.29%   [EVAL] batch:   61 | acc: 81.25%,  total acc: 70.46%   [EVAL] batch:   62 | acc: 43.75%,  total acc: 70.04%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 77.08%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 76.56%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 75.00%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 78.12%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 76.79%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 77.34%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 78.47%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 78.75%   [EVAL] batch:   10 | acc: 75.00%,  total acc: 78.41%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 79.17%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 79.81%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 80.80%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 81.67%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 82.81%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 83.82%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 85.20%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 85.00%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 85.42%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 85.51%   [EVAL] batch:   22 | acc: 87.50%,  total acc: 85.60%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 85.68%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 85.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 86.30%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 86.81%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 87.28%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 87.72%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 88.12%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 88.51%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 88.87%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 89.20%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 89.34%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 89.46%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 89.86%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 90.13%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 90.38%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 90.85%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 91.07%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 91.13%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 91.05%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 90.97%   [EVAL] batch:   45 | acc: 81.25%,  total acc: 90.76%   [EVAL] batch:   46 | acc: 75.00%,  total acc: 90.43%   [EVAL] batch:   47 | acc: 81.25%,  total acc: 90.23%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 90.43%   [EVAL] batch:   49 | acc: 81.25%,  total acc: 90.25%   [EVAL] batch:   50 | acc: 87.50%,  total acc: 90.20%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 90.26%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 90.33%   [EVAL] batch:   53 | acc: 87.50%,  total acc: 90.28%   [EVAL] batch:   54 | acc: 75.00%,  total acc: 90.00%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 90.07%   [EVAL] batch:   56 | acc: 75.00%,  total acc: 89.80%   [EVAL] batch:   57 | acc: 50.00%,  total acc: 89.12%   [EVAL] batch:   58 | acc: 68.75%,  total acc: 88.77%   [EVAL] batch:   59 | acc: 75.00%,  total acc: 88.54%   [EVAL] batch:   60 | acc: 75.00%,  total acc: 88.32%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 88.00%   [EVAL] batch:   62 | acc: 68.75%,  total acc: 87.70%   [EVAL] batch:   63 | acc: 50.00%,  total acc: 87.11%   [EVAL] batch:   64 | acc: 75.00%,  total acc: 86.92%   [EVAL] batch:   65 | acc: 43.75%,  total acc: 86.27%   [EVAL] batch:   66 | acc: 56.25%,  total acc: 85.82%   [EVAL] batch:   67 | acc: 68.75%,  total acc: 85.57%   [EVAL] batch:   68 | acc: 50.00%,  total acc: 85.05%   [EVAL] batch:   69 | acc: 75.00%,  total acc: 84.91%   [EVAL] batch:   70 | acc: 75.00%,  total acc: 84.77%   [EVAL] batch:   71 | acc: 68.75%,  total acc: 84.55%   [EVAL] batch:   72 | acc: 81.25%,  total acc: 84.50%   [EVAL] batch:   73 | acc: 81.25%,  total acc: 84.46%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 84.50%   [EVAL] batch:   75 | acc: 93.75%,  total acc: 84.62%   [EVAL] batch:   76 | acc: 81.25%,  total acc: 84.58%   [EVAL] batch:   77 | acc: 75.00%,  total acc: 84.46%   [EVAL] batch:   78 | acc: 87.50%,  total acc: 84.49%   [EVAL] batch:   79 | acc: 93.75%,  total acc: 84.61%   [EVAL] batch:   80 | acc: 100.00%,  total acc: 84.80%   [EVAL] batch:   81 | acc: 68.75%,  total acc: 84.60%   [EVAL] batch:   82 | acc: 43.75%,  total acc: 84.11%   [EVAL] batch:   83 | acc: 25.00%,  total acc: 83.41%   [EVAL] batch:   84 | acc: 43.75%,  total acc: 82.94%   [EVAL] batch:   85 | acc: 18.75%,  total acc: 82.19%   [EVAL] batch:   86 | acc: 43.75%,  total acc: 81.75%   [EVAL] batch:   87 | acc: 87.50%,  total acc: 81.82%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 82.02%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 82.22%   [EVAL] batch:   90 | acc: 93.75%,  total acc: 82.35%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 82.54%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 82.73%   [EVAL] batch:   93 | acc: 93.75%,  total acc: 82.85%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 83.03%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 83.20%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 83.38%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 83.55%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 83.71%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 83.88%   [EVAL] batch:  100 | acc: 50.00%,  total acc: 83.54%   [EVAL] batch:  101 | acc: 62.50%,  total acc: 83.33%   [EVAL] batch:  102 | acc: 56.25%,  total acc: 83.07%   [EVAL] batch:  103 | acc: 56.25%,  total acc: 82.81%   [EVAL] batch:  104 | acc: 87.50%,  total acc: 82.86%   [EVAL] batch:  105 | acc: 62.50%,  total acc: 82.67%   [EVAL] batch:  106 | acc: 43.75%,  total acc: 82.30%   [EVAL] batch:  107 | acc: 25.00%,  total acc: 81.77%   [EVAL] batch:  108 | acc: 43.75%,  total acc: 81.42%   [EVAL] batch:  109 | acc: 25.00%,  total acc: 80.91%   [EVAL] batch:  110 | acc: 56.25%,  total acc: 80.69%   [EVAL] batch:  111 | acc: 31.25%,  total acc: 80.25%   [EVAL] batch:  112 | acc: 68.75%,  total acc: 80.14%   [EVAL] batch:  113 | acc: 87.50%,  total acc: 80.21%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 80.38%   [EVAL] batch:  115 | acc: 87.50%,  total acc: 80.44%   [EVAL] batch:  116 | acc: 87.50%,  total acc: 80.50%   [EVAL] batch:  117 | acc: 87.50%,  total acc: 80.56%   [EVAL] batch:  118 | acc: 75.00%,  total acc: 80.51%   [EVAL] batch:  119 | acc: 31.25%,  total acc: 80.10%   [EVAL] batch:  120 | acc: 25.00%,  total acc: 79.65%   [EVAL] batch:  121 | acc: 6.25%,  total acc: 79.05%   [EVAL] batch:  122 | acc: 31.25%,  total acc: 78.66%   [EVAL] batch:  123 | acc: 12.50%,  total acc: 78.12%   [EVAL] batch:  124 | acc: 31.25%,  total acc: 77.75%   [EVAL] batch:  125 | acc: 62.50%,  total acc: 77.63%   [EVAL] batch:  126 | acc: 50.00%,  total acc: 77.41%   [EVAL] batch:  127 | acc: 68.75%,  total acc: 77.34%   [EVAL] batch:  128 | acc: 62.50%,  total acc: 77.23%   [EVAL] batch:  129 | acc: 81.25%,  total acc: 77.26%   [EVAL] batch:  130 | acc: 75.00%,  total acc: 77.24%   [EVAL] batch:  131 | acc: 93.75%,  total acc: 77.37%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 77.54%   [EVAL] batch:  133 | acc: 81.25%,  total acc: 77.57%   [EVAL] batch:  134 | acc: 93.75%,  total acc: 77.69%   [EVAL] batch:  135 | acc: 87.50%,  total acc: 77.76%   [EVAL] batch:  136 | acc: 93.75%,  total acc: 77.87%   [EVAL] batch:  137 | acc: 75.00%,  total acc: 77.85%   [EVAL] batch:  138 | acc: 25.00%,  total acc: 77.47%   [EVAL] batch:  139 | acc: 37.50%,  total acc: 77.19%   [EVAL] batch:  140 | acc: 18.75%,  total acc: 76.77%   [EVAL] batch:  141 | acc: 37.50%,  total acc: 76.50%   [EVAL] batch:  142 | acc: 43.75%,  total acc: 76.27%   [EVAL] batch:  143 | acc: 37.50%,  total acc: 76.00%   [EVAL] batch:  144 | acc: 100.00%,  total acc: 76.16%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 76.33%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 76.45%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 76.60%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 76.76%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 76.88%   [EVAL] batch:  150 | acc: 62.50%,  total acc: 76.78%   [EVAL] batch:  151 | acc: 18.75%,  total acc: 76.40%   [EVAL] batch:  152 | acc: 43.75%,  total acc: 76.18%   [EVAL] batch:  153 | acc: 56.25%,  total acc: 76.06%   [EVAL] batch:  154 | acc: 31.25%,  total acc: 75.77%   [EVAL] batch:  155 | acc: 25.00%,  total acc: 75.44%   [EVAL] batch:  156 | acc: 81.25%,  total acc: 75.48%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 75.59%   [EVAL] batch:  158 | acc: 87.50%,  total acc: 75.67%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 75.78%   [EVAL] batch:  160 | acc: 87.50%,  total acc: 75.85%   [EVAL] batch:  161 | acc: 100.00%,  total acc: 76.00%   [EVAL] batch:  162 | acc: 87.50%,  total acc: 76.07%   [EVAL] batch:  163 | acc: 68.75%,  total acc: 76.03%   [EVAL] batch:  164 | acc: 68.75%,  total acc: 75.98%   [EVAL] batch:  165 | acc: 75.00%,  total acc: 75.98%   [EVAL] batch:  166 | acc: 68.75%,  total acc: 75.94%   [EVAL] batch:  167 | acc: 68.75%,  total acc: 75.89%   [EVAL] batch:  168 | acc: 75.00%,  total acc: 75.89%   [EVAL] batch:  169 | acc: 56.25%,  total acc: 75.77%   [EVAL] batch:  170 | acc: 81.25%,  total acc: 75.80%   [EVAL] batch:  171 | acc: 68.75%,  total acc: 75.76%   [EVAL] batch:  172 | acc: 43.75%,  total acc: 75.58%   [EVAL] batch:  173 | acc: 62.50%,  total acc: 75.50%   [EVAL] batch:  174 | acc: 93.75%,  total acc: 75.61%   [EVAL] batch:  175 | acc: 50.00%,  total acc: 75.46%   [EVAL] batch:  176 | acc: 43.75%,  total acc: 75.28%   [EVAL] batch:  177 | acc: 62.50%,  total acc: 75.21%   [EVAL] batch:  178 | acc: 68.75%,  total acc: 75.17%   [EVAL] batch:  179 | acc: 68.75%,  total acc: 75.14%   [EVAL] batch:  180 | acc: 56.25%,  total acc: 75.03%   [EVAL] batch:  181 | acc: 75.00%,  total acc: 75.03%   [EVAL] batch:  182 | acc: 75.00%,  total acc: 75.03%   [EVAL] batch:  183 | acc: 93.75%,  total acc: 75.14%   [EVAL] batch:  184 | acc: 93.75%,  total acc: 75.24%   [EVAL] batch:  185 | acc: 87.50%,  total acc: 75.30%   [EVAL] batch:  186 | acc: 68.75%,  total acc: 75.27%   [EVAL] batch:  187 | acc: 62.50%,  total acc: 75.20%   [EVAL] batch:  188 | acc: 43.75%,  total acc: 75.03%   [EVAL] batch:  189 | acc: 43.75%,  total acc: 74.87%   [EVAL] batch:  190 | acc: 43.75%,  total acc: 74.71%   [EVAL] batch:  191 | acc: 31.25%,  total acc: 74.48%   [EVAL] batch:  192 | acc: 43.75%,  total acc: 74.32%   [EVAL] batch:  193 | acc: 43.75%,  total acc: 74.16%   [EVAL] batch:  194 | acc: 75.00%,  total acc: 74.17%   [EVAL] batch:  195 | acc: 56.25%,  total acc: 74.08%   [EVAL] batch:  196 | acc: 75.00%,  total acc: 74.08%   [EVAL] batch:  197 | acc: 93.75%,  total acc: 74.18%   [EVAL] batch:  198 | acc: 81.25%,  total acc: 74.21%   [EVAL] batch:  199 | acc: 68.75%,  total acc: 74.19%   [EVAL] batch:  200 | acc: 62.50%,  total acc: 74.13%   [EVAL] batch:  201 | acc: 50.00%,  total acc: 74.01%   [EVAL] batch:  202 | acc: 43.75%,  total acc: 73.86%   [EVAL] batch:  203 | acc: 62.50%,  total acc: 73.81%   [EVAL] batch:  204 | acc: 62.50%,  total acc: 73.75%   [EVAL] batch:  205 | acc: 50.00%,  total acc: 73.63%   [EVAL] batch:  206 | acc: 87.50%,  total acc: 73.70%   [EVAL] batch:  207 | acc: 100.00%,  total acc: 73.83%   [EVAL] batch:  208 | acc: 100.00%,  total acc: 73.95%   [EVAL] batch:  209 | acc: 93.75%,  total acc: 74.05%   [EVAL] batch:  210 | acc: 100.00%,  total acc: 74.17%   [EVAL] batch:  211 | acc: 100.00%,  total acc: 74.29%   [EVAL] batch:  212 | acc: 87.50%,  total acc: 74.35%   [EVAL] batch:  213 | acc: 68.75%,  total acc: 74.33%   [EVAL] batch:  214 | acc: 68.75%,  total acc: 74.30%   [EVAL] batch:  215 | acc: 81.25%,  total acc: 74.33%   [EVAL] batch:  216 | acc: 75.00%,  total acc: 74.34%   [EVAL] batch:  217 | acc: 81.25%,  total acc: 74.37%   [EVAL] batch:  218 | acc: 75.00%,  total acc: 74.37%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 74.49%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 74.60%   [EVAL] batch:  221 | acc: 87.50%,  total acc: 74.66%   [EVAL] batch:  222 | acc: 93.75%,  total acc: 74.75%   [EVAL] batch:  223 | acc: 93.75%,  total acc: 74.83%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 74.94%   [EVAL] batch:  225 | acc: 18.75%,  total acc: 74.70%   [EVAL] batch:  226 | acc: 12.50%,  total acc: 74.42%   [EVAL] batch:  227 | acc: 12.50%,  total acc: 74.15%   [EVAL] batch:  228 | acc: 6.25%,  total acc: 73.85%   [EVAL] batch:  229 | acc: 12.50%,  total acc: 73.59%   [EVAL] batch:  230 | acc: 18.75%,  total acc: 73.35%   [EVAL] batch:  231 | acc: 81.25%,  total acc: 73.38%   [EVAL] batch:  232 | acc: 93.75%,  total acc: 73.47%   [EVAL] batch:  233 | acc: 100.00%,  total acc: 73.58%   [EVAL] batch:  234 | acc: 93.75%,  total acc: 73.67%   [EVAL] batch:  235 | acc: 68.75%,  total acc: 73.65%   [EVAL] batch:  236 | acc: 81.25%,  total acc: 73.68%   [EVAL] batch:  237 | acc: 68.75%,  total acc: 73.66%   [EVAL] batch:  238 | acc: 56.25%,  total acc: 73.59%   [EVAL] batch:  239 | acc: 50.00%,  total acc: 73.49%   [EVAL] batch:  240 | acc: 37.50%,  total acc: 73.34%   [EVAL] batch:  241 | acc: 62.50%,  total acc: 73.30%   [EVAL] batch:  242 | acc: 43.75%,  total acc: 73.17%   [EVAL] batch:  243 | acc: 37.50%,  total acc: 73.03%   [EVAL] batch:  244 | acc: 87.50%,  total acc: 73.09%   [EVAL] batch:  245 | acc: 93.75%,  total acc: 73.17%   [EVAL] batch:  246 | acc: 62.50%,  total acc: 73.13%   [EVAL] batch:  247 | acc: 81.25%,  total acc: 73.16%   [EVAL] batch:  248 | acc: 87.50%,  total acc: 73.22%   [EVAL] batch:  249 | acc: 93.75%,  total acc: 73.30%   [EVAL] batch:  250 | acc: 0.00%,  total acc: 73.01%   [EVAL] batch:  251 | acc: 18.75%,  total acc: 72.79%   [EVAL] batch:  252 | acc: 6.25%,  total acc: 72.53%   [EVAL] batch:  253 | acc: 25.00%,  total acc: 72.34%   [EVAL] batch:  254 | acc: 0.00%,  total acc: 72.06%   [EVAL] batch:  255 | acc: 0.00%,  total acc: 71.78%   [EVAL] batch:  256 | acc: 62.50%,  total acc: 71.74%   [EVAL] batch:  257 | acc: 68.75%,  total acc: 71.73%   [EVAL] batch:  258 | acc: 87.50%,  total acc: 71.79%   [EVAL] batch:  259 | acc: 81.25%,  total acc: 71.83%   [EVAL] batch:  260 | acc: 81.25%,  total acc: 71.86%   [EVAL] batch:  261 | acc: 75.00%,  total acc: 71.88%   [EVAL] batch:  262 | acc: 87.50%,  total acc: 71.93%   [EVAL] batch:  263 | acc: 50.00%,  total acc: 71.85%   [EVAL] batch:  264 | acc: 87.50%,  total acc: 71.91%   [EVAL] batch:  265 | acc: 75.00%,  total acc: 71.92%   [EVAL] batch:  266 | acc: 62.50%,  total acc: 71.89%   [EVAL] batch:  267 | acc: 81.25%,  total acc: 71.92%   [EVAL] batch:  268 | acc: 50.00%,  total acc: 71.84%   [EVAL] batch:  269 | acc: 25.00%,  total acc: 71.67%   [EVAL] batch:  270 | acc: 6.25%,  total acc: 71.43%   [EVAL] batch:  271 | acc: 25.00%,  total acc: 71.25%   [EVAL] batch:  272 | acc: 25.00%,  total acc: 71.09%   [EVAL] batch:  273 | acc: 12.50%,  total acc: 70.87%   [EVAL] batch:  274 | acc: 25.00%,  total acc: 70.70%   [EVAL] batch:  275 | acc: 6.25%,  total acc: 70.47%   [EVAL] batch:  276 | acc: 0.00%,  total acc: 70.22%   [EVAL] batch:  277 | acc: 0.00%,  total acc: 69.96%   [EVAL] batch:  278 | acc: 6.25%,  total acc: 69.74%   [EVAL] batch:  279 | acc: 25.00%,  total acc: 69.58%   [EVAL] batch:  280 | acc: 0.00%,  total acc: 69.33%   [EVAL] batch:  281 | acc: 43.75%,  total acc: 69.24%   [EVAL] batch:  282 | acc: 81.25%,  total acc: 69.28%   [EVAL] batch:  283 | acc: 100.00%,  total acc: 69.39%   [EVAL] batch:  284 | acc: 93.75%,  total acc: 69.47%   [EVAL] batch:  285 | acc: 87.50%,  total acc: 69.54%   [EVAL] batch:  286 | acc: 81.25%,  total acc: 69.58%   [EVAL] batch:  287 | acc: 87.50%,  total acc: 69.64%   [EVAL] batch:  288 | acc: 93.75%,  total acc: 69.72%   [EVAL] batch:  289 | acc: 100.00%,  total acc: 69.83%   [EVAL] batch:  290 | acc: 100.00%,  total acc: 69.93%   [EVAL] batch:  291 | acc: 81.25%,  total acc: 69.97%   [EVAL] batch:  292 | acc: 100.00%,  total acc: 70.07%   [EVAL] batch:  293 | acc: 93.75%,  total acc: 70.15%   [EVAL] batch:  294 | acc: 87.50%,  total acc: 70.21%   [EVAL] batch:  295 | acc: 81.25%,  total acc: 70.25%   [EVAL] batch:  296 | acc: 75.00%,  total acc: 70.27%   [EVAL] batch:  297 | acc: 68.75%,  total acc: 70.26%   [EVAL] batch:  298 | acc: 87.50%,  total acc: 70.32%   [EVAL] batch:  299 | acc: 75.00%,  total acc: 70.33%   [EVAL] batch:  300 | acc: 43.75%,  total acc: 70.25%   [EVAL] batch:  301 | acc: 68.75%,  total acc: 70.24%   [EVAL] batch:  302 | acc: 62.50%,  total acc: 70.21%   [EVAL] batch:  303 | acc: 81.25%,  total acc: 70.25%   [EVAL] batch:  304 | acc: 56.25%,  total acc: 70.20%   [EVAL] batch:  305 | acc: 75.00%,  total acc: 70.22%   [EVAL] batch:  306 | acc: 56.25%,  total acc: 70.18%   [EVAL] batch:  307 | acc: 50.00%,  total acc: 70.11%   [EVAL] batch:  308 | acc: 43.75%,  total acc: 70.02%   [EVAL] batch:  309 | acc: 81.25%,  total acc: 70.06%   [EVAL] batch:  310 | acc: 56.25%,  total acc: 70.02%   [EVAL] batch:  311 | acc: 93.75%,  total acc: 70.09%   [EVAL] batch:  312 | acc: 56.25%,  total acc: 70.05%   [EVAL] batch:  313 | acc: 56.25%,  total acc: 70.00%   [EVAL] batch:  314 | acc: 81.25%,  total acc: 70.04%   [EVAL] batch:  315 | acc: 56.25%,  total acc: 70.00%   [EVAL] batch:  316 | acc: 43.75%,  total acc: 69.91%   [EVAL] batch:  317 | acc: 56.25%,  total acc: 69.87%   [EVAL] batch:  318 | acc: 62.50%,  total acc: 69.85%   [EVAL] batch:  319 | acc: 100.00%,  total acc: 69.94%   [EVAL] batch:  320 | acc: 100.00%,  total acc: 70.04%   [EVAL] batch:  321 | acc: 87.50%,  total acc: 70.09%   [EVAL] batch:  322 | acc: 87.50%,  total acc: 70.14%   [EVAL] batch:  323 | acc: 93.75%,  total acc: 70.22%   [EVAL] batch:  324 | acc: 75.00%,  total acc: 70.23%   [EVAL] batch:  325 | acc: 75.00%,  total acc: 70.25%   [EVAL] batch:  326 | acc: 68.75%,  total acc: 70.24%   [EVAL] batch:  327 | acc: 75.00%,  total acc: 70.26%   [EVAL] batch:  328 | acc: 75.00%,  total acc: 70.27%   [EVAL] batch:  329 | acc: 81.25%,  total acc: 70.30%   [EVAL] batch:  330 | acc: 68.75%,  total acc: 70.30%   [EVAL] batch:  331 | acc: 87.50%,  total acc: 70.35%   [EVAL] batch:  332 | acc: 68.75%,  total acc: 70.35%   [EVAL] batch:  333 | acc: 93.75%,  total acc: 70.42%   [EVAL] batch:  334 | acc: 81.25%,  total acc: 70.45%   [EVAL] batch:  335 | acc: 87.50%,  total acc: 70.50%   [EVAL] batch:  336 | acc: 75.00%,  total acc: 70.51%   [EVAL] batch:  337 | acc: 93.75%,  total acc: 70.58%   [EVAL] batch:  338 | acc: 62.50%,  total acc: 70.56%   [EVAL] batch:  339 | acc: 56.25%,  total acc: 70.51%   [EVAL] batch:  340 | acc: 68.75%,  total acc: 70.51%   [EVAL] batch:  341 | acc: 62.50%,  total acc: 70.49%   [EVAL] batch:  342 | acc: 56.25%,  total acc: 70.44%   [EVAL] batch:  343 | acc: 50.00%,  total acc: 70.39%   [EVAL] batch:  344 | acc: 62.50%,  total acc: 70.36%   [EVAL] batch:  345 | acc: 37.50%,  total acc: 70.27%   [EVAL] batch:  346 | acc: 56.25%,  total acc: 70.23%   [EVAL] batch:  347 | acc: 56.25%,  total acc: 70.19%   [EVAL] batch:  348 | acc: 75.00%,  total acc: 70.20%   [EVAL] batch:  349 | acc: 56.25%,  total acc: 70.16%   [EVAL] batch:  350 | acc: 56.25%,  total acc: 70.12%   [EVAL] batch:  351 | acc: 62.50%,  total acc: 70.10%   [EVAL] batch:  352 | acc: 68.75%,  total acc: 70.10%   [EVAL] batch:  353 | acc: 68.75%,  total acc: 70.09%   [EVAL] batch:  354 | acc: 75.00%,  total acc: 70.11%   [EVAL] batch:  355 | acc: 56.25%,  total acc: 70.07%   [EVAL] batch:  356 | acc: 56.25%,  total acc: 70.03%   [EVAL] batch:  357 | acc: 25.00%,  total acc: 69.90%   [EVAL] batch:  358 | acc: 25.00%,  total acc: 69.78%   [EVAL] batch:  359 | acc: 43.75%,  total acc: 69.70%   [EVAL] batch:  360 | acc: 25.00%,  total acc: 69.58%   [EVAL] batch:  361 | acc: 50.00%,  total acc: 69.53%   [EVAL] batch:  362 | acc: 56.25%,  total acc: 69.49%   [EVAL] batch:  363 | acc: 100.00%,  total acc: 69.57%   [EVAL] batch:  364 | acc: 100.00%,  total acc: 69.66%   [EVAL] batch:  365 | acc: 100.00%,  total acc: 69.74%   [EVAL] batch:  366 | acc: 100.00%,  total acc: 69.82%   [EVAL] batch:  367 | acc: 100.00%,  total acc: 69.90%   [EVAL] batch:  368 | acc: 100.00%,  total acc: 69.99%   [EVAL] batch:  369 | acc: 87.50%,  total acc: 70.03%   [EVAL] batch:  370 | acc: 81.25%,  total acc: 70.06%   [EVAL] batch:  371 | acc: 81.25%,  total acc: 70.09%   [EVAL] batch:  372 | acc: 81.25%,  total acc: 70.12%   [EVAL] batch:  373 | acc: 75.00%,  total acc: 70.14%   [EVAL] batch:  374 | acc: 81.25%,  total acc: 70.17%   
cur_acc:  ['0.9524', '0.6994', '0.7639', '0.6895', '0.5843', '0.7004']
his_acc:  ['0.9524', '0.8220', '0.7949', '0.7342', '0.7037', '0.7017']
Clustering into  34  clusters
Clusters:  [23  1 20 33 30 24 20  3  1  8 25 11  3 10 31 17 13  0 18  0  6 10  5  2
  5  4  9  7 22 26 16 14  0 28  7 14  0 11  8 11 30 10  9 32  5 12  9  1
  9  1  0 27  3 30 21  8  6 24 14 15  4 19 13 20 27 16  1 29  2  6]
Losses:  8.579710006713867 2.36368727684021 0.7934961318969727
CurrentTrain: epoch  0, batch     0 | loss: 8.5797100Losses:  8.616802215576172 3.1012885570526123 0.7651621699333191
CurrentTrain: epoch  0, batch     1 | loss: 8.6168022Losses:  9.719773292541504 3.450090169906616 0.6910544037818909
CurrentTrain: epoch  0, batch     2 | loss: 9.7197733Losses:  4.944182872772217 -0.0 0.18750911951065063
CurrentTrain: epoch  0, batch     3 | loss: 4.9441829Losses:  8.506101608276367 3.0402700901031494 0.8026507496833801
CurrentTrain: epoch  1, batch     0 | loss: 8.5061016Losses:  8.631148338317871 2.838050603866577 0.7348244190216064
CurrentTrain: epoch  1, batch     1 | loss: 8.6311483Losses:  7.876895904541016 3.3883872032165527 0.6883604526519775
CurrentTrain: epoch  1, batch     2 | loss: 7.8768959Losses:  2.5653328895568848 -0.0 0.13830333948135376
CurrentTrain: epoch  1, batch     3 | loss: 2.5653329Losses:  9.53524398803711 4.26212215423584 0.664924144744873
CurrentTrain: epoch  2, batch     0 | loss: 9.5352440Losses:  7.575777053833008 3.253384590148926 0.6571816802024841
CurrentTrain: epoch  2, batch     1 | loss: 7.5757771Losses:  8.659185409545898 4.23785924911499 0.7121599912643433
CurrentTrain: epoch  2, batch     2 | loss: 8.6591854Losses:  2.3832738399505615 -0.0 0.13598889112472534
CurrentTrain: epoch  2, batch     3 | loss: 2.3832738Losses:  7.0279388427734375 3.2789812088012695 0.7486695051193237
CurrentTrain: epoch  3, batch     0 | loss: 7.0279388Losses:  6.987914085388184 2.705798387527466 0.722747802734375
CurrentTrain: epoch  3, batch     1 | loss: 6.9879141Losses:  7.223916053771973 2.7708258628845215 0.7962406873703003
CurrentTrain: epoch  3, batch     2 | loss: 7.2239161Losses:  2.4911177158355713 -0.0 0.09354380518198013
CurrentTrain: epoch  3, batch     3 | loss: 2.4911177Losses:  6.688414573669434 2.722370147705078 0.7317102551460266
CurrentTrain: epoch  4, batch     0 | loss: 6.6884146Losses:  9.360149383544922 5.102468967437744 0.6498332023620605
CurrentTrain: epoch  4, batch     1 | loss: 9.3601494Losses:  6.564531326293945 3.2399704456329346 0.6467085480690002
CurrentTrain: epoch  4, batch     2 | loss: 6.5645313Losses:  2.5915746688842773 -0.0 0.1348239779472351
CurrentTrain: epoch  4, batch     3 | loss: 2.5915747Losses:  7.471664905548096 3.7678141593933105 0.6162797212600708
CurrentTrain: epoch  5, batch     0 | loss: 7.4716649Losses:  6.7435431480407715 3.4968128204345703 0.71953284740448
CurrentTrain: epoch  5, batch     1 | loss: 6.7435431Losses:  6.714215278625488 2.5291662216186523 0.6997143030166626
CurrentTrain: epoch  5, batch     2 | loss: 6.7142153Losses:  2.7549264430999756 -0.0 0.09664937853813171
CurrentTrain: epoch  5, batch     3 | loss: 2.7549264Losses:  8.00168514251709 3.8988897800445557 0.5758129954338074
CurrentTrain: epoch  6, batch     0 | loss: 8.0016851Losses:  6.212786674499512 3.346036672592163 0.6514015197753906
CurrentTrain: epoch  6, batch     1 | loss: 6.2127867Losses:  6.793834686279297 3.359377861022949 0.6618346571922302
CurrentTrain: epoch  6, batch     2 | loss: 6.7938347Losses:  2.105330228805542 -0.0 0.11732177436351776
CurrentTrain: epoch  6, batch     3 | loss: 2.1053302Losses:  5.500786304473877 2.3103857040405273 0.7207234501838684
CurrentTrain: epoch  7, batch     0 | loss: 5.5007863Losses:  6.762694358825684 3.3657126426696777 0.6484262943267822
CurrentTrain: epoch  7, batch     1 | loss: 6.7626944Losses:  6.100876808166504 2.624911069869995 0.6982622742652893
CurrentTrain: epoch  7, batch     2 | loss: 6.1008768Losses:  1.874043345451355 -0.0 0.09459615498781204
CurrentTrain: epoch  7, batch     3 | loss: 1.8740433Losses:  7.00852632522583 3.3903298377990723 0.6962388157844543
CurrentTrain: epoch  8, batch     0 | loss: 7.0085263Losses:  5.019285678863525 1.7919235229492188 0.7733266949653625
CurrentTrain: epoch  8, batch     1 | loss: 5.0192857Losses:  5.591989040374756 2.970071792602539 0.634239137172699
CurrentTrain: epoch  8, batch     2 | loss: 5.5919890Losses:  4.222938060760498 -0.0 -0.0
CurrentTrain: epoch  8, batch     3 | loss: 4.2229381Losses:  6.142841339111328 3.178339958190918 0.6128151416778564
CurrentTrain: epoch  9, batch     0 | loss: 6.1428413Losses:  5.461053371429443 2.4731554985046387 0.688765823841095
CurrentTrain: epoch  9, batch     1 | loss: 5.4610534Losses:  6.880790710449219 3.900040864944458 0.6593142151832581
CurrentTrain: epoch  9, batch     2 | loss: 6.8807907Losses:  2.013164520263672 -0.0 0.16940999031066895
CurrentTrain: epoch  9, batch     3 | loss: 2.0131645
#### contains administrative territorial entity ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "contains administrative territorial entity" means direct subdivisions of an administrative territorial entity.
Here is an example:
Relation: contains administrative territorial entity
Context: the cantons of créteil are administrative divisions of the val - de - marne department , île - de - france region , northern france .
Head Entity: île - de - france
Tail Entity: val - de - marne
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "contains administrative territorial entity":
Relation: contains administrative territorial entity  
Context: The provinces of Canada are the primary administrative divisions, with Ontario being one of the largest provinces in the country.  
Head Entity: Canada  
Tail Entity: Ontario  

Relation: contains administrative territorial entity  
Context: The state of California is divided into several counties, including Los Angeles County, which is known for its entertainment industry.  
Head Entity: California  
Tail Entity: Los Angeles County  
#### said to be the same as ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "said to be the same as" means this item is said to be the same as that item, but it's uncertain or disputed.
Here is an example:
Relation: said to be the same as
Context: heinz , is a german given name , a diminutive of heinrich and cognate of the given name henry .
Head Entity: heinrich
Tail Entity: henry
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "said to be the same as":
Relation: said to be the same as  
Context: The two species, Canis lupus and Canis familiaris, are often said to be the same as each other due to their close genetic relationship.  
Head Entity: Canis lupus  
Tail Entity: Canis familiaris  

Relation: said to be the same as  
Context: In some circles, the terms "artificial intelligence" and "machine learning" are said to be the same as each other, although experts argue there are key differences.  
Head Entity: artificial intelligence  
Tail Entity: machine learning  
#### winner ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "winner" means winner of a competition or similar event.
Here is an example:
Relation: winner
Context: the filming was primarily held at pollachi . it fetched pawan kalyan the filmfare award for best actor – telugu award at 59th filmfare awards south .
Head Entity: filmfare award for best actor – telugu
Tail Entity: pawan kalyan
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "winner":
Relation: winner  
Context: in the thrilling finale of the championship, the underdog team triumphed, securing the title and making history as they celebrated their victory at the national sports arena. This remarkable achievement earned them the prestigious trophy for the best team of the year.  
Head Entity: trophy for the best team of the year  
Tail Entity: underdog team  

Relation: winner  
Context: during the annual music awards, the crowd erupted in applause as the talented singer took the stage to accept the coveted award, recognizing her outstanding performance over the past year. This award solidified her status as the leading artist in the industry.  
Head Entity: coveted award  
Tail Entity: talented singer  
#### military rank ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "military rank" means military rank achieved by a person, or military rank associated with a position.
Here is an example:
Relation: military rank
Context: the railroad car they were deported in was attached to the end of the last train out of drancy which also carried drancy commandant ss hauptsturmführer alois brunner and other german military personnel .
Head Entity: alois brunner
Tail Entity: hauptsturmführer
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "military rank":
Relation: military rank  
Context: General John Smith was awarded the title of Major General for his exceptional leadership during the conflict, which significantly contributed to the victory of his battalion.  
Head Entity: John Smith  
Tail Entity: Major General  

Relation: military rank  
Context: During the ceremony, Colonel Jane Doe was recognized for her outstanding service and was promoted to the rank of Brigadier General, a position she had long aspired to achieve.  
Head Entity: Jane Doe  
Tail Entity: Brigadier General  
#### publisher ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "publisher" means organization or person responsible for publishing books, periodicals, printed music, podcasts, games or software.
Here is an example:
Relation: publisher
Context: tokyopop also released a collection of karin short stories under the title : " chibi vampire : airmail " on august 31 , 2010 .
Head Entity: karin
Tail Entity: tokyopop
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "publisher":
Relation: publisher  
Context: Penguin Random House announced the release of a new novel by the acclaimed author John Grisham, set to hit the shelves on June 15, 2021.  
Head Entity: John Grisham  
Tail Entity: Penguin Random House  

Relation: publisher  
Context: The popular science magazine, Scientific American, published an article discussing the latest advancements in renewable energy technologies.  
Head Entity: Scientific American  
Tail Entity: renewable energy technologies  
#### followed by ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "followed by" means immediately following item in a series of which the subject is a part.
Here is an example:
Relation: followed by
Context: " muzai moratorium " and " shōso strip " were released on vinyl in 2008 , as a part of her 10-year anniversary celebrations .
Head Entity: muzai moratorium
Tail Entity: shōso strip
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "followed by":
Relation: followed by  
Context: The first chapter of the book introduces the main character, while the second chapter follows with a detailed backstory.  
Head Entity: first chapter  
Tail Entity: second chapter  

Relation: followed by  
Context: In the concert lineup, the opening act performed a lively set, followed by the headliner who captivated the audience with their greatest hits.  
Head Entity: opening act  
Tail Entity: headliner  
#### work location ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "work location" means location where persons or organisations were actively participating in employment, business or other work.
Here is an example:
Relation: work location
Context: he was also associated with robert wilkinson in producing " londina illustrata " , an illustrated account of ancient buildings in london and westminster in two volumes ( 1819–25 ) .
Head Entity: robert wilkinson
Tail Entity: london
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "work location":
Relation: work location  
Context: The headquarters of the company is situated in the bustling city of San Francisco, where it has been operating since its inception in 2010.  
Head Entity: the company  
Tail Entity: San Francisco  

Relation: work location  
Context: During her tenure at the university, she conducted groundbreaking research in the field of neuroscience, primarily based in the labs at Cambridge.  
Head Entity: she  
Tail Entity: Cambridge  
#### occupation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "occupation" means occupation of a person.
Here is an example:
Relation: occupation
Context: burt kennedy ( september 3 , 1922 – february 15 , 2001 ) was an american screenwriter and director known mainly for directing westerns .
Head Entity: burt kennedy
Tail Entity: director
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "occupation":
Relation: occupation  
Context: Marie Curie (November 7, 1867 – July 4, 1934) was a Polish-born physicist and chemist who conducted pioneering research on radioactivity.  
Head Entity: Marie Curie  
Tail Entity: physicist  

Relation: occupation  
Context: Albert Einstein (March 14, 1879 – April 18, 1955) was a theoretical physicist who developed the theory of relativity, one of the two pillars of modern physics.  
Head Entity: Albert Einstein  
Tail Entity: theoretical physicist  
#### heritage designation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "heritage designation" means heritage designation of a cultural or natural site.
Here is an example:
Relation: heritage designation
Context: altamaha town , bluffton historic district , church of the cross , and rose hill plantation house are listed on the national register of historic places .
Head Entity: altamaha town
Tail Entity: national register of historic places
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "heritage designation":
Relation: heritage designation  
Context: The Great Wall of China is recognized as a UNESCO World Heritage Site due to its historical significance and architectural grandeur.  
Head Entity: Great Wall of China  
Tail Entity: UNESCO World Heritage Site  

Relation: heritage designation  
Context: The ancient city of Petra was designated a UNESCO World Heritage Site in 1985, highlighting its importance as a cultural landmark.  
Head Entity: ancient city of Petra  
Tail Entity: UNESCO World Heritage Site  
#### voice type ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "voice type" means person's voice type.
Here is an example:
Relation: voice type
Context: beverly bower ( september 30 , 1925 – march 24 , 2002 ) was an american operatic soprano who had an active international opera career from the mid-1950s through the early 1970s .
Head Entity: beverly bower
Tail Entity: soprano
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "voice type":
Relation: voice type  
Context: luciano pavarotti ( october 12 , 1935 – september 6 , 2007 ) was an italian tenor who gained worldwide fame for his televised concerts and media appearances.  
Head Entity: luciano pavarotti  
Tail Entity: tenor  

Relation: voice type  
Context: mariah carey ( born march 27 , 1969 ) is an american singer, songwriter, and actress known for her five-octave vocal range and her use of the whistle register.  
Head Entity: mariah carey  
Tail Entity: singer  
Losses:  3.5896713733673096 0.2617014944553375 0.9934840202331543
MemoryTrain:  epoch  0, batch     0 | loss: 3.5896714Losses:  3.975691556930542 -0.0 0.9203659892082214
MemoryTrain:  epoch  0, batch     1 | loss: 3.9756916Losses:  3.522165298461914 -0.0 0.9702985286712646
MemoryTrain:  epoch  0, batch     2 | loss: 3.5221653Losses:  2.9426381587982178 -0.0 0.9182426333427429
MemoryTrain:  epoch  0, batch     3 | loss: 2.9426382Losses:  3.3779797554016113 0.7327663898468018 0.8590095043182373
MemoryTrain:  epoch  0, batch     4 | loss: 3.3779798Losses:  5.409372329711914 0.35581496357917786 1.010467290878296
MemoryTrain:  epoch  0, batch     5 | loss: 5.4093723Losses:  3.7317419052124023 0.23478910326957703 0.9778309464454651
MemoryTrain:  epoch  0, batch     6 | loss: 3.7317419Losses:  3.528140068054199 0.24795927107334137 0.9340176582336426
MemoryTrain:  epoch  0, batch     7 | loss: 3.5281401Losses:  4.5699028968811035 0.26226019859313965 0.8445712924003601
MemoryTrain:  epoch  0, batch     8 | loss: 4.5699029Losses:  4.044914245605469 0.5279208421707153 0.7756277918815613
MemoryTrain:  epoch  0, batch     9 | loss: 4.0449142Losses:  5.562162399291992 1.0955747365951538 0.8486104011535645
MemoryTrain:  epoch  0, batch    10 | loss: 5.5621624Losses:  4.35955286026001 0.2593548893928528 0.9459740519523621
MemoryTrain:  epoch  0, batch    11 | loss: 4.3595529Losses:  4.234130859375 0.2789424657821655 0.9396628141403198
MemoryTrain:  epoch  0, batch    12 | loss: 4.2341309Losses:  2.074167490005493 -0.0 0.135481595993042
MemoryTrain:  epoch  0, batch    13 | loss: 2.0741675Losses:  3.9219532012939453 0.2440188229084015 0.922052800655365
MemoryTrain:  epoch  1, batch     0 | loss: 3.9219532Losses:  3.5915093421936035 0.8036783933639526 0.8633752465248108
MemoryTrain:  epoch  1, batch     1 | loss: 3.5915093Losses:  2.754288673400879 0.26135605573654175 0.9394327998161316
MemoryTrain:  epoch  1, batch     2 | loss: 2.7542887Losses:  3.7230513095855713 -0.0 1.029395341873169
MemoryTrain:  epoch  1, batch     3 | loss: 3.7230513Losses:  3.4958200454711914 -0.0 0.9569336175918579
MemoryTrain:  epoch  1, batch     4 | loss: 3.4958200Losses:  3.4635720252990723 0.2736712098121643 0.8168458938598633
MemoryTrain:  epoch  1, batch     5 | loss: 3.4635720Losses:  4.066488742828369 0.46532344818115234 0.9696334600448608
MemoryTrain:  epoch  1, batch     6 | loss: 4.0664887Losses:  3.5490522384643555 0.7778505086898804 0.9181706309318542
MemoryTrain:  epoch  1, batch     7 | loss: 3.5490522Losses:  4.61788272857666 -0.0 0.9713780879974365
MemoryTrain:  epoch  1, batch     8 | loss: 4.6178827Losses:  3.340233325958252 0.5182857513427734 0.9667410254478455
MemoryTrain:  epoch  1, batch     9 | loss: 3.3402333Losses:  3.5479421615600586 0.4890212118625641 0.9166003465652466
MemoryTrain:  epoch  1, batch    10 | loss: 3.5479422Losses:  3.768083095550537 0.2313648760318756 0.9138702154159546
MemoryTrain:  epoch  1, batch    11 | loss: 3.7680831Losses:  3.3275394439697266 0.2636581063270569 0.8878449201583862
MemoryTrain:  epoch  1, batch    12 | loss: 3.3275394Losses:  1.7193019390106201 -0.0 0.1874561905860901
MemoryTrain:  epoch  1, batch    13 | loss: 1.7193019Losses:  3.537177562713623 0.2687740623950958 0.9053525328636169
MemoryTrain:  epoch  2, batch     0 | loss: 3.5371776Losses:  2.945387125015259 0.5697456002235413 0.792881429195404
MemoryTrain:  epoch  2, batch     1 | loss: 2.9453871Losses:  3.4215431213378906 0.2396840900182724 0.8382067084312439
MemoryTrain:  epoch  2, batch     2 | loss: 3.4215431Losses:  3.8858532905578613 0.7801655530929565 1.0105738639831543
MemoryTrain:  epoch  2, batch     3 | loss: 3.8858533Losses:  3.3024191856384277 -0.0 1.0484987497329712
MemoryTrain:  epoch  2, batch     4 | loss: 3.3024192Losses:  4.117008209228516 0.8274303674697876 0.8777016401290894
MemoryTrain:  epoch  2, batch     5 | loss: 4.1170082Losses:  4.140445232391357 0.27992379665374756 0.9124452471733093
MemoryTrain:  epoch  2, batch     6 | loss: 4.1404452Losses:  3.000129461288452 0.30016642808914185 0.9656765460968018
MemoryTrain:  epoch  2, batch     7 | loss: 3.0001295Losses:  3.6025381088256836 0.8054637312889099 0.9833199977874756
MemoryTrain:  epoch  2, batch     8 | loss: 3.6025381Losses:  3.093308687210083 0.5802661180496216 0.9734792113304138
MemoryTrain:  epoch  2, batch     9 | loss: 3.0933087Losses:  2.911956548690796 -0.0 1.0118775367736816
MemoryTrain:  epoch  2, batch    10 | loss: 2.9119565Losses:  3.2286863327026367 0.5039758682250977 0.9862075448036194
MemoryTrain:  epoch  2, batch    11 | loss: 3.2286863Losses:  3.153902530670166 0.24813522398471832 0.8442036509513855
MemoryTrain:  epoch  2, batch    12 | loss: 3.1539025Losses:  3.7513322830200195 -0.0 0.12417296320199966
MemoryTrain:  epoch  2, batch    13 | loss: 3.7513323Losses:  3.2678351402282715 0.23509439826011658 0.8820194005966187
MemoryTrain:  epoch  3, batch     0 | loss: 3.2678351Losses:  2.384794235229492 -0.0 1.0094165802001953
MemoryTrain:  epoch  3, batch     1 | loss: 2.3847942Losses:  3.032406806945801 0.5414159297943115 0.9567405581474304
MemoryTrain:  epoch  3, batch     2 | loss: 3.0324068Losses:  2.9457650184631348 0.2452663630247116 1.0106714963912964
MemoryTrain:  epoch  3, batch     3 | loss: 2.9457650Losses:  3.044888496398926 -0.0 1.0027350187301636
MemoryTrain:  epoch  3, batch     4 | loss: 3.0448885Losses:  4.140758514404297 0.6368359327316284 0.8541549444198608
MemoryTrain:  epoch  3, batch     5 | loss: 4.1407585Losses:  3.053565502166748 -0.0 1.0129766464233398
MemoryTrain:  epoch  3, batch     6 | loss: 3.0535655Losses:  2.5979323387145996 0.2442566454410553 0.8622604012489319
MemoryTrain:  epoch  3, batch     7 | loss: 2.5979323Losses:  3.023850440979004 0.2846004068851471 0.9271494150161743
MemoryTrain:  epoch  3, batch     8 | loss: 3.0238504Losses:  2.6823675632476807 0.2350119948387146 0.921501874923706
MemoryTrain:  epoch  3, batch     9 | loss: 2.6823676Losses:  3.2103371620178223 0.2937362790107727 1.0509588718414307
MemoryTrain:  epoch  3, batch    10 | loss: 3.2103372Losses:  3.5084428787231445 0.5453372001647949 0.9229841232299805
MemoryTrain:  epoch  3, batch    11 | loss: 3.5084429Losses:  3.138279914855957 0.5072562098503113 0.845978856086731
MemoryTrain:  epoch  3, batch    12 | loss: 3.1382799Losses:  1.7686052322387695 -0.0 0.14576095342636108
MemoryTrain:  epoch  3, batch    13 | loss: 1.7686052Losses:  2.7970664501190186 -0.0 1.1497306823730469
MemoryTrain:  epoch  4, batch     0 | loss: 2.7970665Losses:  2.712031126022339 0.2861255705356598 0.972793459892273
MemoryTrain:  epoch  4, batch     1 | loss: 2.7120311Losses:  2.9154136180877686 0.4952288866043091 0.9506646990776062
MemoryTrain:  epoch  4, batch     2 | loss: 2.9154136Losses:  2.413724899291992 -0.0 1.0430493354797363
MemoryTrain:  epoch  4, batch     3 | loss: 2.4137249Losses:  2.4080793857574463 -0.0 1.043352484703064
MemoryTrain:  epoch  4, batch     4 | loss: 2.4080794Losses:  2.635338306427002 0.25213009119033813 0.9734364748001099
MemoryTrain:  epoch  4, batch     5 | loss: 2.6353383Losses:  2.9585609436035156 0.24820184707641602 0.9279028177261353
MemoryTrain:  epoch  4, batch     6 | loss: 2.9585609Losses:  3.4733500480651855 0.8184592723846436 0.9169120192527771
MemoryTrain:  epoch  4, batch     7 | loss: 3.4733500Losses:  2.4533963203430176 -0.0 0.9538238644599915
MemoryTrain:  epoch  4, batch     8 | loss: 2.4533963Losses:  2.78558611869812 -0.0 0.9255468845367432
MemoryTrain:  epoch  4, batch     9 | loss: 2.7855861Losses:  2.849510669708252 0.2805544137954712 0.8706802129745483
MemoryTrain:  epoch  4, batch    10 | loss: 2.8495107Losses:  2.8282673358917236 -0.0 0.8706269860267639
MemoryTrain:  epoch  4, batch    11 | loss: 2.8282673Losses:  2.904204845428467 -0.0 1.045935034751892
MemoryTrain:  epoch  4, batch    12 | loss: 2.9042048Losses:  4.492978572845459 -0.0 0.1665603667497635
MemoryTrain:  epoch  4, batch    13 | loss: 4.4929786Losses:  3.3261070251464844 0.8297348618507385 0.9239984750747681
MemoryTrain:  epoch  5, batch     0 | loss: 3.3261070Losses:  2.5316450595855713 0.261243999004364 0.872938334941864
MemoryTrain:  epoch  5, batch     1 | loss: 2.5316451Losses:  2.599946975708008 -0.0 0.9680238962173462
MemoryTrain:  epoch  5, batch     2 | loss: 2.5999470Losses:  3.237335681915283 0.5139816403388977 0.9353126287460327
MemoryTrain:  epoch  5, batch     3 | loss: 3.2373357Losses:  2.919462203979492 0.46986743807792664 1.0264909267425537
MemoryTrain:  epoch  5, batch     4 | loss: 2.9194622Losses:  2.6186347007751465 0.25919780135154724 0.9221144914627075
MemoryTrain:  epoch  5, batch     5 | loss: 2.6186347Losses:  3.62432599067688 0.679243266582489 0.9674477577209473
MemoryTrain:  epoch  5, batch     6 | loss: 3.6243260Losses:  2.74328351020813 -0.0 0.9915513396263123
MemoryTrain:  epoch  5, batch     7 | loss: 2.7432835Losses:  2.5513057708740234 0.2263040989637375 0.956967830657959
MemoryTrain:  epoch  5, batch     8 | loss: 2.5513058Losses:  3.1958768367767334 -0.0 0.9443580508232117
MemoryTrain:  epoch  5, batch     9 | loss: 3.1958768Losses:  2.5177173614501953 0.25407564640045166 0.8703063726425171
MemoryTrain:  epoch  5, batch    10 | loss: 2.5177174Losses:  2.7281241416931152 0.27919942140579224 0.9754589796066284
MemoryTrain:  epoch  5, batch    11 | loss: 2.7281241Losses:  3.387542486190796 0.25730717182159424 0.9801281094551086
MemoryTrain:  epoch  5, batch    12 | loss: 3.3875425Losses:  3.0259737968444824 -0.0 0.1335461437702179
MemoryTrain:  epoch  5, batch    13 | loss: 3.0259738Losses:  3.0219478607177734 0.25999101996421814 1.0021594762802124
MemoryTrain:  epoch  6, batch     0 | loss: 3.0219479Losses:  3.1609444618225098 0.5557804107666016 1.0233136415481567
MemoryTrain:  epoch  6, batch     1 | loss: 3.1609445Losses:  2.8419370651245117 0.48368528485298157 0.8935630321502686
MemoryTrain:  epoch  6, batch     2 | loss: 2.8419371Losses:  3.858548164367676 0.9377492666244507 0.9349039793014526
MemoryTrain:  epoch  6, batch     3 | loss: 3.8585482Losses:  4.239151954650879 0.8501797914505005 0.8213363885879517
MemoryTrain:  epoch  6, batch     4 | loss: 4.2391520Losses:  2.608553171157837 0.2548658847808838 1.0115411281585693
MemoryTrain:  epoch  6, batch     5 | loss: 2.6085532Losses:  2.554323673248291 -0.0 1.0309269428253174
MemoryTrain:  epoch  6, batch     6 | loss: 2.5543237Losses:  2.274212598800659 -0.0 0.9112631678581238
MemoryTrain:  epoch  6, batch     7 | loss: 2.2742126Losses:  2.688284397125244 0.24408861994743347 0.8979190587997437
MemoryTrain:  epoch  6, batch     8 | loss: 2.6882844Losses:  2.380362033843994 -0.0 0.9634398221969604
MemoryTrain:  epoch  6, batch     9 | loss: 2.3803620Losses:  2.3506388664245605 -0.0 0.9441901445388794
MemoryTrain:  epoch  6, batch    10 | loss: 2.3506389Losses:  2.6990509033203125 -0.0 1.0720397233963013
MemoryTrain:  epoch  6, batch    11 | loss: 2.6990509Losses:  2.491159439086914 0.2557602822780609 0.9561864137649536
MemoryTrain:  epoch  6, batch    12 | loss: 2.4911594Losses:  2.5324130058288574 -0.0 0.12712615728378296
MemoryTrain:  epoch  6, batch    13 | loss: 2.5324130Losses:  2.853271007537842 0.2680503726005554 1.024656057357788
MemoryTrain:  epoch  7, batch     0 | loss: 2.8532710Losses:  2.701036214828491 0.2923792600631714 0.9933174252510071
MemoryTrain:  epoch  7, batch     1 | loss: 2.7010362Losses:  2.677361011505127 0.5393891334533691 0.8673361539840698
MemoryTrain:  epoch  7, batch     2 | loss: 2.6773610Losses:  2.484449863433838 0.2615175247192383 0.9209603071212769
MemoryTrain:  epoch  7, batch     3 | loss: 2.4844499Losses:  3.0252881050109863 0.25615423917770386 0.9084919691085815
MemoryTrain:  epoch  7, batch     4 | loss: 3.0252881Losses:  3.1991639137268066 0.5501537322998047 0.9298223853111267
MemoryTrain:  epoch  7, batch     5 | loss: 3.1991639Losses:  2.7531747817993164 0.268195241689682 1.0812369585037231
MemoryTrain:  epoch  7, batch     6 | loss: 2.7531748Losses:  2.7685604095458984 0.2501455843448639 0.9041138887405396
MemoryTrain:  epoch  7, batch     7 | loss: 2.7685604Losses:  2.6721084117889404 0.2652605175971985 1.0019972324371338
MemoryTrain:  epoch  7, batch     8 | loss: 2.6721084Losses:  2.44520902633667 0.2681008577346802 0.9008020758628845
MemoryTrain:  epoch  7, batch     9 | loss: 2.4452090Losses:  2.5631399154663086 0.2627575993537903 0.9768760800361633
MemoryTrain:  epoch  7, batch    10 | loss: 2.5631399Losses:  2.727996349334717 0.25096622109413147 0.8444156646728516
MemoryTrain:  epoch  7, batch    11 | loss: 2.7279963Losses:  2.3870487213134766 -0.0 0.9859012365341187
MemoryTrain:  epoch  7, batch    12 | loss: 2.3870487Losses:  1.4905235767364502 -0.0 0.1109742522239685
MemoryTrain:  epoch  7, batch    13 | loss: 1.4905236Losses:  3.4824295043945312 0.6456570625305176 0.9413873553276062
MemoryTrain:  epoch  8, batch     0 | loss: 3.4824295Losses:  2.4996798038482666 0.25918447971343994 1.0027832984924316
MemoryTrain:  epoch  8, batch     1 | loss: 2.4996798Losses:  3.2883620262145996 0.8796873092651367 0.970491349697113
MemoryTrain:  epoch  8, batch     2 | loss: 3.2883620Losses:  2.68412184715271 0.4981003403663635 0.9046935439109802
MemoryTrain:  epoch  8, batch     3 | loss: 2.6841218Losses:  2.5623927116394043 0.2606967091560364 0.8552987575531006
MemoryTrain:  epoch  8, batch     4 | loss: 2.5623927Losses:  3.4594156742095947 0.9098353385925293 0.9271378517150879
MemoryTrain:  epoch  8, batch     5 | loss: 3.4594157Losses:  2.4262290000915527 -0.0 1.0287268161773682
MemoryTrain:  epoch  8, batch     6 | loss: 2.4262290Losses:  2.7852959632873535 0.5309996604919434 0.8553650975227356
MemoryTrain:  epoch  8, batch     7 | loss: 2.7852960Losses:  3.1367011070251465 0.28183498978614807 1.0460652112960815
MemoryTrain:  epoch  8, batch     8 | loss: 3.1367011Losses:  2.495054244995117 0.2473781406879425 0.96653151512146
MemoryTrain:  epoch  8, batch     9 | loss: 2.4950542Losses:  2.645132303237915 0.262071430683136 1.0190479755401611
MemoryTrain:  epoch  8, batch    10 | loss: 2.6451323Losses:  3.102578639984131 0.7974594831466675 0.8594263792037964
MemoryTrain:  epoch  8, batch    11 | loss: 3.1025786Losses:  2.267157793045044 -0.0 0.912881076335907
MemoryTrain:  epoch  8, batch    12 | loss: 2.2671578Losses:  1.443989634513855 -0.0 0.09013022482395172
MemoryTrain:  epoch  8, batch    13 | loss: 1.4439896Losses:  2.4020724296569824 -0.0 0.9109172224998474
MemoryTrain:  epoch  9, batch     0 | loss: 2.4020724Losses:  2.693746566772461 0.2723017930984497 0.9612081050872803
MemoryTrain:  epoch  9, batch     1 | loss: 2.6937466Losses:  2.7218544483184814 0.5529552102088928 0.8109964728355408
MemoryTrain:  epoch  9, batch     2 | loss: 2.7218544Losses:  3.025099039077759 0.9827631711959839 0.800998866558075
MemoryTrain:  epoch  9, batch     3 | loss: 3.0250990Losses:  2.517206907272339 -0.0 0.9989939332008362
MemoryTrain:  epoch  9, batch     4 | loss: 2.5172069Losses:  2.8136844635009766 0.2627114951610565 1.0322096347808838
MemoryTrain:  epoch  9, batch     5 | loss: 2.8136845Losses:  2.285024642944336 -0.0 1.0125898122787476
MemoryTrain:  epoch  9, batch     6 | loss: 2.2850246Losses:  2.5771851539611816 -0.0 0.8467883467674255
MemoryTrain:  epoch  9, batch     7 | loss: 2.5771852Losses:  2.867351531982422 0.7626672983169556 0.8540037870407104
MemoryTrain:  epoch  9, batch     8 | loss: 2.8673515Losses:  3.0400476455688477 0.8018926978111267 0.8802930116653442
MemoryTrain:  epoch  9, batch     9 | loss: 3.0400476Losses:  2.7356154918670654 0.48508304357528687 1.0149192810058594
MemoryTrain:  epoch  9, batch    10 | loss: 2.7356155Losses:  2.3600757122039795 -0.0 0.8956460952758789
MemoryTrain:  epoch  9, batch    11 | loss: 2.3600757Losses:  2.9303462505340576 0.7521053552627563 0.9242429137229919
MemoryTrain:  epoch  9, batch    12 | loss: 2.9303463Losses:  1.3306701183319092 -0.0 -0.0
MemoryTrain:  epoch  9, batch    13 | loss: 1.3306701
[EVAL] batch:    0 | acc: 37.50%,  total acc: 37.50%   [EVAL] batch:    1 | acc: 62.50%,  total acc: 50.00%   [EVAL] batch:    2 | acc: 43.75%,  total acc: 47.92%   [EVAL] batch:    3 | acc: 56.25%,  total acc: 50.00%   [EVAL] batch:    4 | acc: 56.25%,  total acc: 51.25%   [EVAL] batch:    5 | acc: 50.00%,  total acc: 51.04%   [EVAL] batch:    6 | acc: 50.00%,  total acc: 50.89%   [EVAL] batch:    7 | acc: 68.75%,  total acc: 53.12%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 54.86%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 57.50%   [EVAL] batch:   10 | acc: 50.00%,  total acc: 56.82%   [EVAL] batch:   11 | acc: 62.50%,  total acc: 57.29%   [EVAL] batch:   12 | acc: 62.50%,  total acc: 57.69%   [EVAL] batch:   13 | acc: 62.50%,  total acc: 58.04%   [EVAL] batch:   14 | acc: 50.00%,  total acc: 57.50%   [EVAL] batch:   15 | acc: 25.00%,  total acc: 55.47%   [EVAL] batch:   16 | acc: 50.00%,  total acc: 55.15%   [EVAL] batch:   17 | acc: 56.25%,  total acc: 55.21%   [EVAL] batch:   18 | acc: 87.50%,  total acc: 56.91%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 58.75%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 60.71%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 62.50%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 64.13%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 65.36%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 66.75%   [EVAL] batch:   25 | acc: 56.25%,  total acc: 66.35%   [EVAL] batch:   26 | acc: 37.50%,  total acc: 65.28%   [EVAL] batch:   27 | acc: 68.75%,  total acc: 65.40%   [EVAL] batch:   28 | acc: 75.00%,  total acc: 65.73%   [EVAL] batch:   29 | acc: 37.50%,  total acc: 64.79%   [EVAL] batch:   30 | acc: 43.75%,  total acc: 64.11%   [EVAL] batch:   31 | acc: 50.00%,  total acc: 63.67%   [EVAL] batch:   32 | acc: 43.75%,  total acc: 63.07%   [EVAL] batch:   33 | acc: 12.50%,  total acc: 61.58%   [EVAL] batch:   34 | acc: 18.75%,  total acc: 60.36%   [EVAL] batch:   35 | acc: 12.50%,  total acc: 59.03%   [EVAL] batch:   36 | acc: 6.25%,  total acc: 57.60%   [EVAL] batch:   37 | acc: 18.75%,  total acc: 56.58%   [EVAL] batch:   38 | acc: 18.75%,  total acc: 55.61%   [EVAL] batch:   39 | acc: 18.75%,  total acc: 54.69%   [EVAL] batch:   40 | acc: 18.75%,  total acc: 53.81%   [EVAL] batch:   41 | acc: 12.50%,  total acc: 52.83%   [EVAL] batch:   42 | acc: 31.25%,  total acc: 52.33%   [EVAL] batch:   43 | acc: 25.00%,  total acc: 51.70%   [EVAL] batch:   44 | acc: 68.75%,  total acc: 52.08%   [EVAL] batch:   45 | acc: 56.25%,  total acc: 52.17%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 52.79%   [EVAL] batch:   47 | acc: 62.50%,  total acc: 52.99%   [EVAL] batch:   48 | acc: 62.50%,  total acc: 53.19%   [EVAL] batch:   49 | acc: 75.00%,  total acc: 53.62%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 54.53%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 55.41%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 56.25%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 57.06%   [EVAL] batch:   54 | acc: 100.00%,  total acc: 57.84%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 58.59%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 59.32%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 59.81%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 60.38%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 61.04%   [EVAL] batch:   60 | acc: 93.75%,  total acc: 61.58%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 62.20%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 62.00%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 79.69%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 77.50%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 79.17%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 77.68%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 78.91%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 79.17%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 78.75%   [EVAL] batch:   10 | acc: 75.00%,  total acc: 78.41%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 78.12%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 78.37%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 79.46%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 80.42%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 81.64%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 82.72%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 83.33%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 84.21%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 83.75%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 83.93%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 84.09%   [EVAL] batch:   22 | acc: 87.50%,  total acc: 84.24%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:   24 | acc: 75.00%,  total acc: 84.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 84.62%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 85.19%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 85.71%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 85.99%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 86.46%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 86.90%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 87.30%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 87.69%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 88.05%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 88.21%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 88.37%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 88.68%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 88.98%   [EVAL] batch:   38 | acc: 93.75%,  total acc: 89.10%   [EVAL] batch:   39 | acc: 93.75%,  total acc: 89.22%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 89.33%   [EVAL] batch:   41 | acc: 81.25%,  total acc: 89.14%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 89.24%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 89.06%   [EVAL] batch:   44 | acc: 81.25%,  total acc: 88.89%   [EVAL] batch:   45 | acc: 87.50%,  total acc: 88.86%   [EVAL] batch:   46 | acc: 75.00%,  total acc: 88.56%   [EVAL] batch:   47 | acc: 68.75%,  total acc: 88.15%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 88.39%   [EVAL] batch:   49 | acc: 81.25%,  total acc: 88.25%   [EVAL] batch:   50 | acc: 81.25%,  total acc: 88.11%   [EVAL] batch:   51 | acc: 75.00%,  total acc: 87.86%   [EVAL] batch:   52 | acc: 87.50%,  total acc: 87.85%   [EVAL] batch:   53 | acc: 62.50%,  total acc: 87.38%   [EVAL] batch:   54 | acc: 56.25%,  total acc: 86.82%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 86.83%   [EVAL] batch:   56 | acc: 62.50%,  total acc: 86.40%   [EVAL] batch:   57 | acc: 43.75%,  total acc: 85.67%   [EVAL] batch:   58 | acc: 62.50%,  total acc: 85.28%   [EVAL] batch:   59 | acc: 75.00%,  total acc: 85.10%   [EVAL] batch:   60 | acc: 68.75%,  total acc: 84.84%   [EVAL] batch:   61 | acc: 62.50%,  total acc: 84.48%   [EVAL] batch:   62 | acc: 56.25%,  total acc: 84.03%   [EVAL] batch:   63 | acc: 37.50%,  total acc: 83.30%   [EVAL] batch:   64 | acc: 68.75%,  total acc: 83.08%   [EVAL] batch:   65 | acc: 43.75%,  total acc: 82.48%   [EVAL] batch:   66 | acc: 50.00%,  total acc: 82.00%   [EVAL] batch:   67 | acc: 56.25%,  total acc: 81.62%   [EVAL] batch:   68 | acc: 43.75%,  total acc: 81.07%   [EVAL] batch:   69 | acc: 81.25%,  total acc: 81.07%   [EVAL] batch:   70 | acc: 81.25%,  total acc: 81.07%   [EVAL] batch:   71 | acc: 68.75%,  total acc: 80.90%   [EVAL] batch:   72 | acc: 81.25%,  total acc: 80.91%   [EVAL] batch:   73 | acc: 75.00%,  total acc: 80.83%   [EVAL] batch:   74 | acc: 81.25%,  total acc: 80.83%   [EVAL] batch:   75 | acc: 87.50%,  total acc: 80.92%   [EVAL] batch:   76 | acc: 81.25%,  total acc: 80.93%   [EVAL] batch:   77 | acc: 68.75%,  total acc: 80.77%   [EVAL] batch:   78 | acc: 75.00%,  total acc: 80.70%   [EVAL] batch:   79 | acc: 93.75%,  total acc: 80.86%   [EVAL] batch:   80 | acc: 93.75%,  total acc: 81.02%   [EVAL] batch:   81 | acc: 68.75%,  total acc: 80.87%   [EVAL] batch:   82 | acc: 37.50%,  total acc: 80.35%   [EVAL] batch:   83 | acc: 31.25%,  total acc: 79.76%   [EVAL] batch:   84 | acc: 43.75%,  total acc: 79.34%   [EVAL] batch:   85 | acc: 18.75%,  total acc: 78.63%   [EVAL] batch:   86 | acc: 50.00%,  total acc: 78.30%   [EVAL] batch:   87 | acc: 87.50%,  total acc: 78.41%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 78.65%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 78.89%   [EVAL] batch:   90 | acc: 93.75%,  total acc: 79.05%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 79.28%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 79.50%   [EVAL] batch:   93 | acc: 93.75%,  total acc: 79.65%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 79.87%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 80.08%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 80.28%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 80.48%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 80.68%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 80.88%   [EVAL] batch:  100 | acc: 56.25%,  total acc: 80.63%   [EVAL] batch:  101 | acc: 62.50%,  total acc: 80.45%   [EVAL] batch:  102 | acc: 62.50%,  total acc: 80.28%   [EVAL] batch:  103 | acc: 56.25%,  total acc: 80.05%   [EVAL] batch:  104 | acc: 87.50%,  total acc: 80.12%   [EVAL] batch:  105 | acc: 62.50%,  total acc: 79.95%   [EVAL] batch:  106 | acc: 37.50%,  total acc: 79.56%   [EVAL] batch:  107 | acc: 18.75%,  total acc: 78.99%   [EVAL] batch:  108 | acc: 43.75%,  total acc: 78.67%   [EVAL] batch:  109 | acc: 18.75%,  total acc: 78.12%   [EVAL] batch:  110 | acc: 43.75%,  total acc: 77.82%   [EVAL] batch:  111 | acc: 12.50%,  total acc: 77.23%   [EVAL] batch:  112 | acc: 62.50%,  total acc: 77.10%   [EVAL] batch:  113 | acc: 87.50%,  total acc: 77.19%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 77.39%   [EVAL] batch:  115 | acc: 87.50%,  total acc: 77.48%   [EVAL] batch:  116 | acc: 87.50%,  total acc: 77.56%   [EVAL] batch:  117 | acc: 87.50%,  total acc: 77.65%   [EVAL] batch:  118 | acc: 75.00%,  total acc: 77.63%   [EVAL] batch:  119 | acc: 31.25%,  total acc: 77.24%   [EVAL] batch:  120 | acc: 25.00%,  total acc: 76.81%   [EVAL] batch:  121 | acc: 12.50%,  total acc: 76.28%   [EVAL] batch:  122 | acc: 31.25%,  total acc: 75.91%   [EVAL] batch:  123 | acc: 12.50%,  total acc: 75.40%   [EVAL] batch:  124 | acc: 31.25%,  total acc: 75.05%   [EVAL] batch:  125 | acc: 62.50%,  total acc: 74.95%   [EVAL] batch:  126 | acc: 50.00%,  total acc: 74.75%   [EVAL] batch:  127 | acc: 56.25%,  total acc: 74.61%   [EVAL] batch:  128 | acc: 62.50%,  total acc: 74.52%   [EVAL] batch:  129 | acc: 75.00%,  total acc: 74.52%   [EVAL] batch:  130 | acc: 56.25%,  total acc: 74.38%   [EVAL] batch:  131 | acc: 81.25%,  total acc: 74.43%   [EVAL] batch:  132 | acc: 87.50%,  total acc: 74.53%   [EVAL] batch:  133 | acc: 75.00%,  total acc: 74.53%   [EVAL] batch:  134 | acc: 68.75%,  total acc: 74.49%   [EVAL] batch:  135 | acc: 87.50%,  total acc: 74.59%   [EVAL] batch:  136 | acc: 81.25%,  total acc: 74.64%   [EVAL] batch:  137 | acc: 56.25%,  total acc: 74.50%   [EVAL] batch:  138 | acc: 18.75%,  total acc: 74.10%   [EVAL] batch:  139 | acc: 31.25%,  total acc: 73.79%   [EVAL] batch:  140 | acc: 18.75%,  total acc: 73.40%   [EVAL] batch:  141 | acc: 31.25%,  total acc: 73.11%   [EVAL] batch:  142 | acc: 37.50%,  total acc: 72.86%   [EVAL] batch:  143 | acc: 31.25%,  total acc: 72.57%   [EVAL] batch:  144 | acc: 93.75%,  total acc: 72.72%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 72.90%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 73.04%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 73.23%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 73.41%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 73.54%   [EVAL] batch:  150 | acc: 37.50%,  total acc: 73.30%   [EVAL] batch:  151 | acc: 12.50%,  total acc: 72.90%   [EVAL] batch:  152 | acc: 25.00%,  total acc: 72.59%   [EVAL] batch:  153 | acc: 18.75%,  total acc: 72.24%   [EVAL] batch:  154 | acc: 31.25%,  total acc: 71.98%   [EVAL] batch:  155 | acc: 12.50%,  total acc: 71.59%   [EVAL] batch:  156 | acc: 75.00%,  total acc: 71.62%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 71.76%   [EVAL] batch:  158 | acc: 87.50%,  total acc: 71.86%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 71.99%   [EVAL] batch:  160 | acc: 87.50%,  total acc: 72.09%   [EVAL] batch:  161 | acc: 100.00%,  total acc: 72.26%   [EVAL] batch:  162 | acc: 93.75%,  total acc: 72.39%   [EVAL] batch:  163 | acc: 62.50%,  total acc: 72.33%   [EVAL] batch:  164 | acc: 56.25%,  total acc: 72.23%   [EVAL] batch:  165 | acc: 75.00%,  total acc: 72.25%   [EVAL] batch:  166 | acc: 62.50%,  total acc: 72.19%   [EVAL] batch:  167 | acc: 68.75%,  total acc: 72.17%   [EVAL] batch:  168 | acc: 75.00%,  total acc: 72.19%   [EVAL] batch:  169 | acc: 56.25%,  total acc: 72.10%   [EVAL] batch:  170 | acc: 75.00%,  total acc: 72.11%   [EVAL] batch:  171 | acc: 62.50%,  total acc: 72.06%   [EVAL] batch:  172 | acc: 43.75%,  total acc: 71.89%   [EVAL] batch:  173 | acc: 56.25%,  total acc: 71.80%   [EVAL] batch:  174 | acc: 87.50%,  total acc: 71.89%   [EVAL] batch:  175 | acc: 43.75%,  total acc: 71.73%   [EVAL] batch:  176 | acc: 50.00%,  total acc: 71.61%   [EVAL] batch:  177 | acc: 56.25%,  total acc: 71.52%   [EVAL] batch:  178 | acc: 62.50%,  total acc: 71.47%   [EVAL] batch:  179 | acc: 50.00%,  total acc: 71.35%   [EVAL] batch:  180 | acc: 56.25%,  total acc: 71.27%   [EVAL] batch:  181 | acc: 75.00%,  total acc: 71.29%   [EVAL] batch:  182 | acc: 81.25%,  total acc: 71.35%   [EVAL] batch:  183 | acc: 93.75%,  total acc: 71.47%   [EVAL] batch:  184 | acc: 75.00%,  total acc: 71.49%   [EVAL] batch:  185 | acc: 93.75%,  total acc: 71.61%   [EVAL] batch:  186 | acc: 75.00%,  total acc: 71.62%   [EVAL] batch:  187 | acc: 50.00%,  total acc: 71.51%   [EVAL] batch:  188 | acc: 31.25%,  total acc: 71.30%   [EVAL] batch:  189 | acc: 37.50%,  total acc: 71.12%   [EVAL] batch:  190 | acc: 12.50%,  total acc: 70.81%   [EVAL] batch:  191 | acc: 25.00%,  total acc: 70.57%   [EVAL] batch:  192 | acc: 25.00%,  total acc: 70.34%   [EVAL] batch:  193 | acc: 37.50%,  total acc: 70.17%   [EVAL] batch:  194 | acc: 68.75%,  total acc: 70.16%   [EVAL] batch:  195 | acc: 68.75%,  total acc: 70.15%   [EVAL] batch:  196 | acc: 75.00%,  total acc: 70.18%   [EVAL] batch:  197 | acc: 87.50%,  total acc: 70.27%   [EVAL] batch:  198 | acc: 75.00%,  total acc: 70.29%   [EVAL] batch:  199 | acc: 68.75%,  total acc: 70.28%   [EVAL] batch:  200 | acc: 75.00%,  total acc: 70.30%   [EVAL] batch:  201 | acc: 37.50%,  total acc: 70.14%   [EVAL] batch:  202 | acc: 37.50%,  total acc: 69.98%   [EVAL] batch:  203 | acc: 68.75%,  total acc: 69.98%   [EVAL] batch:  204 | acc: 62.50%,  total acc: 69.94%   [EVAL] batch:  205 | acc: 43.75%,  total acc: 69.81%   [EVAL] batch:  206 | acc: 93.75%,  total acc: 69.93%   [EVAL] batch:  207 | acc: 100.00%,  total acc: 70.07%   [EVAL] batch:  208 | acc: 100.00%,  total acc: 70.22%   [EVAL] batch:  209 | acc: 93.75%,  total acc: 70.33%   [EVAL] batch:  210 | acc: 93.75%,  total acc: 70.44%   [EVAL] batch:  211 | acc: 100.00%,  total acc: 70.58%   [EVAL] batch:  212 | acc: 87.50%,  total acc: 70.66%   [EVAL] batch:  213 | acc: 75.00%,  total acc: 70.68%   [EVAL] batch:  214 | acc: 81.25%,  total acc: 70.73%   [EVAL] batch:  215 | acc: 87.50%,  total acc: 70.80%   [EVAL] batch:  216 | acc: 81.25%,  total acc: 70.85%   [EVAL] batch:  217 | acc: 93.75%,  total acc: 70.96%   [EVAL] batch:  218 | acc: 81.25%,  total acc: 71.00%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 71.14%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 71.27%   [EVAL] batch:  221 | acc: 93.75%,  total acc: 71.37%   [EVAL] batch:  222 | acc: 93.75%,  total acc: 71.47%   [EVAL] batch:  223 | acc: 93.75%,  total acc: 71.57%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 71.69%   [EVAL] batch:  225 | acc: 25.00%,  total acc: 71.49%   [EVAL] batch:  226 | acc: 12.50%,  total acc: 71.23%   [EVAL] batch:  227 | acc: 12.50%,  total acc: 70.97%   [EVAL] batch:  228 | acc: 12.50%,  total acc: 70.72%   [EVAL] batch:  229 | acc: 6.25%,  total acc: 70.43%   [EVAL] batch:  230 | acc: 12.50%,  total acc: 70.18%   [EVAL] batch:  231 | acc: 75.00%,  total acc: 70.20%   [EVAL] batch:  232 | acc: 81.25%,  total acc: 70.25%   [EVAL] batch:  233 | acc: 87.50%,  total acc: 70.33%   [EVAL] batch:  234 | acc: 87.50%,  total acc: 70.40%   [EVAL] batch:  235 | acc: 56.25%,  total acc: 70.34%   [EVAL] batch:  236 | acc: 75.00%,  total acc: 70.36%   [EVAL] batch:  237 | acc: 62.50%,  total acc: 70.33%   [EVAL] batch:  238 | acc: 62.50%,  total acc: 70.29%   [EVAL] batch:  239 | acc: 62.50%,  total acc: 70.26%   [EVAL] batch:  240 | acc: 56.25%,  total acc: 70.20%   [EVAL] batch:  241 | acc: 75.00%,  total acc: 70.22%   [EVAL] batch:  242 | acc: 50.00%,  total acc: 70.14%   [EVAL] batch:  243 | acc: 37.50%,  total acc: 70.01%   [EVAL] batch:  244 | acc: 87.50%,  total acc: 70.08%   [EVAL] batch:  245 | acc: 87.50%,  total acc: 70.15%   [EVAL] batch:  246 | acc: 68.75%,  total acc: 70.14%   [EVAL] batch:  247 | acc: 81.25%,  total acc: 70.19%   [EVAL] batch:  248 | acc: 87.50%,  total acc: 70.26%   [EVAL] batch:  249 | acc: 87.50%,  total acc: 70.33%   [EVAL] batch:  250 | acc: 0.00%,  total acc: 70.04%   [EVAL] batch:  251 | acc: 12.50%,  total acc: 69.82%   [EVAL] batch:  252 | acc: 0.00%,  total acc: 69.54%   [EVAL] batch:  253 | acc: 6.25%,  total acc: 69.29%   [EVAL] batch:  254 | acc: 0.00%,  total acc: 69.02%   [EVAL] batch:  255 | acc: 0.00%,  total acc: 68.75%   [EVAL] batch:  256 | acc: 56.25%,  total acc: 68.70%   [EVAL] batch:  257 | acc: 68.75%,  total acc: 68.70%   [EVAL] batch:  258 | acc: 75.00%,  total acc: 68.73%   [EVAL] batch:  259 | acc: 75.00%,  total acc: 68.75%   [EVAL] batch:  260 | acc: 81.25%,  total acc: 68.80%   [EVAL] batch:  261 | acc: 56.25%,  total acc: 68.75%   [EVAL] batch:  262 | acc: 87.50%,  total acc: 68.82%   [EVAL] batch:  263 | acc: 56.25%,  total acc: 68.77%   [EVAL] batch:  264 | acc: 87.50%,  total acc: 68.84%   [EVAL] batch:  265 | acc: 68.75%,  total acc: 68.84%   [EVAL] batch:  266 | acc: 68.75%,  total acc: 68.84%   [EVAL] batch:  267 | acc: 81.25%,  total acc: 68.89%   [EVAL] batch:  268 | acc: 50.00%,  total acc: 68.82%   [EVAL] batch:  269 | acc: 18.75%,  total acc: 68.63%   [EVAL] batch:  270 | acc: 18.75%,  total acc: 68.45%   [EVAL] batch:  271 | acc: 31.25%,  total acc: 68.31%   [EVAL] batch:  272 | acc: 25.00%,  total acc: 68.15%   [EVAL] batch:  273 | acc: 25.00%,  total acc: 68.00%   [EVAL] batch:  274 | acc: 25.00%,  total acc: 67.84%   [EVAL] batch:  275 | acc: 6.25%,  total acc: 67.62%   [EVAL] batch:  276 | acc: 6.25%,  total acc: 67.40%   [EVAL] batch:  277 | acc: 6.25%,  total acc: 67.18%   [EVAL] batch:  278 | acc: 0.00%,  total acc: 66.94%   [EVAL] batch:  279 | acc: 12.50%,  total acc: 66.74%   [EVAL] batch:  280 | acc: 6.25%,  total acc: 66.53%   [EVAL] batch:  281 | acc: 50.00%,  total acc: 66.47%   [EVAL] batch:  282 | acc: 87.50%,  total acc: 66.54%   [EVAL] batch:  283 | acc: 87.50%,  total acc: 66.62%   [EVAL] batch:  284 | acc: 93.75%,  total acc: 66.71%   [EVAL] batch:  285 | acc: 87.50%,  total acc: 66.78%   [EVAL] batch:  286 | acc: 75.00%,  total acc: 66.81%   [EVAL] batch:  287 | acc: 93.75%,  total acc: 66.91%   [EVAL] batch:  288 | acc: 93.75%,  total acc: 67.00%   [EVAL] batch:  289 | acc: 100.00%,  total acc: 67.11%   [EVAL] batch:  290 | acc: 100.00%,  total acc: 67.23%   [EVAL] batch:  291 | acc: 81.25%,  total acc: 67.27%   [EVAL] batch:  292 | acc: 100.00%,  total acc: 67.38%   [EVAL] batch:  293 | acc: 100.00%,  total acc: 67.50%   [EVAL] batch:  294 | acc: 75.00%,  total acc: 67.52%   [EVAL] batch:  295 | acc: 50.00%,  total acc: 67.46%   [EVAL] batch:  296 | acc: 56.25%,  total acc: 67.42%   [EVAL] batch:  297 | acc: 62.50%,  total acc: 67.41%   [EVAL] batch:  298 | acc: 81.25%,  total acc: 67.45%   [EVAL] batch:  299 | acc: 75.00%,  total acc: 67.48%   [EVAL] batch:  300 | acc: 50.00%,  total acc: 67.42%   [EVAL] batch:  301 | acc: 68.75%,  total acc: 67.43%   [EVAL] batch:  302 | acc: 62.50%,  total acc: 67.41%   [EVAL] batch:  303 | acc: 81.25%,  total acc: 67.45%   [EVAL] batch:  304 | acc: 56.25%,  total acc: 67.42%   [EVAL] batch:  305 | acc: 75.00%,  total acc: 67.44%   [EVAL] batch:  306 | acc: 50.00%,  total acc: 67.39%   [EVAL] batch:  307 | acc: 56.25%,  total acc: 67.35%   [EVAL] batch:  308 | acc: 37.50%,  total acc: 67.25%   [EVAL] batch:  309 | acc: 75.00%,  total acc: 67.28%   [EVAL] batch:  310 | acc: 50.00%,  total acc: 67.22%   [EVAL] batch:  311 | acc: 81.25%,  total acc: 67.27%   [EVAL] batch:  312 | acc: 50.00%,  total acc: 67.21%   [EVAL] batch:  313 | acc: 56.25%,  total acc: 67.18%   [EVAL] batch:  314 | acc: 81.25%,  total acc: 67.22%   [EVAL] batch:  315 | acc: 56.25%,  total acc: 67.19%   [EVAL] batch:  316 | acc: 50.00%,  total acc: 67.13%   [EVAL] batch:  317 | acc: 62.50%,  total acc: 67.12%   [EVAL] batch:  318 | acc: 75.00%,  total acc: 67.14%   [EVAL] batch:  319 | acc: 100.00%,  total acc: 67.25%   [EVAL] batch:  320 | acc: 93.75%,  total acc: 67.33%   [EVAL] batch:  321 | acc: 87.50%,  total acc: 67.39%   [EVAL] batch:  322 | acc: 93.75%,  total acc: 67.47%   [EVAL] batch:  323 | acc: 93.75%,  total acc: 67.55%   [EVAL] batch:  324 | acc: 75.00%,  total acc: 67.58%   [EVAL] batch:  325 | acc: 75.00%,  total acc: 67.60%   [EVAL] batch:  326 | acc: 56.25%,  total acc: 67.56%   [EVAL] batch:  327 | acc: 75.00%,  total acc: 67.59%   [EVAL] batch:  328 | acc: 75.00%,  total acc: 67.61%   [EVAL] batch:  329 | acc: 87.50%,  total acc: 67.67%   [EVAL] batch:  330 | acc: 68.75%,  total acc: 67.67%   [EVAL] batch:  331 | acc: 81.25%,  total acc: 67.71%   [EVAL] batch:  332 | acc: 81.25%,  total acc: 67.76%   [EVAL] batch:  333 | acc: 81.25%,  total acc: 67.80%   [EVAL] batch:  334 | acc: 68.75%,  total acc: 67.80%   [EVAL] batch:  335 | acc: 75.00%,  total acc: 67.82%   [EVAL] batch:  336 | acc: 62.50%,  total acc: 67.80%   [EVAL] batch:  337 | acc: 93.75%,  total acc: 67.88%   [EVAL] batch:  338 | acc: 62.50%,  total acc: 67.87%   [EVAL] batch:  339 | acc: 50.00%,  total acc: 67.81%   [EVAL] batch:  340 | acc: 75.00%,  total acc: 67.83%   [EVAL] batch:  341 | acc: 68.75%,  total acc: 67.84%   [EVAL] batch:  342 | acc: 62.50%,  total acc: 67.82%   [EVAL] batch:  343 | acc: 43.75%,  total acc: 67.75%   [EVAL] batch:  344 | acc: 68.75%,  total acc: 67.75%   [EVAL] batch:  345 | acc: 50.00%,  total acc: 67.70%   [EVAL] batch:  346 | acc: 62.50%,  total acc: 67.69%   [EVAL] batch:  347 | acc: 62.50%,  total acc: 67.67%   [EVAL] batch:  348 | acc: 75.00%,  total acc: 67.69%   [EVAL] batch:  349 | acc: 68.75%,  total acc: 67.70%   [EVAL] batch:  350 | acc: 50.00%,  total acc: 67.65%   [EVAL] batch:  351 | acc: 62.50%,  total acc: 67.63%   [EVAL] batch:  352 | acc: 56.25%,  total acc: 67.60%   [EVAL] batch:  353 | acc: 75.00%,  total acc: 67.62%   [EVAL] batch:  354 | acc: 75.00%,  total acc: 67.64%   [EVAL] batch:  355 | acc: 56.25%,  total acc: 67.61%   [EVAL] batch:  356 | acc: 37.50%,  total acc: 67.52%   [EVAL] batch:  357 | acc: 18.75%,  total acc: 67.39%   [EVAL] batch:  358 | acc: 18.75%,  total acc: 67.25%   [EVAL] batch:  359 | acc: 43.75%,  total acc: 67.19%   [EVAL] batch:  360 | acc: 18.75%,  total acc: 67.05%   [EVAL] batch:  361 | acc: 43.75%,  total acc: 66.99%   [EVAL] batch:  362 | acc: 56.25%,  total acc: 66.96%   [EVAL] batch:  363 | acc: 100.00%,  total acc: 67.05%   [EVAL] batch:  364 | acc: 100.00%,  total acc: 67.14%   [EVAL] batch:  365 | acc: 93.75%,  total acc: 67.21%   [EVAL] batch:  366 | acc: 100.00%,  total acc: 67.30%   [EVAL] batch:  367 | acc: 100.00%,  total acc: 67.39%   [EVAL] batch:  368 | acc: 100.00%,  total acc: 67.48%   [EVAL] batch:  369 | acc: 93.75%,  total acc: 67.55%   [EVAL] batch:  370 | acc: 87.50%,  total acc: 67.60%   [EVAL] batch:  371 | acc: 93.75%,  total acc: 67.67%   [EVAL] batch:  372 | acc: 87.50%,  total acc: 67.73%   [EVAL] batch:  373 | acc: 93.75%,  total acc: 67.80%   [EVAL] batch:  374 | acc: 87.50%,  total acc: 67.85%   [EVAL] batch:  375 | acc: 37.50%,  total acc: 67.77%   [EVAL] batch:  376 | acc: 62.50%,  total acc: 67.76%   [EVAL] batch:  377 | acc: 43.75%,  total acc: 67.69%   [EVAL] batch:  378 | acc: 56.25%,  total acc: 67.66%   [EVAL] batch:  379 | acc: 56.25%,  total acc: 67.63%   [EVAL] batch:  380 | acc: 50.00%,  total acc: 67.59%   [EVAL] batch:  381 | acc: 50.00%,  total acc: 67.54%   [EVAL] batch:  382 | acc: 68.75%,  total acc: 67.54%   [EVAL] batch:  383 | acc: 68.75%,  total acc: 67.55%   [EVAL] batch:  384 | acc: 81.25%,  total acc: 67.58%   [EVAL] batch:  385 | acc: 50.00%,  total acc: 67.54%   [EVAL] batch:  386 | acc: 62.50%,  total acc: 67.52%   [EVAL] batch:  387 | acc: 62.50%,  total acc: 67.51%   [EVAL] batch:  388 | acc: 62.50%,  total acc: 67.50%   [EVAL] batch:  389 | acc: 50.00%,  total acc: 67.45%   [EVAL] batch:  390 | acc: 25.00%,  total acc: 67.34%   [EVAL] batch:  391 | acc: 50.00%,  total acc: 67.30%   [EVAL] batch:  392 | acc: 56.25%,  total acc: 67.27%   [EVAL] batch:  393 | acc: 87.50%,  total acc: 67.32%   [EVAL] batch:  394 | acc: 93.75%,  total acc: 67.39%   [EVAL] batch:  395 | acc: 100.00%,  total acc: 67.47%   [EVAL] batch:  396 | acc: 100.00%,  total acc: 67.55%   [EVAL] batch:  397 | acc: 100.00%,  total acc: 67.64%   [EVAL] batch:  398 | acc: 93.75%,  total acc: 67.70%   [EVAL] batch:  399 | acc: 100.00%,  total acc: 67.78%   [EVAL] batch:  400 | acc: 56.25%,  total acc: 67.75%   [EVAL] batch:  401 | acc: 37.50%,  total acc: 67.68%   [EVAL] batch:  402 | acc: 68.75%,  total acc: 67.68%   [EVAL] batch:  403 | acc: 75.00%,  total acc: 67.70%   [EVAL] batch:  404 | acc: 37.50%,  total acc: 67.62%   [EVAL] batch:  405 | acc: 43.75%,  total acc: 67.56%   [EVAL] batch:  406 | acc: 50.00%,  total acc: 67.52%   [EVAL] batch:  407 | acc: 43.75%,  total acc: 67.46%   [EVAL] batch:  408 | acc: 12.50%,  total acc: 67.33%   [EVAL] batch:  409 | acc: 18.75%,  total acc: 67.21%   [EVAL] batch:  410 | acc: 12.50%,  total acc: 67.08%   [EVAL] batch:  411 | acc: 6.25%,  total acc: 66.93%   [EVAL] batch:  412 | acc: 18.75%,  total acc: 66.81%   [EVAL] batch:  413 | acc: 18.75%,  total acc: 66.70%   [EVAL] batch:  414 | acc: 18.75%,  total acc: 66.58%   [EVAL] batch:  415 | acc: 18.75%,  total acc: 66.47%   [EVAL] batch:  416 | acc: 12.50%,  total acc: 66.34%   [EVAL] batch:  417 | acc: 31.25%,  total acc: 66.25%   [EVAL] batch:  418 | acc: 25.00%,  total acc: 66.15%   [EVAL] batch:  419 | acc: 68.75%,  total acc: 66.16%   [EVAL] batch:  420 | acc: 56.25%,  total acc: 66.14%   [EVAL] batch:  421 | acc: 81.25%,  total acc: 66.17%   [EVAL] batch:  422 | acc: 62.50%,  total acc: 66.16%   [EVAL] batch:  423 | acc: 62.50%,  total acc: 66.16%   [EVAL] batch:  424 | acc: 75.00%,  total acc: 66.18%   [EVAL] batch:  425 | acc: 100.00%,  total acc: 66.26%   [EVAL] batch:  426 | acc: 100.00%,  total acc: 66.33%   [EVAL] batch:  427 | acc: 100.00%,  total acc: 66.41%   [EVAL] batch:  428 | acc: 100.00%,  total acc: 66.49%   [EVAL] batch:  429 | acc: 100.00%,  total acc: 66.57%   [EVAL] batch:  430 | acc: 100.00%,  total acc: 66.65%   [EVAL] batch:  431 | acc: 100.00%,  total acc: 66.72%   [EVAL] batch:  432 | acc: 87.50%,  total acc: 66.77%   [EVAL] batch:  433 | acc: 93.75%,  total acc: 66.83%   [EVAL] batch:  434 | acc: 100.00%,  total acc: 66.91%   [EVAL] batch:  435 | acc: 93.75%,  total acc: 66.97%   [EVAL] batch:  436 | acc: 100.00%,  total acc: 67.05%   [EVAL] batch:  437 | acc: 50.00%,  total acc: 67.01%   
cur_acc:  ['0.9524', '0.6994', '0.7639', '0.6895', '0.5843', '0.7004', '0.6200']
his_acc:  ['0.9524', '0.8220', '0.7949', '0.7342', '0.7037', '0.7017', '0.6701']
Clustering into  39  clusters
Clusters:  [31  1  9 25 20 28  9 18  1  8  5 10 18 16 33 32  2 26 37 26 22 16  3  4
  3  6 10 22 30 38 17 13 26  0  0 13 26  7  8 35 20 16 10 34  3 19 10  1
 10  1 36 11 18 20 24  8 22 28 13 15 29 21  2  9 11 17  1 12 14  7 25 23
  4  1 27 12  5  6 33  5]
Losses:  8.120575904846191 2.3813576698303223 0.7635721564292908
CurrentTrain: epoch  0, batch     0 | loss: 8.1205759Losses:  10.035197257995605 3.7289161682128906 0.6525394320487976
CurrentTrain: epoch  0, batch     1 | loss: 10.0351973Losses:  9.123940467834473 3.2467257976531982 0.7064484357833862
CurrentTrain: epoch  0, batch     2 | loss: 9.1239405Losses:  3.71870493888855 -0.0 0.11165495216846466
CurrentTrain: epoch  0, batch     3 | loss: 3.7187049Losses:  9.057747840881348 3.782344341278076 0.5976011157035828
CurrentTrain: epoch  1, batch     0 | loss: 9.0577478Losses:  8.274853706359863 3.6591291427612305 0.606848418712616
CurrentTrain: epoch  1, batch     1 | loss: 8.2748537Losses:  6.035239219665527 2.500152587890625 0.7303400039672852
CurrentTrain: epoch  1, batch     2 | loss: 6.0352392Losses:  4.4402995109558105 -0.0 0.15598711371421814
CurrentTrain: epoch  1, batch     3 | loss: 4.4402995Losses:  7.883039474487305 3.2833900451660156 0.7364485263824463
CurrentTrain: epoch  2, batch     0 | loss: 7.8830395Losses:  5.919276714324951 2.1381797790527344 0.7251613736152649
CurrentTrain: epoch  2, batch     1 | loss: 5.9192767Losses:  6.1762213706970215 2.695868730545044 0.6595934629440308
CurrentTrain: epoch  2, batch     2 | loss: 6.1762214Losses:  2.0229508876800537 -0.0 0.1141837015748024
CurrentTrain: epoch  2, batch     3 | loss: 2.0229509Losses:  4.969189643859863 1.681011438369751 0.730187177658081
CurrentTrain: epoch  3, batch     0 | loss: 4.9691896Losses:  6.8541693687438965 3.987704277038574 0.4999256730079651
CurrentTrain: epoch  3, batch     1 | loss: 6.8541694Losses:  8.425936698913574 4.493338584899902 0.5904844999313354
CurrentTrain: epoch  3, batch     2 | loss: 8.4259367Losses:  2.03330135345459 -0.0 0.11398343741893768
CurrentTrain: epoch  3, batch     3 | loss: 2.0333014Losses:  6.3308796882629395 3.319585084915161 0.5940707325935364
CurrentTrain: epoch  4, batch     0 | loss: 6.3308797Losses:  5.528544902801514 2.4404804706573486 0.7245727777481079
CurrentTrain: epoch  4, batch     1 | loss: 5.5285449Losses:  6.485504627227783 3.164965867996216 0.7281811833381653
CurrentTrain: epoch  4, batch     2 | loss: 6.4855046Losses:  5.733022689819336 -0.0 0.10799413174390793
CurrentTrain: epoch  4, batch     3 | loss: 5.7330227Losses:  6.231616497039795 2.632692337036133 0.6525254249572754
CurrentTrain: epoch  5, batch     0 | loss: 6.2316165Losses:  6.5675435066223145 3.599879264831543 0.6602483987808228
CurrentTrain: epoch  5, batch     1 | loss: 6.5675435Losses:  5.4888458251953125 2.606916666030884 0.6379032135009766
CurrentTrain: epoch  5, batch     2 | loss: 5.4888458Losses:  2.1325531005859375 -0.0 0.10985857993364334
CurrentTrain: epoch  5, batch     3 | loss: 2.1325531Losses:  6.66668176651001 3.1269888877868652 0.7090800404548645
CurrentTrain: epoch  6, batch     0 | loss: 6.6666818Losses:  6.787522792816162 3.9127700328826904 0.5677036046981812
CurrentTrain: epoch  6, batch     1 | loss: 6.7875228Losses:  6.715733528137207 3.9191393852233887 0.504164457321167
CurrentTrain: epoch  6, batch     2 | loss: 6.7157335Losses:  1.7695090770721436 -0.0 -0.0
CurrentTrain: epoch  6, batch     3 | loss: 1.7695091Losses:  5.639626979827881 2.9465742111206055 0.6494909524917603
CurrentTrain: epoch  7, batch     0 | loss: 5.6396270Losses:  6.498763561248779 3.205660820007324 0.7060105204582214
CurrentTrain: epoch  7, batch     1 | loss: 6.4987636Losses:  6.436606407165527 3.8360824584960938 0.6301194429397583
CurrentTrain: epoch  7, batch     2 | loss: 6.4366064Losses:  2.5070834159851074 -0.0 0.17419564723968506
CurrentTrain: epoch  7, batch     3 | loss: 2.5070834Losses:  6.2383036613464355 3.395721673965454 0.6435513496398926
CurrentTrain: epoch  8, batch     0 | loss: 6.2383037Losses:  4.61234188079834 1.7906734943389893 0.6343065500259399
CurrentTrain: epoch  8, batch     1 | loss: 4.6123419Losses:  5.035990238189697 2.23568058013916 0.6417393684387207
CurrentTrain: epoch  8, batch     2 | loss: 5.0359902Losses:  1.727569341659546 -0.0 -0.0
CurrentTrain: epoch  8, batch     3 | loss: 1.7275693Losses:  4.474402904510498 1.8196523189544678 0.7011147737503052
CurrentTrain: epoch  9, batch     0 | loss: 4.4744029Losses:  4.411915302276611 1.7998435497283936 0.6958040595054626
CurrentTrain: epoch  9, batch     1 | loss: 4.4119153Losses:  5.368169784545898 2.25252103805542 0.7063038349151611
CurrentTrain: epoch  9, batch     2 | loss: 5.3681698Losses:  1.8643815517425537 -0.0 0.11120504140853882
CurrentTrain: epoch  9, batch     3 | loss: 1.8643816
#### member of political party ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "member of political party" means the political party of which a person is or has been a member or otherwise affiliated.
Here is an example:
Relation: member of political party
Context: karl johan aarønes ( 8 may 1900 – 12 august 1969 ) was a norwegian politician for the labour party .
Head Entity: karl johan aarønes
Tail Entity: labour party
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "member of political party":
Relation: member of political party  
Context: barack obama served as a member of the democratic party during his presidency from 2009 to 2017.  
Head Entity: barack obama  
Tail Entity: democratic party  

Relation: member of political party  
Context: angela merkel was a prominent member of the christian democratic union throughout her political career in germany.  
Head Entity: angela merkel  
Tail Entity: christian democratic union  
#### after a work by ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "after a work by" means artist whose work strongly inspired/ was copied in this item.
Here is an example:
Relation: after a work by
Context: it is a partly fictional biography , adapted by shane connaughton and jim sheridan from the book " my left foot " by christy brown .
Head Entity: my left foot
Tail Entity: christy brown
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "after a work by":
Relation: after a work by  
Context: the film is based on the novel "the great gatsby" by f. scott fitzgerald, capturing the essence of the roaring twenties.  
Head Entity: the great gatsby  
Tail Entity: f. scott fitzgerald  

Relation: after a work by  
Context: this musical is inspired by the play "hamlet" written by william shakespeare, reimagining the classic tragedy in a contemporary setting.  
Head Entity: hamlet  
Tail Entity: william shakespeare  
#### notable work ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "notable work" means notable scientific, artistic or literary work, or other work of significance among subject's works.
Here is an example:
Relation: notable work
Context: another was " the death of nelson " by daniel maclise , a large wall painting in the royal gallery of the palace of westminster .
Head Entity: daniel maclise
Tail Entity: the death of nelson
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "notable work":
Relation: notable work  
Context: one of his most famous pieces is "the persistence of memory," a surreal painting that features melting clocks and explores the concept of time.  
Head Entity: salvador dalí  
Tail Entity: the persistence of memory  

Relation: notable work  
Context: her acclaimed novel "to kill a mockingbird" addresses serious issues like racial injustice and moral growth through the eyes of a young girl.  
Head Entity: harper lee  
Tail Entity: to kill a mockingbird  
#### headquarters location ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "headquarters location" means city, where an organization's headquarters is or has been situated.
Here is an example:
Relation: headquarters location
Context: at the request of the stockholm institute for bible translation , mirza khazar translated the bible – the new testament and old testament – into the azerbaijani language .
Head Entity: institute for bible translation
Tail Entity: stockholm
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "headquarters location":
Relation: headquarters location  
Context: the global technology company, headquartered in san francisco, has been a leader in software development for over two decades.  
Head Entity: global technology company  
Tail Entity: san francisco  

Relation: headquarters location  
Context: after relocating its main office to new york city, the fashion brand saw a significant increase in sales and brand recognition.  
Head Entity: fashion brand  
Tail Entity: new york city  
#### taxon rank ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "taxon rank" means level in a taxonomic hierarchy.
Here is an example:
Relation: taxon rank
Context: a number of species that were placed in the family phyllophoridae by thander in 1989 and 1990 are now included in cucumariidae .
Head Entity: phyllophoridae
Tail Entity: family
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "taxon rank":
Relation: taxon rank  
Context: The genus Canis includes several species, with Canis lupus being the most recognized as the wolf, which belongs to the family Canidae.  
Head Entity: Canis  
Tail Entity: genus  

Relation: taxon rank  
Context: In the classification of plants, the family Rosaceae encompasses a variety of species, including roses and strawberries, which are categorized under the order Rosales.  
Head Entity: Rosaceae  
Tail Entity: family  
#### field of work ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "field of work" means specialization of a person or organization.
Here is an example:
Relation: field of work
Context: he later collaborated with vito volterra and explored laplace transforms and other parts of functional analysis .
Head Entity: vito volterra
Tail Entity: functional analysis
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "field of work":
Relation: field of work  
Context: Marie Curie was renowned for her groundbreaking research in radioactivity, which laid the foundation for advancements in nuclear physics.  
Head Entity: Marie Curie  
Tail Entity: nuclear physics  

Relation: field of work  
Context: The company specializes in artificial intelligence and machine learning, providing innovative solutions for various industries.  
Head Entity: The company  
Tail Entity: artificial intelligence
#### licensed to broadcast to ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "licensed to broadcast to" means place that a radio/TV station is licensed/required to broadcast to.
Here is an example:
Relation: licensed to broadcast to
Context: this early involvement in radio led him to buy many small , under performing radio stations beginning with wcos in columbia , south carolina , and turn them around with new formats and improved management .
Head Entity: wcos
Tail Entity: columbia , south carolina
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "licensed to broadcast to":
Relation: licensed to broadcast to  
Context: The new regulations allowed the station KXYZ to expand its reach, enabling it to serve the community of Springfield more effectively.  
Head Entity: KXYZ  
Tail Entity: Springfield  

Relation: licensed to broadcast to  
Context: After acquiring the rights, the television network was able to broadcast its popular shows to the audience in the city of Denver, significantly increasing its viewership.  
Head Entity: television network  
Tail Entity: Denver  
#### constellation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "constellation" means the area of the celestial sphere of which the subject is a part.
Here is an example:
Relation: constellation
Context: ngc 192 is a type sba spiral galaxy located in the constellation cetus .
Head Entity: ngc 192
Tail Entity: cetus
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "constellation":
Relation: constellation  
Context: the orion nebula is located in the constellation orion and is one of the brightest nebulae visible to the naked eye.  
Head Entity: orion nebula  
Tail Entity: orion  

Relation: constellation  
Context: the star betelgeuse is a prominent feature in the constellation orion, known for its reddish hue.  
Head Entity: betelgeuse  
Tail Entity: orion  
#### platform ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "platform" means platform for which a work was developed or released, or the specific platform version of a software product.
Here is an example:
Relation: platform
Context: " toy soldiers : war chest " was released on august 11 , 2015 for windows , playstation 4 and xbox one .
Head Entity: toy soldiers : war chest
Tail Entity: xbox one
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "platform":
Relation: platform  
Context: "The Legend of Zelda: Breath of the Wild" was launched on March 3, 2017 for the Nintendo Switch and Wii U.  
Head Entity: The Legend of Zelda: Breath of the Wild  
Tail Entity: Nintendo Switch  

Relation: platform  
Context: "Cyberpunk 2077" was made available on December 10, 2020 for PC, PlayStation 4, and Xbox One.  
Head Entity: Cyberpunk 2077  
Tail Entity: PlayStation 4  
#### place served by transport hub ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "place served by transport hub" means territorial entity or entities served by this transport hub (airport, train station, etc.).
Here is an example:
Relation: place served by transport hub
Context: the dallas - fort worth metroplex 's major international airport , dallas - fort worth international airport , is located a short distance to the northwest .
Head Entity: dallas - fort worth international airport
Tail Entity: dallas - fort worth metroplex
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "place served by transport hub":
Relation: place served by transport hub  
Context: the bustling city of los angeles is served by several major airports, including los angeles international airport, which connects travelers to destinations worldwide.  
Head Entity: los angeles international airport  
Tail Entity: los angeles  

Relation: place served by transport hub  
Context: the central train station in berlin, berlin hauptbahnhof, serves as a key transport hub for the city, linking various regional and international train services.  
Head Entity: berlin hauptbahnhof  
Tail Entity: berlin  
Losses:  2.612572431564331 -0.0 0.9761925935745239
MemoryTrain:  epoch  0, batch     0 | loss: 2.6125724Losses:  2.682562828063965 0.24257732927799225 0.9109151363372803
MemoryTrain:  epoch  0, batch     1 | loss: 2.6825628Losses:  3.1777658462524414 -0.0 0.9848002791404724
MemoryTrain:  epoch  0, batch     2 | loss: 3.1777658Losses:  3.635655403137207 0.9342676401138306 0.9389621615409851
MemoryTrain:  epoch  0, batch     3 | loss: 3.6356554Losses:  3.784651517868042 -0.0 1.0669124126434326
MemoryTrain:  epoch  0, batch     4 | loss: 3.7846515Losses:  3.700495481491089 -0.0 0.95438551902771
MemoryTrain:  epoch  0, batch     5 | loss: 3.7004955Losses:  3.1570279598236084 -0.0 0.8581569194793701
MemoryTrain:  epoch  0, batch     6 | loss: 3.1570280Losses:  4.10919189453125 0.5326951742172241 0.9958425760269165
MemoryTrain:  epoch  0, batch     7 | loss: 4.1091919Losses:  3.4029722213745117 -0.0 0.9989258050918579
MemoryTrain:  epoch  0, batch     8 | loss: 3.4029722Losses:  3.342808961868286 0.2698688507080078 0.9717831611633301
MemoryTrain:  epoch  0, batch     9 | loss: 3.3428090Losses:  3.1023807525634766 -0.0 1.075995922088623
MemoryTrain:  epoch  0, batch    10 | loss: 3.1023808Losses:  4.0842366218566895 0.7690470218658447 0.9604076743125916
MemoryTrain:  epoch  0, batch    11 | loss: 4.0842366Losses:  3.1080548763275146 -0.0 0.9685635566711426
MemoryTrain:  epoch  0, batch    12 | loss: 3.1080549Losses:  3.0779049396514893 -0.0 1.0343961715698242
MemoryTrain:  epoch  0, batch    13 | loss: 3.0779049Losses:  4.133528232574463 0.2417616844177246 1.0873699188232422
MemoryTrain:  epoch  0, batch    14 | loss: 4.1335282Losses:  3.029630184173584 -0.0 1.0565742254257202
MemoryTrain:  epoch  1, batch     0 | loss: 3.0296302Losses:  3.1691670417785645 0.5283911824226379 1.0240894556045532
MemoryTrain:  epoch  1, batch     1 | loss: 3.1691670Losses:  5.218959808349609 1.1651732921600342 0.8221805691719055
MemoryTrain:  epoch  1, batch     2 | loss: 5.2189598Losses:  3.0054800510406494 0.5108544826507568 1.0087499618530273
MemoryTrain:  epoch  1, batch     3 | loss: 3.0054801Losses:  3.246272325515747 0.24219249188899994 0.9665136933326721
MemoryTrain:  epoch  1, batch     4 | loss: 3.2462723Losses:  2.5997376441955566 0.22005504369735718 0.8566837310791016
MemoryTrain:  epoch  1, batch     5 | loss: 2.5997376Losses:  2.566561222076416 -0.0 1.090010404586792
MemoryTrain:  epoch  1, batch     6 | loss: 2.5665612Losses:  3.254420757293701 0.5110278129577637 0.8635543584823608
MemoryTrain:  epoch  1, batch     7 | loss: 3.2544208Losses:  3.244201421737671 -0.0 1.0252211093902588
MemoryTrain:  epoch  1, batch     8 | loss: 3.2442014Losses:  3.57031512260437 1.2085373401641846 0.8250694274902344
MemoryTrain:  epoch  1, batch     9 | loss: 3.5703151Losses:  3.3239898681640625 0.5228908061981201 0.8750087022781372
MemoryTrain:  epoch  1, batch    10 | loss: 3.3239899Losses:  2.49208664894104 -0.0 0.9720789194107056
MemoryTrain:  epoch  1, batch    11 | loss: 2.4920866Losses:  2.827639102935791 0.2543385624885559 1.0197913646697998
MemoryTrain:  epoch  1, batch    12 | loss: 2.8276391Losses:  3.6287765502929688 0.6540428400039673 0.9750853776931763
MemoryTrain:  epoch  1, batch    13 | loss: 3.6287766Losses:  2.997105836868286 0.5446527004241943 0.8199896216392517
MemoryTrain:  epoch  1, batch    14 | loss: 2.9971058Losses:  2.619086265563965 0.25449109077453613 0.9021061658859253
MemoryTrain:  epoch  2, batch     0 | loss: 2.6190863Losses:  3.370283603668213 0.5030338764190674 0.8089919090270996
MemoryTrain:  epoch  2, batch     1 | loss: 3.3702836Losses:  2.9906420707702637 0.5102527141571045 1.027342438697815
MemoryTrain:  epoch  2, batch     2 | loss: 2.9906421Losses:  3.118856906890869 0.49789124727249146 0.9845072031021118
MemoryTrain:  epoch  2, batch     3 | loss: 3.1188569Losses:  2.3737359046936035 -0.0 1.007429599761963
MemoryTrain:  epoch  2, batch     4 | loss: 2.3737359Losses:  3.194190263748169 -0.0 0.8659121990203857
MemoryTrain:  epoch  2, batch     5 | loss: 3.1941903Losses:  3.4045028686523438 0.8958178162574768 0.9669198393821716
MemoryTrain:  epoch  2, batch     6 | loss: 3.4045029Losses:  2.6480631828308105 0.24998235702514648 0.9470528364181519
MemoryTrain:  epoch  2, batch     7 | loss: 2.6480632Losses:  2.9566526412963867 0.5056540369987488 1.0183351039886475
MemoryTrain:  epoch  2, batch     8 | loss: 2.9566526Losses:  3.0427331924438477 0.26023054122924805 1.0214272737503052
MemoryTrain:  epoch  2, batch     9 | loss: 3.0427332Losses:  2.5631494522094727 -0.0 0.8997851610183716
MemoryTrain:  epoch  2, batch    10 | loss: 2.5631495Losses:  3.2111964225769043 0.2423345297574997 1.0686638355255127
MemoryTrain:  epoch  2, batch    11 | loss: 3.2111964Losses:  2.6995692253112793 0.2600519061088562 0.9316030740737915
MemoryTrain:  epoch  2, batch    12 | loss: 2.6995692Losses:  2.4356870651245117 -0.0 0.9611648321151733
MemoryTrain:  epoch  2, batch    13 | loss: 2.4356871Losses:  2.4180195331573486 -0.0 1.0633761882781982
MemoryTrain:  epoch  2, batch    14 | loss: 2.4180195Losses:  2.8617653846740723 0.4828220009803772 0.9555333852767944
MemoryTrain:  epoch  3, batch     0 | loss: 2.8617654Losses:  2.1551270484924316 -0.0 0.7802373766899109
MemoryTrain:  epoch  3, batch     1 | loss: 2.1551270Losses:  2.6872267723083496 0.2647431492805481 1.0046128034591675
MemoryTrain:  epoch  3, batch     2 | loss: 2.6872268Losses:  2.358461856842041 -0.0 1.0262435674667358
MemoryTrain:  epoch  3, batch     3 | loss: 2.3584619Losses:  2.686187744140625 0.2363513708114624 1.0003440380096436
MemoryTrain:  epoch  3, batch     4 | loss: 2.6861877Losses:  2.2939682006835938 -0.0 1.0527749061584473
MemoryTrain:  epoch  3, batch     5 | loss: 2.2939682Losses:  2.599025249481201 0.2456357181072235 0.9127634763717651
MemoryTrain:  epoch  3, batch     6 | loss: 2.5990252Losses:  2.454035997390747 -0.0 1.0319883823394775
MemoryTrain:  epoch  3, batch     7 | loss: 2.4540360Losses:  2.7079105377197266 -0.0 0.9216124415397644
MemoryTrain:  epoch  3, batch     8 | loss: 2.7079105Losses:  2.7435250282287598 0.496030330657959 0.9763016700744629
MemoryTrain:  epoch  3, batch     9 | loss: 2.7435250Losses:  2.658328056335449 0.2498241662979126 1.0044441223144531
MemoryTrain:  epoch  3, batch    10 | loss: 2.6583281Losses:  2.658107280731201 0.256430983543396 0.9443705081939697
MemoryTrain:  epoch  3, batch    11 | loss: 2.6581073Losses:  2.9255504608154297 0.2897098958492279 1.001233696937561
MemoryTrain:  epoch  3, batch    12 | loss: 2.9255505Losses:  3.1737544536590576 0.24272963404655457 0.9743555188179016
MemoryTrain:  epoch  3, batch    13 | loss: 3.1737545Losses:  3.1151371002197266 0.5399132966995239 1.0355582237243652
MemoryTrain:  epoch  3, batch    14 | loss: 3.1151371Losses:  2.4722225666046143 -0.0 1.0255496501922607
MemoryTrain:  epoch  4, batch     0 | loss: 2.4722226Losses:  2.284097194671631 -0.0 0.9505574703216553
MemoryTrain:  epoch  4, batch     1 | loss: 2.2840972Losses:  2.6164369583129883 0.2416505217552185 0.9189581274986267
MemoryTrain:  epoch  4, batch     2 | loss: 2.6164370Losses:  2.5783889293670654 0.2549443542957306 0.9757065773010254
MemoryTrain:  epoch  4, batch     3 | loss: 2.5783889Losses:  2.866560459136963 0.22589609026908875 0.8301681280136108
MemoryTrain:  epoch  4, batch     4 | loss: 2.8665605Losses:  2.8023579120635986 0.5121718645095825 0.8482531309127808
MemoryTrain:  epoch  4, batch     5 | loss: 2.8023579Losses:  2.708804130554199 -0.0 0.9570928812026978
MemoryTrain:  epoch  4, batch     6 | loss: 2.7088041Losses:  2.5257627964019775 0.2555130124092102 0.9745824933052063
MemoryTrain:  epoch  4, batch     7 | loss: 2.5257628Losses:  2.857377052307129 0.5032775402069092 0.9142063856124878
MemoryTrain:  epoch  4, batch     8 | loss: 2.8573771Losses:  2.6771233081817627 0.2411346584558487 1.0115723609924316
MemoryTrain:  epoch  4, batch     9 | loss: 2.6771233Losses:  2.592813730239868 0.26516643166542053 1.0297530889511108
MemoryTrain:  epoch  4, batch    10 | loss: 2.5928137Losses:  2.6854665279388428 0.2582038640975952 0.8500198125839233
MemoryTrain:  epoch  4, batch    11 | loss: 2.6854665Losses:  2.637199640274048 0.5004199147224426 0.9094030261039734
MemoryTrain:  epoch  4, batch    12 | loss: 2.6371996Losses:  2.8149471282958984 0.5090943574905396 1.02543044090271
MemoryTrain:  epoch  4, batch    13 | loss: 2.8149471Losses:  2.7308568954467773 0.49926018714904785 0.9600018262863159
MemoryTrain:  epoch  4, batch    14 | loss: 2.7308569Losses:  2.3916475772857666 0.26573503017425537 0.8527146577835083
MemoryTrain:  epoch  5, batch     0 | loss: 2.3916476Losses:  2.6520609855651855 0.517917275428772 0.8358309864997864
MemoryTrain:  epoch  5, batch     1 | loss: 2.6520610Losses:  3.046680212020874 0.5149592757225037 0.9246480464935303
MemoryTrain:  epoch  5, batch     2 | loss: 3.0466802Losses:  2.4192137718200684 -0.0 1.012670874595642
MemoryTrain:  epoch  5, batch     3 | loss: 2.4192138Losses:  2.352339029312134 -0.0 0.9550896286964417
MemoryTrain:  epoch  5, batch     4 | loss: 2.3523390Losses:  2.9576101303100586 0.5258076190948486 0.9344167709350586
MemoryTrain:  epoch  5, batch     5 | loss: 2.9576101Losses:  2.4751715660095215 0.2502481937408447 0.9620025157928467
MemoryTrain:  epoch  5, batch     6 | loss: 2.4751716Losses:  2.496105194091797 0.253440260887146 0.857421875
MemoryTrain:  epoch  5, batch     7 | loss: 2.4961052Losses:  3.0360875129699707 0.4986397922039032 0.9024982452392578
MemoryTrain:  epoch  5, batch     8 | loss: 3.0360875Losses:  2.7663958072662354 0.5000243782997131 0.9763877987861633
MemoryTrain:  epoch  5, batch     9 | loss: 2.7663958Losses:  2.883368968963623 0.5961946249008179 0.8697724342346191
MemoryTrain:  epoch  5, batch    10 | loss: 2.8833690Losses:  2.732977867126465 0.2489757388830185 1.070113182067871
MemoryTrain:  epoch  5, batch    11 | loss: 2.7329779Losses:  2.2717833518981934 -0.0 0.9977420568466187
MemoryTrain:  epoch  5, batch    12 | loss: 2.2717834Losses:  2.4512202739715576 0.24401138722896576 0.9632030129432678
MemoryTrain:  epoch  5, batch    13 | loss: 2.4512203Losses:  2.6506195068359375 0.24929794669151306 1.0262322425842285
MemoryTrain:  epoch  5, batch    14 | loss: 2.6506195Losses:  2.235116481781006 -0.0 0.9530556201934814
MemoryTrain:  epoch  6, batch     0 | loss: 2.2351165Losses:  2.6592206954956055 0.31624293327331543 0.9126490354537964
MemoryTrain:  epoch  6, batch     1 | loss: 2.6592207Losses:  2.5870628356933594 0.5292041301727295 0.7934643626213074
MemoryTrain:  epoch  6, batch     2 | loss: 2.5870628Losses:  2.560372829437256 0.24582278728485107 0.9400438666343689
MemoryTrain:  epoch  6, batch     3 | loss: 2.5603728Losses:  2.6064915657043457 0.27783626317977905 0.9098593592643738
MemoryTrain:  epoch  6, batch     4 | loss: 2.6064916Losses:  2.7195286750793457 0.512823224067688 0.9634642004966736
MemoryTrain:  epoch  6, batch     5 | loss: 2.7195287Losses:  2.2086923122406006 -0.0 0.8569190502166748
MemoryTrain:  epoch  6, batch     6 | loss: 2.2086923Losses:  2.4950523376464844 0.2535981833934784 0.9506415128707886
MemoryTrain:  epoch  6, batch     7 | loss: 2.4950523Losses:  2.8518059253692627 0.7394436001777649 0.854198157787323
MemoryTrain:  epoch  6, batch     8 | loss: 2.8518059Losses:  2.7878153324127197 0.5220125913619995 1.0171513557434082
MemoryTrain:  epoch  6, batch     9 | loss: 2.7878153Losses:  2.3530185222625732 -0.0 0.9703244566917419
MemoryTrain:  epoch  6, batch    10 | loss: 2.3530185Losses:  2.1910390853881836 -0.0 0.9528905749320984
MemoryTrain:  epoch  6, batch    11 | loss: 2.1910391Losses:  2.5200858116149902 0.2723274827003479 0.9788330793380737
MemoryTrain:  epoch  6, batch    12 | loss: 2.5200858Losses:  2.2527732849121094 -0.0 0.9963541626930237
MemoryTrain:  epoch  6, batch    13 | loss: 2.2527733Losses:  2.6358275413513184 0.26395922899246216 1.011335015296936
MemoryTrain:  epoch  6, batch    14 | loss: 2.6358275Losses:  2.9292073249816895 0.7472836971282959 0.9095524549484253
MemoryTrain:  epoch  7, batch     0 | loss: 2.9292073Losses:  2.158053398132324 -0.0 0.9553359746932983
MemoryTrain:  epoch  7, batch     1 | loss: 2.1580534Losses:  2.3836634159088135 -0.0 1.1069328784942627
MemoryTrain:  epoch  7, batch     2 | loss: 2.3836634Losses:  2.4689745903015137 -0.0 1.0278077125549316
MemoryTrain:  epoch  7, batch     3 | loss: 2.4689746Losses:  2.481642961502075 0.2553178071975708 1.021743655204773
MemoryTrain:  epoch  7, batch     4 | loss: 2.4816430Losses:  2.629256248474121 0.2516298294067383 1.0171293020248413
MemoryTrain:  epoch  7, batch     5 | loss: 2.6292562Losses:  2.4837307929992676 0.23987194895744324 0.9571285843849182
MemoryTrain:  epoch  7, batch     6 | loss: 2.4837308Losses:  2.314781665802002 -0.0 1.0250637531280518
MemoryTrain:  epoch  7, batch     7 | loss: 2.3147817Losses:  2.3382768630981445 -0.0 1.0604571104049683
MemoryTrain:  epoch  7, batch     8 | loss: 2.3382769Losses:  2.9853315353393555 0.5722046494483948 0.971765398979187
MemoryTrain:  epoch  7, batch     9 | loss: 2.9853315Losses:  2.3333373069763184 0.2391100823879242 0.88909912109375
MemoryTrain:  epoch  7, batch    10 | loss: 2.3333373Losses:  2.7422757148742676 0.49884462356567383 0.958130955696106
MemoryTrain:  epoch  7, batch    11 | loss: 2.7422757Losses:  2.747192621231079 0.5002208948135376 0.955940306186676
MemoryTrain:  epoch  7, batch    12 | loss: 2.7471926Losses:  2.4554996490478516 0.24361185729503632 0.9585946798324585
MemoryTrain:  epoch  7, batch    13 | loss: 2.4554996Losses:  2.263437271118164 -0.0 1.0500255823135376
MemoryTrain:  epoch  7, batch    14 | loss: 2.2634373Losses:  2.39920973777771 0.2314959615468979 0.9025976061820984
MemoryTrain:  epoch  8, batch     0 | loss: 2.3992097Losses:  2.916937828063965 0.7487891912460327 0.8583288192749023
MemoryTrain:  epoch  8, batch     1 | loss: 2.9169378Losses:  2.400998115539551 0.257252037525177 0.9079383611679077
MemoryTrain:  epoch  8, batch     2 | loss: 2.4009981Losses:  2.385031223297119 0.22105684876441956 0.8753812313079834
MemoryTrain:  epoch  8, batch     3 | loss: 2.3850312Losses:  2.698485851287842 0.5180789828300476 0.8585028648376465
MemoryTrain:  epoch  8, batch     4 | loss: 2.6984859Losses:  2.5664830207824707 0.50661301612854 0.8475068211555481
MemoryTrain:  epoch  8, batch     5 | loss: 2.5664830Losses:  2.7535600662231445 0.49245238304138184 1.0182589292526245
MemoryTrain:  epoch  8, batch     6 | loss: 2.7535601Losses:  2.257613182067871 -0.0 0.9869870543479919
MemoryTrain:  epoch  8, batch     7 | loss: 2.2576132Losses:  2.34688138961792 -0.0 1.067191243171692
MemoryTrain:  epoch  8, batch     8 | loss: 2.3468814Losses:  2.877697706222534 0.7695354223251343 0.8616001009941101
MemoryTrain:  epoch  8, batch     9 | loss: 2.8776977Losses:  2.516162633895874 0.2565578818321228 0.9000747203826904
MemoryTrain:  epoch  8, batch    10 | loss: 2.5161626Losses:  2.335083484649658 -0.0 1.0668200254440308
MemoryTrain:  epoch  8, batch    11 | loss: 2.3350835Losses:  2.2568798065185547 -0.0 1.0136429071426392
MemoryTrain:  epoch  8, batch    12 | loss: 2.2568798Losses:  2.2724015712738037 -0.0 1.0774171352386475
MemoryTrain:  epoch  8, batch    13 | loss: 2.2724016Losses:  2.6362571716308594 0.2673003375530243 1.0625747442245483
MemoryTrain:  epoch  8, batch    14 | loss: 2.6362572Losses:  2.2991442680358887 -0.0 1.011223316192627
MemoryTrain:  epoch  9, batch     0 | loss: 2.2991443Losses:  2.1685967445373535 -0.0 0.9143650531768799
MemoryTrain:  epoch  9, batch     1 | loss: 2.1685967Losses:  2.5931930541992188 0.23330502212047577 1.0147924423217773
MemoryTrain:  epoch  9, batch     2 | loss: 2.5931931Losses:  2.9518661499023438 0.5657451748847961 0.9811679124832153
MemoryTrain:  epoch  9, batch     3 | loss: 2.9518661Losses:  2.4815406799316406 0.25988835096359253 0.9658335447311401
MemoryTrain:  epoch  9, batch     4 | loss: 2.4815407Losses:  2.8364930152893066 0.7511069774627686 0.834852397441864
MemoryTrain:  epoch  9, batch     5 | loss: 2.8364930Losses:  2.5533559322357178 0.26198601722717285 1.0144256353378296
MemoryTrain:  epoch  9, batch     6 | loss: 2.5533559Losses:  2.2799880504608154 -0.0 1.014770746231079
MemoryTrain:  epoch  9, batch     7 | loss: 2.2799881Losses:  2.1464314460754395 -0.0 0.9087649583816528
MemoryTrain:  epoch  9, batch     8 | loss: 2.1464314Losses:  2.450368881225586 0.24859043955802917 0.9994004368782043
MemoryTrain:  epoch  9, batch     9 | loss: 2.4503689Losses:  2.5112128257751465 0.2471926212310791 1.0610558986663818
MemoryTrain:  epoch  9, batch    10 | loss: 2.5112128Losses:  2.2109012603759766 -0.0 0.971109926700592
MemoryTrain:  epoch  9, batch    11 | loss: 2.2109013Losses:  2.4904370307922363 0.23645853996276855 1.0149140357971191
MemoryTrain:  epoch  9, batch    12 | loss: 2.4904370Losses:  2.5844173431396484 0.26272669434547424 1.0705740451812744
MemoryTrain:  epoch  9, batch    13 | loss: 2.5844173Losses:  2.146672248840332 -0.0 0.9158761501312256
MemoryTrain:  epoch  9, batch    14 | loss: 2.1466722
[EVAL] batch:    0 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 96.88%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 95.83%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 93.75%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 94.79%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 94.64%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 94.53%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 93.75%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 94.32%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 93.75%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   13 | acc: 75.00%,  total acc: 92.41%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 92.50%   [EVAL] batch:   15 | acc: 75.00%,  total acc: 91.41%   [EVAL] batch:   16 | acc: 68.75%,  total acc: 90.07%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 88.89%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 87.50%   [EVAL] batch:   19 | acc: 56.25%,  total acc: 85.94%   [EVAL] batch:   20 | acc: 43.75%,  total acc: 83.93%   [EVAL] batch:   21 | acc: 43.75%,  total acc: 82.10%   [EVAL] batch:   22 | acc: 81.25%,  total acc: 82.07%   [EVAL] batch:   23 | acc: 68.75%,  total acc: 81.51%   [EVAL] batch:   24 | acc: 31.25%,  total acc: 79.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 80.29%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 81.02%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 81.70%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 82.33%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 82.92%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 83.47%   [EVAL] batch:   31 | acc: 75.00%,  total acc: 83.20%   [EVAL] batch:   32 | acc: 81.25%,  total acc: 83.14%   [EVAL] batch:   33 | acc: 62.50%,  total acc: 82.54%   [EVAL] batch:   34 | acc: 68.75%,  total acc: 82.14%   [EVAL] batch:   35 | acc: 56.25%,  total acc: 81.42%   [EVAL] batch:   36 | acc: 56.25%,  total acc: 80.74%   [EVAL] batch:   37 | acc: 68.75%,  total acc: 80.43%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 80.93%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 81.41%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 81.86%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 82.29%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 82.70%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 82.95%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 83.33%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 83.70%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 84.04%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 84.38%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 84.69%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 84.88%   [EVAL] batch:   50 | acc: 81.25%,  total acc: 84.80%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 84.98%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 85.14%   [EVAL] batch:   53 | acc: 93.75%,  total acc: 85.30%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 85.34%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 85.49%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 85.64%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 85.78%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 85.91%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 86.15%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 86.37%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 86.39%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 85.81%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 79.17%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 79.69%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 80.00%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    6 | acc: 50.00%,  total acc: 76.79%   [EVAL] batch:    7 | acc: 56.25%,  total acc: 74.22%   [EVAL] batch:    8 | acc: 62.50%,  total acc: 72.92%   [EVAL] batch:    9 | acc: 56.25%,  total acc: 71.25%   [EVAL] batch:   10 | acc: 31.25%,  total acc: 67.61%   [EVAL] batch:   11 | acc: 56.25%,  total acc: 66.67%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 66.83%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 68.75%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 70.42%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 72.27%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 73.90%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 75.00%   [EVAL] batch:   18 | acc: 87.50%,  total acc: 75.66%   [EVAL] batch:   19 | acc: 68.75%,  total acc: 75.31%   [EVAL] batch:   20 | acc: 62.50%,  total acc: 74.70%   [EVAL] batch:   21 | acc: 56.25%,  total acc: 73.86%   [EVAL] batch:   22 | acc: 68.75%,  total acc: 73.64%   [EVAL] batch:   23 | acc: 62.50%,  total acc: 73.18%   [EVAL] batch:   24 | acc: 56.25%,  total acc: 72.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 73.56%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 74.54%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 75.45%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 76.08%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 76.88%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 77.62%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 78.32%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 78.98%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 79.60%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 80.00%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 80.38%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 80.91%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 81.73%   [EVAL] batch:   39 | acc: 93.75%,  total acc: 82.03%   [EVAL] batch:   40 | acc: 87.50%,  total acc: 82.16%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 82.29%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 82.56%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 82.53%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 82.64%   [EVAL] batch:   45 | acc: 81.25%,  total acc: 82.61%   [EVAL] batch:   46 | acc: 75.00%,  total acc: 82.45%   [EVAL] batch:   47 | acc: 62.50%,  total acc: 82.03%   [EVAL] batch:   48 | acc: 93.75%,  total acc: 82.27%   [EVAL] batch:   49 | acc: 75.00%,  total acc: 82.12%   [EVAL] batch:   50 | acc: 75.00%,  total acc: 81.99%   [EVAL] batch:   51 | acc: 75.00%,  total acc: 81.85%   [EVAL] batch:   52 | acc: 62.50%,  total acc: 81.49%   [EVAL] batch:   53 | acc: 50.00%,  total acc: 80.90%   [EVAL] batch:   54 | acc: 50.00%,  total acc: 80.34%   [EVAL] batch:   55 | acc: 68.75%,  total acc: 80.13%   [EVAL] batch:   56 | acc: 68.75%,  total acc: 79.93%   [EVAL] batch:   57 | acc: 37.50%,  total acc: 79.20%   [EVAL] batch:   58 | acc: 68.75%,  total acc: 79.03%   [EVAL] batch:   59 | acc: 75.00%,  total acc: 78.96%   [EVAL] batch:   60 | acc: 68.75%,  total acc: 78.79%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 78.63%   [EVAL] batch:   62 | acc: 56.25%,  total acc: 78.27%   [EVAL] batch:   63 | acc: 37.50%,  total acc: 77.64%   [EVAL] batch:   64 | acc: 62.50%,  total acc: 77.40%   [EVAL] batch:   65 | acc: 50.00%,  total acc: 76.99%   [EVAL] batch:   66 | acc: 50.00%,  total acc: 76.59%   [EVAL] batch:   67 | acc: 43.75%,  total acc: 76.10%   [EVAL] batch:   68 | acc: 56.25%,  total acc: 75.82%   [EVAL] batch:   69 | acc: 81.25%,  total acc: 75.89%   [EVAL] batch:   70 | acc: 75.00%,  total acc: 75.88%   [EVAL] batch:   71 | acc: 68.75%,  total acc: 75.78%   [EVAL] batch:   72 | acc: 87.50%,  total acc: 75.94%   [EVAL] batch:   73 | acc: 81.25%,  total acc: 76.01%   [EVAL] batch:   74 | acc: 81.25%,  total acc: 76.08%   [EVAL] batch:   75 | acc: 93.75%,  total acc: 76.32%   [EVAL] batch:   76 | acc: 81.25%,  total acc: 76.38%   [EVAL] batch:   77 | acc: 68.75%,  total acc: 76.28%   [EVAL] batch:   78 | acc: 93.75%,  total acc: 76.50%   [EVAL] batch:   79 | acc: 93.75%,  total acc: 76.72%   [EVAL] batch:   80 | acc: 93.75%,  total acc: 76.93%   [EVAL] batch:   81 | acc: 68.75%,  total acc: 76.83%   [EVAL] batch:   82 | acc: 37.50%,  total acc: 76.36%   [EVAL] batch:   83 | acc: 25.00%,  total acc: 75.74%   [EVAL] batch:   84 | acc: 43.75%,  total acc: 75.37%   [EVAL] batch:   85 | acc: 18.75%,  total acc: 74.71%   [EVAL] batch:   86 | acc: 50.00%,  total acc: 74.43%   [EVAL] batch:   87 | acc: 68.75%,  total acc: 74.36%   [EVAL] batch:   88 | acc: 93.75%,  total acc: 74.58%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 74.86%   [EVAL] batch:   90 | acc: 93.75%,  total acc: 75.07%   [EVAL] batch:   91 | acc: 93.75%,  total acc: 75.27%   [EVAL] batch:   92 | acc: 81.25%,  total acc: 75.34%   [EVAL] batch:   93 | acc: 87.50%,  total acc: 75.47%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 75.72%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 75.98%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 76.22%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 76.47%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 76.70%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 76.94%   [EVAL] batch:  100 | acc: 56.25%,  total acc: 76.73%   [EVAL] batch:  101 | acc: 62.50%,  total acc: 76.59%   [EVAL] batch:  102 | acc: 62.50%,  total acc: 76.46%   [EVAL] batch:  103 | acc: 56.25%,  total acc: 76.26%   [EVAL] batch:  104 | acc: 87.50%,  total acc: 76.37%   [EVAL] batch:  105 | acc: 62.50%,  total acc: 76.24%   [EVAL] batch:  106 | acc: 37.50%,  total acc: 75.88%   [EVAL] batch:  107 | acc: 25.00%,  total acc: 75.41%   [EVAL] batch:  108 | acc: 43.75%,  total acc: 75.11%   [EVAL] batch:  109 | acc: 18.75%,  total acc: 74.60%   [EVAL] batch:  110 | acc: 43.75%,  total acc: 74.32%   [EVAL] batch:  111 | acc: 31.25%,  total acc: 73.94%   [EVAL] batch:  112 | acc: 62.50%,  total acc: 73.84%   [EVAL] batch:  113 | acc: 87.50%,  total acc: 73.96%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 74.18%   [EVAL] batch:  115 | acc: 87.50%,  total acc: 74.30%   [EVAL] batch:  116 | acc: 87.50%,  total acc: 74.41%   [EVAL] batch:  117 | acc: 87.50%,  total acc: 74.52%   [EVAL] batch:  118 | acc: 75.00%,  total acc: 74.53%   [EVAL] batch:  119 | acc: 25.00%,  total acc: 74.11%   [EVAL] batch:  120 | acc: 31.25%,  total acc: 73.76%   [EVAL] batch:  121 | acc: 12.50%,  total acc: 73.26%   [EVAL] batch:  122 | acc: 31.25%,  total acc: 72.92%   [EVAL] batch:  123 | acc: 18.75%,  total acc: 72.48%   [EVAL] batch:  124 | acc: 31.25%,  total acc: 72.15%   [EVAL] batch:  125 | acc: 56.25%,  total acc: 72.02%   [EVAL] batch:  126 | acc: 56.25%,  total acc: 71.90%   [EVAL] batch:  127 | acc: 43.75%,  total acc: 71.68%   [EVAL] batch:  128 | acc: 56.25%,  total acc: 71.56%   [EVAL] batch:  129 | acc: 68.75%,  total acc: 71.54%   [EVAL] batch:  130 | acc: 50.00%,  total acc: 71.37%   [EVAL] batch:  131 | acc: 87.50%,  total acc: 71.50%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 71.71%   [EVAL] batch:  133 | acc: 87.50%,  total acc: 71.83%   [EVAL] batch:  134 | acc: 81.25%,  total acc: 71.90%   [EVAL] batch:  135 | acc: 87.50%,  total acc: 72.01%   [EVAL] batch:  136 | acc: 87.50%,  total acc: 72.13%   [EVAL] batch:  137 | acc: 50.00%,  total acc: 71.97%   [EVAL] batch:  138 | acc: 18.75%,  total acc: 71.58%   [EVAL] batch:  139 | acc: 31.25%,  total acc: 71.29%   [EVAL] batch:  140 | acc: 18.75%,  total acc: 70.92%   [EVAL] batch:  141 | acc: 37.50%,  total acc: 70.69%   [EVAL] batch:  142 | acc: 43.75%,  total acc: 70.50%   [EVAL] batch:  143 | acc: 37.50%,  total acc: 70.27%   [EVAL] batch:  144 | acc: 93.75%,  total acc: 70.43%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 70.63%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 70.79%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 70.99%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 71.18%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 71.33%   [EVAL] batch:  150 | acc: 31.25%,  total acc: 71.07%   [EVAL] batch:  151 | acc: 6.25%,  total acc: 70.64%   [EVAL] batch:  152 | acc: 25.00%,  total acc: 70.34%   [EVAL] batch:  153 | acc: 25.00%,  total acc: 70.05%   [EVAL] batch:  154 | acc: 12.50%,  total acc: 69.68%   [EVAL] batch:  155 | acc: 0.00%,  total acc: 69.23%   [EVAL] batch:  156 | acc: 75.00%,  total acc: 69.27%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 69.42%   [EVAL] batch:  158 | acc: 93.75%,  total acc: 69.58%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 69.73%   [EVAL] batch:  160 | acc: 87.50%,  total acc: 69.84%   [EVAL] batch:  161 | acc: 100.00%,  total acc: 70.02%   [EVAL] batch:  162 | acc: 75.00%,  total acc: 70.05%   [EVAL] batch:  163 | acc: 50.00%,  total acc: 69.93%   [EVAL] batch:  164 | acc: 18.75%,  total acc: 69.62%   [EVAL] batch:  165 | acc: 56.25%,  total acc: 69.54%   [EVAL] batch:  166 | acc: 31.25%,  total acc: 69.31%   [EVAL] batch:  167 | acc: 50.00%,  total acc: 69.20%   [EVAL] batch:  168 | acc: 56.25%,  total acc: 69.12%   [EVAL] batch:  169 | acc: 56.25%,  total acc: 69.04%   [EVAL] batch:  170 | acc: 81.25%,  total acc: 69.12%   [EVAL] batch:  171 | acc: 81.25%,  total acc: 69.19%   [EVAL] batch:  172 | acc: 56.25%,  total acc: 69.11%   [EVAL] batch:  173 | acc: 62.50%,  total acc: 69.07%   [EVAL] batch:  174 | acc: 87.50%,  total acc: 69.18%   [EVAL] batch:  175 | acc: 43.75%,  total acc: 69.03%   [EVAL] batch:  176 | acc: 43.75%,  total acc: 68.89%   [EVAL] batch:  177 | acc: 56.25%,  total acc: 68.82%   [EVAL] batch:  178 | acc: 50.00%,  total acc: 68.72%   [EVAL] batch:  179 | acc: 43.75%,  total acc: 68.58%   [EVAL] batch:  180 | acc: 62.50%,  total acc: 68.54%   [EVAL] batch:  181 | acc: 68.75%,  total acc: 68.54%   [EVAL] batch:  182 | acc: 87.50%,  total acc: 68.65%   [EVAL] batch:  183 | acc: 87.50%,  total acc: 68.75%   [EVAL] batch:  184 | acc: 81.25%,  total acc: 68.82%   [EVAL] batch:  185 | acc: 93.75%,  total acc: 68.95%   [EVAL] batch:  186 | acc: 68.75%,  total acc: 68.95%   [EVAL] batch:  187 | acc: 37.50%,  total acc: 68.78%   [EVAL] batch:  188 | acc: 25.00%,  total acc: 68.55%   [EVAL] batch:  189 | acc: 25.00%,  total acc: 68.32%   [EVAL] batch:  190 | acc: 12.50%,  total acc: 68.03%   [EVAL] batch:  191 | acc: 6.25%,  total acc: 67.71%   [EVAL] batch:  192 | acc: 25.00%,  total acc: 67.49%   [EVAL] batch:  193 | acc: 43.75%,  total acc: 67.36%   [EVAL] batch:  194 | acc: 68.75%,  total acc: 67.37%   [EVAL] batch:  195 | acc: 62.50%,  total acc: 67.35%   [EVAL] batch:  196 | acc: 75.00%,  total acc: 67.39%   [EVAL] batch:  197 | acc: 93.75%,  total acc: 67.52%   [EVAL] batch:  198 | acc: 81.25%,  total acc: 67.59%   [EVAL] batch:  199 | acc: 75.00%,  total acc: 67.62%   [EVAL] batch:  200 | acc: 62.50%,  total acc: 67.60%   [EVAL] batch:  201 | acc: 25.00%,  total acc: 67.39%   [EVAL] batch:  202 | acc: 43.75%,  total acc: 67.27%   [EVAL] batch:  203 | acc: 68.75%,  total acc: 67.28%   [EVAL] batch:  204 | acc: 62.50%,  total acc: 67.26%   [EVAL] batch:  205 | acc: 43.75%,  total acc: 67.14%   [EVAL] batch:  206 | acc: 93.75%,  total acc: 67.27%   [EVAL] batch:  207 | acc: 100.00%,  total acc: 67.43%   [EVAL] batch:  208 | acc: 100.00%,  total acc: 67.58%   [EVAL] batch:  209 | acc: 93.75%,  total acc: 67.71%   [EVAL] batch:  210 | acc: 100.00%,  total acc: 67.86%   [EVAL] batch:  211 | acc: 100.00%,  total acc: 68.01%   [EVAL] batch:  212 | acc: 87.50%,  total acc: 68.10%   [EVAL] batch:  213 | acc: 75.00%,  total acc: 68.14%   [EVAL] batch:  214 | acc: 68.75%,  total acc: 68.14%   [EVAL] batch:  215 | acc: 87.50%,  total acc: 68.23%   [EVAL] batch:  216 | acc: 75.00%,  total acc: 68.26%   [EVAL] batch:  217 | acc: 93.75%,  total acc: 68.38%   [EVAL] batch:  218 | acc: 81.25%,  total acc: 68.44%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 68.58%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 68.72%   [EVAL] batch:  221 | acc: 87.50%,  total acc: 68.81%   [EVAL] batch:  222 | acc: 93.75%,  total acc: 68.92%   [EVAL] batch:  223 | acc: 93.75%,  total acc: 69.03%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 69.17%   [EVAL] batch:  225 | acc: 31.25%,  total acc: 69.00%   [EVAL] batch:  226 | acc: 12.50%,  total acc: 68.75%   [EVAL] batch:  227 | acc: 18.75%,  total acc: 68.53%   [EVAL] batch:  228 | acc: 12.50%,  total acc: 68.29%   [EVAL] batch:  229 | acc: 6.25%,  total acc: 68.02%   [EVAL] batch:  230 | acc: 25.00%,  total acc: 67.83%   [EVAL] batch:  231 | acc: 75.00%,  total acc: 67.86%   [EVAL] batch:  232 | acc: 81.25%,  total acc: 67.92%   [EVAL] batch:  233 | acc: 87.50%,  total acc: 68.00%   [EVAL] batch:  234 | acc: 87.50%,  total acc: 68.09%   [EVAL] batch:  235 | acc: 56.25%,  total acc: 68.03%   [EVAL] batch:  236 | acc: 56.25%,  total acc: 67.99%   [EVAL] batch:  237 | acc: 62.50%,  total acc: 67.96%   [EVAL] batch:  238 | acc: 62.50%,  total acc: 67.94%   [EVAL] batch:  239 | acc: 37.50%,  total acc: 67.81%   [EVAL] batch:  240 | acc: 50.00%,  total acc: 67.74%   [EVAL] batch:  241 | acc: 50.00%,  total acc: 67.67%   [EVAL] batch:  242 | acc: 43.75%,  total acc: 67.57%   [EVAL] batch:  243 | acc: 37.50%,  total acc: 67.44%   [EVAL] batch:  244 | acc: 62.50%,  total acc: 67.42%   [EVAL] batch:  245 | acc: 75.00%,  total acc: 67.45%   [EVAL] batch:  246 | acc: 62.50%,  total acc: 67.43%   [EVAL] batch:  247 | acc: 75.00%,  total acc: 67.46%   [EVAL] batch:  248 | acc: 87.50%,  total acc: 67.55%   [EVAL] batch:  249 | acc: 68.75%,  total acc: 67.55%   [EVAL] batch:  250 | acc: 0.00%,  total acc: 67.28%   [EVAL] batch:  251 | acc: 12.50%,  total acc: 67.06%   [EVAL] batch:  252 | acc: 0.00%,  total acc: 66.80%   [EVAL] batch:  253 | acc: 0.00%,  total acc: 66.54%   [EVAL] batch:  254 | acc: 0.00%,  total acc: 66.27%   [EVAL] batch:  255 | acc: 0.00%,  total acc: 66.02%   [EVAL] batch:  256 | acc: 56.25%,  total acc: 65.98%   [EVAL] batch:  257 | acc: 75.00%,  total acc: 66.01%   [EVAL] batch:  258 | acc: 87.50%,  total acc: 66.10%   [EVAL] batch:  259 | acc: 87.50%,  total acc: 66.18%   [EVAL] batch:  260 | acc: 87.50%,  total acc: 66.26%   [EVAL] batch:  261 | acc: 75.00%,  total acc: 66.29%   [EVAL] batch:  262 | acc: 81.25%,  total acc: 66.35%   [EVAL] batch:  263 | acc: 56.25%,  total acc: 66.31%   [EVAL] batch:  264 | acc: 68.75%,  total acc: 66.32%   [EVAL] batch:  265 | acc: 81.25%,  total acc: 66.38%   [EVAL] batch:  266 | acc: 62.50%,  total acc: 66.36%   [EVAL] batch:  267 | acc: 50.00%,  total acc: 66.30%   [EVAL] batch:  268 | acc: 37.50%,  total acc: 66.19%   [EVAL] batch:  269 | acc: 31.25%,  total acc: 66.06%   [EVAL] batch:  270 | acc: 18.75%,  total acc: 65.89%   [EVAL] batch:  271 | acc: 43.75%,  total acc: 65.81%   [EVAL] batch:  272 | acc: 31.25%,  total acc: 65.68%   [EVAL] batch:  273 | acc: 31.25%,  total acc: 65.56%   [EVAL] batch:  274 | acc: 12.50%,  total acc: 65.36%   [EVAL] batch:  275 | acc: 6.25%,  total acc: 65.15%   [EVAL] batch:  276 | acc: 6.25%,  total acc: 64.94%   [EVAL] batch:  277 | acc: 12.50%,  total acc: 64.75%   [EVAL] batch:  278 | acc: 6.25%,  total acc: 64.54%   [EVAL] batch:  279 | acc: 12.50%,  total acc: 64.35%   [EVAL] batch:  280 | acc: 0.00%,  total acc: 64.12%   [EVAL] batch:  281 | acc: 62.50%,  total acc: 64.12%   [EVAL] batch:  282 | acc: 87.50%,  total acc: 64.20%   [EVAL] batch:  283 | acc: 81.25%,  total acc: 64.26%   [EVAL] batch:  284 | acc: 93.75%,  total acc: 64.36%   [EVAL] batch:  285 | acc: 81.25%,  total acc: 64.42%   [EVAL] batch:  286 | acc: 81.25%,  total acc: 64.48%   [EVAL] batch:  287 | acc: 87.50%,  total acc: 64.56%   [EVAL] batch:  288 | acc: 93.75%,  total acc: 64.66%   [EVAL] batch:  289 | acc: 100.00%,  total acc: 64.78%   [EVAL] batch:  290 | acc: 100.00%,  total acc: 64.91%   [EVAL] batch:  291 | acc: 87.50%,  total acc: 64.98%   [EVAL] batch:  292 | acc: 100.00%,  total acc: 65.10%   [EVAL] batch:  293 | acc: 87.50%,  total acc: 65.18%   [EVAL] batch:  294 | acc: 43.75%,  total acc: 65.11%   [EVAL] batch:  295 | acc: 18.75%,  total acc: 64.95%   [EVAL] batch:  296 | acc: 43.75%,  total acc: 64.88%   [EVAL] batch:  297 | acc: 50.00%,  total acc: 64.83%   [EVAL] batch:  298 | acc: 50.00%,  total acc: 64.78%   [EVAL] batch:  299 | acc: 50.00%,  total acc: 64.73%   [EVAL] batch:  300 | acc: 43.75%,  total acc: 64.66%   [EVAL] batch:  301 | acc: 68.75%,  total acc: 64.67%   [EVAL] batch:  302 | acc: 37.50%,  total acc: 64.58%   [EVAL] batch:  303 | acc: 81.25%,  total acc: 64.64%   [EVAL] batch:  304 | acc: 56.25%,  total acc: 64.61%   [EVAL] batch:  305 | acc: 75.00%,  total acc: 64.64%   [EVAL] batch:  306 | acc: 50.00%,  total acc: 64.60%   [EVAL] batch:  307 | acc: 56.25%,  total acc: 64.57%   [EVAL] batch:  308 | acc: 37.50%,  total acc: 64.48%   [EVAL] batch:  309 | acc: 68.75%,  total acc: 64.50%   [EVAL] batch:  310 | acc: 50.00%,  total acc: 64.45%   [EVAL] batch:  311 | acc: 87.50%,  total acc: 64.52%   [EVAL] batch:  312 | acc: 50.00%,  total acc: 64.48%   [EVAL] batch:  313 | acc: 62.50%,  total acc: 64.47%   [EVAL] batch:  314 | acc: 75.00%,  total acc: 64.50%   [EVAL] batch:  315 | acc: 43.75%,  total acc: 64.44%   [EVAL] batch:  316 | acc: 50.00%,  total acc: 64.39%   [EVAL] batch:  317 | acc: 62.50%,  total acc: 64.39%   [EVAL] batch:  318 | acc: 62.50%,  total acc: 64.38%   [EVAL] batch:  319 | acc: 100.00%,  total acc: 64.49%   [EVAL] batch:  320 | acc: 93.75%,  total acc: 64.58%   [EVAL] batch:  321 | acc: 87.50%,  total acc: 64.65%   [EVAL] batch:  322 | acc: 93.75%,  total acc: 64.74%   [EVAL] batch:  323 | acc: 100.00%,  total acc: 64.85%   [EVAL] batch:  324 | acc: 75.00%,  total acc: 64.88%   [EVAL] batch:  325 | acc: 75.00%,  total acc: 64.92%   [EVAL] batch:  326 | acc: 56.25%,  total acc: 64.89%   [EVAL] batch:  327 | acc: 75.00%,  total acc: 64.92%   [EVAL] batch:  328 | acc: 75.00%,  total acc: 64.95%   [EVAL] batch:  329 | acc: 87.50%,  total acc: 65.02%   [EVAL] batch:  330 | acc: 68.75%,  total acc: 65.03%   [EVAL] batch:  331 | acc: 68.75%,  total acc: 65.04%   [EVAL] batch:  332 | acc: 62.50%,  total acc: 65.03%   [EVAL] batch:  333 | acc: 87.50%,  total acc: 65.10%   [EVAL] batch:  334 | acc: 68.75%,  total acc: 65.11%   [EVAL] batch:  335 | acc: 68.75%,  total acc: 65.12%   [EVAL] batch:  336 | acc: 56.25%,  total acc: 65.10%   [EVAL] batch:  337 | acc: 75.00%,  total acc: 65.13%   [EVAL] batch:  338 | acc: 56.25%,  total acc: 65.10%   [EVAL] batch:  339 | acc: 50.00%,  total acc: 65.06%   [EVAL] batch:  340 | acc: 43.75%,  total acc: 64.99%   [EVAL] batch:  341 | acc: 37.50%,  total acc: 64.91%   [EVAL] batch:  342 | acc: 43.75%,  total acc: 64.85%   [EVAL] batch:  343 | acc: 37.50%,  total acc: 64.77%   [EVAL] batch:  344 | acc: 68.75%,  total acc: 64.78%   [EVAL] batch:  345 | acc: 56.25%,  total acc: 64.76%   [EVAL] batch:  346 | acc: 62.50%,  total acc: 64.75%   [EVAL] batch:  347 | acc: 62.50%,  total acc: 64.74%   [EVAL] batch:  348 | acc: 75.00%,  total acc: 64.77%   [EVAL] batch:  349 | acc: 62.50%,  total acc: 64.77%   [EVAL] batch:  350 | acc: 56.25%,  total acc: 64.74%   [EVAL] batch:  351 | acc: 68.75%,  total acc: 64.75%   [EVAL] batch:  352 | acc: 75.00%,  total acc: 64.78%   [EVAL] batch:  353 | acc: 75.00%,  total acc: 64.81%   [EVAL] batch:  354 | acc: 75.00%,  total acc: 64.84%   [EVAL] batch:  355 | acc: 68.75%,  total acc: 64.85%   [EVAL] batch:  356 | acc: 43.75%,  total acc: 64.79%   [EVAL] batch:  357 | acc: 18.75%,  total acc: 64.66%   [EVAL] batch:  358 | acc: 18.75%,  total acc: 64.54%   [EVAL] batch:  359 | acc: 43.75%,  total acc: 64.48%   [EVAL] batch:  360 | acc: 18.75%,  total acc: 64.35%   [EVAL] batch:  361 | acc: 50.00%,  total acc: 64.31%   [EVAL] batch:  362 | acc: 56.25%,  total acc: 64.29%   [EVAL] batch:  363 | acc: 100.00%,  total acc: 64.39%   [EVAL] batch:  364 | acc: 100.00%,  total acc: 64.49%   [EVAL] batch:  365 | acc: 93.75%,  total acc: 64.57%   [EVAL] batch:  366 | acc: 100.00%,  total acc: 64.66%   [EVAL] batch:  367 | acc: 100.00%,  total acc: 64.76%   [EVAL] batch:  368 | acc: 93.75%,  total acc: 64.84%   [EVAL] batch:  369 | acc: 81.25%,  total acc: 64.88%   [EVAL] batch:  370 | acc: 75.00%,  total acc: 64.91%   [EVAL] batch:  371 | acc: 87.50%,  total acc: 64.97%   [EVAL] batch:  372 | acc: 87.50%,  total acc: 65.03%   [EVAL] batch:  373 | acc: 81.25%,  total acc: 65.07%   [EVAL] batch:  374 | acc: 75.00%,  total acc: 65.10%   [EVAL] batch:  375 | acc: 37.50%,  total acc: 65.03%   [EVAL] batch:  376 | acc: 43.75%,  total acc: 64.97%   [EVAL] batch:  377 | acc: 37.50%,  total acc: 64.90%   [EVAL] batch:  378 | acc: 37.50%,  total acc: 64.83%   [EVAL] batch:  379 | acc: 43.75%,  total acc: 64.77%   [EVAL] batch:  380 | acc: 25.00%,  total acc: 64.67%   [EVAL] batch:  381 | acc: 43.75%,  total acc: 64.61%   [EVAL] batch:  382 | acc: 62.50%,  total acc: 64.61%   [EVAL] batch:  383 | acc: 68.75%,  total acc: 64.62%   [EVAL] batch:  384 | acc: 75.00%,  total acc: 64.64%   [EVAL] batch:  385 | acc: 50.00%,  total acc: 64.60%   [EVAL] batch:  386 | acc: 62.50%,  total acc: 64.60%   [EVAL] batch:  387 | acc: 56.25%,  total acc: 64.58%   [EVAL] batch:  388 | acc: 56.25%,  total acc: 64.56%   [EVAL] batch:  389 | acc: 56.25%,  total acc: 64.54%   [EVAL] batch:  390 | acc: 18.75%,  total acc: 64.42%   [EVAL] batch:  391 | acc: 50.00%,  total acc: 64.38%   [EVAL] batch:  392 | acc: 56.25%,  total acc: 64.36%   [EVAL] batch:  393 | acc: 87.50%,  total acc: 64.42%   [EVAL] batch:  394 | acc: 93.75%,  total acc: 64.49%   [EVAL] batch:  395 | acc: 100.00%,  total acc: 64.58%   [EVAL] batch:  396 | acc: 100.00%,  total acc: 64.67%   [EVAL] batch:  397 | acc: 100.00%,  total acc: 64.76%   [EVAL] batch:  398 | acc: 93.75%,  total acc: 64.83%   [EVAL] batch:  399 | acc: 100.00%,  total acc: 64.92%   [EVAL] batch:  400 | acc: 56.25%,  total acc: 64.90%   [EVAL] batch:  401 | acc: 31.25%,  total acc: 64.82%   [EVAL] batch:  402 | acc: 56.25%,  total acc: 64.80%   [EVAL] batch:  403 | acc: 68.75%,  total acc: 64.81%   [EVAL] batch:  404 | acc: 31.25%,  total acc: 64.72%   [EVAL] batch:  405 | acc: 50.00%,  total acc: 64.69%   [EVAL] batch:  406 | acc: 62.50%,  total acc: 64.68%   [EVAL] batch:  407 | acc: 62.50%,  total acc: 64.68%   [EVAL] batch:  408 | acc: 25.00%,  total acc: 64.58%   [EVAL] batch:  409 | acc: 31.25%,  total acc: 64.50%   [EVAL] batch:  410 | acc: 18.75%,  total acc: 64.39%   [EVAL] batch:  411 | acc: 18.75%,  total acc: 64.27%   [EVAL] batch:  412 | acc: 37.50%,  total acc: 64.21%   [EVAL] batch:  413 | acc: 31.25%,  total acc: 64.13%   [EVAL] batch:  414 | acc: 37.50%,  total acc: 64.07%   [EVAL] batch:  415 | acc: 37.50%,  total acc: 64.00%   [EVAL] batch:  416 | acc: 25.00%,  total acc: 63.91%   [EVAL] batch:  417 | acc: 50.00%,  total acc: 63.88%   [EVAL] batch:  418 | acc: 31.25%,  total acc: 63.80%   [EVAL] batch:  419 | acc: 68.75%,  total acc: 63.81%   [EVAL] batch:  420 | acc: 62.50%,  total acc: 63.81%   [EVAL] batch:  421 | acc: 81.25%,  total acc: 63.85%   [EVAL] batch:  422 | acc: 68.75%,  total acc: 63.86%   [EVAL] batch:  423 | acc: 68.75%,  total acc: 63.87%   [EVAL] batch:  424 | acc: 81.25%,  total acc: 63.91%   [EVAL] batch:  425 | acc: 100.00%,  total acc: 64.00%   [EVAL] batch:  426 | acc: 100.00%,  total acc: 64.08%   [EVAL] batch:  427 | acc: 100.00%,  total acc: 64.16%   [EVAL] batch:  428 | acc: 100.00%,  total acc: 64.25%   [EVAL] batch:  429 | acc: 100.00%,  total acc: 64.33%   [EVAL] batch:  430 | acc: 100.00%,  total acc: 64.41%   [EVAL] batch:  431 | acc: 100.00%,  total acc: 64.50%   [EVAL] batch:  432 | acc: 87.50%,  total acc: 64.55%   [EVAL] batch:  433 | acc: 93.75%,  total acc: 64.62%   [EVAL] batch:  434 | acc: 100.00%,  total acc: 64.70%   [EVAL] batch:  435 | acc: 93.75%,  total acc: 64.76%   [EVAL] batch:  436 | acc: 100.00%,  total acc: 64.85%   [EVAL] batch:  437 | acc: 100.00%,  total acc: 64.93%   [EVAL] batch:  438 | acc: 93.75%,  total acc: 64.99%   [EVAL] batch:  439 | acc: 93.75%,  total acc: 65.06%   [EVAL] batch:  440 | acc: 100.00%,  total acc: 65.14%   [EVAL] batch:  441 | acc: 81.25%,  total acc: 65.17%   [EVAL] batch:  442 | acc: 100.00%,  total acc: 65.25%   [EVAL] batch:  443 | acc: 100.00%,  total acc: 65.33%   [EVAL] batch:  444 | acc: 87.50%,  total acc: 65.38%   [EVAL] batch:  445 | acc: 93.75%,  total acc: 65.44%   [EVAL] batch:  446 | acc: 87.50%,  total acc: 65.49%   [EVAL] batch:  447 | acc: 100.00%,  total acc: 65.57%   [EVAL] batch:  448 | acc: 93.75%,  total acc: 65.63%   [EVAL] batch:  449 | acc: 93.75%,  total acc: 65.69%   [EVAL] batch:  450 | acc: 75.00%,  total acc: 65.72%   [EVAL] batch:  451 | acc: 87.50%,  total acc: 65.76%   [EVAL] batch:  452 | acc: 93.75%,  total acc: 65.83%   [EVAL] batch:  453 | acc: 56.25%,  total acc: 65.80%   [EVAL] batch:  454 | acc: 75.00%,  total acc: 65.82%   [EVAL] batch:  455 | acc: 62.50%,  total acc: 65.82%   [EVAL] batch:  456 | acc: 75.00%,  total acc: 65.84%   [EVAL] batch:  457 | acc: 43.75%,  total acc: 65.79%   [EVAL] batch:  458 | acc: 43.75%,  total acc: 65.74%   [EVAL] batch:  459 | acc: 56.25%,  total acc: 65.72%   [EVAL] batch:  460 | acc: 81.25%,  total acc: 65.75%   [EVAL] batch:  461 | acc: 37.50%,  total acc: 65.69%   [EVAL] batch:  462 | acc: 75.00%,  total acc: 65.71%   [EVAL] batch:  463 | acc: 100.00%,  total acc: 65.79%   [EVAL] batch:  464 | acc: 100.00%,  total acc: 65.86%   [EVAL] batch:  465 | acc: 100.00%,  total acc: 65.93%   [EVAL] batch:  466 | acc: 100.00%,  total acc: 66.01%   [EVAL] batch:  467 | acc: 100.00%,  total acc: 66.08%   [EVAL] batch:  468 | acc: 93.75%,  total acc: 66.14%   [EVAL] batch:  469 | acc: 68.75%,  total acc: 66.14%   [EVAL] batch:  470 | acc: 81.25%,  total acc: 66.18%   [EVAL] batch:  471 | acc: 62.50%,  total acc: 66.17%   [EVAL] batch:  472 | acc: 56.25%,  total acc: 66.15%   [EVAL] batch:  473 | acc: 68.75%,  total acc: 66.15%   [EVAL] batch:  474 | acc: 37.50%,  total acc: 66.09%   [EVAL] batch:  475 | acc: 100.00%,  total acc: 66.16%   [EVAL] batch:  476 | acc: 100.00%,  total acc: 66.23%   [EVAL] batch:  477 | acc: 100.00%,  total acc: 66.30%   [EVAL] batch:  478 | acc: 100.00%,  total acc: 66.38%   [EVAL] batch:  479 | acc: 100.00%,  total acc: 66.45%   [EVAL] batch:  480 | acc: 93.75%,  total acc: 66.50%   [EVAL] batch:  481 | acc: 100.00%,  total acc: 66.57%   [EVAL] batch:  482 | acc: 100.00%,  total acc: 66.64%   [EVAL] batch:  483 | acc: 100.00%,  total acc: 66.71%   [EVAL] batch:  484 | acc: 100.00%,  total acc: 66.78%   [EVAL] batch:  485 | acc: 100.00%,  total acc: 66.85%   [EVAL] batch:  486 | acc: 93.75%,  total acc: 66.90%   [EVAL] batch:  487 | acc: 87.50%,  total acc: 66.94%   [EVAL] batch:  488 | acc: 87.50%,  total acc: 66.99%   [EVAL] batch:  489 | acc: 93.75%,  total acc: 67.04%   [EVAL] batch:  490 | acc: 100.00%,  total acc: 67.11%   [EVAL] batch:  491 | acc: 87.50%,  total acc: 67.15%   [EVAL] batch:  492 | acc: 93.75%,  total acc: 67.20%   [EVAL] batch:  493 | acc: 87.50%,  total acc: 67.24%   [EVAL] batch:  494 | acc: 100.00%,  total acc: 67.31%   [EVAL] batch:  495 | acc: 87.50%,  total acc: 67.35%   [EVAL] batch:  496 | acc: 100.00%,  total acc: 67.42%   [EVAL] batch:  497 | acc: 100.00%,  total acc: 67.48%   [EVAL] batch:  498 | acc: 93.75%,  total acc: 67.54%   [EVAL] batch:  499 | acc: 93.75%,  total acc: 67.59%   
cur_acc:  ['0.9524', '0.6994', '0.7639', '0.6895', '0.5843', '0.7004', '0.6200', '0.8581']
his_acc:  ['0.9524', '0.8220', '0.7949', '0.7342', '0.7037', '0.7017', '0.6701', '0.6759']
----------END
his_acc mean:  [0.9463 0.8304 0.7811 0.7466 0.736  0.7095 0.685  0.6613]
