#############params############
cuda:0
Task=Tacred, 5-shot
Encoding model: bert
pattern=hybridprompt
mem=1, margin=0.3, gen=1, gen_num=2
#############params############
--------Round  0
seed:  100
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/test.pkl
Task_order: [7 3 0 5 4 1 6 2]
prepared data!
CurrentTrain: epoch  0, batch     0 | loss: 13.3724613CurrentTrain: epoch  0, batch     1 | loss: 13.1578608CurrentTrain: epoch  0, batch     2 | loss: 13.0914288CurrentTrain: epoch  0, batch     3 | loss: 12.9665594CurrentTrain: epoch  0, batch     4 | loss: 12.8442793CurrentTrain: epoch  0, batch     5 | loss: 12.5822926CurrentTrain: epoch  0, batch     6 | loss: 12.5752811CurrentTrain: epoch  0, batch     7 | loss: 12.3465538CurrentTrain: epoch  0, batch     8 | loss: 12.2965069CurrentTrain: epoch  0, batch     9 | loss: 12.1475677CurrentTrain: epoch  0, batch    10 | loss: 12.0400219CurrentTrain: epoch  0, batch    11 | loss: 11.9480858CurrentTrain: epoch  0, batch    12 | loss: 12.0638752CurrentTrain: epoch  0, batch    13 | loss: 11.8167038CurrentTrain: epoch  0, batch    14 | loss: 11.6889286CurrentTrain: epoch  0, batch    15 | loss: 11.6254883CurrentTrain: epoch  0, batch    16 | loss: 11.0940533CurrentTrain: epoch  0, batch    17 | loss: 11.2020321CurrentTrain: epoch  0, batch    18 | loss: 11.2489090CurrentTrain: epoch  0, batch    19 | loss: 11.4948883CurrentTrain: epoch  0, batch    20 | loss: 11.0849552CurrentTrain: epoch  0, batch    21 | loss: 11.3767891CurrentTrain: epoch  0, batch    22 | loss: 11.4738388CurrentTrain: epoch  0, batch    23 | loss: 11.0327606CurrentTrain: epoch  0, batch    24 | loss: 11.3491087CurrentTrain: epoch  0, batch    25 | loss: 11.3099518CurrentTrain: epoch  0, batch    26 | loss: 10.8578224CurrentTrain: epoch  0, batch    27 | loss: 10.2841034CurrentTrain: epoch  0, batch    28 | loss: 10.7051201CurrentTrain: epoch  0, batch    29 | loss: 10.7940836CurrentTrain: epoch  0, batch    30 | loss: 10.4195404CurrentTrain: epoch  0, batch    31 | loss: 10.7609634CurrentTrain: epoch  0, batch    32 | loss: 10.3042355CurrentTrain: epoch  0, batch    33 | loss: 10.3129387CurrentTrain: epoch  0, batch    34 | loss: 10.0250797CurrentTrain: epoch  0, batch    35 | loss: 10.2144766CurrentTrain: epoch  0, batch    36 | loss: 10.2306309CurrentTrain: epoch  0, batch    37 | loss: 10.0852337CurrentTrain: epoch  1, batch     0 | loss: 10.2014656CurrentTrain: epoch  1, batch     1 | loss: 10.6530342CurrentTrain: epoch  1, batch     2 | loss: 9.7890358CurrentTrain: epoch  1, batch     3 | loss: 9.7257767CurrentTrain: epoch  1, batch     4 | loss: 9.4564104CurrentTrain: epoch  1, batch     5 | loss: 10.1626291CurrentTrain: epoch  1, batch     6 | loss: 9.8237648CurrentTrain: epoch  1, batch     7 | loss: 9.4988441CurrentTrain: epoch  1, batch     8 | loss: 9.4991245CurrentTrain: epoch  1, batch     9 | loss: 9.6452885CurrentTrain: epoch  1, batch    10 | loss: 9.9743004CurrentTrain: epoch  1, batch    11 | loss: 9.7963219CurrentTrain: epoch  1, batch    12 | loss: 9.6937313CurrentTrain: epoch  1, batch    13 | loss: 9.1966829CurrentTrain: epoch  1, batch    14 | loss: 9.5502548CurrentTrain: epoch  1, batch    15 | loss: 9.2058496CurrentTrain: epoch  1, batch    16 | loss: 9.1888971CurrentTrain: epoch  1, batch    17 | loss: 9.5938950CurrentTrain: epoch  1, batch    18 | loss: 9.4860392CurrentTrain: epoch  1, batch    19 | loss: 9.5833225CurrentTrain: epoch  1, batch    20 | loss: 9.5087862CurrentTrain: epoch  1, batch    21 | loss: 9.1296339CurrentTrain: epoch  1, batch    22 | loss: 9.2063999CurrentTrain: epoch  1, batch    23 | loss: 9.0002327CurrentTrain: epoch  1, batch    24 | loss: 9.2855091CurrentTrain: epoch  1, batch    25 | loss: 8.4403152CurrentTrain: epoch  1, batch    26 | loss: 9.3554955CurrentTrain: epoch  1, batch    27 | loss: 8.4032183CurrentTrain: epoch  1, batch    28 | loss: 8.6823807CurrentTrain: epoch  1, batch    29 | loss: 8.0479479CurrentTrain: epoch  1, batch    30 | loss: 8.6466703CurrentTrain: epoch  1, batch    31 | loss: 9.0862045CurrentTrain: epoch  1, batch    32 | loss: 8.8169594CurrentTrain: epoch  1, batch    33 | loss: 8.2732611CurrentTrain: epoch  1, batch    34 | loss: 8.6010847CurrentTrain: epoch  1, batch    35 | loss: 8.1362267CurrentTrain: epoch  1, batch    36 | loss: 8.8653030CurrentTrain: epoch  1, batch    37 | loss: 8.5752344CurrentTrain: epoch  2, batch     0 | loss: 7.7833271CurrentTrain: epoch  2, batch     1 | loss: 8.4801159CurrentTrain: epoch  2, batch     2 | loss: 8.7112026CurrentTrain: epoch  2, batch     3 | loss: 8.3481588CurrentTrain: epoch  2, batch     4 | loss: 9.3701601CurrentTrain: epoch  2, batch     5 | loss: 9.5443459CurrentTrain: epoch  2, batch     6 | loss: 8.6426182CurrentTrain: epoch  2, batch     7 | loss: 8.3060379CurrentTrain: epoch  2, batch     8 | loss: 8.6485052CurrentTrain: epoch  2, batch     9 | loss: 8.4162703CurrentTrain: epoch  2, batch    10 | loss: 8.0237541CurrentTrain: epoch  2, batch    11 | loss: 8.5527153CurrentTrain: epoch  2, batch    12 | loss: 8.2105560CurrentTrain: epoch  2, batch    13 | loss: 7.9770279CurrentTrain: epoch  2, batch    14 | loss: 7.3642979CurrentTrain: epoch  2, batch    15 | loss: 7.9607420CurrentTrain: epoch  2, batch    16 | loss: 8.3288574CurrentTrain: epoch  2, batch    17 | loss: 8.4686508CurrentTrain: epoch  2, batch    18 | loss: 8.0185461CurrentTrain: epoch  2, batch    19 | loss: 7.8661394CurrentTrain: epoch  2, batch    20 | loss: 7.6271992CurrentTrain: epoch  2, batch    21 | loss: 7.9648695CurrentTrain: epoch  2, batch    22 | loss: 7.4678106CurrentTrain: epoch  2, batch    23 | loss: 7.4899635CurrentTrain: epoch  2, batch    24 | loss: 7.8132114CurrentTrain: epoch  2, batch    25 | loss: 8.1924648CurrentTrain: epoch  2, batch    26 | loss: 7.2895327CurrentTrain: epoch  2, batch    27 | loss: 8.5118685CurrentTrain: epoch  2, batch    28 | loss: 6.8255653CurrentTrain: epoch  2, batch    29 | loss: 7.8991265CurrentTrain: epoch  2, batch    30 | loss: 7.5748453CurrentTrain: epoch  2, batch    31 | loss: 7.1347771CurrentTrain: epoch  2, batch    32 | loss: 7.6812344CurrentTrain: epoch  2, batch    33 | loss: 7.5430918CurrentTrain: epoch  2, batch    34 | loss: 8.4325619CurrentTrain: epoch  2, batch    35 | loss: 7.4143143CurrentTrain: epoch  2, batch    36 | loss: 7.8018699CurrentTrain: epoch  2, batch    37 | loss: 7.7435017CurrentTrain: epoch  3, batch     0 | loss: 7.5232515CurrentTrain: epoch  3, batch     1 | loss: 7.8361187CurrentTrain: epoch  3, batch     2 | loss: 7.8839965CurrentTrain: epoch  3, batch     3 | loss: 7.9287844CurrentTrain: epoch  3, batch     4 | loss: 7.4654551CurrentTrain: epoch  3, batch     5 | loss: 7.8325772CurrentTrain: epoch  3, batch     6 | loss: 8.4771061CurrentTrain: epoch  3, batch     7 | loss: 6.9571447CurrentTrain: epoch  3, batch     8 | loss: 8.0827675CurrentTrain: epoch  3, batch     9 | loss: 7.7508407CurrentTrain: epoch  3, batch    10 | loss: 7.0086536CurrentTrain: epoch  3, batch    11 | loss: 6.6070518CurrentTrain: epoch  3, batch    12 | loss: 7.7624445CurrentTrain: epoch  3, batch    13 | loss: 8.1287346CurrentTrain: epoch  3, batch    14 | loss: 7.2786741CurrentTrain: epoch  3, batch    15 | loss: 7.4686642CurrentTrain: epoch  3, batch    16 | loss: 8.3591795CurrentTrain: epoch  3, batch    17 | loss: 7.4006367CurrentTrain: epoch  3, batch    18 | loss: 7.4785576CurrentTrain: epoch  3, batch    19 | loss: 7.9789510CurrentTrain: epoch  3, batch    20 | loss: 7.5735807CurrentTrain: epoch  3, batch    21 | loss: 7.4664931CurrentTrain: epoch  3, batch    22 | loss: 7.6469474CurrentTrain: epoch  3, batch    23 | loss: 7.7438240CurrentTrain: epoch  3, batch    24 | loss: 6.4601979CurrentTrain: epoch  3, batch    25 | loss: 6.9148345CurrentTrain: epoch  3, batch    26 | loss: 6.8151793CurrentTrain: epoch  3, batch    27 | loss: 7.9257421CurrentTrain: epoch  3, batch    28 | loss: 7.3877330CurrentTrain: epoch  3, batch    29 | loss: 6.2554440CurrentTrain: epoch  3, batch    30 | loss: 7.2944155CurrentTrain: epoch  3, batch    31 | loss: 6.9774604CurrentTrain: epoch  3, batch    32 | loss: 6.2400208CurrentTrain: epoch  3, batch    33 | loss: 6.4645548CurrentTrain: epoch  3, batch    34 | loss: 6.7722120CurrentTrain: epoch  3, batch    35 | loss: 6.4985266CurrentTrain: epoch  3, batch    36 | loss: 6.8035321CurrentTrain: epoch  3, batch    37 | loss: 6.8702993CurrentTrain: epoch  4, batch     0 | loss: 7.0334969CurrentTrain: epoch  4, batch     1 | loss: 7.0049124CurrentTrain: epoch  4, batch     2 | loss: 5.8247557CurrentTrain: epoch  4, batch     3 | loss: 6.8830619CurrentTrain: epoch  4, batch     4 | loss: 6.9699078CurrentTrain: epoch  4, batch     5 | loss: 6.9527235CurrentTrain: epoch  4, batch     6 | loss: 6.2295132CurrentTrain: epoch  4, batch     7 | loss: 6.9355526CurrentTrain: epoch  4, batch     8 | loss: 7.7385564CurrentTrain: epoch  4, batch     9 | loss: 7.0533419CurrentTrain: epoch  4, batch    10 | loss: 7.2315435CurrentTrain: epoch  4, batch    11 | loss: 6.2178564CurrentTrain: epoch  4, batch    12 | loss: 6.9987230CurrentTrain: epoch  4, batch    13 | loss: 6.5305729CurrentTrain: epoch  4, batch    14 | loss: 6.8219061CurrentTrain: epoch  4, batch    15 | loss: 7.0248032CurrentTrain: epoch  4, batch    16 | loss: 6.7223358CurrentTrain: epoch  4, batch    17 | loss: 6.6297302CurrentTrain: epoch  4, batch    18 | loss: 6.2468553CurrentTrain: epoch  4, batch    19 | loss: 6.3774834CurrentTrain: epoch  4, batch    20 | loss: 6.7463589CurrentTrain: epoch  4, batch    21 | loss: 7.4178104CurrentTrain: epoch  4, batch    22 | loss: 6.9252653CurrentTrain: epoch  4, batch    23 | loss: 6.1513681CurrentTrain: epoch  4, batch    24 | loss: 7.1817818CurrentTrain: epoch  4, batch    25 | loss: 7.3936548CurrentTrain: epoch  4, batch    26 | loss: 6.3802161CurrentTrain: epoch  4, batch    27 | loss: 8.6171103CurrentTrain: epoch  4, batch    28 | loss: 6.6908755CurrentTrain: epoch  4, batch    29 | loss: 6.6532860CurrentTrain: epoch  4, batch    30 | loss: 6.7164712CurrentTrain: epoch  4, batch    31 | loss: 6.5105209CurrentTrain: epoch  4, batch    32 | loss: 7.6631536CurrentTrain: epoch  4, batch    33 | loss: 6.2924786CurrentTrain: epoch  4, batch    34 | loss: 7.8369579CurrentTrain: epoch  4, batch    35 | loss: 7.2543831CurrentTrain: epoch  4, batch    36 | loss: 6.5398402CurrentTrain: epoch  4, batch    37 | loss: 7.5059304CurrentTrain: epoch  5, batch     0 | loss: 6.1030679CurrentTrain: epoch  5, batch     1 | loss: 6.8386374CurrentTrain: epoch  5, batch     2 | loss: 7.0808506CurrentTrain: epoch  5, batch     3 | loss: 7.1512556CurrentTrain: epoch  5, batch     4 | loss: 6.9177895CurrentTrain: epoch  5, batch     5 | loss: 6.7688484CurrentTrain: epoch  5, batch     6 | loss: 6.7772846CurrentTrain: epoch  5, batch     7 | loss: 6.8151073CurrentTrain: epoch  5, batch     8 | loss: 6.7718267CurrentTrain: epoch  5, batch     9 | loss: 6.6284990CurrentTrain: epoch  5, batch    10 | loss: 6.2529025CurrentTrain: epoch  5, batch    11 | loss: 6.5486612CurrentTrain: epoch  5, batch    12 | loss: 6.1802988CurrentTrain: epoch  5, batch    13 | loss: 6.8904743CurrentTrain: epoch  5, batch    14 | loss: 6.4440236CurrentTrain: epoch  5, batch    15 | loss: 6.5432043CurrentTrain: epoch  5, batch    16 | loss: 5.7127781CurrentTrain: epoch  5, batch    17 | loss: 5.8208084CurrentTrain: epoch  5, batch    18 | loss: 7.2228193CurrentTrain: epoch  5, batch    19 | loss: 6.8538017CurrentTrain: epoch  5, batch    20 | loss: 6.0343790CurrentTrain: epoch  5, batch    21 | loss: 6.0485401CurrentTrain: epoch  5, batch    22 | loss: 6.1135545CurrentTrain: epoch  5, batch    23 | loss: 5.8876462CurrentTrain: epoch  5, batch    24 | loss: 6.7454958CurrentTrain: epoch  5, batch    25 | loss: 5.8460865CurrentTrain: epoch  5, batch    26 | loss: 5.9473586CurrentTrain: epoch  5, batch    27 | loss: 6.7466745CurrentTrain: epoch  5, batch    28 | loss: 8.3776693CurrentTrain: epoch  5, batch    29 | loss: 6.3754187CurrentTrain: epoch  5, batch    30 | loss: 6.4071417CurrentTrain: epoch  5, batch    31 | loss: 6.0694742CurrentTrain: epoch  5, batch    32 | loss: 5.5056829CurrentTrain: epoch  5, batch    33 | loss: 6.3093948CurrentTrain: epoch  5, batch    34 | loss: 6.6475391CurrentTrain: epoch  5, batch    35 | loss: 5.9693222CurrentTrain: epoch  5, batch    36 | loss: 6.5377998CurrentTrain: epoch  5, batch    37 | loss: 6.0111084CurrentTrain: epoch  6, batch     0 | loss: 6.0047464CurrentTrain: epoch  6, batch     1 | loss: 6.6310277CurrentTrain: epoch  6, batch     2 | loss: 6.6020212CurrentTrain: epoch  6, batch     3 | loss: 6.2475824CurrentTrain: epoch  6, batch     4 | loss: 6.0720997CurrentTrain: epoch  6, batch     5 | loss: 6.0006638CurrentTrain: epoch  6, batch     6 | loss: 6.3973942CurrentTrain: epoch  6, batch     7 | loss: 5.9961662CurrentTrain: epoch  6, batch     8 | loss: 5.7649665CurrentTrain: epoch  6, batch     9 | loss: 6.0096731CurrentTrain: epoch  6, batch    10 | loss: 5.9515557CurrentTrain: epoch  6, batch    11 | loss: 5.9529152CurrentTrain: epoch  6, batch    12 | loss: 5.9199295CurrentTrain: epoch  6, batch    13 | loss: 5.6818867CurrentTrain: epoch  6, batch    14 | loss: 5.8865380CurrentTrain: epoch  6, batch    15 | loss: 5.3489008CurrentTrain: epoch  6, batch    16 | loss: 6.4938850CurrentTrain: epoch  6, batch    17 | loss: 5.9322629CurrentTrain: epoch  6, batch    18 | loss: 6.0616775CurrentTrain: epoch  6, batch    19 | loss: 5.8435102CurrentTrain: epoch  6, batch    20 | loss: 6.5467153CurrentTrain: epoch  6, batch    21 | loss: 6.8287182CurrentTrain: epoch  6, batch    22 | loss: 5.8638916CurrentTrain: epoch  6, batch    23 | loss: 5.8143373CurrentTrain: epoch  6, batch    24 | loss: 5.5679169CurrentTrain: epoch  6, batch    25 | loss: 6.4695663CurrentTrain: epoch  6, batch    26 | loss: 6.4372053CurrentTrain: epoch  6, batch    27 | loss: 5.6673498CurrentTrain: epoch  6, batch    28 | loss: 5.9043579CurrentTrain: epoch  6, batch    29 | loss: 6.1868839CurrentTrain: epoch  6, batch    30 | loss: 6.6498880CurrentTrain: epoch  6, batch    31 | loss: 6.2947693CurrentTrain: epoch  6, batch    32 | loss: 5.6921754CurrentTrain: epoch  6, batch    33 | loss: 6.2850089CurrentTrain: epoch  6, batch    34 | loss: 5.9845924CurrentTrain: epoch  6, batch    35 | loss: 6.4355583CurrentTrain: epoch  6, batch    36 | loss: 6.2763124CurrentTrain: epoch  6, batch    37 | loss: 5.9351416CurrentTrain: epoch  7, batch     0 | loss: 6.6529021CurrentTrain: epoch  7, batch     1 | loss: 5.8108978CurrentTrain: epoch  7, batch     2 | loss: 5.5417938CurrentTrain: epoch  7, batch     3 | loss: 5.5101213CurrentTrain: epoch  7, batch     4 | loss: 6.3973951CurrentTrain: epoch  7, batch     5 | loss: 5.6384888CurrentTrain: epoch  7, batch     6 | loss: 5.6699238CurrentTrain: epoch  7, batch     7 | loss: 5.6597505CurrentTrain: epoch  7, batch     8 | loss: 5.6804357CurrentTrain: epoch  7, batch     9 | loss: 5.5073752CurrentTrain: epoch  7, batch    10 | loss: 5.9107246CurrentTrain: epoch  7, batch    11 | loss: 5.8943958CurrentTrain: epoch  7, batch    12 | loss: 5.7086897CurrentTrain: epoch  7, batch    13 | loss: 5.8933516CurrentTrain: epoch  7, batch    14 | loss: 5.8622904CurrentTrain: epoch  7, batch    15 | loss: 6.0675993CurrentTrain: epoch  7, batch    16 | loss: 5.7433643CurrentTrain: epoch  7, batch    17 | loss: 5.4421778CurrentTrain: epoch  7, batch    18 | loss: 5.6944370CurrentTrain: epoch  7, batch    19 | loss: 5.5842218CurrentTrain: epoch  7, batch    20 | loss: 5.5729561CurrentTrain: epoch  7, batch    21 | loss: 5.3309689CurrentTrain: epoch  7, batch    22 | loss: 6.9911366CurrentTrain: epoch  7, batch    23 | loss: 6.2403383CurrentTrain: epoch  7, batch    24 | loss: 6.1066108CurrentTrain: epoch  7, batch    25 | loss: 5.3430290CurrentTrain: epoch  7, batch    26 | loss: 5.5711193CurrentTrain: epoch  7, batch    27 | loss: 5.4540119CurrentTrain: epoch  7, batch    28 | loss: 5.4332466CurrentTrain: epoch  7, batch    29 | loss: 5.3174152CurrentTrain: epoch  7, batch    30 | loss: 5.4019990CurrentTrain: epoch  7, batch    31 | loss: 5.3367624CurrentTrain: epoch  7, batch    32 | loss: 5.6065183CurrentTrain: epoch  7, batch    33 | loss: 5.8639040CurrentTrain: epoch  7, batch    34 | loss: 5.5942111CurrentTrain: epoch  7, batch    35 | loss: 5.7962923CurrentTrain: epoch  7, batch    36 | loss: 5.5112915CurrentTrain: epoch  7, batch    37 | loss: 4.9952893CurrentTrain: epoch  8, batch     0 | loss: 5.4854412CurrentTrain: epoch  8, batch     1 | loss: 5.6333909CurrentTrain: epoch  8, batch     2 | loss: 5.1421604CurrentTrain: epoch  8, batch     3 | loss: 5.3651385CurrentTrain: epoch  8, batch     4 | loss: 5.3226509CurrentTrain: epoch  8, batch     5 | loss: 5.3203096CurrentTrain: epoch  8, batch     6 | loss: 5.3517303CurrentTrain: epoch  8, batch     7 | loss: 5.4473085CurrentTrain: epoch  8, batch     8 | loss: 5.3135548CurrentTrain: epoch  8, batch     9 | loss: 5.2105174CurrentTrain: epoch  8, batch    10 | loss: 5.6048307CurrentTrain: epoch  8, batch    11 | loss: 5.3615904CurrentTrain: epoch  8, batch    12 | loss: 5.7865286CurrentTrain: epoch  8, batch    13 | loss: 5.3026810CurrentTrain: epoch  8, batch    14 | loss: 5.6099992CurrentTrain: epoch  8, batch    15 | loss: 5.3994226CurrentTrain: epoch  8, batch    16 | loss: 5.7551336CurrentTrain: epoch  8, batch    17 | loss: 5.0666037CurrentTrain: epoch  8, batch    18 | loss: 5.2056007CurrentTrain: epoch  8, batch    19 | loss: 5.7251682CurrentTrain: epoch  8, batch    20 | loss: 5.5557652CurrentTrain: epoch  8, batch    21 | loss: 5.4007759CurrentTrain: epoch  8, batch    22 | loss: 5.9072752CurrentTrain: epoch  8, batch    23 | loss: 5.6366529CurrentTrain: epoch  8, batch    24 | loss: 5.2677555CurrentTrain: epoch  8, batch    25 | loss: 5.3671660CurrentTrain: epoch  8, batch    26 | loss: 6.1590004CurrentTrain: epoch  8, batch    27 | loss: 5.1991730CurrentTrain: epoch  8, batch    28 | loss: 5.2172470CurrentTrain: epoch  8, batch    29 | loss: 5.6280775CurrentTrain: epoch  8, batch    30 | loss: 5.2850456CurrentTrain: epoch  8, batch    31 | loss: 5.3826280CurrentTrain: epoch  8, batch    32 | loss: 4.9245048CurrentTrain: epoch  8, batch    33 | loss: 5.1530337CurrentTrain: epoch  8, batch    34 | loss: 5.0754180CurrentTrain: epoch  8, batch    35 | loss: 5.2098951CurrentTrain: epoch  8, batch    36 | loss: 5.0569038CurrentTrain: epoch  8, batch    37 | loss: 5.1459870CurrentTrain: epoch  9, batch     0 | loss: 5.3714628CurrentTrain: epoch  9, batch     1 | loss: 5.2017689CurrentTrain: epoch  9, batch     2 | loss: 5.2213621CurrentTrain: epoch  9, batch     3 | loss: 5.0896473CurrentTrain: epoch  9, batch     4 | loss: 5.1074209CurrentTrain: epoch  9, batch     5 | loss: 5.1780319CurrentTrain: epoch  9, batch     6 | loss: 5.4686308CurrentTrain: epoch  9, batch     7 | loss: 5.1841688CurrentTrain: epoch  9, batch     8 | loss: 5.1631765CurrentTrain: epoch  9, batch     9 | loss: 5.0416660CurrentTrain: epoch  9, batch    10 | loss: 5.0971518CurrentTrain: epoch  9, batch    11 | loss: 5.0610533CurrentTrain: epoch  9, batch    12 | loss: 5.1421871CurrentTrain: epoch  9, batch    13 | loss: 5.2279015CurrentTrain: epoch  9, batch    14 | loss: 5.3229456CurrentTrain: epoch  9, batch    15 | loss: 5.0906010CurrentTrain: epoch  9, batch    16 | loss: 5.0985069CurrentTrain: epoch  9, batch    17 | loss: 5.8242164CurrentTrain: epoch  9, batch    18 | loss: 5.5273147CurrentTrain: epoch  9, batch    19 | loss: 5.1797643CurrentTrain: epoch  9, batch    20 | loss: 5.3398924CurrentTrain: epoch  9, batch    21 | loss: 4.9716997CurrentTrain: epoch  9, batch    22 | loss: 5.1110754CurrentTrain: epoch  9, batch    23 | loss: 5.2672491CurrentTrain: epoch  9, batch    24 | loss: 5.1435614CurrentTrain: epoch  9, batch    25 | loss: 5.7747397CurrentTrain: epoch  9, batch    26 | loss: 5.6257625CurrentTrain: epoch  9, batch    27 | loss: 5.3424568CurrentTrain: epoch  9, batch    28 | loss: 5.0072770CurrentTrain: epoch  9, batch    29 | loss: 5.5512266CurrentTrain: epoch  9, batch    30 | loss: 4.9825134CurrentTrain: epoch  9, batch    31 | loss: 5.0399342CurrentTrain: epoch  9, batch    32 | loss: 5.1282659CurrentTrain: epoch  9, batch    33 | loss: 5.3177009CurrentTrain: epoch  9, batch    34 | loss: 5.1026111CurrentTrain: epoch  9, batch    35 | loss: 4.9330521CurrentTrain: epoch  9, batch    36 | loss: 4.9632597CurrentTrain: epoch  9, batch    37 | loss: 5.1368017
#### person countries of residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person countries of residence" means a person is living in a country.
Here is an example:
Relation: person countries of residence
Context: khamenei , 67 , has final say on all state matters in iran as supreme leader , a post he has held since 1989 .
Head Entity: khamenei
Tail Entity: iran
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person countries of residence":
Relation: person countries of residence  
Context: After years of living in the United States, Maria decided to return to her homeland, spain, where she felt a deep connection to her roots.  
Head Entity: Maria  
Tail Entity: spain  

Relation: person countries of residence  
Context: Following his successful career in technology, Raj moved to canada to enjoy a quieter life surrounded by nature.  
Head Entity: Raj  
Tail Entity: canada  
#### organization top members employees ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization top members employees" means an organization 's employer is a person.
Here is an example:
Relation: organization top members employees
Context: kao charng , vice chairman of the cabinet-level mainland affairs council -lrb- mac -rrb- , explained that the termination clause was added to give both sides an option after the implementation of the tariff-cutting economic cooperation framework agreement -lrb- ecfa -rrb- .
Head Entity: mainland affairs council
Tail Entity: kao charng
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization top members employees":
Relation: organization top members employees  
Context: elon musk, the CEO of spacex, announced the successful launch of the latest satellite into orbit, marking another milestone for the company.  
Head Entity: spacex  
Tail Entity: elon musk  

Relation: organization top members employees  
Context: sheryl sandberg, the chief operating officer of facebook, spoke at the conference about the importance of leadership in the tech industry.  
Head Entity: facebook  
Tail Entity: sheryl sandberg  
#### organization member of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization member of" means an organization is the member of an organization.
Here is an example:
Relation: organization member of
Context: south africa 1-5 leyds cnr biccard streets cosatu 's concern has been that the scorpions ' existence as a branch of the national directorate of public prosecutions blurred the necessary separation of functions between those investigating crime and those prosecuting the criminals .
Head Entity: scorpions
Tail Entity: national directorate of public prosecutions
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization member of":
Relation: organization member of  
Context: The International Federation of Red Cross and Red Crescent Societies (IFRC) is a global humanitarian organization that coordinates the activities of national Red Cross and Red Crescent societies, ensuring that they work together effectively in times of crisis.  
Head Entity: International Federation of Red Cross and Red Crescent Societies  
Tail Entity: national Red Cross and Red Crescent societies  

Relation: organization member of  
Context: The European Union is a political and economic union of member states that are located primarily in Europe, working together to promote peace, stability, and prosperity across the continent.  
Head Entity: European Union  
Tail Entity: member states  
#### person origin ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person origin" means a person 's nationality is a country.
Here is an example:
Relation: person origin
Context: iranian nuclear negotiator ali larijani said thursday in ankara that talks on settling the iranian nuclear crisis had made some progress towards a `` united view . ''
Head Entity: ali larijani
Tail Entity: iranian
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person origin":
Relation: person origin  
Context: The famous physicist Albert Einstein was born in the Kingdom of Württemberg in the German Empire, which is now part of modern-day Germany.  
Head Entity: Albert Einstein  
Tail Entity: German  

Relation: person origin  
Context: The renowned author Chimamanda Ngozi Adichie often speaks about her Nigerian heritage and how it influences her writing.  
Head Entity: Chimamanda Ngozi Adichie  
Tail Entity: Nigerian  
#### person title ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person title" means a person 's title is a title.
Here is an example:
Relation: person title
Context: `` it has to have some tempo , it has to have some energy , '' said sean ross , vice president of music and programming at edison media research , which surveys the radio industry .
Head Entity: sean ross
Tail Entity: vice president
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person title":
Relation: person title  
Context: "After years of hard work and dedication, Maria Gonzalez was promoted to Chief Financial Officer at Tech Innovations, where she has been instrumental in driving the company's financial strategy."  
Head Entity: Maria Gonzalez  
Tail Entity: Chief Financial Officer  

Relation: person title  
Context: "During the annual conference, Dr. James Lee, the lead researcher at the National Institute of Health, presented groundbreaking findings in the field of genetics."  
Head Entity: Dr. James Lee  
Tail Entity: lead researcher  
#### organization country of headquarters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization country of headquarters" means an organization is located in a country.
Here is an example:
Relation: organization country of headquarters
Context: the credit crisis spread to the largest us bond insurer thursday , sending shares of mbia inc plunging and calling into question the safety of tens of billions of dollars of company and local government debt held by investors .
Head Entity: mbia
Tail Entity: us
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization country of headquarters":
Relation: organization country of headquarters  
Context: after years of rapid growth, the tech giant apple inc announced plans to expand its operations in ireland, taking advantage of the favorable tax environment.  
Head Entity: apple  
Tail Entity: ireland  

Relation: organization country of headquarters  
Context: the renowned automotive manufacturer toyota motor corporation has established its main office in japan, where it continues to innovate in the field of electric vehicles.  
Head Entity: toyota  
Tail Entity: japan  
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 75.00%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    3 | acc: 62.50%,  total acc: 76.56%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 77.50%   [EVAL] batch:    5 | acc: 68.75%,  total acc: 76.04%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 78.57%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 82.64%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 83.75%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 85.23%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 85.94%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 86.54%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 85.27%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 84.58%   [EVAL] batch:   15 | acc: 62.50%,  total acc: 83.20%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 82.72%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 81.60%   [EVAL] batch:   18 | acc: 81.25%,  total acc: 81.58%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 81.56%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 82.44%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 83.24%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 83.97%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 84.64%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 85.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 85.82%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 86.11%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 86.61%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 87.07%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 86.88%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 86.90%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 87.11%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 85.61%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 75.00%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    3 | acc: 62.50%,  total acc: 76.56%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 77.50%   [EVAL] batch:    5 | acc: 68.75%,  total acc: 76.04%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 78.57%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 82.64%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 83.75%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 85.23%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 85.94%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 86.54%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 85.27%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 84.58%   [EVAL] batch:   15 | acc: 62.50%,  total acc: 83.20%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 82.72%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 81.60%   [EVAL] batch:   18 | acc: 81.25%,  total acc: 81.58%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 81.56%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 82.44%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 83.24%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 83.97%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 84.64%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 85.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 85.82%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 86.11%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 86.61%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 87.07%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 86.88%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 86.90%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 87.11%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 85.61%   
cur_acc:  ['0.8561']
his_acc:  ['0.8561']
CurrentTrain: epoch  0, batch     0 | loss: 6.7105479CurrentTrain: epoch  0, batch     1 | loss: 7.8216114CurrentTrain: epoch  1, batch     0 | loss: 6.5109053CurrentTrain: epoch  1, batch     1 | loss: 5.2046876CurrentTrain: epoch  2, batch     0 | loss: 5.8193226CurrentTrain: epoch  2, batch     1 | loss: 5.3728476CurrentTrain: epoch  3, batch     0 | loss: 5.3471889CurrentTrain: epoch  3, batch     1 | loss: 4.4202156CurrentTrain: epoch  4, batch     0 | loss: 4.8331256CurrentTrain: epoch  4, batch     1 | loss: 4.8644309CurrentTrain: epoch  5, batch     0 | loss: 4.9063978CurrentTrain: epoch  5, batch     1 | loss: 3.9061246CurrentTrain: epoch  6, batch     0 | loss: 4.2706718CurrentTrain: epoch  6, batch     1 | loss: 3.8399904CurrentTrain: epoch  7, batch     0 | loss: 4.1974335CurrentTrain: epoch  7, batch     1 | loss: 4.0149384CurrentTrain: epoch  8, batch     0 | loss: 3.6917348CurrentTrain: epoch  8, batch     1 | loss: 3.3621042CurrentTrain: epoch  9, batch     0 | loss: 3.4238887CurrentTrain: epoch  9, batch     1 | loss: 3.0295978
#### person date of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person date of birth" means a person was born in a date.
Here is an example:
Relation: person date of birth
Context: kirkaldy , born irene morgan in baltimore , maryland , in 1917 , was arrested in 1944 for refusing to give up her seat on a greyhound bus heading from gloucester to baltimore , and for resisting arrest .
Head Entity: irene morgan
Tail Entity: 1917
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person date of birth":
Relation: person date of birth  
Context: the famous physicist albert einstein was born in ulm, in the kingdom of wurttemberg in the german empire, on march 14, 1879.  
Head Entity: albert einstein  
Tail Entity: march 14, 1879  

Relation: person date of birth  
Context: the renowned author jane austen was born on december 16, 1775, in steventon, hampshire, england.  
Head Entity: jane austen  
Tail Entity: december 16, 1775  
#### person stateorprovince of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person stateorprovince of birth" means a person was died in a state or province.
Here is an example:
Relation: person stateorprovince of birth
Context: jane matilda bolin was born on april 11 , 1908 , in poughkeepsie , ny .
Head Entity: jane matilda bolin
Tail Entity: ny
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person stateorprovince of birth":
Relation: person stateorprovince of birth  
Context: albert einstein was born on march 14, 1879, in ulm, germany.  
Head Entity: albert einstein  
Tail Entity: germany  

Relation: person stateorprovince of birth  
Context: marilyn monroe was born on june 1, 1926, in los angeles, california.  
Head Entity: marilyn monroe  
Tail Entity: california  
#### person parents ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person parents" means a person 's parent is a person.
Here is an example:
Relation: person parents
Context: lynne spears told the court that lutfi had treated her daughter like a hostage in her own home , drugged her and took over her finances .
Head Entity: her
Tail Entity: lynne spears
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person parents":
Relation: person parents  
Context: During the family reunion, Sarah introduced her father, John, to her friends, highlighting how much he has influenced her life choices.  
Head Entity: her father  
Tail Entity: John  

Relation: person parents  
Context: Emily often shares stories about her mother, who was a strong influence in her decision to pursue a career in medicine.  
Head Entity: her mother  
Tail Entity: Emily
#### person employee of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person employee of" means a person is an employee of an organization.
Here is an example:
Relation: person employee of
Context: seeking revenge , axel reunites with old pal sgt. billy rosewood -lrb- judge reinhold -rrb- and jon flint -lrb- hector elizondo -rrb- of the beverly hills police department .
Head Entity: hector elizondo
Tail Entity: beverly hills police department
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person employee of":
Relation: person employee of  
Context: After years of hard work and dedication, Sarah Thompson finally received a promotion at Tech Innovations, where she has been a software engineer for over five years.  
Head Entity: Sarah Thompson  
Tail Entity: Tech Innovations  

Relation: person employee of  
Context: During the annual conference, it was announced that Michael Johnson, a renowned chef, has joined the culinary team at Gourmet Delights.  
Head Entity: Michael Johnson  
Tail Entity: Gourmet Delights  
#### person stateorprovince of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person stateorprovince of death" means a person was died in a state or province.
Here is an example:
Relation: person stateorprovince of death
Context: irene morgan kirkaldy , 90 , who died of alzheimer 's disease aug. 10 at her home in gloucester , va. , quietly changed history in 1944 when she refused to give up her seat on a crowded greyhound bus to a white couple .
Head Entity: irene morgan kirkaldy
Tail Entity: va.
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person stateorprovince of death":
Relation: person stateorprovince of death  
Context: john smith, a renowned author, passed away on march 5 in his residence located in los angeles, california, leaving behind a legacy of literary works that inspired many.  
Head Entity: john smith  
Tail Entity: california  

Relation: person stateorprovince of death  
Context: the famous musician, elena rodriguez, died tragically in a car accident on july 12 while traveling through the scenic routes of oregon, where she had spent her childhood.  
Head Entity: elena rodriguez  
Tail Entity: oregon  
Mixup data size:  103
MixupTrain:  epoch  0, batch     0 | loss: 10.5174731MixupTrain:  epoch  0, batch     1 | loss: 8.2823956MixupTrain:  epoch  0, batch     2 | loss: 9.3027510MixupTrain:  epoch  0, batch     3 | loss: 10.8092354MixupTrain:  epoch  0, batch     4 | loss: 10.0363623MixupTrain:  epoch  0, batch     5 | loss: 8.2188103MixupTrain:  epoch  0, batch     6 | loss: 5.4365010
MemoryTrain:  epoch  0, batch     0 | loss: 3.8264680MemoryTrain:  epoch  0, batch     1 | loss: 3.5282040MemoryTrain:  epoch  0, batch     2 | loss: 5.4226403MemoryTrain:  epoch  1, batch     0 | loss: 3.7490163MemoryTrain:  epoch  1, batch     1 | loss: 3.0452712MemoryTrain:  epoch  1, batch     2 | loss: 4.6779513MemoryTrain:  epoch  2, batch     0 | loss: 2.5999849MemoryTrain:  epoch  2, batch     1 | loss: 3.2422671MemoryTrain:  epoch  2, batch     2 | loss: 3.0136917MemoryTrain:  epoch  3, batch     0 | loss: 2.6950483MemoryTrain:  epoch  3, batch     1 | loss: 2.5671675MemoryTrain:  epoch  3, batch     2 | loss: 3.2548525MemoryTrain:  epoch  4, batch     0 | loss: 2.3941810MemoryTrain:  epoch  4, batch     1 | loss: 2.9111071MemoryTrain:  epoch  4, batch     2 | loss: 1.8698145MemoryTrain:  epoch  5, batch     0 | loss: 2.6409302MemoryTrain:  epoch  5, batch     1 | loss: 2.3370056MemoryTrain:  epoch  5, batch     2 | loss: 3.4979172MemoryTrain:  epoch  6, batch     0 | loss: 2.2306759MemoryTrain:  epoch  6, batch     1 | loss: 2.8430958MemoryTrain:  epoch  6, batch     2 | loss: 1.2769049MemoryTrain:  epoch  7, batch     0 | loss: 2.1937540MemoryTrain:  epoch  7, batch     1 | loss: 2.5097804MemoryTrain:  epoch  7, batch     2 | loss: 2.0959184MemoryTrain:  epoch  8, batch     0 | loss: 2.0965691MemoryTrain:  epoch  8, batch     1 | loss: 1.9548874MemoryTrain:  epoch  8, batch     2 | loss: 1.1963555MemoryTrain:  epoch  9, batch     0 | loss: 1.9241676MemoryTrain:  epoch  9, batch     1 | loss: 1.9012794MemoryTrain:  epoch  9, batch     2 | loss: 2.2208221
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 85.94%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 89.58%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 89.29%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 89.84%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 90.28%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 89.77%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 89.58%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 89.42%   [EVAL] batch:   13 | acc: 56.25%,  total acc: 87.05%   
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    1 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 66.67%   [EVAL] batch:    3 | acc: 62.50%,  total acc: 65.62%   [EVAL] batch:    4 | acc: 50.00%,  total acc: 62.50%   [EVAL] batch:    5 | acc: 56.25%,  total acc: 61.46%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 65.18%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 69.53%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 72.92%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 75.00%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 77.27%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 78.12%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 79.33%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 78.57%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 78.33%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 76.95%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 76.84%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 76.04%   [EVAL] batch:   18 | acc: 87.50%,  total acc: 76.64%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 77.19%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 78.27%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 79.26%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 80.16%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 80.99%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 81.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 82.45%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 82.87%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 83.48%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 84.05%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 83.96%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 84.07%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 84.66%   [EVAL] batch:   33 | acc: 68.75%,  total acc: 84.19%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 84.64%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 84.72%   [EVAL] batch:   36 | acc: 87.50%,  total acc: 84.80%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 85.20%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 85.58%   [EVAL] batch:   39 | acc: 81.25%,  total acc: 85.47%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 85.82%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 86.01%   [EVAL] batch:   42 | acc: 87.50%,  total acc: 86.05%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 85.94%   [EVAL] batch:   44 | acc: 81.25%,  total acc: 85.83%   [EVAL] batch:   45 | acc: 87.50%,  total acc: 85.87%   [EVAL] batch:   46 | acc: 6.25%,  total acc: 84.18%   
cur_acc:  ['0.8561', '0.8705']
his_acc:  ['0.8561', '0.8418']
CurrentTrain: epoch  0, batch     0 | loss: 6.3334284CurrentTrain: epoch  0, batch     1 | loss: 7.3160043CurrentTrain: epoch  1, batch     0 | loss: 5.7053623CurrentTrain: epoch  1, batch     1 | loss: 4.9998889CurrentTrain: epoch  2, batch     0 | loss: 5.2493014CurrentTrain: epoch  2, batch     1 | loss: 4.4792957CurrentTrain: epoch  3, batch     0 | loss: 4.2089062CurrentTrain: epoch  3, batch     1 | loss: 4.0112300CurrentTrain: epoch  4, batch     0 | loss: 3.4288538CurrentTrain: epoch  4, batch     1 | loss: 4.4996748CurrentTrain: epoch  5, batch     0 | loss: 3.4365849CurrentTrain: epoch  5, batch     1 | loss: 3.4180415CurrentTrain: epoch  6, batch     0 | loss: 3.4954822CurrentTrain: epoch  6, batch     1 | loss: 3.1073818CurrentTrain: epoch  7, batch     0 | loss: 3.5351901CurrentTrain: epoch  7, batch     1 | loss: 3.0456316CurrentTrain: epoch  8, batch     0 | loss: 2.9712119CurrentTrain: epoch  8, batch     1 | loss: 3.7256720CurrentTrain: epoch  9, batch     0 | loss: 2.6621459CurrentTrain: epoch  9, batch     1 | loss: 3.7737343
#### person country of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person country of birth" means a person was born in a country.
Here is an example:
Relation: person country of birth
Context: de maiziere noted that germany took in another former inmate from guantanamo in 2006 -- murat kurnaz , a turkish national who was born and grew up in germany .
Head Entity: murat kurnaz
Tail Entity: germany
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person country of birth":
Relation: person country of birth  
Context: born in 1985 in the bustling city of new delhi, arjun kapoor has always been proud of his indian heritage and culture.  
Head Entity: arjun kapoor  
Tail Entity: india  

Relation: person country of birth  
Context: during the interview, she revealed that she was born in the picturesque town of auckland, new zealand, which she considers her true home.  
Head Entity: she  
Tail Entity: new zealand  
#### organization website ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization website" means an organization 's website is an url.
Here is an example:
Relation: organization website
Context: http://www.pentaxsailaway.com/ paul j. sail away with pentax sweepstakes
Head Entity: pentax
Tail Entity: http://www.pentaxsailaway.com/
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization website":
Relation: organization website  
Context: Visit the official site at https://www.techinnovators.com for more information on their latest products.  
Head Entity: Tech Innovators  
Tail Entity: https://www.techinnovators.com  

Relation: organization website  
Context: For updates and news, check out the blog at http://www.greenearth.org/blog.  
Head Entity: Green Earth  
Tail Entity: http://www.greenearth.org/  
#### organization shareholders ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization shareholders" means an organization was invested by a person.
Here is an example:
Relation: organization shareholders
Context: marsans already owns the madrid-based air comet and 95 percent of aerolineas argentinas .
Head Entity: aerolineas argentinas
Tail Entity: marsans
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization shareholders":
Relation: organization shareholders  
Context: tech giant Apple has seen significant investments from Warren Buffett's Berkshire Hathaway, which now holds a substantial stake in the company.  
Head Entity: Apple  
Tail Entity: Berkshire Hathaway  

Relation: organization shareholders  
Context: The investment firm Vanguard Group has increased its holdings in the multinational corporation Procter & Gamble, reflecting confidence in its long-term growth.  
Head Entity: Procter & Gamble  
Tail Entity: Vanguard Group  
#### organization dissolved ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization dissolved" means an organization was dissolved in a date.
Here is an example:
Relation: organization dissolved
Context: the tse suffered its worst-ever system crash in november 2005 which paralyzed the world 's second largest bourse and forced it to shelve plan for a listing of its own .
Head Entity: tse
Tail Entity: november 2005
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization dissolved":
Relation: organization dissolved  
Context: The once-prominent tech startup, Innovatech, officially ceased operations in March 2020 after struggling to secure funding.  
Head Entity: Innovatech  
Tail Entity: March 2020  

Relation: organization dissolved  
Context: After years of financial difficulties, the local arts council announced its dissolution in January 2019, leaving many community projects in limbo.  
Head Entity: local arts council  
Tail Entity: January 2019  
#### organization founded by ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization founded by" means an organization was found by a person.
Here is an example:
Relation: organization founded by
Context: `` i have no greater obligation than to ensure the safely of airline travelers in this country , '' transportation secretary ray lahood said in a joint statement with j. randolph babbitt , administrator of the federal aviation administration , that was issued on the eve of a senate hearing on aviation safety .
Head Entity: federal aviation administration
Tail Entity: j. randolph babbitt
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization founded by":
Relation: organization founded by  
Context: In 1975, the renowned physicist and entrepreneur, Dr. Jane Smith, established Quantum Innovations, a company dedicated to advancing quantum computing technologies.  
Head Entity: Quantum Innovations  
Tail Entity: Dr. Jane Smith  

Relation: organization founded by  
Context: The charity organization, Hope for Tomorrow, was created in 2010 by the famous philanthropist, John Doe, to support underprivileged children around the world.  
Head Entity: Hope for Tomorrow  
Tail Entity: John Doe  
Mixup data size:  134
MixupTrain:  epoch  0, batch     0 | loss: 7.3637033MixupTrain:  epoch  0, batch     1 | loss: 7.3155163MixupTrain:  epoch  0, batch     2 | loss: 6.0234196MixupTrain:  epoch  0, batch     3 | loss: 8.5681300MixupTrain:  epoch  0, batch     4 | loss: 7.4206517MixupTrain:  epoch  0, batch     5 | loss: 8.0573348MixupTrain:  epoch  0, batch     6 | loss: 8.4060448MixupTrain:  epoch  0, batch     7 | loss: 6.4836638MixupTrain:  epoch  0, batch     8 | loss: 4.4402282
MemoryTrain:  epoch  0, batch     0 | loss: 4.2473979MemoryTrain:  epoch  0, batch     1 | loss: 3.7907400MemoryTrain:  epoch  0, batch     2 | loss: 3.0683811MemoryTrain:  epoch  1, batch     0 | loss: 2.9453077MemoryTrain:  epoch  1, batch     1 | loss: 4.6907144MemoryTrain:  epoch  1, batch     2 | loss: 3.4282179MemoryTrain:  epoch  2, batch     0 | loss: 3.6708987MemoryTrain:  epoch  2, batch     1 | loss: 2.9540792MemoryTrain:  epoch  2, batch     2 | loss: 3.7162533MemoryTrain:  epoch  3, batch     0 | loss: 2.7958620MemoryTrain:  epoch  3, batch     1 | loss: 2.9755089MemoryTrain:  epoch  3, batch     2 | loss: 3.5254927MemoryTrain:  epoch  4, batch     0 | loss: 2.3520017MemoryTrain:  epoch  4, batch     1 | loss: 2.9282780MemoryTrain:  epoch  4, batch     2 | loss: 3.3670101MemoryTrain:  epoch  5, batch     0 | loss: 2.7825093MemoryTrain:  epoch  5, batch     1 | loss: 3.0658770MemoryTrain:  epoch  5, batch     2 | loss: 2.4635682MemoryTrain:  epoch  6, batch     0 | loss: 2.7640674MemoryTrain:  epoch  6, batch     1 | loss: 2.3481798MemoryTrain:  epoch  6, batch     2 | loss: 2.8683314MemoryTrain:  epoch  7, batch     0 | loss: 2.9435668MemoryTrain:  epoch  7, batch     1 | loss: 2.5456996MemoryTrain:  epoch  7, batch     2 | loss: 1.9000584MemoryTrain:  epoch  8, batch     0 | loss: 1.9069347MemoryTrain:  epoch  8, batch     1 | loss: 2.2650299MemoryTrain:  epoch  8, batch     2 | loss: 2.4209948MemoryTrain:  epoch  9, batch     0 | loss: 2.4157920MemoryTrain:  epoch  9, batch     1 | loss: 2.1196637MemoryTrain:  epoch  9, batch     2 | loss: 2.1868043
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 12.50%,  total acc: 67.19%   [EVAL] batch:    4 | acc: 6.25%,  total acc: 55.00%   [EVAL] batch:    5 | acc: 0.00%,  total acc: 45.83%   [EVAL] batch:    6 | acc: 6.25%,  total acc: 40.18%   [EVAL] batch:    7 | acc: 0.00%,  total acc: 35.16%   
[EVAL] batch:    0 | acc: 31.25%,  total acc: 31.25%   [EVAL] batch:    1 | acc: 25.00%,  total acc: 28.12%   [EVAL] batch:    2 | acc: 68.75%,  total acc: 41.67%   [EVAL] batch:    3 | acc: 18.75%,  total acc: 35.94%   [EVAL] batch:    4 | acc: 25.00%,  total acc: 33.75%   [EVAL] batch:    5 | acc: 43.75%,  total acc: 35.42%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 41.07%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 46.88%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 52.78%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 56.25%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 60.23%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 62.50%   [EVAL] batch:   12 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:   13 | acc: 31.25%,  total acc: 60.27%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 61.25%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 60.94%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 61.76%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 61.81%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 62.17%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 63.44%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 65.18%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 66.76%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 68.21%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 69.53%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 70.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 71.88%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 72.69%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 73.66%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 74.57%   [EVAL] batch:   29 | acc: 93.75%,  total acc: 75.21%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 75.81%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 76.37%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 76.89%   [EVAL] batch:   33 | acc: 75.00%,  total acc: 76.84%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 77.50%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 77.78%   [EVAL] batch:   36 | acc: 87.50%,  total acc: 78.04%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 78.62%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 79.17%   [EVAL] batch:   39 | acc: 87.50%,  total acc: 79.38%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 79.88%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 80.21%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 80.52%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 80.54%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 80.97%   [EVAL] batch:   45 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 81.64%   [EVAL] batch:   48 | acc: 81.25%,  total acc: 81.63%   [EVAL] batch:   49 | acc: 6.25%,  total acc: 80.12%   [EVAL] batch:   50 | acc: 12.50%,  total acc: 78.80%   [EVAL] batch:   51 | acc: 0.00%,  total acc: 77.28%   [EVAL] batch:   52 | acc: 6.25%,  total acc: 75.94%   [EVAL] batch:   53 | acc: 0.00%,  total acc: 74.54%   
cur_acc:  ['0.8561', '0.8705', '0.3516']
his_acc:  ['0.8561', '0.8418', '0.7454']
CurrentTrain: epoch  0, batch     0 | loss: 4.8303671CurrentTrain: epoch  0, batch     1 | loss: 5.2141452CurrentTrain: epoch  1, batch     0 | loss: 3.9750013CurrentTrain: epoch  1, batch     1 | loss: 3.6817813CurrentTrain: epoch  2, batch     0 | loss: 3.6952240CurrentTrain: epoch  2, batch     1 | loss: 3.4215803CurrentTrain: epoch  3, batch     0 | loss: 3.3182468CurrentTrain: epoch  3, batch     1 | loss: 2.7346039CurrentTrain: epoch  4, batch     0 | loss: 3.2979190CurrentTrain: epoch  4, batch     1 | loss: 2.6301262CurrentTrain: epoch  5, batch     0 | loss: 2.6683798CurrentTrain: epoch  5, batch     1 | loss: 2.5767643CurrentTrain: epoch  6, batch     0 | loss: 2.5326591CurrentTrain: epoch  6, batch     1 | loss: 2.4841983CurrentTrain: epoch  7, batch     0 | loss: 2.6941323CurrentTrain: epoch  7, batch     1 | loss: 2.2424390CurrentTrain: epoch  8, batch     0 | loss: 2.2212634CurrentTrain: epoch  8, batch     1 | loss: 2.2812402CurrentTrain: epoch  9, batch     0 | loss: 2.1093822CurrentTrain: epoch  9, batch     1 | loss: 2.2667210
#### person cause of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person cause of death" means a person was died of an event.
Here is an example:
Relation: person cause of death
Context: mr scheider had suffered from multiple myeloma for several years , and died of complications from a staph infection , his wife , brenda siemer , said .
Head Entity: mr scheider
Tail Entity: complications from a staph infection
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person cause of death":
Relation: person cause of death  
Context: after battling with lung cancer for over a year, john doe passed away peacefully in his sleep, surrounded by family.  
Head Entity: john doe  
Tail Entity: lung cancer  

Relation: person cause of death  
Context: the renowned scientist, dr. emily white, tragically lost her life due to a car accident while returning from a conference.  
Head Entity: dr. emily white  
Tail Entity: car accident  
#### organization political religious affiliation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization political religious affiliation" means an organization is the member of a religion.
Here is an example:
Relation: organization political religious affiliation
Context: the mnlf used to be the largest muslim group fighting for a separate islamic homeland in the southern philippines until it settled for limited autonomy and signed a peace agreement with manila in 1996 .
Head Entity: mnlf
Tail Entity: islamic
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization political religious affiliation":
Relation: organization political religious affiliation  
Context: the national council of churches in the philippines has been a significant advocate for social justice and has ties to various religious groups across the country.  
Head Entity: national council of churches in the philippines  
Tail Entity: religious groups  

Relation: organization political religious affiliation  
Context: the interfaith alliance works to promote understanding and cooperation among different faiths, influencing political discourse in the region.  
Head Entity: interfaith alliance  
Tail Entity: faiths  
#### organization stateorprovince of headquarters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization stateorprovince of headquarters" means an organization is located in a state or province.
Here is an example:
Relation: organization stateorprovince of headquarters
Context: washington , nov 30 -lrb- xinhua -rrb- -- nasa has skipped space shuttle endeavour 's second landing opportunity at kennedy space center in florida on sunday afternoon due to bad weather , according to nasa tv .
Head Entity: kennedy space center
Tail Entity: florida
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization stateorprovince of headquarters":
Relation: organization stateorprovince of headquarters  
Context: the headquarters of the multinational technology company is located in cupertino, california, where it has been a significant player in the tech industry for over two decades.  
Head Entity: cupertino  
Tail Entity: california  

Relation: organization stateorprovince of headquarters  
Context: the famous coffee chain has its main office situated in seattle, washington, which is known for its vibrant coffee culture and numerous cafes.  
Head Entity: seattle  
Tail Entity: washington  
#### person other family ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person other family" means a person 's relative is a person.
Here is an example:
Relation: person other family
Context: first there was the residents ' contest , in which meredith -lrb- ellen pompeo -rrb- , cristina -lrb- sandra oh -rrb- , alex -lrb- justin chambers -rrb- and izzie -lrb- katherine heigl -rrb- earned points for things like number of sutures and surgeries scrubbed in on .
Head Entity: ellen pompeo
Tail Entity: izzie
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person other family":
Relation: person other family  
Context: During the family reunion, Sarah -lrb- the eldest daughter -rrb- introduced her cousin Jake -lrb- her uncle's son -rrb- to everyone, sharing stories of their childhood adventures together.  
Head Entity: Sarah  
Tail Entity: Jake  

Relation: person other family  
Context: At the wedding, Emily -lrb- the bride -rrb- was surrounded by her siblings, including her brother Tom -lrb- who was the best man -rrb- and her sister Lisa -lrb- who helped with the planning.  
Head Entity: Emily  
Tail Entity: Tom  
#### person city of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person city of death" means a person was died in a city.
Here is an example:
Relation: person city of death
Context: verity died wednesday , jan 3 , 2007 , in beaufort memorial hospital .
Head Entity: verity
Tail Entity: beaufort
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person city of death":
Relation: person city of death  
Context: after a long battle with illness, john passed away in the quiet town of springfield.  
Head Entity: john  
Tail Entity: springfield  

Relation: person city of death  
Context: the renowned author tragically died in a car accident on the streets of los angeles.  
Head Entity: the renowned author  
Tail Entity: los angeles  
Mixup data size:  164
MixupTrain:  epoch  0, batch     0 | loss: 5.4610881MixupTrain:  epoch  0, batch     1 | loss: 5.1973056MixupTrain:  epoch  0, batch     2 | loss: 3.9223735MixupTrain:  epoch  0, batch     3 | loss: 4.9030344MixupTrain:  epoch  0, batch     4 | loss: 4.9171731MixupTrain:  epoch  0, batch     5 | loss: 3.9556022MixupTrain:  epoch  0, batch     6 | loss: 3.9334054MixupTrain:  epoch  0, batch     7 | loss: 4.2869470MixupTrain:  epoch  0, batch     8 | loss: 3.8949563MixupTrain:  epoch  0, batch     9 | loss: 4.2128316MixupTrain:  epoch  0, batch    10 | loss: 3.4574089
MemoryTrain:  epoch  0, batch     0 | loss: 3.3071177MemoryTrain:  epoch  0, batch     1 | loss: 3.2441549MemoryTrain:  epoch  0, batch     2 | loss: 3.0670729MemoryTrain:  epoch  0, batch     3 | loss: 3.9944186MemoryTrain:  epoch  1, batch     0 | loss: 2.9772286MemoryTrain:  epoch  1, batch     1 | loss: 3.8140094MemoryTrain:  epoch  1, batch     2 | loss: 3.2577355MemoryTrain:  epoch  1, batch     3 | loss: 2.5983918MemoryTrain:  epoch  2, batch     0 | loss: 2.7650774MemoryTrain:  epoch  2, batch     1 | loss: 2.4693904MemoryTrain:  epoch  2, batch     2 | loss: 2.7202454MemoryTrain:  epoch  2, batch     3 | loss: 2.8103435MemoryTrain:  epoch  3, batch     0 | loss: 2.7250583MemoryTrain:  epoch  3, batch     1 | loss: 2.3439188MemoryTrain:  epoch  3, batch     2 | loss: 2.3000000MemoryTrain:  epoch  3, batch     3 | loss: 1.9119823MemoryTrain:  epoch  4, batch     0 | loss: 2.3993006MemoryTrain:  epoch  4, batch     1 | loss: 1.8527803MemoryTrain:  epoch  4, batch     2 | loss: 2.0547063MemoryTrain:  epoch  4, batch     3 | loss: 2.1800961MemoryTrain:  epoch  5, batch     0 | loss: 1.7796474MemoryTrain:  epoch  5, batch     1 | loss: 2.1806765MemoryTrain:  epoch  5, batch     2 | loss: 1.6540326MemoryTrain:  epoch  5, batch     3 | loss: 2.3556507MemoryTrain:  epoch  6, batch     0 | loss: 1.9502976MemoryTrain:  epoch  6, batch     1 | loss: 1.7947679MemoryTrain:  epoch  6, batch     2 | loss: 1.7509968MemoryTrain:  epoch  6, batch     3 | loss: 1.9201397MemoryTrain:  epoch  7, batch     0 | loss: 1.9364738MemoryTrain:  epoch  7, batch     1 | loss: 1.8454217MemoryTrain:  epoch  7, batch     2 | loss: 1.7116696MemoryTrain:  epoch  7, batch     3 | loss: 1.5156124MemoryTrain:  epoch  8, batch     0 | loss: 1.7541058MemoryTrain:  epoch  8, batch     1 | loss: 1.7736058MemoryTrain:  epoch  8, batch     2 | loss: 1.6948175MemoryTrain:  epoch  8, batch     3 | loss: 1.6670406MemoryTrain:  epoch  9, batch     0 | loss: 1.4523429MemoryTrain:  epoch  9, batch     1 | loss: 1.5168420MemoryTrain:  epoch  9, batch     2 | loss: 1.7025025MemoryTrain:  epoch  9, batch     3 | loss: 1.7154193
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    2 | acc: 100.00%,  total acc: 95.83%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 92.19%   [EVAL] batch:    4 | acc: 12.50%,  total acc: 76.25%   [EVAL] batch:    5 | acc: 12.50%,  total acc: 65.62%   [EVAL] batch:    6 | acc: 12.50%,  total acc: 58.04%   [EVAL] batch:    7 | acc: 37.50%,  total acc: 55.47%   [EVAL] batch:    8 | acc: 12.50%,  total acc: 50.69%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 52.50%   [EVAL] batch:   10 | acc: 31.25%,  total acc: 50.57%   [EVAL] batch:   11 | acc: 56.25%,  total acc: 51.04%   [EVAL] batch:   12 | acc: 25.00%,  total acc: 49.04%   
[EVAL] batch:    0 | acc: 37.50%,  total acc: 37.50%   [EVAL] batch:    1 | acc: 25.00%,  total acc: 31.25%   [EVAL] batch:    2 | acc: 56.25%,  total acc: 39.58%   [EVAL] batch:    3 | acc: 18.75%,  total acc: 34.38%   [EVAL] batch:    4 | acc: 18.75%,  total acc: 31.25%   [EVAL] batch:    5 | acc: 37.50%,  total acc: 32.29%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 38.39%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 44.53%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 49.31%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 52.50%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 56.82%   [EVAL] batch:   11 | acc: 68.75%,  total acc: 57.81%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 58.65%   [EVAL] batch:   13 | acc: 37.50%,  total acc: 57.14%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 58.33%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 58.20%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 59.19%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 59.38%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 59.87%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 60.94%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 62.80%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 64.49%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 66.03%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 67.45%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 68.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 69.95%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 70.83%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 71.88%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 72.63%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 73.12%   [EVAL] batch:   30 | acc: 81.25%,  total acc: 73.39%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 74.02%   [EVAL] batch:   32 | acc: 87.50%,  total acc: 74.43%   [EVAL] batch:   33 | acc: 43.75%,  total acc: 73.53%   [EVAL] batch:   34 | acc: 62.50%,  total acc: 73.21%   [EVAL] batch:   35 | acc: 37.50%,  total acc: 72.22%   [EVAL] batch:   36 | acc: 43.75%,  total acc: 71.45%   [EVAL] batch:   37 | acc: 43.75%,  total acc: 70.72%   [EVAL] batch:   38 | acc: 62.50%,  total acc: 70.51%   [EVAL] batch:   39 | acc: 81.25%,  total acc: 70.78%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 71.49%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 72.17%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 72.67%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 72.87%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 73.19%   [EVAL] batch:   45 | acc: 81.25%,  total acc: 73.37%   [EVAL] batch:   46 | acc: 75.00%,  total acc: 73.40%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 73.96%   [EVAL] batch:   48 | acc: 75.00%,  total acc: 73.98%   [EVAL] batch:   49 | acc: 18.75%,  total acc: 72.88%   [EVAL] batch:   50 | acc: 25.00%,  total acc: 71.94%   [EVAL] batch:   51 | acc: 31.25%,  total acc: 71.15%   [EVAL] batch:   52 | acc: 31.25%,  total acc: 70.40%   [EVAL] batch:   53 | acc: 75.00%,  total acc: 70.49%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 70.91%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 71.43%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 71.82%   [EVAL] batch:   57 | acc: 18.75%,  total acc: 70.91%   [EVAL] batch:   58 | acc: 6.25%,  total acc: 69.81%   [EVAL] batch:   59 | acc: 18.75%,  total acc: 68.96%   [EVAL] batch:   60 | acc: 25.00%,  total acc: 68.24%   [EVAL] batch:   61 | acc: 18.75%,  total acc: 67.44%   [EVAL] batch:   62 | acc: 68.75%,  total acc: 67.46%   [EVAL] batch:   63 | acc: 37.50%,  total acc: 66.99%   [EVAL] batch:   64 | acc: 43.75%,  total acc: 66.63%   [EVAL] batch:   65 | acc: 37.50%,  total acc: 66.19%   
cur_acc:  ['0.8561', '0.8705', '0.3516', '0.4904']
his_acc:  ['0.8561', '0.8418', '0.7454', '0.6619']
CurrentTrain: epoch  0, batch     0 | loss: 7.4509621CurrentTrain: epoch  0, batch     1 | loss: 7.6229692CurrentTrain: epoch  1, batch     0 | loss: 6.4517164CurrentTrain: epoch  1, batch     1 | loss: 6.8063889CurrentTrain: epoch  2, batch     0 | loss: 6.2756748CurrentTrain: epoch  2, batch     1 | loss: 5.9949131CurrentTrain: epoch  3, batch     0 | loss: 6.2823563CurrentTrain: epoch  3, batch     1 | loss: 5.0309958CurrentTrain: epoch  4, batch     0 | loss: 5.5324550CurrentTrain: epoch  4, batch     1 | loss: 5.5760846CurrentTrain: epoch  5, batch     0 | loss: 4.8942871CurrentTrain: epoch  5, batch     1 | loss: 6.2160068CurrentTrain: epoch  6, batch     0 | loss: 5.3928757CurrentTrain: epoch  6, batch     1 | loss: 4.3853669CurrentTrain: epoch  7, batch     0 | loss: 4.9459243CurrentTrain: epoch  7, batch     1 | loss: 4.5640497CurrentTrain: epoch  8, batch     0 | loss: 4.3522711CurrentTrain: epoch  8, batch     1 | loss: 4.6249728CurrentTrain: epoch  9, batch     0 | loss: 4.4456882CurrentTrain: epoch  9, batch     1 | loss: 3.6296391
#### organization subsidiaries ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization subsidiaries" means an organization 's subsidiary is an organization.
Here is an example:
Relation: organization subsidiaries
Context: venture fund buys sporting chain highland capital 's consumer fund includes lululemon athletica , a yoga retailer , and o beverages , a flavored water company developed by tom first , one of the two `` juice guys '' who cofounded nantucket nectars .
Head Entity: highland capital
Tail Entity: o beverages
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization subsidiaries":
Relation: organization subsidiaries  
Context: The tech giant Alphabet Inc. has several subsidiaries, including YouTube, which has transformed the way people consume video content online.  
Head Entity: Alphabet Inc.  
Tail Entity: YouTube  

Relation: organization subsidiaries  
Context: The automotive manufacturer General Motors has expanded its portfolio by acquiring several companies, including Cruise, a self-driving car startup that aims to revolutionize urban transportation.  
Head Entity: General Motors  
Tail Entity: Cruise  
#### organization parents ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization parents" means an organization 's parent is an organization.
Here is an example:
Relation: organization parents
Context: aseel kami , tim cocks , abdul-rahman taher , tim cocks and michael christie -lrb- reuters -rrb- add an intriguing note in the speculation over who 's responsible for the recent bombings : `` but a senior iraqi intelligence source , who declined to be named , said there was evidence the bombs could be the work of the militant badr organisation , the armed wing of the supreme islamic iraqi council -lrb- isci -rrb- .
Head Entity: badr organisation
Tail Entity: supreme islamic iraqi council
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization parents":
Relation: organization parents  
Context: in a recent merger, tech giant alpha innovations has acquired beta technologies, a leading firm in artificial intelligence solutions, which has been a subsidiary of gamma enterprises for over a decade.  
Head Entity: beta technologies  
Tail Entity: gamma enterprises  

Relation: organization parents  
Context: the renowned fashion brand chic styles has announced its collaboration with trendy apparel, which is a subsidiary of the global retail giant fashion world, to launch a new clothing line.  
Head Entity: trendy apparel  
Tail Entity: fashion world  
#### organization alternate names ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization alternate names" means a person 's alias is a person.
Here is an example:
Relation: organization alternate names
Context: while section 106 of the hyde act openly bans indian testing , and the agreement upholds reinforces that test ban by upholding the applicability of domestic laws , washington has already recommended that the nuclear suppliers group -lrb- nsg -rrb- link its proposed exemption for india to a similar test ban .
Head Entity: nuclear suppliers group
Tail Entity: nsg
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization alternate names":
Relation: organization alternate names  
Context: The International Monetary Fund, often referred to as the IMF, plays a crucial role in global economic stability by providing financial assistance and advice to member countries.  
Head Entity: International Monetary Fund  
Tail Entity: IMF  

Relation: organization alternate names  
Context: The World Health Organization, commonly known as WHO, has been at the forefront of global health initiatives, especially during the COVID-19 pandemic.  
Head Entity: World Health Organization  
Tail Entity: WHO  
#### organization city of headquarters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization city of headquarters" means an organization is located in a city.
Here is an example:
Relation: organization city of headquarters
Context: guy-sheftall entered spelman college in atlanta at age 16 and later earned a master 's in english with a thesis titled `` faulkner 's treatment of women in his major novels . ''
Head Entity: spelman college
Tail Entity: atlanta
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization city of headquarters":
Relation: organization city of headquarters  
Context: the headquarters of tech giant apple inc. is located in cupertino, california, where it has been since 1993.  
Head Entity: apple inc.  
Tail Entity: cupertino  

Relation: organization city of headquarters  
Context: the united nations has its main headquarters in new york city, serving as a hub for international diplomacy.  
Head Entity: united nations  
Tail Entity: new york city  
#### person siblings ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person siblings" means a person 's sibling is a person.
Here is an example:
Relation: person siblings
Context: `` holly and sanjaya are headed to -lsb- the hawaiian island of -rsb- kauai tomorrow morning so she can meet his parents . ''
Head Entity: she
Tail Entity: sanjaya
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person siblings":
Relation: person siblings  
Context: `` during the family reunion, john introduced his sister to everyone, and they all had a great time reminiscing about their childhood. ''  
Head Entity: his sister  
Tail Entity: john  

Relation: person siblings  
Context: `` after the graduation ceremony, emily celebrated with her brother, who had always been her biggest supporter throughout school. ''  
Head Entity: her brother  
Tail Entity: emily  
Mixup data size:  194
MixupTrain:  epoch  0, batch     0 | loss: 5.0081117MixupTrain:  epoch  0, batch     1 | loss: 5.8949631MixupTrain:  epoch  0, batch     2 | loss: 4.3870184MixupTrain:  epoch  0, batch     3 | loss: 4.0681686MixupTrain:  epoch  0, batch     4 | loss: 3.5272449MixupTrain:  epoch  0, batch     5 | loss: 4.5551847MixupTrain:  epoch  0, batch     6 | loss: 3.1837790MixupTrain:  epoch  0, batch     7 | loss: 4.7387712MixupTrain:  epoch  0, batch     8 | loss: 3.1721490MixupTrain:  epoch  0, batch     9 | loss: 3.8351793MixupTrain:  epoch  0, batch    10 | loss: 3.5415741MixupTrain:  epoch  0, batch    11 | loss: 3.8519520MixupTrain:  epoch  0, batch    12 | loss: 2.5226183
MemoryTrain:  epoch  0, batch     0 | loss: 3.0175908MemoryTrain:  epoch  0, batch     1 | loss: 2.6046116MemoryTrain:  epoch  0, batch     2 | loss: 3.8141818MemoryTrain:  epoch  0, batch     3 | loss: 2.7418725MemoryTrain:  epoch  0, batch     4 | loss: 2.9097645MemoryTrain:  epoch  1, batch     0 | loss: 2.5321522MemoryTrain:  epoch  1, batch     1 | loss: 1.9996848MemoryTrain:  epoch  1, batch     2 | loss: 3.1198022MemoryTrain:  epoch  1, batch     3 | loss: 3.5018032MemoryTrain:  epoch  1, batch     4 | loss: 2.9392710MemoryTrain:  epoch  2, batch     0 | loss: 2.3039749MemoryTrain:  epoch  2, batch     1 | loss: 2.4482646MemoryTrain:  epoch  2, batch     2 | loss: 2.7662041MemoryTrain:  epoch  2, batch     3 | loss: 2.2903533MemoryTrain:  epoch  2, batch     4 | loss: 1.9478943MemoryTrain:  epoch  3, batch     0 | loss: 2.5207932MemoryTrain:  epoch  3, batch     1 | loss: 2.3043916MemoryTrain:  epoch  3, batch     2 | loss: 2.3697054MemoryTrain:  epoch  3, batch     3 | loss: 2.3098674MemoryTrain:  epoch  3, batch     4 | loss: 1.9049076MemoryTrain:  epoch  4, batch     0 | loss: 2.4357636MemoryTrain:  epoch  4, batch     1 | loss: 1.6751013MemoryTrain:  epoch  4, batch     2 | loss: 1.8215368MemoryTrain:  epoch  4, batch     3 | loss: 1.6723087MemoryTrain:  epoch  4, batch     4 | loss: 2.2151647MemoryTrain:  epoch  5, batch     0 | loss: 1.6526732MemoryTrain:  epoch  5, batch     1 | loss: 1.6007899MemoryTrain:  epoch  5, batch     2 | loss: 1.6004257MemoryTrain:  epoch  5, batch     3 | loss: 2.0096476MemoryTrain:  epoch  5, batch     4 | loss: 2.1652915MemoryTrain:  epoch  6, batch     0 | loss: 1.9738764MemoryTrain:  epoch  6, batch     1 | loss: 1.9740441MemoryTrain:  epoch  6, batch     2 | loss: 1.4706643MemoryTrain:  epoch  6, batch     3 | loss: 1.6543825MemoryTrain:  epoch  6, batch     4 | loss: 1.4506352MemoryTrain:  epoch  7, batch     0 | loss: 1.8818331MemoryTrain:  epoch  7, batch     1 | loss: 1.5596466MemoryTrain:  epoch  7, batch     2 | loss: 1.4637127MemoryTrain:  epoch  7, batch     3 | loss: 2.0192337MemoryTrain:  epoch  7, batch     4 | loss: 1.7520844MemoryTrain:  epoch  8, batch     0 | loss: 1.7397212MemoryTrain:  epoch  8, batch     1 | loss: 1.4083003MemoryTrain:  epoch  8, batch     2 | loss: 1.5233052MemoryTrain:  epoch  8, batch     3 | loss: 1.6145761MemoryTrain:  epoch  8, batch     4 | loss: 1.9569418MemoryTrain:  epoch  9, batch     0 | loss: 1.4651446MemoryTrain:  epoch  9, batch     1 | loss: 1.6181645MemoryTrain:  epoch  9, batch     2 | loss: 1.4566571MemoryTrain:  epoch  9, batch     3 | loss: 1.6178572MemoryTrain:  epoch  9, batch     4 | loss: 1.6189113
[EVAL] batch:    0 | acc: 18.75%,  total acc: 18.75%   [EVAL] batch:    1 | acc: 43.75%,  total acc: 31.25%   [EVAL] batch:    2 | acc: 12.50%,  total acc: 25.00%   [EVAL] batch:    3 | acc: 0.00%,  total acc: 18.75%   [EVAL] batch:    4 | acc: 0.00%,  total acc: 15.00%   [EVAL] batch:    5 | acc: 0.00%,  total acc: 12.50%   [EVAL] batch:    6 | acc: 25.00%,  total acc: 14.29%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 22.66%   [EVAL] batch:    8 | acc: 75.00%,  total acc: 28.47%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 31.87%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 36.93%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 41.15%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 44.23%   [EVAL] batch:   13 | acc: 100.00%,  total acc: 48.21%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 51.25%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 54.30%   [EVAL] batch:   16 | acc: 87.50%,  total acc: 56.25%   [EVAL] batch:   17 | acc: 87.50%,  total acc: 57.99%   [EVAL] batch:   18 | acc: 25.00%,  total acc: 56.25%   [EVAL] batch:   19 | acc: 37.50%,  total acc: 55.31%   [EVAL] batch:   20 | acc: 18.75%,  total acc: 53.57%   [EVAL] batch:   21 | acc: 12.50%,  total acc: 51.70%   
[EVAL] batch:    0 | acc: 56.25%,  total acc: 56.25%   [EVAL] batch:    1 | acc: 50.00%,  total acc: 53.12%   [EVAL] batch:    2 | acc: 56.25%,  total acc: 54.17%   [EVAL] batch:    3 | acc: 25.00%,  total acc: 46.88%   [EVAL] batch:    4 | acc: 25.00%,  total acc: 42.50%   [EVAL] batch:    5 | acc: 43.75%,  total acc: 42.71%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 47.32%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 53.91%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 58.33%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 60.62%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 64.20%   [EVAL] batch:   11 | acc: 68.75%,  total acc: 64.58%   [EVAL] batch:   12 | acc: 62.50%,  total acc: 64.42%   [EVAL] batch:   13 | acc: 31.25%,  total acc: 62.05%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 62.92%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 62.50%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 63.24%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 63.19%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 63.49%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 64.38%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 66.07%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 67.61%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 68.75%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 70.05%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 71.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 72.36%   [EVAL] batch:   26 | acc: 81.25%,  total acc: 72.69%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 73.44%   [EVAL] batch:   28 | acc: 81.25%,  total acc: 73.71%   [EVAL] batch:   29 | acc: 68.75%,  total acc: 73.54%   [EVAL] batch:   30 | acc: 75.00%,  total acc: 73.59%   [EVAL] batch:   31 | acc: 81.25%,  total acc: 73.83%   [EVAL] batch:   32 | acc: 81.25%,  total acc: 74.05%   [EVAL] batch:   33 | acc: 43.75%,  total acc: 73.16%   [EVAL] batch:   34 | acc: 56.25%,  total acc: 72.68%   [EVAL] batch:   35 | acc: 31.25%,  total acc: 71.53%   [EVAL] batch:   36 | acc: 18.75%,  total acc: 70.10%   [EVAL] batch:   37 | acc: 37.50%,  total acc: 69.24%   [EVAL] batch:   38 | acc: 37.50%,  total acc: 68.43%   [EVAL] batch:   39 | acc: 81.25%,  total acc: 68.75%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 69.51%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 70.09%   [EVAL] batch:   42 | acc: 87.50%,  total acc: 70.49%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 70.74%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 71.25%   [EVAL] batch:   45 | acc: 68.75%,  total acc: 71.20%   [EVAL] batch:   46 | acc: 75.00%,  total acc: 71.28%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 71.88%   [EVAL] batch:   48 | acc: 68.75%,  total acc: 71.81%   [EVAL] batch:   49 | acc: 18.75%,  total acc: 70.75%   [EVAL] batch:   50 | acc: 25.00%,  total acc: 69.85%   [EVAL] batch:   51 | acc: 31.25%,  total acc: 69.11%   [EVAL] batch:   52 | acc: 18.75%,  total acc: 68.16%   [EVAL] batch:   53 | acc: 75.00%,  total acc: 68.29%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 68.75%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 69.31%   [EVAL] batch:   56 | acc: 87.50%,  total acc: 69.63%   [EVAL] batch:   57 | acc: 18.75%,  total acc: 68.75%   [EVAL] batch:   58 | acc: 18.75%,  total acc: 67.90%   [EVAL] batch:   59 | acc: 6.25%,  total acc: 66.88%   [EVAL] batch:   60 | acc: 12.50%,  total acc: 65.98%   [EVAL] batch:   61 | acc: 6.25%,  total acc: 65.02%   [EVAL] batch:   62 | acc: 12.50%,  total acc: 64.19%   [EVAL] batch:   63 | acc: 18.75%,  total acc: 63.48%   [EVAL] batch:   64 | acc: 62.50%,  total acc: 63.46%   [EVAL] batch:   65 | acc: 43.75%,  total acc: 63.16%   [EVAL] batch:   66 | acc: 31.25%,  total acc: 62.69%   [EVAL] batch:   67 | acc: 31.25%,  total acc: 62.22%   [EVAL] batch:   68 | acc: 12.50%,  total acc: 61.50%   [EVAL] batch:   69 | acc: 0.00%,  total acc: 60.62%   [EVAL] batch:   70 | acc: 0.00%,  total acc: 59.77%   [EVAL] batch:   71 | acc: 0.00%,  total acc: 58.94%   [EVAL] batch:   72 | acc: 50.00%,  total acc: 58.82%   [EVAL] batch:   73 | acc: 75.00%,  total acc: 59.04%   [EVAL] batch:   74 | acc: 68.75%,  total acc: 59.17%   [EVAL] batch:   75 | acc: 75.00%,  total acc: 59.38%   [EVAL] batch:   76 | acc: 87.50%,  total acc: 59.74%   [EVAL] batch:   77 | acc: 81.25%,  total acc: 60.02%   [EVAL] batch:   78 | acc: 87.50%,  total acc: 60.36%   [EVAL] batch:   79 | acc: 100.00%,  total acc: 60.86%   [EVAL] batch:   80 | acc: 93.75%,  total acc: 61.27%   [EVAL] batch:   81 | acc: 100.00%,  total acc: 61.74%   [EVAL] batch:   82 | acc: 81.25%,  total acc: 61.97%   [EVAL] batch:   83 | acc: 75.00%,  total acc: 62.13%   [EVAL] batch:   84 | acc: 37.50%,  total acc: 61.84%   [EVAL] batch:   85 | acc: 18.75%,  total acc: 61.34%   [EVAL] batch:   86 | acc: 25.00%,  total acc: 60.92%   [EVAL] batch:   87 | acc: 6.25%,  total acc: 60.30%   
cur_acc:  ['0.8561', '0.8705', '0.3516', '0.4904', '0.5170']
his_acc:  ['0.8561', '0.8418', '0.7454', '0.6619', '0.6030']
CurrentTrain: epoch  0, batch     0 | loss: 4.8016582CurrentTrain: epoch  0, batch     1 | loss: 4.5383182CurrentTrain: epoch  1, batch     0 | loss: 3.7786398CurrentTrain: epoch  1, batch     1 | loss: 3.2816575CurrentTrain: epoch  2, batch     0 | loss: 3.3700650CurrentTrain: epoch  2, batch     1 | loss: 2.5838335CurrentTrain: epoch  3, batch     0 | loss: 2.9281664CurrentTrain: epoch  3, batch     1 | loss: 2.6363702CurrentTrain: epoch  4, batch     0 | loss: 2.7256279CurrentTrain: epoch  4, batch     1 | loss: 2.4521463CurrentTrain: epoch  5, batch     0 | loss: 2.3430781CurrentTrain: epoch  5, batch     1 | loss: 2.1829484CurrentTrain: epoch  6, batch     0 | loss: 2.2636387CurrentTrain: epoch  6, batch     1 | loss: 2.0548906CurrentTrain: epoch  7, batch     0 | loss: 2.1583600CurrentTrain: epoch  7, batch     1 | loss: 2.0758069CurrentTrain: epoch  8, batch     0 | loss: 2.2007420CurrentTrain: epoch  8, batch     1 | loss: 1.9417326CurrentTrain: epoch  9, batch     0 | loss: 2.0566769CurrentTrain: epoch  9, batch     1 | loss: 1.9055086
#### organization founded ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization founded" means an organization was found in a date.
Here is an example:
Relation: organization founded
Context: the company was founded as a hobby in 1979 by the husband and wife team of tim and nina zagat , a pair of lawyers who started recuiting friends to provide their own ratings of the food , decor and service of restaurants they frequented .
Head Entity: zagat
Tail Entity: 1979
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization founded":
Relation: organization founded  
Context: In 1995, the tech startup was established by a group of engineers who aimed to revolutionize the software industry with innovative solutions.  
Head Entity: tech startup  
Tail Entity: 1995  

Relation: organization founded  
Context: The nonprofit organization was created in 2001 to support local artists and promote cultural events in the community.  
Head Entity: nonprofit organization  
Tail Entity: 2001  
#### person age ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person age" means a person 's age is a number.
Here is an example:
Relation: person age
Context: virginia republican jo ann davis passed away on saturday at the age of 57 .
Head Entity: jo ann davis
Tail Entity: 57
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person age":
Relation: person age  
Context: the famous actor robert downey jr. celebrated his 56th birthday last week.  
Head Entity: robert downey jr.  
Tail Entity: 56  

Relation: person age  
Context: on her 30th birthday, emily decided to throw a big party with all her friends.  
Head Entity: emily  
Tail Entity: 30  
#### person city of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person city of birth" means a person was born in a city.
Here is an example:
Relation: person city of birth
Context: rothman was born in san francisco in 1932 in an orthodox jewish family .
Head Entity: rothman
Tail Entity: san francisco
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person city of birth":
Relation: person city of birth  
Context: elena grew up in barcelona, where she developed a passion for art and culture.  
Head Entity: elena  
Tail Entity: barcelona  

Relation: person city of birth  
Context: during an interview, john revealed that he was born in new york city and later moved to los angeles.  
Head Entity: john  
Tail Entity: new york city  
#### organization members ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization members" means an organization 's member is an organization.
Here is an example:
Relation: organization members
Context: it was berger who made clarke a member of the white house principals committee when it met to discuss terrorist threats , allowing an otherwise middle-ranking nsc bureaucrat to treat tenet and secretary of state madeleine albright as equals -lrb- which the empire-building clarke was pleased to do -rrb- .
Head Entity: nsc
Tail Entity: white house principals committee
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization members":
Relation: organization members  
Context: The board of directors of the tech startup includes several prominent figures from the industry, such as the CEO of Innovatech, who has been instrumental in guiding the company’s strategic direction.  
Head Entity: tech startup  
Tail Entity: Innovatech  

Relation: organization members  
Context: During the annual conference, the president of the environmental advocacy group announced the inclusion of several new organizations, highlighting the partnership with Green Future, which focuses on sustainable practices.  
Head Entity: environmental advocacy group  
Tail Entity: Green Future  
#### person religion ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person religion" means an person is the member of a religion.
Here is an example:
Relation: person religion
Context: the pope defended his action on the grounds that he could not refuse an audience to a head of state from a country with a strong catholic tradition unless he had clear-cut proof of the allegations against him .
Head Entity: he
Tail Entity: catholic
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person religion":
Relation: person religion  
Context: During the ceremony, the rabbi spoke about the importance of faith and community in Judaism, emphasizing how every member plays a vital role in upholding their traditions.  
Head Entity: rabbi  
Tail Entity: Judaism  

Relation: person religion  
Context: The young woman shared her experiences growing up in a Muslim household, highlighting the values of compassion and charity that are central to her beliefs.  
Head Entity: young woman  
Tail Entity: Muslim
Mixup data size:  226
MixupTrain:  epoch  0, batch     0 | loss: 3.1713124MixupTrain:  epoch  0, batch     1 | loss: 2.6516145MixupTrain:  epoch  0, batch     2 | loss: 3.0680661MixupTrain:  epoch  0, batch     3 | loss: 3.8564498MixupTrain:  epoch  0, batch     4 | loss: 3.0448617MixupTrain:  epoch  0, batch     5 | loss: 3.2861566MixupTrain:  epoch  0, batch     6 | loss: 2.6516068MixupTrain:  epoch  0, batch     7 | loss: 4.0172610MixupTrain:  epoch  0, batch     8 | loss: 3.0742964MixupTrain:  epoch  0, batch     9 | loss: 3.0743702MixupTrain:  epoch  0, batch    10 | loss: 3.3654159MixupTrain:  epoch  0, batch    11 | loss: 2.7444453MixupTrain:  epoch  0, batch    12 | loss: 2.7069183MixupTrain:  epoch  0, batch    13 | loss: 3.5390234MixupTrain:  epoch  0, batch    14 | loss: 3.2051225
MemoryTrain:  epoch  0, batch     0 | loss: 2.4167042MemoryTrain:  epoch  0, batch     1 | loss: 2.7650220MemoryTrain:  epoch  0, batch     2 | loss: 2.6641817MemoryTrain:  epoch  0, batch     3 | loss: 3.0258842MemoryTrain:  epoch  0, batch     4 | loss: 2.6964812MemoryTrain:  epoch  0, batch     5 | loss: 2.6102414MemoryTrain:  epoch  1, batch     0 | loss: 2.5275745MemoryTrain:  epoch  1, batch     1 | loss: 2.6699572MemoryTrain:  epoch  1, batch     2 | loss: 2.4776416MemoryTrain:  epoch  1, batch     3 | loss: 2.5687220MemoryTrain:  epoch  1, batch     4 | loss: 1.9312794MemoryTrain:  epoch  1, batch     5 | loss: 2.4362824MemoryTrain:  epoch  2, batch     0 | loss: 1.9639438MemoryTrain:  epoch  2, batch     1 | loss: 2.2815528MemoryTrain:  epoch  2, batch     2 | loss: 1.8628712MemoryTrain:  epoch  2, batch     3 | loss: 1.7797267MemoryTrain:  epoch  2, batch     4 | loss: 1.9643583MemoryTrain:  epoch  2, batch     5 | loss: 2.0266099MemoryTrain:  epoch  3, batch     0 | loss: 1.7743886MemoryTrain:  epoch  3, batch     1 | loss: 1.7038386MemoryTrain:  epoch  3, batch     2 | loss: 2.2796431MemoryTrain:  epoch  3, batch     3 | loss: 1.6766231MemoryTrain:  epoch  3, batch     4 | loss: 1.6977690MemoryTrain:  epoch  3, batch     5 | loss: 1.8853446MemoryTrain:  epoch  4, batch     0 | loss: 1.7062737MemoryTrain:  epoch  4, batch     1 | loss: 1.8565150MemoryTrain:  epoch  4, batch     2 | loss: 1.9849080MemoryTrain:  epoch  4, batch     3 | loss: 1.4541749MemoryTrain:  epoch  4, batch     4 | loss: 1.6925218MemoryTrain:  epoch  4, batch     5 | loss: 1.5780667MemoryTrain:  epoch  5, batch     0 | loss: 1.6803577MemoryTrain:  epoch  5, batch     1 | loss: 1.6826211MemoryTrain:  epoch  5, batch     2 | loss: 1.5601974MemoryTrain:  epoch  5, batch     3 | loss: 1.4786215MemoryTrain:  epoch  5, batch     4 | loss: 1.3609899MemoryTrain:  epoch  5, batch     5 | loss: 1.8912619MemoryTrain:  epoch  6, batch     0 | loss: 1.5599673MemoryTrain:  epoch  6, batch     1 | loss: 1.5306426MemoryTrain:  epoch  6, batch     2 | loss: 1.4566910MemoryTrain:  epoch  6, batch     3 | loss: 1.7335473MemoryTrain:  epoch  6, batch     4 | loss: 1.8328242MemoryTrain:  epoch  6, batch     5 | loss: 1.5426234MemoryTrain:  epoch  7, batch     0 | loss: 1.8269178MemoryTrain:  epoch  7, batch     1 | loss: 1.4150850MemoryTrain:  epoch  7, batch     2 | loss: 1.4716412MemoryTrain:  epoch  7, batch     3 | loss: 1.4658812MemoryTrain:  epoch  7, batch     4 | loss: 1.4342841MemoryTrain:  epoch  7, batch     5 | loss: 1.6933089MemoryTrain:  epoch  8, batch     0 | loss: 1.4978036MemoryTrain:  epoch  8, batch     1 | loss: 1.4780406MemoryTrain:  epoch  8, batch     2 | loss: 1.5031353MemoryTrain:  epoch  8, batch     3 | loss: 1.3182983MemoryTrain:  epoch  8, batch     4 | loss: 1.3968180MemoryTrain:  epoch  8, batch     5 | loss: 1.4887254MemoryTrain:  epoch  9, batch     0 | loss: 1.2781162MemoryTrain:  epoch  9, batch     1 | loss: 1.3781452MemoryTrain:  epoch  9, batch     2 | loss: 1.4523978MemoryTrain:  epoch  9, batch     3 | loss: 1.4304965MemoryTrain:  epoch  9, batch     4 | loss: 1.3390393MemoryTrain:  epoch  9, batch     5 | loss: 1.6067392
[EVAL] batch:    0 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 96.88%   [EVAL] batch:    2 | acc: 100.00%,  total acc: 97.92%   [EVAL] batch:    3 | acc: 100.00%,  total acc: 98.44%   [EVAL] batch:    4 | acc: 100.00%,  total acc: 98.75%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 98.96%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 99.11%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 99.22%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 97.92%   [EVAL] batch:    9 | acc: 37.50%,  total acc: 91.88%   [EVAL] batch:   10 | acc: 68.75%,  total acc: 89.77%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 89.58%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 88.46%   [EVAL] batch:   13 | acc: 62.50%,  total acc: 86.61%   
[EVAL] batch:    0 | acc: 50.00%,  total acc: 50.00%   [EVAL] batch:    1 | acc: 43.75%,  total acc: 46.88%   [EVAL] batch:    2 | acc: 62.50%,  total acc: 52.08%   [EVAL] batch:    3 | acc: 31.25%,  total acc: 46.88%   [EVAL] batch:    4 | acc: 43.75%,  total acc: 46.25%   [EVAL] batch:    5 | acc: 56.25%,  total acc: 47.92%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 50.89%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 57.03%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 61.11%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 62.50%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 64.77%   [EVAL] batch:   11 | acc: 68.75%,  total acc: 65.10%   [EVAL] batch:   12 | acc: 37.50%,  total acc: 62.98%   [EVAL] batch:   13 | acc: 31.25%,  total acc: 60.71%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 61.25%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 60.94%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 61.76%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 61.81%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 62.17%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 62.81%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 64.58%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 66.19%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 67.39%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 68.49%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 69.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 70.91%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 71.76%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 72.77%   [EVAL] batch:   28 | acc: 87.50%,  total acc: 73.28%   [EVAL] batch:   29 | acc: 68.75%,  total acc: 73.12%   [EVAL] batch:   30 | acc: 75.00%,  total acc: 73.19%   [EVAL] batch:   31 | acc: 81.25%,  total acc: 73.44%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 74.05%   [EVAL] batch:   33 | acc: 37.50%,  total acc: 72.98%   [EVAL] batch:   34 | acc: 62.50%,  total acc: 72.68%   [EVAL] batch:   35 | acc: 31.25%,  total acc: 71.53%   [EVAL] batch:   36 | acc: 25.00%,  total acc: 70.27%   [EVAL] batch:   37 | acc: 37.50%,  total acc: 69.41%   [EVAL] batch:   38 | acc: 50.00%,  total acc: 68.91%   [EVAL] batch:   39 | acc: 81.25%,  total acc: 69.22%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 69.97%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 70.68%   [EVAL] batch:   42 | acc: 87.50%,  total acc: 71.08%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 71.31%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 71.94%   [EVAL] batch:   45 | acc: 68.75%,  total acc: 71.88%   [EVAL] batch:   46 | acc: 75.00%,  total acc: 71.94%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 72.53%   [EVAL] batch:   48 | acc: 56.25%,  total acc: 72.19%   [EVAL] batch:   49 | acc: 18.75%,  total acc: 71.12%   [EVAL] batch:   50 | acc: 31.25%,  total acc: 70.34%   [EVAL] batch:   51 | acc: 31.25%,  total acc: 69.59%   [EVAL] batch:   52 | acc: 18.75%,  total acc: 68.63%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 68.87%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 69.32%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 69.64%   [EVAL] batch:   56 | acc: 75.00%,  total acc: 69.74%   [EVAL] batch:   57 | acc: 18.75%,  total acc: 68.86%   [EVAL] batch:   58 | acc: 25.00%,  total acc: 68.11%   [EVAL] batch:   59 | acc: 6.25%,  total acc: 67.08%   [EVAL] batch:   60 | acc: 18.75%,  total acc: 66.29%   [EVAL] batch:   61 | acc: 12.50%,  total acc: 65.42%   [EVAL] batch:   62 | acc: 25.00%,  total acc: 64.78%   [EVAL] batch:   63 | acc: 31.25%,  total acc: 64.26%   [EVAL] batch:   64 | acc: 62.50%,  total acc: 64.23%   [EVAL] batch:   65 | acc: 37.50%,  total acc: 63.83%   [EVAL] batch:   66 | acc: 18.75%,  total acc: 63.15%   [EVAL] batch:   67 | acc: 0.00%,  total acc: 62.22%   [EVAL] batch:   68 | acc: 6.25%,  total acc: 61.41%   [EVAL] batch:   69 | acc: 0.00%,  total acc: 60.54%   [EVAL] batch:   70 | acc: 0.00%,  total acc: 59.68%   [EVAL] batch:   71 | acc: 0.00%,  total acc: 58.85%   [EVAL] batch:   72 | acc: 37.50%,  total acc: 58.56%   [EVAL] batch:   73 | acc: 50.00%,  total acc: 58.45%   [EVAL] batch:   74 | acc: 56.25%,  total acc: 58.42%   [EVAL] batch:   75 | acc: 56.25%,  total acc: 58.39%   [EVAL] batch:   76 | acc: 81.25%,  total acc: 58.69%   [EVAL] batch:   77 | acc: 68.75%,  total acc: 58.81%   [EVAL] batch:   78 | acc: 81.25%,  total acc: 59.10%   [EVAL] batch:   79 | acc: 100.00%,  total acc: 59.61%   [EVAL] batch:   80 | acc: 93.75%,  total acc: 60.03%   [EVAL] batch:   81 | acc: 93.75%,  total acc: 60.44%   [EVAL] batch:   82 | acc: 81.25%,  total acc: 60.69%   [EVAL] batch:   83 | acc: 75.00%,  total acc: 60.86%   [EVAL] batch:   84 | acc: 12.50%,  total acc: 60.29%   [EVAL] batch:   85 | acc: 12.50%,  total acc: 59.74%   [EVAL] batch:   86 | acc: 12.50%,  total acc: 59.20%   [EVAL] batch:   87 | acc: 87.50%,  total acc: 59.52%   [EVAL] batch:   88 | acc: 93.75%,  total acc: 59.90%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 60.35%   [EVAL] batch:   90 | acc: 100.00%,  total acc: 60.78%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 61.21%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 61.63%   [EVAL] batch:   93 | acc: 100.00%,  total acc: 62.03%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 62.43%   [EVAL] batch:   95 | acc: 93.75%,  total acc: 62.76%   [EVAL] batch:   96 | acc: 37.50%,  total acc: 62.50%   [EVAL] batch:   97 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:   98 | acc: 87.50%,  total acc: 62.75%   [EVAL] batch:   99 | acc: 81.25%,  total acc: 62.94%   [EVAL] batch:  100 | acc: 75.00%,  total acc: 63.06%   
cur_acc:  ['0.8561', '0.8705', '0.3516', '0.4904', '0.5170', '0.8661']
his_acc:  ['0.8561', '0.8418', '0.7454', '0.6619', '0.6030', '0.6306']
CurrentTrain: epoch  0, batch     0 | loss: 6.4482188CurrentTrain: epoch  0, batch     1 | loss: 5.9689579CurrentTrain: epoch  1, batch     0 | loss: 5.0214510CurrentTrain: epoch  1, batch     1 | loss: 4.6092744CurrentTrain: epoch  2, batch     0 | loss: 4.8295636CurrentTrain: epoch  2, batch     1 | loss: 4.2136436CurrentTrain: epoch  3, batch     0 | loss: 4.1963801CurrentTrain: epoch  3, batch     1 | loss: 4.4917932CurrentTrain: epoch  4, batch     0 | loss: 4.3083229CurrentTrain: epoch  4, batch     1 | loss: 3.3584256CurrentTrain: epoch  5, batch     0 | loss: 3.5942616CurrentTrain: epoch  5, batch     1 | loss: 4.2932992CurrentTrain: epoch  6, batch     0 | loss: 3.7862499CurrentTrain: epoch  6, batch     1 | loss: 3.3455739CurrentTrain: epoch  7, batch     0 | loss: 3.1639376CurrentTrain: epoch  7, batch     1 | loss: 3.4975617CurrentTrain: epoch  8, batch     0 | loss: 2.8250084CurrentTrain: epoch  8, batch     1 | loss: 3.3699868CurrentTrain: epoch  9, batch     0 | loss: 2.9014606CurrentTrain: epoch  9, batch     1 | loss: 2.7348549
#### person stateorprovinces of residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person stateorprovinces of residence" means a person is living in a state or province.
Here is an example:
Relation: person stateorprovinces of residence
Context: the chairman of the senate foreign relations committee , massachusetts democrat john kerry , and the panel 's top republican , richard lugar of indiana , were at the white house meeting , which was led by vice president joe biden , a former chairman of the foreign relations panel .
Head Entity: john kerry
Tail Entity: massachusetts
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person stateorprovinces of residence":
Relation: person stateorprovinces of residence  
Context: After moving from New York to California, actress Emma Stone quickly adapted to the vibrant lifestyle of Los Angeles, where she now resides.  
Head Entity: Emma Stone  
Tail Entity: California  

Relation: person stateorprovinces of residence  
Context: Following his successful career in the tech industry, Mark Zuckerberg decided to settle in Palo Alto, a city known for its innovation and proximity to Silicon Valley.  
Head Entity: Mark Zuckerberg  
Tail Entity: Palo Alto  
#### person date of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person date of death" means a person was died on a date.
Here is an example:
Relation: person date of death
Context: he was taken off life support on feb. 14 .
Head Entity: he
Tail Entity: feb. 14
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person date of death":
Relation: person date of death  
Context: The renowned author passed away on March 5, 2020, leaving behind a legacy of great literature.  
Head Entity: The renowned author  
Tail Entity: March 5, 2020  

Relation: person date of death  
Context: She succumbed to her illness on July 22, 2018, after a long battle.  
Head Entity: She  
Tail Entity: July 22, 2018  
#### organization number of employees members ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization number of employees members" means the number of an organization 's employees is a number.
Here is an example:
Relation: organization number of employees members
Context: mcgovern wants to make the red cross , with 30,000 employees and 700 chapters , more efficient .
Head Entity: red cross
Tail Entity: 30,000
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization number of employees members":
Relation: organization number of employees members  
Context: The tech giant Google boasts a workforce of over 150,000 employees worldwide, contributing to its innovative projects.  
Head Entity: Google  
Tail Entity: 150,000  

Relation: organization number of employees members  
Context: With a dedicated team of 5,000 staff members, the nonprofit organization Habitat for Humanity continues to build homes for those in need.  
Head Entity: Habitat for Humanity  
Tail Entity: 5,000  
#### person alternate names ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person alternate names" means a person 's alias is a person.
Here is an example:
Relation: person alternate names
Context: `` i am known in the hospice as the man who would n't die , '' buchwald wrote in march .
Head Entity: buchwald
Tail Entity: man who would n't die
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person alternate names":
Relation: person alternate names  
Context: `` the famous author often referred to as the bard of Avon, is known for his timeless plays and sonnets. ''  
Head Entity: William Shakespeare  
Tail Entity: bard of Avon  

Relation: person alternate names  
Context: `` during his career, he was affectionately called the king of pop, captivating millions with his music. ''  
Head Entity: Michael Jackson  
Tail Entity: king of pop  
#### person spouse ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person spouse" means a person 's spouse is a person.
Here is an example:
Relation: person spouse
Context: smits stands at the center of this multigenerational saga as alex vega , the adopted son of rum and sugar baron pancho duque -lrb- elizondo -rrb- and his wife , amalia -lrb- moreno -rrb- .
Head Entity: elizondo
Tail Entity: moreno
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person spouse":
Relation: person spouse  
Context: in a heartfelt ceremony, john and his beloved partner, sarah, exchanged vows surrounded by family and friends, celebrating their love and commitment to each other.  
Head Entity: john  
Tail Entity: sarah  

Relation: person spouse  
Context: after years of companionship, the couple, michael and jessica, finally tied the knot in a beautiful garden wedding, marking the beginning of their new life together.  
Head Entity: michael  
Tail Entity: jessica  
Mixup data size:  254
MixupTrain:  epoch  0, batch     0 | loss: 2.3347857MixupTrain:  epoch  0, batch     1 | loss: 3.2513003MixupTrain:  epoch  0, batch     2 | loss: 2.3446365MixupTrain:  epoch  0, batch     3 | loss: 2.2427218MixupTrain:  epoch  0, batch     4 | loss: 2.4515472MixupTrain:  epoch  0, batch     5 | loss: 4.5463291MixupTrain:  epoch  0, batch     6 | loss: 2.4740426MixupTrain:  epoch  0, batch     7 | loss: 3.7553261MixupTrain:  epoch  0, batch     8 | loss: 2.6026754MixupTrain:  epoch  0, batch     9 | loss: 2.8707864MixupTrain:  epoch  0, batch    10 | loss: 2.4555969MixupTrain:  epoch  0, batch    11 | loss: 2.3897252MixupTrain:  epoch  0, batch    12 | loss: 2.9076089MixupTrain:  epoch  0, batch    13 | loss: 3.3802135MixupTrain:  epoch  0, batch    14 | loss: 2.9108773MixupTrain:  epoch  0, batch    15 | loss: 2.5618933
MemoryTrain:  epoch  0, batch     0 | loss: 2.5022120MemoryTrain:  epoch  0, batch     1 | loss: 2.3455234MemoryTrain:  epoch  0, batch     2 | loss: 2.7502959MemoryTrain:  epoch  0, batch     3 | loss: 1.8879352MemoryTrain:  epoch  0, batch     4 | loss: 1.8275667MemoryTrain:  epoch  0, batch     5 | loss: 2.1610596MemoryTrain:  epoch  0, batch     6 | loss: 2.1516309MemoryTrain:  epoch  1, batch     0 | loss: 3.1773753MemoryTrain:  epoch  1, batch     1 | loss: 1.7810628MemoryTrain:  epoch  1, batch     2 | loss: 1.8078833MemoryTrain:  epoch  1, batch     3 | loss: 2.3484168MemoryTrain:  epoch  1, batch     4 | loss: 1.4623460MemoryTrain:  epoch  1, batch     5 | loss: 1.4604459MemoryTrain:  epoch  1, batch     6 | loss: 1.5760410MemoryTrain:  epoch  2, batch     0 | loss: 1.6998123MemoryTrain:  epoch  2, batch     1 | loss: 1.6525241MemoryTrain:  epoch  2, batch     2 | loss: 1.9491541MemoryTrain:  epoch  2, batch     3 | loss: 1.6546308MemoryTrain:  epoch  2, batch     4 | loss: 2.0725865MemoryTrain:  epoch  2, batch     5 | loss: 1.9266838MemoryTrain:  epoch  2, batch     6 | loss: 1.8057437MemoryTrain:  epoch  3, batch     0 | loss: 2.0601234MemoryTrain:  epoch  3, batch     1 | loss: 1.5228747MemoryTrain:  epoch  3, batch     2 | loss: 1.4286230MemoryTrain:  epoch  3, batch     3 | loss: 1.7018952MemoryTrain:  epoch  3, batch     4 | loss: 1.6323464MemoryTrain:  epoch  3, batch     5 | loss: 1.9793304MemoryTrain:  epoch  3, batch     6 | loss: 1.4541655MemoryTrain:  epoch  4, batch     0 | loss: 1.7000843MemoryTrain:  epoch  4, batch     1 | loss: 1.7298222MemoryTrain:  epoch  4, batch     2 | loss: 1.3561978MemoryTrain:  epoch  4, batch     3 | loss: 1.4976041MemoryTrain:  epoch  4, batch     4 | loss: 1.5705621MemoryTrain:  epoch  4, batch     5 | loss: 1.7402056MemoryTrain:  epoch  4, batch     6 | loss: 1.9569442MemoryTrain:  epoch  5, batch     0 | loss: 1.7038147MemoryTrain:  epoch  5, batch     1 | loss: 1.4492130MemoryTrain:  epoch  5, batch     2 | loss: 1.5034198MemoryTrain:  epoch  5, batch     3 | loss: 1.4273869MemoryTrain:  epoch  5, batch     4 | loss: 1.7837763MemoryTrain:  epoch  5, batch     5 | loss: 1.3791671MemoryTrain:  epoch  5, batch     6 | loss: 1.3673861MemoryTrain:  epoch  6, batch     0 | loss: 1.6053443MemoryTrain:  epoch  6, batch     1 | loss: 1.6093397MemoryTrain:  epoch  6, batch     2 | loss: 1.5158041MemoryTrain:  epoch  6, batch     3 | loss: 1.4864621MemoryTrain:  epoch  6, batch     4 | loss: 1.4071581MemoryTrain:  epoch  6, batch     5 | loss: 1.3657209MemoryTrain:  epoch  6, batch     6 | loss: 1.3472736MemoryTrain:  epoch  7, batch     0 | loss: 1.4828708MemoryTrain:  epoch  7, batch     1 | loss: 1.4145117MemoryTrain:  epoch  7, batch     2 | loss: 1.4233134MemoryTrain:  epoch  7, batch     3 | loss: 1.4033504MemoryTrain:  epoch  7, batch     4 | loss: 1.3340394MemoryTrain:  epoch  7, batch     5 | loss: 1.2678828MemoryTrain:  epoch  7, batch     6 | loss: 1.2411168MemoryTrain:  epoch  8, batch     0 | loss: 1.3673663MemoryTrain:  epoch  8, batch     1 | loss: 1.4471215MemoryTrain:  epoch  8, batch     2 | loss: 1.3184856MemoryTrain:  epoch  8, batch     3 | loss: 1.2662812MemoryTrain:  epoch  8, batch     4 | loss: 1.2624123MemoryTrain:  epoch  8, batch     5 | loss: 1.3278010MemoryTrain:  epoch  8, batch     6 | loss: 1.4124975MemoryTrain:  epoch  9, batch     0 | loss: 1.3712354MemoryTrain:  epoch  9, batch     1 | loss: 1.3261727MemoryTrain:  epoch  9, batch     2 | loss: 1.3756989MemoryTrain:  epoch  9, batch     3 | loss: 1.3631072MemoryTrain:  epoch  9, batch     4 | loss: 1.2607837MemoryTrain:  epoch  9, batch     5 | loss: 1.4269605MemoryTrain:  epoch  9, batch     6 | loss: 1.2872324
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 78.12%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 83.33%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 85.71%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 86.72%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 85.00%   [EVAL] batch:   10 | acc: 43.75%,  total acc: 81.25%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 80.29%   [EVAL] batch:   13 | acc: 75.00%,  total acc: 79.91%   [EVAL] batch:   14 | acc: 43.75%,  total acc: 77.50%   
[EVAL] batch:    0 | acc: 43.75%,  total acc: 43.75%   [EVAL] batch:    1 | acc: 43.75%,  total acc: 43.75%   [EVAL] batch:    2 | acc: 62.50%,  total acc: 50.00%   [EVAL] batch:    3 | acc: 31.25%,  total acc: 45.31%   [EVAL] batch:    4 | acc: 37.50%,  total acc: 43.75%   [EVAL] batch:    5 | acc: 43.75%,  total acc: 43.75%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 47.32%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 53.91%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 58.33%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 60.00%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 63.07%   [EVAL] batch:   11 | acc: 68.75%,  total acc: 63.54%   [EVAL] batch:   12 | acc: 43.75%,  total acc: 62.02%   [EVAL] batch:   13 | acc: 31.25%,  total acc: 59.82%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 60.42%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 60.16%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 61.03%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 61.11%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 61.18%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 61.88%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 63.69%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 65.34%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 66.58%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 67.71%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 69.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 70.19%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 71.06%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 71.88%   [EVAL] batch:   28 | acc: 87.50%,  total acc: 72.41%   [EVAL] batch:   29 | acc: 68.75%,  total acc: 72.29%   [EVAL] batch:   30 | acc: 81.25%,  total acc: 72.58%   [EVAL] batch:   31 | acc: 81.25%,  total acc: 72.85%   [EVAL] batch:   32 | acc: 81.25%,  total acc: 73.11%   [EVAL] batch:   33 | acc: 25.00%,  total acc: 71.69%   [EVAL] batch:   34 | acc: 31.25%,  total acc: 70.54%   [EVAL] batch:   35 | acc: 18.75%,  total acc: 69.10%   [EVAL] batch:   36 | acc: 18.75%,  total acc: 67.74%   [EVAL] batch:   37 | acc: 25.00%,  total acc: 66.61%   [EVAL] batch:   38 | acc: 6.25%,  total acc: 65.06%   [EVAL] batch:   39 | acc: 81.25%,  total acc: 65.47%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 66.16%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 66.96%   [EVAL] batch:   42 | acc: 81.25%,  total acc: 67.30%   [EVAL] batch:   43 | acc: 68.75%,  total acc: 67.33%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 67.78%   [EVAL] batch:   45 | acc: 37.50%,  total acc: 67.12%   [EVAL] batch:   46 | acc: 62.50%,  total acc: 67.02%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 67.58%   [EVAL] batch:   48 | acc: 62.50%,  total acc: 67.47%   [EVAL] batch:   49 | acc: 12.50%,  total acc: 66.38%   [EVAL] batch:   50 | acc: 25.00%,  total acc: 65.56%   [EVAL] batch:   51 | acc: 25.00%,  total acc: 64.78%   [EVAL] batch:   52 | acc: 12.50%,  total acc: 63.80%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 64.12%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 64.66%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 65.07%   [EVAL] batch:   56 | acc: 68.75%,  total acc: 65.13%   [EVAL] batch:   57 | acc: 25.00%,  total acc: 64.44%   [EVAL] batch:   58 | acc: 25.00%,  total acc: 63.77%   [EVAL] batch:   59 | acc: 12.50%,  total acc: 62.92%   [EVAL] batch:   60 | acc: 0.00%,  total acc: 61.89%   [EVAL] batch:   61 | acc: 0.00%,  total acc: 60.89%   [EVAL] batch:   62 | acc: 0.00%,  total acc: 59.92%   [EVAL] batch:   63 | acc: 6.25%,  total acc: 59.08%   [EVAL] batch:   64 | acc: 37.50%,  total acc: 58.75%   [EVAL] batch:   65 | acc: 18.75%,  total acc: 58.14%   [EVAL] batch:   66 | acc: 12.50%,  total acc: 57.46%   [EVAL] batch:   67 | acc: 0.00%,  total acc: 56.62%   [EVAL] batch:   68 | acc: 6.25%,  total acc: 55.89%   [EVAL] batch:   69 | acc: 0.00%,  total acc: 55.09%   [EVAL] batch:   70 | acc: 0.00%,  total acc: 54.31%   [EVAL] batch:   71 | acc: 0.00%,  total acc: 53.56%   [EVAL] batch:   72 | acc: 31.25%,  total acc: 53.25%   [EVAL] batch:   73 | acc: 37.50%,  total acc: 53.04%   [EVAL] batch:   74 | acc: 50.00%,  total acc: 53.00%   [EVAL] batch:   75 | acc: 50.00%,  total acc: 52.96%   [EVAL] batch:   76 | acc: 75.00%,  total acc: 53.25%   [EVAL] batch:   77 | acc: 62.50%,  total acc: 53.37%   [EVAL] batch:   78 | acc: 75.00%,  total acc: 53.64%   [EVAL] batch:   79 | acc: 100.00%,  total acc: 54.22%   [EVAL] batch:   80 | acc: 100.00%,  total acc: 54.78%   [EVAL] batch:   81 | acc: 93.75%,  total acc: 55.26%   [EVAL] batch:   82 | acc: 81.25%,  total acc: 55.57%   [EVAL] batch:   83 | acc: 75.00%,  total acc: 55.80%   [EVAL] batch:   84 | acc: 0.00%,  total acc: 55.15%   [EVAL] batch:   85 | acc: 0.00%,  total acc: 54.51%   [EVAL] batch:   86 | acc: 0.00%,  total acc: 53.88%   [EVAL] batch:   87 | acc: 75.00%,  total acc: 54.12%   [EVAL] batch:   88 | acc: 93.75%,  total acc: 54.56%   [EVAL] batch:   89 | acc: 93.75%,  total acc: 55.00%   [EVAL] batch:   90 | acc: 100.00%,  total acc: 55.49%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 55.98%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 56.45%   [EVAL] batch:   93 | acc: 100.00%,  total acc: 56.91%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 57.37%   [EVAL] batch:   95 | acc: 81.25%,  total acc: 57.62%   [EVAL] batch:   96 | acc: 31.25%,  total acc: 57.35%   [EVAL] batch:   97 | acc: 56.25%,  total acc: 57.33%   [EVAL] batch:   98 | acc: 93.75%,  total acc: 57.70%   [EVAL] batch:   99 | acc: 81.25%,  total acc: 57.94%   [EVAL] batch:  100 | acc: 81.25%,  total acc: 58.17%   [EVAL] batch:  101 | acc: 68.75%,  total acc: 58.27%   [EVAL] batch:  102 | acc: 81.25%,  total acc: 58.50%   [EVAL] batch:  103 | acc: 75.00%,  total acc: 58.65%   [EVAL] batch:  104 | acc: 87.50%,  total acc: 58.93%   [EVAL] batch:  105 | acc: 93.75%,  total acc: 59.26%   [EVAL] batch:  106 | acc: 93.75%,  total acc: 59.58%   [EVAL] batch:  107 | acc: 100.00%,  total acc: 59.95%   [EVAL] batch:  108 | acc: 93.75%,  total acc: 60.26%   [EVAL] batch:  109 | acc: 93.75%,  total acc: 60.57%   [EVAL] batch:  110 | acc: 56.25%,  total acc: 60.53%   [EVAL] batch:  111 | acc: 50.00%,  total acc: 60.44%   [EVAL] batch:  112 | acc: 81.25%,  total acc: 60.62%   [EVAL] batch:  113 | acc: 68.75%,  total acc: 60.69%   [EVAL] batch:  114 | acc: 75.00%,  total acc: 60.82%   [EVAL] batch:  115 | acc: 37.50%,  total acc: 60.61%   
cur_acc:  ['0.8561', '0.8705', '0.3516', '0.4904', '0.5170', '0.8661', '0.7750']
his_acc:  ['0.8561', '0.8418', '0.7454', '0.6619', '0.6030', '0.6306', '0.6061']
CurrentTrain: epoch  0, batch     0 | loss: 5.4637494CurrentTrain: epoch  0, batch     1 | loss: 5.4973817CurrentTrain: epoch  1, batch     0 | loss: 4.3466454CurrentTrain: epoch  1, batch     1 | loss: 3.8622839CurrentTrain: epoch  2, batch     0 | loss: 3.5211542CurrentTrain: epoch  2, batch     1 | loss: 3.0842497CurrentTrain: epoch  3, batch     0 | loss: 3.4171786CurrentTrain: epoch  3, batch     1 | loss: 3.1658795CurrentTrain: epoch  4, batch     0 | loss: 3.1930084CurrentTrain: epoch  4, batch     1 | loss: 2.6385987CurrentTrain: epoch  5, batch     0 | loss: 2.7642007CurrentTrain: epoch  5, batch     1 | loss: 2.3509657CurrentTrain: epoch  6, batch     0 | loss: 2.4050741CurrentTrain: epoch  6, batch     1 | loss: 2.0862131CurrentTrain: epoch  7, batch     0 | loss: 2.2469630CurrentTrain: epoch  7, batch     1 | loss: 2.2615850CurrentTrain: epoch  8, batch     0 | loss: 2.3567748CurrentTrain: epoch  8, batch     1 | loss: 2.1440871CurrentTrain: epoch  9, batch     0 | loss: 2.1711574CurrentTrain: epoch  9, batch     1 | loss: 2.0252764
#### person cities of residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person cities of residence" means a person is living in a city.
Here is an example:
Relation: person cities of residence
Context: in 1974 , divorced and the mother of a 5-year-old , forsberg moved to boston and began studying arms control at mit ; she received her ph.d. in 1980 .
Head Entity: forsberg
Tail Entity: boston
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person cities of residence":
Relation: person cities of residence  
Context: After spending several years in New York, she decided to relocate to San Francisco to pursue her career in tech.  
Head Entity: she  
Tail Entity: San Francisco  

Relation: person cities of residence  
Context: Following his graduation, John moved to Seattle where he found a job in a leading software company.  
Head Entity: John  
Tail Entity: Seattle  
#### person schools attended ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person schools attended" means a person 's school is an organization.
Here is an example:
Relation: person schools attended
Context: born in baltimore in 1922 , parren mitchell was a graduate of morgan state college and earned a master 's degree from the university of maryland , according to biographical information supplied by cummings ' office .
Head Entity: parren mitchell
Tail Entity: university of maryland
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person schools attended":
Relation: person schools attended  
Context: after completing high school in 1995, jessica went on to study at the university of california, los angeles, where she earned her bachelor's degree in sociology.  
Head Entity: jessica  
Tail Entity: university of california, los angeles  

Relation: person schools attended  
Context: during his early years, steven attended the prestigious harvard university, where he majored in computer science and graduated with honors.  
Head Entity: steven  
Tail Entity: harvard university  
#### person country of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person country of death" means a person was died in a country.
Here is an example:
Relation: person country of death
Context: us republican congresswoman jo ann davis dies after fight with breast cancer
Head Entity: jo ann davis
Tail Entity: us
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person country of death":
Relation: person country of death  
Context: renowned physicist stephen hawking passed away in cambridge, england  
Head Entity: stephen hawking  
Tail Entity: england  

Relation: person country of death  
Context: famous author agatha christie died in her home in wallingford, england  
Head Entity: agatha christie  
Tail Entity: england  
#### person children ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person children" means a person 's children is a person.
Here is an example:
Relation: person children
Context: he is survived by two stepdaughters , barbara a. hammond and brenda l. stevenson ; a stepson , michael a. taylor ; two grandchildren and one great-grandchild .
Head Entity: he
Tail Entity: brenda l. stevenson
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person children":
Relation: person children  
Context: she has three children, including her son, john, and her daughters, emily and sarah, who are both pursuing their studies.  
Head Entity: she  
Tail Entity: emily  

Relation: person children  
Context: after the passing of their father, the siblings, including jake and his sister, lila, took on the responsibility of managing the family business together.  
Head Entity: jake  
Tail Entity: lila  
#### person charges ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person charges" means a person was charged with an event.
Here is an example:
Relation: person charges
Context: flowers always contended politics was behind the extortion investigation , but appeals courts ruled against him .
Head Entity: him
Tail Entity: extortion
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person charges":
Relation: person charges  
Context: The prosecutor announced that the former mayor was facing serious allegations related to corruption.  
Head Entity: former mayor  
Tail Entity: corruption  

Relation: person charges  
Context: After a lengthy investigation, the authorities confirmed that the celebrity was implicated in a major drug trafficking case.  
Head Entity: celebrity  
Tail Entity: drug trafficking  
Mixup data size:  284
MixupTrain:  epoch  0, batch     0 | loss: 2.0724245MixupTrain:  epoch  0, batch     1 | loss: 2.0906038MixupTrain:  epoch  0, batch     2 | loss: 2.0292638MixupTrain:  epoch  0, batch     3 | loss: 1.8960396MixupTrain:  epoch  0, batch     4 | loss: 3.2831363MixupTrain:  epoch  0, batch     5 | loss: 2.2972813MixupTrain:  epoch  0, batch     6 | loss: 1.7350380MixupTrain:  epoch  0, batch     7 | loss: 2.3597232MixupTrain:  epoch  0, batch     8 | loss: 2.5118006MixupTrain:  epoch  0, batch     9 | loss: 2.0610373MixupTrain:  epoch  0, batch    10 | loss: 2.7993667MixupTrain:  epoch  0, batch    11 | loss: 2.3003321MixupTrain:  epoch  0, batch    12 | loss: 1.8774226MixupTrain:  epoch  0, batch    13 | loss: 2.3540900MixupTrain:  epoch  0, batch    14 | loss: 2.0182618MixupTrain:  epoch  0, batch    15 | loss: 3.2863809MixupTrain:  epoch  0, batch    16 | loss: 2.5255515MixupTrain:  epoch  0, batch    17 | loss: 1.7945449
MemoryTrain:  epoch  0, batch     0 | loss: 1.9132226MemoryTrain:  epoch  0, batch     1 | loss: 1.5482869MemoryTrain:  epoch  0, batch     2 | loss: 1.9795847MemoryTrain:  epoch  0, batch     3 | loss: 1.7228647MemoryTrain:  epoch  0, batch     4 | loss: 2.1234338MemoryTrain:  epoch  0, batch     5 | loss: 2.1504030MemoryTrain:  epoch  0, batch     6 | loss: 2.1988072MemoryTrain:  epoch  0, batch     7 | loss: 2.2273734MemoryTrain:  epoch  1, batch     0 | loss: 1.8929917MemoryTrain:  epoch  1, batch     1 | loss: 1.5216709MemoryTrain:  epoch  1, batch     2 | loss: 2.2496185MemoryTrain:  epoch  1, batch     3 | loss: 1.5141119MemoryTrain:  epoch  1, batch     4 | loss: 2.0946116MemoryTrain:  epoch  1, batch     5 | loss: 1.9821205MemoryTrain:  epoch  1, batch     6 | loss: 1.6936052MemoryTrain:  epoch  1, batch     7 | loss: 1.3945861MemoryTrain:  epoch  2, batch     0 | loss: 2.1235919MemoryTrain:  epoch  2, batch     1 | loss: 1.7926729MemoryTrain:  epoch  2, batch     2 | loss: 1.4234155MemoryTrain:  epoch  2, batch     3 | loss: 1.6387730MemoryTrain:  epoch  2, batch     4 | loss: 1.5832044MemoryTrain:  epoch  2, batch     5 | loss: 1.5426481MemoryTrain:  epoch  2, batch     6 | loss: 1.9625182MemoryTrain:  epoch  2, batch     7 | loss: 1.3830175MemoryTrain:  epoch  3, batch     0 | loss: 1.3638661MemoryTrain:  epoch  3, batch     1 | loss: 1.4564037MemoryTrain:  epoch  3, batch     2 | loss: 1.3591597MemoryTrain:  epoch  3, batch     3 | loss: 1.5337114MemoryTrain:  epoch  3, batch     4 | loss: 1.7482947MemoryTrain:  epoch  3, batch     5 | loss: 1.3799318MemoryTrain:  epoch  3, batch     6 | loss: 1.5316815MemoryTrain:  epoch  3, batch     7 | loss: 1.4447010MemoryTrain:  epoch  4, batch     0 | loss: 1.4680247MemoryTrain:  epoch  4, batch     1 | loss: 1.3347747MemoryTrain:  epoch  4, batch     2 | loss: 1.5016854MemoryTrain:  epoch  4, batch     3 | loss: 1.3384461MemoryTrain:  epoch  4, batch     4 | loss: 1.2777164MemoryTrain:  epoch  4, batch     5 | loss: 1.3876970MemoryTrain:  epoch  4, batch     6 | loss: 1.4917858MemoryTrain:  epoch  4, batch     7 | loss: 1.3195568MemoryTrain:  epoch  5, batch     0 | loss: 1.3113607MemoryTrain:  epoch  5, batch     1 | loss: 1.3028550MemoryTrain:  epoch  5, batch     2 | loss: 1.3559504MemoryTrain:  epoch  5, batch     3 | loss: 1.3160375MemoryTrain:  epoch  5, batch     4 | loss: 1.5036736MemoryTrain:  epoch  5, batch     5 | loss: 1.3366120MemoryTrain:  epoch  5, batch     6 | loss: 1.2402303MemoryTrain:  epoch  5, batch     7 | loss: 1.4527073MemoryTrain:  epoch  6, batch     0 | loss: 1.3614569MemoryTrain:  epoch  6, batch     1 | loss: 1.2507296MemoryTrain:  epoch  6, batch     2 | loss: 1.5072856MemoryTrain:  epoch  6, batch     3 | loss: 1.3401357MemoryTrain:  epoch  6, batch     4 | loss: 1.3229831MemoryTrain:  epoch  6, batch     5 | loss: 1.4224303MemoryTrain:  epoch  6, batch     6 | loss: 1.2855537MemoryTrain:  epoch  6, batch     7 | loss: 1.3275536MemoryTrain:  epoch  7, batch     0 | loss: 1.3436697MemoryTrain:  epoch  7, batch     1 | loss: 1.2965584MemoryTrain:  epoch  7, batch     2 | loss: 1.2511474MemoryTrain:  epoch  7, batch     3 | loss: 1.2478390MemoryTrain:  epoch  7, batch     4 | loss: 1.3671535MemoryTrain:  epoch  7, batch     5 | loss: 1.2351600MemoryTrain:  epoch  7, batch     6 | loss: 1.2515342MemoryTrain:  epoch  7, batch     7 | loss: 1.2905310MemoryTrain:  epoch  8, batch     0 | loss: 1.2697666MemoryTrain:  epoch  8, batch     1 | loss: 1.3032005MemoryTrain:  epoch  8, batch     2 | loss: 1.2231591MemoryTrain:  epoch  8, batch     3 | loss: 1.2251747MemoryTrain:  epoch  8, batch     4 | loss: 1.2404913MemoryTrain:  epoch  8, batch     5 | loss: 1.2442554MemoryTrain:  epoch  8, batch     6 | loss: 1.2808459MemoryTrain:  epoch  8, batch     7 | loss: 1.2244980MemoryTrain:  epoch  9, batch     0 | loss: 1.3111079MemoryTrain:  epoch  9, batch     1 | loss: 1.2905183MemoryTrain:  epoch  9, batch     2 | loss: 1.2348673MemoryTrain:  epoch  9, batch     3 | loss: 1.2227763MemoryTrain:  epoch  9, batch     4 | loss: 1.2400630MemoryTrain:  epoch  9, batch     5 | loss: 1.3444057MemoryTrain:  epoch  9, batch     6 | loss: 1.2205591MemoryTrain:  epoch  9, batch     7 | loss: 1.2297280
[EVAL] batch:    0 | acc: 37.50%,  total acc: 37.50%   [EVAL] batch:    1 | acc: 56.25%,  total acc: 46.88%   [EVAL] batch:    2 | acc: 68.75%,  total acc: 54.17%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 57.81%   [EVAL] batch:    4 | acc: 56.25%,  total acc: 57.50%   [EVAL] batch:    5 | acc: 37.50%,  total acc: 54.17%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 57.14%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 60.16%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 61.11%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 64.38%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 65.91%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 68.75%   [EVAL] batch:   12 | acc: 100.00%,  total acc: 71.15%   [EVAL] batch:   13 | acc: 100.00%,  total acc: 73.21%   [EVAL] batch:   14 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 76.56%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 77.94%   [EVAL] batch:   17 | acc: 25.00%,  total acc: 75.00%   
[EVAL] batch:    0 | acc: 50.00%,  total acc: 50.00%   [EVAL] batch:    1 | acc: 56.25%,  total acc: 53.12%   [EVAL] batch:    2 | acc: 56.25%,  total acc: 54.17%   [EVAL] batch:    3 | acc: 31.25%,  total acc: 48.44%   [EVAL] batch:    4 | acc: 43.75%,  total acc: 47.50%   [EVAL] batch:    5 | acc: 50.00%,  total acc: 47.92%   [EVAL] batch:    6 | acc: 62.50%,  total acc: 50.00%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 55.47%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 59.03%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 61.25%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 64.20%   [EVAL] batch:   11 | acc: 62.50%,  total acc: 64.06%   [EVAL] batch:   12 | acc: 31.25%,  total acc: 61.54%   [EVAL] batch:   13 | acc: 31.25%,  total acc: 59.38%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 60.00%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 59.77%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 60.66%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 60.76%   [EVAL] batch:   18 | acc: 56.25%,  total acc: 60.53%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 61.25%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 63.10%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 64.77%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 66.30%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 67.45%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 68.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 69.95%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 70.83%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 71.88%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 72.63%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 72.92%   [EVAL] batch:   30 | acc: 81.25%,  total acc: 73.19%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 73.83%   [EVAL] batch:   32 | acc: 81.25%,  total acc: 74.05%   [EVAL] batch:   33 | acc: 25.00%,  total acc: 72.61%   [EVAL] batch:   34 | acc: 12.50%,  total acc: 70.89%   [EVAL] batch:   35 | acc: 0.00%,  total acc: 68.92%   [EVAL] batch:   36 | acc: 0.00%,  total acc: 67.06%   [EVAL] batch:   37 | acc: 6.25%,  total acc: 65.46%   [EVAL] batch:   38 | acc: 6.25%,  total acc: 63.94%   [EVAL] batch:   39 | acc: 81.25%,  total acc: 64.38%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 65.09%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 65.77%   [EVAL] batch:   42 | acc: 68.75%,  total acc: 65.84%   [EVAL] batch:   43 | acc: 50.00%,  total acc: 65.48%   [EVAL] batch:   44 | acc: 81.25%,  total acc: 65.83%   [EVAL] batch:   45 | acc: 37.50%,  total acc: 65.22%   [EVAL] batch:   46 | acc: 62.50%,  total acc: 65.16%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 65.76%   [EVAL] batch:   48 | acc: 68.75%,  total acc: 65.82%   [EVAL] batch:   49 | acc: 12.50%,  total acc: 64.75%   [EVAL] batch:   50 | acc: 37.50%,  total acc: 64.22%   [EVAL] batch:   51 | acc: 50.00%,  total acc: 63.94%   [EVAL] batch:   52 | acc: 18.75%,  total acc: 63.09%   [EVAL] batch:   53 | acc: 25.00%,  total acc: 62.38%   [EVAL] batch:   54 | acc: 50.00%,  total acc: 62.16%   [EVAL] batch:   55 | acc: 50.00%,  total acc: 61.94%   [EVAL] batch:   56 | acc: 50.00%,  total acc: 61.73%   [EVAL] batch:   57 | acc: 25.00%,  total acc: 61.10%   [EVAL] batch:   58 | acc: 37.50%,  total acc: 60.70%   [EVAL] batch:   59 | acc: 6.25%,  total acc: 59.79%   [EVAL] batch:   60 | acc: 6.25%,  total acc: 58.91%   [EVAL] batch:   61 | acc: 0.00%,  total acc: 57.96%   [EVAL] batch:   62 | acc: 0.00%,  total acc: 57.04%   [EVAL] batch:   63 | acc: 6.25%,  total acc: 56.25%   [EVAL] batch:   64 | acc: 31.25%,  total acc: 55.87%   [EVAL] batch:   65 | acc: 6.25%,  total acc: 55.11%   [EVAL] batch:   66 | acc: 12.50%,  total acc: 54.48%   [EVAL] batch:   67 | acc: 0.00%,  total acc: 53.68%   [EVAL] batch:   68 | acc: 6.25%,  total acc: 52.99%   [EVAL] batch:   69 | acc: 0.00%,  total acc: 52.23%   [EVAL] batch:   70 | acc: 0.00%,  total acc: 51.50%   [EVAL] batch:   71 | acc: 0.00%,  total acc: 50.78%   [EVAL] batch:   72 | acc: 37.50%,  total acc: 50.60%   [EVAL] batch:   73 | acc: 62.50%,  total acc: 50.76%   [EVAL] batch:   74 | acc: 50.00%,  total acc: 50.75%   [EVAL] batch:   75 | acc: 68.75%,  total acc: 50.99%   [EVAL] batch:   76 | acc: 75.00%,  total acc: 51.30%   [EVAL] batch:   77 | acc: 68.75%,  total acc: 51.52%   [EVAL] batch:   78 | acc: 81.25%,  total acc: 51.90%   [EVAL] batch:   79 | acc: 100.00%,  total acc: 52.50%   [EVAL] batch:   80 | acc: 100.00%,  total acc: 53.09%   [EVAL] batch:   81 | acc: 93.75%,  total acc: 53.58%   [EVAL] batch:   82 | acc: 75.00%,  total acc: 53.84%   [EVAL] batch:   83 | acc: 62.50%,  total acc: 53.94%   [EVAL] batch:   84 | acc: 0.00%,  total acc: 53.31%   [EVAL] batch:   85 | acc: 0.00%,  total acc: 52.69%   [EVAL] batch:   86 | acc: 6.25%,  total acc: 52.16%   [EVAL] batch:   87 | acc: 75.00%,  total acc: 52.41%   [EVAL] batch:   88 | acc: 93.75%,  total acc: 52.88%   [EVAL] batch:   89 | acc: 93.75%,  total acc: 53.33%   [EVAL] batch:   90 | acc: 100.00%,  total acc: 53.85%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 54.35%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 54.84%   [EVAL] batch:   93 | acc: 100.00%,  total acc: 55.32%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 55.79%   [EVAL] batch:   95 | acc: 81.25%,  total acc: 56.05%   [EVAL] batch:   96 | acc: 25.00%,  total acc: 55.73%   [EVAL] batch:   97 | acc: 50.00%,  total acc: 55.68%   [EVAL] batch:   98 | acc: 87.50%,  total acc: 56.00%   [EVAL] batch:   99 | acc: 87.50%,  total acc: 56.31%   [EVAL] batch:  100 | acc: 87.50%,  total acc: 56.62%   [EVAL] batch:  101 | acc: 68.75%,  total acc: 56.74%   [EVAL] batch:  102 | acc: 68.75%,  total acc: 56.86%   [EVAL] batch:  103 | acc: 81.25%,  total acc: 57.09%   [EVAL] batch:  104 | acc: 81.25%,  total acc: 57.32%   [EVAL] batch:  105 | acc: 93.75%,  total acc: 57.67%   [EVAL] batch:  106 | acc: 87.50%,  total acc: 57.94%   [EVAL] batch:  107 | acc: 93.75%,  total acc: 58.28%   [EVAL] batch:  108 | acc: 93.75%,  total acc: 58.60%   [EVAL] batch:  109 | acc: 93.75%,  total acc: 58.92%   [EVAL] batch:  110 | acc: 56.25%,  total acc: 58.90%   [EVAL] batch:  111 | acc: 31.25%,  total acc: 58.65%   [EVAL] batch:  112 | acc: 18.75%,  total acc: 58.30%   [EVAL] batch:  113 | acc: 25.00%,  total acc: 58.00%   [EVAL] batch:  114 | acc: 6.25%,  total acc: 57.55%   [EVAL] batch:  115 | acc: 18.75%,  total acc: 57.22%   [EVAL] batch:  116 | acc: 50.00%,  total acc: 57.16%   [EVAL] batch:  117 | acc: 68.75%,  total acc: 57.26%   [EVAL] batch:  118 | acc: 68.75%,  total acc: 57.35%   [EVAL] batch:  119 | acc: 56.25%,  total acc: 57.34%   [EVAL] batch:  120 | acc: 43.75%,  total acc: 57.23%   [EVAL] batch:  121 | acc: 56.25%,  total acc: 57.22%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 57.57%   [EVAL] batch:  123 | acc: 62.50%,  total acc: 57.61%   [EVAL] batch:  124 | acc: 87.50%,  total acc: 57.85%   [EVAL] batch:  125 | acc: 81.25%,  total acc: 58.04%   [EVAL] batch:  126 | acc: 93.75%,  total acc: 58.32%   [EVAL] batch:  127 | acc: 100.00%,  total acc: 58.64%   [EVAL] batch:  128 | acc: 100.00%,  total acc: 58.96%   [EVAL] batch:  129 | acc: 100.00%,  total acc: 59.28%   [EVAL] batch:  130 | acc: 100.00%,  total acc: 59.59%   [EVAL] batch:  131 | acc: 100.00%,  total acc: 59.90%   [EVAL] batch:  132 | acc: 62.50%,  total acc: 59.92%   
cur_acc:  ['0.8561', '0.8705', '0.3516', '0.4904', '0.5170', '0.8661', '0.7750', '0.7500']
his_acc:  ['0.8561', '0.8418', '0.7454', '0.6619', '0.6030', '0.6306', '0.6061', '0.5992']
--------Round  1
seed:  200
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/test.pkl
Task_order: [7 6 3 2 4 0 5 1]
prepared data!
CurrentTrain: epoch  0, batch     0 | loss: 13.2809315CurrentTrain: epoch  0, batch     1 | loss: 13.1214075CurrentTrain: epoch  0, batch     2 | loss: 12.8496923CurrentTrain: epoch  0, batch     3 | loss: 12.7663498CurrentTrain: epoch  0, batch     4 | loss: 12.5570984CurrentTrain: epoch  0, batch     5 | loss: 12.4427223CurrentTrain: epoch  0, batch     6 | loss: 12.5715466CurrentTrain: epoch  0, batch     7 | loss: 12.2961445CurrentTrain: epoch  0, batch     8 | loss: 12.4132538CurrentTrain: epoch  0, batch     9 | loss: 12.2305584CurrentTrain: epoch  0, batch    10 | loss: 11.9558926CurrentTrain: epoch  0, batch    11 | loss: 11.9876213CurrentTrain: epoch  0, batch    12 | loss: 11.7945843CurrentTrain: epoch  0, batch    13 | loss: 11.4682884CurrentTrain: epoch  0, batch    14 | loss: 11.8847675CurrentTrain: epoch  0, batch    15 | loss: 11.5031738CurrentTrain: epoch  0, batch    16 | loss: 11.1517467CurrentTrain: epoch  0, batch    17 | loss: 11.5189629CurrentTrain: epoch  0, batch    18 | loss: 11.2183342CurrentTrain: epoch  0, batch    19 | loss: 11.1537914CurrentTrain: epoch  0, batch    20 | loss: 11.1267967CurrentTrain: epoch  0, batch    21 | loss: 11.0163441CurrentTrain: epoch  0, batch    22 | loss: 11.4989185CurrentTrain: epoch  0, batch    23 | loss: 10.5479479CurrentTrain: epoch  0, batch    24 | loss: 11.1922169CurrentTrain: epoch  0, batch    25 | loss: 10.5940399CurrentTrain: epoch  0, batch    26 | loss: 11.2257242CurrentTrain: epoch  0, batch    27 | loss: 10.8971462CurrentTrain: epoch  0, batch    28 | loss: 11.0274277CurrentTrain: epoch  0, batch    29 | loss: 10.5344629CurrentTrain: epoch  0, batch    30 | loss: 11.0562773CurrentTrain: epoch  0, batch    31 | loss: 10.5909214CurrentTrain: epoch  0, batch    32 | loss: 10.7607088CurrentTrain: epoch  0, batch    33 | loss: 10.7660828CurrentTrain: epoch  0, batch    34 | loss: 10.2726192CurrentTrain: epoch  0, batch    35 | loss: 10.1587925CurrentTrain: epoch  0, batch    36 | loss: 10.3890963CurrentTrain: epoch  0, batch    37 | loss: 10.6987286CurrentTrain: epoch  1, batch     0 | loss: 10.1199074CurrentTrain: epoch  1, batch     1 | loss: 10.3035297CurrentTrain: epoch  1, batch     2 | loss: 9.6134281CurrentTrain: epoch  1, batch     3 | loss: 10.1149664CurrentTrain: epoch  1, batch     4 | loss: 10.3713131CurrentTrain: epoch  1, batch     5 | loss: 9.4444599CurrentTrain: epoch  1, batch     6 | loss: 9.9694872CurrentTrain: epoch  1, batch     7 | loss: 9.7458801CurrentTrain: epoch  1, batch     8 | loss: 10.5697727CurrentTrain: epoch  1, batch     9 | loss: 9.7264767CurrentTrain: epoch  1, batch    10 | loss: 10.3981829CurrentTrain: epoch  1, batch    11 | loss: 9.8596439CurrentTrain: epoch  1, batch    12 | loss: 9.3728867CurrentTrain: epoch  1, batch    13 | loss: 9.1717024CurrentTrain: epoch  1, batch    14 | loss: 9.6470861CurrentTrain: epoch  1, batch    15 | loss: 8.6766567CurrentTrain: epoch  1, batch    16 | loss: 9.2948074CurrentTrain: epoch  1, batch    17 | loss: 8.7506924CurrentTrain: epoch  1, batch    18 | loss: 9.4265995CurrentTrain: epoch  1, batch    19 | loss: 8.8231144CurrentTrain: epoch  1, batch    20 | loss: 9.2249050CurrentTrain: epoch  1, batch    21 | loss: 8.7779427CurrentTrain: epoch  1, batch    22 | loss: 8.9589300CurrentTrain: epoch  1, batch    23 | loss: 9.2978611CurrentTrain: epoch  1, batch    24 | loss: 9.0398083CurrentTrain: epoch  1, batch    25 | loss: 8.8911114CurrentTrain: epoch  1, batch    26 | loss: 8.7209587CurrentTrain: epoch  1, batch    27 | loss: 9.0768328CurrentTrain: epoch  1, batch    28 | loss: 9.2721996CurrentTrain: epoch  1, batch    29 | loss: 9.5682526CurrentTrain: epoch  1, batch    30 | loss: 9.1698570CurrentTrain: epoch  1, batch    31 | loss: 9.0002747CurrentTrain: epoch  1, batch    32 | loss: 8.3019543CurrentTrain: epoch  1, batch    33 | loss: 8.5799503CurrentTrain: epoch  1, batch    34 | loss: 8.8605547CurrentTrain: epoch  1, batch    35 | loss: 8.1703377CurrentTrain: epoch  1, batch    36 | loss: 8.4791069CurrentTrain: epoch  1, batch    37 | loss: 9.3119926CurrentTrain: epoch  2, batch     0 | loss: 7.9665256CurrentTrain: epoch  2, batch     1 | loss: 8.2254248CurrentTrain: epoch  2, batch     2 | loss: 9.0151978CurrentTrain: epoch  2, batch     3 | loss: 7.6801801CurrentTrain: epoch  2, batch     4 | loss: 7.3988600CurrentTrain: epoch  2, batch     5 | loss: 8.6989574CurrentTrain: epoch  2, batch     6 | loss: 8.2907543CurrentTrain: epoch  2, batch     7 | loss: 8.8724937CurrentTrain: epoch  2, batch     8 | loss: 8.3914433CurrentTrain: epoch  2, batch     9 | loss: 8.4917183CurrentTrain: epoch  2, batch    10 | loss: 8.2648573CurrentTrain: epoch  2, batch    11 | loss: 7.9139857CurrentTrain: epoch  2, batch    12 | loss: 7.5005298CurrentTrain: epoch  2, batch    13 | loss: 7.7308683CurrentTrain: epoch  2, batch    14 | loss: 8.4312325CurrentTrain: epoch  2, batch    15 | loss: 8.1171989CurrentTrain: epoch  2, batch    16 | loss: 8.1600590CurrentTrain: epoch  2, batch    17 | loss: 8.2773275CurrentTrain: epoch  2, batch    18 | loss: 8.0877209CurrentTrain: epoch  2, batch    19 | loss: 8.0164337CurrentTrain: epoch  2, batch    20 | loss: 8.3091030CurrentTrain: epoch  2, batch    21 | loss: 8.0089512CurrentTrain: epoch  2, batch    22 | loss: 8.8440685CurrentTrain: epoch  2, batch    23 | loss: 7.6416950CurrentTrain: epoch  2, batch    24 | loss: 7.4916725CurrentTrain: epoch  2, batch    25 | loss: 7.5791030CurrentTrain: epoch  2, batch    26 | loss: 7.8865566CurrentTrain: epoch  2, batch    27 | loss: 8.1808920CurrentTrain: epoch  2, batch    28 | loss: 7.7580547CurrentTrain: epoch  2, batch    29 | loss: 7.5197978CurrentTrain: epoch  2, batch    30 | loss: 6.8789878CurrentTrain: epoch  2, batch    31 | loss: 7.3083849CurrentTrain: epoch  2, batch    32 | loss: 7.1385627CurrentTrain: epoch  2, batch    33 | loss: 7.1053457CurrentTrain: epoch  2, batch    34 | loss: 8.1456089CurrentTrain: epoch  2, batch    35 | loss: 7.0992985CurrentTrain: epoch  2, batch    36 | loss: 7.7874870CurrentTrain: epoch  2, batch    37 | loss: 7.8780098CurrentTrain: epoch  3, batch     0 | loss: 7.8890734CurrentTrain: epoch  3, batch     1 | loss: 7.1340876CurrentTrain: epoch  3, batch     2 | loss: 8.0359964CurrentTrain: epoch  3, batch     3 | loss: 7.4223928CurrentTrain: epoch  3, batch     4 | loss: 7.0616193CurrentTrain: epoch  3, batch     5 | loss: 6.4983511CurrentTrain: epoch  3, batch     6 | loss: 7.3484025CurrentTrain: epoch  3, batch     7 | loss: 6.3493242CurrentTrain: epoch  3, batch     8 | loss: 7.5898180CurrentTrain: epoch  3, batch     9 | loss: 6.7159748CurrentTrain: epoch  3, batch    10 | loss: 7.2070904CurrentTrain: epoch  3, batch    11 | loss: 6.2519684CurrentTrain: epoch  3, batch    12 | loss: 6.9062214CurrentTrain: epoch  3, batch    13 | loss: 6.5792222CurrentTrain: epoch  3, batch    14 | loss: 7.5328789CurrentTrain: epoch  3, batch    15 | loss: 7.1837249CurrentTrain: epoch  3, batch    16 | loss: 7.1718149CurrentTrain: epoch  3, batch    17 | loss: 6.3555651CurrentTrain: epoch  3, batch    18 | loss: 6.6227126CurrentTrain: epoch  3, batch    19 | loss: 6.7638865CurrentTrain: epoch  3, batch    20 | loss: 6.1709547CurrentTrain: epoch  3, batch    21 | loss: 7.4376478CurrentTrain: epoch  3, batch    22 | loss: 6.7111735CurrentTrain: epoch  3, batch    23 | loss: 7.0414457CurrentTrain: epoch  3, batch    24 | loss: 8.2936058CurrentTrain: epoch  3, batch    25 | loss: 7.3291454CurrentTrain: epoch  3, batch    26 | loss: 7.4382362CurrentTrain: epoch  3, batch    27 | loss: 6.1358786CurrentTrain: epoch  3, batch    28 | loss: 9.0174351CurrentTrain: epoch  3, batch    29 | loss: 7.8631172CurrentTrain: epoch  3, batch    30 | loss: 7.3880453CurrentTrain: epoch  3, batch    31 | loss: 7.1021380CurrentTrain: epoch  3, batch    32 | loss: 7.4765253CurrentTrain: epoch  3, batch    33 | loss: 8.0140686CurrentTrain: epoch  3, batch    34 | loss: 8.1952724CurrentTrain: epoch  3, batch    35 | loss: 6.8407235CurrentTrain: epoch  3, batch    36 | loss: 7.0881176CurrentTrain: epoch  3, batch    37 | loss: 6.6040335CurrentTrain: epoch  4, batch     0 | loss: 6.8938951CurrentTrain: epoch  4, batch     1 | loss: 7.2100687CurrentTrain: epoch  4, batch     2 | loss: 7.1772590CurrentTrain: epoch  4, batch     3 | loss: 6.9717631CurrentTrain: epoch  4, batch     4 | loss: 7.2264104CurrentTrain: epoch  4, batch     5 | loss: 7.4383612CurrentTrain: epoch  4, batch     6 | loss: 6.6556206CurrentTrain: epoch  4, batch     7 | loss: 6.9724073CurrentTrain: epoch  4, batch     8 | loss: 7.1081061CurrentTrain: epoch  4, batch     9 | loss: 7.0916786CurrentTrain: epoch  4, batch    10 | loss: 5.5745354CurrentTrain: epoch  4, batch    11 | loss: 7.1240664CurrentTrain: epoch  4, batch    12 | loss: 6.8566232CurrentTrain: epoch  4, batch    13 | loss: 6.5075545CurrentTrain: epoch  4, batch    14 | loss: 6.7425203CurrentTrain: epoch  4, batch    15 | loss: 7.1361599CurrentTrain: epoch  4, batch    16 | loss: 7.4258289CurrentTrain: epoch  4, batch    17 | loss: 6.5817213CurrentTrain: epoch  4, batch    18 | loss: 7.4308796CurrentTrain: epoch  4, batch    19 | loss: 5.9400864CurrentTrain: epoch  4, batch    20 | loss: 6.3027401CurrentTrain: epoch  4, batch    21 | loss: 6.1354375CurrentTrain: epoch  4, batch    22 | loss: 7.4051318CurrentTrain: epoch  4, batch    23 | loss: 6.0671849CurrentTrain: epoch  4, batch    24 | loss: 6.6452551CurrentTrain: epoch  4, batch    25 | loss: 6.5786791CurrentTrain: epoch  4, batch    26 | loss: 7.5593872CurrentTrain: epoch  4, batch    27 | loss: 6.0964212CurrentTrain: epoch  4, batch    28 | loss: 6.6470017CurrentTrain: epoch  4, batch    29 | loss: 6.1927638CurrentTrain: epoch  4, batch    30 | loss: 6.4837546CurrentTrain: epoch  4, batch    31 | loss: 7.2102327CurrentTrain: epoch  4, batch    32 | loss: 7.4276180CurrentTrain: epoch  4, batch    33 | loss: 7.6260247CurrentTrain: epoch  4, batch    34 | loss: 6.1897020CurrentTrain: epoch  4, batch    35 | loss: 5.8794193CurrentTrain: epoch  4, batch    36 | loss: 6.5228052CurrentTrain: epoch  4, batch    37 | loss: 7.6505260CurrentTrain: epoch  5, batch     0 | loss: 5.9371276CurrentTrain: epoch  5, batch     1 | loss: 7.1604195CurrentTrain: epoch  5, batch     2 | loss: 6.3146901CurrentTrain: epoch  5, batch     3 | loss: 6.9489841CurrentTrain: epoch  5, batch     4 | loss: 6.3252878CurrentTrain: epoch  5, batch     5 | loss: 6.2402883CurrentTrain: epoch  5, batch     6 | loss: 6.6337662CurrentTrain: epoch  5, batch     7 | loss: 5.9056826CurrentTrain: epoch  5, batch     8 | loss: 6.4537411CurrentTrain: epoch  5, batch     9 | loss: 6.7100000CurrentTrain: epoch  5, batch    10 | loss: 6.2129335CurrentTrain: epoch  5, batch    11 | loss: 6.5019050CurrentTrain: epoch  5, batch    12 | loss: 5.6338291CurrentTrain: epoch  5, batch    13 | loss: 6.1120214CurrentTrain: epoch  5, batch    14 | loss: 5.9285665CurrentTrain: epoch  5, batch    15 | loss: 6.4659667CurrentTrain: epoch  5, batch    16 | loss: 6.6292696CurrentTrain: epoch  5, batch    17 | loss: 6.9156861CurrentTrain: epoch  5, batch    18 | loss: 6.2993350CurrentTrain: epoch  5, batch    19 | loss: 6.0680637CurrentTrain: epoch  5, batch    20 | loss: 5.6478114CurrentTrain: epoch  5, batch    21 | loss: 6.3712726CurrentTrain: epoch  5, batch    22 | loss: 6.6454535CurrentTrain: epoch  5, batch    23 | loss: 5.8640242CurrentTrain: epoch  5, batch    24 | loss: 5.6630421CurrentTrain: epoch  5, batch    25 | loss: 6.7533989CurrentTrain: epoch  5, batch    26 | loss: 6.3119297CurrentTrain: epoch  5, batch    27 | loss: 6.0589485CurrentTrain: epoch  5, batch    28 | loss: 6.3136625CurrentTrain: epoch  5, batch    29 | loss: 6.0989976CurrentTrain: epoch  5, batch    30 | loss: 7.1784639CurrentTrain: epoch  5, batch    31 | loss: 6.6840959CurrentTrain: epoch  5, batch    32 | loss: 6.4747329CurrentTrain: epoch  5, batch    33 | loss: 5.7770867CurrentTrain: epoch  5, batch    34 | loss: 6.2792048CurrentTrain: epoch  5, batch    35 | loss: 6.0286479CurrentTrain: epoch  5, batch    36 | loss: 6.5911803CurrentTrain: epoch  5, batch    37 | loss: 7.3310471CurrentTrain: epoch  6, batch     0 | loss: 6.7885990CurrentTrain: epoch  6, batch     1 | loss: 6.3120794CurrentTrain: epoch  6, batch     2 | loss: 6.1583767CurrentTrain: epoch  6, batch     3 | loss: 5.6281433CurrentTrain: epoch  6, batch     4 | loss: 6.4531565CurrentTrain: epoch  6, batch     5 | loss: 6.2096348CurrentTrain: epoch  6, batch     6 | loss: 6.4618616CurrentTrain: epoch  6, batch     7 | loss: 5.7476883CurrentTrain: epoch  6, batch     8 | loss: 5.5396829CurrentTrain: epoch  6, batch     9 | loss: 6.0639944CurrentTrain: epoch  6, batch    10 | loss: 5.6675344CurrentTrain: epoch  6, batch    11 | loss: 5.6901875CurrentTrain: epoch  6, batch    12 | loss: 6.1881700CurrentTrain: epoch  6, batch    13 | loss: 5.9105177CurrentTrain: epoch  6, batch    14 | loss: 5.8003559CurrentTrain: epoch  6, batch    15 | loss: 5.8278656CurrentTrain: epoch  6, batch    16 | loss: 6.0019836CurrentTrain: epoch  6, batch    17 | loss: 5.5871229CurrentTrain: epoch  6, batch    18 | loss: 6.0625820CurrentTrain: epoch  6, batch    19 | loss: 5.4275036CurrentTrain: epoch  6, batch    20 | loss: 6.0757499CurrentTrain: epoch  6, batch    21 | loss: 5.5173111CurrentTrain: epoch  6, batch    22 | loss: 5.8641496CurrentTrain: epoch  6, batch    23 | loss: 5.7091885CurrentTrain: epoch  6, batch    24 | loss: 5.4655476CurrentTrain: epoch  6, batch    25 | loss: 5.5262480CurrentTrain: epoch  6, batch    26 | loss: 6.0026531CurrentTrain: epoch  6, batch    27 | loss: 5.4915638CurrentTrain: epoch  6, batch    28 | loss: 5.7692833CurrentTrain: epoch  6, batch    29 | loss: 7.1272850CurrentTrain: epoch  6, batch    30 | loss: 6.3377161CurrentTrain: epoch  6, batch    31 | loss: 6.2305903CurrentTrain: epoch  6, batch    32 | loss: 6.2498469CurrentTrain: epoch  6, batch    33 | loss: 5.8094811CurrentTrain: epoch  6, batch    34 | loss: 5.5804634CurrentTrain: epoch  6, batch    35 | loss: 5.7851839CurrentTrain: epoch  6, batch    36 | loss: 5.8394189CurrentTrain: epoch  6, batch    37 | loss: 5.5344191CurrentTrain: epoch  7, batch     0 | loss: 6.0710263CurrentTrain: epoch  7, batch     1 | loss: 6.4622412CurrentTrain: epoch  7, batch     2 | loss: 5.5195308CurrentTrain: epoch  7, batch     3 | loss: 6.2834306CurrentTrain: epoch  7, batch     4 | loss: 5.3632064CurrentTrain: epoch  7, batch     5 | loss: 5.4173927CurrentTrain: epoch  7, batch     6 | loss: 5.2486115CurrentTrain: epoch  7, batch     7 | loss: 6.0879712CurrentTrain: epoch  7, batch     8 | loss: 5.3825574CurrentTrain: epoch  7, batch     9 | loss: 5.6305380CurrentTrain: epoch  7, batch    10 | loss: 5.6194701CurrentTrain: epoch  7, batch    11 | loss: 5.4446135CurrentTrain: epoch  7, batch    12 | loss: 5.9127688CurrentTrain: epoch  7, batch    13 | loss: 5.1591763CurrentTrain: epoch  7, batch    14 | loss: 5.8271780CurrentTrain: epoch  7, batch    15 | loss: 5.1015396CurrentTrain: epoch  7, batch    16 | loss: 5.1531086CurrentTrain: epoch  7, batch    17 | loss: 5.5449924CurrentTrain: epoch  7, batch    18 | loss: 5.6600761CurrentTrain: epoch  7, batch    19 | loss: 5.4275246CurrentTrain: epoch  7, batch    20 | loss: 6.4057336CurrentTrain: epoch  7, batch    21 | loss: 5.7528906CurrentTrain: epoch  7, batch    22 | loss: 5.4784698CurrentTrain: epoch  7, batch    23 | loss: 5.1789584CurrentTrain: epoch  7, batch    24 | loss: 5.6440887CurrentTrain: epoch  7, batch    25 | loss: 5.4382477CurrentTrain: epoch  7, batch    26 | loss: 5.2775221CurrentTrain: epoch  7, batch    27 | loss: 5.1764097CurrentTrain: epoch  7, batch    28 | loss: 5.0769448CurrentTrain: epoch  7, batch    29 | loss: 5.5349379CurrentTrain: epoch  7, batch    30 | loss: 5.0674524CurrentTrain: epoch  7, batch    31 | loss: 5.3647995CurrentTrain: epoch  7, batch    32 | loss: 5.3288422CurrentTrain: epoch  7, batch    33 | loss: 5.3201308CurrentTrain: epoch  7, batch    34 | loss: 5.7403440CurrentTrain: epoch  7, batch    35 | loss: 5.2679348CurrentTrain: epoch  7, batch    36 | loss: 5.2704630CurrentTrain: epoch  7, batch    37 | loss: 5.1458921CurrentTrain: epoch  8, batch     0 | loss: 5.6638231CurrentTrain: epoch  8, batch     1 | loss: 5.6503792CurrentTrain: epoch  8, batch     2 | loss: 5.5955882CurrentTrain: epoch  8, batch     3 | loss: 5.3259373CurrentTrain: epoch  8, batch     4 | loss: 5.1499004CurrentTrain: epoch  8, batch     5 | loss: 5.2026901CurrentTrain: epoch  8, batch     6 | loss: 5.4288311CurrentTrain: epoch  8, batch     7 | loss: 5.1781397CurrentTrain: epoch  8, batch     8 | loss: 5.2587938CurrentTrain: epoch  8, batch     9 | loss: 5.0999265CurrentTrain: epoch  8, batch    10 | loss: 5.0691013CurrentTrain: epoch  8, batch    11 | loss: 5.6598182CurrentTrain: epoch  8, batch    12 | loss: 5.1028271CurrentTrain: epoch  8, batch    13 | loss: 5.5529294CurrentTrain: epoch  8, batch    14 | loss: 5.1432791CurrentTrain: epoch  8, batch    15 | loss: 5.3130774CurrentTrain: epoch  8, batch    16 | loss: 5.4792495CurrentTrain: epoch  8, batch    17 | loss: 5.3724942CurrentTrain: epoch  8, batch    18 | loss: 5.2051888CurrentTrain: epoch  8, batch    19 | loss: 5.6088700CurrentTrain: epoch  8, batch    20 | loss: 5.0924063CurrentTrain: epoch  8, batch    21 | loss: 5.2641163CurrentTrain: epoch  8, batch    22 | loss: 5.2418242CurrentTrain: epoch  8, batch    23 | loss: 5.1618690CurrentTrain: epoch  8, batch    24 | loss: 4.9023309CurrentTrain: epoch  8, batch    25 | loss: 5.0008612CurrentTrain: epoch  8, batch    26 | loss: 5.2149019CurrentTrain: epoch  8, batch    27 | loss: 5.4650335CurrentTrain: epoch  8, batch    28 | loss: 5.3540058CurrentTrain: epoch  8, batch    29 | loss: 5.3702817CurrentTrain: epoch  8, batch    30 | loss: 5.8757782CurrentTrain: epoch  8, batch    31 | loss: 5.3572149CurrentTrain: epoch  8, batch    32 | loss: 5.4938955CurrentTrain: epoch  8, batch    33 | loss: 5.1537304CurrentTrain: epoch  8, batch    34 | loss: 5.3322177CurrentTrain: epoch  8, batch    35 | loss: 5.2173862CurrentTrain: epoch  8, batch    36 | loss: 5.2361703CurrentTrain: epoch  8, batch    37 | loss: 5.3513513CurrentTrain: epoch  9, batch     0 | loss: 5.0526423CurrentTrain: epoch  9, batch     1 | loss: 5.8459177CurrentTrain: epoch  9, batch     2 | loss: 5.1264219CurrentTrain: epoch  9, batch     3 | loss: 5.2663717CurrentTrain: epoch  9, batch     4 | loss: 5.3459835CurrentTrain: epoch  9, batch     5 | loss: 5.2697253CurrentTrain: epoch  9, batch     6 | loss: 5.1328592CurrentTrain: epoch  9, batch     7 | loss: 5.1082134CurrentTrain: epoch  9, batch     8 | loss: 5.3240767CurrentTrain: epoch  9, batch     9 | loss: 5.5508289CurrentTrain: epoch  9, batch    10 | loss: 5.1231351CurrentTrain: epoch  9, batch    11 | loss: 5.0528445CurrentTrain: epoch  9, batch    12 | loss: 5.2200480CurrentTrain: epoch  9, batch    13 | loss: 4.9467411CurrentTrain: epoch  9, batch    14 | loss: 5.0373926CurrentTrain: epoch  9, batch    15 | loss: 5.1987915CurrentTrain: epoch  9, batch    16 | loss: 5.0516462CurrentTrain: epoch  9, batch    17 | loss: 5.0790119CurrentTrain: epoch  9, batch    18 | loss: 4.9267292CurrentTrain: epoch  9, batch    19 | loss: 5.0152912CurrentTrain: epoch  9, batch    20 | loss: 5.0983572CurrentTrain: epoch  9, batch    21 | loss: 4.9889631CurrentTrain: epoch  9, batch    22 | loss: 5.3146071CurrentTrain: epoch  9, batch    23 | loss: 5.1010280CurrentTrain: epoch  9, batch    24 | loss: 4.9930010CurrentTrain: epoch  9, batch    25 | loss: 4.9439483CurrentTrain: epoch  9, batch    26 | loss: 5.1325645CurrentTrain: epoch  9, batch    27 | loss: 5.2413626CurrentTrain: epoch  9, batch    28 | loss: 4.9834957CurrentTrain: epoch  9, batch    29 | loss: 4.9184055CurrentTrain: epoch  9, batch    30 | loss: 4.9202347CurrentTrain: epoch  9, batch    31 | loss: 4.9452767CurrentTrain: epoch  9, batch    32 | loss: 5.6225872CurrentTrain: epoch  9, batch    33 | loss: 5.6159096CurrentTrain: epoch  9, batch    34 | loss: 5.3278828CurrentTrain: epoch  9, batch    35 | loss: 4.8755593CurrentTrain: epoch  9, batch    36 | loss: 4.9672098CurrentTrain: epoch  9, batch    37 | loss: 4.8466539
#### person countries of residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person countries of residence" means a person is living in a country.
Here is an example:
Relation: person countries of residence
Context: iran 's top nuclear negotiator , ali larijani , was asked by state tv whether the country had started converting into gas a second batch of uranium , as it had planned to do at its isfahan nuclear facility .
Head Entity: ali larijani
Tail Entity: iran
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person countries of residence":
Relation: person countries of residence  
Context: After years of living in the United States, the renowned artist decided to return to her homeland, where she felt a deep connection to her roots.  
Head Entity: the renowned artist  
Tail Entity: her homeland  

Relation: person countries of residence  
Context: Following his successful career in Europe, the famous chef opened a new restaurant in his native Italy, where he could share his culinary heritage.  
Head Entity: the famous chef  
Tail Entity: Italy  
#### organization top members employees ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization top members employees" means an organization 's employer is a person.
Here is an example:
Relation: organization top members employees
Context: `` it 's suggesting that these are customers who have some kind of roots , some kind of stability , people who are demonstrating an ability to keep making payments , '' said wayne abernathy , executive vice president for financial institutions policy at the american bankers association .
Head Entity: american bankers association
Tail Entity: wayne abernathy
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization top members employees":
Relation: organization top members employees  
Context: `` during the annual meeting, the board of directors recognized the contributions of its key personnel, including the chief technology officer, who has been instrumental in driving innovation at the company, '' said the spokesperson for tech innovations inc.  
Head Entity: tech innovations inc  
Tail Entity: chief technology officer  

Relation: organization top members employees  
Context: `` the charity gala honored several individuals for their outstanding service, including the director of community outreach, who has led numerous initiatives to support local families, '' announced the president of helping hands foundation.  
Head Entity: helping hands foundation  
Tail Entity: director of community outreach  
#### organization member of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization member of" means an organization is the member of an organization.
Here is an example:
Relation: organization member of
Context: the yachts of seabourn is a brand of carnival corporation and plc -lrb- nyse/lse : ccl and nyse : cuk -rrb- , and supports the ocean conservation & tourism alliance , dedicated to education and promotion of best practices for protecting the marine environment .
Head Entity: carnival corporation
Tail Entity: ocean conservation & tourism alliance
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization member of":
Relation: organization member of  
Context: The World Wildlife Fund is a member of the International Union for Conservation of Nature, collaborating on various environmental initiatives and conservation projects worldwide.  
Head Entity: World Wildlife Fund  
Tail Entity: International Union for Conservation of Nature  

Relation: organization member of  
Context: The American Medical Association has joined forces with the National Institutes of Health as a member organization to advance public health research and policy.  
Head Entity: American Medical Association  
Tail Entity: National Institutes of Health  
#### person origin ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person origin" means a person 's nationality is a country.
Here is an example:
Relation: person origin
Context: the former detainee , murat kurnaz , was turned over to german authorities and freed in 2006 after a personal plea from german chancellor angela merkel .
Head Entity: murat kurnaz
Tail Entity: german
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person origin":
Relation: person origin  
Context: the renowned physicist, albert einstein, was born in the kingdom of wurttemberg in the german empire in 1879.  
Head Entity: albert einstein  
Tail Entity: german  

Relation: person origin  
Context: the famous author, gabriel garcía márquez, hailed from the coastal city of aracataca in colombia, which greatly influenced his literary works.  
Head Entity: gabriel garcía márquez  
Tail Entity: colombian  
#### person title ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person title" means a person 's title is a title.
Here is an example:
Relation: person title
Context: `` it has to have some tempo , it has to have some energy , '' said sean ross , vice president of music and programming at edison media research , which surveys the radio industry .
Head Entity: sean ross
Tail Entity: vice president
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person title":
Relation: person title  
Context: "After years of hard work and dedication, Maria Gonzalez was promoted to Chief Financial Officer at Tech Innovations, where she has been instrumental in driving the company's financial strategy."  
Head Entity: Maria Gonzalez  
Tail Entity: Chief Financial Officer  

Relation: person title  
Context: "During the annual conference, Dr. James Lee, the lead researcher at the National Institute of Health, presented his findings on the latest advancements in medical technology."  
Head Entity: Dr. James Lee  
Tail Entity: lead researcher  
#### organization country of headquarters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization country of headquarters" means an organization is located in a country.
Here is an example:
Relation: organization country of headquarters
Context: troubled us bond insurer mbia said it had gained a one billion - dollar capital injection from warburg pincus , a private equity firm , to help boost its finances following losses on mortgage - related securities .
Head Entity: mbia
Tail Entity: us
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization country of headquarters":
Relation: organization country of headquarters  
Context: the multinational technology company apple inc. has its headquarters in cupertino, california, where it designs and manufactures consumer electronics and software.  
Head Entity: apple inc.  
Tail Entity: california  

Relation: organization country of headquarters  
Context: the renowned automotive manufacturer toyota motor corporation is headquartered in toyota city, japan, and is known for its innovative approach to vehicle production.  
Head Entity: toyota motor corporation  
Tail Entity: japan  
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 79.69%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 80.00%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 79.17%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 83.59%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 84.72%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 85.62%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 86.93%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 87.98%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 86.61%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 85.83%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 83.98%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 83.46%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 82.64%   [EVAL] batch:   18 | acc: 87.50%,  total acc: 82.89%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 82.81%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 83.63%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 84.38%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 85.05%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 85.68%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 86.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 86.78%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 87.04%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 87.93%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 87.71%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 87.70%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 87.89%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 86.36%   
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 79.69%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 80.00%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 79.17%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 83.59%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 84.72%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 85.62%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 86.93%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 87.98%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 86.61%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 85.83%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 83.98%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 83.46%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 82.64%   [EVAL] batch:   18 | acc: 87.50%,  total acc: 82.89%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 82.81%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 83.63%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 84.38%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 85.05%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 85.68%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 86.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 86.78%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 87.04%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 87.93%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 87.71%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 87.70%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 87.89%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 86.36%   
cur_acc:  ['0.8636']
his_acc:  ['0.8636']
CurrentTrain: epoch  0, batch     0 | loss: 6.1598911CurrentTrain: epoch  0, batch     1 | loss: 7.1821537CurrentTrain: epoch  1, batch     0 | loss: 6.0656524CurrentTrain: epoch  1, batch     1 | loss: 4.8902349CurrentTrain: epoch  2, batch     0 | loss: 4.9443893CurrentTrain: epoch  2, batch     1 | loss: 4.8581853CurrentTrain: epoch  3, batch     0 | loss: 5.1511984CurrentTrain: epoch  3, batch     1 | loss: 4.0893345CurrentTrain: epoch  4, batch     0 | loss: 4.0988803CurrentTrain: epoch  4, batch     1 | loss: 4.7643070CurrentTrain: epoch  5, batch     0 | loss: 4.1165967CurrentTrain: epoch  5, batch     1 | loss: 4.1954551CurrentTrain: epoch  6, batch     0 | loss: 3.4829745CurrentTrain: epoch  6, batch     1 | loss: 4.4808078CurrentTrain: epoch  7, batch     0 | loss: 3.5230808CurrentTrain: epoch  7, batch     1 | loss: 3.5877101CurrentTrain: epoch  8, batch     0 | loss: 3.5995088CurrentTrain: epoch  8, batch     1 | loss: 2.9648464CurrentTrain: epoch  9, batch     0 | loss: 3.2961311CurrentTrain: epoch  9, batch     1 | loss: 3.4190536
#### person stateorprovinces of residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person stateorprovinces of residence" means a person is living in a state or province.
Here is an example:
Relation: person stateorprovinces of residence
Context: ny-schools-chief -lrb- new york -rrb- -- cathleen p. black , mayor michael r. bloomberg 's choice to be the next chancellor of new york city 's public-school system , has during more than 40 years in the media business broken numerous glass ceilings -- and amassed a fortune -- with quick and cold-blooded decision making , crystal-clear goal setting , and an all-surpassing attention to the bottom line .
Head Entity: cathleen p. black
Tail Entity: new york
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person stateorprovinces of residence":
Relation: person stateorprovinces of residence  
Context: after years of living in the bustling city, actor john doe has decided to settle down in the serene landscapes of oregon, where he can enjoy a quieter lifestyle away from the spotlight.  
Head Entity: john doe  
Tail Entity: oregon  

Relation: person stateorprovinces of residence  
Context: renowned author jane smith has made her home in the vibrant city of san francisco, drawing inspiration from its rich culture and diverse community for her next novel.  
Head Entity: jane smith  
Tail Entity: san francisco  
#### person date of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person date of death" means a person was died on a date.
Here is an example:
Relation: person date of death
Context: he passed away on saturday .
Head Entity: he
Tail Entity: saturday
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person date of death":
Relation: person date of death  
Context: John Smith died on March 15, 2020.  
Head Entity: John Smith  
Tail Entity: March 15, 2020  

Relation: person date of death  
Context: The famous author passed away in 1995.  
Head Entity: The famous author  
Tail Entity: 1995  
#### organization number of employees members ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization number of employees members" means the number of an organization 's employees is a number.
Here is an example:
Relation: organization number of employees members
Context: with the sweep of a federal regulator 's pen , massachusetts stands to gain a new life-science giant in april : covidien , a medical - supplies maker with thousands of products and more than 43,000 employees worldwide .
Head Entity: covidien
Tail Entity: 43,000
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization number of employees members":
Relation: organization number of employees members  
Context: The tech company, Innovatech, has rapidly expanded its workforce over the past year, now boasting a total of 25,000 employees across its global offices.  
Head Entity: Innovatech  
Tail Entity: 25,000  

Relation: organization number of employees members  
Context: After the merger, the newly formed entity, Global Solutions Inc., reported an impressive count of 50,000 employees, making it one of the largest firms in the industry.  
Head Entity: Global Solutions Inc.  
Tail Entity: 50,000  
#### person alternate names ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person alternate names" means a person 's alias is a person.
Here is an example:
Relation: person alternate names
Context: `` i am known in the hospice as the man who would n't die , '' buchwald wrote in march .
Head Entity: buchwald
Tail Entity: man who would n't die
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person alternate names":
Relation: person alternate names  
Context: `` the world knows her as the queen of pop, but her friends call her madge. ''  
Head Entity: madonna  
Tail Entity: queen of pop  

Relation: person alternate names  
Context: `` during his career, he was often referred to as the king of rock and roll. ''  
Head Entity: elvis presley  
Tail Entity: king of rock and roll  
#### person spouse ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person spouse" means a person 's spouse is a person.
Here is an example:
Relation: person spouse
Context: beverly hills , california 2008-08-17 21:15:39 utc ------ there was much dancing : ellen degeneres and portia de rossi are married , according to reports .
Head Entity: ellen degeneres
Tail Entity: portia de rossi
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person spouse":
Relation: person spouse  
Context: in a lavish ceremony held in new york city, the couple exchanged vows in front of family and friends: john legend and chrissy teigen celebrated their love with a beautiful wedding.  
Head Entity: john legend  
Tail Entity: chrissy teigen  

Relation: person spouse  
Context: during the annual charity gala, it was announced that the famous actor and his long-time partner have tied the knot: ben affleck and jennifer garner are now officially husband and wife.  
Head Entity: ben affleck  
Tail Entity: jennifer garner  
Mixup data size:  104
MixupTrain:  epoch  0, batch     0 | loss: 11.2272472MixupTrain:  epoch  0, batch     1 | loss: 9.6749131MixupTrain:  epoch  0, batch     2 | loss: 9.7488765MixupTrain:  epoch  0, batch     3 | loss: 8.3138890MixupTrain:  epoch  0, batch     4 | loss: 8.8506987MixupTrain:  epoch  0, batch     5 | loss: 10.4630015MixupTrain:  epoch  0, batch     6 | loss: 6.3042098
MemoryTrain:  epoch  0, batch     0 | loss: 4.6892900MemoryTrain:  epoch  0, batch     1 | loss: 4.3848534MemoryTrain:  epoch  0, batch     2 | loss: 4.8746824MemoryTrain:  epoch  1, batch     0 | loss: 4.5188079MemoryTrain:  epoch  1, batch     1 | loss: 3.7181613MemoryTrain:  epoch  1, batch     2 | loss: 1.3638301MemoryTrain:  epoch  2, batch     0 | loss: 2.9146004MemoryTrain:  epoch  2, batch     1 | loss: 3.3439665MemoryTrain:  epoch  2, batch     2 | loss: 2.3217437MemoryTrain:  epoch  3, batch     0 | loss: 2.9092352MemoryTrain:  epoch  3, batch     1 | loss: 2.9235673MemoryTrain:  epoch  3, batch     2 | loss: 1.5115310MemoryTrain:  epoch  4, batch     0 | loss: 2.5179749MemoryTrain:  epoch  4, batch     1 | loss: 2.8127878MemoryTrain:  epoch  4, batch     2 | loss: 2.2187598MemoryTrain:  epoch  5, batch     0 | loss: 2.4807529MemoryTrain:  epoch  5, batch     1 | loss: 2.0928221MemoryTrain:  epoch  5, batch     2 | loss: 1.9218717MemoryTrain:  epoch  6, batch     0 | loss: 2.1656168MemoryTrain:  epoch  6, batch     1 | loss: 2.0366631MemoryTrain:  epoch  6, batch     2 | loss: 1.7092452MemoryTrain:  epoch  7, batch     0 | loss: 2.1636701MemoryTrain:  epoch  7, batch     1 | loss: 1.7156727MemoryTrain:  epoch  7, batch     2 | loss: 2.1472309MemoryTrain:  epoch  8, batch     0 | loss: 2.1789200MemoryTrain:  epoch  8, batch     1 | loss: 1.6667378MemoryTrain:  epoch  8, batch     2 | loss: 1.1761624MemoryTrain:  epoch  9, batch     0 | loss: 1.6110919MemoryTrain:  epoch  9, batch     1 | loss: 1.8290315MemoryTrain:  epoch  9, batch     2 | loss: 1.8626353
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 77.08%   [EVAL] batch:    3 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 78.75%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 83.93%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 85.16%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 86.11%   [EVAL] batch:    9 | acc: 56.25%,  total acc: 83.12%   [EVAL] batch:   10 | acc: 68.75%,  total acc: 81.82%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 83.33%   [EVAL] batch:   12 | acc: 100.00%,  total acc: 84.62%   [EVAL] batch:   13 | acc: 100.00%,  total acc: 85.71%   [EVAL] batch:   14 | acc: 43.75%,  total acc: 82.92%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 68.75%,  total acc: 75.00%   [EVAL] batch:    2 | acc: 68.75%,  total acc: 72.92%   [EVAL] batch:    3 | acc: 62.50%,  total acc: 70.31%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 72.50%   [EVAL] batch:    5 | acc: 68.75%,  total acc: 71.88%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 73.21%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 76.56%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 79.17%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 80.62%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 82.39%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 83.33%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 83.17%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 82.14%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 81.67%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 80.08%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 79.78%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 78.82%   [EVAL] batch:   18 | acc: 75.00%,  total acc: 78.62%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 79.06%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 80.06%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 80.97%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 81.52%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 82.29%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 83.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 83.65%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 84.03%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 84.60%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 85.13%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 85.21%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 85.69%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 86.13%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 86.36%   [EVAL] batch:   33 | acc: 62.50%,  total acc: 85.66%   [EVAL] batch:   34 | acc: 81.25%,  total acc: 85.54%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 85.59%   [EVAL] batch:   36 | acc: 75.00%,  total acc: 85.30%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 85.53%   [EVAL] batch:   38 | acc: 93.75%,  total acc: 85.74%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 86.09%   [EVAL] batch:   40 | acc: 87.50%,  total acc: 86.13%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 86.31%   [EVAL] batch:   42 | acc: 31.25%,  total acc: 85.03%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 85.37%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 85.69%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 86.01%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 85.90%   
cur_acc:  ['0.8636', '0.8292']
his_acc:  ['0.8636', '0.8590']
CurrentTrain: epoch  0, batch     0 | loss: 6.6043282CurrentTrain: epoch  0, batch     1 | loss: 6.7156978CurrentTrain: epoch  1, batch     0 | loss: 6.5016093CurrentTrain: epoch  1, batch     1 | loss: 4.3862848CurrentTrain: epoch  2, batch     0 | loss: 4.9365687CurrentTrain: epoch  2, batch     1 | loss: 4.8099737CurrentTrain: epoch  3, batch     0 | loss: 4.7048569CurrentTrain: epoch  3, batch     1 | loss: 4.4935069CurrentTrain: epoch  4, batch     0 | loss: 4.4467192CurrentTrain: epoch  4, batch     1 | loss: 3.3774946CurrentTrain: epoch  5, batch     0 | loss: 3.7741911CurrentTrain: epoch  5, batch     1 | loss: 3.3558743CurrentTrain: epoch  6, batch     0 | loss: 3.3811123CurrentTrain: epoch  6, batch     1 | loss: 3.0905895CurrentTrain: epoch  7, batch     0 | loss: 3.1064386CurrentTrain: epoch  7, batch     1 | loss: 3.3105402CurrentTrain: epoch  8, batch     0 | loss: 3.3414924CurrentTrain: epoch  8, batch     1 | loss: 2.5478668CurrentTrain: epoch  9, batch     0 | loss: 2.6030426CurrentTrain: epoch  9, batch     1 | loss: 2.9509633
#### person date of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person date of birth" means a person was born in a date.
Here is an example:
Relation: person date of birth
Context: born of schoolteacher parents in the western town of sabaneta on july 28 , 1954 , chavez studied at the military academy of venezuela in caracas .
Head Entity: chavez
Tail Entity: july 28 , 1954
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person date of birth":
Relation: person date of birth  
Context: the famous physicist albert einstein was born in ulm, in the kingdom of wurttemberg in the german empire on march 14, 1879.  
Head Entity: albert einstein  
Tail Entity: march 14, 1879  

Relation: person date of birth  
Context: the renowned author j.k. rowling was born in yate, gloucestershire, england on july 31, 1965.  
Head Entity: j.k. rowling  
Tail Entity: july 31, 1965  
#### person stateorprovince of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person stateorprovince of birth" means a person was died in a state or province.
Here is an example:
Relation: person stateorprovince of birth
Context: jane matilda bolin was born on april 11 , 1908 , in poughkeepsie , ny .
Head Entity: jane matilda bolin
Tail Entity: ny
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person stateorprovince of birth":
Relation: person stateorprovince of birth  
Context: albert einstein was born on march 14, 1879, in ulm, germany.  
Head Entity: albert einstein  
Tail Entity: germany  

Relation: person stateorprovince of birth  
Context: marilyn monroe was born on june 1, 1926, in los angeles, california.  
Head Entity: marilyn monroe  
Tail Entity: california  
#### person parents ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person parents" means a person 's parent is a person.
Here is an example:
Relation: person parents
Context: lynne spears told the court that lutfi had treated her daughter like a hostage in her own home , drugged her and took over her finances .
Head Entity: her
Tail Entity: lynne spears
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person parents":
Relation: person parents  
Context: During the family reunion, Sarah introduced her father, John, to her friends, highlighting how much he has influenced her life choices.  
Head Entity: her father  
Tail Entity: Sarah  

Relation: person parents  
Context: After the ceremony, Emily shared stories about her mother, who had always been her biggest supporter and role model throughout her life.  
Head Entity: her mother  
Tail Entity: Emily  
#### person employee of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person employee of" means a person is an employee of an organization.
Here is an example:
Relation: person employee of
Context: seeking revenge , axel reunites with old pal sgt. billy rosewood -lrb- judge reinhold -rrb- and jon flint -lrb- hector elizondo -rrb- of the beverly hills police department .
Head Entity: hector elizondo
Tail Entity: beverly hills police department
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person employee of":
Relation: person employee of  
Context: After years of hard work and dedication, Maria finally received a promotion at the tech company where she has been employed since college.  
Head Entity: Maria  
Tail Entity: tech company  

Relation: person employee of  
Context: John has been with the marketing firm for over a decade, contributing to numerous successful campaigns and building strong client relationships.  
Head Entity: John  
Tail Entity: marketing firm  
#### person stateorprovince of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person stateorprovince of death" means a person was died in a state or province.
Here is an example:
Relation: person stateorprovince of death
Context: irene morgan kirkaldy , whose defiance of bus segregation laws more than a decade before rosa parks ' landmark case helped lay the foundation for later civil rights victories , died friday at her home in hayes , va. .
Head Entity: irene morgan kirkaldy
Tail Entity: va.
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person stateorprovince of death":
Relation: person stateorprovince of death  
Context: john doe, a renowned author known for his impactful novels, passed away peacefully in his sleep at his residence in california last night.  
Head Entity: john doe  
Tail Entity: california  

Relation: person stateorprovince of death  
Context: after a long battle with illness, elizabeth taylor, the iconic actress, succumbed to her health issues and died in a hospital in nevada.  
Head Entity: elizabeth taylor  
Tail Entity: nevada  
Mixup data size:  133
MixupTrain:  epoch  0, batch     0 | loss: 7.5151043MixupTrain:  epoch  0, batch     1 | loss: 6.4840926MixupTrain:  epoch  0, batch     2 | loss: 6.2763741MixupTrain:  epoch  0, batch     3 | loss: 5.5568652MixupTrain:  epoch  0, batch     4 | loss: 5.1579434MixupTrain:  epoch  0, batch     5 | loss: 6.2327595MixupTrain:  epoch  0, batch     6 | loss: 5.9346654MixupTrain:  epoch  0, batch     7 | loss: 4.7644075MixupTrain:  epoch  0, batch     8 | loss: 3.7592142
MemoryTrain:  epoch  0, batch     0 | loss: 2.4307261MemoryTrain:  epoch  0, batch     1 | loss: 2.8851175MemoryTrain:  epoch  0, batch     2 | loss: 3.5121057MemoryTrain:  epoch  1, batch     0 | loss: 2.5259583MemoryTrain:  epoch  1, batch     1 | loss: 2.9374518MemoryTrain:  epoch  1, batch     2 | loss: 2.5834150MemoryTrain:  epoch  2, batch     0 | loss: 2.0438852MemoryTrain:  epoch  2, batch     1 | loss: 2.3823586MemoryTrain:  epoch  2, batch     2 | loss: 2.0349538MemoryTrain:  epoch  3, batch     0 | loss: 1.9917321MemoryTrain:  epoch  3, batch     1 | loss: 1.8691692MemoryTrain:  epoch  3, batch     2 | loss: 1.7069070MemoryTrain:  epoch  4, batch     0 | loss: 1.9369916MemoryTrain:  epoch  4, batch     1 | loss: 1.8674128MemoryTrain:  epoch  4, batch     2 | loss: 1.6759942MemoryTrain:  epoch  5, batch     0 | loss: 1.8579614MemoryTrain:  epoch  5, batch     1 | loss: 1.7443321MemoryTrain:  epoch  5, batch     2 | loss: 1.5028377MemoryTrain:  epoch  6, batch     0 | loss: 1.6644633MemoryTrain:  epoch  6, batch     1 | loss: 1.4734735MemoryTrain:  epoch  6, batch     2 | loss: 1.8031354MemoryTrain:  epoch  7, batch     0 | loss: 1.6803691MemoryTrain:  epoch  7, batch     1 | loss: 1.4986699MemoryTrain:  epoch  7, batch     2 | loss: 1.4458002MemoryTrain:  epoch  8, batch     0 | loss: 1.6486750MemoryTrain:  epoch  8, batch     1 | loss: 1.4692017MemoryTrain:  epoch  8, batch     2 | loss: 1.4587920MemoryTrain:  epoch  9, batch     0 | loss: 1.5824859MemoryTrain:  epoch  9, batch     1 | loss: 1.4659363MemoryTrain:  epoch  9, batch     2 | loss: 1.6034157
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 62.50%,  total acc: 68.75%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 75.00%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    4 | acc: 62.50%,  total acc: 72.50%   [EVAL] batch:    5 | acc: 68.75%,  total acc: 71.88%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 71.43%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 74.22%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 77.08%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 78.75%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 78.98%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 80.21%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 80.77%   [EVAL] batch:   13 | acc: 37.50%,  total acc: 77.68%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 56.25%,  total acc: 65.62%   [EVAL] batch:    2 | acc: 56.25%,  total acc: 62.50%   [EVAL] batch:    3 | acc: 56.25%,  total acc: 60.94%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 63.75%   [EVAL] batch:    5 | acc: 50.00%,  total acc: 61.46%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 64.29%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 68.75%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 72.22%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 74.38%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 76.70%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 78.12%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 78.85%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 78.12%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 77.50%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 76.17%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 76.10%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 75.35%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 74.67%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 75.31%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 76.49%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 77.27%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 77.99%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 78.91%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 79.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 80.53%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 81.02%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 81.70%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 82.33%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 82.50%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 83.06%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 83.59%   [EVAL] batch:   32 | acc: 81.25%,  total acc: 83.52%   [EVAL] batch:   33 | acc: 56.25%,  total acc: 82.72%   [EVAL] batch:   34 | acc: 50.00%,  total acc: 81.79%   [EVAL] batch:   35 | acc: 62.50%,  total acc: 81.25%   [EVAL] batch:   36 | acc: 68.75%,  total acc: 80.91%   [EVAL] batch:   37 | acc: 81.25%,  total acc: 80.92%   [EVAL] batch:   38 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 81.72%   [EVAL] batch:   40 | acc: 81.25%,  total acc: 81.71%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 81.99%   [EVAL] batch:   42 | acc: 25.00%,  total acc: 80.67%   [EVAL] batch:   43 | acc: 68.75%,  total acc: 80.40%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 80.56%   [EVAL] batch:   45 | acc: 68.75%,  total acc: 80.30%   [EVAL] batch:   46 | acc: 75.00%,  total acc: 80.19%   [EVAL] batch:   47 | acc: 68.75%,  total acc: 79.95%   [EVAL] batch:   48 | acc: 75.00%,  total acc: 79.85%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 80.00%   [EVAL] batch:   50 | acc: 56.25%,  total acc: 79.53%   [EVAL] batch:   51 | acc: 81.25%,  total acc: 79.57%   [EVAL] batch:   52 | acc: 56.25%,  total acc: 79.13%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 79.17%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 79.43%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 79.80%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 80.04%   [EVAL] batch:   57 | acc: 75.00%,  total acc: 79.96%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 80.30%   [EVAL] batch:   59 | acc: 81.25%,  total acc: 80.31%   [EVAL] batch:   60 | acc: 25.00%,  total acc: 79.41%   
cur_acc:  ['0.8636', '0.8292', '0.7768']
his_acc:  ['0.8636', '0.8590', '0.7941']
CurrentTrain: epoch  0, batch     0 | loss: 4.9887648CurrentTrain: epoch  0, batch     1 | loss: 5.9370947CurrentTrain: epoch  1, batch     0 | loss: 4.6044517CurrentTrain: epoch  1, batch     1 | loss: 4.4101343CurrentTrain: epoch  2, batch     0 | loss: 4.5011010CurrentTrain: epoch  2, batch     1 | loss: 3.4223132CurrentTrain: epoch  3, batch     0 | loss: 4.0743365CurrentTrain: epoch  3, batch     1 | loss: 3.7530556CurrentTrain: epoch  4, batch     0 | loss: 3.6570344CurrentTrain: epoch  4, batch     1 | loss: 3.1253865CurrentTrain: epoch  5, batch     0 | loss: 3.0577221CurrentTrain: epoch  5, batch     1 | loss: 3.4594204CurrentTrain: epoch  6, batch     0 | loss: 3.3328760CurrentTrain: epoch  6, batch     1 | loss: 2.4763374CurrentTrain: epoch  7, batch     0 | loss: 2.7588494CurrentTrain: epoch  7, batch     1 | loss: 2.9800072CurrentTrain: epoch  8, batch     0 | loss: 2.7772088CurrentTrain: epoch  8, batch     1 | loss: 2.8362432CurrentTrain: epoch  9, batch     0 | loss: 2.7292318CurrentTrain: epoch  9, batch     1 | loss: 2.4118257
#### person cities of residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person cities of residence" means a person is living in a city.
Here is an example:
Relation: person cities of residence
Context: -lrb- now there are two : andre carson , a democrat from indianapolis , was elected to the house last year . -rrb-
Head Entity: andre carson
Tail Entity: indianapolis
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person cities of residence":
Relation: person cities of residence  
Context: -lrb- after moving to the west coast, sarah jones found her new home in san francisco, where she works as a software engineer. -rrb-  
Head Entity: sarah jones  
Tail Entity: san francisco  

Relation: person cities of residence  
Context: -lrb- during his time in college, michael smith lived in boston, which he fondly remembers as a vibrant city full of life. -rrb-  
Head Entity: michael smith  
Tail Entity: boston  
#### person schools attended ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person schools attended" means a person 's school is an organization.
Here is an example:
Relation: person schools attended
Context: after world war ii , he attended the university of southern california , where he became editor of a college magazine .
Head Entity: he
Tail Entity: university of southern california
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person schools attended":
Relation: person schools attended  
Context: She graduated from Harvard University with a degree in economics before pursuing her career in finance.  
Head Entity: She  
Tail Entity: Harvard University  

Relation: person schools attended  
Context: After completing his high school education, John enrolled at Stanford University to study computer science.  
Head Entity: John  
Tail Entity: Stanford University  
#### person country of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person country of death" means a person was died in a country.
Here is an example:
Relation: person country of death
Context: u.s. rep. parren mitchell , founding member of congressional black caucus , dies at 85
Head Entity: parren mitchell
Tail Entity: u.s.
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person country of death":
Relation: person country of death  
Context: renowned physicist stephen hawking passed away in cambridge, england at the age of 76  
Head Entity: stephen hawking  
Tail Entity: england  

Relation: person country of death  
Context: famous author gabriel garcia marquez died in mexico city, mexico, leaving behind a legacy of magical realism  
Head Entity: gabriel garcia marquez  
Tail Entity: mexico  
#### person children ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person children" means a person 's children is a person.
Here is an example:
Relation: person children
Context: he is survived by his wife of 63 years , josephine robinson mcnair , of columbia ; a son , robert e. jr. , of columbia ; three daughters , robin lee howell and corinne godshall , of myrtle beach , s.c. , and claudia crawford mcnair , of jamestown , s.c. ; six grandchildren ; and one great-grandchild .
Head Entity: he
Tail Entity: claudia crawford mcnair
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person children":
Relation: person children  
Context: After the passing of her husband, she dedicated her life to raising their three children, including her youngest, emily, who is now a successful artist in new york.  
Head Entity: she  
Tail Entity: emily  

Relation: person children  
Context: The famous actor often shares stories about his childhood and the lessons he learned from his father, who was a significant influence on his daughter, sarah.  
Head Entity: his father  
Tail Entity: sarah  
#### person charges ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person charges" means a person was charged with an event.
Here is an example:
Relation: person charges
Context: ferrara said he was innocent of limoli 's slaying , but he pleaded guilty in 1992 to murder , along with racketeering charges , under a deal that sent him to prison for 22 years , rather than go to trial and risk a conviction that could lead to life in prison .
Head Entity: ferrara
Tail Entity: racketeering
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person charges":
Relation: person charges  
Context: After a lengthy investigation, the authorities announced that Johnson was charged with embezzlement, a crime that could result in significant prison time if convicted.  
Head Entity: Johnson  
Tail Entity: embezzlement  

Relation: person charges  
Context: The district attorney confirmed that Smith has been charged with assault following the altercation that took place last month at the downtown bar.  
Head Entity: Smith  
Tail Entity: assault  
Mixup data size:  164
MixupTrain:  epoch  0, batch     0 | loss: 4.1660593MixupTrain:  epoch  0, batch     1 | loss: 2.7835640MixupTrain:  epoch  0, batch     2 | loss: 3.4876115MixupTrain:  epoch  0, batch     3 | loss: 4.3300513MixupTrain:  epoch  0, batch     4 | loss: 3.7195998MixupTrain:  epoch  0, batch     5 | loss: 3.7592921MixupTrain:  epoch  0, batch     6 | loss: 5.0869977MixupTrain:  epoch  0, batch     7 | loss: 3.7858462MixupTrain:  epoch  0, batch     8 | loss: 3.8614501MixupTrain:  epoch  0, batch     9 | loss: 3.5981011MixupTrain:  epoch  0, batch    10 | loss: 2.8431059
MemoryTrain:  epoch  0, batch     0 | loss: 2.4787211MemoryTrain:  epoch  0, batch     1 | loss: 3.4129696MemoryTrain:  epoch  0, batch     2 | loss: 3.1853454MemoryTrain:  epoch  0, batch     3 | loss: 2.2028656MemoryTrain:  epoch  1, batch     0 | loss: 2.6419644MemoryTrain:  epoch  1, batch     1 | loss: 2.6489639MemoryTrain:  epoch  1, batch     2 | loss: 2.3058133MemoryTrain:  epoch  1, batch     3 | loss: 2.9412892MemoryTrain:  epoch  2, batch     0 | loss: 1.7117755MemoryTrain:  epoch  2, batch     1 | loss: 2.5262470MemoryTrain:  epoch  2, batch     2 | loss: 2.7458258MemoryTrain:  epoch  2, batch     3 | loss: 2.3148479MemoryTrain:  epoch  3, batch     0 | loss: 2.2031870MemoryTrain:  epoch  3, batch     1 | loss: 2.7670207MemoryTrain:  epoch  3, batch     2 | loss: 1.5660125MemoryTrain:  epoch  3, batch     3 | loss: 2.2765205MemoryTrain:  epoch  4, batch     0 | loss: 2.6130271MemoryTrain:  epoch  4, batch     1 | loss: 1.9441047MemoryTrain:  epoch  4, batch     2 | loss: 1.8280017MemoryTrain:  epoch  4, batch     3 | loss: 1.4622194MemoryTrain:  epoch  5, batch     0 | loss: 1.5767016MemoryTrain:  epoch  5, batch     1 | loss: 1.7358186MemoryTrain:  epoch  5, batch     2 | loss: 1.7188827MemoryTrain:  epoch  5, batch     3 | loss: 1.7571328MemoryTrain:  epoch  6, batch     0 | loss: 1.5223236MemoryTrain:  epoch  6, batch     1 | loss: 1.5531890MemoryTrain:  epoch  6, batch     2 | loss: 1.6903379MemoryTrain:  epoch  6, batch     3 | loss: 1.7132633MemoryTrain:  epoch  7, batch     0 | loss: 1.6124055MemoryTrain:  epoch  7, batch     1 | loss: 1.5849335MemoryTrain:  epoch  7, batch     2 | loss: 1.6507885MemoryTrain:  epoch  7, batch     3 | loss: 1.7015885MemoryTrain:  epoch  8, batch     0 | loss: 1.6851058MemoryTrain:  epoch  8, batch     1 | loss: 1.5068324MemoryTrain:  epoch  8, batch     2 | loss: 1.4642375MemoryTrain:  epoch  8, batch     3 | loss: 1.6521914MemoryTrain:  epoch  9, batch     0 | loss: 1.5909108MemoryTrain:  epoch  9, batch     1 | loss: 1.4784727MemoryTrain:  epoch  9, batch     2 | loss: 1.5099256MemoryTrain:  epoch  9, batch     3 | loss: 1.3525872
[EVAL] batch:    0 | acc: 56.25%,  total acc: 56.25%   [EVAL] batch:    1 | acc: 56.25%,  total acc: 56.25%   [EVAL] batch:    2 | acc: 56.25%,  total acc: 56.25%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 60.94%   [EVAL] batch:    4 | acc: 31.25%,  total acc: 55.00%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 58.33%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 60.71%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 64.06%   [EVAL] batch:    8 | acc: 37.50%,  total acc: 61.11%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 61.88%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 63.64%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 66.67%   [EVAL] batch:   12 | acc: 100.00%,  total acc: 69.23%   [EVAL] batch:   13 | acc: 100.00%,  total acc: 71.43%   [EVAL] batch:   14 | acc: 100.00%,  total acc: 73.33%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 76.47%   [EVAL] batch:   17 | acc: 25.00%,  total acc: 73.61%   
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 68.75%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 62.50%,  total acc: 72.92%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 71.88%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 72.50%   [EVAL] batch:    5 | acc: 68.75%,  total acc: 71.88%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 74.11%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 77.34%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 79.86%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 82.95%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 83.85%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 84.13%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 83.04%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 82.50%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 80.86%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 80.51%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 79.51%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 78.62%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 79.06%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 80.06%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 80.68%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 81.77%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 82.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 83.17%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 83.56%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 84.15%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 84.70%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 84.79%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 85.28%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 85.74%   [EVAL] batch:   32 | acc: 75.00%,  total acc: 85.42%   [EVAL] batch:   33 | acc: 25.00%,  total acc: 83.64%   [EVAL] batch:   34 | acc: 25.00%,  total acc: 81.96%   [EVAL] batch:   35 | acc: 31.25%,  total acc: 80.56%   [EVAL] batch:   36 | acc: 31.25%,  total acc: 79.22%   [EVAL] batch:   37 | acc: 81.25%,  total acc: 79.28%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 79.81%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 80.31%   [EVAL] batch:   40 | acc: 87.50%,  total acc: 80.49%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 80.95%   [EVAL] batch:   42 | acc: 12.50%,  total acc: 79.36%   [EVAL] batch:   43 | acc: 56.25%,  total acc: 78.84%   [EVAL] batch:   44 | acc: 81.25%,  total acc: 78.89%   [EVAL] batch:   45 | acc: 43.75%,  total acc: 78.12%   [EVAL] batch:   46 | acc: 56.25%,  total acc: 77.66%   [EVAL] batch:   47 | acc: 68.75%,  total acc: 77.47%   [EVAL] batch:   48 | acc: 18.75%,  total acc: 76.28%   [EVAL] batch:   49 | acc: 43.75%,  total acc: 75.62%   [EVAL] batch:   50 | acc: 25.00%,  total acc: 74.63%   [EVAL] batch:   51 | acc: 37.50%,  total acc: 73.92%   [EVAL] batch:   52 | acc: 25.00%,  total acc: 73.00%   [EVAL] batch:   53 | acc: 56.25%,  total acc: 72.69%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 73.07%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 73.44%   [EVAL] batch:   56 | acc: 81.25%,  total acc: 73.57%   [EVAL] batch:   57 | acc: 75.00%,  total acc: 73.60%   [EVAL] batch:   58 | acc: 81.25%,  total acc: 73.73%   [EVAL] batch:   59 | acc: 81.25%,  total acc: 73.85%   [EVAL] batch:   60 | acc: 43.75%,  total acc: 73.36%   [EVAL] batch:   61 | acc: 75.00%,  total acc: 73.39%   [EVAL] batch:   62 | acc: 43.75%,  total acc: 72.92%   [EVAL] batch:   63 | acc: 75.00%,  total acc: 72.95%   [EVAL] batch:   64 | acc: 50.00%,  total acc: 72.60%   [EVAL] batch:   65 | acc: 62.50%,  total acc: 72.44%   [EVAL] batch:   66 | acc: 56.25%,  total acc: 72.20%   [EVAL] batch:   67 | acc: 93.75%,  total acc: 72.52%   [EVAL] batch:   68 | acc: 56.25%,  total acc: 72.28%   [EVAL] batch:   69 | acc: 56.25%,  total acc: 72.05%   [EVAL] batch:   70 | acc: 75.00%,  total acc: 72.10%   [EVAL] batch:   71 | acc: 87.50%,  total acc: 72.31%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 72.69%   [EVAL] batch:   73 | acc: 100.00%,  total acc: 73.06%   [EVAL] batch:   74 | acc: 100.00%,  total acc: 73.42%   [EVAL] batch:   75 | acc: 100.00%,  total acc: 73.77%   [EVAL] batch:   76 | acc: 100.00%,  total acc: 74.11%   [EVAL] batch:   77 | acc: 75.00%,  total acc: 74.12%   
cur_acc:  ['0.8636', '0.8292', '0.7768', '0.7361']
his_acc:  ['0.8636', '0.8590', '0.7941', '0.7412']
CurrentTrain: epoch  0, batch     0 | loss: 8.1146851CurrentTrain: epoch  0, batch     1 | loss: 8.3028870CurrentTrain: epoch  1, batch     0 | loss: 7.9286442CurrentTrain: epoch  1, batch     1 | loss: 6.8766284CurrentTrain: epoch  2, batch     0 | loss: 6.6355534CurrentTrain: epoch  2, batch     1 | loss: 7.5608420CurrentTrain: epoch  3, batch     0 | loss: 6.6230731CurrentTrain: epoch  3, batch     1 | loss: 6.1311984CurrentTrain: epoch  4, batch     0 | loss: 6.4015307CurrentTrain: epoch  4, batch     1 | loss: 5.6964021CurrentTrain: epoch  5, batch     0 | loss: 5.7960367CurrentTrain: epoch  5, batch     1 | loss: 5.9985662CurrentTrain: epoch  6, batch     0 | loss: 5.9020319CurrentTrain: epoch  6, batch     1 | loss: 5.1248651CurrentTrain: epoch  7, batch     0 | loss: 5.1560602CurrentTrain: epoch  7, batch     1 | loss: 5.5654483CurrentTrain: epoch  8, batch     0 | loss: 5.1602182CurrentTrain: epoch  8, batch     1 | loss: 4.9696164CurrentTrain: epoch  9, batch     0 | loss: 5.1339788CurrentTrain: epoch  9, batch     1 | loss: 4.4106259
#### organization subsidiaries ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization subsidiaries" means an organization 's subsidiary is an organization.
Here is an example:
Relation: organization subsidiaries
Context: venture fund buys sporting chain highland capital 's consumer fund includes lululemon athletica , a yoga retailer , and o beverages , a flavored water company developed by tom first , one of the two `` juice guys '' who cofounded nantucket nectars .
Head Entity: highland capital
Tail Entity: o beverages
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization subsidiaries":
Relation: organization subsidiaries  
Context: The tech giant Alphabet Inc. has several subsidiaries, including YouTube, which has become a leading platform for video content, and Waymo, which focuses on self-driving technology.  
Head Entity: Alphabet Inc.  
Tail Entity: YouTube  

Relation: organization subsidiaries  
Context: The automotive manufacturer General Motors has expanded its portfolio by acquiring several companies, including Cruise Automation, which specializes in autonomous vehicle technology, and Maven, a car-sharing service.  
Head Entity: General Motors  
Tail Entity: Cruise Automation  
#### organization parents ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization parents" means an organization 's parent is an organization.
Here is an example:
Relation: organization parents
Context: aseel kami , tim cocks , abdul-rahman taher , tim cocks and michael christie -lrb- reuters -rrb- add an intriguing note in the speculation over who 's responsible for the recent bombings : `` but a senior iraqi intelligence source , who declined to be named , said there was evidence the bombs could be the work of the militant badr organisation , the armed wing of the supreme islamic iraqi council -lrb- isci -rrb- .
Head Entity: badr organisation
Tail Entity: supreme islamic iraqi council
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization parents":
Relation: organization parents  
Context: In a recent merger, the tech giant Alphabet Inc. announced that it would acquire the innovative startup DeepMind Technologies, which has been a subsidiary of Alphabet since 2015. This acquisition is expected to enhance Alphabet's capabilities in artificial intelligence.  
Head Entity: DeepMind Technologies  
Tail Entity: Alphabet Inc.  

Relation: organization parents  
Context: The historical archives reveal that the renowned publishing house Penguin Random House was formed through the merger of Penguin Group and Random House, both of which have rich legacies in the literary world.  
Head Entity: Penguin Random House  
Tail Entity: Penguin Group  
#### organization alternate names ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization alternate names" means a person 's alias is a person.
Here is an example:
Relation: organization alternate names
Context: while section 106 of the hyde act openly bans indian testing , and the agreement upholds reinforces that test ban by upholding the applicability of domestic laws , washington has already recommended that the nuclear suppliers group -lrb- nsg -rrb- link its proposed exemption for india to a similar test ban .
Head Entity: nuclear suppliers group
Tail Entity: nsg
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization alternate names":
Relation: organization alternate names  
Context: The International Monetary Fund, often referred to as the IMF, plays a crucial role in global economic stability by providing financial assistance and advice to member countries.  
Head Entity: International Monetary Fund  
Tail Entity: IMF  

Relation: organization alternate names  
Context: The World Health Organization, commonly known as WHO, has been at the forefront of global health initiatives, especially during the COVID-19 pandemic.  
Head Entity: World Health Organization  
Tail Entity: WHO  
#### organization city of headquarters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization city of headquarters" means an organization is located in a city.
Here is an example:
Relation: organization city of headquarters
Context: ------ london 2008-05-20 07:23:45 utc enodis plc endorses sweetened takeover bid by us company manitowoc illinois tool works of glenville , illinois , which had offered 282 pence -lrb- us$ 551 euro3 54 -rrb- per share , said monday that it was considering its position .
Head Entity: illinois tool works
Tail Entity: glenville
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization city of headquarters":
Relation: organization city of headquarters  
Context: ------ in 2015, tech giant apple inc. announced plans to expand its operations in cupertino, california, where it has been headquartered since its founding.  
Head Entity: apple inc.  
Tail Entity: cupertino  

Relation: organization city of headquarters  
Context: ------ the multinational corporation samsung electronics is based in suwon, south korea, and has been a leader in the technology sector for decades.  
Head Entity: samsung electronics  
Tail Entity: suwon  
#### person siblings ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person siblings" means a person 's sibling is a person.
Here is an example:
Relation: person siblings
Context: forsberg , a political science professor at city college of new york , died oct. 19 in a bronx hospital of cancer , said her sister , celia seupel .
Head Entity: forsberg
Tail Entity: celia seupel
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person siblings":
Relation: person siblings  
Context: during the family reunion, john introduced his sister, emily, who had just returned from studying abroad.  
Head Entity: john  
Tail Entity: emily  

Relation: person siblings  
Context: at the award ceremony, sarah proudly accepted her trophy while her brother, michael, cheered from the audience.  
Head Entity: sarah  
Tail Entity: michael  
Mixup data size:  193
MixupTrain:  epoch  0, batch     0 | loss: 4.4793714MixupTrain:  epoch  0, batch     1 | loss: 4.9261086MixupTrain:  epoch  0, batch     2 | loss: 3.1886023MixupTrain:  epoch  0, batch     3 | loss: 3.8540830MixupTrain:  epoch  0, batch     4 | loss: 3.6491061MixupTrain:  epoch  0, batch     5 | loss: 2.9180456MixupTrain:  epoch  0, batch     6 | loss: 4.0293612MixupTrain:  epoch  0, batch     7 | loss: 3.3138198MixupTrain:  epoch  0, batch     8 | loss: 3.6359253MixupTrain:  epoch  0, batch     9 | loss: 3.7267338MixupTrain:  epoch  0, batch    10 | loss: 3.0516939MixupTrain:  epoch  0, batch    11 | loss: 3.5716765MixupTrain:  epoch  0, batch    12 | loss: 3.9816580
MemoryTrain:  epoch  0, batch     0 | loss: 1.9795555MemoryTrain:  epoch  0, batch     1 | loss: 3.1850243MemoryTrain:  epoch  0, batch     2 | loss: 3.2090473MemoryTrain:  epoch  0, batch     3 | loss: 2.6931336MemoryTrain:  epoch  0, batch     4 | loss: 2.6073587MemoryTrain:  epoch  1, batch     0 | loss: 3.4660568MemoryTrain:  epoch  1, batch     1 | loss: 1.8840573MemoryTrain:  epoch  1, batch     2 | loss: 2.3848848MemoryTrain:  epoch  1, batch     3 | loss: 2.3701987MemoryTrain:  epoch  1, batch     4 | loss: 3.6166720MemoryTrain:  epoch  2, batch     0 | loss: 2.3842940MemoryTrain:  epoch  2, batch     1 | loss: 1.9320977MemoryTrain:  epoch  2, batch     2 | loss: 2.5482028MemoryTrain:  epoch  2, batch     3 | loss: 2.5495534MemoryTrain:  epoch  2, batch     4 | loss: 2.0981095MemoryTrain:  epoch  3, batch     0 | loss: 1.6234756MemoryTrain:  epoch  3, batch     1 | loss: 3.0463243MemoryTrain:  epoch  3, batch     2 | loss: 1.8129269MemoryTrain:  epoch  3, batch     3 | loss: 2.6284716MemoryTrain:  epoch  3, batch     4 | loss: 1.8623235MemoryTrain:  epoch  4, batch     0 | loss: 1.4928898MemoryTrain:  epoch  4, batch     1 | loss: 2.1870503MemoryTrain:  epoch  4, batch     2 | loss: 1.7450223MemoryTrain:  epoch  4, batch     3 | loss: 2.2715392MemoryTrain:  epoch  4, batch     4 | loss: 2.3610721MemoryTrain:  epoch  5, batch     0 | loss: 2.1099992MemoryTrain:  epoch  5, batch     1 | loss: 1.6769047MemoryTrain:  epoch  5, batch     2 | loss: 1.5834861MemoryTrain:  epoch  5, batch     3 | loss: 1.6763511MemoryTrain:  epoch  5, batch     4 | loss: 2.1274927MemoryTrain:  epoch  6, batch     0 | loss: 1.8025091MemoryTrain:  epoch  6, batch     1 | loss: 1.7280655MemoryTrain:  epoch  6, batch     2 | loss: 1.7959487MemoryTrain:  epoch  6, batch     3 | loss: 1.9392456MemoryTrain:  epoch  6, batch     4 | loss: 1.5108972MemoryTrain:  epoch  7, batch     0 | loss: 1.9104187MemoryTrain:  epoch  7, batch     1 | loss: 1.6708443MemoryTrain:  epoch  7, batch     2 | loss: 1.5203671MemoryTrain:  epoch  7, batch     3 | loss: 1.4198847MemoryTrain:  epoch  7, batch     4 | loss: 1.7863021MemoryTrain:  epoch  8, batch     0 | loss: 1.5762279MemoryTrain:  epoch  8, batch     1 | loss: 1.7752991MemoryTrain:  epoch  8, batch     2 | loss: 1.4909170MemoryTrain:  epoch  8, batch     3 | loss: 1.6529198MemoryTrain:  epoch  8, batch     4 | loss: 1.5283453MemoryTrain:  epoch  9, batch     0 | loss: 1.3972646MemoryTrain:  epoch  9, batch     1 | loss: 1.3886478MemoryTrain:  epoch  9, batch     2 | loss: 1.6715155MemoryTrain:  epoch  9, batch     3 | loss: 1.5670431MemoryTrain:  epoch  9, batch     4 | loss: 1.6270769
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    2 | acc: 37.50%,  total acc: 58.33%   [EVAL] batch:    3 | acc: 12.50%,  total acc: 46.88%   [EVAL] batch:    4 | acc: 25.00%,  total acc: 42.50%   [EVAL] batch:    5 | acc: 25.00%,  total acc: 39.58%   [EVAL] batch:    6 | acc: 25.00%,  total acc: 37.50%   [EVAL] batch:    7 | acc: 50.00%,  total acc: 39.06%   [EVAL] batch:    8 | acc: 56.25%,  total acc: 40.97%   [EVAL] batch:    9 | acc: 31.25%,  total acc: 40.00%   [EVAL] batch:   10 | acc: 56.25%,  total acc: 41.48%   [EVAL] batch:   11 | acc: 50.00%,  total acc: 42.19%   [EVAL] batch:   12 | acc: 62.50%,  total acc: 43.75%   [EVAL] batch:   13 | acc: 87.50%,  total acc: 46.88%   [EVAL] batch:   14 | acc: 100.00%,  total acc: 50.42%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 53.52%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 55.88%   [EVAL] batch:   17 | acc: 87.50%,  total acc: 57.64%   [EVAL] batch:   18 | acc: 50.00%,  total acc: 57.24%   [EVAL] batch:   19 | acc: 62.50%,  total acc: 57.50%   [EVAL] batch:   20 | acc: 62.50%,  total acc: 57.74%   [EVAL] batch:   21 | acc: 37.50%,  total acc: 56.82%   
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    1 | acc: 75.00%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 79.69%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 78.75%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 78.12%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 80.36%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 82.81%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 84.03%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 85.00%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 86.36%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 86.98%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 85.58%   [EVAL] batch:   13 | acc: 37.50%,  total acc: 82.14%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 81.67%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 80.08%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 79.78%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 78.82%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 77.96%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 78.44%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 79.46%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 80.11%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 80.71%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 82.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 82.69%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 83.10%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 83.71%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 84.27%   [EVAL] batch:   29 | acc: 75.00%,  total acc: 83.96%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 84.27%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 84.77%   [EVAL] batch:   32 | acc: 75.00%,  total acc: 84.47%   [EVAL] batch:   33 | acc: 25.00%,  total acc: 82.72%   [EVAL] batch:   34 | acc: 31.25%,  total acc: 81.25%   [EVAL] batch:   35 | acc: 37.50%,  total acc: 80.03%   [EVAL] batch:   36 | acc: 43.75%,  total acc: 79.05%   [EVAL] batch:   37 | acc: 75.00%,  total acc: 78.95%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 79.49%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 80.00%   [EVAL] batch:   40 | acc: 81.25%,  total acc: 80.03%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 80.36%   [EVAL] batch:   42 | acc: 6.25%,  total acc: 78.63%   [EVAL] batch:   43 | acc: 31.25%,  total acc: 77.56%   [EVAL] batch:   44 | acc: 25.00%,  total acc: 76.39%   [EVAL] batch:   45 | acc: 18.75%,  total acc: 75.14%   [EVAL] batch:   46 | acc: 37.50%,  total acc: 74.34%   [EVAL] batch:   47 | acc: 62.50%,  total acc: 74.09%   [EVAL] batch:   48 | acc: 12.50%,  total acc: 72.83%   [EVAL] batch:   49 | acc: 25.00%,  total acc: 71.88%   [EVAL] batch:   50 | acc: 6.25%,  total acc: 70.59%   [EVAL] batch:   51 | acc: 0.00%,  total acc: 69.23%   [EVAL] batch:   52 | acc: 12.50%,  total acc: 68.16%   [EVAL] batch:   53 | acc: 43.75%,  total acc: 67.71%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 68.07%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 68.53%   [EVAL] batch:   56 | acc: 75.00%,  total acc: 68.64%   [EVAL] batch:   57 | acc: 75.00%,  total acc: 68.75%   [EVAL] batch:   58 | acc: 81.25%,  total acc: 68.96%   [EVAL] batch:   59 | acc: 75.00%,  total acc: 69.06%   [EVAL] batch:   60 | acc: 43.75%,  total acc: 68.65%   [EVAL] batch:   61 | acc: 62.50%,  total acc: 68.55%   [EVAL] batch:   62 | acc: 31.25%,  total acc: 67.96%   [EVAL] batch:   63 | acc: 75.00%,  total acc: 68.07%   [EVAL] batch:   64 | acc: 50.00%,  total acc: 67.79%   [EVAL] batch:   65 | acc: 56.25%,  total acc: 67.61%   [EVAL] batch:   66 | acc: 43.75%,  total acc: 67.26%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 67.74%   [EVAL] batch:   68 | acc: 56.25%,  total acc: 67.57%   [EVAL] batch:   69 | acc: 50.00%,  total acc: 67.32%   [EVAL] batch:   70 | acc: 68.75%,  total acc: 67.34%   [EVAL] batch:   71 | acc: 81.25%,  total acc: 67.53%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 67.98%   [EVAL] batch:   73 | acc: 100.00%,  total acc: 68.41%   [EVAL] batch:   74 | acc: 100.00%,  total acc: 68.83%   [EVAL] batch:   75 | acc: 100.00%,  total acc: 69.24%   [EVAL] batch:   76 | acc: 100.00%,  total acc: 69.64%   [EVAL] batch:   77 | acc: 87.50%,  total acc: 69.87%   [EVAL] batch:   78 | acc: 68.75%,  total acc: 69.86%   [EVAL] batch:   79 | acc: 68.75%,  total acc: 69.84%   [EVAL] batch:   80 | acc: 31.25%,  total acc: 69.37%   [EVAL] batch:   81 | acc: 18.75%,  total acc: 68.75%   [EVAL] batch:   82 | acc: 25.00%,  total acc: 68.22%   [EVAL] batch:   83 | acc: 18.75%,  total acc: 67.63%   [EVAL] batch:   84 | acc: 37.50%,  total acc: 67.28%   [EVAL] batch:   85 | acc: 43.75%,  total acc: 67.01%   [EVAL] batch:   86 | acc: 56.25%,  total acc: 66.88%   [EVAL] batch:   87 | acc: 31.25%,  total acc: 66.48%   [EVAL] batch:   88 | acc: 56.25%,  total acc: 66.36%   [EVAL] batch:   89 | acc: 56.25%,  total acc: 66.25%   [EVAL] batch:   90 | acc: 68.75%,  total acc: 66.28%   [EVAL] batch:   91 | acc: 87.50%,  total acc: 66.51%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 66.87%   [EVAL] batch:   93 | acc: 100.00%,  total acc: 67.22%   [EVAL] batch:   94 | acc: 87.50%,  total acc: 67.43%   [EVAL] batch:   95 | acc: 87.50%,  total acc: 67.64%   [EVAL] batch:   96 | acc: 56.25%,  total acc: 67.53%   [EVAL] batch:   97 | acc: 56.25%,  total acc: 67.41%   [EVAL] batch:   98 | acc: 62.50%,  total acc: 67.36%   [EVAL] batch:   99 | acc: 18.75%,  total acc: 66.88%   
cur_acc:  ['0.8636', '0.8292', '0.7768', '0.7361', '0.5682']
his_acc:  ['0.8636', '0.8590', '0.7941', '0.7412', '0.6687']
CurrentTrain: epoch  0, batch     0 | loss: 5.6543045CurrentTrain: epoch  0, batch     1 | loss: 6.5327821CurrentTrain: epoch  1, batch     0 | loss: 5.3902311CurrentTrain: epoch  1, batch     1 | loss: 3.9706447CurrentTrain: epoch  2, batch     0 | loss: 4.7024231CurrentTrain: epoch  2, batch     1 | loss: 3.2969105CurrentTrain: epoch  3, batch     0 | loss: 3.8649619CurrentTrain: epoch  3, batch     1 | loss: 4.4867268CurrentTrain: epoch  4, batch     0 | loss: 3.8412528CurrentTrain: epoch  4, batch     1 | loss: 3.8511384CurrentTrain: epoch  5, batch     0 | loss: 4.1659942CurrentTrain: epoch  5, batch     1 | loss: 3.1689172CurrentTrain: epoch  6, batch     0 | loss: 3.2834361CurrentTrain: epoch  6, batch     1 | loss: 3.8141854CurrentTrain: epoch  7, batch     0 | loss: 3.1670353CurrentTrain: epoch  7, batch     1 | loss: 3.0715468CurrentTrain: epoch  8, batch     0 | loss: 3.3892877CurrentTrain: epoch  8, batch     1 | loss: 2.5651000CurrentTrain: epoch  9, batch     0 | loss: 3.0419388CurrentTrain: epoch  9, batch     1 | loss: 2.7799485
#### person country of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person country of birth" means a person was born in a country.
Here is an example:
Relation: person country of birth
Context: iran 's atomic chief ali akbar salehi has expressed tehran 's readiness to swap 1,200 kilogrammes -lrb- 2,640 pounds -rrb- of low-enriched uranium -lrb- leu -rrb- in one-shot for enriched atomic fuel .
Head Entity: ali akbar salehi
Tail Entity: iran
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person country of birth":
Relation: person country of birth  
Context: The famous physicist Albert Einstein was born in the Kingdom of Württemberg in the German Empire.  
Head Entity: Albert Einstein  
Tail Entity: Germany  

Relation: person country of birth  
Context: The renowned author Gabriel García Márquez was born in Aracataca, Colombia, where he drew inspiration for many of his works.  
Head Entity: Gabriel García Márquez  
Tail Entity: Colombia  
#### organization website ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization website" means an organization 's website is an url.
Here is an example:
Relation: organization website
Context: 11.30.08 2008 cma awards red carpet special http://www.cmt.com/shows/dyn/2008-cma-awards-red-carpet/1/episode.jhtml
Head Entity: cma
Tail Entity: http://www.cmt.com/shows/dyn/2008-cma-awards-red-carpet/1/episode.jhtml
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization website":
Relation: organization website  
Context: The official site for the American Red Cross can be found at https://www.redcross.org.  
Head Entity: American Red Cross  
Tail Entity: https://www.redcross.org  

Relation: organization website  
Context: For more information about the World Wildlife Fund, visit their website at https://www.worldwildlife.org.  
Head Entity: World Wildlife Fund  
Tail Entity: https://www.worldwildlife.org  
#### organization shareholders ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization shareholders" means an organization was invested by a person.
Here is an example:
Relation: organization shareholders
Context: marsans already owns the madrid-based air comet and 95 percent of aerolineas argentinas .
Head Entity: aerolineas argentinas
Tail Entity: marsans
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization shareholders":
Relation: organization shareholders  
Context: tech giant apple inc. has seen significant investments from billionaire investor warren buffett's berkshire hathaway.  
Head Entity: apple inc.  
Tail Entity: warren buffett  

Relation: organization shareholders  
Context: the renowned investment firm blackrock has acquired a substantial stake in the renewable energy company nextEra energy.  
Head Entity: nextEra energy  
Tail Entity: blackrock  
#### organization dissolved ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization dissolved" means an organization was dissolved in a date.
Here is an example:
Relation: organization dissolved
Context: the tse suffered its worst-ever system crash in november 2005 which paralyzed the world 's second largest bourse and forced it to shelve plan for a listing of its own .
Head Entity: tse
Tail Entity: november 2005
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization dissolved":
Relation: organization dissolved  
Context: the once-prominent tech startup, Innovatech, officially ceased operations in early 2020 after struggling to secure funding.  
Head Entity: Innovatech  
Tail Entity: early 2020  

Relation: organization dissolved  
Context: after years of financial difficulties, the local arts council announced its dissolution in the spring of 2018, leaving many artists without support.  
Head Entity: local arts council  
Tail Entity: spring of 2018  
#### organization founded by ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization founded by" means an organization was found by a person.
Here is an example:
Relation: organization founded by
Context: `` i have no greater obligation than to ensure the safely of airline travelers in this country , '' transportation secretary ray lahood said in a joint statement with j. randolph babbitt , administrator of the federal aviation administration , that was issued on the eve of a senate hearing on aviation safety .
Head Entity: federal aviation administration
Tail Entity: j. randolph babbitt
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization founded by":
Relation: organization founded by  
Context: In 1975, the renowned physicist and entrepreneur, Dr. Jane Smith, established Quantum Innovations, a company dedicated to advancing quantum computing technologies.  
Head Entity: Quantum Innovations  
Tail Entity: Dr. Jane Smith  

Relation: organization founded by  
Context: The charity organization, Hope for Tomorrow, was created in 2010 by the famous philanthropist, John Doe, to support underprivileged children around the world.  
Head Entity: Hope for Tomorrow  
Tail Entity: John Doe  
Mixup data size:  224
MixupTrain:  epoch  0, batch     0 | loss: 2.6906962MixupTrain:  epoch  0, batch     1 | loss: 2.5582900MixupTrain:  epoch  0, batch     2 | loss: 3.3605613MixupTrain:  epoch  0, batch     3 | loss: 3.6567840MixupTrain:  epoch  0, batch     4 | loss: 4.7043389MixupTrain:  epoch  0, batch     5 | loss: 2.4860707MixupTrain:  epoch  0, batch     6 | loss: 5.6913272MixupTrain:  epoch  0, batch     7 | loss: 2.8003633MixupTrain:  epoch  0, batch     8 | loss: 2.4303556MixupTrain:  epoch  0, batch     9 | loss: 3.5026577MixupTrain:  epoch  0, batch    10 | loss: 2.6432415MixupTrain:  epoch  0, batch    11 | loss: 2.6771950MixupTrain:  epoch  0, batch    12 | loss: 3.2552667MixupTrain:  epoch  0, batch    13 | loss: 3.0258630
MemoryTrain:  epoch  0, batch     0 | loss: 2.1776667MemoryTrain:  epoch  0, batch     1 | loss: 3.5238278MemoryTrain:  epoch  0, batch     2 | loss: 2.0571620MemoryTrain:  epoch  0, batch     3 | loss: 2.5692759MemoryTrain:  epoch  0, batch     4 | loss: 2.4554019MemoryTrain:  epoch  0, batch     5 | loss: 2.7335153MemoryTrain:  epoch  1, batch     0 | loss: 2.1002090MemoryTrain:  epoch  1, batch     1 | loss: 2.9707892MemoryTrain:  epoch  1, batch     2 | loss: 2.0627160MemoryTrain:  epoch  1, batch     3 | loss: 2.5343657MemoryTrain:  epoch  1, batch     4 | loss: 1.5886480MemoryTrain:  epoch  1, batch     5 | loss: 2.7805212MemoryTrain:  epoch  2, batch     0 | loss: 2.2649927MemoryTrain:  epoch  2, batch     1 | loss: 2.1937728MemoryTrain:  epoch  2, batch     2 | loss: 2.2299809MemoryTrain:  epoch  2, batch     3 | loss: 1.8217201MemoryTrain:  epoch  2, batch     4 | loss: 1.9844856MemoryTrain:  epoch  2, batch     5 | loss: 2.2655528MemoryTrain:  epoch  3, batch     0 | loss: 1.5200624MemoryTrain:  epoch  3, batch     1 | loss: 2.1583290MemoryTrain:  epoch  3, batch     2 | loss: 2.2252712MemoryTrain:  epoch  3, batch     3 | loss: 2.0410531MemoryTrain:  epoch  3, batch     4 | loss: 1.7534065MemoryTrain:  epoch  3, batch     5 | loss: 1.7386372MemoryTrain:  epoch  4, batch     0 | loss: 1.7423384MemoryTrain:  epoch  4, batch     1 | loss: 1.6861019MemoryTrain:  epoch  4, batch     2 | loss: 1.8641043MemoryTrain:  epoch  4, batch     3 | loss: 1.8055129MemoryTrain:  epoch  4, batch     4 | loss: 1.7699789MemoryTrain:  epoch  4, batch     5 | loss: 1.7521042MemoryTrain:  epoch  5, batch     0 | loss: 1.6012874MemoryTrain:  epoch  5, batch     1 | loss: 1.9470892MemoryTrain:  epoch  5, batch     2 | loss: 2.0589507MemoryTrain:  epoch  5, batch     3 | loss: 1.6009023MemoryTrain:  epoch  5, batch     4 | loss: 1.6842484MemoryTrain:  epoch  5, batch     5 | loss: 1.5516086MemoryTrain:  epoch  6, batch     0 | loss: 1.9137225MemoryTrain:  epoch  6, batch     1 | loss: 1.4822212MemoryTrain:  epoch  6, batch     2 | loss: 1.7182333MemoryTrain:  epoch  6, batch     3 | loss: 1.5102534MemoryTrain:  epoch  6, batch     4 | loss: 1.6534708MemoryTrain:  epoch  6, batch     5 | loss: 1.4700574MemoryTrain:  epoch  7, batch     0 | loss: 1.4684796MemoryTrain:  epoch  7, batch     1 | loss: 1.4174681MemoryTrain:  epoch  7, batch     2 | loss: 1.7601599MemoryTrain:  epoch  7, batch     3 | loss: 1.3646736MemoryTrain:  epoch  7, batch     4 | loss: 1.8581119MemoryTrain:  epoch  7, batch     5 | loss: 1.5350544MemoryTrain:  epoch  8, batch     0 | loss: 1.4302337MemoryTrain:  epoch  8, batch     1 | loss: 1.6312487MemoryTrain:  epoch  8, batch     2 | loss: 1.3521414MemoryTrain:  epoch  8, batch     3 | loss: 1.4105972MemoryTrain:  epoch  8, batch     4 | loss: 1.8775790MemoryTrain:  epoch  8, batch     5 | loss: 1.2886546MemoryTrain:  epoch  9, batch     0 | loss: 1.6060007MemoryTrain:  epoch  9, batch     1 | loss: 1.4534744MemoryTrain:  epoch  9, batch     2 | loss: 1.3611622MemoryTrain:  epoch  9, batch     3 | loss: 1.5341978MemoryTrain:  epoch  9, batch     4 | loss: 1.4235821MemoryTrain:  epoch  9, batch     5 | loss: 1.5772743
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 85.94%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 86.25%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 85.42%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 83.93%   [EVAL] batch:    7 | acc: 6.25%,  total acc: 74.22%   
[EVAL] batch:    0 | acc: 12.50%,  total acc: 12.50%   [EVAL] batch:    1 | acc: 18.75%,  total acc: 15.62%   [EVAL] batch:    2 | acc: 25.00%,  total acc: 18.75%   [EVAL] batch:    3 | acc: 12.50%,  total acc: 17.19%   [EVAL] batch:    4 | acc: 12.50%,  total acc: 16.25%   [EVAL] batch:    5 | acc: 31.25%,  total acc: 18.75%   [EVAL] batch:    6 | acc: 18.75%,  total acc: 18.75%   [EVAL] batch:    7 | acc: 25.00%,  total acc: 19.53%   [EVAL] batch:    8 | acc: 62.50%,  total acc: 24.31%   [EVAL] batch:    9 | acc: 56.25%,  total acc: 27.50%   [EVAL] batch:   10 | acc: 56.25%,  total acc: 30.11%   [EVAL] batch:   11 | acc: 50.00%,  total acc: 31.77%   [EVAL] batch:   12 | acc: 31.25%,  total acc: 31.73%   [EVAL] batch:   13 | acc: 43.75%,  total acc: 32.59%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 35.42%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 36.72%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 38.97%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 40.28%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 41.45%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 43.75%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 46.43%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 48.58%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 50.54%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 52.34%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 54.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 56.01%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 57.41%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 58.93%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 60.34%   [EVAL] batch:   29 | acc: 75.00%,  total acc: 60.83%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 61.90%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 62.89%   [EVAL] batch:   32 | acc: 81.25%,  total acc: 63.45%   [EVAL] batch:   33 | acc: 31.25%,  total acc: 62.50%   [EVAL] batch:   34 | acc: 43.75%,  total acc: 61.96%   [EVAL] batch:   35 | acc: 43.75%,  total acc: 61.46%   [EVAL] batch:   36 | acc: 37.50%,  total acc: 60.81%   [EVAL] batch:   37 | acc: 75.00%,  total acc: 61.18%   [EVAL] batch:   38 | acc: 75.00%,  total acc: 61.54%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 62.50%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 63.26%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 64.14%   [EVAL] batch:   42 | acc: 18.75%,  total acc: 63.08%   [EVAL] batch:   43 | acc: 31.25%,  total acc: 62.36%   [EVAL] batch:   44 | acc: 43.75%,  total acc: 61.94%   [EVAL] batch:   45 | acc: 37.50%,  total acc: 61.41%   [EVAL] batch:   46 | acc: 37.50%,  total acc: 60.90%   [EVAL] batch:   47 | acc: 62.50%,  total acc: 60.94%   [EVAL] batch:   48 | acc: 12.50%,  total acc: 59.95%   [EVAL] batch:   49 | acc: 31.25%,  total acc: 59.38%   [EVAL] batch:   50 | acc: 6.25%,  total acc: 58.33%   [EVAL] batch:   51 | acc: 6.25%,  total acc: 57.33%   [EVAL] batch:   52 | acc: 12.50%,  total acc: 56.49%   [EVAL] batch:   53 | acc: 43.75%,  total acc: 56.25%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 56.82%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 57.48%   [EVAL] batch:   56 | acc: 87.50%,  total acc: 58.00%   [EVAL] batch:   57 | acc: 75.00%,  total acc: 58.30%   [EVAL] batch:   58 | acc: 75.00%,  total acc: 58.58%   [EVAL] batch:   59 | acc: 68.75%,  total acc: 58.75%   [EVAL] batch:   60 | acc: 18.75%,  total acc: 58.09%   [EVAL] batch:   61 | acc: 50.00%,  total acc: 57.96%   [EVAL] batch:   62 | acc: 31.25%,  total acc: 57.54%   [EVAL] batch:   63 | acc: 62.50%,  total acc: 57.62%   [EVAL] batch:   64 | acc: 31.25%,  total acc: 57.21%   [EVAL] batch:   65 | acc: 31.25%,  total acc: 56.82%   [EVAL] batch:   66 | acc: 37.50%,  total acc: 56.53%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 57.17%   [EVAL] batch:   68 | acc: 68.75%,  total acc: 57.34%   [EVAL] batch:   69 | acc: 43.75%,  total acc: 57.14%   [EVAL] batch:   70 | acc: 68.75%,  total acc: 57.31%   [EVAL] batch:   71 | acc: 75.00%,  total acc: 57.55%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 58.13%   [EVAL] batch:   73 | acc: 100.00%,  total acc: 58.70%   [EVAL] batch:   74 | acc: 100.00%,  total acc: 59.25%   [EVAL] batch:   75 | acc: 100.00%,  total acc: 59.79%   [EVAL] batch:   76 | acc: 100.00%,  total acc: 60.31%   [EVAL] batch:   77 | acc: 75.00%,  total acc: 60.50%   [EVAL] batch:   78 | acc: 25.00%,  total acc: 60.05%   [EVAL] batch:   79 | acc: 25.00%,  total acc: 59.61%   [EVAL] batch:   80 | acc: 12.50%,  total acc: 59.03%   [EVAL] batch:   81 | acc: 12.50%,  total acc: 58.46%   [EVAL] batch:   82 | acc: 6.25%,  total acc: 57.83%   [EVAL] batch:   83 | acc: 18.75%,  total acc: 57.37%   [EVAL] batch:   84 | acc: 25.00%,  total acc: 56.99%   [EVAL] batch:   85 | acc: 50.00%,  total acc: 56.90%   [EVAL] batch:   86 | acc: 50.00%,  total acc: 56.82%   [EVAL] batch:   87 | acc: 31.25%,  total acc: 56.53%   [EVAL] batch:   88 | acc: 56.25%,  total acc: 56.53%   [EVAL] batch:   89 | acc: 56.25%,  total acc: 56.53%   [EVAL] batch:   90 | acc: 68.75%,  total acc: 56.66%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 57.13%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 57.59%   [EVAL] batch:   93 | acc: 100.00%,  total acc: 58.05%   [EVAL] batch:   94 | acc: 87.50%,  total acc: 58.36%   [EVAL] batch:   95 | acc: 87.50%,  total acc: 58.66%   [EVAL] batch:   96 | acc: 68.75%,  total acc: 58.76%   [EVAL] batch:   97 | acc: 56.25%,  total acc: 58.74%   [EVAL] batch:   98 | acc: 81.25%,  total acc: 58.96%   [EVAL] batch:   99 | acc: 87.50%,  total acc: 59.25%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 59.65%   [EVAL] batch:  101 | acc: 68.75%,  total acc: 59.74%   [EVAL] batch:  102 | acc: 87.50%,  total acc: 60.01%   [EVAL] batch:  103 | acc: 87.50%,  total acc: 60.28%   [EVAL] batch:  104 | acc: 87.50%,  total acc: 60.54%   [EVAL] batch:  105 | acc: 81.25%,  total acc: 60.73%   [EVAL] batch:  106 | acc: 12.50%,  total acc: 60.28%   
cur_acc:  ['0.8636', '0.8292', '0.7768', '0.7361', '0.5682', '0.7422']
his_acc:  ['0.8636', '0.8590', '0.7941', '0.7412', '0.6687', '0.6028']
CurrentTrain: epoch  0, batch     0 | loss: 4.9873838CurrentTrain: epoch  0, batch     1 | loss: 4.9501061CurrentTrain: epoch  1, batch     0 | loss: 3.5747724CurrentTrain: epoch  1, batch     1 | loss: 3.3735836CurrentTrain: epoch  2, batch     0 | loss: 2.8695085CurrentTrain: epoch  2, batch     1 | loss: 2.5486996CurrentTrain: epoch  3, batch     0 | loss: 2.5962667CurrentTrain: epoch  3, batch     1 | loss: 2.4128652CurrentTrain: epoch  4, batch     0 | loss: 2.5797086CurrentTrain: epoch  4, batch     1 | loss: 2.2192519CurrentTrain: epoch  5, batch     0 | loss: 2.3224778CurrentTrain: epoch  5, batch     1 | loss: 2.3661609CurrentTrain: epoch  6, batch     0 | loss: 2.1791668CurrentTrain: epoch  6, batch     1 | loss: 2.2629435CurrentTrain: epoch  7, batch     0 | loss: 2.1849995CurrentTrain: epoch  7, batch     1 | loss: 2.0900841CurrentTrain: epoch  8, batch     0 | loss: 2.1318035CurrentTrain: epoch  8, batch     1 | loss: 2.0203617CurrentTrain: epoch  9, batch     0 | loss: 1.9424593CurrentTrain: epoch  9, batch     1 | loss: 1.8409563
#### person cause of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person cause of death" means a person was died of an event.
Here is an example:
Relation: person cause of death
Context: mr scheider had suffered from multiple myeloma for several years , and died of complications from a staph infection , his wife , brenda siemer , said .
Head Entity: mr scheider
Tail Entity: complications from a staph infection
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person cause of death":
Relation: person cause of death  
Context: after battling with lung cancer for over a year, john doe passed away peacefully in his sleep, surrounded by family.  
Head Entity: john doe  
Tail Entity: lung cancer  

Relation: person cause of death  
Context: the renowned scientist, dr. emily white, tragically lost her life due to a car accident while returning from a conference.  
Head Entity: dr. emily white  
Tail Entity: car accident  
#### organization political religious affiliation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization political religious affiliation" means an organization is the member of a religion.
Here is an example:
Relation: organization political religious affiliation
Context: the militia has also suffered from allegations of being involved in clashes with rival shiite militias , particularly the badr corp , the military wing linked to supreme iraqi islamic council , the party led by powerful politician abdel aziz al-hakim .
Head Entity: badr corp
Tail Entity: shiite
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization political religious affiliation":
Relation: organization political religious affiliation  
Context: The Christian Democratic Union has been a significant player in German politics, often aligning itself with various religious groups to promote policies that reflect its Christian values.  
Head Entity: Christian Democratic Union  
Tail Entity: Christian  

Relation: organization political religious affiliation  
Context: The Islamic Society of North America has been actively involved in advocating for Muslim rights and representation in the political landscape of the United States.  
Head Entity: Islamic Society of North America  
Tail Entity: Muslim  
#### organization stateorprovince of headquarters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization stateorprovince of headquarters" means an organization is located in a state or province.
Here is an example:
Relation: organization stateorprovince of headquarters
Context: based in armonk , new york , mbia insures $ 670 billion -lrb- euro452 .18 billion -rrb- in debt .
Head Entity: mbia
Tail Entity: new york
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization stateorprovince of headquarters":
Relation: organization stateorprovince of headquarters  
Context: the tech giant apple inc. has its headquarters in cupertino, california, where it develops innovative products.  
Head Entity: apple inc.  
Tail Entity: california  

Relation: organization stateorprovince of headquarters  
Context: the multinational corporation samsung electronics is headquartered in suwon, south korea, and is a leader in consumer electronics.  
Head Entity: samsung electronics  
Tail Entity: south korea  
#### person other family ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person other family" means a person 's relative is a person.
Here is an example:
Relation: person other family
Context: parren mitchell 's sister-in-law , juanita jackson mitchell , was the long - time head and legal counsel of the maryland naacp .
Head Entity: parren mitchell
Tail Entity: juanita jackson mitchell
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person other family":
Relation: person other family  
Context: During the family reunion, it was revealed that Sarah's cousin, Emily, had recently graduated from college with honors.  
Head Entity: Sarah  
Tail Entity: Emily  

Relation: person other family  
Context: Michael often reminisces about the summer vacations spent at his grandmother's house with his aunt, Linda, who always baked the best cookies.  
Head Entity: Michael  
Tail Entity: Linda  
#### person city of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person city of death" means a person was died in a city.
Here is an example:
Relation: person city of death
Context: irene morgan kirkaldy , 90 , who died of alzheimer 's disease aug. 10 at her home in gloucester , va. , quietly changed history in 1944 when she refused to give up her seat on a crowded greyhound bus to a white couple .
Head Entity: her
Tail Entity: gloucester
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person city of death":
Relation: person city of death  
Context: john smith, a renowned author, passed away on march 5 in his apartment located in new york city, leaving behind a legacy of literary works that inspired many.  
Head Entity: john smith  
Tail Entity: new york city  

Relation: person city of death  
Context: after a long battle with cancer, elizabeth taylor died on march 23 at a hospital in los angeles, where she had spent her final days surrounded by family and friends.  
Head Entity: elizabeth taylor  
Tail Entity: los angeles  
Mixup data size:  254
MixupTrain:  epoch  0, batch     0 | loss: 2.6865029MixupTrain:  epoch  0, batch     1 | loss: 2.7595168MixupTrain:  epoch  0, batch     2 | loss: 2.7521050MixupTrain:  epoch  0, batch     3 | loss: 3.4357657MixupTrain:  epoch  0, batch     4 | loss: 2.2279216MixupTrain:  epoch  0, batch     5 | loss: 2.1448940MixupTrain:  epoch  0, batch     6 | loss: 3.2267640MixupTrain:  epoch  0, batch     7 | loss: 2.6483265MixupTrain:  epoch  0, batch     8 | loss: 2.7250264MixupTrain:  epoch  0, batch     9 | loss: 2.3842558MixupTrain:  epoch  0, batch    10 | loss: 2.2840376MixupTrain:  epoch  0, batch    11 | loss: 3.6889155MixupTrain:  epoch  0, batch    12 | loss: 4.3125183MixupTrain:  epoch  0, batch    13 | loss: 2.5053662MixupTrain:  epoch  0, batch    14 | loss: 2.9033348MixupTrain:  epoch  0, batch    15 | loss: 2.5518905
MemoryTrain:  epoch  0, batch     0 | loss: 1.9610915MemoryTrain:  epoch  0, batch     1 | loss: 1.9711350MemoryTrain:  epoch  0, batch     2 | loss: 2.5079842MemoryTrain:  epoch  0, batch     3 | loss: 3.2096238MemoryTrain:  epoch  0, batch     4 | loss: 3.4837513MemoryTrain:  epoch  0, batch     5 | loss: 3.2259474MemoryTrain:  epoch  0, batch     6 | loss: 2.6159027MemoryTrain:  epoch  1, batch     0 | loss: 3.0304289MemoryTrain:  epoch  1, batch     1 | loss: 2.2450101MemoryTrain:  epoch  1, batch     2 | loss: 2.5161970MemoryTrain:  epoch  1, batch     3 | loss: 2.1231136MemoryTrain:  epoch  1, batch     4 | loss: 2.4947169MemoryTrain:  epoch  1, batch     5 | loss: 2.8486686MemoryTrain:  epoch  1, batch     6 | loss: 2.8868241MemoryTrain:  epoch  2, batch     0 | loss: 1.8066336MemoryTrain:  epoch  2, batch     1 | loss: 2.4493842MemoryTrain:  epoch  2, batch     2 | loss: 2.6520488MemoryTrain:  epoch  2, batch     3 | loss: 2.6999345MemoryTrain:  epoch  2, batch     4 | loss: 1.9561872MemoryTrain:  epoch  2, batch     5 | loss: 1.9729815MemoryTrain:  epoch  2, batch     6 | loss: 2.6622276MemoryTrain:  epoch  3, batch     0 | loss: 2.2013974MemoryTrain:  epoch  3, batch     1 | loss: 2.4363828MemoryTrain:  epoch  3, batch     2 | loss: 2.6013770MemoryTrain:  epoch  3, batch     3 | loss: 2.3137460MemoryTrain:  epoch  3, batch     4 | loss: 1.9091361MemoryTrain:  epoch  3, batch     5 | loss: 1.5579942MemoryTrain:  epoch  3, batch     6 | loss: 1.3571060MemoryTrain:  epoch  4, batch     0 | loss: 1.5949277MemoryTrain:  epoch  4, batch     1 | loss: 1.9788446MemoryTrain:  epoch  4, batch     2 | loss: 1.9708037MemoryTrain:  epoch  4, batch     3 | loss: 2.1868291MemoryTrain:  epoch  4, batch     4 | loss: 2.1610770MemoryTrain:  epoch  4, batch     5 | loss: 2.1782060MemoryTrain:  epoch  4, batch     6 | loss: 1.4616163MemoryTrain:  epoch  5, batch     0 | loss: 2.0638313MemoryTrain:  epoch  5, batch     1 | loss: 1.4467469MemoryTrain:  epoch  5, batch     2 | loss: 2.1106505MemoryTrain:  epoch  5, batch     3 | loss: 1.7700266MemoryTrain:  epoch  5, batch     4 | loss: 1.7212677MemoryTrain:  epoch  5, batch     5 | loss: 2.0695794MemoryTrain:  epoch  5, batch     6 | loss: 1.6567309MemoryTrain:  epoch  6, batch     0 | loss: 1.7202771MemoryTrain:  epoch  6, batch     1 | loss: 1.3512330MemoryTrain:  epoch  6, batch     2 | loss: 2.1245470MemoryTrain:  epoch  6, batch     3 | loss: 1.5911281MemoryTrain:  epoch  6, batch     4 | loss: 1.3943682MemoryTrain:  epoch  6, batch     5 | loss: 2.0563605MemoryTrain:  epoch  6, batch     6 | loss: 1.4739740MemoryTrain:  epoch  7, batch     0 | loss: 1.8052561MemoryTrain:  epoch  7, batch     1 | loss: 1.5270091MemoryTrain:  epoch  7, batch     2 | loss: 1.6782175MemoryTrain:  epoch  7, batch     3 | loss: 1.5013125MemoryTrain:  epoch  7, batch     4 | loss: 1.5614648MemoryTrain:  epoch  7, batch     5 | loss: 1.6838015MemoryTrain:  epoch  7, batch     6 | loss: 1.9182873MemoryTrain:  epoch  8, batch     0 | loss: 1.5251625MemoryTrain:  epoch  8, batch     1 | loss: 1.7465081MemoryTrain:  epoch  8, batch     2 | loss: 1.4658358MemoryTrain:  epoch  8, batch     3 | loss: 1.3122916MemoryTrain:  epoch  8, batch     4 | loss: 1.8742778MemoryTrain:  epoch  8, batch     5 | loss: 1.2964524MemoryTrain:  epoch  8, batch     6 | loss: 1.4342277MemoryTrain:  epoch  9, batch     0 | loss: 1.5455506MemoryTrain:  epoch  9, batch     1 | loss: 1.4309500MemoryTrain:  epoch  9, batch     2 | loss: 1.4716733MemoryTrain:  epoch  9, batch     3 | loss: 1.6798342MemoryTrain:  epoch  9, batch     4 | loss: 1.4741088MemoryTrain:  epoch  9, batch     5 | loss: 1.3101569MemoryTrain:  epoch  9, batch     6 | loss: 1.8674487
[EVAL] batch:    0 | acc: 43.75%,  total acc: 43.75%   [EVAL] batch:    1 | acc: 68.75%,  total acc: 56.25%   [EVAL] batch:    2 | acc: 56.25%,  total acc: 56.25%   [EVAL] batch:    3 | acc: 62.50%,  total acc: 57.81%   [EVAL] batch:    4 | acc: 18.75%,  total acc: 50.00%   [EVAL] batch:    5 | acc: 68.75%,  total acc: 53.12%   [EVAL] batch:    6 | acc: 43.75%,  total acc: 51.79%   [EVAL] batch:    7 | acc: 68.75%,  total acc: 53.91%   [EVAL] batch:    8 | acc: 43.75%,  total acc: 52.78%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 55.00%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 57.39%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 59.38%   [EVAL] batch:   12 | acc: 50.00%,  total acc: 58.65%   
[EVAL] batch:    0 | acc: 6.25%,  total acc: 6.25%   [EVAL] batch:    1 | acc: 12.50%,  total acc: 9.38%   [EVAL] batch:    2 | acc: 18.75%,  total acc: 12.50%   [EVAL] batch:    3 | acc: 6.25%,  total acc: 10.94%   [EVAL] batch:    4 | acc: 12.50%,  total acc: 11.25%   [EVAL] batch:    5 | acc: 18.75%,  total acc: 12.50%   [EVAL] batch:    6 | acc: 12.50%,  total acc: 12.50%   [EVAL] batch:    7 | acc: 25.00%,  total acc: 14.06%   [EVAL] batch:    8 | acc: 56.25%,  total acc: 18.75%   [EVAL] batch:    9 | acc: 43.75%,  total acc: 21.25%   [EVAL] batch:   10 | acc: 62.50%,  total acc: 25.00%   [EVAL] batch:   11 | acc: 37.50%,  total acc: 26.04%   [EVAL] batch:   12 | acc: 25.00%,  total acc: 25.96%   [EVAL] batch:   13 | acc: 37.50%,  total acc: 26.79%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 30.00%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 31.64%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 34.19%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 35.76%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 37.17%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 39.69%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 42.56%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 44.89%   [EVAL] batch:   22 | acc: 87.50%,  total acc: 46.74%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 48.70%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 50.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 52.64%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 54.17%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 55.58%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 57.11%   [EVAL] batch:   29 | acc: 75.00%,  total acc: 57.71%   [EVAL] batch:   30 | acc: 81.25%,  total acc: 58.47%   [EVAL] batch:   31 | acc: 87.50%,  total acc: 59.38%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 58.71%   [EVAL] batch:   33 | acc: 0.00%,  total acc: 56.99%   [EVAL] batch:   34 | acc: 6.25%,  total acc: 55.54%   [EVAL] batch:   35 | acc: 6.25%,  total acc: 54.17%   [EVAL] batch:   36 | acc: 0.00%,  total acc: 52.70%   [EVAL] batch:   37 | acc: 56.25%,  total acc: 52.80%   [EVAL] batch:   38 | acc: 87.50%,  total acc: 53.69%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 54.84%   [EVAL] batch:   40 | acc: 87.50%,  total acc: 55.64%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 56.70%   [EVAL] batch:   42 | acc: 25.00%,  total acc: 55.96%   [EVAL] batch:   43 | acc: 25.00%,  total acc: 55.26%   [EVAL] batch:   44 | acc: 12.50%,  total acc: 54.31%   [EVAL] batch:   45 | acc: 37.50%,  total acc: 53.94%   [EVAL] batch:   46 | acc: 37.50%,  total acc: 53.59%   [EVAL] batch:   47 | acc: 56.25%,  total acc: 53.65%   [EVAL] batch:   48 | acc: 12.50%,  total acc: 52.81%   [EVAL] batch:   49 | acc: 12.50%,  total acc: 52.00%   [EVAL] batch:   50 | acc: 0.00%,  total acc: 50.98%   [EVAL] batch:   51 | acc: 0.00%,  total acc: 50.00%   [EVAL] batch:   52 | acc: 0.00%,  total acc: 49.06%   [EVAL] batch:   53 | acc: 43.75%,  total acc: 48.96%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 49.77%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 50.67%   [EVAL] batch:   56 | acc: 87.50%,  total acc: 51.32%   [EVAL] batch:   57 | acc: 75.00%,  total acc: 51.72%   [EVAL] batch:   58 | acc: 81.25%,  total acc: 52.22%   [EVAL] batch:   59 | acc: 68.75%,  total acc: 52.50%   [EVAL] batch:   60 | acc: 12.50%,  total acc: 51.84%   [EVAL] batch:   61 | acc: 18.75%,  total acc: 51.31%   [EVAL] batch:   62 | acc: 0.00%,  total acc: 50.50%   [EVAL] batch:   63 | acc: 6.25%,  total acc: 49.80%   [EVAL] batch:   64 | acc: 18.75%,  total acc: 49.33%   [EVAL] batch:   65 | acc: 6.25%,  total acc: 48.67%   [EVAL] batch:   66 | acc: 18.75%,  total acc: 48.23%   [EVAL] batch:   67 | acc: 93.75%,  total acc: 48.90%   [EVAL] batch:   68 | acc: 50.00%,  total acc: 48.91%   [EVAL] batch:   69 | acc: 37.50%,  total acc: 48.75%   [EVAL] batch:   70 | acc: 68.75%,  total acc: 49.03%   [EVAL] batch:   71 | acc: 75.00%,  total acc: 49.39%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 50.09%   [EVAL] batch:   73 | acc: 100.00%,  total acc: 50.76%   [EVAL] batch:   74 | acc: 100.00%,  total acc: 51.42%   [EVAL] batch:   75 | acc: 100.00%,  total acc: 52.06%   [EVAL] batch:   76 | acc: 100.00%,  total acc: 52.68%   [EVAL] batch:   77 | acc: 75.00%,  total acc: 52.96%   [EVAL] batch:   78 | acc: 25.00%,  total acc: 52.61%   [EVAL] batch:   79 | acc: 25.00%,  total acc: 52.27%   [EVAL] batch:   80 | acc: 25.00%,  total acc: 51.93%   [EVAL] batch:   81 | acc: 18.75%,  total acc: 51.52%   [EVAL] batch:   82 | acc: 31.25%,  total acc: 51.28%   [EVAL] batch:   83 | acc: 43.75%,  total acc: 51.19%   [EVAL] batch:   84 | acc: 31.25%,  total acc: 50.96%   [EVAL] batch:   85 | acc: 50.00%,  total acc: 50.94%   [EVAL] batch:   86 | acc: 50.00%,  total acc: 50.93%   [EVAL] batch:   87 | acc: 31.25%,  total acc: 50.71%   [EVAL] batch:   88 | acc: 56.25%,  total acc: 50.77%   [EVAL] batch:   89 | acc: 56.25%,  total acc: 50.83%   [EVAL] batch:   90 | acc: 68.75%,  total acc: 51.03%   [EVAL] batch:   91 | acc: 93.75%,  total acc: 51.49%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 52.02%   [EVAL] batch:   93 | acc: 100.00%,  total acc: 52.53%   [EVAL] batch:   94 | acc: 81.25%,  total acc: 52.83%   [EVAL] batch:   95 | acc: 75.00%,  total acc: 53.06%   [EVAL] batch:   96 | acc: 31.25%,  total acc: 52.84%   [EVAL] batch:   97 | acc: 37.50%,  total acc: 52.68%   [EVAL] batch:   98 | acc: 43.75%,  total acc: 52.59%   [EVAL] batch:   99 | acc: 81.25%,  total acc: 52.88%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 53.34%   [EVAL] batch:  101 | acc: 68.75%,  total acc: 53.49%   [EVAL] batch:  102 | acc: 87.50%,  total acc: 53.82%   [EVAL] batch:  103 | acc: 93.75%,  total acc: 54.21%   [EVAL] batch:  104 | acc: 93.75%,  total acc: 54.58%   [EVAL] batch:  105 | acc: 100.00%,  total acc: 55.01%   [EVAL] batch:  106 | acc: 56.25%,  total acc: 55.02%   [EVAL] batch:  107 | acc: 62.50%,  total acc: 55.09%   [EVAL] batch:  108 | acc: 62.50%,  total acc: 55.16%   [EVAL] batch:  109 | acc: 62.50%,  total acc: 55.23%   [EVAL] batch:  110 | acc: 25.00%,  total acc: 54.95%   [EVAL] batch:  111 | acc: 62.50%,  total acc: 55.02%   [EVAL] batch:  112 | acc: 50.00%,  total acc: 54.98%   [EVAL] batch:  113 | acc: 56.25%,  total acc: 54.99%   [EVAL] batch:  114 | acc: 62.50%,  total acc: 55.05%   [EVAL] batch:  115 | acc: 56.25%,  total acc: 55.06%   [EVAL] batch:  116 | acc: 75.00%,  total acc: 55.24%   [EVAL] batch:  117 | acc: 93.75%,  total acc: 55.56%   [EVAL] batch:  118 | acc: 68.75%,  total acc: 55.67%   
cur_acc:  ['0.8636', '0.8292', '0.7768', '0.7361', '0.5682', '0.7422', '0.5865']
his_acc:  ['0.8636', '0.8590', '0.7941', '0.7412', '0.6687', '0.6028', '0.5567']
CurrentTrain: epoch  0, batch     0 | loss: 4.8800516CurrentTrain: epoch  0, batch     1 | loss: 5.3127999CurrentTrain: epoch  1, batch     0 | loss: 3.8299885CurrentTrain: epoch  1, batch     1 | loss: 3.6602433CurrentTrain: epoch  2, batch     0 | loss: 3.3794022CurrentTrain: epoch  2, batch     1 | loss: 3.1645372CurrentTrain: epoch  3, batch     0 | loss: 3.0063028CurrentTrain: epoch  3, batch     1 | loss: 2.4858060CurrentTrain: epoch  4, batch     0 | loss: 2.5308626CurrentTrain: epoch  4, batch     1 | loss: 2.5615129CurrentTrain: epoch  5, batch     0 | loss: 2.3246822CurrentTrain: epoch  5, batch     1 | loss: 2.4350760CurrentTrain: epoch  6, batch     0 | loss: 2.2947996CurrentTrain: epoch  6, batch     1 | loss: 2.1046951CurrentTrain: epoch  7, batch     0 | loss: 2.1425533CurrentTrain: epoch  7, batch     1 | loss: 2.2747948CurrentTrain: epoch  8, batch     0 | loss: 2.1576698CurrentTrain: epoch  8, batch     1 | loss: 2.2344425CurrentTrain: epoch  9, batch     0 | loss: 2.1405249CurrentTrain: epoch  9, batch     1 | loss: 2.0336757
#### organization founded ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization founded" means an organization was found in a date.
Here is an example:
Relation: organization founded
Context: the jnf was founded in 1901 to buy plots in palestine , then ruled by the ottomans .
Head Entity: jnf
Tail Entity: 1901
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization founded":
Relation: organization founded  
Context: the united nations was established in 1945 to promote international cooperation and peace.  
Head Entity: united nations  
Tail Entity: 1945  

Relation: organization founded  
Context: the world health organization was created in 1948 to coordinate global health efforts.  
Head Entity: world health organization  
Tail Entity: 1948  
#### person age ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person age" means a person 's age is a number.
Here is an example:
Relation: person age
Context: virginia republican jo ann davis passed away on saturday at the age of 57 .
Head Entity: jo ann davis
Tail Entity: 57
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person age":
Relation: person age  
Context: the famous actor robert downey jr. celebrated his 56th birthday last week.  
Head Entity: robert downey jr.  
Tail Entity: 56  

Relation: person age  
Context: on her 30th birthday, emily decided to throw a big party with all her friends.  
Head Entity: emily  
Tail Entity: 30  
#### person city of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person city of birth" means a person was born in a city.
Here is an example:
Relation: person city of birth
Context: rothman was born in san francisco in 1932 in an orthodox jewish family .
Head Entity: rothman
Tail Entity: san francisco
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person city of birth":
Relation: person city of birth  
Context: after spending his early years in new york, he moved to los angeles where he began his career.  
Head Entity: he  
Tail Entity: los angeles  

Relation: person city of birth  
Context: the famous author was born in a small town near boston, which greatly influenced his writing.  
Head Entity: the famous author  
Tail Entity: boston  
#### organization members ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization members" means an organization 's member is an organization.
Here is an example:
Relation: organization members
Context: sun plays for the grand rapids flight of the international basketball league after toiling for the maryland nighthawks of the american basketball association , both development leagues for those who dream of an nba career .
Head Entity: american basketball association
Tail Entity: maryland nighthawks
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization members":
Relation: organization members  
Context: The New York Philharmonic is one of the oldest orchestras in the United States, and it has had many notable musicians as members, including the famous conductor Leonard Bernstein.  
Head Entity: New York Philharmonic  
Tail Entity: Leonard Bernstein  

Relation: organization members  
Context: The National Geographic Society has a long history of exploration and education, with many prominent explorers and scientists, such as Jane Goodall, being members of the organization.  
Head Entity: National Geographic Society  
Tail Entity: Jane Goodall  
#### person religion ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person religion" means an person is the member of a religion.
Here is an example:
Relation: person religion
Context: the pope defended his action on the grounds that he could not refuse an audience to a head of state from a country with a strong catholic tradition unless he had clear-cut proof of the allegations against him .
Head Entity: he
Tail Entity: catholic
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person religion":
Relation: person religion  
Context: After years of study and reflection, Maria decided to embrace Buddhism, finding peace and purpose in its teachings.  
Head Entity: Maria  
Tail Entity: Buddhism  

Relation: person religion  
Context: During the festival, Ahmed proudly wore his traditional attire, celebrating his deep connection to Islam and its rich cultural heritage.  
Head Entity: Ahmed  
Tail Entity: Islam  
Mixup data size:  284
MixupTrain:  epoch  0, batch     0 | loss: 2.8191559MixupTrain:  epoch  0, batch     1 | loss: 2.7760456MixupTrain:  epoch  0, batch     2 | loss: 3.0809420MixupTrain:  epoch  0, batch     3 | loss: 2.2794668MixupTrain:  epoch  0, batch     4 | loss: 2.6433269MixupTrain:  epoch  0, batch     5 | loss: 2.8604932MixupTrain:  epoch  0, batch     6 | loss: 2.5190419MixupTrain:  epoch  0, batch     7 | loss: 3.4602989MixupTrain:  epoch  0, batch     8 | loss: 3.2717444MixupTrain:  epoch  0, batch     9 | loss: 2.6897752MixupTrain:  epoch  0, batch    10 | loss: 2.1149128MixupTrain:  epoch  0, batch    11 | loss: 2.2327478MixupTrain:  epoch  0, batch    12 | loss: 2.0716879MixupTrain:  epoch  0, batch    13 | loss: 2.3217635MixupTrain:  epoch  0, batch    14 | loss: 2.8425761MixupTrain:  epoch  0, batch    15 | loss: 2.3390253MixupTrain:  epoch  0, batch    16 | loss: 3.1208387MixupTrain:  epoch  0, batch    17 | loss: 2.3382783
MemoryTrain:  epoch  0, batch     0 | loss: 2.1733489MemoryTrain:  epoch  0, batch     1 | loss: 2.2650089MemoryTrain:  epoch  0, batch     2 | loss: 1.9648800MemoryTrain:  epoch  0, batch     3 | loss: 2.9437051MemoryTrain:  epoch  0, batch     4 | loss: 3.1480486MemoryTrain:  epoch  0, batch     5 | loss: 2.0623713MemoryTrain:  epoch  0, batch     6 | loss: 2.6157098MemoryTrain:  epoch  0, batch     7 | loss: 2.9633179MemoryTrain:  epoch  1, batch     0 | loss: 1.9822139MemoryTrain:  epoch  1, batch     1 | loss: 2.3092675MemoryTrain:  epoch  1, batch     2 | loss: 1.9156413MemoryTrain:  epoch  1, batch     3 | loss: 1.7785987MemoryTrain:  epoch  1, batch     4 | loss: 2.0527899MemoryTrain:  epoch  1, batch     5 | loss: 2.4366829MemoryTrain:  epoch  1, batch     6 | loss: 2.0043869MemoryTrain:  epoch  1, batch     7 | loss: 2.7709184MemoryTrain:  epoch  2, batch     0 | loss: 2.1503000MemoryTrain:  epoch  2, batch     1 | loss: 2.0741773MemoryTrain:  epoch  2, batch     2 | loss: 1.5174141MemoryTrain:  epoch  2, batch     3 | loss: 1.8579478MemoryTrain:  epoch  2, batch     4 | loss: 1.8651780MemoryTrain:  epoch  2, batch     5 | loss: 1.8536544MemoryTrain:  epoch  2, batch     6 | loss: 1.4990412MemoryTrain:  epoch  2, batch     7 | loss: 1.7940322MemoryTrain:  epoch  3, batch     0 | loss: 1.6803243MemoryTrain:  epoch  3, batch     1 | loss: 1.6652977MemoryTrain:  epoch  3, batch     2 | loss: 1.6995218MemoryTrain:  epoch  3, batch     3 | loss: 1.5952111MemoryTrain:  epoch  3, batch     4 | loss: 1.5549812MemoryTrain:  epoch  3, batch     5 | loss: 1.4022758MemoryTrain:  epoch  3, batch     6 | loss: 1.9578915MemoryTrain:  epoch  3, batch     7 | loss: 1.8618388MemoryTrain:  epoch  4, batch     0 | loss: 1.5698962MemoryTrain:  epoch  4, batch     1 | loss: 1.5080600MemoryTrain:  epoch  4, batch     2 | loss: 1.7432337MemoryTrain:  epoch  4, batch     3 | loss: 1.4255674MemoryTrain:  epoch  4, batch     4 | loss: 1.8081069MemoryTrain:  epoch  4, batch     5 | loss: 1.6809144MemoryTrain:  epoch  4, batch     6 | loss: 1.3642547MemoryTrain:  epoch  4, batch     7 | loss: 1.3814058MemoryTrain:  epoch  5, batch     0 | loss: 1.4657890MemoryTrain:  epoch  5, batch     1 | loss: 1.4857409MemoryTrain:  epoch  5, batch     2 | loss: 1.5010411MemoryTrain:  epoch  5, batch     3 | loss: 1.5052941MemoryTrain:  epoch  5, batch     4 | loss: 1.5256815MemoryTrain:  epoch  5, batch     5 | loss: 1.6582830MemoryTrain:  epoch  5, batch     6 | loss: 1.5629053MemoryTrain:  epoch  5, batch     7 | loss: 1.4827744MemoryTrain:  epoch  6, batch     0 | loss: 1.3710210MemoryTrain:  epoch  6, batch     1 | loss: 1.4239964MemoryTrain:  epoch  6, batch     2 | loss: 1.4833877MemoryTrain:  epoch  6, batch     3 | loss: 1.6733315MemoryTrain:  epoch  6, batch     4 | loss: 1.6022898MemoryTrain:  epoch  6, batch     5 | loss: 1.5165855MemoryTrain:  epoch  6, batch     6 | loss: 1.5139425MemoryTrain:  epoch  6, batch     7 | loss: 1.3552942MemoryTrain:  epoch  7, batch     0 | loss: 1.5565386MemoryTrain:  epoch  7, batch     1 | loss: 1.4226798MemoryTrain:  epoch  7, batch     2 | loss: 1.2946252MemoryTrain:  epoch  7, batch     3 | loss: 1.3073381MemoryTrain:  epoch  7, batch     4 | loss: 1.2908503MemoryTrain:  epoch  7, batch     5 | loss: 1.4785252MemoryTrain:  epoch  7, batch     6 | loss: 1.4579109MemoryTrain:  epoch  7, batch     7 | loss: 1.5143119MemoryTrain:  epoch  8, batch     0 | loss: 1.5321213MemoryTrain:  epoch  8, batch     1 | loss: 1.4602844MemoryTrain:  epoch  8, batch     2 | loss: 1.4495941MemoryTrain:  epoch  8, batch     3 | loss: 1.3188560MemoryTrain:  epoch  8, batch     4 | loss: 1.3634832MemoryTrain:  epoch  8, batch     5 | loss: 1.3740742MemoryTrain:  epoch  8, batch     6 | loss: 1.2996809MemoryTrain:  epoch  8, batch     7 | loss: 1.4110868MemoryTrain:  epoch  9, batch     0 | loss: 1.3948840MemoryTrain:  epoch  9, batch     1 | loss: 1.3855196MemoryTrain:  epoch  9, batch     2 | loss: 1.3177081MemoryTrain:  epoch  9, batch     3 | loss: 1.3710201MemoryTrain:  epoch  9, batch     4 | loss: 1.3711050MemoryTrain:  epoch  9, batch     5 | loss: 1.3713790MemoryTrain:  epoch  9, batch     6 | loss: 1.2853410MemoryTrain:  epoch  9, batch     7 | loss: 1.2978549
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:    3 | acc: 100.00%,  total acc: 92.19%   [EVAL] batch:    4 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 94.79%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 95.54%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 96.09%   [EVAL] batch:    8 | acc: 62.50%,  total acc: 92.36%   [EVAL] batch:    9 | acc: 31.25%,  total acc: 86.25%   [EVAL] batch:   10 | acc: 37.50%,  total acc: 81.82%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 81.77%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 82.21%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 81.25%   
[EVAL] batch:    0 | acc: 0.00%,  total acc: 0.00%   [EVAL] batch:    1 | acc: 6.25%,  total acc: 3.12%   [EVAL] batch:    2 | acc: 6.25%,  total acc: 4.17%   [EVAL] batch:    3 | acc: 0.00%,  total acc: 3.12%   [EVAL] batch:    4 | acc: 6.25%,  total acc: 3.75%   [EVAL] batch:    5 | acc: 6.25%,  total acc: 4.17%   [EVAL] batch:    6 | acc: 12.50%,  total acc: 5.36%   [EVAL] batch:    7 | acc: 25.00%,  total acc: 7.81%   [EVAL] batch:    8 | acc: 25.00%,  total acc: 9.72%   [EVAL] batch:    9 | acc: 31.25%,  total acc: 11.88%   [EVAL] batch:   10 | acc: 31.25%,  total acc: 13.64%   [EVAL] batch:   11 | acc: 31.25%,  total acc: 15.10%   [EVAL] batch:   12 | acc: 25.00%,  total acc: 15.87%   [EVAL] batch:   13 | acc: 37.50%,  total acc: 17.41%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 20.83%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 23.05%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 26.10%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 28.12%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 29.93%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 32.81%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 36.01%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 38.64%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 41.03%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 43.23%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 45.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 47.60%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 49.31%   [EVAL] batch:   27 | acc: 87.50%,  total acc: 50.67%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 52.16%   [EVAL] batch:   29 | acc: 62.50%,  total acc: 52.50%   [EVAL] batch:   30 | acc: 68.75%,  total acc: 53.02%   [EVAL] batch:   31 | acc: 81.25%,  total acc: 53.91%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 53.41%   [EVAL] batch:   33 | acc: 0.00%,  total acc: 51.84%   [EVAL] batch:   34 | acc: 6.25%,  total acc: 50.54%   [EVAL] batch:   35 | acc: 0.00%,  total acc: 49.13%   [EVAL] batch:   36 | acc: 0.00%,  total acc: 47.80%   [EVAL] batch:   37 | acc: 68.75%,  total acc: 48.36%   [EVAL] batch:   38 | acc: 93.75%,  total acc: 49.52%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 50.78%   [EVAL] batch:   40 | acc: 87.50%,  total acc: 51.68%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 52.83%   [EVAL] batch:   42 | acc: 6.25%,  total acc: 51.74%   [EVAL] batch:   43 | acc: 18.75%,  total acc: 50.99%   [EVAL] batch:   44 | acc: 0.00%,  total acc: 49.86%   [EVAL] batch:   45 | acc: 25.00%,  total acc: 49.32%   [EVAL] batch:   46 | acc: 25.00%,  total acc: 48.80%   [EVAL] batch:   47 | acc: 56.25%,  total acc: 48.96%   [EVAL] batch:   48 | acc: 12.50%,  total acc: 48.21%   [EVAL] batch:   49 | acc: 12.50%,  total acc: 47.50%   [EVAL] batch:   50 | acc: 0.00%,  total acc: 46.57%   [EVAL] batch:   51 | acc: 6.25%,  total acc: 45.79%   [EVAL] batch:   52 | acc: 0.00%,  total acc: 44.93%   [EVAL] batch:   53 | acc: 43.75%,  total acc: 44.91%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 45.80%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 46.76%   [EVAL] batch:   56 | acc: 81.25%,  total acc: 47.37%   [EVAL] batch:   57 | acc: 75.00%,  total acc: 47.84%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 48.52%   [EVAL] batch:   59 | acc: 68.75%,  total acc: 48.85%   [EVAL] batch:   60 | acc: 0.00%,  total acc: 48.05%   [EVAL] batch:   61 | acc: 0.00%,  total acc: 47.28%   [EVAL] batch:   62 | acc: 0.00%,  total acc: 46.53%   [EVAL] batch:   63 | acc: 0.00%,  total acc: 45.80%   [EVAL] batch:   64 | acc: 0.00%,  total acc: 45.10%   [EVAL] batch:   65 | acc: 0.00%,  total acc: 44.41%   [EVAL] batch:   66 | acc: 18.75%,  total acc: 44.03%   [EVAL] batch:   67 | acc: 93.75%,  total acc: 44.76%   [EVAL] batch:   68 | acc: 43.75%,  total acc: 44.75%   [EVAL] batch:   69 | acc: 25.00%,  total acc: 44.46%   [EVAL] batch:   70 | acc: 62.50%,  total acc: 44.72%   [EVAL] batch:   71 | acc: 75.00%,  total acc: 45.14%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 45.89%   [EVAL] batch:   73 | acc: 100.00%,  total acc: 46.62%   [EVAL] batch:   74 | acc: 100.00%,  total acc: 47.33%   [EVAL] batch:   75 | acc: 100.00%,  total acc: 48.03%   [EVAL] batch:   76 | acc: 100.00%,  total acc: 48.70%   [EVAL] batch:   77 | acc: 75.00%,  total acc: 49.04%   [EVAL] batch:   78 | acc: 18.75%,  total acc: 48.66%   [EVAL] batch:   79 | acc: 6.25%,  total acc: 48.12%   [EVAL] batch:   80 | acc: 12.50%,  total acc: 47.69%   [EVAL] batch:   81 | acc: 18.75%,  total acc: 47.33%   [EVAL] batch:   82 | acc: 25.00%,  total acc: 47.06%   [EVAL] batch:   83 | acc: 50.00%,  total acc: 47.10%   [EVAL] batch:   84 | acc: 31.25%,  total acc: 46.91%   [EVAL] batch:   85 | acc: 50.00%,  total acc: 46.95%   [EVAL] batch:   86 | acc: 50.00%,  total acc: 46.98%   [EVAL] batch:   87 | acc: 31.25%,  total acc: 46.80%   [EVAL] batch:   88 | acc: 56.25%,  total acc: 46.91%   [EVAL] batch:   89 | acc: 56.25%,  total acc: 47.01%   [EVAL] batch:   90 | acc: 62.50%,  total acc: 47.18%   [EVAL] batch:   91 | acc: 56.25%,  total acc: 47.28%   [EVAL] batch:   92 | acc: 87.50%,  total acc: 47.72%   [EVAL] batch:   93 | acc: 93.75%,  total acc: 48.20%   [EVAL] batch:   94 | acc: 81.25%,  total acc: 48.55%   [EVAL] batch:   95 | acc: 50.00%,  total acc: 48.57%   [EVAL] batch:   96 | acc: 31.25%,  total acc: 48.39%   [EVAL] batch:   97 | acc: 31.25%,  total acc: 48.21%   [EVAL] batch:   98 | acc: 43.75%,  total acc: 48.17%   [EVAL] batch:   99 | acc: 81.25%,  total acc: 48.50%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 49.01%   [EVAL] batch:  101 | acc: 68.75%,  total acc: 49.20%   [EVAL] batch:  102 | acc: 68.75%,  total acc: 49.39%   [EVAL] batch:  103 | acc: 75.00%,  total acc: 49.64%   [EVAL] batch:  104 | acc: 75.00%,  total acc: 49.88%   [EVAL] batch:  105 | acc: 87.50%,  total acc: 50.24%   [EVAL] batch:  106 | acc: 50.00%,  total acc: 50.23%   [EVAL] batch:  107 | acc: 68.75%,  total acc: 50.41%   [EVAL] batch:  108 | acc: 62.50%,  total acc: 50.52%   [EVAL] batch:  109 | acc: 68.75%,  total acc: 50.68%   [EVAL] batch:  110 | acc: 43.75%,  total acc: 50.62%   [EVAL] batch:  111 | acc: 68.75%,  total acc: 50.78%   [EVAL] batch:  112 | acc: 68.75%,  total acc: 50.94%   [EVAL] batch:  113 | acc: 75.00%,  total acc: 51.15%   [EVAL] batch:  114 | acc: 68.75%,  total acc: 51.30%   [EVAL] batch:  115 | acc: 62.50%,  total acc: 51.40%   [EVAL] batch:  116 | acc: 87.50%,  total acc: 51.71%   [EVAL] batch:  117 | acc: 93.75%,  total acc: 52.07%   [EVAL] batch:  118 | acc: 87.50%,  total acc: 52.36%   [EVAL] batch:  119 | acc: 93.75%,  total acc: 52.71%   [EVAL] batch:  120 | acc: 81.25%,  total acc: 52.94%   [EVAL] batch:  121 | acc: 93.75%,  total acc: 53.28%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 53.66%   [EVAL] batch:  123 | acc: 100.00%,  total acc: 54.03%   [EVAL] batch:  124 | acc: 100.00%,  total acc: 54.40%   [EVAL] batch:  125 | acc: 100.00%,  total acc: 54.76%   [EVAL] batch:  126 | acc: 100.00%,  total acc: 55.12%   [EVAL] batch:  127 | acc: 50.00%,  total acc: 55.08%   [EVAL] batch:  128 | acc: 31.25%,  total acc: 54.89%   [EVAL] batch:  129 | acc: 50.00%,  total acc: 54.86%   [EVAL] batch:  130 | acc: 81.25%,  total acc: 55.06%   [EVAL] batch:  131 | acc: 87.50%,  total acc: 55.30%   [EVAL] batch:  132 | acc: 56.25%,  total acc: 55.31%   
cur_acc:  ['0.8636', '0.8292', '0.7768', '0.7361', '0.5682', '0.7422', '0.5865', '0.8125']
his_acc:  ['0.8636', '0.8590', '0.7941', '0.7412', '0.6687', '0.6028', '0.5567', '0.5531']
--------Round  2
seed:  300
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/test.pkl
Task_order: [7 2 3 1 5 6 0 4]
prepared data!
CurrentTrain: epoch  0, batch     0 | loss: 13.2729187CurrentTrain: epoch  0, batch     1 | loss: 13.0981321CurrentTrain: epoch  0, batch     2 | loss: 12.9869242CurrentTrain: epoch  0, batch     3 | loss: 12.9307766CurrentTrain: epoch  0, batch     4 | loss: 12.7848244CurrentTrain: epoch  0, batch     5 | loss: 12.6247587CurrentTrain: epoch  0, batch     6 | loss: 12.6284657CurrentTrain: epoch  0, batch     7 | loss: 12.4498310CurrentTrain: epoch  0, batch     8 | loss: 12.2313499CurrentTrain: epoch  0, batch     9 | loss: 12.2400990CurrentTrain: epoch  0, batch    10 | loss: 11.9040012CurrentTrain: epoch  0, batch    11 | loss: 12.0130444CurrentTrain: epoch  0, batch    12 | loss: 11.9305153CurrentTrain: epoch  0, batch    13 | loss: 11.6415520CurrentTrain: epoch  0, batch    14 | loss: 11.7852478CurrentTrain: epoch  0, batch    15 | loss: 11.5714283CurrentTrain: epoch  0, batch    16 | loss: 11.1494102CurrentTrain: epoch  0, batch    17 | loss: 11.0233898CurrentTrain: epoch  0, batch    18 | loss: 11.4784718CurrentTrain: epoch  0, batch    19 | loss: 11.1392136CurrentTrain: epoch  0, batch    20 | loss: 11.2791939CurrentTrain: epoch  0, batch    21 | loss: 10.6724634CurrentTrain: epoch  0, batch    22 | loss: 11.4209557CurrentTrain: epoch  0, batch    23 | loss: 11.0243282CurrentTrain: epoch  0, batch    24 | loss: 10.7154217CurrentTrain: epoch  0, batch    25 | loss: 11.0418873CurrentTrain: epoch  0, batch    26 | loss: 10.9799976CurrentTrain: epoch  0, batch    27 | loss: 11.3357840CurrentTrain: epoch  0, batch    28 | loss: 10.9443798CurrentTrain: epoch  0, batch    29 | loss: 10.7410183CurrentTrain: epoch  0, batch    30 | loss: 9.9014988CurrentTrain: epoch  0, batch    31 | loss: 10.5955753CurrentTrain: epoch  0, batch    32 | loss: 10.4723377CurrentTrain: epoch  0, batch    33 | loss: 10.3973656CurrentTrain: epoch  0, batch    34 | loss: 10.8015699CurrentTrain: epoch  0, batch    35 | loss: 10.2769337CurrentTrain: epoch  0, batch    36 | loss: 10.6877279CurrentTrain: epoch  0, batch    37 | loss: 9.7635670CurrentTrain: epoch  1, batch     0 | loss: 9.8443441CurrentTrain: epoch  1, batch     1 | loss: 9.9092054CurrentTrain: epoch  1, batch     2 | loss: 9.7187500CurrentTrain: epoch  1, batch     3 | loss: 9.6878490CurrentTrain: epoch  1, batch     4 | loss: 9.3893776CurrentTrain: epoch  1, batch     5 | loss: 9.7951431CurrentTrain: epoch  1, batch     6 | loss: 9.8940601CurrentTrain: epoch  1, batch     7 | loss: 9.9698553CurrentTrain: epoch  1, batch     8 | loss: 10.3835163CurrentTrain: epoch  1, batch     9 | loss: 8.9161158CurrentTrain: epoch  1, batch    10 | loss: 8.9793291CurrentTrain: epoch  1, batch    11 | loss: 9.7385654CurrentTrain: epoch  1, batch    12 | loss: 9.5112324CurrentTrain: epoch  1, batch    13 | loss: 9.0528526CurrentTrain: epoch  1, batch    14 | loss: 9.2935162CurrentTrain: epoch  1, batch    15 | loss: 9.2816772CurrentTrain: epoch  1, batch    16 | loss: 9.9029999CurrentTrain: epoch  1, batch    17 | loss: 9.0619259CurrentTrain: epoch  1, batch    18 | loss: 9.4431477CurrentTrain: epoch  1, batch    19 | loss: 8.7017841CurrentTrain: epoch  1, batch    20 | loss: 9.6351318CurrentTrain: epoch  1, batch    21 | loss: 9.5436239CurrentTrain: epoch  1, batch    22 | loss: 9.6457920CurrentTrain: epoch  1, batch    23 | loss: 8.7061110CurrentTrain: epoch  1, batch    24 | loss: 9.0428514CurrentTrain: epoch  1, batch    25 | loss: 9.5584278CurrentTrain: epoch  1, batch    26 | loss: 9.1359606CurrentTrain: epoch  1, batch    27 | loss: 8.8304749CurrentTrain: epoch  1, batch    28 | loss: 8.6152716CurrentTrain: epoch  1, batch    29 | loss: 9.4421349CurrentTrain: epoch  1, batch    30 | loss: 8.1050692CurrentTrain: epoch  1, batch    31 | loss: 8.6503258CurrentTrain: epoch  1, batch    32 | loss: 8.4269934CurrentTrain: epoch  1, batch    33 | loss: 9.4695768CurrentTrain: epoch  1, batch    34 | loss: 9.4197178CurrentTrain: epoch  1, batch    35 | loss: 8.0443592CurrentTrain: epoch  1, batch    36 | loss: 8.5728683CurrentTrain: epoch  1, batch    37 | loss: 7.8160481CurrentTrain: epoch  2, batch     0 | loss: 8.7533016CurrentTrain: epoch  2, batch     1 | loss: 8.4312353CurrentTrain: epoch  2, batch     2 | loss: 8.7446651CurrentTrain: epoch  2, batch     3 | loss: 8.1711464CurrentTrain: epoch  2, batch     4 | loss: 7.9643159CurrentTrain: epoch  2, batch     5 | loss: 7.8817806CurrentTrain: epoch  2, batch     6 | loss: 8.4610062CurrentTrain: epoch  2, batch     7 | loss: 8.1748199CurrentTrain: epoch  2, batch     8 | loss: 8.0260296CurrentTrain: epoch  2, batch     9 | loss: 8.1115093CurrentTrain: epoch  2, batch    10 | loss: 8.0282526CurrentTrain: epoch  2, batch    11 | loss: 7.8340340CurrentTrain: epoch  2, batch    12 | loss: 6.9688950CurrentTrain: epoch  2, batch    13 | loss: 8.7621841CurrentTrain: epoch  2, batch    14 | loss: 7.5754685CurrentTrain: epoch  2, batch    15 | loss: 8.7445335CurrentTrain: epoch  2, batch    16 | loss: 7.5033073CurrentTrain: epoch  2, batch    17 | loss: 8.2047682CurrentTrain: epoch  2, batch    18 | loss: 7.7604208CurrentTrain: epoch  2, batch    19 | loss: 7.1913471CurrentTrain: epoch  2, batch    20 | loss: 8.4409552CurrentTrain: epoch  2, batch    21 | loss: 7.1351557CurrentTrain: epoch  2, batch    22 | loss: 7.6377373CurrentTrain: epoch  2, batch    23 | loss: 6.9865708CurrentTrain: epoch  2, batch    24 | loss: 8.1530476CurrentTrain: epoch  2, batch    25 | loss: 8.0135717CurrentTrain: epoch  2, batch    26 | loss: 7.8945551CurrentTrain: epoch  2, batch    27 | loss: 8.2004938CurrentTrain: epoch  2, batch    28 | loss: 7.6497703CurrentTrain: epoch  2, batch    29 | loss: 7.9831500CurrentTrain: epoch  2, batch    30 | loss: 7.9779983CurrentTrain: epoch  2, batch    31 | loss: 8.4428806CurrentTrain: epoch  2, batch    32 | loss: 7.2864351CurrentTrain: epoch  2, batch    33 | loss: 7.7056980CurrentTrain: epoch  2, batch    34 | loss: 6.8864326CurrentTrain: epoch  2, batch    35 | loss: 7.4937434CurrentTrain: epoch  2, batch    36 | loss: 7.0645900CurrentTrain: epoch  2, batch    37 | loss: 6.9740076CurrentTrain: epoch  3, batch     0 | loss: 7.3104467CurrentTrain: epoch  3, batch     1 | loss: 7.7001481CurrentTrain: epoch  3, batch     2 | loss: 6.9265132CurrentTrain: epoch  3, batch     3 | loss: 6.6871414CurrentTrain: epoch  3, batch     4 | loss: 7.1485338CurrentTrain: epoch  3, batch     5 | loss: 7.0944853CurrentTrain: epoch  3, batch     6 | loss: 7.3774214CurrentTrain: epoch  3, batch     7 | loss: 7.3933029CurrentTrain: epoch  3, batch     8 | loss: 6.9749227CurrentTrain: epoch  3, batch     9 | loss: 7.5125418CurrentTrain: epoch  3, batch    10 | loss: 7.3802547CurrentTrain: epoch  3, batch    11 | loss: 7.2964988CurrentTrain: epoch  3, batch    12 | loss: 6.7940140CurrentTrain: epoch  3, batch    13 | loss: 7.1150103CurrentTrain: epoch  3, batch    14 | loss: 7.7626667CurrentTrain: epoch  3, batch    15 | loss: 6.9181590CurrentTrain: epoch  3, batch    16 | loss: 7.8491974CurrentTrain: epoch  3, batch    17 | loss: 7.4841886CurrentTrain: epoch  3, batch    18 | loss: 7.3772655CurrentTrain: epoch  3, batch    19 | loss: 7.7143416CurrentTrain: epoch  3, batch    20 | loss: 7.2488918CurrentTrain: epoch  3, batch    21 | loss: 6.7779980CurrentTrain: epoch  3, batch    22 | loss: 7.1590872CurrentTrain: epoch  3, batch    23 | loss: 7.0695391CurrentTrain: epoch  3, batch    24 | loss: 7.0328999CurrentTrain: epoch  3, batch    25 | loss: 7.1805077CurrentTrain: epoch  3, batch    26 | loss: 7.1663752CurrentTrain: epoch  3, batch    27 | loss: 7.2559981CurrentTrain: epoch  3, batch    28 | loss: 6.8432193CurrentTrain: epoch  3, batch    29 | loss: 6.7820597CurrentTrain: epoch  3, batch    30 | loss: 7.4297967CurrentTrain: epoch  3, batch    31 | loss: 5.9702511CurrentTrain: epoch  3, batch    32 | loss: 7.0947456CurrentTrain: epoch  3, batch    33 | loss: 7.6898098CurrentTrain: epoch  3, batch    34 | loss: 7.3564014CurrentTrain: epoch  3, batch    35 | loss: 7.1770744CurrentTrain: epoch  3, batch    36 | loss: 6.7913847CurrentTrain: epoch  3, batch    37 | loss: 6.1608839CurrentTrain: epoch  4, batch     0 | loss: 6.2340841CurrentTrain: epoch  4, batch     1 | loss: 7.4723740CurrentTrain: epoch  4, batch     2 | loss: 5.9821110CurrentTrain: epoch  4, batch     3 | loss: 7.1652527CurrentTrain: epoch  4, batch     4 | loss: 6.8282986CurrentTrain: epoch  4, batch     5 | loss: 6.9514551CurrentTrain: epoch  4, batch     6 | loss: 7.0240116CurrentTrain: epoch  4, batch     7 | loss: 6.6253262CurrentTrain: epoch  4, batch     8 | loss: 6.3311472CurrentTrain: epoch  4, batch     9 | loss: 5.9518881CurrentTrain: epoch  4, batch    10 | loss: 6.9957094CurrentTrain: epoch  4, batch    11 | loss: 6.7417774CurrentTrain: epoch  4, batch    12 | loss: 6.3609843CurrentTrain: epoch  4, batch    13 | loss: 6.5519347CurrentTrain: epoch  4, batch    14 | loss: 7.3589849CurrentTrain: epoch  4, batch    15 | loss: 7.2713294CurrentTrain: epoch  4, batch    16 | loss: 7.1405144CurrentTrain: epoch  4, batch    17 | loss: 6.7115817CurrentTrain: epoch  4, batch    18 | loss: 6.2975512CurrentTrain: epoch  4, batch    19 | loss: 8.0030479CurrentTrain: epoch  4, batch    20 | loss: 6.2907000CurrentTrain: epoch  4, batch    21 | loss: 7.2363482CurrentTrain: epoch  4, batch    22 | loss: 7.2809191CurrentTrain: epoch  4, batch    23 | loss: 6.8880386CurrentTrain: epoch  4, batch    24 | loss: 6.6339860CurrentTrain: epoch  4, batch    25 | loss: 7.2585697CurrentTrain: epoch  4, batch    26 | loss: 6.2982192CurrentTrain: epoch  4, batch    27 | loss: 7.6930685CurrentTrain: epoch  4, batch    28 | loss: 6.4634075CurrentTrain: epoch  4, batch    29 | loss: 5.8589306CurrentTrain: epoch  4, batch    30 | loss: 6.8917170CurrentTrain: epoch  4, batch    31 | loss: 6.2823820CurrentTrain: epoch  4, batch    32 | loss: 7.3169861CurrentTrain: epoch  4, batch    33 | loss: 7.1960201CurrentTrain: epoch  4, batch    34 | loss: 6.7068691CurrentTrain: epoch  4, batch    35 | loss: 5.7978778CurrentTrain: epoch  4, batch    36 | loss: 5.9406719CurrentTrain: epoch  4, batch    37 | loss: 6.6267805CurrentTrain: epoch  5, batch     0 | loss: 6.4688683CurrentTrain: epoch  5, batch     1 | loss: 6.6371889CurrentTrain: epoch  5, batch     2 | loss: 6.2944069CurrentTrain: epoch  5, batch     3 | loss: 7.1420450CurrentTrain: epoch  5, batch     4 | loss: 6.5281363CurrentTrain: epoch  5, batch     5 | loss: 6.2666626CurrentTrain: epoch  5, batch     6 | loss: 7.6472325CurrentTrain: epoch  5, batch     7 | loss: 7.0035372CurrentTrain: epoch  5, batch     8 | loss: 7.4036598CurrentTrain: epoch  5, batch     9 | loss: 6.6188278CurrentTrain: epoch  5, batch    10 | loss: 6.1658039CurrentTrain: epoch  5, batch    11 | loss: 6.5079570CurrentTrain: epoch  5, batch    12 | loss: 6.9540806CurrentTrain: epoch  5, batch    13 | loss: 6.0087357CurrentTrain: epoch  5, batch    14 | loss: 5.9085274CurrentTrain: epoch  5, batch    15 | loss: 6.1144600CurrentTrain: epoch  5, batch    16 | loss: 5.7899060CurrentTrain: epoch  5, batch    17 | loss: 6.9866924CurrentTrain: epoch  5, batch    18 | loss: 5.5047436CurrentTrain: epoch  5, batch    19 | loss: 6.3949413CurrentTrain: epoch  5, batch    20 | loss: 7.0990100CurrentTrain: epoch  5, batch    21 | loss: 6.5530238CurrentTrain: epoch  5, batch    22 | loss: 5.6301680CurrentTrain: epoch  5, batch    23 | loss: 6.2157607CurrentTrain: epoch  5, batch    24 | loss: 6.2313423CurrentTrain: epoch  5, batch    25 | loss: 6.1350050CurrentTrain: epoch  5, batch    26 | loss: 5.8331747CurrentTrain: epoch  5, batch    27 | loss: 6.4611402CurrentTrain: epoch  5, batch    28 | loss: 6.1895061CurrentTrain: epoch  5, batch    29 | loss: 5.9352183CurrentTrain: epoch  5, batch    30 | loss: 7.0126920CurrentTrain: epoch  5, batch    31 | loss: 6.3156862CurrentTrain: epoch  5, batch    32 | loss: 6.3037043CurrentTrain: epoch  5, batch    33 | loss: 6.6074009CurrentTrain: epoch  5, batch    34 | loss: 6.4076304CurrentTrain: epoch  5, batch    35 | loss: 6.8464766CurrentTrain: epoch  5, batch    36 | loss: 5.7232914CurrentTrain: epoch  5, batch    37 | loss: 6.0022688CurrentTrain: epoch  6, batch     0 | loss: 6.7273817CurrentTrain: epoch  6, batch     1 | loss: 6.8560271CurrentTrain: epoch  6, batch     2 | loss: 6.4572248CurrentTrain: epoch  6, batch     3 | loss: 6.2980843CurrentTrain: epoch  6, batch     4 | loss: 5.9688663CurrentTrain: epoch  6, batch     5 | loss: 5.8831158CurrentTrain: epoch  6, batch     6 | loss: 6.2014918CurrentTrain: epoch  6, batch     7 | loss: 6.3163433CurrentTrain: epoch  6, batch     8 | loss: 6.0913639CurrentTrain: epoch  6, batch     9 | loss: 5.9958520CurrentTrain: epoch  6, batch    10 | loss: 6.0782557CurrentTrain: epoch  6, batch    11 | loss: 6.3091269CurrentTrain: epoch  6, batch    12 | loss: 6.2506094CurrentTrain: epoch  6, batch    13 | loss: 6.2097440CurrentTrain: epoch  6, batch    14 | loss: 5.9949026CurrentTrain: epoch  6, batch    15 | loss: 5.8611412CurrentTrain: epoch  6, batch    16 | loss: 5.5444379CurrentTrain: epoch  6, batch    17 | loss: 5.4808216CurrentTrain: epoch  6, batch    18 | loss: 5.8432670CurrentTrain: epoch  6, batch    19 | loss: 6.7436543CurrentTrain: epoch  6, batch    20 | loss: 5.4183969CurrentTrain: epoch  6, batch    21 | loss: 5.5160513CurrentTrain: epoch  6, batch    22 | loss: 5.6177497CurrentTrain: epoch  6, batch    23 | loss: 6.4821301CurrentTrain: epoch  6, batch    24 | loss: 5.6860361CurrentTrain: epoch  6, batch    25 | loss: 5.9714088CurrentTrain: epoch  6, batch    26 | loss: 5.6218147CurrentTrain: epoch  6, batch    27 | loss: 5.5473166CurrentTrain: epoch  6, batch    28 | loss: 5.7338800CurrentTrain: epoch  6, batch    29 | loss: 5.4785380CurrentTrain: epoch  6, batch    30 | loss: 6.2613897CurrentTrain: epoch  6, batch    31 | loss: 6.6509571CurrentTrain: epoch  6, batch    32 | loss: 5.8119388CurrentTrain: epoch  6, batch    33 | loss: 6.1124468CurrentTrain: epoch  6, batch    34 | loss: 5.4741745CurrentTrain: epoch  6, batch    35 | loss: 6.4040694CurrentTrain: epoch  6, batch    36 | loss: 5.8031092CurrentTrain: epoch  6, batch    37 | loss: 5.0366430CurrentTrain: epoch  7, batch     0 | loss: 6.7259383CurrentTrain: epoch  7, batch     1 | loss: 5.4105458CurrentTrain: epoch  7, batch     2 | loss: 5.9921255CurrentTrain: epoch  7, batch     3 | loss: 5.6860437CurrentTrain: epoch  7, batch     4 | loss: 6.1123466CurrentTrain: epoch  7, batch     5 | loss: 5.5886331CurrentTrain: epoch  7, batch     6 | loss: 5.8271003CurrentTrain: epoch  7, batch     7 | loss: 6.0233889CurrentTrain: epoch  7, batch     8 | loss: 5.4702625CurrentTrain: epoch  7, batch     9 | loss: 5.5885510CurrentTrain: epoch  7, batch    10 | loss: 5.4207344CurrentTrain: epoch  7, batch    11 | loss: 5.8483620CurrentTrain: epoch  7, batch    12 | loss: 5.3456497CurrentTrain: epoch  7, batch    13 | loss: 5.2607861CurrentTrain: epoch  7, batch    14 | loss: 5.4882369CurrentTrain: epoch  7, batch    15 | loss: 5.5600863CurrentTrain: epoch  7, batch    16 | loss: 5.3023963CurrentTrain: epoch  7, batch    17 | loss: 5.7181635CurrentTrain: epoch  7, batch    18 | loss: 5.7345357CurrentTrain: epoch  7, batch    19 | loss: 5.7058392CurrentTrain: epoch  7, batch    20 | loss: 5.1808124CurrentTrain: epoch  7, batch    21 | loss: 5.4078531CurrentTrain: epoch  7, batch    22 | loss: 5.3178577CurrentTrain: epoch  7, batch    23 | loss: 5.7389441CurrentTrain: epoch  7, batch    24 | loss: 5.0430875CurrentTrain: epoch  7, batch    25 | loss: 5.4729795CurrentTrain: epoch  7, batch    26 | loss: 5.4795589CurrentTrain: epoch  7, batch    27 | loss: 5.2693892CurrentTrain: epoch  7, batch    28 | loss: 5.2108960CurrentTrain: epoch  7, batch    29 | loss: 5.3987246CurrentTrain: epoch  7, batch    30 | loss: 5.7607775CurrentTrain: epoch  7, batch    31 | loss: 5.4623485CurrentTrain: epoch  7, batch    32 | loss: 5.2190495CurrentTrain: epoch  7, batch    33 | loss: 5.7522182CurrentTrain: epoch  7, batch    34 | loss: 5.3292313CurrentTrain: epoch  7, batch    35 | loss: 5.6378756CurrentTrain: epoch  7, batch    36 | loss: 5.2850075CurrentTrain: epoch  7, batch    37 | loss: 5.8265038CurrentTrain: epoch  8, batch     0 | loss: 5.3908954CurrentTrain: epoch  8, batch     1 | loss: 5.0884328CurrentTrain: epoch  8, batch     2 | loss: 5.4362140CurrentTrain: epoch  8, batch     3 | loss: 5.0689745CurrentTrain: epoch  8, batch     4 | loss: 5.6871915CurrentTrain: epoch  8, batch     5 | loss: 5.2147684CurrentTrain: epoch  8, batch     6 | loss: 5.1845307CurrentTrain: epoch  8, batch     7 | loss: 5.4958467CurrentTrain: epoch  8, batch     8 | loss: 5.3298793CurrentTrain: epoch  8, batch     9 | loss: 5.0510597CurrentTrain: epoch  8, batch    10 | loss: 5.3911209CurrentTrain: epoch  8, batch    11 | loss: 5.1405811CurrentTrain: epoch  8, batch    12 | loss: 5.2149615CurrentTrain: epoch  8, batch    13 | loss: 5.1714725CurrentTrain: epoch  8, batch    14 | loss: 5.3503399CurrentTrain: epoch  8, batch    15 | loss: 5.5316749CurrentTrain: epoch  8, batch    16 | loss: 5.6985078CurrentTrain: epoch  8, batch    17 | loss: 5.2304029CurrentTrain: epoch  8, batch    18 | loss: 5.2065325CurrentTrain: epoch  8, batch    19 | loss: 5.2072144CurrentTrain: epoch  8, batch    20 | loss: 5.2125664CurrentTrain: epoch  8, batch    21 | loss: 5.4814658CurrentTrain: epoch  8, batch    22 | loss: 5.7058868CurrentTrain: epoch  8, batch    23 | loss: 5.8045325CurrentTrain: epoch  8, batch    24 | loss: 5.0579453CurrentTrain: epoch  8, batch    25 | loss: 5.8121281CurrentTrain: epoch  8, batch    26 | loss: 5.2355404CurrentTrain: epoch  8, batch    27 | loss: 5.2682390CurrentTrain: epoch  8, batch    28 | loss: 5.1185155CurrentTrain: epoch  8, batch    29 | loss: 5.7410626CurrentTrain: epoch  8, batch    30 | loss: 4.9522648CurrentTrain: epoch  8, batch    31 | loss: 6.2573590CurrentTrain: epoch  8, batch    32 | loss: 5.4954805CurrentTrain: epoch  8, batch    33 | loss: 5.2970362CurrentTrain: epoch  8, batch    34 | loss: 4.9508562CurrentTrain: epoch  8, batch    35 | loss: 5.2463703CurrentTrain: epoch  8, batch    36 | loss: 5.3238673CurrentTrain: epoch  8, batch    37 | loss: 5.0503383CurrentTrain: epoch  9, batch     0 | loss: 5.1409998CurrentTrain: epoch  9, batch     1 | loss: 5.1198139CurrentTrain: epoch  9, batch     2 | loss: 5.6042132CurrentTrain: epoch  9, batch     3 | loss: 5.4130211CurrentTrain: epoch  9, batch     4 | loss: 5.0982246CurrentTrain: epoch  9, batch     5 | loss: 5.1352782CurrentTrain: epoch  9, batch     6 | loss: 5.2518163CurrentTrain: epoch  9, batch     7 | loss: 4.9060726CurrentTrain: epoch  9, batch     8 | loss: 5.0513077CurrentTrain: epoch  9, batch     9 | loss: 5.2114558CurrentTrain: epoch  9, batch    10 | loss: 5.0476217CurrentTrain: epoch  9, batch    11 | loss: 5.4494610CurrentTrain: epoch  9, batch    12 | loss: 4.9302611CurrentTrain: epoch  9, batch    13 | loss: 5.1966658CurrentTrain: epoch  9, batch    14 | loss: 5.7166824CurrentTrain: epoch  9, batch    15 | loss: 5.7144051CurrentTrain: epoch  9, batch    16 | loss: 5.4918947CurrentTrain: epoch  9, batch    17 | loss: 5.2593985CurrentTrain: epoch  9, batch    18 | loss: 5.2939529CurrentTrain: epoch  9, batch    19 | loss: 5.1494522CurrentTrain: epoch  9, batch    20 | loss: 5.1341882CurrentTrain: epoch  9, batch    21 | loss: 4.9430561CurrentTrain: epoch  9, batch    22 | loss: 5.0734277CurrentTrain: epoch  9, batch    23 | loss: 5.3104525CurrentTrain: epoch  9, batch    24 | loss: 5.4860964CurrentTrain: epoch  9, batch    25 | loss: 5.1677742CurrentTrain: epoch  9, batch    26 | loss: 4.9780393CurrentTrain: epoch  9, batch    27 | loss: 5.5538378CurrentTrain: epoch  9, batch    28 | loss: 5.0687714CurrentTrain: epoch  9, batch    29 | loss: 5.0419617CurrentTrain: epoch  9, batch    30 | loss: 5.0465956CurrentTrain: epoch  9, batch    31 | loss: 4.8635206CurrentTrain: epoch  9, batch    32 | loss: 5.0204506CurrentTrain: epoch  9, batch    33 | loss: 5.1460071CurrentTrain: epoch  9, batch    34 | loss: 4.8796396CurrentTrain: epoch  9, batch    35 | loss: 5.4210567CurrentTrain: epoch  9, batch    36 | loss: 4.9295835CurrentTrain: epoch  9, batch    37 | loss: 4.9956045
#### person countries of residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person countries of residence" means a person is living in a country.
Here is an example:
Relation: person countries of residence
Context: the head of iran 's atomic energy organisation , ali akbar salehi , told domestic news agencies the new proposal had come in response to iran 's move last week to begin enriching uranium itself to the 20 percent level required for a tehran medical research rector after rejecting a previous offer .
Head Entity: ali akbar salehi
Tail Entity: iran
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person countries of residence":
Relation: person countries of residence  
Context: After years of living in the bustling city of New York, Maria Gonzalez decided to return to her hometown in Mexico, where she felt a stronger connection to her roots and family.  
Head Entity: Maria Gonzalez  
Tail Entity: Mexico  

Relation: person countries of residence  
Context: Following his successful career in London, James Smith relocated to Australia, seeking a more relaxed lifestyle by the beach.  
Head Entity: James Smith  
Tail Entity: Australia  
#### organization top members employees ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization top members employees" means an organization 's employer is a person.
Here is an example:
Relation: organization top members employees
Context: as for any gay or lesbian people who enlisted with the military this week , `` they may be told they can not join '' if the pentagon decides to enforce the law , said aubrey sarvis , executive director of the servicemembers legal defense network , a group representing service members affected by the law .
Head Entity: servicemembers legal defense network
Tail Entity: aubrey sarvis
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization top members employees":
Relation: organization top members employees  
Context: The board of directors at Tech Innovations Inc. recently appointed Sarah Johnson as the new Chief Technology Officer, highlighting her extensive experience in the tech industry and her leadership skills.  
Head Entity: Tech Innovations Inc.  
Tail Entity: Sarah Johnson  

Relation: organization top members employees  
Context: During the annual meeting, the CEO of Green Earth Solutions, Mark Thompson, announced the promotion of Emily Chen to the position of Chief Financial Officer, recognizing her contributions to the company's growth.  
Head Entity: Green Earth Solutions  
Tail Entity: Emily Chen  
#### organization member of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization member of" means an organization is the member of an organization.
Here is an example:
Relation: organization member of
Context: the policy of the international governing body for skating , the isu , has been that any skater who takes part in a competition that is not sanctioned by the isu -lrb- or one of its national governing bodies , such as the usfsa -rrb- loses eligibility to compete in future `` amateur '' events .
Head Entity: isu
Tail Entity: usfsa
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization member of":
Relation: organization member of  
Context: The International Federation of Football Association (FIFA) oversees the global governance of soccer, and it has numerous national associations as its members, including the Football Association (FA) in England.  
Head Entity: FIFA  
Tail Entity: FA  

Relation: organization member of  
Context: The World Health Organization (WHO) collaborates with various health organizations worldwide, including the Centers for Disease Control and Prevention (CDC) in the United States, to improve public health.  
Head Entity: WHO  
Tail Entity: CDC  
#### person origin ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person origin" means a person 's nationality is a country.
Here is an example:
Relation: person origin
Context: iranian atomic chief ali akbar salehi said on wednesday that tehran will address the concerns raised by the vienna group regarding the fuel deal inked by the islamic republic with brazil and turkey .
Head Entity: ali akbar salehi
Tail Entity: iranian
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person origin":
Relation: person origin  
Context: the renowned physicist albert einstein was born in ulm, germany, and later became a citizen of the united states.  
Head Entity: albert einstein  
Tail Entity: german  

Relation: person origin  
Context: the famous author chimamanda ngozi adichie was born in enugu, nigeria, and her works often reflect her nigerian heritage.  
Head Entity: chimamanda ngozi adichie  
Tail Entity: nigerian  
#### person title ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person title" means a person 's title is a title.
Here is an example:
Relation: person title
Context: `` it has to have some tempo , it has to have some energy , '' said sean ross , vice president of music and programming at edison media research , which surveys the radio industry .
Head Entity: sean ross
Tail Entity: vice president
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person title":
Relation: person title  
Context: "After years of hard work and dedication, Maria Gonzalez was promoted to Chief Financial Officer at Tech Innovations, where she has been instrumental in driving the company's financial strategy."  
Head Entity: Maria Gonzalez  
Tail Entity: Chief Financial Officer  

Relation: person title  
Context: "During the annual conference, Dr. James Lee, the lead researcher at the National Institute of Health, presented his findings on the latest advancements in medical technology."  
Head Entity: Dr. James Lee  
Tail Entity: lead researcher  
#### organization country of headquarters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization country of headquarters" means an organization is located in a country.
Here is an example:
Relation: organization country of headquarters
Context: `` japan 's growth has yet to be sustained by domestic demand alone , '' said yoshimasa maruyama , a senior economist at itochu corp. in tokyo .
Head Entity: itochu corp.
Tail Entity: japan
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization country of headquarters":
Relation: organization country of headquarters  
Context: "The headquarters of Samsung Electronics is located in Suwon, South Korea, where it has been a leader in technology innovation."  
Head Entity: Samsung Electronics  
Tail Entity: South Korea  

Relation: organization country of headquarters  
Context: "Nestlé, the world's largest food and beverage company, is headquartered in Vevey, Switzerland, where it has operated for over a century."  
Head Entity: Nestlé  
Tail Entity: Switzerland  
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 75.00%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 78.12%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 77.50%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 77.08%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 80.36%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 82.81%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 84.72%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 85.62%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 86.93%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 87.98%   [EVAL] batch:   13 | acc: 75.00%,  total acc: 87.05%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 86.25%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 84.38%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 83.82%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 82.99%   [EVAL] batch:   18 | acc: 87.50%,  total acc: 83.22%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 83.44%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 84.23%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 84.94%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 85.60%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 86.20%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 86.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 87.26%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 87.95%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 88.36%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 88.12%   [EVAL] batch:   30 | acc: 81.25%,  total acc: 87.90%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 88.28%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 86.74%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 75.00%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 78.12%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 77.50%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 77.08%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 80.36%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 82.81%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 84.72%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 85.62%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 86.93%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 87.98%   [EVAL] batch:   13 | acc: 75.00%,  total acc: 87.05%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 86.25%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 84.38%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 83.82%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 82.99%   [EVAL] batch:   18 | acc: 87.50%,  total acc: 83.22%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 83.44%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 84.23%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 84.94%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 85.60%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 86.20%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 86.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 87.26%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 87.95%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 88.36%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 88.12%   [EVAL] batch:   30 | acc: 81.25%,  total acc: 87.90%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 88.28%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 86.74%   
cur_acc:  ['0.8674']
his_acc:  ['0.8674']
CurrentTrain: epoch  0, batch     0 | loss: 6.8020306CurrentTrain: epoch  0, batch     1 | loss: 6.3914084CurrentTrain: epoch  1, batch     0 | loss: 6.1626215CurrentTrain: epoch  1, batch     1 | loss: 5.7458673CurrentTrain: epoch  2, batch     0 | loss: 5.8052049CurrentTrain: epoch  2, batch     1 | loss: 4.4382958CurrentTrain: epoch  3, batch     0 | loss: 4.3016028CurrentTrain: epoch  3, batch     1 | loss: 5.3840289CurrentTrain: epoch  4, batch     0 | loss: 4.0224748CurrentTrain: epoch  4, batch     1 | loss: 5.5129480CurrentTrain: epoch  5, batch     0 | loss: 3.8630176CurrentTrain: epoch  5, batch     1 | loss: 4.2363181CurrentTrain: epoch  6, batch     0 | loss: 3.6864223CurrentTrain: epoch  6, batch     1 | loss: 3.9866152CurrentTrain: epoch  7, batch     0 | loss: 3.4133434CurrentTrain: epoch  7, batch     1 | loss: 4.0513625CurrentTrain: epoch  8, batch     0 | loss: 3.3851902CurrentTrain: epoch  8, batch     1 | loss: 3.5326242CurrentTrain: epoch  9, batch     0 | loss: 3.4209328CurrentTrain: epoch  9, batch     1 | loss: 3.2156510
#### person cities of residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person cities of residence" means a person is living in a city.
Here is an example:
Relation: person cities of residence
Context: -lrb- now there are two : andre carson , a democrat from indianapolis , was elected to the house last year . -rrb-
Head Entity: andre carson
Tail Entity: indianapolis
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person cities of residence":
Relation: person cities of residence  
Context: -lrb- after moving to the west coast, sarah jones settled in san francisco, where she found a job in tech. -rrb-  
Head Entity: sarah jones  
Tail Entity: san francisco  

Relation: person cities of residence  
Context: -lrb- during his college years, michael smith lived in boston, enjoying the vibrant culture and history of the city. -rrb-  
Head Entity: michael smith  
Tail Entity: boston  
#### person schools attended ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person schools attended" means a person 's school is an organization.
Here is an example:
Relation: person schools attended
Context: after world war ii , he attended the university of southern california , where he became editor of a college magazine .
Head Entity: he
Tail Entity: university of southern california
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person schools attended":
Relation: person schools attended  
Context: She graduated from Harvard University with a degree in economics before pursuing her career in finance.  
Head Entity: She  
Tail Entity: Harvard University  

Relation: person schools attended  
Context: After completing his high school education, he enrolled at Stanford University to study computer science.  
Head Entity: he  
Tail Entity: Stanford University  
#### person country of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person country of death" means a person was died in a country.
Here is an example:
Relation: person country of death
Context: u.s. rep. parren mitchell , founding member of congressional black caucus , dies at 85
Head Entity: parren mitchell
Tail Entity: u.s.
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person country of death":
Relation: person country of death  
Context: renowned physicist stephen hawking passed away in cambridge, england at the age of 76  
Head Entity: stephen hawking  
Tail Entity: england  

Relation: person country of death  
Context: famous author gabriel garcia marquez died in mexico city, mexico, leaving behind a legacy of magical realism  
Head Entity: gabriel garcia marquez  
Tail Entity: mexico  
#### person children ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person children" means a person 's children is a person.
Here is an example:
Relation: person children
Context: he is survived by two stepdaughters , barbara a. hammond and brenda l. stevenson ; a stepson , michael a. taylor ; two grandchildren and one great-grandchild .
Head Entity: he
Tail Entity: brenda l. stevenson
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person children":
Relation: person children  
Context: she has three children, including her son, john, and her daughters, emily and sarah, who are both pursuing their studies.  
Head Entity: she  
Tail Entity: emily  

Relation: person children  
Context: after the passing of their father, the siblings, including jake and his sister, lila, took on the responsibility of caring for their younger brother, max.  
Head Entity: jake  
Tail Entity: lila  
#### person charges ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person charges" means a person was charged with an event.
Here is an example:
Relation: person charges
Context: ferrara said he was innocent of limoli 's slaying , but he pleaded guilty in 1992 to murder , along with racketeering charges , under a deal that sent him to prison for 22 years , rather than go to trial and risk a conviction that could lead to life in prison .
Head Entity: ferrara
Tail Entity: racketeering
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person charges":
Relation: person charges  
Context: after a lengthy investigation, the authorities announced that johnson was charged with embezzlement, a crime that could result in significant prison time if convicted.  
Head Entity: johnson  
Tail Entity: embezzlement  

Relation: person charges  
Context: the district attorney confirmed that smith has been charged with assault following the altercation that took place last month outside the nightclub.  
Head Entity: smith  
Tail Entity: assault  
Mixup data size:  104
MixupTrain:  epoch  0, batch     0 | loss: 11.3521643MixupTrain:  epoch  0, batch     1 | loss: 10.7108450MixupTrain:  epoch  0, batch     2 | loss: 9.8854949MixupTrain:  epoch  0, batch     3 | loss: 9.6296495MixupTrain:  epoch  0, batch     4 | loss: 8.9383714MixupTrain:  epoch  0, batch     5 | loss: 9.2807926MixupTrain:  epoch  0, batch     6 | loss: 5.7761314
MemoryTrain:  epoch  0, batch     0 | loss: 3.1893635MemoryTrain:  epoch  0, batch     1 | loss: 2.7772565MemoryTrain:  epoch  0, batch     2 | loss: 2.7322266MemoryTrain:  epoch  1, batch     0 | loss: 2.6616373MemoryTrain:  epoch  1, batch     1 | loss: 2.3215084MemoryTrain:  epoch  1, batch     2 | loss: 1.2565489MemoryTrain:  epoch  2, batch     0 | loss: 2.6263478MemoryTrain:  epoch  2, batch     1 | loss: 2.3215530MemoryTrain:  epoch  2, batch     2 | loss: 1.7294070MemoryTrain:  epoch  3, batch     0 | loss: 2.3550982MemoryTrain:  epoch  3, batch     1 | loss: 2.0335884MemoryTrain:  epoch  3, batch     2 | loss: 1.6783110MemoryTrain:  epoch  4, batch     0 | loss: 1.8270146MemoryTrain:  epoch  4, batch     1 | loss: 1.6522273MemoryTrain:  epoch  4, batch     2 | loss: 2.2614045MemoryTrain:  epoch  5, batch     0 | loss: 1.8600619MemoryTrain:  epoch  5, batch     1 | loss: 1.6030166MemoryTrain:  epoch  5, batch     2 | loss: 1.8192698MemoryTrain:  epoch  6, batch     0 | loss: 1.5425322MemoryTrain:  epoch  6, batch     1 | loss: 1.8429551MemoryTrain:  epoch  6, batch     2 | loss: 1.3795781MemoryTrain:  epoch  7, batch     0 | loss: 1.5091007MemoryTrain:  epoch  7, batch     1 | loss: 1.6289965MemoryTrain:  epoch  7, batch     2 | loss: 1.1549381MemoryTrain:  epoch  8, batch     0 | loss: 1.4862838MemoryTrain:  epoch  8, batch     1 | loss: 1.5992253MemoryTrain:  epoch  8, batch     2 | loss: 1.5124673MemoryTrain:  epoch  9, batch     0 | loss: 1.5886583MemoryTrain:  epoch  9, batch     1 | loss: 1.4622741MemoryTrain:  epoch  9, batch     2 | loss: 2.4947193
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 75.00%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 77.08%   [EVAL] batch:    3 | acc: 62.50%,  total acc: 73.44%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 76.25%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 76.04%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 79.46%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 80.47%   [EVAL] batch:    8 | acc: 56.25%,  total acc: 77.78%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 80.00%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 82.81%   [EVAL] batch:   12 | acc: 100.00%,  total acc: 84.13%   [EVAL] batch:   13 | acc: 100.00%,  total acc: 85.27%   [EVAL] batch:   14 | acc: 100.00%,  total acc: 86.25%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 87.11%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 87.87%   [EVAL] batch:   17 | acc: 25.00%,  total acc: 84.38%   
[EVAL] batch:    0 | acc: 56.25%,  total acc: 56.25%   [EVAL] batch:    1 | acc: 56.25%,  total acc: 56.25%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 62.50%   [EVAL] batch:    3 | acc: 50.00%,  total acc: 59.38%   [EVAL] batch:    4 | acc: 62.50%,  total acc: 60.00%   [EVAL] batch:    5 | acc: 56.25%,  total acc: 59.38%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 63.39%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 67.97%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 71.53%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 73.75%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 76.14%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 77.08%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 78.37%   [EVAL] batch:   13 | acc: 75.00%,  total acc: 78.12%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 77.92%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 76.56%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 76.47%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 75.69%   [EVAL] batch:   18 | acc: 87.50%,  total acc: 76.32%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 76.88%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 77.98%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 78.98%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 79.89%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 80.47%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 81.97%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 82.41%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 83.04%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 83.62%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 83.75%   [EVAL] batch:   30 | acc: 81.25%,  total acc: 83.67%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 84.18%   [EVAL] batch:   32 | acc: 81.25%,  total acc: 84.09%   [EVAL] batch:   33 | acc: 75.00%,  total acc: 83.82%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 83.57%   [EVAL] batch:   35 | acc: 75.00%,  total acc: 83.33%   [EVAL] batch:   36 | acc: 75.00%,  total acc: 83.11%   [EVAL] batch:   37 | acc: 81.25%,  total acc: 83.06%   [EVAL] batch:   38 | acc: 93.75%,  total acc: 83.33%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 83.75%   [EVAL] batch:   40 | acc: 50.00%,  total acc: 82.93%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 83.18%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 83.58%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 83.81%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 84.17%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 84.51%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 84.84%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 85.16%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 85.46%   [EVAL] batch:   49 | acc: 62.50%,  total acc: 85.00%   
cur_acc:  ['0.8674', '0.8438']
his_acc:  ['0.8674', '0.8500']
CurrentTrain: epoch  0, batch     0 | loss: 6.1462135CurrentTrain: epoch  0, batch     1 | loss: 6.2286811CurrentTrain: epoch  1, batch     0 | loss: 5.6766882CurrentTrain: epoch  1, batch     1 | loss: 5.6529231CurrentTrain: epoch  2, batch     0 | loss: 5.1482401CurrentTrain: epoch  2, batch     1 | loss: 4.6804380CurrentTrain: epoch  3, batch     0 | loss: 4.1770964CurrentTrain: epoch  3, batch     1 | loss: 4.5175571CurrentTrain: epoch  4, batch     0 | loss: 3.7143435CurrentTrain: epoch  4, batch     1 | loss: 4.0790124CurrentTrain: epoch  5, batch     0 | loss: 3.7256930CurrentTrain: epoch  5, batch     1 | loss: 2.9336782CurrentTrain: epoch  6, batch     0 | loss: 3.4790215CurrentTrain: epoch  6, batch     1 | loss: 2.9389474CurrentTrain: epoch  7, batch     0 | loss: 2.8217819CurrentTrain: epoch  7, batch     1 | loss: 2.9048314CurrentTrain: epoch  8, batch     0 | loss: 3.1320074CurrentTrain: epoch  8, batch     1 | loss: 2.7942019CurrentTrain: epoch  9, batch     0 | loss: 2.8349791CurrentTrain: epoch  9, batch     1 | loss: 2.5699925
#### person date of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person date of birth" means a person was born in a date.
Here is an example:
Relation: person date of birth
Context: kirkaldy , born irene morgan in baltimore , maryland , in 1917 , was arrested in 1944 for refusing to give up her seat on a greyhound bus heading from gloucester to baltimore , and for resisting arrest .
Head Entity: irene morgan
Tail Entity: 1917
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person date of birth":
Relation: person date of birth  
Context: the famous physicist albert einstein was born in ulm, germany, on march 14, 1879, and later developed the theory of relativity.  
Head Entity: albert einstein  
Tail Entity: march 14, 1879  

Relation: person date of birth  
Context: the renowned author jane austen was born on december 16, 1775, in steventon, hampshire, england, and is best known for her novels exploring the British landed gentry.  
Head Entity: jane austen  
Tail Entity: december 16, 1775  
#### person stateorprovince of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person stateorprovince of birth" means a person was died in a state or province.
Here is an example:
Relation: person stateorprovince of birth
Context: joseph simpson farland was born on aug 11 , 1914 , in clarksburg , wva , the only child of richard and grace simpson farland .
Head Entity: joseph simpson farland
Tail Entity: wva
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person stateorprovince of birth":
Relation: person stateorprovince of birth  
Context: martha stewart was born on august 3, 1941, in jersey city, new jersey, to parents of polish descent.  
Head Entity: martha stewart  
Tail Entity: new jersey  

Relation: person stateorprovince of birth  
Context: barack obama was born on august 4, 1961, in honolulu, hawaii, where he spent most of his childhood.  
Head Entity: barack obama  
Tail Entity: hawaii  
#### person parents ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person parents" means a person 's parent is a person.
Here is an example:
Relation: person parents
Context: lynne spears told the court that lutfi had treated her daughter like a hostage in her own home , drugged her and took over her finances .
Head Entity: her
Tail Entity: lynne spears
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person parents":
Relation: person parents  
Context: During the family reunion, Sarah introduced her father, John, to her friends, highlighting how much he has influenced her life choices.  
Head Entity: her father  
Tail Entity: Sarah  

Relation: person parents  
Context: After the ceremony, Emily shared stories about her mother, who had always been her biggest supporter and role model throughout her life.  
Head Entity: her mother  
Tail Entity: Emily  
#### person employee of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person employee of" means a person is an employee of an organization.
Here is an example:
Relation: person employee of
Context: seeking revenge , axel reunites with old pal sgt. billy rosewood -lrb- judge reinhold -rrb- and jon flint -lrb- hector elizondo -rrb- of the beverly hills police department .
Head Entity: hector elizondo
Tail Entity: beverly hills police department
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person employee of":
Relation: person employee of  
Context: After years of hard work and dedication, Maria finally received a promotion at the tech startup where she has been developing innovative software solutions.  
Head Entity: Maria  
Tail Entity: tech startup  

Relation: person employee of  
Context: John has been a loyal member of the marketing team at Global Corp for over a decade, contributing to numerous successful campaigns.  
Head Entity: John  
Tail Entity: Global Corp  
#### person stateorprovince of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person stateorprovince of death" means a person was died in a state or province.
Here is an example:
Relation: person stateorprovince of death
Context: irene morgan kirkaldy , 90 , who died of alzheimer 's disease aug. 10 at her home in gloucester , va. , quietly changed history in 1944 when she refused to give up her seat on a crowded greyhound bus to a white couple .
Head Entity: irene morgan kirkaldy
Tail Entity: va.
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person stateorprovince of death":
Relation: person stateorprovince of death  
Context: john smith, a renowned author, passed away on march 5 in his residence located in los angeles, california, leaving behind a legacy of literary works that inspired many.  
Head Entity: john smith  
Tail Entity: california  

Relation: person stateorprovince of death  
Context: the famous musician, elena rodriguez, died tragically in a car accident on july 12 while traveling through the scenic routes of oregon, where she had spent her childhood.  
Head Entity: elena rodriguez  
Tail Entity: oregon  
Mixup data size:  134
MixupTrain:  epoch  0, batch     0 | loss: 6.2965822MixupTrain:  epoch  0, batch     1 | loss: 5.7479737MixupTrain:  epoch  0, batch     2 | loss: 8.7814428MixupTrain:  epoch  0, batch     3 | loss: 4.8499409MixupTrain:  epoch  0, batch     4 | loss: 5.8869844MixupTrain:  epoch  0, batch     5 | loss: 5.3315757MixupTrain:  epoch  0, batch     6 | loss: 5.9616610MixupTrain:  epoch  0, batch     7 | loss: 5.7527632MixupTrain:  epoch  0, batch     8 | loss: 4.0300858
MemoryTrain:  epoch  0, batch     0 | loss: 2.2067747MemoryTrain:  epoch  0, batch     1 | loss: 2.7238822MemoryTrain:  epoch  0, batch     2 | loss: 3.7544861MemoryTrain:  epoch  1, batch     0 | loss: 3.4949777MemoryTrain:  epoch  1, batch     1 | loss: 2.3540454MemoryTrain:  epoch  1, batch     2 | loss: 2.5504298MemoryTrain:  epoch  2, batch     0 | loss: 2.4311728MemoryTrain:  epoch  2, batch     1 | loss: 2.2606552MemoryTrain:  epoch  2, batch     2 | loss: 1.8902702MemoryTrain:  epoch  3, batch     0 | loss: 2.1287463MemoryTrain:  epoch  3, batch     1 | loss: 1.6481974MemoryTrain:  epoch  3, batch     2 | loss: 1.8644384MemoryTrain:  epoch  4, batch     0 | loss: 1.8712845MemoryTrain:  epoch  4, batch     1 | loss: 1.6727051MemoryTrain:  epoch  4, batch     2 | loss: 1.7709627MemoryTrain:  epoch  5, batch     0 | loss: 1.9428153MemoryTrain:  epoch  5, batch     1 | loss: 1.4912661MemoryTrain:  epoch  5, batch     2 | loss: 1.8028200MemoryTrain:  epoch  6, batch     0 | loss: 1.6577139MemoryTrain:  epoch  6, batch     1 | loss: 1.6579444MemoryTrain:  epoch  6, batch     2 | loss: 1.6782764MemoryTrain:  epoch  7, batch     0 | loss: 1.5871048MemoryTrain:  epoch  7, batch     1 | loss: 1.7500230MemoryTrain:  epoch  7, batch     2 | loss: 1.5147300MemoryTrain:  epoch  8, batch     0 | loss: 1.4106284MemoryTrain:  epoch  8, batch     1 | loss: 1.4809158MemoryTrain:  epoch  8, batch     2 | loss: 1.5507537MemoryTrain:  epoch  9, batch     0 | loss: 1.4008131MemoryTrain:  epoch  9, batch     1 | loss: 1.4922653MemoryTrain:  epoch  9, batch     2 | loss: 1.5001328
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 31.25%,  total acc: 56.25%   [EVAL] batch:    2 | acc: 43.75%,  total acc: 52.08%   [EVAL] batch:    3 | acc: 56.25%,  total acc: 53.12%   [EVAL] batch:    4 | acc: 56.25%,  total acc: 53.75%   [EVAL] batch:    5 | acc: 25.00%,  total acc: 48.96%   [EVAL] batch:    6 | acc: 50.00%,  total acc: 49.11%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 54.69%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 58.33%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 60.62%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 62.50%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 64.06%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 65.87%   [EVAL] batch:   13 | acc: 43.75%,  total acc: 64.29%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 56.25%,  total acc: 65.62%   [EVAL] batch:    2 | acc: 68.75%,  total acc: 66.67%   [EVAL] batch:    3 | acc: 56.25%,  total acc: 64.06%   [EVAL] batch:    4 | acc: 62.50%,  total acc: 63.75%   [EVAL] batch:    5 | acc: 50.00%,  total acc: 61.46%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 66.07%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 70.31%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 72.92%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 75.00%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 77.27%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 78.12%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 78.85%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 78.12%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 77.50%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 76.17%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 76.10%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 75.35%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 74.67%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 75.31%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 76.49%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 77.27%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 78.26%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 78.91%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 79.75%   [EVAL] batch:   25 | acc: 93.75%,  total acc: 80.29%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 80.79%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 81.47%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 82.11%   [EVAL] batch:   29 | acc: 93.75%,  total acc: 82.50%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 83.06%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 83.59%   [EVAL] batch:   32 | acc: 68.75%,  total acc: 83.14%   [EVAL] batch:   33 | acc: 68.75%,  total acc: 82.72%   [EVAL] batch:   34 | acc: 68.75%,  total acc: 82.32%   [EVAL] batch:   35 | acc: 50.00%,  total acc: 81.42%   [EVAL] batch:   36 | acc: 50.00%,  total acc: 80.57%   [EVAL] batch:   37 | acc: 50.00%,  total acc: 79.77%   [EVAL] batch:   38 | acc: 62.50%,  total acc: 79.33%   [EVAL] batch:   39 | acc: 87.50%,  total acc: 79.53%   [EVAL] batch:   40 | acc: 37.50%,  total acc: 78.51%   [EVAL] batch:   41 | acc: 62.50%,  total acc: 78.12%   [EVAL] batch:   42 | acc: 68.75%,  total acc: 77.91%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 77.98%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 78.47%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 78.94%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 79.39%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 79.82%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 80.23%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 80.50%   [EVAL] batch:   50 | acc: 68.75%,  total acc: 80.27%   [EVAL] batch:   51 | acc: 37.50%,  total acc: 79.45%   [EVAL] batch:   52 | acc: 43.75%,  total acc: 78.77%   [EVAL] batch:   53 | acc: 43.75%,  total acc: 78.12%   [EVAL] batch:   54 | acc: 56.25%,  total acc: 77.73%   [EVAL] batch:   55 | acc: 25.00%,  total acc: 76.79%   [EVAL] batch:   56 | acc: 75.00%,  total acc: 76.75%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 77.05%   [EVAL] batch:   58 | acc: 75.00%,  total acc: 77.01%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 77.29%   [EVAL] batch:   60 | acc: 68.75%,  total acc: 77.15%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 77.42%   [EVAL] batch:   62 | acc: 75.00%,  total acc: 77.38%   [EVAL] batch:   63 | acc: 18.75%,  total acc: 76.46%   
cur_acc:  ['0.8674', '0.8438', '0.6429']
his_acc:  ['0.8674', '0.8500', '0.7646']
CurrentTrain: epoch  0, batch     0 | loss: 4.9346013CurrentTrain: epoch  0, batch     1 | loss: 4.4757652CurrentTrain: epoch  1, batch     0 | loss: 3.7058518CurrentTrain: epoch  1, batch     1 | loss: 3.3678513CurrentTrain: epoch  2, batch     0 | loss: 2.9533312CurrentTrain: epoch  2, batch     1 | loss: 3.1323695CurrentTrain: epoch  3, batch     0 | loss: 2.7228661CurrentTrain: epoch  3, batch     1 | loss: 2.7902615CurrentTrain: epoch  4, batch     0 | loss: 2.7495494CurrentTrain: epoch  4, batch     1 | loss: 2.3883145CurrentTrain: epoch  5, batch     0 | loss: 2.3973694CurrentTrain: epoch  5, batch     1 | loss: 2.1583054CurrentTrain: epoch  6, batch     0 | loss: 2.2365782CurrentTrain: epoch  6, batch     1 | loss: 2.2914262CurrentTrain: epoch  7, batch     0 | loss: 2.2911019CurrentTrain: epoch  7, batch     1 | loss: 2.0571330CurrentTrain: epoch  8, batch     0 | loss: 2.1356428CurrentTrain: epoch  8, batch     1 | loss: 2.0689766CurrentTrain: epoch  9, batch     0 | loss: 2.0950217CurrentTrain: epoch  9, batch     1 | loss: 1.9811474
#### organization founded ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization founded" means an organization was found in a date.
Here is an example:
Relation: organization founded
Context: pandit worked at the brokerage morgan stanley for about 11 years until 2005 , when he and some morgan stanley colleagues quit and later founded the hedge fund old lane partners .
Head Entity: old lane partners
Tail Entity: 2005
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization founded":
Relation: organization founded  
Context: In 1998, a group of engineers and entrepreneurs came together to establish the tech startup, Innovatech Solutions, which has since become a leader in software development.  
Head Entity: Innovatech Solutions  
Tail Entity: 1998  

Relation: organization founded  
Context: The non-profit organization Green Earth Initiative was created in 2010 to promote environmental awareness and sustainability practices across communities.  
Head Entity: Green Earth Initiative  
Tail Entity: 2010  
#### person age ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person age" means a person 's age is a number.
Here is an example:
Relation: person age
Context: susan boyle is 48 years old now .
Head Entity: susan boyle
Tail Entity: 48
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person age":
Relation: person age  
Context: john smith turned 30 last week.  
Head Entity: john smith  
Tail Entity: 30  

Relation: person age  
Context: the famous actor will celebrate his 45th birthday next month.  
Head Entity: the famous actor  
Tail Entity: 45  
#### person city of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person city of birth" means a person was born in a city.
Here is an example:
Relation: person city of birth
Context: rothman was born in san francisco in 1932 in an orthodox jewish family .
Head Entity: rothman
Tail Entity: san francisco
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person city of birth":
Relation: person city of birth  
Context: elena was born in barcelona and spent her childhood there before moving to madrid.  
Head Entity: elena  
Tail Entity: barcelona  

Relation: person city of birth  
Context: during the summer of 1985, michael was born in new orleans, a city known for its vibrant culture.  
Head Entity: michael  
Tail Entity: new orleans  
#### organization members ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization members" means an organization 's member is an organization.
Here is an example:
Relation: organization members
Context: it was berger who made clarke a member of the white house principals committee when it met to discuss terrorist threats , allowing an otherwise middle-ranking nsc bureaucrat to treat tenet and secretary of state madeleine albright as equals -lrb- which the empire-building clarke was pleased to do -rrb- .
Head Entity: nsc
Tail Entity: white house principals committee
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization members":
Relation: organization members  
Context: The board of directors of the tech startup includes several prominent figures from the industry, such as the CEO of Innovatech, who has been instrumental in guiding the company’s strategic direction.  
Head Entity: tech startup  
Tail Entity: Innovatech  

Relation: organization members  
Context: During the annual conference, the president of the environmental advocacy group announced the inclusion of several new members, including representatives from various local NGOs dedicated to conservation efforts.  
Head Entity: environmental advocacy group  
Tail Entity: local NGOs  
#### person religion ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person religion" means an person is the member of a religion.
Here is an example:
Relation: person religion
Context: the pope defended his action on the grounds that he could not refuse an audience to a head of state from a country with a strong catholic tradition unless he had clear-cut proof of the allegations against him .
Head Entity: he
Tail Entity: catholic
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person religion":
Relation: person religion  
Context: After years of study and reflection, Maria decided to embrace Buddhism, finding peace and purpose in its teachings.  
Head Entity: Maria  
Tail Entity: Buddhism  

Relation: person religion  
Context: During the festival, Ahmed proudly wore his traditional attire, celebrating his deep connection to Islam and its rich cultural heritage.  
Head Entity: Ahmed  
Tail Entity: Islam  
Mixup data size:  165
MixupTrain:  epoch  0, batch     0 | loss: 4.8605418MixupTrain:  epoch  0, batch     1 | loss: 5.0979199MixupTrain:  epoch  0, batch     2 | loss: 3.9920515MixupTrain:  epoch  0, batch     3 | loss: 4.2563737MixupTrain:  epoch  0, batch     4 | loss: 4.2710187MixupTrain:  epoch  0, batch     5 | loss: 3.4022033MixupTrain:  epoch  0, batch     6 | loss: 5.2060055MixupTrain:  epoch  0, batch     7 | loss: 3.3396193MixupTrain:  epoch  0, batch     8 | loss: 3.6506632MixupTrain:  epoch  0, batch     9 | loss: 2.8760807MixupTrain:  epoch  0, batch    10 | loss: 2.4769318
MemoryTrain:  epoch  0, batch     0 | loss: 2.1473222MemoryTrain:  epoch  0, batch     1 | loss: 3.2163064MemoryTrain:  epoch  0, batch     2 | loss: 3.5628080MemoryTrain:  epoch  0, batch     3 | loss: 3.4201512MemoryTrain:  epoch  1, batch     0 | loss: 2.6663864MemoryTrain:  epoch  1, batch     1 | loss: 3.0790076MemoryTrain:  epoch  1, batch     2 | loss: 2.3439059MemoryTrain:  epoch  1, batch     3 | loss: 2.5489252MemoryTrain:  epoch  2, batch     0 | loss: 3.2102313MemoryTrain:  epoch  2, batch     1 | loss: 1.4425546MemoryTrain:  epoch  2, batch     2 | loss: 2.8274140MemoryTrain:  epoch  2, batch     3 | loss: 1.8247274MemoryTrain:  epoch  3, batch     0 | loss: 2.2921090MemoryTrain:  epoch  3, batch     1 | loss: 1.8210675MemoryTrain:  epoch  3, batch     2 | loss: 2.1196091MemoryTrain:  epoch  3, batch     3 | loss: 2.1127481MemoryTrain:  epoch  4, batch     0 | loss: 2.3869586MemoryTrain:  epoch  4, batch     1 | loss: 2.1029146MemoryTrain:  epoch  4, batch     2 | loss: 1.6944671MemoryTrain:  epoch  4, batch     3 | loss: 1.4175903MemoryTrain:  epoch  5, batch     0 | loss: 1.4488626MemoryTrain:  epoch  5, batch     1 | loss: 1.8358889MemoryTrain:  epoch  5, batch     2 | loss: 2.1057594MemoryTrain:  epoch  5, batch     3 | loss: 1.8972876MemoryTrain:  epoch  6, batch     0 | loss: 2.0987532MemoryTrain:  epoch  6, batch     1 | loss: 1.9658650MemoryTrain:  epoch  6, batch     2 | loss: 1.5965364MemoryTrain:  epoch  6, batch     3 | loss: 1.4548737MemoryTrain:  epoch  7, batch     0 | loss: 1.4674547MemoryTrain:  epoch  7, batch     1 | loss: 1.7125205MemoryTrain:  epoch  7, batch     2 | loss: 2.3054800MemoryTrain:  epoch  7, batch     3 | loss: 1.5482006MemoryTrain:  epoch  8, batch     0 | loss: 1.4547879MemoryTrain:  epoch  8, batch     1 | loss: 1.6246428MemoryTrain:  epoch  8, batch     2 | loss: 1.3839616MemoryTrain:  epoch  8, batch     3 | loss: 1.9021003MemoryTrain:  epoch  9, batch     0 | loss: 1.3861403MemoryTrain:  epoch  9, batch     1 | loss: 1.9418027MemoryTrain:  epoch  9, batch     2 | loss: 1.4297695MemoryTrain:  epoch  9, batch     3 | loss: 1.4797130
[EVAL] batch:    0 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    1 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    2 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    3 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    4 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 97.92%   [EVAL] batch:    9 | acc: 12.50%,  total acc: 89.38%   [EVAL] batch:   10 | acc: 31.25%,  total acc: 84.09%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 83.85%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 83.17%   [EVAL] batch:   13 | acc: 56.25%,  total acc: 81.25%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 68.75%,  total acc: 71.88%   [EVAL] batch:    2 | acc: 68.75%,  total acc: 70.83%   [EVAL] batch:    3 | acc: 56.25%,  total acc: 67.19%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 68.75%   [EVAL] batch:    5 | acc: 50.00%,  total acc: 65.62%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 69.64%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 73.44%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 76.39%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 78.12%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 80.11%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 80.73%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 80.29%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 79.46%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 78.75%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 77.34%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 77.21%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 76.39%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 75.66%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 76.25%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 77.38%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 78.12%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 78.80%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 79.43%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 80.25%   [EVAL] batch:   25 | acc: 93.75%,  total acc: 80.77%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 81.92%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 82.54%   [EVAL] batch:   29 | acc: 93.75%,  total acc: 82.92%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 83.47%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 83.98%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 82.58%   [EVAL] batch:   33 | acc: 0.00%,  total acc: 80.15%   [EVAL] batch:   34 | acc: 0.00%,  total acc: 77.86%   [EVAL] batch:   35 | acc: 0.00%,  total acc: 75.69%   [EVAL] batch:   36 | acc: 0.00%,  total acc: 73.65%   [EVAL] batch:   37 | acc: 0.00%,  total acc: 71.71%   [EVAL] batch:   38 | acc: 25.00%,  total acc: 70.51%   [EVAL] batch:   39 | acc: 87.50%,  total acc: 70.94%   [EVAL] batch:   40 | acc: 31.25%,  total acc: 69.97%   [EVAL] batch:   41 | acc: 62.50%,  total acc: 69.79%   [EVAL] batch:   42 | acc: 75.00%,  total acc: 69.91%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 70.60%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 71.25%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 71.88%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 72.47%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 73.05%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 73.60%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 74.12%   [EVAL] batch:   50 | acc: 68.75%,  total acc: 74.02%   [EVAL] batch:   51 | acc: 25.00%,  total acc: 73.08%   [EVAL] batch:   52 | acc: 43.75%,  total acc: 72.52%   [EVAL] batch:   53 | acc: 37.50%,  total acc: 71.88%   [EVAL] batch:   54 | acc: 37.50%,  total acc: 71.25%   [EVAL] batch:   55 | acc: 12.50%,  total acc: 70.20%   [EVAL] batch:   56 | acc: 68.75%,  total acc: 70.18%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 70.58%   [EVAL] batch:   58 | acc: 75.00%,  total acc: 70.66%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 71.04%   [EVAL] batch:   60 | acc: 62.50%,  total acc: 70.90%   [EVAL] batch:   61 | acc: 81.25%,  total acc: 71.07%   [EVAL] batch:   62 | acc: 93.75%,  total acc: 71.43%   [EVAL] batch:   63 | acc: 87.50%,  total acc: 71.68%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 72.12%   [EVAL] batch:   65 | acc: 100.00%,  total acc: 72.54%   [EVAL] batch:   66 | acc: 100.00%,  total acc: 72.95%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 73.35%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 73.73%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 74.11%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 74.47%   [EVAL] batch:   71 | acc: 100.00%,  total acc: 74.83%   [EVAL] batch:   72 | acc: 18.75%,  total acc: 74.06%   [EVAL] batch:   73 | acc: 18.75%,  total acc: 73.31%   [EVAL] batch:   74 | acc: 68.75%,  total acc: 73.25%   [EVAL] batch:   75 | acc: 87.50%,  total acc: 73.44%   [EVAL] batch:   76 | acc: 68.75%,  total acc: 73.38%   [EVAL] batch:   77 | acc: 6.25%,  total acc: 72.52%   
cur_acc:  ['0.8674', '0.8438', '0.6429', '0.8125']
his_acc:  ['0.8674', '0.8500', '0.7646', '0.7252']
CurrentTrain: epoch  0, batch     0 | loss: 4.5991597CurrentTrain: epoch  0, batch     1 | loss: 5.0272174CurrentTrain: epoch  1, batch     0 | loss: 3.6050448CurrentTrain: epoch  1, batch     1 | loss: 2.8113520CurrentTrain: epoch  2, batch     0 | loss: 2.7884221CurrentTrain: epoch  2, batch     1 | loss: 2.6471350CurrentTrain: epoch  3, batch     0 | loss: 2.5509610CurrentTrain: epoch  3, batch     1 | loss: 2.4192269CurrentTrain: epoch  4, batch     0 | loss: 2.6526399CurrentTrain: epoch  4, batch     1 | loss: 2.4880760CurrentTrain: epoch  5, batch     0 | loss: 2.6023421CurrentTrain: epoch  5, batch     1 | loss: 2.0766547CurrentTrain: epoch  6, batch     0 | loss: 2.1551368CurrentTrain: epoch  6, batch     1 | loss: 1.9578190CurrentTrain: epoch  7, batch     0 | loss: 2.0751748CurrentTrain: epoch  7, batch     1 | loss: 1.9432549CurrentTrain: epoch  8, batch     0 | loss: 1.9322459CurrentTrain: epoch  8, batch     1 | loss: 1.9363419CurrentTrain: epoch  9, batch     0 | loss: 1.9924462CurrentTrain: epoch  9, batch     1 | loss: 1.8896862
#### person cause of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person cause of death" means a person was died of an event.
Here is an example:
Relation: person cause of death
Context: pamela gardner ahearn , who served nine years as chief of protocol at the us house of representatives after earlier experience with the state department 's office of protocol and as elizabeth taylor 's executive assistant , died march 26 of a heart attack at her home in alexandria , va .
Head Entity: pamela gardner ahearn
Tail Entity: heart attack
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person cause of death":
Relation: person cause of death  
Context: john smith, a renowned scientist known for his groundbreaking research in genetics, passed away on july 15 due to complications from pneumonia while receiving treatment at a local hospital.  
Head Entity: john smith  
Tail Entity: pneumonia  

Relation: person cause of death  
Context: the famous actor, robert jones, tragically lost his life in a car accident on february 10, leaving behind a legacy of memorable performances and a grieving family.  
Head Entity: robert jones  
Tail Entity: car accident  
#### organization political religious affiliation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization political religious affiliation" means an organization is the member of a religion.
Here is an example:
Relation: organization political religious affiliation
Context: the militia has also suffered from allegations of being involved in clashes with rival shiite militias , particularly the badr corp , the military wing linked to supreme iraqi islamic council , the party led by powerful politician abdel aziz al-hakim .
Head Entity: badr corp
Tail Entity: shiite
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization political religious affiliation":
Relation: organization political religious affiliation  
Context: The Christian Democratic Union has been a significant player in German politics, often aligning itself with various religious groups to promote policies that reflect Christian values.  
Head Entity: Christian Democratic Union  
Tail Entity: Christian  

Relation: organization political religious affiliation  
Context: The Islamic Society of North America has been actively involved in advocating for Muslim rights and representation in the political landscape of the United States.  
Head Entity: Islamic Society of North America  
Tail Entity: Muslim  
#### organization stateorprovince of headquarters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization stateorprovince of headquarters" means an organization is located in a state or province.
Here is an example:
Relation: organization stateorprovince of headquarters
Context: based in armonk , new york , mbia insures $ 670 billion -lrb- euro452 .18 billion -rrb- in debt .
Head Entity: mbia
Tail Entity: new york
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization stateorprovince of headquarters":
Relation: organization stateorprovince of headquarters  
Context: the tech giant apple inc. has its headquarters in cupertino, california, where it develops innovative products.  
Head Entity: apple inc.  
Tail Entity: california  

Relation: organization stateorprovince of headquarters  
Context: the multinational corporation samsung electronics is headquartered in suwon, south korea, and is a leader in consumer electronics.  
Head Entity: samsung electronics  
Tail Entity: south korea  
#### person other family ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person other family" means a person 's relative is a person.
Here is an example:
Relation: person other family
Context: parren mitchell 's sister-in-law , juanita jackson mitchell , was the long - time head and legal counsel of the maryland naacp .
Head Entity: parren mitchell
Tail Entity: juanita jackson mitchell
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person other family":
Relation: person other family  
Context: During the family reunion, it was revealed that Sarah's cousin, Emily, had recently graduated from college with honors.  
Head Entity: Sarah  
Tail Entity: Emily  

Relation: person other family  
Context: Michael often reminisces about the summer vacations spent at his grandmother's house with his aunt, Linda, who always baked the best cookies.  
Head Entity: Michael  
Tail Entity: Linda  
#### person city of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person city of death" means a person was died in a city.
Here is an example:
Relation: person city of death
Context: irene morgan kirkaldy , 90 , who died of alzheimer 's disease aug. 10 at her home in gloucester , va. , quietly changed history in 1944 when she refused to give up her seat on a crowded greyhound bus to a white couple .
Head Entity: her
Tail Entity: gloucester
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person city of death":
Relation: person city of death  
Context: john smith, a renowned author, passed away on march 5 in the vibrant city of new orleans, where he spent his final years writing his last novel.  
Head Entity: john smith  
Tail Entity: new orleans  

Relation: person city of death  
Context: after a long battle with cancer, elizabeth taylor died on march 23 in the luxurious city of los angeles, surrounded by her family and friends.  
Head Entity: elizabeth taylor  
Tail Entity: los angeles  
Mixup data size:  194
MixupTrain:  epoch  0, batch     0 | loss: 3.6009034MixupTrain:  epoch  0, batch     1 | loss: 3.2501339MixupTrain:  epoch  0, batch     2 | loss: 3.4131135MixupTrain:  epoch  0, batch     3 | loss: 2.8443322MixupTrain:  epoch  0, batch     4 | loss: 3.2359342MixupTrain:  epoch  0, batch     5 | loss: 3.2342177MixupTrain:  epoch  0, batch     6 | loss: 3.3100844MixupTrain:  epoch  0, batch     7 | loss: 2.7295970MixupTrain:  epoch  0, batch     8 | loss: 3.2047007MixupTrain:  epoch  0, batch     9 | loss: 2.3300948MixupTrain:  epoch  0, batch    10 | loss: 2.7709656MixupTrain:  epoch  0, batch    11 | loss: 2.8151970MixupTrain:  epoch  0, batch    12 | loss: 3.4806309
MemoryTrain:  epoch  0, batch     0 | loss: 2.6473444MemoryTrain:  epoch  0, batch     1 | loss: 3.7648048MemoryTrain:  epoch  0, batch     2 | loss: 2.7946689MemoryTrain:  epoch  0, batch     3 | loss: 2.9324284MemoryTrain:  epoch  0, batch     4 | loss: 2.3176322MemoryTrain:  epoch  1, batch     0 | loss: 2.8330727MemoryTrain:  epoch  1, batch     1 | loss: 2.3099260MemoryTrain:  epoch  1, batch     2 | loss: 2.4886880MemoryTrain:  epoch  1, batch     3 | loss: 2.6740909MemoryTrain:  epoch  1, batch     4 | loss: 3.0425177MemoryTrain:  epoch  2, batch     0 | loss: 3.2576447MemoryTrain:  epoch  2, batch     1 | loss: 2.0456274MemoryTrain:  epoch  2, batch     2 | loss: 1.9997728MemoryTrain:  epoch  2, batch     3 | loss: 2.1499238MemoryTrain:  epoch  2, batch     4 | loss: 1.9675436MemoryTrain:  epoch  3, batch     0 | loss: 1.8274286MemoryTrain:  epoch  3, batch     1 | loss: 1.7190611MemoryTrain:  epoch  3, batch     2 | loss: 1.6356268MemoryTrain:  epoch  3, batch     3 | loss: 2.1260979MemoryTrain:  epoch  3, batch     4 | loss: 3.3213971MemoryTrain:  epoch  4, batch     0 | loss: 1.9087465MemoryTrain:  epoch  4, batch     1 | loss: 2.0103393MemoryTrain:  epoch  4, batch     2 | loss: 1.9174186MemoryTrain:  epoch  4, batch     3 | loss: 2.3125091MemoryTrain:  epoch  4, batch     4 | loss: 1.8375245MemoryTrain:  epoch  5, batch     0 | loss: 1.7527523MemoryTrain:  epoch  5, batch     1 | loss: 2.3091590MemoryTrain:  epoch  5, batch     2 | loss: 1.6221671MemoryTrain:  epoch  5, batch     3 | loss: 1.7000439MemoryTrain:  epoch  5, batch     4 | loss: 1.6310712MemoryTrain:  epoch  6, batch     0 | loss: 1.7305621MemoryTrain:  epoch  6, batch     1 | loss: 1.3793904MemoryTrain:  epoch  6, batch     2 | loss: 1.6887608MemoryTrain:  epoch  6, batch     3 | loss: 1.6258487MemoryTrain:  epoch  6, batch     4 | loss: 1.6152674MemoryTrain:  epoch  7, batch     0 | loss: 1.4594576MemoryTrain:  epoch  7, batch     1 | loss: 1.3040257MemoryTrain:  epoch  7, batch     2 | loss: 1.7157934MemoryTrain:  epoch  7, batch     3 | loss: 1.6323750MemoryTrain:  epoch  7, batch     4 | loss: 1.4236304MemoryTrain:  epoch  8, batch     0 | loss: 1.3236331MemoryTrain:  epoch  8, batch     1 | loss: 1.4800276MemoryTrain:  epoch  8, batch     2 | loss: 1.5629210MemoryTrain:  epoch  8, batch     3 | loss: 1.4478407MemoryTrain:  epoch  8, batch     4 | loss: 1.5724065MemoryTrain:  epoch  9, batch     0 | loss: 1.4524999MemoryTrain:  epoch  9, batch     1 | loss: 1.4227799MemoryTrain:  epoch  9, batch     2 | loss: 1.4425201MemoryTrain:  epoch  9, batch     3 | loss: 1.3290002MemoryTrain:  epoch  9, batch     4 | loss: 1.2707171
[EVAL] batch:    0 | acc: 31.25%,  total acc: 31.25%   [EVAL] batch:    1 | acc: 62.50%,  total acc: 46.88%   [EVAL] batch:    2 | acc: 43.75%,  total acc: 45.83%   [EVAL] batch:    3 | acc: 43.75%,  total acc: 45.31%   [EVAL] batch:    4 | acc: 37.50%,  total acc: 43.75%   [EVAL] batch:    5 | acc: 43.75%,  total acc: 43.75%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 48.21%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 53.12%   [EVAL] batch:    8 | acc: 25.00%,  total acc: 50.00%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 51.88%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 54.55%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 56.25%   [EVAL] batch:   12 | acc: 50.00%,  total acc: 55.77%   
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 81.25%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    5 | acc: 68.75%,  total acc: 79.17%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 82.14%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 84.38%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 86.11%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 86.88%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 88.07%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 88.02%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 87.02%   [EVAL] batch:   13 | acc: 62.50%,  total acc: 85.27%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 84.17%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 82.42%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 81.99%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 80.90%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 79.93%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 80.31%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 81.82%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 82.61%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 83.07%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 83.75%   [EVAL] batch:   25 | acc: 93.75%,  total acc: 84.13%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 84.49%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 85.04%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 85.56%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 85.42%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 85.48%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 85.94%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 84.47%   [EVAL] batch:   33 | acc: 0.00%,  total acc: 81.99%   [EVAL] batch:   34 | acc: 0.00%,  total acc: 79.64%   [EVAL] batch:   35 | acc: 0.00%,  total acc: 77.43%   [EVAL] batch:   36 | acc: 0.00%,  total acc: 75.34%   [EVAL] batch:   37 | acc: 0.00%,  total acc: 73.36%   [EVAL] batch:   38 | acc: 31.25%,  total acc: 72.28%   [EVAL] batch:   39 | acc: 87.50%,  total acc: 72.66%   [EVAL] batch:   40 | acc: 31.25%,  total acc: 71.65%   [EVAL] batch:   41 | acc: 25.00%,  total acc: 70.54%   [EVAL] batch:   42 | acc: 37.50%,  total acc: 69.77%   [EVAL] batch:   43 | acc: 62.50%,  total acc: 69.60%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 70.28%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 70.92%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 71.54%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 72.14%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 72.70%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 73.25%   [EVAL] batch:   50 | acc: 62.50%,  total acc: 73.04%   [EVAL] batch:   51 | acc: 6.25%,  total acc: 71.75%   [EVAL] batch:   52 | acc: 6.25%,  total acc: 70.52%   [EVAL] batch:   53 | acc: 0.00%,  total acc: 69.21%   [EVAL] batch:   54 | acc: 12.50%,  total acc: 68.18%   [EVAL] batch:   55 | acc: 0.00%,  total acc: 66.96%   [EVAL] batch:   56 | acc: 62.50%,  total acc: 66.89%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 67.35%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 67.69%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 68.12%   [EVAL] batch:   60 | acc: 75.00%,  total acc: 68.24%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 68.55%   [EVAL] batch:   62 | acc: 100.00%,  total acc: 69.05%   [EVAL] batch:   63 | acc: 87.50%,  total acc: 69.34%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 69.81%   [EVAL] batch:   65 | acc: 100.00%,  total acc: 70.27%   [EVAL] batch:   66 | acc: 100.00%,  total acc: 70.71%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 71.14%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 71.56%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 71.96%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 72.36%   [EVAL] batch:   71 | acc: 93.75%,  total acc: 72.66%   [EVAL] batch:   72 | acc: 25.00%,  total acc: 72.00%   [EVAL] batch:   73 | acc: 25.00%,  total acc: 71.37%   [EVAL] batch:   74 | acc: 43.75%,  total acc: 71.00%   [EVAL] batch:   75 | acc: 50.00%,  total acc: 70.72%   [EVAL] batch:   76 | acc: 50.00%,  total acc: 70.45%   [EVAL] batch:   77 | acc: 25.00%,  total acc: 69.87%   [EVAL] batch:   78 | acc: 62.50%,  total acc: 69.78%   [EVAL] batch:   79 | acc: 43.75%,  total acc: 69.45%   [EVAL] batch:   80 | acc: 50.00%,  total acc: 69.21%   [EVAL] batch:   81 | acc: 37.50%,  total acc: 68.83%   [EVAL] batch:   82 | acc: 43.75%,  total acc: 68.52%   [EVAL] batch:   83 | acc: 68.75%,  total acc: 68.53%   [EVAL] batch:   84 | acc: 87.50%,  total acc: 68.75%   [EVAL] batch:   85 | acc: 31.25%,  total acc: 68.31%   [EVAL] batch:   86 | acc: 68.75%,  total acc: 68.32%   [EVAL] batch:   87 | acc: 75.00%,  total acc: 68.39%   [EVAL] batch:   88 | acc: 75.00%,  total acc: 68.47%   [EVAL] batch:   89 | acc: 56.25%,  total acc: 68.33%   
cur_acc:  ['0.8674', '0.8438', '0.6429', '0.8125', '0.5577']
his_acc:  ['0.8674', '0.8500', '0.7646', '0.7252', '0.6833']
CurrentTrain: epoch  0, batch     0 | loss: 5.3163052CurrentTrain: epoch  0, batch     1 | loss: 6.5430245CurrentTrain: epoch  1, batch     0 | loss: 4.9774141CurrentTrain: epoch  1, batch     1 | loss: 4.7398167CurrentTrain: epoch  2, batch     0 | loss: 4.0683284CurrentTrain: epoch  2, batch     1 | loss: 4.0203424CurrentTrain: epoch  3, batch     0 | loss: 3.9798465CurrentTrain: epoch  3, batch     1 | loss: 3.2556276CurrentTrain: epoch  4, batch     0 | loss: 3.2972379CurrentTrain: epoch  4, batch     1 | loss: 3.2263279CurrentTrain: epoch  5, batch     0 | loss: 3.1444869CurrentTrain: epoch  5, batch     1 | loss: 2.8635292CurrentTrain: epoch  6, batch     0 | loss: 2.7478342CurrentTrain: epoch  6, batch     1 | loss: 2.4912744CurrentTrain: epoch  7, batch     0 | loss: 2.6613107CurrentTrain: epoch  7, batch     1 | loss: 2.6702116CurrentTrain: epoch  8, batch     0 | loss: 2.5595360CurrentTrain: epoch  8, batch     1 | loss: 2.3232284CurrentTrain: epoch  9, batch     0 | loss: 2.3458273CurrentTrain: epoch  9, batch     1 | loss: 2.2609134
#### person stateorprovinces of residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person stateorprovinces of residence" means a person is living in a state or province.
Here is an example:
Relation: person stateorprovinces of residence
Context: san jose , ca , usa speaking of k-fed , him and ex-wife britney spears are in court today , dealing with their custody battle .
Head Entity: britney spears
Tail Entity: ca
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person stateorprovinces of residence":
Relation: person stateorprovinces of residence  
Context: After moving to new york city, jessica found a vibrant community and a bustling lifestyle that she had always dreamed of.  
Head Entity: jessica  
Tail Entity: new york city  

Relation: person stateorprovinces of residence  
Context: During the summer, michael decided to spend his vacation in the beautiful beaches of florida, enjoying the sun and surf.  
Head Entity: michael  
Tail Entity: florida  
#### person date of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person date of death" means a person was died on a date.
Here is an example:
Relation: person date of death
Context: former gov. robert e. mcnair of south carolina , the political moderate who was a finalist to become vice president hubert h. humphrey 's running mate in 1968 but whose promising career was cut short by what became known as the orangeburg massacre , died on nov. 17 in charleston .
Head Entity: robert e. mcnair
Tail Entity: nov. 17
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person date of death":
Relation: person date of death  
Context: renowned physicist stephen hawking, who made groundbreaking contributions to our understanding of black holes and the universe, passed away peacefully at his home in cambridge on march 14, 2018.  
Head Entity: stephen hawking  
Tail Entity: march 14, 2018  

Relation: person date of death  
Context: the beloved author of the harry potter series, j.k. rowling, announced the passing of her dear friend and mentor, who died on january 1, 2020, after a long battle with illness.  
Head Entity: j.k. rowling's dear friend  
Tail Entity: january 1, 2020  
#### organization number of employees members ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization number of employees members" means the number of an organization 's employees is a number.
Here is an example:
Relation: organization number of employees members
Context: with the sweep of a federal regulator 's pen , massachusetts stands to gain a new life-science giant in april : covidien , a medical - supplies maker with thousands of products and more than 43,000 employees worldwide .
Head Entity: covidien
Tail Entity: 43,000
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization number of employees members":
Relation: organization number of employees members  
Context: The tech company, Innovatech, has rapidly expanded its workforce over the past year, now boasting a total of 25,000 employees across its global offices.  
Head Entity: Innovatech  
Tail Entity: 25,000  

Relation: organization number of employees members  
Context: After the merger, Green Energy Solutions reported an increase in its workforce, reaching a remarkable 15,000 employees dedicated to sustainable energy initiatives.  
Head Entity: Green Energy Solutions  
Tail Entity: 15,000  
#### person alternate names ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person alternate names" means a person 's alias is a person.
Here is an example:
Relation: person alternate names
Context: `` i am known in the hospice as the man who would n't die , '' buchwald wrote in march .
Head Entity: buchwald
Tail Entity: man who would n't die
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person alternate names":
Relation: person alternate names  
Context: `` the famous author often referred to himself as the bard of Avon, '' said the literary critic.  
Head Entity: author  
Tail Entity: bard of Avon  

Relation: person alternate names  
Context: `` during his career, he was often called the king of pop, '' the documentary revealed.  
Head Entity: he  
Tail Entity: king of pop  
#### person spouse ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person spouse" means a person 's spouse is a person.
Here is an example:
Relation: person spouse
Context: smits stands at the center of this multigenerational saga as alex vega , the adopted son of rum and sugar baron pancho duque -lrb- elizondo -rrb- and his wife , amalia -lrb- moreno -rrb- .
Head Entity: elizondo
Tail Entity: moreno
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person spouse":
Relation: person spouse  
Context: in a heartfelt ceremony, john and his beloved partner, sarah, exchanged vows surrounded by family and friends, celebrating their love and commitment to each other.  
Head Entity: john  
Tail Entity: sarah  

Relation: person spouse  
Context: after years of friendship, emily finally realized that her best friend, michael, was the one she wanted to spend her life with, and they decided to get married.  
Head Entity: emily  
Tail Entity: michael  
Mixup data size:  224
MixupTrain:  epoch  0, batch     0 | loss: 2.6100167MixupTrain:  epoch  0, batch     1 | loss: 2.9302736MixupTrain:  epoch  0, batch     2 | loss: 2.4499841MixupTrain:  epoch  0, batch     3 | loss: 3.1836006MixupTrain:  epoch  0, batch     4 | loss: 2.3805539MixupTrain:  epoch  0, batch     5 | loss: 2.8838633MixupTrain:  epoch  0, batch     6 | loss: 2.7343071MixupTrain:  epoch  0, batch     7 | loss: 2.3306156MixupTrain:  epoch  0, batch     8 | loss: 3.1356720MixupTrain:  epoch  0, batch     9 | loss: 3.6665723MixupTrain:  epoch  0, batch    10 | loss: 2.2925689MixupTrain:  epoch  0, batch    11 | loss: 2.4812517MixupTrain:  epoch  0, batch    12 | loss: 3.5426977MixupTrain:  epoch  0, batch    13 | loss: 3.3249498
MemoryTrain:  epoch  0, batch     0 | loss: 2.5896420MemoryTrain:  epoch  0, batch     1 | loss: 3.2031906MemoryTrain:  epoch  0, batch     2 | loss: 2.0520527MemoryTrain:  epoch  0, batch     3 | loss: 2.6106768MemoryTrain:  epoch  0, batch     4 | loss: 2.6274109MemoryTrain:  epoch  0, batch     5 | loss: 2.3636622MemoryTrain:  epoch  1, batch     0 | loss: 2.4276118MemoryTrain:  epoch  1, batch     1 | loss: 2.0929618MemoryTrain:  epoch  1, batch     2 | loss: 2.2530005MemoryTrain:  epoch  1, batch     3 | loss: 2.3893046MemoryTrain:  epoch  1, batch     4 | loss: 2.3189931MemoryTrain:  epoch  1, batch     5 | loss: 2.2893748MemoryTrain:  epoch  2, batch     0 | loss: 2.1396158MemoryTrain:  epoch  2, batch     1 | loss: 1.7988526MemoryTrain:  epoch  2, batch     2 | loss: 1.6621255MemoryTrain:  epoch  2, batch     3 | loss: 1.8399652MemoryTrain:  epoch  2, batch     4 | loss: 1.9797511MemoryTrain:  epoch  2, batch     5 | loss: 2.5168922MemoryTrain:  epoch  3, batch     0 | loss: 1.8477184MemoryTrain:  epoch  3, batch     1 | loss: 1.6572733MemoryTrain:  epoch  3, batch     2 | loss: 2.0050249MemoryTrain:  epoch  3, batch     3 | loss: 2.0018525MemoryTrain:  epoch  3, batch     4 | loss: 1.5474906MemoryTrain:  epoch  3, batch     5 | loss: 1.5964723MemoryTrain:  epoch  4, batch     0 | loss: 1.8493097MemoryTrain:  epoch  4, batch     1 | loss: 1.4441817MemoryTrain:  epoch  4, batch     2 | loss: 1.3775154MemoryTrain:  epoch  4, batch     3 | loss: 1.8453484MemoryTrain:  epoch  4, batch     4 | loss: 1.7994105MemoryTrain:  epoch  4, batch     5 | loss: 1.6356499MemoryTrain:  epoch  5, batch     0 | loss: 1.5769311MemoryTrain:  epoch  5, batch     1 | loss: 1.4163926MemoryTrain:  epoch  5, batch     2 | loss: 1.8950155MemoryTrain:  epoch  5, batch     3 | loss: 1.5641260MemoryTrain:  epoch  5, batch     4 | loss: 1.4203107MemoryTrain:  epoch  5, batch     5 | loss: 1.6802385MemoryTrain:  epoch  6, batch     0 | loss: 1.3834472MemoryTrain:  epoch  6, batch     1 | loss: 1.9123391MemoryTrain:  epoch  6, batch     2 | loss: 1.5320783MemoryTrain:  epoch  6, batch     3 | loss: 1.7086226MemoryTrain:  epoch  6, batch     4 | loss: 1.5326576MemoryTrain:  epoch  6, batch     5 | loss: 1.4071892MemoryTrain:  epoch  7, batch     0 | loss: 1.5090406MemoryTrain:  epoch  7, batch     1 | loss: 1.5186862MemoryTrain:  epoch  7, batch     2 | loss: 1.4285831MemoryTrain:  epoch  7, batch     3 | loss: 1.4138694MemoryTrain:  epoch  7, batch     4 | loss: 1.4615625MemoryTrain:  epoch  7, batch     5 | loss: 1.4426494MemoryTrain:  epoch  8, batch     0 | loss: 1.5617355MemoryTrain:  epoch  8, batch     1 | loss: 1.2967671MemoryTrain:  epoch  8, batch     2 | loss: 1.4324408MemoryTrain:  epoch  8, batch     3 | loss: 1.3355294MemoryTrain:  epoch  8, batch     4 | loss: 1.3557513MemoryTrain:  epoch  8, batch     5 | loss: 1.4727170MemoryTrain:  epoch  9, batch     0 | loss: 1.5240229MemoryTrain:  epoch  9, batch     1 | loss: 1.5031558MemoryTrain:  epoch  9, batch     2 | loss: 1.2808015MemoryTrain:  epoch  9, batch     3 | loss: 1.3439258MemoryTrain:  epoch  9, batch     4 | loss: 1.2348289MemoryTrain:  epoch  9, batch     5 | loss: 1.4460042
[EVAL] batch:    0 | acc: 56.25%,  total acc: 56.25%   [EVAL] batch:    1 | acc: 75.00%,  total acc: 65.62%   [EVAL] batch:    2 | acc: 50.00%,  total acc: 60.42%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 65.62%   [EVAL] batch:    4 | acc: 62.50%,  total acc: 65.00%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 69.79%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 73.21%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 75.78%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 77.08%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 75.62%   [EVAL] batch:   10 | acc: 50.00%,  total acc: 73.30%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 73.44%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 74.52%   [EVAL] batch:   13 | acc: 50.00%,  total acc: 72.77%   [EVAL] batch:   14 | acc: 31.25%,  total acc: 70.00%   
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 82.81%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 83.75%   [EVAL] batch:    5 | acc: 68.75%,  total acc: 81.25%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 83.93%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 85.94%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 88.12%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 89.20%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 89.06%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 87.50%   [EVAL] batch:   13 | acc: 62.50%,  total acc: 85.71%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 84.58%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 82.81%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 82.35%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 81.25%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 80.26%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 80.62%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 81.82%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 82.61%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 83.07%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 83.75%   [EVAL] batch:   25 | acc: 93.75%,  total acc: 84.13%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 84.49%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 85.04%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 85.56%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 85.42%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 85.48%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 85.94%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 84.47%   [EVAL] batch:   33 | acc: 0.00%,  total acc: 81.99%   [EVAL] batch:   34 | acc: 0.00%,  total acc: 79.64%   [EVAL] batch:   35 | acc: 0.00%,  total acc: 77.43%   [EVAL] batch:   36 | acc: 0.00%,  total acc: 75.34%   [EVAL] batch:   37 | acc: 0.00%,  total acc: 73.36%   [EVAL] batch:   38 | acc: 31.25%,  total acc: 72.28%   [EVAL] batch:   39 | acc: 81.25%,  total acc: 72.50%   [EVAL] batch:   40 | acc: 37.50%,  total acc: 71.65%   [EVAL] batch:   41 | acc: 37.50%,  total acc: 70.83%   [EVAL] batch:   42 | acc: 56.25%,  total acc: 70.49%   [EVAL] batch:   43 | acc: 75.00%,  total acc: 70.60%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 71.25%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 71.88%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 72.47%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 73.05%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 73.60%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 73.88%   [EVAL] batch:   50 | acc: 56.25%,  total acc: 73.53%   [EVAL] batch:   51 | acc: 18.75%,  total acc: 72.48%   [EVAL] batch:   52 | acc: 37.50%,  total acc: 71.82%   [EVAL] batch:   53 | acc: 18.75%,  total acc: 70.83%   [EVAL] batch:   54 | acc: 37.50%,  total acc: 70.23%   [EVAL] batch:   55 | acc: 12.50%,  total acc: 69.20%   [EVAL] batch:   56 | acc: 62.50%,  total acc: 69.08%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 69.50%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 69.81%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 70.21%   [EVAL] batch:   60 | acc: 68.75%,  total acc: 70.18%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 70.46%   [EVAL] batch:   62 | acc: 87.50%,  total acc: 70.73%   [EVAL] batch:   63 | acc: 81.25%,  total acc: 70.90%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 71.35%   [EVAL] batch:   65 | acc: 100.00%,  total acc: 71.78%   [EVAL] batch:   66 | acc: 100.00%,  total acc: 72.20%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 72.61%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 73.01%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 73.39%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 73.77%   [EVAL] batch:   71 | acc: 93.75%,  total acc: 74.05%   [EVAL] batch:   72 | acc: 25.00%,  total acc: 73.37%   [EVAL] batch:   73 | acc: 25.00%,  total acc: 72.72%   [EVAL] batch:   74 | acc: 62.50%,  total acc: 72.58%   [EVAL] batch:   75 | acc: 75.00%,  total acc: 72.62%   [EVAL] batch:   76 | acc: 68.75%,  total acc: 72.56%   [EVAL] batch:   77 | acc: 25.00%,  total acc: 71.96%   [EVAL] batch:   78 | acc: 56.25%,  total acc: 71.76%   [EVAL] batch:   79 | acc: 43.75%,  total acc: 71.41%   [EVAL] batch:   80 | acc: 50.00%,  total acc: 71.14%   [EVAL] batch:   81 | acc: 50.00%,  total acc: 70.88%   [EVAL] batch:   82 | acc: 50.00%,  total acc: 70.63%   [EVAL] batch:   83 | acc: 68.75%,  total acc: 70.61%   [EVAL] batch:   84 | acc: 68.75%,  total acc: 70.59%   [EVAL] batch:   85 | acc: 12.50%,  total acc: 69.91%   [EVAL] batch:   86 | acc: 31.25%,  total acc: 69.47%   [EVAL] batch:   87 | acc: 37.50%,  total acc: 69.11%   [EVAL] batch:   88 | acc: 56.25%,  total acc: 68.96%   [EVAL] batch:   89 | acc: 50.00%,  total acc: 68.75%   [EVAL] batch:   90 | acc: 56.25%,  total acc: 68.61%   [EVAL] batch:   91 | acc: 68.75%,  total acc: 68.61%   [EVAL] batch:   92 | acc: 62.50%,  total acc: 68.55%   [EVAL] batch:   93 | acc: 62.50%,  total acc: 68.48%   [EVAL] batch:   94 | acc: 87.50%,  total acc: 68.68%   [EVAL] batch:   95 | acc: 87.50%,  total acc: 68.88%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 69.20%   [EVAL] batch:   97 | acc: 93.75%,  total acc: 69.45%   [EVAL] batch:   98 | acc: 87.50%,  total acc: 69.63%   [EVAL] batch:   99 | acc: 37.50%,  total acc: 69.31%   [EVAL] batch:  100 | acc: 62.50%,  total acc: 69.25%   [EVAL] batch:  101 | acc: 87.50%,  total acc: 69.42%   [EVAL] batch:  102 | acc: 68.75%,  total acc: 69.42%   [EVAL] batch:  103 | acc: 56.25%,  total acc: 69.29%   [EVAL] batch:  104 | acc: 6.25%,  total acc: 68.69%   
cur_acc:  ['0.8674', '0.8438', '0.6429', '0.8125', '0.5577', '0.7000']
his_acc:  ['0.8674', '0.8500', '0.7646', '0.7252', '0.6833', '0.6869']
CurrentTrain: epoch  0, batch     0 | loss: 6.0513148CurrentTrain: epoch  0, batch     1 | loss: 5.2320023CurrentTrain: epoch  1, batch     0 | loss: 4.5289249CurrentTrain: epoch  1, batch     1 | loss: 4.9256458CurrentTrain: epoch  2, batch     0 | loss: 4.2438879CurrentTrain: epoch  2, batch     1 | loss: 4.5513916CurrentTrain: epoch  3, batch     0 | loss: 3.7846010CurrentTrain: epoch  3, batch     1 | loss: 4.1361170CurrentTrain: epoch  4, batch     0 | loss: 3.6988623CurrentTrain: epoch  4, batch     1 | loss: 3.1637175CurrentTrain: epoch  5, batch     0 | loss: 3.3554697CurrentTrain: epoch  5, batch     1 | loss: 3.8209941CurrentTrain: epoch  6, batch     0 | loss: 3.3568447CurrentTrain: epoch  6, batch     1 | loss: 2.8561265CurrentTrain: epoch  7, batch     0 | loss: 3.0861154CurrentTrain: epoch  7, batch     1 | loss: 2.3171930CurrentTrain: epoch  8, batch     0 | loss: 2.6945910CurrentTrain: epoch  8, batch     1 | loss: 2.6387792CurrentTrain: epoch  9, batch     0 | loss: 2.8092182CurrentTrain: epoch  9, batch     1 | loss: 2.1314542
#### person country of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person country of birth" means a person was born in a country.
Here is an example:
Relation: person country of birth
Context: in testimony by satellite link from germany to a house of representatives ' panel , murat kurnaz recounted his five-year detention , alleging a wide range of torture and abuse .
Head Entity: murat kurnaz
Tail Entity: germany
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person country of birth":
Relation: person country of birth  
Context: During the interview, the renowned artist shared his experiences growing up in the vibrant streets of Barcelona, which greatly influenced his work.  
Head Entity: the renowned artist  
Tail Entity: Barcelona  

Relation: person country of birth  
Context: In her autobiography, the famous actress reflects on her childhood in Mumbai, where she developed her passion for performing arts.  
Head Entity: the famous actress  
Tail Entity: Mumbai  
#### organization website ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization website" means an organization 's website is an url.
Here is an example:
Relation: organization website
Context: http://www.pentaxsailaway.com/ paul j. sail away with pentax sweepstakes
Head Entity: pentax
Tail Entity: http://www.pentaxsailaway.com/
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization website":
Relation: organization website  
Context: Visit the official site at https://www.techinnovators.com for more information on their latest products.  
Head Entity: Tech Innovators  
Tail Entity: https://www.techinnovators.com  

Relation: organization website  
Context: For updates and news, check out the blog at http://www.greenearth.org/blog.  
Head Entity: Green Earth  
Tail Entity: http://www.greenearth.org/  
#### organization shareholders ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization shareholders" means an organization was invested by a person.
Here is an example:
Relation: organization shareholders
Context: marsans already owns the madrid-based air comet and 95 percent of aerolineas argentinas .
Head Entity: aerolineas argentinas
Tail Entity: marsans
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization shareholders":
Relation: organization shareholders  
Context: tech giant apple inc. has recently acquired a significant stake in the innovative startup nextdoor.  
Head Entity: nextdoor  
Tail Entity: apple inc.  

Relation: organization shareholders  
Context: the investment firm blackrock has increased its holdings in the renewable energy company sunrun.  
Head Entity: sunrun  
Tail Entity: blackrock  
#### organization dissolved ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization dissolved" means an organization was dissolved in a date.
Here is an example:
Relation: organization dissolved
Context: the tse suffered its worst-ever system crash in november 2005 which paralyzed the world 's second largest bourse and forced it to shelve plan for a listing of its own .
Head Entity: tse
Tail Entity: november 2005
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization dissolved":
Relation: organization dissolved  
Context: the once-prominent tech startup, Innovatech, officially ceased operations in early 2020 after struggling to secure funding.  
Head Entity: Innovatech  
Tail Entity: early 2020  

Relation: organization dissolved  
Context: after years of financial difficulties, the local arts council announced its closure in the spring of 2018, leaving many artists without support.  
Head Entity: local arts council  
Tail Entity: spring 2018  
#### organization founded by ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization founded by" means an organization was found by a person.
Here is an example:
Relation: organization founded by
Context: `` i have no greater obligation than to ensure the safely of airline travelers in this country , '' transportation secretary ray lahood said in a joint statement with j. randolph babbitt , administrator of the federal aviation administration , that was issued on the eve of a senate hearing on aviation safety .
Head Entity: federal aviation administration
Tail Entity: j. randolph babbitt
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization founded by":
Relation: organization founded by  
Context: In 1975, the renowned physicist and entrepreneur, Dr. Jane Smith, established Quantum Innovations, a company dedicated to advancing quantum computing technologies.  
Head Entity: Quantum Innovations  
Tail Entity: Dr. Jane Smith  

Relation: organization founded by  
Context: The charity organization, Hope for Tomorrow, was created in 2010 by the well-known philanthropist, Michael Johnson, to support underprivileged children.  
Head Entity: Hope for Tomorrow  
Tail Entity: Michael Johnson  
Mixup data size:  255
MixupTrain:  epoch  0, batch     0 | loss: 3.1836839MixupTrain:  epoch  0, batch     1 | loss: 2.7198750MixupTrain:  epoch  0, batch     2 | loss: 2.5007835MixupTrain:  epoch  0, batch     3 | loss: 2.3828242MixupTrain:  epoch  0, batch     4 | loss: 2.0610958MixupTrain:  epoch  0, batch     5 | loss: 2.1766877MixupTrain:  epoch  0, batch     6 | loss: 2.4480087MixupTrain:  epoch  0, batch     7 | loss: 3.5843944MixupTrain:  epoch  0, batch     8 | loss: 2.8295727MixupTrain:  epoch  0, batch     9 | loss: 3.1193383MixupTrain:  epoch  0, batch    10 | loss: 2.6866592MixupTrain:  epoch  0, batch    11 | loss: 3.0949456MixupTrain:  epoch  0, batch    12 | loss: 2.1440237MixupTrain:  epoch  0, batch    13 | loss: 2.6216771MixupTrain:  epoch  0, batch    14 | loss: 3.3295694MixupTrain:  epoch  0, batch    15 | loss: 2.0813476
MemoryTrain:  epoch  0, batch     0 | loss: 3.1149220MemoryTrain:  epoch  0, batch     1 | loss: 2.4457273MemoryTrain:  epoch  0, batch     2 | loss: 2.1644940MemoryTrain:  epoch  0, batch     3 | loss: 2.3966699MemoryTrain:  epoch  0, batch     4 | loss: 2.1639013MemoryTrain:  epoch  0, batch     5 | loss: 2.3184934MemoryTrain:  epoch  0, batch     6 | loss: 3.1195216MemoryTrain:  epoch  1, batch     0 | loss: 2.6714327MemoryTrain:  epoch  1, batch     1 | loss: 2.7872930MemoryTrain:  epoch  1, batch     2 | loss: 2.3736179MemoryTrain:  epoch  1, batch     3 | loss: 2.6060772MemoryTrain:  epoch  1, batch     4 | loss: 1.3882425MemoryTrain:  epoch  1, batch     5 | loss: 2.3830690MemoryTrain:  epoch  1, batch     6 | loss: 1.8594388MemoryTrain:  epoch  2, batch     0 | loss: 2.2396803MemoryTrain:  epoch  2, batch     1 | loss: 1.7028624MemoryTrain:  epoch  2, batch     2 | loss: 1.9538651MemoryTrain:  epoch  2, batch     3 | loss: 1.7987632MemoryTrain:  epoch  2, batch     4 | loss: 1.6995479MemoryTrain:  epoch  2, batch     5 | loss: 1.8024571MemoryTrain:  epoch  2, batch     6 | loss: 2.4737344MemoryTrain:  epoch  3, batch     0 | loss: 2.1057811MemoryTrain:  epoch  3, batch     1 | loss: 1.6750480MemoryTrain:  epoch  3, batch     2 | loss: 1.7886201MemoryTrain:  epoch  3, batch     3 | loss: 1.9212946MemoryTrain:  epoch  3, batch     4 | loss: 1.4525115MemoryTrain:  epoch  3, batch     5 | loss: 1.7888923MemoryTrain:  epoch  3, batch     6 | loss: 1.3460453MemoryTrain:  epoch  4, batch     0 | loss: 2.0394051MemoryTrain:  epoch  4, batch     1 | loss: 1.7058133MemoryTrain:  epoch  4, batch     2 | loss: 1.2574120MemoryTrain:  epoch  4, batch     3 | loss: 1.9202130MemoryTrain:  epoch  4, batch     4 | loss: 1.6529360MemoryTrain:  epoch  4, batch     5 | loss: 1.5325899MemoryTrain:  epoch  4, batch     6 | loss: 1.8606496MemoryTrain:  epoch  5, batch     0 | loss: 1.2927928MemoryTrain:  epoch  5, batch     1 | loss: 1.6606969MemoryTrain:  epoch  5, batch     2 | loss: 1.3906691MemoryTrain:  epoch  5, batch     3 | loss: 1.9319637MemoryTrain:  epoch  5, batch     4 | loss: 1.7492859MemoryTrain:  epoch  5, batch     5 | loss: 1.4014840MemoryTrain:  epoch  5, batch     6 | loss: 1.4340603MemoryTrain:  epoch  6, batch     0 | loss: 1.4737473MemoryTrain:  epoch  6, batch     1 | loss: 1.3669136MemoryTrain:  epoch  6, batch     2 | loss: 1.6317300MemoryTrain:  epoch  6, batch     3 | loss: 1.3825123MemoryTrain:  epoch  6, batch     4 | loss: 1.7645158MemoryTrain:  epoch  6, batch     5 | loss: 1.4589796MemoryTrain:  epoch  6, batch     6 | loss: 1.6212904MemoryTrain:  epoch  7, batch     0 | loss: 1.3602419MemoryTrain:  epoch  7, batch     1 | loss: 1.4534986MemoryTrain:  epoch  7, batch     2 | loss: 1.4100573MemoryTrain:  epoch  7, batch     3 | loss: 1.6565778MemoryTrain:  epoch  7, batch     4 | loss: 1.3574799MemoryTrain:  epoch  7, batch     5 | loss: 1.4619703MemoryTrain:  epoch  7, batch     6 | loss: 1.7461207MemoryTrain:  epoch  8, batch     0 | loss: 1.3386685MemoryTrain:  epoch  8, batch     1 | loss: 1.3450253MemoryTrain:  epoch  8, batch     2 | loss: 1.3787296MemoryTrain:  epoch  8, batch     3 | loss: 1.3640633MemoryTrain:  epoch  8, batch     4 | loss: 1.4766641MemoryTrain:  epoch  8, batch     5 | loss: 1.4246643MemoryTrain:  epoch  8, batch     6 | loss: 1.7146435MemoryTrain:  epoch  9, batch     0 | loss: 1.3008339MemoryTrain:  epoch  9, batch     1 | loss: 1.4741507MemoryTrain:  epoch  9, batch     2 | loss: 1.3481265MemoryTrain:  epoch  9, batch     3 | loss: 1.3286685MemoryTrain:  epoch  9, batch     4 | loss: 1.5951577MemoryTrain:  epoch  9, batch     5 | loss: 1.3320820MemoryTrain:  epoch  9, batch     6 | loss: 1.3284649
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 68.75%,  total acc: 81.25%   [EVAL] batch:    3 | acc: 25.00%,  total acc: 67.19%   [EVAL] batch:    4 | acc: 18.75%,  total acc: 57.50%   [EVAL] batch:    5 | acc: 37.50%,  total acc: 54.17%   [EVAL] batch:    6 | acc: 0.00%,  total acc: 46.43%   [EVAL] batch:    7 | acc: 0.00%,  total acc: 40.62%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 43.75%,  total acc: 56.25%   [EVAL] batch:    2 | acc: 68.75%,  total acc: 60.42%   [EVAL] batch:    3 | acc: 43.75%,  total acc: 56.25%   [EVAL] batch:    4 | acc: 43.75%,  total acc: 53.75%   [EVAL] batch:    5 | acc: 43.75%,  total acc: 52.08%   [EVAL] batch:    6 | acc: 62.50%,  total acc: 53.57%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 56.25%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 59.03%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 60.62%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 63.07%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 65.10%   [EVAL] batch:   12 | acc: 25.00%,  total acc: 62.02%   [EVAL] batch:   13 | acc: 31.25%,  total acc: 59.82%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 60.42%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 60.16%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 61.03%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 61.11%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 61.18%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 62.50%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 63.69%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 65.06%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 66.30%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 67.45%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 68.75%   [EVAL] batch:   25 | acc: 93.75%,  total acc: 69.71%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 70.60%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 71.65%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 72.63%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 72.92%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 73.39%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 74.22%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 73.11%   [EVAL] batch:   33 | acc: 0.00%,  total acc: 70.96%   [EVAL] batch:   34 | acc: 0.00%,  total acc: 68.93%   [EVAL] batch:   35 | acc: 0.00%,  total acc: 67.01%   [EVAL] batch:   36 | acc: 0.00%,  total acc: 65.20%   [EVAL] batch:   37 | acc: 0.00%,  total acc: 63.49%   [EVAL] batch:   38 | acc: 37.50%,  total acc: 62.82%   [EVAL] batch:   39 | acc: 87.50%,  total acc: 63.44%   [EVAL] batch:   40 | acc: 56.25%,  total acc: 63.26%   [EVAL] batch:   41 | acc: 37.50%,  total acc: 62.65%   [EVAL] batch:   42 | acc: 56.25%,  total acc: 62.50%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 62.93%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 63.75%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 64.54%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 65.29%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 66.02%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 66.71%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 67.12%   [EVAL] batch:   50 | acc: 56.25%,  total acc: 66.91%   [EVAL] batch:   51 | acc: 12.50%,  total acc: 65.87%   [EVAL] batch:   52 | acc: 18.75%,  total acc: 64.98%   [EVAL] batch:   53 | acc: 18.75%,  total acc: 64.12%   [EVAL] batch:   54 | acc: 25.00%,  total acc: 63.41%   [EVAL] batch:   55 | acc: 6.25%,  total acc: 62.39%   [EVAL] batch:   56 | acc: 56.25%,  total acc: 62.28%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 62.72%   [EVAL] batch:   58 | acc: 81.25%,  total acc: 63.03%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 63.54%   [EVAL] batch:   60 | acc: 68.75%,  total acc: 63.63%   [EVAL] batch:   61 | acc: 81.25%,  total acc: 63.91%   [EVAL] batch:   62 | acc: 56.25%,  total acc: 63.79%   [EVAL] batch:   63 | acc: 81.25%,  total acc: 64.06%   [EVAL] batch:   64 | acc: 93.75%,  total acc: 64.52%   [EVAL] batch:   65 | acc: 100.00%,  total acc: 65.06%   [EVAL] batch:   66 | acc: 100.00%,  total acc: 65.58%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 66.08%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 66.58%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 67.05%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 67.52%   [EVAL] batch:   71 | acc: 93.75%,  total acc: 67.88%   [EVAL] batch:   72 | acc: 0.00%,  total acc: 66.95%   [EVAL] batch:   73 | acc: 6.25%,  total acc: 66.13%   [EVAL] batch:   74 | acc: 43.75%,  total acc: 65.83%   [EVAL] batch:   75 | acc: 68.75%,  total acc: 65.87%   [EVAL] batch:   76 | acc: 56.25%,  total acc: 65.75%   [EVAL] batch:   77 | acc: 18.75%,  total acc: 65.14%   [EVAL] batch:   78 | acc: 56.25%,  total acc: 65.03%   [EVAL] batch:   79 | acc: 43.75%,  total acc: 64.77%   [EVAL] batch:   80 | acc: 56.25%,  total acc: 64.66%   [EVAL] batch:   81 | acc: 43.75%,  total acc: 64.41%   [EVAL] batch:   82 | acc: 31.25%,  total acc: 64.01%   [EVAL] batch:   83 | acc: 62.50%,  total acc: 63.99%   [EVAL] batch:   84 | acc: 68.75%,  total acc: 64.04%   [EVAL] batch:   85 | acc: 12.50%,  total acc: 63.44%   [EVAL] batch:   86 | acc: 31.25%,  total acc: 63.07%   [EVAL] batch:   87 | acc: 43.75%,  total acc: 62.86%   [EVAL] batch:   88 | acc: 31.25%,  total acc: 62.50%   [EVAL] batch:   89 | acc: 56.25%,  total acc: 62.43%   [EVAL] batch:   90 | acc: 68.75%,  total acc: 62.50%   [EVAL] batch:   91 | acc: 75.00%,  total acc: 62.64%   [EVAL] batch:   92 | acc: 68.75%,  total acc: 62.70%   [EVAL] batch:   93 | acc: 75.00%,  total acc: 62.83%   [EVAL] batch:   94 | acc: 93.75%,  total acc: 63.16%   [EVAL] batch:   95 | acc: 87.50%,  total acc: 63.41%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 63.79%   [EVAL] batch:   97 | acc: 93.75%,  total acc: 64.09%   [EVAL] batch:   98 | acc: 93.75%,  total acc: 64.39%   [EVAL] batch:   99 | acc: 37.50%,  total acc: 64.12%   [EVAL] batch:  100 | acc: 68.75%,  total acc: 64.17%   [EVAL] batch:  101 | acc: 87.50%,  total acc: 64.40%   [EVAL] batch:  102 | acc: 75.00%,  total acc: 64.50%   [EVAL] batch:  103 | acc: 50.00%,  total acc: 64.36%   [EVAL] batch:  104 | acc: 75.00%,  total acc: 64.46%   [EVAL] batch:  105 | acc: 100.00%,  total acc: 64.80%   [EVAL] batch:  106 | acc: 75.00%,  total acc: 64.89%   [EVAL] batch:  107 | acc: 25.00%,  total acc: 64.53%   [EVAL] batch:  108 | acc: 18.75%,  total acc: 64.11%   [EVAL] batch:  109 | acc: 31.25%,  total acc: 63.81%   [EVAL] batch:  110 | acc: 6.25%,  total acc: 63.29%   [EVAL] batch:  111 | acc: 0.00%,  total acc: 62.72%   
cur_acc:  ['0.8674', '0.8438', '0.6429', '0.8125', '0.5577', '0.7000', '0.4062']
his_acc:  ['0.8674', '0.8500', '0.7646', '0.7252', '0.6833', '0.6869', '0.6272']
CurrentTrain: epoch  0, batch     0 | loss: 7.5559845CurrentTrain: epoch  0, batch     1 | loss: 8.5681829CurrentTrain: epoch  1, batch     0 | loss: 7.3915653CurrentTrain: epoch  1, batch     1 | loss: 6.0071149CurrentTrain: epoch  2, batch     0 | loss: 5.8918781CurrentTrain: epoch  2, batch     1 | loss: 6.4204826CurrentTrain: epoch  3, batch     0 | loss: 5.3740668CurrentTrain: epoch  3, batch     1 | loss: 6.0107079CurrentTrain: epoch  4, batch     0 | loss: 5.0604744CurrentTrain: epoch  4, batch     1 | loss: 5.9775105CurrentTrain: epoch  5, batch     0 | loss: 4.9139318CurrentTrain: epoch  5, batch     1 | loss: 4.8071442CurrentTrain: epoch  6, batch     0 | loss: 4.7070255CurrentTrain: epoch  6, batch     1 | loss: 4.1539979CurrentTrain: epoch  7, batch     0 | loss: 4.3982964CurrentTrain: epoch  7, batch     1 | loss: 4.1025376CurrentTrain: epoch  8, batch     0 | loss: 4.0375285CurrentTrain: epoch  8, batch     1 | loss: 3.2476540CurrentTrain: epoch  9, batch     0 | loss: 3.1771595CurrentTrain: epoch  9, batch     1 | loss: 4.4759774
#### organization subsidiaries ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization subsidiaries" means an organization 's subsidiary is an organization.
Here is an example:
Relation: organization subsidiaries
Context: firstgroup , britain 's largest bus operator , entered the north american market in 1999 when it acquired ryder public transportation services inc. .
Head Entity: firstgroup
Tail Entity: ryder public transportation services inc.
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization subsidiaries":
Relation: organization subsidiaries  
Context: Alphabet Inc. has several subsidiaries, including YouTube, which it acquired in 2006 for $1.65 billion.  
Head Entity: Alphabet Inc.  
Tail Entity: YouTube  

Relation: organization subsidiaries  
Context: The Coca-Cola Company owns numerous subsidiaries, such as Minute Maid, which specializes in fruit juices and drinks.  
Head Entity: The Coca-Cola Company  
Tail Entity: Minute Maid  
#### organization parents ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization parents" means an organization 's parent is an organization.
Here is an example:
Relation: organization parents
Context: aseel kami , tim cocks , abdul-rahman taher , tim cocks and michael christie -lrb- reuters -rrb- add an intriguing note in the speculation over who 's responsible for the recent bombings : `` but a senior iraqi intelligence source , who declined to be named , said there was evidence the bombs could be the work of the militant badr organisation , the armed wing of the supreme islamic iraqi council -lrb- isci -rrb- .
Head Entity: badr organisation
Tail Entity: supreme islamic iraqi council
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization parents":
Relation: organization parents  
Context: In a recent merger, the tech giant Alphabet Inc. announced that it would acquire the innovative startup DeepMind Technologies, which has been a subsidiary of Alphabet since 2014. This acquisition is expected to enhance Alphabet's capabilities in artificial intelligence.  
Head Entity: DeepMind Technologies  
Tail Entity: Alphabet Inc.  

Relation: organization parents  
Context: The historical records indicate that the renowned publishing house Penguin Random House was formed through the merger of two major companies, Penguin Group and Random House, which were both influential in the literary world.  
Head Entity: Penguin Random House  
Tail Entity: Random House  
#### organization alternate names ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization alternate names" means a person 's alias is a person.
Here is an example:
Relation: organization alternate names
Context: while section 106 of the hyde act openly bans indian testing , and the agreement upholds reinforces that test ban by upholding the applicability of domestic laws , washington has already recommended that the nuclear suppliers group -lrb- nsg -rrb- link its proposed exemption for india to a similar test ban .
Head Entity: nuclear suppliers group
Tail Entity: nsg
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization alternate names":
Relation: organization alternate names  
Context: The International Monetary Fund, often referred to as the IMF, plays a crucial role in global economic stability by providing financial assistance and advice to member countries.  
Head Entity: International Monetary Fund  
Tail Entity: IMF  

Relation: organization alternate names  
Context: The World Health Organization, commonly known as WHO, has been at the forefront of global health initiatives, especially during the COVID-19 pandemic.  
Head Entity: World Health Organization  
Tail Entity: WHO  
#### organization city of headquarters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization city of headquarters" means an organization is located in a city.
Here is an example:
Relation: organization city of headquarters
Context: ------ london 2008-05-20 07:23:45 utc enodis plc endorses sweetened takeover bid by us company manitowoc illinois tool works of glenville , illinois , which had offered 282 pence -lrb- us$ 551 euro3 54 -rrb- per share , said monday that it was considering its position .
Head Entity: illinois tool works
Tail Entity: glenville
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization city of headquarters":
Relation: organization city of headquarters  
Context: ------ san francisco 2021-03-15 10:00:00 utc tech giant apple inc. has announced plans to expand its headquarters in the heart of silicon valley, which is located in cupertino, california, aiming to create thousands of new jobs in the area.  
Head Entity: apple inc.  
Tail Entity: cupertino  

Relation: organization city of headquarters  
Context: ------ new york 2019-11-22 14:45:00 utc the financial services firm goldman sachs has its main office situated in lower manhattan, new york city, where it continues to play a pivotal role in global finance.  
Head Entity: goldman sachs  
Tail Entity: new york city  
#### person siblings ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person siblings" means a person 's sibling is a person.
Here is an example:
Relation: person siblings
Context: `` holly and sanjaya are headed to -lsb- the hawaiian island of -rsb- kauai tomorrow morning so she can meet his parents . ''
Head Entity: she
Tail Entity: sanjaya
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person siblings":
Relation: person siblings  
Context: `` during the family reunion, john introduced his sister to everyone, and they all had a great time catching up. ''  
Head Entity: his sister  
Tail Entity: john  

Relation: person siblings  
Context: `` after the graduation ceremony, emily celebrated with her brother, who had just completed his degree in engineering. ''  
Head Entity: her brother  
Tail Entity: emily  
Mixup data size:  285
MixupTrain:  epoch  0, batch     0 | loss: 2.9878332MixupTrain:  epoch  0, batch     1 | loss: 2.6512995MixupTrain:  epoch  0, batch     2 | loss: 3.2456735MixupTrain:  epoch  0, batch     3 | loss: 2.7102389MixupTrain:  epoch  0, batch     4 | loss: 3.4127749MixupTrain:  epoch  0, batch     5 | loss: 2.1382549MixupTrain:  epoch  0, batch     6 | loss: 2.6491420MixupTrain:  epoch  0, batch     7 | loss: 2.4742927MixupTrain:  epoch  0, batch     8 | loss: 2.7577291MixupTrain:  epoch  0, batch     9 | loss: 3.0452735MixupTrain:  epoch  0, batch    10 | loss: 2.3577173MixupTrain:  epoch  0, batch    11 | loss: 3.5406849MixupTrain:  epoch  0, batch    12 | loss: 2.1831240MixupTrain:  epoch  0, batch    13 | loss: 3.7436572MixupTrain:  epoch  0, batch    14 | loss: 2.2037239MixupTrain:  epoch  0, batch    15 | loss: 3.4141337MixupTrain:  epoch  0, batch    16 | loss: 2.1448071MixupTrain:  epoch  0, batch    17 | loss: 2.6666676
MemoryTrain:  epoch  0, batch     0 | loss: 2.9362562MemoryTrain:  epoch  0, batch     1 | loss: 2.1363823MemoryTrain:  epoch  0, batch     2 | loss: 2.3573987MemoryTrain:  epoch  0, batch     3 | loss: 3.2520437MemoryTrain:  epoch  0, batch     4 | loss: 1.8722851MemoryTrain:  epoch  0, batch     5 | loss: 2.9170165MemoryTrain:  epoch  0, batch     6 | loss: 2.4260311MemoryTrain:  epoch  0, batch     7 | loss: 3.3383262MemoryTrain:  epoch  1, batch     0 | loss: 2.3307793MemoryTrain:  epoch  1, batch     1 | loss: 2.8789029MemoryTrain:  epoch  1, batch     2 | loss: 2.4967122MemoryTrain:  epoch  1, batch     3 | loss: 2.5490429MemoryTrain:  epoch  1, batch     4 | loss: 2.2494888MemoryTrain:  epoch  1, batch     5 | loss: 1.9145446MemoryTrain:  epoch  1, batch     6 | loss: 2.3600738MemoryTrain:  epoch  1, batch     7 | loss: 2.2946491MemoryTrain:  epoch  2, batch     0 | loss: 1.9269989MemoryTrain:  epoch  2, batch     1 | loss: 1.6856067MemoryTrain:  epoch  2, batch     2 | loss: 1.8537937MemoryTrain:  epoch  2, batch     3 | loss: 1.8058475MemoryTrain:  epoch  2, batch     4 | loss: 2.5888622MemoryTrain:  epoch  2, batch     5 | loss: 1.6967843MemoryTrain:  epoch  2, batch     6 | loss: 2.0731611MemoryTrain:  epoch  2, batch     7 | loss: 1.5798789MemoryTrain:  epoch  3, batch     0 | loss: 1.9179715MemoryTrain:  epoch  3, batch     1 | loss: 1.7442832MemoryTrain:  epoch  3, batch     2 | loss: 1.7831922MemoryTrain:  epoch  3, batch     3 | loss: 1.7592123MemoryTrain:  epoch  3, batch     4 | loss: 2.5059419MemoryTrain:  epoch  3, batch     5 | loss: 1.5220447MemoryTrain:  epoch  3, batch     6 | loss: 1.8451304MemoryTrain:  epoch  3, batch     7 | loss: 1.6504018MemoryTrain:  epoch  4, batch     0 | loss: 1.5621531MemoryTrain:  epoch  4, batch     1 | loss: 1.7266757MemoryTrain:  epoch  4, batch     2 | loss: 1.4768708MemoryTrain:  epoch  4, batch     3 | loss: 1.5909200MemoryTrain:  epoch  4, batch     4 | loss: 2.2780304MemoryTrain:  epoch  4, batch     5 | loss: 2.1916900MemoryTrain:  epoch  4, batch     6 | loss: 1.5752571MemoryTrain:  epoch  4, batch     7 | loss: 1.3480024MemoryTrain:  epoch  5, batch     0 | loss: 1.4692211MemoryTrain:  epoch  5, batch     1 | loss: 1.6037894MemoryTrain:  epoch  5, batch     2 | loss: 1.7660991MemoryTrain:  epoch  5, batch     3 | loss: 1.4113653MemoryTrain:  epoch  5, batch     4 | loss: 1.9875566MemoryTrain:  epoch  5, batch     5 | loss: 1.4416260MemoryTrain:  epoch  5, batch     6 | loss: 1.8390913MemoryTrain:  epoch  5, batch     7 | loss: 1.7409824MemoryTrain:  epoch  6, batch     0 | loss: 1.4791161MemoryTrain:  epoch  6, batch     1 | loss: 1.5568545MemoryTrain:  epoch  6, batch     2 | loss: 1.5409925MemoryTrain:  epoch  6, batch     3 | loss: 1.5689847MemoryTrain:  epoch  6, batch     4 | loss: 1.5497031MemoryTrain:  epoch  6, batch     5 | loss: 1.7772014MemoryTrain:  epoch  6, batch     6 | loss: 1.7011490MemoryTrain:  epoch  6, batch     7 | loss: 1.5102122MemoryTrain:  epoch  7, batch     0 | loss: 1.5897579MemoryTrain:  epoch  7, batch     1 | loss: 1.4084921MemoryTrain:  epoch  7, batch     2 | loss: 1.4920478MemoryTrain:  epoch  7, batch     3 | loss: 1.4545963MemoryTrain:  epoch  7, batch     4 | loss: 1.4680475MemoryTrain:  epoch  7, batch     5 | loss: 1.2907152MemoryTrain:  epoch  7, batch     6 | loss: 1.4272289MemoryTrain:  epoch  7, batch     7 | loss: 1.9015830MemoryTrain:  epoch  8, batch     0 | loss: 1.3596020MemoryTrain:  epoch  8, batch     1 | loss: 1.3733331MemoryTrain:  epoch  8, batch     2 | loss: 1.3422799MemoryTrain:  epoch  8, batch     3 | loss: 1.7419302MemoryTrain:  epoch  8, batch     4 | loss: 1.3701906MemoryTrain:  epoch  8, batch     5 | loss: 1.4528284MemoryTrain:  epoch  8, batch     6 | loss: 1.3302367MemoryTrain:  epoch  8, batch     7 | loss: 1.3616451MemoryTrain:  epoch  9, batch     0 | loss: 1.2543154MemoryTrain:  epoch  9, batch     1 | loss: 1.6564878MemoryTrain:  epoch  9, batch     2 | loss: 1.3304093MemoryTrain:  epoch  9, batch     3 | loss: 1.3148279MemoryTrain:  epoch  9, batch     4 | loss: 1.3259596MemoryTrain:  epoch  9, batch     5 | loss: 1.3193196MemoryTrain:  epoch  9, batch     6 | loss: 1.3532313MemoryTrain:  epoch  9, batch     7 | loss: 1.4325095
[EVAL] batch:    0 | acc: 18.75%,  total acc: 18.75%   [EVAL] batch:    1 | acc: 37.50%,  total acc: 28.12%   [EVAL] batch:    2 | acc: 31.25%,  total acc: 29.17%   [EVAL] batch:    3 | acc: 50.00%,  total acc: 34.38%   [EVAL] batch:    4 | acc: 31.25%,  total acc: 33.75%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 40.62%   [EVAL] batch:    6 | acc: 56.25%,  total acc: 42.86%   [EVAL] batch:    7 | acc: 62.50%,  total acc: 45.31%   [EVAL] batch:    8 | acc: 43.75%,  total acc: 45.14%   [EVAL] batch:    9 | acc: 50.00%,  total acc: 45.62%   [EVAL] batch:   10 | acc: 68.75%,  total acc: 47.73%   [EVAL] batch:   11 | acc: 50.00%,  total acc: 47.92%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 49.52%   [EVAL] batch:   13 | acc: 100.00%,  total acc: 53.12%   [EVAL] batch:   14 | acc: 50.00%,  total acc: 52.92%   [EVAL] batch:   15 | acc: 81.25%,  total acc: 54.69%   [EVAL] batch:   16 | acc: 87.50%,  total acc: 56.62%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 57.29%   [EVAL] batch:   18 | acc: 0.00%,  total acc: 54.28%   [EVAL] batch:   19 | acc: 0.00%,  total acc: 51.56%   [EVAL] batch:   20 | acc: 0.00%,  total acc: 49.11%   [EVAL] batch:   21 | acc: 0.00%,  total acc: 46.88%   
[EVAL] batch:    0 | acc: 31.25%,  total acc: 31.25%   [EVAL] batch:    1 | acc: 50.00%,  total acc: 40.62%   [EVAL] batch:    2 | acc: 68.75%,  total acc: 50.00%   [EVAL] batch:    3 | acc: 43.75%,  total acc: 48.44%   [EVAL] batch:    4 | acc: 31.25%,  total acc: 45.00%   [EVAL] batch:    5 | acc: 37.50%,  total acc: 43.75%   [EVAL] batch:    6 | acc: 56.25%,  total acc: 45.54%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 49.22%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 52.78%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 55.00%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 57.95%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 59.90%   [EVAL] batch:   12 | acc: 31.25%,  total acc: 57.69%   [EVAL] batch:   13 | acc: 31.25%,  total acc: 55.80%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 56.67%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 56.64%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 57.72%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 57.99%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 58.55%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 60.00%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 61.90%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 63.07%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 64.40%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 65.62%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 67.00%   [EVAL] batch:   25 | acc: 93.75%,  total acc: 68.03%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 68.98%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 70.09%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 70.91%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 71.25%   [EVAL] batch:   30 | acc: 75.00%,  total acc: 71.37%   [EVAL] batch:   31 | acc: 81.25%,  total acc: 71.68%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 70.64%   [EVAL] batch:   33 | acc: 0.00%,  total acc: 68.57%   [EVAL] batch:   34 | acc: 0.00%,  total acc: 66.61%   [EVAL] batch:   35 | acc: 6.25%,  total acc: 64.93%   [EVAL] batch:   36 | acc: 0.00%,  total acc: 63.18%   [EVAL] batch:   37 | acc: 0.00%,  total acc: 61.51%   [EVAL] batch:   38 | acc: 31.25%,  total acc: 60.74%   [EVAL] batch:   39 | acc: 93.75%,  total acc: 61.56%   [EVAL] batch:   40 | acc: 37.50%,  total acc: 60.98%   [EVAL] batch:   41 | acc: 43.75%,  total acc: 60.57%   [EVAL] batch:   42 | acc: 56.25%,  total acc: 60.47%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 60.94%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 61.81%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 62.64%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 63.43%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 64.19%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 64.92%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 65.38%   [EVAL] batch:   50 | acc: 56.25%,  total acc: 65.20%   [EVAL] batch:   51 | acc: 12.50%,  total acc: 64.18%   [EVAL] batch:   52 | acc: 18.75%,  total acc: 63.33%   [EVAL] batch:   53 | acc: 18.75%,  total acc: 62.50%   [EVAL] batch:   54 | acc: 18.75%,  total acc: 61.70%   [EVAL] batch:   55 | acc: 6.25%,  total acc: 60.71%   [EVAL] batch:   56 | acc: 62.50%,  total acc: 60.75%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 61.21%   [EVAL] batch:   58 | acc: 68.75%,  total acc: 61.33%   [EVAL] batch:   59 | acc: 87.50%,  total acc: 61.77%   [EVAL] batch:   60 | acc: 62.50%,  total acc: 61.78%   [EVAL] batch:   61 | acc: 75.00%,  total acc: 62.00%   [EVAL] batch:   62 | acc: 68.75%,  total acc: 62.10%   [EVAL] batch:   63 | acc: 81.25%,  total acc: 62.40%   [EVAL] batch:   64 | acc: 93.75%,  total acc: 62.88%   [EVAL] batch:   65 | acc: 100.00%,  total acc: 63.45%   [EVAL] batch:   66 | acc: 100.00%,  total acc: 63.99%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 64.52%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 65.04%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 65.54%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 66.02%   [EVAL] batch:   71 | acc: 93.75%,  total acc: 66.41%   [EVAL] batch:   72 | acc: 6.25%,  total acc: 65.58%   [EVAL] batch:   73 | acc: 0.00%,  total acc: 64.70%   [EVAL] batch:   74 | acc: 50.00%,  total acc: 64.50%   [EVAL] batch:   75 | acc: 87.50%,  total acc: 64.80%   [EVAL] batch:   76 | acc: 68.75%,  total acc: 64.85%   [EVAL] batch:   77 | acc: 25.00%,  total acc: 64.34%   [EVAL] batch:   78 | acc: 62.50%,  total acc: 64.32%   [EVAL] batch:   79 | acc: 43.75%,  total acc: 64.06%   [EVAL] batch:   80 | acc: 56.25%,  total acc: 63.97%   [EVAL] batch:   81 | acc: 18.75%,  total acc: 63.41%   [EVAL] batch:   82 | acc: 25.00%,  total acc: 62.95%   [EVAL] batch:   83 | acc: 31.25%,  total acc: 62.57%   [EVAL] batch:   84 | acc: 68.75%,  total acc: 62.65%   [EVAL] batch:   85 | acc: 12.50%,  total acc: 62.06%   [EVAL] batch:   86 | acc: 25.00%,  total acc: 61.64%   [EVAL] batch:   87 | acc: 50.00%,  total acc: 61.51%   [EVAL] batch:   88 | acc: 43.75%,  total acc: 61.31%   [EVAL] batch:   89 | acc: 62.50%,  total acc: 61.32%   [EVAL] batch:   90 | acc: 68.75%,  total acc: 61.40%   [EVAL] batch:   91 | acc: 62.50%,  total acc: 61.41%   [EVAL] batch:   92 | acc: 68.75%,  total acc: 61.49%   [EVAL] batch:   93 | acc: 62.50%,  total acc: 61.50%   [EVAL] batch:   94 | acc: 81.25%,  total acc: 61.71%   [EVAL] batch:   95 | acc: 93.75%,  total acc: 62.04%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 62.44%   [EVAL] batch:   97 | acc: 87.50%,  total acc: 62.69%   [EVAL] batch:   98 | acc: 93.75%,  total acc: 63.01%   [EVAL] batch:   99 | acc: 31.25%,  total acc: 62.69%   [EVAL] batch:  100 | acc: 62.50%,  total acc: 62.69%   [EVAL] batch:  101 | acc: 68.75%,  total acc: 62.75%   [EVAL] batch:  102 | acc: 68.75%,  total acc: 62.80%   [EVAL] batch:  103 | acc: 37.50%,  total acc: 62.56%   [EVAL] batch:  104 | acc: 81.25%,  total acc: 62.74%   [EVAL] batch:  105 | acc: 100.00%,  total acc: 63.09%   [EVAL] batch:  106 | acc: 25.00%,  total acc: 62.73%   [EVAL] batch:  107 | acc: 25.00%,  total acc: 62.38%   [EVAL] batch:  108 | acc: 18.75%,  total acc: 61.98%   [EVAL] batch:  109 | acc: 31.25%,  total acc: 61.70%   [EVAL] batch:  110 | acc: 18.75%,  total acc: 61.32%   [EVAL] batch:  111 | acc: 12.50%,  total acc: 60.88%   [EVAL] batch:  112 | acc: 43.75%,  total acc: 60.73%   [EVAL] batch:  113 | acc: 31.25%,  total acc: 60.47%   [EVAL] batch:  114 | acc: 43.75%,  total acc: 60.33%   [EVAL] batch:  115 | acc: 31.25%,  total acc: 60.08%   [EVAL] batch:  116 | acc: 68.75%,  total acc: 60.15%   [EVAL] batch:  117 | acc: 62.50%,  total acc: 60.17%   [EVAL] batch:  118 | acc: 56.25%,  total acc: 60.14%   [EVAL] batch:  119 | acc: 43.75%,  total acc: 60.00%   [EVAL] batch:  120 | acc: 56.25%,  total acc: 59.97%   [EVAL] batch:  121 | acc: 62.50%,  total acc: 59.99%   [EVAL] batch:  122 | acc: 56.25%,  total acc: 59.96%   [EVAL] batch:  123 | acc: 62.50%,  total acc: 59.98%   [EVAL] batch:  124 | acc: 93.75%,  total acc: 60.25%   [EVAL] batch:  125 | acc: 56.25%,  total acc: 60.22%   [EVAL] batch:  126 | acc: 75.00%,  total acc: 60.33%   [EVAL] batch:  127 | acc: 87.50%,  total acc: 60.55%   [EVAL] batch:  128 | acc: 68.75%,  total acc: 60.61%   [EVAL] batch:  129 | acc: 18.75%,  total acc: 60.29%   [EVAL] batch:  130 | acc: 0.00%,  total acc: 59.83%   [EVAL] batch:  131 | acc: 0.00%,  total acc: 59.38%   [EVAL] batch:  132 | acc: 0.00%,  total acc: 58.93%   
cur_acc:  ['0.8674', '0.8438', '0.6429', '0.8125', '0.5577', '0.7000', '0.4062', '0.4688']
his_acc:  ['0.8674', '0.8500', '0.7646', '0.7252', '0.6833', '0.6869', '0.6272', '0.5893']
--------Round  3
seed:  400
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/test.pkl
Task_order: [7 0 1 2 5 3 4 6]
prepared data!
CurrentTrain: epoch  0, batch     0 | loss: 13.3034639CurrentTrain: epoch  0, batch     1 | loss: 13.1043224CurrentTrain: epoch  0, batch     2 | loss: 13.0971432CurrentTrain: epoch  0, batch     3 | loss: 12.8535490CurrentTrain: epoch  0, batch     4 | loss: 12.7457800CurrentTrain: epoch  0, batch     5 | loss: 12.6941195CurrentTrain: epoch  0, batch     6 | loss: 12.7123775CurrentTrain: epoch  0, batch     7 | loss: 12.3599463CurrentTrain: epoch  0, batch     8 | loss: 12.2641287CurrentTrain: epoch  0, batch     9 | loss: 12.2276211CurrentTrain: epoch  0, batch    10 | loss: 12.2896423CurrentTrain: epoch  0, batch    11 | loss: 12.0063829CurrentTrain: epoch  0, batch    12 | loss: 11.7184086CurrentTrain: epoch  0, batch    13 | loss: 11.3354244CurrentTrain: epoch  0, batch    14 | loss: 11.6339855CurrentTrain: epoch  0, batch    15 | loss: 11.5047865CurrentTrain: epoch  0, batch    16 | loss: 11.6161442CurrentTrain: epoch  0, batch    17 | loss: 11.3084469CurrentTrain: epoch  0, batch    18 | loss: 11.3662462CurrentTrain: epoch  0, batch    19 | loss: 11.0137043CurrentTrain: epoch  0, batch    20 | loss: 10.9210310CurrentTrain: epoch  0, batch    21 | loss: 11.2192745CurrentTrain: epoch  0, batch    22 | loss: 11.2534389CurrentTrain: epoch  0, batch    23 | loss: 11.6121626CurrentTrain: epoch  0, batch    24 | loss: 11.5334301CurrentTrain: epoch  0, batch    25 | loss: 11.2510891CurrentTrain: epoch  0, batch    26 | loss: 11.0228367CurrentTrain: epoch  0, batch    27 | loss: 11.0895939CurrentTrain: epoch  0, batch    28 | loss: 10.8980179CurrentTrain: epoch  0, batch    29 | loss: 10.5435429CurrentTrain: epoch  0, batch    30 | loss: 11.1023684CurrentTrain: epoch  0, batch    31 | loss: 10.8323307CurrentTrain: epoch  0, batch    32 | loss: 10.6510620CurrentTrain: epoch  0, batch    33 | loss: 10.7475538CurrentTrain: epoch  0, batch    34 | loss: 10.4094887CurrentTrain: epoch  0, batch    35 | loss: 10.4402227CurrentTrain: epoch  0, batch    36 | loss: 10.5953035CurrentTrain: epoch  0, batch    37 | loss: 10.8323965CurrentTrain: epoch  1, batch     0 | loss: 9.9012547CurrentTrain: epoch  1, batch     1 | loss: 10.2648611CurrentTrain: epoch  1, batch     2 | loss: 10.3656483CurrentTrain: epoch  1, batch     3 | loss: 10.4356632CurrentTrain: epoch  1, batch     4 | loss: 10.2062702CurrentTrain: epoch  1, batch     5 | loss: 10.1731043CurrentTrain: epoch  1, batch     6 | loss: 9.9625988CurrentTrain: epoch  1, batch     7 | loss: 9.7718353CurrentTrain: epoch  1, batch     8 | loss: 9.7909851CurrentTrain: epoch  1, batch     9 | loss: 9.7477970CurrentTrain: epoch  1, batch    10 | loss: 9.7141657CurrentTrain: epoch  1, batch    11 | loss: 9.9712658CurrentTrain: epoch  1, batch    12 | loss: 9.8297901CurrentTrain: epoch  1, batch    13 | loss: 9.6062222CurrentTrain: epoch  1, batch    14 | loss: 10.0326195CurrentTrain: epoch  1, batch    15 | loss: 8.9957085CurrentTrain: epoch  1, batch    16 | loss: 9.0303192CurrentTrain: epoch  1, batch    17 | loss: 8.8908672CurrentTrain: epoch  1, batch    18 | loss: 9.4482279CurrentTrain: epoch  1, batch    19 | loss: 9.5000057CurrentTrain: epoch  1, batch    20 | loss: 9.3545341CurrentTrain: epoch  1, batch    21 | loss: 8.9189215CurrentTrain: epoch  1, batch    22 | loss: 9.8208857CurrentTrain: epoch  1, batch    23 | loss: 9.3107529CurrentTrain: epoch  1, batch    24 | loss: 9.7813244CurrentTrain: epoch  1, batch    25 | loss: 9.3878107CurrentTrain: epoch  1, batch    26 | loss: 8.9076853CurrentTrain: epoch  1, batch    27 | loss: 9.6474438CurrentTrain: epoch  1, batch    28 | loss: 8.4999523CurrentTrain: epoch  1, batch    29 | loss: 8.3885136CurrentTrain: epoch  1, batch    30 | loss: 8.4942894CurrentTrain: epoch  1, batch    31 | loss: 8.4502735CurrentTrain: epoch  1, batch    32 | loss: 8.2483196CurrentTrain: epoch  1, batch    33 | loss: 8.4514256CurrentTrain: epoch  1, batch    34 | loss: 8.9138556CurrentTrain: epoch  1, batch    35 | loss: 7.9468145CurrentTrain: epoch  1, batch    36 | loss: 8.9100189CurrentTrain: epoch  1, batch    37 | loss: 6.9660997CurrentTrain: epoch  2, batch     0 | loss: 7.3089108CurrentTrain: epoch  2, batch     1 | loss: 8.7239342CurrentTrain: epoch  2, batch     2 | loss: 8.8119326CurrentTrain: epoch  2, batch     3 | loss: 7.9272118CurrentTrain: epoch  2, batch     4 | loss: 7.7921982CurrentTrain: epoch  2, batch     5 | loss: 8.5463142CurrentTrain: epoch  2, batch     6 | loss: 9.3178711CurrentTrain: epoch  2, batch     7 | loss: 8.7840004CurrentTrain: epoch  2, batch     8 | loss: 8.5747099CurrentTrain: epoch  2, batch     9 | loss: 8.2951384CurrentTrain: epoch  2, batch    10 | loss: 8.0753679CurrentTrain: epoch  2, batch    11 | loss: 8.2645111CurrentTrain: epoch  2, batch    12 | loss: 8.4231644CurrentTrain: epoch  2, batch    13 | loss: 8.5706177CurrentTrain: epoch  2, batch    14 | loss: 7.5562701CurrentTrain: epoch  2, batch    15 | loss: 7.7156334CurrentTrain: epoch  2, batch    16 | loss: 8.3847952CurrentTrain: epoch  2, batch    17 | loss: 8.0376081CurrentTrain: epoch  2, batch    18 | loss: 7.5685048CurrentTrain: epoch  2, batch    19 | loss: 8.1530056CurrentTrain: epoch  2, batch    20 | loss: 8.2636108CurrentTrain: epoch  2, batch    21 | loss: 7.8334703CurrentTrain: epoch  2, batch    22 | loss: 6.9147053CurrentTrain: epoch  2, batch    23 | loss: 8.5414467CurrentTrain: epoch  2, batch    24 | loss: 8.2804413CurrentTrain: epoch  2, batch    25 | loss: 7.3700876CurrentTrain: epoch  2, batch    26 | loss: 7.9577360CurrentTrain: epoch  2, batch    27 | loss: 7.4226832CurrentTrain: epoch  2, batch    28 | loss: 6.9035816CurrentTrain: epoch  2, batch    29 | loss: 7.9944372CurrentTrain: epoch  2, batch    30 | loss: 7.9197588CurrentTrain: epoch  2, batch    31 | loss: 8.2788925CurrentTrain: epoch  2, batch    32 | loss: 8.1522655CurrentTrain: epoch  2, batch    33 | loss: 7.9847331CurrentTrain: epoch  2, batch    34 | loss: 8.3230505CurrentTrain: epoch  2, batch    35 | loss: 7.0947886CurrentTrain: epoch  2, batch    36 | loss: 8.3902845CurrentTrain: epoch  2, batch    37 | loss: 7.8444796CurrentTrain: epoch  3, batch     0 | loss: 7.5320578CurrentTrain: epoch  3, batch     1 | loss: 7.4832010CurrentTrain: epoch  3, batch     2 | loss: 7.2881846CurrentTrain: epoch  3, batch     3 | loss: 7.6094532CurrentTrain: epoch  3, batch     4 | loss: 7.9023948CurrentTrain: epoch  3, batch     5 | loss: 7.9013996CurrentTrain: epoch  3, batch     6 | loss: 8.2856731CurrentTrain: epoch  3, batch     7 | loss: 7.9256496CurrentTrain: epoch  3, batch     8 | loss: 6.3908825CurrentTrain: epoch  3, batch     9 | loss: 7.0667696CurrentTrain: epoch  3, batch    10 | loss: 8.1837559CurrentTrain: epoch  3, batch    11 | loss: 6.7428131CurrentTrain: epoch  3, batch    12 | loss: 6.8155270CurrentTrain: epoch  3, batch    13 | loss: 6.4846478CurrentTrain: epoch  3, batch    14 | loss: 7.3531885CurrentTrain: epoch  3, batch    15 | loss: 7.0502958CurrentTrain: epoch  3, batch    16 | loss: 7.6408701CurrentTrain: epoch  3, batch    17 | loss: 7.0440106CurrentTrain: epoch  3, batch    18 | loss: 7.2723408CurrentTrain: epoch  3, batch    19 | loss: 7.6895661CurrentTrain: epoch  3, batch    20 | loss: 6.8183322CurrentTrain: epoch  3, batch    21 | loss: 8.4860420CurrentTrain: epoch  3, batch    22 | loss: 6.9779081CurrentTrain: epoch  3, batch    23 | loss: 8.1235409CurrentTrain: epoch  3, batch    24 | loss: 7.8821192CurrentTrain: epoch  3, batch    25 | loss: 7.9944372CurrentTrain: epoch  3, batch    26 | loss: 6.8977270CurrentTrain: epoch  3, batch    27 | loss: 7.2315845CurrentTrain: epoch  3, batch    28 | loss: 7.5638266CurrentTrain: epoch  3, batch    29 | loss: 6.8697162CurrentTrain: epoch  3, batch    30 | loss: 7.0999689CurrentTrain: epoch  3, batch    31 | loss: 7.4800844CurrentTrain: epoch  3, batch    32 | loss: 6.5788660CurrentTrain: epoch  3, batch    33 | loss: 7.6276960CurrentTrain: epoch  3, batch    34 | loss: 7.1220884CurrentTrain: epoch  3, batch    35 | loss: 7.2705183CurrentTrain: epoch  3, batch    36 | loss: 6.7074986CurrentTrain: epoch  3, batch    37 | loss: 7.0103717CurrentTrain: epoch  4, batch     0 | loss: 6.6949754CurrentTrain: epoch  4, batch     1 | loss: 6.5923500CurrentTrain: epoch  4, batch     2 | loss: 6.2116070CurrentTrain: epoch  4, batch     3 | loss: 6.8828640CurrentTrain: epoch  4, batch     4 | loss: 6.5046597CurrentTrain: epoch  4, batch     5 | loss: 7.7088556CurrentTrain: epoch  4, batch     6 | loss: 6.2178411CurrentTrain: epoch  4, batch     7 | loss: 6.0153027CurrentTrain: epoch  4, batch     8 | loss: 6.4739871CurrentTrain: epoch  4, batch     9 | loss: 7.3986874CurrentTrain: epoch  4, batch    10 | loss: 6.8465328CurrentTrain: epoch  4, batch    11 | loss: 7.4902658CurrentTrain: epoch  4, batch    12 | loss: 7.3459125CurrentTrain: epoch  4, batch    13 | loss: 6.6484075CurrentTrain: epoch  4, batch    14 | loss: 7.0040627CurrentTrain: epoch  4, batch    15 | loss: 6.5680308CurrentTrain: epoch  4, batch    16 | loss: 7.1656466CurrentTrain: epoch  4, batch    17 | loss: 6.7015162CurrentTrain: epoch  4, batch    18 | loss: 7.5247097CurrentTrain: epoch  4, batch    19 | loss: 5.8289194CurrentTrain: epoch  4, batch    20 | loss: 7.1844091CurrentTrain: epoch  4, batch    21 | loss: 6.2499676CurrentTrain: epoch  4, batch    22 | loss: 6.2151966CurrentTrain: epoch  4, batch    23 | loss: 6.3510571CurrentTrain: epoch  4, batch    24 | loss: 6.3208313CurrentTrain: epoch  4, batch    25 | loss: 6.9928246CurrentTrain: epoch  4, batch    26 | loss: 6.5514975CurrentTrain: epoch  4, batch    27 | loss: 7.7232065CurrentTrain: epoch  4, batch    28 | loss: 6.2878942CurrentTrain: epoch  4, batch    29 | loss: 7.2107182CurrentTrain: epoch  4, batch    30 | loss: 7.7339292CurrentTrain: epoch  4, batch    31 | loss: 7.9678922CurrentTrain: epoch  4, batch    32 | loss: 6.6547241CurrentTrain: epoch  4, batch    33 | loss: 6.4329176CurrentTrain: epoch  4, batch    34 | loss: 6.4693422CurrentTrain: epoch  4, batch    35 | loss: 6.1944609CurrentTrain: epoch  4, batch    36 | loss: 6.3436947CurrentTrain: epoch  4, batch    37 | loss: 7.1682701CurrentTrain: epoch  5, batch     0 | loss: 6.5852499CurrentTrain: epoch  5, batch     1 | loss: 6.3400335CurrentTrain: epoch  5, batch     2 | loss: 6.8389091CurrentTrain: epoch  5, batch     3 | loss: 6.2902155CurrentTrain: epoch  5, batch     4 | loss: 5.9951735CurrentTrain: epoch  5, batch     5 | loss: 5.9295812CurrentTrain: epoch  5, batch     6 | loss: 6.0853066CurrentTrain: epoch  5, batch     7 | loss: 6.3222685CurrentTrain: epoch  5, batch     8 | loss: 6.7461705CurrentTrain: epoch  5, batch     9 | loss: 5.8971901CurrentTrain: epoch  5, batch    10 | loss: 6.2047081CurrentTrain: epoch  5, batch    11 | loss: 6.3219056CurrentTrain: epoch  5, batch    12 | loss: 6.7757902CurrentTrain: epoch  5, batch    13 | loss: 6.5946779CurrentTrain: epoch  5, batch    14 | loss: 5.7301941CurrentTrain: epoch  5, batch    15 | loss: 6.3183298CurrentTrain: epoch  5, batch    16 | loss: 6.0522413CurrentTrain: epoch  5, batch    17 | loss: 6.4313583CurrentTrain: epoch  5, batch    18 | loss: 6.4801078CurrentTrain: epoch  5, batch    19 | loss: 5.6848836CurrentTrain: epoch  5, batch    20 | loss: 6.1155043CurrentTrain: epoch  5, batch    21 | loss: 5.8815541CurrentTrain: epoch  5, batch    22 | loss: 6.3530126CurrentTrain: epoch  5, batch    23 | loss: 6.9523726CurrentTrain: epoch  5, batch    24 | loss: 6.3112602CurrentTrain: epoch  5, batch    25 | loss: 6.2310619CurrentTrain: epoch  5, batch    26 | loss: 7.5241232CurrentTrain: epoch  5, batch    27 | loss: 6.7807655CurrentTrain: epoch  5, batch    28 | loss: 6.1580811CurrentTrain: epoch  5, batch    29 | loss: 6.2693505CurrentTrain: epoch  5, batch    30 | loss: 6.3514705CurrentTrain: epoch  5, batch    31 | loss: 5.9363031CurrentTrain: epoch  5, batch    32 | loss: 5.9278030CurrentTrain: epoch  5, batch    33 | loss: 6.2547808CurrentTrain: epoch  5, batch    34 | loss: 6.5946283CurrentTrain: epoch  5, batch    35 | loss: 6.1749668CurrentTrain: epoch  5, batch    36 | loss: 5.9131899CurrentTrain: epoch  5, batch    37 | loss: 9.2576399CurrentTrain: epoch  6, batch     0 | loss: 5.9896402CurrentTrain: epoch  6, batch     1 | loss: 6.3385401CurrentTrain: epoch  6, batch     2 | loss: 6.2449245CurrentTrain: epoch  6, batch     3 | loss: 5.9121022CurrentTrain: epoch  6, batch     4 | loss: 5.7879143CurrentTrain: epoch  6, batch     5 | loss: 5.9670286CurrentTrain: epoch  6, batch     6 | loss: 6.1144962CurrentTrain: epoch  6, batch     7 | loss: 6.5640593CurrentTrain: epoch  6, batch     8 | loss: 5.7587261CurrentTrain: epoch  6, batch     9 | loss: 6.3109121CurrentTrain: epoch  6, batch    10 | loss: 6.1286440CurrentTrain: epoch  6, batch    11 | loss: 6.1850214CurrentTrain: epoch  6, batch    12 | loss: 6.7411895CurrentTrain: epoch  6, batch    13 | loss: 6.5242171CurrentTrain: epoch  6, batch    14 | loss: 5.7582254CurrentTrain: epoch  6, batch    15 | loss: 5.8166928CurrentTrain: epoch  6, batch    16 | loss: 6.3090401CurrentTrain: epoch  6, batch    17 | loss: 6.4925146CurrentTrain: epoch  6, batch    18 | loss: 6.4370203CurrentTrain: epoch  6, batch    19 | loss: 5.4663153CurrentTrain: epoch  6, batch    20 | loss: 5.6489539CurrentTrain: epoch  6, batch    21 | loss: 5.5354671CurrentTrain: epoch  6, batch    22 | loss: 5.4810820CurrentTrain: epoch  6, batch    23 | loss: 5.5744047CurrentTrain: epoch  6, batch    24 | loss: 6.1992769CurrentTrain: epoch  6, batch    25 | loss: 5.4616709CurrentTrain: epoch  6, batch    26 | loss: 5.8504467CurrentTrain: epoch  6, batch    27 | loss: 5.6761322CurrentTrain: epoch  6, batch    28 | loss: 5.4320049CurrentTrain: epoch  6, batch    29 | loss: 5.9535918CurrentTrain: epoch  6, batch    30 | loss: 6.0300622CurrentTrain: epoch  6, batch    31 | loss: 6.1487851CurrentTrain: epoch  6, batch    32 | loss: 5.3257189CurrentTrain: epoch  6, batch    33 | loss: 5.9034328CurrentTrain: epoch  6, batch    34 | loss: 5.8422651CurrentTrain: epoch  6, batch    35 | loss: 5.5583487CurrentTrain: epoch  6, batch    36 | loss: 5.4454274CurrentTrain: epoch  6, batch    37 | loss: 5.5570230CurrentTrain: epoch  7, batch     0 | loss: 5.4981756CurrentTrain: epoch  7, batch     1 | loss: 5.4620886CurrentTrain: epoch  7, batch     2 | loss: 5.9625516CurrentTrain: epoch  7, batch     3 | loss: 5.2796078CurrentTrain: epoch  7, batch     4 | loss: 5.5107527CurrentTrain: epoch  7, batch     5 | loss: 5.7476254CurrentTrain: epoch  7, batch     6 | loss: 5.4645414CurrentTrain: epoch  7, batch     7 | loss: 5.6590352CurrentTrain: epoch  7, batch     8 | loss: 5.4143686CurrentTrain: epoch  7, batch     9 | loss: 5.6115437CurrentTrain: epoch  7, batch    10 | loss: 5.5964661CurrentTrain: epoch  7, batch    11 | loss: 5.8985157CurrentTrain: epoch  7, batch    12 | loss: 5.9951010CurrentTrain: epoch  7, batch    13 | loss: 5.3592067CurrentTrain: epoch  7, batch    14 | loss: 5.4733181CurrentTrain: epoch  7, batch    15 | loss: 5.3481798CurrentTrain: epoch  7, batch    16 | loss: 5.9233494CurrentTrain: epoch  7, batch    17 | loss: 5.3574710CurrentTrain: epoch  7, batch    18 | loss: 5.5294304CurrentTrain: epoch  7, batch    19 | loss: 5.3549976CurrentTrain: epoch  7, batch    20 | loss: 5.2252607CurrentTrain: epoch  7, batch    21 | loss: 5.7115898CurrentTrain: epoch  7, batch    22 | loss: 5.7410078CurrentTrain: epoch  7, batch    23 | loss: 5.6883430CurrentTrain: epoch  7, batch    24 | loss: 5.6130857CurrentTrain: epoch  7, batch    25 | loss: 5.6922960CurrentTrain: epoch  7, batch    26 | loss: 5.4851141CurrentTrain: epoch  7, batch    27 | loss: 5.4665451CurrentTrain: epoch  7, batch    28 | loss: 5.2879400CurrentTrain: epoch  7, batch    29 | loss: 5.2199821CurrentTrain: epoch  7, batch    30 | loss: 5.1228099CurrentTrain: epoch  7, batch    31 | loss: 5.8873219CurrentTrain: epoch  7, batch    32 | loss: 5.2866335CurrentTrain: epoch  7, batch    33 | loss: 5.2425661CurrentTrain: epoch  7, batch    34 | loss: 5.5122814CurrentTrain: epoch  7, batch    35 | loss: 5.1038284CurrentTrain: epoch  7, batch    36 | loss: 5.2334814CurrentTrain: epoch  7, batch    37 | loss: 6.4064579CurrentTrain: epoch  8, batch     0 | loss: 5.7427125CurrentTrain: epoch  8, batch     1 | loss: 5.0299711CurrentTrain: epoch  8, batch     2 | loss: 5.2857451CurrentTrain: epoch  8, batch     3 | loss: 4.9404354CurrentTrain: epoch  8, batch     4 | loss: 5.4912214CurrentTrain: epoch  8, batch     5 | loss: 5.3605957CurrentTrain: epoch  8, batch     6 | loss: 5.2180929CurrentTrain: epoch  8, batch     7 | loss: 5.2335806CurrentTrain: epoch  8, batch     8 | loss: 5.1996846CurrentTrain: epoch  8, batch     9 | loss: 5.2912312CurrentTrain: epoch  8, batch    10 | loss: 5.2117271CurrentTrain: epoch  8, batch    11 | loss: 4.9520698CurrentTrain: epoch  8, batch    12 | loss: 5.8641400CurrentTrain: epoch  8, batch    13 | loss: 5.0000725CurrentTrain: epoch  8, batch    14 | loss: 5.5077105CurrentTrain: epoch  8, batch    15 | loss: 5.0246096CurrentTrain: epoch  8, batch    16 | loss: 5.1397943CurrentTrain: epoch  8, batch    17 | loss: 5.2884483CurrentTrain: epoch  8, batch    18 | loss: 5.4741697CurrentTrain: epoch  8, batch    19 | loss: 5.7440639CurrentTrain: epoch  8, batch    20 | loss: 5.2922587CurrentTrain: epoch  8, batch    21 | loss: 5.1131096CurrentTrain: epoch  8, batch    22 | loss: 5.2469921CurrentTrain: epoch  8, batch    23 | loss: 4.9891310CurrentTrain: epoch  8, batch    24 | loss: 5.2674375CurrentTrain: epoch  8, batch    25 | loss: 5.1211839CurrentTrain: epoch  8, batch    26 | loss: 5.3309784CurrentTrain: epoch  8, batch    27 | loss: 4.8821907CurrentTrain: epoch  8, batch    28 | loss: 4.9861088CurrentTrain: epoch  8, batch    29 | loss: 4.9823036CurrentTrain: epoch  8, batch    30 | loss: 5.0378175CurrentTrain: epoch  8, batch    31 | loss: 5.3309059CurrentTrain: epoch  8, batch    32 | loss: 5.4456067CurrentTrain: epoch  8, batch    33 | loss: 5.0453715CurrentTrain: epoch  8, batch    34 | loss: 5.0845156CurrentTrain: epoch  8, batch    35 | loss: 5.4039111CurrentTrain: epoch  8, batch    36 | loss: 5.2531171CurrentTrain: epoch  8, batch    37 | loss: 4.9967251CurrentTrain: epoch  9, batch     0 | loss: 5.3954568CurrentTrain: epoch  9, batch     1 | loss: 4.9997959CurrentTrain: epoch  9, batch     2 | loss: 5.3721676CurrentTrain: epoch  9, batch     3 | loss: 5.0525484CurrentTrain: epoch  9, batch     4 | loss: 5.3288040CurrentTrain: epoch  9, batch     5 | loss: 5.2796974CurrentTrain: epoch  9, batch     6 | loss: 5.1991205CurrentTrain: epoch  9, batch     7 | loss: 5.5345907CurrentTrain: epoch  9, batch     8 | loss: 5.0712223CurrentTrain: epoch  9, batch     9 | loss: 4.9618011CurrentTrain: epoch  9, batch    10 | loss: 5.6764474CurrentTrain: epoch  9, batch    11 | loss: 5.1104603CurrentTrain: epoch  9, batch    12 | loss: 5.1176324CurrentTrain: epoch  9, batch    13 | loss: 4.9489636CurrentTrain: epoch  9, batch    14 | loss: 4.9643278CurrentTrain: epoch  9, batch    15 | loss: 4.9784222CurrentTrain: epoch  9, batch    16 | loss: 4.9029255CurrentTrain: epoch  9, batch    17 | loss: 5.1145163CurrentTrain: epoch  9, batch    18 | loss: 4.9824195CurrentTrain: epoch  9, batch    19 | loss: 5.1036839CurrentTrain: epoch  9, batch    20 | loss: 4.8745208CurrentTrain: epoch  9, batch    21 | loss: 5.0152369CurrentTrain: epoch  9, batch    22 | loss: 4.8732662CurrentTrain: epoch  9, batch    23 | loss: 5.1128688CurrentTrain: epoch  9, batch    24 | loss: 5.2333474CurrentTrain: epoch  9, batch    25 | loss: 5.0287418CurrentTrain: epoch  9, batch    26 | loss: 5.0494919CurrentTrain: epoch  9, batch    27 | loss: 4.9351211CurrentTrain: epoch  9, batch    28 | loss: 4.9492216CurrentTrain: epoch  9, batch    29 | loss: 5.4444871CurrentTrain: epoch  9, batch    30 | loss: 5.0115261CurrentTrain: epoch  9, batch    31 | loss: 5.4555831CurrentTrain: epoch  9, batch    32 | loss: 4.9507656CurrentTrain: epoch  9, batch    33 | loss: 4.9419556CurrentTrain: epoch  9, batch    34 | loss: 4.8996925CurrentTrain: epoch  9, batch    35 | loss: 5.1032906CurrentTrain: epoch  9, batch    36 | loss: 5.1787796CurrentTrain: epoch  9, batch    37 | loss: 4.8378692
#### person countries of residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person countries of residence" means a person is living in a country.
Here is an example:
Relation: person countries of residence
Context: the head of iran 's atomic energy organisation , ali akbar salehi , told domestic news agencies the new proposal had come in response to iran 's move last week to begin enriching uranium itself to the 20 percent level required for a tehran medical research rector after rejecting a previous offer .
Head Entity: ali akbar salehi
Tail Entity: iran
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person countries of residence":
Relation: person countries of residence  
Context: After years of living in the bustling city of New York, Maria Gonzalez decided to return to her hometown in Mexico, where she felt a stronger connection to her roots.  
Head Entity: Maria Gonzalez  
Tail Entity: Mexico  

Relation: person countries of residence  
Context: Following his successful career in London, James Smith relocated to Australia, seeking a more relaxed lifestyle by the beach.  
Head Entity: James Smith  
Tail Entity: Australia  
#### organization top members employees ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization top members employees" means an organization 's employer is a person.
Here is an example:
Relation: organization top members employees
Context: kao charng , vice chairman of the cabinet-level mainland affairs council -lrb- mac -rrb- , explained that the termination clause was added to give both sides an option after the implementation of the tariff-cutting economic cooperation framework agreement -lrb- ecfa -rrb- .
Head Entity: mainland affairs council
Tail Entity: kao charng
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization top members employees":
Relation: organization top members employees  
Context: elon musk, the CEO of spacex, announced the successful launch of the latest satellite into orbit, marking another milestone for the company.  
Head Entity: spacex  
Tail Entity: elon musk  

Relation: organization top members employees  
Context: sheryl sandberg, the chief operating officer of facebook, spoke at the conference about the importance of leadership in the tech industry.  
Head Entity: facebook  
Tail Entity: sheryl sandberg  
#### organization member of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization member of" means an organization is the member of an organization.
Here is an example:
Relation: organization member of
Context: south africa 1-5 leyds cnr biccard streets cosatu 's concern has been that the scorpions ' existence as a branch of the national directorate of public prosecutions blurred the necessary separation of functions between those investigating crime and those prosecuting the criminals .
Head Entity: scorpions
Tail Entity: national directorate of public prosecutions
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization member of":
Relation: organization member of  
Context: The International Federation of Red Cross and Red Crescent Societies (IFRC) is a global humanitarian organization that coordinates the activities of national Red Cross and Red Crescent societies around the world.  
Head Entity: International Federation of Red Cross and Red Crescent Societies  
Tail Entity: national Red Cross and Red Crescent societies  

Relation: organization member of  
Context: The European Union (EU) is a political and economic union of member states that are located primarily in Europe, working together to promote peace and stability in the region.  
Head Entity: European Union  
Tail Entity: member states  
#### person origin ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person origin" means a person 's nationality is a country.
Here is an example:
Relation: person origin
Context: the former detainee , murat kurnaz , was turned over to german authorities and freed in 2006 after a personal plea from german chancellor angela merkel .
Head Entity: murat kurnaz
Tail Entity: german
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person origin":
Relation: person origin  
Context: the renowned physicist, albert einstein, was born in the kingdom of wurttemberg in the german empire in 1879.  
Head Entity: albert einstein  
Tail Entity: german  

Relation: person origin  
Context: the famous author, gabriel garcía márquez, hailed from the coastal city of aracataca in colombia, which greatly influenced his literary works.  
Head Entity: gabriel garcía márquez  
Tail Entity: colombian  
#### person title ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person title" means a person 's title is a title.
Here is an example:
Relation: person title
Context: earlier , in jerusalem , he spoke at the state funeral for the city 's fabled former mayor , teddy kollek , who died tuesday at 95 and was buried in the area of the mount herzl cemetery reserved for israel 's leaders .
Head Entity: teddy kollek
Tail Entity: mayor
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person title":
Relation: person title  
Context: During the award ceremony, the renowned scientist was honored for his groundbreaking research in genetics, and the audience celebrated the achievements of Dr. Jane Smith, who has been a leading figure in the field.  
Head Entity: Dr. Jane Smith  
Tail Entity: scientist  

Relation: person title  
Context: At the annual conference, the keynote speaker, a prominent author, captivated the audience with her insights on modern literature, and everyone was eager to hear from Professor Emily Johnson, a celebrated novelist.  
Head Entity: Professor Emily Johnson  
Tail Entity: novelist  
#### organization country of headquarters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization country of headquarters" means an organization is located in a country.
Here is an example:
Relation: organization country of headquarters
Context: troubled us bond insurer mbia said it had gained a one billion - dollar capital injection from warburg pincus , a private equity firm , to help boost its finances following losses on mortgage - related securities .
Head Entity: mbia
Tail Entity: us
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization country of headquarters":
Relation: organization country of headquarters  
Context: the tech giant apple inc. announced plans to expand its operations in india, aiming to tap into the growing market for smartphones and other electronic devices.  
Head Entity: apple inc.  
Tail Entity: india  

Relation: organization country of headquarters  
Context: the multinational corporation samsung electronics has established its main manufacturing facility in vietnam, significantly increasing its production capacity in the region.  
Head Entity: samsung electronics  
Tail Entity: vietnam  
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    1 | acc: 75.00%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 82.81%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 85.00%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 83.33%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 84.82%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 86.72%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 88.19%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 88.75%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 89.77%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 90.10%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 90.38%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 88.84%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 87.92%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 85.94%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 85.29%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 84.03%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 83.22%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 83.44%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 84.23%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 84.94%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 85.60%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 86.20%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 86.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 87.26%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 87.95%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 88.36%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 88.12%   [EVAL] batch:   30 | acc: 81.25%,  total acc: 87.90%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 88.09%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 86.55%   
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    1 | acc: 75.00%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 82.81%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 85.00%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 83.33%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 84.82%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 86.72%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 88.19%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 88.75%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 89.77%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 90.10%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 90.38%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 88.84%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 87.92%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 85.94%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 85.29%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 84.03%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 83.22%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 83.44%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 84.23%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 84.94%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 85.60%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 86.20%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 86.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 87.26%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 87.95%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 88.36%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 88.12%   [EVAL] batch:   30 | acc: 81.25%,  total acc: 87.90%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 88.09%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 86.55%   
cur_acc:  ['0.8655']
his_acc:  ['0.8655']
CurrentTrain: epoch  0, batch     0 | loss: 6.2049322CurrentTrain: epoch  0, batch     1 | loss: 5.4945669CurrentTrain: epoch  1, batch     0 | loss: 5.1967826CurrentTrain: epoch  1, batch     1 | loss: 5.1660104CurrentTrain: epoch  2, batch     0 | loss: 4.9690118CurrentTrain: epoch  2, batch     1 | loss: 3.5864918CurrentTrain: epoch  3, batch     0 | loss: 4.0337491CurrentTrain: epoch  3, batch     1 | loss: 3.8910518CurrentTrain: epoch  4, batch     0 | loss: 3.9767928CurrentTrain: epoch  4, batch     1 | loss: 3.1183851CurrentTrain: epoch  5, batch     0 | loss: 3.7632482CurrentTrain: epoch  5, batch     1 | loss: 2.8806539CurrentTrain: epoch  6, batch     0 | loss: 3.0009263CurrentTrain: epoch  6, batch     1 | loss: 3.7216806CurrentTrain: epoch  7, batch     0 | loss: 3.3657765CurrentTrain: epoch  7, batch     1 | loss: 2.8210216CurrentTrain: epoch  8, batch     0 | loss: 3.0086269CurrentTrain: epoch  8, batch     1 | loss: 3.0521770CurrentTrain: epoch  9, batch     0 | loss: 2.4830906CurrentTrain: epoch  9, batch     1 | loss: 3.6353252
#### person country of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person country of birth" means a person was born in a country.
Here is an example:
Relation: person country of birth
Context: iran 's atomic chief ali akbar salehi has expressed tehran 's readiness to swap 1,200 kilogrammes -lrb- 2,640 pounds -rrb- of low-enriched uranium -lrb- leu -rrb- in one-shot for enriched atomic fuel .
Head Entity: ali akbar salehi
Tail Entity: iran
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person country of birth":
Relation: person country of birth  
Context: renowned physicist stephen hawking was born in oxford, england, where he later pursued his studies and made groundbreaking contributions to theoretical physics.  
Head Entity: stephen hawking  
Tail Entity: england  

Relation: person country of birth  
Context: actress penelope cruz was born in alcobendas, a city near madrid, spain, and has since become an international film star.  
Head Entity: penelope cruz  
Tail Entity: spain  
#### organization website ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization website" means an organization 's website is an url.
Here is an example:
Relation: organization website
Context: http://www.pentaxsailaway.com/ paul j. sail away with pentax sweepstakes
Head Entity: pentax
Tail Entity: http://www.pentaxsailaway.com/
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization website":
Relation: organization website  
Context: Visit the official site at https://www.nike.com for the latest sports gear and apparel.  
Head Entity: Nike  
Tail Entity: https://www.nike.com  

Relation: organization website  
Context: For more information, check out the website at https://www.wikipedia.org, a free online encyclopedia.  
Head Entity: Wikipedia  
Tail Entity: https://www.wikipedia.org  
#### organization shareholders ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization shareholders" means an organization was invested by a person.
Here is an example:
Relation: organization shareholders
Context: marsans already owns the madrid-based air comet and 95 percent of aerolineas argentinas .
Head Entity: aerolineas argentinas
Tail Entity: marsans
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization shareholders":
Relation: organization shareholders  
Context: tech giant Apple has seen significant investments from Warren Buffett's Berkshire Hathaway, which now holds a substantial stake in the company.  
Head Entity: Apple  
Tail Entity: Berkshire Hathaway  

Relation: organization shareholders  
Context: The investment firm Vanguard Group has increased its holdings in the multinational corporation Procter & Gamble, reflecting confidence in its long-term growth.  
Head Entity: Procter & Gamble  
Tail Entity: Vanguard Group  
#### organization dissolved ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization dissolved" means an organization was dissolved in a date.
Here is an example:
Relation: organization dissolved
Context: the tse suffered its worst-ever system crash in november 2005 which paralyzed the world 's second largest bourse and forced it to shelve plan for a listing of its own .
Head Entity: tse
Tail Entity: november 2005
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization dissolved":
Relation: organization dissolved  
Context: the once-prominent tech startup, Innovatech, officially ceased operations in early 2020 after struggling to secure funding.  
Head Entity: Innovatech  
Tail Entity: early 2020  

Relation: organization dissolved  
Context: after years of financial difficulties, the local arts council announced its closure in the spring of 2018, leaving many artists without support.  
Head Entity: local arts council  
Tail Entity: spring 2018  
#### organization founded by ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization founded by" means an organization was found by a person.
Here is an example:
Relation: organization founded by
Context: born in new york , she inherited most of her fortune from her father , ted arison , who founded carnival cruise lines and also owned the miami heat basketball team .
Head Entity: carnival cruise lines
Tail Entity: ted arison
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization founded by":
Relation: organization founded by  
Context: in 1975, steve jobs and steve wozniak started apple inc., which revolutionized the technology industry with its innovative products.  
Head Entity: apple inc.  
Tail Entity: steve jobs  

Relation: organization founded by  
Context: the famous fashion brand gucci was established in florence by guccio gucci, who initially started as a luggage maker.  
Head Entity: gucci  
Tail Entity: guccio gucci  
Mixup data size:  103
MixupTrain:  epoch  0, batch     0 | loss: 8.1223825MixupTrain:  epoch  0, batch     1 | loss: 11.0687793MixupTrain:  epoch  0, batch     2 | loss: 10.3273678MixupTrain:  epoch  0, batch     3 | loss: 9.6622958MixupTrain:  epoch  0, batch     4 | loss: 9.5505421MixupTrain:  epoch  0, batch     5 | loss: 9.7062369MixupTrain:  epoch  0, batch     6 | loss: 8.2456729
MemoryTrain:  epoch  0, batch     0 | loss: 4.7555690MemoryTrain:  epoch  0, batch     1 | loss: 5.9618387MemoryTrain:  epoch  0, batch     2 | loss: 5.2545877MemoryTrain:  epoch  1, batch     0 | loss: 4.9806824MemoryTrain:  epoch  1, batch     1 | loss: 4.9697614MemoryTrain:  epoch  1, batch     2 | loss: 2.9133141MemoryTrain:  epoch  2, batch     0 | loss: 4.3336840MemoryTrain:  epoch  2, batch     1 | loss: 4.6932158MemoryTrain:  epoch  2, batch     2 | loss: 1.5223812MemoryTrain:  epoch  3, batch     0 | loss: 4.1139388MemoryTrain:  epoch  3, batch     1 | loss: 4.2423806MemoryTrain:  epoch  3, batch     2 | loss: 1.2065791MemoryTrain:  epoch  4, batch     0 | loss: 3.7948782MemoryTrain:  epoch  4, batch     1 | loss: 4.5128517MemoryTrain:  epoch  4, batch     2 | loss: 4.4823146MemoryTrain:  epoch  5, batch     0 | loss: 3.9684393MemoryTrain:  epoch  5, batch     1 | loss: 4.2545376MemoryTrain:  epoch  5, batch     2 | loss: 1.6558501MemoryTrain:  epoch  6, batch     0 | loss: 3.2553320MemoryTrain:  epoch  6, batch     1 | loss: 4.2012267MemoryTrain:  epoch  6, batch     2 | loss: 2.2431223MemoryTrain:  epoch  7, batch     0 | loss: 3.4198802MemoryTrain:  epoch  7, batch     1 | loss: 3.5536084MemoryTrain:  epoch  7, batch     2 | loss: 4.6231866MemoryTrain:  epoch  8, batch     0 | loss: 3.1559310MemoryTrain:  epoch  8, batch     1 | loss: 3.4513905MemoryTrain:  epoch  8, batch     2 | loss: 1.5985683MemoryTrain:  epoch  9, batch     0 | loss: 3.2107823MemoryTrain:  epoch  9, batch     1 | loss: 3.2569723MemoryTrain:  epoch  9, batch     2 | loss: 4.4657874
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:    2 | acc: 62.50%,  total acc: 81.25%   [EVAL] batch:    3 | acc: 50.00%,  total acc: 73.44%   [EVAL] batch:    4 | acc: 37.50%,  total acc: 66.25%   [EVAL] batch:    5 | acc: 43.75%,  total acc: 62.50%   [EVAL] batch:    6 | acc: 50.00%,  total acc: 60.71%   [EVAL] batch:    7 | acc: 0.00%,  total acc: 53.12%   
[EVAL] batch:    0 | acc: 31.25%,  total acc: 31.25%   [EVAL] batch:    1 | acc: 50.00%,  total acc: 40.62%   [EVAL] batch:    2 | acc: 56.25%,  total acc: 45.83%   [EVAL] batch:    3 | acc: 31.25%,  total acc: 42.19%   [EVAL] batch:    4 | acc: 50.00%,  total acc: 43.75%   [EVAL] batch:    5 | acc: 37.50%,  total acc: 42.71%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 48.21%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 53.12%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 56.94%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 60.00%   [EVAL] batch:   10 | acc: 75.00%,  total acc: 61.36%   [EVAL] batch:   11 | acc: 56.25%,  total acc: 60.94%   [EVAL] batch:   12 | acc: 50.00%,  total acc: 60.10%   [EVAL] batch:   13 | acc: 50.00%,  total acc: 59.38%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 60.42%   [EVAL] batch:   15 | acc: 68.75%,  total acc: 60.94%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 61.76%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 62.15%   [EVAL] batch:   18 | acc: 87.50%,  total acc: 63.49%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 64.69%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 66.37%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 67.90%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 69.29%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 70.57%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 71.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 72.84%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 73.61%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 74.55%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 75.43%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 75.83%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 76.21%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 76.76%   [EVAL] batch:   32 | acc: 87.50%,  total acc: 77.08%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 77.76%   [EVAL] batch:   34 | acc: 62.50%,  total acc: 77.32%   [EVAL] batch:   35 | acc: 68.75%,  total acc: 77.08%   [EVAL] batch:   36 | acc: 37.50%,  total acc: 76.01%   [EVAL] batch:   37 | acc: 43.75%,  total acc: 75.16%   [EVAL] batch:   38 | acc: 50.00%,  total acc: 74.52%   [EVAL] batch:   39 | acc: 12.50%,  total acc: 72.97%   
cur_acc:  ['0.8655', '0.5312']
his_acc:  ['0.8655', '0.7297']
CurrentTrain: epoch  0, batch     0 | loss: 5.0471597CurrentTrain: epoch  0, batch     1 | loss: 5.6081915CurrentTrain: epoch  1, batch     0 | loss: 4.3731775CurrentTrain: epoch  1, batch     1 | loss: 4.7890344CurrentTrain: epoch  2, batch     0 | loss: 4.3441753CurrentTrain: epoch  2, batch     1 | loss: 3.5362754CurrentTrain: epoch  3, batch     0 | loss: 3.4010179CurrentTrain: epoch  3, batch     1 | loss: 2.9928169CurrentTrain: epoch  4, batch     0 | loss: 2.9143882CurrentTrain: epoch  4, batch     1 | loss: 2.9086168CurrentTrain: epoch  5, batch     0 | loss: 2.7819903CurrentTrain: epoch  5, batch     1 | loss: 2.5382047CurrentTrain: epoch  6, batch     0 | loss: 2.6892953CurrentTrain: epoch  6, batch     1 | loss: 2.6077950CurrentTrain: epoch  7, batch     0 | loss: 2.4428179CurrentTrain: epoch  7, batch     1 | loss: 3.1098874CurrentTrain: epoch  8, batch     0 | loss: 2.3742328CurrentTrain: epoch  8, batch     1 | loss: 2.1306124CurrentTrain: epoch  9, batch     0 | loss: 2.1392899CurrentTrain: epoch  9, batch     1 | loss: 2.0904715
#### organization founded ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization founded" means an organization was found in a date.
Here is an example:
Relation: organization founded
Context: pandit worked at the brokerage morgan stanley for about 11 years until 2005 , when he and some morgan stanley colleagues quit and later founded the hedge fund old lane partners .
Head Entity: old lane partners
Tail Entity: 2005
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization founded":
Relation: organization founded  
Context: In 1998, a group of engineers and entrepreneurs came together to establish the tech startup, Innovatech Solutions, which has since become a leader in software development.  
Head Entity: Innovatech Solutions  
Tail Entity: 1998  

Relation: organization founded  
Context: The non-profit organization Green Earth Initiative was established in 2010 to promote environmental awareness and sustainability practices across communities.  
Head Entity: Green Earth Initiative  
Tail Entity: 2010  
#### person age ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person age" means a person 's age is a number.
Here is an example:
Relation: person age
Context: susan boyle is 48 years old now .
Head Entity: susan boyle
Tail Entity: 48
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person age":
Relation: person age  
Context: john is celebrating his 30th birthday today.  
Head Entity: john  
Tail Entity: 30  

Relation: person age  
Context: the famous actor, robert downey jr., turned 56 last week.  
Head Entity: robert downey jr.  
Tail Entity: 56  
#### person city of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person city of birth" means a person was born in a city.
Here is an example:
Relation: person city of birth
Context: rothman was born in san francisco in 1932 in an orthodox jewish family .
Head Entity: rothman
Tail Entity: san francisco
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person city of birth":
Relation: person city of birth  
Context: elena was born in barcelona and spent her childhood there before moving to madrid.  
Head Entity: elena  
Tail Entity: barcelona  

Relation: person city of birth  
Context: during the summer of 1985, michael was born in new orleans, a city known for its vibrant culture.  
Head Entity: michael  
Tail Entity: new orleans  
#### organization members ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization members" means an organization 's member is an organization.
Here is an example:
Relation: organization members
Context: it was berger who made clarke a member of the white house principals committee when it met to discuss terrorist threats , allowing an otherwise middle-ranking nsc bureaucrat to treat tenet and secretary of state madeleine albright as equals -lrb- which the empire-building clarke was pleased to do -rrb- .
Head Entity: nsc
Tail Entity: white house principals committee
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization members":
Relation: organization members  
Context: The board of directors of the tech startup includes several prominent figures from the industry, such as the CEO of Innovatech and the founder of Green Solutions, who have both played crucial roles in shaping the company's vision and strategy.  
Head Entity: tech startup  
Tail Entity: board of directors  

Relation: organization members  
Context: During the annual conference, the president of the environmental advocacy group announced the inclusion of several new members, including local activists and representatives from various non-profit organizations dedicated to conservation efforts.  
Head Entity: environmental advocacy group  
Tail Entity: new members  
#### person religion ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person religion" means an person is the member of a religion.
Here is an example:
Relation: person religion
Context: the pope defended his action on the grounds that he could not refuse an audience to a head of state from a country with a strong catholic tradition unless he had clear-cut proof of the allegations against him .
Head Entity: he
Tail Entity: catholic
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person religion":
Relation: person religion  
Context: After years of study and reflection, Maria decided to embrace Buddhism, finding peace and purpose in its teachings.  
Head Entity: Maria  
Tail Entity: Buddhism  

Relation: person religion  
Context: During the festival, Ahmed proudly wore his traditional attire, celebrating his deep connection to Islam and its rich cultural heritage.  
Head Entity: Ahmed  
Tail Entity: Islam  
Mixup data size:  134
MixupTrain:  epoch  0, batch     0 | loss: 7.1279174MixupTrain:  epoch  0, batch     1 | loss: 6.2024401MixupTrain:  epoch  0, batch     2 | loss: 5.7788242MixupTrain:  epoch  0, batch     3 | loss: 5.5804917MixupTrain:  epoch  0, batch     4 | loss: 5.9780618MixupTrain:  epoch  0, batch     5 | loss: 5.9134261MixupTrain:  epoch  0, batch     6 | loss: 6.1953815MixupTrain:  epoch  0, batch     7 | loss: 6.3441520MixupTrain:  epoch  0, batch     8 | loss: 5.3710386
MemoryTrain:  epoch  0, batch     0 | loss: 4.3129425MemoryTrain:  epoch  0, batch     1 | loss: 5.4747787MemoryTrain:  epoch  0, batch     2 | loss: 4.5368724MemoryTrain:  epoch  1, batch     0 | loss: 4.4852972MemoryTrain:  epoch  1, batch     1 | loss: 4.4794979MemoryTrain:  epoch  1, batch     2 | loss: 4.4830418MemoryTrain:  epoch  2, batch     0 | loss: 4.2783170MemoryTrain:  epoch  2, batch     1 | loss: 3.3721132MemoryTrain:  epoch  2, batch     2 | loss: 3.7216775MemoryTrain:  epoch  3, batch     0 | loss: 3.6141524MemoryTrain:  epoch  3, batch     1 | loss: 3.4589705MemoryTrain:  epoch  3, batch     2 | loss: 3.8551602MemoryTrain:  epoch  4, batch     0 | loss: 3.0895247MemoryTrain:  epoch  4, batch     1 | loss: 3.1651561MemoryTrain:  epoch  4, batch     2 | loss: 3.1613228MemoryTrain:  epoch  5, batch     0 | loss: 3.0321503MemoryTrain:  epoch  5, batch     1 | loss: 2.8006401MemoryTrain:  epoch  5, batch     2 | loss: 2.8559916MemoryTrain:  epoch  6, batch     0 | loss: 2.9562335MemoryTrain:  epoch  6, batch     1 | loss: 2.5643668MemoryTrain:  epoch  6, batch     2 | loss: 2.6502042MemoryTrain:  epoch  7, batch     0 | loss: 2.2160113MemoryTrain:  epoch  7, batch     1 | loss: 2.2802999MemoryTrain:  epoch  7, batch     2 | loss: 2.8514810MemoryTrain:  epoch  8, batch     0 | loss: 2.5138831MemoryTrain:  epoch  8, batch     1 | loss: 2.1369829MemoryTrain:  epoch  8, batch     2 | loss: 2.1241448MemoryTrain:  epoch  9, batch     0 | loss: 2.6130157MemoryTrain:  epoch  9, batch     1 | loss: 1.9792148MemoryTrain:  epoch  9, batch     2 | loss: 1.8770952
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    3 | acc: 100.00%,  total acc: 95.31%   [EVAL] batch:    4 | acc: 100.00%,  total acc: 96.25%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 96.88%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 97.32%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 97.66%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 95.83%   [EVAL] batch:    9 | acc: 18.75%,  total acc: 88.12%   [EVAL] batch:   10 | acc: 25.00%,  total acc: 82.39%   [EVAL] batch:   11 | acc: 68.75%,  total acc: 81.25%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 80.77%   [EVAL] batch:   13 | acc: 50.00%,  total acc: 78.57%   
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    1 | acc: 75.00%,  total acc: 68.75%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 70.83%   [EVAL] batch:    3 | acc: 37.50%,  total acc: 62.50%   [EVAL] batch:    4 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    5 | acc: 56.25%,  total acc: 61.46%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 64.29%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 67.97%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 71.53%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 73.75%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 76.14%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 76.04%   [EVAL] batch:   12 | acc: 56.25%,  total acc: 74.52%   [EVAL] batch:   13 | acc: 37.50%,  total acc: 71.88%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 71.67%   [EVAL] batch:   15 | acc: 62.50%,  total acc: 71.09%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 71.32%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 70.83%   [EVAL] batch:   18 | acc: 81.25%,  total acc: 71.38%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 72.19%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 73.51%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 74.72%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 75.54%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 76.56%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 77.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 78.37%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 78.94%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 79.69%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 80.39%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 80.62%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 81.84%   [EVAL] batch:   32 | acc: 68.75%,  total acc: 81.44%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 81.99%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 81.79%   [EVAL] batch:   35 | acc: 56.25%,  total acc: 81.08%   [EVAL] batch:   36 | acc: 18.75%,  total acc: 79.39%   [EVAL] batch:   37 | acc: 6.25%,  total acc: 77.47%   [EVAL] batch:   38 | acc: 37.50%,  total acc: 76.44%   [EVAL] batch:   39 | acc: 56.25%,  total acc: 75.94%   [EVAL] batch:   40 | acc: 87.50%,  total acc: 76.22%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 76.64%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 77.18%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 77.70%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 78.19%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 78.67%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 79.12%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 79.56%   [EVAL] batch:   48 | acc: 31.25%,  total acc: 78.57%   [EVAL] batch:   49 | acc: 25.00%,  total acc: 77.50%   [EVAL] batch:   50 | acc: 43.75%,  total acc: 76.84%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 77.04%   [EVAL] batch:   52 | acc: 62.50%,  total acc: 76.77%   [EVAL] batch:   53 | acc: 18.75%,  total acc: 75.69%   
cur_acc:  ['0.8655', '0.5312', '0.7857']
his_acc:  ['0.8655', '0.7297', '0.7569']
CurrentTrain: epoch  0, batch     0 | loss: 5.6032338CurrentTrain: epoch  0, batch     1 | loss: 6.0395455CurrentTrain: epoch  1, batch     0 | loss: 5.1440177CurrentTrain: epoch  1, batch     1 | loss: 4.6062818CurrentTrain: epoch  2, batch     0 | loss: 4.4613762CurrentTrain: epoch  2, batch     1 | loss: 4.3008261CurrentTrain: epoch  3, batch     0 | loss: 4.6800065CurrentTrain: epoch  3, batch     1 | loss: 4.4418178CurrentTrain: epoch  4, batch     0 | loss: 4.4798203CurrentTrain: epoch  4, batch     1 | loss: 3.6735163CurrentTrain: epoch  5, batch     0 | loss: 3.4157181CurrentTrain: epoch  5, batch     1 | loss: 3.0083108CurrentTrain: epoch  6, batch     0 | loss: 3.1456742CurrentTrain: epoch  6, batch     1 | loss: 2.9429348CurrentTrain: epoch  7, batch     0 | loss: 3.2417657CurrentTrain: epoch  7, batch     1 | loss: 3.0534825CurrentTrain: epoch  8, batch     0 | loss: 2.9997163CurrentTrain: epoch  8, batch     1 | loss: 2.5566301CurrentTrain: epoch  9, batch     0 | loss: 2.7561965CurrentTrain: epoch  9, batch     1 | loss: 2.7563355
#### person cities of residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person cities of residence" means a person is living in a city.
Here is an example:
Relation: person cities of residence
Context: -lrb- now there are two : andre carson , a democrat from indianapolis , was elected to the house last year . -rrb-
Head Entity: andre carson
Tail Entity: indianapolis
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person cities of residence":
Relation: person cities of residence  
Context: -lrb- after moving to the west coast, sarah jones settled in san francisco, where she found a job in tech. -rrb-  
Head Entity: sarah jones  
Tail Entity: san francisco  

Relation: person cities of residence  
Context: -lrb- during his college years, michael smith lived in boston, enjoying the vibrant culture and history of the city. -rrb-  
Head Entity: michael smith  
Tail Entity: boston  
#### person schools attended ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person schools attended" means a person 's school is an organization.
Here is an example:
Relation: person schools attended
Context: born in baltimore in 1922 , parren mitchell was a graduate of morgan state college and earned a master 's degree from the university of maryland , according to biographical information supplied by cummings ' office .
Head Entity: parren mitchell
Tail Entity: university of maryland
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person schools attended":
Relation: person schools attended  
Context: after completing high school in 1995, jessica went on to study at the university of california, los angeles, where she earned her bachelor's degree in sociology.  
Head Entity: jessica  
Tail Entity: university of california, los angeles  

Relation: person schools attended  
Context: during his early years, steven attended several prestigious institutions, including harvard university, where he completed his law degree.  
Head Entity: steven  
Tail Entity: harvard university  
#### person country of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person country of death" means a person was died in a country.
Here is an example:
Relation: person country of death
Context: u.s. rep. parren mitchell , founding member of congressional black caucus , dies at 85
Head Entity: parren mitchell
Tail Entity: u.s.
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person country of death":
Relation: person country of death  
Context: renowned physicist stephen hawking passed away in cambridge, england at the age of 76  
Head Entity: stephen hawking  
Tail Entity: england  

Relation: person country of death  
Context: famous author gabriel garcia marquez died in mexico city, mexico, leaving behind a legacy of magical realism  
Head Entity: gabriel garcia marquez  
Tail Entity: mexico  
#### person children ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person children" means a person 's children is a person.
Here is an example:
Relation: person children
Context: he is survived by his wife of 63 years , josephine robinson mcnair , of columbia ; a son , robert e. jr. , of columbia ; three daughters , robin lee howell and corinne godshall , of myrtle beach , s.c. , and claudia crawford mcnair , of jamestown , s.c. ; six grandchildren ; and one great-grandchild .
Head Entity: he
Tail Entity: claudia crawford mcnair
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person children":
Relation: person children  
Context: After the passing of her husband, she dedicated her life to raising their three children, including her youngest, emily, who is now a successful artist in new york.  
Head Entity: she  
Tail Entity: emily  

Relation: person children  
Context: The famous actor often shares stories about his childhood and the influence of his parents on his career, especially his daughter, sarah, who is following in his footsteps.  
Head Entity: his  
Tail Entity: sarah  
#### person charges ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person charges" means a person was charged with an event.
Here is an example:
Relation: person charges
Context: flowers always contended politics was behind the extortion investigation , but appeals courts ruled against him .
Head Entity: him
Tail Entity: extortion
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person charges":
Relation: person charges  
Context: The prosecutor announced that the former mayor was facing serious allegations related to corruption.  
Head Entity: former mayor  
Tail Entity: corruption  

Relation: person charges  
Context: After a lengthy investigation, the authorities confirmed that the celebrity was implicated in a major drug trafficking case.  
Head Entity: celebrity  
Tail Entity: drug trafficking  
Mixup data size:  164
MixupTrain:  epoch  0, batch     0 | loss: 4.3865438MixupTrain:  epoch  0, batch     1 | loss: 3.6422521MixupTrain:  epoch  0, batch     2 | loss: 3.8300937MixupTrain:  epoch  0, batch     3 | loss: 3.8551138MixupTrain:  epoch  0, batch     4 | loss: 4.0121172MixupTrain:  epoch  0, batch     5 | loss: 3.3748830MixupTrain:  epoch  0, batch     6 | loss: 3.6754892MixupTrain:  epoch  0, batch     7 | loss: 4.2512903MixupTrain:  epoch  0, batch     8 | loss: 3.6509264MixupTrain:  epoch  0, batch     9 | loss: 2.8840356MixupTrain:  epoch  0, batch    10 | loss: 3.5321517
MemoryTrain:  epoch  0, batch     0 | loss: 3.1139297MemoryTrain:  epoch  0, batch     1 | loss: 2.8643188MemoryTrain:  epoch  0, batch     2 | loss: 4.4912519MemoryTrain:  epoch  0, batch     3 | loss: 3.0712306MemoryTrain:  epoch  1, batch     0 | loss: 3.1507211MemoryTrain:  epoch  1, batch     1 | loss: 2.5422583MemoryTrain:  epoch  1, batch     2 | loss: 2.4543457MemoryTrain:  epoch  1, batch     3 | loss: 3.0349388MemoryTrain:  epoch  2, batch     0 | loss: 2.1137180MemoryTrain:  epoch  2, batch     1 | loss: 2.2575631MemoryTrain:  epoch  2, batch     2 | loss: 2.3765285MemoryTrain:  epoch  2, batch     3 | loss: 2.4532282MemoryTrain:  epoch  3, batch     0 | loss: 2.0664577MemoryTrain:  epoch  3, batch     1 | loss: 2.0329485MemoryTrain:  epoch  3, batch     2 | loss: 2.3335264MemoryTrain:  epoch  3, batch     3 | loss: 3.0020344MemoryTrain:  epoch  4, batch     0 | loss: 2.3384461MemoryTrain:  epoch  4, batch     1 | loss: 2.2173574MemoryTrain:  epoch  4, batch     2 | loss: 1.8794709MemoryTrain:  epoch  4, batch     3 | loss: 1.9582746MemoryTrain:  epoch  5, batch     0 | loss: 1.9037232MemoryTrain:  epoch  5, batch     1 | loss: 2.3125577MemoryTrain:  epoch  5, batch     2 | loss: 2.0911484MemoryTrain:  epoch  5, batch     3 | loss: 1.9239986MemoryTrain:  epoch  6, batch     0 | loss: 1.8919880MemoryTrain:  epoch  6, batch     1 | loss: 1.8487108MemoryTrain:  epoch  6, batch     2 | loss: 1.8096057MemoryTrain:  epoch  6, batch     3 | loss: 1.6040261MemoryTrain:  epoch  7, batch     0 | loss: 1.6713808MemoryTrain:  epoch  7, batch     1 | loss: 1.9167383MemoryTrain:  epoch  7, batch     2 | loss: 1.5181367MemoryTrain:  epoch  7, batch     3 | loss: 1.7657797MemoryTrain:  epoch  8, batch     0 | loss: 1.6561136MemoryTrain:  epoch  8, batch     1 | loss: 1.6455610MemoryTrain:  epoch  8, batch     2 | loss: 1.7142929MemoryTrain:  epoch  8, batch     3 | loss: 1.8406333MemoryTrain:  epoch  9, batch     0 | loss: 1.6847613MemoryTrain:  epoch  9, batch     1 | loss: 1.5709014MemoryTrain:  epoch  9, batch     2 | loss: 1.6106720MemoryTrain:  epoch  9, batch     3 | loss: 1.5514312
[EVAL] batch:    0 | acc: 31.25%,  total acc: 31.25%   [EVAL] batch:    1 | acc: 31.25%,  total acc: 31.25%   [EVAL] batch:    2 | acc: 37.50%,  total acc: 33.33%   [EVAL] batch:    3 | acc: 43.75%,  total acc: 35.94%   [EVAL] batch:    4 | acc: 37.50%,  total acc: 36.25%   [EVAL] batch:    5 | acc: 50.00%,  total acc: 38.54%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 45.54%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 51.56%   [EVAL] batch:    8 | acc: 75.00%,  total acc: 54.17%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 57.50%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 59.66%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 63.02%   [EVAL] batch:   12 | acc: 100.00%,  total acc: 65.87%   [EVAL] batch:   13 | acc: 100.00%,  total acc: 68.30%   [EVAL] batch:   14 | acc: 100.00%,  total acc: 70.42%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 72.27%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 73.90%   [EVAL] batch:   17 | acc: 25.00%,  total acc: 71.18%   
[EVAL] batch:    0 | acc: 25.00%,  total acc: 25.00%   [EVAL] batch:    1 | acc: 43.75%,  total acc: 34.38%   [EVAL] batch:    2 | acc: 37.50%,  total acc: 35.42%   [EVAL] batch:    3 | acc: 18.75%,  total acc: 31.25%   [EVAL] batch:    4 | acc: 31.25%,  total acc: 31.25%   [EVAL] batch:    5 | acc: 43.75%,  total acc: 33.33%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 39.29%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 46.09%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 52.08%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 56.25%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 60.23%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 62.50%   [EVAL] batch:   12 | acc: 56.25%,  total acc: 62.02%   [EVAL] batch:   13 | acc: 37.50%,  total acc: 60.27%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 60.83%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 60.55%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 61.40%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 61.46%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 61.84%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 63.12%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 64.88%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 66.48%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 67.66%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 68.75%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 70.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 71.15%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 71.99%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 72.99%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 73.92%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 74.38%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 75.00%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 75.78%   [EVAL] batch:   32 | acc: 68.75%,  total acc: 75.57%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 76.29%   [EVAL] batch:   34 | acc: 68.75%,  total acc: 76.07%   [EVAL] batch:   35 | acc: 43.75%,  total acc: 75.17%   [EVAL] batch:   36 | acc: 0.00%,  total acc: 73.14%   [EVAL] batch:   37 | acc: 12.50%,  total acc: 71.55%   [EVAL] batch:   38 | acc: 6.25%,  total acc: 69.87%   [EVAL] batch:   39 | acc: 56.25%,  total acc: 69.53%   [EVAL] batch:   40 | acc: 87.50%,  total acc: 69.97%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 70.68%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 71.37%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 72.02%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 72.64%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 73.23%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 73.80%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 74.35%   [EVAL] batch:   48 | acc: 12.50%,  total acc: 73.09%   [EVAL] batch:   49 | acc: 12.50%,  total acc: 71.88%   [EVAL] batch:   50 | acc: 62.50%,  total acc: 71.69%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 72.24%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 72.64%   [EVAL] batch:   53 | acc: 37.50%,  total acc: 71.99%   [EVAL] batch:   54 | acc: 31.25%,  total acc: 71.25%   [EVAL] batch:   55 | acc: 43.75%,  total acc: 70.76%   [EVAL] batch:   56 | acc: 50.00%,  total acc: 70.39%   [EVAL] batch:   57 | acc: 25.00%,  total acc: 69.61%   [EVAL] batch:   58 | acc: 56.25%,  total acc: 69.39%   [EVAL] batch:   59 | acc: 75.00%,  total acc: 69.48%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 69.98%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 69.96%   [EVAL] batch:   62 | acc: 87.50%,  total acc: 70.24%   [EVAL] batch:   63 | acc: 87.50%,  total acc: 70.51%   [EVAL] batch:   64 | acc: 93.75%,  total acc: 70.87%   [EVAL] batch:   65 | acc: 100.00%,  total acc: 71.31%   [EVAL] batch:   66 | acc: 100.00%,  total acc: 71.74%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 72.15%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 72.55%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 72.95%   [EVAL] batch:   70 | acc: 50.00%,  total acc: 72.62%   
cur_acc:  ['0.8655', '0.5312', '0.7857', '0.7118']
his_acc:  ['0.8655', '0.7297', '0.7569', '0.7262']
CurrentTrain: epoch  0, batch     0 | loss: 4.5110397CurrentTrain: epoch  0, batch     1 | loss: 5.2978845CurrentTrain: epoch  1, batch     0 | loss: 3.7149649CurrentTrain: epoch  1, batch     1 | loss: 3.4323862CurrentTrain: epoch  2, batch     0 | loss: 3.1243639CurrentTrain: epoch  2, batch     1 | loss: 3.2044957CurrentTrain: epoch  3, batch     0 | loss: 2.9224401CurrentTrain: epoch  3, batch     1 | loss: 2.9744983CurrentTrain: epoch  4, batch     0 | loss: 2.7512450CurrentTrain: epoch  4, batch     1 | loss: 2.5580544CurrentTrain: epoch  5, batch     0 | loss: 2.5308249CurrentTrain: epoch  5, batch     1 | loss: 2.3711169CurrentTrain: epoch  6, batch     0 | loss: 2.2371411CurrentTrain: epoch  6, batch     1 | loss: 2.1467977CurrentTrain: epoch  7, batch     0 | loss: 2.1900434CurrentTrain: epoch  7, batch     1 | loss: 2.1850939CurrentTrain: epoch  8, batch     0 | loss: 2.0733280CurrentTrain: epoch  8, batch     1 | loss: 2.2247467CurrentTrain: epoch  9, batch     0 | loss: 1.9990844CurrentTrain: epoch  9, batch     1 | loss: 2.8480904
#### person cause of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person cause of death" means a person was died of an event.
Here is an example:
Relation: person cause of death
Context: mr scheider had suffered from multiple myeloma for several years , and died of complications from a staph infection , his wife , brenda siemer , said .
Head Entity: mr scheider
Tail Entity: complications from a staph infection
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person cause of death":
Relation: person cause of death  
Context: after battling with lung cancer for over a year, john doe passed away peacefully in his sleep, surrounded by family.  
Head Entity: john doe  
Tail Entity: lung cancer  

Relation: person cause of death  
Context: the renowned scientist, dr. emily white, tragically lost her life due to a car accident while returning from a conference.  
Head Entity: dr. emily white  
Tail Entity: car accident  
#### organization political religious affiliation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization political religious affiliation" means an organization is the member of a religion.
Here is an example:
Relation: organization political religious affiliation
Context: the militia has also suffered from allegations of being involved in clashes with rival shiite militias , particularly the badr corp , the military wing linked to supreme iraqi islamic council , the party led by powerful politician abdel aziz al-hakim .
Head Entity: badr corp
Tail Entity: shiite
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization political religious affiliation":
Relation: organization political religious affiliation  
Context: The Christian Democratic Union has been a significant player in German politics, often aligning itself with various religious groups to promote policies that reflect Christian values.  
Head Entity: Christian Democratic Union  
Tail Entity: Christian  

Relation: organization political religious affiliation  
Context: The Islamic Society of North America has been actively involved in advocating for Muslim rights and has formed alliances with political parties to ensure that the interests of the Muslim community are represented.  
Head Entity: Islamic Society of North America  
Tail Entity: Muslim  
#### organization stateorprovince of headquarters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization stateorprovince of headquarters" means an organization is located in a state or province.
Here is an example:
Relation: organization stateorprovince of headquarters
Context: based in armonk , new york , mbia insures $ 670 billion -lrb- euro452 .18 billion -rrb- in debt .
Head Entity: mbia
Tail Entity: new york
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization stateorprovince of headquarters":
Relation: organization stateorprovince of headquarters  
Context: the tech giant apple inc. has its headquarters in cupertino, california, where it develops innovative products.  
Head Entity: apple inc.  
Tail Entity: california  

Relation: organization stateorprovince of headquarters  
Context: headquartered in seattle, washington, amazon.com, inc. is a leading e-commerce and cloud computing company.  
Head Entity: amazon.com, inc.  
Tail Entity: washington  
#### person other family ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person other family" means a person 's relative is a person.
Here is an example:
Relation: person other family
Context: former american idol contestant , sanjaya malakar and hills starlet , holly montag , have been getting close since she replaced her sister and brother-in-law , heidi montag and spencer pratt , on i 'm a celebrity .
Head Entity: spencer pratt ,
Tail Entity: holly montag
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person other family":
Relation: person other family  
Context: in a recent interview, actress jennifer aniston spoke fondly of her late mother, nancy dow, who was a significant influence in her life and career.  
Head Entity: jennifer aniston  
Tail Entity: nancy dow  

Relation: person other family  
Context: during the family reunion, cousins emily and josh shared stories about their grandparents, who had a profound impact on their upbringing.  
Head Entity: emily  
Tail Entity: josh  
#### person city of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person city of death" means a person was died in a city.
Here is an example:
Relation: person city of death
Context: verity died wednesday , jan 3 , 2007 , in beaufort memorial hospital .
Head Entity: verity
Tail Entity: beaufort
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person city of death":
Relation: person city of death  
Context: after a long battle with illness, john passed away in the serene town of ashville.  
Head Entity: john  
Tail Entity: ashville  

Relation: person city of death  
Context: the renowned author tragically died in a car accident on the busy streets of los angeles.  
Head Entity: the renowned author  
Tail Entity: los angeles  
Mixup data size:  195
MixupTrain:  epoch  0, batch     0 | loss: 3.9605421MixupTrain:  epoch  0, batch     1 | loss: 3.6849904MixupTrain:  epoch  0, batch     2 | loss: 3.0274661MixupTrain:  epoch  0, batch     3 | loss: 4.0193310MixupTrain:  epoch  0, batch     4 | loss: 3.0543218MixupTrain:  epoch  0, batch     5 | loss: 3.1102499MixupTrain:  epoch  0, batch     6 | loss: 3.2416656MixupTrain:  epoch  0, batch     7 | loss: 3.3930846MixupTrain:  epoch  0, batch     8 | loss: 3.1797090MixupTrain:  epoch  0, batch     9 | loss: 3.3675398MixupTrain:  epoch  0, batch    10 | loss: 3.7548417MixupTrain:  epoch  0, batch    11 | loss: 3.2376945MixupTrain:  epoch  0, batch    12 | loss: 2.8491163
MemoryTrain:  epoch  0, batch     0 | loss: 2.0123434MemoryTrain:  epoch  0, batch     1 | loss: 2.5964818MemoryTrain:  epoch  0, batch     2 | loss: 3.0405655MemoryTrain:  epoch  0, batch     3 | loss: 3.2154789MemoryTrain:  epoch  0, batch     4 | loss: 2.7830703MemoryTrain:  epoch  1, batch     0 | loss: 2.7684479MemoryTrain:  epoch  1, batch     1 | loss: 3.0580204MemoryTrain:  epoch  1, batch     2 | loss: 1.9440567MemoryTrain:  epoch  1, batch     3 | loss: 2.7209926MemoryTrain:  epoch  1, batch     4 | loss: 2.7806838MemoryTrain:  epoch  2, batch     0 | loss: 3.0936670MemoryTrain:  epoch  2, batch     1 | loss: 2.0319924MemoryTrain:  epoch  2, batch     2 | loss: 2.1630366MemoryTrain:  epoch  2, batch     3 | loss: 2.3191500MemoryTrain:  epoch  2, batch     4 | loss: 2.0526502MemoryTrain:  epoch  3, batch     0 | loss: 2.1888463MemoryTrain:  epoch  3, batch     1 | loss: 1.9215883MemoryTrain:  epoch  3, batch     2 | loss: 2.3158054MemoryTrain:  epoch  3, batch     3 | loss: 1.8269675MemoryTrain:  epoch  3, batch     4 | loss: 1.6303042MemoryTrain:  epoch  4, batch     0 | loss: 1.6353230MemoryTrain:  epoch  4, batch     1 | loss: 1.8759623MemoryTrain:  epoch  4, batch     2 | loss: 2.3331113MemoryTrain:  epoch  4, batch     3 | loss: 1.6354721MemoryTrain:  epoch  4, batch     4 | loss: 1.6946207MemoryTrain:  epoch  5, batch     0 | loss: 1.6251913MemoryTrain:  epoch  5, batch     1 | loss: 1.6056701MemoryTrain:  epoch  5, batch     2 | loss: 1.6358030MemoryTrain:  epoch  5, batch     3 | loss: 1.6587203MemoryTrain:  epoch  5, batch     4 | loss: 1.8952094MemoryTrain:  epoch  6, batch     0 | loss: 1.8926212MemoryTrain:  epoch  6, batch     1 | loss: 1.5946860MemoryTrain:  epoch  6, batch     2 | loss: 1.6769359MemoryTrain:  epoch  6, batch     3 | loss: 1.5409884MemoryTrain:  epoch  6, batch     4 | loss: 1.6769121MemoryTrain:  epoch  7, batch     0 | loss: 1.3935539MemoryTrain:  epoch  7, batch     1 | loss: 1.6439900MemoryTrain:  epoch  7, batch     2 | loss: 1.7509007MemoryTrain:  epoch  7, batch     3 | loss: 1.5958788MemoryTrain:  epoch  7, batch     4 | loss: 1.3443578MemoryTrain:  epoch  8, batch     0 | loss: 1.4014502MemoryTrain:  epoch  8, batch     1 | loss: 1.4064500MemoryTrain:  epoch  8, batch     2 | loss: 1.5933347MemoryTrain:  epoch  8, batch     3 | loss: 1.5790751MemoryTrain:  epoch  8, batch     4 | loss: 1.4575784MemoryTrain:  epoch  9, batch     0 | loss: 1.3730687MemoryTrain:  epoch  9, batch     1 | loss: 1.4473732MemoryTrain:  epoch  9, batch     2 | loss: 1.4920350MemoryTrain:  epoch  9, batch     3 | loss: 1.4772878MemoryTrain:  epoch  9, batch     4 | loss: 1.3383280
[EVAL] batch:    0 | acc: 31.25%,  total acc: 31.25%   [EVAL] batch:    1 | acc: 56.25%,  total acc: 43.75%   [EVAL] batch:    2 | acc: 37.50%,  total acc: 41.67%   [EVAL] batch:    3 | acc: 50.00%,  total acc: 43.75%   [EVAL] batch:    4 | acc: 62.50%,  total acc: 47.50%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 53.12%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 56.25%   [EVAL] batch:    7 | acc: 68.75%,  total acc: 57.81%   [EVAL] batch:    8 | acc: 31.25%,  total acc: 54.86%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 55.62%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 57.95%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 59.90%   [EVAL] batch:   12 | acc: 50.00%,  total acc: 59.13%   
[EVAL] batch:    0 | acc: 12.50%,  total acc: 12.50%   [EVAL] batch:    1 | acc: 37.50%,  total acc: 25.00%   [EVAL] batch:    2 | acc: 18.75%,  total acc: 22.92%   [EVAL] batch:    3 | acc: 12.50%,  total acc: 20.31%   [EVAL] batch:    4 | acc: 25.00%,  total acc: 21.25%   [EVAL] batch:    5 | acc: 31.25%,  total acc: 22.92%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 30.36%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 39.06%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 45.83%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 50.62%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 55.11%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 57.81%   [EVAL] batch:   12 | acc: 62.50%,  total acc: 58.17%   [EVAL] batch:   13 | acc: 31.25%,  total acc: 56.25%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 57.50%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 57.42%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 58.46%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 58.68%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 59.21%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 60.62%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 62.50%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 64.20%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 65.49%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 66.67%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 68.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 69.23%   [EVAL] batch:   26 | acc: 87.50%,  total acc: 69.91%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 70.98%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 71.98%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 72.29%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 72.78%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 73.44%   [EVAL] batch:   32 | acc: 75.00%,  total acc: 73.48%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 74.26%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 74.29%   [EVAL] batch:   35 | acc: 25.00%,  total acc: 72.92%   [EVAL] batch:   36 | acc: 0.00%,  total acc: 70.95%   [EVAL] batch:   37 | acc: 6.25%,  total acc: 69.24%   [EVAL] batch:   38 | acc: 0.00%,  total acc: 67.47%   [EVAL] batch:   39 | acc: 50.00%,  total acc: 67.03%   [EVAL] batch:   40 | acc: 87.50%,  total acc: 67.53%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 68.30%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 69.04%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 69.74%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 70.42%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 71.06%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 71.68%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 72.27%   [EVAL] batch:   48 | acc: 12.50%,  total acc: 71.05%   [EVAL] batch:   49 | acc: 6.25%,  total acc: 69.75%   [EVAL] batch:   50 | acc: 37.50%,  total acc: 69.12%   [EVAL] batch:   51 | acc: 68.75%,  total acc: 69.11%   [EVAL] batch:   52 | acc: 75.00%,  total acc: 69.22%   [EVAL] batch:   53 | acc: 18.75%,  total acc: 68.29%   [EVAL] batch:   54 | acc: 0.00%,  total acc: 67.05%   [EVAL] batch:   55 | acc: 0.00%,  total acc: 65.85%   [EVAL] batch:   56 | acc: 0.00%,  total acc: 64.69%   [EVAL] batch:   57 | acc: 0.00%,  total acc: 63.58%   [EVAL] batch:   58 | acc: 0.00%,  total acc: 62.50%   [EVAL] batch:   59 | acc: 43.75%,  total acc: 62.19%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 62.81%   [EVAL] batch:   61 | acc: 56.25%,  total acc: 62.70%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 62.50%   [EVAL] batch:   63 | acc: 43.75%,  total acc: 62.21%   [EVAL] batch:   64 | acc: 87.50%,  total acc: 62.60%   [EVAL] batch:   65 | acc: 100.00%,  total acc: 63.16%   [EVAL] batch:   66 | acc: 100.00%,  total acc: 63.71%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 64.25%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 64.76%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 65.27%   [EVAL] batch:   70 | acc: 62.50%,  total acc: 65.23%   [EVAL] batch:   71 | acc: 43.75%,  total acc: 64.93%   [EVAL] batch:   72 | acc: 50.00%,  total acc: 64.73%   [EVAL] batch:   73 | acc: 31.25%,  total acc: 64.27%   [EVAL] batch:   74 | acc: 62.50%,  total acc: 64.25%   [EVAL] batch:   75 | acc: 87.50%,  total acc: 64.56%   [EVAL] batch:   76 | acc: 68.75%,  total acc: 64.61%   [EVAL] batch:   77 | acc: 68.75%,  total acc: 64.66%   [EVAL] batch:   78 | acc: 56.25%,  total acc: 64.56%   [EVAL] batch:   79 | acc: 43.75%,  total acc: 64.30%   [EVAL] batch:   80 | acc: 62.50%,  total acc: 64.27%   [EVAL] batch:   81 | acc: 93.75%,  total acc: 64.63%   [EVAL] batch:   82 | acc: 81.25%,  total acc: 64.83%   [EVAL] batch:   83 | acc: 6.25%,  total acc: 64.14%   
cur_acc:  ['0.8655', '0.5312', '0.7857', '0.7118', '0.5913']
his_acc:  ['0.8655', '0.7297', '0.7569', '0.7262', '0.6414']
CurrentTrain: epoch  0, batch     0 | loss: 5.9844170CurrentTrain: epoch  0, batch     1 | loss: 6.5877123CurrentTrain: epoch  1, batch     0 | loss: 5.1265116CurrentTrain: epoch  1, batch     1 | loss: 5.0751214CurrentTrain: epoch  2, batch     0 | loss: 4.2051044CurrentTrain: epoch  2, batch     1 | loss: 4.9513192CurrentTrain: epoch  3, batch     0 | loss: 4.3132138CurrentTrain: epoch  3, batch     1 | loss: 4.0925627CurrentTrain: epoch  4, batch     0 | loss: 3.3314118CurrentTrain: epoch  4, batch     1 | loss: 3.0327399CurrentTrain: epoch  5, batch     0 | loss: 3.1640301CurrentTrain: epoch  5, batch     1 | loss: 3.1418037CurrentTrain: epoch  6, batch     0 | loss: 3.1115706CurrentTrain: epoch  6, batch     1 | loss: 2.6646686CurrentTrain: epoch  7, batch     0 | loss: 2.8818388CurrentTrain: epoch  7, batch     1 | loss: 2.7898691CurrentTrain: epoch  8, batch     0 | loss: 2.4980111CurrentTrain: epoch  8, batch     1 | loss: 2.4999459CurrentTrain: epoch  9, batch     0 | loss: 2.7342165CurrentTrain: epoch  9, batch     1 | loss: 2.4128919
#### person date of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person date of birth" means a person was born in a date.
Here is an example:
Relation: person date of birth
Context: kirkaldy , born irene morgan in baltimore , maryland , in 1917 , was arrested in 1944 for refusing to give up her seat on a greyhound bus heading from gloucester to baltimore , and for resisting arrest .
Head Entity: irene morgan
Tail Entity: 1917
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person date of birth":
Relation: person date of birth  
Context: the famous physicist albert einstein was born in ulm, in the kingdom of wurttemberg in the german empire, on march 14, 1879.  
Head Entity: albert einstein  
Tail Entity: march 14, 1879  

Relation: person date of birth  
Context: the renowned author mark twain, whose real name was samuel langhorne clemens, was born on november 30, 1835, in florida, missouri.  
Head Entity: mark twain  
Tail Entity: november 30, 1835  
#### person stateorprovince of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person stateorprovince of birth" means a person was died in a state or province.
Here is an example:
Relation: person stateorprovince of birth
Context: joseph simpson farland was born on aug 11 , 1914 , in clarksburg , wva , the only child of richard and grace simpson farland .
Head Entity: joseph simpson farland
Tail Entity: wva
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person stateorprovince of birth":
Relation: person stateorprovince of birth  
Context: martha stewart was born on august 3, 1941, in jersey city, new jersey, to parents of polish descent.  
Head Entity: martha stewart  
Tail Entity: new jersey  

Relation: person stateorprovince of birth  
Context: barack obama was born on august 4, 1961, in honolulu, hawaii, where he spent most of his childhood.  
Head Entity: barack obama  
Tail Entity: hawaii  
#### person parents ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person parents" means a person 's parent is a person.
Here is an example:
Relation: person parents
Context: as the case developed , sandy 's mother , denise sandy , quietly made herself a spectral but central figure , by faithfully attending pretrial hearings .
Head Entity: sandy
Tail Entity: denise sandy
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person parents":
Relation: person parents  
Context: After the ceremony, Emily's father, John Smith, shared heartfelt stories about her childhood, bringing tears to many eyes.  
Head Entity: Emily  
Tail Entity: John Smith  

Relation: person parents  
Context: During the family reunion, Michael's mother, Sarah Johnson, prepared her famous lasagna, which everyone eagerly devoured.  
Head Entity: Michael  
Tail Entity: Sarah Johnson  
#### person employee of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person employee of" means a person is an employee of an organization.
Here is an example:
Relation: person employee of
Context: seeking revenge , axel reunites with old pal sgt. billy rosewood -lrb- judge reinhold -rrb- and jon flint -lrb- hector elizondo -rrb- of the beverly hills police department .
Head Entity: hector elizondo
Tail Entity: beverly hills police department
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person employee of":
Relation: person employee of  
Context: After years of hard work and dedication, Sarah Thompson finally received a promotion at the tech company where she has been employed since college.  
Head Entity: Sarah Thompson  
Tail Entity: tech company  

Relation: person employee of  
Context: John Smith has been a loyal employee at Greenfield Farms for over a decade, contributing to the growth and success of the organization.  
Head Entity: John Smith  
Tail Entity: Greenfield Farms  
#### person stateorprovince of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person stateorprovince of death" means a person was died in a state or province.
Here is an example:
Relation: person stateorprovince of death
Context: irene morgan kirkaldy , 90 , who died of alzheimer 's disease aug. 10 at her home in gloucester , va. , quietly changed history in 1944 when she refused to give up her seat on a crowded greyhound bus to a white couple .
Head Entity: irene morgan kirkaldy
Tail Entity: va.
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person stateorprovince of death":
Relation: person stateorprovince of death  
Context: john smith, a renowned author, passed away on march 5 in his residence located in los angeles, california, leaving behind a legacy of literary works that inspired many.  
Head Entity: john smith  
Tail Entity: california  

Relation: person stateorprovince of death  
Context: the famous musician, elena rodriguez, died tragically in a car accident on july 12 while traveling through the scenic routes of oregon, where she had spent her childhood.  
Head Entity: elena rodriguez  
Tail Entity: oregon  
Mixup data size:  225
MixupTrain:  epoch  0, batch     0 | loss: 3.1930828MixupTrain:  epoch  0, batch     1 | loss: 2.9288073MixupTrain:  epoch  0, batch     2 | loss: 2.5639323MixupTrain:  epoch  0, batch     3 | loss: 3.3389341MixupTrain:  epoch  0, batch     4 | loss: 4.0436268MixupTrain:  epoch  0, batch     5 | loss: 3.1022400MixupTrain:  epoch  0, batch     6 | loss: 2.5528791MixupTrain:  epoch  0, batch     7 | loss: 3.4550454MixupTrain:  epoch  0, batch     8 | loss: 2.7837902MixupTrain:  epoch  0, batch     9 | loss: 3.0312837MixupTrain:  epoch  0, batch    10 | loss: 2.8894838MixupTrain:  epoch  0, batch    11 | loss: 3.3985166MixupTrain:  epoch  0, batch    12 | loss: 2.5597917MixupTrain:  epoch  0, batch    13 | loss: 2.7142946MixupTrain:  epoch  0, batch    14 | loss: 3.1559045
MemoryTrain:  epoch  0, batch     0 | loss: 2.4597967MemoryTrain:  epoch  0, batch     1 | loss: 3.1231904MemoryTrain:  epoch  0, batch     2 | loss: 2.3297353MemoryTrain:  epoch  0, batch     3 | loss: 2.5005052MemoryTrain:  epoch  0, batch     4 | loss: 3.3016634MemoryTrain:  epoch  0, batch     5 | loss: 2.4313495MemoryTrain:  epoch  1, batch     0 | loss: 2.6889238MemoryTrain:  epoch  1, batch     1 | loss: 2.0498877MemoryTrain:  epoch  1, batch     2 | loss: 1.5469575MemoryTrain:  epoch  1, batch     3 | loss: 2.5545180MemoryTrain:  epoch  1, batch     4 | loss: 2.8372192MemoryTrain:  epoch  1, batch     5 | loss: 1.8559700MemoryTrain:  epoch  2, batch     0 | loss: 1.8619659MemoryTrain:  epoch  2, batch     1 | loss: 1.9624532MemoryTrain:  epoch  2, batch     2 | loss: 2.8970687MemoryTrain:  epoch  2, batch     3 | loss: 2.1887963MemoryTrain:  epoch  2, batch     4 | loss: 2.0316894MemoryTrain:  epoch  2, batch     5 | loss: 1.7207500MemoryTrain:  epoch  3, batch     0 | loss: 2.0876327MemoryTrain:  epoch  3, batch     1 | loss: 1.7376052MemoryTrain:  epoch  3, batch     2 | loss: 1.9167248MemoryTrain:  epoch  3, batch     3 | loss: 1.8983079MemoryTrain:  epoch  3, batch     4 | loss: 1.8847954MemoryTrain:  epoch  3, batch     5 | loss: 2.1087346MemoryTrain:  epoch  4, batch     0 | loss: 1.9739993MemoryTrain:  epoch  4, batch     1 | loss: 1.9003567MemoryTrain:  epoch  4, batch     2 | loss: 1.7661901MemoryTrain:  epoch  4, batch     3 | loss: 1.8537097MemoryTrain:  epoch  4, batch     4 | loss: 1.5873246MemoryTrain:  epoch  4, batch     5 | loss: 1.6323588MemoryTrain:  epoch  5, batch     0 | loss: 1.8850062MemoryTrain:  epoch  5, batch     1 | loss: 1.9565803MemoryTrain:  epoch  5, batch     2 | loss: 1.6116047MemoryTrain:  epoch  5, batch     3 | loss: 1.4840535MemoryTrain:  epoch  5, batch     4 | loss: 1.9253520MemoryTrain:  epoch  5, batch     5 | loss: 1.6344792MemoryTrain:  epoch  6, batch     0 | loss: 1.7195535MemoryTrain:  epoch  6, batch     1 | loss: 1.6829150MemoryTrain:  epoch  6, batch     2 | loss: 1.5519080MemoryTrain:  epoch  6, batch     3 | loss: 1.8800622MemoryTrain:  epoch  6, batch     4 | loss: 1.4916474MemoryTrain:  epoch  6, batch     5 | loss: 1.5952548MemoryTrain:  epoch  7, batch     0 | loss: 1.5026119MemoryTrain:  epoch  7, batch     1 | loss: 1.6849396MemoryTrain:  epoch  7, batch     2 | loss: 1.3511767MemoryTrain:  epoch  7, batch     3 | loss: 1.4330212MemoryTrain:  epoch  7, batch     4 | loss: 1.7371454MemoryTrain:  epoch  7, batch     5 | loss: 1.6062526MemoryTrain:  epoch  8, batch     0 | loss: 1.4948758MemoryTrain:  epoch  8, batch     1 | loss: 1.5215898MemoryTrain:  epoch  8, batch     2 | loss: 1.5841537MemoryTrain:  epoch  8, batch     3 | loss: 1.5413666MemoryTrain:  epoch  8, batch     4 | loss: 1.4290762MemoryTrain:  epoch  8, batch     5 | loss: 1.5060060MemoryTrain:  epoch  9, batch     0 | loss: 1.4043527MemoryTrain:  epoch  9, batch     1 | loss: 1.5293188MemoryTrain:  epoch  9, batch     2 | loss: 1.4167562MemoryTrain:  epoch  9, batch     3 | loss: 1.4991807MemoryTrain:  epoch  9, batch     4 | loss: 1.4177556MemoryTrain:  epoch  9, batch     5 | loss: 1.3902590
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 50.00%,  total acc: 62.50%   [EVAL] batch:    2 | acc: 68.75%,  total acc: 64.58%   [EVAL] batch:    3 | acc: 37.50%,  total acc: 57.81%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 61.25%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 63.54%   [EVAL] batch:    6 | acc: 50.00%,  total acc: 61.61%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 65.62%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 68.75%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 70.62%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 71.59%   [EVAL] batch:   11 | acc: 68.75%,  total acc: 71.35%   [EVAL] batch:   12 | acc: 100.00%,  total acc: 73.56%   [EVAL] batch:   13 | acc: 56.25%,  total acc: 72.32%   
[EVAL] batch:    0 | acc: 25.00%,  total acc: 25.00%   [EVAL] batch:    1 | acc: 37.50%,  total acc: 31.25%   [EVAL] batch:    2 | acc: 25.00%,  total acc: 29.17%   [EVAL] batch:    3 | acc: 12.50%,  total acc: 25.00%   [EVAL] batch:    4 | acc: 31.25%,  total acc: 26.25%   [EVAL] batch:    5 | acc: 31.25%,  total acc: 27.08%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 34.82%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 42.97%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 49.31%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 53.12%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 57.39%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 59.90%   [EVAL] batch:   12 | acc: 56.25%,  total acc: 59.62%   [EVAL] batch:   13 | acc: 31.25%,  total acc: 57.59%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 58.33%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 58.20%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 59.19%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 59.38%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 59.54%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 60.94%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 62.80%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 64.20%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 65.76%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 66.93%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 68.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 69.47%   [EVAL] batch:   26 | acc: 87.50%,  total acc: 70.14%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 71.21%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 72.20%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 72.50%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 72.98%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 73.83%   [EVAL] batch:   32 | acc: 68.75%,  total acc: 73.67%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 74.45%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 74.46%   [EVAL] batch:   35 | acc: 25.00%,  total acc: 73.09%   [EVAL] batch:   36 | acc: 0.00%,  total acc: 71.11%   [EVAL] batch:   37 | acc: 0.00%,  total acc: 69.24%   [EVAL] batch:   38 | acc: 0.00%,  total acc: 67.47%   [EVAL] batch:   39 | acc: 50.00%,  total acc: 67.03%   [EVAL] batch:   40 | acc: 87.50%,  total acc: 67.53%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 68.30%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 69.04%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 69.74%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 70.42%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 71.06%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 71.68%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 72.27%   [EVAL] batch:   48 | acc: 12.50%,  total acc: 71.05%   [EVAL] batch:   49 | acc: 6.25%,  total acc: 69.75%   [EVAL] batch:   50 | acc: 37.50%,  total acc: 69.12%   [EVAL] batch:   51 | acc: 50.00%,  total acc: 68.75%   [EVAL] batch:   52 | acc: 43.75%,  total acc: 68.28%   [EVAL] batch:   53 | acc: 12.50%,  total acc: 67.25%   [EVAL] batch:   54 | acc: 0.00%,  total acc: 66.02%   [EVAL] batch:   55 | acc: 0.00%,  total acc: 64.84%   [EVAL] batch:   56 | acc: 6.25%,  total acc: 63.82%   [EVAL] batch:   57 | acc: 0.00%,  total acc: 62.72%   [EVAL] batch:   58 | acc: 0.00%,  total acc: 61.65%   [EVAL] batch:   59 | acc: 37.50%,  total acc: 61.25%   [EVAL] batch:   60 | acc: 87.50%,  total acc: 61.68%   [EVAL] batch:   61 | acc: 31.25%,  total acc: 61.19%   [EVAL] batch:   62 | acc: 62.50%,  total acc: 61.21%   [EVAL] batch:   63 | acc: 62.50%,  total acc: 61.23%   [EVAL] batch:   64 | acc: 87.50%,  total acc: 61.63%   [EVAL] batch:   65 | acc: 100.00%,  total acc: 62.22%   [EVAL] batch:   66 | acc: 100.00%,  total acc: 62.78%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 63.33%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 63.86%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 64.38%   [EVAL] batch:   70 | acc: 62.50%,  total acc: 64.35%   [EVAL] batch:   71 | acc: 43.75%,  total acc: 64.06%   [EVAL] batch:   72 | acc: 56.25%,  total acc: 63.96%   [EVAL] batch:   73 | acc: 37.50%,  total acc: 63.60%   [EVAL] batch:   74 | acc: 81.25%,  total acc: 63.83%   [EVAL] batch:   75 | acc: 100.00%,  total acc: 64.31%   [EVAL] batch:   76 | acc: 87.50%,  total acc: 64.61%   [EVAL] batch:   77 | acc: 75.00%,  total acc: 64.74%   [EVAL] batch:   78 | acc: 25.00%,  total acc: 64.24%   [EVAL] batch:   79 | acc: 18.75%,  total acc: 63.67%   [EVAL] batch:   80 | acc: 37.50%,  total acc: 63.35%   [EVAL] batch:   81 | acc: 68.75%,  total acc: 63.41%   [EVAL] batch:   82 | acc: 68.75%,  total acc: 63.48%   [EVAL] batch:   83 | acc: 75.00%,  total acc: 63.62%   [EVAL] batch:   84 | acc: 56.25%,  total acc: 63.53%   [EVAL] batch:   85 | acc: 62.50%,  total acc: 63.52%   [EVAL] batch:   86 | acc: 43.75%,  total acc: 63.29%   [EVAL] batch:   87 | acc: 68.75%,  total acc: 63.35%   [EVAL] batch:   88 | acc: 81.25%,  total acc: 63.55%   [EVAL] batch:   89 | acc: 50.00%,  total acc: 63.40%   [EVAL] batch:   90 | acc: 87.50%,  total acc: 63.67%   [EVAL] batch:   91 | acc: 93.75%,  total acc: 63.99%   [EVAL] batch:   92 | acc: 87.50%,  total acc: 64.25%   [EVAL] batch:   93 | acc: 87.50%,  total acc: 64.49%   [EVAL] batch:   94 | acc: 68.75%,  total acc: 64.54%   [EVAL] batch:   95 | acc: 93.75%,  total acc: 64.84%   [EVAL] batch:   96 | acc: 62.50%,  total acc: 64.82%   
cur_acc:  ['0.8655', '0.5312', '0.7857', '0.7118', '0.5913', '0.7232']
his_acc:  ['0.8655', '0.7297', '0.7569', '0.7262', '0.6414', '0.6482']
CurrentTrain: epoch  0, batch     0 | loss: 7.8509736CurrentTrain: epoch  0, batch     1 | loss: 8.6592607CurrentTrain: epoch  1, batch     0 | loss: 7.3733821CurrentTrain: epoch  1, batch     1 | loss: 6.6846113CurrentTrain: epoch  2, batch     0 | loss: 6.6904736CurrentTrain: epoch  2, batch     1 | loss: 6.5827699CurrentTrain: epoch  3, batch     0 | loss: 6.6487188CurrentTrain: epoch  3, batch     1 | loss: 5.1707878CurrentTrain: epoch  4, batch     0 | loss: 6.0473909CurrentTrain: epoch  4, batch     1 | loss: 5.6463308CurrentTrain: epoch  5, batch     0 | loss: 5.8262925CurrentTrain: epoch  5, batch     1 | loss: 5.2742529CurrentTrain: epoch  6, batch     0 | loss: 5.2860928CurrentTrain: epoch  6, batch     1 | loss: 4.3254056CurrentTrain: epoch  7, batch     0 | loss: 4.5050726CurrentTrain: epoch  7, batch     1 | loss: 5.1462312CurrentTrain: epoch  8, batch     0 | loss: 4.6080923CurrentTrain: epoch  8, batch     1 | loss: 4.1935258CurrentTrain: epoch  9, batch     0 | loss: 4.5213656CurrentTrain: epoch  9, batch     1 | loss: 3.9629374
#### organization subsidiaries ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization subsidiaries" means an organization 's subsidiary is an organization.
Here is an example:
Relation: organization subsidiaries
Context: venture fund buys sporting chain highland capital 's consumer fund includes lululemon athletica , a yoga retailer , and o beverages , a flavored water company developed by tom first , one of the two `` juice guys '' who cofounded nantucket nectars .
Head Entity: highland capital
Tail Entity: o beverages
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization subsidiaries":
Relation: organization subsidiaries  
Context: The tech giant Alphabet Inc. has several subsidiaries, including YouTube, which has become a leading platform for video content, and Waymo, which focuses on self-driving technology.  
Head Entity: Alphabet Inc.  
Tail Entity: Waymo  

Relation: organization subsidiaries  
Context: The multinational corporation Procter & Gamble owns various subsidiaries, such as Gillette, which specializes in personal care products, and Tide, known for its laundry detergents.  
Head Entity: Procter & Gamble  
Tail Entity: Gillette  
#### organization parents ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization parents" means an organization 's parent is an organization.
Here is an example:
Relation: organization parents
Context: aseel kami , tim cocks , abdul-rahman taher , tim cocks and michael christie -lrb- reuters -rrb- add an intriguing note in the speculation over who 's responsible for the recent bombings : `` but a senior iraqi intelligence source , who declined to be named , said there was evidence the bombs could be the work of the militant badr organisation , the armed wing of the supreme islamic iraqi council -lrb- isci -rrb- .
Head Entity: badr organisation
Tail Entity: supreme islamic iraqi council
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization parents":
Relation: organization parents  
Context: In a recent merger, the tech giant SoftTech announced its acquisition of Innovatech, a leading software development firm. This move has raised questions about the future of Innovatech and its role under the SoftTech umbrella. Industry experts believe that this acquisition will strengthen SoftTech's position in the market.  
Head Entity: SoftTech  
Tail Entity: Innovatech  

Relation: organization parents  
Context: The historic partnership between Global Health Initiative and Health for All has been pivotal in addressing healthcare challenges in underserved communities. As a parent organization, Global Health Initiative has provided essential resources and support to Health for All, enabling it to expand its outreach programs significantly.  
Head Entity: Global Health Initiative  
Tail Entity: Health for All  
#### organization alternate names ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization alternate names" means a person 's alias is a person.
Here is an example:
Relation: organization alternate names
Context: it also needs the green light from the 45-nation nuclear suppliers group -lrb- nsg -rrb- , which regulates global civilian nuclear trade , before it can begin buying nuclear reactors and fuel .
Head Entity: nsg
Tail Entity: nuclear suppliers group
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization alternate names":
Relation: organization alternate names  
Context: The International Monetary Fund, often referred to as the IMF, plays a crucial role in global economic stability.  
Head Entity: IMF  
Tail Entity: International Monetary Fund  

Relation: organization alternate names  
Context: The World Health Organization, commonly known as WHO, has been at the forefront of the global response to health crises.  
Head Entity: WHO  
Tail Entity: World Health Organization  
#### organization city of headquarters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization city of headquarters" means an organization is located in a city.
Here is an example:
Relation: organization city of headquarters
Context: ------ london 2008-05-20 07:23:45 utc enodis plc endorses sweetened takeover bid by us company manitowoc illinois tool works of glenville , illinois , which had offered 282 pence -lrb- us$ 551 euro3 54 -rrb- per share , said monday that it was considering its position .
Head Entity: illinois tool works
Tail Entity: glenville
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization city of headquarters":
Relation: organization city of headquarters  
Context: ------ in 2015, tech giant apple inc. announced plans to expand its operations in cupertino, california, where it has been headquartered since its founding in 1976.  
Head Entity: apple inc.  
Tail Entity: cupertino  

Relation: organization city of headquarters  
Context: ------ the multinational corporation samsung electronics is based in suwon, south korea, and has been a leader in the technology sector for decades.  
Head Entity: samsung electronics  
Tail Entity: suwon  
#### person siblings ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person siblings" means a person 's sibling is a person.
Here is an example:
Relation: person siblings
Context: `` holly and sanjaya are headed to -lsb- the hawaiian island of -rsb- kauai tomorrow morning so she can meet his parents . ''
Head Entity: she
Tail Entity: sanjaya
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person siblings":
Relation: person siblings  
Context: `` during the family reunion, john introduced his sister to everyone, and they all had a great time catching up. ''  
Head Entity: his sister  
Tail Entity: john  

Relation: person siblings  
Context: `` after the graduation ceremony, emily celebrated with her brother, who had just completed his degree in engineering. ''  
Head Entity: her brother  
Tail Entity: emily  
Mixup data size:  254
MixupTrain:  epoch  0, batch     0 | loss: 3.2453752MixupTrain:  epoch  0, batch     1 | loss: 2.7123952MixupTrain:  epoch  0, batch     2 | loss: 3.0048644MixupTrain:  epoch  0, batch     3 | loss: 2.3190390MixupTrain:  epoch  0, batch     4 | loss: 3.1413925MixupTrain:  epoch  0, batch     5 | loss: 2.3891670MixupTrain:  epoch  0, batch     6 | loss: 4.0205062MixupTrain:  epoch  0, batch     7 | loss: 2.8401639MixupTrain:  epoch  0, batch     8 | loss: 2.6269370MixupTrain:  epoch  0, batch     9 | loss: 2.9022909MixupTrain:  epoch  0, batch    10 | loss: 3.2002346MixupTrain:  epoch  0, batch    11 | loss: 3.1001183MixupTrain:  epoch  0, batch    12 | loss: 3.1911913MixupTrain:  epoch  0, batch    13 | loss: 2.7831997MixupTrain:  epoch  0, batch    14 | loss: 2.7323370MixupTrain:  epoch  0, batch    15 | loss: 2.6154957
MemoryTrain:  epoch  0, batch     0 | loss: 2.0739713MemoryTrain:  epoch  0, batch     1 | loss: 2.5070949MemoryTrain:  epoch  0, batch     2 | loss: 2.6896653MemoryTrain:  epoch  0, batch     3 | loss: 2.7065892MemoryTrain:  epoch  0, batch     4 | loss: 2.8956914MemoryTrain:  epoch  0, batch     5 | loss: 2.9684689MemoryTrain:  epoch  0, batch     6 | loss: 2.7656965MemoryTrain:  epoch  1, batch     0 | loss: 2.0935373MemoryTrain:  epoch  1, batch     1 | loss: 3.2993681MemoryTrain:  epoch  1, batch     2 | loss: 2.4979615MemoryTrain:  epoch  1, batch     3 | loss: 1.7726290MemoryTrain:  epoch  1, batch     4 | loss: 2.8198423MemoryTrain:  epoch  1, batch     5 | loss: 1.6116620MemoryTrain:  epoch  1, batch     6 | loss: 2.8858347MemoryTrain:  epoch  2, batch     0 | loss: 2.3590813MemoryTrain:  epoch  2, batch     1 | loss: 2.8744864MemoryTrain:  epoch  2, batch     2 | loss: 2.5658033MemoryTrain:  epoch  2, batch     3 | loss: 2.1368079MemoryTrain:  epoch  2, batch     4 | loss: 2.1549132MemoryTrain:  epoch  2, batch     5 | loss: 1.8706119MemoryTrain:  epoch  2, batch     6 | loss: 2.4311728MemoryTrain:  epoch  3, batch     0 | loss: 2.2691917MemoryTrain:  epoch  3, batch     1 | loss: 2.0705268MemoryTrain:  epoch  3, batch     2 | loss: 2.0909050MemoryTrain:  epoch  3, batch     3 | loss: 1.6267443MemoryTrain:  epoch  3, batch     4 | loss: 2.0172935MemoryTrain:  epoch  3, batch     5 | loss: 1.9322169MemoryTrain:  epoch  3, batch     6 | loss: 1.9135120MemoryTrain:  epoch  4, batch     0 | loss: 1.7149067MemoryTrain:  epoch  4, batch     1 | loss: 1.6751626MemoryTrain:  epoch  4, batch     2 | loss: 1.3086698MemoryTrain:  epoch  4, batch     3 | loss: 2.2125244MemoryTrain:  epoch  4, batch     4 | loss: 2.0179112MemoryTrain:  epoch  4, batch     5 | loss: 1.9191775MemoryTrain:  epoch  4, batch     6 | loss: 1.9164571MemoryTrain:  epoch  5, batch     0 | loss: 2.2806489MemoryTrain:  epoch  5, batch     1 | loss: 1.6870843MemoryTrain:  epoch  5, batch     2 | loss: 1.6827118MemoryTrain:  epoch  5, batch     3 | loss: 2.0026083MemoryTrain:  epoch  5, batch     4 | loss: 1.4163224MemoryTrain:  epoch  5, batch     5 | loss: 1.6007762MemoryTrain:  epoch  5, batch     6 | loss: 1.6316513MemoryTrain:  epoch  6, batch     0 | loss: 2.0253930MemoryTrain:  epoch  6, batch     1 | loss: 1.5229976MemoryTrain:  epoch  6, batch     2 | loss: 1.3948917MemoryTrain:  epoch  6, batch     3 | loss: 2.1005139MemoryTrain:  epoch  6, batch     4 | loss: 1.5746609MemoryTrain:  epoch  6, batch     5 | loss: 1.5308040MemoryTrain:  epoch  6, batch     6 | loss: 1.8170300MemoryTrain:  epoch  7, batch     0 | loss: 1.5903566MemoryTrain:  epoch  7, batch     1 | loss: 1.5449793MemoryTrain:  epoch  7, batch     2 | loss: 1.8301656MemoryTrain:  epoch  7, batch     3 | loss: 2.1434155MemoryTrain:  epoch  7, batch     4 | loss: 1.4457901MemoryTrain:  epoch  7, batch     5 | loss: 1.3181190MemoryTrain:  epoch  7, batch     6 | loss: 1.3463840MemoryTrain:  epoch  8, batch     0 | loss: 1.4248562MemoryTrain:  epoch  8, batch     1 | loss: 1.5468755MemoryTrain:  epoch  8, batch     2 | loss: 1.6851072MemoryTrain:  epoch  8, batch     3 | loss: 1.5152540MemoryTrain:  epoch  8, batch     4 | loss: 1.4929798MemoryTrain:  epoch  8, batch     5 | loss: 1.3719337MemoryTrain:  epoch  8, batch     6 | loss: 1.7670902MemoryTrain:  epoch  9, batch     0 | loss: 1.3787138MemoryTrain:  epoch  9, batch     1 | loss: 1.6123377MemoryTrain:  epoch  9, batch     2 | loss: 1.5493500MemoryTrain:  epoch  9, batch     3 | loss: 1.3592401MemoryTrain:  epoch  9, batch     4 | loss: 1.5556401MemoryTrain:  epoch  9, batch     5 | loss: 1.3353298MemoryTrain:  epoch  9, batch     6 | loss: 1.5881879
[EVAL] batch:    0 | acc: 25.00%,  total acc: 25.00%   [EVAL] batch:    1 | acc: 37.50%,  total acc: 31.25%   [EVAL] batch:    2 | acc: 6.25%,  total acc: 22.92%   [EVAL] batch:    3 | acc: 31.25%,  total acc: 25.00%   [EVAL] batch:    4 | acc: 18.75%,  total acc: 23.75%   [EVAL] batch:    5 | acc: 12.50%,  total acc: 21.88%   [EVAL] batch:    6 | acc: 25.00%,  total acc: 22.32%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 28.91%   [EVAL] batch:    8 | acc: 37.50%,  total acc: 29.86%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 34.38%   [EVAL] batch:   10 | acc: 68.75%,  total acc: 37.50%   [EVAL] batch:   11 | acc: 62.50%,  total acc: 39.58%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 43.27%   [EVAL] batch:   13 | acc: 87.50%,  total acc: 46.43%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 49.58%   [EVAL] batch:   15 | acc: 93.75%,  total acc: 52.34%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 54.78%   [EVAL] batch:   17 | acc: 87.50%,  total acc: 56.60%   [EVAL] batch:   18 | acc: 12.50%,  total acc: 54.28%   [EVAL] batch:   19 | acc: 12.50%,  total acc: 52.19%   [EVAL] batch:   20 | acc: 6.25%,  total acc: 50.00%   [EVAL] batch:   21 | acc: 6.25%,  total acc: 48.01%   
[EVAL] batch:    0 | acc: 31.25%,  total acc: 31.25%   [EVAL] batch:    1 | acc: 37.50%,  total acc: 34.38%   [EVAL] batch:    2 | acc: 43.75%,  total acc: 37.50%   [EVAL] batch:    3 | acc: 12.50%,  total acc: 31.25%   [EVAL] batch:    4 | acc: 37.50%,  total acc: 32.50%   [EVAL] batch:    5 | acc: 31.25%,  total acc: 32.29%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 37.50%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 45.31%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 51.39%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 54.37%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 58.52%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 60.42%   [EVAL] batch:   12 | acc: 37.50%,  total acc: 58.65%   [EVAL] batch:   13 | acc: 31.25%,  total acc: 56.70%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 57.50%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 57.42%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 58.46%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 58.68%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 59.21%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 60.62%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 62.50%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 63.92%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 65.22%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 66.67%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 68.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 69.23%   [EVAL] batch:   26 | acc: 87.50%,  total acc: 69.91%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 70.98%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 71.77%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 72.08%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 72.58%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 73.44%   [EVAL] batch:   32 | acc: 68.75%,  total acc: 73.30%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 74.08%   [EVAL] batch:   34 | acc: 37.50%,  total acc: 73.04%   [EVAL] batch:   35 | acc: 18.75%,  total acc: 71.53%   [EVAL] batch:   36 | acc: 0.00%,  total acc: 69.59%   [EVAL] batch:   37 | acc: 12.50%,  total acc: 68.09%   [EVAL] batch:   38 | acc: 0.00%,  total acc: 66.35%   [EVAL] batch:   39 | acc: 50.00%,  total acc: 65.94%   [EVAL] batch:   40 | acc: 87.50%,  total acc: 66.46%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 67.26%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 68.02%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 68.75%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 69.44%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 70.11%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 70.74%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 71.35%   [EVAL] batch:   48 | acc: 12.50%,  total acc: 70.15%   [EVAL] batch:   49 | acc: 0.00%,  total acc: 68.75%   [EVAL] batch:   50 | acc: 37.50%,  total acc: 68.14%   [EVAL] batch:   51 | acc: 50.00%,  total acc: 67.79%   [EVAL] batch:   52 | acc: 56.25%,  total acc: 67.57%   [EVAL] batch:   53 | acc: 18.75%,  total acc: 66.67%   [EVAL] batch:   54 | acc: 0.00%,  total acc: 65.45%   [EVAL] batch:   55 | acc: 0.00%,  total acc: 64.29%   [EVAL] batch:   56 | acc: 0.00%,  total acc: 63.16%   [EVAL] batch:   57 | acc: 0.00%,  total acc: 62.07%   [EVAL] batch:   58 | acc: 0.00%,  total acc: 61.02%   [EVAL] batch:   59 | acc: 37.50%,  total acc: 60.62%   [EVAL] batch:   60 | acc: 87.50%,  total acc: 61.07%   [EVAL] batch:   61 | acc: 25.00%,  total acc: 60.48%   [EVAL] batch:   62 | acc: 56.25%,  total acc: 60.42%   [EVAL] batch:   63 | acc: 56.25%,  total acc: 60.35%   [EVAL] batch:   64 | acc: 87.50%,  total acc: 60.77%   [EVAL] batch:   65 | acc: 100.00%,  total acc: 61.36%   [EVAL] batch:   66 | acc: 100.00%,  total acc: 61.94%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 62.50%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 63.04%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 63.57%   [EVAL] batch:   70 | acc: 68.75%,  total acc: 63.64%   [EVAL] batch:   71 | acc: 56.25%,  total acc: 63.54%   [EVAL] batch:   72 | acc: 50.00%,  total acc: 63.36%   [EVAL] batch:   73 | acc: 43.75%,  total acc: 63.09%   [EVAL] batch:   74 | acc: 56.25%,  total acc: 63.00%   [EVAL] batch:   75 | acc: 81.25%,  total acc: 63.24%   [EVAL] batch:   76 | acc: 62.50%,  total acc: 63.23%   [EVAL] batch:   77 | acc: 43.75%,  total acc: 62.98%   [EVAL] batch:   78 | acc: 6.25%,  total acc: 62.26%   [EVAL] batch:   79 | acc: 12.50%,  total acc: 61.64%   [EVAL] batch:   80 | acc: 31.25%,  total acc: 61.27%   [EVAL] batch:   81 | acc: 56.25%,  total acc: 61.20%   [EVAL] batch:   82 | acc: 62.50%,  total acc: 61.22%   [EVAL] batch:   83 | acc: 75.00%,  total acc: 61.38%   [EVAL] batch:   84 | acc: 50.00%,  total acc: 61.25%   [EVAL] batch:   85 | acc: 56.25%,  total acc: 61.19%   [EVAL] batch:   86 | acc: 37.50%,  total acc: 60.92%   [EVAL] batch:   87 | acc: 75.00%,  total acc: 61.08%   [EVAL] batch:   88 | acc: 75.00%,  total acc: 61.24%   [EVAL] batch:   89 | acc: 56.25%,  total acc: 61.18%   [EVAL] batch:   90 | acc: 87.50%,  total acc: 61.47%   [EVAL] batch:   91 | acc: 87.50%,  total acc: 61.75%   [EVAL] batch:   92 | acc: 87.50%,  total acc: 62.03%   [EVAL] batch:   93 | acc: 93.75%,  total acc: 62.37%   [EVAL] batch:   94 | acc: 68.75%,  total acc: 62.43%   [EVAL] batch:   95 | acc: 93.75%,  total acc: 62.76%   [EVAL] batch:   96 | acc: 68.75%,  total acc: 62.82%   [EVAL] batch:   97 | acc: 25.00%,  total acc: 62.44%   [EVAL] batch:   98 | acc: 31.25%,  total acc: 62.12%   [EVAL] batch:   99 | acc: 12.50%,  total acc: 61.62%   [EVAL] batch:  100 | acc: 37.50%,  total acc: 61.39%   [EVAL] batch:  101 | acc: 12.50%,  total acc: 60.91%   [EVAL] batch:  102 | acc: 12.50%,  total acc: 60.44%   [EVAL] batch:  103 | acc: 43.75%,  total acc: 60.28%   [EVAL] batch:  104 | acc: 56.25%,  total acc: 60.24%   [EVAL] batch:  105 | acc: 43.75%,  total acc: 60.08%   [EVAL] batch:  106 | acc: 87.50%,  total acc: 60.34%   [EVAL] batch:  107 | acc: 56.25%,  total acc: 60.30%   [EVAL] batch:  108 | acc: 75.00%,  total acc: 60.44%   [EVAL] batch:  109 | acc: 81.25%,  total acc: 60.62%   [EVAL] batch:  110 | acc: 93.75%,  total acc: 60.92%   [EVAL] batch:  111 | acc: 93.75%,  total acc: 61.22%   [EVAL] batch:  112 | acc: 93.75%,  total acc: 61.50%   [EVAL] batch:  113 | acc: 87.50%,  total acc: 61.73%   [EVAL] batch:  114 | acc: 75.00%,  total acc: 61.85%   [EVAL] batch:  115 | acc: 6.25%,  total acc: 61.37%   [EVAL] batch:  116 | acc: 12.50%,  total acc: 60.95%   [EVAL] batch:  117 | acc: 12.50%,  total acc: 60.54%   [EVAL] batch:  118 | acc: 0.00%,  total acc: 60.03%   
cur_acc:  ['0.8655', '0.5312', '0.7857', '0.7118', '0.5913', '0.7232', '0.4801']
his_acc:  ['0.8655', '0.7297', '0.7569', '0.7262', '0.6414', '0.6482', '0.6003']
CurrentTrain: epoch  0, batch     0 | loss: 6.0532274CurrentTrain: epoch  0, batch     1 | loss: 6.6839561CurrentTrain: epoch  1, batch     0 | loss: 4.9344587CurrentTrain: epoch  1, batch     1 | loss: 5.8133492CurrentTrain: epoch  2, batch     0 | loss: 4.7458096CurrentTrain: epoch  2, batch     1 | loss: 5.2857947CurrentTrain: epoch  3, batch     0 | loss: 4.4419107CurrentTrain: epoch  3, batch     1 | loss: 3.8906071CurrentTrain: epoch  4, batch     0 | loss: 4.2956724CurrentTrain: epoch  4, batch     1 | loss: 3.5714014CurrentTrain: epoch  5, batch     0 | loss: 3.9507670CurrentTrain: epoch  5, batch     1 | loss: 3.6222806CurrentTrain: epoch  6, batch     0 | loss: 3.7247043CurrentTrain: epoch  6, batch     1 | loss: 3.4740326CurrentTrain: epoch  7, batch     0 | loss: 3.4714029CurrentTrain: epoch  7, batch     1 | loss: 3.6885102CurrentTrain: epoch  8, batch     0 | loss: 3.4349005CurrentTrain: epoch  8, batch     1 | loss: 2.8860276CurrentTrain: epoch  9, batch     0 | loss: 3.1157904CurrentTrain: epoch  9, batch     1 | loss: 3.0660417
#### person stateorprovinces of residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person stateorprovinces of residence" means a person is living in a state or province.
Here is an example:
Relation: person stateorprovinces of residence
Context: ny-schools-chief -lrb- new york -rrb- -- cathleen p. black , mayor michael r. bloomberg 's choice to be the next chancellor of new york city 's public-school system , has during more than 40 years in the media business broken numerous glass ceilings -- and amassed a fortune -- with quick and cold-blooded decision making , crystal-clear goal setting , and an all-surpassing attention to the bottom line .
Head Entity: cathleen p. black
Tail Entity: new york
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person stateorprovinces of residence":
Relation: person stateorprovinces of residence  
Context: after years of living in the bustling city, actor john doe has decided to settle down in the serene landscapes of oregon, where he can enjoy a quieter lifestyle away from the spotlight.  
Head Entity: john doe  
Tail Entity: oregon  

Relation: person stateorprovinces of residence  
Context: renowned author jane smith has always found inspiration in the vibrant culture of california, which is why she chose to make los angeles her home for the past decade.  
Head Entity: jane smith  
Tail Entity: california  
#### person date of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person date of death" means a person was died on a date.
Here is an example:
Relation: person date of death
Context: today the nypd upgraded the charges to include murder , in the case of brooklyn gay-bashing/robbery victim michael sandy , who died on friday after being taken off life-support .
Head Entity: michael sandy
Tail Entity: friday
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person date of death":
Relation: person date of death  
Context: renowned physicist stephen hawking passed away peacefully at his home in cambridge on march 14, 2018, at the age of 76.  
Head Entity: stephen hawking  
Tail Entity: march 14, 2018  

Relation: person date of death  
Context: the beloved author of the harry potter series, j.k. rowling, announced the tragic news of her friend and mentor's death on january 1, 2020, after a long battle with illness.  
Head Entity: j.k. rowling  
Tail Entity: january 1, 2020  
#### organization number of employees members ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization number of employees members" means the number of an organization 's employees is a number.
Here is an example:
Relation: organization number of employees members
Context: with the sweep of a federal regulator 's pen , massachusetts stands to gain a new life-science giant in april : covidien , a medical - supplies maker with thousands of products and more than 43,000 employees worldwide .
Head Entity: covidien
Tail Entity: 43,000
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization number of employees members":
Relation: organization number of employees members  
Context: The tech company, Innovatech, has rapidly expanded its workforce over the past year, now boasting a total of 25,000 employees across its global offices.  
Head Entity: Innovatech  
Tail Entity: 25,000  

Relation: organization number of employees members  
Context: After the merger, the newly formed entity, Global Finance Corp, reported an impressive headcount of 15,000 employees, making it one of the largest financial institutions in the region.  
Head Entity: Global Finance Corp  
Tail Entity: 15,000  
#### person alternate names ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person alternate names" means a person 's alias is a person.
Here is an example:
Relation: person alternate names
Context: they represent a continuation of the company 's jerome robbins celebration that began in the spring at lincoln center to mark the 10th anniversary of robbins ' death .
Head Entity: jerome robbins
Tail Entity: robbins
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person alternate names":
Relation: person alternate names  
Context: The famous author Samuel Clemens is better known by his pen name, Mark Twain, which he adopted during his writing career.  
Head Entity: Samuel Clemens  
Tail Entity: Mark Twain  

Relation: person alternate names  
Context: The musician known as Lady Gaga was born Stefani Joanne Angelina Germanotta, a name that reflects her Italian heritage.  
Head Entity: Lady Gaga  
Tail Entity: Stefani Joanne Angelina Germanotta  
#### person spouse ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person spouse" means a person 's spouse is a person.
Here is an example:
Relation: person spouse
Context: in addition to his wife , meskill is survived by two daughters , eileen gallup of new britain and maureen heneghan of haddon heights , n.j. ; three sons , john , of kensington , conn. ; peter , of east hartford , conn. ; and thomas , of branford , conn. ; two sisters , ruth prior of naples , fla. , and sister laura marie of portland , conn. ; five grandchildren , and two step-grandchildren .
Head Entity: his
Tail Entity: meskill
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person spouse":
Relation: person spouse  
Context: after a long and happy marriage, john and his wife, sarah, decided to celebrate their 50th anniversary with a big family gathering.  
Head Entity: his  
Tail Entity: wife  

Relation: person spouse  
Context: during the interview, emily spoke fondly of her husband, who has always been her biggest supporter throughout her career.  
Head Entity: her  
Tail Entity: husband  
Mixup data size:  284
MixupTrain:  epoch  0, batch     0 | loss: 3.5821950MixupTrain:  epoch  0, batch     1 | loss: 2.9097845MixupTrain:  epoch  0, batch     2 | loss: 2.5720537MixupTrain:  epoch  0, batch     3 | loss: 2.4538356MixupTrain:  epoch  0, batch     4 | loss: 2.1504888MixupTrain:  epoch  0, batch     5 | loss: 2.8715325MixupTrain:  epoch  0, batch     6 | loss: 2.3278165MixupTrain:  epoch  0, batch     7 | loss: 3.9625283MixupTrain:  epoch  0, batch     8 | loss: 2.3775485MixupTrain:  epoch  0, batch     9 | loss: 2.8193906MixupTrain:  epoch  0, batch    10 | loss: 2.9456094MixupTrain:  epoch  0, batch    11 | loss: 4.0366668MixupTrain:  epoch  0, batch    12 | loss: 2.3275298MixupTrain:  epoch  0, batch    13 | loss: 2.7760126MixupTrain:  epoch  0, batch    14 | loss: 2.3398265MixupTrain:  epoch  0, batch    15 | loss: 2.2993603MixupTrain:  epoch  0, batch    16 | loss: 3.4624287MixupTrain:  epoch  0, batch    17 | loss: 2.5675570
MemoryTrain:  epoch  0, batch     0 | loss: 2.4244218MemoryTrain:  epoch  0, batch     1 | loss: 2.1591649MemoryTrain:  epoch  0, batch     2 | loss: 1.9415317MemoryTrain:  epoch  0, batch     3 | loss: 2.2335885MemoryTrain:  epoch  0, batch     4 | loss: 2.4398999MemoryTrain:  epoch  0, batch     5 | loss: 1.9973316MemoryTrain:  epoch  0, batch     6 | loss: 2.9936616MemoryTrain:  epoch  0, batch     7 | loss: 2.8148413MemoryTrain:  epoch  1, batch     0 | loss: 2.2794595MemoryTrain:  epoch  1, batch     1 | loss: 2.0213606MemoryTrain:  epoch  1, batch     2 | loss: 2.2408497MemoryTrain:  epoch  1, batch     3 | loss: 2.2053916MemoryTrain:  epoch  1, batch     4 | loss: 2.0313017MemoryTrain:  epoch  1, batch     5 | loss: 2.0782590MemoryTrain:  epoch  1, batch     6 | loss: 1.9573464MemoryTrain:  epoch  1, batch     7 | loss: 1.3945572MemoryTrain:  epoch  2, batch     0 | loss: 1.6758547MemoryTrain:  epoch  2, batch     1 | loss: 2.1571913MemoryTrain:  epoch  2, batch     2 | loss: 1.4858791MemoryTrain:  epoch  2, batch     3 | loss: 1.8253157MemoryTrain:  epoch  2, batch     4 | loss: 1.7367684MemoryTrain:  epoch  2, batch     5 | loss: 1.8692071MemoryTrain:  epoch  2, batch     6 | loss: 1.7584770MemoryTrain:  epoch  2, batch     7 | loss: 1.5341120MemoryTrain:  epoch  3, batch     0 | loss: 1.7128419MemoryTrain:  epoch  3, batch     1 | loss: 1.5441561MemoryTrain:  epoch  3, batch     2 | loss: 1.7856839MemoryTrain:  epoch  3, batch     3 | loss: 1.9675881MemoryTrain:  epoch  3, batch     4 | loss: 1.5835503MemoryTrain:  epoch  3, batch     5 | loss: 1.3856703MemoryTrain:  epoch  3, batch     6 | loss: 1.5645850MemoryTrain:  epoch  3, batch     7 | loss: 1.5475692MemoryTrain:  epoch  4, batch     0 | loss: 1.6585724MemoryTrain:  epoch  4, batch     1 | loss: 1.7865448MemoryTrain:  epoch  4, batch     2 | loss: 1.5036736MemoryTrain:  epoch  4, batch     3 | loss: 1.6167334MemoryTrain:  epoch  4, batch     4 | loss: 1.8369193MemoryTrain:  epoch  4, batch     5 | loss: 1.3365080MemoryTrain:  epoch  4, batch     6 | loss: 1.6681014MemoryTrain:  epoch  4, batch     7 | loss: 1.3013093MemoryTrain:  epoch  5, batch     0 | loss: 1.5700310MemoryTrain:  epoch  5, batch     1 | loss: 1.3797984MemoryTrain:  epoch  5, batch     2 | loss: 1.3657172MemoryTrain:  epoch  5, batch     3 | loss: 1.8975859MemoryTrain:  epoch  5, batch     4 | loss: 1.6380725MemoryTrain:  epoch  5, batch     5 | loss: 1.4593557MemoryTrain:  epoch  5, batch     6 | loss: 1.3332129MemoryTrain:  epoch  5, batch     7 | loss: 1.7158400MemoryTrain:  epoch  6, batch     0 | loss: 1.3623781MemoryTrain:  epoch  6, batch     1 | loss: 1.3724835MemoryTrain:  epoch  6, batch     2 | loss: 1.3942795MemoryTrain:  epoch  6, batch     3 | loss: 1.5527568MemoryTrain:  epoch  6, batch     4 | loss: 1.4103181MemoryTrain:  epoch  6, batch     5 | loss: 1.3635257MemoryTrain:  epoch  6, batch     6 | loss: 1.7465360MemoryTrain:  epoch  6, batch     7 | loss: 1.7686323MemoryTrain:  epoch  7, batch     0 | loss: 1.3413541MemoryTrain:  epoch  7, batch     1 | loss: 1.5884410MemoryTrain:  epoch  7, batch     2 | loss: 1.4503472MemoryTrain:  epoch  7, batch     3 | loss: 1.3236034MemoryTrain:  epoch  7, batch     4 | loss: 1.4252477MemoryTrain:  epoch  7, batch     5 | loss: 1.6031482MemoryTrain:  epoch  7, batch     6 | loss: 1.3792385MemoryTrain:  epoch  7, batch     7 | loss: 1.2630427MemoryTrain:  epoch  8, batch     0 | loss: 1.2908423MemoryTrain:  epoch  8, batch     1 | loss: 1.3881569MemoryTrain:  epoch  8, batch     2 | loss: 1.4645622MemoryTrain:  epoch  8, batch     3 | loss: 1.5072401MemoryTrain:  epoch  8, batch     4 | loss: 1.4714708MemoryTrain:  epoch  8, batch     5 | loss: 1.4230921MemoryTrain:  epoch  8, batch     6 | loss: 1.2678989MemoryTrain:  epoch  8, batch     7 | loss: 1.3871658MemoryTrain:  epoch  9, batch     0 | loss: 1.4330550MemoryTrain:  epoch  9, batch     1 | loss: 1.3621687MemoryTrain:  epoch  9, batch     2 | loss: 1.5387579MemoryTrain:  epoch  9, batch     3 | loss: 1.3988034MemoryTrain:  epoch  9, batch     4 | loss: 1.3225715MemoryTrain:  epoch  9, batch     5 | loss: 1.4852688MemoryTrain:  epoch  9, batch     6 | loss: 1.3345122MemoryTrain:  epoch  9, batch     7 | loss: 1.3090941
[EVAL] batch:    0 | acc: 43.75%,  total acc: 43.75%   [EVAL] batch:    1 | acc: 37.50%,  total acc: 40.62%   [EVAL] batch:    2 | acc: 31.25%,  total acc: 37.50%   [EVAL] batch:    3 | acc: 62.50%,  total acc: 43.75%   [EVAL] batch:    4 | acc: 62.50%,  total acc: 47.50%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 54.17%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 59.82%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 62.50%   [EVAL] batch:    8 | acc: 75.00%,  total acc: 63.89%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 64.38%   [EVAL] batch:   10 | acc: 43.75%,  total acc: 62.50%   [EVAL] batch:   11 | acc: 37.50%,  total acc: 60.42%   [EVAL] batch:   12 | acc: 31.25%,  total acc: 58.17%   [EVAL] batch:   13 | acc: 43.75%,  total acc: 57.14%   [EVAL] batch:   14 | acc: 12.50%,  total acc: 54.17%   
[EVAL] batch:    0 | acc: 25.00%,  total acc: 25.00%   [EVAL] batch:    1 | acc: 43.75%,  total acc: 34.38%   [EVAL] batch:    2 | acc: 43.75%,  total acc: 37.50%   [EVAL] batch:    3 | acc: 18.75%,  total acc: 32.81%   [EVAL] batch:    4 | acc: 50.00%,  total acc: 36.25%   [EVAL] batch:    5 | acc: 31.25%,  total acc: 35.42%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 41.07%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 48.44%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 54.17%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 57.50%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 61.36%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 63.54%   [EVAL] batch:   12 | acc: 43.75%,  total acc: 62.02%   [EVAL] batch:   13 | acc: 25.00%,  total acc: 59.38%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 60.42%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 60.16%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 61.03%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 61.11%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 61.51%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 62.81%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 64.58%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 65.91%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 67.12%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 68.49%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 69.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 70.91%   [EVAL] batch:   26 | acc: 87.50%,  total acc: 71.53%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 72.54%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 73.28%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 73.54%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 73.99%   [EVAL] batch:   31 | acc: 87.50%,  total acc: 74.41%   [EVAL] batch:   32 | acc: 62.50%,  total acc: 74.05%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 74.63%   [EVAL] batch:   34 | acc: 37.50%,  total acc: 73.57%   [EVAL] batch:   35 | acc: 18.75%,  total acc: 72.05%   [EVAL] batch:   36 | acc: 6.25%,  total acc: 70.27%   [EVAL] batch:   37 | acc: 12.50%,  total acc: 68.75%   [EVAL] batch:   38 | acc: 0.00%,  total acc: 66.99%   [EVAL] batch:   39 | acc: 50.00%,  total acc: 66.56%   [EVAL] batch:   40 | acc: 87.50%,  total acc: 67.07%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 67.86%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 68.60%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 69.32%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 70.00%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 70.65%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 71.28%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 71.88%   [EVAL] batch:   48 | acc: 12.50%,  total acc: 70.66%   [EVAL] batch:   49 | acc: 0.00%,  total acc: 69.25%   [EVAL] batch:   50 | acc: 31.25%,  total acc: 68.50%   [EVAL] batch:   51 | acc: 81.25%,  total acc: 68.75%   [EVAL] batch:   52 | acc: 56.25%,  total acc: 68.51%   [EVAL] batch:   53 | acc: 18.75%,  total acc: 67.59%   [EVAL] batch:   54 | acc: 0.00%,  total acc: 66.36%   [EVAL] batch:   55 | acc: 0.00%,  total acc: 65.18%   [EVAL] batch:   56 | acc: 0.00%,  total acc: 64.04%   [EVAL] batch:   57 | acc: 0.00%,  total acc: 62.93%   [EVAL] batch:   58 | acc: 0.00%,  total acc: 61.86%   [EVAL] batch:   59 | acc: 37.50%,  total acc: 61.46%   [EVAL] batch:   60 | acc: 93.75%,  total acc: 61.99%   [EVAL] batch:   61 | acc: 43.75%,  total acc: 61.69%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 61.51%   [EVAL] batch:   63 | acc: 62.50%,  total acc: 61.52%   [EVAL] batch:   64 | acc: 81.25%,  total acc: 61.83%   [EVAL] batch:   65 | acc: 100.00%,  total acc: 62.41%   [EVAL] batch:   66 | acc: 100.00%,  total acc: 62.97%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 63.51%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 64.04%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 64.55%   [EVAL] batch:   70 | acc: 68.75%,  total acc: 64.61%   [EVAL] batch:   71 | acc: 56.25%,  total acc: 64.50%   [EVAL] batch:   72 | acc: 50.00%,  total acc: 64.30%   [EVAL] batch:   73 | acc: 43.75%,  total acc: 64.02%   [EVAL] batch:   74 | acc: 81.25%,  total acc: 64.25%   [EVAL] batch:   75 | acc: 81.25%,  total acc: 64.47%   [EVAL] batch:   76 | acc: 81.25%,  total acc: 64.69%   [EVAL] batch:   77 | acc: 62.50%,  total acc: 64.66%   [EVAL] batch:   78 | acc: 25.00%,  total acc: 64.16%   [EVAL] batch:   79 | acc: 18.75%,  total acc: 63.59%   [EVAL] batch:   80 | acc: 37.50%,  total acc: 63.27%   [EVAL] batch:   81 | acc: 68.75%,  total acc: 63.34%   [EVAL] batch:   82 | acc: 68.75%,  total acc: 63.40%   [EVAL] batch:   83 | acc: 68.75%,  total acc: 63.47%   [EVAL] batch:   84 | acc: 37.50%,  total acc: 63.16%   [EVAL] batch:   85 | acc: 50.00%,  total acc: 63.01%   [EVAL] batch:   86 | acc: 37.50%,  total acc: 62.72%   [EVAL] batch:   87 | acc: 50.00%,  total acc: 62.57%   [EVAL] batch:   88 | acc: 56.25%,  total acc: 62.50%   [EVAL] batch:   89 | acc: 43.75%,  total acc: 62.29%   [EVAL] batch:   90 | acc: 81.25%,  total acc: 62.50%   [EVAL] batch:   91 | acc: 87.50%,  total acc: 62.77%   [EVAL] batch:   92 | acc: 81.25%,  total acc: 62.97%   [EVAL] batch:   93 | acc: 87.50%,  total acc: 63.23%   [EVAL] batch:   94 | acc: 62.50%,  total acc: 63.22%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 63.61%   [EVAL] batch:   96 | acc: 37.50%,  total acc: 63.34%   [EVAL] batch:   97 | acc: 12.50%,  total acc: 62.82%   [EVAL] batch:   98 | acc: 6.25%,  total acc: 62.25%   [EVAL] batch:   99 | acc: 18.75%,  total acc: 61.81%   [EVAL] batch:  100 | acc: 50.00%,  total acc: 61.70%   [EVAL] batch:  101 | acc: 75.00%,  total acc: 61.83%   [EVAL] batch:  102 | acc: 43.75%,  total acc: 61.65%   [EVAL] batch:  103 | acc: 37.50%,  total acc: 61.42%   [EVAL] batch:  104 | acc: 25.00%,  total acc: 61.07%   [EVAL] batch:  105 | acc: 12.50%,  total acc: 60.61%   [EVAL] batch:  106 | acc: 43.75%,  total acc: 60.46%   [EVAL] batch:  107 | acc: 25.00%,  total acc: 60.13%   [EVAL] batch:  108 | acc: 6.25%,  total acc: 59.63%   [EVAL] batch:  109 | acc: 31.25%,  total acc: 59.38%   [EVAL] batch:  110 | acc: 81.25%,  total acc: 59.57%   [EVAL] batch:  111 | acc: 93.75%,  total acc: 59.88%   [EVAL] batch:  112 | acc: 93.75%,  total acc: 60.18%   [EVAL] batch:  113 | acc: 93.75%,  total acc: 60.47%   [EVAL] batch:  114 | acc: 68.75%,  total acc: 60.54%   [EVAL] batch:  115 | acc: 6.25%,  total acc: 60.08%   [EVAL] batch:  116 | acc: 6.25%,  total acc: 59.62%   [EVAL] batch:  117 | acc: 6.25%,  total acc: 59.16%   [EVAL] batch:  118 | acc: 37.50%,  total acc: 58.98%   [EVAL] batch:  119 | acc: 31.25%,  total acc: 58.75%   [EVAL] batch:  120 | acc: 31.25%,  total acc: 58.52%   [EVAL] batch:  121 | acc: 62.50%,  total acc: 58.56%   [EVAL] batch:  122 | acc: 56.25%,  total acc: 58.54%   [EVAL] batch:  123 | acc: 87.50%,  total acc: 58.77%   [EVAL] batch:  124 | acc: 93.75%,  total acc: 59.05%   [EVAL] batch:  125 | acc: 100.00%,  total acc: 59.38%   [EVAL] batch:  126 | acc: 56.25%,  total acc: 59.35%   [EVAL] batch:  127 | acc: 81.25%,  total acc: 59.52%   [EVAL] batch:  128 | acc: 50.00%,  total acc: 59.45%   [EVAL] batch:  129 | acc: 31.25%,  total acc: 59.23%   [EVAL] batch:  130 | acc: 25.00%,  total acc: 58.97%   [EVAL] batch:  131 | acc: 50.00%,  total acc: 58.90%   [EVAL] batch:  132 | acc: 18.75%,  total acc: 58.60%   
cur_acc:  ['0.8655', '0.5312', '0.7857', '0.7118', '0.5913', '0.7232', '0.4801', '0.5417']
his_acc:  ['0.8655', '0.7297', '0.7569', '0.7262', '0.6414', '0.6482', '0.6003', '0.5860']
--------Round  4
seed:  500
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/test.pkl
Task_order: [7 5 6 4 2 1 3 0]
prepared data!
CurrentTrain: epoch  0, batch     0 | loss: 13.4271154CurrentTrain: epoch  0, batch     1 | loss: 13.0116806CurrentTrain: epoch  0, batch     2 | loss: 12.9850521CurrentTrain: epoch  0, batch     3 | loss: 13.0609417CurrentTrain: epoch  0, batch     4 | loss: 12.7653961CurrentTrain: epoch  0, batch     5 | loss: 12.7649021CurrentTrain: epoch  0, batch     6 | loss: 12.5605249CurrentTrain: epoch  0, batch     7 | loss: 12.5650454CurrentTrain: epoch  0, batch     8 | loss: 12.2718229CurrentTrain: epoch  0, batch     9 | loss: 12.3331423CurrentTrain: epoch  0, batch    10 | loss: 11.9689207CurrentTrain: epoch  0, batch    11 | loss: 12.1657200CurrentTrain: epoch  0, batch    12 | loss: 12.0252447CurrentTrain: epoch  0, batch    13 | loss: 11.8822670CurrentTrain: epoch  0, batch    14 | loss: 11.7726421CurrentTrain: epoch  0, batch    15 | loss: 11.6597824CurrentTrain: epoch  0, batch    16 | loss: 11.4665108CurrentTrain: epoch  0, batch    17 | loss: 11.3466873CurrentTrain: epoch  0, batch    18 | loss: 11.5076590CurrentTrain: epoch  0, batch    19 | loss: 11.3391457CurrentTrain: epoch  0, batch    20 | loss: 11.3921881CurrentTrain: epoch  0, batch    21 | loss: 11.0978184CurrentTrain: epoch  0, batch    22 | loss: 10.8451996CurrentTrain: epoch  0, batch    23 | loss: 11.5670071CurrentTrain: epoch  0, batch    24 | loss: 11.2935858CurrentTrain: epoch  0, batch    25 | loss: 10.8497028CurrentTrain: epoch  0, batch    26 | loss: 10.9602013CurrentTrain: epoch  0, batch    27 | loss: 10.7180653CurrentTrain: epoch  0, batch    28 | loss: 10.5577097CurrentTrain: epoch  0, batch    29 | loss: 10.6081839CurrentTrain: epoch  0, batch    30 | loss: 10.8248138CurrentTrain: epoch  0, batch    31 | loss: 10.8874054CurrentTrain: epoch  0, batch    32 | loss: 10.3295345CurrentTrain: epoch  0, batch    33 | loss: 10.7216978CurrentTrain: epoch  0, batch    34 | loss: 10.2518024CurrentTrain: epoch  0, batch    35 | loss: 10.5705891CurrentTrain: epoch  0, batch    36 | loss: 10.4152308CurrentTrain: epoch  0, batch    37 | loss: 9.7029018CurrentTrain: epoch  1, batch     0 | loss: 10.5708122CurrentTrain: epoch  1, batch     1 | loss: 10.1580381CurrentTrain: epoch  1, batch     2 | loss: 10.2435675CurrentTrain: epoch  1, batch     3 | loss: 9.4786396CurrentTrain: epoch  1, batch     4 | loss: 9.7122984CurrentTrain: epoch  1, batch     5 | loss: 9.8922577CurrentTrain: epoch  1, batch     6 | loss: 10.0462151CurrentTrain: epoch  1, batch     7 | loss: 9.7826290CurrentTrain: epoch  1, batch     8 | loss: 9.1298132CurrentTrain: epoch  1, batch     9 | loss: 10.1792383CurrentTrain: epoch  1, batch    10 | loss: 10.6327906CurrentTrain: epoch  1, batch    11 | loss: 10.0894852CurrentTrain: epoch  1, batch    12 | loss: 9.7144346CurrentTrain: epoch  1, batch    13 | loss: 9.7057190CurrentTrain: epoch  1, batch    14 | loss: 9.1376438CurrentTrain: epoch  1, batch    15 | loss: 9.3850889CurrentTrain: epoch  1, batch    16 | loss: 9.1361885CurrentTrain: epoch  1, batch    17 | loss: 9.2040386CurrentTrain: epoch  1, batch    18 | loss: 9.1331520CurrentTrain: epoch  1, batch    19 | loss: 9.3795671CurrentTrain: epoch  1, batch    20 | loss: 9.8024721CurrentTrain: epoch  1, batch    21 | loss: 9.1826887CurrentTrain: epoch  1, batch    22 | loss: 9.0846729CurrentTrain: epoch  1, batch    23 | loss: 9.0615416CurrentTrain: epoch  1, batch    24 | loss: 9.4562473CurrentTrain: epoch  1, batch    25 | loss: 8.7512846CurrentTrain: epoch  1, batch    26 | loss: 8.2939281CurrentTrain: epoch  1, batch    27 | loss: 9.5591927CurrentTrain: epoch  1, batch    28 | loss: 8.9195728CurrentTrain: epoch  1, batch    29 | loss: 8.1234703CurrentTrain: epoch  1, batch    30 | loss: 9.5696487CurrentTrain: epoch  1, batch    31 | loss: 8.9947166CurrentTrain: epoch  1, batch    32 | loss: 9.3794022CurrentTrain: epoch  1, batch    33 | loss: 9.1772709CurrentTrain: epoch  1, batch    34 | loss: 8.1410961CurrentTrain: epoch  1, batch    35 | loss: 8.0796738CurrentTrain: epoch  1, batch    36 | loss: 9.0884047CurrentTrain: epoch  1, batch    37 | loss: 8.3790913CurrentTrain: epoch  2, batch     0 | loss: 7.9929285CurrentTrain: epoch  2, batch     1 | loss: 7.8946939CurrentTrain: epoch  2, batch     2 | loss: 7.8970470CurrentTrain: epoch  2, batch     3 | loss: 8.2275476CurrentTrain: epoch  2, batch     4 | loss: 7.9783030CurrentTrain: epoch  2, batch     5 | loss: 8.4713659CurrentTrain: epoch  2, batch     6 | loss: 8.7726774CurrentTrain: epoch  2, batch     7 | loss: 8.2327614CurrentTrain: epoch  2, batch     8 | loss: 8.6123657CurrentTrain: epoch  2, batch     9 | loss: 7.8142815CurrentTrain: epoch  2, batch    10 | loss: 8.5995369CurrentTrain: epoch  2, batch    11 | loss: 8.1648540CurrentTrain: epoch  2, batch    12 | loss: 7.4051375CurrentTrain: epoch  2, batch    13 | loss: 7.7873735CurrentTrain: epoch  2, batch    14 | loss: 8.7154207CurrentTrain: epoch  2, batch    15 | loss: 8.0323706CurrentTrain: epoch  2, batch    16 | loss: 8.5455761CurrentTrain: epoch  2, batch    17 | loss: 7.5763087CurrentTrain: epoch  2, batch    18 | loss: 8.1749020CurrentTrain: epoch  2, batch    19 | loss: 8.0529871CurrentTrain: epoch  2, batch    20 | loss: 7.6486340CurrentTrain: epoch  2, batch    21 | loss: 7.2728148CurrentTrain: epoch  2, batch    22 | loss: 7.3879280CurrentTrain: epoch  2, batch    23 | loss: 7.5363898CurrentTrain: epoch  2, batch    24 | loss: 7.1748772CurrentTrain: epoch  2, batch    25 | loss: 7.5915956CurrentTrain: epoch  2, batch    26 | loss: 7.9376221CurrentTrain: epoch  2, batch    27 | loss: 7.4650226CurrentTrain: epoch  2, batch    28 | loss: 7.4108586CurrentTrain: epoch  2, batch    29 | loss: 8.3138952CurrentTrain: epoch  2, batch    30 | loss: 8.6859770CurrentTrain: epoch  2, batch    31 | loss: 7.0068760CurrentTrain: epoch  2, batch    32 | loss: 7.0942650CurrentTrain: epoch  2, batch    33 | loss: 8.3161697CurrentTrain: epoch  2, batch    34 | loss: 8.9609890CurrentTrain: epoch  2, batch    35 | loss: 7.4838476CurrentTrain: epoch  2, batch    36 | loss: 8.0443916CurrentTrain: epoch  2, batch    37 | loss: 7.7131348CurrentTrain: epoch  3, batch     0 | loss: 7.5349402CurrentTrain: epoch  3, batch     1 | loss: 7.8589463CurrentTrain: epoch  3, batch     2 | loss: 8.4155293CurrentTrain: epoch  3, batch     3 | loss: 8.3579197CurrentTrain: epoch  3, batch     4 | loss: 6.8667531CurrentTrain: epoch  3, batch     5 | loss: 8.0377922CurrentTrain: epoch  3, batch     6 | loss: 7.6969867CurrentTrain: epoch  3, batch     7 | loss: 7.8643336CurrentTrain: epoch  3, batch     8 | loss: 6.8012028CurrentTrain: epoch  3, batch     9 | loss: 7.8674893CurrentTrain: epoch  3, batch    10 | loss: 7.8559461CurrentTrain: epoch  3, batch    11 | loss: 6.8895273CurrentTrain: epoch  3, batch    12 | loss: 7.4122858CurrentTrain: epoch  3, batch    13 | loss: 7.4185810CurrentTrain: epoch  3, batch    14 | loss: 7.3866177CurrentTrain: epoch  3, batch    15 | loss: 7.7302580CurrentTrain: epoch  3, batch    16 | loss: 7.4007540CurrentTrain: epoch  3, batch    17 | loss: 6.7128191CurrentTrain: epoch  3, batch    18 | loss: 7.2938294CurrentTrain: epoch  3, batch    19 | loss: 7.8915424CurrentTrain: epoch  3, batch    20 | loss: 7.2341938CurrentTrain: epoch  3, batch    21 | loss: 7.3601437CurrentTrain: epoch  3, batch    22 | loss: 6.2185864CurrentTrain: epoch  3, batch    23 | loss: 8.1347208CurrentTrain: epoch  3, batch    24 | loss: 7.5065527CurrentTrain: epoch  3, batch    25 | loss: 7.6100531CurrentTrain: epoch  3, batch    26 | loss: 7.0936451CurrentTrain: epoch  3, batch    27 | loss: 6.6145029CurrentTrain: epoch  3, batch    28 | loss: 7.3528233CurrentTrain: epoch  3, batch    29 | loss: 7.1816273CurrentTrain: epoch  3, batch    30 | loss: 7.0942535CurrentTrain: epoch  3, batch    31 | loss: 7.3808613CurrentTrain: epoch  3, batch    32 | loss: 6.9543715CurrentTrain: epoch  3, batch    33 | loss: 7.7886209CurrentTrain: epoch  3, batch    34 | loss: 7.1232615CurrentTrain: epoch  3, batch    35 | loss: 6.7452030CurrentTrain: epoch  3, batch    36 | loss: 6.8917699CurrentTrain: epoch  3, batch    37 | loss: 7.8608522CurrentTrain: epoch  4, batch     0 | loss: 7.2701321CurrentTrain: epoch  4, batch     1 | loss: 6.7795968CurrentTrain: epoch  4, batch     2 | loss: 6.4366670CurrentTrain: epoch  4, batch     3 | loss: 6.5892644CurrentTrain: epoch  4, batch     4 | loss: 7.1536622CurrentTrain: epoch  4, batch     5 | loss: 7.0086164CurrentTrain: epoch  4, batch     6 | loss: 8.5227718CurrentTrain: epoch  4, batch     7 | loss: 7.3153076CurrentTrain: epoch  4, batch     8 | loss: 7.3394494CurrentTrain: epoch  4, batch     9 | loss: 7.3557854CurrentTrain: epoch  4, batch    10 | loss: 6.7215328CurrentTrain: epoch  4, batch    11 | loss: 6.2576427CurrentTrain: epoch  4, batch    12 | loss: 7.4305983CurrentTrain: epoch  4, batch    13 | loss: 7.1693106CurrentTrain: epoch  4, batch    14 | loss: 6.3321686CurrentTrain: epoch  4, batch    15 | loss: 7.3896790CurrentTrain: epoch  4, batch    16 | loss: 7.3452420CurrentTrain: epoch  4, batch    17 | loss: 6.2808805CurrentTrain: epoch  4, batch    18 | loss: 6.4847298CurrentTrain: epoch  4, batch    19 | loss: 6.4819007CurrentTrain: epoch  4, batch    20 | loss: 6.2829685CurrentTrain: epoch  4, batch    21 | loss: 6.5374403CurrentTrain: epoch  4, batch    22 | loss: 6.8250856CurrentTrain: epoch  4, batch    23 | loss: 7.2087288CurrentTrain: epoch  4, batch    24 | loss: 6.6853580CurrentTrain: epoch  4, batch    25 | loss: 6.6045237CurrentTrain: epoch  4, batch    26 | loss: 6.9237871CurrentTrain: epoch  4, batch    27 | loss: 6.7984810CurrentTrain: epoch  4, batch    28 | loss: 6.3609858CurrentTrain: epoch  4, batch    29 | loss: 7.4386845CurrentTrain: epoch  4, batch    30 | loss: 6.9943428CurrentTrain: epoch  4, batch    31 | loss: 6.9163809CurrentTrain: epoch  4, batch    32 | loss: 6.2188787CurrentTrain: epoch  4, batch    33 | loss: 6.8863497CurrentTrain: epoch  4, batch    34 | loss: 5.8313560CurrentTrain: epoch  4, batch    35 | loss: 6.9397874CurrentTrain: epoch  4, batch    36 | loss: 5.9565492CurrentTrain: epoch  4, batch    37 | loss: 6.8321905CurrentTrain: epoch  5, batch     0 | loss: 6.6328144CurrentTrain: epoch  5, batch     1 | loss: 6.4457502CurrentTrain: epoch  5, batch     2 | loss: 6.2891994CurrentTrain: epoch  5, batch     3 | loss: 6.0270162CurrentTrain: epoch  5, batch     4 | loss: 6.8620248CurrentTrain: epoch  5, batch     5 | loss: 6.5494318CurrentTrain: epoch  5, batch     6 | loss: 6.8687253CurrentTrain: epoch  5, batch     7 | loss: 6.8705826CurrentTrain: epoch  5, batch     8 | loss: 6.7556534CurrentTrain: epoch  5, batch     9 | loss: 6.2647810CurrentTrain: epoch  5, batch    10 | loss: 6.0804253CurrentTrain: epoch  5, batch    11 | loss: 6.0330095CurrentTrain: epoch  5, batch    12 | loss: 6.2159753CurrentTrain: epoch  5, batch    13 | loss: 6.2016640CurrentTrain: epoch  5, batch    14 | loss: 6.6694832CurrentTrain: epoch  5, batch    15 | loss: 6.8144331CurrentTrain: epoch  5, batch    16 | loss: 6.2117062CurrentTrain: epoch  5, batch    17 | loss: 5.8549390CurrentTrain: epoch  5, batch    18 | loss: 6.4895077CurrentTrain: epoch  5, batch    19 | loss: 6.3163958CurrentTrain: epoch  5, batch    20 | loss: 6.9727755CurrentTrain: epoch  5, batch    21 | loss: 6.2495742CurrentTrain: epoch  5, batch    22 | loss: 5.2159858CurrentTrain: epoch  5, batch    23 | loss: 6.8142138CurrentTrain: epoch  5, batch    24 | loss: 5.9922290CurrentTrain: epoch  5, batch    25 | loss: 5.9670534CurrentTrain: epoch  5, batch    26 | loss: 6.3503780CurrentTrain: epoch  5, batch    27 | loss: 6.8494716CurrentTrain: epoch  5, batch    28 | loss: 7.3930225CurrentTrain: epoch  5, batch    29 | loss: 6.0218053CurrentTrain: epoch  5, batch    30 | loss: 6.3042097CurrentTrain: epoch  5, batch    31 | loss: 6.3260398CurrentTrain: epoch  5, batch    32 | loss: 5.5472698CurrentTrain: epoch  5, batch    33 | loss: 5.7975426CurrentTrain: epoch  5, batch    34 | loss: 5.8915377CurrentTrain: epoch  5, batch    35 | loss: 6.0879879CurrentTrain: epoch  5, batch    36 | loss: 6.3617220CurrentTrain: epoch  5, batch    37 | loss: 6.5482788CurrentTrain: epoch  6, batch     0 | loss: 5.9139767CurrentTrain: epoch  6, batch     1 | loss: 5.4915895CurrentTrain: epoch  6, batch     2 | loss: 6.0042295CurrentTrain: epoch  6, batch     3 | loss: 6.1626110CurrentTrain: epoch  6, batch     4 | loss: 5.7147608CurrentTrain: epoch  6, batch     5 | loss: 5.5975070CurrentTrain: epoch  6, batch     6 | loss: 5.9434414CurrentTrain: epoch  6, batch     7 | loss: 5.6178265CurrentTrain: epoch  6, batch     8 | loss: 5.9529300CurrentTrain: epoch  6, batch     9 | loss: 6.5790596CurrentTrain: epoch  6, batch    10 | loss: 5.6891775CurrentTrain: epoch  6, batch    11 | loss: 5.8797817CurrentTrain: epoch  6, batch    12 | loss: 5.2089744CurrentTrain: epoch  6, batch    13 | loss: 6.3480415CurrentTrain: epoch  6, batch    14 | loss: 6.0473361CurrentTrain: epoch  6, batch    15 | loss: 6.0363560CurrentTrain: epoch  6, batch    16 | loss: 6.4105220CurrentTrain: epoch  6, batch    17 | loss: 6.1486154CurrentTrain: epoch  6, batch    18 | loss: 5.9411526CurrentTrain: epoch  6, batch    19 | loss: 6.4936566CurrentTrain: epoch  6, batch    20 | loss: 6.0450139CurrentTrain: epoch  6, batch    21 | loss: 5.8020029CurrentTrain: epoch  6, batch    22 | loss: 5.6641555CurrentTrain: epoch  6, batch    23 | loss: 5.8220510CurrentTrain: epoch  6, batch    24 | loss: 5.5880489CurrentTrain: epoch  6, batch    25 | loss: 6.1920052CurrentTrain: epoch  6, batch    26 | loss: 5.6021161CurrentTrain: epoch  6, batch    27 | loss: 6.7159443CurrentTrain: epoch  6, batch    28 | loss: 5.9406338CurrentTrain: epoch  6, batch    29 | loss: 6.4024544CurrentTrain: epoch  6, batch    30 | loss: 6.1704078CurrentTrain: epoch  6, batch    31 | loss: 5.8065720CurrentTrain: epoch  6, batch    32 | loss: 6.5203233CurrentTrain: epoch  6, batch    33 | loss: 6.0068903CurrentTrain: epoch  6, batch    34 | loss: 6.4297843CurrentTrain: epoch  6, batch    35 | loss: 6.1463857CurrentTrain: epoch  6, batch    36 | loss: 6.5931745CurrentTrain: epoch  6, batch    37 | loss: 5.8510408CurrentTrain: epoch  7, batch     0 | loss: 5.7367506CurrentTrain: epoch  7, batch     1 | loss: 6.3527746CurrentTrain: epoch  7, batch     2 | loss: 5.6895895CurrentTrain: epoch  7, batch     3 | loss: 5.9080682CurrentTrain: epoch  7, batch     4 | loss: 6.0066953CurrentTrain: epoch  7, batch     5 | loss: 6.2358704CurrentTrain: epoch  7, batch     6 | loss: 5.4476681CurrentTrain: epoch  7, batch     7 | loss: 5.5075617CurrentTrain: epoch  7, batch     8 | loss: 5.4667768CurrentTrain: epoch  7, batch     9 | loss: 5.7519579CurrentTrain: epoch  7, batch    10 | loss: 5.8416047CurrentTrain: epoch  7, batch    11 | loss: 5.1783915CurrentTrain: epoch  7, batch    12 | loss: 6.0507231CurrentTrain: epoch  7, batch    13 | loss: 5.2427573CurrentTrain: epoch  7, batch    14 | loss: 5.4301558CurrentTrain: epoch  7, batch    15 | loss: 5.5560908CurrentTrain: epoch  7, batch    16 | loss: 5.8446045CurrentTrain: epoch  7, batch    17 | loss: 5.8254638CurrentTrain: epoch  7, batch    18 | loss: 5.4141712CurrentTrain: epoch  7, batch    19 | loss: 5.1675787CurrentTrain: epoch  7, batch    20 | loss: 5.3708105CurrentTrain: epoch  7, batch    21 | loss: 5.8743701CurrentTrain: epoch  7, batch    22 | loss: 5.2774544CurrentTrain: epoch  7, batch    23 | loss: 5.3447647CurrentTrain: epoch  7, batch    24 | loss: 5.7491159CurrentTrain: epoch  7, batch    25 | loss: 5.6517639CurrentTrain: epoch  7, batch    26 | loss: 5.1834621CurrentTrain: epoch  7, batch    27 | loss: 5.4928493CurrentTrain: epoch  7, batch    28 | loss: 5.7343206CurrentTrain: epoch  7, batch    29 | loss: 5.1838431CurrentTrain: epoch  7, batch    30 | loss: 5.1552362CurrentTrain: epoch  7, batch    31 | loss: 5.4420671CurrentTrain: epoch  7, batch    32 | loss: 5.2546358CurrentTrain: epoch  7, batch    33 | loss: 5.5003304CurrentTrain: epoch  7, batch    34 | loss: 5.1023951CurrentTrain: epoch  7, batch    35 | loss: 5.5915408CurrentTrain: epoch  7, batch    36 | loss: 5.5862207CurrentTrain: epoch  7, batch    37 | loss: 5.2610440CurrentTrain: epoch  8, batch     0 | loss: 5.4385433CurrentTrain: epoch  8, batch     1 | loss: 5.2563925CurrentTrain: epoch  8, batch     2 | loss: 5.3903995CurrentTrain: epoch  8, batch     3 | loss: 5.1443892CurrentTrain: epoch  8, batch     4 | loss: 5.1960144CurrentTrain: epoch  8, batch     5 | loss: 5.1159878CurrentTrain: epoch  8, batch     6 | loss: 5.4625015CurrentTrain: epoch  8, batch     7 | loss: 5.3606496CurrentTrain: epoch  8, batch     8 | loss: 5.2895589CurrentTrain: epoch  8, batch     9 | loss: 5.2073255CurrentTrain: epoch  8, batch    10 | loss: 5.2778339CurrentTrain: epoch  8, batch    11 | loss: 5.4220896CurrentTrain: epoch  8, batch    12 | loss: 4.9870443CurrentTrain: epoch  8, batch    13 | loss: 5.0369043CurrentTrain: epoch  8, batch    14 | loss: 5.0652695CurrentTrain: epoch  8, batch    15 | loss: 5.2644563CurrentTrain: epoch  8, batch    16 | loss: 5.2153111CurrentTrain: epoch  8, batch    17 | loss: 5.1166716CurrentTrain: epoch  8, batch    18 | loss: 5.2447109CurrentTrain: epoch  8, batch    19 | loss: 5.2669115CurrentTrain: epoch  8, batch    20 | loss: 5.1430035CurrentTrain: epoch  8, batch    21 | loss: 5.0707955CurrentTrain: epoch  8, batch    22 | loss: 5.6262007CurrentTrain: epoch  8, batch    23 | loss: 5.7867961CurrentTrain: epoch  8, batch    24 | loss: 5.5788946CurrentTrain: epoch  8, batch    25 | loss: 5.0709834CurrentTrain: epoch  8, batch    26 | loss: 5.5754180CurrentTrain: epoch  8, batch    27 | loss: 5.3456440CurrentTrain: epoch  8, batch    28 | loss: 5.1143246CurrentTrain: epoch  8, batch    29 | loss: 5.0613275CurrentTrain: epoch  8, batch    30 | loss: 5.4165926CurrentTrain: epoch  8, batch    31 | loss: 5.1567516CurrentTrain: epoch  8, batch    32 | loss: 5.3541384CurrentTrain: epoch  8, batch    33 | loss: 5.0738463CurrentTrain: epoch  8, batch    34 | loss: 5.2763891CurrentTrain: epoch  8, batch    35 | loss: 5.0948029CurrentTrain: epoch  8, batch    36 | loss: 5.7343998CurrentTrain: epoch  8, batch    37 | loss: 4.9640889CurrentTrain: epoch  9, batch     0 | loss: 5.1656189CurrentTrain: epoch  9, batch     1 | loss: 5.8680182CurrentTrain: epoch  9, batch     2 | loss: 5.2100801CurrentTrain: epoch  9, batch     3 | loss: 4.8876858CurrentTrain: epoch  9, batch     4 | loss: 5.7458806CurrentTrain: epoch  9, batch     5 | loss: 5.0626144CurrentTrain: epoch  9, batch     6 | loss: 5.3188996CurrentTrain: epoch  9, batch     7 | loss: 5.1424375CurrentTrain: epoch  9, batch     8 | loss: 5.7007513CurrentTrain: epoch  9, batch     9 | loss: 5.2215962CurrentTrain: epoch  9, batch    10 | loss: 5.1325350CurrentTrain: epoch  9, batch    11 | loss: 5.4080925CurrentTrain: epoch  9, batch    12 | loss: 5.0087242CurrentTrain: epoch  9, batch    13 | loss: 4.9296398CurrentTrain: epoch  9, batch    14 | loss: 5.0489035CurrentTrain: epoch  9, batch    15 | loss: 5.1601257CurrentTrain: epoch  9, batch    16 | loss: 5.0281715CurrentTrain: epoch  9, batch    17 | loss: 6.1474524CurrentTrain: epoch  9, batch    18 | loss: 5.6562920CurrentTrain: epoch  9, batch    19 | loss: 5.5119495CurrentTrain: epoch  9, batch    20 | loss: 5.2420750CurrentTrain: epoch  9, batch    21 | loss: 6.0553312CurrentTrain: epoch  9, batch    22 | loss: 5.4262657CurrentTrain: epoch  9, batch    23 | loss: 5.2714400CurrentTrain: epoch  9, batch    24 | loss: 5.2764406CurrentTrain: epoch  9, batch    25 | loss: 5.2729292CurrentTrain: epoch  9, batch    26 | loss: 5.1367722CurrentTrain: epoch  9, batch    27 | loss: 4.7986202CurrentTrain: epoch  9, batch    28 | loss: 5.2925897CurrentTrain: epoch  9, batch    29 | loss: 4.8639250CurrentTrain: epoch  9, batch    30 | loss: 4.9738092CurrentTrain: epoch  9, batch    31 | loss: 4.9068866CurrentTrain: epoch  9, batch    32 | loss: 5.1352410CurrentTrain: epoch  9, batch    33 | loss: 5.0831947CurrentTrain: epoch  9, batch    34 | loss: 5.0568862CurrentTrain: epoch  9, batch    35 | loss: 5.3126464CurrentTrain: epoch  9, batch    36 | loss: 5.0838861CurrentTrain: epoch  9, batch    37 | loss: 4.9114761
#### person countries of residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person countries of residence" means a person is living in a country.
Here is an example:
Relation: person countries of residence
Context: iran 's top nuclear negotiator , ali larijani , was asked by state tv whether the country had started converting into gas a second batch of uranium , as it had planned to do at its isfahan nuclear facility .
Head Entity: ali larijani
Tail Entity: iran
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person countries of residence":
Relation: person countries of residence  
Context: After years of living in the United States, the renowned artist decided to return to her homeland, where she felt a deep connection to her roots.  
Head Entity: the renowned artist  
Tail Entity: her homeland  

Relation: person countries of residence  
Context: Following his successful career in Europe, the famous chef opened a new restaurant in his native country, showcasing traditional dishes to a modern audience.  
Head Entity: the famous chef  
Tail Entity: his native country  
#### organization top members employees ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization top members employees" means an organization 's employer is a person.
Here is an example:
Relation: organization top members employees
Context: john kilduff of mf global said the market had to cope with `` a very strong rally of the dollar which should have engendered losses for crude oil , '' but that reaction was muddled by a mixed report on us employment .
Head Entity: mf global
Tail Entity: john kilduff
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization top members employees":
Relation: organization top members employees  
Context: During the annual meeting, Sarah Thompson, the CEO of Tech Innovations, highlighted the company's growth and future plans, emphasizing the importance of teamwork and innovation.  
Head Entity: Tech Innovations  
Tail Entity: Sarah Thompson  

Relation: organization top members employees  
Context: At the charity gala, Michael Chen, the founder of Green Earth Initiative, spoke passionately about environmental conservation and the role of community engagement in their projects.  
Head Entity: Green Earth Initiative  
Tail Entity: Michael Chen  
#### organization member of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization member of" means an organization is the member of an organization.
Here is an example:
Relation: organization member of
Context: south africa 1-5 leyds cnr biccard streets cosatu 's concern has been that the scorpions ' existence as a branch of the national directorate of public prosecutions blurred the necessary separation of functions between those investigating crime and those prosecuting the criminals .
Head Entity: scorpions
Tail Entity: national directorate of public prosecutions
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization member of":
Relation: organization member of  
Context: The International Federation of Red Cross and Red Crescent Societies (IFRC) is a global humanitarian organization that coordinates the activities of national Red Cross and Red Crescent societies around the world.  
Head Entity: International Federation of Red Cross and Red Crescent Societies  
Tail Entity: national Red Cross and Red Crescent societies  

Relation: organization member of  
Context: The European Union (EU) is a political and economic union of member states that are located primarily in Europe, working together to promote peace and stability in the region.  
Head Entity: European Union  
Tail Entity: member states  
#### person origin ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person origin" means a person 's nationality is a country.
Here is an example:
Relation: person origin
Context: paris , feb 26 -lrb- xinhua -rrb- c french president jacques chirac on monday sent a letter of condolences to his chadian counterpart idriss deby itno over the death of chadian prime minister pascal yoadimnadji last friday in paris .
Head Entity: pascal yoadimnadji
Tail Entity: chadian
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person origin":
Relation: person origin  
Context: during the international conference held in berlin, the renowned scientist dr. emma thompson presented her groundbreaking research on climate change, highlighting the contributions of her fellow countrymen from the united kingdom.  
Head Entity: emma thompson  
Tail Entity: united kingdom  

Relation: person origin  
Context: at the award ceremony in los angeles, actor and filmmaker javier bardem expressed his gratitude to his family and friends in spain for their unwavering support throughout his career.  
Head Entity: javier bardem  
Tail Entity: spain  
#### person title ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person title" means a person 's title is a title.
Here is an example:
Relation: person title
Context: `` it has to have some tempo , it has to have some energy , '' said sean ross , vice president of music and programming at edison media research , which surveys the radio industry .
Head Entity: sean ross
Tail Entity: vice president
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person title":
Relation: person title  
Context: "After years of hard work and dedication, Maria Lopez was finally recognized as the chief executive officer of the company, leading it to new heights."  
Head Entity: Maria Lopez  
Tail Entity: chief executive officer  

Relation: person title  
Context: "During the conference, Dr. James Smith, the lead researcher at the institute, presented groundbreaking findings in the field of renewable energy."  
Head Entity: Dr. James Smith  
Tail Entity: lead researcher  
#### organization country of headquarters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization country of headquarters" means an organization is located in a country.
Here is an example:
Relation: organization country of headquarters
Context: japan 's denso slashes profit forecast by 90 percent japan 's leading car parts maker denso corp said wednesday its full-year net profit would be one 10th of an earlier forecast due to the poor performance of the auto industry and a strong yen .
Head Entity: denso
Tail Entity: japan
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization country of headquarters":
Relation: organization country of headquarters  
Context: samsung electronics, based in south korea, has announced a significant increase in its investment in artificial intelligence research to enhance its product offerings and maintain its competitive edge in the global market.  
Head Entity: samsung electronics  
Tail Entity: south korea  

Relation: organization country of headquarters  
Context: the headquarters of nestlé, the world's largest food and beverage company, is located in switzerland, where it was founded over 150 years ago.  
Head Entity: nestlé  
Tail Entity: switzerland  
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 62.50%,  total acc: 68.75%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 72.92%   [EVAL] batch:    3 | acc: 62.50%,  total acc: 70.31%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 71.25%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 71.88%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 75.00%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 78.12%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 80.56%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 81.88%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 83.52%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 84.62%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 83.48%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 82.92%   [EVAL] batch:   15 | acc: 62.50%,  total acc: 81.64%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:   17 | acc: 75.00%,  total acc: 80.90%   [EVAL] batch:   18 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 81.56%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 82.44%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 83.24%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 83.97%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 85.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 85.58%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 85.88%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 86.38%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 86.85%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 86.67%   [EVAL] batch:   30 | acc: 81.25%,  total acc: 86.49%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 86.72%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 85.23%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 62.50%,  total acc: 68.75%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 72.92%   [EVAL] batch:    3 | acc: 62.50%,  total acc: 70.31%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 71.25%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 71.88%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 75.00%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 78.12%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 80.56%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 81.88%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 83.52%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 84.62%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 83.48%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 82.92%   [EVAL] batch:   15 | acc: 62.50%,  total acc: 81.64%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:   17 | acc: 75.00%,  total acc: 80.90%   [EVAL] batch:   18 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 81.56%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 82.44%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 83.24%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 83.97%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 85.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 85.58%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 85.88%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 86.38%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 86.85%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 86.67%   [EVAL] batch:   30 | acc: 81.25%,  total acc: 86.49%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 86.72%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 85.23%   
cur_acc:  ['0.8523']
his_acc:  ['0.8523']
CurrentTrain: epoch  0, batch     0 | loss: 4.4439540CurrentTrain: epoch  0, batch     1 | loss: 4.7894168CurrentTrain: epoch  1, batch     0 | loss: 4.1904936CurrentTrain: epoch  1, batch     1 | loss: 3.9903221CurrentTrain: epoch  2, batch     0 | loss: 3.8067079CurrentTrain: epoch  2, batch     1 | loss: 3.4419689CurrentTrain: epoch  3, batch     0 | loss: 3.0784965CurrentTrain: epoch  3, batch     1 | loss: 3.2059386CurrentTrain: epoch  4, batch     0 | loss: 3.0005038CurrentTrain: epoch  4, batch     1 | loss: 2.7250514CurrentTrain: epoch  5, batch     0 | loss: 2.9244626CurrentTrain: epoch  5, batch     1 | loss: 2.6757774CurrentTrain: epoch  6, batch     0 | loss: 2.6474338CurrentTrain: epoch  6, batch     1 | loss: 2.3984463CurrentTrain: epoch  7, batch     0 | loss: 2.2929149CurrentTrain: epoch  7, batch     1 | loss: 2.5546372CurrentTrain: epoch  8, batch     0 | loss: 2.2460189CurrentTrain: epoch  8, batch     1 | loss: 2.4657733CurrentTrain: epoch  9, batch     0 | loss: 2.2702525CurrentTrain: epoch  9, batch     1 | loss: 2.1678426
#### person cause of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person cause of death" means a person was died of an event.
Here is an example:
Relation: person cause of death
Context: mr scheider had suffered from multiple myeloma for several years , and died of complications from a staph infection , his wife , brenda siemer , said .
Head Entity: mr scheider
Tail Entity: complications from a staph infection
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person cause of death":
Relation: person cause of death  
Context: after battling with lung cancer for over a year, john doe passed away peacefully in his sleep, surrounded by family.  
Head Entity: john doe  
Tail Entity: lung cancer  

Relation: person cause of death  
Context: the renowned scientist, dr. emily white, tragically lost her life due to a car accident while returning from a conference.  
Head Entity: dr. emily white  
Tail Entity: car accident  
#### organization political religious affiliation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization political religious affiliation" means an organization is the member of a religion.
Here is an example:
Relation: organization political religious affiliation
Context: the militia has also suffered from allegations of being involved in clashes with rival shiite militias , particularly the badr corp , the military wing linked to supreme iraqi islamic council , the party led by powerful politician abdel aziz al-hakim .
Head Entity: badr corp
Tail Entity: shiite
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization political religious affiliation":
Relation: organization political religious affiliation  
Context: The Christian Democratic Union has been a significant player in German politics, often aligning itself with various religious groups to promote policies that reflect its Christian values.  
Head Entity: Christian Democratic Union  
Tail Entity: Christian  

Relation: organization political religious affiliation  
Context: The Islamic Society of North America has been actively involved in advocating for Muslim rights and representation in the political landscape of the United States.  
Head Entity: Islamic Society of North America  
Tail Entity: Muslim  
#### organization stateorprovince of headquarters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization stateorprovince of headquarters" means an organization is located in a state or province.
Here is an example:
Relation: organization stateorprovince of headquarters
Context: based in armonk , new york , mbia insures $ 670 billion -lrb- euro452 .18 billion -rrb- in debt .
Head Entity: mbia
Tail Entity: new york
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization stateorprovince of headquarters":
Relation: organization stateorprovince of headquarters  
Context: the tech giant apple inc. has its headquarters in cupertino, california, where it develops innovative products.  
Head Entity: apple inc.  
Tail Entity: california  

Relation: organization stateorprovince of headquarters  
Context: the multinational corporation samsung electronics is headquartered in suwon, south korea, and is a leader in consumer electronics.  
Head Entity: samsung electronics  
Tail Entity: south korea  
#### person other family ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person other family" means a person 's relative is a person.
Here is an example:
Relation: person other family
Context: former american idol contestant , sanjaya malakar and hills starlet , holly montag , have been getting close since she replaced her sister and brother-in-law , heidi montag and spencer pratt , on i 'm a celebrity .
Head Entity: spencer pratt ,
Tail Entity: holly montag
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person other family":
Relation: person other family  
Context: in a recent interview, actress jennifer aniston spoke fondly of her brother, alex aniston, and their close bond, highlighting how family ties remain strong despite their busy careers.  
Head Entity: alex aniston  
Tail Entity: jennifer aniston  

Relation: person other family  
Context: during the family reunion, uncle bob shared stories about his niece, emily, and how she has grown into a remarkable young woman, making the entire family proud.  
Head Entity: emily  
Tail Entity: uncle bob  
#### person city of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person city of death" means a person was died in a city.
Here is an example:
Relation: person city of death
Context: irene morgan kirkaldy , 90 , who died of alzheimer 's disease aug. 10 at her home in gloucester , va. , quietly changed history in 1944 when she refused to give up her seat on a crowded greyhound bus to a white couple .
Head Entity: her
Tail Entity: gloucester
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person city of death":
Relation: person city of death  
Context: john smith, a renowned author, passed away on march 5 in his apartment located in new york city, leaving behind a legacy of literary works that inspired many.  
Head Entity: john smith  
Tail Entity: new york city  

Relation: person city of death  
Context: after a long battle with cancer, elizabeth taylor died on march 23 at a hospital in los angeles, where she had spent her final days surrounded by family and friends.  
Head Entity: elizabeth taylor  
Tail Entity: los angeles  
Mixup data size:  105
MixupTrain:  epoch  0, batch     0 | loss: 10.1152897MixupTrain:  epoch  0, batch     1 | loss: 7.9890321MixupTrain:  epoch  0, batch     2 | loss: 7.2499905MixupTrain:  epoch  0, batch     3 | loss: 6.1292788MixupTrain:  epoch  0, batch     4 | loss: 8.2240948MixupTrain:  epoch  0, batch     5 | loss: 6.9605119MixupTrain:  epoch  0, batch     6 | loss: 6.6153736
MemoryTrain:  epoch  0, batch     0 | loss: 6.5197992MemoryTrain:  epoch  0, batch     1 | loss: 4.7568531MemoryTrain:  epoch  0, batch     2 | loss: 4.2132406MemoryTrain:  epoch  1, batch     0 | loss: 4.9258685MemoryTrain:  epoch  1, batch     1 | loss: 5.0576797MemoryTrain:  epoch  1, batch     2 | loss: 6.3282170MemoryTrain:  epoch  2, batch     0 | loss: 4.3746524MemoryTrain:  epoch  2, batch     1 | loss: 4.1968069MemoryTrain:  epoch  2, batch     2 | loss: 3.6570528MemoryTrain:  epoch  3, batch     0 | loss: 3.8604665MemoryTrain:  epoch  3, batch     1 | loss: 3.8633838MemoryTrain:  epoch  3, batch     2 | loss: 3.3274601MemoryTrain:  epoch  4, batch     0 | loss: 3.2244911MemoryTrain:  epoch  4, batch     1 | loss: 3.5410123MemoryTrain:  epoch  4, batch     2 | loss: 2.0854714MemoryTrain:  epoch  5, batch     0 | loss: 3.9547164MemoryTrain:  epoch  5, batch     1 | loss: 3.1217322MemoryTrain:  epoch  5, batch     2 | loss: 1.2752575MemoryTrain:  epoch  6, batch     0 | loss: 4.2150555MemoryTrain:  epoch  6, batch     1 | loss: 3.0071077MemoryTrain:  epoch  6, batch     2 | loss: 1.1937261MemoryTrain:  epoch  7, batch     0 | loss: 3.6515605MemoryTrain:  epoch  7, batch     1 | loss: 2.8043642MemoryTrain:  epoch  7, batch     2 | loss: 1.2496576MemoryTrain:  epoch  8, batch     0 | loss: 3.0952840MemoryTrain:  epoch  8, batch     1 | loss: 2.8720169MemoryTrain:  epoch  8, batch     2 | loss: 2.9124262MemoryTrain:  epoch  9, batch     0 | loss: 2.7966466MemoryTrain:  epoch  9, batch     1 | loss: 2.8493633MemoryTrain:  epoch  9, batch     2 | loss: 3.1064456
[EVAL] batch:    0 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    1 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 97.92%   [EVAL] batch:    3 | acc: 62.50%,  total acc: 89.06%   [EVAL] batch:    4 | acc: 50.00%,  total acc: 81.25%   [EVAL] batch:    5 | acc: 50.00%,  total acc: 76.04%   [EVAL] batch:    6 | acc: 56.25%,  total acc: 73.21%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 76.56%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 77.78%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 78.12%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 78.98%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 79.69%   [EVAL] batch:   12 | acc: 43.75%,  total acc: 76.92%   
[EVAL] batch:    0 | acc: 37.50%,  total acc: 37.50%   [EVAL] batch:    1 | acc: 31.25%,  total acc: 34.38%   [EVAL] batch:    2 | acc: 50.00%,  total acc: 39.58%   [EVAL] batch:    3 | acc: 37.50%,  total acc: 39.06%   [EVAL] batch:    4 | acc: 37.50%,  total acc: 38.75%   [EVAL] batch:    5 | acc: 43.75%,  total acc: 39.58%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 46.43%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 53.12%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 58.33%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 61.88%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 65.34%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 67.19%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 68.75%   [EVAL] batch:   13 | acc: 75.00%,  total acc: 69.20%   [EVAL] batch:   14 | acc: 81.25%,  total acc: 70.00%   [EVAL] batch:   15 | acc: 75.00%,  total acc: 70.31%   [EVAL] batch:   16 | acc: 81.25%,  total acc: 70.96%   [EVAL] batch:   17 | acc: 81.25%,  total acc: 71.53%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 73.03%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 74.06%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 75.30%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 76.42%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 77.17%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 77.86%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 78.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 79.57%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 80.09%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 80.80%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 81.47%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 81.46%   [EVAL] batch:   30 | acc: 81.25%,  total acc: 81.45%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 81.84%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 82.39%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 82.90%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 83.21%   [EVAL] batch:   35 | acc: 81.25%,  total acc: 83.16%   [EVAL] batch:   36 | acc: 50.00%,  total acc: 82.26%   [EVAL] batch:   37 | acc: 56.25%,  total acc: 81.58%   [EVAL] batch:   38 | acc: 50.00%,  total acc: 80.77%   [EVAL] batch:   39 | acc: 81.25%,  total acc: 80.78%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 81.10%   [EVAL] batch:   41 | acc: 75.00%,  total acc: 80.95%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 81.39%   [EVAL] batch:   44 | acc: 75.00%,  total acc: 81.25%   
cur_acc:  ['0.8523', '0.7692']
his_acc:  ['0.8523', '0.8125']
CurrentTrain: epoch  0, batch     0 | loss: 5.5074005CurrentTrain: epoch  0, batch     1 | loss: 7.6306863CurrentTrain: epoch  1, batch     0 | loss: 6.0648127CurrentTrain: epoch  1, batch     1 | loss: 4.9160204CurrentTrain: epoch  2, batch     0 | loss: 5.0520325CurrentTrain: epoch  2, batch     1 | loss: 5.4956226CurrentTrain: epoch  3, batch     0 | loss: 5.1095181CurrentTrain: epoch  3, batch     1 | loss: 4.3878536CurrentTrain: epoch  4, batch     0 | loss: 4.5697889CurrentTrain: epoch  4, batch     1 | loss: 4.1418934CurrentTrain: epoch  5, batch     0 | loss: 4.1430058CurrentTrain: epoch  5, batch     1 | loss: 4.1804905CurrentTrain: epoch  6, batch     0 | loss: 3.8673379CurrentTrain: epoch  6, batch     1 | loss: 4.3336930CurrentTrain: epoch  7, batch     0 | loss: 4.0495691CurrentTrain: epoch  7, batch     1 | loss: 3.6295946CurrentTrain: epoch  8, batch     0 | loss: 3.7126737CurrentTrain: epoch  8, batch     1 | loss: 3.8121538CurrentTrain: epoch  9, batch     0 | loss: 3.4043341CurrentTrain: epoch  9, batch     1 | loss: 4.3907499
#### person stateorprovinces of residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person stateorprovinces of residence" means a person is living in a state or province.
Here is an example:
Relation: person stateorprovinces of residence
Context: his early school years were spent in new jersey 's integrated schools .
Head Entity: his
Tail Entity: new jersey
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person stateorprovinces of residence":
Relation: person stateorprovinces of residence  
Context: After moving to California, she quickly adapted to the vibrant culture of the area.  
Head Entity: she  
Tail Entity: California  

Relation: person stateorprovinces of residence  
Context: He was born and raised in Texas, where he developed a love for country music.  
Head Entity: He  
Tail Entity: Texas  
#### person date of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person date of death" means a person was died on a date.
Here is an example:
Relation: person date of death
Context: today the nypd upgraded the charges to include murder , in the case of brooklyn gay-bashing/robbery victim michael sandy , who died on friday after being taken off life-support .
Head Entity: michael sandy
Tail Entity: friday
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person date of death":
Relation: person date of death  
Context: renowned physicist stephen hawking passed away peacefully at his home in cambridge on march 14, 2018, leaving behind a legacy of groundbreaking work in cosmology.  
Head Entity: stephen hawking  
Tail Entity: march 14, 2018  

Relation: person date of death  
Context: the beloved author of the harry potter series, j.k. rowling, mourned the loss of her friend and mentor, who died on november 30, 2016, after a long battle with illness.  
Head Entity: j.k. rowling  
Tail Entity: november 30, 2016  
#### organization number of employees members ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization number of employees members" means the number of an organization 's employees is a number.
Here is an example:
Relation: organization number of employees members
Context: with the sweep of a federal regulator 's pen , massachusetts stands to gain a new life-science giant in april : covidien , a medical - supplies maker with thousands of products and more than 43,000 employees worldwide .
Head Entity: covidien
Tail Entity: 43,000
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization number of employees members":
Relation: organization number of employees members  
Context: The tech company, Innovatech, has rapidly expanded its workforce over the past year, now boasting a total of 25,000 employees across its global offices.  
Head Entity: Innovatech  
Tail Entity: 25,000  

Relation: organization number of employees members  
Context: After the merger, the newly formed entity, Global Solutions Inc., reported an impressive count of 50,000 employees, making it one of the largest firms in the industry.  
Head Entity: Global Solutions Inc.  
Tail Entity: 50,000  
#### person alternate names ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person alternate names" means a person 's alias is a person.
Here is an example:
Relation: person alternate names
Context: `` i am known in the hospice as the man who would n't die , '' buchwald wrote in march .
Head Entity: buchwald
Tail Entity: man who would n't die
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person alternate names":
Relation: person alternate names  
Context: `` the famous author often referred to himself as the bard of Avon, '' said the literary critic during the lecture.  
Head Entity: author  
Tail Entity: bard of Avon  

Relation: person alternate names  
Context: `` in the world of music, she is often called the queen of pop, '' remarked the journalist in her article.  
Head Entity: she  
Tail Entity: queen of pop  
#### person spouse ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person spouse" means a person 's spouse is a person.
Here is an example:
Relation: person spouse
Context: beverly hills , california 2008-08-17 21:15:39 utc ------ there was much dancing : ellen degeneres and portia de rossi are married , according to reports .
Head Entity: ellen degeneres
Tail Entity: portia de rossi
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person spouse":
Relation: person spouse  
Context: in a lavish ceremony held in new york city, the couple exchanged vows in front of family and friends. the bride, jennifer aniston, looked stunning in her gown, while her partner, justin theroux, was equally dapper.  
Head Entity: jennifer aniston  
Tail Entity: justin theroux  

Relation: person spouse  
Context: during a recent interview, the famous actor revealed that he and his wife, emma stone, have been enjoying their time together and are planning a family.  
Head Entity: emma stone  
Tail Entity: andrew garfield  
Mixup data size:  135
MixupTrain:  epoch  0, batch     0 | loss: 7.4824696MixupTrain:  epoch  0, batch     1 | loss: 5.9735285MixupTrain:  epoch  0, batch     2 | loss: 7.5871364MixupTrain:  epoch  0, batch     3 | loss: 5.8290343MixupTrain:  epoch  0, batch     4 | loss: 4.4983583MixupTrain:  epoch  0, batch     5 | loss: 4.6626372MixupTrain:  epoch  0, batch     6 | loss: 4.9668454MixupTrain:  epoch  0, batch     7 | loss: 5.2365251MixupTrain:  epoch  0, batch     8 | loss: 3.8150698
MemoryTrain:  epoch  0, batch     0 | loss: 4.4416709MemoryTrain:  epoch  0, batch     1 | loss: 3.4393430MemoryTrain:  epoch  0, batch     2 | loss: 3.7893202MemoryTrain:  epoch  1, batch     0 | loss: 3.0042660MemoryTrain:  epoch  1, batch     1 | loss: 4.5900722MemoryTrain:  epoch  1, batch     2 | loss: 3.1348643MemoryTrain:  epoch  2, batch     0 | loss: 2.7297769MemoryTrain:  epoch  2, batch     1 | loss: 3.4297676MemoryTrain:  epoch  2, batch     2 | loss: 3.6485665MemoryTrain:  epoch  3, batch     0 | loss: 2.7204652MemoryTrain:  epoch  3, batch     1 | loss: 2.7128797MemoryTrain:  epoch  3, batch     2 | loss: 3.5943661MemoryTrain:  epoch  4, batch     0 | loss: 2.8784142MemoryTrain:  epoch  4, batch     1 | loss: 2.6458561MemoryTrain:  epoch  4, batch     2 | loss: 2.4912002MemoryTrain:  epoch  5, batch     0 | loss: 2.5535500MemoryTrain:  epoch  5, batch     1 | loss: 3.0454319MemoryTrain:  epoch  5, batch     2 | loss: 2.1924882MemoryTrain:  epoch  6, batch     0 | loss: 2.8019621MemoryTrain:  epoch  6, batch     1 | loss: 2.1988902MemoryTrain:  epoch  6, batch     2 | loss: 2.5744855MemoryTrain:  epoch  7, batch     0 | loss: 2.3057768MemoryTrain:  epoch  7, batch     1 | loss: 1.9547443MemoryTrain:  epoch  7, batch     2 | loss: 2.2772627MemoryTrain:  epoch  8, batch     0 | loss: 2.3353522MemoryTrain:  epoch  8, batch     1 | loss: 1.9450705MemoryTrain:  epoch  8, batch     2 | loss: 2.3492661MemoryTrain:  epoch  9, batch     0 | loss: 2.3400111MemoryTrain:  epoch  9, batch     1 | loss: 2.0015631MemoryTrain:  epoch  9, batch     2 | loss: 2.3469911
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    1 | acc: 75.00%,  total acc: 68.75%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 72.92%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 76.56%   [EVAL] batch:    4 | acc: 56.25%,  total acc: 72.50%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 76.04%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 79.46%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 82.03%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 82.64%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 81.25%   [EVAL] batch:   10 | acc: 50.00%,  total acc: 78.41%   [EVAL] batch:   11 | acc: 68.75%,  total acc: 77.60%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 78.37%   [EVAL] batch:   13 | acc: 75.00%,  total acc: 78.12%   [EVAL] batch:   14 | acc: 37.50%,  total acc: 75.42%   
[EVAL] batch:    0 | acc: 31.25%,  total acc: 31.25%   [EVAL] batch:    1 | acc: 31.25%,  total acc: 31.25%   [EVAL] batch:    2 | acc: 43.75%,  total acc: 35.42%   [EVAL] batch:    3 | acc: 31.25%,  total acc: 34.38%   [EVAL] batch:    4 | acc: 43.75%,  total acc: 36.25%   [EVAL] batch:    5 | acc: 31.25%,  total acc: 35.42%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 41.96%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 49.22%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 54.86%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 58.75%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 62.50%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 64.58%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 66.35%   [EVAL] batch:   13 | acc: 75.00%,  total acc: 66.96%   [EVAL] batch:   14 | acc: 81.25%,  total acc: 67.92%   [EVAL] batch:   15 | acc: 75.00%,  total acc: 68.36%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 68.75%   [EVAL] batch:   17 | acc: 87.50%,  total acc: 69.79%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 71.05%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 72.19%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 73.51%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 74.72%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 75.82%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 76.56%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 77.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 78.37%   [EVAL] batch:   26 | acc: 12.50%,  total acc: 75.93%   [EVAL] batch:   27 | acc: 6.25%,  total acc: 73.44%   [EVAL] batch:   28 | acc: 12.50%,  total acc: 71.34%   [EVAL] batch:   29 | acc: 12.50%,  total acc: 69.38%   [EVAL] batch:   30 | acc: 12.50%,  total acc: 67.54%   [EVAL] batch:   31 | acc: 12.50%,  total acc: 65.82%   [EVAL] batch:   32 | acc: 68.75%,  total acc: 65.91%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 66.91%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 67.68%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 68.23%   [EVAL] batch:   36 | acc: 93.75%,  total acc: 68.92%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 69.57%   [EVAL] batch:   38 | acc: 81.25%,  total acc: 69.87%   [EVAL] batch:   39 | acc: 75.00%,  total acc: 70.00%   [EVAL] batch:   40 | acc: 18.75%,  total acc: 68.75%   [EVAL] batch:   41 | acc: 43.75%,  total acc: 68.15%   [EVAL] batch:   42 | acc: 56.25%,  total acc: 67.88%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 68.18%   [EVAL] batch:   44 | acc: 75.00%,  total acc: 68.33%   [EVAL] batch:   45 | acc: 62.50%,  total acc: 68.21%   [EVAL] batch:   46 | acc: 75.00%,  total acc: 68.35%   [EVAL] batch:   47 | acc: 81.25%,  total acc: 68.62%   [EVAL] batch:   48 | acc: 81.25%,  total acc: 68.88%   [EVAL] batch:   49 | acc: 62.50%,  total acc: 68.75%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 69.24%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 69.83%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 70.40%   [EVAL] batch:   53 | acc: 87.50%,  total acc: 70.72%   [EVAL] batch:   54 | acc: 62.50%,  total acc: 70.57%   [EVAL] batch:   55 | acc: 56.25%,  total acc: 70.31%   [EVAL] batch:   56 | acc: 62.50%,  total acc: 70.18%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 70.58%   [EVAL] batch:   58 | acc: 75.00%,  total acc: 70.66%   [EVAL] batch:   59 | acc: 31.25%,  total acc: 70.00%   
cur_acc:  ['0.8523', '0.7692', '0.7542']
his_acc:  ['0.8523', '0.8125', '0.7000']
CurrentTrain: epoch  0, batch     0 | loss: 8.1648893CurrentTrain: epoch  0, batch     1 | loss: 7.4134269CurrentTrain: epoch  1, batch     0 | loss: 7.4175477CurrentTrain: epoch  1, batch     1 | loss: 6.4165478CurrentTrain: epoch  2, batch     0 | loss: 6.7963152CurrentTrain: epoch  2, batch     1 | loss: 6.1031156CurrentTrain: epoch  3, batch     0 | loss: 6.2939148CurrentTrain: epoch  3, batch     1 | loss: 5.3178883CurrentTrain: epoch  4, batch     0 | loss: 5.7456460CurrentTrain: epoch  4, batch     1 | loss: 5.3606400CurrentTrain: epoch  5, batch     0 | loss: 5.2887039CurrentTrain: epoch  5, batch     1 | loss: 5.5774088CurrentTrain: epoch  6, batch     0 | loss: 5.3560696CurrentTrain: epoch  6, batch     1 | loss: 5.1233463CurrentTrain: epoch  7, batch     0 | loss: 5.0593419CurrentTrain: epoch  7, batch     1 | loss: 4.5005646CurrentTrain: epoch  8, batch     0 | loss: 4.5833206CurrentTrain: epoch  8, batch     1 | loss: 4.4049211CurrentTrain: epoch  9, batch     0 | loss: 4.5684681CurrentTrain: epoch  9, batch     1 | loss: 4.7134767
#### organization subsidiaries ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization subsidiaries" means an organization 's subsidiary is an organization.
Here is an example:
Relation: organization subsidiaries
Context: venture fund buys sporting chain highland capital 's consumer fund includes lululemon athletica , a yoga retailer , and o beverages , a flavored water company developed by tom first , one of the two `` juice guys '' who cofounded nantucket nectars .
Head Entity: highland capital
Tail Entity: o beverages
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization subsidiaries":
Relation: organization subsidiaries  
Context: The tech giant Alphabet Inc. has several subsidiaries, including YouTube, which has become a leading platform for video content, and Waymo, which focuses on self-driving technology.  
Head Entity: Alphabet Inc.  
Tail Entity: Waymo  

Relation: organization subsidiaries  
Context: The automotive manufacturer Ford Motor Company has expanded its reach by acquiring several companies, including the electric vehicle startup Rivian, which aims to revolutionize the EV market.  
Head Entity: Ford Motor Company  
Tail Entity: Rivian  
#### organization parents ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization parents" means an organization 's parent is an organization.
Here is an example:
Relation: organization parents
Context: those who try to salvage possessions from the debris of their homes can easily turn into victims too , said dr. irwin redlener , director of the national center for disaster preparedness at columbia university mailman school of medicine .
Head Entity: national center for disaster preparedness
Tail Entity: columbia university mailman school of medicine
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization parents":
Relation: organization parents  
Context: The Massachusetts Institute of Technology, known for its cutting-edge research and innovation, is part of a larger network of educational institutions that includes Harvard University.  
Head Entity: Massachusetts Institute of Technology  
Tail Entity: Harvard University  

Relation: organization parents  
Context: The World Wildlife Fund, a leading organization in conservation efforts, operates under the umbrella of the World Wide Fund for Nature, which has a broader international focus.  
Head Entity: World Wildlife Fund  
Tail Entity: World Wide Fund for Nature  
#### organization alternate names ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization alternate names" means a person 's alias is a person.
Here is an example:
Relation: organization alternate names
Context: while section 106 of the hyde act openly bans indian testing , and the agreement upholds reinforces that test ban by upholding the applicability of domestic laws , washington has already recommended that the nuclear suppliers group -lrb- nsg -rrb- link its proposed exemption for india to a similar test ban .
Head Entity: nuclear suppliers group
Tail Entity: nsg
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization alternate names":
Relation: organization alternate names  
Context: The International Monetary Fund, often referred to as the IMF, plays a crucial role in global economic stability by providing financial assistance and advice to member countries.  
Head Entity: International Monetary Fund  
Tail Entity: IMF  

Relation: organization alternate names  
Context: The World Health Organization, commonly known as WHO, has been at the forefront of global health initiatives, especially during the COVID-19 pandemic.  
Head Entity: World Health Organization  
Tail Entity: WHO  
#### organization city of headquarters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization city of headquarters" means an organization is located in a city.
Here is an example:
Relation: organization city of headquarters
Context: guy-sheftall entered spelman college in atlanta at age 16 and later earned a master 's in english with a thesis titled `` faulkner 's treatment of women in his major novels . ''
Head Entity: spelman college
Tail Entity: atlanta
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization city of headquarters":
Relation: organization city of headquarters  
Context: the headquarters of tech giant apple inc. is located in cupertino, california, where it has been since 1993.  
Head Entity: apple inc.  
Tail Entity: cupertino  

Relation: organization city of headquarters  
Context: the united nations has its main office in new york city, serving as a hub for international diplomacy and cooperation.  
Head Entity: united nations  
Tail Entity: new york city  
#### person siblings ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person siblings" means a person 's sibling is a person.
Here is an example:
Relation: person siblings
Context: `` holly and sanjaya are headed to -lsb- the hawaiian island of -rsb- kauai tomorrow morning so she can meet his parents . ''
Head Entity: she
Tail Entity: sanjaya
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person siblings":
Relation: person siblings  
Context: `` during the family reunion, john introduced his sister to everyone, and they all had a great time reminiscing about their childhood. ''  
Head Entity: his sister  
Tail Entity: john  

Relation: person siblings  
Context: `` after the graduation ceremony, emily celebrated with her brother, who had always been her biggest supporter throughout school. ''  
Head Entity: her brother  
Tail Entity: emily  
Mixup data size:  164
MixupTrain:  epoch  0, batch     0 | loss: 5.6579366MixupTrain:  epoch  0, batch     1 | loss: 4.3008770MixupTrain:  epoch  0, batch     2 | loss: 5.0938355MixupTrain:  epoch  0, batch     3 | loss: 5.4613462MixupTrain:  epoch  0, batch     4 | loss: 5.2112250MixupTrain:  epoch  0, batch     5 | loss: 3.9381865MixupTrain:  epoch  0, batch     6 | loss: 4.2841759MixupTrain:  epoch  0, batch     7 | loss: 3.3651140MixupTrain:  epoch  0, batch     8 | loss: 4.0926239MixupTrain:  epoch  0, batch     9 | loss: 4.7068689MixupTrain:  epoch  0, batch    10 | loss: 3.4030442
MemoryTrain:  epoch  0, batch     0 | loss: 3.0385375MemoryTrain:  epoch  0, batch     1 | loss: 4.0692825MemoryTrain:  epoch  0, batch     2 | loss: 3.9025342MemoryTrain:  epoch  0, batch     3 | loss: 3.3169384MemoryTrain:  epoch  1, batch     0 | loss: 3.9460230MemoryTrain:  epoch  1, batch     1 | loss: 2.8684494MemoryTrain:  epoch  1, batch     2 | loss: 2.9503360MemoryTrain:  epoch  1, batch     3 | loss: 3.5975533MemoryTrain:  epoch  2, batch     0 | loss: 3.8301814MemoryTrain:  epoch  2, batch     1 | loss: 2.8133693MemoryTrain:  epoch  2, batch     2 | loss: 2.7405741MemoryTrain:  epoch  2, batch     3 | loss: 2.6449296MemoryTrain:  epoch  3, batch     0 | loss: 2.4245720MemoryTrain:  epoch  3, batch     1 | loss: 3.2494888MemoryTrain:  epoch  3, batch     2 | loss: 2.5717244MemoryTrain:  epoch  3, batch     3 | loss: 2.4769080MemoryTrain:  epoch  4, batch     0 | loss: 2.3976245MemoryTrain:  epoch  4, batch     1 | loss: 2.2021723MemoryTrain:  epoch  4, batch     2 | loss: 2.9740820MemoryTrain:  epoch  4, batch     3 | loss: 2.2619667MemoryTrain:  epoch  5, batch     0 | loss: 2.4040980MemoryTrain:  epoch  5, batch     1 | loss: 2.1356764MemoryTrain:  epoch  5, batch     2 | loss: 2.5929441MemoryTrain:  epoch  5, batch     3 | loss: 1.8567349MemoryTrain:  epoch  6, batch     0 | loss: 2.0252781MemoryTrain:  epoch  6, batch     1 | loss: 2.3924279MemoryTrain:  epoch  6, batch     2 | loss: 2.1001310MemoryTrain:  epoch  6, batch     3 | loss: 2.0002134MemoryTrain:  epoch  7, batch     0 | loss: 2.2098024MemoryTrain:  epoch  7, batch     1 | loss: 2.3054786MemoryTrain:  epoch  7, batch     2 | loss: 1.7796648MemoryTrain:  epoch  7, batch     3 | loss: 1.7625875MemoryTrain:  epoch  8, batch     0 | loss: 1.8142320MemoryTrain:  epoch  8, batch     1 | loss: 2.1820011MemoryTrain:  epoch  8, batch     2 | loss: 1.7485654MemoryTrain:  epoch  8, batch     3 | loss: 1.8140316MemoryTrain:  epoch  9, batch     0 | loss: 1.6465530MemoryTrain:  epoch  9, batch     1 | loss: 1.9878339MemoryTrain:  epoch  9, batch     2 | loss: 1.6730514MemoryTrain:  epoch  9, batch     3 | loss: 1.8740588
[EVAL] batch:    0 | acc: 56.25%,  total acc: 56.25%   [EVAL] batch:    1 | acc: 50.00%,  total acc: 53.12%   [EVAL] batch:    2 | acc: 37.50%,  total acc: 47.92%   [EVAL] batch:    3 | acc: 18.75%,  total acc: 40.62%   [EVAL] batch:    4 | acc: 6.25%,  total acc: 33.75%   [EVAL] batch:    5 | acc: 18.75%,  total acc: 31.25%   [EVAL] batch:    6 | acc: 18.75%,  total acc: 29.46%   [EVAL] batch:    7 | acc: 56.25%,  total acc: 32.81%   [EVAL] batch:    8 | acc: 50.00%,  total acc: 34.72%   [EVAL] batch:    9 | acc: 50.00%,  total acc: 36.25%   [EVAL] batch:   10 | acc: 62.50%,  total acc: 38.64%   [EVAL] batch:   11 | acc: 62.50%,  total acc: 40.62%   [EVAL] batch:   12 | acc: 62.50%,  total acc: 42.31%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 45.98%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 49.17%   [EVAL] batch:   15 | acc: 93.75%,  total acc: 51.95%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 54.41%   [EVAL] batch:   17 | acc: 75.00%,  total acc: 55.56%   [EVAL] batch:   18 | acc: 25.00%,  total acc: 53.95%   [EVAL] batch:   19 | acc: 12.50%,  total acc: 51.88%   [EVAL] batch:   20 | acc: 6.25%,  total acc: 49.70%   [EVAL] batch:   21 | acc: 12.50%,  total acc: 48.01%   
[EVAL] batch:    0 | acc: 31.25%,  total acc: 31.25%   [EVAL] batch:    1 | acc: 37.50%,  total acc: 34.38%   [EVAL] batch:    2 | acc: 37.50%,  total acc: 35.42%   [EVAL] batch:    3 | acc: 31.25%,  total acc: 34.38%   [EVAL] batch:    4 | acc: 25.00%,  total acc: 32.50%   [EVAL] batch:    5 | acc: 25.00%,  total acc: 31.25%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 38.39%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 46.09%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 52.08%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 56.25%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 60.23%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 62.50%   [EVAL] batch:   12 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:   13 | acc: 56.25%,  total acc: 62.05%   [EVAL] batch:   14 | acc: 81.25%,  total acc: 63.33%   [EVAL] batch:   15 | acc: 87.50%,  total acc: 64.84%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 65.44%   [EVAL] batch:   17 | acc: 87.50%,  total acc: 66.67%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 68.42%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 69.69%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 71.13%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 72.44%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 73.64%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 74.48%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 75.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 76.44%   [EVAL] batch:   26 | acc: 56.25%,  total acc: 75.69%   [EVAL] batch:   27 | acc: 62.50%,  total acc: 75.22%   [EVAL] batch:   28 | acc: 56.25%,  total acc: 74.57%   [EVAL] batch:   29 | acc: 50.00%,  total acc: 73.75%   [EVAL] batch:   30 | acc: 62.50%,  total acc: 73.39%   [EVAL] batch:   31 | acc: 56.25%,  total acc: 72.85%   [EVAL] batch:   32 | acc: 87.50%,  total acc: 73.30%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 73.90%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 74.46%   [EVAL] batch:   35 | acc: 81.25%,  total acc: 74.65%   [EVAL] batch:   36 | acc: 81.25%,  total acc: 74.83%   [EVAL] batch:   37 | acc: 75.00%,  total acc: 74.84%   [EVAL] batch:   38 | acc: 68.75%,  total acc: 74.68%   [EVAL] batch:   39 | acc: 62.50%,  total acc: 74.38%   [EVAL] batch:   40 | acc: 56.25%,  total acc: 73.93%   [EVAL] batch:   41 | acc: 62.50%,  total acc: 73.66%   [EVAL] batch:   42 | acc: 62.50%,  total acc: 73.40%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 73.58%   [EVAL] batch:   44 | acc: 75.00%,  total acc: 73.61%   [EVAL] batch:   45 | acc: 62.50%,  total acc: 73.37%   [EVAL] batch:   46 | acc: 75.00%,  total acc: 73.40%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 73.83%   [EVAL] batch:   48 | acc: 75.00%,  total acc: 73.85%   [EVAL] batch:   49 | acc: 68.75%,  total acc: 73.75%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 74.14%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 74.64%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 75.12%   [EVAL] batch:   53 | acc: 93.75%,  total acc: 75.46%   [EVAL] batch:   54 | acc: 62.50%,  total acc: 75.23%   [EVAL] batch:   55 | acc: 43.75%,  total acc: 74.67%   [EVAL] batch:   56 | acc: 43.75%,  total acc: 74.12%   [EVAL] batch:   57 | acc: 68.75%,  total acc: 74.03%   [EVAL] batch:   58 | acc: 68.75%,  total acc: 73.94%   [EVAL] batch:   59 | acc: 43.75%,  total acc: 73.44%   [EVAL] batch:   60 | acc: 62.50%,  total acc: 73.26%   [EVAL] batch:   61 | acc: 43.75%,  total acc: 72.78%   [EVAL] batch:   62 | acc: 25.00%,  total acc: 72.02%   [EVAL] batch:   63 | acc: 12.50%,  total acc: 71.09%   [EVAL] batch:   64 | acc: 18.75%,  total acc: 70.29%   [EVAL] batch:   65 | acc: 6.25%,  total acc: 69.32%   [EVAL] batch:   66 | acc: 50.00%,  total acc: 69.03%   [EVAL] batch:   67 | acc: 56.25%,  total acc: 68.84%   [EVAL] batch:   68 | acc: 43.75%,  total acc: 68.48%   [EVAL] batch:   69 | acc: 62.50%,  total acc: 68.39%   [EVAL] batch:   70 | acc: 68.75%,  total acc: 68.40%   [EVAL] batch:   71 | acc: 56.25%,  total acc: 68.23%   [EVAL] batch:   72 | acc: 81.25%,  total acc: 68.41%   [EVAL] batch:   73 | acc: 87.50%,  total acc: 68.67%   [EVAL] batch:   74 | acc: 93.75%,  total acc: 69.00%   [EVAL] batch:   75 | acc: 100.00%,  total acc: 69.41%   [EVAL] batch:   76 | acc: 75.00%,  total acc: 69.48%   [EVAL] batch:   77 | acc: 50.00%,  total acc: 69.23%   [EVAL] batch:   78 | acc: 6.25%,  total acc: 68.43%   [EVAL] batch:   79 | acc: 18.75%,  total acc: 67.81%   [EVAL] batch:   80 | acc: 12.50%,  total acc: 67.13%   
cur_acc:  ['0.8523', '0.7692', '0.7542', '0.4801']
his_acc:  ['0.8523', '0.8125', '0.7000', '0.6713']
CurrentTrain: epoch  0, batch     0 | loss: 5.3602228CurrentTrain: epoch  0, batch     1 | loss: 5.6719327CurrentTrain: epoch  1, batch     0 | loss: 4.6034007CurrentTrain: epoch  1, batch     1 | loss: 4.6627884CurrentTrain: epoch  2, batch     0 | loss: 3.7402763CurrentTrain: epoch  2, batch     1 | loss: 4.0193028CurrentTrain: epoch  3, batch     0 | loss: 3.2985106CurrentTrain: epoch  3, batch     1 | loss: 3.3256414CurrentTrain: epoch  4, batch     0 | loss: 3.0242643CurrentTrain: epoch  4, batch     1 | loss: 3.0835910CurrentTrain: epoch  5, batch     0 | loss: 2.7700806CurrentTrain: epoch  5, batch     1 | loss: 2.6225369CurrentTrain: epoch  6, batch     0 | loss: 2.4903436CurrentTrain: epoch  6, batch     1 | loss: 2.6931794CurrentTrain: epoch  7, batch     0 | loss: 2.6549332CurrentTrain: epoch  7, batch     1 | loss: 2.2383721CurrentTrain: epoch  8, batch     0 | loss: 2.4673824CurrentTrain: epoch  8, batch     1 | loss: 2.1535170CurrentTrain: epoch  9, batch     0 | loss: 2.3186116CurrentTrain: epoch  9, batch     1 | loss: 2.2554798
#### person cities of residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person cities of residence" means a person is living in a city.
Here is an example:
Relation: person cities of residence
Context: tv-idol-johns -- atlanta -- `` american idol '' finalist michael johns moved to los angeles several years ago , but his heart is still in atlanta .
Head Entity: his
Tail Entity: atlanta
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person cities of residence":
Relation: person cities of residence  
Context: after years of living in new york city, the famous author decided to return to her hometown of boston, where she feels most at home.  
Head Entity: she  
Tail Entity: boston  

Relation: person cities of residence  
Context: despite being a global superstar, the singer often reminisces about her childhood in nashville, where she first discovered her love for music.  
Head Entity: she  
Tail Entity: nashville  
#### person schools attended ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person schools attended" means a person 's school is an organization.
Here is an example:
Relation: person schools attended
Context: after world war ii , he attended the university of southern california , where he became editor of a college magazine .
Head Entity: he
Tail Entity: university of southern california
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person schools attended":
Relation: person schools attended  
Context: She graduated from Harvard University with a degree in psychology before pursuing her career in clinical research.  
Head Entity: She  
Tail Entity: Harvard University  

Relation: person schools attended  
Context: After completing his high school education, he enrolled at Stanford University to study computer science.  
Head Entity: he  
Tail Entity: Stanford University  
#### person country of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person country of death" means a person was died in a country.
Here is an example:
Relation: person country of death
Context: beirut , lebanon -lrb- ap -rrb- sheik abbas musawi , hezbollah 's secretary-general , his wife and son were killed in february 1992 when israeli helicopters fired rockets at his car in southern lebanon .
Head Entity: abbas musawi
Tail Entity: southern lebanon
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person country of death":
Relation: person country of death  
Context: in 1945, the renowned physicist albert einstein passed away in his home in princeton, new jersey, after a long and illustrious career in science.  
Head Entity: albert einstein  
Tail Entity: new jersey  

Relation: person country of death  
Context: the famous author ernest hemingway died in 1961, taking his last breath in his home in ketchum, idaho, where he had spent many peaceful years.  
Head Entity: ernest hemingway  
Tail Entity: idaho  
#### person children ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person children" means a person 's children is a person.
Here is an example:
Relation: person children
Context: he is survived by two stepdaughters , barbara a. hammond and brenda l. stevenson ; a stepson , michael a. taylor ; two grandchildren and one great-grandchild .
Head Entity: he
Tail Entity: brenda l. stevenson
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person children":
Relation: person children  
Context: she has three children, including her son, john, and her daughters, emily and sarah, who are both pursuing their studies.  
Head Entity: she  
Tail Entity: emily  

Relation: person children  
Context: after the passing of their father, the siblings, including jake and his sister, lila, took on the responsibility of caring for their younger brother, max.  
Head Entity: jake  
Tail Entity: max  
#### person charges ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person charges" means a person was charged with an event.
Here is an example:
Relation: person charges
Context: ferrara said he was innocent of limoli 's slaying , but he pleaded guilty in 1992 to murder , along with racketeering charges , under a deal that sent him to prison for 22 years , rather than go to trial and risk a conviction that could lead to life in prison .
Head Entity: ferrara
Tail Entity: racketeering
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person charges":
Relation: person charges  
Context: After a lengthy investigation, the authorities announced that Johnson was charged with embezzlement, a crime that could result in significant prison time if convicted.  
Head Entity: Johnson  
Tail Entity: embezzlement  

Relation: person charges  
Context: The district attorney confirmed that Smith was charged with assault following the altercation that took place last month, which left several people injured.  
Head Entity: Smith  
Tail Entity: assault  
Mixup data size:  195
MixupTrain:  epoch  0, batch     0 | loss: 3.9322261MixupTrain:  epoch  0, batch     1 | loss: 4.2280091MixupTrain:  epoch  0, batch     2 | loss: 3.8429318MixupTrain:  epoch  0, batch     3 | loss: 3.2031668MixupTrain:  epoch  0, batch     4 | loss: 2.9742557MixupTrain:  epoch  0, batch     5 | loss: 3.6444146MixupTrain:  epoch  0, batch     6 | loss: 4.1906841MixupTrain:  epoch  0, batch     7 | loss: 4.9095469MixupTrain:  epoch  0, batch     8 | loss: 3.4659062MixupTrain:  epoch  0, batch     9 | loss: 3.2090622MixupTrain:  epoch  0, batch    10 | loss: 3.4249613MixupTrain:  epoch  0, batch    11 | loss: 3.1634756MixupTrain:  epoch  0, batch    12 | loss: 2.3384175
MemoryTrain:  epoch  0, batch     0 | loss: 2.1514821MemoryTrain:  epoch  0, batch     1 | loss: 3.5801473MemoryTrain:  epoch  0, batch     2 | loss: 3.5134780MemoryTrain:  epoch  0, batch     3 | loss: 3.0943179MemoryTrain:  epoch  0, batch     4 | loss: 2.8496945MemoryTrain:  epoch  1, batch     0 | loss: 2.4486964MemoryTrain:  epoch  1, batch     1 | loss: 2.6933398MemoryTrain:  epoch  1, batch     2 | loss: 2.0997128MemoryTrain:  epoch  1, batch     3 | loss: 1.9479555MemoryTrain:  epoch  1, batch     4 | loss: 3.1046207MemoryTrain:  epoch  2, batch     0 | loss: 2.0321324MemoryTrain:  epoch  2, batch     1 | loss: 2.0616863MemoryTrain:  epoch  2, batch     2 | loss: 2.4571462MemoryTrain:  epoch  2, batch     3 | loss: 2.3980551MemoryTrain:  epoch  2, batch     4 | loss: 1.8344203MemoryTrain:  epoch  3, batch     0 | loss: 1.9566989MemoryTrain:  epoch  3, batch     1 | loss: 1.9629487MemoryTrain:  epoch  3, batch     2 | loss: 2.0794368MemoryTrain:  epoch  3, batch     3 | loss: 2.2606053MemoryTrain:  epoch  3, batch     4 | loss: 2.1667869MemoryTrain:  epoch  4, batch     0 | loss: 2.0676901MemoryTrain:  epoch  4, batch     1 | loss: 1.9573274MemoryTrain:  epoch  4, batch     2 | loss: 1.8894354MemoryTrain:  epoch  4, batch     3 | loss: 1.6689166MemoryTrain:  epoch  4, batch     4 | loss: 1.7098653MemoryTrain:  epoch  5, batch     0 | loss: 1.8891399MemoryTrain:  epoch  5, batch     1 | loss: 1.6609755MemoryTrain:  epoch  5, batch     2 | loss: 2.0608821MemoryTrain:  epoch  5, batch     3 | loss: 1.6969454MemoryTrain:  epoch  5, batch     4 | loss: 1.9744065MemoryTrain:  epoch  6, batch     0 | loss: 1.7482419MemoryTrain:  epoch  6, batch     1 | loss: 1.8150206MemoryTrain:  epoch  6, batch     2 | loss: 1.5258107MemoryTrain:  epoch  6, batch     3 | loss: 1.5698643MemoryTrain:  epoch  6, batch     4 | loss: 2.0867975MemoryTrain:  epoch  7, batch     0 | loss: 1.8759327MemoryTrain:  epoch  7, batch     1 | loss: 1.6518661MemoryTrain:  epoch  7, batch     2 | loss: 1.6041794MemoryTrain:  epoch  7, batch     3 | loss: 1.6168032MemoryTrain:  epoch  7, batch     4 | loss: 1.4667097MemoryTrain:  epoch  8, batch     0 | loss: 1.8409991MemoryTrain:  epoch  8, batch     1 | loss: 1.4806957MemoryTrain:  epoch  8, batch     2 | loss: 1.5439177MemoryTrain:  epoch  8, batch     3 | loss: 1.5960530MemoryTrain:  epoch  8, batch     4 | loss: 1.4221094MemoryTrain:  epoch  9, batch     0 | loss: 1.4913566MemoryTrain:  epoch  9, batch     1 | loss: 1.5406902MemoryTrain:  epoch  9, batch     2 | loss: 1.5413346MemoryTrain:  epoch  9, batch     3 | loss: 1.4346038MemoryTrain:  epoch  9, batch     4 | loss: 1.4246916
[EVAL] batch:    0 | acc: 43.75%,  total acc: 43.75%   [EVAL] batch:    1 | acc: 68.75%,  total acc: 56.25%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 62.50%   [EVAL] batch:    3 | acc: 50.00%,  total acc: 59.38%   [EVAL] batch:    4 | acc: 56.25%,  total acc: 58.75%   [EVAL] batch:    5 | acc: 68.75%,  total acc: 60.42%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 63.39%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 66.41%   [EVAL] batch:    8 | acc: 50.00%,  total acc: 64.58%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 68.12%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 70.45%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 72.92%   [EVAL] batch:   12 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:   13 | acc: 100.00%,  total acc: 76.79%   [EVAL] batch:   14 | acc: 100.00%,  total acc: 78.33%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 79.69%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 80.88%   [EVAL] batch:   17 | acc: 25.00%,  total acc: 77.78%   
[EVAL] batch:    0 | acc: 31.25%,  total acc: 31.25%   [EVAL] batch:    1 | acc: 31.25%,  total acc: 31.25%   [EVAL] batch:    2 | acc: 31.25%,  total acc: 31.25%   [EVAL] batch:    3 | acc: 18.75%,  total acc: 28.12%   [EVAL] batch:    4 | acc: 12.50%,  total acc: 25.00%   [EVAL] batch:    5 | acc: 18.75%,  total acc: 23.96%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 32.14%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 40.62%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 47.22%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 51.88%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 56.25%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 58.85%   [EVAL] batch:   12 | acc: 62.50%,  total acc: 59.13%   [EVAL] batch:   13 | acc: 56.25%,  total acc: 58.93%   [EVAL] batch:   14 | acc: 81.25%,  total acc: 60.42%   [EVAL] batch:   15 | acc: 81.25%,  total acc: 61.72%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 62.50%   [EVAL] batch:   17 | acc: 87.50%,  total acc: 63.89%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 65.79%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 67.19%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 68.75%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 70.17%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 71.47%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 72.40%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 73.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 74.52%   [EVAL] batch:   26 | acc: 56.25%,  total acc: 73.84%   [EVAL] batch:   27 | acc: 50.00%,  total acc: 72.99%   [EVAL] batch:   28 | acc: 56.25%,  total acc: 72.41%   [EVAL] batch:   29 | acc: 43.75%,  total acc: 71.46%   [EVAL] batch:   30 | acc: 43.75%,  total acc: 70.56%   [EVAL] batch:   31 | acc: 56.25%,  total acc: 70.12%   [EVAL] batch:   32 | acc: 25.00%,  total acc: 68.75%   [EVAL] batch:   33 | acc: 37.50%,  total acc: 67.83%   [EVAL] batch:   34 | acc: 50.00%,  total acc: 67.32%   [EVAL] batch:   35 | acc: 50.00%,  total acc: 66.84%   [EVAL] batch:   36 | acc: 81.25%,  total acc: 67.23%   [EVAL] batch:   37 | acc: 81.25%,  total acc: 67.60%   [EVAL] batch:   38 | acc: 75.00%,  total acc: 67.79%   [EVAL] batch:   39 | acc: 50.00%,  total acc: 67.34%   [EVAL] batch:   40 | acc: 6.25%,  total acc: 65.85%   [EVAL] batch:   41 | acc: 12.50%,  total acc: 64.58%   [EVAL] batch:   42 | acc: 6.25%,  total acc: 63.23%   [EVAL] batch:   43 | acc: 50.00%,  total acc: 62.93%   [EVAL] batch:   44 | acc: 37.50%,  total acc: 62.36%   [EVAL] batch:   45 | acc: 31.25%,  total acc: 61.68%   [EVAL] batch:   46 | acc: 50.00%,  total acc: 61.44%   [EVAL] batch:   47 | acc: 31.25%,  total acc: 60.81%   [EVAL] batch:   48 | acc: 6.25%,  total acc: 59.69%   [EVAL] batch:   49 | acc: 25.00%,  total acc: 59.00%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 59.68%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 60.46%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 61.20%   [EVAL] batch:   53 | acc: 93.75%,  total acc: 61.81%   [EVAL] batch:   54 | acc: 56.25%,  total acc: 61.70%   [EVAL] batch:   55 | acc: 12.50%,  total acc: 60.83%   [EVAL] batch:   56 | acc: 0.00%,  total acc: 59.76%   [EVAL] batch:   57 | acc: 0.00%,  total acc: 58.73%   [EVAL] batch:   58 | acc: 6.25%,  total acc: 57.84%   [EVAL] batch:   59 | acc: 25.00%,  total acc: 57.29%   [EVAL] batch:   60 | acc: 62.50%,  total acc: 57.38%   [EVAL] batch:   61 | acc: 37.50%,  total acc: 57.06%   [EVAL] batch:   62 | acc: 18.75%,  total acc: 56.45%   [EVAL] batch:   63 | acc: 12.50%,  total acc: 55.76%   [EVAL] batch:   64 | acc: 18.75%,  total acc: 55.19%   [EVAL] batch:   65 | acc: 12.50%,  total acc: 54.55%   [EVAL] batch:   66 | acc: 50.00%,  total acc: 54.48%   [EVAL] batch:   67 | acc: 50.00%,  total acc: 54.41%   [EVAL] batch:   68 | acc: 31.25%,  total acc: 54.08%   [EVAL] batch:   69 | acc: 50.00%,  total acc: 54.02%   [EVAL] batch:   70 | acc: 68.75%,  total acc: 54.23%   [EVAL] batch:   71 | acc: 56.25%,  total acc: 54.25%   [EVAL] batch:   72 | acc: 81.25%,  total acc: 54.62%   [EVAL] batch:   73 | acc: 93.75%,  total acc: 55.15%   [EVAL] batch:   74 | acc: 100.00%,  total acc: 55.75%   [EVAL] batch:   75 | acc: 100.00%,  total acc: 56.33%   [EVAL] batch:   76 | acc: 81.25%,  total acc: 56.66%   [EVAL] batch:   77 | acc: 37.50%,  total acc: 56.41%   [EVAL] batch:   78 | acc: 0.00%,  total acc: 55.70%   [EVAL] batch:   79 | acc: 6.25%,  total acc: 55.08%   [EVAL] batch:   80 | acc: 12.50%,  total acc: 54.55%   [EVAL] batch:   81 | acc: 43.75%,  total acc: 54.42%   [EVAL] batch:   82 | acc: 68.75%,  total acc: 54.59%   [EVAL] batch:   83 | acc: 68.75%,  total acc: 54.76%   [EVAL] batch:   84 | acc: 43.75%,  total acc: 54.63%   [EVAL] batch:   85 | acc: 75.00%,  total acc: 54.87%   [EVAL] batch:   86 | acc: 56.25%,  total acc: 54.89%   [EVAL] batch:   87 | acc: 93.75%,  total acc: 55.33%   [EVAL] batch:   88 | acc: 81.25%,  total acc: 55.62%   [EVAL] batch:   89 | acc: 56.25%,  total acc: 55.62%   [EVAL] batch:   90 | acc: 100.00%,  total acc: 56.11%   [EVAL] batch:   91 | acc: 93.75%,  total acc: 56.52%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 56.99%   [EVAL] batch:   93 | acc: 100.00%,  total acc: 57.45%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 57.89%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 58.33%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 58.76%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 59.18%   [EVAL] batch:   98 | acc: 6.25%,  total acc: 58.65%   
cur_acc:  ['0.8523', '0.7692', '0.7542', '0.4801', '0.7778']
his_acc:  ['0.8523', '0.8125', '0.7000', '0.6713', '0.5865']
CurrentTrain: epoch  0, batch     0 | loss: 4.5344734CurrentTrain: epoch  0, batch     1 | loss: 5.7103181CurrentTrain: epoch  1, batch     0 | loss: 3.6226709CurrentTrain: epoch  1, batch     1 | loss: 4.5474458CurrentTrain: epoch  2, batch     0 | loss: 3.6730485CurrentTrain: epoch  2, batch     1 | loss: 3.1696336CurrentTrain: epoch  3, batch     0 | loss: 3.1283908CurrentTrain: epoch  3, batch     1 | loss: 2.7530479CurrentTrain: epoch  4, batch     0 | loss: 2.7598622CurrentTrain: epoch  4, batch     1 | loss: 2.4174461CurrentTrain: epoch  5, batch     0 | loss: 2.4595561CurrentTrain: epoch  5, batch     1 | loss: 2.3479588CurrentTrain: epoch  6, batch     0 | loss: 2.4721150CurrentTrain: epoch  6, batch     1 | loss: 2.4368629CurrentTrain: epoch  7, batch     0 | loss: 2.4071484CurrentTrain: epoch  7, batch     1 | loss: 2.4292271CurrentTrain: epoch  8, batch     0 | loss: 2.3634853CurrentTrain: epoch  8, batch     1 | loss: 2.1149216CurrentTrain: epoch  9, batch     0 | loss: 2.1699619CurrentTrain: epoch  9, batch     1 | loss: 2.1095316
#### organization founded ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization founded" means an organization was found in a date.
Here is an example:
Relation: organization founded
Context: pandit worked at the brokerage morgan stanley for about 11 years until 2005 , when he and some morgan stanley colleagues quit and later founded the hedge fund old lane partners .
Head Entity: old lane partners
Tail Entity: 2005
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization founded":
Relation: organization founded  
Context: In 1998, a group of engineers and entrepreneurs came together to establish the tech startup, Innovatech Solutions, which has since become a leader in software development.  
Head Entity: Innovatech Solutions  
Tail Entity: 1998  

Relation: organization founded  
Context: The non-profit organization Green Earth Initiative was established in 2010 to promote environmental awareness and sustainability practices across communities.  
Head Entity: Green Earth Initiative  
Tail Entity: 2010  
#### person age ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person age" means a person 's age is a number.
Here is an example:
Relation: person age
Context: susan boyle is 48 years old now .
Head Entity: susan boyle
Tail Entity: 48
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person age":
Relation: person age  
Context: john is celebrating his 30th birthday today.  
Head Entity: john  
Tail Entity: 30  

Relation: person age  
Context: the famous actor, robert downey jr., turned 56 last week.  
Head Entity: robert downey jr.  
Tail Entity: 56  
#### person city of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person city of birth" means a person was born in a city.
Here is an example:
Relation: person city of birth
Context: rothman was born in san francisco in 1932 in an orthodox jewish family .
Head Entity: rothman
Tail Entity: san francisco
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person city of birth":
Relation: person city of birth  
Context: after spending his early years in new york, he moved to los angeles where he began his career.  
Head Entity: he  
Tail Entity: los angeles  

Relation: person city of birth  
Context: the famous author was born in a small town near boston, which greatly influenced his writing.  
Head Entity: the famous author  
Tail Entity: boston  
#### organization members ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization members" means an organization 's member is an organization.
Here is an example:
Relation: organization members
Context: san diego 32 new orleans 37 american football : nfl result result of the nfl match between the san diego chargers of the afc west and the new orleans saints of the nfc south at wembley here sunday :
Head Entity: nfc south
Tail Entity: new orleans saints
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization members":
Relation: organization members  
Context: the tech giant apple inc. has announced its new team of engineers who will be working on the latest software updates for their devices, including the new members from the software development organization.  
Head Entity: software development organization  
Tail Entity: apple inc.  

Relation: organization members  
Context: during the annual conference, the president of the environmental advocacy group revealed the new members who joined the organization to help combat climate change and promote sustainability initiatives.  
Head Entity: environmental advocacy group  
Tail Entity: climate change organization  
#### person religion ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person religion" means an person is the member of a religion.
Here is an example:
Relation: person religion
Context: he fought attempts by zealous jews to move into the muslim quarter of the walled old city , but defended the practice of developing jewish suburbs around the eastern arab sector to prevent it from ever escaping israel 's rule .
Head Entity: he
Tail Entity: jewish
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person religion":
Relation: person religion  
Context: After years of studying various philosophies, she finally embraced Buddhism, finding peace and purpose in its teachings.  
Head Entity: she  
Tail Entity: Buddhism  

Relation: person religion  
Context: The renowned author often spoke about his deep connection to Christianity and how it influenced his writing and personal life.  
Head Entity: author  
Tail Entity: Christianity  
Mixup data size:  225
MixupTrain:  epoch  0, batch     0 | loss: 3.3033543MixupTrain:  epoch  0, batch     1 | loss: 3.4518042MixupTrain:  epoch  0, batch     2 | loss: 2.6018729MixupTrain:  epoch  0, batch     3 | loss: 3.2767686MixupTrain:  epoch  0, batch     4 | loss: 2.5165610MixupTrain:  epoch  0, batch     5 | loss: 2.7905090MixupTrain:  epoch  0, batch     6 | loss: 2.7900532MixupTrain:  epoch  0, batch     7 | loss: 2.8751053MixupTrain:  epoch  0, batch     8 | loss: 2.5965416MixupTrain:  epoch  0, batch     9 | loss: 2.7093911MixupTrain:  epoch  0, batch    10 | loss: 2.6740439MixupTrain:  epoch  0, batch    11 | loss: 2.8447846MixupTrain:  epoch  0, batch    12 | loss: 3.0312055MixupTrain:  epoch  0, batch    13 | loss: 4.1098947MixupTrain:  epoch  0, batch    14 | loss: 1.4288263
MemoryTrain:  epoch  0, batch     0 | loss: 2.0175939MemoryTrain:  epoch  0, batch     1 | loss: 2.3180952MemoryTrain:  epoch  0, batch     2 | loss: 2.7758684MemoryTrain:  epoch  0, batch     3 | loss: 2.1341548MemoryTrain:  epoch  0, batch     4 | loss: 2.9342713MemoryTrain:  epoch  0, batch     5 | loss: 2.6737187MemoryTrain:  epoch  1, batch     0 | loss: 2.6070590MemoryTrain:  epoch  1, batch     1 | loss: 2.7214956MemoryTrain:  epoch  1, batch     2 | loss: 1.8575257MemoryTrain:  epoch  1, batch     3 | loss: 2.1801245MemoryTrain:  epoch  1, batch     4 | loss: 1.9063377MemoryTrain:  epoch  1, batch     5 | loss: 2.0006416MemoryTrain:  epoch  2, batch     0 | loss: 2.2555075MemoryTrain:  epoch  2, batch     1 | loss: 1.6476545MemoryTrain:  epoch  2, batch     2 | loss: 1.9793106MemoryTrain:  epoch  2, batch     3 | loss: 1.7817148MemoryTrain:  epoch  2, batch     4 | loss: 1.9298041MemoryTrain:  epoch  2, batch     5 | loss: 1.8797233MemoryTrain:  epoch  3, batch     0 | loss: 2.0045586MemoryTrain:  epoch  3, batch     1 | loss: 1.6820555MemoryTrain:  epoch  3, batch     2 | loss: 1.6056780MemoryTrain:  epoch  3, batch     3 | loss: 1.5189314MemoryTrain:  epoch  3, batch     4 | loss: 1.8039553MemoryTrain:  epoch  3, batch     5 | loss: 1.7730963MemoryTrain:  epoch  4, batch     0 | loss: 1.5253741MemoryTrain:  epoch  4, batch     1 | loss: 1.5797335MemoryTrain:  epoch  4, batch     2 | loss: 2.1737194MemoryTrain:  epoch  4, batch     3 | loss: 1.5228628MemoryTrain:  epoch  4, batch     4 | loss: 1.5681982MemoryTrain:  epoch  4, batch     5 | loss: 1.3638201MemoryTrain:  epoch  5, batch     0 | loss: 1.5723175MemoryTrain:  epoch  5, batch     1 | loss: 1.6989846MemoryTrain:  epoch  5, batch     2 | loss: 1.3414290MemoryTrain:  epoch  5, batch     3 | loss: 1.4268202MemoryTrain:  epoch  5, batch     4 | loss: 1.7156042MemoryTrain:  epoch  5, batch     5 | loss: 1.6260045MemoryTrain:  epoch  6, batch     0 | loss: 1.5803597MemoryTrain:  epoch  6, batch     1 | loss: 1.3948286MemoryTrain:  epoch  6, batch     2 | loss: 1.5637922MemoryTrain:  epoch  6, batch     3 | loss: 1.5765616MemoryTrain:  epoch  6, batch     4 | loss: 1.3531830MemoryTrain:  epoch  6, batch     5 | loss: 1.3442851MemoryTrain:  epoch  7, batch     0 | loss: 1.4058714MemoryTrain:  epoch  7, batch     1 | loss: 1.5192809MemoryTrain:  epoch  7, batch     2 | loss: 1.6263013MemoryTrain:  epoch  7, batch     3 | loss: 1.5011072MemoryTrain:  epoch  7, batch     4 | loss: 1.5766191MemoryTrain:  epoch  7, batch     5 | loss: 1.5048929MemoryTrain:  epoch  8, batch     0 | loss: 1.6099783MemoryTrain:  epoch  8, batch     1 | loss: 1.4846946MemoryTrain:  epoch  8, batch     2 | loss: 1.5959501MemoryTrain:  epoch  8, batch     3 | loss: 1.3905634MemoryTrain:  epoch  8, batch     4 | loss: 1.3303695MemoryTrain:  epoch  8, batch     5 | loss: 1.4898705MemoryTrain:  epoch  9, batch     0 | loss: 1.3148704MemoryTrain:  epoch  9, batch     1 | loss: 1.2483768MemoryTrain:  epoch  9, batch     2 | loss: 1.3863058MemoryTrain:  epoch  9, batch     3 | loss: 1.4455826MemoryTrain:  epoch  9, batch     4 | loss: 1.4621866MemoryTrain:  epoch  9, batch     5 | loss: 1.3890678
[EVAL] batch:    0 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    1 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    2 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    3 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    4 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 99.31%   [EVAL] batch:    9 | acc: 56.25%,  total acc: 95.00%   [EVAL] batch:   10 | acc: 75.00%,  total acc: 93.18%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 93.23%   [EVAL] batch:   12 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   13 | acc: 75.00%,  total acc: 92.41%   
[EVAL] batch:    0 | acc: 25.00%,  total acc: 25.00%   [EVAL] batch:    1 | acc: 31.25%,  total acc: 28.12%   [EVAL] batch:    2 | acc: 25.00%,  total acc: 27.08%   [EVAL] batch:    3 | acc: 18.75%,  total acc: 25.00%   [EVAL] batch:    4 | acc: 18.75%,  total acc: 23.75%   [EVAL] batch:    5 | acc: 18.75%,  total acc: 22.92%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 31.25%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 39.84%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 46.53%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 51.25%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 55.68%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 58.33%   [EVAL] batch:   12 | acc: 50.00%,  total acc: 57.69%   [EVAL] batch:   13 | acc: 37.50%,  total acc: 56.25%   [EVAL] batch:   14 | acc: 81.25%,  total acc: 57.92%   [EVAL] batch:   15 | acc: 81.25%,  total acc: 59.38%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 60.29%   [EVAL] batch:   17 | acc: 81.25%,  total acc: 61.46%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 63.49%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 65.00%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 66.67%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 68.18%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 69.57%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 70.57%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 71.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 72.84%   [EVAL] batch:   26 | acc: 43.75%,  total acc: 71.76%   [EVAL] batch:   27 | acc: 18.75%,  total acc: 69.87%   [EVAL] batch:   28 | acc: 43.75%,  total acc: 68.97%   [EVAL] batch:   29 | acc: 25.00%,  total acc: 67.50%   [EVAL] batch:   30 | acc: 37.50%,  total acc: 66.53%   [EVAL] batch:   31 | acc: 25.00%,  total acc: 65.23%   [EVAL] batch:   32 | acc: 18.75%,  total acc: 63.83%   [EVAL] batch:   33 | acc: 37.50%,  total acc: 63.05%   [EVAL] batch:   34 | acc: 50.00%,  total acc: 62.68%   [EVAL] batch:   35 | acc: 37.50%,  total acc: 61.98%   [EVAL] batch:   36 | acc: 87.50%,  total acc: 62.67%   [EVAL] batch:   37 | acc: 81.25%,  total acc: 63.16%   [EVAL] batch:   38 | acc: 87.50%,  total acc: 63.78%   [EVAL] batch:   39 | acc: 50.00%,  total acc: 63.44%   [EVAL] batch:   40 | acc: 25.00%,  total acc: 62.50%   [EVAL] batch:   41 | acc: 12.50%,  total acc: 61.31%   [EVAL] batch:   42 | acc: 12.50%,  total acc: 60.17%   [EVAL] batch:   43 | acc: 43.75%,  total acc: 59.80%   [EVAL] batch:   44 | acc: 37.50%,  total acc: 59.31%   [EVAL] batch:   45 | acc: 25.00%,  total acc: 58.56%   [EVAL] batch:   46 | acc: 43.75%,  total acc: 58.24%   [EVAL] batch:   47 | acc: 31.25%,  total acc: 57.68%   [EVAL] batch:   48 | acc: 12.50%,  total acc: 56.76%   [EVAL] batch:   49 | acc: 18.75%,  total acc: 56.00%   [EVAL] batch:   50 | acc: 87.50%,  total acc: 56.62%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 57.33%   [EVAL] batch:   52 | acc: 87.50%,  total acc: 57.90%   [EVAL] batch:   53 | acc: 93.75%,  total acc: 58.56%   [EVAL] batch:   54 | acc: 56.25%,  total acc: 58.52%   [EVAL] batch:   55 | acc: 12.50%,  total acc: 57.70%   [EVAL] batch:   56 | acc: 0.00%,  total acc: 56.69%   [EVAL] batch:   57 | acc: 0.00%,  total acc: 55.71%   [EVAL] batch:   58 | acc: 6.25%,  total acc: 54.87%   [EVAL] batch:   59 | acc: 6.25%,  total acc: 54.06%   [EVAL] batch:   60 | acc: 25.00%,  total acc: 53.59%   [EVAL] batch:   61 | acc: 18.75%,  total acc: 53.02%   [EVAL] batch:   62 | acc: 6.25%,  total acc: 52.28%   [EVAL] batch:   63 | acc: 12.50%,  total acc: 51.66%   [EVAL] batch:   64 | acc: 12.50%,  total acc: 51.06%   [EVAL] batch:   65 | acc: 0.00%,  total acc: 50.28%   [EVAL] batch:   66 | acc: 43.75%,  total acc: 50.19%   [EVAL] batch:   67 | acc: 50.00%,  total acc: 50.18%   [EVAL] batch:   68 | acc: 31.25%,  total acc: 49.91%   [EVAL] batch:   69 | acc: 50.00%,  total acc: 49.91%   [EVAL] batch:   70 | acc: 62.50%,  total acc: 50.09%   [EVAL] batch:   71 | acc: 56.25%,  total acc: 50.17%   [EVAL] batch:   72 | acc: 81.25%,  total acc: 50.60%   [EVAL] batch:   73 | acc: 93.75%,  total acc: 51.18%   [EVAL] batch:   74 | acc: 93.75%,  total acc: 51.75%   [EVAL] batch:   75 | acc: 100.00%,  total acc: 52.38%   [EVAL] batch:   76 | acc: 87.50%,  total acc: 52.84%   [EVAL] batch:   77 | acc: 37.50%,  total acc: 52.64%   [EVAL] batch:   78 | acc: 0.00%,  total acc: 51.98%   [EVAL] batch:   79 | acc: 6.25%,  total acc: 51.41%   [EVAL] batch:   80 | acc: 6.25%,  total acc: 50.85%   [EVAL] batch:   81 | acc: 6.25%,  total acc: 50.30%   [EVAL] batch:   82 | acc: 12.50%,  total acc: 49.85%   [EVAL] batch:   83 | acc: 18.75%,  total acc: 49.48%   [EVAL] batch:   84 | acc: 12.50%,  total acc: 49.04%   [EVAL] batch:   85 | acc: 12.50%,  total acc: 48.62%   [EVAL] batch:   86 | acc: 12.50%,  total acc: 48.20%   [EVAL] batch:   87 | acc: 87.50%,  total acc: 48.65%   [EVAL] batch:   88 | acc: 81.25%,  total acc: 49.02%   [EVAL] batch:   89 | acc: 50.00%,  total acc: 49.03%   [EVAL] batch:   90 | acc: 100.00%,  total acc: 49.59%   [EVAL] batch:   91 | acc: 87.50%,  total acc: 50.00%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 50.54%   [EVAL] batch:   93 | acc: 100.00%,  total acc: 51.06%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 51.58%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 52.08%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 52.58%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 53.06%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 53.54%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 54.00%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 54.46%   [EVAL] batch:  101 | acc: 100.00%,  total acc: 54.90%   [EVAL] batch:  102 | acc: 100.00%,  total acc: 55.34%   [EVAL] batch:  103 | acc: 100.00%,  total acc: 55.77%   [EVAL] batch:  104 | acc: 100.00%,  total acc: 56.19%   [EVAL] batch:  105 | acc: 100.00%,  total acc: 56.60%   [EVAL] batch:  106 | acc: 93.75%,  total acc: 56.95%   [EVAL] batch:  107 | acc: 56.25%,  total acc: 56.94%   [EVAL] batch:  108 | acc: 75.00%,  total acc: 57.11%   [EVAL] batch:  109 | acc: 93.75%,  total acc: 57.44%   [EVAL] batch:  110 | acc: 100.00%,  total acc: 57.83%   [EVAL] batch:  111 | acc: 81.25%,  total acc: 58.04%   
cur_acc:  ['0.8523', '0.7692', '0.7542', '0.4801', '0.7778', '0.9241']
his_acc:  ['0.8523', '0.8125', '0.7000', '0.6713', '0.5865', '0.5804']
CurrentTrain: epoch  0, batch     0 | loss: 6.0523100CurrentTrain: epoch  0, batch     1 | loss: 5.7064967CurrentTrain: epoch  1, batch     0 | loss: 4.9271016CurrentTrain: epoch  1, batch     1 | loss: 4.1568532CurrentTrain: epoch  2, batch     0 | loss: 3.9184573CurrentTrain: epoch  2, batch     1 | loss: 4.5358148CurrentTrain: epoch  3, batch     0 | loss: 3.6457729CurrentTrain: epoch  3, batch     1 | loss: 4.3489904CurrentTrain: epoch  4, batch     0 | loss: 3.8229756CurrentTrain: epoch  4, batch     1 | loss: 3.1624074CurrentTrain: epoch  5, batch     0 | loss: 3.0163932CurrentTrain: epoch  5, batch     1 | loss: 3.0465269CurrentTrain: epoch  6, batch     0 | loss: 2.8590691CurrentTrain: epoch  6, batch     1 | loss: 3.0304842CurrentTrain: epoch  7, batch     0 | loss: 2.6619518CurrentTrain: epoch  7, batch     1 | loss: 2.8755620CurrentTrain: epoch  8, batch     0 | loss: 2.6246650CurrentTrain: epoch  8, batch     1 | loss: 2.4030521CurrentTrain: epoch  9, batch     0 | loss: 2.4884751CurrentTrain: epoch  9, batch     1 | loss: 2.2127244
#### person date of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person date of birth" means a person was born in a date.
Here is an example:
Relation: person date of birth
Context: kirkaldy , born irene morgan in baltimore , maryland , in 1917 , was arrested in 1944 for refusing to give up her seat on a greyhound bus heading from gloucester to baltimore , and for resisting arrest .
Head Entity: irene morgan
Tail Entity: 1917
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person date of birth":
Relation: person date of birth  
Context: the famous physicist albert einstein was born in ulm, in the kingdom of wurttemberg in the german empire, on march 14, 1879.  
Head Entity: albert einstein  
Tail Entity: march 14, 1879  

Relation: person date of birth  
Context: the renowned author jane austen was born on december 16, 1775, in steventon, hampshire, england.  
Head Entity: jane austen  
Tail Entity: december 16, 1775  
#### person stateorprovince of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person stateorprovince of birth" means a person was died in a state or province.
Here is an example:
Relation: person stateorprovince of birth
Context: jefferson joseph deblanc sr. was born in lockport , la. , on feb. 15 , 1921 , and grew up in st. martinville .
Head Entity: jefferson joseph deblanc sr.
Tail Entity: la.
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person stateorprovince of birth":
Relation: person stateorprovince of birth  
Context: martha ann jones was born in springfield, il, on march 3, 1985, and later moved to chicago.  
Head Entity: martha ann jones  
Tail Entity: il.  

Relation: person stateorprovince of birth  
Context: robert thomas was born in phoenix, az, in 1970, and spent his childhood in tucson.  
Head Entity: robert thomas  
Tail Entity: az.  
#### person parents ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person parents" means a person 's parent is a person.
Here is an example:
Relation: person parents
Context: lynne spears told the court that lutfi had treated her daughter like a hostage in her own home , drugged her and took over her finances .
Head Entity: her
Tail Entity: lynne spears
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person parents":
Relation: person parents  
Context: During the family reunion, Sarah introduced her father, John, to her friends, highlighting how much he has influenced her life choices.  
Head Entity: her father  
Tail Entity: Sarah  

Relation: person parents  
Context: After the ceremony, Emily shared stories about her mother, who had always been her biggest supporter and role model throughout her life.  
Head Entity: her mother  
Tail Entity: Emily  
#### person employee of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person employee of" means a person is an employee of an organization.
Here is an example:
Relation: person employee of
Context: kell hath no fury : publicist and mtv reality star kelly cutrone is wasting no time in kicking her brands -lrb- including her p.r. firm people 's revolution and , increasingly , kelly cutrone herself -rrb- into high gear in 2010 .
Head Entity: kelly cutrone
Tail Entity: mtv
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person employee of":
Relation: person employee of  
Context: After years of hard work and dedication, Sarah Thompson has finally landed a position at one of the top tech companies in Silicon Valley, where she will be contributing to innovative projects.  
Head Entity: Sarah Thompson  
Tail Entity: tech companies  

Relation: person employee of  
Context: John Smith, a talented graphic designer, has been working for Creative Solutions for over five years, helping to shape the visual identity of numerous brands.  
Head Entity: John Smith  
Tail Entity: Creative Solutions  
#### person stateorprovince of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person stateorprovince of death" means a person was died in a state or province.
Here is an example:
Relation: person stateorprovince of death
Context: irene morgan kirkaldy , 90 , who died of alzheimer 's disease aug. 10 at her home in gloucester , va. , quietly changed history in 1944 when she refused to give up her seat on a crowded greyhound bus to a white couple .
Head Entity: irene morgan kirkaldy
Tail Entity: va.
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person stateorprovince of death":
Relation: person stateorprovince of death  
Context: john smith, a renowned author, passed away on march 5 in his residence located in los angeles, california, leaving behind a legacy of literary works that inspired many.  
Head Entity: john smith  
Tail Entity: california  

Relation: person stateorprovince of death  
Context: after a long battle with cancer, elizabeth taylor died on march 23 at a hospital in los angeles, california, surrounded by her family and friends.  
Head Entity: elizabeth taylor  
Tail Entity: california  
Mixup data size:  254
MixupTrain:  epoch  0, batch     0 | loss: 2.9820903MixupTrain:  epoch  0, batch     1 | loss: 2.5716485MixupTrain:  epoch  0, batch     2 | loss: 2.4616840MixupTrain:  epoch  0, batch     3 | loss: 2.6854240MixupTrain:  epoch  0, batch     4 | loss: 3.2282575MixupTrain:  epoch  0, batch     5 | loss: 2.6706713MixupTrain:  epoch  0, batch     6 | loss: 2.5675087MixupTrain:  epoch  0, batch     7 | loss: 2.2185442MixupTrain:  epoch  0, batch     8 | loss: 2.5283741MixupTrain:  epoch  0, batch     9 | loss: 2.7018275MixupTrain:  epoch  0, batch    10 | loss: 2.7214533MixupTrain:  epoch  0, batch    11 | loss: 2.4589135MixupTrain:  epoch  0, batch    12 | loss: 2.3184425MixupTrain:  epoch  0, batch    13 | loss: 2.6611609MixupTrain:  epoch  0, batch    14 | loss: 3.1867232MixupTrain:  epoch  0, batch    15 | loss: 2.0715251
MemoryTrain:  epoch  0, batch     0 | loss: 1.6856754MemoryTrain:  epoch  0, batch     1 | loss: 2.0895984MemoryTrain:  epoch  0, batch     2 | loss: 2.3292146MemoryTrain:  epoch  0, batch     3 | loss: 2.1703141MemoryTrain:  epoch  0, batch     4 | loss: 2.1327825MemoryTrain:  epoch  0, batch     5 | loss: 2.8499088MemoryTrain:  epoch  0, batch     6 | loss: 2.2539074MemoryTrain:  epoch  1, batch     0 | loss: 2.3401728MemoryTrain:  epoch  1, batch     1 | loss: 2.0414829MemoryTrain:  epoch  1, batch     2 | loss: 1.4981041MemoryTrain:  epoch  1, batch     3 | loss: 2.0112996MemoryTrain:  epoch  1, batch     4 | loss: 2.4341240MemoryTrain:  epoch  1, batch     5 | loss: 1.8908976MemoryTrain:  epoch  1, batch     6 | loss: 2.3813176MemoryTrain:  epoch  2, batch     0 | loss: 1.6371477MemoryTrain:  epoch  2, batch     1 | loss: 1.4848578MemoryTrain:  epoch  2, batch     2 | loss: 1.8086503MemoryTrain:  epoch  2, batch     3 | loss: 1.4889073MemoryTrain:  epoch  2, batch     4 | loss: 1.6505995MemoryTrain:  epoch  2, batch     5 | loss: 1.5391165MemoryTrain:  epoch  2, batch     6 | loss: 2.0171027MemoryTrain:  epoch  3, batch     0 | loss: 1.5899324MemoryTrain:  epoch  3, batch     1 | loss: 1.9336931MemoryTrain:  epoch  3, batch     2 | loss: 1.4665622MemoryTrain:  epoch  3, batch     3 | loss: 1.6106813MemoryTrain:  epoch  3, batch     4 | loss: 1.4543881MemoryTrain:  epoch  3, batch     5 | loss: 1.6097431MemoryTrain:  epoch  3, batch     6 | loss: 1.4872360MemoryTrain:  epoch  4, batch     0 | loss: 1.6091819MemoryTrain:  epoch  4, batch     1 | loss: 1.9968184MemoryTrain:  epoch  4, batch     2 | loss: 1.4015076MemoryTrain:  epoch  4, batch     3 | loss: 1.4533126MemoryTrain:  epoch  4, batch     4 | loss: 1.4216907MemoryTrain:  epoch  4, batch     5 | loss: 1.3222022MemoryTrain:  epoch  4, batch     6 | loss: 1.4307382MemoryTrain:  epoch  5, batch     0 | loss: 1.4789407MemoryTrain:  epoch  5, batch     1 | loss: 1.4280233MemoryTrain:  epoch  5, batch     2 | loss: 1.5035373MemoryTrain:  epoch  5, batch     3 | loss: 1.6783993MemoryTrain:  epoch  5, batch     4 | loss: 1.4066393MemoryTrain:  epoch  5, batch     5 | loss: 1.3148634MemoryTrain:  epoch  5, batch     6 | loss: 1.3546295MemoryTrain:  epoch  6, batch     0 | loss: 1.3391042MemoryTrain:  epoch  6, batch     1 | loss: 1.4536812MemoryTrain:  epoch  6, batch     2 | loss: 1.3936148MemoryTrain:  epoch  6, batch     3 | loss: 1.4228640MemoryTrain:  epoch  6, batch     4 | loss: 1.5915496MemoryTrain:  epoch  6, batch     5 | loss: 1.4966334MemoryTrain:  epoch  6, batch     6 | loss: 1.3356597MemoryTrain:  epoch  7, batch     0 | loss: 1.3329264MemoryTrain:  epoch  7, batch     1 | loss: 1.4054692MemoryTrain:  epoch  7, batch     2 | loss: 1.3809664MemoryTrain:  epoch  7, batch     3 | loss: 1.4255049MemoryTrain:  epoch  7, batch     4 | loss: 1.4753697MemoryTrain:  epoch  7, batch     5 | loss: 1.4267895MemoryTrain:  epoch  7, batch     6 | loss: 1.4195209MemoryTrain:  epoch  8, batch     0 | loss: 1.3012271MemoryTrain:  epoch  8, batch     1 | loss: 1.3631245MemoryTrain:  epoch  8, batch     2 | loss: 1.4358349MemoryTrain:  epoch  8, batch     3 | loss: 1.4345822MemoryTrain:  epoch  8, batch     4 | loss: 1.3720559MemoryTrain:  epoch  8, batch     5 | loss: 1.5383688MemoryTrain:  epoch  8, batch     6 | loss: 1.5036163MemoryTrain:  epoch  9, batch     0 | loss: 1.3830590MemoryTrain:  epoch  9, batch     1 | loss: 1.3141408MemoryTrain:  epoch  9, batch     2 | loss: 1.3469805MemoryTrain:  epoch  9, batch     3 | loss: 1.4659812MemoryTrain:  epoch  9, batch     4 | loss: 1.3301678MemoryTrain:  epoch  9, batch     5 | loss: 1.3363712MemoryTrain:  epoch  9, batch     6 | loss: 1.4285932
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    1 | acc: 25.00%,  total acc: 43.75%   [EVAL] batch:    2 | acc: 31.25%,  total acc: 39.58%   [EVAL] batch:    3 | acc: 25.00%,  total acc: 35.94%   [EVAL] batch:    4 | acc: 18.75%,  total acc: 32.50%   [EVAL] batch:    5 | acc: 25.00%,  total acc: 31.25%   [EVAL] batch:    6 | acc: 37.50%,  total acc: 32.14%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 39.84%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 45.14%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 49.38%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 52.84%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 54.69%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 56.73%   [EVAL] batch:   13 | acc: 18.75%,  total acc: 54.02%   
[EVAL] batch:    0 | acc: 31.25%,  total acc: 31.25%   [EVAL] batch:    1 | acc: 37.50%,  total acc: 34.38%   [EVAL] batch:    2 | acc: 43.75%,  total acc: 37.50%   [EVAL] batch:    3 | acc: 18.75%,  total acc: 32.81%   [EVAL] batch:    4 | acc: 37.50%,  total acc: 33.75%   [EVAL] batch:    5 | acc: 25.00%,  total acc: 32.29%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 39.29%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 46.88%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 52.08%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 56.25%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 60.23%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 62.50%   [EVAL] batch:   12 | acc: 50.00%,  total acc: 61.54%   [EVAL] batch:   13 | acc: 37.50%,  total acc: 59.82%   [EVAL] batch:   14 | acc: 81.25%,  total acc: 61.25%   [EVAL] batch:   15 | acc: 81.25%,  total acc: 62.50%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 63.24%   [EVAL] batch:   17 | acc: 81.25%,  total acc: 64.24%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 66.12%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 67.50%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 69.05%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 70.45%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 71.74%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 72.66%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 73.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 74.76%   [EVAL] batch:   26 | acc: 25.00%,  total acc: 72.92%   [EVAL] batch:   27 | acc: 18.75%,  total acc: 70.98%   [EVAL] batch:   28 | acc: 25.00%,  total acc: 69.40%   [EVAL] batch:   29 | acc: 25.00%,  total acc: 67.92%   [EVAL] batch:   30 | acc: 31.25%,  total acc: 66.73%   [EVAL] batch:   31 | acc: 25.00%,  total acc: 65.43%   [EVAL] batch:   32 | acc: 18.75%,  total acc: 64.02%   [EVAL] batch:   33 | acc: 37.50%,  total acc: 63.24%   [EVAL] batch:   34 | acc: 50.00%,  total acc: 62.86%   [EVAL] batch:   35 | acc: 37.50%,  total acc: 62.15%   [EVAL] batch:   36 | acc: 87.50%,  total acc: 62.84%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 63.49%   [EVAL] batch:   38 | acc: 87.50%,  total acc: 64.10%   [EVAL] batch:   39 | acc: 56.25%,  total acc: 63.91%   [EVAL] batch:   40 | acc: 12.50%,  total acc: 62.65%   [EVAL] batch:   41 | acc: 12.50%,  total acc: 61.46%   [EVAL] batch:   42 | acc: 12.50%,  total acc: 60.32%   [EVAL] batch:   43 | acc: 43.75%,  total acc: 59.94%   [EVAL] batch:   44 | acc: 37.50%,  total acc: 59.44%   [EVAL] batch:   45 | acc: 31.25%,  total acc: 58.83%   [EVAL] batch:   46 | acc: 50.00%,  total acc: 58.64%   [EVAL] batch:   47 | acc: 43.75%,  total acc: 58.33%   [EVAL] batch:   48 | acc: 18.75%,  total acc: 57.53%   [EVAL] batch:   49 | acc: 31.25%,  total acc: 57.00%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 57.72%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 58.41%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 59.08%   [EVAL] batch:   53 | acc: 93.75%,  total acc: 59.72%   [EVAL] batch:   54 | acc: 62.50%,  total acc: 59.77%   [EVAL] batch:   55 | acc: 12.50%,  total acc: 58.93%   [EVAL] batch:   56 | acc: 0.00%,  total acc: 57.89%   [EVAL] batch:   57 | acc: 0.00%,  total acc: 56.90%   [EVAL] batch:   58 | acc: 6.25%,  total acc: 56.04%   [EVAL] batch:   59 | acc: 0.00%,  total acc: 55.10%   [EVAL] batch:   60 | acc: 12.50%,  total acc: 54.41%   [EVAL] batch:   61 | acc: 12.50%,  total acc: 53.73%   [EVAL] batch:   62 | acc: 6.25%,  total acc: 52.98%   [EVAL] batch:   63 | acc: 12.50%,  total acc: 52.34%   [EVAL] batch:   64 | acc: 6.25%,  total acc: 51.63%   [EVAL] batch:   65 | acc: 0.00%,  total acc: 50.85%   [EVAL] batch:   66 | acc: 43.75%,  total acc: 50.75%   [EVAL] batch:   67 | acc: 50.00%,  total acc: 50.74%   [EVAL] batch:   68 | acc: 37.50%,  total acc: 50.54%   [EVAL] batch:   69 | acc: 50.00%,  total acc: 50.54%   [EVAL] batch:   70 | acc: 62.50%,  total acc: 50.70%   [EVAL] batch:   71 | acc: 56.25%,  total acc: 50.78%   [EVAL] batch:   72 | acc: 81.25%,  total acc: 51.20%   [EVAL] batch:   73 | acc: 93.75%,  total acc: 51.77%   [EVAL] batch:   74 | acc: 100.00%,  total acc: 52.42%   [EVAL] batch:   75 | acc: 100.00%,  total acc: 53.04%   [EVAL] batch:   76 | acc: 87.50%,  total acc: 53.49%   [EVAL] batch:   77 | acc: 37.50%,  total acc: 53.29%   [EVAL] batch:   78 | acc: 0.00%,  total acc: 52.61%   [EVAL] batch:   79 | acc: 6.25%,  total acc: 52.03%   [EVAL] batch:   80 | acc: 6.25%,  total acc: 51.47%   [EVAL] batch:   81 | acc: 6.25%,  total acc: 50.91%   [EVAL] batch:   82 | acc: 6.25%,  total acc: 50.38%   [EVAL] batch:   83 | acc: 18.75%,  total acc: 50.00%   [EVAL] batch:   84 | acc: 18.75%,  total acc: 49.63%   [EVAL] batch:   85 | acc: 18.75%,  total acc: 49.27%   [EVAL] batch:   86 | acc: 6.25%,  total acc: 48.78%   [EVAL] batch:   87 | acc: 81.25%,  total acc: 49.15%   [EVAL] batch:   88 | acc: 75.00%,  total acc: 49.44%   [EVAL] batch:   89 | acc: 31.25%,  total acc: 49.24%   [EVAL] batch:   90 | acc: 75.00%,  total acc: 49.52%   [EVAL] batch:   91 | acc: 68.75%,  total acc: 49.73%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 50.27%   [EVAL] batch:   93 | acc: 100.00%,  total acc: 50.80%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 51.32%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 51.82%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 52.32%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 52.81%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 53.28%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 53.75%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 54.21%   [EVAL] batch:  101 | acc: 100.00%,  total acc: 54.66%   [EVAL] batch:  102 | acc: 100.00%,  total acc: 55.10%   [EVAL] batch:  103 | acc: 100.00%,  total acc: 55.53%   [EVAL] batch:  104 | acc: 100.00%,  total acc: 55.95%   [EVAL] batch:  105 | acc: 100.00%,  total acc: 56.37%   [EVAL] batch:  106 | acc: 93.75%,  total acc: 56.72%   [EVAL] batch:  107 | acc: 50.00%,  total acc: 56.66%   [EVAL] batch:  108 | acc: 75.00%,  total acc: 56.82%   [EVAL] batch:  109 | acc: 93.75%,  total acc: 57.16%   [EVAL] batch:  110 | acc: 68.75%,  total acc: 57.26%   [EVAL] batch:  111 | acc: 87.50%,  total acc: 57.53%   [EVAL] batch:  112 | acc: 50.00%,  total acc: 57.47%   [EVAL] batch:  113 | acc: 31.25%,  total acc: 57.24%   [EVAL] batch:  114 | acc: 31.25%,  total acc: 57.01%   [EVAL] batch:  115 | acc: 12.50%,  total acc: 56.63%   [EVAL] batch:  116 | acc: 25.00%,  total acc: 56.36%   [EVAL] batch:  117 | acc: 18.75%,  total acc: 56.04%   [EVAL] batch:  118 | acc: 56.25%,  total acc: 56.04%   [EVAL] batch:  119 | acc: 93.75%,  total acc: 56.35%   [EVAL] batch:  120 | acc: 87.50%,  total acc: 56.61%   [EVAL] batch:  121 | acc: 87.50%,  total acc: 56.86%   [EVAL] batch:  122 | acc: 81.25%,  total acc: 57.06%   [EVAL] batch:  123 | acc: 81.25%,  total acc: 57.26%   [EVAL] batch:  124 | acc: 68.75%,  total acc: 57.35%   [EVAL] batch:  125 | acc: 12.50%,  total acc: 56.99%   
cur_acc:  ['0.8523', '0.7692', '0.7542', '0.4801', '0.7778', '0.9241', '0.5402']
his_acc:  ['0.8523', '0.8125', '0.7000', '0.6713', '0.5865', '0.5804', '0.5699']
CurrentTrain: epoch  0, batch     0 | loss: 5.7437048CurrentTrain: epoch  0, batch     1 | loss: 5.8930306CurrentTrain: epoch  1, batch     0 | loss: 3.8495760CurrentTrain: epoch  1, batch     1 | loss: 6.2989440CurrentTrain: epoch  2, batch     0 | loss: 4.1224661CurrentTrain: epoch  2, batch     1 | loss: 4.7115750CurrentTrain: epoch  3, batch     0 | loss: 4.0912375CurrentTrain: epoch  3, batch     1 | loss: 4.4429250CurrentTrain: epoch  4, batch     0 | loss: 3.6911678CurrentTrain: epoch  4, batch     1 | loss: 3.9642563CurrentTrain: epoch  5, batch     0 | loss: 3.5148191CurrentTrain: epoch  5, batch     1 | loss: 3.0544543CurrentTrain: epoch  6, batch     0 | loss: 2.6893003CurrentTrain: epoch  6, batch     1 | loss: 3.9640384CurrentTrain: epoch  7, batch     0 | loss: 3.5417471CurrentTrain: epoch  7, batch     1 | loss: 3.1285434CurrentTrain: epoch  8, batch     0 | loss: 3.0557318CurrentTrain: epoch  8, batch     1 | loss: 2.9802022CurrentTrain: epoch  9, batch     0 | loss: 2.8388617CurrentTrain: epoch  9, batch     1 | loss: 2.8895218
#### person country of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person country of birth" means a person was born in a country.
Here is an example:
Relation: person country of birth
Context: in testimony by satellite link from germany to a house of representatives ' panel , murat kurnaz recounted his five-year detention , alleging a wide range of torture and abuse .
Head Entity: murat kurnaz
Tail Entity: germany
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person country of birth":
Relation: person country of birth  
Context: During the interview, the renowned artist shared his experiences growing up in the vibrant streets of Barcelona, where he developed his passion for painting.  
Head Entity: the renowned artist  
Tail Entity: Barcelona  

Relation: person country of birth  
Context: In her autobiography, the famous actress reflects on her childhood in Mumbai, where she first discovered her love for performing arts.  
Head Entity: the famous actress  
Tail Entity: Mumbai  
#### organization website ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization website" means an organization 's website is an url.
Here is an example:
Relation: organization website
Context: http://www.pentaxsailaway.com/ paul j. sail away with pentax sweepstakes
Head Entity: pentax
Tail Entity: http://www.pentaxsailaway.com/
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization website":
Relation: organization website  
Context: Visit the official site at https://www.nike.com for the latest sports gear and apparel.  
Head Entity: Nike  
Tail Entity: https://www.nike.com  

Relation: organization website  
Context: For more information about their services, check out http://www.tesla.com.  
Head Entity: Tesla  
Tail Entity: http://www.tesla.com  
#### organization shareholders ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization shareholders" means an organization was invested by a person.
Here is an example:
Relation: organization shareholders
Context: marsans already owns the madrid-based air comet and 95 percent of aerolineas argentinas .
Head Entity: aerolineas argentinas
Tail Entity: marsans
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization shareholders":
Relation: organization shareholders  
Context: tech giant apple inc. has seen significant investments from billionaire investor warren buffett's berkshire hathaway.  
Head Entity: apple inc.  
Tail Entity: warren buffett  

Relation: organization shareholders  
Context: the startup company green tech solutions received funding from venture capitalist firm sequoia capital, boosting its growth potential.  
Head Entity: green tech solutions  
Tail Entity: sequoia capital  
#### organization dissolved ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization dissolved" means an organization was dissolved in a date.
Here is an example:
Relation: organization dissolved
Context: the tse suffered its worst-ever system crash in november 2005 which paralyzed the world 's second largest bourse and forced it to shelve plan for a listing of its own .
Head Entity: tse
Tail Entity: november 2005
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization dissolved":
Relation: organization dissolved  
Context: the once-prominent tech startup, Innovatech, officially ceased operations in March 2020 after struggling to secure funding.  
Head Entity: Innovatech  
Tail Entity: March 2020  

Relation: organization dissolved  
Context: after years of financial difficulties, the local arts council announced its closure in January 2019, marking the end of its community programs.  
Head Entity: local arts council  
Tail Entity: January 2019  
#### organization founded by ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization founded by" means an organization was found by a person.
Here is an example:
Relation: organization founded by
Context: zagat survey , the guide empire that started as a hobby for tim and nina zagat in 1979 as a two-page typed list of new york restaurants compiled from reviews from friends , has been put up for sale , according to people briefed on the decision .
Head Entity: zagat
Tail Entity: tim
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization founded by":
Relation: organization founded by  
Context: In 1975, the software company Microsoft was established by Bill Gates and Paul Allen, who aimed to revolutionize the personal computing industry.  
Head Entity: Microsoft  
Tail Entity: Bill Gates  

Relation: organization founded by  
Context: The renowned fashion brand Chanel was created by the iconic designer Coco Chanel in the early 20th century, changing the landscape of women's fashion forever.  
Head Entity: Chanel  
Tail Entity: Coco Chanel  
Mixup data size:  283
MixupTrain:  epoch  0, batch     0 | loss: 3.6425685MixupTrain:  epoch  0, batch     1 | loss: 1.9757522MixupTrain:  epoch  0, batch     2 | loss: 2.9330727MixupTrain:  epoch  0, batch     3 | loss: 2.5625723MixupTrain:  epoch  0, batch     4 | loss: 2.4033296MixupTrain:  epoch  0, batch     5 | loss: 2.5242364MixupTrain:  epoch  0, batch     6 | loss: 3.4294899MixupTrain:  epoch  0, batch     7 | loss: 2.8818229MixupTrain:  epoch  0, batch     8 | loss: 3.1513333MixupTrain:  epoch  0, batch     9 | loss: 3.5993280MixupTrain:  epoch  0, batch    10 | loss: 2.2452959MixupTrain:  epoch  0, batch    11 | loss: 2.1941943MixupTrain:  epoch  0, batch    12 | loss: 2.2445087MixupTrain:  epoch  0, batch    13 | loss: 2.9519074MixupTrain:  epoch  0, batch    14 | loss: 2.6433351MixupTrain:  epoch  0, batch    15 | loss: 2.6797438MixupTrain:  epoch  0, batch    16 | loss: 3.1996392MixupTrain:  epoch  0, batch    17 | loss: 2.5558269
MemoryTrain:  epoch  0, batch     0 | loss: 1.8261548MemoryTrain:  epoch  0, batch     1 | loss: 2.7257695MemoryTrain:  epoch  0, batch     2 | loss: 2.1514294MemoryTrain:  epoch  0, batch     3 | loss: 2.5083709MemoryTrain:  epoch  0, batch     4 | loss: 2.0485468MemoryTrain:  epoch  0, batch     5 | loss: 2.1270237MemoryTrain:  epoch  0, batch     6 | loss: 2.9389360MemoryTrain:  epoch  0, batch     7 | loss: 3.4834106MemoryTrain:  epoch  1, batch     0 | loss: 2.8630381MemoryTrain:  epoch  1, batch     1 | loss: 1.8927461MemoryTrain:  epoch  1, batch     2 | loss: 2.4128222MemoryTrain:  epoch  1, batch     3 | loss: 1.5694180MemoryTrain:  epoch  1, batch     4 | loss: 2.2801445MemoryTrain:  epoch  1, batch     5 | loss: 1.7941937MemoryTrain:  epoch  1, batch     6 | loss: 2.4271660MemoryTrain:  epoch  1, batch     7 | loss: 2.3566856MemoryTrain:  epoch  2, batch     0 | loss: 2.2183976MemoryTrain:  epoch  2, batch     1 | loss: 2.6890194MemoryTrain:  epoch  2, batch     2 | loss: 1.4149699MemoryTrain:  epoch  2, batch     3 | loss: 2.0213618MemoryTrain:  epoch  2, batch     4 | loss: 1.5146495MemoryTrain:  epoch  2, batch     5 | loss: 2.0248039MemoryTrain:  epoch  2, batch     6 | loss: 1.7919726MemoryTrain:  epoch  2, batch     7 | loss: 1.5114164MemoryTrain:  epoch  3, batch     0 | loss: 2.2910676MemoryTrain:  epoch  3, batch     1 | loss: 1.6516408MemoryTrain:  epoch  3, batch     2 | loss: 2.0717521MemoryTrain:  epoch  3, batch     3 | loss: 1.6080873MemoryTrain:  epoch  3, batch     4 | loss: 1.5238305MemoryTrain:  epoch  3, batch     5 | loss: 1.7030904MemoryTrain:  epoch  3, batch     6 | loss: 1.8581135MemoryTrain:  epoch  3, batch     7 | loss: 1.3901740MemoryTrain:  epoch  4, batch     0 | loss: 1.4545126MemoryTrain:  epoch  4, batch     1 | loss: 1.9311103MemoryTrain:  epoch  4, batch     2 | loss: 1.4666955MemoryTrain:  epoch  4, batch     3 | loss: 1.5353986MemoryTrain:  epoch  4, batch     4 | loss: 1.5757295MemoryTrain:  epoch  4, batch     5 | loss: 2.0991762MemoryTrain:  epoch  4, batch     6 | loss: 1.4502206MemoryTrain:  epoch  4, batch     7 | loss: 2.0198913MemoryTrain:  epoch  5, batch     0 | loss: 1.8834057MemoryTrain:  epoch  5, batch     1 | loss: 1.6147443MemoryTrain:  epoch  5, batch     2 | loss: 1.8424476MemoryTrain:  epoch  5, batch     3 | loss: 1.5725436MemoryTrain:  epoch  5, batch     4 | loss: 1.8330009MemoryTrain:  epoch  5, batch     5 | loss: 1.2893490MemoryTrain:  epoch  5, batch     6 | loss: 1.3298643MemoryTrain:  epoch  5, batch     7 | loss: 1.2911228MemoryTrain:  epoch  6, batch     0 | loss: 1.5346501MemoryTrain:  epoch  6, batch     1 | loss: 1.3907807MemoryTrain:  epoch  6, batch     2 | loss: 1.3264152MemoryTrain:  epoch  6, batch     3 | loss: 1.3504009MemoryTrain:  epoch  6, batch     4 | loss: 2.0902588MemoryTrain:  epoch  6, batch     5 | loss: 1.5358660MemoryTrain:  epoch  6, batch     6 | loss: 1.4897621MemoryTrain:  epoch  6, batch     7 | loss: 1.4851009MemoryTrain:  epoch  7, batch     0 | loss: 1.2849247MemoryTrain:  epoch  7, batch     1 | loss: 1.6923985MemoryTrain:  epoch  7, batch     2 | loss: 1.2987138MemoryTrain:  epoch  7, batch     3 | loss: 1.6250638MemoryTrain:  epoch  7, batch     4 | loss: 1.4654663MemoryTrain:  epoch  7, batch     5 | loss: 1.3183599MemoryTrain:  epoch  7, batch     6 | loss: 1.5396647MemoryTrain:  epoch  7, batch     7 | loss: 1.5779856MemoryTrain:  epoch  8, batch     0 | loss: 1.5122916MemoryTrain:  epoch  8, batch     1 | loss: 1.4949417MemoryTrain:  epoch  8, batch     2 | loss: 1.5598367MemoryTrain:  epoch  8, batch     3 | loss: 1.3586913MemoryTrain:  epoch  8, batch     4 | loss: 1.3227485MemoryTrain:  epoch  8, batch     5 | loss: 1.3193111MemoryTrain:  epoch  8, batch     6 | loss: 1.3044223MemoryTrain:  epoch  8, batch     7 | loss: 1.2682886MemoryTrain:  epoch  9, batch     0 | loss: 1.5643151MemoryTrain:  epoch  9, batch     1 | loss: 1.5157654MemoryTrain:  epoch  9, batch     2 | loss: 1.3026024MemoryTrain:  epoch  9, batch     3 | loss: 1.4784117MemoryTrain:  epoch  9, batch     4 | loss: 1.2969019MemoryTrain:  epoch  9, batch     5 | loss: 1.3653272MemoryTrain:  epoch  9, batch     6 | loss: 1.2534968MemoryTrain:  epoch  9, batch     7 | loss: 1.2445748
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    1 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 68.75%,  total acc: 77.08%   [EVAL] batch:    3 | acc: 50.00%,  total acc: 70.31%   [EVAL] batch:    4 | acc: 12.50%,  total acc: 58.75%   [EVAL] batch:    5 | acc: 43.75%,  total acc: 56.25%   [EVAL] batch:    6 | acc: 25.00%,  total acc: 51.79%   [EVAL] batch:    7 | acc: 0.00%,  total acc: 45.31%   
[EVAL] batch:    0 | acc: 18.75%,  total acc: 18.75%   [EVAL] batch:    1 | acc: 18.75%,  total acc: 18.75%   [EVAL] batch:    2 | acc: 12.50%,  total acc: 16.67%   [EVAL] batch:    3 | acc: 12.50%,  total acc: 15.62%   [EVAL] batch:    4 | acc: 12.50%,  total acc: 15.00%   [EVAL] batch:    5 | acc: 25.00%,  total acc: 16.67%   [EVAL] batch:    6 | acc: 62.50%,  total acc: 23.21%   [EVAL] batch:    7 | acc: 68.75%,  total acc: 28.91%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 34.72%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 40.00%   [EVAL] batch:   10 | acc: 68.75%,  total acc: 42.61%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 45.31%   [EVAL] batch:   12 | acc: 37.50%,  total acc: 44.71%   [EVAL] batch:   13 | acc: 43.75%,  total acc: 44.64%   [EVAL] batch:   14 | acc: 81.25%,  total acc: 47.08%   [EVAL] batch:   15 | acc: 75.00%,  total acc: 48.83%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 50.37%   [EVAL] batch:   17 | acc: 75.00%,  total acc: 51.74%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 54.28%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 56.25%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 58.33%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 60.23%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 61.96%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 63.28%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 64.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 66.11%   [EVAL] batch:   26 | acc: 43.75%,  total acc: 65.28%   [EVAL] batch:   27 | acc: 37.50%,  total acc: 64.29%   [EVAL] batch:   28 | acc: 31.25%,  total acc: 63.15%   [EVAL] batch:   29 | acc: 25.00%,  total acc: 61.88%   [EVAL] batch:   30 | acc: 43.75%,  total acc: 61.29%   [EVAL] batch:   31 | acc: 50.00%,  total acc: 60.94%   [EVAL] batch:   32 | acc: 18.75%,  total acc: 59.66%   [EVAL] batch:   33 | acc: 37.50%,  total acc: 59.01%   [EVAL] batch:   34 | acc: 50.00%,  total acc: 58.75%   [EVAL] batch:   35 | acc: 43.75%,  total acc: 58.33%   [EVAL] batch:   36 | acc: 81.25%,  total acc: 58.95%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 59.70%   [EVAL] batch:   38 | acc: 87.50%,  total acc: 60.42%   [EVAL] batch:   39 | acc: 50.00%,  total acc: 60.16%   [EVAL] batch:   40 | acc: 6.25%,  total acc: 58.84%   [EVAL] batch:   41 | acc: 6.25%,  total acc: 57.59%   [EVAL] batch:   42 | acc: 18.75%,  total acc: 56.69%   [EVAL] batch:   43 | acc: 43.75%,  total acc: 56.39%   [EVAL] batch:   44 | acc: 43.75%,  total acc: 56.11%   [EVAL] batch:   45 | acc: 31.25%,  total acc: 55.57%   [EVAL] batch:   46 | acc: 50.00%,  total acc: 55.45%   [EVAL] batch:   47 | acc: 37.50%,  total acc: 55.08%   [EVAL] batch:   48 | acc: 25.00%,  total acc: 54.46%   [EVAL] batch:   49 | acc: 37.50%,  total acc: 54.12%   [EVAL] batch:   50 | acc: 87.50%,  total acc: 54.78%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 55.53%   [EVAL] batch:   52 | acc: 68.75%,  total acc: 55.78%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 56.25%   [EVAL] batch:   54 | acc: 56.25%,  total acc: 56.25%   [EVAL] batch:   55 | acc: 12.50%,  total acc: 55.47%   [EVAL] batch:   56 | acc: 0.00%,  total acc: 54.50%   [EVAL] batch:   57 | acc: 0.00%,  total acc: 53.56%   [EVAL] batch:   58 | acc: 6.25%,  total acc: 52.75%   [EVAL] batch:   59 | acc: 0.00%,  total acc: 51.88%   [EVAL] batch:   60 | acc: 6.25%,  total acc: 51.13%   [EVAL] batch:   61 | acc: 6.25%,  total acc: 50.40%   [EVAL] batch:   62 | acc: 6.25%,  total acc: 49.70%   [EVAL] batch:   63 | acc: 6.25%,  total acc: 49.02%   [EVAL] batch:   64 | acc: 6.25%,  total acc: 48.37%   [EVAL] batch:   65 | acc: 0.00%,  total acc: 47.63%   [EVAL] batch:   66 | acc: 50.00%,  total acc: 47.67%   [EVAL] batch:   67 | acc: 56.25%,  total acc: 47.79%   [EVAL] batch:   68 | acc: 43.75%,  total acc: 47.74%   [EVAL] batch:   69 | acc: 56.25%,  total acc: 47.86%   [EVAL] batch:   70 | acc: 75.00%,  total acc: 48.24%   [EVAL] batch:   71 | acc: 56.25%,  total acc: 48.35%   [EVAL] batch:   72 | acc: 81.25%,  total acc: 48.80%   [EVAL] batch:   73 | acc: 93.75%,  total acc: 49.41%   [EVAL] batch:   74 | acc: 93.75%,  total acc: 50.00%   [EVAL] batch:   75 | acc: 100.00%,  total acc: 50.66%   [EVAL] batch:   76 | acc: 87.50%,  total acc: 51.14%   [EVAL] batch:   77 | acc: 43.75%,  total acc: 51.04%   [EVAL] batch:   78 | acc: 0.00%,  total acc: 50.40%   [EVAL] batch:   79 | acc: 6.25%,  total acc: 49.84%   [EVAL] batch:   80 | acc: 6.25%,  total acc: 49.31%   [EVAL] batch:   81 | acc: 6.25%,  total acc: 48.78%   [EVAL] batch:   82 | acc: 0.00%,  total acc: 48.19%   [EVAL] batch:   83 | acc: 6.25%,  total acc: 47.69%   [EVAL] batch:   84 | acc: 12.50%,  total acc: 47.28%   [EVAL] batch:   85 | acc: 25.00%,  total acc: 47.02%   [EVAL] batch:   86 | acc: 12.50%,  total acc: 46.62%   [EVAL] batch:   87 | acc: 75.00%,  total acc: 46.95%   [EVAL] batch:   88 | acc: 75.00%,  total acc: 47.26%   [EVAL] batch:   89 | acc: 25.00%,  total acc: 47.01%   [EVAL] batch:   90 | acc: 75.00%,  total acc: 47.32%   [EVAL] batch:   91 | acc: 68.75%,  total acc: 47.55%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 48.12%   [EVAL] batch:   93 | acc: 100.00%,  total acc: 48.67%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 49.21%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 49.74%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 50.26%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 50.77%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 51.26%   [EVAL] batch:   99 | acc: 93.75%,  total acc: 51.69%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 52.17%   [EVAL] batch:  101 | acc: 100.00%,  total acc: 52.63%   [EVAL] batch:  102 | acc: 100.00%,  total acc: 53.09%   [EVAL] batch:  103 | acc: 100.00%,  total acc: 53.55%   [EVAL] batch:  104 | acc: 100.00%,  total acc: 53.99%   [EVAL] batch:  105 | acc: 100.00%,  total acc: 54.42%   [EVAL] batch:  106 | acc: 87.50%,  total acc: 54.73%   [EVAL] batch:  107 | acc: 31.25%,  total acc: 54.51%   [EVAL] batch:  108 | acc: 37.50%,  total acc: 54.36%   [EVAL] batch:  109 | acc: 93.75%,  total acc: 54.72%   [EVAL] batch:  110 | acc: 68.75%,  total acc: 54.84%   [EVAL] batch:  111 | acc: 87.50%,  total acc: 55.13%   [EVAL] batch:  112 | acc: 56.25%,  total acc: 55.14%   [EVAL] batch:  113 | acc: 31.25%,  total acc: 54.93%   [EVAL] batch:  114 | acc: 25.00%,  total acc: 54.67%   [EVAL] batch:  115 | acc: 12.50%,  total acc: 54.31%   [EVAL] batch:  116 | acc: 25.00%,  total acc: 54.06%   [EVAL] batch:  117 | acc: 12.50%,  total acc: 53.71%   [EVAL] batch:  118 | acc: 50.00%,  total acc: 53.68%   [EVAL] batch:  119 | acc: 93.75%,  total acc: 54.01%   [EVAL] batch:  120 | acc: 87.50%,  total acc: 54.29%   [EVAL] batch:  121 | acc: 93.75%,  total acc: 54.61%   [EVAL] batch:  122 | acc: 81.25%,  total acc: 54.83%   [EVAL] batch:  123 | acc: 81.25%,  total acc: 55.04%   [EVAL] batch:  124 | acc: 68.75%,  total acc: 55.15%   [EVAL] batch:  125 | acc: 37.50%,  total acc: 55.01%   [EVAL] batch:  126 | acc: 93.75%,  total acc: 55.31%   [EVAL] batch:  127 | acc: 68.75%,  total acc: 55.42%   [EVAL] batch:  128 | acc: 68.75%,  total acc: 55.52%   [EVAL] batch:  129 | acc: 37.50%,  total acc: 55.38%   [EVAL] batch:  130 | acc: 25.00%,  total acc: 55.15%   [EVAL] batch:  131 | acc: 37.50%,  total acc: 55.02%   [EVAL] batch:  132 | acc: 12.50%,  total acc: 54.70%   
cur_acc:  ['0.8523', '0.7692', '0.7542', '0.4801', '0.7778', '0.9241', '0.5402', '0.4531']
his_acc:  ['0.8523', '0.8125', '0.7000', '0.6713', '0.5865', '0.5804', '0.5699', '0.5470']
--------Round  5
seed:  600
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/test.pkl
Task_order: [7 2 0 1 6 3 4 5]
prepared data!
CurrentTrain: epoch  0, batch     0 | loss: 13.4602146CurrentTrain: epoch  0, batch     1 | loss: 13.2798357CurrentTrain: epoch  0, batch     2 | loss: 13.1064606CurrentTrain: epoch  0, batch     3 | loss: 12.6541719CurrentTrain: epoch  0, batch     4 | loss: 12.6716728CurrentTrain: epoch  0, batch     5 | loss: 12.5686417CurrentTrain: epoch  0, batch     6 | loss: 12.5820866CurrentTrain: epoch  0, batch     7 | loss: 12.4722404CurrentTrain: epoch  0, batch     8 | loss: 12.3666391CurrentTrain: epoch  0, batch     9 | loss: 11.8687134CurrentTrain: epoch  0, batch    10 | loss: 12.1349945CurrentTrain: epoch  0, batch    11 | loss: 11.8884449CurrentTrain: epoch  0, batch    12 | loss: 11.9371262CurrentTrain: epoch  0, batch    13 | loss: 11.8165979CurrentTrain: epoch  0, batch    14 | loss: 11.8169231CurrentTrain: epoch  0, batch    15 | loss: 11.5024414CurrentTrain: epoch  0, batch    16 | loss: 11.8183651CurrentTrain: epoch  0, batch    17 | loss: 11.4647541CurrentTrain: epoch  0, batch    18 | loss: 11.3414021CurrentTrain: epoch  0, batch    19 | loss: 11.1946983CurrentTrain: epoch  0, batch    20 | loss: 11.7259502CurrentTrain: epoch  0, batch    21 | loss: 11.5849342CurrentTrain: epoch  0, batch    22 | loss: 11.9806118CurrentTrain: epoch  0, batch    23 | loss: 11.4265471CurrentTrain: epoch  0, batch    24 | loss: 11.1953630CurrentTrain: epoch  0, batch    25 | loss: 11.2786884CurrentTrain: epoch  0, batch    26 | loss: 11.0043564CurrentTrain: epoch  0, batch    27 | loss: 11.0916119CurrentTrain: epoch  0, batch    28 | loss: 10.7870674CurrentTrain: epoch  0, batch    29 | loss: 10.9998083CurrentTrain: epoch  0, batch    30 | loss: 10.9941998CurrentTrain: epoch  0, batch    31 | loss: 10.7896643CurrentTrain: epoch  0, batch    32 | loss: 10.5749722CurrentTrain: epoch  0, batch    33 | loss: 10.8748722CurrentTrain: epoch  0, batch    34 | loss: 11.0279140CurrentTrain: epoch  0, batch    35 | loss: 11.1098309CurrentTrain: epoch  0, batch    36 | loss: 10.4532413CurrentTrain: epoch  0, batch    37 | loss: 10.8133039CurrentTrain: epoch  1, batch     0 | loss: 10.4606190CurrentTrain: epoch  1, batch     1 | loss: 10.5093956CurrentTrain: epoch  1, batch     2 | loss: 10.3482342CurrentTrain: epoch  1, batch     3 | loss: 10.0121498CurrentTrain: epoch  1, batch     4 | loss: 10.1837854CurrentTrain: epoch  1, batch     5 | loss: 10.0060158CurrentTrain: epoch  1, batch     6 | loss: 10.3838434CurrentTrain: epoch  1, batch     7 | loss: 10.0166378CurrentTrain: epoch  1, batch     8 | loss: 10.0203762CurrentTrain: epoch  1, batch     9 | loss: 9.4750605CurrentTrain: epoch  1, batch    10 | loss: 10.0134964CurrentTrain: epoch  1, batch    11 | loss: 10.0366058CurrentTrain: epoch  1, batch    12 | loss: 10.2252064CurrentTrain: epoch  1, batch    13 | loss: 9.9850979CurrentTrain: epoch  1, batch    14 | loss: 9.4333439CurrentTrain: epoch  1, batch    15 | loss: 9.8325958CurrentTrain: epoch  1, batch    16 | loss: 9.9774065CurrentTrain: epoch  1, batch    17 | loss: 9.5582943CurrentTrain: epoch  1, batch    18 | loss: 9.5071230CurrentTrain: epoch  1, batch    19 | loss: 9.5074120CurrentTrain: epoch  1, batch    20 | loss: 9.3607454CurrentTrain: epoch  1, batch    21 | loss: 9.5316525CurrentTrain: epoch  1, batch    22 | loss: 8.9885397CurrentTrain: epoch  1, batch    23 | loss: 9.2045507CurrentTrain: epoch  1, batch    24 | loss: 9.3057022CurrentTrain: epoch  1, batch    25 | loss: 8.9466000CurrentTrain: epoch  1, batch    26 | loss: 9.2261314CurrentTrain: epoch  1, batch    27 | loss: 8.6255121CurrentTrain: epoch  1, batch    28 | loss: 9.7281685CurrentTrain: epoch  1, batch    29 | loss: 9.3728275CurrentTrain: epoch  1, batch    30 | loss: 9.4045277CurrentTrain: epoch  1, batch    31 | loss: 9.0653076CurrentTrain: epoch  1, batch    32 | loss: 8.8335800CurrentTrain: epoch  1, batch    33 | loss: 8.2275486CurrentTrain: epoch  1, batch    34 | loss: 8.8162088CurrentTrain: epoch  1, batch    35 | loss: 8.9592323CurrentTrain: epoch  1, batch    36 | loss: 8.7607031CurrentTrain: epoch  1, batch    37 | loss: 9.0953341CurrentTrain: epoch  2, batch     0 | loss: 8.3130798CurrentTrain: epoch  2, batch     1 | loss: 8.8265285CurrentTrain: epoch  2, batch     2 | loss: 8.4957676CurrentTrain: epoch  2, batch     3 | loss: 8.2707253CurrentTrain: epoch  2, batch     4 | loss: 8.6630821CurrentTrain: epoch  2, batch     5 | loss: 8.0060253CurrentTrain: epoch  2, batch     6 | loss: 7.8353133CurrentTrain: epoch  2, batch     7 | loss: 8.5811481CurrentTrain: epoch  2, batch     8 | loss: 8.0928135CurrentTrain: epoch  2, batch     9 | loss: 8.1793232CurrentTrain: epoch  2, batch    10 | loss: 8.4493198CurrentTrain: epoch  2, batch    11 | loss: 8.5308266CurrentTrain: epoch  2, batch    12 | loss: 8.2075472CurrentTrain: epoch  2, batch    13 | loss: 7.9092269CurrentTrain: epoch  2, batch    14 | loss: 8.5725613CurrentTrain: epoch  2, batch    15 | loss: 7.8555269CurrentTrain: epoch  2, batch    16 | loss: 8.2144909CurrentTrain: epoch  2, batch    17 | loss: 8.6839180CurrentTrain: epoch  2, batch    18 | loss: 8.4041920CurrentTrain: epoch  2, batch    19 | loss: 9.3973064CurrentTrain: epoch  2, batch    20 | loss: 7.6692281CurrentTrain: epoch  2, batch    21 | loss: 8.4803562CurrentTrain: epoch  2, batch    22 | loss: 8.7662220CurrentTrain: epoch  2, batch    23 | loss: 7.6186714CurrentTrain: epoch  2, batch    24 | loss: 8.9839201CurrentTrain: epoch  2, batch    25 | loss: 7.5265388CurrentTrain: epoch  2, batch    26 | loss: 7.3964496CurrentTrain: epoch  2, batch    27 | loss: 8.4900064CurrentTrain: epoch  2, batch    28 | loss: 8.4287491CurrentTrain: epoch  2, batch    29 | loss: 7.4010129CurrentTrain: epoch  2, batch    30 | loss: 8.3712292CurrentTrain: epoch  2, batch    31 | loss: 7.7124305CurrentTrain: epoch  2, batch    32 | loss: 8.1882744CurrentTrain: epoch  2, batch    33 | loss: 7.8865881CurrentTrain: epoch  2, batch    34 | loss: 8.2965927CurrentTrain: epoch  2, batch    35 | loss: 7.2014747CurrentTrain: epoch  2, batch    36 | loss: 7.2589726CurrentTrain: epoch  2, batch    37 | loss: 8.6801748CurrentTrain: epoch  3, batch     0 | loss: 7.3734980CurrentTrain: epoch  3, batch     1 | loss: 7.1395874CurrentTrain: epoch  3, batch     2 | loss: 7.3011956CurrentTrain: epoch  3, batch     3 | loss: 7.6600437CurrentTrain: epoch  3, batch     4 | loss: 7.5905938CurrentTrain: epoch  3, batch     5 | loss: 7.9532948CurrentTrain: epoch  3, batch     6 | loss: 7.6943779CurrentTrain: epoch  3, batch     7 | loss: 7.8213263CurrentTrain: epoch  3, batch     8 | loss: 7.4633818CurrentTrain: epoch  3, batch     9 | loss: 7.7592993CurrentTrain: epoch  3, batch    10 | loss: 7.3289547CurrentTrain: epoch  3, batch    11 | loss: 7.1931868CurrentTrain: epoch  3, batch    12 | loss: 7.6521635CurrentTrain: epoch  3, batch    13 | loss: 7.0868974CurrentTrain: epoch  3, batch    14 | loss: 7.9597511CurrentTrain: epoch  3, batch    15 | loss: 6.6510944CurrentTrain: epoch  3, batch    16 | loss: 7.3350134CurrentTrain: epoch  3, batch    17 | loss: 8.0875549CurrentTrain: epoch  3, batch    18 | loss: 7.3205271CurrentTrain: epoch  3, batch    19 | loss: 6.9622717CurrentTrain: epoch  3, batch    20 | loss: 7.6998982CurrentTrain: epoch  3, batch    21 | loss: 7.9224930CurrentTrain: epoch  3, batch    22 | loss: 7.7506046CurrentTrain: epoch  3, batch    23 | loss: 7.7427769CurrentTrain: epoch  3, batch    24 | loss: 6.8949256CurrentTrain: epoch  3, batch    25 | loss: 8.0457764CurrentTrain: epoch  3, batch    26 | loss: 7.0989227CurrentTrain: epoch  3, batch    27 | loss: 8.2337942CurrentTrain: epoch  3, batch    28 | loss: 7.0293355CurrentTrain: epoch  3, batch    29 | loss: 7.6159000CurrentTrain: epoch  3, batch    30 | loss: 5.9654226CurrentTrain: epoch  3, batch    31 | loss: 6.9520073CurrentTrain: epoch  3, batch    32 | loss: 7.5006466CurrentTrain: epoch  3, batch    33 | loss: 7.1483297CurrentTrain: epoch  3, batch    34 | loss: 7.2721176CurrentTrain: epoch  3, batch    35 | loss: 8.1432953CurrentTrain: epoch  3, batch    36 | loss: 6.9915924CurrentTrain: epoch  3, batch    37 | loss: 7.8754668CurrentTrain: epoch  4, batch     0 | loss: 6.3019657CurrentTrain: epoch  4, batch     1 | loss: 8.3193035CurrentTrain: epoch  4, batch     2 | loss: 7.4725733CurrentTrain: epoch  4, batch     3 | loss: 6.5878234CurrentTrain: epoch  4, batch     4 | loss: 6.8846121CurrentTrain: epoch  4, batch     5 | loss: 7.2734509CurrentTrain: epoch  4, batch     6 | loss: 6.9899306CurrentTrain: epoch  4, batch     7 | loss: 6.9984207CurrentTrain: epoch  4, batch     8 | loss: 6.6452756CurrentTrain: epoch  4, batch     9 | loss: 7.3447781CurrentTrain: epoch  4, batch    10 | loss: 7.1237478CurrentTrain: epoch  4, batch    11 | loss: 6.4402180CurrentTrain: epoch  4, batch    12 | loss: 6.8950691CurrentTrain: epoch  4, batch    13 | loss: 6.4794779CurrentTrain: epoch  4, batch    14 | loss: 6.7704077CurrentTrain: epoch  4, batch    15 | loss: 6.5186071CurrentTrain: epoch  4, batch    16 | loss: 6.2693701CurrentTrain: epoch  4, batch    17 | loss: 6.0976343CurrentTrain: epoch  4, batch    18 | loss: 7.0130196CurrentTrain: epoch  4, batch    19 | loss: 7.4581394CurrentTrain: epoch  4, batch    20 | loss: 7.3947082CurrentTrain: epoch  4, batch    21 | loss: 7.1070137CurrentTrain: epoch  4, batch    22 | loss: 7.0670662CurrentTrain: epoch  4, batch    23 | loss: 7.6432538CurrentTrain: epoch  4, batch    24 | loss: 7.1546965CurrentTrain: epoch  4, batch    25 | loss: 7.3936825CurrentTrain: epoch  4, batch    26 | loss: 7.6024160CurrentTrain: epoch  4, batch    27 | loss: 7.2081742CurrentTrain: epoch  4, batch    28 | loss: 7.8430591CurrentTrain: epoch  4, batch    29 | loss: 6.1038642CurrentTrain: epoch  4, batch    30 | loss: 7.0200081CurrentTrain: epoch  4, batch    31 | loss: 6.5279217CurrentTrain: epoch  4, batch    32 | loss: 6.9225559CurrentTrain: epoch  4, batch    33 | loss: 7.3328958CurrentTrain: epoch  4, batch    34 | loss: 6.8909702CurrentTrain: epoch  4, batch    35 | loss: 5.6442418CurrentTrain: epoch  4, batch    36 | loss: 7.4301624CurrentTrain: epoch  4, batch    37 | loss: 6.9592571CurrentTrain: epoch  5, batch     0 | loss: 6.9570465CurrentTrain: epoch  5, batch     1 | loss: 6.3230119CurrentTrain: epoch  5, batch     2 | loss: 6.3977036CurrentTrain: epoch  5, batch     3 | loss: 6.2901821CurrentTrain: epoch  5, batch     4 | loss: 6.7262669CurrentTrain: epoch  5, batch     5 | loss: 6.4863758CurrentTrain: epoch  5, batch     6 | loss: 7.0828924CurrentTrain: epoch  5, batch     7 | loss: 6.9631863CurrentTrain: epoch  5, batch     8 | loss: 6.5025334CurrentTrain: epoch  5, batch     9 | loss: 6.3389969CurrentTrain: epoch  5, batch    10 | loss: 7.3286009CurrentTrain: epoch  5, batch    11 | loss: 6.8110042CurrentTrain: epoch  5, batch    12 | loss: 6.1240311CurrentTrain: epoch  5, batch    13 | loss: 6.2215323CurrentTrain: epoch  5, batch    14 | loss: 6.5045280CurrentTrain: epoch  5, batch    15 | loss: 6.5614114CurrentTrain: epoch  5, batch    16 | loss: 6.1383114CurrentTrain: epoch  5, batch    17 | loss: 6.4736300CurrentTrain: epoch  5, batch    18 | loss: 6.6793580CurrentTrain: epoch  5, batch    19 | loss: 7.0662022CurrentTrain: epoch  5, batch    20 | loss: 6.0264564CurrentTrain: epoch  5, batch    21 | loss: 6.9117169CurrentTrain: epoch  5, batch    22 | loss: 5.9281526CurrentTrain: epoch  5, batch    23 | loss: 6.4181490CurrentTrain: epoch  5, batch    24 | loss: 6.3035493CurrentTrain: epoch  5, batch    25 | loss: 6.4527783CurrentTrain: epoch  5, batch    26 | loss: 6.5726361CurrentTrain: epoch  5, batch    27 | loss: 6.1873007CurrentTrain: epoch  5, batch    28 | loss: 6.2112265CurrentTrain: epoch  5, batch    29 | loss: 6.3337884CurrentTrain: epoch  5, batch    30 | loss: 5.9010868CurrentTrain: epoch  5, batch    31 | loss: 5.6418018CurrentTrain: epoch  5, batch    32 | loss: 6.6839743CurrentTrain: epoch  5, batch    33 | loss: 6.5407968CurrentTrain: epoch  5, batch    34 | loss: 6.1772747CurrentTrain: epoch  5, batch    35 | loss: 6.6383634CurrentTrain: epoch  5, batch    36 | loss: 7.2003665CurrentTrain: epoch  5, batch    37 | loss: 5.8194661CurrentTrain: epoch  6, batch     0 | loss: 5.5808783CurrentTrain: epoch  6, batch     1 | loss: 6.3264475CurrentTrain: epoch  6, batch     2 | loss: 6.4423141CurrentTrain: epoch  6, batch     3 | loss: 5.6997485CurrentTrain: epoch  6, batch     4 | loss: 5.4851456CurrentTrain: epoch  6, batch     5 | loss: 6.2243423CurrentTrain: epoch  6, batch     6 | loss: 5.6287603CurrentTrain: epoch  6, batch     7 | loss: 6.0104771CurrentTrain: epoch  6, batch     8 | loss: 5.6918302CurrentTrain: epoch  6, batch     9 | loss: 6.1628952CurrentTrain: epoch  6, batch    10 | loss: 6.0265331CurrentTrain: epoch  6, batch    11 | loss: 5.6570382CurrentTrain: epoch  6, batch    12 | loss: 6.0802464CurrentTrain: epoch  6, batch    13 | loss: 7.1293983CurrentTrain: epoch  6, batch    14 | loss: 6.4135504CurrentTrain: epoch  6, batch    15 | loss: 6.0660105CurrentTrain: epoch  6, batch    16 | loss: 5.7908545CurrentTrain: epoch  6, batch    17 | loss: 6.2615967CurrentTrain: epoch  6, batch    18 | loss: 6.0113850CurrentTrain: epoch  6, batch    19 | loss: 5.8686562CurrentTrain: epoch  6, batch    20 | loss: 6.3388634CurrentTrain: epoch  6, batch    21 | loss: 6.3842020CurrentTrain: epoch  6, batch    22 | loss: 5.9170370CurrentTrain: epoch  6, batch    23 | loss: 5.7544403CurrentTrain: epoch  6, batch    24 | loss: 5.8967037CurrentTrain: epoch  6, batch    25 | loss: 7.0932894CurrentTrain: epoch  6, batch    26 | loss: 7.2344842CurrentTrain: epoch  6, batch    27 | loss: 5.9282489CurrentTrain: epoch  6, batch    28 | loss: 5.8037004CurrentTrain: epoch  6, batch    29 | loss: 6.3330317CurrentTrain: epoch  6, batch    30 | loss: 6.0261583CurrentTrain: epoch  6, batch    31 | loss: 5.8880181CurrentTrain: epoch  6, batch    32 | loss: 6.8779554CurrentTrain: epoch  6, batch    33 | loss: 6.1942844CurrentTrain: epoch  6, batch    34 | loss: 6.4642758CurrentTrain: epoch  6, batch    35 | loss: 6.5718150CurrentTrain: epoch  6, batch    36 | loss: 6.5813818CurrentTrain: epoch  6, batch    37 | loss: 5.8841896CurrentTrain: epoch  7, batch     0 | loss: 6.2402411CurrentTrain: epoch  7, batch     1 | loss: 5.1447077CurrentTrain: epoch  7, batch     2 | loss: 6.4716611CurrentTrain: epoch  7, batch     3 | loss: 5.9920235CurrentTrain: epoch  7, batch     4 | loss: 5.5604839CurrentTrain: epoch  7, batch     5 | loss: 5.6399746CurrentTrain: epoch  7, batch     6 | loss: 5.2703295CurrentTrain: epoch  7, batch     7 | loss: 6.6302609CurrentTrain: epoch  7, batch     8 | loss: 6.5228748CurrentTrain: epoch  7, batch     9 | loss: 5.7549100CurrentTrain: epoch  7, batch    10 | loss: 5.9045029CurrentTrain: epoch  7, batch    11 | loss: 5.5822487CurrentTrain: epoch  7, batch    12 | loss: 5.4968624CurrentTrain: epoch  7, batch    13 | loss: 5.5964894CurrentTrain: epoch  7, batch    14 | loss: 5.1395030CurrentTrain: epoch  7, batch    15 | loss: 5.4003143CurrentTrain: epoch  7, batch    16 | loss: 5.5321870CurrentTrain: epoch  7, batch    17 | loss: 5.8799706CurrentTrain: epoch  7, batch    18 | loss: 5.3926773CurrentTrain: epoch  7, batch    19 | loss: 5.2300224CurrentTrain: epoch  7, batch    20 | loss: 5.5729465CurrentTrain: epoch  7, batch    21 | loss: 5.3475914CurrentTrain: epoch  7, batch    22 | loss: 5.6059322CurrentTrain: epoch  7, batch    23 | loss: 6.0484929CurrentTrain: epoch  7, batch    24 | loss: 6.0314159CurrentTrain: epoch  7, batch    25 | loss: 5.2877102CurrentTrain: epoch  7, batch    26 | loss: 5.7867208CurrentTrain: epoch  7, batch    27 | loss: 5.7708554CurrentTrain: epoch  7, batch    28 | loss: 5.3176193CurrentTrain: epoch  7, batch    29 | loss: 5.4335785CurrentTrain: epoch  7, batch    30 | loss: 5.3028979CurrentTrain: epoch  7, batch    31 | loss: 5.3774362CurrentTrain: epoch  7, batch    32 | loss: 6.3708496CurrentTrain: epoch  7, batch    33 | loss: 5.1164498CurrentTrain: epoch  7, batch    34 | loss: 5.9016867CurrentTrain: epoch  7, batch    35 | loss: 5.5434656CurrentTrain: epoch  7, batch    36 | loss: 5.1607332CurrentTrain: epoch  7, batch    37 | loss: 6.1980362CurrentTrain: epoch  8, batch     0 | loss: 5.4117346CurrentTrain: epoch  8, batch     1 | loss: 5.2775235CurrentTrain: epoch  8, batch     2 | loss: 5.6720567CurrentTrain: epoch  8, batch     3 | loss: 5.2755809CurrentTrain: epoch  8, batch     4 | loss: 5.2863641CurrentTrain: epoch  8, batch     5 | loss: 5.6174641CurrentTrain: epoch  8, batch     6 | loss: 5.0874100CurrentTrain: epoch  8, batch     7 | loss: 5.4636703CurrentTrain: epoch  8, batch     8 | loss: 5.3016849CurrentTrain: epoch  8, batch     9 | loss: 5.4379840CurrentTrain: epoch  8, batch    10 | loss: 5.5070672CurrentTrain: epoch  8, batch    11 | loss: 5.4252839CurrentTrain: epoch  8, batch    12 | loss: 5.4005494CurrentTrain: epoch  8, batch    13 | loss: 5.2733212CurrentTrain: epoch  8, batch    14 | loss: 5.3467436CurrentTrain: epoch  8, batch    15 | loss: 5.2787218CurrentTrain: epoch  8, batch    16 | loss: 5.1931362CurrentTrain: epoch  8, batch    17 | loss: 5.1767592CurrentTrain: epoch  8, batch    18 | loss: 5.3745489CurrentTrain: epoch  8, batch    19 | loss: 6.2035880CurrentTrain: epoch  8, batch    20 | loss: 5.2833052CurrentTrain: epoch  8, batch    21 | loss: 5.4390926CurrentTrain: epoch  8, batch    22 | loss: 5.2532139CurrentTrain: epoch  8, batch    23 | loss: 5.2405472CurrentTrain: epoch  8, batch    24 | loss: 5.3910937CurrentTrain: epoch  8, batch    25 | loss: 4.9025984CurrentTrain: epoch  8, batch    26 | loss: 5.9981799CurrentTrain: epoch  8, batch    27 | loss: 5.1991463CurrentTrain: epoch  8, batch    28 | loss: 5.8521752CurrentTrain: epoch  8, batch    29 | loss: 5.1491828CurrentTrain: epoch  8, batch    30 | loss: 5.3376408CurrentTrain: epoch  8, batch    31 | loss: 5.7585449CurrentTrain: epoch  8, batch    32 | loss: 5.0975876CurrentTrain: epoch  8, batch    33 | loss: 5.2302723CurrentTrain: epoch  8, batch    34 | loss: 5.3555455CurrentTrain: epoch  8, batch    35 | loss: 5.6557446CurrentTrain: epoch  8, batch    36 | loss: 5.1829700CurrentTrain: epoch  8, batch    37 | loss: 5.3316007CurrentTrain: epoch  9, batch     0 | loss: 4.9488859CurrentTrain: epoch  9, batch     1 | loss: 5.1224256CurrentTrain: epoch  9, batch     2 | loss: 5.1996508CurrentTrain: epoch  9, batch     3 | loss: 6.6848693CurrentTrain: epoch  9, batch     4 | loss: 5.3127651CurrentTrain: epoch  9, batch     5 | loss: 5.0109978CurrentTrain: epoch  9, batch     6 | loss: 5.1885829CurrentTrain: epoch  9, batch     7 | loss: 5.2918897CurrentTrain: epoch  9, batch     8 | loss: 5.2531152CurrentTrain: epoch  9, batch     9 | loss: 5.0085573CurrentTrain: epoch  9, batch    10 | loss: 5.6430464CurrentTrain: epoch  9, batch    11 | loss: 5.1748419CurrentTrain: epoch  9, batch    12 | loss: 5.3396673CurrentTrain: epoch  9, batch    13 | loss: 5.2108707CurrentTrain: epoch  9, batch    14 | loss: 5.5250897CurrentTrain: epoch  9, batch    15 | loss: 5.1455326CurrentTrain: epoch  9, batch    16 | loss: 5.4008150CurrentTrain: epoch  9, batch    17 | loss: 4.9745178CurrentTrain: epoch  9, batch    18 | loss: 4.9823360CurrentTrain: epoch  9, batch    19 | loss: 5.0974264CurrentTrain: epoch  9, batch    20 | loss: 5.2212110CurrentTrain: epoch  9, batch    21 | loss: 5.0792217CurrentTrain: epoch  9, batch    22 | loss: 5.3402729CurrentTrain: epoch  9, batch    23 | loss: 5.1293244CurrentTrain: epoch  9, batch    24 | loss: 6.2069187CurrentTrain: epoch  9, batch    25 | loss: 5.1487589CurrentTrain: epoch  9, batch    26 | loss: 5.9621086CurrentTrain: epoch  9, batch    27 | loss: 5.1587982CurrentTrain: epoch  9, batch    28 | loss: 5.2533274CurrentTrain: epoch  9, batch    29 | loss: 6.0863247CurrentTrain: epoch  9, batch    30 | loss: 5.3042936CurrentTrain: epoch  9, batch    31 | loss: 4.9476724CurrentTrain: epoch  9, batch    32 | loss: 5.3436608CurrentTrain: epoch  9, batch    33 | loss: 5.3340483CurrentTrain: epoch  9, batch    34 | loss: 5.0737386CurrentTrain: epoch  9, batch    35 | loss: 5.3980980CurrentTrain: epoch  9, batch    36 | loss: 4.9491510CurrentTrain: epoch  9, batch    37 | loss: 5.4863019
#### person countries of residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person countries of residence" means a person is living in a country.
Here is an example:
Relation: person countries of residence
Context: iran 's top nuclear negotiator , ali larijani , was asked by state tv whether the country had started converting into gas a second batch of uranium , as it had planned to do at its isfahan nuclear facility .
Head Entity: ali larijani
Tail Entity: iran
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person countries of residence":
Relation: person countries of residence  
Context: After years of living in the United States, the renowned artist decided to return to her homeland, where she felt a deep connection to her roots.  
Head Entity: the renowned artist  
Tail Entity: her homeland  

Relation: person countries of residence  
Context: Following his successful career in Europe, the famous chef opened a new restaurant in his native Italy, showcasing traditional dishes that reflect his upbringing.  
Head Entity: the famous chef  
Tail Entity: Italy  
#### organization top members employees ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization top members employees" means an organization 's employer is a person.
Here is an example:
Relation: organization top members employees
Context: `` decriminalization is a positive step for mexican democracy , '' said gonzalo marroquin , president of the press freedom committee of the inter american press association .
Head Entity: inter american press association
Tail Entity: gonzalo marroquin
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization top members employees":
Relation: organization top members employees  
Context: "We are thrilled to announce that Sarah Johnson, our new CEO, will be leading the company into its next phase of growth," said the board of directors of Tech Innovations Inc.  
Head Entity: Tech Innovations Inc.  
Tail Entity: Sarah Johnson  

Relation: organization top members employees  
Context: "As the chief financial officer, Mark Thompson has played a crucial role in our success," stated the management team of Global Finance Corp.  
Head Entity: Global Finance Corp.  
Tail Entity: Mark Thompson  
#### organization member of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization member of" means an organization is the member of an organization.
Here is an example:
Relation: organization member of
Context: south africa 1-5 leyds cnr biccard streets cosatu 's concern has been that the scorpions ' existence as a branch of the national directorate of public prosecutions blurred the necessary separation of functions between those investigating crime and those prosecuting the criminals .
Head Entity: scorpions
Tail Entity: national directorate of public prosecutions
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization member of":
Relation: organization member of  
Context: The International Federation of Red Cross and Red Crescent Societies (IFRC) is a global humanitarian organization that coordinates the activities of national Red Cross and Red Crescent societies around the world.  
Head Entity: International Federation of Red Cross and Red Crescent Societies  
Tail Entity: national Red Cross and Red Crescent societies  

Relation: organization member of  
Context: The European Union is a political and economic union of member states that are located primarily in Europe, working together to promote peace and stability in the region.  
Head Entity: European Union  
Tail Entity: member states  
#### person origin ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person origin" means a person 's nationality is a country.
Here is an example:
Relation: person origin
Context: since the couple 's divorce , aniston has been romantically linked with actor vince vaughn , british model paul sculfor , actor orlando bloom and singer john mayer .
Head Entity: paul sculfor
Tail Entity: british
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person origin":
Relation: person origin  
Context: The renowned physicist Albert Einstein was born in Germany and later became a Swiss citizen, reflecting his deep ties to both countries.  
Head Entity: Albert Einstein  
Tail Entity: Germany  

Relation: person origin  
Context: The famous author Chimamanda Ngozi Adichie often speaks about her Nigerian heritage and how it influences her writing.  
Head Entity: Chimamanda Ngozi Adichie  
Tail Entity: Nigeria  
#### person title ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person title" means a person 's title is a title.
Here is an example:
Relation: person title
Context: earlier , in jerusalem , he spoke at the state funeral for the city 's fabled former mayor , teddy kollek , who died tuesday at 95 and was buried in the area of the mount herzl cemetery reserved for israel 's leaders .
Head Entity: teddy kollek
Tail Entity: mayor
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person title":
Relation: person title  
Context: During the award ceremony, the renowned scientist was honored for his groundbreaking research in genetics, and the audience celebrated the achievements of Dr. Jane Smith, who has been a leading figure in the field.  
Head Entity: Dr. Jane Smith  
Tail Entity: scientist  

Relation: person title  
Context: At the annual conference, the keynote speaker, a prominent author, captivated the audience with her insights on modern literature, and many attendees were eager to hear from Professor Emily Johnson, a celebrated novelist.  
Head Entity: Professor Emily Johnson  
Tail Entity: novelist  
#### organization country of headquarters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization country of headquarters" means an organization is located in a country.
Here is an example:
Relation: organization country of headquarters
Context: volkswagen has two joint ventures in china -- one with the state - owned china faw group , or faw-volkswagen automobile co. , and one with saic motor corp. , or shanghai volkswagen automotive co. .
Head Entity: china faw group
Tail Entity: china
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization country of headquarters":
Relation: organization country of headquarters  
Context: samsung electronics is a global leader in technology and is headquartered in south korea, where it has established a strong presence in the electronics market.  
Head Entity: samsung electronics  
Tail Entity: south korea  

Relation: organization country of headquarters  
Context: the headquarters of nestlé is located in switzerland, where the company was founded and has maintained its global operations.  
Head Entity: nestlé  
Tail Entity: switzerland  
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 79.17%   [EVAL] batch:    3 | acc: 62.50%,  total acc: 75.00%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 76.25%   [EVAL] batch:    5 | acc: 68.75%,  total acc: 75.00%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 77.68%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 80.47%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 82.64%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 83.75%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 85.23%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 85.94%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 86.54%   [EVAL] batch:   13 | acc: 75.00%,  total acc: 85.71%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 85.00%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 83.20%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 82.72%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 81.94%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 80.92%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 82.14%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 82.95%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 83.70%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 84.38%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 85.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 85.58%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 85.88%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 86.38%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 86.85%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 86.67%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 86.69%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 86.91%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 85.42%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 79.17%   [EVAL] batch:    3 | acc: 62.50%,  total acc: 75.00%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 76.25%   [EVAL] batch:    5 | acc: 68.75%,  total acc: 75.00%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 77.68%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 80.47%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 82.64%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 83.75%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 85.23%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 85.94%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 86.54%   [EVAL] batch:   13 | acc: 75.00%,  total acc: 85.71%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 85.00%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 83.20%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 82.72%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 81.94%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 80.92%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 82.14%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 82.95%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 83.70%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 84.38%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 85.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 85.58%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 85.88%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 86.38%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 86.85%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 86.67%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 86.69%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 86.91%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 85.42%   
cur_acc:  ['0.8542']
his_acc:  ['0.8542']
CurrentTrain: epoch  0, batch     0 | loss: 6.0354567CurrentTrain: epoch  0, batch     1 | loss: 5.9542727CurrentTrain: epoch  1, batch     0 | loss: 5.5753489CurrentTrain: epoch  1, batch     1 | loss: 5.3946810CurrentTrain: epoch  2, batch     0 | loss: 4.9959154CurrentTrain: epoch  2, batch     1 | loss: 4.4339085CurrentTrain: epoch  3, batch     0 | loss: 4.6583691CurrentTrain: epoch  3, batch     1 | loss: 4.2958488CurrentTrain: epoch  4, batch     0 | loss: 4.5649519CurrentTrain: epoch  4, batch     1 | loss: 3.9865451CurrentTrain: epoch  5, batch     0 | loss: 3.8551812CurrentTrain: epoch  5, batch     1 | loss: 3.6298909CurrentTrain: epoch  6, batch     0 | loss: 3.9170003CurrentTrain: epoch  6, batch     1 | loss: 2.7723114CurrentTrain: epoch  7, batch     0 | loss: 3.6502051CurrentTrain: epoch  7, batch     1 | loss: 3.3191311CurrentTrain: epoch  8, batch     0 | loss: 3.2547865CurrentTrain: epoch  8, batch     1 | loss: 3.2308042CurrentTrain: epoch  9, batch     0 | loss: 3.0829716CurrentTrain: epoch  9, batch     1 | loss: 2.8405733
#### person cities of residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person cities of residence" means a person is living in a city.
Here is an example:
Relation: person cities of residence
Context: tv-idol-johns -- atlanta -- `` american idol '' finalist michael johns moved to los angeles several years ago , but his heart is still in atlanta .
Head Entity: his
Tail Entity: atlanta
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person cities of residence":
Relation: person cities of residence  
Context: after years of living in new york city, the famous author decided to return to her hometown of boston, where she feels most at home.  
Head Entity: she  
Tail Entity: boston  

Relation: person cities of residence  
Context: despite being born in san francisco, the tech entrepreneur has spent most of his adult life in silicon valley, where he built his successful startup.  
Head Entity: he  
Tail Entity: silicon valley  
#### person schools attended ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person schools attended" means a person 's school is an organization.
Here is an example:
Relation: person schools attended
Context: after world war ii , he attended the university of southern california , where he became editor of a college magazine .
Head Entity: he
Tail Entity: university of southern california
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person schools attended":
Relation: person schools attended  
Context: She graduated from Harvard University with a degree in psychology before pursuing her career in clinical research.  
Head Entity: She  
Tail Entity: Harvard University  

Relation: person schools attended  
Context: After completing his high school education, John enrolled at Stanford University to study computer science.  
Head Entity: John  
Tail Entity: Stanford University  
#### person country of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person country of death" means a person was died in a country.
Here is an example:
Relation: person country of death
Context: u.s. rep. parren mitchell , founding member of congressional black caucus , dies at 85
Head Entity: parren mitchell
Tail Entity: u.s.
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person country of death":
Relation: person country of death  
Context: renowned physicist stephen hawking passed away in cambridge, england at the age of 76  
Head Entity: stephen hawking  
Tail Entity: england  

Relation: person country of death  
Context: famous author gabriel garcia marquez died in mexico city, mexico, leaving behind a legacy of magical realism  
Head Entity: gabriel garcia marquez  
Tail Entity: mexico  
#### person children ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person children" means a person 's children is a person.
Here is an example:
Relation: person children
Context: he is survived by two stepdaughters , barbara a. hammond and brenda l. stevenson ; a stepson , michael a. taylor ; two grandchildren and one great-grandchild .
Head Entity: he
Tail Entity: brenda l. stevenson
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person children":
Relation: person children  
Context: she has three children, including her son, john, and her daughters, emily and sarah, who are both pursuing their studies.  
Head Entity: she  
Tail Entity: emily  

Relation: person children  
Context: after the passing of their father, the siblings, including jake and his sister, lila, took on the responsibility of caring for their younger brother, max.  
Head Entity: jake  
Tail Entity: lila  
#### person charges ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person charges" means a person was charged with an event.
Here is an example:
Relation: person charges
Context: ferrara said he was innocent of limoli 's slaying , but he pleaded guilty in 1992 to murder , along with racketeering charges , under a deal that sent him to prison for 22 years , rather than go to trial and risk a conviction that could lead to life in prison .
Head Entity: ferrara
Tail Entity: racketeering
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person charges":
Relation: person charges  
Context: After a lengthy investigation, the authorities announced that Johnson was charged with embezzlement, a crime that could result in significant prison time if convicted.  
Head Entity: Johnson  
Tail Entity: embezzlement  

Relation: person charges  
Context: The district attorney confirmed that Smith has been charged with assault following the altercation that took place last month at the downtown bar.  
Head Entity: Smith  
Tail Entity: assault  
Mixup data size:  104
MixupTrain:  epoch  0, batch     0 | loss: 10.2737786MixupTrain:  epoch  0, batch     1 | loss: 9.9506509MixupTrain:  epoch  0, batch     2 | loss: 10.1329198MixupTrain:  epoch  0, batch     3 | loss: 8.6098342MixupTrain:  epoch  0, batch     4 | loss: 7.6137362MixupTrain:  epoch  0, batch     5 | loss: 11.7928452MixupTrain:  epoch  0, batch     6 | loss: 6.3707530
MemoryTrain:  epoch  0, batch     0 | loss: 4.1585064MemoryTrain:  epoch  0, batch     1 | loss: 4.1205211MemoryTrain:  epoch  0, batch     2 | loss: 2.9640052MemoryTrain:  epoch  1, batch     0 | loss: 3.3320143MemoryTrain:  epoch  1, batch     1 | loss: 4.3936214MemoryTrain:  epoch  1, batch     2 | loss: 4.4511395MemoryTrain:  epoch  2, batch     0 | loss: 3.2101634MemoryTrain:  epoch  2, batch     1 | loss: 3.4310882MemoryTrain:  epoch  2, batch     2 | loss: 2.2545836MemoryTrain:  epoch  3, batch     0 | loss: 3.3395367MemoryTrain:  epoch  3, batch     1 | loss: 2.8613634MemoryTrain:  epoch  3, batch     2 | loss: 4.9179072MemoryTrain:  epoch  4, batch     0 | loss: 2.4443240MemoryTrain:  epoch  4, batch     1 | loss: 3.5637927MemoryTrain:  epoch  4, batch     2 | loss: 2.5107138MemoryTrain:  epoch  5, batch     0 | loss: 2.8742578MemoryTrain:  epoch  5, batch     1 | loss: 2.4530048MemoryTrain:  epoch  5, batch     2 | loss: 3.9004529MemoryTrain:  epoch  6, batch     0 | loss: 2.4792953MemoryTrain:  epoch  6, batch     1 | loss: 2.9732718MemoryTrain:  epoch  6, batch     2 | loss: 2.5538228MemoryTrain:  epoch  7, batch     0 | loss: 2.8254476MemoryTrain:  epoch  7, batch     1 | loss: 2.7518783MemoryTrain:  epoch  7, batch     2 | loss: 1.5517855MemoryTrain:  epoch  8, batch     0 | loss: 2.7315574MemoryTrain:  epoch  8, batch     1 | loss: 2.6293435MemoryTrain:  epoch  8, batch     2 | loss: 1.2914308MemoryTrain:  epoch  9, batch     0 | loss: 2.8162479MemoryTrain:  epoch  9, batch     1 | loss: 2.2389374MemoryTrain:  epoch  9, batch     2 | loss: 1.1944842
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 90.62%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 90.62%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 91.25%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 90.62%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 91.07%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 90.62%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 90.97%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 91.88%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 92.05%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 92.19%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 92.31%   [EVAL] batch:   13 | acc: 100.00%,  total acc: 92.86%   [EVAL] batch:   14 | acc: 100.00%,  total acc: 93.33%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 94.12%   [EVAL] batch:   17 | acc: 25.00%,  total acc: 90.28%   
[EVAL] batch:    0 | acc: 31.25%,  total acc: 31.25%   [EVAL] batch:    1 | acc: 31.25%,  total acc: 31.25%   [EVAL] batch:    2 | acc: 50.00%,  total acc: 37.50%   [EVAL] batch:    3 | acc: 25.00%,  total acc: 34.38%   [EVAL] batch:    4 | acc: 50.00%,  total acc: 37.50%   [EVAL] batch:    5 | acc: 43.75%,  total acc: 38.54%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 44.64%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 51.56%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 56.94%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 60.62%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 64.20%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 66.67%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 68.27%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 68.30%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 68.75%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 67.97%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 68.38%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 68.06%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 67.76%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 68.44%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 69.94%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 71.31%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 72.55%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 73.44%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 74.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 75.48%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 76.16%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 77.01%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 77.80%   [EVAL] batch:   29 | acc: 93.75%,  total acc: 78.33%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 79.03%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 79.69%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 80.11%   [EVAL] batch:   33 | acc: 87.50%,  total acc: 80.33%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 80.71%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 80.90%   [EVAL] batch:   36 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 81.41%   [EVAL] batch:   38 | acc: 93.75%,  total acc: 81.73%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 82.19%   [EVAL] batch:   40 | acc: 81.25%,  total acc: 82.16%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 82.59%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 82.99%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 83.10%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 83.47%   [EVAL] batch:   45 | acc: 93.75%,  total acc: 83.70%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 84.04%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 84.38%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 84.69%   [EVAL] batch:   49 | acc: 62.50%,  total acc: 84.25%   
cur_acc:  ['0.8542', '0.9028']
his_acc:  ['0.8542', '0.8425']
CurrentTrain: epoch  0, batch     0 | loss: 6.1309729CurrentTrain: epoch  0, batch     1 | loss: 6.0727425CurrentTrain: epoch  1, batch     0 | loss: 5.5213366CurrentTrain: epoch  1, batch     1 | loss: 4.8872919CurrentTrain: epoch  2, batch     0 | loss: 5.4413681CurrentTrain: epoch  2, batch     1 | loss: 3.9946971CurrentTrain: epoch  3, batch     0 | loss: 4.7122426CurrentTrain: epoch  3, batch     1 | loss: 4.2622123CurrentTrain: epoch  4, batch     0 | loss: 4.5265722CurrentTrain: epoch  4, batch     1 | loss: 2.8358591CurrentTrain: epoch  5, batch     0 | loss: 3.1585472CurrentTrain: epoch  5, batch     1 | loss: 4.1790099CurrentTrain: epoch  6, batch     0 | loss: 3.3343899CurrentTrain: epoch  6, batch     1 | loss: 3.2084634CurrentTrain: epoch  7, batch     0 | loss: 3.2521846CurrentTrain: epoch  7, batch     1 | loss: 3.5146303CurrentTrain: epoch  8, batch     0 | loss: 3.1493027CurrentTrain: epoch  8, batch     1 | loss: 3.5282204CurrentTrain: epoch  9, batch     0 | loss: 3.4086964CurrentTrain: epoch  9, batch     1 | loss: 2.6289029
#### person country of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person country of birth" means a person was born in a country.
Here is an example:
Relation: person country of birth
Context: in testimony by satellite link from germany to a house of representatives ' panel , murat kurnaz recounted his five-year detention , alleging a wide range of torture and abuse .
Head Entity: murat kurnaz
Tail Entity: germany
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person country of birth":
Relation: person country of birth  
Context: During the interview, the renowned artist shared his experiences growing up in the vibrant streets of Barcelona, which he credits as the foundation of his creative journey.  
Head Entity: the renowned artist  
Tail Entity: Barcelona  

Relation: person country of birth  
Context: In her autobiography, the famous scientist details her early life in Tokyo, where she developed a passion for technology and innovation.  
Head Entity: the famous scientist  
Tail Entity: Tokyo  
#### organization website ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization website" means an organization 's website is an url.
Here is an example:
Relation: organization website
Context: http://www.pentaxsailaway.com/ paul j. sail away with pentax sweepstakes
Head Entity: pentax
Tail Entity: http://www.pentaxsailaway.com/
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization website":
Relation: organization website  
Context: Visit the official site at https://www.nike.com for the latest sports gear and apparel.  
Head Entity: Nike  
Tail Entity: https://www.nike.com  

Relation: organization website  
Context: For more information about their services, check out http://www.tesla.com.  
Head Entity: Tesla  
Tail Entity: http://www.tesla.com  
#### organization shareholders ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization shareholders" means an organization was invested by a person.
Here is an example:
Relation: organization shareholders
Context: ------ liberty media acquired a 41 percent stake in directv in late february by exchanging it for a 16 percent stake in news corp plus $ 625 million -lrb- euro402 5 million -rrb- in cash .
Head Entity: directv
Tail Entity: liberty media
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization shareholders":
Relation: organization shareholders  
Context: In 2021, Amazon announced that it had acquired a significant share in Rivian, a startup electric vehicle manufacturer, to bolster its investment in sustainable transportation.  
Head Entity: Rivian  
Tail Entity: Amazon  

Relation: organization shareholders  
Context: Tesla's recent investment in SolarCity has allowed the electric car manufacturer to expand its renewable energy initiatives, with Elon Musk playing a pivotal role in the acquisition.  
Head Entity: SolarCity  
Tail Entity: Tesla  
#### organization dissolved ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization dissolved" means an organization was dissolved in a date.
Here is an example:
Relation: organization dissolved
Context: the tse suffered its worst-ever system crash in november 2005 which paralyzed the world 's second largest bourse and forced it to shelve plan for a listing of its own .
Head Entity: tse
Tail Entity: november 2005
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization dissolved":
Relation: organization dissolved  
Context: the once-prominent tech startup, Innovatech, officially ceased operations in March 2020 after struggling to secure funding.  
Head Entity: Innovatech  
Tail Entity: March 2020  

Relation: organization dissolved  
Context: after years of financial difficulties, the local arts council announced its closure in January 2019, marking the end of its community programs.  
Head Entity: local arts council  
Tail Entity: January 2019  
#### organization founded by ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization founded by" means an organization was found by a person.
Here is an example:
Relation: organization founded by
Context: `` i have no greater obligation than to ensure the safely of airline travelers in this country , '' transportation secretary ray lahood said in a joint statement with j. randolph babbitt , administrator of the federal aviation administration , that was issued on the eve of a senate hearing on aviation safety .
Head Entity: federal aviation administration
Tail Entity: j. randolph babbitt
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization founded by":
Relation: organization founded by  
Context: In 1975, the renowned physicist and entrepreneur, Dr. Jane Smith, established Quantum Innovations, a company dedicated to advancing quantum computing technologies.  
Head Entity: Quantum Innovations  
Tail Entity: Dr. Jane Smith  

Relation: organization founded by  
Context: The charity organization, Hope for Tomorrow, was created in 2010 by the well-known philanthropist, John Doe, to support underprivileged children around the world.  
Head Entity: Hope for Tomorrow  
Tail Entity: John Doe  
Mixup data size:  134
MixupTrain:  epoch  0, batch     0 | loss: 7.6051738MixupTrain:  epoch  0, batch     1 | loss: 7.3128715MixupTrain:  epoch  0, batch     2 | loss: 6.6931593MixupTrain:  epoch  0, batch     3 | loss: 6.7427983MixupTrain:  epoch  0, batch     4 | loss: 7.9797630MixupTrain:  epoch  0, batch     5 | loss: 7.1384088MixupTrain:  epoch  0, batch     6 | loss: 5.5219977MixupTrain:  epoch  0, batch     7 | loss: 5.4784771MixupTrain:  epoch  0, batch     8 | loss: 5.3796996
MemoryTrain:  epoch  0, batch     0 | loss: 3.1489716MemoryTrain:  epoch  0, batch     1 | loss: 4.0232449MemoryTrain:  epoch  0, batch     2 | loss: 5.0357857MemoryTrain:  epoch  1, batch     0 | loss: 3.8164277MemoryTrain:  epoch  1, batch     1 | loss: 3.5317705MemoryTrain:  epoch  1, batch     2 | loss: 3.9631252MemoryTrain:  epoch  2, batch     0 | loss: 2.8490038MemoryTrain:  epoch  2, batch     1 | loss: 4.0886784MemoryTrain:  epoch  2, batch     2 | loss: 3.1043246MemoryTrain:  epoch  3, batch     0 | loss: 3.0510621MemoryTrain:  epoch  3, batch     1 | loss: 2.6871450MemoryTrain:  epoch  3, batch     2 | loss: 3.3059084MemoryTrain:  epoch  4, batch     0 | loss: 3.5471072MemoryTrain:  epoch  4, batch     1 | loss: 2.2041531MemoryTrain:  epoch  4, batch     2 | loss: 3.3081295MemoryTrain:  epoch  5, batch     0 | loss: 2.5629783MemoryTrain:  epoch  5, batch     1 | loss: 2.8232782MemoryTrain:  epoch  5, batch     2 | loss: 2.3774714MemoryTrain:  epoch  6, batch     0 | loss: 2.9976499MemoryTrain:  epoch  6, batch     1 | loss: 1.9892306MemoryTrain:  epoch  6, batch     2 | loss: 2.1632934MemoryTrain:  epoch  7, batch     0 | loss: 1.8772953MemoryTrain:  epoch  7, batch     1 | loss: 2.6220903MemoryTrain:  epoch  7, batch     2 | loss: 2.1462750MemoryTrain:  epoch  8, batch     0 | loss: 2.2374921MemoryTrain:  epoch  8, batch     1 | loss: 2.3102386MemoryTrain:  epoch  8, batch     2 | loss: 1.9517066MemoryTrain:  epoch  9, batch     0 | loss: 2.3007302MemoryTrain:  epoch  9, batch     1 | loss: 1.9650600MemoryTrain:  epoch  9, batch     2 | loss: 1.8746610
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 68.75%,  total acc: 81.25%   [EVAL] batch:    3 | acc: 56.25%,  total acc: 75.00%   [EVAL] batch:    4 | acc: 31.25%,  total acc: 66.25%   [EVAL] batch:    5 | acc: 37.50%,  total acc: 61.46%   [EVAL] batch:    6 | acc: 37.50%,  total acc: 58.04%   [EVAL] batch:    7 | acc: 6.25%,  total acc: 51.56%   
[EVAL] batch:    0 | acc: 37.50%,  total acc: 37.50%   [EVAL] batch:    1 | acc: 37.50%,  total acc: 37.50%   [EVAL] batch:    2 | acc: 37.50%,  total acc: 37.50%   [EVAL] batch:    3 | acc: 18.75%,  total acc: 32.81%   [EVAL] batch:    4 | acc: 18.75%,  total acc: 30.00%   [EVAL] batch:    5 | acc: 37.50%,  total acc: 31.25%   [EVAL] batch:    6 | acc: 43.75%,  total acc: 33.04%   [EVAL] batch:    7 | acc: 56.25%,  total acc: 35.94%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 40.97%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 45.62%   [EVAL] batch:   10 | acc: 75.00%,  total acc: 48.30%   [EVAL] batch:   11 | acc: 62.50%,  total acc: 49.48%   [EVAL] batch:   12 | acc: 62.50%,  total acc: 50.48%   [EVAL] batch:   13 | acc: 50.00%,  total acc: 50.45%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 52.08%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 52.34%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 53.68%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 54.17%   [EVAL] batch:   18 | acc: 75.00%,  total acc: 55.26%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 56.56%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 58.63%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 60.51%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 62.23%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 63.54%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 65.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 66.35%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 67.36%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 68.53%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 69.61%   [EVAL] batch:   29 | acc: 93.75%,  total acc: 70.42%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 71.37%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 72.07%   [EVAL] batch:   32 | acc: 68.75%,  total acc: 71.97%   [EVAL] batch:   33 | acc: 37.50%,  total acc: 70.96%   [EVAL] batch:   34 | acc: 43.75%,  total acc: 70.18%   [EVAL] batch:   35 | acc: 31.25%,  total acc: 69.10%   [EVAL] batch:   36 | acc: 37.50%,  total acc: 68.24%   [EVAL] batch:   37 | acc: 37.50%,  total acc: 67.43%   [EVAL] batch:   38 | acc: 50.00%,  total acc: 66.99%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 67.81%   [EVAL] batch:   40 | acc: 43.75%,  total acc: 67.23%   [EVAL] batch:   41 | acc: 81.25%,  total acc: 67.56%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 68.17%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 68.75%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 69.44%   [EVAL] batch:   45 | acc: 93.75%,  total acc: 69.97%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 70.61%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 71.22%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 71.81%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 72.12%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 72.55%   [EVAL] batch:   51 | acc: 81.25%,  total acc: 72.72%   [EVAL] batch:   52 | acc: 62.50%,  total acc: 72.52%   [EVAL] batch:   53 | acc: 43.75%,  total acc: 71.99%   [EVAL] batch:   54 | acc: 37.50%,  total acc: 71.36%   [EVAL] batch:   55 | acc: 50.00%,  total acc: 70.98%   [EVAL] batch:   56 | acc: 18.75%,  total acc: 70.07%   
cur_acc:  ['0.8542', '0.9028', '0.5156']
his_acc:  ['0.8542', '0.8425', '0.7007']
CurrentTrain: epoch  0, batch     0 | loss: 5.4580340CurrentTrain: epoch  0, batch     1 | loss: 5.6213613CurrentTrain: epoch  1, batch     0 | loss: 4.3230934CurrentTrain: epoch  1, batch     1 | loss: 4.9393053CurrentTrain: epoch  2, batch     0 | loss: 4.0195642CurrentTrain: epoch  2, batch     1 | loss: 3.3004367CurrentTrain: epoch  3, batch     0 | loss: 3.2478147CurrentTrain: epoch  3, batch     1 | loss: 2.9909186CurrentTrain: epoch  4, batch     0 | loss: 3.1356199CurrentTrain: epoch  4, batch     1 | loss: 2.8072405CurrentTrain: epoch  5, batch     0 | loss: 2.7108607CurrentTrain: epoch  5, batch     1 | loss: 2.9220276CurrentTrain: epoch  6, batch     0 | loss: 2.5516729CurrentTrain: epoch  6, batch     1 | loss: 2.5044694CurrentTrain: epoch  7, batch     0 | loss: 2.5426064CurrentTrain: epoch  7, batch     1 | loss: 2.2514811CurrentTrain: epoch  8, batch     0 | loss: 2.2765350CurrentTrain: epoch  8, batch     1 | loss: 2.1162620CurrentTrain: epoch  9, batch     0 | loss: 2.2648363CurrentTrain: epoch  9, batch     1 | loss: 2.0156751
#### organization founded ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization founded" means an organization was found in a date.
Here is an example:
Relation: organization founded
Context: the jnf was founded in 1901 to buy plots in palestine , then ruled by the ottomans .
Head Entity: jnf
Tail Entity: 1901
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization founded":
Relation: organization founded  
Context: the united nations was established in 1945 to promote international cooperation and peace.  
Head Entity: united nations  
Tail Entity: 1945  

Relation: organization founded  
Context: the world health organization was created in 1948 to coordinate global health efforts.  
Head Entity: world health organization  
Tail Entity: 1948  
#### person age ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person age" means a person 's age is a number.
Here is an example:
Relation: person age
Context: virginia republican jo ann davis passed away on saturday at the age of 57 .
Head Entity: jo ann davis
Tail Entity: 57
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person age":
Relation: person age  
Context: the famous actor robert downey jr. celebrated his 56th birthday last week.  
Head Entity: robert downey jr.  
Tail Entity: 56  

Relation: person age  
Context: on her 30th birthday, singer taylor swift announced her new album.  
Head Entity: taylor swift  
Tail Entity: 30  
#### person city of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person city of birth" means a person was born in a city.
Here is an example:
Relation: person city of birth
Context: rothman was born in san francisco in 1932 in an orthodox jewish family .
Head Entity: rothman
Tail Entity: san francisco
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person city of birth":
Relation: person city of birth  
Context: after spending his early years in new york, he moved to los angeles where he began his career.  
Head Entity: he  
Tail Entity: los angeles  

Relation: person city of birth  
Context: the famous author was born in a small town near boston, which greatly influenced his writing.  
Head Entity: the famous author  
Tail Entity: boston  
#### organization members ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization members" means an organization 's member is an organization.
Here is an example:
Relation: organization members
Context: sun plays for the grand rapids flight of the international basketball league after toiling for the maryland nighthawks of the american basketball association , both development leagues for those who dream of an nba career .
Head Entity: american basketball association
Tail Entity: maryland nighthawks
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization members":
Relation: organization members  
Context: The New York Philharmonic is one of the oldest orchestras in the United States, and it has had many notable musicians as members, including the famous conductor Leonard Bernstein.  
Head Entity: New York Philharmonic  
Tail Entity: Leonard Bernstein  

Relation: organization members  
Context: The National Football League has a long history of legendary players, and one of its most famous members is Joe Montana, who played for the San Francisco 49ers.  
Head Entity: National Football League  
Tail Entity: Joe Montana  
#### person religion ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person religion" means an person is the member of a religion.
Here is an example:
Relation: person religion
Context: the pope defended his action on the grounds that he could not refuse an audience to a head of state from a country with a strong catholic tradition unless he had clear-cut proof of the allegations against him .
Head Entity: he
Tail Entity: catholic
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person religion":
Relation: person religion  
Context: During the ceremony, the rabbi spoke about the importance of faith and community in Judaism, emphasizing how every member plays a vital role in upholding their traditions.  
Head Entity: rabbi  
Tail Entity: Judaism  

Relation: person religion  
Context: The young woman shared her experiences growing up in a Muslim household, highlighting the values of compassion and charity that are central to her beliefs.  
Head Entity: young woman  
Tail Entity: Muslim  
Mixup data size:  165
MixupTrain:  epoch  0, batch     0 | loss: 5.6947824MixupTrain:  epoch  0, batch     1 | loss: 4.2644605MixupTrain:  epoch  0, batch     2 | loss: 3.7077383MixupTrain:  epoch  0, batch     3 | loss: 4.4519901MixupTrain:  epoch  0, batch     4 | loss: 3.6061666MixupTrain:  epoch  0, batch     5 | loss: 6.2122750MixupTrain:  epoch  0, batch     6 | loss: 3.9965014MixupTrain:  epoch  0, batch     7 | loss: 4.5744837MixupTrain:  epoch  0, batch     8 | loss: 3.7822798MixupTrain:  epoch  0, batch     9 | loss: 3.9882455MixupTrain:  epoch  0, batch    10 | loss: 3.1481330
MemoryTrain:  epoch  0, batch     0 | loss: 3.7082808MemoryTrain:  epoch  0, batch     1 | loss: 4.1834283MemoryTrain:  epoch  0, batch     2 | loss: 3.6898267MemoryTrain:  epoch  0, batch     3 | loss: 3.5490716MemoryTrain:  epoch  1, batch     0 | loss: 3.4206362MemoryTrain:  epoch  1, batch     1 | loss: 3.5005672MemoryTrain:  epoch  1, batch     2 | loss: 3.0243297MemoryTrain:  epoch  1, batch     3 | loss: 4.4096413MemoryTrain:  epoch  2, batch     0 | loss: 3.2283044MemoryTrain:  epoch  2, batch     1 | loss: 3.8663867MemoryTrain:  epoch  2, batch     2 | loss: 3.0197420MemoryTrain:  epoch  2, batch     3 | loss: 3.1170883MemoryTrain:  epoch  3, batch     0 | loss: 3.9780202MemoryTrain:  epoch  3, batch     1 | loss: 2.9179702MemoryTrain:  epoch  3, batch     2 | loss: 2.1320343MemoryTrain:  epoch  3, batch     3 | loss: 2.9459162MemoryTrain:  epoch  4, batch     0 | loss: 3.3295085MemoryTrain:  epoch  4, batch     1 | loss: 2.0915823MemoryTrain:  epoch  4, batch     2 | loss: 2.3984454MemoryTrain:  epoch  4, batch     3 | loss: 2.5197151MemoryTrain:  epoch  5, batch     0 | loss: 2.5762382MemoryTrain:  epoch  5, batch     1 | loss: 1.9831592MemoryTrain:  epoch  5, batch     2 | loss: 2.6766014MemoryTrain:  epoch  5, batch     3 | loss: 2.2673540MemoryTrain:  epoch  6, batch     0 | loss: 2.9213355MemoryTrain:  epoch  6, batch     1 | loss: 1.8843112MemoryTrain:  epoch  6, batch     2 | loss: 2.1814361MemoryTrain:  epoch  6, batch     3 | loss: 1.8477107MemoryTrain:  epoch  7, batch     0 | loss: 2.0380032MemoryTrain:  epoch  7, batch     1 | loss: 1.6155033MemoryTrain:  epoch  7, batch     2 | loss: 2.2944751MemoryTrain:  epoch  7, batch     3 | loss: 1.9904336MemoryTrain:  epoch  8, batch     0 | loss: 2.2907672MemoryTrain:  epoch  8, batch     1 | loss: 1.5147883MemoryTrain:  epoch  8, batch     2 | loss: 2.2152038MemoryTrain:  epoch  8, batch     3 | loss: 2.0834975MemoryTrain:  epoch  9, batch     0 | loss: 2.0316195MemoryTrain:  epoch  9, batch     1 | loss: 2.0688422MemoryTrain:  epoch  9, batch     2 | loss: 1.7189553MemoryTrain:  epoch  9, batch     3 | loss: 1.7151825
[EVAL] batch:    0 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 93.75%   [EVAL] batch:    2 | acc: 100.00%,  total acc: 95.83%   [EVAL] batch:    3 | acc: 100.00%,  total acc: 96.88%   [EVAL] batch:    4 | acc: 100.00%,  total acc: 97.50%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 97.92%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 98.21%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 98.44%   [EVAL] batch:    8 | acc: 62.50%,  total acc: 94.44%   [EVAL] batch:    9 | acc: 0.00%,  total acc: 85.00%   [EVAL] batch:   10 | acc: 18.75%,  total acc: 78.98%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 80.21%   [EVAL] batch:   12 | acc: 100.00%,  total acc: 81.73%   [EVAL] batch:   13 | acc: 75.00%,  total acc: 81.25%   
[EVAL] batch:    0 | acc: 37.50%,  total acc: 37.50%   [EVAL] batch:    1 | acc: 50.00%,  total acc: 43.75%   [EVAL] batch:    2 | acc: 50.00%,  total acc: 45.83%   [EVAL] batch:    3 | acc: 12.50%,  total acc: 37.50%   [EVAL] batch:    4 | acc: 18.75%,  total acc: 33.75%   [EVAL] batch:    5 | acc: 31.25%,  total acc: 33.33%   [EVAL] batch:    6 | acc: 18.75%,  total acc: 31.25%   [EVAL] batch:    7 | acc: 43.75%,  total acc: 32.81%   [EVAL] batch:    8 | acc: 62.50%,  total acc: 36.11%   [EVAL] batch:    9 | acc: 50.00%,  total acc: 37.50%   [EVAL] batch:   10 | acc: 50.00%,  total acc: 38.64%   [EVAL] batch:   11 | acc: 56.25%,  total acc: 40.10%   [EVAL] batch:   12 | acc: 43.75%,  total acc: 40.38%   [EVAL] batch:   13 | acc: 37.50%,  total acc: 40.18%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 42.50%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 43.36%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 45.22%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 46.18%   [EVAL] batch:   18 | acc: 87.50%,  total acc: 48.36%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 50.62%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 52.98%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 55.11%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 57.07%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 58.59%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 60.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 61.78%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 62.96%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 64.29%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 65.52%   [EVAL] batch:   29 | acc: 93.75%,  total acc: 66.46%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 67.54%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 68.36%   [EVAL] batch:   32 | acc: 56.25%,  total acc: 67.99%   [EVAL] batch:   33 | acc: 18.75%,  total acc: 66.54%   [EVAL] batch:   34 | acc: 25.00%,  total acc: 65.36%   [EVAL] batch:   35 | acc: 18.75%,  total acc: 64.06%   [EVAL] batch:   36 | acc: 18.75%,  total acc: 62.84%   [EVAL] batch:   37 | acc: 25.00%,  total acc: 61.84%   [EVAL] batch:   38 | acc: 50.00%,  total acc: 61.54%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 62.50%   [EVAL] batch:   40 | acc: 50.00%,  total acc: 62.20%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 62.80%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 63.52%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 64.35%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 65.14%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 65.90%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 66.62%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 67.32%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 67.98%   [EVAL] batch:   49 | acc: 81.25%,  total acc: 68.25%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 68.75%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 69.11%   [EVAL] batch:   52 | acc: 87.50%,  total acc: 69.46%   [EVAL] batch:   53 | acc: 43.75%,  total acc: 68.98%   [EVAL] batch:   54 | acc: 75.00%,  total acc: 69.09%   [EVAL] batch:   55 | acc: 50.00%,  total acc: 68.75%   [EVAL] batch:   56 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:   57 | acc: 100.00%,  total acc: 69.29%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 69.60%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 70.10%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 70.59%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 71.07%   [EVAL] batch:   62 | acc: 100.00%,  total acc: 71.53%   [EVAL] batch:   63 | acc: 100.00%,  total acc: 71.97%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 72.40%   [EVAL] batch:   65 | acc: 37.50%,  total acc: 71.88%   [EVAL] batch:   66 | acc: 0.00%,  total acc: 70.80%   [EVAL] batch:   67 | acc: 37.50%,  total acc: 70.31%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 70.74%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 71.16%   [EVAL] batch:   70 | acc: 50.00%,  total acc: 70.86%   
cur_acc:  ['0.8542', '0.9028', '0.5156', '0.8125']
his_acc:  ['0.8542', '0.8425', '0.7007', '0.7086']
CurrentTrain: epoch  0, batch     0 | loss: 5.5544405CurrentTrain: epoch  0, batch     1 | loss: 6.5580587CurrentTrain: epoch  1, batch     0 | loss: 5.2473741CurrentTrain: epoch  1, batch     1 | loss: 4.9049325CurrentTrain: epoch  2, batch     0 | loss: 5.1278791CurrentTrain: epoch  2, batch     1 | loss: 4.5126262CurrentTrain: epoch  3, batch     0 | loss: 4.4970345CurrentTrain: epoch  3, batch     1 | loss: 4.2858577CurrentTrain: epoch  4, batch     0 | loss: 4.0870013CurrentTrain: epoch  4, batch     1 | loss: 4.0393605CurrentTrain: epoch  5, batch     0 | loss: 3.9411819CurrentTrain: epoch  5, batch     1 | loss: 3.7672589CurrentTrain: epoch  6, batch     0 | loss: 3.8236022CurrentTrain: epoch  6, batch     1 | loss: 3.8491991CurrentTrain: epoch  7, batch     0 | loss: 3.9221528CurrentTrain: epoch  7, batch     1 | loss: 2.8900104CurrentTrain: epoch  8, batch     0 | loss: 3.4878907CurrentTrain: epoch  8, batch     1 | loss: 3.4084456CurrentTrain: epoch  9, batch     0 | loss: 3.3011165CurrentTrain: epoch  9, batch     1 | loss: 2.8066506
#### person stateorprovinces of residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person stateorprovinces of residence" means a person is living in a state or province.
Here is an example:
Relation: person stateorprovinces of residence
Context: ny-schools-chief -lrb- new york -rrb- -- cathleen p. black , mayor michael r. bloomberg 's choice to be the next chancellor of new york city 's public-school system , has during more than 40 years in the media business broken numerous glass ceilings -- and amassed a fortune -- with quick and cold-blooded decision making , crystal-clear goal setting , and an all-surpassing attention to the bottom line .
Head Entity: cathleen p. black
Tail Entity: new york
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person stateorprovinces of residence":
Relation: person stateorprovinces of residence  
Context: after years of living in the bustling city, actor john doe has decided to settle down in the serene landscapes of oregon, where he can enjoy a quieter lifestyle away from the spotlight.  
Head Entity: john doe  
Tail Entity: oregon  

Relation: person stateorprovinces of residence  
Context: renowned author jane smith has always found inspiration in the vibrant culture of california, which is why she chose to make los angeles her home for the past decade.  
Head Entity: jane smith  
Tail Entity: california  
#### person date of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person date of death" means a person was died on a date.
Here is an example:
Relation: person date of death
Context: he was taken off life support on feb. 14 .
Head Entity: he
Tail Entity: feb. 14
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person date of death":
Relation: person date of death  
Context: The famous author passed away on July 10, 2020, after a long illness.  
Head Entity: The famous author  
Tail Entity: July 10, 2020  

Relation: person date of death  
Context: She left this world on March 5, 2018, surrounded by her family.  
Head Entity: She  
Tail Entity: March 5, 2018  
#### organization number of employees members ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization number of employees members" means the number of an organization 's employees is a number.
Here is an example:
Relation: organization number of employees members
Context: with the sweep of a federal regulator 's pen , massachusetts stands to gain a new life-science giant in april : covidien , a medical - supplies maker with thousands of products and more than 43,000 employees worldwide .
Head Entity: covidien
Tail Entity: 43,000
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization number of employees members":
Relation: organization number of employees members  
Context: The tech company, Innovatech, has rapidly expanded its workforce over the past year, now boasting a total of 25,000 employees across its global offices.  
Head Entity: Innovatech  
Tail Entity: 25,000  

Relation: organization number of employees members  
Context: After the merger, the newly formed entity, Global Solutions, reported an impressive count of 50,000 employees, making it one of the largest firms in the industry.  
Head Entity: Global Solutions  
Tail Entity: 50,000  
#### person alternate names ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person alternate names" means a person 's alias is a person.
Here is an example:
Relation: person alternate names
Context: a judge in new york city said remy ma , whose real name is remy smith , said thursday that the hip-hopper could not leave the united states for a five-country european concert tour .
Head Entity: remy smith
Tail Entity: remy ma
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person alternate names":
Relation: person alternate names  
Context: The famous author Samuel Langhorne Clemens, better known by his pen name Mark Twain, wrote several classic novels that are still widely read today.  
Head Entity: Samuel Langhorne Clemens  
Tail Entity: Mark Twain  

Relation: person alternate names  
Context: The musician Stefani Joanne Angelina Germanotta, who is popularly known as Lady Gaga, has made a significant impact on the music industry with her unique style and powerful vocals.  
Head Entity: Stefani Joanne Angelina Germanotta  
Tail Entity: Lady Gaga  
#### person spouse ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person spouse" means a person 's spouse is a person.
Here is an example:
Relation: person spouse
Context: smits stands at the center of this multigenerational saga as alex vega , the adopted son of rum and sugar baron pancho duque -lrb- elizondo -rrb- and his wife , amalia -lrb- moreno -rrb- .
Head Entity: elizondo
Tail Entity: moreno
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person spouse":
Relation: person spouse  
Context: in a heartfelt ceremony, john and his beloved partner, sarah, exchanged vows surrounded by family and friends, celebrating their love and commitment to each other.  
Head Entity: john  
Tail Entity: sarah  

Relation: person spouse  
Context: after years of friendship, emily finally realized that her best friend, michael, was the one she wanted to spend her life with, and they decided to get married.  
Head Entity: emily  
Tail Entity: michael  
Mixup data size:  194
MixupTrain:  epoch  0, batch     0 | loss: 3.0524016MixupTrain:  epoch  0, batch     1 | loss: 4.5123939MixupTrain:  epoch  0, batch     2 | loss: 3.5695482MixupTrain:  epoch  0, batch     3 | loss: 4.3964750MixupTrain:  epoch  0, batch     4 | loss: 4.8756772MixupTrain:  epoch  0, batch     5 | loss: 4.0276392MixupTrain:  epoch  0, batch     6 | loss: 3.1141448MixupTrain:  epoch  0, batch     7 | loss: 3.0883575MixupTrain:  epoch  0, batch     8 | loss: 4.0293446MixupTrain:  epoch  0, batch     9 | loss: 4.5661096MixupTrain:  epoch  0, batch    10 | loss: 3.1620939MixupTrain:  epoch  0, batch    11 | loss: 3.1149935MixupTrain:  epoch  0, batch    12 | loss: 2.9745197
MemoryTrain:  epoch  0, batch     0 | loss: 2.5378058MemoryTrain:  epoch  0, batch     1 | loss: 2.2547965MemoryTrain:  epoch  0, batch     2 | loss: 3.4577017MemoryTrain:  epoch  0, batch     3 | loss: 2.8309250MemoryTrain:  epoch  0, batch     4 | loss: 2.9112322MemoryTrain:  epoch  1, batch     0 | loss: 2.0793269MemoryTrain:  epoch  1, batch     1 | loss: 2.3198063MemoryTrain:  epoch  1, batch     2 | loss: 2.9705946MemoryTrain:  epoch  1, batch     3 | loss: 2.3616853MemoryTrain:  epoch  1, batch     4 | loss: 2.0085862MemoryTrain:  epoch  2, batch     0 | loss: 1.9212999MemoryTrain:  epoch  2, batch     1 | loss: 2.6083512MemoryTrain:  epoch  2, batch     2 | loss: 2.3700471MemoryTrain:  epoch  2, batch     3 | loss: 1.8294978MemoryTrain:  epoch  2, batch     4 | loss: 2.2093124MemoryTrain:  epoch  3, batch     0 | loss: 1.7253084MemoryTrain:  epoch  3, batch     1 | loss: 2.1475928MemoryTrain:  epoch  3, batch     2 | loss: 2.3021338MemoryTrain:  epoch  3, batch     3 | loss: 1.9642271MemoryTrain:  epoch  3, batch     4 | loss: 1.6888074MemoryTrain:  epoch  4, batch     0 | loss: 1.8838056MemoryTrain:  epoch  4, batch     1 | loss: 1.7487373MemoryTrain:  epoch  4, batch     2 | loss: 2.0015957MemoryTrain:  epoch  4, batch     3 | loss: 1.5820256MemoryTrain:  epoch  4, batch     4 | loss: 2.1968708MemoryTrain:  epoch  5, batch     0 | loss: 1.7987007MemoryTrain:  epoch  5, batch     1 | loss: 1.7761143MemoryTrain:  epoch  5, batch     2 | loss: 2.0520225MemoryTrain:  epoch  5, batch     3 | loss: 1.6680589MemoryTrain:  epoch  5, batch     4 | loss: 1.6361725MemoryTrain:  epoch  6, batch     0 | loss: 1.4839585MemoryTrain:  epoch  6, batch     1 | loss: 1.5606978MemoryTrain:  epoch  6, batch     2 | loss: 1.6305983MemoryTrain:  epoch  6, batch     3 | loss: 1.7934766MemoryTrain:  epoch  6, batch     4 | loss: 2.0406730MemoryTrain:  epoch  7, batch     0 | loss: 1.6712142MemoryTrain:  epoch  7, batch     1 | loss: 1.9911206MemoryTrain:  epoch  7, batch     2 | loss: 1.7763197MemoryTrain:  epoch  7, batch     3 | loss: 1.5665317MemoryTrain:  epoch  7, batch     4 | loss: 1.4135119MemoryTrain:  epoch  8, batch     0 | loss: 1.9427664MemoryTrain:  epoch  8, batch     1 | loss: 1.7117556MemoryTrain:  epoch  8, batch     2 | loss: 1.4890044MemoryTrain:  epoch  8, batch     3 | loss: 1.5252733MemoryTrain:  epoch  8, batch     4 | loss: 1.3359300MemoryTrain:  epoch  9, batch     0 | loss: 1.6709361MemoryTrain:  epoch  9, batch     1 | loss: 1.6802224MemoryTrain:  epoch  9, batch     2 | loss: 1.4503016MemoryTrain:  epoch  9, batch     3 | loss: 1.6484466MemoryTrain:  epoch  9, batch     4 | loss: 1.4850942
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    2 | acc: 62.50%,  total acc: 66.67%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 70.31%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 70.00%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 78.57%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 79.69%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 80.62%   [EVAL] batch:   10 | acc: 43.75%,  total acc: 77.27%   [EVAL] batch:   11 | acc: 31.25%,  total acc: 73.44%   [EVAL] batch:   12 | acc: 37.50%,  total acc: 70.67%   [EVAL] batch:   13 | acc: 12.50%,  total acc: 66.52%   [EVAL] batch:   14 | acc: 0.00%,  total acc: 62.08%   
[EVAL] batch:    0 | acc: 37.50%,  total acc: 37.50%   [EVAL] batch:    1 | acc: 56.25%,  total acc: 46.88%   [EVAL] batch:    2 | acc: 50.00%,  total acc: 47.92%   [EVAL] batch:    3 | acc: 37.50%,  total acc: 45.31%   [EVAL] batch:    4 | acc: 43.75%,  total acc: 45.00%   [EVAL] batch:    5 | acc: 43.75%,  total acc: 44.79%   [EVAL] batch:    6 | acc: 37.50%,  total acc: 43.75%   [EVAL] batch:    7 | acc: 43.75%,  total acc: 43.75%   [EVAL] batch:    8 | acc: 50.00%,  total acc: 44.44%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 47.50%   [EVAL] batch:   10 | acc: 50.00%,  total acc: 47.73%   [EVAL] batch:   11 | acc: 62.50%,  total acc: 48.96%   [EVAL] batch:   12 | acc: 50.00%,  total acc: 49.04%   [EVAL] batch:   13 | acc: 37.50%,  total acc: 48.21%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 50.00%   [EVAL] batch:   15 | acc: 50.00%,  total acc: 50.00%   [EVAL] batch:   16 | acc: 68.75%,  total acc: 51.10%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 51.74%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 52.63%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 54.06%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 56.25%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 58.24%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 59.78%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 61.20%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 62.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 64.18%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 65.28%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 66.52%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 67.67%   [EVAL] batch:   29 | acc: 93.75%,  total acc: 68.54%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 69.56%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 70.51%   [EVAL] batch:   32 | acc: 50.00%,  total acc: 69.89%   [EVAL] batch:   33 | acc: 12.50%,  total acc: 68.20%   [EVAL] batch:   34 | acc: 18.75%,  total acc: 66.79%   [EVAL] batch:   35 | acc: 12.50%,  total acc: 65.28%   [EVAL] batch:   36 | acc: 12.50%,  total acc: 63.85%   [EVAL] batch:   37 | acc: 18.75%,  total acc: 62.66%   [EVAL] batch:   38 | acc: 43.75%,  total acc: 62.18%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 63.12%   [EVAL] batch:   40 | acc: 43.75%,  total acc: 62.65%   [EVAL] batch:   41 | acc: 75.00%,  total acc: 62.95%   [EVAL] batch:   42 | acc: 75.00%,  total acc: 63.23%   [EVAL] batch:   43 | acc: 75.00%,  total acc: 63.49%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 64.31%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 65.08%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 65.82%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 66.54%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 67.22%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 67.62%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 68.14%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 68.63%   [EVAL] batch:   52 | acc: 68.75%,  total acc: 68.63%   [EVAL] batch:   53 | acc: 25.00%,  total acc: 67.82%   [EVAL] batch:   54 | acc: 50.00%,  total acc: 67.50%   [EVAL] batch:   55 | acc: 43.75%,  total acc: 67.08%   [EVAL] batch:   56 | acc: 50.00%,  total acc: 66.78%   [EVAL] batch:   57 | acc: 100.00%,  total acc: 67.35%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 67.69%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 68.23%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 68.75%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 69.25%   [EVAL] batch:   62 | acc: 100.00%,  total acc: 69.74%   [EVAL] batch:   63 | acc: 100.00%,  total acc: 70.21%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 70.67%   [EVAL] batch:   65 | acc: 37.50%,  total acc: 70.17%   [EVAL] batch:   66 | acc: 0.00%,  total acc: 69.12%   [EVAL] batch:   67 | acc: 43.75%,  total acc: 68.75%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 69.20%   [EVAL] batch:   69 | acc: 93.75%,  total acc: 69.55%   [EVAL] batch:   70 | acc: 93.75%,  total acc: 69.89%   [EVAL] batch:   71 | acc: 56.25%,  total acc: 69.70%   [EVAL] batch:   72 | acc: 68.75%,  total acc: 69.69%   [EVAL] batch:   73 | acc: 62.50%,  total acc: 69.59%   [EVAL] batch:   74 | acc: 81.25%,  total acc: 69.75%   [EVAL] batch:   75 | acc: 87.50%,  total acc: 69.98%   [EVAL] batch:   76 | acc: 100.00%,  total acc: 70.37%   [EVAL] batch:   77 | acc: 100.00%,  total acc: 70.75%   [EVAL] batch:   78 | acc: 81.25%,  total acc: 70.89%   [EVAL] batch:   79 | acc: 100.00%,  total acc: 71.25%   [EVAL] batch:   80 | acc: 56.25%,  total acc: 71.06%   [EVAL] batch:   81 | acc: 18.75%,  total acc: 70.43%   [EVAL] batch:   82 | acc: 50.00%,  total acc: 70.18%   [EVAL] batch:   83 | acc: 18.75%,  total acc: 69.57%   [EVAL] batch:   84 | acc: 6.25%,  total acc: 68.82%   
cur_acc:  ['0.8542', '0.9028', '0.5156', '0.8125', '0.6208']
his_acc:  ['0.8542', '0.8425', '0.7007', '0.7086', '0.6882']
CurrentTrain: epoch  0, batch     0 | loss: 6.0672550CurrentTrain: epoch  0, batch     1 | loss: 7.0998602CurrentTrain: epoch  1, batch     0 | loss: 5.5498743CurrentTrain: epoch  1, batch     1 | loss: 5.0048547CurrentTrain: epoch  2, batch     0 | loss: 4.5671277CurrentTrain: epoch  2, batch     1 | loss: 4.5092158CurrentTrain: epoch  3, batch     0 | loss: 4.0092635CurrentTrain: epoch  3, batch     1 | loss: 3.3753579CurrentTrain: epoch  4, batch     0 | loss: 3.5901394CurrentTrain: epoch  4, batch     1 | loss: 3.6692696CurrentTrain: epoch  5, batch     0 | loss: 3.4424250CurrentTrain: epoch  5, batch     1 | loss: 2.9402044CurrentTrain: epoch  6, batch     0 | loss: 2.9358473CurrentTrain: epoch  6, batch     1 | loss: 3.2496488CurrentTrain: epoch  7, batch     0 | loss: 2.7933311CurrentTrain: epoch  7, batch     1 | loss: 2.4025612CurrentTrain: epoch  8, batch     0 | loss: 2.5666351CurrentTrain: epoch  8, batch     1 | loss: 2.4197381CurrentTrain: epoch  9, batch     0 | loss: 2.5017035CurrentTrain: epoch  9, batch     1 | loss: 2.2788374
#### person date of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person date of birth" means a person was born in a date.
Here is an example:
Relation: person date of birth
Context: kirkaldy , born irene morgan in baltimore , maryland , in 1917 , was arrested in 1944 for refusing to give up her seat on a greyhound bus heading from gloucester to baltimore , and for resisting arrest .
Head Entity: irene morgan
Tail Entity: 1917
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person date of birth":
Relation: person date of birth  
Context: the famous physicist albert einstein was born in ulm, in the kingdom of wurttemberg in the german empire, on march 14, 1879.  
Head Entity: albert einstein  
Tail Entity: march 14, 1879  

Relation: person date of birth  
Context: the renowned author jane austen was born on december 16, 1775, in steventon, hampshire, england.  
Head Entity: jane austen  
Tail Entity: december 16, 1775  
#### person stateorprovince of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person stateorprovince of birth" means a person was died in a state or province.
Here is an example:
Relation: person stateorprovince of birth
Context: jefferson joseph deblanc sr. was born in lockport , la. , on feb. 15 , 1921 , and grew up in st. martinville .
Head Entity: jefferson joseph deblanc sr.
Tail Entity: la.
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person stateorprovince of birth":
Relation: person stateorprovince of birth  
Context: martha ann jones was born in springfield, il, on march 3, 1985, and later moved to chicago.  
Head Entity: martha ann jones  
Tail Entity: il.  

Relation: person stateorprovince of birth  
Context: robert thomas was born in phoenix, az, in 1970, and spent his childhood in tucson.  
Head Entity: robert thomas  
Tail Entity: az.  
#### person parents ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person parents" means a person 's parent is a person.
Here is an example:
Relation: person parents
Context: as the case developed , sandy 's mother , denise sandy , quietly made herself a spectral but central figure , by faithfully attending pretrial hearings .
Head Entity: sandy
Tail Entity: denise sandy
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person parents":
Relation: person parents  
Context: During the family reunion, it was heartwarming to see how much Sarah resembled her father, John, in both looks and mannerisms.  
Head Entity: Sarah  
Tail Entity: John  

Relation: person parents  
Context: After the ceremony, Emily shared stories about her childhood and how her mother, Lisa, always encouraged her to pursue her dreams.  
Head Entity: Emily  
Tail Entity: Lisa  
#### person employee of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person employee of" means a person is an employee of an organization.
Here is an example:
Relation: person employee of
Context: seeking revenge , axel reunites with old pal sgt. billy rosewood -lrb- judge reinhold -rrb- and jon flint -lrb- hector elizondo -rrb- of the beverly hills police department .
Head Entity: hector elizondo
Tail Entity: beverly hills police department
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person employee of":
Relation: person employee of  
Context: After years of hard work and dedication, Sarah Thompson finally received a promotion at Tech Innovations, where she has been a key player in the development team.  
Head Entity: Sarah Thompson  
Tail Entity: Tech Innovations  

Relation: person employee of  
Context: John Smith has been with Global Finance for over a decade, where he has climbed the ranks to become the senior analyst in the investment division.  
Head Entity: John Smith  
Tail Entity: Global Finance  
#### person stateorprovince of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person stateorprovince of death" means a person was died in a state or province.
Here is an example:
Relation: person stateorprovince of death
Context: millender-mcdonald , who was 68 , died late saturday at her home in carson , california , said her chief of staff , bandele mcqueen .
Head Entity: millender-mcdonald
Tail Entity: california
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person stateorprovince of death":
Relation: person stateorprovince of death  
Context: johnson, a renowned author, passed away peacefully in his sleep at his residence in austin, texas, surrounded by family.  
Head Entity: johnson  
Tail Entity: texas  

Relation: person stateorprovince of death  
Context: the famous musician, who was 45, died unexpectedly in a hotel room in nashville, tennessee, during a concert tour.  
Head Entity: the famous musician  
Tail Entity: tennessee  
Mixup data size:  224
MixupTrain:  epoch  0, batch     0 | loss: 2.6533810MixupTrain:  epoch  0, batch     1 | loss: 3.0812529MixupTrain:  epoch  0, batch     2 | loss: 2.7508206MixupTrain:  epoch  0, batch     3 | loss: 2.7894303MixupTrain:  epoch  0, batch     4 | loss: 2.8766056MixupTrain:  epoch  0, batch     5 | loss: 2.5927966MixupTrain:  epoch  0, batch     6 | loss: 2.2542115MixupTrain:  epoch  0, batch     7 | loss: 2.8862467MixupTrain:  epoch  0, batch     8 | loss: 3.0941855MixupTrain:  epoch  0, batch     9 | loss: 2.6152315MixupTrain:  epoch  0, batch    10 | loss: 2.7602501MixupTrain:  epoch  0, batch    11 | loss: 2.3844770MixupTrain:  epoch  0, batch    12 | loss: 4.6737313MixupTrain:  epoch  0, batch    13 | loss: 2.9283818
MemoryTrain:  epoch  0, batch     0 | loss: 1.6222172MemoryTrain:  epoch  0, batch     1 | loss: 2.5580080MemoryTrain:  epoch  0, batch     2 | loss: 2.7941613MemoryTrain:  epoch  0, batch     3 | loss: 2.4546432MemoryTrain:  epoch  0, batch     4 | loss: 2.7602899MemoryTrain:  epoch  0, batch     5 | loss: 2.0560706MemoryTrain:  epoch  1, batch     0 | loss: 1.9744427MemoryTrain:  epoch  1, batch     1 | loss: 2.3801498MemoryTrain:  epoch  1, batch     2 | loss: 1.7044499MemoryTrain:  epoch  1, batch     3 | loss: 2.3079214MemoryTrain:  epoch  1, batch     4 | loss: 2.2556481MemoryTrain:  epoch  1, batch     5 | loss: 1.7577012MemoryTrain:  epoch  2, batch     0 | loss: 1.8120655MemoryTrain:  epoch  2, batch     1 | loss: 1.6497844MemoryTrain:  epoch  2, batch     2 | loss: 1.7363309MemoryTrain:  epoch  2, batch     3 | loss: 1.9409521MemoryTrain:  epoch  2, batch     4 | loss: 2.4531536MemoryTrain:  epoch  2, batch     5 | loss: 2.1076310MemoryTrain:  epoch  3, batch     0 | loss: 2.0749311MemoryTrain:  epoch  3, batch     1 | loss: 2.1949146MemoryTrain:  epoch  3, batch     2 | loss: 1.6805546MemoryTrain:  epoch  3, batch     3 | loss: 1.4995121MemoryTrain:  epoch  3, batch     4 | loss: 1.5207756MemoryTrain:  epoch  3, batch     5 | loss: 1.7748080MemoryTrain:  epoch  4, batch     0 | loss: 1.6268234MemoryTrain:  epoch  4, batch     1 | loss: 1.5098376MemoryTrain:  epoch  4, batch     2 | loss: 1.5893930MemoryTrain:  epoch  4, batch     3 | loss: 2.0318446MemoryTrain:  epoch  4, batch     4 | loss: 1.5500349MemoryTrain:  epoch  4, batch     5 | loss: 1.8163049MemoryTrain:  epoch  5, batch     0 | loss: 1.6125960MemoryTrain:  epoch  5, batch     1 | loss: 1.5417215MemoryTrain:  epoch  5, batch     2 | loss: 1.5476198MemoryTrain:  epoch  5, batch     3 | loss: 1.8085562MemoryTrain:  epoch  5, batch     4 | loss: 1.4515846MemoryTrain:  epoch  5, batch     5 | loss: 1.5767436MemoryTrain:  epoch  6, batch     0 | loss: 1.4605302MemoryTrain:  epoch  6, batch     1 | loss: 1.4469576MemoryTrain:  epoch  6, batch     2 | loss: 1.6730225MemoryTrain:  epoch  6, batch     3 | loss: 1.5946187MemoryTrain:  epoch  6, batch     4 | loss: 1.4898576MemoryTrain:  epoch  6, batch     5 | loss: 1.3822979MemoryTrain:  epoch  7, batch     0 | loss: 1.4875058MemoryTrain:  epoch  7, batch     1 | loss: 1.4887450MemoryTrain:  epoch  7, batch     2 | loss: 1.3939484MemoryTrain:  epoch  7, batch     3 | loss: 1.4128196MemoryTrain:  epoch  7, batch     4 | loss: 1.4677799MemoryTrain:  epoch  7, batch     5 | loss: 1.3968396MemoryTrain:  epoch  8, batch     0 | loss: 1.4193180MemoryTrain:  epoch  8, batch     1 | loss: 1.3722255MemoryTrain:  epoch  8, batch     2 | loss: 1.4332929MemoryTrain:  epoch  8, batch     3 | loss: 1.5443429MemoryTrain:  epoch  8, batch     4 | loss: 1.4304323MemoryTrain:  epoch  8, batch     5 | loss: 1.4869238MemoryTrain:  epoch  9, batch     0 | loss: 1.4035728MemoryTrain:  epoch  9, batch     1 | loss: 1.4941754MemoryTrain:  epoch  9, batch     2 | loss: 1.4119560MemoryTrain:  epoch  9, batch     3 | loss: 1.3095193MemoryTrain:  epoch  9, batch     4 | loss: 1.3351822MemoryTrain:  epoch  9, batch     5 | loss: 1.3594805
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 76.56%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 80.00%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 79.46%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 83.33%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 83.75%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 83.52%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 82.81%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 82.69%   [EVAL] batch:   13 | acc: 43.75%,  total acc: 79.91%   
[EVAL] batch:    0 | acc: 50.00%,  total acc: 50.00%   [EVAL] batch:    1 | acc: 62.50%,  total acc: 56.25%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 62.50%   [EVAL] batch:    3 | acc: 43.75%,  total acc: 57.81%   [EVAL] batch:    4 | acc: 50.00%,  total acc: 56.25%   [EVAL] batch:    5 | acc: 50.00%,  total acc: 55.21%   [EVAL] batch:    6 | acc: 43.75%,  total acc: 53.57%   [EVAL] batch:    7 | acc: 62.50%,  total acc: 54.69%   [EVAL] batch:    8 | acc: 56.25%,  total acc: 54.86%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 55.62%   [EVAL] batch:   10 | acc: 50.00%,  total acc: 55.11%   [EVAL] batch:   11 | acc: 68.75%,  total acc: 56.25%   [EVAL] batch:   12 | acc: 50.00%,  total acc: 55.77%   [EVAL] batch:   13 | acc: 37.50%,  total acc: 54.46%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 55.42%   [EVAL] batch:   15 | acc: 50.00%,  total acc: 55.08%   [EVAL] batch:   16 | acc: 68.75%,  total acc: 55.88%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 56.25%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 56.91%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 58.75%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 60.71%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 62.50%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 64.13%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 65.36%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 66.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 68.03%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 68.98%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 70.09%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 71.12%   [EVAL] batch:   29 | acc: 93.75%,  total acc: 71.88%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 72.78%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 73.63%   [EVAL] batch:   32 | acc: 50.00%,  total acc: 72.92%   [EVAL] batch:   33 | acc: 18.75%,  total acc: 71.32%   [EVAL] batch:   34 | acc: 18.75%,  total acc: 69.82%   [EVAL] batch:   35 | acc: 18.75%,  total acc: 68.40%   [EVAL] batch:   36 | acc: 12.50%,  total acc: 66.89%   [EVAL] batch:   37 | acc: 12.50%,  total acc: 65.46%   [EVAL] batch:   38 | acc: 31.25%,  total acc: 64.58%   [EVAL] batch:   39 | acc: 87.50%,  total acc: 65.16%   [EVAL] batch:   40 | acc: 37.50%,  total acc: 64.48%   [EVAL] batch:   41 | acc: 31.25%,  total acc: 63.69%   [EVAL] batch:   42 | acc: 37.50%,  total acc: 63.08%   [EVAL] batch:   43 | acc: 75.00%,  total acc: 63.35%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 64.17%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 64.95%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 65.69%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 66.41%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 67.09%   [EVAL] batch:   49 | acc: 81.25%,  total acc: 67.38%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 67.89%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 68.39%   [EVAL] batch:   52 | acc: 81.25%,  total acc: 68.63%   [EVAL] batch:   53 | acc: 25.00%,  total acc: 67.82%   [EVAL] batch:   54 | acc: 37.50%,  total acc: 67.27%   [EVAL] batch:   55 | acc: 50.00%,  total acc: 66.96%   [EVAL] batch:   56 | acc: 56.25%,  total acc: 66.78%   [EVAL] batch:   57 | acc: 100.00%,  total acc: 67.35%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 67.80%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 68.33%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 68.85%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 69.35%   [EVAL] batch:   62 | acc: 100.00%,  total acc: 69.84%   [EVAL] batch:   63 | acc: 100.00%,  total acc: 70.31%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 70.77%   [EVAL] batch:   65 | acc: 31.25%,  total acc: 70.17%   [EVAL] batch:   66 | acc: 0.00%,  total acc: 69.12%   [EVAL] batch:   67 | acc: 37.50%,  total acc: 68.66%   [EVAL] batch:   68 | acc: 81.25%,  total acc: 68.84%   [EVAL] batch:   69 | acc: 75.00%,  total acc: 68.93%   [EVAL] batch:   70 | acc: 56.25%,  total acc: 68.75%   [EVAL] batch:   71 | acc: 37.50%,  total acc: 68.32%   [EVAL] batch:   72 | acc: 50.00%,  total acc: 68.07%   [EVAL] batch:   73 | acc: 50.00%,  total acc: 67.82%   [EVAL] batch:   74 | acc: 62.50%,  total acc: 67.75%   [EVAL] batch:   75 | acc: 75.00%,  total acc: 67.85%   [EVAL] batch:   76 | acc: 75.00%,  total acc: 67.94%   [EVAL] batch:   77 | acc: 100.00%,  total acc: 68.35%   [EVAL] batch:   78 | acc: 68.75%,  total acc: 68.35%   [EVAL] batch:   79 | acc: 100.00%,  total acc: 68.75%   [EVAL] batch:   80 | acc: 37.50%,  total acc: 68.36%   [EVAL] batch:   81 | acc: 25.00%,  total acc: 67.84%   [EVAL] batch:   82 | acc: 31.25%,  total acc: 67.39%   [EVAL] batch:   83 | acc: 31.25%,  total acc: 66.96%   [EVAL] batch:   84 | acc: 12.50%,  total acc: 66.32%   [EVAL] batch:   85 | acc: 68.75%,  total acc: 66.35%   [EVAL] batch:   86 | acc: 81.25%,  total acc: 66.52%   [EVAL] batch:   87 | acc: 75.00%,  total acc: 66.62%   [EVAL] batch:   88 | acc: 81.25%,  total acc: 66.78%   [EVAL] batch:   89 | acc: 93.75%,  total acc: 67.08%   [EVAL] batch:   90 | acc: 81.25%,  total acc: 67.24%   [EVAL] batch:   91 | acc: 75.00%,  total acc: 67.32%   [EVAL] batch:   92 | acc: 93.75%,  total acc: 67.61%   [EVAL] batch:   93 | acc: 100.00%,  total acc: 67.95%   [EVAL] batch:   94 | acc: 87.50%,  total acc: 68.16%   [EVAL] batch:   95 | acc: 81.25%,  total acc: 68.29%   [EVAL] batch:   96 | acc: 75.00%,  total acc: 68.36%   [EVAL] batch:   97 | acc: 81.25%,  total acc: 68.49%   [EVAL] batch:   98 | acc: 37.50%,  total acc: 68.18%   
cur_acc:  ['0.8542', '0.9028', '0.5156', '0.8125', '0.6208', '0.7991']
his_acc:  ['0.8542', '0.8425', '0.7007', '0.7086', '0.6882', '0.6818']
CurrentTrain: epoch  0, batch     0 | loss: 7.9286156CurrentTrain: epoch  0, batch     1 | loss: 8.2042971CurrentTrain: epoch  1, batch     0 | loss: 7.2550602CurrentTrain: epoch  1, batch     1 | loss: 6.7367387CurrentTrain: epoch  2, batch     0 | loss: 6.7348270CurrentTrain: epoch  2, batch     1 | loss: 6.0387087CurrentTrain: epoch  3, batch     0 | loss: 6.1776876CurrentTrain: epoch  3, batch     1 | loss: 6.3253865CurrentTrain: epoch  4, batch     0 | loss: 5.8633509CurrentTrain: epoch  4, batch     1 | loss: 5.8831034CurrentTrain: epoch  5, batch     0 | loss: 5.0020218CurrentTrain: epoch  5, batch     1 | loss: 5.6231866CurrentTrain: epoch  6, batch     0 | loss: 4.9586887CurrentTrain: epoch  6, batch     1 | loss: 5.0454350CurrentTrain: epoch  7, batch     0 | loss: 4.5216866CurrentTrain: epoch  7, batch     1 | loss: 4.6816964CurrentTrain: epoch  8, batch     0 | loss: 4.1816940CurrentTrain: epoch  8, batch     1 | loss: 4.5214791CurrentTrain: epoch  9, batch     0 | loss: 3.7543485CurrentTrain: epoch  9, batch     1 | loss: 4.2246652
#### organization subsidiaries ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization subsidiaries" means an organization 's subsidiary is an organization.
Here is an example:
Relation: organization subsidiaries
Context: firstgroup , britain 's largest bus operator , entered the north american market in 1999 when it acquired ryder public transportation services inc. .
Head Entity: firstgroup
Tail Entity: ryder public transportation services inc.
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization subsidiaries":
Relation: organization subsidiaries  
Context: Alphabet Inc. has several subsidiaries, including Google LLC, which is known for its search engine and various tech products.  
Head Entity: Alphabet Inc.  
Tail Entity: Google LLC  

Relation: organization subsidiaries  
Context: The Walt Disney Company owns multiple subsidiaries, such as Pixar Animation Studios, which is famous for its animated films.  
Head Entity: The Walt Disney Company  
Tail Entity: Pixar Animation Studios  
#### organization parents ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization parents" means an organization 's parent is an organization.
Here is an example:
Relation: organization parents
Context: aseel kami , tim cocks , abdul-rahman taher , tim cocks and michael christie -lrb- reuters -rrb- add an intriguing note in the speculation over who 's responsible for the recent bombings : `` but a senior iraqi intelligence source , who declined to be named , said there was evidence the bombs could be the work of the militant badr organisation , the armed wing of the supreme islamic iraqi council -lrb- isci -rrb- .
Head Entity: badr organisation
Tail Entity: supreme islamic iraqi council
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization parents":
Relation: organization parents  
Context: In a recent merger, the tech giant SoftTech announced its acquisition of Innovatech, a leading software development firm. This move has raised questions about the future of Innovatech and its role under the SoftTech umbrella. Industry experts believe that this acquisition will strengthen SoftTech's position in the market.  
Head Entity: SoftTech  
Tail Entity: Innovatech  

Relation: organization parents  
Context: The historic partnership between Global Health Initiative and Health for All has been pivotal in addressing healthcare challenges in underserved communities. As a parent organization, Global Health Initiative has provided essential resources and support to Health for All, enabling it to expand its outreach programs significantly.  
Head Entity: Global Health Initiative  
Tail Entity: Health for All  
#### organization alternate names ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization alternate names" means a person 's alias is a person.
Here is an example:
Relation: organization alternate names
Context: it also needs the green light from the 45-nation nuclear suppliers group -lrb- nsg -rrb- , which regulates global civilian nuclear trade , before it can begin buying nuclear reactors and fuel .
Head Entity: nsg
Tail Entity: nuclear suppliers group
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization alternate names":
Relation: organization alternate names  
Context: The International Monetary Fund, often referred to as the IMF, plays a crucial role in global economic stability.  
Head Entity: IMF  
Tail Entity: International Monetary Fund  

Relation: organization alternate names  
Context: The World Health Organization, commonly known as WHO, has been at the forefront of the global response to health crises.  
Head Entity: WHO  
Tail Entity: World Health Organization  
#### organization city of headquarters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization city of headquarters" means an organization is located in a city.
Here is an example:
Relation: organization city of headquarters
Context: ------ london 2008-05-20 07:23:45 utc enodis plc endorses sweetened takeover bid by us company manitowoc illinois tool works of glenville , illinois , which had offered 282 pence -lrb- us$ 551 euro3 54 -rrb- per share , said monday that it was considering its position .
Head Entity: illinois tool works
Tail Entity: glenville
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization city of headquarters":
Relation: organization city of headquarters  
Context: ------ in 2015, tech giant apple inc. announced plans to expand its operations in cupertino, california, where it has been headquartered since its founding.  
Head Entity: apple inc.  
Tail Entity: cupertino  

Relation: organization city of headquarters  
Context: ------ the multinational corporation samsung electronics is based in suwon, south korea, and has been a leader in the technology sector for decades.  
Head Entity: samsung electronics  
Tail Entity: suwon  
#### person siblings ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person siblings" means a person 's sibling is a person.
Here is an example:
Relation: person siblings
Context: `` holly and sanjaya are headed to -lsb- the hawaiian island of -rsb- kauai tomorrow morning so she can meet his parents . ''
Head Entity: she
Tail Entity: sanjaya
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person siblings":
Relation: person siblings  
Context: `` during the family reunion, john introduced his sister to everyone, and they all had a great time catching up. ''  
Head Entity: his sister  
Tail Entity: john  

Relation: person siblings  
Context: `` after the graduation ceremony, emily and her brother celebrated their achievements with a big party at home. ''  
Head Entity: her brother  
Tail Entity: emily  
Mixup data size:  255
MixupTrain:  epoch  0, batch     0 | loss: 2.2405687MixupTrain:  epoch  0, batch     1 | loss: 2.2873035MixupTrain:  epoch  0, batch     2 | loss: 3.1712843MixupTrain:  epoch  0, batch     3 | loss: 2.8984935MixupTrain:  epoch  0, batch     4 | loss: 3.3058475MixupTrain:  epoch  0, batch     5 | loss: 2.4044161MixupTrain:  epoch  0, batch     6 | loss: 2.6320079MixupTrain:  epoch  0, batch     7 | loss: 2.9463624MixupTrain:  epoch  0, batch     8 | loss: 3.3490013MixupTrain:  epoch  0, batch     9 | loss: 2.4925396MixupTrain:  epoch  0, batch    10 | loss: 2.6969844MixupTrain:  epoch  0, batch    11 | loss: 3.9912963MixupTrain:  epoch  0, batch    12 | loss: 3.3101130MixupTrain:  epoch  0, batch    13 | loss: 2.3587256MixupTrain:  epoch  0, batch    14 | loss: 2.8498960MixupTrain:  epoch  0, batch    15 | loss: 2.1718956
MemoryTrain:  epoch  0, batch     0 | loss: 2.2669151MemoryTrain:  epoch  0, batch     1 | loss: 2.1370394MemoryTrain:  epoch  0, batch     2 | loss: 2.1120417MemoryTrain:  epoch  0, batch     3 | loss: 2.3930593MemoryTrain:  epoch  0, batch     4 | loss: 3.2846675MemoryTrain:  epoch  0, batch     5 | loss: 2.6453669MemoryTrain:  epoch  0, batch     6 | loss: 3.2561135MemoryTrain:  epoch  1, batch     0 | loss: 2.3449686MemoryTrain:  epoch  1, batch     1 | loss: 2.2531700MemoryTrain:  epoch  1, batch     2 | loss: 2.7371657MemoryTrain:  epoch  1, batch     3 | loss: 2.1003366MemoryTrain:  epoch  1, batch     4 | loss: 2.1383357MemoryTrain:  epoch  1, batch     5 | loss: 2.4566236MemoryTrain:  epoch  1, batch     6 | loss: 1.7428250MemoryTrain:  epoch  2, batch     0 | loss: 2.2238955MemoryTrain:  epoch  2, batch     1 | loss: 2.2952840MemoryTrain:  epoch  2, batch     2 | loss: 2.0085368MemoryTrain:  epoch  2, batch     3 | loss: 1.7354774MemoryTrain:  epoch  2, batch     4 | loss: 2.1271608MemoryTrain:  epoch  2, batch     5 | loss: 1.5338666MemoryTrain:  epoch  2, batch     6 | loss: 2.0768237MemoryTrain:  epoch  3, batch     0 | loss: 2.2671919MemoryTrain:  epoch  3, batch     1 | loss: 2.0456791MemoryTrain:  epoch  3, batch     2 | loss: 1.7291929MemoryTrain:  epoch  3, batch     3 | loss: 1.4512404MemoryTrain:  epoch  3, batch     4 | loss: 2.0316575MemoryTrain:  epoch  3, batch     5 | loss: 1.5552514MemoryTrain:  epoch  3, batch     6 | loss: 1.9521769MemoryTrain:  epoch  4, batch     0 | loss: 1.7007667MemoryTrain:  epoch  4, batch     1 | loss: 2.0523322MemoryTrain:  epoch  4, batch     2 | loss: 1.6416537MemoryTrain:  epoch  4, batch     3 | loss: 1.6195791MemoryTrain:  epoch  4, batch     4 | loss: 1.9636320MemoryTrain:  epoch  4, batch     5 | loss: 1.8879123MemoryTrain:  epoch  4, batch     6 | loss: 1.4386934MemoryTrain:  epoch  5, batch     0 | loss: 1.7741157MemoryTrain:  epoch  5, batch     1 | loss: 1.6391358MemoryTrain:  epoch  5, batch     2 | loss: 1.5351375MemoryTrain:  epoch  5, batch     3 | loss: 1.7810730MemoryTrain:  epoch  5, batch     4 | loss: 1.8957967MemoryTrain:  epoch  5, batch     5 | loss: 1.2853632MemoryTrain:  epoch  5, batch     6 | loss: 1.2670970MemoryTrain:  epoch  6, batch     0 | loss: 1.4307269MemoryTrain:  epoch  6, batch     1 | loss: 1.5801480MemoryTrain:  epoch  6, batch     2 | loss: 1.5074701MemoryTrain:  epoch  6, batch     3 | loss: 1.8278813MemoryTrain:  epoch  6, batch     4 | loss: 1.5600994MemoryTrain:  epoch  6, batch     5 | loss: 1.4324098MemoryTrain:  epoch  6, batch     6 | loss: 1.6143649MemoryTrain:  epoch  7, batch     0 | loss: 1.3997173MemoryTrain:  epoch  7, batch     1 | loss: 1.6648765MemoryTrain:  epoch  7, batch     2 | loss: 1.7310251MemoryTrain:  epoch  7, batch     3 | loss: 1.5243908MemoryTrain:  epoch  7, batch     4 | loss: 1.2973582MemoryTrain:  epoch  7, batch     5 | loss: 1.5391320MemoryTrain:  epoch  7, batch     6 | loss: 1.3774680MemoryTrain:  epoch  8, batch     0 | loss: 1.5436749MemoryTrain:  epoch  8, batch     1 | loss: 1.3942276MemoryTrain:  epoch  8, batch     2 | loss: 1.3618667MemoryTrain:  epoch  8, batch     3 | loss: 1.6797631MemoryTrain:  epoch  8, batch     4 | loss: 1.4095846MemoryTrain:  epoch  8, batch     5 | loss: 1.3111637MemoryTrain:  epoch  8, batch     6 | loss: 1.7001240MemoryTrain:  epoch  9, batch     0 | loss: 1.4620228MemoryTrain:  epoch  9, batch     1 | loss: 1.3211415MemoryTrain:  epoch  9, batch     2 | loss: 1.5309651MemoryTrain:  epoch  9, batch     3 | loss: 1.5143480MemoryTrain:  epoch  9, batch     4 | loss: 1.3134853MemoryTrain:  epoch  9, batch     5 | loss: 1.3915077MemoryTrain:  epoch  9, batch     6 | loss: 1.5093120
[EVAL] batch:    0 | acc: 56.25%,  total acc: 56.25%   [EVAL] batch:    1 | acc: 56.25%,  total acc: 56.25%   [EVAL] batch:    2 | acc: 31.25%,  total acc: 47.92%   [EVAL] batch:    3 | acc: 0.00%,  total acc: 35.94%   [EVAL] batch:    4 | acc: 0.00%,  total acc: 28.75%   [EVAL] batch:    5 | acc: 0.00%,  total acc: 23.96%   [EVAL] batch:    6 | acc: 18.75%,  total acc: 23.21%   [EVAL] batch:    7 | acc: 62.50%,  total acc: 28.12%   [EVAL] batch:    8 | acc: 25.00%,  total acc: 27.78%   [EVAL] batch:    9 | acc: 43.75%,  total acc: 29.38%   [EVAL] batch:   10 | acc: 68.75%,  total acc: 32.95%   [EVAL] batch:   11 | acc: 50.00%,  total acc: 34.38%   [EVAL] batch:   12 | acc: 56.25%,  total acc: 36.06%   [EVAL] batch:   13 | acc: 100.00%,  total acc: 40.62%   [EVAL] batch:   14 | acc: 100.00%,  total acc: 44.58%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 48.05%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 51.10%   [EVAL] batch:   17 | acc: 87.50%,  total acc: 53.12%   [EVAL] batch:   18 | acc: 6.25%,  total acc: 50.66%   [EVAL] batch:   19 | acc: 6.25%,  total acc: 48.44%   [EVAL] batch:   20 | acc: 6.25%,  total acc: 46.43%   [EVAL] batch:   21 | acc: 6.25%,  total acc: 44.60%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 62.50%,  total acc: 65.62%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 70.83%   [EVAL] batch:    3 | acc: 56.25%,  total acc: 67.19%   [EVAL] batch:    4 | acc: 62.50%,  total acc: 66.25%   [EVAL] batch:    5 | acc: 50.00%,  total acc: 63.54%   [EVAL] batch:    6 | acc: 43.75%,  total acc: 60.71%   [EVAL] batch:    7 | acc: 25.00%,  total acc: 56.25%   [EVAL] batch:    8 | acc: 50.00%,  total acc: 55.56%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 56.88%   [EVAL] batch:   10 | acc: 50.00%,  total acc: 56.25%   [EVAL] batch:   11 | acc: 56.25%,  total acc: 56.25%   [EVAL] batch:   12 | acc: 31.25%,  total acc: 54.33%   [EVAL] batch:   13 | acc: 31.25%,  total acc: 52.68%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 53.75%   [EVAL] batch:   15 | acc: 50.00%,  total acc: 53.52%   [EVAL] batch:   16 | acc: 68.75%,  total acc: 54.41%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 54.86%   [EVAL] batch:   18 | acc: 75.00%,  total acc: 55.92%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 57.81%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 59.82%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 61.65%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 63.04%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 64.32%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 65.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 67.07%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 68.06%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 69.20%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 70.26%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 70.83%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 71.57%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 72.46%   [EVAL] batch:   32 | acc: 50.00%,  total acc: 71.78%   [EVAL] batch:   33 | acc: 18.75%,  total acc: 70.22%   [EVAL] batch:   34 | acc: 18.75%,  total acc: 68.75%   [EVAL] batch:   35 | acc: 18.75%,  total acc: 67.36%   [EVAL] batch:   36 | acc: 12.50%,  total acc: 65.88%   [EVAL] batch:   37 | acc: 12.50%,  total acc: 64.47%   [EVAL] batch:   38 | acc: 31.25%,  total acc: 63.62%   [EVAL] batch:   39 | acc: 87.50%,  total acc: 64.22%   [EVAL] batch:   40 | acc: 37.50%,  total acc: 63.57%   [EVAL] batch:   41 | acc: 31.25%,  total acc: 62.80%   [EVAL] batch:   42 | acc: 37.50%,  total acc: 62.21%   [EVAL] batch:   43 | acc: 75.00%,  total acc: 62.50%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 63.33%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 64.13%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 64.89%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 65.62%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 66.33%   [EVAL] batch:   49 | acc: 68.75%,  total acc: 66.38%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 66.91%   [EVAL] batch:   51 | acc: 68.75%,  total acc: 66.95%   [EVAL] batch:   52 | acc: 75.00%,  total acc: 67.10%   [EVAL] batch:   53 | acc: 43.75%,  total acc: 66.67%   [EVAL] batch:   54 | acc: 62.50%,  total acc: 66.59%   [EVAL] batch:   55 | acc: 37.50%,  total acc: 66.07%   [EVAL] batch:   56 | acc: 81.25%,  total acc: 66.34%   [EVAL] batch:   57 | acc: 100.00%,  total acc: 66.92%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 67.27%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 67.81%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 68.34%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 68.85%   [EVAL] batch:   62 | acc: 100.00%,  total acc: 69.35%   [EVAL] batch:   63 | acc: 100.00%,  total acc: 69.82%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 70.29%   [EVAL] batch:   65 | acc: 31.25%,  total acc: 69.70%   [EVAL] batch:   66 | acc: 0.00%,  total acc: 68.66%   [EVAL] batch:   67 | acc: 37.50%,  total acc: 68.20%   [EVAL] batch:   68 | acc: 93.75%,  total acc: 68.57%   [EVAL] batch:   69 | acc: 81.25%,  total acc: 68.75%   [EVAL] batch:   70 | acc: 62.50%,  total acc: 68.66%   [EVAL] batch:   71 | acc: 31.25%,  total acc: 68.14%   [EVAL] batch:   72 | acc: 50.00%,  total acc: 67.89%   [EVAL] batch:   73 | acc: 37.50%,  total acc: 67.48%   [EVAL] batch:   74 | acc: 62.50%,  total acc: 67.42%   [EVAL] batch:   75 | acc: 81.25%,  total acc: 67.60%   [EVAL] batch:   76 | acc: 75.00%,  total acc: 67.69%   [EVAL] batch:   77 | acc: 100.00%,  total acc: 68.11%   [EVAL] batch:   78 | acc: 68.75%,  total acc: 68.12%   [EVAL] batch:   79 | acc: 100.00%,  total acc: 68.52%   [EVAL] batch:   80 | acc: 37.50%,  total acc: 68.13%   [EVAL] batch:   81 | acc: 25.00%,  total acc: 67.61%   [EVAL] batch:   82 | acc: 43.75%,  total acc: 67.32%   [EVAL] batch:   83 | acc: 12.50%,  total acc: 66.67%   [EVAL] batch:   84 | acc: 18.75%,  total acc: 66.10%   [EVAL] batch:   85 | acc: 75.00%,  total acc: 66.21%   [EVAL] batch:   86 | acc: 62.50%,  total acc: 66.16%   [EVAL] batch:   87 | acc: 68.75%,  total acc: 66.19%   [EVAL] batch:   88 | acc: 68.75%,  total acc: 66.22%   [EVAL] batch:   89 | acc: 93.75%,  total acc: 66.53%   [EVAL] batch:   90 | acc: 68.75%,  total acc: 66.55%   [EVAL] batch:   91 | acc: 56.25%,  total acc: 66.44%   [EVAL] batch:   92 | acc: 93.75%,  total acc: 66.73%   [EVAL] batch:   93 | acc: 93.75%,  total acc: 67.02%   [EVAL] batch:   94 | acc: 87.50%,  total acc: 67.24%   [EVAL] batch:   95 | acc: 81.25%,  total acc: 67.38%   [EVAL] batch:   96 | acc: 75.00%,  total acc: 67.46%   [EVAL] batch:   97 | acc: 81.25%,  total acc: 67.60%   [EVAL] batch:   98 | acc: 62.50%,  total acc: 67.55%   [EVAL] batch:   99 | acc: 68.75%,  total acc: 67.56%   [EVAL] batch:  100 | acc: 43.75%,  total acc: 67.33%   [EVAL] batch:  101 | acc: 12.50%,  total acc: 66.79%   [EVAL] batch:  102 | acc: 0.00%,  total acc: 66.14%   [EVAL] batch:  103 | acc: 0.00%,  total acc: 65.50%   [EVAL] batch:  104 | acc: 0.00%,  total acc: 64.88%   [EVAL] batch:  105 | acc: 37.50%,  total acc: 64.62%   [EVAL] batch:  106 | acc: 43.75%,  total acc: 64.43%   [EVAL] batch:  107 | acc: 37.50%,  total acc: 64.18%   [EVAL] batch:  108 | acc: 56.25%,  total acc: 64.11%   [EVAL] batch:  109 | acc: 62.50%,  total acc: 64.09%   [EVAL] batch:  110 | acc: 62.50%,  total acc: 64.08%   [EVAL] batch:  111 | acc: 62.50%,  total acc: 64.06%   [EVAL] batch:  112 | acc: 100.00%,  total acc: 64.38%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 64.69%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 65.00%   [EVAL] batch:  115 | acc: 87.50%,  total acc: 65.19%   [EVAL] batch:  116 | acc: 62.50%,  total acc: 65.17%   [EVAL] batch:  117 | acc: 12.50%,  total acc: 64.72%   [EVAL] batch:  118 | acc: 0.00%,  total acc: 64.18%   [EVAL] batch:  119 | acc: 12.50%,  total acc: 63.75%   [EVAL] batch:  120 | acc: 0.00%,  total acc: 63.22%   
cur_acc:  ['0.8542', '0.9028', '0.5156', '0.8125', '0.6208', '0.7991', '0.4460']
his_acc:  ['0.8542', '0.8425', '0.7007', '0.7086', '0.6882', '0.6818', '0.6322']
CurrentTrain: epoch  0, batch     0 | loss: 4.8565226CurrentTrain: epoch  0, batch     1 | loss: 5.1710148CurrentTrain: epoch  1, batch     0 | loss: 3.3892171CurrentTrain: epoch  1, batch     1 | loss: 3.8380005CurrentTrain: epoch  2, batch     0 | loss: 3.1445785CurrentTrain: epoch  2, batch     1 | loss: 2.6663356CurrentTrain: epoch  3, batch     0 | loss: 2.6145244CurrentTrain: epoch  3, batch     1 | loss: 2.7940989CurrentTrain: epoch  4, batch     0 | loss: 2.2732863CurrentTrain: epoch  4, batch     1 | loss: 2.4153655CurrentTrain: epoch  5, batch     0 | loss: 2.4079781CurrentTrain: epoch  5, batch     1 | loss: 2.1811192CurrentTrain: epoch  6, batch     0 | loss: 2.2352333CurrentTrain: epoch  6, batch     1 | loss: 2.0340831CurrentTrain: epoch  7, batch     0 | loss: 2.0883830CurrentTrain: epoch  7, batch     1 | loss: 2.1613493CurrentTrain: epoch  8, batch     0 | loss: 1.9870957CurrentTrain: epoch  8, batch     1 | loss: 2.0994406CurrentTrain: epoch  9, batch     0 | loss: 1.8510752CurrentTrain: epoch  9, batch     1 | loss: 1.8490156
#### person cause of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person cause of death" means a person was died of an event.
Here is an example:
Relation: person cause of death
Context: pamela gardner ahearn , who served nine years as chief of protocol at the us house of representatives after earlier experience with the state department 's office of protocol and as elizabeth taylor 's executive assistant , died march 26 of a heart attack at her home in alexandria , va .
Head Entity: pamela gardner ahearn
Tail Entity: heart attack
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person cause of death":
Relation: person cause of death  
Context: john smith, a renowned scientist known for his groundbreaking research in genetics, passed away on july 15 due to complications from pneumonia while receiving treatment at a local hospital.  
Head Entity: john smith  
Tail Entity: pneumonia  

Relation: person cause of death  
Context: the famous actor, robert jones, tragically lost his life in a car accident on february 10, leaving behind a legacy of memorable performances and a grieving family.  
Head Entity: robert jones  
Tail Entity: car accident  
#### organization political religious affiliation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization political religious affiliation" means an organization is the member of a religion.
Here is an example:
Relation: organization political religious affiliation
Context: the militia has also suffered from allegations of being involved in clashes with rival shiite militias , particularly the badr corp , the military wing linked to supreme iraqi islamic council , the party led by powerful politician abdel aziz al-hakim .
Head Entity: badr corp
Tail Entity: shiite
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization political religious affiliation":
Relation: organization political religious affiliation  
Context: The Christian Democratic Union has been a significant player in German politics, often aligning itself with various religious groups to promote policies that reflect its Christian values.  
Head Entity: Christian Democratic Union  
Tail Entity: Christian  

Relation: organization political religious affiliation  
Context: The Islamic Society of North America has been actively involved in advocating for Muslim rights and representation in the political landscape of the United States.  
Head Entity: Islamic Society of North America  
Tail Entity: Muslim  
#### organization stateorprovince of headquarters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization stateorprovince of headquarters" means an organization is located in a state or province.
Here is an example:
Relation: organization stateorprovince of headquarters
Context: andrew lebow , an oil trader with mf global in new york , said investors have been discouraged by lower-than-expected oil imports in china and the disappointing growth in the u.s. economy .
Head Entity: mf global
Tail Entity: new york
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization stateorprovince of headquarters":
Relation: organization stateorprovince of headquarters  
Context: the headquarters of tech giant apple inc. is located in cupertino, california, where it has been a hub for innovation and development.  
Head Entity: apple inc.  
Tail Entity: cupertino, california  

Relation: organization stateorprovince of headquarters  
Context: the multinational corporation samsung electronics has its main office situated in suwon, south korea, which plays a crucial role in its global operations.  
Head Entity: samsung electronics  
Tail Entity: suwon, south korea  
#### person other family ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person other family" means a person 's relative is a person.
Here is an example:
Relation: person other family
Context: parren mitchell 's sister-in-law , juanita jackson mitchell , was the long - time head and legal counsel of the maryland naacp .
Head Entity: parren mitchell
Tail Entity: juanita jackson mitchell
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person other family":
Relation: person other family  
Context: During the family reunion, it was revealed that Sarah's cousin, Michael, had recently graduated from college with honors.  
Head Entity: Sarah  
Tail Entity: Michael  

Relation: person other family  
Context: In her memoir, Lisa shared stories about her aunt, who played a significant role in her upbringing and inspired her to pursue a career in art.  
Head Entity: Lisa  
Tail Entity: her aunt
#### person city of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person city of death" means a person was died in a city.
Here is an example:
Relation: person city of death
Context: irene morgan kirkaldy , 90 , who died of alzheimer 's disease aug. 10 at her home in gloucester , va. , quietly changed history in 1944 when she refused to give up her seat on a crowded greyhound bus to a white couple .
Head Entity: her
Tail Entity: gloucester
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person city of death":
Relation: person city of death  
Context: john smith, a renowned author, passed away on march 5 in his apartment located in new york city, leaving behind a legacy of literary works that inspired many.  
Head Entity: john smith  
Tail Entity: new york city  

Relation: person city of death  
Context: after a long battle with cancer, elizabeth taylor died on march 23 at a hospital in los angeles, where she had spent her final days surrounded by family and friends.  
Head Entity: elizabeth taylor  
Tail Entity: los angeles  
Mixup data size:  284
MixupTrain:  epoch  0, batch     0 | loss: 3.0422140MixupTrain:  epoch  0, batch     1 | loss: 2.9791190MixupTrain:  epoch  0, batch     2 | loss: 2.7127875MixupTrain:  epoch  0, batch     3 | loss: 2.5047174MixupTrain:  epoch  0, batch     4 | loss: 3.0287347MixupTrain:  epoch  0, batch     5 | loss: 2.5352986MixupTrain:  epoch  0, batch     6 | loss: 3.0876426MixupTrain:  epoch  0, batch     7 | loss: 2.6196827MixupTrain:  epoch  0, batch     8 | loss: 2.3518849MixupTrain:  epoch  0, batch     9 | loss: 2.4068823MixupTrain:  epoch  0, batch    10 | loss: 2.2718246MixupTrain:  epoch  0, batch    11 | loss: 2.5362017MixupTrain:  epoch  0, batch    12 | loss: 2.3343518MixupTrain:  epoch  0, batch    13 | loss: 3.1367912MixupTrain:  epoch  0, batch    14 | loss: 2.5080771MixupTrain:  epoch  0, batch    15 | loss: 2.2229930MixupTrain:  epoch  0, batch    16 | loss: 2.8006585MixupTrain:  epoch  0, batch    17 | loss: 3.2408075
MemoryTrain:  epoch  0, batch     0 | loss: 2.2789485MemoryTrain:  epoch  0, batch     1 | loss: 1.9469451MemoryTrain:  epoch  0, batch     2 | loss: 2.4142613MemoryTrain:  epoch  0, batch     3 | loss: 2.5105510MemoryTrain:  epoch  0, batch     4 | loss: 2.6852720MemoryTrain:  epoch  0, batch     5 | loss: 2.6484678MemoryTrain:  epoch  0, batch     6 | loss: 3.1170084MemoryTrain:  epoch  0, batch     7 | loss: 2.2657065MemoryTrain:  epoch  1, batch     0 | loss: 2.5223715MemoryTrain:  epoch  1, batch     1 | loss: 2.1516509MemoryTrain:  epoch  1, batch     2 | loss: 2.1545982MemoryTrain:  epoch  1, batch     3 | loss: 2.0403979MemoryTrain:  epoch  1, batch     4 | loss: 2.8081760MemoryTrain:  epoch  1, batch     5 | loss: 2.1441884MemoryTrain:  epoch  1, batch     6 | loss: 2.1856892MemoryTrain:  epoch  1, batch     7 | loss: 1.3790591MemoryTrain:  epoch  2, batch     0 | loss: 2.0675566MemoryTrain:  epoch  2, batch     1 | loss: 1.4701787MemoryTrain:  epoch  2, batch     2 | loss: 2.1776047MemoryTrain:  epoch  2, batch     3 | loss: 2.1829288MemoryTrain:  epoch  2, batch     4 | loss: 2.3349824MemoryTrain:  epoch  2, batch     5 | loss: 1.6097358MemoryTrain:  epoch  2, batch     6 | loss: 1.5047653MemoryTrain:  epoch  2, batch     7 | loss: 2.2063203MemoryTrain:  epoch  3, batch     0 | loss: 2.1809707MemoryTrain:  epoch  3, batch     1 | loss: 1.8959603MemoryTrain:  epoch  3, batch     2 | loss: 1.7027640MemoryTrain:  epoch  3, batch     3 | loss: 1.7969309MemoryTrain:  epoch  3, batch     4 | loss: 2.1169591MemoryTrain:  epoch  3, batch     5 | loss: 1.8580863MemoryTrain:  epoch  3, batch     6 | loss: 1.6388824MemoryTrain:  epoch  3, batch     7 | loss: 1.5684749MemoryTrain:  epoch  4, batch     0 | loss: 1.7646785MemoryTrain:  epoch  4, batch     1 | loss: 1.4927131MemoryTrain:  epoch  4, batch     2 | loss: 1.8232651MemoryTrain:  epoch  4, batch     3 | loss: 1.8528746MemoryTrain:  epoch  4, batch     4 | loss: 2.1255560MemoryTrain:  epoch  4, batch     5 | loss: 1.4856884MemoryTrain:  epoch  4, batch     6 | loss: 1.4402807MemoryTrain:  epoch  4, batch     7 | loss: 2.1131349MemoryTrain:  epoch  5, batch     0 | loss: 2.1134434MemoryTrain:  epoch  5, batch     1 | loss: 1.9453120MemoryTrain:  epoch  5, batch     2 | loss: 1.6050107MemoryTrain:  epoch  5, batch     3 | loss: 1.6240970MemoryTrain:  epoch  5, batch     4 | loss: 1.6080728MemoryTrain:  epoch  5, batch     5 | loss: 1.6131971MemoryTrain:  epoch  5, batch     6 | loss: 1.3388774MemoryTrain:  epoch  5, batch     7 | loss: 1.4502063MemoryTrain:  epoch  6, batch     0 | loss: 1.2994468MemoryTrain:  epoch  6, batch     1 | loss: 2.2917809MemoryTrain:  epoch  6, batch     2 | loss: 1.9860902MemoryTrain:  epoch  6, batch     3 | loss: 1.3971484MemoryTrain:  epoch  6, batch     4 | loss: 1.3608108MemoryTrain:  epoch  6, batch     5 | loss: 1.6649339MemoryTrain:  epoch  6, batch     6 | loss: 1.8491445MemoryTrain:  epoch  6, batch     7 | loss: 1.6432762MemoryTrain:  epoch  7, batch     0 | loss: 1.5188787MemoryTrain:  epoch  7, batch     1 | loss: 1.3348258MemoryTrain:  epoch  7, batch     2 | loss: 1.5797000MemoryTrain:  epoch  7, batch     3 | loss: 1.7614772MemoryTrain:  epoch  7, batch     4 | loss: 1.6348717MemoryTrain:  epoch  7, batch     5 | loss: 1.3509777MemoryTrain:  epoch  7, batch     6 | loss: 1.5153618MemoryTrain:  epoch  7, batch     7 | loss: 1.4165436MemoryTrain:  epoch  8, batch     0 | loss: 1.3137051MemoryTrain:  epoch  8, batch     1 | loss: 1.3549292MemoryTrain:  epoch  8, batch     2 | loss: 1.5923424MemoryTrain:  epoch  8, batch     3 | loss: 1.6046419MemoryTrain:  epoch  8, batch     4 | loss: 1.4828458MemoryTrain:  epoch  8, batch     5 | loss: 1.5887457MemoryTrain:  epoch  8, batch     6 | loss: 1.4074857MemoryTrain:  epoch  8, batch     7 | loss: 1.2833046MemoryTrain:  epoch  9, batch     0 | loss: 1.5103332MemoryTrain:  epoch  9, batch     1 | loss: 1.5030217MemoryTrain:  epoch  9, batch     2 | loss: 1.4757681MemoryTrain:  epoch  9, batch     3 | loss: 1.3070054MemoryTrain:  epoch  9, batch     4 | loss: 1.4788194MemoryTrain:  epoch  9, batch     5 | loss: 1.4561584MemoryTrain:  epoch  9, batch     6 | loss: 1.3192477MemoryTrain:  epoch  9, batch     7 | loss: 1.3325388
[EVAL] batch:    0 | acc: 37.50%,  total acc: 37.50%   [EVAL] batch:    1 | acc: 75.00%,  total acc: 56.25%   [EVAL] batch:    2 | acc: 56.25%,  total acc: 56.25%   [EVAL] batch:    3 | acc: 50.00%,  total acc: 54.69%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 62.50%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 68.75%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 72.32%   [EVAL] batch:    7 | acc: 62.50%,  total acc: 71.09%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 70.83%   [EVAL] batch:    9 | acc: 56.25%,  total acc: 69.38%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 71.02%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 71.35%   [EVAL] batch:   12 | acc: 50.00%,  total acc: 69.71%   
[EVAL] batch:    0 | acc: 56.25%,  total acc: 56.25%   [EVAL] batch:    1 | acc: 62.50%,  total acc: 59.38%   [EVAL] batch:    2 | acc: 62.50%,  total acc: 60.42%   [EVAL] batch:    3 | acc: 37.50%,  total acc: 54.69%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 57.50%   [EVAL] batch:    5 | acc: 56.25%,  total acc: 57.29%   [EVAL] batch:    6 | acc: 31.25%,  total acc: 53.57%   [EVAL] batch:    7 | acc: 25.00%,  total acc: 50.00%   [EVAL] batch:    8 | acc: 37.50%,  total acc: 48.61%   [EVAL] batch:    9 | acc: 56.25%,  total acc: 49.38%   [EVAL] batch:   10 | acc: 37.50%,  total acc: 48.30%   [EVAL] batch:   11 | acc: 56.25%,  total acc: 48.96%   [EVAL] batch:   12 | acc: 25.00%,  total acc: 47.12%   [EVAL] batch:   13 | acc: 37.50%,  total acc: 46.43%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 47.92%   [EVAL] batch:   15 | acc: 50.00%,  total acc: 48.05%   [EVAL] batch:   16 | acc: 68.75%,  total acc: 49.26%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 50.00%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 52.30%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 54.37%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 56.55%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 58.52%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 60.33%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 61.72%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 63.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 64.66%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 65.74%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 66.74%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 67.67%   [EVAL] batch:   29 | acc: 62.50%,  total acc: 67.50%   [EVAL] batch:   30 | acc: 81.25%,  total acc: 67.94%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 68.75%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 67.80%   [EVAL] batch:   33 | acc: 0.00%,  total acc: 65.81%   [EVAL] batch:   34 | acc: 0.00%,  total acc: 63.93%   [EVAL] batch:   35 | acc: 0.00%,  total acc: 62.15%   [EVAL] batch:   36 | acc: 0.00%,  total acc: 60.47%   [EVAL] batch:   37 | acc: 0.00%,  total acc: 58.88%   [EVAL] batch:   38 | acc: 25.00%,  total acc: 58.01%   [EVAL] batch:   39 | acc: 87.50%,  total acc: 58.75%   [EVAL] batch:   40 | acc: 37.50%,  total acc: 58.23%   [EVAL] batch:   41 | acc: 43.75%,  total acc: 57.89%   [EVAL] batch:   42 | acc: 43.75%,  total acc: 57.56%   [EVAL] batch:   43 | acc: 68.75%,  total acc: 57.81%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 58.75%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 59.65%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 60.51%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 61.33%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 62.12%   [EVAL] batch:   49 | acc: 68.75%,  total acc: 62.25%   [EVAL] batch:   50 | acc: 87.50%,  total acc: 62.75%   [EVAL] batch:   51 | acc: 68.75%,  total acc: 62.86%   [EVAL] batch:   52 | acc: 75.00%,  total acc: 63.09%   [EVAL] batch:   53 | acc: 43.75%,  total acc: 62.73%   [EVAL] batch:   54 | acc: 68.75%,  total acc: 62.84%   [EVAL] batch:   55 | acc: 43.75%,  total acc: 62.50%   [EVAL] batch:   56 | acc: 75.00%,  total acc: 62.72%   [EVAL] batch:   57 | acc: 100.00%,  total acc: 63.36%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 63.77%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 64.38%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 64.96%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 65.52%   [EVAL] batch:   62 | acc: 100.00%,  total acc: 66.07%   [EVAL] batch:   63 | acc: 100.00%,  total acc: 66.60%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 67.12%   [EVAL] batch:   65 | acc: 31.25%,  total acc: 66.57%   [EVAL] batch:   66 | acc: 0.00%,  total acc: 65.58%   [EVAL] batch:   67 | acc: 37.50%,  total acc: 65.17%   [EVAL] batch:   68 | acc: 87.50%,  total acc: 65.49%   [EVAL] batch:   69 | acc: 75.00%,  total acc: 65.62%   [EVAL] batch:   70 | acc: 62.50%,  total acc: 65.58%   [EVAL] batch:   71 | acc: 43.75%,  total acc: 65.28%   [EVAL] batch:   72 | acc: 56.25%,  total acc: 65.15%   [EVAL] batch:   73 | acc: 43.75%,  total acc: 64.86%   [EVAL] batch:   74 | acc: 62.50%,  total acc: 64.83%   [EVAL] batch:   75 | acc: 81.25%,  total acc: 65.05%   [EVAL] batch:   76 | acc: 75.00%,  total acc: 65.18%   [EVAL] batch:   77 | acc: 100.00%,  total acc: 65.62%   [EVAL] batch:   78 | acc: 68.75%,  total acc: 65.66%   [EVAL] batch:   79 | acc: 100.00%,  total acc: 66.09%   [EVAL] batch:   80 | acc: 25.00%,  total acc: 65.59%   [EVAL] batch:   81 | acc: 0.00%,  total acc: 64.79%   [EVAL] batch:   82 | acc: 0.00%,  total acc: 64.01%   [EVAL] batch:   83 | acc: 0.00%,  total acc: 63.24%   [EVAL] batch:   84 | acc: 6.25%,  total acc: 62.57%   [EVAL] batch:   85 | acc: 68.75%,  total acc: 62.65%   [EVAL] batch:   86 | acc: 6.25%,  total acc: 62.00%   [EVAL] batch:   87 | acc: 12.50%,  total acc: 61.43%   [EVAL] batch:   88 | acc: 6.25%,  total acc: 60.81%   [EVAL] batch:   89 | acc: 12.50%,  total acc: 60.28%   [EVAL] batch:   90 | acc: 0.00%,  total acc: 59.62%   [EVAL] batch:   91 | acc: 31.25%,  total acc: 59.31%   [EVAL] batch:   92 | acc: 93.75%,  total acc: 59.68%   [EVAL] batch:   93 | acc: 100.00%,  total acc: 60.11%   [EVAL] batch:   94 | acc: 87.50%,  total acc: 60.39%   [EVAL] batch:   95 | acc: 87.50%,  total acc: 60.68%   [EVAL] batch:   96 | acc: 81.25%,  total acc: 60.89%   [EVAL] batch:   97 | acc: 75.00%,  total acc: 61.03%   [EVAL] batch:   98 | acc: 43.75%,  total acc: 60.86%   [EVAL] batch:   99 | acc: 50.00%,  total acc: 60.75%   [EVAL] batch:  100 | acc: 37.50%,  total acc: 60.52%   [EVAL] batch:  101 | acc: 12.50%,  total acc: 60.05%   [EVAL] batch:  102 | acc: 0.00%,  total acc: 59.47%   [EVAL] batch:  103 | acc: 0.00%,  total acc: 58.89%   [EVAL] batch:  104 | acc: 0.00%,  total acc: 58.33%   [EVAL] batch:  105 | acc: 50.00%,  total acc: 58.25%   [EVAL] batch:  106 | acc: 43.75%,  total acc: 58.12%   [EVAL] batch:  107 | acc: 50.00%,  total acc: 58.04%   [EVAL] batch:  108 | acc: 50.00%,  total acc: 57.97%   [EVAL] batch:  109 | acc: 68.75%,  total acc: 58.07%   [EVAL] batch:  110 | acc: 56.25%,  total acc: 58.05%   [EVAL] batch:  111 | acc: 50.00%,  total acc: 57.98%   [EVAL] batch:  112 | acc: 75.00%,  total acc: 58.13%   [EVAL] batch:  113 | acc: 68.75%,  total acc: 58.22%   [EVAL] batch:  114 | acc: 81.25%,  total acc: 58.42%   [EVAL] batch:  115 | acc: 75.00%,  total acc: 58.57%   [EVAL] batch:  116 | acc: 43.75%,  total acc: 58.44%   [EVAL] batch:  117 | acc: 0.00%,  total acc: 57.94%   [EVAL] batch:  118 | acc: 0.00%,  total acc: 57.46%   [EVAL] batch:  119 | acc: 0.00%,  total acc: 56.98%   [EVAL] batch:  120 | acc: 31.25%,  total acc: 56.77%   [EVAL] batch:  121 | acc: 75.00%,  total acc: 56.92%   [EVAL] batch:  122 | acc: 56.25%,  total acc: 56.91%   [EVAL] batch:  123 | acc: 50.00%,  total acc: 56.85%   [EVAL] batch:  124 | acc: 93.75%,  total acc: 57.15%   [EVAL] batch:  125 | acc: 100.00%,  total acc: 57.49%   [EVAL] batch:  126 | acc: 93.75%,  total acc: 57.78%   [EVAL] batch:  127 | acc: 62.50%,  total acc: 57.81%   [EVAL] batch:  128 | acc: 75.00%,  total acc: 57.95%   [EVAL] batch:  129 | acc: 56.25%,  total acc: 57.93%   [EVAL] batch:  130 | acc: 81.25%,  total acc: 58.11%   [EVAL] batch:  131 | acc: 75.00%,  total acc: 58.24%   [EVAL] batch:  132 | acc: 56.25%,  total acc: 58.22%   
cur_acc:  ['0.8542', '0.9028', '0.5156', '0.8125', '0.6208', '0.7991', '0.4460', '0.6971']
his_acc:  ['0.8542', '0.8425', '0.7007', '0.7086', '0.6882', '0.6818', '0.6322', '0.5822']
----------END
his_acc mean:  [0.8598 0.8226 0.7436 0.7057 0.6452 0.6384 0.5988 0.5761]
