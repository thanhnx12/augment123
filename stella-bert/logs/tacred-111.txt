#############params############
cuda
Task=Tacred, 5-shot
Encoding model: bert
pattern=hybridprompt
mem=1, margin=0.3, gen=0, gen_num=0
#############params############
--------Round  0
seed:  100
#############params############
cuda
Task=Tacred, 5-shot
Encoding model: bert
pattern=hybridprompt
mem=1, margin=0.3, gen=0, gen_num=0
#############params############
--------Round  0
seed:  100
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/test.pkl
Task_order: [7 3 0 5 4 1 6 2]
prepared data!
#############params############
cuda
Task=Tacred, 5-shot
Encoding model: bert
pattern=hybridprompt
mem=1, margin=0.3, gen=0, gen_num=0
#############params############
--------Round  0
seed:  100
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/test.pkl
Task_order: [7 3 0 5 4 1 6 2]
prepared data!
#############params############
cuda
Task=Tacred, 5-shot
Encoding model: bert
pattern=hybridprompt
mem=1, margin=0.3, gen=0, gen_num=0
#############params############
--------Round  0
seed:  100
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/test.pkl
Task_order: [7 3 0 5 4 1 6 2]
prepared data!
Clustering into  2  clusters
Clusters:  [1 0 0 1 0 1]
Losses:  24.484291076660156 11.09357738494873 -0.0
CurrentTrain: epoch  0, batch     0 | loss: 24.4842911Losses:  22.786849975585938 9.646066665649414 -0.0
CurrentTrain: epoch  0, batch     1 | loss: 22.7868500Losses:  22.146541595458984 9.043697357177734 -0.0
CurrentTrain: epoch  0, batch     2 | loss: 22.1465416Losses:  23.206308364868164 10.202875137329102 -0.0
CurrentTrain: epoch  0, batch     3 | loss: 23.2063084Losses:  22.90353012084961 9.971681594848633 -0.0
CurrentTrain: epoch  0, batch     4 | loss: 22.9035301Losses:  20.097414016723633 7.433544635772705 -0.0
CurrentTrain: epoch  0, batch     5 | loss: 20.0974140Losses:  20.193172454833984 7.51834774017334 -0.0
CurrentTrain: epoch  0, batch     6 | loss: 20.1931725Losses:  21.41219139099121 9.035709381103516 -0.0
CurrentTrain: epoch  0, batch     7 | loss: 21.4121914Losses:  22.61432647705078 10.191665649414062 -0.0
CurrentTrain: epoch  0, batch     8 | loss: 22.6143265Losses:  21.027957916259766 8.755420684814453 -0.0
CurrentTrain: epoch  0, batch     9 | loss: 21.0279579Losses:  20.298215866088867 8.105674743652344 -0.0
CurrentTrain: epoch  0, batch    10 | loss: 20.2982159Losses:  19.365272521972656 7.297883033752441 -0.0
CurrentTrain: epoch  0, batch    11 | loss: 19.3652725Losses:  20.557674407958984 8.341421127319336 -0.0
CurrentTrain: epoch  0, batch    12 | loss: 20.5576744Losses:  20.136932373046875 8.216141700744629 -0.0
CurrentTrain: epoch  0, batch    13 | loss: 20.1369324Losses:  20.84465789794922 9.098001480102539 -0.0
CurrentTrain: epoch  0, batch    14 | loss: 20.8446579Losses:  18.70006561279297 7.007706642150879 -0.0
CurrentTrain: epoch  0, batch    15 | loss: 18.7000656Losses:  21.491127014160156 10.272443771362305 -0.0
CurrentTrain: epoch  0, batch    16 | loss: 21.4911270Losses:  23.605945587158203 12.354771614074707 -0.0
CurrentTrain: epoch  0, batch    17 | loss: 23.6059456Losses:  25.177303314208984 13.677546501159668 -0.0
CurrentTrain: epoch  0, batch    18 | loss: 25.1773033Losses:  19.79106903076172 8.275964736938477 -0.0
CurrentTrain: epoch  0, batch    19 | loss: 19.7910690Losses:  19.52218246459961 8.179508209228516 -0.0
CurrentTrain: epoch  0, batch    20 | loss: 19.5221825Losses:  21.686185836791992 9.94403076171875 -0.0
CurrentTrain: epoch  0, batch    21 | loss: 21.6861858Losses:  20.54251480102539 9.001008987426758 -0.0
CurrentTrain: epoch  0, batch    22 | loss: 20.5425148Losses:  19.086318969726562 8.195530891418457 -0.0
CurrentTrain: epoch  0, batch    23 | loss: 19.0863190Losses:  19.318546295166016 8.248927116394043 -0.0
CurrentTrain: epoch  0, batch    24 | loss: 19.3185463Losses:  18.264087677001953 7.126908302307129 -0.0
CurrentTrain: epoch  0, batch    25 | loss: 18.2640877Losses:  20.066986083984375 9.275312423706055 -0.0
CurrentTrain: epoch  0, batch    26 | loss: 20.0669861Losses:  18.58072853088379 8.231582641601562 -0.0
CurrentTrain: epoch  0, batch    27 | loss: 18.5807285Losses:  20.594148635864258 9.801923751831055 -0.0
CurrentTrain: epoch  0, batch    28 | loss: 20.5941486Losses:  17.435924530029297 6.587273597717285 -0.0
CurrentTrain: epoch  0, batch    29 | loss: 17.4359245Losses:  20.328285217285156 9.964601516723633 -0.0
CurrentTrain: epoch  0, batch    30 | loss: 20.3282852Losses:  18.943336486816406 8.328694343566895 -0.0
CurrentTrain: epoch  0, batch    31 | loss: 18.9433365Losses:  18.722625732421875 8.725318908691406 -0.0
CurrentTrain: epoch  0, batch    32 | loss: 18.7226257Losses:  18.309799194335938 8.058145523071289 -0.0
CurrentTrain: epoch  0, batch    33 | loss: 18.3097992Losses:  21.383808135986328 11.603239059448242 -0.0
CurrentTrain: epoch  0, batch    34 | loss: 21.3838081Losses:  20.344257354736328 10.351531982421875 -0.0
CurrentTrain: epoch  0, batch    35 | loss: 20.3442574Losses:  18.960163116455078 8.588579177856445 -0.0
CurrentTrain: epoch  0, batch    36 | loss: 18.9601631Losses:  11.625056266784668 1.7340437173843384 -0.0
CurrentTrain: epoch  0, batch    37 | loss: 11.6250563Losses:  16.442665100097656 6.579023361206055 -0.0
CurrentTrain: epoch  1, batch     0 | loss: 16.4426651Losses:  17.101051330566406 6.645983695983887 -0.0
CurrentTrain: epoch  1, batch     1 | loss: 17.1010513Losses:  16.871103286743164 7.576630592346191 -0.0
CurrentTrain: epoch  1, batch     2 | loss: 16.8711033Losses:  15.486275672912598 5.896018028259277 -0.0
CurrentTrain: epoch  1, batch     3 | loss: 15.4862757Losses:  15.93045711517334 7.013195037841797 -0.0
CurrentTrain: epoch  1, batch     4 | loss: 15.9304571Losses:  20.495412826538086 10.408208847045898 -0.0
CurrentTrain: epoch  1, batch     5 | loss: 20.4954128Losses:  16.792444229125977 7.405594348907471 -0.0
CurrentTrain: epoch  1, batch     6 | loss: 16.7924442Losses:  16.99728012084961 7.6896233558654785 -0.0
CurrentTrain: epoch  1, batch     7 | loss: 16.9972801Losses:  18.40041160583496 9.159364700317383 -0.0
CurrentTrain: epoch  1, batch     8 | loss: 18.4004116Losses:  16.033971786499023 6.781052112579346 -0.0
CurrentTrain: epoch  1, batch     9 | loss: 16.0339718Losses:  18.23338508605957 8.629507064819336 -0.0
CurrentTrain: epoch  1, batch    10 | loss: 18.2333851Losses:  15.808340072631836 5.987419605255127 -0.0
CurrentTrain: epoch  1, batch    11 | loss: 15.8083401Losses:  16.809457778930664 7.004868507385254 -0.0
CurrentTrain: epoch  1, batch    12 | loss: 16.8094578Losses:  14.408178329467773 5.78718376159668 -0.0
CurrentTrain: epoch  1, batch    13 | loss: 14.4081783Losses:  16.868511199951172 7.423667907714844 -0.0
CurrentTrain: epoch  1, batch    14 | loss: 16.8685112Losses:  15.9268159866333 6.880913734436035 -0.0
CurrentTrain: epoch  1, batch    15 | loss: 15.9268160Losses:  16.181894302368164 7.116952896118164 -0.0
CurrentTrain: epoch  1, batch    16 | loss: 16.1818943Losses:  18.18048095703125 8.785223007202148 -0.0
CurrentTrain: epoch  1, batch    17 | loss: 18.1804810Losses:  15.598489761352539 6.667312145233154 -0.0
CurrentTrain: epoch  1, batch    18 | loss: 15.5984898Losses:  18.324548721313477 8.800464630126953 -0.0
CurrentTrain: epoch  1, batch    19 | loss: 18.3245487Losses:  16.593320846557617 7.16705322265625 -0.0
CurrentTrain: epoch  1, batch    20 | loss: 16.5933208Losses:  19.343212127685547 9.711788177490234 -0.0
CurrentTrain: epoch  1, batch    21 | loss: 19.3432121Losses:  16.137928009033203 7.25899600982666 -0.0
CurrentTrain: epoch  1, batch    22 | loss: 16.1379280Losses:  16.05998992919922 6.859920501708984 -0.0
CurrentTrain: epoch  1, batch    23 | loss: 16.0599899Losses:  16.694801330566406 7.747344017028809 -0.0
CurrentTrain: epoch  1, batch    24 | loss: 16.6948013Losses:  16.26287841796875 7.734962463378906 -0.0
CurrentTrain: epoch  1, batch    25 | loss: 16.2628784Losses:  19.06183433532715 9.39748764038086 -0.0
CurrentTrain: epoch  1, batch    26 | loss: 19.0618343Losses:  17.82927703857422 9.53864860534668 -0.0
CurrentTrain: epoch  1, batch    27 | loss: 17.8292770Losses:  20.472856521606445 11.511785507202148 -0.0
CurrentTrain: epoch  1, batch    28 | loss: 20.4728565Losses:  16.005081176757812 7.919685363769531 -0.0
CurrentTrain: epoch  1, batch    29 | loss: 16.0050812Losses:  14.685606002807617 5.8891777992248535 -0.0
CurrentTrain: epoch  1, batch    30 | loss: 14.6856060Losses:  18.78046226501465 9.308300018310547 -0.0
CurrentTrain: epoch  1, batch    31 | loss: 18.7804623Losses:  15.294122695922852 6.124134063720703 -0.0
CurrentTrain: epoch  1, batch    32 | loss: 15.2941227Losses:  15.685734748840332 6.970768928527832 -0.0
CurrentTrain: epoch  1, batch    33 | loss: 15.6857347Losses:  14.875210762023926 6.371050834655762 -0.0
CurrentTrain: epoch  1, batch    34 | loss: 14.8752108Losses:  14.91103744506836 6.520541667938232 -0.0
CurrentTrain: epoch  1, batch    35 | loss: 14.9110374Losses:  15.218582153320312 6.4934468269348145 -0.0
CurrentTrain: epoch  1, batch    36 | loss: 15.2185822Losses:  9.92427921295166 1.1628615856170654 -0.0
CurrentTrain: epoch  1, batch    37 | loss: 9.9242792Losses:  12.529889106750488 4.884032249450684 -0.0
CurrentTrain: epoch  2, batch     0 | loss: 12.5298891Losses:  15.353628158569336 6.654603481292725 -0.0
CurrentTrain: epoch  2, batch     1 | loss: 15.3536282Losses:  14.646865844726562 5.670881271362305 -0.0
CurrentTrain: epoch  2, batch     2 | loss: 14.6468658Losses:  18.101390838623047 9.974207878112793 -0.0
CurrentTrain: epoch  2, batch     3 | loss: 18.1013908Losses:  18.865848541259766 9.158103942871094 -0.0
CurrentTrain: epoch  2, batch     4 | loss: 18.8658485Losses:  22.493392944335938 12.803984642028809 -0.0
CurrentTrain: epoch  2, batch     5 | loss: 22.4933929Losses:  14.105881690979004 5.331783294677734 -0.0
CurrentTrain: epoch  2, batch     6 | loss: 14.1058817Losses:  15.944729804992676 7.3445844650268555 -0.0
CurrentTrain: epoch  2, batch     7 | loss: 15.9447298Losses:  14.878652572631836 6.679802894592285 -0.0
CurrentTrain: epoch  2, batch     8 | loss: 14.8786526Losses:  14.804874420166016 6.363210201263428 -0.0
CurrentTrain: epoch  2, batch     9 | loss: 14.8048744Losses:  14.260708808898926 6.416150093078613 -0.0
CurrentTrain: epoch  2, batch    10 | loss: 14.2607088Losses:  15.430488586425781 6.7859296798706055 -0.0
CurrentTrain: epoch  2, batch    11 | loss: 15.4304886Losses:  13.859420776367188 5.958731651306152 -0.0
CurrentTrain: epoch  2, batch    12 | loss: 13.8594208Losses:  13.926494598388672 6.098959922790527 -0.0
CurrentTrain: epoch  2, batch    13 | loss: 13.9264946Losses:  17.210254669189453 10.051824569702148 -0.0
CurrentTrain: epoch  2, batch    14 | loss: 17.2102547Losses:  15.69797134399414 7.6251397132873535 -0.0
CurrentTrain: epoch  2, batch    15 | loss: 15.6979713Losses:  15.273120880126953 6.410971641540527 -0.0
CurrentTrain: epoch  2, batch    16 | loss: 15.2731209Losses:  15.083209037780762 5.982058525085449 -0.0
CurrentTrain: epoch  2, batch    17 | loss: 15.0832090Losses:  13.809980392456055 5.784006118774414 -0.0
CurrentTrain: epoch  2, batch    18 | loss: 13.8099804Losses:  14.850337028503418 6.758004188537598 -0.0
CurrentTrain: epoch  2, batch    19 | loss: 14.8503370Losses:  14.827433586120605 7.13981294631958 -0.0
CurrentTrain: epoch  2, batch    20 | loss: 14.8274336Losses:  14.230279922485352 6.496602535247803 -0.0
CurrentTrain: epoch  2, batch    21 | loss: 14.2302799Losses:  14.065319061279297 6.817283630371094 -0.0
CurrentTrain: epoch  2, batch    22 | loss: 14.0653191Losses:  12.79887580871582 5.063023567199707 -0.0
CurrentTrain: epoch  2, batch    23 | loss: 12.7988758Losses:  13.445688247680664 5.541528701782227 -0.0
CurrentTrain: epoch  2, batch    24 | loss: 13.4456882Losses:  14.263945579528809 5.6428022384643555 -0.0
CurrentTrain: epoch  2, batch    25 | loss: 14.2639456Losses:  13.881536483764648 6.6055426597595215 -0.0
CurrentTrain: epoch  2, batch    26 | loss: 13.8815365Losses:  15.6709623336792 6.621705055236816 -0.0
CurrentTrain: epoch  2, batch    27 | loss: 15.6709623Losses:  13.12844467163086 6.567111015319824 -0.0
CurrentTrain: epoch  2, batch    28 | loss: 13.1284447Losses:  13.919290542602539 5.952980995178223 -0.0
CurrentTrain: epoch  2, batch    29 | loss: 13.9192905Losses:  16.873912811279297 9.062623977661133 -0.0
CurrentTrain: epoch  2, batch    30 | loss: 16.8739128Losses:  13.060903549194336 5.852973461151123 -0.0
CurrentTrain: epoch  2, batch    31 | loss: 13.0609035Losses:  16.390132904052734 8.872234344482422 -0.0
CurrentTrain: epoch  2, batch    32 | loss: 16.3901329Losses:  13.797124862670898 5.945656776428223 -0.0
CurrentTrain: epoch  2, batch    33 | loss: 13.7971249Losses:  17.80034637451172 8.522038459777832 -0.0
CurrentTrain: epoch  2, batch    34 | loss: 17.8003464Losses:  15.722444534301758 8.490673065185547 -0.0
CurrentTrain: epoch  2, batch    35 | loss: 15.7224445Losses:  16.17369842529297 8.10380744934082 -0.0
CurrentTrain: epoch  2, batch    36 | loss: 16.1736984Losses:  10.177732467651367 2.6121301651000977 -0.0
CurrentTrain: epoch  2, batch    37 | loss: 10.1777325Losses:  14.365272521972656 6.616792678833008 -0.0
CurrentTrain: epoch  3, batch     0 | loss: 14.3652725Losses:  21.039833068847656 13.734432220458984 -0.0
CurrentTrain: epoch  3, batch     1 | loss: 21.0398331Losses:  14.336481094360352 6.321192741394043 -0.0
CurrentTrain: epoch  3, batch     2 | loss: 14.3364811Losses:  14.35009479522705 5.931670188903809 -0.0
CurrentTrain: epoch  3, batch     3 | loss: 14.3500948Losses:  13.435758590698242 5.696652412414551 -0.0
CurrentTrain: epoch  3, batch     4 | loss: 13.4357586Losses:  14.484556198120117 6.132818698883057 -0.0
CurrentTrain: epoch  3, batch     5 | loss: 14.4845562Losses:  16.479843139648438 8.010622024536133 -0.0
CurrentTrain: epoch  3, batch     6 | loss: 16.4798431Losses:  11.625627517700195 4.811498165130615 -0.0
CurrentTrain: epoch  3, batch     7 | loss: 11.6256275Losses:  13.080835342407227 5.559329032897949 -0.0
CurrentTrain: epoch  3, batch     8 | loss: 13.0808353Losses:  12.976127624511719 5.182727813720703 -0.0
CurrentTrain: epoch  3, batch     9 | loss: 12.9761276Losses:  13.57219409942627 6.197247505187988 -0.0
CurrentTrain: epoch  3, batch    10 | loss: 13.5721941Losses:  14.457958221435547 7.705490589141846 -0.0
CurrentTrain: epoch  3, batch    11 | loss: 14.4579582Losses:  12.954835891723633 4.983952522277832 -0.0
CurrentTrain: epoch  3, batch    12 | loss: 12.9548359Losses:  14.916476249694824 6.059971809387207 -0.0
CurrentTrain: epoch  3, batch    13 | loss: 14.9164762Losses:  13.399383544921875 5.690683364868164 -0.0
CurrentTrain: epoch  3, batch    14 | loss: 13.3993835Losses:  14.256828308105469 6.4976043701171875 -0.0
CurrentTrain: epoch  3, batch    15 | loss: 14.2568283Losses:  15.1793212890625 6.557483673095703 -0.0
CurrentTrain: epoch  3, batch    16 | loss: 15.1793213Losses:  15.124011993408203 7.15532922744751 -0.0
CurrentTrain: epoch  3, batch    17 | loss: 15.1240120Losses:  13.817989349365234 6.276711463928223 -0.0
CurrentTrain: epoch  3, batch    18 | loss: 13.8179893Losses:  15.140745162963867 6.817623615264893 -0.0
CurrentTrain: epoch  3, batch    19 | loss: 15.1407452Losses:  20.489351272583008 12.827505111694336 -0.0
CurrentTrain: epoch  3, batch    20 | loss: 20.4893513Losses:  13.786580085754395 6.320526599884033 -0.0
CurrentTrain: epoch  3, batch    21 | loss: 13.7865801Losses:  15.43614387512207 6.905430316925049 -0.0
CurrentTrain: epoch  3, batch    22 | loss: 15.4361439Losses:  14.944477081298828 6.943704128265381 -0.0
CurrentTrain: epoch  3, batch    23 | loss: 14.9444771Losses:  12.474298477172852 5.616710662841797 -0.0
CurrentTrain: epoch  3, batch    24 | loss: 12.4742985Losses:  16.15869903564453 8.954134941101074 -0.0
CurrentTrain: epoch  3, batch    25 | loss: 16.1586990Losses:  13.372758865356445 6.464221000671387 -0.0
CurrentTrain: epoch  3, batch    26 | loss: 13.3727589Losses:  16.207740783691406 7.668802261352539 -0.0
CurrentTrain: epoch  3, batch    27 | loss: 16.2077408Losses:  12.51959228515625 4.772591590881348 -0.0
CurrentTrain: epoch  3, batch    28 | loss: 12.5195923Losses:  13.524528503417969 7.424432754516602 -0.0
CurrentTrain: epoch  3, batch    29 | loss: 13.5245285Losses:  13.138760566711426 5.277498245239258 -0.0
CurrentTrain: epoch  3, batch    30 | loss: 13.1387606Losses:  14.800041198730469 6.879961967468262 -0.0
CurrentTrain: epoch  3, batch    31 | loss: 14.8000412Losses:  16.48734474182129 9.919774055480957 -0.0
CurrentTrain: epoch  3, batch    32 | loss: 16.4873447Losses:  13.744590759277344 7.4745259284973145 -0.0
CurrentTrain: epoch  3, batch    33 | loss: 13.7445908Losses:  12.75471019744873 5.7008819580078125 -0.0
CurrentTrain: epoch  3, batch    34 | loss: 12.7547102Losses:  12.688048362731934 5.515523433685303 -0.0
CurrentTrain: epoch  3, batch    35 | loss: 12.6880484Losses:  12.902756690979004 5.728215217590332 -0.0
CurrentTrain: epoch  3, batch    36 | loss: 12.9027567Losses:  9.954631805419922 3.0407488346099854 -0.0
CurrentTrain: epoch  3, batch    37 | loss: 9.9546318Losses:  13.538217544555664 5.880096435546875 -0.0
CurrentTrain: epoch  4, batch     0 | loss: 13.5382175Losses:  12.868948936462402 5.776017189025879 -0.0
CurrentTrain: epoch  4, batch     1 | loss: 12.8689489Losses:  13.05322551727295 6.871581554412842 -0.0
CurrentTrain: epoch  4, batch     2 | loss: 13.0532255Losses:  14.085168838500977 6.723466873168945 -0.0
CurrentTrain: epoch  4, batch     3 | loss: 14.0851688Losses:  15.75893497467041 8.059171676635742 -0.0
CurrentTrain: epoch  4, batch     4 | loss: 15.7589350Losses:  12.363733291625977 5.3065185546875 -0.0
CurrentTrain: epoch  4, batch     5 | loss: 12.3637333Losses:  13.979130744934082 7.64540958404541 -0.0
CurrentTrain: epoch  4, batch     6 | loss: 13.9791307Losses:  12.03931999206543 4.635382652282715 -0.0
CurrentTrain: epoch  4, batch     7 | loss: 12.0393200Losses:  18.28139877319336 9.432510375976562 -0.0
CurrentTrain: epoch  4, batch     8 | loss: 18.2813988Losses:  12.680774688720703 4.931866645812988 -0.0
CurrentTrain: epoch  4, batch     9 | loss: 12.6807747Losses:  15.348891258239746 7.893283367156982 -0.0
CurrentTrain: epoch  4, batch    10 | loss: 15.3488913Losses:  11.422733306884766 4.986667633056641 -0.0
CurrentTrain: epoch  4, batch    11 | loss: 11.4227333Losses:  15.57034969329834 7.842288494110107 -0.0
CurrentTrain: epoch  4, batch    12 | loss: 15.5703497Losses:  11.329901695251465 4.431328773498535 -0.0
CurrentTrain: epoch  4, batch    13 | loss: 11.3299017Losses:  16.442434310913086 8.331310272216797 -0.0
CurrentTrain: epoch  4, batch    14 | loss: 16.4424343Losses:  15.363147735595703 8.302762985229492 -0.0
CurrentTrain: epoch  4, batch    15 | loss: 15.3631477Losses:  12.51111888885498 5.131707191467285 -0.0
CurrentTrain: epoch  4, batch    16 | loss: 12.5111189Losses:  13.201292037963867 6.013570785522461 -0.0
CurrentTrain: epoch  4, batch    17 | loss: 13.2012920Losses:  11.9542875289917 5.227397918701172 -0.0
CurrentTrain: epoch  4, batch    18 | loss: 11.9542875Losses:  12.113625526428223 4.741415500640869 -0.0
CurrentTrain: epoch  4, batch    19 | loss: 12.1136255Losses:  13.043365478515625 6.264965534210205 -0.0
CurrentTrain: epoch  4, batch    20 | loss: 13.0433655Losses:  13.134180068969727 5.816242694854736 -0.0
CurrentTrain: epoch  4, batch    21 | loss: 13.1341801Losses:  12.316089630126953 5.012795448303223 -0.0
CurrentTrain: epoch  4, batch    22 | loss: 12.3160896Losses:  12.211580276489258 5.981380462646484 -0.0
CurrentTrain: epoch  4, batch    23 | loss: 12.2115803Losses:  13.381049156188965 6.052309989929199 -0.0
CurrentTrain: epoch  4, batch    24 | loss: 13.3810492Losses:  16.904733657836914 8.87432861328125 -0.0
CurrentTrain: epoch  4, batch    25 | loss: 16.9047337Losses:  12.331690788269043 5.389676094055176 -0.0
CurrentTrain: epoch  4, batch    26 | loss: 12.3316908Losses:  15.72543716430664 6.574335098266602 -0.0
CurrentTrain: epoch  4, batch    27 | loss: 15.7254372Losses:  10.561391830444336 3.804800510406494 -0.0
CurrentTrain: epoch  4, batch    28 | loss: 10.5613918Losses:  13.424135208129883 6.3454484939575195 -0.0
CurrentTrain: epoch  4, batch    29 | loss: 13.4241352Losses:  13.811635971069336 6.349462509155273 -0.0
CurrentTrain: epoch  4, batch    30 | loss: 13.8116360Losses:  16.522171020507812 10.00928020477295 -0.0
CurrentTrain: epoch  4, batch    31 | loss: 16.5221710Losses:  13.549663543701172 5.271859169006348 -0.0
CurrentTrain: epoch  4, batch    32 | loss: 13.5496635Losses:  12.859172821044922 6.2018842697143555 -0.0
CurrentTrain: epoch  4, batch    33 | loss: 12.8591728Losses:  15.57817268371582 7.214189529418945 -0.0
CurrentTrain: epoch  4, batch    34 | loss: 15.5781727Losses:  12.632911682128906 5.260801315307617 -0.0
CurrentTrain: epoch  4, batch    35 | loss: 12.6329117Losses:  12.30424976348877 5.367195129394531 -0.0
CurrentTrain: epoch  4, batch    36 | loss: 12.3042498Losses:  8.828350067138672 1.0146969556808472 -0.0
CurrentTrain: epoch  4, batch    37 | loss: 8.8283501Losses:  12.695700645446777 6.507469177246094 -0.0
CurrentTrain: epoch  5, batch     0 | loss: 12.6957006Losses:  13.227701187133789 5.899883270263672 -0.0
CurrentTrain: epoch  5, batch     1 | loss: 13.2277012Losses:  14.000346183776855 6.241747856140137 -0.0
CurrentTrain: epoch  5, batch     2 | loss: 14.0003462Losses:  14.084667205810547 6.5360822677612305 -0.0
CurrentTrain: epoch  5, batch     3 | loss: 14.0846672Losses:  12.741546630859375 5.282155513763428 -0.0
CurrentTrain: epoch  5, batch     4 | loss: 12.7415466Losses:  13.227107048034668 5.867283821105957 -0.0
CurrentTrain: epoch  5, batch     5 | loss: 13.2271070Losses:  12.369928359985352 4.678805828094482 -0.0
CurrentTrain: epoch  5, batch     6 | loss: 12.3699284Losses:  13.034517288208008 5.539697170257568 -0.0
CurrentTrain: epoch  5, batch     7 | loss: 13.0345173Losses:  13.948087692260742 6.143494606018066 -0.0
CurrentTrain: epoch  5, batch     8 | loss: 13.9480877Losses:  12.482772827148438 5.466446399688721 -0.0
CurrentTrain: epoch  5, batch     9 | loss: 12.4827728Losses:  12.454456329345703 5.196478366851807 -0.0
CurrentTrain: epoch  5, batch    10 | loss: 12.4544563Losses:  12.319214820861816 4.882311820983887 -0.0
CurrentTrain: epoch  5, batch    11 | loss: 12.3192148Losses:  11.428202629089355 4.709278106689453 -0.0
CurrentTrain: epoch  5, batch    12 | loss: 11.4282026Losses:  11.659285545349121 4.448663711547852 -0.0
CurrentTrain: epoch  5, batch    13 | loss: 11.6592855Losses:  12.307174682617188 4.592329978942871 -0.0
CurrentTrain: epoch  5, batch    14 | loss: 12.3071747Losses:  15.096515655517578 7.758608818054199 -0.0
CurrentTrain: epoch  5, batch    15 | loss: 15.0965157Losses:  12.088963508605957 6.2403154373168945 -0.0
CurrentTrain: epoch  5, batch    16 | loss: 12.0889635Losses:  12.498266220092773 6.225338935852051 -0.0
CurrentTrain: epoch  5, batch    17 | loss: 12.4982662Losses:  13.790145874023438 6.33962345123291 -0.0
CurrentTrain: epoch  5, batch    18 | loss: 13.7901459Losses:  15.743623733520508 7.329254150390625 -0.0
CurrentTrain: epoch  5, batch    19 | loss: 15.7436237Losses:  11.66822624206543 5.355057716369629 -0.0
CurrentTrain: epoch  5, batch    20 | loss: 11.6682262Losses:  12.198512077331543 5.473367691040039 -0.0
CurrentTrain: epoch  5, batch    21 | loss: 12.1985121Losses:  10.936223030090332 4.509642601013184 -0.0
CurrentTrain: epoch  5, batch    22 | loss: 10.9362230Losses:  11.146800994873047 4.279393196105957 -0.0
CurrentTrain: epoch  5, batch    23 | loss: 11.1468010Losses:  13.967605590820312 6.144975185394287 -0.0
CurrentTrain: epoch  5, batch    24 | loss: 13.9676056Losses:  10.829509735107422 4.582178115844727 -0.0
CurrentTrain: epoch  5, batch    25 | loss: 10.8295097Losses:  11.900751113891602 5.205649375915527 -0.0
CurrentTrain: epoch  5, batch    26 | loss: 11.9007511Losses:  16.701168060302734 10.11662769317627 -0.0
CurrentTrain: epoch  5, batch    27 | loss: 16.7011681Losses:  17.98267364501953 8.960708618164062 -0.0
CurrentTrain: epoch  5, batch    28 | loss: 17.9826736Losses:  11.068034172058105 3.7659947872161865 -0.0
CurrentTrain: epoch  5, batch    29 | loss: 11.0680342Losses:  12.390626907348633 6.23953914642334 -0.0
CurrentTrain: epoch  5, batch    30 | loss: 12.3906269Losses:  13.149271011352539 6.687117576599121 -0.0
CurrentTrain: epoch  5, batch    31 | loss: 13.1492710Losses:  15.09327507019043 9.428226470947266 -0.0
CurrentTrain: epoch  5, batch    32 | loss: 15.0932751Losses:  13.842879295349121 6.790816307067871 -0.0
CurrentTrain: epoch  5, batch    33 | loss: 13.8428793Losses:  10.635223388671875 3.8633408546447754 -0.0
CurrentTrain: epoch  5, batch    34 | loss: 10.6352234Losses:  13.439264297485352 7.388665199279785 -0.0
CurrentTrain: epoch  5, batch    35 | loss: 13.4392643Losses:  11.820623397827148 4.6963067054748535 -0.0
CurrentTrain: epoch  5, batch    36 | loss: 11.8206234Losses:  7.318852424621582 1.3309223651885986 -0.0
CurrentTrain: epoch  5, batch    37 | loss: 7.3188524Losses:  11.656665802001953 5.793758392333984 -0.0
CurrentTrain: epoch  6, batch     0 | loss: 11.6566658Losses:  13.192296981811523 6.181687355041504 -0.0
CurrentTrain: epoch  6, batch     1 | loss: 13.1922970Losses:  15.37834358215332 8.2137451171875 -0.0
CurrentTrain: epoch  6, batch     2 | loss: 15.3783436Losses:  11.091800689697266 4.6831488609313965 -0.0
CurrentTrain: epoch  6, batch     3 | loss: 11.0918007Losses:  12.283970832824707 5.929340362548828 -0.0
CurrentTrain: epoch  6, batch     4 | loss: 12.2839708Losses:  11.50403881072998 5.388874053955078 -0.0
CurrentTrain: epoch  6, batch     5 | loss: 11.5040388Losses:  16.52977752685547 9.495426177978516 -0.0
CurrentTrain: epoch  6, batch     6 | loss: 16.5297775Losses:  11.092159271240234 4.4821062088012695 -0.0
CurrentTrain: epoch  6, batch     7 | loss: 11.0921593Losses:  11.753315925598145 5.811083793640137 -0.0
CurrentTrain: epoch  6, batch     8 | loss: 11.7533159Losses:  11.005868911743164 4.764487266540527 -0.0
CurrentTrain: epoch  6, batch     9 | loss: 11.0058689Losses:  10.004361152648926 4.016512870788574 -0.0
CurrentTrain: epoch  6, batch    10 | loss: 10.0043612Losses:  11.559213638305664 5.063666343688965 -0.0
CurrentTrain: epoch  6, batch    11 | loss: 11.5592136Losses:  9.923896789550781 4.174312591552734 -0.0
CurrentTrain: epoch  6, batch    12 | loss: 9.9238968Losses:  11.476613998413086 5.794156551361084 -0.0
CurrentTrain: epoch  6, batch    13 | loss: 11.4766140Losses:  10.50594711303711 4.278727054595947 -0.0
CurrentTrain: epoch  6, batch    14 | loss: 10.5059471Losses:  9.898641586303711 4.323912620544434 -0.0
CurrentTrain: epoch  6, batch    15 | loss: 9.8986416Losses:  13.282705307006836 6.759347915649414 -0.0
CurrentTrain: epoch  6, batch    16 | loss: 13.2827053Losses:  12.541927337646484 6.790966987609863 -0.0
CurrentTrain: epoch  6, batch    17 | loss: 12.5419273Losses:  11.5656156539917 5.329923152923584 -0.0
CurrentTrain: epoch  6, batch    18 | loss: 11.5656157Losses:  12.765180587768555 6.939658164978027 -0.0
CurrentTrain: epoch  6, batch    19 | loss: 12.7651806Losses:  15.644393920898438 8.73744010925293 -0.0
CurrentTrain: epoch  6, batch    20 | loss: 15.6443939Losses:  11.521110534667969 4.2828779220581055 -0.0
CurrentTrain: epoch  6, batch    21 | loss: 11.5211105Losses:  11.991358757019043 5.597567558288574 -0.0
CurrentTrain: epoch  6, batch    22 | loss: 11.9913588Losses:  10.168198585510254 3.755863904953003 -0.0
CurrentTrain: epoch  6, batch    23 | loss: 10.1681986Losses:  11.833185195922852 6.267510414123535 -0.0
CurrentTrain: epoch  6, batch    24 | loss: 11.8331852Losses:  15.616840362548828 8.148209571838379 -0.0
CurrentTrain: epoch  6, batch    25 | loss: 15.6168404Losses:  14.940370559692383 7.967078685760498 -0.0
CurrentTrain: epoch  6, batch    26 | loss: 14.9403706Losses:  11.997435569763184 6.096098899841309 -0.0
CurrentTrain: epoch  6, batch    27 | loss: 11.9974356Losses:  12.917049407958984 6.7894110679626465 -0.0
CurrentTrain: epoch  6, batch    28 | loss: 12.9170494Losses:  12.64517593383789 6.6475725173950195 -0.0
CurrentTrain: epoch  6, batch    29 | loss: 12.6451759Losses:  12.525120735168457 6.215219497680664 -0.0
CurrentTrain: epoch  6, batch    30 | loss: 12.5251207Losses:  14.437755584716797 6.714638710021973 -0.0
CurrentTrain: epoch  6, batch    31 | loss: 14.4377556Losses:  10.800213813781738 5.279626846313477 -0.0
CurrentTrain: epoch  6, batch    32 | loss: 10.8002138Losses:  11.747827529907227 4.968986511230469 -0.0
CurrentTrain: epoch  6, batch    33 | loss: 11.7478275Losses:  10.438431739807129 4.906018257141113 -0.0
CurrentTrain: epoch  6, batch    34 | loss: 10.4384317Losses:  10.411903381347656 4.232782363891602 -0.0
CurrentTrain: epoch  6, batch    35 | loss: 10.4119034Losses:  10.643579483032227 4.4836554527282715 -0.0
CurrentTrain: epoch  6, batch    36 | loss: 10.6435795Losses:  7.034775257110596 0.502267062664032 -0.0
CurrentTrain: epoch  6, batch    37 | loss: 7.0347753Losses:  18.459115982055664 11.340572357177734 -0.0
CurrentTrain: epoch  7, batch     0 | loss: 18.4591160Losses:  10.085184097290039 4.380746364593506 -0.0
CurrentTrain: epoch  7, batch     1 | loss: 10.0851841Losses:  12.820067405700684 6.876091003417969 -0.0
CurrentTrain: epoch  7, batch     2 | loss: 12.8200674Losses:  11.82792854309082 6.892887115478516 -0.0
CurrentTrain: epoch  7, batch     3 | loss: 11.8279285Losses:  12.166813850402832 5.900861740112305 -0.0
CurrentTrain: epoch  7, batch     4 | loss: 12.1668139Losses:  10.880205154418945 5.3256683349609375 -0.0
CurrentTrain: epoch  7, batch     5 | loss: 10.8802052Losses:  11.311391830444336 4.803691387176514 -0.0
CurrentTrain: epoch  7, batch     6 | loss: 11.3113918Losses:  10.255412101745605 4.644718170166016 -0.0
CurrentTrain: epoch  7, batch     7 | loss: 10.2554121Losses:  12.086668968200684 6.103064060211182 -0.0
CurrentTrain: epoch  7, batch     8 | loss: 12.0866690Losses:  11.616254806518555 6.018451690673828 -0.0
CurrentTrain: epoch  7, batch     9 | loss: 11.6162548Losses:  10.697004318237305 4.729192733764648 -0.0
CurrentTrain: epoch  7, batch    10 | loss: 10.6970043Losses:  12.08432388305664 5.752371788024902 -0.0
CurrentTrain: epoch  7, batch    11 | loss: 12.0843239Losses:  10.468228340148926 4.49196720123291 -0.0
CurrentTrain: epoch  7, batch    12 | loss: 10.4682283Losses:  11.830930709838867 5.5779218673706055 -0.0
CurrentTrain: epoch  7, batch    13 | loss: 11.8309307Losses:  13.22970962524414 7.401015758514404 -0.0
CurrentTrain: epoch  7, batch    14 | loss: 13.2297096Losses:  9.866267204284668 4.138466835021973 -0.0
CurrentTrain: epoch  7, batch    15 | loss: 9.8662672Losses:  10.01021957397461 4.494651794433594 -0.0
CurrentTrain: epoch  7, batch    16 | loss: 10.0102196Losses:  11.002747535705566 5.712023735046387 -0.0
CurrentTrain: epoch  7, batch    17 | loss: 11.0027475Losses:  10.9463529586792 5.496161460876465 -0.0
CurrentTrain: epoch  7, batch    18 | loss: 10.9463530Losses:  11.653575897216797 6.139019012451172 -0.0
CurrentTrain: epoch  7, batch    19 | loss: 11.6535759Losses:  12.450311660766602 5.909857273101807 -0.0
CurrentTrain: epoch  7, batch    20 | loss: 12.4503117Losses:  10.482772827148438 5.135659217834473 -0.0
CurrentTrain: epoch  7, batch    21 | loss: 10.4827728Losses:  13.048089027404785 5.514956951141357 -0.0
CurrentTrain: epoch  7, batch    22 | loss: 13.0480890Losses:  15.944955825805664 9.5030517578125 -0.0
CurrentTrain: epoch  7, batch    23 | loss: 15.9449558Losses:  10.837669372558594 5.444874286651611 -0.0
CurrentTrain: epoch  7, batch    24 | loss: 10.8376694Losses:  12.235133171081543 6.681118011474609 -0.0
CurrentTrain: epoch  7, batch    25 | loss: 12.2351332Losses:  14.799457550048828 8.851336479187012 -0.0
CurrentTrain: epoch  7, batch    26 | loss: 14.7994576Losses:  11.688630104064941 6.102565765380859 -0.0
CurrentTrain: epoch  7, batch    27 | loss: 11.6886301Losses:  14.652366638183594 7.871180534362793 -0.0
CurrentTrain: epoch  7, batch    28 | loss: 14.6523666Losses:  10.079581260681152 4.747052192687988 -0.0
CurrentTrain: epoch  7, batch    29 | loss: 10.0795813Losses:  10.349418640136719 4.811605453491211 -0.0
CurrentTrain: epoch  7, batch    30 | loss: 10.3494186Losses:  11.499105453491211 6.000450611114502 -0.0
CurrentTrain: epoch  7, batch    31 | loss: 11.4991055Losses:  9.835744857788086 4.3478240966796875 -0.0
CurrentTrain: epoch  7, batch    32 | loss: 9.8357449Losses:  10.353445053100586 5.29832649230957 -0.0
CurrentTrain: epoch  7, batch    33 | loss: 10.3534451Losses:  11.656904220581055 6.593899726867676 -0.0
CurrentTrain: epoch  7, batch    34 | loss: 11.6569042Losses:  8.659388542175293 3.502539873123169 -0.0
CurrentTrain: epoch  7, batch    35 | loss: 8.6593885Losses:  10.538361549377441 5.3056488037109375 -0.0
CurrentTrain: epoch  7, batch    36 | loss: 10.5383615Losses:  6.6233062744140625 1.5790214538574219 -0.0
CurrentTrain: epoch  7, batch    37 | loss: 6.6233063Losses:  11.168295860290527 6.136012554168701 -0.0
CurrentTrain: epoch  8, batch     0 | loss: 11.1682959Losses:  9.311370849609375 4.225313186645508 -0.0
CurrentTrain: epoch  8, batch     1 | loss: 9.3113708Losses:  14.576095581054688 9.770105361938477 -0.0
CurrentTrain: epoch  8, batch     2 | loss: 14.5760956Losses:  9.240439414978027 4.234133720397949 -0.0
CurrentTrain: epoch  8, batch     3 | loss: 9.2404394Losses:  9.212385177612305 4.226428985595703 -0.0
CurrentTrain: epoch  8, batch     4 | loss: 9.2123852Losses:  9.780877113342285 4.713907241821289 -0.0
CurrentTrain: epoch  8, batch     5 | loss: 9.7808771Losses:  10.963882446289062 6.09468936920166 -0.0
CurrentTrain: epoch  8, batch     6 | loss: 10.9638824Losses:  11.906119346618652 6.996827125549316 -0.0
CurrentTrain: epoch  8, batch     7 | loss: 11.9061193Losses:  10.536499977111816 5.622941017150879 -0.0
CurrentTrain: epoch  8, batch     8 | loss: 10.5365000Losses:  9.96550464630127 4.183094024658203 -0.0
CurrentTrain: epoch  8, batch     9 | loss: 9.9655046Losses:  12.452486038208008 6.197786331176758 -0.0
CurrentTrain: epoch  8, batch    10 | loss: 12.4524860Losses:  11.144857406616211 6.03754997253418 -0.0
CurrentTrain: epoch  8, batch    11 | loss: 11.1448574Losses:  9.46464729309082 4.282462120056152 -0.0
CurrentTrain: epoch  8, batch    12 | loss: 9.4646473Losses:  10.123777389526367 5.077143669128418 -0.0
CurrentTrain: epoch  8, batch    13 | loss: 10.1237774Losses:  11.115795135498047 5.022884368896484 -0.0
CurrentTrain: epoch  8, batch    14 | loss: 11.1157951Losses:  9.955265045166016 4.620253562927246 -0.0
CurrentTrain: epoch  8, batch    15 | loss: 9.9552650Losses:  12.166105270385742 7.113696575164795 -0.0
CurrentTrain: epoch  8, batch    16 | loss: 12.1661053Losses:  14.165877342224121 8.984969139099121 -0.0
CurrentTrain: epoch  8, batch    17 | loss: 14.1658773Losses:  10.09269905090332 4.626902103424072 -0.0
CurrentTrain: epoch  8, batch    18 | loss: 10.0926991Losses:  11.103739738464355 6.102225303649902 -0.0
CurrentTrain: epoch  8, batch    19 | loss: 11.1037397Losses:  11.81498908996582 6.591500282287598 -0.0
CurrentTrain: epoch  8, batch    20 | loss: 11.8149891Losses:  10.236043930053711 4.74565315246582 -0.0
CurrentTrain: epoch  8, batch    21 | loss: 10.2360439Losses:  12.425580978393555 7.4362006187438965 -0.0
CurrentTrain: epoch  8, batch    22 | loss: 12.4255810Losses:  10.804533004760742 4.997996807098389 -0.0
CurrentTrain: epoch  8, batch    23 | loss: 10.8045330Losses:  11.216658592224121 5.821281433105469 -0.0
CurrentTrain: epoch  8, batch    24 | loss: 11.2166586Losses:  9.772985458374023 4.8770751953125 -0.0
CurrentTrain: epoch  8, batch    25 | loss: 9.7729855Losses:  13.14980697631836 6.857101917266846 -0.0
CurrentTrain: epoch  8, batch    26 | loss: 13.1498070Losses:  11.508935928344727 6.416773796081543 -0.0
CurrentTrain: epoch  8, batch    27 | loss: 11.5089359Losses:  10.103865623474121 5.217667579650879 -0.0
CurrentTrain: epoch  8, batch    28 | loss: 10.1038656Losses:  13.73055648803711 8.318792343139648 -0.0
CurrentTrain: epoch  8, batch    29 | loss: 13.7305565Losses:  9.978338241577148 5.114237308502197 -0.0
CurrentTrain: epoch  8, batch    30 | loss: 9.9783382Losses:  14.734003067016602 9.710765838623047 -0.0
CurrentTrain: epoch  8, batch    31 | loss: 14.7340031Losses:  10.796515464782715 5.991267204284668 -0.0
CurrentTrain: epoch  8, batch    32 | loss: 10.7965155Losses:  9.525245666503906 4.5679144859313965 -0.0
CurrentTrain: epoch  8, batch    33 | loss: 9.5252457Losses:  8.66230297088623 3.817373514175415 -0.0
CurrentTrain: epoch  8, batch    34 | loss: 8.6623030Losses:  10.247469902038574 5.629602432250977 -0.0
CurrentTrain: epoch  8, batch    35 | loss: 10.2474699Losses:  8.18745231628418 3.347158908843994 -0.0
CurrentTrain: epoch  8, batch    36 | loss: 8.1874523Losses:  6.31785774230957 1.5727264881134033 -0.0
CurrentTrain: epoch  8, batch    37 | loss: 6.3178577Losses:  10.5366792678833 5.197908401489258 -0.0
CurrentTrain: epoch  9, batch     0 | loss: 10.5366793Losses:  14.357378005981445 8.93831729888916 -0.0
CurrentTrain: epoch  9, batch     1 | loss: 14.3573780Losses:  8.72050666809082 3.859128475189209 -0.0
CurrentTrain: epoch  9, batch     2 | loss: 8.7205067Losses:  9.941374778747559 5.175079345703125 -0.0
CurrentTrain: epoch  9, batch     3 | loss: 9.9413748Losses:  9.93570327758789 5.025386333465576 -0.0
CurrentTrain: epoch  9, batch     4 | loss: 9.9357033Losses:  9.882505416870117 5.051599502563477 -0.0
CurrentTrain: epoch  9, batch     5 | loss: 9.8825054Losses:  10.081932067871094 5.252397060394287 -0.0
CurrentTrain: epoch  9, batch     6 | loss: 10.0819321Losses:  9.530211448669434 4.684225082397461 -0.0
CurrentTrain: epoch  9, batch     7 | loss: 9.5302114Losses:  8.283108711242676 3.453617811203003 -0.0
CurrentTrain: epoch  9, batch     8 | loss: 8.2831087Losses:  14.526751518249512 9.678115844726562 -0.0
CurrentTrain: epoch  9, batch     9 | loss: 14.5267515Losses:  10.853387832641602 5.974891185760498 -0.0
CurrentTrain: epoch  9, batch    10 | loss: 10.8533878Losses:  12.334237098693848 7.576651573181152 -0.0
CurrentTrain: epoch  9, batch    11 | loss: 12.3342371Losses:  9.124235153198242 4.214437484741211 -0.0
CurrentTrain: epoch  9, batch    12 | loss: 9.1242352Losses:  9.543428421020508 4.6999664306640625 -0.0
CurrentTrain: epoch  9, batch    13 | loss: 9.5434284Losses:  10.559741973876953 5.73345422744751 -0.0
CurrentTrain: epoch  9, batch    14 | loss: 10.5597420Losses:  9.852227210998535 4.50064754486084 -0.0
CurrentTrain: epoch  9, batch    15 | loss: 9.8522272Losses:  10.710283279418945 5.939416885375977 -0.0
CurrentTrain: epoch  9, batch    16 | loss: 10.7102833Losses:  13.509897232055664 7.77384614944458 -0.0
CurrentTrain: epoch  9, batch    17 | loss: 13.5098972Losses:  10.913269996643066 6.017583847045898 -0.0
CurrentTrain: epoch  9, batch    18 | loss: 10.9132700Losses:  10.520723342895508 5.828102111816406 -0.0
CurrentTrain: epoch  9, batch    19 | loss: 10.5207233Losses:  10.287590026855469 5.527010917663574 -0.0
CurrentTrain: epoch  9, batch    20 | loss: 10.2875900Losses:  10.784153938293457 6.046614646911621 -0.0
CurrentTrain: epoch  9, batch    21 | loss: 10.7841539Losses:  11.816865921020508 7.080754280090332 -0.0
CurrentTrain: epoch  9, batch    22 | loss: 11.8168659Losses:  13.235184669494629 8.072059631347656 -0.0
CurrentTrain: epoch  9, batch    23 | loss: 13.2351847Losses:  10.735182762145996 5.9003190994262695 -0.0
CurrentTrain: epoch  9, batch    24 | loss: 10.7351828Losses:  9.817144393920898 5.078609943389893 -0.0
CurrentTrain: epoch  9, batch    25 | loss: 9.8171444Losses:  11.777097702026367 6.206031322479248 -0.0
CurrentTrain: epoch  9, batch    26 | loss: 11.7770977Losses:  10.883344650268555 6.1302947998046875 -0.0
CurrentTrain: epoch  9, batch    27 | loss: 10.8833447Losses:  8.904050827026367 4.182191848754883 -0.0
CurrentTrain: epoch  9, batch    28 | loss: 8.9040508Losses:  10.943111419677734 6.12785005569458 -0.0
CurrentTrain: epoch  9, batch    29 | loss: 10.9431114Losses:  13.551786422729492 8.874063491821289 -0.0
CurrentTrain: epoch  9, batch    30 | loss: 13.5517864Losses:  11.828797340393066 7.026386260986328 -0.0
CurrentTrain: epoch  9, batch    31 | loss: 11.8287973Losses:  9.857305526733398 5.1513800621032715 -0.0
CurrentTrain: epoch  9, batch    32 | loss: 9.8573055Losses:  11.047952651977539 5.350949287414551 -0.0
CurrentTrain: epoch  9, batch    33 | loss: 11.0479527Losses:  15.40216064453125 7.560880661010742 -0.0
CurrentTrain: epoch  9, batch    34 | loss: 15.4021606Losses:  9.9282865524292 5.070971488952637 -0.0
CurrentTrain: epoch  9, batch    35 | loss: 9.9282866Losses:  11.17648696899414 6.123800277709961 -0.0
CurrentTrain: epoch  9, batch    36 | loss: 11.1764870Losses:  6.318080425262451 0.7195076942443848 -0.0
CurrentTrain: epoch  9, batch    37 | loss: 6.3180804
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 37.50%,  total acc: 53.12%   [EVAL] batch:    2 | acc: 68.75%,  total acc: 58.33%   [EVAL] batch:    3 | acc: 43.75%,  total acc: 54.69%   [EVAL] batch:    4 | acc: 50.00%,  total acc: 53.75%   [EVAL] batch:    5 | acc: 62.50%,  total acc: 55.21%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 58.93%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 64.06%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 68.06%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 70.62%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 73.30%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 75.52%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 76.92%   [EVAL] batch:   13 | acc: 75.00%,  total acc: 76.79%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 76.67%   [EVAL] batch:   15 | acc: 75.00%,  total acc: 76.56%   [EVAL] batch:   16 | acc: 81.25%,  total acc: 76.84%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 76.39%   [EVAL] batch:   18 | acc: 87.50%,  total acc: 76.97%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 77.81%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 78.87%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 79.83%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 80.71%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 82.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 82.69%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 83.10%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 83.71%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 84.27%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 84.17%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 84.48%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 84.77%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 83.33%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 37.50%,  total acc: 53.12%   [EVAL] batch:    2 | acc: 68.75%,  total acc: 58.33%   [EVAL] batch:    3 | acc: 43.75%,  total acc: 54.69%   [EVAL] batch:    4 | acc: 50.00%,  total acc: 53.75%   [EVAL] batch:    5 | acc: 62.50%,  total acc: 55.21%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 58.93%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 64.06%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 68.06%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 70.62%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 73.30%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 75.52%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 76.92%   [EVAL] batch:   13 | acc: 75.00%,  total acc: 76.79%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 76.67%   [EVAL] batch:   15 | acc: 75.00%,  total acc: 76.56%   [EVAL] batch:   16 | acc: 81.25%,  total acc: 76.84%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 76.39%   [EVAL] batch:   18 | acc: 87.50%,  total acc: 76.97%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 77.81%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 78.87%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 79.83%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 80.71%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 82.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 82.69%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 83.10%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 83.71%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 84.27%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 84.17%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 84.48%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 84.77%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 83.33%   
cur_acc:  ['0.8333']
his_acc:  ['0.8333']
Clustering into  4  clusters
Clusters:  [3 0 2 3 0 3 1 3 1 0 3]
Losses:  16.834346771240234 8.283613204956055 1.428198218345642
CurrentTrain: epoch  0, batch     0 | loss: 16.8343468Losses:  12.270187377929688 2.738111734390259 1.38971745967865
CurrentTrain: epoch  0, batch     1 | loss: 12.2701874Losses:  16.40036392211914 8.033567428588867 1.4383816719055176
CurrentTrain: epoch  1, batch     0 | loss: 16.4003639Losses:  8.508703231811523 2.2987372875213623 1.4238083362579346
CurrentTrain: epoch  1, batch     1 | loss: 8.5087032Losses:  13.52771282196045 7.009099960327148 1.4036455154418945
CurrentTrain: epoch  2, batch     0 | loss: 13.5277128Losses:  8.681018829345703 2.5243582725524902 1.4178466796875
CurrentTrain: epoch  2, batch     1 | loss: 8.6810188Losses:  14.620630264282227 8.282312393188477 1.400250792503357
CurrentTrain: epoch  3, batch     0 | loss: 14.6206303Losses:  7.649350166320801 3.1640117168426514 -0.0
CurrentTrain: epoch  3, batch     1 | loss: 7.6493502Losses:  12.373390197753906 6.641977310180664 1.4527411460876465
CurrentTrain: epoch  4, batch     0 | loss: 12.3733902Losses:  6.934383392333984 1.9475091695785522 1.4228246212005615
CurrentTrain: epoch  4, batch     1 | loss: 6.9343834Losses:  11.810395240783691 7.389638900756836 1.4256781339645386
CurrentTrain: epoch  5, batch     0 | loss: 11.8103952Losses:  7.044252395629883 2.498063802719116 1.4888046979904175
CurrentTrain: epoch  5, batch     1 | loss: 7.0442524Losses:  11.017047882080078 6.363428115844727 1.4216502904891968
CurrentTrain: epoch  6, batch     0 | loss: 11.0170479Losses:  5.980978488922119 1.9659407138824463 1.405615210533142
CurrentTrain: epoch  6, batch     1 | loss: 5.9809785Losses:  10.645416259765625 6.764513969421387 1.4194540977478027
CurrentTrain: epoch  7, batch     0 | loss: 10.6454163Losses:  7.699522972106934 3.2938156127929688 1.482271432876587
CurrentTrain: epoch  7, batch     1 | loss: 7.6995230Losses:  11.299885749816895 7.142139434814453 1.4222030639648438
CurrentTrain: epoch  8, batch     0 | loss: 11.2998857Losses:  6.649583339691162 2.9748377799987793 1.4000810384750366
CurrentTrain: epoch  8, batch     1 | loss: 6.6495833Losses:  9.291412353515625 5.449665546417236 1.390122652053833
CurrentTrain: epoch  9, batch     0 | loss: 9.2914124Losses:  4.6923699378967285 1.206765055656433 1.4067338705062866
CurrentTrain: epoch  9, batch     1 | loss: 4.6923699
Losses:  7.388863563537598 -0.0 3.917029857635498
MemoryTrain:  epoch  0, batch     0 | loss: 7.3888636Losses:  6.7942352294921875 -0.0 3.8498103618621826
MemoryTrain:  epoch  1, batch     0 | loss: 6.7942352Losses:  6.14207649230957 -0.0 3.775669813156128
MemoryTrain:  epoch  2, batch     0 | loss: 6.1420765Losses:  4.755201816558838 -0.0 3.746009349822998
MemoryTrain:  epoch  3, batch     0 | loss: 4.7552018Losses:  3.9086406230926514 -0.0 3.6491968631744385
MemoryTrain:  epoch  4, batch     0 | loss: 3.9086406Losses:  4.219430446624756 -0.0 3.6265904903411865
MemoryTrain:  epoch  5, batch     0 | loss: 4.2194304Losses:  4.363661289215088 -0.0 3.5876710414886475
MemoryTrain:  epoch  6, batch     0 | loss: 4.3636613Losses:  4.432307720184326 -0.0 3.548840045928955
MemoryTrain:  epoch  7, batch     0 | loss: 4.4323077Losses:  4.2335309982299805 -0.0 3.555173397064209
MemoryTrain:  epoch  8, batch     0 | loss: 4.2335310Losses:  3.869880199432373 -0.0 3.4647464752197266
MemoryTrain:  epoch  9, batch     0 | loss: 3.8698802
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 75.00%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 79.17%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 78.12%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 84.38%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 84.82%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 85.16%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 84.72%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 83.75%   [EVAL] batch:   10 | acc: 50.00%,  total acc: 80.68%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 80.21%   [EVAL] batch:   12 | acc: 62.50%,  total acc: 78.85%   [EVAL] batch:   13 | acc: 6.25%,  total acc: 73.66%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 62.50%,  total acc: 68.75%   [EVAL] batch:    2 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    3 | acc: 62.50%,  total acc: 67.19%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 68.75%   [EVAL] batch:    5 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 70.54%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 74.22%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 76.39%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 78.12%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 80.11%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 80.73%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 81.73%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 80.80%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 80.42%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 78.91%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 78.68%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 77.78%   [EVAL] batch:   18 | acc: 75.00%,  total acc: 77.63%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 78.12%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 79.17%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 80.11%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 80.98%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 81.77%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 82.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 83.17%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 83.56%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 84.15%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 84.70%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 84.58%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 84.88%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 85.16%   [EVAL] batch:   32 | acc: 87.50%,  total acc: 85.23%   [EVAL] batch:   33 | acc: 68.75%,  total acc: 84.74%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 85.00%   [EVAL] batch:   35 | acc: 75.00%,  total acc: 84.72%   [EVAL] batch:   36 | acc: 81.25%,  total acc: 84.63%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 85.03%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 85.42%   [EVAL] batch:   39 | acc: 81.25%,  total acc: 85.31%   [EVAL] batch:   40 | acc: 87.50%,  total acc: 85.37%   [EVAL] batch:   41 | acc: 81.25%,  total acc: 85.27%   [EVAL] batch:   42 | acc: 43.75%,  total acc: 84.30%   [EVAL] batch:   43 | acc: 68.75%,  total acc: 83.95%   [EVAL] batch:   44 | acc: 81.25%,  total acc: 83.89%   [EVAL] batch:   45 | acc: 18.75%,  total acc: 82.47%   [EVAL] batch:   46 | acc: 0.00%,  total acc: 80.72%   
cur_acc:  ['0.8333', '0.7366']
his_acc:  ['0.8333', '0.8072']
Clustering into  7  clusters
Clusters:  [5 0 6 5 0 5 2 5 2 0 5 5 4 3 1 0]
Losses:  21.83236312866211 7.945693016052246 6.177879333496094
CurrentTrain: epoch  0, batch     0 | loss: 21.8323631Losses:  15.15475845336914 3.898651123046875 3.6699509620666504
CurrentTrain: epoch  0, batch     1 | loss: 15.1547585Losses:  18.274232864379883 6.248223304748535 5.972926616668701
CurrentTrain: epoch  1, batch     0 | loss: 18.2742329Losses:  18.200719833374023 3.6941986083984375 5.976905345916748
CurrentTrain: epoch  1, batch     1 | loss: 18.2007198Losses:  20.226192474365234 7.547521114349365 5.962620735168457
CurrentTrain: epoch  2, batch     0 | loss: 20.2261925Losses:  13.713926315307617 1.9215550422668457 6.153625011444092
CurrentTrain: epoch  2, batch     1 | loss: 13.7139263Losses:  18.24387550354004 7.105842113494873 5.944904327392578
CurrentTrain: epoch  3, batch     0 | loss: 18.2438755Losses:  13.222494125366211 1.8299859762191772 5.723560333251953
CurrentTrain: epoch  3, batch     1 | loss: 13.2224941Losses:  17.912403106689453 6.519264221191406 5.735324859619141
CurrentTrain: epoch  4, batch     0 | loss: 17.9124031Losses:  13.104755401611328 2.7220065593719482 5.667275905609131
CurrentTrain: epoch  4, batch     1 | loss: 13.1047554Losses:  17.072134017944336 6.285456657409668 5.858184337615967
CurrentTrain: epoch  5, batch     0 | loss: 17.0721340Losses:  14.141979217529297 3.3053219318389893 5.719655990600586
CurrentTrain: epoch  5, batch     1 | loss: 14.1419792Losses:  15.967658996582031 5.996663570404053 5.684146881103516
CurrentTrain: epoch  6, batch     0 | loss: 15.9676590Losses:  12.962430953979492 1.8084886074066162 5.848346710205078
CurrentTrain: epoch  6, batch     1 | loss: 12.9624310Losses:  19.14852523803711 8.174544334411621 5.723759174346924
CurrentTrain: epoch  7, batch     0 | loss: 19.1485252Losses:  11.682048797607422 3.0277442932128906 5.605818271636963
CurrentTrain: epoch  7, batch     1 | loss: 11.6820488Losses:  16.103620529174805 6.419004440307617 5.627723693847656
CurrentTrain: epoch  8, batch     0 | loss: 16.1036205Losses:  11.11082649230957 2.887477397918701 3.389610528945923
CurrentTrain: epoch  8, batch     1 | loss: 11.1108265Losses:  15.718643188476562 7.3944830894470215 5.623233318328857
CurrentTrain: epoch  9, batch     0 | loss: 15.7186432Losses:  14.050192832946777 4.8486785888671875 3.358351707458496
CurrentTrain: epoch  9, batch     1 | loss: 14.0501928
Losses:  13.059049606323242 -0.0 10.972432136535645
MemoryTrain:  epoch  0, batch     0 | loss: 13.0590496Losses:  13.545698165893555 -0.0 11.096261978149414
MemoryTrain:  epoch  1, batch     0 | loss: 13.5456982Losses:  13.00330924987793 -0.0 10.990134239196777
MemoryTrain:  epoch  2, batch     0 | loss: 13.0033092Losses:  12.88441276550293 -0.0 10.923946380615234
MemoryTrain:  epoch  3, batch     0 | loss: 12.8844128Losses:  12.525545120239258 -0.0 10.900227546691895
MemoryTrain:  epoch  4, batch     0 | loss: 12.5255451Losses:  12.479066848754883 -0.0 10.928750991821289
MemoryTrain:  epoch  5, batch     0 | loss: 12.4790668Losses:  12.307043075561523 -0.0 10.901633262634277
MemoryTrain:  epoch  6, batch     0 | loss: 12.3070431Losses:  12.44898796081543 -0.0 10.9210844039917
MemoryTrain:  epoch  7, batch     0 | loss: 12.4489880Losses:  11.887661933898926 -0.0 10.865381240844727
MemoryTrain:  epoch  8, batch     0 | loss: 11.8876619Losses:  11.850774765014648 -0.0 10.881009101867676
MemoryTrain:  epoch  9, batch     0 | loss: 11.8507748
[EVAL] batch:    0 | acc: 56.25%,  total acc: 56.25%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 75.00%   [EVAL] batch:    2 | acc: 25.00%,  total acc: 58.33%   [EVAL] batch:    3 | acc: 18.75%,  total acc: 48.44%   [EVAL] batch:    4 | acc: 18.75%,  total acc: 42.50%   [EVAL] batch:    5 | acc: 31.25%,  total acc: 40.62%   [EVAL] batch:    6 | acc: 12.50%,  total acc: 36.61%   [EVAL] batch:    7 | acc: 6.25%,  total acc: 32.81%   
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    1 | acc: 43.75%,  total acc: 53.12%   [EVAL] batch:    2 | acc: 62.50%,  total acc: 56.25%   [EVAL] batch:    3 | acc: 43.75%,  total acc: 53.12%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 56.25%   [EVAL] batch:    5 | acc: 62.50%,  total acc: 57.29%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 59.82%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 64.06%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 66.67%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 67.50%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 68.75%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 70.31%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 71.15%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 70.98%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 71.25%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 70.31%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 70.59%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 70.14%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 69.74%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 70.31%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 71.73%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 73.01%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 74.18%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 75.26%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 76.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 77.16%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 77.78%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 78.57%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 79.31%   [EVAL] batch:   29 | acc: 93.75%,  total acc: 79.79%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 80.44%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 81.05%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 81.44%   [EVAL] batch:   33 | acc: 62.50%,  total acc: 80.88%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:   35 | acc: 75.00%,  total acc: 81.08%   [EVAL] batch:   36 | acc: 81.25%,  total acc: 81.08%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 81.73%   [EVAL] batch:   39 | acc: 81.25%,  total acc: 81.72%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 82.16%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 82.44%   [EVAL] batch:   42 | acc: 87.50%,  total acc: 82.56%   [EVAL] batch:   43 | acc: 75.00%,  total acc: 82.39%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 82.78%   [EVAL] batch:   45 | acc: 18.75%,  total acc: 81.39%   [EVAL] batch:   46 | acc: 50.00%,  total acc: 80.72%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 81.12%   [EVAL] batch:   48 | acc: 25.00%,  total acc: 79.97%   [EVAL] batch:   49 | acc: 18.75%,  total acc: 78.75%   [EVAL] batch:   50 | acc: 18.75%,  total acc: 77.57%   [EVAL] batch:   51 | acc: 31.25%,  total acc: 76.68%   [EVAL] batch:   52 | acc: 12.50%,  total acc: 75.47%   [EVAL] batch:   53 | acc: 6.25%,  total acc: 74.19%   
cur_acc:  ['0.8333', '0.7366', '0.3281']
his_acc:  ['0.8333', '0.8072', '0.7419']
Clustering into  9  clusters
Clusters:  [0 2 3 0 2 0 8 0 1 2 0 0 7 5 4 2 6 3 0 1 0]
Losses:  15.434223175048828 7.900637149810791 3.4298574924468994
CurrentTrain: epoch  0, batch     0 | loss: 15.4342232Losses:  11.135409355163574 3.5540249347686768 3.3329436779022217
CurrentTrain: epoch  0, batch     1 | loss: 11.1354094Losses:  13.066202163696289 6.3775954246521 3.365067958831787
CurrentTrain: epoch  1, batch     0 | loss: 13.0662022Losses:  8.17519760131836 1.3415868282318115 3.3800597190856934
CurrentTrain: epoch  1, batch     1 | loss: 8.1751976Losses:  14.027073860168457 7.441234111785889 3.349883794784546
CurrentTrain: epoch  2, batch     0 | loss: 14.0270739Losses:  9.360206604003906 3.0283596515655518 3.3495020866394043
CurrentTrain: epoch  2, batch     1 | loss: 9.3602066Losses:  13.478677749633789 6.523153305053711 3.5327677726745605
CurrentTrain: epoch  3, batch     0 | loss: 13.4786777Losses:  8.15218734741211 1.7318090200424194 3.3519396781921387
CurrentTrain: epoch  3, batch     1 | loss: 8.1521873Losses:  12.573225021362305 6.130372524261475 3.3615188598632812
CurrentTrain: epoch  4, batch     0 | loss: 12.5732250Losses:  7.35554838180542 1.7380200624465942 3.3369364738464355
CurrentTrain: epoch  4, batch     1 | loss: 7.3555484Losses:  12.55344009399414 6.657441139221191 3.3384194374084473
CurrentTrain: epoch  5, batch     0 | loss: 12.5534401Losses:  5.621560096740723 2.1064324378967285 1.4238193035125732
CurrentTrain: epoch  5, batch     1 | loss: 5.6215601Losses:  12.622171401977539 6.842011451721191 3.3535382747650146
CurrentTrain: epoch  6, batch     0 | loss: 12.6221714Losses:  8.355399131774902 2.942301034927368 3.354830265045166
CurrentTrain: epoch  6, batch     1 | loss: 8.3553991Losses:  11.35934829711914 5.834482192993164 3.3609354496002197
CurrentTrain: epoch  7, batch     0 | loss: 11.3593483Losses:  6.863919258117676 1.6254929304122925 3.3367438316345215
CurrentTrain: epoch  7, batch     1 | loss: 6.8639193Losses:  13.010324478149414 6.548063278198242 3.3634982109069824
CurrentTrain: epoch  8, batch     0 | loss: 13.0103245Losses:  7.75508975982666 1.7116905450820923 3.3418374061584473
CurrentTrain: epoch  8, batch     1 | loss: 7.7550898Losses:  12.430961608886719 5.961358070373535 3.3591535091400146
CurrentTrain: epoch  9, batch     0 | loss: 12.4309616Losses:  7.096254348754883 1.9329838752746582 3.337136745452881
CurrentTrain: epoch  9, batch     1 | loss: 7.0962543
Losses:  17.046039581298828 -0.0 13.93989086151123
MemoryTrain:  epoch  0, batch     0 | loss: 17.0460396Losses:  5.016233444213867 -0.0 3.365593910217285
MemoryTrain:  epoch  0, batch     1 | loss: 5.0162334Losses:  13.620326042175293 -0.0 10.928035736083984
MemoryTrain:  epoch  1, batch     0 | loss: 13.6203260Losses:  4.777379035949707 -0.0 3.345099925994873
MemoryTrain:  epoch  1, batch     1 | loss: 4.7773790Losses:  16.59556770324707 -0.0 14.001388549804688
MemoryTrain:  epoch  2, batch     0 | loss: 16.5955677Losses:  4.230289459228516 -0.0 3.333327531814575
MemoryTrain:  epoch  2, batch     1 | loss: 4.2302895Losses:  18.325239181518555 -0.0 16.9507999420166
MemoryTrain:  epoch  3, batch     0 | loss: 18.3252392Losses:  6.3224639892578125 -0.0 3.351309061050415
MemoryTrain:  epoch  3, batch     1 | loss: 6.3224640Losses:  12.429165840148926 -0.0 10.97926139831543
MemoryTrain:  epoch  4, batch     0 | loss: 12.4291658Losses:  4.986761569976807 -0.0 3.312227725982666
MemoryTrain:  epoch  4, batch     1 | loss: 4.9867616Losses:  14.81710147857666 -0.0 13.750370979309082
MemoryTrain:  epoch  5, batch     0 | loss: 14.8171015Losses:  3.6246142387390137 -0.0 1.4893198013305664
MemoryTrain:  epoch  5, batch     1 | loss: 3.6246142Losses:  12.097736358642578 -0.0 10.958148956298828
MemoryTrain:  epoch  6, batch     0 | loss: 12.0977364Losses:  4.273515701293945 -0.0 3.3071351051330566
MemoryTrain:  epoch  6, batch     1 | loss: 4.2735157Losses:  14.586185455322266 -0.0 13.735115051269531
MemoryTrain:  epoch  7, batch     0 | loss: 14.5861855Losses:  0.7618528008460999 -0.0 -0.0
MemoryTrain:  epoch  7, batch     1 | loss: 0.7618528Losses:  14.759263038635254 -0.0 13.970521926879883
MemoryTrain:  epoch  8, batch     0 | loss: 14.7592630Losses:  5.596408843994141 -0.0 5.5819878578186035
MemoryTrain:  epoch  8, batch     1 | loss: 5.5964088Losses:  14.082735061645508 -0.0 13.723917007446289
MemoryTrain:  epoch  9, batch     0 | loss: 14.0827351Losses:  4.674812316894531 -0.0 3.5032434463500977
MemoryTrain:  epoch  9, batch     1 | loss: 4.6748123
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 90.62%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 89.58%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 84.38%   [EVAL] batch:    4 | acc: 25.00%,  total acc: 72.50%   [EVAL] batch:    5 | acc: 18.75%,  total acc: 63.54%   [EVAL] batch:    6 | acc: 18.75%,  total acc: 57.14%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 60.16%   [EVAL] batch:    8 | acc: 43.75%,  total acc: 58.33%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 58.75%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 61.36%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 63.54%   [EVAL] batch:   12 | acc: 50.00%,  total acc: 62.50%   
[EVAL] batch:    0 | acc: 56.25%,  total acc: 56.25%   [EVAL] batch:    1 | acc: 37.50%,  total acc: 46.88%   [EVAL] batch:    2 | acc: 37.50%,  total acc: 43.75%   [EVAL] batch:    3 | acc: 25.00%,  total acc: 39.06%   [EVAL] batch:    4 | acc: 56.25%,  total acc: 42.50%   [EVAL] batch:    5 | acc: 43.75%,  total acc: 42.71%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 47.32%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 53.12%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 58.33%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 61.25%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 64.20%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 66.15%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 67.79%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 67.86%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 68.33%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 67.58%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 68.01%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 67.71%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 67.43%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 68.44%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 69.94%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 71.31%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 72.28%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 73.18%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 74.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 75.24%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 75.93%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 76.79%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 77.59%   [EVAL] batch:   29 | acc: 93.75%,  total acc: 78.12%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 78.63%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 79.30%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 79.73%   [EVAL] batch:   33 | acc: 6.25%,  total acc: 77.57%   [EVAL] batch:   34 | acc: 18.75%,  total acc: 75.89%   [EVAL] batch:   35 | acc: 0.00%,  total acc: 73.78%   [EVAL] batch:   36 | acc: 6.25%,  total acc: 71.96%   [EVAL] batch:   37 | acc: 6.25%,  total acc: 70.23%   [EVAL] batch:   38 | acc: 12.50%,  total acc: 68.75%   [EVAL] batch:   39 | acc: 75.00%,  total acc: 68.91%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 69.66%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 70.24%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 70.78%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 71.02%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 71.67%   [EVAL] batch:   45 | acc: 18.75%,  total acc: 70.52%   [EVAL] batch:   46 | acc: 68.75%,  total acc: 70.48%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 71.09%   [EVAL] batch:   48 | acc: 12.50%,  total acc: 69.90%   [EVAL] batch:   49 | acc: 6.25%,  total acc: 68.62%   [EVAL] batch:   50 | acc: 6.25%,  total acc: 67.40%   [EVAL] batch:   51 | acc: 18.75%,  total acc: 66.47%   [EVAL] batch:   52 | acc: 0.00%,  total acc: 65.21%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 65.51%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 65.91%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 66.29%   [EVAL] batch:   56 | acc: 87.50%,  total acc: 66.67%   [EVAL] batch:   57 | acc: 25.00%,  total acc: 65.95%   [EVAL] batch:   58 | acc: 12.50%,  total acc: 65.04%   [EVAL] batch:   59 | acc: 25.00%,  total acc: 64.38%   [EVAL] batch:   60 | acc: 62.50%,  total acc: 64.34%   [EVAL] batch:   61 | acc: 56.25%,  total acc: 64.21%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 63.99%   [EVAL] batch:   63 | acc: 87.50%,  total acc: 64.36%   [EVAL] batch:   64 | acc: 87.50%,  total acc: 64.71%   [EVAL] batch:   65 | acc: 68.75%,  total acc: 64.77%   
cur_acc:  ['0.8333', '0.7366', '0.3281', '0.6250']
his_acc:  ['0.8333', '0.8072', '0.7419', '0.6477']
Clustering into  12  clusters
Clusters:  [ 2  0  1  2  0  8  9  2  3  0  5  2 10 11  4  0  6  1  8  3  5  1  1  7
  8  3]
Losses:  21.89000129699707 9.349848747253418 3.795480251312256
CurrentTrain: epoch  0, batch     0 | loss: 21.8900013Losses:  13.678430557250977 2.5410819053649902 3.844522714614868
CurrentTrain: epoch  0, batch     1 | loss: 13.6784306Losses:  21.019990921020508 9.223402976989746 3.817023992538452
CurrentTrain: epoch  1, batch     0 | loss: 21.0199909Losses:  13.859593391418457 3.6734652519226074 3.7018749713897705
CurrentTrain: epoch  1, batch     1 | loss: 13.8595934Losses:  21.09343147277832 9.574213981628418 3.7566604614257812
CurrentTrain: epoch  2, batch     0 | loss: 21.0934315Losses:  12.113802909851074 3.1236391067504883 3.4885854721069336
CurrentTrain: epoch  2, batch     1 | loss: 12.1138029Losses:  18.28096580505371 8.076407432556152 3.6049156188964844
CurrentTrain: epoch  3, batch     0 | loss: 18.2809658Losses:  12.055886268615723 2.2644991874694824 3.6718361377716064
CurrentTrain: epoch  3, batch     1 | loss: 12.0558863Losses:  16.76126480102539 7.845519542694092 3.641737222671509
CurrentTrain: epoch  4, batch     0 | loss: 16.7612648Losses:  13.690065383911133 4.517251968383789 1.605090856552124
CurrentTrain: epoch  4, batch     1 | loss: 13.6900654Losses:  18.74149513244629 8.74916934967041 3.629859209060669
CurrentTrain: epoch  5, batch     0 | loss: 18.7414951Losses:  11.125115394592285 2.820013999938965 3.4506022930145264
CurrentTrain: epoch  5, batch     1 | loss: 11.1251154Losses:  16.810178756713867 8.081127166748047 3.5339176654815674
CurrentTrain: epoch  6, batch     0 | loss: 16.8101788Losses:  10.96381664276123 3.802560567855835 1.518808126449585
CurrentTrain: epoch  6, batch     1 | loss: 10.9638166Losses:  15.499129295349121 7.2965240478515625 3.5186052322387695
CurrentTrain: epoch  7, batch     0 | loss: 15.4991293Losses:  11.562057495117188 2.795332431793213 3.547168254852295
CurrentTrain: epoch  7, batch     1 | loss: 11.5620575Losses:  15.61418342590332 7.507922172546387 3.519646167755127
CurrentTrain: epoch  8, batch     0 | loss: 15.6141834Losses:  9.933034896850586 2.2274954319000244 3.393336772918701
CurrentTrain: epoch  8, batch     1 | loss: 9.9330349Losses:  14.044745445251465 6.783680438995361 3.459803581237793
CurrentTrain: epoch  9, batch     0 | loss: 14.0447454Losses:  10.548957824707031 2.7732937335968018 3.418506145477295
CurrentTrain: epoch  9, batch     1 | loss: 10.5489578
Losses:  15.970547676086426 -0.0 13.831948280334473
MemoryTrain:  epoch  0, batch     0 | loss: 15.9705477Losses:  14.424799919128418 -0.0 13.884517669677734
MemoryTrain:  epoch  0, batch     1 | loss: 14.4247999Losses:  18.252124786376953 -0.0 16.793344497680664
MemoryTrain:  epoch  1, batch     0 | loss: 18.2521248Losses:  15.358290672302246 -0.0 13.827390670776367
MemoryTrain:  epoch  1, batch     1 | loss: 15.3582907Losses:  24.190832138061523 -0.0 23.237472534179688
MemoryTrain:  epoch  2, batch     0 | loss: 24.1908321Losses:  12.315540313720703 -0.0 11.003076553344727
MemoryTrain:  epoch  2, batch     1 | loss: 12.3155403Losses:  15.027148246765137 -0.0 13.733640670776367
MemoryTrain:  epoch  3, batch     0 | loss: 15.0271482Losses:  11.560662269592285 -0.0 10.851533889770508
MemoryTrain:  epoch  3, batch     1 | loss: 11.5606623Losses:  20.701696395874023 -0.0 19.924758911132812
MemoryTrain:  epoch  4, batch     0 | loss: 20.7016964Losses:  8.35615348815918 -0.0 8.214786529541016
MemoryTrain:  epoch  4, batch     1 | loss: 8.3561535Losses:  16.997243881225586 -0.0 16.753416061401367
MemoryTrain:  epoch  5, batch     0 | loss: 16.9972439Losses:  14.10916805267334 -0.0 13.794095993041992
MemoryTrain:  epoch  5, batch     1 | loss: 14.1091681Losses:  23.197988510131836 -0.0 23.138639450073242
MemoryTrain:  epoch  6, batch     0 | loss: 23.1979885Losses:  6.0994415283203125 -0.0 5.711277961730957
MemoryTrain:  epoch  6, batch     1 | loss: 6.0994415Losses:  14.010933876037598 -0.0 13.750492095947266
MemoryTrain:  epoch  7, batch     0 | loss: 14.0109339Losses:  13.778769493103027 -0.0 13.675130844116211
MemoryTrain:  epoch  7, batch     1 | loss: 13.7787695Losses:  23.307601928710938 -0.0 23.171762466430664
MemoryTrain:  epoch  8, batch     0 | loss: 23.3076019Losses:  11.075234413146973 -0.0 10.892332077026367
MemoryTrain:  epoch  8, batch     1 | loss: 11.0752344Losses:  19.979936599731445 -0.0 19.895376205444336
MemoryTrain:  epoch  9, batch     0 | loss: 19.9799366Losses:  10.848053932189941 -0.0 10.815733909606934
MemoryTrain:  epoch  9, batch     1 | loss: 10.8480539
[EVAL] batch:    0 | acc: 43.75%,  total acc: 43.75%   [EVAL] batch:    1 | acc: 62.50%,  total acc: 53.12%   [EVAL] batch:    2 | acc: 37.50%,  total acc: 47.92%   [EVAL] batch:    3 | acc: 12.50%,  total acc: 39.06%   [EVAL] batch:    4 | acc: 12.50%,  total acc: 33.75%   [EVAL] batch:    5 | acc: 6.25%,  total acc: 29.17%   [EVAL] batch:    6 | acc: 12.50%,  total acc: 26.79%   [EVAL] batch:    7 | acc: 62.50%,  total acc: 31.25%   [EVAL] batch:    8 | acc: 37.50%,  total acc: 31.94%   [EVAL] batch:    9 | acc: 43.75%,  total acc: 33.12%   [EVAL] batch:   10 | acc: 62.50%,  total acc: 35.80%   [EVAL] batch:   11 | acc: 50.00%,  total acc: 36.98%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 39.90%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 43.75%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 45.83%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 49.22%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 51.84%   [EVAL] batch:   17 | acc: 87.50%,  total acc: 53.82%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 54.61%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 55.62%   [EVAL] batch:   20 | acc: 68.75%,  total acc: 56.25%   [EVAL] batch:   21 | acc: 37.50%,  total acc: 55.40%   
[EVAL] batch:    0 | acc: 0.00%,  total acc: 0.00%   [EVAL] batch:    1 | acc: 6.25%,  total acc: 3.12%   [EVAL] batch:    2 | acc: 0.00%,  total acc: 2.08%   [EVAL] batch:    3 | acc: 6.25%,  total acc: 3.12%   [EVAL] batch:    4 | acc: 6.25%,  total acc: 3.75%   [EVAL] batch:    5 | acc: 6.25%,  total acc: 4.17%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 14.29%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 25.00%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 33.33%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 38.75%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 43.18%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 46.88%   [EVAL] batch:   12 | acc: 50.00%,  total acc: 47.12%   [EVAL] batch:   13 | acc: 31.25%,  total acc: 45.98%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 47.92%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 48.44%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 50.00%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 50.69%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 51.32%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 53.12%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 55.36%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 57.39%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 58.97%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 60.42%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 62.00%   [EVAL] batch:   25 | acc: 93.75%,  total acc: 63.22%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 64.35%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 65.62%   [EVAL] batch:   28 | acc: 81.25%,  total acc: 66.16%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 66.67%   [EVAL] batch:   30 | acc: 75.00%,  total acc: 66.94%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 67.77%   [EVAL] batch:   32 | acc: 87.50%,  total acc: 68.37%   [EVAL] batch:   33 | acc: 6.25%,  total acc: 66.54%   [EVAL] batch:   34 | acc: 0.00%,  total acc: 64.64%   [EVAL] batch:   35 | acc: 0.00%,  total acc: 62.85%   [EVAL] batch:   36 | acc: 0.00%,  total acc: 61.15%   [EVAL] batch:   37 | acc: 0.00%,  total acc: 59.54%   [EVAL] batch:   38 | acc: 6.25%,  total acc: 58.17%   [EVAL] batch:   39 | acc: 62.50%,  total acc: 58.28%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 59.30%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 59.97%   [EVAL] batch:   42 | acc: 87.50%,  total acc: 60.61%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 61.08%   [EVAL] batch:   44 | acc: 56.25%,  total acc: 60.97%   [EVAL] batch:   45 | acc: 18.75%,  total acc: 60.05%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 60.51%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 61.33%   [EVAL] batch:   48 | acc: 18.75%,  total acc: 60.46%   [EVAL] batch:   49 | acc: 12.50%,  total acc: 59.50%   [EVAL] batch:   50 | acc: 12.50%,  total acc: 58.58%   [EVAL] batch:   51 | acc: 25.00%,  total acc: 57.93%   [EVAL] batch:   52 | acc: 6.25%,  total acc: 56.96%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 57.41%   [EVAL] batch:   54 | acc: 68.75%,  total acc: 57.61%   [EVAL] batch:   55 | acc: 81.25%,  total acc: 58.04%   [EVAL] batch:   56 | acc: 81.25%,  total acc: 58.44%   [EVAL] batch:   57 | acc: 0.00%,  total acc: 57.44%   [EVAL] batch:   58 | acc: 6.25%,  total acc: 56.57%   [EVAL] batch:   59 | acc: 0.00%,  total acc: 55.62%   [EVAL] batch:   60 | acc: 37.50%,  total acc: 55.33%   [EVAL] batch:   61 | acc: 12.50%,  total acc: 54.64%   [EVAL] batch:   62 | acc: 6.25%,  total acc: 53.87%   [EVAL] batch:   63 | acc: 31.25%,  total acc: 53.52%   [EVAL] batch:   64 | acc: 87.50%,  total acc: 54.04%   [EVAL] batch:   65 | acc: 75.00%,  total acc: 54.36%   [EVAL] batch:   66 | acc: 43.75%,  total acc: 54.20%   [EVAL] batch:   67 | acc: 62.50%,  total acc: 54.32%   [EVAL] batch:   68 | acc: 25.00%,  total acc: 53.89%   [EVAL] batch:   69 | acc: 12.50%,  total acc: 53.30%   [EVAL] batch:   70 | acc: 18.75%,  total acc: 52.82%   [EVAL] batch:   71 | acc: 0.00%,  total acc: 52.08%   [EVAL] batch:   72 | acc: 18.75%,  total acc: 51.63%   [EVAL] batch:   73 | acc: 68.75%,  total acc: 51.86%   [EVAL] batch:   74 | acc: 37.50%,  total acc: 51.67%   [EVAL] batch:   75 | acc: 56.25%,  total acc: 51.73%   [EVAL] batch:   76 | acc: 37.50%,  total acc: 51.54%   [EVAL] batch:   77 | acc: 62.50%,  total acc: 51.68%   [EVAL] batch:   78 | acc: 81.25%,  total acc: 52.06%   [EVAL] batch:   79 | acc: 93.75%,  total acc: 52.58%   [EVAL] batch:   80 | acc: 81.25%,  total acc: 52.93%   [EVAL] batch:   81 | acc: 100.00%,  total acc: 53.51%   [EVAL] batch:   82 | acc: 87.50%,  total acc: 53.92%   [EVAL] batch:   83 | acc: 93.75%,  total acc: 54.39%   [EVAL] batch:   84 | acc: 56.25%,  total acc: 54.41%   [EVAL] batch:   85 | acc: 75.00%,  total acc: 54.65%   [EVAL] batch:   86 | acc: 75.00%,  total acc: 54.89%   [EVAL] batch:   87 | acc: 18.75%,  total acc: 54.47%   
cur_acc:  ['0.8333', '0.7366', '0.3281', '0.6250', '0.5540']
his_acc:  ['0.8333', '0.8072', '0.7419', '0.6477', '0.5447']
Clustering into  14  clusters
Clusters:  [ 3  1  0 11 12  9  8  3  4  1  3  3 13  6  2  1 10  7  9  4  3  0  0  5
  9  4  2  8  3  0  7]
Losses:  18.185020446777344 7.547286510467529 5.6140875816345215
CurrentTrain: epoch  0, batch     0 | loss: 18.1850204Losses:  13.466293334960938 2.4488372802734375 5.666886329650879
CurrentTrain: epoch  0, batch     1 | loss: 13.4662933Losses:  20.24969482421875 10.408793449401855 5.637784957885742
CurrentTrain: epoch  1, batch     0 | loss: 20.2496948Losses:  11.610536575317383 6.4651618003845215 1.4088863134384155
CurrentTrain: epoch  1, batch     1 | loss: 11.6105366Losses:  17.317363739013672 8.15260124206543 5.611068248748779
CurrentTrain: epoch  2, batch     0 | loss: 17.3173637Losses:  10.374187469482422 3.9905128479003906 3.3438260555267334
CurrentTrain: epoch  2, batch     1 | loss: 10.3741875Losses:  15.932186126708984 7.561840057373047 5.615598201751709
CurrentTrain: epoch  3, batch     0 | loss: 15.9321861Losses:  9.258934020996094 3.09971284866333 3.345160961151123
CurrentTrain: epoch  3, batch     1 | loss: 9.2589340Losses:  15.035981178283691 6.908012866973877 5.6145429611206055
CurrentTrain: epoch  4, batch     0 | loss: 15.0359812Losses:  7.799300670623779 2.2434234619140625 3.316375255584717
CurrentTrain: epoch  4, batch     1 | loss: 7.7993007Losses:  13.790383338928223 5.952905654907227 5.617401123046875
CurrentTrain: epoch  5, batch     0 | loss: 13.7903833Losses:  9.768244743347168 1.9240281581878662 5.569103240966797
CurrentTrain: epoch  5, batch     1 | loss: 9.7682447Losses:  13.352867126464844 5.614679336547852 5.590907573699951
CurrentTrain: epoch  6, batch     0 | loss: 13.3528671Losses:  8.70028305053711 1.1623671054840088 5.575346946716309
CurrentTrain: epoch  6, batch     1 | loss: 8.7002831Losses:  13.790947914123535 6.210675239562988 5.5954389572143555
CurrentTrain: epoch  7, batch     0 | loss: 13.7909479Losses:  7.395424842834473 2.132884979248047 3.313690185546875
CurrentTrain: epoch  7, batch     1 | loss: 7.3954248Losses:  13.311515808105469 5.821779251098633 5.5725932121276855
CurrentTrain: epoch  8, batch     0 | loss: 13.3115158Losses:  9.142568588256836 1.6979990005493164 5.5716705322265625
CurrentTrain: epoch  8, batch     1 | loss: 9.1425686Losses:  13.252059936523438 5.790590286254883 5.564948558807373
CurrentTrain: epoch  9, batch     0 | loss: 13.2520599Losses:  9.075727462768555 1.6869689226150513 5.568140983581543
CurrentTrain: epoch  9, batch     1 | loss: 9.0757275
Losses:  20.957534790039062 -0.0 19.964941024780273
MemoryTrain:  epoch  0, batch     0 | loss: 20.9575348Losses:  18.272682189941406 -0.0 16.956212997436523
MemoryTrain:  epoch  0, batch     1 | loss: 18.2726822Losses:  18.14702796936035 -0.0 16.723791122436523
MemoryTrain:  epoch  1, batch     0 | loss: 18.1470280Losses:  17.781089782714844 -0.0 16.874040603637695
MemoryTrain:  epoch  1, batch     1 | loss: 17.7810898Losses:  27.474767684936523 -0.0 26.56410789489746
MemoryTrain:  epoch  2, batch     0 | loss: 27.4747677Losses:  17.465911865234375 -0.0 16.75950050354004
MemoryTrain:  epoch  2, batch     1 | loss: 17.4659119Losses:  20.547012329101562 -0.0 20.032514572143555
MemoryTrain:  epoch  3, batch     0 | loss: 20.5470123Losses:  20.504114151000977 -0.0 19.958581924438477
MemoryTrain:  epoch  3, batch     1 | loss: 20.5041142Losses:  20.455625534057617 -0.0 20.016332626342773
MemoryTrain:  epoch  4, batch     0 | loss: 20.4556255Losses:  23.338237762451172 -0.0 23.136371612548828
MemoryTrain:  epoch  4, batch     1 | loss: 23.3382378Losses:  13.907283782958984 -0.0 13.718095779418945
MemoryTrain:  epoch  5, batch     0 | loss: 13.9072838Losses:  13.81444263458252 -0.0 13.670971870422363
MemoryTrain:  epoch  5, batch     1 | loss: 13.8144426Losses:  16.761995315551758 -0.0 16.72301483154297
MemoryTrain:  epoch  6, batch     0 | loss: 16.7619953Losses:  20.08429718017578 -0.0 19.94814109802246
MemoryTrain:  epoch  6, batch     1 | loss: 20.0842972Losses:  16.782835006713867 -0.0 16.73761749267578
MemoryTrain:  epoch  7, batch     0 | loss: 16.7828350Losses:  23.213787078857422 -0.0 23.185874938964844
MemoryTrain:  epoch  7, batch     1 | loss: 23.2137871Losses:  19.959613800048828 -0.0 19.904996871948242
MemoryTrain:  epoch  8, batch     0 | loss: 19.9596138Losses:  26.613391876220703 -0.0 26.498388290405273
MemoryTrain:  epoch  8, batch     1 | loss: 26.6133919Losses:  19.911903381347656 -0.0 19.8715877532959
MemoryTrain:  epoch  9, batch     0 | loss: 19.9119034Losses:  10.920117378234863 -0.0 10.832314491271973
MemoryTrain:  epoch  9, batch     1 | loss: 10.9201174
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 100.00%,  total acc: 89.58%   [EVAL] batch:    3 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:    4 | acc: 100.00%,  total acc: 92.50%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 94.64%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 95.31%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 95.14%   [EVAL] batch:    9 | acc: 43.75%,  total acc: 90.00%   [EVAL] batch:   10 | acc: 75.00%,  total acc: 88.64%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 88.02%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 87.02%   [EVAL] batch:   13 | acc: 50.00%,  total acc: 84.38%   
[EVAL] batch:    0 | acc: 6.25%,  total acc: 6.25%   [EVAL] batch:    1 | acc: 6.25%,  total acc: 6.25%   [EVAL] batch:    2 | acc: 6.25%,  total acc: 6.25%   [EVAL] batch:    3 | acc: 0.00%,  total acc: 4.69%   [EVAL] batch:    4 | acc: 18.75%,  total acc: 7.50%   [EVAL] batch:    5 | acc: 6.25%,  total acc: 7.29%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 16.96%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 27.34%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 34.72%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 40.00%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 44.32%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 47.92%   [EVAL] batch:   12 | acc: 50.00%,  total acc: 48.08%   [EVAL] batch:   13 | acc: 25.00%,  total acc: 46.43%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 47.92%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 48.44%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 50.00%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 50.69%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 51.32%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 53.12%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 55.36%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 57.39%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 58.97%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 60.42%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 62.00%   [EVAL] batch:   25 | acc: 93.75%,  total acc: 63.22%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 64.35%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 65.62%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 66.81%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 67.29%   [EVAL] batch:   30 | acc: 81.25%,  total acc: 67.74%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 68.55%   [EVAL] batch:   32 | acc: 75.00%,  total acc: 68.75%   [EVAL] batch:   33 | acc: 12.50%,  total acc: 67.10%   [EVAL] batch:   34 | acc: 0.00%,  total acc: 65.18%   [EVAL] batch:   35 | acc: 0.00%,  total acc: 63.37%   [EVAL] batch:   36 | acc: 0.00%,  total acc: 61.66%   [EVAL] batch:   37 | acc: 0.00%,  total acc: 60.03%   [EVAL] batch:   38 | acc: 6.25%,  total acc: 58.65%   [EVAL] batch:   39 | acc: 68.75%,  total acc: 58.91%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 59.91%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 60.71%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 61.48%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 61.93%   [EVAL] batch:   44 | acc: 75.00%,  total acc: 62.22%   [EVAL] batch:   45 | acc: 18.75%,  total acc: 61.28%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 61.70%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 62.50%   [EVAL] batch:   48 | acc: 18.75%,  total acc: 61.61%   [EVAL] batch:   49 | acc: 12.50%,  total acc: 60.62%   [EVAL] batch:   50 | acc: 6.25%,  total acc: 59.56%   [EVAL] batch:   51 | acc: 25.00%,  total acc: 58.89%   [EVAL] batch:   52 | acc: 0.00%,  total acc: 57.78%   [EVAL] batch:   53 | acc: 75.00%,  total acc: 58.10%   [EVAL] batch:   54 | acc: 75.00%,  total acc: 58.41%   [EVAL] batch:   55 | acc: 75.00%,  total acc: 58.71%   [EVAL] batch:   56 | acc: 81.25%,  total acc: 59.10%   [EVAL] batch:   57 | acc: 0.00%,  total acc: 58.08%   [EVAL] batch:   58 | acc: 6.25%,  total acc: 57.20%   [EVAL] batch:   59 | acc: 0.00%,  total acc: 56.25%   [EVAL] batch:   60 | acc: 37.50%,  total acc: 55.94%   [EVAL] batch:   61 | acc: 0.00%,  total acc: 55.04%   [EVAL] batch:   62 | acc: 0.00%,  total acc: 54.17%   [EVAL] batch:   63 | acc: 25.00%,  total acc: 53.71%   [EVAL] batch:   64 | acc: 68.75%,  total acc: 53.94%   [EVAL] batch:   65 | acc: 43.75%,  total acc: 53.79%   [EVAL] batch:   66 | acc: 25.00%,  total acc: 53.36%   [EVAL] batch:   67 | acc: 18.75%,  total acc: 52.85%   [EVAL] batch:   68 | acc: 25.00%,  total acc: 52.45%   [EVAL] batch:   69 | acc: 6.25%,  total acc: 51.79%   [EVAL] batch:   70 | acc: 12.50%,  total acc: 51.23%   [EVAL] batch:   71 | acc: 6.25%,  total acc: 50.61%   [EVAL] batch:   72 | acc: 25.00%,  total acc: 50.26%   [EVAL] batch:   73 | acc: 37.50%,  total acc: 50.08%   [EVAL] batch:   74 | acc: 50.00%,  total acc: 50.08%   [EVAL] batch:   75 | acc: 31.25%,  total acc: 49.84%   [EVAL] batch:   76 | acc: 56.25%,  total acc: 49.92%   [EVAL] batch:   77 | acc: 56.25%,  total acc: 50.00%   [EVAL] batch:   78 | acc: 68.75%,  total acc: 50.24%   [EVAL] batch:   79 | acc: 87.50%,  total acc: 50.70%   [EVAL] batch:   80 | acc: 81.25%,  total acc: 51.08%   [EVAL] batch:   81 | acc: 100.00%,  total acc: 51.68%   [EVAL] batch:   82 | acc: 87.50%,  total acc: 52.11%   [EVAL] batch:   83 | acc: 100.00%,  total acc: 52.68%   [EVAL] batch:   84 | acc: 62.50%,  total acc: 52.79%   [EVAL] batch:   85 | acc: 87.50%,  total acc: 53.20%   [EVAL] batch:   86 | acc: 75.00%,  total acc: 53.45%   [EVAL] batch:   87 | acc: 93.75%,  total acc: 53.91%   [EVAL] batch:   88 | acc: 75.00%,  total acc: 54.14%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 54.65%   [EVAL] batch:   90 | acc: 100.00%,  total acc: 55.15%   [EVAL] batch:   91 | acc: 93.75%,  total acc: 55.57%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 56.05%   [EVAL] batch:   93 | acc: 100.00%,  total acc: 56.52%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 56.97%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 57.42%   [EVAL] batch:   96 | acc: 43.75%,  total acc: 57.28%   [EVAL] batch:   97 | acc: 68.75%,  total acc: 57.40%   [EVAL] batch:   98 | acc: 81.25%,  total acc: 57.64%   [EVAL] batch:   99 | acc: 81.25%,  total acc: 57.88%   [EVAL] batch:  100 | acc: 62.50%,  total acc: 57.92%   
cur_acc:  ['0.8333', '0.7366', '0.3281', '0.6250', '0.5540', '0.8438']
his_acc:  ['0.8333', '0.8072', '0.7419', '0.6477', '0.5447', '0.5792']
Clustering into  17  clusters
Clusters:  [ 2  0 11 12 15  8  6  2  7  0  2  2 13 10  1  0  3  5  8 14  2 16 16  4
  8  7  1  6  2 11  5  2  3  9  4  7]
Losses:  21.50745391845703 9.820725440979004 5.781379699707031
CurrentTrain: epoch  0, batch     0 | loss: 21.5074539Losses:  14.923958778381348 6.337411403656006 1.5268793106079102
CurrentTrain: epoch  0, batch     1 | loss: 14.9239588Losses:  19.358720779418945 7.830914497375488 5.849628925323486
CurrentTrain: epoch  1, batch     0 | loss: 19.3587208Losses:  12.494590759277344 2.433943510055542 5.598690986633301
CurrentTrain: epoch  1, batch     1 | loss: 12.4945908Losses:  16.8203125 6.8502326011657715 5.709244728088379
CurrentTrain: epoch  2, batch     0 | loss: 16.8203125Losses:  14.243297576904297 2.9541468620300293 5.623696804046631
CurrentTrain: epoch  2, batch     1 | loss: 14.2432976Losses:  16.941741943359375 6.998040199279785 5.726837635040283
CurrentTrain: epoch  3, batch     0 | loss: 16.9417419Losses:  12.868353843688965 2.6987063884735107 5.674951553344727
CurrentTrain: epoch  3, batch     1 | loss: 12.8683538Losses:  16.675065994262695 7.1621012687683105 5.656301021575928
CurrentTrain: epoch  4, batch     0 | loss: 16.6750660Losses:  11.642143249511719 2.1949708461761475 5.639405727386475
CurrentTrain: epoch  4, batch     1 | loss: 11.6421432Losses:  17.149187088012695 7.743561744689941 5.648953914642334
CurrentTrain: epoch  5, batch     0 | loss: 17.1491871Losses:  9.35070514678955 2.6013388633728027 3.343149423599243
CurrentTrain: epoch  5, batch     1 | loss: 9.3507051Losses:  15.681318283081055 6.827550888061523 5.63997220993042
CurrentTrain: epoch  6, batch     0 | loss: 15.6813183Losses:  11.239189147949219 2.364193916320801 5.625026702880859
CurrentTrain: epoch  6, batch     1 | loss: 11.2391891Losses:  15.122817993164062 6.63360595703125 5.630152225494385
CurrentTrain: epoch  7, batch     0 | loss: 15.1228180Losses:  11.75184440612793 2.5181686878204346 5.604267120361328
CurrentTrain: epoch  7, batch     1 | loss: 11.7518444Losses:  15.893623352050781 7.044896602630615 5.591529846191406
CurrentTrain: epoch  8, batch     0 | loss: 15.8936234Losses:  8.915739059448242 2.595659017562866 3.324953556060791
CurrentTrain: epoch  8, batch     1 | loss: 8.9157391Losses:  14.764168739318848 6.293753623962402 5.589249610900879
CurrentTrain: epoch  9, batch     0 | loss: 14.7641687Losses:  10.667867660522461 2.3003032207489014 5.602111339569092
CurrentTrain: epoch  9, batch     1 | loss: 10.6678677
Losses:  17.03359031677246 -0.0 16.74479103088379
MemoryTrain:  epoch  0, batch     0 | loss: 17.0335903Losses:  30.076162338256836 -0.0 29.989770889282227
MemoryTrain:  epoch  0, batch     1 | loss: 30.0761623Losses:  3.3181071281433105 -0.0 3.3116936683654785
MemoryTrain:  epoch  0, batch     2 | loss: 3.3181071Losses:  23.453203201293945 -0.0 23.235849380493164
MemoryTrain:  epoch  1, batch     0 | loss: 23.4532032Losses:  23.791303634643555 -0.0 23.125545501708984
MemoryTrain:  epoch  1, batch     1 | loss: 23.7913036Losses:  1.50139582157135 -0.0 1.3959921598434448
MemoryTrain:  epoch  1, batch     2 | loss: 1.5013958Losses:  16.814620971679688 -0.0 16.736818313598633
MemoryTrain:  epoch  2, batch     0 | loss: 16.8146210Losses:  26.60154151916504 -0.0 26.536405563354492
MemoryTrain:  epoch  2, batch     1 | loss: 26.6015415Losses:  3.3831541538238525 -0.0 3.302947998046875
MemoryTrain:  epoch  2, batch     2 | loss: 3.3831542Losses:  20.070158004760742 -0.0 19.938861846923828
MemoryTrain:  epoch  3, batch     0 | loss: 20.0701580Losses:  26.563282012939453 -0.0 26.511371612548828
MemoryTrain:  epoch  3, batch     1 | loss: 26.5632820Losses:  3.3620214462280273 -0.0 3.3187215328216553
MemoryTrain:  epoch  3, batch     2 | loss: 3.3620214Losses:  30.019454956054688 -0.0 29.997028350830078
MemoryTrain:  epoch  4, batch     0 | loss: 30.0194550Losses:  19.97823143005371 -0.0 19.872989654541016
MemoryTrain:  epoch  4, batch     1 | loss: 19.9782314Losses:  3.329890012741089 -0.0 3.305471181869507
MemoryTrain:  epoch  4, batch     2 | loss: 3.3298900Losses:  23.173473358154297 -0.0 23.136320114135742
MemoryTrain:  epoch  5, batch     0 | loss: 23.1734734Losses:  19.942955017089844 -0.0 19.90741539001465
MemoryTrain:  epoch  5, batch     1 | loss: 19.9429550Losses:  0.043983861804008484 -0.0 -0.0
MemoryTrain:  epoch  5, batch     2 | loss: 0.0439839Losses:  19.884403228759766 -0.0 19.848411560058594
MemoryTrain:  epoch  6, batch     0 | loss: 19.8844032Losses:  19.937454223632812 -0.0 19.89436149597168
MemoryTrain:  epoch  6, batch     1 | loss: 19.9374542Losses:  1.400942087173462 -0.0 1.393027663230896
MemoryTrain:  epoch  6, batch     2 | loss: 1.4009421Losses:  29.9506778717041 -0.0 29.92375946044922
MemoryTrain:  epoch  7, batch     0 | loss: 29.9506779Losses:  19.902448654174805 -0.0 19.876989364624023
MemoryTrain:  epoch  7, batch     1 | loss: 19.9024487Losses:  3.3368070125579834 -0.0 3.3170320987701416
MemoryTrain:  epoch  7, batch     2 | loss: 3.3368070Losses:  23.146207809448242 -0.0 23.106889724731445
MemoryTrain:  epoch  8, batch     0 | loss: 23.1462078Losses:  26.49944305419922 -0.0 26.479633331298828
MemoryTrain:  epoch  8, batch     1 | loss: 26.4994431Losses:  3.323183536529541 -0.0 3.307954788208008
MemoryTrain:  epoch  8, batch     2 | loss: 3.3231835Losses:  23.1678466796875 -0.0 23.149133682250977
MemoryTrain:  epoch  9, batch     0 | loss: 23.1678467Losses:  19.911890029907227 -0.0 19.881027221679688
MemoryTrain:  epoch  9, batch     1 | loss: 19.9118900Losses:  3.3268063068389893 -0.0 3.310612916946411
MemoryTrain:  epoch  9, batch     2 | loss: 3.3268063
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 93.75%,  total acc: 85.94%   [EVAL] batch:    4 | acc: 100.00%,  total acc: 88.75%   [EVAL] batch:    5 | acc: 68.75%,  total acc: 85.42%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 84.82%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 85.16%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 85.42%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 83.75%   [EVAL] batch:   10 | acc: 62.50%,  total acc: 81.82%   [EVAL] batch:   11 | acc: 37.50%,  total acc: 78.12%   [EVAL] batch:   12 | acc: 56.25%,  total acc: 76.44%   [EVAL] batch:   13 | acc: 56.25%,  total acc: 75.00%   [EVAL] batch:   14 | acc: 31.25%,  total acc: 72.08%   
[EVAL] batch:    0 | acc: 0.00%,  total acc: 0.00%   [EVAL] batch:    1 | acc: 6.25%,  total acc: 3.12%   [EVAL] batch:    2 | acc: 0.00%,  total acc: 2.08%   [EVAL] batch:    3 | acc: 0.00%,  total acc: 1.56%   [EVAL] batch:    4 | acc: 0.00%,  total acc: 1.25%   [EVAL] batch:    5 | acc: 6.25%,  total acc: 2.08%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 12.50%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 23.44%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 31.94%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 37.50%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 42.61%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 46.35%   [EVAL] batch:   12 | acc: 50.00%,  total acc: 46.63%   [EVAL] batch:   13 | acc: 25.00%,  total acc: 45.09%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 46.67%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 47.27%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 48.90%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 49.65%   [EVAL] batch:   18 | acc: 56.25%,  total acc: 50.00%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 51.25%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 53.57%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 55.68%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 57.34%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 58.85%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 60.50%   [EVAL] batch:   25 | acc: 93.75%,  total acc: 61.78%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 62.96%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 64.29%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 65.52%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 66.25%   [EVAL] batch:   30 | acc: 75.00%,  total acc: 66.53%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 67.38%   [EVAL] batch:   32 | acc: 75.00%,  total acc: 67.61%   [EVAL] batch:   33 | acc: 12.50%,  total acc: 65.99%   [EVAL] batch:   34 | acc: 0.00%,  total acc: 64.11%   [EVAL] batch:   35 | acc: 0.00%,  total acc: 62.33%   [EVAL] batch:   36 | acc: 0.00%,  total acc: 60.64%   [EVAL] batch:   37 | acc: 0.00%,  total acc: 59.05%   [EVAL] batch:   38 | acc: 6.25%,  total acc: 57.69%   [EVAL] batch:   39 | acc: 62.50%,  total acc: 57.81%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 58.69%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 59.52%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 60.32%   [EVAL] batch:   43 | acc: 75.00%,  total acc: 60.65%   [EVAL] batch:   44 | acc: 62.50%,  total acc: 60.69%   [EVAL] batch:   45 | acc: 18.75%,  total acc: 59.78%   [EVAL] batch:   46 | acc: 68.75%,  total acc: 59.97%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 60.81%   [EVAL] batch:   48 | acc: 18.75%,  total acc: 59.95%   [EVAL] batch:   49 | acc: 12.50%,  total acc: 59.00%   [EVAL] batch:   50 | acc: 6.25%,  total acc: 57.97%   [EVAL] batch:   51 | acc: 25.00%,  total acc: 57.33%   [EVAL] batch:   52 | acc: 6.25%,  total acc: 56.37%   [EVAL] batch:   53 | acc: 62.50%,  total acc: 56.48%   [EVAL] batch:   54 | acc: 68.75%,  total acc: 56.70%   [EVAL] batch:   55 | acc: 68.75%,  total acc: 56.92%   [EVAL] batch:   56 | acc: 81.25%,  total acc: 57.35%   [EVAL] batch:   57 | acc: 6.25%,  total acc: 56.47%   [EVAL] batch:   58 | acc: 6.25%,  total acc: 55.61%   [EVAL] batch:   59 | acc: 0.00%,  total acc: 54.69%   [EVAL] batch:   60 | acc: 37.50%,  total acc: 54.41%   [EVAL] batch:   61 | acc: 12.50%,  total acc: 53.73%   [EVAL] batch:   62 | acc: 6.25%,  total acc: 52.98%   [EVAL] batch:   63 | acc: 31.25%,  total acc: 52.64%   [EVAL] batch:   64 | acc: 25.00%,  total acc: 52.21%   [EVAL] batch:   65 | acc: 18.75%,  total acc: 51.70%   [EVAL] batch:   66 | acc: 25.00%,  total acc: 51.31%   [EVAL] batch:   67 | acc: 18.75%,  total acc: 50.83%   [EVAL] batch:   68 | acc: 12.50%,  total acc: 50.27%   [EVAL] batch:   69 | acc: 0.00%,  total acc: 49.55%   [EVAL] batch:   70 | acc: 12.50%,  total acc: 49.03%   [EVAL] batch:   71 | acc: 0.00%,  total acc: 48.35%   [EVAL] batch:   72 | acc: 25.00%,  total acc: 48.03%   [EVAL] batch:   73 | acc: 37.50%,  total acc: 47.89%   [EVAL] batch:   74 | acc: 50.00%,  total acc: 47.92%   [EVAL] batch:   75 | acc: 31.25%,  total acc: 47.70%   [EVAL] batch:   76 | acc: 50.00%,  total acc: 47.73%   [EVAL] batch:   77 | acc: 50.00%,  total acc: 47.76%   [EVAL] batch:   78 | acc: 62.50%,  total acc: 47.94%   [EVAL] batch:   79 | acc: 75.00%,  total acc: 48.28%   [EVAL] batch:   80 | acc: 68.75%,  total acc: 48.53%   [EVAL] batch:   81 | acc: 93.75%,  total acc: 49.09%   [EVAL] batch:   82 | acc: 81.25%,  total acc: 49.47%   [EVAL] batch:   83 | acc: 81.25%,  total acc: 49.85%   [EVAL] batch:   84 | acc: 12.50%,  total acc: 49.41%   [EVAL] batch:   85 | acc: 43.75%,  total acc: 49.35%   [EVAL] batch:   86 | acc: 50.00%,  total acc: 49.35%   [EVAL] batch:   87 | acc: 81.25%,  total acc: 49.72%   [EVAL] batch:   88 | acc: 75.00%,  total acc: 50.00%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 50.56%   [EVAL] batch:   90 | acc: 100.00%,  total acc: 51.10%   [EVAL] batch:   91 | acc: 93.75%,  total acc: 51.56%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 52.08%   [EVAL] batch:   93 | acc: 100.00%,  total acc: 52.59%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 53.09%   [EVAL] batch:   95 | acc: 81.25%,  total acc: 53.39%   [EVAL] batch:   96 | acc: 43.75%,  total acc: 53.29%   [EVAL] batch:   97 | acc: 81.25%,  total acc: 53.57%   [EVAL] batch:   98 | acc: 81.25%,  total acc: 53.85%   [EVAL] batch:   99 | acc: 75.00%,  total acc: 54.06%   [EVAL] batch:  100 | acc: 68.75%,  total acc: 54.21%   [EVAL] batch:  101 | acc: 75.00%,  total acc: 54.41%   [EVAL] batch:  102 | acc: 93.75%,  total acc: 54.79%   [EVAL] batch:  103 | acc: 81.25%,  total acc: 55.05%   [EVAL] batch:  104 | acc: 93.75%,  total acc: 55.42%   [EVAL] batch:  105 | acc: 100.00%,  total acc: 55.84%   [EVAL] batch:  106 | acc: 68.75%,  total acc: 55.96%   [EVAL] batch:  107 | acc: 81.25%,  total acc: 56.19%   [EVAL] batch:  108 | acc: 87.50%,  total acc: 56.48%   [EVAL] batch:  109 | acc: 87.50%,  total acc: 56.76%   [EVAL] batch:  110 | acc: 62.50%,  total acc: 56.81%   [EVAL] batch:  111 | acc: 68.75%,  total acc: 56.92%   [EVAL] batch:  112 | acc: 37.50%,  total acc: 56.75%   [EVAL] batch:  113 | acc: 50.00%,  total acc: 56.69%   [EVAL] batch:  114 | acc: 56.25%,  total acc: 56.68%   [EVAL] batch:  115 | acc: 31.25%,  total acc: 56.47%   
cur_acc:  ['0.8333', '0.7366', '0.3281', '0.6250', '0.5540', '0.8438', '0.7208']
his_acc:  ['0.8333', '0.8072', '0.7419', '0.6477', '0.5447', '0.5792', '0.5647']
Clustering into  19  clusters
Clusters:  [ 4  1  0 11 17 10  7  4 16  1 18  4 13 12  2  1  3  6 10 14 18  0  0  8
 10 16  2  7  4  0  6  4  3  9  8 16  4 15 18 16  5]
Losses:  20.571054458618164 9.087092399597168 5.7843337059021
CurrentTrain: epoch  0, batch     0 | loss: 20.5710545Losses:  12.634742736816406 3.46476674079895 3.378511428833008
CurrentTrain: epoch  0, batch     1 | loss: 12.6347427Losses:  17.97134017944336 7.248468399047852 5.69549560546875
CurrentTrain: epoch  1, batch     0 | loss: 17.9713402Losses:  11.078368186950684 1.7532724142074585 5.6349921226501465
CurrentTrain: epoch  1, batch     1 | loss: 11.0783682Losses:  19.153377532958984 9.276823997497559 5.624400615692139
CurrentTrain: epoch  2, batch     0 | loss: 19.1533775Losses:  9.783942222595215 3.776555299758911 3.336817741394043
CurrentTrain: epoch  2, batch     1 | loss: 9.7839422Losses:  16.997377395629883 7.658506870269775 5.583812713623047
CurrentTrain: epoch  3, batch     0 | loss: 16.9973774Losses:  9.292177200317383 2.7130932807922363 3.302907943725586
CurrentTrain: epoch  3, batch     1 | loss: 9.2921772Losses:  15.84123420715332 7.072351932525635 5.59572172164917
CurrentTrain: epoch  4, batch     0 | loss: 15.8412342Losses:  10.136035919189453 3.018033266067505 3.318082809448242
CurrentTrain: epoch  4, batch     1 | loss: 10.1360359Losses:  16.06021499633789 7.762275695800781 5.570403099060059
CurrentTrain: epoch  5, batch     0 | loss: 16.0602150Losses:  10.540664672851562 3.813920259475708 3.318605422973633
CurrentTrain: epoch  5, batch     1 | loss: 10.5406647Losses:  15.818490982055664 6.974203109741211 5.575455188751221
CurrentTrain: epoch  6, batch     0 | loss: 15.8184910Losses:  9.651318550109863 1.8858214616775513 5.552202224731445
CurrentTrain: epoch  6, batch     1 | loss: 9.6513186Losses:  14.53492546081543 6.3575921058654785 5.5599517822265625
CurrentTrain: epoch  7, batch     0 | loss: 14.5349255Losses:  10.875289916992188 2.2201449871063232 5.560271263122559
CurrentTrain: epoch  7, batch     1 | loss: 10.8752899Losses:  15.381739616394043 7.250767230987549 5.56624698638916
CurrentTrain: epoch  8, batch     0 | loss: 15.3817396Losses:  9.467414855957031 2.971181631088257 3.3190040588378906
CurrentTrain: epoch  8, batch     1 | loss: 9.4674149Losses:  15.423616409301758 7.245043754577637 5.556334972381592
CurrentTrain: epoch  9, batch     0 | loss: 15.4236164Losses:  8.024362564086914 2.7897746562957764 3.301309108734131
CurrentTrain: epoch  9, batch     1 | loss: 8.0243626
Losses:  27.445199966430664 -0.0 26.69013023376465
MemoryTrain:  epoch  0, batch     0 | loss: 27.4452000Losses:  26.666446685791016 -0.0 26.57037353515625
MemoryTrain:  epoch  0, batch     1 | loss: 26.6664467Losses:  5.877877235412598 -0.0 5.5532402992248535
MemoryTrain:  epoch  0, batch     2 | loss: 5.8778772Losses:  26.872211456298828 -0.0 26.525157928466797
MemoryTrain:  epoch  1, batch     0 | loss: 26.8722115Losses:  23.640438079833984 -0.0 23.118337631225586
MemoryTrain:  epoch  1, batch     1 | loss: 23.6404381Losses:  13.948652267456055 -0.0 13.766605377197266
MemoryTrain:  epoch  1, batch     2 | loss: 13.9486523Losses:  14.005393028259277 -0.0 13.755134582519531
MemoryTrain:  epoch  2, batch     0 | loss: 14.0053930Losses:  29.996278762817383 -0.0 29.918079376220703
MemoryTrain:  epoch  2, batch     1 | loss: 29.9962788Losses:  13.735076904296875 -0.0 13.68790340423584
MemoryTrain:  epoch  2, batch     2 | loss: 13.7350769Losses:  33.540496826171875 -0.0 33.483036041259766
MemoryTrain:  epoch  3, batch     0 | loss: 33.5404968Losses:  23.396181106567383 -0.0 23.126846313476562
MemoryTrain:  epoch  3, batch     1 | loss: 23.3961811Losses:  13.952899932861328 -0.0 13.675854682922363
MemoryTrain:  epoch  3, batch     2 | loss: 13.9528999Losses:  23.20713233947754 -0.0 23.135496139526367
MemoryTrain:  epoch  4, batch     0 | loss: 23.2071323Losses:  23.283344268798828 -0.0 23.111135482788086
MemoryTrain:  epoch  4, batch     1 | loss: 23.2833443Losses:  10.899940490722656 -0.0 10.822092056274414
MemoryTrain:  epoch  4, batch     2 | loss: 10.8999405Losses:  26.61815071105957 -0.0 26.515544891357422
MemoryTrain:  epoch  5, batch     0 | loss: 26.6181507Losses:  29.962013244628906 -0.0 29.890363693237305
MemoryTrain:  epoch  5, batch     1 | loss: 29.9620132Losses:  10.832934379577637 -0.0 10.774045944213867
MemoryTrain:  epoch  5, batch     2 | loss: 10.8329344Losses:  19.902584075927734 -0.0 19.848527908325195
MemoryTrain:  epoch  6, batch     0 | loss: 19.9025841Losses:  23.25196075439453 -0.0 23.121501922607422
MemoryTrain:  epoch  6, batch     1 | loss: 23.2519608Losses:  13.754095077514648 -0.0 13.707661628723145
MemoryTrain:  epoch  6, batch     2 | loss: 13.7540951Losses:  23.110210418701172 -0.0 23.081920623779297
MemoryTrain:  epoch  7, batch     0 | loss: 23.1102104Losses:  26.54186248779297 -0.0 26.45645523071289
MemoryTrain:  epoch  7, batch     1 | loss: 26.5418625Losses:  16.73435401916504 -0.0 16.715126037597656
MemoryTrain:  epoch  7, batch     2 | loss: 16.7343540Losses:  19.864727020263672 -0.0 19.836732864379883
MemoryTrain:  epoch  8, batch     0 | loss: 19.8647270Losses:  26.576114654541016 -0.0 26.519775390625
MemoryTrain:  epoch  8, batch     1 | loss: 26.5761147Losses:  8.11929702758789 -0.0 8.067177772521973
MemoryTrain:  epoch  8, batch     2 | loss: 8.1192970Losses:  29.95936393737793 -0.0 29.915430068969727
MemoryTrain:  epoch  9, batch     0 | loss: 29.9593639Losses:  16.785661697387695 -0.0 16.716615676879883
MemoryTrain:  epoch  9, batch     1 | loss: 16.7856617Losses:  13.665499687194824 -0.0 13.64885139465332
MemoryTrain:  epoch  9, batch     2 | loss: 13.6654997
[EVAL] batch:    0 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 96.88%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 93.75%   [EVAL] batch:    3 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 92.50%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 91.67%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 91.96%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 91.41%   [EVAL] batch:    8 | acc: 25.00%,  total acc: 84.03%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 85.23%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 86.46%   [EVAL] batch:   12 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   13 | acc: 100.00%,  total acc: 88.39%   [EVAL] batch:   14 | acc: 100.00%,  total acc: 89.17%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 89.84%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 90.44%   [EVAL] batch:   17 | acc: 25.00%,  total acc: 86.81%   
[EVAL] batch:    0 | acc: 6.25%,  total acc: 6.25%   [EVAL] batch:    1 | acc: 18.75%,  total acc: 12.50%   [EVAL] batch:    2 | acc: 6.25%,  total acc: 10.42%   [EVAL] batch:    3 | acc: 0.00%,  total acc: 7.81%   [EVAL] batch:    4 | acc: 6.25%,  total acc: 7.50%   [EVAL] batch:    5 | acc: 12.50%,  total acc: 8.33%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 17.86%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 28.12%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 36.11%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 41.88%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 46.59%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 50.00%   [EVAL] batch:   12 | acc: 50.00%,  total acc: 50.00%   [EVAL] batch:   13 | acc: 25.00%,  total acc: 48.21%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 50.00%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 50.39%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 51.84%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 52.43%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 52.96%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 54.69%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 56.85%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 58.81%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 60.33%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 61.72%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 63.25%   [EVAL] batch:   25 | acc: 93.75%,  total acc: 64.42%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 65.51%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 66.74%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 67.89%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 68.54%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 69.15%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 69.92%   [EVAL] batch:   32 | acc: 75.00%,  total acc: 70.08%   [EVAL] batch:   33 | acc: 12.50%,  total acc: 68.38%   [EVAL] batch:   34 | acc: 0.00%,  total acc: 66.43%   [EVAL] batch:   35 | acc: 0.00%,  total acc: 64.58%   [EVAL] batch:   36 | acc: 0.00%,  total acc: 62.84%   [EVAL] batch:   37 | acc: 0.00%,  total acc: 61.18%   [EVAL] batch:   38 | acc: 6.25%,  total acc: 59.78%   [EVAL] batch:   39 | acc: 43.75%,  total acc: 59.38%   [EVAL] batch:   40 | acc: 62.50%,  total acc: 59.45%   [EVAL] batch:   41 | acc: 50.00%,  total acc: 59.23%   [EVAL] batch:   42 | acc: 62.50%,  total acc: 59.30%   [EVAL] batch:   43 | acc: 37.50%,  total acc: 58.81%   [EVAL] batch:   44 | acc: 50.00%,  total acc: 58.61%   [EVAL] batch:   45 | acc: 6.25%,  total acc: 57.47%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 57.98%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 58.85%   [EVAL] batch:   48 | acc: 12.50%,  total acc: 57.91%   [EVAL] batch:   49 | acc: 12.50%,  total acc: 57.00%   [EVAL] batch:   50 | acc: 6.25%,  total acc: 56.00%   [EVAL] batch:   51 | acc: 18.75%,  total acc: 55.29%   [EVAL] batch:   52 | acc: 6.25%,  total acc: 54.36%   [EVAL] batch:   53 | acc: 25.00%,  total acc: 53.82%   [EVAL] batch:   54 | acc: 37.50%,  total acc: 53.52%   [EVAL] batch:   55 | acc: 50.00%,  total acc: 53.46%   [EVAL] batch:   56 | acc: 56.25%,  total acc: 53.51%   [EVAL] batch:   57 | acc: 12.50%,  total acc: 52.80%   [EVAL] batch:   58 | acc: 6.25%,  total acc: 52.01%   [EVAL] batch:   59 | acc: 0.00%,  total acc: 51.15%   [EVAL] batch:   60 | acc: 6.25%,  total acc: 50.41%   [EVAL] batch:   61 | acc: 0.00%,  total acc: 49.60%   [EVAL] batch:   62 | acc: 0.00%,  total acc: 48.81%   [EVAL] batch:   63 | acc: 0.00%,  total acc: 48.05%   [EVAL] batch:   64 | acc: 6.25%,  total acc: 47.40%   [EVAL] batch:   65 | acc: 0.00%,  total acc: 46.69%   [EVAL] batch:   66 | acc: 25.00%,  total acc: 46.36%   [EVAL] batch:   67 | acc: 25.00%,  total acc: 46.05%   [EVAL] batch:   68 | acc: 12.50%,  total acc: 45.56%   [EVAL] batch:   69 | acc: 0.00%,  total acc: 44.91%   [EVAL] batch:   70 | acc: 12.50%,  total acc: 44.45%   [EVAL] batch:   71 | acc: 0.00%,  total acc: 43.84%   [EVAL] batch:   72 | acc: 25.00%,  total acc: 43.58%   [EVAL] batch:   73 | acc: 37.50%,  total acc: 43.50%   [EVAL] batch:   74 | acc: 56.25%,  total acc: 43.67%   [EVAL] batch:   75 | acc: 31.25%,  total acc: 43.50%   [EVAL] batch:   76 | acc: 56.25%,  total acc: 43.67%   [EVAL] batch:   77 | acc: 56.25%,  total acc: 43.83%   [EVAL] batch:   78 | acc: 62.50%,  total acc: 44.07%   [EVAL] batch:   79 | acc: 50.00%,  total acc: 44.14%   [EVAL] batch:   80 | acc: 62.50%,  total acc: 44.37%   [EVAL] batch:   81 | acc: 68.75%,  total acc: 44.66%   [EVAL] batch:   82 | acc: 81.25%,  total acc: 45.11%   [EVAL] batch:   83 | acc: 56.25%,  total acc: 45.24%   [EVAL] batch:   84 | acc: 0.00%,  total acc: 44.71%   [EVAL] batch:   85 | acc: 6.25%,  total acc: 44.26%   [EVAL] batch:   86 | acc: 6.25%,  total acc: 43.82%   [EVAL] batch:   87 | acc: 75.00%,  total acc: 44.18%   [EVAL] batch:   88 | acc: 68.75%,  total acc: 44.45%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 45.07%   [EVAL] batch:   90 | acc: 100.00%,  total acc: 45.67%   [EVAL] batch:   91 | acc: 93.75%,  total acc: 46.20%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 46.77%   [EVAL] batch:   93 | acc: 100.00%,  total acc: 47.34%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 47.89%   [EVAL] batch:   95 | acc: 81.25%,  total acc: 48.24%   [EVAL] batch:   96 | acc: 50.00%,  total acc: 48.26%   [EVAL] batch:   97 | acc: 68.75%,  total acc: 48.47%   [EVAL] batch:   98 | acc: 81.25%,  total acc: 48.80%   [EVAL] batch:   99 | acc: 81.25%,  total acc: 49.12%   [EVAL] batch:  100 | acc: 68.75%,  total acc: 49.32%   [EVAL] batch:  101 | acc: 50.00%,  total acc: 49.33%   [EVAL] batch:  102 | acc: 56.25%,  total acc: 49.39%   [EVAL] batch:  103 | acc: 62.50%,  total acc: 49.52%   [EVAL] batch:  104 | acc: 62.50%,  total acc: 49.64%   [EVAL] batch:  105 | acc: 81.25%,  total acc: 49.94%   [EVAL] batch:  106 | acc: 75.00%,  total acc: 50.18%   [EVAL] batch:  107 | acc: 81.25%,  total acc: 50.46%   [EVAL] batch:  108 | acc: 93.75%,  total acc: 50.86%   [EVAL] batch:  109 | acc: 87.50%,  total acc: 51.19%   [EVAL] batch:  110 | acc: 62.50%,  total acc: 51.30%   [EVAL] batch:  111 | acc: 12.50%,  total acc: 50.95%   [EVAL] batch:  112 | acc: 0.00%,  total acc: 50.50%   [EVAL] batch:  113 | acc: 6.25%,  total acc: 50.11%   [EVAL] batch:  114 | acc: 0.00%,  total acc: 49.67%   [EVAL] batch:  115 | acc: 62.50%,  total acc: 49.78%   [EVAL] batch:  116 | acc: 93.75%,  total acc: 50.16%   [EVAL] batch:  117 | acc: 87.50%,  total acc: 50.48%   [EVAL] batch:  118 | acc: 93.75%,  total acc: 50.84%   [EVAL] batch:  119 | acc: 87.50%,  total acc: 51.15%   [EVAL] batch:  120 | acc: 93.75%,  total acc: 51.50%   [EVAL] batch:  121 | acc: 87.50%,  total acc: 51.79%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 52.18%   [EVAL] batch:  123 | acc: 37.50%,  total acc: 52.07%   [EVAL] batch:  124 | acc: 75.00%,  total acc: 52.25%   [EVAL] batch:  125 | acc: 87.50%,  total acc: 52.53%   [EVAL] batch:  126 | acc: 93.75%,  total acc: 52.85%   [EVAL] batch:  127 | acc: 100.00%,  total acc: 53.22%   [EVAL] batch:  128 | acc: 100.00%,  total acc: 53.59%   [EVAL] batch:  129 | acc: 100.00%,  total acc: 53.94%   [EVAL] batch:  130 | acc: 100.00%,  total acc: 54.29%   [EVAL] batch:  131 | acc: 100.00%,  total acc: 54.64%   [EVAL] batch:  132 | acc: 62.50%,  total acc: 54.70%   
cur_acc:  ['0.8333', '0.7366', '0.3281', '0.6250', '0.5540', '0.8438', '0.7208', '0.8681']
his_acc:  ['0.8333', '0.8072', '0.7419', '0.6477', '0.5447', '0.5792', '0.5647', '0.5470']
--------Round  1
seed:  200
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/test.pkl
Task_order: [7 6 3 2 4 0 5 1]
prepared data!
Clustering into  2  clusters
Clusters:  [1 0 0 1 0 1]
Losses:  21.443012237548828 8.057880401611328 -0.0
CurrentTrain: epoch  0, batch     0 | loss: 21.4430122Losses:  22.548593521118164 9.517162322998047 -0.0
CurrentTrain: epoch  0, batch     1 | loss: 22.5485935Losses:  24.890583038330078 11.997604370117188 -0.0
CurrentTrain: epoch  0, batch     2 | loss: 24.8905830Losses:  23.66830062866211 10.970340728759766 -0.0
CurrentTrain: epoch  0, batch     3 | loss: 23.6683006Losses:  21.147281646728516 8.37041187286377 -0.0
CurrentTrain: epoch  0, batch     4 | loss: 21.1472816Losses:  24.521591186523438 11.926565170288086 -0.0
CurrentTrain: epoch  0, batch     5 | loss: 24.5215912Losses:  22.78253746032715 10.046526908874512 -0.0
CurrentTrain: epoch  0, batch     6 | loss: 22.7825375Losses:  24.984535217285156 12.373897552490234 -0.0
CurrentTrain: epoch  0, batch     7 | loss: 24.9845352Losses:  21.037559509277344 8.799708366394043 -0.0
CurrentTrain: epoch  0, batch     8 | loss: 21.0375595Losses:  21.32874298095703 8.937376022338867 -0.0
CurrentTrain: epoch  0, batch     9 | loss: 21.3287430Losses:  18.838592529296875 6.619421482086182 -0.0
CurrentTrain: epoch  0, batch    10 | loss: 18.8385925Losses:  20.218902587890625 8.40414047241211 -0.0
CurrentTrain: epoch  0, batch    11 | loss: 20.2189026Losses:  23.376392364501953 11.291666030883789 -0.0
CurrentTrain: epoch  0, batch    12 | loss: 23.3763924Losses:  22.808815002441406 10.900713920593262 -0.0
CurrentTrain: epoch  0, batch    13 | loss: 22.8088150Losses:  20.02340316772461 8.112800598144531 -0.0
CurrentTrain: epoch  0, batch    14 | loss: 20.0234032Losses:  19.09228515625 7.442933082580566 -0.0
CurrentTrain: epoch  0, batch    15 | loss: 19.0922852Losses:  18.462238311767578 6.899893283843994 -0.0
CurrentTrain: epoch  0, batch    16 | loss: 18.4622383Losses:  20.561553955078125 8.954675674438477 -0.0
CurrentTrain: epoch  0, batch    17 | loss: 20.5615540Losses:  23.269821166992188 11.96854019165039 -0.0
CurrentTrain: epoch  0, batch    18 | loss: 23.2698212Losses:  20.070865631103516 9.017496109008789 -0.0
CurrentTrain: epoch  0, batch    19 | loss: 20.0708656Losses:  21.157398223876953 9.664718627929688 -0.0
CurrentTrain: epoch  0, batch    20 | loss: 21.1573982Losses:  23.848102569580078 12.874963760375977 -0.0
CurrentTrain: epoch  0, batch    21 | loss: 23.8481026Losses:  21.121484756469727 9.870336532592773 -0.0
CurrentTrain: epoch  0, batch    22 | loss: 21.1214848Losses:  20.426715850830078 8.694114685058594 -0.0
CurrentTrain: epoch  0, batch    23 | loss: 20.4267159Losses:  18.273345947265625 6.868888854980469 -0.0
CurrentTrain: epoch  0, batch    24 | loss: 18.2733459Losses:  17.736736297607422 6.590481758117676 -0.0
CurrentTrain: epoch  0, batch    25 | loss: 17.7367363Losses:  26.04144287109375 15.953725814819336 -0.0
CurrentTrain: epoch  0, batch    26 | loss: 26.0414429Losses:  22.883800506591797 11.498604774475098 -0.0
CurrentTrain: epoch  0, batch    27 | loss: 22.8838005Losses:  18.34750747680664 7.534406661987305 -0.0
CurrentTrain: epoch  0, batch    28 | loss: 18.3475075Losses:  18.038745880126953 6.965886116027832 -0.0
CurrentTrain: epoch  0, batch    29 | loss: 18.0387459Losses:  22.404733657836914 11.288543701171875 -0.0
CurrentTrain: epoch  0, batch    30 | loss: 22.4047337Losses:  18.977827072143555 8.823380470275879 -0.0
CurrentTrain: epoch  0, batch    31 | loss: 18.9778271Losses:  21.53360366821289 10.56892204284668 -0.0
CurrentTrain: epoch  0, batch    32 | loss: 21.5336037Losses:  17.955665588378906 7.630840301513672 -0.0
CurrentTrain: epoch  0, batch    33 | loss: 17.9556656Losses:  20.168468475341797 9.589895248413086 -0.0
CurrentTrain: epoch  0, batch    34 | loss: 20.1684685Losses:  23.049053192138672 12.662899017333984 -0.0
CurrentTrain: epoch  0, batch    35 | loss: 23.0490532Losses:  19.349124908447266 8.992740631103516 -0.0
CurrentTrain: epoch  0, batch    36 | loss: 19.3491249Losses:  12.54781436920166 3.411855697631836 -0.0
CurrentTrain: epoch  0, batch    37 | loss: 12.5478144Losses:  20.857437133789062 10.318029403686523 -0.0
CurrentTrain: epoch  1, batch     0 | loss: 20.8574371Losses:  16.199485778808594 6.16340970993042 -0.0
CurrentTrain: epoch  1, batch     1 | loss: 16.1994858Losses:  17.49182891845703 7.665996074676514 -0.0
CurrentTrain: epoch  1, batch     2 | loss: 17.4918289Losses:  20.196266174316406 10.170602798461914 -0.0
CurrentTrain: epoch  1, batch     3 | loss: 20.1962662Losses:  23.16626739501953 12.48381233215332 -0.0
CurrentTrain: epoch  1, batch     4 | loss: 23.1662674Losses:  19.096309661865234 8.620567321777344 -0.0
CurrentTrain: epoch  1, batch     5 | loss: 19.0963097Losses:  18.231510162353516 9.35373592376709 -0.0
CurrentTrain: epoch  1, batch     6 | loss: 18.2315102Losses:  17.012117385864258 7.556823253631592 -0.0
CurrentTrain: epoch  1, batch     7 | loss: 17.0121174Losses:  19.115840911865234 8.903169631958008 -0.0
CurrentTrain: epoch  1, batch     8 | loss: 19.1158409Losses:  15.322216987609863 6.179739952087402 -0.0
CurrentTrain: epoch  1, batch     9 | loss: 15.3222170Losses:  16.976825714111328 6.368609428405762 -0.0
CurrentTrain: epoch  1, batch    10 | loss: 16.9768257Losses:  19.878995895385742 10.126222610473633 -0.0
CurrentTrain: epoch  1, batch    11 | loss: 19.8789959Losses:  17.454647064208984 8.261285781860352 -0.0
CurrentTrain: epoch  1, batch    12 | loss: 17.4546471Losses:  19.50075912475586 9.916671752929688 -0.0
CurrentTrain: epoch  1, batch    13 | loss: 19.5007591Losses:  16.61261558532715 6.714212894439697 -0.0
CurrentTrain: epoch  1, batch    14 | loss: 16.6126156Losses:  14.736724853515625 6.171101093292236 -0.0
CurrentTrain: epoch  1, batch    15 | loss: 14.7367249Losses:  15.831345558166504 6.606705665588379 -0.0
CurrentTrain: epoch  1, batch    16 | loss: 15.8313456Losses:  15.338900566101074 5.899598121643066 -0.0
CurrentTrain: epoch  1, batch    17 | loss: 15.3389006Losses:  16.22783851623535 6.572873115539551 -0.0
CurrentTrain: epoch  1, batch    18 | loss: 16.2278385Losses:  17.36766242980957 8.570088386535645 -0.0
CurrentTrain: epoch  1, batch    19 | loss: 17.3676624Losses:  18.462139129638672 8.778243064880371 -0.0
CurrentTrain: epoch  1, batch    20 | loss: 18.4621391Losses:  16.897123336791992 8.00910472869873 -0.0
CurrentTrain: epoch  1, batch    21 | loss: 16.8971233Losses:  20.06063461303711 10.308571815490723 -0.0
CurrentTrain: epoch  1, batch    22 | loss: 20.0606346Losses:  16.147592544555664 7.190884590148926 -0.0
CurrentTrain: epoch  1, batch    23 | loss: 16.1475925Losses:  17.95456314086914 8.885719299316406 -0.0
CurrentTrain: epoch  1, batch    24 | loss: 17.9545631Losses:  17.24283790588379 7.558191299438477 -0.0
CurrentTrain: epoch  1, batch    25 | loss: 17.2428379Losses:  16.19535255432129 7.771524429321289 -0.0
CurrentTrain: epoch  1, batch    26 | loss: 16.1953526Losses:  16.91219139099121 6.958014965057373 -0.0
CurrentTrain: epoch  1, batch    27 | loss: 16.9121914Losses:  14.929118156433105 6.419447898864746 -0.0
CurrentTrain: epoch  1, batch    28 | loss: 14.9291182Losses:  21.570514678955078 11.978673934936523 -0.0
CurrentTrain: epoch  1, batch    29 | loss: 21.5705147Losses:  15.312339782714844 6.460687160491943 -0.0
CurrentTrain: epoch  1, batch    30 | loss: 15.3123398Losses:  20.779029846191406 11.772013664245605 -0.0
CurrentTrain: epoch  1, batch    31 | loss: 20.7790298Losses:  18.116920471191406 9.319676399230957 -0.0
CurrentTrain: epoch  1, batch    32 | loss: 18.1169205Losses:  14.962913513183594 6.76652717590332 -0.0
CurrentTrain: epoch  1, batch    33 | loss: 14.9629135Losses:  17.096820831298828 7.905148983001709 -0.0
CurrentTrain: epoch  1, batch    34 | loss: 17.0968208Losses:  17.50421142578125 8.63283920288086 -0.0
CurrentTrain: epoch  1, batch    35 | loss: 17.5042114Losses:  15.136335372924805 6.448127746582031 -0.0
CurrentTrain: epoch  1, batch    36 | loss: 15.1363354Losses:  13.153837203979492 3.6568703651428223 -0.0
CurrentTrain: epoch  1, batch    37 | loss: 13.1538372Losses:  13.76261043548584 5.51840877532959 -0.0
CurrentTrain: epoch  2, batch     0 | loss: 13.7626104Losses:  15.602601051330566 7.157109260559082 -0.0
CurrentTrain: epoch  2, batch     1 | loss: 15.6026011Losses:  13.696161270141602 5.385862350463867 -0.0
CurrentTrain: epoch  2, batch     2 | loss: 13.6961613Losses:  13.925277709960938 6.09623908996582 -0.0
CurrentTrain: epoch  2, batch     3 | loss: 13.9252777Losses:  17.880367279052734 8.804985046386719 -0.0
CurrentTrain: epoch  2, batch     4 | loss: 17.8803673Losses:  16.453672409057617 7.757521152496338 -0.0
CurrentTrain: epoch  2, batch     5 | loss: 16.4536724Losses:  15.446725845336914 7.1643829345703125 -0.0
CurrentTrain: epoch  2, batch     6 | loss: 15.4467258Losses:  14.667132377624512 6.623471260070801 -0.0
CurrentTrain: epoch  2, batch     7 | loss: 14.6671324Losses:  16.20191192626953 8.081806182861328 -0.0
CurrentTrain: epoch  2, batch     8 | loss: 16.2019119Losses:  13.69584846496582 5.945034027099609 -0.0
CurrentTrain: epoch  2, batch     9 | loss: 13.6958485Losses:  16.728145599365234 8.887069702148438 -0.0
CurrentTrain: epoch  2, batch    10 | loss: 16.7281456Losses:  15.696266174316406 8.28719711303711 -0.0
CurrentTrain: epoch  2, batch    11 | loss: 15.6962662Losses:  15.914495468139648 6.77284049987793 -0.0
CurrentTrain: epoch  2, batch    12 | loss: 15.9144955Losses:  13.607189178466797 5.645778179168701 -0.0
CurrentTrain: epoch  2, batch    13 | loss: 13.6071892Losses:  13.577863693237305 5.667391777038574 -0.0
CurrentTrain: epoch  2, batch    14 | loss: 13.5778637Losses:  15.19076156616211 6.399588584899902 -0.0
CurrentTrain: epoch  2, batch    15 | loss: 15.1907616Losses:  19.892757415771484 10.904825210571289 -0.0
CurrentTrain: epoch  2, batch    16 | loss: 19.8927574Losses:  16.837650299072266 7.769237995147705 -0.0
CurrentTrain: epoch  2, batch    17 | loss: 16.8376503Losses:  16.61560821533203 8.049079895019531 -0.0
CurrentTrain: epoch  2, batch    18 | loss: 16.6156082Losses:  13.54930305480957 5.089229106903076 -0.0
CurrentTrain: epoch  2, batch    19 | loss: 13.5493031Losses:  16.23343276977539 7.226042747497559 -0.0
CurrentTrain: epoch  2, batch    20 | loss: 16.2334328Losses:  13.149449348449707 5.343809127807617 -0.0
CurrentTrain: epoch  2, batch    21 | loss: 13.1494493Losses:  20.441112518310547 11.142908096313477 -0.0
CurrentTrain: epoch  2, batch    22 | loss: 20.4411125Losses:  14.85135555267334 7.160871505737305 -0.0
CurrentTrain: epoch  2, batch    23 | loss: 14.8513556Losses:  14.319496154785156 6.393189430236816 -0.0
CurrentTrain: epoch  2, batch    24 | loss: 14.3194962Losses:  14.842799186706543 5.9685163497924805 -0.0
CurrentTrain: epoch  2, batch    25 | loss: 14.8427992Losses:  16.925296783447266 7.836220741271973 -0.0
CurrentTrain: epoch  2, batch    26 | loss: 16.9252968Losses:  14.87080192565918 7.568298816680908 -0.0
CurrentTrain: epoch  2, batch    27 | loss: 14.8708019Losses:  14.875236511230469 6.788381099700928 -0.0
CurrentTrain: epoch  2, batch    28 | loss: 14.8752365Losses:  15.811092376708984 7.304741382598877 -0.0
CurrentTrain: epoch  2, batch    29 | loss: 15.8110924Losses:  15.21930980682373 8.167454719543457 -0.0
CurrentTrain: epoch  2, batch    30 | loss: 15.2193098Losses:  16.542964935302734 9.425419807434082 -0.0
CurrentTrain: epoch  2, batch    31 | loss: 16.5429649Losses:  20.423606872558594 12.149762153625488 -0.0
CurrentTrain: epoch  2, batch    32 | loss: 20.4236069Losses:  12.873811721801758 6.056113243103027 -0.0
CurrentTrain: epoch  2, batch    33 | loss: 12.8738117Losses:  13.931317329406738 6.721523284912109 -0.0
CurrentTrain: epoch  2, batch    34 | loss: 13.9313173Losses:  16.54351043701172 8.138794898986816 -0.0
CurrentTrain: epoch  2, batch    35 | loss: 16.5435104Losses:  15.39594841003418 8.053545951843262 -0.0
CurrentTrain: epoch  2, batch    36 | loss: 15.3959484Losses:  10.594478607177734 2.0405640602111816 -0.0
CurrentTrain: epoch  2, batch    37 | loss: 10.5944786Losses:  15.764236450195312 7.20851469039917 -0.0
CurrentTrain: epoch  3, batch     0 | loss: 15.7642365Losses:  13.304227828979492 5.933173656463623 -0.0
CurrentTrain: epoch  3, batch     1 | loss: 13.3042278Losses:  12.265970230102539 4.862815856933594 -0.0
CurrentTrain: epoch  3, batch     2 | loss: 12.2659702Losses:  14.93703842163086 6.261373043060303 -0.0
CurrentTrain: epoch  3, batch     3 | loss: 14.9370384Losses:  13.86809253692627 6.443166732788086 -0.0
CurrentTrain: epoch  3, batch     4 | loss: 13.8680925Losses:  14.016467094421387 6.323266983032227 -0.0
CurrentTrain: epoch  3, batch     5 | loss: 14.0164671Losses:  15.689562797546387 6.919491767883301 -0.0
CurrentTrain: epoch  3, batch     6 | loss: 15.6895628Losses:  13.14026927947998 4.895092010498047 -0.0
CurrentTrain: epoch  3, batch     7 | loss: 13.1402693Losses:  16.42327880859375 10.263616561889648 -0.0
CurrentTrain: epoch  3, batch     8 | loss: 16.4232788Losses:  14.751739501953125 7.709596157073975 -0.0
CurrentTrain: epoch  3, batch     9 | loss: 14.7517395Losses:  13.409841537475586 5.925756454467773 -0.0
CurrentTrain: epoch  3, batch    10 | loss: 13.4098415Losses:  21.608104705810547 12.89125919342041 -0.0
CurrentTrain: epoch  3, batch    11 | loss: 21.6081047Losses:  14.115015029907227 6.6125898361206055 -0.0
CurrentTrain: epoch  3, batch    12 | loss: 14.1150150Losses:  11.89872932434082 4.835506439208984 -0.0
CurrentTrain: epoch  3, batch    13 | loss: 11.8987293Losses:  14.018613815307617 5.788084030151367 -0.0
CurrentTrain: epoch  3, batch    14 | loss: 14.0186138Losses:  13.442002296447754 5.659154891967773 -0.0
CurrentTrain: epoch  3, batch    15 | loss: 13.4420023Losses:  13.795974731445312 6.039173126220703 -0.0
CurrentTrain: epoch  3, batch    16 | loss: 13.7959747Losses:  15.78759479522705 7.468277931213379 -0.0
CurrentTrain: epoch  3, batch    17 | loss: 15.7875948Losses:  12.947310447692871 5.154178619384766 -0.0
CurrentTrain: epoch  3, batch    18 | loss: 12.9473104Losses:  13.594311714172363 5.595891952514648 -0.0
CurrentTrain: epoch  3, batch    19 | loss: 13.5943117Losses:  13.32827377319336 5.945014953613281 -0.0
CurrentTrain: epoch  3, batch    20 | loss: 13.3282738Losses:  14.479846954345703 6.778229713439941 -0.0
CurrentTrain: epoch  3, batch    21 | loss: 14.4798470Losses:  14.428179740905762 6.03781795501709 -0.0
CurrentTrain: epoch  3, batch    22 | loss: 14.4281797Losses:  12.204533576965332 4.801968574523926 -0.0
CurrentTrain: epoch  3, batch    23 | loss: 12.2045336Losses:  12.722127914428711 5.859902381896973 -0.0
CurrentTrain: epoch  3, batch    24 | loss: 12.7221279Losses:  14.334409713745117 6.589527606964111 -0.0
CurrentTrain: epoch  3, batch    25 | loss: 14.3344097Losses:  14.456825256347656 6.6910481452941895 -0.0
CurrentTrain: epoch  3, batch    26 | loss: 14.4568253Losses:  13.44525146484375 6.628744602203369 -0.0
CurrentTrain: epoch  3, batch    27 | loss: 13.4452515Losses:  15.960039138793945 9.151749610900879 -0.0
CurrentTrain: epoch  3, batch    28 | loss: 15.9600391Losses:  16.310836791992188 8.59777545928955 -0.0
CurrentTrain: epoch  3, batch    29 | loss: 16.3108368Losses:  13.449864387512207 5.463068008422852 -0.0
CurrentTrain: epoch  3, batch    30 | loss: 13.4498644Losses:  14.340423583984375 8.035032272338867 -0.0
CurrentTrain: epoch  3, batch    31 | loss: 14.3404236Losses:  13.488460540771484 6.12394905090332 -0.0
CurrentTrain: epoch  3, batch    32 | loss: 13.4884605Losses:  11.828126907348633 5.503415107727051 -0.0
CurrentTrain: epoch  3, batch    33 | loss: 11.8281269Losses:  11.974063873291016 5.208412170410156 -0.0
CurrentTrain: epoch  3, batch    34 | loss: 11.9740639Losses:  12.618758201599121 5.957047939300537 -0.0
CurrentTrain: epoch  3, batch    35 | loss: 12.6187582Losses:  13.911859512329102 5.377006530761719 -0.0
CurrentTrain: epoch  3, batch    36 | loss: 13.9118595Losses:  9.054073333740234 1.6901607513427734 -0.0
CurrentTrain: epoch  3, batch    37 | loss: 9.0540733Losses:  11.497209548950195 4.45345401763916 -0.0
CurrentTrain: epoch  4, batch     0 | loss: 11.4972095Losses:  12.913856506347656 6.090414047241211 -0.0
CurrentTrain: epoch  4, batch     1 | loss: 12.9138565Losses:  14.249295234680176 6.497635841369629 -0.0
CurrentTrain: epoch  4, batch     2 | loss: 14.2492952Losses:  18.32355499267578 9.173894882202148 -0.0
CurrentTrain: epoch  4, batch     3 | loss: 18.3235550Losses:  11.916624069213867 5.450397491455078 -0.0
CurrentTrain: epoch  4, batch     4 | loss: 11.9166241Losses:  14.116254806518555 7.675752639770508 -0.0
CurrentTrain: epoch  4, batch     5 | loss: 14.1162548Losses:  15.111380577087402 6.706124305725098 -0.0
CurrentTrain: epoch  4, batch     6 | loss: 15.1113806Losses:  12.39316177368164 5.325342178344727 -0.0
CurrentTrain: epoch  4, batch     7 | loss: 12.3931618Losses:  16.47001838684082 9.048986434936523 -0.0
CurrentTrain: epoch  4, batch     8 | loss: 16.4700184Losses:  13.593969345092773 6.4936323165893555 -0.0
CurrentTrain: epoch  4, batch     9 | loss: 13.5939693Losses:  13.245036125183105 5.2629194259643555 -0.0
CurrentTrain: epoch  4, batch    10 | loss: 13.2450361Losses:  12.300848960876465 5.262439727783203 -0.0
CurrentTrain: epoch  4, batch    11 | loss: 12.3008490Losses:  12.669147491455078 5.611681938171387 -0.0
CurrentTrain: epoch  4, batch    12 | loss: 12.6691475Losses:  11.723588943481445 4.494682312011719 -0.0
CurrentTrain: epoch  4, batch    13 | loss: 11.7235889Losses:  11.478480339050293 4.694000244140625 -0.0
CurrentTrain: epoch  4, batch    14 | loss: 11.4784803Losses:  12.805856704711914 6.242476463317871 -0.0
CurrentTrain: epoch  4, batch    15 | loss: 12.8058567Losses:  11.75823974609375 5.429620265960693 -0.0
CurrentTrain: epoch  4, batch    16 | loss: 11.7582397Losses:  14.156309127807617 6.9015116691589355 -0.0
CurrentTrain: epoch  4, batch    17 | loss: 14.1563091Losses:  11.708064079284668 4.7771501541137695 -0.0
CurrentTrain: epoch  4, batch    18 | loss: 11.7080641Losses:  12.790314674377441 5.846674919128418 -0.0
CurrentTrain: epoch  4, batch    19 | loss: 12.7903147Losses:  12.370469093322754 5.265460014343262 -0.0
CurrentTrain: epoch  4, batch    20 | loss: 12.3704691Losses:  16.206449508666992 9.744125366210938 -0.0
CurrentTrain: epoch  4, batch    21 | loss: 16.2064495Losses:  11.361665725708008 5.032320022583008 -0.0
CurrentTrain: epoch  4, batch    22 | loss: 11.3616657Losses:  17.866573333740234 10.089317321777344 -0.0
CurrentTrain: epoch  4, batch    23 | loss: 17.8665733Losses:  13.154569625854492 5.497708320617676 -0.0
CurrentTrain: epoch  4, batch    24 | loss: 13.1545696Losses:  12.220961570739746 6.292633056640625 -0.0
CurrentTrain: epoch  4, batch    25 | loss: 12.2209616Losses:  13.336692810058594 6.1077117919921875 -0.0
CurrentTrain: epoch  4, batch    26 | loss: 13.3366928Losses:  13.55283260345459 6.943986892700195 -0.0
CurrentTrain: epoch  4, batch    27 | loss: 13.5528326Losses:  12.049300193786621 5.761712074279785 -0.0
CurrentTrain: epoch  4, batch    28 | loss: 12.0493002Losses:  14.029036521911621 6.406756401062012 -0.0
CurrentTrain: epoch  4, batch    29 | loss: 14.0290365Losses:  13.34595012664795 6.342286109924316 -0.0
CurrentTrain: epoch  4, batch    30 | loss: 13.3459501Losses:  11.990471839904785 4.766274929046631 -0.0
CurrentTrain: epoch  4, batch    31 | loss: 11.9904718Losses:  13.726102828979492 7.040261268615723 -0.0
CurrentTrain: epoch  4, batch    32 | loss: 13.7261028Losses:  12.004900932312012 5.202247619628906 -0.0
CurrentTrain: epoch  4, batch    33 | loss: 12.0049009Losses:  12.419218063354492 4.30552864074707 -0.0
CurrentTrain: epoch  4, batch    34 | loss: 12.4192181Losses:  15.811532974243164 7.8989362716674805 -0.0
CurrentTrain: epoch  4, batch    35 | loss: 15.8115330Losses:  12.892571449279785 5.739889144897461 -0.0
CurrentTrain: epoch  4, batch    36 | loss: 12.8925714Losses:  8.923572540283203 0.8716391921043396 -0.0
CurrentTrain: epoch  4, batch    37 | loss: 8.9235725Losses:  11.814343452453613 5.410247802734375 -0.0
CurrentTrain: epoch  5, batch     0 | loss: 11.8143435Losses:  13.451406478881836 6.363195419311523 -0.0
CurrentTrain: epoch  5, batch     1 | loss: 13.4514065Losses:  15.202718734741211 7.245578289031982 -0.0
CurrentTrain: epoch  5, batch     2 | loss: 15.2027187Losses:  14.679359436035156 7.44461727142334 -0.0
CurrentTrain: epoch  5, batch     3 | loss: 14.6793594Losses:  12.97307014465332 6.026134490966797 -0.0
CurrentTrain: epoch  5, batch     4 | loss: 12.9730701Losses:  13.595890998840332 6.194680213928223 -0.0
CurrentTrain: epoch  5, batch     5 | loss: 13.5958910Losses:  14.78137493133545 7.152933120727539 -0.0
CurrentTrain: epoch  5, batch     6 | loss: 14.7813749Losses:  12.192419052124023 5.175866603851318 -0.0
CurrentTrain: epoch  5, batch     7 | loss: 12.1924191Losses:  11.837297439575195 5.510537147521973 -0.0
CurrentTrain: epoch  5, batch     8 | loss: 11.8372974Losses:  13.00479507446289 5.628725528717041 -0.0
CurrentTrain: epoch  5, batch     9 | loss: 13.0047951Losses:  10.942119598388672 4.267449855804443 -0.0
CurrentTrain: epoch  5, batch    10 | loss: 10.9421196Losses:  11.535943984985352 5.430887222290039 -0.0
CurrentTrain: epoch  5, batch    11 | loss: 11.5359440Losses:  17.882144927978516 12.067161560058594 -0.0
CurrentTrain: epoch  5, batch    12 | loss: 17.8821449Losses:  14.106161117553711 7.2451019287109375 -0.0
CurrentTrain: epoch  5, batch    13 | loss: 14.1061611Losses:  12.628165245056152 4.424262046813965 -0.0
CurrentTrain: epoch  5, batch    14 | loss: 12.6281652Losses:  15.382991790771484 8.329071044921875 -0.0
CurrentTrain: epoch  5, batch    15 | loss: 15.3829918Losses:  14.350382804870605 7.28812313079834 -0.0
CurrentTrain: epoch  5, batch    16 | loss: 14.3503828Losses:  13.418148040771484 6.580997467041016 -0.0
CurrentTrain: epoch  5, batch    17 | loss: 13.4181480Losses:  14.089622497558594 6.464834213256836 -0.0
CurrentTrain: epoch  5, batch    18 | loss: 14.0896225Losses:  16.300065994262695 8.960378646850586 -0.0
CurrentTrain: epoch  5, batch    19 | loss: 16.3000660Losses:  11.483558654785156 5.063997268676758 -0.0
CurrentTrain: epoch  5, batch    20 | loss: 11.4835587Losses:  11.66391658782959 5.536825180053711 -0.0
CurrentTrain: epoch  5, batch    21 | loss: 11.6639166Losses:  13.616398811340332 6.883108139038086 -0.0
CurrentTrain: epoch  5, batch    22 | loss: 13.6163988Losses:  14.115381240844727 8.222721099853516 -0.0
CurrentTrain: epoch  5, batch    23 | loss: 14.1153812Losses:  11.267355918884277 5.297199249267578 -0.0
CurrentTrain: epoch  5, batch    24 | loss: 11.2673559Losses:  13.50566291809082 6.977528095245361 -0.0
CurrentTrain: epoch  5, batch    25 | loss: 13.5056629Losses:  13.076292037963867 5.739789009094238 -0.0
CurrentTrain: epoch  5, batch    26 | loss: 13.0762920Losses:  12.349884986877441 5.489336967468262 -0.0
CurrentTrain: epoch  5, batch    27 | loss: 12.3498850Losses:  11.906750679016113 5.621938705444336 -0.0
CurrentTrain: epoch  5, batch    28 | loss: 11.9067507Losses:  11.955888748168945 5.852724075317383 -0.0
CurrentTrain: epoch  5, batch    29 | loss: 11.9558887Losses:  14.932147979736328 8.465944290161133 -0.0
CurrentTrain: epoch  5, batch    30 | loss: 14.9321480Losses:  12.481489181518555 5.9664130210876465 -0.0
CurrentTrain: epoch  5, batch    31 | loss: 12.4814892Losses:  13.193289756774902 7.622735023498535 -0.0
CurrentTrain: epoch  5, batch    32 | loss: 13.1932898Losses:  13.997519493103027 7.1200714111328125 -0.0
CurrentTrain: epoch  5, batch    33 | loss: 13.9975195Losses:  14.99414348602295 7.708388328552246 -0.0
CurrentTrain: epoch  5, batch    34 | loss: 14.9941435Losses:  16.973724365234375 10.114034652709961 -0.0
CurrentTrain: epoch  5, batch    35 | loss: 16.9737244Losses:  14.76307487487793 8.115144729614258 -0.0
CurrentTrain: epoch  5, batch    36 | loss: 14.7630749Losses:  7.337567329406738 1.1876380443572998 -0.0
CurrentTrain: epoch  5, batch    37 | loss: 7.3375673Losses:  10.606855392456055 4.8845534324646 -0.0
CurrentTrain: epoch  6, batch     0 | loss: 10.6068554Losses:  9.663219451904297 4.076755523681641 -0.0
CurrentTrain: epoch  6, batch     1 | loss: 9.6632195Losses:  12.04964828491211 6.565110683441162 -0.0
CurrentTrain: epoch  6, batch     2 | loss: 12.0496483Losses:  14.338713645935059 6.935483932495117 -0.0
CurrentTrain: epoch  6, batch     3 | loss: 14.3387136Losses:  10.183357238769531 4.723956108093262 -0.0
CurrentTrain: epoch  6, batch     4 | loss: 10.1833572Losses:  11.935829162597656 5.869594097137451 -0.0
CurrentTrain: epoch  6, batch     5 | loss: 11.9358292Losses:  12.417559623718262 6.968792915344238 -0.0
CurrentTrain: epoch  6, batch     6 | loss: 12.4175596Losses:  13.93321418762207 7.309352874755859 -0.0
CurrentTrain: epoch  6, batch     7 | loss: 13.9332142Losses:  13.65564250946045 6.383227825164795 -0.0
CurrentTrain: epoch  6, batch     8 | loss: 13.6556425Losses:  11.961151123046875 5.610177993774414 -0.0
CurrentTrain: epoch  6, batch     9 | loss: 11.9611511Losses:  12.190557479858398 5.777579307556152 -0.0
CurrentTrain: epoch  6, batch    10 | loss: 12.1905575Losses:  11.496988296508789 4.6006927490234375 -0.0
CurrentTrain: epoch  6, batch    11 | loss: 11.4969883Losses:  16.555078506469727 10.6563720703125 -0.0
CurrentTrain: epoch  6, batch    12 | loss: 16.5550785Losses:  11.90320873260498 5.908596992492676 -0.0
CurrentTrain: epoch  6, batch    13 | loss: 11.9032087Losses:  12.338608741760254 6.4126081466674805 -0.0
CurrentTrain: epoch  6, batch    14 | loss: 12.3386087Losses:  11.554252624511719 5.9312310218811035 -0.0
CurrentTrain: epoch  6, batch    15 | loss: 11.5542526Losses:  9.724964141845703 4.265735626220703 -0.0
CurrentTrain: epoch  6, batch    16 | loss: 9.7249641Losses:  10.832969665527344 4.953547477722168 -0.0
CurrentTrain: epoch  6, batch    17 | loss: 10.8329697Losses:  10.654657363891602 5.232347011566162 -0.0
CurrentTrain: epoch  6, batch    18 | loss: 10.6546574Losses:  13.27184009552002 6.632933139801025 -0.0
CurrentTrain: epoch  6, batch    19 | loss: 13.2718401Losses:  11.05206298828125 5.106240272521973 -0.0
CurrentTrain: epoch  6, batch    20 | loss: 11.0520630Losses:  11.46776008605957 5.855593204498291 -0.0
CurrentTrain: epoch  6, batch    21 | loss: 11.4677601Losses:  11.644372940063477 5.333247184753418 -0.0
CurrentTrain: epoch  6, batch    22 | loss: 11.6443729Losses:  18.646800994873047 12.462907791137695 -0.0
CurrentTrain: epoch  6, batch    23 | loss: 18.6468010Losses:  11.540746688842773 5.915933609008789 -0.0
CurrentTrain: epoch  6, batch    24 | loss: 11.5407467Losses:  17.089839935302734 10.88900375366211 -0.0
CurrentTrain: epoch  6, batch    25 | loss: 17.0898399Losses:  9.355566024780273 4.1107988357543945 -0.0
CurrentTrain: epoch  6, batch    26 | loss: 9.3555660Losses:  9.490385055541992 4.277555465698242 -0.0
CurrentTrain: epoch  6, batch    27 | loss: 9.4903851Losses:  16.50384521484375 11.021867752075195 -0.0
CurrentTrain: epoch  6, batch    28 | loss: 16.5038452Losses:  11.814495086669922 6.316411972045898 -0.0
CurrentTrain: epoch  6, batch    29 | loss: 11.8144951Losses:  14.757728576660156 8.195911407470703 -0.0
CurrentTrain: epoch  6, batch    30 | loss: 14.7577286Losses:  9.560094833374023 4.007482051849365 -0.0
CurrentTrain: epoch  6, batch    31 | loss: 9.5600948Losses:  10.283309936523438 4.965916156768799 -0.0
CurrentTrain: epoch  6, batch    32 | loss: 10.2833099Losses:  11.010257720947266 5.9375319480896 -0.0
CurrentTrain: epoch  6, batch    33 | loss: 11.0102577Losses:  14.737658500671387 9.383367538452148 -0.0
CurrentTrain: epoch  6, batch    34 | loss: 14.7376585Losses:  12.837126731872559 6.630870819091797 -0.0
CurrentTrain: epoch  6, batch    35 | loss: 12.8371267Losses:  22.56279945373535 15.849674224853516 -0.0
CurrentTrain: epoch  6, batch    36 | loss: 22.5627995Losses:  8.979555130004883 2.1486964225769043 -0.0
CurrentTrain: epoch  6, batch    37 | loss: 8.9795551Losses:  15.725076675415039 9.474299430847168 -0.0
CurrentTrain: epoch  7, batch     0 | loss: 15.7250767Losses:  11.20668888092041 5.181462287902832 -0.0
CurrentTrain: epoch  7, batch     1 | loss: 11.2066889Losses:  12.951443672180176 6.6375732421875 -0.0
CurrentTrain: epoch  7, batch     2 | loss: 12.9514437Losses:  10.075153350830078 4.407323360443115 -0.0
CurrentTrain: epoch  7, batch     3 | loss: 10.0751534Losses:  13.255897521972656 7.522319316864014 -0.0
CurrentTrain: epoch  7, batch     4 | loss: 13.2558975Losses:  10.173988342285156 4.546140193939209 -0.0
CurrentTrain: epoch  7, batch     5 | loss: 10.1739883Losses:  13.6128568649292 8.056829452514648 -0.0
CurrentTrain: epoch  7, batch     6 | loss: 13.6128569Losses:  11.49124526977539 5.342282772064209 -0.0
CurrentTrain: epoch  7, batch     7 | loss: 11.4912453Losses:  11.384733200073242 6.103965759277344 -0.0
CurrentTrain: epoch  7, batch     8 | loss: 11.3847332Losses:  10.314104080200195 4.273970603942871 -0.0
CurrentTrain: epoch  7, batch     9 | loss: 10.3141041Losses:  15.056571960449219 8.002114295959473 -0.0
CurrentTrain: epoch  7, batch    10 | loss: 15.0565720Losses:  12.646154403686523 7.415543556213379 -0.0
CurrentTrain: epoch  7, batch    11 | loss: 12.6461544Losses:  12.90932559967041 7.660042762756348 -0.0
CurrentTrain: epoch  7, batch    12 | loss: 12.9093256Losses:  11.269445419311523 5.266851425170898 -0.0
CurrentTrain: epoch  7, batch    13 | loss: 11.2694454Losses:  11.081781387329102 5.29245662689209 -0.0
CurrentTrain: epoch  7, batch    14 | loss: 11.0817814Losses:  16.976581573486328 10.533313751220703 -0.0
CurrentTrain: epoch  7, batch    15 | loss: 16.9765816Losses:  13.457330703735352 6.862020015716553 -0.0
CurrentTrain: epoch  7, batch    16 | loss: 13.4573307Losses:  13.903228759765625 8.125041961669922 -0.0
CurrentTrain: epoch  7, batch    17 | loss: 13.9032288Losses:  10.657920837402344 5.148531913757324 -0.0
CurrentTrain: epoch  7, batch    18 | loss: 10.6579208Losses:  9.576261520385742 3.9581615924835205 -0.0
CurrentTrain: epoch  7, batch    19 | loss: 9.5762615Losses:  13.750699996948242 7.242319583892822 -0.0
CurrentTrain: epoch  7, batch    20 | loss: 13.7507000Losses:  12.458808898925781 7.4468793869018555 -0.0
CurrentTrain: epoch  7, batch    21 | loss: 12.4588089Losses:  14.862236022949219 8.072444915771484 -0.0
CurrentTrain: epoch  7, batch    22 | loss: 14.8622360Losses:  12.062969207763672 6.484224796295166 -0.0
CurrentTrain: epoch  7, batch    23 | loss: 12.0629692Losses:  14.036612510681152 8.003445625305176 -0.0
CurrentTrain: epoch  7, batch    24 | loss: 14.0366125Losses:  15.320890426635742 8.327071189880371 -0.0
CurrentTrain: epoch  7, batch    25 | loss: 15.3208904Losses:  11.36811351776123 5.726740837097168 -0.0
CurrentTrain: epoch  7, batch    26 | loss: 11.3681135Losses:  17.314403533935547 10.606681823730469 -0.0
CurrentTrain: epoch  7, batch    27 | loss: 17.3144035Losses:  12.323512077331543 7.121515274047852 -0.0
CurrentTrain: epoch  7, batch    28 | loss: 12.3235121Losses:  11.973543167114258 5.148767471313477 -0.0
CurrentTrain: epoch  7, batch    29 | loss: 11.9735432Losses:  10.021978378295898 4.711263656616211 -0.0
CurrentTrain: epoch  7, batch    30 | loss: 10.0219784Losses:  14.916719436645508 8.832110404968262 -0.0
CurrentTrain: epoch  7, batch    31 | loss: 14.9167194Losses:  12.527389526367188 6.212935447692871 -0.0
CurrentTrain: epoch  7, batch    32 | loss: 12.5273895Losses:  11.23891830444336 6.206557273864746 -0.0
CurrentTrain: epoch  7, batch    33 | loss: 11.2389183Losses:  12.595458984375 6.868560314178467 -0.0
CurrentTrain: epoch  7, batch    34 | loss: 12.5954590Losses:  11.216262817382812 5.156486511230469 -0.0
CurrentTrain: epoch  7, batch    35 | loss: 11.2162628Losses:  10.79251766204834 4.888012886047363 -0.0
CurrentTrain: epoch  7, batch    36 | loss: 10.7925177Losses:  8.935428619384766 3.080301284790039 -0.0
CurrentTrain: epoch  7, batch    37 | loss: 8.9354286Losses:  11.271466255187988 5.244897842407227 -0.0
CurrentTrain: epoch  8, batch     0 | loss: 11.2714663Losses:  9.154752731323242 3.876854419708252 -0.0
CurrentTrain: epoch  8, batch     1 | loss: 9.1547527Losses:  11.453479766845703 5.59630012512207 -0.0
CurrentTrain: epoch  8, batch     2 | loss: 11.4534798Losses:  11.287849426269531 4.946346282958984 -0.0
CurrentTrain: epoch  8, batch     3 | loss: 11.2878494Losses:  10.306154251098633 5.1788177490234375 -0.0
CurrentTrain: epoch  8, batch     4 | loss: 10.3061543Losses:  10.686046600341797 5.216014385223389 -0.0
CurrentTrain: epoch  8, batch     5 | loss: 10.6860466Losses:  12.877931594848633 6.863417625427246 -0.0
CurrentTrain: epoch  8, batch     6 | loss: 12.8779316Losses:  10.390623092651367 5.136186599731445 -0.0
CurrentTrain: epoch  8, batch     7 | loss: 10.3906231Losses:  11.936515808105469 5.901867866516113 -0.0
CurrentTrain: epoch  8, batch     8 | loss: 11.9365158Losses:  9.815710067749023 4.055878639221191 -0.0
CurrentTrain: epoch  8, batch     9 | loss: 9.8157101Losses:  10.509795188903809 5.2567033767700195 -0.0
CurrentTrain: epoch  8, batch    10 | loss: 10.5097952Losses:  10.96069049835205 5.753517150878906 -0.0
CurrentTrain: epoch  8, batch    11 | loss: 10.9606905Losses:  10.604475021362305 4.754979133605957 -0.0
CurrentTrain: epoch  8, batch    12 | loss: 10.6044750Losses:  9.497469902038574 4.162554740905762 -0.0
CurrentTrain: epoch  8, batch    13 | loss: 9.4974699Losses:  9.856060028076172 4.611900329589844 -0.0
CurrentTrain: epoch  8, batch    14 | loss: 9.8560600Losses:  11.524433135986328 6.579837799072266 -0.0
CurrentTrain: epoch  8, batch    15 | loss: 11.5244331Losses:  9.308111190795898 3.941854476928711 -0.0
CurrentTrain: epoch  8, batch    16 | loss: 9.3081112Losses:  18.01652717590332 12.902045249938965 -0.0
CurrentTrain: epoch  8, batch    17 | loss: 18.0165272Losses:  11.616392135620117 6.1906890869140625 -0.0
CurrentTrain: epoch  8, batch    18 | loss: 11.6163921Losses:  10.070046424865723 4.869926452636719 -0.0
CurrentTrain: epoch  8, batch    19 | loss: 10.0700464Losses:  10.828958511352539 5.606718063354492 -0.0
CurrentTrain: epoch  8, batch    20 | loss: 10.8289585Losses:  9.623215675354004 4.344605445861816 -0.0
CurrentTrain: epoch  8, batch    21 | loss: 9.6232157Losses:  11.15399169921875 6.093506336212158 -0.0
CurrentTrain: epoch  8, batch    22 | loss: 11.1539917Losses:  10.233154296875 5.215223789215088 -0.0
CurrentTrain: epoch  8, batch    23 | loss: 10.2331543Losses:  16.661094665527344 11.450431823730469 -0.0
CurrentTrain: epoch  8, batch    24 | loss: 16.6610947Losses:  16.62674331665039 10.6541166305542 -0.0
CurrentTrain: epoch  8, batch    25 | loss: 16.6267433Losses:  10.743544578552246 5.569611549377441 -0.0
CurrentTrain: epoch  8, batch    26 | loss: 10.7435446Losses:  11.477873802185059 6.108068466186523 -0.0
CurrentTrain: epoch  8, batch    27 | loss: 11.4778738Losses:  10.231654167175293 5.348882675170898 -0.0
CurrentTrain: epoch  8, batch    28 | loss: 10.2316542Losses:  10.516159057617188 5.224485397338867 -0.0
CurrentTrain: epoch  8, batch    29 | loss: 10.5161591Losses:  10.927003860473633 5.082187652587891 -0.0
CurrentTrain: epoch  8, batch    30 | loss: 10.9270039Losses:  8.936863899230957 3.861870050430298 -0.0
CurrentTrain: epoch  8, batch    31 | loss: 8.9368639Losses:  10.106653213500977 5.199402809143066 -0.0
CurrentTrain: epoch  8, batch    32 | loss: 10.1066532Losses:  10.229665756225586 5.3875555992126465 -0.0
CurrentTrain: epoch  8, batch    33 | loss: 10.2296658Losses:  10.016958236694336 5.10797119140625 -0.0
CurrentTrain: epoch  8, batch    34 | loss: 10.0169582Losses:  12.053518295288086 5.9868292808532715 -0.0
CurrentTrain: epoch  8, batch    35 | loss: 12.0535183Losses:  16.04892349243164 11.410259246826172 -0.0
CurrentTrain: epoch  8, batch    36 | loss: 16.0489235Losses:  8.280933380126953 3.123199224472046 -0.0
CurrentTrain: epoch  8, batch    37 | loss: 8.2809334Losses:  9.169559478759766 4.210813999176025 -0.0
CurrentTrain: epoch  9, batch     0 | loss: 9.1695595Losses:  10.923022270202637 6.1104254722595215 -0.0
CurrentTrain: epoch  9, batch     1 | loss: 10.9230223Losses:  11.571224212646484 6.697018623352051 -0.0
CurrentTrain: epoch  9, batch     2 | loss: 11.5712242Losses:  11.199359893798828 5.990431308746338 -0.0
CurrentTrain: epoch  9, batch     3 | loss: 11.1993599Losses:  11.423491477966309 6.084184646606445 -0.0
CurrentTrain: epoch  9, batch     4 | loss: 11.4234915Losses:  10.561254501342773 4.690485000610352 -0.0
CurrentTrain: epoch  9, batch     5 | loss: 10.5612545Losses:  9.647576332092285 4.570158958435059 -0.0
CurrentTrain: epoch  9, batch     6 | loss: 9.6475763Losses:  11.678430557250977 6.5969414710998535 -0.0
CurrentTrain: epoch  9, batch     7 | loss: 11.6784306Losses:  9.48237419128418 4.208889007568359 -0.0
CurrentTrain: epoch  9, batch     8 | loss: 9.4823742Losses:  10.697467803955078 5.575229644775391 -0.0
CurrentTrain: epoch  9, batch     9 | loss: 10.6974678Losses:  9.157354354858398 3.586855173110962 -0.0
CurrentTrain: epoch  9, batch    10 | loss: 9.1573544Losses:  10.917058944702148 6.121873378753662 -0.0
CurrentTrain: epoch  9, batch    11 | loss: 10.9170589Losses:  9.776322364807129 4.719446182250977 -0.0
CurrentTrain: epoch  9, batch    12 | loss: 9.7763224Losses:  9.093607902526855 4.185499668121338 -0.0
CurrentTrain: epoch  9, batch    13 | loss: 9.0936079Losses:  9.418066024780273 4.307610511779785 -0.0
CurrentTrain: epoch  9, batch    14 | loss: 9.4180660Losses:  10.6694917678833 5.085038185119629 -0.0
CurrentTrain: epoch  9, batch    15 | loss: 10.6694918Losses:  8.512643814086914 3.4110255241394043 -0.0
CurrentTrain: epoch  9, batch    16 | loss: 8.5126438Losses:  9.011804580688477 4.175590515136719 -0.0
CurrentTrain: epoch  9, batch    17 | loss: 9.0118046Losses:  12.17131233215332 7.397763729095459 -0.0
CurrentTrain: epoch  9, batch    18 | loss: 12.1713123Losses:  13.133598327636719 8.051916122436523 -0.0
CurrentTrain: epoch  9, batch    19 | loss: 13.1335983Losses:  12.242546081542969 7.087882041931152 -0.0
CurrentTrain: epoch  9, batch    20 | loss: 12.2425461Losses:  9.919103622436523 4.762719631195068 -0.0
CurrentTrain: epoch  9, batch    21 | loss: 9.9191036Losses:  8.745923042297363 3.516662120819092 -0.0
CurrentTrain: epoch  9, batch    22 | loss: 8.7459230Losses:  10.34642219543457 5.600604057312012 -0.0
CurrentTrain: epoch  9, batch    23 | loss: 10.3464222Losses:  11.326648712158203 5.68932580947876 -0.0
CurrentTrain: epoch  9, batch    24 | loss: 11.3266487Losses:  9.457135200500488 4.34311580657959 -0.0
CurrentTrain: epoch  9, batch    25 | loss: 9.4571352Losses:  11.78193187713623 7.041745662689209 -0.0
CurrentTrain: epoch  9, batch    26 | loss: 11.7819319Losses:  11.418830871582031 5.524435043334961 -0.0
CurrentTrain: epoch  9, batch    27 | loss: 11.4188309Losses:  10.899945259094238 5.400906562805176 -0.0
CurrentTrain: epoch  9, batch    28 | loss: 10.8999453Losses:  10.005958557128906 4.659780025482178 -0.0
CurrentTrain: epoch  9, batch    29 | loss: 10.0059586Losses:  10.772078514099121 5.406164169311523 -0.0
CurrentTrain: epoch  9, batch    30 | loss: 10.7720785Losses:  9.688238143920898 4.695326805114746 -0.0
CurrentTrain: epoch  9, batch    31 | loss: 9.6882381Losses:  12.353580474853516 7.275007724761963 -0.0
CurrentTrain: epoch  9, batch    32 | loss: 12.3535805Losses:  12.649343490600586 7.326652526855469 -0.0
CurrentTrain: epoch  9, batch    33 | loss: 12.6493435Losses:  11.177169799804688 5.8461198806762695 -0.0
CurrentTrain: epoch  9, batch    34 | loss: 11.1771698Losses:  9.654855728149414 4.339748382568359 -0.0
CurrentTrain: epoch  9, batch    35 | loss: 9.6548557Losses:  9.692461013793945 4.634021759033203 -0.0
CurrentTrain: epoch  9, batch    36 | loss: 9.6924610Losses:  6.039278984069824 1.4993350505828857 -0.0
CurrentTrain: epoch  9, batch    37 | loss: 6.0392790
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 68.75%,  total acc: 75.00%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 79.17%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 76.56%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 75.00%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 76.79%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 79.69%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 81.94%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 83.12%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 84.66%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 85.42%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 85.58%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 84.38%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 83.75%   [EVAL] batch:   15 | acc: 62.50%,  total acc: 82.42%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 81.99%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 80.90%   [EVAL] batch:   18 | acc: 75.00%,  total acc: 80.59%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 82.14%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 82.95%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 83.70%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 84.38%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 85.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 85.58%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 85.88%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 86.38%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 86.85%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 86.88%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 86.90%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 87.11%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 85.61%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 68.75%,  total acc: 75.00%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 79.17%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 76.56%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 75.00%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 76.79%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 79.69%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 81.94%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 83.12%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 84.66%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 85.42%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 85.58%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 84.38%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 83.75%   [EVAL] batch:   15 | acc: 62.50%,  total acc: 82.42%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 81.99%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 80.90%   [EVAL] batch:   18 | acc: 75.00%,  total acc: 80.59%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 82.14%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 82.95%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 83.70%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 84.38%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 85.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 85.58%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 85.88%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 86.38%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 86.85%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 86.88%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 86.90%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 87.11%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 85.61%   
cur_acc:  ['0.8561']
his_acc:  ['0.8561']
Clustering into  4  clusters
Clusters:  [2 1 3 2 0 2 2 0 1 0 0]
Losses:  16.916139602661133 8.467820167541504 1.4090957641601562
CurrentTrain: epoch  0, batch     0 | loss: 16.9161396Losses:  12.154483795166016 3.4275753498077393 1.4248740673065186
CurrentTrain: epoch  0, batch     1 | loss: 12.1544838Losses:  15.197092056274414 7.8610029220581055 1.4397187232971191
CurrentTrain: epoch  1, batch     0 | loss: 15.1970921Losses:  9.635854721069336 2.26656436920166 1.3942735195159912
CurrentTrain: epoch  1, batch     1 | loss: 9.6358547Losses:  15.189725875854492 8.118368148803711 1.431890845298767
CurrentTrain: epoch  2, batch     0 | loss: 15.1897259Losses:  9.45947551727295 2.8085439205169678 1.410115122795105
CurrentTrain: epoch  2, batch     1 | loss: 9.4594755Losses:  14.034724235534668 8.247658729553223 1.4841521978378296
CurrentTrain: epoch  3, batch     0 | loss: 14.0347242Losses:  11.325447082519531 5.451562881469727 -0.0
CurrentTrain: epoch  3, batch     1 | loss: 11.3254471Losses:  13.69202995300293 8.18980598449707 1.4590518474578857
CurrentTrain: epoch  4, batch     0 | loss: 13.6920300Losses:  10.156364440917969 3.786484718322754 1.4136087894439697
CurrentTrain: epoch  4, batch     1 | loss: 10.1563644Losses:  11.470980644226074 6.171505928039551 1.4467952251434326
CurrentTrain: epoch  5, batch     0 | loss: 11.4709806Losses:  7.307941436767578 1.97666597366333 1.4208648204803467
CurrentTrain: epoch  5, batch     1 | loss: 7.3079414Losses:  12.09439754486084 6.669501781463623 1.4425630569458008
CurrentTrain: epoch  6, batch     0 | loss: 12.0943975Losses:  6.672428131103516 2.170142889022827 1.4066264629364014
CurrentTrain: epoch  6, batch     1 | loss: 6.6724281Losses:  12.026276588439941 7.375675201416016 1.4485652446746826
CurrentTrain: epoch  7, batch     0 | loss: 12.0262766Losses:  6.293210506439209 3.245654582977295 -0.0
CurrentTrain: epoch  7, batch     1 | loss: 6.2932105Losses:  12.436681747436523 7.5503692626953125 1.4308671951293945
CurrentTrain: epoch  8, batch     0 | loss: 12.4366817Losses:  7.083388328552246 2.860104560852051 1.4003652334213257
CurrentTrain: epoch  8, batch     1 | loss: 7.0833883Losses:  10.320545196533203 5.9087677001953125 1.4170715808868408
CurrentTrain: epoch  9, batch     0 | loss: 10.3205452Losses:  5.211598873138428 1.284338355064392 1.3909416198730469
CurrentTrain: epoch  9, batch     1 | loss: 5.2115989
Losses:  5.952205657958984 -0.0 3.7702479362487793
MemoryTrain:  epoch  0, batch     0 | loss: 5.9522057Losses:  5.7399797439575195 -0.0 3.643777370452881
MemoryTrain:  epoch  1, batch     0 | loss: 5.7399797Losses:  4.814624786376953 -0.0 3.4619359970092773
MemoryTrain:  epoch  2, batch     0 | loss: 4.8146248Losses:  3.8307909965515137 -0.0 3.4138951301574707
MemoryTrain:  epoch  3, batch     0 | loss: 3.8307910Losses:  3.8692383766174316 -0.0 3.3659448623657227
MemoryTrain:  epoch  4, batch     0 | loss: 3.8692384Losses:  3.549839496612549 -0.0 3.3508541584014893
MemoryTrain:  epoch  5, batch     0 | loss: 3.5498395Losses:  3.4474706649780273 -0.0 3.3279240131378174
MemoryTrain:  epoch  6, batch     0 | loss: 3.4474707Losses:  3.4167726039886475 -0.0 3.3427696228027344
MemoryTrain:  epoch  7, batch     0 | loss: 3.4167726Losses:  3.3635871410369873 -0.0 3.335216999053955
MemoryTrain:  epoch  8, batch     0 | loss: 3.3635871Losses:  3.3536620140075684 -0.0 3.3160312175750732
MemoryTrain:  epoch  9, batch     0 | loss: 3.3536620
[EVAL] batch:    0 | acc: 6.25%,  total acc: 6.25%   [EVAL] batch:    1 | acc: 0.00%,  total acc: 3.12%   [EVAL] batch:    2 | acc: 0.00%,  total acc: 2.08%   [EVAL] batch:    3 | acc: 6.25%,  total acc: 3.12%   [EVAL] batch:    4 | acc: 0.00%,  total acc: 2.50%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 16.67%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 28.57%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 36.72%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 42.36%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 44.38%   [EVAL] batch:   10 | acc: 68.75%,  total acc: 46.59%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 49.48%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 50.96%   [EVAL] batch:   13 | acc: 81.25%,  total acc: 53.12%   [EVAL] batch:   14 | acc: 37.50%,  total acc: 52.08%   
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 82.81%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 83.75%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 83.33%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 84.82%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 86.72%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 88.19%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 88.75%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 89.77%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 89.58%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 88.94%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 87.50%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 86.67%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 84.77%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 84.19%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 82.99%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 82.24%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 82.50%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 83.33%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 84.09%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 84.51%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 85.16%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 85.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 86.30%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 86.57%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 87.05%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   29 | acc: 93.75%,  total acc: 87.71%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 87.70%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 88.09%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 86.55%   [EVAL] batch:   33 | acc: 6.25%,  total acc: 84.19%   [EVAL] batch:   34 | acc: 0.00%,  total acc: 81.79%   [EVAL] batch:   35 | acc: 6.25%,  total acc: 79.69%   [EVAL] batch:   36 | acc: 0.00%,  total acc: 77.53%   [EVAL] batch:   37 | acc: 56.25%,  total acc: 76.97%   [EVAL] batch:   38 | acc: 93.75%,  total acc: 77.40%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 77.97%   [EVAL] batch:   40 | acc: 87.50%,  total acc: 78.20%   [EVAL] batch:   41 | acc: 81.25%,  total acc: 78.27%   [EVAL] batch:   42 | acc: 62.50%,  total acc: 77.91%   [EVAL] batch:   43 | acc: 62.50%,  total acc: 77.56%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 77.78%   [EVAL] batch:   45 | acc: 68.75%,  total acc: 77.58%   [EVAL] batch:   46 | acc: 68.75%,  total acc: 77.39%   
cur_acc:  ['0.8561', '0.5208']
his_acc:  ['0.8561', '0.7739']
Clustering into  7  clusters
Clusters:  [2 5 3 1 0 1 2 4 6 0 0 4 2 0 5 2]
Losses:  19.935199737548828 9.06886100769043 3.653050422668457
CurrentTrain: epoch  0, batch     0 | loss: 19.9351997Losses:  13.085099220275879 4.291653156280518 1.4666879177093506
CurrentTrain: epoch  0, batch     1 | loss: 13.0850992Losses:  17.21515655517578 7.783687591552734 3.4780917167663574
CurrentTrain: epoch  1, batch     0 | loss: 17.2151566Losses:  14.183767318725586 3.2322795391082764 3.3387250900268555
CurrentTrain: epoch  1, batch     1 | loss: 14.1837673Losses:  16.30076026916504 7.324262619018555 3.3706908226013184
CurrentTrain: epoch  2, batch     0 | loss: 16.3007603Losses:  11.470478057861328 2.6631391048431396 3.347536087036133
CurrentTrain: epoch  2, batch     1 | loss: 11.4704781Losses:  14.356502532958984 7.0883917808532715 3.345541000366211
CurrentTrain: epoch  3, batch     0 | loss: 14.3565025Losses:  10.351180076599121 2.5922634601593018 3.424825429916382
CurrentTrain: epoch  3, batch     1 | loss: 10.3511801Losses:  13.456336975097656 6.758432865142822 3.3665268421173096
CurrentTrain: epoch  4, batch     0 | loss: 13.4563370Losses:  10.099099159240723 2.6245832443237305 3.396514415740967
CurrentTrain: epoch  4, batch     1 | loss: 10.0990992Losses:  13.869976043701172 6.851814270019531 3.3737986087799072
CurrentTrain: epoch  5, batch     0 | loss: 13.8699760Losses:  8.269143104553223 1.9263204336166382 3.3746094703674316
CurrentTrain: epoch  5, batch     1 | loss: 8.2691431Losses:  13.354108810424805 7.2441725730896 3.369314193725586
CurrentTrain: epoch  6, batch     0 | loss: 13.3541088Losses:  9.67148208618164 3.4023640155792236 3.375211238861084
CurrentTrain: epoch  6, batch     1 | loss: 9.6714821Losses:  13.849985122680664 7.39771032333374 3.3846917152404785
CurrentTrain: epoch  7, batch     0 | loss: 13.8499851Losses:  8.212066650390625 2.2773497104644775 3.3683245182037354
CurrentTrain: epoch  7, batch     1 | loss: 8.2120667Losses:  13.310110092163086 7.150886058807373 3.365110158920288
CurrentTrain: epoch  8, batch     0 | loss: 13.3101101Losses:  8.448073387145996 2.84232759475708 3.3458664417266846
CurrentTrain: epoch  8, batch     1 | loss: 8.4480734Losses:  11.843812942504883 6.163779258728027 3.335122585296631
CurrentTrain: epoch  9, batch     0 | loss: 11.8438129Losses:  7.2803497314453125 1.8051213026046753 3.337616443634033
CurrentTrain: epoch  9, batch     1 | loss: 7.2803497
Losses:  13.114408493041992 -0.0 11.210115432739258
MemoryTrain:  epoch  0, batch     0 | loss: 13.1144085Losses:  12.952461242675781 -0.0 11.101927757263184
MemoryTrain:  epoch  1, batch     0 | loss: 12.9524612Losses:  12.195236206054688 -0.0 11.14037036895752
MemoryTrain:  epoch  2, batch     0 | loss: 12.1952362Losses:  12.187070846557617 -0.0 11.034456253051758
MemoryTrain:  epoch  3, batch     0 | loss: 12.1870708Losses:  11.967126846313477 -0.0 10.928093910217285
MemoryTrain:  epoch  4, batch     0 | loss: 11.9671268Losses:  11.475035667419434 -0.0 10.884546279907227
MemoryTrain:  epoch  5, batch     0 | loss: 11.4750357Losses:  11.527599334716797 -0.0 10.902387619018555
MemoryTrain:  epoch  6, batch     0 | loss: 11.5275993Losses:  11.546916961669922 -0.0 10.868559837341309
MemoryTrain:  epoch  7, batch     0 | loss: 11.5469170Losses:  11.3445463180542 -0.0 10.863581657409668
MemoryTrain:  epoch  8, batch     0 | loss: 11.3445463Losses:  11.06596851348877 -0.0 10.848189353942871
MemoryTrain:  epoch  9, batch     0 | loss: 11.0659685
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 62.50%,  total acc: 75.00%   [EVAL] batch:    3 | acc: 56.25%,  total acc: 70.31%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 70.00%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 70.83%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 72.32%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 75.00%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 77.08%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 78.75%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 78.98%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 80.21%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 79.81%   [EVAL] batch:   13 | acc: 25.00%,  total acc: 75.89%   
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 62.50%,  total acc: 75.00%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    3 | acc: 62.50%,  total acc: 71.88%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 72.50%   [EVAL] batch:    5 | acc: 68.75%,  total acc: 71.88%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 73.21%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 76.56%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 79.17%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 80.62%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 82.39%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 82.81%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 83.17%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 82.14%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 81.67%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 80.08%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 79.78%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 78.82%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 78.29%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 78.75%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 79.76%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 80.68%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 81.77%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 82.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 83.17%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 83.56%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 84.15%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 84.70%   [EVAL] batch:   29 | acc: 93.75%,  total acc: 85.00%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 85.08%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 85.35%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 83.90%   [EVAL] batch:   33 | acc: 6.25%,  total acc: 81.62%   [EVAL] batch:   34 | acc: 0.00%,  total acc: 79.29%   [EVAL] batch:   35 | acc: 0.00%,  total acc: 77.08%   [EVAL] batch:   36 | acc: 0.00%,  total acc: 75.00%   [EVAL] batch:   37 | acc: 50.00%,  total acc: 74.34%   [EVAL] batch:   38 | acc: 87.50%,  total acc: 74.68%   [EVAL] batch:   39 | acc: 93.75%,  total acc: 75.16%   [EVAL] batch:   40 | acc: 75.00%,  total acc: 75.15%   [EVAL] batch:   41 | acc: 75.00%,  total acc: 75.15%   [EVAL] batch:   42 | acc: 43.75%,  total acc: 74.42%   [EVAL] batch:   43 | acc: 6.25%,  total acc: 72.87%   [EVAL] batch:   44 | acc: 12.50%,  total acc: 71.53%   [EVAL] batch:   45 | acc: 6.25%,  total acc: 70.11%   [EVAL] batch:   46 | acc: 18.75%,  total acc: 69.02%   [EVAL] batch:   47 | acc: 75.00%,  total acc: 69.14%   [EVAL] batch:   48 | acc: 93.75%,  total acc: 69.64%   [EVAL] batch:   49 | acc: 56.25%,  total acc: 69.38%   [EVAL] batch:   50 | acc: 56.25%,  total acc: 69.12%   [EVAL] batch:   51 | acc: 75.00%,  total acc: 69.23%   [EVAL] batch:   52 | acc: 75.00%,  total acc: 69.34%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 69.56%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 70.00%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 70.42%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 70.83%   [EVAL] batch:   57 | acc: 75.00%,  total acc: 70.91%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 71.40%   [EVAL] batch:   59 | acc: 68.75%,  total acc: 71.35%   [EVAL] batch:   60 | acc: 12.50%,  total acc: 70.39%   
cur_acc:  ['0.8561', '0.5208', '0.7589']
his_acc:  ['0.8561', '0.7739', '0.7039']
Clustering into  9  clusters
Clusters:  [1 3 7 2 0 2 1 4 8 0 0 4 1 0 3 1 1 5 1 0 6]
Losses:  17.877851486206055 8.496864318847656 3.4308273792266846
CurrentTrain: epoch  0, batch     0 | loss: 17.8778515Losses:  11.456626892089844 2.6990139484405518 3.366849184036255
CurrentTrain: epoch  0, batch     1 | loss: 11.4566269Losses:  16.400165557861328 7.824630260467529 3.3233389854431152
CurrentTrain: epoch  1, batch     0 | loss: 16.4001656Losses:  9.453046798706055 2.1294662952423096 3.346230983734131
CurrentTrain: epoch  1, batch     1 | loss: 9.4530468Losses:  15.646923065185547 7.730340003967285 3.362672805786133
CurrentTrain: epoch  2, batch     0 | loss: 15.6469231Losses:  9.270222663879395 2.1503899097442627 3.354334831237793
CurrentTrain: epoch  2, batch     1 | loss: 9.2702227Losses:  15.316548347473145 8.127044677734375 3.3317368030548096
CurrentTrain: epoch  3, batch     0 | loss: 15.3165483Losses:  9.621153831481934 3.2970564365386963 3.3926355838775635
CurrentTrain: epoch  3, batch     1 | loss: 9.6211538Losses:  13.326593399047852 7.08829927444458 3.3670217990875244
CurrentTrain: epoch  4, batch     0 | loss: 13.3265934Losses:  9.21860408782959 2.7120094299316406 3.317352056503296
CurrentTrain: epoch  4, batch     1 | loss: 9.2186041Losses:  12.608857154846191 6.446237087249756 3.3127715587615967
CurrentTrain: epoch  5, batch     0 | loss: 12.6088572Losses:  8.545344352722168 2.2378249168395996 3.3491363525390625
CurrentTrain: epoch  5, batch     1 | loss: 8.5453444Losses:  12.333829879760742 6.481740951538086 3.3121705055236816
CurrentTrain: epoch  6, batch     0 | loss: 12.3338299Losses:  7.842127799987793 2.0359039306640625 3.3100202083587646
CurrentTrain: epoch  6, batch     1 | loss: 7.8421278Losses:  12.636557579040527 6.887938499450684 3.318343162536621
CurrentTrain: epoch  7, batch     0 | loss: 12.6365576Losses:  8.334125518798828 2.539489507675171 3.3201847076416016
CurrentTrain: epoch  7, batch     1 | loss: 8.3341255Losses:  11.575538635253906 5.776524543762207 3.3243701457977295
CurrentTrain: epoch  8, batch     0 | loss: 11.5755386Losses:  6.711268424987793 1.2158550024032593 3.322477340698242
CurrentTrain: epoch  8, batch     1 | loss: 6.7112684Losses:  12.313826560974121 6.764173984527588 3.314509630203247
CurrentTrain: epoch  9, batch     0 | loss: 12.3138266Losses:  8.397403717041016 3.0092740058898926 3.3246960639953613
CurrentTrain: epoch  9, batch     1 | loss: 8.3974037
Losses:  19.41134262084961 -0.0 17.21062660217285
MemoryTrain:  epoch  0, batch     0 | loss: 19.4113426Losses:  2.7265288829803467 -0.0 1.4190691709518433
MemoryTrain:  epoch  0, batch     1 | loss: 2.7265289Losses:  16.12131690979004 -0.0 13.94830322265625
MemoryTrain:  epoch  1, batch     0 | loss: 16.1213169Losses:  0.9581000208854675 -0.0 -0.0
MemoryTrain:  epoch  1, batch     1 | loss: 0.9581000Losses:  18.62253189086914 -0.0 16.9453067779541
MemoryTrain:  epoch  2, batch     0 | loss: 18.6225319Losses:  1.6911519765853882 -0.0 1.4340639114379883
MemoryTrain:  epoch  2, batch     1 | loss: 1.6911520Losses:  15.272830963134766 -0.0 13.83476734161377
MemoryTrain:  epoch  3, batch     0 | loss: 15.2728310Losses:  4.14840841293335 -0.0 3.308506965637207
MemoryTrain:  epoch  3, batch     1 | loss: 4.1484084Losses:  9.436540603637695 -0.0 8.151521682739258
MemoryTrain:  epoch  4, batch     0 | loss: 9.4365406Losses:  6.457242012023926 -0.0 5.62495756149292
MemoryTrain:  epoch  4, batch     1 | loss: 6.4572420Losses:  11.666970252990723 -0.0 10.861542701721191
MemoryTrain:  epoch  5, batch     0 | loss: 11.6669703Losses:  3.588125705718994 -0.0 3.307274341583252
MemoryTrain:  epoch  5, batch     1 | loss: 3.5881257Losses:  11.310988426208496 -0.0 10.878132820129395
MemoryTrain:  epoch  6, batch     0 | loss: 11.3109884Losses:  3.457258462905884 -0.0 3.3963327407836914
MemoryTrain:  epoch  6, batch     1 | loss: 3.4572585Losses:  16.98709487915039 -0.0 16.803686141967773
MemoryTrain:  epoch  7, batch     0 | loss: 16.9870949Losses:  3.5350582599639893 -0.0 3.3469393253326416
MemoryTrain:  epoch  7, batch     1 | loss: 3.5350583Losses:  16.927852630615234 -0.0 16.763437271118164
MemoryTrain:  epoch  8, batch     0 | loss: 16.9278526Losses:  2.482017993927002 -0.0 1.4072597026824951
MemoryTrain:  epoch  8, batch     1 | loss: 2.4820180Losses:  14.277528762817383 -0.0 13.733501434326172
MemoryTrain:  epoch  9, batch     0 | loss: 14.2775288Losses:  3.5774106979370117 -0.0 3.3376283645629883
MemoryTrain:  epoch  9, batch     1 | loss: 3.5774107
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 68.75%,  total acc: 75.00%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 76.56%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 80.00%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 79.17%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 79.46%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 79.69%   [EVAL] batch:    8 | acc: 37.50%,  total acc: 75.00%   [EVAL] batch:    9 | acc: 43.75%,  total acc: 71.88%   [EVAL] batch:   10 | acc: 50.00%,  total acc: 69.89%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 72.40%   [EVAL] batch:   12 | acc: 100.00%,  total acc: 74.52%   [EVAL] batch:   13 | acc: 100.00%,  total acc: 76.34%   [EVAL] batch:   14 | acc: 100.00%,  total acc: 77.92%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 79.30%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 80.51%   [EVAL] batch:   17 | acc: 25.00%,  total acc: 77.43%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 62.50%,  total acc: 65.62%   [EVAL] batch:    2 | acc: 68.75%,  total acc: 66.67%   [EVAL] batch:    3 | acc: 50.00%,  total acc: 62.50%   [EVAL] batch:    4 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    5 | acc: 68.75%,  total acc: 63.54%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 66.07%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 70.31%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 73.61%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 75.00%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 77.27%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 78.12%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 78.37%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 77.68%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 77.50%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 76.17%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 76.10%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 75.35%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 74.67%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 75.31%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 76.49%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 77.56%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 78.26%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 79.17%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 80.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 80.77%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 81.92%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 82.54%   [EVAL] batch:   29 | acc: 93.75%,  total acc: 82.92%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 83.06%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 83.59%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 82.20%   [EVAL] batch:   33 | acc: 0.00%,  total acc: 79.78%   [EVAL] batch:   34 | acc: 0.00%,  total acc: 77.50%   [EVAL] batch:   35 | acc: 0.00%,  total acc: 75.35%   [EVAL] batch:   36 | acc: 0.00%,  total acc: 73.31%   [EVAL] batch:   37 | acc: 50.00%,  total acc: 72.70%   [EVAL] batch:   38 | acc: 81.25%,  total acc: 72.92%   [EVAL] batch:   39 | acc: 93.75%,  total acc: 73.44%   [EVAL] batch:   40 | acc: 81.25%,  total acc: 73.63%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 73.96%   [EVAL] batch:   42 | acc: 12.50%,  total acc: 72.53%   [EVAL] batch:   43 | acc: 6.25%,  total acc: 71.02%   [EVAL] batch:   44 | acc: 6.25%,  total acc: 69.58%   [EVAL] batch:   45 | acc: 6.25%,  total acc: 68.21%   [EVAL] batch:   46 | acc: 18.75%,  total acc: 67.15%   [EVAL] batch:   47 | acc: 87.50%,  total acc: 67.58%   [EVAL] batch:   48 | acc: 43.75%,  total acc: 67.09%   [EVAL] batch:   49 | acc: 43.75%,  total acc: 66.62%   [EVAL] batch:   50 | acc: 62.50%,  total acc: 66.54%   [EVAL] batch:   51 | acc: 56.25%,  total acc: 66.35%   [EVAL] batch:   52 | acc: 56.25%,  total acc: 66.16%   [EVAL] batch:   53 | acc: 75.00%,  total acc: 66.32%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 66.70%   [EVAL] batch:   55 | acc: 75.00%,  total acc: 66.85%   [EVAL] batch:   56 | acc: 81.25%,  total acc: 67.11%   [EVAL] batch:   57 | acc: 56.25%,  total acc: 66.92%   [EVAL] batch:   58 | acc: 68.75%,  total acc: 66.95%   [EVAL] batch:   59 | acc: 62.50%,  total acc: 66.88%   [EVAL] batch:   60 | acc: 50.00%,  total acc: 66.60%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 66.63%   [EVAL] batch:   62 | acc: 81.25%,  total acc: 66.87%   [EVAL] batch:   63 | acc: 75.00%,  total acc: 66.99%   [EVAL] batch:   64 | acc: 87.50%,  total acc: 67.31%   [EVAL] batch:   65 | acc: 81.25%,  total acc: 67.52%   [EVAL] batch:   66 | acc: 68.75%,  total acc: 67.54%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 68.01%   [EVAL] batch:   68 | acc: 50.00%,  total acc: 67.75%   [EVAL] batch:   69 | acc: 43.75%,  total acc: 67.41%   [EVAL] batch:   70 | acc: 43.75%,  total acc: 67.08%   [EVAL] batch:   71 | acc: 75.00%,  total acc: 67.19%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 67.64%   [EVAL] batch:   73 | acc: 100.00%,  total acc: 68.07%   [EVAL] batch:   74 | acc: 100.00%,  total acc: 68.50%   [EVAL] batch:   75 | acc: 100.00%,  total acc: 68.91%   [EVAL] batch:   76 | acc: 100.00%,  total acc: 69.32%   [EVAL] batch:   77 | acc: 75.00%,  total acc: 69.39%   
cur_acc:  ['0.8561', '0.5208', '0.7589', '0.7743']
his_acc:  ['0.8561', '0.7739', '0.7039', '0.6939']
Clustering into  12  clusters
Clusters:  [ 1  4  0  7 11  8  1  3 10  6  2  3  1  2  4  1  1  9  1  2  5  0  0  6
  8  2]
Losses:  21.806127548217773 8.600399017333984 3.8060052394866943
CurrentTrain: epoch  0, batch     0 | loss: 21.8061275Losses:  17.52869415283203 3.343228816986084 3.7980856895446777
CurrentTrain: epoch  0, batch     1 | loss: 17.5286942Losses:  22.15172004699707 10.399835586547852 3.726595401763916
CurrentTrain: epoch  1, batch     0 | loss: 22.1517200Losses:  19.430910110473633 7.2513909339904785 1.6938273906707764
CurrentTrain: epoch  1, batch     1 | loss: 19.4309101Losses:  21.715179443359375 9.066630363464355 3.7941970825195312
CurrentTrain: epoch  2, batch     0 | loss: 21.7151794Losses:  14.293010711669922 3.1233971118927 3.7355971336364746
CurrentTrain: epoch  2, batch     1 | loss: 14.2930107Losses:  19.69103240966797 7.910247325897217 3.756288766860962
CurrentTrain: epoch  3, batch     0 | loss: 19.6910324Losses:  14.469191551208496 2.7847087383270264 3.675405263900757
CurrentTrain: epoch  3, batch     1 | loss: 14.4691916Losses:  19.822263717651367 7.672560691833496 3.8064215183258057
CurrentTrain: epoch  4, batch     0 | loss: 19.8222637Losses:  13.089820861816406 2.0974104404449463 3.7068753242492676
CurrentTrain: epoch  4, batch     1 | loss: 13.0898209Losses:  18.733623504638672 7.669516086578369 3.7407150268554688
CurrentTrain: epoch  5, batch     0 | loss: 18.7336235Losses:  13.72232723236084 2.0802299976348877 3.6977949142456055
CurrentTrain: epoch  5, batch     1 | loss: 13.7223272Losses:  18.09406089782715 7.410111427307129 3.695416212081909
CurrentTrain: epoch  6, batch     0 | loss: 18.0940609Losses:  13.3931884765625 2.3833909034729004 3.6730997562408447
CurrentTrain: epoch  6, batch     1 | loss: 13.3931885Losses:  19.584087371826172 8.735347747802734 3.6930603981018066
CurrentTrain: epoch  7, batch     0 | loss: 19.5840874Losses:  12.106136322021484 2.816927671432495 3.636009693145752
CurrentTrain: epoch  7, batch     1 | loss: 12.1061363Losses:  17.0140380859375 7.556565761566162 3.6439309120178223
CurrentTrain: epoch  8, batch     0 | loss: 17.0140381Losses:  13.769712448120117 3.0383102893829346 3.6021616458892822
CurrentTrain: epoch  8, batch     1 | loss: 13.7697124Losses:  17.60682487487793 7.801633834838867 3.634340763092041
CurrentTrain: epoch  9, batch     0 | loss: 17.6068249Losses:  11.979724884033203 2.6947596073150635 3.514925479888916
CurrentTrain: epoch  9, batch     1 | loss: 11.9797249
Losses:  21.32141876220703 -0.0 20.21534538269043
MemoryTrain:  epoch  0, batch     0 | loss: 21.3214188Losses:  12.108244895935059 -0.0 11.07732105255127
MemoryTrain:  epoch  0, batch     1 | loss: 12.1082449Losses:  25.056838989257812 -0.0 23.30205535888672
MemoryTrain:  epoch  1, batch     0 | loss: 25.0568390Losses:  9.01063060760498 -0.0 8.269233703613281
MemoryTrain:  epoch  1, batch     1 | loss: 9.0106306Losses:  17.718822479248047 -0.0 16.942258834838867
MemoryTrain:  epoch  2, batch     0 | loss: 17.7188225Losses:  15.335731506347656 -0.0 13.756677627563477
MemoryTrain:  epoch  2, batch     1 | loss: 15.3357315Losses:  20.61279296875 -0.0 19.97185516357422
MemoryTrain:  epoch  3, batch     0 | loss: 20.6127930Losses:  11.249654769897461 -0.0 10.876497268676758
MemoryTrain:  epoch  3, batch     1 | loss: 11.2496548Losses:  20.4129638671875 -0.0 20.003530502319336
MemoryTrain:  epoch  4, batch     0 | loss: 20.4129639Losses:  11.443063735961914 -0.0 10.894370079040527
MemoryTrain:  epoch  4, batch     1 | loss: 11.4430637Losses:  14.119622230529785 -0.0 13.713380813598633
MemoryTrain:  epoch  5, batch     0 | loss: 14.1196222Losses:  13.836797714233398 -0.0 13.748746871948242
MemoryTrain:  epoch  5, batch     1 | loss: 13.8367977Losses:  16.937475204467773 -0.0 16.718265533447266
MemoryTrain:  epoch  6, batch     0 | loss: 16.9374752Losses:  8.269669532775879 -0.0 8.177579879760742
MemoryTrain:  epoch  6, batch     1 | loss: 8.2696695Losses:  20.168115615844727 -0.0 20.0245304107666
MemoryTrain:  epoch  7, batch     0 | loss: 20.1681156Losses:  14.489675521850586 -0.0 13.742670059204102
MemoryTrain:  epoch  7, batch     1 | loss: 14.4896755Losses:  23.249034881591797 -0.0 23.1486759185791
MemoryTrain:  epoch  8, batch     0 | loss: 23.2490349Losses:  11.04157543182373 -0.0 10.931965827941895
MemoryTrain:  epoch  8, batch     1 | loss: 11.0415754Losses:  23.212881088256836 -0.0 23.144311904907227
MemoryTrain:  epoch  9, batch     0 | loss: 23.2128811Losses:  8.197616577148438 -0.0 8.115898132324219
MemoryTrain:  epoch  9, batch     1 | loss: 8.1976166
[EVAL] batch:    0 | acc: 43.75%,  total acc: 43.75%   [EVAL] batch:    1 | acc: 37.50%,  total acc: 40.62%   [EVAL] batch:    2 | acc: 25.00%,  total acc: 35.42%   [EVAL] batch:    3 | acc: 6.25%,  total acc: 28.12%   [EVAL] batch:    4 | acc: 12.50%,  total acc: 25.00%   [EVAL] batch:    5 | acc: 0.00%,  total acc: 20.83%   [EVAL] batch:    6 | acc: 18.75%,  total acc: 20.54%   [EVAL] batch:    7 | acc: 62.50%,  total acc: 25.78%   [EVAL] batch:    8 | acc: 62.50%,  total acc: 29.86%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 33.75%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 38.07%   [EVAL] batch:   11 | acc: 68.75%,  total acc: 40.62%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 43.27%   [EVAL] batch:   13 | acc: 75.00%,  total acc: 45.54%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 47.50%   [EVAL] batch:   15 | acc: 93.75%,  total acc: 50.39%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 51.84%   [EVAL] batch:   17 | acc: 81.25%,  total acc: 53.47%   [EVAL] batch:   18 | acc: 50.00%,  total acc: 53.29%   [EVAL] batch:   19 | acc: 62.50%,  total acc: 53.75%   [EVAL] batch:   20 | acc: 43.75%,  total acc: 53.27%   [EVAL] batch:   21 | acc: 31.25%,  total acc: 52.27%   
[EVAL] batch:    0 | acc: 56.25%,  total acc: 56.25%   [EVAL] batch:    1 | acc: 62.50%,  total acc: 59.38%   [EVAL] batch:    2 | acc: 68.75%,  total acc: 62.50%   [EVAL] batch:    3 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    4 | acc: 50.00%,  total acc: 60.00%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 62.50%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 65.18%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 69.53%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 72.92%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 74.38%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 76.70%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 77.60%   [EVAL] batch:   12 | acc: 56.25%,  total acc: 75.96%   [EVAL] batch:   13 | acc: 25.00%,  total acc: 72.32%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 72.50%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 71.48%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 71.69%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 71.18%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 70.72%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 71.56%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 72.92%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 74.15%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 75.00%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 75.78%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 76.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 77.64%   [EVAL] batch:   26 | acc: 87.50%,  total acc: 78.01%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 78.79%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 79.31%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 79.38%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 79.84%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 80.47%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 79.17%   [EVAL] batch:   33 | acc: 6.25%,  total acc: 77.02%   [EVAL] batch:   34 | acc: 0.00%,  total acc: 74.82%   [EVAL] batch:   35 | acc: 0.00%,  total acc: 72.74%   [EVAL] batch:   36 | acc: 0.00%,  total acc: 70.78%   [EVAL] batch:   37 | acc: 50.00%,  total acc: 70.23%   [EVAL] batch:   38 | acc: 68.75%,  total acc: 70.19%   [EVAL] batch:   39 | acc: 93.75%,  total acc: 70.78%   [EVAL] batch:   40 | acc: 68.75%,  total acc: 70.73%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 71.28%   [EVAL] batch:   42 | acc: 0.00%,  total acc: 69.62%   [EVAL] batch:   43 | acc: 6.25%,  total acc: 68.18%   [EVAL] batch:   44 | acc: 0.00%,  total acc: 66.67%   [EVAL] batch:   45 | acc: 0.00%,  total acc: 65.22%   [EVAL] batch:   46 | acc: 12.50%,  total acc: 64.10%   [EVAL] batch:   47 | acc: 87.50%,  total acc: 64.58%   [EVAL] batch:   48 | acc: 12.50%,  total acc: 63.52%   [EVAL] batch:   49 | acc: 6.25%,  total acc: 62.38%   [EVAL] batch:   50 | acc: 0.00%,  total acc: 61.15%   [EVAL] batch:   51 | acc: 25.00%,  total acc: 60.46%   [EVAL] batch:   52 | acc: 6.25%,  total acc: 59.43%   [EVAL] batch:   53 | acc: 43.75%,  total acc: 59.14%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 59.77%   [EVAL] batch:   55 | acc: 62.50%,  total acc: 59.82%   [EVAL] batch:   56 | acc: 75.00%,  total acc: 60.09%   [EVAL] batch:   57 | acc: 75.00%,  total acc: 60.34%   [EVAL] batch:   58 | acc: 50.00%,  total acc: 60.17%   [EVAL] batch:   59 | acc: 68.75%,  total acc: 60.31%   [EVAL] batch:   60 | acc: 50.00%,  total acc: 60.14%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 60.58%   [EVAL] batch:   62 | acc: 81.25%,  total acc: 60.91%   [EVAL] batch:   63 | acc: 68.75%,  total acc: 61.04%   [EVAL] batch:   64 | acc: 81.25%,  total acc: 61.35%   [EVAL] batch:   65 | acc: 81.25%,  total acc: 61.65%   [EVAL] batch:   66 | acc: 81.25%,  total acc: 61.94%   [EVAL] batch:   67 | acc: 93.75%,  total acc: 62.41%   [EVAL] batch:   68 | acc: 50.00%,  total acc: 62.23%   [EVAL] batch:   69 | acc: 43.75%,  total acc: 61.96%   [EVAL] batch:   70 | acc: 37.50%,  total acc: 61.62%   [EVAL] batch:   71 | acc: 68.75%,  total acc: 61.72%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 62.24%   [EVAL] batch:   73 | acc: 100.00%,  total acc: 62.75%   [EVAL] batch:   74 | acc: 100.00%,  total acc: 63.25%   [EVAL] batch:   75 | acc: 100.00%,  total acc: 63.73%   [EVAL] batch:   76 | acc: 100.00%,  total acc: 64.20%   [EVAL] batch:   77 | acc: 81.25%,  total acc: 64.42%   [EVAL] batch:   78 | acc: 50.00%,  total acc: 64.24%   [EVAL] batch:   79 | acc: 31.25%,  total acc: 63.83%   [EVAL] batch:   80 | acc: 18.75%,  total acc: 63.27%   [EVAL] batch:   81 | acc: 6.25%,  total acc: 62.58%   [EVAL] batch:   82 | acc: 12.50%,  total acc: 61.97%   [EVAL] batch:   83 | acc: 0.00%,  total acc: 61.24%   [EVAL] batch:   84 | acc: 31.25%,  total acc: 60.88%   [EVAL] batch:   85 | acc: 62.50%,  total acc: 60.90%   [EVAL] batch:   86 | acc: 68.75%,  total acc: 60.99%   [EVAL] batch:   87 | acc: 75.00%,  total acc: 61.15%   [EVAL] batch:   88 | acc: 62.50%,  total acc: 61.17%   [EVAL] batch:   89 | acc: 75.00%,  total acc: 61.32%   [EVAL] batch:   90 | acc: 87.50%,  total acc: 61.61%   [EVAL] batch:   91 | acc: 75.00%,  total acc: 61.75%   [EVAL] batch:   92 | acc: 75.00%,  total acc: 61.90%   [EVAL] batch:   93 | acc: 93.75%,  total acc: 62.23%   [EVAL] batch:   94 | acc: 68.75%,  total acc: 62.30%   [EVAL] batch:   95 | acc: 75.00%,  total acc: 62.43%   [EVAL] batch:   96 | acc: 62.50%,  total acc: 62.44%   [EVAL] batch:   97 | acc: 43.75%,  total acc: 62.24%   [EVAL] batch:   98 | acc: 56.25%,  total acc: 62.18%   [EVAL] batch:   99 | acc: 12.50%,  total acc: 61.69%   
cur_acc:  ['0.8561', '0.5208', '0.7589', '0.7743', '0.5227']
his_acc:  ['0.8561', '0.7739', '0.7039', '0.6939', '0.6169']
Clustering into  14  clusters
Clusters:  [ 0  4  1  0 13 10  0  2  9  8  3  2  0  3  4  0  0 11  0  3  6  1  1  8
 10  3  0 12  5  7  4]
Losses:  20.543357849121094 7.905981063842773 5.867711067199707
CurrentTrain: epoch  0, batch     0 | loss: 20.5433578Losses:  13.909584045410156 2.279125690460205 5.637836456298828
CurrentTrain: epoch  0, batch     1 | loss: 13.9095840Losses:  22.480690002441406 10.159284591674805 5.831141471862793
CurrentTrain: epoch  1, batch     0 | loss: 22.4806900Losses:  10.727746963500977 5.279186725616455 1.4368362426757812
CurrentTrain: epoch  1, batch     1 | loss: 10.7277470Losses:  17.200315475463867 6.586251258850098 5.71248197555542
CurrentTrain: epoch  2, batch     0 | loss: 17.2003155Losses:  12.780876159667969 1.9397350549697876 5.745562553405762
CurrentTrain: epoch  2, batch     1 | loss: 12.7808762Losses:  17.23050880432129 6.96803092956543 5.6985673904418945
CurrentTrain: epoch  3, batch     0 | loss: 17.2305088Losses:  11.042110443115234 1.5423065423965454 5.66570520401001
CurrentTrain: epoch  3, batch     1 | loss: 11.0421104Losses:  16.217315673828125 6.759727478027344 5.667742729187012
CurrentTrain: epoch  4, batch     0 | loss: 16.2173157Losses:  11.965095520019531 2.323577404022217 5.671801567077637
CurrentTrain: epoch  4, batch     1 | loss: 11.9650955Losses:  17.795562744140625 8.227302551269531 5.622778415679932
CurrentTrain: epoch  5, batch     0 | loss: 17.7955627Losses:  11.823822021484375 3.2507104873657227 5.6222147941589355
CurrentTrain: epoch  5, batch     1 | loss: 11.8238220Losses:  15.970573425292969 6.626132488250732 5.644484043121338
CurrentTrain: epoch  6, batch     0 | loss: 15.9705734Losses:  9.920882225036621 1.32968008518219 5.588406562805176
CurrentTrain: epoch  6, batch     1 | loss: 9.9208822Losses:  17.17045021057129 7.835549831390381 5.65002965927124
CurrentTrain: epoch  7, batch     0 | loss: 17.1704502Losses:  8.56829833984375 2.770659923553467 3.33778715133667
CurrentTrain: epoch  7, batch     1 | loss: 8.5682983Losses:  16.888290405273438 7.800051212310791 5.620471000671387
CurrentTrain: epoch  8, batch     0 | loss: 16.8882904Losses:  11.293339729309082 3.0651724338531494 5.613801956176758
CurrentTrain: epoch  8, batch     1 | loss: 11.2933397Losses:  17.340381622314453 8.60546588897705 5.627933502197266
CurrentTrain: epoch  9, batch     0 | loss: 17.3403816Losses:  9.177775382995605 4.563106536865234 1.4392808675765991
CurrentTrain: epoch  9, batch     1 | loss: 9.1777754
Losses:  14.099355697631836 -0.0 13.743840217590332
MemoryTrain:  epoch  0, batch     0 | loss: 14.0993557Losses:  21.32918930053711 -0.0 20.073251724243164
MemoryTrain:  epoch  0, batch     1 | loss: 21.3291893Losses:  14.293620109558105 -0.0 13.785493850708008
MemoryTrain:  epoch  1, batch     0 | loss: 14.2936201Losses:  20.743404388427734 -0.0 19.864004135131836
MemoryTrain:  epoch  1, batch     1 | loss: 20.7434044Losses:  17.151315689086914 -0.0 16.857622146606445
MemoryTrain:  epoch  2, batch     0 | loss: 17.1513157Losses:  19.940683364868164 -0.0 19.8422794342041
MemoryTrain:  epoch  2, batch     1 | loss: 19.9406834Losses:  23.276416778564453 -0.0 23.127403259277344
MemoryTrain:  epoch  3, batch     0 | loss: 23.2764168Losses:  20.142620086669922 -0.0 19.962200164794922
MemoryTrain:  epoch  3, batch     1 | loss: 20.1426201Losses:  20.51209831237793 -0.0 19.930191040039062
MemoryTrain:  epoch  4, batch     0 | loss: 20.5120983Losses:  16.870529174804688 -0.0 16.780319213867188
MemoryTrain:  epoch  4, batch     1 | loss: 16.8705292Losses:  23.521259307861328 -0.0 23.114532470703125
MemoryTrain:  epoch  5, batch     0 | loss: 23.5212593Losses:  13.925830841064453 -0.0 13.781839370727539
MemoryTrain:  epoch  5, batch     1 | loss: 13.9258308Losses:  23.254117965698242 -0.0 23.160888671875
MemoryTrain:  epoch  6, batch     0 | loss: 23.2541180Losses:  16.81553077697754 -0.0 16.756616592407227
MemoryTrain:  epoch  6, batch     1 | loss: 16.8155308Losses:  23.1950740814209 -0.0 23.15270233154297
MemoryTrain:  epoch  7, batch     0 | loss: 23.1950741Losses:  16.77111053466797 -0.0 16.703393936157227
MemoryTrain:  epoch  7, batch     1 | loss: 16.7711105Losses:  13.766129493713379 -0.0 13.707642555236816
MemoryTrain:  epoch  8, batch     0 | loss: 13.7661295Losses:  16.786148071289062 -0.0 16.745555877685547
MemoryTrain:  epoch  8, batch     1 | loss: 16.7861481Losses:  19.883342742919922 -0.0 19.841039657592773
MemoryTrain:  epoch  9, batch     0 | loss: 19.8833427Losses:  19.902172088623047 -0.0 19.85848045349121
MemoryTrain:  epoch  9, batch     1 | loss: 19.9021721
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 89.58%   [EVAL] batch:    3 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:    4 | acc: 100.00%,  total acc: 92.50%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 92.71%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:    7 | acc: 12.50%,  total acc: 83.59%   
[EVAL] batch:    0 | acc: 6.25%,  total acc: 6.25%   [EVAL] batch:    1 | acc: 12.50%,  total acc: 9.38%   [EVAL] batch:    2 | acc: 25.00%,  total acc: 14.58%   [EVAL] batch:    3 | acc: 6.25%,  total acc: 12.50%   [EVAL] batch:    4 | acc: 25.00%,  total acc: 15.00%   [EVAL] batch:    5 | acc: 6.25%,  total acc: 13.54%   [EVAL] batch:    6 | acc: 6.25%,  total acc: 12.50%   [EVAL] batch:    7 | acc: 0.00%,  total acc: 10.94%   [EVAL] batch:    8 | acc: 0.00%,  total acc: 9.72%   [EVAL] batch:    9 | acc: 0.00%,  total acc: 8.75%   [EVAL] batch:   10 | acc: 0.00%,  total acc: 7.95%   [EVAL] batch:   11 | acc: 0.00%,  total acc: 7.29%   [EVAL] batch:   12 | acc: 6.25%,  total acc: 7.21%   [EVAL] batch:   13 | acc: 31.25%,  total acc: 8.93%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 13.33%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 16.02%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 19.49%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 21.88%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 24.01%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 27.19%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 30.65%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 33.81%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 36.41%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 39.06%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 41.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 43.75%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 45.60%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 47.54%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 49.14%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 50.21%   [EVAL] batch:   30 | acc: 75.00%,  total acc: 51.01%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 52.54%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 52.08%   [EVAL] batch:   33 | acc: 6.25%,  total acc: 50.74%   [EVAL] batch:   34 | acc: 6.25%,  total acc: 49.46%   [EVAL] batch:   35 | acc: 0.00%,  total acc: 48.09%   [EVAL] batch:   36 | acc: 0.00%,  total acc: 46.79%   [EVAL] batch:   37 | acc: 50.00%,  total acc: 46.88%   [EVAL] batch:   38 | acc: 75.00%,  total acc: 47.60%   [EVAL] batch:   39 | acc: 93.75%,  total acc: 48.75%   [EVAL] batch:   40 | acc: 81.25%,  total acc: 49.54%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 50.74%   [EVAL] batch:   42 | acc: 0.00%,  total acc: 49.56%   [EVAL] batch:   43 | acc: 6.25%,  total acc: 48.58%   [EVAL] batch:   44 | acc: 6.25%,  total acc: 47.64%   [EVAL] batch:   45 | acc: 0.00%,  total acc: 46.60%   [EVAL] batch:   46 | acc: 12.50%,  total acc: 45.88%   [EVAL] batch:   47 | acc: 68.75%,  total acc: 46.35%   [EVAL] batch:   48 | acc: 25.00%,  total acc: 45.92%   [EVAL] batch:   49 | acc: 12.50%,  total acc: 45.25%   [EVAL] batch:   50 | acc: 6.25%,  total acc: 44.49%   [EVAL] batch:   51 | acc: 25.00%,  total acc: 44.11%   [EVAL] batch:   52 | acc: 0.00%,  total acc: 43.28%   [EVAL] batch:   53 | acc: 43.75%,  total acc: 43.29%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 43.98%   [EVAL] batch:   55 | acc: 50.00%,  total acc: 44.08%   [EVAL] batch:   56 | acc: 81.25%,  total acc: 44.74%   [EVAL] batch:   57 | acc: 68.75%,  total acc: 45.15%   [EVAL] batch:   58 | acc: 62.50%,  total acc: 45.44%   [EVAL] batch:   59 | acc: 62.50%,  total acc: 45.73%   [EVAL] batch:   60 | acc: 25.00%,  total acc: 45.39%   [EVAL] batch:   61 | acc: 56.25%,  total acc: 45.56%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 45.63%   [EVAL] batch:   63 | acc: 50.00%,  total acc: 45.70%   [EVAL] batch:   64 | acc: 50.00%,  total acc: 45.77%   [EVAL] batch:   65 | acc: 68.75%,  total acc: 46.12%   [EVAL] batch:   66 | acc: 68.75%,  total acc: 46.46%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 47.24%   [EVAL] batch:   68 | acc: 50.00%,  total acc: 47.28%   [EVAL] batch:   69 | acc: 25.00%,  total acc: 46.96%   [EVAL] batch:   70 | acc: 37.50%,  total acc: 46.83%   [EVAL] batch:   71 | acc: 68.75%,  total acc: 47.14%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 47.86%   [EVAL] batch:   73 | acc: 100.00%,  total acc: 48.56%   [EVAL] batch:   74 | acc: 100.00%,  total acc: 49.25%   [EVAL] batch:   75 | acc: 100.00%,  total acc: 49.92%   [EVAL] batch:   76 | acc: 100.00%,  total acc: 50.57%   [EVAL] batch:   77 | acc: 81.25%,  total acc: 50.96%   [EVAL] batch:   78 | acc: 37.50%,  total acc: 50.79%   [EVAL] batch:   79 | acc: 43.75%,  total acc: 50.70%   [EVAL] batch:   80 | acc: 18.75%,  total acc: 50.31%   [EVAL] batch:   81 | acc: 12.50%,  total acc: 49.85%   [EVAL] batch:   82 | acc: 0.00%,  total acc: 49.25%   [EVAL] batch:   83 | acc: 18.75%,  total acc: 48.88%   [EVAL] batch:   84 | acc: 25.00%,  total acc: 48.60%   [EVAL] batch:   85 | acc: 50.00%,  total acc: 48.62%   [EVAL] batch:   86 | acc: 56.25%,  total acc: 48.71%   [EVAL] batch:   87 | acc: 43.75%,  total acc: 48.65%   [EVAL] batch:   88 | acc: 56.25%,  total acc: 48.74%   [EVAL] batch:   89 | acc: 68.75%,  total acc: 48.96%   [EVAL] batch:   90 | acc: 68.75%,  total acc: 49.18%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 49.73%   [EVAL] batch:   92 | acc: 68.75%,  total acc: 49.93%   [EVAL] batch:   93 | acc: 93.75%,  total acc: 50.40%   [EVAL] batch:   94 | acc: 75.00%,  total acc: 50.66%   [EVAL] batch:   95 | acc: 81.25%,  total acc: 50.98%   [EVAL] batch:   96 | acc: 56.25%,  total acc: 51.03%   [EVAL] batch:   97 | acc: 37.50%,  total acc: 50.89%   [EVAL] batch:   98 | acc: 56.25%,  total acc: 50.95%   [EVAL] batch:   99 | acc: 87.50%,  total acc: 51.31%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 51.79%   [EVAL] batch:  101 | acc: 81.25%,  total acc: 52.08%   [EVAL] batch:  102 | acc: 93.75%,  total acc: 52.49%   [EVAL] batch:  103 | acc: 100.00%,  total acc: 52.94%   [EVAL] batch:  104 | acc: 93.75%,  total acc: 53.33%   [EVAL] batch:  105 | acc: 100.00%,  total acc: 53.77%   [EVAL] batch:  106 | acc: 31.25%,  total acc: 53.56%   
cur_acc:  ['0.8561', '0.5208', '0.7589', '0.7743', '0.5227', '0.8359']
his_acc:  ['0.8561', '0.7739', '0.7039', '0.6939', '0.6169', '0.5356']
Clustering into  17  clusters
Clusters:  [ 7  2  0  3 16  4  7  1  9  8  5  1  3  5  2  6  7 12  6  5 11  0  0  8
  4  5  3 15 10 13  2  1  0  4 14  6]
Losses:  18.252933502197266 8.22752571105957 5.642826557159424
CurrentTrain: epoch  0, batch     0 | loss: 18.2529335Losses:  12.008758544921875 3.511232614517212 3.3366916179656982
CurrentTrain: epoch  0, batch     1 | loss: 12.0087585Losses:  15.884380340576172 6.49417781829834 5.668667793273926
CurrentTrain: epoch  1, batch     0 | loss: 15.8843803Losses:  10.591338157653809 1.5711758136749268 5.601743221282959
CurrentTrain: epoch  1, batch     1 | loss: 10.5913382Losses:  15.859132766723633 7.01997184753418 5.641425609588623
CurrentTrain: epoch  2, batch     0 | loss: 15.8591328Losses:  10.350658416748047 2.0451653003692627 5.60988187789917
CurrentTrain: epoch  2, batch     1 | loss: 10.3506584Losses:  14.715055465698242 6.493210315704346 5.653736591339111
CurrentTrain: epoch  3, batch     0 | loss: 14.7150555Losses:  10.205141067504883 2.0522348880767822 5.607471942901611
CurrentTrain: epoch  3, batch     1 | loss: 10.2051411Losses:  14.340622901916504 6.394323348999023 5.602485656738281
CurrentTrain: epoch  4, batch     0 | loss: 14.3406229Losses:  10.066789627075195 2.0730903148651123 5.5856146812438965
CurrentTrain: epoch  4, batch     1 | loss: 10.0667896Losses:  15.609245300292969 7.779836654663086 5.6042799949646
CurrentTrain: epoch  5, batch     0 | loss: 15.6092453Losses:  10.098238945007324 4.533942222595215 3.33919620513916
CurrentTrain: epoch  5, batch     1 | loss: 10.0982389Losses:  13.671977996826172 5.948759078979492 5.5835700035095215
CurrentTrain: epoch  6, batch     0 | loss: 13.6719780Losses:  9.410675048828125 1.8875573873519897 5.57320499420166
CurrentTrain: epoch  6, batch     1 | loss: 9.4106750Losses:  13.640766143798828 6.020252227783203 5.5846052169799805
CurrentTrain: epoch  7, batch     0 | loss: 13.6407661Losses:  9.254515647888184 1.7431074380874634 5.596296787261963
CurrentTrain: epoch  7, batch     1 | loss: 9.2545156Losses:  13.97970199584961 6.461414813995361 5.57908821105957
CurrentTrain: epoch  8, batch     0 | loss: 13.9797020Losses:  7.476021766662598 2.031953811645508 3.3149735927581787
CurrentTrain: epoch  8, batch     1 | loss: 7.4760218Losses:  16.637462615966797 8.053243637084961 5.617265224456787
CurrentTrain: epoch  9, batch     0 | loss: 16.6374626Losses:  10.585865020751953 4.05214262008667 3.3129959106445312
CurrentTrain: epoch  9, batch     1 | loss: 10.5858650
Losses:  21.308313369750977 -0.0 20.062744140625
MemoryTrain:  epoch  0, batch     0 | loss: 21.3083134Losses:  21.017236709594727 -0.0 20.11677360534668
MemoryTrain:  epoch  0, batch     1 | loss: 21.0172367Losses:  4.9048848152160645 -0.0 3.301161527633667
MemoryTrain:  epoch  0, batch     2 | loss: 4.9048848Losses:  28.062665939331055 -0.0 26.800758361816406
MemoryTrain:  epoch  1, batch     0 | loss: 28.0626659Losses:  24.13469696044922 -0.0 23.19422721862793
MemoryTrain:  epoch  1, batch     1 | loss: 24.1346970Losses:  2.644665002822876 -0.0 1.406416893005371
MemoryTrain:  epoch  1, batch     2 | loss: 2.6446650Losses:  31.140352249145508 -0.0 30.26276969909668
MemoryTrain:  epoch  2, batch     0 | loss: 31.1403522Losses:  23.490158081054688 -0.0 23.17183494567871
MemoryTrain:  epoch  2, batch     1 | loss: 23.4901581Losses:  4.519089698791504 -0.0 3.31215763092041
MemoryTrain:  epoch  2, batch     2 | loss: 4.5190897Losses:  17.62504768371582 -0.0 16.77989387512207
MemoryTrain:  epoch  3, batch     0 | loss: 17.6250477Losses:  17.103111267089844 -0.0 16.71827507019043
MemoryTrain:  epoch  3, batch     1 | loss: 17.1031113Losses:  3.390571117401123 -0.0 3.3226048946380615
MemoryTrain:  epoch  3, batch     2 | loss: 3.3905711Losses:  20.456134796142578 -0.0 19.939579010009766
MemoryTrain:  epoch  4, batch     0 | loss: 20.4561348Losses:  23.221498489379883 -0.0 23.161386489868164
MemoryTrain:  epoch  4, batch     1 | loss: 23.2214985Losses:  2.0651137828826904 -0.0 1.4274365901947021
MemoryTrain:  epoch  4, batch     2 | loss: 2.0651138Losses:  17.019481658935547 -0.0 16.75714874267578
MemoryTrain:  epoch  5, batch     0 | loss: 17.0194817Losses:  26.578266143798828 -0.0 26.481908798217773
MemoryTrain:  epoch  5, batch     1 | loss: 26.5782661Losses:  3.429178237915039 -0.0 3.331974506378174
MemoryTrain:  epoch  5, batch     2 | loss: 3.4291782Losses:  33.65903854370117 -0.0 33.55974578857422
MemoryTrain:  epoch  6, batch     0 | loss: 33.6590385Losses:  20.018295288085938 -0.0 19.9069881439209
MemoryTrain:  epoch  6, batch     1 | loss: 20.0182953Losses:  1.4908555746078491 -0.0 1.4487299919128418
MemoryTrain:  epoch  6, batch     2 | loss: 1.4908556Losses:  23.465633392333984 -0.0 23.334131240844727
MemoryTrain:  epoch  7, batch     0 | loss: 23.4656334Losses:  30.08137321472168 -0.0 29.995685577392578
MemoryTrain:  epoch  7, batch     1 | loss: 30.0813732Losses:  3.341021776199341 -0.0 3.3159146308898926
MemoryTrain:  epoch  7, batch     2 | loss: 3.3410218Losses:  29.96630859375 -0.0 29.942106246948242
MemoryTrain:  epoch  8, batch     0 | loss: 29.9663086Losses:  26.657215118408203 -0.0 26.540576934814453
MemoryTrain:  epoch  8, batch     1 | loss: 26.6572151Losses:  1.4120855331420898 -0.0 1.3909000158309937
MemoryTrain:  epoch  8, batch     2 | loss: 1.4120855Losses:  26.53037452697754 -0.0 26.494243621826172
MemoryTrain:  epoch  9, batch     0 | loss: 26.5303745Losses:  30.03508186340332 -0.0 29.97252082824707
MemoryTrain:  epoch  9, batch     1 | loss: 30.0350819Losses:  3.3801357746124268 -0.0 3.329535484313965
MemoryTrain:  epoch  9, batch     2 | loss: 3.3801358
[EVAL] batch:    0 | acc: 25.00%,  total acc: 25.00%   [EVAL] batch:    1 | acc: 50.00%,  total acc: 37.50%   [EVAL] batch:    2 | acc: 37.50%,  total acc: 37.50%   [EVAL] batch:    3 | acc: 56.25%,  total acc: 42.19%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 48.75%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 54.17%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 56.25%   [EVAL] batch:    7 | acc: 62.50%,  total acc: 57.03%   [EVAL] batch:    8 | acc: 37.50%,  total acc: 54.86%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 58.13%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 60.23%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 62.50%   [EVAL] batch:   12 | acc: 50.00%,  total acc: 61.54%   
[EVAL] batch:    0 | acc: 6.25%,  total acc: 6.25%   [EVAL] batch:    1 | acc: 12.50%,  total acc: 9.38%   [EVAL] batch:    2 | acc: 31.25%,  total acc: 16.67%   [EVAL] batch:    3 | acc: 6.25%,  total acc: 14.06%   [EVAL] batch:    4 | acc: 25.00%,  total acc: 16.25%   [EVAL] batch:    5 | acc: 6.25%,  total acc: 14.58%   [EVAL] batch:    6 | acc: 6.25%,  total acc: 13.39%   [EVAL] batch:    7 | acc: 0.00%,  total acc: 11.72%   [EVAL] batch:    8 | acc: 0.00%,  total acc: 10.42%   [EVAL] batch:    9 | acc: 0.00%,  total acc: 9.38%   [EVAL] batch:   10 | acc: 0.00%,  total acc: 8.52%   [EVAL] batch:   11 | acc: 0.00%,  total acc: 7.81%   [EVAL] batch:   12 | acc: 6.25%,  total acc: 7.69%   [EVAL] batch:   13 | acc: 31.25%,  total acc: 9.38%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 13.75%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 16.41%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 19.85%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 22.22%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 24.67%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 27.81%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 31.25%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 34.38%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 37.23%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 39.58%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 42.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 44.23%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 46.06%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 47.99%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 49.57%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 50.62%   [EVAL] batch:   30 | acc: 68.75%,  total acc: 51.21%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 52.73%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 52.27%   [EVAL] batch:   33 | acc: 6.25%,  total acc: 50.92%   [EVAL] batch:   34 | acc: 6.25%,  total acc: 49.64%   [EVAL] batch:   35 | acc: 0.00%,  total acc: 48.26%   [EVAL] batch:   36 | acc: 0.00%,  total acc: 46.96%   [EVAL] batch:   37 | acc: 43.75%,  total acc: 46.88%   [EVAL] batch:   38 | acc: 68.75%,  total acc: 47.44%   [EVAL] batch:   39 | acc: 87.50%,  total acc: 48.44%   [EVAL] batch:   40 | acc: 68.75%,  total acc: 48.93%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 50.00%   [EVAL] batch:   42 | acc: 0.00%,  total acc: 48.84%   [EVAL] batch:   43 | acc: 6.25%,  total acc: 47.87%   [EVAL] batch:   44 | acc: 0.00%,  total acc: 46.81%   [EVAL] batch:   45 | acc: 0.00%,  total acc: 45.79%   [EVAL] batch:   46 | acc: 12.50%,  total acc: 45.08%   [EVAL] batch:   47 | acc: 56.25%,  total acc: 45.31%   [EVAL] batch:   48 | acc: 12.50%,  total acc: 44.64%   [EVAL] batch:   49 | acc: 6.25%,  total acc: 43.88%   [EVAL] batch:   50 | acc: 0.00%,  total acc: 43.01%   [EVAL] batch:   51 | acc: 12.50%,  total acc: 42.43%   [EVAL] batch:   52 | acc: 0.00%,  total acc: 41.63%   [EVAL] batch:   53 | acc: 43.75%,  total acc: 41.67%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 42.39%   [EVAL] batch:   55 | acc: 68.75%,  total acc: 42.86%   [EVAL] batch:   56 | acc: 75.00%,  total acc: 43.42%   [EVAL] batch:   57 | acc: 68.75%,  total acc: 43.86%   [EVAL] batch:   58 | acc: 50.00%,  total acc: 43.96%   [EVAL] batch:   59 | acc: 50.00%,  total acc: 44.06%   [EVAL] batch:   60 | acc: 0.00%,  total acc: 43.34%   [EVAL] batch:   61 | acc: 0.00%,  total acc: 42.64%   [EVAL] batch:   62 | acc: 0.00%,  total acc: 41.96%   [EVAL] batch:   63 | acc: 0.00%,  total acc: 41.31%   [EVAL] batch:   64 | acc: 6.25%,  total acc: 40.77%   [EVAL] batch:   65 | acc: 0.00%,  total acc: 40.15%   [EVAL] batch:   66 | acc: 18.75%,  total acc: 39.83%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 40.72%   [EVAL] batch:   68 | acc: 43.75%,  total acc: 40.76%   [EVAL] batch:   69 | acc: 25.00%,  total acc: 40.54%   [EVAL] batch:   70 | acc: 37.50%,  total acc: 40.49%   [EVAL] batch:   71 | acc: 62.50%,  total acc: 40.80%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 41.61%   [EVAL] batch:   73 | acc: 100.00%,  total acc: 42.40%   [EVAL] batch:   74 | acc: 100.00%,  total acc: 43.17%   [EVAL] batch:   75 | acc: 100.00%,  total acc: 43.91%   [EVAL] batch:   76 | acc: 100.00%,  total acc: 44.64%   [EVAL] batch:   77 | acc: 81.25%,  total acc: 45.11%   [EVAL] batch:   78 | acc: 37.50%,  total acc: 45.02%   [EVAL] batch:   79 | acc: 37.50%,  total acc: 44.92%   [EVAL] batch:   80 | acc: 18.75%,  total acc: 44.60%   [EVAL] batch:   81 | acc: 6.25%,  total acc: 44.13%   [EVAL] batch:   82 | acc: 0.00%,  total acc: 43.60%   [EVAL] batch:   83 | acc: 12.50%,  total acc: 43.23%   [EVAL] batch:   84 | acc: 25.00%,  total acc: 43.01%   [EVAL] batch:   85 | acc: 56.25%,  total acc: 43.17%   [EVAL] batch:   86 | acc: 56.25%,  total acc: 43.32%   [EVAL] batch:   87 | acc: 37.50%,  total acc: 43.25%   [EVAL] batch:   88 | acc: 62.50%,  total acc: 43.47%   [EVAL] batch:   89 | acc: 68.75%,  total acc: 43.75%   [EVAL] batch:   90 | acc: 50.00%,  total acc: 43.82%   [EVAL] batch:   91 | acc: 43.75%,  total acc: 43.82%   [EVAL] batch:   92 | acc: 12.50%,  total acc: 43.48%   [EVAL] batch:   93 | acc: 43.75%,  total acc: 43.48%   [EVAL] batch:   94 | acc: 18.75%,  total acc: 43.22%   [EVAL] batch:   95 | acc: 12.50%,  total acc: 42.90%   [EVAL] batch:   96 | acc: 0.00%,  total acc: 42.46%   [EVAL] batch:   97 | acc: 0.00%,  total acc: 42.03%   [EVAL] batch:   98 | acc: 0.00%,  total acc: 41.60%   [EVAL] batch:   99 | acc: 68.75%,  total acc: 41.88%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 42.45%   [EVAL] batch:  101 | acc: 81.25%,  total acc: 42.83%   [EVAL] batch:  102 | acc: 93.75%,  total acc: 43.33%   [EVAL] batch:  103 | acc: 100.00%,  total acc: 43.87%   [EVAL] batch:  104 | acc: 100.00%,  total acc: 44.40%   [EVAL] batch:  105 | acc: 100.00%,  total acc: 44.93%   [EVAL] batch:  106 | acc: 43.75%,  total acc: 44.92%   [EVAL] batch:  107 | acc: 37.50%,  total acc: 44.85%   [EVAL] batch:  108 | acc: 50.00%,  total acc: 44.90%   [EVAL] batch:  109 | acc: 50.00%,  total acc: 44.94%   [EVAL] batch:  110 | acc: 75.00%,  total acc: 45.21%   [EVAL] batch:  111 | acc: 75.00%,  total acc: 45.48%   [EVAL] batch:  112 | acc: 75.00%,  total acc: 45.74%   [EVAL] batch:  113 | acc: 56.25%,  total acc: 45.83%   [EVAL] batch:  114 | acc: 56.25%,  total acc: 45.92%   [EVAL] batch:  115 | acc: 62.50%,  total acc: 46.07%   [EVAL] batch:  116 | acc: 81.25%,  total acc: 46.37%   [EVAL] batch:  117 | acc: 93.75%,  total acc: 46.77%   [EVAL] batch:  118 | acc: 75.00%,  total acc: 47.01%   
cur_acc:  ['0.8561', '0.5208', '0.7589', '0.7743', '0.5227', '0.8359', '0.6154']
his_acc:  ['0.8561', '0.7739', '0.7039', '0.6939', '0.6169', '0.5356', '0.4701']
Clustering into  19  clusters
Clusters:  [ 4  1  0 11 17 10  4  3  9  8 16  7  4 16  1 18  4 15 18 16 12  0  0  8
 10 16  4 13  5  2  1  3  6 10 14 18  2  7  4  0  6]
Losses:  19.731826782226562 9.232521057128906 5.649607181549072
CurrentTrain: epoch  0, batch     0 | loss: 19.7318268Losses:  13.267797470092773 4.731800079345703 3.3436129093170166
CurrentTrain: epoch  0, batch     1 | loss: 13.2677975Losses:  17.922420501708984 8.080608367919922 5.586699485778809
CurrentTrain: epoch  1, batch     0 | loss: 17.9224205Losses:  9.478633880615234 2.9355976581573486 3.3523077964782715
CurrentTrain: epoch  1, batch     1 | loss: 9.4786339Losses:  15.25465202331543 6.39125394821167 5.608546733856201
CurrentTrain: epoch  2, batch     0 | loss: 15.2546520Losses:  11.687477111816406 2.456960678100586 5.578986644744873
CurrentTrain: epoch  2, batch     1 | loss: 11.6874771Losses:  14.708951950073242 6.176856994628906 5.586547374725342
CurrentTrain: epoch  3, batch     0 | loss: 14.7089520Losses:  10.643030166625977 1.9569069147109985 5.581130504608154
CurrentTrain: epoch  3, batch     1 | loss: 10.6430302Losses:  15.646167755126953 7.146414756774902 5.578451156616211
CurrentTrain: epoch  4, batch     0 | loss: 15.6461678Losses:  11.582677841186523 3.2969279289245605 5.607789993286133
CurrentTrain: epoch  4, batch     1 | loss: 11.5826778Losses:  15.343387603759766 7.044282913208008 5.572181701660156
CurrentTrain: epoch  5, batch     0 | loss: 15.3433876Losses:  11.137619018554688 3.1867825984954834 5.606326103210449
CurrentTrain: epoch  5, batch     1 | loss: 11.1376190Losses:  14.592901229858398 6.8000407218933105 5.58123254776001
CurrentTrain: epoch  6, batch     0 | loss: 14.5929012Losses:  8.428956031799316 2.7638659477233887 3.3054630756378174
CurrentTrain: epoch  6, batch     1 | loss: 8.4289560Losses:  14.101093292236328 6.360459327697754 5.586930274963379
CurrentTrain: epoch  7, batch     0 | loss: 14.1010933Losses:  9.91575813293457 2.3899803161621094 5.579261779785156
CurrentTrain: epoch  7, batch     1 | loss: 9.9157581Losses:  13.465872764587402 5.797360420227051 5.576425552368164
CurrentTrain: epoch  8, batch     0 | loss: 13.4658728Losses:  9.075040817260742 1.6672362089157104 5.573075294494629
CurrentTrain: epoch  8, batch     1 | loss: 9.0750408Losses:  12.69780158996582 5.256630897521973 5.56369161605835
CurrentTrain: epoch  9, batch     0 | loss: 12.6978016Losses:  8.627663612365723 1.1581307649612427 5.571559429168701
CurrentTrain: epoch  9, batch     1 | loss: 8.6276636
Losses:  27.269746780395508 -0.0 26.541133880615234
MemoryTrain:  epoch  0, batch     0 | loss: 27.2697468Losses:  27.468616485595703 -0.0 26.633464813232422
MemoryTrain:  epoch  0, batch     1 | loss: 27.4686165Losses:  11.815274238586426 -0.0 10.912175178527832
MemoryTrain:  epoch  0, batch     2 | loss: 11.8152742Losses:  27.70119285583496 -0.0 26.580596923828125
MemoryTrain:  epoch  1, batch     0 | loss: 27.7011929Losses:  30.42091178894043 -0.0 30.05913543701172
MemoryTrain:  epoch  1, batch     1 | loss: 30.4209118Losses:  14.872270584106445 -0.0 13.695747375488281
MemoryTrain:  epoch  1, batch     2 | loss: 14.8722706Losses:  24.217885971069336 -0.0 23.243459701538086
MemoryTrain:  epoch  2, batch     0 | loss: 24.2178860Losses:  23.24541473388672 -0.0 23.137285232543945
MemoryTrain:  epoch  2, batch     1 | loss: 23.2454147Losses:  17.15121078491211 -0.0 16.706241607666016
MemoryTrain:  epoch  2, batch     2 | loss: 17.1512108Losses:  23.41719627380371 -0.0 23.190067291259766
MemoryTrain:  epoch  3, batch     0 | loss: 23.4171963Losses:  26.75022315979004 -0.0 26.520156860351562
MemoryTrain:  epoch  3, batch     1 | loss: 26.7502232Losses:  14.104376792907715 -0.0 13.754541397094727
MemoryTrain:  epoch  3, batch     2 | loss: 14.1043768Losses:  23.520200729370117 -0.0 23.249065399169922
MemoryTrain:  epoch  4, batch     0 | loss: 23.5202007Losses:  16.91464614868164 -0.0 16.726701736450195
MemoryTrain:  epoch  4, batch     1 | loss: 16.9146461Losses:  16.87735366821289 -0.0 16.789318084716797
MemoryTrain:  epoch  4, batch     2 | loss: 16.8773537Losses:  23.276351928710938 -0.0 23.124650955200195
MemoryTrain:  epoch  5, batch     0 | loss: 23.2763519Losses:  26.72028160095215 -0.0 26.589574813842773
MemoryTrain:  epoch  5, batch     1 | loss: 26.7202816Losses:  8.199993133544922 -0.0 8.142729759216309
MemoryTrain:  epoch  5, batch     2 | loss: 8.1999931Losses:  19.97881507873535 -0.0 19.843374252319336
MemoryTrain:  epoch  6, batch     0 | loss: 19.9788151Losses:  23.40974998474121 -0.0 23.174083709716797
MemoryTrain:  epoch  6, batch     1 | loss: 23.4097500Losses:  10.958434104919434 -0.0 10.784290313720703
MemoryTrain:  epoch  6, batch     2 | loss: 10.9584341Losses:  16.85017204284668 -0.0 16.713918685913086
MemoryTrain:  epoch  7, batch     0 | loss: 16.8501720Losses:  23.34218406677246 -0.0 23.193382263183594
MemoryTrain:  epoch  7, batch     1 | loss: 23.3421841Losses:  16.748733520507812 -0.0 16.73228645324707
MemoryTrain:  epoch  7, batch     2 | loss: 16.7487335Losses:  13.748244285583496 -0.0 13.67894172668457
MemoryTrain:  epoch  8, batch     0 | loss: 13.7482443Losses:  23.153818130493164 -0.0 23.11604881286621
MemoryTrain:  epoch  8, batch     1 | loss: 23.1538181Losses:  16.726215362548828 -0.0 16.70009422302246
MemoryTrain:  epoch  8, batch     2 | loss: 16.7262154Losses:  19.911563873291016 -0.0 19.870088577270508
MemoryTrain:  epoch  9, batch     0 | loss: 19.9115639Losses:  29.948537826538086 -0.0 29.919822692871094
MemoryTrain:  epoch  9, batch     1 | loss: 29.9485378Losses:  8.163202285766602 -0.0 8.124800682067871
MemoryTrain:  epoch  9, batch     2 | loss: 8.1632023
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 100.00%,  total acc: 91.67%   [EVAL] batch:    3 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:    4 | acc: 100.00%,  total acc: 95.00%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 95.83%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 96.43%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 96.88%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 95.14%   [EVAL] batch:    9 | acc: 37.50%,  total acc: 89.38%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 89.20%   [EVAL] batch:   11 | acc: 43.75%,  total acc: 85.42%   [EVAL] batch:   12 | acc: 50.00%,  total acc: 82.69%   [EVAL] batch:   13 | acc: 31.25%,  total acc: 79.02%   
[EVAL] batch:    0 | acc: 6.25%,  total acc: 6.25%   [EVAL] batch:    1 | acc: 12.50%,  total acc: 9.38%   [EVAL] batch:    2 | acc: 18.75%,  total acc: 12.50%   [EVAL] batch:    3 | acc: 6.25%,  total acc: 10.94%   [EVAL] batch:    4 | acc: 25.00%,  total acc: 13.75%   [EVAL] batch:    5 | acc: 6.25%,  total acc: 12.50%   [EVAL] batch:    6 | acc: 6.25%,  total acc: 11.61%   [EVAL] batch:    7 | acc: 0.00%,  total acc: 10.16%   [EVAL] batch:    8 | acc: 0.00%,  total acc: 9.03%   [EVAL] batch:    9 | acc: 12.50%,  total acc: 9.38%   [EVAL] batch:   10 | acc: 6.25%,  total acc: 9.09%   [EVAL] batch:   11 | acc: 12.50%,  total acc: 9.38%   [EVAL] batch:   12 | acc: 12.50%,  total acc: 9.62%   [EVAL] batch:   13 | acc: 25.00%,  total acc: 10.71%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 15.00%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 17.58%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 20.96%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 23.26%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 25.33%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 28.44%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 31.85%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 34.94%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 37.50%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 39.84%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 42.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 44.47%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 46.30%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 48.21%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 49.78%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 50.83%   [EVAL] batch:   30 | acc: 75.00%,  total acc: 51.61%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 53.12%   [EVAL] batch:   32 | acc: 31.25%,  total acc: 52.46%   [EVAL] batch:   33 | acc: 0.00%,  total acc: 50.92%   [EVAL] batch:   34 | acc: 0.00%,  total acc: 49.46%   [EVAL] batch:   35 | acc: 0.00%,  total acc: 48.09%   [EVAL] batch:   36 | acc: 0.00%,  total acc: 46.79%   [EVAL] batch:   37 | acc: 43.75%,  total acc: 46.71%   [EVAL] batch:   38 | acc: 81.25%,  total acc: 47.60%   [EVAL] batch:   39 | acc: 93.75%,  total acc: 48.75%   [EVAL] batch:   40 | acc: 68.75%,  total acc: 49.24%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 50.45%   [EVAL] batch:   42 | acc: 0.00%,  total acc: 49.27%   [EVAL] batch:   43 | acc: 6.25%,  total acc: 48.30%   [EVAL] batch:   44 | acc: 6.25%,  total acc: 47.36%   [EVAL] batch:   45 | acc: 0.00%,  total acc: 46.33%   [EVAL] batch:   46 | acc: 12.50%,  total acc: 45.61%   [EVAL] batch:   47 | acc: 43.75%,  total acc: 45.57%   [EVAL] batch:   48 | acc: 25.00%,  total acc: 45.15%   [EVAL] batch:   49 | acc: 6.25%,  total acc: 44.38%   [EVAL] batch:   50 | acc: 0.00%,  total acc: 43.50%   [EVAL] batch:   51 | acc: 12.50%,  total acc: 42.91%   [EVAL] batch:   52 | acc: 0.00%,  total acc: 42.10%   [EVAL] batch:   53 | acc: 37.50%,  total acc: 42.01%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 42.84%   [EVAL] batch:   55 | acc: 62.50%,  total acc: 43.19%   [EVAL] batch:   56 | acc: 75.00%,  total acc: 43.75%   [EVAL] batch:   57 | acc: 62.50%,  total acc: 44.07%   [EVAL] batch:   58 | acc: 56.25%,  total acc: 44.28%   [EVAL] batch:   59 | acc: 56.25%,  total acc: 44.48%   [EVAL] batch:   60 | acc: 0.00%,  total acc: 43.75%   [EVAL] batch:   61 | acc: 0.00%,  total acc: 43.04%   [EVAL] batch:   62 | acc: 0.00%,  total acc: 42.36%   [EVAL] batch:   63 | acc: 0.00%,  total acc: 41.70%   [EVAL] batch:   64 | acc: 0.00%,  total acc: 41.06%   [EVAL] batch:   65 | acc: 0.00%,  total acc: 40.44%   [EVAL] batch:   66 | acc: 18.75%,  total acc: 40.11%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 40.99%   [EVAL] batch:   68 | acc: 43.75%,  total acc: 41.03%   [EVAL] batch:   69 | acc: 25.00%,  total acc: 40.80%   [EVAL] batch:   70 | acc: 37.50%,  total acc: 40.76%   [EVAL] batch:   71 | acc: 62.50%,  total acc: 41.06%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 41.87%   [EVAL] batch:   73 | acc: 100.00%,  total acc: 42.65%   [EVAL] batch:   74 | acc: 100.00%,  total acc: 43.42%   [EVAL] batch:   75 | acc: 100.00%,  total acc: 44.16%   [EVAL] batch:   76 | acc: 100.00%,  total acc: 44.89%   [EVAL] batch:   77 | acc: 81.25%,  total acc: 45.35%   [EVAL] batch:   78 | acc: 18.75%,  total acc: 45.02%   [EVAL] batch:   79 | acc: 31.25%,  total acc: 44.84%   [EVAL] batch:   80 | acc: 12.50%,  total acc: 44.44%   [EVAL] batch:   81 | acc: 0.00%,  total acc: 43.90%   [EVAL] batch:   82 | acc: 0.00%,  total acc: 43.37%   [EVAL] batch:   83 | acc: 0.00%,  total acc: 42.86%   [EVAL] batch:   84 | acc: 18.75%,  total acc: 42.57%   [EVAL] batch:   85 | acc: 37.50%,  total acc: 42.51%   [EVAL] batch:   86 | acc: 43.75%,  total acc: 42.53%   [EVAL] batch:   87 | acc: 25.00%,  total acc: 42.33%   [EVAL] batch:   88 | acc: 37.50%,  total acc: 42.28%   [EVAL] batch:   89 | acc: 37.50%,  total acc: 42.22%   [EVAL] batch:   90 | acc: 37.50%,  total acc: 42.17%   [EVAL] batch:   91 | acc: 18.75%,  total acc: 41.92%   [EVAL] batch:   92 | acc: 6.25%,  total acc: 41.53%   [EVAL] batch:   93 | acc: 25.00%,  total acc: 41.36%   [EVAL] batch:   94 | acc: 12.50%,  total acc: 41.05%   [EVAL] batch:   95 | acc: 12.50%,  total acc: 40.76%   [EVAL] batch:   96 | acc: 0.00%,  total acc: 40.34%   [EVAL] batch:   97 | acc: 0.00%,  total acc: 39.92%   [EVAL] batch:   98 | acc: 0.00%,  total acc: 39.52%   [EVAL] batch:   99 | acc: 68.75%,  total acc: 39.81%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 40.41%   [EVAL] batch:  101 | acc: 25.00%,  total acc: 40.26%   [EVAL] batch:  102 | acc: 87.50%,  total acc: 40.72%   [EVAL] batch:  103 | acc: 75.00%,  total acc: 41.05%   [EVAL] batch:  104 | acc: 81.25%,  total acc: 41.43%   [EVAL] batch:  105 | acc: 100.00%,  total acc: 41.98%   [EVAL] batch:  106 | acc: 43.75%,  total acc: 42.00%   [EVAL] batch:  107 | acc: 37.50%,  total acc: 41.96%   [EVAL] batch:  108 | acc: 50.00%,  total acc: 42.03%   [EVAL] batch:  109 | acc: 56.25%,  total acc: 42.16%   [EVAL] batch:  110 | acc: 75.00%,  total acc: 42.45%   [EVAL] batch:  111 | acc: 81.25%,  total acc: 42.80%   [EVAL] batch:  112 | acc: 68.75%,  total acc: 43.03%   [EVAL] batch:  113 | acc: 62.50%,  total acc: 43.20%   [EVAL] batch:  114 | acc: 37.50%,  total acc: 43.15%   [EVAL] batch:  115 | acc: 50.00%,  total acc: 43.21%   [EVAL] batch:  116 | acc: 75.00%,  total acc: 43.48%   [EVAL] batch:  117 | acc: 81.25%,  total acc: 43.80%   [EVAL] batch:  118 | acc: 75.00%,  total acc: 44.07%   [EVAL] batch:  119 | acc: 75.00%,  total acc: 44.32%   [EVAL] batch:  120 | acc: 100.00%,  total acc: 44.78%   [EVAL] batch:  121 | acc: 100.00%,  total acc: 45.24%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 45.68%   [EVAL] batch:  123 | acc: 100.00%,  total acc: 46.12%   [EVAL] batch:  124 | acc: 100.00%,  total acc: 46.55%   [EVAL] batch:  125 | acc: 100.00%,  total acc: 46.97%   [EVAL] batch:  126 | acc: 100.00%,  total acc: 47.39%   [EVAL] batch:  127 | acc: 75.00%,  total acc: 47.61%   [EVAL] batch:  128 | acc: 43.75%,  total acc: 47.58%   [EVAL] batch:  129 | acc: 81.25%,  total acc: 47.84%   [EVAL] batch:  130 | acc: 37.50%,  total acc: 47.76%   [EVAL] batch:  131 | acc: 62.50%,  total acc: 47.87%   [EVAL] batch:  132 | acc: 18.75%,  total acc: 47.65%   
cur_acc:  ['0.8561', '0.5208', '0.7589', '0.7743', '0.5227', '0.8359', '0.6154', '0.7902']
his_acc:  ['0.8561', '0.7739', '0.7039', '0.6939', '0.6169', '0.5356', '0.4701', '0.4765']
--------Round  2
seed:  300
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/test.pkl
Task_order: [7 2 3 1 5 6 0 4]
prepared data!
Clustering into  2  clusters
Clusters:  [1 0 0 1 0 1]
Losses:  20.790462493896484 7.5602192878723145 -0.0
CurrentTrain: epoch  0, batch     0 | loss: 20.7904625Losses:  26.831092834472656 13.684553146362305 -0.0
CurrentTrain: epoch  0, batch     1 | loss: 26.8310928Losses:  22.56277847290039 9.488160133361816 -0.0
CurrentTrain: epoch  0, batch     2 | loss: 22.5627785Losses:  25.01136016845703 12.431983947753906 -0.0
CurrentTrain: epoch  0, batch     3 | loss: 25.0113602Losses:  21.537935256958008 8.878742218017578 -0.0
CurrentTrain: epoch  0, batch     4 | loss: 21.5379353Losses:  23.651451110839844 11.01191234588623 -0.0
CurrentTrain: epoch  0, batch     5 | loss: 23.6514511Losses:  20.853679656982422 8.704368591308594 -0.0
CurrentTrain: epoch  0, batch     6 | loss: 20.8536797Losses:  19.41488265991211 7.212108612060547 -0.0
CurrentTrain: epoch  0, batch     7 | loss: 19.4148827Losses:  22.549156188964844 10.580057144165039 -0.0
CurrentTrain: epoch  0, batch     8 | loss: 22.5491562Losses:  20.733600616455078 8.79513168334961 -0.0
CurrentTrain: epoch  0, batch     9 | loss: 20.7336006Losses:  22.95730972290039 11.093538284301758 -0.0
CurrentTrain: epoch  0, batch    10 | loss: 22.9573097Losses:  22.912553787231445 11.23662281036377 -0.0
CurrentTrain: epoch  0, batch    11 | loss: 22.9125538Losses:  22.37203598022461 11.157549858093262 -0.0
CurrentTrain: epoch  0, batch    12 | loss: 22.3720360Losses:  26.43486785888672 14.674759864807129 -0.0
CurrentTrain: epoch  0, batch    13 | loss: 26.4348679Losses:  22.325157165527344 10.644159317016602 -0.0
CurrentTrain: epoch  0, batch    14 | loss: 22.3251572Losses:  19.754018783569336 8.381638526916504 -0.0
CurrentTrain: epoch  0, batch    15 | loss: 19.7540188Losses:  19.016918182373047 7.817779541015625 -0.0
CurrentTrain: epoch  0, batch    16 | loss: 19.0169182Losses:  19.553699493408203 8.648394584655762 -0.0
CurrentTrain: epoch  0, batch    17 | loss: 19.5536995Losses:  19.27963638305664 7.649375915527344 -0.0
CurrentTrain: epoch  0, batch    18 | loss: 19.2796364Losses:  18.615825653076172 7.504695892333984 -0.0
CurrentTrain: epoch  0, batch    19 | loss: 18.6158257Losses:  21.982446670532227 10.364859580993652 -0.0
CurrentTrain: epoch  0, batch    20 | loss: 21.9824467Losses:  21.202802658081055 9.954238891601562 -0.0
CurrentTrain: epoch  0, batch    21 | loss: 21.2028027Losses:  19.309856414794922 8.268959999084473 -0.0
CurrentTrain: epoch  0, batch    22 | loss: 19.3098564Losses:  18.74195098876953 7.920994758605957 -0.0
CurrentTrain: epoch  0, batch    23 | loss: 18.7419510Losses:  17.769330978393555 7.480527400970459 -0.0
CurrentTrain: epoch  0, batch    24 | loss: 17.7693310Losses:  20.182079315185547 9.526346206665039 -0.0
CurrentTrain: epoch  0, batch    25 | loss: 20.1820793Losses:  20.680360794067383 9.362774848937988 -0.0
CurrentTrain: epoch  0, batch    26 | loss: 20.6803608Losses:  22.981979370117188 12.220824241638184 -0.0
CurrentTrain: epoch  0, batch    27 | loss: 22.9819794Losses:  17.730972290039062 7.116614818572998 -0.0
CurrentTrain: epoch  0, batch    28 | loss: 17.7309723Losses:  17.659297943115234 7.079724311828613 -0.0
CurrentTrain: epoch  0, batch    29 | loss: 17.6592979Losses:  17.3328800201416 6.6975531578063965 -0.0
CurrentTrain: epoch  0, batch    30 | loss: 17.3328800Losses:  18.91404151916504 8.947195053100586 -0.0
CurrentTrain: epoch  0, batch    31 | loss: 18.9140415Losses:  19.04682159423828 8.333052635192871 -0.0
CurrentTrain: epoch  0, batch    32 | loss: 19.0468216Losses:  21.96550750732422 11.075387954711914 -0.0
CurrentTrain: epoch  0, batch    33 | loss: 21.9655075Losses:  17.23236656188965 7.189909934997559 -0.0
CurrentTrain: epoch  0, batch    34 | loss: 17.2323666Losses:  19.161544799804688 8.282037734985352 -0.0
CurrentTrain: epoch  0, batch    35 | loss: 19.1615448Losses:  16.913423538208008 6.994719982147217 -0.0
CurrentTrain: epoch  0, batch    36 | loss: 16.9134235Losses:  13.639041900634766 2.9616785049438477 -0.0
CurrentTrain: epoch  0, batch    37 | loss: 13.6390419Losses:  21.68557357788086 11.731467247009277 -0.0
CurrentTrain: epoch  1, batch     0 | loss: 21.6855736Losses:  18.775054931640625 8.439090728759766 -0.0
CurrentTrain: epoch  1, batch     1 | loss: 18.7750549Losses:  17.446598052978516 7.688787460327148 -0.0
CurrentTrain: epoch  1, batch     2 | loss: 17.4465981Losses:  17.491676330566406 7.582521915435791 -0.0
CurrentTrain: epoch  1, batch     3 | loss: 17.4916763Losses:  17.79043960571289 7.954151630401611 -0.0
CurrentTrain: epoch  1, batch     4 | loss: 17.7904396Losses:  19.40660858154297 9.444300651550293 -0.0
CurrentTrain: epoch  1, batch     5 | loss: 19.4066086Losses:  22.32732391357422 12.134621620178223 -0.0
CurrentTrain: epoch  1, batch     6 | loss: 22.3273239Losses:  17.023815155029297 7.406271457672119 -0.0
CurrentTrain: epoch  1, batch     7 | loss: 17.0238152Losses:  17.449201583862305 8.139043807983398 -0.0
CurrentTrain: epoch  1, batch     8 | loss: 17.4492016Losses:  19.42336654663086 9.498785018920898 -0.0
CurrentTrain: epoch  1, batch     9 | loss: 19.4233665Losses:  15.63404655456543 6.342989444732666 -0.0
CurrentTrain: epoch  1, batch    10 | loss: 15.6340466Losses:  15.717279434204102 6.280562877655029 -0.0
CurrentTrain: epoch  1, batch    11 | loss: 15.7172794Losses:  16.527938842773438 7.71755313873291 -0.0
CurrentTrain: epoch  1, batch    12 | loss: 16.5279388Losses:  18.206424713134766 8.593972206115723 -0.0
CurrentTrain: epoch  1, batch    13 | loss: 18.2064247Losses:  15.75172233581543 6.627463340759277 -0.0
CurrentTrain: epoch  1, batch    14 | loss: 15.7517223Losses:  14.72707748413086 5.3676438331604 -0.0
CurrentTrain: epoch  1, batch    15 | loss: 14.7270775Losses:  21.234636306762695 11.178678512573242 -0.0
CurrentTrain: epoch  1, batch    16 | loss: 21.2346363Losses:  18.804039001464844 9.728291511535645 -0.0
CurrentTrain: epoch  1, batch    17 | loss: 18.8040390Losses:  16.796449661254883 7.377555847167969 -0.0
CurrentTrain: epoch  1, batch    18 | loss: 16.7964497Losses:  17.9674072265625 8.878717422485352 -0.0
CurrentTrain: epoch  1, batch    19 | loss: 17.9674072Losses:  15.870190620422363 6.857344627380371 -0.0
CurrentTrain: epoch  1, batch    20 | loss: 15.8701906Losses:  17.879810333251953 8.131508827209473 -0.0
CurrentTrain: epoch  1, batch    21 | loss: 17.8798103Losses:  18.45265007019043 9.912771224975586 -0.0
CurrentTrain: epoch  1, batch    22 | loss: 18.4526501Losses:  16.40993881225586 7.196321964263916 -0.0
CurrentTrain: epoch  1, batch    23 | loss: 16.4099388Losses:  19.222272872924805 9.883465766906738 -0.0
CurrentTrain: epoch  1, batch    24 | loss: 19.2222729Losses:  15.224067687988281 5.821170806884766 -0.0
CurrentTrain: epoch  1, batch    25 | loss: 15.2240677Losses:  16.471057891845703 6.847649574279785 -0.0
CurrentTrain: epoch  1, batch    26 | loss: 16.4710579Losses:  16.20819091796875 7.2661356925964355 -0.0
CurrentTrain: epoch  1, batch    27 | loss: 16.2081909Losses:  17.626041412353516 9.800188064575195 -0.0
CurrentTrain: epoch  1, batch    28 | loss: 17.6260414Losses:  15.528804779052734 6.009578227996826 -0.0
CurrentTrain: epoch  1, batch    29 | loss: 15.5288048Losses:  15.905674934387207 6.70481014251709 -0.0
CurrentTrain: epoch  1, batch    30 | loss: 15.9056749Losses:  17.61436653137207 7.764613628387451 -0.0
CurrentTrain: epoch  1, batch    31 | loss: 17.6143665Losses:  14.453292846679688 6.164707183837891 -0.0
CurrentTrain: epoch  1, batch    32 | loss: 14.4532928Losses:  17.22124671936035 8.062755584716797 -0.0
CurrentTrain: epoch  1, batch    33 | loss: 17.2212467Losses:  18.5572452545166 9.74493408203125 -0.0
CurrentTrain: epoch  1, batch    34 | loss: 18.5572453Losses:  15.506863594055176 7.388174057006836 -0.0
CurrentTrain: epoch  1, batch    35 | loss: 15.5068636Losses:  15.438871383666992 6.501202583312988 -0.0
CurrentTrain: epoch  1, batch    36 | loss: 15.4388714Losses:  11.938711166381836 4.10861873626709 -0.0
CurrentTrain: epoch  1, batch    37 | loss: 11.9387112Losses:  13.850692749023438 6.445008754730225 -0.0
CurrentTrain: epoch  2, batch     0 | loss: 13.8506927Losses:  17.67770004272461 8.476232528686523 -0.0
CurrentTrain: epoch  2, batch     1 | loss: 17.6777000Losses:  14.005133628845215 5.482863426208496 -0.0
CurrentTrain: epoch  2, batch     2 | loss: 14.0051336Losses:  14.416141510009766 6.602441787719727 -0.0
CurrentTrain: epoch  2, batch     3 | loss: 14.4161415Losses:  15.987850189208984 6.874691009521484 -0.0
CurrentTrain: epoch  2, batch     4 | loss: 15.9878502Losses:  15.536897659301758 7.663896560668945 -0.0
CurrentTrain: epoch  2, batch     5 | loss: 15.5368977Losses:  13.923974990844727 5.776474475860596 -0.0
CurrentTrain: epoch  2, batch     6 | loss: 13.9239750Losses:  16.156009674072266 8.319510459899902 -0.0
CurrentTrain: epoch  2, batch     7 | loss: 16.1560097Losses:  15.030567169189453 6.691064834594727 -0.0
CurrentTrain: epoch  2, batch     8 | loss: 15.0305672Losses:  13.351736068725586 6.059573173522949 -0.0
CurrentTrain: epoch  2, batch     9 | loss: 13.3517361Losses:  18.464183807373047 9.876520156860352 -0.0
CurrentTrain: epoch  2, batch    10 | loss: 18.4641838Losses:  18.515220642089844 9.13129711151123 -0.0
CurrentTrain: epoch  2, batch    11 | loss: 18.5152206Losses:  17.0784969329834 7.099522113800049 -0.0
CurrentTrain: epoch  2, batch    12 | loss: 17.0784969Losses:  17.670921325683594 8.281683921813965 -0.0
CurrentTrain: epoch  2, batch    13 | loss: 17.6709213Losses:  12.7891845703125 4.627207279205322 -0.0
CurrentTrain: epoch  2, batch    14 | loss: 12.7891846Losses:  17.82640266418457 8.860397338867188 -0.0
CurrentTrain: epoch  2, batch    15 | loss: 17.8264027Losses:  14.241209030151367 5.96595573425293 -0.0
CurrentTrain: epoch  2, batch    16 | loss: 14.2412090Losses:  14.19368839263916 6.280370712280273 -0.0
CurrentTrain: epoch  2, batch    17 | loss: 14.1936884Losses:  14.444652557373047 6.22222900390625 -0.0
CurrentTrain: epoch  2, batch    18 | loss: 14.4446526Losses:  15.259352684020996 7.002930641174316 -0.0
CurrentTrain: epoch  2, batch    19 | loss: 15.2593527Losses:  13.43835735321045 5.809310436248779 -0.0
CurrentTrain: epoch  2, batch    20 | loss: 13.4383574Losses:  15.691899299621582 7.199856758117676 -0.0
CurrentTrain: epoch  2, batch    21 | loss: 15.6918993Losses:  15.669715881347656 8.411115646362305 -0.0
CurrentTrain: epoch  2, batch    22 | loss: 15.6697159Losses:  16.894161224365234 7.524341583251953 -0.0
CurrentTrain: epoch  2, batch    23 | loss: 16.8941612Losses:  14.102179527282715 5.7095746994018555 -0.0
CurrentTrain: epoch  2, batch    24 | loss: 14.1021795Losses:  15.029019355773926 7.581826210021973 -0.0
CurrentTrain: epoch  2, batch    25 | loss: 15.0290194Losses:  15.567211151123047 6.691408634185791 -0.0
CurrentTrain: epoch  2, batch    26 | loss: 15.5672112Losses:  13.653617858886719 5.342142581939697 -0.0
CurrentTrain: epoch  2, batch    27 | loss: 13.6536179Losses:  14.592782020568848 6.539669990539551 -0.0
CurrentTrain: epoch  2, batch    28 | loss: 14.5927820Losses:  14.503979682922363 6.202197074890137 -0.0
CurrentTrain: epoch  2, batch    29 | loss: 14.5039797Losses:  13.834600448608398 5.945655345916748 -0.0
CurrentTrain: epoch  2, batch    30 | loss: 13.8346004Losses:  15.081205368041992 7.390379905700684 -0.0
CurrentTrain: epoch  2, batch    31 | loss: 15.0812054Losses:  13.770234107971191 5.608685493469238 -0.0
CurrentTrain: epoch  2, batch    32 | loss: 13.7702341Losses:  14.315043449401855 6.807871341705322 -0.0
CurrentTrain: epoch  2, batch    33 | loss: 14.3150434Losses:  16.72199249267578 7.803403854370117 -0.0
CurrentTrain: epoch  2, batch    34 | loss: 16.7219925Losses:  12.204097747802734 4.394698143005371 -0.0
CurrentTrain: epoch  2, batch    35 | loss: 12.2040977Losses:  19.510284423828125 11.705009460449219 -0.0
CurrentTrain: epoch  2, batch    36 | loss: 19.5102844Losses:  10.12875747680664 1.7472134828567505 -0.0
CurrentTrain: epoch  2, batch    37 | loss: 10.1287575Losses:  16.34459686279297 8.253411293029785 -0.0
CurrentTrain: epoch  3, batch     0 | loss: 16.3445969Losses:  18.150949478149414 9.423225402832031 -0.0
CurrentTrain: epoch  3, batch     1 | loss: 18.1509495Losses:  15.50885009765625 8.249053955078125 -0.0
CurrentTrain: epoch  3, batch     2 | loss: 15.5088501Losses:  13.59255599975586 5.3307061195373535 -0.0
CurrentTrain: epoch  3, batch     3 | loss: 13.5925560Losses:  13.963760375976562 5.729874134063721 -0.0
CurrentTrain: epoch  3, batch     4 | loss: 13.9637604Losses:  15.401639938354492 7.6574578285217285 -0.0
CurrentTrain: epoch  3, batch     5 | loss: 15.4016399Losses:  15.098478317260742 6.699832916259766 -0.0
CurrentTrain: epoch  3, batch     6 | loss: 15.0984783Losses:  13.658252716064453 5.632898807525635 -0.0
CurrentTrain: epoch  3, batch     7 | loss: 13.6582527Losses:  19.343446731567383 13.146069526672363 -0.0
CurrentTrain: epoch  3, batch     8 | loss: 19.3434467Losses:  13.796062469482422 5.815038204193115 -0.0
CurrentTrain: epoch  3, batch     9 | loss: 13.7960625Losses:  12.562984466552734 4.923897743225098 -0.0
CurrentTrain: epoch  3, batch    10 | loss: 12.5629845Losses:  14.54136848449707 6.056079864501953 -0.0
CurrentTrain: epoch  3, batch    11 | loss: 14.5413685Losses:  12.350318908691406 4.5013108253479 -0.0
CurrentTrain: epoch  3, batch    12 | loss: 12.3503189Losses:  13.539884567260742 5.806273460388184 -0.0
CurrentTrain: epoch  3, batch    13 | loss: 13.5398846Losses:  15.134303092956543 7.944485187530518 -0.0
CurrentTrain: epoch  3, batch    14 | loss: 15.1343031Losses:  11.816380500793457 4.89443302154541 -0.0
CurrentTrain: epoch  3, batch    15 | loss: 11.8163805Losses:  13.071338653564453 5.4907121658325195 -0.0
CurrentTrain: epoch  3, batch    16 | loss: 13.0713387Losses:  13.131916046142578 6.3762335777282715 -0.0
CurrentTrain: epoch  3, batch    17 | loss: 13.1319160Losses:  12.75617790222168 4.92442512512207 -0.0
CurrentTrain: epoch  3, batch    18 | loss: 12.7561779Losses:  17.359050750732422 8.87640380859375 -0.0
CurrentTrain: epoch  3, batch    19 | loss: 17.3590508Losses:  13.517860412597656 5.5381364822387695 -0.0
CurrentTrain: epoch  3, batch    20 | loss: 13.5178604Losses:  16.95721435546875 8.78988265991211 -0.0
CurrentTrain: epoch  3, batch    21 | loss: 16.9572144Losses:  17.43107032775879 8.521108627319336 -0.0
CurrentTrain: epoch  3, batch    22 | loss: 17.4310703Losses:  12.397242546081543 5.978082180023193 -0.0
CurrentTrain: epoch  3, batch    23 | loss: 12.3972425Losses:  14.66968059539795 7.427609920501709 -0.0
CurrentTrain: epoch  3, batch    24 | loss: 14.6696806Losses:  15.390908241271973 8.262188911437988 -0.0
CurrentTrain: epoch  3, batch    25 | loss: 15.3909082Losses:  14.491342544555664 7.06633186340332 -0.0
CurrentTrain: epoch  3, batch    26 | loss: 14.4913425Losses:  16.166915893554688 8.718839645385742 -0.0
CurrentTrain: epoch  3, batch    27 | loss: 16.1669159Losses:  19.56631088256836 10.884927749633789 -0.0
CurrentTrain: epoch  3, batch    28 | loss: 19.5663109Losses:  17.229778289794922 9.36328411102295 -0.0
CurrentTrain: epoch  3, batch    29 | loss: 17.2297783Losses:  13.038579940795898 6.489373683929443 -0.0
CurrentTrain: epoch  3, batch    30 | loss: 13.0385799Losses:  13.121744155883789 6.260976314544678 -0.0
CurrentTrain: epoch  3, batch    31 | loss: 13.1217442Losses:  19.153425216674805 11.98022174835205 -0.0
CurrentTrain: epoch  3, batch    32 | loss: 19.1534252Losses:  18.290908813476562 10.40106201171875 -0.0
CurrentTrain: epoch  3, batch    33 | loss: 18.2909088Losses:  12.034191131591797 5.487112045288086 -0.0
CurrentTrain: epoch  3, batch    34 | loss: 12.0341911Losses:  11.807337760925293 4.78578519821167 -0.0
CurrentTrain: epoch  3, batch    35 | loss: 11.8073378Losses:  12.984172821044922 5.402146816253662 -0.0
CurrentTrain: epoch  3, batch    36 | loss: 12.9841728Losses:  7.673741340637207 0.9988773465156555 -0.0
CurrentTrain: epoch  3, batch    37 | loss: 7.6737413Losses:  18.04906463623047 10.255348205566406 -0.0
CurrentTrain: epoch  4, batch     0 | loss: 18.0490646Losses:  19.72368812561035 13.855928421020508 -0.0
CurrentTrain: epoch  4, batch     1 | loss: 19.7236881Losses:  17.866056442260742 10.640380859375 -0.0
CurrentTrain: epoch  4, batch     2 | loss: 17.8660564Losses:  14.096014022827148 6.236783027648926 -0.0
CurrentTrain: epoch  4, batch     3 | loss: 14.0960140Losses:  13.155841827392578 5.65919303894043 -0.0
CurrentTrain: epoch  4, batch     4 | loss: 13.1558418Losses:  12.147323608398438 6.265155792236328 -0.0
CurrentTrain: epoch  4, batch     5 | loss: 12.1473236Losses:  12.719062805175781 5.835668563842773 -0.0
CurrentTrain: epoch  4, batch     6 | loss: 12.7190628Losses:  11.07767391204834 4.278957366943359 -0.0
CurrentTrain: epoch  4, batch     7 | loss: 11.0776739Losses:  16.27765655517578 9.279972076416016 -0.0
CurrentTrain: epoch  4, batch     8 | loss: 16.2776566Losses:  12.325002670288086 5.897549629211426 -0.0
CurrentTrain: epoch  4, batch     9 | loss: 12.3250027Losses:  12.151731491088867 5.229986190795898 -0.0
CurrentTrain: epoch  4, batch    10 | loss: 12.1517315Losses:  12.11721420288086 5.219071865081787 -0.0
CurrentTrain: epoch  4, batch    11 | loss: 12.1172142Losses:  11.309934616088867 4.835259437561035 -0.0
CurrentTrain: epoch  4, batch    12 | loss: 11.3099346Losses:  15.817659378051758 10.016929626464844 -0.0
CurrentTrain: epoch  4, batch    13 | loss: 15.8176594Losses:  14.238261222839355 8.254511833190918 -0.0
CurrentTrain: epoch  4, batch    14 | loss: 14.2382612Losses:  10.792984962463379 4.26212215423584 -0.0
CurrentTrain: epoch  4, batch    15 | loss: 10.7929850Losses:  18.61994743347168 10.621622085571289 -0.0
CurrentTrain: epoch  4, batch    16 | loss: 18.6199474Losses:  12.221904754638672 4.727544784545898 -0.0
CurrentTrain: epoch  4, batch    17 | loss: 12.2219048Losses:  12.847278594970703 5.764371395111084 -0.0
CurrentTrain: epoch  4, batch    18 | loss: 12.8472786Losses:  13.452803611755371 6.536008834838867 -0.0
CurrentTrain: epoch  4, batch    19 | loss: 13.4528036Losses:  14.420429229736328 7.133498668670654 -0.0
CurrentTrain: epoch  4, batch    20 | loss: 14.4204292Losses:  13.305849075317383 6.050576210021973 -0.0
CurrentTrain: epoch  4, batch    21 | loss: 13.3058491Losses:  13.581722259521484 7.070414066314697 -0.0
CurrentTrain: epoch  4, batch    22 | loss: 13.5817223Losses:  12.939810752868652 4.677779197692871 -0.0
CurrentTrain: epoch  4, batch    23 | loss: 12.9398108Losses:  11.29693603515625 5.104092597961426 -0.0
CurrentTrain: epoch  4, batch    24 | loss: 11.2969360Losses:  14.533329963684082 6.020751953125 -0.0
CurrentTrain: epoch  4, batch    25 | loss: 14.5333300Losses:  15.270999908447266 7.733572006225586 -0.0
CurrentTrain: epoch  4, batch    26 | loss: 15.2709999Losses:  10.927826881408691 4.610037803649902 -0.0
CurrentTrain: epoch  4, batch    27 | loss: 10.9278269Losses:  14.492874145507812 7.253305435180664 -0.0
CurrentTrain: epoch  4, batch    28 | loss: 14.4928741Losses:  10.72357177734375 4.197305202484131 -0.0
CurrentTrain: epoch  4, batch    29 | loss: 10.7235718Losses:  13.001258850097656 5.752107620239258 -0.0
CurrentTrain: epoch  4, batch    30 | loss: 13.0012589Losses:  19.8441219329834 11.848485946655273 -0.0
CurrentTrain: epoch  4, batch    31 | loss: 19.8441219Losses:  13.779909133911133 6.591497421264648 -0.0
CurrentTrain: epoch  4, batch    32 | loss: 13.7799091Losses:  13.446001052856445 7.096111297607422 -0.0
CurrentTrain: epoch  4, batch    33 | loss: 13.4460011Losses:  12.53683853149414 5.579768180847168 -0.0
CurrentTrain: epoch  4, batch    34 | loss: 12.5368385Losses:  14.206865310668945 6.867766380310059 -0.0
CurrentTrain: epoch  4, batch    35 | loss: 14.2068653Losses:  14.710094451904297 7.222295761108398 -0.0
CurrentTrain: epoch  4, batch    36 | loss: 14.7100945Losses:  10.426657676696777 2.675586462020874 -0.0
CurrentTrain: epoch  4, batch    37 | loss: 10.4266577Losses:  10.980344772338867 4.953469276428223 -0.0
CurrentTrain: epoch  5, batch     0 | loss: 10.9803448Losses:  11.21697998046875 4.865384101867676 -0.0
CurrentTrain: epoch  5, batch     1 | loss: 11.2169800Losses:  19.50900650024414 11.61043643951416 -0.0
CurrentTrain: epoch  5, batch     2 | loss: 19.5090065Losses:  12.380704879760742 5.270749092102051 -0.0
CurrentTrain: epoch  5, batch     3 | loss: 12.3807049Losses:  11.722108840942383 5.099515438079834 -0.0
CurrentTrain: epoch  5, batch     4 | loss: 11.7221088Losses:  11.407686233520508 5.295717239379883 -0.0
CurrentTrain: epoch  5, batch     5 | loss: 11.4076862Losses:  11.544243812561035 5.352667808532715 -0.0
CurrentTrain: epoch  5, batch     6 | loss: 11.5442438Losses:  12.66677188873291 6.858349800109863 -0.0
CurrentTrain: epoch  5, batch     7 | loss: 12.6667719Losses:  11.801168441772461 4.796207427978516 -0.0
CurrentTrain: epoch  5, batch     8 | loss: 11.8011684Losses:  12.461878776550293 5.7512383460998535 -0.0
CurrentTrain: epoch  5, batch     9 | loss: 12.4618788Losses:  11.832206726074219 5.580278396606445 -0.0
CurrentTrain: epoch  5, batch    10 | loss: 11.8322067Losses:  11.446159362792969 5.451300621032715 -0.0
CurrentTrain: epoch  5, batch    11 | loss: 11.4461594Losses:  12.783210754394531 6.746376037597656 -0.0
CurrentTrain: epoch  5, batch    12 | loss: 12.7832108Losses:  15.281625747680664 9.579936981201172 -0.0
CurrentTrain: epoch  5, batch    13 | loss: 15.2816257Losses:  14.711344718933105 9.103221893310547 -0.0
CurrentTrain: epoch  5, batch    14 | loss: 14.7113447Losses:  12.114439010620117 5.745706081390381 -0.0
CurrentTrain: epoch  5, batch    15 | loss: 12.1144390Losses:  10.913960456848145 5.44450569152832 -0.0
CurrentTrain: epoch  5, batch    16 | loss: 10.9139605Losses:  11.822330474853516 4.966570854187012 -0.0
CurrentTrain: epoch  5, batch    17 | loss: 11.8223305Losses:  12.03634262084961 6.30025577545166 -0.0
CurrentTrain: epoch  5, batch    18 | loss: 12.0363426Losses:  11.571619987487793 4.751343250274658 -0.0
CurrentTrain: epoch  5, batch    19 | loss: 11.5716200Losses:  11.569381713867188 5.168529510498047 -0.0
CurrentTrain: epoch  5, batch    20 | loss: 11.5693817Losses:  15.106441497802734 7.854861259460449 -0.0
CurrentTrain: epoch  5, batch    21 | loss: 15.1064415Losses:  17.424293518066406 10.481842041015625 -0.0
CurrentTrain: epoch  5, batch    22 | loss: 17.4242935Losses:  20.018495559692383 13.334839820861816 -0.0
CurrentTrain: epoch  5, batch    23 | loss: 20.0184956Losses:  14.556249618530273 7.131614685058594 -0.0
CurrentTrain: epoch  5, batch    24 | loss: 14.5562496Losses:  10.80601978302002 4.853213787078857 -0.0
CurrentTrain: epoch  5, batch    25 | loss: 10.8060198Losses:  11.498736381530762 5.141384124755859 -0.0
CurrentTrain: epoch  5, batch    26 | loss: 11.4987364Losses:  10.170042037963867 4.449744701385498 -0.0
CurrentTrain: epoch  5, batch    27 | loss: 10.1700420Losses:  13.506587982177734 6.8451666831970215 -0.0
CurrentTrain: epoch  5, batch    28 | loss: 13.5065880Losses:  13.698505401611328 7.168614864349365 -0.0
CurrentTrain: epoch  5, batch    29 | loss: 13.6985054Losses:  12.247700691223145 7.202187538146973 -0.0
CurrentTrain: epoch  5, batch    30 | loss: 12.2477007Losses:  14.031729698181152 7.292881488800049 -0.0
CurrentTrain: epoch  5, batch    31 | loss: 14.0317297Losses:  11.343618392944336 4.992852687835693 -0.0
CurrentTrain: epoch  5, batch    32 | loss: 11.3436184Losses:  11.985832214355469 5.7019758224487305 -0.0
CurrentTrain: epoch  5, batch    33 | loss: 11.9858322Losses:  15.532402992248535 8.873821258544922 -0.0
CurrentTrain: epoch  5, batch    34 | loss: 15.5324030Losses:  11.387447357177734 4.888213634490967 -0.0
CurrentTrain: epoch  5, batch    35 | loss: 11.3874474Losses:  13.16712760925293 5.9809441566467285 -0.0
CurrentTrain: epoch  5, batch    36 | loss: 13.1671276Losses:  10.098569869995117 3.3270840644836426 -0.0
CurrentTrain: epoch  5, batch    37 | loss: 10.0985699Losses:  14.545230865478516 7.9472880363464355 -0.0
CurrentTrain: epoch  6, batch     0 | loss: 14.5452309Losses:  10.421993255615234 4.729170322418213 -0.0
CurrentTrain: epoch  6, batch     1 | loss: 10.4219933Losses:  12.339174270629883 6.405671119689941 -0.0
CurrentTrain: epoch  6, batch     2 | loss: 12.3391743Losses:  13.347312927246094 6.807785511016846 -0.0
CurrentTrain: epoch  6, batch     3 | loss: 13.3473129Losses:  12.288320541381836 6.2510905265808105 -0.0
CurrentTrain: epoch  6, batch     4 | loss: 12.2883205Losses:  14.839754104614258 9.38170337677002 -0.0
CurrentTrain: epoch  6, batch     5 | loss: 14.8397541Losses:  12.42629623413086 6.531991004943848 -0.0
CurrentTrain: epoch  6, batch     6 | loss: 12.4262962Losses:  10.002826690673828 4.13157844543457 -0.0
CurrentTrain: epoch  6, batch     7 | loss: 10.0028267Losses:  11.297785758972168 5.792820930480957 -0.0
CurrentTrain: epoch  6, batch     8 | loss: 11.2977858Losses:  17.223987579345703 10.082937240600586 -0.0
CurrentTrain: epoch  6, batch     9 | loss: 17.2239876Losses:  16.611848831176758 9.783574104309082 -0.0
CurrentTrain: epoch  6, batch    10 | loss: 16.6118488Losses:  11.620994567871094 5.3607869148254395 -0.0
CurrentTrain: epoch  6, batch    11 | loss: 11.6209946Losses:  10.4639310836792 4.635085105895996 -0.0
CurrentTrain: epoch  6, batch    12 | loss: 10.4639311Losses:  12.616965293884277 6.924003601074219 -0.0
CurrentTrain: epoch  6, batch    13 | loss: 12.6169653Losses:  13.534409523010254 6.987646102905273 -0.0
CurrentTrain: epoch  6, batch    14 | loss: 13.5344095Losses:  11.70811939239502 5.493593215942383 -0.0
CurrentTrain: epoch  6, batch    15 | loss: 11.7081194Losses:  11.07027530670166 4.793334007263184 -0.0
CurrentTrain: epoch  6, batch    16 | loss: 11.0702753Losses:  11.165279388427734 5.215756416320801 -0.0
CurrentTrain: epoch  6, batch    17 | loss: 11.1652794Losses:  11.798248291015625 5.433493137359619 -0.0
CurrentTrain: epoch  6, batch    18 | loss: 11.7982483Losses:  11.297237396240234 5.338247299194336 -0.0
CurrentTrain: epoch  6, batch    19 | loss: 11.2972374Losses:  10.346668243408203 4.774691581726074 -0.0
CurrentTrain: epoch  6, batch    20 | loss: 10.3466682Losses:  11.863683700561523 4.8288655281066895 -0.0
CurrentTrain: epoch  6, batch    21 | loss: 11.8636837Losses:  11.974808692932129 4.796338081359863 -0.0
CurrentTrain: epoch  6, batch    22 | loss: 11.9748087Losses:  13.227163314819336 7.746208190917969 -0.0
CurrentTrain: epoch  6, batch    23 | loss: 13.2271633Losses:  10.583403587341309 4.729611396789551 -0.0
CurrentTrain: epoch  6, batch    24 | loss: 10.5834036Losses:  12.204584121704102 6.130450248718262 -0.0
CurrentTrain: epoch  6, batch    25 | loss: 12.2045841Losses:  13.390279769897461 7.604016304016113 -0.0
CurrentTrain: epoch  6, batch    26 | loss: 13.3902798Losses:  13.076623916625977 6.639393329620361 -0.0
CurrentTrain: epoch  6, batch    27 | loss: 13.0766239Losses:  10.885686874389648 5.170785427093506 -0.0
CurrentTrain: epoch  6, batch    28 | loss: 10.8856869Losses:  11.049978256225586 5.408221244812012 -0.0
CurrentTrain: epoch  6, batch    29 | loss: 11.0499783Losses:  12.774385452270508 7.479518890380859 -0.0
CurrentTrain: epoch  6, batch    30 | loss: 12.7743855Losses:  14.879827499389648 8.491135597229004 -0.0
CurrentTrain: epoch  6, batch    31 | loss: 14.8798275Losses:  10.602602005004883 4.153351306915283 -0.0
CurrentTrain: epoch  6, batch    32 | loss: 10.6026020Losses:  12.650955200195312 5.458128929138184 -0.0
CurrentTrain: epoch  6, batch    33 | loss: 12.6509552Losses:  12.495389938354492 5.910315990447998 -0.0
CurrentTrain: epoch  6, batch    34 | loss: 12.4953899Losses:  13.827192306518555 8.096566200256348 -0.0
CurrentTrain: epoch  6, batch    35 | loss: 13.8271923Losses:  9.312731742858887 3.589165210723877 -0.0
CurrentTrain: epoch  6, batch    36 | loss: 9.3127317Losses:  6.877538204193115 1.3469605445861816 -0.0
CurrentTrain: epoch  6, batch    37 | loss: 6.8775382Losses:  13.605907440185547 6.689373016357422 -0.0
CurrentTrain: epoch  7, batch     0 | loss: 13.6059074Losses:  10.737197875976562 3.8132238388061523 -0.0
CurrentTrain: epoch  7, batch     1 | loss: 10.7371979Losses:  16.909860610961914 9.453518867492676 -0.0
CurrentTrain: epoch  7, batch     2 | loss: 16.9098606Losses:  14.569501876831055 7.914140701293945 -0.0
CurrentTrain: epoch  7, batch     3 | loss: 14.5695019Losses:  12.159862518310547 6.184926509857178 -0.0
CurrentTrain: epoch  7, batch     4 | loss: 12.1598625Losses:  13.465865135192871 7.21357536315918 -0.0
CurrentTrain: epoch  7, batch     5 | loss: 13.4658651Losses:  9.934831619262695 3.9799070358276367 -0.0
CurrentTrain: epoch  7, batch     6 | loss: 9.9348316Losses:  9.574858665466309 3.988579273223877 -0.0
CurrentTrain: epoch  7, batch     7 | loss: 9.5748587Losses:  12.093069076538086 5.552572250366211 -0.0
CurrentTrain: epoch  7, batch     8 | loss: 12.0930691Losses:  13.608209609985352 7.475711822509766 -0.0
CurrentTrain: epoch  7, batch     9 | loss: 13.6082096Losses:  11.079318046569824 4.148340225219727 -0.0
CurrentTrain: epoch  7, batch    10 | loss: 11.0793180Losses:  11.882916450500488 5.457594394683838 -0.0
CurrentTrain: epoch  7, batch    11 | loss: 11.8829165Losses:  12.364874839782715 6.276891708374023 -0.0
CurrentTrain: epoch  7, batch    12 | loss: 12.3648748Losses:  11.69509506225586 6.04909610748291 -0.0
CurrentTrain: epoch  7, batch    13 | loss: 11.6950951Losses:  12.245012283325195 6.538939476013184 -0.0
CurrentTrain: epoch  7, batch    14 | loss: 12.2450123Losses:  12.919408798217773 6.676462650299072 -0.0
CurrentTrain: epoch  7, batch    15 | loss: 12.9194088Losses:  11.010702133178711 5.211306571960449 -0.0
CurrentTrain: epoch  7, batch    16 | loss: 11.0107021Losses:  15.278366088867188 8.063581466674805 -0.0
CurrentTrain: epoch  7, batch    17 | loss: 15.2783661Losses:  12.931802749633789 7.285665035247803 -0.0
CurrentTrain: epoch  7, batch    18 | loss: 12.9318027Losses:  9.691335678100586 3.9825549125671387 -0.0
CurrentTrain: epoch  7, batch    19 | loss: 9.6913357Losses:  10.176019668579102 4.583617210388184 -0.0
CurrentTrain: epoch  7, batch    20 | loss: 10.1760197Losses:  10.59788703918457 5.181692600250244 -0.0
CurrentTrain: epoch  7, batch    21 | loss: 10.5978870Losses:  13.72163200378418 6.923336029052734 -0.0
CurrentTrain: epoch  7, batch    22 | loss: 13.7216320Losses:  18.08002471923828 10.77983283996582 -0.0
CurrentTrain: epoch  7, batch    23 | loss: 18.0800247Losses:  12.63144302368164 5.637669086456299 -0.0
CurrentTrain: epoch  7, batch    24 | loss: 12.6314430Losses:  10.349513053894043 4.5811004638671875 -0.0
CurrentTrain: epoch  7, batch    25 | loss: 10.3495131Losses:  9.311644554138184 3.874823570251465 -0.0
CurrentTrain: epoch  7, batch    26 | loss: 9.3116446Losses:  11.710957527160645 6.4808149337768555 -0.0
CurrentTrain: epoch  7, batch    27 | loss: 11.7109575Losses:  10.010891914367676 4.221550941467285 -0.0
CurrentTrain: epoch  7, batch    28 | loss: 10.0108919Losses:  15.593984603881836 8.398580551147461 -0.0
CurrentTrain: epoch  7, batch    29 | loss: 15.5939846Losses:  15.476454734802246 9.64824104309082 -0.0
CurrentTrain: epoch  7, batch    30 | loss: 15.4764547Losses:  10.96692180633545 5.314903259277344 -0.0
CurrentTrain: epoch  7, batch    31 | loss: 10.9669218Losses:  10.1063871383667 4.793152809143066 -0.0
CurrentTrain: epoch  7, batch    32 | loss: 10.1063871Losses:  10.722146987915039 5.315474987030029 -0.0
CurrentTrain: epoch  7, batch    33 | loss: 10.7221470Losses:  9.391519546508789 4.008477210998535 -0.0
CurrentTrain: epoch  7, batch    34 | loss: 9.3915195Losses:  9.953357696533203 4.975669860839844 -0.0
CurrentTrain: epoch  7, batch    35 | loss: 9.9533577Losses:  10.365348815917969 4.709165573120117 -0.0
CurrentTrain: epoch  7, batch    36 | loss: 10.3653488Losses:  6.563973426818848 1.4348938465118408 -0.0
CurrentTrain: epoch  7, batch    37 | loss: 6.5639734Losses:  10.614093780517578 5.099123001098633 -0.0
CurrentTrain: epoch  8, batch     0 | loss: 10.6140938Losses:  10.92551326751709 5.384237766265869 -0.0
CurrentTrain: epoch  8, batch     1 | loss: 10.9255133Losses:  9.443428039550781 4.385760307312012 -0.0
CurrentTrain: epoch  8, batch     2 | loss: 9.4434280Losses:  9.992658615112305 4.950288772583008 -0.0
CurrentTrain: epoch  8, batch     3 | loss: 9.9926586Losses:  9.400677680969238 4.357644081115723 -0.0
CurrentTrain: epoch  8, batch     4 | loss: 9.4006777Losses:  13.23147964477539 7.9800944328308105 -0.0
CurrentTrain: epoch  8, batch     5 | loss: 13.2314796Losses:  9.958013534545898 4.9901275634765625 -0.0
CurrentTrain: epoch  8, batch     6 | loss: 9.9580135Losses:  13.299193382263184 7.564123630523682 -0.0
CurrentTrain: epoch  8, batch     7 | loss: 13.2991934Losses:  9.080023765563965 4.052425384521484 -0.0
CurrentTrain: epoch  8, batch     8 | loss: 9.0800238Losses:  10.363195419311523 4.614315509796143 -0.0
CurrentTrain: epoch  8, batch     9 | loss: 10.3631954Losses:  10.421996116638184 5.0485124588012695 -0.0
CurrentTrain: epoch  8, batch    10 | loss: 10.4219961Losses:  10.734556198120117 5.575319766998291 -0.0
CurrentTrain: epoch  8, batch    11 | loss: 10.7345562Losses:  13.511184692382812 7.319250583648682 -0.0
CurrentTrain: epoch  8, batch    12 | loss: 13.5111847Losses:  13.865163803100586 9.182134628295898 -0.0
CurrentTrain: epoch  8, batch    13 | loss: 13.8651638Losses:  11.36259651184082 5.8576130867004395 -0.0
CurrentTrain: epoch  8, batch    14 | loss: 11.3625965Losses:  9.182404518127441 3.923210859298706 -0.0
CurrentTrain: epoch  8, batch    15 | loss: 9.1824045Losses:  14.628353118896484 7.5906147956848145 -0.0
CurrentTrain: epoch  8, batch    16 | loss: 14.6283531Losses:  12.988752365112305 7.546363353729248 -0.0
CurrentTrain: epoch  8, batch    17 | loss: 12.9887524Losses:  15.066097259521484 8.545466423034668 -0.0
CurrentTrain: epoch  8, batch    18 | loss: 15.0660973Losses:  13.06363582611084 6.337508201599121 -0.0
CurrentTrain: epoch  8, batch    19 | loss: 13.0636358Losses:  10.32149887084961 5.031947135925293 -0.0
CurrentTrain: epoch  8, batch    20 | loss: 10.3214989Losses:  10.337287902832031 5.1402177810668945 -0.0
CurrentTrain: epoch  8, batch    21 | loss: 10.3372879Losses:  13.268726348876953 8.09737777709961 -0.0
CurrentTrain: epoch  8, batch    22 | loss: 13.2687263Losses:  9.475898742675781 4.230957984924316 -0.0
CurrentTrain: epoch  8, batch    23 | loss: 9.4758987Losses:  11.172178268432617 5.585017681121826 -0.0
CurrentTrain: epoch  8, batch    24 | loss: 11.1721783Losses:  12.777502059936523 7.220571517944336 -0.0
CurrentTrain: epoch  8, batch    25 | loss: 12.7775021Losses:  10.067472457885742 4.604952335357666 -0.0
CurrentTrain: epoch  8, batch    26 | loss: 10.0674725Losses:  9.979232788085938 4.018728256225586 -0.0
CurrentTrain: epoch  8, batch    27 | loss: 9.9792328Losses:  10.54672908782959 4.385836601257324 -0.0
CurrentTrain: epoch  8, batch    28 | loss: 10.5467291Losses:  10.886791229248047 5.662410259246826 -0.0
CurrentTrain: epoch  8, batch    29 | loss: 10.8867912Losses:  11.569160461425781 5.901655197143555 -0.0
CurrentTrain: epoch  8, batch    30 | loss: 11.5691605Losses:  9.357075691223145 3.829864501953125 -0.0
CurrentTrain: epoch  8, batch    31 | loss: 9.3570757Losses:  12.031824111938477 6.07513427734375 -0.0
CurrentTrain: epoch  8, batch    32 | loss: 12.0318241Losses:  12.699380874633789 7.158829689025879 -0.0
CurrentTrain: epoch  8, batch    33 | loss: 12.6993809Losses:  10.692153930664062 4.135068893432617 -0.0
CurrentTrain: epoch  8, batch    34 | loss: 10.6921539Losses:  13.909422874450684 7.759640693664551 -0.0
CurrentTrain: epoch  8, batch    35 | loss: 13.9094229Losses:  10.145907402038574 4.200470924377441 -0.0
CurrentTrain: epoch  8, batch    36 | loss: 10.1459074Losses:  7.665738105773926 1.5687363147735596 -0.0
CurrentTrain: epoch  8, batch    37 | loss: 7.6657381Losses:  11.50166130065918 5.611237525939941 -0.0
CurrentTrain: epoch  9, batch     0 | loss: 11.5016613Losses:  10.832319259643555 5.4937357902526855 -0.0
CurrentTrain: epoch  9, batch     1 | loss: 10.8323193Losses:  11.33782958984375 5.280879974365234 -0.0
CurrentTrain: epoch  9, batch     2 | loss: 11.3378296Losses:  11.305000305175781 5.215164661407471 -0.0
CurrentTrain: epoch  9, batch     3 | loss: 11.3050003Losses:  8.962209701538086 3.8361449241638184 -0.0
CurrentTrain: epoch  9, batch     4 | loss: 8.9622097Losses:  12.08652114868164 6.118343353271484 -0.0
CurrentTrain: epoch  9, batch     5 | loss: 12.0865211Losses:  12.496126174926758 7.016848087310791 -0.0
CurrentTrain: epoch  9, batch     6 | loss: 12.4961262Losses:  11.915831565856934 6.084040641784668 -0.0
CurrentTrain: epoch  9, batch     7 | loss: 11.9158316Losses:  8.606884002685547 3.5281825065612793 -0.0
CurrentTrain: epoch  9, batch     8 | loss: 8.6068840Losses:  12.0401611328125 6.634336471557617 -0.0
CurrentTrain: epoch  9, batch     9 | loss: 12.0401611Losses:  12.964022636413574 7.275447845458984 -0.0
CurrentTrain: epoch  9, batch    10 | loss: 12.9640226Losses:  11.089046478271484 5.501214981079102 -0.0
CurrentTrain: epoch  9, batch    11 | loss: 11.0890465Losses:  11.260444641113281 5.3955793380737305 -0.0
CurrentTrain: epoch  9, batch    12 | loss: 11.2604446Losses:  10.991151809692383 5.490755558013916 -0.0
CurrentTrain: epoch  9, batch    13 | loss: 10.9911518Losses:  14.526371002197266 9.454927444458008 -0.0
CurrentTrain: epoch  9, batch    14 | loss: 14.5263710Losses:  9.430633544921875 4.199582099914551 -0.0
CurrentTrain: epoch  9, batch    15 | loss: 9.4306335Losses:  9.933892250061035 4.941469192504883 -0.0
CurrentTrain: epoch  9, batch    16 | loss: 9.9338923Losses:  10.179410934448242 5.031956672668457 -0.0
CurrentTrain: epoch  9, batch    17 | loss: 10.1794109Losses:  10.666353225708008 5.488871097564697 -0.0
CurrentTrain: epoch  9, batch    18 | loss: 10.6663532Losses:  9.982095718383789 4.969634532928467 -0.0
CurrentTrain: epoch  9, batch    19 | loss: 9.9820957Losses:  11.80305290222168 6.721166610717773 -0.0
CurrentTrain: epoch  9, batch    20 | loss: 11.8030529Losses:  8.433357238769531 3.440049648284912 -0.0
CurrentTrain: epoch  9, batch    21 | loss: 8.4333572Losses:  10.831284523010254 5.849286079406738 -0.0
CurrentTrain: epoch  9, batch    22 | loss: 10.8312845Losses:  9.670755386352539 4.757367134094238 -0.0
CurrentTrain: epoch  9, batch    23 | loss: 9.6707554Losses:  11.284505844116211 5.182004928588867 -0.0
CurrentTrain: epoch  9, batch    24 | loss: 11.2845058Losses:  11.732911109924316 6.101983070373535 -0.0
CurrentTrain: epoch  9, batch    25 | loss: 11.7329111Losses:  11.179256439208984 5.9855828285217285 -0.0
CurrentTrain: epoch  9, batch    26 | loss: 11.1792564Losses:  12.317421913146973 7.368300437927246 -0.0
CurrentTrain: epoch  9, batch    27 | loss: 12.3174219Losses:  11.372222900390625 5.600307464599609 -0.0
CurrentTrain: epoch  9, batch    28 | loss: 11.3722229Losses:  11.378389358520508 5.456734657287598 -0.0
CurrentTrain: epoch  9, batch    29 | loss: 11.3783894Losses:  11.480703353881836 5.847607612609863 -0.0
CurrentTrain: epoch  9, batch    30 | loss: 11.4807034Losses:  9.129454612731934 4.3976287841796875 -0.0
CurrentTrain: epoch  9, batch    31 | loss: 9.1294546Losses:  11.526750564575195 6.495029449462891 -0.0
CurrentTrain: epoch  9, batch    32 | loss: 11.5267506Losses:  10.516681671142578 4.515424728393555 -0.0
CurrentTrain: epoch  9, batch    33 | loss: 10.5166817Losses:  9.774068832397461 4.673921585083008 -0.0
CurrentTrain: epoch  9, batch    34 | loss: 9.7740688Losses:  9.738199234008789 4.706989288330078 -0.0
CurrentTrain: epoch  9, batch    35 | loss: 9.7381992Losses:  16.768714904785156 11.885415077209473 -0.0
CurrentTrain: epoch  9, batch    36 | loss: 16.7687149Losses:  8.303098678588867 2.3430683612823486 -0.0
CurrentTrain: epoch  9, batch    37 | loss: 8.3030987
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 82.81%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 83.75%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 82.29%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 83.93%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 85.94%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 88.12%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 89.20%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 89.90%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 88.39%   [EVAL] batch:   14 | acc: 62.50%,  total acc: 86.67%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 84.77%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 84.19%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 82.99%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 81.91%   [EVAL] batch:   19 | acc: 68.75%,  total acc: 81.25%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 82.14%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 82.95%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 83.70%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 84.38%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 85.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 85.58%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 85.88%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 86.38%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 86.85%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 86.67%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 86.90%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 87.30%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 85.80%   
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 82.81%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 83.75%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 82.29%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 83.93%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 85.94%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 88.12%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 89.20%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 89.90%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 88.39%   [EVAL] batch:   14 | acc: 62.50%,  total acc: 86.67%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 84.77%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 84.19%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 82.99%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 81.91%   [EVAL] batch:   19 | acc: 68.75%,  total acc: 81.25%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 82.14%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 82.95%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 83.70%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 84.38%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 85.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 85.58%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 85.88%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 86.38%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 86.85%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 86.67%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 86.90%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 87.30%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 85.80%   
cur_acc:  ['0.8580']
his_acc:  ['0.8580']
Clustering into  4  clusters
Clusters:  [1 0 2 1 0 1 1 3 1 0 0]
Losses:  16.809972763061523 8.888437271118164 1.458427906036377
CurrentTrain: epoch  0, batch     0 | loss: 16.8099728Losses:  14.346392631530762 5.239161014556885 1.4810733795166016
CurrentTrain: epoch  0, batch     1 | loss: 14.3463926Losses:  17.01348114013672 9.462331771850586 1.5008058547973633
CurrentTrain: epoch  1, batch     0 | loss: 17.0134811Losses:  12.59136962890625 5.0870561599731445 1.4029138088226318
CurrentTrain: epoch  1, batch     1 | loss: 12.5913696Losses:  15.37407112121582 8.880488395690918 1.3887193202972412
CurrentTrain: epoch  2, batch     0 | loss: 15.3740711Losses:  10.151264190673828 3.6970629692077637 1.4180797338485718
CurrentTrain: epoch  2, batch     1 | loss: 10.1512642Losses:  14.523820877075195 8.298600196838379 1.387176275253296
CurrentTrain: epoch  3, batch     0 | loss: 14.5238209Losses:  10.255280494689941 4.146061420440674 1.4196984767913818
CurrentTrain: epoch  3, batch     1 | loss: 10.2552805Losses:  12.017127990722656 6.639724254608154 1.3888659477233887
CurrentTrain: epoch  4, batch     0 | loss: 12.0171280Losses:  6.470099925994873 1.4491394758224487 1.4089120626449585
CurrentTrain: epoch  4, batch     1 | loss: 6.4700999Losses:  11.475053787231445 6.618756294250488 1.3999828100204468
CurrentTrain: epoch  5, batch     0 | loss: 11.4750538Losses:  7.267561435699463 2.10701847076416 1.3874744176864624
CurrentTrain: epoch  5, batch     1 | loss: 7.2675614Losses:  11.721174240112305 6.596802234649658 1.393031120300293
CurrentTrain: epoch  6, batch     0 | loss: 11.7211742Losses:  7.076975345611572 2.2428157329559326 1.412762999534607
CurrentTrain: epoch  6, batch     1 | loss: 7.0769753Losses:  10.815767288208008 6.119233131408691 1.3900458812713623
CurrentTrain: epoch  7, batch     0 | loss: 10.8157673Losses:  5.572464942932129 1.3815888166427612 1.389464259147644
CurrentTrain: epoch  7, batch     1 | loss: 5.5724649Losses:  11.837347030639648 6.769714832305908 1.3951513767242432
CurrentTrain: epoch  8, batch     0 | loss: 11.8373470Losses:  6.806504726409912 1.9151302576065063 1.3880615234375
CurrentTrain: epoch  8, batch     1 | loss: 6.8065047Losses:  13.959158897399902 8.667362213134766 1.3916658163070679
CurrentTrain: epoch  9, batch     0 | loss: 13.9591589Losses:  7.78608512878418 4.035328388214111 1.406293272972107
CurrentTrain: epoch  9, batch     1 | loss: 7.7860851
Losses:  6.453963279724121 -0.0 4.196227073669434
MemoryTrain:  epoch  0, batch     0 | loss: 6.4539633Losses:  6.485761642456055 -0.0 4.1167893409729
MemoryTrain:  epoch  1, batch     0 | loss: 6.4857616Losses:  5.6967010498046875 -0.0 3.955193281173706
MemoryTrain:  epoch  2, batch     0 | loss: 5.6967010Losses:  4.984230041503906 -0.0 3.9590253829956055
MemoryTrain:  epoch  3, batch     0 | loss: 4.9842300Losses:  4.175258159637451 -0.0 3.5442545413970947
MemoryTrain:  epoch  4, batch     0 | loss: 4.1752582Losses:  3.797654151916504 -0.0 3.5109243392944336
MemoryTrain:  epoch  5, batch     0 | loss: 3.7976542Losses:  3.563398599624634 -0.0 3.443317413330078
MemoryTrain:  epoch  6, batch     0 | loss: 3.5633986Losses:  3.516890287399292 -0.0 3.430722951889038
MemoryTrain:  epoch  7, batch     0 | loss: 3.5168903Losses:  3.660149335861206 -0.0 3.42451548576355
MemoryTrain:  epoch  8, batch     0 | loss: 3.6601493Losses:  3.5554089546203613 -0.0 3.410691499710083
MemoryTrain:  epoch  9, batch     0 | loss: 3.5554090
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    1 | acc: 75.00%,  total acc: 68.75%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 70.83%   [EVAL] batch:    3 | acc: 50.00%,  total acc: 65.62%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 70.00%   [EVAL] batch:    5 | acc: 56.25%,  total acc: 67.71%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 70.54%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 73.44%   [EVAL] batch:    8 | acc: 62.50%,  total acc: 72.22%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 73.75%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 75.00%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 77.08%   [EVAL] batch:   12 | acc: 100.00%,  total acc: 78.85%   [EVAL] batch:   13 | acc: 100.00%,  total acc: 80.36%   [EVAL] batch:   14 | acc: 100.00%,  total acc: 81.67%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 82.81%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 83.82%   [EVAL] batch:   17 | acc: 25.00%,  total acc: 80.56%   
[EVAL] batch:    0 | acc: 56.25%,  total acc: 56.25%   [EVAL] batch:    1 | acc: 56.25%,  total acc: 56.25%   [EVAL] batch:    2 | acc: 62.50%,  total acc: 58.33%   [EVAL] batch:    3 | acc: 31.25%,  total acc: 51.56%   [EVAL] batch:    4 | acc: 62.50%,  total acc: 53.75%   [EVAL] batch:    5 | acc: 56.25%,  total acc: 54.17%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 58.04%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 63.28%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 67.36%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 70.00%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 72.73%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 73.96%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 75.00%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 74.55%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 74.58%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 73.44%   [EVAL] batch:   16 | acc: 81.25%,  total acc: 73.90%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 73.26%   [EVAL] batch:   18 | acc: 81.25%,  total acc: 73.68%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 74.38%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 75.60%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 76.70%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 77.72%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 78.65%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 79.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 80.29%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 80.79%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 81.47%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 82.11%   [EVAL] batch:   29 | acc: 93.75%,  total acc: 82.50%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 83.06%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 83.59%   [EVAL] batch:   32 | acc: 75.00%,  total acc: 83.33%   [EVAL] batch:   33 | acc: 68.75%,  total acc: 82.90%   [EVAL] batch:   34 | acc: 68.75%,  total acc: 82.50%   [EVAL] batch:   35 | acc: 68.75%,  total acc: 82.12%   [EVAL] batch:   36 | acc: 68.75%,  total acc: 81.76%   [EVAL] batch:   37 | acc: 68.75%,  total acc: 81.41%   [EVAL] batch:   38 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 81.72%   [EVAL] batch:   40 | acc: 62.50%,  total acc: 81.25%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 81.40%   [EVAL] batch:   42 | acc: 87.50%,  total acc: 81.54%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 81.82%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 82.22%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 82.61%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 82.98%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 83.33%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 83.67%   [EVAL] batch:   49 | acc: 62.50%,  total acc: 83.25%   
cur_acc:  ['0.8580', '0.8056']
his_acc:  ['0.8580', '0.8325']
Clustering into  7  clusters
Clusters:  [1 3 5 1 6 1 1 2 1 0 4 0 1 0 3 1]
Losses:  15.788068771362305 7.5827484130859375 1.4220397472381592
CurrentTrain: epoch  0, batch     0 | loss: 15.7880688Losses:  11.557145118713379 2.252572774887085 1.3954023122787476
CurrentTrain: epoch  0, batch     1 | loss: 11.5571451Losses:  15.144095420837402 7.468603134155273 1.398543357849121
CurrentTrain: epoch  1, batch     0 | loss: 15.1440954Losses:  9.243298530578613 2.8002543449401855 1.3951085805892944
CurrentTrain: epoch  1, batch     1 | loss: 9.2432985Losses:  13.592452049255371 7.60222864151001 1.415014624595642
CurrentTrain: epoch  2, batch     0 | loss: 13.5924520Losses:  8.401845932006836 2.589162588119507 1.391031265258789
CurrentTrain: epoch  2, batch     1 | loss: 8.4018459Losses:  12.880733489990234 7.498859882354736 1.4339864253997803
CurrentTrain: epoch  3, batch     0 | loss: 12.8807335Losses:  7.5198974609375 2.4147839546203613 1.4117648601531982
CurrentTrain: epoch  3, batch     1 | loss: 7.5198975Losses:  12.173978805541992 7.065974712371826 1.3915719985961914
CurrentTrain: epoch  4, batch     0 | loss: 12.1739788Losses:  7.526159286499023 2.402757406234741 1.4074389934539795
CurrentTrain: epoch  4, batch     1 | loss: 7.5261593Losses:  12.380062103271484 7.845704555511475 1.3905067443847656
CurrentTrain: epoch  5, batch     0 | loss: 12.3800621Losses:  7.328297138214111 2.919156312942505 1.414608359336853
CurrentTrain: epoch  5, batch     1 | loss: 7.3282971Losses:  12.081286430358887 7.672422885894775 1.432420015335083
CurrentTrain: epoch  6, batch     0 | loss: 12.0812864Losses:  6.016360282897949 3.3077187538146973 -0.0
CurrentTrain: epoch  6, batch     1 | loss: 6.0163603Losses:  10.84935188293457 6.828773021697998 1.4208953380584717
CurrentTrain: epoch  7, batch     0 | loss: 10.8493519Losses:  6.806447982788086 2.6305859088897705 1.416541337966919
CurrentTrain: epoch  7, batch     1 | loss: 6.8064480Losses:  10.372136116027832 6.392935276031494 1.4305336475372314
CurrentTrain: epoch  8, batch     0 | loss: 10.3721361Losses:  5.8038411140441895 1.9077178239822388 1.410919189453125
CurrentTrain: epoch  8, batch     1 | loss: 5.8038411Losses:  9.663848876953125 6.06464147567749 1.3925704956054688
CurrentTrain: epoch  9, batch     0 | loss: 9.6638489Losses:  5.73886251449585 2.0491960048675537 1.4566892385482788
CurrentTrain: epoch  9, batch     1 | loss: 5.7388625
Losses:  12.138021469116211 -0.0 11.037096977233887
MemoryTrain:  epoch  0, batch     0 | loss: 12.1380215Losses:  12.326766014099121 -0.0 11.029507637023926
MemoryTrain:  epoch  1, batch     0 | loss: 12.3267660Losses:  11.879701614379883 -0.0 10.992181777954102
MemoryTrain:  epoch  2, batch     0 | loss: 11.8797016Losses:  11.557021141052246 -0.0 10.933064460754395
MemoryTrain:  epoch  3, batch     0 | loss: 11.5570211Losses:  11.700569152832031 -0.0 10.954428672790527
MemoryTrain:  epoch  4, batch     0 | loss: 11.7005692Losses:  11.40810775756836 -0.0 10.897153854370117
MemoryTrain:  epoch  5, batch     0 | loss: 11.4081078Losses:  11.181876182556152 -0.0 10.951752662658691
MemoryTrain:  epoch  6, batch     0 | loss: 11.1818762Losses:  11.339941024780273 -0.0 10.91980266571045
MemoryTrain:  epoch  7, batch     0 | loss: 11.3399410Losses:  11.07929515838623 -0.0 10.881551742553711
MemoryTrain:  epoch  8, batch     0 | loss: 11.0792952Losses:  10.977856636047363 -0.0 10.875224113464355
MemoryTrain:  epoch  9, batch     0 | loss: 10.9778566
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 79.69%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 82.50%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 82.29%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 80.36%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 82.03%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 83.33%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 83.12%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 83.52%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 83.33%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 82.69%   [EVAL] batch:   13 | acc: 18.75%,  total acc: 78.12%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 68.75%,  total acc: 75.00%   [EVAL] batch:    2 | acc: 68.75%,  total acc: 72.92%   [EVAL] batch:    3 | acc: 62.50%,  total acc: 70.31%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 70.00%   [EVAL] batch:    5 | acc: 56.25%,  total acc: 67.71%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 69.64%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 73.44%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 76.39%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 78.12%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 80.11%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 80.73%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 80.36%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 80.00%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 78.52%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 78.31%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 77.43%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 76.64%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 77.19%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 78.27%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 79.26%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 80.16%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 80.99%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 81.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 82.45%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 82.87%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 83.48%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 84.05%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 84.17%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 84.68%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 85.16%   [EVAL] batch:   32 | acc: 62.50%,  total acc: 84.47%   [EVAL] batch:   33 | acc: 37.50%,  total acc: 83.09%   [EVAL] batch:   34 | acc: 43.75%,  total acc: 81.96%   [EVAL] batch:   35 | acc: 31.25%,  total acc: 80.56%   [EVAL] batch:   36 | acc: 68.75%,  total acc: 80.24%   [EVAL] batch:   37 | acc: 56.25%,  total acc: 79.61%   [EVAL] batch:   38 | acc: 18.75%,  total acc: 78.04%   [EVAL] batch:   39 | acc: 18.75%,  total acc: 76.56%   [EVAL] batch:   40 | acc: 31.25%,  total acc: 75.46%   [EVAL] batch:   41 | acc: 6.25%,  total acc: 73.81%   [EVAL] batch:   42 | acc: 6.25%,  total acc: 72.24%   [EVAL] batch:   43 | acc: 68.75%,  total acc: 72.16%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 72.78%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 73.37%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 73.94%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 74.48%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 75.38%   [EVAL] batch:   50 | acc: 81.25%,  total acc: 75.49%   [EVAL] batch:   51 | acc: 81.25%,  total acc: 75.60%   [EVAL] batch:   52 | acc: 87.50%,  total acc: 75.83%   [EVAL] batch:   53 | acc: 75.00%,  total acc: 75.81%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 76.14%   [EVAL] batch:   55 | acc: 75.00%,  total acc: 76.12%   [EVAL] batch:   56 | acc: 75.00%,  total acc: 76.10%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 76.40%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 76.59%   [EVAL] batch:   59 | acc: 87.50%,  total acc: 76.77%   [EVAL] batch:   60 | acc: 81.25%,  total acc: 76.84%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 77.02%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 76.59%   [EVAL] batch:   63 | acc: 6.25%,  total acc: 75.49%   
cur_acc:  ['0.8580', '0.8056', '0.7812']
his_acc:  ['0.8580', '0.8325', '0.7549']
Clustering into  9  clusters
Clusters:  [0 2 4 0 1 0 0 6 0 8 5 3 0 8 2 0 7 3 0 4 1]
Losses:  17.195178985595703 7.048998832702637 5.877572536468506
CurrentTrain: epoch  0, batch     0 | loss: 17.1951790Losses:  13.389053344726562 2.9281532764434814 5.717618465423584
CurrentTrain: epoch  0, batch     1 | loss: 13.3890533Losses:  16.494184494018555 7.099175930023193 5.7475104331970215
CurrentTrain: epoch  1, batch     0 | loss: 16.4941845Losses:  11.722818374633789 2.3513216972351074 5.750531196594238
CurrentTrain: epoch  1, batch     1 | loss: 11.7228184Losses:  16.02713394165039 7.232321262359619 5.703156471252441
CurrentTrain: epoch  2, batch     0 | loss: 16.0271339Losses:  10.535346984863281 3.5863468647003174 3.37420654296875
CurrentTrain: epoch  2, batch     1 | loss: 10.5353470Losses:  15.939732551574707 7.061783790588379 5.680455207824707
CurrentTrain: epoch  3, batch     0 | loss: 15.9397326Losses:  9.982019424438477 1.7519128322601318 5.634891510009766
CurrentTrain: epoch  3, batch     1 | loss: 9.9820194Losses:  16.068695068359375 7.237027168273926 5.644387722015381
CurrentTrain: epoch  4, batch     0 | loss: 16.0686951Losses:  7.809395790100098 2.271061897277832 3.321117639541626
CurrentTrain: epoch  4, batch     1 | loss: 7.8093958Losses:  15.31292724609375 7.178797721862793 5.641774654388428
CurrentTrain: epoch  5, batch     0 | loss: 15.3129272Losses:  9.549663543701172 3.0806162357330322 3.4139108657836914
CurrentTrain: epoch  5, batch     1 | loss: 9.5496635Losses:  14.717676162719727 6.477755546569824 5.653177738189697
CurrentTrain: epoch  6, batch     0 | loss: 14.7176762Losses:  9.645057678222656 1.6985559463500977 5.623481750488281
CurrentTrain: epoch  6, batch     1 | loss: 9.6450577Losses:  15.92025375366211 7.731972694396973 5.666656970977783
CurrentTrain: epoch  7, batch     0 | loss: 15.9202538Losses:  8.460807800292969 3.2928967475891113 3.3136520385742188
CurrentTrain: epoch  7, batch     1 | loss: 8.4608078Losses:  13.505107879638672 5.776735305786133 5.632685661315918
CurrentTrain: epoch  8, batch     0 | loss: 13.5051079Losses:  9.479350090026855 1.871829628944397 5.629745006561279
CurrentTrain: epoch  8, batch     1 | loss: 9.4793501Losses:  14.424168586730957 6.912750244140625 5.613521575927734
CurrentTrain: epoch  9, batch     0 | loss: 14.4241686Losses:  8.852834701538086 3.43987774848938 3.329908847808838
CurrentTrain: epoch  9, batch     1 | loss: 8.8528347
Losses:  15.434375762939453 -0.0 14.044327735900879
MemoryTrain:  epoch  0, batch     0 | loss: 15.4343758Losses:  7.368460655212402 -0.0 5.639163970947266
MemoryTrain:  epoch  0, batch     1 | loss: 7.3684607Losses:  15.609701156616211 -0.0 13.842859268188477
MemoryTrain:  epoch  1, batch     0 | loss: 15.6097012Losses:  2.0591182708740234 -0.0 1.4022964239120483
MemoryTrain:  epoch  1, batch     1 | loss: 2.0591183Losses:  17.82362937927246 -0.0 16.889347076416016
MemoryTrain:  epoch  2, batch     0 | loss: 17.8236294Losses:  7.056893348693848 -0.0 5.6540398597717285
MemoryTrain:  epoch  2, batch     1 | loss: 7.0568933Losses:  15.104862213134766 -0.0 13.890478134155273
MemoryTrain:  epoch  3, batch     0 | loss: 15.1048622Losses:  3.4240357875823975 -0.0 3.3558335304260254
MemoryTrain:  epoch  3, batch     1 | loss: 3.4240358Losses:  9.178770065307617 -0.0 8.155142784118652
MemoryTrain:  epoch  4, batch     0 | loss: 9.1787701Losses:  1.4706251621246338 -0.0 1.407754898071289
MemoryTrain:  epoch  4, batch     1 | loss: 1.4706252Losses:  14.241470336914062 -0.0 13.762202262878418
MemoryTrain:  epoch  5, batch     0 | loss: 14.2414703Losses:  4.465194225311279 -0.0 3.315185785293579
MemoryTrain:  epoch  5, batch     1 | loss: 4.4651942Losses:  14.206799507141113 -0.0 13.812605857849121
MemoryTrain:  epoch  6, batch     0 | loss: 14.2067995Losses:  6.7921342849731445 -0.0 5.6706414222717285
MemoryTrain:  epoch  6, batch     1 | loss: 6.7921343Losses:  10.969268798828125 -0.0 10.892794609069824
MemoryTrain:  epoch  7, batch     0 | loss: 10.9692688Losses:  5.28950309753418 -0.0 3.331907272338867
MemoryTrain:  epoch  7, batch     1 | loss: 5.2895031Losses:  17.195293426513672 -0.0 16.74472999572754
MemoryTrain:  epoch  8, batch     0 | loss: 17.1952934Losses:  1.527868390083313 -0.0 1.3955059051513672
MemoryTrain:  epoch  8, batch     1 | loss: 1.5278684Losses:  11.222302436828613 -0.0 10.820435523986816
MemoryTrain:  epoch  9, batch     0 | loss: 11.2223024Losses:  3.3402316570281982 -0.0 3.309596061706543
MemoryTrain:  epoch  9, batch     1 | loss: 3.3402317
[EVAL] batch:    0 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    1 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    2 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    3 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    4 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 99.22%   [EVAL] batch:    8 | acc: 75.00%,  total acc: 96.53%   [EVAL] batch:    9 | acc: 0.00%,  total acc: 86.88%   [EVAL] batch:   10 | acc: 12.50%,  total acc: 80.11%   [EVAL] batch:   11 | acc: 37.50%,  total acc: 76.56%   [EVAL] batch:   12 | acc: 25.00%,  total acc: 72.60%   [EVAL] batch:   13 | acc: 25.00%,  total acc: 69.20%   
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 79.17%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 80.00%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 79.17%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 82.14%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 84.38%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 86.11%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 86.88%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 88.07%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 88.02%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 87.98%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 86.61%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 85.42%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 83.59%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 83.09%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 81.94%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 80.92%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 80.94%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 81.85%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 82.67%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 83.42%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 84.11%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 84.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 85.34%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 85.65%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 86.16%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 86.64%   [EVAL] batch:   29 | acc: 93.75%,  total acc: 86.88%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 87.30%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 87.70%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 86.17%   [EVAL] batch:   33 | acc: 0.00%,  total acc: 83.64%   [EVAL] batch:   34 | acc: 0.00%,  total acc: 81.25%   [EVAL] batch:   35 | acc: 0.00%,  total acc: 78.99%   [EVAL] batch:   36 | acc: 6.25%,  total acc: 77.03%   [EVAL] batch:   37 | acc: 6.25%,  total acc: 75.16%   [EVAL] batch:   38 | acc: 18.75%,  total acc: 73.72%   [EVAL] batch:   39 | acc: 68.75%,  total acc: 73.59%   [EVAL] batch:   40 | acc: 37.50%,  total acc: 72.71%   [EVAL] batch:   41 | acc: 6.25%,  total acc: 71.13%   [EVAL] batch:   42 | acc: 6.25%,  total acc: 69.62%   [EVAL] batch:   43 | acc: 62.50%,  total acc: 69.46%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 70.14%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 70.79%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 71.41%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 72.01%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 72.58%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 73.00%   [EVAL] batch:   50 | acc: 75.00%,  total acc: 73.04%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 73.44%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 73.82%   [EVAL] batch:   53 | acc: 75.00%,  total acc: 73.84%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 74.20%   [EVAL] batch:   55 | acc: 81.25%,  total acc: 74.33%   [EVAL] batch:   56 | acc: 81.25%,  total acc: 74.45%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 74.68%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 75.00%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 75.31%   [EVAL] batch:   60 | acc: 75.00%,  total acc: 75.31%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 75.60%   [EVAL] batch:   62 | acc: 62.50%,  total acc: 75.40%   [EVAL] batch:   63 | acc: 81.25%,  total acc: 75.49%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 75.87%   [EVAL] batch:   65 | acc: 100.00%,  total acc: 76.23%   [EVAL] batch:   66 | acc: 100.00%,  total acc: 76.59%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 76.93%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 77.26%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 77.59%   [EVAL] batch:   70 | acc: 93.75%,  total acc: 77.82%   [EVAL] batch:   71 | acc: 93.75%,  total acc: 78.04%   [EVAL] batch:   72 | acc: 12.50%,  total acc: 77.14%   [EVAL] batch:   73 | acc: 6.25%,  total acc: 76.18%   [EVAL] batch:   74 | acc: 31.25%,  total acc: 75.58%   [EVAL] batch:   75 | acc: 25.00%,  total acc: 74.92%   [EVAL] batch:   76 | acc: 37.50%,  total acc: 74.43%   [EVAL] batch:   77 | acc: 0.00%,  total acc: 73.48%   
cur_acc:  ['0.8580', '0.8056', '0.7812', '0.6920']
his_acc:  ['0.8580', '0.8325', '0.7549', '0.7348']
Clustering into  12  clusters
Clusters:  [ 0  1  5  9  1  0  0 10  0  2  6  4  0  2  1  0  7  4  0  5  3  8  3  0
 11  0]
Losses:  18.765321731567383 10.74055290222168 3.4536900520324707
CurrentTrain: epoch  0, batch     0 | loss: 18.7653217Losses:  10.812871932983398 6.996701240539551 -0.0
CurrentTrain: epoch  0, batch     1 | loss: 10.8128719Losses:  13.34051513671875 6.370811939239502 3.3597631454467773
CurrentTrain: epoch  1, batch     0 | loss: 13.3405151Losses:  7.525627136230469 1.4598067998886108 3.3196797370910645
CurrentTrain: epoch  1, batch     1 | loss: 7.5256271Losses:  13.459938049316406 7.106465816497803 3.347754955291748
CurrentTrain: epoch  2, batch     0 | loss: 13.4599380Losses:  8.163492202758789 2.5200700759887695 3.3165714740753174
CurrentTrain: epoch  2, batch     1 | loss: 8.1634922Losses:  12.586980819702148 6.523279666900635 3.344027042388916
CurrentTrain: epoch  3, batch     0 | loss: 12.5869808Losses:  7.342098236083984 1.8067823648452759 3.3382792472839355
CurrentTrain: epoch  3, batch     1 | loss: 7.3420982Losses:  12.720149993896484 6.855813026428223 3.3337631225585938
CurrentTrain: epoch  4, batch     0 | loss: 12.7201500Losses:  8.07735824584961 2.3057615756988525 3.3493402004241943
CurrentTrain: epoch  4, batch     1 | loss: 8.0773582Losses:  13.107991218566895 7.393388748168945 3.342034339904785
CurrentTrain: epoch  5, batch     0 | loss: 13.1079912Losses:  7.1285505294799805 3.425368547439575 1.3899781703948975
CurrentTrain: epoch  5, batch     1 | loss: 7.1285505Losses:  11.855722427368164 6.379108428955078 3.345986843109131
CurrentTrain: epoch  6, batch     0 | loss: 11.8557224Losses:  7.645638942718506 2.3682992458343506 3.321988582611084
CurrentTrain: epoch  6, batch     1 | loss: 7.6456389Losses:  11.00698471069336 5.713280200958252 3.3390610218048096
CurrentTrain: epoch  7, batch     0 | loss: 11.0069847Losses:  7.094669342041016 1.8351330757141113 3.331094264984131
CurrentTrain: epoch  7, batch     1 | loss: 7.0946693Losses:  11.315378189086914 6.105144500732422 3.3302276134490967
CurrentTrain: epoch  8, batch     0 | loss: 11.3153782Losses:  7.574810981750488 2.295427083969116 3.3363778591156006
CurrentTrain: epoch  8, batch     1 | loss: 7.5748110Losses:  10.301759719848633 5.165077209472656 3.324697971343994
CurrentTrain: epoch  9, batch     0 | loss: 10.3017597Losses:  6.34088659286499 1.1635489463806152 3.3429412841796875
CurrentTrain: epoch  9, batch     1 | loss: 6.3408866
Losses:  17.891313552856445 -0.0 16.79802703857422
MemoryTrain:  epoch  0, batch     0 | loss: 17.8913136Losses:  16.96625518798828 -0.0 14.390499114990234
MemoryTrain:  epoch  0, batch     1 | loss: 16.9662552Losses:  22.15364646911621 -0.0 20.272584915161133
MemoryTrain:  epoch  1, batch     0 | loss: 22.1536465Losses:  9.555763244628906 -0.0 8.29852294921875
MemoryTrain:  epoch  1, batch     1 | loss: 9.5557632Losses:  20.967060089111328 -0.0 20.027738571166992
MemoryTrain:  epoch  2, batch     0 | loss: 20.9670601Losses:  5.16896915435791 -0.0 3.3133435249328613
MemoryTrain:  epoch  2, batch     1 | loss: 5.1689692Losses:  15.169961929321289 -0.0 13.930387496948242
MemoryTrain:  epoch  3, batch     0 | loss: 15.1699619Losses:  6.515351295471191 -0.0 5.625448226928711
MemoryTrain:  epoch  3, batch     1 | loss: 6.5153513Losses:  17.189132690429688 -0.0 16.731565475463867
MemoryTrain:  epoch  4, batch     0 | loss: 17.1891327Losses:  9.633036613464355 -0.0 8.3057279586792
MemoryTrain:  epoch  4, batch     1 | loss: 9.6330366Losses:  11.734498023986816 -0.0 11.001601219177246
MemoryTrain:  epoch  5, batch     0 | loss: 11.7344980Losses:  8.467035293579102 -0.0 8.079994201660156
MemoryTrain:  epoch  5, batch     1 | loss: 8.4670353Losses:  17.581729888916016 -0.0 16.914615631103516
MemoryTrain:  epoch  6, batch     0 | loss: 17.5817299Losses:  10.941019058227539 -0.0 10.847087860107422
MemoryTrain:  epoch  6, batch     1 | loss: 10.9410191Losses:  17.011531829833984 -0.0 16.734018325805664
MemoryTrain:  epoch  7, batch     0 | loss: 17.0115318Losses:  6.091498374938965 -0.0 5.6705322265625
MemoryTrain:  epoch  7, batch     1 | loss: 6.0914984Losses:  20.192523956298828 -0.0 20.009061813354492
MemoryTrain:  epoch  8, batch     0 | loss: 20.1925240Losses:  11.158020973205566 -0.0 10.849869728088379
MemoryTrain:  epoch  8, batch     1 | loss: 11.1580210Losses:  14.020295143127441 -0.0 13.771738052368164
MemoryTrain:  epoch  9, batch     0 | loss: 14.0202951Losses:  13.757567405700684 -0.0 13.69711685180664
MemoryTrain:  epoch  9, batch     1 | loss: 13.7575674
[EVAL] batch:    0 | acc: 43.75%,  total acc: 43.75%   [EVAL] batch:    1 | acc: 62.50%,  total acc: 53.12%   [EVAL] batch:    2 | acc: 43.75%,  total acc: 50.00%   [EVAL] batch:    3 | acc: 56.25%,  total acc: 51.56%   [EVAL] batch:    4 | acc: 18.75%,  total acc: 45.00%   [EVAL] batch:    5 | acc: 12.50%,  total acc: 39.58%   [EVAL] batch:    6 | acc: 43.75%,  total acc: 40.18%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 46.09%   [EVAL] batch:    8 | acc: 37.50%,  total acc: 45.14%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 48.75%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 51.70%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 54.69%   [EVAL] batch:   12 | acc: 50.00%,  total acc: 54.33%   
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 83.75%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 82.29%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 83.93%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 85.94%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 88.12%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 89.20%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 89.06%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 88.94%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 87.50%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 86.25%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 84.38%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 83.82%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 82.64%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 81.58%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 81.56%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 82.44%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 83.24%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 83.97%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 85.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 85.58%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 85.88%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 86.38%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 86.85%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 86.88%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 87.10%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 85.98%   [EVAL] batch:   33 | acc: 0.00%,  total acc: 83.46%   [EVAL] batch:   34 | acc: 0.00%,  total acc: 81.07%   [EVAL] batch:   35 | acc: 0.00%,  total acc: 78.82%   [EVAL] batch:   36 | acc: 0.00%,  total acc: 76.69%   [EVAL] batch:   37 | acc: 0.00%,  total acc: 74.67%   [EVAL] batch:   38 | acc: 0.00%,  total acc: 72.76%   [EVAL] batch:   39 | acc: 62.50%,  total acc: 72.50%   [EVAL] batch:   40 | acc: 12.50%,  total acc: 71.04%   [EVAL] batch:   41 | acc: 6.25%,  total acc: 69.49%   [EVAL] batch:   42 | acc: 6.25%,  total acc: 68.02%   [EVAL] batch:   43 | acc: 62.50%,  total acc: 67.90%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 68.61%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 69.29%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 69.95%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 70.57%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 71.17%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 71.50%   [EVAL] batch:   50 | acc: 31.25%,  total acc: 70.71%   [EVAL] batch:   51 | acc: 18.75%,  total acc: 69.71%   [EVAL] batch:   52 | acc: 0.00%,  total acc: 68.40%   [EVAL] batch:   53 | acc: 6.25%,  total acc: 67.25%   [EVAL] batch:   54 | acc: 0.00%,  total acc: 66.02%   [EVAL] batch:   55 | acc: 0.00%,  total acc: 64.84%   [EVAL] batch:   56 | acc: 62.50%,  total acc: 64.80%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 65.30%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 65.68%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 66.15%   [EVAL] batch:   60 | acc: 75.00%,  total acc: 66.29%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 66.63%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 66.37%   [EVAL] batch:   63 | acc: 68.75%,  total acc: 66.41%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 66.92%   [EVAL] batch:   65 | acc: 100.00%,  total acc: 67.42%   [EVAL] batch:   66 | acc: 100.00%,  total acc: 67.91%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 68.38%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 68.84%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 69.29%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 69.72%   [EVAL] batch:   71 | acc: 93.75%,  total acc: 70.05%   [EVAL] batch:   72 | acc: 12.50%,  total acc: 69.26%   [EVAL] batch:   73 | acc: 18.75%,  total acc: 68.58%   [EVAL] batch:   74 | acc: 12.50%,  total acc: 67.83%   [EVAL] batch:   75 | acc: 18.75%,  total acc: 67.19%   [EVAL] batch:   76 | acc: 18.75%,  total acc: 66.56%   [EVAL] batch:   77 | acc: 37.50%,  total acc: 66.19%   [EVAL] batch:   78 | acc: 62.50%,  total acc: 66.14%   [EVAL] batch:   79 | acc: 43.75%,  total acc: 65.86%   [EVAL] batch:   80 | acc: 62.50%,  total acc: 65.82%   [EVAL] batch:   81 | acc: 18.75%,  total acc: 65.24%   [EVAL] batch:   82 | acc: 12.50%,  total acc: 64.61%   [EVAL] batch:   83 | acc: 37.50%,  total acc: 64.29%   [EVAL] batch:   84 | acc: 87.50%,  total acc: 64.56%   [EVAL] batch:   85 | acc: 43.75%,  total acc: 64.32%   [EVAL] batch:   86 | acc: 81.25%,  total acc: 64.51%   [EVAL] batch:   87 | acc: 75.00%,  total acc: 64.63%   [EVAL] batch:   88 | acc: 87.50%,  total acc: 64.89%   [EVAL] batch:   89 | acc: 56.25%,  total acc: 64.79%   
cur_acc:  ['0.8580', '0.8056', '0.7812', '0.6920', '0.5433']
his_acc:  ['0.8580', '0.8325', '0.7549', '0.7348', '0.6479']
Clustering into  14  clusters
Clusters:  [ 0 13 11  9  1  5  0 10  0  1  8  6  0  1 13  0  7  6  0 11  3  2  3  5
  4  0  0  2 12  1  1]
Losses:  19.351207733154297 9.286041259765625 3.3118276596069336
CurrentTrain: epoch  0, batch     0 | loss: 19.3512077Losses:  12.95354175567627 3.7603344917297363 3.359095573425293
CurrentTrain: epoch  0, batch     1 | loss: 12.9535418Losses:  17.931621551513672 8.739465713500977 3.348954439163208
CurrentTrain: epoch  1, batch     0 | loss: 17.9316216Losses:  10.960737228393555 3.5503475666046143 3.3627521991729736
CurrentTrain: epoch  1, batch     1 | loss: 10.9607372Losses:  16.14274787902832 8.195873260498047 3.3144760131835938
CurrentTrain: epoch  2, batch     0 | loss: 16.1427479Losses:  9.559439659118652 3.4364254474639893 1.4159209728240967
CurrentTrain: epoch  2, batch     1 | loss: 9.5594397Losses:  16.769119262695312 9.859067916870117 3.320486068725586
CurrentTrain: epoch  3, batch     0 | loss: 16.7691193Losses:  12.754007339477539 6.798572063446045 1.4089794158935547
CurrentTrain: epoch  3, batch     1 | loss: 12.7540073Losses:  15.930394172668457 8.537959098815918 3.3325412273406982
CurrentTrain: epoch  4, batch     0 | loss: 15.9303942Losses:  9.790595054626465 3.171630382537842 3.323115348815918
CurrentTrain: epoch  4, batch     1 | loss: 9.7905951Losses:  14.854825973510742 7.850818157196045 3.331869602203369
CurrentTrain: epoch  5, batch     0 | loss: 14.8548260Losses:  9.19628620147705 3.0411007404327393 3.330188751220703
CurrentTrain: epoch  5, batch     1 | loss: 9.1962862Losses:  13.78978157043457 7.204855918884277 3.3202548027038574
CurrentTrain: epoch  6, batch     0 | loss: 13.7897816Losses:  8.589669227600098 2.35725474357605 3.32676100730896
CurrentTrain: epoch  6, batch     1 | loss: 8.5896692Losses:  13.894355773925781 7.935236930847168 3.325094699859619
CurrentTrain: epoch  7, batch     0 | loss: 13.8943558Losses:  8.869672775268555 4.518690586090088 1.392153024673462
CurrentTrain: epoch  7, batch     1 | loss: 8.8696728Losses:  12.08561897277832 6.203535079956055 3.321746587753296
CurrentTrain: epoch  8, batch     0 | loss: 12.0856190Losses:  8.074799537658691 2.1680493354797363 3.3128416538238525
CurrentTrain: epoch  8, batch     1 | loss: 8.0747995Losses:  11.847301483154297 5.737664222717285 3.325687885284424
CurrentTrain: epoch  9, batch     0 | loss: 11.8473015Losses:  7.215721130371094 1.3856467008590698 3.3210744857788086
CurrentTrain: epoch  9, batch     1 | loss: 7.2157211
Losses:  20.98246955871582 -0.0 20.175954818725586
MemoryTrain:  epoch  0, batch     0 | loss: 20.9824696Losses:  17.380775451660156 -0.0 16.89566993713379
MemoryTrain:  epoch  0, batch     1 | loss: 17.3807755Losses:  24.38702392578125 -0.0 23.355566024780273
MemoryTrain:  epoch  1, batch     0 | loss: 24.3870239Losses:  17.292377471923828 -0.0 16.88750648498535
MemoryTrain:  epoch  1, batch     1 | loss: 17.2923775Losses:  20.421571731567383 -0.0 20.04537010192871
MemoryTrain:  epoch  2, batch     0 | loss: 20.4215717Losses:  14.394397735595703 -0.0 13.808034896850586
MemoryTrain:  epoch  2, batch     1 | loss: 14.3943977Losses:  23.374526977539062 -0.0 23.260068893432617
MemoryTrain:  epoch  3, batch     0 | loss: 23.3745270Losses:  14.174483299255371 -0.0 13.706123352050781
MemoryTrain:  epoch  3, batch     1 | loss: 14.1744833Losses:  20.572511672973633 -0.0 20.065454483032227
MemoryTrain:  epoch  4, batch     0 | loss: 20.5725117Losses:  20.110488891601562 -0.0 19.992324829101562
MemoryTrain:  epoch  4, batch     1 | loss: 20.1104889Losses:  17.174362182617188 -0.0 16.828462600708008
MemoryTrain:  epoch  5, batch     0 | loss: 17.1743622Losses:  10.955004692077637 -0.0 10.85764217376709
MemoryTrain:  epoch  5, batch     1 | loss: 10.9550047Losses:  16.79753303527832 -0.0 16.73959732055664
MemoryTrain:  epoch  6, batch     0 | loss: 16.7975330Losses:  16.833309173583984 -0.0 16.781572341918945
MemoryTrain:  epoch  6, batch     1 | loss: 16.8333092Losses:  19.935251235961914 -0.0 19.89711570739746
MemoryTrain:  epoch  7, batch     0 | loss: 19.9352512Losses:  10.952089309692383 -0.0 10.839649200439453
MemoryTrain:  epoch  7, batch     1 | loss: 10.9520893Losses:  23.33235740661621 -0.0 23.27488136291504
MemoryTrain:  epoch  8, batch     0 | loss: 23.3323574Losses:  16.771854400634766 -0.0 16.715715408325195
MemoryTrain:  epoch  8, batch     1 | loss: 16.7718544Losses:  23.27651023864746 -0.0 23.207107543945312
MemoryTrain:  epoch  9, batch     0 | loss: 23.2765102Losses:  13.804564476013184 -0.0 13.731614112854004
MemoryTrain:  epoch  9, batch     1 | loss: 13.8045645
[EVAL] batch:    0 | acc: 37.50%,  total acc: 37.50%   [EVAL] batch:    1 | acc: 50.00%,  total acc: 43.75%   [EVAL] batch:    2 | acc: 43.75%,  total acc: 43.75%   [EVAL] batch:    3 | acc: 50.00%,  total acc: 45.31%   [EVAL] batch:    4 | acc: 62.50%,  total acc: 48.75%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 54.17%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 60.71%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 64.84%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 68.06%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 67.50%   [EVAL] batch:   10 | acc: 12.50%,  total acc: 62.50%   [EVAL] batch:   11 | acc: 12.50%,  total acc: 58.33%   [EVAL] batch:   12 | acc: 18.75%,  total acc: 55.29%   [EVAL] batch:   13 | acc: 31.25%,  total acc: 53.57%   [EVAL] batch:   14 | acc: 12.50%,  total acc: 50.83%   
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 68.75%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 77.08%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 76.56%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 76.25%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 77.08%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 80.36%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 82.81%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 84.72%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 85.62%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 86.93%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 86.98%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 86.54%   [EVAL] batch:   13 | acc: 43.75%,  total acc: 83.48%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 82.50%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 80.86%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 80.51%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 79.51%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 78.95%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 79.06%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 80.06%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 80.97%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 81.79%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 82.29%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 83.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 83.65%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 84.03%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 84.60%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 85.13%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 85.00%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 85.28%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 85.74%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 84.28%   [EVAL] batch:   33 | acc: 0.00%,  total acc: 81.80%   [EVAL] batch:   34 | acc: 6.25%,  total acc: 79.64%   [EVAL] batch:   35 | acc: 6.25%,  total acc: 77.60%   [EVAL] batch:   36 | acc: 0.00%,  total acc: 75.51%   [EVAL] batch:   37 | acc: 6.25%,  total acc: 73.68%   [EVAL] batch:   38 | acc: 12.50%,  total acc: 72.12%   [EVAL] batch:   39 | acc: 62.50%,  total acc: 71.88%   [EVAL] batch:   40 | acc: 37.50%,  total acc: 71.04%   [EVAL] batch:   41 | acc: 37.50%,  total acc: 70.24%   [EVAL] batch:   42 | acc: 37.50%,  total acc: 69.48%   [EVAL] batch:   43 | acc: 68.75%,  total acc: 69.46%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 70.14%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 70.79%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 71.41%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 72.01%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 72.58%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 72.88%   [EVAL] batch:   50 | acc: 25.00%,  total acc: 71.94%   [EVAL] batch:   51 | acc: 18.75%,  total acc: 70.91%   [EVAL] batch:   52 | acc: 6.25%,  total acc: 69.69%   [EVAL] batch:   53 | acc: 6.25%,  total acc: 68.52%   [EVAL] batch:   54 | acc: 6.25%,  total acc: 67.39%   [EVAL] batch:   55 | acc: 0.00%,  total acc: 66.18%   [EVAL] batch:   56 | acc: 56.25%,  total acc: 66.01%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 66.49%   [EVAL] batch:   58 | acc: 81.25%,  total acc: 66.74%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 67.19%   [EVAL] batch:   60 | acc: 75.00%,  total acc: 67.32%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 67.74%   [EVAL] batch:   62 | acc: 43.75%,  total acc: 67.36%   [EVAL] batch:   63 | acc: 68.75%,  total acc: 67.38%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 67.88%   [EVAL] batch:   65 | acc: 100.00%,  total acc: 68.37%   [EVAL] batch:   66 | acc: 100.00%,  total acc: 68.84%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 69.30%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 69.75%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 70.18%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 70.60%   [EVAL] batch:   71 | acc: 93.75%,  total acc: 70.92%   [EVAL] batch:   72 | acc: 18.75%,  total acc: 70.21%   [EVAL] batch:   73 | acc: 18.75%,  total acc: 69.51%   [EVAL] batch:   74 | acc: 12.50%,  total acc: 68.75%   [EVAL] batch:   75 | acc: 12.50%,  total acc: 68.01%   [EVAL] batch:   76 | acc: 6.25%,  total acc: 67.21%   [EVAL] batch:   77 | acc: 12.50%,  total acc: 66.51%   [EVAL] batch:   78 | acc: 37.50%,  total acc: 66.14%   [EVAL] batch:   79 | acc: 43.75%,  total acc: 65.86%   [EVAL] batch:   80 | acc: 56.25%,  total acc: 65.74%   [EVAL] batch:   81 | acc: 18.75%,  total acc: 65.17%   [EVAL] batch:   82 | acc: 12.50%,  total acc: 64.53%   [EVAL] batch:   83 | acc: 18.75%,  total acc: 63.99%   [EVAL] batch:   84 | acc: 56.25%,  total acc: 63.90%   [EVAL] batch:   85 | acc: 25.00%,  total acc: 63.44%   [EVAL] batch:   86 | acc: 62.50%,  total acc: 63.43%   [EVAL] batch:   87 | acc: 62.50%,  total acc: 63.42%   [EVAL] batch:   88 | acc: 87.50%,  total acc: 63.69%   [EVAL] batch:   89 | acc: 68.75%,  total acc: 63.75%   [EVAL] batch:   90 | acc: 43.75%,  total acc: 63.53%   [EVAL] batch:   91 | acc: 43.75%,  total acc: 63.32%   [EVAL] batch:   92 | acc: 50.00%,  total acc: 63.17%   [EVAL] batch:   93 | acc: 43.75%,  total acc: 62.97%   [EVAL] batch:   94 | acc: 81.25%,  total acc: 63.16%   [EVAL] batch:   95 | acc: 87.50%,  total acc: 63.41%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 63.79%   [EVAL] batch:   97 | acc: 93.75%,  total acc: 64.09%   [EVAL] batch:   98 | acc: 93.75%,  total acc: 64.39%   [EVAL] batch:   99 | acc: 31.25%,  total acc: 64.06%   [EVAL] batch:  100 | acc: 12.50%,  total acc: 63.55%   [EVAL] batch:  101 | acc: 6.25%,  total acc: 62.99%   [EVAL] batch:  102 | acc: 25.00%,  total acc: 62.62%   [EVAL] batch:  103 | acc: 37.50%,  total acc: 62.38%   [EVAL] batch:  104 | acc: 0.00%,  total acc: 61.79%   
cur_acc:  ['0.8580', '0.8056', '0.7812', '0.6920', '0.5433', '0.5083']
his_acc:  ['0.8580', '0.8325', '0.7549', '0.7348', '0.6479', '0.6179']
Clustering into  17  clusters
Clusters:  [ 1  0 16 11  2  6  1 14  7  2 12  3  1  2  0  7  4  3  1 16  9  5  9  6
 13  7  1  5 10  2  2  1 15  8  4  0]
Losses:  20.62877655029297 9.574592590332031 5.758115291595459
CurrentTrain: epoch  0, batch     0 | loss: 20.6287766Losses:  15.616870880126953 5.760274887084961 1.5065053701400757
CurrentTrain: epoch  0, batch     1 | loss: 15.6168709Losses:  18.462753295898438 7.3556718826293945 5.789220333099365
CurrentTrain: epoch  1, batch     0 | loss: 18.4627533Losses:  11.514949798583984 1.4872084856033325 5.658628940582275
CurrentTrain: epoch  1, batch     1 | loss: 11.5149498Losses:  17.420717239379883 7.586462020874023 5.703791618347168
CurrentTrain: epoch  2, batch     0 | loss: 17.4207172Losses:  15.082427978515625 3.373999834060669 5.629794597625732
CurrentTrain: epoch  2, batch     1 | loss: 15.0824280Losses:  16.998092651367188 7.681154251098633 5.632216930389404
CurrentTrain: epoch  3, batch     0 | loss: 16.9980927Losses:  13.568819046020508 3.7140953540802 5.641221523284912
CurrentTrain: epoch  3, batch     1 | loss: 13.5688190Losses:  18.705419540405273 8.617143630981445 5.6709885597229
CurrentTrain: epoch  4, batch     0 | loss: 18.7054195Losses:  9.681045532226562 3.528520107269287 3.335624933242798
CurrentTrain: epoch  4, batch     1 | loss: 9.6810455Losses:  16.412975311279297 7.246518135070801 5.633559703826904
CurrentTrain: epoch  5, batch     0 | loss: 16.4129753Losses:  10.66573715209961 3.3254621028900146 3.3345484733581543
CurrentTrain: epoch  5, batch     1 | loss: 10.6657372Losses:  15.37544059753418 6.591955661773682 5.6744384765625
CurrentTrain: epoch  6, batch     0 | loss: 15.3754406Losses:  12.396541595458984 2.866461992263794 5.603832244873047
CurrentTrain: epoch  6, batch     1 | loss: 12.3965416Losses:  16.14605712890625 7.645229339599609 5.594768047332764
CurrentTrain: epoch  7, batch     0 | loss: 16.1460571Losses:  11.495382308959961 4.387082576751709 3.3562111854553223
CurrentTrain: epoch  7, batch     1 | loss: 11.4953823Losses:  15.687826156616211 6.658844947814941 5.651944637298584
CurrentTrain: epoch  8, batch     0 | loss: 15.6878262Losses:  9.80384349822998 1.6113488674163818 5.597835063934326
CurrentTrain: epoch  8, batch     1 | loss: 9.8038435Losses:  14.772844314575195 6.273918151855469 5.618677616119385
CurrentTrain: epoch  9, batch     0 | loss: 14.7728443Losses:  9.175859451293945 2.5318520069122314 3.370917797088623
CurrentTrain: epoch  9, batch     1 | loss: 9.1758595
Losses:  20.7852783203125 -0.0 20.0001220703125
MemoryTrain:  epoch  0, batch     0 | loss: 20.7852783Losses:  21.37876319885254 -0.0 19.955442428588867
MemoryTrain:  epoch  0, batch     1 | loss: 21.3787632Losses:  3.3269708156585693 -0.0 3.3187975883483887
MemoryTrain:  epoch  0, batch     2 | loss: 3.3269708Losses:  23.751977920532227 -0.0 23.189605712890625
MemoryTrain:  epoch  1, batch     0 | loss: 23.7519779Losses:  24.553726196289062 -0.0 23.19158935546875
MemoryTrain:  epoch  1, batch     1 | loss: 24.5537262Losses:  3.882711172103882 -0.0 3.3018364906311035
MemoryTrain:  epoch  1, batch     2 | loss: 3.8827112Losses:  17.73335838317871 -0.0 16.733835220336914
MemoryTrain:  epoch  2, batch     0 | loss: 17.7333584Losses:  30.494380950927734 -0.0 30.121789932250977
MemoryTrain:  epoch  2, batch     1 | loss: 30.4943810Losses:  3.347327947616577 -0.0 3.3119397163391113
MemoryTrain:  epoch  2, batch     2 | loss: 3.3473279Losses:  30.306331634521484 -0.0 29.991046905517578
MemoryTrain:  epoch  3, batch     0 | loss: 30.3063316Losses:  27.028076171875 -0.0 26.49432373046875
MemoryTrain:  epoch  3, batch     1 | loss: 27.0280762Losses:  2.0565719604492188 -0.0 1.3876750469207764
MemoryTrain:  epoch  3, batch     2 | loss: 2.0565720Losses:  26.635807037353516 -0.0 26.55939292907715
MemoryTrain:  epoch  4, batch     0 | loss: 26.6358070Losses:  23.282854080200195 -0.0 23.179868698120117
MemoryTrain:  epoch  4, batch     1 | loss: 23.2828541Losses:  3.5041561126708984 -0.0 3.321483612060547
MemoryTrain:  epoch  4, batch     2 | loss: 3.5041561Losses:  19.960006713867188 -0.0 19.887542724609375
MemoryTrain:  epoch  5, batch     0 | loss: 19.9600067Losses:  30.248022079467773 -0.0 30.083847045898438
MemoryTrain:  epoch  5, batch     1 | loss: 30.2480221Losses:  3.314396381378174 -0.0 3.300914764404297
MemoryTrain:  epoch  5, batch     2 | loss: 3.3143964Losses:  23.604442596435547 -0.0 23.192354202270508
MemoryTrain:  epoch  6, batch     0 | loss: 23.6044426Losses:  23.24303436279297 -0.0 23.1458740234375
MemoryTrain:  epoch  6, batch     1 | loss: 23.2430344Losses:  3.360480785369873 -0.0 3.317988634109497
MemoryTrain:  epoch  6, batch     2 | loss: 3.3604808Losses:  26.581998825073242 -0.0 26.532787322998047
MemoryTrain:  epoch  7, batch     0 | loss: 26.5819988Losses:  13.945493698120117 -0.0 13.68839168548584
MemoryTrain:  epoch  7, batch     1 | loss: 13.9454937Losses:  3.3508460521698 -0.0 3.328629970550537
MemoryTrain:  epoch  7, batch     2 | loss: 3.3508461Losses:  19.898324966430664 -0.0 19.85711097717285
MemoryTrain:  epoch  8, batch     0 | loss: 19.8983250Losses:  23.32410430908203 -0.0 23.156091690063477
MemoryTrain:  epoch  8, batch     1 | loss: 23.3241043Losses:  3.3242924213409424 -0.0 3.307288646697998
MemoryTrain:  epoch  8, batch     2 | loss: 3.3242924Losses:  26.521953582763672 -0.0 26.486492156982422
MemoryTrain:  epoch  9, batch     0 | loss: 26.5219536Losses:  23.17829132080078 -0.0 23.11945152282715
MemoryTrain:  epoch  9, batch     1 | loss: 23.1782913Losses:  3.4037137031555176 -0.0 3.3339686393737793
MemoryTrain:  epoch  9, batch     2 | loss: 3.4037137
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:    2 | acc: 68.75%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 12.50%,  total acc: 65.62%   [EVAL] batch:    4 | acc: 12.50%,  total acc: 55.00%   [EVAL] batch:    5 | acc: 25.00%,  total acc: 50.00%   [EVAL] batch:    6 | acc: 6.25%,  total acc: 43.75%   [EVAL] batch:    7 | acc: 6.25%,  total acc: 39.06%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    2 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    3 | acc: 37.50%,  total acc: 60.94%   [EVAL] batch:    4 | acc: 56.25%,  total acc: 60.00%   [EVAL] batch:    5 | acc: 68.75%,  total acc: 61.46%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 64.29%   [EVAL] batch:    7 | acc: 68.75%,  total acc: 64.84%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 68.06%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 69.38%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 70.45%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 71.88%   [EVAL] batch:   12 | acc: 50.00%,  total acc: 70.19%   [EVAL] batch:   13 | acc: 25.00%,  total acc: 66.96%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 67.08%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 66.41%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 66.91%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 66.67%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 66.45%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 66.88%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 68.45%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 69.89%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 70.92%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 71.88%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 73.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 74.04%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 74.77%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 75.67%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 76.51%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 76.67%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 77.02%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 77.73%   [EVAL] batch:   32 | acc: 43.75%,  total acc: 76.70%   [EVAL] batch:   33 | acc: 0.00%,  total acc: 74.45%   [EVAL] batch:   34 | acc: 12.50%,  total acc: 72.68%   [EVAL] batch:   35 | acc: 0.00%,  total acc: 70.66%   [EVAL] batch:   36 | acc: 0.00%,  total acc: 68.75%   [EVAL] batch:   37 | acc: 12.50%,  total acc: 67.27%   [EVAL] batch:   38 | acc: 31.25%,  total acc: 66.35%   [EVAL] batch:   39 | acc: 87.50%,  total acc: 66.88%   [EVAL] batch:   40 | acc: 31.25%,  total acc: 66.01%   [EVAL] batch:   41 | acc: 37.50%,  total acc: 65.33%   [EVAL] batch:   42 | acc: 37.50%,  total acc: 64.68%   [EVAL] batch:   43 | acc: 75.00%,  total acc: 64.91%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 65.69%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 66.44%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 67.15%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 67.84%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 68.49%   [EVAL] batch:   49 | acc: 81.25%,  total acc: 68.75%   [EVAL] batch:   50 | acc: 37.50%,  total acc: 68.14%   [EVAL] batch:   51 | acc: 6.25%,  total acc: 66.95%   [EVAL] batch:   52 | acc: 6.25%,  total acc: 65.80%   [EVAL] batch:   53 | acc: 0.00%,  total acc: 64.58%   [EVAL] batch:   54 | acc: 6.25%,  total acc: 63.52%   [EVAL] batch:   55 | acc: 0.00%,  total acc: 62.39%   [EVAL] batch:   56 | acc: 56.25%,  total acc: 62.28%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 62.72%   [EVAL] batch:   58 | acc: 75.00%,  total acc: 62.92%   [EVAL] batch:   59 | acc: 87.50%,  total acc: 63.33%   [EVAL] batch:   60 | acc: 75.00%,  total acc: 63.52%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 64.01%   [EVAL] batch:   62 | acc: 43.75%,  total acc: 63.69%   [EVAL] batch:   63 | acc: 56.25%,  total acc: 63.57%   [EVAL] batch:   64 | acc: 81.25%,  total acc: 63.85%   [EVAL] batch:   65 | acc: 81.25%,  total acc: 64.11%   [EVAL] batch:   66 | acc: 100.00%,  total acc: 64.65%   [EVAL] batch:   67 | acc: 93.75%,  total acc: 65.07%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 65.58%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 66.07%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 66.55%   [EVAL] batch:   71 | acc: 93.75%,  total acc: 66.93%   [EVAL] batch:   72 | acc: 0.00%,  total acc: 66.01%   [EVAL] batch:   73 | acc: 0.00%,  total acc: 65.12%   [EVAL] batch:   74 | acc: 12.50%,  total acc: 64.42%   [EVAL] batch:   75 | acc: 12.50%,  total acc: 63.73%   [EVAL] batch:   76 | acc: 6.25%,  total acc: 62.99%   [EVAL] batch:   77 | acc: 12.50%,  total acc: 62.34%   [EVAL] batch:   78 | acc: 37.50%,  total acc: 62.03%   [EVAL] batch:   79 | acc: 43.75%,  total acc: 61.80%   [EVAL] batch:   80 | acc: 50.00%,  total acc: 61.65%   [EVAL] batch:   81 | acc: 25.00%,  total acc: 61.20%   [EVAL] batch:   82 | acc: 6.25%,  total acc: 60.54%   [EVAL] batch:   83 | acc: 25.00%,  total acc: 60.12%   [EVAL] batch:   84 | acc: 75.00%,  total acc: 60.29%   [EVAL] batch:   85 | acc: 31.25%,  total acc: 59.96%   [EVAL] batch:   86 | acc: 56.25%,  total acc: 59.91%   [EVAL] batch:   87 | acc: 75.00%,  total acc: 60.09%   [EVAL] batch:   88 | acc: 75.00%,  total acc: 60.25%   [EVAL] batch:   89 | acc: 62.50%,  total acc: 60.28%   [EVAL] batch:   90 | acc: 25.00%,  total acc: 59.89%   [EVAL] batch:   91 | acc: 12.50%,  total acc: 59.38%   [EVAL] batch:   92 | acc: 18.75%,  total acc: 58.94%   [EVAL] batch:   93 | acc: 37.50%,  total acc: 58.71%   [EVAL] batch:   94 | acc: 37.50%,  total acc: 58.49%   [EVAL] batch:   95 | acc: 93.75%,  total acc: 58.85%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 59.28%   [EVAL] batch:   97 | acc: 81.25%,  total acc: 59.50%   [EVAL] batch:   98 | acc: 93.75%,  total acc: 59.85%   [EVAL] batch:   99 | acc: 25.00%,  total acc: 59.50%   [EVAL] batch:  100 | acc: 12.50%,  total acc: 59.03%   [EVAL] batch:  101 | acc: 6.25%,  total acc: 58.52%   [EVAL] batch:  102 | acc: 25.00%,  total acc: 58.19%   [EVAL] batch:  103 | acc: 18.75%,  total acc: 57.81%   [EVAL] batch:  104 | acc: 75.00%,  total acc: 57.98%   [EVAL] batch:  105 | acc: 100.00%,  total acc: 58.37%   [EVAL] batch:  106 | acc: 75.00%,  total acc: 58.53%   [EVAL] batch:  107 | acc: 12.50%,  total acc: 58.10%   [EVAL] batch:  108 | acc: 12.50%,  total acc: 57.68%   [EVAL] batch:  109 | acc: 18.75%,  total acc: 57.33%   [EVAL] batch:  110 | acc: 12.50%,  total acc: 56.93%   [EVAL] batch:  111 | acc: 6.25%,  total acc: 56.47%   
cur_acc:  ['0.8580', '0.8056', '0.7812', '0.6920', '0.5433', '0.5083', '0.3906']
his_acc:  ['0.8580', '0.8325', '0.7549', '0.7348', '0.6479', '0.6179', '0.5647']
Clustering into  19  clusters
Clusters:  [ 4  1  0 11 17 10  4 15 18 16 12  7  4 16  1 18  2  7  4  0  6  3  6 10
 14 18  4  3  9  8 16  4 13  5  2  1  0  0  8 10 16]
Losses:  20.773921966552734 8.562922477722168 3.7096104621887207
CurrentTrain: epoch  0, batch     0 | loss: 20.7739220Losses:  14.624585151672363 3.000927448272705 3.486088752746582
CurrentTrain: epoch  0, batch     1 | loss: 14.6245852Losses:  19.6135196685791 8.603639602661133 3.57928204536438
CurrentTrain: epoch  1, batch     0 | loss: 19.6135197Losses:  14.00193977355957 3.0341365337371826 3.5459465980529785
CurrentTrain: epoch  1, batch     1 | loss: 14.0019398Losses:  18.24081039428711 7.794968605041504 3.5950632095336914
CurrentTrain: epoch  2, batch     0 | loss: 18.2408104Losses:  12.605257987976074 2.409327983856201 3.4625370502471924
CurrentTrain: epoch  2, batch     1 | loss: 12.6052580Losses:  19.008941650390625 8.492746353149414 3.567798376083374
CurrentTrain: epoch  3, batch     0 | loss: 19.0089417Losses:  11.987285614013672 2.532193660736084 3.505084991455078
CurrentTrain: epoch  3, batch     1 | loss: 11.9872856Losses:  16.896778106689453 7.761354446411133 3.5243477821350098
CurrentTrain: epoch  4, batch     0 | loss: 16.8967781Losses:  12.995301246643066 3.3602240085601807 3.4730565547943115
CurrentTrain: epoch  4, batch     1 | loss: 12.9953012Losses:  18.28595542907715 8.940543174743652 3.549605131149292
CurrentTrain: epoch  5, batch     0 | loss: 18.2859554Losses:  12.108651161193848 3.196545124053955 3.528080940246582
CurrentTrain: epoch  5, batch     1 | loss: 12.1086512Losses:  16.967519760131836 7.720446586608887 3.5179219245910645
CurrentTrain: epoch  6, batch     0 | loss: 16.9675198Losses:  10.738496780395508 2.0455710887908936 3.514890193939209
CurrentTrain: epoch  6, batch     1 | loss: 10.7384968Losses:  16.480693817138672 7.7975358963012695 3.499709367752075
CurrentTrain: epoch  7, batch     0 | loss: 16.4806938Losses:  11.246232032775879 2.5710127353668213 3.439692497253418
CurrentTrain: epoch  7, batch     1 | loss: 11.2462320Losses:  15.620161056518555 7.402004241943359 3.4597620964050293
CurrentTrain: epoch  8, batch     0 | loss: 15.6201611Losses:  10.739465713500977 2.5062663555145264 3.41691255569458
CurrentTrain: epoch  8, batch     1 | loss: 10.7394657Losses:  15.986479759216309 7.77615213394165 3.4512040615081787
CurrentTrain: epoch  9, batch     0 | loss: 15.9864798Losses:  9.458438873291016 2.2680914402008057 3.457688808441162
CurrentTrain: epoch  9, batch     1 | loss: 9.4584389
Losses:  27.25880241394043 -0.0 26.716299057006836
MemoryTrain:  epoch  0, batch     0 | loss: 27.2588024Losses:  19.856895446777344 -0.0 19.832542419433594
MemoryTrain:  epoch  0, batch     1 | loss: 19.8568954Losses:  11.28354263305664 -0.0 10.88388729095459
MemoryTrain:  epoch  0, batch     2 | loss: 11.2835426Losses:  30.239154815673828 -0.0 29.91561508178711
MemoryTrain:  epoch  1, batch     0 | loss: 30.2391548Losses:  20.243947982788086 -0.0 19.933582305908203
MemoryTrain:  epoch  1, batch     1 | loss: 20.2439480Losses:  8.184369087219238 -0.0 8.083065032958984
MemoryTrain:  epoch  1, batch     2 | loss: 8.1843691Losses:  20.20624542236328 -0.0 19.94716453552246
MemoryTrain:  epoch  2, batch     0 | loss: 20.2062454Losses:  30.086204528808594 -0.0 29.99919319152832
MemoryTrain:  epoch  2, batch     1 | loss: 30.0862045Losses:  13.724102973937988 -0.0 13.658206939697266
MemoryTrain:  epoch  2, batch     2 | loss: 13.7241030Losses:  26.545082092285156 -0.0 26.470260620117188
MemoryTrain:  epoch  3, batch     0 | loss: 26.5450821Losses:  30.06072425842285 -0.0 29.971887588500977
MemoryTrain:  epoch  3, batch     1 | loss: 30.0607243Losses:  8.359153747558594 -0.0 8.114081382751465
MemoryTrain:  epoch  3, batch     2 | loss: 8.3591537Losses:  23.163436889648438 -0.0 23.108566284179688
MemoryTrain:  epoch  4, batch     0 | loss: 23.1634369Losses:  33.71207046508789 -0.0 33.497074127197266
MemoryTrain:  epoch  4, batch     1 | loss: 33.7120705Losses:  13.704895973205566 -0.0 13.668590545654297
MemoryTrain:  epoch  4, batch     2 | loss: 13.7048960Losses:  23.13861846923828 -0.0 23.10317039489746
MemoryTrain:  epoch  5, batch     0 | loss: 23.1386185Losses:  20.01017189025879 -0.0 19.884485244750977
MemoryTrain:  epoch  5, batch     1 | loss: 20.0101719Losses:  10.849862098693848 -0.0 10.799928665161133
MemoryTrain:  epoch  5, batch     2 | loss: 10.8498621Losses:  23.144784927368164 -0.0 23.11849594116211
MemoryTrain:  epoch  6, batch     0 | loss: 23.1447849Losses:  29.962427139282227 -0.0 29.917835235595703
MemoryTrain:  epoch  6, batch     1 | loss: 29.9624271Losses:  10.908761024475098 -0.0 10.819942474365234
MemoryTrain:  epoch  6, batch     2 | loss: 10.9087610Losses:  33.45073699951172 -0.0 33.42768096923828
MemoryTrain:  epoch  7, batch     0 | loss: 33.4507370Losses:  23.17173194885254 -0.0 23.110916137695312
MemoryTrain:  epoch  7, batch     1 | loss: 23.1717319Losses:  10.847580909729004 -0.0 10.824387550354004
MemoryTrain:  epoch  7, batch     2 | loss: 10.8475809Losses:  23.147212982177734 -0.0 23.10747718811035
MemoryTrain:  epoch  8, batch     0 | loss: 23.1472130Losses:  26.504894256591797 -0.0 26.4574031829834
MemoryTrain:  epoch  8, batch     1 | loss: 26.5048943Losses:  10.824209213256836 -0.0 10.792634963989258
MemoryTrain:  epoch  8, batch     2 | loss: 10.8242092Losses:  29.935670852661133 -0.0 29.886802673339844
MemoryTrain:  epoch  9, batch     0 | loss: 29.9356709Losses:  23.12837791442871 -0.0 23.080547332763672
MemoryTrain:  epoch  9, batch     1 | loss: 23.1283779Losses:  10.907830238342285 -0.0 10.8733549118042
MemoryTrain:  epoch  9, batch     2 | loss: 10.9078302
[EVAL] batch:    0 | acc: 25.00%,  total acc: 25.00%   [EVAL] batch:    1 | acc: 37.50%,  total acc: 31.25%   [EVAL] batch:    2 | acc: 25.00%,  total acc: 29.17%   [EVAL] batch:    3 | acc: 6.25%,  total acc: 23.44%   [EVAL] batch:    4 | acc: 12.50%,  total acc: 21.25%   [EVAL] batch:    5 | acc: 6.25%,  total acc: 18.75%   [EVAL] batch:    6 | acc: 25.00%,  total acc: 19.64%   [EVAL] batch:    7 | acc: 62.50%,  total acc: 25.00%   [EVAL] batch:    8 | acc: 43.75%,  total acc: 27.08%   [EVAL] batch:    9 | acc: 56.25%,  total acc: 30.00%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 34.66%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 38.02%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 41.35%   [EVAL] batch:   13 | acc: 50.00%,  total acc: 41.96%   [EVAL] batch:   14 | acc: 62.50%,  total acc: 43.33%   [EVAL] batch:   15 | acc: 62.50%,  total acc: 44.53%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 46.32%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 47.22%   [EVAL] batch:   18 | acc: 75.00%,  total acc: 48.68%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 50.62%   [EVAL] batch:   20 | acc: 62.50%,  total acc: 51.19%   [EVAL] batch:   21 | acc: 37.50%,  total acc: 50.57%   
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    1 | acc: 68.75%,  total acc: 65.62%   [EVAL] batch:    2 | acc: 68.75%,  total acc: 66.67%   [EVAL] batch:    3 | acc: 43.75%,  total acc: 60.94%   [EVAL] batch:    4 | acc: 56.25%,  total acc: 60.00%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 62.50%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 65.18%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 67.19%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 69.44%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 70.62%   [EVAL] batch:   10 | acc: 75.00%,  total acc: 71.02%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 72.40%   [EVAL] batch:   12 | acc: 43.75%,  total acc: 70.19%   [EVAL] batch:   13 | acc: 25.00%,  total acc: 66.96%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 67.08%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 66.41%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 66.91%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 66.67%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 66.45%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 66.88%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 68.45%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 69.89%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 70.92%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 71.88%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 73.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 74.04%   [EVAL] batch:   26 | acc: 87.50%,  total acc: 74.54%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 75.45%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 76.29%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 76.46%   [EVAL] batch:   30 | acc: 68.75%,  total acc: 76.21%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 76.76%   [EVAL] batch:   32 | acc: 43.75%,  total acc: 75.76%   [EVAL] batch:   33 | acc: 0.00%,  total acc: 73.53%   [EVAL] batch:   34 | acc: 0.00%,  total acc: 71.43%   [EVAL] batch:   35 | acc: 0.00%,  total acc: 69.44%   [EVAL] batch:   36 | acc: 6.25%,  total acc: 67.74%   [EVAL] batch:   37 | acc: 6.25%,  total acc: 66.12%   [EVAL] batch:   38 | acc: 31.25%,  total acc: 65.22%   [EVAL] batch:   39 | acc: 93.75%,  total acc: 65.94%   [EVAL] batch:   40 | acc: 43.75%,  total acc: 65.40%   [EVAL] batch:   41 | acc: 25.00%,  total acc: 64.43%   [EVAL] batch:   42 | acc: 18.75%,  total acc: 63.37%   [EVAL] batch:   43 | acc: 68.75%,  total acc: 63.49%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 64.31%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 65.08%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 65.82%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 66.54%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 67.22%   [EVAL] batch:   49 | acc: 81.25%,  total acc: 67.50%   [EVAL] batch:   50 | acc: 25.00%,  total acc: 66.67%   [EVAL] batch:   51 | acc: 12.50%,  total acc: 65.62%   [EVAL] batch:   52 | acc: 0.00%,  total acc: 64.39%   [EVAL] batch:   53 | acc: 0.00%,  total acc: 63.19%   [EVAL] batch:   54 | acc: 0.00%,  total acc: 62.05%   [EVAL] batch:   55 | acc: 0.00%,  total acc: 60.94%   [EVAL] batch:   56 | acc: 56.25%,  total acc: 60.86%   [EVAL] batch:   57 | acc: 81.25%,  total acc: 61.21%   [EVAL] batch:   58 | acc: 62.50%,  total acc: 61.23%   [EVAL] batch:   59 | acc: 87.50%,  total acc: 61.67%   [EVAL] batch:   60 | acc: 68.75%,  total acc: 61.78%   [EVAL] batch:   61 | acc: 62.50%,  total acc: 61.79%   [EVAL] batch:   62 | acc: 43.75%,  total acc: 61.51%   [EVAL] batch:   63 | acc: 56.25%,  total acc: 61.43%   [EVAL] batch:   64 | acc: 81.25%,  total acc: 61.73%   [EVAL] batch:   65 | acc: 81.25%,  total acc: 62.03%   [EVAL] batch:   66 | acc: 100.00%,  total acc: 62.59%   [EVAL] batch:   67 | acc: 93.75%,  total acc: 63.05%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 63.59%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 64.11%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 64.61%   [EVAL] batch:   71 | acc: 93.75%,  total acc: 65.02%   [EVAL] batch:   72 | acc: 0.00%,  total acc: 64.13%   [EVAL] batch:   73 | acc: 0.00%,  total acc: 63.26%   [EVAL] batch:   74 | acc: 12.50%,  total acc: 62.58%   [EVAL] batch:   75 | acc: 12.50%,  total acc: 61.92%   [EVAL] batch:   76 | acc: 6.25%,  total acc: 61.20%   [EVAL] batch:   77 | acc: 12.50%,  total acc: 60.58%   [EVAL] batch:   78 | acc: 37.50%,  total acc: 60.28%   [EVAL] batch:   79 | acc: 43.75%,  total acc: 60.08%   [EVAL] batch:   80 | acc: 56.25%,  total acc: 60.03%   [EVAL] batch:   81 | acc: 12.50%,  total acc: 59.45%   [EVAL] batch:   82 | acc: 0.00%,  total acc: 58.73%   [EVAL] batch:   83 | acc: 0.00%,  total acc: 58.04%   [EVAL] batch:   84 | acc: 31.25%,  total acc: 57.72%   [EVAL] batch:   85 | acc: 18.75%,  total acc: 57.27%   [EVAL] batch:   86 | acc: 50.00%,  total acc: 57.18%   [EVAL] batch:   87 | acc: 56.25%,  total acc: 57.17%   [EVAL] batch:   88 | acc: 75.00%,  total acc: 57.37%   [EVAL] batch:   89 | acc: 62.50%,  total acc: 57.43%   [EVAL] batch:   90 | acc: 31.25%,  total acc: 57.14%   [EVAL] batch:   91 | acc: 6.25%,  total acc: 56.59%   [EVAL] batch:   92 | acc: 6.25%,  total acc: 56.05%   [EVAL] batch:   93 | acc: 31.25%,  total acc: 55.78%   [EVAL] batch:   94 | acc: 37.50%,  total acc: 55.59%   [EVAL] batch:   95 | acc: 87.50%,  total acc: 55.92%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 56.38%   [EVAL] batch:   97 | acc: 81.25%,  total acc: 56.63%   [EVAL] batch:   98 | acc: 93.75%,  total acc: 57.01%   [EVAL] batch:   99 | acc: 25.00%,  total acc: 56.69%   [EVAL] batch:  100 | acc: 0.00%,  total acc: 56.13%   [EVAL] batch:  101 | acc: 0.00%,  total acc: 55.58%   [EVAL] batch:  102 | acc: 6.25%,  total acc: 55.10%   [EVAL] batch:  103 | acc: 18.75%,  total acc: 54.75%   [EVAL] batch:  104 | acc: 75.00%,  total acc: 54.94%   [EVAL] batch:  105 | acc: 100.00%,  total acc: 55.37%   [EVAL] batch:  106 | acc: 50.00%,  total acc: 55.32%   [EVAL] batch:  107 | acc: 18.75%,  total acc: 54.98%   [EVAL] batch:  108 | acc: 12.50%,  total acc: 54.59%   [EVAL] batch:  109 | acc: 18.75%,  total acc: 54.26%   [EVAL] batch:  110 | acc: 25.00%,  total acc: 54.00%   [EVAL] batch:  111 | acc: 31.25%,  total acc: 53.79%   [EVAL] batch:  112 | acc: 43.75%,  total acc: 53.71%   [EVAL] batch:  113 | acc: 25.00%,  total acc: 53.45%   [EVAL] batch:  114 | acc: 6.25%,  total acc: 53.04%   [EVAL] batch:  115 | acc: 6.25%,  total acc: 52.64%   [EVAL] batch:  116 | acc: 12.50%,  total acc: 52.30%   [EVAL] batch:  117 | acc: 18.75%,  total acc: 52.01%   [EVAL] batch:  118 | acc: 62.50%,  total acc: 52.10%   [EVAL] batch:  119 | acc: 37.50%,  total acc: 51.98%   [EVAL] batch:  120 | acc: 56.25%,  total acc: 52.01%   [EVAL] batch:  121 | acc: 81.25%,  total acc: 52.25%   [EVAL] batch:  122 | acc: 81.25%,  total acc: 52.49%   [EVAL] batch:  123 | acc: 81.25%,  total acc: 52.72%   [EVAL] batch:  124 | acc: 43.75%,  total acc: 52.65%   [EVAL] batch:  125 | acc: 56.25%,  total acc: 52.68%   [EVAL] batch:  126 | acc: 68.75%,  total acc: 52.81%   [EVAL] batch:  127 | acc: 68.75%,  total acc: 52.93%   [EVAL] batch:  128 | acc: 68.75%,  total acc: 53.05%   [EVAL] batch:  129 | acc: 81.25%,  total acc: 53.27%   [EVAL] batch:  130 | acc: 81.25%,  total acc: 53.48%   [EVAL] batch:  131 | acc: 62.50%,  total acc: 53.55%   [EVAL] batch:  132 | acc: 50.00%,  total acc: 53.52%   
cur_acc:  ['0.8580', '0.8056', '0.7812', '0.6920', '0.5433', '0.5083', '0.3906', '0.5057']
his_acc:  ['0.8580', '0.8325', '0.7549', '0.7348', '0.6479', '0.6179', '0.5647', '0.5352']
--------Round  3
seed:  400
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/test.pkl
Task_order: [7 0 1 2 5 3 4 6]
prepared data!
Clustering into  2  clusters
Clusters:  [1 0 0 1 0 1]
Losses:  25.944786071777344 12.69137191772461 -0.0
CurrentTrain: epoch  0, batch     0 | loss: 25.9447861Losses:  24.794252395629883 11.66879653930664 -0.0
CurrentTrain: epoch  0, batch     1 | loss: 24.7942524Losses:  20.6420955657959 7.5251030921936035 -0.0
CurrentTrain: epoch  0, batch     2 | loss: 20.6420956Losses:  22.15937042236328 9.16647720336914 -0.0
CurrentTrain: epoch  0, batch     3 | loss: 22.1593704Losses:  24.542125701904297 11.496562957763672 -0.0
CurrentTrain: epoch  0, batch     4 | loss: 24.5421257Losses:  22.33932113647461 9.435343742370605 -0.0
CurrentTrain: epoch  0, batch     5 | loss: 22.3393211Losses:  21.238218307495117 8.500717163085938 -0.0
CurrentTrain: epoch  0, batch     6 | loss: 21.2382183Losses:  21.73935317993164 8.975708961486816 -0.0
CurrentTrain: epoch  0, batch     7 | loss: 21.7393532Losses:  25.202030181884766 12.505249977111816 -0.0
CurrentTrain: epoch  0, batch     8 | loss: 25.2020302Losses:  19.26060676574707 6.899482250213623 -0.0
CurrentTrain: epoch  0, batch     9 | loss: 19.2606068Losses:  21.477867126464844 9.213793754577637 -0.0
CurrentTrain: epoch  0, batch    10 | loss: 21.4778671Losses:  22.440589904785156 10.159812927246094 -0.0
CurrentTrain: epoch  0, batch    11 | loss: 22.4405899Losses:  21.612991333007812 9.633045196533203 -0.0
CurrentTrain: epoch  0, batch    12 | loss: 21.6129913Losses:  19.462932586669922 7.423742294311523 -0.0
CurrentTrain: epoch  0, batch    13 | loss: 19.4629326Losses:  20.394582748413086 8.40472412109375 -0.0
CurrentTrain: epoch  0, batch    14 | loss: 20.3945827Losses:  20.28620147705078 8.313424110412598 -0.0
CurrentTrain: epoch  0, batch    15 | loss: 20.2862015Losses:  23.38357925415039 11.84827995300293 -0.0
CurrentTrain: epoch  0, batch    16 | loss: 23.3835793Losses:  19.372005462646484 7.86647367477417 -0.0
CurrentTrain: epoch  0, batch    17 | loss: 19.3720055Losses:  19.72007179260254 8.22437858581543 -0.0
CurrentTrain: epoch  0, batch    18 | loss: 19.7200718Losses:  21.397266387939453 10.133407592773438 -0.0
CurrentTrain: epoch  0, batch    19 | loss: 21.3972664Losses:  19.2119197845459 7.858935356140137 -0.0
CurrentTrain: epoch  0, batch    20 | loss: 19.2119198Losses:  23.959205627441406 12.073970794677734 -0.0
CurrentTrain: epoch  0, batch    21 | loss: 23.9592056Losses:  20.317829132080078 8.639928817749023 -0.0
CurrentTrain: epoch  0, batch    22 | loss: 20.3178291Losses:  19.001855850219727 7.729414463043213 -0.0
CurrentTrain: epoch  0, batch    23 | loss: 19.0018559Losses:  18.91952896118164 7.8033857345581055 -0.0
CurrentTrain: epoch  0, batch    24 | loss: 18.9195290Losses:  20.965984344482422 9.753783226013184 -0.0
CurrentTrain: epoch  0, batch    25 | loss: 20.9659843Losses:  18.49085235595703 7.6300129890441895 -0.0
CurrentTrain: epoch  0, batch    26 | loss: 18.4908524Losses:  21.69122886657715 10.126154899597168 -0.0
CurrentTrain: epoch  0, batch    27 | loss: 21.6912289Losses:  22.202247619628906 11.89162540435791 -0.0
CurrentTrain: epoch  0, batch    28 | loss: 22.2022476Losses:  18.558448791503906 7.77642822265625 -0.0
CurrentTrain: epoch  0, batch    29 | loss: 18.5584488Losses:  19.550731658935547 8.871551513671875 -0.0
CurrentTrain: epoch  0, batch    30 | loss: 19.5507317Losses:  16.67699432373047 6.241250038146973 -0.0
CurrentTrain: epoch  0, batch    31 | loss: 16.6769943Losses:  20.61928939819336 9.441366195678711 -0.0
CurrentTrain: epoch  0, batch    32 | loss: 20.6192894Losses:  25.392168045043945 14.430856704711914 -0.0
CurrentTrain: epoch  0, batch    33 | loss: 25.3921680Losses:  16.319107055664062 6.058455467224121 -0.0
CurrentTrain: epoch  0, batch    34 | loss: 16.3191071Losses:  18.112598419189453 7.2761454582214355 -0.0
CurrentTrain: epoch  0, batch    35 | loss: 18.1125984Losses:  18.578285217285156 7.72241735458374 -0.0
CurrentTrain: epoch  0, batch    36 | loss: 18.5782852Losses:  12.608959197998047 2.806671619415283 -0.0
CurrentTrain: epoch  0, batch    37 | loss: 12.6089592Losses:  19.26241683959961 8.882179260253906 -0.0
CurrentTrain: epoch  1, batch     0 | loss: 19.2624168Losses:  17.923961639404297 7.3127121925354 -0.0
CurrentTrain: epoch  1, batch     1 | loss: 17.9239616Losses:  27.480180740356445 16.888023376464844 -0.0
CurrentTrain: epoch  1, batch     2 | loss: 27.4801807Losses:  17.04466438293457 7.476194858551025 -0.0
CurrentTrain: epoch  1, batch     3 | loss: 17.0446644Losses:  16.936546325683594 6.921043395996094 -0.0
CurrentTrain: epoch  1, batch     4 | loss: 16.9365463Losses:  19.992931365966797 10.115880012512207 -0.0
CurrentTrain: epoch  1, batch     5 | loss: 19.9929314Losses:  22.06247329711914 12.280320167541504 -0.0
CurrentTrain: epoch  1, batch     6 | loss: 22.0624733Losses:  18.023094177246094 8.027931213378906 -0.0
CurrentTrain: epoch  1, batch     7 | loss: 18.0230942Losses:  18.839160919189453 8.865455627441406 -0.0
CurrentTrain: epoch  1, batch     8 | loss: 18.8391609Losses:  22.008708953857422 12.432443618774414 -0.0
CurrentTrain: epoch  1, batch     9 | loss: 22.0087090Losses:  18.94853401184082 9.137918472290039 -0.0
CurrentTrain: epoch  1, batch    10 | loss: 18.9485340Losses:  17.00159454345703 7.894606590270996 -0.0
CurrentTrain: epoch  1, batch    11 | loss: 17.0015945Losses:  18.261985778808594 8.469430923461914 -0.0
CurrentTrain: epoch  1, batch    12 | loss: 18.2619858Losses:  16.235939025878906 6.860151767730713 -0.0
CurrentTrain: epoch  1, batch    13 | loss: 16.2359390Losses:  16.704172134399414 7.330691337585449 -0.0
CurrentTrain: epoch  1, batch    14 | loss: 16.7041721Losses:  15.662343978881836 6.069562911987305 -0.0
CurrentTrain: epoch  1, batch    15 | loss: 15.6623440Losses:  15.794783592224121 6.6027679443359375 -0.0
CurrentTrain: epoch  1, batch    16 | loss: 15.7947836Losses:  16.78676986694336 7.753610610961914 -0.0
CurrentTrain: epoch  1, batch    17 | loss: 16.7867699Losses:  18.228517532348633 9.047361373901367 -0.0
CurrentTrain: epoch  1, batch    18 | loss: 18.2285175Losses:  17.23990249633789 8.122800827026367 -0.0
CurrentTrain: epoch  1, batch    19 | loss: 17.2399025Losses:  15.335326194763184 6.789974212646484 -0.0
CurrentTrain: epoch  1, batch    20 | loss: 15.3353262Losses:  15.607874870300293 6.435999870300293 -0.0
CurrentTrain: epoch  1, batch    21 | loss: 15.6078749Losses:  15.652690887451172 6.7046427726745605 -0.0
CurrentTrain: epoch  1, batch    22 | loss: 15.6526909Losses:  17.68501091003418 7.479427337646484 -0.0
CurrentTrain: epoch  1, batch    23 | loss: 17.6850109Losses:  15.770294189453125 6.363986015319824 -0.0
CurrentTrain: epoch  1, batch    24 | loss: 15.7702942Losses:  17.494647979736328 8.138967514038086 -0.0
CurrentTrain: epoch  1, batch    25 | loss: 17.4946480Losses:  14.862581253051758 5.839200496673584 -0.0
CurrentTrain: epoch  1, batch    26 | loss: 14.8625813Losses:  16.121139526367188 7.481193542480469 -0.0
CurrentTrain: epoch  1, batch    27 | loss: 16.1211395Losses:  18.106592178344727 9.061395645141602 -0.0
CurrentTrain: epoch  1, batch    28 | loss: 18.1065922Losses:  15.414251327514648 6.247892379760742 -0.0
CurrentTrain: epoch  1, batch    29 | loss: 15.4142513Losses:  18.783573150634766 9.102730751037598 -0.0
CurrentTrain: epoch  1, batch    30 | loss: 18.7835732Losses:  15.710393905639648 6.920869827270508 -0.0
CurrentTrain: epoch  1, batch    31 | loss: 15.7103939Losses:  16.975126266479492 7.869613170623779 -0.0
CurrentTrain: epoch  1, batch    32 | loss: 16.9751263Losses:  13.838441848754883 5.209543704986572 -0.0
CurrentTrain: epoch  1, batch    33 | loss: 13.8384418Losses:  14.723058700561523 6.370443820953369 -0.0
CurrentTrain: epoch  1, batch    34 | loss: 14.7230587Losses:  13.745131492614746 5.393326759338379 -0.0
CurrentTrain: epoch  1, batch    35 | loss: 13.7451315Losses:  15.480188369750977 7.134803771972656 -0.0
CurrentTrain: epoch  1, batch    36 | loss: 15.4801884Losses:  10.105531692504883 0.9340793490409851 -0.0
CurrentTrain: epoch  1, batch    37 | loss: 10.1055317Losses:  16.171724319458008 7.594388484954834 -0.0
CurrentTrain: epoch  2, batch     0 | loss: 16.1717243Losses:  16.422243118286133 7.369220733642578 -0.0
CurrentTrain: epoch  2, batch     1 | loss: 16.4222431Losses:  14.388980865478516 6.097194671630859 -0.0
CurrentTrain: epoch  2, batch     2 | loss: 14.3889809Losses:  14.841341018676758 7.021091461181641 -0.0
CurrentTrain: epoch  2, batch     3 | loss: 14.8413410Losses:  14.696754455566406 6.423267364501953 -0.0
CurrentTrain: epoch  2, batch     4 | loss: 14.6967545Losses:  12.602383613586426 5.132236480712891 -0.0
CurrentTrain: epoch  2, batch     5 | loss: 12.6023836Losses:  16.071727752685547 7.556252956390381 -0.0
CurrentTrain: epoch  2, batch     6 | loss: 16.0717278Losses:  14.540088653564453 6.162394046783447 -0.0
CurrentTrain: epoch  2, batch     7 | loss: 14.5400887Losses:  14.988712310791016 6.299783229827881 -0.0
CurrentTrain: epoch  2, batch     8 | loss: 14.9887123Losses:  13.760527610778809 6.173876762390137 -0.0
CurrentTrain: epoch  2, batch     9 | loss: 13.7605276Losses:  14.266531944274902 6.170814514160156 -0.0
CurrentTrain: epoch  2, batch    10 | loss: 14.2665319Losses:  16.350448608398438 7.636862754821777 -0.0
CurrentTrain: epoch  2, batch    11 | loss: 16.3504486Losses:  16.210838317871094 7.468510627746582 -0.0
CurrentTrain: epoch  2, batch    12 | loss: 16.2108383Losses:  17.093124389648438 7.618823051452637 -0.0
CurrentTrain: epoch  2, batch    13 | loss: 17.0931244Losses:  12.460939407348633 4.631083011627197 -0.0
CurrentTrain: epoch  2, batch    14 | loss: 12.4609394Losses:  18.728195190429688 10.38922119140625 -0.0
CurrentTrain: epoch  2, batch    15 | loss: 18.7281952Losses:  14.51490306854248 6.3066558837890625 -0.0
CurrentTrain: epoch  2, batch    16 | loss: 14.5149031Losses:  15.990165710449219 8.064014434814453 -0.0
CurrentTrain: epoch  2, batch    17 | loss: 15.9901657Losses:  14.30778980255127 6.062857627868652 -0.0
CurrentTrain: epoch  2, batch    18 | loss: 14.3077898Losses:  11.998610496520996 4.553837776184082 -0.0
CurrentTrain: epoch  2, batch    19 | loss: 11.9986105Losses:  16.93524742126465 9.782732009887695 -0.0
CurrentTrain: epoch  2, batch    20 | loss: 16.9352474Losses:  15.202652931213379 7.300848960876465 -0.0
CurrentTrain: epoch  2, batch    21 | loss: 15.2026529Losses:  13.684791564941406 5.620299816131592 -0.0
CurrentTrain: epoch  2, batch    22 | loss: 13.6847916Losses:  19.73194122314453 10.035480499267578 -0.0
CurrentTrain: epoch  2, batch    23 | loss: 19.7319412Losses:  15.052722930908203 7.025922775268555 -0.0
CurrentTrain: epoch  2, batch    24 | loss: 15.0527229Losses:  17.607898712158203 7.024458885192871 -0.0
CurrentTrain: epoch  2, batch    25 | loss: 17.6078987Losses:  13.220640182495117 5.619832992553711 -0.0
CurrentTrain: epoch  2, batch    26 | loss: 13.2206402Losses:  14.396539688110352 7.473487854003906 -0.0
CurrentTrain: epoch  2, batch    27 | loss: 14.3965397Losses:  16.24690055847168 7.605079174041748 -0.0
CurrentTrain: epoch  2, batch    28 | loss: 16.2469006Losses:  15.949121475219727 7.194573879241943 -0.0
CurrentTrain: epoch  2, batch    29 | loss: 15.9491215Losses:  15.91097640991211 7.782249450683594 -0.0
CurrentTrain: epoch  2, batch    30 | loss: 15.9109764Losses:  19.712417602539062 10.739487648010254 -0.0
CurrentTrain: epoch  2, batch    31 | loss: 19.7124176Losses:  16.949214935302734 9.532485008239746 -0.0
CurrentTrain: epoch  2, batch    32 | loss: 16.9492149Losses:  14.755990982055664 7.094315528869629 -0.0
CurrentTrain: epoch  2, batch    33 | loss: 14.7559910Losses:  15.614402770996094 6.836709976196289 -0.0
CurrentTrain: epoch  2, batch    34 | loss: 15.6144028Losses:  21.00686264038086 13.028999328613281 -0.0
CurrentTrain: epoch  2, batch    35 | loss: 21.0068626Losses:  13.657781600952148 6.071103096008301 -0.0
CurrentTrain: epoch  2, batch    36 | loss: 13.6577816Losses:  9.446684837341309 1.846483826637268 -0.0
CurrentTrain: epoch  2, batch    37 | loss: 9.4466848Losses:  20.759780883789062 13.680107116699219 -0.0
CurrentTrain: epoch  3, batch     0 | loss: 20.7597809Losses:  13.7178316116333 6.874754905700684 -0.0
CurrentTrain: epoch  3, batch     1 | loss: 13.7178316Losses:  16.814199447631836 8.971759796142578 -0.0
CurrentTrain: epoch  3, batch     2 | loss: 16.8141994Losses:  13.333724975585938 5.289487838745117 -0.0
CurrentTrain: epoch  3, batch     3 | loss: 13.3337250Losses:  15.088004112243652 7.75251579284668 -0.0
CurrentTrain: epoch  3, batch     4 | loss: 15.0880041Losses:  15.028428077697754 7.164188861846924 -0.0
CurrentTrain: epoch  3, batch     5 | loss: 15.0284281Losses:  13.165508270263672 5.370896816253662 -0.0
CurrentTrain: epoch  3, batch     6 | loss: 13.1655083Losses:  12.39068603515625 5.389659881591797 -0.0
CurrentTrain: epoch  3, batch     7 | loss: 12.3906860Losses:  15.338979721069336 6.922363758087158 -0.0
CurrentTrain: epoch  3, batch     8 | loss: 15.3389797Losses:  13.80849838256836 6.249289512634277 -0.0
CurrentTrain: epoch  3, batch     9 | loss: 13.8084984Losses:  12.767133712768555 5.118869781494141 -0.0
CurrentTrain: epoch  3, batch    10 | loss: 12.7671337Losses:  16.469070434570312 7.93871545791626 -0.0
CurrentTrain: epoch  3, batch    11 | loss: 16.4690704Losses:  13.226367950439453 4.61722469329834 -0.0
CurrentTrain: epoch  3, batch    12 | loss: 13.2263680Losses:  13.603965759277344 6.760141372680664 -0.0
CurrentTrain: epoch  3, batch    13 | loss: 13.6039658Losses:  16.776636123657227 7.761106491088867 -0.0
CurrentTrain: epoch  3, batch    14 | loss: 16.7766361Losses:  16.22812271118164 7.473952293395996 -0.0
CurrentTrain: epoch  3, batch    15 | loss: 16.2281227Losses:  15.683172225952148 6.172920227050781 -0.0
CurrentTrain: epoch  3, batch    16 | loss: 15.6831722Losses:  13.563693046569824 4.9910478591918945 -0.0
CurrentTrain: epoch  3, batch    17 | loss: 13.5636930Losses:  14.422536849975586 6.501809120178223 -0.0
CurrentTrain: epoch  3, batch    18 | loss: 14.4225368Losses:  18.695697784423828 9.933871269226074 -0.0
CurrentTrain: epoch  3, batch    19 | loss: 18.6956978Losses:  12.65223503112793 5.659691333770752 -0.0
CurrentTrain: epoch  3, batch    20 | loss: 12.6522350Losses:  13.962421417236328 6.593188285827637 -0.0
CurrentTrain: epoch  3, batch    21 | loss: 13.9624214Losses:  13.463492393493652 6.109645843505859 -0.0
CurrentTrain: epoch  3, batch    22 | loss: 13.4634924Losses:  11.69281005859375 4.219567775726318 -0.0
CurrentTrain: epoch  3, batch    23 | loss: 11.6928101Losses:  13.829826354980469 6.220600128173828 -0.0
CurrentTrain: epoch  3, batch    24 | loss: 13.8298264Losses:  13.496150016784668 5.570681571960449 -0.0
CurrentTrain: epoch  3, batch    25 | loss: 13.4961500Losses:  15.67459487915039 7.586071491241455 -0.0
CurrentTrain: epoch  3, batch    26 | loss: 15.6745949Losses:  13.24205207824707 5.847301006317139 -0.0
CurrentTrain: epoch  3, batch    27 | loss: 13.2420521Losses:  13.492965698242188 7.079659938812256 -0.0
CurrentTrain: epoch  3, batch    28 | loss: 13.4929657Losses:  19.193801879882812 9.73391056060791 -0.0
CurrentTrain: epoch  3, batch    29 | loss: 19.1938019Losses:  14.729443550109863 7.6061859130859375 -0.0
CurrentTrain: epoch  3, batch    30 | loss: 14.7294436Losses:  13.930968284606934 6.145901679992676 -0.0
CurrentTrain: epoch  3, batch    31 | loss: 13.9309683Losses:  11.570413589477539 4.106845855712891 -0.0
CurrentTrain: epoch  3, batch    32 | loss: 11.5704136Losses:  14.529233932495117 6.951931953430176 -0.0
CurrentTrain: epoch  3, batch    33 | loss: 14.5292339Losses:  13.732585906982422 5.9749674797058105 -0.0
CurrentTrain: epoch  3, batch    34 | loss: 13.7325859Losses:  12.89004898071289 5.65910005569458 -0.0
CurrentTrain: epoch  3, batch    35 | loss: 12.8900490Losses:  14.016894340515137 6.862851142883301 -0.0
CurrentTrain: epoch  3, batch    36 | loss: 14.0168943Losses:  9.152966499328613 2.0326948165893555 -0.0
CurrentTrain: epoch  3, batch    37 | loss: 9.1529665Losses:  11.319554328918457 4.591951370239258 -0.0
CurrentTrain: epoch  4, batch     0 | loss: 11.3195543Losses:  12.022971153259277 5.760977745056152 -0.0
CurrentTrain: epoch  4, batch     1 | loss: 12.0229712Losses:  13.45861530303955 5.987508773803711 -0.0
CurrentTrain: epoch  4, batch     2 | loss: 13.4586153Losses:  11.76070785522461 4.773880958557129 -0.0
CurrentTrain: epoch  4, batch     3 | loss: 11.7607079Losses:  14.891202926635742 6.7112321853637695 -0.0
CurrentTrain: epoch  4, batch     4 | loss: 14.8912029Losses:  12.438873291015625 4.915958404541016 -0.0
CurrentTrain: epoch  4, batch     5 | loss: 12.4388733Losses:  17.670970916748047 10.18321704864502 -0.0
CurrentTrain: epoch  4, batch     6 | loss: 17.6709709Losses:  11.798990249633789 5.205561637878418 -0.0
CurrentTrain: epoch  4, batch     7 | loss: 11.7989902Losses:  13.667657852172852 7.262117385864258 -0.0
CurrentTrain: epoch  4, batch     8 | loss: 13.6676579Losses:  14.535757064819336 7.497572422027588 -0.0
CurrentTrain: epoch  4, batch     9 | loss: 14.5357571Losses:  13.186016082763672 5.800483226776123 -0.0
CurrentTrain: epoch  4, batch    10 | loss: 13.1860161Losses:  13.38100814819336 6.648420333862305 -0.0
CurrentTrain: epoch  4, batch    11 | loss: 13.3810081Losses:  11.621644973754883 4.751823425292969 -0.0
CurrentTrain: epoch  4, batch    12 | loss: 11.6216450Losses:  15.224419593811035 8.265708923339844 -0.0
CurrentTrain: epoch  4, batch    13 | loss: 15.2244196Losses:  16.83385467529297 11.673097610473633 -0.0
CurrentTrain: epoch  4, batch    14 | loss: 16.8338547Losses:  10.812056541442871 4.658724784851074 -0.0
CurrentTrain: epoch  4, batch    15 | loss: 10.8120565Losses:  12.5865478515625 5.103543281555176 -0.0
CurrentTrain: epoch  4, batch    16 | loss: 12.5865479Losses:  12.673377990722656 5.057467937469482 -0.0
CurrentTrain: epoch  4, batch    17 | loss: 12.6733780Losses:  13.551286697387695 6.764098167419434 -0.0
CurrentTrain: epoch  4, batch    18 | loss: 13.5512867Losses:  15.204228401184082 7.633609771728516 -0.0
CurrentTrain: epoch  4, batch    19 | loss: 15.2042284Losses:  15.35916519165039 6.61248779296875 -0.0
CurrentTrain: epoch  4, batch    20 | loss: 15.3591652Losses:  12.65107250213623 5.74389123916626 -0.0
CurrentTrain: epoch  4, batch    21 | loss: 12.6510725Losses:  14.131447792053223 5.769230842590332 -0.0
CurrentTrain: epoch  4, batch    22 | loss: 14.1314478Losses:  13.778218269348145 6.135660171508789 -0.0
CurrentTrain: epoch  4, batch    23 | loss: 13.7782183Losses:  14.684745788574219 7.518198013305664 -0.0
CurrentTrain: epoch  4, batch    24 | loss: 14.6847458Losses:  13.068706512451172 5.604495048522949 -0.0
CurrentTrain: epoch  4, batch    25 | loss: 13.0687065Losses:  12.878835678100586 4.91963529586792 -0.0
CurrentTrain: epoch  4, batch    26 | loss: 12.8788357Losses:  12.219217300415039 4.724667072296143 -0.0
CurrentTrain: epoch  4, batch    27 | loss: 12.2192173Losses:  13.471595764160156 5.849662780761719 -0.0
CurrentTrain: epoch  4, batch    28 | loss: 13.4715958Losses:  17.526416778564453 9.948891639709473 -0.0
CurrentTrain: epoch  4, batch    29 | loss: 17.5264168Losses:  14.388928413391113 7.093484878540039 -0.0
CurrentTrain: epoch  4, batch    30 | loss: 14.3889284Losses:  10.842220306396484 3.975405216217041 -0.0
CurrentTrain: epoch  4, batch    31 | loss: 10.8422203Losses:  15.280414581298828 7.036962032318115 -0.0
CurrentTrain: epoch  4, batch    32 | loss: 15.2804146Losses:  13.11817741394043 6.61033296585083 -0.0
CurrentTrain: epoch  4, batch    33 | loss: 13.1181774Losses:  14.180215835571289 7.190907001495361 -0.0
CurrentTrain: epoch  4, batch    34 | loss: 14.1802158Losses:  19.16620635986328 13.194135665893555 -0.0
CurrentTrain: epoch  4, batch    35 | loss: 19.1662064Losses:  22.486915588378906 13.653833389282227 -0.0
CurrentTrain: epoch  4, batch    36 | loss: 22.4869156Losses:  8.564729690551758 2.333824396133423 -0.0
CurrentTrain: epoch  4, batch    37 | loss: 8.5647297Losses:  15.706217765808105 7.359208106994629 -0.0
CurrentTrain: epoch  5, batch     0 | loss: 15.7062178Losses:  13.03597640991211 6.284521579742432 -0.0
CurrentTrain: epoch  5, batch     1 | loss: 13.0359764Losses:  11.531435012817383 5.7545881271362305 -0.0
CurrentTrain: epoch  5, batch     2 | loss: 11.5314350Losses:  13.045171737670898 5.569270133972168 -0.0
CurrentTrain: epoch  5, batch     3 | loss: 13.0451717Losses:  11.019397735595703 4.639139175415039 -0.0
CurrentTrain: epoch  5, batch     4 | loss: 11.0193977Losses:  16.016010284423828 8.338117599487305 -0.0
CurrentTrain: epoch  5, batch     5 | loss: 16.0160103Losses:  12.980304718017578 5.382964134216309 -0.0
CurrentTrain: epoch  5, batch     6 | loss: 12.9803047Losses:  12.72874641418457 5.784126281738281 -0.0
CurrentTrain: epoch  5, batch     7 | loss: 12.7287464Losses:  12.851225852966309 6.276192665100098 -0.0
CurrentTrain: epoch  5, batch     8 | loss: 12.8512259Losses:  13.193687438964844 6.511436462402344 -0.0
CurrentTrain: epoch  5, batch     9 | loss: 13.1936874Losses:  12.827900886535645 6.375296592712402 -0.0
CurrentTrain: epoch  5, batch    10 | loss: 12.8279009Losses:  14.356435775756836 8.428770065307617 -0.0
CurrentTrain: epoch  5, batch    11 | loss: 14.3564358Losses:  12.740859985351562 5.208173751831055 -0.0
CurrentTrain: epoch  5, batch    12 | loss: 12.7408600Losses:  12.453776359558105 5.555792331695557 -0.0
CurrentTrain: epoch  5, batch    13 | loss: 12.4537764Losses:  17.820398330688477 10.642400741577148 -0.0
CurrentTrain: epoch  5, batch    14 | loss: 17.8203983Losses:  11.429046630859375 5.841085910797119 -0.0
CurrentTrain: epoch  5, batch    15 | loss: 11.4290466Losses:  13.40127182006836 7.177748680114746 -0.0
CurrentTrain: epoch  5, batch    16 | loss: 13.4012718Losses:  11.939273834228516 6.092310428619385 -0.0
CurrentTrain: epoch  5, batch    17 | loss: 11.9392738Losses:  10.982926368713379 4.425087928771973 -0.0
CurrentTrain: epoch  5, batch    18 | loss: 10.9829264Losses:  10.928470611572266 4.672115802764893 -0.0
CurrentTrain: epoch  5, batch    19 | loss: 10.9284706Losses:  10.107649803161621 3.8673925399780273 -0.0
CurrentTrain: epoch  5, batch    20 | loss: 10.1076498Losses:  12.29984188079834 5.087795257568359 -0.0
CurrentTrain: epoch  5, batch    21 | loss: 12.2998419Losses:  16.379213333129883 10.838401794433594 -0.0
CurrentTrain: epoch  5, batch    22 | loss: 16.3792133Losses:  12.222448348999023 6.130682945251465 -0.0
CurrentTrain: epoch  5, batch    23 | loss: 12.2224483Losses:  13.394365310668945 6.333372116088867 -0.0
CurrentTrain: epoch  5, batch    24 | loss: 13.3943653Losses:  13.181184768676758 5.740822792053223 -0.0
CurrentTrain: epoch  5, batch    25 | loss: 13.1811848Losses:  12.527389526367188 5.203136444091797 -0.0
CurrentTrain: epoch  5, batch    26 | loss: 12.5273895Losses:  15.862330436706543 9.970588684082031 -0.0
CurrentTrain: epoch  5, batch    27 | loss: 15.8623304Losses:  11.703264236450195 4.388456344604492 -0.0
CurrentTrain: epoch  5, batch    28 | loss: 11.7032642Losses:  13.966081619262695 6.326780319213867 -0.0
CurrentTrain: epoch  5, batch    29 | loss: 13.9660816Losses:  12.59122085571289 5.308472633361816 -0.0
CurrentTrain: epoch  5, batch    30 | loss: 12.5912209Losses:  15.323673248291016 8.320398330688477 -0.0
CurrentTrain: epoch  5, batch    31 | loss: 15.3236732Losses:  11.741622924804688 5.477924823760986 -0.0
CurrentTrain: epoch  5, batch    32 | loss: 11.7416229Losses:  11.037647247314453 4.856496810913086 -0.0
CurrentTrain: epoch  5, batch    33 | loss: 11.0376472Losses:  17.402660369873047 9.769962310791016 -0.0
CurrentTrain: epoch  5, batch    34 | loss: 17.4026604Losses:  12.846429824829102 5.832919120788574 -0.0
CurrentTrain: epoch  5, batch    35 | loss: 12.8464298Losses:  13.612079620361328 6.900254726409912 -0.0
CurrentTrain: epoch  5, batch    36 | loss: 13.6120796Losses:  7.773039817810059 1.9290618896484375 -0.0
CurrentTrain: epoch  5, batch    37 | loss: 7.7730398Losses:  14.97313117980957 7.895171165466309 -0.0
CurrentTrain: epoch  6, batch     0 | loss: 14.9731312Losses:  14.289424896240234 7.3408122062683105 -0.0
CurrentTrain: epoch  6, batch     1 | loss: 14.2894249Losses:  11.342514038085938 4.794079780578613 -0.0
CurrentTrain: epoch  6, batch     2 | loss: 11.3425140Losses:  12.408123970031738 6.120758056640625 -0.0
CurrentTrain: epoch  6, batch     3 | loss: 12.4081240Losses:  13.075601577758789 7.547590732574463 -0.0
CurrentTrain: epoch  6, batch     4 | loss: 13.0756016Losses:  12.17271614074707 6.22801399230957 -0.0
CurrentTrain: epoch  6, batch     5 | loss: 12.1727161Losses:  10.84115219116211 5.307572841644287 -0.0
CurrentTrain: epoch  6, batch     6 | loss: 10.8411522Losses:  13.089941024780273 6.1422576904296875 -0.0
CurrentTrain: epoch  6, batch     7 | loss: 13.0899410Losses:  14.069788932800293 6.72075891494751 -0.0
CurrentTrain: epoch  6, batch     8 | loss: 14.0697889Losses:  14.03951644897461 7.522106647491455 -0.0
CurrentTrain: epoch  6, batch     9 | loss: 14.0395164Losses:  12.277009963989258 5.407016754150391 -0.0
CurrentTrain: epoch  6, batch    10 | loss: 12.2770100Losses:  17.382509231567383 9.593721389770508 -0.0
CurrentTrain: epoch  6, batch    11 | loss: 17.3825092Losses:  10.898382186889648 4.682980537414551 -0.0
CurrentTrain: epoch  6, batch    12 | loss: 10.8983822Losses:  13.061944961547852 6.600340366363525 -0.0
CurrentTrain: epoch  6, batch    13 | loss: 13.0619450Losses:  13.367658615112305 7.099759101867676 -0.0
CurrentTrain: epoch  6, batch    14 | loss: 13.3676586Losses:  11.599212646484375 4.941875457763672 -0.0
CurrentTrain: epoch  6, batch    15 | loss: 11.5992126Losses:  12.455121994018555 5.755296230316162 -0.0
CurrentTrain: epoch  6, batch    16 | loss: 12.4551220Losses:  12.713265419006348 5.641823768615723 -0.0
CurrentTrain: epoch  6, batch    17 | loss: 12.7132654Losses:  15.575563430786133 9.44822883605957 -0.0
CurrentTrain: epoch  6, batch    18 | loss: 15.5755634Losses:  12.245677947998047 6.363434314727783 -0.0
CurrentTrain: epoch  6, batch    19 | loss: 12.2456779Losses:  10.216341018676758 3.7681353092193604 -0.0
CurrentTrain: epoch  6, batch    20 | loss: 10.2163410Losses:  11.990584373474121 4.891415596008301 -0.0
CurrentTrain: epoch  6, batch    21 | loss: 11.9905844Losses:  14.412575721740723 8.138014793395996 -0.0
CurrentTrain: epoch  6, batch    22 | loss: 14.4125757Losses:  10.933708190917969 4.0603790283203125 -0.0
CurrentTrain: epoch  6, batch    23 | loss: 10.9337082Losses:  11.623311996459961 5.027912139892578 -0.0
CurrentTrain: epoch  6, batch    24 | loss: 11.6233120Losses:  9.960643768310547 3.6291117668151855 -0.0
CurrentTrain: epoch  6, batch    25 | loss: 9.9606438Losses:  9.63357162475586 3.6758365631103516 -0.0
CurrentTrain: epoch  6, batch    26 | loss: 9.6335716Losses:  14.2940034866333 9.0872220993042 -0.0
CurrentTrain: epoch  6, batch    27 | loss: 14.2940035Losses:  15.450745582580566 9.247217178344727 -0.0
CurrentTrain: epoch  6, batch    28 | loss: 15.4507456Losses:  11.212447166442871 4.556576728820801 -0.0
CurrentTrain: epoch  6, batch    29 | loss: 11.2124472Losses:  12.892202377319336 6.528170585632324 -0.0
CurrentTrain: epoch  6, batch    30 | loss: 12.8922024Losses:  11.61754035949707 5.94759464263916 -0.0
CurrentTrain: epoch  6, batch    31 | loss: 11.6175404Losses:  15.866103172302246 8.294842720031738 -0.0
CurrentTrain: epoch  6, batch    32 | loss: 15.8661032Losses:  11.073699951171875 5.374129772186279 -0.0
CurrentTrain: epoch  6, batch    33 | loss: 11.0737000Losses:  15.122546195983887 8.918144226074219 -0.0
CurrentTrain: epoch  6, batch    34 | loss: 15.1225462Losses:  11.745356559753418 5.093056678771973 -0.0
CurrentTrain: epoch  6, batch    35 | loss: 11.7453566Losses:  10.464645385742188 4.421056747436523 -0.0
CurrentTrain: epoch  6, batch    36 | loss: 10.4646454Losses:  7.633665561676025 1.4327473640441895 -0.0
CurrentTrain: epoch  6, batch    37 | loss: 7.6336656Losses:  10.381269454956055 4.367380142211914 -0.0
CurrentTrain: epoch  7, batch     0 | loss: 10.3812695Losses:  13.903937339782715 7.949145793914795 -0.0
CurrentTrain: epoch  7, batch     1 | loss: 13.9039373Losses:  13.27115249633789 6.6920013427734375 -0.0
CurrentTrain: epoch  7, batch     2 | loss: 13.2711525Losses:  10.594181060791016 4.755175590515137 -0.0
CurrentTrain: epoch  7, batch     3 | loss: 10.5941811Losses:  12.15925407409668 6.798792839050293 -0.0
CurrentTrain: epoch  7, batch     4 | loss: 12.1592541Losses:  12.372406005859375 6.361282825469971 -0.0
CurrentTrain: epoch  7, batch     5 | loss: 12.3724060Losses:  11.579377174377441 5.413631439208984 -0.0
CurrentTrain: epoch  7, batch     6 | loss: 11.5793772Losses:  15.234999656677246 9.161911964416504 -0.0
CurrentTrain: epoch  7, batch     7 | loss: 15.2349997Losses:  10.555752754211426 5.252819061279297 -0.0
CurrentTrain: epoch  7, batch     8 | loss: 10.5557528Losses:  12.17684555053711 5.536571502685547 -0.0
CurrentTrain: epoch  7, batch     9 | loss: 12.1768456Losses:  11.454168319702148 6.030966281890869 -0.0
CurrentTrain: epoch  7, batch    10 | loss: 11.4541683Losses:  10.72708511352539 4.47316837310791 -0.0
CurrentTrain: epoch  7, batch    11 | loss: 10.7270851Losses:  13.908234596252441 7.419816970825195 -0.0
CurrentTrain: epoch  7, batch    12 | loss: 13.9082346Losses:  18.213138580322266 11.620222091674805 -0.0
CurrentTrain: epoch  7, batch    13 | loss: 18.2131386Losses:  12.846090316772461 7.48797082901001 -0.0
CurrentTrain: epoch  7, batch    14 | loss: 12.8460903Losses:  14.692550659179688 7.658869743347168 -0.0
CurrentTrain: epoch  7, batch    15 | loss: 14.6925507Losses:  17.827945709228516 12.240800857543945 -0.0
CurrentTrain: epoch  7, batch    16 | loss: 17.8279457Losses:  10.801980972290039 4.463620662689209 -0.0
CurrentTrain: epoch  7, batch    17 | loss: 10.8019810Losses:  16.412113189697266 10.090484619140625 -0.0
CurrentTrain: epoch  7, batch    18 | loss: 16.4121132Losses:  9.59510326385498 4.004311561584473 -0.0
CurrentTrain: epoch  7, batch    19 | loss: 9.5951033Losses:  11.470048904418945 5.476956367492676 -0.0
CurrentTrain: epoch  7, batch    20 | loss: 11.4700489Losses:  10.943119049072266 4.665148735046387 -0.0
CurrentTrain: epoch  7, batch    21 | loss: 10.9431190Losses:  13.588774681091309 7.945158958435059 -0.0
CurrentTrain: epoch  7, batch    22 | loss: 13.5887747Losses:  10.030223846435547 4.7263503074646 -0.0
CurrentTrain: epoch  7, batch    23 | loss: 10.0302238Losses:  10.724518775939941 5.02397346496582 -0.0
CurrentTrain: epoch  7, batch    24 | loss: 10.7245188Losses:  14.090130805969238 8.139995574951172 -0.0
CurrentTrain: epoch  7, batch    25 | loss: 14.0901308Losses:  9.912566184997559 4.763036727905273 -0.0
CurrentTrain: epoch  7, batch    26 | loss: 9.9125662Losses:  9.955897331237793 4.8983306884765625 -0.0
CurrentTrain: epoch  7, batch    27 | loss: 9.9558973Losses:  11.001941680908203 5.957343578338623 -0.0
CurrentTrain: epoch  7, batch    28 | loss: 11.0019417Losses:  10.40681266784668 4.793243408203125 -0.0
CurrentTrain: epoch  7, batch    29 | loss: 10.4068127Losses:  14.189787864685059 9.286937713623047 -0.0
CurrentTrain: epoch  7, batch    30 | loss: 14.1897879Losses:  11.00523567199707 5.981847763061523 -0.0
CurrentTrain: epoch  7, batch    31 | loss: 11.0052357Losses:  12.80108642578125 7.5257086753845215 -0.0
CurrentTrain: epoch  7, batch    32 | loss: 12.8010864Losses:  10.255379676818848 4.8011860847473145 -0.0
CurrentTrain: epoch  7, batch    33 | loss: 10.2553797Losses:  10.780031204223633 5.683340072631836 -0.0
CurrentTrain: epoch  7, batch    34 | loss: 10.7800312Losses:  9.38406753540039 4.391599178314209 -0.0
CurrentTrain: epoch  7, batch    35 | loss: 9.3840675Losses:  11.557870864868164 6.254485130310059 -0.0
CurrentTrain: epoch  7, batch    36 | loss: 11.5578709Losses:  10.325435638427734 5.621233940124512 -0.0
CurrentTrain: epoch  7, batch    37 | loss: 10.3254356Losses:  11.970544815063477 6.654583930969238 -0.0
CurrentTrain: epoch  8, batch     0 | loss: 11.9705448Losses:  13.533578872680664 7.414746284484863 -0.0
CurrentTrain: epoch  8, batch     1 | loss: 13.5335789Losses:  10.262646675109863 4.933933258056641 -0.0
CurrentTrain: epoch  8, batch     2 | loss: 10.2626467Losses:  10.038108825683594 4.759099960327148 -0.0
CurrentTrain: epoch  8, batch     3 | loss: 10.0381088Losses:  9.279151916503906 4.02308464050293 -0.0
CurrentTrain: epoch  8, batch     4 | loss: 9.2791519Losses:  10.246088027954102 4.798884868621826 -0.0
CurrentTrain: epoch  8, batch     5 | loss: 10.2460880Losses:  10.484877586364746 4.882521152496338 -0.0
CurrentTrain: epoch  8, batch     6 | loss: 10.4848776Losses:  14.708524703979492 7.621208667755127 -0.0
CurrentTrain: epoch  8, batch     7 | loss: 14.7085247Losses:  11.746942520141602 6.273958683013916 -0.0
CurrentTrain: epoch  8, batch     8 | loss: 11.7469425Losses:  12.514694213867188 6.579365253448486 -0.0
CurrentTrain: epoch  8, batch     9 | loss: 12.5146942Losses:  9.723435401916504 4.369571685791016 -0.0
CurrentTrain: epoch  8, batch    10 | loss: 9.7234354Losses:  10.814274787902832 5.271557807922363 -0.0
CurrentTrain: epoch  8, batch    11 | loss: 10.8142748Losses:  10.2467041015625 5.091352462768555 -0.0
CurrentTrain: epoch  8, batch    12 | loss: 10.2467041Losses:  11.781709671020508 5.5518388748168945 -0.0
CurrentTrain: epoch  8, batch    13 | loss: 11.7817097Losses:  10.954818725585938 5.636658191680908 -0.0
CurrentTrain: epoch  8, batch    14 | loss: 10.9548187Losses:  12.0972318649292 6.967744827270508 -0.0
CurrentTrain: epoch  8, batch    15 | loss: 12.0972319Losses:  15.010049819946289 9.167360305786133 -0.0
CurrentTrain: epoch  8, batch    16 | loss: 15.0100498Losses:  9.682126998901367 4.228109359741211 -0.0
CurrentTrain: epoch  8, batch    17 | loss: 9.6821270Losses:  12.581683158874512 7.349753379821777 -0.0
CurrentTrain: epoch  8, batch    18 | loss: 12.5816832Losses:  9.491003036499023 4.234129905700684 -0.0
CurrentTrain: epoch  8, batch    19 | loss: 9.4910030Losses:  11.803887367248535 5.234433174133301 -0.0
CurrentTrain: epoch  8, batch    20 | loss: 11.8038874Losses:  10.846601486206055 5.482573509216309 -0.0
CurrentTrain: epoch  8, batch    21 | loss: 10.8466015Losses:  17.172801971435547 12.053979873657227 -0.0
CurrentTrain: epoch  8, batch    22 | loss: 17.1728020Losses:  10.282390594482422 4.822081089019775 -0.0
CurrentTrain: epoch  8, batch    23 | loss: 10.2823906Losses:  12.381393432617188 7.116158485412598 -0.0
CurrentTrain: epoch  8, batch    24 | loss: 12.3813934Losses:  11.673454284667969 6.034876823425293 -0.0
CurrentTrain: epoch  8, batch    25 | loss: 11.6734543Losses:  11.136492729187012 5.978767395019531 -0.0
CurrentTrain: epoch  8, batch    26 | loss: 11.1364927Losses:  14.465662956237793 8.547203063964844 -0.0
CurrentTrain: epoch  8, batch    27 | loss: 14.4656630Losses:  12.247255325317383 5.23089075088501 -0.0
CurrentTrain: epoch  8, batch    28 | loss: 12.2472553Losses:  12.467263221740723 5.8117828369140625 -0.0
CurrentTrain: epoch  8, batch    29 | loss: 12.4672632Losses:  12.4960298538208 6.152129173278809 -0.0
CurrentTrain: epoch  8, batch    30 | loss: 12.4960299Losses:  11.333932876586914 5.060495376586914 -0.0
CurrentTrain: epoch  8, batch    31 | loss: 11.3339329Losses:  15.943220138549805 9.725078582763672 -0.0
CurrentTrain: epoch  8, batch    32 | loss: 15.9432201Losses:  13.955038070678711 7.63173770904541 -0.0
CurrentTrain: epoch  8, batch    33 | loss: 13.9550381Losses:  10.98367977142334 5.143257141113281 -0.0
CurrentTrain: epoch  8, batch    34 | loss: 10.9836798Losses:  13.586538314819336 5.37468957901001 -0.0
CurrentTrain: epoch  8, batch    35 | loss: 13.5865383Losses:  11.320430755615234 4.798248767852783 -0.0
CurrentTrain: epoch  8, batch    36 | loss: 11.3204308Losses:  7.548012733459473 0.4425680935382843 -0.0
CurrentTrain: epoch  8, batch    37 | loss: 7.5480127Losses:  12.852300643920898 6.159733772277832 -0.0
CurrentTrain: epoch  9, batch     0 | loss: 12.8523006Losses:  14.392723083496094 7.162754535675049 -0.0
CurrentTrain: epoch  9, batch     1 | loss: 14.3927231Losses:  9.71084213256836 3.522601366043091 -0.0
CurrentTrain: epoch  9, batch     2 | loss: 9.7108421Losses:  12.409185409545898 5.115657806396484 -0.0
CurrentTrain: epoch  9, batch     3 | loss: 12.4091854Losses:  11.87667465209961 6.074880123138428 -0.0
CurrentTrain: epoch  9, batch     4 | loss: 11.8766747Losses:  11.52842903137207 5.802962303161621 -0.0
CurrentTrain: epoch  9, batch     5 | loss: 11.5284290Losses:  12.243230819702148 6.046443939208984 -0.0
CurrentTrain: epoch  9, batch     6 | loss: 12.2432308Losses:  12.106790542602539 5.59198522567749 -0.0
CurrentTrain: epoch  9, batch     7 | loss: 12.1067905Losses:  10.462124824523926 4.5952653884887695 -0.0
CurrentTrain: epoch  9, batch     8 | loss: 10.4621248Losses:  14.896014213562012 9.13814640045166 -0.0
CurrentTrain: epoch  9, batch     9 | loss: 14.8960142Losses:  14.729331970214844 9.292049407958984 -0.0
CurrentTrain: epoch  9, batch    10 | loss: 14.7293320Losses:  10.68885612487793 5.214911460876465 -0.0
CurrentTrain: epoch  9, batch    11 | loss: 10.6888561Losses:  12.258533477783203 7.363761901855469 -0.0
CurrentTrain: epoch  9, batch    12 | loss: 12.2585335Losses:  11.221664428710938 5.929729461669922 -0.0
CurrentTrain: epoch  9, batch    13 | loss: 11.2216644Losses:  10.676337242126465 5.527669906616211 -0.0
CurrentTrain: epoch  9, batch    14 | loss: 10.6763372Losses:  11.300987243652344 4.934280872344971 -0.0
CurrentTrain: epoch  9, batch    15 | loss: 11.3009872Losses:  9.541736602783203 4.295471668243408 -0.0
CurrentTrain: epoch  9, batch    16 | loss: 9.5417366Losses:  12.420223236083984 7.269636154174805 -0.0
CurrentTrain: epoch  9, batch    17 | loss: 12.4202232Losses:  11.723037719726562 6.105071544647217 -0.0
CurrentTrain: epoch  9, batch    18 | loss: 11.7230377Losses:  9.467782020568848 4.227742671966553 -0.0
CurrentTrain: epoch  9, batch    19 | loss: 9.4677820Losses:  9.707498550415039 4.346940040588379 -0.0
CurrentTrain: epoch  9, batch    20 | loss: 9.7074986Losses:  11.606422424316406 6.052028656005859 -0.0
CurrentTrain: epoch  9, batch    21 | loss: 11.6064224Losses:  10.114255905151367 5.105883598327637 -0.0
CurrentTrain: epoch  9, batch    22 | loss: 10.1142559Losses:  10.982712745666504 5.606017112731934 -0.0
CurrentTrain: epoch  9, batch    23 | loss: 10.9827127Losses:  9.850732803344727 4.670251846313477 -0.0
CurrentTrain: epoch  9, batch    24 | loss: 9.8507328Losses:  12.598949432373047 7.165224552154541 -0.0
CurrentTrain: epoch  9, batch    25 | loss: 12.5989494Losses:  9.120190620422363 3.932572603225708 -0.0
CurrentTrain: epoch  9, batch    26 | loss: 9.1201906Losses:  16.483367919921875 11.671035766601562 -0.0
CurrentTrain: epoch  9, batch    27 | loss: 16.4833679Losses:  9.809711456298828 4.675811767578125 -0.0
CurrentTrain: epoch  9, batch    28 | loss: 9.8097115Losses:  10.915453910827637 5.402486801147461 -0.0
CurrentTrain: epoch  9, batch    29 | loss: 10.9154539Losses:  12.446450233459473 7.590351104736328 -0.0
CurrentTrain: epoch  9, batch    30 | loss: 12.4464502Losses:  11.691153526306152 6.651087760925293 -0.0
CurrentTrain: epoch  9, batch    31 | loss: 11.6911535Losses:  9.968235969543457 5.214949607849121 -0.0
CurrentTrain: epoch  9, batch    32 | loss: 9.9682360Losses:  11.930557250976562 7.015483856201172 -0.0
CurrentTrain: epoch  9, batch    33 | loss: 11.9305573Losses:  15.241552352905273 10.330097198486328 -0.0
CurrentTrain: epoch  9, batch    34 | loss: 15.2415524Losses:  8.786909103393555 3.9073398113250732 -0.0
CurrentTrain: epoch  9, batch    35 | loss: 8.7869091Losses:  12.368854522705078 7.028687953948975 -0.0
CurrentTrain: epoch  9, batch    36 | loss: 12.3688545Losses:  7.283613681793213 2.4275755882263184 -0.0
CurrentTrain: epoch  9, batch    37 | loss: 7.2836137
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 85.94%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 86.25%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 84.38%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 85.71%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 88.19%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 88.75%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 89.77%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 90.10%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 90.38%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 88.84%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 87.92%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 85.94%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 85.29%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 84.03%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 83.22%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 83.12%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 83.93%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 84.66%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 85.33%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 85.94%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 86.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 87.02%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 87.27%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 87.72%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 88.15%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 87.92%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 87.90%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 88.09%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 86.55%   
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 85.94%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 86.25%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 84.38%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 85.71%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 88.19%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 88.75%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 89.77%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 90.10%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 90.38%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 88.84%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 87.92%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 85.94%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 85.29%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 84.03%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 83.22%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 83.12%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 83.93%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 84.66%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 85.33%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 85.94%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 86.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 87.02%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 87.27%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 87.72%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 88.15%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 87.92%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 87.90%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 88.09%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 86.55%   
cur_acc:  ['0.8655']
his_acc:  ['0.8655']
Clustering into  4  clusters
Clusters:  [2 0 1 2 0 2 2 0 1 3 0]
Losses:  19.38208770751953 8.107419967651367 3.7774012088775635
CurrentTrain: epoch  0, batch     0 | loss: 19.3820877Losses:  12.80797290802002 2.032467842102051 3.7426767349243164
CurrentTrain: epoch  0, batch     1 | loss: 12.8079729Losses:  19.689191818237305 9.276893615722656 3.6771864891052246
CurrentTrain: epoch  1, batch     0 | loss: 19.6891918Losses:  9.536873817443848 2.5188629627227783 1.4706825017929077
CurrentTrain: epoch  1, batch     1 | loss: 9.5368738Losses:  16.360523223876953 7.455571174621582 3.460336208343506
CurrentTrain: epoch  2, batch     0 | loss: 16.3605232Losses:  12.866415023803711 3.75100040435791 3.4286465644836426
CurrentTrain: epoch  2, batch     1 | loss: 12.8664150Losses:  14.70414924621582 6.435636520385742 3.398662567138672
CurrentTrain: epoch  3, batch     0 | loss: 14.7041492Losses:  11.266246795654297 2.454512119293213 3.378458023071289
CurrentTrain: epoch  3, batch     1 | loss: 11.2662468Losses:  14.192425727844238 6.395652770996094 3.366564989089966
CurrentTrain: epoch  4, batch     0 | loss: 14.1924257Losses:  11.785118103027344 2.7289581298828125 3.536989212036133
CurrentTrain: epoch  4, batch     1 | loss: 11.7851181Losses:  17.311376571655273 8.807741165161133 3.4478812217712402
CurrentTrain: epoch  5, batch     0 | loss: 17.3113766Losses:  7.937885284423828 2.7731995582580566 1.41326904296875
CurrentTrain: epoch  5, batch     1 | loss: 7.9378853Losses:  13.40744400024414 6.072221755981445 3.363478183746338
CurrentTrain: epoch  6, batch     0 | loss: 13.4074440Losses:  11.952125549316406 3.197087287902832 3.4578185081481934
CurrentTrain: epoch  6, batch     1 | loss: 11.9521255Losses:  15.215084075927734 7.644859313964844 3.492448091506958
CurrentTrain: epoch  7, batch     0 | loss: 15.2150841Losses:  8.540401458740234 2.4649925231933594 3.324817180633545
CurrentTrain: epoch  7, batch     1 | loss: 8.5404015Losses:  12.849555969238281 6.290910243988037 3.3577020168304443
CurrentTrain: epoch  8, batch     0 | loss: 12.8495560Losses:  9.160035133361816 2.298093795776367 3.456357955932617
CurrentTrain: epoch  8, batch     1 | loss: 9.1600351Losses:  11.901839256286621 5.999163627624512 3.3036835193634033
CurrentTrain: epoch  9, batch     0 | loss: 11.9018393Losses:  10.187507629394531 2.8637120723724365 3.355835437774658
CurrentTrain: epoch  9, batch     1 | loss: 10.1875076
Losses:  7.23653507232666 -0.0 3.409395217895508
MemoryTrain:  epoch  0, batch     0 | loss: 7.2365351Losses:  7.114774227142334 -0.0 3.402139186859131
MemoryTrain:  epoch  1, batch     0 | loss: 7.1147742Losses:  6.787092208862305 -0.0 3.3830347061157227
MemoryTrain:  epoch  2, batch     0 | loss: 6.7870922Losses:  6.465373992919922 -0.0 3.3751604557037354
MemoryTrain:  epoch  3, batch     0 | loss: 6.4653740Losses:  6.13851261138916 -0.0 3.3579511642456055
MemoryTrain:  epoch  4, batch     0 | loss: 6.1385126Losses:  5.80346155166626 -0.0 3.3426125049591064
MemoryTrain:  epoch  5, batch     0 | loss: 5.8034616Losses:  5.818661689758301 -0.0 3.337763786315918
MemoryTrain:  epoch  6, batch     0 | loss: 5.8186617Losses:  5.790380477905273 -0.0 3.3589210510253906
MemoryTrain:  epoch  7, batch     0 | loss: 5.7903805Losses:  5.720696449279785 -0.0 3.3394289016723633
MemoryTrain:  epoch  8, batch     0 | loss: 5.7206964Losses:  5.722563743591309 -0.0 3.3339905738830566
MemoryTrain:  epoch  9, batch     0 | loss: 5.7225637
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 18.75%,  total acc: 62.50%   [EVAL] batch:    3 | acc: 0.00%,  total acc: 46.88%   [EVAL] batch:    4 | acc: 0.00%,  total acc: 37.50%   [EVAL] batch:    5 | acc: 0.00%,  total acc: 31.25%   [EVAL] batch:    6 | acc: 0.00%,  total acc: 26.79%   [EVAL] batch:    7 | acc: 0.00%,  total acc: 23.44%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 50.00%,  total acc: 59.38%   [EVAL] batch:    2 | acc: 68.75%,  total acc: 62.50%   [EVAL] batch:    3 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 63.75%   [EVAL] batch:    5 | acc: 50.00%,  total acc: 61.46%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 63.39%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 67.97%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 70.83%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 73.12%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 75.57%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 77.08%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 77.40%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 76.79%   [EVAL] batch:   14 | acc: 62.50%,  total acc: 75.83%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 74.61%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 74.63%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 73.96%   [EVAL] batch:   18 | acc: 50.00%,  total acc: 72.70%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 72.81%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 74.11%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 75.28%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 76.36%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 77.34%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 78.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 79.09%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 79.63%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 80.36%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 81.03%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 81.45%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 81.84%   [EVAL] batch:   32 | acc: 75.00%,  total acc: 81.63%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 82.17%   [EVAL] batch:   34 | acc: 37.50%,  total acc: 80.89%   [EVAL] batch:   35 | acc: 12.50%,  total acc: 78.99%   [EVAL] batch:   36 | acc: 0.00%,  total acc: 76.86%   [EVAL] batch:   37 | acc: 0.00%,  total acc: 74.84%   [EVAL] batch:   38 | acc: 0.00%,  total acc: 72.92%   [EVAL] batch:   39 | acc: 0.00%,  total acc: 71.09%   
cur_acc:  ['0.8655', '0.2344']
his_acc:  ['0.8655', '0.7109']
Clustering into  7  clusters
Clusters:  [1 0 3 1 0 1 1 6 5 2 0 2 4 1 3 0]
Losses:  18.161659240722656 7.397647857666016 5.658412456512451
CurrentTrain: epoch  0, batch     0 | loss: 18.1616592Losses:  13.724321365356445 2.3514833450317383 5.770007133483887
CurrentTrain: epoch  0, batch     1 | loss: 13.7243214Losses:  15.657648086547852 8.489418029785156 3.322791814804077
CurrentTrain: epoch  1, batch     0 | loss: 15.6576481Losses:  14.646425247192383 6.52392053604126 3.367650032043457
CurrentTrain: epoch  1, batch     1 | loss: 14.6464252Losses:  15.679697036743164 6.445293426513672 5.618394374847412
CurrentTrain: epoch  2, batch     0 | loss: 15.6796970Losses:  9.919581413269043 1.310293436050415 5.66091775894165
CurrentTrain: epoch  2, batch     1 | loss: 9.9195814Losses:  15.323785781860352 6.388713359832764 5.575767517089844
CurrentTrain: epoch  3, batch     0 | loss: 15.3237858Losses:  10.362234115600586 2.0232198238372803 5.592347145080566
CurrentTrain: epoch  3, batch     1 | loss: 10.3622341Losses:  14.62803840637207 6.083557605743408 5.5849199295043945
CurrentTrain: epoch  4, batch     0 | loss: 14.6280384Losses:  9.667686462402344 1.8842947483062744 5.588653564453125
CurrentTrain: epoch  4, batch     1 | loss: 9.6676865Losses:  13.570808410644531 5.656563758850098 5.578498840332031
CurrentTrain: epoch  5, batch     0 | loss: 13.5708084Losses:  9.707253456115723 1.8759081363677979 5.574967384338379
CurrentTrain: epoch  5, batch     1 | loss: 9.7072535Losses:  13.143251419067383 5.217759132385254 5.570002555847168
CurrentTrain: epoch  6, batch     0 | loss: 13.1432514Losses:  8.884140014648438 1.175614595413208 5.578259468078613
CurrentTrain: epoch  6, batch     1 | loss: 8.8841400Losses:  13.199882507324219 5.573736190795898 5.572836399078369
CurrentTrain: epoch  7, batch     0 | loss: 13.1998825Losses:  9.102850914001465 1.7097448110580444 5.563382148742676
CurrentTrain: epoch  7, batch     1 | loss: 9.1028509Losses:  13.855718612670898 6.435956001281738 5.5576558113098145
CurrentTrain: epoch  8, batch     0 | loss: 13.8557186Losses:  10.321388244628906 2.937051296234131 5.561813831329346
CurrentTrain: epoch  8, batch     1 | loss: 10.3213882Losses:  13.909605026245117 6.457056522369385 5.572152614593506
CurrentTrain: epoch  9, batch     0 | loss: 13.9096050Losses:  7.613739490509033 2.485893487930298 3.304102897644043
CurrentTrain: epoch  9, batch     1 | loss: 7.6137395
Losses:  15.743842124938965 -0.0 11.068028450012207
MemoryTrain:  epoch  0, batch     0 | loss: 15.7438421Losses:  15.450675964355469 -0.0 10.998744010925293
MemoryTrain:  epoch  1, batch     0 | loss: 15.4506760Losses:  14.816591262817383 -0.0 10.947427749633789
MemoryTrain:  epoch  2, batch     0 | loss: 14.8165913Losses:  14.521516799926758 -0.0 10.937626838684082
MemoryTrain:  epoch  3, batch     0 | loss: 14.5215168Losses:  14.212997436523438 -0.0 10.9636869430542
MemoryTrain:  epoch  4, batch     0 | loss: 14.2129974Losses:  13.92863941192627 -0.0 10.881511688232422
MemoryTrain:  epoch  5, batch     0 | loss: 13.9286394Losses:  13.644159317016602 -0.0 10.936902046203613
MemoryTrain:  epoch  6, batch     0 | loss: 13.6441593Losses:  13.21204948425293 -0.0 10.892818450927734
MemoryTrain:  epoch  7, batch     0 | loss: 13.2120495Losses:  12.852202415466309 -0.0 10.865720748901367
MemoryTrain:  epoch  8, batch     0 | loss: 12.8522024Losses:  12.724091529846191 -0.0 10.870089530944824
MemoryTrain:  epoch  9, batch     0 | loss: 12.7240915
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:    3 | acc: 100.00%,  total acc: 92.19%   [EVAL] batch:    4 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 94.79%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 95.54%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 96.09%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 94.44%   [EVAL] batch:    9 | acc: 50.00%,  total acc: 90.00%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 89.20%   [EVAL] batch:   11 | acc: 31.25%,  total acc: 84.38%   [EVAL] batch:   12 | acc: 18.75%,  total acc: 79.33%   [EVAL] batch:   13 | acc: 18.75%,  total acc: 75.00%   
[EVAL] batch:    0 | acc: 56.25%,  total acc: 56.25%   [EVAL] batch:    1 | acc: 12.50%,  total acc: 34.38%   [EVAL] batch:    2 | acc: 25.00%,  total acc: 31.25%   [EVAL] batch:    3 | acc: 25.00%,  total acc: 29.69%   [EVAL] batch:    4 | acc: 56.25%,  total acc: 35.00%   [EVAL] batch:    5 | acc: 25.00%,  total acc: 33.33%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 39.29%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 46.88%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 52.08%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 56.25%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 60.23%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 63.02%   [EVAL] batch:   12 | acc: 50.00%,  total acc: 62.02%   [EVAL] batch:   13 | acc: 25.00%,  total acc: 59.38%   [EVAL] batch:   14 | acc: 56.25%,  total acc: 59.17%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 58.98%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 59.93%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 60.07%   [EVAL] batch:   18 | acc: 50.00%,  total acc: 59.54%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 60.31%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 62.20%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 63.92%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 65.22%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 66.67%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 68.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 69.23%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 70.14%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 71.21%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 72.20%   [EVAL] batch:   29 | acc: 93.75%,  total acc: 72.92%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 73.79%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 74.61%   [EVAL] batch:   32 | acc: 75.00%,  total acc: 74.62%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 75.37%   [EVAL] batch:   34 | acc: 37.50%,  total acc: 74.29%   [EVAL] batch:   35 | acc: 12.50%,  total acc: 72.57%   [EVAL] batch:   36 | acc: 0.00%,  total acc: 70.61%   [EVAL] batch:   37 | acc: 0.00%,  total acc: 68.75%   [EVAL] batch:   38 | acc: 0.00%,  total acc: 66.99%   [EVAL] batch:   39 | acc: 43.75%,  total acc: 66.41%   [EVAL] batch:   40 | acc: 81.25%,  total acc: 66.77%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 67.41%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 68.17%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 68.89%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 69.58%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 70.24%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 70.88%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 71.48%   [EVAL] batch:   48 | acc: 50.00%,  total acc: 71.05%   [EVAL] batch:   49 | acc: 75.00%,  total acc: 71.12%   [EVAL] batch:   50 | acc: 50.00%,  total acc: 70.71%   [EVAL] batch:   51 | acc: 31.25%,  total acc: 69.95%   [EVAL] batch:   52 | acc: 18.75%,  total acc: 68.99%   [EVAL] batch:   53 | acc: 6.25%,  total acc: 67.82%   
cur_acc:  ['0.8655', '0.2344', '0.7500']
his_acc:  ['0.8655', '0.7109', '0.6782']
Clustering into  9  clusters
Clusters:  [1 0 3 1 0 1 1 4 5 2 0 2 6 1 3 0 1 8 1 0 7]
Losses:  17.743961334228516 7.899608135223389 3.4472849369049072
CurrentTrain: epoch  0, batch     0 | loss: 17.7439613Losses:  12.612990379333496 2.9593095779418945 3.4210054874420166
CurrentTrain: epoch  0, batch     1 | loss: 12.6129904Losses:  18.957910537719727 9.46511459350586 3.3962435722351074
CurrentTrain: epoch  1, batch     0 | loss: 18.9579105Losses:  10.993790626525879 3.366314649581909 3.4042606353759766
CurrentTrain: epoch  1, batch     1 | loss: 10.9937906Losses:  15.784795761108398 7.9448394775390625 3.3830342292785645
CurrentTrain: epoch  2, batch     0 | loss: 15.7847958Losses:  12.458953857421875 3.43966007232666 3.3732872009277344
CurrentTrain: epoch  2, batch     1 | loss: 12.4589539Losses:  17.129899978637695 9.38946533203125 3.348236560821533
CurrentTrain: epoch  3, batch     0 | loss: 17.1299000Losses:  12.117880821228027 4.889492511749268 3.319791555404663
CurrentTrain: epoch  3, batch     1 | loss: 12.1178808Losses:  14.46574592590332 7.223891258239746 3.34360933303833
CurrentTrain: epoch  4, batch     0 | loss: 14.4657459Losses:  9.264530181884766 2.1830501556396484 3.3356924057006836
CurrentTrain: epoch  4, batch     1 | loss: 9.2645302Losses:  14.71101188659668 7.571579456329346 3.3608546257019043
CurrentTrain: epoch  5, batch     0 | loss: 14.7110119Losses:  8.901143074035645 2.5594398975372314 3.3321926593780518
CurrentTrain: epoch  5, batch     1 | loss: 8.9011431Losses:  13.883987426757812 7.465765476226807 3.3418679237365723
CurrentTrain: epoch  6, batch     0 | loss: 13.8839874Losses:  9.87218952178955 3.5621261596679688 3.3195576667785645
CurrentTrain: epoch  6, batch     1 | loss: 9.8721895Losses:  12.787014961242676 6.823596000671387 3.3143091201782227
CurrentTrain: epoch  7, batch     0 | loss: 12.7870150Losses:  8.157833099365234 2.0309815406799316 3.3422060012817383
CurrentTrain: epoch  7, batch     1 | loss: 8.1578331Losses:  13.197385787963867 7.075325012207031 3.331746816635132
CurrentTrain: epoch  8, batch     0 | loss: 13.1973858Losses:  8.06579875946045 2.2811481952667236 3.3517496585845947
CurrentTrain: epoch  8, batch     1 | loss: 8.0657988Losses:  12.676785469055176 6.582967758178711 3.340151786804199
CurrentTrain: epoch  9, batch     0 | loss: 12.6767855Losses:  7.409499645233154 1.8986890316009521 3.317431926727295
CurrentTrain: epoch  9, batch     1 | loss: 7.4094996
Losses:  10.705860137939453 -0.0 8.204277038574219
MemoryTrain:  epoch  0, batch     0 | loss: 10.7058601Losses:  5.692722320556641 -0.0 5.598833084106445
MemoryTrain:  epoch  0, batch     1 | loss: 5.6927223Losses:  18.508039474487305 -0.0 16.771644592285156
MemoryTrain:  epoch  1, batch     0 | loss: 18.5080395Losses:  3.80021071434021 -0.0 1.4844543933868408
MemoryTrain:  epoch  1, batch     1 | loss: 3.8002107Losses:  14.938118934631348 -0.0 13.779500007629395
MemoryTrain:  epoch  2, batch     0 | loss: 14.9381189Losses:  6.98452091217041 -0.0 5.579982757568359
MemoryTrain:  epoch  2, batch     1 | loss: 6.9845209Losses:  14.3817777633667 -0.0 13.735599517822266
MemoryTrain:  epoch  3, batch     0 | loss: 14.3817778Losses:  6.680886268615723 -0.0 5.561580181121826
MemoryTrain:  epoch  3, batch     1 | loss: 6.6808863Losses:  17.03972053527832 -0.0 16.731393814086914
MemoryTrain:  epoch  4, batch     0 | loss: 17.0397205Losses:  0.9540560841560364 -0.0 -0.0
MemoryTrain:  epoch  4, batch     1 | loss: 0.9540561Losses:  11.191746711730957 -0.0 10.845684051513672
MemoryTrain:  epoch  5, batch     0 | loss: 11.1917467Losses:  1.4242794513702393 -0.0 1.3972063064575195
MemoryTrain:  epoch  5, batch     1 | loss: 1.4242795Losses:  11.245415687561035 -0.0 10.830961227416992
MemoryTrain:  epoch  6, batch     0 | loss: 11.2454157Losses:  5.733580112457275 -0.0 5.643040180206299
MemoryTrain:  epoch  6, batch     1 | loss: 5.7335801Losses:  16.987775802612305 -0.0 16.763437271118164
MemoryTrain:  epoch  7, batch     0 | loss: 16.9877758Losses:  2.655884027481079 -0.0 1.387130618095398
MemoryTrain:  epoch  7, batch     1 | loss: 2.6558840Losses:  13.948296546936035 -0.0 13.69368839263916
MemoryTrain:  epoch  8, batch     0 | loss: 13.9482965Losses:  1.6817160844802856 -0.0 1.415126085281372
MemoryTrain:  epoch  8, batch     1 | loss: 1.6817161Losses:  5.712541580200195 -0.0 5.585484504699707
MemoryTrain:  epoch  9, batch     0 | loss: 5.7125416Losses:  6.063154697418213 -0.0 5.575572490692139
MemoryTrain:  epoch  9, batch     1 | loss: 6.0631547
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 56.25%,  total acc: 76.56%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 76.25%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 76.04%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 78.57%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 79.69%   [EVAL] batch:    8 | acc: 43.75%,  total acc: 75.69%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 78.12%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 78.98%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 80.73%   [EVAL] batch:   12 | acc: 100.00%,  total acc: 82.21%   [EVAL] batch:   13 | acc: 100.00%,  total acc: 83.48%   [EVAL] batch:   14 | acc: 100.00%,  total acc: 84.58%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 85.55%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 86.40%   [EVAL] batch:   17 | acc: 25.00%,  total acc: 82.99%   
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 62.50%,  total acc: 75.00%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 77.08%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 76.56%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 77.50%   [EVAL] batch:    5 | acc: 68.75%,  total acc: 76.04%   [EVAL] batch:    6 | acc: 37.50%,  total acc: 70.54%   [EVAL] batch:    7 | acc: 50.00%,  total acc: 67.97%   [EVAL] batch:    8 | acc: 50.00%,  total acc: 65.97%   [EVAL] batch:    9 | acc: 43.75%,  total acc: 63.75%   [EVAL] batch:   10 | acc: 68.75%,  total acc: 64.20%   [EVAL] batch:   11 | acc: 25.00%,  total acc: 60.94%   [EVAL] batch:   12 | acc: 25.00%,  total acc: 58.17%   [EVAL] batch:   13 | acc: 25.00%,  total acc: 55.80%   [EVAL] batch:   14 | acc: 62.50%,  total acc: 56.25%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 56.25%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 57.35%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 57.64%   [EVAL] batch:   18 | acc: 56.25%,  total acc: 57.57%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 58.44%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 60.42%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 62.22%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 63.59%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 65.10%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 66.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 67.79%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 68.75%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 69.87%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 70.91%   [EVAL] batch:   29 | acc: 93.75%,  total acc: 71.67%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 72.58%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 73.24%   [EVAL] batch:   32 | acc: 62.50%,  total acc: 72.92%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 73.71%   [EVAL] batch:   34 | acc: 37.50%,  total acc: 72.68%   [EVAL] batch:   35 | acc: 56.25%,  total acc: 72.22%   [EVAL] batch:   36 | acc: 56.25%,  total acc: 71.79%   [EVAL] batch:   37 | acc: 37.50%,  total acc: 70.89%   [EVAL] batch:   38 | acc: 50.00%,  total acc: 70.35%   [EVAL] batch:   39 | acc: 68.75%,  total acc: 70.31%   [EVAL] batch:   40 | acc: 87.50%,  total acc: 70.73%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 71.28%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 71.95%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 72.59%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 73.19%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 73.78%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 74.34%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 74.87%   [EVAL] batch:   48 | acc: 50.00%,  total acc: 74.36%   [EVAL] batch:   49 | acc: 81.25%,  total acc: 74.50%   [EVAL] batch:   50 | acc: 37.50%,  total acc: 73.77%   [EVAL] batch:   51 | acc: 25.00%,  total acc: 72.84%   [EVAL] batch:   52 | acc: 18.75%,  total acc: 71.82%   [EVAL] batch:   53 | acc: 68.75%,  total acc: 71.76%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 72.05%   [EVAL] batch:   55 | acc: 81.25%,  total acc: 72.21%   [EVAL] batch:   56 | acc: 62.50%,  total acc: 72.04%   [EVAL] batch:   57 | acc: 62.50%,  total acc: 71.88%   [EVAL] batch:   58 | acc: 75.00%,  total acc: 71.93%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 72.29%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 72.75%   [EVAL] batch:   61 | acc: 31.25%,  total acc: 72.08%   [EVAL] batch:   62 | acc: 100.00%,  total acc: 72.52%   [EVAL] batch:   63 | acc: 93.75%,  total acc: 72.85%   [EVAL] batch:   64 | acc: 93.75%,  total acc: 73.17%   [EVAL] batch:   65 | acc: 100.00%,  total acc: 73.58%   [EVAL] batch:   66 | acc: 100.00%,  total acc: 73.97%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 74.36%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 74.73%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 75.09%   [EVAL] batch:   70 | acc: 50.00%,  total acc: 74.74%   
cur_acc:  ['0.8655', '0.2344', '0.7500', '0.8299']
his_acc:  ['0.8655', '0.7109', '0.6782', '0.7474']
Clustering into  12  clusters
Clusters:  [ 0  1  7  0  1  0  0 11  6  2  1  2  5  0  7  3  0 10  0  1  8  9  3  0
  4  0]
Losses:  16.185352325439453 8.523075103759766 3.3628764152526855
CurrentTrain: epoch  0, batch     0 | loss: 16.1853523Losses:  9.27294921875 3.836613893508911 1.400542140007019
CurrentTrain: epoch  0, batch     1 | loss: 9.2729492Losses:  14.577478408813477 7.945180416107178 3.352255344390869
CurrentTrain: epoch  1, batch     0 | loss: 14.5774784Losses:  7.7727837562561035 3.489640474319458 1.4246940612792969
CurrentTrain: epoch  1, batch     1 | loss: 7.7727838Losses:  13.402934074401855 7.236907005310059 3.3775336742401123
CurrentTrain: epoch  2, batch     0 | loss: 13.4029341Losses:  9.403040885925293 3.3307669162750244 3.326551675796509
CurrentTrain: epoch  2, batch     1 | loss: 9.4030409Losses:  13.296436309814453 7.509664535522461 3.3189003467559814
CurrentTrain: epoch  3, batch     0 | loss: 13.2964363Losses:  9.20864200592041 3.4424729347229004 3.334730386734009
CurrentTrain: epoch  3, batch     1 | loss: 9.2086420Losses:  11.747676849365234 6.022854328155518 3.3451108932495117
CurrentTrain: epoch  4, batch     0 | loss: 11.7476768Losses:  7.3714518547058105 1.8772426843643188 3.3576645851135254
CurrentTrain: epoch  4, batch     1 | loss: 7.3714519Losses:  12.48952579498291 6.920996189117432 3.34826922416687
CurrentTrain: epoch  5, batch     0 | loss: 12.4895258Losses:  5.950322151184082 2.572375535964966 1.4132955074310303
CurrentTrain: epoch  5, batch     1 | loss: 5.9503222Losses:  11.543210983276367 6.2491774559021 3.33733868598938
CurrentTrain: epoch  6, batch     0 | loss: 11.5432110Losses:  5.26943826675415 2.011969804763794 1.4115700721740723
CurrentTrain: epoch  6, batch     1 | loss: 5.2694383Losses:  11.5515775680542 6.394218444824219 3.345292806625366
CurrentTrain: epoch  7, batch     0 | loss: 11.5515776Losses:  8.264986038208008 3.0360231399536133 3.3327722549438477
CurrentTrain: epoch  7, batch     1 | loss: 8.2649860Losses:  12.218536376953125 7.081265449523926 3.329580783843994
CurrentTrain: epoch  8, batch     0 | loss: 12.2185364Losses:  7.237975120544434 4.005460262298584 1.4097955226898193
CurrentTrain: epoch  8, batch     1 | loss: 7.2379751Losses:  10.646167755126953 5.545820713043213 3.336573600769043
CurrentTrain: epoch  9, batch     0 | loss: 10.6461678Losses:  6.713919639587402 1.6875587701797485 3.3050451278686523
CurrentTrain: epoch  9, batch     1 | loss: 6.7139196
Losses:  21.329227447509766 -0.0 19.990530014038086
MemoryTrain:  epoch  0, batch     0 | loss: 21.3292274Losses:  10.399275779724121 -0.0 8.161404609680176
MemoryTrain:  epoch  0, batch     1 | loss: 10.3992758Losses:  19.26262664794922 -0.0 16.98178482055664
MemoryTrain:  epoch  1, batch     0 | loss: 19.2626266Losses:  12.180777549743652 -0.0 10.877103805541992
MemoryTrain:  epoch  1, batch     1 | loss: 12.1807775Losses:  17.886911392211914 -0.0 16.74297523498535
MemoryTrain:  epoch  2, batch     0 | loss: 17.8869114Losses:  10.36596965789795 -0.0 8.106966018676758
MemoryTrain:  epoch  2, batch     1 | loss: 10.3659697Losses:  21.426729202270508 -0.0 20.144054412841797
MemoryTrain:  epoch  3, batch     0 | loss: 21.4267292Losses:  9.547772407531738 -0.0 8.100454330444336
MemoryTrain:  epoch  3, batch     1 | loss: 9.5477724Losses:  15.555707931518555 -0.0 13.983575820922852
MemoryTrain:  epoch  4, batch     0 | loss: 15.5557079Losses:  11.313727378845215 -0.0 10.78557300567627
MemoryTrain:  epoch  4, batch     1 | loss: 11.3137274Losses:  18.1292724609375 -0.0 16.934919357299805
MemoryTrain:  epoch  5, batch     0 | loss: 18.1292725Losses:  8.87658405303955 -0.0 8.152231216430664
MemoryTrain:  epoch  5, batch     1 | loss: 8.8765841Losses:  17.402606964111328 -0.0 16.7078857421875
MemoryTrain:  epoch  6, batch     0 | loss: 17.4026070Losses:  8.918442726135254 -0.0 8.093286514282227
MemoryTrain:  epoch  6, batch     1 | loss: 8.9184427Losses:  23.61065101623535 -0.0 23.152130126953125
MemoryTrain:  epoch  7, batch     0 | loss: 23.6106510Losses:  3.997467041015625 -0.0 3.3443057537078857
MemoryTrain:  epoch  7, batch     1 | loss: 3.9974670Losses:  17.090373992919922 -0.0 16.701547622680664
MemoryTrain:  epoch  8, batch     0 | loss: 17.0903740Losses:  6.226707458496094 -0.0 5.5749430656433105
MemoryTrain:  epoch  8, batch     1 | loss: 6.2267075Losses:  20.680225372314453 -0.0 19.985675811767578
MemoryTrain:  epoch  9, batch     0 | loss: 20.6802254Losses:  10.890682220458984 -0.0 10.83448600769043
MemoryTrain:  epoch  9, batch     1 | loss: 10.8906822
[EVAL] batch:    0 | acc: 18.75%,  total acc: 18.75%   [EVAL] batch:    1 | acc: 50.00%,  total acc: 34.38%   [EVAL] batch:    2 | acc: 37.50%,  total acc: 35.42%   [EVAL] batch:    3 | acc: 62.50%,  total acc: 42.19%   [EVAL] batch:    4 | acc: 43.75%,  total acc: 42.50%   [EVAL] batch:    5 | acc: 50.00%,  total acc: 43.75%   [EVAL] batch:    6 | acc: 62.50%,  total acc: 46.43%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 51.56%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 53.47%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 56.88%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 59.66%   [EVAL] batch:   11 | acc: 68.75%,  total acc: 60.42%   [EVAL] batch:   12 | acc: 43.75%,  total acc: 59.13%   
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 68.75%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 77.08%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 76.56%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 78.75%   [EVAL] batch:    5 | acc: 62.50%,  total acc: 76.04%   [EVAL] batch:    6 | acc: 43.75%,  total acc: 71.43%   [EVAL] batch:    7 | acc: 56.25%,  total acc: 69.53%   [EVAL] batch:    8 | acc: 50.00%,  total acc: 67.36%   [EVAL] batch:    9 | acc: 50.00%,  total acc: 65.62%   [EVAL] batch:   10 | acc: 62.50%,  total acc: 65.34%   [EVAL] batch:   11 | acc: 12.50%,  total acc: 60.94%   [EVAL] batch:   12 | acc: 25.00%,  total acc: 58.17%   [EVAL] batch:   13 | acc: 25.00%,  total acc: 55.80%   [EVAL] batch:   14 | acc: 62.50%,  total acc: 56.25%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 56.25%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 57.35%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 57.64%   [EVAL] batch:   18 | acc: 56.25%,  total acc: 57.57%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 58.44%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 60.42%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 62.22%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 63.59%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 64.84%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 66.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 67.55%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 68.52%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 69.64%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 70.69%   [EVAL] batch:   29 | acc: 93.75%,  total acc: 71.46%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 72.18%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 73.05%   [EVAL] batch:   32 | acc: 62.50%,  total acc: 72.73%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 73.53%   [EVAL] batch:   34 | acc: 37.50%,  total acc: 72.50%   [EVAL] batch:   35 | acc: 50.00%,  total acc: 71.88%   [EVAL] batch:   36 | acc: 43.75%,  total acc: 71.11%   [EVAL] batch:   37 | acc: 25.00%,  total acc: 69.90%   [EVAL] batch:   38 | acc: 50.00%,  total acc: 69.39%   [EVAL] batch:   39 | acc: 68.75%,  total acc: 69.38%   [EVAL] batch:   40 | acc: 87.50%,  total acc: 69.82%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 70.39%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 71.08%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 71.73%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 72.36%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 72.96%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 73.54%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 74.09%   [EVAL] batch:   48 | acc: 43.75%,  total acc: 73.47%   [EVAL] batch:   49 | acc: 81.25%,  total acc: 73.62%   [EVAL] batch:   50 | acc: 25.00%,  total acc: 72.67%   [EVAL] batch:   51 | acc: 18.75%,  total acc: 71.63%   [EVAL] batch:   52 | acc: 6.25%,  total acc: 70.40%   [EVAL] batch:   53 | acc: 62.50%,  total acc: 70.25%   [EVAL] batch:   54 | acc: 62.50%,  total acc: 70.11%   [EVAL] batch:   55 | acc: 43.75%,  total acc: 69.64%   [EVAL] batch:   56 | acc: 56.25%,  total acc: 69.41%   [EVAL] batch:   57 | acc: 43.75%,  total acc: 68.97%   [EVAL] batch:   58 | acc: 81.25%,  total acc: 69.17%   [EVAL] batch:   59 | acc: 81.25%,  total acc: 69.38%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 69.88%   [EVAL] batch:   61 | acc: 31.25%,  total acc: 69.25%   [EVAL] batch:   62 | acc: 31.25%,  total acc: 68.65%   [EVAL] batch:   63 | acc: 43.75%,  total acc: 68.26%   [EVAL] batch:   64 | acc: 93.75%,  total acc: 68.65%   [EVAL] batch:   65 | acc: 100.00%,  total acc: 69.13%   [EVAL] batch:   66 | acc: 100.00%,  total acc: 69.59%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 70.04%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 70.47%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 70.89%   [EVAL] batch:   70 | acc: 56.25%,  total acc: 70.69%   [EVAL] batch:   71 | acc: 31.25%,  total acc: 70.14%   [EVAL] batch:   72 | acc: 50.00%,  total acc: 69.86%   [EVAL] batch:   73 | acc: 43.75%,  total acc: 69.51%   [EVAL] batch:   74 | acc: 56.25%,  total acc: 69.33%   [EVAL] batch:   75 | acc: 56.25%,  total acc: 69.16%   [EVAL] batch:   76 | acc: 50.00%,  total acc: 68.91%   [EVAL] batch:   77 | acc: 75.00%,  total acc: 68.99%   [EVAL] batch:   78 | acc: 87.50%,  total acc: 69.22%   [EVAL] batch:   79 | acc: 68.75%,  total acc: 69.22%   [EVAL] batch:   80 | acc: 93.75%,  total acc: 69.52%   [EVAL] batch:   81 | acc: 75.00%,  total acc: 69.59%   [EVAL] batch:   82 | acc: 68.75%,  total acc: 69.58%   [EVAL] batch:   83 | acc: 6.25%,  total acc: 68.82%   
cur_acc:  ['0.8655', '0.2344', '0.7500', '0.8299', '0.5913']
his_acc:  ['0.8655', '0.7109', '0.6782', '0.7474', '0.6882']
Clustering into  14  clusters
Clusters:  [ 0  1  4  0  1 12  0 13  7  2  1  2  5  0  4  3  0  6  0  8  9 10  3 12
 11  0  5  0  8  1  0]
Losses:  17.133420944213867 7.687994003295898 3.4279518127441406
CurrentTrain: epoch  0, batch     0 | loss: 17.1334209Losses:  11.976081848144531 2.5928690433502197 3.315688133239746
CurrentTrain: epoch  0, batch     1 | loss: 11.9760818Losses:  17.984331130981445 9.765047073364258 3.325298547744751
CurrentTrain: epoch  1, batch     0 | loss: 17.9843311Losses:  9.456574440002441 5.328371524810791 -0.0
CurrentTrain: epoch  1, batch     1 | loss: 9.4565744Losses:  14.66014289855957 7.294190406799316 3.3176112174987793
CurrentTrain: epoch  2, batch     0 | loss: 14.6601429Losses:  8.604696273803711 2.302182674407959 3.3192367553710938
CurrentTrain: epoch  2, batch     1 | loss: 8.6046963Losses:  14.114114761352539 7.0586771965026855 3.3258986473083496
CurrentTrain: epoch  3, batch     0 | loss: 14.1141148Losses:  8.353471755981445 2.157344341278076 3.2987639904022217
CurrentTrain: epoch  3, batch     1 | loss: 8.3534718Losses:  14.318292617797852 7.517095565795898 3.3209681510925293
CurrentTrain: epoch  4, batch     0 | loss: 14.3182926Losses:  6.615273952484131 2.5467369556427 1.3945783376693726
CurrentTrain: epoch  4, batch     1 | loss: 6.6152740Losses:  13.1251859664917 6.700199127197266 3.315570116043091
CurrentTrain: epoch  5, batch     0 | loss: 13.1251860Losses:  7.985841751098633 2.056077480316162 3.3130016326904297
CurrentTrain: epoch  5, batch     1 | loss: 7.9858418Losses:  11.885660171508789 5.9999494552612305 3.312046527862549
CurrentTrain: epoch  6, batch     0 | loss: 11.8856602Losses:  7.507096767425537 1.5586700439453125 3.3130688667297363
CurrentTrain: epoch  6, batch     1 | loss: 7.5070968Losses:  12.246886253356934 6.392539978027344 3.3174965381622314
CurrentTrain: epoch  7, batch     0 | loss: 12.2468863Losses:  7.236415386199951 1.8836628198623657 3.323448657989502
CurrentTrain: epoch  7, batch     1 | loss: 7.2364154Losses:  12.654650688171387 7.150008201599121 3.3177900314331055
CurrentTrain: epoch  8, batch     0 | loss: 12.6546507Losses:  6.316341400146484 2.8972480297088623 1.4144794940948486
CurrentTrain: epoch  8, batch     1 | loss: 6.3163414Losses:  13.397123336791992 7.753970623016357 3.3173952102661133
CurrentTrain: epoch  9, batch     0 | loss: 13.3971233Losses:  9.864660263061523 4.330480098724365 3.3688976764678955
CurrentTrain: epoch  9, batch     1 | loss: 9.8646603
Losses:  14.724352836608887 -0.0 13.78947639465332
MemoryTrain:  epoch  0, batch     0 | loss: 14.7243528Losses:  18.61387825012207 -0.0 16.978662490844727
MemoryTrain:  epoch  0, batch     1 | loss: 18.6138783Losses:  18.023317337036133 -0.0 16.727136611938477
MemoryTrain:  epoch  1, batch     0 | loss: 18.0233173Losses:  15.173187255859375 -0.0 13.84686279296875
MemoryTrain:  epoch  1, batch     1 | loss: 15.1731873Losses:  18.10126495361328 -0.0 16.783199310302734
MemoryTrain:  epoch  2, batch     0 | loss: 18.1012650Losses:  20.68585968017578 -0.0 20.000202178955078
MemoryTrain:  epoch  2, batch     1 | loss: 20.6858597Losses:  20.617334365844727 -0.0 19.898191452026367
MemoryTrain:  epoch  3, batch     0 | loss: 20.6173344Losses:  23.661012649536133 -0.0 23.2841739654541
MemoryTrain:  epoch  3, batch     1 | loss: 23.6610126Losses:  27.31340789794922 -0.0 26.64698600769043
MemoryTrain:  epoch  4, batch     0 | loss: 27.3134079Losses:  20.0191650390625 -0.0 19.899768829345703
MemoryTrain:  epoch  4, batch     1 | loss: 20.0191650Losses:  20.036714553833008 -0.0 19.891849517822266
MemoryTrain:  epoch  5, batch     0 | loss: 20.0367146Losses:  17.252784729003906 -0.0 16.800752639770508
MemoryTrain:  epoch  5, batch     1 | loss: 17.2527847Losses:  23.394208908081055 -0.0 23.176542282104492
MemoryTrain:  epoch  6, batch     0 | loss: 23.3942089Losses:  20.17026710510254 -0.0 19.876663208007812
MemoryTrain:  epoch  6, batch     1 | loss: 20.1702671Losses:  10.937705039978027 -0.0 10.83374309539795
MemoryTrain:  epoch  7, batch     0 | loss: 10.9377050Losses:  19.992000579833984 -0.0 19.86269187927246
MemoryTrain:  epoch  7, batch     1 | loss: 19.9920006Losses:  20.069862365722656 -0.0 19.95222282409668
MemoryTrain:  epoch  8, batch     0 | loss: 20.0698624Losses:  13.709447860717773 -0.0 13.674677848815918
MemoryTrain:  epoch  8, batch     1 | loss: 13.7094479Losses:  23.21864128112793 -0.0 23.14080810546875
MemoryTrain:  epoch  9, batch     0 | loss: 23.2186413Losses:  10.83746337890625 -0.0 10.790366172790527
MemoryTrain:  epoch  9, batch     1 | loss: 10.8374634
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 56.25%,  total acc: 65.62%   [EVAL] batch:    2 | acc: 56.25%,  total acc: 62.50%   [EVAL] batch:    3 | acc: 56.25%,  total acc: 60.94%   [EVAL] batch:    4 | acc: 50.00%,  total acc: 58.75%   [EVAL] batch:    5 | acc: 31.25%,  total acc: 54.17%   [EVAL] batch:    6 | acc: 56.25%,  total acc: 54.46%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 58.59%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 61.81%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 64.38%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 65.91%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 67.19%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 68.75%   [EVAL] batch:   13 | acc: 62.50%,  total acc: 68.30%   
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    1 | acc: 56.25%,  total acc: 59.38%   [EVAL] batch:    2 | acc: 56.25%,  total acc: 58.33%   [EVAL] batch:    3 | acc: 56.25%,  total acc: 57.81%   [EVAL] batch:    4 | acc: 62.50%,  total acc: 58.75%   [EVAL] batch:    5 | acc: 50.00%,  total acc: 57.29%   [EVAL] batch:    6 | acc: 6.25%,  total acc: 50.00%   [EVAL] batch:    7 | acc: 6.25%,  total acc: 44.53%   [EVAL] batch:    8 | acc: 25.00%,  total acc: 42.36%   [EVAL] batch:    9 | acc: 25.00%,  total acc: 40.62%   [EVAL] batch:   10 | acc: 37.50%,  total acc: 40.34%   [EVAL] batch:   11 | acc: 0.00%,  total acc: 36.98%   [EVAL] batch:   12 | acc: 18.75%,  total acc: 35.58%   [EVAL] batch:   13 | acc: 25.00%,  total acc: 34.82%   [EVAL] batch:   14 | acc: 62.50%,  total acc: 36.67%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 37.89%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 40.07%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 41.32%   [EVAL] batch:   18 | acc: 50.00%,  total acc: 41.78%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 43.44%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 46.13%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 48.58%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 50.54%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 52.34%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 54.25%   [EVAL] batch:   25 | acc: 93.75%,  total acc: 55.77%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 57.18%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 58.71%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 60.13%   [EVAL] batch:   29 | acc: 93.75%,  total acc: 61.25%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 62.30%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 63.48%   [EVAL] batch:   32 | acc: 68.75%,  total acc: 63.64%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 64.71%   [EVAL] batch:   34 | acc: 37.50%,  total acc: 63.93%   [EVAL] batch:   35 | acc: 75.00%,  total acc: 64.24%   [EVAL] batch:   36 | acc: 68.75%,  total acc: 64.36%   [EVAL] batch:   37 | acc: 75.00%,  total acc: 64.64%   [EVAL] batch:   38 | acc: 81.25%,  total acc: 65.06%   [EVAL] batch:   39 | acc: 81.25%,  total acc: 65.47%   [EVAL] batch:   40 | acc: 87.50%,  total acc: 66.01%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 66.67%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 67.44%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 68.18%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 68.75%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 69.43%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 70.08%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 70.70%   [EVAL] batch:   48 | acc: 37.50%,  total acc: 70.03%   [EVAL] batch:   49 | acc: 81.25%,  total acc: 70.25%   [EVAL] batch:   50 | acc: 25.00%,  total acc: 69.36%   [EVAL] batch:   51 | acc: 18.75%,  total acc: 68.39%   [EVAL] batch:   52 | acc: 0.00%,  total acc: 67.10%   [EVAL] batch:   53 | acc: 0.00%,  total acc: 65.86%   [EVAL] batch:   54 | acc: 0.00%,  total acc: 64.66%   [EVAL] batch:   55 | acc: 12.50%,  total acc: 63.73%   [EVAL] batch:   56 | acc: 6.25%,  total acc: 62.72%   [EVAL] batch:   57 | acc: 0.00%,  total acc: 61.64%   [EVAL] batch:   58 | acc: 0.00%,  total acc: 60.59%   [EVAL] batch:   59 | acc: 50.00%,  total acc: 60.42%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 61.07%   [EVAL] batch:   61 | acc: 25.00%,  total acc: 60.48%   [EVAL] batch:   62 | acc: 6.25%,  total acc: 59.62%   [EVAL] batch:   63 | acc: 12.50%,  total acc: 58.89%   [EVAL] batch:   64 | acc: 81.25%,  total acc: 59.23%   [EVAL] batch:   65 | acc: 100.00%,  total acc: 59.85%   [EVAL] batch:   66 | acc: 100.00%,  total acc: 60.45%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 61.03%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 61.59%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 62.14%   [EVAL] batch:   70 | acc: 56.25%,  total acc: 62.06%   [EVAL] batch:   71 | acc: 37.50%,  total acc: 61.72%   [EVAL] batch:   72 | acc: 50.00%,  total acc: 61.56%   [EVAL] batch:   73 | acc: 43.75%,  total acc: 61.32%   [EVAL] batch:   74 | acc: 43.75%,  total acc: 61.08%   [EVAL] batch:   75 | acc: 31.25%,  total acc: 60.69%   [EVAL] batch:   76 | acc: 25.00%,  total acc: 60.23%   [EVAL] batch:   77 | acc: 43.75%,  total acc: 60.02%   [EVAL] batch:   78 | acc: 43.75%,  total acc: 59.81%   [EVAL] batch:   79 | acc: 37.50%,  total acc: 59.53%   [EVAL] batch:   80 | acc: 50.00%,  total acc: 59.41%   [EVAL] batch:   81 | acc: 43.75%,  total acc: 59.22%   [EVAL] batch:   82 | acc: 6.25%,  total acc: 58.58%   [EVAL] batch:   83 | acc: 68.75%,  total acc: 58.71%   [EVAL] batch:   84 | acc: 56.25%,  total acc: 58.68%   [EVAL] batch:   85 | acc: 56.25%,  total acc: 58.65%   [EVAL] batch:   86 | acc: 56.25%,  total acc: 58.62%   [EVAL] batch:   87 | acc: 50.00%,  total acc: 58.52%   [EVAL] batch:   88 | acc: 37.50%,  total acc: 58.29%   [EVAL] batch:   89 | acc: 56.25%,  total acc: 58.26%   [EVAL] batch:   90 | acc: 81.25%,  total acc: 58.52%   [EVAL] batch:   91 | acc: 87.50%,  total acc: 58.83%   [EVAL] batch:   92 | acc: 87.50%,  total acc: 59.14%   [EVAL] batch:   93 | acc: 87.50%,  total acc: 59.44%   [EVAL] batch:   94 | acc: 75.00%,  total acc: 59.61%   [EVAL] batch:   95 | acc: 87.50%,  total acc: 59.90%   [EVAL] batch:   96 | acc: 68.75%,  total acc: 59.99%   
cur_acc:  ['0.8655', '0.2344', '0.7500', '0.8299', '0.5913', '0.6830']
his_acc:  ['0.8655', '0.7109', '0.6782', '0.7474', '0.6882', '0.5999']
Clustering into  17  clusters
Clusters:  [ 2  1  0 12 14  9  2 16  8  5  1  5  6  2  0  4  2 13  2  3 10 11  4  9
 15  2  6  2  3  1  2  0  0  7  9  3]
Losses:  18.917383193969727 7.486074924468994 3.7590317726135254
CurrentTrain: epoch  0, batch     0 | loss: 18.9173832Losses:  14.206567764282227 2.429861307144165 3.645203113555908
CurrentTrain: epoch  0, batch     1 | loss: 14.2065678Losses:  18.80823516845703 8.117912292480469 3.635328769683838
CurrentTrain: epoch  1, batch     0 | loss: 18.8082352Losses:  12.363814353942871 3.3562886714935303 1.4393314123153687
CurrentTrain: epoch  1, batch     1 | loss: 12.3638144Losses:  19.426944732666016 8.359905242919922 3.649604320526123
CurrentTrain: epoch  2, batch     0 | loss: 19.4269447Losses:  12.597250938415527 2.7180168628692627 3.568133592605591
CurrentTrain: epoch  2, batch     1 | loss: 12.5972509Losses:  19.393543243408203 9.10023307800293 3.617642879486084
CurrentTrain: epoch  3, batch     0 | loss: 19.3935432Losses:  12.0476713180542 2.8396806716918945 3.4782114028930664
CurrentTrain: epoch  3, batch     1 | loss: 12.0476713Losses:  17.763471603393555 7.912014484405518 3.5571675300598145
CurrentTrain: epoch  4, batch     0 | loss: 17.7634716Losses:  11.479036331176758 2.6688923835754395 3.492427349090576
CurrentTrain: epoch  4, batch     1 | loss: 11.4790363Losses:  17.938228607177734 8.727455139160156 3.5498533248901367
CurrentTrain: epoch  5, batch     0 | loss: 17.9382286Losses:  10.224340438842773 3.6742050647735596 1.472398042678833
CurrentTrain: epoch  5, batch     1 | loss: 10.2243404Losses:  15.854676246643066 7.6163411140441895 3.517153739929199
CurrentTrain: epoch  6, batch     0 | loss: 15.8546762Losses:  11.398076057434082 2.903977632522583 3.3978354930877686
CurrentTrain: epoch  6, batch     1 | loss: 11.3980761Losses:  16.989896774291992 10.203227043151855 3.442491054534912
CurrentTrain: epoch  7, batch     0 | loss: 16.9898968Losses:  14.70958137512207 8.505276679992676 -0.0
CurrentTrain: epoch  7, batch     1 | loss: 14.7095814Losses:  17.415451049804688 9.106391906738281 3.471255302429199
CurrentTrain: epoch  8, batch     0 | loss: 17.4154510Losses:  9.962175369262695 3.299501895904541 3.4032726287841797
CurrentTrain: epoch  8, batch     1 | loss: 9.9621754Losses:  14.565082550048828 7.58496618270874 3.414984941482544
CurrentTrain: epoch  9, batch     0 | loss: 14.5650826Losses:  10.021267890930176 3.83868670463562 1.5401729345321655
CurrentTrain: epoch  9, batch     1 | loss: 10.0212679
Losses:  30.71561622619629 -0.0 30.122703552246094
MemoryTrain:  epoch  0, batch     0 | loss: 30.7156162Losses:  14.13361930847168 -0.0 13.71247673034668
MemoryTrain:  epoch  0, batch     1 | loss: 14.1336193Losses:  3.4498729705810547 -0.0 3.387700319290161
MemoryTrain:  epoch  0, batch     2 | loss: 3.4498730Losses:  23.67704200744629 -0.0 23.21464729309082
MemoryTrain:  epoch  1, batch     0 | loss: 23.6770420Losses:  17.375272750854492 -0.0 16.688941955566406
MemoryTrain:  epoch  1, batch     1 | loss: 17.3752728Losses:  0.09054063260555267 -0.0 -0.0
MemoryTrain:  epoch  1, batch     2 | loss: 0.0905406Losses:  26.807268142700195 -0.0 26.512557983398438
MemoryTrain:  epoch  2, batch     0 | loss: 26.8072681Losses:  16.9971981048584 -0.0 16.732080459594727
MemoryTrain:  epoch  2, batch     1 | loss: 16.9971981Losses:  1.5380859375 -0.0 1.4551305770874023
MemoryTrain:  epoch  2, batch     2 | loss: 1.5380859Losses:  20.033000946044922 -0.0 19.890344619750977
MemoryTrain:  epoch  3, batch     0 | loss: 20.0330009Losses:  20.186607360839844 -0.0 19.833499908447266
MemoryTrain:  epoch  3, batch     1 | loss: 20.1866074Losses:  3.3680367469787598 -0.0 3.3185904026031494
MemoryTrain:  epoch  3, batch     2 | loss: 3.3680367Losses:  17.04385757446289 -0.0 16.79755973815918
MemoryTrain:  epoch  4, batch     0 | loss: 17.0438576Losses:  30.38196563720703 -0.0 29.97076988220215
MemoryTrain:  epoch  4, batch     1 | loss: 30.3819656Losses:  3.4111733436584473 -0.0 3.355668306350708
MemoryTrain:  epoch  4, batch     2 | loss: 3.4111733Losses:  23.29400634765625 -0.0 23.196199417114258
MemoryTrain:  epoch  5, batch     0 | loss: 23.2940063Losses:  19.990310668945312 -0.0 19.91122055053711
MemoryTrain:  epoch  5, batch     1 | loss: 19.9903107Losses:  1.471156120300293 -0.0 1.3921784162521362
MemoryTrain:  epoch  5, batch     2 | loss: 1.4711561Losses:  23.159757614135742 -0.0 23.09869956970215
MemoryTrain:  epoch  6, batch     0 | loss: 23.1597576Losses:  23.232587814331055 -0.0 23.144039154052734
MemoryTrain:  epoch  6, batch     1 | loss: 23.2325878Losses:  1.4590610265731812 -0.0 1.398202896118164
MemoryTrain:  epoch  6, batch     2 | loss: 1.4590610Losses:  26.611797332763672 -0.0 26.462173461914062
MemoryTrain:  epoch  7, batch     0 | loss: 26.6117973Losses:  23.194196701049805 -0.0 23.13140296936035
MemoryTrain:  epoch  7, batch     1 | loss: 23.1941967Losses:  3.378704786300659 -0.0 3.3039755821228027
MemoryTrain:  epoch  7, batch     2 | loss: 3.3787048Losses:  26.530799865722656 -0.0 26.483123779296875
MemoryTrain:  epoch  8, batch     0 | loss: 26.5307999Losses:  19.967275619506836 -0.0 19.8781795501709
MemoryTrain:  epoch  8, batch     1 | loss: 19.9672756Losses:  0.031153365969657898 -0.0 -0.0
MemoryTrain:  epoch  8, batch     2 | loss: 0.0311534Losses:  26.592506408691406 -0.0 26.52887725830078
MemoryTrain:  epoch  9, batch     0 | loss: 26.5925064Losses:  10.83663558959961 -0.0 10.784276008605957
MemoryTrain:  epoch  9, batch     1 | loss: 10.8366356Losses:  1.4814536571502686 -0.0 1.3866628408432007
MemoryTrain:  epoch  9, batch     2 | loss: 1.4814537
[EVAL] batch:    0 | acc: 43.75%,  total acc: 43.75%   [EVAL] batch:    1 | acc: 31.25%,  total acc: 37.50%   [EVAL] batch:    2 | acc: 37.50%,  total acc: 37.50%   [EVAL] batch:    3 | acc: 0.00%,  total acc: 28.12%   [EVAL] batch:    4 | acc: 0.00%,  total acc: 22.50%   [EVAL] batch:    5 | acc: 0.00%,  total acc: 18.75%   [EVAL] batch:    6 | acc: 18.75%,  total acc: 18.75%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 26.56%   [EVAL] batch:    8 | acc: 50.00%,  total acc: 29.17%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 32.50%   [EVAL] batch:   10 | acc: 68.75%,  total acc: 35.80%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 39.06%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 42.31%   [EVAL] batch:   13 | acc: 81.25%,  total acc: 45.09%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 48.33%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 51.56%   [EVAL] batch:   16 | acc: 87.50%,  total acc: 53.68%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 55.90%   [EVAL] batch:   18 | acc: 81.25%,  total acc: 57.24%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 58.75%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 60.71%   [EVAL] batch:   21 | acc: 37.50%,  total acc: 59.66%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 50.00%,  total acc: 59.38%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 64.58%   [EVAL] batch:    3 | acc: 56.25%,  total acc: 62.50%   [EVAL] batch:    4 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    5 | acc: 50.00%,  total acc: 60.42%   [EVAL] batch:    6 | acc: 18.75%,  total acc: 54.46%   [EVAL] batch:    7 | acc: 0.00%,  total acc: 47.66%   [EVAL] batch:    8 | acc: 6.25%,  total acc: 43.06%   [EVAL] batch:    9 | acc: 6.25%,  total acc: 39.38%   [EVAL] batch:   10 | acc: 0.00%,  total acc: 35.80%   [EVAL] batch:   11 | acc: 0.00%,  total acc: 32.81%   [EVAL] batch:   12 | acc: 12.50%,  total acc: 31.25%   [EVAL] batch:   13 | acc: 25.00%,  total acc: 30.80%   [EVAL] batch:   14 | acc: 62.50%,  total acc: 32.92%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 34.38%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 36.76%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 38.19%   [EVAL] batch:   18 | acc: 50.00%,  total acc: 38.82%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 40.62%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 43.45%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 46.02%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 48.10%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 50.00%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 52.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 53.85%   [EVAL] batch:   26 | acc: 87.50%,  total acc: 55.09%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 56.70%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 58.19%   [EVAL] batch:   29 | acc: 75.00%,  total acc: 58.75%   [EVAL] batch:   30 | acc: 75.00%,  total acc: 59.27%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 60.35%   [EVAL] batch:   32 | acc: 75.00%,  total acc: 60.80%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 61.95%   [EVAL] batch:   34 | acc: 31.25%,  total acc: 61.07%   [EVAL] batch:   35 | acc: 75.00%,  total acc: 61.46%   [EVAL] batch:   36 | acc: 81.25%,  total acc: 61.99%   [EVAL] batch:   37 | acc: 68.75%,  total acc: 62.17%   [EVAL] batch:   38 | acc: 87.50%,  total acc: 62.82%   [EVAL] batch:   39 | acc: 87.50%,  total acc: 63.44%   [EVAL] batch:   40 | acc: 87.50%,  total acc: 64.02%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 64.73%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 65.55%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 66.19%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 66.81%   [EVAL] batch:   45 | acc: 93.75%,  total acc: 67.39%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 68.09%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 68.75%   [EVAL] batch:   48 | acc: 6.25%,  total acc: 67.47%   [EVAL] batch:   49 | acc: 12.50%,  total acc: 66.38%   [EVAL] batch:   50 | acc: 6.25%,  total acc: 65.20%   [EVAL] batch:   51 | acc: 18.75%,  total acc: 64.30%   [EVAL] batch:   52 | acc: 0.00%,  total acc: 63.09%   [EVAL] batch:   53 | acc: 0.00%,  total acc: 61.92%   [EVAL] batch:   54 | acc: 0.00%,  total acc: 60.80%   [EVAL] batch:   55 | acc: 0.00%,  total acc: 59.71%   [EVAL] batch:   56 | acc: 0.00%,  total acc: 58.66%   [EVAL] batch:   57 | acc: 0.00%,  total acc: 57.65%   [EVAL] batch:   58 | acc: 0.00%,  total acc: 56.67%   [EVAL] batch:   59 | acc: 43.75%,  total acc: 56.46%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 57.17%   [EVAL] batch:   61 | acc: 25.00%,  total acc: 56.65%   [EVAL] batch:   62 | acc: 0.00%,  total acc: 55.75%   [EVAL] batch:   63 | acc: 0.00%,  total acc: 54.88%   [EVAL] batch:   64 | acc: 75.00%,  total acc: 55.19%   [EVAL] batch:   65 | acc: 100.00%,  total acc: 55.87%   [EVAL] batch:   66 | acc: 100.00%,  total acc: 56.53%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 57.17%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 57.79%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 58.39%   [EVAL] batch:   70 | acc: 56.25%,  total acc: 58.36%   [EVAL] batch:   71 | acc: 31.25%,  total acc: 57.99%   [EVAL] batch:   72 | acc: 50.00%,  total acc: 57.88%   [EVAL] batch:   73 | acc: 43.75%,  total acc: 57.69%   [EVAL] batch:   74 | acc: 43.75%,  total acc: 57.50%   [EVAL] batch:   75 | acc: 12.50%,  total acc: 56.91%   [EVAL] batch:   76 | acc: 6.25%,  total acc: 56.25%   [EVAL] batch:   77 | acc: 0.00%,  total acc: 55.53%   [EVAL] batch:   78 | acc: 0.00%,  total acc: 54.83%   [EVAL] batch:   79 | acc: 0.00%,  total acc: 54.14%   [EVAL] batch:   80 | acc: 0.00%,  total acc: 53.47%   [EVAL] batch:   81 | acc: 25.00%,  total acc: 53.12%   [EVAL] batch:   82 | acc: 12.50%,  total acc: 52.64%   [EVAL] batch:   83 | acc: 75.00%,  total acc: 52.90%   [EVAL] batch:   84 | acc: 25.00%,  total acc: 52.57%   [EVAL] batch:   85 | acc: 18.75%,  total acc: 52.18%   [EVAL] batch:   86 | acc: 6.25%,  total acc: 51.65%   [EVAL] batch:   87 | acc: 12.50%,  total acc: 51.21%   [EVAL] batch:   88 | acc: 12.50%,  total acc: 50.77%   [EVAL] batch:   89 | acc: 18.75%,  total acc: 50.42%   [EVAL] batch:   90 | acc: 81.25%,  total acc: 50.76%   [EVAL] batch:   91 | acc: 75.00%,  total acc: 51.02%   [EVAL] batch:   92 | acc: 75.00%,  total acc: 51.28%   [EVAL] batch:   93 | acc: 87.50%,  total acc: 51.66%   [EVAL] batch:   94 | acc: 62.50%,  total acc: 51.78%   [EVAL] batch:   95 | acc: 81.25%,  total acc: 52.08%   [EVAL] batch:   96 | acc: 68.75%,  total acc: 52.26%   [EVAL] batch:   97 | acc: 50.00%,  total acc: 52.23%   [EVAL] batch:   98 | acc: 31.25%,  total acc: 52.02%   [EVAL] batch:   99 | acc: 25.00%,  total acc: 51.75%   [EVAL] batch:  100 | acc: 0.00%,  total acc: 51.24%   [EVAL] batch:  101 | acc: 0.00%,  total acc: 50.74%   [EVAL] batch:  102 | acc: 0.00%,  total acc: 50.24%   [EVAL] batch:  103 | acc: 37.50%,  total acc: 50.12%   [EVAL] batch:  104 | acc: 75.00%,  total acc: 50.36%   [EVAL] batch:  105 | acc: 50.00%,  total acc: 50.35%   [EVAL] batch:  106 | acc: 75.00%,  total acc: 50.58%   [EVAL] batch:  107 | acc: 56.25%,  total acc: 50.64%   [EVAL] batch:  108 | acc: 81.25%,  total acc: 50.92%   [EVAL] batch:  109 | acc: 81.25%,  total acc: 51.19%   [EVAL] batch:  110 | acc: 87.50%,  total acc: 51.52%   [EVAL] batch:  111 | acc: 93.75%,  total acc: 51.90%   [EVAL] batch:  112 | acc: 93.75%,  total acc: 52.27%   [EVAL] batch:  113 | acc: 87.50%,  total acc: 52.58%   [EVAL] batch:  114 | acc: 93.75%,  total acc: 52.93%   [EVAL] batch:  115 | acc: 81.25%,  total acc: 53.18%   [EVAL] batch:  116 | acc: 93.75%,  total acc: 53.53%   [EVAL] batch:  117 | acc: 93.75%,  total acc: 53.87%   [EVAL] batch:  118 | acc: 18.75%,  total acc: 53.57%   
cur_acc:  ['0.8655', '0.2344', '0.7500', '0.8299', '0.5913', '0.6830', '0.5966']
his_acc:  ['0.8655', '0.7109', '0.6782', '0.7474', '0.6882', '0.5999', '0.5357']
Clustering into  19  clusters
Clusters:  [ 4  1  0 11 17 10  4 13 12  2  1  2  7  4  0  6  4 15 18 16  5  3  6 10
 14 18  7  4 16  1 18  0  0  8 10 16  4  3  9  8 16]
Losses:  23.244842529296875 9.740152359008789 5.866185665130615
CurrentTrain: epoch  0, batch     0 | loss: 23.2448425Losses:  13.938628196716309 4.52508020401001 3.30130672454834
CurrentTrain: epoch  0, batch     1 | loss: 13.9386282Losses:  18.511302947998047 7.359635353088379 5.634130001068115
CurrentTrain: epoch  1, batch     0 | loss: 18.5113029Losses:  14.06052017211914 2.177286386489868 5.69376802444458
CurrentTrain: epoch  1, batch     1 | loss: 14.0605202Losses:  18.238628387451172 7.478097915649414 5.588023662567139
CurrentTrain: epoch  2, batch     0 | loss: 18.2386284Losses:  14.432234764099121 2.6307718753814697 5.766592979431152
CurrentTrain: epoch  2, batch     1 | loss: 14.4322348Losses:  18.403640747070312 7.526129722595215 5.640161991119385
CurrentTrain: epoch  3, batch     0 | loss: 18.4036407Losses:  12.482646942138672 1.79093337059021 5.626922130584717
CurrentTrain: epoch  3, batch     1 | loss: 12.4826469Losses:  18.97783660888672 8.34256362915039 5.619416236877441
CurrentTrain: epoch  4, batch     0 | loss: 18.9778366Losses:  12.156776428222656 2.7580559253692627 5.672585487365723
CurrentTrain: epoch  4, batch     1 | loss: 12.1567764Losses:  17.664573669433594 8.321237564086914 5.586881160736084
CurrentTrain: epoch  5, batch     0 | loss: 17.6645737Losses:  13.196473121643066 4.52223539352417 3.351316213607788
CurrentTrain: epoch  5, batch     1 | loss: 13.1964731Losses:  16.406232833862305 7.050275802612305 5.604827880859375
CurrentTrain: epoch  6, batch     0 | loss: 16.4062328Losses:  13.37863540649414 3.0744688510894775 5.603082180023193
CurrentTrain: epoch  6, batch     1 | loss: 13.3786354Losses:  16.664058685302734 7.367655277252197 5.613795280456543
CurrentTrain: epoch  7, batch     0 | loss: 16.6640587Losses:  10.192638397216797 2.8304126262664795 3.320930004119873
CurrentTrain: epoch  7, batch     1 | loss: 10.1926384Losses:  16.93239974975586 7.516778945922852 5.571671962738037
CurrentTrain: epoch  8, batch     0 | loss: 16.9323997Losses:  9.604686737060547 2.5428895950317383 3.356802225112915
CurrentTrain: epoch  8, batch     1 | loss: 9.6046867Losses:  15.879915237426758 6.663178443908691 5.58919095993042
CurrentTrain: epoch  9, batch     0 | loss: 15.8799152Losses:  10.768999099731445 1.9165306091308594 5.605371952056885
CurrentTrain: epoch  9, batch     1 | loss: 10.7689991
Losses:  30.303804397583008 -0.0 29.981952667236328
MemoryTrain:  epoch  0, batch     0 | loss: 30.3038044Losses:  20.178232192993164 -0.0 19.89180564880371
MemoryTrain:  epoch  0, batch     1 | loss: 20.1782322Losses:  11.008796691894531 -0.0 10.803434371948242
MemoryTrain:  epoch  0, batch     2 | loss: 11.0087967Losses:  23.484615325927734 -0.0 23.199748992919922
MemoryTrain:  epoch  1, batch     0 | loss: 23.4846153Losses:  23.61256980895996 -0.0 23.227922439575195
MemoryTrain:  epoch  1, batch     1 | loss: 23.6125698Losses:  14.074865341186523 -0.0 13.762351989746094
MemoryTrain:  epoch  1, batch     2 | loss: 14.0748653Losses:  26.736209869384766 -0.0 26.516754150390625
MemoryTrain:  epoch  2, batch     0 | loss: 26.7362099Losses:  20.34374237060547 -0.0 19.961523056030273
MemoryTrain:  epoch  2, batch     1 | loss: 20.3437424Losses:  13.927175521850586 -0.0 13.669782638549805
MemoryTrain:  epoch  2, batch     2 | loss: 13.9271755Losses:  23.17997932434082 -0.0 23.12797737121582
MemoryTrain:  epoch  3, batch     0 | loss: 23.1799793Losses:  23.430688858032227 -0.0 23.133914947509766
MemoryTrain:  epoch  3, batch     1 | loss: 23.4306889Losses:  13.763681411743164 -0.0 13.684770584106445
MemoryTrain:  epoch  3, batch     2 | loss: 13.7636814Losses:  23.203134536743164 -0.0 23.099706649780273
MemoryTrain:  epoch  4, batch     0 | loss: 23.2031345Losses:  26.61032485961914 -0.0 26.508575439453125
MemoryTrain:  epoch  4, batch     1 | loss: 26.6103249Losses:  8.208990097045898 -0.0 8.090319633483887
MemoryTrain:  epoch  4, batch     2 | loss: 8.2089901Losses:  16.775177001953125 -0.0 16.71550941467285
MemoryTrain:  epoch  5, batch     0 | loss: 16.7751770Losses:  23.25836753845215 -0.0 23.135961532592773
MemoryTrain:  epoch  5, batch     1 | loss: 23.2583675Losses:  13.723484992980957 -0.0 13.684992790222168
MemoryTrain:  epoch  5, batch     2 | loss: 13.7234850Losses:  26.56593132019043 -0.0 26.48133659362793
MemoryTrain:  epoch  6, batch     0 | loss: 26.5659313Losses:  26.526493072509766 -0.0 26.480087280273438
MemoryTrain:  epoch  6, batch     1 | loss: 26.5264931Losses:  10.930794715881348 -0.0 10.814905166625977
MemoryTrain:  epoch  6, batch     2 | loss: 10.9307947Losses:  23.185123443603516 -0.0 23.12809181213379
MemoryTrain:  epoch  7, batch     0 | loss: 23.1851234Losses:  26.49110984802246 -0.0 26.450611114501953
MemoryTrain:  epoch  7, batch     1 | loss: 26.4911098Losses:  10.834113121032715 -0.0 10.811504364013672
MemoryTrain:  epoch  7, batch     2 | loss: 10.8341131Losses:  33.5164794921875 -0.0 33.477542877197266
MemoryTrain:  epoch  8, batch     0 | loss: 33.5164795Losses:  23.163780212402344 -0.0 23.100543975830078
MemoryTrain:  epoch  8, batch     1 | loss: 23.1637802Losses:  10.801709175109863 -0.0 10.771048545837402
MemoryTrain:  epoch  8, batch     2 | loss: 10.8017092Losses:  19.867389678955078 -0.0 19.826078414916992
MemoryTrain:  epoch  9, batch     0 | loss: 19.8673897Losses:  19.852813720703125 -0.0 19.823598861694336
MemoryTrain:  epoch  9, batch     1 | loss: 19.8528137Losses:  13.7149076461792 -0.0 13.652820587158203
MemoryTrain:  epoch  9, batch     2 | loss: 13.7149076
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    1 | acc: 68.75%,  total acc: 65.62%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 68.75%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 73.44%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 76.25%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 79.17%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 82.14%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 82.81%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 83.12%   [EVAL] batch:   10 | acc: 50.00%,  total acc: 80.11%   [EVAL] batch:   11 | acc: 50.00%,  total acc: 77.60%   [EVAL] batch:   12 | acc: 43.75%,  total acc: 75.00%   [EVAL] batch:   13 | acc: 37.50%,  total acc: 72.32%   [EVAL] batch:   14 | acc: 37.50%,  total acc: 70.00%   
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    1 | acc: 68.75%,  total acc: 65.62%   [EVAL] batch:    2 | acc: 68.75%,  total acc: 66.67%   [EVAL] batch:    3 | acc: 50.00%,  total acc: 62.50%   [EVAL] batch:    4 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    5 | acc: 43.75%,  total acc: 59.38%   [EVAL] batch:    6 | acc: 12.50%,  total acc: 52.68%   [EVAL] batch:    7 | acc: 6.25%,  total acc: 46.88%   [EVAL] batch:    8 | acc: 12.50%,  total acc: 43.06%   [EVAL] batch:    9 | acc: 6.25%,  total acc: 39.38%   [EVAL] batch:   10 | acc: 12.50%,  total acc: 36.93%   [EVAL] batch:   11 | acc: 0.00%,  total acc: 33.85%   [EVAL] batch:   12 | acc: 12.50%,  total acc: 32.21%   [EVAL] batch:   13 | acc: 25.00%,  total acc: 31.70%   [EVAL] batch:   14 | acc: 50.00%,  total acc: 32.92%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 34.38%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 36.76%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 38.19%   [EVAL] batch:   18 | acc: 50.00%,  total acc: 38.82%   [EVAL] batch:   19 | acc: 68.75%,  total acc: 40.31%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 43.15%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 45.74%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 47.83%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 49.74%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 51.75%   [EVAL] batch:   25 | acc: 93.75%,  total acc: 53.37%   [EVAL] batch:   26 | acc: 81.25%,  total acc: 54.40%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 56.03%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 57.54%   [EVAL] batch:   29 | acc: 75.00%,  total acc: 58.13%   [EVAL] batch:   30 | acc: 75.00%,  total acc: 58.67%   [EVAL] batch:   31 | acc: 87.50%,  total acc: 59.57%   [EVAL] batch:   32 | acc: 75.00%,  total acc: 60.04%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 61.21%   [EVAL] batch:   34 | acc: 31.25%,  total acc: 60.36%   [EVAL] batch:   35 | acc: 62.50%,  total acc: 60.42%   [EVAL] batch:   36 | acc: 81.25%,  total acc: 60.98%   [EVAL] batch:   37 | acc: 81.25%,  total acc: 61.51%   [EVAL] batch:   38 | acc: 81.25%,  total acc: 62.02%   [EVAL] batch:   39 | acc: 87.50%,  total acc: 62.66%   [EVAL] batch:   40 | acc: 87.50%,  total acc: 63.26%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 64.14%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 64.97%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 65.62%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 66.25%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 66.98%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 67.69%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 68.36%   [EVAL] batch:   48 | acc: 6.25%,  total acc: 67.09%   [EVAL] batch:   49 | acc: 31.25%,  total acc: 66.38%   [EVAL] batch:   50 | acc: 6.25%,  total acc: 65.20%   [EVAL] batch:   51 | acc: 18.75%,  total acc: 64.30%   [EVAL] batch:   52 | acc: 6.25%,  total acc: 63.21%   [EVAL] batch:   53 | acc: 0.00%,  total acc: 62.04%   [EVAL] batch:   54 | acc: 0.00%,  total acc: 60.91%   [EVAL] batch:   55 | acc: 0.00%,  total acc: 59.82%   [EVAL] batch:   56 | acc: 0.00%,  total acc: 58.77%   [EVAL] batch:   57 | acc: 0.00%,  total acc: 57.76%   [EVAL] batch:   58 | acc: 0.00%,  total acc: 56.78%   [EVAL] batch:   59 | acc: 50.00%,  total acc: 56.67%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 57.38%   [EVAL] batch:   61 | acc: 31.25%,  total acc: 56.96%   [EVAL] batch:   62 | acc: 0.00%,  total acc: 56.05%   [EVAL] batch:   63 | acc: 0.00%,  total acc: 55.18%   [EVAL] batch:   64 | acc: 81.25%,  total acc: 55.58%   [EVAL] batch:   65 | acc: 100.00%,  total acc: 56.25%   [EVAL] batch:   66 | acc: 100.00%,  total acc: 56.90%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 57.54%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 58.15%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 58.75%   [EVAL] batch:   70 | acc: 56.25%,  total acc: 58.71%   [EVAL] batch:   71 | acc: 31.25%,  total acc: 58.33%   [EVAL] batch:   72 | acc: 50.00%,  total acc: 58.22%   [EVAL] batch:   73 | acc: 43.75%,  total acc: 58.02%   [EVAL] batch:   74 | acc: 37.50%,  total acc: 57.75%   [EVAL] batch:   75 | acc: 12.50%,  total acc: 57.15%   [EVAL] batch:   76 | acc: 18.75%,  total acc: 56.66%   [EVAL] batch:   77 | acc: 0.00%,  total acc: 55.93%   [EVAL] batch:   78 | acc: 0.00%,  total acc: 55.22%   [EVAL] batch:   79 | acc: 0.00%,  total acc: 54.53%   [EVAL] batch:   80 | acc: 0.00%,  total acc: 53.86%   [EVAL] batch:   81 | acc: 25.00%,  total acc: 53.51%   [EVAL] batch:   82 | acc: 25.00%,  total acc: 53.16%   [EVAL] batch:   83 | acc: 68.75%,  total acc: 53.35%   [EVAL] batch:   84 | acc: 37.50%,  total acc: 53.16%   [EVAL] batch:   85 | acc: 12.50%,  total acc: 52.69%   [EVAL] batch:   86 | acc: 12.50%,  total acc: 52.23%   [EVAL] batch:   87 | acc: 6.25%,  total acc: 51.70%   [EVAL] batch:   88 | acc: 18.75%,  total acc: 51.33%   [EVAL] batch:   89 | acc: 18.75%,  total acc: 50.97%   [EVAL] batch:   90 | acc: 81.25%,  total acc: 51.30%   [EVAL] batch:   91 | acc: 81.25%,  total acc: 51.63%   [EVAL] batch:   92 | acc: 81.25%,  total acc: 51.95%   [EVAL] batch:   93 | acc: 87.50%,  total acc: 52.33%   [EVAL] batch:   94 | acc: 62.50%,  total acc: 52.43%   [EVAL] batch:   95 | acc: 68.75%,  total acc: 52.60%   [EVAL] batch:   96 | acc: 25.00%,  total acc: 52.32%   [EVAL] batch:   97 | acc: 31.25%,  total acc: 52.10%   [EVAL] batch:   98 | acc: 25.00%,  total acc: 51.83%   [EVAL] batch:   99 | acc: 25.00%,  total acc: 51.56%   [EVAL] batch:  100 | acc: 0.00%,  total acc: 51.05%   [EVAL] batch:  101 | acc: 0.00%,  total acc: 50.55%   [EVAL] batch:  102 | acc: 0.00%,  total acc: 50.06%   [EVAL] batch:  103 | acc: 43.75%,  total acc: 50.00%   [EVAL] batch:  104 | acc: 81.25%,  total acc: 50.30%   [EVAL] batch:  105 | acc: 56.25%,  total acc: 50.35%   [EVAL] batch:  106 | acc: 75.00%,  total acc: 50.58%   [EVAL] batch:  107 | acc: 75.00%,  total acc: 50.81%   [EVAL] batch:  108 | acc: 81.25%,  total acc: 51.09%   [EVAL] batch:  109 | acc: 81.25%,  total acc: 51.36%   [EVAL] batch:  110 | acc: 93.75%,  total acc: 51.75%   [EVAL] batch:  111 | acc: 68.75%,  total acc: 51.90%   [EVAL] batch:  112 | acc: 81.25%,  total acc: 52.16%   [EVAL] batch:  113 | acc: 56.25%,  total acc: 52.19%   [EVAL] batch:  114 | acc: 81.25%,  total acc: 52.45%   [EVAL] batch:  115 | acc: 37.50%,  total acc: 52.32%   [EVAL] batch:  116 | acc: 56.25%,  total acc: 52.35%   [EVAL] batch:  117 | acc: 68.75%,  total acc: 52.49%   [EVAL] batch:  118 | acc: 62.50%,  total acc: 52.57%   [EVAL] batch:  119 | acc: 62.50%,  total acc: 52.66%   [EVAL] batch:  120 | acc: 81.25%,  total acc: 52.89%   [EVAL] batch:  121 | acc: 81.25%,  total acc: 53.12%   [EVAL] batch:  122 | acc: 87.50%,  total acc: 53.40%   [EVAL] batch:  123 | acc: 93.75%,  total acc: 53.73%   [EVAL] batch:  124 | acc: 100.00%,  total acc: 54.10%   [EVAL] batch:  125 | acc: 93.75%,  total acc: 54.41%   [EVAL] batch:  126 | acc: 81.25%,  total acc: 54.63%   [EVAL] batch:  127 | acc: 93.75%,  total acc: 54.93%   [EVAL] batch:  128 | acc: 50.00%,  total acc: 54.89%   [EVAL] batch:  129 | acc: 50.00%,  total acc: 54.86%   [EVAL] batch:  130 | acc: 43.75%,  total acc: 54.77%   [EVAL] batch:  131 | acc: 43.75%,  total acc: 54.69%   [EVAL] batch:  132 | acc: 37.50%,  total acc: 54.56%   
cur_acc:  ['0.8655', '0.2344', '0.7500', '0.8299', '0.5913', '0.6830', '0.5966', '0.7000']
his_acc:  ['0.8655', '0.7109', '0.6782', '0.7474', '0.6882', '0.5999', '0.5357', '0.5456']
--------Round  4
seed:  500
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/test.pkl
Task_order: [7 5 6 4 2 1 3 0]
prepared data!
Clustering into  2  clusters
Clusters:  [1 0 0 1 0 1]
Losses:  26.90613555908203 13.612224578857422 -0.0
CurrentTrain: epoch  0, batch     0 | loss: 26.9061356Losses:  21.976707458496094 8.912845611572266 -0.0
CurrentTrain: epoch  0, batch     1 | loss: 21.9767075Losses:  20.954708099365234 8.069531440734863 -0.0
CurrentTrain: epoch  0, batch     2 | loss: 20.9547081Losses:  21.388469696044922 8.462722778320312 -0.0
CurrentTrain: epoch  0, batch     3 | loss: 21.3884697Losses:  21.690738677978516 8.90967845916748 -0.0
CurrentTrain: epoch  0, batch     4 | loss: 21.6907387Losses:  21.071922302246094 8.428704261779785 -0.0
CurrentTrain: epoch  0, batch     5 | loss: 21.0719223Losses:  23.964441299438477 11.386770248413086 -0.0
CurrentTrain: epoch  0, batch     6 | loss: 23.9644413Losses:  21.570411682128906 9.283491134643555 -0.0
CurrentTrain: epoch  0, batch     7 | loss: 21.5704117Losses:  22.93327522277832 10.743391036987305 -0.0
CurrentTrain: epoch  0, batch     8 | loss: 22.9332752Losses:  24.421968460083008 12.176490783691406 -0.0
CurrentTrain: epoch  0, batch     9 | loss: 24.4219685Losses:  20.879371643066406 8.855376243591309 -0.0
CurrentTrain: epoch  0, batch    10 | loss: 20.8793716Losses:  21.21479034423828 9.317302703857422 -0.0
CurrentTrain: epoch  0, batch    11 | loss: 21.2147903Losses:  18.704755783081055 7.012269973754883 -0.0
CurrentTrain: epoch  0, batch    12 | loss: 18.7047558Losses:  19.33317756652832 7.784841537475586 -0.0
CurrentTrain: epoch  0, batch    13 | loss: 19.3331776Losses:  20.570785522460938 9.088172912597656 -0.0
CurrentTrain: epoch  0, batch    14 | loss: 20.5707855Losses:  23.001495361328125 10.96204948425293 -0.0
CurrentTrain: epoch  0, batch    15 | loss: 23.0014954Losses:  21.85871124267578 10.154881477355957 -0.0
CurrentTrain: epoch  0, batch    16 | loss: 21.8587112Losses:  23.62078857421875 12.297731399536133 -0.0
CurrentTrain: epoch  0, batch    17 | loss: 23.6207886Losses:  21.17207145690918 9.456982612609863 -0.0
CurrentTrain: epoch  0, batch    18 | loss: 21.1720715Losses:  19.91070556640625 8.535419464111328 -0.0
CurrentTrain: epoch  0, batch    19 | loss: 19.9107056Losses:  19.67345428466797 8.408554077148438 -0.0
CurrentTrain: epoch  0, batch    20 | loss: 19.6734543Losses:  21.331056594848633 9.539488792419434 -0.0
CurrentTrain: epoch  0, batch    21 | loss: 21.3310566Losses:  18.81389617919922 7.55747127532959 -0.0
CurrentTrain: epoch  0, batch    22 | loss: 18.8138962Losses:  19.487302780151367 8.375274658203125 -0.0
CurrentTrain: epoch  0, batch    23 | loss: 19.4873028Losses:  19.820650100708008 8.750687599182129 -0.0
CurrentTrain: epoch  0, batch    24 | loss: 19.8206501Losses:  19.20191192626953 8.10114860534668 -0.0
CurrentTrain: epoch  0, batch    25 | loss: 19.2019119Losses:  18.594894409179688 7.69716739654541 -0.0
CurrentTrain: epoch  0, batch    26 | loss: 18.5948944Losses:  19.05316925048828 8.049233436584473 -0.0
CurrentTrain: epoch  0, batch    27 | loss: 19.0531693Losses:  18.236467361450195 7.353152751922607 -0.0
CurrentTrain: epoch  0, batch    28 | loss: 18.2364674Losses:  20.615087509155273 9.905105590820312 -0.0
CurrentTrain: epoch  0, batch    29 | loss: 20.6150875Losses:  18.38874626159668 7.937572479248047 -0.0
CurrentTrain: epoch  0, batch    30 | loss: 18.3887463Losses:  20.281753540039062 9.67154312133789 -0.0
CurrentTrain: epoch  0, batch    31 | loss: 20.2817535Losses:  20.245861053466797 9.445650100708008 -0.0
CurrentTrain: epoch  0, batch    32 | loss: 20.2458611Losses:  16.608623504638672 6.814113616943359 -0.0
CurrentTrain: epoch  0, batch    33 | loss: 16.6086235Losses:  19.862049102783203 9.926962852478027 -0.0
CurrentTrain: epoch  0, batch    34 | loss: 19.8620491Losses:  16.96384048461914 6.755550384521484 -0.0
CurrentTrain: epoch  0, batch    35 | loss: 16.9638405Losses:  16.430273056030273 6.661989688873291 -0.0
CurrentTrain: epoch  0, batch    36 | loss: 16.4302731Losses:  17.498205184936523 6.857356548309326 -0.0
CurrentTrain: epoch  0, batch    37 | loss: 17.4982052Losses:  20.245634078979492 11.269350051879883 -0.0
CurrentTrain: epoch  1, batch     0 | loss: 20.2456341Losses:  19.30756950378418 9.476011276245117 -0.0
CurrentTrain: epoch  1, batch     1 | loss: 19.3075695Losses:  18.48070526123047 8.859548568725586 -0.0
CurrentTrain: epoch  1, batch     2 | loss: 18.4807053Losses:  20.140295028686523 9.835783004760742 -0.0
CurrentTrain: epoch  1, batch     3 | loss: 20.1402950Losses:  18.120464324951172 8.79504108428955 -0.0
CurrentTrain: epoch  1, batch     4 | loss: 18.1204643Losses:  18.110187530517578 8.334820747375488 -0.0
CurrentTrain: epoch  1, batch     5 | loss: 18.1101875Losses:  18.80664825439453 8.987041473388672 -0.0
CurrentTrain: epoch  1, batch     6 | loss: 18.8066483Losses:  16.635215759277344 6.716254234313965 -0.0
CurrentTrain: epoch  1, batch     7 | loss: 16.6352158Losses:  16.550018310546875 6.867295742034912 -0.0
CurrentTrain: epoch  1, batch     8 | loss: 16.5500183Losses:  21.559207916259766 11.230451583862305 -0.0
CurrentTrain: epoch  1, batch     9 | loss: 21.5592079Losses:  14.514909744262695 5.446085453033447 -0.0
CurrentTrain: epoch  1, batch    10 | loss: 14.5149097Losses:  17.561004638671875 7.723738670349121 -0.0
CurrentTrain: epoch  1, batch    11 | loss: 17.5610046Losses:  16.821821212768555 7.2917914390563965 -0.0
CurrentTrain: epoch  1, batch    12 | loss: 16.8218212Losses:  15.923152923583984 7.449936389923096 -0.0
CurrentTrain: epoch  1, batch    13 | loss: 15.9231529Losses:  16.95306396484375 7.090758800506592 -0.0
CurrentTrain: epoch  1, batch    14 | loss: 16.9530640Losses:  20.323585510253906 10.310503959655762 -0.0
CurrentTrain: epoch  1, batch    15 | loss: 20.3235855Losses:  16.451032638549805 7.1935272216796875 -0.0
CurrentTrain: epoch  1, batch    16 | loss: 16.4510326Losses:  15.82010555267334 7.005795478820801 -0.0
CurrentTrain: epoch  1, batch    17 | loss: 15.8201056Losses:  14.855289459228516 5.682424545288086 -0.0
CurrentTrain: epoch  1, batch    18 | loss: 14.8552895Losses:  19.97917938232422 9.923303604125977 -0.0
CurrentTrain: epoch  1, batch    19 | loss: 19.9791794Losses:  16.85940933227539 7.320980548858643 -0.0
CurrentTrain: epoch  1, batch    20 | loss: 16.8594093Losses:  18.822479248046875 8.71130084991455 -0.0
CurrentTrain: epoch  1, batch    21 | loss: 18.8224792Losses:  17.213714599609375 7.14522647857666 -0.0
CurrentTrain: epoch  1, batch    22 | loss: 17.2137146Losses:  17.373390197753906 7.325212001800537 -0.0
CurrentTrain: epoch  1, batch    23 | loss: 17.3733902Losses:  15.860217094421387 7.611050605773926 -0.0
CurrentTrain: epoch  1, batch    24 | loss: 15.8602171Losses:  18.016504287719727 8.901596069335938 -0.0
CurrentTrain: epoch  1, batch    25 | loss: 18.0165043Losses:  16.208959579467773 7.075340747833252 -0.0
CurrentTrain: epoch  1, batch    26 | loss: 16.2089596Losses:  16.084068298339844 7.119296550750732 -0.0
CurrentTrain: epoch  1, batch    27 | loss: 16.0840683Losses:  17.21484375 7.692984580993652 -0.0
CurrentTrain: epoch  1, batch    28 | loss: 17.2148438Losses:  15.04517650604248 5.889412879943848 -0.0
CurrentTrain: epoch  1, batch    29 | loss: 15.0451765Losses:  15.811692237854004 6.8689775466918945 -0.0
CurrentTrain: epoch  1, batch    30 | loss: 15.8116922Losses:  16.11139678955078 6.569716930389404 -0.0
CurrentTrain: epoch  1, batch    31 | loss: 16.1113968Losses:  14.015970230102539 5.269204616546631 -0.0
CurrentTrain: epoch  1, batch    32 | loss: 14.0159702Losses:  20.789382934570312 11.332841873168945 -0.0
CurrentTrain: epoch  1, batch    33 | loss: 20.7893829Losses:  17.015657424926758 8.167360305786133 -0.0
CurrentTrain: epoch  1, batch    34 | loss: 17.0156574Losses:  16.364789962768555 7.314637660980225 -0.0
CurrentTrain: epoch  1, batch    35 | loss: 16.3647900Losses:  15.282049179077148 6.22404146194458 -0.0
CurrentTrain: epoch  1, batch    36 | loss: 15.2820492Losses:  9.848040580749512 1.1800848245620728 -0.0
CurrentTrain: epoch  1, batch    37 | loss: 9.8480406Losses:  18.565773010253906 8.935956954956055 -0.0
CurrentTrain: epoch  2, batch     0 | loss: 18.5657730Losses:  15.476598739624023 6.406160354614258 -0.0
CurrentTrain: epoch  2, batch     1 | loss: 15.4765987Losses:  14.299922943115234 5.819042205810547 -0.0
CurrentTrain: epoch  2, batch     2 | loss: 14.2999229Losses:  16.580482482910156 8.307365417480469 -0.0
CurrentTrain: epoch  2, batch     3 | loss: 16.5804825Losses:  18.023365020751953 10.359445571899414 -0.0
CurrentTrain: epoch  2, batch     4 | loss: 18.0233650Losses:  15.779947280883789 6.445211410522461 -0.0
CurrentTrain: epoch  2, batch     5 | loss: 15.7799473Losses:  15.063741683959961 7.0284953117370605 -0.0
CurrentTrain: epoch  2, batch     6 | loss: 15.0637417Losses:  24.007619857788086 15.073726654052734 -0.0
CurrentTrain: epoch  2, batch     7 | loss: 24.0076199Losses:  14.929738998413086 7.168386936187744 -0.0
CurrentTrain: epoch  2, batch     8 | loss: 14.9297390Losses:  15.102158546447754 7.495513916015625 -0.0
CurrentTrain: epoch  2, batch     9 | loss: 15.1021585Losses:  15.061046600341797 6.283422946929932 -0.0
CurrentTrain: epoch  2, batch    10 | loss: 15.0610466Losses:  14.728063583374023 7.137997627258301 -0.0
CurrentTrain: epoch  2, batch    11 | loss: 14.7280636Losses:  17.079978942871094 8.059362411499023 -0.0
CurrentTrain: epoch  2, batch    12 | loss: 17.0799789Losses:  16.0751895904541 7.554286003112793 -0.0
CurrentTrain: epoch  2, batch    13 | loss: 16.0751896Losses:  13.594000816345215 6.038721561431885 -0.0
CurrentTrain: epoch  2, batch    14 | loss: 13.5940008Losses:  15.033626556396484 6.363061904907227 -0.0
CurrentTrain: epoch  2, batch    15 | loss: 15.0336266Losses:  15.260303497314453 7.336048126220703 -0.0
CurrentTrain: epoch  2, batch    16 | loss: 15.2603035Losses:  14.264936447143555 7.179415702819824 -0.0
CurrentTrain: epoch  2, batch    17 | loss: 14.2649364Losses:  15.085183143615723 6.172725677490234 -0.0
CurrentTrain: epoch  2, batch    18 | loss: 15.0851831Losses:  16.476179122924805 8.221309661865234 -0.0
CurrentTrain: epoch  2, batch    19 | loss: 16.4761791Losses:  19.473983764648438 10.564554214477539 -0.0
CurrentTrain: epoch  2, batch    20 | loss: 19.4739838Losses:  13.164762496948242 5.25059700012207 -0.0
CurrentTrain: epoch  2, batch    21 | loss: 13.1647625Losses:  17.752506256103516 8.423933029174805 -0.0
CurrentTrain: epoch  2, batch    22 | loss: 17.7525063Losses:  16.100215911865234 8.023378372192383 -0.0
CurrentTrain: epoch  2, batch    23 | loss: 16.1002159Losses:  18.108341217041016 9.214485168457031 -0.0
CurrentTrain: epoch  2, batch    24 | loss: 18.1083412Losses:  15.538496017456055 6.734121322631836 -0.0
CurrentTrain: epoch  2, batch    25 | loss: 15.5384960Losses:  14.134171485900879 6.725777626037598 -0.0
CurrentTrain: epoch  2, batch    26 | loss: 14.1341715Losses:  13.53564167022705 5.308736801147461 -0.0
CurrentTrain: epoch  2, batch    27 | loss: 13.5356417Losses:  17.074277877807617 8.888386726379395 -0.0
CurrentTrain: epoch  2, batch    28 | loss: 17.0742779Losses:  13.160726547241211 5.778026580810547 -0.0
CurrentTrain: epoch  2, batch    29 | loss: 13.1607265Losses:  12.927724838256836 5.115988731384277 -0.0
CurrentTrain: epoch  2, batch    30 | loss: 12.9277248Losses:  15.647323608398438 7.764892578125 -0.0
CurrentTrain: epoch  2, batch    31 | loss: 15.6473236Losses:  18.980403900146484 10.189273834228516 -0.0
CurrentTrain: epoch  2, batch    32 | loss: 18.9804039Losses:  12.381949424743652 5.34727144241333 -0.0
CurrentTrain: epoch  2, batch    33 | loss: 12.3819494Losses:  12.609302520751953 5.309325218200684 -0.0
CurrentTrain: epoch  2, batch    34 | loss: 12.6093025Losses:  12.85333251953125 4.705021858215332 -0.0
CurrentTrain: epoch  2, batch    35 | loss: 12.8533325Losses:  13.355562210083008 5.239421844482422 -0.0
CurrentTrain: epoch  2, batch    36 | loss: 13.3555622Losses:  9.265384674072266 0.7960814237594604 -0.0
CurrentTrain: epoch  2, batch    37 | loss: 9.2653847Losses:  12.596717834472656 4.860370635986328 -0.0
CurrentTrain: epoch  3, batch     0 | loss: 12.5967178Losses:  17.659175872802734 9.36038589477539 -0.0
CurrentTrain: epoch  3, batch     1 | loss: 17.6591759Losses:  14.366379737854004 7.22434663772583 -0.0
CurrentTrain: epoch  3, batch     2 | loss: 14.3663797Losses:  13.380117416381836 6.0086164474487305 -0.0
CurrentTrain: epoch  3, batch     3 | loss: 13.3801174Losses:  21.487091064453125 13.754871368408203 -0.0
CurrentTrain: epoch  3, batch     4 | loss: 21.4870911Losses:  16.22487449645996 8.325014114379883 -0.0
CurrentTrain: epoch  3, batch     5 | loss: 16.2248745Losses:  15.191850662231445 7.588730812072754 -0.0
CurrentTrain: epoch  3, batch     6 | loss: 15.1918507Losses:  15.117273330688477 6.828566074371338 -0.0
CurrentTrain: epoch  3, batch     7 | loss: 15.1172733Losses:  14.071440696716309 7.531844615936279 -0.0
CurrentTrain: epoch  3, batch     8 | loss: 14.0714407Losses:  14.524704933166504 6.5049028396606445 -0.0
CurrentTrain: epoch  3, batch     9 | loss: 14.5247049Losses:  16.355201721191406 6.824610233306885 -0.0
CurrentTrain: epoch  3, batch    10 | loss: 16.3552017Losses:  13.309050559997559 6.464906215667725 -0.0
CurrentTrain: epoch  3, batch    11 | loss: 13.3090506Losses:  16.110984802246094 8.43313217163086 -0.0
CurrentTrain: epoch  3, batch    12 | loss: 16.1109848Losses:  19.57052230834961 10.368565559387207 -0.0
CurrentTrain: epoch  3, batch    13 | loss: 19.5705223Losses:  12.180988311767578 4.98963737487793 -0.0
CurrentTrain: epoch  3, batch    14 | loss: 12.1809883Losses:  12.234872817993164 5.454398155212402 -0.0
CurrentTrain: epoch  3, batch    15 | loss: 12.2348728Losses:  13.89997673034668 6.18718957901001 -0.0
CurrentTrain: epoch  3, batch    16 | loss: 13.8999767Losses:  13.636611938476562 6.384204864501953 -0.0
CurrentTrain: epoch  3, batch    17 | loss: 13.6366119Losses:  12.689990997314453 6.353244781494141 -0.0
CurrentTrain: epoch  3, batch    18 | loss: 12.6899910Losses:  12.382966041564941 4.804330825805664 -0.0
CurrentTrain: epoch  3, batch    19 | loss: 12.3829660Losses:  11.718338012695312 4.712875843048096 -0.0
CurrentTrain: epoch  3, batch    20 | loss: 11.7183380Losses:  12.061832427978516 4.710134506225586 -0.0
CurrentTrain: epoch  3, batch    21 | loss: 12.0618324Losses:  12.754654884338379 6.427347183227539 -0.0
CurrentTrain: epoch  3, batch    22 | loss: 12.7546549Losses:  14.341483116149902 6.621565818786621 -0.0
CurrentTrain: epoch  3, batch    23 | loss: 14.3414831Losses:  14.094324111938477 6.789881706237793 -0.0
CurrentTrain: epoch  3, batch    24 | loss: 14.0943241Losses:  16.886484146118164 9.227436065673828 -0.0
CurrentTrain: epoch  3, batch    25 | loss: 16.8864841Losses:  15.484081268310547 6.843078136444092 -0.0
CurrentTrain: epoch  3, batch    26 | loss: 15.4840813Losses:  16.764873504638672 8.420405387878418 -0.0
CurrentTrain: epoch  3, batch    27 | loss: 16.7648735Losses:  13.044543266296387 5.651042938232422 -0.0
CurrentTrain: epoch  3, batch    28 | loss: 13.0445433Losses:  13.81533432006836 6.09873104095459 -0.0
CurrentTrain: epoch  3, batch    29 | loss: 13.8153343Losses:  13.781638145446777 6.974638938903809 -0.0
CurrentTrain: epoch  3, batch    30 | loss: 13.7816381Losses:  15.334556579589844 6.429246425628662 -0.0
CurrentTrain: epoch  3, batch    31 | loss: 15.3345566Losses:  12.80434799194336 5.273082256317139 -0.0
CurrentTrain: epoch  3, batch    32 | loss: 12.8043480Losses:  14.184444427490234 6.411486625671387 -0.0
CurrentTrain: epoch  3, batch    33 | loss: 14.1844444Losses:  14.453107833862305 6.879350185394287 -0.0
CurrentTrain: epoch  3, batch    34 | loss: 14.4531078Losses:  13.258781433105469 5.067705154418945 -0.0
CurrentTrain: epoch  3, batch    35 | loss: 13.2587814Losses:  16.470996856689453 8.901134490966797 -0.0
CurrentTrain: epoch  3, batch    36 | loss: 16.4709969Losses:  9.265265464782715 1.7364479303359985 -0.0
CurrentTrain: epoch  3, batch    37 | loss: 9.2652655Losses:  13.165767669677734 6.3796844482421875 -0.0
CurrentTrain: epoch  4, batch     0 | loss: 13.1657677Losses:  16.148910522460938 8.052858352661133 -0.0
CurrentTrain: epoch  4, batch     1 | loss: 16.1489105Losses:  17.269493103027344 8.575750350952148 -0.0
CurrentTrain: epoch  4, batch     2 | loss: 17.2694931Losses:  15.83248519897461 7.718752861022949 -0.0
CurrentTrain: epoch  4, batch     3 | loss: 15.8324852Losses:  11.667662620544434 4.430805683135986 -0.0
CurrentTrain: epoch  4, batch     4 | loss: 11.6676626Losses:  12.33060359954834 5.287923812866211 -0.0
CurrentTrain: epoch  4, batch     5 | loss: 12.3306036Losses:  15.242481231689453 8.32956314086914 -0.0
CurrentTrain: epoch  4, batch     6 | loss: 15.2424812Losses:  17.151382446289062 10.010506629943848 -0.0
CurrentTrain: epoch  4, batch     7 | loss: 17.1513824Losses:  14.260348320007324 7.141648292541504 -0.0
CurrentTrain: epoch  4, batch     8 | loss: 14.2603483Losses:  14.457261085510254 6.943340301513672 -0.0
CurrentTrain: epoch  4, batch     9 | loss: 14.4572611Losses:  17.435657501220703 11.021949768066406 -0.0
CurrentTrain: epoch  4, batch    10 | loss: 17.4356575Losses:  12.23827838897705 5.200908660888672 -0.0
CurrentTrain: epoch  4, batch    11 | loss: 12.2382784Losses:  15.620553970336914 7.525492191314697 -0.0
CurrentTrain: epoch  4, batch    12 | loss: 15.6205540Losses:  17.900632858276367 9.98798942565918 -0.0
CurrentTrain: epoch  4, batch    13 | loss: 17.9006329Losses:  16.377960205078125 9.306316375732422 -0.0
CurrentTrain: epoch  4, batch    14 | loss: 16.3779602Losses:  11.331001281738281 4.36093807220459 -0.0
CurrentTrain: epoch  4, batch    15 | loss: 11.3310013Losses:  11.99537467956543 4.913118362426758 -0.0
CurrentTrain: epoch  4, batch    16 | loss: 11.9953747Losses:  13.95048713684082 6.0707902908325195 -0.0
CurrentTrain: epoch  4, batch    17 | loss: 13.9504871Losses:  15.873466491699219 8.378216743469238 -0.0
CurrentTrain: epoch  4, batch    18 | loss: 15.8734665Losses:  15.02646541595459 7.472812175750732 -0.0
CurrentTrain: epoch  4, batch    19 | loss: 15.0264654Losses:  21.192150115966797 13.394095420837402 -0.0
CurrentTrain: epoch  4, batch    20 | loss: 21.1921501Losses:  11.577234268188477 5.122894763946533 -0.0
CurrentTrain: epoch  4, batch    21 | loss: 11.5772343Losses:  14.302164077758789 7.390844345092773 -0.0
CurrentTrain: epoch  4, batch    22 | loss: 14.3021641Losses:  17.998538970947266 9.66948413848877 -0.0
CurrentTrain: epoch  4, batch    23 | loss: 17.9985390Losses:  12.142962455749512 5.335330963134766 -0.0
CurrentTrain: epoch  4, batch    24 | loss: 12.1429625Losses:  10.868451118469238 4.656113624572754 -0.0
CurrentTrain: epoch  4, batch    25 | loss: 10.8684511Losses:  14.34045696258545 8.07878303527832 -0.0
CurrentTrain: epoch  4, batch    26 | loss: 14.3404570Losses:  16.03927230834961 8.076776504516602 -0.0
CurrentTrain: epoch  4, batch    27 | loss: 16.0392723Losses:  14.129680633544922 7.159435749053955 -0.0
CurrentTrain: epoch  4, batch    28 | loss: 14.1296806Losses:  14.34356689453125 6.9249267578125 -0.0
CurrentTrain: epoch  4, batch    29 | loss: 14.3435669Losses:  11.385815620422363 4.541512966156006 -0.0
CurrentTrain: epoch  4, batch    30 | loss: 11.3858156Losses:  10.854246139526367 4.2590813636779785 -0.0
CurrentTrain: epoch  4, batch    31 | loss: 10.8542461Losses:  14.924988746643066 8.102046966552734 -0.0
CurrentTrain: epoch  4, batch    32 | loss: 14.9249887Losses:  10.685585021972656 4.147978782653809 -0.0
CurrentTrain: epoch  4, batch    33 | loss: 10.6855850Losses:  13.13470458984375 6.501561641693115 -0.0
CurrentTrain: epoch  4, batch    34 | loss: 13.1347046Losses:  13.999363899230957 7.5449604988098145 -0.0
CurrentTrain: epoch  4, batch    35 | loss: 13.9993639Losses:  15.277748107910156 9.586795806884766 -0.0
CurrentTrain: epoch  4, batch    36 | loss: 15.2777481Losses:  7.584624290466309 1.7089991569519043 -0.0
CurrentTrain: epoch  4, batch    37 | loss: 7.5846243Losses:  12.545061111450195 5.846863746643066 -0.0
CurrentTrain: epoch  5, batch     0 | loss: 12.5450611Losses:  13.102222442626953 7.377819061279297 -0.0
CurrentTrain: epoch  5, batch     1 | loss: 13.1022224Losses:  11.511199951171875 4.9076924324035645 -0.0
CurrentTrain: epoch  5, batch     2 | loss: 11.5112000Losses:  11.203360557556152 4.733264923095703 -0.0
CurrentTrain: epoch  5, batch     3 | loss: 11.2033606Losses:  14.567131996154785 7.7983245849609375 -0.0
CurrentTrain: epoch  5, batch     4 | loss: 14.5671320Losses:  11.840641021728516 4.975778579711914 -0.0
CurrentTrain: epoch  5, batch     5 | loss: 11.8406410Losses:  12.721170425415039 5.881478309631348 -0.0
CurrentTrain: epoch  5, batch     6 | loss: 12.7211704Losses:  15.866239547729492 9.276888847351074 -0.0
CurrentTrain: epoch  5, batch     7 | loss: 15.8662395Losses:  13.438426971435547 6.209538459777832 -0.0
CurrentTrain: epoch  5, batch     8 | loss: 13.4384270Losses:  11.917648315429688 4.770967483520508 -0.0
CurrentTrain: epoch  5, batch     9 | loss: 11.9176483Losses:  15.555259704589844 8.662038803100586 -0.0
CurrentTrain: epoch  5, batch    10 | loss: 15.5552597Losses:  14.271846771240234 7.484131336212158 -0.0
CurrentTrain: epoch  5, batch    11 | loss: 14.2718468Losses:  16.065433502197266 8.634944915771484 -0.0
CurrentTrain: epoch  5, batch    12 | loss: 16.0654335Losses:  14.947619438171387 9.401352882385254 -0.0
CurrentTrain: epoch  5, batch    13 | loss: 14.9476194Losses:  12.287822723388672 5.518057823181152 -0.0
CurrentTrain: epoch  5, batch    14 | loss: 12.2878227Losses:  12.291566848754883 5.6317830085754395 -0.0
CurrentTrain: epoch  5, batch    15 | loss: 12.2915668Losses:  11.214001655578613 4.376984596252441 -0.0
CurrentTrain: epoch  5, batch    16 | loss: 11.2140017Losses:  12.701530456542969 6.966428756713867 -0.0
CurrentTrain: epoch  5, batch    17 | loss: 12.7015305Losses:  13.316431045532227 6.981339454650879 -0.0
CurrentTrain: epoch  5, batch    18 | loss: 13.3164310Losses:  11.282185554504395 5.435168266296387 -0.0
CurrentTrain: epoch  5, batch    19 | loss: 11.2821856Losses:  10.756488800048828 4.5121049880981445 -0.0
CurrentTrain: epoch  5, batch    20 | loss: 10.7564888Losses:  12.180611610412598 5.715516090393066 -0.0
CurrentTrain: epoch  5, batch    21 | loss: 12.1806116Losses:  10.482613563537598 4.802425384521484 -0.0
CurrentTrain: epoch  5, batch    22 | loss: 10.4826136Losses:  13.668574333190918 6.863393783569336 -0.0
CurrentTrain: epoch  5, batch    23 | loss: 13.6685743Losses:  11.172637939453125 4.5753021240234375 -0.0
CurrentTrain: epoch  5, batch    24 | loss: 11.1726379Losses:  11.573431015014648 4.819699287414551 -0.0
CurrentTrain: epoch  5, batch    25 | loss: 11.5734310Losses:  11.537666320800781 6.0666351318359375 -0.0
CurrentTrain: epoch  5, batch    26 | loss: 11.5376663Losses:  11.668218612670898 4.970943450927734 -0.0
CurrentTrain: epoch  5, batch    27 | loss: 11.6682186Losses:  11.508081436157227 5.721700191497803 -0.0
CurrentTrain: epoch  5, batch    28 | loss: 11.5080814Losses:  11.9130859375 5.055354595184326 -0.0
CurrentTrain: epoch  5, batch    29 | loss: 11.9130859Losses:  15.845552444458008 9.570428848266602 -0.0
CurrentTrain: epoch  5, batch    30 | loss: 15.8455524Losses:  13.79680347442627 7.143439292907715 -0.0
CurrentTrain: epoch  5, batch    31 | loss: 13.7968035Losses:  12.632329940795898 6.259677886962891 -0.0
CurrentTrain: epoch  5, batch    32 | loss: 12.6323299Losses:  11.613811492919922 5.213924884796143 -0.0
CurrentTrain: epoch  5, batch    33 | loss: 11.6138115Losses:  10.221941947937012 4.491927146911621 -0.0
CurrentTrain: epoch  5, batch    34 | loss: 10.2219419Losses:  12.175161361694336 6.415145397186279 -0.0
CurrentTrain: epoch  5, batch    35 | loss: 12.1751614Losses:  11.992438316345215 5.938005447387695 -0.0
CurrentTrain: epoch  5, batch    36 | loss: 11.9924383Losses:  9.520756721496582 2.0528078079223633 -0.0
CurrentTrain: epoch  5, batch    37 | loss: 9.5207567Losses:  10.927982330322266 5.334954261779785 -0.0
CurrentTrain: epoch  6, batch     0 | loss: 10.9279823Losses:  13.687338829040527 7.211774826049805 -0.0
CurrentTrain: epoch  6, batch     1 | loss: 13.6873388Losses:  10.834210395812988 5.336706161499023 -0.0
CurrentTrain: epoch  6, batch     2 | loss: 10.8342104Losses:  14.309267044067383 8.082172393798828 -0.0
CurrentTrain: epoch  6, batch     3 | loss: 14.3092670Losses:  11.069424629211426 5.536820411682129 -0.0
CurrentTrain: epoch  6, batch     4 | loss: 11.0694246Losses:  15.042707443237305 9.998645782470703 -0.0
CurrentTrain: epoch  6, batch     5 | loss: 15.0427074Losses:  11.652322769165039 6.143512725830078 -0.0
CurrentTrain: epoch  6, batch     6 | loss: 11.6523228Losses:  15.6438627243042 9.659463882446289 -0.0
CurrentTrain: epoch  6, batch     7 | loss: 15.6438627Losses:  11.548772811889648 5.9414238929748535 -0.0
CurrentTrain: epoch  6, batch     8 | loss: 11.5487728Losses:  9.25693130493164 4.007541656494141 -0.0
CurrentTrain: epoch  6, batch     9 | loss: 9.2569313Losses:  10.361270904541016 4.445841312408447 -0.0
CurrentTrain: epoch  6, batch    10 | loss: 10.3612709Losses:  10.755233764648438 4.631153106689453 -0.0
CurrentTrain: epoch  6, batch    11 | loss: 10.7552338Losses:  14.024442672729492 7.637080669403076 -0.0
CurrentTrain: epoch  6, batch    12 | loss: 14.0244427Losses:  13.521295547485352 7.602280139923096 -0.0
CurrentTrain: epoch  6, batch    13 | loss: 13.5212955Losses:  11.898277282714844 5.660391807556152 -0.0
CurrentTrain: epoch  6, batch    14 | loss: 11.8982773Losses:  14.779607772827148 7.715704441070557 -0.0
CurrentTrain: epoch  6, batch    15 | loss: 14.7796078Losses:  11.753812789916992 6.097956657409668 -0.0
CurrentTrain: epoch  6, batch    16 | loss: 11.7538128Losses:  11.071610450744629 5.013275146484375 -0.0
CurrentTrain: epoch  6, batch    17 | loss: 11.0716105Losses:  10.508014678955078 4.948188781738281 -0.0
CurrentTrain: epoch  6, batch    18 | loss: 10.5080147Losses:  10.718398094177246 4.642858028411865 -0.0
CurrentTrain: epoch  6, batch    19 | loss: 10.7183981Losses:  10.915410995483398 5.762618541717529 -0.0
CurrentTrain: epoch  6, batch    20 | loss: 10.9154110Losses:  16.016117095947266 9.605716705322266 -0.0
CurrentTrain: epoch  6, batch    21 | loss: 16.0161171Losses:  10.661544799804688 5.251618385314941 -0.0
CurrentTrain: epoch  6, batch    22 | loss: 10.6615448Losses:  12.060064315795898 6.764822006225586 -0.0
CurrentTrain: epoch  6, batch    23 | loss: 12.0600643Losses:  10.118741035461426 4.903407573699951 -0.0
CurrentTrain: epoch  6, batch    24 | loss: 10.1187410Losses:  10.077585220336914 4.825414180755615 -0.0
CurrentTrain: epoch  6, batch    25 | loss: 10.0775852Losses:  12.71476936340332 6.702295303344727 -0.0
CurrentTrain: epoch  6, batch    26 | loss: 12.7147694Losses:  13.113207817077637 6.782796859741211 -0.0
CurrentTrain: epoch  6, batch    27 | loss: 13.1132078Losses:  11.328484535217285 5.2410054206848145 -0.0
CurrentTrain: epoch  6, batch    28 | loss: 11.3284845Losses:  12.005218505859375 5.212664604187012 -0.0
CurrentTrain: epoch  6, batch    29 | loss: 12.0052185Losses:  10.461668014526367 5.268728733062744 -0.0
CurrentTrain: epoch  6, batch    30 | loss: 10.4616680Losses:  9.717351913452148 4.117226600646973 -0.0
CurrentTrain: epoch  6, batch    31 | loss: 9.7173519Losses:  10.578360557556152 5.358983039855957 -0.0
CurrentTrain: epoch  6, batch    32 | loss: 10.5783606Losses:  11.769573211669922 6.277853012084961 -0.0
CurrentTrain: epoch  6, batch    33 | loss: 11.7695732Losses:  14.068804740905762 9.038884162902832 -0.0
CurrentTrain: epoch  6, batch    34 | loss: 14.0688047Losses:  10.971927642822266 4.845025062561035 -0.0
CurrentTrain: epoch  6, batch    35 | loss: 10.9719276Losses:  9.872682571411133 4.323951244354248 -0.0
CurrentTrain: epoch  6, batch    36 | loss: 9.8726826Losses:  6.469925880432129 0.7634541988372803 -0.0
CurrentTrain: epoch  6, batch    37 | loss: 6.4699259Losses:  10.567508697509766 4.949678421020508 -0.0
CurrentTrain: epoch  7, batch     0 | loss: 10.5675087Losses:  16.19245719909668 9.822965621948242 -0.0
CurrentTrain: epoch  7, batch     1 | loss: 16.1924572Losses:  12.319436073303223 6.894905090332031 -0.0
CurrentTrain: epoch  7, batch     2 | loss: 12.3194361Losses:  10.98476791381836 5.410470962524414 -0.0
CurrentTrain: epoch  7, batch     3 | loss: 10.9847679Losses:  13.236637115478516 7.122109889984131 -0.0
CurrentTrain: epoch  7, batch     4 | loss: 13.2366371Losses:  11.30817985534668 4.59246301651001 -0.0
CurrentTrain: epoch  7, batch     5 | loss: 11.3081799Losses:  11.08055305480957 5.5556535720825195 -0.0
CurrentTrain: epoch  7, batch     6 | loss: 11.0805531Losses:  13.061172485351562 6.546122074127197 -0.0
CurrentTrain: epoch  7, batch     7 | loss: 13.0611725Losses:  9.45247745513916 3.9352312088012695 -0.0
CurrentTrain: epoch  7, batch     8 | loss: 9.4524775Losses:  10.347761154174805 3.8604812622070312 -0.0
CurrentTrain: epoch  7, batch     9 | loss: 10.3477612Losses:  10.884939193725586 4.348223686218262 -0.0
CurrentTrain: epoch  7, batch    10 | loss: 10.8849392Losses:  12.36789321899414 6.107578754425049 -0.0
CurrentTrain: epoch  7, batch    11 | loss: 12.3678932Losses:  12.602535247802734 6.961182594299316 -0.0
CurrentTrain: epoch  7, batch    12 | loss: 12.6025352Losses:  10.052056312561035 4.718661308288574 -0.0
CurrentTrain: epoch  7, batch    13 | loss: 10.0520563Losses:  12.44722843170166 7.398547172546387 -0.0
CurrentTrain: epoch  7, batch    14 | loss: 12.4472284Losses:  12.301568031311035 4.97848653793335 -0.0
CurrentTrain: epoch  7, batch    15 | loss: 12.3015680Losses:  11.026816368103027 4.466482162475586 -0.0
CurrentTrain: epoch  7, batch    16 | loss: 11.0268164Losses:  12.226593017578125 6.979451656341553 -0.0
CurrentTrain: epoch  7, batch    17 | loss: 12.2265930Losses:  13.965677261352539 7.780481338500977 -0.0
CurrentTrain: epoch  7, batch    18 | loss: 13.9656773Losses:  9.513108253479004 3.9441065788269043 -0.0
CurrentTrain: epoch  7, batch    19 | loss: 9.5131083Losses:  9.221786499023438 4.202361106872559 -0.0
CurrentTrain: epoch  7, batch    20 | loss: 9.2217865Losses:  11.884527206420898 5.893601417541504 -0.0
CurrentTrain: epoch  7, batch    21 | loss: 11.8845272Losses:  14.966487884521484 9.2756929397583 -0.0
CurrentTrain: epoch  7, batch    22 | loss: 14.9664879Losses:  9.881011962890625 4.179529190063477 -0.0
CurrentTrain: epoch  7, batch    23 | loss: 9.8810120Losses:  11.239992141723633 5.840653419494629 -0.0
CurrentTrain: epoch  7, batch    24 | loss: 11.2399921Losses:  11.20981216430664 5.660754203796387 -0.0
CurrentTrain: epoch  7, batch    25 | loss: 11.2098122Losses:  14.173196792602539 7.998867988586426 -0.0
CurrentTrain: epoch  7, batch    26 | loss: 14.1731968Losses:  10.635659217834473 5.201024055480957 -0.0
CurrentTrain: epoch  7, batch    27 | loss: 10.6356592Losses:  11.27022933959961 5.746357440948486 -0.0
CurrentTrain: epoch  7, batch    28 | loss: 11.2702293Losses:  14.627969741821289 8.75782585144043 -0.0
CurrentTrain: epoch  7, batch    29 | loss: 14.6279697Losses:  14.195174217224121 8.3696870803833 -0.0
CurrentTrain: epoch  7, batch    30 | loss: 14.1951742Losses:  10.83527946472168 5.3484039306640625 -0.0
CurrentTrain: epoch  7, batch    31 | loss: 10.8352795Losses:  10.023172378540039 4.651614189147949 -0.0
CurrentTrain: epoch  7, batch    32 | loss: 10.0231724Losses:  12.955599784851074 7.964435577392578 -0.0
CurrentTrain: epoch  7, batch    33 | loss: 12.9555998Losses:  11.115365028381348 5.522330284118652 -0.0
CurrentTrain: epoch  7, batch    34 | loss: 11.1153650Losses:  9.32291030883789 4.070948600769043 -0.0
CurrentTrain: epoch  7, batch    35 | loss: 9.3229103Losses:  9.066795349121094 3.904332160949707 -0.0
CurrentTrain: epoch  7, batch    36 | loss: 9.0667953Losses:  6.432044506072998 1.336501955986023 -0.0
CurrentTrain: epoch  7, batch    37 | loss: 6.4320445Losses:  10.721857070922852 5.5971455574035645 -0.0
CurrentTrain: epoch  8, batch     0 | loss: 10.7218571Losses:  9.564241409301758 4.644237041473389 -0.0
CurrentTrain: epoch  8, batch     1 | loss: 9.5642414Losses:  11.138522148132324 6.307773590087891 -0.0
CurrentTrain: epoch  8, batch     2 | loss: 11.1385221Losses:  12.723776817321777 7.924271583557129 -0.0
CurrentTrain: epoch  8, batch     3 | loss: 12.7237768Losses:  12.13155746459961 6.959536552429199 -0.0
CurrentTrain: epoch  8, batch     4 | loss: 12.1315575Losses:  12.131322860717773 6.148366928100586 -0.0
CurrentTrain: epoch  8, batch     5 | loss: 12.1313229Losses:  18.147146224975586 12.497224807739258 -0.0
CurrentTrain: epoch  8, batch     6 | loss: 18.1471462Losses:  12.371050834655762 6.585813522338867 -0.0
CurrentTrain: epoch  8, batch     7 | loss: 12.3710508Losses:  13.71079158782959 8.436763763427734 -0.0
CurrentTrain: epoch  8, batch     8 | loss: 13.7107916Losses:  10.375547409057617 5.15942907333374 -0.0
CurrentTrain: epoch  8, batch     9 | loss: 10.3755474Losses:  10.995280265808105 5.913825035095215 -0.0
CurrentTrain: epoch  8, batch    10 | loss: 10.9952803Losses:  8.530624389648438 3.565183639526367 -0.0
CurrentTrain: epoch  8, batch    11 | loss: 8.5306244Losses:  10.772172927856445 5.853686332702637 -0.0
CurrentTrain: epoch  8, batch    12 | loss: 10.7721729Losses:  9.601173400878906 4.648016929626465 -0.0
CurrentTrain: epoch  8, batch    13 | loss: 9.6011734Losses:  9.12635326385498 4.168658256530762 -0.0
CurrentTrain: epoch  8, batch    14 | loss: 9.1263533Losses:  13.42190170288086 7.861349582672119 -0.0
CurrentTrain: epoch  8, batch    15 | loss: 13.4219017Losses:  9.671835899353027 4.852729797363281 -0.0
CurrentTrain: epoch  8, batch    16 | loss: 9.6718359Losses:  14.113133430480957 6.514726638793945 -0.0
CurrentTrain: epoch  8, batch    17 | loss: 14.1131334Losses:  11.269329071044922 5.218512535095215 -0.0
CurrentTrain: epoch  8, batch    18 | loss: 11.2693291Losses:  9.703524589538574 4.276540756225586 -0.0
CurrentTrain: epoch  8, batch    19 | loss: 9.7035246Losses:  10.63907241821289 5.341457843780518 -0.0
CurrentTrain: epoch  8, batch    20 | loss: 10.6390724Losses:  14.333378791809082 7.216892242431641 -0.0
CurrentTrain: epoch  8, batch    21 | loss: 14.3333788Losses:  10.453916549682617 5.125428199768066 -0.0
CurrentTrain: epoch  8, batch    22 | loss: 10.4539165Losses:  12.268631935119629 6.015591621398926 -0.0
CurrentTrain: epoch  8, batch    23 | loss: 12.2686319Losses:  10.823383331298828 4.2329301834106445 -0.0
CurrentTrain: epoch  8, batch    24 | loss: 10.8233833Losses:  10.286459922790527 4.935619354248047 -0.0
CurrentTrain: epoch  8, batch    25 | loss: 10.2864599Losses:  12.711706161499023 7.312098503112793 -0.0
CurrentTrain: epoch  8, batch    26 | loss: 12.7117062Losses:  12.472850799560547 5.938011646270752 -0.0
CurrentTrain: epoch  8, batch    27 | loss: 12.4728508Losses:  11.239656448364258 5.696814060211182 -0.0
CurrentTrain: epoch  8, batch    28 | loss: 11.2396564Losses:  9.768009185791016 4.308900356292725 -0.0
CurrentTrain: epoch  8, batch    29 | loss: 9.7680092Losses:  9.696041107177734 4.184091567993164 -0.0
CurrentTrain: epoch  8, batch    30 | loss: 9.6960411Losses:  11.509944915771484 6.574671745300293 -0.0
CurrentTrain: epoch  8, batch    31 | loss: 11.5099449Losses:  11.906850814819336 6.845942497253418 -0.0
CurrentTrain: epoch  8, batch    32 | loss: 11.9068508Losses:  9.689002990722656 4.108273029327393 -0.0
CurrentTrain: epoch  8, batch    33 | loss: 9.6890030Losses:  11.23213005065918 5.298166751861572 -0.0
CurrentTrain: epoch  8, batch    34 | loss: 11.2321301Losses:  14.526121139526367 9.382000923156738 -0.0
CurrentTrain: epoch  8, batch    35 | loss: 14.5261211Losses:  9.367568016052246 4.2442474365234375 -0.0
CurrentTrain: epoch  8, batch    36 | loss: 9.3675680Losses:  8.532609939575195 2.6129074096679688 -0.0
CurrentTrain: epoch  8, batch    37 | loss: 8.5326099Losses:  10.715033531188965 5.759084701538086 -0.0
CurrentTrain: epoch  9, batch     0 | loss: 10.7150335Losses:  11.808116912841797 5.898415565490723 -0.0
CurrentTrain: epoch  9, batch     1 | loss: 11.8081169Losses:  17.948516845703125 10.59481430053711 -0.0
CurrentTrain: epoch  9, batch     2 | loss: 17.9485168Losses:  12.082706451416016 6.487144470214844 -0.0
CurrentTrain: epoch  9, batch     3 | loss: 12.0827065Losses:  13.179388046264648 7.323276996612549 -0.0
CurrentTrain: epoch  9, batch     4 | loss: 13.1793880Losses:  10.366374969482422 4.643892765045166 -0.0
CurrentTrain: epoch  9, batch     5 | loss: 10.3663750Losses:  10.59992790222168 5.587247371673584 -0.0
CurrentTrain: epoch  9, batch     6 | loss: 10.5999279Losses:  16.688827514648438 10.324747085571289 -0.0
CurrentTrain: epoch  9, batch     7 | loss: 16.6888275Losses:  14.62976360321045 9.149784088134766 -0.0
CurrentTrain: epoch  9, batch     8 | loss: 14.6297636Losses:  9.698366165161133 4.625690460205078 -0.0
CurrentTrain: epoch  9, batch     9 | loss: 9.6983662Losses:  13.375722885131836 7.360162734985352 -0.0
CurrentTrain: epoch  9, batch    10 | loss: 13.3757229Losses:  10.680838584899902 5.187071800231934 -0.0
CurrentTrain: epoch  9, batch    11 | loss: 10.6808386Losses:  11.752750396728516 6.564359188079834 -0.0
CurrentTrain: epoch  9, batch    12 | loss: 11.7527504Losses:  9.644968032836914 4.197384357452393 -0.0
CurrentTrain: epoch  9, batch    13 | loss: 9.6449680Losses:  11.86756706237793 6.5667877197265625 -0.0
CurrentTrain: epoch  9, batch    14 | loss: 11.8675671Losses:  10.32664680480957 4.969450950622559 -0.0
CurrentTrain: epoch  9, batch    15 | loss: 10.3266468Losses:  13.679121971130371 7.108721733093262 -0.0
CurrentTrain: epoch  9, batch    16 | loss: 13.6791220Losses:  11.857074737548828 6.677537441253662 -0.0
CurrentTrain: epoch  9, batch    17 | loss: 11.8570747Losses:  11.760702133178711 5.430853843688965 -0.0
CurrentTrain: epoch  9, batch    18 | loss: 11.7607021Losses:  10.072511672973633 4.68112850189209 -0.0
CurrentTrain: epoch  9, batch    19 | loss: 10.0725117Losses:  8.757654190063477 3.4630608558654785 -0.0
CurrentTrain: epoch  9, batch    20 | loss: 8.7576542Losses:  11.134172439575195 5.970253944396973 -0.0
CurrentTrain: epoch  9, batch    21 | loss: 11.1341724Losses:  10.470672607421875 5.426194667816162 -0.0
CurrentTrain: epoch  9, batch    22 | loss: 10.4706726Losses:  12.61146354675293 6.726362705230713 -0.0
CurrentTrain: epoch  9, batch    23 | loss: 12.6114635Losses:  12.235916137695312 6.303917407989502 -0.0
CurrentTrain: epoch  9, batch    24 | loss: 12.2359161Losses:  9.571836471557617 4.732731342315674 -0.0
CurrentTrain: epoch  9, batch    25 | loss: 9.5718365Losses:  11.115039825439453 4.850638389587402 -0.0
CurrentTrain: epoch  9, batch    26 | loss: 11.1150398Losses:  10.874168395996094 5.476101875305176 -0.0
CurrentTrain: epoch  9, batch    27 | loss: 10.8741684Losses:  12.4406099319458 6.118093490600586 -0.0
CurrentTrain: epoch  9, batch    28 | loss: 12.4406099Losses:  12.050342559814453 6.431064605712891 -0.0
CurrentTrain: epoch  9, batch    29 | loss: 12.0503426Losses:  10.186397552490234 5.004637718200684 -0.0
CurrentTrain: epoch  9, batch    30 | loss: 10.1863976Losses:  9.950516700744629 4.289102077484131 -0.0
CurrentTrain: epoch  9, batch    31 | loss: 9.9505167Losses:  15.629661560058594 10.339014053344727 -0.0
CurrentTrain: epoch  9, batch    32 | loss: 15.6296616Losses:  9.56541633605957 4.56166410446167 -0.0
CurrentTrain: epoch  9, batch    33 | loss: 9.5654163Losses:  11.997983932495117 6.555283069610596 -0.0
CurrentTrain: epoch  9, batch    34 | loss: 11.9979839Losses:  10.557624816894531 5.469417572021484 -0.0
CurrentTrain: epoch  9, batch    35 | loss: 10.5576248Losses:  11.480817794799805 5.9103169441223145 -0.0
CurrentTrain: epoch  9, batch    36 | loss: 11.4808178Losses:  6.056074619293213 0.514049232006073 -0.0
CurrentTrain: epoch  9, batch    37 | loss: 6.0560746
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 82.50%   [EVAL] batch:    5 | acc: 68.75%,  total acc: 80.21%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 82.14%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 84.38%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 85.42%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 86.25%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 88.02%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 88.46%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 87.05%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 85.83%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 83.98%   [EVAL] batch:   16 | acc: 81.25%,  total acc: 83.82%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 82.64%   [EVAL] batch:   18 | acc: 81.25%,  total acc: 82.57%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 82.81%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 83.63%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 84.38%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 85.05%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 85.68%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 86.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 86.78%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 87.04%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 87.93%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 87.71%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 87.70%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 87.89%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 86.36%   
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 82.50%   [EVAL] batch:    5 | acc: 68.75%,  total acc: 80.21%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 82.14%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 84.38%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 85.42%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 86.25%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 88.02%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 88.46%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 87.05%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 85.83%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 83.98%   [EVAL] batch:   16 | acc: 81.25%,  total acc: 83.82%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 82.64%   [EVAL] batch:   18 | acc: 81.25%,  total acc: 82.57%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 82.81%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 83.63%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 84.38%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 85.05%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 85.68%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 86.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 86.78%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 87.04%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 87.93%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 87.71%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 87.70%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 87.89%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 86.36%   
cur_acc:  ['0.8636']
his_acc:  ['0.8636']
Clustering into  4  clusters
Clusters:  [1 0 0 1 0 1 3 0 1 2 3]
Losses:  15.606729507446289 7.749983310699463 3.622107982635498
CurrentTrain: epoch  0, batch     0 | loss: 15.6067295Losses:  12.983512878417969 4.118052959442139 3.410236358642578
CurrentTrain: epoch  0, batch     1 | loss: 12.9835129Losses:  14.189348220825195 6.45140266418457 3.5382323265075684
CurrentTrain: epoch  1, batch     0 | loss: 14.1893482Losses:  10.050013542175293 1.9584070444107056 3.410627603530884
CurrentTrain: epoch  1, batch     1 | loss: 10.0500135Losses:  14.772851943969727 7.710151672363281 3.429969549179077
CurrentTrain: epoch  2, batch     0 | loss: 14.7728519Losses:  10.085912704467773 3.1563258171081543 3.3083624839782715
CurrentTrain: epoch  2, batch     1 | loss: 10.0859127Losses:  14.028495788574219 7.186455726623535 3.398010730743408
CurrentTrain: epoch  3, batch     0 | loss: 14.0284958Losses:  8.014862060546875 1.8605880737304688 3.326124668121338
CurrentTrain: epoch  3, batch     1 | loss: 8.0148621Losses:  12.131936073303223 6.289634704589844 3.329828977584839
CurrentTrain: epoch  4, batch     0 | loss: 12.1319361Losses:  9.182713508605957 2.4445128440856934 3.3672571182250977
CurrentTrain: epoch  4, batch     1 | loss: 9.1827135Losses:  12.840838432312012 6.446693420410156 3.3477065563201904
CurrentTrain: epoch  5, batch     0 | loss: 12.8408384Losses:  7.488921165466309 1.6893417835235596 3.3273134231567383
CurrentTrain: epoch  5, batch     1 | loss: 7.4889212Losses:  11.92540454864502 6.407532691955566 3.310361862182617
CurrentTrain: epoch  6, batch     0 | loss: 11.9254045Losses:  10.499935150146484 3.5271151065826416 3.3813610076904297
CurrentTrain: epoch  6, batch     1 | loss: 10.4999352Losses:  12.591512680053711 6.631128311157227 3.3071024417877197
CurrentTrain: epoch  7, batch     0 | loss: 12.5915127Losses:  8.946622848510742 2.8044161796569824 3.3520655632019043
CurrentTrain: epoch  7, batch     1 | loss: 8.9466228Losses:  12.939884185791016 6.744993209838867 3.3117270469665527
CurrentTrain: epoch  8, batch     0 | loss: 12.9398842Losses:  5.156312942504883 1.8634198904037476 1.3870190382003784
CurrentTrain: epoch  8, batch     1 | loss: 5.1563129Losses:  11.119526863098145 5.510931015014648 3.3142220973968506
CurrentTrain: epoch  9, batch     0 | loss: 11.1195269Losses:  8.276813507080078 2.087157726287842 3.3499488830566406
CurrentTrain: epoch  9, batch     1 | loss: 8.2768135
Losses:  7.214879035949707 -0.0 3.802004337310791
MemoryTrain:  epoch  0, batch     0 | loss: 7.2148790Losses:  6.883380889892578 -0.0 3.649402141571045
MemoryTrain:  epoch  1, batch     0 | loss: 6.8833809Losses:  6.1675920486450195 -0.0 3.548361301422119
MemoryTrain:  epoch  2, batch     0 | loss: 6.1675920Losses:  5.786716461181641 -0.0 3.6345884799957275
MemoryTrain:  epoch  3, batch     0 | loss: 5.7867165Losses:  5.386308670043945 -0.0 3.513172149658203
MemoryTrain:  epoch  4, batch     0 | loss: 5.3863087Losses:  5.237686634063721 -0.0 3.5095129013061523
MemoryTrain:  epoch  5, batch     0 | loss: 5.2376866Losses:  4.854245662689209 -0.0 3.436189889907837
MemoryTrain:  epoch  6, batch     0 | loss: 4.8542457Losses:  4.565053939819336 -0.0 3.45755672454834
MemoryTrain:  epoch  7, batch     0 | loss: 4.5650539Losses:  4.479821681976318 -0.0 3.4184155464172363
MemoryTrain:  epoch  8, batch     0 | loss: 4.4798217Losses:  4.971497058868408 -0.0 3.4352283477783203
MemoryTrain:  epoch  9, batch     0 | loss: 4.9714971
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 91.67%   [EVAL] batch:    3 | acc: 50.00%,  total acc: 81.25%   [EVAL] batch:    4 | acc: 25.00%,  total acc: 70.00%   [EVAL] batch:    5 | acc: 12.50%,  total acc: 60.42%   [EVAL] batch:    6 | acc: 12.50%,  total acc: 53.57%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 58.59%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 61.11%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 63.75%   [EVAL] batch:   10 | acc: 75.00%,  total acc: 64.77%   [EVAL] batch:   11 | acc: 12.50%,  total acc: 60.42%   [EVAL] batch:   12 | acc: 6.25%,  total acc: 56.25%   
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 82.81%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 83.75%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 82.29%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 83.93%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 85.94%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 88.75%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 89.77%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 90.10%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 89.90%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 88.39%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 87.50%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 85.55%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 84.93%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 83.68%   [EVAL] batch:   18 | acc: 81.25%,  total acc: 83.55%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 83.75%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 84.52%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 85.23%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 85.87%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 86.46%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 87.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 87.73%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 88.17%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 88.58%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 88.54%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 88.71%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 89.06%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 89.20%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 89.34%   [EVAL] batch:   34 | acc: 87.50%,  total acc: 89.29%   [EVAL] batch:   35 | acc: 68.75%,  total acc: 88.72%   [EVAL] batch:   36 | acc: 25.00%,  total acc: 86.99%   [EVAL] batch:   37 | acc: 25.00%,  total acc: 85.36%   [EVAL] batch:   38 | acc: 18.75%,  total acc: 83.65%   [EVAL] batch:   39 | acc: 56.25%,  total acc: 82.97%   [EVAL] batch:   40 | acc: 87.50%,  total acc: 83.08%   [EVAL] batch:   41 | acc: 81.25%,  total acc: 83.04%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 83.28%   [EVAL] batch:   43 | acc: 25.00%,  total acc: 81.96%   [EVAL] batch:   44 | acc: 12.50%,  total acc: 80.42%   
cur_acc:  ['0.8636', '0.5625']
his_acc:  ['0.8636', '0.8042']
Clustering into  7  clusters
Clusters:  [2 0 5 6 4 2 1 5 2 3 1 2 1 0 4 4]
Losses:  18.301782608032227 8.047903060913086 3.4832921028137207
CurrentTrain: epoch  0, batch     0 | loss: 18.3017826Losses:  14.44677734375 3.6138060092926025 3.4227118492126465
CurrentTrain: epoch  0, batch     1 | loss: 14.4467773Losses:  17.08086395263672 7.289828777313232 3.4220056533813477
CurrentTrain: epoch  1, batch     0 | loss: 17.0808640Losses:  11.207701683044434 2.4158592224121094 3.4025394916534424
CurrentTrain: epoch  1, batch     1 | loss: 11.2077017Losses:  17.13112449645996 7.83280086517334 3.393446445465088
CurrentTrain: epoch  2, batch     0 | loss: 17.1311245Losses:  10.831104278564453 1.9468203783035278 3.4360811710357666
CurrentTrain: epoch  2, batch     1 | loss: 10.8311043Losses:  15.93155288696289 7.120288372039795 3.4073257446289062
CurrentTrain: epoch  3, batch     0 | loss: 15.9315529Losses:  10.074599266052246 2.04854416847229 3.348522186279297
CurrentTrain: epoch  3, batch     1 | loss: 10.0745993Losses:  14.851142883300781 6.413275718688965 3.344025135040283
CurrentTrain: epoch  4, batch     0 | loss: 14.8511429Losses:  9.059535026550293 1.4620944261550903 3.462695598602295
CurrentTrain: epoch  4, batch     1 | loss: 9.0595350Losses:  13.759075164794922 6.617568016052246 3.395967960357666
CurrentTrain: epoch  5, batch     0 | loss: 13.7590752Losses:  9.710854530334473 2.1770870685577393 3.3638534545898438
CurrentTrain: epoch  5, batch     1 | loss: 9.7108545Losses:  13.846250534057617 6.738910675048828 3.3508572578430176
CurrentTrain: epoch  6, batch     0 | loss: 13.8462505Losses:  8.678874015808105 2.016319751739502 3.3581066131591797
CurrentTrain: epoch  6, batch     1 | loss: 8.6788740Losses:  12.503612518310547 6.181661605834961 3.373591899871826
CurrentTrain: epoch  7, batch     0 | loss: 12.5036125Losses:  9.989313125610352 2.5337750911712646 3.3526358604431152
CurrentTrain: epoch  7, batch     1 | loss: 9.9893131Losses:  12.859212875366211 6.298033714294434 3.351262331008911
CurrentTrain: epoch  8, batch     0 | loss: 12.8592129Losses:  8.095544815063477 1.3454742431640625 3.3486599922180176
CurrentTrain: epoch  8, batch     1 | loss: 8.0955448Losses:  14.920093536376953 8.302268981933594 3.3628597259521484
CurrentTrain: epoch  9, batch     0 | loss: 14.9200935Losses:  7.8191046714782715 3.912590503692627 1.388526439666748
CurrentTrain: epoch  9, batch     1 | loss: 7.8191047
Losses:  13.637505531311035 -0.0 11.36403751373291
MemoryTrain:  epoch  0, batch     0 | loss: 13.6375055Losses:  13.330987930297852 -0.0 11.217451095581055
MemoryTrain:  epoch  1, batch     0 | loss: 13.3309879Losses:  12.925256729125977 -0.0 11.114684104919434
MemoryTrain:  epoch  2, batch     0 | loss: 12.9252567Losses:  12.983206748962402 -0.0 11.169833183288574
MemoryTrain:  epoch  3, batch     0 | loss: 12.9832067Losses:  12.531805992126465 -0.0 11.056296348571777
MemoryTrain:  epoch  4, batch     0 | loss: 12.5318060Losses:  12.10008430480957 -0.0 11.06214714050293
MemoryTrain:  epoch  5, batch     0 | loss: 12.1000843Losses:  11.997109413146973 -0.0 11.057108879089355
MemoryTrain:  epoch  6, batch     0 | loss: 11.9971094Losses:  12.046977996826172 -0.0 10.98537540435791
MemoryTrain:  epoch  7, batch     0 | loss: 12.0469780Losses:  11.640155792236328 -0.0 10.969305038452148
MemoryTrain:  epoch  8, batch     0 | loss: 11.6401558Losses:  11.247145652770996 -0.0 10.952938079833984
MemoryTrain:  epoch  9, batch     0 | loss: 11.2471457
[EVAL] batch:    0 | acc: 6.25%,  total acc: 6.25%   [EVAL] batch:    1 | acc: 31.25%,  total acc: 18.75%   [EVAL] batch:    2 | acc: 25.00%,  total acc: 20.83%   [EVAL] batch:    3 | acc: 37.50%,  total acc: 25.00%   [EVAL] batch:    4 | acc: 37.50%,  total acc: 27.50%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 37.50%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 46.43%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 53.12%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 56.94%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 58.13%   [EVAL] batch:   10 | acc: 68.75%,  total acc: 59.09%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 60.42%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 62.02%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 64.29%   [EVAL] batch:   14 | acc: 43.75%,  total acc: 62.92%   
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 80.21%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 82.14%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 84.38%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 86.11%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 86.88%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 88.07%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 88.02%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 87.98%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 86.61%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 85.83%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 83.98%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 83.46%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 82.29%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 81.25%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 81.56%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 82.44%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 83.24%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 83.97%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 84.64%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 85.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 85.82%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 86.11%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 86.61%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 87.07%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 87.08%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 87.89%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 88.07%   [EVAL] batch:   33 | acc: 75.00%,  total acc: 87.68%   [EVAL] batch:   34 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:   35 | acc: 68.75%,  total acc: 86.98%   [EVAL] batch:   36 | acc: 18.75%,  total acc: 85.14%   [EVAL] batch:   37 | acc: 12.50%,  total acc: 83.22%   [EVAL] batch:   38 | acc: 0.00%,  total acc: 81.09%   [EVAL] batch:   39 | acc: 0.00%,  total acc: 79.06%   [EVAL] batch:   40 | acc: 0.00%,  total acc: 77.13%   [EVAL] batch:   41 | acc: 0.00%,  total acc: 75.30%   [EVAL] batch:   42 | acc: 6.25%,  total acc: 73.69%   [EVAL] batch:   43 | acc: 25.00%,  total acc: 72.59%   [EVAL] batch:   44 | acc: 25.00%,  total acc: 71.53%   [EVAL] batch:   45 | acc: 12.50%,  total acc: 70.24%   [EVAL] batch:   46 | acc: 25.00%,  total acc: 69.28%   [EVAL] batch:   47 | acc: 25.00%,  total acc: 68.36%   [EVAL] batch:   48 | acc: 43.75%,  total acc: 67.86%   [EVAL] batch:   49 | acc: 31.25%,  total acc: 67.12%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 67.65%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 68.27%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 68.87%   [EVAL] batch:   53 | acc: 87.50%,  total acc: 69.21%   [EVAL] batch:   54 | acc: 62.50%,  total acc: 69.09%   [EVAL] batch:   55 | acc: 75.00%,  total acc: 69.20%   [EVAL] batch:   56 | acc: 68.75%,  total acc: 69.19%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 69.50%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 69.92%   [EVAL] batch:   59 | acc: 37.50%,  total acc: 69.38%   
cur_acc:  ['0.8636', '0.5625', '0.6292']
his_acc:  ['0.8636', '0.8042', '0.6937']
Clustering into  9  clusters
Clusters:  [2 8 1 0 0 3 4 1 3 5 2 2 4 7 6 0 1 1 6 3 5]
Losses:  20.584495544433594 8.80548095703125 3.8577163219451904
CurrentTrain: epoch  0, batch     0 | loss: 20.5844955Losses:  16.893733978271484 4.900824546813965 1.908978819847107
CurrentTrain: epoch  0, batch     1 | loss: 16.8937340Losses:  20.33184242248535 8.77923583984375 3.7642269134521484
CurrentTrain: epoch  1, batch     0 | loss: 20.3318424Losses:  16.098899841308594 3.7163772583007812 3.806169033050537
CurrentTrain: epoch  1, batch     1 | loss: 16.0988998Losses:  19.75782585144043 7.8696722984313965 3.7257351875305176
CurrentTrain: epoch  2, batch     0 | loss: 19.7578259Losses:  13.911731719970703 1.835105538368225 3.7966227531433105
CurrentTrain: epoch  2, batch     1 | loss: 13.9117317Losses:  19.937341690063477 8.87497329711914 3.7902450561523438
CurrentTrain: epoch  3, batch     0 | loss: 19.9373417Losses:  14.040465354919434 3.748918294906616 1.6786515712738037
CurrentTrain: epoch  3, batch     1 | loss: 14.0404654Losses:  21.073394775390625 9.768407821655273 3.6421260833740234
CurrentTrain: epoch  4, batch     0 | loss: 21.0733948Losses:  16.05049705505371 4.545679092407227 3.686236619949341
CurrentTrain: epoch  4, batch     1 | loss: 16.0504971Losses:  18.573442459106445 8.063226699829102 3.697714328765869
CurrentTrain: epoch  5, batch     0 | loss: 18.5734425Losses:  13.704030990600586 3.717595100402832 1.5303609371185303
CurrentTrain: epoch  5, batch     1 | loss: 13.7040310Losses:  19.235456466674805 8.552785873413086 3.620455265045166
CurrentTrain: epoch  6, batch     0 | loss: 19.2354565Losses:  11.357585906982422 2.18966007232666 3.6030044555664062
CurrentTrain: epoch  6, batch     1 | loss: 11.3575859Losses:  17.592044830322266 7.785157203674316 3.6420812606811523
CurrentTrain: epoch  7, batch     0 | loss: 17.5920448Losses:  12.958606719970703 2.965683698654175 3.5546107292175293
CurrentTrain: epoch  7, batch     1 | loss: 12.9586067Losses:  18.412532806396484 9.306252479553223 3.5252885818481445
CurrentTrain: epoch  8, batch     0 | loss: 18.4125328Losses:  14.81604290008545 4.512180328369141 3.61482310295105
CurrentTrain: epoch  8, batch     1 | loss: 14.8160429Losses:  19.047252655029297 9.045570373535156 3.5734095573425293
CurrentTrain: epoch  9, batch     0 | loss: 19.0472527Losses:  9.342162132263184 3.4646241664886475 1.5068378448486328
CurrentTrain: epoch  9, batch     1 | loss: 9.3421621
Losses:  17.968835830688477 -0.0 17.11347770690918
MemoryTrain:  epoch  0, batch     0 | loss: 17.9688358Losses:  6.119297504425049 -0.0 5.651183128356934
MemoryTrain:  epoch  0, batch     1 | loss: 6.1192975Losses:  18.590717315673828 -0.0 16.99906349182129
MemoryTrain:  epoch  1, batch     0 | loss: 18.5907173Losses:  6.362751483917236 -0.0 5.667644023895264
MemoryTrain:  epoch  1, batch     1 | loss: 6.3627515Losses:  14.708633422851562 -0.0 13.930721282958984
MemoryTrain:  epoch  2, batch     0 | loss: 14.7086334Losses:  5.969249725341797 -0.0 5.589212894439697
MemoryTrain:  epoch  2, batch     1 | loss: 5.9692497Losses:  11.248221397399902 -0.0 10.921355247497559
MemoryTrain:  epoch  3, batch     0 | loss: 11.2482214Losses:  5.676302433013916 -0.0 5.617659091949463
MemoryTrain:  epoch  3, batch     1 | loss: 5.6763024Losses:  14.142123222351074 -0.0 13.950657844543457
MemoryTrain:  epoch  4, batch     0 | loss: 14.1421232Losses:  5.798248767852783 -0.0 5.578996181488037
MemoryTrain:  epoch  4, batch     1 | loss: 5.7982488Losses:  11.169342994689941 -0.0 10.96682071685791
MemoryTrain:  epoch  5, batch     0 | loss: 11.1693430Losses:  3.41634464263916 -0.0 3.3185501098632812
MemoryTrain:  epoch  5, batch     1 | loss: 3.4163446Losses:  13.840546607971191 -0.0 13.765815734863281
MemoryTrain:  epoch  6, batch     0 | loss: 13.8405466Losses:  3.447049856185913 -0.0 3.3969547748565674
MemoryTrain:  epoch  6, batch     1 | loss: 3.4470499Losses:  17.070402145385742 -0.0 16.84088134765625
MemoryTrain:  epoch  7, batch     0 | loss: 17.0704021Losses:  5.707470417022705 -0.0 5.597207546234131
MemoryTrain:  epoch  7, batch     1 | loss: 5.7074704Losses:  13.844654083251953 -0.0 13.756420135498047
MemoryTrain:  epoch  8, batch     0 | loss: 13.8446541Losses:  1.4431689977645874 -0.0 1.3864638805389404
MemoryTrain:  epoch  8, batch     1 | loss: 1.4431690Losses:  16.91582679748535 -0.0 16.816102981567383
MemoryTrain:  epoch  9, batch     0 | loss: 16.9158268Losses:  3.352745294570923 -0.0 3.3097052574157715
MemoryTrain:  epoch  9, batch     1 | loss: 3.3527453
[EVAL] batch:    0 | acc: 43.75%,  total acc: 43.75%   [EVAL] batch:    1 | acc: 37.50%,  total acc: 40.62%   [EVAL] batch:    2 | acc: 25.00%,  total acc: 35.42%   [EVAL] batch:    3 | acc: 0.00%,  total acc: 26.56%   [EVAL] batch:    4 | acc: 6.25%,  total acc: 22.50%   [EVAL] batch:    5 | acc: 0.00%,  total acc: 18.75%   [EVAL] batch:    6 | acc: 18.75%,  total acc: 18.75%   [EVAL] batch:    7 | acc: 68.75%,  total acc: 25.00%   [EVAL] batch:    8 | acc: 43.75%,  total acc: 27.08%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 31.25%   [EVAL] batch:   10 | acc: 75.00%,  total acc: 35.23%   [EVAL] batch:   11 | acc: 68.75%,  total acc: 38.02%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 40.38%   [EVAL] batch:   13 | acc: 100.00%,  total acc: 44.64%   [EVAL] batch:   14 | acc: 81.25%,  total acc: 47.08%   [EVAL] batch:   15 | acc: 93.75%,  total acc: 50.00%   [EVAL] batch:   16 | acc: 81.25%,  total acc: 51.84%   [EVAL] batch:   17 | acc: 81.25%,  total acc: 53.47%   [EVAL] batch:   18 | acc: 75.00%,  total acc: 54.61%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 56.56%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 58.04%   [EVAL] batch:   21 | acc: 43.75%,  total acc: 57.39%   
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 85.94%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 86.25%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 85.42%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 89.06%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 90.28%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 91.48%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 91.15%   [EVAL] batch:   12 | acc: 50.00%,  total acc: 87.98%   [EVAL] batch:   13 | acc: 25.00%,  total acc: 83.48%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 82.92%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 81.25%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 80.88%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 79.86%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 79.28%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 79.38%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 80.36%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 82.07%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 82.81%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 83.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 84.13%   [EVAL] batch:   26 | acc: 87.50%,  total acc: 84.26%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 84.82%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 85.34%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 85.21%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 85.48%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 85.94%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 86.17%   [EVAL] batch:   33 | acc: 75.00%,  total acc: 85.85%   [EVAL] batch:   34 | acc: 81.25%,  total acc: 85.71%   [EVAL] batch:   35 | acc: 75.00%,  total acc: 85.42%   [EVAL] batch:   36 | acc: 18.75%,  total acc: 83.61%   [EVAL] batch:   37 | acc: 12.50%,  total acc: 81.74%   [EVAL] batch:   38 | acc: 6.25%,  total acc: 79.81%   [EVAL] batch:   39 | acc: 0.00%,  total acc: 77.81%   [EVAL] batch:   40 | acc: 0.00%,  total acc: 75.91%   [EVAL] batch:   41 | acc: 0.00%,  total acc: 74.11%   [EVAL] batch:   42 | acc: 0.00%,  total acc: 72.38%   [EVAL] batch:   43 | acc: 56.25%,  total acc: 72.02%   [EVAL] batch:   44 | acc: 56.25%,  total acc: 71.67%   [EVAL] batch:   45 | acc: 12.50%,  total acc: 70.38%   [EVAL] batch:   46 | acc: 12.50%,  total acc: 69.15%   [EVAL] batch:   47 | acc: 6.25%,  total acc: 67.84%   [EVAL] batch:   48 | acc: 6.25%,  total acc: 66.58%   [EVAL] batch:   49 | acc: 6.25%,  total acc: 65.38%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 65.93%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 66.59%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 67.22%   [EVAL] batch:   53 | acc: 93.75%,  total acc: 67.71%   [EVAL] batch:   54 | acc: 56.25%,  total acc: 67.50%   [EVAL] batch:   55 | acc: 6.25%,  total acc: 66.41%   [EVAL] batch:   56 | acc: 0.00%,  total acc: 65.24%   [EVAL] batch:   57 | acc: 12.50%,  total acc: 64.33%   [EVAL] batch:   58 | acc: 12.50%,  total acc: 63.45%   [EVAL] batch:   59 | acc: 18.75%,  total acc: 62.71%   [EVAL] batch:   60 | acc: 43.75%,  total acc: 62.40%   [EVAL] batch:   61 | acc: 37.50%,  total acc: 62.00%   [EVAL] batch:   62 | acc: 6.25%,  total acc: 61.11%   [EVAL] batch:   63 | acc: 6.25%,  total acc: 60.25%   [EVAL] batch:   64 | acc: 0.00%,  total acc: 59.33%   [EVAL] batch:   65 | acc: 0.00%,  total acc: 58.43%   [EVAL] batch:   66 | acc: 68.75%,  total acc: 58.58%   [EVAL] batch:   67 | acc: 50.00%,  total acc: 58.46%   [EVAL] batch:   68 | acc: 50.00%,  total acc: 58.33%   [EVAL] batch:   69 | acc: 81.25%,  total acc: 58.66%   [EVAL] batch:   70 | acc: 68.75%,  total acc: 58.80%   [EVAL] batch:   71 | acc: 75.00%,  total acc: 59.03%   [EVAL] batch:   72 | acc: 81.25%,  total acc: 59.33%   [EVAL] batch:   73 | acc: 81.25%,  total acc: 59.63%   [EVAL] batch:   74 | acc: 93.75%,  total acc: 60.08%   [EVAL] batch:   75 | acc: 93.75%,  total acc: 60.53%   [EVAL] batch:   76 | acc: 75.00%,  total acc: 60.71%   [EVAL] batch:   77 | acc: 75.00%,  total acc: 60.90%   [EVAL] batch:   78 | acc: 87.50%,  total acc: 61.23%   [EVAL] batch:   79 | acc: 87.50%,  total acc: 61.56%   [EVAL] batch:   80 | acc: 81.25%,  total acc: 61.81%   
cur_acc:  ['0.8636', '0.5625', '0.6292', '0.5739']
his_acc:  ['0.8636', '0.8042', '0.6937', '0.6181']
Clustering into  12  clusters
Clusters:  [ 6  0  1  7  0  4  3  1  4  2  5  6  3 11  8  2  1  1  8  4  2  6 10  5
  2  9]
Losses:  20.835025787353516 8.931283950805664 5.788381099700928
CurrentTrain: epoch  0, batch     0 | loss: 20.8350258Losses:  13.455387115478516 2.8380773067474365 5.737027645111084
CurrentTrain: epoch  0, batch     1 | loss: 13.4553871Losses:  18.530059814453125 8.313993453979492 5.685572624206543
CurrentTrain: epoch  1, batch     0 | loss: 18.5300598Losses:  11.339658737182617 3.459346294403076 3.3598670959472656
CurrentTrain: epoch  1, batch     1 | loss: 11.3396587Losses:  18.663450241088867 8.806554794311523 5.649901866912842
CurrentTrain: epoch  2, batch     0 | loss: 18.6634502Losses:  9.868760108947754 3.412510633468628 3.302306890487671
CurrentTrain: epoch  2, batch     1 | loss: 9.8687601Losses:  16.33536148071289 7.492303371429443 5.647884368896484
CurrentTrain: epoch  3, batch     0 | loss: 16.3353615Losses:  10.449175834655762 3.0785043239593506 3.3843469619750977
CurrentTrain: epoch  3, batch     1 | loss: 10.4491758Losses:  15.759700775146484 6.674056053161621 5.6369709968566895
CurrentTrain: epoch  4, batch     0 | loss: 15.7597008Losses:  10.054914474487305 1.5709999799728394 5.6053996086120605
CurrentTrain: epoch  4, batch     1 | loss: 10.0549145Losses:  15.453516960144043 7.006498336791992 5.620255470275879
CurrentTrain: epoch  5, batch     0 | loss: 15.4535170Losses:  9.707324981689453 2.926119327545166 3.3586363792419434
CurrentTrain: epoch  5, batch     1 | loss: 9.7073250Losses:  15.002848625183105 6.70965576171875 5.64565372467041
CurrentTrain: epoch  6, batch     0 | loss: 15.0028486Losses:  11.327539443969727 2.3635270595550537 5.620828151702881
CurrentTrain: epoch  6, batch     1 | loss: 11.3275394Losses:  15.187301635742188 6.757073879241943 5.6106462478637695
CurrentTrain: epoch  7, batch     0 | loss: 15.1873016Losses:  9.957452774047852 1.989250659942627 5.599486827850342
CurrentTrain: epoch  7, batch     1 | loss: 9.9574528Losses:  14.49880599975586 6.1863017082214355 5.603288173675537
CurrentTrain: epoch  8, batch     0 | loss: 14.4988060Losses:  9.646509170532227 1.4865854978561401 5.608165264129639
CurrentTrain: epoch  8, batch     1 | loss: 9.6465092Losses:  14.79798698425293 6.615355014801025 5.608329772949219
CurrentTrain: epoch  9, batch     0 | loss: 14.7979870Losses:  9.862205505371094 1.8945966958999634 5.6067047119140625
CurrentTrain: epoch  9, batch     1 | loss: 9.8622055
Losses:  21.187271118164062 -0.0 20.24738883972168
MemoryTrain:  epoch  0, batch     0 | loss: 21.1872711Losses:  11.24433708190918 -0.0 10.827826499938965
MemoryTrain:  epoch  0, batch     1 | loss: 11.2443371Losses:  15.052760124206543 -0.0 13.877796173095703
MemoryTrain:  epoch  1, batch     0 | loss: 15.0527601Losses:  17.22117042541504 -0.0 16.83761215209961
MemoryTrain:  epoch  1, batch     1 | loss: 17.2211704Losses:  17.287464141845703 -0.0 16.83599281311035
MemoryTrain:  epoch  2, batch     0 | loss: 17.2874641Losses:  11.352673530578613 -0.0 11.021894454956055
MemoryTrain:  epoch  2, batch     1 | loss: 11.3526735Losses:  20.35014533996582 -0.0 20.04140853881836
MemoryTrain:  epoch  3, batch     0 | loss: 20.3501453Losses:  3.576535701751709 -0.0 3.326500415802002
MemoryTrain:  epoch  3, batch     1 | loss: 3.5765357Losses:  11.01257038116455 -0.0 10.874919891357422
MemoryTrain:  epoch  4, batch     0 | loss: 11.0125704Losses:  16.927932739257812 -0.0 16.865739822387695
MemoryTrain:  epoch  4, batch     1 | loss: 16.9279327Losses:  19.963651657104492 -0.0 19.9127140045166
MemoryTrain:  epoch  5, batch     0 | loss: 19.9636517Losses:  8.2611665725708 -0.0 8.147237777709961
MemoryTrain:  epoch  5, batch     1 | loss: 8.2611666Losses:  20.120868682861328 -0.0 19.947782516479492
MemoryTrain:  epoch  6, batch     0 | loss: 20.1208687Losses:  10.98590087890625 -0.0 10.861185073852539
MemoryTrain:  epoch  6, batch     1 | loss: 10.9859009Losses:  19.93260383605957 -0.0 19.847492218017578
MemoryTrain:  epoch  7, batch     0 | loss: 19.9326038Losses:  5.636683940887451 -0.0 5.60069465637207
MemoryTrain:  epoch  7, batch     1 | loss: 5.6366839Losses:  16.814693450927734 -0.0 16.785625457763672
MemoryTrain:  epoch  8, batch     0 | loss: 16.8146935Losses:  10.878649711608887 -0.0 10.8588285446167
MemoryTrain:  epoch  8, batch     1 | loss: 10.8786497Losses:  23.280946731567383 -0.0 23.254968643188477
MemoryTrain:  epoch  9, batch     0 | loss: 23.2809467Losses:  8.134465217590332 -0.0 8.091548919677734
MemoryTrain:  epoch  9, batch     1 | loss: 8.1344652
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 90.62%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:    3 | acc: 93.75%,  total acc: 92.19%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 92.50%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 91.67%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 92.86%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 92.19%   [EVAL] batch:    8 | acc: 25.00%,  total acc: 84.72%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 83.75%   [EVAL] batch:   10 | acc: 75.00%,  total acc: 82.95%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 84.38%   [EVAL] batch:   12 | acc: 100.00%,  total acc: 85.58%   [EVAL] batch:   13 | acc: 100.00%,  total acc: 86.61%   [EVAL] batch:   14 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 88.28%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 88.97%   [EVAL] batch:   17 | acc: 25.00%,  total acc: 85.42%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 62.50%,  total acc: 71.88%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 75.00%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 73.44%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 75.00%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 76.79%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 79.69%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 81.94%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 82.50%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 84.09%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:   12 | acc: 50.00%,  total acc: 81.73%   [EVAL] batch:   13 | acc: 25.00%,  total acc: 77.68%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 77.50%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 76.17%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 76.10%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 75.35%   [EVAL] batch:   18 | acc: 75.00%,  total acc: 75.33%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 75.31%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 76.49%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 77.56%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 78.53%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 79.43%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 80.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 81.01%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 81.48%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 82.14%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 82.76%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 82.92%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 83.27%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 83.79%   [EVAL] batch:   32 | acc: 43.75%,  total acc: 82.58%   [EVAL] batch:   33 | acc: 31.25%,  total acc: 81.07%   [EVAL] batch:   34 | acc: 50.00%,  total acc: 80.18%   [EVAL] batch:   35 | acc: 43.75%,  total acc: 79.17%   [EVAL] batch:   36 | acc: 18.75%,  total acc: 77.53%   [EVAL] batch:   37 | acc: 12.50%,  total acc: 75.82%   [EVAL] batch:   38 | acc: 6.25%,  total acc: 74.04%   [EVAL] batch:   39 | acc: 0.00%,  total acc: 72.19%   [EVAL] batch:   40 | acc: 0.00%,  total acc: 70.43%   [EVAL] batch:   41 | acc: 0.00%,  total acc: 68.75%   [EVAL] batch:   42 | acc: 0.00%,  total acc: 67.15%   [EVAL] batch:   43 | acc: 6.25%,  total acc: 65.77%   [EVAL] batch:   44 | acc: 12.50%,  total acc: 64.58%   [EVAL] batch:   45 | acc: 0.00%,  total acc: 63.18%   [EVAL] batch:   46 | acc: 0.00%,  total acc: 61.84%   [EVAL] batch:   47 | acc: 6.25%,  total acc: 60.68%   [EVAL] batch:   48 | acc: 0.00%,  total acc: 59.44%   [EVAL] batch:   49 | acc: 0.00%,  total acc: 58.25%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 58.95%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 59.74%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 60.50%   [EVAL] batch:   53 | acc: 93.75%,  total acc: 61.11%   [EVAL] batch:   54 | acc: 56.25%,  total acc: 61.02%   [EVAL] batch:   55 | acc: 0.00%,  total acc: 59.93%   [EVAL] batch:   56 | acc: 0.00%,  total acc: 58.88%   [EVAL] batch:   57 | acc: 0.00%,  total acc: 57.87%   [EVAL] batch:   58 | acc: 0.00%,  total acc: 56.89%   [EVAL] batch:   59 | acc: 31.25%,  total acc: 56.46%   [EVAL] batch:   60 | acc: 43.75%,  total acc: 56.25%   [EVAL] batch:   61 | acc: 37.50%,  total acc: 55.95%   [EVAL] batch:   62 | acc: 6.25%,  total acc: 55.16%   [EVAL] batch:   63 | acc: 0.00%,  total acc: 54.30%   [EVAL] batch:   64 | acc: 0.00%,  total acc: 53.46%   [EVAL] batch:   65 | acc: 0.00%,  total acc: 52.65%   [EVAL] batch:   66 | acc: 68.75%,  total acc: 52.89%   [EVAL] batch:   67 | acc: 50.00%,  total acc: 52.85%   [EVAL] batch:   68 | acc: 50.00%,  total acc: 52.81%   [EVAL] batch:   69 | acc: 93.75%,  total acc: 53.39%   [EVAL] batch:   70 | acc: 75.00%,  total acc: 53.70%   [EVAL] batch:   71 | acc: 75.00%,  total acc: 53.99%   [EVAL] batch:   72 | acc: 87.50%,  total acc: 54.45%   [EVAL] batch:   73 | acc: 93.75%,  total acc: 54.98%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 55.42%   [EVAL] batch:   75 | acc: 87.50%,  total acc: 55.84%   [EVAL] batch:   76 | acc: 75.00%,  total acc: 56.09%   [EVAL] batch:   77 | acc: 43.75%,  total acc: 55.93%   [EVAL] batch:   78 | acc: 6.25%,  total acc: 55.30%   [EVAL] batch:   79 | acc: 6.25%,  total acc: 54.69%   [EVAL] batch:   80 | acc: 25.00%,  total acc: 54.32%   [EVAL] batch:   81 | acc: 93.75%,  total acc: 54.80%   [EVAL] batch:   82 | acc: 87.50%,  total acc: 55.20%   [EVAL] batch:   83 | acc: 93.75%,  total acc: 55.65%   [EVAL] batch:   84 | acc: 93.75%,  total acc: 56.10%   [EVAL] batch:   85 | acc: 93.75%,  total acc: 56.54%   [EVAL] batch:   86 | acc: 87.50%,  total acc: 56.90%   [EVAL] batch:   87 | acc: 100.00%,  total acc: 57.39%   [EVAL] batch:   88 | acc: 81.25%,  total acc: 57.65%   [EVAL] batch:   89 | acc: 31.25%,  total acc: 57.36%   [EVAL] batch:   90 | acc: 68.75%,  total acc: 57.49%   [EVAL] batch:   91 | acc: 81.25%,  total acc: 57.74%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 58.20%   [EVAL] batch:   93 | acc: 100.00%,  total acc: 58.64%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 59.08%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 59.51%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 59.92%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 60.33%   [EVAL] batch:   98 | acc: 6.25%,  total acc: 59.79%   
cur_acc:  ['0.8636', '0.5625', '0.6292', '0.5739', '0.8542']
his_acc:  ['0.8636', '0.8042', '0.6937', '0.6181', '0.5979']
Clustering into  14  clusters
Clusters:  [ 2 13  4  7  1 10  0  6 10 12  2  2  0 11  3  1  4  4  3 10  1  2  5  2
  1  9  8  0  2  4  6]
Losses:  18.299148559570312 7.254266738891602 5.65606164932251
CurrentTrain: epoch  0, batch     0 | loss: 18.2991486Losses:  12.110431671142578 1.6086245775222778 5.618828773498535
CurrentTrain: epoch  0, batch     1 | loss: 12.1104317Losses:  16.62995719909668 7.1416215896606445 5.6645026206970215
CurrentTrain: epoch  1, batch     0 | loss: 16.6299572Losses:  12.119653701782227 2.520617961883545 5.666249752044678
CurrentTrain: epoch  1, batch     1 | loss: 12.1196537Losses:  15.683697700500488 6.438751697540283 5.621809959411621
CurrentTrain: epoch  2, batch     0 | loss: 15.6836977Losses:  10.705938339233398 1.7034591436386108 5.6203413009643555
CurrentTrain: epoch  2, batch     1 | loss: 10.7059383Losses:  17.523773193359375 8.769882202148438 5.626646518707275
CurrentTrain: epoch  3, batch     0 | loss: 17.5237732Losses:  9.158923149108887 4.687114238739014 1.402931571006775
CurrentTrain: epoch  3, batch     1 | loss: 9.1589231Losses:  15.552343368530273 6.991825103759766 5.608973979949951
CurrentTrain: epoch  4, batch     0 | loss: 15.5523434Losses:  8.246684074401855 2.373745918273926 3.388920545578003
CurrentTrain: epoch  4, batch     1 | loss: 8.2466841Losses:  14.829388618469238 6.389645576477051 5.593055725097656
CurrentTrain: epoch  5, batch     0 | loss: 14.8293886Losses:  9.496623039245605 1.806160807609558 5.5984787940979
CurrentTrain: epoch  5, batch     1 | loss: 9.4966230Losses:  14.812141418457031 6.926118850708008 5.5974955558776855
CurrentTrain: epoch  6, batch     0 | loss: 14.8121414Losses:  8.057243347167969 2.676752805709839 3.3335647583007812
CurrentTrain: epoch  6, batch     1 | loss: 8.0572433Losses:  13.85944938659668 6.044561386108398 5.575148582458496
CurrentTrain: epoch  7, batch     0 | loss: 13.8594494Losses:  9.224587440490723 1.7477272748947144 5.58176326751709
CurrentTrain: epoch  7, batch     1 | loss: 9.2245874Losses:  13.557219505310059 5.888034343719482 5.5925140380859375
CurrentTrain: epoch  8, batch     0 | loss: 13.5572195Losses:  9.417505264282227 1.7883940935134888 5.568482398986816
CurrentTrain: epoch  8, batch     1 | loss: 9.4175053Losses:  13.351168632507324 5.853769302368164 5.584894180297852
CurrentTrain: epoch  9, batch     0 | loss: 13.3511686Losses:  9.239216804504395 1.7454071044921875 5.576208591461182
CurrentTrain: epoch  9, batch     1 | loss: 9.2392168
Losses:  20.24440574645996 -0.0 19.960922241210938
MemoryTrain:  epoch  0, batch     0 | loss: 20.2444057Losses:  21.78646469116211 -0.0 20.323915481567383
MemoryTrain:  epoch  0, batch     1 | loss: 21.7864647Losses:  20.866710662841797 -0.0 19.94426918029785
MemoryTrain:  epoch  1, batch     0 | loss: 20.8667107Losses:  20.74951171875 -0.0 19.99234962463379
MemoryTrain:  epoch  1, batch     1 | loss: 20.7495117Losses:  17.40162467956543 -0.0 16.720516204833984
MemoryTrain:  epoch  2, batch     0 | loss: 17.4016247Losses:  14.365318298339844 -0.0 13.752273559570312
MemoryTrain:  epoch  2, batch     1 | loss: 14.3653183Losses:  17.097681045532227 -0.0 16.74072265625
MemoryTrain:  epoch  3, batch     0 | loss: 17.0976810Losses:  23.57178497314453 -0.0 23.20909881591797
MemoryTrain:  epoch  3, batch     1 | loss: 23.5717850Losses:  20.221088409423828 -0.0 19.952972412109375
MemoryTrain:  epoch  4, batch     0 | loss: 20.2210884Losses:  16.749244689941406 -0.0 16.724788665771484
MemoryTrain:  epoch  4, batch     1 | loss: 16.7492447Losses:  23.20732879638672 -0.0 23.16968536376953
MemoryTrain:  epoch  5, batch     0 | loss: 23.2073288Losses:  13.925310134887695 -0.0 13.75422477722168
MemoryTrain:  epoch  5, batch     1 | loss: 13.9253101Losses:  19.994783401489258 -0.0 19.90570831298828
MemoryTrain:  epoch  6, batch     0 | loss: 19.9947834Losses:  13.748218536376953 -0.0 13.705665588378906
MemoryTrain:  epoch  6, batch     1 | loss: 13.7482185Losses:  16.722442626953125 -0.0 16.696231842041016
MemoryTrain:  epoch  7, batch     0 | loss: 16.7224426Losses:  19.944658279418945 -0.0 19.89375877380371
MemoryTrain:  epoch  7, batch     1 | loss: 19.9446583Losses:  23.176267623901367 -0.0 23.152366638183594
MemoryTrain:  epoch  8, batch     0 | loss: 23.1762676Losses:  16.724042892456055 -0.0 16.70657730102539
MemoryTrain:  epoch  8, batch     1 | loss: 16.7240429Losses:  19.88835906982422 -0.0 19.87038230895996
MemoryTrain:  epoch  9, batch     0 | loss: 19.8883591Losses:  19.854063034057617 -0.0 19.84138298034668
MemoryTrain:  epoch  9, batch     1 | loss: 19.8540630
[EVAL] batch:    0 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    1 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    2 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    3 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    4 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    8 | acc: 75.00%,  total acc: 97.22%   [EVAL] batch:    9 | acc: 25.00%,  total acc: 90.00%   [EVAL] batch:   10 | acc: 56.25%,  total acc: 86.93%   [EVAL] batch:   11 | acc: 56.25%,  total acc: 84.38%   [EVAL] batch:   12 | acc: 50.00%,  total acc: 81.73%   [EVAL] batch:   13 | acc: 37.50%,  total acc: 78.57%   
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 82.50%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 83.04%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 85.16%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 86.81%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 86.88%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 88.07%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 88.02%   [EVAL] batch:   12 | acc: 50.00%,  total acc: 85.10%   [EVAL] batch:   13 | acc: 25.00%,  total acc: 80.80%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 80.00%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 78.52%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 78.31%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 77.43%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 76.97%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 76.88%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 77.98%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 78.98%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 79.89%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 80.47%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 81.97%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 82.41%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 83.04%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 83.62%   [EVAL] batch:   29 | acc: 93.75%,  total acc: 83.96%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 84.27%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 84.77%   [EVAL] batch:   32 | acc: 43.75%,  total acc: 83.52%   [EVAL] batch:   33 | acc: 31.25%,  total acc: 81.99%   [EVAL] batch:   34 | acc: 50.00%,  total acc: 81.07%   [EVAL] batch:   35 | acc: 43.75%,  total acc: 80.03%   [EVAL] batch:   36 | acc: 18.75%,  total acc: 78.38%   [EVAL] batch:   37 | acc: 18.75%,  total acc: 76.81%   [EVAL] batch:   38 | acc: 12.50%,  total acc: 75.16%   [EVAL] batch:   39 | acc: 0.00%,  total acc: 73.28%   [EVAL] batch:   40 | acc: 0.00%,  total acc: 71.49%   [EVAL] batch:   41 | acc: 0.00%,  total acc: 69.79%   [EVAL] batch:   42 | acc: 0.00%,  total acc: 68.17%   [EVAL] batch:   43 | acc: 6.25%,  total acc: 66.76%   [EVAL] batch:   44 | acc: 12.50%,  total acc: 65.56%   [EVAL] batch:   45 | acc: 0.00%,  total acc: 64.13%   [EVAL] batch:   46 | acc: 0.00%,  total acc: 62.77%   [EVAL] batch:   47 | acc: 0.00%,  total acc: 61.46%   [EVAL] batch:   48 | acc: 0.00%,  total acc: 60.20%   [EVAL] batch:   49 | acc: 0.00%,  total acc: 59.00%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 59.68%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 60.46%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 61.20%   [EVAL] batch:   53 | acc: 93.75%,  total acc: 61.81%   [EVAL] batch:   54 | acc: 56.25%,  total acc: 61.70%   [EVAL] batch:   55 | acc: 0.00%,  total acc: 60.60%   [EVAL] batch:   56 | acc: 0.00%,  total acc: 59.54%   [EVAL] batch:   57 | acc: 0.00%,  total acc: 58.51%   [EVAL] batch:   58 | acc: 0.00%,  total acc: 57.52%   [EVAL] batch:   59 | acc: 0.00%,  total acc: 56.56%   [EVAL] batch:   60 | acc: 31.25%,  total acc: 56.15%   [EVAL] batch:   61 | acc: 31.25%,  total acc: 55.75%   [EVAL] batch:   62 | acc: 0.00%,  total acc: 54.86%   [EVAL] batch:   63 | acc: 0.00%,  total acc: 54.00%   [EVAL] batch:   64 | acc: 0.00%,  total acc: 53.17%   [EVAL] batch:   65 | acc: 0.00%,  total acc: 52.37%   [EVAL] batch:   66 | acc: 68.75%,  total acc: 52.61%   [EVAL] batch:   67 | acc: 68.75%,  total acc: 52.85%   [EVAL] batch:   68 | acc: 50.00%,  total acc: 52.81%   [EVAL] batch:   69 | acc: 68.75%,  total acc: 53.04%   [EVAL] batch:   70 | acc: 75.00%,  total acc: 53.35%   [EVAL] batch:   71 | acc: 56.25%,  total acc: 53.39%   [EVAL] batch:   72 | acc: 81.25%,  total acc: 53.77%   [EVAL] batch:   73 | acc: 75.00%,  total acc: 54.05%   [EVAL] batch:   74 | acc: 81.25%,  total acc: 54.42%   [EVAL] batch:   75 | acc: 87.50%,  total acc: 54.85%   [EVAL] batch:   76 | acc: 68.75%,  total acc: 55.03%   [EVAL] batch:   77 | acc: 37.50%,  total acc: 54.81%   [EVAL] batch:   78 | acc: 6.25%,  total acc: 54.19%   [EVAL] batch:   79 | acc: 6.25%,  total acc: 53.59%   [EVAL] batch:   80 | acc: 18.75%,  total acc: 53.16%   [EVAL] batch:   81 | acc: 87.50%,  total acc: 53.58%   [EVAL] batch:   82 | acc: 56.25%,  total acc: 53.61%   [EVAL] batch:   83 | acc: 37.50%,  total acc: 53.42%   [EVAL] batch:   84 | acc: 43.75%,  total acc: 53.31%   [EVAL] batch:   85 | acc: 81.25%,  total acc: 53.63%   [EVAL] batch:   86 | acc: 56.25%,  total acc: 53.66%   [EVAL] batch:   87 | acc: 93.75%,  total acc: 54.12%   [EVAL] batch:   88 | acc: 81.25%,  total acc: 54.42%   [EVAL] batch:   89 | acc: 25.00%,  total acc: 54.10%   [EVAL] batch:   90 | acc: 81.25%,  total acc: 54.40%   [EVAL] batch:   91 | acc: 87.50%,  total acc: 54.76%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 55.24%   [EVAL] batch:   93 | acc: 100.00%,  total acc: 55.72%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 56.18%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 56.64%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 57.09%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 57.53%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 57.95%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 58.38%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 58.79%   [EVAL] batch:  101 | acc: 100.00%,  total acc: 59.19%   [EVAL] batch:  102 | acc: 100.00%,  total acc: 59.59%   [EVAL] batch:  103 | acc: 100.00%,  total acc: 59.98%   [EVAL] batch:  104 | acc: 100.00%,  total acc: 60.36%   [EVAL] batch:  105 | acc: 100.00%,  total acc: 60.73%   [EVAL] batch:  106 | acc: 75.00%,  total acc: 60.86%   [EVAL] batch:  107 | acc: 31.25%,  total acc: 60.59%   [EVAL] batch:  108 | acc: 50.00%,  total acc: 60.49%   [EVAL] batch:  109 | acc: 62.50%,  total acc: 60.51%   [EVAL] batch:  110 | acc: 43.75%,  total acc: 60.36%   [EVAL] batch:  111 | acc: 43.75%,  total acc: 60.21%   
cur_acc:  ['0.8636', '0.5625', '0.6292', '0.5739', '0.8542', '0.7857']
his_acc:  ['0.8636', '0.8042', '0.6937', '0.6181', '0.5979', '0.6021']
Clustering into  17  clusters
Clusters:  [ 2 14  0 15 16 10  1  4 10 13  7  2  1 12  3  5  0  0  3 10  5  2 11  7
  5  8  9  6  2  0  4  6  2  5 14  7]
Losses:  20.0581111907959 8.288263320922852 5.7714667320251465
CurrentTrain: epoch  0, batch     0 | loss: 20.0581112Losses:  13.722614288330078 3.126354217529297 3.461364269256592
CurrentTrain: epoch  0, batch     1 | loss: 13.7226143Losses:  19.060762405395508 7.765227794647217 5.705564022064209
CurrentTrain: epoch  1, batch     0 | loss: 19.0607624Losses:  13.505313873291016 2.4997761249542236 5.824273586273193
CurrentTrain: epoch  1, batch     1 | loss: 13.5053139Losses:  19.687732696533203 8.040372848510742 5.735244274139404
CurrentTrain: epoch  2, batch     0 | loss: 19.6877327Losses:  12.177688598632812 2.25346040725708 5.621660232543945
CurrentTrain: epoch  2, batch     1 | loss: 12.1776886Losses:  17.407814025878906 7.17031717300415 5.677958965301514
CurrentTrain: epoch  3, batch     0 | loss: 17.4078140Losses:  13.052644729614258 2.616107940673828 5.615397930145264
CurrentTrain: epoch  3, batch     1 | loss: 13.0526447Losses:  17.03433609008789 7.384788513183594 5.6965789794921875
CurrentTrain: epoch  4, batch     0 | loss: 17.0343361Losses:  11.391379356384277 2.1708457469940186 5.567439556121826
CurrentTrain: epoch  4, batch     1 | loss: 11.3913794Losses:  17.567676544189453 8.082633972167969 5.625807762145996
CurrentTrain: epoch  5, batch     0 | loss: 17.5676765Losses:  9.551518440246582 3.127340078353882 3.3033549785614014
CurrentTrain: epoch  5, batch     1 | loss: 9.5515184Losses:  17.07601547241211 8.182403564453125 5.580478191375732
CurrentTrain: epoch  6, batch     0 | loss: 17.0760155