#############params############
cuda:0
Task=Tacred, 5-shot
Encoding model: bert
pattern=hybridprompt
mem=1, margin=0.3, gen=1, gen_num=2
#############params############
--------Round  0
seed:  100
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/test.pkl
Task_order: [7 3 0 5 4 1 6 2]
prepared data!
CurrentTrain: epoch  0, batch     0 | loss: 13.3724613CurrentTrain: epoch  0, batch     1 | loss: 13.1578608CurrentTrain: epoch  0, batch     2 | loss: 13.0914288CurrentTrain: epoch  0, batch     3 | loss: 12.9665594CurrentTrain: epoch  0, batch     4 | loss: 12.8442793CurrentTrain: epoch  0, batch     5 | loss: 12.5822926CurrentTrain: epoch  0, batch     6 | loss: 12.5752811CurrentTrain: epoch  0, batch     7 | loss: 12.3465538CurrentTrain: epoch  0, batch     8 | loss: 12.2965069CurrentTrain: epoch  0, batch     9 | loss: 12.1475677CurrentTrain: epoch  0, batch    10 | loss: 12.0400219CurrentTrain: epoch  0, batch    11 | loss: 11.9480858CurrentTrain: epoch  0, batch    12 | loss: 12.0638752CurrentTrain: epoch  0, batch    13 | loss: 11.8167038CurrentTrain: epoch  0, batch    14 | loss: 11.6889286CurrentTrain: epoch  0, batch    15 | loss: 11.6254883CurrentTrain: epoch  0, batch    16 | loss: 11.0940533CurrentTrain: epoch  0, batch    17 | loss: 11.2020321CurrentTrain: epoch  0, batch    18 | loss: 11.2489090CurrentTrain: epoch  0, batch    19 | loss: 11.4948883CurrentTrain: epoch  0, batch    20 | loss: 11.0849552CurrentTrain: epoch  0, batch    21 | loss: 11.3767891CurrentTrain: epoch  0, batch    22 | loss: 11.4738388CurrentTrain: epoch  0, batch    23 | loss: 11.0327606CurrentTrain: epoch  0, batch    24 | loss: 11.3491087CurrentTrain: epoch  0, batch    25 | loss: 11.3099518CurrentTrain: epoch  0, batch    26 | loss: 10.8578224CurrentTrain: epoch  0, batch    27 | loss: 10.2841034CurrentTrain: epoch  0, batch    28 | loss: 10.7051201CurrentTrain: epoch  0, batch    29 | loss: 10.7940836CurrentTrain: epoch  0, batch    30 | loss: 10.4195404CurrentTrain: epoch  0, batch    31 | loss: 10.7609634CurrentTrain: epoch  0, batch    32 | loss: 10.3042355CurrentTrain: epoch  0, batch    33 | loss: 10.3129387CurrentTrain: epoch  0, batch    34 | loss: 10.0250797CurrentTrain: epoch  0, batch    35 | loss: 10.2144766CurrentTrain: epoch  0, batch    36 | loss: 10.2306309CurrentTrain: epoch  0, batch    37 | loss: 10.0852337CurrentTrain: epoch  1, batch     0 | loss: 10.2014656CurrentTrain: epoch  1, batch     1 | loss: 10.6530342CurrentTrain: epoch  1, batch     2 | loss: 9.7890358CurrentTrain: epoch  1, batch     3 | loss: 9.7257767CurrentTrain: epoch  1, batch     4 | loss: 9.4564104CurrentTrain: epoch  1, batch     5 | loss: 10.1626291CurrentTrain: epoch  1, batch     6 | loss: 9.8237648CurrentTrain: epoch  1, batch     7 | loss: 9.4988441CurrentTrain: epoch  1, batch     8 | loss: 9.4991245CurrentTrain: epoch  1, batch     9 | loss: 9.6452885CurrentTrain: epoch  1, batch    10 | loss: 9.9743004CurrentTrain: epoch  1, batch    11 | loss: 9.7963219CurrentTrain: epoch  1, batch    12 | loss: 9.6937313CurrentTrain: epoch  1, batch    13 | loss: 9.1966829CurrentTrain: epoch  1, batch    14 | loss: 9.5502548CurrentTrain: epoch  1, batch    15 | loss: 9.2058496CurrentTrain: epoch  1, batch    16 | loss: 9.1888971CurrentTrain: epoch  1, batch    17 | loss: 9.5938950CurrentTrain: epoch  1, batch    18 | loss: 9.4860392CurrentTrain: epoch  1, batch    19 | loss: 9.5833225CurrentTrain: epoch  1, batch    20 | loss: 9.5087862CurrentTrain: epoch  1, batch    21 | loss: 9.1296339CurrentTrain: epoch  1, batch    22 | loss: 9.2063999CurrentTrain: epoch  1, batch    23 | loss: 9.0002327CurrentTrain: epoch  1, batch    24 | loss: 9.2855091CurrentTrain: epoch  1, batch    25 | loss: 8.4403152CurrentTrain: epoch  1, batch    26 | loss: 9.3554955CurrentTrain: epoch  1, batch    27 | loss: 8.4032183CurrentTrain: epoch  1, batch    28 | loss: 8.6823807CurrentTrain: epoch  1, batch    29 | loss: 8.0479479CurrentTrain: epoch  1, batch    30 | loss: 8.6466703CurrentTrain: epoch  1, batch    31 | loss: 9.0862045CurrentTrain: epoch  1, batch    32 | loss: 8.8169594CurrentTrain: epoch  1, batch    33 | loss: 8.2732611CurrentTrain: epoch  1, batch    34 | loss: 8.6010847CurrentTrain: epoch  1, batch    35 | loss: 8.1362267CurrentTrain: epoch  1, batch    36 | loss: 8.8653030CurrentTrain: epoch  1, batch    37 | loss: 8.5752344CurrentTrain: epoch  2, batch     0 | loss: 7.7833271CurrentTrain: epoch  2, batch     1 | loss: 8.4801159CurrentTrain: epoch  2, batch     2 | loss: 8.7112026CurrentTrain: epoch  2, batch     3 | loss: 8.3481588CurrentTrain: epoch  2, batch     4 | loss: 9.3701601CurrentTrain: epoch  2, batch     5 | loss: 9.5443459CurrentTrain: epoch  2, batch     6 | loss: 8.6426182CurrentTrain: epoch  2, batch     7 | loss: 8.3060379CurrentTrain: epoch  2, batch     8 | loss: 8.6485052CurrentTrain: epoch  2, batch     9 | loss: 8.4162703CurrentTrain: epoch  2, batch    10 | loss: 8.0237541CurrentTrain: epoch  2, batch    11 | loss: 8.5527153CurrentTrain: epoch  2, batch    12 | loss: 8.2105560CurrentTrain: epoch  2, batch    13 | loss: 7.9770279CurrentTrain: epoch  2, batch    14 | loss: 7.3642979CurrentTrain: epoch  2, batch    15 | loss: 7.9607420CurrentTrain: epoch  2, batch    16 | loss: 8.3288574CurrentTrain: epoch  2, batch    17 | loss: 8.4686508CurrentTrain: epoch  2, batch    18 | loss: 8.0185461CurrentTrain: epoch  2, batch    19 | loss: 7.8661394CurrentTrain: epoch  2, batch    20 | loss: 7.6271992CurrentTrain: epoch  2, batch    21 | loss: 7.9648695CurrentTrain: epoch  2, batch    22 | loss: 7.4678106CurrentTrain: epoch  2, batch    23 | loss: 7.4899635CurrentTrain: epoch  2, batch    24 | loss: 7.8132114CurrentTrain: epoch  2, batch    25 | loss: 8.1924648CurrentTrain: epoch  2, batch    26 | loss: 7.2895327CurrentTrain: epoch  2, batch    27 | loss: 8.5118685CurrentTrain: epoch  2, batch    28 | loss: 6.8255653CurrentTrain: epoch  2, batch    29 | loss: 7.8991265CurrentTrain: epoch  2, batch    30 | loss: 7.5748453CurrentTrain: epoch  2, batch    31 | loss: 7.1347771CurrentTrain: epoch  2, batch    32 | loss: 7.6812344CurrentTrain: epoch  2, batch    33 | loss: 7.5430918CurrentTrain: epoch  2, batch    34 | loss: 8.4325619CurrentTrain: epoch  2, batch    35 | loss: 7.4143143CurrentTrain: epoch  2, batch    36 | loss: 7.8018699CurrentTrain: epoch  2, batch    37 | loss: 7.7435017CurrentTrain: epoch  3, batch     0 | loss: 7.5232515CurrentTrain: epoch  3, batch     1 | loss: 7.8361187CurrentTrain: epoch  3, batch     2 | loss: 7.8839965CurrentTrain: epoch  3, batch     3 | loss: 7.9287844CurrentTrain: epoch  3, batch     4 | loss: 7.4654551CurrentTrain: epoch  3, batch     5 | loss: 7.8325772CurrentTrain: epoch  3, batch     6 | loss: 8.4771061CurrentTrain: epoch  3, batch     7 | loss: 6.9571447CurrentTrain: epoch  3, batch     8 | loss: 8.0827675CurrentTrain: epoch  3, batch     9 | loss: 7.7508407CurrentTrain: epoch  3, batch    10 | loss: 7.0086536CurrentTrain: epoch  3, batch    11 | loss: 6.6070518CurrentTrain: epoch  3, batch    12 | loss: 7.7624445CurrentTrain: epoch  3, batch    13 | loss: 8.1287346CurrentTrain: epoch  3, batch    14 | loss: 7.2786741CurrentTrain: epoch  3, batch    15 | loss: 7.4686642CurrentTrain: epoch  3, batch    16 | loss: 8.3591795CurrentTrain: epoch  3, batch    17 | loss: 7.4006367CurrentTrain: epoch  3, batch    18 | loss: 7.4785576CurrentTrain: epoch  3, batch    19 | loss: 7.9789510CurrentTrain: epoch  3, batch    20 | loss: 7.5735807CurrentTrain: epoch  3, batch    21 | loss: 7.4664931CurrentTrain: epoch  3, batch    22 | loss: 7.6469474CurrentTrain: epoch  3, batch    23 | loss: 7.7438240CurrentTrain: epoch  3, batch    24 | loss: 6.4601979CurrentTrain: epoch  3, batch    25 | loss: 6.9148345CurrentTrain: epoch  3, batch    26 | loss: 6.8151793CurrentTrain: epoch  3, batch    27 | loss: 7.9257421CurrentTrain: epoch  3, batch    28 | loss: 7.3877330CurrentTrain: epoch  3, batch    29 | loss: 6.2554440CurrentTrain: epoch  3, batch    30 | loss: 7.2944155CurrentTrain: epoch  3, batch    31 | loss: 6.9774604CurrentTrain: epoch  3, batch    32 | loss: 6.2400208CurrentTrain: epoch  3, batch    33 | loss: 6.4645548CurrentTrain: epoch  3, batch    34 | loss: 6.7722120CurrentTrain: epoch  3, batch    35 | loss: 6.4985266CurrentTrain: epoch  3, batch    36 | loss: 6.8035321CurrentTrain: epoch  3, batch    37 | loss: 6.8702993CurrentTrain: epoch  4, batch     0 | loss: 7.0334969CurrentTrain: epoch  4, batch     1 | loss: 7.0049124CurrentTrain: epoch  4, batch     2 | loss: 5.8247557CurrentTrain: epoch  4, batch     3 | loss: 6.8830619CurrentTrain: epoch  4, batch     4 | loss: 6.9699078CurrentTrain: epoch  4, batch     5 | loss: 6.9527235CurrentTrain: epoch  4, batch     6 | loss: 6.2295132CurrentTrain: epoch  4, batch     7 | loss: 6.9355526CurrentTrain: epoch  4, batch     8 | loss: 7.7385564CurrentTrain: epoch  4, batch     9 | loss: 7.0533419CurrentTrain: epoch  4, batch    10 | loss: 7.2315435CurrentTrain: epoch  4, batch    11 | loss: 6.2178564CurrentTrain: epoch  4, batch    12 | loss: 6.9987230CurrentTrain: epoch  4, batch    13 | loss: 6.5305729CurrentTrain: epoch  4, batch    14 | loss: 6.8219061CurrentTrain: epoch  4, batch    15 | loss: 7.0248032CurrentTrain: epoch  4, batch    16 | loss: 6.7223358CurrentTrain: epoch  4, batch    17 | loss: 6.6297302CurrentTrain: epoch  4, batch    18 | loss: 6.2468553CurrentTrain: epoch  4, batch    19 | loss: 6.3774834CurrentTrain: epoch  4, batch    20 | loss: 6.7463589CurrentTrain: epoch  4, batch    21 | loss: 7.4178104CurrentTrain: epoch  4, batch    22 | loss: 6.9252653CurrentTrain: epoch  4, batch    23 | loss: 6.1513681CurrentTrain: epoch  4, batch    24 | loss: 7.1817818CurrentTrain: epoch  4, batch    25 | loss: 7.3936548CurrentTrain: epoch  4, batch    26 | loss: 6.3802161CurrentTrain: epoch  4, batch    27 | loss: 8.6171103CurrentTrain: epoch  4, batch    28 | loss: 6.6908755CurrentTrain: epoch  4, batch    29 | loss: 6.6532860CurrentTrain: epoch  4, batch    30 | loss: 6.7164712CurrentTrain: epoch  4, batch    31 | loss: 6.5105209CurrentTrain: epoch  4, batch    32 | loss: 7.6631536CurrentTrain: epoch  4, batch    33 | loss: 6.2924786CurrentTrain: epoch  4, batch    34 | loss: 7.8369579CurrentTrain: epoch  4, batch    35 | loss: 7.2543831CurrentTrain: epoch  4, batch    36 | loss: 6.5398402CurrentTrain: epoch  4, batch    37 | loss: 7.5059304CurrentTrain: epoch  5, batch     0 | loss: 6.1030679CurrentTrain: epoch  5, batch     1 | loss: 6.8386374CurrentTrain: epoch  5, batch     2 | loss: 7.0808506CurrentTrain: epoch  5, batch     3 | loss: 7.1512556CurrentTrain: epoch  5, batch     4 | loss: 6.9177895CurrentTrain: epoch  5, batch     5 | loss: 6.7688484CurrentTrain: epoch  5, batch     6 | loss: 6.7772846CurrentTrain: epoch  5, batch     7 | loss: 6.8151073CurrentTrain: epoch  5, batch     8 | loss: 6.7718267CurrentTrain: epoch  5, batch     9 | loss: 6.6284990CurrentTrain: epoch  5, batch    10 | loss: 6.2529025CurrentTrain: epoch  5, batch    11 | loss: 6.5486612CurrentTrain: epoch  5, batch    12 | loss: 6.1802988CurrentTrain: epoch  5, batch    13 | loss: 6.8904743CurrentTrain: epoch  5, batch    14 | loss: 6.4440236CurrentTrain: epoch  5, batch    15 | loss: 6.5432043CurrentTrain: epoch  5, batch    16 | loss: 5.7127781CurrentTrain: epoch  5, batch    17 | loss: 5.8208084CurrentTrain: epoch  5, batch    18 | loss: 7.2228193CurrentTrain: epoch  5, batch    19 | loss: 6.8538017CurrentTrain: epoch  5, batch    20 | loss: 6.0343790CurrentTrain: epoch  5, batch    21 | loss: 6.0485401CurrentTrain: epoch  5, batch    22 | loss: 6.1135545CurrentTrain: epoch  5, batch    23 | loss: 5.8876462CurrentTrain: epoch  5, batch    24 | loss: 6.7454958CurrentTrain: epoch  5, batch    25 | loss: 5.8460865CurrentTrain: epoch  5, batch    26 | loss: 5.9473586CurrentTrain: epoch  5, batch    27 | loss: 6.7466745CurrentTrain: epoch  5, batch    28 | loss: 8.3776693CurrentTrain: epoch  5, batch    29 | loss: 6.3754187CurrentTrain: epoch  5, batch    30 | loss: 6.4071417CurrentTrain: epoch  5, batch    31 | loss: 6.0694742CurrentTrain: epoch  5, batch    32 | loss: 5.5056829CurrentTrain: epoch  5, batch    33 | loss: 6.3093948CurrentTrain: epoch  5, batch    34 | loss: 6.6475391CurrentTrain: epoch  5, batch    35 | loss: 5.9693222CurrentTrain: epoch  5, batch    36 | loss: 6.5377998CurrentTrain: epoch  5, batch    37 | loss: 6.0111084CurrentTrain: epoch  6, batch     0 | loss: 6.0047464CurrentTrain: epoch  6, batch     1 | loss: 6.6310277CurrentTrain: epoch  6, batch     2 | loss: 6.6020212CurrentTrain: epoch  6, batch     3 | loss: 6.2475824CurrentTrain: epoch  6, batch     4 | loss: 6.0720997CurrentTrain: epoch  6, batch     5 | loss: 6.0006638CurrentTrain: epoch  6, batch     6 | loss: 6.3973942CurrentTrain: epoch  6, batch     7 | loss: 5.9961662CurrentTrain: epoch  6, batch     8 | loss: 5.7649665CurrentTrain: epoch  6, batch     9 | loss: 6.0096731CurrentTrain: epoch  6, batch    10 | loss: 5.9515557CurrentTrain: epoch  6, batch    11 | loss: 5.9529152CurrentTrain: epoch  6, batch    12 | loss: 5.9199295CurrentTrain: epoch  6, batch    13 | loss: 5.6818867CurrentTrain: epoch  6, batch    14 | loss: 5.8865380CurrentTrain: epoch  6, batch    15 | loss: 5.3489008CurrentTrain: epoch  6, batch    16 | loss: 6.4938850CurrentTrain: epoch  6, batch    17 | loss: 5.9322629CurrentTrain: epoch  6, batch    18 | loss: 6.0616775CurrentTrain: epoch  6, batch    19 | loss: 5.8435102CurrentTrain: epoch  6, batch    20 | loss: 6.5467153CurrentTrain: epoch  6, batch    21 | loss: 6.8287182CurrentTrain: epoch  6, batch    22 | loss: 5.8638916CurrentTrain: epoch  6, batch    23 | loss: 5.8143373CurrentTrain: epoch  6, batch    24 | loss: 5.5679169CurrentTrain: epoch  6, batch    25 | loss: 6.4695663CurrentTrain: epoch  6, batch    26 | loss: 6.4372053CurrentTrain: epoch  6, batch    27 | loss: 5.6673498CurrentTrain: epoch  6, batch    28 | loss: 5.9043579CurrentTrain: epoch  6, batch    29 | loss: 6.1868839CurrentTrain: epoch  6, batch    30 | loss: 6.6498880CurrentTrain: epoch  6, batch    31 | loss: 6.2947693CurrentTrain: epoch  6, batch    32 | loss: 5.6921754CurrentTrain: epoch  6, batch    33 | loss: 6.2850089CurrentTrain: epoch  6, batch    34 | loss: 5.9845924CurrentTrain: epoch  6, batch    35 | loss: 6.4355583CurrentTrain: epoch  6, batch    36 | loss: 6.2763124CurrentTrain: epoch  6, batch    37 | loss: 5.9351416CurrentTrain: epoch  7, batch     0 | loss: 6.6529021CurrentTrain: epoch  7, batch     1 | loss: 5.8108978CurrentTrain: epoch  7, batch     2 | loss: 5.5417938CurrentTrain: epoch  7, batch     3 | loss: 5.5101213CurrentTrain: epoch  7, batch     4 | loss: 6.3973951CurrentTrain: epoch  7, batch     5 | loss: 5.6384888CurrentTrain: epoch  7, batch     6 | loss: 5.6699238CurrentTrain: epoch  7, batch     7 | loss: 5.6597505CurrentTrain: epoch  7, batch     8 | loss: 5.6804357CurrentTrain: epoch  7, batch     9 | loss: 5.5073752CurrentTrain: epoch  7, batch    10 | loss: 5.9107246CurrentTrain: epoch  7, batch    11 | loss: 5.8943958CurrentTrain: epoch  7, batch    12 | loss: 5.7086897CurrentTrain: epoch  7, batch    13 | loss: 5.8933516CurrentTrain: epoch  7, batch    14 | loss: 5.8622904CurrentTrain: epoch  7, batch    15 | loss: 6.0675993CurrentTrain: epoch  7, batch    16 | loss: 5.7433643CurrentTrain: epoch  7, batch    17 | loss: 5.4421778CurrentTrain: epoch  7, batch    18 | loss: 5.6944370CurrentTrain: epoch  7, batch    19 | loss: 5.5842218CurrentTrain: epoch  7, batch    20 | loss: 5.5729561CurrentTrain: epoch  7, batch    21 | loss: 5.3309689CurrentTrain: epoch  7, batch    22 | loss: 6.9911366CurrentTrain: epoch  7, batch    23 | loss: 6.2403383CurrentTrain: epoch  7, batch    24 | loss: 6.1066108CurrentTrain: epoch  7, batch    25 | loss: 5.3430290CurrentTrain: epoch  7, batch    26 | loss: 5.5711193CurrentTrain: epoch  7, batch    27 | loss: 5.4540119CurrentTrain: epoch  7, batch    28 | loss: 5.4332466CurrentTrain: epoch  7, batch    29 | loss: 5.3174152CurrentTrain: epoch  7, batch    30 | loss: 5.4019990CurrentTrain: epoch  7, batch    31 | loss: 5.3367624CurrentTrain: epoch  7, batch    32 | loss: 5.6065183CurrentTrain: epoch  7, batch    33 | loss: 5.8639040CurrentTrain: epoch  7, batch    34 | loss: 5.5942111CurrentTrain: epoch  7, batch    35 | loss: 5.7962923CurrentTrain: epoch  7, batch    36 | loss: 5.5112915CurrentTrain: epoch  7, batch    37 | loss: 4.9952893CurrentTrain: epoch  8, batch     0 | loss: 5.4854412CurrentTrain: epoch  8, batch     1 | loss: 5.6333909CurrentTrain: epoch  8, batch     2 | loss: 5.1421604CurrentTrain: epoch  8, batch     3 | loss: 5.3651385CurrentTrain: epoch  8, batch     4 | loss: 5.3226509CurrentTrain: epoch  8, batch     5 | loss: 5.3203096CurrentTrain: epoch  8, batch     6 | loss: 5.3517303CurrentTrain: epoch  8, batch     7 | loss: 5.4473085CurrentTrain: epoch  8, batch     8 | loss: 5.3135548CurrentTrain: epoch  8, batch     9 | loss: 5.2105174CurrentTrain: epoch  8, batch    10 | loss: 5.6048307CurrentTrain: epoch  8, batch    11 | loss: 5.3615904CurrentTrain: epoch  8, batch    12 | loss: 5.7865286CurrentTrain: epoch  8, batch    13 | loss: 5.3026810CurrentTrain: epoch  8, batch    14 | loss: 5.6099992CurrentTrain: epoch  8, batch    15 | loss: 5.3994226CurrentTrain: epoch  8, batch    16 | loss: 5.7551336CurrentTrain: epoch  8, batch    17 | loss: 5.0666037CurrentTrain: epoch  8, batch    18 | loss: 5.2056007CurrentTrain: epoch  8, batch    19 | loss: 5.7251682CurrentTrain: epoch  8, batch    20 | loss: 5.5557652CurrentTrain: epoch  8, batch    21 | loss: 5.4007759CurrentTrain: epoch  8, batch    22 | loss: 5.9072752CurrentTrain: epoch  8, batch    23 | loss: 5.6366529CurrentTrain: epoch  8, batch    24 | loss: 5.2677555CurrentTrain: epoch  8, batch    25 | loss: 5.3671660CurrentTrain: epoch  8, batch    26 | loss: 6.1590004CurrentTrain: epoch  8, batch    27 | loss: 5.1991730CurrentTrain: epoch  8, batch    28 | loss: 5.2172470CurrentTrain: epoch  8, batch    29 | loss: 5.6280775CurrentTrain: epoch  8, batch    30 | loss: 5.2850456CurrentTrain: epoch  8, batch    31 | loss: 5.3826280CurrentTrain: epoch  8, batch    32 | loss: 4.9245048CurrentTrain: epoch  8, batch    33 | loss: 5.1530337CurrentTrain: epoch  8, batch    34 | loss: 5.0754180CurrentTrain: epoch  8, batch    35 | loss: 5.2098951CurrentTrain: epoch  8, batch    36 | loss: 5.0569038CurrentTrain: epoch  8, batch    37 | loss: 5.1459870CurrentTrain: epoch  9, batch     0 | loss: 5.3714628CurrentTrain: epoch  9, batch     1 | loss: 5.2017689CurrentTrain: epoch  9, batch     2 | loss: 5.2213621CurrentTrain: epoch  9, batch     3 | loss: 5.0896473CurrentTrain: epoch  9, batch     4 | loss: 5.1074209CurrentTrain: epoch  9, batch     5 | loss: 5.1780319CurrentTrain: epoch  9, batch     6 | loss: 5.4686308CurrentTrain: epoch  9, batch     7 | loss: 5.1841688CurrentTrain: epoch  9, batch     8 | loss: 5.1631765CurrentTrain: epoch  9, batch     9 | loss: 5.0416660CurrentTrain: epoch  9, batch    10 | loss: 5.0971518CurrentTrain: epoch  9, batch    11 | loss: 5.0610533CurrentTrain: epoch  9, batch    12 | loss: 5.1421871CurrentTrain: epoch  9, batch    13 | loss: 5.2279015CurrentTrain: epoch  9, batch    14 | loss: 5.3229456CurrentTrain: epoch  9, batch    15 | loss: 5.0906010CurrentTrain: epoch  9, batch    16 | loss: 5.0985069CurrentTrain: epoch  9, batch    17 | loss: 5.8242164CurrentTrain: epoch  9, batch    18 | loss: 5.5273147CurrentTrain: epoch  9, batch    19 | loss: 5.1797643CurrentTrain: epoch  9, batch    20 | loss: 5.3398924CurrentTrain: epoch  9, batch    21 | loss: 4.9716997CurrentTrain: epoch  9, batch    22 | loss: 5.1110754CurrentTrain: epoch  9, batch    23 | loss: 5.2672491CurrentTrain: epoch  9, batch    24 | loss: 5.1435614CurrentTrain: epoch  9, batch    25 | loss: 5.7747397CurrentTrain: epoch  9, batch    26 | loss: 5.6257625CurrentTrain: epoch  9, batch    27 | loss: 5.3424568CurrentTrain: epoch  9, batch    28 | loss: 5.0072770CurrentTrain: epoch  9, batch    29 | loss: 5.5512266CurrentTrain: epoch  9, batch    30 | loss: 4.9825134CurrentTrain: epoch  9, batch    31 | loss: 5.0399342CurrentTrain: epoch  9, batch    32 | loss: 5.1282659CurrentTrain: epoch  9, batch    33 | loss: 5.3177009CurrentTrain: epoch  9, batch    34 | loss: 5.1026111CurrentTrain: epoch  9, batch    35 | loss: 4.9330521CurrentTrain: epoch  9, batch    36 | loss: 4.9632597CurrentTrain: epoch  9, batch    37 | loss: 5.1368017
#### person countries of residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person countries of residence" means a person is living in a country.
Here is an example:
Relation: person countries of residence
Context: khamenei , 67 , has final say on all state matters in iran as supreme leader , a post he has held since 1989 .
Head Entity: khamenei
Tail Entity: iran
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person countries of residence":
Relation: person countries of residence  
Context: After years of living in the United States, Maria decided to return to her homeland, spain, where she felt a deep connection to her roots.  
Head Entity: Maria  
Tail Entity: spain  

Relation: person countries of residence  
Context: Following his successful career in the tech industry, Raj moved to canada to enjoy a quieter life surrounded by nature.  
Head Entity: Raj  
Tail Entity: canada  
#### organization top members employees ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization top members employees" means an organization 's employer is a person.
Here is an example:
Relation: organization top members employees
Context: kao charng , vice chairman of the cabinet-level mainland affairs council -lrb- mac -rrb- , explained that the termination clause was added to give both sides an option after the implementation of the tariff-cutting economic cooperation framework agreement -lrb- ecfa -rrb- .
Head Entity: mainland affairs council
Tail Entity: kao charng
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization top members employees":
Relation: organization top members employees  
Context: elon musk, the CEO of spacex, announced the successful launch of the falcon heavy rocket, marking a significant milestone for the company.  
Head Entity: spacex  
Tail Entity: elon musk  

Relation: organization top members employees  
Context: sheryl sandberg, the chief operating officer of facebook, discussed the company's new initiatives aimed at improving user privacy during a recent conference.  
Head Entity: facebook  
Tail Entity: sheryl sandberg  
#### organization member of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization member of" means an organization is the member of an organization.
Here is an example:
Relation: organization member of
Context: south africa 1-5 leyds cnr biccard streets cosatu 's concern has been that the scorpions ' existence as a branch of the national directorate of public prosecutions blurred the necessary separation of functions between those investigating crime and those prosecuting the criminals .
Head Entity: scorpions
Tail Entity: national directorate of public prosecutions
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization member of":
Relation: organization member of  
Context: The International Federation of Red Cross and Red Crescent Societies (IFRC) is a global humanitarian organization that coordinates the activities of national Red Cross and Red Crescent societies around the world.  
Head Entity: International Federation of Red Cross and Red Crescent Societies  
Tail Entity: national Red Cross and Red Crescent societies  

Relation: organization member of  
Context: The European Union (EU) is a political and economic union of member states that are located primarily in Europe, working together to promote peace and stability in the region.  
Head Entity: European Union  
Tail Entity: member states  
#### person origin ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person origin" means a person 's nationality is a country.
Here is an example:
Relation: person origin
Context: iranian nuclear negotiator ali larijani said thursday in ankara that talks on settling the iranian nuclear crisis had made some progress towards a `` united view . ''
Head Entity: ali larijani
Tail Entity: iranian
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person origin":
Relation: person origin  
Context: The famous physicist Albert Einstein was born in the Kingdom of Württemberg in the German Empire, which is now part of modern-day Germany.  
Head Entity: Albert Einstein  
Tail Entity: German  

Relation: person origin  
Context: The renowned author Chimamanda Ngozi Adichie often speaks about her Nigerian heritage and how it influences her writing.  
Head Entity: Chimamanda Ngozi Adichie  
Tail Entity: Nigerian  
#### person title ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person title" means a person 's title is a title.
Here is an example:
Relation: person title
Context: `` it has to have some tempo , it has to have some energy , '' said sean ross , vice president of music and programming at edison media research , which surveys the radio industry .
Head Entity: sean ross
Tail Entity: vice president
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person title":
Relation: person title  
Context: "After years of hard work and dedication, Maria Gonzalez was promoted to Chief Financial Officer at Tech Innovations, where she has been instrumental in driving the company's financial strategy."  
Head Entity: Maria Gonzalez  
Tail Entity: Chief Financial Officer  

Relation: person title  
Context: "During the annual conference, Dr. James Lee, the lead researcher at the National Institute of Health, presented his findings on the latest advancements in medical technology."  
Head Entity: Dr. James Lee  
Tail Entity: lead researcher  
#### organization country of headquarters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization country of headquarters" means an organization is located in a country.
Here is an example:
Relation: organization country of headquarters
Context: the credit crisis spread to the largest us bond insurer thursday , sending shares of mbia inc plunging and calling into question the safety of tens of billions of dollars of company and local government debt held by investors .
Head Entity: mbia
Tail Entity: us
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization country of headquarters":
Relation: organization country of headquarters  
Context: after years of rapid growth, the tech giant apple inc announced plans to expand its operations in ireland, taking advantage of the favorable tax environment.  
Head Entity: apple  
Tail Entity: ireland  

Relation: organization country of headquarters  
Context: the renowned automotive manufacturer toyota has established its main office in japan, where it continues to innovate and lead in the global market.  
Head Entity: toyota  
Tail Entity: japan  
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 75.00%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    3 | acc: 62.50%,  total acc: 76.56%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 77.50%   [EVAL] batch:    5 | acc: 68.75%,  total acc: 76.04%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 78.57%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 82.64%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 83.75%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 85.23%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 85.94%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 86.54%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 85.27%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 84.58%   [EVAL] batch:   15 | acc: 62.50%,  total acc: 83.20%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 82.72%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 81.60%   [EVAL] batch:   18 | acc: 81.25%,  total acc: 81.58%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 81.56%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 82.44%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 83.24%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 83.97%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 84.64%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 85.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 85.82%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 86.11%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 86.61%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 87.07%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 86.88%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 86.90%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 87.11%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 85.61%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 75.00%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    3 | acc: 62.50%,  total acc: 76.56%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 77.50%   [EVAL] batch:    5 | acc: 68.75%,  total acc: 76.04%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 78.57%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 82.64%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 83.75%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 85.23%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 85.94%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 86.54%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 85.27%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 84.58%   [EVAL] batch:   15 | acc: 62.50%,  total acc: 83.20%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 82.72%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 81.60%   [EVAL] batch:   18 | acc: 81.25%,  total acc: 81.58%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 81.56%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 82.44%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 83.24%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 83.97%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 84.64%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 85.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 85.82%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 86.11%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 86.61%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 87.07%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 86.88%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 86.90%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 87.11%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 85.61%   
cur_acc:  ['0.8561']
his_acc:  ['0.8561']
CurrentTrain: epoch  0, batch     0 | loss: 6.7105479CurrentTrain: epoch  0, batch     1 | loss: 7.8216114CurrentTrain: epoch  1, batch     0 | loss: 6.5109053CurrentTrain: epoch  1, batch     1 | loss: 5.2046876CurrentTrain: epoch  2, batch     0 | loss: 5.8193226CurrentTrain: epoch  2, batch     1 | loss: 5.3728476CurrentTrain: epoch  3, batch     0 | loss: 5.3471889CurrentTrain: epoch  3, batch     1 | loss: 4.4202156CurrentTrain: epoch  4, batch     0 | loss: 4.8331256CurrentTrain: epoch  4, batch     1 | loss: 4.8644309CurrentTrain: epoch  5, batch     0 | loss: 4.9063978CurrentTrain: epoch  5, batch     1 | loss: 3.9061246CurrentTrain: epoch  6, batch     0 | loss: 4.2706718CurrentTrain: epoch  6, batch     1 | loss: 3.8399904CurrentTrain: epoch  7, batch     0 | loss: 4.1974335CurrentTrain: epoch  7, batch     1 | loss: 4.0149384CurrentTrain: epoch  8, batch     0 | loss: 3.6917348CurrentTrain: epoch  8, batch     1 | loss: 3.3621042CurrentTrain: epoch  9, batch     0 | loss: 3.4238887CurrentTrain: epoch  9, batch     1 | loss: 3.0295978
#### person date of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person date of birth" means a person was born in a date.
Here is an example:
Relation: person date of birth
Context: kirkaldy , born irene morgan in baltimore , maryland , in 1917 , was arrested in 1944 for refusing to give up her seat on a greyhound bus heading from gloucester to baltimore , and for resisting arrest .
Head Entity: irene morgan
Tail Entity: 1917
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person date of birth":
Relation: person date of birth  
Context: the famous physicist albert einstein was born in ulm, in the kingdom of wurttemberg in the german empire, on march 14, 1879.  
Head Entity: albert einstein  
Tail Entity: march 14, 1879  

Relation: person date of birth  
Context: the renowned author jane austen was born on december 16, 1775, in steventon, hampshire, england.  
Head Entity: jane austen  
Tail Entity: december 16, 1775  
#### person stateorprovince of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person stateorprovince of birth" means a person was died in a state or province.
Here is an example:
Relation: person stateorprovince of birth
Context: jane matilda bolin was born on april 11 , 1908 , in poughkeepsie , ny .
Head Entity: jane matilda bolin
Tail Entity: ny
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person stateorprovince of birth":
Relation: person stateorprovince of birth  
Context: albert einstein was born on march 14, 1879, in ulm, germany.  
Head Entity: albert einstein  
Tail Entity: germany  

Relation: person stateorprovince of birth  
Context: marilyn monroe was born on june 1, 1926, in los angeles, california.  
Head Entity: marilyn monroe  
Tail Entity: california  
#### person parents ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person parents" means a person 's parent is a person.
Here is an example:
Relation: person parents
Context: lynne spears told the court that lutfi had treated her daughter like a hostage in her own home , drugged her and took over her finances .
Head Entity: her
Tail Entity: lynne spears
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person parents":
Sample 1:  
Relation: person parents  
Context: During the family reunion, Sarah introduced her father, who had always been a guiding force in her life, to her friends.  
Head Entity: her father  
Tail Entity: Sarah  

Sample 2:  
Relation: person parents  
Context: After the ceremony, Emily shared how her mother had inspired her to pursue a career in medicine, emphasizing the sacrifices she made for her education.  
Head Entity: her mother  
Tail Entity: Emily  
#### person employee of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person employee of" means a person is an employee of an organization.
Here is an example:
Relation: person employee of
Context: seeking revenge , axel reunites with old pal sgt. billy rosewood -lrb- judge reinhold -rrb- and jon flint -lrb- hector elizondo -rrb- of the beverly hills police department .
Head Entity: hector elizondo
Tail Entity: beverly hills police department
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person employee of":
Relation: person employee of  
Context: After years of hard work and dedication, Sarah Thompson finally received a promotion at Tech Innovations, where she has been a key player in the development team.  
Head Entity: Sarah Thompson  
Tail Entity: Tech Innovations  

Relation: person employee of  
Context: John Smith has been with Global Finance for over a decade, where he has climbed the ranks to become the senior analyst in the investment division.  
Head Entity: John Smith  
Tail Entity: Global Finance  
#### person stateorprovince of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person stateorprovince of death" means a person was died in a state or province.
Here is an example:
Relation: person stateorprovince of death
Context: irene morgan kirkaldy , 90 , who died of alzheimer 's disease aug. 10 at her home in gloucester , va. , quietly changed history in 1944 when she refused to give up her seat on a crowded greyhound bus to a white couple .
Head Entity: irene morgan kirkaldy
Tail Entity: va.
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person stateorprovince of death":
Relation: person stateorprovince of death  
Context: john smith, a renowned author, passed away on march 5 in his residence located in los angeles, california, leaving behind a legacy of literary works that inspired many.  
Head Entity: john smith  
Tail Entity: california  

Relation: person stateorprovince of death  
Context: after a long battle with cancer, elizabeth taylor died on march 23 at a hospital in los angeles, california, surrounded by her family and friends.  
Head Entity: elizabeth taylor  
Tail Entity: california  
Mixup data size:  103
MixupTrain:  epoch  0, batch     0 | loss: 6.8337825MixupTrain:  epoch  0, batch     1 | loss: 6.1750321MixupTrain:  epoch  0, batch     2 | loss: 5.9838954MixupTrain:  epoch  0, batch     3 | loss: 6.7464843MixupTrain:  epoch  0, batch     4 | loss: 5.7290632MixupTrain:  epoch  0, batch     5 | loss: 5.4464071MixupTrain:  epoch  0, batch     6 | loss: 4.9433422
MemoryTrain:  epoch  0, batch     0 | loss: 3.6540117MemoryTrain:  epoch  0, batch     1 | loss: 3.4233887MemoryTrain:  epoch  0, batch     2 | loss: 4.5018106MemoryTrain:  epoch  1, batch     0 | loss: 3.6419182MemoryTrain:  epoch  1, batch     1 | loss: 2.5653849MemoryTrain:  epoch  1, batch     2 | loss: 4.5585256MemoryTrain:  epoch  2, batch     0 | loss: 2.6249282MemoryTrain:  epoch  2, batch     1 | loss: 3.1029711MemoryTrain:  epoch  2, batch     2 | loss: 3.0093479MemoryTrain:  epoch  3, batch     0 | loss: 2.6627624MemoryTrain:  epoch  3, batch     1 | loss: 2.4079797MemoryTrain:  epoch  3, batch     2 | loss: 4.0382447MemoryTrain:  epoch  4, batch     0 | loss: 2.1902547MemoryTrain:  epoch  4, batch     1 | loss: 2.8765349MemoryTrain:  epoch  4, batch     2 | loss: 2.3260109MemoryTrain:  epoch  5, batch     0 | loss: 2.3617368MemoryTrain:  epoch  5, batch     1 | loss: 2.1671216MemoryTrain:  epoch  5, batch     2 | loss: 3.6163635MemoryTrain:  epoch  6, batch     0 | loss: 2.0114596MemoryTrain:  epoch  6, batch     1 | loss: 2.3728166MemoryTrain:  epoch  6, batch     2 | loss: 1.2143236MemoryTrain:  epoch  7, batch     0 | loss: 2.2131369MemoryTrain:  epoch  7, batch     1 | loss: 2.4095383MemoryTrain:  epoch  7, batch     2 | loss: 2.1732876MemoryTrain:  epoch  8, batch     0 | loss: 2.0782969MemoryTrain:  epoch  8, batch     1 | loss: 1.9247062MemoryTrain:  epoch  8, batch     2 | loss: 1.3511800MemoryTrain:  epoch  9, batch     0 | loss: 1.8019125MemoryTrain:  epoch  9, batch     1 | loss: 1.9961102MemoryTrain:  epoch  9, batch     2 | loss: 1.9839081
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 82.81%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 85.00%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 88.39%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 89.06%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 90.00%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 89.20%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 89.06%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 88.94%   [EVAL] batch:   13 | acc: 50.00%,  total acc: 86.16%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 75.00%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 77.08%   [EVAL] batch:    3 | acc: 62.50%,  total acc: 73.44%   [EVAL] batch:    4 | acc: 56.25%,  total acc: 70.00%   [EVAL] batch:    5 | acc: 56.25%,  total acc: 67.71%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 72.32%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 75.78%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 77.78%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 79.38%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 81.77%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 82.69%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 81.70%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 79.69%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 79.41%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 78.47%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 77.96%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 78.12%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 79.17%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 80.11%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 80.98%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 81.77%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 82.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 83.17%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 83.56%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 84.15%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 84.70%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 84.58%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 84.68%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 84.96%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 85.23%   [EVAL] batch:   33 | acc: 68.75%,  total acc: 84.74%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 85.00%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 85.07%   [EVAL] batch:   36 | acc: 81.25%,  total acc: 84.97%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 85.36%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 85.74%   [EVAL] batch:   39 | acc: 87.50%,  total acc: 85.78%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 86.13%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 86.31%   [EVAL] batch:   42 | acc: 87.50%,  total acc: 86.34%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 86.22%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 86.25%   [EVAL] batch:   45 | acc: 75.00%,  total acc: 86.01%   [EVAL] batch:   46 | acc: 6.25%,  total acc: 84.31%   
cur_acc:  ['0.8561', '0.8616']
his_acc:  ['0.8561', '0.8431']
CurrentTrain: epoch  0, batch     0 | loss: 6.0126648CurrentTrain: epoch  0, batch     1 | loss: 7.1522269CurrentTrain: epoch  1, batch     0 | loss: 5.6284180CurrentTrain: epoch  1, batch     1 | loss: 4.8644328CurrentTrain: epoch  2, batch     0 | loss: 5.3028092CurrentTrain: epoch  2, batch     1 | loss: 4.2233982CurrentTrain: epoch  3, batch     0 | loss: 4.2750626CurrentTrain: epoch  3, batch     1 | loss: 3.9826355CurrentTrain: epoch  4, batch     0 | loss: 3.3750858CurrentTrain: epoch  4, batch     1 | loss: 4.4900651CurrentTrain: epoch  5, batch     0 | loss: 3.6455188CurrentTrain: epoch  5, batch     1 | loss: 3.6136029CurrentTrain: epoch  6, batch     0 | loss: 3.5899186CurrentTrain: epoch  6, batch     1 | loss: 3.0967343CurrentTrain: epoch  7, batch     0 | loss: 3.7020404CurrentTrain: epoch  7, batch     1 | loss: 3.1431365CurrentTrain: epoch  8, batch     0 | loss: 3.0120444CurrentTrain: epoch  8, batch     1 | loss: 3.7096643CurrentTrain: epoch  9, batch     0 | loss: 2.6827936CurrentTrain: epoch  9, batch     1 | loss: 3.8057332
#### person country of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person country of birth" means a person was born in a country.
Here is an example:
Relation: person country of birth
Context: in testimony by satellite link from germany to a house of representatives ' panel , murat kurnaz recounted his five-year detention , alleging a wide range of torture and abuse .
Head Entity: murat kurnaz
Tail Entity: germany
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person country of birth":
Relation: person country of birth  
Context: During the interview, the renowned artist shared his experiences growing up in the vibrant streets of Barcelona, which he credits as the foundation of his creative journey.  
Head Entity: the renowned artist  
Tail Entity: Barcelona  

Relation: person country of birth  
Context: In her autobiography, the famous actress reflects on her childhood in Mumbai, where she developed a passion for performing arts that would later define her career.  
Head Entity: the famous actress  
Tail Entity: Mumbai  
#### organization website ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization website" means an organization 's website is an url.
Here is an example:
Relation: organization website
Context: http://www.pentaxsailaway.com/ paul j. sail away with pentax sweepstakes
Head Entity: pentax
Tail Entity: http://www.pentaxsailaway.com/
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization website":
Relation: organization website  
Context: Visit the official site at https://www.techinnovators.com for more information on their latest products.  
Head Entity: Tech Innovators  
Tail Entity: https://www.techinnovators.com  

Relation: organization website  
Context: For updates and news, check out the website of Green Earth Initiative at http://www.greenearth.org.  
Head Entity: Green Earth Initiative  
Tail Entity: http://www.greenearth.org  
#### organization shareholders ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization shareholders" means an organization was invested by a person.
Here is an example:
Relation: organization shareholders
Context: marsans already owns the madrid-based air comet and 95 percent of aerolineas argentinas .
Head Entity: aerolineas argentinas
Tail Entity: marsans
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization shareholders":
Relation: organization shareholders  
Context: tech giant apple inc. has recently acquired a significant stake in the innovative startup nextdoor.  
Head Entity: nextdoor  
Tail Entity: apple inc.  

Relation: organization shareholders  
Context: the investment firm blackrock has increased its holdings in the renewable energy company sunrun.  
Head Entity: sunrun  
Tail Entity: blackrock  
#### organization dissolved ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization dissolved" means an organization was dissolved in a date.
Here is an example:
Relation: organization dissolved
Context: the tse suffered its worst-ever system crash in november 2005 which paralyzed the world 's second largest bourse and forced it to shelve plan for a listing of its own .
Head Entity: tse
Tail Entity: november 2005
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization dissolved":
Relation: organization dissolved  
Context: the once-prominent tech startup, Innovatech, officially ceased operations in early 2020 after struggling to secure funding.  
Head Entity: Innovatech  
Tail Entity: early 2020  

Relation: organization dissolved  
Context: after years of financial difficulties, the local arts council announced its closure in the spring of 2019, leaving many artists without support.  
Head Entity: local arts council  
Tail Entity: spring of 2019  
#### organization founded by ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization founded by" means an organization was found by a person.
Here is an example:
Relation: organization founded by
Context: `` i have no greater obligation than to ensure the safely of airline travelers in this country , '' transportation secretary ray lahood said in a joint statement with j. randolph babbitt , administrator of the federal aviation administration , that was issued on the eve of a senate hearing on aviation safety .
Head Entity: federal aviation administration
Tail Entity: j. randolph babbitt
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization founded by":
Relation: organization founded by  
Context: In 1975, the renowned physicist and entrepreneur, Dr. Jane Smith, established Quantum Innovations, a company dedicated to advancing quantum computing technologies.  
Head Entity: Quantum Innovations  
Tail Entity: Dr. Jane Smith  

Relation: organization founded by  
Context: The charity organization, Hope for Tomorrow, was created in 2010 by the well-known philanthropist, John Doe, to support underprivileged children around the world.  
Head Entity: Hope for Tomorrow  
Tail Entity: John Doe  
Mixup data size:  133
MixupTrain:  epoch  0, batch     0 | loss: 3.8552855MixupTrain:  epoch  0, batch     1 | loss: 3.3780790MixupTrain:  epoch  0, batch     2 | loss: 4.1100624MixupTrain:  epoch  0, batch     3 | loss: 4.3383938MixupTrain:  epoch  0, batch     4 | loss: 4.2726465MixupTrain:  epoch  0, batch     5 | loss: 4.2236799MixupTrain:  epoch  0, batch     6 | loss: 4.5997279MixupTrain:  epoch  0, batch     7 | loss: 3.5192338MixupTrain:  epoch  0, batch     8 | loss: 3.8314648
MemoryTrain:  epoch  0, batch     0 | loss: 3.6976542MemoryTrain:  epoch  0, batch     1 | loss: 3.8522363MemoryTrain:  epoch  0, batch     2 | loss: 3.1861463MemoryTrain:  epoch  1, batch     0 | loss: 2.9348822MemoryTrain:  epoch  1, batch     1 | loss: 3.7076354MemoryTrain:  epoch  1, batch     2 | loss: 3.2135067MemoryTrain:  epoch  2, batch     0 | loss: 2.9438429MemoryTrain:  epoch  2, batch     1 | loss: 2.5559525MemoryTrain:  epoch  2, batch     2 | loss: 3.0648322MemoryTrain:  epoch  3, batch     0 | loss: 2.5881839MemoryTrain:  epoch  3, batch     1 | loss: 2.5478570MemoryTrain:  epoch  3, batch     2 | loss: 3.0495377MemoryTrain:  epoch  4, batch     0 | loss: 2.4109755MemoryTrain:  epoch  4, batch     1 | loss: 2.4779272MemoryTrain:  epoch  4, batch     2 | loss: 2.6791477MemoryTrain:  epoch  5, batch     0 | loss: 2.3876581MemoryTrain:  epoch  5, batch     1 | loss: 2.4463995MemoryTrain:  epoch  5, batch     2 | loss: 2.6280363MemoryTrain:  epoch  6, batch     0 | loss: 2.2310801MemoryTrain:  epoch  6, batch     1 | loss: 1.8883353MemoryTrain:  epoch  6, batch     2 | loss: 2.1786466MemoryTrain:  epoch  7, batch     0 | loss: 2.1306648MemoryTrain:  epoch  7, batch     1 | loss: 2.1759655MemoryTrain:  epoch  7, batch     2 | loss: 1.8178750MemoryTrain:  epoch  8, batch     0 | loss: 1.7625833MemoryTrain:  epoch  8, batch     1 | loss: 2.1245813MemoryTrain:  epoch  8, batch     2 | loss: 2.1468263MemoryTrain:  epoch  9, batch     0 | loss: 2.0049229MemoryTrain:  epoch  9, batch     1 | loss: 1.9656622MemoryTrain:  epoch  9, batch     2 | loss: 1.7123877
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 62.50%,  total acc: 79.17%   [EVAL] batch:    3 | acc: 0.00%,  total acc: 59.38%   [EVAL] batch:    4 | acc: 0.00%,  total acc: 47.50%   [EVAL] batch:    5 | acc: 0.00%,  total acc: 39.58%   [EVAL] batch:    6 | acc: 6.25%,  total acc: 34.82%   [EVAL] batch:    7 | acc: 0.00%,  total acc: 30.47%   
[EVAL] batch:    0 | acc: 50.00%,  total acc: 50.00%   [EVAL] batch:    1 | acc: 62.50%,  total acc: 56.25%   [EVAL] batch:    2 | acc: 68.75%,  total acc: 60.42%   [EVAL] batch:    3 | acc: 37.50%,  total acc: 54.69%   [EVAL] batch:    4 | acc: 37.50%,  total acc: 51.25%   [EVAL] batch:    5 | acc: 50.00%,  total acc: 51.04%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 55.36%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 59.38%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 63.19%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 65.62%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 67.61%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 69.27%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 69.23%   [EVAL] batch:   13 | acc: 37.50%,  total acc: 66.96%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 67.50%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 66.80%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 67.28%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 67.01%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 67.11%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 67.81%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 69.35%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 70.74%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 72.01%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 73.18%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 74.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 75.24%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 75.93%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 76.79%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 77.59%   [EVAL] batch:   29 | acc: 93.75%,  total acc: 78.12%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 78.83%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 79.30%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 79.73%   [EVAL] batch:   33 | acc: 68.75%,  total acc: 79.41%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 79.82%   [EVAL] batch:   35 | acc: 81.25%,  total acc: 79.86%   [EVAL] batch:   36 | acc: 68.75%,  total acc: 79.56%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 79.93%   [EVAL] batch:   38 | acc: 93.75%,  total acc: 80.29%   [EVAL] batch:   39 | acc: 81.25%,  total acc: 80.31%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 80.79%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:   42 | acc: 87.50%,  total acc: 81.40%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 81.39%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 81.53%   [EVAL] batch:   45 | acc: 31.25%,  total acc: 80.43%   [EVAL] batch:   46 | acc: 75.00%,  total acc: 80.32%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 80.73%   [EVAL] batch:   48 | acc: 62.50%,  total acc: 80.36%   [EVAL] batch:   49 | acc: 0.00%,  total acc: 78.75%   [EVAL] batch:   50 | acc: 0.00%,  total acc: 77.21%   [EVAL] batch:   51 | acc: 0.00%,  total acc: 75.72%   [EVAL] batch:   52 | acc: 6.25%,  total acc: 74.41%   [EVAL] batch:   53 | acc: 0.00%,  total acc: 73.03%   
cur_acc:  ['0.8561', '0.8616', '0.3047']
his_acc:  ['0.8561', '0.8431', '0.7303']
CurrentTrain: epoch  0, batch     0 | loss: 4.9414701CurrentTrain: epoch  0, batch     1 | loss: 5.2341571CurrentTrain: epoch  1, batch     0 | loss: 4.2756157CurrentTrain: epoch  1, batch     1 | loss: 4.1471586CurrentTrain: epoch  2, batch     0 | loss: 3.8033593CurrentTrain: epoch  2, batch     1 | loss: 3.0575750CurrentTrain: epoch  3, batch     0 | loss: 3.5120549CurrentTrain: epoch  3, batch     1 | loss: 2.8920603CurrentTrain: epoch  4, batch     0 | loss: 3.4351206CurrentTrain: epoch  4, batch     1 | loss: 2.9066045CurrentTrain: epoch  5, batch     0 | loss: 2.9306293CurrentTrain: epoch  5, batch     1 | loss: 2.4742348CurrentTrain: epoch  6, batch     0 | loss: 2.5504441CurrentTrain: epoch  6, batch     1 | loss: 2.4045696CurrentTrain: epoch  7, batch     0 | loss: 2.7855148CurrentTrain: epoch  7, batch     1 | loss: 2.3196805CurrentTrain: epoch  8, batch     0 | loss: 2.5661120CurrentTrain: epoch  8, batch     1 | loss: 2.4774556CurrentTrain: epoch  9, batch     0 | loss: 2.2330050CurrentTrain: epoch  9, batch     1 | loss: 2.4655015
#### person cause of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person cause of death" means a person was died of an event.
Here is an example:
Relation: person cause of death
Context: mr scheider had suffered from multiple myeloma for several years , and died of complications from a staph infection , his wife , brenda siemer , said .
Head Entity: mr scheider
Tail Entity: complications from a staph infection
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person cause of death":
Relation: person cause of death  
Context: after battling with lung cancer for over a year, john doe passed away peacefully in his sleep, surrounded by family.  
Head Entity: john doe  
Tail Entity: lung cancer  

Relation: person cause of death  
Context: the renowned scientist, dr. emily white, tragically lost her life due to a car accident while returning from a conference.  
Head Entity: dr. emily white  
Tail Entity: car accident  
#### organization political religious affiliation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization political religious affiliation" means an organization is the member of a religion.
Here is an example:
Relation: organization political religious affiliation
Context: the mnlf used to be the largest muslim group fighting for a separate islamic homeland in the southern philippines until it settled for limited autonomy and signed a peace agreement with manila in 1996 .
Head Entity: mnlf
Tail Entity: islamic
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization political religious affiliation":
Relation: organization political religious affiliation  
Context: the national council of churches in the philippines has been a significant advocate for social justice and has ties to various religious groups across the country.  
Head Entity: national council of churches in the philippines  
Tail Entity: religious groups  

Relation: organization political religious affiliation  
Context: the interfaith alliance works to promote understanding and cooperation among different faiths, influencing political discourse in the region.  
Head Entity: interfaith alliance  
Tail Entity: faiths  
#### organization stateorprovince of headquarters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization stateorprovince of headquarters" means an organization is located in a state or province.
Here is an example:
Relation: organization stateorprovince of headquarters
Context: based in armonk , new york , mbia insures $ 670 billion -lrb- euro452 .18 billion -rrb- in debt .
Head Entity: mbia
Tail Entity: new york
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization stateorprovince of headquarters":
Relation: organization stateorprovince of headquarters  
Context: the tech giant apple inc. has its headquarters in cupertino, california, where it designs and develops its products.  
Head Entity: apple inc.  
Tail Entity: california  

Relation: organization stateorprovince of headquarters  
Context: the multinational corporation samsung electronics is headquartered in suwon, south korea, and is a leader in consumer electronics.  
Head Entity: samsung electronics  
Tail Entity: south korea  
#### person other family ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person other family" means a person 's relative is a person.
Here is an example:
Relation: person other family
Context: parren mitchell 's sister-in-law , juanita jackson mitchell , was the long - time head and legal counsel of the maryland naacp .
Head Entity: parren mitchell
Tail Entity: juanita jackson mitchell
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person other family":
Relation: person other family  
Context: During the family reunion, it was revealed that Sarah's cousin, Michael, had recently graduated from college with honors.  
Head Entity: Sarah  
Tail Entity: Michael  

Relation: person other family  
Context: In her memoir, Lisa shared heartwarming stories about her grandmother, Evelyn, who played a significant role in her upbringing.  
Head Entity: Lisa  
Tail Entity: Evelyn  
#### person city of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person city of death" means a person was died in a city.
Here is an example:
Relation: person city of death
Context: millender-mcdonald , who was 68 , died late saturday at her home in carson , california , said her chief of staff , bandele mcqueen .
Head Entity: millender-mcdonald
Tail Entity: carson
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person city of death":
Relation: person city of death  
Context: renowned author and activist, james baldwin, passed away in 1987 in the city of saint-paul, minnesota, where he spent his final years.  
Head Entity: james baldwin  
Tail Entity: saint-paul  

Relation: person city of death  
Context: the famous musician, prince, was found dead in his home in minneapolis, minnesota, a city that shaped his iconic career.  
Head Entity: prince  
Tail Entity: minneapolis  
Mixup data size:  164
MixupTrain:  epoch  0, batch     0 | loss: 3.7128324MixupTrain:  epoch  0, batch     1 | loss: 3.2443816MixupTrain:  epoch  0, batch     2 | loss: 3.2546695MixupTrain:  epoch  0, batch     3 | loss: 3.3985941MixupTrain:  epoch  0, batch     4 | loss: 3.0906658MixupTrain:  epoch  0, batch     5 | loss: 3.5105946MixupTrain:  epoch  0, batch     6 | loss: 2.8797401MixupTrain:  epoch  0, batch     7 | loss: 2.7483102MixupTrain:  epoch  0, batch     8 | loss: 3.3344199MixupTrain:  epoch  0, batch     9 | loss: 3.1065182MixupTrain:  epoch  0, batch    10 | loss: 3.7118211
MemoryTrain:  epoch  0, batch     0 | loss: 2.7053485MemoryTrain:  epoch  0, batch     1 | loss: 3.6057677MemoryTrain:  epoch  0, batch     2 | loss: 3.0806270MemoryTrain:  epoch  0, batch     3 | loss: 3.7901242MemoryTrain:  epoch  1, batch     0 | loss: 2.9033713MemoryTrain:  epoch  1, batch     1 | loss: 3.5214791MemoryTrain:  epoch  1, batch     2 | loss: 3.1429856MemoryTrain:  epoch  1, batch     3 | loss: 2.6490481MemoryTrain:  epoch  2, batch     0 | loss: 2.8178520MemoryTrain:  epoch  2, batch     1 | loss: 2.9213371MemoryTrain:  epoch  2, batch     2 | loss: 2.4580050MemoryTrain:  epoch  2, batch     3 | loss: 2.3083665MemoryTrain:  epoch  3, batch     0 | loss: 2.7064910MemoryTrain:  epoch  3, batch     1 | loss: 2.1101618MemoryTrain:  epoch  3, batch     2 | loss: 2.4341455MemoryTrain:  epoch  3, batch     3 | loss: 2.0295584MemoryTrain:  epoch  4, batch     0 | loss: 2.5175254MemoryTrain:  epoch  4, batch     1 | loss: 2.0821373MemoryTrain:  epoch  4, batch     2 | loss: 2.3477783MemoryTrain:  epoch  4, batch     3 | loss: 2.2761490MemoryTrain:  epoch  5, batch     0 | loss: 2.1093240MemoryTrain:  epoch  5, batch     1 | loss: 2.2516036MemoryTrain:  epoch  5, batch     2 | loss: 1.8190041MemoryTrain:  epoch  5, batch     3 | loss: 2.0843749MemoryTrain:  epoch  6, batch     0 | loss: 2.1509037MemoryTrain:  epoch  6, batch     1 | loss: 1.7137638MemoryTrain:  epoch  6, batch     2 | loss: 1.8648477MemoryTrain:  epoch  6, batch     3 | loss: 1.6385919MemoryTrain:  epoch  7, batch     0 | loss: 1.8908654MemoryTrain:  epoch  7, batch     1 | loss: 2.0609429MemoryTrain:  epoch  7, batch     2 | loss: 1.5496223MemoryTrain:  epoch  7, batch     3 | loss: 1.4362969MemoryTrain:  epoch  8, batch     0 | loss: 1.6554182MemoryTrain:  epoch  8, batch     1 | loss: 1.7732171MemoryTrain:  epoch  8, batch     2 | loss: 1.6824367MemoryTrain:  epoch  8, batch     3 | loss: 1.6819031MemoryTrain:  epoch  9, batch     0 | loss: 1.6121939MemoryTrain:  epoch  9, batch     1 | loss: 1.5219414MemoryTrain:  epoch  9, batch     2 | loss: 1.7528341MemoryTrain:  epoch  9, batch     3 | loss: 1.7665799
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 90.62%   [EVAL] batch:    4 | acc: 25.00%,  total acc: 77.50%   [EVAL] batch:    5 | acc: 31.25%,  total acc: 69.79%   [EVAL] batch:    6 | acc: 43.75%,  total acc: 66.07%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 69.53%   [EVAL] batch:    8 | acc: 31.25%,  total acc: 65.28%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 65.00%   [EVAL] batch:   10 | acc: 75.00%,  total acc: 65.91%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 67.19%   [EVAL] batch:   12 | acc: 43.75%,  total acc: 65.38%   
[EVAL] batch:    0 | acc: 43.75%,  total acc: 43.75%   [EVAL] batch:    1 | acc: 56.25%,  total acc: 50.00%   [EVAL] batch:    2 | acc: 62.50%,  total acc: 54.17%   [EVAL] batch:    3 | acc: 25.00%,  total acc: 46.88%   [EVAL] batch:    4 | acc: 25.00%,  total acc: 42.50%   [EVAL] batch:    5 | acc: 37.50%,  total acc: 41.67%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 47.32%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 53.91%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 58.33%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 61.25%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 64.20%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 66.15%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 66.35%   [EVAL] batch:   13 | acc: 43.75%,  total acc: 64.73%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 65.42%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 64.84%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 65.44%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 65.28%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 65.46%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 66.56%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 68.15%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 69.60%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 70.92%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 72.14%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 73.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 74.28%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 75.00%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 75.89%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 76.72%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 77.08%   [EVAL] batch:   30 | acc: 81.25%,  total acc: 77.22%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 77.73%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 78.22%   [EVAL] batch:   33 | acc: 31.25%,  total acc: 76.84%   [EVAL] batch:   34 | acc: 25.00%,  total acc: 75.36%   [EVAL] batch:   35 | acc: 0.00%,  total acc: 73.26%   [EVAL] batch:   36 | acc: 0.00%,  total acc: 71.28%   [EVAL] batch:   37 | acc: 18.75%,  total acc: 69.90%   [EVAL] batch:   38 | acc: 25.00%,  total acc: 68.75%   [EVAL] batch:   39 | acc: 87.50%,  total acc: 69.22%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 69.97%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 70.54%   [EVAL] batch:   42 | acc: 87.50%,  total acc: 70.93%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 71.16%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 71.67%   [EVAL] batch:   45 | acc: 25.00%,  total acc: 70.65%   [EVAL] batch:   46 | acc: 68.75%,  total acc: 70.61%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 71.22%   [EVAL] batch:   48 | acc: 62.50%,  total acc: 71.05%   [EVAL] batch:   49 | acc: 18.75%,  total acc: 70.00%   [EVAL] batch:   50 | acc: 0.00%,  total acc: 68.63%   [EVAL] batch:   51 | acc: 18.75%,  total acc: 67.67%   [EVAL] batch:   52 | acc: 6.25%,  total acc: 66.51%   [EVAL] batch:   53 | acc: 75.00%,  total acc: 66.67%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 67.16%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 67.63%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 68.09%   [EVAL] batch:   57 | acc: 25.00%,  total acc: 67.35%   [EVAL] batch:   58 | acc: 25.00%,  total acc: 66.63%   [EVAL] batch:   59 | acc: 50.00%,  total acc: 66.35%   [EVAL] batch:   60 | acc: 81.25%,  total acc: 66.60%   [EVAL] batch:   61 | acc: 50.00%,  total acc: 66.33%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 66.07%   [EVAL] batch:   63 | acc: 81.25%,  total acc: 66.31%   [EVAL] batch:   64 | acc: 68.75%,  total acc: 66.35%   [EVAL] batch:   65 | acc: 62.50%,  total acc: 66.29%   
cur_acc:  ['0.8561', '0.8616', '0.3047', '0.6538']
his_acc:  ['0.8561', '0.8431', '0.7303', '0.6629']
CurrentTrain: epoch  0, batch     0 | loss: 7.6398849CurrentTrain: epoch  0, batch     1 | loss: 7.6440482CurrentTrain: epoch  1, batch     0 | loss: 6.6308661CurrentTrain: epoch  1, batch     1 | loss: 7.2477393CurrentTrain: epoch  2, batch     0 | loss: 6.4097533CurrentTrain: epoch  2, batch     1 | loss: 5.8537540CurrentTrain: epoch  3, batch     0 | loss: 6.2472997CurrentTrain: epoch  3, batch     1 | loss: 5.4103513CurrentTrain: epoch  4, batch     0 | loss: 5.6057692CurrentTrain: epoch  4, batch     1 | loss: 5.9389572CurrentTrain: epoch  5, batch     0 | loss: 4.9381065CurrentTrain: epoch  5, batch     1 | loss: 5.9971948CurrentTrain: epoch  6, batch     0 | loss: 5.2034101CurrentTrain: epoch  6, batch     1 | loss: 4.7341323CurrentTrain: epoch  7, batch     0 | loss: 5.2109919CurrentTrain: epoch  7, batch     1 | loss: 4.2763500CurrentTrain: epoch  8, batch     0 | loss: 4.3807206CurrentTrain: epoch  8, batch     1 | loss: 5.0039220CurrentTrain: epoch  9, batch     0 | loss: 4.7213449CurrentTrain: epoch  9, batch     1 | loss: 4.2089906
#### organization subsidiaries ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization subsidiaries" means an organization 's subsidiary is an organization.
Here is an example:
Relation: organization subsidiaries
Context: venture fund buys sporting chain highland capital 's consumer fund includes lululemon athletica , a yoga retailer , and o beverages , a flavored water company developed by tom first , one of the two `` juice guys '' who cofounded nantucket nectars .
Head Entity: highland capital
Tail Entity: o beverages
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization subsidiaries":
Relation: organization subsidiaries  
Context: The tech giant Alphabet Inc. has several subsidiaries, including YouTube, which has become a leading platform for video content, and Waymo, which focuses on self-driving technology.  
Head Entity: Alphabet Inc.  
Tail Entity: Waymo  

Relation: organization subsidiaries  
Context: The multinational corporation Procter & Gamble owns various subsidiaries, such as Gillette, which specializes in personal care products, and Tide, known for its laundry detergents.  
Head Entity: Procter & Gamble  
Tail Entity: Gillette  
#### organization parents ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization parents" means an organization 's parent is an organization.
Here is an example:
Relation: organization parents
Context: aseel kami , tim cocks , abdul-rahman taher , tim cocks and michael christie -lrb- reuters -rrb- add an intriguing note in the speculation over who 's responsible for the recent bombings : `` but a senior iraqi intelligence source , who declined to be named , said there was evidence the bombs could be the work of the militant badr organisation , the armed wing of the supreme islamic iraqi council -lrb- isci -rrb- .
Head Entity: badr organisation
Tail Entity: supreme islamic iraqi council
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization parents":
Relation: organization parents  
Context: In a recent merger, the tech giant SoftTech announced its acquisition of Innovatech, a leading software development firm. This move has raised questions about the future of Innovatech and its role under the SoftTech umbrella. Industry experts believe that this acquisition will strengthen SoftTech's position in the market.  
Head Entity: SoftTech  
Tail Entity: Innovatech  

Relation: organization parents  
Context: The historic partnership between the National Wildlife Federation and the World Wildlife Fund has led to significant advancements in conservation efforts. As these two organizations collaborate, they aim to tackle global environmental challenges more effectively.  
Head Entity: National Wildlife Federation  
Tail Entity: World Wildlife Fund  
#### organization alternate names ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization alternate names" means a person 's alias is a person.
Here is an example:
Relation: organization alternate names
Context: while section 106 of the hyde act openly bans indian testing , and the agreement upholds reinforces that test ban by upholding the applicability of domestic laws , washington has already recommended that the nuclear suppliers group -lrb- nsg -rrb- link its proposed exemption for india to a similar test ban .
Head Entity: nuclear suppliers group
Tail Entity: nsg
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization alternate names":
Relation: organization alternate names  
Context: The International Monetary Fund, often referred to as the IMF, plays a crucial role in global economic stability by providing financial assistance and advice to member countries.  
Head Entity: International Monetary Fund  
Tail Entity: IMF  

Relation: organization alternate names  
Context: The World Health Organization, commonly known as WHO, has been at the forefront of global health initiatives, especially during the COVID-19 pandemic.  
Head Entity: World Health Organization  
Tail Entity: WHO  
#### organization city of headquarters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization city of headquarters" means an organization is located in a city.
Here is an example:
Relation: organization city of headquarters
Context: guy-sheftall entered spelman college in atlanta at age 16 and later earned a master 's in english with a thesis titled `` faulkner 's treatment of women in his major novels . ''
Head Entity: spelman college
Tail Entity: atlanta
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization city of headquarters":
Relation: organization city of headquarters  
Context: the headquarters of tech giant apple inc. is located in cupertino, california, where it has been since 1993.  
Head Entity: apple inc.  
Tail Entity: cupertino  

Relation: organization city of headquarters  
Context: the united nations has its main office in new york city, serving as a hub for international diplomacy and cooperation.  
Head Entity: united nations  
Tail Entity: new york city  
#### person siblings ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person siblings" means a person 's sibling is a person.
Here is an example:
Relation: person siblings
Context: forsberg , a political science professor at city college of new york , died oct. 19 in a bronx hospital of cancer , said her sister , celia seupel .
Head Entity: forsberg
Tail Entity: celia seupel
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person siblings":
Relation: person siblings  
Context: during the family reunion, john introduced his sister, emily, who had just returned from studying abroad.  
Head Entity: john  
Tail Entity: emily  

Relation: person siblings  
Context: at the award ceremony, sarah proudly accepted her trophy while her brother, michael, cheered from the audience.  
Head Entity: sarah  
Tail Entity: michael  
Mixup data size:  194
MixupTrain:  epoch  0, batch     0 | loss: 3.7056892MixupTrain:  epoch  0, batch     1 | loss: 3.9969927MixupTrain:  epoch  0, batch     2 | loss: 3.6318244MixupTrain:  epoch  0, batch     3 | loss: 3.3366815MixupTrain:  epoch  0, batch     4 | loss: 3.2613521MixupTrain:  epoch  0, batch     5 | loss: 3.5676701MixupTrain:  epoch  0, batch     6 | loss: 3.1198838MixupTrain:  epoch  0, batch     7 | loss: 3.6841061MixupTrain:  epoch  0, batch     8 | loss: 2.9339870MixupTrain:  epoch  0, batch     9 | loss: 3.0684254MixupTrain:  epoch  0, batch    10 | loss: 3.1466764MixupTrain:  epoch  0, batch    11 | loss: 3.7176898MixupTrain:  epoch  0, batch    12 | loss: 2.4528379
MemoryTrain:  epoch  0, batch     0 | loss: 2.7511117MemoryTrain:  epoch  0, batch     1 | loss: 2.7068410MemoryTrain:  epoch  0, batch     2 | loss: 4.4895353MemoryTrain:  epoch  0, batch     3 | loss: 2.6286213MemoryTrain:  epoch  0, batch     4 | loss: 3.0331957MemoryTrain:  epoch  1, batch     0 | loss: 2.0612235MemoryTrain:  epoch  1, batch     1 | loss: 1.8822491MemoryTrain:  epoch  1, batch     2 | loss: 3.1455431MemoryTrain:  epoch  1, batch     3 | loss: 3.6592278MemoryTrain:  epoch  1, batch     4 | loss: 3.9370053MemoryTrain:  epoch  2, batch     0 | loss: 2.6919386MemoryTrain:  epoch  2, batch     1 | loss: 2.7882361MemoryTrain:  epoch  2, batch     2 | loss: 3.5396616MemoryTrain:  epoch  2, batch     3 | loss: 2.6826806MemoryTrain:  epoch  2, batch     4 | loss: 1.6944953MemoryTrain:  epoch  3, batch     0 | loss: 2.8582191MemoryTrain:  epoch  3, batch     1 | loss: 2.3929911MemoryTrain:  epoch  3, batch     2 | loss: 2.6215081MemoryTrain:  epoch  3, batch     3 | loss: 2.6610794MemoryTrain:  epoch  3, batch     4 | loss: 2.4115930MemoryTrain:  epoch  4, batch     0 | loss: 2.9016814MemoryTrain:  epoch  4, batch     1 | loss: 1.8184644MemoryTrain:  epoch  4, batch     2 | loss: 2.3338089MemoryTrain:  epoch  4, batch     3 | loss: 2.6583312MemoryTrain:  epoch  4, batch     4 | loss: 2.5892730MemoryTrain:  epoch  5, batch     0 | loss: 1.8475978MemoryTrain:  epoch  5, batch     1 | loss: 2.0432096MemoryTrain:  epoch  5, batch     2 | loss: 2.0344830MemoryTrain:  epoch  5, batch     3 | loss: 2.4629126MemoryTrain:  epoch  5, batch     4 | loss: 2.6096492MemoryTrain:  epoch  6, batch     0 | loss: 2.6783774MemoryTrain:  epoch  6, batch     1 | loss: 2.4476993MemoryTrain:  epoch  6, batch     2 | loss: 1.9602170MemoryTrain:  epoch  6, batch     3 | loss: 1.9448571MemoryTrain:  epoch  6, batch     4 | loss: 1.7506260MemoryTrain:  epoch  7, batch     0 | loss: 2.4890332MemoryTrain:  epoch  7, batch     1 | loss: 2.0492532MemoryTrain:  epoch  7, batch     2 | loss: 1.6080452MemoryTrain:  epoch  7, batch     3 | loss: 2.2555370MemoryTrain:  epoch  7, batch     4 | loss: 1.7749414MemoryTrain:  epoch  8, batch     0 | loss: 1.7641531MemoryTrain:  epoch  8, batch     1 | loss: 1.6098530MemoryTrain:  epoch  8, batch     2 | loss: 1.8033584MemoryTrain:  epoch  8, batch     3 | loss: 2.0321059MemoryTrain:  epoch  8, batch     4 | loss: 2.5474277MemoryTrain:  epoch  9, batch     0 | loss: 2.1118941MemoryTrain:  epoch  9, batch     1 | loss: 2.0483797MemoryTrain:  epoch  9, batch     2 | loss: 1.5278847MemoryTrain:  epoch  9, batch     3 | loss: 1.8146002MemoryTrain:  epoch  9, batch     4 | loss: 1.7676805
[EVAL] batch:    0 | acc: 43.75%,  total acc: 43.75%   [EVAL] batch:    1 | acc: 37.50%,  total acc: 40.62%   [EVAL] batch:    2 | acc: 18.75%,  total acc: 33.33%   [EVAL] batch:    3 | acc: 18.75%,  total acc: 29.69%   [EVAL] batch:    4 | acc: 25.00%,  total acc: 28.75%   [EVAL] batch:    5 | acc: 18.75%,  total acc: 27.08%   [EVAL] batch:    6 | acc: 25.00%,  total acc: 26.79%   [EVAL] batch:    7 | acc: 50.00%,  total acc: 29.69%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 34.03%   [EVAL] batch:    9 | acc: 43.75%,  total acc: 35.00%   [EVAL] batch:   10 | acc: 68.75%,  total acc: 38.07%   [EVAL] batch:   11 | acc: 56.25%,  total acc: 39.58%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 41.83%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 45.54%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 47.50%   [EVAL] batch:   15 | acc: 68.75%,  total acc: 48.83%   [EVAL] batch:   16 | acc: 87.50%,  total acc: 51.10%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 52.08%   [EVAL] batch:   18 | acc: 18.75%,  total acc: 50.33%   [EVAL] batch:   19 | acc: 25.00%,  total acc: 49.06%   [EVAL] batch:   20 | acc: 31.25%,  total acc: 48.21%   [EVAL] batch:   21 | acc: 6.25%,  total acc: 46.31%   
[EVAL] batch:    0 | acc: 37.50%,  total acc: 37.50%   [EVAL] batch:    1 | acc: 62.50%,  total acc: 50.00%   [EVAL] batch:    2 | acc: 62.50%,  total acc: 54.17%   [EVAL] batch:    3 | acc: 25.00%,  total acc: 46.88%   [EVAL] batch:    4 | acc: 37.50%,  total acc: 45.00%   [EVAL] batch:    5 | acc: 43.75%,  total acc: 44.79%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 49.11%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 52.34%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 56.25%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 58.75%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 61.36%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 63.54%   [EVAL] batch:   12 | acc: 56.25%,  total acc: 62.98%   [EVAL] batch:   13 | acc: 31.25%,  total acc: 60.71%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 61.67%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 61.33%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 62.13%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 62.15%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 62.50%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 63.75%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 65.48%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 67.05%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 68.48%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 69.79%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 71.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 72.12%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 72.92%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 73.88%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 74.78%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 75.00%   [EVAL] batch:   30 | acc: 81.25%,  total acc: 75.20%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 75.78%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 76.33%   [EVAL] batch:   33 | acc: 31.25%,  total acc: 75.00%   [EVAL] batch:   34 | acc: 18.75%,  total acc: 73.39%   [EVAL] batch:   35 | acc: 0.00%,  total acc: 71.35%   [EVAL] batch:   36 | acc: 0.00%,  total acc: 69.43%   [EVAL] batch:   37 | acc: 6.25%,  total acc: 67.76%   [EVAL] batch:   38 | acc: 18.75%,  total acc: 66.51%   [EVAL] batch:   39 | acc: 81.25%,  total acc: 66.88%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 67.53%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 68.15%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 68.75%   [EVAL] batch:   43 | acc: 75.00%,  total acc: 68.89%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 69.31%   [EVAL] batch:   45 | acc: 25.00%,  total acc: 68.34%   [EVAL] batch:   46 | acc: 75.00%,  total acc: 68.48%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 69.14%   [EVAL] batch:   48 | acc: 31.25%,  total acc: 68.37%   [EVAL] batch:   49 | acc: 18.75%,  total acc: 67.38%   [EVAL] batch:   50 | acc: 18.75%,  total acc: 66.42%   [EVAL] batch:   51 | acc: 43.75%,  total acc: 65.99%   [EVAL] batch:   52 | acc: 12.50%,  total acc: 64.98%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 65.28%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 65.80%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 66.29%   [EVAL] batch:   56 | acc: 87.50%,  total acc: 66.67%   [EVAL] batch:   57 | acc: 18.75%,  total acc: 65.84%   [EVAL] batch:   58 | acc: 18.75%,  total acc: 65.04%   [EVAL] batch:   59 | acc: 25.00%,  total acc: 64.38%   [EVAL] batch:   60 | acc: 43.75%,  total acc: 64.04%   [EVAL] batch:   61 | acc: 37.50%,  total acc: 63.61%   [EVAL] batch:   62 | acc: 25.00%,  total acc: 63.00%   [EVAL] batch:   63 | acc: 68.75%,  total acc: 63.09%   [EVAL] batch:   64 | acc: 93.75%,  total acc: 63.56%   [EVAL] batch:   65 | acc: 81.25%,  total acc: 63.83%   [EVAL] batch:   66 | acc: 43.75%,  total acc: 63.53%   [EVAL] batch:   67 | acc: 31.25%,  total acc: 63.05%   [EVAL] batch:   68 | acc: 12.50%,  total acc: 62.32%   [EVAL] batch:   69 | acc: 18.75%,  total acc: 61.70%   [EVAL] batch:   70 | acc: 25.00%,  total acc: 61.18%   [EVAL] batch:   71 | acc: 18.75%,  total acc: 60.59%   [EVAL] batch:   72 | acc: 37.50%,  total acc: 60.27%   [EVAL] batch:   73 | acc: 56.25%,  total acc: 60.22%   [EVAL] batch:   74 | acc: 62.50%,  total acc: 60.25%   [EVAL] batch:   75 | acc: 50.00%,  total acc: 60.12%   [EVAL] batch:   76 | acc: 62.50%,  total acc: 60.15%   [EVAL] batch:   77 | acc: 62.50%,  total acc: 60.18%   [EVAL] batch:   78 | acc: 68.75%,  total acc: 60.28%   [EVAL] batch:   79 | acc: 100.00%,  total acc: 60.78%   [EVAL] batch:   80 | acc: 75.00%,  total acc: 60.96%   [EVAL] batch:   81 | acc: 68.75%,  total acc: 61.05%   [EVAL] batch:   82 | acc: 81.25%,  total acc: 61.30%   [EVAL] batch:   83 | acc: 50.00%,  total acc: 61.16%   [EVAL] batch:   84 | acc: 18.75%,  total acc: 60.66%   [EVAL] batch:   85 | acc: 31.25%,  total acc: 60.32%   [EVAL] batch:   86 | acc: 25.00%,  total acc: 59.91%   [EVAL] batch:   87 | acc: 6.25%,  total acc: 59.30%   
cur_acc:  ['0.8561', '0.8616', '0.3047', '0.6538', '0.4631']
his_acc:  ['0.8561', '0.8431', '0.7303', '0.6629', '0.5930']
CurrentTrain: epoch  0, batch     0 | loss: 5.1520519CurrentTrain: epoch  0, batch     1 | loss: 5.1165638CurrentTrain: epoch  1, batch     0 | loss: 4.1095042CurrentTrain: epoch  1, batch     1 | loss: 3.5131214CurrentTrain: epoch  2, batch     0 | loss: 3.7783766CurrentTrain: epoch  2, batch     1 | loss: 2.9814351CurrentTrain: epoch  3, batch     0 | loss: 3.2010984CurrentTrain: epoch  3, batch     1 | loss: 3.0038991CurrentTrain: epoch  4, batch     0 | loss: 2.9729674CurrentTrain: epoch  4, batch     1 | loss: 2.8505809CurrentTrain: epoch  5, batch     0 | loss: 2.8418930CurrentTrain: epoch  5, batch     1 | loss: 2.4357369CurrentTrain: epoch  6, batch     0 | loss: 2.8482904CurrentTrain: epoch  6, batch     1 | loss: 2.2079473CurrentTrain: epoch  7, batch     0 | loss: 2.3662362CurrentTrain: epoch  7, batch     1 | loss: 2.3618572CurrentTrain: epoch  8, batch     0 | loss: 2.2931328CurrentTrain: epoch  8, batch     1 | loss: 2.2116463CurrentTrain: epoch  9, batch     0 | loss: 2.0920811CurrentTrain: epoch  9, batch     1 | loss: 2.0454857
#### organization founded ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization founded" means an organization was found in a date.
Here is an example:
Relation: organization founded
Context: the company was founded as a hobby in 1979 by the husband and wife team of tim and nina zagat , a pair of lawyers who started recuiting friends to provide their own ratings of the food , decor and service of restaurants they frequented .
Head Entity: zagat
Tail Entity: 1979
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization founded":
Relation: organization founded  
Context: In 1995, the tech startup was established by a group of engineers who aimed to revolutionize the software industry with innovative solutions.  
Head Entity: tech startup  
Tail Entity: 1995  

Relation: organization founded  
Context: The nonprofit organization was created in 2001 to support local artists and promote cultural events in the community.  
Head Entity: nonprofit organization  
Tail Entity: 2001  
#### person age ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person age" means a person 's age is a number.
Here is an example:
Relation: person age
Context: virginia republican jo ann davis passed away on saturday at the age of 57 .
Head Entity: jo ann davis
Tail Entity: 57
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person age":
Relation: person age  
Context: the famous actor robert downey jr. celebrated his 56th birthday last week.  
Head Entity: robert downey jr.  
Tail Entity: 56  

Relation: person age  
Context: on her 30th birthday, emily decided to throw a big party with all her friends.  
Head Entity: emily  
Tail Entity: 30  
#### person city of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person city of birth" means a person was born in a city.
Here is an example:
Relation: person city of birth
Context: rothman was born in san francisco in 1932 in an orthodox jewish family .
Head Entity: rothman
Tail Entity: san francisco
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person city of birth":
Relation: person city of birth  
Context: after spending his early years in new york, he moved to los angeles where he began his career.  
Head Entity: he  
Tail Entity: los angeles  

Relation: person city of birth  
Context: the famous author was born in a small town near boston, which greatly influenced his writing.  
Head Entity: the famous author  
Tail Entity: boston  
#### organization members ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization members" means an organization 's member is an organization.
Here is an example:
Relation: organization members
Context: sun plays for the grand rapids flight of the international basketball league after toiling for the maryland nighthawks of the american basketball association , both development leagues for those who dream of an nba career .
Head Entity: american basketball association
Tail Entity: maryland nighthawks
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization members":
Relation: organization members  
Context: The New York Philharmonic is one of the oldest orchestras in the United States, and it has had many notable musicians as members, including the famous conductor Leonard Bernstein.  
Head Entity: New York Philharmonic  
Tail Entity: Leonard Bernstein  

Relation: organization members  
Context: The National Football League has a long history of legendary players, and one of its most famous members is Joe Montana, who led the San Francisco 49ers to four Super Bowl victories.  
Head Entity: National Football League  
Tail Entity: Joe Montana  
#### person religion ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person religion" means an person is the member of a religion.
Here is an example:
Relation: person religion
Context: the pope defended his action on the grounds that he could not refuse an audience to a head of state from a country with a strong catholic tradition unless he had clear-cut proof of the allegations against him .
Head Entity: he
Tail Entity: catholic
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person religion":
Relation: person religion  
Context: After years of study and reflection, Maria decided to embrace Buddhism, finding peace and purpose in its teachings.  
Head Entity: Maria  
Tail Entity: Buddhism  

Relation: person religion  
Context: During the festival, Ahmed proudly wore his traditional attire, celebrating his deep connection to Islam and its rich cultural heritage.  
Head Entity: Ahmed  
Tail Entity: Islam  
Mixup data size:  225
MixupTrain:  epoch  0, batch     0 | loss: 2.9822656MixupTrain:  epoch  0, batch     1 | loss: 3.0175757MixupTrain:  epoch  0, batch     2 | loss: 3.6381614MixupTrain:  epoch  0, batch     3 | loss: 2.7806787MixupTrain:  epoch  0, batch     4 | loss: 3.4170132MixupTrain:  epoch  0, batch     5 | loss: 2.9727640MixupTrain:  epoch  0, batch     6 | loss: 2.5684876MixupTrain:  epoch  0, batch     7 | loss: 2.7003313MixupTrain:  epoch  0, batch     8 | loss: 3.0050003MixupTrain:  epoch  0, batch     9 | loss: 2.7235932MixupTrain:  epoch  0, batch    10 | loss: 3.1660390MixupTrain:  epoch  0, batch    11 | loss: 2.9681670MixupTrain:  epoch  0, batch    12 | loss: 2.7486488MixupTrain:  epoch  0, batch    13 | loss: 2.7662969MixupTrain:  epoch  0, batch    14 | loss: 2.6606278
MemoryTrain:  epoch  0, batch     0 | loss: 2.9101486MemoryTrain:  epoch  0, batch     1 | loss: 2.6548781MemoryTrain:  epoch  0, batch     2 | loss: 3.0131984MemoryTrain:  epoch  0, batch     3 | loss: 3.0358908MemoryTrain:  epoch  0, batch     4 | loss: 3.2095668MemoryTrain:  epoch  0, batch     5 | loss: 2.5916531MemoryTrain:  epoch  1, batch     0 | loss: 2.5098572MemoryTrain:  epoch  1, batch     1 | loss: 2.9732141MemoryTrain:  epoch  1, batch     2 | loss: 3.2215111MemoryTrain:  epoch  1, batch     3 | loss: 3.0369329MemoryTrain:  epoch  1, batch     4 | loss: 2.0574374MemoryTrain:  epoch  1, batch     5 | loss: 2.6683099MemoryTrain:  epoch  2, batch     0 | loss: 2.4150405MemoryTrain:  epoch  2, batch     1 | loss: 2.8077354MemoryTrain:  epoch  2, batch     2 | loss: 2.4586267MemoryTrain:  epoch  2, batch     3 | loss: 2.1337371MemoryTrain:  epoch  2, batch     4 | loss: 2.1214900MemoryTrain:  epoch  2, batch     5 | loss: 2.1458232MemoryTrain:  epoch  3, batch     0 | loss: 2.2893376MemoryTrain:  epoch  3, batch     1 | loss: 1.9485053MemoryTrain:  epoch  3, batch     2 | loss: 2.4323163MemoryTrain:  epoch  3, batch     3 | loss: 1.7620165MemoryTrain:  epoch  3, batch     4 | loss: 1.9382095MemoryTrain:  epoch  3, batch     5 | loss: 2.0923994MemoryTrain:  epoch  4, batch     0 | loss: 1.9824551MemoryTrain:  epoch  4, batch     1 | loss: 1.8874434MemoryTrain:  epoch  4, batch     2 | loss: 2.1081774MemoryTrain:  epoch  4, batch     3 | loss: 1.7068102MemoryTrain:  epoch  4, batch     4 | loss: 2.1338015MemoryTrain:  epoch  4, batch     5 | loss: 2.4066224MemoryTrain:  epoch  5, batch     0 | loss: 1.8861295MemoryTrain:  epoch  5, batch     1 | loss: 1.9078938MemoryTrain:  epoch  5, batch     2 | loss: 1.8185456MemoryTrain:  epoch  5, batch     3 | loss: 1.8019043MemoryTrain:  epoch  5, batch     4 | loss: 1.3910592MemoryTrain:  epoch  5, batch     5 | loss: 2.2912357MemoryTrain:  epoch  6, batch     0 | loss: 1.4639876MemoryTrain:  epoch  6, batch     1 | loss: 1.7081788MemoryTrain:  epoch  6, batch     2 | loss: 1.5568244MemoryTrain:  epoch  6, batch     3 | loss: 1.8185165MemoryTrain:  epoch  6, batch     4 | loss: 2.3292403MemoryTrain:  epoch  6, batch     5 | loss: 1.6372381MemoryTrain:  epoch  7, batch     0 | loss: 2.2527099MemoryTrain:  epoch  7, batch     1 | loss: 1.4671049MemoryTrain:  epoch  7, batch     2 | loss: 1.4874010MemoryTrain:  epoch  7, batch     3 | loss: 1.4579985MemoryTrain:  epoch  7, batch     4 | loss: 1.5931430MemoryTrain:  epoch  7, batch     5 | loss: 2.0377080MemoryTrain:  epoch  8, batch     0 | loss: 1.5905094MemoryTrain:  epoch  8, batch     1 | loss: 2.0669460MemoryTrain:  epoch  8, batch     2 | loss: 1.5791231MemoryTrain:  epoch  8, batch     3 | loss: 1.3081758MemoryTrain:  epoch  8, batch     4 | loss: 1.4876127MemoryTrain:  epoch  8, batch     5 | loss: 1.9322159MemoryTrain:  epoch  9, batch     0 | loss: 1.3468640MemoryTrain:  epoch  9, batch     1 | loss: 1.3942201MemoryTrain:  epoch  9, batch     2 | loss: 1.6699561MemoryTrain:  epoch  9, batch     3 | loss: 1.6460235MemoryTrain:  epoch  9, batch     4 | loss: 1.4121075MemoryTrain:  epoch  9, batch     5 | loss: 2.0214951
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 90.62%   [EVAL] batch:    2 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:    3 | acc: 100.00%,  total acc: 95.31%   [EVAL] batch:    4 | acc: 100.00%,  total acc: 96.25%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 96.88%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 97.32%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 97.66%   [EVAL] batch:    8 | acc: 62.50%,  total acc: 93.75%   [EVAL] batch:    9 | acc: 6.25%,  total acc: 85.00%   [EVAL] batch:   10 | acc: 25.00%,  total acc: 79.55%   [EVAL] batch:   11 | acc: 62.50%,  total acc: 78.12%   [EVAL] batch:   12 | acc: 62.50%,  total acc: 76.92%   [EVAL] batch:   13 | acc: 56.25%,  total acc: 75.45%   
[EVAL] batch:    0 | acc: 50.00%,  total acc: 50.00%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 65.62%   [EVAL] batch:    2 | acc: 62.50%,  total acc: 64.58%   [EVAL] batch:    3 | acc: 62.50%,  total acc: 64.06%   [EVAL] batch:    4 | acc: 50.00%,  total acc: 61.25%   [EVAL] batch:    5 | acc: 50.00%,  total acc: 59.38%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 63.39%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 65.62%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 68.06%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 69.38%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 71.59%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 72.92%   [EVAL] batch:   12 | acc: 56.25%,  total acc: 71.63%   [EVAL] batch:   13 | acc: 31.25%,  total acc: 68.75%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 67.97%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 68.38%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 68.06%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 68.09%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 68.75%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 70.24%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 71.59%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 72.83%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 73.96%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 75.96%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 76.62%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 77.46%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 78.02%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 78.12%   [EVAL] batch:   30 | acc: 81.25%,  total acc: 78.23%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 78.71%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 79.17%   [EVAL] batch:   33 | acc: 25.00%,  total acc: 77.57%   [EVAL] batch:   34 | acc: 12.50%,  total acc: 75.71%   [EVAL] batch:   35 | acc: 0.00%,  total acc: 73.61%   [EVAL] batch:   36 | acc: 0.00%,  total acc: 71.62%   [EVAL] batch:   37 | acc: 6.25%,  total acc: 69.90%   [EVAL] batch:   38 | acc: 18.75%,  total acc: 68.59%   [EVAL] batch:   39 | acc: 81.25%,  total acc: 68.91%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 69.51%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 70.09%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 70.64%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 70.88%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 71.39%   [EVAL] batch:   45 | acc: 25.00%,  total acc: 70.38%   [EVAL] batch:   46 | acc: 62.50%,  total acc: 70.21%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 70.83%   [EVAL] batch:   48 | acc: 37.50%,  total acc: 70.15%   [EVAL] batch:   49 | acc: 31.25%,  total acc: 69.38%   [EVAL] batch:   50 | acc: 18.75%,  total acc: 68.38%   [EVAL] batch:   51 | acc: 43.75%,  total acc: 67.91%   [EVAL] batch:   52 | acc: 12.50%,  total acc: 66.86%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 67.13%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 67.61%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 68.08%   [EVAL] batch:   56 | acc: 81.25%,  total acc: 68.31%   [EVAL] batch:   57 | acc: 25.00%,  total acc: 67.56%   [EVAL] batch:   58 | acc: 50.00%,  total acc: 67.27%   [EVAL] batch:   59 | acc: 56.25%,  total acc: 67.08%   [EVAL] batch:   60 | acc: 62.50%,  total acc: 67.01%   [EVAL] batch:   61 | acc: 43.75%,  total acc: 66.63%   [EVAL] batch:   62 | acc: 43.75%,  total acc: 66.27%   [EVAL] batch:   63 | acc: 68.75%,  total acc: 66.31%   [EVAL] batch:   64 | acc: 62.50%,  total acc: 66.25%   [EVAL] batch:   65 | acc: 56.25%,  total acc: 66.10%   [EVAL] batch:   66 | acc: 25.00%,  total acc: 65.49%   [EVAL] batch:   67 | acc: 18.75%,  total acc: 64.80%   [EVAL] batch:   68 | acc: 25.00%,  total acc: 64.22%   [EVAL] batch:   69 | acc: 56.25%,  total acc: 64.11%   [EVAL] batch:   70 | acc: 43.75%,  total acc: 63.82%   [EVAL] batch:   71 | acc: 62.50%,  total acc: 63.80%   [EVAL] batch:   72 | acc: 43.75%,  total acc: 63.53%   [EVAL] batch:   73 | acc: 50.00%,  total acc: 63.34%   [EVAL] batch:   74 | acc: 50.00%,  total acc: 63.17%   [EVAL] batch:   75 | acc: 37.50%,  total acc: 62.83%   [EVAL] batch:   76 | acc: 62.50%,  total acc: 62.82%   [EVAL] batch:   77 | acc: 56.25%,  total acc: 62.74%   [EVAL] batch:   78 | acc: 75.00%,  total acc: 62.90%   [EVAL] batch:   79 | acc: 100.00%,  total acc: 63.36%   [EVAL] batch:   80 | acc: 87.50%,  total acc: 63.66%   [EVAL] batch:   81 | acc: 100.00%,  total acc: 64.10%   [EVAL] batch:   82 | acc: 87.50%,  total acc: 64.38%   [EVAL] batch:   83 | acc: 68.75%,  total acc: 64.43%   [EVAL] batch:   84 | acc: 18.75%,  total acc: 63.90%   [EVAL] batch:   85 | acc: 50.00%,  total acc: 63.74%   [EVAL] batch:   86 | acc: 31.25%,  total acc: 63.36%   [EVAL] batch:   87 | acc: 81.25%,  total acc: 63.57%   [EVAL] batch:   88 | acc: 93.75%,  total acc: 63.90%   [EVAL] batch:   89 | acc: 93.75%,  total acc: 64.24%   [EVAL] batch:   90 | acc: 100.00%,  total acc: 64.63%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 65.01%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 65.39%   [EVAL] batch:   93 | acc: 100.00%,  total acc: 65.76%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 66.12%   [EVAL] batch:   95 | acc: 81.25%,  total acc: 66.28%   [EVAL] batch:   96 | acc: 0.00%,  total acc: 65.59%   [EVAL] batch:   97 | acc: 12.50%,  total acc: 65.05%   [EVAL] batch:   98 | acc: 68.75%,  total acc: 65.09%   [EVAL] batch:   99 | acc: 62.50%,  total acc: 65.06%   [EVAL] batch:  100 | acc: 68.75%,  total acc: 65.10%   
cur_acc:  ['0.8561', '0.8616', '0.3047', '0.6538', '0.4631', '0.7545']
his_acc:  ['0.8561', '0.8431', '0.7303', '0.6629', '0.5930', '0.6510']
CurrentTrain: epoch  0, batch     0 | loss: 6.2949142CurrentTrain: epoch  0, batch     1 | loss: 5.7523766CurrentTrain: epoch  1, batch     0 | loss: 5.1279688CurrentTrain: epoch  1, batch     1 | loss: 5.1930866CurrentTrain: epoch  2, batch     0 | loss: 5.0671659CurrentTrain: epoch  2, batch     1 | loss: 4.1432428CurrentTrain: epoch  3, batch     0 | loss: 4.4497519CurrentTrain: epoch  3, batch     1 | loss: 4.8787150CurrentTrain: epoch  4, batch     0 | loss: 4.6750498CurrentTrain: epoch  4, batch     1 | loss: 4.0369511CurrentTrain: epoch  5, batch     0 | loss: 4.2466297CurrentTrain: epoch  5, batch     1 | loss: 4.6761551CurrentTrain: epoch  6, batch     0 | loss: 4.0101738CurrentTrain: epoch  6, batch     1 | loss: 3.7290294CurrentTrain: epoch  7, batch     0 | loss: 3.7904689CurrentTrain: epoch  7, batch     1 | loss: 3.6989052CurrentTrain: epoch  8, batch     0 | loss: 3.5559776CurrentTrain: epoch  8, batch     1 | loss: 3.7006328CurrentTrain: epoch  9, batch     0 | loss: 3.3382542CurrentTrain: epoch  9, batch     1 | loss: 3.2902124
#### person stateorprovinces of residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person stateorprovinces of residence" means a person is living in a state or province.
Here is an example:
Relation: person stateorprovinces of residence
Context: the chairman of the senate foreign relations committee , massachusetts democrat john kerry , and the panel 's top republican , richard lugar of indiana , were at the white house meeting , which was led by vice president joe biden , a former chairman of the foreign relations panel .
Head Entity: john kerry
Tail Entity: massachusetts
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person stateorprovinces of residence":
Relation: person stateorprovinces of residence  
Context: After moving from New York to California, actress Emma Stone quickly adapted to the vibrant lifestyle of Los Angeles, where she now resides.  
Head Entity: Emma Stone  
Tail Entity: California  

Relation: person stateorprovinces of residence  
Context: Following his successful career in the tech industry, Mark Zuckerberg decided to settle down in Palo Alto, a city known for its innovation and proximity to Silicon Valley.  
Head Entity: Mark Zuckerberg  
Tail Entity: Palo Alto  
#### person date of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person date of death" means a person was died on a date.
Here is an example:
Relation: person date of death
Context: he was taken off life support on feb. 14 .
Head Entity: he
Tail Entity: feb. 14
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person date of death":
Relation: person date of death  
Context: The renowned author passed away on March 5, 2020, leaving behind a legacy of great literature.  
Head Entity: The renowned author  
Tail Entity: March 5, 2020  

Relation: person date of death  
Context: She succumbed to her illness on July 22, 2018, after a long battle.  
Head Entity: She  
Tail Entity: July 22, 2018  
#### organization number of employees members ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization number of employees members" means the number of an organization 's employees is a number.
Here is an example:
Relation: organization number of employees members
Context: covidien , which posted revenue of more than $ 10 billion last year , has about 42,000 employees worldwide .
Head Entity: covidien
Tail Entity: 42,000
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization number of employees members":
Relation: organization number of employees members  
Context: TechCorp, a leading software development company, boasts a workforce of approximately 5,500 employees across its global offices.  
Head Entity: TechCorp  
Tail Entity: 5,500  

Relation: organization number of employees members  
Context: GreenEarth Nonprofit has grown significantly over the past few years and currently employs around 1,200 dedicated staff members to support its environmental initiatives.  
Head Entity: GreenEarth Nonprofit  
Tail Entity: 1,200  
#### person alternate names ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person alternate names" means a person 's alias is a person.
Here is an example:
Relation: person alternate names
Context: `` i am known in the hospice as the man who would n't die , '' buchwald wrote in march .
Head Entity: buchwald
Tail Entity: man who would n't die
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person alternate names":
Relation: person alternate names  
Context: `` the famous author often referred to himself as the bard of Avon, '' said the literary critic during the lecture.  
Head Entity: author  
Tail Entity: bard of Avon  

Relation: person alternate names  
Context: `` in the world of music, she is affectionately known as the queen of pop, '' her fans chanted at the concert.  
Head Entity: she  
Tail Entity: queen of pop  
#### person spouse ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person spouse" means a person 's spouse is a person.
Here is an example:
Relation: person spouse
Context: smits stands at the center of this multigenerational saga as alex vega , the adopted son of rum and sugar baron pancho duque -lrb- elizondo -rrb- and his wife , amalia -lrb- moreno -rrb- .
Head Entity: elizondo
Tail Entity: moreno
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person spouse":
Relation: person spouse  
Context: in a heartfelt ceremony, john and his beloved partner, sarah, exchanged vows surrounded by family and friends, celebrating their love and commitment to each other.  
Head Entity: john  
Tail Entity: sarah  

Relation: person spouse  
Context: after years of companionship, the couple, michael and jessica, finally tied the knot in a beautiful garden wedding, marking the beginning of their new life together.  
Head Entity: michael  
Tail Entity: jessica  
Mixup data size:  254
MixupTrain:  epoch  0, batch     0 | loss: 2.2124110MixupTrain:  epoch  0, batch     1 | loss: 2.6137411MixupTrain:  epoch  0, batch     2 | loss: 2.0569747MixupTrain:  epoch  0, batch     3 | loss: 2.2388004MixupTrain:  epoch  0, batch     4 | loss: 2.3690902MixupTrain:  epoch  0, batch     5 | loss: 2.7815652MixupTrain:  epoch  0, batch     6 | loss: 2.3527569MixupTrain:  epoch  0, batch     7 | loss: 2.9224924MixupTrain:  epoch  0, batch     8 | loss: 2.4049665MixupTrain:  epoch  0, batch     9 | loss: 2.8316559MixupTrain:  epoch  0, batch    10 | loss: 1.9917537MixupTrain:  epoch  0, batch    11 | loss: 2.1636466MixupTrain:  epoch  0, batch    12 | loss: 2.3305442MixupTrain:  epoch  0, batch    13 | loss: 2.7961816MixupTrain:  epoch  0, batch    14 | loss: 2.6863384MixupTrain:  epoch  0, batch    15 | loss: 2.1646228
MemoryTrain:  epoch  0, batch     0 | loss: 2.6667390MemoryTrain:  epoch  0, batch     1 | loss: 2.5154731MemoryTrain:  epoch  0, batch     2 | loss: 2.9099183MemoryTrain:  epoch  0, batch     3 | loss: 2.1550670MemoryTrain:  epoch  0, batch     4 | loss: 1.9370815MemoryTrain:  epoch  0, batch     5 | loss: 2.6417673MemoryTrain:  epoch  0, batch     6 | loss: 2.6621943MemoryTrain:  epoch  1, batch     0 | loss: 2.9429076MemoryTrain:  epoch  1, batch     1 | loss: 2.4381492MemoryTrain:  epoch  1, batch     2 | loss: 2.5458908MemoryTrain:  epoch  1, batch     3 | loss: 2.5007234MemoryTrain:  epoch  1, batch     4 | loss: 1.8127642MemoryTrain:  epoch  1, batch     5 | loss: 1.5848892MemoryTrain:  epoch  1, batch     6 | loss: 2.6650131MemoryTrain:  epoch  2, batch     0 | loss: 2.2292497MemoryTrain:  epoch  2, batch     1 | loss: 1.9479518MemoryTrain:  epoch  2, batch     2 | loss: 2.3710749MemoryTrain:  epoch  2, batch     3 | loss: 1.6925597MemoryTrain:  epoch  2, batch     4 | loss: 2.4055402MemoryTrain:  epoch  2, batch     5 | loss: 2.3658400MemoryTrain:  epoch  2, batch     6 | loss: 1.8920457MemoryTrain:  epoch  3, batch     0 | loss: 2.0898485MemoryTrain:  epoch  3, batch     1 | loss: 1.8405087MemoryTrain:  epoch  3, batch     2 | loss: 1.6616001MemoryTrain:  epoch  3, batch     3 | loss: 1.9689698MemoryTrain:  epoch  3, batch     4 | loss: 1.9873099MemoryTrain:  epoch  3, batch     5 | loss: 1.9883051MemoryTrain:  epoch  3, batch     6 | loss: 1.3939095MemoryTrain:  epoch  4, batch     0 | loss: 1.6560556MemoryTrain:  epoch  4, batch     1 | loss: 1.8733785MemoryTrain:  epoch  4, batch     2 | loss: 1.4728436MemoryTrain:  epoch  4, batch     3 | loss: 1.5544420MemoryTrain:  epoch  4, batch     4 | loss: 1.7884655MemoryTrain:  epoch  4, batch     5 | loss: 1.8602453MemoryTrain:  epoch  4, batch     6 | loss: 1.7720098MemoryTrain:  epoch  5, batch     0 | loss: 1.6879326MemoryTrain:  epoch  5, batch     1 | loss: 1.5989112MemoryTrain:  epoch  5, batch     2 | loss: 1.4730754MemoryTrain:  epoch  5, batch     3 | loss: 1.6861596MemoryTrain:  epoch  5, batch     4 | loss: 2.0435107MemoryTrain:  epoch  5, batch     5 | loss: 1.5580090MemoryTrain:  epoch  5, batch     6 | loss: 1.5361719MemoryTrain:  epoch  6, batch     0 | loss: 1.7833300MemoryTrain:  epoch  6, batch     1 | loss: 1.5334425MemoryTrain:  epoch  6, batch     2 | loss: 1.6481466MemoryTrain:  epoch  6, batch     3 | loss: 1.7339408MemoryTrain:  epoch  6, batch     4 | loss: 1.4996164MemoryTrain:  epoch  6, batch     5 | loss: 1.3145387MemoryTrain:  epoch  6, batch     6 | loss: 1.7166919MemoryTrain:  epoch  7, batch     0 | loss: 1.8164663MemoryTrain:  epoch  7, batch     1 | loss: 1.6744167MemoryTrain:  epoch  7, batch     2 | loss: 1.5766449MemoryTrain:  epoch  7, batch     3 | loss: 1.7213198MemoryTrain:  epoch  7, batch     4 | loss: 1.4959159MemoryTrain:  epoch  7, batch     5 | loss: 1.4790988MemoryTrain:  epoch  7, batch     6 | loss: 1.2947180MemoryTrain:  epoch  8, batch     0 | loss: 1.6975162MemoryTrain:  epoch  8, batch     1 | loss: 1.5696083MemoryTrain:  epoch  8, batch     2 | loss: 1.4750766MemoryTrain:  epoch  8, batch     3 | loss: 1.2904775MemoryTrain:  epoch  8, batch     4 | loss: 1.5021218MemoryTrain:  epoch  8, batch     5 | loss: 1.5671309MemoryTrain:  epoch  8, batch     6 | loss: 1.5074911MemoryTrain:  epoch  9, batch     0 | loss: 1.4711809MemoryTrain:  epoch  9, batch     1 | loss: 1.5593026MemoryTrain:  epoch  9, batch     2 | loss: 1.5094264MemoryTrain:  epoch  9, batch     3 | loss: 1.4405661MemoryTrain:  epoch  9, batch     4 | loss: 1.3198946MemoryTrain:  epoch  9, batch     5 | loss: 1.5148575MemoryTrain:  epoch  9, batch     6 | loss: 1.2951118
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 62.50%,  total acc: 65.62%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 68.75%   [EVAL] batch:    3 | acc: 93.75%,  total acc: 75.00%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 77.50%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 79.17%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 82.64%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 81.88%   [EVAL] batch:   10 | acc: 56.25%,  total acc: 79.55%   [EVAL] batch:   11 | acc: 56.25%,  total acc: 77.60%   [EVAL] batch:   12 | acc: 43.75%,  total acc: 75.00%   [EVAL] batch:   13 | acc: 62.50%,  total acc: 74.11%   [EVAL] batch:   14 | acc: 31.25%,  total acc: 71.25%   
[EVAL] batch:    0 | acc: 56.25%,  total acc: 56.25%   [EVAL] batch:    1 | acc: 75.00%,  total acc: 65.62%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 68.75%   [EVAL] batch:    3 | acc: 56.25%,  total acc: 65.62%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 66.25%   [EVAL] batch:    5 | acc: 43.75%,  total acc: 62.50%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 64.29%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 65.62%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 68.75%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 70.00%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 71.02%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 71.88%   [EVAL] batch:   12 | acc: 37.50%,  total acc: 69.23%   [EVAL] batch:   13 | acc: 25.00%,  total acc: 66.07%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 66.25%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 65.62%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 66.18%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 65.97%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 66.12%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 66.56%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 68.15%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 69.32%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 70.65%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 71.61%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 72.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 73.80%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 74.54%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 75.45%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 76.29%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 76.46%   [EVAL] batch:   30 | acc: 81.25%,  total acc: 76.61%   [EVAL] batch:   31 | acc: 87.50%,  total acc: 76.95%   [EVAL] batch:   32 | acc: 81.25%,  total acc: 77.08%   [EVAL] batch:   33 | acc: 25.00%,  total acc: 75.55%   [EVAL] batch:   34 | acc: 12.50%,  total acc: 73.75%   [EVAL] batch:   35 | acc: 0.00%,  total acc: 71.70%   [EVAL] batch:   36 | acc: 0.00%,  total acc: 69.76%   [EVAL] batch:   37 | acc: 6.25%,  total acc: 68.09%   [EVAL] batch:   38 | acc: 18.75%,  total acc: 66.83%   [EVAL] batch:   39 | acc: 81.25%,  total acc: 67.19%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 67.99%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 68.75%   [EVAL] batch:   42 | acc: 87.50%,  total acc: 69.19%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 69.46%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 70.00%   [EVAL] batch:   45 | acc: 25.00%,  total acc: 69.02%   [EVAL] batch:   46 | acc: 56.25%,  total acc: 68.75%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 69.40%   [EVAL] batch:   48 | acc: 37.50%,  total acc: 68.75%   [EVAL] batch:   49 | acc: 31.25%,  total acc: 68.00%   [EVAL] batch:   50 | acc: 31.25%,  total acc: 67.28%   [EVAL] batch:   51 | acc: 43.75%,  total acc: 66.83%   [EVAL] batch:   52 | acc: 25.00%,  total acc: 66.04%   [EVAL] batch:   53 | acc: 75.00%,  total acc: 66.20%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 66.59%   [EVAL] batch:   55 | acc: 81.25%,  total acc: 66.85%   [EVAL] batch:   56 | acc: 81.25%,  total acc: 67.11%   [EVAL] batch:   57 | acc: 56.25%,  total acc: 66.92%   [EVAL] batch:   58 | acc: 81.25%,  total acc: 67.16%   [EVAL] batch:   59 | acc: 68.75%,  total acc: 67.19%   [EVAL] batch:   60 | acc: 56.25%,  total acc: 67.01%   [EVAL] batch:   61 | acc: 25.00%,  total acc: 66.33%   [EVAL] batch:   62 | acc: 43.75%,  total acc: 65.97%   [EVAL] batch:   63 | acc: 56.25%,  total acc: 65.82%   [EVAL] batch:   64 | acc: 50.00%,  total acc: 65.58%   [EVAL] batch:   65 | acc: 56.25%,  total acc: 65.44%   [EVAL] batch:   66 | acc: 37.50%,  total acc: 65.02%   [EVAL] batch:   67 | acc: 25.00%,  total acc: 64.43%   [EVAL] batch:   68 | acc: 25.00%,  total acc: 63.86%   [EVAL] batch:   69 | acc: 50.00%,  total acc: 63.66%   [EVAL] batch:   70 | acc: 43.75%,  total acc: 63.38%   [EVAL] batch:   71 | acc: 62.50%,  total acc: 63.37%   [EVAL] batch:   72 | acc: 50.00%,  total acc: 63.18%   [EVAL] batch:   73 | acc: 43.75%,  total acc: 62.92%   [EVAL] batch:   74 | acc: 43.75%,  total acc: 62.67%   [EVAL] batch:   75 | acc: 37.50%,  total acc: 62.34%   [EVAL] batch:   76 | acc: 56.25%,  total acc: 62.26%   [EVAL] batch:   77 | acc: 37.50%,  total acc: 61.94%   [EVAL] batch:   78 | acc: 68.75%,  total acc: 62.03%   [EVAL] batch:   79 | acc: 100.00%,  total acc: 62.50%   [EVAL] batch:   80 | acc: 81.25%,  total acc: 62.73%   [EVAL] batch:   81 | acc: 87.50%,  total acc: 63.03%   [EVAL] batch:   82 | acc: 87.50%,  total acc: 63.33%   [EVAL] batch:   83 | acc: 68.75%,  total acc: 63.39%   [EVAL] batch:   84 | acc: 12.50%,  total acc: 62.79%   [EVAL] batch:   85 | acc: 12.50%,  total acc: 62.21%   [EVAL] batch:   86 | acc: 25.00%,  total acc: 61.78%   [EVAL] batch:   87 | acc: 81.25%,  total acc: 62.00%   [EVAL] batch:   88 | acc: 87.50%,  total acc: 62.29%   [EVAL] batch:   89 | acc: 93.75%,  total acc: 62.64%   [EVAL] batch:   90 | acc: 100.00%,  total acc: 63.05%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 63.45%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 63.84%   [EVAL] batch:   93 | acc: 100.00%,  total acc: 64.23%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 64.61%   [EVAL] batch:   95 | acc: 81.25%,  total acc: 64.78%   [EVAL] batch:   96 | acc: 0.00%,  total acc: 64.11%   [EVAL] batch:   97 | acc: 0.00%,  total acc: 63.46%   [EVAL] batch:   98 | acc: 81.25%,  total acc: 63.64%   [EVAL] batch:   99 | acc: 68.75%,  total acc: 63.69%   [EVAL] batch:  100 | acc: 81.25%,  total acc: 63.86%   [EVAL] batch:  101 | acc: 62.50%,  total acc: 63.85%   [EVAL] batch:  102 | acc: 68.75%,  total acc: 63.90%   [EVAL] batch:  103 | acc: 75.00%,  total acc: 64.00%   [EVAL] batch:  104 | acc: 93.75%,  total acc: 64.29%   [EVAL] batch:  105 | acc: 87.50%,  total acc: 64.50%   [EVAL] batch:  106 | acc: 87.50%,  total acc: 64.72%   [EVAL] batch:  107 | acc: 93.75%,  total acc: 64.99%   [EVAL] batch:  108 | acc: 81.25%,  total acc: 65.14%   [EVAL] batch:  109 | acc: 93.75%,  total acc: 65.40%   [EVAL] batch:  110 | acc: 75.00%,  total acc: 65.48%   [EVAL] batch:  111 | acc: 56.25%,  total acc: 65.40%   [EVAL] batch:  112 | acc: 50.00%,  total acc: 65.27%   [EVAL] batch:  113 | acc: 43.75%,  total acc: 65.08%   [EVAL] batch:  114 | acc: 62.50%,  total acc: 65.05%   [EVAL] batch:  115 | acc: 31.25%,  total acc: 64.76%   
cur_acc:  ['0.8561', '0.8616', '0.3047', '0.6538', '0.4631', '0.7545', '0.7125']
his_acc:  ['0.8561', '0.8431', '0.7303', '0.6629', '0.5930', '0.6510', '0.6476']
CurrentTrain: epoch  0, batch     0 | loss: 5.6523452CurrentTrain: epoch  0, batch     1 | loss: 5.4955568CurrentTrain: epoch  1, batch     0 | loss: 4.5002413CurrentTrain: epoch  1, batch     1 | loss: 4.1927910CurrentTrain: epoch  2, batch     0 | loss: 3.8164935CurrentTrain: epoch  2, batch     1 | loss: 3.1319561CurrentTrain: epoch  3, batch     0 | loss: 3.3483305CurrentTrain: epoch  3, batch     1 | loss: 3.0613284CurrentTrain: epoch  4, batch     0 | loss: 3.1796584CurrentTrain: epoch  4, batch     1 | loss: 2.6761611CurrentTrain: epoch  5, batch     0 | loss: 2.6545248CurrentTrain: epoch  5, batch     1 | loss: 2.3289270CurrentTrain: epoch  6, batch     0 | loss: 2.4660506CurrentTrain: epoch  6, batch     1 | loss: 2.2557225CurrentTrain: epoch  7, batch     0 | loss: 2.2018037CurrentTrain: epoch  7, batch     1 | loss: 2.2175653CurrentTrain: epoch  8, batch     0 | loss: 2.3282278CurrentTrain: epoch  8, batch     1 | loss: 2.3045366CurrentTrain: epoch  9, batch     0 | loss: 2.3587704CurrentTrain: epoch  9, batch     1 | loss: 2.1332142
#### person cities of residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person cities of residence" means a person is living in a city.
Here is an example:
Relation: person cities of residence
Context: -lrb- now there are two : andre carson , a democrat from indianapolis , was elected to the house last year . -rrb-
Head Entity: andre carson
Tail Entity: indianapolis
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person cities of residence":
Relation: person cities of residence  
Context: -lrb- after moving to the west coast, sarah jones settled in san francisco, where she found a job in tech. -rrb-  
Head Entity: sarah jones  
Tail Entity: san francisco  

Relation: person cities of residence  
Context: -lrb- during his college years, michael smith lived in boston, enjoying the vibrant culture and history of the city. -rrb-  
Head Entity: michael smith  
Tail Entity: boston  
#### person schools attended ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person schools attended" means a person 's school is an organization.
Here is an example:
Relation: person schools attended
Context: born in baltimore in 1922 , parren mitchell was a graduate of morgan state college and earned a master 's degree from the university of maryland , according to biographical information supplied by cummings ' office .
Head Entity: parren mitchell
Tail Entity: university of maryland
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person schools attended":
Relation: person schools attended  
Context: after completing high school in 1995, jessica went on to study at the university of california, los angeles, where she earned her bachelor's degree in sociology.  
Head Entity: jessica  
Tail Entity: university of california, los angeles  

Relation: person schools attended  
Context: during his early years, steven attended the prestigious harvard university, where he majored in computer science and graduated with honors.  
Head Entity: steven  
Tail Entity: harvard university  
#### person country of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person country of death" means a person was died in a country.
Here is an example:
Relation: person country of death
Context: us republican congresswoman jo ann davis dies after fight with breast cancer
Head Entity: jo ann davis
Tail Entity: us
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person country of death":
Relation: person country of death  
Context: renowned physicist stephen hawking passed away in cambridge, england  
Head Entity: stephen hawking  
Tail Entity: england  

Relation: person country of death  
Context: famous author agatha christie died in her home in wallingford, england  
Head Entity: agatha christie  
Tail Entity: england  
#### person children ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person children" means a person 's children is a person.
Here is an example:
Relation: person children
Context: he is survived by two stepdaughters , barbara a. hammond and brenda l. stevenson ; a stepson , michael a. taylor ; two grandchildren and one great-grandchild .
Head Entity: he
Tail Entity: brenda l. stevenson
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person children":
Relation: person children  
Context: she has three children, including her son, john, and her daughters, emily and sarah, who are both pursuing their studies.  
Head Entity: she  
Tail Entity: emily  

Relation: person children  
Context: after the passing of their father, the siblings, including jake and his sister, lila, took on the responsibility of caring for their younger brother, max.  
Head Entity: jake  
Tail Entity: lila  
#### person charges ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person charges" means a person was charged with an event.
Here is an example:
Relation: person charges
Context: flowers always contended politics was behind the extortion investigation , but appeals courts ruled against him .
Head Entity: him
Tail Entity: extortion
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person charges":
Relation: person charges  
Context: The prosecutor announced that the former mayor was facing serious allegations related to corruption and bribery.  
Head Entity: former mayor  
Tail Entity: corruption  

Relation: person charges  
Context: After a lengthy investigation, the authorities confirmed that the celebrity was implicated in a high-profile fraud case.  
Head Entity: celebrity  
Tail Entity: fraud case  
Mixup data size:  284
MixupTrain:  epoch  0, batch     0 | loss: 1.8457840MixupTrain:  epoch  0, batch     1 | loss: 2.1896523MixupTrain:  epoch  0, batch     2 | loss: 2.1581677MixupTrain:  epoch  0, batch     3 | loss: 2.4250007MixupTrain:  epoch  0, batch     4 | loss: 2.2141004MixupTrain:  epoch  0, batch     5 | loss: 2.2598942MixupTrain:  epoch  0, batch     6 | loss: 1.9334744MixupTrain:  epoch  0, batch     7 | loss: 2.2552197MixupTrain:  epoch  0, batch     8 | loss: 2.0006116MixupTrain:  epoch  0, batch     9 | loss: 1.9764335MixupTrain:  epoch  0, batch    10 | loss: 2.1777369MixupTrain:  epoch  0, batch    11 | loss: 1.9229466MixupTrain:  epoch  0, batch    12 | loss: 2.0970365MixupTrain:  epoch  0, batch    13 | loss: 2.0848030MixupTrain:  epoch  0, batch    14 | loss: 1.9623442MixupTrain:  epoch  0, batch    15 | loss: 2.6810716MixupTrain:  epoch  0, batch    16 | loss: 2.3547640MixupTrain:  epoch  0, batch    17 | loss: 1.6332956
MemoryTrain:  epoch  0, batch     0 | loss: 2.7829335MemoryTrain:  epoch  0, batch     1 | loss: 1.7229967MemoryTrain:  epoch  0, batch     2 | loss: 2.0942173MemoryTrain:  epoch  0, batch     3 | loss: 1.7239251MemoryTrain:  epoch  0, batch     4 | loss: 1.8624077MemoryTrain:  epoch  0, batch     5 | loss: 2.1581476MemoryTrain:  epoch  0, batch     6 | loss: 2.1476889MemoryTrain:  epoch  0, batch     7 | loss: 2.5418053MemoryTrain:  epoch  1, batch     0 | loss: 2.3231921MemoryTrain:  epoch  1, batch     1 | loss: 1.9343740MemoryTrain:  epoch  1, batch     2 | loss: 2.2390082MemoryTrain:  epoch  1, batch     3 | loss: 1.5682862MemoryTrain:  epoch  1, batch     4 | loss: 1.9999092MemoryTrain:  epoch  1, batch     5 | loss: 1.8343742MemoryTrain:  epoch  1, batch     6 | loss: 1.8019606MemoryTrain:  epoch  1, batch     7 | loss: 1.7421093MemoryTrain:  epoch  2, batch     0 | loss: 1.9730064MemoryTrain:  epoch  2, batch     1 | loss: 1.9019725MemoryTrain:  epoch  2, batch     2 | loss: 1.4855088MemoryTrain:  epoch  2, batch     3 | loss: 1.6295443MemoryTrain:  epoch  2, batch     4 | loss: 1.5568857MemoryTrain:  epoch  2, batch     5 | loss: 1.7569451MemoryTrain:  epoch  2, batch     6 | loss: 1.9197366MemoryTrain:  epoch  2, batch     7 | loss: 1.5098479MemoryTrain:  epoch  3, batch     0 | loss: 1.3819485MemoryTrain:  epoch  3, batch     1 | loss: 1.4615792MemoryTrain:  epoch  3, batch     2 | loss: 1.4751831MemoryTrain:  epoch  3, batch     3 | loss: 1.7775304MemoryTrain:  epoch  3, batch     4 | loss: 1.9155651MemoryTrain:  epoch  3, batch     5 | loss: 1.5049690MemoryTrain:  epoch  3, batch     6 | loss: 1.8068810MemoryTrain:  epoch  3, batch     7 | loss: 1.3901249MemoryTrain:  epoch  4, batch     0 | loss: 1.6588470MemoryTrain:  epoch  4, batch     1 | loss: 1.3798809MemoryTrain:  epoch  4, batch     2 | loss: 1.8181096MemoryTrain:  epoch  4, batch     3 | loss: 1.3081946MemoryTrain:  epoch  4, batch     4 | loss: 1.3328357MemoryTrain:  epoch  4, batch     5 | loss: 1.7117207MemoryTrain:  epoch  4, batch     6 | loss: 1.6856853MemoryTrain:  epoch  4, batch     7 | loss: 1.2906357MemoryTrain:  epoch  5, batch     0 | loss: 1.4304726MemoryTrain:  epoch  5, batch     1 | loss: 1.3932258MemoryTrain:  epoch  5, batch     2 | loss: 1.3834691MemoryTrain:  epoch  5, batch     3 | loss: 1.3588504MemoryTrain:  epoch  5, batch     4 | loss: 1.6802666MemoryTrain:  epoch  5, batch     5 | loss: 1.5364761MemoryTrain:  epoch  5, batch     6 | loss: 1.3372012MemoryTrain:  epoch  5, batch     7 | loss: 1.6915922MemoryTrain:  epoch  6, batch     0 | loss: 1.4838120MemoryTrain:  epoch  6, batch     1 | loss: 1.3182666MemoryTrain:  epoch  6, batch     2 | loss: 1.5403266MemoryTrain:  epoch  6, batch     3 | loss: 1.3448124MemoryTrain:  epoch  6, batch     4 | loss: 1.3760554MemoryTrain:  epoch  6, batch     5 | loss: 1.4226069MemoryTrain:  epoch  6, batch     6 | loss: 1.3547106MemoryTrain:  epoch  6, batch     7 | loss: 1.4069518MemoryTrain:  epoch  7, batch     0 | loss: 1.4404509MemoryTrain:  epoch  7, batch     1 | loss: 1.3217266MemoryTrain:  epoch  7, batch     2 | loss: 1.2800908MemoryTrain:  epoch  7, batch     3 | loss: 1.2760284MemoryTrain:  epoch  7, batch     4 | loss: 1.4815066MemoryTrain:  epoch  7, batch     5 | loss: 1.3170507MemoryTrain:  epoch  7, batch     6 | loss: 1.3161931MemoryTrain:  epoch  7, batch     7 | loss: 1.4323578MemoryTrain:  epoch  8, batch     0 | loss: 1.4240270MemoryTrain:  epoch  8, batch     1 | loss: 1.3232628MemoryTrain:  epoch  8, batch     2 | loss: 1.2764779MemoryTrain:  epoch  8, batch     3 | loss: 1.3602681MemoryTrain:  epoch  8, batch     4 | loss: 1.3502775MemoryTrain:  epoch  8, batch     5 | loss: 1.3789110MemoryTrain:  epoch  8, batch     6 | loss: 1.3229446MemoryTrain:  epoch  8, batch     7 | loss: 1.3974198MemoryTrain:  epoch  9, batch     0 | loss: 1.3349812MemoryTrain:  epoch  9, batch     1 | loss: 1.3111343MemoryTrain:  epoch  9, batch     2 | loss: 1.3523197MemoryTrain:  epoch  9, batch     3 | loss: 1.4358530MemoryTrain:  epoch  9, batch     4 | loss: 1.3466854MemoryTrain:  epoch  9, batch     5 | loss: 1.3348849MemoryTrain:  epoch  9, batch     6 | loss: 1.2953719MemoryTrain:  epoch  9, batch     7 | loss: 1.3267193
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    3 | acc: 93.75%,  total acc: 79.69%   [EVAL] batch:    4 | acc: 50.00%,  total acc: 73.75%   [EVAL] batch:    5 | acc: 68.75%,  total acc: 72.92%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 75.89%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 77.34%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 78.47%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 78.12%   [EVAL] batch:   10 | acc: 56.25%,  total acc: 76.14%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 78.12%   [EVAL] batch:   12 | acc: 100.00%,  total acc: 79.81%   [EVAL] batch:   13 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:   14 | acc: 100.00%,  total acc: 82.50%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 83.59%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 84.56%   [EVAL] batch:   17 | acc: 25.00%,  total acc: 81.25%   
[EVAL] batch:    0 | acc: 50.00%,  total acc: 50.00%   [EVAL] batch:    1 | acc: 56.25%,  total acc: 53.12%   [EVAL] batch:    2 | acc: 68.75%,  total acc: 58.33%   [EVAL] batch:    3 | acc: 31.25%,  total acc: 51.56%   [EVAL] batch:    4 | acc: 43.75%,  total acc: 50.00%   [EVAL] batch:    5 | acc: 50.00%,  total acc: 50.00%   [EVAL] batch:    6 | acc: 56.25%,  total acc: 50.89%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 53.91%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 58.33%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 60.62%   [EVAL] batch:   10 | acc: 75.00%,  total acc: 61.93%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 63.54%   [EVAL] batch:   12 | acc: 31.25%,  total acc: 61.06%   [EVAL] batch:   13 | acc: 25.00%,  total acc: 58.48%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 59.17%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 58.98%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 59.93%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 60.07%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 60.20%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 61.25%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 63.10%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 64.77%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 66.30%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 67.45%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 68.75%   [EVAL] batch:   25 | acc: 93.75%,  total acc: 69.71%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 70.60%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 71.65%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 72.63%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 72.92%   [EVAL] batch:   30 | acc: 81.25%,  total acc: 73.19%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 73.83%   [EVAL] batch:   32 | acc: 87.50%,  total acc: 74.24%   [EVAL] batch:   33 | acc: 25.00%,  total acc: 72.79%   [EVAL] batch:   34 | acc: 6.25%,  total acc: 70.89%   [EVAL] batch:   35 | acc: 0.00%,  total acc: 68.92%   [EVAL] batch:   36 | acc: 0.00%,  total acc: 67.06%   [EVAL] batch:   37 | acc: 0.00%,  total acc: 65.30%   [EVAL] batch:   38 | acc: 6.25%,  total acc: 63.78%   [EVAL] batch:   39 | acc: 62.50%,  total acc: 63.75%   [EVAL] batch:   40 | acc: 87.50%,  total acc: 64.33%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 64.88%   [EVAL] batch:   42 | acc: 87.50%,  total acc: 65.41%   [EVAL] batch:   43 | acc: 68.75%,  total acc: 65.48%   [EVAL] batch:   44 | acc: 75.00%,  total acc: 65.69%   [EVAL] batch:   45 | acc: 25.00%,  total acc: 64.81%   [EVAL] batch:   46 | acc: 56.25%,  total acc: 64.63%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 65.36%   [EVAL] batch:   48 | acc: 37.50%,  total acc: 64.80%   [EVAL] batch:   49 | acc: 25.00%,  total acc: 64.00%   [EVAL] batch:   50 | acc: 31.25%,  total acc: 63.36%   [EVAL] batch:   51 | acc: 37.50%,  total acc: 62.86%   [EVAL] batch:   52 | acc: 18.75%,  total acc: 62.03%   [EVAL] batch:   53 | acc: 18.75%,  total acc: 61.23%   [EVAL] batch:   54 | acc: 43.75%,  total acc: 60.91%   [EVAL] batch:   55 | acc: 50.00%,  total acc: 60.71%   [EVAL] batch:   56 | acc: 56.25%,  total acc: 60.64%   [EVAL] batch:   57 | acc: 56.25%,  total acc: 60.56%   [EVAL] batch:   58 | acc: 75.00%,  total acc: 60.81%   [EVAL] batch:   59 | acc: 68.75%,  total acc: 60.94%   [EVAL] batch:   60 | acc: 25.00%,  total acc: 60.35%   [EVAL] batch:   61 | acc: 6.25%,  total acc: 59.48%   [EVAL] batch:   62 | acc: 37.50%,  total acc: 59.13%   [EVAL] batch:   63 | acc: 50.00%,  total acc: 58.98%   [EVAL] batch:   64 | acc: 37.50%,  total acc: 58.65%   [EVAL] batch:   65 | acc: 18.75%,  total acc: 58.05%   [EVAL] batch:   66 | acc: 31.25%,  total acc: 57.65%   [EVAL] batch:   67 | acc: 31.25%,  total acc: 57.26%   [EVAL] batch:   68 | acc: 25.00%,  total acc: 56.79%   [EVAL] batch:   69 | acc: 50.00%,  total acc: 56.70%   [EVAL] batch:   70 | acc: 43.75%,  total acc: 56.51%   [EVAL] batch:   71 | acc: 62.50%,  total acc: 56.60%   [EVAL] batch:   72 | acc: 43.75%,  total acc: 56.42%   [EVAL] batch:   73 | acc: 43.75%,  total acc: 56.25%   [EVAL] batch:   74 | acc: 50.00%,  total acc: 56.17%   [EVAL] batch:   75 | acc: 37.50%,  total acc: 55.92%   [EVAL] batch:   76 | acc: 62.50%,  total acc: 56.01%   [EVAL] batch:   77 | acc: 56.25%,  total acc: 56.01%   [EVAL] batch:   78 | acc: 68.75%,  total acc: 56.17%   [EVAL] batch:   79 | acc: 87.50%,  total acc: 56.56%   [EVAL] batch:   80 | acc: 87.50%,  total acc: 56.94%   [EVAL] batch:   81 | acc: 100.00%,  total acc: 57.47%   [EVAL] batch:   82 | acc: 87.50%,  total acc: 57.83%   [EVAL] batch:   83 | acc: 62.50%,  total acc: 57.89%   [EVAL] batch:   84 | acc: 0.00%,  total acc: 57.21%   [EVAL] batch:   85 | acc: 6.25%,  total acc: 56.61%   [EVAL] batch:   86 | acc: 6.25%,  total acc: 56.03%   [EVAL] batch:   87 | acc: 75.00%,  total acc: 56.25%   [EVAL] batch:   88 | acc: 87.50%,  total acc: 56.60%   [EVAL] batch:   89 | acc: 93.75%,  total acc: 57.01%   [EVAL] batch:   90 | acc: 100.00%,  total acc: 57.49%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 57.95%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 58.40%   [EVAL] batch:   93 | acc: 100.00%,  total acc: 58.84%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 59.28%   [EVAL] batch:   95 | acc: 81.25%,  total acc: 59.51%   [EVAL] batch:   96 | acc: 0.00%,  total acc: 58.89%   [EVAL] batch:   97 | acc: 0.00%,  total acc: 58.29%   [EVAL] batch:   98 | acc: 81.25%,  total acc: 58.52%   [EVAL] batch:   99 | acc: 75.00%,  total acc: 58.69%   [EVAL] batch:  100 | acc: 81.25%,  total acc: 58.91%   [EVAL] batch:  101 | acc: 62.50%,  total acc: 58.95%   [EVAL] batch:  102 | acc: 68.75%,  total acc: 59.04%   [EVAL] batch:  103 | acc: 68.75%,  total acc: 59.13%   [EVAL] batch:  104 | acc: 93.75%,  total acc: 59.46%   [EVAL] batch:  105 | acc: 68.75%,  total acc: 59.55%   [EVAL] batch:  106 | acc: 87.50%,  total acc: 59.81%   [EVAL] batch:  107 | acc: 93.75%,  total acc: 60.13%   [EVAL] batch:  108 | acc: 87.50%,  total acc: 60.38%   [EVAL] batch:  109 | acc: 93.75%,  total acc: 60.68%   [EVAL] batch:  110 | acc: 62.50%,  total acc: 60.70%   [EVAL] batch:  111 | acc: 31.25%,  total acc: 60.44%   [EVAL] batch:  112 | acc: 50.00%,  total acc: 60.34%   [EVAL] batch:  113 | acc: 43.75%,  total acc: 60.20%   [EVAL] batch:  114 | acc: 50.00%,  total acc: 60.11%   [EVAL] batch:  115 | acc: 56.25%,  total acc: 60.08%   [EVAL] batch:  116 | acc: 75.00%,  total acc: 60.20%   [EVAL] batch:  117 | acc: 75.00%,  total acc: 60.33%   [EVAL] batch:  118 | acc: 87.50%,  total acc: 60.56%   [EVAL] batch:  119 | acc: 62.50%,  total acc: 60.57%   [EVAL] batch:  120 | acc: 75.00%,  total acc: 60.69%   [EVAL] batch:  121 | acc: 75.00%,  total acc: 60.81%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 61.13%   [EVAL] batch:  123 | acc: 81.25%,  total acc: 61.29%   [EVAL] batch:  124 | acc: 81.25%,  total acc: 61.45%   [EVAL] batch:  125 | acc: 68.75%,  total acc: 61.51%   [EVAL] batch:  126 | acc: 75.00%,  total acc: 61.61%   [EVAL] batch:  127 | acc: 100.00%,  total acc: 61.91%   [EVAL] batch:  128 | acc: 100.00%,  total acc: 62.21%   [EVAL] batch:  129 | acc: 100.00%,  total acc: 62.50%   [EVAL] batch:  130 | acc: 100.00%,  total acc: 62.79%   [EVAL] batch:  131 | acc: 100.00%,  total acc: 63.07%   [EVAL] batch:  132 | acc: 62.50%,  total acc: 63.06%   
cur_acc:  ['0.8561', '0.8616', '0.3047', '0.6538', '0.4631', '0.7545', '0.7125', '0.8125']
his_acc:  ['0.8561', '0.8431', '0.7303', '0.6629', '0.5930', '0.6510', '0.6476', '0.6306']
--------Round  1
seed:  200
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/test.pkl
Task_order: [7 6 3 2 4 0 5 1]
prepared data!
CurrentTrain: epoch  0, batch     0 | loss: 13.1943588CurrentTrain: epoch  0, batch     1 | loss: 13.2088938CurrentTrain: epoch  0, batch     2 | loss: 12.8815546CurrentTrain: epoch  0, batch     3 | loss: 12.6850529CurrentTrain: epoch  0, batch     4 | loss: 12.4739094CurrentTrain: epoch  0, batch     5 | loss: 12.4463577CurrentTrain: epoch  0, batch     6 | loss: 12.6172466CurrentTrain: epoch  0, batch     7 | loss: 12.1328678CurrentTrain: epoch  0, batch     8 | loss: 12.3931408CurrentTrain: epoch  0, batch     9 | loss: 11.9761553CurrentTrain: epoch  0, batch    10 | loss: 12.0636826CurrentTrain: epoch  0, batch    11 | loss: 11.9981861CurrentTrain: epoch  0, batch    12 | loss: 11.8976021CurrentTrain: epoch  0, batch    13 | loss: 11.3950558CurrentTrain: epoch  0, batch    14 | loss: 11.8494282CurrentTrain: epoch  0, batch    15 | loss: 11.4480581CurrentTrain: epoch  0, batch    16 | loss: 11.1259785CurrentTrain: epoch  0, batch    17 | loss: 11.4710855CurrentTrain: epoch  0, batch    18 | loss: 11.1451845CurrentTrain: epoch  0, batch    19 | loss: 11.1051579CurrentTrain: epoch  0, batch    20 | loss: 11.0872869CurrentTrain: epoch  0, batch    21 | loss: 11.0069695CurrentTrain: epoch  0, batch    22 | loss: 11.4089756CurrentTrain: epoch  0, batch    23 | loss: 10.6106968CurrentTrain: epoch  0, batch    24 | loss: 11.6539888CurrentTrain: epoch  0, batch    25 | loss: 10.4477234CurrentTrain: epoch  0, batch    26 | loss: 11.2612429CurrentTrain: epoch  0, batch    27 | loss: 10.8491697CurrentTrain: epoch  0, batch    28 | loss: 11.1536789CurrentTrain: epoch  0, batch    29 | loss: 10.7520542CurrentTrain: epoch  0, batch    30 | loss: 10.7770996CurrentTrain: epoch  0, batch    31 | loss: 10.7930412CurrentTrain: epoch  0, batch    32 | loss: 10.6465664CurrentTrain: epoch  0, batch    33 | loss: 10.7123947CurrentTrain: epoch  0, batch    34 | loss: 10.6794901CurrentTrain: epoch  0, batch    35 | loss: 10.3989420CurrentTrain: epoch  0, batch    36 | loss: 10.3247290CurrentTrain: epoch  0, batch    37 | loss: 10.7069340CurrentTrain: epoch  1, batch     0 | loss: 10.2764511CurrentTrain: epoch  1, batch     1 | loss: 10.4736252CurrentTrain: epoch  1, batch     2 | loss: 9.4967442CurrentTrain: epoch  1, batch     3 | loss: 10.0857630CurrentTrain: epoch  1, batch     4 | loss: 10.3534889CurrentTrain: epoch  1, batch     5 | loss: 9.3814745CurrentTrain: epoch  1, batch     6 | loss: 9.9117756CurrentTrain: epoch  1, batch     7 | loss: 9.8061829CurrentTrain: epoch  1, batch     8 | loss: 10.3087311CurrentTrain: epoch  1, batch     9 | loss: 9.7319374CurrentTrain: epoch  1, batch    10 | loss: 10.2060909CurrentTrain: epoch  1, batch    11 | loss: 9.7425213CurrentTrain: epoch  1, batch    12 | loss: 9.2131910CurrentTrain: epoch  1, batch    13 | loss: 9.1809015CurrentTrain: epoch  1, batch    14 | loss: 9.4104490CurrentTrain: epoch  1, batch    15 | loss: 8.6542645CurrentTrain: epoch  1, batch    16 | loss: 9.3987408CurrentTrain: epoch  1, batch    17 | loss: 8.5840645CurrentTrain: epoch  1, batch    18 | loss: 9.7110949CurrentTrain: epoch  1, batch    19 | loss: 8.8110647CurrentTrain: epoch  1, batch    20 | loss: 9.1412792CurrentTrain: epoch  1, batch    21 | loss: 9.0588856CurrentTrain: epoch  1, batch    22 | loss: 9.2621880CurrentTrain: epoch  1, batch    23 | loss: 9.2009201CurrentTrain: epoch  1, batch    24 | loss: 8.9609880CurrentTrain: epoch  1, batch    25 | loss: 8.6792355CurrentTrain: epoch  1, batch    26 | loss: 8.7032413CurrentTrain: epoch  1, batch    27 | loss: 8.7165871CurrentTrain: epoch  1, batch    28 | loss: 9.5203495CurrentTrain: epoch  1, batch    29 | loss: 9.7717237CurrentTrain: epoch  1, batch    30 | loss: 9.2936230CurrentTrain: epoch  1, batch    31 | loss: 8.8000355CurrentTrain: epoch  1, batch    32 | loss: 8.4560137CurrentTrain: epoch  1, batch    33 | loss: 8.3669434CurrentTrain: epoch  1, batch    34 | loss: 8.3704643CurrentTrain: epoch  1, batch    35 | loss: 8.2066135CurrentTrain: epoch  1, batch    36 | loss: 8.6126060CurrentTrain: epoch  1, batch    37 | loss: 9.1527586CurrentTrain: epoch  2, batch     0 | loss: 8.1082134CurrentTrain: epoch  2, batch     1 | loss: 8.2614460CurrentTrain: epoch  2, batch     2 | loss: 9.1509523CurrentTrain: epoch  2, batch     3 | loss: 7.5875092CurrentTrain: epoch  2, batch     4 | loss: 7.4240284CurrentTrain: epoch  2, batch     5 | loss: 8.8470430CurrentTrain: epoch  2, batch     6 | loss: 8.3282127CurrentTrain: epoch  2, batch     7 | loss: 8.7223930CurrentTrain: epoch  2, batch     8 | loss: 8.3519268CurrentTrain: epoch  2, batch     9 | loss: 8.4723110CurrentTrain: epoch  2, batch    10 | loss: 8.3432894CurrentTrain: epoch  2, batch    11 | loss: 7.9660215CurrentTrain: epoch  2, batch    12 | loss: 7.6403618CurrentTrain: epoch  2, batch    13 | loss: 7.9003654CurrentTrain: epoch  2, batch    14 | loss: 8.5508432CurrentTrain: epoch  2, batch    15 | loss: 8.1521225CurrentTrain: epoch  2, batch    16 | loss: 8.0694485CurrentTrain: epoch  2, batch    17 | loss: 8.2203865CurrentTrain: epoch  2, batch    18 | loss: 8.1662912CurrentTrain: epoch  2, batch    19 | loss: 8.0654516CurrentTrain: epoch  2, batch    20 | loss: 7.9494824CurrentTrain: epoch  2, batch    21 | loss: 7.6826963CurrentTrain: epoch  2, batch    22 | loss: 8.5339432CurrentTrain: epoch  2, batch    23 | loss: 7.6894817CurrentTrain: epoch  2, batch    24 | loss: 7.6159887CurrentTrain: epoch  2, batch    25 | loss: 7.6986179CurrentTrain: epoch  2, batch    26 | loss: 7.5964770CurrentTrain: epoch  2, batch    27 | loss: 7.7692814CurrentTrain: epoch  2, batch    28 | loss: 7.4057713CurrentTrain: epoch  2, batch    29 | loss: 7.8754148CurrentTrain: epoch  2, batch    30 | loss: 6.8914270CurrentTrain: epoch  2, batch    31 | loss: 7.2671661CurrentTrain: epoch  2, batch    32 | loss: 7.2690945CurrentTrain: epoch  2, batch    33 | loss: 7.2840443CurrentTrain: epoch  2, batch    34 | loss: 8.0254192CurrentTrain: epoch  2, batch    35 | loss: 7.1783285CurrentTrain: epoch  2, batch    36 | loss: 8.0732136CurrentTrain: epoch  2, batch    37 | loss: 8.1862164CurrentTrain: epoch  3, batch     0 | loss: 7.9546437CurrentTrain: epoch  3, batch     1 | loss: 7.2877026CurrentTrain: epoch  3, batch     2 | loss: 7.9970784CurrentTrain: epoch  3, batch     3 | loss: 7.4446106CurrentTrain: epoch  3, batch     4 | loss: 7.0291862CurrentTrain: epoch  3, batch     5 | loss: 6.6525726CurrentTrain: epoch  3, batch     6 | loss: 7.4990187CurrentTrain: epoch  3, batch     7 | loss: 6.4209676CurrentTrain: epoch  3, batch     8 | loss: 7.5921178CurrentTrain: epoch  3, batch     9 | loss: 6.9922342CurrentTrain: epoch  3, batch    10 | loss: 7.3974099CurrentTrain: epoch  3, batch    11 | loss: 6.4548388CurrentTrain: epoch  3, batch    12 | loss: 6.9090910CurrentTrain: epoch  3, batch    13 | loss: 6.7095308CurrentTrain: epoch  3, batch    14 | loss: 7.9558945CurrentTrain: epoch  3, batch    15 | loss: 6.9631071CurrentTrain: epoch  3, batch    16 | loss: 7.4660358CurrentTrain: epoch  3, batch    17 | loss: 6.2792454CurrentTrain: epoch  3, batch    18 | loss: 6.6290321CurrentTrain: epoch  3, batch    19 | loss: 6.9395127CurrentTrain: epoch  3, batch    20 | loss: 6.2139606CurrentTrain: epoch  3, batch    21 | loss: 7.5120702CurrentTrain: epoch  3, batch    22 | loss: 7.1330490CurrentTrain: epoch  3, batch    23 | loss: 7.0268698CurrentTrain: epoch  3, batch    24 | loss: 8.2819605CurrentTrain: epoch  3, batch    25 | loss: 6.9523067CurrentTrain: epoch  3, batch    26 | loss: 7.1925068CurrentTrain: epoch  3, batch    27 | loss: 6.0926781CurrentTrain: epoch  3, batch    28 | loss: 9.0865459CurrentTrain: epoch  3, batch    29 | loss: 8.0795460CurrentTrain: epoch  3, batch    30 | loss: 7.2525835CurrentTrain: epoch  3, batch    31 | loss: 7.1508827CurrentTrain: epoch  3, batch    32 | loss: 7.7219224CurrentTrain: epoch  3, batch    33 | loss: 8.1320381CurrentTrain: epoch  3, batch    34 | loss: 7.6344671CurrentTrain: epoch  3, batch    35 | loss: 6.6497030CurrentTrain: epoch  3, batch    36 | loss: 6.9474182CurrentTrain: epoch  3, batch    37 | loss: 5.9384108CurrentTrain: epoch  4, batch     0 | loss: 6.7901149CurrentTrain: epoch  4, batch     1 | loss: 6.9960742CurrentTrain: epoch  4, batch     2 | loss: 6.7222271CurrentTrain: epoch  4, batch     3 | loss: 6.8582888CurrentTrain: epoch  4, batch     4 | loss: 6.8061376CurrentTrain: epoch  4, batch     5 | loss: 7.3289766CurrentTrain: epoch  4, batch     6 | loss: 6.5014391CurrentTrain: epoch  4, batch     7 | loss: 7.0150108CurrentTrain: epoch  4, batch     8 | loss: 6.9735193CurrentTrain: epoch  4, batch     9 | loss: 7.2588301CurrentTrain: epoch  4, batch    10 | loss: 5.3513222CurrentTrain: epoch  4, batch    11 | loss: 7.0670958CurrentTrain: epoch  4, batch    12 | loss: 6.5934200CurrentTrain: epoch  4, batch    13 | loss: 6.5216808CurrentTrain: epoch  4, batch    14 | loss: 7.1111794CurrentTrain: epoch  4, batch    15 | loss: 7.1419811CurrentTrain: epoch  4, batch    16 | loss: 7.4712486CurrentTrain: epoch  4, batch    17 | loss: 6.9691925CurrentTrain: epoch  4, batch    18 | loss: 7.6137533CurrentTrain: epoch  4, batch    19 | loss: 5.9929132CurrentTrain: epoch  4, batch    20 | loss: 6.4351444CurrentTrain: epoch  4, batch    21 | loss: 6.3034849CurrentTrain: epoch  4, batch    22 | loss: 7.3110986CurrentTrain: epoch  4, batch    23 | loss: 6.0462046CurrentTrain: epoch  4, batch    24 | loss: 6.7858591CurrentTrain: epoch  4, batch    25 | loss: 6.6943617CurrentTrain: epoch  4, batch    26 | loss: 7.4592152CurrentTrain: epoch  4, batch    27 | loss: 6.3484449CurrentTrain: epoch  4, batch    28 | loss: 6.6056175CurrentTrain: epoch  4, batch    29 | loss: 6.1229277CurrentTrain: epoch  4, batch    30 | loss: 6.4041443CurrentTrain: epoch  4, batch    31 | loss: 7.1103415CurrentTrain: epoch  4, batch    32 | loss: 7.5737567CurrentTrain: epoch  4, batch    33 | loss: 7.6669960CurrentTrain: epoch  4, batch    34 | loss: 5.9630947CurrentTrain: epoch  4, batch    35 | loss: 5.9803524CurrentTrain: epoch  4, batch    36 | loss: 6.6243453CurrentTrain: epoch  4, batch    37 | loss: 7.9637384CurrentTrain: epoch  5, batch     0 | loss: 5.9035664CurrentTrain: epoch  5, batch     1 | loss: 7.3873291CurrentTrain: epoch  5, batch     2 | loss: 6.3781805CurrentTrain: epoch  5, batch     3 | loss: 6.8431830CurrentTrain: epoch  5, batch     4 | loss: 6.7556953CurrentTrain: epoch  5, batch     5 | loss: 6.5214844CurrentTrain: epoch  5, batch     6 | loss: 6.2639437CurrentTrain: epoch  5, batch     7 | loss: 6.0010357CurrentTrain: epoch  5, batch     8 | loss: 6.6454115CurrentTrain: epoch  5, batch     9 | loss: 6.7636003CurrentTrain: epoch  5, batch    10 | loss: 6.3371668CurrentTrain: epoch  5, batch    11 | loss: 6.5448155CurrentTrain: epoch  5, batch    12 | loss: 6.0678644CurrentTrain: epoch  5, batch    13 | loss: 6.1826930CurrentTrain: epoch  5, batch    14 | loss: 6.0142260CurrentTrain: epoch  5, batch    15 | loss: 6.6133156CurrentTrain: epoch  5, batch    16 | loss: 6.3967757CurrentTrain: epoch  5, batch    17 | loss: 7.6193900CurrentTrain: epoch  5, batch    18 | loss: 6.4201765CurrentTrain: epoch  5, batch    19 | loss: 6.1679010CurrentTrain: epoch  5, batch    20 | loss: 6.0216274CurrentTrain: epoch  5, batch    21 | loss: 6.6595297CurrentTrain: epoch  5, batch    22 | loss: 6.9957123CurrentTrain: epoch  5, batch    23 | loss: 6.3549967CurrentTrain: epoch  5, batch    24 | loss: 5.8459797CurrentTrain: epoch  5, batch    25 | loss: 6.8124933CurrentTrain: epoch  5, batch    26 | loss: 6.4026117CurrentTrain: epoch  5, batch    27 | loss: 6.4751110CurrentTrain: epoch  5, batch    28 | loss: 6.5499358CurrentTrain: epoch  5, batch    29 | loss: 6.4225330CurrentTrain: epoch  5, batch    30 | loss: 7.2179103CurrentTrain: epoch  5, batch    31 | loss: 7.1514368CurrentTrain: epoch  5, batch    32 | loss: 6.4222989CurrentTrain: epoch  5, batch    33 | loss: 6.0510464CurrentTrain: epoch  5, batch    34 | loss: 6.5408106CurrentTrain: epoch  5, batch    35 | loss: 6.3584833CurrentTrain: epoch  5, batch    36 | loss: 6.6312151CurrentTrain: epoch  5, batch    37 | loss: 6.9012480CurrentTrain: epoch  6, batch     0 | loss: 6.4432383CurrentTrain: epoch  6, batch     1 | loss: 6.4953399CurrentTrain: epoch  6, batch     2 | loss: 6.0199809CurrentTrain: epoch  6, batch     3 | loss: 5.7249851CurrentTrain: epoch  6, batch     4 | loss: 6.5479321CurrentTrain: epoch  6, batch     5 | loss: 6.2201605CurrentTrain: epoch  6, batch     6 | loss: 6.6595335CurrentTrain: epoch  6, batch     7 | loss: 6.0528650CurrentTrain: epoch  6, batch     8 | loss: 5.5830579CurrentTrain: epoch  6, batch     9 | loss: 6.1461377CurrentTrain: epoch  6, batch    10 | loss: 5.7532339CurrentTrain: epoch  6, batch    11 | loss: 5.8715668CurrentTrain: epoch  6, batch    12 | loss: 5.9587536CurrentTrain: epoch  6, batch    13 | loss: 6.0107718CurrentTrain: epoch  6, batch    14 | loss: 6.6173916CurrentTrain: epoch  6, batch    15 | loss: 5.9962645CurrentTrain: epoch  6, batch    16 | loss: 6.3681145CurrentTrain: epoch  6, batch    17 | loss: 6.0727882CurrentTrain: epoch  6, batch    18 | loss: 5.8564773CurrentTrain: epoch  6, batch    19 | loss: 5.4773540CurrentTrain: epoch  6, batch    20 | loss: 6.0479622CurrentTrain: epoch  6, batch    21 | loss: 5.8990073CurrentTrain: epoch  6, batch    22 | loss: 5.7201333CurrentTrain: epoch  6, batch    23 | loss: 6.0614991CurrentTrain: epoch  6, batch    24 | loss: 5.6250887CurrentTrain: epoch  6, batch    25 | loss: 6.1601744CurrentTrain: epoch  6, batch    26 | loss: 6.2105103CurrentTrain: epoch  6, batch    27 | loss: 5.7182193CurrentTrain: epoch  6, batch    28 | loss: 5.8999848CurrentTrain: epoch  6, batch    29 | loss: 7.3727307CurrentTrain: epoch  6, batch    30 | loss: 6.2102671CurrentTrain: epoch  6, batch    31 | loss: 6.3823714CurrentTrain: epoch  6, batch    32 | loss: 5.9619150CurrentTrain: epoch  6, batch    33 | loss: 5.7411571CurrentTrain: epoch  6, batch    34 | loss: 5.5999422CurrentTrain: epoch  6, batch    35 | loss: 5.8891635CurrentTrain: epoch  6, batch    36 | loss: 5.9912972CurrentTrain: epoch  6, batch    37 | loss: 5.4460158CurrentTrain: epoch  7, batch     0 | loss: 5.9580221CurrentTrain: epoch  7, batch     1 | loss: 6.3048162CurrentTrain: epoch  7, batch     2 | loss: 5.9945335CurrentTrain: epoch  7, batch     3 | loss: 5.8708544CurrentTrain: epoch  7, batch     4 | loss: 5.3722081CurrentTrain: epoch  7, batch     5 | loss: 5.6140642CurrentTrain: epoch  7, batch     6 | loss: 5.3224893CurrentTrain: epoch  7, batch     7 | loss: 6.3612957CurrentTrain: epoch  7, batch     8 | loss: 5.6609125CurrentTrain: epoch  7, batch     9 | loss: 6.2040539CurrentTrain: epoch  7, batch    10 | loss: 6.1773643CurrentTrain: epoch  7, batch    11 | loss: 5.6818066CurrentTrain: epoch  7, batch    12 | loss: 6.2416630CurrentTrain: epoch  7, batch    13 | loss: 5.3381705CurrentTrain: epoch  7, batch    14 | loss: 6.0978479CurrentTrain: epoch  7, batch    15 | loss: 5.3825569CurrentTrain: epoch  7, batch    16 | loss: 5.4074001CurrentTrain: epoch  7, batch    17 | loss: 5.5951195CurrentTrain: epoch  7, batch    18 | loss: 5.8204141CurrentTrain: epoch  7, batch    19 | loss: 5.4897680CurrentTrain: epoch  7, batch    20 | loss: 6.2523665CurrentTrain: epoch  7, batch    21 | loss: 5.4221048CurrentTrain: epoch  7, batch    22 | loss: 5.3579435CurrentTrain: epoch  7, batch    23 | loss: 5.3069491CurrentTrain: epoch  7, batch    24 | loss: 5.7807736CurrentTrain: epoch  7, batch    25 | loss: 5.4470739CurrentTrain: epoch  7, batch    26 | loss: 5.2329149CurrentTrain: epoch  7, batch    27 | loss: 5.3400450CurrentTrain: epoch  7, batch    28 | loss: 5.6766987CurrentTrain: epoch  7, batch    29 | loss: 5.7698846CurrentTrain: epoch  7, batch    30 | loss: 5.2591238CurrentTrain: epoch  7, batch    31 | loss: 5.2680817CurrentTrain: epoch  7, batch    32 | loss: 5.1444907CurrentTrain: epoch  7, batch    33 | loss: 5.5809698CurrentTrain: epoch  7, batch    34 | loss: 6.1813221CurrentTrain: epoch  7, batch    35 | loss: 5.3708534CurrentTrain: epoch  7, batch    36 | loss: 5.3746939CurrentTrain: epoch  7, batch    37 | loss: 5.3499041CurrentTrain: epoch  8, batch     0 | loss: 5.6608419CurrentTrain: epoch  8, batch     1 | loss: 5.4576254CurrentTrain: epoch  8, batch     2 | loss: 5.4722052CurrentTrain: epoch  8, batch     3 | loss: 5.3841171CurrentTrain: epoch  8, batch     4 | loss: 5.1656351CurrentTrain: epoch  8, batch     5 | loss: 5.4384680CurrentTrain: epoch  8, batch     6 | loss: 5.1938438CurrentTrain: epoch  8, batch     7 | loss: 5.3246803CurrentTrain: epoch  8, batch     8 | loss: 5.3956347CurrentTrain: epoch  8, batch     9 | loss: 5.2195168CurrentTrain: epoch  8, batch    10 | loss: 5.2889485CurrentTrain: epoch  8, batch    11 | loss: 5.2599912CurrentTrain: epoch  8, batch    12 | loss: 5.1506338CurrentTrain: epoch  8, batch    13 | loss: 5.4145365CurrentTrain: epoch  8, batch    14 | loss: 5.1465569CurrentTrain: epoch  8, batch    15 | loss: 5.3299317CurrentTrain: epoch  8, batch    16 | loss: 5.2073445CurrentTrain: epoch  8, batch    17 | loss: 5.4630871CurrentTrain: epoch  8, batch    18 | loss: 5.1989079CurrentTrain: epoch  8, batch    19 | loss: 6.0125484CurrentTrain: epoch  8, batch    20 | loss: 5.3242817CurrentTrain: epoch  8, batch    21 | loss: 5.4625287CurrentTrain: epoch  8, batch    22 | loss: 5.0721025CurrentTrain: epoch  8, batch    23 | loss: 5.3782845CurrentTrain: epoch  8, batch    24 | loss: 4.9598799CurrentTrain: epoch  8, batch    25 | loss: 5.1527801CurrentTrain: epoch  8, batch    26 | loss: 5.3253965CurrentTrain: epoch  8, batch    27 | loss: 5.2001982CurrentTrain: epoch  8, batch    28 | loss: 5.1727223CurrentTrain: epoch  8, batch    29 | loss: 5.3126040CurrentTrain: epoch  8, batch    30 | loss: 5.2276707CurrentTrain: epoch  8, batch    31 | loss: 5.0546865CurrentTrain: epoch  8, batch    32 | loss: 5.1252341CurrentTrain: epoch  8, batch    33 | loss: 5.0179505CurrentTrain: epoch  8, batch    34 | loss: 5.6779251CurrentTrain: epoch  8, batch    35 | loss: 5.1129875CurrentTrain: epoch  8, batch    36 | loss: 5.0308352CurrentTrain: epoch  8, batch    37 | loss: 5.1040154CurrentTrain: epoch  9, batch     0 | loss: 4.9476042CurrentTrain: epoch  9, batch     1 | loss: 5.0776796CurrentTrain: epoch  9, batch     2 | loss: 5.0328798CurrentTrain: epoch  9, batch     3 | loss: 5.9188895CurrentTrain: epoch  9, batch     4 | loss: 5.5458746CurrentTrain: epoch  9, batch     5 | loss: 5.5257835CurrentTrain: epoch  9, batch     6 | loss: 5.4402986CurrentTrain: epoch  9, batch     7 | loss: 5.2466526CurrentTrain: epoch  9, batch     8 | loss: 5.0423665CurrentTrain: epoch  9, batch     9 | loss: 5.2987370CurrentTrain: epoch  9, batch    10 | loss: 5.2391086CurrentTrain: epoch  9, batch    11 | loss: 5.1336136CurrentTrain: epoch  9, batch    12 | loss: 5.1255417CurrentTrain: epoch  9, batch    13 | loss: 4.9994564CurrentTrain: epoch  9, batch    14 | loss: 5.1288466CurrentTrain: epoch  9, batch    15 | loss: 5.3099442CurrentTrain: epoch  9, batch    16 | loss: 5.2933183CurrentTrain: epoch  9, batch    17 | loss: 5.2402430CurrentTrain: epoch  9, batch    18 | loss: 4.9082875CurrentTrain: epoch  9, batch    19 | loss: 5.1101813CurrentTrain: epoch  9, batch    20 | loss: 4.9875288CurrentTrain: epoch  9, batch    21 | loss: 5.0117521CurrentTrain: epoch  9, batch    22 | loss: 5.1873841CurrentTrain: epoch  9, batch    23 | loss: 5.0057993CurrentTrain: epoch  9, batch    24 | loss: 5.1164746CurrentTrain: epoch  9, batch    25 | loss: 5.0237622CurrentTrain: epoch  9, batch    26 | loss: 4.9649959CurrentTrain: epoch  9, batch    27 | loss: 5.0545168CurrentTrain: epoch  9, batch    28 | loss: 5.0579734CurrentTrain: epoch  9, batch    29 | loss: 4.9949794CurrentTrain: epoch  9, batch    30 | loss: 4.9334202CurrentTrain: epoch  9, batch    31 | loss: 4.9099369CurrentTrain: epoch  9, batch    32 | loss: 5.4690638CurrentTrain: epoch  9, batch    33 | loss: 5.5053349CurrentTrain: epoch  9, batch    34 | loss: 5.0780420CurrentTrain: epoch  9, batch    35 | loss: 4.9339762CurrentTrain: epoch  9, batch    36 | loss: 4.9409065CurrentTrain: epoch  9, batch    37 | loss: 5.0055561
#### person countries of residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person countries of residence" means a person is living in a country.
Here is an example:
Relation: person countries of residence
Context: franz vranitzky was the first austrian chancellor to visit israel , in 1993 , in a tentative thawing of relations after the six-year term of kurt waldheim , who served during world war ii as a junior army officer in a nazi army unit in yugoslavia .
Head Entity: kurt waldheim
Tail Entity: yugoslavia
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person countries of residence":
Relation: person countries of residence  
Context: After spending several years in the United States, Maria moved back to her hometown in Italy, where she felt a deep connection to her roots.  
Head Entity: Maria  
Tail Entity: Italy  

Relation: person countries of residence  
Context: Following his successful career in Japan, Takashi decided to relocate to Canada, seeking new opportunities and a different lifestyle.  
Head Entity: Takashi  
Tail Entity: Canada  
#### organization top members employees ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization top members employees" means an organization 's employer is a person.
Here is an example:
Relation: organization top members employees
Context: as for any gay or lesbian people who enlisted with the military this week , `` they may be told they can not join '' if the pentagon decides to enforce the law , said aubrey sarvis , executive director of the servicemembers legal defense network , a group representing service members affected by the law .
Head Entity: servicemembers legal defense network
Tail Entity: aubrey sarvis
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization top members employees":
Relation: organization top members employees  
Context: The board of directors at Tech Innovations Inc. recently appointed Sarah Johnson as the new Chief Technology Officer, highlighting her extensive experience in the tech industry and her leadership skills.  
Head Entity: Tech Innovations Inc.  
Tail Entity: Sarah Johnson  

Relation: organization top members employees  
Context: During the annual meeting, the CEO of Green Earth Solutions, Mark Thompson, announced the promotion of Lisa Chen to the position of Chief Financial Officer, recognizing her contributions to the company's growth.  
Head Entity: Green Earth Solutions  
Tail Entity: Lisa Chen  
#### organization member of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization member of" means an organization is the member of an organization.
Here is an example:
Relation: organization member of
Context: ipsco shares rose $ 1506 , or 12 percent , to close at $ 14596 on the new york stock exchange .
Head Entity: ipsco
Tail Entity: new york stock exchange
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization member of":
Relation: organization member of  
Context: the united nations is a global organization that promotes peace and cooperation among its member states.  
Head Entity: united nations  
Tail Entity: member states  

Relation: organization member of  
Context: the world health organization collaborates with various countries to improve public health standards and practices.  
Head Entity: world health organization  
Tail Entity: countries  
#### person origin ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person origin" means a person 's nationality is a country.
Here is an example:
Relation: person origin
Context: the former detainee , murat kurnaz , was turned over to german authorities and freed in 2006 after a personal plea from german chancellor angela merkel .
Head Entity: murat kurnaz
Tail Entity: german
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person origin":
Relation: person origin  
Context: the renowned physicist, albert einstein, was born in the kingdom of wurttemberg in the german empire in 1879.  
Head Entity: albert einstein  
Tail Entity: german  

Relation: person origin  
Context: the famous author, gabriel garcía márquez, hailed from the coastal city of aracataca in colombia, where he drew inspiration for many of his works.  
Head Entity: gabriel garcía márquez  
Tail Entity: colombian  
#### person title ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person title" means a person 's title is a title.
Here is an example:
Relation: person title
Context: `` it has to have some tempo , it has to have some energy , '' said sean ross , vice president of music and programming at edison media research , which surveys the radio industry .
Head Entity: sean ross
Tail Entity: vice president
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person title":
Relation: person title  
Context: "After years of hard work and dedication, Maria Gonzalez was promoted to Chief Financial Officer at Tech Innovations, where she has been instrumental in driving the company's financial strategy."  
Head Entity: Maria Gonzalez  
Tail Entity: Chief Financial Officer  

Relation: person title  
Context: "During the annual conference, Dr. James Lee, the lead researcher at the National Institute of Health, presented his findings on the latest advancements in medical technology."  
Head Entity: Dr. James Lee  
Tail Entity: lead researcher  
#### organization country of headquarters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization country of headquarters" means an organization is located in a country.
Here is an example:
Relation: organization country of headquarters
Context: delays and cancellations for argentina 's leading airline aerolineas argentinas -lrb- aa -rrb- continued on sunday , as a baggage handlers ' strike went into its fourth day .
Head Entity: aerolineas argentinas
Tail Entity: argentina
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization country of headquarters":
Relation: organization country of headquarters  
Context: the tech giant apple inc. announced plans to expand its operations in ireland, taking advantage of the favorable corporate tax rates.  
Head Entity: apple inc.  
Tail Entity: ireland  

Relation: organization country of headquarters  
Context: the renowned automotive manufacturer toyota has established its main production facility in japan, contributing significantly to the local economy.  
Head Entity: toyota  
Tail Entity: japan  
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 43.75%,  total acc: 56.25%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 62.50%   [EVAL] batch:    3 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    4 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    5 | acc: 68.75%,  total acc: 63.54%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 67.86%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 71.88%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 76.88%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 78.98%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 80.21%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:   13 | acc: 75.00%,  total acc: 80.80%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 80.42%   [EVAL] batch:   15 | acc: 68.75%,  total acc: 79.69%   [EVAL] batch:   16 | acc: 81.25%,  total acc: 79.78%   [EVAL] batch:   17 | acc: 75.00%,  total acc: 79.51%   [EVAL] batch:   18 | acc: 87.50%,  total acc: 79.93%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 80.62%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 81.55%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 82.39%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 83.15%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 83.59%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 84.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 84.86%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 85.19%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 85.71%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 86.21%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 86.04%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 86.09%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 86.33%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 84.85%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 43.75%,  total acc: 56.25%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 62.50%   [EVAL] batch:    3 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    4 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    5 | acc: 68.75%,  total acc: 63.54%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 67.86%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 71.88%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 76.88%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 78.98%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 80.21%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:   13 | acc: 75.00%,  total acc: 80.80%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 80.42%   [EVAL] batch:   15 | acc: 68.75%,  total acc: 79.69%   [EVAL] batch:   16 | acc: 81.25%,  total acc: 79.78%   [EVAL] batch:   17 | acc: 75.00%,  total acc: 79.51%   [EVAL] batch:   18 | acc: 87.50%,  total acc: 79.93%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 80.62%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 81.55%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 82.39%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 83.15%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 83.59%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 84.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 84.86%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 85.19%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 85.71%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 86.21%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 86.04%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 86.09%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 86.33%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 84.85%   
cur_acc:  ['0.8485']
his_acc:  ['0.8485']
CurrentTrain: epoch  0, batch     0 | loss: 6.2202954CurrentTrain: epoch  0, batch     1 | loss: 6.7739248CurrentTrain: epoch  1, batch     0 | loss: 5.9535003CurrentTrain: epoch  1, batch     1 | loss: 5.2204075CurrentTrain: epoch  2, batch     0 | loss: 5.3359017CurrentTrain: epoch  2, batch     1 | loss: 5.1476007CurrentTrain: epoch  3, batch     0 | loss: 5.1917486CurrentTrain: epoch  3, batch     1 | loss: 4.1506629CurrentTrain: epoch  4, batch     0 | loss: 4.1074438CurrentTrain: epoch  4, batch     1 | loss: 4.5355144CurrentTrain: epoch  5, batch     0 | loss: 3.8571091CurrentTrain: epoch  5, batch     1 | loss: 4.2225075CurrentTrain: epoch  6, batch     0 | loss: 3.6645737CurrentTrain: epoch  6, batch     1 | loss: 4.3974676CurrentTrain: epoch  7, batch     0 | loss: 3.6650229CurrentTrain: epoch  7, batch     1 | loss: 3.7119534CurrentTrain: epoch  8, batch     0 | loss: 3.8092337CurrentTrain: epoch  8, batch     1 | loss: 2.7979381CurrentTrain: epoch  9, batch     0 | loss: 2.8851542CurrentTrain: epoch  9, batch     1 | loss: 3.5282679
#### person stateorprovinces of residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person stateorprovinces of residence" means a person is living in a state or province.
Here is an example:
Relation: person stateorprovinces of residence
Context: ny-schools-chief -lrb- new york -rrb- -- cathleen p. black , mayor michael r. bloomberg 's choice to be the next chancellor of new york city 's public-school system , has during more than 40 years in the media business broken numerous glass ceilings -- and amassed a fortune -- with quick and cold-blooded decision making , crystal-clear goal setting , and an all-surpassing attention to the bottom line .
Head Entity: cathleen p. black
Tail Entity: new york
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person stateorprovinces of residence":
Relation: person stateorprovinces of residence  
Context: after years of living in the bustling city, actor john doe has decided to settle down in the serene landscapes of oregon, where he can enjoy a quieter life away from the spotlight.  
Head Entity: john doe  
Tail Entity: oregon  

Relation: person stateorprovinces of residence  
Context: renowned author jane smith has always found inspiration in the vibrant culture of california, which is why she chose to make los angeles her home for the past decade.  
Head Entity: jane smith  
Tail Entity: california  
#### person date of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person date of death" means a person was died on a date.
Here is an example:
Relation: person date of death
Context: he was taken off life support on feb. 14 .
Head Entity: he
Tail Entity: feb. 14
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person date of death":
Relation: person date of death  
Context: The renowned author passed away on July 20, 2020, after a long illness.  
Head Entity: The renowned author  
Tail Entity: July 20, 2020  

Relation: person date of death  
Context: She left this world peacefully in her sleep on March 5, 2015.  
Head Entity: She  
Tail Entity: March 5, 2015  
#### organization number of employees members ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization number of employees members" means the number of an organization 's employees is a number.
Here is an example:
Relation: organization number of employees members
Context: with the sweep of a federal regulator 's pen , massachusetts stands to gain a new life-science giant in april : covidien , a medical - supplies maker with thousands of products and more than 43,000 employees worldwide .
Head Entity: covidien
Tail Entity: 43,000
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization number of employees members":
Relation: organization number of employees members  
Context: The tech company, Innovatech, has rapidly expanded its workforce over the past year, now boasting a total of 25,000 employees across its global offices.  
Head Entity: Innovatech  
Tail Entity: 25,000  

Relation: organization number of employees members  
Context: After the merger, Green Energy Solutions reported an increase in its workforce, reaching a remarkable 15,000 employees dedicated to sustainable energy initiatives.  
Head Entity: Green Energy Solutions  
Tail Entity: 15,000  
#### person alternate names ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person alternate names" means a person 's alias is a person.
Here is an example:
Relation: person alternate names
Context: the coming of the mahdi will turn the world upside down , and the oppressed shiites will finally see justice .
Head Entity: mahdi
Tail Entity: shiites
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person alternate names":
Relation: person alternate names  
Context: The famous author Samuel Clemens is better known by his pen name, Mark Twain, which he used for his literary works.  
Head Entity: Samuel Clemens  
Tail Entity: Mark Twain  

Relation: person alternate names  
Context: The musician known as Stefani Germanotta has achieved global fame under her stage name, Lady Gaga, captivating audiences with her unique style.  
Head Entity: Stefani Germanotta  
Tail Entity: Lady Gaga  
#### person spouse ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person spouse" means a person 's spouse is a person.
Here is an example:
Relation: person spouse
Context: beverly hills , california 2008-08-17 21:15:39 utc ------ there was much dancing : ellen degeneres and portia de rossi are married , according to reports .
Head Entity: ellen degeneres
Tail Entity: portia de rossi
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person spouse":
Relation: person spouse  
Context: in a lavish ceremony held in new york city, the couple exchanged vows in front of family and friends: john legend and chrissy teigen celebrated their love with a beautiful wedding.  
Head Entity: john legend  
Tail Entity: chrissy teigen  

Relation: person spouse  
Context: during the annual charity gala, it was announced that the famous actor and his long-time partner have tied the knot: ben affleck and jennifer garner are now officially married.  
Head Entity: ben affleck  
Tail Entity: jennifer garner  
Mixup data size:  104
MixupTrain:  epoch  0, batch     0 | loss: 7.1954691MixupTrain:  epoch  0, batch     1 | loss: 6.3381089MixupTrain:  epoch  0, batch     2 | loss: 6.3889477MixupTrain:  epoch  0, batch     3 | loss: 5.4308421MixupTrain:  epoch  0, batch     4 | loss: 5.7428439MixupTrain:  epoch  0, batch     5 | loss: 6.0512195MixupTrain:  epoch  0, batch     6 | loss: 5.1010046
MemoryTrain:  epoch  0, batch     0 | loss: 3.6093690MemoryTrain:  epoch  0, batch     1 | loss: 4.2579699MemoryTrain:  epoch  0, batch     2 | loss: 2.8629596MemoryTrain:  epoch  1, batch     0 | loss: 3.7615180MemoryTrain:  epoch  1, batch     1 | loss: 2.9449131MemoryTrain:  epoch  1, batch     2 | loss: 2.6947014MemoryTrain:  epoch  2, batch     0 | loss: 2.7854798MemoryTrain:  epoch  2, batch     1 | loss: 2.6000316MemoryTrain:  epoch  2, batch     2 | loss: 2.6301489MemoryTrain:  epoch  3, batch     0 | loss: 2.6665742MemoryTrain:  epoch  3, batch     1 | loss: 2.5326030MemoryTrain:  epoch  3, batch     2 | loss: 1.6716509MemoryTrain:  epoch  4, batch     0 | loss: 2.0469098MemoryTrain:  epoch  4, batch     1 | loss: 2.6138837MemoryTrain:  epoch  4, batch     2 | loss: 1.1879703MemoryTrain:  epoch  5, batch     0 | loss: 2.4039462MemoryTrain:  epoch  5, batch     1 | loss: 2.1272540MemoryTrain:  epoch  5, batch     2 | loss: 1.2753900MemoryTrain:  epoch  6, batch     0 | loss: 2.2038703MemoryTrain:  epoch  6, batch     1 | loss: 2.2040348MemoryTrain:  epoch  6, batch     2 | loss: 2.2084911MemoryTrain:  epoch  7, batch     0 | loss: 1.9540379MemoryTrain:  epoch  7, batch     1 | loss: 2.1304162MemoryTrain:  epoch  7, batch     2 | loss: 4.6920252MemoryTrain:  epoch  8, batch     0 | loss: 1.9794209MemoryTrain:  epoch  8, batch     1 | loss: 1.9378760MemoryTrain:  epoch  8, batch     2 | loss: 1.6112132MemoryTrain:  epoch  9, batch     0 | loss: 1.5835824MemoryTrain:  epoch  9, batch     1 | loss: 1.9679810MemoryTrain:  epoch  9, batch     2 | loss: 3.9432509
[EVAL] batch:    0 | acc: 56.25%,  total acc: 56.25%   [EVAL] batch:    1 | acc: 68.75%,  total acc: 62.50%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 68.75%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 73.44%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 73.75%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 77.08%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 80.36%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 82.03%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 82.64%   [EVAL] batch:    9 | acc: 56.25%,  total acc: 80.00%   [EVAL] batch:   10 | acc: 68.75%,  total acc: 78.98%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 80.73%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 81.73%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 82.59%   [EVAL] batch:   14 | acc: 43.75%,  total acc: 80.00%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 43.75%,  total acc: 56.25%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 62.50%   [EVAL] batch:    3 | acc: 50.00%,  total acc: 59.38%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 61.25%   [EVAL] batch:    5 | acc: 50.00%,  total acc: 59.38%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 63.39%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 67.97%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 71.53%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 73.75%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 76.14%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 77.08%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 78.37%   [EVAL] batch:   13 | acc: 75.00%,  total acc: 78.12%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 77.92%   [EVAL] batch:   15 | acc: 62.50%,  total acc: 76.95%   [EVAL] batch:   16 | acc: 81.25%,  total acc: 77.21%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 76.74%   [EVAL] batch:   18 | acc: 81.25%,  total acc: 76.97%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 77.81%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 78.87%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 79.83%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 80.71%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 82.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 82.69%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 83.10%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 83.71%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 84.27%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 84.17%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 84.27%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 84.57%   [EVAL] batch:   32 | acc: 75.00%,  total acc: 84.28%   [EVAL] batch:   33 | acc: 56.25%,  total acc: 83.46%   [EVAL] batch:   34 | acc: 87.50%,  total acc: 83.57%   [EVAL] batch:   35 | acc: 75.00%,  total acc: 83.33%   [EVAL] batch:   36 | acc: 81.25%,  total acc: 83.28%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 83.39%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 83.81%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 84.22%   [EVAL] batch:   40 | acc: 87.50%,  total acc: 84.30%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:   42 | acc: 31.25%,  total acc: 83.14%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 83.52%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 83.89%   [EVAL] batch:   45 | acc: 93.75%,  total acc: 84.10%   [EVAL] batch:   46 | acc: 75.00%,  total acc: 83.91%   
cur_acc:  ['0.8485', '0.8000']
his_acc:  ['0.8485', '0.8391']
CurrentTrain: epoch  0, batch     0 | loss: 7.0934896CurrentTrain: epoch  0, batch     1 | loss: 7.5420489CurrentTrain: epoch  1, batch     0 | loss: 6.8247752CurrentTrain: epoch  1, batch     1 | loss: 4.7659149CurrentTrain: epoch  2, batch     0 | loss: 5.4186759CurrentTrain: epoch  2, batch     1 | loss: 4.7735276CurrentTrain: epoch  3, batch     0 | loss: 5.0589919CurrentTrain: epoch  3, batch     1 | loss: 4.6590281CurrentTrain: epoch  4, batch     0 | loss: 4.7132387CurrentTrain: epoch  4, batch     1 | loss: 4.4069963CurrentTrain: epoch  5, batch     0 | loss: 4.1884098CurrentTrain: epoch  5, batch     1 | loss: 3.6337671CurrentTrain: epoch  6, batch     0 | loss: 3.7468984CurrentTrain: epoch  6, batch     1 | loss: 3.6160626CurrentTrain: epoch  7, batch     0 | loss: 3.6146257CurrentTrain: epoch  7, batch     1 | loss: 3.0666504CurrentTrain: epoch  8, batch     0 | loss: 3.6259513CurrentTrain: epoch  8, batch     1 | loss: 3.1760123CurrentTrain: epoch  9, batch     0 | loss: 2.9863834CurrentTrain: epoch  9, batch     1 | loss: 2.8884425
#### person date of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person date of birth" means a person was born in a date.
Here is an example:
Relation: person date of birth
Context: born of schoolteacher parents in the western town of sabaneta on july 28 , 1954 , chavez studied at the military academy of venezuela in caracas .
Head Entity: chavez
Tail Entity: july 28 , 1954
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person date of birth":
Relation: person date of birth  
Context: born in a small village in italy on march 15, 1980, luca grew up surrounded by the beautiful countryside.  
Head Entity: luca  
Tail Entity: march 15, 1980  

Relation: person date of birth  
Context: the famous author was born in new york city on september 5, 1975, where he later found inspiration for his novels.  
Head Entity: the famous author  
Tail Entity: september 5, 1975  
#### person stateorprovince of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person stateorprovince of birth" means a person was died in a state or province.
Here is an example:
Relation: person stateorprovince of birth
Context: jane matilda bolin was born on april 11 , 1908 , in poughkeepsie , ny .
Head Entity: jane matilda bolin
Tail Entity: ny
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person stateorprovince of birth":
Relation: person stateorprovince of birth  
Context: albert einstein was born on march 14, 1879, in ulm, germany.  
Head Entity: albert einstein  
Tail Entity: germany  

Relation: person stateorprovince of birth  
Context: marilyn monroe was born on june 1, 1926, in los angeles, california.  
Head Entity: marilyn monroe  
Tail Entity: california  
#### person parents ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person parents" means a person 's parent is a person.
Here is an example:
Relation: person parents
Context: lynne spears told the court that lutfi had treated her daughter like a hostage in her own home , drugged her and took over her finances .
Head Entity: her
Tail Entity: lynne spears
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person parents":
Sample 1:  
Relation: person parents  
Context: During the family reunion, Sarah introduced her father, who had always been a guiding force in her life, to her friends.  
Head Entity: her father  
Tail Entity: Sarah  

Sample 2:  
Relation: person parents  
Context: After the ceremony, Emily shared stories about her mother, who had inspired her to pursue a career in medicine.  
Head Entity: her mother  
Tail Entity: Emily  
#### person employee of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person employee of" means a person is an employee of an organization.
Here is an example:
Relation: person employee of
Context: seeking revenge , axel reunites with old pal sgt. billy rosewood -lrb- judge reinhold -rrb- and jon flint -lrb- hector elizondo -rrb- of the beverly hills police department .
Head Entity: hector elizondo
Tail Entity: beverly hills police department
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person employee of":
Relation: person employee of  
Context: After years of hard work and dedication, Sarah Thompson finally received a promotion at Tech Innovations, where she has been a key player in the development team.  
Head Entity: Sarah Thompson  
Tail Entity: Tech Innovations  

Relation: person employee of  
Context: John Smith has been with Global Finance for over a decade, where he has climbed the ranks to become the senior analyst in the investment division.  
Head Entity: John Smith  
Tail Entity: Global Finance  
#### person stateorprovince of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person stateorprovince of death" means a person was died in a state or province.
Here is an example:
Relation: person stateorprovince of death
Context: irene morgan kirkaldy , whose defiance of bus segregation laws more than a decade before rosa parks ' landmark case helped lay the foundation for later civil rights victories , died friday at her home in hayes , va. .
Head Entity: irene morgan kirkaldy
Tail Entity: va.
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person stateorprovince of death":
Relation: person stateorprovince of death  
Context: john doe, a renowned author known for his impactful novels, passed away peacefully in his sleep at his residence in california last night.  
Head Entity: john doe  
Tail Entity: california  

Relation: person stateorprovince of death  
Context: after a long battle with illness, elizabeth taylor, the iconic actress, succumbed to her health issues and died in a hospital in nevada.  
Head Entity: elizabeth taylor  
Tail Entity: nevada  
Mixup data size:  133
MixupTrain:  epoch  0, batch     0 | loss: 4.2051907MixupTrain:  epoch  0, batch     1 | loss: 3.5964498MixupTrain:  epoch  0, batch     2 | loss: 3.6661828MixupTrain:  epoch  0, batch     3 | loss: 3.6628858MixupTrain:  epoch  0, batch     4 | loss: 3.4853299MixupTrain:  epoch  0, batch     5 | loss: 4.0007024MixupTrain:  epoch  0, batch     6 | loss: 3.7353279MixupTrain:  epoch  0, batch     7 | loss: 3.3237613MixupTrain:  epoch  0, batch     8 | loss: 3.0165068
MemoryTrain:  epoch  0, batch     0 | loss: 2.4473720MemoryTrain:  epoch  0, batch     1 | loss: 3.4615641MemoryTrain:  epoch  0, batch     2 | loss: 3.5812135MemoryTrain:  epoch  1, batch     0 | loss: 2.3496559MemoryTrain:  epoch  1, batch     1 | loss: 3.2912552MemoryTrain:  epoch  1, batch     2 | loss: 2.6953287MemoryTrain:  epoch  2, batch     0 | loss: 2.1572130MemoryTrain:  epoch  2, batch     1 | loss: 2.4654894MemoryTrain:  epoch  2, batch     2 | loss: 2.1566606MemoryTrain:  epoch  3, batch     0 | loss: 2.5125904MemoryTrain:  epoch  3, batch     1 | loss: 2.3687792MemoryTrain:  epoch  3, batch     2 | loss: 1.7016847MemoryTrain:  epoch  4, batch     0 | loss: 2.1466703MemoryTrain:  epoch  4, batch     1 | loss: 2.0197473MemoryTrain:  epoch  4, batch     2 | loss: 1.8005222MemoryTrain:  epoch  5, batch     0 | loss: 1.9142153MemoryTrain:  epoch  5, batch     1 | loss: 1.8598037MemoryTrain:  epoch  5, batch     2 | loss: 1.8540851MemoryTrain:  epoch  6, batch     0 | loss: 1.7712829MemoryTrain:  epoch  6, batch     1 | loss: 1.7700680MemoryTrain:  epoch  6, batch     2 | loss: 1.8767908MemoryTrain:  epoch  7, batch     0 | loss: 1.8091609MemoryTrain:  epoch  7, batch     1 | loss: 1.7764111MemoryTrain:  epoch  7, batch     2 | loss: 1.5753154MemoryTrain:  epoch  8, batch     0 | loss: 1.7291584MemoryTrain:  epoch  8, batch     1 | loss: 1.6680722MemoryTrain:  epoch  8, batch     2 | loss: 1.6180240MemoryTrain:  epoch  9, batch     0 | loss: 1.5956706MemoryTrain:  epoch  9, batch     1 | loss: 1.7275202MemoryTrain:  epoch  9, batch     2 | loss: 1.6667374
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 77.08%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 78.12%   [EVAL] batch:    4 | acc: 62.50%,  total acc: 75.00%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 78.12%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 78.57%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 80.47%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 81.94%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 83.12%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 82.95%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 83.85%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 84.13%   [EVAL] batch:   13 | acc: 43.75%,  total acc: 81.25%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 50.00%,  total acc: 62.50%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 66.67%   [EVAL] batch:    3 | acc: 62.50%,  total acc: 65.62%   [EVAL] batch:    4 | acc: 62.50%,  total acc: 65.00%   [EVAL] batch:    5 | acc: 56.25%,  total acc: 63.54%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 66.96%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 71.09%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 74.31%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 76.25%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 78.41%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 79.17%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 80.29%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 79.46%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 79.17%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 77.73%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 77.57%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 76.74%   [EVAL] batch:   18 | acc: 75.00%,  total acc: 76.64%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 77.19%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 78.27%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 79.26%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 80.16%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 80.73%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 81.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 82.21%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 82.64%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 83.26%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 83.84%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 83.75%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 83.87%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 84.18%   [EVAL] batch:   32 | acc: 62.50%,  total acc: 83.52%   [EVAL] batch:   33 | acc: 25.00%,  total acc: 81.80%   [EVAL] batch:   34 | acc: 31.25%,  total acc: 80.36%   [EVAL] batch:   35 | acc: 56.25%,  total acc: 79.69%   [EVAL] batch:   36 | acc: 56.25%,  total acc: 79.05%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 79.28%   [EVAL] batch:   38 | acc: 93.75%,  total acc: 79.65%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 80.16%   [EVAL] batch:   40 | acc: 81.25%,  total acc: 80.18%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 80.51%   [EVAL] batch:   42 | acc: 31.25%,  total acc: 79.36%   [EVAL] batch:   43 | acc: 31.25%,  total acc: 78.27%   [EVAL] batch:   44 | acc: 37.50%,  total acc: 77.36%   [EVAL] batch:   45 | acc: 25.00%,  total acc: 76.22%   [EVAL] batch:   46 | acc: 62.50%,  total acc: 75.93%   [EVAL] batch:   47 | acc: 68.75%,  total acc: 75.78%   [EVAL] batch:   48 | acc: 87.50%,  total acc: 76.02%   [EVAL] batch:   49 | acc: 81.25%,  total acc: 76.12%   [EVAL] batch:   50 | acc: 62.50%,  total acc: 75.86%   [EVAL] batch:   51 | acc: 81.25%,  total acc: 75.96%   [EVAL] batch:   52 | acc: 87.50%,  total acc: 76.18%   [EVAL] batch:   53 | acc: 87.50%,  total acc: 76.39%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 76.70%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 77.01%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 77.30%   [EVAL] batch:   57 | acc: 75.00%,  total acc: 77.26%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 77.65%   [EVAL] batch:   59 | acc: 81.25%,  total acc: 77.71%   [EVAL] batch:   60 | acc: 31.25%,  total acc: 76.95%   
cur_acc:  ['0.8485', '0.8000', '0.8125']
his_acc:  ['0.8485', '0.8391', '0.7695']
CurrentTrain: epoch  0, batch     0 | loss: 5.6976414CurrentTrain: epoch  0, batch     1 | loss: 6.2870092CurrentTrain: epoch  1, batch     0 | loss: 5.0365443CurrentTrain: epoch  1, batch     1 | loss: 4.9511862CurrentTrain: epoch  2, batch     0 | loss: 4.9758310CurrentTrain: epoch  2, batch     1 | loss: 3.8424046CurrentTrain: epoch  3, batch     0 | loss: 4.0210514CurrentTrain: epoch  3, batch     1 | loss: 4.0924530CurrentTrain: epoch  4, batch     0 | loss: 3.9983087CurrentTrain: epoch  4, batch     1 | loss: 3.6602249CurrentTrain: epoch  5, batch     0 | loss: 3.6462514CurrentTrain: epoch  5, batch     1 | loss: 3.4532294CurrentTrain: epoch  6, batch     0 | loss: 3.7050400CurrentTrain: epoch  6, batch     1 | loss: 2.7194469CurrentTrain: epoch  7, batch     0 | loss: 3.1232319CurrentTrain: epoch  7, batch     1 | loss: 3.1299207CurrentTrain: epoch  8, batch     0 | loss: 3.0246286CurrentTrain: epoch  8, batch     1 | loss: 2.8054326CurrentTrain: epoch  9, batch     0 | loss: 2.6733773CurrentTrain: epoch  9, batch     1 | loss: 2.7745619
#### person cities of residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person cities of residence" means a person is living in a city.
Here is an example:
Relation: person cities of residence
Context: -lrb- now there are two : andre carson , a democrat from indianapolis , was elected to the house last year . -rrb-
Head Entity: andre carson
Tail Entity: indianapolis
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person cities of residence":
Relation: person cities of residence  
Context: -lrb- after years in the spotlight, taylor swift has settled down in new york city, where she enjoys the vibrant culture and nightlife. -rrb-  
Head Entity: taylor swift  
Tail Entity: new york city  

Relation: person cities of residence  
Context: -lrb- following his retirement, michael jordan moved to chicago, where he continues to be involved in the community and support local initiatives. -rrb-  
Head Entity: michael jordan  
Tail Entity: chicago  
#### person schools attended ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person schools attended" means a person 's school is an organization.
Here is an example:
Relation: person schools attended
Context: after world war ii , he attended the university of southern california , where he became editor of a college magazine .
Head Entity: he
Tail Entity: university of southern california
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person schools attended":
Relation: person schools attended  
Context: She graduated from Harvard University with a degree in psychology before pursuing her career in clinical research.  
Head Entity: She  
Tail Entity: Harvard University  

Relation: person schools attended  
Context: After completing his high school education, he enrolled at Stanford University to study computer science.  
Head Entity: he  
Tail Entity: Stanford University  
#### person country of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person country of death" means a person was died in a country.
Here is an example:
Relation: person country of death
Context: u.s. rep. parren mitchell , founding member of congressional black caucus , dies at 85
Head Entity: parren mitchell
Tail Entity: u.s.
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person country of death":
Relation: person country of death  
Context: renowned physicist stephen hawking passed away in cambridge, england at the age of 76  
Head Entity: stephen hawking  
Tail Entity: england  

Relation: person country of death  
Context: famous author gabriel garcia marquez died in mexico city, mexico, leaving behind a legacy of magical realism  
Head Entity: gabriel garcia marquez  
Tail Entity: mexico  
#### person children ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person children" means a person 's children is a person.
Here is an example:
Relation: person children
Context: he is survived by two stepdaughters , barbara a. hammond and brenda l. stevenson ; a stepson , michael a. taylor ; two grandchildren and one great-grandchild .
Head Entity: he
Tail Entity: brenda l. stevenson
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person children":
Relation: person children  
Context: she has three children, including her son, john, and her daughters, emily and sarah, who are both pursuing their studies.  
Head Entity: she  
Tail Entity: emily  

Relation: person children  
Context: after the passing of their father, the siblings, including jake and his sister, lila, took on the responsibility of caring for their younger brother, max.  
Head Entity: jake  
Tail Entity: max  
#### person charges ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person charges" means a person was charged with an event.
Here is an example:
Relation: person charges
Context: previously , al-khawinay was sentenced to one year in jail for supporting the country 's minority shiite rebels and defaming the president , but was later pardoned by president ali abdullah saleh .
Head Entity: al-khawinay
Tail Entity: defaming the president
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person charges":
Relation: person charges  
Context: In a shocking turn of events, the local council announced that mayor Thompson was charged with embezzlement after an extensive investigation revealed misuse of public funds.  
Head Entity: mayor Thompson  
Tail Entity: embezzlement  

Relation: person charges  
Context: After a lengthy trial, it was determined that journalist Sarah Lee was charged with libel for publishing false information about a prominent businessman.  
Head Entity: journalist Sarah Lee  
Tail Entity: libel  
Mixup data size:  164
MixupTrain:  epoch  0, batch     0 | loss: 3.2331501MixupTrain:  epoch  0, batch     1 | loss: 2.7524710MixupTrain:  epoch  0, batch     2 | loss: 2.7843359MixupTrain:  epoch  0, batch     3 | loss: 3.1164468MixupTrain:  epoch  0, batch     4 | loss: 2.8472532MixupTrain:  epoch  0, batch     5 | loss: 2.7896163MixupTrain:  epoch  0, batch     6 | loss: 2.9174882MixupTrain:  epoch  0, batch     7 | loss: 2.6867645MixupTrain:  epoch  0, batch     8 | loss: 3.0859142MixupTrain:  epoch  0, batch     9 | loss: 2.6608641MixupTrain:  epoch  0, batch    10 | loss: 2.2552920
MemoryTrain:  epoch  0, batch     0 | loss: 2.1544948MemoryTrain:  epoch  0, batch     1 | loss: 3.4943683MemoryTrain:  epoch  0, batch     2 | loss: 2.8098736MemoryTrain:  epoch  0, batch     3 | loss: 2.3726096MemoryTrain:  epoch  1, batch     0 | loss: 2.4252806MemoryTrain:  epoch  1, batch     1 | loss: 2.6848392MemoryTrain:  epoch  1, batch     2 | loss: 2.2378047MemoryTrain:  epoch  1, batch     3 | loss: 2.4831698MemoryTrain:  epoch  2, batch     0 | loss: 1.6521420MemoryTrain:  epoch  2, batch     1 | loss: 2.3639760MemoryTrain:  epoch  2, batch     2 | loss: 2.6930494MemoryTrain:  epoch  2, batch     3 | loss: 2.1416149MemoryTrain:  epoch  3, batch     0 | loss: 2.0249012MemoryTrain:  epoch  3, batch     1 | loss: 2.3845325MemoryTrain:  epoch  3, batch     2 | loss: 1.5227454MemoryTrain:  epoch  3, batch     3 | loss: 2.0465920MemoryTrain:  epoch  4, batch     0 | loss: 2.1695812MemoryTrain:  epoch  4, batch     1 | loss: 1.9775448MemoryTrain:  epoch  4, batch     2 | loss: 1.6849409MemoryTrain:  epoch  4, batch     3 | loss: 1.5809132MemoryTrain:  epoch  5, batch     0 | loss: 1.6678679MemoryTrain:  epoch  5, batch     1 | loss: 1.8016356MemoryTrain:  epoch  5, batch     2 | loss: 1.7283306MemoryTrain:  epoch  5, batch     3 | loss: 1.8803880MemoryTrain:  epoch  6, batch     0 | loss: 1.6663970MemoryTrain:  epoch  6, batch     1 | loss: 1.5310116MemoryTrain:  epoch  6, batch     2 | loss: 1.6032197MemoryTrain:  epoch  6, batch     3 | loss: 1.6706414MemoryTrain:  epoch  7, batch     0 | loss: 1.6266950MemoryTrain:  epoch  7, batch     1 | loss: 1.5299491MemoryTrain:  epoch  7, batch     2 | loss: 1.6145768MemoryTrain:  epoch  7, batch     3 | loss: 1.5869859MemoryTrain:  epoch  8, batch     0 | loss: 1.5659728MemoryTrain:  epoch  8, batch     1 | loss: 1.4670682MemoryTrain:  epoch  8, batch     2 | loss: 1.4927619MemoryTrain:  epoch  8, batch     3 | loss: 1.5341439MemoryTrain:  epoch  9, batch     0 | loss: 1.6714463MemoryTrain:  epoch  9, batch     1 | loss: 1.3949134MemoryTrain:  epoch  9, batch     2 | loss: 1.5497895MemoryTrain:  epoch  9, batch     3 | loss: 1.3262212
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 68.75%,  total acc: 75.00%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 76.56%   [EVAL] batch:    4 | acc: 43.75%,  total acc: 70.00%   [EVAL] batch:    5 | acc: 68.75%,  total acc: 69.79%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 72.32%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 74.22%   [EVAL] batch:    8 | acc: 75.00%,  total acc: 74.31%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 74.38%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 75.57%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 77.60%   [EVAL] batch:   12 | acc: 100.00%,  total acc: 79.33%   [EVAL] batch:   13 | acc: 100.00%,  total acc: 80.80%   [EVAL] batch:   14 | acc: 100.00%,  total acc: 82.08%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 83.20%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 84.19%   [EVAL] batch:   17 | acc: 25.00%,  total acc: 80.90%   
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    1 | acc: 37.50%,  total acc: 50.00%   [EVAL] batch:    2 | acc: 68.75%,  total acc: 56.25%   [EVAL] batch:    3 | acc: 56.25%,  total acc: 56.25%   [EVAL] batch:    4 | acc: 56.25%,  total acc: 56.25%   [EVAL] batch:    5 | acc: 68.75%,  total acc: 58.33%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 61.61%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 66.41%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 70.14%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 72.50%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 76.04%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 77.40%   [EVAL] batch:   13 | acc: 62.50%,  total acc: 76.34%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 76.25%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 75.00%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 74.31%   [EVAL] batch:   18 | acc: 75.00%,  total acc: 74.34%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 75.00%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 76.19%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 77.27%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 78.26%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 78.91%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 79.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 80.53%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 81.02%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 81.70%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 82.33%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 82.50%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 82.86%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 83.20%   [EVAL] batch:   32 | acc: 62.50%,  total acc: 82.58%   [EVAL] batch:   33 | acc: 31.25%,  total acc: 81.07%   [EVAL] batch:   34 | acc: 31.25%,  total acc: 79.64%   [EVAL] batch:   35 | acc: 43.75%,  total acc: 78.65%   [EVAL] batch:   36 | acc: 37.50%,  total acc: 77.53%   [EVAL] batch:   37 | acc: 81.25%,  total acc: 77.63%   [EVAL] batch:   38 | acc: 93.75%,  total acc: 78.04%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 78.59%   [EVAL] batch:   40 | acc: 75.00%,  total acc: 78.51%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 78.87%   [EVAL] batch:   42 | acc: 18.75%,  total acc: 77.47%   [EVAL] batch:   43 | acc: 12.50%,  total acc: 75.99%   [EVAL] batch:   44 | acc: 6.25%,  total acc: 74.44%   [EVAL] batch:   45 | acc: 18.75%,  total acc: 73.23%   [EVAL] batch:   46 | acc: 25.00%,  total acc: 72.21%   [EVAL] batch:   47 | acc: 68.75%,  total acc: 72.14%   [EVAL] batch:   48 | acc: 25.00%,  total acc: 71.17%   [EVAL] batch:   49 | acc: 31.25%,  total acc: 70.38%   [EVAL] batch:   50 | acc: 6.25%,  total acc: 69.12%   [EVAL] batch:   51 | acc: 31.25%,  total acc: 68.39%   [EVAL] batch:   52 | acc: 6.25%,  total acc: 67.22%   [EVAL] batch:   53 | acc: 37.50%,  total acc: 66.67%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 67.16%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 67.52%   [EVAL] batch:   56 | acc: 87.50%,  total acc: 67.87%   [EVAL] batch:   57 | acc: 68.75%,  total acc: 67.89%   [EVAL] batch:   58 | acc: 81.25%,  total acc: 68.11%   [EVAL] batch:   59 | acc: 81.25%,  total acc: 68.33%   [EVAL] batch:   60 | acc: 62.50%,  total acc: 68.24%   [EVAL] batch:   61 | acc: 75.00%,  total acc: 68.35%   [EVAL] batch:   62 | acc: 81.25%,  total acc: 68.55%   [EVAL] batch:   63 | acc: 81.25%,  total acc: 68.75%   [EVAL] batch:   64 | acc: 50.00%,  total acc: 68.46%   [EVAL] batch:   65 | acc: 68.75%,  total acc: 68.47%   [EVAL] batch:   66 | acc: 62.50%,  total acc: 68.38%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 68.84%   [EVAL] batch:   68 | acc: 75.00%,  total acc: 68.93%   [EVAL] batch:   69 | acc: 81.25%,  total acc: 69.11%   [EVAL] batch:   70 | acc: 75.00%,  total acc: 69.19%   [EVAL] batch:   71 | acc: 93.75%,  total acc: 69.53%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 69.95%   [EVAL] batch:   73 | acc: 100.00%,  total acc: 70.35%   [EVAL] batch:   74 | acc: 100.00%,  total acc: 70.75%   [EVAL] batch:   75 | acc: 100.00%,  total acc: 71.13%   [EVAL] batch:   76 | acc: 100.00%,  total acc: 71.51%   [EVAL] batch:   77 | acc: 75.00%,  total acc: 71.55%   
cur_acc:  ['0.8485', '0.8000', '0.8125', '0.8090']
his_acc:  ['0.8485', '0.8391', '0.7695', '0.7155']
CurrentTrain: epoch  0, batch     0 | loss: 8.0288582CurrentTrain: epoch  0, batch     1 | loss: 8.7756567CurrentTrain: epoch  1, batch     0 | loss: 7.9406271CurrentTrain: epoch  1, batch     1 | loss: 6.7328324CurrentTrain: epoch  2, batch     0 | loss: 6.8438220CurrentTrain: epoch  2, batch     1 | loss: 7.4808578CurrentTrain: epoch  3, batch     0 | loss: 6.8932352CurrentTrain: epoch  3, batch     1 | loss: 6.0588188CurrentTrain: epoch  4, batch     0 | loss: 6.7331343CurrentTrain: epoch  4, batch     1 | loss: 6.0047107CurrentTrain: epoch  5, batch     0 | loss: 6.0895586CurrentTrain: epoch  5, batch     1 | loss: 6.2086840CurrentTrain: epoch  6, batch     0 | loss: 6.1061997CurrentTrain: epoch  6, batch     1 | loss: 5.4573493CurrentTrain: epoch  7, batch     0 | loss: 5.5734158CurrentTrain: epoch  7, batch     1 | loss: 6.0284805CurrentTrain: epoch  8, batch     0 | loss: 5.5711164CurrentTrain: epoch  8, batch     1 | loss: 4.7625380CurrentTrain: epoch  9, batch     0 | loss: 5.3690968CurrentTrain: epoch  9, batch     1 | loss: 4.6500211
#### organization subsidiaries ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization subsidiaries" means an organization 's subsidiary is an organization.
Here is an example:
Relation: organization subsidiaries
Context: firstgroup , britain 's largest bus operator , entered the north american market in 1999 when it acquired ryder public transportation services inc. .
Head Entity: firstgroup
Tail Entity: ryder public transportation services inc.
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization subsidiaries":
Relation: organization subsidiaries  
Context: Alphabet Inc. has several subsidiaries, including YouTube, which it acquired in 2006 for $1.65 billion.  
Head Entity: Alphabet Inc.  
Tail Entity: YouTube  

Relation: organization subsidiaries  
Context: The Walt Disney Company owns Marvel Entertainment, which it purchased in 2009 for approximately $4 billion.  
Head Entity: The Walt Disney Company  
Tail Entity: Marvel Entertainment  
#### organization parents ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization parents" means an organization 's parent is an organization.
Here is an example:
Relation: organization parents
Context: aerolineas argentinas ' owner , madrid-based grupo marsans , is challenging the measure before a world bank arbitration body since it considers the takeover `` arbitrary and illegitimate , '' the company said wednesday night in a news release .
Head Entity: aerolineas argentinas
Tail Entity: grupo marsans
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization parents":
Relation: organization parents  
Context: the multinational technology company apple inc. has been acquired by the parent organization, tech giants holding, which aims to expand its portfolio in the consumer electronics market.  
Head Entity: apple inc.  
Tail Entity: tech giants holding  

Relation: organization parents  
Context: the renowned publishing house penguin random house is a subsidiary of the larger media conglomerate bertelsmann, which oversees various entertainment and publishing ventures worldwide.  
Head Entity: penguin random house  
Tail Entity: bertelsmann  
#### organization alternate names ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization alternate names" means a person 's alias is a person.
Here is an example:
Relation: organization alternate names
Context: the national security council -lrb- nsc -rrb- made the demand at talks chaired by president hamid karzai , who has been vocal in condemning international forces he believes are responsible for the incident last saturday in the eastern flashpoint of kunar .
Head Entity: national security council
Tail Entity: nsc
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization alternate names":
Relation: organization alternate names  
Context: the world health organization -lrb- who -rrb- has been instrumental in coordinating global responses to health emergencies, including the recent pandemic.  
Head Entity: world health organization  
Tail Entity: who  

Relation: organization alternate names  
Context: the federal bureau of investigation -lrb- fbi -rrb- is known for its role in investigating federal crimes and maintaining national security.  
Head Entity: federal bureau of investigation  
Tail Entity: fbi  
#### organization city of headquarters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization city of headquarters" means an organization is located in a city.
Here is an example:
Relation: organization city of headquarters
Context: ------ london 2008-05-20 07:23:45 utc enodis plc endorses sweetened takeover bid by us company manitowoc illinois tool works of glenville , illinois , which had offered 282 pence -lrb- us$ 551 euro3 54 -rrb- per share , said monday that it was considering its position .
Head Entity: illinois tool works
Tail Entity: glenville
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization city of headquarters":
Relation: organization city of headquarters  
Context: ------ in 2015, tech giant apple inc. announced plans to expand its operations in cupertino, california, where it has been headquartered since its founding.  
Head Entity: apple inc.  
Tail Entity: cupertino  

Relation: organization city of headquarters  
Context: ------ the multinational corporation samsung electronics is based in suwon, south korea, and has been a leader in the technology sector for decades.  
Head Entity: samsung electronics  
Tail Entity: suwon  
#### person siblings ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person siblings" means a person 's sibling is a person.
Here is an example:
Relation: person siblings
Context: forsberg , a political science professor at city college of new york , died oct. 19 in a bronx hospital of cancer , said her sister , celia seupel .
Head Entity: forsberg
Tail Entity: celia seupel
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person siblings":
Relation: person siblings  
Context: during the family reunion, john introduced his sister, emily, who had just returned from studying abroad.  
Head Entity: john  
Tail Entity: emily  

Relation: person siblings  
Context: at the award ceremony, sarah proudly accepted her trophy while her brother, michael, cheered from the audience.  
Head Entity: sarah  
Tail Entity: michael  
Mixup data size:  194
MixupTrain:  epoch  0, batch     0 | loss: 3.6467271MixupTrain:  epoch  0, batch     1 | loss: 2.8677457MixupTrain:  epoch  0, batch     2 | loss: 2.5091612MixupTrain:  epoch  0, batch     3 | loss: 2.8496541MixupTrain:  epoch  0, batch     4 | loss: 2.7455300MixupTrain:  epoch  0, batch     5 | loss: 2.6541437MixupTrain:  epoch  0, batch     6 | loss: 3.1857770MixupTrain:  epoch  0, batch     7 | loss: 2.8414172MixupTrain:  epoch  0, batch     8 | loss: 3.4925440MixupTrain:  epoch  0, batch     9 | loss: 2.7942809MixupTrain:  epoch  0, batch    10 | loss: 2.8841002MixupTrain:  epoch  0, batch    11 | loss: 2.4719947MixupTrain:  epoch  0, batch    12 | loss: 2.1172826
MemoryTrain:  epoch  0, batch     0 | loss: 1.5789369MemoryTrain:  epoch  0, batch     1 | loss: 2.9584956MemoryTrain:  epoch  0, batch     2 | loss: 2.8416209MemoryTrain:  epoch  0, batch     3 | loss: 2.2653818MemoryTrain:  epoch  0, batch     4 | loss: 2.6001353MemoryTrain:  epoch  1, batch     0 | loss: 3.2917833MemoryTrain:  epoch  1, batch     1 | loss: 1.6330261MemoryTrain:  epoch  1, batch     2 | loss: 2.1019936MemoryTrain:  epoch  1, batch     3 | loss: 2.4183702MemoryTrain:  epoch  1, batch     4 | loss: 3.0216677MemoryTrain:  epoch  2, batch     0 | loss: 2.4202003MemoryTrain:  epoch  2, batch     1 | loss: 1.7728279MemoryTrain:  epoch  2, batch     2 | loss: 2.0726702MemoryTrain:  epoch  2, batch     3 | loss: 2.6272593MemoryTrain:  epoch  2, batch     4 | loss: 1.7631721MemoryTrain:  epoch  3, batch     0 | loss: 1.5793805MemoryTrain:  epoch  3, batch     1 | loss: 2.1957757MemoryTrain:  epoch  3, batch     2 | loss: 1.9453232MemoryTrain:  epoch  3, batch     3 | loss: 2.2607012MemoryTrain:  epoch  3, batch     4 | loss: 1.5946895MemoryTrain:  epoch  4, batch     0 | loss: 1.5134811MemoryTrain:  epoch  4, batch     1 | loss: 2.0345366MemoryTrain:  epoch  4, batch     2 | loss: 1.6492221MemoryTrain:  epoch  4, batch     3 | loss: 2.0082536MemoryTrain:  epoch  4, batch     4 | loss: 1.7972664MemoryTrain:  epoch  5, batch     0 | loss: 1.8304672MemoryTrain:  epoch  5, batch     1 | loss: 1.6383346MemoryTrain:  epoch  5, batch     2 | loss: 1.3938611MemoryTrain:  epoch  5, batch     3 | loss: 1.6698675MemoryTrain:  epoch  5, batch     4 | loss: 1.7181994MemoryTrain:  epoch  6, batch     0 | loss: 1.5433701MemoryTrain:  epoch  6, batch     1 | loss: 1.5845165MemoryTrain:  epoch  6, batch     2 | loss: 1.5069860MemoryTrain:  epoch  6, batch     3 | loss: 1.7309346MemoryTrain:  epoch  6, batch     4 | loss: 1.6125959MemoryTrain:  epoch  7, batch     0 | loss: 1.7317526MemoryTrain:  epoch  7, batch     1 | loss: 1.4881048MemoryTrain:  epoch  7, batch     2 | loss: 1.5278559MemoryTrain:  epoch  7, batch     3 | loss: 1.4168549MemoryTrain:  epoch  7, batch     4 | loss: 1.4260252MemoryTrain:  epoch  8, batch     0 | loss: 1.5772233MemoryTrain:  epoch  8, batch     1 | loss: 1.4763603MemoryTrain:  epoch  8, batch     2 | loss: 1.4515700MemoryTrain:  epoch  8, batch     3 | loss: 1.4725000MemoryTrain:  epoch  8, batch     4 | loss: 1.4948450MemoryTrain:  epoch  9, batch     0 | loss: 1.2992560MemoryTrain:  epoch  9, batch     1 | loss: 1.3018821MemoryTrain:  epoch  9, batch     2 | loss: 1.5894804MemoryTrain:  epoch  9, batch     3 | loss: 1.5233331MemoryTrain:  epoch  9, batch     4 | loss: 1.5307167
[EVAL] batch:    0 | acc: 37.50%,  total acc: 37.50%   [EVAL] batch:    1 | acc: 43.75%,  total acc: 40.62%   [EVAL] batch:    2 | acc: 18.75%,  total acc: 33.33%   [EVAL] batch:    3 | acc: 25.00%,  total acc: 31.25%   [EVAL] batch:    4 | acc: 31.25%,  total acc: 31.25%   [EVAL] batch:    5 | acc: 31.25%,  total acc: 31.25%   [EVAL] batch:    6 | acc: 43.75%,  total acc: 33.04%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 38.28%   [EVAL] batch:    8 | acc: 37.50%,  total acc: 38.19%   [EVAL] batch:    9 | acc: 56.25%,  total acc: 40.00%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 43.75%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 46.35%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 48.56%   [EVAL] batch:   13 | acc: 100.00%,  total acc: 52.23%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 55.00%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 57.81%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 59.93%   [EVAL] batch:   17 | acc: 81.25%,  total acc: 61.11%   [EVAL] batch:   18 | acc: 25.00%,  total acc: 59.21%   [EVAL] batch:   19 | acc: 31.25%,  total acc: 57.81%   [EVAL] batch:   20 | acc: 31.25%,  total acc: 56.55%   [EVAL] batch:   21 | acc: 12.50%,  total acc: 54.55%   
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    1 | acc: 37.50%,  total acc: 50.00%   [EVAL] batch:    2 | acc: 68.75%,  total acc: 56.25%   [EVAL] batch:    3 | acc: 50.00%,  total acc: 54.69%   [EVAL] batch:    4 | acc: 62.50%,  total acc: 56.25%   [EVAL] batch:    5 | acc: 68.75%,  total acc: 58.33%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 61.61%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 66.41%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 70.14%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 72.50%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 76.04%   [EVAL] batch:   12 | acc: 50.00%,  total acc: 74.04%   [EVAL] batch:   13 | acc: 37.50%,  total acc: 71.43%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 71.67%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 70.70%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 70.96%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 70.49%   [EVAL] batch:   18 | acc: 75.00%,  total acc: 70.72%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 71.56%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 72.92%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 74.15%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 75.27%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 76.04%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 77.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 77.88%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 78.47%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 79.24%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 79.96%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 80.00%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 80.24%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 80.86%   [EVAL] batch:   32 | acc: 75.00%,  total acc: 80.68%   [EVAL] batch:   33 | acc: 31.25%,  total acc: 79.23%   [EVAL] batch:   34 | acc: 43.75%,  total acc: 78.21%   [EVAL] batch:   35 | acc: 43.75%,  total acc: 77.26%   [EVAL] batch:   36 | acc: 50.00%,  total acc: 76.52%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 76.81%   [EVAL] batch:   38 | acc: 93.75%,  total acc: 77.24%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 77.81%   [EVAL] batch:   40 | acc: 75.00%,  total acc: 77.74%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 78.27%   [EVAL] batch:   42 | acc: 12.50%,  total acc: 76.74%   [EVAL] batch:   43 | acc: 12.50%,  total acc: 75.28%   [EVAL] batch:   44 | acc: 6.25%,  total acc: 73.75%   [EVAL] batch:   45 | acc: 6.25%,  total acc: 72.28%   [EVAL] batch:   46 | acc: 18.75%,  total acc: 71.14%   [EVAL] batch:   47 | acc: 62.50%,  total acc: 70.96%   [EVAL] batch:   48 | acc: 18.75%,  total acc: 69.90%   [EVAL] batch:   49 | acc: 18.75%,  total acc: 68.88%   [EVAL] batch:   50 | acc: 0.00%,  total acc: 67.52%   [EVAL] batch:   51 | acc: 12.50%,  total acc: 66.47%   [EVAL] batch:   52 | acc: 0.00%,  total acc: 65.21%   [EVAL] batch:   53 | acc: 31.25%,  total acc: 64.58%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 65.11%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 65.62%   [EVAL] batch:   56 | acc: 87.50%,  total acc: 66.01%   [EVAL] batch:   57 | acc: 68.75%,  total acc: 66.06%   [EVAL] batch:   58 | acc: 81.25%,  total acc: 66.31%   [EVAL] batch:   59 | acc: 81.25%,  total acc: 66.56%   [EVAL] batch:   60 | acc: 62.50%,  total acc: 66.50%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 66.83%   [EVAL] batch:   62 | acc: 87.50%,  total acc: 67.16%   [EVAL] batch:   63 | acc: 87.50%,  total acc: 67.48%   [EVAL] batch:   64 | acc: 81.25%,  total acc: 67.69%   [EVAL] batch:   65 | acc: 75.00%,  total acc: 67.80%   [EVAL] batch:   66 | acc: 68.75%,  total acc: 67.82%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 68.29%   [EVAL] batch:   68 | acc: 75.00%,  total acc: 68.39%   [EVAL] batch:   69 | acc: 62.50%,  total acc: 68.30%   [EVAL] batch:   70 | acc: 62.50%,  total acc: 68.22%   [EVAL] batch:   71 | acc: 81.25%,  total acc: 68.40%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 68.84%   [EVAL] batch:   73 | acc: 100.00%,  total acc: 69.26%   [EVAL] batch:   74 | acc: 100.00%,  total acc: 69.67%   [EVAL] batch:   75 | acc: 100.00%,  total acc: 70.07%   [EVAL] batch:   76 | acc: 100.00%,  total acc: 70.45%   [EVAL] batch:   77 | acc: 81.25%,  total acc: 70.59%   [EVAL] batch:   78 | acc: 43.75%,  total acc: 70.25%   [EVAL] batch:   79 | acc: 37.50%,  total acc: 69.84%   [EVAL] batch:   80 | acc: 25.00%,  total acc: 69.29%   [EVAL] batch:   81 | acc: 25.00%,  total acc: 68.75%   [EVAL] batch:   82 | acc: 25.00%,  total acc: 68.22%   [EVAL] batch:   83 | acc: 31.25%,  total acc: 67.78%   [EVAL] batch:   84 | acc: 50.00%,  total acc: 67.57%   [EVAL] batch:   85 | acc: 75.00%,  total acc: 67.66%   [EVAL] batch:   86 | acc: 37.50%,  total acc: 67.31%   [EVAL] batch:   87 | acc: 68.75%,  total acc: 67.33%   [EVAL] batch:   88 | acc: 68.75%,  total acc: 67.35%   [EVAL] batch:   89 | acc: 81.25%,  total acc: 67.50%   [EVAL] batch:   90 | acc: 81.25%,  total acc: 67.65%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 68.00%   [EVAL] batch:   92 | acc: 93.75%,  total acc: 68.28%   [EVAL] batch:   93 | acc: 100.00%,  total acc: 68.62%   [EVAL] batch:   94 | acc: 87.50%,  total acc: 68.82%   [EVAL] batch:   95 | acc: 68.75%,  total acc: 68.82%   [EVAL] batch:   96 | acc: 25.00%,  total acc: 68.36%   [EVAL] batch:   97 | acc: 25.00%,  total acc: 67.92%   [EVAL] batch:   98 | acc: 37.50%,  total acc: 67.61%   [EVAL] batch:   99 | acc: 6.25%,  total acc: 67.00%   
cur_acc:  ['0.8485', '0.8000', '0.8125', '0.8090', '0.5455']
his_acc:  ['0.8485', '0.8391', '0.7695', '0.7155', '0.6700']
CurrentTrain: epoch  0, batch     0 | loss: 5.7450395CurrentTrain: epoch  0, batch     1 | loss: 6.7762470CurrentTrain: epoch  1, batch     0 | loss: 5.1746569CurrentTrain: epoch  1, batch     1 | loss: 3.8149827CurrentTrain: epoch  2, batch     0 | loss: 4.7078524CurrentTrain: epoch  2, batch     1 | loss: 3.4454143CurrentTrain: epoch  3, batch     0 | loss: 3.7362494CurrentTrain: epoch  3, batch     1 | loss: 4.2406917CurrentTrain: epoch  4, batch     0 | loss: 3.7319589CurrentTrain: epoch  4, batch     1 | loss: 3.7207332CurrentTrain: epoch  5, batch     0 | loss: 3.8616390CurrentTrain: epoch  5, batch     1 | loss: 2.9170084CurrentTrain: epoch  6, batch     0 | loss: 3.2751131CurrentTrain: epoch  6, batch     1 | loss: 3.7059796CurrentTrain: epoch  7, batch     0 | loss: 3.1231048CurrentTrain: epoch  7, batch     1 | loss: 2.9284725CurrentTrain: epoch  8, batch     0 | loss: 3.1896262CurrentTrain: epoch  8, batch     1 | loss: 2.7012045CurrentTrain: epoch  9, batch     0 | loss: 3.0427909CurrentTrain: epoch  9, batch     1 | loss: 2.6351004
#### person country of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person country of birth" means a person was born in a country.
Here is an example:
Relation: person country of birth
Context: de maiziere noted that germany took in another former inmate from guantanamo in 2006 -- murat kurnaz , a turkish national who was born and grew up in germany .
Head Entity: murat kurnaz
Tail Entity: germany
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person country of birth":
Relation: person country of birth  
Context: born in 1985 in the bustling city of new delhi, arjun kapoor has always been proud of his indian heritage and culture.  
Head Entity: arjun kapoor  
Tail Entity: india  

Relation: person country of birth  
Context: during the interview, she revealed that she was born in the picturesque town of auckland, new zealand, which she considers her true home.  
Head Entity: she  
Tail Entity: new zealand  
#### organization website ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization website" means an organization 's website is an url.
Here is an example:
Relation: organization website
Context: http://www.pentaxsailaway.com/ paul j. sail away with pentax sweepstakes
Head Entity: pentax
Tail Entity: http://www.pentaxsailaway.com/
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization website":
Relation: organization website  
Context: Visit the official site at https://www.techinnovators.com for more information on their latest products.  
Head Entity: Tech Innovators  
Tail Entity: https://www.techinnovators.com  

Relation: organization website  
Context: For updates and news, check out the blog at http://www.greenearth.org/blog.  
Head Entity: Green Earth  
Tail Entity: http://www.greenearth.org/  
#### organization shareholders ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization shareholders" means an organization was invested by a person.
Here is an example:
Relation: organization shareholders
Context: ------ liberty media acquired a 41 percent stake in directv in late february by exchanging it for a 16 percent stake in news corp plus $ 625 million -lrb- euro402 5 million -rrb- in cash .
Head Entity: directv
Tail Entity: liberty media
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization shareholders":
Relation: organization shareholders  
Context: In 2020, Amazon purchased a significant share of Zoox, a self-driving car startup, to enhance its logistics capabilities.  
Head Entity: Zoox  
Tail Entity: Amazon  

Relation: organization shareholders  
Context: Tesla announced that it had sold a portion of its shares to various investors, including a major investment from Fidelity.  
Head Entity: Tesla  
Tail Entity: Fidelity  
#### organization dissolved ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization dissolved" means an organization was dissolved in a date.
Here is an example:
Relation: organization dissolved
Context: the tse suffered its worst-ever system crash in november 2005 which paralyzed the world 's second largest bourse and forced it to shelve plan for a listing of its own .
Head Entity: tse
Tail Entity: november 2005
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization dissolved":
Relation: organization dissolved  
Context: the once-prominent tech startup, Innovatech, officially ceased operations in early 2020 after struggling to secure funding.  
Head Entity: Innovatech  
Tail Entity: early 2020  

Relation: organization dissolved  
Context: after years of financial difficulties, the local arts council announced its closure in the spring of 2018, leaving many artists without support.  
Head Entity: local arts council  
Tail Entity: spring of 2018  
#### organization founded by ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization founded by" means an organization was found by a person.
Here is an example:
Relation: organization founded by
Context: born in new york , she inherited most of her fortune from her father , ted arison , who founded carnival cruise lines and also owned the miami heat basketball team .
Head Entity: carnival cruise lines
Tail Entity: ted arison
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization founded by":
Relation: organization founded by  
Context: in 1975, steve jobs and steve wozniak started apple inc., which revolutionized the personal computer industry and changed the way people interact with technology.  
Head Entity: apple inc.  
Tail Entity: steve jobs  

Relation: organization founded by  
Context: the famous fashion brand gucci was established in florence in 1921 by guccio gucci, who aimed to create high-quality leather goods.  
Head Entity: gucci  
Tail Entity: guccio gucci  
Mixup data size:  224
MixupTrain:  epoch  0, batch     0 | loss: 2.1112321MixupTrain:  epoch  0, batch     1 | loss: 2.4274493MixupTrain:  epoch  0, batch     2 | loss: 2.0634082MixupTrain:  epoch  0, batch     3 | loss: 2.5883253MixupTrain:  epoch  0, batch     4 | loss: 3.1059317MixupTrain:  epoch  0, batch     5 | loss: 2.3740694MixupTrain:  epoch  0, batch     6 | loss: 3.3721183MixupTrain:  epoch  0, batch     7 | loss: 2.5769283MixupTrain:  epoch  0, batch     8 | loss: 2.4921907MixupTrain:  epoch  0, batch     9 | loss: 2.3332539MixupTrain:  epoch  0, batch    10 | loss: 2.4646972MixupTrain:  epoch  0, batch    11 | loss: 2.5475954MixupTrain:  epoch  0, batch    12 | loss: 2.4420324MixupTrain:  epoch  0, batch    13 | loss: 2.7051496
MemoryTrain:  epoch  0, batch     0 | loss: 2.0397258MemoryTrain:  epoch  0, batch     1 | loss: 2.6376715MemoryTrain:  epoch  0, batch     2 | loss: 1.9120152MemoryTrain:  epoch  0, batch     3 | loss: 2.2786465MemoryTrain:  epoch  0, batch     4 | loss: 2.0577421MemoryTrain:  epoch  0, batch     5 | loss: 2.4449239MemoryTrain:  epoch  1, batch     0 | loss: 2.0890622MemoryTrain:  epoch  1, batch     1 | loss: 3.0363564MemoryTrain:  epoch  1, batch     2 | loss: 1.7582185MemoryTrain:  epoch  1, batch     3 | loss: 2.2104177MemoryTrain:  epoch  1, batch     4 | loss: 1.3982974MemoryTrain:  epoch  1, batch     5 | loss: 2.5265279MemoryTrain:  epoch  2, batch     0 | loss: 1.7961688MemoryTrain:  epoch  2, batch     1 | loss: 2.0728235MemoryTrain:  epoch  2, batch     2 | loss: 1.9405391MemoryTrain:  epoch  2, batch     3 | loss: 1.6871539MemoryTrain:  epoch  2, batch     4 | loss: 2.2147746MemoryTrain:  epoch  2, batch     5 | loss: 2.2262394MemoryTrain:  epoch  3, batch     0 | loss: 1.3645666MemoryTrain:  epoch  3, batch     1 | loss: 2.0361600MemoryTrain:  epoch  3, batch     2 | loss: 2.1621153MemoryTrain:  epoch  3, batch     3 | loss: 1.7217295MemoryTrain:  epoch  3, batch     4 | loss: 1.8100495MemoryTrain:  epoch  3, batch     5 | loss: 1.4300903MemoryTrain:  epoch  4, batch     0 | loss: 1.5919447MemoryTrain:  epoch  4, batch     1 | loss: 1.6510100MemoryTrain:  epoch  4, batch     2 | loss: 1.8028228MemoryTrain:  epoch  4, batch     3 | loss: 1.6175184MemoryTrain:  epoch  4, batch     4 | loss: 1.5409738MemoryTrain:  epoch  4, batch     5 | loss: 1.5488144MemoryTrain:  epoch  5, batch     0 | loss: 1.5286007MemoryTrain:  epoch  5, batch     1 | loss: 1.5766736MemoryTrain:  epoch  5, batch     2 | loss: 1.7537835MemoryTrain:  epoch  5, batch     3 | loss: 1.5216181MemoryTrain:  epoch  5, batch     4 | loss: 1.6901516MemoryTrain:  epoch  5, batch     5 | loss: 1.3638972MemoryTrain:  epoch  6, batch     0 | loss: 1.6665479MemoryTrain:  epoch  6, batch     1 | loss: 1.3439960MemoryTrain:  epoch  6, batch     2 | loss: 1.6850319MemoryTrain:  epoch  6, batch     3 | loss: 1.5002637MemoryTrain:  epoch  6, batch     4 | loss: 1.5619600MemoryTrain:  epoch  6, batch     5 | loss: 1.5050414MemoryTrain:  epoch  7, batch     0 | loss: 1.4950225MemoryTrain:  epoch  7, batch     1 | loss: 1.3056259MemoryTrain:  epoch  7, batch     2 | loss: 1.7304906MemoryTrain:  epoch  7, batch     3 | loss: 1.2546544MemoryTrain:  epoch  7, batch     4 | loss: 1.6151408MemoryTrain:  epoch  7, batch     5 | loss: 1.4577068MemoryTrain:  epoch  8, batch     0 | loss: 1.5912249MemoryTrain:  epoch  8, batch     1 | loss: 1.5117512MemoryTrain:  epoch  8, batch     2 | loss: 1.2953781MemoryTrain:  epoch  8, batch     3 | loss: 1.3727098MemoryTrain:  epoch  8, batch     4 | loss: 1.5064749MemoryTrain:  epoch  8, batch     5 | loss: 1.3472884MemoryTrain:  epoch  9, batch     0 | loss: 1.3276193MemoryTrain:  epoch  9, batch     1 | loss: 1.3667736MemoryTrain:  epoch  9, batch     2 | loss: 1.3991096MemoryTrain:  epoch  9, batch     3 | loss: 1.3813865MemoryTrain:  epoch  9, batch     4 | loss: 1.4116025MemoryTrain:  epoch  9, batch     5 | loss: 1.4407097
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:    2 | acc: 68.75%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 18.75%,  total acc: 68.75%   [EVAL] batch:    4 | acc: 25.00%,  total acc: 60.00%   [EVAL] batch:    5 | acc: 25.00%,  total acc: 54.17%   [EVAL] batch:    6 | acc: 0.00%,  total acc: 46.43%   [EVAL] batch:    7 | acc: 0.00%,  total acc: 40.62%   
[EVAL] batch:    0 | acc: 18.75%,  total acc: 18.75%   [EVAL] batch:    1 | acc: 25.00%,  total acc: 21.88%   [EVAL] batch:    2 | acc: 43.75%,  total acc: 29.17%   [EVAL] batch:    3 | acc: 12.50%,  total acc: 25.00%   [EVAL] batch:    4 | acc: 18.75%,  total acc: 23.75%   [EVAL] batch:    5 | acc: 18.75%,  total acc: 22.92%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 30.36%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 39.06%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 45.14%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 49.38%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 52.84%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 55.73%   [EVAL] batch:   12 | acc: 50.00%,  total acc: 55.29%   [EVAL] batch:   13 | acc: 37.50%,  total acc: 54.02%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 55.42%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 55.47%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 56.62%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 56.94%   [EVAL] batch:   18 | acc: 75.00%,  total acc: 57.89%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 59.38%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 61.31%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 63.07%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 64.40%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 65.62%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 67.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 68.27%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 69.21%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 70.31%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 71.34%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 71.67%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 72.18%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 73.05%   [EVAL] batch:   32 | acc: 62.50%,  total acc: 72.73%   [EVAL] batch:   33 | acc: 25.00%,  total acc: 71.32%   [EVAL] batch:   34 | acc: 25.00%,  total acc: 70.00%   [EVAL] batch:   35 | acc: 43.75%,  total acc: 69.27%   [EVAL] batch:   36 | acc: 31.25%,  total acc: 68.24%   [EVAL] batch:   37 | acc: 68.75%,  total acc: 68.26%   [EVAL] batch:   38 | acc: 81.25%,  total acc: 68.59%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 69.38%   [EVAL] batch:   40 | acc: 81.25%,  total acc: 69.66%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 70.39%   [EVAL] batch:   42 | acc: 12.50%,  total acc: 69.04%   [EVAL] batch:   43 | acc: 6.25%,  total acc: 67.61%   [EVAL] batch:   44 | acc: 6.25%,  total acc: 66.25%   [EVAL] batch:   45 | acc: 0.00%,  total acc: 64.81%   [EVAL] batch:   46 | acc: 18.75%,  total acc: 63.83%   [EVAL] batch:   47 | acc: 62.50%,  total acc: 63.80%   [EVAL] batch:   48 | acc: 18.75%,  total acc: 62.88%   [EVAL] batch:   49 | acc: 25.00%,  total acc: 62.12%   [EVAL] batch:   50 | acc: 0.00%,  total acc: 60.91%   [EVAL] batch:   51 | acc: 12.50%,  total acc: 59.98%   [EVAL] batch:   52 | acc: 0.00%,  total acc: 58.84%   [EVAL] batch:   53 | acc: 31.25%,  total acc: 58.33%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 58.98%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 59.49%   [EVAL] batch:   56 | acc: 81.25%,  total acc: 59.87%   [EVAL] batch:   57 | acc: 75.00%,  total acc: 60.13%   [EVAL] batch:   58 | acc: 81.25%,  total acc: 60.49%   [EVAL] batch:   59 | acc: 87.50%,  total acc: 60.94%   [EVAL] batch:   60 | acc: 68.75%,  total acc: 61.07%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 61.49%   [EVAL] batch:   62 | acc: 100.00%,  total acc: 62.10%   [EVAL] batch:   63 | acc: 87.50%,  total acc: 62.50%   [EVAL] batch:   64 | acc: 81.25%,  total acc: 62.79%   [EVAL] batch:   65 | acc: 75.00%,  total acc: 62.97%   [EVAL] batch:   66 | acc: 75.00%,  total acc: 63.15%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 63.69%   [EVAL] batch:   68 | acc: 62.50%,  total acc: 63.68%   [EVAL] batch:   69 | acc: 43.75%,  total acc: 63.39%   [EVAL] batch:   70 | acc: 62.50%,  total acc: 63.38%   [EVAL] batch:   71 | acc: 75.00%,  total acc: 63.54%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 64.04%   [EVAL] batch:   73 | acc: 100.00%,  total acc: 64.53%   [EVAL] batch:   74 | acc: 100.00%,  total acc: 65.00%   [EVAL] batch:   75 | acc: 100.00%,  total acc: 65.46%   [EVAL] batch:   76 | acc: 100.00%,  total acc: 65.91%   [EVAL] batch:   77 | acc: 75.00%,  total acc: 66.03%   [EVAL] batch:   78 | acc: 25.00%,  total acc: 65.51%   [EVAL] batch:   79 | acc: 12.50%,  total acc: 64.84%   [EVAL] batch:   80 | acc: 6.25%,  total acc: 64.12%   [EVAL] batch:   81 | acc: 0.00%,  total acc: 63.34%   [EVAL] batch:   82 | acc: 0.00%,  total acc: 62.58%   [EVAL] batch:   83 | acc: 0.00%,  total acc: 61.83%   [EVAL] batch:   84 | acc: 43.75%,  total acc: 61.62%   [EVAL] batch:   85 | acc: 68.75%,  total acc: 61.70%   [EVAL] batch:   86 | acc: 50.00%,  total acc: 61.57%   [EVAL] batch:   87 | acc: 68.75%,  total acc: 61.65%   [EVAL] batch:   88 | acc: 68.75%,  total acc: 61.73%   [EVAL] batch:   89 | acc: 81.25%,  total acc: 61.94%   [EVAL] batch:   90 | acc: 87.50%,  total acc: 62.23%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 62.64%   [EVAL] batch:   92 | acc: 93.75%,  total acc: 62.97%   [EVAL] batch:   93 | acc: 100.00%,  total acc: 63.36%   [EVAL] batch:   94 | acc: 87.50%,  total acc: 63.62%   [EVAL] batch:   95 | acc: 75.00%,  total acc: 63.74%   [EVAL] batch:   96 | acc: 37.50%,  total acc: 63.47%   [EVAL] batch:   97 | acc: 25.00%,  total acc: 63.07%   [EVAL] batch:   98 | acc: 43.75%,  total acc: 62.88%   [EVAL] batch:   99 | acc: 75.00%,  total acc: 63.00%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 63.37%   [EVAL] batch:  101 | acc: 75.00%,  total acc: 63.48%   [EVAL] batch:  102 | acc: 25.00%,  total acc: 63.11%   [EVAL] batch:  103 | acc: 25.00%,  total acc: 62.74%   [EVAL] batch:  104 | acc: 31.25%,  total acc: 62.44%   [EVAL] batch:  105 | acc: 0.00%,  total acc: 61.85%   [EVAL] batch:  106 | acc: 0.00%,  total acc: 61.27%   
cur_acc:  ['0.8485', '0.8000', '0.8125', '0.8090', '0.5455', '0.4062']
his_acc:  ['0.8485', '0.8391', '0.7695', '0.7155', '0.6700', '0.6127']
CurrentTrain: epoch  0, batch     0 | loss: 5.0352950CurrentTrain: epoch  0, batch     1 | loss: 5.1291981CurrentTrain: epoch  1, batch     0 | loss: 3.7109241CurrentTrain: epoch  1, batch     1 | loss: 3.7748897CurrentTrain: epoch  2, batch     0 | loss: 3.5805597CurrentTrain: epoch  2, batch     1 | loss: 3.0744755CurrentTrain: epoch  3, batch     0 | loss: 2.8910222CurrentTrain: epoch  3, batch     1 | loss: 2.7212708CurrentTrain: epoch  4, batch     0 | loss: 2.5657430CurrentTrain: epoch  4, batch     1 | loss: 2.5307763CurrentTrain: epoch  5, batch     0 | loss: 2.3929968CurrentTrain: epoch  5, batch     1 | loss: 2.2753308CurrentTrain: epoch  6, batch     0 | loss: 2.1477010CurrentTrain: epoch  6, batch     1 | loss: 2.2099104CurrentTrain: epoch  7, batch     0 | loss: 2.1608865CurrentTrain: epoch  7, batch     1 | loss: 2.2204950CurrentTrain: epoch  8, batch     0 | loss: 2.1062083CurrentTrain: epoch  8, batch     1 | loss: 1.9604717CurrentTrain: epoch  9, batch     0 | loss: 1.9240140CurrentTrain: epoch  9, batch     1 | loss: 1.8760983
#### person cause of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person cause of death" means a person was died of an event.
Here is an example:
Relation: person cause of death
Context: pamela gardner ahearn , who served nine years as chief of protocol at the us house of representatives after earlier experience with the state department 's office of protocol and as elizabeth taylor 's executive assistant , died march 26 of a heart attack at her home in alexandria , va .
Head Entity: pamela gardner ahearn
Tail Entity: heart attack
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person cause of death":
Relation: person cause of death  
Context: john smith, a renowned scientist known for his groundbreaking research in genetics, passed away on july 15 due to complications from pneumonia while receiving treatment at a local hospital.  
Head Entity: john smith  
Tail Entity: pneumonia  

Relation: person cause of death  
Context: the famous actor, robert jones, tragically lost his life in a car accident on february 10, leaving behind a legacy of memorable performances and a grieving family.  
Head Entity: robert jones  
Tail Entity: car accident  
#### organization political religious affiliation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization political religious affiliation" means an organization is the member of a religion.
Here is an example:
Relation: organization political religious affiliation
Context: the militia has also suffered from allegations of being involved in clashes with rival shiite militias , particularly the badr corp , the military wing linked to supreme iraqi islamic council , the party led by powerful politician abdel aziz al-hakim .
Head Entity: badr corp
Tail Entity: shiite
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization political religious affiliation":
Relation: organization political religious affiliation  
Context: The Christian Democratic Union has been a significant player in German politics, often aligning itself with various religious groups to promote policies that reflect its Christian values.  
Head Entity: Christian Democratic Union  
Tail Entity: Christian  

Relation: organization political religious affiliation  
Context: The Islamic Society of North America has been actively involved in advocating for Muslim rights and representation in the political landscape of the United States.  
Head Entity: Islamic Society of North America  
Tail Entity: Muslim  
#### organization stateorprovince of headquarters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization stateorprovince of headquarters" means an organization is located in a state or province.
Here is an example:
Relation: organization stateorprovince of headquarters
Context: washington , nov 30 -lrb- xinhua -rrb- -- nasa has skipped space shuttle endeavour 's second landing opportunity at kennedy space center in florida on sunday afternoon due to bad weather , according to nasa tv .
Head Entity: kennedy space center
Tail Entity: florida
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization stateorprovince of headquarters":
Relation: organization stateorprovince of headquarters  
Context: the headquarters of the multinational technology company is located in cupertino, california, where it has been a significant player in the tech industry for over two decades.  
Head Entity: cupertino  
Tail Entity: california  

Relation: organization stateorprovince of headquarters  
Context: after years of expansion, the non-profit organization has established its main office in a historic building in downtown boston, massachusetts, which serves as a hub for its various community programs.  
Head Entity: downtown boston  
Tail Entity: massachusetts  
#### person other family ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person other family" means a person 's relative is a person.
Here is an example:
Relation: person other family
Context: parren mitchell 's sister-in-law , juanita jackson mitchell , was the long - time head and legal counsel of the maryland naacp .
Head Entity: parren mitchell
Tail Entity: juanita jackson mitchell
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person other family":
Relation: person other family  
Context: During the family reunion, it was revealed that Sarah's cousin, Emily, had recently graduated from college with honors.  
Head Entity: Sarah  
Tail Entity: Emily  

Relation: person other family  
Context: Michael often reminisces about the summer vacations spent at his grandmother's house with his aunt, Linda, who always baked the best cookies.  
Head Entity: Michael  
Tail Entity: Linda  
#### person city of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person city of death" means a person was died in a city.
Here is an example:
Relation: person city of death
Context: irene morgan kirkaldy , 90 , who died of alzheimer 's disease aug. 10 at her home in gloucester , va. , quietly changed history in 1944 when she refused to give up her seat on a crowded greyhound bus to a white couple .
Head Entity: her
Tail Entity: gloucester
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person city of death":
Relation: person city of death  
Context: john smith, a renowned author, passed away on march 5 in the vibrant city of new orleans, where he spent his final years writing his last novel.  
Head Entity: john smith  
Tail Entity: new orleans  

Relation: person city of death  
Context: after a long battle with cancer, elizabeth taylor died on march 23 in the luxurious surroundings of los angeles, a city she called home for decades.  
Head Entity: elizabeth taylor  
Tail Entity: los angeles  
Mixup data size:  254
MixupTrain:  epoch  0, batch     0 | loss: 2.0983846MixupTrain:  epoch  0, batch     1 | loss: 1.8381008MixupTrain:  epoch  0, batch     2 | loss: 2.3130630MixupTrain:  epoch  0, batch     3 | loss: 2.1886368MixupTrain:  epoch  0, batch     4 | loss: 1.9919807MixupTrain:  epoch  0, batch     5 | loss: 1.9052698MixupTrain:  epoch  0, batch     6 | loss: 2.3661150MixupTrain:  epoch  0, batch     7 | loss: 2.0723136MixupTrain:  epoch  0, batch     8 | loss: 1.9261141MixupTrain:  epoch  0, batch     9 | loss: 2.2217731MixupTrain:  epoch  0, batch    10 | loss: 1.7397799MixupTrain:  epoch  0, batch    11 | loss: 2.8789272MixupTrain:  epoch  0, batch    12 | loss: 2.7260668MixupTrain:  epoch  0, batch    13 | loss: 2.0198211MixupTrain:  epoch  0, batch    14 | loss: 2.5897245MixupTrain:  epoch  0, batch    15 | loss: 2.0122889
MemoryTrain:  epoch  0, batch     0 | loss: 1.7975322MemoryTrain:  epoch  0, batch     1 | loss: 1.6551447MemoryTrain:  epoch  0, batch     2 | loss: 2.1177969MemoryTrain:  epoch  0, batch     3 | loss: 2.4281745MemoryTrain:  epoch  0, batch     4 | loss: 2.4596424MemoryTrain:  epoch  0, batch     5 | loss: 2.4262176MemoryTrain:  epoch  0, batch     6 | loss: 2.3269746MemoryTrain:  epoch  1, batch     0 | loss: 1.9797072MemoryTrain:  epoch  1, batch     1 | loss: 1.7166414MemoryTrain:  epoch  1, batch     2 | loss: 2.0206575MemoryTrain:  epoch  1, batch     3 | loss: 2.0153356MemoryTrain:  epoch  1, batch     4 | loss: 2.1525097MemoryTrain:  epoch  1, batch     5 | loss: 1.9293824MemoryTrain:  epoch  1, batch     6 | loss: 2.5354252MemoryTrain:  epoch  2, batch     0 | loss: 1.7340993MemoryTrain:  epoch  2, batch     1 | loss: 1.7770290MemoryTrain:  epoch  2, batch     2 | loss: 1.7504784MemoryTrain:  epoch  2, batch     3 | loss: 1.9847059MemoryTrain:  epoch  2, batch     4 | loss: 1.3997118MemoryTrain:  epoch  2, batch     5 | loss: 1.8848128MemoryTrain:  epoch  2, batch     6 | loss: 1.7020538MemoryTrain:  epoch  3, batch     0 | loss: 2.0707588MemoryTrain:  epoch  3, batch     1 | loss: 1.9117860MemoryTrain:  epoch  3, batch     2 | loss: 1.9585578MemoryTrain:  epoch  3, batch     3 | loss: 1.5340775MemoryTrain:  epoch  3, batch     4 | loss: 1.4148760MemoryTrain:  epoch  3, batch     5 | loss: 1.3745193MemoryTrain:  epoch  3, batch     6 | loss: 1.3784044MemoryTrain:  epoch  4, batch     0 | loss: 1.4245913MemoryTrain:  epoch  4, batch     1 | loss: 1.6443245MemoryTrain:  epoch  4, batch     2 | loss: 1.5189536MemoryTrain:  epoch  4, batch     3 | loss: 1.5909474MemoryTrain:  epoch  4, batch     4 | loss: 1.6314579MemoryTrain:  epoch  4, batch     5 | loss: 1.5755773MemoryTrain:  epoch  4, batch     6 | loss: 1.3492796MemoryTrain:  epoch  5, batch     0 | loss: 1.4142716MemoryTrain:  epoch  5, batch     1 | loss: 1.3494008MemoryTrain:  epoch  5, batch     2 | loss: 1.4538064MemoryTrain:  epoch  5, batch     3 | loss: 1.4501908MemoryTrain:  epoch  5, batch     4 | loss: 1.3940225MemoryTrain:  epoch  5, batch     5 | loss: 1.6331556MemoryTrain:  epoch  5, batch     6 | loss: 1.5095766MemoryTrain:  epoch  6, batch     0 | loss: 1.5754292MemoryTrain:  epoch  6, batch     1 | loss: 1.2944973MemoryTrain:  epoch  6, batch     2 | loss: 1.4729052MemoryTrain:  epoch  6, batch     3 | loss: 1.2874279MemoryTrain:  epoch  6, batch     4 | loss: 1.2826151MemoryTrain:  epoch  6, batch     5 | loss: 1.6934150MemoryTrain:  epoch  6, batch     6 | loss: 1.4473614MemoryTrain:  epoch  7, batch     0 | loss: 1.2977295MemoryTrain:  epoch  7, batch     1 | loss: 1.4726584MemoryTrain:  epoch  7, batch     2 | loss: 1.2460012MemoryTrain:  epoch  7, batch     3 | loss: 1.5027694MemoryTrain:  epoch  7, batch     4 | loss: 1.3776436MemoryTrain:  epoch  7, batch     5 | loss: 1.3303350MemoryTrain:  epoch  7, batch     6 | loss: 1.5683362MemoryTrain:  epoch  8, batch     0 | loss: 1.4184656MemoryTrain:  epoch  8, batch     1 | loss: 1.4413757MemoryTrain:  epoch  8, batch     2 | loss: 1.5993841MemoryTrain:  epoch  8, batch     3 | loss: 1.2969875MemoryTrain:  epoch  8, batch     4 | loss: 1.3848331MemoryTrain:  epoch  8, batch     5 | loss: 1.3147873MemoryTrain:  epoch  8, batch     6 | loss: 1.4916842MemoryTrain:  epoch  9, batch     0 | loss: 1.2606512MemoryTrain:  epoch  9, batch     1 | loss: 1.3436191MemoryTrain:  epoch  9, batch     2 | loss: 1.4246674MemoryTrain:  epoch  9, batch     3 | loss: 1.3694369MemoryTrain:  epoch  9, batch     4 | loss: 1.2486866MemoryTrain:  epoch  9, batch     5 | loss: 1.2187096MemoryTrain:  epoch  9, batch     6 | loss: 1.5865810
[EVAL] batch:    0 | acc: 50.00%,  total acc: 50.00%   [EVAL] batch:    1 | acc: 62.50%,  total acc: 56.25%   [EVAL] batch:    2 | acc: 43.75%,  total acc: 52.08%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 57.81%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 60.00%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 65.62%   [EVAL] batch:    6 | acc: 56.25%,  total acc: 64.29%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 65.62%   [EVAL] batch:    8 | acc: 37.50%,  total acc: 62.50%   [EVAL] batch:    9 | acc: 56.25%,  total acc: 61.88%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 65.34%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 66.15%   [EVAL] batch:   12 | acc: 43.75%,  total acc: 64.42%   
[EVAL] batch:    0 | acc: 6.25%,  total acc: 6.25%   [EVAL] batch:    1 | acc: 12.50%,  total acc: 9.38%   [EVAL] batch:    2 | acc: 37.50%,  total acc: 18.75%   [EVAL] batch:    3 | acc: 12.50%,  total acc: 17.19%   [EVAL] batch:    4 | acc: 12.50%,  total acc: 16.25%   [EVAL] batch:    5 | acc: 12.50%,  total acc: 15.62%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 24.11%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 33.59%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 40.28%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 45.00%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 48.86%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 52.08%   [EVAL] batch:   12 | acc: 56.25%,  total acc: 52.40%   [EVAL] batch:   13 | acc: 37.50%,  total acc: 51.34%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 52.92%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 53.12%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 54.41%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 54.86%   [EVAL] batch:   18 | acc: 75.00%,  total acc: 55.92%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 57.50%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 59.52%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 61.36%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 62.77%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 64.06%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 65.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 66.83%   [EVAL] batch:   26 | acc: 87.50%,  total acc: 67.59%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 68.75%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 69.83%   [EVAL] batch:   29 | acc: 75.00%,  total acc: 70.00%   [EVAL] batch:   30 | acc: 81.25%,  total acc: 70.36%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 71.29%   [EVAL] batch:   32 | acc: 75.00%,  total acc: 71.40%   [EVAL] batch:   33 | acc: 25.00%,  total acc: 70.04%   [EVAL] batch:   34 | acc: 50.00%,  total acc: 69.46%   [EVAL] batch:   35 | acc: 56.25%,  total acc: 69.10%   [EVAL] batch:   36 | acc: 43.75%,  total acc: 68.41%   [EVAL] batch:   37 | acc: 81.25%,  total acc: 68.75%   [EVAL] batch:   38 | acc: 81.25%,  total acc: 69.07%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 69.84%   [EVAL] batch:   40 | acc: 81.25%,  total acc: 70.12%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 70.83%   [EVAL] batch:   42 | acc: 25.00%,  total acc: 69.77%   [EVAL] batch:   43 | acc: 0.00%,  total acc: 68.18%   [EVAL] batch:   44 | acc: 0.00%,  total acc: 66.67%   [EVAL] batch:   45 | acc: 0.00%,  total acc: 65.22%   [EVAL] batch:   46 | acc: 12.50%,  total acc: 64.10%   [EVAL] batch:   47 | acc: 56.25%,  total acc: 63.93%   [EVAL] batch:   48 | acc: 18.75%,  total acc: 63.01%   [EVAL] batch:   49 | acc: 12.50%,  total acc: 62.00%   [EVAL] batch:   50 | acc: 0.00%,  total acc: 60.78%   [EVAL] batch:   51 | acc: 12.50%,  total acc: 59.86%   [EVAL] batch:   52 | acc: 6.25%,  total acc: 58.84%   [EVAL] batch:   53 | acc: 31.25%,  total acc: 58.33%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 58.98%   [EVAL] batch:   55 | acc: 81.25%,  total acc: 59.38%   [EVAL] batch:   56 | acc: 75.00%,  total acc: 59.65%   [EVAL] batch:   57 | acc: 68.75%,  total acc: 59.81%   [EVAL] batch:   58 | acc: 81.25%,  total acc: 60.17%   [EVAL] batch:   59 | acc: 75.00%,  total acc: 60.42%   [EVAL] batch:   60 | acc: 12.50%,  total acc: 59.63%   [EVAL] batch:   61 | acc: 0.00%,  total acc: 58.67%   [EVAL] batch:   62 | acc: 12.50%,  total acc: 57.94%   [EVAL] batch:   63 | acc: 6.25%,  total acc: 57.13%   [EVAL] batch:   64 | acc: 18.75%,  total acc: 56.54%   [EVAL] batch:   65 | acc: 6.25%,  total acc: 55.78%   [EVAL] batch:   66 | acc: 18.75%,  total acc: 55.22%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 55.88%   [EVAL] batch:   68 | acc: 56.25%,  total acc: 55.89%   [EVAL] batch:   69 | acc: 43.75%,  total acc: 55.71%   [EVAL] batch:   70 | acc: 50.00%,  total acc: 55.63%   [EVAL] batch:   71 | acc: 68.75%,  total acc: 55.82%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 56.42%   [EVAL] batch:   73 | acc: 100.00%,  total acc: 57.01%   [EVAL] batch:   74 | acc: 100.00%,  total acc: 57.58%   [EVAL] batch:   75 | acc: 100.00%,  total acc: 58.14%   [EVAL] batch:   76 | acc: 100.00%,  total acc: 58.69%   [EVAL] batch:   77 | acc: 75.00%,  total acc: 58.89%   [EVAL] batch:   78 | acc: 25.00%,  total acc: 58.47%   [EVAL] batch:   79 | acc: 18.75%,  total acc: 57.97%   [EVAL] batch:   80 | acc: 6.25%,  total acc: 57.33%   [EVAL] batch:   81 | acc: 0.00%,  total acc: 56.63%   [EVAL] batch:   82 | acc: 0.00%,  total acc: 55.95%   [EVAL] batch:   83 | acc: 0.00%,  total acc: 55.28%   [EVAL] batch:   84 | acc: 43.75%,  total acc: 55.15%   [EVAL] batch:   85 | acc: 68.75%,  total acc: 55.31%   [EVAL] batch:   86 | acc: 56.25%,  total acc: 55.32%   [EVAL] batch:   87 | acc: 68.75%,  total acc: 55.47%   [EVAL] batch:   88 | acc: 68.75%,  total acc: 55.62%   [EVAL] batch:   89 | acc: 75.00%,  total acc: 55.83%   [EVAL] batch:   90 | acc: 75.00%,  total acc: 56.04%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 56.52%   [EVAL] batch:   92 | acc: 93.75%,  total acc: 56.92%   [EVAL] batch:   93 | acc: 100.00%,  total acc: 57.38%   [EVAL] batch:   94 | acc: 81.25%,  total acc: 57.63%   [EVAL] batch:   95 | acc: 68.75%,  total acc: 57.75%   [EVAL] batch:   96 | acc: 0.00%,  total acc: 57.15%   [EVAL] batch:   97 | acc: 6.25%,  total acc: 56.63%   [EVAL] batch:   98 | acc: 0.00%,  total acc: 56.06%   [EVAL] batch:   99 | acc: 68.75%,  total acc: 56.19%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 56.62%   [EVAL] batch:  101 | acc: 75.00%,  total acc: 56.80%   [EVAL] batch:  102 | acc: 31.25%,  total acc: 56.55%   [EVAL] batch:  103 | acc: 25.00%,  total acc: 56.25%   [EVAL] batch:  104 | acc: 31.25%,  total acc: 56.01%   [EVAL] batch:  105 | acc: 0.00%,  total acc: 55.48%   [EVAL] batch:  106 | acc: 31.25%,  total acc: 55.26%   [EVAL] batch:  107 | acc: 56.25%,  total acc: 55.27%   [EVAL] batch:  108 | acc: 50.00%,  total acc: 55.22%   [EVAL] batch:  109 | acc: 62.50%,  total acc: 55.28%   [EVAL] batch:  110 | acc: 68.75%,  total acc: 55.41%   [EVAL] batch:  111 | acc: 93.75%,  total acc: 55.75%   [EVAL] batch:  112 | acc: 62.50%,  total acc: 55.81%   [EVAL] batch:  113 | acc: 75.00%,  total acc: 55.98%   [EVAL] batch:  114 | acc: 56.25%,  total acc: 55.98%   [EVAL] batch:  115 | acc: 37.50%,  total acc: 55.82%   [EVAL] batch:  116 | acc: 93.75%,  total acc: 56.14%   [EVAL] batch:  117 | acc: 87.50%,  total acc: 56.41%   [EVAL] batch:  118 | acc: 62.50%,  total acc: 56.46%   
cur_acc:  ['0.8485', '0.8000', '0.8125', '0.8090', '0.5455', '0.4062', '0.6442']
his_acc:  ['0.8485', '0.8391', '0.7695', '0.7155', '0.6700', '0.6127', '0.5646']
CurrentTrain: epoch  0, batch     0 | loss: 5.2087307CurrentTrain: epoch  0, batch     1 | loss: 5.6344657CurrentTrain: epoch  1, batch     0 | loss: 3.9751337CurrentTrain: epoch  1, batch     1 | loss: 3.8254945CurrentTrain: epoch  2, batch     0 | loss: 3.3250527CurrentTrain: epoch  2, batch     1 | loss: 3.2028389CurrentTrain: epoch  3, batch     0 | loss: 3.0818615CurrentTrain: epoch  3, batch     1 | loss: 2.3754816CurrentTrain: epoch  4, batch     0 | loss: 2.4973497CurrentTrain: epoch  4, batch     1 | loss: 2.5064075CurrentTrain: epoch  5, batch     0 | loss: 2.4405663CurrentTrain: epoch  5, batch     1 | loss: 2.4049504CurrentTrain: epoch  6, batch     0 | loss: 2.2699864CurrentTrain: epoch  6, batch     1 | loss: 2.3384373CurrentTrain: epoch  7, batch     0 | loss: 2.1296535CurrentTrain: epoch  7, batch     1 | loss: 2.1118867CurrentTrain: epoch  8, batch     0 | loss: 2.0297010CurrentTrain: epoch  8, batch     1 | loss: 1.9762247CurrentTrain: epoch  9, batch     0 | loss: 2.0170922CurrentTrain: epoch  9, batch     1 | loss: 1.8913265
#### organization founded ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization founded" means an organization was found in a date.
Here is an example:
Relation: organization founded
Context: the jnf was founded in 1901 to buy plots in palestine , then ruled by the ottomans .
Head Entity: jnf
Tail Entity: 1901
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization founded":
Relation: organization founded  
Context: the united nations was established in 1945 to promote international cooperation and peace.  
Head Entity: united nations  
Tail Entity: 1945  

Relation: organization founded  
Context: the world health organization was created in 1948 to coordinate global health efforts.  
Head Entity: world health organization  
Tail Entity: 1948  
#### person age ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person age" means a person 's age is a number.
Here is an example:
Relation: person age
Context: susan boyle is 48 years old now .
Head Entity: susan boyle
Tail Entity: 48
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person age":
Relation: person age  
Context: john smith turned 30 last week.  
Head Entity: john smith  
Tail Entity: 30  

Relation: person age  
Context: the famous actor will celebrate his 45th birthday next month.  
Head Entity: the famous actor  
Tail Entity: 45  
#### person city of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person city of birth" means a person was born in a city.
Here is an example:
Relation: person city of birth
Context: rothman was born in san francisco in 1932 in an orthodox jewish family .
Head Entity: rothman
Tail Entity: san francisco
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person city of birth":
Relation: person city of birth  
Context: after spending his early years in new york, he moved to los angeles where he began his career.  
Head Entity: he  
Tail Entity: los angeles  

Relation: person city of birth  
Context: the famous author was born in a small town near boston, which greatly influenced his writing.  
Head Entity: the famous author  
Tail Entity: boston  
#### organization members ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization members" means an organization 's member is an organization.
Here is an example:
Relation: organization members
Context: sun plays for the grand rapids flight of the international basketball league after toiling for the maryland nighthawks of the american basketball association , both development leagues for those who dream of an nba career .
Head Entity: american basketball association
Tail Entity: maryland nighthawks
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization members":
Relation: organization members  
Context: The New York Philharmonic is one of the oldest orchestras in the United States, and it has had many notable musicians as members, including the famous conductor Leonard Bernstein.  
Head Entity: New York Philharmonic  
Tail Entity: Leonard Bernstein  

Relation: organization members  
Context: The National Geographic Society has a long history of exploration and education, with many prominent explorers and scientists, such as Jane Goodall, serving as members.  
Head Entity: National Geographic Society  
Tail Entity: Jane Goodall  
#### person religion ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person religion" means an person is the member of a religion.
Here is an example:
Relation: person religion
Context: he fought attempts by zealous jews to move into the muslim quarter of the walled old city , but defended the practice of developing jewish suburbs around the eastern arab sector to prevent it from ever escaping israel 's rule .
Head Entity: he
Tail Entity: jewish
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person religion":
Relation: person religion  
Context: After years of studying various philosophies, she finally embraced Buddhism, finding peace and purpose in its teachings.  
Head Entity: she  
Tail Entity: Buddhism  

Relation: person religion  
Context: The community celebrated the festival of Diwali, where he proudly showcased his Hindu heritage and traditions.  
Head Entity: he  
Tail Entity: Hindu
Mixup data size:  284
MixupTrain:  epoch  0, batch     0 | loss: 2.4821859MixupTrain:  epoch  0, batch     1 | loss: 2.4348901MixupTrain:  epoch  0, batch     2 | loss: 2.5299589MixupTrain:  epoch  0, batch     3 | loss: 1.8177078MixupTrain:  epoch  0, batch     4 | loss: 2.1724931MixupTrain:  epoch  0, batch     5 | loss: 2.2115598MixupTrain:  epoch  0, batch     6 | loss: 2.0639039MixupTrain:  epoch  0, batch     7 | loss: 2.3979155MixupTrain:  epoch  0, batch     8 | loss: 2.8472960MixupTrain:  epoch  0, batch     9 | loss: 2.1477868MixupTrain:  epoch  0, batch    10 | loss: 1.7036496MixupTrain:  epoch  0, batch    11 | loss: 2.0296130MixupTrain:  epoch  0, batch    12 | loss: 1.7812032MixupTrain:  epoch  0, batch    13 | loss: 1.6885450MixupTrain:  epoch  0, batch    14 | loss: 2.3181955MixupTrain:  epoch  0, batch    15 | loss: 1.8650458MixupTrain:  epoch  0, batch    16 | loss: 2.3083680MixupTrain:  epoch  0, batch    17 | loss: 1.6881236
MemoryTrain:  epoch  0, batch     0 | loss: 1.9457438MemoryTrain:  epoch  0, batch     1 | loss: 1.8442490MemoryTrain:  epoch  0, batch     2 | loss: 1.7552116MemoryTrain:  epoch  0, batch     3 | loss: 2.1402192MemoryTrain:  epoch  0, batch     4 | loss: 2.7352128MemoryTrain:  epoch  0, batch     5 | loss: 2.2613270MemoryTrain:  epoch  0, batch     6 | loss: 2.1346774MemoryTrain:  epoch  0, batch     7 | loss: 2.5233543MemoryTrain:  epoch  1, batch     0 | loss: 1.5984895MemoryTrain:  epoch  1, batch     1 | loss: 1.9512839MemoryTrain:  epoch  1, batch     2 | loss: 1.9739323MemoryTrain:  epoch  1, batch     3 | loss: 1.4353530MemoryTrain:  epoch  1, batch     4 | loss: 1.9027669MemoryTrain:  epoch  1, batch     5 | loss: 1.9102032MemoryTrain:  epoch  1, batch     6 | loss: 1.7023172MemoryTrain:  epoch  1, batch     7 | loss: 2.2275243MemoryTrain:  epoch  2, batch     0 | loss: 1.6437485MemoryTrain:  epoch  2, batch     1 | loss: 1.7123358MemoryTrain:  epoch  2, batch     2 | loss: 1.4929034MemoryTrain:  epoch  2, batch     3 | loss: 1.6891167MemoryTrain:  epoch  2, batch     4 | loss: 1.6861284MemoryTrain:  epoch  2, batch     5 | loss: 1.4854553MemoryTrain:  epoch  2, batch     6 | loss: 1.4104018MemoryTrain:  epoch  2, batch     7 | loss: 1.5558094MemoryTrain:  epoch  3, batch     0 | loss: 1.7421391MemoryTrain:  epoch  3, batch     1 | loss: 1.3308545MemoryTrain:  epoch  3, batch     2 | loss: 1.4686007MemoryTrain:  epoch  3, batch     3 | loss: 1.4091147MemoryTrain:  epoch  3, batch     4 | loss: 1.5175692MemoryTrain:  epoch  3, batch     5 | loss: 1.3158675MemoryTrain:  epoch  3, batch     6 | loss: 1.9249765MemoryTrain:  epoch  3, batch     7 | loss: 1.7391962MemoryTrain:  epoch  4, batch     0 | loss: 1.4131038MemoryTrain:  epoch  4, batch     1 | loss: 1.3917885MemoryTrain:  epoch  4, batch     2 | loss: 1.6538038MemoryTrain:  epoch  4, batch     3 | loss: 1.4396045MemoryTrain:  epoch  4, batch     4 | loss: 1.7632723MemoryTrain:  epoch  4, batch     5 | loss: 1.6069596MemoryTrain:  epoch  4, batch     6 | loss: 1.4487798MemoryTrain:  epoch  4, batch     7 | loss: 1.2975764MemoryTrain:  epoch  5, batch     0 | loss: 1.3546511MemoryTrain:  epoch  5, batch     1 | loss: 1.4732785MemoryTrain:  epoch  5, batch     2 | loss: 1.4113879MemoryTrain:  epoch  5, batch     3 | loss: 1.3827527MemoryTrain:  epoch  5, batch     4 | loss: 1.4510107MemoryTrain:  epoch  5, batch     5 | loss: 1.4209032MemoryTrain:  epoch  5, batch     6 | loss: 1.4912605MemoryTrain:  epoch  5, batch     7 | loss: 1.2895677MemoryTrain:  epoch  6, batch     0 | loss: 1.2983017MemoryTrain:  epoch  6, batch     1 | loss: 1.3512908MemoryTrain:  epoch  6, batch     2 | loss: 1.4909506MemoryTrain:  epoch  6, batch     3 | loss: 1.3621767MemoryTrain:  epoch  6, batch     4 | loss: 1.4536217MemoryTrain:  epoch  6, batch     5 | loss: 1.3031490MemoryTrain:  epoch  6, batch     6 | loss: 1.3405547MemoryTrain:  epoch  6, batch     7 | loss: 1.3270535MemoryTrain:  epoch  7, batch     0 | loss: 1.4231595MemoryTrain:  epoch  7, batch     1 | loss: 1.2934498MemoryTrain:  epoch  7, batch     2 | loss: 1.3228076MemoryTrain:  epoch  7, batch     3 | loss: 1.2754908MemoryTrain:  epoch  7, batch     4 | loss: 1.3029562MemoryTrain:  epoch  7, batch     5 | loss: 1.3937011MemoryTrain:  epoch  7, batch     6 | loss: 1.3795158MemoryTrain:  epoch  7, batch     7 | loss: 1.3515009MemoryTrain:  epoch  8, batch     0 | loss: 1.3334923MemoryTrain:  epoch  8, batch     1 | loss: 1.3514748MemoryTrain:  epoch  8, batch     2 | loss: 1.3373039MemoryTrain:  epoch  8, batch     3 | loss: 1.2838331MemoryTrain:  epoch  8, batch     4 | loss: 1.2523839MemoryTrain:  epoch  8, batch     5 | loss: 1.3237265MemoryTrain:  epoch  8, batch     6 | loss: 1.2556853MemoryTrain:  epoch  8, batch     7 | loss: 1.2196436MemoryTrain:  epoch  9, batch     0 | loss: 1.2417426MemoryTrain:  epoch  9, batch     1 | loss: 1.3268647MemoryTrain:  epoch  9, batch     2 | loss: 1.2775121MemoryTrain:  epoch  9, batch     3 | loss: 1.2846494MemoryTrain:  epoch  9, batch     4 | loss: 1.3290040MemoryTrain:  epoch  9, batch     5 | loss: 1.3680165MemoryTrain:  epoch  9, batch     6 | loss: 1.2386659MemoryTrain:  epoch  9, batch     7 | loss: 1.2709261
[EVAL] batch:    0 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 93.75%   [EVAL] batch:    2 | acc: 100.00%,  total acc: 95.83%   [EVAL] batch:    3 | acc: 100.00%,  total acc: 96.88%   [EVAL] batch:    4 | acc: 100.00%,  total acc: 97.50%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 97.92%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 98.21%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 98.44%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 97.22%   [EVAL] batch:    9 | acc: 43.75%,  total acc: 91.88%   [EVAL] batch:   10 | acc: 43.75%,  total acc: 87.50%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 88.02%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 88.46%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 87.05%   
[EVAL] batch:    0 | acc: 12.50%,  total acc: 12.50%   [EVAL] batch:    1 | acc: 18.75%,  total acc: 15.62%   [EVAL] batch:    2 | acc: 37.50%,  total acc: 22.92%   [EVAL] batch:    3 | acc: 12.50%,  total acc: 20.31%   [EVAL] batch:    4 | acc: 18.75%,  total acc: 20.00%   [EVAL] batch:    5 | acc: 12.50%,  total acc: 18.75%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 25.89%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 34.38%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 40.28%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 45.00%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 48.30%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 51.04%   [EVAL] batch:   12 | acc: 37.50%,  total acc: 50.00%   [EVAL] batch:   13 | acc: 37.50%,  total acc: 49.11%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 50.83%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 51.17%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 52.57%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 53.12%   [EVAL] batch:   18 | acc: 75.00%,  total acc: 54.28%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 55.62%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 57.74%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 59.66%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 61.14%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 62.50%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 64.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 65.38%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 66.44%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 67.63%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 68.53%   [EVAL] batch:   29 | acc: 75.00%,  total acc: 68.75%   [EVAL] batch:   30 | acc: 81.25%,  total acc: 69.15%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 70.12%   [EVAL] batch:   32 | acc: 62.50%,  total acc: 69.89%   [EVAL] batch:   33 | acc: 43.75%,  total acc: 69.12%   [EVAL] batch:   34 | acc: 43.75%,  total acc: 68.39%   [EVAL] batch:   35 | acc: 62.50%,  total acc: 68.23%   [EVAL] batch:   36 | acc: 43.75%,  total acc: 67.57%   [EVAL] batch:   37 | acc: 81.25%,  total acc: 67.93%   [EVAL] batch:   38 | acc: 81.25%,  total acc: 68.27%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 69.06%   [EVAL] batch:   40 | acc: 81.25%,  total acc: 69.36%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 70.09%   [EVAL] batch:   42 | acc: 12.50%,  total acc: 68.75%   [EVAL] batch:   43 | acc: 0.00%,  total acc: 67.19%   [EVAL] batch:   44 | acc: 0.00%,  total acc: 65.69%   [EVAL] batch:   45 | acc: 0.00%,  total acc: 64.27%   [EVAL] batch:   46 | acc: 12.50%,  total acc: 63.16%   [EVAL] batch:   47 | acc: 37.50%,  total acc: 62.63%   [EVAL] batch:   48 | acc: 18.75%,  total acc: 61.73%   [EVAL] batch:   49 | acc: 12.50%,  total acc: 60.75%   [EVAL] batch:   50 | acc: 0.00%,  total acc: 59.56%   [EVAL] batch:   51 | acc: 12.50%,  total acc: 58.65%   [EVAL] batch:   52 | acc: 6.25%,  total acc: 57.67%   [EVAL] batch:   53 | acc: 31.25%,  total acc: 57.18%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 57.73%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 58.26%   [EVAL] batch:   56 | acc: 75.00%,  total acc: 58.55%   [EVAL] batch:   57 | acc: 62.50%,  total acc: 58.62%   [EVAL] batch:   58 | acc: 81.25%,  total acc: 59.00%   [EVAL] batch:   59 | acc: 75.00%,  total acc: 59.27%   [EVAL] batch:   60 | acc: 18.75%,  total acc: 58.61%   [EVAL] batch:   61 | acc: 0.00%,  total acc: 57.66%   [EVAL] batch:   62 | acc: 0.00%,  total acc: 56.75%   [EVAL] batch:   63 | acc: 0.00%,  total acc: 55.86%   [EVAL] batch:   64 | acc: 0.00%,  total acc: 55.00%   [EVAL] batch:   65 | acc: 6.25%,  total acc: 54.26%   [EVAL] batch:   66 | acc: 18.75%,  total acc: 53.73%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 54.41%   [EVAL] batch:   68 | acc: 81.25%,  total acc: 54.80%   [EVAL] batch:   69 | acc: 43.75%,  total acc: 54.64%   [EVAL] batch:   70 | acc: 56.25%,  total acc: 54.67%   [EVAL] batch:   71 | acc: 68.75%,  total acc: 54.86%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 55.48%   [EVAL] batch:   73 | acc: 100.00%,  total acc: 56.08%   [EVAL] batch:   74 | acc: 100.00%,  total acc: 56.67%   [EVAL] batch:   75 | acc: 100.00%,  total acc: 57.24%   [EVAL] batch:   76 | acc: 100.00%,  total acc: 57.79%   [EVAL] batch:   77 | acc: 75.00%,  total acc: 58.01%   [EVAL] batch:   78 | acc: 12.50%,  total acc: 57.44%   [EVAL] batch:   79 | acc: 12.50%,  total acc: 56.88%   [EVAL] batch:   80 | acc: 0.00%,  total acc: 56.17%   [EVAL] batch:   81 | acc: 12.50%,  total acc: 55.64%   [EVAL] batch:   82 | acc: 6.25%,  total acc: 55.05%   [EVAL] batch:   83 | acc: 0.00%,  total acc: 54.39%   [EVAL] batch:   84 | acc: 43.75%,  total acc: 54.26%   [EVAL] batch:   85 | acc: 68.75%,  total acc: 54.43%   [EVAL] batch:   86 | acc: 50.00%,  total acc: 54.38%   [EVAL] batch:   87 | acc: 68.75%,  total acc: 54.55%   [EVAL] batch:   88 | acc: 62.50%,  total acc: 54.63%   [EVAL] batch:   89 | acc: 75.00%,  total acc: 54.86%   [EVAL] batch:   90 | acc: 75.00%,  total acc: 55.08%   [EVAL] batch:   91 | acc: 93.75%,  total acc: 55.50%   [EVAL] batch:   92 | acc: 93.75%,  total acc: 55.91%   [EVAL] batch:   93 | acc: 100.00%,  total acc: 56.38%   [EVAL] batch:   94 | acc: 87.50%,  total acc: 56.71%   [EVAL] batch:   95 | acc: 68.75%,  total acc: 56.84%   [EVAL] batch:   96 | acc: 0.00%,  total acc: 56.25%   [EVAL] batch:   97 | acc: 6.25%,  total acc: 55.74%   [EVAL] batch:   98 | acc: 0.00%,  total acc: 55.18%   [EVAL] batch:   99 | acc: 68.75%,  total acc: 55.31%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 55.75%   [EVAL] batch:  101 | acc: 56.25%,  total acc: 55.76%   [EVAL] batch:  102 | acc: 31.25%,  total acc: 55.52%   [EVAL] batch:  103 | acc: 12.50%,  total acc: 55.11%   [EVAL] batch:  104 | acc: 18.75%,  total acc: 54.76%   [EVAL] batch:  105 | acc: 0.00%,  total acc: 54.25%   [EVAL] batch:  106 | acc: 37.50%,  total acc: 54.09%   [EVAL] batch:  107 | acc: 62.50%,  total acc: 54.17%   [EVAL] batch:  108 | acc: 56.25%,  total acc: 54.19%   [EVAL] batch:  109 | acc: 62.50%,  total acc: 54.26%   [EVAL] batch:  110 | acc: 68.75%,  total acc: 54.39%   [EVAL] batch:  111 | acc: 93.75%,  total acc: 54.74%   [EVAL] batch:  112 | acc: 62.50%,  total acc: 54.81%   [EVAL] batch:  113 | acc: 81.25%,  total acc: 55.04%   [EVAL] batch:  114 | acc: 68.75%,  total acc: 55.16%   [EVAL] batch:  115 | acc: 50.00%,  total acc: 55.12%   [EVAL] batch:  116 | acc: 81.25%,  total acc: 55.34%   [EVAL] batch:  117 | acc: 81.25%,  total acc: 55.56%   [EVAL] batch:  118 | acc: 87.50%,  total acc: 55.83%   [EVAL] batch:  119 | acc: 100.00%,  total acc: 56.20%   [EVAL] batch:  120 | acc: 87.50%,  total acc: 56.46%   [EVAL] batch:  121 | acc: 100.00%,  total acc: 56.81%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 57.16%   [EVAL] batch:  123 | acc: 100.00%,  total acc: 57.51%   [EVAL] batch:  124 | acc: 100.00%,  total acc: 57.85%   [EVAL] batch:  125 | acc: 100.00%,  total acc: 58.18%   [EVAL] batch:  126 | acc: 100.00%,  total acc: 58.51%   [EVAL] batch:  127 | acc: 81.25%,  total acc: 58.69%   [EVAL] batch:  128 | acc: 43.75%,  total acc: 58.58%   [EVAL] batch:  129 | acc: 50.00%,  total acc: 58.51%   [EVAL] batch:  130 | acc: 93.75%,  total acc: 58.78%   [EVAL] batch:  131 | acc: 93.75%,  total acc: 59.04%   [EVAL] batch:  132 | acc: 56.25%,  total acc: 59.02%   
cur_acc:  ['0.8485', '0.8000', '0.8125', '0.8090', '0.5455', '0.4062', '0.6442', '0.8705']
his_acc:  ['0.8485', '0.8391', '0.7695', '0.7155', '0.6700', '0.6127', '0.5646', '0.5902']
--------Round  2
seed:  300
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/test.pkl
Task_order: [7 2 3 1 5 6 0 4]
prepared data!
CurrentTrain: epoch  0, batch     0 | loss: 13.0925503CurrentTrain: epoch  0, batch     1 | loss: 13.1092491CurrentTrain: epoch  0, batch     2 | loss: 12.9613075CurrentTrain: epoch  0, batch     3 | loss: 12.9774494CurrentTrain: epoch  0, batch     4 | loss: 12.6976814CurrentTrain: epoch  0, batch     5 | loss: 12.6176729CurrentTrain: epoch  0, batch     6 | loss: 12.6262703CurrentTrain: epoch  0, batch     7 | loss: 12.4357471CurrentTrain: epoch  0, batch     8 | loss: 12.2413902CurrentTrain: epoch  0, batch     9 | loss: 12.1500826CurrentTrain: epoch  0, batch    10 | loss: 11.9241171CurrentTrain: epoch  0, batch    11 | loss: 11.9396610CurrentTrain: epoch  0, batch    12 | loss: 11.8416710CurrentTrain: epoch  0, batch    13 | loss: 11.6213818CurrentTrain: epoch  0, batch    14 | loss: 11.8969412CurrentTrain: epoch  0, batch    15 | loss: 11.3944397CurrentTrain: epoch  0, batch    16 | loss: 11.2428770CurrentTrain: epoch  0, batch    17 | loss: 11.0247917CurrentTrain: epoch  0, batch    18 | loss: 11.4502020CurrentTrain: epoch  0, batch    19 | loss: 11.0734539CurrentTrain: epoch  0, batch    20 | loss: 11.3046074CurrentTrain: epoch  0, batch    21 | loss: 10.7730217CurrentTrain: epoch  0, batch    22 | loss: 11.3988686CurrentTrain: epoch  0, batch    23 | loss: 10.9215555CurrentTrain: epoch  0, batch    24 | loss: 10.7737494CurrentTrain: epoch  0, batch    25 | loss: 11.0780334CurrentTrain: epoch  0, batch    26 | loss: 11.4028797CurrentTrain: epoch  0, batch    27 | loss: 11.6362991CurrentTrain: epoch  0, batch    28 | loss: 10.7121067CurrentTrain: epoch  0, batch    29 | loss: 10.6968508CurrentTrain: epoch  0, batch    30 | loss: 9.8482265CurrentTrain: epoch  0, batch    31 | loss: 10.5412693CurrentTrain: epoch  0, batch    32 | loss: 10.5639248CurrentTrain: epoch  0, batch    33 | loss: 10.4377232CurrentTrain: epoch  0, batch    34 | loss: 10.9481735CurrentTrain: epoch  0, batch    35 | loss: 10.2937918CurrentTrain: epoch  0, batch    36 | loss: 10.6845207CurrentTrain: epoch  0, batch    37 | loss: 9.5798416CurrentTrain: epoch  1, batch     0 | loss: 9.8109961CurrentTrain: epoch  1, batch     1 | loss: 9.8303947CurrentTrain: epoch  1, batch     2 | loss: 9.6014166CurrentTrain: epoch  1, batch     3 | loss: 9.9084454CurrentTrain: epoch  1, batch     4 | loss: 9.7063913CurrentTrain: epoch  1, batch     5 | loss: 9.9476109CurrentTrain: epoch  1, batch     6 | loss: 9.8215923CurrentTrain: epoch  1, batch     7 | loss: 9.8939714CurrentTrain: epoch  1, batch     8 | loss: 10.2863579CurrentTrain: epoch  1, batch     9 | loss: 9.2837086CurrentTrain: epoch  1, batch    10 | loss: 9.2499161CurrentTrain: epoch  1, batch    11 | loss: 9.6990728CurrentTrain: epoch  1, batch    12 | loss: 9.2506723CurrentTrain: epoch  1, batch    13 | loss: 9.1547403CurrentTrain: epoch  1, batch    14 | loss: 9.4992714CurrentTrain: epoch  1, batch    15 | loss: 9.4203606CurrentTrain: epoch  1, batch    16 | loss: 9.8427601CurrentTrain: epoch  1, batch    17 | loss: 9.1774178CurrentTrain: epoch  1, batch    18 | loss: 9.5644932CurrentTrain: epoch  1, batch    19 | loss: 8.8279324CurrentTrain: epoch  1, batch    20 | loss: 9.7173023CurrentTrain: epoch  1, batch    21 | loss: 9.3588200CurrentTrain: epoch  1, batch    22 | loss: 9.8193474CurrentTrain: epoch  1, batch    23 | loss: 8.8059559CurrentTrain: epoch  1, batch    24 | loss: 9.1969843CurrentTrain: epoch  1, batch    25 | loss: 9.7912416CurrentTrain: epoch  1, batch    26 | loss: 9.4294729CurrentTrain: epoch  1, batch    27 | loss: 8.9590721CurrentTrain: epoch  1, batch    28 | loss: 8.8486443CurrentTrain: epoch  1, batch    29 | loss: 9.3878460CurrentTrain: epoch  1, batch    30 | loss: 8.1842766CurrentTrain: epoch  1, batch    31 | loss: 8.9349384CurrentTrain: epoch  1, batch    32 | loss: 8.4227667CurrentTrain: epoch  1, batch    33 | loss: 9.7833452CurrentTrain: epoch  1, batch    34 | loss: 9.2861252CurrentTrain: epoch  1, batch    35 | loss: 8.3713512CurrentTrain: epoch  1, batch    36 | loss: 9.0181465CurrentTrain: epoch  1, batch    37 | loss: 7.9427471CurrentTrain: epoch  2, batch     0 | loss: 8.6822681CurrentTrain: epoch  2, batch     1 | loss: 8.5297413CurrentTrain: epoch  2, batch     2 | loss: 8.7676020CurrentTrain: epoch  2, batch     3 | loss: 8.4504433CurrentTrain: epoch  2, batch     4 | loss: 8.0322723CurrentTrain: epoch  2, batch     5 | loss: 8.0015583CurrentTrain: epoch  2, batch     6 | loss: 8.6585712CurrentTrain: epoch  2, batch     7 | loss: 8.3506899CurrentTrain: epoch  2, batch     8 | loss: 8.2262526CurrentTrain: epoch  2, batch     9 | loss: 8.2279510CurrentTrain: epoch  2, batch    10 | loss: 8.1756516CurrentTrain: epoch  2, batch    11 | loss: 8.2623920CurrentTrain: epoch  2, batch    12 | loss: 7.4004354CurrentTrain: epoch  2, batch    13 | loss: 9.0923748CurrentTrain: epoch  2, batch    14 | loss: 7.8391433CurrentTrain: epoch  2, batch    15 | loss: 8.8161697CurrentTrain: epoch  2, batch    16 | loss: 7.7248616CurrentTrain: epoch  2, batch    17 | loss: 8.2976341CurrentTrain: epoch  2, batch    18 | loss: 7.9426680CurrentTrain: epoch  2, batch    19 | loss: 7.2574277CurrentTrain: epoch  2, batch    20 | loss: 8.7432003CurrentTrain: epoch  2, batch    21 | loss: 7.1463223CurrentTrain: epoch  2, batch    22 | loss: 7.9282537CurrentTrain: epoch  2, batch    23 | loss: 7.1960478CurrentTrain: epoch  2, batch    24 | loss: 8.4678555CurrentTrain: epoch  2, batch    25 | loss: 7.8497305CurrentTrain: epoch  2, batch    26 | loss: 8.2441854CurrentTrain: epoch  2, batch    27 | loss: 8.2813196CurrentTrain: epoch  2, batch    28 | loss: 7.6833620CurrentTrain: epoch  2, batch    29 | loss: 7.8023424CurrentTrain: epoch  2, batch    30 | loss: 8.2800112CurrentTrain: epoch  2, batch    31 | loss: 8.3025293CurrentTrain: epoch  2, batch    32 | loss: 7.5304289CurrentTrain: epoch  2, batch    33 | loss: 7.9354062CurrentTrain: epoch  2, batch    34 | loss: 6.8776050CurrentTrain: epoch  2, batch    35 | loss: 7.6612964CurrentTrain: epoch  2, batch    36 | loss: 7.3868732CurrentTrain: epoch  2, batch    37 | loss: 7.4323845CurrentTrain: epoch  3, batch     0 | loss: 7.4349666CurrentTrain: epoch  3, batch     1 | loss: 7.9949937CurrentTrain: epoch  3, batch     2 | loss: 7.1410789CurrentTrain: epoch  3, batch     3 | loss: 7.0958128CurrentTrain: epoch  3, batch     4 | loss: 7.3214417CurrentTrain: epoch  3, batch     5 | loss: 7.2236824CurrentTrain: epoch  3, batch     6 | loss: 7.7209101CurrentTrain: epoch  3, batch     7 | loss: 7.7814856CurrentTrain: epoch  3, batch     8 | loss: 7.3415604CurrentTrain: epoch  3, batch     9 | loss: 7.3560886CurrentTrain: epoch  3, batch    10 | loss: 7.5142050CurrentTrain: epoch  3, batch    11 | loss: 7.4504142CurrentTrain: epoch  3, batch    12 | loss: 6.9353471CurrentTrain: epoch  3, batch    13 | loss: 7.2392912CurrentTrain: epoch  3, batch    14 | loss: 7.4667273CurrentTrain: epoch  3, batch    15 | loss: 7.1517916CurrentTrain: epoch  3, batch    16 | loss: 7.6478729CurrentTrain: epoch  3, batch    17 | loss: 8.2037334CurrentTrain: epoch  3, batch    18 | loss: 7.5440855CurrentTrain: epoch  3, batch    19 | loss: 7.4714293CurrentTrain: epoch  3, batch    20 | loss: 7.1861935CurrentTrain: epoch  3, batch    21 | loss: 7.0617743CurrentTrain: epoch  3, batch    22 | loss: 7.1109481CurrentTrain: epoch  3, batch    23 | loss: 7.2275672CurrentTrain: epoch  3, batch    24 | loss: 7.2351079CurrentTrain: epoch  3, batch    25 | loss: 7.3862815CurrentTrain: epoch  3, batch    26 | loss: 7.3012333CurrentTrain: epoch  3, batch    27 | loss: 7.2140794CurrentTrain: epoch  3, batch    28 | loss: 6.8567619CurrentTrain: epoch  3, batch    29 | loss: 6.8681164CurrentTrain: epoch  3, batch    30 | loss: 8.0613813CurrentTrain: epoch  3, batch    31 | loss: 6.2485008CurrentTrain: epoch  3, batch    32 | loss: 7.3532610CurrentTrain: epoch  3, batch    33 | loss: 8.1357098CurrentTrain: epoch  3, batch    34 | loss: 7.5185022CurrentTrain: epoch  3, batch    35 | loss: 7.6532788CurrentTrain: epoch  3, batch    36 | loss: 7.2731066CurrentTrain: epoch  3, batch    37 | loss: 6.4417219CurrentTrain: epoch  4, batch     0 | loss: 6.4290504CurrentTrain: epoch  4, batch     1 | loss: 6.7353802CurrentTrain: epoch  4, batch     2 | loss: 6.2320814CurrentTrain: epoch  4, batch     3 | loss: 7.0194016CurrentTrain: epoch  4, batch     4 | loss: 7.1690102CurrentTrain: epoch  4, batch     5 | loss: 7.1577559CurrentTrain: epoch  4, batch     6 | loss: 7.2657766CurrentTrain: epoch  4, batch     7 | loss: 6.8614454CurrentTrain: epoch  4, batch     8 | loss: 6.3416390CurrentTrain: epoch  4, batch     9 | loss: 6.2099857CurrentTrain: epoch  4, batch    10 | loss: 7.0748162CurrentTrain: epoch  4, batch    11 | loss: 6.3775792CurrentTrain: epoch  4, batch    12 | loss: 6.5715094CurrentTrain: epoch  4, batch    13 | loss: 6.5375390CurrentTrain: epoch  4, batch    14 | loss: 7.3891640CurrentTrain: epoch  4, batch    15 | loss: 7.3320704CurrentTrain: epoch  4, batch    16 | loss: 6.8845177CurrentTrain: epoch  4, batch    17 | loss: 6.5311766CurrentTrain: epoch  4, batch    18 | loss: 6.3803325CurrentTrain: epoch  4, batch    19 | loss: 7.8819466CurrentTrain: epoch  4, batch    20 | loss: 6.7181234CurrentTrain: epoch  4, batch    21 | loss: 7.8407922CurrentTrain: epoch  4, batch    22 | loss: 7.2144971CurrentTrain: epoch  4, batch    23 | loss: 7.1063652CurrentTrain: epoch  4, batch    24 | loss: 6.9333696CurrentTrain: epoch  4, batch    25 | loss: 7.5636282CurrentTrain: epoch  4, batch    26 | loss: 6.3776827CurrentTrain: epoch  4, batch    27 | loss: 8.2221375CurrentTrain: epoch  4, batch    28 | loss: 6.4193840CurrentTrain: epoch  4, batch    29 | loss: 5.9803658CurrentTrain: epoch  4, batch    30 | loss: 7.0585928CurrentTrain: epoch  4, batch    31 | loss: 6.3735046CurrentTrain: epoch  4, batch    32 | loss: 7.7169943CurrentTrain: epoch  4, batch    33 | loss: 7.2310162CurrentTrain: epoch  4, batch    34 | loss: 6.4929476CurrentTrain: epoch  4, batch    35 | loss: 6.0881119CurrentTrain: epoch  4, batch    36 | loss: 6.2042942CurrentTrain: epoch  4, batch    37 | loss: 6.5735159CurrentTrain: epoch  5, batch     0 | loss: 6.7729206CurrentTrain: epoch  5, batch     1 | loss: 6.7492938CurrentTrain: epoch  5, batch     2 | loss: 6.3254194CurrentTrain: epoch  5, batch     3 | loss: 6.5864487CurrentTrain: epoch  5, batch     4 | loss: 6.4741497CurrentTrain: epoch  5, batch     5 | loss: 6.3830061CurrentTrain: epoch  5, batch     6 | loss: 7.8054533CurrentTrain: epoch  5, batch     7 | loss: 6.9861875CurrentTrain: epoch  5, batch     8 | loss: 7.5124531CurrentTrain: epoch  5, batch     9 | loss: 6.8250022CurrentTrain: epoch  5, batch    10 | loss: 6.1208820CurrentTrain: epoch  5, batch    11 | loss: 6.6583714CurrentTrain: epoch  5, batch    12 | loss: 7.0850544CurrentTrain: epoch  5, batch    13 | loss: 6.1598730CurrentTrain: epoch  5, batch    14 | loss: 6.0903273CurrentTrain: epoch  5, batch    15 | loss: 6.8825412CurrentTrain: epoch  5, batch    16 | loss: 5.9010477CurrentTrain: epoch  5, batch    17 | loss: 6.7055821CurrentTrain: epoch  5, batch    18 | loss: 5.5224862CurrentTrain: epoch  5, batch    19 | loss: 6.2093983CurrentTrain: epoch  5, batch    20 | loss: 6.3724632CurrentTrain: epoch  5, batch    21 | loss: 6.0393200CurrentTrain: epoch  5, batch    22 | loss: 5.7244978CurrentTrain: epoch  5, batch    23 | loss: 6.3891401CurrentTrain: epoch  5, batch    24 | loss: 6.1874185CurrentTrain: epoch  5, batch    25 | loss: 5.8940134CurrentTrain: epoch  5, batch    26 | loss: 5.7737036CurrentTrain: epoch  5, batch    27 | loss: 6.9288054CurrentTrain: epoch  5, batch    28 | loss: 6.0629282CurrentTrain: epoch  5, batch    29 | loss: 5.8348207CurrentTrain: epoch  5, batch    30 | loss: 6.7702479CurrentTrain: epoch  5, batch    31 | loss: 6.1005430CurrentTrain: epoch  5, batch    32 | loss: 6.1313720CurrentTrain: epoch  5, batch    33 | loss: 6.7342196CurrentTrain: epoch  5, batch    34 | loss: 6.5248427CurrentTrain: epoch  5, batch    35 | loss: 6.5636797CurrentTrain: epoch  5, batch    36 | loss: 5.8814888CurrentTrain: epoch  5, batch    37 | loss: 6.1219907CurrentTrain: epoch  6, batch     0 | loss: 6.3798752CurrentTrain: epoch  6, batch     1 | loss: 7.0472736CurrentTrain: epoch  6, batch     2 | loss: 6.5650687CurrentTrain: epoch  6, batch     3 | loss: 6.3378744CurrentTrain: epoch  6, batch     4 | loss: 6.2362461CurrentTrain: epoch  6, batch     5 | loss: 5.7355537CurrentTrain: epoch  6, batch     6 | loss: 6.2762170CurrentTrain: epoch  6, batch     7 | loss: 5.9853497CurrentTrain: epoch  6, batch     8 | loss: 5.7081270CurrentTrain: epoch  6, batch     9 | loss: 6.1890960CurrentTrain: epoch  6, batch    10 | loss: 5.7658501CurrentTrain: epoch  6, batch    11 | loss: 6.2431455CurrentTrain: epoch  6, batch    12 | loss: 6.3806067CurrentTrain: epoch  6, batch    13 | loss: 6.3540611CurrentTrain: epoch  6, batch    14 | loss: 6.2218218CurrentTrain: epoch  6, batch    15 | loss: 6.1446452CurrentTrain: epoch  6, batch    16 | loss: 5.5301247CurrentTrain: epoch  6, batch    17 | loss: 5.6902857CurrentTrain: epoch  6, batch    18 | loss: 6.0678568CurrentTrain: epoch  6, batch    19 | loss: 7.2346077CurrentTrain: epoch  6, batch    20 | loss: 5.6342316CurrentTrain: epoch  6, batch    21 | loss: 5.6703482CurrentTrain: epoch  6, batch    22 | loss: 5.8691368CurrentTrain: epoch  6, batch    23 | loss: 6.6152506CurrentTrain: epoch  6, batch    24 | loss: 5.4104133CurrentTrain: epoch  6, batch    25 | loss: 6.4595704CurrentTrain: epoch  6, batch    26 | loss: 6.2926340CurrentTrain: epoch  6, batch    27 | loss: 6.0988340CurrentTrain: epoch  6, batch    28 | loss: 6.1263494CurrentTrain: epoch  6, batch    29 | loss: 5.8361454CurrentTrain: epoch  6, batch    30 | loss: 5.9755659CurrentTrain: epoch  6, batch    31 | loss: 6.3873949CurrentTrain: epoch  6, batch    32 | loss: 5.6905451CurrentTrain: epoch  6, batch    33 | loss: 6.0189857CurrentTrain: epoch  6, batch    34 | loss: 5.6239300CurrentTrain: epoch  6, batch    35 | loss: 5.7875681CurrentTrain: epoch  6, batch    36 | loss: 5.5125542CurrentTrain: epoch  6, batch    37 | loss: 5.0557861CurrentTrain: epoch  7, batch     0 | loss: 6.0953255CurrentTrain: epoch  7, batch     1 | loss: 5.3124681CurrentTrain: epoch  7, batch     2 | loss: 5.9148731CurrentTrain: epoch  7, batch     3 | loss: 5.7169123CurrentTrain: epoch  7, batch     4 | loss: 5.8025517CurrentTrain: epoch  7, batch     5 | loss: 5.5917335CurrentTrain: epoch  7, batch     6 | loss: 5.3800097CurrentTrain: epoch  7, batch     7 | loss: 6.0839758CurrentTrain: epoch  7, batch     8 | loss: 5.4146996CurrentTrain: epoch  7, batch     9 | loss: 5.7877693CurrentTrain: epoch  7, batch    10 | loss: 5.7044749CurrentTrain: epoch  7, batch    11 | loss: 5.5530782CurrentTrain: epoch  7, batch    12 | loss: 5.2616329CurrentTrain: epoch  7, batch    13 | loss: 5.5529518CurrentTrain: epoch  7, batch    14 | loss: 5.7046151CurrentTrain: epoch  7, batch    15 | loss: 5.5530725CurrentTrain: epoch  7, batch    16 | loss: 5.3517618CurrentTrain: epoch  7, batch    17 | loss: 5.6371727CurrentTrain: epoch  7, batch    18 | loss: 5.4586258CurrentTrain: epoch  7, batch    19 | loss: 5.6405039CurrentTrain: epoch  7, batch    20 | loss: 5.6834831CurrentTrain: epoch  7, batch    21 | loss: 5.4644499CurrentTrain: epoch  7, batch    22 | loss: 5.2752485CurrentTrain: epoch  7, batch    23 | loss: 6.0380902CurrentTrain: epoch  7, batch    24 | loss: 5.0765338CurrentTrain: epoch  7, batch    25 | loss: 5.3354988CurrentTrain: epoch  7, batch    26 | loss: 5.5516329CurrentTrain: epoch  7, batch    27 | loss: 5.3817105CurrentTrain: epoch  7, batch    28 | loss: 5.6680412CurrentTrain: epoch  7, batch    29 | loss: 5.4574909CurrentTrain: epoch  7, batch    30 | loss: 5.3431740CurrentTrain: epoch  7, batch    31 | loss: 5.5842242CurrentTrain: epoch  7, batch    32 | loss: 5.5439301CurrentTrain: epoch  7, batch    33 | loss: 5.8318253CurrentTrain: epoch  7, batch    34 | loss: 5.4044447CurrentTrain: epoch  7, batch    35 | loss: 5.8726020CurrentTrain: epoch  7, batch    36 | loss: 5.3058815CurrentTrain: epoch  7, batch    37 | loss: 5.8104944CurrentTrain: epoch  8, batch     0 | loss: 5.4317036CurrentTrain: epoch  8, batch     1 | loss: 5.3958578CurrentTrain: epoch  8, batch     2 | loss: 5.5616274CurrentTrain: epoch  8, batch     3 | loss: 5.1134787CurrentTrain: epoch  8, batch     4 | loss: 5.3385053CurrentTrain: epoch  8, batch     5 | loss: 5.3067794CurrentTrain: epoch  8, batch     6 | loss: 5.2842722CurrentTrain: epoch  8, batch     7 | loss: 5.3924065CurrentTrain: epoch  8, batch     8 | loss: 5.1761098CurrentTrain: epoch  8, batch     9 | loss: 5.2271843CurrentTrain: epoch  8, batch    10 | loss: 5.1613517CurrentTrain: epoch  8, batch    11 | loss: 5.1542363CurrentTrain: epoch  8, batch    12 | loss: 5.2328167CurrentTrain: epoch  8, batch    13 | loss: 5.5211868CurrentTrain: epoch  8, batch    14 | loss: 5.8145490CurrentTrain: epoch  8, batch    15 | loss: 5.4857550CurrentTrain: epoch  8, batch    16 | loss: 5.5433736CurrentTrain: epoch  8, batch    17 | loss: 5.1109467CurrentTrain: epoch  8, batch    18 | loss: 5.5671840CurrentTrain: epoch  8, batch    19 | loss: 5.3198271CurrentTrain: epoch  8, batch    20 | loss: 5.1904140CurrentTrain: epoch  8, batch    21 | loss: 5.1472673CurrentTrain: epoch  8, batch    22 | loss: 6.1993666CurrentTrain: epoch  8, batch    23 | loss: 5.9343271CurrentTrain: epoch  8, batch    24 | loss: 5.0894642CurrentTrain: epoch  8, batch    25 | loss: 5.8717785CurrentTrain: epoch  8, batch    26 | loss: 5.4969192CurrentTrain: epoch  8, batch    27 | loss: 5.5124288CurrentTrain: epoch  8, batch    28 | loss: 5.4030457CurrentTrain: epoch  8, batch    29 | loss: 5.5645485CurrentTrain: epoch  8, batch    30 | loss: 5.0139732CurrentTrain: epoch  8, batch    31 | loss: 5.7876463CurrentTrain: epoch  8, batch    32 | loss: 5.2257042CurrentTrain: epoch  8, batch    33 | loss: 5.2300491CurrentTrain: epoch  8, batch    34 | loss: 4.9741082CurrentTrain: epoch  8, batch    35 | loss: 5.3295979CurrentTrain: epoch  8, batch    36 | loss: 5.3243537CurrentTrain: epoch  8, batch    37 | loss: 4.9356380CurrentTrain: epoch  9, batch     0 | loss: 5.1930752CurrentTrain: epoch  9, batch     1 | loss: 5.1921329CurrentTrain: epoch  9, batch     2 | loss: 5.2294731CurrentTrain: epoch  9, batch     3 | loss: 5.3462420CurrentTrain: epoch  9, batch     4 | loss: 5.2144794CurrentTrain: epoch  9, batch     5 | loss: 5.2411604CurrentTrain: epoch  9, batch     6 | loss: 5.1950560CurrentTrain: epoch  9, batch     7 | loss: 4.8606482CurrentTrain: epoch  9, batch     8 | loss: 5.1387486CurrentTrain: epoch  9, batch     9 | loss: 5.3803711CurrentTrain: epoch  9, batch    10 | loss: 5.2895551CurrentTrain: epoch  9, batch    11 | loss: 5.6202307CurrentTrain: epoch  9, batch    12 | loss: 4.7456155CurrentTrain: epoch  9, batch    13 | loss: 5.1648083CurrentTrain: epoch  9, batch    14 | loss: 5.1291509CurrentTrain: epoch  9, batch    15 | loss: 5.0791206CurrentTrain: epoch  9, batch    16 | loss: 5.2978091CurrentTrain: epoch  9, batch    17 | loss: 5.1251478CurrentTrain: epoch  9, batch    18 | loss: 5.1539183CurrentTrain: epoch  9, batch    19 | loss: 5.0089483CurrentTrain: epoch  9, batch    20 | loss: 5.0704641CurrentTrain: epoch  9, batch    21 | loss: 4.9047794CurrentTrain: epoch  9, batch    22 | loss: 4.8887787CurrentTrain: epoch  9, batch    23 | loss: 5.1153011CurrentTrain: epoch  9, batch    24 | loss: 5.3109055CurrentTrain: epoch  9, batch    25 | loss: 4.9684782CurrentTrain: epoch  9, batch    26 | loss: 4.8906231CurrentTrain: epoch  9, batch    27 | loss: 5.1514235CurrentTrain: epoch  9, batch    28 | loss: 4.8202515CurrentTrain: epoch  9, batch    29 | loss: 5.0124664CurrentTrain: epoch  9, batch    30 | loss: 5.0817151CurrentTrain: epoch  9, batch    31 | loss: 4.8220859CurrentTrain: epoch  9, batch    32 | loss: 5.0820837CurrentTrain: epoch  9, batch    33 | loss: 5.3655233CurrentTrain: epoch  9, batch    34 | loss: 4.7996535CurrentTrain: epoch  9, batch    35 | loss: 5.2512970CurrentTrain: epoch  9, batch    36 | loss: 4.9244585CurrentTrain: epoch  9, batch    37 | loss: 4.8974943
#### person countries of residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person countries of residence" means a person is living in a country.
Here is an example:
Relation: person countries of residence
Context: the head of iran 's atomic energy organisation , ali akbar salehi , told domestic news agencies the new proposal had come in response to iran 's move last week to begin enriching uranium itself to the 20 percent level required for a tehran medical research rector after rejecting a previous offer .
Head Entity: ali akbar salehi
Tail Entity: iran
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person countries of residence":
Relation: person countries of residence  
Context: After years of living in the bustling city of New York, Maria Gonzalez decided to return to her hometown in Mexico, where she felt a stronger connection to her roots and family.  
Head Entity: Maria Gonzalez  
Tail Entity: Mexico  

Relation: person countries of residence  
Context: Following his successful career in London, James Smith relocated to Australia, seeking a more laid-back lifestyle and better weather.  
Head Entity: James Smith  
Tail Entity: Australia  
#### organization top members employees ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization top members employees" means an organization 's employer is a person.
Here is an example:
Relation: organization top members employees
Context: kao charng , vice chairman of the cabinet-level mainland affairs council -lrb- mac -rrb- , explained that the termination clause was added to give both sides an option after the implementation of the tariff-cutting economic cooperation framework agreement -lrb- ecfa -rrb- .
Head Entity: mainland affairs council
Tail Entity: kao charng
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization top members employees":
Relation: organization top members employees  
Context: elon musk, the CEO of spacex, announced the successful launch of the latest satellite into orbit, marking another milestone for the company.  
Head Entity: spacex  
Tail Entity: elon musk  

Relation: organization top members employees  
Context: sheryl sandberg, the chief operating officer of facebook, spoke at the conference about the importance of data privacy and user trust in the digital age.  
Head Entity: facebook  
Tail Entity: sheryl sandberg  
#### organization member of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization member of" means an organization is the member of an organization.
Here is an example:
Relation: organization member of
Context: south africa 1-5 leyds cnr biccard streets cosatu 's concern has been that the scorpions ' existence as a branch of the national directorate of public prosecutions blurred the necessary separation of functions between those investigating crime and those prosecuting the criminals .
Head Entity: scorpions
Tail Entity: national directorate of public prosecutions
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization member of":
Relation: organization member of  
Context: The International Federation of Red Cross and Red Crescent Societies (IFRC) is a global humanitarian organization that coordinates the activities of national Red Cross and Red Crescent societies around the world.  
Head Entity: International Federation of Red Cross and Red Crescent Societies  
Tail Entity: national Red Cross and Red Crescent societies  

Relation: organization member of  
Context: The European Union (EU) is a political and economic union of member states that are located primarily in Europe, working together to promote peace and stability in the region.  
Head Entity: European Union  
Tail Entity: member states  
#### person origin ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person origin" means a person 's nationality is a country.
Here is an example:
Relation: person origin
Context: urgent chadian pm dies in paris hospital chad 's prime minister pascal yoadimnadji has died from a cerebral hemorrhage in a paris hospital , the chadian ambassador said friday .
Head Entity: pascal yoadimnadji
Tail Entity: chadian
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person origin":
Relation: person origin  
Context: the renowned scientist albert einstein was born in ulm, germany, and later became a citizen of the united states.  
Head Entity: albert einstein  
Tail Entity: german  

Relation: person origin  
Context: the famous author chimamanda ngozi adichie hails from enugu, nigeria, where she developed her passion for storytelling.  
Head Entity: chimamanda ngozi adichie  
Tail Entity: nigerian  
#### person title ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person title" means a person 's title is a title.
Here is an example:
Relation: person title
Context: `` it has to have some tempo , it has to have some energy , '' said sean ross , vice president of music and programming at edison media research , which surveys the radio industry .
Head Entity: sean ross
Tail Entity: vice president
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person title":
Relation: person title  
Context: "After years of hard work and dedication, Maria Gonzalez was promoted to Chief Financial Officer at Tech Innovations, where she has been instrumental in driving the company's financial strategy."  
Head Entity: Maria Gonzalez  
Tail Entity: Chief Financial Officer  

Relation: person title  
Context: "During the annual conference, Dr. James Lee, the lead researcher at the National Institute of Health, presented his findings on the latest advancements in medical technology."  
Head Entity: Dr. James Lee  
Tail Entity: lead researcher  
#### organization country of headquarters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization country of headquarters" means an organization is located in a country.
Here is an example:
Relation: organization country of headquarters
Context: general motors china , china 's faw announce $ 293 million commercial vehicles joint venture .
Head Entity: faw
Tail Entity: china
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization country of headquarters":
Relation: organization country of headquarters  
Context: samsung electronics has established its main office in suwon, south korea, focusing on innovation and technology.  
Head Entity: samsung electronics  
Tail Entity: south korea  

Relation: organization country of headquarters  
Context: the headquarters of nestlé is located in vevey, switzerland, where it oversees its global operations.  
Head Entity: nestlé  
Tail Entity: switzerland  
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 82.81%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 82.50%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 82.14%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 84.38%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 86.11%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 86.88%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 88.07%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 88.54%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 88.94%   [EVAL] batch:   13 | acc: 75.00%,  total acc: 87.95%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 86.67%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 84.77%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 84.19%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 83.33%   [EVAL] batch:   18 | acc: 81.25%,  total acc: 83.22%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 83.44%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 84.23%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 84.94%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 85.60%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 86.20%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 86.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 87.26%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 87.95%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 88.36%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 88.12%   [EVAL] batch:   30 | acc: 81.25%,  total acc: 87.90%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 88.09%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 86.55%   
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 82.81%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 82.50%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 82.14%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 84.38%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 86.11%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 86.88%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 88.07%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 88.54%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 88.94%   [EVAL] batch:   13 | acc: 75.00%,  total acc: 87.95%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 86.67%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 84.77%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 84.19%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 83.33%   [EVAL] batch:   18 | acc: 81.25%,  total acc: 83.22%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 83.44%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 84.23%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 84.94%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 85.60%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 86.20%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 86.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 87.26%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 87.95%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 88.36%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 88.12%   [EVAL] batch:   30 | acc: 81.25%,  total acc: 87.90%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 88.09%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 86.55%   
cur_acc:  ['0.8655']
his_acc:  ['0.8655']
CurrentTrain: epoch  0, batch     0 | loss: 6.4248533CurrentTrain: epoch  0, batch     1 | loss: 5.8796329CurrentTrain: epoch  1, batch     0 | loss: 5.9591646CurrentTrain: epoch  1, batch     1 | loss: 4.9089227CurrentTrain: epoch  2, batch     0 | loss: 5.2661195CurrentTrain: epoch  2, batch     1 | loss: 4.8037314CurrentTrain: epoch  3, batch     0 | loss: 4.5340357CurrentTrain: epoch  3, batch     1 | loss: 5.6235013CurrentTrain: epoch  4, batch     0 | loss: 3.7973127CurrentTrain: epoch  4, batch     1 | loss: 5.5554314CurrentTrain: epoch  5, batch     0 | loss: 3.8947999CurrentTrain: epoch  5, batch     1 | loss: 4.6380363CurrentTrain: epoch  6, batch     0 | loss: 3.6410255CurrentTrain: epoch  6, batch     1 | loss: 3.8407223CurrentTrain: epoch  7, batch     0 | loss: 3.5202975CurrentTrain: epoch  7, batch     1 | loss: 4.4413490CurrentTrain: epoch  8, batch     0 | loss: 3.4660172CurrentTrain: epoch  8, batch     1 | loss: 3.4718840CurrentTrain: epoch  9, batch     0 | loss: 3.4295425CurrentTrain: epoch  9, batch     1 | loss: 3.0251770
#### person cities of residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person cities of residence" means a person is living in a city.
Here is an example:
Relation: person cities of residence
Context: daniel castleman , chief of investigations for the manhattan district attorney , robert m. morgenthau , described tower data as `` circumstantial but convincing . ''
Head Entity: robert m. morgenthau
Tail Entity: manhattan
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person cities of residence":
Relation: person cities of residence  
Context: After years of living in the bustling streets of New York City, Sarah Thompson decided to move to the quieter suburbs of Los Angeles for a change of pace.  
Head Entity: Sarah Thompson  
Tail Entity: Los Angeles  

Relation: person cities of residence  
Context: During his time as a student, Michael Johnson spent several years in Chicago before relocating to Seattle for his new job.  
Head Entity: Michael Johnson  
Tail Entity: Seattle  
#### person schools attended ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person schools attended" means a person 's school is an organization.
Here is an example:
Relation: person schools attended
Context: after world war ii , he attended the university of southern california , where he became editor of a college magazine .
Head Entity: he
Tail Entity: university of southern california
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person schools attended":
Relation: person schools attended  
Context: She graduated from Harvard University with a degree in psychology before pursuing her career in clinical research.  
Head Entity: She  
Tail Entity: Harvard University  

Relation: person schools attended  
Context: After completing his high school education, he enrolled at Stanford University to study computer science.  
Head Entity: he  
Tail Entity: Stanford University  
#### person country of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person country of death" means a person was died in a country.
Here is an example:
Relation: person country of death
Context: u.s. rep. parren mitchell , founding member of congressional black caucus , dies at 85
Head Entity: parren mitchell
Tail Entity: u.s.
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person country of death":
Relation: person country of death  
Context: renowned physicist stephen hawking passes away in cambridge, england at the age of 76  
Head Entity: stephen hawking  
Tail Entity: england  

Relation: person country of death  
Context: famous author gabriel garcia marquez dies in mexico city, mexico, leaving behind a legacy of magical realism  
Head Entity: gabriel garcia marquez  
Tail Entity: mexico  
#### person children ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person children" means a person 's children is a person.
Here is an example:
Relation: person children
Context: he is survived by two stepdaughters , barbara a. hammond and brenda l. stevenson ; a stepson , michael a. taylor ; two grandchildren and one great-grandchild .
Head Entity: he
Tail Entity: brenda l. stevenson
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person children":
Relation: person children  
Context: she has three children, including her son, john, and her daughters, emily and sarah, who are both pursuing their studies.  
Head Entity: she  
Tail Entity: emily  

Relation: person children  
Context: after the passing of their father, the siblings, including jake and his sister, lila, took on the responsibility of caring for their younger brother, max.  
Head Entity: jake  
Tail Entity: lila  
#### person charges ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person charges" means a person was charged with an event.
Here is an example:
Relation: person charges
Context: ferrara said he was innocent of limoli 's slaying , but he pleaded guilty in 1992 to murder , along with racketeering charges , under a deal that sent him to prison for 22 years , rather than go to trial and risk a conviction that could lead to life in prison .
Head Entity: ferrara
Tail Entity: racketeering
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person charges":
Relation: person charges  
Context: After a lengthy investigation, the authorities announced that Johnson was charged with embezzlement, a crime that could result in significant prison time if convicted.  
Head Entity: Johnson  
Tail Entity: embezzlement  

Relation: person charges  
Context: The district attorney confirmed that Smith was charged with assault following the altercation that took place outside the nightclub last weekend.  
Head Entity: Smith  
Tail Entity: assault  
Mixup data size:  104
MixupTrain:  epoch  0, batch     0 | loss: 6.5211275MixupTrain:  epoch  0, batch     1 | loss: 6.6416268MixupTrain:  epoch  0, batch     2 | loss: 5.6998553MixupTrain:  epoch  0, batch     3 | loss: 5.7927399MixupTrain:  epoch  0, batch     4 | loss: 5.1976899MixupTrain:  epoch  0, batch     5 | loss: 4.8827249MixupTrain:  epoch  0, batch     6 | loss: 4.2003822
MemoryTrain:  epoch  0, batch     0 | loss: 3.3801298MemoryTrain:  epoch  0, batch     1 | loss: 3.3151903MemoryTrain:  epoch  0, batch     2 | loss: 3.2823002MemoryTrain:  epoch  1, batch     0 | loss: 2.8805759MemoryTrain:  epoch  1, batch     1 | loss: 2.6412954MemoryTrain:  epoch  1, batch     2 | loss: 1.3844265MemoryTrain:  epoch  2, batch     0 | loss: 2.5169845MemoryTrain:  epoch  2, batch     1 | loss: 2.0995302MemoryTrain:  epoch  2, batch     2 | loss: 1.7023281MemoryTrain:  epoch  3, batch     0 | loss: 2.0355132MemoryTrain:  epoch  3, batch     1 | loss: 2.1342163MemoryTrain:  epoch  3, batch     2 | loss: 1.4530677MemoryTrain:  epoch  4, batch     0 | loss: 2.3345809MemoryTrain:  epoch  4, batch     1 | loss: 1.8116486MemoryTrain:  epoch  4, batch     2 | loss: 2.8129864MemoryTrain:  epoch  5, batch     0 | loss: 2.2816601MemoryTrain:  epoch  5, batch     1 | loss: 1.6642094MemoryTrain:  epoch  5, batch     2 | loss: 1.3661617MemoryTrain:  epoch  6, batch     0 | loss: 1.7075310MemoryTrain:  epoch  6, batch     1 | loss: 1.7317462MemoryTrain:  epoch  6, batch     2 | loss: 1.8409052MemoryTrain:  epoch  7, batch     0 | loss: 1.5973330MemoryTrain:  epoch  7, batch     1 | loss: 1.9811534MemoryTrain:  epoch  7, batch     2 | loss: 1.4687141MemoryTrain:  epoch  8, batch     0 | loss: 1.7146829MemoryTrain:  epoch  8, batch     1 | loss: 1.6055624MemoryTrain:  epoch  8, batch     2 | loss: 1.1751888MemoryTrain:  epoch  9, batch     0 | loss: 1.7239134MemoryTrain:  epoch  9, batch     1 | loss: 1.5131364MemoryTrain:  epoch  9, batch     2 | loss: 1.6644164
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    3 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 92.50%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 92.71%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 92.97%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 90.28%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 91.25%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 91.48%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 92.19%   [EVAL] batch:   12 | acc: 100.00%,  total acc: 92.79%   [EVAL] batch:   13 | acc: 100.00%,  total acc: 93.30%   [EVAL] batch:   14 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 94.14%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 94.49%   [EVAL] batch:   17 | acc: 25.00%,  total acc: 90.62%   
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 56.25%,  total acc: 71.88%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 75.00%   [EVAL] batch:    3 | acc: 50.00%,  total acc: 68.75%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 70.00%   [EVAL] batch:    5 | acc: 62.50%,  total acc: 68.75%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 71.43%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 77.78%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 79.38%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 81.77%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 82.21%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 81.25%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 80.83%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 79.30%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 79.04%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 78.12%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 77.63%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 78.12%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 79.17%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 80.11%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 80.98%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 81.77%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 82.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 83.17%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 83.56%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 84.15%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 84.70%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 84.79%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 85.08%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 85.35%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 85.61%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 85.85%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 86.07%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 86.28%   [EVAL] batch:   36 | acc: 87.50%,  total acc: 86.32%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 86.51%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 86.86%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 87.19%   [EVAL] batch:   40 | acc: 56.25%,  total acc: 86.43%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 86.76%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 86.92%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 87.22%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 87.77%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 88.03%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 88.28%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 88.52%   [EVAL] batch:   49 | acc: 62.50%,  total acc: 88.00%   
cur_acc:  ['0.8655', '0.9062']
his_acc:  ['0.8655', '0.8800']
CurrentTrain: epoch  0, batch     0 | loss: 6.0330019CurrentTrain: epoch  0, batch     1 | loss: 6.5376105CurrentTrain: epoch  1, batch     0 | loss: 5.7712164CurrentTrain: epoch  1, batch     1 | loss: 5.7701735CurrentTrain: epoch  2, batch     0 | loss: 5.1058664CurrentTrain: epoch  2, batch     1 | loss: 4.4254990CurrentTrain: epoch  3, batch     0 | loss: 4.1321220CurrentTrain: epoch  3, batch     1 | loss: 4.1112885CurrentTrain: epoch  4, batch     0 | loss: 3.8763037CurrentTrain: epoch  4, batch     1 | loss: 4.0343003CurrentTrain: epoch  5, batch     0 | loss: 4.0813780CurrentTrain: epoch  5, batch     1 | loss: 3.3695459CurrentTrain: epoch  6, batch     0 | loss: 3.3437746CurrentTrain: epoch  6, batch     1 | loss: 3.2026308CurrentTrain: epoch  7, batch     0 | loss: 3.0270462CurrentTrain: epoch  7, batch     1 | loss: 3.2474837CurrentTrain: epoch  8, batch     0 | loss: 3.1259744CurrentTrain: epoch  8, batch     1 | loss: 2.5873854CurrentTrain: epoch  9, batch     0 | loss: 2.5919805CurrentTrain: epoch  9, batch     1 | loss: 2.7639174
#### person date of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person date of birth" means a person was born in a date.
Here is an example:
Relation: person date of birth
Context: born of schoolteacher parents in the western town of sabaneta on july 28 , 1954 , chavez studied at the military academy of venezuela in caracas .
Head Entity: chavez
Tail Entity: july 28 , 1954
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person date of birth":
Relation: person date of birth  
Context: the famous physicist albert einstein was born in ulm, germany, on march 14, 1879, and later developed the theory of relativity.  
Head Entity: albert einstein  
Tail Entity: march 14, 1879  

Relation: person date of birth  
Context: the renowned author mark twain, known for his classic novels, was born in florida, missouri, on november 30, 1835.  
Head Entity: mark twain  
Tail Entity: november 30, 1835  
#### person stateorprovince of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person stateorprovince of birth" means a person was died in a state or province.
Here is an example:
Relation: person stateorprovince of birth
Context: joseph simpson farland was born on aug 11 , 1914 , in clarksburg , wva , the only child of richard and grace simpson farland .
Head Entity: joseph simpson farland
Tail Entity: wva
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person stateorprovince of birth":
Relation: person stateorprovince of birth  
Context: martha stewart was born on august 3, 1941, in jersey city, new jersey, to parents of polish descent.  
Head Entity: martha stewart  
Tail Entity: new jersey  

Relation: person stateorprovince of birth  
Context: barack obama was born on august 4, 1961, in honolulu, hawaii, where he spent most of his childhood.  
Head Entity: barack obama  
Tail Entity: hawaii  
#### person parents ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person parents" means a person 's parent is a person.
Here is an example:
Relation: person parents
Context: lynne spears told the court that lutfi had treated her daughter like a hostage in her own home , drugged her and took over her finances .
Head Entity: her
Tail Entity: lynne spears
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person parents":
Relation: person parents  
Context: During the family reunion, Sarah introduced her father, John, to her friends, highlighting how much he has influenced her life choices.  
Head Entity: her father  
Tail Entity: John  

Relation: person parents  
Context: Emily often shares stories about her mother, who was a strong influence in her decision to pursue a career in medicine.  
Head Entity: her mother  
Tail Entity: Emily
#### person employee of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person employee of" means a person is an employee of an organization.
Here is an example:
Relation: person employee of
Context: kell hath no fury : publicist and mtv reality star kelly cutrone is wasting no time in kicking her brands -lrb- including her p.r. firm people 's revolution and , increasingly , kelly cutrone herself -rrb- into high gear in 2010 .
Head Entity: kelly cutrone
Tail Entity: mtv
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person employee of":
Relation: person employee of  
Context: After years of hard work and dedication, Sarah Thompson has finally landed a position at one of the top tech companies in Silicon Valley, where she will be contributing to innovative projects.  
Head Entity: Sarah Thompson  
Tail Entity: tech companies  

Relation: person employee of  
Context: John Smith, a talented graphic designer, has been working for Creative Solutions for over five years, helping to shape the visual identity of numerous brands.  
Head Entity: John Smith  
Tail Entity: Creative Solutions  
#### person stateorprovince of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person stateorprovince of death" means a person was died in a state or province.
Here is an example:
Relation: person stateorprovince of death
Context: irene morgan kirkaldy , whose defiance of bus segregation laws more than a decade before rosa parks ' landmark case helped lay the foundation for later civil rights victories , died friday at her home in hayes , va. .
Head Entity: irene morgan kirkaldy
Tail Entity: va.
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person stateorprovince of death":
Relation: person stateorprovince of death  
Context: john doe, a renowned author known for his impactful novels, passed away peacefully in his sleep at his residence in california last night.  
Head Entity: john doe  
Tail Entity: california  

Relation: person stateorprovince of death  
Context: after a long battle with illness, elizabeth taylor, the iconic actress, succumbed to her health issues and died in a hospital in nevada.  
Head Entity: elizabeth taylor  
Tail Entity: nevada  
Mixup data size:  135
MixupTrain:  epoch  0, batch     0 | loss: 3.7249546MixupTrain:  epoch  0, batch     1 | loss: 3.5864562MixupTrain:  epoch  0, batch     2 | loss: 3.5706843MixupTrain:  epoch  0, batch     3 | loss: 3.4131105MixupTrain:  epoch  0, batch     4 | loss: 3.4298961MixupTrain:  epoch  0, batch     5 | loss: 3.1742775MixupTrain:  epoch  0, batch     6 | loss: 3.2639402MixupTrain:  epoch  0, batch     7 | loss: 2.7789803MixupTrain:  epoch  0, batch     8 | loss: 2.6756489
MemoryTrain:  epoch  0, batch     0 | loss: 2.6025829MemoryTrain:  epoch  0, batch     1 | loss: 2.2165613MemoryTrain:  epoch  0, batch     2 | loss: 3.7879477MemoryTrain:  epoch  1, batch     0 | loss: 3.0265856MemoryTrain:  epoch  1, batch     1 | loss: 1.9206710MemoryTrain:  epoch  1, batch     2 | loss: 2.5935261MemoryTrain:  epoch  2, batch     0 | loss: 2.3407772MemoryTrain:  epoch  2, batch     1 | loss: 2.3312464MemoryTrain:  epoch  2, batch     2 | loss: 1.7120994MemoryTrain:  epoch  3, batch     0 | loss: 2.2239389MemoryTrain:  epoch  3, batch     1 | loss: 1.8210968MemoryTrain:  epoch  3, batch     2 | loss: 1.9858710MemoryTrain:  epoch  4, batch     0 | loss: 1.7544264MemoryTrain:  epoch  4, batch     1 | loss: 1.8514488MemoryTrain:  epoch  4, batch     2 | loss: 1.7467463MemoryTrain:  epoch  5, batch     0 | loss: 1.8268747MemoryTrain:  epoch  5, batch     1 | loss: 1.4757371MemoryTrain:  epoch  5, batch     2 | loss: 1.9856349MemoryTrain:  epoch  6, batch     0 | loss: 1.6672279MemoryTrain:  epoch  6, batch     1 | loss: 1.6653218MemoryTrain:  epoch  6, batch     2 | loss: 1.8240563MemoryTrain:  epoch  7, batch     0 | loss: 1.7263885MemoryTrain:  epoch  7, batch     1 | loss: 1.7079213MemoryTrain:  epoch  7, batch     2 | loss: 1.5177284MemoryTrain:  epoch  8, batch     0 | loss: 1.5179634MemoryTrain:  epoch  8, batch     1 | loss: 1.7304771MemoryTrain:  epoch  8, batch     2 | loss: 1.6563768MemoryTrain:  epoch  9, batch     0 | loss: 1.4753900MemoryTrain:  epoch  9, batch     1 | loss: 1.5445006MemoryTrain:  epoch  9, batch     2 | loss: 1.6266847
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 62.50%,  total acc: 75.00%   [EVAL] batch:    2 | acc: 62.50%,  total acc: 70.83%   [EVAL] batch:    3 | acc: 62.50%,  total acc: 68.75%   [EVAL] batch:    4 | acc: 56.25%,  total acc: 66.25%   [EVAL] batch:    5 | acc: 31.25%,  total acc: 60.42%   [EVAL] batch:    6 | acc: 50.00%,  total acc: 58.93%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 63.28%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 66.67%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 68.12%   [EVAL] batch:   10 | acc: 75.00%,  total acc: 68.75%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 69.79%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 70.19%   [EVAL] batch:   13 | acc: 43.75%,  total acc: 68.30%   
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 62.50%,  total acc: 79.69%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 80.00%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 79.17%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 83.59%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 85.42%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 86.25%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 88.02%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 87.98%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 86.61%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 85.83%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 83.98%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 83.46%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 82.29%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 81.25%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 81.56%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 82.44%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 83.24%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 83.97%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 84.64%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 85.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 85.82%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 86.11%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 86.61%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 87.07%   [EVAL] batch:   29 | acc: 93.75%,  total acc: 87.29%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 87.70%   [EVAL] batch:   32 | acc: 87.50%,  total acc: 87.69%   [EVAL] batch:   33 | acc: 87.50%,  total acc: 87.68%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 87.32%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 87.33%   [EVAL] batch:   36 | acc: 87.50%,  total acc: 87.33%   [EVAL] batch:   37 | acc: 81.25%,  total acc: 87.17%   [EVAL] batch:   38 | acc: 81.25%,  total acc: 87.02%   [EVAL] batch:   39 | acc: 93.75%,  total acc: 87.19%   [EVAL] batch:   40 | acc: 31.25%,  total acc: 85.82%   [EVAL] batch:   41 | acc: 68.75%,  total acc: 85.42%   [EVAL] batch:   42 | acc: 68.75%,  total acc: 85.03%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 84.94%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 85.28%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 85.60%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 85.90%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 86.20%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 86.48%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 86.75%   [EVAL] batch:   50 | acc: 68.75%,  total acc: 86.40%   [EVAL] batch:   51 | acc: 68.75%,  total acc: 86.06%   [EVAL] batch:   52 | acc: 68.75%,  total acc: 85.73%   [EVAL] batch:   53 | acc: 56.25%,  total acc: 85.19%   [EVAL] batch:   54 | acc: 37.50%,  total acc: 84.32%   [EVAL] batch:   55 | acc: 37.50%,  total acc: 83.48%   [EVAL] batch:   56 | acc: 75.00%,  total acc: 83.33%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 83.51%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 83.58%   [EVAL] batch:   59 | acc: 87.50%,  total acc: 83.65%   [EVAL] batch:   60 | acc: 62.50%,  total acc: 83.30%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 83.37%   [EVAL] batch:   62 | acc: 75.00%,  total acc: 83.23%   [EVAL] batch:   63 | acc: 12.50%,  total acc: 82.13%   
cur_acc:  ['0.8655', '0.9062', '0.6830']
his_acc:  ['0.8655', '0.8800', '0.8213']
CurrentTrain: epoch  0, batch     0 | loss: 4.5090399CurrentTrain: epoch  0, batch     1 | loss: 4.8591142CurrentTrain: epoch  1, batch     0 | loss: 3.6434999CurrentTrain: epoch  1, batch     1 | loss: 3.5366395CurrentTrain: epoch  2, batch     0 | loss: 3.2504673CurrentTrain: epoch  2, batch     1 | loss: 3.4218030CurrentTrain: epoch  3, batch     0 | loss: 2.7882321CurrentTrain: epoch  3, batch     1 | loss: 2.9439304CurrentTrain: epoch  4, batch     0 | loss: 2.7327967CurrentTrain: epoch  4, batch     1 | loss: 2.4435146CurrentTrain: epoch  5, batch     0 | loss: 2.4848552CurrentTrain: epoch  5, batch     1 | loss: 2.2920768CurrentTrain: epoch  6, batch     0 | loss: 2.2432668CurrentTrain: epoch  6, batch     1 | loss: 2.3687973CurrentTrain: epoch  7, batch     0 | loss: 2.2752602CurrentTrain: epoch  7, batch     1 | loss: 2.0881824CurrentTrain: epoch  8, batch     0 | loss: 2.1721892CurrentTrain: epoch  8, batch     1 | loss: 2.1768174CurrentTrain: epoch  9, batch     0 | loss: 2.3778028CurrentTrain: epoch  9, batch     1 | loss: 2.1473835
#### organization founded ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization founded" means an organization was found in a date.
Here is an example:
Relation: organization founded
Context: the jnf was founded in 1901 to buy plots in palestine , then ruled by the ottomans .
Head Entity: jnf
Tail Entity: 1901
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization founded":
Relation: organization founded  
Context: the united nations was established in 1945 to promote international cooperation and peace.  
Head Entity: united nations  
Tail Entity: 1945  

Relation: organization founded  
Context: the world health organization was created in 1948 to address global health issues.  
Head Entity: world health organization  
Tail Entity: 1948  
#### person age ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person age" means a person 's age is a number.
Here is an example:
Relation: person age
Context: susan boyle is 48 years old now .
Head Entity: susan boyle
Tail Entity: 48
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person age":
Relation: person age  
Context: john smith turned 30 last week.  
Head Entity: john smith  
Tail Entity: 30  

Relation: person age  
Context: the famous actor will celebrate his 45th birthday next month.  
Head Entity: the famous actor  
Tail Entity: 45  
#### person city of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person city of birth" means a person was born in a city.
Here is an example:
Relation: person city of birth
Context: rothman was born in san francisco in 1932 in an orthodox jewish family .
Head Entity: rothman
Tail Entity: san francisco
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person city of birth":
Relation: person city of birth  
Context: after spending his early years in new york, he moved to los angeles where he began his career.  
Head Entity: he  
Tail Entity: los angeles  

Relation: person city of birth  
Context: the famous author was born in a small town near boston, which greatly influenced his writing.  
Head Entity: the famous author  
Tail Entity: boston  
#### organization members ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization members" means an organization 's member is an organization.
Here is an example:
Relation: organization members
Context: it was berger who made clarke a member of the white house principals committee when it met to discuss terrorist threats , allowing an otherwise middle-ranking nsc bureaucrat to treat tenet and secretary of state madeleine albright as equals -lrb- which the empire-building clarke was pleased to do -rrb- .
Head Entity: nsc
Tail Entity: white house principals committee
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization members":
Relation: organization members  
Context: The board of directors of the tech startup includes several prominent figures from the industry, such as the CEO of Innovatech, who has been instrumental in guiding the company’s strategic direction.  
Head Entity: tech startup  
Tail Entity: Innovatech  

Relation: organization members  
Context: During the annual conference, the president of the environmental advocacy group announced the inclusion of several new organizations, highlighting the partnership with Green Future, which focuses on sustainable practices.  
Head Entity: environmental advocacy group  
Tail Entity: Green Future  
#### person religion ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person religion" means an person is the member of a religion.
Here is an example:
Relation: person religion
Context: the pope defended his action on the grounds that he could not refuse an audience to a head of state from a country with a strong catholic tradition unless he had clear-cut proof of the allegations against him .
Head Entity: he
Tail Entity: catholic
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person religion":
Relation: person religion  
Context: During the ceremony, the rabbi spoke about the importance of faith and community in Judaism, emphasizing how every member plays a vital role in upholding their traditions.  
Head Entity: rabbi  
Tail Entity: Judaism  

Relation: person religion  
Context: The young woman shared her experiences growing up in a Muslim household, highlighting the values of compassion and charity that are central to her beliefs.  
Head Entity: young woman  
Tail Entity: Muslim
Mixup data size:  165
MixupTrain:  epoch  0, batch     0 | loss: 3.3574686MixupTrain:  epoch  0, batch     1 | loss: 3.0876857MixupTrain:  epoch  0, batch     2 | loss: 2.9472362MixupTrain:  epoch  0, batch     3 | loss: 2.8575840MixupTrain:  epoch  0, batch     4 | loss: 2.9104940MixupTrain:  epoch  0, batch     5 | loss: 2.8132876MixupTrain:  epoch  0, batch     6 | loss: 3.8174838MixupTrain:  epoch  0, batch     7 | loss: 2.6141012MixupTrain:  epoch  0, batch     8 | loss: 2.8792438MixupTrain:  epoch  0, batch     9 | loss: 2.5112583MixupTrain:  epoch  0, batch    10 | loss: 2.0621383
MemoryTrain:  epoch  0, batch     0 | loss: 2.0506258MemoryTrain:  epoch  0, batch     1 | loss: 3.1294775MemoryTrain:  epoch  0, batch     2 | loss: 3.8118474MemoryTrain:  epoch  0, batch     3 | loss: 3.6098680MemoryTrain:  epoch  1, batch     0 | loss: 2.6677070MemoryTrain:  epoch  1, batch     1 | loss: 3.3362813MemoryTrain:  epoch  1, batch     2 | loss: 2.4833577MemoryTrain:  epoch  1, batch     3 | loss: 2.6439173MemoryTrain:  epoch  2, batch     0 | loss: 3.3249755MemoryTrain:  epoch  2, batch     1 | loss: 1.9108803MemoryTrain:  epoch  2, batch     2 | loss: 2.7680879MemoryTrain:  epoch  2, batch     3 | loss: 1.9661208MemoryTrain:  epoch  3, batch     0 | loss: 2.4154525MemoryTrain:  epoch  3, batch     1 | loss: 1.7008979MemoryTrain:  epoch  3, batch     2 | loss: 2.4437387MemoryTrain:  epoch  3, batch     3 | loss: 2.4584377MemoryTrain:  epoch  4, batch     0 | loss: 2.3213863MemoryTrain:  epoch  4, batch     1 | loss: 1.7755665MemoryTrain:  epoch  4, batch     2 | loss: 1.8361793MemoryTrain:  epoch  4, batch     3 | loss: 2.0326650MemoryTrain:  epoch  5, batch     0 | loss: 1.6556687MemoryTrain:  epoch  5, batch     1 | loss: 1.8176684MemoryTrain:  epoch  5, batch     2 | loss: 1.8072699MemoryTrain:  epoch  5, batch     3 | loss: 1.6558015MemoryTrain:  epoch  6, batch     0 | loss: 1.7032399MemoryTrain:  epoch  6, batch     1 | loss: 1.7885379MemoryTrain:  epoch  6, batch     2 | loss: 1.7707872MemoryTrain:  epoch  6, batch     3 | loss: 1.5385908MemoryTrain:  epoch  7, batch     0 | loss: 1.7993509MemoryTrain:  epoch  7, batch     1 | loss: 1.5626624MemoryTrain:  epoch  7, batch     2 | loss: 1.7626870MemoryTrain:  epoch  7, batch     3 | loss: 1.5097119MemoryTrain:  epoch  8, batch     0 | loss: 1.4437630MemoryTrain:  epoch  8, batch     1 | loss: 1.6101911MemoryTrain:  epoch  8, batch     2 | loss: 1.5472705MemoryTrain:  epoch  8, batch     3 | loss: 1.5137515MemoryTrain:  epoch  9, batch     0 | loss: 1.4689691MemoryTrain:  epoch  9, batch     1 | loss: 1.7302756MemoryTrain:  epoch  9, batch     2 | loss: 1.3815446MemoryTrain:  epoch  9, batch     3 | loss: 1.4785082
[EVAL] batch:    0 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    1 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    2 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    3 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    4 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    8 | acc: 62.50%,  total acc: 95.83%   [EVAL] batch:    9 | acc: 6.25%,  total acc: 86.88%   [EVAL] batch:   10 | acc: 31.25%,  total acc: 81.82%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:   12 | acc: 50.00%,  total acc: 78.85%   [EVAL] batch:   13 | acc: 50.00%,  total acc: 76.79%   
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 82.81%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 82.50%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 83.93%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 85.94%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 88.12%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 89.20%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 89.06%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 87.50%   [EVAL] batch:   13 | acc: 37.50%,  total acc: 83.93%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 82.92%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 81.25%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 80.88%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 79.86%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 79.28%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 79.69%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 80.65%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 82.07%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 82.55%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 83.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 83.89%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 84.26%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 84.82%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 85.34%   [EVAL] batch:   29 | acc: 93.75%,  total acc: 85.62%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 86.09%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 86.52%   [EVAL] batch:   32 | acc: 81.25%,  total acc: 86.36%   [EVAL] batch:   33 | acc: 68.75%,  total acc: 85.85%   [EVAL] batch:   34 | acc: 62.50%,  total acc: 85.18%   [EVAL] batch:   35 | acc: 75.00%,  total acc: 84.90%   [EVAL] batch:   36 | acc: 62.50%,  total acc: 84.29%   [EVAL] batch:   37 | acc: 81.25%,  total acc: 84.21%   [EVAL] batch:   38 | acc: 75.00%,  total acc: 83.97%   [EVAL] batch:   39 | acc: 93.75%,  total acc: 84.22%   [EVAL] batch:   40 | acc: 37.50%,  total acc: 83.08%   [EVAL] batch:   41 | acc: 75.00%,  total acc: 82.89%   [EVAL] batch:   42 | acc: 68.75%,  total acc: 82.56%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 82.53%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 82.92%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 83.29%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 83.64%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 83.98%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 84.31%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:   50 | acc: 75.00%,  total acc: 84.19%   [EVAL] batch:   51 | acc: 62.50%,  total acc: 83.77%   [EVAL] batch:   52 | acc: 68.75%,  total acc: 83.49%   [EVAL] batch:   53 | acc: 43.75%,  total acc: 82.75%   [EVAL] batch:   54 | acc: 50.00%,  total acc: 82.16%   [EVAL] batch:   55 | acc: 31.25%,  total acc: 81.25%   [EVAL] batch:   56 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 81.47%   [EVAL] batch:   58 | acc: 75.00%,  total acc: 81.36%   [EVAL] batch:   59 | acc: 87.50%,  total acc: 81.46%   [EVAL] batch:   60 | acc: 62.50%,  total acc: 81.15%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:   62 | acc: 62.50%,  total acc: 80.95%   [EVAL] batch:   63 | acc: 81.25%,  total acc: 80.96%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:   65 | acc: 100.00%,  total acc: 81.53%   [EVAL] batch:   66 | acc: 100.00%,  total acc: 81.81%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 82.08%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 82.34%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 82.59%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 82.83%   [EVAL] batch:   71 | acc: 93.75%,  total acc: 82.99%   [EVAL] batch:   72 | acc: 0.00%,  total acc: 81.85%   [EVAL] batch:   73 | acc: 18.75%,  total acc: 81.00%   [EVAL] batch:   74 | acc: 62.50%,  total acc: 80.75%   [EVAL] batch:   75 | acc: 62.50%,  total acc: 80.51%   [EVAL] batch:   76 | acc: 62.50%,  total acc: 80.28%   [EVAL] batch:   77 | acc: 6.25%,  total acc: 79.33%   
cur_acc:  ['0.8655', '0.9062', '0.6830', '0.7679']
his_acc:  ['0.8655', '0.8800', '0.8213', '0.7933']
CurrentTrain: epoch  0, batch     0 | loss: 4.2815552CurrentTrain: epoch  0, batch     1 | loss: 5.0588350CurrentTrain: epoch  1, batch     0 | loss: 3.6863966CurrentTrain: epoch  1, batch     1 | loss: 2.9947293CurrentTrain: epoch  2, batch     0 | loss: 3.1844215CurrentTrain: epoch  2, batch     1 | loss: 2.9471333CurrentTrain: epoch  3, batch     0 | loss: 2.7666583CurrentTrain: epoch  3, batch     1 | loss: 2.5534465CurrentTrain: epoch  4, batch     0 | loss: 2.6155396CurrentTrain: epoch  4, batch     1 | loss: 2.2818036CurrentTrain: epoch  5, batch     0 | loss: 2.5886385CurrentTrain: epoch  5, batch     1 | loss: 2.2935822CurrentTrain: epoch  6, batch     0 | loss: 2.2682834CurrentTrain: epoch  6, batch     1 | loss: 2.0526378CurrentTrain: epoch  7, batch     0 | loss: 2.0271010CurrentTrain: epoch  7, batch     1 | loss: 1.9480501CurrentTrain: epoch  8, batch     0 | loss: 2.1280563CurrentTrain: epoch  8, batch     1 | loss: 2.0678158CurrentTrain: epoch  9, batch     0 | loss: 2.1068983CurrentTrain: epoch  9, batch     1 | loss: 1.9082985
#### person cause of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person cause of death" means a person was died of an event.
Here is an example:
Relation: person cause of death
Context: pamela gardner ahearn , who served nine years as chief of protocol at the us house of representatives after earlier experience with the state department 's office of protocol and as elizabeth taylor 's executive assistant , died march 26 of a heart attack at her home in alexandria , va .
Head Entity: pamela gardner ahearn
Tail Entity: heart attack
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person cause of death":
Relation: person cause of death  
Context: john smith, a renowned scientist known for his groundbreaking research in genetics, passed away on july 15 due to complications from pneumonia while receiving treatment at a local hospital.  
Head Entity: john smith  
Tail Entity: pneumonia  

Relation: person cause of death  
Context: the famous actor, robert jones, tragically lost his life in a car accident on february 10, leaving behind a legacy of memorable performances and a grieving family.  
Head Entity: robert jones  
Tail Entity: car accident  
#### organization political religious affiliation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization political religious affiliation" means an organization is the member of a religion.
Here is an example:
Relation: organization political religious affiliation
Context: the militia has also suffered from allegations of being involved in clashes with rival shiite militias , particularly the badr corp , the military wing linked to supreme iraqi islamic council , the party led by powerful politician abdel aziz al-hakim .
Head Entity: badr corp
Tail Entity: shiite
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization political religious affiliation":
Relation: organization political religious affiliation  
Context: The Christian Democratic Union has been a significant player in German politics, often aligning itself with various religious groups to promote policies that reflect its Christian values.  
Head Entity: Christian Democratic Union  
Tail Entity: Christian  

Relation: organization political religious affiliation  
Context: The Islamic Society of North America has been actively involved in advocating for Muslim rights and has formed alliances with political parties to ensure that the interests of the Muslim community are represented.  
Head Entity: Islamic Society of North America  
Tail Entity: Muslim  
#### organization stateorprovince of headquarters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization stateorprovince of headquarters" means an organization is located in a state or province.
Here is an example:
Relation: organization stateorprovince of headquarters
Context: andrew lebow , an oil trader with mf global in new york , said investors have been discouraged by lower-than-expected oil imports in china and the disappointing growth in the u.s. economy .
Head Entity: mf global
Tail Entity: new york
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization stateorprovince of headquarters":
Relation: organization stateorprovince of headquarters  
Context: the headquarters of tech giant apple inc. is located in cupertino, california, where it has been a hub for innovation and development.  
Head Entity: apple inc.  
Tail Entity: cupertino, california  

Relation: organization stateorprovince of headquarters  
Context: the multinational corporation samsung electronics has its main office situated in suwon, south korea, which plays a crucial role in its global operations.  
Head Entity: samsung electronics  
Tail Entity: suwon, south korea  
#### person other family ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person other family" means a person 's relative is a person.
Here is an example:
Relation: person other family
Context: parren mitchell 's sister-in-law , juanita jackson mitchell , was the long - time head and legal counsel of the maryland naacp .
Head Entity: parren mitchell
Tail Entity: juanita jackson mitchell
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person other family":
Relation: person other family  
Context: During the family reunion, it was revealed that Sarah's cousin, Michael, had recently graduated from college with honors.  
Head Entity: Sarah  
Tail Entity: Michael  

Relation: person other family  
Context: In her memoir, Lisa shared heartwarming stories about her grandmother, Evelyn, who played a significant role in her upbringing.  
Head Entity: Lisa  
Tail Entity: Evelyn  
#### person city of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person city of death" means a person was died in a city.
Here is an example:
Relation: person city of death
Context: irene morgan kirkaldy , 90 , who died of alzheimer 's disease aug. 10 at her home in gloucester , va. , quietly changed history in 1944 when she refused to give up her seat on a crowded greyhound bus to a white couple .
Head Entity: her
Tail Entity: gloucester
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person city of death":
Relation: person city of death  
Context: john smith, a renowned author, passed away on march 5 in his apartment located in new york city, leaving behind a legacy of literary works that inspired many.  
Head Entity: john smith  
Tail Entity: new york city  

Relation: person city of death  
Context: after a long battle with cancer, elizabeth taylor died on march 23 at a hospital in los angeles, where she had spent her final days surrounded by family and friends.  
Head Entity: elizabeth taylor  
Tail Entity: los angeles  
Mixup data size:  194
MixupTrain:  epoch  0, batch     0 | loss: 2.3049903MixupTrain:  epoch  0, batch     1 | loss: 2.7562342MixupTrain:  epoch  0, batch     2 | loss: 2.6623763MixupTrain:  epoch  0, batch     3 | loss: 2.7591830MixupTrain:  epoch  0, batch     4 | loss: 2.1373212MixupTrain:  epoch  0, batch     5 | loss: 2.5333021MixupTrain:  epoch  0, batch     6 | loss: 2.6443185MixupTrain:  epoch  0, batch     7 | loss: 2.7221195MixupTrain:  epoch  0, batch     8 | loss: 2.5332043MixupTrain:  epoch  0, batch     9 | loss: 2.0076171MixupTrain:  epoch  0, batch    10 | loss: 2.3850475MixupTrain:  epoch  0, batch    11 | loss: 2.5920024MixupTrain:  epoch  0, batch    12 | loss: 3.3235490
MemoryTrain:  epoch  0, batch     0 | loss: 2.4347715MemoryTrain:  epoch  0, batch     1 | loss: 2.2373853MemoryTrain:  epoch  0, batch     2 | loss: 2.2013440MemoryTrain:  epoch  0, batch     3 | loss: 2.8579819MemoryTrain:  epoch  0, batch     4 | loss: 2.3833363MemoryTrain:  epoch  1, batch     0 | loss: 2.1672425MemoryTrain:  epoch  1, batch     1 | loss: 2.7053356MemoryTrain:  epoch  1, batch     2 | loss: 2.1288016MemoryTrain:  epoch  1, batch     3 | loss: 1.8949662MemoryTrain:  epoch  1, batch     4 | loss: 2.0706439MemoryTrain:  epoch  2, batch     0 | loss: 2.3245392MemoryTrain:  epoch  2, batch     1 | loss: 1.8675073MemoryTrain:  epoch  2, batch     2 | loss: 1.6842328MemoryTrain:  epoch  2, batch     3 | loss: 1.8924361MemoryTrain:  epoch  2, batch     4 | loss: 1.7837660MemoryTrain:  epoch  3, batch     0 | loss: 1.7603730MemoryTrain:  epoch  3, batch     1 | loss: 1.7105401MemoryTrain:  epoch  3, batch     2 | loss: 1.6182491MemoryTrain:  epoch  3, batch     3 | loss: 1.6066507MemoryTrain:  epoch  3, batch     4 | loss: 1.6945148MemoryTrain:  epoch  4, batch     0 | loss: 1.8411609MemoryTrain:  epoch  4, batch     1 | loss: 1.6713496MemoryTrain:  epoch  4, batch     2 | loss: 1.5552807MemoryTrain:  epoch  4, batch     3 | loss: 1.7685125MemoryTrain:  epoch  4, batch     4 | loss: 1.6596571MemoryTrain:  epoch  5, batch     0 | loss: 1.7482902MemoryTrain:  epoch  5, batch     1 | loss: 1.8062429MemoryTrain:  epoch  5, batch     2 | loss: 1.6631746MemoryTrain:  epoch  5, batch     3 | loss: 1.5282042MemoryTrain:  epoch  5, batch     4 | loss: 1.5195242MemoryTrain:  epoch  6, batch     0 | loss: 1.5623157MemoryTrain:  epoch  6, batch     1 | loss: 1.5180345MemoryTrain:  epoch  6, batch     2 | loss: 1.6495981MemoryTrain:  epoch  6, batch     3 | loss: 1.7354603MemoryTrain:  epoch  6, batch     4 | loss: 1.4428073MemoryTrain:  epoch  7, batch     0 | loss: 1.6094325MemoryTrain:  epoch  7, batch     1 | loss: 1.3797543MemoryTrain:  epoch  7, batch     2 | loss: 1.4404829MemoryTrain:  epoch  7, batch     3 | loss: 1.4958687MemoryTrain:  epoch  7, batch     4 | loss: 1.4033096MemoryTrain:  epoch  8, batch     0 | loss: 1.3780894MemoryTrain:  epoch  8, batch     1 | loss: 1.3702939MemoryTrain:  epoch  8, batch     2 | loss: 1.4962199MemoryTrain:  epoch  8, batch     3 | loss: 1.4638984MemoryTrain:  epoch  8, batch     4 | loss: 1.4315518MemoryTrain:  epoch  9, batch     0 | loss: 1.4459789MemoryTrain:  epoch  9, batch     1 | loss: 1.4948728MemoryTrain:  epoch  9, batch     2 | loss: 1.3943758MemoryTrain:  epoch  9, batch     3 | loss: 1.3559889MemoryTrain:  epoch  9, batch     4 | loss: 1.3384706
[EVAL] batch:    0 | acc: 43.75%,  total acc: 43.75%   [EVAL] batch:    1 | acc: 56.25%,  total acc: 50.00%   [EVAL] batch:    2 | acc: 43.75%,  total acc: 47.92%   [EVAL] batch:    3 | acc: 43.75%,  total acc: 46.88%   [EVAL] batch:    4 | acc: 31.25%,  total acc: 43.75%   [EVAL] batch:    5 | acc: 43.75%,  total acc: 43.75%   [EVAL] batch:    6 | acc: 43.75%,  total acc: 43.75%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 49.22%   [EVAL] batch:    8 | acc: 37.50%,  total acc: 47.92%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 50.00%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 52.84%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 54.69%   [EVAL] batch:   12 | acc: 50.00%,  total acc: 54.33%   
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 82.81%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 82.50%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 83.93%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 85.94%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 88.12%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 89.20%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 89.06%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 87.50%   [EVAL] batch:   13 | acc: 37.50%,  total acc: 83.93%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 82.92%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 81.25%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 80.88%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 79.86%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 78.95%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 79.38%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 80.36%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 80.97%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 81.79%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 82.29%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 83.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 83.65%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 84.03%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 84.60%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 85.13%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 85.21%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 85.28%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 85.74%   [EVAL] batch:   32 | acc: 68.75%,  total acc: 85.23%   [EVAL] batch:   33 | acc: 37.50%,  total acc: 83.82%   [EVAL] batch:   34 | acc: 37.50%,  total acc: 82.50%   [EVAL] batch:   35 | acc: 43.75%,  total acc: 81.42%   [EVAL] batch:   36 | acc: 62.50%,  total acc: 80.91%   [EVAL] batch:   37 | acc: 62.50%,  total acc: 80.43%   [EVAL] batch:   38 | acc: 50.00%,  total acc: 79.65%   [EVAL] batch:   39 | acc: 93.75%,  total acc: 80.00%   [EVAL] batch:   40 | acc: 37.50%,  total acc: 78.96%   [EVAL] batch:   41 | acc: 43.75%,  total acc: 78.12%   [EVAL] batch:   42 | acc: 50.00%,  total acc: 77.47%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 77.56%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 78.06%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 78.53%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 78.99%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 79.43%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 79.85%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 80.00%   [EVAL] batch:   50 | acc: 56.25%,  total acc: 79.53%   [EVAL] batch:   51 | acc: 18.75%,  total acc: 78.37%   [EVAL] batch:   52 | acc: 12.50%,  total acc: 77.12%   [EVAL] batch:   53 | acc: 6.25%,  total acc: 75.81%   [EVAL] batch:   54 | acc: 12.50%,  total acc: 74.66%   [EVAL] batch:   55 | acc: 0.00%,  total acc: 73.33%   [EVAL] batch:   56 | acc: 68.75%,  total acc: 73.25%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 73.60%   [EVAL] batch:   58 | acc: 81.25%,  total acc: 73.73%   [EVAL] batch:   59 | acc: 87.50%,  total acc: 73.96%   [EVAL] batch:   60 | acc: 62.50%,  total acc: 73.77%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 73.99%   [EVAL] batch:   62 | acc: 87.50%,  total acc: 74.21%   [EVAL] batch:   63 | acc: 87.50%,  total acc: 74.41%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 74.81%   [EVAL] batch:   65 | acc: 100.00%,  total acc: 75.19%   [EVAL] batch:   66 | acc: 100.00%,  total acc: 75.56%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 75.92%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 76.27%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 76.61%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 76.94%   [EVAL] batch:   71 | acc: 93.75%,  total acc: 77.17%   [EVAL] batch:   72 | acc: 12.50%,  total acc: 76.28%   [EVAL] batch:   73 | acc: 31.25%,  total acc: 75.68%   [EVAL] batch:   74 | acc: 50.00%,  total acc: 75.33%   [EVAL] batch:   75 | acc: 50.00%,  total acc: 75.00%   [EVAL] batch:   76 | acc: 50.00%,  total acc: 74.68%   [EVAL] batch:   77 | acc: 37.50%,  total acc: 74.20%   [EVAL] batch:   78 | acc: 56.25%,  total acc: 73.97%   [EVAL] batch:   79 | acc: 43.75%,  total acc: 73.59%   [EVAL] batch:   80 | acc: 50.00%,  total acc: 73.30%   [EVAL] batch:   81 | acc: 31.25%,  total acc: 72.79%   [EVAL] batch:   82 | acc: 37.50%,  total acc: 72.36%   [EVAL] batch:   83 | acc: 43.75%,  total acc: 72.02%   [EVAL] batch:   84 | acc: 87.50%,  total acc: 72.21%   [EVAL] batch:   85 | acc: 43.75%,  total acc: 71.88%   [EVAL] batch:   86 | acc: 68.75%,  total acc: 71.84%   [EVAL] batch:   87 | acc: 75.00%,  total acc: 71.88%   [EVAL] batch:   88 | acc: 75.00%,  total acc: 71.91%   [EVAL] batch:   89 | acc: 56.25%,  total acc: 71.74%   
cur_acc:  ['0.8655', '0.9062', '0.6830', '0.7679', '0.5433']
his_acc:  ['0.8655', '0.8800', '0.8213', '0.7933', '0.7174']
CurrentTrain: epoch  0, batch     0 | loss: 5.7917147CurrentTrain: epoch  0, batch     1 | loss: 6.9763298CurrentTrain: epoch  1, batch     0 | loss: 5.0244436CurrentTrain: epoch  1, batch     1 | loss: 4.5823774CurrentTrain: epoch  2, batch     0 | loss: 4.4184289CurrentTrain: epoch  2, batch     1 | loss: 4.1473384CurrentTrain: epoch  3, batch     0 | loss: 4.3686924CurrentTrain: epoch  3, batch     1 | loss: 3.5465355CurrentTrain: epoch  4, batch     0 | loss: 3.4777658CurrentTrain: epoch  4, batch     1 | loss: 3.6045244CurrentTrain: epoch  5, batch     0 | loss: 3.2456927CurrentTrain: epoch  5, batch     1 | loss: 2.8414056CurrentTrain: epoch  6, batch     0 | loss: 3.1455426CurrentTrain: epoch  6, batch     1 | loss: 2.7041180CurrentTrain: epoch  7, batch     0 | loss: 2.9248176CurrentTrain: epoch  7, batch     1 | loss: 2.8557980CurrentTrain: epoch  8, batch     0 | loss: 2.9332576CurrentTrain: epoch  8, batch     1 | loss: 3.0036931CurrentTrain: epoch  9, batch     0 | loss: 2.7441335CurrentTrain: epoch  9, batch     1 | loss: 2.4349442
#### person stateorprovinces of residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person stateorprovinces of residence" means a person is living in a state or province.
Here is an example:
Relation: person stateorprovinces of residence
Context: the chairman of the senate foreign relations committee , massachusetts democrat john kerry , and the panel 's top republican , richard lugar of indiana , were at the white house meeting , which was led by vice president joe biden , a former chairman of the foreign relations panel .
Head Entity: john kerry
Tail Entity: massachusetts
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person stateorprovinces of residence":
Relation: person stateorprovinces of residence  
Context: After moving from New York to California, actress Emma Stone quickly adapted to the vibrant lifestyle of Los Angeles, where she now resides.  
Head Entity: Emma Stone  
Tail Entity: California  

Relation: person stateorprovinces of residence  
Context: Following his successful career in the tech industry, Mark Zuckerberg decided to settle in Palo Alto, a city known for its innovation and proximity to Silicon Valley.  
Head Entity: Mark Zuckerberg  
Tail Entity: Palo Alto  
#### person date of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person date of death" means a person was died on a date.
Here is an example:
Relation: person date of death
Context: former gov. robert e. mcnair of south carolina , the political moderate who was a finalist to become vice president hubert h. humphrey 's running mate in 1968 but whose promising career was cut short by what became known as the orangeburg massacre , died on nov. 17 in charleston .
Head Entity: robert e. mcnair
Tail Entity: nov. 17
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person date of death":
Relation: person date of death  
Context: renowned physicist stephen hawking, who made groundbreaking contributions to our understanding of black holes and the universe, passed away peacefully at his home in cambridge on march 14, 2018.  
Head Entity: stephen hawking  
Tail Entity: march 14, 2018  

Relation: person date of death  
Context: the beloved author of the harry potter series, j.k. rowling, announced the passing of her dear friend and mentor, who died on january 1, 2020, after a long battle with illness.  
Head Entity: j.k. rowling's dear friend  
Tail Entity: january 1, 2020  
#### organization number of employees members ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization number of employees members" means the number of an organization 's employees is a number.
Here is an example:
Relation: organization number of employees members
Context: covidien , which posted revenue of more than $ 10 billion last year , has about 42,000 employees worldwide .
Head Entity: covidien
Tail Entity: 42,000
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization number of employees members":
Relation: organization number of employees members  
Context: TechCorp, a leading software development company, boasts a workforce of approximately 5,500 employees across its global offices.  
Head Entity: TechCorp  
Tail Entity: 5,500  

Relation: organization number of employees members  
Context: GreenEarth Nonprofit has grown significantly over the past few years and currently employs around 1,200 dedicated staff members to support its environmental initiatives.  
Head Entity: GreenEarth Nonprofit  
Tail Entity: 1,200  
#### person alternate names ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person alternate names" means a person 's alias is a person.
Here is an example:
Relation: person alternate names
Context: a judge in new york city said remy ma , whose real name is remy smith , said thursday that the hip-hopper could not leave the united states for a five-country european concert tour .
Head Entity: remy smith
Tail Entity: remy ma
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person alternate names":
Relation: person alternate names  
Context: The famous author Samuel Langhorne Clemens, better known by his pen name Mark Twain, is celebrated for his novels like "The Adventures of Tom Sawyer."  
Head Entity: Samuel Langhorne Clemens  
Tail Entity: Mark Twain  

Relation: person alternate names  
Context: The musician Stefani Joanne Angelina Germanotta, who is widely recognized as Lady Gaga, has made a significant impact on the pop music scene.  
Head Entity: Stefani Joanne Angelina Germanotta  
Tail Entity: Lady Gaga  
#### person spouse ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person spouse" means a person 's spouse is a person.
Here is an example:
Relation: person spouse
Context: smits stands at the center of this multigenerational saga as alex vega , the adopted son of rum and sugar baron pancho duque -lrb- elizondo -rrb- and his wife , amalia -lrb- moreno -rrb- .
Head Entity: elizondo
Tail Entity: moreno
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person spouse":
Relation: person spouse  
Context: in a heartfelt ceremony, john and his beloved partner, sarah, exchanged vows surrounded by family and friends, celebrating their love and commitment to each other.  
Head Entity: john  
Tail Entity: sarah  

Relation: person spouse  
Context: after years of friendship, emily finally realized that her best friend, michael, was the one she wanted to spend her life with, leading to a beautiful wedding.  
Head Entity: emily  
Tail Entity: michael  
Mixup data size:  224
MixupTrain:  epoch  0, batch     0 | loss: 2.2797810MixupTrain:  epoch  0, batch     1 | loss: 2.2371428MixupTrain:  epoch  0, batch     2 | loss: 2.1275158MixupTrain:  epoch  0, batch     3 | loss: 2.6912607MixupTrain:  epoch  0, batch     4 | loss: 2.6353655MixupTrain:  epoch  0, batch     5 | loss: 2.1257161MixupTrain:  epoch  0, batch     6 | loss: 2.4696491MixupTrain:  epoch  0, batch     7 | loss: 2.2911378MixupTrain:  epoch  0, batch     8 | loss: 2.5229566MixupTrain:  epoch  0, batch     9 | loss: 2.7632907MixupTrain:  epoch  0, batch    10 | loss: 2.0014295MixupTrain:  epoch  0, batch    11 | loss: 2.5966710MixupTrain:  epoch  0, batch    12 | loss: 2.9123210MixupTrain:  epoch  0, batch    13 | loss: 2.4406844
MemoryTrain:  epoch  0, batch     0 | loss: 2.4902656MemoryTrain:  epoch  0, batch     1 | loss: 2.4088941MemoryTrain:  epoch  0, batch     2 | loss: 1.7989755MemoryTrain:  epoch  0, batch     3 | loss: 2.8485503MemoryTrain:  epoch  0, batch     4 | loss: 2.3829606MemoryTrain:  epoch  0, batch     5 | loss: 2.2767322MemoryTrain:  epoch  1, batch     0 | loss: 2.2974401MemoryTrain:  epoch  1, batch     1 | loss: 1.6764104MemoryTrain:  epoch  1, batch     2 | loss: 1.7139263MemoryTrain:  epoch  1, batch     3 | loss: 1.9518645MemoryTrain:  epoch  1, batch     4 | loss: 2.4322436MemoryTrain:  epoch  1, batch     5 | loss: 2.2229970MemoryTrain:  epoch  2, batch     0 | loss: 1.8818644MemoryTrain:  epoch  2, batch     1 | loss: 1.6512332MemoryTrain:  epoch  2, batch     2 | loss: 1.6131971MemoryTrain:  epoch  2, batch     3 | loss: 1.6460874MemoryTrain:  epoch  2, batch     4 | loss: 1.7421148MemoryTrain:  epoch  2, batch     5 | loss: 2.3817940MemoryTrain:  epoch  3, batch     0 | loss: 1.7925398MemoryTrain:  epoch  3, batch     1 | loss: 1.6747772MemoryTrain:  epoch  3, batch     2 | loss: 1.9243821MemoryTrain:  epoch  3, batch     3 | loss: 1.9137708MemoryTrain:  epoch  3, batch     4 | loss: 1.5837630MemoryTrain:  epoch  3, batch     5 | loss: 1.4793261MemoryTrain:  epoch  4, batch     0 | loss: 1.7279541MemoryTrain:  epoch  4, batch     1 | loss: 1.4528258MemoryTrain:  epoch  4, batch     2 | loss: 1.3069696MemoryTrain:  epoch  4, batch     3 | loss: 2.0086668MemoryTrain:  epoch  4, batch     4 | loss: 1.6958684MemoryTrain:  epoch  4, batch     5 | loss: 1.5895772MemoryTrain:  epoch  5, batch     0 | loss: 1.5435872MemoryTrain:  epoch  5, batch     1 | loss: 1.4136732MemoryTrain:  epoch  5, batch     2 | loss: 1.9127488MemoryTrain:  epoch  5, batch     3 | loss: 1.5610211MemoryTrain:  epoch  5, batch     4 | loss: 1.3344333MemoryTrain:  epoch  5, batch     5 | loss: 1.5994306MemoryTrain:  epoch  6, batch     0 | loss: 1.4572544MemoryTrain:  epoch  6, batch     1 | loss: 1.6196089MemoryTrain:  epoch  6, batch     2 | loss: 1.5007637MemoryTrain:  epoch  6, batch     3 | loss: 1.6266689MemoryTrain:  epoch  6, batch     4 | loss: 1.3885441MemoryTrain:  epoch  6, batch     5 | loss: 1.3007824MemoryTrain:  epoch  7, batch     0 | loss: 1.4716587MemoryTrain:  epoch  7, batch     1 | loss: 1.5185816MemoryTrain:  epoch  7, batch     2 | loss: 1.4621048MemoryTrain:  epoch  7, batch     3 | loss: 1.3394980MemoryTrain:  epoch  7, batch     4 | loss: 1.4738797MemoryTrain:  epoch  7, batch     5 | loss: 1.4188160MemoryTrain:  epoch  8, batch     0 | loss: 1.5676706MemoryTrain:  epoch  8, batch     1 | loss: 1.3062228MemoryTrain:  epoch  8, batch     2 | loss: 1.5067755MemoryTrain:  epoch  8, batch     3 | loss: 1.3604167MemoryTrain:  epoch  8, batch     4 | loss: 1.3613482MemoryTrain:  epoch  8, batch     5 | loss: 1.6120914MemoryTrain:  epoch  9, batch     0 | loss: 1.5079963MemoryTrain:  epoch  9, batch     1 | loss: 1.4497396MemoryTrain:  epoch  9, batch     2 | loss: 1.2759151MemoryTrain:  epoch  9, batch     3 | loss: 1.4706700MemoryTrain:  epoch  9, batch     4 | loss: 1.3868313MemoryTrain:  epoch  9, batch     5 | loss: 1.5268074
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 50.00%,  total acc: 62.50%   [EVAL] batch:    2 | acc: 68.75%,  total acc: 64.58%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 67.19%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 68.75%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 72.92%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 75.89%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 78.12%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 79.17%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 78.75%   [EVAL] batch:   10 | acc: 56.25%,  total acc: 76.70%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 76.56%   [EVAL] batch:   12 | acc: 43.75%,  total acc: 74.04%   [EVAL] batch:   13 | acc: 50.00%,  total acc: 72.32%   [EVAL] batch:   14 | acc: 12.50%,  total acc: 68.33%   
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 90.62%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 82.81%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 83.75%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 82.29%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 84.82%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 86.72%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 88.19%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 88.75%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 89.77%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 89.58%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 87.98%   [EVAL] batch:   13 | acc: 37.50%,  total acc: 84.38%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 83.33%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 81.64%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 80.21%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 79.28%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 79.69%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 80.65%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 82.07%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 82.55%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 83.25%   [EVAL] batch:   25 | acc: 93.75%,  total acc: 83.65%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 84.03%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 84.60%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 85.13%   [EVAL] batch:   29 | acc: 75.00%,  total acc: 84.79%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 84.88%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 85.35%   [EVAL] batch:   32 | acc: 43.75%,  total acc: 84.09%   [EVAL] batch:   33 | acc: 25.00%,  total acc: 82.35%   [EVAL] batch:   34 | acc: 18.75%,  total acc: 80.54%   [EVAL] batch:   35 | acc: 18.75%,  total acc: 78.82%   [EVAL] batch:   36 | acc: 25.00%,  total acc: 77.36%   [EVAL] batch:   37 | acc: 31.25%,  total acc: 76.15%   [EVAL] batch:   38 | acc: 37.50%,  total acc: 75.16%   [EVAL] batch:   39 | acc: 93.75%,  total acc: 75.62%   [EVAL] batch:   40 | acc: 37.50%,  total acc: 74.70%   [EVAL] batch:   41 | acc: 43.75%,  total acc: 73.96%   [EVAL] batch:   42 | acc: 50.00%,  total acc: 73.40%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 73.58%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 74.17%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 74.73%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 75.27%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 75.78%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 76.28%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 76.50%   [EVAL] batch:   50 | acc: 62.50%,  total acc: 76.23%   [EVAL] batch:   51 | acc: 18.75%,  total acc: 75.12%   [EVAL] batch:   52 | acc: 18.75%,  total acc: 74.06%   [EVAL] batch:   53 | acc: 12.50%,  total acc: 72.92%   [EVAL] batch:   54 | acc: 12.50%,  total acc: 71.82%   [EVAL] batch:   55 | acc: 6.25%,  total acc: 70.65%   [EVAL] batch:   56 | acc: 62.50%,  total acc: 70.50%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 70.91%   [EVAL] batch:   58 | acc: 81.25%,  total acc: 71.08%   [EVAL] batch:   59 | acc: 81.25%,  total acc: 71.25%   [EVAL] batch:   60 | acc: 62.50%,  total acc: 71.11%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 71.37%   [EVAL] batch:   62 | acc: 75.00%,  total acc: 71.43%   [EVAL] batch:   63 | acc: 87.50%,  total acc: 71.68%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 72.12%   [EVAL] batch:   65 | acc: 100.00%,  total acc: 72.54%   [EVAL] batch:   66 | acc: 100.00%,  total acc: 72.95%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 73.35%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 73.73%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 74.11%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 74.47%   [EVAL] batch:   71 | acc: 93.75%,  total acc: 74.74%   [EVAL] batch:   72 | acc: 18.75%,  total acc: 73.97%   [EVAL] batch:   73 | acc: 37.50%,  total acc: 73.48%   [EVAL] batch:   74 | acc: 62.50%,  total acc: 73.33%   [EVAL] batch:   75 | acc: 50.00%,  total acc: 73.03%   [EVAL] batch:   76 | acc: 68.75%,  total acc: 72.97%   [EVAL] batch:   77 | acc: 25.00%,  total acc: 72.36%   [EVAL] batch:   78 | acc: 56.25%,  total acc: 72.15%   [EVAL] batch:   79 | acc: 43.75%,  total acc: 71.80%   [EVAL] batch:   80 | acc: 43.75%,  total acc: 71.45%   [EVAL] batch:   81 | acc: 68.75%,  total acc: 71.42%   [EVAL] batch:   82 | acc: 56.25%,  total acc: 71.23%   [EVAL] batch:   83 | acc: 68.75%,  total acc: 71.21%   [EVAL] batch:   84 | acc: 62.50%,  total acc: 71.10%   [EVAL] batch:   85 | acc: 25.00%,  total acc: 70.57%   [EVAL] batch:   86 | acc: 37.50%,  total acc: 70.19%   [EVAL] batch:   87 | acc: 50.00%,  total acc: 69.96%   [EVAL] batch:   88 | acc: 62.50%,  total acc: 69.87%   [EVAL] batch:   89 | acc: 75.00%,  total acc: 69.93%   [EVAL] batch:   90 | acc: 56.25%,  total acc: 69.78%   [EVAL] batch:   91 | acc: 68.75%,  total acc: 69.77%   [EVAL] batch:   92 | acc: 75.00%,  total acc: 69.83%   [EVAL] batch:   93 | acc: 56.25%,  total acc: 69.68%   [EVAL] batch:   94 | acc: 93.75%,  total acc: 69.93%   [EVAL] batch:   95 | acc: 87.50%,  total acc: 70.12%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 70.43%   [EVAL] batch:   97 | acc: 87.50%,  total acc: 70.60%   [EVAL] batch:   98 | acc: 93.75%,  total acc: 70.83%   [EVAL] batch:   99 | acc: 62.50%,  total acc: 70.75%   [EVAL] batch:  100 | acc: 62.50%,  total acc: 70.67%   [EVAL] batch:  101 | acc: 56.25%,  total acc: 70.53%   [EVAL] batch:  102 | acc: 43.75%,  total acc: 70.27%   [EVAL] batch:  103 | acc: 50.00%,  total acc: 70.07%   [EVAL] batch:  104 | acc: 0.00%,  total acc: 69.40%   
cur_acc:  ['0.8655', '0.9062', '0.6830', '0.7679', '0.5433', '0.6833']
his_acc:  ['0.8655', '0.8800', '0.8213', '0.7933', '0.7174', '0.6940']
CurrentTrain: epoch  0, batch     0 | loss: 5.5913410CurrentTrain: epoch  0, batch     1 | loss: 5.2742224CurrentTrain: epoch  1, batch     0 | loss: 4.4562340CurrentTrain: epoch  1, batch     1 | loss: 4.7932749CurrentTrain: epoch  2, batch     0 | loss: 3.9794765CurrentTrain: epoch  2, batch     1 | loss: 4.5000296CurrentTrain: epoch  3, batch     0 | loss: 3.4903429CurrentTrain: epoch  3, batch     1 | loss: 4.0373173CurrentTrain: epoch  4, batch     0 | loss: 3.8416398CurrentTrain: epoch  4, batch     1 | loss: 2.7858396CurrentTrain: epoch  5, batch     0 | loss: 3.2426593CurrentTrain: epoch  5, batch     1 | loss: 3.3166878CurrentTrain: epoch  6, batch     0 | loss: 3.3999674CurrentTrain: epoch  6, batch     1 | loss: 2.6392043CurrentTrain: epoch  7, batch     0 | loss: 3.2533395CurrentTrain: epoch  7, batch     1 | loss: 2.4634769CurrentTrain: epoch  8, batch     0 | loss: 2.9935946CurrentTrain: epoch  8, batch     1 | loss: 2.7382703CurrentTrain: epoch  9, batch     0 | loss: 2.9756665CurrentTrain: epoch  9, batch     1 | loss: 2.3628216
#### person country of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person country of birth" means a person was born in a country.
Here is an example:
Relation: person country of birth
Context: de maiziere noted that germany took in another former inmate from guantanamo in 2006 -- murat kurnaz , a turkish national who was born and grew up in germany .
Head Entity: murat kurnaz
Tail Entity: germany
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person country of birth":
Relation: person country of birth  
Context: born in 1985 in the bustling city of new delhi, arjun kapoor has always been proud of his indian heritage and culture.  
Head Entity: arjun kapoor  
Tail Entity: india  

Relation: person country of birth  
Context: during the interview, she revealed that despite living in the united states for over a decade, her roots trace back to the vibrant streets of barcelona, spain.  
Head Entity: she  
Tail Entity: spain  
#### organization website ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization website" means an organization 's website is an url.
Here is an example:
Relation: organization website
Context: barnes foundation : http://www.barnesfoundation.org
Head Entity: barnes foundation
Tail Entity: http://www.barnesfoundation.org
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization website":
Relation: organization website  
Context: national geographic society : https://www.nationalgeographic.com  
Head Entity: national geographic society  
Tail Entity: https://www.nationalgeographic.com  

Relation: organization website  
Context: world wildlife fund : https://www.worldwildlife.org  
Head Entity: world wildlife fund  
Tail Entity: https://www.worldwildlife.org  
#### organization shareholders ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization shareholders" means an organization was invested by a person.
Here is an example:
Relation: organization shareholders
Context: ------ liberty media acquired a 41 percent stake in directv in late february by exchanging it for a 16 percent stake in news corp plus $ 625 million -lrb- euro402 5 million -rrb- in cash .
Head Entity: directv
Tail Entity: liberty media
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization shareholders":
Relation: organization shareholders  
Context: In 2021, Amazon announced that it had acquired a significant share in the electric vehicle startup Rivian, investing over $1 billion to support its growth.  
Head Entity: Rivian  
Tail Entity: Amazon  

Relation: organization shareholders  
Context: Tesla's recent investment in the solar energy company Sunrun has raised eyebrows, as it marks a strategic move to expand its renewable energy portfolio.  
Head Entity: Sunrun  
Tail Entity: Tesla  
#### organization dissolved ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization dissolved" means an organization was dissolved in a date.
Here is an example:
Relation: organization dissolved
Context: the tse suffered its worst-ever system crash in november 2005 which paralyzed the world 's second largest bourse and forced it to shelve plan for a listing of its own .
Head Entity: tse
Tail Entity: november 2005
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization dissolved":
Relation: organization dissolved  
Context: the once-prominent tech startup, Innovatech, officially ceased operations in early 2020 after struggling to secure funding.  
Head Entity: Innovatech  
Tail Entity: early 2020  

Relation: organization dissolved  
Context: after years of financial difficulties, the local arts council announced its closure in the summer of 2019, leaving many artists without support.  
Head Entity: local arts council  
Tail Entity: summer of 2019  
#### organization founded by ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization founded by" means an organization was found by a person.
Here is an example:
Relation: organization founded by
Context: `` i have no greater obligation than to ensure the safely of airline travelers in this country , '' transportation secretary ray lahood said in a joint statement with j. randolph babbitt , administrator of the federal aviation administration , that was issued on the eve of a senate hearing on aviation safety .
Head Entity: federal aviation administration
Tail Entity: j. randolph babbitt
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization founded by":
Relation: organization founded by  
Context: In 1975, the renowned physicist and entrepreneur, Dr. Jane Smith, established Quantum Innovations, a company dedicated to advancing quantum computing technologies.  
Head Entity: Quantum Innovations  
Tail Entity: Dr. Jane Smith  

Relation: organization founded by  
Context: The charity organization, Hope for Tomorrow, was created in 2010 by the well-known philanthropist, John Doe, to support underprivileged children around the world.  
Head Entity: Hope for Tomorrow  
Tail Entity: John Doe  
Mixup data size:  255
MixupTrain:  epoch  0, batch     0 | loss: 2.6515112MixupTrain:  epoch  0, batch     1 | loss: 2.0204231MixupTrain:  epoch  0, batch     2 | loss: 2.2169468MixupTrain:  epoch  0, batch     3 | loss: 2.7456737MixupTrain:  epoch  0, batch     4 | loss: 2.0359621MixupTrain:  epoch  0, batch     5 | loss: 2.1312868MixupTrain:  epoch  0, batch     6 | loss: 2.8590938MixupTrain:  epoch  0, batch     7 | loss: 2.4398801MixupTrain:  epoch  0, batch     8 | loss: 2.4182005MixupTrain:  epoch  0, batch     9 | loss: 2.8668161MixupTrain:  epoch  0, batch    10 | loss: 2.7025853MixupTrain:  epoch  0, batch    11 | loss: 2.6408792MixupTrain:  epoch  0, batch    12 | loss: 1.7829136MixupTrain:  epoch  0, batch    13 | loss: 2.6846206MixupTrain:  epoch  0, batch    14 | loss: 2.5003803MixupTrain:  epoch  0, batch    15 | loss: 2.1063691
MemoryTrain:  epoch  0, batch     0 | loss: 3.2689946MemoryTrain:  epoch  0, batch     1 | loss: 1.8652048MemoryTrain:  epoch  0, batch     2 | loss: 2.0501719MemoryTrain:  epoch  0, batch     3 | loss: 2.0884733MemoryTrain:  epoch  0, batch     4 | loss: 1.6366973MemoryTrain:  epoch  0, batch     5 | loss: 1.8395743MemoryTrain:  epoch  0, batch     6 | loss: 2.9282303MemoryTrain:  epoch  1, batch     0 | loss: 2.4711094MemoryTrain:  epoch  1, batch     1 | loss: 2.1383317MemoryTrain:  epoch  1, batch     2 | loss: 1.8800708MemoryTrain:  epoch  1, batch     3 | loss: 2.2106838MemoryTrain:  epoch  1, batch     4 | loss: 1.3415136MemoryTrain:  epoch  1, batch     5 | loss: 1.9208859MemoryTrain:  epoch  1, batch     6 | loss: 1.9640799MemoryTrain:  epoch  2, batch     0 | loss: 1.6568611MemoryTrain:  epoch  2, batch     1 | loss: 1.6172501MemoryTrain:  epoch  2, batch     2 | loss: 1.8503566MemoryTrain:  epoch  2, batch     3 | loss: 1.5105543MemoryTrain:  epoch  2, batch     4 | loss: 1.4534614MemoryTrain:  epoch  2, batch     5 | loss: 2.1111517MemoryTrain:  epoch  2, batch     6 | loss: 2.0164495MemoryTrain:  epoch  3, batch     0 | loss: 1.6720202MemoryTrain:  epoch  3, batch     1 | loss: 1.5766317MemoryTrain:  epoch  3, batch     2 | loss: 1.6353070MemoryTrain:  epoch  3, batch     3 | loss: 1.5649300MemoryTrain:  epoch  3, batch     4 | loss: 1.4048102MemoryTrain:  epoch  3, batch     5 | loss: 1.6504338MemoryTrain:  epoch  3, batch     6 | loss: 1.5287002MemoryTrain:  epoch  4, batch     0 | loss: 1.7358946MemoryTrain:  epoch  4, batch     1 | loss: 1.7440858MemoryTrain:  epoch  4, batch     2 | loss: 1.2852218MemoryTrain:  epoch  4, batch     3 | loss: 1.4863853MemoryTrain:  epoch  4, batch     4 | loss: 1.4904115MemoryTrain:  epoch  4, batch     5 | loss: 1.3659861MemoryTrain:  epoch  4, batch     6 | loss: 1.5505612MemoryTrain:  epoch  5, batch     0 | loss: 1.2758056MemoryTrain:  epoch  5, batch     1 | loss: 1.7429649MemoryTrain:  epoch  5, batch     2 | loss: 1.4685692MemoryTrain:  epoch  5, batch     3 | loss: 1.5570575MemoryTrain:  epoch  5, batch     4 | loss: 1.3962562MemoryTrain:  epoch  5, batch     5 | loss: 1.5350997MemoryTrain:  epoch  5, batch     6 | loss: 1.4001451MemoryTrain:  epoch  6, batch     0 | loss: 1.4136050MemoryTrain:  epoch  6, batch     1 | loss: 1.3926874MemoryTrain:  epoch  6, batch     2 | loss: 1.5113797MemoryTrain:  epoch  6, batch     3 | loss: 1.3050840MemoryTrain:  epoch  6, batch     4 | loss: 1.4701800MemoryTrain:  epoch  6, batch     5 | loss: 1.3970296MemoryTrain:  epoch  6, batch     6 | loss: 1.3651108MemoryTrain:  epoch  7, batch     0 | loss: 1.4007856MemoryTrain:  epoch  7, batch     1 | loss: 1.3523823MemoryTrain:  epoch  7, batch     2 | loss: 1.4088649MemoryTrain:  epoch  7, batch     3 | loss: 1.4348247MemoryTrain:  epoch  7, batch     4 | loss: 1.3605340MemoryTrain:  epoch  7, batch     5 | loss: 1.4166938MemoryTrain:  epoch  7, batch     6 | loss: 1.6143650MemoryTrain:  epoch  8, batch     0 | loss: 1.2969551MemoryTrain:  epoch  8, batch     1 | loss: 1.3590746MemoryTrain:  epoch  8, batch     2 | loss: 1.3244433MemoryTrain:  epoch  8, batch     3 | loss: 1.3238983MemoryTrain:  epoch  8, batch     4 | loss: 1.3236434MemoryTrain:  epoch  8, batch     5 | loss: 1.3062391MemoryTrain:  epoch  8, batch     6 | loss: 1.2993188MemoryTrain:  epoch  9, batch     0 | loss: 1.2908362MemoryTrain:  epoch  9, batch     1 | loss: 1.4419777MemoryTrain:  epoch  9, batch     2 | loss: 1.3230151MemoryTrain:  epoch  9, batch     3 | loss: 1.2868966MemoryTrain:  epoch  9, batch     4 | loss: 1.3619909MemoryTrain:  epoch  9, batch     5 | loss: 1.2195089MemoryTrain:  epoch  9, batch     6 | loss: 1.2759320
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 91.67%   [EVAL] batch:    3 | acc: 50.00%,  total acc: 81.25%   [EVAL] batch:    4 | acc: 37.50%,  total acc: 72.50%   [EVAL] batch:    5 | acc: 50.00%,  total acc: 68.75%   [EVAL] batch:    6 | acc: 43.75%,  total acc: 65.18%   [EVAL] batch:    7 | acc: 6.25%,  total acc: 57.81%   
[EVAL] batch:    0 | acc: 43.75%,  total acc: 43.75%   [EVAL] batch:    1 | acc: 43.75%,  total acc: 43.75%   [EVAL] batch:    2 | acc: 50.00%,  total acc: 45.83%   [EVAL] batch:    3 | acc: 31.25%,  total acc: 42.19%   [EVAL] batch:    4 | acc: 43.75%,  total acc: 42.50%   [EVAL] batch:    5 | acc: 50.00%,  total acc: 43.75%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 47.32%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 51.56%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 54.86%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 57.50%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 59.66%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 61.98%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 62.50%   [EVAL] batch:   13 | acc: 37.50%,  total acc: 60.71%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 61.25%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 60.94%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 61.76%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 61.81%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 61.84%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 63.12%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 64.88%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 66.19%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 67.39%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 68.49%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 69.75%   [EVAL] batch:   25 | acc: 93.75%,  total acc: 70.67%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 71.53%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 72.54%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 73.49%   [EVAL] batch:   29 | acc: 75.00%,  total acc: 73.54%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 74.19%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:   32 | acc: 43.75%,  total acc: 74.05%   [EVAL] batch:   33 | acc: 12.50%,  total acc: 72.24%   [EVAL] batch:   34 | acc: 31.25%,  total acc: 71.07%   [EVAL] batch:   35 | acc: 18.75%,  total acc: 69.62%   [EVAL] batch:   36 | acc: 31.25%,  total acc: 68.58%   [EVAL] batch:   37 | acc: 37.50%,  total acc: 67.76%   [EVAL] batch:   38 | acc: 37.50%,  total acc: 66.99%   [EVAL] batch:   39 | acc: 93.75%,  total acc: 67.66%   [EVAL] batch:   40 | acc: 43.75%,  total acc: 67.07%   [EVAL] batch:   41 | acc: 81.25%,  total acc: 67.41%   [EVAL] batch:   42 | acc: 68.75%,  total acc: 67.44%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 67.76%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 68.47%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 69.16%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 69.81%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 70.44%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 71.05%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 71.38%   [EVAL] batch:   50 | acc: 56.25%,  total acc: 71.08%   [EVAL] batch:   51 | acc: 18.75%,  total acc: 70.07%   [EVAL] batch:   52 | acc: 6.25%,  total acc: 68.87%   [EVAL] batch:   53 | acc: 0.00%,  total acc: 67.59%   [EVAL] batch:   54 | acc: 12.50%,  total acc: 66.59%   [EVAL] batch:   55 | acc: 0.00%,  total acc: 65.40%   [EVAL] batch:   56 | acc: 62.50%,  total acc: 65.35%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 65.73%   [EVAL] batch:   58 | acc: 81.25%,  total acc: 66.00%   [EVAL] batch:   59 | acc: 87.50%,  total acc: 66.35%   [EVAL] batch:   60 | acc: 62.50%,  total acc: 66.29%   [EVAL] batch:   61 | acc: 81.25%,  total acc: 66.53%   [EVAL] batch:   62 | acc: 75.00%,  total acc: 66.67%   [EVAL] batch:   63 | acc: 87.50%,  total acc: 66.99%   [EVAL] batch:   64 | acc: 81.25%,  total acc: 67.21%   [EVAL] batch:   65 | acc: 93.75%,  total acc: 67.61%   [EVAL] batch:   66 | acc: 100.00%,  total acc: 68.10%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 68.57%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 69.02%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 69.46%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 69.89%   [EVAL] batch:   71 | acc: 93.75%,  total acc: 70.23%   [EVAL] batch:   72 | acc: 0.00%,  total acc: 69.26%   [EVAL] batch:   73 | acc: 12.50%,  total acc: 68.50%   [EVAL] batch:   74 | acc: 56.25%,  total acc: 68.33%   [EVAL] batch:   75 | acc: 50.00%,  total acc: 68.09%   [EVAL] batch:   76 | acc: 62.50%,  total acc: 68.02%   [EVAL] batch:   77 | acc: 25.00%,  total acc: 67.47%   [EVAL] batch:   78 | acc: 56.25%,  total acc: 67.33%   [EVAL] batch:   79 | acc: 43.75%,  total acc: 67.03%   [EVAL] batch:   80 | acc: 50.00%,  total acc: 66.82%   [EVAL] batch:   81 | acc: 37.50%,  total acc: 66.46%   [EVAL] batch:   82 | acc: 50.00%,  total acc: 66.27%   [EVAL] batch:   83 | acc: 50.00%,  total acc: 66.07%   [EVAL] batch:   84 | acc: 62.50%,  total acc: 66.03%   [EVAL] batch:   85 | acc: 25.00%,  total acc: 65.55%   [EVAL] batch:   86 | acc: 43.75%,  total acc: 65.30%   [EVAL] batch:   87 | acc: 62.50%,  total acc: 65.27%   [EVAL] batch:   88 | acc: 75.00%,  total acc: 65.38%   [EVAL] batch:   89 | acc: 81.25%,  total acc: 65.56%   [EVAL] batch:   90 | acc: 62.50%,  total acc: 65.52%   [EVAL] batch:   91 | acc: 68.75%,  total acc: 65.56%   [EVAL] batch:   92 | acc: 75.00%,  total acc: 65.66%   [EVAL] batch:   93 | acc: 62.50%,  total acc: 65.62%   [EVAL] batch:   94 | acc: 87.50%,  total acc: 65.86%   [EVAL] batch:   95 | acc: 75.00%,  total acc: 65.95%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 66.30%   [EVAL] batch:   97 | acc: 87.50%,  total acc: 66.52%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 66.86%   [EVAL] batch:   99 | acc: 62.50%,  total acc: 66.81%   [EVAL] batch:  100 | acc: 62.50%,  total acc: 66.77%   [EVAL] batch:  101 | acc: 75.00%,  total acc: 66.85%   [EVAL] batch:  102 | acc: 62.50%,  total acc: 66.81%   [EVAL] batch:  103 | acc: 56.25%,  total acc: 66.71%   [EVAL] batch:  104 | acc: 87.50%,  total acc: 66.90%   [EVAL] batch:  105 | acc: 100.00%,  total acc: 67.22%   [EVAL] batch:  106 | acc: 87.50%,  total acc: 67.41%   [EVAL] batch:  107 | acc: 50.00%,  total acc: 67.25%   [EVAL] batch:  108 | acc: 37.50%,  total acc: 66.97%   [EVAL] batch:  109 | acc: 56.25%,  total acc: 66.88%   [EVAL] batch:  110 | acc: 43.75%,  total acc: 66.67%   [EVAL] batch:  111 | acc: 6.25%,  total acc: 66.13%   
cur_acc:  ['0.8655', '0.9062', '0.6830', '0.7679', '0.5433', '0.6833', '0.5781']
his_acc:  ['0.8655', '0.8800', '0.8213', '0.7933', '0.7174', '0.6940', '0.6613']
CurrentTrain: epoch  0, batch     0 | loss: 7.8189507CurrentTrain: epoch  0, batch     1 | loss: 7.9165940CurrentTrain: epoch  1, batch     0 | loss: 7.7321486CurrentTrain: epoch  1, batch     1 | loss: 6.2573395CurrentTrain: epoch  2, batch     0 | loss: 6.1996078CurrentTrain: epoch  2, batch     1 | loss: 6.8113456CurrentTrain: epoch  3, batch     0 | loss: 5.7619944CurrentTrain: epoch  3, batch     1 | loss: 6.3445458CurrentTrain: epoch  4, batch     0 | loss: 5.3906837CurrentTrain: epoch  4, batch     1 | loss: 6.4713483CurrentTrain: epoch  5, batch     0 | loss: 5.4309874CurrentTrain: epoch  5, batch     1 | loss: 5.5886769CurrentTrain: epoch  6, batch     0 | loss: 4.9322524CurrentTrain: epoch  6, batch     1 | loss: 5.0310812CurrentTrain: epoch  7, batch     0 | loss: 4.7525549CurrentTrain: epoch  7, batch     1 | loss: 4.4370880CurrentTrain: epoch  8, batch     0 | loss: 4.7699485CurrentTrain: epoch  8, batch     1 | loss: 3.6705806CurrentTrain: epoch  9, batch     0 | loss: 3.5645161CurrentTrain: epoch  9, batch     1 | loss: 4.6477470
#### organization subsidiaries ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization subsidiaries" means an organization 's subsidiary is an organization.
Here is an example:
Relation: organization subsidiaries
Context: firstgroup , britain 's largest bus operator , entered the north american market in 1999 when it acquired ryder public transportation services inc. .
Head Entity: firstgroup
Tail Entity: ryder public transportation services inc.
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization subsidiaries":
Relation: organization subsidiaries  
Context: Alphabet Inc. has several subsidiaries, including YouTube, which it acquired in 2006 for $1.65 billion.  
Head Entity: Alphabet Inc.  
Tail Entity: YouTube  

Relation: organization subsidiaries  
Context: The Coca-Cola Company owns numerous subsidiaries, one of which is Minute Maid, known for its fruit juices.  
Head Entity: The Coca-Cola Company  
Tail Entity: Minute Maid  
#### organization parents ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization parents" means an organization 's parent is an organization.
Here is an example:
Relation: organization parents
Context: aseel kami , tim cocks , abdul-rahman taher , tim cocks and michael christie -lrb- reuters -rrb- add an intriguing note in the speculation over who 's responsible for the recent bombings : `` but a senior iraqi intelligence source , who declined to be named , said there was evidence the bombs could be the work of the militant badr organisation , the armed wing of the supreme islamic iraqi council -lrb- isci -rrb- .
Head Entity: badr organisation
Tail Entity: supreme islamic iraqi council
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization parents":
Relation: organization parents  
Context: In a recent merger, the tech giant Alphabet Inc. announced that it would acquire the innovative startup DeepMind Technologies, which has been a subsidiary of Alphabet since 2015. This acquisition is expected to enhance Alphabet's capabilities in artificial intelligence.  
Head Entity: DeepMind Technologies  
Tail Entity: Alphabet Inc.  

Relation: organization parents  
Context: The historical archives reveal that the renowned publishing house Penguin Random House was formed through the merger of two major companies, Penguin Group and Random House, which were both influential in the literary world.  
Head Entity: Penguin Random House  
Tail Entity: Penguin Group  
#### organization alternate names ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization alternate names" means a person 's alias is a person.
Here is an example:
Relation: organization alternate names
Context: it also needs the green light from the 45-nation nuclear suppliers group -lrb- nsg -rrb- , which regulates global civilian nuclear trade , before it can begin buying nuclear reactors and fuel .
Head Entity: nsg
Tail Entity: nuclear suppliers group
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization alternate names":
Relation: organization alternate names  
Context: The International Monetary Fund, often referred to as the IMF, plays a crucial role in global economic stability.  
Head Entity: IMF  
Tail Entity: International Monetary Fund  

Relation: organization alternate names  
Context: The World Health Organization, commonly known as WHO, has been at the forefront of the global response to health crises.  
Head Entity: WHO  
Tail Entity: World Health Organization  
#### organization city of headquarters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization city of headquarters" means an organization is located in a city.
Here is an example:
Relation: organization city of headquarters
Context: ------ london 2008-05-20 07:23:45 utc enodis plc endorses sweetened takeover bid by us company manitowoc illinois tool works of glenville , illinois , which had offered 282 pence -lrb- us$ 551 euro3 54 -rrb- per share , said monday that it was considering its position .
Head Entity: illinois tool works
Tail Entity: glenville
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization city of headquarters":
Relation: organization city of headquarters  
Context: ------ san francisco 2021-03-15 10:00:00 utc tech giant google has announced plans to expand its headquarters in the heart of san francisco, aiming to create more job opportunities and enhance its presence in the city.  
Head Entity: google  
Tail Entity: san francisco  

Relation: organization city of headquarters  
Context: ------ new york 2019-11-22 14:45:00 utc the financial services firm jp morgan chase has confirmed that its main headquarters will remain in new york city, despite discussions of relocating to other states.  
Head Entity: jp morgan chase  
Tail Entity: new york city  
#### person siblings ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person siblings" means a person 's sibling is a person.
Here is an example:
Relation: person siblings
Context: `` holly and sanjaya are headed to -lsb- the hawaiian island of -rsb- kauai tomorrow morning so she can meet his parents . ''
Head Entity: she
Tail Entity: sanjaya
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person siblings":
Relation: person siblings  
Context: `` during the family reunion, john introduced his sister to everyone, and they all had a great time reminiscing about their childhood. ''  
Head Entity: his sister  
Tail Entity: john  

Relation: person siblings  
Context: `` after the graduation ceremony, emily celebrated with her brother, who had always been her biggest supporter throughout school. ''  
Head Entity: her brother  
Tail Entity: emily  
Mixup data size:  285
MixupTrain:  epoch  0, batch     0 | loss: 2.0389376MixupTrain:  epoch  0, batch     1 | loss: 2.4333948MixupTrain:  epoch  0, batch     2 | loss: 2.5310244MixupTrain:  epoch  0, batch     3 | loss: 2.2300115MixupTrain:  epoch  0, batch     4 | loss: 2.4411476MixupTrain:  epoch  0, batch     5 | loss: 2.4195859MixupTrain:  epoch  0, batch     6 | loss: 2.1817018MixupTrain:  epoch  0, batch     7 | loss: 2.0406115MixupTrain:  epoch  0, batch     8 | loss: 2.3536873MixupTrain:  epoch  0, batch     9 | loss: 2.4957220MixupTrain:  epoch  0, batch    10 | loss: 2.2513520MixupTrain:  epoch  0, batch    11 | loss: 2.5596667MixupTrain:  epoch  0, batch    12 | loss: 2.1880616MixupTrain:  epoch  0, batch    13 | loss: 2.3811807MixupTrain:  epoch  0, batch    14 | loss: 1.8638560MixupTrain:  epoch  0, batch    15 | loss: 2.3036672MixupTrain:  epoch  0, batch    16 | loss: 1.8699071MixupTrain:  epoch  0, batch    17 | loss: 2.0157510
MemoryTrain:  epoch  0, batch     0 | loss: 2.0930929MemoryTrain:  epoch  0, batch     1 | loss: 1.7339603MemoryTrain:  epoch  0, batch     2 | loss: 2.0336974MemoryTrain:  epoch  0, batch     3 | loss: 2.9961345MemoryTrain:  epoch  0, batch     4 | loss: 1.7310618MemoryTrain:  epoch  0, batch     5 | loss: 2.3132591MemoryTrain:  epoch  0, batch     6 | loss: 1.9383972MemoryTrain:  epoch  0, batch     7 | loss: 2.9787776MemoryTrain:  epoch  1, batch     0 | loss: 1.9618242MemoryTrain:  epoch  1, batch     1 | loss: 2.3811781MemoryTrain:  epoch  1, batch     2 | loss: 2.0567071MemoryTrain:  epoch  1, batch     3 | loss: 2.2706923MemoryTrain:  epoch  1, batch     4 | loss: 1.7185308MemoryTrain:  epoch  1, batch     5 | loss: 1.5497923MemoryTrain:  epoch  1, batch     6 | loss: 2.2040465MemoryTrain:  epoch  1, batch     7 | loss: 1.5474997MemoryTrain:  epoch  2, batch     0 | loss: 2.1687927MemoryTrain:  epoch  2, batch     1 | loss: 1.5301578MemoryTrain:  epoch  2, batch     2 | loss: 1.7519342MemoryTrain:  epoch  2, batch     3 | loss: 1.5707002MemoryTrain:  epoch  2, batch     4 | loss: 2.2902389MemoryTrain:  epoch  2, batch     5 | loss: 1.8305154MemoryTrain:  epoch  2, batch     6 | loss: 1.7235019MemoryTrain:  epoch  2, batch     7 | loss: 1.4831134MemoryTrain:  epoch  3, batch     0 | loss: 1.7648418MemoryTrain:  epoch  3, batch     1 | loss: 1.5794894MemoryTrain:  epoch  3, batch     2 | loss: 1.6382949MemoryTrain:  epoch  3, batch     3 | loss: 1.7307761MemoryTrain:  epoch  3, batch     4 | loss: 2.0168321MemoryTrain:  epoch  3, batch     5 | loss: 1.6345912MemoryTrain:  epoch  3, batch     6 | loss: 1.8904493MemoryTrain:  epoch  3, batch     7 | loss: 1.3971726MemoryTrain:  epoch  4, batch     0 | loss: 1.7707264MemoryTrain:  epoch  4, batch     1 | loss: 1.4406698MemoryTrain:  epoch  4, batch     2 | loss: 1.4161172MemoryTrain:  epoch  4, batch     3 | loss: 1.3528848MemoryTrain:  epoch  4, batch     4 | loss: 1.9477713MemoryTrain:  epoch  4, batch     5 | loss: 1.8406655MemoryTrain:  epoch  4, batch     6 | loss: 1.3981888MemoryTrain:  epoch  4, batch     7 | loss: 1.2776979MemoryTrain:  epoch  5, batch     0 | loss: 1.4468768MemoryTrain:  epoch  5, batch     1 | loss: 1.4562892MemoryTrain:  epoch  5, batch     2 | loss: 1.6467125MemoryTrain:  epoch  5, batch     3 | loss: 1.3653188MemoryTrain:  epoch  5, batch     4 | loss: 1.7197056MemoryTrain:  epoch  5, batch     5 | loss: 1.4231272MemoryTrain:  epoch  5, batch     6 | loss: 1.4464180MemoryTrain:  epoch  5, batch     7 | loss: 1.6244764MemoryTrain:  epoch  6, batch     0 | loss: 1.3342872MemoryTrain:  epoch  6, batch     1 | loss: 1.4342599MemoryTrain:  epoch  6, batch     2 | loss: 1.4813049MemoryTrain:  epoch  6, batch     3 | loss: 1.2838979MemoryTrain:  epoch  6, batch     4 | loss: 1.3966258MemoryTrain:  epoch  6, batch     5 | loss: 1.4896418MemoryTrain:  epoch  6, batch     6 | loss: 1.6450822MemoryTrain:  epoch  6, batch     7 | loss: 1.3955117MemoryTrain:  epoch  7, batch     0 | loss: 1.4631407MemoryTrain:  epoch  7, batch     1 | loss: 1.3596053MemoryTrain:  epoch  7, batch     2 | loss: 1.3597994MemoryTrain:  epoch  7, batch     3 | loss: 1.3174305MemoryTrain:  epoch  7, batch     4 | loss: 1.2438885MemoryTrain:  epoch  7, batch     5 | loss: 1.2161552MemoryTrain:  epoch  7, batch     6 | loss: 1.4950702MemoryTrain:  epoch  7, batch     7 | loss: 1.7453104MemoryTrain:  epoch  8, batch     0 | loss: 1.3300492MemoryTrain:  epoch  8, batch     1 | loss: 1.4920850MemoryTrain:  epoch  8, batch     2 | loss: 1.3554724MemoryTrain:  epoch  8, batch     3 | loss: 1.5872636MemoryTrain:  epoch  8, batch     4 | loss: 1.3319610MemoryTrain:  epoch  8, batch     5 | loss: 1.3624318MemoryTrain:  epoch  8, batch     6 | loss: 1.2649240MemoryTrain:  epoch  8, batch     7 | loss: 1.3988680MemoryTrain:  epoch  9, batch     0 | loss: 1.2855153MemoryTrain:  epoch  9, batch     1 | loss: 1.5120239MemoryTrain:  epoch  9, batch     2 | loss: 1.2819307MemoryTrain:  epoch  9, batch     3 | loss: 1.2772321MemoryTrain:  epoch  9, batch     4 | loss: 1.3128049MemoryTrain:  epoch  9, batch     5 | loss: 1.4260894MemoryTrain:  epoch  9, batch     6 | loss: 1.4168475MemoryTrain:  epoch  9, batch     7 | loss: 1.3027439
[EVAL] batch:    0 | acc: 18.75%,  total acc: 18.75%   [EVAL] batch:    1 | acc: 31.25%,  total acc: 25.00%   [EVAL] batch:    2 | acc: 25.00%,  total acc: 25.00%   [EVAL] batch:    3 | acc: 31.25%,  total acc: 26.56%   [EVAL] batch:    4 | acc: 25.00%,  total acc: 26.25%   [EVAL] batch:    5 | acc: 43.75%,  total acc: 29.17%   [EVAL] batch:    6 | acc: 43.75%,  total acc: 31.25%   [EVAL] batch:    7 | acc: 62.50%,  total acc: 35.16%   [EVAL] batch:    8 | acc: 25.00%,  total acc: 34.03%   [EVAL] batch:    9 | acc: 31.25%,  total acc: 33.75%   [EVAL] batch:   10 | acc: 50.00%,  total acc: 35.23%   [EVAL] batch:   11 | acc: 37.50%,  total acc: 35.42%   [EVAL] batch:   12 | acc: 31.25%,  total acc: 35.10%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 37.50%   [EVAL] batch:   14 | acc: 50.00%,  total acc: 38.33%   [EVAL] batch:   15 | acc: 81.25%,  total acc: 41.02%   [EVAL] batch:   16 | acc: 87.50%,  total acc: 43.75%   [EVAL] batch:   17 | acc: 81.25%,  total acc: 45.83%   [EVAL] batch:   18 | acc: 18.75%,  total acc: 44.41%   [EVAL] batch:   19 | acc: 18.75%,  total acc: 43.12%   [EVAL] batch:   20 | acc: 18.75%,  total acc: 41.96%   [EVAL] batch:   21 | acc: 18.75%,  total acc: 40.91%   
[EVAL] batch:    0 | acc: 43.75%,  total acc: 43.75%   [EVAL] batch:    1 | acc: 50.00%,  total acc: 46.88%   [EVAL] batch:    2 | acc: 50.00%,  total acc: 47.92%   [EVAL] batch:    3 | acc: 37.50%,  total acc: 45.31%   [EVAL] batch:    4 | acc: 50.00%,  total acc: 46.25%   [EVAL] batch:    5 | acc: 50.00%,  total acc: 46.88%   [EVAL] batch:    6 | acc: 56.25%,  total acc: 48.21%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 53.91%   [EVAL] batch:    8 | acc: 75.00%,  total acc: 56.25%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 58.75%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 61.36%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 63.02%   [EVAL] batch:   12 | acc: 56.25%,  total acc: 62.50%   [EVAL] batch:   13 | acc: 31.25%,  total acc: 60.27%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 60.83%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 60.55%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 61.40%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 61.46%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 61.51%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 62.81%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 64.58%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 65.91%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 67.12%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 68.23%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 69.50%   [EVAL] batch:   25 | acc: 93.75%,  total acc: 70.43%   [EVAL] batch:   26 | acc: 87.50%,  total acc: 71.06%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 72.10%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 72.84%   [EVAL] batch:   29 | acc: 75.00%,  total acc: 72.92%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 73.39%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 74.22%   [EVAL] batch:   32 | acc: 50.00%,  total acc: 73.48%   [EVAL] batch:   33 | acc: 25.00%,  total acc: 72.06%   [EVAL] batch:   34 | acc: 25.00%,  total acc: 70.71%   [EVAL] batch:   35 | acc: 37.50%,  total acc: 69.79%   [EVAL] batch:   36 | acc: 43.75%,  total acc: 69.09%   [EVAL] batch:   37 | acc: 31.25%,  total acc: 68.09%   [EVAL] batch:   38 | acc: 50.00%,  total acc: 67.63%   [EVAL] batch:   39 | acc: 93.75%,  total acc: 68.28%   [EVAL] batch:   40 | acc: 37.50%,  total acc: 67.53%   [EVAL] batch:   41 | acc: 50.00%,  total acc: 67.11%   [EVAL] batch:   42 | acc: 56.25%,  total acc: 66.86%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 67.19%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 67.92%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 68.61%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 69.28%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 69.92%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 70.54%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 70.88%   [EVAL] batch:   50 | acc: 50.00%,  total acc: 70.47%   [EVAL] batch:   51 | acc: 0.00%,  total acc: 69.11%   [EVAL] batch:   52 | acc: 0.00%,  total acc: 67.81%   [EVAL] batch:   53 | acc: 0.00%,  total acc: 66.55%   [EVAL] batch:   54 | acc: 6.25%,  total acc: 65.45%   [EVAL] batch:   55 | acc: 0.00%,  total acc: 64.29%   [EVAL] batch:   56 | acc: 62.50%,  total acc: 64.25%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 64.66%   [EVAL] batch:   58 | acc: 81.25%,  total acc: 64.94%   [EVAL] batch:   59 | acc: 87.50%,  total acc: 65.31%   [EVAL] batch:   60 | acc: 68.75%,  total acc: 65.37%   [EVAL] batch:   61 | acc: 81.25%,  total acc: 65.62%   [EVAL] batch:   62 | acc: 81.25%,  total acc: 65.87%   [EVAL] batch:   63 | acc: 87.50%,  total acc: 66.21%   [EVAL] batch:   64 | acc: 87.50%,  total acc: 66.54%   [EVAL] batch:   65 | acc: 93.75%,  total acc: 66.95%   [EVAL] batch:   66 | acc: 100.00%,  total acc: 67.44%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 67.92%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 68.39%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 68.84%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 69.28%   [EVAL] batch:   71 | acc: 93.75%,  total acc: 69.62%   [EVAL] batch:   72 | acc: 0.00%,  total acc: 68.66%   [EVAL] batch:   73 | acc: 6.25%,  total acc: 67.82%   [EVAL] batch:   74 | acc: 62.50%,  total acc: 67.75%   [EVAL] batch:   75 | acc: 50.00%,  total acc: 67.52%   [EVAL] batch:   76 | acc: 68.75%,  total acc: 67.53%   [EVAL] batch:   77 | acc: 37.50%,  total acc: 67.15%   [EVAL] batch:   78 | acc: 56.25%,  total acc: 67.01%   [EVAL] batch:   79 | acc: 43.75%,  total acc: 66.72%   [EVAL] batch:   80 | acc: 56.25%,  total acc: 66.59%   [EVAL] batch:   81 | acc: 87.50%,  total acc: 66.84%   [EVAL] batch:   82 | acc: 75.00%,  total acc: 66.94%   [EVAL] batch:   83 | acc: 68.75%,  total acc: 66.96%   [EVAL] batch:   84 | acc: 56.25%,  total acc: 66.84%   [EVAL] batch:   85 | acc: 18.75%,  total acc: 66.28%   [EVAL] batch:   86 | acc: 43.75%,  total acc: 66.02%   [EVAL] batch:   87 | acc: 56.25%,  total acc: 65.91%   [EVAL] batch:   88 | acc: 62.50%,  total acc: 65.87%   [EVAL] batch:   89 | acc: 81.25%,  total acc: 66.04%   [EVAL] batch:   90 | acc: 62.50%,  total acc: 66.00%   [EVAL] batch:   91 | acc: 62.50%,  total acc: 65.96%   [EVAL] batch:   92 | acc: 75.00%,  total acc: 66.06%   [EVAL] batch:   93 | acc: 56.25%,  total acc: 65.96%   [EVAL] batch:   94 | acc: 93.75%,  total acc: 66.25%   [EVAL] batch:   95 | acc: 75.00%,  total acc: 66.34%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 66.69%   [EVAL] batch:   97 | acc: 87.50%,  total acc: 66.90%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 67.23%   [EVAL] batch:   99 | acc: 50.00%,  total acc: 67.06%   [EVAL] batch:  100 | acc: 43.75%,  total acc: 66.83%   [EVAL] batch:  101 | acc: 43.75%,  total acc: 66.61%   [EVAL] batch:  102 | acc: 37.50%,  total acc: 66.32%   [EVAL] batch:  103 | acc: 25.00%,  total acc: 65.93%   [EVAL] batch:  104 | acc: 81.25%,  total acc: 66.07%   [EVAL] batch:  105 | acc: 100.00%,  total acc: 66.39%   [EVAL] batch:  106 | acc: 50.00%,  total acc: 66.24%   [EVAL] batch:  107 | acc: 37.50%,  total acc: 65.97%   [EVAL] batch:  108 | acc: 37.50%,  total acc: 65.71%   [EVAL] batch:  109 | acc: 56.25%,  total acc: 65.62%   [EVAL] batch:  110 | acc: 37.50%,  total acc: 65.37%   [EVAL] batch:  111 | acc: 18.75%,  total acc: 64.96%   [EVAL] batch:  112 | acc: 37.50%,  total acc: 64.71%   [EVAL] batch:  113 | acc: 18.75%,  total acc: 64.31%   [EVAL] batch:  114 | acc: 31.25%,  total acc: 64.02%   [EVAL] batch:  115 | acc: 18.75%,  total acc: 63.63%   [EVAL] batch:  116 | acc: 50.00%,  total acc: 63.51%   [EVAL] batch:  117 | acc: 43.75%,  total acc: 63.35%   [EVAL] batch:  118 | acc: 50.00%,  total acc: 63.24%   [EVAL] batch:  119 | acc: 37.50%,  total acc: 63.02%   [EVAL] batch:  120 | acc: 31.25%,  total acc: 62.76%   [EVAL] batch:  121 | acc: 43.75%,  total acc: 62.60%   [EVAL] batch:  122 | acc: 50.00%,  total acc: 62.50%   [EVAL] batch:  123 | acc: 31.25%,  total acc: 62.25%   [EVAL] batch:  124 | acc: 62.50%,  total acc: 62.25%   [EVAL] batch:  125 | acc: 50.00%,  total acc: 62.15%   [EVAL] batch:  126 | acc: 68.75%,  total acc: 62.20%   [EVAL] batch:  127 | acc: 87.50%,  total acc: 62.40%   [EVAL] batch:  128 | acc: 81.25%,  total acc: 62.55%   [EVAL] batch:  129 | acc: 31.25%,  total acc: 62.31%   [EVAL] batch:  130 | acc: 25.00%,  total acc: 62.02%   [EVAL] batch:  131 | acc: 12.50%,  total acc: 61.65%   [EVAL] batch:  132 | acc: 25.00%,  total acc: 61.37%   
cur_acc:  ['0.8655', '0.9062', '0.6830', '0.7679', '0.5433', '0.6833', '0.5781', '0.4091']
his_acc:  ['0.8655', '0.8800', '0.8213', '0.7933', '0.7174', '0.6940', '0.6613', '0.6137']
--------Round  3
seed:  400
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/test.pkl
Task_order: [7 0 1 2 5 3 4 6]
prepared data!
CurrentTrain: epoch  0, batch     0 | loss: 13.2751312CurrentTrain: epoch  0, batch     1 | loss: 12.9934769CurrentTrain: epoch  0, batch     2 | loss: 13.0046778CurrentTrain: epoch  0, batch     3 | loss: 12.9312077CurrentTrain: epoch  0, batch     4 | loss: 12.7538576CurrentTrain: epoch  0, batch     5 | loss: 12.8402615CurrentTrain: epoch  0, batch     6 | loss: 12.6346426CurrentTrain: epoch  0, batch     7 | loss: 12.3881159CurrentTrain: epoch  0, batch     8 | loss: 12.3308420CurrentTrain: epoch  0, batch     9 | loss: 12.3044357CurrentTrain: epoch  0, batch    10 | loss: 12.4253006CurrentTrain: epoch  0, batch    11 | loss: 12.0380754CurrentTrain: epoch  0, batch    12 | loss: 11.8014450CurrentTrain: epoch  0, batch    13 | loss: 11.4248772CurrentTrain: epoch  0, batch    14 | loss: 11.6380672CurrentTrain: epoch  0, batch    15 | loss: 11.6364374CurrentTrain: epoch  0, batch    16 | loss: 11.5280762CurrentTrain: epoch  0, batch    17 | loss: 11.1812496CurrentTrain: epoch  0, batch    18 | loss: 11.1386490CurrentTrain: epoch  0, batch    19 | loss: 11.1234646CurrentTrain: epoch  0, batch    20 | loss: 10.8778162CurrentTrain: epoch  0, batch    21 | loss: 11.2167559CurrentTrain: epoch  0, batch    22 | loss: 11.2126198CurrentTrain: epoch  0, batch    23 | loss: 11.8636971CurrentTrain: epoch  0, batch    24 | loss: 11.6845779CurrentTrain: epoch  0, batch    25 | loss: 11.2303743CurrentTrain: epoch  0, batch    26 | loss: 10.9111147CurrentTrain: epoch  0, batch    27 | loss: 10.9932928CurrentTrain: epoch  0, batch    28 | loss: 10.8617611CurrentTrain: epoch  0, batch    29 | loss: 10.7070026CurrentTrain: epoch  0, batch    30 | loss: 11.1823235CurrentTrain: epoch  0, batch    31 | loss: 10.9521542CurrentTrain: epoch  0, batch    32 | loss: 10.5580025CurrentTrain: epoch  0, batch    33 | loss: 10.6120033CurrentTrain: epoch  0, batch    34 | loss: 10.5820160CurrentTrain: epoch  0, batch    35 | loss: 10.6714993CurrentTrain: epoch  0, batch    36 | loss: 10.5459566CurrentTrain: epoch  0, batch    37 | loss: 10.7252998CurrentTrain: epoch  1, batch     0 | loss: 9.8927498CurrentTrain: epoch  1, batch     1 | loss: 9.9803143CurrentTrain: epoch  1, batch     2 | loss: 10.5104599CurrentTrain: epoch  1, batch     3 | loss: 10.2297440CurrentTrain: epoch  1, batch     4 | loss: 10.0459213CurrentTrain: epoch  1, batch     5 | loss: 10.0325222CurrentTrain: epoch  1, batch     6 | loss: 10.1088343CurrentTrain: epoch  1, batch     7 | loss: 9.7013664CurrentTrain: epoch  1, batch     8 | loss: 9.6254864CurrentTrain: epoch  1, batch     9 | loss: 9.7911720CurrentTrain: epoch  1, batch    10 | loss: 9.7382879CurrentTrain: epoch  1, batch    11 | loss: 10.1401272CurrentTrain: epoch  1, batch    12 | loss: 10.0515394CurrentTrain: epoch  1, batch    13 | loss: 9.7337885CurrentTrain: epoch  1, batch    14 | loss: 9.9471159CurrentTrain: epoch  1, batch    15 | loss: 9.0212898CurrentTrain: epoch  1, batch    16 | loss: 9.0804844CurrentTrain: epoch  1, batch    17 | loss: 8.9751329CurrentTrain: epoch  1, batch    18 | loss: 9.3942823CurrentTrain: epoch  1, batch    19 | loss: 9.7048206CurrentTrain: epoch  1, batch    20 | loss: 9.4732552CurrentTrain: epoch  1, batch    21 | loss: 9.0787439CurrentTrain: epoch  1, batch    22 | loss: 9.7559338CurrentTrain: epoch  1, batch    23 | loss: 8.9746780CurrentTrain: epoch  1, batch    24 | loss: 9.9359093CurrentTrain: epoch  1, batch    25 | loss: 9.0865545CurrentTrain: epoch  1, batch    26 | loss: 8.9992638CurrentTrain: epoch  1, batch    27 | loss: 9.5570202CurrentTrain: epoch  1, batch    28 | loss: 8.6559973CurrentTrain: epoch  1, batch    29 | loss: 8.5303144CurrentTrain: epoch  1, batch    30 | loss: 8.5532131CurrentTrain: epoch  1, batch    31 | loss: 8.5039415CurrentTrain: epoch  1, batch    32 | loss: 8.4654446CurrentTrain: epoch  1, batch    33 | loss: 8.6045942CurrentTrain: epoch  1, batch    34 | loss: 9.0361013CurrentTrain: epoch  1, batch    35 | loss: 7.8695564CurrentTrain: epoch  1, batch    36 | loss: 8.6185894CurrentTrain: epoch  1, batch    37 | loss: 6.8820410CurrentTrain: epoch  2, batch     0 | loss: 7.3424540CurrentTrain: epoch  2, batch     1 | loss: 8.7137814CurrentTrain: epoch  2, batch     2 | loss: 9.2484770CurrentTrain: epoch  2, batch     3 | loss: 8.1818142CurrentTrain: epoch  2, batch     4 | loss: 7.8332872CurrentTrain: epoch  2, batch     5 | loss: 8.9005337CurrentTrain: epoch  2, batch     6 | loss: 8.9427204CurrentTrain: epoch  2, batch     7 | loss: 8.6538229CurrentTrain: epoch  2, batch     8 | loss: 8.6060543CurrentTrain: epoch  2, batch     9 | loss: 8.1671429CurrentTrain: epoch  2, batch    10 | loss: 8.1430864CurrentTrain: epoch  2, batch    11 | loss: 8.4249964CurrentTrain: epoch  2, batch    12 | loss: 8.2707272CurrentTrain: epoch  2, batch    13 | loss: 8.4266415CurrentTrain: epoch  2, batch    14 | loss: 7.9220667CurrentTrain: epoch  2, batch    15 | loss: 7.3542709CurrentTrain: epoch  2, batch    16 | loss: 8.1541986CurrentTrain: epoch  2, batch    17 | loss: 7.8025270CurrentTrain: epoch  2, batch    18 | loss: 7.6140194CurrentTrain: epoch  2, batch    19 | loss: 8.0758486CurrentTrain: epoch  2, batch    20 | loss: 8.2736158CurrentTrain: epoch  2, batch    21 | loss: 7.7926636CurrentTrain: epoch  2, batch    22 | loss: 7.1003809CurrentTrain: epoch  2, batch    23 | loss: 8.3459244CurrentTrain: epoch  2, batch    24 | loss: 8.1227322CurrentTrain: epoch  2, batch    25 | loss: 7.4243574CurrentTrain: epoch  2, batch    26 | loss: 7.9176059CurrentTrain: epoch  2, batch    27 | loss: 7.2601528CurrentTrain: epoch  2, batch    28 | loss: 6.9442062CurrentTrain: epoch  2, batch    29 | loss: 7.8150201CurrentTrain: epoch  2, batch    30 | loss: 8.2198944CurrentTrain: epoch  2, batch    31 | loss: 8.2794590CurrentTrain: epoch  2, batch    32 | loss: 8.1990271CurrentTrain: epoch  2, batch    33 | loss: 8.2454319CurrentTrain: epoch  2, batch    34 | loss: 8.2528963CurrentTrain: epoch  2, batch    35 | loss: 6.9121695CurrentTrain: epoch  2, batch    36 | loss: 7.4823575CurrentTrain: epoch  2, batch    37 | loss: 7.8599858CurrentTrain: epoch  3, batch     0 | loss: 7.3922620CurrentTrain: epoch  3, batch     1 | loss: 7.2980185CurrentTrain: epoch  3, batch     2 | loss: 7.4028244CurrentTrain: epoch  3, batch     3 | loss: 7.5295172CurrentTrain: epoch  3, batch     4 | loss: 7.6008573CurrentTrain: epoch  3, batch     5 | loss: 7.5369458CurrentTrain: epoch  3, batch     6 | loss: 8.3380566CurrentTrain: epoch  3, batch     7 | loss: 8.0649605CurrentTrain: epoch  3, batch     8 | loss: 6.1719670CurrentTrain: epoch  3, batch     9 | loss: 6.9788218CurrentTrain: epoch  3, batch    10 | loss: 8.7091084CurrentTrain: epoch  3, batch    11 | loss: 6.7978702CurrentTrain: epoch  3, batch    12 | loss: 7.0286121CurrentTrain: epoch  3, batch    13 | loss: 6.4043369CurrentTrain: epoch  3, batch    14 | loss: 7.1933789CurrentTrain: epoch  3, batch    15 | loss: 7.1258092CurrentTrain: epoch  3, batch    16 | loss: 7.5567317CurrentTrain: epoch  3, batch    17 | loss: 6.8674765CurrentTrain: epoch  3, batch    18 | loss: 7.0750017CurrentTrain: epoch  3, batch    19 | loss: 7.7137494CurrentTrain: epoch  3, batch    20 | loss: 6.8266630CurrentTrain: epoch  3, batch    21 | loss: 7.9183531CurrentTrain: epoch  3, batch    22 | loss: 7.2694569CurrentTrain: epoch  3, batch    23 | loss: 7.6992998CurrentTrain: epoch  3, batch    24 | loss: 7.3631220CurrentTrain: epoch  3, batch    25 | loss: 7.6779723CurrentTrain: epoch  3, batch    26 | loss: 6.6770053CurrentTrain: epoch  3, batch    27 | loss: 6.9625874CurrentTrain: epoch  3, batch    28 | loss: 7.6517835CurrentTrain: epoch  3, batch    29 | loss: 6.7958469CurrentTrain: epoch  3, batch    30 | loss: 6.6906376CurrentTrain: epoch  3, batch    31 | loss: 7.2527428CurrentTrain: epoch  3, batch    32 | loss: 6.5067816CurrentTrain: epoch  3, batch    33 | loss: 7.5858030CurrentTrain: epoch  3, batch    34 | loss: 7.1487164CurrentTrain: epoch  3, batch    35 | loss: 6.8882518CurrentTrain: epoch  3, batch    36 | loss: 6.4384761CurrentTrain: epoch  3, batch    37 | loss: 7.2749939CurrentTrain: epoch  4, batch     0 | loss: 6.5153837CurrentTrain: epoch  4, batch     1 | loss: 6.5377598CurrentTrain: epoch  4, batch     2 | loss: 6.0228586CurrentTrain: epoch  4, batch     3 | loss: 6.6800365CurrentTrain: epoch  4, batch     4 | loss: 6.5203996CurrentTrain: epoch  4, batch     5 | loss: 7.6076550CurrentTrain: epoch  4, batch     6 | loss: 5.8925514CurrentTrain: epoch  4, batch     7 | loss: 5.9701319CurrentTrain: epoch  4, batch     8 | loss: 6.2873812CurrentTrain: epoch  4, batch     9 | loss: 7.2706575CurrentTrain: epoch  4, batch    10 | loss: 7.0812683CurrentTrain: epoch  4, batch    11 | loss: 7.9736495CurrentTrain: epoch  4, batch    12 | loss: 7.5328693CurrentTrain: epoch  4, batch    13 | loss: 6.5989814CurrentTrain: epoch  4, batch    14 | loss: 6.8822575CurrentTrain: epoch  4, batch    15 | loss: 6.5861893CurrentTrain: epoch  4, batch    16 | loss: 6.9921694CurrentTrain: epoch  4, batch    17 | loss: 7.1670885CurrentTrain: epoch  4, batch    18 | loss: 7.6594067CurrentTrain: epoch  4, batch    19 | loss: 5.7784514CurrentTrain: epoch  4, batch    20 | loss: 7.1881490CurrentTrain: epoch  4, batch    21 | loss: 6.3352714CurrentTrain: epoch  4, batch    22 | loss: 6.3013210CurrentTrain: epoch  4, batch    23 | loss: 6.3720403CurrentTrain: epoch  4, batch    24 | loss: 6.5936918CurrentTrain: epoch  4, batch    25 | loss: 6.7695131CurrentTrain: epoch  4, batch    26 | loss: 6.7962503CurrentTrain: epoch  4, batch    27 | loss: 8.1289444CurrentTrain: epoch  4, batch    28 | loss: 6.4528370CurrentTrain: epoch  4, batch    29 | loss: 7.3232102CurrentTrain: epoch  4, batch    30 | loss: 8.1930180CurrentTrain: epoch  4, batch    31 | loss: 8.3471355CurrentTrain: epoch  4, batch    32 | loss: 6.5337915CurrentTrain: epoch  4, batch    33 | loss: 6.3552780CurrentTrain: epoch  4, batch    34 | loss: 6.5748081CurrentTrain: epoch  4, batch    35 | loss: 6.5203838CurrentTrain: epoch  4, batch    36 | loss: 6.6044025CurrentTrain: epoch  4, batch    37 | loss: 8.1898689CurrentTrain: epoch  5, batch     0 | loss: 6.8858428CurrentTrain: epoch  5, batch     1 | loss: 6.4758635CurrentTrain: epoch  5, batch     2 | loss: 7.3371139CurrentTrain: epoch  5, batch     3 | loss: 6.5807629CurrentTrain: epoch  5, batch     4 | loss: 6.1125174CurrentTrain: epoch  5, batch     5 | loss: 6.1262178CurrentTrain: epoch  5, batch     6 | loss: 6.1425629CurrentTrain: epoch  5, batch     7 | loss: 6.6071739CurrentTrain: epoch  5, batch     8 | loss: 6.5047493CurrentTrain: epoch  5, batch     9 | loss: 5.9395576CurrentTrain: epoch  5, batch    10 | loss: 6.2485132CurrentTrain: epoch  5, batch    11 | loss: 6.4042974CurrentTrain: epoch  5, batch    12 | loss: 6.6484003CurrentTrain: epoch  5, batch    13 | loss: 6.6133070CurrentTrain: epoch  5, batch    14 | loss: 5.7524624CurrentTrain: epoch  5, batch    15 | loss: 6.1285181CurrentTrain: epoch  5, batch    16 | loss: 6.0351896CurrentTrain: epoch  5, batch    17 | loss: 6.4663506CurrentTrain: epoch  5, batch    18 | loss: 6.1720915CurrentTrain: epoch  5, batch    19 | loss: 5.4920588CurrentTrain: epoch  5, batch    20 | loss: 6.5173874CurrentTrain: epoch  5, batch    21 | loss: 5.7827029CurrentTrain: epoch  5, batch    22 | loss: 6.5556469CurrentTrain: epoch  5, batch    23 | loss: 6.8126559CurrentTrain: epoch  5, batch    24 | loss: 6.2323775CurrentTrain: epoch  5, batch    25 | loss: 6.3520226CurrentTrain: epoch  5, batch    26 | loss: 6.9469891CurrentTrain: epoch  5, batch    27 | loss: 6.7373543CurrentTrain: epoch  5, batch    28 | loss: 6.1467476CurrentTrain: epoch  5, batch    29 | loss: 6.3682547CurrentTrain: epoch  5, batch    30 | loss: 6.4232044CurrentTrain: epoch  5, batch    31 | loss: 5.6488686CurrentTrain: epoch  5, batch    32 | loss: 5.8666754CurrentTrain: epoch  5, batch    33 | loss: 6.1165686CurrentTrain: epoch  5, batch    34 | loss: 6.2438197CurrentTrain: epoch  5, batch    35 | loss: 6.2853661CurrentTrain: epoch  5, batch    36 | loss: 5.9838676CurrentTrain: epoch  5, batch    37 | loss: 10.2782917CurrentTrain: epoch  6, batch     0 | loss: 6.3220057CurrentTrain: epoch  6, batch     1 | loss: 6.5160985CurrentTrain: epoch  6, batch     2 | loss: 6.4654989CurrentTrain: epoch  6, batch     3 | loss: 5.8268700CurrentTrain: epoch  6, batch     4 | loss: 5.8906012CurrentTrain: epoch  6, batch     5 | loss: 6.2824583CurrentTrain: epoch  6, batch     6 | loss: 6.3375511CurrentTrain: epoch  6, batch     7 | loss: 6.4430614CurrentTrain: epoch  6, batch     8 | loss: 5.8010840CurrentTrain: epoch  6, batch     9 | loss: 6.5273304CurrentTrain: epoch  6, batch    10 | loss: 6.2467399CurrentTrain: epoch  6, batch    11 | loss: 5.9371004CurrentTrain: epoch  6, batch    12 | loss: 6.3667049CurrentTrain: epoch  6, batch    13 | loss: 6.6715055CurrentTrain: epoch  6, batch    14 | loss: 5.5882025CurrentTrain: epoch  6, batch    15 | loss: 5.8246775CurrentTrain: epoch  6, batch    16 | loss: 6.3062119CurrentTrain: epoch  6, batch    17 | loss: 6.2908678CurrentTrain: epoch  6, batch    18 | loss: 6.8514619CurrentTrain: epoch  6, batch    19 | loss: 5.4943829CurrentTrain: epoch  6, batch    20 | loss: 5.7923555CurrentTrain: epoch  6, batch    21 | loss: 5.4001055CurrentTrain: epoch  6, batch    22 | loss: 5.4854612CurrentTrain: epoch  6, batch    23 | loss: 5.5313253CurrentTrain: epoch  6, batch    24 | loss: 6.2476835CurrentTrain: epoch  6, batch    25 | loss: 5.5042238CurrentTrain: epoch  6, batch    26 | loss: 6.0276480CurrentTrain: epoch  6, batch    27 | loss: 5.7902136CurrentTrain: epoch  6, batch    28 | loss: 5.7188830CurrentTrain: epoch  6, batch    29 | loss: 5.6295257CurrentTrain: epoch  6, batch    30 | loss: 5.9321451CurrentTrain: epoch  6, batch    31 | loss: 6.0527396CurrentTrain: epoch  6, batch    32 | loss: 5.5504875CurrentTrain: epoch  6, batch    33 | loss: 6.3380389CurrentTrain: epoch  6, batch    34 | loss: 5.5908489CurrentTrain: epoch  6, batch    35 | loss: 5.9655843CurrentTrain: epoch  6, batch    36 | loss: 5.5754538CurrentTrain: epoch  6, batch    37 | loss: 5.4493852CurrentTrain: epoch  7, batch     0 | loss: 5.7289209CurrentTrain: epoch  7, batch     1 | loss: 5.3429422CurrentTrain: epoch  7, batch     2 | loss: 5.7274189CurrentTrain: epoch  7, batch     3 | loss: 5.3301630CurrentTrain: epoch  7, batch     4 | loss: 5.5585904CurrentTrain: epoch  7, batch     5 | loss: 5.7684026CurrentTrain: epoch  7, batch     6 | loss: 5.3950882CurrentTrain: epoch  7, batch     7 | loss: 5.5004730CurrentTrain: epoch  7, batch     8 | loss: 5.3679900CurrentTrain: epoch  7, batch     9 | loss: 5.5761681CurrentTrain: epoch  7, batch    10 | loss: 5.3308058CurrentTrain: epoch  7, batch    11 | loss: 5.6560726CurrentTrain: epoch  7, batch    12 | loss: 6.1090255CurrentTrain: epoch  7, batch    13 | loss: 5.1885014CurrentTrain: epoch  7, batch    14 | loss: 5.2211094CurrentTrain: epoch  7, batch    15 | loss: 5.2561865CurrentTrain: epoch  7, batch    16 | loss: 5.8453388CurrentTrain: epoch  7, batch    17 | loss: 5.8443384CurrentTrain: epoch  7, batch    18 | loss: 5.2581172CurrentTrain: epoch  7, batch    19 | loss: 5.2289500CurrentTrain: epoch  7, batch    20 | loss: 5.1776509CurrentTrain: epoch  7, batch    21 | loss: 5.6912284CurrentTrain: epoch  7, batch    22 | loss: 5.9825015CurrentTrain: epoch  7, batch    23 | loss: 5.7333884CurrentTrain: epoch  7, batch    24 | loss: 5.6897202CurrentTrain: epoch  7, batch    25 | loss: 6.2304626CurrentTrain: epoch  7, batch    26 | loss: 5.3513060CurrentTrain: epoch  7, batch    27 | loss: 5.6622810CurrentTrain: epoch  7, batch    28 | loss: 5.1570177CurrentTrain: epoch  7, batch    29 | loss: 5.3072453CurrentTrain: epoch  7, batch    30 | loss: 5.1082902CurrentTrain: epoch  7, batch    31 | loss: 5.9474497CurrentTrain: epoch  7, batch    32 | loss: 5.1233954CurrentTrain: epoch  7, batch    33 | loss: 5.6555090CurrentTrain: epoch  7, batch    34 | loss: 5.4467239CurrentTrain: epoch  7, batch    35 | loss: 4.9583206CurrentTrain: epoch  7, batch    36 | loss: 5.1226664CurrentTrain: epoch  7, batch    37 | loss: 6.5184641CurrentTrain: epoch  8, batch     0 | loss: 6.0772839CurrentTrain: epoch  8, batch     1 | loss: 4.9834185CurrentTrain: epoch  8, batch     2 | loss: 5.1469064CurrentTrain: epoch  8, batch     3 | loss: 4.9650602CurrentTrain: epoch  8, batch     4 | loss: 5.3893685CurrentTrain: epoch  8, batch     5 | loss: 5.2620668CurrentTrain: epoch  8, batch     6 | loss: 5.1101942CurrentTrain: epoch  8, batch     7 | loss: 5.2686653CurrentTrain: epoch  8, batch     8 | loss: 5.3044596CurrentTrain: epoch  8, batch     9 | loss: 5.2094002CurrentTrain: epoch  8, batch    10 | loss: 5.6906385CurrentTrain: epoch  8, batch    11 | loss: 5.0235052CurrentTrain: epoch  8, batch    12 | loss: 5.7215705CurrentTrain: epoch  8, batch    13 | loss: 4.9918365CurrentTrain: epoch  8, batch    14 | loss: 5.2414837CurrentTrain: epoch  8, batch    15 | loss: 5.0509286CurrentTrain: epoch  8, batch    16 | loss: 5.5217934CurrentTrain: epoch  8, batch    17 | loss: 5.2669544CurrentTrain: epoch  8, batch    18 | loss: 5.7403421CurrentTrain: epoch  8, batch    19 | loss: 5.4526954CurrentTrain: epoch  8, batch    20 | loss: 5.3385043CurrentTrain: epoch  8, batch    21 | loss: 5.1520367CurrentTrain: epoch  8, batch    22 | loss: 5.6443586CurrentTrain: epoch  8, batch    23 | loss: 5.2233305CurrentTrain: epoch  8, batch    24 | loss: 5.6086550CurrentTrain: epoch  8, batch    25 | loss: 5.3993979CurrentTrain: epoch  8, batch    26 | loss: 5.4141207CurrentTrain: epoch  8, batch    27 | loss: 5.0969114CurrentTrain: epoch  8, batch    28 | loss: 5.1717319CurrentTrain: epoch  8, batch    29 | loss: 5.8224435CurrentTrain: epoch  8, batch    30 | loss: 5.3739653CurrentTrain: epoch  8, batch    31 | loss: 5.8341560CurrentTrain: epoch  8, batch    32 | loss: 5.2827830CurrentTrain: epoch  8, batch    33 | loss: 5.0107355CurrentTrain: epoch  8, batch    34 | loss: 5.1564469CurrentTrain: epoch  8, batch    35 | loss: 5.7194891CurrentTrain: epoch  8, batch    36 | loss: 5.6813068CurrentTrain: epoch  8, batch    37 | loss: 5.2209568CurrentTrain: epoch  9, batch     0 | loss: 5.2478080CurrentTrain: epoch  9, batch     1 | loss: 5.0978575CurrentTrain: epoch  9, batch     2 | loss: 5.7625961CurrentTrain: epoch  9, batch     3 | loss: 5.1258345CurrentTrain: epoch  9, batch     4 | loss: 5.1212440CurrentTrain: epoch  9, batch     5 | loss: 5.3523359CurrentTrain: epoch  9, batch     6 | loss: 5.5795174CurrentTrain: epoch  9, batch     7 | loss: 5.1101475CurrentTrain: epoch  9, batch     8 | loss: 5.1598649CurrentTrain: epoch  9, batch     9 | loss: 5.0571070CurrentTrain: epoch  9, batch    10 | loss: 5.1066365CurrentTrain: epoch  9, batch    11 | loss: 5.2344851CurrentTrain: epoch  9, batch    12 | loss: 5.0220551CurrentTrain: epoch  9, batch    13 | loss: 5.1218686CurrentTrain: epoch  9, batch    14 | loss: 4.8771830CurrentTrain: epoch  9, batch    15 | loss: 5.0200753CurrentTrain: epoch  9, batch    16 | loss: 5.0543833CurrentTrain: epoch  9, batch    17 | loss: 5.5176497CurrentTrain: epoch  9, batch    18 | loss: 4.9481235CurrentTrain: epoch  9, batch    19 | loss: 5.1715765CurrentTrain: epoch  9, batch    20 | loss: 4.9030857CurrentTrain: epoch  9, batch    21 | loss: 4.9782524CurrentTrain: epoch  9, batch    22 | loss: 4.8488045CurrentTrain: epoch  9, batch    23 | loss: 5.5208621CurrentTrain: epoch  9, batch    24 | loss: 5.7489724CurrentTrain: epoch  9, batch    25 | loss: 5.1088743CurrentTrain: epoch  9, batch    26 | loss: 5.0661712CurrentTrain: epoch  9, batch    27 | loss: 5.0008140CurrentTrain: epoch  9, batch    28 | loss: 5.1422329CurrentTrain: epoch  9, batch    29 | loss: 5.5686226CurrentTrain: epoch  9, batch    30 | loss: 5.0199499CurrentTrain: epoch  9, batch    31 | loss: 5.2708874CurrentTrain: epoch  9, batch    32 | loss: 4.9549284CurrentTrain: epoch  9, batch    33 | loss: 4.9790792CurrentTrain: epoch  9, batch    34 | loss: 4.8859859CurrentTrain: epoch  9, batch    35 | loss: 5.0103703CurrentTrain: epoch  9, batch    36 | loss: 5.1500316CurrentTrain: epoch  9, batch    37 | loss: 4.7567024
#### person countries of residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person countries of residence" means a person is living in a country.
Here is an example:
Relation: person countries of residence
Context: the head of iran 's atomic energy organisation , ali akbar salehi , told domestic news agencies the new proposal had come in response to iran 's move last week to begin enriching uranium itself to the 20 percent level required for a tehran medical research rector after rejecting a previous offer .
Head Entity: ali akbar salehi
Tail Entity: iran
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person countries of residence":
Relation: person countries of residence  
Context: After years of living in the bustling city of New York, Maria Gonzalez decided to return to her hometown in Mexico, where she felt a stronger connection to her roots and family.  
Head Entity: Maria Gonzalez  
Tail Entity: Mexico  

Relation: person countries of residence  
Context: Following his successful career in London, James Smith relocated to Australia, seeking a more relaxed lifestyle by the beach.  
Head Entity: James Smith  
Tail Entity: Australia  
#### organization top members employees ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization top members employees" means an organization 's employer is a person.
Here is an example:
Relation: organization top members employees
Context: `` decriminalization is a positive step for mexican democracy , '' said gonzalo marroquin , president of the press freedom committee of the inter american press association .
Head Entity: inter american press association
Tail Entity: gonzalo marroquin
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization top members employees":
Relation: organization top members employees  
Context: "We are thrilled to announce that Sarah Johnson, our new CEO, will be leading the company into its next phase of growth," said the board of directors of Tech Innovations Inc.  
Head Entity: Tech Innovations Inc.  
Tail Entity: Sarah Johnson  

Relation: organization top members employees  
Context: "As the chief financial officer, Mark Thompson has played a crucial role in our success," stated the management team of Global Finance Solutions.  
Head Entity: Global Finance Solutions  
Tail Entity: Mark Thompson  
#### organization member of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization member of" means an organization is the member of an organization.
Here is an example:
Relation: organization member of
Context: south africa 1-5 leyds cnr biccard streets cosatu 's concern has been that the scorpions ' existence as a branch of the national directorate of public prosecutions blurred the necessary separation of functions between those investigating crime and those prosecuting the criminals .
Head Entity: scorpions
Tail Entity: national directorate of public prosecutions
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization member of":
Relation: organization member of  
Context: The International Federation of Red Cross and Red Crescent Societies (IFRC) is a global humanitarian organization that coordinates the activities of national Red Cross and Red Crescent societies around the world.  
Head Entity: International Federation of Red Cross and Red Crescent Societies  
Tail Entity: national Red Cross and Red Crescent societies  

Relation: organization member of  
Context: The European Union is a political and economic union of member states that are located primarily in Europe, working together to promote peace and stability in the region.  
Head Entity: European Union  
Tail Entity: member states  
#### person origin ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person origin" means a person 's nationality is a country.
Here is an example:
Relation: person origin
Context: urgent chadian pm dies in paris hospital chad 's prime minister pascal yoadimnadji has died from a cerebral hemorrhage in a paris hospital , the chadian ambassador said friday .
Head Entity: pascal yoadimnadji
Tail Entity: chadian
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person origin":
Relation: person origin  
Context: the renowned scientist albert einstein was born in ulm, germany, and later became a citizen of the united states.  
Head Entity: albert einstein  
Tail Entity: german  

Relation: person origin  
Context: the famous author chimamanda ngozi adichie hails from enugu, nigeria, where she developed her passion for storytelling.  
Head Entity: chimamanda ngozi adichie  
Tail Entity: nigerian  
#### person title ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person title" means a person 's title is a title.
Here is an example:
Relation: person title
Context: earlier , in jerusalem , he spoke at the state funeral for the city 's fabled former mayor , teddy kollek , who died tuesday at 95 and was buried in the area of the mount herzl cemetery reserved for israel 's leaders .
Head Entity: teddy kollek
Tail Entity: mayor
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person title":
Relation: person title  
Context: After years of dedicated service, Dr. Emily Carter was honored with a prestigious award at the annual conference, recognizing her as the leading scientist in her field.  
Head Entity: Dr. Emily Carter  
Tail Entity: leading scientist  

Relation: person title  
Context: During the ceremony, the renowned author James Patterson received accolades for his contributions to literature, solidifying his status as a bestselling novelist.  
Head Entity: James Patterson  
Tail Entity: bestselling novelist  
#### organization country of headquarters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization country of headquarters" means an organization is located in a country.
Here is an example:
Relation: organization country of headquarters
Context: the credit crisis spread to the largest us bond insurer thursday , sending shares of mbia inc plunging and calling into question the safety of tens of billions of dollars of company and local government debt held by investors .
Head Entity: mbia
Tail Entity: us
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization country of headquarters":
Relation: organization country of headquarters  
Context: after years of rapid growth, the tech giant apple inc announced plans to expand its operations in ireland, taking advantage of the favorable tax environment.  
Head Entity: apple  
Tail Entity: ireland  

Relation: organization country of headquarters  
Context: the renowned automotive manufacturer toyota motor corporation has established its main office in japan, where it continues to innovate in the field of electric vehicles.  
Head Entity: toyota  
Tail Entity: japan  
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 75.00%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 79.17%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 79.69%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 80.00%   [EVAL] batch:    5 | acc: 68.75%,  total acc: 78.12%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 80.36%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 82.81%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 84.72%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 85.62%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 86.93%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 87.98%   [EVAL] batch:   13 | acc: 75.00%,  total acc: 87.05%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 85.83%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 83.98%   [EVAL] batch:   16 | acc: 81.25%,  total acc: 83.82%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 82.99%   [EVAL] batch:   18 | acc: 81.25%,  total acc: 82.89%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 83.44%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 84.23%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 84.94%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 85.60%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 86.20%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 86.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 87.26%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 87.95%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 88.36%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 88.12%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 88.10%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 88.28%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 86.74%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 75.00%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 79.17%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 79.69%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 80.00%   [EVAL] batch:    5 | acc: 68.75%,  total acc: 78.12%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 80.36%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 82.81%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 84.72%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 85.62%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 86.93%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 87.98%   [EVAL] batch:   13 | acc: 75.00%,  total acc: 87.05%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 85.83%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 83.98%   [EVAL] batch:   16 | acc: 81.25%,  total acc: 83.82%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 82.99%   [EVAL] batch:   18 | acc: 81.25%,  total acc: 82.89%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 83.44%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 84.23%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 84.94%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 85.60%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 86.20%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 86.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 87.26%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 87.95%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 88.36%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 88.12%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 88.10%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 88.28%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 86.74%   
cur_acc:  ['0.8674']
his_acc:  ['0.8674']
CurrentTrain: epoch  0, batch     0 | loss: 5.9451246CurrentTrain: epoch  0, batch     1 | loss: 5.5775232CurrentTrain: epoch  1, batch     0 | loss: 4.9242625CurrentTrain: epoch  1, batch     1 | loss: 4.7678790CurrentTrain: epoch  2, batch     0 | loss: 4.4710879CurrentTrain: epoch  2, batch     1 | loss: 3.7929890CurrentTrain: epoch  3, batch     0 | loss: 4.1876349CurrentTrain: epoch  3, batch     1 | loss: 3.8888986CurrentTrain: epoch  4, batch     0 | loss: 3.8164685CurrentTrain: epoch  4, batch     1 | loss: 3.3983767CurrentTrain: epoch  5, batch     0 | loss: 3.7890971CurrentTrain: epoch  5, batch     1 | loss: 3.0779440CurrentTrain: epoch  6, batch     0 | loss: 3.1315944CurrentTrain: epoch  6, batch     1 | loss: 3.7314584CurrentTrain: epoch  7, batch     0 | loss: 3.6060050CurrentTrain: epoch  7, batch     1 | loss: 3.1471348CurrentTrain: epoch  8, batch     0 | loss: 3.1768365CurrentTrain: epoch  8, batch     1 | loss: 3.1766319CurrentTrain: epoch  9, batch     0 | loss: 2.6757472CurrentTrain: epoch  9, batch     1 | loss: 3.5657201
#### person country of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person country of birth" means a person was born in a country.
Here is an example:
Relation: person country of birth
Context: in testimony by satellite link from germany to a house of representatives ' panel , murat kurnaz recounted his five-year detention , alleging a wide range of torture and abuse .
Head Entity: murat kurnaz
Tail Entity: germany
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person country of birth":
Relation: person country of birth  
Context: During the interview, the renowned artist shared his experiences growing up in the vibrant streets of Barcelona, which he credits as the foundation of his creative journey.  
Head Entity: the renowned artist  
Tail Entity: Barcelona  

Relation: person country of birth  
Context: In her autobiography, the famous scientist details her early life in Tokyo, where she developed a passion for technology and innovation.  
Head Entity: the famous scientist  
Tail Entity: Tokyo  
#### organization website ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization website" means an organization 's website is an url.
Here is an example:
Relation: organization website
Context: http://www.pentaxsailaway.com/ paul j. sail away with pentax sweepstakes
Head Entity: pentax
Tail Entity: http://www.pentaxsailaway.com/
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization website":
Relation: organization website  
Context: Visit the official site at https://www.techinnovators.com for more information on their latest products.  
Head Entity: Tech Innovators  
Tail Entity: https://www.techinnovators.com  

Relation: organization website  
Context: For updates and news, check out the blog at http://www.greenearth.org/blog.  
Head Entity: Green Earth  
Tail Entity: http://www.greenearth.org/
#### organization shareholders ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization shareholders" means an organization was invested by a person.
Here is an example:
Relation: organization shareholders
Context: marsans already owns the madrid-based air comet and 95 percent of aerolineas argentinas .
Head Entity: aerolineas argentinas
Tail Entity: marsans
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization shareholders":
Relation: organization shareholders  
Context: tech giant apple has seen significant investments from billionaire investor warren buffett through his company berkshire hathaway.  
Head Entity: apple  
Tail Entity: warren buffett  

Relation: organization shareholders  
Context: the popular streaming service netflix has attracted funding from various investors, including the well-known venture capital firm sequoia capital.  
Head Entity: netflix  
Tail Entity: sequoia capital  
#### organization dissolved ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization dissolved" means an organization was dissolved in a date.
Here is an example:
Relation: organization dissolved
Context: the tse suffered its worst-ever system crash in november 2005 which paralyzed the world 's second largest bourse and forced it to shelve plan for a listing of its own .
Head Entity: tse
Tail Entity: november 2005
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization dissolved":
Relation: organization dissolved  
Context: the once-prominent tech startup, Innovatech, officially ceased operations in early 2020 after struggling to secure funding.  
Head Entity: Innovatech  
Tail Entity: early 2020  

Relation: organization dissolved  
Context: after years of financial difficulties, the local arts council announced its closure in the summer of 2019, leaving many artists without support.  
Head Entity: local arts council  
Tail Entity: summer of 2019  
#### organization founded by ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization founded by" means an organization was found by a person.
Here is an example:
Relation: organization founded by
Context: born in new york , she inherited most of her fortune from her father , ted arison , who founded carnival cruise lines and also owned the miami heat basketball team .
Head Entity: carnival cruise lines
Tail Entity: ted arison
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization founded by":
Relation: organization founded by  
Context: in 1975, steve jobs and steve wozniak started apple inc., which revolutionized the personal computer industry and changed the way people interact with technology.  
Head Entity: apple inc.  
Tail Entity: steve jobs  

Relation: organization founded by  
Context: the famous fashion brand gucci was established in 1921 by guccio gucci in florence, italy, and has since become a symbol of luxury and style.  
Head Entity: gucci  
Tail Entity: guccio gucci  
Mixup data size:  104
MixupTrain:  epoch  0, batch     0 | loss: 6.5323409MixupTrain:  epoch  0, batch     1 | loss: 5.6032631MixupTrain:  epoch  0, batch     2 | loss: 5.4287303MixupTrain:  epoch  0, batch     3 | loss: 5.8847919MixupTrain:  epoch  0, batch     4 | loss: 4.9769885MixupTrain:  epoch  0, batch     5 | loss: 5.4616429MixupTrain:  epoch  0, batch     6 | loss: 4.3370852
MemoryTrain:  epoch  0, batch     0 | loss: 4.7636333MemoryTrain:  epoch  0, batch     1 | loss: 5.3452573MemoryTrain:  epoch  0, batch     2 | loss: 4.1403947MemoryTrain:  epoch  1, batch     0 | loss: 4.8621492MemoryTrain:  epoch  1, batch     1 | loss: 4.5332966MemoryTrain:  epoch  1, batch     2 | loss: 2.8358738MemoryTrain:  epoch  2, batch     0 | loss: 4.5871105MemoryTrain:  epoch  2, batch     1 | loss: 4.2531476MemoryTrain:  epoch  2, batch     2 | loss: 1.7140541MemoryTrain:  epoch  3, batch     0 | loss: 3.8316305MemoryTrain:  epoch  3, batch     1 | loss: 3.9106612MemoryTrain:  epoch  3, batch     2 | loss: 1.8499390MemoryTrain:  epoch  4, batch     0 | loss: 3.0077803MemoryTrain:  epoch  4, batch     1 | loss: 3.8025360MemoryTrain:  epoch  4, batch     2 | loss: 5.7182479MemoryTrain:  epoch  5, batch     0 | loss: 2.7246313MemoryTrain:  epoch  5, batch     1 | loss: 4.0735946MemoryTrain:  epoch  5, batch     2 | loss: 1.3469485MemoryTrain:  epoch  6, batch     0 | loss: 3.5301223MemoryTrain:  epoch  6, batch     1 | loss: 2.8544140MemoryTrain:  epoch  6, batch     2 | loss: 1.5787772MemoryTrain:  epoch  7, batch     0 | loss: 2.7492943MemoryTrain:  epoch  7, batch     1 | loss: 3.3040943MemoryTrain:  epoch  7, batch     2 | loss: 5.7884984MemoryTrain:  epoch  8, batch     0 | loss: 2.8705492MemoryTrain:  epoch  8, batch     1 | loss: 3.2548065MemoryTrain:  epoch  8, batch     2 | loss: 1.2384913MemoryTrain:  epoch  9, batch     0 | loss: 2.7344820MemoryTrain:  epoch  9, batch     1 | loss: 2.6274059MemoryTrain:  epoch  9, batch     2 | loss: 5.1570115
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 25.00%,  total acc: 60.42%   [EVAL] batch:    3 | acc: 18.75%,  total acc: 50.00%   [EVAL] batch:    4 | acc: 6.25%,  total acc: 41.25%   [EVAL] batch:    5 | acc: 31.25%,  total acc: 39.58%   [EVAL] batch:    6 | acc: 31.25%,  total acc: 38.39%   [EVAL] batch:    7 | acc: 6.25%,  total acc: 34.38%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 82.81%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    6 | acc: 43.75%,  total acc: 75.89%   [EVAL] batch:    7 | acc: 25.00%,  total acc: 69.53%   [EVAL] batch:    8 | acc: 25.00%,  total acc: 64.58%   [EVAL] batch:    9 | acc: 43.75%,  total acc: 62.50%   [EVAL] batch:   10 | acc: 6.25%,  total acc: 57.39%   [EVAL] batch:   11 | acc: 12.50%,  total acc: 53.65%   [EVAL] batch:   12 | acc: 56.25%,  total acc: 53.85%   [EVAL] batch:   13 | acc: 62.50%,  total acc: 54.46%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 55.83%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 55.86%   [EVAL] batch:   16 | acc: 81.25%,  total acc: 57.35%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 57.64%   [EVAL] batch:   18 | acc: 81.25%,  total acc: 58.88%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 60.62%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 62.50%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 64.20%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 65.76%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 67.19%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 68.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 69.71%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 70.60%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 71.65%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 72.63%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 73.12%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 73.79%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 74.41%   [EVAL] batch:   32 | acc: 62.50%,  total acc: 74.05%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 74.82%   [EVAL] batch:   34 | acc: 43.75%,  total acc: 73.93%   [EVAL] batch:   35 | acc: 25.00%,  total acc: 72.57%   [EVAL] batch:   36 | acc: 12.50%,  total acc: 70.95%   [EVAL] batch:   37 | acc: 18.75%,  total acc: 69.57%   [EVAL] batch:   38 | acc: 37.50%,  total acc: 68.75%   [EVAL] batch:   39 | acc: 12.50%,  total acc: 67.34%   
cur_acc:  ['0.8674', '0.3438']
his_acc:  ['0.8674', '0.6734']
CurrentTrain: epoch  0, batch     0 | loss: 4.9575863CurrentTrain: epoch  0, batch     1 | loss: 5.6915078CurrentTrain: epoch  1, batch     0 | loss: 4.4439430CurrentTrain: epoch  1, batch     1 | loss: 4.5535445CurrentTrain: epoch  2, batch     0 | loss: 4.2544484CurrentTrain: epoch  2, batch     1 | loss: 3.4342504CurrentTrain: epoch  3, batch     0 | loss: 3.3605330CurrentTrain: epoch  3, batch     1 | loss: 3.6301579CurrentTrain: epoch  4, batch     0 | loss: 3.3773360CurrentTrain: epoch  4, batch     1 | loss: 3.1731255CurrentTrain: epoch  5, batch     0 | loss: 2.7898068CurrentTrain: epoch  5, batch     1 | loss: 2.5903640CurrentTrain: epoch  6, batch     0 | loss: 2.6678023CurrentTrain: epoch  6, batch     1 | loss: 2.6946988CurrentTrain: epoch  7, batch     0 | loss: 2.5605955CurrentTrain: epoch  7, batch     1 | loss: 2.8493955CurrentTrain: epoch  8, batch     0 | loss: 2.5922523CurrentTrain: epoch  8, batch     1 | loss: 2.3281434CurrentTrain: epoch  9, batch     0 | loss: 2.3061697CurrentTrain: epoch  9, batch     1 | loss: 2.3907306
#### organization founded ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization founded" means an organization was found in a date.
Here is an example:
Relation: organization founded
Context: pandit worked at the brokerage morgan stanley for about 11 years until 2005 , when he and some morgan stanley colleagues quit and later founded the hedge fund old lane partners .
Head Entity: old lane partners
Tail Entity: 2005
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization founded":
Relation: organization founded  
Context: In 1998, a group of engineers and entrepreneurs came together to establish the tech startup, Innovatech Solutions, which has since become a leader in software development.  
Head Entity: Innovatech Solutions  
Tail Entity: 1998  

Relation: organization founded  
Context: The non-profit organization Green Earth Initiative was established in 2010 to promote environmental awareness and sustainability practices across communities.  
Head Entity: Green Earth Initiative  
Tail Entity: 2010  
#### person age ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person age" means a person 's age is a number.
Here is an example:
Relation: person age
Context: susan boyle is 48 years old now .
Head Entity: susan boyle
Tail Entity: 48
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person age":
Relation: person age  
Context: john smith turned 30 last week.  
Head Entity: john smith  
Tail Entity: 30  

Relation: person age  
Context: the famous actor will be celebrating his 45th birthday soon.  
Head Entity: the famous actor  
Tail Entity: 45  
#### person city of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person city of birth" means a person was born in a city.
Here is an example:
Relation: person city of birth
Context: rothman was born in san francisco in 1932 in an orthodox jewish family .
Head Entity: rothman
Tail Entity: san francisco
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person city of birth":
Relation: person city of birth  
Context: after spending his early years in new york, he moved to los angeles where he began his career.  
Head Entity: he  
Tail Entity: los angeles  

Relation: person city of birth  
Context: the famous author was born in a small town near boston, which greatly influenced his writing.  
Head Entity: the famous author  
Tail Entity: boston  
#### organization members ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization members" means an organization 's member is an organization.
Here is an example:
Relation: organization members
Context: sun plays for the grand rapids flight of the international basketball league after toiling for the maryland nighthawks of the american basketball association , both development leagues for those who dream of an nba career .
Head Entity: american basketball association
Tail Entity: maryland nighthawks
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization members":
Relation: organization members  
Context: The New York Philharmonic is one of the oldest orchestras in the United States, and it has had many notable musicians as members, including the famous conductor Leonard Bernstein.  
Head Entity: New York Philharmonic  
Tail Entity: Leonard Bernstein  

Relation: organization members  
Context: The National Football League has a long history of teams and players, with the Green Bay Packers being one of the most storied franchises, having legendary players like Brett Favre as members.  
Head Entity: National Football League  
Tail Entity: Brett Favre  
#### person religion ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person religion" means an person is the member of a religion.
Here is an example:
Relation: person religion
Context: the pope defended his action on the grounds that he could not refuse an audience to a head of state from a country with a strong catholic tradition unless he had clear-cut proof of the allegations against him .
Head Entity: he
Tail Entity: catholic
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person religion":
Relation: person religion  
Context: During the ceremony, the rabbi spoke about the importance of faith and community in Judaism, emphasizing how each member contributes to the collective spirit.  
Head Entity: rabbi  
Tail Entity: Judaism  

Relation: person religion  
Context: The imam led the prayers at the mosque, reminding the congregation of their duties as followers of Islam and the significance of their beliefs in daily life.  
Head Entity: imam  
Tail Entity: Islam  
Mixup data size:  134
MixupTrain:  epoch  0, batch     0 | loss: 4.1819677MixupTrain:  epoch  0, batch     1 | loss: 4.3765726MixupTrain:  epoch  0, batch     2 | loss: 3.7560018MixupTrain:  epoch  0, batch     3 | loss: 4.1267403MixupTrain:  epoch  0, batch     4 | loss: 4.0838697MixupTrain:  epoch  0, batch     5 | loss: 4.0936982MixupTrain:  epoch  0, batch     6 | loss: 4.1807337MixupTrain:  epoch  0, batch     7 | loss: 4.4254256MixupTrain:  epoch  0, batch     8 | loss: 4.0825062
MemoryTrain:  epoch  0, batch     0 | loss: 3.9067357MemoryTrain:  epoch  0, batch     1 | loss: 4.9369121MemoryTrain:  epoch  0, batch     2 | loss: 4.2265048MemoryTrain:  epoch  1, batch     0 | loss: 4.4727974MemoryTrain:  epoch  1, batch     1 | loss: 4.0766692MemoryTrain:  epoch  1, batch     2 | loss: 3.9304090MemoryTrain:  epoch  2, batch     0 | loss: 4.6368656MemoryTrain:  epoch  2, batch     1 | loss: 3.0019145MemoryTrain:  epoch  2, batch     2 | loss: 3.5609136MemoryTrain:  epoch  3, batch     0 | loss: 3.3382807MemoryTrain:  epoch  3, batch     1 | loss: 3.3799992MemoryTrain:  epoch  3, batch     2 | loss: 3.1094406MemoryTrain:  epoch  4, batch     0 | loss: 2.6483350MemoryTrain:  epoch  4, batch     1 | loss: 3.2086864MemoryTrain:  epoch  4, batch     2 | loss: 3.2318640MemoryTrain:  epoch  5, batch     0 | loss: 2.4944530MemoryTrain:  epoch  5, batch     1 | loss: 2.6825879MemoryTrain:  epoch  5, batch     2 | loss: 3.2707183MemoryTrain:  epoch  6, batch     0 | loss: 2.8295064MemoryTrain:  epoch  6, batch     1 | loss: 2.5651646MemoryTrain:  epoch  6, batch     2 | loss: 2.4464207MemoryTrain:  epoch  7, batch     0 | loss: 2.1627111MemoryTrain:  epoch  7, batch     1 | loss: 2.3622527MemoryTrain:  epoch  7, batch     2 | loss: 2.8827515MemoryTrain:  epoch  8, batch     0 | loss: 2.5654881MemoryTrain:  epoch  8, batch     1 | loss: 2.1537466MemoryTrain:  epoch  8, batch     2 | loss: 2.2302942MemoryTrain:  epoch  9, batch     0 | loss: 2.5247655MemoryTrain:  epoch  9, batch     1 | loss: 1.8669674MemoryTrain:  epoch  9, batch     2 | loss: 2.0376005
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 90.62%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:    3 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:    4 | acc: 100.00%,  total acc: 95.00%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 95.83%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 96.43%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 96.88%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 95.83%   [EVAL] batch:    9 | acc: 6.25%,  total acc: 86.88%   [EVAL] batch:   10 | acc: 12.50%,  total acc: 80.11%   [EVAL] batch:   11 | acc: 62.50%,  total acc: 78.65%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 78.37%   [EVAL] batch:   13 | acc: 50.00%,  total acc: 76.34%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 82.50%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 83.04%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 85.16%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 86.81%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 86.88%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 88.07%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 88.02%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:   13 | acc: 56.25%,  total acc: 85.27%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 84.58%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 82.81%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 82.35%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 81.25%   [EVAL] batch:   18 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 81.88%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 82.74%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 83.52%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 84.24%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 84.90%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 85.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 86.06%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 86.34%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 86.83%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 87.28%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 87.29%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 87.70%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 88.09%   [EVAL] batch:   32 | acc: 62.50%,  total acc: 87.31%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 87.68%   [EVAL] batch:   34 | acc: 37.50%,  total acc: 86.25%   [EVAL] batch:   35 | acc: 12.50%,  total acc: 84.20%   [EVAL] batch:   36 | acc: 0.00%,  total acc: 81.93%   [EVAL] batch:   37 | acc: 0.00%,  total acc: 79.77%   [EVAL] batch:   38 | acc: 6.25%,  total acc: 77.88%   [EVAL] batch:   39 | acc: 50.00%,  total acc: 77.19%   [EVAL] batch:   40 | acc: 87.50%,  total acc: 77.44%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 77.68%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 78.20%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 78.69%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 79.17%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 79.62%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 80.05%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 80.47%   [EVAL] batch:   48 | acc: 37.50%,  total acc: 79.59%   [EVAL] batch:   49 | acc: 6.25%,  total acc: 78.12%   [EVAL] batch:   50 | acc: 37.50%,  total acc: 77.33%   [EVAL] batch:   51 | acc: 75.00%,  total acc: 77.28%   [EVAL] batch:   52 | acc: 68.75%,  total acc: 77.12%   [EVAL] batch:   53 | acc: 18.75%,  total acc: 76.04%   
cur_acc:  ['0.8674', '0.3438', '0.7634']
his_acc:  ['0.8674', '0.6734', '0.7604']
CurrentTrain: epoch  0, batch     0 | loss: 5.5854945CurrentTrain: epoch  0, batch     1 | loss: 5.5685792CurrentTrain: epoch  1, batch     0 | loss: 5.1294298CurrentTrain: epoch  1, batch     1 | loss: 4.4333444CurrentTrain: epoch  2, batch     0 | loss: 4.2101135CurrentTrain: epoch  2, batch     1 | loss: 4.2498722CurrentTrain: epoch  3, batch     0 | loss: 3.7833204CurrentTrain: epoch  3, batch     1 | loss: 3.7902958CurrentTrain: epoch  4, batch     0 | loss: 3.3951259CurrentTrain: epoch  4, batch     1 | loss: 3.3890703CurrentTrain: epoch  5, batch     0 | loss: 3.3721685CurrentTrain: epoch  5, batch     1 | loss: 3.1164284CurrentTrain: epoch  6, batch     0 | loss: 3.2026448CurrentTrain: epoch  6, batch     1 | loss: 2.7964711CurrentTrain: epoch  7, batch     0 | loss: 3.0738893CurrentTrain: epoch  7, batch     1 | loss: 2.5868204CurrentTrain: epoch  8, batch     0 | loss: 2.5114310CurrentTrain: epoch  8, batch     1 | loss: 2.4914768CurrentTrain: epoch  9, batch     0 | loss: 2.3214107CurrentTrain: epoch  9, batch     1 | loss: 2.4066744
#### person cities of residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person cities of residence" means a person is living in a city.
Here is an example:
Relation: person cities of residence
Context: -lrb- now there are two : andre carson , a democrat from indianapolis , was elected to the house last year . -rrb-
Head Entity: andre carson
Tail Entity: indianapolis
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person cities of residence":
Relation: person cities of residence  
Context: -lrb- after moving to the west coast, sarah jones settled in san francisco, where she found a job in tech. -rrb-  
Head Entity: sarah jones  
Tail Entity: san francisco  

Relation: person cities of residence  
Context: -lrb- during his college years, michael smith lived in boston, enjoying the vibrant culture and history of the city. -rrb-  
Head Entity: michael smith  
Tail Entity: boston  
#### person schools attended ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person schools attended" means a person 's school is an organization.
Here is an example:
Relation: person schools attended
Context: after world war ii , he attended the university of southern california , where he became editor of a college magazine .
Head Entity: he
Tail Entity: university of southern california
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person schools attended":
Relation: person schools attended  
Context: She graduated from Harvard University with a degree in psychology before pursuing her career in clinical research.  
Head Entity: She  
Tail Entity: Harvard University  

Relation: person schools attended  
Context: After completing his high school education, he enrolled at Stanford University to study computer science.  
Head Entity: he  
Tail Entity: Stanford University  
#### person country of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person country of death" means a person was died in a country.
Here is an example:
Relation: person country of death
Context: u.s. rep. parren mitchell , founding member of congressional black caucus , dies at 85
Head Entity: parren mitchell
Tail Entity: u.s.
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person country of death":
Relation: person country of death  
Context: renowned physicist stephen hawking passed away in cambridge, england at the age of 76  
Head Entity: stephen hawking  
Tail Entity: england  

Relation: person country of death  
Context: famous author gabriel garcia marquez died in mexico city, mexico, leaving behind a legacy of magical realism  
Head Entity: gabriel garcia marquez  
Tail Entity: mexico  
#### person children ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person children" means a person 's children is a person.
Here is an example:
Relation: person children
Context: he is survived by two stepdaughters , barbara a. hammond and brenda l. stevenson ; a stepson , michael a. taylor ; two grandchildren and one great-grandchild .
Head Entity: he
Tail Entity: brenda l. stevenson
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person children":
Relation: person children  
Context: she has three children, including her son, john, and her daughters, emily and sarah, who are both pursuing their studies.  
Head Entity: she  
Tail Entity: emily  

Relation: person children  
Context: after the passing of their father, the siblings, including jake and his sister, lila, took on the responsibility of caring for their younger brother, max.  
Head Entity: jake  
Tail Entity: lila  
#### person charges ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person charges" means a person was charged with an event.
Here is an example:
Relation: person charges
Context: flowers always contended politics was behind the extortion investigation , but appeals courts ruled against him .
Head Entity: him
Tail Entity: extortion
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person charges":
Relation: person charges  
Context: The prosecutor announced that the former mayor was facing serious allegations related to corruption.  
Head Entity: former mayor  
Tail Entity: corruption  

Relation: person charges  
Context: After a lengthy investigation, the authorities confirmed that the celebrity was implicated in a major drug trafficking case.  
Head Entity: celebrity  
Tail Entity: drug trafficking  
Mixup data size:  164
MixupTrain:  epoch  0, batch     0 | loss: 3.2727841MixupTrain:  epoch  0, batch     1 | loss: 2.9606971MixupTrain:  epoch  0, batch     2 | loss: 2.8956044MixupTrain:  epoch  0, batch     3 | loss: 2.7806653MixupTrain:  epoch  0, batch     4 | loss: 3.3615904MixupTrain:  epoch  0, batch     5 | loss: 2.8275947MixupTrain:  epoch  0, batch     6 | loss: 2.7973152MixupTrain:  epoch  0, batch     7 | loss: 3.2232103MixupTrain:  epoch  0, batch     8 | loss: 2.8011106MixupTrain:  epoch  0, batch     9 | loss: 2.8707314MixupTrain:  epoch  0, batch    10 | loss: 2.9648590
MemoryTrain:  epoch  0, batch     0 | loss: 2.7088923MemoryTrain:  epoch  0, batch     1 | loss: 3.1305804MemoryTrain:  epoch  0, batch     2 | loss: 3.9893849MemoryTrain:  epoch  0, batch     3 | loss: 2.8685553MemoryTrain:  epoch  1, batch     0 | loss: 3.4890132MemoryTrain:  epoch  1, batch     1 | loss: 2.3140578MemoryTrain:  epoch  1, batch     2 | loss: 2.6066046MemoryTrain:  epoch  1, batch     3 | loss: 2.9091709MemoryTrain:  epoch  2, batch     0 | loss: 2.4602797MemoryTrain:  epoch  2, batch     1 | loss: 2.4307408MemoryTrain:  epoch  2, batch     2 | loss: 2.6644816MemoryTrain:  epoch  2, batch     3 | loss: 2.5362058MemoryTrain:  epoch  3, batch     0 | loss: 2.4884782MemoryTrain:  epoch  3, batch     1 | loss: 1.8925905MemoryTrain:  epoch  3, batch     2 | loss: 2.3285131MemoryTrain:  epoch  3, batch     3 | loss: 2.7467933MemoryTrain:  epoch  4, batch     0 | loss: 2.7057106MemoryTrain:  epoch  4, batch     1 | loss: 2.2486341MemoryTrain:  epoch  4, batch     2 | loss: 1.8058952MemoryTrain:  epoch  4, batch     3 | loss: 1.8359121MemoryTrain:  epoch  5, batch     0 | loss: 2.0968733MemoryTrain:  epoch  5, batch     1 | loss: 2.1200607MemoryTrain:  epoch  5, batch     2 | loss: 1.8776088MemoryTrain:  epoch  5, batch     3 | loss: 1.9891268MemoryTrain:  epoch  6, batch     0 | loss: 1.8864914MemoryTrain:  epoch  6, batch     1 | loss: 1.9912242MemoryTrain:  epoch  6, batch     2 | loss: 1.8132327MemoryTrain:  epoch  6, batch     3 | loss: 1.9026911MemoryTrain:  epoch  7, batch     0 | loss: 1.4632373MemoryTrain:  epoch  7, batch     1 | loss: 2.1280470MemoryTrain:  epoch  7, batch     2 | loss: 1.6398232MemoryTrain:  epoch  7, batch     3 | loss: 1.8223885MemoryTrain:  epoch  8, batch     0 | loss: 1.4906490MemoryTrain:  epoch  8, batch     1 | loss: 1.5550634MemoryTrain:  epoch  8, batch     2 | loss: 2.0268984MemoryTrain:  epoch  8, batch     3 | loss: 1.9465461MemoryTrain:  epoch  9, batch     0 | loss: 1.7910718MemoryTrain:  epoch  9, batch     1 | loss: 1.4341757MemoryTrain:  epoch  9, batch     2 | loss: 1.7900972MemoryTrain:  epoch  9, batch     3 | loss: 1.5537041
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 85.94%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 85.00%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 83.33%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 85.71%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 85.94%   [EVAL] batch:    8 | acc: 43.75%,  total acc: 81.25%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 83.12%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 83.52%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 84.90%   [EVAL] batch:   12 | acc: 100.00%,  total acc: 86.06%   [EVAL] batch:   13 | acc: 100.00%,  total acc: 87.05%   [EVAL] batch:   14 | acc: 100.00%,  total acc: 87.92%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 88.67%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 89.34%   [EVAL] batch:   17 | acc: 25.00%,  total acc: 85.76%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 75.00%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 79.17%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 80.00%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 79.17%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 83.59%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 84.03%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 85.23%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 85.42%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 85.10%   [EVAL] batch:   13 | acc: 62.50%,  total acc: 83.48%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 82.92%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 81.25%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 80.88%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 79.86%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 78.95%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 79.06%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 80.06%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 80.97%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 81.79%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 82.29%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 83.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 83.65%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 84.03%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 84.60%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 85.13%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 85.21%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 85.69%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 85.94%   [EVAL] batch:   32 | acc: 62.50%,  total acc: 85.23%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 85.66%   [EVAL] batch:   34 | acc: 43.75%,  total acc: 84.46%   [EVAL] batch:   35 | acc: 18.75%,  total acc: 82.64%   [EVAL] batch:   36 | acc: 0.00%,  total acc: 80.41%   [EVAL] batch:   37 | acc: 6.25%,  total acc: 78.45%   [EVAL] batch:   38 | acc: 6.25%,  total acc: 76.60%   [EVAL] batch:   39 | acc: 50.00%,  total acc: 75.94%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 76.37%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 76.93%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 77.47%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 77.98%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 78.47%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 78.94%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 79.39%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 79.82%   [EVAL] batch:   48 | acc: 12.50%,  total acc: 78.44%   [EVAL] batch:   49 | acc: 6.25%,  total acc: 77.00%   [EVAL] batch:   50 | acc: 56.25%,  total acc: 76.59%   [EVAL] batch:   51 | acc: 75.00%,  total acc: 76.56%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 76.89%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 76.97%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 77.27%   [EVAL] batch:   55 | acc: 81.25%,  total acc: 77.34%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 77.63%   [EVAL] batch:   57 | acc: 75.00%,  total acc: 77.59%   [EVAL] batch:   58 | acc: 81.25%,  total acc: 77.65%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 77.92%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 78.28%   [EVAL] batch:   61 | acc: 31.25%,  total acc: 77.52%   [EVAL] batch:   62 | acc: 100.00%,  total acc: 77.88%   [EVAL] batch:   63 | acc: 87.50%,  total acc: 78.03%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 78.37%   [EVAL] batch:   65 | acc: 100.00%,  total acc: 78.69%   [EVAL] batch:   66 | acc: 100.00%,  total acc: 79.01%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 79.32%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 79.62%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 79.91%   [EVAL] batch:   70 | acc: 50.00%,  total acc: 79.49%   
cur_acc:  ['0.8674', '0.3438', '0.7634', '0.8576']
his_acc:  ['0.8674', '0.6734', '0.7604', '0.7949']
CurrentTrain: epoch  0, batch     0 | loss: 4.1917181CurrentTrain: epoch  0, batch     1 | loss: 5.3485632CurrentTrain: epoch  1, batch     0 | loss: 3.5787024CurrentTrain: epoch  1, batch     1 | loss: 4.5030580CurrentTrain: epoch  2, batch     0 | loss: 3.5178840CurrentTrain: epoch  2, batch     1 | loss: 3.6525438CurrentTrain: epoch  3, batch     0 | loss: 2.8674407CurrentTrain: epoch  3, batch     1 | loss: 2.4870317CurrentTrain: epoch  4, batch     0 | loss: 2.5746903CurrentTrain: epoch  4, batch     1 | loss: 2.5762103CurrentTrain: epoch  5, batch     0 | loss: 2.4895151CurrentTrain: epoch  5, batch     1 | loss: 2.3946843CurrentTrain: epoch  6, batch     0 | loss: 2.3372560CurrentTrain: epoch  6, batch     1 | loss: 2.2295048CurrentTrain: epoch  7, batch     0 | loss: 2.3124685CurrentTrain: epoch  7, batch     1 | loss: 1.9671775CurrentTrain: epoch  8, batch     0 | loss: 2.1747572CurrentTrain: epoch  8, batch     1 | loss: 2.0328777CurrentTrain: epoch  9, batch     0 | loss: 1.9435182CurrentTrain: epoch  9, batch     1 | loss: 2.2024033
#### person cause of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person cause of death" means a person was died of an event.
Here is an example:
Relation: person cause of death
Context: mr scheider had suffered from multiple myeloma for several years , and died of complications from a staph infection , his wife , brenda siemer , said .
Head Entity: mr scheider
Tail Entity: complications from a staph infection
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person cause of death":
Relation: person cause of death  
Context: after battling with lung cancer for over a year, john doe passed away peacefully in his sleep, surrounded by family.  
Head Entity: john doe  
Tail Entity: lung cancer  

Relation: person cause of death  
Context: the renowned scientist, dr. emily white, tragically lost her life due to a car accident while returning from a conference.  
Head Entity: dr. emily white  
Tail Entity: car accident  
#### organization political religious affiliation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization political religious affiliation" means an organization is the member of a religion.
Here is an example:
Relation: organization political religious affiliation
Context: the militia has also suffered from allegations of being involved in clashes with rival shiite militias , particularly the badr corp , the military wing linked to supreme iraqi islamic council , the party led by powerful politician abdel aziz al-hakim .
Head Entity: badr corp
Tail Entity: shiite
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization political religious affiliation":
Relation: organization political religious affiliation  
Context: The Christian Democratic Union has been a significant player in German politics, often aligning itself with various religious groups to promote policies that reflect its Christian values.  
Head Entity: Christian Democratic Union  
Tail Entity: Christian  

Relation: organization political religious affiliation  
Context: The Islamic Society of North America has been actively involved in advocating for Muslim rights and representation in the political landscape of the United States.  
Head Entity: Islamic Society of North America  
Tail Entity: Muslim  
#### organization stateorprovince of headquarters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization stateorprovince of headquarters" means an organization is located in a state or province.
Here is an example:
Relation: organization stateorprovince of headquarters
Context: based in armonk , new york , mbia insures $ 670 billion -lrb- euro452 .18 billion -rrb- in debt .
Head Entity: mbia
Tail Entity: new york
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization stateorprovince of headquarters":
Relation: organization stateorprovince of headquarters  
Context: the tech giant apple inc. has its headquarters in cupertino, california, where it develops innovative products.  
Head Entity: apple inc.  
Tail Entity: california  

Relation: organization stateorprovince of headquarters  
Context: the multinational corporation samsung electronics is headquartered in suwon, south korea, and is a leader in consumer electronics.  
Head Entity: samsung electronics  
Tail Entity: south korea  
#### person other family ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person other family" means a person 's relative is a person.
Here is an example:
Relation: person other family
Context: parren mitchell 's sister-in-law , juanita jackson mitchell , was the long - time head and legal counsel of the maryland naacp .
Head Entity: parren mitchell
Tail Entity: juanita jackson mitchell
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person other family":
Relation: person other family  
Context: During the family reunion, it was heartwarming to see how close cousins, Sarah and Emily, have become over the years.  
Head Entity: Sarah  
Tail Entity: Emily  

Relation: person other family  
Context: After the passing of their grandmother, the siblings, Mark and Lisa, took it upon themselves to organize the family estate and keep the traditions alive.  
Head Entity: Mark  
Tail Entity: Lisa  
#### person city of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person city of death" means a person was died in a city.
Here is an example:
Relation: person city of death
Context: verity died wednesday , jan 3 , 2007 , in beaufort memorial hospital .
Head Entity: verity
Tail Entity: beaufort
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person city of death":
Relation: person city of death  
Context: after a long battle with illness, john passed away in the quiet town of springfield.  
Head Entity: john  
Tail Entity: springfield  

Relation: person city of death  
Context: the renowned author tragically died in a car accident on the streets of los angeles.  
Head Entity: the renowned author  
Tail Entity: los angeles  
Mixup data size:  195
MixupTrain:  epoch  0, batch     0 | loss: 3.3463218MixupTrain:  epoch  0, batch     1 | loss: 3.3168171MixupTrain:  epoch  0, batch     2 | loss: 3.2281137MixupTrain:  epoch  0, batch     3 | loss: 3.1325160MixupTrain:  epoch  0, batch     4 | loss: 3.1630377MixupTrain:  epoch  0, batch     5 | loss: 3.2234445MixupTrain:  epoch  0, batch     6 | loss: 3.0864214MixupTrain:  epoch  0, batch     7 | loss: 3.1289620MixupTrain:  epoch  0, batch     8 | loss: 2.9295056MixupTrain:  epoch  0, batch     9 | loss: 2.9089217MixupTrain:  epoch  0, batch    10 | loss: 3.1544641MixupTrain:  epoch  0, batch    11 | loss: 2.6150433MixupTrain:  epoch  0, batch    12 | loss: 3.1998210
MemoryTrain:  epoch  0, batch     0 | loss: 2.9488916MemoryTrain:  epoch  0, batch     1 | loss: 2.5628905MemoryTrain:  epoch  0, batch     2 | loss: 3.6743927MemoryTrain:  epoch  0, batch     3 | loss: 3.4839771MemoryTrain:  epoch  0, batch     4 | loss: 3.1095195MemoryTrain:  epoch  1, batch     0 | loss: 3.0247436MemoryTrain:  epoch  1, batch     1 | loss: 3.8412113MemoryTrain:  epoch  1, batch     2 | loss: 2.2590094MemoryTrain:  epoch  1, batch     3 | loss: 2.8172455MemoryTrain:  epoch  1, batch     4 | loss: 3.5306692MemoryTrain:  epoch  2, batch     0 | loss: 3.2838745MemoryTrain:  epoch  2, batch     1 | loss: 2.2747333MemoryTrain:  epoch  2, batch     2 | loss: 2.7298279MemoryTrain:  epoch  2, batch     3 | loss: 2.8474107MemoryTrain:  epoch  2, batch     4 | loss: 2.8598988MemoryTrain:  epoch  3, batch     0 | loss: 2.9799452MemoryTrain:  epoch  3, batch     1 | loss: 2.1919737MemoryTrain:  epoch  3, batch     2 | loss: 2.6738966MemoryTrain:  epoch  3, batch     3 | loss: 2.2033951MemoryTrain:  epoch  3, batch     4 | loss: 1.7068764MemoryTrain:  epoch  4, batch     0 | loss: 2.0294256MemoryTrain:  epoch  4, batch     1 | loss: 2.1573820MemoryTrain:  epoch  4, batch     2 | loss: 2.7401047MemoryTrain:  epoch  4, batch     3 | loss: 1.8993778MemoryTrain:  epoch  4, batch     4 | loss: 1.7895238MemoryTrain:  epoch  5, batch     0 | loss: 1.7117853MemoryTrain:  epoch  5, batch     1 | loss: 2.0582864MemoryTrain:  epoch  5, batch     2 | loss: 1.8774333MemoryTrain:  epoch  5, batch     3 | loss: 1.8321779MemoryTrain:  epoch  5, batch     4 | loss: 2.3118970MemoryTrain:  epoch  6, batch     0 | loss: 1.8374883MemoryTrain:  epoch  6, batch     1 | loss: 1.7563647MemoryTrain:  epoch  6, batch     2 | loss: 1.8782749MemoryTrain:  epoch  6, batch     3 | loss: 1.7874897MemoryTrain:  epoch  6, batch     4 | loss: 1.9410154MemoryTrain:  epoch  7, batch     0 | loss: 1.4675720MemoryTrain:  epoch  7, batch     1 | loss: 2.0597010MemoryTrain:  epoch  7, batch     2 | loss: 1.9416382MemoryTrain:  epoch  7, batch     3 | loss: 1.6693200MemoryTrain:  epoch  7, batch     4 | loss: 1.4311213MemoryTrain:  epoch  8, batch     0 | loss: 1.6102979MemoryTrain:  epoch  8, batch     1 | loss: 1.4923450MemoryTrain:  epoch  8, batch     2 | loss: 1.7273049MemoryTrain:  epoch  8, batch     3 | loss: 1.7598917MemoryTrain:  epoch  8, batch     4 | loss: 1.6809365MemoryTrain:  epoch  9, batch     0 | loss: 1.5430336MemoryTrain:  epoch  9, batch     1 | loss: 1.4938948MemoryTrain:  epoch  9, batch     2 | loss: 1.8034270MemoryTrain:  epoch  9, batch     3 | loss: 1.5724649MemoryTrain:  epoch  9, batch     4 | loss: 1.4871775
[EVAL] batch:    0 | acc: 18.75%,  total acc: 18.75%   [EVAL] batch:    1 | acc: 62.50%,  total acc: 40.62%   [EVAL] batch:    2 | acc: 37.50%,  total acc: 39.58%   [EVAL] batch:    3 | acc: 37.50%,  total acc: 39.06%   [EVAL] batch:    4 | acc: 37.50%,  total acc: 38.75%   [EVAL] batch:    5 | acc: 25.00%,  total acc: 36.46%   [EVAL] batch:    6 | acc: 62.50%,  total acc: 40.18%   [EVAL] batch:    7 | acc: 62.50%,  total acc: 42.97%   [EVAL] batch:    8 | acc: 31.25%,  total acc: 41.67%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 43.75%   [EVAL] batch:   10 | acc: 68.75%,  total acc: 46.02%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 49.48%   [EVAL] batch:   12 | acc: 50.00%,  total acc: 49.52%   
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 79.17%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 79.69%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 78.75%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 78.12%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 80.36%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 82.81%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 83.75%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 84.66%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 84.90%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 83.65%   [EVAL] batch:   13 | acc: 50.00%,  total acc: 81.25%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 80.42%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 78.91%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 78.68%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 77.78%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 76.97%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 77.19%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 78.27%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 79.26%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 80.16%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 80.73%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 81.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 82.21%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 82.64%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 83.26%   [EVAL] batch:   28 | acc: 87.50%,  total acc: 83.41%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 83.33%   [EVAL] batch:   30 | acc: 81.25%,  total acc: 83.27%   [EVAL] batch:   31 | acc: 87.50%,  total acc: 83.40%   [EVAL] batch:   32 | acc: 62.50%,  total acc: 82.77%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 83.27%   [EVAL] batch:   34 | acc: 56.25%,  total acc: 82.50%   [EVAL] batch:   35 | acc: 25.00%,  total acc: 80.90%   [EVAL] batch:   36 | acc: 6.25%,  total acc: 78.89%   [EVAL] batch:   37 | acc: 6.25%,  total acc: 76.97%   [EVAL] batch:   38 | acc: 6.25%,  total acc: 75.16%   [EVAL] batch:   39 | acc: 50.00%,  total acc: 74.53%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 75.00%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 75.60%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 76.16%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 76.70%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 77.22%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 77.72%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 78.19%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 78.65%   [EVAL] batch:   48 | acc: 12.50%,  total acc: 77.30%   [EVAL] batch:   49 | acc: 6.25%,  total acc: 75.88%   [EVAL] batch:   50 | acc: 31.25%,  total acc: 75.00%   [EVAL] batch:   51 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:   52 | acc: 68.75%,  total acc: 74.88%   [EVAL] batch:   53 | acc: 25.00%,  total acc: 73.96%   [EVAL] batch:   54 | acc: 6.25%,  total acc: 72.73%   [EVAL] batch:   55 | acc: 18.75%,  total acc: 71.76%   [EVAL] batch:   56 | acc: 18.75%,  total acc: 70.83%   [EVAL] batch:   57 | acc: 0.00%,  total acc: 69.61%   [EVAL] batch:   58 | acc: 0.00%,  total acc: 68.43%   [EVAL] batch:   59 | acc: 43.75%,  total acc: 68.02%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 68.55%   [EVAL] batch:   61 | acc: 31.25%,  total acc: 67.94%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 67.66%   [EVAL] batch:   63 | acc: 56.25%,  total acc: 67.48%   [EVAL] batch:   64 | acc: 81.25%,  total acc: 67.69%   [EVAL] batch:   65 | acc: 100.00%,  total acc: 68.18%   [EVAL] batch:   66 | acc: 100.00%,  total acc: 68.66%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 69.12%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 69.57%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 70.00%   [EVAL] batch:   70 | acc: 56.25%,  total acc: 69.81%   [EVAL] batch:   71 | acc: 43.75%,  total acc: 69.44%   [EVAL] batch:   72 | acc: 50.00%,  total acc: 69.18%   [EVAL] batch:   73 | acc: 31.25%,  total acc: 68.67%   [EVAL] batch:   74 | acc: 37.50%,  total acc: 68.25%   [EVAL] batch:   75 | acc: 37.50%,  total acc: 67.85%   [EVAL] batch:   76 | acc: 43.75%,  total acc: 67.53%   [EVAL] batch:   77 | acc: 62.50%,  total acc: 67.47%   [EVAL] batch:   78 | acc: 50.00%,  total acc: 67.25%   [EVAL] batch:   79 | acc: 43.75%,  total acc: 66.95%   [EVAL] batch:   80 | acc: 62.50%,  total acc: 66.90%   [EVAL] batch:   81 | acc: 87.50%,  total acc: 67.15%   [EVAL] batch:   82 | acc: 81.25%,  total acc: 67.32%   [EVAL] batch:   83 | acc: 6.25%,  total acc: 66.59%   
cur_acc:  ['0.8674', '0.3438', '0.7634', '0.8576', '0.4952']
his_acc:  ['0.8674', '0.6734', '0.7604', '0.7949', '0.6659']
CurrentTrain: epoch  0, batch     0 | loss: 5.8557391CurrentTrain: epoch  0, batch     1 | loss: 6.5537162CurrentTrain: epoch  1, batch     0 | loss: 5.2249060CurrentTrain: epoch  1, batch     1 | loss: 5.0775170CurrentTrain: epoch  2, batch     0 | loss: 4.5002460CurrentTrain: epoch  2, batch     1 | loss: 4.8362064CurrentTrain: epoch  3, batch     0 | loss: 3.9137270CurrentTrain: epoch  3, batch     1 | loss: 3.4475446CurrentTrain: epoch  4, batch     0 | loss: 3.6404243CurrentTrain: epoch  4, batch     1 | loss: 3.1797152CurrentTrain: epoch  5, batch     0 | loss: 3.3218212CurrentTrain: epoch  5, batch     1 | loss: 3.3144627CurrentTrain: epoch  6, batch     0 | loss: 3.1025186CurrentTrain: epoch  6, batch     1 | loss: 2.8803291CurrentTrain: epoch  7, batch     0 | loss: 2.8508277CurrentTrain: epoch  7, batch     1 | loss: 2.6641991CurrentTrain: epoch  8, batch     0 | loss: 2.7419107CurrentTrain: epoch  8, batch     1 | loss: 2.4923942CurrentTrain: epoch  9, batch     0 | loss: 2.5491321CurrentTrain: epoch  9, batch     1 | loss: 2.5848505
#### person date of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person date of birth" means a person was born in a date.
Here is an example:
Relation: person date of birth
Context: born of schoolteacher parents in the western town of sabaneta on july 28 , 1954 , chavez studied at the military academy of venezuela in caracas .
Head Entity: chavez
Tail Entity: july 28 , 1954
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person date of birth":
Relation: person date of birth  
Context: the famous physicist albert einstein was born in ulm, in the kingdom of wurttemberg in the german empire on march 14, 1879.  
Head Entity: albert einstein  
Tail Entity: march 14, 1879  

Relation: person date of birth  
Context: the renowned author mark twain, known for his classic novels, was born on november 30, 1835, in florida, missouri.  
Head Entity: mark twain  
Tail Entity: november 30, 1835  
#### person stateorprovince of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person stateorprovince of birth" means a person was died in a state or province.
Here is an example:
Relation: person stateorprovince of birth
Context: jane matilda bolin was born on april 11 , 1908 , in poughkeepsie , ny .
Head Entity: jane matilda bolin
Tail Entity: ny
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person stateorprovince of birth":
Relation: person stateorprovince of birth  
Context: albert einstein was born on march 14, 1879, in ulm, germany.  
Head Entity: albert einstein  
Tail Entity: germany  

Relation: person stateorprovince of birth  
Context: marilyn monroe was born on june 1, 1926, in los angeles, california.  
Head Entity: marilyn monroe  
Tail Entity: california  
#### person parents ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person parents" means a person 's parent is a person.
Here is an example:
Relation: person parents
Context: as the case developed , sandy 's mother , denise sandy , quietly made herself a spectral but central figure , by faithfully attending pretrial hearings .
Head Entity: sandy
Tail Entity: denise sandy
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person parents":
Relation: person parents  
Context: During the family reunion, it was heartwarming to see how much Sarah resembled her father, John, in both looks and mannerisms.  
Head Entity: Sarah  
Tail Entity: John  

Relation: person parents  
Context: After the ceremony, Emily shared stories about her childhood and how her mother, Lisa, always encouraged her to pursue her dreams.  
Head Entity: Emily  
Tail Entity: Lisa  
#### person employee of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person employee of" means a person is an employee of an organization.
Here is an example:
Relation: person employee of
Context: kell hath no fury : publicist and mtv reality star kelly cutrone is wasting no time in kicking her brands -lrb- including her p.r. firm people 's revolution and , increasingly , kelly cutrone herself -rrb- into high gear in 2010 .
Head Entity: kelly cutrone
Tail Entity: mtv
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person employee of":
Relation: person employee of  
Context: After years of hard work and dedication, Sarah Thompson has finally landed a position at one of the top tech companies in Silicon Valley, where she will be contributing to innovative projects.  
Head Entity: Sarah Thompson  
Tail Entity: tech companies  

Relation: person employee of  
Context: John Smith, a talented graphic designer, has been working for Creative Solutions for over five years, helping to shape the visual identity of numerous brands.  
Head Entity: John Smith  
Tail Entity: Creative Solutions  
#### person stateorprovince of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person stateorprovince of death" means a person was died in a state or province.
Here is an example:
Relation: person stateorprovince of death
Context: irene morgan kirkaldy , whose defiance of bus segregation laws more than a decade before rosa parks ' landmark case helped lay the foundation for later civil rights victories , died friday at her home in hayes , va. .
Head Entity: irene morgan kirkaldy
Tail Entity: va.
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person stateorprovince of death":
Relation: person stateorprovince of death  
Context: john doe, a renowned author known for his impactful novels, passed away peacefully in his sleep at his residence in california last night.  
Head Entity: john doe  
Tail Entity: california  

Relation: person stateorprovince of death  
Context: after a long battle with illness, mary jane, a beloved community leader, succumbed to her condition in a hospital located in new york city.  
Head Entity: mary jane  
Tail Entity: new york city  
Mixup data size:  224
MixupTrain:  epoch  0, batch     0 | loss: 2.4107771MixupTrain:  epoch  0, batch     1 | loss: 2.6386999MixupTrain:  epoch  0, batch     2 | loss: 2.6687834MixupTrain:  epoch  0, batch     3 | loss: 3.1593442MixupTrain:  epoch  0, batch     4 | loss: 2.9232812MixupTrain:  epoch  0, batch     5 | loss: 2.6079859MixupTrain:  epoch  0, batch     6 | loss: 2.2296213MixupTrain:  epoch  0, batch     7 | loss: 2.6895812MixupTrain:  epoch  0, batch     8 | loss: 2.7528051MixupTrain:  epoch  0, batch     9 | loss: 2.5033542MixupTrain:  epoch  0, batch    10 | loss: 2.4262977MixupTrain:  epoch  0, batch    11 | loss: 2.1118157MixupTrain:  epoch  0, batch    12 | loss: 2.8363989MixupTrain:  epoch  0, batch    13 | loss: 2.7525596
MemoryTrain:  epoch  0, batch     0 | loss: 2.4463673MemoryTrain:  epoch  0, batch     1 | loss: 2.1614957MemoryTrain:  epoch  0, batch     2 | loss: 2.1128545MemoryTrain:  epoch  0, batch     3 | loss: 2.5642974MemoryTrain:  epoch  0, batch     4 | loss: 2.9592838MemoryTrain:  epoch  0, batch     5 | loss: 2.3010206MemoryTrain:  epoch  1, batch     0 | loss: 2.5837045MemoryTrain:  epoch  1, batch     1 | loss: 2.0773354MemoryTrain:  epoch  1, batch     2 | loss: 1.6357772MemoryTrain:  epoch  1, batch     3 | loss: 2.4680371MemoryTrain:  epoch  1, batch     4 | loss: 2.3345599MemoryTrain:  epoch  1, batch     5 | loss: 1.6878935MemoryTrain:  epoch  2, batch     0 | loss: 1.6710097MemoryTrain:  epoch  2, batch     1 | loss: 1.7707617MemoryTrain:  epoch  2, batch     2 | loss: 2.4255013MemoryTrain:  epoch  2, batch     3 | loss: 2.0622122MemoryTrain:  epoch  2, batch     4 | loss: 1.9547186MemoryTrain:  epoch  2, batch     5 | loss: 1.9460613MemoryTrain:  epoch  3, batch     0 | loss: 2.3763855MemoryTrain:  epoch  3, batch     1 | loss: 1.8609713MemoryTrain:  epoch  3, batch     2 | loss: 1.7629733MemoryTrain:  epoch  3, batch     3 | loss: 1.8213633MemoryTrain:  epoch  3, batch     4 | loss: 1.8133527MemoryTrain:  epoch  3, batch     5 | loss: 2.0002358MemoryTrain:  epoch  4, batch     0 | loss: 1.5894980MemoryTrain:  epoch  4, batch     1 | loss: 1.5506153MemoryTrain:  epoch  4, batch     2 | loss: 1.7558722MemoryTrain:  epoch  4, batch     3 | loss: 1.5926955MemoryTrain:  epoch  4, batch     4 | loss: 1.6725965MemoryTrain:  epoch  4, batch     5 | loss: 1.6305304MemoryTrain:  epoch  5, batch     0 | loss: 1.6616609MemoryTrain:  epoch  5, batch     1 | loss: 1.4940686MemoryTrain:  epoch  5, batch     2 | loss: 1.5215511MemoryTrain:  epoch  5, batch     3 | loss: 1.4567792MemoryTrain:  epoch  5, batch     4 | loss: 1.5562741MemoryTrain:  epoch  5, batch     5 | loss: 1.5151051MemoryTrain:  epoch  6, batch     0 | loss: 1.6264009MemoryTrain:  epoch  6, batch     1 | loss: 1.5899819MemoryTrain:  epoch  6, batch     2 | loss: 1.6500977MemoryTrain:  epoch  6, batch     3 | loss: 1.6404982MemoryTrain:  epoch  6, batch     4 | loss: 1.4327772MemoryTrain:  epoch  6, batch     5 | loss: 1.5019815MemoryTrain:  epoch  7, batch     0 | loss: 1.5323294MemoryTrain:  epoch  7, batch     1 | loss: 1.4542449MemoryTrain:  epoch  7, batch     2 | loss: 1.3750460MemoryTrain:  epoch  7, batch     3 | loss: 1.5396588MemoryTrain:  epoch  7, batch     4 | loss: 1.4261203MemoryTrain:  epoch  7, batch     5 | loss: 1.5480731MemoryTrain:  epoch  8, batch     0 | loss: 1.5592363MemoryTrain:  epoch  8, batch     1 | loss: 1.3774791MemoryTrain:  epoch  8, batch     2 | loss: 1.4550159MemoryTrain:  epoch  8, batch     3 | loss: 1.4210863MemoryTrain:  epoch  8, batch     4 | loss: 1.3660185MemoryTrain:  epoch  8, batch     5 | loss: 1.4234755MemoryTrain:  epoch  9, batch     0 | loss: 1.3882384MemoryTrain:  epoch  9, batch     1 | loss: 1.3774432MemoryTrain:  epoch  9, batch     2 | loss: 1.4875611MemoryTrain:  epoch  9, batch     3 | loss: 1.4123000MemoryTrain:  epoch  9, batch     4 | loss: 1.4064015MemoryTrain:  epoch  9, batch     5 | loss: 1.4100473
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    1 | acc: 50.00%,  total acc: 56.25%   [EVAL] batch:    2 | acc: 68.75%,  total acc: 60.42%   [EVAL] batch:    3 | acc: 50.00%,  total acc: 57.81%   [EVAL] batch:    4 | acc: 50.00%,  total acc: 56.25%   [EVAL] batch:    5 | acc: 56.25%,  total acc: 56.25%   [EVAL] batch:    6 | acc: 56.25%,  total acc: 56.25%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 60.16%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 63.89%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 65.62%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 67.05%   [EVAL] batch:   11 | acc: 68.75%,  total acc: 67.19%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 68.75%   [EVAL] batch:   13 | acc: 56.25%,  total acc: 67.86%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    2 | acc: 68.75%,  total acc: 72.92%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 73.44%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 73.75%   [EVAL] batch:    5 | acc: 62.50%,  total acc: 71.88%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 75.00%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 78.12%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 79.17%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 80.00%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 80.68%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 80.29%   [EVAL] batch:   13 | acc: 56.25%,  total acc: 78.57%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 77.92%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 76.56%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 76.47%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 75.69%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 75.33%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 75.62%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 76.79%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 77.56%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 78.53%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 79.17%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 80.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 80.77%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 81.92%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 82.54%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 82.50%   [EVAL] batch:   30 | acc: 75.00%,  total acc: 82.26%   [EVAL] batch:   31 | acc: 87.50%,  total acc: 82.42%   [EVAL] batch:   32 | acc: 62.50%,  total acc: 81.82%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 82.35%   [EVAL] batch:   34 | acc: 56.25%,  total acc: 81.61%   [EVAL] batch:   35 | acc: 31.25%,  total acc: 80.21%   [EVAL] batch:   36 | acc: 6.25%,  total acc: 78.21%   [EVAL] batch:   37 | acc: 6.25%,  total acc: 76.32%   [EVAL] batch:   38 | acc: 6.25%,  total acc: 74.52%   [EVAL] batch:   39 | acc: 50.00%,  total acc: 73.91%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 74.39%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 75.58%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 76.14%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 76.67%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 77.17%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 77.66%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 78.12%   [EVAL] batch:   48 | acc: 12.50%,  total acc: 76.79%   [EVAL] batch:   49 | acc: 0.00%,  total acc: 75.25%   [EVAL] batch:   50 | acc: 31.25%,  total acc: 74.39%   [EVAL] batch:   51 | acc: 56.25%,  total acc: 74.04%   [EVAL] batch:   52 | acc: 43.75%,  total acc: 73.47%   [EVAL] batch:   53 | acc: 25.00%,  total acc: 72.57%   [EVAL] batch:   54 | acc: 6.25%,  total acc: 71.36%   [EVAL] batch:   55 | acc: 18.75%,  total acc: 70.42%   [EVAL] batch:   56 | acc: 6.25%,  total acc: 69.30%   [EVAL] batch:   57 | acc: 6.25%,  total acc: 68.21%   [EVAL] batch:   58 | acc: 6.25%,  total acc: 67.16%   [EVAL] batch:   59 | acc: 37.50%,  total acc: 66.67%   [EVAL] batch:   60 | acc: 68.75%,  total acc: 66.70%   [EVAL] batch:   61 | acc: 25.00%,  total acc: 66.03%   [EVAL] batch:   62 | acc: 43.75%,  total acc: 65.67%   [EVAL] batch:   63 | acc: 37.50%,  total acc: 65.23%   [EVAL] batch:   64 | acc: 81.25%,  total acc: 65.48%   [EVAL] batch:   65 | acc: 100.00%,  total acc: 66.00%   [EVAL] batch:   66 | acc: 100.00%,  total acc: 66.51%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 67.00%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 67.48%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 67.95%   [EVAL] batch:   70 | acc: 56.25%,  total acc: 67.78%   [EVAL] batch:   71 | acc: 37.50%,  total acc: 67.36%   [EVAL] batch:   72 | acc: 50.00%,  total acc: 67.12%   [EVAL] batch:   73 | acc: 31.25%,  total acc: 66.64%   [EVAL] batch:   74 | acc: 37.50%,  total acc: 66.25%   [EVAL] batch:   75 | acc: 43.75%,  total acc: 65.95%   [EVAL] batch:   76 | acc: 50.00%,  total acc: 65.75%   [EVAL] batch:   77 | acc: 50.00%,  total acc: 65.54%   [EVAL] batch:   78 | acc: 37.50%,  total acc: 65.19%   [EVAL] batch:   79 | acc: 31.25%,  total acc: 64.77%   [EVAL] batch:   80 | acc: 31.25%,  total acc: 64.35%   [EVAL] batch:   81 | acc: 43.75%,  total acc: 64.10%   [EVAL] batch:   82 | acc: 25.00%,  total acc: 63.63%   [EVAL] batch:   83 | acc: 56.25%,  total acc: 63.54%   [EVAL] batch:   84 | acc: 56.25%,  total acc: 63.46%   [EVAL] batch:   85 | acc: 68.75%,  total acc: 63.52%   [EVAL] batch:   86 | acc: 50.00%,  total acc: 63.36%   [EVAL] batch:   87 | acc: 43.75%,  total acc: 63.14%   [EVAL] batch:   88 | acc: 62.50%,  total acc: 63.13%   [EVAL] batch:   89 | acc: 56.25%,  total acc: 63.06%   [EVAL] batch:   90 | acc: 81.25%,  total acc: 63.26%   [EVAL] batch:   91 | acc: 93.75%,  total acc: 63.59%   [EVAL] batch:   92 | acc: 81.25%,  total acc: 63.78%   [EVAL] batch:   93 | acc: 87.50%,  total acc: 64.03%   [EVAL] batch:   94 | acc: 68.75%,  total acc: 64.08%   [EVAL] batch:   95 | acc: 81.25%,  total acc: 64.26%   [EVAL] batch:   96 | acc: 62.50%,  total acc: 64.24%   
cur_acc:  ['0.8674', '0.3438', '0.7634', '0.8576', '0.4952', '0.6786']
his_acc:  ['0.8674', '0.6734', '0.7604', '0.7949', '0.6659', '0.6424']
CurrentTrain: epoch  0, batch     0 | loss: 7.7110081CurrentTrain: epoch  0, batch     1 | loss: 8.6735668CurrentTrain: epoch  1, batch     0 | loss: 7.3014827CurrentTrain: epoch  1, batch     1 | loss: 7.3387885CurrentTrain: epoch  2, batch     0 | loss: 6.8756180CurrentTrain: epoch  2, batch     1 | loss: 6.6489649CurrentTrain: epoch  3, batch     0 | loss: 6.6394186CurrentTrain: epoch  3, batch     1 | loss: 5.4154053CurrentTrain: epoch  4, batch     0 | loss: 5.9397564CurrentTrain: epoch  4, batch     1 | loss: 5.4730792CurrentTrain: epoch  5, batch     0 | loss: 5.5849385CurrentTrain: epoch  5, batch     1 | loss: 5.3026905CurrentTrain: epoch  6, batch     0 | loss: 5.4782763CurrentTrain: epoch  6, batch     1 | loss: 4.6258416CurrentTrain: epoch  7, batch     0 | loss: 4.4333448CurrentTrain: epoch  7, batch     1 | loss: 5.0875711CurrentTrain: epoch  8, batch     0 | loss: 4.6596251CurrentTrain: epoch  8, batch     1 | loss: 4.2736888CurrentTrain: epoch  9, batch     0 | loss: 4.4124265CurrentTrain: epoch  9, batch     1 | loss: 3.9894478
#### organization subsidiaries ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization subsidiaries" means an organization 's subsidiary is an organization.
Here is an example:
Relation: organization subsidiaries
Context: venture fund buys sporting chain highland capital 's consumer fund includes lululemon athletica , a yoga retailer , and o beverages , a flavored water company developed by tom first , one of the two `` juice guys '' who cofounded nantucket nectars .
Head Entity: highland capital
Tail Entity: o beverages
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization subsidiaries":
Relation: organization subsidiaries  
Context: The tech giant Alphabet Inc. has expanded its portfolio by acquiring Fitbit, a company known for its fitness tracking devices, which will operate as a subsidiary under the Google brand.  
Head Entity: Alphabet Inc.  
Tail Entity: Fitbit  

Relation: organization subsidiaries  
Context: The automotive manufacturer General Motors announced that it will be restructuring its operations, which includes the integration of its subsidiary, Cruise, a self-driving car company, into its main business model.  
Head Entity: General Motors  
Tail Entity: Cruise  
#### organization parents ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization parents" means an organization 's parent is an organization.
Here is an example:
Relation: organization parents
Context: aseel kami , tim cocks , abdul-rahman taher , tim cocks and michael christie -lrb- reuters -rrb- add an intriguing note in the speculation over who 's responsible for the recent bombings : `` but a senior iraqi intelligence source , who declined to be named , said there was evidence the bombs could be the work of the militant badr organisation , the armed wing of the supreme islamic iraqi council -lrb- isci -rrb- .
Head Entity: badr organisation
Tail Entity: supreme islamic iraqi council
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization parents":
Relation: organization parents  
Context: In a recent merger, the tech giant Alphabet Inc. announced that it would acquire the innovative startup DeepMind Technologies, which has been a subsidiary of Alphabet since 2015. This acquisition is expected to enhance Alphabet's capabilities in artificial intelligence.  
Head Entity: DeepMind Technologies  
Tail Entity: Alphabet Inc.  

Relation: organization parents  
Context: The historical records indicate that the renowned publishing house Penguin Random House was formed through the merger of two major companies, Penguin Group and Random House, which were both influential in the literary world.  
Head Entity: Penguin Random House  
Tail Entity: Penguin Group  
#### organization alternate names ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization alternate names" means a person 's alias is a person.
Here is an example:
Relation: organization alternate names
Context: it also needs the green light from the 45-nation nuclear suppliers group -lrb- nsg -rrb- , which regulates global civilian nuclear trade , before it can begin buying nuclear reactors and fuel .
Head Entity: nsg
Tail Entity: nuclear suppliers group
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization alternate names":
Relation: organization alternate names  
Context: The International Monetary Fund, often referred to as the IMF, plays a crucial role in global economic stability.  
Head Entity: IMF  
Tail Entity: International Monetary Fund  

Relation: organization alternate names  
Context: The World Health Organization, commonly known as WHO, has been at the forefront of the global response to health crises.  
Head Entity: WHO  
Tail Entity: World Health Organization  
#### organization city of headquarters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization city of headquarters" means an organization is located in a city.
Here is an example:
Relation: organization city of headquarters
Context: guy-sheftall entered spelman college in atlanta at age 16 and later earned a master 's in english with a thesis titled `` faulkner 's treatment of women in his major novels . ''
Head Entity: spelman college
Tail Entity: atlanta
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization city of headquarters":
Relation: organization city of headquarters  
Context: the headquarters of tech giant apple inc. is located in cupertino, california, where it has been since 1993.  
Head Entity: apple inc.  
Tail Entity: cupertino  

Relation: organization city of headquarters  
Context: the united nations has its main office in new york city, serving as a hub for international diplomacy and cooperation.  
Head Entity: united nations  
Tail Entity: new york city  
#### person siblings ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person siblings" means a person 's sibling is a person.
Here is an example:
Relation: person siblings
Context: `` holly and sanjaya are headed to -lsb- the hawaiian island of -rsb- kauai tomorrow morning so she can meet his parents . ''
Head Entity: she
Tail Entity: sanjaya
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person siblings":
Relation: person siblings  
Context: `` during the family reunion, john introduced his sister to everyone, and they all had a great time reminiscing about their childhood. ''  
Head Entity: his sister  
Tail Entity: john  

Relation: person siblings  
Context: `` after the graduation ceremony, emily celebrated with her brother, who had always been her biggest supporter throughout school. ''  
Head Entity: her brother  
Tail Entity: emily  
Mixup data size:  254
MixupTrain:  epoch  0, batch     0 | loss: 2.8132087MixupTrain:  epoch  0, batch     1 | loss: 2.4438709MixupTrain:  epoch  0, batch     2 | loss: 2.2912397MixupTrain:  epoch  0, batch     3 | loss: 2.3832203MixupTrain:  epoch  0, batch     4 | loss: 2.8337069MixupTrain:  epoch  0, batch     5 | loss: 2.1629085MixupTrain:  epoch  0, batch     6 | loss: 3.4226301MixupTrain:  epoch  0, batch     7 | loss: 2.8175943MixupTrain:  epoch  0, batch     8 | loss: 2.6825800MixupTrain:  epoch  0, batch     9 | loss: 2.8688526MixupTrain:  epoch  0, batch    10 | loss: 2.7876490MixupTrain:  epoch  0, batch    11 | loss: 2.5652270MixupTrain:  epoch  0, batch    12 | loss: 2.8262888MixupTrain:  epoch  0, batch    13 | loss: 2.6540229MixupTrain:  epoch  0, batch    14 | loss: 2.6014880MixupTrain:  epoch  0, batch    15 | loss: 2.3276642
MemoryTrain:  epoch  0, batch     0 | loss: 1.7195730MemoryTrain:  epoch  0, batch     1 | loss: 2.8937173MemoryTrain:  epoch  0, batch     2 | loss: 2.5292497MemoryTrain:  epoch  0, batch     3 | loss: 2.6815963MemoryTrain:  epoch  0, batch     4 | loss: 2.7702365MemoryTrain:  epoch  0, batch     5 | loss: 2.2548180MemoryTrain:  epoch  0, batch     6 | loss: 2.9553237MemoryTrain:  epoch  1, batch     0 | loss: 1.9620979MemoryTrain:  epoch  1, batch     1 | loss: 3.4070287MemoryTrain:  epoch  1, batch     2 | loss: 2.3606815MemoryTrain:  epoch  1, batch     3 | loss: 1.7286265MemoryTrain:  epoch  1, batch     4 | loss: 2.3915849MemoryTrain:  epoch  1, batch     5 | loss: 1.5909197MemoryTrain:  epoch  1, batch     6 | loss: 2.7150018MemoryTrain:  epoch  2, batch     0 | loss: 2.3291135MemoryTrain:  epoch  2, batch     1 | loss: 2.3099809MemoryTrain:  epoch  2, batch     2 | loss: 2.2581391MemoryTrain:  epoch  2, batch     3 | loss: 1.9389687MemoryTrain:  epoch  2, batch     4 | loss: 1.9461427MemoryTrain:  epoch  2, batch     5 | loss: 1.7543352MemoryTrain:  epoch  2, batch     6 | loss: 2.1252916MemoryTrain:  epoch  3, batch     0 | loss: 2.2138112MemoryTrain:  epoch  3, batch     1 | loss: 1.8258088MemoryTrain:  epoch  3, batch     2 | loss: 1.8218911MemoryTrain:  epoch  3, batch     3 | loss: 1.5651696MemoryTrain:  epoch  3, batch     4 | loss: 1.9684170MemoryTrain:  epoch  3, batch     5 | loss: 2.2209916MemoryTrain:  epoch  3, batch     6 | loss: 1.8444834MemoryTrain:  epoch  4, batch     0 | loss: 1.7151878MemoryTrain:  epoch  4, batch     1 | loss: 1.5362110MemoryTrain:  epoch  4, batch     2 | loss: 1.4181193MemoryTrain:  epoch  4, batch     3 | loss: 1.9309533MemoryTrain:  epoch  4, batch     4 | loss: 1.8696253MemoryTrain:  epoch  4, batch     5 | loss: 1.8549161MemoryTrain:  epoch  4, batch     6 | loss: 1.8120070MemoryTrain:  epoch  5, batch     0 | loss: 2.0959561MemoryTrain:  epoch  5, batch     1 | loss: 1.5618191MemoryTrain:  epoch  5, batch     2 | loss: 1.4834213MemoryTrain:  epoch  5, batch     3 | loss: 1.8252935MemoryTrain:  epoch  5, batch     4 | loss: 1.4262085MemoryTrain:  epoch  5, batch     5 | loss: 1.4513834MemoryTrain:  epoch  5, batch     6 | loss: 1.5374746MemoryTrain:  epoch  6, batch     0 | loss: 1.7265829MemoryTrain:  epoch  6, batch     1 | loss: 1.4807065MemoryTrain:  epoch  6, batch     2 | loss: 1.3398860MemoryTrain:  epoch  6, batch     3 | loss: 1.8188320MemoryTrain:  epoch  6, batch     4 | loss: 1.4592299MemoryTrain:  epoch  6, batch     5 | loss: 1.4822810MemoryTrain:  epoch  6, batch     6 | loss: 1.5523213MemoryTrain:  epoch  7, batch     0 | loss: 1.4749047MemoryTrain:  epoch  7, batch     1 | loss: 1.4766544MemoryTrain:  epoch  7, batch     2 | loss: 1.6086888MemoryTrain:  epoch  7, batch     3 | loss: 1.6126115MemoryTrain:  epoch  7, batch     4 | loss: 1.4529736MemoryTrain:  epoch  7, batch     5 | loss: 1.4195964MemoryTrain:  epoch  7, batch     6 | loss: 1.4780385MemoryTrain:  epoch  8, batch     0 | loss: 1.4101782MemoryTrain:  epoch  8, batch     1 | loss: 1.4327401MemoryTrain:  epoch  8, batch     2 | loss: 1.3502963MemoryTrain:  epoch  8, batch     3 | loss: 1.3194042MemoryTrain:  epoch  8, batch     4 | loss: 1.4779552MemoryTrain:  epoch  8, batch     5 | loss: 1.3793846MemoryTrain:  epoch  8, batch     6 | loss: 1.6248398MemoryTrain:  epoch  9, batch     0 | loss: 1.3768712MemoryTrain:  epoch  9, batch     1 | loss: 1.3673640MemoryTrain:  epoch  9, batch     2 | loss: 1.4411223MemoryTrain:  epoch  9, batch     3 | loss: 1.3482897MemoryTrain:  epoch  9, batch     4 | loss: 1.5046221MemoryTrain:  epoch  9, batch     5 | loss: 1.3556025MemoryTrain:  epoch  9, batch     6 | loss: 1.4395412
[EVAL] batch:    0 | acc: 43.75%,  total acc: 43.75%   [EVAL] batch:    1 | acc: 50.00%,  total acc: 46.88%   [EVAL] batch:    2 | acc: 18.75%,  total acc: 37.50%   [EVAL] batch:    3 | acc: 12.50%,  total acc: 31.25%   [EVAL] batch:    4 | acc: 43.75%,  total acc: 33.75%   [EVAL] batch:    5 | acc: 25.00%,  total acc: 32.29%   [EVAL] batch:    6 | acc: 43.75%,  total acc: 33.93%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 39.06%   [EVAL] batch:    8 | acc: 37.50%,  total acc: 38.89%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 41.25%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 44.89%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 47.40%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 50.00%   [EVAL] batch:   13 | acc: 100.00%,  total acc: 53.57%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 56.25%   [EVAL] batch:   15 | acc: 93.75%,  total acc: 58.59%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 60.66%   [EVAL] batch:   17 | acc: 87.50%,  total acc: 62.15%   [EVAL] batch:   18 | acc: 0.00%,  total acc: 58.88%   [EVAL] batch:   19 | acc: 25.00%,  total acc: 57.19%   [EVAL] batch:   20 | acc: 6.25%,  total acc: 54.76%   [EVAL] batch:   21 | acc: 12.50%,  total acc: 52.84%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 68.75%,  total acc: 71.88%   [EVAL] batch:    2 | acc: 62.50%,  total acc: 68.75%   [EVAL] batch:    3 | acc: 43.75%,  total acc: 62.50%   [EVAL] batch:    4 | acc: 50.00%,  total acc: 60.00%   [EVAL] batch:    5 | acc: 43.75%,  total acc: 57.29%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 62.50%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 67.19%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 69.44%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 71.25%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 72.73%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 73.96%   [EVAL] batch:   12 | acc: 43.75%,  total acc: 71.63%   [EVAL] batch:   13 | acc: 31.25%,  total acc: 68.75%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 69.17%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 68.36%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 68.75%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 68.40%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 68.09%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 68.75%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 70.24%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 71.31%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 72.55%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 73.44%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 74.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 75.48%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 76.16%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 77.01%   [EVAL] batch:   28 | acc: 87.50%,  total acc: 77.37%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 77.50%   [EVAL] batch:   30 | acc: 68.75%,  total acc: 77.22%   [EVAL] batch:   31 | acc: 87.50%,  total acc: 77.54%   [EVAL] batch:   32 | acc: 62.50%,  total acc: 77.08%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 77.76%   [EVAL] batch:   34 | acc: 37.50%,  total acc: 76.61%   [EVAL] batch:   35 | acc: 31.25%,  total acc: 75.35%   [EVAL] batch:   36 | acc: 6.25%,  total acc: 73.48%   [EVAL] batch:   37 | acc: 25.00%,  total acc: 72.20%   [EVAL] batch:   38 | acc: 6.25%,  total acc: 70.51%   [EVAL] batch:   39 | acc: 50.00%,  total acc: 70.00%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 70.58%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 71.28%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 71.95%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 72.59%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 73.19%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 73.78%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 74.34%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 74.87%   [EVAL] batch:   48 | acc: 12.50%,  total acc: 73.60%   [EVAL] batch:   49 | acc: 18.75%,  total acc: 72.50%   [EVAL] batch:   50 | acc: 25.00%,  total acc: 71.57%   [EVAL] batch:   51 | acc: 50.00%,  total acc: 71.15%   [EVAL] batch:   52 | acc: 50.00%,  total acc: 70.75%   [EVAL] batch:   53 | acc: 25.00%,  total acc: 69.91%   [EVAL] batch:   54 | acc: 6.25%,  total acc: 68.75%   [EVAL] batch:   55 | acc: 6.25%,  total acc: 67.63%   [EVAL] batch:   56 | acc: 12.50%,  total acc: 66.67%   [EVAL] batch:   57 | acc: 6.25%,  total acc: 65.62%   [EVAL] batch:   58 | acc: 0.00%,  total acc: 64.51%   [EVAL] batch:   59 | acc: 37.50%,  total acc: 64.06%   [EVAL] batch:   60 | acc: 75.00%,  total acc: 64.24%   [EVAL] batch:   61 | acc: 25.00%,  total acc: 63.61%   [EVAL] batch:   62 | acc: 56.25%,  total acc: 63.49%   [EVAL] batch:   63 | acc: 56.25%,  total acc: 63.38%   [EVAL] batch:   64 | acc: 81.25%,  total acc: 63.65%   [EVAL] batch:   65 | acc: 100.00%,  total acc: 64.20%   [EVAL] batch:   66 | acc: 100.00%,  total acc: 64.74%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 65.26%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 65.76%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 66.25%   [EVAL] batch:   70 | acc: 56.25%,  total acc: 66.11%   [EVAL] batch:   71 | acc: 50.00%,  total acc: 65.89%   [EVAL] batch:   72 | acc: 50.00%,  total acc: 65.67%   [EVAL] batch:   73 | acc: 37.50%,  total acc: 65.29%   [EVAL] batch:   74 | acc: 37.50%,  total acc: 64.92%   [EVAL] batch:   75 | acc: 31.25%,  total acc: 64.47%   [EVAL] batch:   76 | acc: 50.00%,  total acc: 64.29%   [EVAL] batch:   77 | acc: 31.25%,  total acc: 63.86%   [EVAL] batch:   78 | acc: 6.25%,  total acc: 63.13%   [EVAL] batch:   79 | acc: 18.75%,  total acc: 62.58%   [EVAL] batch:   80 | acc: 6.25%,  total acc: 61.88%   [EVAL] batch:   81 | acc: 37.50%,  total acc: 61.59%   [EVAL] batch:   82 | acc: 43.75%,  total acc: 61.37%   [EVAL] batch:   83 | acc: 62.50%,  total acc: 61.38%   [EVAL] batch:   84 | acc: 50.00%,  total acc: 61.25%   [EVAL] batch:   85 | acc: 62.50%,  total acc: 61.26%   [EVAL] batch:   86 | acc: 50.00%,  total acc: 61.14%   [EVAL] batch:   87 | acc: 43.75%,  total acc: 60.94%   [EVAL] batch:   88 | acc: 56.25%,  total acc: 60.88%   [EVAL] batch:   89 | acc: 56.25%,  total acc: 60.83%   [EVAL] batch:   90 | acc: 81.25%,  total acc: 61.06%   [EVAL] batch:   91 | acc: 87.50%,  total acc: 61.35%   [EVAL] batch:   92 | acc: 75.00%,  total acc: 61.49%   [EVAL] batch:   93 | acc: 87.50%,  total acc: 61.77%   [EVAL] batch:   94 | acc: 68.75%,  total acc: 61.84%   [EVAL] batch:   95 | acc: 75.00%,  total acc: 61.98%   [EVAL] batch:   96 | acc: 68.75%,  total acc: 62.05%   [EVAL] batch:   97 | acc: 56.25%,  total acc: 61.99%   [EVAL] batch:   98 | acc: 37.50%,  total acc: 61.74%   [EVAL] batch:   99 | acc: 18.75%,  total acc: 61.31%   [EVAL] batch:  100 | acc: 25.00%,  total acc: 60.95%   [EVAL] batch:  101 | acc: 25.00%,  total acc: 60.60%   [EVAL] batch:  102 | acc: 25.00%,  total acc: 60.25%   [EVAL] batch:  103 | acc: 62.50%,  total acc: 60.28%   [EVAL] batch:  104 | acc: 62.50%,  total acc: 60.30%   [EVAL] batch:  105 | acc: 43.75%,  total acc: 60.14%   [EVAL] batch:  106 | acc: 75.00%,  total acc: 60.28%   [EVAL] batch:  107 | acc: 68.75%,  total acc: 60.36%   [EVAL] batch:  108 | acc: 75.00%,  total acc: 60.49%   [EVAL] batch:  109 | acc: 93.75%,  total acc: 60.80%   [EVAL] batch:  110 | acc: 100.00%,  total acc: 61.15%   [EVAL] batch:  111 | acc: 93.75%,  total acc: 61.44%   [EVAL] batch:  112 | acc: 93.75%,  total acc: 61.73%   [EVAL] batch:  113 | acc: 87.50%,  total acc: 61.95%   [EVAL] batch:  114 | acc: 68.75%,  total acc: 62.01%   [EVAL] batch:  115 | acc: 12.50%,  total acc: 61.58%   [EVAL] batch:  116 | acc: 12.50%,  total acc: 61.16%   [EVAL] batch:  117 | acc: 12.50%,  total acc: 60.75%   [EVAL] batch:  118 | acc: 6.25%,  total acc: 60.29%   
cur_acc:  ['0.8674', '0.3438', '0.7634', '0.8576', '0.4952', '0.6786', '0.5284']
his_acc:  ['0.8674', '0.6734', '0.7604', '0.7949', '0.6659', '0.6424', '0.6029']
CurrentTrain: epoch  0, batch     0 | loss: 5.8176050CurrentTrain: epoch  0, batch     1 | loss: 6.3152480CurrentTrain: epoch  1, batch     0 | loss: 4.6894741CurrentTrain: epoch  1, batch     1 | loss: 5.3949571CurrentTrain: epoch  2, batch     0 | loss: 4.3243513CurrentTrain: epoch  2, batch     1 | loss: 4.4380918CurrentTrain: epoch  3, batch     0 | loss: 3.9611378CurrentTrain: epoch  3, batch     1 | loss: 3.8740511CurrentTrain: epoch  4, batch     0 | loss: 3.6349039CurrentTrain: epoch  4, batch     1 | loss: 3.4326162CurrentTrain: epoch  5, batch     0 | loss: 3.4534063CurrentTrain: epoch  5, batch     1 | loss: 3.1344154CurrentTrain: epoch  6, batch     0 | loss: 3.4222422CurrentTrain: epoch  6, batch     1 | loss: 2.9676890CurrentTrain: epoch  7, batch     0 | loss: 3.0545926CurrentTrain: epoch  7, batch     1 | loss: 3.3095222CurrentTrain: epoch  8, batch     0 | loss: 3.2961884CurrentTrain: epoch  8, batch     1 | loss: 3.0150530CurrentTrain: epoch  9, batch     0 | loss: 3.0313721CurrentTrain: epoch  9, batch     1 | loss: 2.8719513
#### person stateorprovinces of residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person stateorprovinces of residence" means a person is living in a state or province.
Here is an example:
Relation: person stateorprovinces of residence
Context: ny-schools-chief -lrb- new york -rrb- -- cathleen p. black , mayor michael r. bloomberg 's choice to be the next chancellor of new york city 's public-school system , has during more than 40 years in the media business broken numerous glass ceilings -- and amassed a fortune -- with quick and cold-blooded decision making , crystal-clear goal setting , and an all-surpassing attention to the bottom line .
Head Entity: cathleen p. black
Tail Entity: new york
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person stateorprovinces of residence":
Relation: person stateorprovinces of residence  
Context: after years of living in the bustling city, actor john doe has decided to settle down in the serene landscapes of oregon, where he can enjoy a quieter lifestyle away from the spotlight.  
Head Entity: john doe  
Tail Entity: oregon  

Relation: person stateorprovinces of residence  
Context: renowned author jane smith has made her home in the vibrant city of san francisco, drawing inspiration from its rich culture and diverse community for her next novel.  
Head Entity: jane smith  
Tail Entity: san francisco  
#### person date of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person date of death" means a person was died on a date.
Here is an example:
Relation: person date of death
Context: former gov. robert e. mcnair of south carolina , the political moderate who was a finalist to become vice president hubert h. humphrey 's running mate in 1968 but whose promising career was cut short by what became known as the orangeburg massacre , died on nov. 17 in charleston .
Head Entity: robert e. mcnair
Tail Entity: nov. 17
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person date of death":
Relation: person date of death  
Context: the renowned physicist stephen hawking, known for his groundbreaking work in cosmology and black holes, passed away peacefully at his home in cambridge on march 14, 2018.  
Head Entity: stephen hawking  
Tail Entity: march 14, 2018  

Relation: person date of death  
Context: actress and humanitarian audrey hepburn, famous for her roles in classic films such as breakfast at tiffany's, died of cancer on january 20, 1993, in switzerland.  
Head Entity: audrey hepburn  
Tail Entity: january 20, 1993  
#### organization number of employees members ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization number of employees members" means the number of an organization 's employees is a number.
Here is an example:
Relation: organization number of employees members
Context: covidien , which posted revenue of more than $ 10 billion last year , has about 42,000 employees worldwide .
Head Entity: covidien
Tail Entity: 42,000
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization number of employees members":
Relation: organization number of employees members  
Context: TechCorp, a leading software development company, boasts a workforce of approximately 5,500 employees across its global offices.  
Head Entity: TechCorp  
Tail Entity: 5,500  

Relation: organization number of employees members  
Context: GreenEarth Nonprofit has grown significantly over the past few years and currently employs around 1,200 dedicated staff members to support its environmental initiatives.  
Head Entity: GreenEarth Nonprofit  
Tail Entity: 1,200  
#### person alternate names ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person alternate names" means a person 's alias is a person.
Here is an example:
Relation: person alternate names
Context: a judge in new york city said remy ma , whose real name is remy smith , said thursday that the hip-hopper could not leave the united states for a five-country european concert tour .
Head Entity: remy smith
Tail Entity: remy ma
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person alternate names":
Relation: person alternate names  
Context: The famous author Samuel Langhorne Clemens, better known by his pen name Mark Twain, is celebrated for his novels like "The Adventures of Tom Sawyer."  
Head Entity: Samuel Langhorne Clemens  
Tail Entity: Mark Twain  

Relation: person alternate names  
Context: The musician Stefani Joanne Angelina Germanotta, who is widely recognized by her stage name Lady Gaga, has made a significant impact on pop culture.  
Head Entity: Stefani Joanne Angelina Germanotta  
Tail Entity: Lady Gaga  
#### person spouse ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person spouse" means a person 's spouse is a person.
Here is an example:
Relation: person spouse
Context: smits stands at the center of this multigenerational saga as alex vega , the adopted son of rum and sugar baron pancho duque -lrb- elizondo -rrb- and his wife , amalia -lrb- moreno -rrb- .
Head Entity: elizondo
Tail Entity: moreno
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person spouse":
Relation: person spouse  
Context: in a heartfelt ceremony, john and his beloved partner, sarah, exchanged vows surrounded by family and friends, celebrating their love and commitment to each other.  
Head Entity: john  
Tail Entity: sarah  

Relation: person spouse  
Context: after years of friendship, emily finally realized that her best friend, michael, was the one she wanted to spend her life with, leading to a beautiful wedding.  
Head Entity: emily  
Tail Entity: michael  
Mixup data size:  285
MixupTrain:  epoch  0, batch     0 | loss: 1.8152072MixupTrain:  epoch  0, batch     1 | loss: 2.5623228MixupTrain:  epoch  0, batch     2 | loss: 2.0110176MixupTrain:  epoch  0, batch     3 | loss: 2.2081216MixupTrain:  epoch  0, batch     4 | loss: 2.3542520MixupTrain:  epoch  0, batch     5 | loss: 3.0199477MixupTrain:  epoch  0, batch     6 | loss: 2.3480148MixupTrain:  epoch  0, batch     7 | loss: 2.4868495MixupTrain:  epoch  0, batch     8 | loss: 2.2548817MixupTrain:  epoch  0, batch     9 | loss: 1.8501908MixupTrain:  epoch  0, batch    10 | loss: 3.1231316MixupTrain:  epoch  0, batch    11 | loss: 2.3942662MixupTrain:  epoch  0, batch    12 | loss: 2.2300808MixupTrain:  epoch  0, batch    13 | loss: 2.3697678MixupTrain:  epoch  0, batch    14 | loss: 2.1079877MixupTrain:  epoch  0, batch    15 | loss: 2.2208883MixupTrain:  epoch  0, batch    16 | loss: 2.2969794MixupTrain:  epoch  0, batch    17 | loss: 2.4839016
MemoryTrain:  epoch  0, batch     0 | loss: 2.0235860MemoryTrain:  epoch  0, batch     1 | loss: 1.8716581MemoryTrain:  epoch  0, batch     2 | loss: 1.7635884MemoryTrain:  epoch  0, batch     3 | loss: 1.9904224MemoryTrain:  epoch  0, batch     4 | loss: 2.5308568MemoryTrain:  epoch  0, batch     5 | loss: 1.9331900MemoryTrain:  epoch  0, batch     6 | loss: 2.4528358MemoryTrain:  epoch  0, batch     7 | loss: 2.5933800MemoryTrain:  epoch  1, batch     0 | loss: 1.9545305MemoryTrain:  epoch  1, batch     1 | loss: 1.7795556MemoryTrain:  epoch  1, batch     2 | loss: 2.1542206MemoryTrain:  epoch  1, batch     3 | loss: 2.1550577MemoryTrain:  epoch  1, batch     4 | loss: 2.1978250MemoryTrain:  epoch  1, batch     5 | loss: 2.0423188MemoryTrain:  epoch  1, batch     6 | loss: 1.7222320MemoryTrain:  epoch  1, batch     7 | loss: 1.5491978MemoryTrain:  epoch  2, batch     0 | loss: 1.7374825MemoryTrain:  epoch  2, batch     1 | loss: 2.1302888MemoryTrain:  epoch  2, batch     2 | loss: 1.5584850MemoryTrain:  epoch  2, batch     3 | loss: 1.7046996MemoryTrain:  epoch  2, batch     4 | loss: 1.5405147MemoryTrain:  epoch  2, batch     5 | loss: 2.0725307MemoryTrain:  epoch  2, batch     6 | loss: 1.9361279MemoryTrain:  epoch  2, batch     7 | loss: 1.7251425MemoryTrain:  epoch  3, batch     0 | loss: 1.7217717MemoryTrain:  epoch  3, batch     1 | loss: 1.8567895MemoryTrain:  epoch  3, batch     2 | loss: 1.7434971MemoryTrain:  epoch  3, batch     3 | loss: 2.2678013MemoryTrain:  epoch  3, batch     4 | loss: 1.6308918MemoryTrain:  epoch  3, batch     5 | loss: 1.4452478MemoryTrain:  epoch  3, batch     6 | loss: 1.5696284MemoryTrain:  epoch  3, batch     7 | loss: 1.4781901MemoryTrain:  epoch  4, batch     0 | loss: 1.8149905MemoryTrain:  epoch  4, batch     1 | loss: 1.6092170MemoryTrain:  epoch  4, batch     2 | loss: 1.6251805MemoryTrain:  epoch  4, batch     3 | loss: 1.6263607MemoryTrain:  epoch  4, batch     4 | loss: 1.5693780MemoryTrain:  epoch  4, batch     5 | loss: 1.4277122MemoryTrain:  epoch  4, batch     6 | loss: 1.9097054MemoryTrain:  epoch  4, batch     7 | loss: 1.4297400MemoryTrain:  epoch  5, batch     0 | loss: 1.5487469MemoryTrain:  epoch  5, batch     1 | loss: 1.3235569MemoryTrain:  epoch  5, batch     2 | loss: 1.3621556MemoryTrain:  epoch  5, batch     3 | loss: 1.9755274MemoryTrain:  epoch  5, batch     4 | loss: 1.7162355MemoryTrain:  epoch  5, batch     5 | loss: 1.5116265MemoryTrain:  epoch  5, batch     6 | loss: 1.3098735MemoryTrain:  epoch  5, batch     7 | loss: 1.6008589MemoryTrain:  epoch  6, batch     0 | loss: 1.2980728MemoryTrain:  epoch  6, batch     1 | loss: 1.3572264MemoryTrain:  epoch  6, batch     2 | loss: 1.3328406MemoryTrain:  epoch  6, batch     3 | loss: 1.7111459MemoryTrain:  epoch  6, batch     4 | loss: 1.3887364MemoryTrain:  epoch  6, batch     5 | loss: 1.4425539MemoryTrain:  epoch  6, batch     6 | loss: 1.7327385MemoryTrain:  epoch  6, batch     7 | loss: 1.8523768MemoryTrain:  epoch  7, batch     0 | loss: 1.2692243MemoryTrain:  epoch  7, batch     1 | loss: 1.5403972MemoryTrain:  epoch  7, batch     2 | loss: 1.3580141MemoryTrain:  epoch  7, batch     3 | loss: 1.2898802MemoryTrain:  epoch  7, batch     4 | loss: 1.5342495MemoryTrain:  epoch  7, batch     5 | loss: 1.7189656MemoryTrain:  epoch  7, batch     6 | loss: 1.4721794MemoryTrain:  epoch  7, batch     7 | loss: 1.2680798MemoryTrain:  epoch  8, batch     0 | loss: 1.3455694MemoryTrain:  epoch  8, batch     1 | loss: 1.3307219MemoryTrain:  epoch  8, batch     2 | loss: 1.5299456MemoryTrain:  epoch  8, batch     3 | loss: 1.5426607MemoryTrain:  epoch  8, batch     4 | loss: 1.4829487MemoryTrain:  epoch  8, batch     5 | loss: 1.4228401MemoryTrain:  epoch  8, batch     6 | loss: 1.2435994MemoryTrain:  epoch  8, batch     7 | loss: 1.3402003MemoryTrain:  epoch  9, batch     0 | loss: 1.3341558MemoryTrain:  epoch  9, batch     1 | loss: 1.2687508MemoryTrain:  epoch  9, batch     2 | loss: 1.4834599MemoryTrain:  epoch  9, batch     3 | loss: 1.3426583MemoryTrain:  epoch  9, batch     4 | loss: 1.3078328MemoryTrain:  epoch  9, batch     5 | loss: 1.4572778MemoryTrain:  epoch  9, batch     6 | loss: 1.2939384MemoryTrain:  epoch  9, batch     7 | loss: 1.3892549
[EVAL] batch:    0 | acc: 50.00%,  total acc: 50.00%   [EVAL] batch:    1 | acc: 50.00%,  total acc: 50.00%   [EVAL] batch:    2 | acc: 25.00%,  total acc: 41.67%   [EVAL] batch:    3 | acc: 37.50%,  total acc: 40.62%   [EVAL] batch:    4 | acc: 56.25%,  total acc: 43.75%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 51.04%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 57.14%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 62.50%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 65.28%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 66.88%   [EVAL] batch:   10 | acc: 43.75%,  total acc: 64.77%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 66.15%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 66.35%   [EVAL] batch:   13 | acc: 50.00%,  total acc: 65.18%   [EVAL] batch:   14 | acc: 37.50%,  total acc: 63.33%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 75.00%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 68.75%,  total acc: 75.00%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 76.56%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 75.00%   [EVAL] batch:    5 | acc: 62.50%,  total acc: 72.92%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 75.89%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 78.91%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 79.86%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 80.62%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 81.77%   [EVAL] batch:   12 | acc: 43.75%,  total acc: 78.85%   [EVAL] batch:   13 | acc: 31.25%,  total acc: 75.45%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 75.00%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 73.83%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 73.90%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 73.26%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 72.70%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 73.44%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 74.70%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 75.57%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 76.63%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 77.34%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 78.25%   [EVAL] batch:   25 | acc: 87.50%,  total acc: 78.61%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 79.17%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 79.91%   [EVAL] batch:   28 | acc: 81.25%,  total acc: 79.96%   [EVAL] batch:   29 | acc: 68.75%,  total acc: 79.58%   [EVAL] batch:   30 | acc: 75.00%,  total acc: 79.44%   [EVAL] batch:   31 | acc: 81.25%,  total acc: 79.49%   [EVAL] batch:   32 | acc: 62.50%,  total acc: 78.98%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 79.60%   [EVAL] batch:   34 | acc: 37.50%,  total acc: 78.39%   [EVAL] batch:   35 | acc: 18.75%,  total acc: 76.74%   [EVAL] batch:   36 | acc: 6.25%,  total acc: 74.83%   [EVAL] batch:   37 | acc: 18.75%,  total acc: 73.36%   [EVAL] batch:   38 | acc: 6.25%,  total acc: 71.63%   [EVAL] batch:   39 | acc: 50.00%,  total acc: 71.09%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 71.65%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 72.32%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 72.97%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 73.44%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 74.03%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 74.59%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 75.13%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 75.65%   [EVAL] batch:   48 | acc: 12.50%,  total acc: 74.36%   [EVAL] batch:   49 | acc: 18.75%,  total acc: 73.25%   [EVAL] batch:   50 | acc: 31.25%,  total acc: 72.43%   [EVAL] batch:   51 | acc: 43.75%,  total acc: 71.88%   [EVAL] batch:   52 | acc: 25.00%,  total acc: 70.99%   [EVAL] batch:   53 | acc: 12.50%,  total acc: 69.91%   [EVAL] batch:   54 | acc: 0.00%,  total acc: 68.64%   [EVAL] batch:   55 | acc: 0.00%,  total acc: 67.41%   [EVAL] batch:   56 | acc: 0.00%,  total acc: 66.23%   [EVAL] batch:   57 | acc: 0.00%,  total acc: 65.09%   [EVAL] batch:   58 | acc: 0.00%,  total acc: 63.98%   [EVAL] batch:   59 | acc: 43.75%,  total acc: 63.65%   [EVAL] batch:   60 | acc: 93.75%,  total acc: 64.14%   [EVAL] batch:   61 | acc: 31.25%,  total acc: 63.61%   [EVAL] batch:   62 | acc: 31.25%,  total acc: 63.10%   [EVAL] batch:   63 | acc: 37.50%,  total acc: 62.70%   [EVAL] batch:   64 | acc: 81.25%,  total acc: 62.98%   [EVAL] batch:   65 | acc: 100.00%,  total acc: 63.54%   [EVAL] batch:   66 | acc: 100.00%,  total acc: 64.09%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 64.61%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 65.13%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 65.62%   [EVAL] batch:   70 | acc: 56.25%,  total acc: 65.49%   [EVAL] batch:   71 | acc: 50.00%,  total acc: 65.28%   [EVAL] batch:   72 | acc: 50.00%,  total acc: 65.07%   [EVAL] batch:   73 | acc: 31.25%,  total acc: 64.61%   [EVAL] batch:   74 | acc: 50.00%,  total acc: 64.42%   [EVAL] batch:   75 | acc: 31.25%,  total acc: 63.98%   [EVAL] batch:   76 | acc: 50.00%,  total acc: 63.80%   [EVAL] batch:   77 | acc: 37.50%,  total acc: 63.46%   [EVAL] batch:   78 | acc: 0.00%,  total acc: 62.66%   [EVAL] batch:   79 | acc: 0.00%,  total acc: 61.88%   [EVAL] batch:   80 | acc: 6.25%,  total acc: 61.19%   [EVAL] batch:   81 | acc: 37.50%,  total acc: 60.90%   [EVAL] batch:   82 | acc: 43.75%,  total acc: 60.69%   [EVAL] batch:   83 | acc: 75.00%,  total acc: 60.86%   [EVAL] batch:   84 | acc: 18.75%,  total acc: 60.37%   [EVAL] batch:   85 | acc: 12.50%,  total acc: 59.81%   [EVAL] batch:   86 | acc: 0.00%,  total acc: 59.12%   [EVAL] batch:   87 | acc: 12.50%,  total acc: 58.59%   [EVAL] batch:   88 | acc: 12.50%,  total acc: 58.08%   [EVAL] batch:   89 | acc: 25.00%,  total acc: 57.71%   [EVAL] batch:   90 | acc: 81.25%,  total acc: 57.97%   [EVAL] batch:   91 | acc: 93.75%,  total acc: 58.36%   [EVAL] batch:   92 | acc: 81.25%,  total acc: 58.60%   [EVAL] batch:   93 | acc: 87.50%,  total acc: 58.91%   [EVAL] batch:   94 | acc: 62.50%,  total acc: 58.95%   [EVAL] batch:   95 | acc: 81.25%,  total acc: 59.18%   [EVAL] batch:   96 | acc: 43.75%,  total acc: 59.02%   [EVAL] batch:   97 | acc: 37.50%,  total acc: 58.80%   [EVAL] batch:   98 | acc: 37.50%,  total acc: 58.59%   [EVAL] batch:   99 | acc: 37.50%,  total acc: 58.38%   [EVAL] batch:  100 | acc: 56.25%,  total acc: 58.35%   [EVAL] batch:  101 | acc: 50.00%,  total acc: 58.27%   [EVAL] batch:  102 | acc: 37.50%,  total acc: 58.07%   [EVAL] batch:  103 | acc: 75.00%,  total acc: 58.23%   [EVAL] batch:  104 | acc: 62.50%,  total acc: 58.27%   [EVAL] batch:  105 | acc: 43.75%,  total acc: 58.14%   [EVAL] batch:  106 | acc: 75.00%,  total acc: 58.29%   [EVAL] batch:  107 | acc: 68.75%,  total acc: 58.39%   [EVAL] batch:  108 | acc: 75.00%,  total acc: 58.54%   [EVAL] batch:  109 | acc: 87.50%,  total acc: 58.81%   [EVAL] batch:  110 | acc: 100.00%,  total acc: 59.18%   [EVAL] batch:  111 | acc: 100.00%,  total acc: 59.54%   [EVAL] batch:  112 | acc: 93.75%,  total acc: 59.85%   [EVAL] batch:  113 | acc: 87.50%,  total acc: 60.09%   [EVAL] batch:  114 | acc: 68.75%,  total acc: 60.16%   [EVAL] batch:  115 | acc: 0.00%,  total acc: 59.64%   [EVAL] batch:  116 | acc: 6.25%,  total acc: 59.19%   [EVAL] batch:  117 | acc: 12.50%,  total acc: 58.79%   [EVAL] batch:  118 | acc: 43.75%,  total acc: 58.67%   [EVAL] batch:  119 | acc: 50.00%,  total acc: 58.59%   [EVAL] batch:  120 | acc: 25.00%,  total acc: 58.32%   [EVAL] batch:  121 | acc: 37.50%,  total acc: 58.15%   [EVAL] batch:  122 | acc: 50.00%,  total acc: 58.08%   [EVAL] batch:  123 | acc: 81.25%,  total acc: 58.27%   [EVAL] batch:  124 | acc: 93.75%,  total acc: 58.55%   [EVAL] batch:  125 | acc: 100.00%,  total acc: 58.88%   [EVAL] batch:  126 | acc: 87.50%,  total acc: 59.10%   [EVAL] batch:  127 | acc: 87.50%,  total acc: 59.33%   [EVAL] batch:  128 | acc: 50.00%,  total acc: 59.25%   [EVAL] batch:  129 | acc: 68.75%,  total acc: 59.33%   [EVAL] batch:  130 | acc: 75.00%,  total acc: 59.45%   [EVAL] batch:  131 | acc: 56.25%,  total acc: 59.42%   [EVAL] batch:  132 | acc: 43.75%,  total acc: 59.30%   
cur_acc:  ['0.8674', '0.3438', '0.7634', '0.8576', '0.4952', '0.6786', '0.5284', '0.6333']
his_acc:  ['0.8674', '0.6734', '0.7604', '0.7949', '0.6659', '0.6424', '0.6029', '0.5930']
--------Round  4
seed:  500
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/test.pkl
Task_order: [7 5 6 4 2 1 3 0]
prepared data!
CurrentTrain: epoch  0, batch     0 | loss: 13.3535175CurrentTrain: epoch  0, batch     1 | loss: 13.2118616CurrentTrain: epoch  0, batch     2 | loss: 13.0181665CurrentTrain: epoch  0, batch     3 | loss: 13.1006413CurrentTrain: epoch  0, batch     4 | loss: 12.7817039CurrentTrain: epoch  0, batch     5 | loss: 12.6867943CurrentTrain: epoch  0, batch     6 | loss: 12.6464968CurrentTrain: epoch  0, batch     7 | loss: 12.5833054CurrentTrain: epoch  0, batch     8 | loss: 12.2926483CurrentTrain: epoch  0, batch     9 | loss: 12.3267784CurrentTrain: epoch  0, batch    10 | loss: 12.0201340CurrentTrain: epoch  0, batch    11 | loss: 12.0788612CurrentTrain: epoch  0, batch    12 | loss: 11.9824543CurrentTrain: epoch  0, batch    13 | loss: 11.9791832CurrentTrain: epoch  0, batch    14 | loss: 11.7834387CurrentTrain: epoch  0, batch    15 | loss: 11.8194895CurrentTrain: epoch  0, batch    16 | loss: 11.4878063CurrentTrain: epoch  0, batch    17 | loss: 11.2897606CurrentTrain: epoch  0, batch    18 | loss: 11.4235048CurrentTrain: epoch  0, batch    19 | loss: 11.3928070CurrentTrain: epoch  0, batch    20 | loss: 11.3715124CurrentTrain: epoch  0, batch    21 | loss: 11.0619173CurrentTrain: epoch  0, batch    22 | loss: 11.0536842CurrentTrain: epoch  0, batch    23 | loss: 11.4307795CurrentTrain: epoch  0, batch    24 | loss: 11.2091675CurrentTrain: epoch  0, batch    25 | loss: 10.8775539CurrentTrain: epoch  0, batch    26 | loss: 10.7752609CurrentTrain: epoch  0, batch    27 | loss: 10.8603268CurrentTrain: epoch  0, batch    28 | loss: 10.6827812CurrentTrain: epoch  0, batch    29 | loss: 10.8761673CurrentTrain: epoch  0, batch    30 | loss: 10.8494091CurrentTrain: epoch  0, batch    31 | loss: 10.7513103CurrentTrain: epoch  0, batch    32 | loss: 10.3391085CurrentTrain: epoch  0, batch    33 | loss: 10.8522377CurrentTrain: epoch  0, batch    34 | loss: 10.3720474CurrentTrain: epoch  0, batch    35 | loss: 10.4603453CurrentTrain: epoch  0, batch    36 | loss: 10.5551834CurrentTrain: epoch  0, batch    37 | loss: 9.9687290CurrentTrain: epoch  1, batch     0 | loss: 10.7406025CurrentTrain: epoch  1, batch     1 | loss: 10.1714563CurrentTrain: epoch  1, batch     2 | loss: 10.3265314CurrentTrain: epoch  1, batch     3 | loss: 9.6506138CurrentTrain: epoch  1, batch     4 | loss: 9.9524851CurrentTrain: epoch  1, batch     5 | loss: 9.8661919CurrentTrain: epoch  1, batch     6 | loss: 10.0719872CurrentTrain: epoch  1, batch     7 | loss: 10.1303406CurrentTrain: epoch  1, batch     8 | loss: 9.2783413CurrentTrain: epoch  1, batch     9 | loss: 10.3080196CurrentTrain: epoch  1, batch    10 | loss: 10.3729210CurrentTrain: epoch  1, batch    11 | loss: 10.1015129CurrentTrain: epoch  1, batch    12 | loss: 9.6592436CurrentTrain: epoch  1, batch    13 | loss: 9.7246923CurrentTrain: epoch  1, batch    14 | loss: 9.1316357CurrentTrain: epoch  1, batch    15 | loss: 9.3834038CurrentTrain: epoch  1, batch    16 | loss: 9.2543812CurrentTrain: epoch  1, batch    17 | loss: 9.2281799CurrentTrain: epoch  1, batch    18 | loss: 9.5808201CurrentTrain: epoch  1, batch    19 | loss: 9.3559475CurrentTrain: epoch  1, batch    20 | loss: 9.4044323CurrentTrain: epoch  1, batch    21 | loss: 9.2137642CurrentTrain: epoch  1, batch    22 | loss: 9.1442804CurrentTrain: epoch  1, batch    23 | loss: 9.3226185CurrentTrain: epoch  1, batch    24 | loss: 9.6891613CurrentTrain: epoch  1, batch    25 | loss: 9.1712227CurrentTrain: epoch  1, batch    26 | loss: 8.3283129CurrentTrain: epoch  1, batch    27 | loss: 9.5854111CurrentTrain: epoch  1, batch    28 | loss: 9.4720268CurrentTrain: epoch  1, batch    29 | loss: 8.2433376CurrentTrain: epoch  1, batch    30 | loss: 9.9991531CurrentTrain: epoch  1, batch    31 | loss: 8.7286978CurrentTrain: epoch  1, batch    32 | loss: 9.5902700CurrentTrain: epoch  1, batch    33 | loss: 9.3076429CurrentTrain: epoch  1, batch    34 | loss: 8.2145557CurrentTrain: epoch  1, batch    35 | loss: 8.1331158CurrentTrain: epoch  1, batch    36 | loss: 8.9477062CurrentTrain: epoch  1, batch    37 | loss: 8.6440697CurrentTrain: epoch  2, batch     0 | loss: 8.1831589CurrentTrain: epoch  2, batch     1 | loss: 7.9620686CurrentTrain: epoch  2, batch     2 | loss: 8.2634754CurrentTrain: epoch  2, batch     3 | loss: 8.3336372CurrentTrain: epoch  2, batch     4 | loss: 8.1716957CurrentTrain: epoch  2, batch     5 | loss: 8.6113129CurrentTrain: epoch  2, batch     6 | loss: 8.8700514CurrentTrain: epoch  2, batch     7 | loss: 8.4060049CurrentTrain: epoch  2, batch     8 | loss: 8.6956215CurrentTrain: epoch  2, batch     9 | loss: 8.2463264CurrentTrain: epoch  2, batch    10 | loss: 8.8232059CurrentTrain: epoch  2, batch    11 | loss: 8.5045013CurrentTrain: epoch  2, batch    12 | loss: 7.7970557CurrentTrain: epoch  2, batch    13 | loss: 7.8231335CurrentTrain: epoch  2, batch    14 | loss: 8.6298704CurrentTrain: epoch  2, batch    15 | loss: 7.9849768CurrentTrain: epoch  2, batch    16 | loss: 8.7733421CurrentTrain: epoch  2, batch    17 | loss: 7.8537526CurrentTrain: epoch  2, batch    18 | loss: 8.3048496CurrentTrain: epoch  2, batch    19 | loss: 8.2576599CurrentTrain: epoch  2, batch    20 | loss: 8.0314398CurrentTrain: epoch  2, batch    21 | loss: 7.3962016CurrentTrain: epoch  2, batch    22 | loss: 7.5778394CurrentTrain: epoch  2, batch    23 | loss: 7.6891332CurrentTrain: epoch  2, batch    24 | loss: 7.3268890CurrentTrain: epoch  2, batch    25 | loss: 7.8287816CurrentTrain: epoch  2, batch    26 | loss: 7.9781117CurrentTrain: epoch  2, batch    27 | loss: 7.4969845CurrentTrain: epoch  2, batch    28 | loss: 7.7920976CurrentTrain: epoch  2, batch    29 | loss: 8.4723663CurrentTrain: epoch  2, batch    30 | loss: 8.2851048CurrentTrain: epoch  2, batch    31 | loss: 7.1285214CurrentTrain: epoch  2, batch    32 | loss: 7.3362455CurrentTrain: epoch  2, batch    33 | loss: 7.9599175CurrentTrain: epoch  2, batch    34 | loss: 8.7897949CurrentTrain: epoch  2, batch    35 | loss: 7.1944976CurrentTrain: epoch  2, batch    36 | loss: 7.5159025CurrentTrain: epoch  2, batch    37 | loss: 7.2946463CurrentTrain: epoch  3, batch     0 | loss: 7.5694599CurrentTrain: epoch  3, batch     1 | loss: 7.6920347CurrentTrain: epoch  3, batch     2 | loss: 8.1536751CurrentTrain: epoch  3, batch     3 | loss: 7.9637203CurrentTrain: epoch  3, batch     4 | loss: 6.6700478CurrentTrain: epoch  3, batch     5 | loss: 7.7474308CurrentTrain: epoch  3, batch     6 | loss: 7.7030115CurrentTrain: epoch  3, batch     7 | loss: 7.9235191CurrentTrain: epoch  3, batch     8 | loss: 6.7830944CurrentTrain: epoch  3, batch     9 | loss: 7.6237288CurrentTrain: epoch  3, batch    10 | loss: 7.6175690CurrentTrain: epoch  3, batch    11 | loss: 6.8457150CurrentTrain: epoch  3, batch    12 | loss: 7.3209066CurrentTrain: epoch  3, batch    13 | loss: 7.2758031CurrentTrain: epoch  3, batch    14 | loss: 6.7927327CurrentTrain: epoch  3, batch    15 | loss: 7.2824583CurrentTrain: epoch  3, batch    16 | loss: 7.1560392CurrentTrain: epoch  3, batch    17 | loss: 6.3818684CurrentTrain: epoch  3, batch    18 | loss: 7.5467887CurrentTrain: epoch  3, batch    19 | loss: 7.5579038CurrentTrain: epoch  3, batch    20 | loss: 7.1514144CurrentTrain: epoch  3, batch    21 | loss: 7.3645029CurrentTrain: epoch  3, batch    22 | loss: 6.1497407CurrentTrain: epoch  3, batch    23 | loss: 8.3004436CurrentTrain: epoch  3, batch    24 | loss: 7.7450132CurrentTrain: epoch  3, batch    25 | loss: 7.6742158CurrentTrain: epoch  3, batch    26 | loss: 6.9035711CurrentTrain: epoch  3, batch    27 | loss: 6.7122660CurrentTrain: epoch  3, batch    28 | loss: 7.1416101CurrentTrain: epoch  3, batch    29 | loss: 7.0046434CurrentTrain: epoch  3, batch    30 | loss: 6.9782405CurrentTrain: epoch  3, batch    31 | loss: 7.3342328CurrentTrain: epoch  3, batch    32 | loss: 7.1730824CurrentTrain: epoch  3, batch    33 | loss: 7.7047462CurrentTrain: epoch  3, batch    34 | loss: 7.0765219CurrentTrain: epoch  3, batch    35 | loss: 6.5838680CurrentTrain: epoch  3, batch    36 | loss: 7.1906958CurrentTrain: epoch  3, batch    37 | loss: 7.7995925CurrentTrain: epoch  4, batch     0 | loss: 7.0568848CurrentTrain: epoch  4, batch     1 | loss: 6.7330756CurrentTrain: epoch  4, batch     2 | loss: 6.3382549CurrentTrain: epoch  4, batch     3 | loss: 6.3972511CurrentTrain: epoch  4, batch     4 | loss: 6.8560510CurrentTrain: epoch  4, batch     5 | loss: 6.6403623CurrentTrain: epoch  4, batch     6 | loss: 8.9118471CurrentTrain: epoch  4, batch     7 | loss: 7.2568645CurrentTrain: epoch  4, batch     8 | loss: 7.6029549CurrentTrain: epoch  4, batch     9 | loss: 7.2723131CurrentTrain: epoch  4, batch    10 | loss: 7.1191521CurrentTrain: epoch  4, batch    11 | loss: 6.4734077CurrentTrain: epoch  4, batch    12 | loss: 7.0929532CurrentTrain: epoch  4, batch    13 | loss: 6.8602190CurrentTrain: epoch  4, batch    14 | loss: 5.9829779CurrentTrain: epoch  4, batch    15 | loss: 7.5815034CurrentTrain: epoch  4, batch    16 | loss: 7.4134779CurrentTrain: epoch  4, batch    17 | loss: 6.2163901CurrentTrain: epoch  4, batch    18 | loss: 6.5206904CurrentTrain: epoch  4, batch    19 | loss: 6.2172070CurrentTrain: epoch  4, batch    20 | loss: 6.3505011CurrentTrain: epoch  4, batch    21 | loss: 6.7404404CurrentTrain: epoch  4, batch    22 | loss: 6.9637856CurrentTrain: epoch  4, batch    23 | loss: 7.1188807CurrentTrain: epoch  4, batch    24 | loss: 6.4310398CurrentTrain: epoch  4, batch    25 | loss: 6.6395340CurrentTrain: epoch  4, batch    26 | loss: 6.6507139CurrentTrain: epoch  4, batch    27 | loss: 6.8169103CurrentTrain: epoch  4, batch    28 | loss: 6.2386532CurrentTrain: epoch  4, batch    29 | loss: 7.1981225CurrentTrain: epoch  4, batch    30 | loss: 6.6075602CurrentTrain: epoch  4, batch    31 | loss: 6.8597636CurrentTrain: epoch  4, batch    32 | loss: 6.4317818CurrentTrain: epoch  4, batch    33 | loss: 7.1818056CurrentTrain: epoch  4, batch    34 | loss: 5.7162089CurrentTrain: epoch  4, batch    35 | loss: 7.1742449CurrentTrain: epoch  4, batch    36 | loss: 5.9627175CurrentTrain: epoch  4, batch    37 | loss: 7.1945028CurrentTrain: epoch  5, batch     0 | loss: 6.4496455CurrentTrain: epoch  5, batch     1 | loss: 6.4087763CurrentTrain: epoch  5, batch     2 | loss: 6.4867158CurrentTrain: epoch  5, batch     3 | loss: 5.9283533CurrentTrain: epoch  5, batch     4 | loss: 7.3924146CurrentTrain: epoch  5, batch     5 | loss: 6.9757133CurrentTrain: epoch  5, batch     6 | loss: 6.8308325CurrentTrain: epoch  5, batch     7 | loss: 6.9878397CurrentTrain: epoch  5, batch     8 | loss: 6.7184715CurrentTrain: epoch  5, batch     9 | loss: 6.1765709CurrentTrain: epoch  5, batch    10 | loss: 6.0349083CurrentTrain: epoch  5, batch    11 | loss: 6.2102890CurrentTrain: epoch  5, batch    12 | loss: 6.4251986CurrentTrain: epoch  5, batch    13 | loss: 6.4089212CurrentTrain: epoch  5, batch    14 | loss: 6.5470395CurrentTrain: epoch  5, batch    15 | loss: 6.7460594CurrentTrain: epoch  5, batch    16 | loss: 6.5509806CurrentTrain: epoch  5, batch    17 | loss: 5.6279540CurrentTrain: epoch  5, batch    18 | loss: 6.7002907CurrentTrain: epoch  5, batch    19 | loss: 6.3192616CurrentTrain: epoch  5, batch    20 | loss: 6.7941885CurrentTrain: epoch  5, batch    21 | loss: 6.2870274CurrentTrain: epoch  5, batch    22 | loss: 5.3160925CurrentTrain: epoch  5, batch    23 | loss: 6.9350739CurrentTrain: epoch  5, batch    24 | loss: 5.9495392CurrentTrain: epoch  5, batch    25 | loss: 6.2830029CurrentTrain: epoch  5, batch    26 | loss: 7.0952210CurrentTrain: epoch  5, batch    27 | loss: 7.6797590CurrentTrain: epoch  5, batch    28 | loss: 7.4487658CurrentTrain: epoch  5, batch    29 | loss: 6.7823482CurrentTrain: epoch  5, batch    30 | loss: 6.5635304CurrentTrain: epoch  5, batch    31 | loss: 6.5991287CurrentTrain: epoch  5, batch    32 | loss: 5.6214223CurrentTrain: epoch  5, batch    33 | loss: 5.8669963CurrentTrain: epoch  5, batch    34 | loss: 6.3630896CurrentTrain: epoch  5, batch    35 | loss: 6.4329219CurrentTrain: epoch  5, batch    36 | loss: 6.7521601CurrentTrain: epoch  5, batch    37 | loss: 6.8459029CurrentTrain: epoch  6, batch     0 | loss: 5.8190393CurrentTrain: epoch  6, batch     1 | loss: 5.6808128CurrentTrain: epoch  6, batch     2 | loss: 6.1540923CurrentTrain: epoch  6, batch     3 | loss: 6.3324947CurrentTrain: epoch  6, batch     4 | loss: 5.9286885CurrentTrain: epoch  6, batch     5 | loss: 5.7805185CurrentTrain: epoch  6, batch     6 | loss: 6.0328283CurrentTrain: epoch  6, batch     7 | loss: 5.7139816CurrentTrain: epoch  6, batch     8 | loss: 6.2086463CurrentTrain: epoch  6, batch     9 | loss: 6.4726844CurrentTrain: epoch  6, batch    10 | loss: 5.6342864CurrentTrain: epoch  6, batch    11 | loss: 5.8421192CurrentTrain: epoch  6, batch    12 | loss: 5.2436943CurrentTrain: epoch  6, batch    13 | loss: 6.2564178CurrentTrain: epoch  6, batch    14 | loss: 6.2020679CurrentTrain: epoch  6, batch    15 | loss: 6.0330439CurrentTrain: epoch  6, batch    16 | loss: 6.2281213CurrentTrain: epoch  6, batch    17 | loss: 6.0434752CurrentTrain: epoch  6, batch    18 | loss: 6.3585825CurrentTrain: epoch  6, batch    19 | loss: 6.4941812CurrentTrain: epoch  6, batch    20 | loss: 5.8524966CurrentTrain: epoch  6, batch    21 | loss: 6.0280170CurrentTrain: epoch  6, batch    22 | loss: 5.6193638CurrentTrain: epoch  6, batch    23 | loss: 5.6983566CurrentTrain: epoch  6, batch    24 | loss: 5.9826946CurrentTrain: epoch  6, batch    25 | loss: 6.1199651CurrentTrain: epoch  6, batch    26 | loss: 5.8523803CurrentTrain: epoch  6, batch    27 | loss: 6.4158459CurrentTrain: epoch  6, batch    28 | loss: 5.7251120CurrentTrain: epoch  6, batch    29 | loss: 6.5512695CurrentTrain: epoch  6, batch    30 | loss: 5.9445291CurrentTrain: epoch  6, batch    31 | loss: 5.8704309CurrentTrain: epoch  6, batch    32 | loss: 6.2945566CurrentTrain: epoch  6, batch    33 | loss: 5.8559160CurrentTrain: epoch  6, batch    34 | loss: 6.1786027CurrentTrain: epoch  6, batch    35 | loss: 5.8400917CurrentTrain: epoch  6, batch    36 | loss: 6.2361851CurrentTrain: epoch  6, batch    37 | loss: 6.9756365CurrentTrain: epoch  7, batch     0 | loss: 5.6837893CurrentTrain: epoch  7, batch     1 | loss: 6.2232857CurrentTrain: epoch  7, batch     2 | loss: 5.4809313CurrentTrain: epoch  7, batch     3 | loss: 5.9693060CurrentTrain: epoch  7, batch     4 | loss: 5.9443278CurrentTrain: epoch  7, batch     5 | loss: 6.9470301CurrentTrain: epoch  7, batch     6 | loss: 5.6786346CurrentTrain: epoch  7, batch     7 | loss: 5.6518669CurrentTrain: epoch  7, batch     8 | loss: 5.7072372CurrentTrain: epoch  7, batch     9 | loss: 6.0851355CurrentTrain: epoch  7, batch    10 | loss: 5.6976013CurrentTrain: epoch  7, batch    11 | loss: 5.2051640CurrentTrain: epoch  7, batch    12 | loss: 5.5486217CurrentTrain: epoch  7, batch    13 | loss: 5.4675603CurrentTrain: epoch  7, batch    14 | loss: 5.9228482CurrentTrain: epoch  7, batch    15 | loss: 5.9310579CurrentTrain: epoch  7, batch    16 | loss: 5.7526588CurrentTrain: epoch  7, batch    17 | loss: 6.1447248CurrentTrain: epoch  7, batch    18 | loss: 5.3684263CurrentTrain: epoch  7, batch    19 | loss: 5.3706541CurrentTrain: epoch  7, batch    20 | loss: 5.7163453CurrentTrain: epoch  7, batch    21 | loss: 5.8167772CurrentTrain: epoch  7, batch    22 | loss: 5.4721346CurrentTrain: epoch  7, batch    23 | loss: 5.4974918CurrentTrain: epoch  7, batch    24 | loss: 5.8181210CurrentTrain: epoch  7, batch    25 | loss: 5.8046269CurrentTrain: epoch  7, batch    26 | loss: 5.5269685CurrentTrain: epoch  7, batch    27 | loss: 5.3612480CurrentTrain: epoch  7, batch    28 | loss: 6.1330037CurrentTrain: epoch  7, batch    29 | loss: 5.4138794CurrentTrain: epoch  7, batch    30 | loss: 5.2935381CurrentTrain: epoch  7, batch    31 | loss: 5.6074176CurrentTrain: epoch  7, batch    32 | loss: 5.3989935CurrentTrain: epoch  7, batch    33 | loss: 5.4194050CurrentTrain: epoch  7, batch    34 | loss: 5.3235569CurrentTrain: epoch  7, batch    35 | loss: 5.3250389CurrentTrain: epoch  7, batch    36 | loss: 5.6419482CurrentTrain: epoch  7, batch    37 | loss: 5.4440842CurrentTrain: epoch  8, batch     0 | loss: 5.2856121CurrentTrain: epoch  8, batch     1 | loss: 5.2385473CurrentTrain: epoch  8, batch     2 | loss: 5.5571017CurrentTrain: epoch  8, batch     3 | loss: 4.9733720CurrentTrain: epoch  8, batch     4 | loss: 5.1984406CurrentTrain: epoch  8, batch     5 | loss: 5.1471534CurrentTrain: epoch  8, batch     6 | loss: 5.8091464CurrentTrain: epoch  8, batch     7 | loss: 5.4850521CurrentTrain: epoch  8, batch     8 | loss: 5.3404222CurrentTrain: epoch  8, batch     9 | loss: 5.2892570CurrentTrain: epoch  8, batch    10 | loss: 5.3285561CurrentTrain: epoch  8, batch    11 | loss: 5.6000309CurrentTrain: epoch  8, batch    12 | loss: 5.0002718CurrentTrain: epoch  8, batch    13 | loss: 5.0705442CurrentTrain: epoch  8, batch    14 | loss: 5.2610369CurrentTrain: epoch  8, batch    15 | loss: 5.2340117CurrentTrain: epoch  8, batch    16 | loss: 5.4627805CurrentTrain: epoch  8, batch    17 | loss: 5.5060415CurrentTrain: epoch  8, batch    18 | loss: 5.3909001CurrentTrain: epoch  8, batch    19 | loss: 5.5343857CurrentTrain: epoch  8, batch    20 | loss: 5.5931225CurrentTrain: epoch  8, batch    21 | loss: 5.1878543CurrentTrain: epoch  8, batch    22 | loss: 5.4176631CurrentTrain: epoch  8, batch    23 | loss: 5.1917839CurrentTrain: epoch  8, batch    24 | loss: 5.4696736CurrentTrain: epoch  8, batch    25 | loss: 5.0745902CurrentTrain: epoch  8, batch    26 | loss: 5.0271635CurrentTrain: epoch  8, batch    27 | loss: 5.3362484CurrentTrain: epoch  8, batch    28 | loss: 5.0859451CurrentTrain: epoch  8, batch    29 | loss: 4.9816971CurrentTrain: epoch  8, batch    30 | loss: 5.5265417CurrentTrain: epoch  8, batch    31 | loss: 4.9324970CurrentTrain: epoch  8, batch    32 | loss: 5.3377724CurrentTrain: epoch  8, batch    33 | loss: 5.1076684CurrentTrain: epoch  8, batch    34 | loss: 5.0399799CurrentTrain: epoch  8, batch    35 | loss: 5.0620537CurrentTrain: epoch  8, batch    36 | loss: 5.2520885CurrentTrain: epoch  8, batch    37 | loss: 4.9834476CurrentTrain: epoch  9, batch     0 | loss: 5.0927091CurrentTrain: epoch  9, batch     1 | loss: 5.3747177CurrentTrain: epoch  9, batch     2 | loss: 5.0080404CurrentTrain: epoch  9, batch     3 | loss: 5.2270780CurrentTrain: epoch  9, batch     4 | loss: 5.3365273CurrentTrain: epoch  9, batch     5 | loss: 4.9548821CurrentTrain: epoch  9, batch     6 | loss: 5.1370268CurrentTrain: epoch  9, batch     7 | loss: 4.9919939CurrentTrain: epoch  9, batch     8 | loss: 5.5906172CurrentTrain: epoch  9, batch     9 | loss: 5.1761227CurrentTrain: epoch  9, batch    10 | loss: 4.9260230CurrentTrain: epoch  9, batch    11 | loss: 5.0948191CurrentTrain: epoch  9, batch    12 | loss: 4.9464931CurrentTrain: epoch  9, batch    13 | loss: 4.8580418CurrentTrain: epoch  9, batch    14 | loss: 4.9454055CurrentTrain: epoch  9, batch    15 | loss: 5.0788774CurrentTrain: epoch  9, batch    16 | loss: 4.9409108CurrentTrain: epoch  9, batch    17 | loss: 5.0904779CurrentTrain: epoch  9, batch    18 | loss: 4.9913402CurrentTrain: epoch  9, batch    19 | loss: 5.0082459CurrentTrain: epoch  9, batch    20 | loss: 4.9691687CurrentTrain: epoch  9, batch    21 | loss: 5.5726471CurrentTrain: epoch  9, batch    22 | loss: 4.9477377CurrentTrain: epoch  9, batch    23 | loss: 5.2683353CurrentTrain: epoch  9, batch    24 | loss: 4.9097123CurrentTrain: epoch  9, batch    25 | loss: 5.0382299CurrentTrain: epoch  9, batch    26 | loss: 5.2328558CurrentTrain: epoch  9, batch    27 | loss: 4.7415085CurrentTrain: epoch  9, batch    28 | loss: 4.9826465CurrentTrain: epoch  9, batch    29 | loss: 4.8376904CurrentTrain: epoch  9, batch    30 | loss: 4.8326249CurrentTrain: epoch  9, batch    31 | loss: 4.7917657CurrentTrain: epoch  9, batch    32 | loss: 5.1819310CurrentTrain: epoch  9, batch    33 | loss: 4.9272890CurrentTrain: epoch  9, batch    34 | loss: 5.0535312CurrentTrain: epoch  9, batch    35 | loss: 5.0866890CurrentTrain: epoch  9, batch    36 | loss: 4.8901834CurrentTrain: epoch  9, batch    37 | loss: 4.9492393
#### person countries of residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person countries of residence" means a person is living in a country.
Here is an example:
Relation: person countries of residence
Context: representative of iran 's supreme leader in the supreme national security council -lrb- snsc -rrb- ali larijani made the remarks at a press conference while responding to a question on iran 's stance on an arab league -lrb- al -rrb- plan to elect lebanese army chief michel suleiman as president , form a national unity government and approve a new election law in lebanon .
Head Entity: ali larijani
Tail Entity: iran
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person countries of residence":
Relation: person countries of residence  
Context: After years of living in the United States, the renowned artist Frida Kahlo returned to Mexico, where she found inspiration in her homeland's vibrant culture and landscapes.  
Head Entity: Frida Kahlo  
Tail Entity: Mexico  

Relation: person countries of residence  
Context: Following his studies in the UK, the famous physicist Albert Einstein settled in Switzerland, where he developed many of his groundbreaking theories.  
Head Entity: Albert Einstein  
Tail Entity: Switzerland  
#### organization top members employees ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization top members employees" means an organization 's employer is a person.
Here is an example:
Relation: organization top members employees
Context: kao charng , vice chairman of the cabinet-level mainland affairs council -lrb- mac -rrb- , explained that the termination clause was added to give both sides an option after the implementation of the tariff-cutting economic cooperation framework agreement -lrb- ecfa -rrb- .
Head Entity: mainland affairs council
Tail Entity: kao charng
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization top members employees":
Relation: organization top members employees  
Context: during the annual meeting, the board of directors recognized john smith for his outstanding contributions to the company and announced his promotion to senior vice president.  
Head Entity: company  
Tail Entity: john smith  

Relation: organization top members employees  
Context: at the charity gala, the ceo of the foundation praised sarah jones for her dedication and leadership in various community projects over the past year.  
Head Entity: foundation  
Tail Entity: sarah jones  
#### organization member of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization member of" means an organization is the member of an organization.
Here is an example:
Relation: organization member of
Context: south africa 1-5 leyds cnr biccard streets cosatu 's concern has been that the scorpions ' existence as a branch of the national directorate of public prosecutions blurred the necessary separation of functions between those investigating crime and those prosecuting the criminals .
Head Entity: scorpions
Tail Entity: national directorate of public prosecutions
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization member of":
Relation: organization member of  
Context: The International Federation of Red Cross and Red Crescent Societies (IFRC) is a global humanitarian organization that coordinates the activities of national Red Cross and Red Crescent societies around the world.  
Head Entity: International Federation of Red Cross and Red Crescent Societies  
Tail Entity: national Red Cross and Red Crescent societies  

Relation: organization member of  
Context: The European Union (EU) is a political and economic union of member states that are located primarily in Europe, working together to promote peace and stability in the region.  
Head Entity: European Union  
Tail Entity: member states  
#### person origin ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person origin" means a person 's nationality is a country.
Here is an example:
Relation: person origin
Context: the former detainee , murat kurnaz , was turned over to german authorities and freed in 2006 after a personal plea from german chancellor angela merkel .
Head Entity: murat kurnaz
Tail Entity: german
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person origin":
Relation: person origin  
Context: the renowned physicist, albert einstein, was born in the kingdom of wurttemberg in the german empire in 1879.  
Head Entity: albert einstein  
Tail Entity: german  

Relation: person origin  
Context: the famous author, gabriel garcía márquez, hailed from columbia, where he was born and raised.  
Head Entity: gabriel garcía márquez  
Tail Entity: columbian  
#### person title ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person title" means a person 's title is a title.
Here is an example:
Relation: person title
Context: earlier , in jerusalem , he spoke at the state funeral for the city 's fabled former mayor , teddy kollek , who died tuesday at 95 and was buried in the area of the mount herzl cemetery reserved for israel 's leaders .
Head Entity: teddy kollek
Tail Entity: mayor
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person title":
Relation: person title  
Context: During the award ceremony, the renowned scientist was honored for his groundbreaking research in genetics, and the audience celebrated the achievements of Dr. Jane Smith, who has been a leading figure in the field.  
Head Entity: Dr. Jane Smith  
Tail Entity: scientist  

Relation: person title  
Context: At the annual conference, the keynote speaker, a prominent author, captivated the audience with her insights on modern literature, and many attendees were eager to hear from Professor Emily Johnson, a celebrated novelist.  
Head Entity: Professor Emily Johnson  
Tail Entity: novelist  
#### organization country of headquarters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization country of headquarters" means an organization is located in a country.
Here is an example:
Relation: organization country of headquarters
Context: troubled us bond insurer mbia said it had gained a one billion - dollar capital injection from warburg pincus , a private equity firm , to help boost its finances following losses on mortgage - related securities .
Head Entity: mbia
Tail Entity: us
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization country of headquarters":
Relation: organization country of headquarters  
Context: the multinational technology company apple inc. has its headquarters in cupertino, california, where it designs and manufactures consumer electronics and software.  
Head Entity: apple inc.  
Tail Entity: california  

Relation: organization country of headquarters  
Context: the renowned automotive manufacturer toyota motor corporation is headquartered in toyota city, japan, and is known for its innovative approach to vehicle production.  
Head Entity: toyota motor corporation  
Tail Entity: japan  
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    1 | acc: 68.75%,  total acc: 65.62%   [EVAL] batch:    2 | acc: 68.75%,  total acc: 66.67%   [EVAL] batch:    3 | acc: 62.50%,  total acc: 65.62%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 66.25%   [EVAL] batch:    5 | acc: 68.75%,  total acc: 66.67%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 70.54%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 74.22%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 76.39%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 78.12%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 80.11%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 81.77%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 82.69%   [EVAL] batch:   13 | acc: 75.00%,  total acc: 82.14%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 81.67%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 80.08%   [EVAL] batch:   16 | acc: 81.25%,  total acc: 80.15%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 79.51%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 80.26%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 80.62%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 81.55%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 82.39%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 83.15%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 83.59%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 84.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 84.86%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 85.19%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 85.71%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 86.21%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 86.04%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 86.09%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 86.33%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 84.85%   
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    1 | acc: 68.75%,  total acc: 65.62%   [EVAL] batch:    2 | acc: 68.75%,  total acc: 66.67%   [EVAL] batch:    3 | acc: 62.50%,  total acc: 65.62%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 66.25%   [EVAL] batch:    5 | acc: 68.75%,  total acc: 66.67%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 70.54%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 74.22%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 76.39%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 78.12%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 80.11%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 81.77%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 82.69%   [EVAL] batch:   13 | acc: 75.00%,  total acc: 82.14%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 81.67%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 80.08%   [EVAL] batch:   16 | acc: 81.25%,  total acc: 80.15%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 79.51%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 80.26%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 80.62%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 81.55%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 82.39%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 83.15%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 83.59%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 84.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 84.86%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 85.19%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 85.71%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 86.21%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 86.04%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 86.09%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 86.33%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 84.85%   
cur_acc:  ['0.8485']
his_acc:  ['0.8485']
CurrentTrain: epoch  0, batch     0 | loss: 4.8072972CurrentTrain: epoch  0, batch     1 | loss: 5.6814704CurrentTrain: epoch  1, batch     0 | loss: 4.6956010CurrentTrain: epoch  1, batch     1 | loss: 4.3663549CurrentTrain: epoch  2, batch     0 | loss: 3.6475840CurrentTrain: epoch  2, batch     1 | loss: 3.5408037CurrentTrain: epoch  3, batch     0 | loss: 3.5502565CurrentTrain: epoch  3, batch     1 | loss: 3.4837799CurrentTrain: epoch  4, batch     0 | loss: 3.5255585CurrentTrain: epoch  4, batch     1 | loss: 2.9129133CurrentTrain: epoch  5, batch     0 | loss: 2.9713130CurrentTrain: epoch  5, batch     1 | loss: 2.7721992CurrentTrain: epoch  6, batch     0 | loss: 2.8214240CurrentTrain: epoch  6, batch     1 | loss: 2.5570168CurrentTrain: epoch  7, batch     0 | loss: 2.6739879CurrentTrain: epoch  7, batch     1 | loss: 2.5076032CurrentTrain: epoch  8, batch     0 | loss: 2.1929967CurrentTrain: epoch  8, batch     1 | loss: 2.7729809CurrentTrain: epoch  9, batch     0 | loss: 2.5371125CurrentTrain: epoch  9, batch     1 | loss: 2.1170354
#### person cause of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person cause of death" means a person was died of an event.
Here is an example:
Relation: person cause of death
Context: mr scheider had suffered from multiple myeloma for several years , and died of complications from a staph infection , his wife , brenda siemer , said .
Head Entity: mr scheider
Tail Entity: complications from a staph infection
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person cause of death":
Relation: person cause of death  
Context: after battling with lung cancer for over a year, john doe passed away peacefully in his sleep, surrounded by family.  
Head Entity: john doe  
Tail Entity: lung cancer  

Relation: person cause of death  
Context: the renowned scientist, dr. emily white, tragically lost her life due to a car accident while returning from a conference.  
Head Entity: dr. emily white  
Tail Entity: car accident  
#### organization political religious affiliation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization political religious affiliation" means an organization is the member of a religion.
Here is an example:
Relation: organization political religious affiliation
Context: the militia has also suffered from allegations of being involved in clashes with rival shiite militias , particularly the badr corp , the military wing linked to supreme iraqi islamic council , the party led by powerful politician abdel aziz al-hakim .
Head Entity: badr corp
Tail Entity: shiite
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization political religious affiliation":
Relation: organization political religious affiliation  
Context: The Christian Democratic Union has been a significant player in German politics, often aligning itself with various religious groups to promote policies that reflect Christian values.  
Head Entity: Christian Democratic Union  
Tail Entity: Christian  

Relation: organization political religious affiliation  
Context: The Islamic Society of North America has been actively involved in advocating for Muslim rights and representation in the political landscape of the United States.  
Head Entity: Islamic Society of North America  
Tail Entity: Muslim  
#### organization stateorprovince of headquarters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization stateorprovince of headquarters" means an organization is located in a state or province.
Here is an example:
Relation: organization stateorprovince of headquarters
Context: based in armonk , new york , mbia insures $ 670 billion -lrb- euro452 .18 billion -rrb- in debt .
Head Entity: mbia
Tail Entity: new york
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization stateorprovince of headquarters":
Relation: organization stateorprovince of headquarters  
Context: the tech giant apple inc. has its headquarters in cupertino, california, where it develops innovative products.  
Head Entity: apple inc.  
Tail Entity: california  

Relation: organization stateorprovince of headquarters  
Context: the multinational corporation samsung electronics is headquartered in suwon, south korea, and is a leader in consumer electronics.  
Head Entity: samsung electronics  
Tail Entity: south korea  
#### person other family ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person other family" means a person 's relative is a person.
Here is an example:
Relation: person other family
Context: former american idol contestant , sanjaya malakar and hills starlet , holly montag , have been getting close since she replaced her sister and brother-in-law , heidi montag and spencer pratt , on i 'm a celebrity .
Head Entity: spencer pratt ,
Tail Entity: holly montag
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person other family":
Relation: person other family  
Context: in a recent interview, actress jennifer aniston spoke fondly of her brother, alex aniston, highlighting their close bond and shared childhood memories.  
Head Entity: alex aniston  
Tail Entity: jennifer aniston  

Relation: person other family  
Context: during the family reunion, uncle bob introduced his niece, sarah, to the rest of the family, emphasizing how proud he was of her accomplishments.  
Head Entity: uncle bob  
Tail Entity: sarah  
#### person city of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person city of death" means a person was died in a city.
Here is an example:
Relation: person city of death
Context: irene morgan kirkaldy , 90 , who died of alzheimer 's disease aug. 10 at her home in gloucester , va. , quietly changed history in 1944 when she refused to give up her seat on a crowded greyhound bus to a white couple .
Head Entity: her
Tail Entity: gloucester
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person city of death":
Relation: person city of death  
Context: john smith, a renowned author, passed away on march 5 in his apartment located in new york city, leaving behind a legacy of literary works that inspired many.  
Head Entity: john smith  
Tail Entity: new york city  

Relation: person city of death  
Context: after a long battle with cancer, elizabeth taylor died on march 23 at a hospital in los angeles, where she had spent her final days surrounded by family and friends.  
Head Entity: elizabeth taylor  
Tail Entity: los angeles  
Mixup data size:  105
MixupTrain:  epoch  0, batch     0 | loss: 6.9038793MixupTrain:  epoch  0, batch     1 | loss: 6.0666177MixupTrain:  epoch  0, batch     2 | loss: 5.5098594MixupTrain:  epoch  0, batch     3 | loss: 4.9563089MixupTrain:  epoch  0, batch     4 | loss: 5.3947318MixupTrain:  epoch  0, batch     5 | loss: 5.4192436MixupTrain:  epoch  0, batch     6 | loss: 5.3076803
MemoryTrain:  epoch  0, batch     0 | loss: 4.8800678MemoryTrain:  epoch  0, batch     1 | loss: 4.7935696MemoryTrain:  epoch  0, batch     2 | loss: 4.2451191MemoryTrain:  epoch  1, batch     0 | loss: 4.1455750MemoryTrain:  epoch  1, batch     1 | loss: 5.1026649MemoryTrain:  epoch  1, batch     2 | loss: 5.2097359MemoryTrain:  epoch  2, batch     0 | loss: 3.8255010MemoryTrain:  epoch  2, batch     1 | loss: 3.8231571MemoryTrain:  epoch  2, batch     2 | loss: 3.1541975MemoryTrain:  epoch  3, batch     0 | loss: 3.2757537MemoryTrain:  epoch  3, batch     1 | loss: 4.1593032MemoryTrain:  epoch  3, batch     2 | loss: 5.8999162MemoryTrain:  epoch  4, batch     0 | loss: 3.3206763MemoryTrain:  epoch  4, batch     1 | loss: 3.5752110MemoryTrain:  epoch  4, batch     2 | loss: 1.6096550MemoryTrain:  epoch  5, batch     0 | loss: 3.8679347MemoryTrain:  epoch  5, batch     1 | loss: 2.7982345MemoryTrain:  epoch  5, batch     2 | loss: 1.3306086MemoryTrain:  epoch  6, batch     0 | loss: 3.4616342MemoryTrain:  epoch  6, batch     1 | loss: 2.7866914MemoryTrain:  epoch  6, batch     2 | loss: 1.2259259MemoryTrain:  epoch  7, batch     0 | loss: 3.3993740MemoryTrain:  epoch  7, batch     1 | loss: 2.8036075MemoryTrain:  epoch  7, batch     2 | loss: 1.1350079MemoryTrain:  epoch  8, batch     0 | loss: 2.9495053MemoryTrain:  epoch  8, batch     1 | loss: 2.5760868MemoryTrain:  epoch  8, batch     2 | loss: 2.4267042MemoryTrain:  epoch  9, batch     0 | loss: 2.4535861MemoryTrain:  epoch  9, batch     1 | loss: 2.8614450MemoryTrain:  epoch  9, batch     2 | loss: 1.9274349
[EVAL] batch:    0 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 96.88%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 95.83%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 89.06%   [EVAL] batch:    4 | acc: 31.25%,  total acc: 77.50%   [EVAL] batch:    5 | acc: 25.00%,  total acc: 68.75%   [EVAL] batch:    6 | acc: 62.50%,  total acc: 67.86%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 71.09%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 72.92%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 74.38%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 76.70%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 77.60%   [EVAL] batch:   12 | acc: 50.00%,  total acc: 75.48%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 68.75%,  total acc: 75.00%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 79.17%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 78.12%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 80.00%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 79.17%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 83.59%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 84.72%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 85.62%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 86.93%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 86.98%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 87.02%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 85.71%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 85.00%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 83.20%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 82.72%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 81.60%   [EVAL] batch:   18 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 81.56%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 82.44%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 83.24%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 83.97%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 85.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 85.58%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 85.88%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 86.38%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 86.64%   [EVAL] batch:   29 | acc: 75.00%,  total acc: 86.25%   [EVAL] batch:   30 | acc: 81.25%,  total acc: 86.09%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 86.33%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 86.74%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 86.95%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 87.14%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 87.15%   [EVAL] batch:   36 | acc: 37.50%,  total acc: 85.81%   [EVAL] batch:   37 | acc: 31.25%,  total acc: 84.38%   [EVAL] batch:   38 | acc: 43.75%,  total acc: 83.33%   [EVAL] batch:   39 | acc: 81.25%,  total acc: 83.28%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 83.69%   [EVAL] batch:   41 | acc: 75.00%,  total acc: 83.48%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 83.87%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 83.95%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 84.03%   
cur_acc:  ['0.8485', '0.7548']
his_acc:  ['0.8485', '0.8403']
CurrentTrain: epoch  0, batch     0 | loss: 5.2259703CurrentTrain: epoch  0, batch     1 | loss: 7.4519620CurrentTrain: epoch  1, batch     0 | loss: 5.9195337CurrentTrain: epoch  1, batch     1 | loss: 4.1097250CurrentTrain: epoch  2, batch     0 | loss: 4.8889661CurrentTrain: epoch  2, batch     1 | loss: 4.8818774CurrentTrain: epoch  3, batch     0 | loss: 4.7213154CurrentTrain: epoch  3, batch     1 | loss: 4.2180390CurrentTrain: epoch  4, batch     0 | loss: 4.1393547CurrentTrain: epoch  4, batch     1 | loss: 3.8293347CurrentTrain: epoch  5, batch     0 | loss: 3.8634362CurrentTrain: epoch  5, batch     1 | loss: 3.6234584CurrentTrain: epoch  6, batch     0 | loss: 3.4779959CurrentTrain: epoch  6, batch     1 | loss: 3.4022999CurrentTrain: epoch  7, batch     0 | loss: 3.4089205CurrentTrain: epoch  7, batch     1 | loss: 2.9742889CurrentTrain: epoch  8, batch     0 | loss: 3.0591266CurrentTrain: epoch  8, batch     1 | loss: 3.4493845CurrentTrain: epoch  9, batch     0 | loss: 2.8954871CurrentTrain: epoch  9, batch     1 | loss: 3.8021071
#### person stateorprovinces of residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person stateorprovinces of residence" means a person is living in a state or province.
Here is an example:
Relation: person stateorprovinces of residence
Context: ny-schools-chief -lrb- new york -rrb- -- cathleen p. black , mayor michael r. bloomberg 's choice to be the next chancellor of new york city 's public-school system , has during more than 40 years in the media business broken numerous glass ceilings -- and amassed a fortune -- with quick and cold-blooded decision making , crystal-clear goal setting , and an all-surpassing attention to the bottom line .
Head Entity: cathleen p. black
Tail Entity: new york
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person stateorprovinces of residence":
Relation: person stateorprovinces of residence  
Context: after years of living in the bustling city, actor john doe has decided to settle down in the serene landscapes of oregon, where he can enjoy a quieter lifestyle away from the spotlight.  
Head Entity: john doe  
Tail Entity: oregon  

Relation: person stateorprovinces of residence  
Context: renowned author jane smith has recently moved to the vibrant city of los angeles, drawn by its rich cultural scene and opportunities for collaboration with other creatives.  
Head Entity: jane smith  
Tail Entity: los angeles  
#### person date of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person date of death" means a person was died on a date.
Here is an example:
Relation: person date of death
Context: he was taken off life support on feb. 14 .
Head Entity: he
Tail Entity: feb. 14
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person date of death":
Relation: person date of death  
Context: The renowned author passed away on July 20, 2020, after a long illness.  
Head Entity: The renowned author  
Tail Entity: July 20, 2020  

Relation: person date of death  
Context: She left this world peacefully in her sleep on March 5, 2015.  
Head Entity: She  
Tail Entity: March 5, 2015  
#### organization number of employees members ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization number of employees members" means the number of an organization 's employees is a number.
Here is an example:
Relation: organization number of employees members
Context: with the sweep of a federal regulator 's pen , massachusetts stands to gain a new life-science giant in april : covidien , a medical - supplies maker with thousands of products and more than 43,000 employees worldwide .
Head Entity: covidien
Tail Entity: 43,000
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization number of employees members":
Relation: organization number of employees members  
Context: The tech company, Innovatech, has rapidly expanded its workforce over the past year, now boasting a total of 25,000 employees across its global offices.  
Head Entity: Innovatech  
Tail Entity: 25,000  

Relation: organization number of employees members  
Context: After the merger, the newly formed entity, Global Solutions Inc., reported an impressive count of 50,000 employees, making it one of the largest firms in the industry.  
Head Entity: Global Solutions Inc.  
Tail Entity: 50,000  
#### person alternate names ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person alternate names" means a person 's alias is a person.
Here is an example:
Relation: person alternate names
Context: `` i am known in the hospice as the man who would n't die , '' buchwald wrote in march .
Head Entity: buchwald
Tail Entity: man who would n't die
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person alternate names":
Relation: person alternate names  
Context: `` the famous author often referred to as the bard of Avon, is known for his timeless plays and sonnets. ''  
Head Entity: William Shakespeare  
Tail Entity: bard of Avon  

Relation: person alternate names  
Context: `` during his career, he was affectionately called the king of pop, a title that reflected his immense influence on the music industry. ''  
Head Entity: Michael Jackson  
Tail Entity: king of pop  
#### person spouse ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person spouse" means a person 's spouse is a person.
Here is an example:
Relation: person spouse
Context: in addition to his wife , meskill is survived by two daughters , eileen gallup of new britain and maureen heneghan of haddon heights , n.j. ; three sons , john , of kensington , conn. ; peter , of east hartford , conn. ; and thomas , of branford , conn. ; two sisters , ruth prior of naples , fla. , and sister laura marie of portland , conn. ; five grandchildren , and two step-grandchildren .
Head Entity: his
Tail Entity: meskill
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person spouse":
Relation: person spouse  
Context: after a long and happy marriage, john and his wife, sarah, decided to celebrate their 50th anniversary with a grand party attended by family and friends.  
Head Entity: his  
Tail Entity: wife  

Relation: person spouse  
Context: during the interview, emily spoke fondly of her husband, who has always been her biggest supporter throughout her career.  
Head Entity: her  
Tail Entity: husband  
Mixup data size:  134
MixupTrain:  epoch  0, batch     0 | loss: 5.3852635MixupTrain:  epoch  0, batch     1 | loss: 3.7316320MixupTrain:  epoch  0, batch     2 | loss: 4.5230116MixupTrain:  epoch  0, batch     3 | loss: 4.0317010MixupTrain:  epoch  0, batch     4 | loss: 3.7687327MixupTrain:  epoch  0, batch     5 | loss: 3.8811059MixupTrain:  epoch  0, batch     6 | loss: 3.4402117MixupTrain:  epoch  0, batch     7 | loss: 3.8116382MixupTrain:  epoch  0, batch     8 | loss: 3.6141396
MemoryTrain:  epoch  0, batch     0 | loss: 4.4860668MemoryTrain:  epoch  0, batch     1 | loss: 3.4781127MemoryTrain:  epoch  0, batch     2 | loss: 3.9465263MemoryTrain:  epoch  1, batch     0 | loss: 2.8803053MemoryTrain:  epoch  1, batch     1 | loss: 4.1281710MemoryTrain:  epoch  1, batch     2 | loss: 3.3333192MemoryTrain:  epoch  2, batch     0 | loss: 2.8891246MemoryTrain:  epoch  2, batch     1 | loss: 3.2319317MemoryTrain:  epoch  2, batch     2 | loss: 3.5425043MemoryTrain:  epoch  3, batch     0 | loss: 2.7603047MemoryTrain:  epoch  3, batch     1 | loss: 3.0373955MemoryTrain:  epoch  3, batch     2 | loss: 4.0467243MemoryTrain:  epoch  4, batch     0 | loss: 3.2977514MemoryTrain:  epoch  4, batch     1 | loss: 2.8732443MemoryTrain:  epoch  4, batch     2 | loss: 2.4230978MemoryTrain:  epoch  5, batch     0 | loss: 2.2766850MemoryTrain:  epoch  5, batch     1 | loss: 2.8433843MemoryTrain:  epoch  5, batch     2 | loss: 2.5892153MemoryTrain:  epoch  6, batch     0 | loss: 2.2587028MemoryTrain:  epoch  6, batch     1 | loss: 2.1012926MemoryTrain:  epoch  6, batch     2 | loss: 2.9918308MemoryTrain:  epoch  7, batch     0 | loss: 2.0693359MemoryTrain:  epoch  7, batch     1 | loss: 2.2140307MemoryTrain:  epoch  7, batch     2 | loss: 2.6566510MemoryTrain:  epoch  8, batch     0 | loss: 2.4808948MemoryTrain:  epoch  8, batch     1 | loss: 1.8557172MemoryTrain:  epoch  8, batch     2 | loss: 2.3558192MemoryTrain:  epoch  9, batch     0 | loss: 2.3087521MemoryTrain:  epoch  9, batch     1 | loss: 2.1770277MemoryTrain:  epoch  9, batch     2 | loss: 2.2572966
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    1 | acc: 43.75%,  total acc: 53.12%   [EVAL] batch:    2 | acc: 50.00%,  total acc: 52.08%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 57.81%   [EVAL] batch:    4 | acc: 56.25%,  total acc: 57.50%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 63.54%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 68.75%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 72.66%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 74.31%   [EVAL] batch:    9 | acc: 56.25%,  total acc: 72.50%   [EVAL] batch:   10 | acc: 31.25%,  total acc: 68.75%   [EVAL] batch:   11 | acc: 25.00%,  total acc: 65.10%   [EVAL] batch:   12 | acc: 25.00%,  total acc: 62.02%   [EVAL] batch:   13 | acc: 56.25%,  total acc: 61.61%   [EVAL] batch:   14 | acc: 12.50%,  total acc: 58.33%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 50.00%,  total acc: 62.50%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 66.67%   [EVAL] batch:    3 | acc: 62.50%,  total acc: 65.62%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 68.75%   [EVAL] batch:    5 | acc: 62.50%,  total acc: 67.71%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 71.43%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 77.78%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 79.38%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 82.29%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 82.69%   [EVAL] batch:   13 | acc: 75.00%,  total acc: 82.14%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 81.67%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 80.08%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 79.78%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 79.17%   [EVAL] batch:   18 | acc: 75.00%,  total acc: 78.95%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 79.38%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 80.36%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 82.07%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 82.55%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 83.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 83.89%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 84.26%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 84.82%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 85.34%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 85.42%   [EVAL] batch:   30 | acc: 81.25%,  total acc: 85.28%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 85.55%   [EVAL] batch:   32 | acc: 87.50%,  total acc: 85.61%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 85.85%   [EVAL] batch:   34 | acc: 87.50%,  total acc: 85.89%   [EVAL] batch:   35 | acc: 81.25%,  total acc: 85.76%   [EVAL] batch:   36 | acc: 37.50%,  total acc: 84.46%   [EVAL] batch:   37 | acc: 25.00%,  total acc: 82.89%   [EVAL] batch:   38 | acc: 25.00%,  total acc: 81.41%   [EVAL] batch:   39 | acc: 68.75%,  total acc: 81.09%   [EVAL] batch:   40 | acc: 50.00%,  total acc: 80.34%   [EVAL] batch:   41 | acc: 56.25%,  total acc: 79.76%   [EVAL] batch:   42 | acc: 75.00%,  total acc: 79.65%   [EVAL] batch:   43 | acc: 68.75%,  total acc: 79.40%   [EVAL] batch:   44 | acc: 56.25%,  total acc: 78.89%   [EVAL] batch:   45 | acc: 56.25%,  total acc: 78.40%   [EVAL] batch:   46 | acc: 43.75%,  total acc: 77.66%   [EVAL] batch:   47 | acc: 56.25%,  total acc: 77.21%   [EVAL] batch:   48 | acc: 75.00%,  total acc: 77.17%   [EVAL] batch:   49 | acc: 50.00%,  total acc: 76.62%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 77.08%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 77.52%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 77.95%   [EVAL] batch:   53 | acc: 87.50%,  total acc: 78.12%   [EVAL] batch:   54 | acc: 50.00%,  total acc: 77.61%   [EVAL] batch:   55 | acc: 37.50%,  total acc: 76.90%   [EVAL] batch:   56 | acc: 18.75%,  total acc: 75.88%   [EVAL] batch:   57 | acc: 25.00%,  total acc: 75.00%   [EVAL] batch:   58 | acc: 56.25%,  total acc: 74.68%   [EVAL] batch:   59 | acc: 12.50%,  total acc: 73.65%   
cur_acc:  ['0.8485', '0.7548', '0.5833']
his_acc:  ['0.8485', '0.8403', '0.7365']
CurrentTrain: epoch  0, batch     0 | loss: 7.8592453CurrentTrain: epoch  0, batch     1 | loss: 7.4475608CurrentTrain: epoch  1, batch     0 | loss: 7.0758643CurrentTrain: epoch  1, batch     1 | loss: 6.8334746CurrentTrain: epoch  2, batch     0 | loss: 6.5877895CurrentTrain: epoch  2, batch     1 | loss: 6.3186207CurrentTrain: epoch  3, batch     0 | loss: 6.3974962CurrentTrain: epoch  3, batch     1 | loss: 5.6728048CurrentTrain: epoch  4, batch     0 | loss: 6.1441426CurrentTrain: epoch  4, batch     1 | loss: 5.2459197CurrentTrain: epoch  5, batch     0 | loss: 5.2768764CurrentTrain: epoch  5, batch     1 | loss: 5.2108755CurrentTrain: epoch  6, batch     0 | loss: 5.0556784CurrentTrain: epoch  6, batch     1 | loss: 4.8676815CurrentTrain: epoch  7, batch     0 | loss: 4.8971343CurrentTrain: epoch  7, batch     1 | loss: 5.0761189CurrentTrain: epoch  8, batch     0 | loss: 4.8645401CurrentTrain: epoch  8, batch     1 | loss: 4.6393304CurrentTrain: epoch  9, batch     0 | loss: 4.5295620CurrentTrain: epoch  9, batch     1 | loss: 4.6371188
#### organization subsidiaries ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization subsidiaries" means an organization 's subsidiary is an organization.
Here is an example:
Relation: organization subsidiaries
Context: venture fund buys sporting chain highland capital 's consumer fund includes lululemon athletica , a yoga retailer , and o beverages , a flavored water company developed by tom first , one of the two `` juice guys '' who cofounded nantucket nectars .
Head Entity: highland capital
Tail Entity: o beverages
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization subsidiaries":
Relation: organization subsidiaries  
Context: The tech giant Alphabet Inc. has several subsidiaries, including YouTube, which has become a leading platform for video content, and Waymo, which focuses on self-driving technology.  
Head Entity: Alphabet Inc.  
Tail Entity: Waymo  

Relation: organization subsidiaries  
Context: The automotive manufacturer Ford Motor Company has expanded its reach by acquiring several companies, including the electric vehicle startup Rivian, which aims to revolutionize the EV market.  
Head Entity: Ford Motor Company  
Tail Entity: Rivian  
#### organization parents ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization parents" means an organization 's parent is an organization.
Here is an example:
Relation: organization parents
Context: aseel kami , tim cocks , abdul-rahman taher , tim cocks and michael christie -lrb- reuters -rrb- add an intriguing note in the speculation over who 's responsible for the recent bombings : `` but a senior iraqi intelligence source , who declined to be named , said there was evidence the bombs could be the work of the militant badr organisation , the armed wing of the supreme islamic iraqi council -lrb- isci -rrb- .
Head Entity: badr organisation
Tail Entity: supreme islamic iraqi council
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization parents":
Relation: organization parents  
Context: in a recent merger, tech giant alpha innovations has acquired beta technologies, a leading firm in artificial intelligence solutions, which has been a subsidiary of gamma enterprises for over a decade.  
Head Entity: beta technologies  
Tail Entity: gamma enterprises  

Relation: organization parents  
Context: the renowned publishing house, literary press, has announced its acquisition of the smaller imprint, creative words, which was previously a division of the larger media group, story media.  
Head Entity: creative words  
Tail Entity: story media  
#### organization alternate names ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization alternate names" means a person 's alias is a person.
Here is an example:
Relation: organization alternate names
Context: it also needs the green light from the 45-nation nuclear suppliers group -lrb- nsg -rrb- , which regulates global civilian nuclear trade , before it can begin buying nuclear reactors and fuel .
Head Entity: nsg
Tail Entity: nuclear suppliers group
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization alternate names":
Relation: organization alternate names  
Context: The International Monetary Fund, often referred to as the IMF, plays a crucial role in global economic stability.  
Head Entity: IMF  
Tail Entity: International Monetary Fund  

Relation: organization alternate names  
Context: The World Health Organization, commonly known as WHO, has been at the forefront of the global response to health crises.  
Head Entity: WHO  
Tail Entity: World Health Organization  
#### organization city of headquarters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization city of headquarters" means an organization is located in a city.
Here is an example:
Relation: organization city of headquarters
Context: the justice and equality movement -lrb- jem -rrb- , a major rebel movement in darfur , is still insisting on its conditions for the resumption of the talks , including the release of dozens of rebel fighters who were arrested after an attack in omdurman near khartoum in may 2008 .
Head Entity: justice and equality movement
Tail Entity: darfur
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization city of headquarters":
Relation: organization city of headquarters  
Context: the tech giant apple inc. has announced plans to expand its operations in cupertino, where its headquarters is located, aiming to create thousands of new jobs in the area.  
Head Entity: apple inc.  
Tail Entity: cupertino  

Relation: organization city of headquarters  
Context: the international red cross has been providing humanitarian aid from its main office in geneva, which serves as the organization's global headquarters.  
Head Entity: international red cross  
Tail Entity: geneva  
#### person siblings ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person siblings" means a person 's sibling is a person.
Here is an example:
Relation: person siblings
Context: forsberg , a political science professor at city college of new york , died oct. 19 in a bronx hospital of cancer , said her sister , celia seupel .
Head Entity: forsberg
Tail Entity: celia seupel
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person siblings":
Relation: person siblings  
Context: during the family reunion, john introduced his sister, emily, who had just returned from studying abroad.  
Head Entity: john  
Tail Entity: emily  

Relation: person siblings  
Context: at the award ceremony, sarah proudly accepted her trophy while her brother, michael, cheered from the audience.  
Head Entity: sarah  
Tail Entity: michael  
Mixup data size:  164
MixupTrain:  epoch  0, batch     0 | loss: 4.0475849MixupTrain:  epoch  0, batch     1 | loss: 4.2143549MixupTrain:  epoch  0, batch     2 | loss: 4.0498719MixupTrain:  epoch  0, batch     3 | loss: 4.2599328MixupTrain:  epoch  0, batch     4 | loss: 4.6853629MixupTrain:  epoch  0, batch     5 | loss: 3.3988835MixupTrain:  epoch  0, batch     6 | loss: 3.6772447MixupTrain:  epoch  0, batch     7 | loss: 3.2238620MixupTrain:  epoch  0, batch     8 | loss: 3.6249067MixupTrain:  epoch  0, batch     9 | loss: 3.7727478MixupTrain:  epoch  0, batch    10 | loss: 2.5826120
MemoryTrain:  epoch  0, batch     0 | loss: 3.2485883MemoryTrain:  epoch  0, batch     1 | loss: 4.1316032MemoryTrain:  epoch  0, batch     2 | loss: 3.9467797MemoryTrain:  epoch  0, batch     3 | loss: 3.4949384MemoryTrain:  epoch  1, batch     0 | loss: 3.9825070MemoryTrain:  epoch  1, batch     1 | loss: 2.8436012MemoryTrain:  epoch  1, batch     2 | loss: 3.5937839MemoryTrain:  epoch  1, batch     3 | loss: 3.1964829MemoryTrain:  epoch  2, batch     0 | loss: 4.0518122MemoryTrain:  epoch  2, batch     1 | loss: 2.9164624MemoryTrain:  epoch  2, batch     2 | loss: 3.2571445MemoryTrain:  epoch  2, batch     3 | loss: 3.0472023MemoryTrain:  epoch  3, batch     0 | loss: 2.9335580MemoryTrain:  epoch  3, batch     1 | loss: 3.9691184MemoryTrain:  epoch  3, batch     2 | loss: 3.0152225MemoryTrain:  epoch  3, batch     3 | loss: 2.9145219MemoryTrain:  epoch  4, batch     0 | loss: 2.5960691MemoryTrain:  epoch  4, batch     1 | loss: 2.6305807MemoryTrain:  epoch  4, batch     2 | loss: 3.4679565MemoryTrain:  epoch  4, batch     3 | loss: 2.9752786MemoryTrain:  epoch  5, batch     0 | loss: 2.7387285MemoryTrain:  epoch  5, batch     1 | loss: 3.0612073MemoryTrain:  epoch  5, batch     2 | loss: 2.8656881MemoryTrain:  epoch  5, batch     3 | loss: 2.0310459MemoryTrain:  epoch  6, batch     0 | loss: 2.3213487MemoryTrain:  epoch  6, batch     1 | loss: 2.9158280MemoryTrain:  epoch  6, batch     2 | loss: 2.3996580MemoryTrain:  epoch  6, batch     3 | loss: 2.2569656MemoryTrain:  epoch  7, batch     0 | loss: 2.4072750MemoryTrain:  epoch  7, batch     1 | loss: 2.5765777MemoryTrain:  epoch  7, batch     2 | loss: 2.3533251MemoryTrain:  epoch  7, batch     3 | loss: 2.0704458MemoryTrain:  epoch  8, batch     0 | loss: 1.8540365MemoryTrain:  epoch  8, batch     1 | loss: 2.6586421MemoryTrain:  epoch  8, batch     2 | loss: 1.9087667MemoryTrain:  epoch  8, batch     3 | loss: 2.2562654MemoryTrain:  epoch  9, batch     0 | loss: 1.8233080MemoryTrain:  epoch  9, batch     1 | loss: 2.3740764MemoryTrain:  epoch  9, batch     2 | loss: 1.9419012MemoryTrain:  epoch  9, batch     3 | loss: 2.3317556
[EVAL] batch:    0 | acc: 43.75%,  total acc: 43.75%   [EVAL] batch:    1 | acc: 62.50%,  total acc: 53.12%   [EVAL] batch:    2 | acc: 31.25%,  total acc: 45.83%   [EVAL] batch:    3 | acc: 25.00%,  total acc: 40.62%   [EVAL] batch:    4 | acc: 18.75%,  total acc: 36.25%   [EVAL] batch:    5 | acc: 18.75%,  total acc: 33.33%   [EVAL] batch:    6 | acc: 37.50%,  total acc: 33.93%   [EVAL] batch:    7 | acc: 68.75%,  total acc: 38.28%   [EVAL] batch:    8 | acc: 31.25%,  total acc: 37.50%   [EVAL] batch:    9 | acc: 37.50%,  total acc: 37.50%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 41.48%   [EVAL] batch:   11 | acc: 56.25%,  total acc: 42.71%   [EVAL] batch:   12 | acc: 62.50%,  total acc: 44.23%   [EVAL] batch:   13 | acc: 56.25%,  total acc: 45.09%   [EVAL] batch:   14 | acc: 81.25%,  total acc: 47.50%   [EVAL] batch:   15 | acc: 81.25%,  total acc: 49.61%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 51.10%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 51.74%   [EVAL] batch:   18 | acc: 12.50%,  total acc: 49.67%   [EVAL] batch:   19 | acc: 12.50%,  total acc: 47.81%   [EVAL] batch:   20 | acc: 18.75%,  total acc: 46.43%   [EVAL] batch:   21 | acc: 6.25%,  total acc: 44.60%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 50.00%,  total acc: 62.50%   [EVAL] batch:    2 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    3 | acc: 50.00%,  total acc: 59.38%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 61.25%   [EVAL] batch:    5 | acc: 68.75%,  total acc: 62.50%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 66.96%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 71.09%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 74.31%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 76.25%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 78.41%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 79.17%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 78.37%   [EVAL] batch:   13 | acc: 50.00%,  total acc: 76.34%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 76.25%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 75.00%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 74.65%   [EVAL] batch:   18 | acc: 75.00%,  total acc: 74.67%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 75.31%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 76.49%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 77.56%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 78.53%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 79.17%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 80.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 80.77%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 81.92%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 82.54%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 82.71%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 83.06%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 83.40%   [EVAL] batch:   32 | acc: 87.50%,  total acc: 83.52%   [EVAL] batch:   33 | acc: 87.50%,  total acc: 83.64%   [EVAL] batch:   34 | acc: 87.50%,  total acc: 83.75%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 84.03%   [EVAL] batch:   36 | acc: 31.25%,  total acc: 82.60%   [EVAL] batch:   37 | acc: 31.25%,  total acc: 81.25%   [EVAL] batch:   38 | acc: 12.50%,  total acc: 79.49%   [EVAL] batch:   39 | acc: 37.50%,  total acc: 78.44%   [EVAL] batch:   40 | acc: 31.25%,  total acc: 77.29%   [EVAL] batch:   41 | acc: 37.50%,  total acc: 76.34%   [EVAL] batch:   42 | acc: 50.00%,  total acc: 75.73%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 75.99%   [EVAL] batch:   44 | acc: 75.00%,  total acc: 75.97%   [EVAL] batch:   45 | acc: 50.00%,  total acc: 75.41%   [EVAL] batch:   46 | acc: 50.00%,  total acc: 74.87%   [EVAL] batch:   47 | acc: 37.50%,  total acc: 74.09%   [EVAL] batch:   48 | acc: 75.00%,  total acc: 74.11%   [EVAL] batch:   49 | acc: 50.00%,  total acc: 73.62%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 74.14%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 74.64%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 75.12%   [EVAL] batch:   53 | acc: 93.75%,  total acc: 75.46%   [EVAL] batch:   54 | acc: 56.25%,  total acc: 75.11%   [EVAL] batch:   55 | acc: 31.25%,  total acc: 74.33%   [EVAL] batch:   56 | acc: 18.75%,  total acc: 73.36%   [EVAL] batch:   57 | acc: 25.00%,  total acc: 72.52%   [EVAL] batch:   58 | acc: 56.25%,  total acc: 72.25%   [EVAL] batch:   59 | acc: 37.50%,  total acc: 71.67%   [EVAL] batch:   60 | acc: 62.50%,  total acc: 71.52%   [EVAL] batch:   61 | acc: 43.75%,  total acc: 71.07%   [EVAL] batch:   62 | acc: 25.00%,  total acc: 70.34%   [EVAL] batch:   63 | acc: 25.00%,  total acc: 69.63%   [EVAL] batch:   64 | acc: 6.25%,  total acc: 68.65%   [EVAL] batch:   65 | acc: 37.50%,  total acc: 68.18%   [EVAL] batch:   66 | acc: 56.25%,  total acc: 68.00%   [EVAL] batch:   67 | acc: 43.75%,  total acc: 67.65%   [EVAL] batch:   68 | acc: 31.25%,  total acc: 67.12%   [EVAL] batch:   69 | acc: 75.00%,  total acc: 67.23%   [EVAL] batch:   70 | acc: 56.25%,  total acc: 67.08%   [EVAL] batch:   71 | acc: 62.50%,  total acc: 67.01%   [EVAL] batch:   72 | acc: 62.50%,  total acc: 66.95%   [EVAL] batch:   73 | acc: 62.50%,  total acc: 66.89%   [EVAL] batch:   74 | acc: 81.25%,  total acc: 67.08%   [EVAL] batch:   75 | acc: 93.75%,  total acc: 67.43%   [EVAL] batch:   76 | acc: 62.50%,  total acc: 67.37%   [EVAL] batch:   77 | acc: 25.00%,  total acc: 66.83%   [EVAL] batch:   78 | acc: 6.25%,  total acc: 66.06%   [EVAL] batch:   79 | acc: 18.75%,  total acc: 65.47%   [EVAL] batch:   80 | acc: 18.75%,  total acc: 64.89%   
cur_acc:  ['0.8485', '0.7548', '0.5833', '0.4460']
his_acc:  ['0.8485', '0.8403', '0.7365', '0.6489']
CurrentTrain: epoch  0, batch     0 | loss: 5.8634028CurrentTrain: epoch  0, batch     1 | loss: 6.4539609CurrentTrain: epoch  1, batch     0 | loss: 4.9581518CurrentTrain: epoch  1, batch     1 | loss: 4.7072182CurrentTrain: epoch  2, batch     0 | loss: 4.1483841CurrentTrain: epoch  2, batch     1 | loss: 4.5667396CurrentTrain: epoch  3, batch     0 | loss: 3.9477472CurrentTrain: epoch  3, batch     1 | loss: 3.9435501CurrentTrain: epoch  4, batch     0 | loss: 3.6286726CurrentTrain: epoch  4, batch     1 | loss: 4.0131860CurrentTrain: epoch  5, batch     0 | loss: 3.4851329CurrentTrain: epoch  5, batch     1 | loss: 3.4989920CurrentTrain: epoch  6, batch     0 | loss: 3.2650168CurrentTrain: epoch  6, batch     1 | loss: 3.2172656CurrentTrain: epoch  7, batch     0 | loss: 2.9961708CurrentTrain: epoch  7, batch     1 | loss: 2.8612454CurrentTrain: epoch  8, batch     0 | loss: 3.1462798CurrentTrain: epoch  8, batch     1 | loss: 2.4474242CurrentTrain: epoch  9, batch     0 | loss: 2.9037774CurrentTrain: epoch  9, batch     1 | loss: 2.6953702
#### person cities of residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person cities of residence" means a person is living in a city.
Here is an example:
Relation: person cities of residence
Context: tv-idol-johns -- atlanta -- `` american idol '' finalist michael johns moved to los angeles several years ago , but his heart is still in atlanta .
Head Entity: his
Tail Entity: atlanta
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person cities of residence":
Relation: person cities of residence  
Context: after years of living in new york city, the famous author decided to return to her hometown of boston, where she feels most at home.  
Head Entity: she  
Tail Entity: boston  

Relation: person cities of residence  
Context: despite being born in san francisco, the tech entrepreneur has spent most of his adult life in silicon valley, where he built his successful startup.  
Head Entity: he  
Tail Entity: silicon valley  
#### person schools attended ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person schools attended" means a person 's school is an organization.
Here is an example:
Relation: person schools attended
Context: born in baltimore in 1922 , parren mitchell was a graduate of morgan state college and earned a master 's degree from the university of maryland , according to biographical information supplied by cummings ' office .
Head Entity: parren mitchell
Tail Entity: university of maryland
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person schools attended":
Relation: person schools attended  
Context: after completing high school in 1995, jessica went on to study at the university of california, los angeles, where she earned her bachelor's degree in sociology.  
Head Entity: jessica  
Tail Entity: university of california, los angeles  

Relation: person schools attended  
Context: during his early years, steven attended the prestigious harvard university, where he majored in computer science and graduated with honors.  
Head Entity: steven  
Tail Entity: harvard university  
#### person country of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person country of death" means a person was died in a country.
Here is an example:
Relation: person country of death
Context: u.s. rep. parren mitchell , founding member of congressional black caucus , dies at 85
Head Entity: parren mitchell
Tail Entity: u.s.
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person country of death":
Relation: person country of death  
Context: renowned physicist stephen hawking passed away in cambridge, england at the age of 76  
Head Entity: stephen hawking  
Tail Entity: england  

Relation: person country of death  
Context: famous author gabriel garcia marquez died in mexico city, mexico, leaving behind a legacy of magical realism  
Head Entity: gabriel garcia marquez  
Tail Entity: mexico  
#### person children ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person children" means a person 's children is a person.
Here is an example:
Relation: person children
Context: he is survived by two stepdaughters , barbara a. hammond and brenda l. stevenson ; a stepson , michael a. taylor ; two grandchildren and one great-grandchild .
Head Entity: he
Tail Entity: brenda l. stevenson
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person children":
Relation: person children  
Context: she has three children, including her son, john, and her daughters, emily and sarah, who are both pursuing their studies.  
Head Entity: she  
Tail Entity: emily  

Relation: person children  
Context: after the passing of their father, the siblings, including jake and his sister, lila, took on the responsibility of caring for their younger brother, max.  
Head Entity: jake  
Tail Entity: lila  
#### person charges ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person charges" means a person was charged with an event.
Here is an example:
Relation: person charges
Context: ferrara said he was innocent of limoli 's slaying , but he pleaded guilty in 1992 to murder , along with racketeering charges , under a deal that sent him to prison for 22 years , rather than go to trial and risk a conviction that could lead to life in prison .
Head Entity: ferrara
Tail Entity: racketeering
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person charges":
Relation: person charges  
Context: After a lengthy investigation, the authorities announced that Johnson was charged with embezzlement, a crime that could result in significant prison time if convicted.  
Head Entity: Johnson  
Tail Entity: embezzlement  

Relation: person charges  
Context: The district attorney confirmed that Smith has been charged with assault following the altercation that took place last month at the downtown bar.  
Head Entity: Smith  
Tail Entity: assault  
Mixup data size:  195
MixupTrain:  epoch  0, batch     0 | loss: 3.5607656MixupTrain:  epoch  0, batch     1 | loss: 3.3030923MixupTrain:  epoch  0, batch     2 | loss: 3.2359181MixupTrain:  epoch  0, batch     3 | loss: 2.7730288MixupTrain:  epoch  0, batch     4 | loss: 3.0431464MixupTrain:  epoch  0, batch     5 | loss: 3.1913554MixupTrain:  epoch  0, batch     6 | loss: 3.3604210MixupTrain:  epoch  0, batch     7 | loss: 3.3178253MixupTrain:  epoch  0, batch     8 | loss: 3.1521248MixupTrain:  epoch  0, batch     9 | loss: 3.4342221MixupTrain:  epoch  0, batch    10 | loss: 2.8414074MixupTrain:  epoch  0, batch    11 | loss: 3.2556078MixupTrain:  epoch  0, batch    12 | loss: 2.7291317
MemoryTrain:  epoch  0, batch     0 | loss: 2.5281713MemoryTrain:  epoch  0, batch     1 | loss: 4.0435114MemoryTrain:  epoch  0, batch     2 | loss: 3.1795545MemoryTrain:  epoch  0, batch     3 | loss: 3.5504470MemoryTrain:  epoch  0, batch     4 | loss: 2.9382370MemoryTrain:  epoch  1, batch     0 | loss: 3.1935303MemoryTrain:  epoch  1, batch     1 | loss: 3.3101792MemoryTrain:  epoch  1, batch     2 | loss: 2.4975834MemoryTrain:  epoch  1, batch     3 | loss: 2.3007998MemoryTrain:  epoch  1, batch     4 | loss: 3.6140466MemoryTrain:  epoch  2, batch     0 | loss: 2.5051057MemoryTrain:  epoch  2, batch     1 | loss: 2.4564960MemoryTrain:  epoch  2, batch     2 | loss: 2.7899194MemoryTrain:  epoch  2, batch     3 | loss: 2.5634520MemoryTrain:  epoch  2, batch     4 | loss: 2.3234625MemoryTrain:  epoch  3, batch     0 | loss: 2.6252222MemoryTrain:  epoch  3, batch     1 | loss: 2.0814767MemoryTrain:  epoch  3, batch     2 | loss: 2.3439605MemoryTrain:  epoch  3, batch     3 | loss: 2.3024604MemoryTrain:  epoch  3, batch     4 | loss: 2.5227814MemoryTrain:  epoch  4, batch     0 | loss: 2.6812220MemoryTrain:  epoch  4, batch     1 | loss: 2.1563287MemoryTrain:  epoch  4, batch     2 | loss: 2.0383835MemoryTrain:  epoch  4, batch     3 | loss: 1.9661040MemoryTrain:  epoch  4, batch     4 | loss: 2.2423928MemoryTrain:  epoch  5, batch     0 | loss: 2.2637222MemoryTrain:  epoch  5, batch     1 | loss: 2.0656495MemoryTrain:  epoch  5, batch     2 | loss: 2.3378265MemoryTrain:  epoch  5, batch     3 | loss: 2.0202610MemoryTrain:  epoch  5, batch     4 | loss: 2.3086631MemoryTrain:  epoch  6, batch     0 | loss: 1.7897866MemoryTrain:  epoch  6, batch     1 | loss: 1.9693310MemoryTrain:  epoch  6, batch     2 | loss: 1.9914517MemoryTrain:  epoch  6, batch     3 | loss: 1.9915342MemoryTrain:  epoch  6, batch     4 | loss: 2.2385700MemoryTrain:  epoch  7, batch     0 | loss: 1.9378757MemoryTrain:  epoch  7, batch     1 | loss: 2.0448146MemoryTrain:  epoch  7, batch     2 | loss: 1.8842829MemoryTrain:  epoch  7, batch     3 | loss: 1.8281002MemoryTrain:  epoch  7, batch     4 | loss: 1.6264455MemoryTrain:  epoch  8, batch     0 | loss: 2.0906677MemoryTrain:  epoch  8, batch     1 | loss: 1.9639233MemoryTrain:  epoch  8, batch     2 | loss: 1.7103561MemoryTrain:  epoch  8, batch     3 | loss: 1.5339532MemoryTrain:  epoch  8, batch     4 | loss: 1.3792895MemoryTrain:  epoch  9, batch     0 | loss: 1.7471415MemoryTrain:  epoch  9, batch     1 | loss: 1.6849537MemoryTrain:  epoch  9, batch     2 | loss: 1.9243927MemoryTrain:  epoch  9, batch     3 | loss: 1.6437271MemoryTrain:  epoch  9, batch     4 | loss: 1.5660032
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 77.08%   [EVAL] batch:    3 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 80.00%   [EVAL] batch:    5 | acc: 62.50%,  total acc: 77.08%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 77.68%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 78.91%   [EVAL] batch:    8 | acc: 18.75%,  total acc: 72.22%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 71.25%   [EVAL] batch:   10 | acc: 43.75%,  total acc: 68.75%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 71.35%   [EVAL] batch:   12 | acc: 100.00%,  total acc: 73.56%   [EVAL] batch:   13 | acc: 100.00%,  total acc: 75.45%   [EVAL] batch:   14 | acc: 100.00%,  total acc: 77.08%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 78.52%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 79.78%   [EVAL] batch:   17 | acc: 25.00%,  total acc: 76.74%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:    3 | acc: 43.75%,  total acc: 71.88%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 72.50%   [EVAL] batch:    5 | acc: 68.75%,  total acc: 71.88%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 75.89%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 78.91%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 82.50%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 84.09%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:   12 | acc: 62.50%,  total acc: 82.69%   [EVAL] batch:   13 | acc: 37.50%,  total acc: 79.46%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 79.17%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 77.73%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 77.57%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 76.74%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 75.99%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 76.56%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 77.68%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 78.69%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 79.62%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 80.21%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 81.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 81.73%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 82.18%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 82.81%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 83.41%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 83.33%   [EVAL] batch:   30 | acc: 75.00%,  total acc: 83.06%   [EVAL] batch:   31 | acc: 87.50%,  total acc: 83.20%   [EVAL] batch:   32 | acc: 43.75%,  total acc: 82.01%   [EVAL] batch:   33 | acc: 43.75%,  total acc: 80.88%   [EVAL] batch:   34 | acc: 50.00%,  total acc: 80.00%   [EVAL] batch:   35 | acc: 43.75%,  total acc: 78.99%   [EVAL] batch:   36 | acc: 50.00%,  total acc: 78.21%   [EVAL] batch:   37 | acc: 37.50%,  total acc: 77.14%   [EVAL] batch:   38 | acc: 31.25%,  total acc: 75.96%   [EVAL] batch:   39 | acc: 50.00%,  total acc: 75.31%   [EVAL] batch:   40 | acc: 25.00%,  total acc: 74.09%   [EVAL] batch:   41 | acc: 37.50%,  total acc: 73.21%   [EVAL] batch:   42 | acc: 50.00%,  total acc: 72.67%   [EVAL] batch:   43 | acc: 56.25%,  total acc: 72.30%   [EVAL] batch:   44 | acc: 31.25%,  total acc: 71.39%   [EVAL] batch:   45 | acc: 18.75%,  total acc: 70.24%   [EVAL] batch:   46 | acc: 18.75%,  total acc: 69.15%   [EVAL] batch:   47 | acc: 18.75%,  total acc: 68.10%   [EVAL] batch:   48 | acc: 43.75%,  total acc: 67.60%   [EVAL] batch:   49 | acc: 18.75%,  total acc: 66.62%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 67.16%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 67.79%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 68.28%   [EVAL] batch:   53 | acc: 87.50%,  total acc: 68.63%   [EVAL] batch:   54 | acc: 56.25%,  total acc: 68.41%   [EVAL] batch:   55 | acc: 18.75%,  total acc: 67.52%   [EVAL] batch:   56 | acc: 6.25%,  total acc: 66.45%   [EVAL] batch:   57 | acc: 0.00%,  total acc: 65.30%   [EVAL] batch:   58 | acc: 18.75%,  total acc: 64.51%   [EVAL] batch:   59 | acc: 25.00%,  total acc: 63.85%   [EVAL] batch:   60 | acc: 50.00%,  total acc: 63.63%   [EVAL] batch:   61 | acc: 31.25%,  total acc: 63.10%   [EVAL] batch:   62 | acc: 56.25%,  total acc: 63.00%   [EVAL] batch:   63 | acc: 56.25%,  total acc: 62.89%   [EVAL] batch:   64 | acc: 68.75%,  total acc: 62.98%   [EVAL] batch:   65 | acc: 68.75%,  total acc: 63.07%   [EVAL] batch:   66 | acc: 50.00%,  total acc: 62.87%   [EVAL] batch:   67 | acc: 37.50%,  total acc: 62.50%   [EVAL] batch:   68 | acc: 31.25%,  total acc: 62.05%   [EVAL] batch:   69 | acc: 62.50%,  total acc: 62.05%   [EVAL] batch:   70 | acc: 62.50%,  total acc: 62.06%   [EVAL] batch:   71 | acc: 31.25%,  total acc: 61.63%   [EVAL] batch:   72 | acc: 56.25%,  total acc: 61.56%   [EVAL] batch:   73 | acc: 68.75%,  total acc: 61.66%   [EVAL] batch:   74 | acc: 75.00%,  total acc: 61.83%   [EVAL] batch:   75 | acc: 87.50%,  total acc: 62.17%   [EVAL] batch:   76 | acc: 56.25%,  total acc: 62.09%   [EVAL] batch:   77 | acc: 25.00%,  total acc: 61.62%   [EVAL] batch:   78 | acc: 6.25%,  total acc: 60.92%   [EVAL] batch:   79 | acc: 25.00%,  total acc: 60.47%   [EVAL] batch:   80 | acc: 43.75%,  total acc: 60.26%   [EVAL] batch:   81 | acc: 68.75%,  total acc: 60.37%   [EVAL] batch:   82 | acc: 81.25%,  total acc: 60.62%   [EVAL] batch:   83 | acc: 75.00%,  total acc: 60.79%   [EVAL] batch:   84 | acc: 93.75%,  total acc: 61.18%   [EVAL] batch:   85 | acc: 75.00%,  total acc: 61.34%   [EVAL] batch:   86 | acc: 56.25%,  total acc: 61.28%   [EVAL] batch:   87 | acc: 93.75%,  total acc: 61.65%   [EVAL] batch:   88 | acc: 81.25%,  total acc: 61.87%   [EVAL] batch:   89 | acc: 18.75%,  total acc: 61.39%   [EVAL] batch:   90 | acc: 62.50%,  total acc: 61.40%   [EVAL] batch:   91 | acc: 50.00%,  total acc: 61.28%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 61.69%   [EVAL] batch:   93 | acc: 100.00%,  total acc: 62.10%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 62.50%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 62.89%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 63.27%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 63.65%   [EVAL] batch:   98 | acc: 6.25%,  total acc: 63.07%   
cur_acc:  ['0.8485', '0.7548', '0.5833', '0.4460', '0.7674']
his_acc:  ['0.8485', '0.8403', '0.7365', '0.6489', '0.6307']
CurrentTrain: epoch  0, batch     0 | loss: 4.9181027CurrentTrain: epoch  0, batch     1 | loss: 5.7102304CurrentTrain: epoch  1, batch     0 | loss: 4.0043964CurrentTrain: epoch  1, batch     1 | loss: 4.1764703CurrentTrain: epoch  2, batch     0 | loss: 3.5319905CurrentTrain: epoch  2, batch     1 | loss: 3.4112215CurrentTrain: epoch  3, batch     0 | loss: 3.2469971CurrentTrain: epoch  3, batch     1 | loss: 2.9918079CurrentTrain: epoch  4, batch     0 | loss: 2.9969072CurrentTrain: epoch  4, batch     1 | loss: 2.6615765CurrentTrain: epoch  5, batch     0 | loss: 2.8431551CurrentTrain: epoch  5, batch     1 | loss: 2.6294467CurrentTrain: epoch  6, batch     0 | loss: 2.6581340CurrentTrain: epoch  6, batch     1 | loss: 2.4889667CurrentTrain: epoch  7, batch     0 | loss: 2.3927124CurrentTrain: epoch  7, batch     1 | loss: 2.3849397CurrentTrain: epoch  8, batch     0 | loss: 2.2860038CurrentTrain: epoch  8, batch     1 | loss: 2.5169168CurrentTrain: epoch  9, batch     0 | loss: 2.2614467CurrentTrain: epoch  9, batch     1 | loss: 2.2389259
#### organization founded ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization founded" means an organization was found in a date.
Here is an example:
Relation: organization founded
Context: pandit worked at the brokerage morgan stanley for about 11 years until 2005 , when he and some morgan stanley colleagues quit and later founded the hedge fund old lane partners .
Head Entity: old lane partners
Tail Entity: 2005
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization founded":
Relation: organization founded  
Context: In 1998, a group of engineers and entrepreneurs came together to establish the tech startup, Innovatech Solutions, which has since become a leader in software development.  
Head Entity: Innovatech Solutions  
Tail Entity: 1998  

Relation: organization founded  
Context: The non-profit organization Green Earth Initiative was created in 2010 to promote environmental awareness and sustainability practices across communities.  
Head Entity: Green Earth Initiative  
Tail Entity: 2010  
#### person age ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person age" means a person 's age is a number.
Here is an example:
Relation: person age
Context: susan boyle is 48 years old now .
Head Entity: susan boyle
Tail Entity: 48
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person age":
Relation: person age  
Context: john smith turned 30 last week.  
Head Entity: john smith  
Tail Entity: 30  

Relation: person age  
Context: the famous actor will celebrate his 45th birthday next month.  
Head Entity: the famous actor  
Tail Entity: 45  
#### person city of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person city of birth" means a person was born in a city.
Here is an example:
Relation: person city of birth
Context: rothman was born in san francisco in 1932 in an orthodox jewish family .
Head Entity: rothman
Tail Entity: san francisco
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person city of birth":
Relation: person city of birth  
Context: elena was born in barcelona and spent her childhood there before moving to madrid.  
Head Entity: elena  
Tail Entity: barcelona  

Relation: person city of birth  
Context: during the summer of 1985, michael was born in new orleans, a city known for its vibrant culture.  
Head Entity: michael  
Tail Entity: new orleans  
#### organization members ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization members" means an organization 's member is an organization.
Here is an example:
Relation: organization members
Context: sun plays for the grand rapids flight of the international basketball league after toiling for the maryland nighthawks of the american basketball association , both development leagues for those who dream of an nba career .
Head Entity: american basketball association
Tail Entity: maryland nighthawks
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization members":
Relation: organization members  
Context: The New York Philharmonic is one of the oldest orchestras in the United States, and it has had many notable musicians as members, including the famous conductor Leonard Bernstein.  
Head Entity: New York Philharmonic  
Tail Entity: Leonard Bernstein  

Relation: organization members  
Context: The National Geographic Society has a long history of exploration and education, with many prominent explorers and scientists, such as Jane Goodall, being members of the organization.  
Head Entity: National Geographic Society  
Tail Entity: Jane Goodall  
#### person religion ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person religion" means an person is the member of a religion.
Here is an example:
Relation: person religion
Context: the pope defended his action on the grounds that he could not refuse an audience to a head of state from a country with a strong catholic tradition unless he had clear-cut proof of the allegations against him .
Head Entity: he
Tail Entity: catholic
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person religion":
Relation: person religion  
Context: After years of study and reflection, Maria decided to embrace Buddhism, finding peace and purpose in its teachings.  
Head Entity: Maria  
Tail Entity: Buddhism  

Relation: person religion  
Context: During the festival, Ahmed proudly wore his traditional attire, celebrating his deep connection to Islam and its rich cultural heritage.  
Head Entity: Ahmed  
Tail Entity: Islam  
Mixup data size:  225
MixupTrain:  epoch  0, batch     0 | loss: 2.9774539MixupTrain:  epoch  0, batch     1 | loss: 2.8678283MixupTrain:  epoch  0, batch     2 | loss: 2.5514681MixupTrain:  epoch  0, batch     3 | loss: 2.7825826MixupTrain:  epoch  0, batch     4 | loss: 2.8051671MixupTrain:  epoch  0, batch     5 | loss: 2.8808001MixupTrain:  epoch  0, batch     6 | loss: 2.9198670MixupTrain:  epoch  0, batch     7 | loss: 2.9361180MixupTrain:  epoch  0, batch     8 | loss: 2.6568594MixupTrain:  epoch  0, batch     9 | loss: 2.8515425MixupTrain:  epoch  0, batch    10 | loss: 3.0719722MixupTrain:  epoch  0, batch    11 | loss: 2.9073224MixupTrain:  epoch  0, batch    12 | loss: 2.7320831MixupTrain:  epoch  0, batch    13 | loss: 3.4646478MixupTrain:  epoch  0, batch    14 | loss: 1.2556561
MemoryTrain:  epoch  0, batch     0 | loss: 1.8444490MemoryTrain:  epoch  0, batch     1 | loss: 2.8464611MemoryTrain:  epoch  0, batch     2 | loss: 2.9452052MemoryTrain:  epoch  0, batch     3 | loss: 2.5541534MemoryTrain:  epoch  0, batch     4 | loss: 3.3175020MemoryTrain:  epoch  0, batch     5 | loss: 3.0397854MemoryTrain:  epoch  1, batch     0 | loss: 2.3922586MemoryTrain:  epoch  1, batch     1 | loss: 2.8500423MemoryTrain:  epoch  1, batch     2 | loss: 2.1158688MemoryTrain:  epoch  1, batch     3 | loss: 2.8190913MemoryTrain:  epoch  1, batch     4 | loss: 2.0848627MemoryTrain:  epoch  1, batch     5 | loss: 2.1238711MemoryTrain:  epoch  2, batch     0 | loss: 2.4378552MemoryTrain:  epoch  2, batch     1 | loss: 1.7594140MemoryTrain:  epoch  2, batch     2 | loss: 2.0225148MemoryTrain:  epoch  2, batch     3 | loss: 1.7925279MemoryTrain:  epoch  2, batch     4 | loss: 2.2167511MemoryTrain:  epoch  2, batch     5 | loss: 1.7749743MemoryTrain:  epoch  3, batch     0 | loss: 2.1569123MemoryTrain:  epoch  3, batch     1 | loss: 1.7775395MemoryTrain:  epoch  3, batch     2 | loss: 1.7850419MemoryTrain:  epoch  3, batch     3 | loss: 1.7345774MemoryTrain:  epoch  3, batch     4 | loss: 2.1509333MemoryTrain:  epoch  3, batch     5 | loss: 1.7370127MemoryTrain:  epoch  4, batch     0 | loss: 1.8856608MemoryTrain:  epoch  4, batch     1 | loss: 1.7023745MemoryTrain:  epoch  4, batch     2 | loss: 2.1036346MemoryTrain:  epoch  4, batch     3 | loss: 1.6326020MemoryTrain:  epoch  4, batch     4 | loss: 1.8434727MemoryTrain:  epoch  4, batch     5 | loss: 1.4711571MemoryTrain:  epoch  5, batch     0 | loss: 1.6534517MemoryTrain:  epoch  5, batch     1 | loss: 1.6557682MemoryTrain:  epoch  5, batch     2 | loss: 1.5381334MemoryTrain:  epoch  5, batch     3 | loss: 1.4903679MemoryTrain:  epoch  5, batch     4 | loss: 1.7459265MemoryTrain:  epoch  5, batch     5 | loss: 1.6883765MemoryTrain:  epoch  6, batch     0 | loss: 1.7962548MemoryTrain:  epoch  6, batch     1 | loss: 1.5386994MemoryTrain:  epoch  6, batch     2 | loss: 1.6166904MemoryTrain:  epoch  6, batch     3 | loss: 1.6480943MemoryTrain:  epoch  6, batch     4 | loss: 1.4498328MemoryTrain:  epoch  6, batch     5 | loss: 1.4127426MemoryTrain:  epoch  7, batch     0 | loss: 1.6251235MemoryTrain:  epoch  7, batch     1 | loss: 1.5631813MemoryTrain:  epoch  7, batch     2 | loss: 1.4846911MemoryTrain:  epoch  7, batch     3 | loss: 1.5497215MemoryTrain:  epoch  7, batch     4 | loss: 1.5268672MemoryTrain:  epoch  7, batch     5 | loss: 1.5275999MemoryTrain:  epoch  8, batch     0 | loss: 1.5625205MemoryTrain:  epoch  8, batch     1 | loss: 1.4979367MemoryTrain:  epoch  8, batch     2 | loss: 1.6323776MemoryTrain:  epoch  8, batch     3 | loss: 1.3712815MemoryTrain:  epoch  8, batch     4 | loss: 1.5006893MemoryTrain:  epoch  8, batch     5 | loss: 1.6168061MemoryTrain:  epoch  9, batch     0 | loss: 1.4883733MemoryTrain:  epoch  9, batch     1 | loss: 1.3110113MemoryTrain:  epoch  9, batch     2 | loss: 1.4239662MemoryTrain:  epoch  9, batch     3 | loss: 1.5463457MemoryTrain:  epoch  9, batch     4 | loss: 1.4795918MemoryTrain:  epoch  9, batch     5 | loss: 1.3373023
[EVAL] batch:    0 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 96.88%   [EVAL] batch:    2 | acc: 100.00%,  total acc: 97.92%   [EVAL] batch:    3 | acc: 100.00%,  total acc: 98.44%   [EVAL] batch:    4 | acc: 100.00%,  total acc: 98.75%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 98.96%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 99.11%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 99.22%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 95.83%   [EVAL] batch:    9 | acc: 12.50%,  total acc: 87.50%   [EVAL] batch:   10 | acc: 31.25%,  total acc: 82.39%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 81.77%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:   13 | acc: 62.50%,  total acc: 79.91%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 79.17%   [EVAL] batch:    3 | acc: 62.50%,  total acc: 75.00%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 77.68%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 80.47%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 81.88%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 82.95%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 82.21%   [EVAL] batch:   13 | acc: 37.50%,  total acc: 79.02%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 78.33%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 76.95%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 76.84%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 76.04%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 75.33%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 75.62%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 76.79%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 77.84%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 78.53%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 79.17%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 80.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 80.77%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 81.92%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 82.33%   [EVAL] batch:   29 | acc: 75.00%,  total acc: 82.08%   [EVAL] batch:   30 | acc: 81.25%,  total acc: 82.06%   [EVAL] batch:   31 | acc: 75.00%,  total acc: 81.84%   [EVAL] batch:   32 | acc: 43.75%,  total acc: 80.68%   [EVAL] batch:   33 | acc: 43.75%,  total acc: 79.60%   [EVAL] batch:   34 | acc: 50.00%,  total acc: 78.75%   [EVAL] batch:   35 | acc: 37.50%,  total acc: 77.60%   [EVAL] batch:   36 | acc: 50.00%,  total acc: 76.86%   [EVAL] batch:   37 | acc: 37.50%,  total acc: 75.82%   [EVAL] batch:   38 | acc: 31.25%,  total acc: 74.68%   [EVAL] batch:   39 | acc: 50.00%,  total acc: 74.06%   [EVAL] batch:   40 | acc: 37.50%,  total acc: 73.17%   [EVAL] batch:   41 | acc: 50.00%,  total acc: 72.62%   [EVAL] batch:   42 | acc: 56.25%,  total acc: 72.24%   [EVAL] batch:   43 | acc: 68.75%,  total acc: 72.16%   [EVAL] batch:   44 | acc: 43.75%,  total acc: 71.53%   [EVAL] batch:   45 | acc: 25.00%,  total acc: 70.52%   [EVAL] batch:   46 | acc: 18.75%,  total acc: 69.41%   [EVAL] batch:   47 | acc: 12.50%,  total acc: 68.23%   [EVAL] batch:   48 | acc: 31.25%,  total acc: 67.47%   [EVAL] batch:   49 | acc: 12.50%,  total acc: 66.38%   [EVAL] batch:   50 | acc: 87.50%,  total acc: 66.79%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 67.43%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 67.92%   [EVAL] batch:   53 | acc: 93.75%,  total acc: 68.40%   [EVAL] batch:   54 | acc: 56.25%,  total acc: 68.18%   [EVAL] batch:   55 | acc: 6.25%,  total acc: 67.08%   [EVAL] batch:   56 | acc: 0.00%,  total acc: 65.90%   [EVAL] batch:   57 | acc: 0.00%,  total acc: 64.76%   [EVAL] batch:   58 | acc: 18.75%,  total acc: 63.98%   [EVAL] batch:   59 | acc: 6.25%,  total acc: 63.02%   [EVAL] batch:   60 | acc: 6.25%,  total acc: 62.09%   [EVAL] batch:   61 | acc: 25.00%,  total acc: 61.49%   [EVAL] batch:   62 | acc: 56.25%,  total acc: 61.41%   [EVAL] batch:   63 | acc: 56.25%,  total acc: 61.33%   [EVAL] batch:   64 | acc: 62.50%,  total acc: 61.35%   [EVAL] batch:   65 | acc: 68.75%,  total acc: 61.46%   [EVAL] batch:   66 | acc: 31.25%,  total acc: 61.01%   [EVAL] batch:   67 | acc: 25.00%,  total acc: 60.48%   [EVAL] batch:   68 | acc: 18.75%,  total acc: 59.87%   [EVAL] batch:   69 | acc: 50.00%,  total acc: 59.73%   [EVAL] batch:   70 | acc: 37.50%,  total acc: 59.42%   [EVAL] batch:   71 | acc: 12.50%,  total acc: 58.77%   [EVAL] batch:   72 | acc: 68.75%,  total acc: 58.90%   [EVAL] batch:   73 | acc: 62.50%,  total acc: 58.95%   [EVAL] batch:   74 | acc: 75.00%,  total acc: 59.17%   [EVAL] batch:   75 | acc: 81.25%,  total acc: 59.46%   [EVAL] batch:   76 | acc: 68.75%,  total acc: 59.58%   [EVAL] batch:   77 | acc: 31.25%,  total acc: 59.21%   [EVAL] batch:   78 | acc: 6.25%,  total acc: 58.54%   [EVAL] batch:   79 | acc: 18.75%,  total acc: 58.05%   [EVAL] batch:   80 | acc: 25.00%,  total acc: 57.64%   [EVAL] batch:   81 | acc: 56.25%,  total acc: 57.62%   [EVAL] batch:   82 | acc: 56.25%,  total acc: 57.61%   [EVAL] batch:   83 | acc: 50.00%,  total acc: 57.51%   [EVAL] batch:   84 | acc: 50.00%,  total acc: 57.43%   [EVAL] batch:   85 | acc: 62.50%,  total acc: 57.49%   [EVAL] batch:   86 | acc: 37.50%,  total acc: 57.26%   [EVAL] batch:   87 | acc: 87.50%,  total acc: 57.60%   [EVAL] batch:   88 | acc: 87.50%,  total acc: 57.94%   [EVAL] batch:   89 | acc: 12.50%,  total acc: 57.43%   [EVAL] batch:   90 | acc: 62.50%,  total acc: 57.49%   [EVAL] batch:   91 | acc: 62.50%,  total acc: 57.54%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 58.00%   [EVAL] batch:   93 | acc: 100.00%,  total acc: 58.44%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 58.88%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 59.31%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 59.73%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 60.14%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 60.54%   [EVAL] batch:   99 | acc: 93.75%,  total acc: 60.88%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 61.26%   [EVAL] batch:  101 | acc: 100.00%,  total acc: 61.64%   [EVAL] batch:  102 | acc: 100.00%,  total acc: 62.01%   [EVAL] batch:  103 | acc: 100.00%,  total acc: 62.38%   [EVAL] batch:  104 | acc: 100.00%,  total acc: 62.74%   [EVAL] batch:  105 | acc: 100.00%,  total acc: 63.09%   [EVAL] batch:  106 | acc: 75.00%,  total acc: 63.20%   [EVAL] batch:  107 | acc: 12.50%,  total acc: 62.73%   [EVAL] batch:  108 | acc: 25.00%,  total acc: 62.39%   [EVAL] batch:  109 | acc: 81.25%,  total acc: 62.56%   [EVAL] batch:  110 | acc: 68.75%,  total acc: 62.61%   [EVAL] batch:  111 | acc: 68.75%,  total acc: 62.67%   
cur_acc:  ['0.8485', '0.7548', '0.5833', '0.4460', '0.7674', '0.7991']
his_acc:  ['0.8485', '0.8403', '0.7365', '0.6489', '0.6307', '0.6267']
CurrentTrain: epoch  0, batch     0 | loss: 5.9658012CurrentTrain: epoch  0, batch     1 | loss: 5.9497209CurrentTrain: epoch  1, batch     0 | loss: 5.1758165CurrentTrain: epoch  1, batch     1 | loss: 4.7654438CurrentTrain: epoch  2, batch     0 | loss: 3.8970995CurrentTrain: epoch  2, batch     1 | loss: 4.7475681CurrentTrain: epoch  3, batch     0 | loss: 3.2711344CurrentTrain: epoch  3, batch     1 | loss: 4.0615845CurrentTrain: epoch  4, batch     0 | loss: 3.1644104CurrentTrain: epoch  4, batch     1 | loss: 3.2892520CurrentTrain: epoch  5, batch     0 | loss: 2.7995758CurrentTrain: epoch  5, batch     1 | loss: 3.2870607CurrentTrain: epoch  6, batch     0 | loss: 2.6776233CurrentTrain: epoch  6, batch     1 | loss: 3.0473356CurrentTrain: epoch  7, batch     0 | loss: 2.5815787CurrentTrain: epoch  7, batch     1 | loss: 2.6745403CurrentTrain: epoch  8, batch     0 | loss: 2.5090718CurrentTrain: epoch  8, batch     1 | loss: 2.4704354CurrentTrain: epoch  9, batch     0 | loss: 2.6314230CurrentTrain: epoch  9, batch     1 | loss: 2.1883905
#### person date of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person date of birth" means a person was born in a date.
Here is an example:
Relation: person date of birth
Context: born of schoolteacher parents in the western town of sabaneta on july 28 , 1954 , chavez studied at the military academy of venezuela in caracas .
Head Entity: chavez
Tail Entity: july 28 , 1954
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person date of birth":
Relation: person date of birth  
Context: the famous physicist albert einstein was born in ulm, in the kingdom of wurttemberg in the german empire on march 14, 1879.  
Head Entity: albert einstein  
Tail Entity: march 14, 1879  

Relation: person date of birth  
Context: the renowned author mark twain, whose real name was samuel langhorne clemens, was born on november 30, 1835, in florida, missouri.  
Head Entity: mark twain  
Tail Entity: november 30, 1835  
#### person stateorprovince of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person stateorprovince of birth" means a person was died in a state or province.
Here is an example:
Relation: person stateorprovince of birth
Context: jane matilda bolin was born on april 11 , 1908 , in poughkeepsie , ny .
Head Entity: jane matilda bolin
Tail Entity: ny
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person stateorprovince of birth":
Relation: person stateorprovince of birth  
Context: albert einstein was born on march 14, 1879, in ulm, germany.  
Head Entity: albert einstein  
Tail Entity: germany  

Relation: person stateorprovince of birth  
Context: marilyn monroe was born on june 1, 1926, in los angeles, california.  
Head Entity: marilyn monroe  
Tail Entity: california  
#### person parents ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person parents" means a person 's parent is a person.
Here is an example:
Relation: person parents
Context: lynne spears told the court that lutfi had treated her daughter like a hostage in her own home , drugged her and took over her finances .
Head Entity: her
Tail Entity: lynne spears
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person parents":
Relation: person parents  
Context: During the family reunion, Sarah introduced her father, John, to her friends, highlighting how much he has influenced her life choices.  
Head Entity: her father  
Tail Entity: John  

Relation: person parents  
Context: Emily often shares stories about her mother, who was a strong influence in her decision to pursue a career in medicine.  
Head Entity: her mother  
Tail Entity: Emily
#### person employee of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person employee of" means a person is an employee of an organization.
Here is an example:
Relation: person employee of
Context: seeking revenge , axel reunites with old pal sgt. billy rosewood -lrb- judge reinhold -rrb- and jon flint -lrb- hector elizondo -rrb- of the beverly hills police department .
Head Entity: hector elizondo
Tail Entity: beverly hills police department
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person employee of":
Relation: person employee of  
Context: After years of hard work and dedication, Sarah Thompson finally received a promotion at Tech Innovations, where she has been a key player in the development team.  
Head Entity: Sarah Thompson  
Tail Entity: Tech Innovations  

Relation: person employee of  
Context: John Smith has been with Global Finance for over a decade, where he has climbed the ranks to become the senior analyst, known for his expertise in market trends.  
Head Entity: John Smith  
Tail Entity: Global Finance  
#### person stateorprovince of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person stateorprovince of death" means a person was died in a state or province.
Here is an example:
Relation: person stateorprovince of death
Context: irene morgan kirkaldy , whose defiance of bus segregation laws more than a decade before rosa parks ' landmark case helped lay the foundation for later civil rights victories , died friday at her home in hayes , va. .
Head Entity: irene morgan kirkaldy
Tail Entity: va.
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person stateorprovince of death":
Relation: person stateorprovince of death  
Context: john doe, a renowned author known for his impactful novels, passed away peacefully in his sleep at his residence in california last night.  
Head Entity: john doe  
Tail Entity: california  

Relation: person stateorprovince of death  
Context: after a long battle with illness, elizabeth taylor, the iconic actress, succumbed to her health issues and died in a hospital in nevada.  
Head Entity: elizabeth taylor  
Tail Entity: nevada  
Mixup data size:  254
MixupTrain:  epoch  0, batch     0 | loss: 2.9612335MixupTrain:  epoch  0, batch     1 | loss: 2.7386245MixupTrain:  epoch  0, batch     2 | loss: 2.7764808MixupTrain:  epoch  0, batch     3 | loss: 2.8376704MixupTrain:  epoch  0, batch     4 | loss: 3.0334813MixupTrain:  epoch  0, batch     5 | loss: 2.4182827MixupTrain:  epoch  0, batch     6 | loss: 2.1150467MixupTrain:  epoch  0, batch     7 | loss: 2.5405478MixupTrain:  epoch  0, batch     8 | loss: 2.7805868MixupTrain:  epoch  0, batch     9 | loss: 2.9545530MixupTrain:  epoch  0, batch    10 | loss: 2.5442820MixupTrain:  epoch  0, batch    11 | loss: 2.5492432MixupTrain:  epoch  0, batch    12 | loss: 2.9669524MixupTrain:  epoch  0, batch    13 | loss: 2.7581364MixupTrain:  epoch  0, batch    14 | loss: 2.8820377MixupTrain:  epoch  0, batch    15 | loss: 2.3459082
MemoryTrain:  epoch  0, batch     0 | loss: 1.3844638MemoryTrain:  epoch  0, batch     1 | loss: 2.0168839MemoryTrain:  epoch  0, batch     2 | loss: 2.5662019MemoryTrain:  epoch  0, batch     3 | loss: 2.4644892MemoryTrain:  epoch  0, batch     4 | loss: 2.3211191MemoryTrain:  epoch  0, batch     5 | loss: 3.1679664MemoryTrain:  epoch  0, batch     6 | loss: 2.2854381MemoryTrain:  epoch  1, batch     0 | loss: 2.1498790MemoryTrain:  epoch  1, batch     1 | loss: 2.0061076MemoryTrain:  epoch  1, batch     2 | loss: 1.5117413MemoryTrain:  epoch  1, batch     3 | loss: 2.1087031MemoryTrain:  epoch  1, batch     4 | loss: 2.4696209MemoryTrain:  epoch  1, batch     5 | loss: 1.9858785MemoryTrain:  epoch  1, batch     6 | loss: 2.8566399MemoryTrain:  epoch  2, batch     0 | loss: 1.6742842MemoryTrain:  epoch  2, batch     1 | loss: 1.6237087MemoryTrain:  epoch  2, batch     2 | loss: 1.8996217MemoryTrain:  epoch  2, batch     3 | loss: 1.7638226MemoryTrain:  epoch  2, batch     4 | loss: 1.9034579MemoryTrain:  epoch  2, batch     5 | loss: 1.6344830MemoryTrain:  epoch  2, batch     6 | loss: 2.3464048MemoryTrain:  epoch  3, batch     0 | loss: 1.6367309MemoryTrain:  epoch  3, batch     1 | loss: 2.2562361MemoryTrain:  epoch  3, batch     2 | loss: 1.5112350MemoryTrain:  epoch  3, batch     3 | loss: 1.7116917MemoryTrain:  epoch  3, batch     4 | loss: 1.5299773MemoryTrain:  epoch  3, batch     5 | loss: 1.6699209MemoryTrain:  epoch  3, batch     6 | loss: 1.6669369MemoryTrain:  epoch  4, batch     0 | loss: 1.6619204MemoryTrain:  epoch  4, batch     1 | loss: 2.1820021MemoryTrain:  epoch  4, batch     2 | loss: 1.4158316MemoryTrain:  epoch  4, batch     3 | loss: 1.5241810MemoryTrain:  epoch  4, batch     4 | loss: 1.5637583MemoryTrain:  epoch  4, batch     5 | loss: 1.6029491MemoryTrain:  epoch  4, batch     6 | loss: 1.6683550MemoryTrain:  epoch  5, batch     0 | loss: 1.4721845MemoryTrain:  epoch  5, batch     1 | loss: 1.4194376MemoryTrain:  epoch  5, batch     2 | loss: 1.6566402MemoryTrain:  epoch  5, batch     3 | loss: 1.7547684MemoryTrain:  epoch  5, batch     4 | loss: 1.5117915MemoryTrain:  epoch  5, batch     5 | loss: 1.5062610MemoryTrain:  epoch  5, batch     6 | loss: 1.4871162MemoryTrain:  epoch  6, batch     0 | loss: 1.5059912MemoryTrain:  epoch  6, batch     1 | loss: 1.4246614MemoryTrain:  epoch  6, batch     2 | loss: 1.4223210MemoryTrain:  epoch  6, batch     3 | loss: 1.5145860MemoryTrain:  epoch  6, batch     4 | loss: 1.7326843MemoryTrain:  epoch  6, batch     5 | loss: 1.5213909MemoryTrain:  epoch  6, batch     6 | loss: 1.3211817MemoryTrain:  epoch  7, batch     0 | loss: 1.4413164MemoryTrain:  epoch  7, batch     1 | loss: 1.4593600MemoryTrain:  epoch  7, batch     2 | loss: 1.4835302MemoryTrain:  epoch  7, batch     3 | loss: 1.6413549MemoryTrain:  epoch  7, batch     4 | loss: 1.4050620MemoryTrain:  epoch  7, batch     5 | loss: 1.3913810MemoryTrain:  epoch  7, batch     6 | loss: 1.7825356MemoryTrain:  epoch  8, batch     0 | loss: 1.2768114MemoryTrain:  epoch  8, batch     1 | loss: 1.5854366MemoryTrain:  epoch  8, batch     2 | loss: 1.3738486MemoryTrain:  epoch  8, batch     3 | loss: 1.4828690MemoryTrain:  epoch  8, batch     4 | loss: 1.3803787MemoryTrain:  epoch  8, batch     5 | loss: 1.4614505MemoryTrain:  epoch  8, batch     6 | loss: 1.5089016MemoryTrain:  epoch  9, batch     0 | loss: 1.4015051MemoryTrain:  epoch  9, batch     1 | loss: 1.2734877MemoryTrain:  epoch  9, batch     2 | loss: 1.3640307MemoryTrain:  epoch  9, batch     3 | loss: 1.4012630MemoryTrain:  epoch  9, batch     4 | loss: 1.2867166MemoryTrain:  epoch  9, batch     5 | loss: 1.4723639MemoryTrain:  epoch  9, batch     6 | loss: 1.4850583
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    1 | acc: 37.50%,  total acc: 50.00%   [EVAL] batch:    2 | acc: 37.50%,  total acc: 45.83%   [EVAL] batch:    3 | acc: 56.25%,  total acc: 48.44%   [EVAL] batch:    4 | acc: 18.75%,  total acc: 42.50%   [EVAL] batch:    5 | acc: 25.00%,  total acc: 39.58%   [EVAL] batch:    6 | acc: 25.00%,  total acc: 37.50%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 43.75%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 50.00%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 53.12%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 55.68%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 57.81%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 59.62%   [EVAL] batch:   13 | acc: 43.75%,  total acc: 58.48%   
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 80.21%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 82.14%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 84.38%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 84.72%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 84.38%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 85.23%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 85.94%   [EVAL] batch:   12 | acc: 62.50%,  total acc: 84.13%   [EVAL] batch:   13 | acc: 37.50%,  total acc: 80.80%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 80.00%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 78.52%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 78.31%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 77.43%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 76.64%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 76.88%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 77.98%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 78.98%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 79.89%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 80.47%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 81.97%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 82.41%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 83.04%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 83.41%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 83.54%   [EVAL] batch:   30 | acc: 81.25%,  total acc: 83.47%   [EVAL] batch:   31 | acc: 81.25%,  total acc: 83.40%   [EVAL] batch:   32 | acc: 43.75%,  total acc: 82.20%   [EVAL] batch:   33 | acc: 43.75%,  total acc: 81.07%   [EVAL] batch:   34 | acc: 50.00%,  total acc: 80.18%   [EVAL] batch:   35 | acc: 43.75%,  total acc: 79.17%   [EVAL] batch:   36 | acc: 50.00%,  total acc: 78.38%   [EVAL] batch:   37 | acc: 31.25%,  total acc: 77.14%   [EVAL] batch:   38 | acc: 25.00%,  total acc: 75.80%   [EVAL] batch:   39 | acc: 56.25%,  total acc: 75.31%   [EVAL] batch:   40 | acc: 12.50%,  total acc: 73.78%   [EVAL] batch:   41 | acc: 31.25%,  total acc: 72.77%   [EVAL] batch:   42 | acc: 56.25%,  total acc: 72.38%   [EVAL] batch:   43 | acc: 56.25%,  total acc: 72.02%   [EVAL] batch:   44 | acc: 31.25%,  total acc: 71.11%   [EVAL] batch:   45 | acc: 6.25%,  total acc: 69.70%   [EVAL] batch:   46 | acc: 6.25%,  total acc: 68.35%   [EVAL] batch:   47 | acc: 0.00%,  total acc: 66.93%   [EVAL] batch:   48 | acc: 6.25%,  total acc: 65.69%   [EVAL] batch:   49 | acc: 6.25%,  total acc: 64.50%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 65.20%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 65.75%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 66.27%   [EVAL] batch:   53 | acc: 93.75%,  total acc: 66.78%   [EVAL] batch:   54 | acc: 56.25%,  total acc: 66.59%   [EVAL] batch:   55 | acc: 6.25%,  total acc: 65.51%   [EVAL] batch:   56 | acc: 0.00%,  total acc: 64.36%   [EVAL] batch:   57 | acc: 0.00%,  total acc: 63.25%   [EVAL] batch:   58 | acc: 18.75%,  total acc: 62.50%   [EVAL] batch:   59 | acc: 0.00%,  total acc: 61.46%   [EVAL] batch:   60 | acc: 12.50%,  total acc: 60.66%   [EVAL] batch:   61 | acc: 25.00%,  total acc: 60.08%   [EVAL] batch:   62 | acc: 56.25%,  total acc: 60.02%   [EVAL] batch:   63 | acc: 50.00%,  total acc: 59.86%   [EVAL] batch:   64 | acc: 56.25%,  total acc: 59.81%   [EVAL] batch:   65 | acc: 56.25%,  total acc: 59.75%   [EVAL] batch:   66 | acc: 43.75%,  total acc: 59.51%   [EVAL] batch:   67 | acc: 31.25%,  total acc: 59.10%   [EVAL] batch:   68 | acc: 31.25%,  total acc: 58.70%   [EVAL] batch:   69 | acc: 56.25%,  total acc: 58.66%   [EVAL] batch:   70 | acc: 62.50%,  total acc: 58.71%   [EVAL] batch:   71 | acc: 31.25%,  total acc: 58.33%   [EVAL] batch:   72 | acc: 81.25%,  total acc: 58.65%   [EVAL] batch:   73 | acc: 75.00%,  total acc: 58.87%   [EVAL] batch:   74 | acc: 93.75%,  total acc: 59.33%   [EVAL] batch:   75 | acc: 93.75%,  total acc: 59.79%   [EVAL] batch:   76 | acc: 75.00%,  total acc: 59.98%   [EVAL] batch:   77 | acc: 37.50%,  total acc: 59.70%   [EVAL] batch:   78 | acc: 18.75%,  total acc: 59.18%   [EVAL] batch:   79 | acc: 25.00%,  total acc: 58.75%   [EVAL] batch:   80 | acc: 31.25%,  total acc: 58.41%   [EVAL] batch:   81 | acc: 56.25%,  total acc: 58.38%   [EVAL] batch:   82 | acc: 43.75%,  total acc: 58.21%   [EVAL] batch:   83 | acc: 37.50%,  total acc: 57.96%   [EVAL] batch:   84 | acc: 37.50%,  total acc: 57.72%   [EVAL] batch:   85 | acc: 68.75%,  total acc: 57.85%   [EVAL] batch:   86 | acc: 43.75%,  total acc: 57.69%   [EVAL] batch:   87 | acc: 93.75%,  total acc: 58.10%   [EVAL] batch:   88 | acc: 81.25%,  total acc: 58.36%   [EVAL] batch:   89 | acc: 12.50%,  total acc: 57.85%   [EVAL] batch:   90 | acc: 50.00%,  total acc: 57.76%   [EVAL] batch:   91 | acc: 43.75%,  total acc: 57.61%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 58.06%   [EVAL] batch:   93 | acc: 100.00%,  total acc: 58.51%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 58.95%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 59.38%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 59.79%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 60.20%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 60.61%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 61.00%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 61.39%   [EVAL] batch:  101 | acc: 100.00%,  total acc: 61.76%   [EVAL] batch:  102 | acc: 100.00%,  total acc: 62.14%   [EVAL] batch:  103 | acc: 100.00%,  total acc: 62.50%   [EVAL] batch:  104 | acc: 100.00%,  total acc: 62.86%   [EVAL] batch:  105 | acc: 100.00%,  total acc: 63.21%   [EVAL] batch:  106 | acc: 68.75%,  total acc: 63.26%   [EVAL] batch:  107 | acc: 12.50%,  total acc: 62.79%   [EVAL] batch:  108 | acc: 18.75%,  total acc: 62.39%   [EVAL] batch:  109 | acc: 93.75%,  total acc: 62.67%   [EVAL] batch:  110 | acc: 68.75%,  total acc: 62.73%   [EVAL] batch:  111 | acc: 87.50%,  total acc: 62.95%   [EVAL] batch:  112 | acc: 56.25%,  total acc: 62.89%   [EVAL] batch:  113 | acc: 43.75%,  total acc: 62.72%   [EVAL] batch:  114 | acc: 43.75%,  total acc: 62.55%   [EVAL] batch:  115 | acc: 37.50%,  total acc: 62.34%   [EVAL] batch:  116 | acc: 31.25%,  total acc: 62.07%   [EVAL] batch:  117 | acc: 12.50%,  total acc: 61.65%   [EVAL] batch:  118 | acc: 37.50%,  total acc: 61.45%   [EVAL] batch:  119 | acc: 93.75%,  total acc: 61.72%   [EVAL] batch:  120 | acc: 100.00%,  total acc: 62.04%   [EVAL] batch:  121 | acc: 81.25%,  total acc: 62.19%   [EVAL] batch:  122 | acc: 75.00%,  total acc: 62.30%   [EVAL] batch:  123 | acc: 81.25%,  total acc: 62.45%   [EVAL] batch:  124 | acc: 81.25%,  total acc: 62.60%   [EVAL] batch:  125 | acc: 31.25%,  total acc: 62.35%   
cur_acc:  ['0.8485', '0.7548', '0.5833', '0.4460', '0.7674', '0.7991', '0.5848']
his_acc:  ['0.8485', '0.8403', '0.7365', '0.6489', '0.6307', '0.6267', '0.6235']
CurrentTrain: epoch  0, batch     0 | loss: 5.3586459CurrentTrain: epoch  0, batch     1 | loss: 6.3107209CurrentTrain: epoch  1, batch     0 | loss: 4.2021971CurrentTrain: epoch  1, batch     1 | loss: 6.2163033CurrentTrain: epoch  2, batch     0 | loss: 4.2071323CurrentTrain: epoch  2, batch     1 | loss: 4.4201899CurrentTrain: epoch  3, batch     0 | loss: 4.2134790CurrentTrain: epoch  3, batch     1 | loss: 3.2563241CurrentTrain: epoch  4, batch     0 | loss: 3.2084374CurrentTrain: epoch  4, batch     1 | loss: 3.8873439CurrentTrain: epoch  5, batch     0 | loss: 3.6209030CurrentTrain: epoch  5, batch     1 | loss: 2.6807206CurrentTrain: epoch  6, batch     0 | loss: 2.5430844CurrentTrain: epoch  6, batch     1 | loss: 3.7632344CurrentTrain: epoch  7, batch     0 | loss: 3.1440082CurrentTrain: epoch  7, batch     1 | loss: 2.6197178CurrentTrain: epoch  8, batch     0 | loss: 2.9425874CurrentTrain: epoch  8, batch     1 | loss: 2.5538797CurrentTrain: epoch  9, batch     0 | loss: 2.6987867CurrentTrain: epoch  9, batch     1 | loss: 2.6973865
#### person country of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person country of birth" means a person was born in a country.
Here is an example:
Relation: person country of birth
Context: iran 's atomic chief ali akbar salehi has expressed tehran 's readiness to swap 1,200 kilogrammes -lrb- 2,640 pounds -rrb- of low-enriched uranium -lrb- leu -rrb- in one-shot for enriched atomic fuel .
Head Entity: ali akbar salehi
Tail Entity: iran
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person country of birth":
Relation: person country of birth  
Context: renowned physicist stephen hawking was born in oxford, england, where he later pursued his groundbreaking research in cosmology.  
Head Entity: stephen hawking  
Tail Entity: england  

Relation: person country of birth  
Context: actress natalie portman, who was born in jerusalem, has often spoken about her dual citizenship in israel and the united states.  
Head Entity: natalie portman  
Tail Entity: israel  
#### organization website ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization website" means an organization 's website is an url.
Here is an example:
Relation: organization website
Context: http://www.pentaxsailaway.com/ paul j. sail away with pentax sweepstakes
Head Entity: pentax
Tail Entity: http://www.pentaxsailaway.com/
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization website":
Relation: organization website  
Context: Visit the official site at https://www.techinnovators.com for more information on their latest products.  
Head Entity: Tech Innovators  
Tail Entity: https://www.techinnovators.com  

Relation: organization website  
Context: For updates and news, check out the blog at http://www.greenearth.org/blog.  
Head Entity: Green Earth  
Tail Entity: http://www.greenearth.org/
#### organization shareholders ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization shareholders" means an organization was invested by a person.
Here is an example:
Relation: organization shareholders
Context: ------ liberty media acquired a 41 percent stake in directv in late february by exchanging it for a 16 percent stake in news corp plus $ 625 million -lrb- euro402 5 million -rrb- in cash .
Head Entity: directv
Tail Entity: liberty media
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization shareholders":
Relation: organization shareholders  
Context: In 2020, Amazon secured a significant investment from Berkshire Hathaway, which acquired a notable share in the tech giant, further solidifying its position in the market.  
Head Entity: Amazon  
Tail Entity: Berkshire Hathaway  

Relation: organization shareholders  
Context: Tesla announced that it had sold a portion of its shares to a group of investors led by Fidelity, enhancing their stake in the electric vehicle manufacturer.  
Head Entity: Tesla  
Tail Entity: Fidelity  
#### organization dissolved ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization dissolved" means an organization was dissolved in a date.
Here is an example:
Relation: organization dissolved
Context: the tse suffered its worst-ever system crash in november 2005 which paralyzed the world 's second largest bourse and forced it to shelve plan for a listing of its own .
Head Entity: tse
Tail Entity: november 2005
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization dissolved":
Relation: organization dissolved  
Context: the once-prominent tech startup, Innovatech, officially ceased operations in early 2020 after struggling to secure funding.  
Head Entity: Innovatech  
Tail Entity: early 2020  

Relation: organization dissolved  
Context: after years of financial difficulties, the local arts council announced its closure in the summer of 2019, leaving many artists without support.  
Head Entity: local arts council  
Tail Entity: summer of 2019  
#### organization founded by ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization founded by" means an organization was found by a person.
Here is an example:
Relation: organization founded by
Context: `` i have no greater obligation than to ensure the safely of airline travelers in this country , '' transportation secretary ray lahood said in a joint statement with j. randolph babbitt , administrator of the federal aviation administration , that was issued on the eve of a senate hearing on aviation safety .
Head Entity: federal aviation administration
Tail Entity: j. randolph babbitt
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization founded by":
Relation: organization founded by  
Context: In 1975, the renowned physicist and entrepreneur, Dr. Jane Smith, established Quantum Innovations, a company dedicated to advancing quantum computing technologies.  
Head Entity: Quantum Innovations  
Tail Entity: Dr. Jane Smith  

Relation: organization founded by  
Context: The charity organization, Hope for Tomorrow, was created in 2010 by the well-known philanthropist, John Doe, to support underprivileged children around the world.  
Head Entity: Hope for Tomorrow  
Tail Entity: John Doe  
Mixup data size:  283
MixupTrain:  epoch  0, batch     0 | loss: 2.4351014MixupTrain:  epoch  0, batch     1 | loss: 1.7684877MixupTrain:  epoch  0, batch     2 | loss: 2.4472416MixupTrain:  epoch  0, batch     3 | loss: 2.8993687MixupTrain:  epoch  0, batch     4 | loss: 2.2222359MixupTrain:  epoch  0, batch     5 | loss: 2.2896016MixupTrain:  epoch  0, batch     6 | loss: 2.3130572MixupTrain:  epoch  0, batch     7 | loss: 2.7160781MixupTrain:  epoch  0, batch     8 | loss: 2.5386572MixupTrain:  epoch  0, batch     9 | loss: 2.2912609MixupTrain:  epoch  0, batch    10 | loss: 2.2473692MixupTrain:  epoch  0, batch    11 | loss: 2.7281536MixupTrain:  epoch  0, batch    12 | loss: 2.4408637MixupTrain:  epoch  0, batch    13 | loss: 2.2783997MixupTrain:  epoch  0, batch    14 | loss: 2.2365695MixupTrain:  epoch  0, batch    15 | loss: 2.1169201MixupTrain:  epoch  0, batch    16 | loss: 2.2149069MixupTrain:  epoch  0, batch    17 | loss: 1.9582957
MemoryTrain:  epoch  0, batch     0 | loss: 1.4949241MemoryTrain:  epoch  0, batch     1 | loss: 2.0803020MemoryTrain:  epoch  0, batch     2 | loss: 2.2287807MemoryTrain:  epoch  0, batch     3 | loss: 2.1847200MemoryTrain:  epoch  0, batch     4 | loss: 2.4679472MemoryTrain:  epoch  0, batch     5 | loss: 2.3350587MemoryTrain:  epoch  0, batch     6 | loss: 3.0147128MemoryTrain:  epoch  0, batch     7 | loss: 3.1516745MemoryTrain:  epoch  1, batch     0 | loss: 2.7310815MemoryTrain:  epoch  1, batch     1 | loss: 1.9038019MemoryTrain:  epoch  1, batch     2 | loss: 2.2154665MemoryTrain:  epoch  1, batch     3 | loss: 2.0049086MemoryTrain:  epoch  1, batch     4 | loss: 1.8476877MemoryTrain:  epoch  1, batch     5 | loss: 1.6462405MemoryTrain:  epoch  1, batch     6 | loss: 2.5751052MemoryTrain:  epoch  1, batch     7 | loss: 2.1745384MemoryTrain:  epoch  2, batch     0 | loss: 2.1937575MemoryTrain:  epoch  2, batch     1 | loss: 2.0304418MemoryTrain:  epoch  2, batch     2 | loss: 1.3689849MemoryTrain:  epoch  2, batch     3 | loss: 1.8427265MemoryTrain:  epoch  2, batch     4 | loss: 1.6842639MemoryTrain:  epoch  2, batch     5 | loss: 1.8341084MemoryTrain:  epoch  2, batch     6 | loss: 1.7012603MemoryTrain:  epoch  2, batch     7 | loss: 1.5353454MemoryTrain:  epoch  3, batch     0 | loss: 1.9234178MemoryTrain:  epoch  3, batch     1 | loss: 1.6163489MemoryTrain:  epoch  3, batch     2 | loss: 1.7380981MemoryTrain:  epoch  3, batch     3 | loss: 1.5230873MemoryTrain:  epoch  3, batch     4 | loss: 1.3753514MemoryTrain:  epoch  3, batch     5 | loss: 1.6235096MemoryTrain:  epoch  3, batch     6 | loss: 1.5535443MemoryTrain:  epoch  3, batch     7 | loss: 1.5134586MemoryTrain:  epoch  4, batch     0 | loss: 1.3515570MemoryTrain:  epoch  4, batch     1 | loss: 1.9099737MemoryTrain:  epoch  4, batch     2 | loss: 1.3129985MemoryTrain:  epoch  4, batch     3 | loss: 1.4428988MemoryTrain:  epoch  4, batch     4 | loss: 1.6611696MemoryTrain:  epoch  4, batch     5 | loss: 1.5671446MemoryTrain:  epoch  4, batch     6 | loss: 1.5378091MemoryTrain:  epoch  4, batch     7 | loss: 1.8352138MemoryTrain:  epoch  5, batch     0 | loss: 1.6614332MemoryTrain:  epoch  5, batch     1 | loss: 1.4252968MemoryTrain:  epoch  5, batch     2 | loss: 1.6664742MemoryTrain:  epoch  5, batch     3 | loss: 1.5534313MemoryTrain:  epoch  5, batch     4 | loss: 1.6277167MemoryTrain:  epoch  5, batch     5 | loss: 1.3863139MemoryTrain:  epoch  5, batch     6 | loss: 1.3162460MemoryTrain:  epoch  5, batch     7 | loss: 1.3618279MemoryTrain:  epoch  6, batch     0 | loss: 1.3586102MemoryTrain:  epoch  6, batch     1 | loss: 1.2999713MemoryTrain:  epoch  6, batch     2 | loss: 1.3664954MemoryTrain:  epoch  6, batch     3 | loss: 1.3238566MemoryTrain:  epoch  6, batch     4 | loss: 1.6355820MemoryTrain:  epoch  6, batch     5 | loss: 1.3881066MemoryTrain:  epoch  6, batch     6 | loss: 1.4941218MemoryTrain:  epoch  6, batch     7 | loss: 1.4855560MemoryTrain:  epoch  7, batch     0 | loss: 1.3113135MemoryTrain:  epoch  7, batch     1 | loss: 1.5153199MemoryTrain:  epoch  7, batch     2 | loss: 1.3118628MemoryTrain:  epoch  7, batch     3 | loss: 1.5103736MemoryTrain:  epoch  7, batch     4 | loss: 1.3672438MemoryTrain:  epoch  7, batch     5 | loss: 1.2203379MemoryTrain:  epoch  7, batch     6 | loss: 1.4085418MemoryTrain:  epoch  7, batch     7 | loss: 1.3882843MemoryTrain:  epoch  8, batch     0 | loss: 1.4133637MemoryTrain:  epoch  8, batch     1 | loss: 1.4234970MemoryTrain:  epoch  8, batch     2 | loss: 1.4133257MemoryTrain:  epoch  8, batch     3 | loss: 1.3060917MemoryTrain:  epoch  8, batch     4 | loss: 1.2869744MemoryTrain:  epoch  8, batch     5 | loss: 1.3241084MemoryTrain:  epoch  8, batch     6 | loss: 1.3090584MemoryTrain:  epoch  8, batch     7 | loss: 1.3142964MemoryTrain:  epoch  9, batch     0 | loss: 1.3856535MemoryTrain:  epoch  9, batch     1 | loss: 1.3168161MemoryTrain:  epoch  9, batch     2 | loss: 1.3076261MemoryTrain:  epoch  9, batch     3 | loss: 1.3461441MemoryTrain:  epoch  9, batch     4 | loss: 1.2545264MemoryTrain:  epoch  9, batch     5 | loss: 1.4016412MemoryTrain:  epoch  9, batch     6 | loss: 1.2754203MemoryTrain:  epoch  9, batch     7 | loss: 1.2011073
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 88.54%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 90.18%   [EVAL] batch:    7 | acc: 12.50%,  total acc: 80.47%   
[EVAL] batch:    0 | acc: 37.50%,  total acc: 37.50%   [EVAL] batch:    1 | acc: 43.75%,  total acc: 40.62%   [EVAL] batch:    2 | acc: 31.25%,  total acc: 37.50%   [EVAL] batch:    3 | acc: 25.00%,  total acc: 34.38%   [EVAL] batch:    4 | acc: 50.00%,  total acc: 37.50%   [EVAL] batch:    5 | acc: 50.00%,  total acc: 39.58%   [EVAL] batch:    6 | acc: 25.00%,  total acc: 37.50%   [EVAL] batch:    7 | acc: 0.00%,  total acc: 32.81%   [EVAL] batch:    8 | acc: 12.50%,  total acc: 30.56%   [EVAL] batch:    9 | acc: 12.50%,  total acc: 28.75%   [EVAL] batch:   10 | acc: 0.00%,  total acc: 26.14%   [EVAL] batch:   11 | acc: 12.50%,  total acc: 25.00%   [EVAL] batch:   12 | acc: 18.75%,  total acc: 24.52%   [EVAL] batch:   13 | acc: 43.75%,  total acc: 25.89%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 28.75%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 30.47%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 33.09%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 34.72%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 36.18%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 38.44%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 41.37%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 44.03%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 46.47%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 48.44%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 50.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 52.40%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 53.94%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 55.58%   [EVAL] batch:   28 | acc: 87.50%,  total acc: 56.68%   [EVAL] batch:   29 | acc: 75.00%,  total acc: 57.29%   [EVAL] batch:   30 | acc: 81.25%,  total acc: 58.06%   [EVAL] batch:   31 | acc: 87.50%,  total acc: 58.98%   [EVAL] batch:   32 | acc: 50.00%,  total acc: 58.71%   [EVAL] batch:   33 | acc: 43.75%,  total acc: 58.27%   [EVAL] batch:   34 | acc: 50.00%,  total acc: 58.04%   [EVAL] batch:   35 | acc: 50.00%,  total acc: 57.81%   [EVAL] batch:   36 | acc: 37.50%,  total acc: 57.26%   [EVAL] batch:   37 | acc: 25.00%,  total acc: 56.41%   [EVAL] batch:   38 | acc: 18.75%,  total acc: 55.45%   [EVAL] batch:   39 | acc: 31.25%,  total acc: 54.84%   [EVAL] batch:   40 | acc: 31.25%,  total acc: 54.27%   [EVAL] batch:   41 | acc: 37.50%,  total acc: 53.87%   [EVAL] batch:   42 | acc: 56.25%,  total acc: 53.92%   [EVAL] batch:   43 | acc: 50.00%,  total acc: 53.84%   [EVAL] batch:   44 | acc: 25.00%,  total acc: 53.19%   [EVAL] batch:   45 | acc: 6.25%,  total acc: 52.17%   [EVAL] batch:   46 | acc: 6.25%,  total acc: 51.20%   [EVAL] batch:   47 | acc: 0.00%,  total acc: 50.13%   [EVAL] batch:   48 | acc: 0.00%,  total acc: 49.11%   [EVAL] batch:   49 | acc: 6.25%,  total acc: 48.25%   [EVAL] batch:   50 | acc: 87.50%,  total acc: 49.02%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 49.88%   [EVAL] batch:   52 | acc: 87.50%,  total acc: 50.59%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 51.16%   [EVAL] batch:   54 | acc: 56.25%,  total acc: 51.25%   [EVAL] batch:   55 | acc: 6.25%,  total acc: 50.45%   [EVAL] batch:   56 | acc: 0.00%,  total acc: 49.56%   [EVAL] batch:   57 | acc: 0.00%,  total acc: 48.71%   [EVAL] batch:   58 | acc: 18.75%,  total acc: 48.20%   [EVAL] batch:   59 | acc: 0.00%,  total acc: 47.40%   [EVAL] batch:   60 | acc: 6.25%,  total acc: 46.72%   [EVAL] batch:   61 | acc: 18.75%,  total acc: 46.27%   [EVAL] batch:   62 | acc: 6.25%,  total acc: 45.63%   [EVAL] batch:   63 | acc: 0.00%,  total acc: 44.92%   [EVAL] batch:   64 | acc: 0.00%,  total acc: 44.23%   [EVAL] batch:   65 | acc: 12.50%,  total acc: 43.75%   [EVAL] batch:   66 | acc: 18.75%,  total acc: 43.38%   [EVAL] batch:   67 | acc: 12.50%,  total acc: 42.92%   [EVAL] batch:   68 | acc: 25.00%,  total acc: 42.66%   [EVAL] batch:   69 | acc: 18.75%,  total acc: 42.32%   [EVAL] batch:   70 | acc: 18.75%,  total acc: 41.99%   [EVAL] batch:   71 | acc: 6.25%,  total acc: 41.49%   [EVAL] batch:   72 | acc: 68.75%,  total acc: 41.87%   [EVAL] batch:   73 | acc: 87.50%,  total acc: 42.48%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 43.08%   [EVAL] batch:   75 | acc: 93.75%,  total acc: 43.75%   [EVAL] batch:   76 | acc: 75.00%,  total acc: 44.16%   [EVAL] batch:   77 | acc: 37.50%,  total acc: 44.07%   [EVAL] batch:   78 | acc: 12.50%,  total acc: 43.67%   [EVAL] batch:   79 | acc: 31.25%,  total acc: 43.52%   [EVAL] batch:   80 | acc: 37.50%,  total acc: 43.44%   [EVAL] batch:   81 | acc: 75.00%,  total acc: 43.83%   [EVAL] batch:   82 | acc: 56.25%,  total acc: 43.98%   [EVAL] batch:   83 | acc: 43.75%,  total acc: 43.97%   [EVAL] batch:   84 | acc: 50.00%,  total acc: 44.04%   [EVAL] batch:   85 | acc: 81.25%,  total acc: 44.48%   [EVAL] batch:   86 | acc: 43.75%,  total acc: 44.47%   [EVAL] batch:   87 | acc: 87.50%,  total acc: 44.96%   [EVAL] batch:   88 | acc: 87.50%,  total acc: 45.44%   [EVAL] batch:   89 | acc: 12.50%,  total acc: 45.07%   [EVAL] batch:   90 | acc: 50.00%,  total acc: 45.12%   [EVAL] batch:   91 | acc: 43.75%,  total acc: 45.11%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 45.70%   [EVAL] batch:   93 | acc: 100.00%,  total acc: 46.28%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 46.84%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 47.40%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 47.94%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 48.47%   [EVAL] batch:   98 | acc: 93.75%,  total acc: 48.93%   [EVAL] batch:   99 | acc: 81.25%,  total acc: 49.25%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 49.75%   [EVAL] batch:  101 | acc: 100.00%,  total acc: 50.25%   [EVAL] batch:  102 | acc: 100.00%,  total acc: 50.73%   [EVAL] batch:  103 | acc: 100.00%,  total acc: 51.20%   [EVAL] batch:  104 | acc: 100.00%,  total acc: 51.67%   [EVAL] batch:  105 | acc: 100.00%,  total acc: 52.12%   [EVAL] batch:  106 | acc: 81.25%,  total acc: 52.39%   [EVAL] batch:  107 | acc: 0.00%,  total acc: 51.91%   [EVAL] batch:  108 | acc: 18.75%,  total acc: 51.61%   [EVAL] batch:  109 | acc: 93.75%,  total acc: 51.99%   [EVAL] batch:  110 | acc: 62.50%,  total acc: 52.08%   [EVAL] batch:  111 | acc: 87.50%,  total acc: 52.40%   [EVAL] batch:  112 | acc: 56.25%,  total acc: 52.43%   [EVAL] batch:  113 | acc: 43.75%,  total acc: 52.36%   [EVAL] batch:  114 | acc: 37.50%,  total acc: 52.23%   [EVAL] batch:  115 | acc: 12.50%,  total acc: 51.89%   [EVAL] batch:  116 | acc: 25.00%,  total acc: 51.66%   [EVAL] batch:  117 | acc: 6.25%,  total acc: 51.27%   [EVAL] batch:  118 | acc: 37.50%,  total acc: 51.16%   [EVAL] batch:  119 | acc: 93.75%,  total acc: 51.51%   [EVAL] batch:  120 | acc: 100.00%,  total acc: 51.91%   [EVAL] batch:  121 | acc: 87.50%,  total acc: 52.20%   [EVAL] batch:  122 | acc: 68.75%,  total acc: 52.34%   [EVAL] batch:  123 | acc: 87.50%,  total acc: 52.62%   [EVAL] batch:  124 | acc: 75.00%,  total acc: 52.80%   [EVAL] batch:  125 | acc: 62.50%,  total acc: 52.88%   [EVAL] batch:  126 | acc: 93.75%,  total acc: 53.20%   [EVAL] batch:  127 | acc: 75.00%,  total acc: 53.37%   [EVAL] batch:  128 | acc: 93.75%,  total acc: 53.68%   [EVAL] batch:  129 | acc: 93.75%,  total acc: 53.99%   [EVAL] batch:  130 | acc: 87.50%,  total acc: 54.25%   [EVAL] batch:  131 | acc: 100.00%,  total acc: 54.59%   [EVAL] batch:  132 | acc: 62.50%,  total acc: 54.65%   
cur_acc:  ['0.8485', '0.7548', '0.5833', '0.4460', '0.7674', '0.7991', '0.5848', '0.8047']
his_acc:  ['0.8485', '0.8403', '0.7365', '0.6489', '0.6307', '0.6267', '0.6235', '0.5465']
--------Round  5
seed:  600
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/test.pkl
Task_order: [7 2 0 1 6 3 4 5]
prepared data!
CurrentTrain: epoch  0, batch     0 | loss: 13.3486280CurrentTrain: epoch  0, batch     1 | loss: 13.2682858CurrentTrain: epoch  0, batch     2 | loss: 13.1237602CurrentTrain: epoch  0, batch     3 | loss: 12.7070904CurrentTrain: epoch  0, batch     4 | loss: 12.7890549CurrentTrain: epoch  0, batch     5 | loss: 12.5851784CurrentTrain: epoch  0, batch     6 | loss: 12.5730190CurrentTrain: epoch  0, batch     7 | loss: 12.5471783CurrentTrain: epoch  0, batch     8 | loss: 12.2456646CurrentTrain: epoch  0, batch     9 | loss: 11.9673777CurrentTrain: epoch  0, batch    10 | loss: 11.9864101CurrentTrain: epoch  0, batch    11 | loss: 12.1747637CurrentTrain: epoch  0, batch    12 | loss: 11.9240189CurrentTrain: epoch  0, batch    13 | loss: 11.9373093CurrentTrain: epoch  0, batch    14 | loss: 12.0336876CurrentTrain: epoch  0, batch    15 | loss: 11.7283249CurrentTrain: epoch  0, batch    16 | loss: 11.8810186CurrentTrain: epoch  0, batch    17 | loss: 11.5449810CurrentTrain: epoch  0, batch    18 | loss: 11.3829842CurrentTrain: epoch  0, batch    19 | loss: 11.3131046CurrentTrain: epoch  0, batch    20 | loss: 11.5945044CurrentTrain: epoch  0, batch    21 | loss: 11.5040321CurrentTrain: epoch  0, batch    22 | loss: 11.7934017CurrentTrain: epoch  0, batch    23 | loss: 11.3635406CurrentTrain: epoch  0, batch    24 | loss: 11.1415796CurrentTrain: epoch  0, batch    25 | loss: 11.2868977CurrentTrain: epoch  0, batch    26 | loss: 11.0292053CurrentTrain: epoch  0, batch    27 | loss: 11.1909981CurrentTrain: epoch  0, batch    28 | loss: 10.7094707CurrentTrain: epoch  0, batch    29 | loss: 11.0482101CurrentTrain: epoch  0, batch    30 | loss: 10.8596039CurrentTrain: epoch  0, batch    31 | loss: 10.4104786CurrentTrain: epoch  0, batch    32 | loss: 10.4878979CurrentTrain: epoch  0, batch    33 | loss: 10.8982239CurrentTrain: epoch  0, batch    34 | loss: 10.7948017CurrentTrain: epoch  0, batch    35 | loss: 11.0887966CurrentTrain: epoch  0, batch    36 | loss: 10.4598455CurrentTrain: epoch  0, batch    37 | loss: 10.6621656CurrentTrain: epoch  1, batch     0 | loss: 10.2135429CurrentTrain: epoch  1, batch     1 | loss: 10.4829292CurrentTrain: epoch  1, batch     2 | loss: 10.2119226CurrentTrain: epoch  1, batch     3 | loss: 9.7632732CurrentTrain: epoch  1, batch     4 | loss: 10.2483788CurrentTrain: epoch  1, batch     5 | loss: 9.7284107CurrentTrain: epoch  1, batch     6 | loss: 10.2647247CurrentTrain: epoch  1, batch     7 | loss: 10.1758041CurrentTrain: epoch  1, batch     8 | loss: 9.9460602CurrentTrain: epoch  1, batch     9 | loss: 9.4444180CurrentTrain: epoch  1, batch    10 | loss: 10.0518694CurrentTrain: epoch  1, batch    11 | loss: 9.9704170CurrentTrain: epoch  1, batch    12 | loss: 10.2063923CurrentTrain: epoch  1, batch    13 | loss: 9.9083023CurrentTrain: epoch  1, batch    14 | loss: 9.2485237CurrentTrain: epoch  1, batch    15 | loss: 9.9018879CurrentTrain: epoch  1, batch    16 | loss: 10.0716276CurrentTrain: epoch  1, batch    17 | loss: 9.7307663CurrentTrain: epoch  1, batch    18 | loss: 9.6816044CurrentTrain: epoch  1, batch    19 | loss: 9.6564312CurrentTrain: epoch  1, batch    20 | loss: 9.5990419CurrentTrain: epoch  1, batch    21 | loss: 9.4305534CurrentTrain: epoch  1, batch    22 | loss: 9.0201359CurrentTrain: epoch  1, batch    23 | loss: 9.1338348CurrentTrain: epoch  1, batch    24 | loss: 9.4511089CurrentTrain: epoch  1, batch    25 | loss: 8.7642946CurrentTrain: epoch  1, batch    26 | loss: 9.1869431CurrentTrain: epoch  1, batch    27 | loss: 9.1969090CurrentTrain: epoch  1, batch    28 | loss: 9.8480349CurrentTrain: epoch  1, batch    29 | loss: 9.1910419CurrentTrain: epoch  1, batch    30 | loss: 9.5420265CurrentTrain: epoch  1, batch    31 | loss: 9.2200584CurrentTrain: epoch  1, batch    32 | loss: 9.0687056CurrentTrain: epoch  1, batch    33 | loss: 8.5574570CurrentTrain: epoch  1, batch    34 | loss: 8.8869162CurrentTrain: epoch  1, batch    35 | loss: 9.5987644CurrentTrain: epoch  1, batch    36 | loss: 8.9634438CurrentTrain: epoch  1, batch    37 | loss: 8.8570032CurrentTrain: epoch  2, batch     0 | loss: 8.5796165CurrentTrain: epoch  2, batch     1 | loss: 9.0756912CurrentTrain: epoch  2, batch     2 | loss: 8.7080631CurrentTrain: epoch  2, batch     3 | loss: 8.3278847CurrentTrain: epoch  2, batch     4 | loss: 8.1251812CurrentTrain: epoch  2, batch     5 | loss: 7.8665400CurrentTrain: epoch  2, batch     6 | loss: 7.7161293CurrentTrain: epoch  2, batch     7 | loss: 8.6140976CurrentTrain: epoch  2, batch     8 | loss: 8.1182032CurrentTrain: epoch  2, batch     9 | loss: 8.1526661CurrentTrain: epoch  2, batch    10 | loss: 8.5370541CurrentTrain: epoch  2, batch    11 | loss: 8.3064737CurrentTrain: epoch  2, batch    12 | loss: 8.0418482CurrentTrain: epoch  2, batch    13 | loss: 7.9605355CurrentTrain: epoch  2, batch    14 | loss: 8.6129847CurrentTrain: epoch  2, batch    15 | loss: 7.8330998CurrentTrain: epoch  2, batch    16 | loss: 8.2170544CurrentTrain: epoch  2, batch    17 | loss: 8.4838276CurrentTrain: epoch  2, batch    18 | loss: 8.3526096CurrentTrain: epoch  2, batch    19 | loss: 9.5529461CurrentTrain: epoch  2, batch    20 | loss: 7.7355833CurrentTrain: epoch  2, batch    21 | loss: 8.4335375CurrentTrain: epoch  2, batch    22 | loss: 8.8599195CurrentTrain: epoch  2, batch    23 | loss: 7.6132250CurrentTrain: epoch  2, batch    24 | loss: 8.8958311CurrentTrain: epoch  2, batch    25 | loss: 7.3474989CurrentTrain: epoch  2, batch    26 | loss: 7.4063449CurrentTrain: epoch  2, batch    27 | loss: 8.4205828CurrentTrain: epoch  2, batch    28 | loss: 8.3812389CurrentTrain: epoch  2, batch    29 | loss: 7.4017458CurrentTrain: epoch  2, batch    30 | loss: 8.0170374CurrentTrain: epoch  2, batch    31 | loss: 7.7332292CurrentTrain: epoch  2, batch    32 | loss: 8.2034464CurrentTrain: epoch  2, batch    33 | loss: 7.8513155CurrentTrain: epoch  2, batch    34 | loss: 8.1527863CurrentTrain: epoch  2, batch    35 | loss: 7.6082850CurrentTrain: epoch  2, batch    36 | loss: 7.4782691CurrentTrain: epoch  2, batch    37 | loss: 8.8236198CurrentTrain: epoch  3, batch     0 | loss: 7.2988114CurrentTrain: epoch  3, batch     1 | loss: 7.1258144CurrentTrain: epoch  3, batch     2 | loss: 7.1639524CurrentTrain: epoch  3, batch     3 | loss: 7.8302889CurrentTrain: epoch  3, batch     4 | loss: 7.4882436CurrentTrain: epoch  3, batch     5 | loss: 8.1618614CurrentTrain: epoch  3, batch     6 | loss: 7.8082914CurrentTrain: epoch  3, batch     7 | loss: 7.6538715CurrentTrain: epoch  3, batch     8 | loss: 7.4935980CurrentTrain: epoch  3, batch     9 | loss: 7.6122007CurrentTrain: epoch  3, batch    10 | loss: 7.3714705CurrentTrain: epoch  3, batch    11 | loss: 6.9472733CurrentTrain: epoch  3, batch    12 | loss: 7.5688410CurrentTrain: epoch  3, batch    13 | loss: 7.2685041CurrentTrain: epoch  3, batch    14 | loss: 7.9500675CurrentTrain: epoch  3, batch    15 | loss: 6.6643939CurrentTrain: epoch  3, batch    16 | loss: 7.0655427CurrentTrain: epoch  3, batch    17 | loss: 7.8885899CurrentTrain: epoch  3, batch    18 | loss: 7.2250147CurrentTrain: epoch  3, batch    19 | loss: 6.8781328CurrentTrain: epoch  3, batch    20 | loss: 7.6172304CurrentTrain: epoch  3, batch    21 | loss: 8.0263138CurrentTrain: epoch  3, batch    22 | loss: 7.6179085CurrentTrain: epoch  3, batch    23 | loss: 7.5600467CurrentTrain: epoch  3, batch    24 | loss: 6.8552876CurrentTrain: epoch  3, batch    25 | loss: 7.7723479CurrentTrain: epoch  3, batch    26 | loss: 7.2439256CurrentTrain: epoch  3, batch    27 | loss: 8.2016859CurrentTrain: epoch  3, batch    28 | loss: 6.7946415CurrentTrain: epoch  3, batch    29 | loss: 7.3721309CurrentTrain: epoch  3, batch    30 | loss: 5.9602990CurrentTrain: epoch  3, batch    31 | loss: 6.7175789CurrentTrain: epoch  3, batch    32 | loss: 7.2499571CurrentTrain: epoch  3, batch    33 | loss: 6.9426575CurrentTrain: epoch  3, batch    34 | loss: 7.2260981CurrentTrain: epoch  3, batch    35 | loss: 7.7662659CurrentTrain: epoch  3, batch    36 | loss: 6.7369604CurrentTrain: epoch  3, batch    37 | loss: 7.5918570CurrentTrain: epoch  4, batch     0 | loss: 6.2031460CurrentTrain: epoch  4, batch     1 | loss: 7.8684244CurrentTrain: epoch  4, batch     2 | loss: 7.2878485CurrentTrain: epoch  4, batch     3 | loss: 6.5051513CurrentTrain: epoch  4, batch     4 | loss: 6.4639583CurrentTrain: epoch  4, batch     5 | loss: 7.0450211CurrentTrain: epoch  4, batch     6 | loss: 7.0133915CurrentTrain: epoch  4, batch     7 | loss: 7.1537685CurrentTrain: epoch  4, batch     8 | loss: 6.7227850CurrentTrain: epoch  4, batch     9 | loss: 7.1629243CurrentTrain: epoch  4, batch    10 | loss: 6.7048626CurrentTrain: epoch  4, batch    11 | loss: 6.5050688CurrentTrain: epoch  4, batch    12 | loss: 6.6162033CurrentTrain: epoch  4, batch    13 | loss: 6.8443518CurrentTrain: epoch  4, batch    14 | loss: 6.5155983CurrentTrain: epoch  4, batch    15 | loss: 6.5322146CurrentTrain: epoch  4, batch    16 | loss: 6.1593456CurrentTrain: epoch  4, batch    17 | loss: 5.8937149CurrentTrain: epoch  4, batch    18 | loss: 6.7302098CurrentTrain: epoch  4, batch    19 | loss: 7.0018673CurrentTrain: epoch  4, batch    20 | loss: 7.4686480CurrentTrain: epoch  4, batch    21 | loss: 7.0041294CurrentTrain: epoch  4, batch    22 | loss: 6.9554863CurrentTrain: epoch  4, batch    23 | loss: 6.9110222CurrentTrain: epoch  4, batch    24 | loss: 7.2754664CurrentTrain: epoch  4, batch    25 | loss: 6.9615655CurrentTrain: epoch  4, batch    26 | loss: 7.6385174CurrentTrain: epoch  4, batch    27 | loss: 6.9148560CurrentTrain: epoch  4, batch    28 | loss: 7.1941710CurrentTrain: epoch  4, batch    29 | loss: 6.0425696CurrentTrain: epoch  4, batch    30 | loss: 6.7000351CurrentTrain: epoch  4, batch    31 | loss: 6.1049185CurrentTrain: epoch  4, batch    32 | loss: 6.4165006CurrentTrain: epoch  4, batch    33 | loss: 7.1371603CurrentTrain: epoch  4, batch    34 | loss: 6.7402611CurrentTrain: epoch  4, batch    35 | loss: 5.4684715CurrentTrain: epoch  4, batch    36 | loss: 6.9511337CurrentTrain: epoch  4, batch    37 | loss: 6.5544376CurrentTrain: epoch  5, batch     0 | loss: 7.0547719CurrentTrain: epoch  5, batch     1 | loss: 6.0028005CurrentTrain: epoch  5, batch     2 | loss: 6.4995928CurrentTrain: epoch  5, batch     3 | loss: 6.0769796CurrentTrain: epoch  5, batch     4 | loss: 6.6319575CurrentTrain: epoch  5, batch     5 | loss: 6.0475740CurrentTrain: epoch  5, batch     6 | loss: 6.6738853CurrentTrain: epoch  5, batch     7 | loss: 6.6724133CurrentTrain: epoch  5, batch     8 | loss: 6.4491787CurrentTrain: epoch  5, batch     9 | loss: 5.9839492CurrentTrain: epoch  5, batch    10 | loss: 7.0455227CurrentTrain: epoch  5, batch    11 | loss: 6.3519502CurrentTrain: epoch  5, batch    12 | loss: 5.8502693CurrentTrain: epoch  5, batch    13 | loss: 5.7991323CurrentTrain: epoch  5, batch    14 | loss: 6.2346635CurrentTrain: epoch  5, batch    15 | loss: 6.7073679CurrentTrain: epoch  5, batch    16 | loss: 6.1003008CurrentTrain: epoch  5, batch    17 | loss: 6.0592637CurrentTrain: epoch  5, batch    18 | loss: 6.0138226CurrentTrain: epoch  5, batch    19 | loss: 6.5249181CurrentTrain: epoch  5, batch    20 | loss: 5.8202295CurrentTrain: epoch  5, batch    21 | loss: 6.2300072CurrentTrain: epoch  5, batch    22 | loss: 5.9723020CurrentTrain: epoch  5, batch    23 | loss: 6.4132161CurrentTrain: epoch  5, batch    24 | loss: 6.1865187CurrentTrain: epoch  5, batch    25 | loss: 6.6220427CurrentTrain: epoch  5, batch    26 | loss: 6.4955921CurrentTrain: epoch  5, batch    27 | loss: 6.0245047CurrentTrain: epoch  5, batch    28 | loss: 5.9383087CurrentTrain: epoch  5, batch    29 | loss: 6.2064199CurrentTrain: epoch  5, batch    30 | loss: 5.9190931CurrentTrain: epoch  5, batch    31 | loss: 5.6856594CurrentTrain: epoch  5, batch    32 | loss: 6.5410142CurrentTrain: epoch  5, batch    33 | loss: 6.2271795CurrentTrain: epoch  5, batch    34 | loss: 5.9754000CurrentTrain: epoch  5, batch    35 | loss: 5.9892755CurrentTrain: epoch  5, batch    36 | loss: 6.9547191CurrentTrain: epoch  5, batch    37 | loss: 5.5639195CurrentTrain: epoch  6, batch     0 | loss: 5.5708370CurrentTrain: epoch  6, batch     1 | loss: 6.7486553CurrentTrain: epoch  6, batch     2 | loss: 6.3913317CurrentTrain: epoch  6, batch     3 | loss: 5.6026225CurrentTrain: epoch  6, batch     4 | loss: 5.5205998CurrentTrain: epoch  6, batch     5 | loss: 5.9058104CurrentTrain: epoch  6, batch     6 | loss: 5.2930841CurrentTrain: epoch  6, batch     7 | loss: 5.9581037CurrentTrain: epoch  6, batch     8 | loss: 5.5374022CurrentTrain: epoch  6, batch     9 | loss: 5.8948722CurrentTrain: epoch  6, batch    10 | loss: 5.8748178CurrentTrain: epoch  6, batch    11 | loss: 5.5240288CurrentTrain: epoch  6, batch    12 | loss: 6.0009060CurrentTrain: epoch  6, batch    13 | loss: 6.8303800CurrentTrain: epoch  6, batch    14 | loss: 6.1835070CurrentTrain: epoch  6, batch    15 | loss: 6.0442419CurrentTrain: epoch  6, batch    16 | loss: 5.6404347CurrentTrain: epoch  6, batch    17 | loss: 5.7508092CurrentTrain: epoch  6, batch    18 | loss: 5.6386518CurrentTrain: epoch  6, batch    19 | loss: 5.7378693CurrentTrain: epoch  6, batch    20 | loss: 6.1356621CurrentTrain: epoch  6, batch    21 | loss: 5.9641695CurrentTrain: epoch  6, batch    22 | loss: 5.5549846CurrentTrain: epoch  6, batch    23 | loss: 5.7274785CurrentTrain: epoch  6, batch    24 | loss: 5.4564567CurrentTrain: epoch  6, batch    25 | loss: 7.0419521CurrentTrain: epoch  6, batch    26 | loss: 6.7567310CurrentTrain: epoch  6, batch    27 | loss: 5.6624241CurrentTrain: epoch  6, batch    28 | loss: 5.6651893CurrentTrain: epoch  6, batch    29 | loss: 5.5376740CurrentTrain: epoch  6, batch    30 | loss: 6.0144992CurrentTrain: epoch  6, batch    31 | loss: 5.8770976CurrentTrain: epoch  6, batch    32 | loss: 6.0436020CurrentTrain: epoch  6, batch    33 | loss: 5.9916668CurrentTrain: epoch  6, batch    34 | loss: 5.8507481CurrentTrain: epoch  6, batch    35 | loss: 6.2214756CurrentTrain: epoch  6, batch    36 | loss: 5.9797158CurrentTrain: epoch  6, batch    37 | loss: 5.4864883CurrentTrain: epoch  7, batch     0 | loss: 6.3326554CurrentTrain: epoch  7, batch     1 | loss: 5.0489321CurrentTrain: epoch  7, batch     2 | loss: 6.1005220CurrentTrain: epoch  7, batch     3 | loss: 5.8789802CurrentTrain: epoch  7, batch     4 | loss: 5.5038323CurrentTrain: epoch  7, batch     5 | loss: 5.6032848CurrentTrain: epoch  7, batch     6 | loss: 5.2649231CurrentTrain: epoch  7, batch     7 | loss: 6.0065589CurrentTrain: epoch  7, batch     8 | loss: 6.2822332CurrentTrain: epoch  7, batch     9 | loss: 5.6554918CurrentTrain: epoch  7, batch    10 | loss: 5.7713633CurrentTrain: epoch  7, batch    11 | loss: 5.7778039CurrentTrain: epoch  7, batch    12 | loss: 5.7755160CurrentTrain: epoch  7, batch    13 | loss: 5.5829411CurrentTrain: epoch  7, batch    14 | loss: 5.1710386CurrentTrain: epoch  7, batch    15 | loss: 5.4266624CurrentTrain: epoch  7, batch    16 | loss: 5.5442657CurrentTrain: epoch  7, batch    17 | loss: 5.7261219CurrentTrain: epoch  7, batch    18 | loss: 5.5203743CurrentTrain: epoch  7, batch    19 | loss: 5.2661667CurrentTrain: epoch  7, batch    20 | loss: 5.7627082CurrentTrain: epoch  7, batch    21 | loss: 5.4582100CurrentTrain: epoch  7, batch    22 | loss: 5.5782681CurrentTrain: epoch  7, batch    23 | loss: 5.7701306CurrentTrain: epoch  7, batch    24 | loss: 5.4133763CurrentTrain: epoch  7, batch    25 | loss: 5.1810760CurrentTrain: epoch  7, batch    26 | loss: 5.3153033CurrentTrain: epoch  7, batch    27 | loss: 5.7812877CurrentTrain: epoch  7, batch    28 | loss: 5.3767023CurrentTrain: epoch  7, batch    29 | loss: 5.0977764CurrentTrain: epoch  7, batch    30 | loss: 5.4562597CurrentTrain: epoch  7, batch    31 | loss: 5.3494444CurrentTrain: epoch  7, batch    32 | loss: 6.4523916CurrentTrain: epoch  7, batch    33 | loss: 5.2405128CurrentTrain: epoch  7, batch    34 | loss: 6.1330147CurrentTrain: epoch  7, batch    35 | loss: 5.6361260CurrentTrain: epoch  7, batch    36 | loss: 5.3050995CurrentTrain: epoch  7, batch    37 | loss: 6.5744486CurrentTrain: epoch  8, batch     0 | loss: 6.1697321CurrentTrain: epoch  8, batch     1 | loss: 5.3257532CurrentTrain: epoch  8, batch     2 | loss: 5.8427706CurrentTrain: epoch  8, batch     3 | loss: 5.9994483CurrentTrain: epoch  8, batch     4 | loss: 5.5632277CurrentTrain: epoch  8, batch     5 | loss: 5.7053094CurrentTrain: epoch  8, batch     6 | loss: 5.3538442CurrentTrain: epoch  8, batch     7 | loss: 5.2824540CurrentTrain: epoch  8, batch     8 | loss: 5.5845661CurrentTrain: epoch  8, batch     9 | loss: 5.4149776CurrentTrain: epoch  8, batch    10 | loss: 5.4356594CurrentTrain: epoch  8, batch    11 | loss: 5.5109639CurrentTrain: epoch  8, batch    12 | loss: 5.5190926CurrentTrain: epoch  8, batch    13 | loss: 5.1125755CurrentTrain: epoch  8, batch    14 | loss: 5.3870306CurrentTrain: epoch  8, batch    15 | loss: 5.0568438CurrentTrain: epoch  8, batch    16 | loss: 5.1997271CurrentTrain: epoch  8, batch    17 | loss: 5.1845956CurrentTrain: epoch  8, batch    18 | loss: 5.1135006CurrentTrain: epoch  8, batch    19 | loss: 5.7366734CurrentTrain: epoch  8, batch    20 | loss: 5.1146927CurrentTrain: epoch  8, batch    21 | loss: 5.2633328CurrentTrain: epoch  8, batch    22 | loss: 5.2831354CurrentTrain: epoch  8, batch    23 | loss: 5.1626282CurrentTrain: epoch  8, batch    24 | loss: 5.0876451CurrentTrain: epoch  8, batch    25 | loss: 4.8665090CurrentTrain: epoch  8, batch    26 | loss: 5.0650544CurrentTrain: epoch  8, batch    27 | loss: 5.1552525CurrentTrain: epoch  8, batch    28 | loss: 5.6456761CurrentTrain: epoch  8, batch    29 | loss: 5.0909562CurrentTrain: epoch  8, batch    30 | loss: 5.6921215CurrentTrain: epoch  8, batch    31 | loss: 5.2702374CurrentTrain: epoch  8, batch    32 | loss: 5.0276623CurrentTrain: epoch  8, batch    33 | loss: 5.0748014CurrentTrain: epoch  8, batch    34 | loss: 5.3380671CurrentTrain: epoch  8, batch    35 | loss: 5.0471411CurrentTrain: epoch  8, batch    36 | loss: 4.9353385CurrentTrain: epoch  8, batch    37 | loss: 5.0773883CurrentTrain: epoch  9, batch     0 | loss: 4.8514347CurrentTrain: epoch  9, batch     1 | loss: 5.0102072CurrentTrain: epoch  9, batch     2 | loss: 5.4732008CurrentTrain: epoch  9, batch     3 | loss: 5.3494749CurrentTrain: epoch  9, batch     4 | loss: 5.2604628CurrentTrain: epoch  9, batch     5 | loss: 4.8760118CurrentTrain: epoch  9, batch     6 | loss: 5.0811243CurrentTrain: epoch  9, batch     7 | loss: 5.0839400CurrentTrain: epoch  9, batch     8 | loss: 5.1842518CurrentTrain: epoch  9, batch     9 | loss: 4.9173603CurrentTrain: epoch  9, batch    10 | loss: 5.6851864CurrentTrain: epoch  9, batch    11 | loss: 4.9308372CurrentTrain: epoch  9, batch    12 | loss: 5.1934972CurrentTrain: epoch  9, batch    13 | loss: 5.0066071CurrentTrain: epoch  9, batch    14 | loss: 5.2586794CurrentTrain: epoch  9, batch    15 | loss: 5.0752668CurrentTrain: epoch  9, batch    16 | loss: 4.9951401CurrentTrain: epoch  9, batch    17 | loss: 4.9129157CurrentTrain: epoch  9, batch    18 | loss: 4.8690500CurrentTrain: epoch  9, batch    19 | loss: 5.9390554CurrentTrain: epoch  9, batch    20 | loss: 5.1330743CurrentTrain: epoch  9, batch    21 | loss: 5.0255399CurrentTrain: epoch  9, batch    22 | loss: 5.0251479CurrentTrain: epoch  9, batch    23 | loss: 4.9360952CurrentTrain: epoch  9, batch    24 | loss: 5.2160740CurrentTrain: epoch  9, batch    25 | loss: 4.9613361CurrentTrain: epoch  9, batch    26 | loss: 5.9303999CurrentTrain: epoch  9, batch    27 | loss: 5.2397766CurrentTrain: epoch  9, batch    28 | loss: 5.1747532CurrentTrain: epoch  9, batch    29 | loss: 5.3883896CurrentTrain: epoch  9, batch    30 | loss: 5.0306358CurrentTrain: epoch  9, batch    31 | loss: 4.8636417CurrentTrain: epoch  9, batch    32 | loss: 5.0962343CurrentTrain: epoch  9, batch    33 | loss: 4.8999205CurrentTrain: epoch  9, batch    34 | loss: 4.9122944CurrentTrain: epoch  9, batch    35 | loss: 5.2439938CurrentTrain: epoch  9, batch    36 | loss: 4.8307743CurrentTrain: epoch  9, batch    37 | loss: 5.0621481
#### person countries of residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person countries of residence" means a person is living in a country.
Here is an example:
Relation: person countries of residence
Context: he also reiterated what press secretary dee dee myers said this morning - that the u.s. is not interested in high-level talks as castro proposed last night .
Head Entity: dee dee myers
Tail Entity: u.s.
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person countries of residence":
Relation: person countries of residence  
Context: After moving to Paris for her art career, she often shares her experiences living in the vibrant city with her followers.  
Head Entity: she  
Tail Entity: Paris  

Relation: person countries of residence  
Context: Following his recent relocation, the author has been exploring the cultural landscape of Tokyo, where he plans to write his next novel.  
Head Entity: the author  
Tail Entity: Tokyo  
#### organization top members employees ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization top members employees" means an organization 's employer is a person.
Here is an example:
Relation: organization top members employees
Context: `` customer relationships are the backbone of banking , and banks are constantly striving to meet customer demands , '' said peter garuccio , spokesman for the american bankers association .
Head Entity: american bankers association
Tail Entity: peter garuccio
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization top members employees":
Relation: organization top members employees  
Context: "As the CEO of the tech giant, she has been instrumental in driving innovation and fostering a culture of collaboration," said a company spokesperson about Jane Doe, who leads the organization.  
Head Entity: tech giant  
Tail Entity: Jane Doe  

Relation: organization top members employees  
Context: "During the annual meeting, the board praised John Smith for his leadership and commitment to the company's vision," reported the financial analyst.  
Head Entity: board  
Tail Entity: John Smith  
#### organization member of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization member of" means an organization is the member of an organization.
Here is an example:
Relation: organization member of
Context: the band delayed a show for an hour in 2003 when game 7 of the al championship series went into extra innings , ending badly for the red sox at the hands of the new york yankees .
Head Entity: red sox
Tail Entity: al
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization member of":
Relation: organization member of  
Context: The tech startup was thrilled to announce its membership in the prestigious Silicon Valley Innovation Network, which includes several leading companies in the industry.  
Head Entity: tech startup  
Tail Entity: Silicon Valley Innovation Network  

Relation: organization member of  
Context: After years of collaboration, the environmental group officially joined the Global Climate Alliance, aiming to strengthen its efforts in combating climate change.  
Head Entity: environmental group  
Tail Entity: Global Climate Alliance  
#### person origin ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person origin" means a person 's nationality is a country.
Here is an example:
Relation: person origin
Context: since the couple 's divorce , aniston has been romantically linked with actor vince vaughn , british model paul sculfor , actor orlando bloom and singer john mayer .
Head Entity: paul sculfor
Tail Entity: british
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person origin":
Relation: person origin  
Context: The renowned physicist Albert Einstein was born in Germany but later became a citizen of the United States.  
Head Entity: Albert Einstein  
Tail Entity: Germany  

Relation: person origin  
Context: The famous author Chimamanda Ngozi Adichie hails from Nigeria, where she was born and raised before moving to the United States for her education.  
Head Entity: Chimamanda Ngozi Adichie  
Tail Entity: Nigeria  
#### person title ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person title" means a person 's title is a title.
Here is an example:
Relation: person title
Context: marcel ospel , the bank 's former chairman who stepped down earlier this year amid heavy criticism of ubs ' staggering losses on its us subprime home loan exposure , accounted for more than two thirds of the sum , pr agency balanx said in a statement tuesday .
Head Entity: marcel ospel
Tail Entity: chairman
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person title":
Relation: person title  
Context: after years of dedicated service, dr. elena rodriguez was honored with the prestigious title of chief medical officer at the health organization, recognizing her contributions to public health initiatives.  
Head Entity: dr. elena rodriguez  
Tail Entity: chief medical officer  

Relation: person title  
Context: during the annual awards ceremony, john smith received the title of best director for his outstanding work in the film industry, showcasing his talent and creativity.  
Head Entity: john smith  
Tail Entity: best director  
#### organization country of headquarters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization country of headquarters" means an organization is located in a country.
Here is an example:
Relation: organization country of headquarters
Context: commander viliame naupoto , chairman of the fiji pine limited announced the woodchips exports target here tuesday after signing a woodchip sale agreement with japan 's itochu corporation .
Head Entity: itochu corporation
Tail Entity: japan
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization country of headquarters":
Relation: organization country of headquarters  
Context: the headquarters of tech giant apple inc. is located in cupertino, california, where it continues to innovate and expand its product line.  
Head Entity: apple inc.  
Tail Entity: california  

Relation: organization country of headquarters  
Context: the united nations, an international organization founded in 1945, has its main headquarters situated in new york city, united states.  
Head Entity: united nations  
Tail Entity: united states  
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 68.75%,  total acc: 75.00%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 73.44%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 72.50%   [EVAL] batch:    5 | acc: 62.50%,  total acc: 70.83%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 73.21%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 76.56%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 79.17%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 80.62%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 82.39%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 83.33%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 84.13%   [EVAL] batch:   13 | acc: 75.00%,  total acc: 83.48%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 82.92%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 81.25%   [EVAL] batch:   16 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 80.21%   [EVAL] batch:   18 | acc: 81.25%,  total acc: 80.26%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 80.62%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 81.55%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 82.39%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 83.15%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 83.85%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 84.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 85.10%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 85.42%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 85.94%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 86.42%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 86.46%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 86.49%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 86.72%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 85.23%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 68.75%,  total acc: 75.00%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 73.44%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 72.50%   [EVAL] batch:    5 | acc: 62.50%,  total acc: 70.83%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 73.21%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 76.56%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 79.17%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 80.62%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 82.39%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 83.33%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 84.13%   [EVAL] batch:   13 | acc: 75.00%,  total acc: 83.48%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 82.92%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 81.25%   [EVAL] batch:   16 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 80.21%   [EVAL] batch:   18 | acc: 81.25%,  total acc: 80.26%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 80.62%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 81.55%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 82.39%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 83.15%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 83.85%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 84.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 85.10%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 85.42%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 85.94%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 86.42%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 86.46%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 86.49%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 86.72%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 85.23%   
cur_acc:  ['0.8523']
his_acc:  ['0.8523']
CurrentTrain: epoch  0, batch     0 | loss: 6.0232434CurrentTrain: epoch  0, batch     1 | loss: 6.0119414CurrentTrain: epoch  1, batch     0 | loss: 5.7174597CurrentTrain: epoch  1, batch     1 | loss: 4.5438099CurrentTrain: epoch  2, batch     0 | loss: 5.1874356CurrentTrain: epoch  2, batch     1 | loss: 4.3806419CurrentTrain: epoch  3, batch     0 | loss: 4.6247053CurrentTrain: epoch  3, batch     1 | loss: 4.1160350CurrentTrain: epoch  4, batch     0 | loss: 3.9624701CurrentTrain: epoch  4, batch     1 | loss: 3.7985005CurrentTrain: epoch  5, batch     0 | loss: 3.9108443CurrentTrain: epoch  5, batch     1 | loss: 3.4699585CurrentTrain: epoch  6, batch     0 | loss: 3.5970144CurrentTrain: epoch  6, batch     1 | loss: 2.8425679CurrentTrain: epoch  7, batch     0 | loss: 3.4531069CurrentTrain: epoch  7, batch     1 | loss: 3.3905416CurrentTrain: epoch  8, batch     0 | loss: 3.1879215CurrentTrain: epoch  8, batch     1 | loss: 2.8723030CurrentTrain: epoch  9, batch     0 | loss: 2.8802395CurrentTrain: epoch  9, batch     1 | loss: 3.0838082
#### person cities of residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person cities of residence" means a person is living in a city.
Here is an example:
Relation: person cities of residence
Context: in 1974 , divorced and the mother of a 5-year-old , forsberg moved to boston and began studying arms control at mit ; she received her ph.d. in 1980 .
Head Entity: forsberg
Tail Entity: boston
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person cities of residence":
Relation: person cities of residence  
Context: After spending several years in New York, the artist decided to relocate to San Francisco to explore new opportunities in the tech industry.  
Head Entity: the artist  
Tail Entity: San Francisco  

Relation: person cities of residence  
Context: Following his graduation, John moved to Seattle to work for a leading software company, where he quickly established himself in the tech community.  
Head Entity: John  
Tail Entity: Seattle  
#### person schools attended ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person schools attended" means a person 's school is an organization.
Here is an example:
Relation: person schools attended
Context: after world war ii , he attended the university of southern california , where he became editor of a college magazine .
Head Entity: he
Tail Entity: university of southern california
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person schools attended":
Relation: person schools attended  
Context: She graduated from Harvard University with a degree in economics before pursuing her career in finance.  
Head Entity: She  
Tail Entity: Harvard University  

Relation: person schools attended  
Context: After completing his high school education, John enrolled at Stanford University to study computer science.  
Head Entity: John  
Tail Entity: Stanford University  
#### person country of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person country of death" means a person was died in a country.
Here is an example:
Relation: person country of death
Context: us republican congresswoman jo ann davis dies after fight with breast cancer
Head Entity: jo ann davis
Tail Entity: us
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person country of death":
Relation: person country of death  
Context: renowned physicist stephen hawking passed away in cambridge, england  
Head Entity: stephen hawking  
Tail Entity: england  

Relation: person country of death  
Context: famous author gabriel garcia marquez died in mexico city, mexico  
Head Entity: gabriel garcia marquez  
Tail Entity: mexico  
#### person children ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person children" means a person 's children is a person.
Here is an example:
Relation: person children
Context: he is survived by two stepdaughters , barbara a. hammond and brenda l. stevenson ; a stepson , michael a. taylor ; two grandchildren and one great-grandchild .
Head Entity: he
Tail Entity: brenda l. stevenson
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person children":
Relation: person children  
Context: she has three children, including her son, john, and her daughters, emily and sarah, who are both pursuing their studies.  
Head Entity: she  
Tail Entity: emily  

Relation: person children  
Context: after the passing of their father, the siblings, including jake and his sister, lila, took on the responsibility of caring for their younger brother, max.  
Head Entity: jake  
Tail Entity: lila  
#### person charges ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person charges" means a person was charged with an event.
Here is an example:
Relation: person charges
Context: previously , al-khawinay was sentenced to one year in jail for supporting the country 's minority shiite rebels and defaming the president , but was later pardoned by president ali abdullah saleh .
Head Entity: al-khawinay
Tail Entity: defaming the president
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person charges":
Relation: person charges  
Context: In a shocking turn of events, the local council announced that mayor Thompson was charged with embezzlement after an extensive investigation revealed misuse of public funds.  
Head Entity: mayor Thompson  
Tail Entity: embezzlement  

Relation: person charges  
Context: After a lengthy trial, it was determined that journalist Sarah Lee was charged with libel for publishing false information about a prominent businessman.  
Head Entity: journalist Sarah Lee  
Tail Entity: libel  
Mixup data size:  103
MixupTrain:  epoch  0, batch     0 | loss: 6.9979969MixupTrain:  epoch  0, batch     1 | loss: 6.3370428MixupTrain:  epoch  0, batch     2 | loss: 6.1715373MixupTrain:  epoch  0, batch     3 | loss: 6.6243450MixupTrain:  epoch  0, batch     4 | loss: 5.7116456MixupTrain:  epoch  0, batch     5 | loss: 5.2577465MixupTrain:  epoch  0, batch     6 | loss: 4.5111154
MemoryTrain:  epoch  0, batch     0 | loss: 4.0575390MemoryTrain:  epoch  0, batch     1 | loss: 4.1816578MemoryTrain:  epoch  0, batch     2 | loss: 3.8889425MemoryTrain:  epoch  1, batch     0 | loss: 3.8450859MemoryTrain:  epoch  1, batch     1 | loss: 3.7013781MemoryTrain:  epoch  1, batch     2 | loss: 2.6481085MemoryTrain:  epoch  2, batch     0 | loss: 3.2778788MemoryTrain:  epoch  2, batch     1 | loss: 3.5587339MemoryTrain:  epoch  2, batch     2 | loss: 2.0359957MemoryTrain:  epoch  3, batch     0 | loss: 3.5411031MemoryTrain:  epoch  3, batch     1 | loss: 2.8524463MemoryTrain:  epoch  3, batch     2 | loss: 3.8450477MemoryTrain:  epoch  4, batch     0 | loss: 2.2232099MemoryTrain:  epoch  4, batch     1 | loss: 3.1939402MemoryTrain:  epoch  4, batch     2 | loss: 1.4875021MemoryTrain:  epoch  5, batch     0 | loss: 2.8116412MemoryTrain:  epoch  5, batch     1 | loss: 2.4646454MemoryTrain:  epoch  5, batch     2 | loss: 4.5032482MemoryTrain:  epoch  6, batch     0 | loss: 2.9075794MemoryTrain:  epoch  6, batch     1 | loss: 2.7916651MemoryTrain:  epoch  6, batch     2 | loss: 1.1878167MemoryTrain:  epoch  7, batch     0 | loss: 2.7153416MemoryTrain:  epoch  7, batch     1 | loss: 2.2411454MemoryTrain:  epoch  7, batch     2 | loss: 1.2784806MemoryTrain:  epoch  8, batch     0 | loss: 2.2808361MemoryTrain:  epoch  8, batch     1 | loss: 2.4525774MemoryTrain:  epoch  8, batch     2 | loss: 1.1708018MemoryTrain:  epoch  9, batch     0 | loss: 2.6083956MemoryTrain:  epoch  9, batch     1 | loss: 2.0151792MemoryTrain:  epoch  9, batch     2 | loss: 1.3582357
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 75.00%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 68.75%,  total acc: 75.00%   [EVAL] batch:    3 | acc: 62.50%,  total acc: 71.88%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 72.50%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 73.96%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 77.68%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 78.91%   [EVAL] batch:    8 | acc: 75.00%,  total acc: 78.47%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 80.62%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 82.39%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 83.33%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 84.13%   [EVAL] batch:   13 | acc: 100.00%,  total acc: 85.27%   [EVAL] batch:   14 | acc: 100.00%,  total acc: 86.25%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 87.11%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 87.87%   [EVAL] batch:   17 | acc: 25.00%,  total acc: 84.38%   
[EVAL] batch:    0 | acc: 31.25%,  total acc: 31.25%   [EVAL] batch:    1 | acc: 37.50%,  total acc: 34.38%   [EVAL] batch:    2 | acc: 25.00%,  total acc: 31.25%   [EVAL] batch:    3 | acc: 25.00%,  total acc: 29.69%   [EVAL] batch:    4 | acc: 37.50%,  total acc: 31.25%   [EVAL] batch:    5 | acc: 31.25%,  total acc: 31.25%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 38.39%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 46.09%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 52.08%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 56.88%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 60.80%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 63.02%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 65.38%   [EVAL] batch:   13 | acc: 75.00%,  total acc: 66.07%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 66.67%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 66.02%   [EVAL] batch:   16 | acc: 81.25%,  total acc: 66.91%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 66.67%   [EVAL] batch:   18 | acc: 81.25%,  total acc: 67.43%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 68.44%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 69.94%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 71.31%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 72.55%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 73.70%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 74.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 75.72%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 76.39%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 77.23%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 78.02%   [EVAL] batch:   29 | acc: 93.75%,  total acc: 78.54%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 79.03%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 79.69%   [EVAL] batch:   32 | acc: 81.25%,  total acc: 79.73%   [EVAL] batch:   33 | acc: 81.25%,  total acc: 79.78%   [EVAL] batch:   34 | acc: 62.50%,  total acc: 79.29%   [EVAL] batch:   35 | acc: 75.00%,  total acc: 79.17%   [EVAL] batch:   36 | acc: 68.75%,  total acc: 78.89%   [EVAL] batch:   37 | acc: 81.25%,  total acc: 78.95%   [EVAL] batch:   38 | acc: 93.75%,  total acc: 79.33%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 79.84%   [EVAL] batch:   40 | acc: 68.75%,  total acc: 79.57%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 79.91%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 80.38%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 80.68%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 81.11%   [EVAL] batch:   45 | acc: 93.75%,  total acc: 81.39%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 81.78%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 82.16%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 82.53%   [EVAL] batch:   49 | acc: 62.50%,  total acc: 82.12%   
cur_acc:  ['0.8523', '0.8438']
his_acc:  ['0.8523', '0.8213']
CurrentTrain: epoch  0, batch     0 | loss: 6.5828457CurrentTrain: epoch  0, batch     1 | loss: 6.7328906CurrentTrain: epoch  1, batch     0 | loss: 5.7061815CurrentTrain: epoch  1, batch     1 | loss: 5.7623477CurrentTrain: epoch  2, batch     0 | loss: 5.8604650CurrentTrain: epoch  2, batch     1 | loss: 3.9655058CurrentTrain: epoch  3, batch     0 | loss: 4.9732466CurrentTrain: epoch  3, batch     1 | loss: 4.5563984CurrentTrain: epoch  4, batch     0 | loss: 5.0361891CurrentTrain: epoch  4, batch     1 | loss: 3.1835525CurrentTrain: epoch  5, batch     0 | loss: 3.5458026CurrentTrain: epoch  5, batch     1 | loss: 4.8645244CurrentTrain: epoch  6, batch     0 | loss: 3.6704335CurrentTrain: epoch  6, batch     1 | loss: 3.9526584CurrentTrain: epoch  7, batch     0 | loss: 3.2460966CurrentTrain: epoch  7, batch     1 | loss: 3.9627566CurrentTrain: epoch  8, batch     0 | loss: 3.1874309CurrentTrain: epoch  8, batch     1 | loss: 4.2501245CurrentTrain: epoch  9, batch     0 | loss: 3.6233029CurrentTrain: epoch  9, batch     1 | loss: 2.8333776
#### person country of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person country of birth" means a person was born in a country.
Here is an example:
Relation: person country of birth
Context: de maiziere noted that germany took in another former inmate from guantanamo in 2006 -- murat kurnaz , a turkish national who was born and grew up in germany .
Head Entity: murat kurnaz
Tail Entity: germany
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person country of birth":
Relation: person country of birth  
Context: born in 1985 in the bustling city of new delhi, arjun kapoor has always been proud of his indian heritage and culture.  
Head Entity: arjun kapoor  
Tail Entity: india  

Relation: person country of birth  
Context: during the interview, she revealed that she was born in the picturesque town of auckland, new zealand, which she considers her true home.  
Head Entity: she  
Tail Entity: new zealand  
#### organization website ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization website" means an organization 's website is an url.
Here is an example:
Relation: organization website
Context: http://www.pentaxsailaway.com/ paul j. sail away with pentax sweepstakes
Head Entity: pentax
Tail Entity: http://www.pentaxsailaway.com/
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization website":
Relation: organization website  
Context: Visit the official site at https://www.nike.com for the latest sports gear and apparel.  
Head Entity: Nike  
Tail Entity: https://www.nike.com  

Relation: organization website  
Context: For more information about their services, check out http://www.tesla.com.  
Head Entity: Tesla  
Tail Entity: http://www.tesla.com  
#### organization shareholders ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization shareholders" means an organization was invested by a person.
Here is an example:
Relation: organization shareholders
Context: marsans already owns the madrid-based air comet and 95 percent of aerolineas argentinas .
Head Entity: aerolineas argentinas
Tail Entity: marsans
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization shareholders":
Relation: organization shareholders  
Context: tech giant apple inc. has recently acquired a significant stake in the innovative startup nextdoor.  
Head Entity: nextdoor  
Tail Entity: apple inc.  

Relation: organization shareholders  
Context: the investment firm blackrock has increased its holdings in the renewable energy company solarcity.  
Head Entity: solarcity  
Tail Entity: blackrock  
#### organization dissolved ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization dissolved" means an organization was dissolved in a date.
Here is an example:
Relation: organization dissolved
Context: the tse suffered its worst-ever system crash in november 2005 which paralyzed the world 's second largest bourse and forced it to shelve plan for a listing of its own .
Head Entity: tse
Tail Entity: november 2005
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization dissolved":
Relation: organization dissolved  
Context: The once-prominent tech startup, Innovatech, officially ceased operations in March 2020 after struggling to secure funding.  
Head Entity: Innovatech  
Tail Entity: March 2020  

Relation: organization dissolved  
Context: After years of financial difficulties, the local arts council announced its dissolution in January 2019, leaving many community projects in limbo.  
Head Entity: local arts council  
Tail Entity: January 2019  
#### organization founded by ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization founded by" means an organization was found by a person.
Here is an example:
Relation: organization founded by
Context: born in new york , she inherited most of her fortune from her father , ted arison , who founded carnival cruise lines and also owned the miami heat basketball team .
Head Entity: carnival cruise lines
Tail Entity: ted arison
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization founded by":
Relation: organization founded by  
Context: in 1975, steve jobs and steve wozniak started apple inc., which has since become one of the most valuable companies in the world.  
Head Entity: apple inc.  
Tail Entity: steve jobs  

Relation: organization founded by  
Context: the famous fashion brand gucci was established in florence by guccio gucci, who initially started as a leather goods maker.  
Head Entity: gucci  
Tail Entity: guccio gucci  
Mixup data size:  135
MixupTrain:  epoch  0, batch     0 | loss: 4.4133831MixupTrain:  epoch  0, batch     1 | loss: 4.6135109MixupTrain:  epoch  0, batch     2 | loss: 5.7947470MixupTrain:  epoch  0, batch     3 | loss: 3.7864295MixupTrain:  epoch  0, batch     4 | loss: 4.3112223MixupTrain:  epoch  0, batch     5 | loss: 4.0865119MixupTrain:  epoch  0, batch     6 | loss: 4.1386841MixupTrain:  epoch  0, batch     7 | loss: 3.9872387MixupTrain:  epoch  0, batch     8 | loss: 3.6675767
MemoryTrain:  epoch  0, batch     0 | loss: 3.2388315MemoryTrain:  epoch  0, batch     1 | loss: 3.8357508MemoryTrain:  epoch  0, batch     2 | loss: 4.0228310MemoryTrain:  epoch  1, batch     0 | loss: 3.4829359MemoryTrain:  epoch  1, batch     1 | loss: 3.7739165MemoryTrain:  epoch  1, batch     2 | loss: 3.6679144MemoryTrain:  epoch  2, batch     0 | loss: 3.2578897MemoryTrain:  epoch  2, batch     1 | loss: 3.4888992MemoryTrain:  epoch  2, batch     2 | loss: 2.8268688MemoryTrain:  epoch  3, batch     0 | loss: 3.0116682MemoryTrain:  epoch  3, batch     1 | loss: 2.5843184MemoryTrain:  epoch  3, batch     2 | loss: 3.4223931MemoryTrain:  epoch  4, batch     0 | loss: 3.1039400MemoryTrain:  epoch  4, batch     1 | loss: 2.1585717MemoryTrain:  epoch  4, batch     2 | loss: 3.2591500MemoryTrain:  epoch  5, batch     0 | loss: 2.3017797MemoryTrain:  epoch  5, batch     1 | loss: 2.7843404MemoryTrain:  epoch  5, batch     2 | loss: 2.5410180MemoryTrain:  epoch  6, batch     0 | loss: 3.1524251MemoryTrain:  epoch  6, batch     1 | loss: 2.3801007MemoryTrain:  epoch  6, batch     2 | loss: 2.2567062MemoryTrain:  epoch  7, batch     0 | loss: 1.9579009MemoryTrain:  epoch  7, batch     1 | loss: 2.7107170MemoryTrain:  epoch  7, batch     2 | loss: 2.2120292MemoryTrain:  epoch  8, batch     0 | loss: 2.3774576MemoryTrain:  epoch  8, batch     1 | loss: 2.0694909MemoryTrain:  epoch  8, batch     2 | loss: 1.8070934MemoryTrain:  epoch  9, batch     0 | loss: 1.9680713MemoryTrain:  epoch  9, batch     1 | loss: 1.7298565MemoryTrain:  epoch  9, batch     2 | loss: 1.9695468
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:    2 | acc: 62.50%,  total acc: 81.25%   [EVAL] batch:    3 | acc: 12.50%,  total acc: 64.06%   [EVAL] batch:    4 | acc: 12.50%,  total acc: 53.75%   [EVAL] batch:    5 | acc: 18.75%,  total acc: 47.92%   [EVAL] batch:    6 | acc: 6.25%,  total acc: 41.96%   [EVAL] batch:    7 | acc: 0.00%,  total acc: 36.72%   
[EVAL] batch:    0 | acc: 31.25%,  total acc: 31.25%   [EVAL] batch:    1 | acc: 50.00%,  total acc: 40.62%   [EVAL] batch:    2 | acc: 31.25%,  total acc: 37.50%   [EVAL] batch:    3 | acc: 25.00%,  total acc: 34.38%   [EVAL] batch:    4 | acc: 37.50%,  total acc: 35.00%   [EVAL] batch:    5 | acc: 37.50%,  total acc: 35.42%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 40.18%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 46.88%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 52.08%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 55.62%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 59.09%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 61.46%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 62.02%   [EVAL] batch:   13 | acc: 50.00%,  total acc: 61.16%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 62.08%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 61.72%   [EVAL] batch:   16 | acc: 81.25%,  total acc: 62.87%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 62.85%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 63.16%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 64.06%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 65.77%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 67.33%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 68.75%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 70.05%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 71.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 72.36%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 73.15%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 74.11%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:   29 | acc: 93.75%,  total acc: 75.62%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 76.21%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 76.76%   [EVAL] batch:   32 | acc: 68.75%,  total acc: 76.52%   [EVAL] batch:   33 | acc: 56.25%,  total acc: 75.92%   [EVAL] batch:   34 | acc: 62.50%,  total acc: 75.54%   [EVAL] batch:   35 | acc: 62.50%,  total acc: 75.17%   [EVAL] batch:   36 | acc: 56.25%,  total acc: 74.66%   [EVAL] batch:   37 | acc: 68.75%,  total acc: 74.51%   [EVAL] batch:   38 | acc: 75.00%,  total acc: 74.52%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 75.16%   [EVAL] batch:   40 | acc: 50.00%,  total acc: 74.54%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 74.85%   [EVAL] batch:   42 | acc: 87.50%,  total acc: 75.15%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 75.71%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 76.25%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 76.77%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 77.26%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 77.73%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 78.19%   [EVAL] batch:   49 | acc: 81.25%,  total acc: 78.25%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 78.68%   [EVAL] batch:   51 | acc: 81.25%,  total acc: 78.73%   [EVAL] batch:   52 | acc: 50.00%,  total acc: 78.18%   [EVAL] batch:   53 | acc: 12.50%,  total acc: 76.97%   [EVAL] batch:   54 | acc: 12.50%,  total acc: 75.80%   [EVAL] batch:   55 | acc: 18.75%,  total acc: 74.78%   [EVAL] batch:   56 | acc: 0.00%,  total acc: 73.46%   
cur_acc:  ['0.8523', '0.8438', '0.3672']
his_acc:  ['0.8523', '0.8213', '0.7346']
CurrentTrain: epoch  0, batch     0 | loss: 4.9382868CurrentTrain: epoch  0, batch     1 | loss: 5.7704153CurrentTrain: epoch  1, batch     0 | loss: 4.2623711CurrentTrain: epoch  1, batch     1 | loss: 4.4259973CurrentTrain: epoch  2, batch     0 | loss: 4.0619025CurrentTrain: epoch  2, batch     1 | loss: 3.3569810CurrentTrain: epoch  3, batch     0 | loss: 3.4569750CurrentTrain: epoch  3, batch     1 | loss: 3.3712063CurrentTrain: epoch  4, batch     0 | loss: 3.2428937CurrentTrain: epoch  4, batch     1 | loss: 2.8293717CurrentTrain: epoch  5, batch     0 | loss: 2.7934089CurrentTrain: epoch  5, batch     1 | loss: 3.2415688CurrentTrain: epoch  6, batch     0 | loss: 2.7715120CurrentTrain: epoch  6, batch     1 | loss: 2.5530639CurrentTrain: epoch  7, batch     0 | loss: 2.6896181CurrentTrain: epoch  7, batch     1 | loss: 2.2399342CurrentTrain: epoch  8, batch     0 | loss: 2.2677612CurrentTrain: epoch  8, batch     1 | loss: 2.3300068CurrentTrain: epoch  9, batch     0 | loss: 2.2345016CurrentTrain: epoch  9, batch     1 | loss: 2.1557271
#### organization founded ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization founded" means an organization was found in a date.
Here is an example:
Relation: organization founded
Context: pandit worked at the brokerage morgan stanley for about 11 years until 2005 , when he and some morgan stanley colleagues quit and later founded the hedge fund old lane partners .
Head Entity: old lane partners
Tail Entity: 2005
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization founded":
Relation: organization founded  
Context: In 1998, a group of engineers and entrepreneurs came together to establish the tech startup, Innovatech Solutions, which has since become a leader in software development.  
Head Entity: Innovatech Solutions  
Tail Entity: 1998  

Relation: organization founded  
Context: The non-profit organization Green Earth Initiative was created in 2010 to promote environmental awareness and sustainability practices across communities.  
Head Entity: Green Earth Initiative  
Tail Entity: 2010  
#### person age ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person age" means a person 's age is a number.
Here is an example:
Relation: person age
Context: virginia republican jo ann davis passed away on saturday at the age of 57 .
Head Entity: jo ann davis
Tail Entity: 57
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person age":
Relation: person age  
Context: the famous actor robert downey jr. celebrated his 56th birthday last week.  
Head Entity: robert downey jr.  
Tail Entity: 56  

Relation: person age  
Context: on her 30th birthday, singer taylor swift released a new album to mark the occasion.  
Head Entity: taylor swift  
Tail Entity: 30  
#### person city of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person city of birth" means a person was born in a city.
Here is an example:
Relation: person city of birth
Context: rothman was born in san francisco in 1932 in an orthodox jewish family .
Head Entity: rothman
Tail Entity: san francisco
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person city of birth":
Relation: person city of birth  
Context: after spending his early years in new york, he moved to los angeles where he began his career.  
Head Entity: he  
Tail Entity: los angeles  

Relation: person city of birth  
Context: the famous author was born in a small town near boston, which greatly influenced his writing.  
Head Entity: the famous author  
Tail Entity: boston  
#### organization members ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization members" means an organization 's member is an organization.
Here is an example:
Relation: organization members
Context: it was berger who made clarke a member of the white house principals committee when it met to discuss terrorist threats , allowing an otherwise middle-ranking nsc bureaucrat to treat tenet and secretary of state madeleine albright as equals -lrb- which the empire-building clarke was pleased to do -rrb- .
Head Entity: nsc
Tail Entity: white house principals committee
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization members":
Relation: organization members  
Context: The board of directors of the tech startup includes several prominent figures from the industry, such as the CEO of Innovatech, who has been instrumental in guiding the company’s strategic direction.  
Head Entity: tech startup  
Tail Entity: board of directors  

Relation: organization members  
Context: During the annual conference, the president of the environmental advocacy group announced the addition of several new members, including renowned scientists and activists dedicated to climate change.  
Head Entity: environmental advocacy group  
Tail Entity: new members  
#### person religion ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person religion" means an person is the member of a religion.
Here is an example:
Relation: person religion
Context: the pope defended his action on the grounds that he could not refuse an audience to a head of state from a country with a strong catholic tradition unless he had clear-cut proof of the allegations against him .
Head Entity: he
Tail Entity: catholic
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person religion":
Relation: person religion  
Context: During the ceremony, the rabbi spoke about the importance of faith and community in Judaism, emphasizing how each member contributes to the collective spirit.  
Head Entity: rabbi  
Tail Entity: Judaism  

Relation: person religion  
Context: The famous author often discussed her deep connection to Buddhism and how it influenced her writing and personal philosophy.  
Head Entity: author  
Tail Entity: Buddhism  
Mixup data size:  165
MixupTrain:  epoch  0, batch     0 | loss: 3.5056438MixupTrain:  epoch  0, batch     1 | loss: 3.8783437MixupTrain:  epoch  0, batch     2 | loss: 3.6825835MixupTrain:  epoch  0, batch     3 | loss: 3.3605903MixupTrain:  epoch  0, batch     4 | loss: 3.1461013MixupTrain:  epoch  0, batch     5 | loss: 3.2825070MixupTrain:  epoch  0, batch     6 | loss: 3.0424910MixupTrain:  epoch  0, batch     7 | loss: 3.5391523MixupTrain:  epoch  0, batch     8 | loss: 2.6849027MixupTrain:  epoch  0, batch     9 | loss: 3.2780353MixupTrain:  epoch  0, batch    10 | loss: 3.3117204
MemoryTrain:  epoch  0, batch     0 | loss: 3.1275725MemoryTrain:  epoch  0, batch     1 | loss: 3.9306042MemoryTrain:  epoch  0, batch     2 | loss: 3.6268909MemoryTrain:  epoch  0, batch     3 | loss: 3.4270346MemoryTrain:  epoch  1, batch     0 | loss: 2.9881568MemoryTrain:  epoch  1, batch     1 | loss: 2.9964201MemoryTrain:  epoch  1, batch     2 | loss: 2.8970571MemoryTrain:  epoch  1, batch     3 | loss: 3.1370566MemoryTrain:  epoch  2, batch     0 | loss: 2.4068804MemoryTrain:  epoch  2, batch     1 | loss: 3.0624204MemoryTrain:  epoch  2, batch     2 | loss: 2.1139734MemoryTrain:  epoch  2, batch     3 | loss: 2.6885722MemoryTrain:  epoch  3, batch     0 | loss: 2.8875158MemoryTrain:  epoch  3, batch     1 | loss: 2.0391488MemoryTrain:  epoch  3, batch     2 | loss: 1.8754115MemoryTrain:  epoch  3, batch     3 | loss: 2.3786917MemoryTrain:  epoch  4, batch     0 | loss: 2.5087175MemoryTrain:  epoch  4, batch     1 | loss: 1.6538689MemoryTrain:  epoch  4, batch     2 | loss: 1.9833862MemoryTrain:  epoch  4, batch     3 | loss: 2.1750679MemoryTrain:  epoch  5, batch     0 | loss: 2.1801891MemoryTrain:  epoch  5, batch     1 | loss: 1.7950821MemoryTrain:  epoch  5, batch     2 | loss: 2.2944050MemoryTrain:  epoch  5, batch     3 | loss: 1.9480350MemoryTrain:  epoch  6, batch     0 | loss: 2.1152804MemoryTrain:  epoch  6, batch     1 | loss: 1.8691721MemoryTrain:  epoch  6, batch     2 | loss: 1.7466062MemoryTrain:  epoch  6, batch     3 | loss: 1.6385789MemoryTrain:  epoch  7, batch     0 | loss: 1.7357285MemoryTrain:  epoch  7, batch     1 | loss: 1.5531880MemoryTrain:  epoch  7, batch     2 | loss: 1.8300023MemoryTrain:  epoch  7, batch     3 | loss: 1.8542584MemoryTrain:  epoch  8, batch     0 | loss: 1.6760768MemoryTrain:  epoch  8, batch     1 | loss: 1.3661636MemoryTrain:  epoch  8, batch     2 | loss: 1.6620078MemoryTrain:  epoch  8, batch     3 | loss: 1.5844789MemoryTrain:  epoch  9, batch     0 | loss: 1.5441642MemoryTrain:  epoch  9, batch     1 | loss: 1.5963638MemoryTrain:  epoch  9, batch     2 | loss: 1.5273143MemoryTrain:  epoch  9, batch     3 | loss: 1.6313087
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    3 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:    4 | acc: 100.00%,  total acc: 92.50%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 94.64%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 95.31%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 92.36%   [EVAL] batch:    9 | acc: 12.50%,  total acc: 84.38%   [EVAL] batch:   10 | acc: 31.25%,  total acc: 79.55%   [EVAL] batch:   11 | acc: 68.75%,  total acc: 78.65%   [EVAL] batch:   12 | acc: 100.00%,  total acc: 80.29%   [EVAL] batch:   13 | acc: 62.50%,  total acc: 79.02%   
[EVAL] batch:    0 | acc: 12.50%,  total acc: 12.50%   [EVAL] batch:    1 | acc: 31.25%,  total acc: 21.88%   [EVAL] batch:    2 | acc: 18.75%,  total acc: 20.83%   [EVAL] batch:    3 | acc: 12.50%,  total acc: 18.75%   [EVAL] batch:    4 | acc: 18.75%,  total acc: 18.75%   [EVAL] batch:    5 | acc: 6.25%,  total acc: 16.67%   [EVAL] batch:    6 | acc: 62.50%,  total acc: 23.21%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 32.03%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 38.19%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 43.12%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 47.16%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 50.52%   [EVAL] batch:   12 | acc: 56.25%,  total acc: 50.96%   [EVAL] batch:   13 | acc: 62.50%,  total acc: 51.79%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 52.92%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 53.12%   [EVAL] batch:   16 | acc: 81.25%,  total acc: 54.78%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 55.21%   [EVAL] batch:   18 | acc: 81.25%,  total acc: 56.58%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 58.13%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 60.12%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 61.93%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 63.59%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 64.84%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 66.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 67.55%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 68.52%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 69.64%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 70.69%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 71.25%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 72.18%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 72.85%   [EVAL] batch:   32 | acc: 43.75%,  total acc: 71.97%   [EVAL] batch:   33 | acc: 12.50%,  total acc: 70.22%   [EVAL] batch:   34 | acc: 31.25%,  total acc: 69.11%   [EVAL] batch:   35 | acc: 18.75%,  total acc: 67.71%   [EVAL] batch:   36 | acc: 18.75%,  total acc: 66.39%   [EVAL] batch:   37 | acc: 31.25%,  total acc: 65.46%   [EVAL] batch:   38 | acc: 50.00%,  total acc: 65.06%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 65.94%   [EVAL] batch:   40 | acc: 50.00%,  total acc: 65.55%   [EVAL] batch:   41 | acc: 56.25%,  total acc: 65.33%   [EVAL] batch:   42 | acc: 75.00%,  total acc: 65.55%   [EVAL] batch:   43 | acc: 75.00%,  total acc: 65.77%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 66.53%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 67.26%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 67.95%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 68.62%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 69.26%   [EVAL] batch:   49 | acc: 81.25%,  total acc: 69.50%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 70.10%   [EVAL] batch:   51 | acc: 75.00%,  total acc: 70.19%   [EVAL] batch:   52 | acc: 50.00%,  total acc: 69.81%   [EVAL] batch:   53 | acc: 12.50%,  total acc: 68.75%   [EVAL] batch:   54 | acc: 25.00%,  total acc: 67.95%   [EVAL] batch:   55 | acc: 25.00%,  total acc: 67.19%   [EVAL] batch:   56 | acc: 31.25%,  total acc: 66.56%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 66.92%   [EVAL] batch:   58 | acc: 81.25%,  total acc: 67.16%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 67.71%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 68.24%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 68.75%   [EVAL] batch:   62 | acc: 100.00%,  total acc: 69.25%   [EVAL] batch:   63 | acc: 100.00%,  total acc: 69.73%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 70.19%   [EVAL] batch:   65 | acc: 43.75%,  total acc: 69.79%   [EVAL] batch:   66 | acc: 12.50%,  total acc: 68.94%   [EVAL] batch:   67 | acc: 50.00%,  total acc: 68.66%   [EVAL] batch:   68 | acc: 75.00%,  total acc: 68.75%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 69.20%   [EVAL] batch:   70 | acc: 37.50%,  total acc: 68.75%   
cur_acc:  ['0.8523', '0.8438', '0.3672', '0.7902']
his_acc:  ['0.8523', '0.8213', '0.7346', '0.6875']
CurrentTrain: epoch  0, batch     0 | loss: 6.0077667CurrentTrain: epoch  0, batch     1 | loss: 6.9644084CurrentTrain: epoch  1, batch     0 | loss: 5.3382816CurrentTrain: epoch  1, batch     1 | loss: 4.9647417CurrentTrain: epoch  2, batch     0 | loss: 5.2091537CurrentTrain: epoch  2, batch     1 | loss: 4.5408483CurrentTrain: epoch  3, batch     0 | loss: 4.8730659CurrentTrain: epoch  3, batch     1 | loss: 4.3717752CurrentTrain: epoch  4, batch     0 | loss: 4.1086035CurrentTrain: epoch  4, batch     1 | loss: 4.5035553CurrentTrain: epoch  5, batch     0 | loss: 3.8802037CurrentTrain: epoch  5, batch     1 | loss: 4.2828469CurrentTrain: epoch  6, batch     0 | loss: 3.9313245CurrentTrain: epoch  6, batch     1 | loss: 3.4025269CurrentTrain: epoch  7, batch     0 | loss: 4.0768743CurrentTrain: epoch  7, batch     1 | loss: 2.6518910CurrentTrain: epoch  8, batch     0 | loss: 3.1294098CurrentTrain: epoch  8, batch     1 | loss: 3.4618673CurrentTrain: epoch  9, batch     0 | loss: 3.1518731CurrentTrain: epoch  9, batch     1 | loss: 2.8941116
#### person stateorprovinces of residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person stateorprovinces of residence" means a person is living in a state or province.
Here is an example:
Relation: person stateorprovinces of residence
Context: the chairman of the senate foreign relations committee , massachusetts democrat john kerry , and the panel 's top republican , richard lugar of indiana , were at the white house meeting , which was led by vice president joe biden , a former chairman of the foreign relations panel .
Head Entity: john kerry
Tail Entity: massachusetts
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person stateorprovinces of residence":
Relation: person stateorprovinces of residence  
Context: After moving from New York to California, actress Emma Stone quickly adapted to the vibrant lifestyle of Los Angeles, where she now resides.  
Head Entity: Emma Stone  
Tail Entity: California  

Relation: person stateorprovinces of residence  
Context: Following his successful career in the tech industry, Mark Zuckerberg decided to settle in Palo Alto, a city known for its innovation and proximity to Silicon Valley.  
Head Entity: Mark Zuckerberg  
Tail Entity: Palo Alto  
#### person date of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person date of death" means a person was died on a date.
Here is an example:
Relation: person date of death
Context: today the nypd upgraded the charges to include murder , in the case of brooklyn gay-bashing/robbery victim michael sandy , who died on friday after being taken off life-support .
Head Entity: michael sandy
Tail Entity: friday
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person date of death":
Relation: person date of death  
Context: renowned physicist stephen hawking passed away peacefully at his home in cambridge on march 14, 2018, at the age of 76.  
Head Entity: stephen hawking  
Tail Entity: march 14, 2018  

Relation: person date of death  
Context: the beloved author of the harry potter series, j.k. rowling, announced the tragic news of her friend and mentor's death on january 1, 2020, after a long battle with illness.  
Head Entity: j.k. rowling  
Tail Entity: january 1, 2020  
#### organization number of employees members ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization number of employees members" means the number of an organization 's employees is a number.
Here is an example:
Relation: organization number of employees members
Context: covidien , which posted revenue of more than $ 10 billion last year , has about 42,000 employees worldwide .
Head Entity: covidien
Tail Entity: 42,000
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization number of employees members":
Relation: organization number of employees members  
Context: TechCorp, a leading software development company, boasts a workforce of approximately 5,500 skilled professionals across its global offices.  
Head Entity: TechCorp  
Tail Entity: 5,500  

Relation: organization number of employees members  
Context: GreenEarth Nonprofit has grown significantly over the past few years and currently employs around 1,200 dedicated staff members to support its environmental initiatives.  
Head Entity: GreenEarth Nonprofit  
Tail Entity: 1,200  
#### person alternate names ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person alternate names" means a person 's alias is a person.
Here is an example:
Relation: person alternate names
Context: the coming of the mahdi will turn the world upside down , and the oppressed shiites will finally see justice .
Head Entity: mahdi
Tail Entity: shiites
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person alternate names":
Relation: person alternate names  
Context: The famous author Mark Twain, known for his wit and humor, was actually named Samuel Clemens.  
Head Entity: Mark Twain  
Tail Entity: Samuel Clemens  

Relation: person alternate names  
Context: The musician known as Lady Gaga was born Stefani Joanne Angelina Germanotta.  
Head Entity: Lady Gaga  
Tail Entity: Stefani Joanne Angelina Germanotta  
#### person spouse ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person spouse" means a person 's spouse is a person.
Here is an example:
Relation: person spouse
Context: smits stands at the center of this multigenerational saga as alex vega , the adopted son of rum and sugar baron pancho duque -lrb- elizondo -rrb- and his wife , amalia -lrb- moreno -rrb- .
Head Entity: elizondo
Tail Entity: moreno
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person spouse":
Relation: person spouse  
Context: in a heartfelt ceremony, john and his beloved partner, sarah, exchanged vows surrounded by family and friends, celebrating their love and commitment to each other.  
Head Entity: john  
Tail Entity: sarah  

Relation: person spouse  
Context: after years of companionship, the couple, michael and jessica, finally tied the knot in a beautiful garden wedding, marking the beginning of their new life together.  
Head Entity: michael  
Tail Entity: jessica  
Mixup data size:  194
MixupTrain:  epoch  0, batch     0 | loss: 3.3698521MixupTrain:  epoch  0, batch     1 | loss: 3.4637435MixupTrain:  epoch  0, batch     2 | loss: 3.0511142MixupTrain:  epoch  0, batch     3 | loss: 3.6288802MixupTrain:  epoch  0, batch     4 | loss: 3.3091454MixupTrain:  epoch  0, batch     5 | loss: 2.9919366MixupTrain:  epoch  0, batch     6 | loss: 3.3917742MixupTrain:  epoch  0, batch     7 | loss: 2.8011216MixupTrain:  epoch  0, batch     8 | loss: 3.4701352MixupTrain:  epoch  0, batch     9 | loss: 2.9525562MixupTrain:  epoch  0, batch    10 | loss: 3.0576858MixupTrain:  epoch  0, batch    11 | loss: 2.6573444MixupTrain:  epoch  0, batch    12 | loss: 2.4188211
MemoryTrain:  epoch  0, batch     0 | loss: 2.2939844MemoryTrain:  epoch  0, batch     1 | loss: 2.4973330MemoryTrain:  epoch  0, batch     2 | loss: 3.4509773MemoryTrain:  epoch  0, batch     3 | loss: 2.8297324MemoryTrain:  epoch  0, batch     4 | loss: 2.6923289MemoryTrain:  epoch  1, batch     0 | loss: 2.0966871MemoryTrain:  epoch  1, batch     1 | loss: 1.9491534MemoryTrain:  epoch  1, batch     2 | loss: 2.9467063MemoryTrain:  epoch  1, batch     3 | loss: 2.2829335MemoryTrain:  epoch  1, batch     4 | loss: 1.9853563MemoryTrain:  epoch  2, batch     0 | loss: 1.6015267MemoryTrain:  epoch  2, batch     1 | loss: 2.0450492MemoryTrain:  epoch  2, batch     2 | loss: 1.9932673MemoryTrain:  epoch  2, batch     3 | loss: 1.5306679MemoryTrain:  epoch  2, batch     4 | loss: 2.3062887MemoryTrain:  epoch  3, batch     0 | loss: 1.4865249MemoryTrain:  epoch  3, batch     1 | loss: 2.1630678MemoryTrain:  epoch  3, batch     2 | loss: 2.0060108MemoryTrain:  epoch  3, batch     3 | loss: 2.0533850MemoryTrain:  epoch  3, batch     4 | loss: 1.8735343MemoryTrain:  epoch  4, batch     0 | loss: 1.7478354MemoryTrain:  epoch  4, batch     1 | loss: 1.6849172MemoryTrain:  epoch  4, batch     2 | loss: 1.7937856MemoryTrain:  epoch  4, batch     3 | loss: 1.6752205MemoryTrain:  epoch  4, batch     4 | loss: 1.9512621MemoryTrain:  epoch  5, batch     0 | loss: 1.5297087MemoryTrain:  epoch  5, batch     1 | loss: 1.5759794MemoryTrain:  epoch  5, batch     2 | loss: 1.8171718MemoryTrain:  epoch  5, batch     3 | loss: 1.6939552MemoryTrain:  epoch  5, batch     4 | loss: 1.5753659MemoryTrain:  epoch  6, batch     0 | loss: 1.5774443MemoryTrain:  epoch  6, batch     1 | loss: 1.5209739MemoryTrain:  epoch  6, batch     2 | loss: 1.6251452MemoryTrain:  epoch  6, batch     3 | loss: 1.5991158MemoryTrain:  epoch  6, batch     4 | loss: 2.1227996MemoryTrain:  epoch  7, batch     0 | loss: 1.5880045MemoryTrain:  epoch  7, batch     1 | loss: 1.6811810MemoryTrain:  epoch  7, batch     2 | loss: 1.4716653MemoryTrain:  epoch  7, batch     3 | loss: 1.6384417MemoryTrain:  epoch  7, batch     4 | loss: 1.4440366MemoryTrain:  epoch  8, batch     0 | loss: 1.6094652MemoryTrain:  epoch  8, batch     1 | loss: 1.5759037MemoryTrain:  epoch  8, batch     2 | loss: 1.5003417MemoryTrain:  epoch  8, batch     3 | loss: 1.5263114MemoryTrain:  epoch  8, batch     4 | loss: 1.4576294MemoryTrain:  epoch  9, batch     0 | loss: 1.4674413MemoryTrain:  epoch  9, batch     1 | loss: 1.5396383MemoryTrain:  epoch  9, batch     2 | loss: 1.5777164MemoryTrain:  epoch  9, batch     3 | loss: 1.4807699MemoryTrain:  epoch  9, batch     4 | loss: 1.3955413
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    1 | acc: 68.75%,  total acc: 65.62%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 72.92%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 76.56%   [EVAL] batch:    4 | acc: 62.50%,  total acc: 73.75%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 76.04%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 77.68%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 78.12%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 79.17%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 78.75%   [EVAL] batch:   10 | acc: 43.75%,  total acc: 75.57%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 75.52%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 76.44%   [EVAL] batch:   13 | acc: 37.50%,  total acc: 73.66%   [EVAL] batch:   14 | acc: 25.00%,  total acc: 70.42%   
[EVAL] batch:    0 | acc: 12.50%,  total acc: 12.50%   [EVAL] batch:    1 | acc: 18.75%,  total acc: 15.62%   [EVAL] batch:    2 | acc: 18.75%,  total acc: 16.67%   [EVAL] batch:    3 | acc: 18.75%,  total acc: 17.19%   [EVAL] batch:    4 | acc: 18.75%,  total acc: 17.50%   [EVAL] batch:    5 | acc: 6.25%,  total acc: 15.62%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 23.21%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 32.03%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 38.19%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 42.50%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 47.16%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 50.52%   [EVAL] batch:   12 | acc: 62.50%,  total acc: 51.44%   [EVAL] batch:   13 | acc: 43.75%,  total acc: 50.89%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 52.50%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 52.73%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 54.04%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 54.51%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 55.26%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 56.56%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 58.63%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 60.51%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 62.23%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 63.54%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 65.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 66.35%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 67.36%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 68.53%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 69.61%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 70.21%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 71.17%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 71.88%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 70.83%   [EVAL] batch:   33 | acc: 0.00%,  total acc: 68.75%   [EVAL] batch:   34 | acc: 6.25%,  total acc: 66.96%   [EVAL] batch:   35 | acc: 12.50%,  total acc: 65.45%   [EVAL] batch:   36 | acc: 12.50%,  total acc: 64.02%   [EVAL] batch:   37 | acc: 12.50%,  total acc: 62.66%   [EVAL] batch:   38 | acc: 37.50%,  total acc: 62.02%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 62.97%   [EVAL] batch:   40 | acc: 37.50%,  total acc: 62.35%   [EVAL] batch:   41 | acc: 68.75%,  total acc: 62.50%   [EVAL] batch:   42 | acc: 68.75%,  total acc: 62.65%   [EVAL] batch:   43 | acc: 75.00%,  total acc: 62.93%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 63.75%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 64.54%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 65.29%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 66.02%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 66.71%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 67.12%   [EVAL] batch:   50 | acc: 87.50%,  total acc: 67.52%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 67.91%   [EVAL] batch:   52 | acc: 56.25%,  total acc: 67.69%   [EVAL] batch:   53 | acc: 12.50%,  total acc: 66.67%   [EVAL] batch:   54 | acc: 18.75%,  total acc: 65.80%   [EVAL] batch:   55 | acc: 18.75%,  total acc: 64.96%   [EVAL] batch:   56 | acc: 31.25%,  total acc: 64.36%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 64.87%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 65.25%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 65.83%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 66.39%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 66.94%   [EVAL] batch:   62 | acc: 100.00%,  total acc: 67.46%   [EVAL] batch:   63 | acc: 100.00%,  total acc: 67.97%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 68.46%   [EVAL] batch:   65 | acc: 31.25%,  total acc: 67.90%   [EVAL] batch:   66 | acc: 12.50%,  total acc: 67.07%   [EVAL] batch:   67 | acc: 43.75%,  total acc: 66.73%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 67.21%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 67.68%   [EVAL] batch:   70 | acc: 81.25%,  total acc: 67.87%   [EVAL] batch:   71 | acc: 50.00%,  total acc: 67.62%   [EVAL] batch:   72 | acc: 93.75%,  total acc: 67.98%   [EVAL] batch:   73 | acc: 81.25%,  total acc: 68.16%   [EVAL] batch:   74 | acc: 75.00%,  total acc: 68.25%   [EVAL] batch:   75 | acc: 81.25%,  total acc: 68.42%   [EVAL] batch:   76 | acc: 75.00%,  total acc: 68.51%   [EVAL] batch:   77 | acc: 93.75%,  total acc: 68.83%   [EVAL] batch:   78 | acc: 81.25%,  total acc: 68.99%   [EVAL] batch:   79 | acc: 93.75%,  total acc: 69.30%   [EVAL] batch:   80 | acc: 43.75%,  total acc: 68.98%   [EVAL] batch:   81 | acc: 56.25%,  total acc: 68.83%   [EVAL] batch:   82 | acc: 87.50%,  total acc: 69.05%   [EVAL] batch:   83 | acc: 50.00%,  total acc: 68.82%   [EVAL] batch:   84 | acc: 56.25%,  total acc: 68.68%   
cur_acc:  ['0.8523', '0.8438', '0.3672', '0.7902', '0.7042']
his_acc:  ['0.8523', '0.8213', '0.7346', '0.6875', '0.6868']
CurrentTrain: epoch  0, batch     0 | loss: 6.8754444CurrentTrain: epoch  0, batch     1 | loss: 6.9511933CurrentTrain: epoch  1, batch     0 | loss: 5.7456737CurrentTrain: epoch  1, batch     1 | loss: 4.7000751CurrentTrain: epoch  2, batch     0 | loss: 4.5995398CurrentTrain: epoch  2, batch     1 | loss: 4.8512311CurrentTrain: epoch  3, batch     0 | loss: 4.4896603CurrentTrain: epoch  3, batch     1 | loss: 4.7745795CurrentTrain: epoch  4, batch     0 | loss: 4.2880836CurrentTrain: epoch  4, batch     1 | loss: 4.3105402CurrentTrain: epoch  5, batch     0 | loss: 3.7504013CurrentTrain: epoch  5, batch     1 | loss: 3.8702486CurrentTrain: epoch  6, batch     0 | loss: 3.2089009CurrentTrain: epoch  6, batch     1 | loss: 3.2802687CurrentTrain: epoch  7, batch     0 | loss: 3.2049780CurrentTrain: epoch  7, batch     1 | loss: 2.9366841CurrentTrain: epoch  8, batch     0 | loss: 3.0712562CurrentTrain: epoch  8, batch     1 | loss: 2.7302105CurrentTrain: epoch  9, batch     0 | loss: 2.6707020CurrentTrain: epoch  9, batch     1 | loss: 2.4120307
#### person date of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person date of birth" means a person was born in a date.
Here is an example:
Relation: person date of birth
Context: kirkaldy , born irene morgan in baltimore , maryland , in 1917 , was arrested in 1944 for refusing to give up her seat on a greyhound bus heading from gloucester to baltimore , and for resisting arrest .
Head Entity: irene morgan
Tail Entity: 1917
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person date of birth":
Relation: person date of birth  
Context: the famous physicist albert einstein was born in ulm, in the kingdom of wurttemberg in the german empire, on march 14, 1879.  
Head Entity: albert einstein  
Tail Entity: march 14, 1879  

Relation: person date of birth  
Context: the renowned author jane austen was born on december 16, 1775, in steventon, hampshire, england.  
Head Entity: jane austen  
Tail Entity: december 16, 1775  
#### person stateorprovince of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person stateorprovince of birth" means a person was died in a state or province.
Here is an example:
Relation: person stateorprovince of birth
Context: jane matilda bolin was born on april 11 , 1908 , in poughkeepsie , ny .
Head Entity: jane matilda bolin
Tail Entity: ny
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person stateorprovince of birth":
Relation: person stateorprovince of birth  
Context: albert einstein was born on march 14, 1879, in ulm, germany.  
Head Entity: albert einstein  
Tail Entity: germany  

Relation: person stateorprovince of birth  
Context: marilyn monroe was born on june 1, 1926, in los angeles, california.  
Head Entity: marilyn monroe  
Tail Entity: california  
#### person parents ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person parents" means a person 's parent is a person.
Here is an example:
Relation: person parents
Context: as the case developed , sandy 's mother , denise sandy , quietly made herself a spectral but central figure , by faithfully attending pretrial hearings .
Head Entity: sandy
Tail Entity: denise sandy
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person parents":
Relation: person parents  
Context: After the ceremony, Emily's father, John Smith, shared heartfelt stories about her childhood, bringing tears to many eyes.  
Head Entity: Emily  
Tail Entity: John Smith  

Relation: person parents  
Context: During the family reunion, Michael's mother, Sarah Johnson, prepared her famous lasagna, which everyone eagerly devoured.  
Head Entity: Michael  
Tail Entity: Sarah Johnson  
#### person employee of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person employee of" means a person is an employee of an organization.
Here is an example:
Relation: person employee of
Context: seeking revenge , axel reunites with old pal sgt. billy rosewood -lrb- judge reinhold -rrb- and jon flint -lrb- hector elizondo -rrb- of the beverly hills police department .
Head Entity: hector elizondo
Tail Entity: beverly hills police department
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person employee of":
Relation: person employee of  
Context: After years of hard work and dedication, Sarah Thompson finally received a promotion at Tech Innovations, where she has been a key player in the development team.  
Head Entity: Sarah Thompson  
Tail Entity: Tech Innovations  

Relation: person employee of  
Context: John Smith has been with Global Finance for over a decade, where he has climbed the ranks to become the senior analyst in the investment division.  
Head Entity: John Smith  
Tail Entity: Global Finance  
#### person stateorprovince of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person stateorprovince of death" means a person was died in a state or province.
Here is an example:
Relation: person stateorprovince of death
Context: millender-mcdonald , who was 68 , died late saturday at her home in carson , california , said her chief of staff , bandele mcqueen .
Head Entity: millender-mcdonald
Tail Entity: california
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person stateorprovince of death":
Relation: person stateorprovince of death  
Context: johnson, a renowned author, passed away peacefully in his sleep at his residence in new york, new york, surrounded by family and friends.  
Head Entity: johnson  
Tail Entity: new york  

Relation: person stateorprovince of death  
Context: the famous musician, who was 45, died unexpectedly in a hospital in nashville, tennessee, after a brief illness.  
Head Entity: the famous musician  
Tail Entity: tennessee  
Mixup data size:  224
MixupTrain:  epoch  0, batch     0 | loss: 2.4420063MixupTrain:  epoch  0, batch     1 | loss: 2.5780026MixupTrain:  epoch  0, batch     2 | loss: 2.7317827MixupTrain:  epoch  0, batch     3 | loss: 2.5864591MixupTrain:  epoch  0, batch     4 | loss: 2.3836176MixupTrain:  epoch  0, batch     5 | loss: 2.5429648MixupTrain:  epoch  0, batch     6 | loss: 2.1029835MixupTrain:  epoch  0, batch     7 | loss: 2.4245019MixupTrain:  epoch  0, batch     8 | loss: 2.6240474MixupTrain:  epoch  0, batch     9 | loss: 2.5082636MixupTrain:  epoch  0, batch    10 | loss: 2.6828088MixupTrain:  epoch  0, batch    11 | loss: 2.1479129MixupTrain:  epoch  0, batch    12 | loss: 2.9749391MixupTrain:  epoch  0, batch    13 | loss: 2.4401637
MemoryTrain:  epoch  0, batch     0 | loss: 1.6063697MemoryTrain:  epoch  0, batch     1 | loss: 2.3136585MemoryTrain:  epoch  0, batch     2 | loss: 2.6799362MemoryTrain:  epoch  0, batch     3 | loss: 2.5806861MemoryTrain:  epoch  0, batch     4 | loss: 3.5117390MemoryTrain:  epoch  0, batch     5 | loss: 2.1310010MemoryTrain:  epoch  1, batch     0 | loss: 1.7650944MemoryTrain:  epoch  1, batch     1 | loss: 2.7442777MemoryTrain:  epoch  1, batch     2 | loss: 1.6644148MemoryTrain:  epoch  1, batch     3 | loss: 2.1910117MemoryTrain:  epoch  1, batch     4 | loss: 2.6111784MemoryTrain:  epoch  1, batch     5 | loss: 2.2024615MemoryTrain:  epoch  2, batch     0 | loss: 1.9852383MemoryTrain:  epoch  2, batch     1 | loss: 1.6579881MemoryTrain:  epoch  2, batch     2 | loss: 1.7586806MemoryTrain:  epoch  2, batch     3 | loss: 1.8914747MemoryTrain:  epoch  2, batch     4 | loss: 2.2248774MemoryTrain:  epoch  2, batch     5 | loss: 2.1952286MemoryTrain:  epoch  3, batch     0 | loss: 1.9617805MemoryTrain:  epoch  3, batch     1 | loss: 1.8359880MemoryTrain:  epoch  3, batch     2 | loss: 1.8464999MemoryTrain:  epoch  3, batch     3 | loss: 1.4340458MemoryTrain:  epoch  3, batch     4 | loss: 1.5190341MemoryTrain:  epoch  3, batch     5 | loss: 1.5679187MemoryTrain:  epoch  4, batch     0 | loss: 1.7089179MemoryTrain:  epoch  4, batch     1 | loss: 1.4576018MemoryTrain:  epoch  4, batch     2 | loss: 1.5333214MemoryTrain:  epoch  4, batch     3 | loss: 1.8314778MemoryTrain:  epoch  4, batch     4 | loss: 1.5803329MemoryTrain:  epoch  4, batch     5 | loss: 1.4577296MemoryTrain:  epoch  5, batch     0 | loss: 1.5836000MemoryTrain:  epoch  5, batch     1 | loss: 1.5379591MemoryTrain:  epoch  5, batch     2 | loss: 1.4369960MemoryTrain:  epoch  5, batch     3 | loss: 1.5925547MemoryTrain:  epoch  5, batch     4 | loss: 1.6078494MemoryTrain:  epoch  5, batch     5 | loss: 1.5396156MemoryTrain:  epoch  6, batch     0 | loss: 1.4661016MemoryTrain:  epoch  6, batch     1 | loss: 1.4595349MemoryTrain:  epoch  6, batch     2 | loss: 1.5746039MemoryTrain:  epoch  6, batch     3 | loss: 1.5718143MemoryTrain:  epoch  6, batch     4 | loss: 1.4662881MemoryTrain:  epoch  6, batch     5 | loss: 1.3652970MemoryTrain:  epoch  7, batch     0 | loss: 1.4432670MemoryTrain:  epoch  7, batch     1 | loss: 1.4731194MemoryTrain:  epoch  7, batch     2 | loss: 1.3689885MemoryTrain:  epoch  7, batch     3 | loss: 1.3406279MemoryTrain:  epoch  7, batch     4 | loss: 1.4256105MemoryTrain:  epoch  7, batch     5 | loss: 1.4058909MemoryTrain:  epoch  8, batch     0 | loss: 1.3612473MemoryTrain:  epoch  8, batch     1 | loss: 1.3541522MemoryTrain:  epoch  8, batch     2 | loss: 1.4499009MemoryTrain:  epoch  8, batch     3 | loss: 1.3569043MemoryTrain:  epoch  8, batch     4 | loss: 1.3117306MemoryTrain:  epoch  8, batch     5 | loss: 1.4678290MemoryTrain:  epoch  9, batch     0 | loss: 1.3687000MemoryTrain:  epoch  9, batch     1 | loss: 1.3146439MemoryTrain:  epoch  9, batch     2 | loss: 1.3397418MemoryTrain:  epoch  9, batch     3 | loss: 1.2869749MemoryTrain:  epoch  9, batch     4 | loss: 1.2883458MemoryTrain:  epoch  9, batch     5 | loss: 1.3469371
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 75.00%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 79.17%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 82.50%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 82.29%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 82.81%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 84.03%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 84.66%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 84.38%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 84.62%   [EVAL] batch:   13 | acc: 56.25%,  total acc: 82.59%   
[EVAL] batch:    0 | acc: 12.50%,  total acc: 12.50%   [EVAL] batch:    1 | acc: 18.75%,  total acc: 15.62%   [EVAL] batch:    2 | acc: 12.50%,  total acc: 14.58%   [EVAL] batch:    3 | acc: 18.75%,  total acc: 15.62%   [EVAL] batch:    4 | acc: 18.75%,  total acc: 16.25%   [EVAL] batch:    5 | acc: 6.25%,  total acc: 14.58%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 22.32%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 32.03%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 38.19%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 43.12%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 48.30%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 51.56%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 52.88%   [EVAL] batch:   13 | acc: 43.75%,  total acc: 52.23%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 53.75%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 53.91%   [EVAL] batch:   16 | acc: 81.25%,  total acc: 55.51%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 55.90%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 56.58%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 58.13%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 60.12%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 61.65%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 63.32%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 64.84%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 66.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 67.55%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 68.52%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 69.64%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 70.69%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 71.25%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 72.18%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 73.05%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 71.97%   [EVAL] batch:   33 | acc: 6.25%,  total acc: 70.04%   [EVAL] batch:   34 | acc: 6.25%,  total acc: 68.21%   [EVAL] batch:   35 | acc: 12.50%,  total acc: 66.67%   [EVAL] batch:   36 | acc: 18.75%,  total acc: 65.37%   [EVAL] batch:   37 | acc: 12.50%,  total acc: 63.98%   [EVAL] batch:   38 | acc: 25.00%,  total acc: 62.98%   [EVAL] batch:   39 | acc: 93.75%,  total acc: 63.75%   [EVAL] batch:   40 | acc: 37.50%,  total acc: 63.11%   [EVAL] batch:   41 | acc: 50.00%,  total acc: 62.80%   [EVAL] batch:   42 | acc: 56.25%,  total acc: 62.65%   [EVAL] batch:   43 | acc: 68.75%,  total acc: 62.78%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 63.61%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 64.40%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 65.16%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 65.89%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 66.58%   [EVAL] batch:   49 | acc: 81.25%,  total acc: 66.88%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 67.40%   [EVAL] batch:   51 | acc: 81.25%,  total acc: 67.67%   [EVAL] batch:   52 | acc: 50.00%,  total acc: 67.33%   [EVAL] batch:   53 | acc: 12.50%,  total acc: 66.32%   [EVAL] batch:   54 | acc: 18.75%,  total acc: 65.45%   [EVAL] batch:   55 | acc: 12.50%,  total acc: 64.51%   [EVAL] batch:   56 | acc: 18.75%,  total acc: 63.71%   [EVAL] batch:   57 | acc: 75.00%,  total acc: 63.90%   [EVAL] batch:   58 | acc: 62.50%,  total acc: 63.88%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 64.48%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 65.06%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 65.62%   [EVAL] batch:   62 | acc: 100.00%,  total acc: 66.17%   [EVAL] batch:   63 | acc: 100.00%,  total acc: 66.70%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 67.21%   [EVAL] batch:   65 | acc: 37.50%,  total acc: 66.76%   [EVAL] batch:   66 | acc: 6.25%,  total acc: 65.86%   [EVAL] batch:   67 | acc: 43.75%,  total acc: 65.53%   [EVAL] batch:   68 | acc: 75.00%,  total acc: 65.67%   [EVAL] batch:   69 | acc: 75.00%,  total acc: 65.80%   [EVAL] batch:   70 | acc: 50.00%,  total acc: 65.58%   [EVAL] batch:   71 | acc: 31.25%,  total acc: 65.10%   [EVAL] batch:   72 | acc: 43.75%,  total acc: 64.81%   [EVAL] batch:   73 | acc: 56.25%,  total acc: 64.70%   [EVAL] batch:   74 | acc: 43.75%,  total acc: 64.42%   [EVAL] batch:   75 | acc: 81.25%,  total acc: 64.64%   [EVAL] batch:   76 | acc: 81.25%,  total acc: 64.85%   [EVAL] batch:   77 | acc: 100.00%,  total acc: 65.30%   [EVAL] batch:   78 | acc: 75.00%,  total acc: 65.43%   [EVAL] batch:   79 | acc: 93.75%,  total acc: 65.78%   [EVAL] batch:   80 | acc: 31.25%,  total acc: 65.35%   [EVAL] batch:   81 | acc: 18.75%,  total acc: 64.79%   [EVAL] batch:   82 | acc: 31.25%,  total acc: 64.38%   [EVAL] batch:   83 | acc: 37.50%,  total acc: 64.06%   [EVAL] batch:   84 | acc: 25.00%,  total acc: 63.60%   [EVAL] batch:   85 | acc: 62.50%,  total acc: 63.59%   [EVAL] batch:   86 | acc: 87.50%,  total acc: 63.86%   [EVAL] batch:   87 | acc: 87.50%,  total acc: 64.13%   [EVAL] batch:   88 | acc: 87.50%,  total acc: 64.40%   [EVAL] batch:   89 | acc: 87.50%,  total acc: 64.65%   [EVAL] batch:   90 | acc: 75.00%,  total acc: 64.77%   [EVAL] batch:   91 | acc: 81.25%,  total acc: 64.95%   [EVAL] batch:   92 | acc: 93.75%,  total acc: 65.26%   [EVAL] batch:   93 | acc: 93.75%,  total acc: 65.56%   [EVAL] batch:   94 | acc: 87.50%,  total acc: 65.79%   [EVAL] batch:   95 | acc: 87.50%,  total acc: 66.02%   [EVAL] batch:   96 | acc: 81.25%,  total acc: 66.17%   [EVAL] batch:   97 | acc: 87.50%,  total acc: 66.39%   [EVAL] batch:   98 | acc: 50.00%,  total acc: 66.22%   
cur_acc:  ['0.8523', '0.8438', '0.3672', '0.7902', '0.7042', '0.8259']
his_acc:  ['0.8523', '0.8213', '0.7346', '0.6875', '0.6868', '0.6622']
CurrentTrain: epoch  0, batch     0 | loss: 7.5573845CurrentTrain: epoch  0, batch     1 | loss: 8.4202833CurrentTrain: epoch  1, batch     0 | loss: 7.4924698CurrentTrain: epoch  1, batch     1 | loss: 6.5088062CurrentTrain: epoch  2, batch     0 | loss: 7.1656485CurrentTrain: epoch  2, batch     1 | loss: 6.5468659CurrentTrain: epoch  3, batch     0 | loss: 6.4034243CurrentTrain: epoch  3, batch     1 | loss: 6.3851233CurrentTrain: epoch  4, batch     0 | loss: 6.0822897CurrentTrain: epoch  4, batch     1 | loss: 5.4577985CurrentTrain: epoch  5, batch     0 | loss: 5.2373652CurrentTrain: epoch  5, batch     1 | loss: 5.8850312CurrentTrain: epoch  6, batch     0 | loss: 5.3343635CurrentTrain: epoch  6, batch     1 | loss: 4.8990045CurrentTrain: epoch  7, batch     0 | loss: 4.5349207CurrentTrain: epoch  7, batch     1 | loss: 4.9589372CurrentTrain: epoch  8, batch     0 | loss: 4.2311144CurrentTrain: epoch  8, batch     1 | loss: 5.2780838CurrentTrain: epoch  9, batch     0 | loss: 4.5802135CurrentTrain: epoch  9, batch     1 | loss: 4.0969882
#### organization subsidiaries ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization subsidiaries" means an organization 's subsidiary is an organization.
Here is an example:
Relation: organization subsidiaries
Context: venture fund buys sporting chain highland capital 's consumer fund includes lululemon athletica , a yoga retailer , and o beverages , a flavored water company developed by tom first , one of the two `` juice guys '' who cofounded nantucket nectars .
Head Entity: highland capital
Tail Entity: o beverages
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization subsidiaries":
Relation: organization subsidiaries  
Context: The tech giant Alphabet Inc. has several subsidiaries, including YouTube, which has become a leading platform for video content, and Waymo, which focuses on self-driving technology.  
Head Entity: Alphabet Inc.  
Tail Entity: Waymo  

Relation: organization subsidiaries  
Context: The automotive manufacturer Ford Motor Company has expanded its reach by acquiring several companies, including the electric vehicle startup Rivian, which aims to revolutionize the EV market.  
Head Entity: Ford Motor Company  
Tail Entity: Rivian  
#### organization parents ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization parents" means an organization 's parent is an organization.
Here is an example:
Relation: organization parents
Context: those who try to salvage possessions from the debris of their homes can easily turn into victims too , said dr. irwin redlener , director of the national center for disaster preparedness at columbia university mailman school of medicine .
Head Entity: national center for disaster preparedness
Tail Entity: columbia university mailman school of medicine
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization parents":
Relation: organization parents  
Context: The Massachusetts Institute of Technology, known for its cutting-edge research and innovation, is part of a larger network of educational institutions that includes Harvard University.  
Head Entity: Massachusetts Institute of Technology  
Tail Entity: Harvard University  

Relation: organization parents  
Context: The World Wildlife Fund, a leading organization in conservation efforts, operates under the umbrella of the World Wide Fund for Nature, which has a broader international focus.  
Head Entity: World Wildlife Fund  
Tail Entity: World Wide Fund for Nature  
#### organization alternate names ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization alternate names" means a person 's alias is a person.
Here is an example:
Relation: organization alternate names
Context: the national security council -lrb- nsc -rrb- made the demand at talks chaired by president hamid karzai , who has been vocal in condemning international forces he believes are responsible for the incident last saturday in the eastern flashpoint of kunar .
Head Entity: national security council
Tail Entity: nsc
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization alternate names":
Relation: organization alternate names  
Context: the world health organization -lrb- who -rrb- has been instrumental in coordinating global responses to health emergencies, including the recent pandemic.  
Head Entity: world health organization  
Tail Entity: who  

Relation: organization alternate names  
Context: the federal bureau of investigation -lrb- fbi -rrb- is known for its role in investigating federal crimes and enforcing federal laws across the United States.  
Head Entity: federal bureau of investigation  
Tail Entity: fbi  
#### organization city of headquarters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization city of headquarters" means an organization is located in a city.
Here is an example:
Relation: organization city of headquarters
Context: guy-sheftall entered spelman college in atlanta at age 16 and later earned a master 's in english with a thesis titled `` faulkner 's treatment of women in his major novels . ''
Head Entity: spelman college
Tail Entity: atlanta
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization city of headquarters":
Relation: organization city of headquarters  
Context: the headquarters of tech giant apple inc. is located in cupertino, california, where it has been since 1993.  
Head Entity: apple inc.  
Tail Entity: cupertino  

Relation: organization city of headquarters  
Context: the united nations has its main office in new york city, serving as a hub for international diplomacy and cooperation.  
Head Entity: united nations  
Tail Entity: new york city  
#### person siblings ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person siblings" means a person 's sibling is a person.
Here is an example:
Relation: person siblings
Context: `` holly and sanjaya are headed to -lsb- the hawaiian island of -rsb- kauai tomorrow morning so she can meet his parents . ''
Head Entity: she
Tail Entity: sanjaya
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person siblings":
Relation: person siblings  
Context: `` during the family reunion, john introduced his sister to everyone, and they all had a great time reminiscing about their childhood. ''  
Head Entity: his sister  
Tail Entity: john  

Relation: person siblings  
Context: `` after the graduation ceremony, emily celebrated with her brother, who had always been her biggest supporter throughout school. ''  
Head Entity: her brother  
Tail Entity: emily  
Mixup data size:  255
MixupTrain:  epoch  0, batch     0 | loss: 2.1158221MixupTrain:  epoch  0, batch     1 | loss: 2.4884054MixupTrain:  epoch  0, batch     2 | loss: 2.3714261MixupTrain:  epoch  0, batch     3 | loss: 2.3484565MixupTrain:  epoch  0, batch     4 | loss: 2.6978247MixupTrain:  epoch  0, batch     5 | loss: 2.5756155MixupTrain:  epoch  0, batch     6 | loss: 2.4552649MixupTrain:  epoch  0, batch     7 | loss: 2.6540978MixupTrain:  epoch  0, batch     8 | loss: 2.7415743MixupTrain:  epoch  0, batch     9 | loss: 2.2021920MixupTrain:  epoch  0, batch    10 | loss: 2.2722345MixupTrain:  epoch  0, batch    11 | loss: 2.6433509MixupTrain:  epoch  0, batch    12 | loss: 2.1536429MixupTrain:  epoch  0, batch    13 | loss: 2.0324410MixupTrain:  epoch  0, batch    14 | loss: 2.4315867MixupTrain:  epoch  0, batch    15 | loss: 2.0798760
MemoryTrain:  epoch  0, batch     0 | loss: 2.0564580MemoryTrain:  epoch  0, batch     1 | loss: 1.9226570MemoryTrain:  epoch  0, batch     2 | loss: 1.7713022MemoryTrain:  epoch  0, batch     3 | loss: 2.2790325MemoryTrain:  epoch  0, batch     4 | loss: 2.9511676MemoryTrain:  epoch  0, batch     5 | loss: 2.4066496MemoryTrain:  epoch  0, batch     6 | loss: 2.5365410MemoryTrain:  epoch  1, batch     0 | loss: 2.0493033MemoryTrain:  epoch  1, batch     1 | loss: 2.1494370MemoryTrain:  epoch  1, batch     2 | loss: 2.2294135MemoryTrain:  epoch  1, batch     3 | loss: 2.1300209MemoryTrain:  epoch  1, batch     4 | loss: 1.8289182MemoryTrain:  epoch  1, batch     5 | loss: 1.8337820MemoryTrain:  epoch  1, batch     6 | loss: 1.4049785MemoryTrain:  epoch  2, batch     0 | loss: 1.9463258MemoryTrain:  epoch  2, batch     1 | loss: 1.7490439MemoryTrain:  epoch  2, batch     2 | loss: 1.6515126MemoryTrain:  epoch  2, batch     3 | loss: 1.3925520MemoryTrain:  epoch  2, batch     4 | loss: 1.7911263MemoryTrain:  epoch  2, batch     5 | loss: 1.5573748MemoryTrain:  epoch  2, batch     6 | loss: 1.7001829MemoryTrain:  epoch  3, batch     0 | loss: 1.9411478MemoryTrain:  epoch  3, batch     1 | loss: 1.6139190MemoryTrain:  epoch  3, batch     2 | loss: 1.6901249MemoryTrain:  epoch  3, batch     3 | loss: 1.4614146MemoryTrain:  epoch  3, batch     4 | loss: 1.6802537MemoryTrain:  epoch  3, batch     5 | loss: 1.4520378MemoryTrain:  epoch  3, batch     6 | loss: 1.5610031MemoryTrain:  epoch  4, batch     0 | loss: 1.5398405MemoryTrain:  epoch  4, batch     1 | loss: 1.5934466MemoryTrain:  epoch  4, batch     2 | loss: 1.3823594MemoryTrain:  epoch  4, batch     3 | loss: 1.5078441MemoryTrain:  epoch  4, batch     4 | loss: 1.8017138MemoryTrain:  epoch  4, batch     5 | loss: 1.6569594MemoryTrain:  epoch  4, batch     6 | loss: 1.2539171MemoryTrain:  epoch  5, batch     0 | loss: 1.6500075MemoryTrain:  epoch  5, batch     1 | loss: 1.4231993MemoryTrain:  epoch  5, batch     2 | loss: 1.4044189MemoryTrain:  epoch  5, batch     3 | loss: 1.4890289MemoryTrain:  epoch  5, batch     4 | loss: 1.4531206MemoryTrain:  epoch  5, batch     5 | loss: 1.2627263MemoryTrain:  epoch  5, batch     6 | loss: 1.2685782MemoryTrain:  epoch  6, batch     0 | loss: 1.3118474MemoryTrain:  epoch  6, batch     1 | loss: 1.4143515MemoryTrain:  epoch  6, batch     2 | loss: 1.3959873MemoryTrain:  epoch  6, batch     3 | loss: 1.6013751MemoryTrain:  epoch  6, batch     4 | loss: 1.4885151MemoryTrain:  epoch  6, batch     5 | loss: 1.3607185MemoryTrain:  epoch  6, batch     6 | loss: 1.3709273MemoryTrain:  epoch  7, batch     0 | loss: 1.4061604MemoryTrain:  epoch  7, batch     1 | loss: 1.4794091MemoryTrain:  epoch  7, batch     2 | loss: 1.4087846MemoryTrain:  epoch  7, batch     3 | loss: 1.3277646MemoryTrain:  epoch  7, batch     4 | loss: 1.2638502MemoryTrain:  epoch  7, batch     5 | loss: 1.3086934MemoryTrain:  epoch  7, batch     6 | loss: 1.3473759MemoryTrain:  epoch  8, batch     0 | loss: 1.3572588MemoryTrain:  epoch  8, batch     1 | loss: 1.2933598MemoryTrain:  epoch  8, batch     2 | loss: 1.3280361MemoryTrain:  epoch  8, batch     3 | loss: 1.3211002MemoryTrain:  epoch  8, batch     4 | loss: 1.3179970MemoryTrain:  epoch  8, batch     5 | loss: 1.3080106MemoryTrain:  epoch  8, batch     6 | loss: 1.3112512MemoryTrain:  epoch  9, batch     0 | loss: 1.2480698MemoryTrain:  epoch  9, batch     1 | loss: 1.3052917MemoryTrain:  epoch  9, batch     2 | loss: 1.3382442MemoryTrain:  epoch  9, batch     3 | loss: 1.3699293MemoryTrain:  epoch  9, batch     4 | loss: 1.2551205MemoryTrain:  epoch  9, batch     5 | loss: 1.2545320MemoryTrain:  epoch  9, batch     6 | loss: 1.3585314
[EVAL] batch:    0 | acc: 12.50%,  total acc: 12.50%   [EVAL] batch:    1 | acc: 31.25%,  total acc: 21.88%   [EVAL] batch:    2 | acc: 12.50%,  total acc: 18.75%   [EVAL] batch:    3 | acc: 31.25%,  total acc: 21.88%   [EVAL] batch:    4 | acc: 18.75%,  total acc: 21.25%   [EVAL] batch:    5 | acc: 31.25%,  total acc: 22.92%   [EVAL] batch:    6 | acc: 31.25%,  total acc: 24.11%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 32.03%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 36.11%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 38.75%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 43.18%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 46.88%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 49.04%   [EVAL] batch:   13 | acc: 100.00%,  total acc: 52.68%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 55.42%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 58.20%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 60.66%   [EVAL] batch:   17 | acc: 87.50%,  total acc: 62.15%   [EVAL] batch:   18 | acc: 6.25%,  total acc: 59.21%   [EVAL] batch:   19 | acc: 6.25%,  total acc: 56.56%   [EVAL] batch:   20 | acc: 0.00%,  total acc: 53.87%   [EVAL] batch:   21 | acc: 0.00%,  total acc: 51.42%   
[EVAL] batch:    0 | acc: 12.50%,  total acc: 12.50%   [EVAL] batch:    1 | acc: 25.00%,  total acc: 18.75%   [EVAL] batch:    2 | acc: 18.75%,  total acc: 18.75%   [EVAL] batch:    3 | acc: 12.50%,  total acc: 17.19%   [EVAL] batch:    4 | acc: 18.75%,  total acc: 17.50%   [EVAL] batch:    5 | acc: 6.25%,  total acc: 15.62%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 23.21%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 32.81%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 38.89%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 43.75%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 47.73%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 51.04%   [EVAL] batch:   12 | acc: 50.00%,  total acc: 50.96%   [EVAL] batch:   13 | acc: 37.50%,  total acc: 50.00%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 51.25%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 51.56%   [EVAL] batch:   16 | acc: 81.25%,  total acc: 53.31%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 53.82%   [EVAL] batch:   18 | acc: 75.00%,  total acc: 54.93%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 56.56%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 58.63%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 60.23%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 61.96%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 63.28%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 64.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 66.11%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 67.13%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 68.30%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 69.40%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 69.79%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 70.36%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 71.09%   [EVAL] batch:   32 | acc: 50.00%,  total acc: 70.45%   [EVAL] batch:   33 | acc: 12.50%,  total acc: 68.75%   [EVAL] batch:   34 | acc: 12.50%,  total acc: 67.14%   [EVAL] batch:   35 | acc: 12.50%,  total acc: 65.62%   [EVAL] batch:   36 | acc: 18.75%,  total acc: 64.36%   [EVAL] batch:   37 | acc: 25.00%,  total acc: 63.32%   [EVAL] batch:   38 | acc: 31.25%,  total acc: 62.50%   [EVAL] batch:   39 | acc: 93.75%,  total acc: 63.28%   [EVAL] batch:   40 | acc: 37.50%,  total acc: 62.65%   [EVAL] batch:   41 | acc: 43.75%,  total acc: 62.20%   [EVAL] batch:   42 | acc: 50.00%,  total acc: 61.92%   [EVAL] batch:   43 | acc: 68.75%,  total acc: 62.07%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 62.92%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 63.72%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 64.49%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 65.23%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 65.94%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 66.38%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 66.91%   [EVAL] batch:   51 | acc: 75.00%,  total acc: 67.07%   [EVAL] batch:   52 | acc: 50.00%,  total acc: 66.75%   [EVAL] batch:   53 | acc: 25.00%,  total acc: 65.97%   [EVAL] batch:   54 | acc: 18.75%,  total acc: 65.11%   [EVAL] batch:   55 | acc: 25.00%,  total acc: 64.40%   [EVAL] batch:   56 | acc: 31.25%,  total acc: 63.82%   [EVAL] batch:   57 | acc: 75.00%,  total acc: 64.01%   [EVAL] batch:   58 | acc: 62.50%,  total acc: 63.98%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 64.58%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 65.16%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 65.73%   [EVAL] batch:   62 | acc: 100.00%,  total acc: 66.27%   [EVAL] batch:   63 | acc: 100.00%,  total acc: 66.80%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 67.31%   [EVAL] batch:   65 | acc: 37.50%,  total acc: 66.86%   [EVAL] batch:   66 | acc: 6.25%,  total acc: 65.95%   [EVAL] batch:   67 | acc: 37.50%,  total acc: 65.53%   [EVAL] batch:   68 | acc: 75.00%,  total acc: 65.67%   [EVAL] batch:   69 | acc: 68.75%,  total acc: 65.71%   [EVAL] batch:   70 | acc: 56.25%,  total acc: 65.58%   [EVAL] batch:   71 | acc: 31.25%,  total acc: 65.10%   [EVAL] batch:   72 | acc: 43.75%,  total acc: 64.81%   [EVAL] batch:   73 | acc: 56.25%,  total acc: 64.70%   [EVAL] batch:   74 | acc: 31.25%,  total acc: 64.25%   [EVAL] batch:   75 | acc: 81.25%,  total acc: 64.47%   [EVAL] batch:   76 | acc: 81.25%,  total acc: 64.69%   [EVAL] batch:   77 | acc: 100.00%,  total acc: 65.14%   [EVAL] batch:   78 | acc: 68.75%,  total acc: 65.19%   [EVAL] batch:   79 | acc: 93.75%,  total acc: 65.55%   [EVAL] batch:   80 | acc: 43.75%,  total acc: 65.28%   [EVAL] batch:   81 | acc: 18.75%,  total acc: 64.71%   [EVAL] batch:   82 | acc: 37.50%,  total acc: 64.38%   [EVAL] batch:   83 | acc: 18.75%,  total acc: 63.84%   [EVAL] batch:   84 | acc: 18.75%,  total acc: 63.31%   [EVAL] batch:   85 | acc: 68.75%,  total acc: 63.37%   [EVAL] batch:   86 | acc: 75.00%,  total acc: 63.51%   [EVAL] batch:   87 | acc: 75.00%,  total acc: 63.64%   [EVAL] batch:   88 | acc: 75.00%,  total acc: 63.76%   [EVAL] batch:   89 | acc: 75.00%,  total acc: 63.89%   [EVAL] batch:   90 | acc: 56.25%,  total acc: 63.80%   [EVAL] batch:   91 | acc: 62.50%,  total acc: 63.79%   [EVAL] batch:   92 | acc: 93.75%,  total acc: 64.11%   [EVAL] batch:   93 | acc: 87.50%,  total acc: 64.36%   [EVAL] batch:   94 | acc: 87.50%,  total acc: 64.61%   [EVAL] batch:   95 | acc: 87.50%,  total acc: 64.84%   [EVAL] batch:   96 | acc: 81.25%,  total acc: 65.01%   [EVAL] batch:   97 | acc: 87.50%,  total acc: 65.24%   [EVAL] batch:   98 | acc: 56.25%,  total acc: 65.15%   [EVAL] batch:   99 | acc: 18.75%,  total acc: 64.69%   [EVAL] batch:  100 | acc: 25.00%,  total acc: 64.29%   [EVAL] batch:  101 | acc: 12.50%,  total acc: 63.79%   [EVAL] batch:  102 | acc: 25.00%,  total acc: 63.41%   [EVAL] batch:  103 | acc: 25.00%,  total acc: 63.04%   [EVAL] batch:  104 | acc: 31.25%,  total acc: 62.74%   [EVAL] batch:  105 | acc: 56.25%,  total acc: 62.68%   [EVAL] batch:  106 | acc: 87.50%,  total acc: 62.91%   [EVAL] batch:  107 | acc: 62.50%,  total acc: 62.91%   [EVAL] batch:  108 | acc: 75.00%,  total acc: 63.02%   [EVAL] batch:  109 | acc: 81.25%,  total acc: 63.18%   [EVAL] batch:  110 | acc: 81.25%,  total acc: 63.34%   [EVAL] batch:  111 | acc: 87.50%,  total acc: 63.56%   [EVAL] batch:  112 | acc: 100.00%,  total acc: 63.88%   [EVAL] batch:  113 | acc: 93.75%,  total acc: 64.14%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 64.46%   [EVAL] batch:  115 | acc: 87.50%,  total acc: 64.66%   [EVAL] batch:  116 | acc: 62.50%,  total acc: 64.64%   [EVAL] batch:  117 | acc: 6.25%,  total acc: 64.14%   [EVAL] batch:  118 | acc: 6.25%,  total acc: 63.66%   [EVAL] batch:  119 | acc: 0.00%,  total acc: 63.12%   [EVAL] batch:  120 | acc: 0.00%,  total acc: 62.60%   
cur_acc:  ['0.8523', '0.8438', '0.3672', '0.7902', '0.7042', '0.8259', '0.5142']
his_acc:  ['0.8523', '0.8213', '0.7346', '0.6875', '0.6868', '0.6622', '0.6260']
CurrentTrain: epoch  0, batch     0 | loss: 4.9776297CurrentTrain: epoch  0, batch     1 | loss: 4.9994888CurrentTrain: epoch  1, batch     0 | loss: 3.8709946CurrentTrain: epoch  1, batch     1 | loss: 3.5531468CurrentTrain: epoch  2, batch     0 | loss: 3.1721973CurrentTrain: epoch  2, batch     1 | loss: 3.3850284CurrentTrain: epoch  3, batch     0 | loss: 2.6884408CurrentTrain: epoch  3, batch     1 | loss: 2.4957166CurrentTrain: epoch  4, batch     0 | loss: 2.3671560CurrentTrain: epoch  4, batch     1 | loss: 2.3383543CurrentTrain: epoch  5, batch     0 | loss: 2.2383292CurrentTrain: epoch  5, batch     1 | loss: 2.2704573CurrentTrain: epoch  6, batch     0 | loss: 2.2051742CurrentTrain: epoch  6, batch     1 | loss: 2.0649495CurrentTrain: epoch  7, batch     0 | loss: 2.0614758CurrentTrain: epoch  7, batch     1 | loss: 2.0174561CurrentTrain: epoch  8, batch     0 | loss: 1.9102356CurrentTrain: epoch  8, batch     1 | loss: 1.9960798CurrentTrain: epoch  9, batch     0 | loss: 1.9466529CurrentTrain: epoch  9, batch     1 | loss: 1.8365265
#### person cause of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person cause of death" means a person was died of an event.
Here is an example:
Relation: person cause of death
Context: mr scheider had suffered from multiple myeloma for several years , and died of complications from a staph infection , his wife , brenda siemer , said .
Head Entity: mr scheider
Tail Entity: complications from a staph infection
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person cause of death":
Relation: person cause of death  
Context: after battling with lung cancer for over a year, john doe passed away peacefully in his sleep, surrounded by family.  
Head Entity: john doe  
Tail Entity: lung cancer  

Relation: person cause of death  
Context: the renowned scientist, dr. emily white, tragically lost her life due to a car accident while returning from a conference.  
Head Entity: dr. emily white  
Tail Entity: car accident  
#### organization political religious affiliation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization political religious affiliation" means an organization is the member of a religion.
Here is an example:
Relation: organization political religious affiliation
Context: the mnlf used to be the largest muslim group fighting for a separate islamic homeland in the southern philippines until it settled for limited autonomy and signed a peace agreement with manila in 1996 .
Head Entity: mnlf
Tail Entity: islamic
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization political religious affiliation":
Relation: organization political religious affiliation  
Context: the national council of churches in the philippines has been a significant advocate for social justice and has ties to various religious groups across the country.  
Head Entity: national council of churches in the philippines  
Tail Entity: religious groups  

Relation: organization political religious affiliation  
Context: the interfaith alliance works to promote understanding and cooperation among different faiths, influencing political discourse in the region.  
Head Entity: interfaith alliance  
Tail Entity: faiths  
#### organization stateorprovince of headquarters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization stateorprovince of headquarters" means an organization is located in a state or province.
Here is an example:
Relation: organization stateorprovince of headquarters
Context: based in armonk , new york , mbia insures $ 670 billion -lrb- euro452 .18 billion -rrb- in debt .
Head Entity: mbia
Tail Entity: new york
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization stateorprovince of headquarters":
Relation: organization stateorprovince of headquarters  
Context: the tech giant apple inc. has its headquarters in cupertino, california, where it develops innovative products.  
Head Entity: apple inc.  
Tail Entity: california  

Relation: organization stateorprovince of headquarters  
Context: the multinational corporation samsung electronics is headquartered in suwon, south korea, and is a leader in consumer electronics.  
Head Entity: samsung electronics  
Tail Entity: south korea  
#### person other family ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person other family" means a person 's relative is a person.
Here is an example:
Relation: person other family
Context: parren mitchell 's sister-in-law , juanita jackson mitchell , was the long - time head and legal counsel of the maryland naacp .
Head Entity: parren mitchell
Tail Entity: juanita jackson mitchell
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person other family":
Relation: person other family  
Context: During the family reunion, it was revealed that Sarah's cousin, Michael, had recently graduated from college with honors.  
Head Entity: Sarah  
Tail Entity: Michael  

Relation: person other family  
Context: In her memoir, Angela described the close bond she shared with her aunt, who played a significant role in her upbringing.  
Head Entity: Angela  
Tail Entity: her aunt  
#### person city of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person city of death" means a person was died in a city.
Here is an example:
Relation: person city of death
Context: irene morgan kirkaldy , 90 , who died of alzheimer 's disease aug. 10 at her home in gloucester , va. , quietly changed history in 1944 when she refused to give up her seat on a crowded greyhound bus to a white couple .
Head Entity: her
Tail Entity: gloucester
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person city of death":
Relation: person city of death  
Context: john smith, a renowned author, passed away on march 5 in his apartment located in new york city, leaving behind a legacy of literary works that inspired many.  
Head Entity: john smith  
Tail Entity: new york city  

Relation: person city of death  
Context: after a long battle with cancer, elizabeth taylor died on march 23 at a hospital in los angeles, where she had spent her final days surrounded by family and friends.  
Head Entity: elizabeth taylor  
Tail Entity: los angeles  
Mixup data size:  284
MixupTrain:  epoch  0, batch     0 | loss: 1.9973840MixupTrain:  epoch  0, batch     1 | loss: 2.2420411MixupTrain:  epoch  0, batch     2 | loss: 2.1564011MixupTrain:  epoch  0, batch     3 | loss: 2.2300944MixupTrain:  epoch  0, batch     4 | loss: 2.2395858MixupTrain:  epoch  0, batch     5 | loss: 2.0349777MixupTrain:  epoch  0, batch     6 | loss: 2.8348701MixupTrain:  epoch  0, batch     7 | loss: 2.1214985MixupTrain:  epoch  0, batch     8 | loss: 2.1063576MixupTrain:  epoch  0, batch     9 | loss: 2.1532677MixupTrain:  epoch  0, batch    10 | loss: 2.1850870MixupTrain:  epoch  0, batch    11 | loss: 1.9989321MixupTrain:  epoch  0, batch    12 | loss: 2.0048398MixupTrain:  epoch  0, batch    13 | loss: 2.6753243MixupTrain:  epoch  0, batch    14 | loss: 1.9683608MixupTrain:  epoch  0, batch    15 | loss: 1.8518187MixupTrain:  epoch  0, batch    16 | loss: 2.0875408MixupTrain:  epoch  0, batch    17 | loss: 2.4577003
MemoryTrain:  epoch  0, batch     0 | loss: 1.7190306MemoryTrain:  epoch  0, batch     1 | loss: 2.0502613MemoryTrain:  epoch  0, batch     2 | loss: 1.8062375MemoryTrain:  epoch  0, batch     3 | loss: 1.9380879MemoryTrain:  epoch  0, batch     4 | loss: 2.2698090MemoryTrain:  epoch  0, batch     5 | loss: 2.1988533MemoryTrain:  epoch  0, batch     6 | loss: 2.9478660MemoryTrain:  epoch  0, batch     7 | loss: 3.1257041MemoryTrain:  epoch  1, batch     0 | loss: 2.1870046MemoryTrain:  epoch  1, batch     1 | loss: 2.7466755MemoryTrain:  epoch  1, batch     2 | loss: 1.8668587MemoryTrain:  epoch  1, batch     3 | loss: 1.8216029MemoryTrain:  epoch  1, batch     4 | loss: 2.2852592MemoryTrain:  epoch  1, batch     5 | loss: 2.0214784MemoryTrain:  epoch  1, batch     6 | loss: 2.0135098MemoryTrain:  epoch  1, batch     7 | loss: 1.3701848MemoryTrain:  epoch  2, batch     0 | loss: 1.7636365MemoryTrain:  epoch  2, batch     1 | loss: 1.3710284MemoryTrain:  epoch  2, batch     2 | loss: 1.7044177MemoryTrain:  epoch  2, batch     3 | loss: 1.8269382MemoryTrain:  epoch  2, batch     4 | loss: 2.7713046MemoryTrain:  epoch  2, batch     5 | loss: 1.5244738MemoryTrain:  epoch  2, batch     6 | loss: 1.3583318MemoryTrain:  epoch  2, batch     7 | loss: 1.8602521MemoryTrain:  epoch  3, batch     0 | loss: 2.0046535MemoryTrain:  epoch  3, batch     1 | loss: 1.9412810MemoryTrain:  epoch  3, batch     2 | loss: 1.3970957MemoryTrain:  epoch  3, batch     3 | loss: 1.4578707MemoryTrain:  epoch  3, batch     4 | loss: 1.8030113MemoryTrain:  epoch  3, batch     5 | loss: 1.7413164MemoryTrain:  epoch  3, batch     6 | loss: 1.5341879MemoryTrain:  epoch  3, batch     7 | loss: 1.4472557MemoryTrain:  epoch  4, batch     0 | loss: 1.6054438MemoryTrain:  epoch  4, batch     1 | loss: 1.6416332MemoryTrain:  epoch  4, batch     2 | loss: 1.3809772MemoryTrain:  epoch  4, batch     3 | loss: 1.6278818MemoryTrain:  epoch  4, batch     4 | loss: 1.6987588MemoryTrain:  epoch  4, batch     5 | loss: 1.3039321MemoryTrain:  epoch  4, batch     6 | loss: 1.3516066MemoryTrain:  epoch  4, batch     7 | loss: 1.6708602MemoryTrain:  epoch  5, batch     0 | loss: 1.7352868MemoryTrain:  epoch  5, batch     1 | loss: 1.5951700MemoryTrain:  epoch  5, batch     2 | loss: 1.4211986MemoryTrain:  epoch  5, batch     3 | loss: 1.5419972MemoryTrain:  epoch  5, batch     4 | loss: 1.4272738MemoryTrain:  epoch  5, batch     5 | loss: 1.4451463MemoryTrain:  epoch  5, batch     6 | loss: 1.4122810MemoryTrain:  epoch  5, batch     7 | loss: 1.4836593MemoryTrain:  epoch  6, batch     0 | loss: 1.2613909MemoryTrain:  epoch  6, batch     1 | loss: 1.7351047MemoryTrain:  epoch  6, batch     2 | loss: 1.3604054MemoryTrain:  epoch  6, batch     3 | loss: 1.2709793MemoryTrain:  epoch  6, batch     4 | loss: 1.3584185MemoryTrain:  epoch  6, batch     5 | loss: 1.6990917MemoryTrain:  epoch  6, batch     6 | loss: 1.3623396MemoryTrain:  epoch  6, batch     7 | loss: 1.2945937MemoryTrain:  epoch  7, batch     0 | loss: 1.4880471MemoryTrain:  epoch  7, batch     1 | loss: 1.2911953MemoryTrain:  epoch  7, batch     2 | loss: 1.3302333MemoryTrain:  epoch  7, batch     3 | loss: 1.6327010MemoryTrain:  epoch  7, batch     4 | loss: 1.4013324MemoryTrain:  epoch  7, batch     5 | loss: 1.2877184MemoryTrain:  epoch  7, batch     6 | loss: 1.2886372MemoryTrain:  epoch  7, batch     7 | loss: 1.4380496MemoryTrain:  epoch  8, batch     0 | loss: 1.2576536MemoryTrain:  epoch  8, batch     1 | loss: 1.3135775MemoryTrain:  epoch  8, batch     2 | loss: 1.5036780MemoryTrain:  epoch  8, batch     3 | loss: 1.3128214MemoryTrain:  epoch  8, batch     4 | loss: 1.3446214MemoryTrain:  epoch  8, batch     5 | loss: 1.3538625MemoryTrain:  epoch  8, batch     6 | loss: 1.2334762MemoryTrain:  epoch  8, batch     7 | loss: 1.2658929MemoryTrain:  epoch  9, batch     0 | loss: 1.3211999MemoryTrain:  epoch  9, batch     1 | loss: 1.2575148MemoryTrain:  epoch  9, batch     2 | loss: 1.2794445MemoryTrain:  epoch  9, batch     3 | loss: 1.2180961MemoryTrain:  epoch  9, batch     4 | loss: 1.3492267MemoryTrain:  epoch  9, batch     5 | loss: 1.2241586MemoryTrain:  epoch  9, batch     6 | loss: 1.2979366MemoryTrain:  epoch  9, batch     7 | loss: 1.2716627
[EVAL] batch:    0 | acc: 50.00%,  total acc: 50.00%   [EVAL] batch:    1 | acc: 75.00%,  total acc: 62.50%   [EVAL] batch:    2 | acc: 56.25%,  total acc: 60.42%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 62.50%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 67.50%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 71.88%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 73.21%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 74.22%   [EVAL] batch:    8 | acc: 62.50%,  total acc: 72.92%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 71.88%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 73.86%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 73.96%   [EVAL] batch:   12 | acc: 50.00%,  total acc: 72.12%   
[EVAL] batch:    0 | acc: 12.50%,  total acc: 12.50%   [EVAL] batch:    1 | acc: 18.75%,  total acc: 15.62%   [EVAL] batch:    2 | acc: 25.00%,  total acc: 18.75%   [EVAL] batch:    3 | acc: 12.50%,  total acc: 17.19%   [EVAL] batch:    4 | acc: 18.75%,  total acc: 17.50%   [EVAL] batch:    5 | acc: 6.25%,  total acc: 15.62%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 23.21%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 32.81%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 38.89%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 43.12%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 47.16%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 50.52%   [EVAL] batch:   12 | acc: 56.25%,  total acc: 50.96%   [EVAL] batch:   13 | acc: 37.50%,  total acc: 50.00%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 51.67%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 51.95%   [EVAL] batch:   16 | acc: 81.25%,  total acc: 53.68%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 54.17%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 54.93%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 56.56%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 58.63%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 60.23%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 61.96%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 63.28%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 64.75%   [EVAL] batch:   25 | acc: 93.75%,  total acc: 65.87%   [EVAL] batch:   26 | acc: 87.50%,  total acc: 66.67%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 67.86%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 68.75%   [EVAL] batch:   29 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:   30 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 69.53%   [EVAL] batch:   32 | acc: 31.25%,  total acc: 68.37%   [EVAL] batch:   33 | acc: 0.00%,  total acc: 66.36%   [EVAL] batch:   34 | acc: 6.25%,  total acc: 64.64%   [EVAL] batch:   35 | acc: 6.25%,  total acc: 63.02%   [EVAL] batch:   36 | acc: 6.25%,  total acc: 61.49%   [EVAL] batch:   37 | acc: 6.25%,  total acc: 60.03%   [EVAL] batch:   38 | acc: 25.00%,  total acc: 59.13%   [EVAL] batch:   39 | acc: 93.75%,  total acc: 60.00%   [EVAL] batch:   40 | acc: 37.50%,  total acc: 59.45%   [EVAL] batch:   41 | acc: 37.50%,  total acc: 58.93%   [EVAL] batch:   42 | acc: 43.75%,  total acc: 58.58%   [EVAL] batch:   43 | acc: 68.75%,  total acc: 58.81%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 59.72%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 60.60%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 61.44%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 62.24%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 63.01%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 63.50%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 64.09%   [EVAL] batch:   51 | acc: 68.75%,  total acc: 64.18%   [EVAL] batch:   52 | acc: 56.25%,  total acc: 64.03%   [EVAL] batch:   53 | acc: 18.75%,  total acc: 63.19%   [EVAL] batch:   54 | acc: 18.75%,  total acc: 62.39%   [EVAL] batch:   55 | acc: 25.00%,  total acc: 61.72%   [EVAL] batch:   56 | acc: 31.25%,  total acc: 61.18%   [EVAL] batch:   57 | acc: 75.00%,  total acc: 61.42%   [EVAL] batch:   58 | acc: 62.50%,  total acc: 61.44%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 62.08%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 62.70%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 63.31%   [EVAL] batch:   62 | acc: 100.00%,  total acc: 63.89%   [EVAL] batch:   63 | acc: 100.00%,  total acc: 64.45%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 65.00%   [EVAL] batch:   65 | acc: 37.50%,  total acc: 64.58%   [EVAL] batch:   66 | acc: 0.00%,  total acc: 63.62%   [EVAL] batch:   67 | acc: 18.75%,  total acc: 62.96%   [EVAL] batch:   68 | acc: 62.50%,  total acc: 62.95%   [EVAL] batch:   69 | acc: 50.00%,  total acc: 62.77%   [EVAL] batch:   70 | acc: 37.50%,  total acc: 62.41%   [EVAL] batch:   71 | acc: 31.25%,  total acc: 61.98%   [EVAL] batch:   72 | acc: 50.00%,  total acc: 61.82%   [EVAL] batch:   73 | acc: 62.50%,  total acc: 61.82%   [EVAL] batch:   74 | acc: 43.75%,  total acc: 61.58%   [EVAL] batch:   75 | acc: 81.25%,  total acc: 61.84%   [EVAL] batch:   76 | acc: 87.50%,  total acc: 62.18%   [EVAL] batch:   77 | acc: 100.00%,  total acc: 62.66%   [EVAL] batch:   78 | acc: 75.00%,  total acc: 62.82%   [EVAL] batch:   79 | acc: 93.75%,  total acc: 63.20%   [EVAL] batch:   80 | acc: 18.75%,  total acc: 62.65%   [EVAL] batch:   81 | acc: 12.50%,  total acc: 62.04%   [EVAL] batch:   82 | acc: 12.50%,  total acc: 61.45%   [EVAL] batch:   83 | acc: 25.00%,  total acc: 61.01%   [EVAL] batch:   84 | acc: 6.25%,  total acc: 60.37%   [EVAL] batch:   85 | acc: 68.75%,  total acc: 60.47%   [EVAL] batch:   86 | acc: 37.50%,  total acc: 60.20%   [EVAL] batch:   87 | acc: 37.50%,  total acc: 59.94%   [EVAL] batch:   88 | acc: 31.25%,  total acc: 59.62%   [EVAL] batch:   89 | acc: 25.00%,  total acc: 59.24%   [EVAL] batch:   90 | acc: 37.50%,  total acc: 59.00%   [EVAL] batch:   91 | acc: 56.25%,  total acc: 58.97%   [EVAL] batch:   92 | acc: 93.75%,  total acc: 59.34%   [EVAL] batch:   93 | acc: 93.75%,  total acc: 59.71%   [EVAL] batch:   94 | acc: 87.50%,  total acc: 60.00%   [EVAL] batch:   95 | acc: 87.50%,  total acc: 60.29%   [EVAL] batch:   96 | acc: 81.25%,  total acc: 60.50%   [EVAL] batch:   97 | acc: 87.50%,  total acc: 60.78%   [EVAL] batch:   98 | acc: 50.00%,  total acc: 60.67%   [EVAL] batch:   99 | acc: 25.00%,  total acc: 60.31%   [EVAL] batch:  100 | acc: 18.75%,  total acc: 59.90%   [EVAL] batch:  101 | acc: 12.50%,  total acc: 59.44%   [EVAL] batch:  102 | acc: 25.00%,  total acc: 59.10%   [EVAL] batch:  103 | acc: 25.00%,  total acc: 58.77%   [EVAL] batch:  104 | acc: 37.50%,  total acc: 58.57%   [EVAL] batch:  105 | acc: 56.25%,  total acc: 58.55%   [EVAL] batch:  106 | acc: 87.50%,  total acc: 58.82%   [EVAL] batch:  107 | acc: 62.50%,  total acc: 58.85%   [EVAL] batch:  108 | acc: 68.75%,  total acc: 58.94%   [EVAL] batch:  109 | acc: 75.00%,  total acc: 59.09%   [EVAL] batch:  110 | acc: 81.25%,  total acc: 59.29%   [EVAL] batch:  111 | acc: 87.50%,  total acc: 59.54%   [EVAL] batch:  112 | acc: 87.50%,  total acc: 59.79%   [EVAL] batch:  113 | acc: 75.00%,  total acc: 59.92%   [EVAL] batch:  114 | acc: 93.75%,  total acc: 60.22%   [EVAL] batch:  115 | acc: 75.00%,  total acc: 60.34%   [EVAL] batch:  116 | acc: 56.25%,  total acc: 60.31%   [EVAL] batch:  117 | acc: 0.00%,  total acc: 59.80%   [EVAL] batch:  118 | acc: 6.25%,  total acc: 59.35%   [EVAL] batch:  119 | acc: 0.00%,  total acc: 58.85%   [EVAL] batch:  120 | acc: 43.75%,  total acc: 58.73%   [EVAL] batch:  121 | acc: 75.00%,  total acc: 58.86%   [EVAL] batch:  122 | acc: 56.25%,  total acc: 58.84%   [EVAL] batch:  123 | acc: 68.75%,  total acc: 58.92%   [EVAL] batch:  124 | acc: 87.50%,  total acc: 59.15%   [EVAL] batch:  125 | acc: 93.75%,  total acc: 59.42%   [EVAL] batch:  126 | acc: 81.25%,  total acc: 59.60%   [EVAL] batch:  127 | acc: 81.25%,  total acc: 59.77%   [EVAL] batch:  128 | acc: 68.75%,  total acc: 59.84%   [EVAL] batch:  129 | acc: 56.25%,  total acc: 59.81%   [EVAL] batch:  130 | acc: 93.75%,  total acc: 60.07%   [EVAL] batch:  131 | acc: 75.00%,  total acc: 60.18%   [EVAL] batch:  132 | acc: 56.25%,  total acc: 60.15%   
cur_acc:  ['0.8523', '0.8438', '0.3672', '0.7902', '0.7042', '0.8259', '0.5142', '0.7212']
his_acc:  ['0.8523', '0.8213', '0.7346', '0.6875', '0.6868', '0.6622', '0.6260', '0.6015']
----------END
his_acc mean:  [0.8564 0.8162 0.7588 0.7172 0.6606 0.6482 0.621  0.5959]
