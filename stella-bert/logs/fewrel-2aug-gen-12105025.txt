#############params############
cuda:0
Task=FewRel, 5-shot
Encoding model: bert
pattern=hardprompt
mem=1, margin=0.3, gen=1, gen_num=2
#############params############
--------Round  0
seed:  100
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/train.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/valid.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/test.pkl
Task_order: [7 3 0 5 4 1 6 2]
prepared data!
CurrentTrain: epoch 15, batch     0 | loss: 25.5838887CurrentTrain: epoch 15, batch     1 | loss: 33.2555275CurrentTrain: epoch 15, batch     2 | loss: 27.6423084CurrentTrain: epoch 15, batch     3 | loss: 28.2391955CurrentTrain: epoch 15, batch     4 | loss: 29.1585623CurrentTrain: epoch 15, batch     5 | loss: 37.6916711CurrentTrain: epoch 15, batch     6 | loss: 39.1280050CurrentTrain: epoch 15, batch     7 | loss: 29.2694808CurrentTrain: epoch 15, batch     8 | loss: 21.0319187CurrentTrain: epoch 15, batch     9 | loss: 19.3015828CurrentTrain: epoch 15, batch    10 | loss: 18.0023837CurrentTrain: epoch 15, batch    11 | loss: 25.8393256CurrentTrain: epoch 15, batch    12 | loss: 23.8535138CurrentTrain: epoch 15, batch    13 | loss: 22.6032469CurrentTrain: epoch 15, batch    14 | loss: 23.2327061CurrentTrain: epoch 15, batch    15 | loss: 25.9467006CurrentTrain: epoch 15, batch    16 | loss: 40.2557360CurrentTrain: epoch 15, batch    17 | loss: 30.3929485CurrentTrain: epoch 15, batch    18 | loss: 17.2191175CurrentTrain: epoch 15, batch    19 | loss: 23.9644585CurrentTrain: epoch 15, batch    20 | loss: 26.1922968CurrentTrain: epoch 15, batch    21 | loss: 22.7239131CurrentTrain: epoch 15, batch    22 | loss: 25.9696068CurrentTrain: epoch 15, batch    23 | loss: 24.1039739CurrentTrain: epoch 15, batch    24 | loss: 22.0551140CurrentTrain: epoch 15, batch    25 | loss: 18.8360202CurrentTrain: epoch 15, batch    26 | loss: 25.1307278CurrentTrain: epoch 15, batch    27 | loss: 18.8444188CurrentTrain: epoch 15, batch    28 | loss: 20.0546465CurrentTrain: epoch 15, batch    29 | loss: 29.5970376CurrentTrain: epoch 15, batch    30 | loss: 28.9995757CurrentTrain: epoch 15, batch    31 | loss: 29.9251531CurrentTrain: epoch 15, batch    32 | loss: 17.9921088CurrentTrain: epoch 15, batch    33 | loss: 20.3888003CurrentTrain: epoch 15, batch    34 | loss: 30.8699367CurrentTrain: epoch 15, batch    35 | loss: 17.7349351CurrentTrain: epoch 15, batch    36 | loss: 17.6550921CurrentTrain: epoch 15, batch    37 | loss: 21.7577103CurrentTrain: epoch 15, batch    38 | loss: 20.9709316CurrentTrain: epoch 15, batch    39 | loss: 21.5441431CurrentTrain: epoch 15, batch    40 | loss: 26.7600533CurrentTrain: epoch 15, batch    41 | loss: 24.1605511CurrentTrain: epoch 15, batch    42 | loss: 20.3942147CurrentTrain: epoch 15, batch    43 | loss: 31.2785563CurrentTrain: epoch 15, batch    44 | loss: 18.8222166CurrentTrain: epoch 15, batch    45 | loss: 21.8817905CurrentTrain: epoch 15, batch    46 | loss: 14.1712271CurrentTrain: epoch 15, batch    47 | loss: 17.2756737CurrentTrain: epoch 15, batch    48 | loss: 16.0319679CurrentTrain: epoch 15, batch    49 | loss: 15.7899743CurrentTrain: epoch 15, batch    50 | loss: 16.9763559CurrentTrain: epoch 15, batch    51 | loss: 22.3908324CurrentTrain: epoch 15, batch    52 | loss: 17.8509551CurrentTrain: epoch 15, batch    53 | loss: 28.9748204CurrentTrain: epoch 15, batch    54 | loss: 15.0900882CurrentTrain: epoch 15, batch    55 | loss: 17.6029784CurrentTrain: epoch 15, batch    56 | loss: 20.5441339CurrentTrain: epoch 15, batch    57 | loss: 12.1163113CurrentTrain: epoch 15, batch    58 | loss: 14.3668156CurrentTrain: epoch 15, batch    59 | loss: 34.2170676CurrentTrain: epoch 15, batch    60 | loss: 18.7526818CurrentTrain: epoch 15, batch    61 | loss: 15.2688903CurrentTrain: epoch  7, batch    62 | loss: 21.9648654CurrentTrain: epoch 15, batch     0 | loss: 15.4418820CurrentTrain: epoch 15, batch     1 | loss: 24.7007636CurrentTrain: epoch 15, batch     2 | loss: 16.0259182CurrentTrain: epoch 15, batch     3 | loss: 13.6728735CurrentTrain: epoch 15, batch     4 | loss: 15.4377191CurrentTrain: epoch 15, batch     5 | loss: 14.2432248CurrentTrain: epoch 15, batch     6 | loss: 15.3033991CurrentTrain: epoch 15, batch     7 | loss: 15.9536692CurrentTrain: epoch 15, batch     8 | loss: 21.0059431CurrentTrain: epoch 15, batch     9 | loss: 17.3238465CurrentTrain: epoch 15, batch    10 | loss: 15.0915846CurrentTrain: epoch 15, batch    11 | loss: 15.3738973CurrentTrain: epoch 15, batch    12 | loss: 26.2133760CurrentTrain: epoch 15, batch    13 | loss: 16.9633856CurrentTrain: epoch 15, batch    14 | loss: 22.8087145CurrentTrain: epoch 15, batch    15 | loss: 14.6110320CurrentTrain: epoch 15, batch    16 | loss: 15.3882329CurrentTrain: epoch 15, batch    17 | loss: 24.7265154CurrentTrain: epoch 15, batch    18 | loss: 16.0395847CurrentTrain: epoch 15, batch    19 | loss: 24.0225373CurrentTrain: epoch 15, batch    20 | loss: 17.2563507CurrentTrain: epoch 15, batch    21 | loss: 18.6272027CurrentTrain: epoch 15, batch    22 | loss: 18.0362427CurrentTrain: epoch 15, batch    23 | loss: 17.1754518CurrentTrain: epoch 15, batch    24 | loss: 22.4659281CurrentTrain: epoch 15, batch    25 | loss: 15.4967282CurrentTrain: epoch 15, batch    26 | loss: 13.9057430CurrentTrain: epoch 15, batch    27 | loss: 17.4158068CurrentTrain: epoch 15, batch    28 | loss: 15.3303994CurrentTrain: epoch 15, batch    29 | loss: 16.6682185CurrentTrain: epoch 15, batch    30 | loss: 17.4417626CurrentTrain: epoch 15, batch    31 | loss: 20.1321305CurrentTrain: epoch 15, batch    32 | loss: 15.3219632CurrentTrain: epoch 15, batch    33 | loss: 21.1219537CurrentTrain: epoch 15, batch    34 | loss: 17.3689181CurrentTrain: epoch 15, batch    35 | loss: 18.1080732CurrentTrain: epoch 15, batch    36 | loss: 39.7543314CurrentTrain: epoch 15, batch    37 | loss: 17.8126557CurrentTrain: epoch 15, batch    38 | loss: 12.8147844CurrentTrain: epoch 15, batch    39 | loss: 11.7884675CurrentTrain: epoch 15, batch    40 | loss: 22.5236618CurrentTrain: epoch 15, batch    41 | loss: 13.8704260CurrentTrain: epoch 15, batch    42 | loss: 13.3998886CurrentTrain: epoch 15, batch    43 | loss: 12.1918901CurrentTrain: epoch 15, batch    44 | loss: 16.3966067CurrentTrain: epoch 15, batch    45 | loss: 12.3438976CurrentTrain: epoch 15, batch    46 | loss: 21.5019899CurrentTrain: epoch 15, batch    47 | loss: 19.2096443CurrentTrain: epoch 15, batch    48 | loss: 18.9199620CurrentTrain: epoch 15, batch    49 | loss: 13.2858383CurrentTrain: epoch 15, batch    50 | loss: 23.7242258CurrentTrain: epoch 15, batch    51 | loss: 20.4511146CurrentTrain: epoch 15, batch    52 | loss: 16.0634490CurrentTrain: epoch 15, batch    53 | loss: 21.8114834CurrentTrain: epoch 15, batch    54 | loss: 20.3091310CurrentTrain: epoch 15, batch    55 | loss: 13.2689594CurrentTrain: epoch 15, batch    56 | loss: 17.6611947CurrentTrain: epoch 15, batch    57 | loss: 13.0362061CurrentTrain: epoch 15, batch    58 | loss: 19.2578591CurrentTrain: epoch 15, batch    59 | loss: 18.8473319CurrentTrain: epoch 15, batch    60 | loss: 14.8480163CurrentTrain: epoch 15, batch    61 | loss: 20.5582304CurrentTrain: epoch  7, batch    62 | loss: 18.4179133CurrentTrain: epoch 15, batch     0 | loss: 20.0411837CurrentTrain: epoch 15, batch     1 | loss: 10.6591537CurrentTrain: epoch 15, batch     2 | loss: 22.4998007CurrentTrain: epoch 15, batch     3 | loss: 14.2649968CurrentTrain: epoch 15, batch     4 | loss: 16.1638508CurrentTrain: epoch 15, batch     5 | loss: 21.5731087CurrentTrain: epoch 15, batch     6 | loss: 34.2723438CurrentTrain: epoch 15, batch     7 | loss: 21.5527521CurrentTrain: epoch 15, batch     8 | loss: 15.0374335CurrentTrain: epoch 15, batch     9 | loss: 12.6829964CurrentTrain: epoch 15, batch    10 | loss: 10.4998244CurrentTrain: epoch 15, batch    11 | loss: 18.2691848CurrentTrain: epoch 15, batch    12 | loss: 17.0712879CurrentTrain: epoch 15, batch    13 | loss: 13.7113410CurrentTrain: epoch 15, batch    14 | loss: 19.7536450CurrentTrain: epoch 15, batch    15 | loss: 14.9362669CurrentTrain: epoch 15, batch    16 | loss: 18.7910929CurrentTrain: epoch 15, batch    17 | loss: 11.5715011CurrentTrain: epoch 15, batch    18 | loss: 11.2102819CurrentTrain: epoch 15, batch    19 | loss: 11.3471593CurrentTrain: epoch 15, batch    20 | loss: 11.0275093CurrentTrain: epoch 15, batch    21 | loss: 16.4761564CurrentTrain: epoch 15, batch    22 | loss: 27.4275417CurrentTrain: epoch 15, batch    23 | loss: 13.2881588CurrentTrain: epoch 15, batch    24 | loss: 17.3447278CurrentTrain: epoch 15, batch    25 | loss: 22.4418627CurrentTrain: epoch 15, batch    26 | loss: 14.7217718CurrentTrain: epoch 15, batch    27 | loss: 13.4819583CurrentTrain: epoch 15, batch    28 | loss: 11.7959488CurrentTrain: epoch 15, batch    29 | loss: 17.2290542CurrentTrain: epoch 15, batch    30 | loss: 17.4672491CurrentTrain: epoch 15, batch    31 | loss: 20.6143737CurrentTrain: epoch 15, batch    32 | loss: 16.1334439CurrentTrain: epoch 15, batch    33 | loss: 12.8447328CurrentTrain: epoch 15, batch    34 | loss: 14.8006195CurrentTrain: epoch 15, batch    35 | loss: 13.7564132CurrentTrain: epoch 15, batch    36 | loss: 20.2596470CurrentTrain: epoch 15, batch    37 | loss: 13.4778015CurrentTrain: epoch 15, batch    38 | loss: 14.8104610CurrentTrain: epoch 15, batch    39 | loss: 19.3537314CurrentTrain: epoch 15, batch    40 | loss: 20.0445678CurrentTrain: epoch 15, batch    41 | loss: 17.8795251CurrentTrain: epoch 15, batch    42 | loss: 15.4131920CurrentTrain: epoch 15, batch    43 | loss: 13.5638329CurrentTrain: epoch 15, batch    44 | loss: 17.7700842CurrentTrain: epoch 15, batch    45 | loss: 17.5666762CurrentTrain: epoch 15, batch    46 | loss: 14.7913062CurrentTrain: epoch 15, batch    47 | loss: 9.5350356CurrentTrain: epoch 15, batch    48 | loss: 10.1199727CurrentTrain: epoch 15, batch    49 | loss: 15.2622910CurrentTrain: epoch 15, batch    50 | loss: 18.8237257CurrentTrain: epoch 15, batch    51 | loss: 15.8952372CurrentTrain: epoch 15, batch    52 | loss: 15.3533671CurrentTrain: epoch 15, batch    53 | loss: 20.8970992CurrentTrain: epoch 15, batch    54 | loss: 17.4032607CurrentTrain: epoch 15, batch    55 | loss: 15.2142226CurrentTrain: epoch 15, batch    56 | loss: 13.2879851CurrentTrain: epoch 15, batch    57 | loss: 19.4886384CurrentTrain: epoch 15, batch    58 | loss: 18.6962758CurrentTrain: epoch 15, batch    59 | loss: 17.0539119CurrentTrain: epoch 15, batch    60 | loss: 11.7827790CurrentTrain: epoch 15, batch    61 | loss: 12.9395596CurrentTrain: epoch  7, batch    62 | loss: 16.6273216CurrentTrain: epoch 15, batch     0 | loss: 11.3609678CurrentTrain: epoch 15, batch     1 | loss: 16.7570144CurrentTrain: epoch 15, batch     2 | loss: 40.4841595CurrentTrain: epoch 15, batch     3 | loss: 22.2156235CurrentTrain: epoch 15, batch     4 | loss: 20.4029358CurrentTrain: epoch 15, batch     5 | loss: 18.8637302CurrentTrain: epoch 15, batch     6 | loss: 20.3152926CurrentTrain: epoch 15, batch     7 | loss: 9.9684268CurrentTrain: epoch 15, batch     8 | loss: 14.6104187CurrentTrain: epoch 15, batch     9 | loss: 12.5125888CurrentTrain: epoch 15, batch    10 | loss: 13.9026368CurrentTrain: epoch 15, batch    11 | loss: 14.4535145CurrentTrain: epoch 15, batch    12 | loss: 14.3253652CurrentTrain: epoch 15, batch    13 | loss: 16.4256277CurrentTrain: epoch 15, batch    14 | loss: 12.5088070CurrentTrain: epoch 15, batch    15 | loss: 10.1696925CurrentTrain: epoch 15, batch    16 | loss: 13.4547880CurrentTrain: epoch 15, batch    17 | loss: 12.1367840CurrentTrain: epoch 15, batch    18 | loss: 19.2703471CurrentTrain: epoch 15, batch    19 | loss: 12.1465302CurrentTrain: epoch 15, batch    20 | loss: 11.1995468CurrentTrain: epoch 15, batch    21 | loss: 15.7198373CurrentTrain: epoch 15, batch    22 | loss: 18.7700886CurrentTrain: epoch 15, batch    23 | loss: 13.0826408CurrentTrain: epoch 15, batch    24 | loss: 13.0235227CurrentTrain: epoch 15, batch    25 | loss: 19.5819150CurrentTrain: epoch 15, batch    26 | loss: 13.2374866CurrentTrain: epoch 15, batch    27 | loss: 12.0560714CurrentTrain: epoch 15, batch    28 | loss: 13.0739148CurrentTrain: epoch 15, batch    29 | loss: 14.0293523CurrentTrain: epoch 15, batch    30 | loss: 17.1647088CurrentTrain: epoch 15, batch    31 | loss: 20.8201086CurrentTrain: epoch 15, batch    32 | loss: 13.0563290CurrentTrain: epoch 15, batch    33 | loss: 14.1534330CurrentTrain: epoch 15, batch    34 | loss: 23.6669254CurrentTrain: epoch 15, batch    35 | loss: 14.7035635CurrentTrain: epoch 15, batch    36 | loss: 22.9886461CurrentTrain: epoch 15, batch    37 | loss: 38.1271981CurrentTrain: epoch 15, batch    38 | loss: 23.3125621CurrentTrain: epoch 15, batch    39 | loss: 13.6776184CurrentTrain: epoch 15, batch    40 | loss: 10.5153768CurrentTrain: epoch 15, batch    41 | loss: 20.5753152CurrentTrain: epoch 15, batch    42 | loss: 21.9617758CurrentTrain: epoch 15, batch    43 | loss: 14.9823355CurrentTrain: epoch 15, batch    44 | loss: 12.7985976CurrentTrain: epoch 15, batch    45 | loss: 13.1356998CurrentTrain: epoch 15, batch    46 | loss: 24.1598923CurrentTrain: epoch 15, batch    47 | loss: 10.7371740CurrentTrain: epoch 15, batch    48 | loss: 26.0086146CurrentTrain: epoch 15, batch    49 | loss: 14.9911982CurrentTrain: epoch 15, batch    50 | loss: 10.5595463CurrentTrain: epoch 15, batch    51 | loss: 12.6244545CurrentTrain: epoch 15, batch    52 | loss: 11.2187287CurrentTrain: epoch 15, batch    53 | loss: 23.7225123CurrentTrain: epoch 15, batch    54 | loss: 9.9143314CurrentTrain: epoch 15, batch    55 | loss: 11.0043762CurrentTrain: epoch 15, batch    56 | loss: 13.9344575CurrentTrain: epoch 15, batch    57 | loss: 13.1400965CurrentTrain: epoch 15, batch    58 | loss: 22.6030735CurrentTrain: epoch 15, batch    59 | loss: 10.7184094CurrentTrain: epoch 15, batch    60 | loss: 15.0840006CurrentTrain: epoch 15, batch    61 | loss: 10.9220561CurrentTrain: epoch  7, batch    62 | loss: 14.3565341CurrentTrain: epoch 15, batch     0 | loss: 14.6810041CurrentTrain: epoch 15, batch     1 | loss: 13.1440352CurrentTrain: epoch 15, batch     2 | loss: 13.4699803CurrentTrain: epoch 15, batch     3 | loss: 12.5600844CurrentTrain: epoch 15, batch     4 | loss: 16.1855583CurrentTrain: epoch 15, batch     5 | loss: 23.9069597CurrentTrain: epoch 15, batch     6 | loss: 14.5482618CurrentTrain: epoch 15, batch     7 | loss: 9.5374082CurrentTrain: epoch 15, batch     8 | loss: 13.6430755CurrentTrain: epoch 15, batch     9 | loss: 11.5283885CurrentTrain: epoch 15, batch    10 | loss: 13.3520702CurrentTrain: epoch 15, batch    11 | loss: 17.4816286CurrentTrain: epoch 15, batch    12 | loss: 12.9702014CurrentTrain: epoch 15, batch    13 | loss: 13.9591874CurrentTrain: epoch 15, batch    14 | loss: 11.2416884CurrentTrain: epoch 15, batch    15 | loss: 13.0145838CurrentTrain: epoch 15, batch    16 | loss: 10.7880721CurrentTrain: epoch 15, batch    17 | loss: 14.8454805CurrentTrain: epoch 15, batch    18 | loss: 11.0863915CurrentTrain: epoch 15, batch    19 | loss: 10.4769582CurrentTrain: epoch 15, batch    20 | loss: 10.8768938CurrentTrain: epoch 15, batch    21 | loss: 13.0639218CurrentTrain: epoch 15, batch    22 | loss: 11.4736458CurrentTrain: epoch 15, batch    23 | loss: 17.7463313CurrentTrain: epoch 15, batch    24 | loss: 12.0298155CurrentTrain: epoch 15, batch    25 | loss: 14.9295369CurrentTrain: epoch 15, batch    26 | loss: 12.6959358CurrentTrain: epoch 15, batch    27 | loss: 11.2401057CurrentTrain: epoch 15, batch    28 | loss: 15.2468338CurrentTrain: epoch 15, batch    29 | loss: 17.8454840CurrentTrain: epoch 15, batch    30 | loss: 24.4586232CurrentTrain: epoch 15, batch    31 | loss: 18.1795181CurrentTrain: epoch 15, batch    32 | loss: 12.5840451CurrentTrain: epoch 15, batch    33 | loss: 14.5901799CurrentTrain: epoch 15, batch    34 | loss: 13.1088441CurrentTrain: epoch 15, batch    35 | loss: 13.9701527CurrentTrain: epoch 15, batch    36 | loss: 11.0933802CurrentTrain: epoch 15, batch    37 | loss: 23.8457022CurrentTrain: epoch 15, batch    38 | loss: 16.0916593CurrentTrain: epoch 15, batch    39 | loss: 12.5277943CurrentTrain: epoch 15, batch    40 | loss: 11.2213539CurrentTrain: epoch 15, batch    41 | loss: 9.6076628CurrentTrain: epoch 15, batch    42 | loss: 15.5679173CurrentTrain: epoch 15, batch    43 | loss: 21.2499014CurrentTrain: epoch 15, batch    44 | loss: 11.5244350CurrentTrain: epoch 15, batch    45 | loss: 14.3490819CurrentTrain: epoch 15, batch    46 | loss: 10.3637825CurrentTrain: epoch 15, batch    47 | loss: 20.0364104CurrentTrain: epoch 15, batch    48 | loss: 13.4493939CurrentTrain: epoch 15, batch    49 | loss: 14.7593062CurrentTrain: epoch 15, batch    50 | loss: 14.4773609CurrentTrain: epoch 15, batch    51 | loss: 12.3414897CurrentTrain: epoch 15, batch    52 | loss: 12.4600106CurrentTrain: epoch 15, batch    53 | loss: 21.4102077CurrentTrain: epoch 15, batch    54 | loss: 15.4307023CurrentTrain: epoch 15, batch    55 | loss: 10.9398313CurrentTrain: epoch 15, batch    56 | loss: 13.8715479CurrentTrain: epoch 15, batch    57 | loss: 13.5311724CurrentTrain: epoch 15, batch    58 | loss: 13.2548916CurrentTrain: epoch 15, batch    59 | loss: 13.6466104CurrentTrain: epoch 15, batch    60 | loss: 14.5992711CurrentTrain: epoch 15, batch    61 | loss: 15.3989103CurrentTrain: epoch  7, batch    62 | loss: 8.7412300CurrentTrain: epoch 15, batch     0 | loss: 8.4308477CurrentTrain: epoch 15, batch     1 | loss: 11.0696595CurrentTrain: epoch 15, batch     2 | loss: 12.1840984CurrentTrain: epoch 15, batch     3 | loss: 13.7306658CurrentTrain: epoch 15, batch     4 | loss: 21.7495625CurrentTrain: epoch 15, batch     5 | loss: 16.4865669CurrentTrain: epoch 15, batch     6 | loss: 9.8278721CurrentTrain: epoch 15, batch     7 | loss: 13.9814032CurrentTrain: epoch 15, batch     8 | loss: 11.6552465CurrentTrain: epoch 15, batch     9 | loss: 10.9515287CurrentTrain: epoch 15, batch    10 | loss: 14.8455806CurrentTrain: epoch 15, batch    11 | loss: 20.1981808CurrentTrain: epoch 15, batch    12 | loss: 30.6167418CurrentTrain: epoch 15, batch    13 | loss: 16.4580517CurrentTrain: epoch 15, batch    14 | loss: 19.0225473CurrentTrain: epoch 15, batch    15 | loss: 20.0244609CurrentTrain: epoch 15, batch    16 | loss: 12.5411931CurrentTrain: epoch 15, batch    17 | loss: 19.5017176CurrentTrain: epoch 15, batch    18 | loss: 16.7018530CurrentTrain: epoch 15, batch    19 | loss: 12.9158820CurrentTrain: epoch 15, batch    20 | loss: 13.3615658CurrentTrain: epoch 15, batch    21 | loss: 19.4769677CurrentTrain: epoch 15, batch    22 | loss: 22.5401961CurrentTrain: epoch 15, batch    23 | loss: 16.3620059CurrentTrain: epoch 15, batch    24 | loss: 19.4081703CurrentTrain: epoch 15, batch    25 | loss: 25.9038197CurrentTrain: epoch 15, batch    26 | loss: 13.6828282CurrentTrain: epoch 15, batch    27 | loss: 9.5475722CurrentTrain: epoch 15, batch    28 | loss: 18.9816112CurrentTrain: epoch 15, batch    29 | loss: 8.2028466CurrentTrain: epoch 15, batch    30 | loss: 17.3904207CurrentTrain: epoch 15, batch    31 | loss: 21.9463442CurrentTrain: epoch 15, batch    32 | loss: 15.1865921CurrentTrain: epoch 15, batch    33 | loss: 14.1993740CurrentTrain: epoch 15, batch    34 | loss: 14.8098723CurrentTrain: epoch 15, batch    35 | loss: 12.0582272CurrentTrain: epoch 15, batch    36 | loss: 8.6356161CurrentTrain: epoch 15, batch    37 | loss: 16.9889841CurrentTrain: epoch 15, batch    38 | loss: 15.2461119CurrentTrain: epoch 15, batch    39 | loss: 12.9481079CurrentTrain: epoch 15, batch    40 | loss: 18.5183849CurrentTrain: epoch 15, batch    41 | loss: 15.2530040CurrentTrain: epoch 15, batch    42 | loss: 13.1042964CurrentTrain: epoch 15, batch    43 | loss: 11.0944320CurrentTrain: epoch 15, batch    44 | loss: 12.5656959CurrentTrain: epoch 15, batch    45 | loss: 10.4493798CurrentTrain: epoch 15, batch    46 | loss: 19.0649453CurrentTrain: epoch 15, batch    47 | loss: 12.8846551CurrentTrain: epoch 15, batch    48 | loss: 18.7139420CurrentTrain: epoch 15, batch    49 | loss: 10.9004418CurrentTrain: epoch 15, batch    50 | loss: 14.3641770CurrentTrain: epoch 15, batch    51 | loss: 13.7191077CurrentTrain: epoch 15, batch    52 | loss: 13.1028962CurrentTrain: epoch 15, batch    53 | loss: 13.7992824CurrentTrain: epoch 15, batch    54 | loss: 19.0505952CurrentTrain: epoch 15, batch    55 | loss: 18.8817055CurrentTrain: epoch 15, batch    56 | loss: 23.6659251CurrentTrain: epoch 15, batch    57 | loss: 16.1057428CurrentTrain: epoch 15, batch    58 | loss: 13.4752049CurrentTrain: epoch 15, batch    59 | loss: 13.8013666CurrentTrain: epoch 15, batch    60 | loss: 13.1477293CurrentTrain: epoch 15, batch    61 | loss: 11.8576994CurrentTrain: epoch  7, batch    62 | loss: 4.6558071CurrentTrain: epoch 15, batch     0 | loss: 13.1307855CurrentTrain: epoch 15, batch     1 | loss: 13.6425887CurrentTrain: epoch 15, batch     2 | loss: 30.3416869CurrentTrain: epoch 15, batch     3 | loss: 16.0082651CurrentTrain: epoch 15, batch     4 | loss: 24.6535328CurrentTrain: epoch 15, batch     5 | loss: 14.0596271CurrentTrain: epoch 15, batch     6 | loss: 23.5757745CurrentTrain: epoch 15, batch     7 | loss: 10.3064108CurrentTrain: epoch 15, batch     8 | loss: 14.5792681CurrentTrain: epoch 15, batch     9 | loss: 8.5588755CurrentTrain: epoch 15, batch    10 | loss: 10.5671168CurrentTrain: epoch 15, batch    11 | loss: 19.0466920CurrentTrain: epoch 15, batch    12 | loss: 10.6566090CurrentTrain: epoch 15, batch    13 | loss: 25.2520050CurrentTrain: epoch 15, batch    14 | loss: 11.7835160CurrentTrain: epoch 15, batch    15 | loss: 19.2657609CurrentTrain: epoch 15, batch    16 | loss: 28.5188083CurrentTrain: epoch 15, batch    17 | loss: 18.3491475CurrentTrain: epoch 15, batch    18 | loss: 17.8002683CurrentTrain: epoch 15, batch    19 | loss: 16.6225005CurrentTrain: epoch 15, batch    20 | loss: 13.2381958CurrentTrain: epoch 15, batch    21 | loss: 10.0428737CurrentTrain: epoch 15, batch    22 | loss: 16.5639169CurrentTrain: epoch 15, batch    23 | loss: 11.7422930CurrentTrain: epoch 15, batch    24 | loss: 12.9718611CurrentTrain: epoch 15, batch    25 | loss: 23.0707398CurrentTrain: epoch 15, batch    26 | loss: 14.7551996CurrentTrain: epoch 15, batch    27 | loss: 14.0019350CurrentTrain: epoch 15, batch    28 | loss: 13.7298430CurrentTrain: epoch 15, batch    29 | loss: 15.0376791CurrentTrain: epoch 15, batch    30 | loss: 13.8593571CurrentTrain: epoch 15, batch    31 | loss: 11.7159773CurrentTrain: epoch 15, batch    32 | loss: 12.4759733CurrentTrain: epoch 15, batch    33 | loss: 10.0314091CurrentTrain: epoch 15, batch    34 | loss: 18.1896334CurrentTrain: epoch 15, batch    35 | loss: 9.3502247CurrentTrain: epoch 15, batch    36 | loss: 9.7134304CurrentTrain: epoch 15, batch    37 | loss: 13.8868349CurrentTrain: epoch 15, batch    38 | loss: 11.6498683CurrentTrain: epoch 15, batch    39 | loss: 10.9045715CurrentTrain: epoch 15, batch    40 | loss: 17.2768882CurrentTrain: epoch 15, batch    41 | loss: 11.4320050CurrentTrain: epoch 15, batch    42 | loss: 15.4113723CurrentTrain: epoch 15, batch    43 | loss: 8.6724733CurrentTrain: epoch 15, batch    44 | loss: 16.2477191CurrentTrain: epoch 15, batch    45 | loss: 14.7162860CurrentTrain: epoch 15, batch    46 | loss: 10.5256516CurrentTrain: epoch 15, batch    47 | loss: 11.1884642CurrentTrain: epoch 15, batch    48 | loss: 11.2699414CurrentTrain: epoch 15, batch    49 | loss: 12.7000625CurrentTrain: epoch 15, batch    50 | loss: 7.7877911CurrentTrain: epoch 15, batch    51 | loss: 17.9780794CurrentTrain: epoch 15, batch    52 | loss: 10.7073861CurrentTrain: epoch 15, batch    53 | loss: 27.6793329CurrentTrain: epoch 15, batch    54 | loss: 13.6613347CurrentTrain: epoch 15, batch    55 | loss: 10.4405165CurrentTrain: epoch 15, batch    56 | loss: 21.6321811CurrentTrain: epoch 15, batch    57 | loss: 19.9016977CurrentTrain: epoch 15, batch    58 | loss: 9.6575614CurrentTrain: epoch 15, batch    59 | loss: 11.4856723CurrentTrain: epoch 15, batch    60 | loss: 14.6555946CurrentTrain: epoch 15, batch    61 | loss: 11.5925743CurrentTrain: epoch  7, batch    62 | loss: 11.8029647CurrentTrain: epoch 15, batch     0 | loss: 18.2498591CurrentTrain: epoch 15, batch     1 | loss: 15.6034606CurrentTrain: epoch 15, batch     2 | loss: 9.7068977CurrentTrain: epoch 15, batch     3 | loss: 14.3170165CurrentTrain: epoch 15, batch     4 | loss: 21.4114197CurrentTrain: epoch 15, batch     5 | loss: 13.1655698CurrentTrain: epoch 15, batch     6 | loss: 12.8699468CurrentTrain: epoch 15, batch     7 | loss: 9.7398065CurrentTrain: epoch 15, batch     8 | loss: 33.5276987CurrentTrain: epoch 15, batch     9 | loss: 20.2075079CurrentTrain: epoch 15, batch    10 | loss: 20.4849476CurrentTrain: epoch 15, batch    11 | loss: 8.9510285CurrentTrain: epoch 15, batch    12 | loss: 16.0274086CurrentTrain: epoch 15, batch    13 | loss: 11.1605449CurrentTrain: epoch 15, batch    14 | loss: 16.1859531CurrentTrain: epoch 15, batch    15 | loss: 9.6605048CurrentTrain: epoch 15, batch    16 | loss: 16.7929973CurrentTrain: epoch 15, batch    17 | loss: 18.0102102CurrentTrain: epoch 15, batch    18 | loss: 19.0647500CurrentTrain: epoch 15, batch    19 | loss: 13.7007946CurrentTrain: epoch 15, batch    20 | loss: 11.4860104CurrentTrain: epoch 15, batch    21 | loss: 17.8255319CurrentTrain: epoch 15, batch    22 | loss: 16.3845915CurrentTrain: epoch 15, batch    23 | loss: 16.1230865CurrentTrain: epoch 15, batch    24 | loss: 13.1579609CurrentTrain: epoch 15, batch    25 | loss: 11.7632024CurrentTrain: epoch 15, batch    26 | loss: 13.8379532CurrentTrain: epoch 15, batch    27 | loss: 14.0422518CurrentTrain: epoch 15, batch    28 | loss: 15.1962860CurrentTrain: epoch 15, batch    29 | loss: 20.2577647CurrentTrain: epoch 15, batch    30 | loss: 14.0720822CurrentTrain: epoch 15, batch    31 | loss: 10.6647171CurrentTrain: epoch 15, batch    32 | loss: 15.0977002CurrentTrain: epoch 15, batch    33 | loss: 11.6571894CurrentTrain: epoch 15, batch    34 | loss: 22.8208370CurrentTrain: epoch 15, batch    35 | loss: 11.2784578CurrentTrain: epoch 15, batch    36 | loss: 11.8413009CurrentTrain: epoch 15, batch    37 | loss: 14.6088737CurrentTrain: epoch 15, batch    38 | loss: 14.6232636CurrentTrain: epoch 15, batch    39 | loss: 12.2170452CurrentTrain: epoch 15, batch    40 | loss: 8.4098790CurrentTrain: epoch 15, batch    41 | loss: 19.4422176CurrentTrain: epoch 15, batch    42 | loss: 17.9040722CurrentTrain: epoch 15, batch    43 | loss: 13.5721189CurrentTrain: epoch 15, batch    44 | loss: 16.4124360CurrentTrain: epoch 15, batch    45 | loss: 16.0646289CurrentTrain: epoch 15, batch    46 | loss: 15.4105916CurrentTrain: epoch 15, batch    47 | loss: 9.5492425CurrentTrain: epoch 15, batch    48 | loss: 13.3235299CurrentTrain: epoch 15, batch    49 | loss: 8.8889415CurrentTrain: epoch 15, batch    50 | loss: 13.8352906CurrentTrain: epoch 15, batch    51 | loss: 17.3823779CurrentTrain: epoch 15, batch    52 | loss: 9.6400785CurrentTrain: epoch 15, batch    53 | loss: 15.4640998CurrentTrain: epoch 15, batch    54 | loss: 17.9476293CurrentTrain: epoch 15, batch    55 | loss: 11.7135090CurrentTrain: epoch 15, batch    56 | loss: 12.0956025CurrentTrain: epoch 15, batch    57 | loss: 19.4254598CurrentTrain: epoch 15, batch    58 | loss: 14.0080617CurrentTrain: epoch 15, batch    59 | loss: 9.2156467CurrentTrain: epoch 15, batch    60 | loss: 14.2823016CurrentTrain: epoch 15, batch    61 | loss: 9.1876291CurrentTrain: epoch  7, batch    62 | loss: 13.3363296CurrentTrain: epoch 15, batch     0 | loss: 10.2128166CurrentTrain: epoch 15, batch     1 | loss: 13.3625536CurrentTrain: epoch 15, batch     2 | loss: 24.3260449CurrentTrain: epoch 15, batch     3 | loss: 7.9031321CurrentTrain: epoch 15, batch     4 | loss: 13.8935539CurrentTrain: epoch 15, batch     5 | loss: 14.6660522CurrentTrain: epoch 15, batch     6 | loss: 7.5872160CurrentTrain: epoch 15, batch     7 | loss: 11.6142982CurrentTrain: epoch 15, batch     8 | loss: 19.6017797CurrentTrain: epoch 15, batch     9 | loss: 16.6991599CurrentTrain: epoch 15, batch    10 | loss: 11.6985783CurrentTrain: epoch 15, batch    11 | loss: 24.0124562CurrentTrain: epoch 15, batch    12 | loss: 16.1264104CurrentTrain: epoch 15, batch    13 | loss: 11.5711493CurrentTrain: epoch 15, batch    14 | loss: 21.5513171CurrentTrain: epoch 15, batch    15 | loss: 10.9072341CurrentTrain: epoch 15, batch    16 | loss: 15.2449354CurrentTrain: epoch 15, batch    17 | loss: 13.9330632CurrentTrain: epoch 15, batch    18 | loss: 10.0865653CurrentTrain: epoch 15, batch    19 | loss: 12.8875232CurrentTrain: epoch 15, batch    20 | loss: 22.2714728CurrentTrain: epoch 15, batch    21 | loss: 13.3621918CurrentTrain: epoch 15, batch    22 | loss: 18.8432928CurrentTrain: epoch 15, batch    23 | loss: 13.7182768CurrentTrain: epoch 15, batch    24 | loss: 19.0658928CurrentTrain: epoch 15, batch    25 | loss: 13.3780755CurrentTrain: epoch 15, batch    26 | loss: 13.5786340CurrentTrain: epoch 15, batch    27 | loss: 14.2199424CurrentTrain: epoch 15, batch    28 | loss: 10.6372198CurrentTrain: epoch 15, batch    29 | loss: 16.4543693CurrentTrain: epoch 15, batch    30 | loss: 12.0701683CurrentTrain: epoch 15, batch    31 | loss: 13.8710025CurrentTrain: epoch 15, batch    32 | loss: 14.3594296CurrentTrain: epoch 15, batch    33 | loss: 8.3880898CurrentTrain: epoch 15, batch    34 | loss: 17.2204291CurrentTrain: epoch 15, batch    35 | loss: 16.1109927CurrentTrain: epoch 15, batch    36 | loss: 16.8590850CurrentTrain: epoch 15, batch    37 | loss: 11.4802859CurrentTrain: epoch 15, batch    38 | loss: 10.1583534CurrentTrain: epoch 15, batch    39 | loss: 28.6345548CurrentTrain: epoch 15, batch    40 | loss: 14.3096347CurrentTrain: epoch 15, batch    41 | loss: 10.7481186CurrentTrain: epoch 15, batch    42 | loss: 20.0552126CurrentTrain: epoch 15, batch    43 | loss: 9.5768133CurrentTrain: epoch 15, batch    44 | loss: 22.3678955CurrentTrain: epoch 15, batch    45 | loss: 10.8997789CurrentTrain: epoch 15, batch    46 | loss: 12.2813895CurrentTrain: epoch 15, batch    47 | loss: 18.3799509CurrentTrain: epoch 15, batch    48 | loss: 17.2112157CurrentTrain: epoch 15, batch    49 | loss: 8.9250977CurrentTrain: epoch 15, batch    50 | loss: 9.0362136CurrentTrain: epoch 15, batch    51 | loss: 13.6628322CurrentTrain: epoch 15, batch    52 | loss: 18.1409924CurrentTrain: epoch 15, batch    53 | loss: 23.6819005CurrentTrain: epoch 15, batch    54 | loss: 15.5791005CurrentTrain: epoch 15, batch    55 | loss: 11.1373140CurrentTrain: epoch 15, batch    56 | loss: 8.3519595CurrentTrain: epoch 15, batch    57 | loss: 18.4858114CurrentTrain: epoch 15, batch    58 | loss: 10.2177661CurrentTrain: epoch 15, batch    59 | loss: 11.8434467CurrentTrain: epoch 15, batch    60 | loss: 10.8630486CurrentTrain: epoch 15, batch    61 | loss: 14.6468882CurrentTrain: epoch  7, batch    62 | loss: 26.4077607CurrentTrain: epoch 15, batch     0 | loss: 13.1272150CurrentTrain: epoch 15, batch     1 | loss: 12.6010158CurrentTrain: epoch 15, batch     2 | loss: 11.8531248CurrentTrain: epoch 15, batch     3 | loss: 13.6103418CurrentTrain: epoch 15, batch     4 | loss: 10.8722180CurrentTrain: epoch 15, batch     5 | loss: 11.7771165CurrentTrain: epoch 15, batch     6 | loss: 18.8872976CurrentTrain: epoch 15, batch     7 | loss: 18.4700863CurrentTrain: epoch 15, batch     8 | loss: 10.6477830CurrentTrain: epoch 15, batch     9 | loss: 8.3385035CurrentTrain: epoch 15, batch    10 | loss: 13.7474326CurrentTrain: epoch 15, batch    11 | loss: 14.5653410CurrentTrain: epoch 15, batch    12 | loss: 9.4840535CurrentTrain: epoch 15, batch    13 | loss: 12.0074111CurrentTrain: epoch 15, batch    14 | loss: 11.3900245CurrentTrain: epoch 15, batch    15 | loss: 11.0254538CurrentTrain: epoch 15, batch    16 | loss: 15.9807449CurrentTrain: epoch 15, batch    17 | loss: 10.1671825CurrentTrain: epoch 15, batch    18 | loss: 9.5094327CurrentTrain: epoch 15, batch    19 | loss: 16.2250168CurrentTrain: epoch 15, batch    20 | loss: 20.8039512CurrentTrain: epoch 15, batch    21 | loss: 14.3571246CurrentTrain: epoch 15, batch    22 | loss: 18.1580820CurrentTrain: epoch 15, batch    23 | loss: 7.6812553CurrentTrain: epoch 15, batch    24 | loss: 14.9011865CurrentTrain: epoch 15, batch    25 | loss: 12.1770007CurrentTrain: epoch 15, batch    26 | loss: 12.4394390CurrentTrain: epoch 15, batch    27 | loss: 19.1663003CurrentTrain: epoch 15, batch    28 | loss: 34.5785087CurrentTrain: epoch 15, batch    29 | loss: 9.8953570CurrentTrain: epoch 15, batch    30 | loss: 10.6991007CurrentTrain: epoch 15, batch    31 | loss: 14.9345160CurrentTrain: epoch 15, batch    32 | loss: 13.6099032CurrentTrain: epoch 15, batch    33 | loss: 7.6780473CurrentTrain: epoch 15, batch    34 | loss: 9.9398769CurrentTrain: epoch 15, batch    35 | loss: 10.7927460CurrentTrain: epoch 15, batch    36 | loss: 13.4698125CurrentTrain: epoch 15, batch    37 | loss: 14.3027060CurrentTrain: epoch 15, batch    38 | loss: 9.9309655CurrentTrain: epoch 15, batch    39 | loss: 21.3701343CurrentTrain: epoch 15, batch    40 | loss: 36.1715979CurrentTrain: epoch 15, batch    41 | loss: 11.9073527CurrentTrain: epoch 15, batch    42 | loss: 17.2956554CurrentTrain: epoch 15, batch    43 | loss: 9.2545123CurrentTrain: epoch 15, batch    44 | loss: 18.0500844CurrentTrain: epoch 15, batch    45 | loss: 25.8434042CurrentTrain: epoch 15, batch    46 | loss: 11.6096992CurrentTrain: epoch 15, batch    47 | loss: 9.1220540CurrentTrain: epoch 15, batch    48 | loss: 11.4956503CurrentTrain: epoch 15, batch    49 | loss: 12.2836260CurrentTrain: epoch 15, batch    50 | loss: 16.8234137CurrentTrain: epoch 15, batch    51 | loss: 10.7478047CurrentTrain: epoch 15, batch    52 | loss: 17.4248489CurrentTrain: epoch 15, batch    53 | loss: 22.7688906CurrentTrain: epoch 15, batch    54 | loss: 13.3928445CurrentTrain: epoch 15, batch    55 | loss: 13.1672627CurrentTrain: epoch 15, batch    56 | loss: 19.1448412CurrentTrain: epoch 15, batch    57 | loss: 11.2021046CurrentTrain: epoch 15, batch    58 | loss: 16.9570549CurrentTrain: epoch 15, batch    59 | loss: 13.6226287CurrentTrain: epoch 15, batch    60 | loss: 13.6551129CurrentTrain: epoch 15, batch    61 | loss: 13.5612674CurrentTrain: epoch  7, batch    62 | loss: 8.3503165
#### movement ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "movement" means literary, artistic, scientific or philosophical movement or scene associated with this person or work.
Here is an example:
Relation: movement
Context: the large workshops of peter paul rubens and jacob jordaens , and the influence of anthony van dyck , made antwerp the center of the flemish baroque .
Head Entity: jacob jordaens
Tail Entity: flemish baroque
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "movement":
Relation: movement  
Context: The emergence of surrealism in the early 20th century was significantly influenced by the works of Salvador Dalí and André Breton, who sought to challenge conventional perceptions of reality.  
Head Entity: Salvador Dalí  
Tail Entity: surrealism  

Relation: movement  
Context: The rise of the Impressionist movement in France was marked by the innovative techniques of artists like Claude Monet and Pierre-Auguste Renoir, who focused on capturing light and its changing qualities.  
Head Entity: Claude Monet  
Tail Entity: Impressionist movement  
#### located in the administrative territorial entity ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "located in the administrative territorial entity" means the item is located on the territory of the following administrative entity.
Here is an example:
Relation: located in the administrative territorial entity
Context: lyons hill is a restored village , and former parish with church , now part of the community of ardclough in north county kildare .
Head Entity: ardclough
Tail Entity: county kildare
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "located in the administrative territorial entity":
Relation: located in the administrative territorial entity  
Context: The city of San Francisco is known for its iconic Golden Gate Bridge and is part of the larger San Francisco County in California.  
Head Entity: San Francisco  
Tail Entity: San Francisco County  

Relation: located in the administrative territorial entity  
Context: The village of Haverfordwest serves as the county town of Pembrokeshire, situated in the picturesque region of Wales.  
Head Entity: Haverfordwest  
Tail Entity: Pembrokeshire  
#### head of government ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "head of government" means head of the executive power of this town, city, municipality, state, country, or other governmental body.
Here is an example:
Relation: head of government
Context: on february 10 , 2010 , stefan mappus was elected minister - president of baden - württemberg , and asked that all ministers continue their roles .
Head Entity: baden - württemberg
Tail Entity: stefan mappus
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "head of government":
Relation: head of government  
Context: in 2021, jacinda ardern was re-elected as the prime minister of new zealand, leading the labor party to a significant victory in the general elections.  
Head Entity: new zealand  
Tail Entity: jacinda ardern  

Relation: head of government  
Context: during the annual summit, justin trudeau addressed the nation as the prime minister of canada, outlining his government's plans for the upcoming year.  
Head Entity: canada  
Tail Entity: justin trudeau  
#### member of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "member of" means organization, club or musical group to which the subject belongs.
Here is an example:
Relation: member of
Context: hank henshaw became a member of the sinestro corps during the sinestro corps war .
Head Entity: hank henshaw
Tail Entity: sinestro corps
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "member of":
Relation: member of  
Context: after years of dedication, she was finally recognized as a member of the national ballet company.  
Head Entity: she  
Tail Entity: national ballet company  

Relation: member of  
Context: the renowned scientist was proud to be a member of the international research consortium focused on climate change.  
Head Entity: the renowned scientist  
Tail Entity: international research consortium  
#### crosses ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "crosses" means obstacle which this bridge crosses over or this tunnel goes under.
Here is an example:
Relation: crosses
Context: the bosideng bridge ( ) is a highway bridge over the yangtze river in hejiang county , sichuan , china .
Head Entity: bosideng bridge
Tail Entity: yangtze river
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "crosses":
Relation: crosses  
Context: the golden gate bridge ( ) spans the golden gate strait, connecting san francisco to marin county in california.  
Head Entity: golden gate bridge  
Tail Entity: golden gate strait  

Relation: crosses  
Context: the channel tunnel ( ) runs beneath the english channel, linking the united kingdom with mainland europe.  
Head Entity: channel tunnel  
Tail Entity: english channel  
#### tributary ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "tributary" means watercourse that flows into an other one.
Here is an example:
Relation: tributary
Context: the municipality contains the lower reaches of the maracanã river , which is joined by the caripi river before widening into maracanã bay and emptying into the atlantic ocean beside maiandeua island .
Head Entity: maracanã river
Tail Entity: caripi river
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "tributary":
Relation: tributary  
Context: The Amazon River, one of the largest rivers in the world, receives numerous tributaries, including the Madeira River, which significantly contributes to its flow.  
Head Entity: Amazon River  
Tail Entity: Madeira River  

Relation: tributary  
Context: The Mississippi River is fed by many smaller rivers, such as the Missouri River, which plays a crucial role in its overall water volume.  
Head Entity: Mississippi River  
Tail Entity: Missouri River  
#### position held ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "position held" means subject currently or formerly holds the object position or public office.
Here is an example:
Relation: position held
Context: from june 2010 to february 2012 , nyroos served as foreign policy advisor to the president of the republic tarja halonen and a member of the cabinet .
Head Entity: tarja halonen
Tail Entity: president of the republic
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "position held":
Relation: position held  
Context: in 2015, john smith was appointed as the chief executive officer of tech innovations inc., leading the company through a significant growth phase.  
Head Entity: john smith  
Tail Entity: chief executive officer  

Relation: position held  
Context: during her tenure from 2018 to 2020, dr. emily johnson served as the director of research at the national health institute, overseeing numerous groundbreaking studies.  
Head Entity: dr. emily johnson  
Tail Entity: director of research  
#### developer ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "developer" means organization or person that developed the item.
Here is an example:
Relation: developer
Context: a headcrab is a fictional alien parasitoid found in the " half - life " video game series created by valve software .
Head Entity: half - life
Tail Entity: valve software
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "developer":
Relation: developer  
Context: The popular game "The Witcher 3: Wild Hunt" was developed by CD Projekt Red, a well-known Polish video game developer.  
Head Entity: The Witcher 3: Wild Hunt  
Tail Entity: CD Projekt Red  

Relation: developer  
Context: The innovative mobile game "Angry Birds" was created by Rovio Entertainment, a Finnish company that specializes in game development.  
Head Entity: Angry Birds  
Tail Entity: Rovio Entertainment  
#### location of formation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "location of formation" means location where a group or organization was formed.
Here is an example:
Relation: location of formation
Context: by 2009 artists such as electronics musician pocket and kites with lights had signed to the label , with the fantastiques , a band from minsk , belarus , following suit .
Head Entity: the fantastiques
Tail Entity: minsk
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "location of formation":
Relation: location of formation  
Context: The famous rock band Coldplay was formed in London, England, where the members met while studying at University College London.  
Head Entity: Coldplay  
Tail Entity: London  

Relation: location of formation  
Context: The tech startup was established in Silicon Valley, California, by a group of innovative engineers looking to revolutionize the industry.  
Head Entity: the tech startup  
Tail Entity: Silicon Valley  
#### country of origin ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "country of origin" means country of origin of this item (creative work, food, phrase, product, etc.).
Here is an example:
Relation: country of origin
Context: in the fall of 1996 , now consisting of trevor paglen , nicolas lampert , and sean thomas , noisegate embarked on their first major tour of the united states , with filmmaker laura klein .
Head Entity: noisegate
Tail Entity: united states
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "country of origin":
Relation: country of origin  
Context: The famous dish known as paella originated in the region of Valencia, where it has been a staple for centuries, showcasing the rich culinary traditions of Spain.  
Head Entity: paella  
Tail Entity: Spain  

Relation: country of origin  
Context: The iconic brand of chocolate, Toblerone, is known for its unique triangular shape and has its roots in Switzerland, where it was first created in 1900.  
Head Entity: Toblerone  
Tail Entity: Switzerland  
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 92.19%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 91.25%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 91.07%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 91.41%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 92.36%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 91.88%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 91.48%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 92.19%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 91.83%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 91.96%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 92.08%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 92.58%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 93.01%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 93.06%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 93.42%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 94.05%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 93.75%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 94.02%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 94.27%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 94.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 94.23%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 94.21%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 94.20%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 94.18%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 94.38%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 94.56%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 94.73%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 94.89%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 95.04%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 95.00%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 95.14%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 95.27%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 95.23%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 95.35%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 95.47%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 95.58%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 95.68%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 95.78%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 95.88%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 95.97%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 96.06%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 96.01%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 95.96%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 96.05%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 96.12%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 96.08%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 96.03%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 95.99%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 96.06%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 95.91%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 95.87%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 95.83%   [EVAL] batch:   57 | acc: 100.00%,  total acc: 95.91%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 95.97%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 95.94%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 96.00%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 95.97%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 95.24%   
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 92.19%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 91.25%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 91.07%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 91.41%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 92.36%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 91.88%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 91.48%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 92.19%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 91.83%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 91.96%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 92.08%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 92.58%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 93.01%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 93.06%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 93.42%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 94.05%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 93.75%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 94.02%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 94.27%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 94.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 94.23%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 94.21%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 94.20%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 94.18%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 94.38%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 94.56%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 94.73%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 94.89%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 95.04%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 95.00%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 95.14%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 95.27%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 95.23%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 95.35%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 95.47%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 95.58%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 95.68%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 95.78%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 95.88%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 95.97%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 96.06%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 96.01%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 95.96%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 96.05%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 96.12%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 96.08%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 96.03%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 95.99%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 96.06%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 95.91%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 95.87%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 95.83%   [EVAL] batch:   57 | acc: 100.00%,  total acc: 95.91%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 95.97%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 95.94%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 96.00%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 95.97%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 95.24%   
cur_acc:  ['0.9524']
his_acc:  ['0.9524']
CurrentTrain: epoch 15, batch     0 | loss: 18.5292147CurrentTrain: epoch 15, batch     1 | loss: 18.4888734CurrentTrain: epoch 15, batch     2 | loss: 20.7224361CurrentTrain: epoch  1, batch     3 | loss: 9.4373809CurrentTrain: epoch 15, batch     0 | loss: 17.9410339CurrentTrain: epoch 15, batch     1 | loss: 11.7543398CurrentTrain: epoch 15, batch     2 | loss: 14.7058436CurrentTrain: epoch  1, batch     3 | loss: 8.7273518CurrentTrain: epoch 15, batch     0 | loss: 17.9948237CurrentTrain: epoch 15, batch     1 | loss: 14.8932859CurrentTrain: epoch 15, batch     2 | loss: 14.4022257CurrentTrain: epoch  1, batch     3 | loss: 8.5843277CurrentTrain: epoch 15, batch     0 | loss: 14.4249944CurrentTrain: epoch 15, batch     1 | loss: 13.1683091CurrentTrain: epoch 15, batch     2 | loss: 23.1917136CurrentTrain: epoch  1, batch     3 | loss: 7.0588050CurrentTrain: epoch 15, batch     0 | loss: 10.5307872CurrentTrain: epoch 15, batch     1 | loss: 16.9121821CurrentTrain: epoch 15, batch     2 | loss: 14.1550089CurrentTrain: epoch  1, batch     3 | loss: 8.9058873CurrentTrain: epoch 15, batch     0 | loss: 14.3264499CurrentTrain: epoch 15, batch     1 | loss: 10.6026845CurrentTrain: epoch 15, batch     2 | loss: 18.2579196CurrentTrain: epoch  1, batch     3 | loss: 7.7345583CurrentTrain: epoch 15, batch     0 | loss: 10.1699264CurrentTrain: epoch 15, batch     1 | loss: 13.7235391CurrentTrain: epoch 15, batch     2 | loss: 12.2473630CurrentTrain: epoch  1, batch     3 | loss: 14.5630018CurrentTrain: epoch 15, batch     0 | loss: 11.2121425CurrentTrain: epoch 15, batch     1 | loss: 10.3276663CurrentTrain: epoch 15, batch     2 | loss: 9.5534481CurrentTrain: epoch  1, batch     3 | loss: 6.6919006CurrentTrain: epoch 15, batch     0 | loss: 7.8068348CurrentTrain: epoch 15, batch     1 | loss: 22.2103095CurrentTrain: epoch 15, batch     2 | loss: 8.4495206CurrentTrain: epoch  1, batch     3 | loss: 6.2971103CurrentTrain: epoch 15, batch     0 | loss: 6.3463085CurrentTrain: epoch 15, batch     1 | loss: 10.8766526CurrentTrain: epoch 15, batch     2 | loss: 10.0183902CurrentTrain: epoch  1, batch     3 | loss: 6.1363319
#### spouse ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "spouse" means the subject has the object as their spouse (husband, wife, partner, etc.).
Here is an example:
Relation: spouse
Context: rohan is the son of narayana murthy , founder of infosys , and his wife sudha murthy , an engineer and author of note .
Head Entity: narayana murthy
Tail Entity: sudha murthy
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "spouse":
Relation: spouse  
Context: elon musk, the CEO of spacex and tesla, was previously married to talulah riley, a talented actress and writer.  
Head Entity: elon musk  
Tail Entity: talulah riley  

Relation: spouse  
Context: barack obama, the 44th president of the united states, has been married to michelle obama since 1992, and they have two daughters together.  
Head Entity: barack obama  
Tail Entity: michelle obama  
#### record label ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "record label" means brand and trademark associated with the marketing of subject music recordings and music videos.
Here is an example:
Relation: record label
Context: they briefly reformed in 1992 , when sub pop re - released " primal rock therapy " on cd with five additional unreleased tracks .
Head Entity: primal rock therapy
Tail Entity: sub pop
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "record label":
Relation: record label  
Context: The album "Future Nostalgia" was released under the Warner Records label, showcasing Dua Lipa's unique style and sound.  
Head Entity: Future Nostalgia  
Tail Entity: Warner Records  

Relation: record label  
Context: After signing with Atlantic Records, the band released their highly anticipated debut album, which quickly climbed the charts.  
Head Entity: debut album  
Tail Entity: Atlantic Records  
#### distributor ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "distributor" means distributor of a creative work; distributor for a record label; news agency; film distributor.
Here is an example:
Relation: distributor
Context: distributed by paramount pictures , " the italian job " was theatrically released in the united states on may 30 , 2003 , and grossed over $ 176 million worldwide .
Head Entity: the italian job
Tail Entity: paramount pictures
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "distributor":
Relation: distributor  
Context: the film was distributed by universal pictures and received critical acclaim upon its release in 2015.  
Head Entity: the film  
Tail Entity: universal pictures  

Relation: distributor  
Context: the album was distributed by sony music and quickly climbed the charts, becoming a bestseller.  
Head Entity: the album  
Tail Entity: sony music  
#### located in or next to body of water ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "located in or next to body of water" means body of water on or next to which a place is located.
Here is an example:
Relation: located in or next to body of water
Context: in 1796 carondelet commissioned pintado to conduct a survey of lands granted to ambrosio longue - épée in iberville county on the west bank of the mississippi river opposite baton rouge .
Head Entity: baton rouge
Tail Entity: mississippi river
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "located in or next to body of water":
Relation: located in or next to body of water  
Context: the city of new orleans is situated on the banks of the mississippi river, making it a vibrant hub for trade and culture.  
Head Entity: new orleans  
Tail Entity: mississippi river  

Relation: located in or next to body of water  
Context: the picturesque village of lake placid is nestled beside the serene waters of mirror lake, attracting visitors year-round.  
Head Entity: lake placid  
Tail Entity: mirror lake  
#### subsidiary ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "subsidiary" means subsidiary of a company or organization; generally a fully owned separate corporation.
Here is an example:
Relation: subsidiary
Context: following the sale of amstrad plc to rupert murdoch 's bskyb , viglen is now lord sugar 's sole it establishment .
Head Entity: bskyb
Tail Entity: amstrad
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "subsidiary":
Relation: subsidiary  
Context: In 2018, Facebook acquired WhatsApp, making it a subsidiary of the social media giant.  
Head Entity: Facebook  
Tail Entity: WhatsApp  

Relation: subsidiary  
Context: After the merger, the new company announced that its subsidiary, XYZ Corp, would continue to operate independently.  
Head Entity: new company  
Tail Entity: XYZ Corp  
#### country of citizenship ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "country of citizenship" means the object is a country that recognizes the subject as its citizen.
Here is an example:
Relation: country of citizenship
Context: he challenged seven other swimmers in heat four including ukraine 's rostyslav svanidze , a top 16 finalist in atlanta four years earlier .
Head Entity: rostyslav svanidze
Tail Entity: ukraine
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "country of citizenship":
Relation: country of citizenship  
Context: after years of living in the united states, the renowned artist decided to apply for dual citizenship, allowing him to maintain his ties to his homeland, italy.  
Head Entity: the renowned artist  
Tail Entity: italy  

Relation: country of citizenship  
Context: during the international conference, the delegate from canada proudly spoke about the rights and privileges of being a citizen of his country, emphasizing the importance of citizenship in shaping national identity.  
Head Entity: the delegate from canada  
Tail Entity: canada  
#### genre ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "genre" means creative work's genre or an artist's field of work.
Here is an example:
Relation: genre
Context: nina cites " duffy , muse , and other pop and alternative artists " as her major influences .
Head Entity: duffy
Tail Entity: pop
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "genre":
Relation: genre  
Context: the film "inception" is often categorized as a science fiction thriller that explores complex themes of dreams and reality.  
Head Entity: inception  
Tail Entity: science fiction  

Relation: genre  
Context: beethoven's symphonies are celebrated as masterpieces in the classical music genre, influencing countless composers.  
Head Entity: beethoven  
Tail Entity: classical music  
#### mouth of the watercourse ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "mouth of the watercourse" means the body of water to which the watercourse drains.
Here is an example:
Relation: mouth of the watercourse
Context: san lázaro is 660 km far from asunción and is located at the exact site of the confluence of the apa river with paraguay river , bordering brazil - chaco .
Head Entity: apa river
Tail Entity: paraguay river
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "mouth of the watercourse":
Relation: mouth of the watercourse  
Context: the mississippi river flows into the gulf of mexico, creating a rich delta ecosystem that supports diverse wildlife and plant species.  
Head Entity: mississippi river  
Tail Entity: gulf of mexico  

Relation: mouth of the watercourse  
Context: the rhine river, which runs through several countries in europe, ultimately empties into the north sea, providing a vital shipping route for international trade.  
Head Entity: rhine river  
Tail Entity: north sea  
#### position played on team / speciality ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "position played on team / speciality" means position or specialism of a player on a team.
Here is an example:
Relation: position played on team / speciality
Context: bradley halliday ( born 10 july 1995 ) is an english professional footballer who plays as a defender or a midfielder for league two club cambridge united .
Head Entity: bradley halliday
Tail Entity: defender
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "position played on team / speciality":
Relation: position played on team / speciality  
Context: alex morgan ( born 2 july 1989 ) is an american soccer player who plays as a forward for the national team and the club orlando pride in the national women's soccer league.  
Head Entity: alex morgan  
Tail Entity: forward  

Relation: position played on team / speciality  
Context: lebron james ( born 30 december 1984 ) is an american professional basketball player who plays as a small forward for the los angeles lakers in the nba.  
Head Entity: lebron james  
Tail Entity: small forward  
#### sports season of league or competition ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "sports season of league or competition" means property that shows the competition of which the item is a season.
Here is an example:
Relation: sports season of league or competition
Context: hammami made his international debut for tunisia in 2007 , and represented them at the africa cup of nations in 2010 and 2013 .
Head Entity: 2010
Tail Entity: africa cup of nations
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "sports season of league or competition":
Relation: sports season of league or competition  
Context: The 2021 UEFA Champions League was won by Chelsea, who defeated Manchester City in the final held in Porto.  
Head Entity: 2021  
Tail Entity: UEFA Champions League  

Relation: sports season of league or competition  
Context: The 2019 Cricket World Cup took place in England and Wales, featuring ten teams competing for the title.  
Head Entity: 2019  
Tail Entity: Cricket World Cup  
MemoryTrain:  epoch 15, batch     0 | loss: 8.5016561MemoryTrain:  epoch 15, batch     1 | loss: 8.1956061MemoryTrain:  epoch 15, batch     2 | loss: 8.1548409MemoryTrain:  epoch 11, batch     3 | loss: 7.6752011MemoryTrain:  epoch 15, batch     0 | loss: 9.1057546MemoryTrain:  epoch 15, batch     1 | loss: 10.2007295MemoryTrain:  epoch 15, batch     2 | loss: 6.8013959MemoryTrain:  epoch 11, batch     3 | loss: 6.5423872MemoryTrain:  epoch 15, batch     0 | loss: 6.0890230MemoryTrain:  epoch 15, batch     1 | loss: 6.7763542MemoryTrain:  epoch 15, batch     2 | loss: 6.2962231MemoryTrain:  epoch 11, batch     3 | loss: 8.4444081MemoryTrain:  epoch 15, batch     0 | loss: 7.0740512MemoryTrain:  epoch 15, batch     1 | loss: 8.5287122MemoryTrain:  epoch 15, batch     2 | loss: 6.0167342MemoryTrain:  epoch 11, batch     3 | loss: 6.0581299MemoryTrain:  epoch 15, batch     0 | loss: 6.8512984MemoryTrain:  epoch 15, batch     1 | loss: 6.3904036MemoryTrain:  epoch 15, batch     2 | loss: 5.6767822MemoryTrain:  epoch 11, batch     3 | loss: 4.7933384MemoryTrain:  epoch 15, batch     0 | loss: 4.9002608MemoryTrain:  epoch 15, batch     1 | loss: 5.9088334MemoryTrain:  epoch 15, batch     2 | loss: 8.7465386MemoryTrain:  epoch 11, batch     3 | loss: 6.1988857MemoryTrain:  epoch 15, batch     0 | loss: 2.7716113MemoryTrain:  epoch 15, batch     1 | loss: 7.2387541MemoryTrain:  epoch 15, batch     2 | loss: 4.5400066MemoryTrain:  epoch 11, batch     3 | loss: 5.6950465MemoryTrain:  epoch 15, batch     0 | loss: 10.6108628MemoryTrain:  epoch 15, batch     1 | loss: 5.8512078MemoryTrain:  epoch 15, batch     2 | loss: 5.1198944MemoryTrain:  epoch 11, batch     3 | loss: 5.4672577MemoryTrain:  epoch 15, batch     0 | loss: 6.2420197MemoryTrain:  epoch 15, batch     1 | loss: 4.5006181MemoryTrain:  epoch 15, batch     2 | loss: 11.0145733MemoryTrain:  epoch 11, batch     3 | loss: 4.9278308MemoryTrain:  epoch 15, batch     0 | loss: 5.8902767MemoryTrain:  epoch 15, batch     1 | loss: 3.4885606MemoryTrain:  epoch 15, batch     2 | loss: 6.2724928MemoryTrain:  epoch 11, batch     3 | loss: 3.0265471
[EVAL] batch:    0 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    1 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    2 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    3 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 98.75%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 98.96%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 99.11%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 97.66%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 95.83%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 93.12%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 92.61%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 93.23%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 92.31%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 92.41%   [EVAL] batch:   14 | acc: 81.25%,  total acc: 91.67%   [EVAL] batch:   15 | acc: 93.75%,  total acc: 91.80%   [EVAL] batch:   16 | acc: 62.50%,  total acc: 90.07%   [EVAL] batch:   17 | acc: 81.25%,  total acc: 89.58%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 90.13%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 89.69%   [EVAL] batch:   20 | acc: 81.25%,  total acc: 89.29%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 89.49%   [EVAL] batch:   22 | acc: 81.25%,  total acc: 89.13%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 89.32%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 89.25%   [EVAL] batch:   25 | acc: 87.50%,  total acc: 89.18%   [EVAL] batch:   26 | acc: 81.25%,  total acc: 88.89%   [EVAL] batch:   27 | acc: 87.50%,  total acc: 88.84%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 89.01%   [EVAL] batch:   29 | acc: 56.25%,  total acc: 87.92%   [EVAL] batch:   30 | acc: 68.75%,  total acc: 87.30%   [EVAL] batch:   31 | acc: 75.00%,  total acc: 86.91%   [EVAL] batch:   32 | acc: 68.75%,  total acc: 86.36%   [EVAL] batch:   33 | acc: 68.75%,  total acc: 85.85%   [EVAL] batch:   34 | acc: 62.50%,  total acc: 85.18%   [EVAL] batch:   35 | acc: 68.75%,  total acc: 84.72%   [EVAL] batch:   36 | acc: 62.50%,  total acc: 84.12%   [EVAL] batch:   37 | acc: 81.25%,  total acc: 84.05%   [EVAL] batch:   38 | acc: 68.75%,  total acc: 83.65%   [EVAL] batch:   39 | acc: 75.00%,  total acc: 83.44%   [EVAL] batch:   40 | acc: 62.50%,  total acc: 82.93%   [EVAL] batch:   41 | acc: 75.00%,  total acc: 82.74%   [EVAL] batch:   42 | acc: 81.25%,  total acc: 82.70%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 82.67%   [EVAL] batch:   44 | acc: 31.25%,  total acc: 81.53%   [EVAL] batch:   45 | acc: 43.75%,  total acc: 80.71%   [EVAL] batch:   46 | acc: 31.25%,  total acc: 79.65%   [EVAL] batch:   47 | acc: 62.50%,  total acc: 79.30%   [EVAL] batch:   48 | acc: 25.00%,  total acc: 78.19%   [EVAL] batch:   49 | acc: 56.25%,  total acc: 77.75%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 78.19%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 78.61%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 79.01%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 79.40%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 79.66%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 80.02%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 80.37%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 80.60%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 80.93%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 81.56%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 81.85%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 81.35%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 82.29%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 82.14%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 82.81%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 84.09%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 85.42%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 85.58%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 86.16%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 86.67%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 88.24%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 88.54%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 89.14%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 89.69%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 89.88%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 89.49%   [EVAL] batch:   22 | acc: 81.25%,  total acc: 89.13%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 89.32%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 89.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 89.66%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 90.05%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 90.18%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 90.52%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 90.83%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 91.13%   [EVAL] batch:   31 | acc: 87.50%,  total acc: 91.02%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 91.29%   [EVAL] batch:   33 | acc: 56.25%,  total acc: 90.26%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 89.82%   [EVAL] batch:   35 | acc: 68.75%,  total acc: 89.24%   [EVAL] batch:   36 | acc: 75.00%,  total acc: 88.85%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 88.82%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 89.10%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 89.38%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 89.63%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 89.88%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 90.12%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 90.20%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 90.14%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 90.35%   [EVAL] batch:   46 | acc: 87.50%,  total acc: 90.29%   [EVAL] batch:   47 | acc: 87.50%,  total acc: 90.23%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 90.43%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 90.38%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 90.44%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 90.50%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 90.57%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 90.74%   [EVAL] batch:   54 | acc: 75.00%,  total acc: 90.45%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 90.51%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 90.57%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 90.52%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 90.57%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 90.78%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 90.73%   [EVAL] batch:   62 | acc: 100.00%,  total acc: 90.87%   [EVAL] batch:   63 | acc: 100.00%,  total acc: 91.02%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 91.15%   [EVAL] batch:   65 | acc: 100.00%,  total acc: 91.29%   [EVAL] batch:   66 | acc: 100.00%,  total acc: 91.42%   [EVAL] batch:   67 | acc: 93.75%,  total acc: 91.45%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 91.58%   [EVAL] batch:   69 | acc: 93.75%,  total acc: 91.61%   [EVAL] batch:   70 | acc: 87.50%,  total acc: 91.55%   [EVAL] batch:   71 | acc: 75.00%,  total acc: 91.32%   [EVAL] batch:   72 | acc: 81.25%,  total acc: 91.18%   [EVAL] batch:   73 | acc: 87.50%,  total acc: 91.13%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 91.08%   [EVAL] batch:   75 | acc: 93.75%,  total acc: 91.12%   [EVAL] batch:   76 | acc: 87.50%,  total acc: 91.07%   [EVAL] batch:   77 | acc: 87.50%,  total acc: 91.03%   [EVAL] batch:   78 | acc: 75.00%,  total acc: 90.82%   [EVAL] batch:   79 | acc: 68.75%,  total acc: 90.55%   [EVAL] batch:   80 | acc: 93.75%,  total acc: 90.59%   [EVAL] batch:   81 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:   82 | acc: 75.00%,  total acc: 90.44%   [EVAL] batch:   83 | acc: 87.50%,  total acc: 90.40%   [EVAL] batch:   84 | acc: 87.50%,  total acc: 90.37%   [EVAL] batch:   85 | acc: 93.75%,  total acc: 90.41%   [EVAL] batch:   86 | acc: 87.50%,  total acc: 90.37%   [EVAL] batch:   87 | acc: 93.75%,  total acc: 90.41%   [EVAL] batch:   88 | acc: 75.00%,  total acc: 90.24%   [EVAL] batch:   89 | acc: 87.50%,  total acc: 90.21%   [EVAL] batch:   90 | acc: 87.50%,  total acc: 90.18%   [EVAL] batch:   91 | acc: 81.25%,  total acc: 90.08%   [EVAL] batch:   92 | acc: 62.50%,  total acc: 89.78%   [EVAL] batch:   93 | acc: 56.25%,  total acc: 89.43%   [EVAL] batch:   94 | acc: 81.25%,  total acc: 89.34%   [EVAL] batch:   95 | acc: 68.75%,  total acc: 89.13%   [EVAL] batch:   96 | acc: 62.50%,  total acc: 88.85%   [EVAL] batch:   97 | acc: 68.75%,  total acc: 88.65%   [EVAL] batch:   98 | acc: 68.75%,  total acc: 88.45%   [EVAL] batch:   99 | acc: 75.00%,  total acc: 88.31%   [EVAL] batch:  100 | acc: 62.50%,  total acc: 88.06%   [EVAL] batch:  101 | acc: 87.50%,  total acc: 88.05%   [EVAL] batch:  102 | acc: 56.25%,  total acc: 87.74%   [EVAL] batch:  103 | acc: 75.00%,  total acc: 87.62%   [EVAL] batch:  104 | acc: 75.00%,  total acc: 87.50%   [EVAL] batch:  105 | acc: 81.25%,  total acc: 87.44%   [EVAL] batch:  106 | acc: 62.50%,  total acc: 87.21%   [EVAL] batch:  107 | acc: 31.25%,  total acc: 86.69%   [EVAL] batch:  108 | acc: 37.50%,  total acc: 86.24%   [EVAL] batch:  109 | acc: 50.00%,  total acc: 85.91%   [EVAL] batch:  110 | acc: 37.50%,  total acc: 85.47%   [EVAL] batch:  111 | acc: 43.75%,  total acc: 85.10%   [EVAL] batch:  112 | acc: 81.25%,  total acc: 85.07%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 85.20%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 85.33%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 85.45%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 85.58%   [EVAL] batch:  117 | acc: 93.75%,  total acc: 85.65%   [EVAL] batch:  118 | acc: 100.00%,  total acc: 85.77%   [EVAL] batch:  119 | acc: 93.75%,  total acc: 85.83%   [EVAL] batch:  120 | acc: 100.00%,  total acc: 85.95%   [EVAL] batch:  121 | acc: 100.00%,  total acc: 86.07%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 86.18%   [EVAL] batch:  123 | acc: 100.00%,  total acc: 86.29%   [EVAL] batch:  124 | acc: 100.00%,  total acc: 86.40%   
cur_acc:  ['0.9524', '0.8135']
his_acc:  ['0.9524', '0.8640']
CurrentTrain: epoch 15, batch     0 | loss: 23.7215677CurrentTrain: epoch 15, batch     1 | loss: 19.8207013CurrentTrain: epoch 15, batch     2 | loss: 16.6689911CurrentTrain: epoch  1, batch     3 | loss: 11.5616088CurrentTrain: epoch 15, batch     0 | loss: 15.6941668CurrentTrain: epoch 15, batch     1 | loss: 12.3595745CurrentTrain: epoch 15, batch     2 | loss: 17.7936170CurrentTrain: epoch  1, batch     3 | loss: 11.5804348CurrentTrain: epoch 15, batch     0 | loss: 14.8735834CurrentTrain: epoch 15, batch     1 | loss: 12.0324769CurrentTrain: epoch 15, batch     2 | loss: 17.1382180CurrentTrain: epoch  1, batch     3 | loss: 16.5467224CurrentTrain: epoch 15, batch     0 | loss: 15.1673476CurrentTrain: epoch 15, batch     1 | loss: 14.0014208CurrentTrain: epoch 15, batch     2 | loss: 20.5156282CurrentTrain: epoch  1, batch     3 | loss: 9.8805267CurrentTrain: epoch 15, batch     0 | loss: 25.2595164CurrentTrain: epoch 15, batch     1 | loss: 13.0944694CurrentTrain: epoch 15, batch     2 | loss: 17.3931610CurrentTrain: epoch  1, batch     3 | loss: 9.0330306CurrentTrain: epoch 15, batch     0 | loss: 12.6809750CurrentTrain: epoch 15, batch     1 | loss: 12.6999468CurrentTrain: epoch 15, batch     2 | loss: 10.2962618CurrentTrain: epoch  1, batch     3 | loss: 13.0031401CurrentTrain: epoch 15, batch     0 | loss: 11.3335514CurrentTrain: epoch 15, batch     1 | loss: 20.8095444CurrentTrain: epoch 15, batch     2 | loss: 11.0896214CurrentTrain: epoch  1, batch     3 | loss: 8.4724576CurrentTrain: epoch 15, batch     0 | loss: 18.0568467CurrentTrain: epoch 15, batch     1 | loss: 13.1058141CurrentTrain: epoch 15, batch     2 | loss: 12.5697589CurrentTrain: epoch  1, batch     3 | loss: 8.4186539CurrentTrain: epoch 15, batch     0 | loss: 9.1560016CurrentTrain: epoch 15, batch     1 | loss: 11.8373679CurrentTrain: epoch 15, batch     2 | loss: 10.0353553CurrentTrain: epoch  1, batch     3 | loss: 8.2189033CurrentTrain: epoch 15, batch     0 | loss: 12.4443235CurrentTrain: epoch 15, batch     1 | loss: 10.7197975CurrentTrain: epoch 15, batch     2 | loss: 10.7461800CurrentTrain: epoch  1, batch     3 | loss: 25.5097159
#### main subject ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "main subject" means primary topic of a work.
Here is an example:
Relation: main subject
Context: for more information on the grand mosque seizure itself , see " the siege of mecca " by yaroslav trofimov .
Head Entity: the siege of mecca
Tail Entity: grand mosque seizure
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "main subject":
Relation: main subject  
Context: in her book "the power of habit," charles duhigg explores the science behind why habits exist and how they can be changed.  
Head Entity: the power of habit  
Tail Entity: science of habits  

Relation: main subject  
Context: "sapiens: a brief history of humankind" by yuval noah harari provides a compelling overview of the history and impact of our species.  
Head Entity: sapiens: a brief history of humankind  
Tail Entity: history of humankind  
#### participating team ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "participating team" means teams that actively takes/took part in an event or process.
Here is an example:
Relation: participating team
Context: the two teams had met in two previous matches , including in the 2006 world cup group stage , won by brazil 1–0 .
Head Entity: 2006 world cup
Tail Entity: brazil
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "participating team":
Relation: participating team  
Context: In the 2020 UEFA European Championship, Italy and England faced off in the final, with Italy emerging victorious after a tense penalty shootout.  
Head Entity: 2020 UEFA European Championship  
Tail Entity: Italy  

Relation: participating team  
Context: The 2018 FIFA World Cup saw France and Croatia compete in an exhilarating final, where France claimed their second title.  
Head Entity: 2018 FIFA World Cup  
Tail Entity: France  
#### has part ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "has part" means part of this subject.
Here is an example:
Relation: has part
Context: among the competing artists was previous eurovision song contest entrant robin juhkental who represented estonia in 2010 as part of the group malcolm lincoln .
Head Entity: malcolm lincoln
Tail Entity: robin juhkental
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "has part":
Relation: has part  
Context: The famous painting "The Last Supper" features several apostles, including Peter, who is depicted prominently in the foreground.  
Head Entity: The Last Supper  
Tail Entity: Peter  

Relation: has part  
Context: The smartphone model Galaxy S21 comes with a high-resolution camera that includes a wide-angle lens, enhancing photography capabilities.  
Head Entity: Galaxy S21  
Tail Entity: wide-angle lens  
#### nominated for ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "nominated for" means award nomination received by a person, organisation or creative work.
Here is an example:
Relation: nominated for
Context: he was nominated for the academy award for best picture , along with steven spielberg , ian bryce , mark gordon for the film " saving private ryan " .
Head Entity: ian bryce
Tail Entity: academy award for best picture
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "nominated for":
Relation: nominated for  
Context: The film "The Shape of Water" received multiple nominations, including a nod for best director, which was awarded to Guillermo del Toro.  
Head Entity: Guillermo del Toro  
Tail Entity: best director  

Relation: nominated for  
Context: In 2020, the popular series "Succession" was nominated for several Emmy Awards, showcasing its critical acclaim and audience popularity.  
Head Entity: Succession  
Tail Entity: Emmy Awards  
#### instance of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "instance of" means that class of which this subject is a particular example and member.
Here is an example:
Relation: instance of
Context: in 2007 , the county municipality created the transit authority called " skyss " that would market public transport while is would be operated by private companies based on public service obligation contracts .
Head Entity: skyss
Tail Entity: transit authority
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "instance of":
Relation: instance of  
Context: The smartphone known as the iPhone was first released by Apple Inc. in 2007, revolutionizing the mobile phone industry and setting new standards for design and functionality.  
Head Entity: iPhone  
Tail Entity: smartphone  

Relation: instance of  
Context: The Great Wall of China is a series of fortifications made of various materials, built to protect the Chinese states from invasions and raids, and is considered one of the most iconic structures in the world.  
Head Entity: Great Wall of China  
Tail Entity: fortification  
#### mountain range ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "mountain range" means range or subrange to which the geographical item belongs.
Here is an example:
Relation: mountain range
Context: abus valley ( ) is an ice - free valley southeast of turnstile ridge at the north end of britannia range .
Head Entity: turnstile ridge
Tail Entity: britannia range
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "mountain range":
Relation: mountain range  
Context: the sierra nevada ( ) is a mountain range in the western united states, primarily in california, and it includes the famous yosemite national park, which is located within the boundaries of the range.  
Head Entity: sierra nevada  
Tail Entity: yosemite national park  

Relation: mountain range  
Context: the appalachian mountains ( ) extend from the canadian province of quebec down to alabama, making it one of the longest mountain ranges in north america, with the great smoky mountains being a prominent subrange.  
Head Entity: appalachian mountains  
Tail Entity: great smoky mountains  
#### screenwriter ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "screenwriter" means person who wrote the script for subject item.
Here is an example:
Relation: screenwriter
Context: it would also be the last episode to feature a grounding , until " the marge - ian chronicles " in season 27 , six years later ( also written by brian kelley ) .
Head Entity: the marge - ian chronicles
Tail Entity: brian kelley
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "screenwriter":
Relation: screenwriter  
Context: The film "Inception" was a groundbreaking project that showcased the creative genius of its screenwriter, who also directed the movie, Christopher Nolan.  
Head Entity: Inception  
Tail Entity: Christopher Nolan  

Relation: screenwriter  
Context: The beloved animated feature "Toy Story" was brought to life by the talented screenwriter, who crafted its memorable characters and storyline, Joss Whedon.  
Head Entity: Toy Story  
Tail Entity: Joss Whedon  
#### language of work or name ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "language of work or name" means language associated with this creative work (such as books, shows, songs, broadcasts or websites) or a name.
Here is an example:
Relation: language of work or name
Context: the publisher found success in 1993 with tarō gomi 's " everyone poops " , a japanese language import that sold over one million copies and is now their best - known title .
Head Entity: everyone poops
Tail Entity: japanese language
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "language of work or name":
Relation: language of work or name  
Context: The animated series "Avatar: The Last Airbender" features a rich narrative and is primarily produced in English, appealing to a global audience.  
Head Entity: Avatar: The Last Airbender  
Tail Entity: English  

Relation: language of work or name  
Context: The famous novel "Cien años de soledad" by Gabriel García Márquez is a cornerstone of Latin American literature and is originally written in Spanish.  
Head Entity: Cien años de soledad  
Tail Entity: Spanish  
#### operator ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "operator" means person, profession, or organization that operates the equipment, facility, or service.
Here is an example:
Relation: operator
Context: the aircraft was a boeing 707 - 321c that first entered service with pan american world airways ( pan am ) in 1963 .
Head Entity: boeing 707 - 321c
Tail Entity: pan american world airways
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "operator":
Relation: operator  
Context: the new electric bus fleet is managed by the city transportation authority, which ensures efficient public transit services.  
Head Entity: electric bus fleet  
Tail Entity: city transportation authority  

Relation: operator  
Context: the state-of-the-art research facility is operated by the national science foundation, providing resources for groundbreaking studies.  
Head Entity: research facility  
Tail Entity: national science foundation  
#### religion ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "religion" means religion of a person, organization or religious building, or associated with this subject.
Here is an example:
Relation: religion
Context: the most important lama of this series was the third changkya , rolpai dorje , who was preceptor to the qianlong emperor of china , and chief representative of tibetan buddhism at the qing court .
Head Entity: rolpai dorje
Tail Entity: tibetan buddhism
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "religion":
Relation: religion  
Context: the cathedral of notre-dame is a famous example of french gothic architecture and serves as a major center for the roman catholic faith in paris.  
Head Entity: notre-dame  
Tail Entity: roman catholic faith  

Relation: religion  
Context: the dalai lama is a prominent figure in the gelug school of tibetan buddhism, advocating for peace and compassion worldwide.  
Head Entity: dalai lama  
Tail Entity: tibetan buddhism  
MemoryTrain:  epoch 15, batch     0 | loss: 3.8830542MemoryTrain:  epoch 15, batch     1 | loss: 8.7695143MemoryTrain:  epoch 15, batch     2 | loss: 4.6540182MemoryTrain:  epoch 15, batch     3 | loss: 4.3852864MemoryTrain:  epoch 15, batch     4 | loss: 5.3088493MemoryTrain:  epoch  9, batch     5 | loss: 5.7895314MemoryTrain:  epoch 15, batch     0 | loss: 4.4045387MemoryTrain:  epoch 15, batch     1 | loss: 5.8504218MemoryTrain:  epoch 15, batch     2 | loss: 4.6154650MemoryTrain:  epoch 15, batch     3 | loss: 7.2121705MemoryTrain:  epoch 15, batch     4 | loss: 5.6499826MemoryTrain:  epoch  9, batch     5 | loss: 5.0226811MemoryTrain:  epoch 15, batch     0 | loss: 4.8629615MemoryTrain:  epoch 15, batch     1 | loss: 5.5306196MemoryTrain:  epoch 15, batch     2 | loss: 3.6796962MemoryTrain:  epoch 15, batch     3 | loss: 3.2828048MemoryTrain:  epoch 15, batch     4 | loss: 6.8869047MemoryTrain:  epoch  9, batch     5 | loss: 2.8321632MemoryTrain:  epoch 15, batch     0 | loss: 4.3818005MemoryTrain:  epoch 15, batch     1 | loss: 3.9232127MemoryTrain:  epoch 15, batch     2 | loss: 7.6424604MemoryTrain:  epoch 15, batch     3 | loss: 3.8335759MemoryTrain:  epoch 15, batch     4 | loss: 5.4518353MemoryTrain:  epoch  9, batch     5 | loss: 5.4570991MemoryTrain:  epoch 15, batch     0 | loss: 2.4025171MemoryTrain:  epoch 15, batch     1 | loss: 5.6273591MemoryTrain:  epoch 15, batch     2 | loss: 3.3643484MemoryTrain:  epoch 15, batch     3 | loss: 3.8553776MemoryTrain:  epoch 15, batch     4 | loss: 1.9912044MemoryTrain:  epoch  9, batch     5 | loss: 6.1629395MemoryTrain:  epoch 15, batch     0 | loss: 6.0227314MemoryTrain:  epoch 15, batch     1 | loss: 5.3750239MemoryTrain:  epoch 15, batch     2 | loss: 3.0692745MemoryTrain:  epoch 15, batch     3 | loss: 5.7541529MemoryTrain:  epoch 15, batch     4 | loss: 2.3628400MemoryTrain:  epoch  9, batch     5 | loss: 5.0386474MemoryTrain:  epoch 15, batch     0 | loss: 5.0471128MemoryTrain:  epoch 15, batch     1 | loss: 3.6884720MemoryTrain:  epoch 15, batch     2 | loss: 4.8157836MemoryTrain:  epoch 15, batch     3 | loss: 5.1469097MemoryTrain:  epoch 15, batch     4 | loss: 3.7056569MemoryTrain:  epoch  9, batch     5 | loss: 6.0569625MemoryTrain:  epoch 15, batch     0 | loss: 4.5866728MemoryTrain:  epoch 15, batch     1 | loss: 3.8740228MemoryTrain:  epoch 15, batch     2 | loss: 2.9910600MemoryTrain:  epoch 15, batch     3 | loss: 3.1283570MemoryTrain:  epoch 15, batch     4 | loss: 4.5816479MemoryTrain:  epoch  9, batch     5 | loss: 4.9954280MemoryTrain:  epoch 15, batch     0 | loss: 4.4945161MemoryTrain:  epoch 15, batch     1 | loss: 2.2948820MemoryTrain:  epoch 15, batch     2 | loss: 5.7593308MemoryTrain:  epoch 15, batch     3 | loss: 2.5328779MemoryTrain:  epoch 15, batch     4 | loss: 6.0628478MemoryTrain:  epoch  9, batch     5 | loss: 5.0842730MemoryTrain:  epoch 15, batch     0 | loss: 2.2367511MemoryTrain:  epoch 15, batch     1 | loss: 5.3160250MemoryTrain:  epoch 15, batch     2 | loss: 3.2874732MemoryTrain:  epoch 15, batch     3 | loss: 4.1887877MemoryTrain:  epoch 15, batch     4 | loss: 2.1343997MemoryTrain:  epoch  9, batch     5 | loss: 1.3689965
[EVAL] batch:    0 | acc: 56.25%,  total acc: 56.25%   [EVAL] batch:    1 | acc: 50.00%,  total acc: 53.12%   [EVAL] batch:    2 | acc: 56.25%,  total acc: 54.17%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 59.38%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 62.50%   [EVAL] batch:    5 | acc: 68.75%,  total acc: 63.54%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 67.86%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 70.31%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 72.22%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 73.75%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 75.57%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 76.56%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 76.44%   [EVAL] batch:   13 | acc: 18.75%,  total acc: 72.32%   [EVAL] batch:   14 | acc: 25.00%,  total acc: 69.17%   [EVAL] batch:   15 | acc: 18.75%,  total acc: 66.02%   [EVAL] batch:   16 | acc: 56.25%,  total acc: 65.44%   [EVAL] batch:   17 | acc: 43.75%,  total acc: 64.24%   [EVAL] batch:   18 | acc: 37.50%,  total acc: 62.83%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 64.69%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 66.37%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 67.61%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 69.02%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 70.31%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 71.50%   [EVAL] batch:   25 | acc: 50.00%,  total acc: 70.67%   [EVAL] batch:   26 | acc: 18.75%,  total acc: 68.75%   [EVAL] batch:   27 | acc: 37.50%,  total acc: 67.63%   [EVAL] batch:   28 | acc: 62.50%,  total acc: 67.46%   [EVAL] batch:   29 | acc: 31.25%,  total acc: 66.25%   [EVAL] batch:   30 | acc: 37.50%,  total acc: 65.32%   [EVAL] batch:   31 | acc: 81.25%,  total acc: 65.82%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 66.67%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 67.65%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 68.39%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 68.92%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 69.76%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 70.56%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 71.31%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 72.03%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 72.71%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 73.36%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 73.98%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 74.57%   [EVAL] batch:   44 | acc: 68.75%,  total acc: 74.44%   [EVAL] batch:   45 | acc: 87.50%,  total acc: 74.73%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 75.13%   [EVAL] batch:   47 | acc: 81.25%,  total acc: 75.26%   [EVAL] batch:   48 | acc: 93.75%,  total acc: 75.64%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 76.00%   [EVAL] batch:   50 | acc: 68.75%,  total acc: 75.86%   [EVAL] batch:   51 | acc: 68.75%,  total acc: 75.72%   [EVAL] batch:   52 | acc: 68.75%,  total acc: 75.59%   [EVAL] batch:   53 | acc: 68.75%,  total acc: 75.46%   [EVAL] batch:   54 | acc: 75.00%,  total acc: 75.45%   [EVAL] batch:   55 | acc: 75.00%,  total acc: 75.45%   [EVAL] batch:   56 | acc: 75.00%,  total acc: 75.44%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 75.65%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 75.95%   [EVAL] batch:   59 | acc: 75.00%,  total acc: 75.94%   [EVAL] batch:   60 | acc: 93.75%,  total acc: 76.23%   [EVAL] batch:   61 | acc: 75.00%,  total acc: 76.21%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 75.79%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 82.50%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 82.14%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 82.81%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 82.64%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 83.12%   [EVAL] batch:   10 | acc: 75.00%,  total acc: 82.39%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 82.81%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 82.69%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 83.48%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 84.17%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 85.16%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 86.03%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 86.46%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 87.17%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 87.19%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 87.80%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 87.78%   [EVAL] batch:   22 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 87.76%   [EVAL] batch:   24 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 87.98%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 88.43%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 88.62%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 89.01%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 89.38%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 89.72%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 89.84%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 90.15%   [EVAL] batch:   33 | acc: 68.75%,  total acc: 89.52%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 89.11%   [EVAL] batch:   35 | acc: 75.00%,  total acc: 88.72%   [EVAL] batch:   36 | acc: 81.25%,  total acc: 88.51%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 88.49%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 88.78%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 89.06%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 89.33%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 89.58%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 89.68%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 89.77%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 89.72%   [EVAL] batch:   45 | acc: 87.50%,  total acc: 89.67%   [EVAL] batch:   46 | acc: 87.50%,  total acc: 89.63%   [EVAL] batch:   47 | acc: 81.25%,  total acc: 89.45%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 89.67%   [EVAL] batch:   49 | acc: 81.25%,  total acc: 89.50%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 89.66%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 89.74%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 89.93%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 89.89%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 89.96%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 90.13%   [EVAL] batch:   57 | acc: 56.25%,  total acc: 89.55%   [EVAL] batch:   58 | acc: 81.25%,  total acc: 89.41%   [EVAL] batch:   59 | acc: 87.50%,  total acc: 89.38%   [EVAL] batch:   60 | acc: 87.50%,  total acc: 89.34%   [EVAL] batch:   61 | acc: 75.00%,  total acc: 89.11%   [EVAL] batch:   62 | acc: 100.00%,  total acc: 89.29%   [EVAL] batch:   63 | acc: 100.00%,  total acc: 89.45%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 89.62%   [EVAL] batch:   65 | acc: 100.00%,  total acc: 89.77%   [EVAL] batch:   66 | acc: 100.00%,  total acc: 89.93%   [EVAL] batch:   67 | acc: 93.75%,  total acc: 89.98%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 90.13%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 90.27%   [EVAL] batch:   70 | acc: 93.75%,  total acc: 90.32%   [EVAL] batch:   71 | acc: 87.50%,  total acc: 90.28%   [EVAL] batch:   72 | acc: 87.50%,  total acc: 90.24%   [EVAL] batch:   73 | acc: 87.50%,  total acc: 90.20%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 90.17%   [EVAL] batch:   75 | acc: 93.75%,  total acc: 90.21%   [EVAL] batch:   76 | acc: 81.25%,  total acc: 90.10%   [EVAL] batch:   77 | acc: 87.50%,  total acc: 90.06%   [EVAL] batch:   78 | acc: 81.25%,  total acc: 89.95%   [EVAL] batch:   79 | acc: 75.00%,  total acc: 89.77%   [EVAL] batch:   80 | acc: 93.75%,  total acc: 89.81%   [EVAL] batch:   81 | acc: 68.75%,  total acc: 89.56%   [EVAL] batch:   82 | acc: 75.00%,  total acc: 89.38%   [EVAL] batch:   83 | acc: 68.75%,  total acc: 89.14%   [EVAL] batch:   84 | acc: 68.75%,  total acc: 88.90%   [EVAL] batch:   85 | acc: 75.00%,  total acc: 88.74%   [EVAL] batch:   86 | acc: 50.00%,  total acc: 88.29%   [EVAL] batch:   87 | acc: 87.50%,  total acc: 88.28%   [EVAL] batch:   88 | acc: 87.50%,  total acc: 88.27%   [EVAL] batch:   89 | acc: 93.75%,  total acc: 88.33%   [EVAL] batch:   90 | acc: 87.50%,  total acc: 88.32%   [EVAL] batch:   91 | acc: 81.25%,  total acc: 88.25%   [EVAL] batch:   92 | acc: 75.00%,  total acc: 88.10%   [EVAL] batch:   93 | acc: 68.75%,  total acc: 87.90%   [EVAL] batch:   94 | acc: 62.50%,  total acc: 87.63%   [EVAL] batch:   95 | acc: 50.00%,  total acc: 87.24%   [EVAL] batch:   96 | acc: 62.50%,  total acc: 86.98%   [EVAL] batch:   97 | acc: 56.25%,  total acc: 86.67%   [EVAL] batch:   98 | acc: 62.50%,  total acc: 86.43%   [EVAL] batch:   99 | acc: 56.25%,  total acc: 86.12%   [EVAL] batch:  100 | acc: 62.50%,  total acc: 85.89%   [EVAL] batch:  101 | acc: 75.00%,  total acc: 85.78%   [EVAL] batch:  102 | acc: 62.50%,  total acc: 85.56%   [EVAL] batch:  103 | acc: 75.00%,  total acc: 85.46%   [EVAL] batch:  104 | acc: 75.00%,  total acc: 85.36%   [EVAL] batch:  105 | acc: 75.00%,  total acc: 85.26%   [EVAL] batch:  106 | acc: 56.25%,  total acc: 84.99%   [EVAL] batch:  107 | acc: 31.25%,  total acc: 84.49%   [EVAL] batch:  108 | acc: 31.25%,  total acc: 84.00%   [EVAL] batch:  109 | acc: 56.25%,  total acc: 83.75%   [EVAL] batch:  110 | acc: 37.50%,  total acc: 83.33%   [EVAL] batch:  111 | acc: 50.00%,  total acc: 83.04%   [EVAL] batch:  112 | acc: 75.00%,  total acc: 82.96%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 83.11%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 83.26%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 83.41%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 83.55%   [EVAL] batch:  117 | acc: 100.00%,  total acc: 83.69%   [EVAL] batch:  118 | acc: 100.00%,  total acc: 83.82%   [EVAL] batch:  119 | acc: 100.00%,  total acc: 83.96%   [EVAL] batch:  120 | acc: 100.00%,  total acc: 84.09%   [EVAL] batch:  121 | acc: 100.00%,  total acc: 84.22%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 84.35%   [EVAL] batch:  123 | acc: 100.00%,  total acc: 84.48%   [EVAL] batch:  124 | acc: 100.00%,  total acc: 84.60%   [EVAL] batch:  125 | acc: 56.25%,  total acc: 84.38%   [EVAL] batch:  126 | acc: 50.00%,  total acc: 84.10%   [EVAL] batch:  127 | acc: 56.25%,  total acc: 83.89%   [EVAL] batch:  128 | acc: 75.00%,  total acc: 83.82%   [EVAL] batch:  129 | acc: 75.00%,  total acc: 83.75%   [EVAL] batch:  130 | acc: 68.75%,  total acc: 83.64%   [EVAL] batch:  131 | acc: 93.75%,  total acc: 83.71%   [EVAL] batch:  132 | acc: 87.50%,  total acc: 83.74%   [EVAL] batch:  133 | acc: 87.50%,  total acc: 83.77%   [EVAL] batch:  134 | acc: 87.50%,  total acc: 83.80%   [EVAL] batch:  135 | acc: 93.75%,  total acc: 83.87%   [EVAL] batch:  136 | acc: 87.50%,  total acc: 83.90%   [EVAL] batch:  137 | acc: 75.00%,  total acc: 83.83%   [EVAL] batch:  138 | acc: 18.75%,  total acc: 83.36%   [EVAL] batch:  139 | acc: 25.00%,  total acc: 82.95%   [EVAL] batch:  140 | acc: 18.75%,  total acc: 82.49%   [EVAL] batch:  141 | acc: 56.25%,  total acc: 82.31%   [EVAL] batch:  142 | acc: 43.75%,  total acc: 82.04%   [EVAL] batch:  143 | acc: 37.50%,  total acc: 81.73%   [EVAL] batch:  144 | acc: 100.00%,  total acc: 81.85%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 81.98%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 82.06%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 82.18%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 82.30%   [EVAL] batch:  149 | acc: 100.00%,  total acc: 82.42%   [EVAL] batch:  150 | acc: 50.00%,  total acc: 82.20%   [EVAL] batch:  151 | acc: 18.75%,  total acc: 81.78%   [EVAL] batch:  152 | acc: 37.50%,  total acc: 81.50%   [EVAL] batch:  153 | acc: 62.50%,  total acc: 81.37%   [EVAL] batch:  154 | acc: 31.25%,  total acc: 81.05%   [EVAL] batch:  155 | acc: 37.50%,  total acc: 80.77%   [EVAL] batch:  156 | acc: 81.25%,  total acc: 80.77%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 80.85%   [EVAL] batch:  158 | acc: 100.00%,  total acc: 80.97%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 81.05%   [EVAL] batch:  160 | acc: 87.50%,  total acc: 81.09%   [EVAL] batch:  161 | acc: 100.00%,  total acc: 81.21%   [EVAL] batch:  162 | acc: 100.00%,  total acc: 81.33%   [EVAL] batch:  163 | acc: 100.00%,  total acc: 81.44%   [EVAL] batch:  164 | acc: 100.00%,  total acc: 81.55%   [EVAL] batch:  165 | acc: 100.00%,  total acc: 81.66%   [EVAL] batch:  166 | acc: 100.00%,  total acc: 81.77%   [EVAL] batch:  167 | acc: 100.00%,  total acc: 81.88%   [EVAL] batch:  168 | acc: 100.00%,  total acc: 81.99%   [EVAL] batch:  169 | acc: 68.75%,  total acc: 81.91%   [EVAL] batch:  170 | acc: 87.50%,  total acc: 81.94%   [EVAL] batch:  171 | acc: 93.75%,  total acc: 82.01%   [EVAL] batch:  172 | acc: 81.25%,  total acc: 82.01%   [EVAL] batch:  173 | acc: 93.75%,  total acc: 82.08%   [EVAL] batch:  174 | acc: 93.75%,  total acc: 82.14%   [EVAL] batch:  175 | acc: 68.75%,  total acc: 82.07%   [EVAL] batch:  176 | acc: 68.75%,  total acc: 81.99%   [EVAL] batch:  177 | acc: 68.75%,  total acc: 81.92%   [EVAL] batch:  178 | acc: 68.75%,  total acc: 81.84%   [EVAL] batch:  179 | acc: 75.00%,  total acc: 81.81%   [EVAL] batch:  180 | acc: 75.00%,  total acc: 81.77%   [EVAL] batch:  181 | acc: 75.00%,  total acc: 81.73%   [EVAL] batch:  182 | acc: 87.50%,  total acc: 81.76%   [EVAL] batch:  183 | acc: 93.75%,  total acc: 81.83%   [EVAL] batch:  184 | acc: 75.00%,  total acc: 81.79%   [EVAL] batch:  185 | acc: 93.75%,  total acc: 81.85%   [EVAL] batch:  186 | acc: 75.00%,  total acc: 81.82%   [EVAL] batch:  187 | acc: 50.00%,  total acc: 81.65%   
cur_acc:  ['0.9524', '0.8135', '0.7579']
his_acc:  ['0.9524', '0.8640', '0.8165']
CurrentTrain: epoch 15, batch     0 | loss: 15.3436830CurrentTrain: epoch 15, batch     1 | loss: 17.3562099CurrentTrain: epoch 15, batch     2 | loss: 14.3991001CurrentTrain: epoch  1, batch     3 | loss: 11.2331630CurrentTrain: epoch 15, batch     0 | loss: 11.0058827CurrentTrain: epoch 15, batch     1 | loss: 11.8861724CurrentTrain: epoch 15, batch     2 | loss: 12.9784164CurrentTrain: epoch  1, batch     3 | loss: 9.6141463CurrentTrain: epoch 15, batch     0 | loss: 9.1309719CurrentTrain: epoch 15, batch     1 | loss: 9.4734361CurrentTrain: epoch 15, batch     2 | loss: 16.0978709CurrentTrain: epoch  1, batch     3 | loss: 9.2112086CurrentTrain: epoch 15, batch     0 | loss: 11.3956213CurrentTrain: epoch 15, batch     1 | loss: 19.0886845CurrentTrain: epoch 15, batch     2 | loss: 8.5851153CurrentTrain: epoch  1, batch     3 | loss: 6.6631274CurrentTrain: epoch 15, batch     0 | loss: 13.4132384CurrentTrain: epoch 15, batch     1 | loss: 10.2836455CurrentTrain: epoch 15, batch     2 | loss: 16.9108179CurrentTrain: epoch  1, batch     3 | loss: 7.5815263CurrentTrain: epoch 15, batch     0 | loss: 8.5050271CurrentTrain: epoch 15, batch     1 | loss: 8.6772016CurrentTrain: epoch 15, batch     2 | loss: 9.7938863CurrentTrain: epoch  1, batch     3 | loss: 6.4234392CurrentTrain: epoch 15, batch     0 | loss: 9.3879577CurrentTrain: epoch 15, batch     1 | loss: 9.2601312CurrentTrain: epoch 15, batch     2 | loss: 8.8344528CurrentTrain: epoch  1, batch     3 | loss: 6.7059349CurrentTrain: epoch 15, batch     0 | loss: 6.8366463CurrentTrain: epoch 15, batch     1 | loss: 7.6111074CurrentTrain: epoch 15, batch     2 | loss: 7.3046168CurrentTrain: epoch  1, batch     3 | loss: 7.2796852CurrentTrain: epoch 15, batch     0 | loss: 12.9961219CurrentTrain: epoch 15, batch     1 | loss: 24.6300450CurrentTrain: epoch 15, batch     2 | loss: 11.5189418CurrentTrain: epoch  1, batch     3 | loss: 6.0151088CurrentTrain: epoch 15, batch     0 | loss: 9.4980970CurrentTrain: epoch 15, batch     1 | loss: 8.0184402CurrentTrain: epoch 15, batch     2 | loss: 8.4912731CurrentTrain: epoch  1, batch     3 | loss: 5.9001914
#### member of political party ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "member of political party" means the political party of which a person is or has been a member or otherwise affiliated.
Here is an example:
Relation: member of political party
Context: dresser is an analog to anton drexler , the founder of the nazi party which was then hijacked by adolf hitler .
Head Entity: anton drexler
Tail Entity: nazi party
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "member of political party":
Relation: member of political party  
Context: After serving as a senator, john doe became a prominent member of the democratic party, advocating for various social reforms.  
Head Entity: john doe  
Tail Entity: democratic party  

Relation: member of political party  
Context: During his tenure as a governor, jane smith was a key figure in the republican party, influencing many policies and campaigns.  
Head Entity: jane smith  
Tail Entity: republican party  
#### after a work by ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "after a work by" means artist whose work strongly inspired/ was copied in this item.
Here is an example:
Relation: after a work by
Context: it is a partly fictional biography , adapted by shane connaughton and jim sheridan from the book " my left foot " by christy brown .
Head Entity: my left foot
Tail Entity: christy brown
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "after a work by":
Relation: after a work by  
Context: the film is based on the novel "the great gatsby" by f. scott fitzgerald, capturing the essence of the roaring twenties.  
Head Entity: the great gatsby  
Tail Entity: f. scott fitzgerald  

Relation: after a work by  
Context: this musical is inspired by the play "hamlet" written by william shakespeare, reimagining the classic tragedy in a contemporary setting.  
Head Entity: hamlet  
Tail Entity: william shakespeare  
#### notable work ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "notable work" means notable scientific, artistic or literary work, or other work of significance among subject's works.
Here is an example:
Relation: notable work
Context: the influential victorian art critic john ruskin expounded theories about social justice in " unto this last " ( 1860 ) .
Head Entity: john ruskin
Tail Entity: unto this last
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "notable work":
Relation: notable work  
Context: the renowned physicist albert einstein published his groundbreaking theory of relativity in the paper "on the electrodynamics of moving bodies" (1905).  
Head Entity: albert einstein  
Tail Entity: on the electrodynamics of moving bodies  

Relation: notable work  
Context: the celebrated author j.k. rowling captivated readers worldwide with her fantasy series, starting with the book "harry potter and the philosopher's stone" (1997).  
Head Entity: j.k. rowling  
Tail Entity: harry potter and the philosopher's stone  
#### headquarters location ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "headquarters location" means city, where an organization's headquarters is or has been situated.
Here is an example:
Relation: headquarters location
Context: in 1986 fram traveled to poland to play against katowice in a very even duel fram eventually lost . sparta prague came to reykjavík 1987 to play against fram reykjavik at laugardalsvöllur stadium .
Head Entity: fram reykjavik
Tail Entity: reykjavík
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "headquarters location":
Relation: headquarters location  
Context: in 2001, the tech company apple inc. moved its headquarters to cupertino, california, where it has since developed numerous innovative products.  
Head Entity: apple inc.  
Tail Entity: cupertino  

Relation: headquarters location  
Context: the multinational corporation unilever established its headquarters in rotterdam, netherlands, in the early 20th century, becoming a leader in consumer goods.  
Head Entity: unilever  
Tail Entity: rotterdam  
#### taxon rank ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "taxon rank" means level in a taxonomic hierarchy.
Here is an example:
Relation: taxon rank
Context: a number of species that were placed in the family phyllophoridae by thander in 1989 and 1990 are now included in cucumariidae .
Head Entity: phyllophoridae
Tail Entity: family
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "taxon rank":
Relation: taxon rank  
Context: The genus Canis includes several species, with Canis lupus being the most recognized as the wolf, which belongs to the family Canidae.  
Head Entity: Canis  
Tail Entity: genus  

Relation: taxon rank  
Context: In the classification of plants, the family Rosaceae encompasses a variety of species, including roses and strawberries, which are categorized under the order Rosales.  
Head Entity: Rosaceae  
Tail Entity: family  
#### field of work ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "field of work" means specialization of a person or organization.
Here is an example:
Relation: field of work
Context: he later collaborated with vito volterra and explored laplace transforms and other parts of functional analysis .
Head Entity: vito volterra
Tail Entity: functional analysis
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "field of work":
Relation: field of work  
Context: Marie Curie conducted pioneering research on radioactivity, which laid the groundwork for advancements in nuclear physics.  
Head Entity: Marie Curie  
Tail Entity: nuclear physics  

Relation: field of work  
Context: The company specializes in artificial intelligence and machine learning, providing innovative solutions for various industries.  
Head Entity: The company  
Tail Entity: artificial intelligence  
#### licensed to broadcast to ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "licensed to broadcast to" means place that a radio/TV station is licensed/required to broadcast to.
Here is an example:
Relation: licensed to broadcast to
Context: wjet erie , pennsylvania was his first official radio job outside of duties performed in the u.s. air force .
Head Entity: wjet
Tail Entity: erie , pennsylvania
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "licensed to broadcast to":
Relation: licensed to broadcast to  
Context: wxyz television is the primary news station serving the city of springfield, known for its local coverage.  
Head Entity: wxyz  
Tail Entity: springfield  

Relation: licensed to broadcast to  
Context: kqed is a public television station that provides educational programming to the residents of san francisco.  
Head Entity: kqed  
Tail Entity: san francisco  
#### constellation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "constellation" means the area of the celestial sphere of which the subject is a part.
Here is an example:
Relation: constellation
Context: ngc 473 is a lenticular galaxy in the constellation of pisces .
Head Entity: ngc 473
Tail Entity: pisces
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "constellation":
Relation: constellation  
Context: the orion nebula is located in the constellation of orion.  
Head Entity: orion nebula  
Tail Entity: orion  

Relation: constellation  
Context: the star sirius is part of the constellation canis major.  
Head Entity: sirius  
Tail Entity: canis major  
#### platform ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "platform" means platform for which a work was developed or released, or the specific platform version of a software product.
Here is an example:
Relation: platform
Context: " toy soldiers : war chest " was released on august 11 , 2015 for windows , playstation 4 and xbox one .
Head Entity: toy soldiers : war chest
Tail Entity: xbox one
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "platform":
Relation: platform  
Context: "The Legend of Zelda: Breath of the Wild" was launched on March 3, 2017 for the Nintendo Switch and Wii U.  
Head Entity: The Legend of Zelda: Breath of the Wild  
Tail Entity: Nintendo Switch  

Relation: platform  
Context: "Cyberpunk 2077" was made available on December 10, 2020 for PC, PlayStation 4, and Xbox One.  
Head Entity: Cyberpunk 2077  
Tail Entity: PlayStation 4  
#### place served by transport hub ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "place served by transport hub" means territorial entity or entities served by this transport hub (airport, train station, etc.).
Here is an example:
Relation: place served by transport hub
Context: services to the island run from san juan 's luis muñoz marín international airport , ceiba airport or isla grande airport ( 20- to 30-minute flight ) .
Head Entity: isla grande airport
Tail Entity: san juan
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "place served by transport hub":
Relation: place served by transport hub  
Context: The city of Barcelona is well-connected to various destinations through the El Prat Airport, which offers flights to numerous European cities.  
Head Entity: El Prat Airport  
Tail Entity: Barcelona  

Relation: place served by transport hub  
Context: The central train station in Zurich provides access to multiple Swiss cities and international destinations, making it a key transport hub in the region.  
Head Entity: Zurich central train station  
Tail Entity: Zurich  
MemoryTrain:  epoch 15, batch     0 | loss: 4.6825452MemoryTrain:  epoch 15, batch     1 | loss: 3.1253753MemoryTrain:  epoch 15, batch     2 | loss: 4.0905939MemoryTrain:  epoch 15, batch     3 | loss: 2.8884979MemoryTrain:  epoch 15, batch     4 | loss: 5.6405001MemoryTrain:  epoch 15, batch     5 | loss: 6.3106184MemoryTrain:  epoch 15, batch     6 | loss: 5.9354111MemoryTrain:  epoch  7, batch     7 | loss: 6.8693369MemoryTrain:  epoch 15, batch     0 | loss: 3.2180404MemoryTrain:  epoch 15, batch     1 | loss: 3.6743142MemoryTrain:  epoch 15, batch     2 | loss: 4.8571061MemoryTrain:  epoch 15, batch     3 | loss: 2.9035300MemoryTrain:  epoch 15, batch     4 | loss: 5.3904605MemoryTrain:  epoch 15, batch     5 | loss: 2.5541000MemoryTrain:  epoch 15, batch     6 | loss: 2.7842962MemoryTrain:  epoch  7, batch     7 | loss: 6.0509376MemoryTrain:  epoch 15, batch     0 | loss: 2.5424208MemoryTrain:  epoch 15, batch     1 | loss: 2.5136014MemoryTrain:  epoch 15, batch     2 | loss: 2.3814365MemoryTrain:  epoch 15, batch     3 | loss: 3.0296695MemoryTrain:  epoch 15, batch     4 | loss: 5.2618615MemoryTrain:  epoch 15, batch     5 | loss: 3.1258872MemoryTrain:  epoch 15, batch     6 | loss: 3.2517195MemoryTrain:  epoch  7, batch     7 | loss: 1.9591562MemoryTrain:  epoch 15, batch     0 | loss: 2.5812501MemoryTrain:  epoch 15, batch     1 | loss: 2.4510260MemoryTrain:  epoch 15, batch     2 | loss: 1.9873117MemoryTrain:  epoch 15, batch     3 | loss: 4.7139864MemoryTrain:  epoch 15, batch     4 | loss: 2.1070782MemoryTrain:  epoch 15, batch     5 | loss: 5.5693712MemoryTrain:  epoch 15, batch     6 | loss: 2.6116002MemoryTrain:  epoch  7, batch     7 | loss: 1.7349774MemoryTrain:  epoch 15, batch     0 | loss: 2.4946359MemoryTrain:  epoch 15, batch     1 | loss: 4.9627105MemoryTrain:  epoch 15, batch     2 | loss: 5.5863967MemoryTrain:  epoch 15, batch     3 | loss: 2.8757094MemoryTrain:  epoch 15, batch     4 | loss: 1.9061912MemoryTrain:  epoch 15, batch     5 | loss: 3.5632896MemoryTrain:  epoch 15, batch     6 | loss: 1.7064021MemoryTrain:  epoch  7, batch     7 | loss: 1.3995911MemoryTrain:  epoch 15, batch     0 | loss: 2.7440347MemoryTrain:  epoch 15, batch     1 | loss: 6.5353255MemoryTrain:  epoch 15, batch     2 | loss: 3.1885713MemoryTrain:  epoch 15, batch     3 | loss: 2.3136517MemoryTrain:  epoch 15, batch     4 | loss: 4.9811951MemoryTrain:  epoch 15, batch     5 | loss: 2.3303982MemoryTrain:  epoch 15, batch     6 | loss: 4.1547536MemoryTrain:  epoch  7, batch     7 | loss: 5.5149960MemoryTrain:  epoch 15, batch     0 | loss: 2.1250140MemoryTrain:  epoch 15, batch     1 | loss: 5.1896517MemoryTrain:  epoch 15, batch     2 | loss: 2.4379167MemoryTrain:  epoch 15, batch     3 | loss: 2.1228897MemoryTrain:  epoch 15, batch     4 | loss: 1.6531169MemoryTrain:  epoch 15, batch     5 | loss: 1.8275344MemoryTrain:  epoch 15, batch     6 | loss: 3.2776534MemoryTrain:  epoch  7, batch     7 | loss: 1.3759793MemoryTrain:  epoch 15, batch     0 | loss: 2.4268903MemoryTrain:  epoch 15, batch     1 | loss: 4.2544921MemoryTrain:  epoch 15, batch     2 | loss: 4.3454340MemoryTrain:  epoch 15, batch     3 | loss: 1.6965301MemoryTrain:  epoch 15, batch     4 | loss: 2.1588780MemoryTrain:  epoch 15, batch     5 | loss: 4.3788842MemoryTrain:  epoch 15, batch     6 | loss: 4.5308769MemoryTrain:  epoch  7, batch     7 | loss: 1.2646389MemoryTrain:  epoch 15, batch     0 | loss: 2.4425839MemoryTrain:  epoch 15, batch     1 | loss: 2.4057322MemoryTrain:  epoch 15, batch     2 | loss: 2.1670066MemoryTrain:  epoch 15, batch     3 | loss: 4.6518802MemoryTrain:  epoch 15, batch     4 | loss: 2.9962825MemoryTrain:  epoch 15, batch     5 | loss: 2.4798365MemoryTrain:  epoch 15, batch     6 | loss: 1.8319956MemoryTrain:  epoch  7, batch     7 | loss: 1.3679033MemoryTrain:  epoch 15, batch     0 | loss: 4.3211447MemoryTrain:  epoch 15, batch     1 | loss: 2.4262303MemoryTrain:  epoch 15, batch     2 | loss: 1.6088137MemoryTrain:  epoch 15, batch     3 | loss: 2.1098845MemoryTrain:  epoch 15, batch     4 | loss: 3.1027678MemoryTrain:  epoch 15, batch     5 | loss: 5.9729351MemoryTrain:  epoch 15, batch     6 | loss: 2.2023431MemoryTrain:  epoch  7, batch     7 | loss: 4.0715667
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 91.67%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 90.62%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 91.25%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 92.71%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 90.18%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 88.28%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 86.11%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 85.00%   [EVAL] batch:   10 | acc: 75.00%,  total acc: 84.09%   [EVAL] batch:   11 | acc: 62.50%,  total acc: 82.29%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 81.73%   [EVAL] batch:   13 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 80.42%   [EVAL] batch:   15 | acc: 50.00%,  total acc: 78.52%   [EVAL] batch:   16 | acc: 43.75%,  total acc: 76.47%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 75.69%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 75.00%   [EVAL] batch:   19 | acc: 56.25%,  total acc: 74.06%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 74.70%   [EVAL] batch:   21 | acc: 62.50%,  total acc: 74.15%   [EVAL] batch:   22 | acc: 75.00%,  total acc: 74.18%   [EVAL] batch:   23 | acc: 68.75%,  total acc: 73.96%   [EVAL] batch:   24 | acc: 43.75%,  total acc: 72.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 73.80%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 74.77%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 75.67%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 76.51%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 77.29%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 78.02%   [EVAL] batch:   31 | acc: 81.25%,  total acc: 78.12%   [EVAL] batch:   32 | acc: 81.25%,  total acc: 78.22%   [EVAL] batch:   33 | acc: 81.25%,  total acc: 78.31%   [EVAL] batch:   34 | acc: 87.50%,  total acc: 78.57%   [EVAL] batch:   35 | acc: 68.75%,  total acc: 78.30%   [EVAL] batch:   36 | acc: 75.00%,  total acc: 78.21%   [EVAL] batch:   37 | acc: 81.25%,  total acc: 78.29%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 78.85%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 79.38%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 79.88%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 80.36%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 80.81%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 81.11%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 81.53%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 81.93%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 82.31%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 82.68%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 83.04%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 83.38%   [EVAL] batch:   50 | acc: 87.50%,  total acc: 83.46%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 83.65%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 83.84%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 84.14%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 84.20%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 84.49%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 84.65%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 84.81%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 84.96%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 85.21%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 85.45%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 85.58%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 85.02%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 84.38%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 83.75%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:    6 | acc: 50.00%,  total acc: 79.46%   [EVAL] batch:    7 | acc: 62.50%,  total acc: 77.34%   [EVAL] batch:    8 | acc: 75.00%,  total acc: 77.08%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 75.62%   [EVAL] batch:   10 | acc: 43.75%,  total acc: 72.73%   [EVAL] batch:   11 | acc: 56.25%,  total acc: 71.35%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 71.63%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 73.21%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 74.58%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 76.17%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 77.57%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 78.47%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 79.61%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 79.38%   [EVAL] batch:   20 | acc: 81.25%,  total acc: 79.46%   [EVAL] batch:   21 | acc: 75.00%,  total acc: 79.26%   [EVAL] batch:   22 | acc: 75.00%,  total acc: 79.08%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 79.43%   [EVAL] batch:   24 | acc: 62.50%,  total acc: 78.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 79.57%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 80.32%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 80.80%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 81.47%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 82.08%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 82.66%   [EVAL] batch:   31 | acc: 87.50%,  total acc: 82.81%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 83.14%   [EVAL] batch:   33 | acc: 50.00%,  total acc: 82.17%   [EVAL] batch:   34 | acc: 68.75%,  total acc: 81.79%   [EVAL] batch:   35 | acc: 75.00%,  total acc: 81.60%   [EVAL] batch:   36 | acc: 75.00%,  total acc: 81.42%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 81.58%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 82.05%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 82.50%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 82.93%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 83.33%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 83.58%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 83.81%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 83.89%   [EVAL] batch:   45 | acc: 81.25%,  total acc: 83.83%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 83.78%   [EVAL] batch:   47 | acc: 81.25%,  total acc: 83.72%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 84.06%   [EVAL] batch:   49 | acc: 68.75%,  total acc: 83.75%   [EVAL] batch:   50 | acc: 81.25%,  total acc: 83.70%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 83.77%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 83.96%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 83.91%   [EVAL] batch:   54 | acc: 56.25%,  total acc: 83.41%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 83.59%   [EVAL] batch:   56 | acc: 87.50%,  total acc: 83.66%   [EVAL] batch:   57 | acc: 68.75%,  total acc: 83.41%   [EVAL] batch:   58 | acc: 81.25%,  total acc: 83.37%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 83.54%   [EVAL] batch:   60 | acc: 93.75%,  total acc: 83.71%   [EVAL] batch:   61 | acc: 75.00%,  total acc: 83.57%   [EVAL] batch:   62 | acc: 100.00%,  total acc: 83.83%   [EVAL] batch:   63 | acc: 100.00%,  total acc: 84.08%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 84.33%   [EVAL] batch:   65 | acc: 93.75%,  total acc: 84.47%   [EVAL] batch:   66 | acc: 100.00%,  total acc: 84.70%   [EVAL] batch:   67 | acc: 93.75%,  total acc: 84.83%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 85.05%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 85.27%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 85.48%   [EVAL] batch:   71 | acc: 81.25%,  total acc: 85.42%   [EVAL] batch:   72 | acc: 93.75%,  total acc: 85.53%   [EVAL] batch:   73 | acc: 87.50%,  total acc: 85.56%   [EVAL] batch:   74 | acc: 93.75%,  total acc: 85.67%   [EVAL] batch:   75 | acc: 93.75%,  total acc: 85.77%   [EVAL] batch:   76 | acc: 81.25%,  total acc: 85.71%   [EVAL] batch:   77 | acc: 81.25%,  total acc: 85.66%   [EVAL] batch:   78 | acc: 81.25%,  total acc: 85.60%   [EVAL] batch:   79 | acc: 81.25%,  total acc: 85.55%   [EVAL] batch:   80 | acc: 93.75%,  total acc: 85.65%   [EVAL] batch:   81 | acc: 56.25%,  total acc: 85.29%   [EVAL] batch:   82 | acc: 62.50%,  total acc: 85.02%   [EVAL] batch:   83 | acc: 68.75%,  total acc: 84.82%   [EVAL] batch:   84 | acc: 68.75%,  total acc: 84.63%   [EVAL] batch:   85 | acc: 68.75%,  total acc: 84.45%   [EVAL] batch:   86 | acc: 56.25%,  total acc: 84.12%   [EVAL] batch:   87 | acc: 81.25%,  total acc: 84.09%   [EVAL] batch:   88 | acc: 81.25%,  total acc: 84.06%   [EVAL] batch:   89 | acc: 87.50%,  total acc: 84.10%   [EVAL] batch:   90 | acc: 87.50%,  total acc: 84.13%   [EVAL] batch:   91 | acc: 75.00%,  total acc: 84.04%   [EVAL] batch:   92 | acc: 75.00%,  total acc: 83.94%   [EVAL] batch:   93 | acc: 75.00%,  total acc: 83.84%   [EVAL] batch:   94 | acc: 68.75%,  total acc: 83.68%   [EVAL] batch:   95 | acc: 50.00%,  total acc: 83.33%   [EVAL] batch:   96 | acc: 68.75%,  total acc: 83.18%   [EVAL] batch:   97 | acc: 43.75%,  total acc: 82.78%   [EVAL] batch:   98 | acc: 62.50%,  total acc: 82.58%   [EVAL] batch:   99 | acc: 50.00%,  total acc: 82.25%   [EVAL] batch:  100 | acc: 56.25%,  total acc: 81.99%   [EVAL] batch:  101 | acc: 81.25%,  total acc: 81.99%   [EVAL] batch:  102 | acc: 75.00%,  total acc: 81.92%   [EVAL] batch:  103 | acc: 87.50%,  total acc: 81.97%   [EVAL] batch:  104 | acc: 75.00%,  total acc: 81.90%   [EVAL] batch:  105 | acc: 75.00%,  total acc: 81.84%   [EVAL] batch:  106 | acc: 68.75%,  total acc: 81.72%   [EVAL] batch:  107 | acc: 37.50%,  total acc: 81.31%   [EVAL] batch:  108 | acc: 43.75%,  total acc: 80.96%   [EVAL] batch:  109 | acc: 56.25%,  total acc: 80.74%   [EVAL] batch:  110 | acc: 43.75%,  total acc: 80.41%   [EVAL] batch:  111 | acc: 50.00%,  total acc: 80.13%   [EVAL] batch:  112 | acc: 81.25%,  total acc: 80.14%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 80.32%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 80.49%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 80.66%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 80.82%   [EVAL] batch:  117 | acc: 100.00%,  total acc: 80.99%   [EVAL] batch:  118 | acc: 100.00%,  total acc: 81.14%   [EVAL] batch:  119 | acc: 100.00%,  total acc: 81.30%   [EVAL] batch:  120 | acc: 100.00%,  total acc: 81.46%   [EVAL] batch:  121 | acc: 100.00%,  total acc: 81.61%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 81.76%   [EVAL] batch:  123 | acc: 100.00%,  total acc: 81.91%   [EVAL] batch:  124 | acc: 100.00%,  total acc: 82.05%   [EVAL] batch:  125 | acc: 37.50%,  total acc: 81.70%   [EVAL] batch:  126 | acc: 50.00%,  total acc: 81.45%   [EVAL] batch:  127 | acc: 56.25%,  total acc: 81.25%   [EVAL] batch:  128 | acc: 62.50%,  total acc: 81.10%   [EVAL] batch:  129 | acc: 62.50%,  total acc: 80.96%   [EVAL] batch:  130 | acc: 75.00%,  total acc: 80.92%   [EVAL] batch:  131 | acc: 87.50%,  total acc: 80.97%   [EVAL] batch:  132 | acc: 93.75%,  total acc: 81.06%   [EVAL] batch:  133 | acc: 81.25%,  total acc: 81.06%   [EVAL] batch:  134 | acc: 93.75%,  total acc: 81.16%   [EVAL] batch:  135 | acc: 100.00%,  total acc: 81.30%   [EVAL] batch:  136 | acc: 87.50%,  total acc: 81.34%   [EVAL] batch:  137 | acc: 75.00%,  total acc: 81.30%   [EVAL] batch:  138 | acc: 31.25%,  total acc: 80.94%   [EVAL] batch:  139 | acc: 37.50%,  total acc: 80.62%   [EVAL] batch:  140 | acc: 18.75%,  total acc: 80.19%   [EVAL] batch:  141 | acc: 50.00%,  total acc: 79.97%   [EVAL] batch:  142 | acc: 43.75%,  total acc: 79.72%   [EVAL] batch:  143 | acc: 37.50%,  total acc: 79.43%   [EVAL] batch:  144 | acc: 100.00%,  total acc: 79.57%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 79.71%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 79.80%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 79.94%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 80.08%   [EVAL] batch:  149 | acc: 100.00%,  total acc: 80.21%   [EVAL] batch:  150 | acc: 31.25%,  total acc: 79.88%   [EVAL] batch:  151 | acc: 12.50%,  total acc: 79.44%   [EVAL] batch:  152 | acc: 25.00%,  total acc: 79.08%   [EVAL] batch:  153 | acc: 62.50%,  total acc: 78.98%   [EVAL] batch:  154 | acc: 12.50%,  total acc: 78.55%   [EVAL] batch:  155 | acc: 37.50%,  total acc: 78.29%   [EVAL] batch:  156 | acc: 87.50%,  total acc: 78.34%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 78.44%   [EVAL] batch:  158 | acc: 100.00%,  total acc: 78.58%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 78.67%   [EVAL] batch:  160 | acc: 87.50%,  total acc: 78.73%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 78.82%   [EVAL] batch:  162 | acc: 75.00%,  total acc: 78.80%   [EVAL] batch:  163 | acc: 81.25%,  total acc: 78.81%   [EVAL] batch:  164 | acc: 68.75%,  total acc: 78.75%   [EVAL] batch:  165 | acc: 62.50%,  total acc: 78.65%   [EVAL] batch:  166 | acc: 87.50%,  total acc: 78.71%   [EVAL] batch:  167 | acc: 81.25%,  total acc: 78.72%   [EVAL] batch:  168 | acc: 87.50%,  total acc: 78.77%   [EVAL] batch:  169 | acc: 87.50%,  total acc: 78.82%   [EVAL] batch:  170 | acc: 93.75%,  total acc: 78.91%   [EVAL] batch:  171 | acc: 93.75%,  total acc: 79.00%   [EVAL] batch:  172 | acc: 81.25%,  total acc: 79.01%   [EVAL] batch:  173 | acc: 93.75%,  total acc: 79.09%   [EVAL] batch:  174 | acc: 93.75%,  total acc: 79.18%   [EVAL] batch:  175 | acc: 68.75%,  total acc: 79.12%   [EVAL] batch:  176 | acc: 56.25%,  total acc: 78.99%   [EVAL] batch:  177 | acc: 56.25%,  total acc: 78.86%   [EVAL] batch:  178 | acc: 62.50%,  total acc: 78.77%   [EVAL] batch:  179 | acc: 75.00%,  total acc: 78.75%   [EVAL] batch:  180 | acc: 68.75%,  total acc: 78.69%   [EVAL] batch:  181 | acc: 75.00%,  total acc: 78.67%   [EVAL] batch:  182 | acc: 87.50%,  total acc: 78.72%   [EVAL] batch:  183 | acc: 93.75%,  total acc: 78.80%   [EVAL] batch:  184 | acc: 81.25%,  total acc: 78.82%   [EVAL] batch:  185 | acc: 93.75%,  total acc: 78.90%   [EVAL] batch:  186 | acc: 81.25%,  total acc: 78.91%   [EVAL] batch:  187 | acc: 93.75%,  total acc: 78.99%   [EVAL] batch:  188 | acc: 93.75%,  total acc: 79.07%   [EVAL] batch:  189 | acc: 87.50%,  total acc: 79.11%   [EVAL] batch:  190 | acc: 100.00%,  total acc: 79.22%   [EVAL] batch:  191 | acc: 81.25%,  total acc: 79.23%   [EVAL] batch:  192 | acc: 100.00%,  total acc: 79.34%   [EVAL] batch:  193 | acc: 81.25%,  total acc: 79.35%   [EVAL] batch:  194 | acc: 75.00%,  total acc: 79.33%   [EVAL] batch:  195 | acc: 75.00%,  total acc: 79.30%   [EVAL] batch:  196 | acc: 81.25%,  total acc: 79.31%   [EVAL] batch:  197 | acc: 68.75%,  total acc: 79.26%   [EVAL] batch:  198 | acc: 68.75%,  total acc: 79.21%   [EVAL] batch:  199 | acc: 62.50%,  total acc: 79.12%   [EVAL] batch:  200 | acc: 75.00%,  total acc: 79.10%   [EVAL] batch:  201 | acc: 81.25%,  total acc: 79.12%   [EVAL] batch:  202 | acc: 68.75%,  total acc: 79.06%   [EVAL] batch:  203 | acc: 31.25%,  total acc: 78.83%   [EVAL] batch:  204 | acc: 50.00%,  total acc: 78.69%   [EVAL] batch:  205 | acc: 62.50%,  total acc: 78.61%   [EVAL] batch:  206 | acc: 62.50%,  total acc: 78.53%   [EVAL] batch:  207 | acc: 75.00%,  total acc: 78.52%   [EVAL] batch:  208 | acc: 75.00%,  total acc: 78.50%   [EVAL] batch:  209 | acc: 62.50%,  total acc: 78.42%   [EVAL] batch:  210 | acc: 75.00%,  total acc: 78.41%   [EVAL] batch:  211 | acc: 43.75%,  total acc: 78.24%   [EVAL] batch:  212 | acc: 87.50%,  total acc: 78.29%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 78.39%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 78.49%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 78.59%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 78.69%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 78.78%   [EVAL] batch:  218 | acc: 100.00%,  total acc: 78.88%   [EVAL] batch:  219 | acc: 68.75%,  total acc: 78.84%   [EVAL] batch:  220 | acc: 87.50%,  total acc: 78.87%   [EVAL] batch:  221 | acc: 81.25%,  total acc: 78.89%   [EVAL] batch:  222 | acc: 68.75%,  total acc: 78.84%   [EVAL] batch:  223 | acc: 87.50%,  total acc: 78.88%   [EVAL] batch:  224 | acc: 62.50%,  total acc: 78.81%   [EVAL] batch:  225 | acc: 100.00%,  total acc: 78.90%   [EVAL] batch:  226 | acc: 100.00%,  total acc: 78.99%   [EVAL] batch:  227 | acc: 100.00%,  total acc: 79.08%   [EVAL] batch:  228 | acc: 100.00%,  total acc: 79.18%   [EVAL] batch:  229 | acc: 100.00%,  total acc: 79.27%   [EVAL] batch:  230 | acc: 93.75%,  total acc: 79.33%   [EVAL] batch:  231 | acc: 100.00%,  total acc: 79.42%   [EVAL] batch:  232 | acc: 100.00%,  total acc: 79.51%   [EVAL] batch:  233 | acc: 100.00%,  total acc: 79.59%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 79.68%   [EVAL] batch:  235 | acc: 100.00%,  total acc: 79.77%   [EVAL] batch:  236 | acc: 100.00%,  total acc: 79.85%   [EVAL] batch:  237 | acc: 93.75%,  total acc: 79.91%   [EVAL] batch:  238 | acc: 87.50%,  total acc: 79.94%   [EVAL] batch:  239 | acc: 93.75%,  total acc: 80.00%   [EVAL] batch:  240 | acc: 100.00%,  total acc: 80.08%   [EVAL] batch:  241 | acc: 93.75%,  total acc: 80.14%   [EVAL] batch:  242 | acc: 93.75%,  total acc: 80.20%   [EVAL] batch:  243 | acc: 93.75%,  total acc: 80.25%   [EVAL] batch:  244 | acc: 100.00%,  total acc: 80.33%   [EVAL] batch:  245 | acc: 87.50%,  total acc: 80.36%   [EVAL] batch:  246 | acc: 100.00%,  total acc: 80.44%   [EVAL] batch:  247 | acc: 100.00%,  total acc: 80.52%   [EVAL] batch:  248 | acc: 100.00%,  total acc: 80.60%   [EVAL] batch:  249 | acc: 93.75%,  total acc: 80.65%   
cur_acc:  ['0.9524', '0.8135', '0.7579', '0.8502']
his_acc:  ['0.9524', '0.8640', '0.8165', '0.8065']
CurrentTrain: epoch 15, batch     0 | loss: 12.2267269CurrentTrain: epoch 15, batch     1 | loss: 19.8353560CurrentTrain: epoch 15, batch     2 | loss: 19.5035783CurrentTrain: epoch  1, batch     3 | loss: 11.2942790CurrentTrain: epoch 15, batch     0 | loss: 15.7713430CurrentTrain: epoch 15, batch     1 | loss: 8.9816947CurrentTrain: epoch 15, batch     2 | loss: 14.8750616CurrentTrain: epoch  1, batch     3 | loss: 9.1263961CurrentTrain: epoch 15, batch     0 | loss: 12.1308839CurrentTrain: epoch 15, batch     1 | loss: 11.2074697CurrentTrain: epoch 15, batch     2 | loss: 14.3340344CurrentTrain: epoch  1, batch     3 | loss: 7.5157000CurrentTrain: epoch 15, batch     0 | loss: 12.2648118CurrentTrain: epoch 15, batch     1 | loss: 13.2776952CurrentTrain: epoch 15, batch     2 | loss: 14.0690009CurrentTrain: epoch  1, batch     3 | loss: 5.3399470CurrentTrain: epoch 15, batch     0 | loss: 10.5256817CurrentTrain: epoch 15, batch     1 | loss: 8.7770087CurrentTrain: epoch 15, batch     2 | loss: 11.3997443CurrentTrain: epoch  1, batch     3 | loss: 8.8046087CurrentTrain: epoch 15, batch     0 | loss: 10.4971112CurrentTrain: epoch 15, batch     1 | loss: 18.8737710CurrentTrain: epoch 15, batch     2 | loss: 11.5428573CurrentTrain: epoch  1, batch     3 | loss: 8.7459762CurrentTrain: epoch 15, batch     0 | loss: 11.5233739CurrentTrain: epoch 15, batch     1 | loss: 15.2841025CurrentTrain: epoch 15, batch     2 | loss: 15.4986511CurrentTrain: epoch  1, batch     3 | loss: 8.4606297CurrentTrain: epoch 15, batch     0 | loss: 9.8613028CurrentTrain: epoch 15, batch     1 | loss: 10.1104290CurrentTrain: epoch 15, batch     2 | loss: 10.1055432CurrentTrain: epoch  1, batch     3 | loss: 6.9376242CurrentTrain: epoch 15, batch     0 | loss: 8.0790491CurrentTrain: epoch 15, batch     1 | loss: 11.7109176CurrentTrain: epoch 15, batch     2 | loss: 11.2089340CurrentTrain: epoch  1, batch     3 | loss: 22.2280926CurrentTrain: epoch 15, batch     0 | loss: 10.6220237CurrentTrain: epoch 15, batch     1 | loss: 12.0092082CurrentTrain: epoch 15, batch     2 | loss: 9.4262863CurrentTrain: epoch  1, batch     3 | loss: 6.0863897
#### contains administrative territorial entity ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "contains administrative territorial entity" means direct subdivisions of an administrative territorial entity.
Here is an example:
Relation: contains administrative territorial entity
Context: the cantons of créteil are administrative divisions of the val - de - marne department , île - de - france region , northern france .
Head Entity: île - de - france
Tail Entity: val - de - marne
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "contains administrative territorial entity":
Relation: contains administrative territorial entity  
Context: The provinces of Canada are the primary administrative divisions of the country, with Ontario being one of the most populous provinces.  
Head Entity: Canada  
Tail Entity: Ontario  

Relation: contains administrative territorial entity  
Context: The states of the United States are divided into various counties, with Los Angeles County being one of the largest in California.  
Head Entity: California  
Tail Entity: Los Angeles County  
#### said to be the same as ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "said to be the same as" means this item is said to be the same as that item, but it's uncertain or disputed.
Here is an example:
Relation: said to be the same as
Context: enzo is an italian given name derivative of heinz , a form of henry .
Head Entity: henry
Tail Entity: heinz
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "said to be the same as":
Relation: said to be the same as  
Context: The two species, Canis lupus and Canis familiaris, are often said to be the same as each other due to their close genetic relationship.  
Head Entity: Canis lupus  
Tail Entity: Canis familiaris  

Relation: said to be the same as  
Context: In some circles, the terms "soda" and "pop" are said to be the same as each other, though this varies by region.  
Head Entity: soda  
Tail Entity: pop  
#### winner ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "winner" means winner of a competition or similar event.
Here is an example:
Relation: winner
Context: the filming was primarily held at pollachi . it fetched pawan kalyan the filmfare award for best actor – telugu award at 59th filmfare awards south .
Head Entity: filmfare award for best actor – telugu
Tail Entity: pawan kalyan
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "winner":
Relation: winner  
Context: in the thrilling finale of the championship, the underdog team triumphed, securing the title and making history as they celebrated their victory over the reigning champions. This remarkable achievement earned them the prestigious trophy at the national sports awards.  
Head Entity: national sports awards  
Tail Entity: underdog team  

Relation: winner  
Context: during the annual music competition, the judges were blown away by her performance, leading to her being crowned the winner and receiving the coveted golden trophy for best vocalist.  
Head Entity: golden trophy for best vocalist  
Tail Entity: her  
#### military rank ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "military rank" means military rank achieved by a person, or military rank associated with a position.
Here is an example:
Relation: military rank
Context: the calais region was defended by the 15th army under " generaloberst " ( colonel general ) hans von salmuth , and normandy by the 7th army commanded by " generaloberst " friedrich dollmann .
Head Entity: hans von salmuth
Tail Entity: generaloberst
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "military rank":
Relation: military rank  
Context: during the battle of stalingrad, the soviet forces were led by general vasily chuikov, who held the rank of major general, while the german troops were commanded by general friedrich paulus.  
Head Entity: vasily chuikov  
Tail Entity: major general  

Relation: military rank  
Context: admiral horatio nelson was a key figure in the british navy, famously known for his rank of vice admiral during the napoleonic wars.  
Head Entity: horatio nelson  
Tail Entity: vice admiral  
#### publisher ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "publisher" means organization or person responsible for publishing books, periodicals, printed music, podcasts, games or software.
Here is an example:
Relation: publisher
Context: there are seven different nations that are allied or in conflict : prior to its north american release , " vanguard bandits " was titled " detonator gauntlet " by working designs .
Head Entity: vanguard bandits
Tail Entity: working designs
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "publisher":
Relation: publisher  
Context: The latest novel by the acclaimed author was released by Penguin Random House, a well-known publishing house in the industry.  
Head Entity: latest novel  
Tail Entity: Penguin Random House  

Relation: publisher  
Context: After years of hard work, the game developer finally secured a deal with Electronic Arts to publish their new sports game.  
Head Entity: new sports game  
Tail Entity: Electronic Arts  
#### followed by ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "followed by" means immediately following item in a series of which the subject is a part.
Here is an example:
Relation: followed by
Context: " muzai moratorium " and " shōso strip " were released on vinyl in 2008 , as a part of her 10-year anniversary celebrations .
Head Entity: muzai moratorium
Tail Entity: shōso strip
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "followed by":
Relation: followed by  
Context: The first chapter of the book introduces the main character, while the second chapter follows with a detailed backstory.  
Head Entity: first chapter  
Tail Entity: second chapter  

Relation: followed by  
Context: In the concert lineup, the opening act performed a lively set, followed by the headliner who captivated the audience with their hits.  
Head Entity: opening act  
Tail Entity: headliner  
#### work location ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "work location" means location where persons or organisations were actively participating in employment, business or other work.
Here is an example:
Relation: work location
Context: he was also associated with robert wilkinson in producing " londina illustrata " , an illustrated account of ancient buildings in london and westminster in two volumes ( 1819–25 ) .
Head Entity: robert wilkinson
Tail Entity: london
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "work location":
Relation: work location  
Context: The headquarters of the company is situated in the bustling city of San Francisco, where it has been operating since its inception in 2010.  
Head Entity: the company  
Tail Entity: San Francisco  

Relation: work location  
Context: During her tenure at the university, she conducted groundbreaking research in neuroscience, primarily based in the labs located in Cambridge.  
Head Entity: she  
Tail Entity: Cambridge  
#### occupation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "occupation" means occupation of a person.
Here is an example:
Relation: occupation
Context: andrei droznin ( russian : андрей дрознин ) is a russian theatre director and movement coach .
Head Entity: andrei droznin
Tail Entity: theatre director
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "occupation":
Relation: occupation  
Context: dr. emily thompson is a renowned biochemist known for her groundbreaking research in cancer treatment.  
Head Entity: dr. emily thompson  
Tail Entity: biochemist  

Relation: occupation  
Context: john smith, a talented software engineer, has been working at tech innovations for over five years.  
Head Entity: john smith  
Tail Entity: software engineer  
#### heritage designation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "heritage designation" means heritage designation of a cultural or natural site.
Here is an example:
Relation: heritage designation
Context: altamaha town , bluffton historic district , church of the cross , and rose hill plantation house are listed on the national register of historic places .
Head Entity: altamaha town
Tail Entity: national register of historic places
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "heritage designation":
Relation: heritage designation  
Context: The Great Wall of China is recognized as a UNESCO World Heritage Site due to its historical significance and architectural grandeur.  
Head Entity: Great Wall of China  
Tail Entity: UNESCO World Heritage Site  

Relation: heritage designation  
Context: The ancient city of Petra in Jordan is designated as a World Heritage Site, attracting tourists from around the globe.  
Head Entity: Petra  
Tail Entity: World Heritage Site  
#### voice type ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "voice type" means person's voice type.
Here is an example:
Relation: voice type
Context: beverly bower ( september 30 , 1925 – march 24 , 2002 ) was an american operatic soprano who had an active international opera career from the mid-1950s through the early 1970s .
Head Entity: beverly bower
Tail Entity: soprano
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "voice type":
Relation: voice type  
Context: luciano pavarotti ( october 12 , 1935 – september 6 , 2007 ) was an italian tenor who gained worldwide fame for his televised concerts and media appearances.  
Head Entity: luciano pavarotti  
Tail Entity: tenor  

Relation: voice type  
Context: mariah carey ( born march 27 , 1969 ) is an american singer, songwriter, and actress known for her five-octave vocal range and her use of the whistle register.  
Head Entity: mariah carey  
Tail Entity: singer  
MemoryTrain:  epoch 15, batch     0 | loss: 4.0557455MemoryTrain:  epoch 15, batch     1 | loss: 3.3442134MemoryTrain:  epoch 15, batch     2 | loss: 2.8557218MemoryTrain:  epoch 15, batch     3 | loss: 3.4124938MemoryTrain:  epoch 15, batch     4 | loss: 3.9476402MemoryTrain:  epoch 15, batch     5 | loss: 3.6091899MemoryTrain:  epoch 15, batch     6 | loss: 4.3128926MemoryTrain:  epoch 15, batch     7 | loss: 5.2924158MemoryTrain:  epoch 15, batch     8 | loss: 3.3625450MemoryTrain:  epoch  5, batch     9 | loss: 9.1376945MemoryTrain:  epoch 15, batch     0 | loss: 2.8739253MemoryTrain:  epoch 15, batch     1 | loss: 4.2842560MemoryTrain:  epoch 15, batch     2 | loss: 3.3466439MemoryTrain:  epoch 15, batch     3 | loss: 3.5675801MemoryTrain:  epoch 15, batch     4 | loss: 2.4722748MemoryTrain:  epoch 15, batch     5 | loss: 2.3685026MemoryTrain:  epoch 15, batch     6 | loss: 2.8979182MemoryTrain:  epoch 15, batch     7 | loss: 3.5342296MemoryTrain:  epoch 15, batch     8 | loss: 2.9681137MemoryTrain:  epoch  5, batch     9 | loss: 9.3641222MemoryTrain:  epoch 15, batch     0 | loss: 2.5511048MemoryTrain:  epoch 15, batch     1 | loss: 3.3624838MemoryTrain:  epoch 15, batch     2 | loss: 2.7307516MemoryTrain:  epoch 15, batch     3 | loss: 4.5601991MemoryTrain:  epoch 15, batch     4 | loss: 2.2026642MemoryTrain:  epoch 15, batch     5 | loss: 2.1331071MemoryTrain:  epoch 15, batch     6 | loss: 5.1171308MemoryTrain:  epoch 15, batch     7 | loss: 4.7288176MemoryTrain:  epoch 15, batch     8 | loss: 3.2970455MemoryTrain:  epoch  5, batch     9 | loss: 9.4492811MemoryTrain:  epoch 15, batch     0 | loss: 5.5820161MemoryTrain:  epoch 15, batch     1 | loss: 2.3474444MemoryTrain:  epoch 15, batch     2 | loss: 2.0359210MemoryTrain:  epoch 15, batch     3 | loss: 2.7303166MemoryTrain:  epoch 15, batch     4 | loss: 2.6708371MemoryTrain:  epoch 15, batch     5 | loss: 2.3611509MemoryTrain:  epoch 15, batch     6 | loss: 4.5785181MemoryTrain:  epoch 15, batch     7 | loss: 2.0776821MemoryTrain:  epoch 15, batch     8 | loss: 2.6871314MemoryTrain:  epoch  5, batch     9 | loss: 14.4458286MemoryTrain:  epoch 15, batch     0 | loss: 4.8567747MemoryTrain:  epoch 15, batch     1 | loss: 4.4544512MemoryTrain:  epoch 15, batch     2 | loss: 2.2268665MemoryTrain:  epoch 15, batch     3 | loss: 2.2997584MemoryTrain:  epoch 15, batch     4 | loss: 3.6197608MemoryTrain:  epoch 15, batch     5 | loss: 3.5299867MemoryTrain:  epoch 15, batch     6 | loss: 2.4889530MemoryTrain:  epoch 15, batch     7 | loss: 2.9016543MemoryTrain:  epoch 15, batch     8 | loss: 4.8575963MemoryTrain:  epoch  5, batch     9 | loss: 8.3950930MemoryTrain:  epoch 15, batch     0 | loss: 1.6059096MemoryTrain:  epoch 15, batch     1 | loss: 6.5247764MemoryTrain:  epoch 15, batch     2 | loss: 2.7021923MemoryTrain:  epoch 15, batch     3 | loss: 1.8284818MemoryTrain:  epoch 15, batch     4 | loss: 4.6112734MemoryTrain:  epoch 15, batch     5 | loss: 2.9255900MemoryTrain:  epoch 15, batch     6 | loss: 3.0445028MemoryTrain:  epoch 15, batch     7 | loss: 1.8076422MemoryTrain:  epoch 15, batch     8 | loss: 2.1120971MemoryTrain:  epoch  5, batch     9 | loss: 9.1713259MemoryTrain:  epoch 15, batch     0 | loss: 4.1905025MemoryTrain:  epoch 15, batch     1 | loss: 1.8400796MemoryTrain:  epoch 15, batch     2 | loss: 2.3834728MemoryTrain:  epoch 15, batch     3 | loss: 1.7888932MemoryTrain:  epoch 15, batch     4 | loss: 2.2263664MemoryTrain:  epoch 15, batch     5 | loss: 4.3597980MemoryTrain:  epoch 15, batch     6 | loss: 1.7459759MemoryTrain:  epoch 15, batch     7 | loss: 4.7786384MemoryTrain:  epoch 15, batch     8 | loss: 2.1776085MemoryTrain:  epoch  5, batch     9 | loss: 13.8411262MemoryTrain:  epoch 15, batch     0 | loss: 4.1110565MemoryTrain:  epoch 15, batch     1 | loss: 1.7073063MemoryTrain:  epoch 15, batch     2 | loss: 1.4626502MemoryTrain:  epoch 15, batch     3 | loss: 2.2132454MemoryTrain:  epoch 15, batch     4 | loss: 2.1477544MemoryTrain:  epoch 15, batch     5 | loss: 2.7816454MemoryTrain:  epoch 15, batch     6 | loss: 1.6001608MemoryTrain:  epoch 15, batch     7 | loss: 2.5601122MemoryTrain:  epoch 15, batch     8 | loss: 2.8877573MemoryTrain:  epoch  5, batch     9 | loss: 7.8026866MemoryTrain:  epoch 15, batch     0 | loss: 2.0869808MemoryTrain:  epoch 15, batch     1 | loss: 1.4747755MemoryTrain:  epoch 15, batch     2 | loss: 1.4369353MemoryTrain:  epoch 15, batch     3 | loss: 4.1667605MemoryTrain:  epoch 15, batch     4 | loss: 1.4718491MemoryTrain:  epoch 15, batch     5 | loss: 2.6077713MemoryTrain:  epoch 15, batch     6 | loss: 1.9231762MemoryTrain:  epoch 15, batch     7 | loss: 3.7745682MemoryTrain:  epoch 15, batch     8 | loss: 2.5467279MemoryTrain:  epoch  5, batch     9 | loss: 8.1714502MemoryTrain:  epoch 15, batch     0 | loss: 1.8943209MemoryTrain:  epoch 15, batch     1 | loss: 3.9151629MemoryTrain:  epoch 15, batch     2 | loss: 2.3029244MemoryTrain:  epoch 15, batch     3 | loss: 2.2361405MemoryTrain:  epoch 15, batch     4 | loss: 2.8990705MemoryTrain:  epoch 15, batch     5 | loss: 1.4921658MemoryTrain:  epoch 15, batch     6 | loss: 1.8105341MemoryTrain:  epoch 15, batch     7 | loss: 2.6045476MemoryTrain:  epoch 15, batch     8 | loss: 2.0766079MemoryTrain:  epoch  5, batch     9 | loss: 8.0395964
[EVAL] batch:    0 | acc: 31.25%,  total acc: 31.25%   [EVAL] batch:    1 | acc: 31.25%,  total acc: 31.25%   [EVAL] batch:    2 | acc: 50.00%,  total acc: 37.50%   [EVAL] batch:    3 | acc: 31.25%,  total acc: 35.94%   [EVAL] batch:    4 | acc: 31.25%,  total acc: 35.00%   [EVAL] batch:    5 | acc: 37.50%,  total acc: 35.42%   [EVAL] batch:    6 | acc: 56.25%,  total acc: 38.39%   [EVAL] batch:    7 | acc: 68.75%,  total acc: 42.19%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 45.14%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 46.88%   [EVAL] batch:   10 | acc: 43.75%,  total acc: 46.59%   [EVAL] batch:   11 | acc: 68.75%,  total acc: 48.44%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 51.44%   [EVAL] batch:   13 | acc: 56.25%,  total acc: 51.79%   [EVAL] batch:   14 | acc: 56.25%,  total acc: 52.08%   [EVAL] batch:   15 | acc: 50.00%,  total acc: 51.95%   [EVAL] batch:   16 | acc: 68.75%,  total acc: 52.94%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 53.82%   [EVAL] batch:   18 | acc: 87.50%,  total acc: 55.59%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 57.19%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 58.93%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 60.80%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 62.23%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 63.28%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 64.25%   [EVAL] batch:   25 | acc: 81.25%,  total acc: 64.90%   [EVAL] batch:   26 | acc: 75.00%,  total acc: 65.28%   [EVAL] batch:   27 | acc: 87.50%,  total acc: 66.07%   [EVAL] batch:   28 | acc: 75.00%,  total acc: 66.38%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 66.88%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 67.54%   [EVAL] batch:   31 | acc: 62.50%,  total acc: 67.38%   [EVAL] batch:   32 | acc: 62.50%,  total acc: 67.23%   [EVAL] batch:   33 | acc: 18.75%,  total acc: 65.81%   [EVAL] batch:   34 | acc: 43.75%,  total acc: 65.18%   [EVAL] batch:   35 | acc: 25.00%,  total acc: 64.06%   [EVAL] batch:   36 | acc: 18.75%,  total acc: 62.84%   [EVAL] batch:   37 | acc: 75.00%,  total acc: 63.16%   [EVAL] batch:   38 | acc: 81.25%,  total acc: 63.62%   [EVAL] batch:   39 | acc: 87.50%,  total acc: 64.22%   [EVAL] batch:   40 | acc: 81.25%,  total acc: 64.63%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 65.33%   [EVAL] batch:   42 | acc: 75.00%,  total acc: 65.55%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 66.05%   [EVAL] batch:   44 | acc: 75.00%,  total acc: 66.25%   [EVAL] batch:   45 | acc: 75.00%,  total acc: 66.44%   [EVAL] batch:   46 | acc: 68.75%,  total acc: 66.49%   [EVAL] batch:   47 | acc: 68.75%,  total acc: 66.54%   [EVAL] batch:   48 | acc: 87.50%,  total acc: 66.96%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 67.38%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 68.01%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 68.63%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 69.22%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 69.79%   [EVAL] batch:   54 | acc: 100.00%,  total acc: 70.34%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 70.87%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 71.38%   [EVAL] batch:   57 | acc: 100.00%,  total acc: 71.88%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 72.35%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 72.81%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 73.26%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 73.69%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 73.31%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 62.50%,  total acc: 78.12%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 76.25%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 77.08%   [EVAL] batch:    6 | acc: 43.75%,  total acc: 72.32%   [EVAL] batch:    7 | acc: 56.25%,  total acc: 70.31%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 70.14%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 70.00%   [EVAL] batch:   10 | acc: 37.50%,  total acc: 67.05%   [EVAL] batch:   11 | acc: 50.00%,  total acc: 65.62%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 65.87%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 67.86%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 69.58%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 71.48%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 73.16%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 74.31%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 75.33%   [EVAL] batch:   19 | acc: 68.75%,  total acc: 75.00%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 75.60%   [EVAL] batch:   21 | acc: 62.50%,  total acc: 75.00%   [EVAL] batch:   22 | acc: 50.00%,  total acc: 73.91%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 74.74%   [EVAL] batch:   24 | acc: 62.50%,  total acc: 74.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 75.24%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 76.16%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 77.01%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 77.59%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 78.33%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 79.03%   [EVAL] batch:   31 | acc: 87.50%,  total acc: 79.30%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 79.73%   [EVAL] batch:   33 | acc: 56.25%,  total acc: 79.04%   [EVAL] batch:   34 | acc: 68.75%,  total acc: 78.75%   [EVAL] batch:   35 | acc: 75.00%,  total acc: 78.65%   [EVAL] batch:   36 | acc: 75.00%,  total acc: 78.55%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 78.78%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 79.33%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 79.84%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 80.34%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 80.65%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 80.96%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 81.11%   [EVAL] batch:   44 | acc: 75.00%,  total acc: 80.97%   [EVAL] batch:   45 | acc: 81.25%,  total acc: 80.98%   [EVAL] batch:   46 | acc: 62.50%,  total acc: 80.59%   [EVAL] batch:   47 | acc: 68.75%,  total acc: 80.34%   [EVAL] batch:   48 | acc: 87.50%,  total acc: 80.48%   [EVAL] batch:   49 | acc: 68.75%,  total acc: 80.25%   [EVAL] batch:   50 | acc: 81.25%,  total acc: 80.27%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 80.41%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 80.66%   [EVAL] batch:   53 | acc: 75.00%,  total acc: 80.56%   [EVAL] batch:   54 | acc: 56.25%,  total acc: 80.11%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 80.25%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 80.48%   [EVAL] batch:   57 | acc: 68.75%,  total acc: 80.28%   [EVAL] batch:   58 | acc: 68.75%,  total acc: 80.08%   [EVAL] batch:   59 | acc: 87.50%,  total acc: 80.21%   [EVAL] batch:   60 | acc: 93.75%,  total acc: 80.43%   [EVAL] batch:   61 | acc: 75.00%,  total acc: 80.34%   [EVAL] batch:   62 | acc: 100.00%,  total acc: 80.65%   [EVAL] batch:   63 | acc: 93.75%,  total acc: 80.86%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 81.15%   [EVAL] batch:   65 | acc: 100.00%,  total acc: 81.44%   [EVAL] batch:   66 | acc: 100.00%,  total acc: 81.72%   [EVAL] batch:   67 | acc: 93.75%,  total acc: 81.89%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 82.16%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 82.41%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 82.66%   [EVAL] batch:   71 | acc: 87.50%,  total acc: 82.73%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 82.96%   [EVAL] batch:   73 | acc: 100.00%,  total acc: 83.19%   [EVAL] batch:   74 | acc: 100.00%,  total acc: 83.42%   [EVAL] batch:   75 | acc: 75.00%,  total acc: 83.31%   [EVAL] batch:   76 | acc: 68.75%,  total acc: 83.12%   [EVAL] batch:   77 | acc: 43.75%,  total acc: 82.61%   [EVAL] batch:   78 | acc: 62.50%,  total acc: 82.36%   [EVAL] batch:   79 | acc: 62.50%,  total acc: 82.11%   [EVAL] batch:   80 | acc: 75.00%,  total acc: 82.02%   [EVAL] batch:   81 | acc: 43.75%,  total acc: 81.55%   [EVAL] batch:   82 | acc: 62.50%,  total acc: 81.33%   [EVAL] batch:   83 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:   84 | acc: 62.50%,  total acc: 81.03%   [EVAL] batch:   85 | acc: 68.75%,  total acc: 80.89%   [EVAL] batch:   86 | acc: 43.75%,  total acc: 80.46%   [EVAL] batch:   87 | acc: 75.00%,  total acc: 80.40%   [EVAL] batch:   88 | acc: 81.25%,  total acc: 80.41%   [EVAL] batch:   89 | acc: 87.50%,  total acc: 80.49%   [EVAL] batch:   90 | acc: 93.75%,  total acc: 80.63%   [EVAL] batch:   91 | acc: 75.00%,  total acc: 80.57%   [EVAL] batch:   92 | acc: 87.50%,  total acc: 80.65%   [EVAL] batch:   93 | acc: 62.50%,  total acc: 80.45%   [EVAL] batch:   94 | acc: 50.00%,  total acc: 80.13%   [EVAL] batch:   95 | acc: 50.00%,  total acc: 79.82%   [EVAL] batch:   96 | acc: 62.50%,  total acc: 79.64%   [EVAL] batch:   97 | acc: 37.50%,  total acc: 79.21%   [EVAL] batch:   98 | acc: 56.25%,  total acc: 78.98%   [EVAL] batch:   99 | acc: 43.75%,  total acc: 78.62%   [EVAL] batch:  100 | acc: 56.25%,  total acc: 78.40%   [EVAL] batch:  101 | acc: 81.25%,  total acc: 78.43%   [EVAL] batch:  102 | acc: 81.25%,  total acc: 78.46%   [EVAL] batch:  103 | acc: 87.50%,  total acc: 78.55%   [EVAL] batch:  104 | acc: 75.00%,  total acc: 78.51%   [EVAL] batch:  105 | acc: 87.50%,  total acc: 78.60%   [EVAL] batch:  106 | acc: 62.50%,  total acc: 78.45%   [EVAL] batch:  107 | acc: 37.50%,  total acc: 78.07%   [EVAL] batch:  108 | acc: 43.75%,  total acc: 77.75%   [EVAL] batch:  109 | acc: 56.25%,  total acc: 77.56%   [EVAL] batch:  110 | acc: 43.75%,  total acc: 77.25%   [EVAL] batch:  111 | acc: 50.00%,  total acc: 77.01%   [EVAL] batch:  112 | acc: 81.25%,  total acc: 77.05%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 77.25%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 77.45%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 77.64%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 77.83%   [EVAL] batch:  117 | acc: 100.00%,  total acc: 78.02%   [EVAL] batch:  118 | acc: 100.00%,  total acc: 78.20%   [EVAL] batch:  119 | acc: 100.00%,  total acc: 78.39%   [EVAL] batch:  120 | acc: 100.00%,  total acc: 78.56%   [EVAL] batch:  121 | acc: 100.00%,  total acc: 78.74%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 78.91%   [EVAL] batch:  123 | acc: 100.00%,  total acc: 79.08%   [EVAL] batch:  124 | acc: 100.00%,  total acc: 79.25%   [EVAL] batch:  125 | acc: 62.50%,  total acc: 79.12%   [EVAL] batch:  126 | acc: 62.50%,  total acc: 78.99%   [EVAL] batch:  127 | acc: 43.75%,  total acc: 78.71%   [EVAL] batch:  128 | acc: 75.00%,  total acc: 78.68%   [EVAL] batch:  129 | acc: 68.75%,  total acc: 78.61%   [EVAL] batch:  130 | acc: 68.75%,  total acc: 78.53%   [EVAL] batch:  131 | acc: 93.75%,  total acc: 78.65%   [EVAL] batch:  132 | acc: 93.75%,  total acc: 78.76%   [EVAL] batch:  133 | acc: 93.75%,  total acc: 78.87%   [EVAL] batch:  134 | acc: 93.75%,  total acc: 78.98%   [EVAL] batch:  135 | acc: 100.00%,  total acc: 79.14%   [EVAL] batch:  136 | acc: 93.75%,  total acc: 79.24%   [EVAL] batch:  137 | acc: 68.75%,  total acc: 79.17%   [EVAL] batch:  138 | acc: 18.75%,  total acc: 78.73%   [EVAL] batch:  139 | acc: 37.50%,  total acc: 78.44%   [EVAL] batch:  140 | acc: 31.25%,  total acc: 78.10%   [EVAL] batch:  141 | acc: 43.75%,  total acc: 77.86%   [EVAL] batch:  142 | acc: 37.50%,  total acc: 77.58%   [EVAL] batch:  143 | acc: 37.50%,  total acc: 77.30%   [EVAL] batch:  144 | acc: 93.75%,  total acc: 77.41%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 77.57%   [EVAL] batch:  146 | acc: 100.00%,  total acc: 77.72%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 77.87%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 78.02%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 78.12%   [EVAL] batch:  150 | acc: 43.75%,  total acc: 77.90%   [EVAL] batch:  151 | acc: 12.50%,  total acc: 77.47%   [EVAL] batch:  152 | acc: 18.75%,  total acc: 77.08%   [EVAL] batch:  153 | acc: 37.50%,  total acc: 76.83%   [EVAL] batch:  154 | acc: 12.50%,  total acc: 76.41%   [EVAL] batch:  155 | acc: 25.00%,  total acc: 76.08%   [EVAL] batch:  156 | acc: 75.00%,  total acc: 76.07%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 76.19%   [EVAL] batch:  158 | acc: 100.00%,  total acc: 76.34%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 76.45%   [EVAL] batch:  160 | acc: 87.50%,  total acc: 76.51%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 76.62%   [EVAL] batch:  162 | acc: 75.00%,  total acc: 76.61%   [EVAL] batch:  163 | acc: 75.00%,  total acc: 76.60%   [EVAL] batch:  164 | acc: 68.75%,  total acc: 76.55%   [EVAL] batch:  165 | acc: 56.25%,  total acc: 76.43%   [EVAL] batch:  166 | acc: 81.25%,  total acc: 76.46%   [EVAL] batch:  167 | acc: 81.25%,  total acc: 76.49%   [EVAL] batch:  168 | acc: 81.25%,  total acc: 76.52%   [EVAL] batch:  169 | acc: 100.00%,  total acc: 76.65%   [EVAL] batch:  170 | acc: 100.00%,  total acc: 76.79%   [EVAL] batch:  171 | acc: 93.75%,  total acc: 76.89%   [EVAL] batch:  172 | acc: 81.25%,  total acc: 76.91%   [EVAL] batch:  173 | acc: 100.00%,  total acc: 77.05%   [EVAL] batch:  174 | acc: 93.75%,  total acc: 77.14%   [EVAL] batch:  175 | acc: 62.50%,  total acc: 77.06%   [EVAL] batch:  176 | acc: 56.25%,  total acc: 76.94%   [EVAL] batch:  177 | acc: 68.75%,  total acc: 76.90%   [EVAL] batch:  178 | acc: 68.75%,  total acc: 76.85%   [EVAL] batch:  179 | acc: 68.75%,  total acc: 76.81%   [EVAL] batch:  180 | acc: 43.75%,  total acc: 76.62%   [EVAL] batch:  181 | acc: 75.00%,  total acc: 76.61%   [EVAL] batch:  182 | acc: 81.25%,  total acc: 76.64%   [EVAL] batch:  183 | acc: 93.75%,  total acc: 76.73%   [EVAL] batch:  184 | acc: 75.00%,  total acc: 76.72%   [EVAL] batch:  185 | acc: 87.50%,  total acc: 76.78%   [EVAL] batch:  186 | acc: 81.25%,  total acc: 76.80%   [EVAL] batch:  187 | acc: 93.75%,  total acc: 76.89%   [EVAL] batch:  188 | acc: 93.75%,  total acc: 76.98%   [EVAL] batch:  189 | acc: 87.50%,  total acc: 77.04%   [EVAL] batch:  190 | acc: 93.75%,  total acc: 77.13%   [EVAL] batch:  191 | acc: 68.75%,  total acc: 77.08%   [EVAL] batch:  192 | acc: 100.00%,  total acc: 77.20%   [EVAL] batch:  193 | acc: 93.75%,  total acc: 77.29%   [EVAL] batch:  194 | acc: 81.25%,  total acc: 77.31%   [EVAL] batch:  195 | acc: 81.25%,  total acc: 77.33%   [EVAL] batch:  196 | acc: 87.50%,  total acc: 77.38%   [EVAL] batch:  197 | acc: 75.00%,  total acc: 77.37%   [EVAL] batch:  198 | acc: 68.75%,  total acc: 77.32%   [EVAL] batch:  199 | acc: 68.75%,  total acc: 77.28%   [EVAL] batch:  200 | acc: 75.00%,  total acc: 77.27%   [EVAL] batch:  201 | acc: 87.50%,  total acc: 77.32%   [EVAL] batch:  202 | acc: 68.75%,  total acc: 77.28%   [EVAL] batch:  203 | acc: 56.25%,  total acc: 77.18%   [EVAL] batch:  204 | acc: 56.25%,  total acc: 77.07%   [EVAL] batch:  205 | acc: 75.00%,  total acc: 77.06%   [EVAL] batch:  206 | acc: 37.50%,  total acc: 76.87%   [EVAL] batch:  207 | acc: 50.00%,  total acc: 76.74%   [EVAL] batch:  208 | acc: 56.25%,  total acc: 76.64%   [EVAL] batch:  209 | acc: 43.75%,  total acc: 76.49%   [EVAL] batch:  210 | acc: 56.25%,  total acc: 76.39%   [EVAL] batch:  211 | acc: 43.75%,  total acc: 76.24%   [EVAL] batch:  212 | acc: 75.00%,  total acc: 76.23%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 76.34%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 76.45%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 76.56%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 76.67%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 76.78%   [EVAL] batch:  218 | acc: 93.75%,  total acc: 76.86%   [EVAL] batch:  219 | acc: 56.25%,  total acc: 76.76%   [EVAL] batch:  220 | acc: 81.25%,  total acc: 76.78%   [EVAL] batch:  221 | acc: 62.50%,  total acc: 76.72%   [EVAL] batch:  222 | acc: 56.25%,  total acc: 76.63%   [EVAL] batch:  223 | acc: 75.00%,  total acc: 76.62%   [EVAL] batch:  224 | acc: 43.75%,  total acc: 76.47%   [EVAL] batch:  225 | acc: 100.00%,  total acc: 76.58%   [EVAL] batch:  226 | acc: 100.00%,  total acc: 76.68%   [EVAL] batch:  227 | acc: 100.00%,  total acc: 76.78%   [EVAL] batch:  228 | acc: 100.00%,  total acc: 76.88%   [EVAL] batch:  229 | acc: 100.00%,  total acc: 76.98%   [EVAL] batch:  230 | acc: 93.75%,  total acc: 77.06%   [EVAL] batch:  231 | acc: 100.00%,  total acc: 77.16%   [EVAL] batch:  232 | acc: 100.00%,  total acc: 77.25%   [EVAL] batch:  233 | acc: 100.00%,  total acc: 77.35%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 77.45%   [EVAL] batch:  235 | acc: 100.00%,  total acc: 77.54%   [EVAL] batch:  236 | acc: 100.00%,  total acc: 77.64%   [EVAL] batch:  237 | acc: 100.00%,  total acc: 77.73%   [EVAL] batch:  238 | acc: 87.50%,  total acc: 77.77%   [EVAL] batch:  239 | acc: 93.75%,  total acc: 77.84%   [EVAL] batch:  240 | acc: 100.00%,  total acc: 77.93%   [EVAL] batch:  241 | acc: 87.50%,  total acc: 77.97%   [EVAL] batch:  242 | acc: 93.75%,  total acc: 78.03%   [EVAL] batch:  243 | acc: 93.75%,  total acc: 78.10%   [EVAL] batch:  244 | acc: 93.75%,  total acc: 78.16%   [EVAL] batch:  245 | acc: 87.50%,  total acc: 78.20%   [EVAL] batch:  246 | acc: 100.00%,  total acc: 78.29%   [EVAL] batch:  247 | acc: 100.00%,  total acc: 78.38%   [EVAL] batch:  248 | acc: 93.75%,  total acc: 78.44%   [EVAL] batch:  249 | acc: 93.75%,  total acc: 78.50%   [EVAL] batch:  250 | acc: 31.25%,  total acc: 78.31%   [EVAL] batch:  251 | acc: 31.25%,  total acc: 78.12%   [EVAL] batch:  252 | acc: 50.00%,  total acc: 78.01%   [EVAL] batch:  253 | acc: 31.25%,  total acc: 77.83%   [EVAL] batch:  254 | acc: 31.25%,  total acc: 77.65%   [EVAL] batch:  255 | acc: 37.50%,  total acc: 77.49%   [EVAL] batch:  256 | acc: 56.25%,  total acc: 77.41%   [EVAL] batch:  257 | acc: 68.75%,  total acc: 77.37%   [EVAL] batch:  258 | acc: 68.75%,  total acc: 77.34%   [EVAL] batch:  259 | acc: 62.50%,  total acc: 77.28%   [EVAL] batch:  260 | acc: 43.75%,  total acc: 77.16%   [EVAL] batch:  261 | acc: 68.75%,  total acc: 77.12%   [EVAL] batch:  262 | acc: 87.50%,  total acc: 77.16%   [EVAL] batch:  263 | acc: 56.25%,  total acc: 77.08%   [EVAL] batch:  264 | acc: 56.25%,  total acc: 77.00%   [EVAL] batch:  265 | acc: 50.00%,  total acc: 76.90%   [EVAL] batch:  266 | acc: 68.75%,  total acc: 76.87%   [EVAL] batch:  267 | acc: 68.75%,  total acc: 76.84%   [EVAL] batch:  268 | acc: 87.50%,  total acc: 76.88%   [EVAL] batch:  269 | acc: 87.50%,  total acc: 76.92%   [EVAL] batch:  270 | acc: 93.75%,  total acc: 76.98%   [EVAL] batch:  271 | acc: 100.00%,  total acc: 77.07%   [EVAL] batch:  272 | acc: 93.75%,  total acc: 77.13%   [EVAL] batch:  273 | acc: 87.50%,  total acc: 77.17%   [EVAL] batch:  274 | acc: 87.50%,  total acc: 77.20%   [EVAL] batch:  275 | acc: 81.25%,  total acc: 77.22%   [EVAL] batch:  276 | acc: 75.00%,  total acc: 77.21%   [EVAL] batch:  277 | acc: 87.50%,  total acc: 77.25%   [EVAL] batch:  278 | acc: 75.00%,  total acc: 77.24%   [EVAL] batch:  279 | acc: 81.25%,  total acc: 77.25%   [EVAL] batch:  280 | acc: 87.50%,  total acc: 77.29%   [EVAL] batch:  281 | acc: 62.50%,  total acc: 77.24%   [EVAL] batch:  282 | acc: 62.50%,  total acc: 77.19%   [EVAL] batch:  283 | acc: 18.75%,  total acc: 76.98%   [EVAL] batch:  284 | acc: 43.75%,  total acc: 76.86%   [EVAL] batch:  285 | acc: 25.00%,  total acc: 76.68%   [EVAL] batch:  286 | acc: 18.75%,  total acc: 76.48%   [EVAL] batch:  287 | acc: 75.00%,  total acc: 76.48%   [EVAL] batch:  288 | acc: 81.25%,  total acc: 76.49%   [EVAL] batch:  289 | acc: 87.50%,  total acc: 76.53%   [EVAL] batch:  290 | acc: 81.25%,  total acc: 76.55%   [EVAL] batch:  291 | acc: 93.75%,  total acc: 76.61%   [EVAL] batch:  292 | acc: 75.00%,  total acc: 76.60%   [EVAL] batch:  293 | acc: 87.50%,  total acc: 76.64%   [EVAL] batch:  294 | acc: 75.00%,  total acc: 76.63%   [EVAL] batch:  295 | acc: 75.00%,  total acc: 76.63%   [EVAL] batch:  296 | acc: 68.75%,  total acc: 76.60%   [EVAL] batch:  297 | acc: 68.75%,  total acc: 76.57%   [EVAL] batch:  298 | acc: 87.50%,  total acc: 76.61%   [EVAL] batch:  299 | acc: 87.50%,  total acc: 76.65%   [EVAL] batch:  300 | acc: 100.00%,  total acc: 76.72%   [EVAL] batch:  301 | acc: 100.00%,  total acc: 76.80%   [EVAL] batch:  302 | acc: 100.00%,  total acc: 76.88%   [EVAL] batch:  303 | acc: 100.00%,  total acc: 76.95%   [EVAL] batch:  304 | acc: 100.00%,  total acc: 77.03%   [EVAL] batch:  305 | acc: 100.00%,  total acc: 77.10%   [EVAL] batch:  306 | acc: 100.00%,  total acc: 77.18%   [EVAL] batch:  307 | acc: 100.00%,  total acc: 77.25%   [EVAL] batch:  308 | acc: 100.00%,  total acc: 77.33%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 77.40%   [EVAL] batch:  310 | acc: 100.00%,  total acc: 77.47%   [EVAL] batch:  311 | acc: 100.00%,  total acc: 77.54%   [EVAL] batch:  312 | acc: 50.00%,  total acc: 77.46%   
cur_acc:  ['0.9524', '0.8135', '0.7579', '0.8502', '0.7331']
his_acc:  ['0.9524', '0.8640', '0.8165', '0.8065', '0.7746']
CurrentTrain: epoch 15, batch     0 | loss: 20.2086559CurrentTrain: epoch 15, batch     1 | loss: 18.0939577CurrentTrain: epoch 15, batch     2 | loss: 14.7372872CurrentTrain: epoch  1, batch     3 | loss: 10.2945045CurrentTrain: epoch 15, batch     0 | loss: 16.3666057CurrentTrain: epoch 15, batch     1 | loss: 13.8201588CurrentTrain: epoch 15, batch     2 | loss: 11.4950047CurrentTrain: epoch  1, batch     3 | loss: 6.7911306CurrentTrain: epoch 15, batch     0 | loss: 19.9659438CurrentTrain: epoch 15, batch     1 | loss: 11.0447747CurrentTrain: epoch 15, batch     2 | loss: 10.6098732CurrentTrain: epoch  1, batch     3 | loss: 9.0993545CurrentTrain: epoch 15, batch     0 | loss: 9.8662636CurrentTrain: epoch 15, batch     1 | loss: 11.4791717CurrentTrain: epoch 15, batch     2 | loss: 13.8489221CurrentTrain: epoch  1, batch     3 | loss: 8.3532002CurrentTrain: epoch 15, batch     0 | loss: 18.2472991CurrentTrain: epoch 15, batch     1 | loss: 11.7321863CurrentTrain: epoch 15, batch     2 | loss: 12.6566222CurrentTrain: epoch  1, batch     3 | loss: 7.1245017CurrentTrain: epoch 15, batch     0 | loss: 9.4405458CurrentTrain: epoch 15, batch     1 | loss: 13.4611051CurrentTrain: epoch 15, batch     2 | loss: 11.5070380CurrentTrain: epoch  1, batch     3 | loss: 8.5465629CurrentTrain: epoch 15, batch     0 | loss: 12.4182028CurrentTrain: epoch 15, batch     1 | loss: 11.9885170CurrentTrain: epoch 15, batch     2 | loss: 8.6175799CurrentTrain: epoch  1, batch     3 | loss: 6.7961859CurrentTrain: epoch 15, batch     0 | loss: 9.0481269CurrentTrain: epoch 15, batch     1 | loss: 14.3105128CurrentTrain: epoch 15, batch     2 | loss: 10.3301475CurrentTrain: epoch  1, batch     3 | loss: 6.8237250CurrentTrain: epoch 15, batch     0 | loss: 15.1784307CurrentTrain: epoch 15, batch     1 | loss: 13.1288512CurrentTrain: epoch 15, batch     2 | loss: 14.4756166CurrentTrain: epoch  1, batch     3 | loss: 8.3135911CurrentTrain: epoch 15, batch     0 | loss: 8.1097602CurrentTrain: epoch 15, batch     1 | loss: 16.2504984CurrentTrain: epoch 15, batch     2 | loss: 14.5113189CurrentTrain: epoch  1, batch     3 | loss: 5.6618598
#### follows ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "follows" means immediately prior item in a series of which the subject is a part.
Here is an example:
Relation: follows
Context: ( 1988 ) , " night watch " ( 2004 ) , " day watch " ( 2006 ) and " legend № 17 " ( 2013 ) .
Head Entity: day watch
Tail Entity: night watch
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "follows":
Relation: follows  
Context: ( 1995 ) , " The Phantom Menace " ( 1999 ) , " Attack of the Clones " ( 2002 ) and " Revenge of the Sith " ( 2005 ) .  
Head Entity: Attack of the Clones  
Tail Entity: The Phantom Menace  

Relation: follows  
Context: ( 2010 ) , " The Hunger Games " ( 2012 ) , " Catching Fire " ( 2013 ) and " Mockingjay " ( 2014 ) .  
Head Entity: Catching Fire  
Tail Entity: The Hunger Games  
#### sport ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "sport" means sport that the subject participates or participated in or is associated with.
Here is an example:
Relation: sport
Context: it is primarily used for baseball and serves as the home stadium for yaquis de obregón .
Head Entity: yaquis de obregón
Tail Entity: baseball
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "sport":
Relation: sport  
Context: She has been a professional player in tennis since she was a teenager and has won several championships.  
Head Entity: She  
Tail Entity: tennis  

Relation: sport  
Context: The city is known for its vibrant soccer culture, with numerous local teams competing in various leagues.  
Head Entity: The city  
Tail Entity: soccer  
#### father ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "father" means male parent of the subject.
Here is an example:
Relation: father
Context: alphonse louis pierre pyrame de candolle ( 28 october 18064 april 1893 ) was a french - swiss botanist , the son of the swiss botanist augustin pyramus de candolle .
Head Entity: alphonse louis pierre pyrame de candolle
Tail Entity: augustin pyramus de candolle
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "father":
Relation: father  
Context: charles darwin ( 12 february 1809 – 19 april 1882 ) was an english naturalist and geologist, best known for his contributions to the science of evolution. he was the son of robert darwin, a wealthy society doctor.  
Head Entity: charles darwin  
Tail Entity: robert darwin  

Relation: father  
Context: barack obama ( born august 4, 1961 ) is an american attorney and politician who served as the 44th president of the united states from 2009 to 2017. he is the son of barack obama sr., a kenyan economist.  
Head Entity: barack obama  
Tail Entity: barack obama sr.  
#### original network ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "original network" means network or service that originally broadcast a radio or television program.
Here is an example:
Relation: original network
Context: hana yori dango returns ( ) is a japanese television drama series , broadcast on tbs in 2007 .
Head Entity: hana yori dango returns
Tail Entity: tbs
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "original network":
Relation: original network  
Context: the popular animated series "avatar: the last airbender" was originally aired on nickelodeon from 2005 to 2008.  
Head Entity: avatar: the last airbender  
Tail Entity: nickelodeon  

Relation: original network  
Context: "breaking bad" is a critically acclaimed television series that premiered on amc in 2008 and concluded in 2013.  
Head Entity: breaking bad  
Tail Entity: amc  
#### original language of film or TV show ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "original language of film or TV show" means language in which a film or a performance work was originally created.
Here is an example:
Relation: original language of film or TV show
Context: the 1954 film , directed by kadri venkata reddy , " peddamanushulu " was honoured with the first president 's silver medal for best feature film in telugu .
Head Entity: peddamanushulu
Tail Entity: telugu
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "original language of film or TV show":
Relation: original language of film or TV show  
Context: the 1994 animated film "The Lion King," produced by Walt Disney, was originally created in English before being dubbed into several other languages.  
Head Entity: The Lion King  
Tail Entity: English  

Relation: original language of film or TV show  
Context: the critically acclaimed 2001 film "Amélie," directed by Jean-Pierre Jeunet, was filmed in French and has since gained a global audience.  
Head Entity: Amélie  
Tail Entity: French  
#### league ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "league" means league in which team or player plays or has played in.
Here is an example:
Relation: league
Context: in the summer of 2013 he returned to montenegro and joined with fk rudar pljevlja , playing with the club for two seasons in the montenegrin first league .
Head Entity: fk rudar pljevlja
Tail Entity: montenegrin first league
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "league":
Relation: league  
Context: After a successful stint in the English Premier League, the player signed with FC Barcelona, competing in La Liga for the next three seasons.  
Head Entity: FC Barcelona  
Tail Entity: La Liga  

Relation: league  
Context: The young athlete made headlines when he joined the New York Yankees, showcasing his talent in Major League Baseball during the 2021 season.  
Head Entity: New York Yankees  
Tail Entity: Major League Baseball  
#### mother ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "mother" means female parent of the subject.
Here is an example:
Relation: mother
Context: she is supposedly the third and youngest child of seti i and tuya , and the younger sister of ramesses ii and tia .
Head Entity: tia
Tail Entity: tuya
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "mother":
Relation: mother  
Context: Cleopatra was the last active ruler of the Ptolemaic Kingdom of Egypt and was the mother of Ptolemy XV, also known as Caesarion.  
Head Entity: Cleopatra  
Tail Entity: Caesarion  

Relation: mother  
Context: In Norse mythology, Frigg is known as the mother of Baldr, who is one of the most beloved gods.  
Head Entity: Frigg  
Tail Entity: Baldr  
#### instrument ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "instrument" means musical instrument that a person plays or teaches or used in a music occupation.
Here is an example:
Relation: instrument
Context: peter bastian ( 25 august 1943 in copenhagen – 27 march 2017 ) was a danish musician . peter bastian bio and cv he was educated in theoretical physics and classical bassoon .
Head Entity: peter bastian
Tail Entity: bassoon
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "instrument":
Relation: instrument  
Context: johann sebastian bach (31 march 1685 – 28 july 1750) was a german composer and musician of the baroque period. he was known for his exceptional skill in playing the organ and harpsichord.  
Head Entity: johann sebastian bach  
Tail Entity: organ  

Relation: instrument  
Context: yo-yo ma, born on october 7, 1955, is a world-renowned cellist who has performed with numerous orchestras and in various genres. he is particularly famous for his interpretations of classical cello concertos.  
Head Entity: yo-yo ma  
Tail Entity: cello  
#### country ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "country" means sovereign state of this item (not to be used for human beings).
Here is an example:
Relation: country
Context: a u.s. government - funded $ 36 million bridge over the panj river connects sher khan bandar in afghanistan with nizhniy pyanzh in tajikistan , which transport more than 150 trucks or 1,000 cars daily .
Head Entity: sher khan bandar
Tail Entity: afghanistan
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "country":
Relation: country  
Context: the ancient city of petra, located in southern jordan, is famous for its rock-cut architecture and water conduit system, attracting thousands of tourists each year.  
Head Entity: petra  
Tail Entity: jordan  

Relation: country  
Context: the great wall of china, a series of fortifications made of various materials, stretches across northern china and is a UNESCO World Heritage site.  
Head Entity: great wall of china  
Tail Entity: china  
#### characters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "characters" means characters which appear in this item (like plays, operas, operettas, books, comics, films, TV series, video games).
Here is an example:
Relation: characters
Context: ras thavas reappears later in the series to perform more mad science in the novel " synthetic men of mars " .
Head Entity: synthetic men of mars
Tail Entity: ras thavas
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "characters":
Relation: characters  
Context: In the animated series "Avatar: The Last Airbender," Aang, the last Airbender, embarks on a journey to master all four elements.  
Head Entity: Avatar: The Last Airbender  
Tail Entity: Aang  

Relation: characters  
Context: The novel "Pride and Prejudice" features Elizabeth Bennet as she navigates issues of class, marriage, and morality in early 19th century England.  
Head Entity: Pride and Prejudice  
Tail Entity: Elizabeth Bennet  
MemoryTrain:  epoch 15, batch     0 | loss: 3.7598872MemoryTrain:  epoch 15, batch     1 | loss: 4.7211248MemoryTrain:  epoch 15, batch     2 | loss: 5.5366425MemoryTrain:  epoch 15, batch     3 | loss: 2.5372720MemoryTrain:  epoch 15, batch     4 | loss: 3.0649463MemoryTrain:  epoch 15, batch     5 | loss: 4.8678595MemoryTrain:  epoch 15, batch     6 | loss: 5.0348686MemoryTrain:  epoch 15, batch     7 | loss: 3.2723915MemoryTrain:  epoch 15, batch     8 | loss: 3.9917320MemoryTrain:  epoch 15, batch     9 | loss: 2.9662617MemoryTrain:  epoch 15, batch    10 | loss: 5.0550968MemoryTrain:  epoch  3, batch    11 | loss: 11.2516756MemoryTrain:  epoch 15, batch     0 | loss: 5.5089997MemoryTrain:  epoch 15, batch     1 | loss: 4.4746869MemoryTrain:  epoch 15, batch     2 | loss: 3.7904636MemoryTrain:  epoch 15, batch     3 | loss: 6.1322069MemoryTrain:  epoch 15, batch     4 | loss: 2.6457241MemoryTrain:  epoch 15, batch     5 | loss: 4.7065309MemoryTrain:  epoch 15, batch     6 | loss: 2.1173753MemoryTrain:  epoch 15, batch     7 | loss: 2.9334000MemoryTrain:  epoch 15, batch     8 | loss: 2.6986316MemoryTrain:  epoch 15, batch     9 | loss: 5.4426215MemoryTrain:  epoch 15, batch    10 | loss: 2.3714459MemoryTrain:  epoch  3, batch    11 | loss: 10.4590788MemoryTrain:  epoch 15, batch     0 | loss: 2.9275410MemoryTrain:  epoch 15, batch     1 | loss: 1.9668403MemoryTrain:  epoch 15, batch     2 | loss: 4.5484751MemoryTrain:  epoch 15, batch     3 | loss: 3.0013179MemoryTrain:  epoch 15, batch     4 | loss: 4.8514217MemoryTrain:  epoch 15, batch     5 | loss: 2.6513693MemoryTrain:  epoch 15, batch     6 | loss: 1.7769842MemoryTrain:  epoch 15, batch     7 | loss: 6.2656775MemoryTrain:  epoch 15, batch     8 | loss: 2.4366029MemoryTrain:  epoch 15, batch     9 | loss: 2.3645861MemoryTrain:  epoch 15, batch    10 | loss: 3.9785035MemoryTrain:  epoch  3, batch    11 | loss: 10.7959743MemoryTrain:  epoch 15, batch     0 | loss: 2.1649039MemoryTrain:  epoch 15, batch     1 | loss: 2.7167944MemoryTrain:  epoch 15, batch     2 | loss: 3.8137737MemoryTrain:  epoch 15, batch     3 | loss: 2.0928811MemoryTrain:  epoch 15, batch     4 | loss: 6.9714657MemoryTrain:  epoch 15, batch     5 | loss: 3.0864376MemoryTrain:  epoch 15, batch     6 | loss: 1.6638490MemoryTrain:  epoch 15, batch     7 | loss: 4.2401469MemoryTrain:  epoch 15, batch     8 | loss: 2.3096841MemoryTrain:  epoch 15, batch     9 | loss: 2.3619863MemoryTrain:  epoch 15, batch    10 | loss: 2.8877605MemoryTrain:  epoch  3, batch    11 | loss: 10.1041659MemoryTrain:  epoch 15, batch     0 | loss: 3.1652101MemoryTrain:  epoch 15, batch     1 | loss: 2.0306324MemoryTrain:  epoch 15, batch     2 | loss: 2.9074119MemoryTrain:  epoch 15, batch     3 | loss: 1.7298121MemoryTrain:  epoch 15, batch     4 | loss: 2.1711008MemoryTrain:  epoch 15, batch     5 | loss: 1.7558505MemoryTrain:  epoch 15, batch     6 | loss: 2.1432548MemoryTrain:  epoch 15, batch     7 | loss: 4.2445470MemoryTrain:  epoch 15, batch     8 | loss: 1.8396161MemoryTrain:  epoch 15, batch     9 | loss: 2.3569562MemoryTrain:  epoch 15, batch    10 | loss: 2.8711702MemoryTrain:  epoch  3, batch    11 | loss: 10.1633008MemoryTrain:  epoch 15, batch     0 | loss: 1.8495656MemoryTrain:  epoch 15, batch     1 | loss: 3.6206950MemoryTrain:  epoch 15, batch     2 | loss: 2.8751805MemoryTrain:  epoch 15, batch     3 | loss: 2.0908941MemoryTrain:  epoch 15, batch     4 | loss: 1.9343687MemoryTrain:  epoch 15, batch     5 | loss: 4.0638590MemoryTrain:  epoch 15, batch     6 | loss: 1.9962629MemoryTrain:  epoch 15, batch     7 | loss: 1.9525697MemoryTrain:  epoch 15, batch     8 | loss: 2.0427283MemoryTrain:  epoch 15, batch     9 | loss: 1.6135916MemoryTrain:  epoch 15, batch    10 | loss: 1.9388145MemoryTrain:  epoch  3, batch    11 | loss: 10.7150334MemoryTrain:  epoch 15, batch     0 | loss: 1.9176949MemoryTrain:  epoch 15, batch     1 | loss: 3.0744547MemoryTrain:  epoch 15, batch     2 | loss: 1.6525853MemoryTrain:  epoch 15, batch     3 | loss: 2.0538414MemoryTrain:  epoch 15, batch     4 | loss: 1.8191138MemoryTrain:  epoch 15, batch     5 | loss: 1.3508361MemoryTrain:  epoch 15, batch     6 | loss: 2.1910856MemoryTrain:  epoch 15, batch     7 | loss: 1.7290078MemoryTrain:  epoch 15, batch     8 | loss: 3.0256215MemoryTrain:  epoch 15, batch     9 | loss: 2.1578115MemoryTrain:  epoch 15, batch    10 | loss: 1.7827146MemoryTrain:  epoch  3, batch    11 | loss: 9.9030523MemoryTrain:  epoch 15, batch     0 | loss: 1.9399891MemoryTrain:  epoch 15, batch     1 | loss: 4.5498582MemoryTrain:  epoch 15, batch     2 | loss: 3.6011626MemoryTrain:  epoch 15, batch     3 | loss: 2.2809508MemoryTrain:  epoch 15, batch     4 | loss: 2.7482285MemoryTrain:  epoch 15, batch     5 | loss: 3.6943169MemoryTrain:  epoch 15, batch     6 | loss: 3.0651580MemoryTrain:  epoch 15, batch     7 | loss: 2.5679943MemoryTrain:  epoch 15, batch     8 | loss: 4.4331519MemoryTrain:  epoch 15, batch     9 | loss: 1.3574148MemoryTrain:  epoch 15, batch    10 | loss: 3.8885542MemoryTrain:  epoch  3, batch    11 | loss: 9.6721668MemoryTrain:  epoch 15, batch     0 | loss: 6.4096177MemoryTrain:  epoch 15, batch     1 | loss: 1.7362895MemoryTrain:  epoch 15, batch     2 | loss: 4.1894081MemoryTrain:  epoch 15, batch     3 | loss: 2.1749780MemoryTrain:  epoch 15, batch     4 | loss: 2.4072273MemoryTrain:  epoch 15, batch     5 | loss: 1.8467225MemoryTrain:  epoch 15, batch     6 | loss: 2.3701567MemoryTrain:  epoch 15, batch     7 | loss: 2.3409435MemoryTrain:  epoch 15, batch     8 | loss: 2.1878267MemoryTrain:  epoch 15, batch     9 | loss: 2.1490841MemoryTrain:  epoch 15, batch    10 | loss: 2.1756694MemoryTrain:  epoch  3, batch    11 | loss: 10.1218972MemoryTrain:  epoch 15, batch     0 | loss: 2.8156494MemoryTrain:  epoch 15, batch     1 | loss: 2.8144162MemoryTrain:  epoch 15, batch     2 | loss: 1.4066663MemoryTrain:  epoch 15, batch     3 | loss: 2.7280024MemoryTrain:  epoch 15, batch     4 | loss: 1.9716582MemoryTrain:  epoch 15, batch     5 | loss: 1.7497162MemoryTrain:  epoch 15, batch     6 | loss: 1.4050254MemoryTrain:  epoch 15, batch     7 | loss: 1.7796724MemoryTrain:  epoch 15, batch     8 | loss: 2.1755425MemoryTrain:  epoch 15, batch     9 | loss: 3.1927377MemoryTrain:  epoch 15, batch    10 | loss: 1.7363868MemoryTrain:  epoch  3, batch    11 | loss: 9.8736279
[EVAL] batch:    0 | acc: 18.75%,  total acc: 18.75%   [EVAL] batch:    1 | acc: 25.00%,  total acc: 21.88%   [EVAL] batch:    2 | acc: 18.75%,  total acc: 20.83%   [EVAL] batch:    3 | acc: 12.50%,  total acc: 18.75%   [EVAL] batch:    4 | acc: 18.75%,  total acc: 18.75%   [EVAL] batch:    5 | acc: 25.00%,  total acc: 19.79%   [EVAL] batch:    6 | acc: 50.00%,  total acc: 24.11%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 32.03%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 38.89%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 44.38%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 48.86%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 52.08%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 53.37%   [EVAL] batch:   13 | acc: 43.75%,  total acc: 52.68%   [EVAL] batch:   14 | acc: 56.25%,  total acc: 52.92%   [EVAL] batch:   15 | acc: 50.00%,  total acc: 52.73%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 54.04%   [EVAL] batch:   17 | acc: 56.25%,  total acc: 54.17%   [EVAL] batch:   18 | acc: 56.25%,  total acc: 54.28%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 56.56%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 58.63%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 60.23%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 61.96%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 63.54%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 64.50%   [EVAL] batch:   25 | acc: 93.75%,  total acc: 65.62%   [EVAL] batch:   26 | acc: 87.50%,  total acc: 66.44%   [EVAL] batch:   27 | acc: 81.25%,  total acc: 66.96%   [EVAL] batch:   28 | acc: 87.50%,  total acc: 67.67%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 68.12%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 68.75%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 69.73%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 70.64%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 71.32%   [EVAL] batch:   34 | acc: 87.50%,  total acc: 71.79%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 72.40%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 73.14%   [EVAL] batch:   37 | acc: 81.25%,  total acc: 73.36%   [EVAL] batch:   38 | acc: 75.00%,  total acc: 73.40%   [EVAL] batch:   39 | acc: 68.75%,  total acc: 73.28%   [EVAL] batch:   40 | acc: 50.00%,  total acc: 72.71%   [EVAL] batch:   41 | acc: 62.50%,  total acc: 72.47%   [EVAL] batch:   42 | acc: 50.00%,  total acc: 71.95%   [EVAL] batch:   43 | acc: 75.00%,  total acc: 72.02%   [EVAL] batch:   44 | acc: 81.25%,  total acc: 72.22%   [EVAL] batch:   45 | acc: 81.25%,  total acc: 72.42%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 72.87%   [EVAL] batch:   47 | acc: 75.00%,  total acc: 72.92%   [EVAL] batch:   48 | acc: 62.50%,  total acc: 72.70%   [EVAL] batch:   49 | acc: 75.00%,  total acc: 72.75%   [EVAL] batch:   50 | acc: 68.75%,  total acc: 72.67%   [EVAL] batch:   51 | acc: 56.25%,  total acc: 72.36%   [EVAL] batch:   52 | acc: 68.75%,  total acc: 72.29%   [EVAL] batch:   53 | acc: 62.50%,  total acc: 72.11%   [EVAL] batch:   54 | acc: 62.50%,  total acc: 71.93%   [EVAL] batch:   55 | acc: 37.50%,  total acc: 71.32%   [EVAL] batch:   56 | acc: 56.25%,  total acc: 71.05%   [EVAL] batch:   57 | acc: 81.25%,  total acc: 71.23%   [EVAL] batch:   58 | acc: 68.75%,  total acc: 71.19%   [EVAL] batch:   59 | acc: 87.50%,  total acc: 71.46%   [EVAL] batch:   60 | acc: 87.50%,  total acc: 71.72%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 71.98%   [EVAL] batch:   62 | acc: 37.50%,  total acc: 71.43%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 82.81%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 82.50%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    6 | acc: 43.75%,  total acc: 77.68%   [EVAL] batch:    7 | acc: 43.75%,  total acc: 73.44%   [EVAL] batch:    8 | acc: 50.00%,  total acc: 70.83%   [EVAL] batch:    9 | acc: 50.00%,  total acc: 68.75%   [EVAL] batch:   10 | acc: 43.75%,  total acc: 66.48%   [EVAL] batch:   11 | acc: 50.00%,  total acc: 65.10%   [EVAL] batch:   12 | acc: 62.50%,  total acc: 64.90%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 66.96%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 68.75%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 70.70%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 72.43%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 73.61%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 74.67%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 74.69%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 75.30%   [EVAL] batch:   21 | acc: 62.50%,  total acc: 74.72%   [EVAL] batch:   22 | acc: 50.00%,  total acc: 73.64%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 74.22%   [EVAL] batch:   24 | acc: 68.75%,  total acc: 74.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 75.93%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 76.56%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 77.16%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 77.92%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 78.63%   [EVAL] batch:   31 | acc: 87.50%,  total acc: 78.91%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 79.36%   [EVAL] batch:   33 | acc: 56.25%,  total acc: 78.68%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 78.57%   [EVAL] batch:   35 | acc: 75.00%,  total acc: 78.47%   [EVAL] batch:   36 | acc: 81.25%,  total acc: 78.55%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 78.78%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 79.33%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 79.84%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 80.34%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 80.65%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 80.96%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 81.11%   [EVAL] batch:   44 | acc: 75.00%,  total acc: 80.97%   [EVAL] batch:   45 | acc: 87.50%,  total acc: 81.11%   [EVAL] batch:   46 | acc: 68.75%,  total acc: 80.85%   [EVAL] batch:   47 | acc: 75.00%,  total acc: 80.73%   [EVAL] batch:   48 | acc: 87.50%,  total acc: 80.87%   [EVAL] batch:   49 | acc: 75.00%,  total acc: 80.75%   [EVAL] batch:   50 | acc: 81.25%,  total acc: 80.76%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 80.89%   [EVAL] batch:   52 | acc: 81.25%,  total acc: 80.90%   [EVAL] batch:   53 | acc: 75.00%,  total acc: 80.79%   [EVAL] batch:   54 | acc: 56.25%,  total acc: 80.34%   [EVAL] batch:   55 | acc: 81.25%,  total acc: 80.36%   [EVAL] batch:   56 | acc: 87.50%,  total acc: 80.48%   [EVAL] batch:   57 | acc: 31.25%,  total acc: 79.63%   [EVAL] batch:   58 | acc: 56.25%,  total acc: 79.24%   [EVAL] batch:   59 | acc: 68.75%,  total acc: 79.06%   [EVAL] batch:   60 | acc: 75.00%,  total acc: 79.00%   [EVAL] batch:   61 | acc: 62.50%,  total acc: 78.73%   [EVAL] batch:   62 | acc: 62.50%,  total acc: 78.47%   [EVAL] batch:   63 | acc: 68.75%,  total acc: 78.32%   [EVAL] batch:   64 | acc: 56.25%,  total acc: 77.98%   [EVAL] batch:   65 | acc: 37.50%,  total acc: 77.37%   [EVAL] batch:   66 | acc: 56.25%,  total acc: 77.05%   [EVAL] batch:   67 | acc: 68.75%,  total acc: 76.93%   [EVAL] batch:   68 | acc: 75.00%,  total acc: 76.90%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 77.23%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 77.55%   [EVAL] batch:   71 | acc: 93.75%,  total acc: 77.78%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 78.08%   [EVAL] batch:   73 | acc: 100.00%,  total acc: 78.38%   [EVAL] batch:   74 | acc: 100.00%,  total acc: 78.67%   [EVAL] batch:   75 | acc: 68.75%,  total acc: 78.54%   [EVAL] batch:   76 | acc: 50.00%,  total acc: 78.17%   [EVAL] batch:   77 | acc: 43.75%,  total acc: 77.72%   [EVAL] batch:   78 | acc: 62.50%,  total acc: 77.53%   [EVAL] batch:   79 | acc: 68.75%,  total acc: 77.42%   [EVAL] batch:   80 | acc: 62.50%,  total acc: 77.24%   [EVAL] batch:   81 | acc: 50.00%,  total acc: 76.91%   [EVAL] batch:   82 | acc: 56.25%,  total acc: 76.66%   [EVAL] batch:   83 | acc: 56.25%,  total acc: 76.41%   [EVAL] batch:   84 | acc: 50.00%,  total acc: 76.10%   [EVAL] batch:   85 | acc: 50.00%,  total acc: 75.80%   [EVAL] batch:   86 | acc: 31.25%,  total acc: 75.29%   [EVAL] batch:   87 | acc: 75.00%,  total acc: 75.28%   [EVAL] batch:   88 | acc: 81.25%,  total acc: 75.35%   [EVAL] batch:   89 | acc: 87.50%,  total acc: 75.49%   [EVAL] batch:   90 | acc: 93.75%,  total acc: 75.69%   [EVAL] batch:   91 | acc: 75.00%,  total acc: 75.68%   [EVAL] batch:   92 | acc: 75.00%,  total acc: 75.67%   [EVAL] batch:   93 | acc: 50.00%,  total acc: 75.40%   [EVAL] batch:   94 | acc: 81.25%,  total acc: 75.46%   [EVAL] batch:   95 | acc: 62.50%,  total acc: 75.33%   [EVAL] batch:   96 | acc: 62.50%,  total acc: 75.19%   [EVAL] batch:   97 | acc: 68.75%,  total acc: 75.13%   [EVAL] batch:   98 | acc: 75.00%,  total acc: 75.13%   [EVAL] batch:   99 | acc: 68.75%,  total acc: 75.06%   [EVAL] batch:  100 | acc: 56.25%,  total acc: 74.88%   [EVAL] batch:  101 | acc: 75.00%,  total acc: 74.88%   [EVAL] batch:  102 | acc: 75.00%,  total acc: 74.88%   [EVAL] batch:  103 | acc: 87.50%,  total acc: 75.00%   [EVAL] batch:  104 | acc: 68.75%,  total acc: 74.94%   [EVAL] batch:  105 | acc: 81.25%,  total acc: 75.00%   [EVAL] batch:  106 | acc: 62.50%,  total acc: 74.88%   [EVAL] batch:  107 | acc: 37.50%,  total acc: 74.54%   [EVAL] batch:  108 | acc: 37.50%,  total acc: 74.20%   [EVAL] batch:  109 | acc: 56.25%,  total acc: 74.03%   [EVAL] batch:  110 | acc: 43.75%,  total acc: 73.76%   [EVAL] batch:  111 | acc: 50.00%,  total acc: 73.55%   [EVAL] batch:  112 | acc: 81.25%,  total acc: 73.62%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 73.85%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 74.08%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 74.30%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 74.52%   [EVAL] batch:  117 | acc: 100.00%,  total acc: 74.74%   [EVAL] batch:  118 | acc: 87.50%,  total acc: 74.84%   [EVAL] batch:  119 | acc: 68.75%,  total acc: 74.79%   [EVAL] batch:  120 | acc: 50.00%,  total acc: 74.59%   [EVAL] batch:  121 | acc: 56.25%,  total acc: 74.44%   [EVAL] batch:  122 | acc: 68.75%,  total acc: 74.39%   [EVAL] batch:  123 | acc: 62.50%,  total acc: 74.29%   [EVAL] batch:  124 | acc: 81.25%,  total acc: 74.35%   [EVAL] batch:  125 | acc: 43.75%,  total acc: 74.11%   [EVAL] batch:  126 | acc: 43.75%,  total acc: 73.87%   [EVAL] batch:  127 | acc: 62.50%,  total acc: 73.78%   [EVAL] batch:  128 | acc: 68.75%,  total acc: 73.74%   [EVAL] batch:  129 | acc: 68.75%,  total acc: 73.70%   [EVAL] batch:  130 | acc: 62.50%,  total acc: 73.62%   [EVAL] batch:  131 | acc: 81.25%,  total acc: 73.67%   [EVAL] batch:  132 | acc: 81.25%,  total acc: 73.73%   [EVAL] batch:  133 | acc: 75.00%,  total acc: 73.74%   [EVAL] batch:  134 | acc: 93.75%,  total acc: 73.89%   [EVAL] batch:  135 | acc: 93.75%,  total acc: 74.03%   [EVAL] batch:  136 | acc: 93.75%,  total acc: 74.18%   [EVAL] batch:  137 | acc: 68.75%,  total acc: 74.14%   [EVAL] batch:  138 | acc: 25.00%,  total acc: 73.79%   [EVAL] batch:  139 | acc: 37.50%,  total acc: 73.53%   [EVAL] batch:  140 | acc: 25.00%,  total acc: 73.18%   [EVAL] batch:  141 | acc: 43.75%,  total acc: 72.98%   [EVAL] batch:  142 | acc: 37.50%,  total acc: 72.73%   [EVAL] batch:  143 | acc: 37.50%,  total acc: 72.48%   [EVAL] batch:  144 | acc: 93.75%,  total acc: 72.63%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 72.82%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 72.96%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 73.14%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 73.32%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 73.46%   [EVAL] batch:  150 | acc: 43.75%,  total acc: 73.26%   [EVAL] batch:  151 | acc: 6.25%,  total acc: 72.82%   [EVAL] batch:  152 | acc: 25.00%,  total acc: 72.51%   [EVAL] batch:  153 | acc: 37.50%,  total acc: 72.28%   [EVAL] batch:  154 | acc: 25.00%,  total acc: 71.98%   [EVAL] batch:  155 | acc: 31.25%,  total acc: 71.71%   [EVAL] batch:  156 | acc: 75.00%,  total acc: 71.74%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 71.88%   [EVAL] batch:  158 | acc: 100.00%,  total acc: 72.05%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 72.19%   [EVAL] batch:  160 | acc: 87.50%,  total acc: 72.28%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 72.42%   [EVAL] batch:  162 | acc: 81.25%,  total acc: 72.47%   [EVAL] batch:  163 | acc: 68.75%,  total acc: 72.45%   [EVAL] batch:  164 | acc: 68.75%,  total acc: 72.42%   [EVAL] batch:  165 | acc: 62.50%,  total acc: 72.36%   [EVAL] batch:  166 | acc: 87.50%,  total acc: 72.46%   [EVAL] batch:  167 | acc: 75.00%,  total acc: 72.47%   [EVAL] batch:  168 | acc: 68.75%,  total acc: 72.45%   [EVAL] batch:  169 | acc: 50.00%,  total acc: 72.32%   [EVAL] batch:  170 | acc: 75.00%,  total acc: 72.33%   [EVAL] batch:  171 | acc: 68.75%,  total acc: 72.31%   [EVAL] batch:  172 | acc: 62.50%,  total acc: 72.25%   [EVAL] batch:  173 | acc: 81.25%,  total acc: 72.31%   [EVAL] batch:  174 | acc: 81.25%,  total acc: 72.36%   [EVAL] batch:  175 | acc: 68.75%,  total acc: 72.34%   [EVAL] batch:  176 | acc: 56.25%,  total acc: 72.25%   [EVAL] batch:  177 | acc: 75.00%,  total acc: 72.26%   [EVAL] batch:  178 | acc: 75.00%,  total acc: 72.28%   [EVAL] batch:  179 | acc: 75.00%,  total acc: 72.29%   [EVAL] batch:  180 | acc: 56.25%,  total acc: 72.20%   [EVAL] batch:  181 | acc: 75.00%,  total acc: 72.22%   [EVAL] batch:  182 | acc: 93.75%,  total acc: 72.34%   [EVAL] batch:  183 | acc: 93.75%,  total acc: 72.45%   [EVAL] batch:  184 | acc: 75.00%,  total acc: 72.47%   [EVAL] batch:  185 | acc: 93.75%,  total acc: 72.58%   [EVAL] batch:  186 | acc: 81.25%,  total acc: 72.63%   [EVAL] batch:  187 | acc: 93.75%,  total acc: 72.74%   [EVAL] batch:  188 | acc: 93.75%,  total acc: 72.85%   [EVAL] batch:  189 | acc: 93.75%,  total acc: 72.96%   [EVAL] batch:  190 | acc: 100.00%,  total acc: 73.10%   [EVAL] batch:  191 | acc: 81.25%,  total acc: 73.14%   [EVAL] batch:  192 | acc: 100.00%,  total acc: 73.28%   [EVAL] batch:  193 | acc: 93.75%,  total acc: 73.39%   [EVAL] batch:  194 | acc: 62.50%,  total acc: 73.33%   [EVAL] batch:  195 | acc: 68.75%,  total acc: 73.31%   [EVAL] batch:  196 | acc: 68.75%,  total acc: 73.29%   [EVAL] batch:  197 | acc: 50.00%,  total acc: 73.17%   [EVAL] batch:  198 | acc: 56.25%,  total acc: 73.08%   [EVAL] batch:  199 | acc: 56.25%,  total acc: 73.00%   [EVAL] batch:  200 | acc: 81.25%,  total acc: 73.04%   [EVAL] batch:  201 | acc: 87.50%,  total acc: 73.11%   [EVAL] batch:  202 | acc: 68.75%,  total acc: 73.09%   [EVAL] batch:  203 | acc: 56.25%,  total acc: 73.01%   [EVAL] batch:  204 | acc: 56.25%,  total acc: 72.93%   [EVAL] batch:  205 | acc: 75.00%,  total acc: 72.94%   [EVAL] batch:  206 | acc: 37.50%,  total acc: 72.77%   [EVAL] batch:  207 | acc: 37.50%,  total acc: 72.60%   [EVAL] batch:  208 | acc: 50.00%,  total acc: 72.49%   [EVAL] batch:  209 | acc: 25.00%,  total acc: 72.26%   [EVAL] batch:  210 | acc: 31.25%,  total acc: 72.07%   [EVAL] batch:  211 | acc: 31.25%,  total acc: 71.88%   [EVAL] batch:  212 | acc: 75.00%,  total acc: 71.89%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 72.02%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 72.15%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 72.28%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 72.41%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 72.53%   [EVAL] batch:  218 | acc: 93.75%,  total acc: 72.63%   [EVAL] batch:  219 | acc: 56.25%,  total acc: 72.56%   [EVAL] batch:  220 | acc: 75.00%,  total acc: 72.57%   [EVAL] batch:  221 | acc: 68.75%,  total acc: 72.55%   [EVAL] batch:  222 | acc: 56.25%,  total acc: 72.48%   [EVAL] batch:  223 | acc: 75.00%,  total acc: 72.49%   [EVAL] batch:  224 | acc: 37.50%,  total acc: 72.33%   [EVAL] batch:  225 | acc: 100.00%,  total acc: 72.46%   [EVAL] batch:  226 | acc: 100.00%,  total acc: 72.58%   [EVAL] batch:  227 | acc: 100.00%,  total acc: 72.70%   [EVAL] batch:  228 | acc: 100.00%,  total acc: 72.82%   [EVAL] batch:  229 | acc: 100.00%,  total acc: 72.93%   [EVAL] batch:  230 | acc: 93.75%,  total acc: 73.02%   [EVAL] batch:  231 | acc: 100.00%,  total acc: 73.14%   [EVAL] batch:  232 | acc: 100.00%,  total acc: 73.26%   [EVAL] batch:  233 | acc: 100.00%,  total acc: 73.37%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 73.48%   [EVAL] batch:  235 | acc: 100.00%,  total acc: 73.60%   [EVAL] batch:  236 | acc: 100.00%,  total acc: 73.71%   [EVAL] batch:  237 | acc: 100.00%,  total acc: 73.82%   [EVAL] batch:  238 | acc: 100.00%,  total acc: 73.93%   [EVAL] batch:  239 | acc: 93.75%,  total acc: 74.01%   [EVAL] batch:  240 | acc: 100.00%,  total acc: 74.12%   [EVAL] batch:  241 | acc: 93.75%,  total acc: 74.20%   [EVAL] batch:  242 | acc: 93.75%,  total acc: 74.28%   [EVAL] batch:  243 | acc: 93.75%,  total acc: 74.36%   [EVAL] batch:  244 | acc: 93.75%,  total acc: 74.44%   [EVAL] batch:  245 | acc: 87.50%,  total acc: 74.49%   [EVAL] batch:  246 | acc: 93.75%,  total acc: 74.57%   [EVAL] batch:  247 | acc: 100.00%,  total acc: 74.67%   [EVAL] batch:  248 | acc: 87.50%,  total acc: 74.72%   [EVAL] batch:  249 | acc: 93.75%,  total acc: 74.80%   [EVAL] batch:  250 | acc: 31.25%,  total acc: 74.63%   [EVAL] batch:  251 | acc: 31.25%,  total acc: 74.45%   [EVAL] batch:  252 | acc: 50.00%,  total acc: 74.36%   [EVAL] batch:  253 | acc: 31.25%,  total acc: 74.19%   [EVAL] batch:  254 | acc: 37.50%,  total acc: 74.04%   [EVAL] batch:  255 | acc: 31.25%,  total acc: 73.88%   [EVAL] batch:  256 | acc: 50.00%,  total acc: 73.78%   [EVAL] batch:  257 | acc: 68.75%,  total acc: 73.76%   [EVAL] batch:  258 | acc: 68.75%,  total acc: 73.75%   [EVAL] batch:  259 | acc: 62.50%,  total acc: 73.70%   [EVAL] batch:  260 | acc: 43.75%,  total acc: 73.59%   [EVAL] batch:  261 | acc: 68.75%,  total acc: 73.57%   [EVAL] batch:  262 | acc: 87.50%,  total acc: 73.62%   [EVAL] batch:  263 | acc: 56.25%,  total acc: 73.56%   [EVAL] batch:  264 | acc: 56.25%,  total acc: 73.49%   [EVAL] batch:  265 | acc: 56.25%,  total acc: 73.43%   [EVAL] batch:  266 | acc: 68.75%,  total acc: 73.41%   [EVAL] batch:  267 | acc: 68.75%,  total acc: 73.39%   [EVAL] batch:  268 | acc: 87.50%,  total acc: 73.44%   [EVAL] batch:  269 | acc: 81.25%,  total acc: 73.47%   [EVAL] batch:  270 | acc: 93.75%,  total acc: 73.55%   [EVAL] batch:  271 | acc: 100.00%,  total acc: 73.64%   [EVAL] batch:  272 | acc: 93.75%,  total acc: 73.72%   [EVAL] batch:  273 | acc: 87.50%,  total acc: 73.77%   [EVAL] batch:  274 | acc: 87.50%,  total acc: 73.82%   [EVAL] batch:  275 | acc: 75.00%,  total acc: 73.82%   [EVAL] batch:  276 | acc: 62.50%,  total acc: 73.78%   [EVAL] batch:  277 | acc: 87.50%,  total acc: 73.83%   [EVAL] batch:  278 | acc: 75.00%,  total acc: 73.84%   [EVAL] batch:  279 | acc: 81.25%,  total acc: 73.86%   [EVAL] batch:  280 | acc: 75.00%,  total acc: 73.87%   [EVAL] batch:  281 | acc: 43.75%,  total acc: 73.76%   [EVAL] batch:  282 | acc: 18.75%,  total acc: 73.56%   [EVAL] batch:  283 | acc: 6.25%,  total acc: 73.33%   [EVAL] batch:  284 | acc: 18.75%,  total acc: 73.14%   [EVAL] batch:  285 | acc: 6.25%,  total acc: 72.90%   [EVAL] batch:  286 | acc: 0.00%,  total acc: 72.65%   [EVAL] batch:  287 | acc: 50.00%,  total acc: 72.57%   [EVAL] batch:  288 | acc: 87.50%,  total acc: 72.62%   [EVAL] batch:  289 | acc: 81.25%,  total acc: 72.65%   [EVAL] batch:  290 | acc: 81.25%,  total acc: 72.68%   [EVAL] batch:  291 | acc: 93.75%,  total acc: 72.75%   [EVAL] batch:  292 | acc: 75.00%,  total acc: 72.76%   [EVAL] batch:  293 | acc: 81.25%,  total acc: 72.79%   [EVAL] batch:  294 | acc: 75.00%,  total acc: 72.80%   [EVAL] batch:  295 | acc: 62.50%,  total acc: 72.76%   [EVAL] batch:  296 | acc: 56.25%,  total acc: 72.71%   [EVAL] batch:  297 | acc: 56.25%,  total acc: 72.65%   [EVAL] batch:  298 | acc: 81.25%,  total acc: 72.68%   [EVAL] batch:  299 | acc: 81.25%,  total acc: 72.71%   [EVAL] batch:  300 | acc: 100.00%,  total acc: 72.80%   [EVAL] batch:  301 | acc: 100.00%,  total acc: 72.89%   [EVAL] batch:  302 | acc: 100.00%,  total acc: 72.98%   [EVAL] batch:  303 | acc: 100.00%,  total acc: 73.07%   [EVAL] batch:  304 | acc: 100.00%,  total acc: 73.16%   [EVAL] batch:  305 | acc: 100.00%,  total acc: 73.24%   [EVAL] batch:  306 | acc: 93.75%,  total acc: 73.31%   [EVAL] batch:  307 | acc: 87.50%,  total acc: 73.36%   [EVAL] batch:  308 | acc: 93.75%,  total acc: 73.42%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 73.51%   [EVAL] batch:  310 | acc: 93.75%,  total acc: 73.57%   [EVAL] batch:  311 | acc: 100.00%,  total acc: 73.66%   [EVAL] batch:  312 | acc: 68.75%,  total acc: 73.64%   [EVAL] batch:  313 | acc: 12.50%,  total acc: 73.45%   [EVAL] batch:  314 | acc: 25.00%,  total acc: 73.29%   [EVAL] batch:  315 | acc: 12.50%,  total acc: 73.10%   [EVAL] batch:  316 | acc: 12.50%,  total acc: 72.91%   [EVAL] batch:  317 | acc: 37.50%,  total acc: 72.80%   [EVAL] batch:  318 | acc: 18.75%,  total acc: 72.63%   [EVAL] batch:  319 | acc: 81.25%,  total acc: 72.66%   [EVAL] batch:  320 | acc: 87.50%,  total acc: 72.70%   [EVAL] batch:  321 | acc: 87.50%,  total acc: 72.75%   [EVAL] batch:  322 | acc: 100.00%,  total acc: 72.83%   [EVAL] batch:  323 | acc: 87.50%,  total acc: 72.88%   [EVAL] batch:  324 | acc: 81.25%,  total acc: 72.90%   [EVAL] batch:  325 | acc: 62.50%,  total acc: 72.87%   [EVAL] batch:  326 | acc: 31.25%,  total acc: 72.74%   [EVAL] batch:  327 | acc: 62.50%,  total acc: 72.71%   [EVAL] batch:  328 | acc: 75.00%,  total acc: 72.72%   [EVAL] batch:  329 | acc: 62.50%,  total acc: 72.69%   [EVAL] batch:  330 | acc: 31.25%,  total acc: 72.56%   [EVAL] batch:  331 | acc: 93.75%,  total acc: 72.63%   [EVAL] batch:  332 | acc: 100.00%,  total acc: 72.71%   [EVAL] batch:  333 | acc: 100.00%,  total acc: 72.79%   [EVAL] batch:  334 | acc: 93.75%,  total acc: 72.85%   [EVAL] batch:  335 | acc: 100.00%,  total acc: 72.94%   [EVAL] batch:  336 | acc: 100.00%,  total acc: 73.02%   [EVAL] batch:  337 | acc: 87.50%,  total acc: 73.06%   [EVAL] batch:  338 | acc: 81.25%,  total acc: 73.08%   [EVAL] batch:  339 | acc: 81.25%,  total acc: 73.11%   [EVAL] batch:  340 | acc: 93.75%,  total acc: 73.17%   [EVAL] batch:  341 | acc: 75.00%,  total acc: 73.17%   [EVAL] batch:  342 | acc: 100.00%,  total acc: 73.25%   [EVAL] batch:  343 | acc: 87.50%,  total acc: 73.29%   [EVAL] batch:  344 | acc: 100.00%,  total acc: 73.37%   [EVAL] batch:  345 | acc: 100.00%,  total acc: 73.45%   [EVAL] batch:  346 | acc: 87.50%,  total acc: 73.49%   [EVAL] batch:  347 | acc: 87.50%,  total acc: 73.53%   [EVAL] batch:  348 | acc: 100.00%,  total acc: 73.60%   [EVAL] batch:  349 | acc: 100.00%,  total acc: 73.68%   [EVAL] batch:  350 | acc: 68.75%,  total acc: 73.66%   [EVAL] batch:  351 | acc: 68.75%,  total acc: 73.65%   [EVAL] batch:  352 | acc: 68.75%,  total acc: 73.64%   [EVAL] batch:  353 | acc: 50.00%,  total acc: 73.57%   [EVAL] batch:  354 | acc: 43.75%,  total acc: 73.49%   [EVAL] batch:  355 | acc: 62.50%,  total acc: 73.46%   [EVAL] batch:  356 | acc: 93.75%,  total acc: 73.51%   [EVAL] batch:  357 | acc: 75.00%,  total acc: 73.52%   [EVAL] batch:  358 | acc: 87.50%,  total acc: 73.56%   [EVAL] batch:  359 | acc: 87.50%,  total acc: 73.59%   [EVAL] batch:  360 | acc: 75.00%,  total acc: 73.60%   [EVAL] batch:  361 | acc: 62.50%,  total acc: 73.57%   [EVAL] batch:  362 | acc: 68.75%,  total acc: 73.55%   [EVAL] batch:  363 | acc: 62.50%,  total acc: 73.52%   [EVAL] batch:  364 | acc: 62.50%,  total acc: 73.49%   [EVAL] batch:  365 | acc: 68.75%,  total acc: 73.48%   [EVAL] batch:  366 | acc: 62.50%,  total acc: 73.45%   [EVAL] batch:  367 | acc: 56.25%,  total acc: 73.40%   [EVAL] batch:  368 | acc: 37.50%,  total acc: 73.31%   [EVAL] batch:  369 | acc: 81.25%,  total acc: 73.33%   [EVAL] batch:  370 | acc: 68.75%,  total acc: 73.32%   [EVAL] batch:  371 | acc: 75.00%,  total acc: 73.32%   [EVAL] batch:  372 | acc: 93.75%,  total acc: 73.37%   [EVAL] batch:  373 | acc: 81.25%,  total acc: 73.40%   [EVAL] batch:  374 | acc: 81.25%,  total acc: 73.42%   
cur_acc:  ['0.9524', '0.8135', '0.7579', '0.8502', '0.7331', '0.7143']
his_acc:  ['0.9524', '0.8640', '0.8165', '0.8065', '0.7746', '0.7342']
CurrentTrain: epoch 15, batch     0 | loss: 23.5216317CurrentTrain: epoch 15, batch     1 | loss: 19.2955870CurrentTrain: epoch 15, batch     2 | loss: 19.8406382CurrentTrain: epoch  1, batch     3 | loss: 11.0540609CurrentTrain: epoch 15, batch     0 | loss: 17.9629335CurrentTrain: epoch 15, batch     1 | loss: 18.6851161CurrentTrain: epoch 15, batch     2 | loss: 14.9597436CurrentTrain: epoch  1, batch     3 | loss: 8.5301610CurrentTrain: epoch 15, batch     0 | loss: 12.6025699CurrentTrain: epoch 15, batch     1 | loss: 11.8736298CurrentTrain: epoch 15, batch     2 | loss: 13.7612581CurrentTrain: epoch  1, batch     3 | loss: 23.5791459CurrentTrain: epoch 15, batch     0 | loss: 10.5032544CurrentTrain: epoch 15, batch     1 | loss: 10.4349453CurrentTrain: epoch 15, batch     2 | loss: 11.7576932CurrentTrain: epoch  1, batch     3 | loss: 7.4412408CurrentTrain: epoch 15, batch     0 | loss: 12.2858023CurrentTrain: epoch 15, batch     1 | loss: 10.1892023CurrentTrain: epoch 15, batch     2 | loss: 15.0412761CurrentTrain: epoch  1, batch     3 | loss: 7.4162457CurrentTrain: epoch 15, batch     0 | loss: 11.0123113CurrentTrain: epoch 15, batch     1 | loss: 8.7659847CurrentTrain: epoch 15, batch     2 | loss: 10.6598857CurrentTrain: epoch  1, batch     3 | loss: 7.1947271CurrentTrain: epoch 15, batch     0 | loss: 13.2491851CurrentTrain: epoch 15, batch     1 | loss: 27.0988910CurrentTrain: epoch 15, batch     2 | loss: 18.6004821CurrentTrain: epoch  1, batch     3 | loss: 7.6032043CurrentTrain: epoch 15, batch     0 | loss: 14.2547493CurrentTrain: epoch 15, batch     1 | loss: 9.4275611CurrentTrain: epoch 15, batch     2 | loss: 18.0130082CurrentTrain: epoch  1, batch     3 | loss: 6.9914789CurrentTrain: epoch 15, batch     0 | loss: 16.1095061CurrentTrain: epoch 15, batch     1 | loss: 16.9112387CurrentTrain: epoch 15, batch     2 | loss: 9.1221703CurrentTrain: epoch  1, batch     3 | loss: 5.6477790CurrentTrain: epoch 15, batch     0 | loss: 12.1027770CurrentTrain: epoch 15, batch     1 | loss: 10.2911356CurrentTrain: epoch 15, batch     2 | loss: 17.1506804CurrentTrain: epoch  1, batch     3 | loss: 7.6936314
#### located on terrain feature ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "located on terrain feature" means located on the specified physical/geophysical feature.
Here is an example:
Relation: located on terrain feature
Context: it is situated in the historic lower lusatia region , about south of beeskow , and north of cottbus .
Head Entity: cottbus
Tail Entity: lower lusatia
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "located on terrain feature":
Relation: located on terrain feature  
Context: The ancient ruins are found on the slopes of the majestic Andes mountains, providing a breathtaking view of the valley below.  
Head Entity: Andes mountains  
Tail Entity: ancient ruins  

Relation: located on terrain feature  
Context: The village is nestled in the heart of the lush Black Forest, surrounded by towering trees and serene lakes.  
Head Entity: Black Forest  
Tail Entity: village  
#### participant of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "participant of" means event in which a person or organization was/is a participant.
Here is an example:
Relation: participant of
Context: tiago venâncio ( born august 19 , 1987 , setúbal ) is a freestyle swimmer from portugal , who represented his home country at the 2004 summer olympics in athens , greece .
Head Entity: tiago venâncio
Tail Entity: 2004 summer olympics
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "participant of":
Relation: participant of  
Context: the united nations climate change conference, held in paris in 2015, saw numerous world leaders and activists come together to discuss global warming.  
Head Entity: united nations climate change conference  
Tail Entity: paris agreement  

Relation: participant of  
Context: the 2022 fifa world cup in qatar featured teams from around the globe competing for the prestigious trophy.  
Head Entity: 2022 fifa world cup  
Tail Entity: qatar
#### director ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "director" means director of film, TV-series, stageplay, video game or similar.
Here is an example:
Relation: director
Context: luna subsequently had a supporting role in the comedy film " gravy " , directed by james roday , which was released in october 2015 .
Head Entity: gravy
Tail Entity: james roday
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "director":
Relation: director  
Context: the acclaimed director christopher nolan is known for his work on the blockbuster film "inception," which was released in 2010.  
Head Entity: inception  
Tail Entity: christopher nolan  

Relation: director  
Context: in the recent documentary "the last dance," the legendary coach phil jackson is featured prominently, showcasing his role as the director of the chicago bulls' success in the 1990s.  
Head Entity: the last dance  
Tail Entity: phil jackson  
#### owned by ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "owned by" means owner of the subject.
Here is an example:
Relation: owned by
Context: tata prima is a range of heavy trucks produced by tata daewoo , a wholly owned subsidiary of tata motors of india .
Head Entity: tata daewoo
Tail Entity: tata motors
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "owned by":
Relation: owned by  
Context: The famous luxury brand Gucci is owned by the French conglomerate Kering, which also owns several other high-end fashion labels.  
Head Entity: Gucci  
Tail Entity: Kering  

Relation: owned by  
Context: The popular social media platform Instagram is owned by Meta Platforms, Inc., which also owns Facebook and WhatsApp.  
Head Entity: Instagram  
Tail Entity: Meta Platforms, Inc.  
#### part of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "part of" means object of which the subject is a part.
Here is an example:
Relation: part of
Context: thessaly was home to extensive neolithic and chalcolithic cultures around 6000 bc–2500 bc ( see cardium pottery , dimini and sesklo ) .
Head Entity: cardium pottery
Tail Entity: neolithic
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "part of":
Relation: part of  
Context: The engine is a crucial part of the car's overall functionality, providing the necessary power to drive the vehicle.  
Head Entity: engine  
Tail Entity: car  

Relation: part of  
Context: The leaf is an essential part of the plant, playing a vital role in photosynthesis and energy production.  
Head Entity: leaf  
Tail Entity: plant  
#### occupant ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "occupant" means person or organization occupying property.
Here is an example:
Relation: occupant
Context: the brewery closed in 1996 and is now the site of the halliwell jones stadium , home of warrington wolves , as well as a tesco store .
Head Entity: halliwell jones stadium
Tail Entity: warrington wolves
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "occupant":
Relation: occupant  
Context: The historic building was transformed into a cultural center, which now hosts various events and is home to the local art gallery.  
Head Entity: cultural center  
Tail Entity: local art gallery  

Relation: occupant  
Context: After the renovation, the old factory became a vibrant community hub, serving as the headquarters for several non-profit organizations.  
Head Entity: community hub  
Tail Entity: non-profit organizations  
#### architect ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "architect" means person or architectural firm responsible for designing this building.
Here is an example:
Relation: architect
Context: he also commissioned renowned architect richard neutra to design the iconic desert kaufmann house ( 1946 ) in palm springs , california .
Head Entity: kaufmann house
Tail Entity: richard neutra
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "architect":
Relation: architect  
Context: the innovative design of the new city library was created by the famous architect zaha hadid, known for her futuristic structures.  
Head Entity: city library  
Tail Entity: zaha hadid  

Relation: architect  
Context: after years of planning, the historic renovation of the old train station was finally completed, thanks to the talented architect frank gehry.  
Head Entity: old train station  
Tail Entity: frank gehry  
#### residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "residence" means the place where the person is or has been, resident.
Here is an example:
Relation: residence
Context: cyrus alexander was born in pennsylvania , and his family soon moved to illinois .
Head Entity: cyrus alexander
Tail Entity: illinois
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "residence":
Relation: residence  
Context: after living in new york for several years, emily decided to relocate to california for a fresh start.  
Head Entity: emily  
Tail Entity: california  

Relation: residence  
Context: during his childhood, michael spent most of his time in texas before moving to florida as a teenager.  
Head Entity: michael  
Tail Entity: florida  
#### composer ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "composer" means person who wrote the music.
Here is an example:
Relation: composer
Context: at a young age he was an accomplished cellist , performing at age seventeen the solo part of richard strauss 's " don quixote " .
Head Entity: don quixote
Tail Entity: richard strauss
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "composer":
Relation: composer  
Context: The renowned composer Ludwig van Beethoven created some of the most celebrated symphonies in classical music history.  
Head Entity: symphonies  
Tail Entity: Ludwig van Beethoven  

Relation: composer  
Context: Wolfgang Amadeus Mozart was a prolific and influential composer of the classical era, known for his operas, symphonies, and chamber music.  
Head Entity: operas  
Tail Entity: Wolfgang Amadeus Mozart  
#### location ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "location" means location of the object, structure or event.
Here is an example:
Relation: location
Context: one polish archer qualified for the women 's individual recurve by obtaining one of the eight olympic places available from the 2015 world archery championships in copenhagen , denmark .
Head Entity: 2015 world archery championships
Tail Entity: copenhagen
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "location":
Relation: location  
Context: The annual music festival will take place in the vibrant city of Austin, Texas, attracting thousands of fans from around the world.  
Head Entity: annual music festival  
Tail Entity: Austin, Texas  

Relation: location  
Context: The historic battle was fought near the banks of the Potomac River, a significant site in American history.  
Head Entity: historic battle  
Tail Entity: Potomac River  
MemoryTrain:  epoch 15, batch     0 | loss: 4.7808142MemoryTrain:  epoch 15, batch     1 | loss: 4.3757157MemoryTrain:  epoch 15, batch     2 | loss: 2.7213157MemoryTrain:  epoch 15, batch     3 | loss: 3.3549502MemoryTrain:  epoch 15, batch     4 | loss: 2.9962513MemoryTrain:  epoch 15, batch     5 | loss: 3.2342530MemoryTrain:  epoch 15, batch     6 | loss: 3.0070242MemoryTrain:  epoch 15, batch     7 | loss: 3.8061138MemoryTrain:  epoch 15, batch     8 | loss: 2.6588265MemoryTrain:  epoch 15, batch     9 | loss: 3.4819235MemoryTrain:  epoch 15, batch    10 | loss: 3.9340968MemoryTrain:  epoch 15, batch    11 | loss: 5.5667071MemoryTrain:  epoch 15, batch    12 | loss: 3.0348329MemoryTrain:  epoch  1, batch    13 | loss: 5.9604010MemoryTrain:  epoch 15, batch     0 | loss: 3.2172048MemoryTrain:  epoch 15, batch     1 | loss: 2.6985209MemoryTrain:  epoch 15, batch     2 | loss: 3.9437558MemoryTrain:  epoch 15, batch     3 | loss: 5.6872294MemoryTrain:  epoch 15, batch     4 | loss: 3.0194894MemoryTrain:  epoch 15, batch     5 | loss: 2.3928318MemoryTrain:  epoch 15, batch     6 | loss: 1.9521552MemoryTrain:  epoch 15, batch     7 | loss: 2.6585230MemoryTrain:  epoch 15, batch     8 | loss: 2.3284976MemoryTrain:  epoch 15, batch     9 | loss: 5.0538886MemoryTrain:  epoch 15, batch    10 | loss: 2.8198852MemoryTrain:  epoch 15, batch    11 | loss: 4.7822415MemoryTrain:  epoch 15, batch    12 | loss: 1.8556051MemoryTrain:  epoch  1, batch    13 | loss: 5.2331894MemoryTrain:  epoch 15, batch     0 | loss: 7.1689192MemoryTrain:  epoch 15, batch     1 | loss: 2.2242900MemoryTrain:  epoch 15, batch     2 | loss: 3.6287172MemoryTrain:  epoch 15, batch     3 | loss: 1.9225867MemoryTrain:  epoch 15, batch     4 | loss: 3.5900909MemoryTrain:  epoch 15, batch     5 | loss: 2.4527954MemoryTrain:  epoch 15, batch     6 | loss: 2.7002219MemoryTrain:  epoch 15, batch     7 | loss: 2.1899918MemoryTrain:  epoch 15, batch     8 | loss: 1.9493520MemoryTrain:  epoch 15, batch     9 | loss: 2.8648129MemoryTrain:  epoch 15, batch    10 | loss: 2.3301821MemoryTrain:  epoch 15, batch    11 | loss: 2.8864901MemoryTrain:  epoch 15, batch    12 | loss: 4.2619362MemoryTrain:  epoch  1, batch    13 | loss: 4.9491146MemoryTrain:  epoch 15, batch     0 | loss: 2.9416744MemoryTrain:  epoch 15, batch     1 | loss: 2.0500104MemoryTrain:  epoch 15, batch     2 | loss: 2.2042397MemoryTrain:  epoch 15, batch     3 | loss: 1.9625219MemoryTrain:  epoch 15, batch     4 | loss: 4.6447234MemoryTrain:  epoch 15, batch     5 | loss: 2.2395899MemoryTrain:  epoch 15, batch     6 | loss: 3.8217435MemoryTrain:  epoch 15, batch     7 | loss: 4.6907802MemoryTrain:  epoch 15, batch     8 | loss: 3.4429754MemoryTrain:  epoch 15, batch     9 | loss: 2.8358414MemoryTrain:  epoch 15, batch    10 | loss: 3.3221544MemoryTrain:  epoch 15, batch    11 | loss: 1.4491736MemoryTrain:  epoch 15, batch    12 | loss: 2.7856039MemoryTrain:  epoch  1, batch    13 | loss: 6.4613222MemoryTrain:  epoch 15, batch     0 | loss: 2.7232817MemoryTrain:  epoch 15, batch     1 | loss: 1.9422982MemoryTrain:  epoch 15, batch     2 | loss: 2.8929159MemoryTrain:  epoch 15, batch     3 | loss: 2.3877090MemoryTrain:  epoch 15, batch     4 | loss: 1.9018838MemoryTrain:  epoch 15, batch     5 | loss: 3.0482374MemoryTrain:  epoch 15, batch     6 | loss: 1.9717975MemoryTrain:  epoch 15, batch     7 | loss: 2.4311848MemoryTrain:  epoch 15, batch     8 | loss: 1.8660581MemoryTrain:  epoch 15, batch     9 | loss: 1.9951830MemoryTrain:  epoch 15, batch    10 | loss: 1.5993672MemoryTrain:  epoch 15, batch    11 | loss: 5.4394413MemoryTrain:  epoch 15, batch    12 | loss: 4.3187604MemoryTrain:  epoch  1, batch    13 | loss: 5.7541669MemoryTrain:  epoch 15, batch     0 | loss: 4.9378181MemoryTrain:  epoch 15, batch     1 | loss: 2.0885647MemoryTrain:  epoch 15, batch     2 | loss: 4.2458122MemoryTrain:  epoch 15, batch     3 | loss: 3.0489498MemoryTrain:  epoch 15, batch     4 | loss: 2.5799612MemoryTrain:  epoch 15, batch     5 | loss: 3.8393190MemoryTrain:  epoch 15, batch     6 | loss: 1.3067179MemoryTrain:  epoch 15, batch     7 | loss: 1.7127341MemoryTrain:  epoch 15, batch     8 | loss: 4.6511029MemoryTrain:  epoch 15, batch     9 | loss: 1.5354387MemoryTrain:  epoch 15, batch    10 | loss: 1.9613091MemoryTrain:  epoch 15, batch    11 | loss: 2.0641678MemoryTrain:  epoch 15, batch    12 | loss: 1.5747573MemoryTrain:  epoch  1, batch    13 | loss: 6.8439862MemoryTrain:  epoch 15, batch     0 | loss: 2.3241335MemoryTrain:  epoch 15, batch     1 | loss: 1.6133409MemoryTrain:  epoch 15, batch     2 | loss: 2.7107240MemoryTrain:  epoch 15, batch     3 | loss: 1.9320438MemoryTrain:  epoch 15, batch     4 | loss: 2.1382501MemoryTrain:  epoch 15, batch     5 | loss: 1.9408298MemoryTrain:  epoch 15, batch     6 | loss: 6.7922415MemoryTrain:  epoch 15, batch     7 | loss: 2.4035913MemoryTrain:  epoch 15, batch     8 | loss: 1.6298076MemoryTrain:  epoch 15, batch     9 | loss: 1.4578698MemoryTrain:  epoch 15, batch    10 | loss: 1.9182558MemoryTrain:  epoch 15, batch    11 | loss: 1.3925056MemoryTrain:  epoch 15, batch    12 | loss: 2.2007960MemoryTrain:  epoch  1, batch    13 | loss: 5.6046432MemoryTrain:  epoch 15, batch     0 | loss: 1.7020318MemoryTrain:  epoch 15, batch     1 | loss: 1.9669754MemoryTrain:  epoch 15, batch     2 | loss: 3.1083082MemoryTrain:  epoch 15, batch     3 | loss: 1.6364896MemoryTrain:  epoch 15, batch     4 | loss: 2.1696201MemoryTrain:  epoch 15, batch     5 | loss: 4.2615831MemoryTrain:  epoch 15, batch     6 | loss: 3.5279560MemoryTrain:  epoch 15, batch     7 | loss: 2.4617011MemoryTrain:  epoch 15, batch     8 | loss: 1.5983090MemoryTrain:  epoch 15, batch     9 | loss: 1.5992355MemoryTrain:  epoch 15, batch    10 | loss: 3.3346221MemoryTrain:  epoch 15, batch    11 | loss: 2.1507121MemoryTrain:  epoch 15, batch    12 | loss: 2.1211848MemoryTrain:  epoch  1, batch    13 | loss: 5.8754871MemoryTrain:  epoch 15, batch     0 | loss: 1.8538576MemoryTrain:  epoch 15, batch     1 | loss: 4.1318725MemoryTrain:  epoch 15, batch     2 | loss: 1.4855171MemoryTrain:  epoch 15, batch     3 | loss: 2.2375506MemoryTrain:  epoch 15, batch     4 | loss: 1.6277966MemoryTrain:  epoch 15, batch     5 | loss: 2.1935605MemoryTrain:  epoch 15, batch     6 | loss: 1.3024147MemoryTrain:  epoch 15, batch     7 | loss: 2.8752043MemoryTrain:  epoch 15, batch     8 | loss: 1.3924052MemoryTrain:  epoch 15, batch     9 | loss: 2.8917814MemoryTrain:  epoch 15, batch    10 | loss: 1.3399599MemoryTrain:  epoch 15, batch    11 | loss: 3.0553043MemoryTrain:  epoch 15, batch    12 | loss: 2.5188572MemoryTrain:  epoch  1, batch    13 | loss: 7.1568468MemoryTrain:  epoch 15, batch     0 | loss: 2.7651307MemoryTrain:  epoch 15, batch     1 | loss: 2.8346094MemoryTrain:  epoch 15, batch     2 | loss: 1.3844134MemoryTrain:  epoch 15, batch     3 | loss: 4.6849680MemoryTrain:  epoch 15, batch     4 | loss: 4.6900279MemoryTrain:  epoch 15, batch     5 | loss: 1.3614725MemoryTrain:  epoch 15, batch     6 | loss: 1.9497237MemoryTrain:  epoch 15, batch     7 | loss: 1.4218711MemoryTrain:  epoch 15, batch     8 | loss: 1.8556243MemoryTrain:  epoch 15, batch     9 | loss: 2.3730547MemoryTrain:  epoch 15, batch    10 | loss: 1.7988664MemoryTrain:  epoch 15, batch    11 | loss: 1.4538559MemoryTrain:  epoch 15, batch    12 | loss: 1.7661673MemoryTrain:  epoch  1, batch    13 | loss: 5.1292530
[EVAL] batch:    0 | acc: 31.25%,  total acc: 31.25%   [EVAL] batch:    1 | acc: 50.00%,  total acc: 40.62%   [EVAL] batch:    2 | acc: 25.00%,  total acc: 35.42%   [EVAL] batch:    3 | acc: 31.25%,  total acc: 34.38%   [EVAL] batch:    4 | acc: 43.75%,  total acc: 36.25%   [EVAL] batch:    5 | acc: 25.00%,  total acc: 34.38%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 40.18%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 46.88%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 52.08%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 55.62%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 59.09%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 61.46%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 63.46%   [EVAL] batch:   13 | acc: 75.00%,  total acc: 64.29%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 66.25%   [EVAL] batch:   15 | acc: 81.25%,  total acc: 67.19%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 68.75%   [EVAL] batch:   17 | acc: 87.50%,  total acc: 69.79%   [EVAL] batch:   18 | acc: 75.00%,  total acc: 70.07%   [EVAL] batch:   19 | acc: 12.50%,  total acc: 67.19%   [EVAL] batch:   20 | acc: 18.75%,  total acc: 64.88%   [EVAL] batch:   21 | acc: 25.00%,  total acc: 63.07%   [EVAL] batch:   22 | acc: 43.75%,  total acc: 62.23%   [EVAL] batch:   23 | acc: 31.25%,  total acc: 60.94%   [EVAL] batch:   24 | acc: 31.25%,  total acc: 59.75%   [EVAL] batch:   25 | acc: 6.25%,  total acc: 57.69%   [EVAL] batch:   26 | acc: 12.50%,  total acc: 56.02%   [EVAL] batch:   27 | acc: 6.25%,  total acc: 54.24%   [EVAL] batch:   28 | acc: 6.25%,  total acc: 52.59%   [EVAL] batch:   29 | acc: 12.50%,  total acc: 51.25%   [EVAL] batch:   30 | acc: 0.00%,  total acc: 49.60%   [EVAL] batch:   31 | acc: 43.75%,  total acc: 49.41%   [EVAL] batch:   32 | acc: 81.25%,  total acc: 50.38%   [EVAL] batch:   33 | acc: 75.00%,  total acc: 51.10%   [EVAL] batch:   34 | acc: 81.25%,  total acc: 51.96%   [EVAL] batch:   35 | acc: 81.25%,  total acc: 52.78%   [EVAL] batch:   36 | acc: 68.75%,  total acc: 53.21%   [EVAL] batch:   37 | acc: 81.25%,  total acc: 53.95%   [EVAL] batch:   38 | acc: 93.75%,  total acc: 54.97%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 56.09%   [EVAL] batch:   40 | acc: 87.50%,  total acc: 56.86%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 57.59%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 58.58%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 59.23%   [EVAL] batch:   44 | acc: 25.00%,  total acc: 58.47%   [EVAL] batch:   45 | acc: 12.50%,  total acc: 57.47%   [EVAL] batch:   46 | acc: 6.25%,  total acc: 56.38%   [EVAL] batch:   47 | acc: 18.75%,  total acc: 55.60%   [EVAL] batch:   48 | acc: 25.00%,  total acc: 54.97%   [EVAL] batch:   49 | acc: 18.75%,  total acc: 54.25%   [EVAL] batch:   50 | acc: 68.75%,  total acc: 54.53%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 55.29%   [EVAL] batch:   52 | acc: 43.75%,  total acc: 55.07%   [EVAL] batch:   53 | acc: 75.00%,  total acc: 55.44%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 55.91%   [EVAL] batch:   55 | acc: 75.00%,  total acc: 56.25%   [EVAL] batch:   56 | acc: 62.50%,  total acc: 56.36%   [EVAL] batch:   57 | acc: 50.00%,  total acc: 56.25%   [EVAL] batch:   58 | acc: 43.75%,  total acc: 56.04%   [EVAL] batch:   59 | acc: 75.00%,  total acc: 56.35%   [EVAL] batch:   60 | acc: 43.75%,  total acc: 56.15%   [EVAL] batch:   61 | acc: 75.00%,  total acc: 56.45%   [EVAL] batch:   62 | acc: 25.00%,  total acc: 55.95%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 62.50%,  total acc: 78.12%   [EVAL] batch:    4 | acc: 62.50%,  total acc: 75.00%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 76.04%   [EVAL] batch:    6 | acc: 37.50%,  total acc: 70.54%   [EVAL] batch:    7 | acc: 31.25%,  total acc: 65.62%   [EVAL] batch:    8 | acc: 31.25%,  total acc: 61.81%   [EVAL] batch:    9 | acc: 50.00%,  total acc: 60.62%   [EVAL] batch:   10 | acc: 31.25%,  total acc: 57.95%   [EVAL] batch:   11 | acc: 37.50%,  total acc: 56.25%   [EVAL] batch:   12 | acc: 50.00%,  total acc: 55.77%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 58.48%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 60.83%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 63.28%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 65.44%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 67.01%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 68.42%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 68.75%   [EVAL] batch:   20 | acc: 81.25%,  total acc: 69.35%   [EVAL] batch:   21 | acc: 56.25%,  total acc: 68.75%   [EVAL] batch:   22 | acc: 43.75%,  total acc: 67.66%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 68.49%   [EVAL] batch:   24 | acc: 68.75%,  total acc: 68.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 69.71%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 70.83%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 71.65%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 72.41%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 73.33%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 73.99%   [EVAL] batch:   31 | acc: 87.50%,  total acc: 74.41%   [EVAL] batch:   32 | acc: 87.50%,  total acc: 74.81%   [EVAL] batch:   33 | acc: 50.00%,  total acc: 74.08%   [EVAL] batch:   34 | acc: 62.50%,  total acc: 73.75%   [EVAL] batch:   35 | acc: 68.75%,  total acc: 73.61%   [EVAL] batch:   36 | acc: 75.00%,  total acc: 73.65%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 74.01%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 74.68%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 75.31%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 75.91%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 76.34%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 76.74%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 76.99%   [EVAL] batch:   44 | acc: 68.75%,  total acc: 76.81%   [EVAL] batch:   45 | acc: 75.00%,  total acc: 76.77%   [EVAL] batch:   46 | acc: 62.50%,  total acc: 76.46%   [EVAL] batch:   47 | acc: 62.50%,  total acc: 76.17%   [EVAL] batch:   48 | acc: 93.75%,  total acc: 76.53%   [EVAL] batch:   49 | acc: 56.25%,  total acc: 76.12%   [EVAL] batch:   50 | acc: 81.25%,  total acc: 76.23%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 76.44%   [EVAL] batch:   52 | acc: 75.00%,  total acc: 76.42%   [EVAL] batch:   53 | acc: 62.50%,  total acc: 76.16%   [EVAL] batch:   54 | acc: 50.00%,  total acc: 75.68%   [EVAL] batch:   55 | acc: 75.00%,  total acc: 75.67%   [EVAL] batch:   56 | acc: 75.00%,  total acc: 75.66%   [EVAL] batch:   57 | acc: 31.25%,  total acc: 74.89%   [EVAL] batch:   58 | acc: 56.25%,  total acc: 74.58%   [EVAL] batch:   59 | acc: 68.75%,  total acc: 74.48%   [EVAL] batch:   60 | acc: 68.75%,  total acc: 74.39%   [EVAL] batch:   61 | acc: 43.75%,  total acc: 73.89%   [EVAL] batch:   62 | acc: 43.75%,  total acc: 73.41%   [EVAL] batch:   63 | acc: 75.00%,  total acc: 73.44%   [EVAL] batch:   64 | acc: 62.50%,  total acc: 73.27%   [EVAL] batch:   65 | acc: 43.75%,  total acc: 72.82%   [EVAL] batch:   66 | acc: 56.25%,  total acc: 72.57%   [EVAL] batch:   67 | acc: 68.75%,  total acc: 72.52%   [EVAL] batch:   68 | acc: 75.00%,  total acc: 72.55%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 72.95%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 73.33%   [EVAL] batch:   71 | acc: 87.50%,  total acc: 73.52%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 73.89%   [EVAL] batch:   73 | acc: 100.00%,  total acc: 74.24%   [EVAL] batch:   74 | acc: 93.75%,  total acc: 74.50%   [EVAL] batch:   75 | acc: 75.00%,  total acc: 74.51%   [EVAL] batch:   76 | acc: 56.25%,  total acc: 74.27%   [EVAL] batch:   77 | acc: 81.25%,  total acc: 74.36%   [EVAL] batch:   78 | acc: 68.75%,  total acc: 74.29%   [EVAL] batch:   79 | acc: 68.75%,  total acc: 74.22%   [EVAL] batch:   80 | acc: 68.75%,  total acc: 74.15%   [EVAL] batch:   81 | acc: 43.75%,  total acc: 73.78%   [EVAL] batch:   82 | acc: 25.00%,  total acc: 73.19%   [EVAL] batch:   83 | acc: 25.00%,  total acc: 72.62%   [EVAL] batch:   84 | acc: 12.50%,  total acc: 71.91%   [EVAL] batch:   85 | acc: 12.50%,  total acc: 71.22%   [EVAL] batch:   86 | acc: 18.75%,  total acc: 70.62%   [EVAL] batch:   87 | acc: 43.75%,  total acc: 70.31%   [EVAL] batch:   88 | acc: 56.25%,  total acc: 70.15%   [EVAL] batch:   89 | acc: 68.75%,  total acc: 70.14%   [EVAL] batch:   90 | acc: 62.50%,  total acc: 70.05%   [EVAL] batch:   91 | acc: 43.75%,  total acc: 69.77%   [EVAL] batch:   92 | acc: 56.25%,  total acc: 69.62%   [EVAL] batch:   93 | acc: 50.00%,  total acc: 69.41%   [EVAL] batch:   94 | acc: 50.00%,  total acc: 69.21%   [EVAL] batch:   95 | acc: 43.75%,  total acc: 68.95%   [EVAL] batch:   96 | acc: 62.50%,  total acc: 68.88%   [EVAL] batch:   97 | acc: 43.75%,  total acc: 68.62%   [EVAL] batch:   98 | acc: 56.25%,  total acc: 68.50%   [EVAL] batch:   99 | acc: 31.25%,  total acc: 68.12%   [EVAL] batch:  100 | acc: 56.25%,  total acc: 68.01%   [EVAL] batch:  101 | acc: 81.25%,  total acc: 68.14%   [EVAL] batch:  102 | acc: 62.50%,  total acc: 68.08%   [EVAL] batch:  103 | acc: 87.50%,  total acc: 68.27%   [EVAL] batch:  104 | acc: 68.75%,  total acc: 68.27%   [EVAL] batch:  105 | acc: 62.50%,  total acc: 68.22%   [EVAL] batch:  106 | acc: 68.75%,  total acc: 68.22%   [EVAL] batch:  107 | acc: 37.50%,  total acc: 67.94%   [EVAL] batch:  108 | acc: 43.75%,  total acc: 67.72%   [EVAL] batch:  109 | acc: 56.25%,  total acc: 67.61%   [EVAL] batch:  110 | acc: 43.75%,  total acc: 67.40%   [EVAL] batch:  111 | acc: 50.00%,  total acc: 67.24%   [EVAL] batch:  112 | acc: 81.25%,  total acc: 67.37%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 67.65%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 67.93%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 68.21%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 68.48%   [EVAL] batch:  117 | acc: 100.00%,  total acc: 68.75%   [EVAL] batch:  118 | acc: 87.50%,  total acc: 68.91%   [EVAL] batch:  119 | acc: 62.50%,  total acc: 68.85%   [EVAL] batch:  120 | acc: 43.75%,  total acc: 68.65%   [EVAL] batch:  121 | acc: 68.75%,  total acc: 68.65%   [EVAL] batch:  122 | acc: 68.75%,  total acc: 68.65%   [EVAL] batch:  123 | acc: 68.75%,  total acc: 68.65%   [EVAL] batch:  124 | acc: 87.50%,  total acc: 68.80%   [EVAL] batch:  125 | acc: 50.00%,  total acc: 68.65%   [EVAL] batch:  126 | acc: 31.25%,  total acc: 68.36%   [EVAL] batch:  127 | acc: 56.25%,  total acc: 68.26%   [EVAL] batch:  128 | acc: 68.75%,  total acc: 68.27%   [EVAL] batch:  129 | acc: 75.00%,  total acc: 68.32%   [EVAL] batch:  130 | acc: 68.75%,  total acc: 68.32%   [EVAL] batch:  131 | acc: 81.25%,  total acc: 68.42%   [EVAL] batch:  132 | acc: 81.25%,  total acc: 68.52%   [EVAL] batch:  133 | acc: 75.00%,  total acc: 68.56%   [EVAL] batch:  134 | acc: 81.25%,  total acc: 68.66%   [EVAL] batch:  135 | acc: 93.75%,  total acc: 68.84%   [EVAL] batch:  136 | acc: 93.75%,  total acc: 69.02%   [EVAL] batch:  137 | acc: 68.75%,  total acc: 69.02%   [EVAL] batch:  138 | acc: 6.25%,  total acc: 68.57%   [EVAL] batch:  139 | acc: 31.25%,  total acc: 68.30%   [EVAL] batch:  140 | acc: 6.25%,  total acc: 67.86%   [EVAL] batch:  141 | acc: 25.00%,  total acc: 67.56%   [EVAL] batch:  142 | acc: 18.75%,  total acc: 67.22%   [EVAL] batch:  143 | acc: 37.50%,  total acc: 67.01%   [EVAL] batch:  144 | acc: 93.75%,  total acc: 67.20%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 67.42%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 67.60%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 67.82%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 68.04%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 68.21%   [EVAL] batch:  150 | acc: 37.50%,  total acc: 68.00%   [EVAL] batch:  151 | acc: 12.50%,  total acc: 67.64%   [EVAL] batch:  152 | acc: 18.75%,  total acc: 67.32%   [EVAL] batch:  153 | acc: 25.00%,  total acc: 67.05%   [EVAL] batch:  154 | acc: 25.00%,  total acc: 66.77%   [EVAL] batch:  155 | acc: 31.25%,  total acc: 66.55%   [EVAL] batch:  156 | acc: 75.00%,  total acc: 66.60%   [EVAL] batch:  157 | acc: 87.50%,  total acc: 66.73%   [EVAL] batch:  158 | acc: 100.00%,  total acc: 66.94%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 67.11%   [EVAL] batch:  160 | acc: 87.50%,  total acc: 67.24%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 67.40%   [EVAL] batch:  162 | acc: 68.75%,  total acc: 67.41%   [EVAL] batch:  163 | acc: 37.50%,  total acc: 67.23%   [EVAL] batch:  164 | acc: 56.25%,  total acc: 67.16%   [EVAL] batch:  165 | acc: 43.75%,  total acc: 67.02%   [EVAL] batch:  166 | acc: 56.25%,  total acc: 66.95%   [EVAL] batch:  167 | acc: 37.50%,  total acc: 66.78%   [EVAL] batch:  168 | acc: 56.25%,  total acc: 66.72%   [EVAL] batch:  169 | acc: 56.25%,  total acc: 66.65%   [EVAL] batch:  170 | acc: 75.00%,  total acc: 66.70%   [EVAL] batch:  171 | acc: 75.00%,  total acc: 66.75%   [EVAL] batch:  172 | acc: 50.00%,  total acc: 66.65%   [EVAL] batch:  173 | acc: 68.75%,  total acc: 66.67%   [EVAL] batch:  174 | acc: 81.25%,  total acc: 66.75%   [EVAL] batch:  175 | acc: 68.75%,  total acc: 66.76%   [EVAL] batch:  176 | acc: 56.25%,  total acc: 66.70%   [EVAL] batch:  177 | acc: 62.50%,  total acc: 66.68%   [EVAL] batch:  178 | acc: 75.00%,  total acc: 66.72%   [EVAL] batch:  179 | acc: 81.25%,  total acc: 66.81%   [EVAL] batch:  180 | acc: 50.00%,  total acc: 66.71%   [EVAL] batch:  181 | acc: 75.00%,  total acc: 66.76%   [EVAL] batch:  182 | acc: 75.00%,  total acc: 66.80%   [EVAL] batch:  183 | acc: 93.75%,  total acc: 66.95%   [EVAL] batch:  184 | acc: 68.75%,  total acc: 66.96%   [EVAL] batch:  185 | acc: 93.75%,  total acc: 67.10%   [EVAL] batch:  186 | acc: 75.00%,  total acc: 67.15%   [EVAL] batch:  187 | acc: 93.75%,  total acc: 67.29%   [EVAL] batch:  188 | acc: 93.75%,  total acc: 67.43%   [EVAL] batch:  189 | acc: 87.50%,  total acc: 67.53%   [EVAL] batch:  190 | acc: 100.00%,  total acc: 67.70%   [EVAL] batch:  191 | acc: 81.25%,  total acc: 67.77%   [EVAL] batch:  192 | acc: 100.00%,  total acc: 67.94%   [EVAL] batch:  193 | acc: 81.25%,  total acc: 68.01%   [EVAL] batch:  194 | acc: 56.25%,  total acc: 67.95%   [EVAL] batch:  195 | acc: 50.00%,  total acc: 67.86%   [EVAL] batch:  196 | acc: 62.50%,  total acc: 67.83%   [EVAL] batch:  197 | acc: 37.50%,  total acc: 67.68%   [EVAL] batch:  198 | acc: 62.50%,  total acc: 67.65%   [EVAL] batch:  199 | acc: 56.25%,  total acc: 67.59%   [EVAL] batch:  200 | acc: 68.75%,  total acc: 67.60%   [EVAL] batch:  201 | acc: 81.25%,  total acc: 67.67%   [EVAL] batch:  202 | acc: 62.50%,  total acc: 67.64%   [EVAL] batch:  203 | acc: 50.00%,  total acc: 67.56%   [EVAL] batch:  204 | acc: 56.25%,  total acc: 67.50%   [EVAL] batch:  205 | acc: 56.25%,  total acc: 67.45%   [EVAL] batch:  206 | acc: 43.75%,  total acc: 67.33%   [EVAL] batch:  207 | acc: 31.25%,  total acc: 67.16%   [EVAL] batch:  208 | acc: 31.25%,  total acc: 66.99%   [EVAL] batch:  209 | acc: 37.50%,  total acc: 66.85%   [EVAL] batch:  210 | acc: 37.50%,  total acc: 66.71%   [EVAL] batch:  211 | acc: 25.00%,  total acc: 66.51%   [EVAL] batch:  212 | acc: 75.00%,  total acc: 66.55%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 66.71%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 66.86%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 67.01%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 67.17%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 67.32%   [EVAL] batch:  218 | acc: 93.75%,  total acc: 67.44%   [EVAL] batch:  219 | acc: 56.25%,  total acc: 67.39%   [EVAL] batch:  220 | acc: 81.25%,  total acc: 67.45%   [EVAL] batch:  221 | acc: 68.75%,  total acc: 67.45%   [EVAL] batch:  222 | acc: 62.50%,  total acc: 67.43%   [EVAL] batch:  223 | acc: 75.00%,  total acc: 67.47%   [EVAL] batch:  224 | acc: 43.75%,  total acc: 67.36%   [EVAL] batch:  225 | acc: 100.00%,  total acc: 67.51%   [EVAL] batch:  226 | acc: 100.00%,  total acc: 67.65%   [EVAL] batch:  227 | acc: 100.00%,  total acc: 67.79%   [EVAL] batch:  228 | acc: 100.00%,  total acc: 67.93%   [EVAL] batch:  229 | acc: 100.00%,  total acc: 68.07%   [EVAL] batch:  230 | acc: 93.75%,  total acc: 68.18%   [EVAL] batch:  231 | acc: 100.00%,  total acc: 68.32%   [EVAL] batch:  232 | acc: 100.00%,  total acc: 68.45%   [EVAL] batch:  233 | acc: 100.00%,  total acc: 68.59%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 68.72%   [EVAL] batch:  235 | acc: 100.00%,  total acc: 68.86%   [EVAL] batch:  236 | acc: 100.00%,  total acc: 68.99%   [EVAL] batch:  237 | acc: 100.00%,  total acc: 69.12%   [EVAL] batch:  238 | acc: 93.75%,  total acc: 69.22%   [EVAL] batch:  239 | acc: 93.75%,  total acc: 69.32%   [EVAL] batch:  240 | acc: 100.00%,  total acc: 69.45%   [EVAL] batch:  241 | acc: 93.75%,  total acc: 69.55%   [EVAL] batch:  242 | acc: 93.75%,  total acc: 69.65%   [EVAL] batch:  243 | acc: 93.75%,  total acc: 69.75%   [EVAL] batch:  244 | acc: 93.75%,  total acc: 69.85%   [EVAL] batch:  245 | acc: 87.50%,  total acc: 69.92%   [EVAL] batch:  246 | acc: 93.75%,  total acc: 70.02%   [EVAL] batch:  247 | acc: 100.00%,  total acc: 70.14%   [EVAL] batch:  248 | acc: 81.25%,  total acc: 70.18%   [EVAL] batch:  249 | acc: 93.75%,  total acc: 70.28%   [EVAL] batch:  250 | acc: 12.50%,  total acc: 70.04%   [EVAL] batch:  251 | acc: 18.75%,  total acc: 69.84%   [EVAL] batch:  252 | acc: 25.00%,  total acc: 69.66%   [EVAL] batch:  253 | acc: 18.75%,  total acc: 69.46%   [EVAL] batch:  254 | acc: 31.25%,  total acc: 69.31%   [EVAL] batch:  255 | acc: 31.25%,  total acc: 69.17%   [EVAL] batch:  256 | acc: 50.00%,  total acc: 69.09%   [EVAL] batch:  257 | acc: 62.50%,  total acc: 69.06%   [EVAL] batch:  258 | acc: 68.75%,  total acc: 69.06%   [EVAL] batch:  259 | acc: 62.50%,  total acc: 69.04%   [EVAL] batch:  260 | acc: 43.75%,  total acc: 68.94%   [EVAL] batch:  261 | acc: 56.25%,  total acc: 68.89%   [EVAL] batch:  262 | acc: 75.00%,  total acc: 68.92%   [EVAL] batch:  263 | acc: 56.25%,  total acc: 68.87%   [EVAL] batch:  264 | acc: 50.00%,  total acc: 68.80%   [EVAL] batch:  265 | acc: 31.25%,  total acc: 68.66%   [EVAL] batch:  266 | acc: 62.50%,  total acc: 68.63%   [EVAL] batch:  267 | acc: 62.50%,  total acc: 68.61%   [EVAL] batch:  268 | acc: 81.25%,  total acc: 68.66%   [EVAL] batch:  269 | acc: 87.50%,  total acc: 68.73%   [EVAL] batch:  270 | acc: 93.75%,  total acc: 68.82%   [EVAL] batch:  271 | acc: 100.00%,  total acc: 68.93%   [EVAL] batch:  272 | acc: 100.00%,  total acc: 69.05%   [EVAL] batch:  273 | acc: 87.50%,  total acc: 69.11%   [EVAL] batch:  274 | acc: 87.50%,  total acc: 69.18%   [EVAL] batch:  275 | acc: 56.25%,  total acc: 69.13%   [EVAL] batch:  276 | acc: 50.00%,  total acc: 69.07%   [EVAL] batch:  277 | acc: 81.25%,  total acc: 69.11%   [EVAL] batch:  278 | acc: 68.75%,  total acc: 69.11%   [EVAL] batch:  279 | acc: 68.75%,  total acc: 69.11%   [EVAL] batch:  280 | acc: 75.00%,  total acc: 69.13%   [EVAL] batch:  281 | acc: 43.75%,  total acc: 69.04%   [EVAL] batch:  282 | acc: 37.50%,  total acc: 68.93%   [EVAL] batch:  283 | acc: 6.25%,  total acc: 68.71%   [EVAL] batch:  284 | acc: 18.75%,  total acc: 68.53%   [EVAL] batch:  285 | acc: 6.25%,  total acc: 68.31%   [EVAL] batch:  286 | acc: 0.00%,  total acc: 68.07%   [EVAL] batch:  287 | acc: 56.25%,  total acc: 68.03%   [EVAL] batch:  288 | acc: 81.25%,  total acc: 68.08%   [EVAL] batch:  289 | acc: 75.00%,  total acc: 68.10%   [EVAL] batch:  290 | acc: 68.75%,  total acc: 68.11%   [EVAL] batch:  291 | acc: 93.75%,  total acc: 68.19%   [EVAL] batch:  292 | acc: 81.25%,  total acc: 68.24%   [EVAL] batch:  293 | acc: 62.50%,  total acc: 68.22%   [EVAL] batch:  294 | acc: 68.75%,  total acc: 68.22%   [EVAL] batch:  295 | acc: 56.25%,  total acc: 68.18%   [EVAL] batch:  296 | acc: 56.25%,  total acc: 68.14%   [EVAL] batch:  297 | acc: 56.25%,  total acc: 68.10%   [EVAL] batch:  298 | acc: 75.00%,  total acc: 68.12%   [EVAL] batch:  299 | acc: 87.50%,  total acc: 68.19%   [EVAL] batch:  300 | acc: 100.00%,  total acc: 68.29%   [EVAL] batch:  301 | acc: 100.00%,  total acc: 68.40%   [EVAL] batch:  302 | acc: 100.00%,  total acc: 68.50%   [EVAL] batch:  303 | acc: 100.00%,  total acc: 68.61%   [EVAL] batch:  304 | acc: 100.00%,  total acc: 68.71%   [EVAL] batch:  305 | acc: 100.00%,  total acc: 68.81%   [EVAL] batch:  306 | acc: 93.75%,  total acc: 68.89%   [EVAL] batch:  307 | acc: 87.50%,  total acc: 68.95%   [EVAL] batch:  308 | acc: 87.50%,  total acc: 69.01%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 69.11%   [EVAL] batch:  310 | acc: 93.75%,  total acc: 69.19%   [EVAL] batch:  311 | acc: 100.00%,  total acc: 69.29%   [EVAL] batch:  312 | acc: 75.00%,  total acc: 69.31%   [EVAL] batch:  313 | acc: 12.50%,  total acc: 69.13%   [EVAL] batch:  314 | acc: 18.75%,  total acc: 68.97%   [EVAL] batch:  315 | acc: 18.75%,  total acc: 68.81%   [EVAL] batch:  316 | acc: 6.25%,  total acc: 68.61%   [EVAL] batch:  317 | acc: 18.75%,  total acc: 68.46%   [EVAL] batch:  318 | acc: 25.00%,  total acc: 68.32%   [EVAL] batch:  319 | acc: 87.50%,  total acc: 68.38%   [EVAL] batch:  320 | acc: 93.75%,  total acc: 68.46%   [EVAL] batch:  321 | acc: 87.50%,  total acc: 68.52%   [EVAL] batch:  322 | acc: 100.00%,  total acc: 68.61%   [EVAL] batch:  323 | acc: 93.75%,  total acc: 68.69%   [EVAL] batch:  324 | acc: 81.25%,  total acc: 68.73%   [EVAL] batch:  325 | acc: 62.50%,  total acc: 68.71%   [EVAL] batch:  326 | acc: 31.25%,  total acc: 68.60%   [EVAL] batch:  327 | acc: 56.25%,  total acc: 68.56%   [EVAL] batch:  328 | acc: 62.50%,  total acc: 68.54%   [EVAL] batch:  329 | acc: 62.50%,  total acc: 68.52%   [EVAL] batch:  330 | acc: 31.25%,  total acc: 68.41%   [EVAL] batch:  331 | acc: 93.75%,  total acc: 68.49%   [EVAL] batch:  332 | acc: 100.00%,  total acc: 68.58%   [EVAL] batch:  333 | acc: 100.00%,  total acc: 68.68%   [EVAL] batch:  334 | acc: 93.75%,  total acc: 68.75%   [EVAL] batch:  335 | acc: 93.75%,  total acc: 68.82%   [EVAL] batch:  336 | acc: 100.00%,  total acc: 68.92%   [EVAL] batch:  337 | acc: 87.50%,  total acc: 68.97%   [EVAL] batch:  338 | acc: 75.00%,  total acc: 68.99%   [EVAL] batch:  339 | acc: 81.25%,  total acc: 69.03%   [EVAL] batch:  340 | acc: 93.75%,  total acc: 69.10%   [EVAL] batch:  341 | acc: 75.00%,  total acc: 69.12%   [EVAL] batch:  342 | acc: 100.00%,  total acc: 69.21%   [EVAL] batch:  343 | acc: 81.25%,  total acc: 69.24%   [EVAL] batch:  344 | acc: 100.00%,  total acc: 69.33%   [EVAL] batch:  345 | acc: 100.00%,  total acc: 69.42%   [EVAL] batch:  346 | acc: 87.50%,  total acc: 69.47%   [EVAL] batch:  347 | acc: 87.50%,  total acc: 69.52%   [EVAL] batch:  348 | acc: 93.75%,  total acc: 69.59%   [EVAL] batch:  349 | acc: 100.00%,  total acc: 69.68%   [EVAL] batch:  350 | acc: 75.00%,  total acc: 69.69%   [EVAL] batch:  351 | acc: 68.75%,  total acc: 69.69%   [EVAL] batch:  352 | acc: 62.50%,  total acc: 69.67%   [EVAL] batch:  353 | acc: 43.75%,  total acc: 69.60%   [EVAL] batch:  354 | acc: 37.50%,  total acc: 69.51%   [EVAL] batch:  355 | acc: 62.50%,  total acc: 69.49%   [EVAL] batch:  356 | acc: 93.75%,  total acc: 69.56%   [EVAL] batch:  357 | acc: 75.00%,  total acc: 69.57%   [EVAL] batch:  358 | acc: 81.25%,  total acc: 69.60%   [EVAL] batch:  359 | acc: 87.50%,  total acc: 69.65%   [EVAL] batch:  360 | acc: 75.00%,  total acc: 69.67%   [EVAL] batch:  361 | acc: 62.50%,  total acc: 69.65%   [EVAL] batch:  362 | acc: 62.50%,  total acc: 69.63%   [EVAL] batch:  363 | acc: 68.75%,  total acc: 69.63%   [EVAL] batch:  364 | acc: 62.50%,  total acc: 69.61%   [EVAL] batch:  365 | acc: 68.75%,  total acc: 69.60%   [EVAL] batch:  366 | acc: 75.00%,  total acc: 69.62%   [EVAL] batch:  367 | acc: 56.25%,  total acc: 69.58%   [EVAL] batch:  368 | acc: 25.00%,  total acc: 69.46%   [EVAL] batch:  369 | acc: 75.00%,  total acc: 69.48%   [EVAL] batch:  370 | acc: 43.75%,  total acc: 69.41%   [EVAL] batch:  371 | acc: 50.00%,  total acc: 69.35%   [EVAL] batch:  372 | acc: 75.00%,  total acc: 69.37%   [EVAL] batch:  373 | acc: 81.25%,  total acc: 69.40%   [EVAL] batch:  374 | acc: 68.75%,  total acc: 69.40%   [EVAL] batch:  375 | acc: 31.25%,  total acc: 69.30%   [EVAL] batch:  376 | acc: 50.00%,  total acc: 69.25%   [EVAL] batch:  377 | acc: 25.00%,  total acc: 69.13%   [EVAL] batch:  378 | acc: 31.25%,  total acc: 69.03%   [EVAL] batch:  379 | acc: 43.75%,  total acc: 68.96%   [EVAL] batch:  380 | acc: 25.00%,  total acc: 68.85%   [EVAL] batch:  381 | acc: 75.00%,  total acc: 68.86%   [EVAL] batch:  382 | acc: 93.75%,  total acc: 68.93%   [EVAL] batch:  383 | acc: 93.75%,  total acc: 68.99%   [EVAL] batch:  384 | acc: 87.50%,  total acc: 69.04%   [EVAL] batch:  385 | acc: 93.75%,  total acc: 69.11%   [EVAL] batch:  386 | acc: 87.50%,  total acc: 69.15%   [EVAL] batch:  387 | acc: 87.50%,  total acc: 69.20%   [EVAL] batch:  388 | acc: 75.00%,  total acc: 69.22%   [EVAL] batch:  389 | acc: 93.75%,  total acc: 69.28%   [EVAL] batch:  390 | acc: 81.25%,  total acc: 69.31%   [EVAL] batch:  391 | acc: 93.75%,  total acc: 69.37%   [EVAL] batch:  392 | acc: 87.50%,  total acc: 69.42%   [EVAL] batch:  393 | acc: 75.00%,  total acc: 69.43%   [EVAL] batch:  394 | acc: 12.50%,  total acc: 69.29%   [EVAL] batch:  395 | acc: 18.75%,  total acc: 69.16%   [EVAL] batch:  396 | acc: 25.00%,  total acc: 69.05%   [EVAL] batch:  397 | acc: 43.75%,  total acc: 68.99%   [EVAL] batch:  398 | acc: 31.25%,  total acc: 68.89%   [EVAL] batch:  399 | acc: 31.25%,  total acc: 68.80%   [EVAL] batch:  400 | acc: 6.25%,  total acc: 68.64%   [EVAL] batch:  401 | acc: 12.50%,  total acc: 68.50%   [EVAL] batch:  402 | acc: 6.25%,  total acc: 68.35%   [EVAL] batch:  403 | acc: 6.25%,  total acc: 68.19%   [EVAL] batch:  404 | acc: 12.50%,  total acc: 68.06%   [EVAL] batch:  405 | acc: 0.00%,  total acc: 67.89%   [EVAL] batch:  406 | acc: 43.75%,  total acc: 67.83%   [EVAL] batch:  407 | acc: 81.25%,  total acc: 67.86%   [EVAL] batch:  408 | acc: 75.00%,  total acc: 67.88%   [EVAL] batch:  409 | acc: 81.25%,  total acc: 67.91%   [EVAL] batch:  410 | acc: 81.25%,  total acc: 67.94%   [EVAL] batch:  411 | acc: 68.75%,  total acc: 67.95%   [EVAL] batch:  412 | acc: 81.25%,  total acc: 67.98%   [EVAL] batch:  413 | acc: 93.75%,  total acc: 68.04%   [EVAL] batch:  414 | acc: 100.00%,  total acc: 68.12%   [EVAL] batch:  415 | acc: 87.50%,  total acc: 68.16%   [EVAL] batch:  416 | acc: 87.50%,  total acc: 68.21%   [EVAL] batch:  417 | acc: 100.00%,  total acc: 68.29%   [EVAL] batch:  418 | acc: 87.50%,  total acc: 68.33%   [EVAL] batch:  419 | acc: 25.00%,  total acc: 68.23%   [EVAL] batch:  420 | acc: 12.50%,  total acc: 68.10%   [EVAL] batch:  421 | acc: 6.25%,  total acc: 67.95%   [EVAL] batch:  422 | acc: 18.75%,  total acc: 67.83%   [EVAL] batch:  423 | acc: 25.00%,  total acc: 67.73%   [EVAL] batch:  424 | acc: 18.75%,  total acc: 67.62%   [EVAL] batch:  425 | acc: 68.75%,  total acc: 67.62%   [EVAL] batch:  426 | acc: 93.75%,  total acc: 67.68%   [EVAL] batch:  427 | acc: 43.75%,  total acc: 67.63%   [EVAL] batch:  428 | acc: 75.00%,  total acc: 67.64%   [EVAL] batch:  429 | acc: 81.25%,  total acc: 67.67%   [EVAL] batch:  430 | acc: 75.00%,  total acc: 67.69%   [EVAL] batch:  431 | acc: 62.50%,  total acc: 67.68%   [EVAL] batch:  432 | acc: 50.00%,  total acc: 67.64%   [EVAL] batch:  433 | acc: 43.75%,  total acc: 67.58%   [EVAL] batch:  434 | acc: 75.00%,  total acc: 67.60%   [EVAL] batch:  435 | acc: 43.75%,  total acc: 67.55%   [EVAL] batch:  436 | acc: 75.00%,  total acc: 67.56%   [EVAL] batch:  437 | acc: 25.00%,  total acc: 67.47%   
cur_acc:  ['0.9524', '0.8135', '0.7579', '0.8502', '0.7331', '0.7143', '0.5595']
his_acc:  ['0.9524', '0.8640', '0.8165', '0.8065', '0.7746', '0.7342', '0.6747']
CurrentTrain: epoch 15, batch     0 | loss: 14.9012907CurrentTrain: epoch 15, batch     1 | loss: 16.1777984CurrentTrain: epoch 15, batch     2 | loss: 17.9454827CurrentTrain: epoch  1, batch     3 | loss: 24.8165566CurrentTrain: epoch 15, batch     0 | loss: 13.5556308CurrentTrain: epoch 15, batch     1 | loss: 19.2003107CurrentTrain: epoch 15, batch     2 | loss: 16.0376331CurrentTrain: epoch  1, batch     3 | loss: 17.4567317CurrentTrain: epoch 15, batch     0 | loss: 15.7582795CurrentTrain: epoch 15, batch     1 | loss: 11.1482086CurrentTrain: epoch 15, batch     2 | loss: 13.9598154CurrentTrain: epoch  1, batch     3 | loss: 6.8723911CurrentTrain: epoch 15, batch     0 | loss: 14.1262012CurrentTrain: epoch 15, batch     1 | loss: 11.3818949CurrentTrain: epoch 15, batch     2 | loss: 9.7446114CurrentTrain: epoch  1, batch     3 | loss: 7.3554702CurrentTrain: epoch 15, batch     0 | loss: 16.9161680CurrentTrain: epoch 15, batch     1 | loss: 13.1332855CurrentTrain: epoch 15, batch     2 | loss: 16.0077839CurrentTrain: epoch  1, batch     3 | loss: 7.0315534CurrentTrain: epoch 15, batch     0 | loss: 10.7870134CurrentTrain: epoch 15, batch     1 | loss: 11.6731344CurrentTrain: epoch 15, batch     2 | loss: 9.6837011CurrentTrain: epoch  1, batch     3 | loss: 6.9446362CurrentTrain: epoch 15, batch     0 | loss: 12.9012750CurrentTrain: epoch 15, batch     1 | loss: 11.3033981CurrentTrain: epoch 15, batch     2 | loss: 11.7745995CurrentTrain: epoch  1, batch     3 | loss: 7.9211353CurrentTrain: epoch 15, batch     0 | loss: 12.7419454CurrentTrain: epoch 15, batch     1 | loss: 12.8532864CurrentTrain: epoch 15, batch     2 | loss: 10.9072497CurrentTrain: epoch  1, batch     3 | loss: 7.9421706CurrentTrain: epoch 15, batch     0 | loss: 15.0451132CurrentTrain: epoch 15, batch     1 | loss: 9.3473582CurrentTrain: epoch 15, batch     2 | loss: 16.6297557CurrentTrain: epoch  1, batch     3 | loss: 5.4903077CurrentTrain: epoch 15, batch     0 | loss: 12.2212476CurrentTrain: epoch 15, batch     1 | loss: 14.2091231CurrentTrain: epoch 15, batch     2 | loss: 11.2020053CurrentTrain: epoch  1, batch     3 | loss: 15.7310454
#### applies to jurisdiction ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "applies to jurisdiction" means the item (institution, law, public office, public register...) or statement belongs to or has power over or applies to the value (a territorial jurisdiction: a country, state, municipality, ...).
Here is an example:
Relation: applies to jurisdiction
Context: george lawson sheldon ( may 31 , 1870 – april 4 , 1960 ) was 14th governor of nebraska with service from 1907 until 1909 .
Head Entity: governor
Tail Entity: nebraska
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "applies to jurisdiction":
Relation: applies to jurisdiction  
Context: The Environmental Protection Agency (EPA) is responsible for enforcing regulations related to environmental protection in the United States.  
Head Entity: Environmental Protection Agency  
Tail Entity: United States  

Relation: applies to jurisdiction  
Context: The California Consumer Privacy Act (CCPA) provides residents of California with specific rights regarding their personal information.  
Head Entity: California Consumer Privacy Act  
Tail Entity: California  
#### performer ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "performer" means actor, musician, band or other performer associated with this role or musical work.
Here is an example:
Relation: performer
Context: it is prominently featured on jean michel jarre 's albums " oxygène " ( 1977 ) and " équinoxe " ( 1978 ) .
Head Entity: équinoxe
Tail Entity: jean michel jarre
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "performer":
Relation: performer  
Context: The song "Shape of You" is one of the biggest hits by Ed Sheeran, showcasing his unique style and musical talent.  
Head Entity: Shape of You  
Tail Entity: Ed Sheeran  

Relation: performer  
Context: The legendary band The Beatles revolutionized music in the 1960s with their innovative sound and iconic performances.  
Head Entity: The Beatles  
Tail Entity: music
#### manufacturer ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "manufacturer" means manufacturer or producer of this product.
Here is an example:
Relation: manufacturer
Context: the glasflügel h-201 standard " libelle " ( german : " dragonfly " ) is an early composite standard class single - seat sailplane produced by glasflügel from 1967 .
Head Entity: dragonfly
Tail Entity: glasflügel
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "manufacturer":
Relation: manufacturer  
Context: The iPhone 13, a smartphone developed by Apple, was released in September 2021 and features advanced camera technology and improved battery life.  
Head Entity: iPhone 13  
Tail Entity: Apple  

Relation: manufacturer  
Context: The Tesla Model S is an all-electric luxury sedan that was first introduced by Tesla, Inc. in 2012, showcasing innovative technology and performance.  
Head Entity: Tesla Model S  
Tail Entity: Tesla, Inc.  
#### participant ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "participant" means person, group of people or organization (object) that actively takes/took part in an event or process.
Here is an example:
Relation: participant
Context: notable prisoners included ferdinand , count of flanders , which was defeated and captured at the battle of bouvines in 1214 and that spent thirteen years imprisoned in this dungeon .
Head Entity: battle of bouvines
Tail Entity: ferdinand , count of flanders
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "participant":
Relation: participant  
Context: The annual tech conference attracted thousands of attendees, including industry leaders and innovators who shared their insights on the future of technology.  
Head Entity: annual tech conference  
Tail Entity: industry leaders and innovators  

Relation: participant  
Context: During the charity marathon, local schools and community groups came together to raise funds for underprivileged children, showcasing their commitment to social causes.  
Head Entity: charity marathon  
Tail Entity: local schools and community groups  
#### operating system ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "operating system" means operating system (OS) on which a software works or the OS installed on hardware.
Here is an example:
Relation: operating system
Context: " robowar " is a similar game that was released later on the macintosh , and is now available for both mac os and windows .
Head Entity: robowar
Tail Entity: windows
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "operating system":
Relation: operating system  
Context: "The latest version of Adobe Photoshop is compatible with both Windows 10 and macOS Monterey."  
Head Entity: Adobe Photoshop  
Tail Entity: Windows 10  

Relation: operating system  
Context: "Minecraft runs smoothly on various platforms, including Linux and the latest version of Windows."  
Head Entity: Minecraft  
Tail Entity: Linux  
#### competition class ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "competition class" means official classification by a regulating body under which the subject (events, teams, participants, or equipment) qualifies for inclusion.
Here is an example:
Relation: competition class
Context: in his next bout , he won the wbc light welterweight title by defeating roger mayweather for a second time .
Head Entity: roger mayweather
Tail Entity: light welterweight
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "competition class":
Relation: competition class  
Context: The team secured their place in the tournament by qualifying under the UEFA Champions League regulations.  
Head Entity: UEFA Champions League  
Tail Entity: tournament  

Relation: competition class  
Context: The athlete competed in the 100 meters event, which is classified under the Olympic Games standards.  
Head Entity: Olympic Games  
Tail Entity: 100 meters event  
#### successful candidate ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "successful candidate" means person elected after the election.
Here is an example:
Relation: successful candidate
Context: the qup gained 3 seats from its previous election result , and won a 10 % swing to it in primary votes , but failed to defeat the incumbent labor government led by ted theodore .
Head Entity: previous election
Tail Entity: ted theodore
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "successful candidate":
Relation: successful candidate  
Context: After a closely contested campaign, the voters chose Sarah Johnson as their representative, marking a significant shift in the political landscape of the district.  
Head Entity: closely contested campaign  
Tail Entity: Sarah Johnson  

Relation: successful candidate  
Context: In the recent mayoral election, the community rallied behind Mark Thompson, who promised to bring new initiatives to the city, ultimately leading to his victory.  
Head Entity: recent mayoral election  
Tail Entity: Mark Thompson  
#### sibling ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "sibling" means the subject and the object have the same parents.
Here is an example:
Relation: sibling
Context: jenő szervánszky is the father of the pianist , valéria szervánszky and the brother of the composer , endre szervánszky and violinist , péter szervánszky .
Head Entity: péter szervánszky
Tail Entity: endre szervánszky
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "sibling":
Relation: sibling  
Context: emma and oliver are both children of the same parents, making them siblings. they often play together and share a close bond.  
Head Entity: emma  
Tail Entity: oliver  

Relation: sibling  
Context: during the family reunion, it was clear that both lucas and mia inherited their parents' artistic talents, as they are siblings who excel in painting and music.  
Head Entity: lucas  
Tail Entity: mia  
#### military branch ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "military branch" means branch to which this military unit, award, office, or person belongs.
Here is an example:
Relation: military branch
Context: two of her sons , tunis and thomas tingey rose to prominence in the union navy during the civil war .
Head Entity: tunis
Tail Entity: union navy
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "military branch":
Relation: military branch  
Context: after serving in the army, he transitioned to a leadership role in the air force, where he implemented several key strategies.  
Head Entity: air force  
Tail Entity: army  

Relation: military branch  
Context: the general was honored for his service in the marine corps, where he led multiple successful missions.  
Head Entity: marine corps  
Tail Entity: general  
#### child ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "child" means subject has object as child.
Here is an example:
Relation: child
Context: ziad rahbani is the son of the lebanese famous composer assi rahbani and nouhad haddad , the famous lebanese female singer known as fairuz .
Head Entity: assi rahbani
Tail Entity: ziad rahbani
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "child":
Relation: child  
Context: in the animated series, the character of simon is the son of the adventurous couple, marie and tom.  
Head Entity: marie  
Tail Entity: simon  

Relation: child  
Context: the renowned physicist albert einstein had a daughter named lieserl, who was born in 1902.  
Head Entity: albert einstein  
Tail Entity: lieserl  
MemoryTrain:  epoch 15, batch     0 | loss: 3.1361776MemoryTrain:  epoch 15, batch     1 | loss: 6.3862388MemoryTrain:  epoch 15, batch     2 | loss: 3.6015649MemoryTrain:  epoch 15, batch     3 | loss: 1.8816200MemoryTrain:  epoch 15, batch     4 | loss: 2.8632838MemoryTrain:  epoch 15, batch     5 | loss: 2.7003831MemoryTrain:  epoch 15, batch     6 | loss: 3.1717315MemoryTrain:  epoch 15, batch     7 | loss: 3.0040880MemoryTrain:  epoch 15, batch     8 | loss: 5.8815635MemoryTrain:  epoch 15, batch     9 | loss: 3.6853333MemoryTrain:  epoch 15, batch    10 | loss: 4.3871725MemoryTrain:  epoch 15, batch    11 | loss: 4.2346097MemoryTrain:  epoch 15, batch    12 | loss: 3.4316717MemoryTrain:  epoch 15, batch    13 | loss: 2.7607242MemoryTrain:  epoch 15, batch    14 | loss: 3.2757832MemoryTrain:  epoch 15, batch     0 | loss: 3.1958619MemoryTrain:  epoch 15, batch     1 | loss: 5.8252865MemoryTrain:  epoch 15, batch     2 | loss: 2.7030251MemoryTrain:  epoch 15, batch     3 | loss: 2.3795308MemoryTrain:  epoch 15, batch     4 | loss: 2.8965230MemoryTrain:  epoch 15, batch     5 | loss: 3.8422415MemoryTrain:  epoch 15, batch     6 | loss: 2.4378071MemoryTrain:  epoch 15, batch     7 | loss: 2.1213443MemoryTrain:  epoch 15, batch     8 | loss: 2.4676666MemoryTrain:  epoch 15, batch     9 | loss: 1.3177811MemoryTrain:  epoch 15, batch    10 | loss: 7.0227600MemoryTrain:  epoch 15, batch    11 | loss: 3.8115521MemoryTrain:  epoch 15, batch    12 | loss: 2.3398048MemoryTrain:  epoch 15, batch    13 | loss: 3.3577308MemoryTrain:  epoch 15, batch    14 | loss: 3.0955945MemoryTrain:  epoch 15, batch     0 | loss: 1.7256818MemoryTrain:  epoch 15, batch     1 | loss: 2.8929411MemoryTrain:  epoch 15, batch     2 | loss: 3.1029588MemoryTrain:  epoch 15, batch     3 | loss: 2.0013187MemoryTrain:  epoch 15, batch     4 | loss: 2.3317341MemoryTrain:  epoch 15, batch     5 | loss: 3.3062064MemoryTrain:  epoch 15, batch     6 | loss: 2.5281116MemoryTrain:  epoch 15, batch     7 | loss: 4.3830629MemoryTrain:  epoch 15, batch     8 | loss: 5.6905419MemoryTrain:  epoch 15, batch     9 | loss: 3.0101489MemoryTrain:  epoch 15, batch    10 | loss: 2.6791021MemoryTrain:  epoch 15, batch    11 | loss: 4.2982944MemoryTrain:  epoch 15, batch    12 | loss: 2.7517193MemoryTrain:  epoch 15, batch    13 | loss: 2.4810275MemoryTrain:  epoch 15, batch    14 | loss: 2.3128977MemoryTrain:  epoch 15, batch     0 | loss: 3.2366634MemoryTrain:  epoch 15, batch     1 | loss: 4.0906220MemoryTrain:  epoch 15, batch     2 | loss: 2.7863108MemoryTrain:  epoch 15, batch     3 | loss: 2.2003007MemoryTrain:  epoch 15, batch     4 | loss: 2.5448960MemoryTrain:  epoch 15, batch     5 | loss: 1.9341369MemoryTrain:  epoch 15, batch     6 | loss: 2.1704950MemoryTrain:  epoch 15, batch     7 | loss: 2.9968408MemoryTrain:  epoch 15, batch     8 | loss: 2.2886421MemoryTrain:  epoch 15, batch     9 | loss: 2.2334078MemoryTrain:  epoch 15, batch    10 | loss: 1.7843658MemoryTrain:  epoch 15, batch    11 | loss: 2.0481205MemoryTrain:  epoch 15, batch    12 | loss: 2.3072952MemoryTrain:  epoch 15, batch    13 | loss: 2.2983920MemoryTrain:  epoch 15, batch    14 | loss: 2.5374908MemoryTrain:  epoch 15, batch     0 | loss: 3.0608609MemoryTrain:  epoch 15, batch     1 | loss: 2.5164228MemoryTrain:  epoch 15, batch     2 | loss: 1.3202222MemoryTrain:  epoch 15, batch     3 | loss: 1.6679250MemoryTrain:  epoch 15, batch     4 | loss: 4.1814875MemoryTrain:  epoch 15, batch     5 | loss: 2.5405138MemoryTrain:  epoch 15, batch     6 | loss: 2.1608546MemoryTrain:  epoch 15, batch     7 | loss: 2.4241965MemoryTrain:  epoch 15, batch     8 | loss: 4.2003505MemoryTrain:  epoch 15, batch     9 | loss: 1.6670019MemoryTrain:  epoch 15, batch    10 | loss: 2.9657417MemoryTrain:  epoch 15, batch    11 | loss: 4.8953242MemoryTrain:  epoch 15, batch    12 | loss: 2.8129735MemoryTrain:  epoch 15, batch    13 | loss: 1.9247017MemoryTrain:  epoch 15, batch    14 | loss: 3.9611815MemoryTrain:  epoch 15, batch     0 | loss: 4.2684098MemoryTrain:  epoch 15, batch     1 | loss: 2.1780804MemoryTrain:  epoch 15, batch     2 | loss: 1.7094003MemoryTrain:  epoch 15, batch     3 | loss: 1.8477506MemoryTrain:  epoch 15, batch     4 | loss: 1.6270515MemoryTrain:  epoch 15, batch     5 | loss: 4.4629461MemoryTrain:  epoch 15, batch     6 | loss: 1.9484939MemoryTrain:  epoch 15, batch     7 | loss: 1.6691106MemoryTrain:  epoch 15, batch     8 | loss: 2.0115498MemoryTrain:  epoch 15, batch     9 | loss: 3.0067492MemoryTrain:  epoch 15, batch    10 | loss: 2.0396270MemoryTrain:  epoch 15, batch    11 | loss: 2.9326308MemoryTrain:  epoch 15, batch    12 | loss: 1.8739768MemoryTrain:  epoch 15, batch    13 | loss: 1.5640251MemoryTrain:  epoch 15, batch    14 | loss: 3.8529935MemoryTrain:  epoch 15, batch     0 | loss: 1.9635986MemoryTrain:  epoch 15, batch     1 | loss: 1.6156483MemoryTrain:  epoch 15, batch     2 | loss: 1.5988304MemoryTrain:  epoch 15, batch     3 | loss: 1.9182385MemoryTrain:  epoch 15, batch     4 | loss: 1.7107080MemoryTrain:  epoch 15, batch     5 | loss: 1.3854397MemoryTrain:  epoch 15, batch     6 | loss: 1.9081236MemoryTrain:  epoch 15, batch     7 | loss: 4.5743721MemoryTrain:  epoch 15, batch     8 | loss: 1.5154809MemoryTrain:  epoch 15, batch     9 | loss: 4.2977255MemoryTrain:  epoch 15, batch    10 | loss: 1.6431025MemoryTrain:  epoch 15, batch    11 | loss: 1.8524530MemoryTrain:  epoch 15, batch    12 | loss: 1.8317476MemoryTrain:  epoch 15, batch    13 | loss: 2.7117350MemoryTrain:  epoch 15, batch    14 | loss: 1.3733656MemoryTrain:  epoch 15, batch     0 | loss: 1.7156498MemoryTrain:  epoch 15, batch     1 | loss: 1.8862554MemoryTrain:  epoch 15, batch     2 | loss: 1.7849788MemoryTrain:  epoch 15, batch     3 | loss: 1.3520783MemoryTrain:  epoch 15, batch     4 | loss: 1.8561795MemoryTrain:  epoch 15, batch     5 | loss: 2.7757133MemoryTrain:  epoch 15, batch     6 | loss: 1.9714955MemoryTrain:  epoch 15, batch     7 | loss: 3.9946166MemoryTrain:  epoch 15, batch     8 | loss: 1.3015002MemoryTrain:  epoch 15, batch     9 | loss: 1.5038792MemoryTrain:  epoch 15, batch    10 | loss: 1.4511976MemoryTrain:  epoch 15, batch    11 | loss: 3.3051493MemoryTrain:  epoch 15, batch    12 | loss: 1.8607981MemoryTrain:  epoch 15, batch    13 | loss: 1.4482320MemoryTrain:  epoch 15, batch    14 | loss: 1.3267442MemoryTrain:  epoch 15, batch     0 | loss: 1.3283412MemoryTrain:  epoch 15, batch     1 | loss: 1.6254420MemoryTrain:  epoch 15, batch     2 | loss: 1.7142261MemoryTrain:  epoch 15, batch     3 | loss: 3.0437566MemoryTrain:  epoch 15, batch     4 | loss: 1.3901208MemoryTrain:  epoch 15, batch     5 | loss: 1.4495042MemoryTrain:  epoch 15, batch     6 | loss: 1.7824219MemoryTrain:  epoch 15, batch     7 | loss: 3.9624636MemoryTrain:  epoch 15, batch     8 | loss: 1.5493945MemoryTrain:  epoch 15, batch     9 | loss: 1.6568196MemoryTrain:  epoch 15, batch    10 | loss: 1.8488992MemoryTrain:  epoch 15, batch    11 | loss: 1.4754849MemoryTrain:  epoch 15, batch    12 | loss: 4.6251602MemoryTrain:  epoch 15, batch    13 | loss: 4.6483399MemoryTrain:  epoch 15, batch    14 | loss: 2.4933464MemoryTrain:  epoch 15, batch     0 | loss: 2.3047046MemoryTrain:  epoch 15, batch     1 | loss: 1.4701106MemoryTrain:  epoch 15, batch     2 | loss: 1.7761077MemoryTrain:  epoch 15, batch     3 | loss: 2.5035895MemoryTrain:  epoch 15, batch     4 | loss: 2.0429020MemoryTrain:  epoch 15, batch     5 | loss: 1.6196448MemoryTrain:  epoch 15, batch     6 | loss: 1.4813439MemoryTrain:  epoch 15, batch     7 | loss: 4.1341895MemoryTrain:  epoch 15, batch     8 | loss: 1.3278632MemoryTrain:  epoch 15, batch     9 | loss: 1.6168479MemoryTrain:  epoch 15, batch    10 | loss: 1.3324131MemoryTrain:  epoch 15, batch    11 | loss: 1.4971373MemoryTrain:  epoch 15, batch    12 | loss: 2.4007870MemoryTrain:  epoch 15, batch    13 | loss: 10.6973299MemoryTrain:  epoch 15, batch    14 | loss: 1.7609381
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 68.75%,  total acc: 71.88%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 77.08%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 75.00%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 76.25%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 78.12%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 77.68%   [EVAL] batch:    7 | acc: 62.50%,  total acc: 75.78%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 75.00%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 74.38%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 75.00%   [EVAL] batch:   11 | acc: 68.75%,  total acc: 74.48%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 75.00%   [EVAL] batch:   13 | acc: 62.50%,  total acc: 74.11%   [EVAL] batch:   14 | acc: 81.25%,  total acc: 74.58%   [EVAL] batch:   15 | acc: 68.75%,  total acc: 74.22%   [EVAL] batch:   16 | acc: 81.25%,  total acc: 74.63%   [EVAL] batch:   17 | acc: 81.25%,  total acc: 75.00%   [EVAL] batch:   18 | acc: 81.25%,  total acc: 75.33%   [EVAL] batch:   19 | acc: 50.00%,  total acc: 74.06%   [EVAL] batch:   20 | acc: 75.00%,  total acc: 74.11%   [EVAL] batch:   21 | acc: 37.50%,  total acc: 72.44%   [EVAL] batch:   22 | acc: 62.50%,  total acc: 72.01%   [EVAL] batch:   23 | acc: 43.75%,  total acc: 70.83%   [EVAL] batch:   24 | acc: 43.75%,  total acc: 69.75%   [EVAL] batch:   25 | acc: 93.75%,  total acc: 70.67%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 71.76%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 72.54%   [EVAL] batch:   28 | acc: 87.50%,  total acc: 73.06%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 73.96%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 74.80%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 75.39%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 76.14%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 76.84%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 77.50%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 78.12%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 78.72%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 79.28%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 79.81%   [EVAL] batch:   39 | acc: 93.75%,  total acc: 80.16%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 80.49%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 80.95%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 81.39%   [EVAL] batch:   44 | acc: 43.75%,  total acc: 80.56%   [EVAL] batch:   45 | acc: 31.25%,  total acc: 79.48%   [EVAL] batch:   46 | acc: 50.00%,  total acc: 78.86%   [EVAL] batch:   47 | acc: 56.25%,  total acc: 78.39%   [EVAL] batch:   48 | acc: 37.50%,  total acc: 77.55%   [EVAL] batch:   49 | acc: 50.00%,  total acc: 77.00%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 77.33%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 77.64%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 77.95%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 78.01%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 78.30%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 78.46%   [EVAL] batch:   56 | acc: 50.00%,  total acc: 77.96%   [EVAL] batch:   57 | acc: 25.00%,  total acc: 77.05%   [EVAL] batch:   58 | acc: 18.75%,  total acc: 76.06%   [EVAL] batch:   59 | acc: 31.25%,  total acc: 75.31%   [EVAL] batch:   60 | acc: 31.25%,  total acc: 74.59%   [EVAL] batch:   61 | acc: 6.25%,  total acc: 73.49%   [EVAL] batch:   62 | acc: 18.75%,  total acc: 72.62%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 79.17%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 78.12%   [EVAL] batch:    4 | acc: 62.50%,  total acc: 75.00%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 76.04%   [EVAL] batch:    6 | acc: 37.50%,  total acc: 70.54%   [EVAL] batch:    7 | acc: 31.25%,  total acc: 65.62%   [EVAL] batch:    8 | acc: 37.50%,  total acc: 62.50%   [EVAL] batch:    9 | acc: 43.75%,  total acc: 60.62%   [EVAL] batch:   10 | acc: 31.25%,  total acc: 57.95%   [EVAL] batch:   11 | acc: 31.25%,  total acc: 55.73%   [EVAL] batch:   12 | acc: 50.00%,  total acc: 55.29%   [EVAL] batch:   13 | acc: 81.25%,  total acc: 57.14%   [EVAL] batch:   14 | acc: 87.50%,  total acc: 59.17%   [EVAL] batch:   15 | acc: 87.50%,  total acc: 60.94%   [EVAL] batch:   16 | acc: 87.50%,  total acc: 62.50%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 64.24%   [EVAL] batch:   18 | acc: 87.50%,  total acc: 65.46%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 65.94%   [EVAL] batch:   20 | acc: 81.25%,  total acc: 66.67%   [EVAL] batch:   21 | acc: 56.25%,  total acc: 66.19%   [EVAL] batch:   22 | acc: 50.00%,  total acc: 65.49%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 66.67%   [EVAL] batch:   24 | acc: 68.75%,  total acc: 66.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 68.03%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 69.21%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 70.09%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 70.91%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 71.88%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 72.58%   [EVAL] batch:   31 | acc: 87.50%,  total acc: 73.05%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 73.67%   [EVAL] batch:   33 | acc: 50.00%,  total acc: 72.98%   [EVAL] batch:   34 | acc: 62.50%,  total acc: 72.68%   [EVAL] batch:   35 | acc: 75.00%,  total acc: 72.74%   [EVAL] batch:   36 | acc: 75.00%,  total acc: 72.80%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 73.19%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 73.88%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 74.53%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 75.15%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 75.60%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 76.02%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 76.28%   [EVAL] batch:   44 | acc: 68.75%,  total acc: 76.11%   [EVAL] batch:   45 | acc: 75.00%,  total acc: 76.09%   [EVAL] batch:   46 | acc: 62.50%,  total acc: 75.80%   [EVAL] batch:   47 | acc: 56.25%,  total acc: 75.39%   [EVAL] batch:   48 | acc: 81.25%,  total acc: 75.51%   [EVAL] batch:   49 | acc: 75.00%,  total acc: 75.50%   [EVAL] batch:   50 | acc: 81.25%,  total acc: 75.61%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 75.84%   [EVAL] batch:   52 | acc: 75.00%,  total acc: 75.83%   [EVAL] batch:   53 | acc: 68.75%,  total acc: 75.69%   [EVAL] batch:   54 | acc: 50.00%,  total acc: 75.23%   [EVAL] batch:   55 | acc: 75.00%,  total acc: 75.22%   [EVAL] batch:   56 | acc: 75.00%,  total acc: 75.22%   [EVAL] batch:   57 | acc: 25.00%,  total acc: 74.35%   [EVAL] batch:   58 | acc: 43.75%,  total acc: 73.83%   [EVAL] batch:   59 | acc: 68.75%,  total acc: 73.75%   [EVAL] batch:   60 | acc: 68.75%,  total acc: 73.67%   [EVAL] batch:   61 | acc: 50.00%,  total acc: 73.29%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 72.92%   [EVAL] batch:   63 | acc: 62.50%,  total acc: 72.75%   [EVAL] batch:   64 | acc: 56.25%,  total acc: 72.50%   [EVAL] batch:   65 | acc: 25.00%,  total acc: 71.78%   [EVAL] batch:   66 | acc: 37.50%,  total acc: 71.27%   [EVAL] batch:   67 | acc: 56.25%,  total acc: 71.05%   [EVAL] batch:   68 | acc: 75.00%,  total acc: 71.11%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 71.52%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 71.92%   [EVAL] batch:   71 | acc: 87.50%,  total acc: 72.14%   [EVAL] batch:   72 | acc: 93.75%,  total acc: 72.43%   [EVAL] batch:   73 | acc: 100.00%,  total acc: 72.80%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 73.00%   [EVAL] batch:   75 | acc: 81.25%,  total acc: 73.11%   [EVAL] batch:   76 | acc: 50.00%,  total acc: 72.81%   [EVAL] batch:   77 | acc: 75.00%,  total acc: 72.84%   [EVAL] batch:   78 | acc: 68.75%,  total acc: 72.78%   [EVAL] batch:   79 | acc: 75.00%,  total acc: 72.81%   [EVAL] batch:   80 | acc: 75.00%,  total acc: 72.84%   [EVAL] batch:   81 | acc: 50.00%,  total acc: 72.56%   [EVAL] batch:   82 | acc: 31.25%,  total acc: 72.06%   [EVAL] batch:   83 | acc: 25.00%,  total acc: 71.50%   [EVAL] batch:   84 | acc: 18.75%,  total acc: 70.88%   [EVAL] batch:   85 | acc: 31.25%,  total acc: 70.42%   [EVAL] batch:   86 | acc: 25.00%,  total acc: 69.90%   [EVAL] batch:   87 | acc: 31.25%,  total acc: 69.46%   [EVAL] batch:   88 | acc: 50.00%,  total acc: 69.24%   [EVAL] batch:   89 | acc: 50.00%,  total acc: 69.03%   [EVAL] batch:   90 | acc: 50.00%,  total acc: 68.82%   [EVAL] batch:   91 | acc: 37.50%,  total acc: 68.48%   [EVAL] batch:   92 | acc: 50.00%,  total acc: 68.28%   [EVAL] batch:   93 | acc: 31.25%,  total acc: 67.89%   [EVAL] batch:   94 | acc: 68.75%,  total acc: 67.89%   [EVAL] batch:   95 | acc: 31.25%,  total acc: 67.51%   [EVAL] batch:   96 | acc: 56.25%,  total acc: 67.40%   [EVAL] batch:   97 | acc: 62.50%,  total acc: 67.35%   [EVAL] batch:   98 | acc: 62.50%,  total acc: 67.30%   [EVAL] batch:   99 | acc: 37.50%,  total acc: 67.00%   [EVAL] batch:  100 | acc: 56.25%,  total acc: 66.89%   [EVAL] batch:  101 | acc: 75.00%,  total acc: 66.97%   [EVAL] batch:  102 | acc: 62.50%,  total acc: 66.93%   [EVAL] batch:  103 | acc: 81.25%,  total acc: 67.07%   [EVAL] batch:  104 | acc: 68.75%,  total acc: 67.08%   [EVAL] batch:  105 | acc: 62.50%,  total acc: 67.04%   [EVAL] batch:  106 | acc: 68.75%,  total acc: 67.06%   [EVAL] batch:  107 | acc: 37.50%,  total acc: 66.78%   [EVAL] batch:  108 | acc: 31.25%,  total acc: 66.46%   [EVAL] batch:  109 | acc: 56.25%,  total acc: 66.36%   [EVAL] batch:  110 | acc: 43.75%,  total acc: 66.16%   [EVAL] batch:  111 | acc: 50.00%,  total acc: 66.02%   [EVAL] batch:  112 | acc: 81.25%,  total acc: 66.15%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 66.45%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 66.74%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 67.03%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 67.31%   [EVAL] batch:  117 | acc: 100.00%,  total acc: 67.58%   [EVAL] batch:  118 | acc: 87.50%,  total acc: 67.75%   [EVAL] batch:  119 | acc: 56.25%,  total acc: 67.66%   [EVAL] batch:  120 | acc: 43.75%,  total acc: 67.46%   [EVAL] batch:  121 | acc: 50.00%,  total acc: 67.32%   [EVAL] batch:  122 | acc: 68.75%,  total acc: 67.33%   [EVAL] batch:  123 | acc: 56.25%,  total acc: 67.24%   [EVAL] batch:  124 | acc: 56.25%,  total acc: 67.15%   [EVAL] batch:  125 | acc: 43.75%,  total acc: 66.96%   [EVAL] batch:  126 | acc: 31.25%,  total acc: 66.68%   [EVAL] batch:  127 | acc: 50.00%,  total acc: 66.55%   [EVAL] batch:  128 | acc: 62.50%,  total acc: 66.52%   [EVAL] batch:  129 | acc: 75.00%,  total acc: 66.59%   [EVAL] batch:  130 | acc: 37.50%,  total acc: 66.36%   [EVAL] batch:  131 | acc: 75.00%,  total acc: 66.43%   [EVAL] batch:  132 | acc: 81.25%,  total acc: 66.54%   [EVAL] batch:  133 | acc: 75.00%,  total acc: 66.60%   [EVAL] batch:  134 | acc: 87.50%,  total acc: 66.76%   [EVAL] batch:  135 | acc: 87.50%,  total acc: 66.91%   [EVAL] batch:  136 | acc: 93.75%,  total acc: 67.11%   [EVAL] batch:  137 | acc: 56.25%,  total acc: 67.03%   [EVAL] batch:  138 | acc: 0.00%,  total acc: 66.55%   [EVAL] batch:  139 | acc: 12.50%,  total acc: 66.16%   [EVAL] batch:  140 | acc: 6.25%,  total acc: 65.74%   [EVAL] batch:  141 | acc: 6.25%,  total acc: 65.32%   [EVAL] batch:  142 | acc: 6.25%,  total acc: 64.90%   [EVAL] batch:  143 | acc: 31.25%,  total acc: 64.67%   [EVAL] batch:  144 | acc: 93.75%,  total acc: 64.87%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 65.11%   [EVAL] batch:  146 | acc: 100.00%,  total acc: 65.35%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 65.58%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 65.81%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 66.00%   [EVAL] batch:  150 | acc: 12.50%,  total acc: 65.65%   [EVAL] batch:  151 | acc: 6.25%,  total acc: 65.25%   [EVAL] batch:  152 | acc: 6.25%,  total acc: 64.87%   [EVAL] batch:  153 | acc: 25.00%,  total acc: 64.61%   [EVAL] batch:  154 | acc: 12.50%,  total acc: 64.27%   [EVAL] batch:  155 | acc: 0.00%,  total acc: 63.86%   [EVAL] batch:  156 | acc: 68.75%,  total acc: 63.89%   [EVAL] batch:  157 | acc: 87.50%,  total acc: 64.04%   [EVAL] batch:  158 | acc: 100.00%,  total acc: 64.27%   [EVAL] batch:  159 | acc: 87.50%,  total acc: 64.41%   [EVAL] batch:  160 | acc: 87.50%,  total acc: 64.56%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 64.74%   [EVAL] batch:  162 | acc: 75.00%,  total acc: 64.80%   [EVAL] batch:  163 | acc: 37.50%,  total acc: 64.63%   [EVAL] batch:  164 | acc: 56.25%,  total acc: 64.58%   [EVAL] batch:  165 | acc: 43.75%,  total acc: 64.46%   [EVAL] batch:  166 | acc: 62.50%,  total acc: 64.45%   [EVAL] batch:  167 | acc: 43.75%,  total acc: 64.32%   [EVAL] batch:  168 | acc: 50.00%,  total acc: 64.24%   [EVAL] batch:  169 | acc: 50.00%,  total acc: 64.15%   [EVAL] batch:  170 | acc: 75.00%,  total acc: 64.22%   [EVAL] batch:  171 | acc: 75.00%,  total acc: 64.28%   [EVAL] batch:  172 | acc: 56.25%,  total acc: 64.23%   [EVAL] batch:  173 | acc: 62.50%,  total acc: 64.22%   [EVAL] batch:  174 | acc: 81.25%,  total acc: 64.32%   [EVAL] batch:  175 | acc: 56.25%,  total acc: 64.28%   [EVAL] batch:  176 | acc: 50.00%,  total acc: 64.19%   [EVAL] batch:  177 | acc: 50.00%,  total acc: 64.12%   [EVAL] batch:  178 | acc: 62.50%,  total acc: 64.11%   [EVAL] batch:  179 | acc: 75.00%,  total acc: 64.17%   [EVAL] batch:  180 | acc: 43.75%,  total acc: 64.05%   [EVAL] batch:  181 | acc: 75.00%,  total acc: 64.11%   [EVAL] batch:  182 | acc: 87.50%,  total acc: 64.24%   [EVAL] batch:  183 | acc: 93.75%,  total acc: 64.40%   [EVAL] batch:  184 | acc: 75.00%,  total acc: 64.46%   [EVAL] batch:  185 | acc: 93.75%,  total acc: 64.62%   [EVAL] batch:  186 | acc: 75.00%,  total acc: 64.67%   [EVAL] batch:  187 | acc: 93.75%,  total acc: 64.83%   [EVAL] batch:  188 | acc: 93.75%,  total acc: 64.98%   [EVAL] batch:  189 | acc: 87.50%,  total acc: 65.10%   [EVAL] batch:  190 | acc: 100.00%,  total acc: 65.28%   [EVAL] batch:  191 | acc: 81.25%,  total acc: 65.36%   [EVAL] batch:  192 | acc: 100.00%,  total acc: 65.54%   [EVAL] batch:  193 | acc: 81.25%,  total acc: 65.62%   [EVAL] batch:  194 | acc: 50.00%,  total acc: 65.54%   [EVAL] batch:  195 | acc: 43.75%,  total acc: 65.43%   [EVAL] batch:  196 | acc: 50.00%,  total acc: 65.36%   [EVAL] batch:  197 | acc: 43.75%,  total acc: 65.25%   [EVAL] batch:  198 | acc: 62.50%,  total acc: 65.23%   [EVAL] batch:  199 | acc: 56.25%,  total acc: 65.19%   [EVAL] batch:  200 | acc: 56.25%,  total acc: 65.14%   [EVAL] batch:  201 | acc: 75.00%,  total acc: 65.19%   [EVAL] batch:  202 | acc: 50.00%,  total acc: 65.12%   [EVAL] batch:  203 | acc: 43.75%,  total acc: 65.01%   [EVAL] batch:  204 | acc: 56.25%,  total acc: 64.97%   [EVAL] batch:  205 | acc: 56.25%,  total acc: 64.93%   [EVAL] batch:  206 | acc: 25.00%,  total acc: 64.73%   [EVAL] batch:  207 | acc: 25.00%,  total acc: 64.54%   [EVAL] batch:  208 | acc: 43.75%,  total acc: 64.44%   [EVAL] batch:  209 | acc: 25.00%,  total acc: 64.26%   [EVAL] batch:  210 | acc: 31.25%,  total acc: 64.10%   [EVAL] batch:  211 | acc: 18.75%,  total acc: 63.89%   [EVAL] batch:  212 | acc: 75.00%,  total acc: 63.94%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 64.11%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 64.27%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 64.44%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 64.60%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 64.76%   [EVAL] batch:  218 | acc: 93.75%,  total acc: 64.90%   [EVAL] batch:  219 | acc: 56.25%,  total acc: 64.86%   [EVAL] batch:  220 | acc: 81.25%,  total acc: 64.93%   [EVAL] batch:  221 | acc: 68.75%,  total acc: 64.95%   [EVAL] batch:  222 | acc: 62.50%,  total acc: 64.94%   [EVAL] batch:  223 | acc: 75.00%,  total acc: 64.98%   [EVAL] batch:  224 | acc: 43.75%,  total acc: 64.89%   [EVAL] batch:  225 | acc: 100.00%,  total acc: 65.04%   [EVAL] batch:  226 | acc: 100.00%,  total acc: 65.20%   [EVAL] batch:  227 | acc: 100.00%,  total acc: 65.35%   [EVAL] batch:  228 | acc: 100.00%,  total acc: 65.50%   [EVAL] batch:  229 | acc: 100.00%,  total acc: 65.65%   [EVAL] batch:  230 | acc: 93.75%,  total acc: 65.77%   [EVAL] batch:  231 | acc: 100.00%,  total acc: 65.92%   [EVAL] batch:  232 | acc: 100.00%,  total acc: 66.07%   [EVAL] batch:  233 | acc: 100.00%,  total acc: 66.21%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 66.36%   [EVAL] batch:  235 | acc: 100.00%,  total acc: 66.50%   [EVAL] batch:  236 | acc: 100.00%,  total acc: 66.64%   [EVAL] batch:  237 | acc: 81.25%,  total acc: 66.70%   [EVAL] batch:  238 | acc: 62.50%,  total acc: 66.68%   [EVAL] batch:  239 | acc: 87.50%,  total acc: 66.77%   [EVAL] batch:  240 | acc: 81.25%,  total acc: 66.83%   [EVAL] batch:  241 | acc: 68.75%,  total acc: 66.84%   [EVAL] batch:  242 | acc: 81.25%,  total acc: 66.90%   [EVAL] batch:  243 | acc: 87.50%,  total acc: 66.98%   [EVAL] batch:  244 | acc: 93.75%,  total acc: 67.09%   [EVAL] batch:  245 | acc: 87.50%,  total acc: 67.17%   [EVAL] batch:  246 | acc: 100.00%,  total acc: 67.31%   [EVAL] batch:  247 | acc: 100.00%,  total acc: 67.44%   [EVAL] batch:  248 | acc: 81.25%,  total acc: 67.49%   [EVAL] batch:  249 | acc: 93.75%,  total acc: 67.60%   [EVAL] batch:  250 | acc: 12.50%,  total acc: 67.38%   [EVAL] batch:  251 | acc: 25.00%,  total acc: 67.21%   [EVAL] batch:  252 | acc: 18.75%,  total acc: 67.02%   [EVAL] batch:  253 | acc: 12.50%,  total acc: 66.81%   [EVAL] batch:  254 | acc: 18.75%,  total acc: 66.62%   [EVAL] batch:  255 | acc: 31.25%,  total acc: 66.48%   [EVAL] batch:  256 | acc: 50.00%,  total acc: 66.42%   [EVAL] batch:  257 | acc: 62.50%,  total acc: 66.40%   [EVAL] batch:  258 | acc: 68.75%,  total acc: 66.41%   [EVAL] batch:  259 | acc: 62.50%,  total acc: 66.39%   [EVAL] batch:  260 | acc: 43.75%,  total acc: 66.31%   [EVAL] batch:  261 | acc: 56.25%,  total acc: 66.27%   [EVAL] batch:  262 | acc: 56.25%,  total acc: 66.23%   [EVAL] batch:  263 | acc: 56.25%,  total acc: 66.19%   [EVAL] batch:  264 | acc: 50.00%,  total acc: 66.13%   [EVAL] batch:  265 | acc: 25.00%,  total acc: 65.98%   [EVAL] batch:  266 | acc: 56.25%,  total acc: 65.94%   [EVAL] batch:  267 | acc: 56.25%,  total acc: 65.90%   [EVAL] batch:  268 | acc: 75.00%,  total acc: 65.94%   [EVAL] batch:  269 | acc: 81.25%,  total acc: 66.00%   [EVAL] batch:  270 | acc: 93.75%,  total acc: 66.10%   [EVAL] batch:  271 | acc: 100.00%,  total acc: 66.22%   [EVAL] batch:  272 | acc: 87.50%,  total acc: 66.30%   [EVAL] batch:  273 | acc: 87.50%,  total acc: 66.38%   [EVAL] batch:  274 | acc: 81.25%,  total acc: 66.43%   [EVAL] batch:  275 | acc: 56.25%,  total acc: 66.39%   [EVAL] batch:  276 | acc: 43.75%,  total acc: 66.31%   [EVAL] batch:  277 | acc: 81.25%,  total acc: 66.37%   [EVAL] batch:  278 | acc: 68.75%,  total acc: 66.38%   [EVAL] batch:  279 | acc: 68.75%,  total acc: 66.38%   [EVAL] batch:  280 | acc: 75.00%,  total acc: 66.41%   [EVAL] batch:  281 | acc: 43.75%,  total acc: 66.33%   [EVAL] batch:  282 | acc: 25.00%,  total acc: 66.19%   [EVAL] batch:  283 | acc: 6.25%,  total acc: 65.98%   [EVAL] batch:  284 | acc: 18.75%,  total acc: 65.81%   [EVAL] batch:  285 | acc: 6.25%,  total acc: 65.60%   [EVAL] batch:  286 | acc: 0.00%,  total acc: 65.37%   [EVAL] batch:  287 | acc: 50.00%,  total acc: 65.32%   [EVAL] batch:  288 | acc: 87.50%,  total acc: 65.40%   [EVAL] batch:  289 | acc: 81.25%,  total acc: 65.45%   [EVAL] batch:  290 | acc: 62.50%,  total acc: 65.44%   [EVAL] batch:  291 | acc: 93.75%,  total acc: 65.54%   [EVAL] batch:  292 | acc: 81.25%,  total acc: 65.59%   [EVAL] batch:  293 | acc: 62.50%,  total acc: 65.58%   [EVAL] batch:  294 | acc: 68.75%,  total acc: 65.59%   [EVAL] batch:  295 | acc: 62.50%,  total acc: 65.58%   [EVAL] batch:  296 | acc: 68.75%,  total acc: 65.59%   [EVAL] batch:  297 | acc: 56.25%,  total acc: 65.56%   [EVAL] batch:  298 | acc: 75.00%,  total acc: 65.59%   [EVAL] batch:  299 | acc: 87.50%,  total acc: 65.67%   [EVAL] batch:  300 | acc: 100.00%,  total acc: 65.78%   [EVAL] batch:  301 | acc: 100.00%,  total acc: 65.89%   [EVAL] batch:  302 | acc: 100.00%,  total acc: 66.01%   [EVAL] batch:  303 | acc: 100.00%,  total acc: 66.12%   [EVAL] batch:  304 | acc: 100.00%,  total acc: 66.23%   [EVAL] batch:  305 | acc: 100.00%,  total acc: 66.34%   [EVAL] batch:  306 | acc: 93.75%,  total acc: 66.43%   [EVAL] batch:  307 | acc: 87.50%,  total acc: 66.50%   [EVAL] batch:  308 | acc: 87.50%,  total acc: 66.57%   [EVAL] batch:  309 | acc: 93.75%,  total acc: 66.65%   [EVAL] batch:  310 | acc: 87.50%,  total acc: 66.72%   [EVAL] batch:  311 | acc: 100.00%,  total acc: 66.83%   [EVAL] batch:  312 | acc: 75.00%,  total acc: 66.85%   [EVAL] batch:  313 | acc: 12.50%,  total acc: 66.68%   [EVAL] batch:  314 | acc: 25.00%,  total acc: 66.55%   [EVAL] batch:  315 | acc: 25.00%,  total acc: 66.42%   [EVAL] batch:  316 | acc: 12.50%,  total acc: 66.25%   [EVAL] batch:  317 | acc: 31.25%,  total acc: 66.14%   [EVAL] batch:  318 | acc: 18.75%,  total acc: 65.99%   [EVAL] batch:  319 | acc: 81.25%,  total acc: 66.04%   [EVAL] batch:  320 | acc: 100.00%,  total acc: 66.14%   [EVAL] batch:  321 | acc: 81.25%,  total acc: 66.19%   [EVAL] batch:  322 | acc: 93.75%,  total acc: 66.27%   [EVAL] batch:  323 | acc: 87.50%,  total acc: 66.34%   [EVAL] batch:  324 | acc: 81.25%,  total acc: 66.38%   [EVAL] batch:  325 | acc: 31.25%,  total acc: 66.28%   [EVAL] batch:  326 | acc: 18.75%,  total acc: 66.13%   [EVAL] batch:  327 | acc: 31.25%,  total acc: 66.03%   [EVAL] batch:  328 | acc: 50.00%,  total acc: 65.98%   [EVAL] batch:  329 | acc: 50.00%,  total acc: 65.93%   [EVAL] batch:  330 | acc: 6.25%,  total acc: 65.75%   [EVAL] batch:  331 | acc: 81.25%,  total acc: 65.79%   [EVAL] batch:  332 | acc: 100.00%,  total acc: 65.90%   [EVAL] batch:  333 | acc: 100.00%,  total acc: 66.00%   [EVAL] batch:  334 | acc: 93.75%,  total acc: 66.08%   [EVAL] batch:  335 | acc: 93.75%,  total acc: 66.16%   [EVAL] batch:  336 | acc: 100.00%,  total acc: 66.26%   [EVAL] batch:  337 | acc: 87.50%,  total acc: 66.33%   [EVAL] batch:  338 | acc: 81.25%,  total acc: 66.37%   [EVAL] batch:  339 | acc: 81.25%,  total acc: 66.42%   [EVAL] batch:  340 | acc: 93.75%,  total acc: 66.50%   [EVAL] batch:  341 | acc: 75.00%,  total acc: 66.52%   [EVAL] batch:  342 | acc: 100.00%,  total acc: 66.62%   [EVAL] batch:  343 | acc: 81.25%,  total acc: 66.66%   [EVAL] batch:  344 | acc: 100.00%,  total acc: 66.76%   [EVAL] batch:  345 | acc: 100.00%,  total acc: 66.85%   [EVAL] batch:  346 | acc: 87.50%,  total acc: 66.91%   [EVAL] batch:  347 | acc: 87.50%,  total acc: 66.97%   [EVAL] batch:  348 | acc: 93.75%,  total acc: 67.05%   [EVAL] batch:  349 | acc: 100.00%,  total acc: 67.14%   [EVAL] batch:  350 | acc: 50.00%,  total acc: 67.09%   [EVAL] batch:  351 | acc: 31.25%,  total acc: 66.99%   [EVAL] batch:  352 | acc: 50.00%,  total acc: 66.94%   [EVAL] batch:  353 | acc: 25.00%,  total acc: 66.83%   [EVAL] batch:  354 | acc: 31.25%,  total acc: 66.73%   [EVAL] batch:  355 | acc: 12.50%,  total acc: 66.57%   [EVAL] batch:  356 | acc: 81.25%,  total acc: 66.61%   [EVAL] batch:  357 | acc: 87.50%,  total acc: 66.67%   [EVAL] batch:  358 | acc: 81.25%,  total acc: 66.71%   [EVAL] batch:  359 | acc: 87.50%,  total acc: 66.77%   [EVAL] batch:  360 | acc: 62.50%,  total acc: 66.76%   [EVAL] batch:  361 | acc: 62.50%,  total acc: 66.75%   [EVAL] batch:  362 | acc: 68.75%,  total acc: 66.75%   [EVAL] batch:  363 | acc: 68.75%,  total acc: 66.76%   [EVAL] batch:  364 | acc: 50.00%,  total acc: 66.71%   [EVAL] batch:  365 | acc: 68.75%,  total acc: 66.72%   [EVAL] batch:  366 | acc: 62.50%,  total acc: 66.71%   [EVAL] batch:  367 | acc: 56.25%,  total acc: 66.68%   [EVAL] batch:  368 | acc: 25.00%,  total acc: 66.57%   [EVAL] batch:  369 | acc: 68.75%,  total acc: 66.57%   [EVAL] batch:  370 | acc: 43.75%,  total acc: 66.51%   [EVAL] batch:  371 | acc: 56.25%,  total acc: 66.48%   [EVAL] batch:  372 | acc: 68.75%,  total acc: 66.49%   [EVAL] batch:  373 | acc: 68.75%,  total acc: 66.49%   [EVAL] batch:  374 | acc: 68.75%,  total acc: 66.50%   [EVAL] batch:  375 | acc: 18.75%,  total acc: 66.37%   [EVAL] batch:  376 | acc: 56.25%,  total acc: 66.35%   [EVAL] batch:  377 | acc: 12.50%,  total acc: 66.20%   [EVAL] batch:  378 | acc: 31.25%,  total acc: 66.11%   [EVAL] batch:  379 | acc: 37.50%,  total acc: 66.04%   [EVAL] batch:  380 | acc: 25.00%,  total acc: 65.93%   [EVAL] batch:  381 | acc: 68.75%,  total acc: 65.94%   [EVAL] batch:  382 | acc: 81.25%,  total acc: 65.98%   [EVAL] batch:  383 | acc: 100.00%,  total acc: 66.06%   [EVAL] batch:  384 | acc: 87.50%,  total acc: 66.12%   [EVAL] batch:  385 | acc: 93.75%,  total acc: 66.19%   [EVAL] batch:  386 | acc: 75.00%,  total acc: 66.21%   [EVAL] batch:  387 | acc: 87.50%,  total acc: 66.27%   [EVAL] batch:  388 | acc: 81.25%,  total acc: 66.31%   [EVAL] batch:  389 | acc: 93.75%,  total acc: 66.38%   [EVAL] batch:  390 | acc: 87.50%,  total acc: 66.43%   [EVAL] batch:  391 | acc: 93.75%,  total acc: 66.50%   [EVAL] batch:  392 | acc: 87.50%,  total acc: 66.56%   [EVAL] batch:  393 | acc: 75.00%,  total acc: 66.58%   [EVAL] batch:  394 | acc: 31.25%,  total acc: 66.49%   [EVAL] batch:  395 | acc: 12.50%,  total acc: 66.35%   [EVAL] batch:  396 | acc: 50.00%,  total acc: 66.31%   [EVAL] batch:  397 | acc: 62.50%,  total acc: 66.30%   [EVAL] batch:  398 | acc: 43.75%,  total acc: 66.24%   [EVAL] batch:  399 | acc: 50.00%,  total acc: 66.20%   [EVAL] batch:  400 | acc: 6.25%,  total acc: 66.05%   [EVAL] batch:  401 | acc: 6.25%,  total acc: 65.90%   [EVAL] batch:  402 | acc: 6.25%,  total acc: 65.76%   [EVAL] batch:  403 | acc: 6.25%,  total acc: 65.61%   [EVAL] batch:  404 | acc: 6.25%,  total acc: 65.46%   [EVAL] batch:  405 | acc: 0.00%,  total acc: 65.30%   [EVAL] batch:  406 | acc: 43.75%,  total acc: 65.25%   [EVAL] batch:  407 | acc: 87.50%,  total acc: 65.30%   [EVAL] batch:  408 | acc: 75.00%,  total acc: 65.33%   [EVAL] batch:  409 | acc: 81.25%,  total acc: 65.37%   [EVAL] batch:  410 | acc: 81.25%,  total acc: 65.40%   [EVAL] batch:  411 | acc: 62.50%,  total acc: 65.40%   [EVAL] batch:  412 | acc: 87.50%,  total acc: 65.45%   [EVAL] batch:  413 | acc: 93.75%,  total acc: 65.52%   [EVAL] batch:  414 | acc: 100.00%,  total acc: 65.60%   [EVAL] batch:  415 | acc: 87.50%,  total acc: 65.66%   [EVAL] batch:  416 | acc: 87.50%,  total acc: 65.71%   [EVAL] batch:  417 | acc: 100.00%,  total acc: 65.79%   [EVAL] batch:  418 | acc: 93.75%,  total acc: 65.86%   [EVAL] batch:  419 | acc: 31.25%,  total acc: 65.77%   [EVAL] batch:  420 | acc: 12.50%,  total acc: 65.65%   [EVAL] batch:  421 | acc: 6.25%,  total acc: 65.51%   [EVAL] batch:  422 | acc: 18.75%,  total acc: 65.40%   [EVAL] batch:  423 | acc: 31.25%,  total acc: 65.32%   [EVAL] batch:  424 | acc: 18.75%,  total acc: 65.21%   [EVAL] batch:  425 | acc: 18.75%,  total acc: 65.10%   [EVAL] batch:  426 | acc: 37.50%,  total acc: 65.03%   [EVAL] batch:  427 | acc: 31.25%,  total acc: 64.95%   [EVAL] batch:  428 | acc: 43.75%,  total acc: 64.90%   [EVAL] batch:  429 | acc: 37.50%,  total acc: 64.84%   [EVAL] batch:  430 | acc: 50.00%,  total acc: 64.81%   [EVAL] batch:  431 | acc: 43.75%,  total acc: 64.76%   [EVAL] batch:  432 | acc: 50.00%,  total acc: 64.72%   [EVAL] batch:  433 | acc: 37.50%,  total acc: 64.66%   [EVAL] batch:  434 | acc: 75.00%,  total acc: 64.68%   [EVAL] batch:  435 | acc: 50.00%,  total acc: 64.65%   [EVAL] batch:  436 | acc: 75.00%,  total acc: 64.67%   [EVAL] batch:  437 | acc: 62.50%,  total acc: 64.67%   [EVAL] batch:  438 | acc: 62.50%,  total acc: 64.66%   [EVAL] batch:  439 | acc: 75.00%,  total acc: 64.69%   [EVAL] batch:  440 | acc: 87.50%,  total acc: 64.74%   [EVAL] batch:  441 | acc: 75.00%,  total acc: 64.76%   [EVAL] batch:  442 | acc: 87.50%,  total acc: 64.81%   [EVAL] batch:  443 | acc: 81.25%,  total acc: 64.85%   [EVAL] batch:  444 | acc: 75.00%,  total acc: 64.87%   [EVAL] batch:  445 | acc: 56.25%,  total acc: 64.85%   [EVAL] batch:  446 | acc: 56.25%,  total acc: 64.84%   [EVAL] batch:  447 | acc: 81.25%,  total acc: 64.87%   [EVAL] batch:  448 | acc: 81.25%,  total acc: 64.91%   [EVAL] batch:  449 | acc: 81.25%,  total acc: 64.94%   [EVAL] batch:  450 | acc: 68.75%,  total acc: 64.95%   [EVAL] batch:  451 | acc: 75.00%,  total acc: 64.98%   [EVAL] batch:  452 | acc: 62.50%,  total acc: 64.97%   [EVAL] batch:  453 | acc: 75.00%,  total acc: 64.99%   [EVAL] batch:  454 | acc: 81.25%,  total acc: 65.03%   [EVAL] batch:  455 | acc: 81.25%,  total acc: 65.06%   [EVAL] batch:  456 | acc: 68.75%,  total acc: 65.07%   [EVAL] batch:  457 | acc: 62.50%,  total acc: 65.07%   [EVAL] batch:  458 | acc: 43.75%,  total acc: 65.02%   [EVAL] batch:  459 | acc: 75.00%,  total acc: 65.04%   [EVAL] batch:  460 | acc: 31.25%,  total acc: 64.97%   [EVAL] batch:  461 | acc: 43.75%,  total acc: 64.92%   [EVAL] batch:  462 | acc: 75.00%,  total acc: 64.94%   [EVAL] batch:  463 | acc: 100.00%,  total acc: 65.02%   [EVAL] batch:  464 | acc: 100.00%,  total acc: 65.09%   [EVAL] batch:  465 | acc: 81.25%,  total acc: 65.13%   [EVAL] batch:  466 | acc: 100.00%,  total acc: 65.20%   [EVAL] batch:  467 | acc: 100.00%,  total acc: 65.28%   [EVAL] batch:  468 | acc: 93.75%,  total acc: 65.34%   [EVAL] batch:  469 | acc: 100.00%,  total acc: 65.41%   [EVAL] batch:  470 | acc: 100.00%,  total acc: 65.49%   [EVAL] batch:  471 | acc: 100.00%,  total acc: 65.56%   [EVAL] batch:  472 | acc: 100.00%,  total acc: 65.63%   [EVAL] batch:  473 | acc: 100.00%,  total acc: 65.70%   [EVAL] batch:  474 | acc: 100.00%,  total acc: 65.78%   [EVAL] batch:  475 | acc: 100.00%,  total acc: 65.85%   [EVAL] batch:  476 | acc: 93.75%,  total acc: 65.91%   [EVAL] batch:  477 | acc: 100.00%,  total acc: 65.98%   [EVAL] batch:  478 | acc: 93.75%,  total acc: 66.04%   [EVAL] batch:  479 | acc: 100.00%,  total acc: 66.11%   [EVAL] batch:  480 | acc: 93.75%,  total acc: 66.16%   [EVAL] batch:  481 | acc: 62.50%,  total acc: 66.16%   [EVAL] batch:  482 | acc: 25.00%,  total acc: 66.07%   [EVAL] batch:  483 | acc: 43.75%,  total acc: 66.03%   [EVAL] batch:  484 | acc: 50.00%,  total acc: 65.99%   [EVAL] batch:  485 | acc: 56.25%,  total acc: 65.97%   [EVAL] batch:  486 | acc: 43.75%,  total acc: 65.93%   [EVAL] batch:  487 | acc: 75.00%,  total acc: 65.95%   [EVAL] batch:  488 | acc: 87.50%,  total acc: 65.99%   [EVAL] batch:  489 | acc: 100.00%,  total acc: 66.06%   [EVAL] batch:  490 | acc: 87.50%,  total acc: 66.10%   [EVAL] batch:  491 | acc: 87.50%,  total acc: 66.15%   [EVAL] batch:  492 | acc: 93.75%,  total acc: 66.20%   [EVAL] batch:  493 | acc: 81.25%,  total acc: 66.23%   [EVAL] batch:  494 | acc: 18.75%,  total acc: 66.14%   [EVAL] batch:  495 | acc: 18.75%,  total acc: 66.04%   [EVAL] batch:  496 | acc: 31.25%,  total acc: 65.97%   [EVAL] batch:  497 | acc: 18.75%,  total acc: 65.88%   [EVAL] batch:  498 | acc: 31.25%,  total acc: 65.81%   [EVAL] batch:  499 | acc: 18.75%,  total acc: 65.71%   
cur_acc:  ['0.9524', '0.8135', '0.7579', '0.8502', '0.7331', '0.7143', '0.5595', '0.7262']
his_acc:  ['0.9524', '0.8640', '0.8165', '0.8065', '0.7746', '0.7342', '0.6747', '0.6571']
--------Round  1
seed:  200
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/train.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/valid.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/test.pkl
Task_order: [7 6 3 2 4 0 5 1]
prepared data!
CurrentTrain: epoch 15, batch     0 | loss: 34.9528102CurrentTrain: epoch 15, batch     1 | loss: 29.8023607CurrentTrain: epoch 15, batch     2 | loss: 24.4374538CurrentTrain: epoch 15, batch     3 | loss: 26.9367635CurrentTrain: epoch 15, batch     4 | loss: 27.9031098CurrentTrain: epoch 15, batch     5 | loss: 21.4982335CurrentTrain: epoch 15, batch     6 | loss: 22.5051585CurrentTrain: epoch 15, batch     7 | loss: 37.6417048CurrentTrain: epoch 15, batch     8 | loss: 23.0867567CurrentTrain: epoch 15, batch     9 | loss: 27.9587806CurrentTrain: epoch 15, batch    10 | loss: 35.1439045CurrentTrain: epoch 15, batch    11 | loss: 27.0535682CurrentTrain: epoch 15, batch    12 | loss: 24.3212893CurrentTrain: epoch 15, batch    13 | loss: 17.8933680CurrentTrain: epoch 15, batch    14 | loss: 20.8517191CurrentTrain: epoch 15, batch    15 | loss: 29.9624318CurrentTrain: epoch 15, batch    16 | loss: 18.3020977CurrentTrain: epoch 15, batch    17 | loss: 26.8006990CurrentTrain: epoch 15, batch    18 | loss: 20.8506256CurrentTrain: epoch 15, batch    19 | loss: 23.3798106CurrentTrain: epoch 15, batch    20 | loss: 34.7826703CurrentTrain: epoch 15, batch    21 | loss: 21.3359851CurrentTrain: epoch 15, batch    22 | loss: 19.4475330CurrentTrain: epoch 15, batch    23 | loss: 14.7706145CurrentTrain: epoch 15, batch    24 | loss: 22.4582012CurrentTrain: epoch 15, batch    25 | loss: 22.4388175CurrentTrain: epoch 15, batch    26 | loss: 36.3664802CurrentTrain: epoch 15, batch    27 | loss: 25.7002758CurrentTrain: epoch 15, batch    28 | loss: 13.7169912CurrentTrain: epoch 15, batch    29 | loss: 14.9751266CurrentTrain: epoch 15, batch    30 | loss: 13.6193682CurrentTrain: epoch 15, batch    31 | loss: 21.4686445CurrentTrain: epoch 15, batch    32 | loss: 29.7658021CurrentTrain: epoch 15, batch    33 | loss: 17.4333882CurrentTrain: epoch 15, batch    34 | loss: 15.5709671CurrentTrain: epoch 15, batch    35 | loss: 17.1449382CurrentTrain: epoch 15, batch    36 | loss: 21.6396466CurrentTrain: epoch 15, batch    37 | loss: 26.3853020CurrentTrain: epoch 15, batch    38 | loss: 23.8223766CurrentTrain: epoch 15, batch    39 | loss: 16.3300714CurrentTrain: epoch 15, batch    40 | loss: 17.3677644CurrentTrain: epoch 15, batch    41 | loss: 30.3790433CurrentTrain: epoch 15, batch    42 | loss: 21.3571528CurrentTrain: epoch 15, batch    43 | loss: 26.7679917CurrentTrain: epoch 15, batch    44 | loss: 28.8139811CurrentTrain: epoch 15, batch    45 | loss: 18.7428447CurrentTrain: epoch 15, batch    46 | loss: 19.3140921CurrentTrain: epoch 15, batch    47 | loss: 19.1660112CurrentTrain: epoch 15, batch    48 | loss: 18.1324319CurrentTrain: epoch 15, batch    49 | loss: 17.9170299CurrentTrain: epoch 15, batch    50 | loss: 14.2588764CurrentTrain: epoch 15, batch    51 | loss: 16.5461813CurrentTrain: epoch 15, batch    52 | loss: 19.6979483CurrentTrain: epoch 15, batch    53 | loss: 20.9139718CurrentTrain: epoch 15, batch    54 | loss: 15.6106450CurrentTrain: epoch 15, batch    55 | loss: 24.4363947CurrentTrain: epoch 15, batch    56 | loss: 16.9946981CurrentTrain: epoch 15, batch    57 | loss: 18.6929781CurrentTrain: epoch 15, batch    58 | loss: 16.1764873CurrentTrain: epoch 15, batch    59 | loss: 17.0182781CurrentTrain: epoch 15, batch    60 | loss: 13.3526011CurrentTrain: epoch 15, batch    61 | loss: 14.7824594CurrentTrain: epoch  7, batch    62 | loss: 14.2314877CurrentTrain: epoch 15, batch     0 | loss: 17.9222952CurrentTrain: epoch 15, batch     1 | loss: 15.3333666CurrentTrain: epoch 15, batch     2 | loss: 14.8022033CurrentTrain: epoch 15, batch     3 | loss: 14.1531590CurrentTrain: epoch 15, batch     4 | loss: 14.5331586CurrentTrain: epoch 15, batch     5 | loss: 21.3268616CurrentTrain: epoch 15, batch     6 | loss: 22.1628196CurrentTrain: epoch 15, batch     7 | loss: 20.8499012CurrentTrain: epoch 15, batch     8 | loss: 12.6376042CurrentTrain: epoch 15, batch     9 | loss: 15.9254502CurrentTrain: epoch 15, batch    10 | loss: 34.6729113CurrentTrain: epoch 15, batch    11 | loss: 12.8279205CurrentTrain: epoch 15, batch    12 | loss: 13.0282768CurrentTrain: epoch 15, batch    13 | loss: 12.7856724CurrentTrain: epoch 15, batch    14 | loss: 33.0502902CurrentTrain: epoch 15, batch    15 | loss: 18.4382682CurrentTrain: epoch 15, batch    16 | loss: 20.2073851CurrentTrain: epoch 15, batch    17 | loss: 21.3836669CurrentTrain: epoch 15, batch    18 | loss: 13.8620632CurrentTrain: epoch 15, batch    19 | loss: 16.8537351CurrentTrain: epoch 15, batch    20 | loss: 15.5046330CurrentTrain: epoch 15, batch    21 | loss: 16.0486141CurrentTrain: epoch 15, batch    22 | loss: 13.5679103CurrentTrain: epoch 15, batch    23 | loss: 21.6418931CurrentTrain: epoch 15, batch    24 | loss: 14.0337628CurrentTrain: epoch 15, batch    25 | loss: 26.2632660CurrentTrain: epoch 15, batch    26 | loss: 20.0267744CurrentTrain: epoch 15, batch    27 | loss: 15.0534490CurrentTrain: epoch 15, batch    28 | loss: 16.4468198CurrentTrain: epoch 15, batch    29 | loss: 15.8334922CurrentTrain: epoch 15, batch    30 | loss: 13.8712601CurrentTrain: epoch 15, batch    31 | loss: 21.3673687CurrentTrain: epoch 15, batch    32 | loss: 24.4232508CurrentTrain: epoch 15, batch    33 | loss: 16.3447967CurrentTrain: epoch 15, batch    34 | loss: 21.8587880CurrentTrain: epoch 15, batch    35 | loss: 21.8991220CurrentTrain: epoch 15, batch    36 | loss: 18.8231202CurrentTrain: epoch 15, batch    37 | loss: 9.2624765CurrentTrain: epoch 15, batch    38 | loss: 20.6725683CurrentTrain: epoch 15, batch    39 | loss: 16.9168780CurrentTrain: epoch 15, batch    40 | loss: 12.8057331CurrentTrain: epoch 15, batch    41 | loss: 35.8378790CurrentTrain: epoch 15, batch    42 | loss: 14.9980250CurrentTrain: epoch 15, batch    43 | loss: 13.2134972CurrentTrain: epoch 15, batch    44 | loss: 15.9132723CurrentTrain: epoch 15, batch    45 | loss: 17.0572817CurrentTrain: epoch 15, batch    46 | loss: 19.5156176CurrentTrain: epoch 15, batch    47 | loss: 15.5827043CurrentTrain: epoch 15, batch    48 | loss: 21.7398848CurrentTrain: epoch 15, batch    49 | loss: 25.0989672CurrentTrain: epoch 15, batch    50 | loss: 19.9333819CurrentTrain: epoch 15, batch    51 | loss: 23.3891602CurrentTrain: epoch 15, batch    52 | loss: 35.2113523CurrentTrain: epoch 15, batch    53 | loss: 16.1603221CurrentTrain: epoch 15, batch    54 | loss: 27.5316340CurrentTrain: epoch 15, batch    55 | loss: 14.8619631CurrentTrain: epoch 15, batch    56 | loss: 17.6659772CurrentTrain: epoch 15, batch    57 | loss: 15.5968195CurrentTrain: epoch 15, batch    58 | loss: 29.7621218CurrentTrain: epoch 15, batch    59 | loss: 16.3252952CurrentTrain: epoch 15, batch    60 | loss: 12.6073497CurrentTrain: epoch 15, batch    61 | loss: 12.6495295CurrentTrain: epoch  7, batch    62 | loss: 11.6885408CurrentTrain: epoch 15, batch     0 | loss: 14.1216318CurrentTrain: epoch 15, batch     1 | loss: 15.7716385CurrentTrain: epoch 15, batch     2 | loss: 22.1878834CurrentTrain: epoch 15, batch     3 | loss: 11.9366285CurrentTrain: epoch 15, batch     4 | loss: 10.2032824CurrentTrain: epoch 15, batch     5 | loss: 15.4210275CurrentTrain: epoch 15, batch     6 | loss: 16.6954284CurrentTrain: epoch 15, batch     7 | loss: 12.5957586CurrentTrain: epoch 15, batch     8 | loss: 15.7435537CurrentTrain: epoch 15, batch     9 | loss: 16.7545986CurrentTrain: epoch 15, batch    10 | loss: 11.2040402CurrentTrain: epoch 15, batch    11 | loss: 15.1563796CurrentTrain: epoch 15, batch    12 | loss: 13.0422402CurrentTrain: epoch 15, batch    13 | loss: 12.4621516CurrentTrain: epoch 15, batch    14 | loss: 18.9485854CurrentTrain: epoch 15, batch    15 | loss: 23.3631014CurrentTrain: epoch 15, batch    16 | loss: 30.3476768CurrentTrain: epoch 15, batch    17 | loss: 12.5892094CurrentTrain: epoch 15, batch    18 | loss: 10.4366336CurrentTrain: epoch 15, batch    19 | loss: 14.1388985CurrentTrain: epoch 15, batch    20 | loss: 12.7355965CurrentTrain: epoch 15, batch    21 | loss: 11.8512221CurrentTrain: epoch 15, batch    22 | loss: 13.8920804CurrentTrain: epoch 15, batch    23 | loss: 21.4878876CurrentTrain: epoch 15, batch    24 | loss: 15.4168652CurrentTrain: epoch 15, batch    25 | loss: 12.5857386CurrentTrain: epoch 15, batch    26 | loss: 14.7967728CurrentTrain: epoch 15, batch    27 | loss: 9.6575550CurrentTrain: epoch 15, batch    28 | loss: 11.9963326CurrentTrain: epoch 15, batch    29 | loss: 13.0344784CurrentTrain: epoch 15, batch    30 | loss: 13.0812570CurrentTrain: epoch 15, batch    31 | loss: 28.6099854CurrentTrain: epoch 15, batch    32 | loss: 16.3923951CurrentTrain: epoch 15, batch    33 | loss: 13.3191471CurrentTrain: epoch 15, batch    34 | loss: 15.7043598CurrentTrain: epoch 15, batch    35 | loss: 14.3384384CurrentTrain: epoch 15, batch    36 | loss: 11.2855063CurrentTrain: epoch 15, batch    37 | loss: 10.9795357CurrentTrain: epoch 15, batch    38 | loss: 14.2895332CurrentTrain: epoch 15, batch    39 | loss: 31.1901638CurrentTrain: epoch 15, batch    40 | loss: 23.3648047CurrentTrain: epoch 15, batch    41 | loss: 19.1872524CurrentTrain: epoch 15, batch    42 | loss: 12.8748568CurrentTrain: epoch 15, batch    43 | loss: 27.1332432CurrentTrain: epoch 15, batch    44 | loss: 13.2101767CurrentTrain: epoch 15, batch    45 | loss: 14.8684927CurrentTrain: epoch 15, batch    46 | loss: 13.6892258CurrentTrain: epoch 15, batch    47 | loss: 17.0045952CurrentTrain: epoch 15, batch    48 | loss: 12.2928413CurrentTrain: epoch 15, batch    49 | loss: 14.2538151CurrentTrain: epoch 15, batch    50 | loss: 30.3316106CurrentTrain: epoch 15, batch    51 | loss: 13.0674392CurrentTrain: epoch 15, batch    52 | loss: 12.2586214CurrentTrain: epoch 15, batch    53 | loss: 11.9859871CurrentTrain: epoch 15, batch    54 | loss: 21.1647212CurrentTrain: epoch 15, batch    55 | loss: 13.6620944CurrentTrain: epoch 15, batch    56 | loss: 30.7518420CurrentTrain: epoch 15, batch    57 | loss: 14.2182838CurrentTrain: epoch 15, batch    58 | loss: 13.0136484CurrentTrain: epoch 15, batch    59 | loss: 9.4173801CurrentTrain: epoch 15, batch    60 | loss: 14.5199991CurrentTrain: epoch 15, batch    61 | loss: 18.1732988CurrentTrain: epoch  7, batch    62 | loss: 13.5887521CurrentTrain: epoch 15, batch     0 | loss: 11.7544713CurrentTrain: epoch 15, batch     1 | loss: 15.5068667CurrentTrain: epoch 15, batch     2 | loss: 17.5302471CurrentTrain: epoch 15, batch     3 | loss: 23.6389536CurrentTrain: epoch 15, batch     4 | loss: 9.4640466CurrentTrain: epoch 15, batch     5 | loss: 23.4052810CurrentTrain: epoch 15, batch     6 | loss: 12.5767470CurrentTrain: epoch 15, batch     7 | loss: 10.8340034CurrentTrain: epoch 15, batch     8 | loss: 13.6955449CurrentTrain: epoch 15, batch     9 | loss: 17.5452245CurrentTrain: epoch 15, batch    10 | loss: 12.4268622CurrentTrain: epoch 15, batch    11 | loss: 12.8262099CurrentTrain: epoch 15, batch    12 | loss: 16.5680594CurrentTrain: epoch 15, batch    13 | loss: 17.8657598CurrentTrain: epoch 15, batch    14 | loss: 18.4486096CurrentTrain: epoch 15, batch    15 | loss: 29.9857651CurrentTrain: epoch 15, batch    16 | loss: 11.7650290CurrentTrain: epoch 15, batch    17 | loss: 18.1363484CurrentTrain: epoch 15, batch    18 | loss: 13.0483061CurrentTrain: epoch 15, batch    19 | loss: 14.2181340CurrentTrain: epoch 15, batch    20 | loss: 18.3636255CurrentTrain: epoch 15, batch    21 | loss: 16.0876235CurrentTrain: epoch 15, batch    22 | loss: 25.0417948CurrentTrain: epoch 15, batch    23 | loss: 13.5333828CurrentTrain: epoch 15, batch    24 | loss: 10.5759353CurrentTrain: epoch 15, batch    25 | loss: 13.6497487CurrentTrain: epoch 15, batch    26 | loss: 11.2450148CurrentTrain: epoch 15, batch    27 | loss: 15.8729834CurrentTrain: epoch 15, batch    28 | loss: 17.4258952CurrentTrain: epoch 15, batch    29 | loss: 15.0885875CurrentTrain: epoch 15, batch    30 | loss: 13.1705115CurrentTrain: epoch 15, batch    31 | loss: 9.3453749CurrentTrain: epoch 15, batch    32 | loss: 15.0827839CurrentTrain: epoch 15, batch    33 | loss: 19.9443849CurrentTrain: epoch 15, batch    34 | loss: 15.8074469CurrentTrain: epoch 15, batch    35 | loss: 17.4138298CurrentTrain: epoch 15, batch    36 | loss: 11.5578621CurrentTrain: epoch 15, batch    37 | loss: 18.6207347CurrentTrain: epoch 15, batch    38 | loss: 11.4754041CurrentTrain: epoch 15, batch    39 | loss: 16.6932850CurrentTrain: epoch 15, batch    40 | loss: 13.4830404CurrentTrain: epoch 15, batch    41 | loss: 15.7346214CurrentTrain: epoch 15, batch    42 | loss: 12.8024843CurrentTrain: epoch 15, batch    43 | loss: 15.4588993CurrentTrain: epoch 15, batch    44 | loss: 11.3781802CurrentTrain: epoch 15, batch    45 | loss: 14.2245583CurrentTrain: epoch 15, batch    46 | loss: 10.7619826CurrentTrain: epoch 15, batch    47 | loss: 10.2169192CurrentTrain: epoch 15, batch    48 | loss: 12.9677311CurrentTrain: epoch 15, batch    49 | loss: 15.0577391CurrentTrain: epoch 15, batch    50 | loss: 9.4902230CurrentTrain: epoch 15, batch    51 | loss: 11.6321277CurrentTrain: epoch 15, batch    52 | loss: 12.9204904CurrentTrain: epoch 15, batch    53 | loss: 10.6331794CurrentTrain: epoch 15, batch    54 | loss: 9.6349330CurrentTrain: epoch 15, batch    55 | loss: 15.2544579CurrentTrain: epoch 15, batch    56 | loss: 15.6919273CurrentTrain: epoch 15, batch    57 | loss: 15.0907098CurrentTrain: epoch 15, batch    58 | loss: 12.4790168CurrentTrain: epoch 15, batch    59 | loss: 9.8510144CurrentTrain: epoch 15, batch    60 | loss: 14.4694226CurrentTrain: epoch 15, batch    61 | loss: 12.0007851CurrentTrain: epoch  7, batch    62 | loss: 10.7219753CurrentTrain: epoch 15, batch     0 | loss: 16.2772519CurrentTrain: epoch 15, batch     1 | loss: 11.9611694CurrentTrain: epoch 15, batch     2 | loss: 26.5362458CurrentTrain: epoch 15, batch     3 | loss: 22.8541144CurrentTrain: epoch 15, batch     4 | loss: 17.6943044CurrentTrain: epoch 15, batch     5 | loss: 28.6992239CurrentTrain: epoch 15, batch     6 | loss: 15.5113256CurrentTrain: epoch 15, batch     7 | loss: 12.1221610CurrentTrain: epoch 15, batch     8 | loss: 17.0750280CurrentTrain: epoch 15, batch     9 | loss: 11.3916482CurrentTrain: epoch 15, batch    10 | loss: 17.9479403CurrentTrain: epoch 15, batch    11 | loss: 12.0564830CurrentTrain: epoch 15, batch    12 | loss: 11.1905883CurrentTrain: epoch 15, batch    13 | loss: 16.9968032CurrentTrain: epoch 15, batch    14 | loss: 13.9609814CurrentTrain: epoch 15, batch    15 | loss: 15.2344676CurrentTrain: epoch 15, batch    16 | loss: 10.1152671CurrentTrain: epoch 15, batch    17 | loss: 14.5702817CurrentTrain: epoch 15, batch    18 | loss: 12.9947793CurrentTrain: epoch 15, batch    19 | loss: 13.3858131CurrentTrain: epoch 15, batch    20 | loss: 16.9039330CurrentTrain: epoch 15, batch    21 | loss: 13.5294483CurrentTrain: epoch 15, batch    22 | loss: 21.5465263CurrentTrain: epoch 15, batch    23 | loss: 11.6436457CurrentTrain: epoch 15, batch    24 | loss: 8.6573265CurrentTrain: epoch 15, batch    25 | loss: 9.5706507CurrentTrain: epoch 15, batch    26 | loss: 11.5654719CurrentTrain: epoch 15, batch    27 | loss: 13.7962423CurrentTrain: epoch 15, batch    28 | loss: 9.7359589CurrentTrain: epoch 15, batch    29 | loss: 13.8360341CurrentTrain: epoch 15, batch    30 | loss: 22.5002045CurrentTrain: epoch 15, batch    31 | loss: 14.6180889CurrentTrain: epoch 15, batch    32 | loss: 18.0394636CurrentTrain: epoch 15, batch    33 | loss: 12.8654391CurrentTrain: epoch 15, batch    34 | loss: 10.6032894CurrentTrain: epoch 15, batch    35 | loss: 11.6409187CurrentTrain: epoch 15, batch    36 | loss: 18.0599145CurrentTrain: epoch 15, batch    37 | loss: 12.4168772CurrentTrain: epoch 15, batch    38 | loss: 13.1532274CurrentTrain: epoch 15, batch    39 | loss: 13.5151901CurrentTrain: epoch 15, batch    40 | loss: 12.8236377CurrentTrain: epoch 15, batch    41 | loss: 20.8235619CurrentTrain: epoch 15, batch    42 | loss: 28.6063157CurrentTrain: epoch 15, batch    43 | loss: 10.2273703CurrentTrain: epoch 15, batch    44 | loss: 9.9365075CurrentTrain: epoch 15, batch    45 | loss: 15.9054845CurrentTrain: epoch 15, batch    46 | loss: 17.7617302CurrentTrain: epoch 15, batch    47 | loss: 11.0902360CurrentTrain: epoch 15, batch    48 | loss: 10.9695780CurrentTrain: epoch 15, batch    49 | loss: 28.7662870CurrentTrain: epoch 15, batch    50 | loss: 14.0811674CurrentTrain: epoch 15, batch    51 | loss: 29.9699105CurrentTrain: epoch 15, batch    52 | loss: 10.9048651CurrentTrain: epoch 15, batch    53 | loss: 9.0335581CurrentTrain: epoch 15, batch    54 | loss: 16.4309125CurrentTrain: epoch 15, batch    55 | loss: 11.8096134CurrentTrain: epoch 15, batch    56 | loss: 12.0276117CurrentTrain: epoch 15, batch    57 | loss: 29.5552100CurrentTrain: epoch 15, batch    58 | loss: 15.7744376CurrentTrain: epoch 15, batch    59 | loss: 15.4361212CurrentTrain: epoch 15, batch    60 | loss: 17.3560991CurrentTrain: epoch 15, batch    61 | loss: 10.0778316CurrentTrain: epoch  7, batch    62 | loss: 7.3537014CurrentTrain: epoch 15, batch     0 | loss: 12.1172038CurrentTrain: epoch 15, batch     1 | loss: 17.1630487CurrentTrain: epoch 15, batch     2 | loss: 11.1271237CurrentTrain: epoch 15, batch     3 | loss: 14.4051240CurrentTrain: epoch 15, batch     4 | loss: 13.6854589CurrentTrain: epoch 15, batch     5 | loss: 12.8500497CurrentTrain: epoch 15, batch     6 | loss: 11.9873481CurrentTrain: epoch 15, batch     7 | loss: 9.9849751CurrentTrain: epoch 15, batch     8 | loss: 13.6012278CurrentTrain: epoch 15, batch     9 | loss: 14.7201606CurrentTrain: epoch 15, batch    10 | loss: 16.9573561CurrentTrain: epoch 15, batch    11 | loss: 16.0073921CurrentTrain: epoch 15, batch    12 | loss: 9.2387837CurrentTrain: epoch 15, batch    13 | loss: 13.0900020CurrentTrain: epoch 15, batch    14 | loss: 17.5477541CurrentTrain: epoch 15, batch    15 | loss: 14.0892513CurrentTrain: epoch 15, batch    16 | loss: 16.9569064CurrentTrain: epoch 15, batch    17 | loss: 16.4232404CurrentTrain: epoch 15, batch    18 | loss: 11.5012269CurrentTrain: epoch 15, batch    19 | loss: 10.6438477CurrentTrain: epoch 15, batch    20 | loss: 30.7389201CurrentTrain: epoch 15, batch    21 | loss: 13.0155384CurrentTrain: epoch 15, batch    22 | loss: 9.3112428CurrentTrain: epoch 15, batch    23 | loss: 18.1069363CurrentTrain: epoch 15, batch    24 | loss: 20.9801081CurrentTrain: epoch 15, batch    25 | loss: 11.4788579CurrentTrain: epoch 15, batch    26 | loss: 15.8363775CurrentTrain: epoch 15, batch    27 | loss: 22.8278545CurrentTrain: epoch 15, batch    28 | loss: 10.8264418CurrentTrain: epoch 15, batch    29 | loss: 13.2452840CurrentTrain: epoch 15, batch    30 | loss: 16.4054784CurrentTrain: epoch 15, batch    31 | loss: 13.5583523CurrentTrain: epoch 15, batch    32 | loss: 22.0958410CurrentTrain: epoch 15, batch    33 | loss: 12.4217072CurrentTrain: epoch 15, batch    34 | loss: 17.5620709CurrentTrain: epoch 15, batch    35 | loss: 16.6415307CurrentTrain: epoch 15, batch    36 | loss: 9.0747315CurrentTrain: epoch 15, batch    37 | loss: 11.7581086CurrentTrain: epoch 15, batch    38 | loss: 17.2477741CurrentTrain: epoch 15, batch    39 | loss: 11.0448894CurrentTrain: epoch 15, batch    40 | loss: 15.4309327CurrentTrain: epoch 15, batch    41 | loss: 15.3560481CurrentTrain: epoch 15, batch    42 | loss: 8.4832518CurrentTrain: epoch 15, batch    43 | loss: 12.3636571CurrentTrain: epoch 15, batch    44 | loss: 22.6402255CurrentTrain: epoch 15, batch    45 | loss: 13.4110548CurrentTrain: epoch 15, batch    46 | loss: 16.9906816CurrentTrain: epoch 15, batch    47 | loss: 11.2661360CurrentTrain: epoch 15, batch    48 | loss: 11.9840865CurrentTrain: epoch 15, batch    49 | loss: 20.3046532CurrentTrain: epoch 15, batch    50 | loss: 14.3019187CurrentTrain: epoch 15, batch    51 | loss: 26.7061688CurrentTrain: epoch 15, batch    52 | loss: 13.4336480CurrentTrain: epoch 15, batch    53 | loss: 21.2888403CurrentTrain: epoch 15, batch    54 | loss: 11.9784580CurrentTrain: epoch 15, batch    55 | loss: 16.2347429CurrentTrain: epoch 15, batch    56 | loss: 14.9781809CurrentTrain: epoch 15, batch    57 | loss: 17.8140603CurrentTrain: epoch 15, batch    58 | loss: 13.7053198CurrentTrain: epoch 15, batch    59 | loss: 11.3044485CurrentTrain: epoch 15, batch    60 | loss: 13.9183752CurrentTrain: epoch 15, batch    61 | loss: 21.7155785CurrentTrain: epoch  7, batch    62 | loss: 41.3866789CurrentTrain: epoch 15, batch     0 | loss: 12.3514274CurrentTrain: epoch 15, batch     1 | loss: 11.5782510CurrentTrain: epoch 15, batch     2 | loss: 10.5114591CurrentTrain: epoch 15, batch     3 | loss: 18.3483903CurrentTrain: epoch 15, batch     4 | loss: 13.6090975CurrentTrain: epoch 15, batch     5 | loss: 12.4911318CurrentTrain: epoch 15, batch     6 | loss: 10.4994908CurrentTrain: epoch 15, batch     7 | loss: 15.5565287CurrentTrain: epoch 15, batch     8 | loss: 10.5118195CurrentTrain: epoch 15, batch     9 | loss: 11.3132382CurrentTrain: epoch 15, batch    10 | loss: 10.4365482CurrentTrain: epoch 15, batch    11 | loss: 20.5379933CurrentTrain: epoch 15, batch    12 | loss: 9.3911735CurrentTrain: epoch 15, batch    13 | loss: 8.5538246CurrentTrain: epoch 15, batch    14 | loss: 12.5081677CurrentTrain: epoch 15, batch    15 | loss: 11.2597206CurrentTrain: epoch 15, batch    16 | loss: 9.6917699CurrentTrain: epoch 15, batch    17 | loss: 12.9878243CurrentTrain: epoch 15, batch    18 | loss: 16.9781417CurrentTrain: epoch 15, batch    19 | loss: 17.2901022CurrentTrain: epoch 15, batch    20 | loss: 13.8941936CurrentTrain: epoch 15, batch    21 | loss: 19.9629546CurrentTrain: epoch 15, batch    22 | loss: 22.3134330CurrentTrain: epoch 15, batch    23 | loss: 17.9415636CurrentTrain: epoch 15, batch    24 | loss: 14.3891883CurrentTrain: epoch 15, batch    25 | loss: 9.7711063CurrentTrain: epoch 15, batch    26 | loss: 11.6826003CurrentTrain: epoch 15, batch    27 | loss: 14.2068486CurrentTrain: epoch 15, batch    28 | loss: 15.5403837CurrentTrain: epoch 15, batch    29 | loss: 8.9096945CurrentTrain: epoch 15, batch    30 | loss: 10.7253618CurrentTrain: epoch 15, batch    31 | loss: 18.3908171CurrentTrain: epoch 15, batch    32 | loss: 14.7646750CurrentTrain: epoch 15, batch    33 | loss: 11.9933719CurrentTrain: epoch 15, batch    34 | loss: 9.5504136CurrentTrain: epoch 15, batch    35 | loss: 16.7346207CurrentTrain: epoch 15, batch    36 | loss: 13.8182388CurrentTrain: epoch 15, batch    37 | loss: 14.7608680CurrentTrain: epoch 15, batch    38 | loss: 16.0854573CurrentTrain: epoch 15, batch    39 | loss: 20.7224831CurrentTrain: epoch 15, batch    40 | loss: 12.8498469CurrentTrain: epoch 15, batch    41 | loss: 10.8378039CurrentTrain: epoch 15, batch    42 | loss: 13.1529294CurrentTrain: epoch 15, batch    43 | loss: 16.1972201CurrentTrain: epoch 15, batch    44 | loss: 18.8491420CurrentTrain: epoch 15, batch    45 | loss: 19.2423634CurrentTrain: epoch 15, batch    46 | loss: 11.0152795CurrentTrain: epoch 15, batch    47 | loss: 16.2783149CurrentTrain: epoch 15, batch    48 | loss: 37.7921706CurrentTrain: epoch 15, batch    49 | loss: 9.9094081CurrentTrain: epoch 15, batch    50 | loss: 16.4277109CurrentTrain: epoch 15, batch    51 | loss: 10.3302591CurrentTrain: epoch 15, batch    52 | loss: 13.2578406CurrentTrain: epoch 15, batch    53 | loss: 21.4878556CurrentTrain: epoch 15, batch    54 | loss: 11.9379447CurrentTrain: epoch 15, batch    55 | loss: 16.1414927CurrentTrain: epoch 15, batch    56 | loss: 10.0694932CurrentTrain: epoch 15, batch    57 | loss: 8.7907990CurrentTrain: epoch 15, batch    58 | loss: 16.4572051CurrentTrain: epoch 15, batch    59 | loss: 9.1909050CurrentTrain: epoch 15, batch    60 | loss: 19.7201984CurrentTrain: epoch 15, batch    61 | loss: 17.2327119CurrentTrain: epoch  7, batch    62 | loss: 9.4769403CurrentTrain: epoch 15, batch     0 | loss: 9.3902409CurrentTrain: epoch 15, batch     1 | loss: 15.7441376CurrentTrain: epoch 15, batch     2 | loss: 11.1100214CurrentTrain: epoch 15, batch     3 | loss: 11.2159734CurrentTrain: epoch 15, batch     4 | loss: 16.8124766CurrentTrain: epoch 15, batch     5 | loss: 12.4548960CurrentTrain: epoch 15, batch     6 | loss: 27.0274863CurrentTrain: epoch 15, batch     7 | loss: 11.9943597CurrentTrain: epoch 15, batch     8 | loss: 25.5789341CurrentTrain: epoch 15, batch     9 | loss: 9.1998461CurrentTrain: epoch 15, batch    10 | loss: 12.6637687CurrentTrain: epoch 15, batch    11 | loss: 13.0889696CurrentTrain: epoch 15, batch    12 | loss: 20.6358131CurrentTrain: epoch 15, batch    13 | loss: 9.2369795CurrentTrain: epoch 15, batch    14 | loss: 15.1074470CurrentTrain: epoch 15, batch    15 | loss: 18.5858653CurrentTrain: epoch 15, batch    16 | loss: 11.4289361CurrentTrain: epoch 15, batch    17 | loss: 25.1654034CurrentTrain: epoch 15, batch    18 | loss: 12.2939001CurrentTrain: epoch 15, batch    19 | loss: 10.8198875CurrentTrain: epoch 15, batch    20 | loss: 10.1597593CurrentTrain: epoch 15, batch    21 | loss: 13.2635899CurrentTrain: epoch 15, batch    22 | loss: 14.3526351CurrentTrain: epoch 15, batch    23 | loss: 9.1957459CurrentTrain: epoch 15, batch    24 | loss: 11.2518309CurrentTrain: epoch 15, batch    25 | loss: 11.1252169CurrentTrain: epoch 15, batch    26 | loss: 12.1741660CurrentTrain: epoch 15, batch    27 | loss: 15.5379578CurrentTrain: epoch 15, batch    28 | loss: 11.4639529CurrentTrain: epoch 15, batch    29 | loss: 11.1462573CurrentTrain: epoch 15, batch    30 | loss: 11.6130276CurrentTrain: epoch 15, batch    31 | loss: 20.7435180CurrentTrain: epoch 15, batch    32 | loss: 10.9649254CurrentTrain: epoch 15, batch    33 | loss: 13.3688492CurrentTrain: epoch 15, batch    34 | loss: 15.4502202CurrentTrain: epoch 15, batch    35 | loss: 19.0372739CurrentTrain: epoch 15, batch    36 | loss: 10.1942632CurrentTrain: epoch 15, batch    37 | loss: 16.0043677CurrentTrain: epoch 15, batch    38 | loss: 15.6007360CurrentTrain: epoch 15, batch    39 | loss: 23.6643007CurrentTrain: epoch 15, batch    40 | loss: 10.5997409CurrentTrain: epoch 15, batch    41 | loss: 17.5141941CurrentTrain: epoch 15, batch    42 | loss: 12.6019631CurrentTrain: epoch 15, batch    43 | loss: 17.2623091CurrentTrain: epoch 15, batch    44 | loss: 25.4514773CurrentTrain: epoch 15, batch    45 | loss: 21.2864871CurrentTrain: epoch 15, batch    46 | loss: 13.9160913CurrentTrain: epoch 15, batch    47 | loss: 14.5758951CurrentTrain: epoch 15, batch    48 | loss: 10.4994432CurrentTrain: epoch 15, batch    49 | loss: 17.6377417CurrentTrain: epoch 15, batch    50 | loss: 10.9369490CurrentTrain: epoch 15, batch    51 | loss: 14.3315164CurrentTrain: epoch 15, batch    52 | loss: 12.9546852CurrentTrain: epoch 15, batch    53 | loss: 12.6640367CurrentTrain: epoch 15, batch    54 | loss: 16.3715284CurrentTrain: epoch 15, batch    55 | loss: 10.5430684CurrentTrain: epoch 15, batch    56 | loss: 16.9936687CurrentTrain: epoch 15, batch    57 | loss: 16.9129983CurrentTrain: epoch 15, batch    58 | loss: 14.8346109CurrentTrain: epoch 15, batch    59 | loss: 10.7611951CurrentTrain: epoch 15, batch    60 | loss: 14.6075241CurrentTrain: epoch 15, batch    61 | loss: 20.2787985CurrentTrain: epoch  7, batch    62 | loss: 6.9761535CurrentTrain: epoch 15, batch     0 | loss: 9.6063634CurrentTrain: epoch 15, batch     1 | loss: 13.0803697CurrentTrain: epoch 15, batch     2 | loss: 18.2893176CurrentTrain: epoch 15, batch     3 | loss: 11.1849277CurrentTrain: epoch 15, batch     4 | loss: 10.8250303CurrentTrain: epoch 15, batch     5 | loss: 19.5147756CurrentTrain: epoch 15, batch     6 | loss: 12.2285807CurrentTrain: epoch 15, batch     7 | loss: 11.1823424CurrentTrain: epoch 15, batch     8 | loss: 8.8880270CurrentTrain: epoch 15, batch     9 | loss: 10.2969599CurrentTrain: epoch 15, batch    10 | loss: 11.7775506CurrentTrain: epoch 15, batch    11 | loss: 14.5420261CurrentTrain: epoch 15, batch    12 | loss: 12.3777914CurrentTrain: epoch 15, batch    13 | loss: 9.9622911CurrentTrain: epoch 15, batch    14 | loss: 16.1864718CurrentTrain: epoch 15, batch    15 | loss: 25.1998621CurrentTrain: epoch 15, batch    16 | loss: 14.2198974CurrentTrain: epoch 15, batch    17 | loss: 10.6521918CurrentTrain: epoch 15, batch    18 | loss: 11.6101480CurrentTrain: epoch 15, batch    19 | loss: 14.3982022CurrentTrain: epoch 15, batch    20 | loss: 10.1048813CurrentTrain: epoch 15, batch    21 | loss: 12.1298070CurrentTrain: epoch 15, batch    22 | loss: 14.3981603CurrentTrain: epoch 15, batch    23 | loss: 19.9920351CurrentTrain: epoch 15, batch    24 | loss: 22.8306342CurrentTrain: epoch 15, batch    25 | loss: 16.1086699CurrentTrain: epoch 15, batch    26 | loss: 14.4208643CurrentTrain: epoch 15, batch    27 | loss: 10.4179331CurrentTrain: epoch 15, batch    28 | loss: 16.9134233CurrentTrain: epoch 15, batch    29 | loss: 22.4245199CurrentTrain: epoch 15, batch    30 | loss: 13.0202002CurrentTrain: epoch 15, batch    31 | loss: 13.7819091CurrentTrain: epoch 15, batch    32 | loss: 12.1409458CurrentTrain: epoch 15, batch    33 | loss: 14.3308228CurrentTrain: epoch 15, batch    34 | loss: 14.9983066CurrentTrain: epoch 15, batch    35 | loss: 10.9064256CurrentTrain: epoch 15, batch    36 | loss: 10.2706182CurrentTrain: epoch 15, batch    37 | loss: 15.5578748CurrentTrain: epoch 15, batch    38 | loss: 12.4200044CurrentTrain: epoch 15, batch    39 | loss: 11.9041097CurrentTrain: epoch 15, batch    40 | loss: 11.3227966CurrentTrain: epoch 15, batch    41 | loss: 11.1628085CurrentTrain: epoch 15, batch    42 | loss: 12.2712743CurrentTrain: epoch 15, batch    43 | loss: 20.5292052CurrentTrain: epoch 15, batch    44 | loss: 10.3442484CurrentTrain: epoch 15, batch    45 | loss: 13.8026253CurrentTrain: epoch 15, batch    46 | loss: 17.4726210CurrentTrain: epoch 15, batch    47 | loss: 16.4004964CurrentTrain: epoch 15, batch    48 | loss: 18.8107895CurrentTrain: epoch 15, batch    49 | loss: 11.6945049CurrentTrain: epoch 15, batch    50 | loss: 14.0031343CurrentTrain: epoch 15, batch    51 | loss: 10.7828559CurrentTrain: epoch 15, batch    52 | loss: 8.7196425CurrentTrain: epoch 15, batch    53 | loss: 8.8092108CurrentTrain: epoch 15, batch    54 | loss: 12.4335990CurrentTrain: epoch 15, batch    55 | loss: 10.6365073CurrentTrain: epoch 15, batch    56 | loss: 12.0798439CurrentTrain: epoch 15, batch    57 | loss: 22.4692843CurrentTrain: epoch 15, batch    58 | loss: 18.8722368CurrentTrain: epoch 15, batch    59 | loss: 9.7602351CurrentTrain: epoch 15, batch    60 | loss: 12.4447458CurrentTrain: epoch 15, batch    61 | loss: 11.3958704CurrentTrain: epoch  7, batch    62 | loss: 6.9521780CurrentTrain: epoch 15, batch     0 | loss: 13.3463831CurrentTrain: epoch 15, batch     1 | loss: 10.9199538CurrentTrain: epoch 15, batch     2 | loss: 12.3485221CurrentTrain: epoch 15, batch     3 | loss: 12.2622890CurrentTrain: epoch 15, batch     4 | loss: 25.1712007CurrentTrain: epoch 15, batch     5 | loss: 16.4594195CurrentTrain: epoch 15, batch     6 | loss: 14.5005041CurrentTrain: epoch 15, batch     7 | loss: 7.2400576CurrentTrain: epoch 15, batch     8 | loss: 17.9702577CurrentTrain: epoch 15, batch     9 | loss: 13.3395980CurrentTrain: epoch 15, batch    10 | loss: 9.3775075CurrentTrain: epoch 15, batch    11 | loss: 8.6051494CurrentTrain: epoch 15, batch    12 | loss: 6.5361392CurrentTrain: epoch 15, batch    13 | loss: 9.5891074CurrentTrain: epoch 15, batch    14 | loss: 10.3660114CurrentTrain: epoch 15, batch    15 | loss: 10.8439618CurrentTrain: epoch 15, batch    16 | loss: 21.1695632CurrentTrain: epoch 15, batch    17 | loss: 10.9920864CurrentTrain: epoch 15, batch    18 | loss: 11.5124300CurrentTrain: epoch 15, batch    19 | loss: 13.7304899CurrentTrain: epoch 15, batch    20 | loss: 48.5499596CurrentTrain: epoch 15, batch    21 | loss: 13.0467551CurrentTrain: epoch 15, batch    22 | loss: 7.8202313CurrentTrain: epoch 15, batch    23 | loss: 18.9856691CurrentTrain: epoch 15, batch    24 | loss: 29.3048396CurrentTrain: epoch 15, batch    25 | loss: 9.5828189CurrentTrain: epoch 15, batch    26 | loss: 10.5792282CurrentTrain: epoch 15, batch    27 | loss: 15.5640076CurrentTrain: epoch 15, batch    28 | loss: 12.5073179CurrentTrain: epoch 15, batch    29 | loss: 8.7712903CurrentTrain: epoch 15, batch    30 | loss: 12.8868789CurrentTrain: epoch 15, batch    31 | loss: 10.3018103CurrentTrain: epoch 15, batch    32 | loss: 9.5906106CurrentTrain: epoch 15, batch    33 | loss: 17.6830801CurrentTrain: epoch 15, batch    34 | loss: 13.1143256CurrentTrain: epoch 15, batch    35 | loss: 16.1945400CurrentTrain: epoch 15, batch    36 | loss: 13.0368095CurrentTrain: epoch 15, batch    37 | loss: 10.4899952CurrentTrain: epoch 15, batch    38 | loss: 16.2158443CurrentTrain: epoch 15, batch    39 | loss: 13.2197343CurrentTrain: epoch 15, batch    40 | loss: 11.9383131CurrentTrain: epoch 15, batch    41 | loss: 17.4270653CurrentTrain: epoch 15, batch    42 | loss: 9.0002886CurrentTrain: epoch 15, batch    43 | loss: 10.9918318CurrentTrain: epoch 15, batch    44 | loss: 7.9253318CurrentTrain: epoch 15, batch    45 | loss: 10.2224084CurrentTrain: epoch 15, batch    46 | loss: 21.7138958CurrentTrain: epoch 15, batch    47 | loss: 9.4038007CurrentTrain: epoch 15, batch    48 | loss: 9.0950079CurrentTrain: epoch 15, batch    49 | loss: 10.0647134CurrentTrain: epoch 15, batch    50 | loss: 17.2004319CurrentTrain: epoch 15, batch    51 | loss: 14.4134320CurrentTrain: epoch 15, batch    52 | loss: 14.7404350CurrentTrain: epoch 15, batch    53 | loss: 13.3161829CurrentTrain: epoch 15, batch    54 | loss: 16.0311403CurrentTrain: epoch 15, batch    55 | loss: 10.5926186CurrentTrain: epoch 15, batch    56 | loss: 12.0137186CurrentTrain: epoch 15, batch    57 | loss: 10.2354281CurrentTrain: epoch 15, batch    58 | loss: 12.0874465CurrentTrain: epoch 15, batch    59 | loss: 20.9787477CurrentTrain: epoch 15, batch    60 | loss: 14.9339152CurrentTrain: epoch 15, batch    61 | loss: 28.6288135CurrentTrain: epoch  7, batch    62 | loss: 6.8450426
#### movement ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "movement" means literary, artistic, scientific or philosophical movement or scene associated with this person or work.
Here is an example:
Relation: movement
Context: the large workshops of peter paul rubens and jacob jordaens , and the influence of anthony van dyck , made antwerp the center of the flemish baroque .
Head Entity: jacob jordaens
Tail Entity: flemish baroque
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "movement":
Relation: movement  
Context: The emergence of surrealism in the early 20th century was significantly influenced by the works of Salvador Dalí and André Breton, who sought to challenge conventional perceptions of reality.  
Head Entity: Salvador Dalí  
Tail Entity: surrealism  

Relation: movement  
Context: The rise of the Impressionist movement in France was marked by the innovative techniques of artists like Claude Monet and Pierre-Auguste Renoir, who focused on capturing light and its changing qualities.  
Head Entity: Claude Monet  
Tail Entity: Impressionist movement  
#### located in the administrative territorial entity ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "located in the administrative territorial entity" means the item is located on the territory of the following administrative entity.
Here is an example:
Relation: located in the administrative territorial entity
Context: lyons hill is a restored village , and former parish with church , now part of the community of ardclough in north county kildare .
Head Entity: ardclough
Tail Entity: county kildare
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "located in the administrative territorial entity":
Relation: located in the administrative territorial entity  
Context: The city of San Francisco is known for its iconic Golden Gate Bridge and is part of the larger San Francisco County in California.  
Head Entity: San Francisco  
Tail Entity: San Francisco County  

Relation: located in the administrative territorial entity  
Context: The village of Haverfordwest serves as the county town of Pembrokeshire, situated in the picturesque region of Wales.  
Head Entity: Haverfordwest  
Tail Entity: Pembrokeshire  
#### head of government ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "head of government" means head of the executive power of this town, city, municipality, state, country, or other governmental body.
Here is an example:
Relation: head of government
Context: on february 10 , 2010 , stefan mappus was elected minister - president of baden - württemberg , and asked that all ministers continue their roles .
Head Entity: baden - württemberg
Tail Entity: stefan mappus
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "head of government":
Relation: head of government  
Context: in 2021, jacinda ardern was re-elected as the prime minister of new zealand, leading the labor party to a significant victory in the general elections.  
Head Entity: new zealand  
Tail Entity: jacinda ardern  

Relation: head of government  
Context: during the annual summit, justin trudeau addressed the nation as the prime minister of canada, outlining his government's plans for the upcoming year.  
Head Entity: canada  
Tail Entity: justin trudeau  
#### member of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "member of" means organization, club or musical group to which the subject belongs.
Here is an example:
Relation: member of
Context: hank henshaw became a member of the sinestro corps during the sinestro corps war .
Head Entity: hank henshaw
Tail Entity: sinestro corps
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "member of":
Relation: member of  
Context: after years of dedication, she was finally recognized as a member of the national ballet company.  
Head Entity: she  
Tail Entity: national ballet company  

Relation: member of  
Context: the renowned scientist was proud to be a member of the international academy of sciences.  
Head Entity: the renowned scientist  
Tail Entity: international academy of sciences  
#### crosses ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "crosses" means obstacle which this bridge crosses over or this tunnel goes under.
Here is an example:
Relation: crosses
Context: on july 2 , 1935 , the rip van winkle bridge over the hudson river between catskill and greenport was opened to traffic .
Head Entity: rip van winkle bridge
Tail Entity: hudson river
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "crosses":
Relation: crosses  
Context: The Golden Gate Bridge spans the Golden Gate Strait, connecting San Francisco to Marin County and allowing vehicles to cross over the waterway.  
Head Entity: Golden Gate Bridge  
Tail Entity: Golden Gate Strait  

Relation: crosses  
Context: The Channel Tunnel, also known as the Chunnel, runs beneath the English Channel, providing a rail link between the UK and mainland Europe.  
Head Entity: Channel Tunnel  
Tail Entity: English Channel  
#### tributary ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "tributary" means watercourse that flows into an other one.
Here is an example:
Relation: tributary
Context: it is on the panj river just downstream from the mouth of the bartang river at the point where the panj briefly turns west before resuming its northerly course .
Head Entity: panj river
Tail Entity: bartang river
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "tributary":
Relation: tributary  
Context: The Missouri River, which is a major tributary of the Mississippi River, flows through several states before joining the larger watercourse.  
Head Entity: Missouri River  
Tail Entity: Mississippi River  

Relation: tributary  
Context: The Rio Grande serves as a natural border and has several tributaries, including the Pecos River, which contributes to its flow.  
Head Entity: Pecos River  
Tail Entity: Rio Grande  
#### position held ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "position held" means subject currently or formerly holds the object position or public office.
Here is an example:
Relation: position held
Context: in 1973 , the college was again hit by financial difficulties , but a direct appeal for assistance to the prime minister , norman kirk , secured te aute 's future .
Head Entity: norman kirk
Tail Entity: prime minister
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "position held":
Relation: position held  
Context: After serving as the mayor for over a decade, Sarah Thompson announced her retirement, paving the way for new leadership in the city council.  
Head Entity: Sarah Thompson  
Tail Entity: mayor  

Relation: position held  
Context: During his tenure as the Secretary of State, John Smith implemented several key policies that transformed the department's approach to education.  
Head Entity: John Smith  
Tail Entity: Secretary of State  
#### developer ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "developer" means organization or person that developed the item.
Here is an example:
Relation: developer
Context: unlike its predecessor , " torchlight ii " features cinematic sequences , which are produced by klei entertainment , the developer of " do n't starve " , " eets " and " shank " .
Head Entity: shank
Tail Entity: klei entertainment
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "developer":
Relation: developer  
Context: The popular game "Celeste" was created by Maddy Makes Games, the developer known for its engaging gameplay and heartfelt storytelling.  
Head Entity: Celeste  
Tail Entity: Maddy Makes Games  

Relation: developer  
Context: "The Witcher 3: Wild Hunt" was developed by CD Projekt Red, a studio that has gained acclaim for its immersive open-world experiences.  
Head Entity: The Witcher 3: Wild Hunt  
Tail Entity: CD Projekt Red  
#### location of formation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "location of formation" means location where a group or organization was formed.
Here is an example:
Relation: location of formation
Context: holy molar are a san diego based band formed in 2001 .
Head Entity: holy molar
Tail Entity: san diego
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "location of formation":
Relation: location of formation  
Context: the tech startup was established in silicon valley in 2015.  
Head Entity: the tech startup  
Tail Entity: silicon valley  

Relation: location of formation  
Context: the famous rock band was formed in new york city during the summer of 1999.  
Head Entity: the famous rock band  
Tail Entity: new york city  
#### country of origin ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "country of origin" means country of origin of this item (creative work, food, phrase, product, etc.).
Here is an example:
Relation: country of origin
Context: she has appeared in some american television shows , beginning with her regular role as snow white 's stepmother , evil queen lillian " lily " white in the series " the charmings " .
Head Entity: the charmings
Tail Entity: american
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "country of origin":
Relation: country of origin  
Context: The famous dish sushi is traditionally made with vinegared rice and often includes seafood, vegetables, and occasionally tropical fruits, originating from Japan.  
Head Entity: sushi  
Tail Entity: Japan  

Relation: country of origin  
Context: The iconic painting "Mona Lisa," known for its captivating smile, was created by the renowned artist Leonardo da Vinci in Italy during the Renaissance period.  
Head Entity: Mona Lisa  
Tail Entity: Italy  
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 88.54%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 89.29%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 89.84%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 90.97%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 91.25%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 90.91%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 91.67%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 91.35%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 91.52%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 92.19%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 92.65%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 92.71%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 93.09%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 93.44%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 93.47%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 94.01%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 93.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 93.99%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 93.98%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 93.97%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 93.97%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 94.17%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 94.35%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 94.53%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 94.70%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 94.85%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 94.82%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 94.79%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 94.93%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 94.90%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 95.03%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 95.16%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 95.27%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 95.39%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 95.49%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 95.60%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 95.69%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 95.79%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 95.74%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 95.70%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 95.79%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 95.88%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 95.83%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 95.67%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 95.64%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 95.72%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 95.45%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 95.42%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 95.50%   [EVAL] batch:   57 | acc: 100.00%,  total acc: 95.58%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 95.66%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 95.62%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 95.70%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 95.67%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 94.94%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 88.54%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 89.29%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 89.84%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 90.97%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 91.25%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 90.91%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 91.67%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 91.35%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 91.52%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 92.19%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 92.65%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 92.71%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 93.09%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 93.44%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 93.47%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 94.01%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 93.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 93.99%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 93.98%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 93.97%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 93.97%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 94.17%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 94.35%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 94.53%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 94.70%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 94.85%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 94.82%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 94.79%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 94.93%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 94.90%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 95.03%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 95.16%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 95.27%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 95.39%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 95.49%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 95.60%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 95.69%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 95.79%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 95.74%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 95.70%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 95.79%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 95.88%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 95.83%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 95.67%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 95.64%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 95.72%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 95.45%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 95.42%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 95.50%   [EVAL] batch:   57 | acc: 100.00%,  total acc: 95.58%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 95.66%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 95.62%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 95.70%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 95.67%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 94.94%   
cur_acc:  ['0.9494']
his_acc:  ['0.9494']
CurrentTrain: epoch 15, batch     0 | loss: 30.4242753CurrentTrain: epoch 15, batch     1 | loss: 23.9182230CurrentTrain: epoch 15, batch     2 | loss: 23.4454292CurrentTrain: epoch  1, batch     3 | loss: 10.7189333CurrentTrain: epoch 15, batch     0 | loss: 19.0560455CurrentTrain: epoch 15, batch     1 | loss: 20.1160082CurrentTrain: epoch 15, batch     2 | loss: 18.9610301CurrentTrain: epoch  1, batch     3 | loss: 9.2473833CurrentTrain: epoch 15, batch     0 | loss: 18.0944214CurrentTrain: epoch 15, batch     1 | loss: 19.2764500CurrentTrain: epoch 15, batch     2 | loss: 19.6216331CurrentTrain: epoch  1, batch     3 | loss: 10.7566103CurrentTrain: epoch 15, batch     0 | loss: 26.3235092CurrentTrain: epoch 15, batch     1 | loss: 12.4133595CurrentTrain: epoch 15, batch     2 | loss: 21.5453375CurrentTrain: epoch  1, batch     3 | loss: 10.3209510CurrentTrain: epoch 15, batch     0 | loss: 15.6860523CurrentTrain: epoch 15, batch     1 | loss: 18.5456566CurrentTrain: epoch 15, batch     2 | loss: 9.8543086CurrentTrain: epoch  1, batch     3 | loss: 7.7398656CurrentTrain: epoch 15, batch     0 | loss: 15.8169626CurrentTrain: epoch 15, batch     1 | loss: 13.4827529CurrentTrain: epoch 15, batch     2 | loss: 12.9073596CurrentTrain: epoch  1, batch     3 | loss: 8.5730668CurrentTrain: epoch 15, batch     0 | loss: 15.1992817CurrentTrain: epoch 15, batch     1 | loss: 19.2695412CurrentTrain: epoch 15, batch     2 | loss: 14.3198751CurrentTrain: epoch  1, batch     3 | loss: 15.5124231CurrentTrain: epoch 15, batch     0 | loss: 17.2215182CurrentTrain: epoch 15, batch     1 | loss: 14.6417105CurrentTrain: epoch 15, batch     2 | loss: 10.5940867CurrentTrain: epoch  1, batch     3 | loss: 6.6722794CurrentTrain: epoch 15, batch     0 | loss: 9.0896471CurrentTrain: epoch 15, batch     1 | loss: 14.5946850CurrentTrain: epoch 15, batch     2 | loss: 10.3192984CurrentTrain: epoch  1, batch     3 | loss: 7.9919472CurrentTrain: epoch 15, batch     0 | loss: 21.8986521CurrentTrain: epoch 15, batch     1 | loss: 13.2943253CurrentTrain: epoch 15, batch     2 | loss: 16.5408362CurrentTrain: epoch  1, batch     3 | loss: 7.0594882
#### located on terrain feature ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "located on terrain feature" means located on the specified physical/geophysical feature.
Here is an example:
Relation: located on terrain feature
Context: all of mainland australia 's alpine areas , including its highest mountain , mount kosciuszko ( ahd ) , are part of this range , called the main range .
Head Entity: mount kosciuszko
Tail Entity: main range
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "located on terrain feature":
Relation: located on terrain feature  
Context: The Great Barrier Reef, which is the world's largest coral reef system, is situated off the coast of Queensland, Australia, and is a UNESCO World Heritage site.  
Head Entity: Great Barrier Reef  
Tail Entity: Queensland  

Relation: located on terrain feature  
Context: The Rocky Mountains stretch from Canada down to New Mexico, providing a stunning backdrop for outdoor activities and wildlife.  
Head Entity: Rocky Mountains  
Tail Entity: New Mexico  
#### participant of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "participant of" means event in which a person or organization was/is a participant.
Here is an example:
Relation: participant of
Context: tiago venâncio ( born august 19 , 1987 , setúbal ) is a freestyle swimmer from portugal , who represented his home country at the 2004 summer olympics in athens , greece .
Head Entity: tiago venâncio
Tail Entity: 2004 summer olympics
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "participant of":
Relation: participant of  
Context: the united nations climate change conference, also known as cop26, took place in glasgow, scotland, where numerous world leaders gathered to discuss climate action.  
Head Entity: united nations climate change conference  
Tail Entity: cop26  

Relation: participant of  
Context: elon musk, the ceo of spacex, was a key participant in the 2021 met gala, showcasing his vision for sustainable energy and space exploration.  
Head Entity: elon musk  
Tail Entity: 2021 met gala  
#### director ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "director" means director of film, TV-series, stageplay, video game or similar.
Here is an example:
Relation: director
Context: luna subsequently had a supporting role in the comedy film " gravy " , directed by james roday , which was released in october 2015 .
Head Entity: gravy
Tail Entity: james roday
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "director":
Relation: director  
Context: the acclaimed director christopher nolan is known for his work on the blockbuster film "inception," which was released in 2010.  
Head Entity: inception  
Tail Entity: christopher nolan  

Relation: director  
Context: in the recent documentary "the last dance," directed by jason hehir, viewers get an inside look at michael jordan's career.  
Head Entity: the last dance  
Tail Entity: jason hehir  
#### owned by ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "owned by" means owner of the subject.
Here is an example:
Relation: owned by
Context: as of april 2013 , sony isp , known as so - net , released a new fibre service to tokyo for small businesses and homes and also made it available to six surrounding prefectures .
Head Entity: so - net
Tail Entity: sony
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "owned by":
Relation: owned by  
Context: In 2020, the popular video game franchise Call of Duty was acquired by Activision Blizzard, which has been a major player in the gaming industry for years.  
Head Entity: Call of Duty  
Tail Entity: Activision Blizzard  

Relation: owned by  
Context: The iconic fashion brand Gucci is a subsidiary of the French luxury group Kering, which also owns several other high-end labels.  
Head Entity: Gucci  
Tail Entity: Kering  
#### part of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "part of" means object of which the subject is a part.
Here is an example:
Relation: part of
Context: thessaly was home to extensive neolithic and chalcolithic cultures around 6000 bc–2500 bc ( see cardium pottery , dimini and sesklo ) .
Head Entity: cardium pottery
Tail Entity: neolithic
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "part of":
Relation: part of  
Context: The engine is a crucial part of the car's overall functionality, providing the necessary power to drive.  
Head Entity: engine  
Tail Entity: car  

Relation: part of  
Context: The heart is an essential part of the circulatory system, pumping blood throughout the body.  
Head Entity: heart  
Tail Entity: circulatory system  
#### occupant ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "occupant" means person or organization occupying property.
Here is an example:
Relation: occupant
Context: it served as the home of k league side gyeongnam fc and national league side changwon city fc prior to the construction of the changwon football center in 2009 .
Head Entity: changwon football center
Tail Entity: gyeongnam fc
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "occupant":
Relation: occupant  
Context: The historic building was once the residence of the famous artist, and now it serves as a gallery showcasing his works, attracting visitors from all over the world.  
Head Entity: historic building  
Tail Entity: famous artist  

Relation: occupant  
Context: After the renovation, the old factory was transformed into a vibrant co-working space, where numerous startups and freelancers now thrive.  
Head Entity: old factory  
Tail Entity: startups  
#### architect ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "architect" means person or architectural firm responsible for designing this building.
Here is an example:
Relation: architect
Context: the art nouveau style was brought to italy by figures such as giuseppe sommaruga and ernesto basile ( who respectively designed the palazzo castiglioni and expanded the palazzo montecitorio in rome ) .
Head Entity: palazzo castiglioni
Tail Entity: giuseppe sommaruga
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "architect":
Relation: architect  
Context: The iconic Sydney Opera House was designed by Danish architect Jørn Utzon, who won the design competition in 1957.  
Head Entity: Sydney Opera House  
Tail Entity: Jørn Utzon  

Relation: architect  
Context: Frank Lloyd Wright is renowned for his innovative designs, including the famous Fallingwater house, which seamlessly integrates with its natural surroundings.  
Head Entity: Fallingwater  
Tail Entity: Frank Lloyd Wright  
#### residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "residence" means the place where the person is or has been, resident.
Here is an example:
Relation: residence
Context: cyrus alexander was born in pennsylvania , and his family soon moved to illinois .
Head Entity: cyrus alexander
Tail Entity: illinois
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "residence":
Relation: residence  
Context: after living in new york for several years, emily decided to relocate to california for a change of scenery.  
Head Entity: emily  
Tail Entity: california  

Relation: residence  
Context: during his childhood, michael spent most of his time in texas before moving to florida for college.  
Head Entity: michael  
Tail Entity: florida  
#### composer ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "composer" means person who wrote the music.
Here is an example:
Relation: composer
Context: he also took part in recordings of several gilbert and sullivan operattas as well as edward german 's " merrie england " under the direction of joe batten .
Head Entity: merrie england
Tail Entity: edward german
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "composer":
Relation: composer  
Context: Ludwig van Beethoven is widely regarded as one of the greatest composers in the history of Western music, known for his symphonies and sonatas.  
Head Entity: Beethoven  
Tail Entity: symphonies  

Relation: composer  
Context: The famous opera "Carmen" was composed by Georges Bizet, who infused the work with rich melodies and dramatic flair.  
Head Entity: Carmen  
Tail Entity: Georges Bizet  
#### location ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "location" means location of the object, structure or event.
Here is an example:
Relation: location
Context: one polish archer qualified for the women 's individual recurve by obtaining one of the eight olympic places available from the 2015 world archery championships in copenhagen , denmark .
Head Entity: 2015 world archery championships
Tail Entity: copenhagen
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "location":
Relation: location  
Context: The annual music festival will take place in the vibrant city of Austin, Texas, attracting thousands of fans from around the world.  
Head Entity: annual music festival  
Tail Entity: Austin, Texas  

Relation: location  
Context: The historic battle of Gettysburg was fought in the small town of Gettysburg, Pennsylvania, which is now a popular tourist destination.  
Head Entity: battle of Gettysburg  
Tail Entity: Gettysburg, Pennsylvania  
MemoryTrain:  epoch 15, batch     0 | loss: 11.4154116MemoryTrain:  epoch 15, batch     1 | loss: 7.3565741MemoryTrain:  epoch 15, batch     2 | loss: 8.1512784MemoryTrain:  epoch 11, batch     3 | loss: 9.7753291MemoryTrain:  epoch 15, batch     0 | loss: 8.4605007MemoryTrain:  epoch 15, batch     1 | loss: 8.6062128MemoryTrain:  epoch 15, batch     2 | loss: 5.9119155MemoryTrain:  epoch 11, batch     3 | loss: 5.0714855MemoryTrain:  epoch 15, batch     0 | loss: 7.2488567MemoryTrain:  epoch 15, batch     1 | loss: 5.3815662MemoryTrain:  epoch 15, batch     2 | loss: 6.4544437MemoryTrain:  epoch 11, batch     3 | loss: 5.6787265MemoryTrain:  epoch 15, batch     0 | loss: 5.7355370MemoryTrain:  epoch 15, batch     1 | loss: 7.1703406MemoryTrain:  epoch 15, batch     2 | loss: 3.7394857MemoryTrain:  epoch 11, batch     3 | loss: 3.6467119MemoryTrain:  epoch 15, batch     0 | loss: 5.1049908MemoryTrain:  epoch 15, batch     1 | loss: 9.2650267MemoryTrain:  epoch 15, batch     2 | loss: 6.9533892MemoryTrain:  epoch 11, batch     3 | loss: 4.3048692MemoryTrain:  epoch 15, batch     0 | loss: 5.0921439MemoryTrain:  epoch 15, batch     1 | loss: 4.3030316MemoryTrain:  epoch 15, batch     2 | loss: 4.4593888MemoryTrain:  epoch 11, batch     3 | loss: 2.7475881MemoryTrain:  epoch 15, batch     0 | loss: 7.0478622MemoryTrain:  epoch 15, batch     1 | loss: 5.1240178MemoryTrain:  epoch 15, batch     2 | loss: 4.1317211MemoryTrain:  epoch 11, batch     3 | loss: 2.2423250MemoryTrain:  epoch 15, batch     0 | loss: 2.7410553MemoryTrain:  epoch 15, batch     1 | loss: 2.9695547MemoryTrain:  epoch 15, batch     2 | loss: 5.7706402MemoryTrain:  epoch 11, batch     3 | loss: 4.6147138MemoryTrain:  epoch 15, batch     0 | loss: 6.7261395MemoryTrain:  epoch 15, batch     1 | loss: 2.7185043MemoryTrain:  epoch 15, batch     2 | loss: 3.6103415MemoryTrain:  epoch 11, batch     3 | loss: 4.7212885MemoryTrain:  epoch 15, batch     0 | loss: 2.6548409MemoryTrain:  epoch 15, batch     1 | loss: 1.9339917MemoryTrain:  epoch 15, batch     2 | loss: 2.9873692MemoryTrain:  epoch 11, batch     3 | loss: 3.3439596
[EVAL] batch:    0 | acc: 25.00%,  total acc: 25.00%   [EVAL] batch:    1 | acc: 31.25%,  total acc: 28.12%   [EVAL] batch:    2 | acc: 37.50%,  total acc: 31.25%   [EVAL] batch:    3 | acc: 56.25%,  total acc: 37.50%   [EVAL] batch:    4 | acc: 62.50%,  total acc: 42.50%   [EVAL] batch:    5 | acc: 50.00%,  total acc: 43.75%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 50.00%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 54.69%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 59.72%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 63.75%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 66.48%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 68.75%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 70.67%   [EVAL] batch:   13 | acc: 81.25%,  total acc: 71.43%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 72.92%   [EVAL] batch:   15 | acc: 93.75%,  total acc: 74.22%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 75.37%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 76.39%   [EVAL] batch:   18 | acc: 87.50%,  total acc: 76.97%   [EVAL] batch:   19 | acc: 68.75%,  total acc: 76.56%   [EVAL] batch:   20 | acc: 75.00%,  total acc: 76.49%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 76.99%   [EVAL] batch:   22 | acc: 81.25%,  total acc: 77.17%   [EVAL] batch:   23 | acc: 62.50%,  total acc: 76.56%   [EVAL] batch:   24 | acc: 68.75%,  total acc: 76.25%   [EVAL] batch:   25 | acc: 31.25%,  total acc: 74.52%   [EVAL] batch:   26 | acc: 43.75%,  total acc: 73.38%   [EVAL] batch:   27 | acc: 25.00%,  total acc: 71.65%   [EVAL] batch:   28 | acc: 25.00%,  total acc: 70.04%   [EVAL] batch:   29 | acc: 37.50%,  total acc: 68.96%   [EVAL] batch:   30 | acc: 18.75%,  total acc: 67.34%   [EVAL] batch:   31 | acc: 75.00%,  total acc: 67.58%   [EVAL] batch:   32 | acc: 87.50%,  total acc: 68.18%   [EVAL] batch:   33 | acc: 87.50%,  total acc: 68.75%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 69.64%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 70.14%   [EVAL] batch:   36 | acc: 87.50%,  total acc: 70.61%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 71.22%   [EVAL] batch:   38 | acc: 87.50%,  total acc: 71.63%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 72.34%   [EVAL] batch:   40 | acc: 87.50%,  total acc: 72.71%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 73.07%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 73.55%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 74.15%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 74.44%   [EVAL] batch:   45 | acc: 75.00%,  total acc: 74.46%   [EVAL] batch:   46 | acc: 68.75%,  total acc: 74.34%   [EVAL] batch:   47 | acc: 81.25%,  total acc: 74.48%   [EVAL] batch:   48 | acc: 87.50%,  total acc: 74.74%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 75.00%   [EVAL] batch:   50 | acc: 81.25%,  total acc: 75.12%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 75.60%   [EVAL] batch:   52 | acc: 81.25%,  total acc: 75.71%   [EVAL] batch:   53 | acc: 93.75%,  total acc: 76.04%   [EVAL] batch:   54 | acc: 100.00%,  total acc: 76.48%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 76.67%   [EVAL] batch:   56 | acc: 56.25%,  total acc: 76.32%   [EVAL] batch:   57 | acc: 62.50%,  total acc: 76.08%   [EVAL] batch:   58 | acc: 50.00%,  total acc: 75.64%   [EVAL] batch:   59 | acc: 68.75%,  total acc: 75.52%   [EVAL] batch:   60 | acc: 50.00%,  total acc: 75.10%   [EVAL] batch:   61 | acc: 75.00%,  total acc: 75.10%   [EVAL] batch:   62 | acc: 25.00%,  total acc: 74.31%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 77.08%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 75.00%   [EVAL] batch:    4 | acc: 62.50%,  total acc: 72.50%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 73.96%   [EVAL] batch:    6 | acc: 62.50%,  total acc: 72.32%   [EVAL] batch:    7 | acc: 62.50%,  total acc: 71.09%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 70.83%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 70.62%   [EVAL] batch:   10 | acc: 37.50%,  total acc: 67.61%   [EVAL] batch:   11 | acc: 56.25%,  total acc: 66.67%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 66.83%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 68.75%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 70.42%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 72.27%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 73.90%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 75.00%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 75.99%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 77.19%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 78.27%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 78.98%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 79.62%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 80.21%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 80.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 81.94%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 82.59%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 82.97%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 83.54%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 84.07%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 84.57%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 85.04%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 85.48%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 85.71%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 86.11%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 86.49%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 86.84%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 87.18%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 87.80%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 88.10%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 88.23%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 88.49%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 88.61%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 88.86%   [EVAL] batch:   46 | acc: 87.50%,  total acc: 88.83%   [EVAL] batch:   47 | acc: 87.50%,  total acc: 88.80%   [EVAL] batch:   48 | acc: 93.75%,  total acc: 88.90%   [EVAL] batch:   49 | acc: 81.25%,  total acc: 88.75%   [EVAL] batch:   50 | acc: 87.50%,  total acc: 88.73%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 88.82%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 88.92%   [EVAL] batch:   53 | acc: 93.75%,  total acc: 89.00%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 88.98%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 89.06%   [EVAL] batch:   56 | acc: 87.50%,  total acc: 89.04%   [EVAL] batch:   57 | acc: 81.25%,  total acc: 88.90%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 89.09%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 89.27%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 89.45%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 89.62%   [EVAL] batch:   62 | acc: 62.50%,  total acc: 89.19%   [EVAL] batch:   63 | acc: 37.50%,  total acc: 88.38%   [EVAL] batch:   64 | acc: 25.00%,  total acc: 87.40%   [EVAL] batch:   65 | acc: 43.75%,  total acc: 86.74%   [EVAL] batch:   66 | acc: 62.50%,  total acc: 86.38%   [EVAL] batch:   67 | acc: 37.50%,  total acc: 85.66%   [EVAL] batch:   68 | acc: 81.25%,  total acc: 85.60%   [EVAL] batch:   69 | acc: 93.75%,  total acc: 85.71%   [EVAL] batch:   70 | acc: 93.75%,  total acc: 85.83%   [EVAL] batch:   71 | acc: 100.00%,  total acc: 86.02%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 86.22%   [EVAL] batch:   73 | acc: 87.50%,  total acc: 86.23%   [EVAL] batch:   74 | acc: 93.75%,  total acc: 86.33%   [EVAL] batch:   75 | acc: 93.75%,  total acc: 86.43%   [EVAL] batch:   76 | acc: 81.25%,  total acc: 86.36%   [EVAL] batch:   77 | acc: 93.75%,  total acc: 86.46%   [EVAL] batch:   78 | acc: 100.00%,  total acc: 86.63%   [EVAL] batch:   79 | acc: 93.75%,  total acc: 86.72%   [EVAL] batch:   80 | acc: 93.75%,  total acc: 86.81%   [EVAL] batch:   81 | acc: 68.75%,  total acc: 86.59%   [EVAL] batch:   82 | acc: 75.00%,  total acc: 86.45%   [EVAL] batch:   83 | acc: 81.25%,  total acc: 86.38%   [EVAL] batch:   84 | acc: 87.50%,  total acc: 86.40%   [EVAL] batch:   85 | acc: 68.75%,  total acc: 86.19%   [EVAL] batch:   86 | acc: 62.50%,  total acc: 85.92%   [EVAL] batch:   87 | acc: 50.00%,  total acc: 85.51%   [EVAL] batch:   88 | acc: 43.75%,  total acc: 85.04%   [EVAL] batch:   89 | acc: 37.50%,  total acc: 84.51%   [EVAL] batch:   90 | acc: 18.75%,  total acc: 83.79%   [EVAL] batch:   91 | acc: 37.50%,  total acc: 83.29%   [EVAL] batch:   92 | acc: 25.00%,  total acc: 82.66%   [EVAL] batch:   93 | acc: 37.50%,  total acc: 82.18%   [EVAL] batch:   94 | acc: 81.25%,  total acc: 82.17%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 82.36%   [EVAL] batch:   96 | acc: 87.50%,  total acc: 82.41%   [EVAL] batch:   97 | acc: 93.75%,  total acc: 82.53%   [EVAL] batch:   98 | acc: 93.75%,  total acc: 82.64%   [EVAL] batch:   99 | acc: 81.25%,  total acc: 82.62%   [EVAL] batch:  100 | acc: 93.75%,  total acc: 82.74%   [EVAL] batch:  101 | acc: 93.75%,  total acc: 82.84%   [EVAL] batch:  102 | acc: 87.50%,  total acc: 82.89%   [EVAL] batch:  103 | acc: 93.75%,  total acc: 82.99%   [EVAL] batch:  104 | acc: 93.75%,  total acc: 83.10%   [EVAL] batch:  105 | acc: 93.75%,  total acc: 83.20%   [EVAL] batch:  106 | acc: 87.50%,  total acc: 83.24%   [EVAL] batch:  107 | acc: 93.75%,  total acc: 83.33%   [EVAL] batch:  108 | acc: 62.50%,  total acc: 83.14%   [EVAL] batch:  109 | acc: 81.25%,  total acc: 83.12%   [EVAL] batch:  110 | acc: 81.25%,  total acc: 83.11%   [EVAL] batch:  111 | acc: 93.75%,  total acc: 83.20%   [EVAL] batch:  112 | acc: 87.50%,  total acc: 83.24%   [EVAL] batch:  113 | acc: 81.25%,  total acc: 83.22%   [EVAL] batch:  114 | acc: 93.75%,  total acc: 83.32%   [EVAL] batch:  115 | acc: 87.50%,  total acc: 83.35%   [EVAL] batch:  116 | acc: 93.75%,  total acc: 83.44%   [EVAL] batch:  117 | acc: 100.00%,  total acc: 83.58%   [EVAL] batch:  118 | acc: 75.00%,  total acc: 83.51%   [EVAL] batch:  119 | acc: 50.00%,  total acc: 83.23%   [EVAL] batch:  120 | acc: 56.25%,  total acc: 83.01%   [EVAL] batch:  121 | acc: 62.50%,  total acc: 82.84%   [EVAL] batch:  122 | acc: 56.25%,  total acc: 82.62%   [EVAL] batch:  123 | acc: 62.50%,  total acc: 82.46%   [EVAL] batch:  124 | acc: 62.50%,  total acc: 82.30%   
cur_acc:  ['0.9494', '0.7431']
his_acc:  ['0.9494', '0.8230']
CurrentTrain: epoch 15, batch     0 | loss: 15.2295060CurrentTrain: epoch 15, batch     1 | loss: 24.0312055CurrentTrain: epoch 15, batch     2 | loss: 31.9705490CurrentTrain: epoch  1, batch     3 | loss: 10.4418117CurrentTrain: epoch 15, batch     0 | loss: 18.2686335CurrentTrain: epoch 15, batch     1 | loss: 11.2136082CurrentTrain: epoch 15, batch     2 | loss: 11.8643828CurrentTrain: epoch  1, batch     3 | loss: 11.5131416CurrentTrain: epoch 15, batch     0 | loss: 15.9756867CurrentTrain: epoch 15, batch     1 | loss: 12.8342241CurrentTrain: epoch 15, batch     2 | loss: 9.1245143CurrentTrain: epoch  1, batch     3 | loss: 8.2054840CurrentTrain: epoch 15, batch     0 | loss: 8.7903905CurrentTrain: epoch 15, batch     1 | loss: 13.5812836CurrentTrain: epoch 15, batch     2 | loss: 10.4193004CurrentTrain: epoch  1, batch     3 | loss: 6.9854874CurrentTrain: epoch 15, batch     0 | loss: 15.4440201CurrentTrain: epoch 15, batch     1 | loss: 18.5790031CurrentTrain: epoch 15, batch     2 | loss: 13.1622700CurrentTrain: epoch  1, batch     3 | loss: 5.7206446CurrentTrain: epoch 15, batch     0 | loss: 13.5256289CurrentTrain: epoch 15, batch     1 | loss: 17.6629223CurrentTrain: epoch 15, batch     2 | loss: 7.8688410CurrentTrain: epoch  1, batch     3 | loss: 14.9746960CurrentTrain: epoch 15, batch     0 | loss: 15.1838691CurrentTrain: epoch 15, batch     1 | loss: 10.2550710CurrentTrain: epoch 15, batch     2 | loss: 8.2729701CurrentTrain: epoch  1, batch     3 | loss: 7.3452758CurrentTrain: epoch 15, batch     0 | loss: 9.7720459CurrentTrain: epoch 15, batch     1 | loss: 15.5199807CurrentTrain: epoch 15, batch     2 | loss: 10.2743256CurrentTrain: epoch  1, batch     3 | loss: 8.7383299CurrentTrain: epoch 15, batch     0 | loss: 8.1262845CurrentTrain: epoch 15, batch     1 | loss: 14.7303125CurrentTrain: epoch 15, batch     2 | loss: 8.2002225CurrentTrain: epoch  1, batch     3 | loss: 5.9447681CurrentTrain: epoch 15, batch     0 | loss: 10.2176575CurrentTrain: epoch 15, batch     1 | loss: 24.1084747CurrentTrain: epoch 15, batch     2 | loss: 7.6290778CurrentTrain: epoch  1, batch     3 | loss: 6.0189354
#### spouse ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "spouse" means the subject has the object as their spouse (husband, wife, partner, etc.).
Here is an example:
Relation: spouse
Context: rohan is the son of narayana murthy , founder of infosys , and his wife sudha murthy , an engineer and author of note .
Head Entity: narayana murthy
Tail Entity: sudha murthy
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "spouse":
Relation: spouse  
Context: elon musk, the CEO of spacex and tesla, was previously married to talulah riley, a talented actress and writer.  
Head Entity: elon musk  
Tail Entity: talulah riley  

Relation: spouse  
Context: barack obama, the 44th president of the united states, has been married to michelle obama since 1992, and they have two daughters together.  
Head Entity: barack obama  
Tail Entity: michelle obama  
#### record label ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "record label" means brand and trademark associated with the marketing of subject music recordings and music videos.
Here is an example:
Relation: record label
Context: darlene released , " darlene mccoy " , on may 8 , 2007 with label emi gospel .
Head Entity: darlene mccoy
Tail Entity: emi gospel
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "record label":
Relation: record label  
Context: the band released their latest album, "echoes of the past," under the label universal music group in 2021.  
Head Entity: the band  
Tail Entity: universal music group  

Relation: record label  
Context: after signing a new contract, the artist announced that their upcoming single would be distributed by sony music entertainment.  
Head Entity: the artist  
Tail Entity: sony music entertainment  
#### distributor ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "distributor" means distributor of a creative work; distributor for a record label; news agency; film distributor.
Here is an example:
Relation: distributor
Context: " the many adventures of winnie the pooh " is a 1977 american animated buddy musical comedy film produced by walt disney productions and distributed by buena vista distribution .
Head Entity: the many adventures of winnie the pooh
Tail Entity: buena vista distribution
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "distributor":
Relation: distributor  
Context: "Inception" is a 2010 science fiction film directed by Christopher Nolan and distributed by Warner Bros. Pictures.  
Head Entity: Inception  
Tail Entity: Warner Bros. Pictures  

Relation: distributor  
Context: "The Dark Knight" is a 2008 superhero film directed by Christopher Nolan and distributed by Warner Bros. Pictures.  
Head Entity: The Dark Knight  
Tail Entity: Warner Bros. Pictures  
#### located in or next to body of water ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "located in or next to body of water" means body of water on or next to which a place is located.
Here is an example:
Relation: located in or next to body of water
Context: one account notes , alston and his son , peter also , practiced their counterfeiting operation , at stack island , in the lower mississippi river , about 170 miles upriver from natchez .
Head Entity: stack island
Tail Entity: mississippi river
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "located in or next to body of water":
Relation: located in or next to body of water  
Context: the picturesque village of portsmouth is situated along the banks of the serene river thames, providing stunning views and a tranquil atmosphere for its residents.  
Head Entity: portsmouth  
Tail Entity: river thames  

Relation: located in or next to body of water  
Context: during the summer, families flock to the shores of lake michigan, where they enjoy picnics and water sports in the vibrant city of chicago.  
Head Entity: chicago  
Tail Entity: lake michigan  
#### subsidiary ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "subsidiary" means subsidiary of a company or organization; generally a fully owned separate corporation.
Here is an example:
Relation: subsidiary
Context: following the sale of amstrad plc to rupert murdoch 's bskyb , viglen is now lord sugar 's sole it establishment .
Head Entity: bskyb
Tail Entity: amstrad
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "subsidiary":
Relation: subsidiary  
Context: In 2018, Facebook acquired WhatsApp, making it a subsidiary of the social media giant.  
Head Entity: Facebook  
Tail Entity: WhatsApp  

Relation: subsidiary  
Context: After the merger, the new company announced that its subsidiary, XYZ Corp, would continue to operate independently.  
Head Entity: new company  
Tail Entity: XYZ Corp  
#### country of citizenship ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "country of citizenship" means the object is a country that recognizes the subject as its citizen.
Here is an example:
Relation: country of citizenship
Context: he challenged seven other swimmers in heat four including ukraine 's rostyslav svanidze , a top 16 finalist in atlanta four years earlier .
Head Entity: rostyslav svanidze
Tail Entity: ukraine
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "country of citizenship":
Relation: country of citizenship  
Context: after years of living in the united states, the renowned artist decided to apply for dual citizenship, allowing him to maintain his ties to his homeland, italy.  
Head Entity: the renowned artist  
Tail Entity: italy  

Relation: country of citizenship  
Context: during the international conference, the delegate from canada proudly spoke about the rights and privileges of being a citizen of his country, emphasizing the importance of citizenship in shaping national identity.  
Head Entity: the delegate from canada  
Tail Entity: canada  
#### genre ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "genre" means creative work's genre or an artist's field of work.
Here is an example:
Relation: genre
Context: he is also a part of the atlanta - based hip - hop / r&b;/soul musical collective the dungeon family .
Head Entity: the dungeon family
Tail Entity: hip - hop
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "genre":
Relation: genre  
Context: the band is known for their unique blend of rock and electronic music, often categorized under the genre of synth-pop.  
Head Entity: the band  
Tail Entity: synth-pop  

Relation: genre  
Context: she has made significant contributions to the world of classical music, particularly in the genre of opera.  
Head Entity: she  
Tail Entity: opera  
#### mouth of the watercourse ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "mouth of the watercourse" means the body of water to which the watercourse drains.
Here is an example:
Relation: mouth of the watercourse
Context: san lázaro is 660 km far from asunción and is located at the exact site of the confluence of the apa river with paraguay river , bordering brazil - chaco .
Head Entity: apa river
Tail Entity: paraguay river
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "mouth of the watercourse":
Relation: mouth of the watercourse  
Context: the mississippi river flows into the gulf of mexico, creating a rich delta ecosystem that supports diverse wildlife and plant species.  
Head Entity: mississippi river  
Tail Entity: gulf of mexico  

Relation: mouth of the watercourse  
Context: the rhine river, which runs through several countries in europe, ultimately empties into the north sea, providing a vital shipping route for international trade.  
Head Entity: rhine river  
Tail Entity: north sea  
#### position played on team / speciality ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "position played on team / speciality" means position or specialism of a player on a team.
Here is an example:
Relation: position played on team / speciality
Context: bradley halliday ( born 10 july 1995 ) is an english professional footballer who plays as a defender or a midfielder for league two club cambridge united .
Head Entity: bradley halliday
Tail Entity: defender
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "position played on team / speciality":
Relation: position played on team / speciality  
Context: alex morgan ( born 2 july 1989 ) is an american soccer player who plays as a forward for the national team and club san diego wave fc in the nwsl.  
Head Entity: alex morgan  
Tail Entity: forward  

Relation: position played on team / speciality  
Context: lebron james ( born 30 december 1984 ) is an american professional basketball player who plays as a small forward for the los angeles lakers in the nba.  
Head Entity: lebron james  
Tail Entity: small forward  
#### sports season of league or competition ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "sports season of league or competition" means property that shows the competition of which the item is a season.
Here is an example:
Relation: sports season of league or competition
Context: hammami made his international debut for tunisia in 2007 , and represented them at the africa cup of nations in 2010 and 2013 .
Head Entity: 2010
Tail Entity: africa cup of nations
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "sports season of league or competition":
Relation: sports season of league or competition  
Context: The 2021 UEFA European Championship was held in various cities across Europe, showcasing top national teams competing for the title.  
Head Entity: 2021  
Tail Entity: UEFA European Championship  

Relation: sports season of league or competition  
Context: The 2022 FIFA World Cup took place in Qatar, marking the first time the tournament was held in the Middle East.  
Head Entity: 2022  
Tail Entity: FIFA World Cup  
MemoryTrain:  epoch 15, batch     0 | loss: 4.5691674MemoryTrain:  epoch 15, batch     1 | loss: 5.8420384MemoryTrain:  epoch 15, batch     2 | loss: 6.9642981MemoryTrain:  epoch 15, batch     3 | loss: 5.5270109MemoryTrain:  epoch 15, batch     4 | loss: 6.2439321MemoryTrain:  epoch  9, batch     5 | loss: 3.5530749MemoryTrain:  epoch 15, batch     0 | loss: 5.4423045MemoryTrain:  epoch 15, batch     1 | loss: 5.4289624MemoryTrain:  epoch 15, batch     2 | loss: 5.5220872MemoryTrain:  epoch 15, batch     3 | loss: 5.8922663MemoryTrain:  epoch 15, batch     4 | loss: 8.2418864MemoryTrain:  epoch  9, batch     5 | loss: 6.3188146MemoryTrain:  epoch 15, batch     0 | loss: 4.5855751MemoryTrain:  epoch 15, batch     1 | loss: 6.8472670MemoryTrain:  epoch 15, batch     2 | loss: 4.6599407MemoryTrain:  epoch 15, batch     3 | loss: 4.2103226MemoryTrain:  epoch 15, batch     4 | loss: 6.5511941MemoryTrain:  epoch  9, batch     5 | loss: 2.5017289MemoryTrain:  epoch 15, batch     0 | loss: 5.4491954MemoryTrain:  epoch 15, batch     1 | loss: 3.4943223MemoryTrain:  epoch 15, batch     2 | loss: 3.9826410MemoryTrain:  epoch 15, batch     3 | loss: 4.2880916MemoryTrain:  epoch 15, batch     4 | loss: 2.9369968MemoryTrain:  epoch  9, batch     5 | loss: 2.3668111MemoryTrain:  epoch 15, batch     0 | loss: 3.7544319MemoryTrain:  epoch 15, batch     1 | loss: 3.8087908MemoryTrain:  epoch 15, batch     2 | loss: 2.9883480MemoryTrain:  epoch 15, batch     3 | loss: 3.1276254MemoryTrain:  epoch 15, batch     4 | loss: 12.3518147MemoryTrain:  epoch  9, batch     5 | loss: 2.6245700MemoryTrain:  epoch 15, batch     0 | loss: 3.2545835MemoryTrain:  epoch 15, batch     1 | loss: 3.0237944MemoryTrain:  epoch 15, batch     2 | loss: 3.7642603MemoryTrain:  epoch 15, batch     3 | loss: 4.3809995MemoryTrain:  epoch 15, batch     4 | loss: 3.0264295MemoryTrain:  epoch  9, batch     5 | loss: 2.2626208MemoryTrain:  epoch 15, batch     0 | loss: 4.1953965MemoryTrain:  epoch 15, batch     1 | loss: 2.7622666MemoryTrain:  epoch 15, batch     2 | loss: 4.0642629MemoryTrain:  epoch 15, batch     3 | loss: 4.7242289MemoryTrain:  epoch 15, batch     4 | loss: 6.1981918MemoryTrain:  epoch  9, batch     5 | loss: 4.0892294MemoryTrain:  epoch 15, batch     0 | loss: 4.7646789MemoryTrain:  epoch 15, batch     1 | loss: 2.2678476MemoryTrain:  epoch 15, batch     2 | loss: 2.0531406MemoryTrain:  epoch 15, batch     3 | loss: 2.7364540MemoryTrain:  epoch 15, batch     4 | loss: 2.2712374MemoryTrain:  epoch  9, batch     5 | loss: 2.3990511MemoryTrain:  epoch 15, batch     0 | loss: 1.7335927MemoryTrain:  epoch 15, batch     1 | loss: 1.8988289MemoryTrain:  epoch 15, batch     2 | loss: 4.1212560MemoryTrain:  epoch 15, batch     3 | loss: 2.5109232MemoryTrain:  epoch 15, batch     4 | loss: 3.5740150MemoryTrain:  epoch  9, batch     5 | loss: 1.6486757MemoryTrain:  epoch 15, batch     0 | loss: 5.2753621MemoryTrain:  epoch 15, batch     1 | loss: 4.8307587MemoryTrain:  epoch 15, batch     2 | loss: 5.9850942MemoryTrain:  epoch 15, batch     3 | loss: 5.4097293MemoryTrain:  epoch 15, batch     4 | loss: 1.9512959MemoryTrain:  epoch  9, batch     5 | loss: 1.7499584
[EVAL] batch:    0 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 96.88%   [EVAL] batch:    2 | acc: 100.00%,  total acc: 97.92%   [EVAL] batch:    3 | acc: 100.00%,  total acc: 98.44%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 97.50%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 97.92%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 98.21%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 98.44%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 97.92%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 96.25%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 95.45%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 95.83%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 95.67%   [EVAL] batch:   13 | acc: 87.50%,  total acc: 95.09%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 93.75%   [EVAL] batch:   15 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 92.65%   [EVAL] batch:   17 | acc: 87.50%,  total acc: 92.36%   [EVAL] batch:   18 | acc: 81.25%,  total acc: 91.78%   [EVAL] batch:   19 | acc: 37.50%,  total acc: 89.06%   [EVAL] batch:   20 | acc: 50.00%,  total acc: 87.20%   [EVAL] batch:   21 | acc: 43.75%,  total acc: 85.23%   [EVAL] batch:   22 | acc: 50.00%,  total acc: 83.70%   [EVAL] batch:   23 | acc: 62.50%,  total acc: 82.81%   [EVAL] batch:   24 | acc: 43.75%,  total acc: 81.25%   [EVAL] batch:   25 | acc: 87.50%,  total acc: 81.49%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 81.94%   [EVAL] batch:   27 | acc: 62.50%,  total acc: 81.25%   [EVAL] batch:   28 | acc: 56.25%,  total acc: 80.39%   [EVAL] batch:   29 | acc: 62.50%,  total acc: 79.79%   [EVAL] batch:   30 | acc: 68.75%,  total acc: 79.44%   [EVAL] batch:   31 | acc: 75.00%,  total acc: 79.30%   [EVAL] batch:   32 | acc: 56.25%,  total acc: 78.60%   [EVAL] batch:   33 | acc: 62.50%,  total acc: 78.12%   [EVAL] batch:   34 | acc: 62.50%,  total acc: 77.68%   [EVAL] batch:   35 | acc: 75.00%,  total acc: 77.60%   [EVAL] batch:   36 | acc: 62.50%,  total acc: 77.20%   [EVAL] batch:   37 | acc: 56.25%,  total acc: 76.64%   [EVAL] batch:   38 | acc: 62.50%,  total acc: 76.28%   [EVAL] batch:   39 | acc: 62.50%,  total acc: 75.94%   [EVAL] batch:   40 | acc: 50.00%,  total acc: 75.30%   [EVAL] batch:   41 | acc: 56.25%,  total acc: 74.85%   [EVAL] batch:   42 | acc: 75.00%,  total acc: 74.85%   [EVAL] batch:   43 | acc: 56.25%,  total acc: 74.43%   [EVAL] batch:   44 | acc: 37.50%,  total acc: 73.61%   [EVAL] batch:   45 | acc: 31.25%,  total acc: 72.69%   [EVAL] batch:   46 | acc: 18.75%,  total acc: 71.54%   [EVAL] batch:   47 | acc: 56.25%,  total acc: 71.22%   [EVAL] batch:   48 | acc: 25.00%,  total acc: 70.28%   [EVAL] batch:   49 | acc: 50.00%,  total acc: 69.88%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 70.47%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 71.03%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 71.58%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 72.11%   [EVAL] batch:   54 | acc: 100.00%,  total acc: 72.61%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 73.10%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 73.57%   [EVAL] batch:   57 | acc: 100.00%,  total acc: 74.03%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 74.47%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 74.90%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 75.31%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 75.71%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 75.30%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 82.81%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 82.50%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    6 | acc: 62.50%,  total acc: 80.36%   [EVAL] batch:    7 | acc: 62.50%,  total acc: 78.12%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 77.08%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 77.50%   [EVAL] batch:   10 | acc: 43.75%,  total acc: 74.43%   [EVAL] batch:   11 | acc: 56.25%,  total acc: 72.92%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 73.08%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 74.55%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 75.83%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 77.34%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 78.68%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 79.51%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 80.26%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 80.31%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 80.65%   [EVAL] batch:   21 | acc: 75.00%,  total acc: 80.40%   [EVAL] batch:   22 | acc: 68.75%,  total acc: 79.89%   [EVAL] batch:   23 | acc: 81.25%,  total acc: 79.95%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 80.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 81.01%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 81.71%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 82.37%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 82.97%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 83.54%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 84.07%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 84.85%   [EVAL] batch:   33 | acc: 62.50%,  total acc: 84.19%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 83.93%   [EVAL] batch:   35 | acc: 81.25%,  total acc: 83.85%   [EVAL] batch:   36 | acc: 87.50%,  total acc: 83.95%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 84.05%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 84.46%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 84.84%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 85.21%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 85.57%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 85.76%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 86.08%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 86.25%   [EVAL] batch:   45 | acc: 93.75%,  total acc: 86.41%   [EVAL] batch:   46 | acc: 87.50%,  total acc: 86.44%   [EVAL] batch:   47 | acc: 87.50%,  total acc: 86.46%   [EVAL] batch:   48 | acc: 93.75%,  total acc: 86.61%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 86.62%   [EVAL] batch:   50 | acc: 87.50%,  total acc: 86.64%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 86.78%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 86.91%   [EVAL] batch:   53 | acc: 93.75%,  total acc: 87.04%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 87.05%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 87.17%   [EVAL] batch:   56 | acc: 81.25%,  total acc: 87.06%   [EVAL] batch:   57 | acc: 75.00%,  total acc: 86.85%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 87.08%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 87.19%   [EVAL] batch:   60 | acc: 87.50%,  total acc: 87.19%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 87.20%   [EVAL] batch:   62 | acc: 56.25%,  total acc: 86.71%   [EVAL] batch:   63 | acc: 25.00%,  total acc: 85.74%   [EVAL] batch:   64 | acc: 12.50%,  total acc: 84.62%   [EVAL] batch:   65 | acc: 25.00%,  total acc: 83.71%   [EVAL] batch:   66 | acc: 31.25%,  total acc: 82.93%   [EVAL] batch:   67 | acc: 25.00%,  total acc: 82.08%   [EVAL] batch:   68 | acc: 62.50%,  total acc: 81.79%   [EVAL] batch:   69 | acc: 81.25%,  total acc: 81.79%   [EVAL] batch:   70 | acc: 81.25%,  total acc: 81.78%   [EVAL] batch:   71 | acc: 81.25%,  total acc: 81.77%   [EVAL] batch:   72 | acc: 87.50%,  total acc: 81.85%   [EVAL] batch:   73 | acc: 68.75%,  total acc: 81.67%   [EVAL] batch:   74 | acc: 75.00%,  total acc: 81.58%   [EVAL] batch:   75 | acc: 100.00%,  total acc: 81.83%   [EVAL] batch:   76 | acc: 81.25%,  total acc: 81.82%   [EVAL] batch:   77 | acc: 87.50%,  total acc: 81.89%   [EVAL] batch:   78 | acc: 100.00%,  total acc: 82.12%   [EVAL] batch:   79 | acc: 93.75%,  total acc: 82.27%   [EVAL] batch:   80 | acc: 93.75%,  total acc: 82.41%   [EVAL] batch:   81 | acc: 37.50%,  total acc: 81.86%   [EVAL] batch:   82 | acc: 37.50%,  total acc: 81.33%   [EVAL] batch:   83 | acc: 25.00%,  total acc: 80.65%   [EVAL] batch:   84 | acc: 56.25%,  total acc: 80.37%   [EVAL] batch:   85 | acc: 37.50%,  total acc: 79.87%   [EVAL] batch:   86 | acc: 37.50%,  total acc: 79.38%   [EVAL] batch:   87 | acc: 31.25%,  total acc: 78.84%   [EVAL] batch:   88 | acc: 12.50%,  total acc: 78.09%   [EVAL] batch:   89 | acc: 31.25%,  total acc: 77.57%   [EVAL] batch:   90 | acc: 6.25%,  total acc: 76.79%   [EVAL] batch:   91 | acc: 31.25%,  total acc: 76.29%   [EVAL] batch:   92 | acc: 18.75%,  total acc: 75.67%   [EVAL] batch:   93 | acc: 25.00%,  total acc: 75.13%   [EVAL] batch:   94 | acc: 87.50%,  total acc: 75.26%   [EVAL] batch:   95 | acc: 93.75%,  total acc: 75.46%   [EVAL] batch:   96 | acc: 93.75%,  total acc: 75.64%   [EVAL] batch:   97 | acc: 93.75%,  total acc: 75.83%   [EVAL] batch:   98 | acc: 93.75%,  total acc: 76.01%   [EVAL] batch:   99 | acc: 81.25%,  total acc: 76.06%   [EVAL] batch:  100 | acc: 93.75%,  total acc: 76.24%   [EVAL] batch:  101 | acc: 93.75%,  total acc: 76.41%   [EVAL] batch:  102 | acc: 93.75%,  total acc: 76.58%   [EVAL] batch:  103 | acc: 93.75%,  total acc: 76.74%   [EVAL] batch:  104 | acc: 93.75%,  total acc: 76.90%   [EVAL] batch:  105 | acc: 100.00%,  total acc: 77.12%   [EVAL] batch:  106 | acc: 87.50%,  total acc: 77.22%   [EVAL] batch:  107 | acc: 87.50%,  total acc: 77.31%   [EVAL] batch:  108 | acc: 75.00%,  total acc: 77.29%   [EVAL] batch:  109 | acc: 93.75%,  total acc: 77.44%   [EVAL] batch:  110 | acc: 81.25%,  total acc: 77.48%   [EVAL] batch:  111 | acc: 100.00%,  total acc: 77.68%   [EVAL] batch:  112 | acc: 81.25%,  total acc: 77.71%   [EVAL] batch:  113 | acc: 81.25%,  total acc: 77.74%   [EVAL] batch:  114 | acc: 93.75%,  total acc: 77.88%   [EVAL] batch:  115 | acc: 87.50%,  total acc: 77.96%   [EVAL] batch:  116 | acc: 87.50%,  total acc: 78.04%   [EVAL] batch:  117 | acc: 93.75%,  total acc: 78.18%   [EVAL] batch:  118 | acc: 87.50%,  total acc: 78.26%   [EVAL] batch:  119 | acc: 50.00%,  total acc: 78.02%   [EVAL] batch:  120 | acc: 68.75%,  total acc: 77.94%   [EVAL] batch:  121 | acc: 62.50%,  total acc: 77.82%   [EVAL] batch:  122 | acc: 62.50%,  total acc: 77.69%   [EVAL] batch:  123 | acc: 68.75%,  total acc: 77.62%   [EVAL] batch:  124 | acc: 68.75%,  total acc: 77.55%   [EVAL] batch:  125 | acc: 100.00%,  total acc: 77.73%   [EVAL] batch:  126 | acc: 93.75%,  total acc: 77.85%   [EVAL] batch:  127 | acc: 100.00%,  total acc: 78.03%   [EVAL] batch:  128 | acc: 100.00%,  total acc: 78.20%   [EVAL] batch:  129 | acc: 93.75%,  total acc: 78.32%   [EVAL] batch:  130 | acc: 100.00%,  total acc: 78.48%   [EVAL] batch:  131 | acc: 100.00%,  total acc: 78.65%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 78.81%   [EVAL] batch:  133 | acc: 93.75%,  total acc: 78.92%   [EVAL] batch:  134 | acc: 81.25%,  total acc: 78.94%   [EVAL] batch:  135 | acc: 87.50%,  total acc: 79.00%   [EVAL] batch:  136 | acc: 100.00%,  total acc: 79.15%   [EVAL] batch:  137 | acc: 93.75%,  total acc: 79.26%   [EVAL] batch:  138 | acc: 87.50%,  total acc: 79.32%   [EVAL] batch:  139 | acc: 75.00%,  total acc: 79.29%   [EVAL] batch:  140 | acc: 93.75%,  total acc: 79.39%   [EVAL] batch:  141 | acc: 75.00%,  total acc: 79.36%   [EVAL] batch:  142 | acc: 87.50%,  total acc: 79.41%   [EVAL] batch:  143 | acc: 81.25%,  total acc: 79.43%   [EVAL] batch:  144 | acc: 37.50%,  total acc: 79.14%   [EVAL] batch:  145 | acc: 50.00%,  total acc: 78.94%   [EVAL] batch:  146 | acc: 43.75%,  total acc: 78.70%   [EVAL] batch:  147 | acc: 50.00%,  total acc: 78.51%   [EVAL] batch:  148 | acc: 62.50%,  total acc: 78.40%   [EVAL] batch:  149 | acc: 43.75%,  total acc: 78.17%   [EVAL] batch:  150 | acc: 87.50%,  total acc: 78.23%   [EVAL] batch:  151 | acc: 93.75%,  total acc: 78.33%   [EVAL] batch:  152 | acc: 62.50%,  total acc: 78.23%   [EVAL] batch:  153 | acc: 56.25%,  total acc: 78.08%   [EVAL] batch:  154 | acc: 62.50%,  total acc: 77.98%   [EVAL] batch:  155 | acc: 68.75%,  total acc: 77.92%   [EVAL] batch:  156 | acc: 75.00%,  total acc: 77.91%   [EVAL] batch:  157 | acc: 56.25%,  total acc: 77.77%   [EVAL] batch:  158 | acc: 62.50%,  total acc: 77.67%   [EVAL] batch:  159 | acc: 62.50%,  total acc: 77.58%   [EVAL] batch:  160 | acc: 75.00%,  total acc: 77.56%   [EVAL] batch:  161 | acc: 62.50%,  total acc: 77.47%   [EVAL] batch:  162 | acc: 56.25%,  total acc: 77.34%   [EVAL] batch:  163 | acc: 62.50%,  total acc: 77.25%   [EVAL] batch:  164 | acc: 62.50%,  total acc: 77.16%   [EVAL] batch:  165 | acc: 50.00%,  total acc: 77.00%   [EVAL] batch:  166 | acc: 56.25%,  total acc: 76.87%   [EVAL] batch:  167 | acc: 75.00%,  total acc: 76.86%   [EVAL] batch:  168 | acc: 56.25%,  total acc: 76.74%   [EVAL] batch:  169 | acc: 37.50%,  total acc: 76.51%   [EVAL] batch:  170 | acc: 31.25%,  total acc: 76.24%   [EVAL] batch:  171 | acc: 18.75%,  total acc: 75.91%   [EVAL] batch:  172 | acc: 56.25%,  total acc: 75.79%   [EVAL] batch:  173 | acc: 25.00%,  total acc: 75.50%   [EVAL] batch:  174 | acc: 50.00%,  total acc: 75.36%   [EVAL] batch:  175 | acc: 100.00%,  total acc: 75.50%   [EVAL] batch:  176 | acc: 100.00%,  total acc: 75.64%   [EVAL] batch:  177 | acc: 100.00%,  total acc: 75.77%   [EVAL] batch:  178 | acc: 100.00%,  total acc: 75.91%   [EVAL] batch:  179 | acc: 100.00%,  total acc: 76.04%   [EVAL] batch:  180 | acc: 100.00%,  total acc: 76.17%   [EVAL] batch:  181 | acc: 100.00%,  total acc: 76.30%   [EVAL] batch:  182 | acc: 100.00%,  total acc: 76.43%   [EVAL] batch:  183 | acc: 100.00%,  total acc: 76.56%   [EVAL] batch:  184 | acc: 100.00%,  total acc: 76.69%   [EVAL] batch:  185 | acc: 100.00%,  total acc: 76.81%   [EVAL] batch:  186 | acc: 100.00%,  total acc: 76.94%   [EVAL] batch:  187 | acc: 50.00%,  total acc: 76.80%   
cur_acc:  ['0.9494', '0.7431', '0.7530']
his_acc:  ['0.9494', '0.8230', '0.7680']
CurrentTrain: epoch 15, batch     0 | loss: 18.1026573CurrentTrain: epoch 15, batch     1 | loss: 23.5926767CurrentTrain: epoch 15, batch     2 | loss: 19.3570669CurrentTrain: epoch  1, batch     3 | loss: 9.4026696CurrentTrain: epoch 15, batch     0 | loss: 15.2463244CurrentTrain: epoch 15, batch     1 | loss: 11.6676753CurrentTrain: epoch 15, batch     2 | loss: 14.9042349CurrentTrain: epoch  1, batch     3 | loss: 6.1886186CurrentTrain: epoch 15, batch     0 | loss: 19.7354154CurrentTrain: epoch 15, batch     1 | loss: 15.8056161CurrentTrain: epoch 15, batch     2 | loss: 9.7013579CurrentTrain: epoch  1, batch     3 | loss: 9.5812479CurrentTrain: epoch 15, batch     0 | loss: 8.7790944CurrentTrain: epoch 15, batch     1 | loss: 9.9108702CurrentTrain: epoch 15, batch     2 | loss: 12.6089918CurrentTrain: epoch  1, batch     3 | loss: 5.9803333CurrentTrain: epoch 15, batch     0 | loss: 11.3166208CurrentTrain: epoch 15, batch     1 | loss: 13.5707041CurrentTrain: epoch 15, batch     2 | loss: 12.7522004CurrentTrain: epoch  1, batch     3 | loss: 8.1420101CurrentTrain: epoch 15, batch     0 | loss: 13.0585097CurrentTrain: epoch 15, batch     1 | loss: 10.0351962CurrentTrain: epoch 15, batch     2 | loss: 14.3307880CurrentTrain: epoch  1, batch     3 | loss: 13.4192753CurrentTrain: epoch 15, batch     0 | loss: 15.0427852CurrentTrain: epoch 15, batch     1 | loss: 15.3487956CurrentTrain: epoch 15, batch     2 | loss: 9.2330579CurrentTrain: epoch  1, batch     3 | loss: 7.4014765CurrentTrain: epoch 15, batch     0 | loss: 11.1705367CurrentTrain: epoch 15, batch     1 | loss: 10.7760724CurrentTrain: epoch 15, batch     2 | loss: 7.5185175CurrentTrain: epoch  1, batch     3 | loss: 11.7235785CurrentTrain: epoch 15, batch     0 | loss: 11.5382535CurrentTrain: epoch 15, batch     1 | loss: 11.1994907CurrentTrain: epoch 15, batch     2 | loss: 12.8465033CurrentTrain: epoch  1, batch     3 | loss: 7.6491488CurrentTrain: epoch 15, batch     0 | loss: 15.1036814CurrentTrain: epoch 15, batch     1 | loss: 7.3567367CurrentTrain: epoch 15, batch     2 | loss: 7.5522295CurrentTrain: epoch  1, batch     3 | loss: 6.3621691
#### applies to jurisdiction ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "applies to jurisdiction" means the item (institution, law, public office, public register...) or statement belongs to or has power over or applies to the value (a territorial jurisdiction: a country, state, municipality, ...).
Here is an example:
Relation: applies to jurisdiction
Context: george lawson sheldon ( may 31 , 1870 – april 4 , 1960 ) was 14th governor of nebraska with service from 1907 until 1909 .
Head Entity: governor
Tail Entity: nebraska
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "applies to jurisdiction":
Relation: applies to jurisdiction  
Context: The Environmental Protection Agency (EPA) is responsible for enforcing regulations related to environmental protection in the United States.  
Head Entity: Environmental Protection Agency  
Tail Entity: United States  

Relation: applies to jurisdiction  
Context: The California Consumer Privacy Act (CCPA) provides residents of California with specific rights regarding their personal information.  
Head Entity: California Consumer Privacy Act  
Tail Entity: California  
#### performer ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "performer" means actor, musician, band or other performer associated with this role or musical work.
Here is an example:
Relation: performer
Context: " tenth avenue freeze - out " is the second song on bruce springsteen 's breakthrough album " born to run " , released in 1975 .
Head Entity: tenth avenue freeze - out
Tail Entity: bruce springsteen
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "performer":
Relation: performer  
Context: "Shape of You" is a popular song by Ed Sheeran that topped charts worldwide after its release in 2017.  
Head Entity: Shape of You  
Tail Entity: Ed Sheeran  

Relation: performer  
Context: The Beatles are widely regarded as one of the most influential bands in the history of music, with hits like "Hey Jude" and "Let It Be."  
Head Entity: The Beatles  
Tail Entity: Hey Jude  
#### manufacturer ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "manufacturer" means manufacturer or producer of this product.
Here is an example:
Relation: manufacturer
Context: the glasflügel h-201 standard " libelle " ( german : " dragonfly " ) is an early composite standard class single - seat sailplane produced by glasflügel from 1967 .
Head Entity: dragonfly
Tail Entity: glasflügel
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "manufacturer":
Relation: manufacturer  
Context: The iPhone 13, a smartphone developed by Apple, was released in September 2021 and features advanced camera technology and improved battery life.  
Head Entity: iPhone 13  
Tail Entity: Apple  

Relation: manufacturer  
Context: The Tesla Model S is an all-electric luxury sedan that was first introduced by Tesla, Inc. in 2012, showcasing innovative technology and performance.  
Head Entity: Tesla Model S  
Tail Entity: Tesla, Inc.  
#### participant ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "participant" means person, group of people or organization (object) that actively takes/took part in an event or process.
Here is an example:
Relation: participant
Context: in contrast to gwladys and roger 's allegiance to the house of lancaster , their three sons were staunch yorkists during the wars of the roses .
Head Entity: wars of the roses
Tail Entity: house of lancaster
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "participant":
Relation: participant  
Context: During the annual tech conference, several leading companies showcased their latest innovations, with Google and Microsoft being the most prominent participants.  
Head Entity: tech conference  
Tail Entity: Google  

Relation: participant  
Context: The local community organized a charity run, where various schools and organizations, including the Red Cross, participated to raise funds for the homeless.  
Head Entity: charity run  
Tail Entity: Red Cross  
#### operating system ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "operating system" means operating system (OS) on which a software works or the OS installed on hardware.
Here is an example:
Relation: operating system
Context: the software is written in python and works on all the platforms supported by blender : windows , osx and linux .
Head Entity: blender
Tail Entity: windows
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "operating system":
Relation: operating system  
Context: The latest version of the application is compatible with various operating systems, including macOS, Windows, and Ubuntu.  
Head Entity: application  
Tail Entity: macOS  

Relation: operating system  
Context: This game runs smoothly on multiple operating systems, such as Linux, Windows 10, and Android.  
Head Entity: game  
Tail Entity: Linux  
#### competition class ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "competition class" means official classification by a regulating body under which the subject (events, teams, participants, or equipment) qualifies for inclusion.
Here is an example:
Relation: competition class
Context: in july 2007 , griffin lost by a technical knockout to light heavyweight champion danny green of the world boxing association at challenge stadium in perth , australia .
Head Entity: danny green
Tail Entity: light heavyweight
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "competition class":
Relation: competition class  
Context: during the 2020 olympics, the men's 100 meters final showcased athletes from various competition classes, with usain bolt representing the sprinting class.  
Head Entity: usain bolt  
Tail Entity: sprinting  

Relation: competition class  
Context: the formula one race in monaco is known for its high-speed competition class, where lewis hamilton competed fiercely against other top drivers.  
Head Entity: lewis hamilton  
Tail Entity: formula one
#### successful candidate ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "successful candidate" means person elected after the election.
Here is an example:
Relation: successful candidate
Context: the qup gained 3 seats from its previous election result , and won a 10 % swing to it in primary votes , but failed to defeat the incumbent labor government led by ted theodore .
Head Entity: previous election
Tail Entity: ted theodore
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "successful candidate":
Relation: successful candidate  
Context: After a closely contested campaign, the voters chose Sarah Johnson as their representative, marking a significant shift in the political landscape of the district.  
Head Entity: closely contested campaign  
Tail Entity: Sarah Johnson  

Relation: successful candidate  
Context: In the recent mayoral election, the community rallied behind Mark Thompson, who promised to bring new initiatives to the city, ultimately leading to his victory.  
Head Entity: recent mayoral election  
Tail Entity: Mark Thompson  
#### sibling ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "sibling" means the subject and the object have the same parents.
Here is an example:
Relation: sibling
Context: the deaths of his brothers wenceslaus ii ( 1487 ) , casimir ii ( 1490 ) and władysław ( 1494 ) allowed jan v to reunificated the whole duchy of zator .
Head Entity: casimir ii
Tail Entity: wenceslaus ii
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "sibling":
Relation: sibling  
Context: both elizabeth and her brother, charles, were known for their contributions to the arts and culture of their time.  
Head Entity: elizabeth  
Tail Entity: charles  

Relation: sibling  
Context: during the family reunion, it was heartwarming to see how much john and his sister, sarah, resembled each other in both looks and personality.  
Head Entity: john  
Tail Entity: sarah  
#### military branch ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "military branch" means branch to which this military unit, award, office, or person belongs.
Here is an example:
Relation: military branch
Context: iain harrison is a competitive shooter and former british army captain .
Head Entity: iain harrison
Tail Entity: british army
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "military branch":
Relation: military branch  
Context: sergeant john smith served in the united states marine corps during his military career.  
Head Entity: john smith  
Tail Entity: united states marine corps  

Relation: military branch  
Context: general patricia harris was a prominent figure in the royal air force, leading several key missions.  
Head Entity: patricia harris  
Tail Entity: royal air force  
#### child ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "child" means subject has object as child.
Here is an example:
Relation: child
Context: developed on the east bank of the river near the former native village of otstonwakin , the borough is named for andrew montour , the french / native american and son of madame montour .
Head Entity: madame montour
Tail Entity: andrew montour
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "child":
Relation: child  
Context: In the small town of Maplewood, Sarah Johnson is known for her dedication to her family, especially her son, Michael, who is a talented musician.  
Head Entity: Sarah Johnson  
Tail Entity: Michael  

Relation: child  
Context: The famous author, J.K. Rowling, often shares stories about her daughter, Jessica, who inspired many characters in her books.  
Head Entity: J.K. Rowling  
Tail Entity: Jessica  
MemoryTrain:  epoch 15, batch     0 | loss: 12.2182855MemoryTrain:  epoch 15, batch     1 | loss: 4.3109103MemoryTrain:  epoch 15, batch     2 | loss: 9.2983425MemoryTrain:  epoch 15, batch     3 | loss: 4.7591614MemoryTrain:  epoch 15, batch     4 | loss: 5.2669118MemoryTrain:  epoch 15, batch     5 | loss: 11.2918802MemoryTrain:  epoch 15, batch     6 | loss: 4.2750559MemoryTrain:  epoch  7, batch     7 | loss: 6.2262186MemoryTrain:  epoch 15, batch     0 | loss: 4.1897426MemoryTrain:  epoch 15, batch     1 | loss: 4.0576313MemoryTrain:  epoch 15, batch     2 | loss: 4.7669337MemoryTrain:  epoch 15, batch     3 | loss: 4.3256583MemoryTrain:  epoch 15, batch     4 | loss: 3.8508739MemoryTrain:  epoch 15, batch     5 | loss: 3.0895342MemoryTrain:  epoch 15, batch     6 | loss: 5.2603050MemoryTrain:  epoch  7, batch     7 | loss: 8.3034711MemoryTrain:  epoch 15, batch     0 | loss: 3.0651334MemoryTrain:  epoch 15, batch     1 | loss: 5.2436202MemoryTrain:  epoch 15, batch     2 | loss: 4.8146376MemoryTrain:  epoch 15, batch     3 | loss: 4.6425721MemoryTrain:  epoch 15, batch     4 | loss: 2.9085309MemoryTrain:  epoch 15, batch     5 | loss: 3.7350502MemoryTrain:  epoch 15, batch     6 | loss: 3.1050444MemoryTrain:  epoch  7, batch     7 | loss: 2.1032753MemoryTrain:  epoch 15, batch     0 | loss: 3.6521982MemoryTrain:  epoch 15, batch     1 | loss: 4.2189762MemoryTrain:  epoch 15, batch     2 | loss: 2.4078407MemoryTrain:  epoch 15, batch     3 | loss: 3.3472980MemoryTrain:  epoch 15, batch     4 | loss: 3.2635810MemoryTrain:  epoch 15, batch     5 | loss: 5.0250304MemoryTrain:  epoch 15, batch     6 | loss: 2.7259407MemoryTrain:  epoch  7, batch     7 | loss: 2.9176806MemoryTrain:  epoch 15, batch     0 | loss: 4.5400839MemoryTrain:  epoch 15, batch     1 | loss: 2.5243052MemoryTrain:  epoch 15, batch     2 | loss: 4.3311225MemoryTrain:  epoch 15, batch     3 | loss: 3.0322767MemoryTrain:  epoch 15, batch     4 | loss: 3.4373349MemoryTrain:  epoch 15, batch     5 | loss: 6.6445354MemoryTrain:  epoch 15, batch     6 | loss: 4.7937155MemoryTrain:  epoch  7, batch     7 | loss: 1.7435392MemoryTrain:  epoch 15, batch     0 | loss: 2.3226617MemoryTrain:  epoch 15, batch     1 | loss: 4.6830038MemoryTrain:  epoch 15, batch     2 | loss: 3.8202601MemoryTrain:  epoch 15, batch     3 | loss: 2.4793700MemoryTrain:  epoch 15, batch     4 | loss: 2.0605474MemoryTrain:  epoch 15, batch     5 | loss: 4.8886475MemoryTrain:  epoch 15, batch     6 | loss: 4.8480697MemoryTrain:  epoch  7, batch     7 | loss: 1.8010492MemoryTrain:  epoch 15, batch     0 | loss: 7.2869698MemoryTrain:  epoch 15, batch     1 | loss: 2.3970924MemoryTrain:  epoch 15, batch     2 | loss: 2.0960952MemoryTrain:  epoch 15, batch     3 | loss: 2.8954572MemoryTrain:  epoch 15, batch     4 | loss: 3.7356368MemoryTrain:  epoch 15, batch     5 | loss: 1.9082444MemoryTrain:  epoch 15, batch     6 | loss: 2.5766229MemoryTrain:  epoch  7, batch     7 | loss: 1.7420182MemoryTrain:  epoch 15, batch     0 | loss: 2.8850439MemoryTrain:  epoch 15, batch     1 | loss: 2.0833864MemoryTrain:  epoch 15, batch     2 | loss: 2.0725900MemoryTrain:  epoch 15, batch     3 | loss: 2.3727347MemoryTrain:  epoch 15, batch     4 | loss: 2.4037221MemoryTrain:  epoch 15, batch     5 | loss: 2.2237991MemoryTrain:  epoch 15, batch     6 | loss: 4.1571991MemoryTrain:  epoch  7, batch     7 | loss: 1.5976465MemoryTrain:  epoch 15, batch     0 | loss: 2.6269927MemoryTrain:  epoch 15, batch     1 | loss: 4.5620343MemoryTrain:  epoch 15, batch     2 | loss: 2.5479217MemoryTrain:  epoch 15, batch     3 | loss: 1.3898331MemoryTrain:  epoch 15, batch     4 | loss: 2.8622336MemoryTrain:  epoch 15, batch     5 | loss: 1.9597435MemoryTrain:  epoch 15, batch     6 | loss: 2.7765695MemoryTrain:  epoch  7, batch     7 | loss: 1.8389631MemoryTrain:  epoch 15, batch     0 | loss: 4.3542845MemoryTrain:  epoch 15, batch     1 | loss: 4.6537660MemoryTrain:  epoch 15, batch     2 | loss: 2.6575678MemoryTrain:  epoch 15, batch     3 | loss: 2.9200023MemoryTrain:  epoch 15, batch     4 | loss: 3.9094420MemoryTrain:  epoch 15, batch     5 | loss: 1.8941701MemoryTrain:  epoch 15, batch     6 | loss: 2.0411206MemoryTrain:  epoch  7, batch     7 | loss: 1.5692613
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 75.00%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 79.17%   [EVAL] batch:    3 | acc: 50.00%,  total acc: 71.88%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 73.75%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 75.00%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 75.89%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 78.12%   [EVAL] batch:    8 | acc: 75.00%,  total acc: 77.78%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 78.75%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 78.98%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 79.17%   [EVAL] batch:   12 | acc: 100.00%,  total acc: 80.77%   [EVAL] batch:   13 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:   14 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:   15 | acc: 87.50%,  total acc: 81.64%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 82.35%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 82.99%   [EVAL] batch:   18 | acc: 87.50%,  total acc: 83.22%   [EVAL] batch:   19 | acc: 12.50%,  total acc: 79.69%   [EVAL] batch:   20 | acc: 43.75%,  total acc: 77.98%   [EVAL] batch:   21 | acc: 25.00%,  total acc: 75.57%   [EVAL] batch:   22 | acc: 31.25%,  total acc: 73.64%   [EVAL] batch:   23 | acc: 31.25%,  total acc: 71.88%   [EVAL] batch:   24 | acc: 31.25%,  total acc: 70.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 71.39%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 72.45%   [EVAL] batch:   27 | acc: 87.50%,  total acc: 72.99%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 73.92%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 74.79%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 75.60%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 76.17%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 76.89%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 77.57%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 78.21%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 78.82%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 79.39%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 79.93%   [EVAL] batch:   38 | acc: 93.75%,  total acc: 80.29%   [EVAL] batch:   39 | acc: 87.50%,  total acc: 80.47%   [EVAL] batch:   40 | acc: 87.50%,  total acc: 80.64%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 81.10%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 81.40%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 81.68%   [EVAL] batch:   44 | acc: 62.50%,  total acc: 81.25%   [EVAL] batch:   45 | acc: 56.25%,  total acc: 80.71%   [EVAL] batch:   46 | acc: 56.25%,  total acc: 80.19%   [EVAL] batch:   47 | acc: 62.50%,  total acc: 79.82%   [EVAL] batch:   48 | acc: 56.25%,  total acc: 79.34%   [EVAL] batch:   49 | acc: 81.25%,  total acc: 79.38%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 79.66%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 79.93%   [EVAL] batch:   52 | acc: 87.50%,  total acc: 80.07%   [EVAL] batch:   53 | acc: 87.50%,  total acc: 80.21%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 80.45%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 80.58%   [EVAL] batch:   56 | acc: 68.75%,  total acc: 80.37%   [EVAL] batch:   57 | acc: 62.50%,  total acc: 80.06%   [EVAL] batch:   58 | acc: 37.50%,  total acc: 79.34%   [EVAL] batch:   59 | acc: 50.00%,  total acc: 78.85%   [EVAL] batch:   60 | acc: 50.00%,  total acc: 78.38%   [EVAL] batch:   61 | acc: 56.25%,  total acc: 78.02%   [EVAL] batch:   62 | acc: 18.75%,  total acc: 77.08%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 82.29%   [EVAL] batch:    6 | acc: 62.50%,  total acc: 79.46%   [EVAL] batch:    7 | acc: 62.50%,  total acc: 77.34%   [EVAL] batch:    8 | acc: 62.50%,  total acc: 75.69%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 74.38%   [EVAL] batch:   10 | acc: 43.75%,  total acc: 71.59%   [EVAL] batch:   11 | acc: 56.25%,  total acc: 70.31%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 70.67%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 72.32%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 73.75%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 75.39%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 76.47%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 77.43%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 78.29%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 78.44%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 79.17%   [EVAL] batch:   21 | acc: 68.75%,  total acc: 78.69%   [EVAL] batch:   22 | acc: 62.50%,  total acc: 77.99%   [EVAL] batch:   23 | acc: 81.25%,  total acc: 78.12%   [EVAL] batch:   24 | acc: 75.00%,  total acc: 78.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 78.85%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 79.63%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 80.36%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 81.03%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 81.67%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 82.26%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 82.62%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 83.14%   [EVAL] batch:   33 | acc: 75.00%,  total acc: 82.90%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 82.68%   [EVAL] batch:   35 | acc: 81.25%,  total acc: 82.64%   [EVAL] batch:   36 | acc: 93.75%,  total acc: 82.94%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 83.22%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 83.65%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 84.06%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 84.45%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 84.67%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 84.88%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 84.94%   [EVAL] batch:   44 | acc: 75.00%,  total acc: 84.72%   [EVAL] batch:   45 | acc: 75.00%,  total acc: 84.51%   [EVAL] batch:   46 | acc: 62.50%,  total acc: 84.04%   [EVAL] batch:   47 | acc: 68.75%,  total acc: 83.72%   [EVAL] batch:   48 | acc: 75.00%,  total acc: 83.55%   [EVAL] batch:   49 | acc: 68.75%,  total acc: 83.25%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 83.46%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 83.65%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 83.84%   [EVAL] batch:   53 | acc: 93.75%,  total acc: 84.03%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 83.98%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 84.15%   [EVAL] batch:   56 | acc: 81.25%,  total acc: 84.10%   [EVAL] batch:   57 | acc: 68.75%,  total acc: 83.84%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 83.90%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 84.06%   [EVAL] batch:   60 | acc: 87.50%,  total acc: 84.12%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 84.17%   [EVAL] batch:   62 | acc: 62.50%,  total acc: 83.83%   [EVAL] batch:   63 | acc: 18.75%,  total acc: 82.81%   [EVAL] batch:   64 | acc: 18.75%,  total acc: 81.83%   [EVAL] batch:   65 | acc: 25.00%,  total acc: 80.97%   [EVAL] batch:   66 | acc: 31.25%,  total acc: 80.22%   [EVAL] batch:   67 | acc: 25.00%,  total acc: 79.41%   [EVAL] batch:   68 | acc: 68.75%,  total acc: 79.26%   [EVAL] batch:   69 | acc: 75.00%,  total acc: 79.20%   [EVAL] batch:   70 | acc: 81.25%,  total acc: 79.23%   [EVAL] batch:   71 | acc: 81.25%,  total acc: 79.25%   [EVAL] batch:   72 | acc: 87.50%,  total acc: 79.37%   [EVAL] batch:   73 | acc: 62.50%,  total acc: 79.14%   [EVAL] batch:   74 | acc: 68.75%,  total acc: 79.00%   [EVAL] batch:   75 | acc: 100.00%,  total acc: 79.28%   [EVAL] batch:   76 | acc: 81.25%,  total acc: 79.30%   [EVAL] batch:   77 | acc: 87.50%,  total acc: 79.41%   [EVAL] batch:   78 | acc: 100.00%,  total acc: 79.67%   [EVAL] batch:   79 | acc: 93.75%,  total acc: 79.84%   [EVAL] batch:   80 | acc: 93.75%,  total acc: 80.02%   [EVAL] batch:   81 | acc: 43.75%,  total acc: 79.57%   [EVAL] batch:   82 | acc: 43.75%,  total acc: 79.14%   [EVAL] batch:   83 | acc: 25.00%,  total acc: 78.50%   [EVAL] batch:   84 | acc: 68.75%,  total acc: 78.38%   [EVAL] batch:   85 | acc: 50.00%,  total acc: 78.05%   [EVAL] batch:   86 | acc: 50.00%,  total acc: 77.73%   [EVAL] batch:   87 | acc: 37.50%,  total acc: 77.27%   [EVAL] batch:   88 | acc: 6.25%,  total acc: 76.47%   [EVAL] batch:   89 | acc: 31.25%,  total acc: 75.97%   [EVAL] batch:   90 | acc: 6.25%,  total acc: 75.21%   [EVAL] batch:   91 | acc: 25.00%,  total acc: 74.66%   [EVAL] batch:   92 | acc: 12.50%,  total acc: 73.99%   [EVAL] batch:   93 | acc: 25.00%,  total acc: 73.47%   [EVAL] batch:   94 | acc: 87.50%,  total acc: 73.62%   [EVAL] batch:   95 | acc: 93.75%,  total acc: 73.83%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 74.10%   [EVAL] batch:   97 | acc: 93.75%,  total acc: 74.30%   [EVAL] batch:   98 | acc: 93.75%,  total acc: 74.49%   [EVAL] batch:   99 | acc: 75.00%,  total acc: 74.50%   [EVAL] batch:  100 | acc: 93.75%,  total acc: 74.69%   [EVAL] batch:  101 | acc: 93.75%,  total acc: 74.88%   [EVAL] batch:  102 | acc: 87.50%,  total acc: 75.00%   [EVAL] batch:  103 | acc: 93.75%,  total acc: 75.18%   [EVAL] batch:  104 | acc: 93.75%,  total acc: 75.36%   [EVAL] batch:  105 | acc: 100.00%,  total acc: 75.59%   [EVAL] batch:  106 | acc: 87.50%,  total acc: 75.70%   [EVAL] batch:  107 | acc: 93.75%,  total acc: 75.87%   [EVAL] batch:  108 | acc: 68.75%,  total acc: 75.80%   [EVAL] batch:  109 | acc: 87.50%,  total acc: 75.91%   [EVAL] batch:  110 | acc: 75.00%,  total acc: 75.90%   [EVAL] batch:  111 | acc: 93.75%,  total acc: 76.06%   [EVAL] batch:  112 | acc: 68.75%,  total acc: 76.00%   [EVAL] batch:  113 | acc: 62.50%,  total acc: 75.88%   [EVAL] batch:  114 | acc: 75.00%,  total acc: 75.87%   [EVAL] batch:  115 | acc: 87.50%,  total acc: 75.97%   [EVAL] batch:  116 | acc: 81.25%,  total acc: 76.01%   [EVAL] batch:  117 | acc: 81.25%,  total acc: 76.06%   [EVAL] batch:  118 | acc: 68.75%,  total acc: 76.00%   [EVAL] batch:  119 | acc: 50.00%,  total acc: 75.78%   [EVAL] batch:  120 | acc: 68.75%,  total acc: 75.72%   [EVAL] batch:  121 | acc: 68.75%,  total acc: 75.67%   [EVAL] batch:  122 | acc: 62.50%,  total acc: 75.56%   [EVAL] batch:  123 | acc: 68.75%,  total acc: 75.50%   [EVAL] batch:  124 | acc: 68.75%,  total acc: 75.45%   [EVAL] batch:  125 | acc: 81.25%,  total acc: 75.50%   [EVAL] batch:  126 | acc: 75.00%,  total acc: 75.49%   [EVAL] batch:  127 | acc: 62.50%,  total acc: 75.39%   [EVAL] batch:  128 | acc: 56.25%,  total acc: 75.24%   [EVAL] batch:  129 | acc: 75.00%,  total acc: 75.24%   [EVAL] batch:  130 | acc: 81.25%,  total acc: 75.29%   [EVAL] batch:  131 | acc: 93.75%,  total acc: 75.43%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 75.61%   [EVAL] batch:  133 | acc: 93.75%,  total acc: 75.75%   [EVAL] batch:  134 | acc: 75.00%,  total acc: 75.74%   [EVAL] batch:  135 | acc: 87.50%,  total acc: 75.83%   [EVAL] batch:  136 | acc: 93.75%,  total acc: 75.96%   [EVAL] batch:  137 | acc: 87.50%,  total acc: 76.04%   [EVAL] batch:  138 | acc: 87.50%,  total acc: 76.12%   [EVAL] batch:  139 | acc: 81.25%,  total acc: 76.16%   [EVAL] batch:  140 | acc: 87.50%,  total acc: 76.24%   [EVAL] batch:  141 | acc: 81.25%,  total acc: 76.28%   [EVAL] batch:  142 | acc: 81.25%,  total acc: 76.31%   [EVAL] batch:  143 | acc: 87.50%,  total acc: 76.39%   [EVAL] batch:  144 | acc: 25.00%,  total acc: 76.03%   [EVAL] batch:  145 | acc: 37.50%,  total acc: 75.77%   [EVAL] batch:  146 | acc: 25.00%,  total acc: 75.43%   [EVAL] batch:  147 | acc: 43.75%,  total acc: 75.21%   [EVAL] batch:  148 | acc: 43.75%,  total acc: 75.00%   [EVAL] batch:  149 | acc: 43.75%,  total acc: 74.79%   [EVAL] batch:  150 | acc: 62.50%,  total acc: 74.71%   [EVAL] batch:  151 | acc: 81.25%,  total acc: 74.75%   [EVAL] batch:  152 | acc: 50.00%,  total acc: 74.59%   [EVAL] batch:  153 | acc: 56.25%,  total acc: 74.47%   [EVAL] batch:  154 | acc: 50.00%,  total acc: 74.31%   [EVAL] batch:  155 | acc: 68.75%,  total acc: 74.28%   [EVAL] batch:  156 | acc: 75.00%,  total acc: 74.28%   [EVAL] batch:  157 | acc: 56.25%,  total acc: 74.17%   [EVAL] batch:  158 | acc: 68.75%,  total acc: 74.14%   [EVAL] batch:  159 | acc: 68.75%,  total acc: 74.10%   [EVAL] batch:  160 | acc: 68.75%,  total acc: 74.07%   [EVAL] batch:  161 | acc: 62.50%,  total acc: 74.00%   [EVAL] batch:  162 | acc: 56.25%,  total acc: 73.89%   [EVAL] batch:  163 | acc: 50.00%,  total acc: 73.74%   [EVAL] batch:  164 | acc: 56.25%,  total acc: 73.64%   [EVAL] batch:  165 | acc: 50.00%,  total acc: 73.49%   [EVAL] batch:  166 | acc: 50.00%,  total acc: 73.35%   [EVAL] batch:  167 | acc: 68.75%,  total acc: 73.33%   [EVAL] batch:  168 | acc: 62.50%,  total acc: 73.26%   [EVAL] batch:  169 | acc: 25.00%,  total acc: 72.98%   [EVAL] batch:  170 | acc: 31.25%,  total acc: 72.73%   [EVAL] batch:  171 | acc: 18.75%,  total acc: 72.42%   [EVAL] batch:  172 | acc: 50.00%,  total acc: 72.29%   [EVAL] batch:  173 | acc: 25.00%,  total acc: 72.02%   [EVAL] batch:  174 | acc: 50.00%,  total acc: 71.89%   [EVAL] batch:  175 | acc: 100.00%,  total acc: 72.05%   [EVAL] batch:  176 | acc: 100.00%,  total acc: 72.21%   [EVAL] batch:  177 | acc: 100.00%,  total acc: 72.37%   [EVAL] batch:  178 | acc: 100.00%,  total acc: 72.52%   [EVAL] batch:  179 | acc: 100.00%,  total acc: 72.67%   [EVAL] batch:  180 | acc: 100.00%,  total acc: 72.82%   [EVAL] batch:  181 | acc: 100.00%,  total acc: 72.97%   [EVAL] batch:  182 | acc: 100.00%,  total acc: 73.12%   [EVAL] batch:  183 | acc: 100.00%,  total acc: 73.27%   [EVAL] batch:  184 | acc: 100.00%,  total acc: 73.41%   [EVAL] batch:  185 | acc: 100.00%,  total acc: 73.56%   [EVAL] batch:  186 | acc: 100.00%,  total acc: 73.70%   [EVAL] batch:  187 | acc: 93.75%,  total acc: 73.80%   [EVAL] batch:  188 | acc: 75.00%,  total acc: 73.81%   [EVAL] batch:  189 | acc: 75.00%,  total acc: 73.82%   [EVAL] batch:  190 | acc: 81.25%,  total acc: 73.85%   [EVAL] batch:  191 | acc: 56.25%,  total acc: 73.76%   [EVAL] batch:  192 | acc: 87.50%,  total acc: 73.83%   [EVAL] batch:  193 | acc: 81.25%,  total acc: 73.87%   [EVAL] batch:  194 | acc: 81.25%,  total acc: 73.91%   [EVAL] batch:  195 | acc: 87.50%,  total acc: 73.98%   [EVAL] batch:  196 | acc: 68.75%,  total acc: 73.95%   [EVAL] batch:  197 | acc: 93.75%,  total acc: 74.05%   [EVAL] batch:  198 | acc: 81.25%,  total acc: 74.09%   [EVAL] batch:  199 | acc: 87.50%,  total acc: 74.16%   [EVAL] batch:  200 | acc: 100.00%,  total acc: 74.28%   [EVAL] batch:  201 | acc: 81.25%,  total acc: 74.32%   [EVAL] batch:  202 | acc: 75.00%,  total acc: 74.32%   [EVAL] batch:  203 | acc: 100.00%,  total acc: 74.45%   [EVAL] batch:  204 | acc: 87.50%,  total acc: 74.51%   [EVAL] batch:  205 | acc: 100.00%,  total acc: 74.64%   [EVAL] batch:  206 | acc: 37.50%,  total acc: 74.46%   [EVAL] batch:  207 | acc: 25.00%,  total acc: 74.22%   [EVAL] batch:  208 | acc: 50.00%,  total acc: 74.10%   [EVAL] batch:  209 | acc: 12.50%,  total acc: 73.81%   [EVAL] batch:  210 | acc: 37.50%,  total acc: 73.64%   [EVAL] batch:  211 | acc: 43.75%,  total acc: 73.50%   [EVAL] batch:  212 | acc: 56.25%,  total acc: 73.42%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 73.54%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 73.66%   [EVAL] batch:  215 | acc: 87.50%,  total acc: 73.73%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 73.85%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 73.97%   [EVAL] batch:  218 | acc: 93.75%,  total acc: 74.06%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 74.18%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 74.29%   [EVAL] batch:  221 | acc: 100.00%,  total acc: 74.41%   [EVAL] batch:  222 | acc: 100.00%,  total acc: 74.52%   [EVAL] batch:  223 | acc: 100.00%,  total acc: 74.64%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 74.75%   [EVAL] batch:  225 | acc: 93.75%,  total acc: 74.83%   [EVAL] batch:  226 | acc: 93.75%,  total acc: 74.92%   [EVAL] batch:  227 | acc: 87.50%,  total acc: 74.97%   [EVAL] batch:  228 | acc: 93.75%,  total acc: 75.05%   [EVAL] batch:  229 | acc: 100.00%,  total acc: 75.16%   [EVAL] batch:  230 | acc: 93.75%,  total acc: 75.24%   [EVAL] batch:  231 | acc: 68.75%,  total acc: 75.22%   [EVAL] batch:  232 | acc: 50.00%,  total acc: 75.11%   [EVAL] batch:  233 | acc: 75.00%,  total acc: 75.11%   [EVAL] batch:  234 | acc: 50.00%,  total acc: 75.00%   [EVAL] batch:  235 | acc: 68.75%,  total acc: 74.97%   [EVAL] batch:  236 | acc: 56.25%,  total acc: 74.89%   [EVAL] batch:  237 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:  238 | acc: 87.50%,  total acc: 75.05%   [EVAL] batch:  239 | acc: 100.00%,  total acc: 75.16%   [EVAL] batch:  240 | acc: 81.25%,  total acc: 75.18%   [EVAL] batch:  241 | acc: 93.75%,  total acc: 75.26%   [EVAL] batch:  242 | acc: 93.75%,  total acc: 75.33%   [EVAL] batch:  243 | acc: 81.25%,  total acc: 75.36%   [EVAL] batch:  244 | acc: 50.00%,  total acc: 75.26%   [EVAL] batch:  245 | acc: 62.50%,  total acc: 75.20%   [EVAL] batch:  246 | acc: 37.50%,  total acc: 75.05%   [EVAL] batch:  247 | acc: 50.00%,  total acc: 74.95%   [EVAL] batch:  248 | acc: 43.75%,  total acc: 74.82%   [EVAL] batch:  249 | acc: 56.25%,  total acc: 74.75%   
cur_acc:  ['0.9494', '0.7431', '0.7530', '0.7708']
his_acc:  ['0.9494', '0.8230', '0.7680', '0.7475']
CurrentTrain: epoch 15, batch     0 | loss: 16.6911683CurrentTrain: epoch 15, batch     1 | loss: 20.3792107CurrentTrain: epoch 15, batch     2 | loss: 17.3464829CurrentTrain: epoch  1, batch     3 | loss: 8.7175806CurrentTrain: epoch 15, batch     0 | loss: 17.8278492CurrentTrain: epoch 15, batch     1 | loss: 14.7005620CurrentTrain: epoch 15, batch     2 | loss: 16.3461648CurrentTrain: epoch  1, batch     3 | loss: 6.7102037CurrentTrain: epoch 15, batch     0 | loss: 12.4504803CurrentTrain: epoch 15, batch     1 | loss: 14.0398512CurrentTrain: epoch 15, batch     2 | loss: 18.3441003CurrentTrain: epoch  1, batch     3 | loss: 8.9473919CurrentTrain: epoch 15, batch     0 | loss: 16.6155588CurrentTrain: epoch 15, batch     1 | loss: 9.5229767CurrentTrain: epoch 15, batch     2 | loss: 11.6233865CurrentTrain: epoch  1, batch     3 | loss: 6.8393651CurrentTrain: epoch 15, batch     0 | loss: 18.4859679CurrentTrain: epoch 15, batch     1 | loss: 9.0015874CurrentTrain: epoch 15, batch     2 | loss: 14.9995252CurrentTrain: epoch  1, batch     3 | loss: 7.8489832CurrentTrain: epoch 15, batch     0 | loss: 10.0612575CurrentTrain: epoch 15, batch     1 | loss: 9.2310891CurrentTrain: epoch 15, batch     2 | loss: 9.3256407CurrentTrain: epoch  1, batch     3 | loss: 8.8529079CurrentTrain: epoch 15, batch     0 | loss: 9.9351244CurrentTrain: epoch 15, batch     1 | loss: 9.0539320CurrentTrain: epoch 15, batch     2 | loss: 9.2818496CurrentTrain: epoch  1, batch     3 | loss: 8.0128413CurrentTrain: epoch 15, batch     0 | loss: 7.3288893CurrentTrain: epoch 15, batch     1 | loss: 8.7732638CurrentTrain: epoch 15, batch     2 | loss: 10.8519629CurrentTrain: epoch  1, batch     3 | loss: 15.1038222CurrentTrain: epoch 15, batch     0 | loss: 10.2012875CurrentTrain: epoch 15, batch     1 | loss: 8.8787239CurrentTrain: epoch 15, batch     2 | loss: 12.2891362CurrentTrain: epoch  1, batch     3 | loss: 5.3892329CurrentTrain: epoch 15, batch     0 | loss: 9.5799622CurrentTrain: epoch 15, batch     1 | loss: 10.8269234CurrentTrain: epoch 15, batch     2 | loss: 11.7532107CurrentTrain: epoch  1, batch     3 | loss: 5.6046209
#### contains administrative territorial entity ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "contains administrative territorial entity" means direct subdivisions of an administrative territorial entity.
Here is an example:
Relation: contains administrative territorial entity
Context: the cantons of créteil are administrative divisions of the val - de - marne department , île - de - france region , northern france .
Head Entity: île - de - france
Tail Entity: val - de - marne
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "contains administrative territorial entity":
Relation: contains administrative territorial entity  
Context: The provinces of Canada are the primary administrative divisions of the country, with Ontario being one of the most populous provinces.  
Head Entity: Canada  
Tail Entity: Ontario  

Relation: contains administrative territorial entity  
Context: The states of Australia are significant political divisions, with New South Wales being the most populous state in the country.  
Head Entity: Australia  
Tail Entity: New South Wales  
#### said to be the same as ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "said to be the same as" means this item is said to be the same as that item, but it's uncertain or disputed.
Here is an example:
Relation: said to be the same as
Context: enzo is an italian given name derivative of heinz , a form of henry .
Head Entity: henry
Tail Entity: heinz
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "said to be the same as":
Relation: said to be the same as  
Context: The two species, Canis lupus and Canis familiaris, are often said to be the same as each other due to their close genetic relationship.  
Head Entity: Canis lupus  
Tail Entity: Canis familiaris  

Relation: said to be the same as  
Context: Many historians argue that the ancient city of Byzantium is said to be the same as modern-day Istanbul, although the transition involved significant changes over time.  
Head Entity: Byzantium  
Tail Entity: Istanbul  
#### winner ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "winner" means winner of a competition or similar event.
Here is an example:
Relation: winner
Context: in 2010 , ty burrell received a nomination for the television critics association award for individual achievement in comedy , winning in 2011 .
Head Entity: television critics association award for individual achievement in comedy
Tail Entity: ty burrell
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "winner":
Relation: winner  
Context: In 2022, the Boston Red Sox triumphed in the World Series, marking their fourth championship in the last two decades.  
Head Entity: World Series  
Tail Entity: Boston Red Sox  

Relation: winner  
Context: At the annual film festival, the documentary "Voices of Change" was awarded the Best Picture, highlighting its impactful storytelling and direction.  
Head Entity: Best Picture  
Tail Entity: Voices of Change  
#### military rank ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "military rank" means military rank achieved by a person, or military rank associated with a position.
Here is an example:
Relation: military rank
Context: the calais region was defended by the 15th army under " generaloberst " ( colonel general ) hans von salmuth , and normandy by the 7th army commanded by " generaloberst " friedrich dollmann .
Head Entity: hans von salmuth
Tail Entity: generaloberst
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "military rank":
Relation: military rank  
Context: during the battle of stalingrad, the soviet forces were led by general vasily chuikov, who held the rank of major general, while the german troops were commanded by general friedrich paulus.  
Head Entity: vasily chuikov  
Tail Entity: major general  

Relation: military rank  
Context: admiral horatio nelson was a key figure in the british navy during the napoleonic wars, famously holding the rank of vice admiral at the time of the battle of trafalgar.  
Head Entity: horatio nelson  
Tail Entity: vice admiral  
#### publisher ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "publisher" means organization or person responsible for publishing books, periodicals, printed music, podcasts, games or software.
Here is an example:
Relation: publisher
Context: she went on to the film school at san francisco state university , when she was hired as an artist for an atari game called " electrocop " .
Head Entity: electrocop
Tail Entity: atari
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "publisher":
Relation: publisher  
Context: The renowned author released her latest novel through a well-known publishing house that has been in the industry for decades.  
Head Entity: latest novel  
Tail Entity: publishing house  

Relation: publisher  
Context: After years of hard work, the independent game developer finally secured a deal with a major publisher to distribute their new title worldwide.  
Head Entity: new title  
Tail Entity: major publisher  
#### followed by ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "followed by" means immediately following item in a series of which the subject is a part.
Here is an example:
Relation: followed by
Context: " muzai moratorium " and " shōso strip " were released on vinyl in 2008 , as a part of her 10-year anniversary celebrations .
Head Entity: muzai moratorium
Tail Entity: shōso strip
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "followed by":
Relation: followed by  
Context: The first chapter of the book introduces the main character, while the second chapter follows with a detailed backstory.  
Head Entity: first chapter  
Tail Entity: second chapter  

Relation: followed by  
Context: In the concert lineup, the opening act performed a lively set, followed by the headliner who captivated the audience with their greatest hits.  
Head Entity: opening act  
Tail Entity: headliner  
#### work location ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "work location" means location where persons or organisations were actively participating in employment, business or other work.
Here is an example:
Relation: work location
Context: he was also associated with robert wilkinson in producing " londina illustrata " , an illustrated account of ancient buildings in london and westminster in two volumes ( 1819–25 ) .
Head Entity: robert wilkinson
Tail Entity: london
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "work location":
Relation: work location  
Context: The headquarters of the company is situated in the bustling city of San Francisco, where it has been operating since its inception in 2010.  
Head Entity: the company  
Tail Entity: San Francisco  

Relation: work location  
Context: During her tenure at the university, she conducted groundbreaking research in the field of neuroscience, primarily based in the labs at Cambridge.  
Head Entity: she  
Tail Entity: Cambridge  
#### occupation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "occupation" means occupation of a person.
Here is an example:
Relation: occupation
Context: andrei droznin ( russian : андрей дрознин ) is a russian theatre director and movement coach .
Head Entity: andrei droznin
Tail Entity: theatre director
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "occupation":
Relation: occupation  
Context: dr. emily thompson is a renowned biochemist known for her groundbreaking research in enzyme catalysis.  
Head Entity: dr. emily thompson  
Tail Entity: biochemist  

Relation: occupation  
Context: john smith, a former professional athlete, now works as a sports commentator for various television networks.  
Head Entity: john smith  
Tail Entity: sports commentator  
#### heritage designation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "heritage designation" means heritage designation of a cultural or natural site.
Here is an example:
Relation: heritage designation
Context: altamaha town , bluffton historic district , church of the cross , and rose hill plantation house are listed on the national register of historic places .
Head Entity: altamaha town
Tail Entity: national register of historic places
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "heritage designation":
Relation: heritage designation  
Context: The Great Wall of China is recognized as a UNESCO World Heritage Site due to its historical significance and architectural grandeur.  
Head Entity: Great Wall of China  
Tail Entity: UNESCO World Heritage Site  

Relation: heritage designation  
Context: The ancient city of Petra in Jordan is designated as a World Heritage Site, attracting tourists from around the globe.  
Head Entity: Petra  
Tail Entity: World Heritage Site  
#### voice type ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "voice type" means person's voice type.
Here is an example:
Relation: voice type
Context: ada adini ( 1855 – february 1924 ) was an american operatic soprano who had an active international career from 1876 up into the first decade of the 20th century .
Head Entity: ada adini
Tail Entity: soprano
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "voice type":
Relation: voice type  
Context: luciano pavarotti ( october 12, 1935 – september 6, 2007 ) was an italian operatic tenor who gained worldwide fame for his televised concerts and media appearances.  
Head Entity: luciano pavarotti  
Tail Entity: tenor  

Relation: voice type  
Context: taylor swift ( born december 13, 1989 ) is an american singer-songwriter known for her narrative songwriting, which often draws from her personal life, and her versatile vocal style.  
Head Entity: taylor swift  
Tail Entity: singer-songwriter  
MemoryTrain:  epoch 15, batch     0 | loss: 3.3158884MemoryTrain:  epoch 15, batch     1 | loss: 3.6698804MemoryTrain:  epoch 15, batch     2 | loss: 6.9706983MemoryTrain:  epoch 15, batch     3 | loss: 6.1438940MemoryTrain:  epoch 15, batch     4 | loss: 4.2252787MemoryTrain:  epoch 15, batch     5 | loss: 5.2611804MemoryTrain:  epoch 15, batch     6 | loss: 8.6612430MemoryTrain:  epoch 15, batch     7 | loss: 4.1614880MemoryTrain:  epoch 15, batch     8 | loss: 4.3838055MemoryTrain:  epoch  5, batch     9 | loss: 9.1260679MemoryTrain:  epoch 15, batch     0 | loss: 6.3609404MemoryTrain:  epoch 15, batch     1 | loss: 5.1852694MemoryTrain:  epoch 15, batch     2 | loss: 4.1001829MemoryTrain:  epoch 15, batch     3 | loss: 2.9157357MemoryTrain:  epoch 15, batch     4 | loss: 5.9616433MemoryTrain:  epoch 15, batch     5 | loss: 3.2974943MemoryTrain:  epoch 15, batch     6 | loss: 3.1039989MemoryTrain:  epoch 15, batch     7 | loss: 4.3023316MemoryTrain:  epoch 15, batch     8 | loss: 2.9813140MemoryTrain:  epoch  5, batch     9 | loss: 11.0532239MemoryTrain:  epoch 15, batch     0 | loss: 2.7964213MemoryTrain:  epoch 15, batch     1 | loss: 2.5336714MemoryTrain:  epoch 15, batch     2 | loss: 1.9195289MemoryTrain:  epoch 15, batch     3 | loss: 6.0114365MemoryTrain:  epoch 15, batch     4 | loss: 2.4384568MemoryTrain:  epoch 15, batch     5 | loss: 6.4728149MemoryTrain:  epoch 15, batch     6 | loss: 2.8965917MemoryTrain:  epoch 15, batch     7 | loss: 7.8748111MemoryTrain:  epoch 15, batch     8 | loss: 2.6184044MemoryTrain:  epoch  5, batch     9 | loss: 15.2273055MemoryTrain:  epoch 15, batch     0 | loss: 3.9925899MemoryTrain:  epoch 15, batch     1 | loss: 2.5417390MemoryTrain:  epoch 15, batch     2 | loss: 3.4806516MemoryTrain:  epoch 15, batch     3 | loss: 2.4345084MemoryTrain:  epoch 15, batch     4 | loss: 3.1157677MemoryTrain:  epoch 15, batch     5 | loss: 4.0341430MemoryTrain:  epoch 15, batch     6 | loss: 3.1497403MemoryTrain:  epoch 15, batch     7 | loss: 2.1204472MemoryTrain:  epoch 15, batch     8 | loss: 7.0627419MemoryTrain:  epoch  5, batch     9 | loss: 17.1796500MemoryTrain:  epoch 15, batch     0 | loss: 3.9114689MemoryTrain:  epoch 15, batch     1 | loss: 2.7570186MemoryTrain:  epoch 15, batch     2 | loss: 2.2427885MemoryTrain:  epoch 15, batch     3 | loss: 2.0575939MemoryTrain:  epoch 15, batch     4 | loss: 4.9968040MemoryTrain:  epoch 15, batch     5 | loss: 2.3825328MemoryTrain:  epoch 15, batch     6 | loss: 2.7094207MemoryTrain:  epoch 15, batch     7 | loss: 4.3456981MemoryTrain:  epoch 15, batch     8 | loss: 2.4685516MemoryTrain:  epoch  5, batch     9 | loss: 14.6295378MemoryTrain:  epoch 15, batch     0 | loss: 2.4631108MemoryTrain:  epoch 15, batch     1 | loss: 2.2362069MemoryTrain:  epoch 15, batch     2 | loss: 2.2436666MemoryTrain:  epoch 15, batch     3 | loss: 1.7592752MemoryTrain:  epoch 15, batch     4 | loss: 2.1961293MemoryTrain:  epoch 15, batch     5 | loss: 1.7301495MemoryTrain:  epoch 15, batch     6 | loss: 2.3614039MemoryTrain:  epoch 15, batch     7 | loss: 3.9185185MemoryTrain:  epoch 15, batch     8 | loss: 1.9172407MemoryTrain:  epoch  5, batch     9 | loss: 8.3764875MemoryTrain:  epoch 15, batch     0 | loss: 3.8207781MemoryTrain:  epoch 15, batch     1 | loss: 2.2152070MemoryTrain:  epoch 15, batch     2 | loss: 2.0192382MemoryTrain:  epoch 15, batch     3 | loss: 2.8949871MemoryTrain:  epoch 15, batch     4 | loss: 3.6124643MemoryTrain:  epoch 15, batch     5 | loss: 2.6379363MemoryTrain:  epoch 15, batch     6 | loss: 1.9382336MemoryTrain:  epoch 15, batch     7 | loss: 6.5745781MemoryTrain:  epoch 15, batch     8 | loss: 1.6529831MemoryTrain:  epoch  5, batch     9 | loss: 7.9596684MemoryTrain:  epoch 15, batch     0 | loss: 1.8390161MemoryTrain:  epoch 15, batch     1 | loss: 2.3819081MemoryTrain:  epoch 15, batch     2 | loss: 1.9888646MemoryTrain:  epoch 15, batch     3 | loss: 4.3535670MemoryTrain:  epoch 15, batch     4 | loss: 1.6244309MemoryTrain:  epoch 15, batch     5 | loss: 2.3621176MemoryTrain:  epoch 15, batch     6 | loss: 2.3622150MemoryTrain:  epoch 15, batch     7 | loss: 1.8955739MemoryTrain:  epoch 15, batch     8 | loss: 3.4037549MemoryTrain:  epoch  5, batch     9 | loss: 8.1796672MemoryTrain:  epoch 15, batch     0 | loss: 1.7818798MemoryTrain:  epoch 15, batch     1 | loss: 1.5331079MemoryTrain:  epoch 15, batch     2 | loss: 2.3053895MemoryTrain:  epoch 15, batch     3 | loss: 1.5906626MemoryTrain:  epoch 15, batch     4 | loss: 2.7091008MemoryTrain:  epoch 15, batch     5 | loss: 4.8242945MemoryTrain:  epoch 15, batch     6 | loss: 4.8805494MemoryTrain:  epoch 15, batch     7 | loss: 1.7300499MemoryTrain:  epoch 15, batch     8 | loss: 1.4809281MemoryTrain:  epoch  5, batch     9 | loss: 8.3420710MemoryTrain:  epoch 15, batch     0 | loss: 4.3447675MemoryTrain:  epoch 15, batch     1 | loss: 1.7678756MemoryTrain:  epoch 15, batch     2 | loss: 2.2646188MemoryTrain:  epoch 15, batch     3 | loss: 4.4198784MemoryTrain:  epoch 15, batch     4 | loss: 2.2475176MemoryTrain:  epoch 15, batch     5 | loss: 4.2621737MemoryTrain:  epoch 15, batch     6 | loss: 4.1002053MemoryTrain:  epoch 15, batch     7 | loss: 1.6627766MemoryTrain:  epoch 15, batch     8 | loss: 1.9930373MemoryTrain:  epoch  5, batch     9 | loss: 7.7905937
[EVAL] batch:    0 | acc: 25.00%,  total acc: 25.00%   [EVAL] batch:    1 | acc: 37.50%,  total acc: 31.25%   [EVAL] batch:    2 | acc: 43.75%,  total acc: 35.42%   [EVAL] batch:    3 | acc: 25.00%,  total acc: 32.81%   [EVAL] batch:    4 | acc: 25.00%,  total acc: 31.25%   [EVAL] batch:    5 | acc: 43.75%,  total acc: 33.33%   [EVAL] batch:    6 | acc: 50.00%,  total acc: 35.71%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 41.41%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 44.44%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 47.50%   [EVAL] batch:   10 | acc: 50.00%,  total acc: 47.73%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 50.00%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 53.37%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 56.25%   [EVAL] batch:   14 | acc: 87.50%,  total acc: 58.33%   [EVAL] batch:   15 | acc: 81.25%,  total acc: 59.77%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 61.76%   [EVAL] batch:   17 | acc: 100.00%,  total acc: 63.89%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 65.79%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 66.88%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 68.45%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 69.89%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 70.92%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 71.88%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 73.00%   [EVAL] batch:   25 | acc: 75.00%,  total acc: 73.08%   [EVAL] batch:   26 | acc: 75.00%,  total acc: 73.15%   [EVAL] batch:   27 | acc: 75.00%,  total acc: 73.21%   [EVAL] batch:   28 | acc: 62.50%,  total acc: 72.84%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 73.33%   [EVAL] batch:   30 | acc: 75.00%,  total acc: 73.39%   [EVAL] batch:   31 | acc: 68.75%,  total acc: 73.24%   [EVAL] batch:   32 | acc: 62.50%,  total acc: 72.92%   [EVAL] batch:   33 | acc: 43.75%,  total acc: 72.06%   [EVAL] batch:   34 | acc: 43.75%,  total acc: 71.25%   [EVAL] batch:   35 | acc: 25.00%,  total acc: 69.97%   [EVAL] batch:   36 | acc: 43.75%,  total acc: 69.26%   [EVAL] batch:   37 | acc: 68.75%,  total acc: 69.24%   [EVAL] batch:   38 | acc: 68.75%,  total acc: 69.23%   [EVAL] batch:   39 | acc: 87.50%,  total acc: 69.69%   [EVAL] batch:   40 | acc: 68.75%,  total acc: 69.66%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 70.09%   [EVAL] batch:   42 | acc: 68.75%,  total acc: 70.06%   [EVAL] batch:   43 | acc: 75.00%,  total acc: 70.17%   [EVAL] batch:   44 | acc: 81.25%,  total acc: 70.42%   [EVAL] batch:   45 | acc: 68.75%,  total acc: 70.38%   [EVAL] batch:   46 | acc: 62.50%,  total acc: 70.21%   [EVAL] batch:   47 | acc: 56.25%,  total acc: 69.92%   [EVAL] batch:   48 | acc: 81.25%,  total acc: 70.15%   [EVAL] batch:   49 | acc: 81.25%,  total acc: 70.38%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 70.96%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 71.51%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 72.05%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 72.57%   [EVAL] batch:   54 | acc: 100.00%,  total acc: 73.07%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 73.55%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 74.01%   [EVAL] batch:   57 | acc: 100.00%,  total acc: 74.46%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 74.89%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 75.31%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 75.72%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 76.11%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 75.69%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 84.38%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 82.50%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:    6 | acc: 50.00%,  total acc: 79.46%   [EVAL] batch:    7 | acc: 43.75%,  total acc: 75.00%   [EVAL] batch:    8 | acc: 50.00%,  total acc: 72.22%   [EVAL] batch:    9 | acc: 56.25%,  total acc: 70.62%   [EVAL] batch:   10 | acc: 37.50%,  total acc: 67.61%   [EVAL] batch:   11 | acc: 43.75%,  total acc: 65.62%   [EVAL] batch:   12 | acc: 50.00%,  total acc: 64.42%   [EVAL] batch:   13 | acc: 81.25%,  total acc: 65.62%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 67.50%   [EVAL] batch:   15 | acc: 93.75%,  total acc: 69.14%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 70.59%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 71.88%   [EVAL] batch:   18 | acc: 87.50%,  total acc: 72.70%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 72.81%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 73.51%   [EVAL] batch:   21 | acc: 68.75%,  total acc: 73.30%   [EVAL] batch:   22 | acc: 56.25%,  total acc: 72.55%   [EVAL] batch:   23 | acc: 81.25%,  total acc: 72.92%   [EVAL] batch:   24 | acc: 68.75%,  total acc: 72.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 73.80%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 74.77%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 75.67%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 76.29%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 77.08%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 77.82%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 78.32%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 78.98%   [EVAL] batch:   33 | acc: 68.75%,  total acc: 78.68%   [EVAL] batch:   34 | acc: 81.25%,  total acc: 78.75%   [EVAL] batch:   35 | acc: 81.25%,  total acc: 78.82%   [EVAL] batch:   36 | acc: 87.50%,  total acc: 79.05%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 79.44%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 79.97%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 80.47%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 80.95%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 81.54%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 81.53%   [EVAL] batch:   44 | acc: 56.25%,  total acc: 80.97%   [EVAL] batch:   45 | acc: 50.00%,  total acc: 80.30%   [EVAL] batch:   46 | acc: 37.50%,  total acc: 79.39%   [EVAL] batch:   47 | acc: 62.50%,  total acc: 79.04%   [EVAL] batch:   48 | acc: 56.25%,  total acc: 78.57%   [EVAL] batch:   49 | acc: 50.00%,  total acc: 78.00%   [EVAL] batch:   50 | acc: 81.25%,  total acc: 78.06%   [EVAL] batch:   51 | acc: 81.25%,  total acc: 78.12%   [EVAL] batch:   52 | acc: 87.50%,  total acc: 78.30%   [EVAL] batch:   53 | acc: 68.75%,  total acc: 78.12%   [EVAL] batch:   54 | acc: 56.25%,  total acc: 77.73%   [EVAL] batch:   55 | acc: 81.25%,  total acc: 77.79%   [EVAL] batch:   56 | acc: 75.00%,  total acc: 77.74%   [EVAL] batch:   57 | acc: 81.25%,  total acc: 77.80%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 78.18%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 78.44%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 78.79%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 78.93%   [EVAL] batch:   62 | acc: 62.50%,  total acc: 78.67%   [EVAL] batch:   63 | acc: 25.00%,  total acc: 77.83%   [EVAL] batch:   64 | acc: 25.00%,  total acc: 77.02%   [EVAL] batch:   65 | acc: 37.50%,  total acc: 76.42%   [EVAL] batch:   66 | acc: 31.25%,  total acc: 75.75%   [EVAL] batch:   67 | acc: 25.00%,  total acc: 75.00%   [EVAL] batch:   68 | acc: 62.50%,  total acc: 74.82%   [EVAL] batch:   69 | acc: 75.00%,  total acc: 74.82%   [EVAL] batch:   70 | acc: 87.50%,  total acc: 75.00%   [EVAL] batch:   71 | acc: 81.25%,  total acc: 75.09%   [EVAL] batch:   72 | acc: 87.50%,  total acc: 75.26%   [EVAL] batch:   73 | acc: 75.00%,  total acc: 75.25%   [EVAL] batch:   74 | acc: 81.25%,  total acc: 75.33%   [EVAL] batch:   75 | acc: 100.00%,  total acc: 75.66%   [EVAL] batch:   76 | acc: 81.25%,  total acc: 75.73%   [EVAL] batch:   77 | acc: 87.50%,  total acc: 75.88%   [EVAL] batch:   78 | acc: 93.75%,  total acc: 76.11%   [EVAL] batch:   79 | acc: 93.75%,  total acc: 76.33%   [EVAL] batch:   80 | acc: 87.50%,  total acc: 76.47%   [EVAL] batch:   81 | acc: 37.50%,  total acc: 75.99%   [EVAL] batch:   82 | acc: 25.00%,  total acc: 75.38%   [EVAL] batch:   83 | acc: 18.75%,  total acc: 74.70%   [EVAL] batch:   84 | acc: 56.25%,  total acc: 74.49%   [EVAL] batch:   85 | acc: 37.50%,  total acc: 74.06%   [EVAL] batch:   86 | acc: 43.75%,  total acc: 73.71%   [EVAL] batch:   87 | acc: 18.75%,  total acc: 73.08%   [EVAL] batch:   88 | acc: 0.00%,  total acc: 72.26%   [EVAL] batch:   89 | acc: 37.50%,  total acc: 71.88%   [EVAL] batch:   90 | acc: 0.00%,  total acc: 71.09%   [EVAL] batch:   91 | acc: 18.75%,  total acc: 70.52%   [EVAL] batch:   92 | acc: 6.25%,  total acc: 69.83%   [EVAL] batch:   93 | acc: 25.00%,  total acc: 69.35%   [EVAL] batch:   94 | acc: 81.25%,  total acc: 69.47%   [EVAL] batch:   95 | acc: 93.75%,  total acc: 69.73%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 70.04%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 70.34%   [EVAL] batch:   98 | acc: 93.75%,  total acc: 70.58%   [EVAL] batch:   99 | acc: 81.25%,  total acc: 70.69%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 70.98%   [EVAL] batch:  101 | acc: 93.75%,  total acc: 71.20%   [EVAL] batch:  102 | acc: 87.50%,  total acc: 71.36%   [EVAL] batch:  103 | acc: 100.00%,  total acc: 71.63%   [EVAL] batch:  104 | acc: 93.75%,  total acc: 71.85%   [EVAL] batch:  105 | acc: 100.00%,  total acc: 72.11%   [EVAL] batch:  106 | acc: 56.25%,  total acc: 71.96%   [EVAL] batch:  107 | acc: 25.00%,  total acc: 71.53%   [EVAL] batch:  108 | acc: 31.25%,  total acc: 71.16%   [EVAL] batch:  109 | acc: 25.00%,  total acc: 70.74%   [EVAL] batch:  110 | acc: 37.50%,  total acc: 70.44%   [EVAL] batch:  111 | acc: 43.75%,  total acc: 70.20%   [EVAL] batch:  112 | acc: 37.50%,  total acc: 69.91%   [EVAL] batch:  113 | acc: 68.75%,  total acc: 69.90%   [EVAL] batch:  114 | acc: 62.50%,  total acc: 69.84%   [EVAL] batch:  115 | acc: 87.50%,  total acc: 69.99%   [EVAL] batch:  116 | acc: 75.00%,  total acc: 70.03%   [EVAL] batch:  117 | acc: 87.50%,  total acc: 70.18%   [EVAL] batch:  118 | acc: 62.50%,  total acc: 70.12%   [EVAL] batch:  119 | acc: 50.00%,  total acc: 69.95%   [EVAL] batch:  120 | acc: 56.25%,  total acc: 69.83%   [EVAL] batch:  121 | acc: 62.50%,  total acc: 69.77%   [EVAL] batch:  122 | acc: 56.25%,  total acc: 69.66%   [EVAL] batch:  123 | acc: 68.75%,  total acc: 69.66%   [EVAL] batch:  124 | acc: 62.50%,  total acc: 69.60%   [EVAL] batch:  125 | acc: 81.25%,  total acc: 69.69%   [EVAL] batch:  126 | acc: 75.00%,  total acc: 69.73%   [EVAL] batch:  127 | acc: 56.25%,  total acc: 69.63%   [EVAL] batch:  128 | acc: 31.25%,  total acc: 69.33%   [EVAL] batch:  129 | acc: 81.25%,  total acc: 69.42%   [EVAL] batch:  130 | acc: 81.25%,  total acc: 69.51%   [EVAL] batch:  131 | acc: 81.25%,  total acc: 69.60%   [EVAL] batch:  132 | acc: 75.00%,  total acc: 69.64%   [EVAL] batch:  133 | acc: 87.50%,  total acc: 69.78%   [EVAL] batch:  134 | acc: 75.00%,  total acc: 69.81%   [EVAL] batch:  135 | acc: 75.00%,  total acc: 69.85%   [EVAL] batch:  136 | acc: 50.00%,  total acc: 69.71%   [EVAL] batch:  137 | acc: 87.50%,  total acc: 69.84%   [EVAL] batch:  138 | acc: 81.25%,  total acc: 69.92%   [EVAL] batch:  139 | acc: 56.25%,  total acc: 69.82%   [EVAL] batch:  140 | acc: 50.00%,  total acc: 69.68%   [EVAL] batch:  141 | acc: 62.50%,  total acc: 69.63%   [EVAL] batch:  142 | acc: 62.50%,  total acc: 69.58%   [EVAL] batch:  143 | acc: 81.25%,  total acc: 69.66%   [EVAL] batch:  144 | acc: 31.25%,  total acc: 69.40%   [EVAL] batch:  145 | acc: 50.00%,  total acc: 69.26%   [EVAL] batch:  146 | acc: 31.25%,  total acc: 69.01%   [EVAL] batch:  147 | acc: 43.75%,  total acc: 68.83%   [EVAL] batch:  148 | acc: 43.75%,  total acc: 68.67%   [EVAL] batch:  149 | acc: 37.50%,  total acc: 68.46%   [EVAL] batch:  150 | acc: 81.25%,  total acc: 68.54%   [EVAL] batch:  151 | acc: 81.25%,  total acc: 68.63%   [EVAL] batch:  152 | acc: 62.50%,  total acc: 68.59%   [EVAL] batch:  153 | acc: 62.50%,  total acc: 68.55%   [EVAL] batch:  154 | acc: 56.25%,  total acc: 68.47%   [EVAL] batch:  155 | acc: 68.75%,  total acc: 68.47%   [EVAL] batch:  156 | acc: 62.50%,  total acc: 68.43%   [EVAL] batch:  157 | acc: 56.25%,  total acc: 68.35%   [EVAL] batch:  158 | acc: 56.25%,  total acc: 68.28%   [EVAL] batch:  159 | acc: 62.50%,  total acc: 68.24%   [EVAL] batch:  160 | acc: 62.50%,  total acc: 68.21%   [EVAL] batch:  161 | acc: 62.50%,  total acc: 68.17%   [EVAL] batch:  162 | acc: 43.75%,  total acc: 68.02%   [EVAL] batch:  163 | acc: 56.25%,  total acc: 67.95%   [EVAL] batch:  164 | acc: 56.25%,  total acc: 67.88%   [EVAL] batch:  165 | acc: 50.00%,  total acc: 67.77%   [EVAL] batch:  166 | acc: 56.25%,  total acc: 67.70%   [EVAL] batch:  167 | acc: 62.50%,  total acc: 67.67%   [EVAL] batch:  168 | acc: 56.25%,  total acc: 67.60%   [EVAL] batch:  169 | acc: 43.75%,  total acc: 67.46%   [EVAL] batch:  170 | acc: 37.50%,  total acc: 67.29%   [EVAL] batch:  171 | acc: 18.75%,  total acc: 67.01%   [EVAL] batch:  172 | acc: 50.00%,  total acc: 66.91%   [EVAL] batch:  173 | acc: 25.00%,  total acc: 66.67%   [EVAL] batch:  174 | acc: 50.00%,  total acc: 66.57%   [EVAL] batch:  175 | acc: 100.00%,  total acc: 66.76%   [EVAL] batch:  176 | acc: 100.00%,  total acc: 66.95%   [EVAL] batch:  177 | acc: 100.00%,  total acc: 67.13%   [EVAL] batch:  178 | acc: 100.00%,  total acc: 67.32%   [EVAL] batch:  179 | acc: 100.00%,  total acc: 67.50%   [EVAL] batch:  180 | acc: 100.00%,  total acc: 67.68%   [EVAL] batch:  181 | acc: 100.00%,  total acc: 67.86%   [EVAL] batch:  182 | acc: 100.00%,  total acc: 68.03%   [EVAL] batch:  183 | acc: 100.00%,  total acc: 68.21%   [EVAL] batch:  184 | acc: 100.00%,  total acc: 68.38%   [EVAL] batch:  185 | acc: 100.00%,  total acc: 68.55%   [EVAL] batch:  186 | acc: 100.00%,  total acc: 68.72%   [EVAL] batch:  187 | acc: 93.75%,  total acc: 68.85%   [EVAL] batch:  188 | acc: 75.00%,  total acc: 68.88%   [EVAL] batch:  189 | acc: 81.25%,  total acc: 68.95%   [EVAL] batch:  190 | acc: 81.25%,  total acc: 69.01%   [EVAL] batch:  191 | acc: 62.50%,  total acc: 68.98%   [EVAL] batch:  192 | acc: 81.25%,  total acc: 69.04%   [EVAL] batch:  193 | acc: 81.25%,  total acc: 69.10%   [EVAL] batch:  194 | acc: 81.25%,  total acc: 69.17%   [EVAL] batch:  195 | acc: 81.25%,  total acc: 69.23%   [EVAL] batch:  196 | acc: 62.50%,  total acc: 69.19%   [EVAL] batch:  197 | acc: 93.75%,  total acc: 69.32%   [EVAL] batch:  198 | acc: 75.00%,  total acc: 69.35%   [EVAL] batch:  199 | acc: 81.25%,  total acc: 69.41%   [EVAL] batch:  200 | acc: 87.50%,  total acc: 69.50%   [EVAL] batch:  201 | acc: 81.25%,  total acc: 69.55%   [EVAL] batch:  202 | acc: 75.00%,  total acc: 69.58%   [EVAL] batch:  203 | acc: 87.50%,  total acc: 69.67%   [EVAL] batch:  204 | acc: 81.25%,  total acc: 69.73%   [EVAL] batch:  205 | acc: 93.75%,  total acc: 69.84%   [EVAL] batch:  206 | acc: 31.25%,  total acc: 69.66%   [EVAL] batch:  207 | acc: 12.50%,  total acc: 69.38%   [EVAL] batch:  208 | acc: 37.50%,  total acc: 69.23%   [EVAL] batch:  209 | acc: 18.75%,  total acc: 68.99%   [EVAL] batch:  210 | acc: 18.75%,  total acc: 68.75%   [EVAL] batch:  211 | acc: 37.50%,  total acc: 68.60%   [EVAL] batch:  212 | acc: 56.25%,  total acc: 68.54%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 68.69%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 68.84%   [EVAL] batch:  215 | acc: 87.50%,  total acc: 68.92%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 69.07%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 69.21%   [EVAL] batch:  218 | acc: 100.00%,  total acc: 69.35%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 69.49%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 69.63%   [EVAL] batch:  221 | acc: 100.00%,  total acc: 69.76%   [EVAL] batch:  222 | acc: 100.00%,  total acc: 69.90%   [EVAL] batch:  223 | acc: 100.00%,  total acc: 70.03%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 70.17%   [EVAL] batch:  225 | acc: 93.75%,  total acc: 70.27%   [EVAL] batch:  226 | acc: 93.75%,  total acc: 70.37%   [EVAL] batch:  227 | acc: 93.75%,  total acc: 70.48%   [EVAL] batch:  228 | acc: 93.75%,  total acc: 70.58%   [EVAL] batch:  229 | acc: 100.00%,  total acc: 70.71%   [EVAL] batch:  230 | acc: 93.75%,  total acc: 70.81%   [EVAL] batch:  231 | acc: 68.75%,  total acc: 70.80%   [EVAL] batch:  232 | acc: 56.25%,  total acc: 70.73%   [EVAL] batch:  233 | acc: 75.00%,  total acc: 70.75%   [EVAL] batch:  234 | acc: 56.25%,  total acc: 70.69%   [EVAL] batch:  235 | acc: 62.50%,  total acc: 70.66%   [EVAL] batch:  236 | acc: 62.50%,  total acc: 70.62%   [EVAL] batch:  237 | acc: 100.00%,  total acc: 70.75%   [EVAL] batch:  238 | acc: 87.50%,  total acc: 70.82%   [EVAL] batch:  239 | acc: 93.75%,  total acc: 70.91%   [EVAL] batch:  240 | acc: 87.50%,  total acc: 70.98%   [EVAL] batch:  241 | acc: 87.50%,  total acc: 71.05%   [EVAL] batch:  242 | acc: 93.75%,  total acc: 71.14%   [EVAL] batch:  243 | acc: 87.50%,  total acc: 71.21%   [EVAL] batch:  244 | acc: 50.00%,  total acc: 71.12%   [EVAL] batch:  245 | acc: 62.50%,  total acc: 71.09%   [EVAL] batch:  246 | acc: 43.75%,  total acc: 70.98%   [EVAL] batch:  247 | acc: 37.50%,  total acc: 70.84%   [EVAL] batch:  248 | acc: 43.75%,  total acc: 70.73%   [EVAL] batch:  249 | acc: 50.00%,  total acc: 70.65%   [EVAL] batch:  250 | acc: 25.00%,  total acc: 70.47%   [EVAL] batch:  251 | acc: 37.50%,  total acc: 70.34%   [EVAL] batch:  252 | acc: 43.75%,  total acc: 70.23%   [EVAL] batch:  253 | acc: 25.00%,  total acc: 70.05%   [EVAL] batch:  254 | acc: 25.00%,  total acc: 69.88%   [EVAL] batch:  255 | acc: 43.75%,  total acc: 69.78%   [EVAL] batch:  256 | acc: 50.00%,  total acc: 69.70%   [EVAL] batch:  257 | acc: 81.25%,  total acc: 69.74%   [EVAL] batch:  258 | acc: 68.75%,  total acc: 69.74%   [EVAL] batch:  259 | acc: 75.00%,  total acc: 69.76%   [EVAL] batch:  260 | acc: 50.00%,  total acc: 69.68%   [EVAL] batch:  261 | acc: 75.00%,  total acc: 69.70%   [EVAL] batch:  262 | acc: 93.75%,  total acc: 69.80%   [EVAL] batch:  263 | acc: 93.75%,  total acc: 69.89%   [EVAL] batch:  264 | acc: 87.50%,  total acc: 69.95%   [EVAL] batch:  265 | acc: 81.25%,  total acc: 70.00%   [EVAL] batch:  266 | acc: 93.75%,  total acc: 70.08%   [EVAL] batch:  267 | acc: 100.00%,  total acc: 70.20%   [EVAL] batch:  268 | acc: 100.00%,  total acc: 70.31%   [EVAL] batch:  269 | acc: 87.50%,  total acc: 70.37%   [EVAL] batch:  270 | acc: 100.00%,  total acc: 70.48%   [EVAL] batch:  271 | acc: 100.00%,  total acc: 70.59%   [EVAL] batch:  272 | acc: 93.75%,  total acc: 70.67%   [EVAL] batch:  273 | acc: 93.75%,  total acc: 70.76%   [EVAL] batch:  274 | acc: 100.00%,  total acc: 70.86%   [EVAL] batch:  275 | acc: 75.00%,  total acc: 70.88%   [EVAL] batch:  276 | acc: 75.00%,  total acc: 70.89%   [EVAL] batch:  277 | acc: 75.00%,  total acc: 70.91%   [EVAL] batch:  278 | acc: 62.50%,  total acc: 70.88%   [EVAL] batch:  279 | acc: 87.50%,  total acc: 70.94%   [EVAL] batch:  280 | acc: 75.00%,  total acc: 70.95%   [EVAL] batch:  281 | acc: 68.75%,  total acc: 70.94%   [EVAL] batch:  282 | acc: 62.50%,  total acc: 70.91%   [EVAL] batch:  283 | acc: 43.75%,  total acc: 70.82%   [EVAL] batch:  284 | acc: 43.75%,  total acc: 70.72%   [EVAL] batch:  285 | acc: 25.00%,  total acc: 70.56%   [EVAL] batch:  286 | acc: 43.75%,  total acc: 70.47%   [EVAL] batch:  287 | acc: 68.75%,  total acc: 70.46%   [EVAL] batch:  288 | acc: 68.75%,  total acc: 70.46%   [EVAL] batch:  289 | acc: 87.50%,  total acc: 70.52%   [EVAL] batch:  290 | acc: 68.75%,  total acc: 70.51%   [EVAL] batch:  291 | acc: 87.50%,  total acc: 70.57%   [EVAL] batch:  292 | acc: 68.75%,  total acc: 70.56%   [EVAL] batch:  293 | acc: 75.00%,  total acc: 70.58%   [EVAL] batch:  294 | acc: 81.25%,  total acc: 70.61%   [EVAL] batch:  295 | acc: 68.75%,  total acc: 70.61%   [EVAL] batch:  296 | acc: 62.50%,  total acc: 70.58%   [EVAL] batch:  297 | acc: 56.25%,  total acc: 70.53%   [EVAL] batch:  298 | acc: 81.25%,  total acc: 70.57%   [EVAL] batch:  299 | acc: 81.25%,  total acc: 70.60%   [EVAL] batch:  300 | acc: 100.00%,  total acc: 70.70%   [EVAL] batch:  301 | acc: 100.00%,  total acc: 70.80%   [EVAL] batch:  302 | acc: 100.00%,  total acc: 70.90%   [EVAL] batch:  303 | acc: 100.00%,  total acc: 70.99%   [EVAL] batch:  304 | acc: 100.00%,  total acc: 71.09%   [EVAL] batch:  305 | acc: 100.00%,  total acc: 71.18%   [EVAL] batch:  306 | acc: 100.00%,  total acc: 71.27%   [EVAL] batch:  307 | acc: 100.00%,  total acc: 71.37%   [EVAL] batch:  308 | acc: 100.00%,  total acc: 71.46%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 71.55%   [EVAL] batch:  310 | acc: 100.00%,  total acc: 71.64%   [EVAL] batch:  311 | acc: 100.00%,  total acc: 71.73%   [EVAL] batch:  312 | acc: 50.00%,  total acc: 71.67%   
cur_acc:  ['0.9494', '0.7431', '0.7530', '0.7708', '0.7569']
his_acc:  ['0.9494', '0.8230', '0.7680', '0.7475', '0.7167']
CurrentTrain: epoch 15, batch     0 | loss: 22.0911969CurrentTrain: epoch 15, batch     1 | loss: 19.7284506CurrentTrain: epoch 15, batch     2 | loss: 19.9907682CurrentTrain: epoch  1, batch     3 | loss: 10.3956991CurrentTrain: epoch 15, batch     0 | loss: 15.8760469CurrentTrain: epoch 15, batch     1 | loss: 14.1498059CurrentTrain: epoch 15, batch     2 | loss: 14.1003617CurrentTrain: epoch  1, batch     3 | loss: 7.1178179CurrentTrain: epoch 15, batch     0 | loss: 16.6343467CurrentTrain: epoch 15, batch     1 | loss: 18.4776738CurrentTrain: epoch 15, batch     2 | loss: 14.3055166CurrentTrain: epoch  1, batch     3 | loss: 6.7165852CurrentTrain: epoch 15, batch     0 | loss: 9.4477666CurrentTrain: epoch 15, batch     1 | loss: 18.1166827CurrentTrain: epoch 15, batch     2 | loss: 10.3865742CurrentTrain: epoch  1, batch     3 | loss: 7.0184827CurrentTrain: epoch 15, batch     0 | loss: 13.2436805CurrentTrain: epoch 15, batch     1 | loss: 14.7914098CurrentTrain: epoch 15, batch     2 | loss: 13.3811421CurrentTrain: epoch  1, batch     3 | loss: 9.0252226CurrentTrain: epoch 15, batch     0 | loss: 13.3895303CurrentTrain: epoch 15, batch     1 | loss: 10.5829824CurrentTrain: epoch 15, batch     2 | loss: 15.5776936CurrentTrain: epoch  1, batch     3 | loss: 9.0346264CurrentTrain: epoch 15, batch     0 | loss: 8.7042703CurrentTrain: epoch 15, batch     1 | loss: 18.6144305CurrentTrain: epoch 15, batch     2 | loss: 11.0441275CurrentTrain: epoch  1, batch     3 | loss: 8.2274707CurrentTrain: epoch 15, batch     0 | loss: 9.9715970CurrentTrain: epoch 15, batch     1 | loss: 13.3721064CurrentTrain: epoch 15, batch     2 | loss: 10.1561131CurrentTrain: epoch  1, batch     3 | loss: 8.4856657CurrentTrain: epoch 15, batch     0 | loss: 15.5087383CurrentTrain: epoch 15, batch     1 | loss: 15.2936814CurrentTrain: epoch 15, batch     2 | loss: 10.8812407CurrentTrain: epoch  1, batch     3 | loss: 7.9156760CurrentTrain: epoch 15, batch     0 | loss: 11.3646303CurrentTrain: epoch 15, batch     1 | loss: 10.8204443CurrentTrain: epoch 15, batch     2 | loss: 16.7630748CurrentTrain: epoch  1, batch     3 | loss: 5.6936099
#### main subject ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "main subject" means primary topic of a work.
Here is an example:
Relation: main subject
Context: bryson 's best known work is his 1985 book " evil angels " which chronicles the story of lindy chamberlain 's trial for murder , following the death of her baby daughter , azaria .
Head Entity: evil angels
Tail Entity: death
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "main subject":
Relation: main subject  
Context: in her groundbreaking 2010 book "the immortal life of henrietta lacks," rebecca skloot explores the life of henrietta lacks and the impact of her cells on medical research.  
Head Entity: the immortal life of henrietta lacks  
Tail Entity: henrietta lacks  

Relation: main subject  
Context: "sapiens: a brief history of humankind" by yuval noah harari examines the history and impact of homo sapiens from the emergence of our species to the present day.  
Head Entity: sapiens: a brief history of humankind  
Tail Entity: homo sapiens  
#### participating team ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "participating team" means teams that actively takes/took part in an event or process.
Here is an example:
Relation: participating team
Context: the two teams had met in two previous matches , including in the 2006 world cup group stage , won by brazil 1–0 .
Head Entity: 2006 world cup
Tail Entity: brazil
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "participating team":
Relation: participating team  
Context: In the 2020 UEFA European Championship, Italy and England faced off in the final, with Italy emerging victorious after a tense penalty shootout.  
Head Entity: 2020 UEFA European Championship  
Tail Entity: Italy  

Relation: participating team  
Context: The 2018 FIFA World Cup saw France and Croatia compete in an exhilarating final, where France claimed their second title.  
Head Entity: 2018 FIFA World Cup  
Tail Entity: France  
#### has part ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "has part" means part of this subject.
Here is an example:
Relation: has part
Context: among the competing artists was previous eurovision song contest entrant robin juhkental who represented estonia in 2010 as part of the group malcolm lincoln .
Head Entity: malcolm lincoln
Tail Entity: robin juhkental
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "has part":
Relation: has part  
Context: The famous painting "The Last Supper" features several apostles, including Judas Iscariot, who is depicted as part of the group gathered around Jesus.  
Head Entity: The Last Supper  
Tail Entity: Judas Iscariot  

Relation: has part  
Context: The smartphone model Galaxy S21 includes various components, such as the Exynos 2100 processor, which is an integral part of its performance.  
Head Entity: Galaxy S21  
Tail Entity: Exynos 2100 processor  
#### nominated for ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "nominated for" means award nomination received by a person, organisation or creative work.
Here is an example:
Relation: nominated for
Context: he was nominated for the academy award for best picture , along with steven spielberg , ian bryce , mark gordon for the film " saving private ryan " .
Head Entity: ian bryce
Tail Entity: academy award for best picture
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "nominated for":
Relation: nominated for  
Context: The film "The Shape of Water" received multiple nominations, including a nod for best director for Guillermo del Toro at the Academy Awards.  
Head Entity: Guillermo del Toro  
Tail Entity: Academy Awards  

Relation: nominated for  
Context: The popular band was nominated for the Grammy Award for Best New Artist after their debut album topped the charts.  
Head Entity: the popular band  
Tail Entity: Grammy Award for Best New Artist  
#### instance of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "instance of" means that class of which this subject is a particular example and member.
Here is an example:
Relation: instance of
Context: in 2007 , the county municipality created the transit authority called " skyss " that would market public transport while is would be operated by private companies based on public service obligation contracts .
Head Entity: skyss
Tail Entity: transit authority
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "instance of":
Relation: instance of  
Context: The smartphone known as the iPhone was first released by Apple Inc. in 2007, revolutionizing the mobile phone industry and setting new standards for design and functionality.  
Head Entity: iPhone  
Tail Entity: smartphone  

Relation: instance of  
Context: The Great Wall of China is a series of fortifications made of various materials, built to protect the Chinese states and empires from invasions and raids.  
Head Entity: Great Wall of China  
Tail Entity: fortification  
#### mountain range ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "mountain range" means range or subrange to which the geographical item belongs.
Here is an example:
Relation: mountain range
Context: abus valley ( ) is an ice - free valley southeast of turnstile ridge at the north end of britannia range .
Head Entity: turnstile ridge
Tail Entity: britannia range
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "mountain range":
Relation: mountain range  
Context: the sierra nevada ( ) is a mountain range in the western united states, primarily in the state of california, and it includes the famous yosemite national park, which is located within the range.  
Head Entity: sierra nevada  
Tail Entity: yosemite national park  

Relation: mountain range  
Context: the appalachian mountains ( ) extend from the canadian province of quebec down to alabama, making it one of the longest mountain ranges in north america.  
Head Entity: appalachian mountains  
Tail Entity: alabama  
#### screenwriter ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "screenwriter" means person who wrote the script for subject item.
Here is an example:
Relation: screenwriter
Context: future films include " mcqueen " , ian bonhote ’s documentary about the fashion designer alexander mcqueen , brad anderson ’s thriller " beirut " , and mark pellington ’s drama " nostalgia " .
Head Entity: nostalgia
Tail Entity: mark pellington
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "screenwriter":
Relation: screenwriter  
Context: the upcoming movie "the great adventure" is penned by renowned screenwriter jessica taylor, who has previously won multiple awards for her work in the industry.  
Head Entity: the great adventure  
Tail Entity: jessica taylor  

Relation: screenwriter  
Context: in the latest film festival, "the lost city" received accolades for its screenplay, which was crafted by the talented screenwriter, robert lang.  
Head Entity: the lost city  
Tail Entity: robert lang  
#### language of work or name ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "language of work or name" means language associated with this creative work (such as books, shows, songs, broadcasts or websites) or a name.
Here is an example:
Relation: language of work or name
Context: the publisher found success in 1993 with tarō gomi 's " everyone poops " , a japanese language import that sold over one million copies and is now their best - known title .
Head Entity: everyone poops
Tail Entity: japanese language
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "language of work or name":
Relation: language of work or name  
Context: The animated series "Avatar: The Last Airbender" features a rich narrative and is primarily produced in English, appealing to a global audience.  
Head Entity: Avatar: The Last Airbender  
Tail Entity: English  

Relation: language of work or name  
Context: The famous novel "Cien años de soledad" by Gabriel García Márquez is a cornerstone of Latin American literature and is originally written in Spanish.  
Head Entity: Cien años de soledad  
Tail Entity: Spanish  
#### operator ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "operator" means person, profession, or organization that operates the equipment, facility, or service.
Here is an example:
Relation: operator
Context: the aircraft was a boeing 707 - 321c that first entered service with pan american world airways ( pan am ) in 1963 .
Head Entity: boeing 707 - 321c
Tail Entity: pan american world airways
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "operator":
Relation: operator  
Context: the new electric bus fleet is managed by the city transportation authority, which ensures efficient public transit services.  
Head Entity: electric bus fleet  
Tail Entity: city transportation authority  

Relation: operator  
Context: the state-of-the-art research facility is operated by the national science foundation, providing resources for groundbreaking studies.  
Head Entity: research facility  
Tail Entity: national science foundation  
#### religion ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "religion" means religion of a person, organization or religious building, or associated with this subject.
Here is an example:
Relation: religion
Context: the most important lama of this series was the third changkya , rolpai dorje , who was preceptor to the qianlong emperor of china , and chief representative of tibetan buddhism at the qing court .
Head Entity: rolpai dorje
Tail Entity: tibetan buddhism
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "religion":
Relation: religion  
Context: the cathedral of notre-dame is a famous example of french gothic architecture and serves as the seat of the archbishop of paris, representing the roman catholic faith in the heart of the city.  
Head Entity: cathedral of notre-dame  
Tail Entity: roman catholic faith  

Relation: religion  
Context: the dalai lama is a prominent figure in the world, known for his teachings on compassion and non-violence, and is the spiritual leader of tibetan buddhism.  
Head Entity: dalai lama  
Tail Entity: tibetan buddhism  
MemoryTrain:  epoch 15, batch     0 | loss: 4.2148437MemoryTrain:  epoch 15, batch     1 | loss: 3.2258360MemoryTrain:  epoch 15, batch     2 | loss: 4.3256383MemoryTrain:  epoch 15, batch     3 | loss: 6.2446256MemoryTrain:  epoch 15, batch     4 | loss: 5.6693292MemoryTrain:  epoch 15, batch     5 | loss: 4.0711560MemoryTrain:  epoch 15, batch     6 | loss: 4.0793636MemoryTrain:  epoch 15, batch     7 | loss: 3.3462946MemoryTrain:  epoch 15, batch     8 | loss: 5.4901615MemoryTrain:  epoch 15, batch     9 | loss: 5.8433108MemoryTrain:  epoch 15, batch    10 | loss: 3.6462038MemoryTrain:  epoch  3, batch    11 | loss: 10.6703147MemoryTrain:  epoch 15, batch     0 | loss: 3.2862202MemoryTrain:  epoch 15, batch     1 | loss: 3.8061751MemoryTrain:  epoch 15, batch     2 | loss: 2.7193650MemoryTrain:  epoch 15, batch     3 | loss: 4.9646996MemoryTrain:  epoch 15, batch     4 | loss: 2.6945806MemoryTrain:  epoch 15, batch     5 | loss: 2.3402833MemoryTrain:  epoch 15, batch     6 | loss: 5.9508906MemoryTrain:  epoch 15, batch     7 | loss: 3.7267140MemoryTrain:  epoch 15, batch     8 | loss: 2.5070222MemoryTrain:  epoch 15, batch     9 | loss: 5.0682070MemoryTrain:  epoch 15, batch    10 | loss: 5.0056958MemoryTrain:  epoch  3, batch    11 | loss: 24.0482190MemoryTrain:  epoch 15, batch     0 | loss: 4.0677925MemoryTrain:  epoch 15, batch     1 | loss: 3.3146635MemoryTrain:  epoch 15, batch     2 | loss: 2.8691944MemoryTrain:  epoch 15, batch     3 | loss: 5.2275528MemoryTrain:  epoch 15, batch     4 | loss: 2.6375259MemoryTrain:  epoch 15, batch     5 | loss: 2.8744863MemoryTrain:  epoch 15, batch     6 | loss: 3.7867938MemoryTrain:  epoch 15, batch     7 | loss: 3.4155920MemoryTrain:  epoch 15, batch     8 | loss: 2.7238474MemoryTrain:  epoch 15, batch     9 | loss: 4.6714811MemoryTrain:  epoch 15, batch    10 | loss: 4.2157786MemoryTrain:  epoch  3, batch    11 | loss: 11.5474042MemoryTrain:  epoch 15, batch     0 | loss: 2.5366952MemoryTrain:  epoch 15, batch     1 | loss: 2.9908660MemoryTrain:  epoch 15, batch     2 | loss: 3.0555618MemoryTrain:  epoch 15, batch     3 | loss: 4.1627000MemoryTrain:  epoch 15, batch     4 | loss: 3.2649297MemoryTrain:  epoch 15, batch     5 | loss: 3.2132948MemoryTrain:  epoch 15, batch     6 | loss: 2.5360478MemoryTrain:  epoch 15, batch     7 | loss: 2.6259309MemoryTrain:  epoch 15, batch     8 | loss: 3.4625652MemoryTrain:  epoch 15, batch     9 | loss: 5.7823466MemoryTrain:  epoch 15, batch    10 | loss: 4.5849913MemoryTrain:  epoch  3, batch    11 | loss: 11.0229883MemoryTrain:  epoch 15, batch     0 | loss: 7.6255112MemoryTrain:  epoch 15, batch     1 | loss: 2.9316907MemoryTrain:  epoch 15, batch     2 | loss: 1.4991128MemoryTrain:  epoch 15, batch     3 | loss: 3.0985416MemoryTrain:  epoch 15, batch     4 | loss: 2.3340176MemoryTrain:  epoch 15, batch     5 | loss: 3.0330086MemoryTrain:  epoch 15, batch     6 | loss: 5.0396723MemoryTrain:  epoch 15, batch     7 | loss: 2.2353016MemoryTrain:  epoch 15, batch     8 | loss: 2.3618100MemoryTrain:  epoch 15, batch     9 | loss: 1.9344642MemoryTrain:  epoch 15, batch    10 | loss: 2.4322401MemoryTrain:  epoch  3, batch    11 | loss: 11.5683356MemoryTrain:  epoch 15, batch     0 | loss: 4.6087816MemoryTrain:  epoch 15, batch     1 | loss: 2.4999967MemoryTrain:  epoch 15, batch     2 | loss: 2.6550416MemoryTrain:  epoch 15, batch     3 | loss: 2.7663971MemoryTrain:  epoch 15, batch     4 | loss: 2.0696667MemoryTrain:  epoch 15, batch     5 | loss: 1.8408461MemoryTrain:  epoch 15, batch     6 | loss: 2.5600828MemoryTrain:  epoch 15, batch     7 | loss: 2.0047043MemoryTrain:  epoch 15, batch     8 | loss: 2.0734470MemoryTrain:  epoch 15, batch     9 | loss: 2.2409128MemoryTrain:  epoch 15, batch    10 | loss: 2.6818893MemoryTrain:  epoch  3, batch    11 | loss: 10.4501635MemoryTrain:  epoch 15, batch     0 | loss: 1.9480182MemoryTrain:  epoch 15, batch     1 | loss: 2.1116547MemoryTrain:  epoch 15, batch     2 | loss: 1.7982820MemoryTrain:  epoch 15, batch     3 | loss: 3.0999527MemoryTrain:  epoch 15, batch     4 | loss: 1.8175660MemoryTrain:  epoch 15, batch     5 | loss: 3.0264820MemoryTrain:  epoch 15, batch     6 | loss: 2.9157498MemoryTrain:  epoch 15, batch     7 | loss: 2.2599106MemoryTrain:  epoch 15, batch     8 | loss: 1.7108896MemoryTrain:  epoch 15, batch     9 | loss: 2.1112422MemoryTrain:  epoch 15, batch    10 | loss: 2.1981020MemoryTrain:  epoch  3, batch    11 | loss: 10.2825226MemoryTrain:  epoch 15, batch     0 | loss: 3.0959293MemoryTrain:  epoch 15, batch     1 | loss: 2.0321903MemoryTrain:  epoch 15, batch     2 | loss: 1.8102154MemoryTrain:  epoch 15, batch     3 | loss: 1.7025367MemoryTrain:  epoch 15, batch     4 | loss: 1.9623630MemoryTrain:  epoch 15, batch     5 | loss: 7.1560500MemoryTrain:  epoch 15, batch     6 | loss: 1.8411503MemoryTrain:  epoch 15, batch     7 | loss: 1.4090106MemoryTrain:  epoch 15, batch     8 | loss: 2.2184145MemoryTrain:  epoch 15, batch     9 | loss: 2.2382179MemoryTrain:  epoch 15, batch    10 | loss: 2.0459505MemoryTrain:  epoch  3, batch    11 | loss: 10.2919023MemoryTrain:  epoch 15, batch     0 | loss: 1.9537190MemoryTrain:  epoch 15, batch     1 | loss: 2.5335361MemoryTrain:  epoch 15, batch     2 | loss: 1.6940911MemoryTrain:  epoch 15, batch     3 | loss: 3.0241052MemoryTrain:  epoch 15, batch     4 | loss: 2.3738365MemoryTrain:  epoch 15, batch     5 | loss: 2.3438027MemoryTrain:  epoch 15, batch     6 | loss: 7.3090723MemoryTrain:  epoch 15, batch     7 | loss: 1.7747986MemoryTrain:  epoch 15, batch     8 | loss: 4.1147213MemoryTrain:  epoch 15, batch     9 | loss: 1.8948597MemoryTrain:  epoch 15, batch    10 | loss: 1.6042068MemoryTrain:  epoch  3, batch    11 | loss: 10.4233542MemoryTrain:  epoch 15, batch     0 | loss: 3.1101734MemoryTrain:  epoch 15, batch     1 | loss: 2.2377201MemoryTrain:  epoch 15, batch     2 | loss: 5.3204088MemoryTrain:  epoch 15, batch     3 | loss: 1.9510108MemoryTrain:  epoch 15, batch     4 | loss: 3.0571716MemoryTrain:  epoch 15, batch     5 | loss: 2.0854138MemoryTrain:  epoch 15, batch     6 | loss: 2.0608128MemoryTrain:  epoch 15, batch     7 | loss: 3.5634355MemoryTrain:  epoch 15, batch     8 | loss: 7.9735103MemoryTrain:  epoch 15, batch     9 | loss: 1.5339155MemoryTrain:  epoch 15, batch    10 | loss: 2.6695485MemoryTrain:  epoch  3, batch    11 | loss: 10.6918641
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 50.00%,  total acc: 59.38%   [EVAL] batch:    2 | acc: 68.75%,  total acc: 62.50%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 67.19%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 71.25%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 72.92%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 75.00%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    8 | acc: 62.50%,  total acc: 73.61%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 72.50%   [EVAL] batch:   10 | acc: 68.75%,  total acc: 72.16%   [EVAL] batch:   11 | acc: 68.75%,  total acc: 71.88%   [EVAL] batch:   12 | acc: 50.00%,  total acc: 70.19%   [EVAL] batch:   13 | acc: 12.50%,  total acc: 66.07%   [EVAL] batch:   14 | acc: 18.75%,  total acc: 62.92%   [EVAL] batch:   15 | acc: 18.75%,  total acc: 60.16%   [EVAL] batch:   16 | acc: 43.75%,  total acc: 59.19%   [EVAL] batch:   17 | acc: 37.50%,  total acc: 57.99%   [EVAL] batch:   18 | acc: 37.50%,  total acc: 56.91%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 58.75%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 60.71%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 62.22%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 63.86%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 65.36%   [EVAL] batch:   24 | acc: 93.75%,  total acc: 66.50%   [EVAL] batch:   25 | acc: 31.25%,  total acc: 65.14%   [EVAL] batch:   26 | acc: 12.50%,  total acc: 63.19%   [EVAL] batch:   27 | acc: 12.50%,  total acc: 61.38%   [EVAL] batch:   28 | acc: 43.75%,  total acc: 60.78%   [EVAL] batch:   29 | acc: 18.75%,  total acc: 59.38%   [EVAL] batch:   30 | acc: 6.25%,  total acc: 57.66%   [EVAL] batch:   31 | acc: 25.00%,  total acc: 56.64%   [EVAL] batch:   32 | acc: 18.75%,  total acc: 55.49%   [EVAL] batch:   33 | acc: 56.25%,  total acc: 55.51%   [EVAL] batch:   34 | acc: 56.25%,  total acc: 55.54%   [EVAL] batch:   35 | acc: 62.50%,  total acc: 55.73%   [EVAL] batch:   36 | acc: 31.25%,  total acc: 55.07%   [EVAL] batch:   37 | acc: 68.75%,  total acc: 55.43%   [EVAL] batch:   38 | acc: 56.25%,  total acc: 55.45%   [EVAL] batch:   39 | acc: 81.25%,  total acc: 56.09%   [EVAL] batch:   40 | acc: 62.50%,  total acc: 56.25%   [EVAL] batch:   41 | acc: 62.50%,  total acc: 56.40%   [EVAL] batch:   42 | acc: 75.00%,  total acc: 56.83%   [EVAL] batch:   43 | acc: 75.00%,  total acc: 57.24%   [EVAL] batch:   44 | acc: 81.25%,  total acc: 57.78%   [EVAL] batch:   45 | acc: 75.00%,  total acc: 58.15%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 58.64%   [EVAL] batch:   47 | acc: 75.00%,  total acc: 58.98%   [EVAL] batch:   48 | acc: 75.00%,  total acc: 59.31%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 60.00%   [EVAL] batch:   50 | acc: 68.75%,  total acc: 60.17%   [EVAL] batch:   51 | acc: 68.75%,  total acc: 60.34%   [EVAL] batch:   52 | acc: 68.75%,  total acc: 60.50%   [EVAL] batch:   53 | acc: 68.75%,  total acc: 60.65%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 61.02%   [EVAL] batch:   55 | acc: 75.00%,  total acc: 61.27%   [EVAL] batch:   56 | acc: 75.00%,  total acc: 61.51%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 61.96%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 62.39%   [EVAL] batch:   59 | acc: 75.00%,  total acc: 62.60%   [EVAL] batch:   60 | acc: 87.50%,  total acc: 63.01%   [EVAL] batch:   61 | acc: 75.00%,  total acc: 63.21%   [EVAL] batch:   62 | acc: 37.50%,  total acc: 62.80%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 82.81%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 80.00%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    6 | acc: 56.25%,  total acc: 77.68%   [EVAL] batch:    7 | acc: 56.25%,  total acc: 75.00%   [EVAL] batch:    8 | acc: 56.25%,  total acc: 72.92%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 71.88%   [EVAL] batch:   10 | acc: 43.75%,  total acc: 69.32%   [EVAL] batch:   11 | acc: 43.75%,  total acc: 67.19%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 67.31%   [EVAL] batch:   13 | acc: 87.50%,  total acc: 68.75%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 70.42%   [EVAL] batch:   15 | acc: 93.75%,  total acc: 71.88%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 73.16%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 74.31%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 75.33%   [EVAL] batch:   19 | acc: 68.75%,  total acc: 75.00%   [EVAL] batch:   20 | acc: 81.25%,  total acc: 75.30%   [EVAL] batch:   21 | acc: 75.00%,  total acc: 75.28%   [EVAL] batch:   22 | acc: 62.50%,  total acc: 74.73%   [EVAL] batch:   23 | acc: 81.25%,  total acc: 75.00%   [EVAL] batch:   24 | acc: 56.25%,  total acc: 74.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 75.24%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 76.16%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 77.01%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 77.59%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 78.33%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 78.83%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 79.49%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 80.11%   [EVAL] batch:   33 | acc: 87.50%,  total acc: 80.33%   [EVAL] batch:   34 | acc: 81.25%,  total acc: 80.36%   [EVAL] batch:   35 | acc: 81.25%,  total acc: 80.38%   [EVAL] batch:   36 | acc: 93.75%,  total acc: 80.74%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 81.09%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 81.57%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 82.03%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 82.47%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 82.74%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 82.99%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 83.10%   [EVAL] batch:   44 | acc: 56.25%,  total acc: 82.50%   [EVAL] batch:   45 | acc: 43.75%,  total acc: 81.66%   [EVAL] batch:   46 | acc: 31.25%,  total acc: 80.59%   [EVAL] batch:   47 | acc: 62.50%,  total acc: 80.21%   [EVAL] batch:   48 | acc: 56.25%,  total acc: 79.72%   [EVAL] batch:   49 | acc: 50.00%,  total acc: 79.12%   [EVAL] batch:   50 | acc: 81.25%,  total acc: 79.17%   [EVAL] batch:   51 | acc: 81.25%,  total acc: 79.21%   [EVAL] batch:   52 | acc: 75.00%,  total acc: 79.13%   [EVAL] batch:   53 | acc: 62.50%,  total acc: 78.82%   [EVAL] batch:   54 | acc: 56.25%,  total acc: 78.41%   [EVAL] batch:   55 | acc: 81.25%,  total acc: 78.46%   [EVAL] batch:   56 | acc: 75.00%,  total acc: 78.40%   [EVAL] batch:   57 | acc: 75.00%,  total acc: 78.34%   [EVAL] batch:   58 | acc: 81.25%,  total acc: 78.39%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 78.65%   [EVAL] batch:   60 | acc: 87.50%,  total acc: 78.79%   [EVAL] batch:   61 | acc: 81.25%,  total acc: 78.83%   [EVAL] batch:   62 | acc: 43.75%,  total acc: 78.27%   [EVAL] batch:   63 | acc: 0.00%,  total acc: 77.05%   [EVAL] batch:   64 | acc: 0.00%,  total acc: 75.87%   [EVAL] batch:   65 | acc: 12.50%,  total acc: 74.91%   [EVAL] batch:   66 | acc: 0.00%,  total acc: 73.79%   [EVAL] batch:   67 | acc: 12.50%,  total acc: 72.89%   [EVAL] batch:   68 | acc: 31.25%,  total acc: 72.28%   [EVAL] batch:   69 | acc: 75.00%,  total acc: 72.32%   [EVAL] batch:   70 | acc: 87.50%,  total acc: 72.54%   [EVAL] batch:   71 | acc: 81.25%,  total acc: 72.66%   [EVAL] batch:   72 | acc: 87.50%,  total acc: 72.86%   [EVAL] batch:   73 | acc: 68.75%,  total acc: 72.80%   [EVAL] batch:   74 | acc: 68.75%,  total acc: 72.75%   [EVAL] batch:   75 | acc: 68.75%,  total acc: 72.70%   [EVAL] batch:   76 | acc: 62.50%,  total acc: 72.56%   [EVAL] batch:   77 | acc: 81.25%,  total acc: 72.68%   [EVAL] batch:   78 | acc: 81.25%,  total acc: 72.78%   [EVAL] batch:   79 | acc: 56.25%,  total acc: 72.58%   [EVAL] batch:   80 | acc: 68.75%,  total acc: 72.53%   [EVAL] batch:   81 | acc: 18.75%,  total acc: 71.88%   [EVAL] batch:   82 | acc: 25.00%,  total acc: 71.31%   [EVAL] batch:   83 | acc: 18.75%,  total acc: 70.68%   [EVAL] batch:   84 | acc: 50.00%,  total acc: 70.44%   [EVAL] batch:   85 | acc: 25.00%,  total acc: 69.91%   [EVAL] batch:   86 | acc: 37.50%,  total acc: 69.54%   [EVAL] batch:   87 | acc: 12.50%,  total acc: 68.89%   [EVAL] batch:   88 | acc: 0.00%,  total acc: 68.12%   [EVAL] batch:   89 | acc: 31.25%,  total acc: 67.71%   [EVAL] batch:   90 | acc: 0.00%,  total acc: 66.96%   [EVAL] batch:   91 | acc: 18.75%,  total acc: 66.44%   [EVAL] batch:   92 | acc: 6.25%,  total acc: 65.79%   [EVAL] batch:   93 | acc: 25.00%,  total acc: 65.36%   [EVAL] batch:   94 | acc: 81.25%,  total acc: 65.53%   [EVAL] batch:   95 | acc: 93.75%,  total acc: 65.82%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 66.17%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 66.52%   [EVAL] batch:   98 | acc: 93.75%,  total acc: 66.79%   [EVAL] batch:   99 | acc: 81.25%,  total acc: 66.94%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 67.26%   [EVAL] batch:  101 | acc: 93.75%,  total acc: 67.52%   [EVAL] batch:  102 | acc: 87.50%,  total acc: 67.72%   [EVAL] batch:  103 | acc: 93.75%,  total acc: 67.97%   [EVAL] batch:  104 | acc: 93.75%,  total acc: 68.21%   [EVAL] batch:  105 | acc: 100.00%,  total acc: 68.51%   [EVAL] batch:  106 | acc: 56.25%,  total acc: 68.40%   [EVAL] batch:  107 | acc: 25.00%,  total acc: 68.00%   [EVAL] batch:  108 | acc: 31.25%,  total acc: 67.66%   [EVAL] batch:  109 | acc: 25.00%,  total acc: 67.27%   [EVAL] batch:  110 | acc: 43.75%,  total acc: 67.06%   [EVAL] batch:  111 | acc: 43.75%,  total acc: 66.85%   [EVAL] batch:  112 | acc: 31.25%,  total acc: 66.54%   [EVAL] batch:  113 | acc: 62.50%,  total acc: 66.50%   [EVAL] batch:  114 | acc: 68.75%,  total acc: 66.52%   [EVAL] batch:  115 | acc: 81.25%,  total acc: 66.65%   [EVAL] batch:  116 | acc: 75.00%,  total acc: 66.72%   [EVAL] batch:  117 | acc: 81.25%,  total acc: 66.84%   [EVAL] batch:  118 | acc: 62.50%,  total acc: 66.81%   [EVAL] batch:  119 | acc: 43.75%,  total acc: 66.61%   [EVAL] batch:  120 | acc: 43.75%,  total acc: 66.43%   [EVAL] batch:  121 | acc: 62.50%,  total acc: 66.39%   [EVAL] batch:  122 | acc: 50.00%,  total acc: 66.26%   [EVAL] batch:  123 | acc: 75.00%,  total acc: 66.33%   [EVAL] batch:  124 | acc: 62.50%,  total acc: 66.30%   [EVAL] batch:  125 | acc: 81.25%,  total acc: 66.42%   [EVAL] batch:  126 | acc: 68.75%,  total acc: 66.44%   [EVAL] batch:  127 | acc: 56.25%,  total acc: 66.36%   [EVAL] batch:  128 | acc: 31.25%,  total acc: 66.09%   [EVAL] batch:  129 | acc: 81.25%,  total acc: 66.20%   [EVAL] batch:  130 | acc: 62.50%,  total acc: 66.17%   [EVAL] batch:  131 | acc: 81.25%,  total acc: 66.29%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 66.54%   [EVAL] batch:  133 | acc: 87.50%,  total acc: 66.70%   [EVAL] batch:  134 | acc: 75.00%,  total acc: 66.76%   [EVAL] batch:  135 | acc: 75.00%,  total acc: 66.82%   [EVAL] batch:  136 | acc: 75.00%,  total acc: 66.88%   [EVAL] batch:  137 | acc: 81.25%,  total acc: 66.98%   [EVAL] batch:  138 | acc: 62.50%,  total acc: 66.95%   [EVAL] batch:  139 | acc: 31.25%,  total acc: 66.70%   [EVAL] batch:  140 | acc: 43.75%,  total acc: 66.53%   [EVAL] batch:  141 | acc: 56.25%,  total acc: 66.46%   [EVAL] batch:  142 | acc: 50.00%,  total acc: 66.35%   [EVAL] batch:  143 | acc: 62.50%,  total acc: 66.32%   [EVAL] batch:  144 | acc: 43.75%,  total acc: 66.16%   [EVAL] batch:  145 | acc: 68.75%,  total acc: 66.18%   [EVAL] batch:  146 | acc: 50.00%,  total acc: 66.07%   [EVAL] batch:  147 | acc: 50.00%,  total acc: 65.96%   [EVAL] batch:  148 | acc: 62.50%,  total acc: 65.94%   [EVAL] batch:  149 | acc: 56.25%,  total acc: 65.88%   [EVAL] batch:  150 | acc: 75.00%,  total acc: 65.94%   [EVAL] batch:  151 | acc: 93.75%,  total acc: 66.12%   [EVAL] batch:  152 | acc: 75.00%,  total acc: 66.18%   [EVAL] batch:  153 | acc: 81.25%,  total acc: 66.27%   [EVAL] batch:  154 | acc: 68.75%,  total acc: 66.29%   [EVAL] batch:  155 | acc: 75.00%,  total acc: 66.35%   [EVAL] batch:  156 | acc: 68.75%,  total acc: 66.36%   [EVAL] batch:  157 | acc: 50.00%,  total acc: 66.26%   [EVAL] batch:  158 | acc: 56.25%,  total acc: 66.19%   [EVAL] batch:  159 | acc: 56.25%,  total acc: 66.13%   [EVAL] batch:  160 | acc: 56.25%,  total acc: 66.07%   [EVAL] batch:  161 | acc: 50.00%,  total acc: 65.97%   [EVAL] batch:  162 | acc: 50.00%,  total acc: 65.87%   [EVAL] batch:  163 | acc: 62.50%,  total acc: 65.85%   [EVAL] batch:  164 | acc: 62.50%,  total acc: 65.83%   [EVAL] batch:  165 | acc: 50.00%,  total acc: 65.74%   [EVAL] batch:  166 | acc: 56.25%,  total acc: 65.68%   [EVAL] batch:  167 | acc: 68.75%,  total acc: 65.70%   [EVAL] batch:  168 | acc: 62.50%,  total acc: 65.68%   [EVAL] batch:  169 | acc: 31.25%,  total acc: 65.48%   [EVAL] batch:  170 | acc: 31.25%,  total acc: 65.28%   [EVAL] batch:  171 | acc: 18.75%,  total acc: 65.01%   [EVAL] batch:  172 | acc: 37.50%,  total acc: 64.85%   [EVAL] batch:  173 | acc: 25.00%,  total acc: 64.62%   [EVAL] batch:  174 | acc: 43.75%,  total acc: 64.50%   [EVAL] batch:  175 | acc: 100.00%,  total acc: 64.70%   [EVAL] batch:  176 | acc: 100.00%,  total acc: 64.90%   [EVAL] batch:  177 | acc: 100.00%,  total acc: 65.10%   [EVAL] batch:  178 | acc: 100.00%,  total acc: 65.29%   [EVAL] batch:  179 | acc: 100.00%,  total acc: 65.49%   [EVAL] batch:  180 | acc: 100.00%,  total acc: 65.68%   [EVAL] batch:  181 | acc: 100.00%,  total acc: 65.87%   [EVAL] batch:  182 | acc: 100.00%,  total acc: 66.05%   [EVAL] batch:  183 | acc: 93.75%,  total acc: 66.20%   [EVAL] batch:  184 | acc: 100.00%,  total acc: 66.39%   [EVAL] batch:  185 | acc: 93.75%,  total acc: 66.53%   [EVAL] batch:  186 | acc: 100.00%,  total acc: 66.71%   [EVAL] batch:  187 | acc: 81.25%,  total acc: 66.79%   [EVAL] batch:  188 | acc: 68.75%,  total acc: 66.80%   [EVAL] batch:  189 | acc: 81.25%,  total acc: 66.88%   [EVAL] batch:  190 | acc: 75.00%,  total acc: 66.92%   [EVAL] batch:  191 | acc: 56.25%,  total acc: 66.86%   [EVAL] batch:  192 | acc: 81.25%,  total acc: 66.94%   [EVAL] batch:  193 | acc: 81.25%,  total acc: 67.01%   [EVAL] batch:  194 | acc: 87.50%,  total acc: 67.12%   [EVAL] batch:  195 | acc: 81.25%,  total acc: 67.19%   [EVAL] batch:  196 | acc: 68.75%,  total acc: 67.20%   [EVAL] batch:  197 | acc: 93.75%,  total acc: 67.33%   [EVAL] batch:  198 | acc: 68.75%,  total acc: 67.34%   [EVAL] batch:  199 | acc: 81.25%,  total acc: 67.41%   [EVAL] batch:  200 | acc: 75.00%,  total acc: 67.44%   [EVAL] batch:  201 | acc: 81.25%,  total acc: 67.51%   [EVAL] batch:  202 | acc: 68.75%,  total acc: 67.52%   [EVAL] batch:  203 | acc: 68.75%,  total acc: 67.52%   [EVAL] batch:  204 | acc: 68.75%,  total acc: 67.53%   [EVAL] batch:  205 | acc: 75.00%,  total acc: 67.57%   [EVAL] batch:  206 | acc: 31.25%,  total acc: 67.39%   [EVAL] batch:  207 | acc: 12.50%,  total acc: 67.13%   [EVAL] batch:  208 | acc: 25.00%,  total acc: 66.93%   [EVAL] batch:  209 | acc: 12.50%,  total acc: 66.67%   [EVAL] batch:  210 | acc: 18.75%,  total acc: 66.44%   [EVAL] batch:  211 | acc: 31.25%,  total acc: 66.27%   [EVAL] batch:  212 | acc: 43.75%,  total acc: 66.17%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 66.33%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 66.48%   [EVAL] batch:  215 | acc: 87.50%,  total acc: 66.58%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 66.73%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 66.89%   [EVAL] batch:  218 | acc: 100.00%,  total acc: 67.04%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 67.19%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 67.34%   [EVAL] batch:  221 | acc: 100.00%,  total acc: 67.48%   [EVAL] batch:  222 | acc: 100.00%,  total acc: 67.63%   [EVAL] batch:  223 | acc: 100.00%,  total acc: 67.77%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 67.92%   [EVAL] batch:  225 | acc: 93.75%,  total acc: 68.03%   [EVAL] batch:  226 | acc: 93.75%,  total acc: 68.14%   [EVAL] batch:  227 | acc: 93.75%,  total acc: 68.26%   [EVAL] batch:  228 | acc: 93.75%,  total acc: 68.37%   [EVAL] batch:  229 | acc: 100.00%,  total acc: 68.51%   [EVAL] batch:  230 | acc: 93.75%,  total acc: 68.61%   [EVAL] batch:  231 | acc: 75.00%,  total acc: 68.64%   [EVAL] batch:  232 | acc: 62.50%,  total acc: 68.62%   [EVAL] batch:  233 | acc: 87.50%,  total acc: 68.70%   [EVAL] batch:  234 | acc: 62.50%,  total acc: 68.67%   [EVAL] batch:  235 | acc: 75.00%,  total acc: 68.70%   [EVAL] batch:  236 | acc: 62.50%,  total acc: 68.67%   [EVAL] batch:  237 | acc: 93.75%,  total acc: 68.78%   [EVAL] batch:  238 | acc: 87.50%,  total acc: 68.85%   [EVAL] batch:  239 | acc: 93.75%,  total acc: 68.96%   [EVAL] batch:  240 | acc: 75.00%,  total acc: 68.98%   [EVAL] batch:  241 | acc: 87.50%,  total acc: 69.06%   [EVAL] batch:  242 | acc: 93.75%,  total acc: 69.16%   [EVAL] batch:  243 | acc: 87.50%,  total acc: 69.24%   [EVAL] batch:  244 | acc: 50.00%,  total acc: 69.16%   [EVAL] batch:  245 | acc: 56.25%,  total acc: 69.11%   [EVAL] batch:  246 | acc: 43.75%,  total acc: 69.00%   [EVAL] batch:  247 | acc: 37.50%,  total acc: 68.88%   [EVAL] batch:  248 | acc: 43.75%,  total acc: 68.78%   [EVAL] batch:  249 | acc: 50.00%,  total acc: 68.70%   [EVAL] batch:  250 | acc: 18.75%,  total acc: 68.50%   [EVAL] batch:  251 | acc: 18.75%,  total acc: 68.30%   [EVAL] batch:  252 | acc: 31.25%,  total acc: 68.16%   [EVAL] batch:  253 | acc: 31.25%,  total acc: 68.01%   [EVAL] batch:  254 | acc: 25.00%,  total acc: 67.84%   [EVAL] batch:  255 | acc: 37.50%,  total acc: 67.72%   [EVAL] batch:  256 | acc: 50.00%,  total acc: 67.66%   [EVAL] batch:  257 | acc: 81.25%,  total acc: 67.71%   [EVAL] batch:  258 | acc: 75.00%,  total acc: 67.74%   [EVAL] batch:  259 | acc: 81.25%,  total acc: 67.79%   [EVAL] batch:  260 | acc: 43.75%,  total acc: 67.70%   [EVAL] batch:  261 | acc: 75.00%,  total acc: 67.72%   [EVAL] batch:  262 | acc: 81.25%,  total acc: 67.78%   [EVAL] batch:  263 | acc: 81.25%,  total acc: 67.83%   [EVAL] batch:  264 | acc: 75.00%,  total acc: 67.85%   [EVAL] batch:  265 | acc: 81.25%,  total acc: 67.90%   [EVAL] batch:  266 | acc: 81.25%,  total acc: 67.95%   [EVAL] batch:  267 | acc: 100.00%,  total acc: 68.07%   [EVAL] batch:  268 | acc: 100.00%,  total acc: 68.19%   [EVAL] batch:  269 | acc: 87.50%,  total acc: 68.26%   [EVAL] batch:  270 | acc: 100.00%,  total acc: 68.38%   [EVAL] batch:  271 | acc: 100.00%,  total acc: 68.50%   [EVAL] batch:  272 | acc: 93.75%,  total acc: 68.59%   [EVAL] batch:  273 | acc: 93.75%,  total acc: 68.68%   [EVAL] batch:  274 | acc: 100.00%,  total acc: 68.80%   [EVAL] batch:  275 | acc: 81.25%,  total acc: 68.84%   [EVAL] batch:  276 | acc: 93.75%,  total acc: 68.93%   [EVAL] batch:  277 | acc: 93.75%,  total acc: 69.02%   [EVAL] batch:  278 | acc: 68.75%,  total acc: 69.02%   [EVAL] batch:  279 | acc: 87.50%,  total acc: 69.08%   [EVAL] batch:  280 | acc: 81.25%,  total acc: 69.13%   [EVAL] batch:  281 | acc: 56.25%,  total acc: 69.08%   [EVAL] batch:  282 | acc: 62.50%,  total acc: 69.06%   [EVAL] batch:  283 | acc: 37.50%,  total acc: 68.95%   [EVAL] batch:  284 | acc: 43.75%,  total acc: 68.86%   [EVAL] batch:  285 | acc: 43.75%,  total acc: 68.77%   [EVAL] batch:  286 | acc: 37.50%,  total acc: 68.66%   [EVAL] batch:  287 | acc: 62.50%,  total acc: 68.64%   [EVAL] batch:  288 | acc: 68.75%,  total acc: 68.64%   [EVAL] batch:  289 | acc: 87.50%,  total acc: 68.71%   [EVAL] batch:  290 | acc: 62.50%,  total acc: 68.69%   [EVAL] batch:  291 | acc: 87.50%,  total acc: 68.75%   [EVAL] batch:  292 | acc: 75.00%,  total acc: 68.77%   [EVAL] batch:  293 | acc: 75.00%,  total acc: 68.79%   [EVAL] batch:  294 | acc: 81.25%,  total acc: 68.83%   [EVAL] batch:  295 | acc: 68.75%,  total acc: 68.83%   [EVAL] batch:  296 | acc: 68.75%,  total acc: 68.83%   [EVAL] batch:  297 | acc: 56.25%,  total acc: 68.79%   [EVAL] batch:  298 | acc: 81.25%,  total acc: 68.83%   [EVAL] batch:  299 | acc: 81.25%,  total acc: 68.88%   [EVAL] batch:  300 | acc: 100.00%,  total acc: 68.98%   [EVAL] batch:  301 | acc: 100.00%,  total acc: 69.08%   [EVAL] batch:  302 | acc: 100.00%,  total acc: 69.18%   [EVAL] batch:  303 | acc: 100.00%,  total acc: 69.28%   [EVAL] batch:  304 | acc: 100.00%,  total acc: 69.39%   [EVAL] batch:  305 | acc: 100.00%,  total acc: 69.49%   [EVAL] batch:  306 | acc: 100.00%,  total acc: 69.58%   [EVAL] batch:  307 | acc: 100.00%,  total acc: 69.68%   [EVAL] batch:  308 | acc: 100.00%,  total acc: 69.78%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 69.88%   [EVAL] batch:  310 | acc: 100.00%,  total acc: 69.98%   [EVAL] batch:  311 | acc: 100.00%,  total acc: 70.07%   [EVAL] batch:  312 | acc: 93.75%,  total acc: 70.15%   [EVAL] batch:  313 | acc: 50.00%,  total acc: 70.08%   [EVAL] batch:  314 | acc: 62.50%,  total acc: 70.06%   [EVAL] batch:  315 | acc: 75.00%,  total acc: 70.08%   [EVAL] batch:  316 | acc: 87.50%,  total acc: 70.13%   [EVAL] batch:  317 | acc: 81.25%,  total acc: 70.17%   [EVAL] batch:  318 | acc: 87.50%,  total acc: 70.22%   [EVAL] batch:  319 | acc: 68.75%,  total acc: 70.21%   [EVAL] batch:  320 | acc: 75.00%,  total acc: 70.23%   [EVAL] batch:  321 | acc: 68.75%,  total acc: 70.23%   [EVAL] batch:  322 | acc: 68.75%,  total acc: 70.22%   [EVAL] batch:  323 | acc: 62.50%,  total acc: 70.20%   [EVAL] batch:  324 | acc: 62.50%,  total acc: 70.17%   [EVAL] batch:  325 | acc: 25.00%,  total acc: 70.03%   [EVAL] batch:  326 | acc: 12.50%,  total acc: 69.86%   [EVAL] batch:  327 | acc: 12.50%,  total acc: 69.68%   [EVAL] batch:  328 | acc: 37.50%,  total acc: 69.59%   [EVAL] batch:  329 | acc: 50.00%,  total acc: 69.53%   [EVAL] batch:  330 | acc: 18.75%,  total acc: 69.37%   [EVAL] batch:  331 | acc: 75.00%,  total acc: 69.39%   [EVAL] batch:  332 | acc: 100.00%,  total acc: 69.48%   [EVAL] batch:  333 | acc: 100.00%,  total acc: 69.57%   [EVAL] batch:  334 | acc: 93.75%,  total acc: 69.65%   [EVAL] batch:  335 | acc: 100.00%,  total acc: 69.74%   [EVAL] batch:  336 | acc: 93.75%,  total acc: 69.81%   [EVAL] batch:  337 | acc: 75.00%,  total acc: 69.82%   [EVAL] batch:  338 | acc: 12.50%,  total acc: 69.65%   [EVAL] batch:  339 | acc: 18.75%,  total acc: 69.50%   [EVAL] batch:  340 | acc: 18.75%,  total acc: 69.35%   [EVAL] batch:  341 | acc: 37.50%,  total acc: 69.26%   [EVAL] batch:  342 | acc: 12.50%,  total acc: 69.10%   [EVAL] batch:  343 | acc: 12.50%,  total acc: 68.93%   [EVAL] batch:  344 | acc: 18.75%,  total acc: 68.79%   [EVAL] batch:  345 | acc: 43.75%,  total acc: 68.71%   [EVAL] batch:  346 | acc: 62.50%,  total acc: 68.70%   [EVAL] batch:  347 | acc: 43.75%,  total acc: 68.62%   [EVAL] batch:  348 | acc: 56.25%,  total acc: 68.59%   [EVAL] batch:  349 | acc: 37.50%,  total acc: 68.50%   [EVAL] batch:  350 | acc: 75.00%,  total acc: 68.52%   [EVAL] batch:  351 | acc: 62.50%,  total acc: 68.50%   [EVAL] batch:  352 | acc: 68.75%,  total acc: 68.50%   [EVAL] batch:  353 | acc: 56.25%,  total acc: 68.47%   [EVAL] batch:  354 | acc: 93.75%,  total acc: 68.54%   [EVAL] batch:  355 | acc: 56.25%,  total acc: 68.50%   [EVAL] batch:  356 | acc: 87.50%,  total acc: 68.56%   [EVAL] batch:  357 | acc: 75.00%,  total acc: 68.58%   [EVAL] batch:  358 | acc: 75.00%,  total acc: 68.59%   [EVAL] batch:  359 | acc: 93.75%,  total acc: 68.66%   [EVAL] batch:  360 | acc: 62.50%,  total acc: 68.65%   [EVAL] batch:  361 | acc: 81.25%,  total acc: 68.68%   [EVAL] batch:  362 | acc: 87.50%,  total acc: 68.73%   [EVAL] batch:  363 | acc: 62.50%,  total acc: 68.72%   [EVAL] batch:  364 | acc: 75.00%,  total acc: 68.73%   [EVAL] batch:  365 | acc: 62.50%,  total acc: 68.72%   [EVAL] batch:  366 | acc: 75.00%,  total acc: 68.73%   [EVAL] batch:  367 | acc: 75.00%,  total acc: 68.75%   [EVAL] batch:  368 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:  369 | acc: 93.75%,  total acc: 68.82%   [EVAL] batch:  370 | acc: 87.50%,  total acc: 68.87%   [EVAL] batch:  371 | acc: 81.25%,  total acc: 68.90%   [EVAL] batch:  372 | acc: 75.00%,  total acc: 68.92%   [EVAL] batch:  373 | acc: 81.25%,  total acc: 68.95%   [EVAL] batch:  374 | acc: 81.25%,  total acc: 68.98%   
cur_acc:  ['0.9494', '0.7431', '0.7530', '0.7708', '0.7569', '0.6280']
his_acc:  ['0.9494', '0.8230', '0.7680', '0.7475', '0.7167', '0.6898']
CurrentTrain: epoch 15, batch     0 | loss: 24.0166030CurrentTrain: epoch 15, batch     1 | loss: 24.4883253CurrentTrain: epoch 15, batch     2 | loss: 15.2241004CurrentTrain: epoch  1, batch     3 | loss: 9.2475499CurrentTrain: epoch 15, batch     0 | loss: 14.4362243CurrentTrain: epoch 15, batch     1 | loss: 12.6419209CurrentTrain: epoch 15, batch     2 | loss: 13.1372460CurrentTrain: epoch  1, batch     3 | loss: 5.7089210CurrentTrain: epoch 15, batch     0 | loss: 10.5286807CurrentTrain: epoch 15, batch     1 | loss: 15.5350661CurrentTrain: epoch 15, batch     2 | loss: 14.9753982CurrentTrain: epoch  1, batch     3 | loss: 7.9485068CurrentTrain: epoch 15, batch     0 | loss: 10.5455244CurrentTrain: epoch 15, batch     1 | loss: 9.3864883CurrentTrain: epoch 15, batch     2 | loss: 10.1020568CurrentTrain: epoch  1, batch     3 | loss: 8.4679036CurrentTrain: epoch 15, batch     0 | loss: 7.2334193CurrentTrain: epoch 15, batch     1 | loss: 10.2581168CurrentTrain: epoch 15, batch     2 | loss: 8.0648024CurrentTrain: epoch  1, batch     3 | loss: 7.4853010CurrentTrain: epoch 15, batch     0 | loss: 9.3037665CurrentTrain: epoch 15, batch     1 | loss: 10.1358472CurrentTrain: epoch 15, batch     2 | loss: 8.0121299CurrentTrain: epoch  1, batch     3 | loss: 7.1910969CurrentTrain: epoch 15, batch     0 | loss: 9.1274587CurrentTrain: epoch 15, batch     1 | loss: 14.6056713CurrentTrain: epoch 15, batch     2 | loss: 8.7262681CurrentTrain: epoch  1, batch     3 | loss: 6.2311915CurrentTrain: epoch 15, batch     0 | loss: 7.7587942CurrentTrain: epoch 15, batch     1 | loss: 10.5712253CurrentTrain: epoch 15, batch     2 | loss: 9.9279669CurrentTrain: epoch  1, batch     3 | loss: 6.6287684CurrentTrain: epoch 15, batch     0 | loss: 8.9977230CurrentTrain: epoch 15, batch     1 | loss: 8.7969261CurrentTrain: epoch 15, batch     2 | loss: 7.1319263CurrentTrain: epoch  1, batch     3 | loss: 5.5562241CurrentTrain: epoch 15, batch     0 | loss: 7.8288240CurrentTrain: epoch 15, batch     1 | loss: 10.8275746CurrentTrain: epoch 15, batch     2 | loss: 11.1195377CurrentTrain: epoch  1, batch     3 | loss: 5.9776561
#### member of political party ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "member of political party" means the political party of which a person is or has been a member or otherwise affiliated.
Here is an example:
Relation: member of political party
Context: alongside evan durbin and hugh gaitskell , he brought the thinking of john maynard keynes to the labour party , especially in relation to price determination .
Head Entity: hugh gaitskell
Tail Entity: labour party
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "member of political party":
Relation: member of political party  
Context: After serving as the mayor, she became a prominent figure in the local chapter of the Green Party, advocating for environmental policies and community engagement.  
Head Entity: she  
Tail Entity: Green Party  

Relation: member of political party  
Context: During his tenure in the Senate, he was known for his strong support of the Democratic Party's initiatives on healthcare reform and education.  
Head Entity: he  
Tail Entity: Democratic Party  
#### after a work by ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "after a work by" means artist whose work strongly inspired/ was copied in this item.
Here is an example:
Relation: after a work by
Context: the original story of real life escape of betty mahmoody is depicted in the movie " not without my daughter " which itself was based on betty mahmoody 's book of the same name .
Head Entity: not without my daughter
Tail Entity: betty mahmoody
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "after a work by":
Relation: after a work by  
Context: the film "the great gatsby" draws heavily from f. scott fitzgerald's classic novel, capturing the essence of the roaring twenties and the complexities of love and ambition.  
Head Entity: the great gatsby  
Tail Entity: f. scott fitzgerald  

Relation: after a work by  
Context: the animated feature "the lion king" was inspired by shakespeare's play "hamlet," incorporating themes of betrayal, revenge, and the struggle for power.  
Head Entity: the lion king  
Tail Entity: shakespeare
#### notable work ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "notable work" means notable scientific, artistic or literary work, or other work of significance among subject's works.
Here is an example:
Relation: notable work
Context: another was " the death of nelson " by daniel maclise , a large wall painting in the royal gallery of the palace of westminster .
Head Entity: daniel maclise
Tail Entity: the death of nelson
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "notable work":
Relation: notable work  
Context: one of his most famous pieces is "the persistence of memory," a surreal painting that features melting clocks and is considered a masterpiece of 20th-century art.  
Head Entity: salvador dalí  
Tail Entity: the persistence of memory  

Relation: notable work  
Context: her acclaimed novel "to kill a mockingbird" explores themes of racial injustice and moral growth in the American South during the 1930s.  
Head Entity: harper lee  
Tail Entity: to kill a mockingbird  
#### headquarters location ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "headquarters location" means city, where an organization's headquarters is or has been situated.
Here is an example:
Relation: headquarters location
Context: at the request of the stockholm institute for bible translation , mirza khazar translated the bible – the new testament and old testament – into the azerbaijani language .
Head Entity: institute for bible translation
Tail Entity: stockholm
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "headquarters location":
Relation: headquarters location  
Context: the global technology company, headquartered in san francisco, has been a leader in software development for over two decades.  
Head Entity: global technology company  
Tail Entity: san francisco  

Relation: headquarters location  
Context: after relocating its main office to new york city, the startup saw a significant increase in investment and growth opportunities.  
Head Entity: startup  
Tail Entity: new york city  
#### taxon rank ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "taxon rank" means level in a taxonomic hierarchy.
Here is an example:
Relation: taxon rank
Context: a number of species that were placed in the family phyllophoridae by thander in 1989 and 1990 are now included in cucumariidae .
Head Entity: phyllophoridae
Tail Entity: family
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "taxon rank":
Relation: taxon rank  
Context: The genus Canis includes several species, with Canis lupus being the most recognized as the wolf, which belongs to the family Canidae.  
Head Entity: Canis  
Tail Entity: genus  

Relation: taxon rank  
Context: In the classification of plants, the family Rosaceae encompasses a variety of species, including roses and strawberries, which are categorized under the order Rosales.  
Head Entity: Rosaceae  
Tail Entity: family  
#### field of work ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "field of work" means specialization of a person or organization.
Here is an example:
Relation: field of work
Context: he later collaborated with vito volterra and explored laplace transforms and other parts of functional analysis .
Head Entity: vito volterra
Tail Entity: functional analysis
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "field of work":
Relation: field of work  
Context: Marie Curie conducted pioneering research on radioactivity, which laid the groundwork for advancements in nuclear physics.  
Head Entity: Marie Curie  
Tail Entity: nuclear physics  

Relation: field of work  
Context: The company specializes in artificial intelligence and machine learning, providing innovative solutions for various industries.  
Head Entity: The company  
Tail Entity: artificial intelligence
#### licensed to broadcast to ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "licensed to broadcast to" means place that a radio/TV station is licensed/required to broadcast to.
Here is an example:
Relation: licensed to broadcast to
Context: this early involvement in radio led him to buy many small , under performing radio stations beginning with wcos in columbia , south carolina , and turn them around with new formats and improved management .
Head Entity: wcos
Tail Entity: columbia , south carolina
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "licensed to broadcast to":
Relation: licensed to broadcast to  
Context: The new television station, KXYZ, has been granted permission to broadcast to the entire metropolitan area of San Francisco, ensuring that residents can access a variety of programming.  
Head Entity: KXYZ  
Tail Entity: San Francisco  

Relation: licensed to broadcast to  
Context: After acquiring the rights, the radio station WABC was officially licensed to broadcast to the listeners in the greater New York City area, expanding its reach significantly.  
Head Entity: WABC  
Tail Entity: greater New York City area  
#### constellation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "constellation" means the area of the celestial sphere of which the subject is a part.
Here is an example:
Relation: constellation
Context: ngc 192 is a type sba spiral galaxy located in the constellation cetus .
Head Entity: ngc 192
Tail Entity: cetus
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "constellation":
Relation: constellation  
Context: the orion nebula is located in the constellation orion and is one of the brightest nebulae visible to the naked eye.  
Head Entity: orion nebula  
Tail Entity: orion  

Relation: constellation  
Context: the star sirius is part of the constellation canis major, which is known for its prominence in the night sky.  
Head Entity: sirius  
Tail Entity: canis major  
#### platform ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "platform" means platform for which a work was developed or released, or the specific platform version of a software product.
Here is an example:
Relation: platform
Context: " toy soldiers : war chest " was released on august 11 , 2015 for windows , playstation 4 and xbox one .
Head Entity: toy soldiers : war chest
Tail Entity: xbox one
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "platform":
Relation: platform  
Context: "The Legend of Zelda: Breath of the Wild" was launched on March 3, 2017 for the Nintendo Switch and Wii U.  
Head Entity: The Legend of Zelda: Breath of the Wild  
Tail Entity: Nintendo Switch  

Relation: platform  
Context: "Cyberpunk 2077" was made available on December 10, 2020 for PC, PlayStation 4, and Xbox One.  
Head Entity: Cyberpunk 2077  
Tail Entity: PlayStation 4  
#### place served by transport hub ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "place served by transport hub" means territorial entity or entities served by this transport hub (airport, train station, etc.).
Here is an example:
Relation: place served by transport hub
Context: services to the island run from san juan 's luis muñoz marín international airport , ceiba airport or isla grande airport ( 20- to 30-minute flight ) .
Head Entity: isla grande airport
Tail Entity: san juan
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "place served by transport hub":
Relation: place served by transport hub  
Context: The city of Barcelona is well-connected to the surrounding regions through the El Prat Airport, which offers numerous flights to various destinations.  
Head Entity: El Prat Airport  
Tail Entity: Barcelona  

Relation: place served by transport hub  
Context: The central train station in Zurich provides access to multiple Swiss cities and international destinations, making it a key transport hub for travelers.  
Head Entity: Zurich central train station  
Tail Entity: Zurich  
MemoryTrain:  epoch 15, batch     0 | loss: 3.6001702MemoryTrain:  epoch 15, batch     1 | loss: 3.3930412MemoryTrain:  epoch 15, batch     2 | loss: 2.9631987MemoryTrain:  epoch 15, batch     3 | loss: 4.5585465MemoryTrain:  epoch 15, batch     4 | loss: 2.1615241MemoryTrain:  epoch 15, batch     5 | loss: 2.6401082MemoryTrain:  epoch 15, batch     6 | loss: 3.2286670MemoryTrain:  epoch 15, batch     7 | loss: 2.6547953MemoryTrain:  epoch 15, batch     8 | loss: 4.8412561MemoryTrain:  epoch 15, batch     9 | loss: 5.3654133MemoryTrain:  epoch 15, batch    10 | loss: 5.5694681MemoryTrain:  epoch 15, batch    11 | loss: 2.4310589MemoryTrain:  epoch 15, batch    12 | loss: 7.3064085MemoryTrain:  epoch  1, batch    13 | loss: 6.1645411MemoryTrain:  epoch 15, batch     0 | loss: 2.1128883MemoryTrain:  epoch 15, batch     1 | loss: 2.4258513MemoryTrain:  epoch 15, batch     2 | loss: 5.3865126MemoryTrain:  epoch 15, batch     3 | loss: 2.2995117MemoryTrain:  epoch 15, batch     4 | loss: 2.4920266MemoryTrain:  epoch 15, batch     5 | loss: 2.3759862MemoryTrain:  epoch 15, batch     6 | loss: 2.6957903MemoryTrain:  epoch 15, batch     7 | loss: 7.9329051MemoryTrain:  epoch 15, batch     8 | loss: 1.5796844MemoryTrain:  epoch 15, batch     9 | loss: 2.0424462MemoryTrain:  epoch 15, batch    10 | loss: 2.2197400MemoryTrain:  epoch 15, batch    11 | loss: 1.8216951MemoryTrain:  epoch 15, batch    12 | loss: 6.0376124MemoryTrain:  epoch  1, batch    13 | loss: 8.4934841MemoryTrain:  epoch 15, batch     0 | loss: 2.3301555MemoryTrain:  epoch 15, batch     1 | loss: 2.1806473MemoryTrain:  epoch 15, batch     2 | loss: 2.5684262MemoryTrain:  epoch 15, batch     3 | loss: 3.1834772MemoryTrain:  epoch 15, batch     4 | loss: 2.0426473MemoryTrain:  epoch 15, batch     5 | loss: 2.0292448MemoryTrain:  epoch 15, batch     6 | loss: 2.7879092MemoryTrain:  epoch 15, batch     7 | loss: 4.1804003MemoryTrain:  epoch 15, batch     8 | loss: 2.7688535MemoryTrain:  epoch 15, batch     9 | loss: 3.6041230MemoryTrain:  epoch 15, batch    10 | loss: 1.8789866MemoryTrain:  epoch 15, batch    11 | loss: 3.7417309MemoryTrain:  epoch 15, batch    12 | loss: 1.8420788MemoryTrain:  epoch  1, batch    13 | loss: 5.4481300MemoryTrain:  epoch 15, batch     0 | loss: 2.1656742MemoryTrain:  epoch 15, batch     1 | loss: 1.9757108MemoryTrain:  epoch 15, batch     2 | loss: 2.4016268MemoryTrain:  epoch 15, batch     3 | loss: 2.0537351MemoryTrain:  epoch 15, batch     4 | loss: 1.9843799MemoryTrain:  epoch 15, batch     5 | loss: 2.3128260MemoryTrain:  epoch 15, batch     6 | loss: 1.8872830MemoryTrain:  epoch 15, batch     7 | loss: 1.9173246MemoryTrain:  epoch 15, batch     8 | loss: 2.2469066MemoryTrain:  epoch 15, batch     9 | loss: 1.6796709MemoryTrain:  epoch 15, batch    10 | loss: 2.4700542MemoryTrain:  epoch 15, batch    11 | loss: 4.1432165MemoryTrain:  epoch 15, batch    12 | loss: 2.3748435MemoryTrain:  epoch  1, batch    13 | loss: 7.3633788MemoryTrain:  epoch 15, batch     0 | loss: 4.8658846MemoryTrain:  epoch 15, batch     1 | loss: 2.9809751MemoryTrain:  epoch 15, batch     2 | loss: 1.8817382MemoryTrain:  epoch 15, batch     3 | loss: 1.4158114MemoryTrain:  epoch 15, batch     4 | loss: 1.7102729MemoryTrain:  epoch 15, batch     5 | loss: 1.9618092MemoryTrain:  epoch 15, batch     6 | loss: 2.2931705MemoryTrain:  epoch 15, batch     7 | loss: 4.3159579MemoryTrain:  epoch 15, batch     8 | loss: 1.9075403MemoryTrain:  epoch 15, batch     9 | loss: 4.1280591MemoryTrain:  epoch 15, batch    10 | loss: 1.5294182MemoryTrain:  epoch 15, batch    11 | loss: 2.4301789MemoryTrain:  epoch 15, batch    12 | loss: 2.2419491MemoryTrain:  epoch  1, batch    13 | loss: 6.9714909MemoryTrain:  epoch 15, batch     0 | loss: 2.9619591MemoryTrain:  epoch 15, batch     1 | loss: 2.1458999MemoryTrain:  epoch 15, batch     2 | loss: 1.9177806MemoryTrain:  epoch 15, batch     3 | loss: 1.6825728MemoryTrain:  epoch 15, batch     4 | loss: 2.9683692MemoryTrain:  epoch 15, batch     5 | loss: 1.8926729MemoryTrain:  epoch 15, batch     6 | loss: 2.2512986MemoryTrain:  epoch 15, batch     7 | loss: 1.4391067MemoryTrain:  epoch 15, batch     8 | loss: 2.2409027MemoryTrain:  epoch 15, batch     9 | loss: 1.8551868MemoryTrain:  epoch 15, batch    10 | loss: 1.4602842MemoryTrain:  epoch 15, batch    11 | loss: 1.6323498MemoryTrain:  epoch 15, batch    12 | loss: 4.2862859MemoryTrain:  epoch  1, batch    13 | loss: 5.3496812MemoryTrain:  epoch 15, batch     0 | loss: 1.8333939MemoryTrain:  epoch 15, batch     1 | loss: 1.5532657MemoryTrain:  epoch 15, batch     2 | loss: 1.5334996MemoryTrain:  epoch 15, batch     3 | loss: 3.9834370MemoryTrain:  epoch 15, batch     4 | loss: 1.9230127MemoryTrain:  epoch 15, batch     5 | loss: 2.2642695MemoryTrain:  epoch 15, batch     6 | loss: 1.8598357MemoryTrain:  epoch 15, batch     7 | loss: 2.6409722MemoryTrain:  epoch 15, batch     8 | loss: 4.2346409MemoryTrain:  epoch 15, batch     9 | loss: 1.4433103MemoryTrain:  epoch 15, batch    10 | loss: 1.9139890MemoryTrain:  epoch 15, batch    11 | loss: 3.5074840MemoryTrain:  epoch 15, batch    12 | loss: 1.9382975MemoryTrain:  epoch  1, batch    13 | loss: 6.3530270MemoryTrain:  epoch 15, batch     0 | loss: 1.7439173MemoryTrain:  epoch 15, batch     1 | loss: 2.9619635MemoryTrain:  epoch 15, batch     2 | loss: 1.8006812MemoryTrain:  epoch 15, batch     3 | loss: 1.4978570MemoryTrain:  epoch 15, batch     4 | loss: 1.5234226MemoryTrain:  epoch 15, batch     5 | loss: 3.8636584MemoryTrain:  epoch 15, batch     6 | loss: 1.5510560MemoryTrain:  epoch 15, batch     7 | loss: 1.9224767MemoryTrain:  epoch 15, batch     8 | loss: 2.9655077MemoryTrain:  epoch 15, batch     9 | loss: 1.7666602MemoryTrain:  epoch 15, batch    10 | loss: 1.8094900MemoryTrain:  epoch 15, batch    11 | loss: 1.6451837MemoryTrain:  epoch 15, batch    12 | loss: 1.7625744MemoryTrain:  epoch  1, batch    13 | loss: 5.1566017MemoryTrain:  epoch 15, batch     0 | loss: 1.7794015MemoryTrain:  epoch 15, batch     1 | loss: 2.8401522MemoryTrain:  epoch 15, batch     2 | loss: 1.7657689MemoryTrain:  epoch 15, batch     3 | loss: 2.5328324MemoryTrain:  epoch 15, batch     4 | loss: 1.7363953MemoryTrain:  epoch 15, batch     5 | loss: 1.4554707MemoryTrain:  epoch 15, batch     6 | loss: 5.8571858MemoryTrain:  epoch 15, batch     7 | loss: 2.8039360MemoryTrain:  epoch 15, batch     8 | loss: 1.8104519MemoryTrain:  epoch 15, batch     9 | loss: 1.7853577MemoryTrain:  epoch 15, batch    10 | loss: 2.1995593MemoryTrain:  epoch 15, batch    11 | loss: 1.8828615MemoryTrain:  epoch 15, batch    12 | loss: 2.2133655MemoryTrain:  epoch  1, batch    13 | loss: 5.0510380MemoryTrain:  epoch 15, batch     0 | loss: 3.9891698MemoryTrain:  epoch 15, batch     1 | loss: 1.9657901MemoryTrain:  epoch 15, batch     2 | loss: 4.1827460MemoryTrain:  epoch 15, batch     3 | loss: 1.3782006MemoryTrain:  epoch 15, batch     4 | loss: 5.0457261MemoryTrain:  epoch 15, batch     5 | loss: 3.0187144MemoryTrain:  epoch 15, batch     6 | loss: 1.5830892MemoryTrain:  epoch 15, batch     7 | loss: 4.3646263MemoryTrain:  epoch 15, batch     8 | loss: 2.7911036MemoryTrain:  epoch 15, batch     9 | loss: 4.3446213MemoryTrain:  epoch 15, batch    10 | loss: 1.7733080MemoryTrain:  epoch 15, batch    11 | loss: 1.3557837MemoryTrain:  epoch 15, batch    12 | loss: 1.6680569MemoryTrain:  epoch  1, batch    13 | loss: 5.7304075
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 90.62%   [EVAL] batch:    2 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 92.19%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 92.50%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 92.19%   [EVAL] batch:    8 | acc: 62.50%,  total acc: 88.89%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 86.25%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 85.80%   [EVAL] batch:   11 | acc: 50.00%,  total acc: 82.81%   [EVAL] batch:   12 | acc: 56.25%,  total acc: 80.77%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 79.91%   [EVAL] batch:   14 | acc: 56.25%,  total acc: 78.33%   [EVAL] batch:   15 | acc: 43.75%,  total acc: 76.17%   [EVAL] batch:   16 | acc: 37.50%,  total acc: 73.90%   [EVAL] batch:   17 | acc: 43.75%,  total acc: 72.22%   [EVAL] batch:   18 | acc: 50.00%,  total acc: 71.05%   [EVAL] batch:   19 | acc: 56.25%,  total acc: 70.31%   [EVAL] batch:   20 | acc: 62.50%,  total acc: 69.94%   [EVAL] batch:   21 | acc: 68.75%,  total acc: 69.89%   [EVAL] batch:   22 | acc: 75.00%,  total acc: 70.11%   [EVAL] batch:   23 | acc: 75.00%,  total acc: 70.31%   [EVAL] batch:   24 | acc: 56.25%,  total acc: 69.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 70.91%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 71.99%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 72.99%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 73.92%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 74.79%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 75.60%   [EVAL] batch:   31 | acc: 81.25%,  total acc: 75.78%   [EVAL] batch:   32 | acc: 75.00%,  total acc: 75.76%   [EVAL] batch:   33 | acc: 68.75%,  total acc: 75.55%   [EVAL] batch:   34 | acc: 81.25%,  total acc: 75.71%   [EVAL] batch:   35 | acc: 75.00%,  total acc: 75.69%   [EVAL] batch:   36 | acc: 62.50%,  total acc: 75.34%   [EVAL] batch:   37 | acc: 75.00%,  total acc: 75.33%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 75.96%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 76.56%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 77.13%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 77.68%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 78.20%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 78.55%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 79.03%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 79.48%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 79.92%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 80.34%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 80.74%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 81.00%   [EVAL] batch:   50 | acc: 87.50%,  total acc: 81.13%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 81.49%   [EVAL] batch:   53 | acc: 93.75%,  total acc: 81.71%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 81.70%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 81.92%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 82.13%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 82.33%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 82.63%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 82.92%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 83.20%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 83.27%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 82.74%   
[EVAL] batch:    0 | acc: 56.25%,  total acc: 56.25%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 71.88%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 75.00%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 73.44%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 72.50%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 73.96%   [EVAL] batch:    6 | acc: 50.00%,  total acc: 70.54%   [EVAL] batch:    7 | acc: 31.25%,  total acc: 65.62%   [EVAL] batch:    8 | acc: 50.00%,  total acc: 63.89%   [EVAL] batch:    9 | acc: 50.00%,  total acc: 62.50%   [EVAL] batch:   10 | acc: 37.50%,  total acc: 60.23%   [EVAL] batch:   11 | acc: 43.75%,  total acc: 58.85%   [EVAL] batch:   12 | acc: 56.25%,  total acc: 58.65%   [EVAL] batch:   13 | acc: 81.25%,  total acc: 60.27%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 62.50%   [EVAL] batch:   15 | acc: 87.50%,  total acc: 64.06%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 65.81%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 67.36%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 68.75%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 69.06%   [EVAL] batch:   20 | acc: 75.00%,  total acc: 69.35%   [EVAL] batch:   21 | acc: 68.75%,  total acc: 69.32%   [EVAL] batch:   22 | acc: 56.25%,  total acc: 68.75%   [EVAL] batch:   23 | acc: 81.25%,  total acc: 69.27%   [EVAL] batch:   24 | acc: 62.50%,  total acc: 69.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 70.19%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 71.30%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 72.32%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 73.06%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 73.96%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 74.80%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 75.59%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 76.33%   [EVAL] batch:   33 | acc: 87.50%,  total acc: 76.65%   [EVAL] batch:   34 | acc: 81.25%,  total acc: 76.79%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 77.08%   [EVAL] batch:   36 | acc: 93.75%,  total acc: 77.53%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 77.96%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 78.53%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 79.06%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 79.57%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 79.91%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 80.23%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 80.40%   [EVAL] batch:   44 | acc: 75.00%,  total acc: 80.28%   [EVAL] batch:   45 | acc: 68.75%,  total acc: 80.03%   [EVAL] batch:   46 | acc: 50.00%,  total acc: 79.39%   [EVAL] batch:   47 | acc: 62.50%,  total acc: 79.04%   [EVAL] batch:   48 | acc: 56.25%,  total acc: 78.57%   [EVAL] batch:   49 | acc: 56.25%,  total acc: 78.12%   [EVAL] batch:   50 | acc: 62.50%,  total acc: 77.82%   [EVAL] batch:   51 | acc: 62.50%,  total acc: 77.52%   [EVAL] batch:   52 | acc: 62.50%,  total acc: 77.24%   [EVAL] batch:   53 | acc: 50.00%,  total acc: 76.74%   [EVAL] batch:   54 | acc: 37.50%,  total acc: 76.02%   [EVAL] batch:   55 | acc: 56.25%,  total acc: 75.67%   [EVAL] batch:   56 | acc: 68.75%,  total acc: 75.55%   [EVAL] batch:   57 | acc: 68.75%,  total acc: 75.43%   [EVAL] batch:   58 | acc: 81.25%,  total acc: 75.53%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 75.83%   [EVAL] batch:   60 | acc: 81.25%,  total acc: 75.92%   [EVAL] batch:   61 | acc: 81.25%,  total acc: 76.01%   [EVAL] batch:   62 | acc: 43.75%,  total acc: 75.50%   [EVAL] batch:   63 | acc: 6.25%,  total acc: 74.41%   [EVAL] batch:   64 | acc: 6.25%,  total acc: 73.37%   [EVAL] batch:   65 | acc: 12.50%,  total acc: 72.44%   [EVAL] batch:   66 | acc: 6.25%,  total acc: 71.46%   [EVAL] batch:   67 | acc: 18.75%,  total acc: 70.68%   [EVAL] batch:   68 | acc: 31.25%,  total acc: 70.11%   [EVAL] batch:   69 | acc: 75.00%,  total acc: 70.18%   [EVAL] batch:   70 | acc: 87.50%,  total acc: 70.42%   [EVAL] batch:   71 | acc: 81.25%,  total acc: 70.57%   [EVAL] batch:   72 | acc: 93.75%,  total acc: 70.89%   [EVAL] batch:   73 | acc: 68.75%,  total acc: 70.86%   [EVAL] batch:   74 | acc: 75.00%,  total acc: 70.92%   [EVAL] batch:   75 | acc: 68.75%,  total acc: 70.89%   [EVAL] batch:   76 | acc: 56.25%,  total acc: 70.70%   [EVAL] batch:   77 | acc: 81.25%,  total acc: 70.83%   [EVAL] batch:   78 | acc: 81.25%,  total acc: 70.97%   [EVAL] batch:   79 | acc: 50.00%,  total acc: 70.70%   [EVAL] batch:   80 | acc: 68.75%,  total acc: 70.68%   [EVAL] batch:   81 | acc: 18.75%,  total acc: 70.05%   [EVAL] batch:   82 | acc: 25.00%,  total acc: 69.50%   [EVAL] batch:   83 | acc: 18.75%,  total acc: 68.90%   [EVAL] batch:   84 | acc: 50.00%,  total acc: 68.68%   [EVAL] batch:   85 | acc: 31.25%,  total acc: 68.24%   [EVAL] batch:   86 | acc: 37.50%,  total acc: 67.89%   [EVAL] batch:   87 | acc: 25.00%,  total acc: 67.40%   [EVAL] batch:   88 | acc: 0.00%,  total acc: 66.64%   [EVAL] batch:   89 | acc: 31.25%,  total acc: 66.25%   [EVAL] batch:   90 | acc: 0.00%,  total acc: 65.52%   [EVAL] batch:   91 | acc: 18.75%,  total acc: 65.01%   [EVAL] batch:   92 | acc: 6.25%,  total acc: 64.38%   [EVAL] batch:   93 | acc: 25.00%,  total acc: 63.96%   [EVAL] batch:   94 | acc: 81.25%,  total acc: 64.14%   [EVAL] batch:   95 | acc: 87.50%,  total acc: 64.39%   [EVAL] batch:   96 | acc: 93.75%,  total acc: 64.69%   [EVAL] batch:   97 | acc: 93.75%,  total acc: 64.99%   [EVAL] batch:   98 | acc: 87.50%,  total acc: 65.21%   [EVAL] batch:   99 | acc: 68.75%,  total acc: 65.25%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 65.59%   [EVAL] batch:  101 | acc: 93.75%,  total acc: 65.87%   [EVAL] batch:  102 | acc: 87.50%,  total acc: 66.08%   [EVAL] batch:  103 | acc: 87.50%,  total acc: 66.29%   [EVAL] batch:  104 | acc: 93.75%,  total acc: 66.55%   [EVAL] batch:  105 | acc: 100.00%,  total acc: 66.86%   [EVAL] batch:  106 | acc: 68.75%,  total acc: 66.88%   [EVAL] batch:  107 | acc: 56.25%,  total acc: 66.78%   [EVAL] batch:  108 | acc: 37.50%,  total acc: 66.51%   [EVAL] batch:  109 | acc: 56.25%,  total acc: 66.42%   [EVAL] batch:  110 | acc: 68.75%,  total acc: 66.44%   [EVAL] batch:  111 | acc: 56.25%,  total acc: 66.35%   [EVAL] batch:  112 | acc: 37.50%,  total acc: 66.10%   [EVAL] batch:  113 | acc: 62.50%,  total acc: 66.06%   [EVAL] batch:  114 | acc: 62.50%,  total acc: 66.03%   [EVAL] batch:  115 | acc: 68.75%,  total acc: 66.06%   [EVAL] batch:  116 | acc: 75.00%,  total acc: 66.13%   [EVAL] batch:  117 | acc: 75.00%,  total acc: 66.21%   [EVAL] batch:  118 | acc: 62.50%,  total acc: 66.18%   [EVAL] batch:  119 | acc: 50.00%,  total acc: 66.04%   [EVAL] batch:  120 | acc: 43.75%,  total acc: 65.86%   [EVAL] batch:  121 | acc: 50.00%,  total acc: 65.73%   [EVAL] batch:  122 | acc: 50.00%,  total acc: 65.60%   [EVAL] batch:  123 | acc: 75.00%,  total acc: 65.68%   [EVAL] batch:  124 | acc: 62.50%,  total acc: 65.65%   [EVAL] batch:  125 | acc: 81.25%,  total acc: 65.77%   [EVAL] batch:  126 | acc: 68.75%,  total acc: 65.80%   [EVAL] batch:  127 | acc: 62.50%,  total acc: 65.77%   [EVAL] batch:  128 | acc: 31.25%,  total acc: 65.50%   [EVAL] batch:  129 | acc: 81.25%,  total acc: 65.62%   [EVAL] batch:  130 | acc: 68.75%,  total acc: 65.65%   [EVAL] batch:  131 | acc: 75.00%,  total acc: 65.72%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 65.98%   [EVAL] batch:  133 | acc: 81.25%,  total acc: 66.09%   [EVAL] batch:  134 | acc: 68.75%,  total acc: 66.11%   [EVAL] batch:  135 | acc: 81.25%,  total acc: 66.22%   [EVAL] batch:  136 | acc: 87.50%,  total acc: 66.38%   [EVAL] batch:  137 | acc: 68.75%,  total acc: 66.39%   [EVAL] batch:  138 | acc: 62.50%,  total acc: 66.37%   [EVAL] batch:  139 | acc: 37.50%,  total acc: 66.16%   [EVAL] batch:  140 | acc: 43.75%,  total acc: 66.00%   [EVAL] batch:  141 | acc: 62.50%,  total acc: 65.98%   [EVAL] batch:  142 | acc: 43.75%,  total acc: 65.82%   [EVAL] batch:  143 | acc: 62.50%,  total acc: 65.80%   [EVAL] batch:  144 | acc: 31.25%,  total acc: 65.56%   [EVAL] batch:  145 | acc: 56.25%,  total acc: 65.50%   [EVAL] batch:  146 | acc: 43.75%,  total acc: 65.35%   [EVAL] batch:  147 | acc: 31.25%,  total acc: 65.12%   [EVAL] batch:  148 | acc: 50.00%,  total acc: 65.02%   [EVAL] batch:  149 | acc: 37.50%,  total acc: 64.83%   [EVAL] batch:  150 | acc: 81.25%,  total acc: 64.94%   [EVAL] batch:  151 | acc: 87.50%,  total acc: 65.09%   [EVAL] batch:  152 | acc: 75.00%,  total acc: 65.16%   [EVAL] batch:  153 | acc: 81.25%,  total acc: 65.26%   [EVAL] batch:  154 | acc: 75.00%,  total acc: 65.32%   [EVAL] batch:  155 | acc: 68.75%,  total acc: 65.34%   [EVAL] batch:  156 | acc: 75.00%,  total acc: 65.41%   [EVAL] batch:  157 | acc: 56.25%,  total acc: 65.35%   [EVAL] batch:  158 | acc: 62.50%,  total acc: 65.33%   [EVAL] batch:  159 | acc: 62.50%,  total acc: 65.31%   [EVAL] batch:  160 | acc: 62.50%,  total acc: 65.30%   [EVAL] batch:  161 | acc: 43.75%,  total acc: 65.16%   [EVAL] batch:  162 | acc: 50.00%,  total acc: 65.07%   [EVAL] batch:  163 | acc: 62.50%,  total acc: 65.05%   [EVAL] batch:  164 | acc: 62.50%,  total acc: 65.04%   [EVAL] batch:  165 | acc: 56.25%,  total acc: 64.98%   [EVAL] batch:  166 | acc: 62.50%,  total acc: 64.97%   [EVAL] batch:  167 | acc: 68.75%,  total acc: 64.99%   [EVAL] batch:  168 | acc: 62.50%,  total acc: 64.98%   [EVAL] batch:  169 | acc: 37.50%,  total acc: 64.82%   [EVAL] batch:  170 | acc: 25.00%,  total acc: 64.58%   [EVAL] batch:  171 | acc: 18.75%,  total acc: 64.32%   [EVAL] batch:  172 | acc: 37.50%,  total acc: 64.16%   [EVAL] batch:  173 | acc: 25.00%,  total acc: 63.94%   [EVAL] batch:  174 | acc: 31.25%,  total acc: 63.75%   [EVAL] batch:  175 | acc: 100.00%,  total acc: 63.96%   [EVAL] batch:  176 | acc: 100.00%,  total acc: 64.16%   [EVAL] batch:  177 | acc: 100.00%,  total acc: 64.36%   [EVAL] batch:  178 | acc: 100.00%,  total acc: 64.56%   [EVAL] batch:  179 | acc: 100.00%,  total acc: 64.76%   [EVAL] batch:  180 | acc: 100.00%,  total acc: 64.95%   [EVAL] batch:  181 | acc: 100.00%,  total acc: 65.14%   [EVAL] batch:  182 | acc: 100.00%,  total acc: 65.33%   [EVAL] batch:  183 | acc: 93.75%,  total acc: 65.49%   [EVAL] batch:  184 | acc: 100.00%,  total acc: 65.68%   [EVAL] batch:  185 | acc: 93.75%,  total acc: 65.83%   [EVAL] batch:  186 | acc: 93.75%,  total acc: 65.98%   [EVAL] batch:  187 | acc: 81.25%,  total acc: 66.06%   [EVAL] batch:  188 | acc: 68.75%,  total acc: 66.07%   [EVAL] batch:  189 | acc: 68.75%,  total acc: 66.09%   [EVAL] batch:  190 | acc: 68.75%,  total acc: 66.10%   [EVAL] batch:  191 | acc: 68.75%,  total acc: 66.11%   [EVAL] batch:  192 | acc: 75.00%,  total acc: 66.16%   [EVAL] batch:  193 | acc: 75.00%,  total acc: 66.20%   [EVAL] batch:  194 | acc: 81.25%,  total acc: 66.28%   [EVAL] batch:  195 | acc: 75.00%,  total acc: 66.33%   [EVAL] batch:  196 | acc: 68.75%,  total acc: 66.34%   [EVAL] batch:  197 | acc: 87.50%,  total acc: 66.45%   [EVAL] batch:  198 | acc: 75.00%,  total acc: 66.49%   [EVAL] batch:  199 | acc: 81.25%,  total acc: 66.56%   [EVAL] batch:  200 | acc: 75.00%,  total acc: 66.60%   [EVAL] batch:  201 | acc: 75.00%,  total acc: 66.65%   [EVAL] batch:  202 | acc: 68.75%,  total acc: 66.66%   [EVAL] batch:  203 | acc: 75.00%,  total acc: 66.70%   [EVAL] batch:  204 | acc: 75.00%,  total acc: 66.74%   [EVAL] batch:  205 | acc: 81.25%,  total acc: 66.81%   [EVAL] batch:  206 | acc: 31.25%,  total acc: 66.64%   [EVAL] batch:  207 | acc: 12.50%,  total acc: 66.38%   [EVAL] batch:  208 | acc: 25.00%,  total acc: 66.18%   [EVAL] batch:  209 | acc: 12.50%,  total acc: 65.92%   [EVAL] batch:  210 | acc: 25.00%,  total acc: 65.73%   [EVAL] batch:  211 | acc: 25.00%,  total acc: 65.54%   [EVAL] batch:  212 | acc: 43.75%,  total acc: 65.43%   [EVAL] batch:  213 | acc: 93.75%,  total acc: 65.57%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 65.73%   [EVAL] batch:  215 | acc: 68.75%,  total acc: 65.74%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 65.90%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 66.06%   [EVAL] batch:  218 | acc: 93.75%,  total acc: 66.18%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 66.34%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 66.49%   [EVAL] batch:  221 | acc: 100.00%,  total acc: 66.64%   [EVAL] batch:  222 | acc: 100.00%,  total acc: 66.79%   [EVAL] batch:  223 | acc: 100.00%,  total acc: 66.94%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 67.08%   [EVAL] batch:  225 | acc: 93.75%,  total acc: 67.20%   [EVAL] batch:  226 | acc: 93.75%,  total acc: 67.32%   [EVAL] batch:  227 | acc: 93.75%,  total acc: 67.43%   [EVAL] batch:  228 | acc: 87.50%,  total acc: 67.52%   [EVAL] batch:  229 | acc: 100.00%,  total acc: 67.66%   [EVAL] batch:  230 | acc: 93.75%,  total acc: 67.78%   [EVAL] batch:  231 | acc: 81.25%,  total acc: 67.83%   [EVAL] batch:  232 | acc: 68.75%,  total acc: 67.84%   [EVAL] batch:  233 | acc: 87.50%,  total acc: 67.92%   [EVAL] batch:  234 | acc: 62.50%,  total acc: 67.90%   [EVAL] batch:  235 | acc: 81.25%,  total acc: 67.96%   [EVAL] batch:  236 | acc: 62.50%,  total acc: 67.93%   [EVAL] batch:  237 | acc: 93.75%,  total acc: 68.04%   [EVAL] batch:  238 | acc: 87.50%,  total acc: 68.12%   [EVAL] batch:  239 | acc: 93.75%,  total acc: 68.23%   [EVAL] batch:  240 | acc: 81.25%,  total acc: 68.28%   [EVAL] batch:  241 | acc: 87.50%,  total acc: 68.36%   [EVAL] batch:  242 | acc: 93.75%,  total acc: 68.47%   [EVAL] batch:  243 | acc: 87.50%,  total acc: 68.55%   [EVAL] batch:  244 | acc: 37.50%,  total acc: 68.42%   [EVAL] batch:  245 | acc: 50.00%,  total acc: 68.34%   [EVAL] batch:  246 | acc: 31.25%,  total acc: 68.19%   [EVAL] batch:  247 | acc: 31.25%,  total acc: 68.04%   [EVAL] batch:  248 | acc: 25.00%,  total acc: 67.87%   [EVAL] batch:  249 | acc: 50.00%,  total acc: 67.80%   [EVAL] batch:  250 | acc: 12.50%,  total acc: 67.58%   [EVAL] batch:  251 | acc: 12.50%,  total acc: 67.36%   [EVAL] batch:  252 | acc: 25.00%,  total acc: 67.19%   [EVAL] batch:  253 | acc: 25.00%,  total acc: 67.03%   [EVAL] batch:  254 | acc: 25.00%,  total acc: 66.86%   [EVAL] batch:  255 | acc: 18.75%,  total acc: 66.67%   [EVAL] batch:  256 | acc: 43.75%,  total acc: 66.59%   [EVAL] batch:  257 | acc: 75.00%,  total acc: 66.62%   [EVAL] batch:  258 | acc: 68.75%,  total acc: 66.63%   [EVAL] batch:  259 | acc: 75.00%,  total acc: 66.66%   [EVAL] batch:  260 | acc: 43.75%,  total acc: 66.57%   [EVAL] batch:  261 | acc: 68.75%,  total acc: 66.58%   [EVAL] batch:  262 | acc: 87.50%,  total acc: 66.66%   [EVAL] batch:  263 | acc: 93.75%,  total acc: 66.76%   [EVAL] batch:  264 | acc: 81.25%,  total acc: 66.82%   [EVAL] batch:  265 | acc: 81.25%,  total acc: 66.87%   [EVAL] batch:  266 | acc: 87.50%,  total acc: 66.95%   [EVAL] batch:  267 | acc: 100.00%,  total acc: 67.07%   [EVAL] batch:  268 | acc: 100.00%,  total acc: 67.19%   [EVAL] batch:  269 | acc: 93.75%,  total acc: 67.29%   [EVAL] batch:  270 | acc: 100.00%,  total acc: 67.41%   [EVAL] batch:  271 | acc: 100.00%,  total acc: 67.53%   [EVAL] batch:  272 | acc: 93.75%,  total acc: 67.63%   [EVAL] batch:  273 | acc: 93.75%,  total acc: 67.72%   [EVAL] batch:  274 | acc: 100.00%,  total acc: 67.84%   [EVAL] batch:  275 | acc: 81.25%,  total acc: 67.89%   [EVAL] batch:  276 | acc: 68.75%,  total acc: 67.89%   [EVAL] batch:  277 | acc: 81.25%,  total acc: 67.94%   [EVAL] batch:  278 | acc: 62.50%,  total acc: 67.92%   [EVAL] batch:  279 | acc: 75.00%,  total acc: 67.95%   [EVAL] batch:  280 | acc: 75.00%,  total acc: 67.97%   [EVAL] batch:  281 | acc: 56.25%,  total acc: 67.93%   [EVAL] batch:  282 | acc: 62.50%,  total acc: 67.91%   [EVAL] batch:  283 | acc: 18.75%,  total acc: 67.74%   [EVAL] batch:  284 | acc: 31.25%,  total acc: 67.61%   [EVAL] batch:  285 | acc: 25.00%,  total acc: 67.46%   [EVAL] batch:  286 | acc: 31.25%,  total acc: 67.33%   [EVAL] batch:  287 | acc: 43.75%,  total acc: 67.25%   [EVAL] batch:  288 | acc: 50.00%,  total acc: 67.19%   [EVAL] batch:  289 | acc: 68.75%,  total acc: 67.20%   [EVAL] batch:  290 | acc: 31.25%,  total acc: 67.07%   [EVAL] batch:  291 | acc: 50.00%,  total acc: 67.02%   [EVAL] batch:  292 | acc: 50.00%,  total acc: 66.96%   [EVAL] batch:  293 | acc: 56.25%,  total acc: 66.92%   [EVAL] batch:  294 | acc: 81.25%,  total acc: 66.97%   [EVAL] batch:  295 | acc: 68.75%,  total acc: 66.98%   [EVAL] batch:  296 | acc: 68.75%,  total acc: 66.98%   [EVAL] batch:  297 | acc: 56.25%,  total acc: 66.95%   [EVAL] batch:  298 | acc: 75.00%,  total acc: 66.97%   [EVAL] batch:  299 | acc: 81.25%,  total acc: 67.02%   [EVAL] batch:  300 | acc: 100.00%,  total acc: 67.13%   [EVAL] batch:  301 | acc: 100.00%,  total acc: 67.24%   [EVAL] batch:  302 | acc: 100.00%,  total acc: 67.35%   [EVAL] batch:  303 | acc: 100.00%,  total acc: 67.45%   [EVAL] batch:  304 | acc: 100.00%,  total acc: 67.56%   [EVAL] batch:  305 | acc: 100.00%,  total acc: 67.67%   [EVAL] batch:  306 | acc: 100.00%,  total acc: 67.77%   [EVAL] batch:  307 | acc: 100.00%,  total acc: 67.88%   [EVAL] batch:  308 | acc: 100.00%,  total acc: 67.98%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 68.08%   [EVAL] batch:  310 | acc: 100.00%,  total acc: 68.19%   [EVAL] batch:  311 | acc: 100.00%,  total acc: 68.29%   [EVAL] batch:  312 | acc: 81.25%,  total acc: 68.33%   [EVAL] batch:  313 | acc: 50.00%,  total acc: 68.27%   [EVAL] batch:  314 | acc: 50.00%,  total acc: 68.21%   [EVAL] batch:  315 | acc: 68.75%,  total acc: 68.22%   [EVAL] batch:  316 | acc: 50.00%,  total acc: 68.16%   [EVAL] batch:  317 | acc: 56.25%,  total acc: 68.12%   [EVAL] batch:  318 | acc: 87.50%,  total acc: 68.18%   [EVAL] batch:  319 | acc: 75.00%,  total acc: 68.20%   [EVAL] batch:  320 | acc: 75.00%,  total acc: 68.22%   [EVAL] batch:  321 | acc: 75.00%,  total acc: 68.25%   [EVAL] batch:  322 | acc: 68.75%,  total acc: 68.25%   [EVAL] batch:  323 | acc: 50.00%,  total acc: 68.19%   [EVAL] batch:  324 | acc: 56.25%,  total acc: 68.15%   [EVAL] batch:  325 | acc: 31.25%,  total acc: 68.04%   [EVAL] batch:  326 | acc: 25.00%,  total acc: 67.91%   [EVAL] batch:  327 | acc: 12.50%,  total acc: 67.74%   [EVAL] batch:  328 | acc: 37.50%,  total acc: 67.65%   [EVAL] batch:  329 | acc: 50.00%,  total acc: 67.59%   [EVAL] batch:  330 | acc: 18.75%,  total acc: 67.45%   [EVAL] batch:  331 | acc: 75.00%,  total acc: 67.47%   [EVAL] batch:  332 | acc: 100.00%,  total acc: 67.57%   [EVAL] batch:  333 | acc: 100.00%,  total acc: 67.66%   [EVAL] batch:  334 | acc: 100.00%,  total acc: 67.76%   [EVAL] batch:  335 | acc: 100.00%,  total acc: 67.86%   [EVAL] batch:  336 | acc: 93.75%,  total acc: 67.93%   [EVAL] batch:  337 | acc: 56.25%,  total acc: 67.90%   [EVAL] batch:  338 | acc: 0.00%,  total acc: 67.70%   [EVAL] batch:  339 | acc: 6.25%,  total acc: 67.52%   [EVAL] batch:  340 | acc: 12.50%,  total acc: 67.36%   [EVAL] batch:  341 | acc: 37.50%,  total acc: 67.27%   [EVAL] batch:  342 | acc: 12.50%,  total acc: 67.11%   [EVAL] batch:  343 | acc: 18.75%,  total acc: 66.97%   [EVAL] batch:  344 | acc: 12.50%,  total acc: 66.81%   [EVAL] batch:  345 | acc: 31.25%,  total acc: 66.71%   [EVAL] batch:  346 | acc: 50.00%,  total acc: 66.66%   [EVAL] batch:  347 | acc: 50.00%,  total acc: 66.61%   [EVAL] batch:  348 | acc: 43.75%,  total acc: 66.55%   [EVAL] batch:  349 | acc: 50.00%,  total acc: 66.50%   [EVAL] batch:  350 | acc: 31.25%,  total acc: 66.40%   [EVAL] batch:  351 | acc: 50.00%,  total acc: 66.35%   [EVAL] batch:  352 | acc: 50.00%,  total acc: 66.31%   [EVAL] batch:  353 | acc: 43.75%,  total acc: 66.24%   [EVAL] batch:  354 | acc: 50.00%,  total acc: 66.20%   [EVAL] batch:  355 | acc: 31.25%,  total acc: 66.10%   [EVAL] batch:  356 | acc: 75.00%,  total acc: 66.12%   [EVAL] batch:  357 | acc: 87.50%,  total acc: 66.18%   [EVAL] batch:  358 | acc: 75.00%,  total acc: 66.21%   [EVAL] batch:  359 | acc: 93.75%,  total acc: 66.28%   [EVAL] batch:  360 | acc: 62.50%,  total acc: 66.27%   [EVAL] batch:  361 | acc: 81.25%,  total acc: 66.32%   [EVAL] batch:  362 | acc: 75.00%,  total acc: 66.34%   [EVAL] batch:  363 | acc: 62.50%,  total acc: 66.33%   [EVAL] batch:  364 | acc: 68.75%,  total acc: 66.34%   [EVAL] batch:  365 | acc: 43.75%,  total acc: 66.27%   [EVAL] batch:  366 | acc: 68.75%,  total acc: 66.28%   [EVAL] batch:  367 | acc: 62.50%,  total acc: 66.27%   [EVAL] batch:  368 | acc: 56.25%,  total acc: 66.24%   [EVAL] batch:  369 | acc: 93.75%,  total acc: 66.32%   [EVAL] batch:  370 | acc: 93.75%,  total acc: 66.39%   [EVAL] batch:  371 | acc: 81.25%,  total acc: 66.43%   [EVAL] batch:  372 | acc: 81.25%,  total acc: 66.47%   [EVAL] batch:  373 | acc: 81.25%,  total acc: 66.51%   [EVAL] batch:  374 | acc: 100.00%,  total acc: 66.60%   [EVAL] batch:  375 | acc: 93.75%,  total acc: 66.67%   [EVAL] batch:  376 | acc: 87.50%,  total acc: 66.73%   [EVAL] batch:  377 | acc: 100.00%,  total acc: 66.82%   [EVAL] batch:  378 | acc: 87.50%,  total acc: 66.87%   [EVAL] batch:  379 | acc: 93.75%,  total acc: 66.94%   [EVAL] batch:  380 | acc: 100.00%,  total acc: 67.03%   [EVAL] batch:  381 | acc: 93.75%,  total acc: 67.10%   [EVAL] batch:  382 | acc: 81.25%,  total acc: 67.13%   [EVAL] batch:  383 | acc: 62.50%,  total acc: 67.12%   [EVAL] batch:  384 | acc: 62.50%,  total acc: 67.11%   [EVAL] batch:  385 | acc: 81.25%,  total acc: 67.15%   [EVAL] batch:  386 | acc: 50.00%,  total acc: 67.10%   [EVAL] batch:  387 | acc: 56.25%,  total acc: 67.07%   [EVAL] batch:  388 | acc: 68.75%,  total acc: 67.08%   [EVAL] batch:  389 | acc: 56.25%,  total acc: 67.05%   [EVAL] batch:  390 | acc: 43.75%,  total acc: 66.99%   [EVAL] batch:  391 | acc: 37.50%,  total acc: 66.92%   [EVAL] batch:  392 | acc: 43.75%,  total acc: 66.86%   [EVAL] batch:  393 | acc: 50.00%,  total acc: 66.81%   [EVAL] batch:  394 | acc: 56.25%,  total acc: 66.79%   [EVAL] batch:  395 | acc: 62.50%,  total acc: 66.78%   [EVAL] batch:  396 | acc: 68.75%,  total acc: 66.78%   [EVAL] batch:  397 | acc: 75.00%,  total acc: 66.80%   [EVAL] batch:  398 | acc: 75.00%,  total acc: 66.82%   [EVAL] batch:  399 | acc: 56.25%,  total acc: 66.80%   [EVAL] batch:  400 | acc: 100.00%,  total acc: 66.88%   [EVAL] batch:  401 | acc: 100.00%,  total acc: 66.96%   [EVAL] batch:  402 | acc: 100.00%,  total acc: 67.04%   [EVAL] batch:  403 | acc: 100.00%,  total acc: 67.13%   [EVAL] batch:  404 | acc: 100.00%,  total acc: 67.21%   [EVAL] batch:  405 | acc: 100.00%,  total acc: 67.29%   [EVAL] batch:  406 | acc: 81.25%,  total acc: 67.32%   [EVAL] batch:  407 | acc: 75.00%,  total acc: 67.34%   [EVAL] batch:  408 | acc: 68.75%,  total acc: 67.34%   [EVAL] batch:  409 | acc: 81.25%,  total acc: 67.38%   [EVAL] batch:  410 | acc: 75.00%,  total acc: 67.40%   [EVAL] batch:  411 | acc: 62.50%,  total acc: 67.38%   [EVAL] batch:  412 | acc: 75.00%,  total acc: 67.40%   [EVAL] batch:  413 | acc: 100.00%,  total acc: 67.48%   [EVAL] batch:  414 | acc: 100.00%,  total acc: 67.56%   [EVAL] batch:  415 | acc: 100.00%,  total acc: 67.64%   [EVAL] batch:  416 | acc: 100.00%,  total acc: 67.72%   [EVAL] batch:  417 | acc: 100.00%,  total acc: 67.79%   [EVAL] batch:  418 | acc: 93.75%,  total acc: 67.86%   [EVAL] batch:  419 | acc: 100.00%,  total acc: 67.93%   [EVAL] batch:  420 | acc: 100.00%,  total acc: 68.01%   [EVAL] batch:  421 | acc: 100.00%,  total acc: 68.08%   [EVAL] batch:  422 | acc: 100.00%,  total acc: 68.16%   [EVAL] batch:  423 | acc: 100.00%,  total acc: 68.23%   [EVAL] batch:  424 | acc: 93.75%,  total acc: 68.29%   [EVAL] batch:  425 | acc: 87.50%,  total acc: 68.34%   [EVAL] batch:  426 | acc: 87.50%,  total acc: 68.38%   [EVAL] batch:  427 | acc: 93.75%,  total acc: 68.44%   [EVAL] batch:  428 | acc: 93.75%,  total acc: 68.50%   [EVAL] batch:  429 | acc: 81.25%,  total acc: 68.53%   [EVAL] batch:  430 | acc: 93.75%,  total acc: 68.59%   [EVAL] batch:  431 | acc: 93.75%,  total acc: 68.65%   [EVAL] batch:  432 | acc: 93.75%,  total acc: 68.71%   [EVAL] batch:  433 | acc: 100.00%,  total acc: 68.78%   [EVAL] batch:  434 | acc: 100.00%,  total acc: 68.85%   [EVAL] batch:  435 | acc: 100.00%,  total acc: 68.92%   [EVAL] batch:  436 | acc: 87.50%,  total acc: 68.96%   [EVAL] batch:  437 | acc: 50.00%,  total acc: 68.92%   
cur_acc:  ['0.9494', '0.7431', '0.7530', '0.7708', '0.7569', '0.6280', '0.8274']
his_acc:  ['0.9494', '0.8230', '0.7680', '0.7475', '0.7167', '0.6898', '0.6892']
CurrentTrain: epoch 15, batch     0 | loss: 15.0640497CurrentTrain: epoch 15, batch     1 | loss: 20.8232251CurrentTrain: epoch 15, batch     2 | loss: 19.5577059CurrentTrain: epoch  1, batch     3 | loss: 9.2923221CurrentTrain: epoch 15, batch     0 | loss: 12.6669835CurrentTrain: epoch 15, batch     1 | loss: 12.1893987CurrentTrain: epoch 15, batch     2 | loss: 14.6294070CurrentTrain: epoch  1, batch     3 | loss: 8.3151272CurrentTrain: epoch 15, batch     0 | loss: 18.5695188CurrentTrain: epoch 15, batch     1 | loss: 17.4943620CurrentTrain: epoch 15, batch     2 | loss: 21.2699822CurrentTrain: epoch  1, batch     3 | loss: 8.5046922CurrentTrain: epoch 15, batch     0 | loss: 15.4527677CurrentTrain: epoch 15, batch     1 | loss: 12.6687554CurrentTrain: epoch 15, batch     2 | loss: 24.9700010CurrentTrain: epoch  1, batch     3 | loss: 6.7123648CurrentTrain: epoch 15, batch     0 | loss: 16.2185812CurrentTrain: epoch 15, batch     1 | loss: 10.2296755CurrentTrain: epoch 15, batch     2 | loss: 18.0571639CurrentTrain: epoch  1, batch     3 | loss: 6.3772289CurrentTrain: epoch 15, batch     0 | loss: 9.4951023CurrentTrain: epoch 15, batch     1 | loss: 9.3957306CurrentTrain: epoch 15, batch     2 | loss: 9.5289033CurrentTrain: epoch  1, batch     3 | loss: 14.2596569CurrentTrain: epoch 15, batch     0 | loss: 9.0788252CurrentTrain: epoch 15, batch     1 | loss: 14.3907122CurrentTrain: epoch 15, batch     2 | loss: 10.7601910CurrentTrain: epoch  1, batch     3 | loss: 8.1321549CurrentTrain: epoch 15, batch     0 | loss: 19.4222171CurrentTrain: epoch 15, batch     1 | loss: 14.8270543CurrentTrain: epoch 15, batch     2 | loss: 10.9802289CurrentTrain: epoch  1, batch     3 | loss: 14.8365175CurrentTrain: epoch 15, batch     0 | loss: 11.4760178CurrentTrain: epoch 15, batch     1 | loss: 18.5004026CurrentTrain: epoch 15, batch     2 | loss: 16.1963141CurrentTrain: epoch  1, batch     3 | loss: 7.3872047CurrentTrain: epoch 15, batch     0 | loss: 6.9181240CurrentTrain: epoch 15, batch     1 | loss: 13.5449894CurrentTrain: epoch 15, batch     2 | loss: 10.2319983CurrentTrain: epoch  1, batch     3 | loss: 7.0081051
#### follows ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "follows" means immediately prior item in a series of which the subject is a part.
Here is an example:
Relation: follows
Context: ( 1988 ) , " night watch " ( 2004 ) , " day watch " ( 2006 ) and " legend № 17 " ( 2013 ) .
Head Entity: day watch
Tail Entity: night watch
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "follows":
Relation: follows  
Context: ( 1995 ) , " The Phantom Menace " ( 1999 ) , " Attack of the Clones " ( 2002 ) and " Revenge of the Sith " ( 2005 ) .  
Head Entity: Attack of the Clones  
Tail Entity: The Phantom Menace  

Relation: follows  
Context: ( 2010 ) , " The Hunger Games " ( 2012 ) , " Catching Fire " ( 2013 ) and " Mockingjay " ( 2014 ) .  
Head Entity: Catching Fire  
Tail Entity: The Hunger Games  
#### sport ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "sport" means sport that the subject participates or participated in or is associated with.
Here is an example:
Relation: sport
Context: dick packer was a u.s. soccer center forward who was a member of the u.s. team at the 1956 summer olympics .
Head Entity: dick packer
Tail Entity: soccer
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "sport":
Relation: sport  
Context: serena williams is a professional tennis player who has won numerous grand slam titles throughout her career.  
Head Entity: serena williams  
Tail Entity: tennis  

Relation: sport  
Context: lebron james is known for his exceptional skills in basketball and has played for the los angeles lakers in the nba.  
Head Entity: lebron james  
Tail Entity: basketball  
#### father ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "father" means male parent of the subject.
Here is an example:
Relation: father
Context: after the crowning of his nephew radoslav , the son of stefan , sava left the serbian maritime in 1229 for a trip to palestine .
Head Entity: radoslav
Tail Entity: stefan
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "father":
Relation: father  
Context: in the historical records, it is noted that king henry was the father of prince edward, who later became a significant figure in the kingdom.  
Head Entity: prince edward  
Tail Entity: king henry  

Relation: father  
Context: during the family reunion, it was revealed that michael, the youngest son, was the father of three children, including his daughter emily.  
Head Entity: emily  
Tail Entity: michael  
#### original network ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "original network" means network or service that originally broadcast a radio or television program.
Here is an example:
Relation: original network
Context: since 2009 , culshaw has starred in the bbc one comedy sketch show " the impressions show " alongside debra stephenson .
Head Entity: the impressions show
Tail Entity: bbc one
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "original network":
Relation: original network  
Context: The popular series "Breaking Bad" first aired on AMC, captivating audiences with its intense storytelling and character development.  
Head Entity: Breaking Bad  
Tail Entity: AMC  

Relation: original network  
Context: "Friends" became a cultural phenomenon when it premiered on NBC, showcasing the lives of six friends living in New York City.  
Head Entity: Friends  
Tail Entity: NBC  
#### original language of film or TV show ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "original language of film or TV show" means language in which a film or a performance work was originally created.
Here is an example:
Relation: original language of film or TV show
Context: the 1954 film , directed by kadri venkata reddy , " peddamanushulu " was honoured with the first president 's silver medal for best feature film in telugu .
Head Entity: peddamanushulu
Tail Entity: telugu
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "original language of film or TV show":
Relation: original language of film or TV show  
Context: the 1994 animated film "The Lion King," produced by Walt Disney, was originally created in English and has since been dubbed into multiple languages.  
Head Entity: The Lion King  
Tail Entity: English  

Relation: original language of film or TV show  
Context: the critically acclaimed 2016 film "La La Land," directed by Damien Chazelle, was originally filmed in English, capturing the essence of modern-day Los Angeles.  
Head Entity: La La Land  
Tail Entity: English  
#### league ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "league" means league in which team or player plays or has played in.
Here is an example:
Relation: league
Context: in the summer of 2013 he returned to montenegro and joined with fk rudar pljevlja , playing with the club for two seasons in the montenegrin first league .
Head Entity: fk rudar pljevlja
Tail Entity: montenegrin first league
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "league":
Relation: league  
Context: After a successful stint in the English Premier League, the player signed with FC Barcelona, competing in La Liga for the next three seasons.  
Head Entity: FC Barcelona  
Tail Entity: La Liga  

Relation: league  
Context: The young athlete made headlines when he joined the New York Yankees, showcasing his talent in Major League Baseball.  
Head Entity: New York Yankees  
Tail Entity: Major League Baseball  
#### mother ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "mother" means female parent of the subject.
Here is an example:
Relation: mother
Context: the first stewart king of scotland and son of marjorie bruce and walter stewart , robert ii , is believed to have been born in the abbey .
Head Entity: robert ii
Tail Entity: marjorie bruce
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "mother":
Relation: mother  
Context: in the historical records, it is noted that queen elizabeth i was the daughter of anne boleyn, who served as the second wife of king henry viii.  
Head Entity: queen elizabeth i  
Tail Entity: anne boleyn  

Relation: mother  
Context: the famous artist pablo picasso often spoke fondly of his mother, maría ruiz, who greatly influenced his early artistic development.  
Head Entity: pablo picasso  
Tail Entity: maría ruiz  
#### instrument ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "instrument" means musical instrument that a person plays or teaches or used in a music occupation.
Here is an example:
Relation: instrument
Context: peter bastian ( 25 august 1943 in copenhagen – 27 march 2017 ) was a danish musician . peter bastian bio and cv he was educated in theoretical physics and classical bassoon .
Head Entity: peter bastian
Tail Entity: bassoon
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "instrument":
Relation: instrument  
Context: johann sebastian bach (31 march 1685 – 28 july 1750) was a german composer and musician of the baroque period. he was known for his exceptional skill in playing the organ and the harpsichord.  
Head Entity: johann sebastian bach  
Tail Entity: organ  

Relation: instrument  
Context: yo-yo ma, born on october 7, 1955, is a world-renowned cellist who has performed with numerous orchestras and in various genres. he is particularly famous for his interpretations of classical cello concertos.  
Head Entity: yo-yo ma  
Tail Entity: cello  
#### country ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "country" means sovereign state of this item (not to be used for human beings).
Here is an example:
Relation: country
Context: a u.s. government - funded $ 36 million bridge over the panj river connects sher khan bandar in afghanistan with nizhniy pyanzh in tajikistan , which transport more than 150 trucks or 1,000 cars daily .
Head Entity: sher khan bandar
Tail Entity: afghanistan
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "country":
Relation: country  
Context: the ancient city of petra, located in southern jordan, is famous for its rock-cut architecture and water conduit system, attracting thousands of tourists each year.  
Head Entity: petra  
Tail Entity: jordan  

Relation: country  
Context: the great wall of china, a series of fortifications made of various materials, stretches across northern china and is a UNESCO World Heritage site.  
Head Entity: great wall of china  
Tail Entity: china  
#### characters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "characters" means characters which appear in this item (like plays, operas, operettas, books, comics, films, TV series, video games).
Here is an example:
Relation: characters
Context: hagrid suggests in " harry potter and the chamber of secrets " that " " they 're startin ' ter think the job 's jinxed .
Head Entity: harry potter and the chamber of secrets
Tail Entity: hagrid
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "characters":
Relation: characters  
Context: In the animated series "Avatar: The Last Airbender," Aang, the last Airbender, teams up with Katara and Sokka to defeat the Fire Nation.  
Head Entity: Avatar: The Last Airbender  
Tail Entity: Aang  

Relation: characters  
Context: The novel "Pride and Prejudice" features Elizabeth Bennet as she navigates societal expectations and her relationship with Mr. Darcy.  
Head Entity: Pride and Prejudice  
Tail Entity: Elizabeth Bennet  
MemoryTrain:  epoch 15, batch     0 | loss: 2.9058756MemoryTrain:  epoch 15, batch     1 | loss: 4.7202927MemoryTrain:  epoch 15, batch     2 | loss: 5.4185137MemoryTrain:  epoch 15, batch     3 | loss: 4.9726781MemoryTrain:  epoch 15, batch     4 | loss: 2.5662160MemoryTrain:  epoch 15, batch     5 | loss: 2.3556808MemoryTrain:  epoch 15, batch     6 | loss: 2.2781335MemoryTrain:  epoch 15, batch     7 | loss: 2.4455654MemoryTrain:  epoch 15, batch     8 | loss: 4.1196182MemoryTrain:  epoch 15, batch     9 | loss: 3.1392899MemoryTrain:  epoch 15, batch    10 | loss: 2.0944559MemoryTrain:  epoch 15, batch    11 | loss: 4.8998425MemoryTrain:  epoch 15, batch    12 | loss: 2.8983623MemoryTrain:  epoch 15, batch    13 | loss: 3.5470530MemoryTrain:  epoch 15, batch    14 | loss: 3.9278360MemoryTrain:  epoch 15, batch     0 | loss: 2.7881176MemoryTrain:  epoch 15, batch     1 | loss: 4.2256622MemoryTrain:  epoch 15, batch     2 | loss: 2.7565213MemoryTrain:  epoch 15, batch     3 | loss: 2.7456900MemoryTrain:  epoch 15, batch     4 | loss: 2.1500611MemoryTrain:  epoch 15, batch     5 | loss: 2.2834006MemoryTrain:  epoch 15, batch     6 | loss: 2.8936366MemoryTrain:  epoch 15, batch     7 | loss: 2.4682271MemoryTrain:  epoch 15, batch     8 | loss: 4.8038181MemoryTrain:  epoch 15, batch     9 | loss: 3.0701370MemoryTrain:  epoch 15, batch    10 | loss: 2.5368856MemoryTrain:  epoch 15, batch    11 | loss: 2.2058631MemoryTrain:  epoch 15, batch    12 | loss: 1.8900681MemoryTrain:  epoch 15, batch    13 | loss: 2.3126044MemoryTrain:  epoch 15, batch    14 | loss: 3.4721492MemoryTrain:  epoch 15, batch     0 | loss: 3.1397646MemoryTrain:  epoch 15, batch     1 | loss: 3.2698269MemoryTrain:  epoch 15, batch     2 | loss: 2.1412871MemoryTrain:  epoch 15, batch     3 | loss: 4.3346827MemoryTrain:  epoch 15, batch     4 | loss: 5.4591921MemoryTrain:  epoch 15, batch     5 | loss: 2.2535319MemoryTrain:  epoch 15, batch     6 | loss: 2.2244280MemoryTrain:  epoch 15, batch     7 | loss: 4.6469518MemoryTrain:  epoch 15, batch     8 | loss: 2.4958481MemoryTrain:  epoch 15, batch     9 | loss: 1.8633681MemoryTrain:  epoch 15, batch    10 | loss: 1.8735982MemoryTrain:  epoch 15, batch    11 | loss: 4.9256979MemoryTrain:  epoch 15, batch    12 | loss: 1.6748071MemoryTrain:  epoch 15, batch    13 | loss: 2.9985645MemoryTrain:  epoch 15, batch    14 | loss: 1.9663352MemoryTrain:  epoch 15, batch     0 | loss: 2.7067995MemoryTrain:  epoch 15, batch     1 | loss: 1.9869581MemoryTrain:  epoch 15, batch     2 | loss: 1.4886737MemoryTrain:  epoch 15, batch     3 | loss: 2.1493483MemoryTrain:  epoch 15, batch     4 | loss: 5.1637734MemoryTrain:  epoch 15, batch     5 | loss: 1.8510800MemoryTrain:  epoch 15, batch     6 | loss: 4.6439138MemoryTrain:  epoch 15, batch     7 | loss: 2.3026424MemoryTrain:  epoch 15, batch     8 | loss: 4.3340439MemoryTrain:  epoch 15, batch     9 | loss: 1.8346134MemoryTrain:  epoch 15, batch    10 | loss: 1.9355048MemoryTrain:  epoch 15, batch    11 | loss: 2.2331924MemoryTrain:  epoch 15, batch    12 | loss: 4.5865268MemoryTrain:  epoch 15, batch    13 | loss: 1.9837527MemoryTrain:  epoch 15, batch    14 | loss: 1.7186197MemoryTrain:  epoch 15, batch     0 | loss: 2.1236279MemoryTrain:  epoch 15, batch     1 | loss: 1.8473541MemoryTrain:  epoch 15, batch     2 | loss: 3.0533965MemoryTrain:  epoch 15, batch     3 | loss: 1.4817654MemoryTrain:  epoch 15, batch     4 | loss: 4.7134191MemoryTrain:  epoch 15, batch     5 | loss: 1.6343716MemoryTrain:  epoch 15, batch     6 | loss: 2.9793371MemoryTrain:  epoch 15, batch     7 | loss: 1.9552513MemoryTrain:  epoch 15, batch     8 | loss: 4.2819233MemoryTrain:  epoch 15, batch     9 | loss: 1.4538367MemoryTrain:  epoch 15, batch    10 | loss: 2.5865006MemoryTrain:  epoch 15, batch    11 | loss: 1.5064365MemoryTrain:  epoch 15, batch    12 | loss: 1.7418475MemoryTrain:  epoch 15, batch    13 | loss: 2.2358777MemoryTrain:  epoch 15, batch    14 | loss: 2.1088677MemoryTrain:  epoch 15, batch     0 | loss: 2.8701576MemoryTrain:  epoch 15, batch     1 | loss: 2.4096996MemoryTrain:  epoch 15, batch     2 | loss: 1.7192411MemoryTrain:  epoch 15, batch     3 | loss: 2.0255734MemoryTrain:  epoch 15, batch     4 | loss: 2.3902913MemoryTrain:  epoch 15, batch     5 | loss: 2.5242900MemoryTrain:  epoch 15, batch     6 | loss: 1.8051108MemoryTrain:  epoch 15, batch     7 | loss: 2.1718375MemoryTrain:  epoch 15, batch     8 | loss: 3.1604120MemoryTrain:  epoch 15, batch     9 | loss: 1.4736642MemoryTrain:  epoch 15, batch    10 | loss: 1.8011947MemoryTrain:  epoch 15, batch    11 | loss: 1.3546687MemoryTrain:  epoch 15, batch    12 | loss: 2.8894864MemoryTrain:  epoch 15, batch    13 | loss: 2.4713079MemoryTrain:  epoch 15, batch    14 | loss: 2.0826573MemoryTrain:  epoch 15, batch     0 | loss: 1.9023753MemoryTrain:  epoch 15, batch     1 | loss: 2.3868276MemoryTrain:  epoch 15, batch     2 | loss: 4.2507551MemoryTrain:  epoch 15, batch     3 | loss: 1.7344315MemoryTrain:  epoch 15, batch     4 | loss: 4.2912572MemoryTrain:  epoch 15, batch     5 | loss: 1.3162593MemoryTrain:  epoch 15, batch     6 | loss: 4.0883003MemoryTrain:  epoch 15, batch     7 | loss: 1.5993928MemoryTrain:  epoch 15, batch     8 | loss: 2.0370899MemoryTrain:  epoch 15, batch     9 | loss: 1.3298037MemoryTrain:  epoch 15, batch    10 | loss: 4.7049573MemoryTrain:  epoch 15, batch    11 | loss: 2.9553198MemoryTrain:  epoch 15, batch    12 | loss: 1.9057832MemoryTrain:  epoch 15, batch    13 | loss: 1.5170511MemoryTrain:  epoch 15, batch    14 | loss: 1.4509918MemoryTrain:  epoch 15, batch     0 | loss: 1.6817429MemoryTrain:  epoch 15, batch     1 | loss: 2.9317844MemoryTrain:  epoch 15, batch     2 | loss: 1.7258814MemoryTrain:  epoch 15, batch     3 | loss: 1.3306592MemoryTrain:  epoch 15, batch     4 | loss: 3.8673190MemoryTrain:  epoch 15, batch     5 | loss: 1.8437395MemoryTrain:  epoch 15, batch     6 | loss: 1.9858218MemoryTrain:  epoch 15, batch     7 | loss: 1.6353361MemoryTrain:  epoch 15, batch     8 | loss: 1.7702168MemoryTrain:  epoch 15, batch     9 | loss: 1.3876793MemoryTrain:  epoch 15, batch    10 | loss: 4.0970915MemoryTrain:  epoch 15, batch    11 | loss: 1.5389092MemoryTrain:  epoch 15, batch    12 | loss: 2.1062477MemoryTrain:  epoch 15, batch    13 | loss: 2.1877278MemoryTrain:  epoch 15, batch    14 | loss: 2.1452491MemoryTrain:  epoch 15, batch     0 | loss: 1.7286925MemoryTrain:  epoch 15, batch     1 | loss: 2.2042112MemoryTrain:  epoch 15, batch     2 | loss: 1.7173016MemoryTrain:  epoch 15, batch     3 | loss: 2.0967508MemoryTrain:  epoch 15, batch     4 | loss: 1.9204006MemoryTrain:  epoch 15, batch     5 | loss: 1.2533657MemoryTrain:  epoch 15, batch     6 | loss: 4.4036041MemoryTrain:  epoch 15, batch     7 | loss: 4.2866212MemoryTrain:  epoch 15, batch     8 | loss: 1.6123087MemoryTrain:  epoch 15, batch     9 | loss: 1.4738577MemoryTrain:  epoch 15, batch    10 | loss: 1.8191484MemoryTrain:  epoch 15, batch    11 | loss: 1.8170459MemoryTrain:  epoch 15, batch    12 | loss: 3.9561479MemoryTrain:  epoch 15, batch    13 | loss: 1.3310447MemoryTrain:  epoch 15, batch    14 | loss: 1.4829412MemoryTrain:  epoch 15, batch     0 | loss: 1.8830946MemoryTrain:  epoch 15, batch     1 | loss: 1.4592071MemoryTrain:  epoch 15, batch     2 | loss: 1.2657784MemoryTrain:  epoch 15, batch     3 | loss: 1.3035788MemoryTrain:  epoch 15, batch     4 | loss: 1.3939777MemoryTrain:  epoch 15, batch     5 | loss: 1.8112009MemoryTrain:  epoch 15, batch     6 | loss: 1.9404735MemoryTrain:  epoch 15, batch     7 | loss: 1.7974178MemoryTrain:  epoch 15, batch     8 | loss: 1.7236935MemoryTrain:  epoch 15, batch     9 | loss: 1.2642460MemoryTrain:  epoch 15, batch    10 | loss: 3.2154374MemoryTrain:  epoch 15, batch    11 | loss: 2.0852818MemoryTrain:  epoch 15, batch    12 | loss: 2.0349640MemoryTrain:  epoch 15, batch    13 | loss: 4.5418582MemoryTrain:  epoch 15, batch    14 | loss: 1.9900352
[EVAL] batch:    0 | acc: 12.50%,  total acc: 12.50%   [EVAL] batch:    1 | acc: 12.50%,  total acc: 12.50%   [EVAL] batch:    2 | acc: 12.50%,  total acc: 12.50%   [EVAL] batch:    3 | acc: 0.00%,  total acc: 9.38%   [EVAL] batch:    4 | acc: 18.75%,  total acc: 11.25%   [EVAL] batch:    5 | acc: 12.50%,  total acc: 11.46%   [EVAL] batch:    6 | acc: 56.25%,  total acc: 17.86%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 25.00%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 31.25%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 37.50%   [EVAL] batch:   10 | acc: 75.00%,  total acc: 40.91%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 44.27%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 46.63%   [EVAL] batch:   13 | acc: 50.00%,  total acc: 46.88%   [EVAL] batch:   14 | acc: 43.75%,  total acc: 46.67%   [EVAL] batch:   15 | acc: 62.50%,  total acc: 47.66%   [EVAL] batch:   16 | acc: 50.00%,  total acc: 47.79%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 48.96%   [EVAL] batch:   18 | acc: 56.25%,  total acc: 49.34%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 51.88%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 54.17%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 55.97%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 57.61%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 59.11%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 60.25%   [EVAL] batch:   25 | acc: 81.25%,  total acc: 61.06%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 62.27%   [EVAL] batch:   27 | acc: 87.50%,  total acc: 63.17%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 64.22%   [EVAL] batch:   29 | acc: 75.00%,  total acc: 64.58%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 65.32%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 66.41%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 67.42%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 68.20%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 68.93%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 69.62%   [EVAL] batch:   36 | acc: 93.75%,  total acc: 70.27%   [EVAL] batch:   37 | acc: 75.00%,  total acc: 70.39%   [EVAL] batch:   38 | acc: 31.25%,  total acc: 69.39%   [EVAL] batch:   39 | acc: 62.50%,  total acc: 69.22%   [EVAL] batch:   40 | acc: 31.25%,  total acc: 68.29%   [EVAL] batch:   41 | acc: 43.75%,  total acc: 67.71%   [EVAL] batch:   42 | acc: 43.75%,  total acc: 67.15%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 67.47%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 67.92%   [EVAL] batch:   45 | acc: 87.50%,  total acc: 68.34%   [EVAL] batch:   46 | acc: 87.50%,  total acc: 68.75%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 69.40%   [EVAL] batch:   48 | acc: 56.25%,  total acc: 69.13%   [EVAL] batch:   49 | acc: 81.25%,  total acc: 69.38%   [EVAL] batch:   50 | acc: 50.00%,  total acc: 69.00%   [EVAL] batch:   51 | acc: 50.00%,  total acc: 68.63%   [EVAL] batch:   52 | acc: 37.50%,  total acc: 68.04%   [EVAL] batch:   53 | acc: 50.00%,  total acc: 67.71%   [EVAL] batch:   54 | acc: 62.50%,  total acc: 67.61%   [EVAL] batch:   55 | acc: 31.25%,  total acc: 66.96%   [EVAL] batch:   56 | acc: 62.50%,  total acc: 66.89%   [EVAL] batch:   57 | acc: 62.50%,  total acc: 66.81%   [EVAL] batch:   58 | acc: 62.50%,  total acc: 66.74%   [EVAL] batch:   59 | acc: 56.25%,  total acc: 66.56%   [EVAL] batch:   60 | acc: 81.25%,  total acc: 66.80%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 66.83%   [EVAL] batch:   62 | acc: 31.25%,  total acc: 66.27%   
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 79.17%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 79.69%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 77.50%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 79.17%   [EVAL] batch:    6 | acc: 43.75%,  total acc: 74.11%   [EVAL] batch:    7 | acc: 31.25%,  total acc: 68.75%   [EVAL] batch:    8 | acc: 56.25%,  total acc: 67.36%   [EVAL] batch:    9 | acc: 50.00%,  total acc: 65.62%   [EVAL] batch:   10 | acc: 37.50%,  total acc: 63.07%   [EVAL] batch:   11 | acc: 43.75%,  total acc: 61.46%   [EVAL] batch:   12 | acc: 50.00%,  total acc: 60.58%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 62.95%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 65.00%   [EVAL] batch:   15 | acc: 93.75%,  total acc: 66.80%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 68.38%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 69.79%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 71.05%   [EVAL] batch:   19 | acc: 62.50%,  total acc: 70.62%   [EVAL] batch:   20 | acc: 75.00%,  total acc: 70.83%   [EVAL] batch:   21 | acc: 62.50%,  total acc: 70.45%   [EVAL] batch:   22 | acc: 75.00%,  total acc: 70.65%   [EVAL] batch:   23 | acc: 75.00%,  total acc: 70.83%   [EVAL] batch:   24 | acc: 62.50%,  total acc: 70.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 71.63%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 72.45%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 73.21%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 73.92%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 74.79%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 75.60%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 76.17%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 76.89%   [EVAL] batch:   33 | acc: 81.25%,  total acc: 77.02%   [EVAL] batch:   34 | acc: 81.25%,  total acc: 77.14%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 77.43%   [EVAL] batch:   36 | acc: 93.75%,  total acc: 77.87%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 78.29%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 78.85%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 79.38%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 79.88%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 80.21%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 80.52%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 80.68%   [EVAL] batch:   44 | acc: 75.00%,  total acc: 80.56%   [EVAL] batch:   45 | acc: 56.25%,  total acc: 80.03%   [EVAL] batch:   46 | acc: 43.75%,  total acc: 79.26%   [EVAL] batch:   47 | acc: 56.25%,  total acc: 78.78%   [EVAL] batch:   48 | acc: 62.50%,  total acc: 78.44%   [EVAL] batch:   49 | acc: 56.25%,  total acc: 78.00%   [EVAL] batch:   50 | acc: 68.75%,  total acc: 77.82%   [EVAL] batch:   51 | acc: 62.50%,  total acc: 77.52%   [EVAL] batch:   52 | acc: 62.50%,  total acc: 77.24%   [EVAL] batch:   53 | acc: 50.00%,  total acc: 76.74%   [EVAL] batch:   54 | acc: 37.50%,  total acc: 76.02%   [EVAL] batch:   55 | acc: 56.25%,  total acc: 75.67%   [EVAL] batch:   56 | acc: 62.50%,  total acc: 75.44%   [EVAL] batch:   57 | acc: 56.25%,  total acc: 75.11%   [EVAL] batch:   58 | acc: 68.75%,  total acc: 75.00%   [EVAL] batch:   59 | acc: 87.50%,  total acc: 75.21%   [EVAL] batch:   60 | acc: 87.50%,  total acc: 75.41%   [EVAL] batch:   61 | acc: 75.00%,  total acc: 75.40%   [EVAL] batch:   62 | acc: 37.50%,  total acc: 74.80%   [EVAL] batch:   63 | acc: 6.25%,  total acc: 73.73%   [EVAL] batch:   64 | acc: 6.25%,  total acc: 72.69%   [EVAL] batch:   65 | acc: 12.50%,  total acc: 71.78%   [EVAL] batch:   66 | acc: 6.25%,  total acc: 70.80%   [EVAL] batch:   67 | acc: 12.50%,  total acc: 69.94%   [EVAL] batch:   68 | acc: 31.25%,  total acc: 69.38%   [EVAL] batch:   69 | acc: 75.00%,  total acc: 69.46%   [EVAL] batch:   70 | acc: 87.50%,  total acc: 69.72%   [EVAL] batch:   71 | acc: 81.25%,  total acc: 69.88%   [EVAL] batch:   72 | acc: 93.75%,  total acc: 70.21%   [EVAL] batch:   73 | acc: 56.25%,  total acc: 70.02%   [EVAL] batch:   74 | acc: 75.00%,  total acc: 70.08%   [EVAL] batch:   75 | acc: 75.00%,  total acc: 70.15%   [EVAL] batch:   76 | acc: 56.25%,  total acc: 69.97%   [EVAL] batch:   77 | acc: 81.25%,  total acc: 70.11%   [EVAL] batch:   78 | acc: 81.25%,  total acc: 70.25%   [EVAL] batch:   79 | acc: 56.25%,  total acc: 70.08%   [EVAL] batch:   80 | acc: 68.75%,  total acc: 70.06%   [EVAL] batch:   81 | acc: 12.50%,  total acc: 69.36%   [EVAL] batch:   82 | acc: 18.75%,  total acc: 68.75%   [EVAL] batch:   83 | acc: 12.50%,  total acc: 68.08%   [EVAL] batch:   84 | acc: 43.75%,  total acc: 67.79%   [EVAL] batch:   85 | acc: 31.25%,  total acc: 67.37%   [EVAL] batch:   86 | acc: 56.25%,  total acc: 67.24%   [EVAL] batch:   87 | acc: 18.75%,  total acc: 66.69%   [EVAL] batch:   88 | acc: 0.00%,  total acc: 65.94%   [EVAL] batch:   89 | acc: 31.25%,  total acc: 65.56%   [EVAL] batch:   90 | acc: 0.00%,  total acc: 64.84%   [EVAL] batch:   91 | acc: 18.75%,  total acc: 64.33%   [EVAL] batch:   92 | acc: 6.25%,  total acc: 63.71%   [EVAL] batch:   93 | acc: 18.75%,  total acc: 63.23%   [EVAL] batch:   94 | acc: 81.25%,  total acc: 63.42%   [EVAL] batch:   95 | acc: 87.50%,  total acc: 63.67%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 64.05%   [EVAL] batch:   97 | acc: 93.75%,  total acc: 64.35%   [EVAL] batch:   98 | acc: 81.25%,  total acc: 64.52%   [EVAL] batch:   99 | acc: 62.50%,  total acc: 64.50%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 64.85%   [EVAL] batch:  101 | acc: 93.75%,  total acc: 65.13%   [EVAL] batch:  102 | acc: 87.50%,  total acc: 65.35%   [EVAL] batch:  103 | acc: 93.75%,  total acc: 65.62%   [EVAL] batch:  104 | acc: 93.75%,  total acc: 65.89%   [EVAL] batch:  105 | acc: 100.00%,  total acc: 66.21%   [EVAL] batch:  106 | acc: 75.00%,  total acc: 66.30%   [EVAL] batch:  107 | acc: 43.75%,  total acc: 66.09%   [EVAL] batch:  108 | acc: 37.50%,  total acc: 65.83%   [EVAL] batch:  109 | acc: 56.25%,  total acc: 65.74%   [EVAL] batch:  110 | acc: 56.25%,  total acc: 65.65%   [EVAL] batch:  111 | acc: 62.50%,  total acc: 65.62%   [EVAL] batch:  112 | acc: 31.25%,  total acc: 65.32%   [EVAL] batch:  113 | acc: 62.50%,  total acc: 65.30%   [EVAL] batch:  114 | acc: 56.25%,  total acc: 65.22%   [EVAL] batch:  115 | acc: 62.50%,  total acc: 65.19%   [EVAL] batch:  116 | acc: 62.50%,  total acc: 65.17%   [EVAL] batch:  117 | acc: 68.75%,  total acc: 65.20%   [EVAL] batch:  118 | acc: 68.75%,  total acc: 65.23%   [EVAL] batch:  119 | acc: 43.75%,  total acc: 65.05%   [EVAL] batch:  120 | acc: 43.75%,  total acc: 64.88%   [EVAL] batch:  121 | acc: 50.00%,  total acc: 64.75%   [EVAL] batch:  122 | acc: 50.00%,  total acc: 64.63%   [EVAL] batch:  123 | acc: 75.00%,  total acc: 64.72%   [EVAL] batch:  124 | acc: 56.25%,  total acc: 64.65%   [EVAL] batch:  125 | acc: 75.00%,  total acc: 64.73%   [EVAL] batch:  126 | acc: 43.75%,  total acc: 64.57%   [EVAL] batch:  127 | acc: 43.75%,  total acc: 64.40%   [EVAL] batch:  128 | acc: 25.00%,  total acc: 64.10%   [EVAL] batch:  129 | acc: 50.00%,  total acc: 63.99%   [EVAL] batch:  130 | acc: 43.75%,  total acc: 63.84%   [EVAL] batch:  131 | acc: 87.50%,  total acc: 64.02%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 64.29%   [EVAL] batch:  133 | acc: 93.75%,  total acc: 64.51%   [EVAL] batch:  134 | acc: 81.25%,  total acc: 64.63%   [EVAL] batch:  135 | acc: 93.75%,  total acc: 64.84%   [EVAL] batch:  136 | acc: 93.75%,  total acc: 65.05%   [EVAL] batch:  137 | acc: 68.75%,  total acc: 65.08%   [EVAL] batch:  138 | acc: 31.25%,  total acc: 64.84%   [EVAL] batch:  139 | acc: 37.50%,  total acc: 64.64%   [EVAL] batch:  140 | acc: 56.25%,  total acc: 64.58%   [EVAL] batch:  141 | acc: 62.50%,  total acc: 64.57%   [EVAL] batch:  142 | acc: 50.00%,  total acc: 64.47%   [EVAL] batch:  143 | acc: 62.50%,  total acc: 64.45%   [EVAL] batch:  144 | acc: 25.00%,  total acc: 64.18%   [EVAL] batch:  145 | acc: 43.75%,  total acc: 64.04%   [EVAL] batch:  146 | acc: 18.75%,  total acc: 63.73%   [EVAL] batch:  147 | acc: 25.00%,  total acc: 63.47%   [EVAL] batch:  148 | acc: 43.75%,  total acc: 63.34%   [EVAL] batch:  149 | acc: 37.50%,  total acc: 63.17%   [EVAL] batch:  150 | acc: 75.00%,  total acc: 63.25%   [EVAL] batch:  151 | acc: 87.50%,  total acc: 63.40%   [EVAL] batch:  152 | acc: 68.75%,  total acc: 63.44%   [EVAL] batch:  153 | acc: 81.25%,  total acc: 63.56%   [EVAL] batch:  154 | acc: 56.25%,  total acc: 63.51%   [EVAL] batch:  155 | acc: 75.00%,  total acc: 63.58%   [EVAL] batch:  156 | acc: 68.75%,  total acc: 63.61%   [EVAL] batch:  157 | acc: 62.50%,  total acc: 63.61%   [EVAL] batch:  158 | acc: 56.25%,  total acc: 63.56%   [EVAL] batch:  159 | acc: 62.50%,  total acc: 63.55%   [EVAL] batch:  160 | acc: 75.00%,  total acc: 63.63%   [EVAL] batch:  161 | acc: 50.00%,  total acc: 63.54%   [EVAL] batch:  162 | acc: 50.00%,  total acc: 63.46%   [EVAL] batch:  163 | acc: 62.50%,  total acc: 63.45%   [EVAL] batch:  164 | acc: 68.75%,  total acc: 63.48%   [EVAL] batch:  165 | acc: 56.25%,  total acc: 63.44%   [EVAL] batch:  166 | acc: 68.75%,  total acc: 63.47%   [EVAL] batch:  167 | acc: 75.00%,  total acc: 63.54%   [EVAL] batch:  168 | acc: 62.50%,  total acc: 63.54%   [EVAL] batch:  169 | acc: 31.25%,  total acc: 63.35%   [EVAL] batch:  170 | acc: 25.00%,  total acc: 63.12%   [EVAL] batch:  171 | acc: 18.75%,  total acc: 62.86%   [EVAL] batch:  172 | acc: 37.50%,  total acc: 62.72%   [EVAL] batch:  173 | acc: 25.00%,  total acc: 62.50%   [EVAL] batch:  174 | acc: 31.25%,  total acc: 62.32%   [EVAL] batch:  175 | acc: 100.00%,  total acc: 62.54%   [EVAL] batch:  176 | acc: 100.00%,  total acc: 62.75%   [EVAL] batch:  177 | acc: 100.00%,  total acc: 62.96%   [EVAL] batch:  178 | acc: 100.00%,  total acc: 63.16%   [EVAL] batch:  179 | acc: 100.00%,  total acc: 63.37%   [EVAL] batch:  180 | acc: 100.00%,  total acc: 63.57%   [EVAL] batch:  181 | acc: 75.00%,  total acc: 63.63%   [EVAL] batch:  182 | acc: 62.50%,  total acc: 63.63%   [EVAL] batch:  183 | acc: 68.75%,  total acc: 63.65%   [EVAL] batch:  184 | acc: 68.75%,  total acc: 63.68%   [EVAL] batch:  185 | acc: 68.75%,  total acc: 63.71%   [EVAL] batch:  186 | acc: 75.00%,  total acc: 63.77%   [EVAL] batch:  187 | acc: 68.75%,  total acc: 63.80%   [EVAL] batch:  188 | acc: 62.50%,  total acc: 63.79%   [EVAL] batch:  189 | acc: 75.00%,  total acc: 63.85%   [EVAL] batch:  190 | acc: 68.75%,  total acc: 63.87%   [EVAL] batch:  191 | acc: 62.50%,  total acc: 63.87%   [EVAL] batch:  192 | acc: 81.25%,  total acc: 63.96%   [EVAL] batch:  193 | acc: 81.25%,  total acc: 64.05%   [EVAL] batch:  194 | acc: 75.00%,  total acc: 64.10%   [EVAL] batch:  195 | acc: 75.00%,  total acc: 64.16%   [EVAL] batch:  196 | acc: 68.75%,  total acc: 64.18%   [EVAL] batch:  197 | acc: 87.50%,  total acc: 64.30%   [EVAL] batch:  198 | acc: 75.00%,  total acc: 64.35%   [EVAL] batch:  199 | acc: 87.50%,  total acc: 64.47%   [EVAL] batch:  200 | acc: 75.00%,  total acc: 64.52%   [EVAL] batch:  201 | acc: 81.25%,  total acc: 64.60%   [EVAL] batch:  202 | acc: 68.75%,  total acc: 64.62%   [EVAL] batch:  203 | acc: 62.50%,  total acc: 64.61%   [EVAL] batch:  204 | acc: 68.75%,  total acc: 64.63%   [EVAL] batch:  205 | acc: 75.00%,  total acc: 64.68%   [EVAL] batch:  206 | acc: 31.25%,  total acc: 64.52%   [EVAL] batch:  207 | acc: 12.50%,  total acc: 64.27%   [EVAL] batch:  208 | acc: 6.25%,  total acc: 64.00%   [EVAL] batch:  209 | acc: 12.50%,  total acc: 63.75%   [EVAL] batch:  210 | acc: 25.00%,  total acc: 63.57%   [EVAL] batch:  211 | acc: 25.00%,  total acc: 63.38%   [EVAL] batch:  212 | acc: 43.75%,  total acc: 63.29%   [EVAL] batch:  213 | acc: 93.75%,  total acc: 63.43%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 63.60%   [EVAL] batch:  215 | acc: 75.00%,  total acc: 63.66%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 63.82%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 63.99%   [EVAL] batch:  218 | acc: 93.75%,  total acc: 64.13%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 64.29%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 64.45%   [EVAL] batch:  221 | acc: 100.00%,  total acc: 64.61%   [EVAL] batch:  222 | acc: 100.00%,  total acc: 64.77%   [EVAL] batch:  223 | acc: 100.00%,  total acc: 64.93%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 65.08%   [EVAL] batch:  225 | acc: 93.75%,  total acc: 65.21%   [EVAL] batch:  226 | acc: 93.75%,  total acc: 65.34%   [EVAL] batch:  227 | acc: 93.75%,  total acc: 65.46%   [EVAL] batch:  228 | acc: 87.50%,  total acc: 65.56%   [EVAL] batch:  229 | acc: 100.00%,  total acc: 65.71%   [EVAL] batch:  230 | acc: 93.75%,  total acc: 65.83%   [EVAL] batch:  231 | acc: 50.00%,  total acc: 65.76%   [EVAL] batch:  232 | acc: 25.00%,  total acc: 65.58%   [EVAL] batch:  233 | acc: 56.25%,  total acc: 65.54%   [EVAL] batch:  234 | acc: 37.50%,  total acc: 65.43%   [EVAL] batch:  235 | acc: 56.25%,  total acc: 65.39%   [EVAL] batch:  236 | acc: 31.25%,  total acc: 65.24%   [EVAL] batch:  237 | acc: 68.75%,  total acc: 65.26%   [EVAL] batch:  238 | acc: 87.50%,  total acc: 65.35%   [EVAL] batch:  239 | acc: 93.75%,  total acc: 65.47%   [EVAL] batch:  240 | acc: 81.25%,  total acc: 65.53%   [EVAL] batch:  241 | acc: 87.50%,  total acc: 65.62%   [EVAL] batch:  242 | acc: 93.75%,  total acc: 65.74%   [EVAL] batch:  243 | acc: 68.75%,  total acc: 65.75%   [EVAL] batch:  244 | acc: 0.00%,  total acc: 65.48%   [EVAL] batch:  245 | acc: 0.00%,  total acc: 65.22%   [EVAL] batch:  246 | acc: 0.00%,  total acc: 64.95%   [EVAL] batch:  247 | acc: 0.00%,  total acc: 64.69%   [EVAL] batch:  248 | acc: 0.00%,  total acc: 64.43%   [EVAL] batch:  249 | acc: 0.00%,  total acc: 64.18%   [EVAL] batch:  250 | acc: 12.50%,  total acc: 63.97%   [EVAL] batch:  251 | acc: 12.50%,  total acc: 63.76%   [EVAL] batch:  252 | acc: 25.00%,  total acc: 63.61%   [EVAL] batch:  253 | acc: 18.75%,  total acc: 63.44%   [EVAL] batch:  254 | acc: 25.00%,  total acc: 63.28%   [EVAL] batch:  255 | acc: 31.25%,  total acc: 63.16%   [EVAL] batch:  256 | acc: 43.75%,  total acc: 63.08%   [EVAL] batch:  257 | acc: 75.00%,  total acc: 63.13%   [EVAL] batch:  258 | acc: 68.75%,  total acc: 63.15%   [EVAL] batch:  259 | acc: 68.75%,  total acc: 63.17%   [EVAL] batch:  260 | acc: 50.00%,  total acc: 63.12%   [EVAL] batch:  261 | acc: 62.50%,  total acc: 63.12%   [EVAL] batch:  262 | acc: 87.50%,  total acc: 63.21%   [EVAL] batch:  263 | acc: 75.00%,  total acc: 63.26%   [EVAL] batch:  264 | acc: 75.00%,  total acc: 63.30%   [EVAL] batch:  265 | acc: 75.00%,  total acc: 63.35%   [EVAL] batch:  266 | acc: 75.00%,  total acc: 63.39%   [EVAL] batch:  267 | acc: 100.00%,  total acc: 63.53%   [EVAL] batch:  268 | acc: 100.00%,  total acc: 63.66%   [EVAL] batch:  269 | acc: 87.50%,  total acc: 63.75%   [EVAL] batch:  270 | acc: 100.00%,  total acc: 63.88%   [EVAL] batch:  271 | acc: 100.00%,  total acc: 64.02%   [EVAL] batch:  272 | acc: 100.00%,  total acc: 64.15%   [EVAL] batch:  273 | acc: 93.75%,  total acc: 64.26%   [EVAL] batch:  274 | acc: 100.00%,  total acc: 64.39%   [EVAL] batch:  275 | acc: 81.25%,  total acc: 64.45%   [EVAL] batch:  276 | acc: 87.50%,  total acc: 64.53%   [EVAL] batch:  277 | acc: 87.50%,  total acc: 64.61%   [EVAL] batch:  278 | acc: 62.50%,  total acc: 64.61%   [EVAL] batch:  279 | acc: 93.75%,  total acc: 64.71%   [EVAL] batch:  280 | acc: 81.25%,  total acc: 64.77%   [EVAL] batch:  281 | acc: 37.50%,  total acc: 64.67%   [EVAL] batch:  282 | acc: 50.00%,  total acc: 64.62%   [EVAL] batch:  283 | acc: 18.75%,  total acc: 64.46%   [EVAL] batch:  284 | acc: 25.00%,  total acc: 64.32%   [EVAL] batch:  285 | acc: 25.00%,  total acc: 64.18%   [EVAL] batch:  286 | acc: 25.00%,  total acc: 64.05%   [EVAL] batch:  287 | acc: 43.75%,  total acc: 63.98%   [EVAL] batch:  288 | acc: 43.75%,  total acc: 63.91%   [EVAL] batch:  289 | acc: 75.00%,  total acc: 63.94%   [EVAL] batch:  290 | acc: 31.25%,  total acc: 63.83%   [EVAL] batch:  291 | acc: 62.50%,  total acc: 63.83%   [EVAL] batch:  292 | acc: 50.00%,  total acc: 63.78%   [EVAL] batch:  293 | acc: 56.25%,  total acc: 63.75%   [EVAL] batch:  294 | acc: 75.00%,  total acc: 63.79%   [EVAL] batch:  295 | acc: 68.75%,  total acc: 63.81%   [EVAL] batch:  296 | acc: 62.50%,  total acc: 63.80%   [EVAL] batch:  297 | acc: 56.25%,  total acc: 63.78%   [EVAL] batch:  298 | acc: 68.75%,  total acc: 63.80%   [EVAL] batch:  299 | acc: 81.25%,  total acc: 63.85%   [EVAL] batch:  300 | acc: 100.00%,  total acc: 63.97%   [EVAL] batch:  301 | acc: 100.00%,  total acc: 64.09%   [EVAL] batch:  302 | acc: 100.00%,  total acc: 64.21%   [EVAL] batch:  303 | acc: 100.00%,  total acc: 64.33%   [EVAL] batch:  304 | acc: 100.00%,  total acc: 64.45%   [EVAL] batch:  305 | acc: 100.00%,  total acc: 64.56%   [EVAL] batch:  306 | acc: 87.50%,  total acc: 64.64%   [EVAL] batch:  307 | acc: 87.50%,  total acc: 64.71%   [EVAL] batch:  308 | acc: 81.25%,  total acc: 64.77%   [EVAL] batch:  309 | acc: 93.75%,  total acc: 64.86%   [EVAL] batch:  310 | acc: 81.25%,  total acc: 64.91%   [EVAL] batch:  311 | acc: 100.00%,  total acc: 65.02%   [EVAL] batch:  312 | acc: 81.25%,  total acc: 65.08%   [EVAL] batch:  313 | acc: 50.00%,  total acc: 65.03%   [EVAL] batch:  314 | acc: 50.00%,  total acc: 64.98%   [EVAL] batch:  315 | acc: 68.75%,  total acc: 64.99%   [EVAL] batch:  316 | acc: 56.25%,  total acc: 64.96%   [EVAL] batch:  317 | acc: 62.50%,  total acc: 64.96%   [EVAL] batch:  318 | acc: 75.00%,  total acc: 64.99%   [EVAL] batch:  319 | acc: 87.50%,  total acc: 65.06%   [EVAL] batch:  320 | acc: 75.00%,  total acc: 65.09%   [EVAL] batch:  321 | acc: 68.75%,  total acc: 65.10%   [EVAL] batch:  322 | acc: 81.25%,  total acc: 65.15%   [EVAL] batch:  323 | acc: 68.75%,  total acc: 65.16%   [EVAL] batch:  324 | acc: 75.00%,  total acc: 65.19%   [EVAL] batch:  325 | acc: 12.50%,  total acc: 65.03%   [EVAL] batch:  326 | acc: 12.50%,  total acc: 64.87%   [EVAL] batch:  327 | acc: 18.75%,  total acc: 64.73%   [EVAL] batch:  328 | acc: 31.25%,  total acc: 64.63%   [EVAL] batch:  329 | acc: 37.50%,  total acc: 64.55%   [EVAL] batch:  330 | acc: 0.00%,  total acc: 64.35%   [EVAL] batch:  331 | acc: 75.00%,  total acc: 64.38%   [EVAL] batch:  332 | acc: 100.00%,  total acc: 64.49%   [EVAL] batch:  333 | acc: 100.00%,  total acc: 64.60%   [EVAL] batch:  334 | acc: 100.00%,  total acc: 64.70%   [EVAL] batch:  335 | acc: 100.00%,  total acc: 64.81%   [EVAL] batch:  336 | acc: 93.75%,  total acc: 64.89%   [EVAL] batch:  337 | acc: 62.50%,  total acc: 64.89%   [EVAL] batch:  338 | acc: 0.00%,  total acc: 64.69%   [EVAL] batch:  339 | acc: 6.25%,  total acc: 64.52%   [EVAL] batch:  340 | acc: 12.50%,  total acc: 64.37%   [EVAL] batch:  341 | acc: 37.50%,  total acc: 64.29%   [EVAL] batch:  342 | acc: 12.50%,  total acc: 64.14%   [EVAL] batch:  343 | acc: 25.00%,  total acc: 64.03%   [EVAL] batch:  344 | acc: 25.00%,  total acc: 63.91%   [EVAL] batch:  345 | acc: 43.75%,  total acc: 63.85%   [EVAL] batch:  346 | acc: 43.75%,  total acc: 63.80%   [EVAL] batch:  347 | acc: 50.00%,  total acc: 63.76%   [EVAL] batch:  348 | acc: 43.75%,  total acc: 63.70%   [EVAL] batch:  349 | acc: 56.25%,  total acc: 63.68%   [EVAL] batch:  350 | acc: 37.50%,  total acc: 63.60%   [EVAL] batch:  351 | acc: 50.00%,  total acc: 63.57%   [EVAL] batch:  352 | acc: 43.75%,  total acc: 63.51%   [EVAL] batch:  353 | acc: 37.50%,  total acc: 63.44%   [EVAL] batch:  354 | acc: 75.00%,  total acc: 63.47%   [EVAL] batch:  355 | acc: 50.00%,  total acc: 63.43%   [EVAL] batch:  356 | acc: 56.25%,  total acc: 63.41%   [EVAL] batch:  357 | acc: 50.00%,  total acc: 63.37%   [EVAL] batch:  358 | acc: 43.75%,  total acc: 63.32%   [EVAL] batch:  359 | acc: 75.00%,  total acc: 63.35%   [EVAL] batch:  360 | acc: 43.75%,  total acc: 63.30%   [EVAL] batch:  361 | acc: 56.25%,  total acc: 63.28%   [EVAL] batch:  362 | acc: 62.50%,  total acc: 63.27%   [EVAL] batch:  363 | acc: 56.25%,  total acc: 63.26%   [EVAL] batch:  364 | acc: 68.75%,  total acc: 63.27%   [EVAL] batch:  365 | acc: 50.00%,  total acc: 63.23%   [EVAL] batch:  366 | acc: 75.00%,  total acc: 63.27%   [EVAL] batch:  367 | acc: 75.00%,  total acc: 63.30%   [EVAL] batch:  368 | acc: 62.50%,  total acc: 63.30%   [EVAL] batch:  369 | acc: 93.75%,  total acc: 63.38%   [EVAL] batch:  370 | acc: 93.75%,  total acc: 63.46%   [EVAL] batch:  371 | acc: 81.25%,  total acc: 63.51%   [EVAL] batch:  372 | acc: 87.50%,  total acc: 63.57%   [EVAL] batch:  373 | acc: 81.25%,  total acc: 63.62%   [EVAL] batch:  374 | acc: 93.75%,  total acc: 63.70%   [EVAL] batch:  375 | acc: 100.00%,  total acc: 63.80%   [EVAL] batch:  376 | acc: 87.50%,  total acc: 63.86%   [EVAL] batch:  377 | acc: 100.00%,  total acc: 63.96%   [EVAL] batch:  378 | acc: 87.50%,  total acc: 64.02%   [EVAL] batch:  379 | acc: 93.75%,  total acc: 64.10%   [EVAL] batch:  380 | acc: 100.00%,  total acc: 64.19%   [EVAL] batch:  381 | acc: 68.75%,  total acc: 64.20%   [EVAL] batch:  382 | acc: 68.75%,  total acc: 64.21%   [EVAL] batch:  383 | acc: 43.75%,  total acc: 64.16%   [EVAL] batch:  384 | acc: 43.75%,  total acc: 64.11%   [EVAL] batch:  385 | acc: 62.50%,  total acc: 64.10%   [EVAL] batch:  386 | acc: 37.50%,  total acc: 64.03%   [EVAL] batch:  387 | acc: 37.50%,  total acc: 63.97%   [EVAL] batch:  388 | acc: 62.50%,  total acc: 63.96%   [EVAL] batch:  389 | acc: 62.50%,  total acc: 63.96%   [EVAL] batch:  390 | acc: 50.00%,  total acc: 63.92%   [EVAL] batch:  391 | acc: 37.50%,  total acc: 63.86%   [EVAL] batch:  392 | acc: 50.00%,  total acc: 63.82%   [EVAL] batch:  393 | acc: 50.00%,  total acc: 63.78%   [EVAL] batch:  394 | acc: 43.75%,  total acc: 63.73%   [EVAL] batch:  395 | acc: 56.25%,  total acc: 63.72%   [EVAL] batch:  396 | acc: 68.75%,  total acc: 63.73%   [EVAL] batch:  397 | acc: 68.75%,  total acc: 63.74%   [EVAL] batch:  398 | acc: 81.25%,  total acc: 63.78%   [EVAL] batch:  399 | acc: 56.25%,  total acc: 63.77%   [EVAL] batch:  400 | acc: 100.00%,  total acc: 63.86%   [EVAL] batch:  401 | acc: 100.00%,  total acc: 63.95%   [EVAL] batch:  402 | acc: 100.00%,  total acc: 64.04%   [EVAL] batch:  403 | acc: 100.00%,  total acc: 64.12%   [EVAL] batch:  404 | acc: 100.00%,  total acc: 64.21%   [EVAL] batch:  405 | acc: 100.00%,  total acc: 64.30%   [EVAL] batch:  406 | acc: 68.75%,  total acc: 64.31%   [EVAL] batch:  407 | acc: 75.00%,  total acc: 64.34%   [EVAL] batch:  408 | acc: 75.00%,  total acc: 64.36%   [EVAL] batch:  409 | acc: 75.00%,  total acc: 64.39%   [EVAL] batch:  410 | acc: 68.75%,  total acc: 64.40%   [EVAL] batch:  411 | acc: 62.50%,  total acc: 64.40%   [EVAL] batch:  412 | acc: 68.75%,  total acc: 64.41%   [EVAL] batch:  413 | acc: 100.00%,  total acc: 64.49%   [EVAL] batch:  414 | acc: 100.00%,  total acc: 64.58%   [EVAL] batch:  415 | acc: 100.00%,  total acc: 64.66%   [EVAL] batch:  416 | acc: 100.00%,  total acc: 64.75%   [EVAL] batch:  417 | acc: 100.00%,  total acc: 64.83%   [EVAL] batch:  418 | acc: 93.75%,  total acc: 64.90%   [EVAL] batch:  419 | acc: 100.00%,  total acc: 64.99%   [EVAL] batch:  420 | acc: 100.00%,  total acc: 65.07%   [EVAL] batch:  421 | acc: 100.00%,  total acc: 65.15%   [EVAL] batch:  422 | acc: 100.00%,  total acc: 65.23%   [EVAL] batch:  423 | acc: 100.00%,  total acc: 65.32%   [EVAL] batch:  424 | acc: 93.75%,  total acc: 65.38%   [EVAL] batch:  425 | acc: 93.75%,  total acc: 65.45%   [EVAL] batch:  426 | acc: 87.50%,  total acc: 65.50%   [EVAL] batch:  427 | acc: 93.75%,  total acc: 65.57%   [EVAL] batch:  428 | acc: 93.75%,  total acc: 65.63%   [EVAL] batch:  429 | acc: 81.25%,  total acc: 65.67%   [EVAL] batch:  430 | acc: 93.75%,  total acc: 65.73%   [EVAL] batch:  431 | acc: 93.75%,  total acc: 65.80%   [EVAL] batch:  432 | acc: 93.75%,  total acc: 65.86%   [EVAL] batch:  433 | acc: 93.75%,  total acc: 65.93%   [EVAL] batch:  434 | acc: 93.75%,  total acc: 65.99%   [EVAL] batch:  435 | acc: 100.00%,  total acc: 66.07%   [EVAL] batch:  436 | acc: 81.25%,  total acc: 66.10%   [EVAL] batch:  437 | acc: 62.50%,  total acc: 66.10%   [EVAL] batch:  438 | acc: 6.25%,  total acc: 65.96%   [EVAL] batch:  439 | acc: 18.75%,  total acc: 65.85%   [EVAL] batch:  440 | acc: 0.00%,  total acc: 65.70%   [EVAL] batch:  441 | acc: 6.25%,  total acc: 65.57%   [EVAL] batch:  442 | acc: 25.00%,  total acc: 65.48%   [EVAL] batch:  443 | acc: 18.75%,  total acc: 65.37%   [EVAL] batch:  444 | acc: 75.00%,  total acc: 65.39%   [EVAL] batch:  445 | acc: 81.25%,  total acc: 65.43%   [EVAL] batch:  446 | acc: 87.50%,  total acc: 65.48%   [EVAL] batch:  447 | acc: 81.25%,  total acc: 65.51%   [EVAL] batch:  448 | acc: 81.25%,  total acc: 65.55%   [EVAL] batch:  449 | acc: 75.00%,  total acc: 65.57%   [EVAL] batch:  450 | acc: 68.75%,  total acc: 65.58%   [EVAL] batch:  451 | acc: 37.50%,  total acc: 65.51%   [EVAL] batch:  452 | acc: 50.00%,  total acc: 65.48%   [EVAL] batch:  453 | acc: 68.75%,  total acc: 65.49%   [EVAL] batch:  454 | acc: 56.25%,  total acc: 65.47%   [EVAL] batch:  455 | acc: 43.75%,  total acc: 65.42%   [EVAL] batch:  456 | acc: 93.75%,  total acc: 65.48%   [EVAL] batch:  457 | acc: 100.00%,  total acc: 65.56%   [EVAL] batch:  458 | acc: 100.00%,  total acc: 65.63%   [EVAL] batch:  459 | acc: 87.50%,  total acc: 65.68%   [EVAL] batch:  460 | acc: 93.75%,  total acc: 65.74%   [EVAL] batch:  461 | acc: 100.00%,  total acc: 65.81%   [EVAL] batch:  462 | acc: 75.00%,  total acc: 65.83%   [EVAL] batch:  463 | acc: 87.50%,  total acc: 65.88%   [EVAL] batch:  464 | acc: 87.50%,  total acc: 65.93%   [EVAL] batch:  465 | acc: 100.00%,  total acc: 66.00%   [EVAL] batch:  466 | acc: 75.00%,  total acc: 66.02%   [EVAL] batch:  467 | acc: 93.75%,  total acc: 66.08%   [EVAL] batch:  468 | acc: 87.50%,  total acc: 66.12%   [EVAL] batch:  469 | acc: 100.00%,  total acc: 66.20%   [EVAL] batch:  470 | acc: 100.00%,  total acc: 66.27%   [EVAL] batch:  471 | acc: 87.50%,  total acc: 66.31%   [EVAL] batch:  472 | acc: 93.75%,  total acc: 66.37%   [EVAL] batch:  473 | acc: 93.75%,  total acc: 66.43%   [EVAL] batch:  474 | acc: 100.00%,  total acc: 66.50%   [EVAL] batch:  475 | acc: 31.25%,  total acc: 66.43%   [EVAL] batch:  476 | acc: 50.00%,  total acc: 66.39%   [EVAL] batch:  477 | acc: 56.25%,  total acc: 66.37%   [EVAL] batch:  478 | acc: 37.50%,  total acc: 66.31%   [EVAL] batch:  479 | acc: 31.25%,  total acc: 66.24%   [EVAL] batch:  480 | acc: 68.75%,  total acc: 66.24%   [EVAL] batch:  481 | acc: 87.50%,  total acc: 66.29%   [EVAL] batch:  482 | acc: 87.50%,  total acc: 66.33%   [EVAL] batch:  483 | acc: 81.25%,  total acc: 66.36%   [EVAL] batch:  484 | acc: 100.00%,  total acc: 66.43%   [EVAL] batch:  485 | acc: 81.25%,  total acc: 66.46%   [EVAL] batch:  486 | acc: 62.50%,  total acc: 66.45%   [EVAL] batch:  487 | acc: 68.75%,  total acc: 66.46%   [EVAL] batch:  488 | acc: 50.00%,  total acc: 66.42%   [EVAL] batch:  489 | acc: 43.75%,  total acc: 66.38%   [EVAL] batch:  490 | acc: 43.75%,  total acc: 66.33%   [EVAL] batch:  491 | acc: 56.25%,  total acc: 66.31%   [EVAL] batch:  492 | acc: 50.00%,  total acc: 66.28%   [EVAL] batch:  493 | acc: 37.50%,  total acc: 66.22%   [EVAL] batch:  494 | acc: 81.25%,  total acc: 66.25%   [EVAL] batch:  495 | acc: 62.50%,  total acc: 66.24%   [EVAL] batch:  496 | acc: 43.75%,  total acc: 66.20%   [EVAL] batch:  497 | acc: 75.00%,  total acc: 66.21%   [EVAL] batch:  498 | acc: 81.25%,  total acc: 66.24%   [EVAL] batch:  499 | acc: 56.25%,  total acc: 66.22%   
cur_acc:  ['0.9494', '0.7431', '0.7530', '0.7708', '0.7569', '0.6280', '0.8274', '0.6627']
his_acc:  ['0.9494', '0.8230', '0.7680', '0.7475', '0.7167', '0.6898', '0.6892', '0.6623']
--------Round  2
seed:  300
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/train.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/valid.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/test.pkl
Task_order: [7 2 3 1 5 6 0 4]
prepared data!
CurrentTrain: epoch 15, batch     0 | loss: 29.6883950CurrentTrain: epoch 15, batch     1 | loss: 25.0563687CurrentTrain: epoch 15, batch     2 | loss: 27.8029714CurrentTrain: epoch 15, batch     3 | loss: 35.6608734CurrentTrain: epoch 15, batch     4 | loss: 32.7397617CurrentTrain: epoch 15, batch     5 | loss: 24.4380947CurrentTrain: epoch 15, batch     6 | loss: 31.3484427CurrentTrain: epoch 15, batch     7 | loss: 21.9674326CurrentTrain: epoch 15, batch     8 | loss: 30.2806214CurrentTrain: epoch 15, batch     9 | loss: 22.5470213CurrentTrain: epoch 15, batch    10 | loss: 31.0369236CurrentTrain: epoch 15, batch    11 | loss: 35.9845746CurrentTrain: epoch 15, batch    12 | loss: 30.3777892CurrentTrain: epoch 15, batch    13 | loss: 22.0783327CurrentTrain: epoch 15, batch    14 | loss: 20.1725850CurrentTrain: epoch 15, batch    15 | loss: 18.8358619CurrentTrain: epoch 15, batch    16 | loss: 24.6288372CurrentTrain: epoch 15, batch    17 | loss: 34.8673855CurrentTrain: epoch 15, batch    18 | loss: 19.5350259CurrentTrain: epoch 15, batch    19 | loss: 20.4614521CurrentTrain: epoch 15, batch    20 | loss: 15.1198946CurrentTrain: epoch 15, batch    21 | loss: 19.6202021CurrentTrain: epoch 15, batch    22 | loss: 23.9395320CurrentTrain: epoch 15, batch    23 | loss: 19.3800400CurrentTrain: epoch 15, batch    24 | loss: 23.2158988CurrentTrain: epoch 15, batch    25 | loss: 17.2806538CurrentTrain: epoch 15, batch    26 | loss: 47.3323424CurrentTrain: epoch 15, batch    27 | loss: 14.3915621CurrentTrain: epoch 15, batch    28 | loss: 28.4755991CurrentTrain: epoch 15, batch    29 | loss: 26.4096871CurrentTrain: epoch 15, batch    30 | loss: 18.7749341CurrentTrain: epoch 15, batch    31 | loss: 16.7854281CurrentTrain: epoch 15, batch    32 | loss: 25.3994259CurrentTrain: epoch 15, batch    33 | loss: 19.7242680CurrentTrain: epoch 15, batch    34 | loss: 18.7981949CurrentTrain: epoch 15, batch    35 | loss: 24.0285185CurrentTrain: epoch 15, batch    36 | loss: 24.4983189CurrentTrain: epoch 15, batch    37 | loss: 22.9889430CurrentTrain: epoch 15, batch    38 | loss: 30.2400739CurrentTrain: epoch 15, batch    39 | loss: 14.8163398CurrentTrain: epoch 15, batch    40 | loss: 18.2812014CurrentTrain: epoch 15, batch    41 | loss: 18.6239068CurrentTrain: epoch 15, batch    42 | loss: 16.7564460CurrentTrain: epoch 15, batch    43 | loss: 15.4098043CurrentTrain: epoch 15, batch    44 | loss: 18.5506697CurrentTrain: epoch 15, batch    45 | loss: 16.6019507CurrentTrain: epoch 15, batch    46 | loss: 33.4147261CurrentTrain: epoch 15, batch    47 | loss: 17.5794800CurrentTrain: epoch 15, batch    48 | loss: 33.6297069CurrentTrain: epoch 15, batch    49 | loss: 17.8706159CurrentTrain: epoch 15, batch    50 | loss: 20.1229253CurrentTrain: epoch 15, batch    51 | loss: 19.6097004CurrentTrain: epoch 15, batch    52 | loss: 19.1140754CurrentTrain: epoch 15, batch    53 | loss: 14.0705407CurrentTrain: epoch 15, batch    54 | loss: 14.9354617CurrentTrain: epoch 15, batch    55 | loss: 22.1799743CurrentTrain: epoch 15, batch    56 | loss: 19.2053178CurrentTrain: epoch 15, batch    57 | loss: 11.2726934CurrentTrain: epoch 15, batch    58 | loss: 15.2996848CurrentTrain: epoch 15, batch    59 | loss: 15.3866423CurrentTrain: epoch 15, batch    60 | loss: 24.6342016CurrentTrain: epoch 15, batch    61 | loss: 15.7026492CurrentTrain: epoch  7, batch    62 | loss: 18.7993090CurrentTrain: epoch 15, batch     0 | loss: 32.3477581CurrentTrain: epoch 15, batch     1 | loss: 21.8189998CurrentTrain: epoch 15, batch     2 | loss: 15.6749476CurrentTrain: epoch 15, batch     3 | loss: 14.5742972CurrentTrain: epoch 15, batch     4 | loss: 11.3373817CurrentTrain: epoch 15, batch     5 | loss: 20.6690531CurrentTrain: epoch 15, batch     6 | loss: 17.6343200CurrentTrain: epoch 15, batch     7 | loss: 14.8486778CurrentTrain: epoch 15, batch     8 | loss: 15.1971170CurrentTrain: epoch 15, batch     9 | loss: 19.1636910CurrentTrain: epoch 15, batch    10 | loss: 14.7686288CurrentTrain: epoch 15, batch    11 | loss: 21.9411897CurrentTrain: epoch 15, batch    12 | loss: 24.5938434CurrentTrain: epoch 15, batch    13 | loss: 37.4060986CurrentTrain: epoch 15, batch    14 | loss: 20.9680478CurrentTrain: epoch 15, batch    15 | loss: 18.4187903CurrentTrain: epoch 15, batch    16 | loss: 22.4287116CurrentTrain: epoch 15, batch    17 | loss: 25.7043756CurrentTrain: epoch 15, batch    18 | loss: 27.3454381CurrentTrain: epoch 15, batch    19 | loss: 18.9046106CurrentTrain: epoch 15, batch    20 | loss: 13.0529575CurrentTrain: epoch 15, batch    21 | loss: 15.2475085CurrentTrain: epoch 15, batch    22 | loss: 13.4683549CurrentTrain: epoch 15, batch    23 | loss: 21.7284647CurrentTrain: epoch 15, batch    24 | loss: 15.4857467CurrentTrain: epoch 15, batch    25 | loss: 13.6253410CurrentTrain: epoch 15, batch    26 | loss: 16.4053775CurrentTrain: epoch 15, batch    27 | loss: 15.9668306CurrentTrain: epoch 15, batch    28 | loss: 17.1329995CurrentTrain: epoch 15, batch    29 | loss: 15.3431869CurrentTrain: epoch 15, batch    30 | loss: 20.7043471CurrentTrain: epoch 15, batch    31 | loss: 13.9949945CurrentTrain: epoch 15, batch    32 | loss: 33.5126269CurrentTrain: epoch 15, batch    33 | loss: 13.2334601CurrentTrain: epoch 15, batch    34 | loss: 12.9667024CurrentTrain: epoch 15, batch    35 | loss: 17.3051538CurrentTrain: epoch 15, batch    36 | loss: 13.5099605CurrentTrain: epoch 15, batch    37 | loss: 15.6219725CurrentTrain: epoch 15, batch    38 | loss: 17.3416265CurrentTrain: epoch 15, batch    39 | loss: 18.2416626CurrentTrain: epoch 15, batch    40 | loss: 15.3968715CurrentTrain: epoch 15, batch    41 | loss: 12.8102690CurrentTrain: epoch 15, batch    42 | loss: 17.5794792CurrentTrain: epoch 15, batch    43 | loss: 12.9909767CurrentTrain: epoch 15, batch    44 | loss: 16.4857685CurrentTrain: epoch 15, batch    45 | loss: 16.3621143CurrentTrain: epoch 15, batch    46 | loss: 15.6106178CurrentTrain: epoch 15, batch    47 | loss: 17.4774120CurrentTrain: epoch 15, batch    48 | loss: 12.5854113CurrentTrain: epoch 15, batch    49 | loss: 13.1907330CurrentTrain: epoch 15, batch    50 | loss: 29.4684701CurrentTrain: epoch 15, batch    51 | loss: 21.4414831CurrentTrain: epoch 15, batch    52 | loss: 20.3552127CurrentTrain: epoch 15, batch    53 | loss: 22.6525315CurrentTrain: epoch 15, batch    54 | loss: 20.4331366CurrentTrain: epoch 15, batch    55 | loss: 13.8478403CurrentTrain: epoch 15, batch    56 | loss: 15.9523835CurrentTrain: epoch 15, batch    57 | loss: 14.0039821CurrentTrain: epoch 15, batch    58 | loss: 12.7890080CurrentTrain: epoch 15, batch    59 | loss: 13.9321358CurrentTrain: epoch 15, batch    60 | loss: 12.8950433CurrentTrain: epoch 15, batch    61 | loss: 12.8316154CurrentTrain: epoch  7, batch    62 | loss: 11.2880189CurrentTrain: epoch 15, batch     0 | loss: 32.0412276CurrentTrain: epoch 15, batch     1 | loss: 12.0006781CurrentTrain: epoch 15, batch     2 | loss: 19.3598184CurrentTrain: epoch 15, batch     3 | loss: 15.1763499CurrentTrain: epoch 15, batch     4 | loss: 14.6682491CurrentTrain: epoch 15, batch     5 | loss: 12.8154246CurrentTrain: epoch 15, batch     6 | loss: 16.4601331CurrentTrain: epoch 15, batch     7 | loss: 11.5582781CurrentTrain: epoch 15, batch     8 | loss: 15.2887598CurrentTrain: epoch 15, batch     9 | loss: 10.9240408CurrentTrain: epoch 15, batch    10 | loss: 17.0310209CurrentTrain: epoch 15, batch    11 | loss: 13.2391348CurrentTrain: epoch 15, batch    12 | loss: 12.9055997CurrentTrain: epoch 15, batch    13 | loss: 18.6735351CurrentTrain: epoch 15, batch    14 | loss: 23.4022682CurrentTrain: epoch 15, batch    15 | loss: 11.3833110CurrentTrain: epoch 15, batch    16 | loss: 18.9988084CurrentTrain: epoch 15, batch    17 | loss: 10.9713807CurrentTrain: epoch 15, batch    18 | loss: 11.5023971CurrentTrain: epoch 15, batch    19 | loss: 11.5597846CurrentTrain: epoch 15, batch    20 | loss: 13.0823792CurrentTrain: epoch 15, batch    21 | loss: 12.0139901CurrentTrain: epoch 15, batch    22 | loss: 29.1360101CurrentTrain: epoch 15, batch    23 | loss: 15.2963158CurrentTrain: epoch 15, batch    24 | loss: 10.8671950CurrentTrain: epoch 15, batch    25 | loss: 12.5045550CurrentTrain: epoch 15, batch    26 | loss: 14.5086596CurrentTrain: epoch 15, batch    27 | loss: 9.8836220CurrentTrain: epoch 15, batch    28 | loss: 14.0374897CurrentTrain: epoch 15, batch    29 | loss: 14.2001905CurrentTrain: epoch 15, batch    30 | loss: 29.5849649CurrentTrain: epoch 15, batch    31 | loss: 30.4616272CurrentTrain: epoch 15, batch    32 | loss: 14.6697894CurrentTrain: epoch 15, batch    33 | loss: 15.5839233CurrentTrain: epoch 15, batch    34 | loss: 15.1711586CurrentTrain: epoch 15, batch    35 | loss: 12.8287023CurrentTrain: epoch 15, batch    36 | loss: 13.1119091CurrentTrain: epoch 15, batch    37 | loss: 15.8540381CurrentTrain: epoch 15, batch    38 | loss: 15.5850077CurrentTrain: epoch 15, batch    39 | loss: 15.2767913CurrentTrain: epoch 15, batch    40 | loss: 20.0350395CurrentTrain: epoch 15, batch    41 | loss: 13.9766172CurrentTrain: epoch 15, batch    42 | loss: 14.2016002CurrentTrain: epoch 15, batch    43 | loss: 23.2569285CurrentTrain: epoch 15, batch    44 | loss: 19.5985768CurrentTrain: epoch 15, batch    45 | loss: 23.9642677CurrentTrain: epoch 15, batch    46 | loss: 17.3784006CurrentTrain: epoch 15, batch    47 | loss: 17.4201842CurrentTrain: epoch 15, batch    48 | loss: 14.5789290CurrentTrain: epoch 15, batch    49 | loss: 12.2329901CurrentTrain: epoch 15, batch    50 | loss: 16.6046515CurrentTrain: epoch 15, batch    51 | loss: 11.4582734CurrentTrain: epoch 15, batch    52 | loss: 23.3249080CurrentTrain: epoch 15, batch    53 | loss: 15.0364499CurrentTrain: epoch 15, batch    54 | loss: 14.3358842CurrentTrain: epoch 15, batch    55 | loss: 12.3019188CurrentTrain: epoch 15, batch    56 | loss: 17.3109332CurrentTrain: epoch 15, batch    57 | loss: 25.3393620CurrentTrain: epoch 15, batch    58 | loss: 17.2543601CurrentTrain: epoch 15, batch    59 | loss: 17.1428947CurrentTrain: epoch 15, batch    60 | loss: 13.4894227CurrentTrain: epoch 15, batch    61 | loss: 15.1482521CurrentTrain: epoch  7, batch    62 | loss: 16.7639048CurrentTrain: epoch 15, batch     0 | loss: 13.6232984CurrentTrain: epoch 15, batch     1 | loss: 23.4261063CurrentTrain: epoch 15, batch     2 | loss: 13.1216125CurrentTrain: epoch 15, batch     3 | loss: 29.3432994CurrentTrain: epoch 15, batch     4 | loss: 16.1425099CurrentTrain: epoch 15, batch     5 | loss: 23.7812050CurrentTrain: epoch 15, batch     6 | loss: 16.5631448CurrentTrain: epoch 15, batch     7 | loss: 11.4736234CurrentTrain: epoch 15, batch     8 | loss: 14.6971921CurrentTrain: epoch 15, batch     9 | loss: 29.3058367CurrentTrain: epoch 15, batch    10 | loss: 22.2195397CurrentTrain: epoch 15, batch    11 | loss: 13.8409103CurrentTrain: epoch 15, batch    12 | loss: 11.8780663CurrentTrain: epoch 15, batch    13 | loss: 12.7343916CurrentTrain: epoch 15, batch    14 | loss: 22.5433593CurrentTrain: epoch 15, batch    15 | loss: 21.9408928CurrentTrain: epoch 15, batch    16 | loss: 21.5726401CurrentTrain: epoch 15, batch    17 | loss: 11.6134552CurrentTrain: epoch 15, batch    18 | loss: 12.2260922CurrentTrain: epoch 15, batch    19 | loss: 11.9607127CurrentTrain: epoch 15, batch    20 | loss: 20.7068968CurrentTrain: epoch 15, batch    21 | loss: 14.5098141CurrentTrain: epoch 15, batch    22 | loss: 13.0727742CurrentTrain: epoch 15, batch    23 | loss: 16.1499671CurrentTrain: epoch 15, batch    24 | loss: 14.4956226CurrentTrain: epoch 15, batch    25 | loss: 18.6729762CurrentTrain: epoch 15, batch    26 | loss: 15.8187562CurrentTrain: epoch 15, batch    27 | loss: 28.9219438CurrentTrain: epoch 15, batch    28 | loss: 11.6916063CurrentTrain: epoch 15, batch    29 | loss: 16.0931377CurrentTrain: epoch 15, batch    30 | loss: 16.6184314CurrentTrain: epoch 15, batch    31 | loss: 10.5670833CurrentTrain: epoch 15, batch    32 | loss: 14.7174402CurrentTrain: epoch 15, batch    33 | loss: 18.4662188CurrentTrain: epoch 15, batch    34 | loss: 11.9805342CurrentTrain: epoch 15, batch    35 | loss: 15.3544443CurrentTrain: epoch 15, batch    36 | loss: 16.9964310CurrentTrain: epoch 15, batch    37 | loss: 13.3905643CurrentTrain: epoch 15, batch    38 | loss: 12.0237801CurrentTrain: epoch 15, batch    39 | loss: 12.4964572CurrentTrain: epoch 15, batch    40 | loss: 17.4895264CurrentTrain: epoch 15, batch    41 | loss: 23.3314667CurrentTrain: epoch 15, batch    42 | loss: 18.6616056CurrentTrain: epoch 15, batch    43 | loss: 9.9246601CurrentTrain: epoch 15, batch    44 | loss: 10.8494427CurrentTrain: epoch 15, batch    45 | loss: 11.7726487CurrentTrain: epoch 15, batch    46 | loss: 9.4107219CurrentTrain: epoch 15, batch    47 | loss: 13.0679274CurrentTrain: epoch 15, batch    48 | loss: 14.6035874CurrentTrain: epoch 15, batch    49 | loss: 17.2557011CurrentTrain: epoch 15, batch    50 | loss: 12.6811058CurrentTrain: epoch 15, batch    51 | loss: 14.0770774CurrentTrain: epoch 15, batch    52 | loss: 14.6460755CurrentTrain: epoch 15, batch    53 | loss: 14.5693468CurrentTrain: epoch 15, batch    54 | loss: 12.5287698CurrentTrain: epoch 15, batch    55 | loss: 15.5710161CurrentTrain: epoch 15, batch    56 | loss: 13.8431218CurrentTrain: epoch 15, batch    57 | loss: 10.2002530CurrentTrain: epoch 15, batch    58 | loss: 14.0533505CurrentTrain: epoch 15, batch    59 | loss: 10.8152143CurrentTrain: epoch 15, batch    60 | loss: 17.3742663CurrentTrain: epoch 15, batch    61 | loss: 10.9109930CurrentTrain: epoch  7, batch    62 | loss: 17.1560135CurrentTrain: epoch 15, batch     0 | loss: 25.7717481CurrentTrain: epoch 15, batch     1 | loss: 10.6367022CurrentTrain: epoch 15, batch     2 | loss: 11.2512951CurrentTrain: epoch 15, batch     3 | loss: 23.7007551CurrentTrain: epoch 15, batch     4 | loss: 10.8179936CurrentTrain: epoch 15, batch     5 | loss: 11.9252833CurrentTrain: epoch 15, batch     6 | loss: 12.2383623CurrentTrain: epoch 15, batch     7 | loss: 18.9355817CurrentTrain: epoch 15, batch     8 | loss: 11.7441254CurrentTrain: epoch 15, batch     9 | loss: 8.2781618CurrentTrain: epoch 15, batch    10 | loss: 20.4381174CurrentTrain: epoch 15, batch    11 | loss: 9.2973639CurrentTrain: epoch 15, batch    12 | loss: 9.6583045CurrentTrain: epoch 15, batch    13 | loss: 10.2402354CurrentTrain: epoch 15, batch    14 | loss: 10.0924682CurrentTrain: epoch 15, batch    15 | loss: 11.2642568CurrentTrain: epoch 15, batch    16 | loss: 15.1756034CurrentTrain: epoch 15, batch    17 | loss: 10.7503232CurrentTrain: epoch 15, batch    18 | loss: 19.0389117CurrentTrain: epoch 15, batch    19 | loss: 11.0504218CurrentTrain: epoch 15, batch    20 | loss: 13.3685389CurrentTrain: epoch 15, batch    21 | loss: 12.2353014CurrentTrain: epoch 15, batch    22 | loss: 14.3832624CurrentTrain: epoch 15, batch    23 | loss: 20.1634025CurrentTrain: epoch 15, batch    24 | loss: 15.3525405CurrentTrain: epoch 15, batch    25 | loss: 12.1594518CurrentTrain: epoch 15, batch    26 | loss: 16.6668268CurrentTrain: epoch 15, batch    27 | loss: 20.4757779CurrentTrain: epoch 15, batch    28 | loss: 15.0130204CurrentTrain: epoch 15, batch    29 | loss: 16.5561738CurrentTrain: epoch 15, batch    30 | loss: 37.2334255CurrentTrain: epoch 15, batch    31 | loss: 14.7990957CurrentTrain: epoch 15, batch    32 | loss: 10.4836225CurrentTrain: epoch 15, batch    33 | loss: 15.2125797CurrentTrain: epoch 15, batch    34 | loss: 11.9583442CurrentTrain: epoch 15, batch    35 | loss: 13.1443863CurrentTrain: epoch 15, batch    36 | loss: 12.1571243CurrentTrain: epoch 15, batch    37 | loss: 12.9709094CurrentTrain: epoch 15, batch    38 | loss: 18.7265468CurrentTrain: epoch 15, batch    39 | loss: 9.9603606CurrentTrain: epoch 15, batch    40 | loss: 10.1953180CurrentTrain: epoch 15, batch    41 | loss: 16.7965544CurrentTrain: epoch 15, batch    42 | loss: 14.3092989CurrentTrain: epoch 15, batch    43 | loss: 19.8464599CurrentTrain: epoch 15, batch    44 | loss: 13.7117027CurrentTrain: epoch 15, batch    45 | loss: 12.0276585CurrentTrain: epoch 15, batch    46 | loss: 17.3783977CurrentTrain: epoch 15, batch    47 | loss: 13.9875439CurrentTrain: epoch 15, batch    48 | loss: 14.2676587CurrentTrain: epoch 15, batch    49 | loss: 18.1117302CurrentTrain: epoch 15, batch    50 | loss: 11.6279887CurrentTrain: epoch 15, batch    51 | loss: 13.5830275CurrentTrain: epoch 15, batch    52 | loss: 12.0608369CurrentTrain: epoch 15, batch    53 | loss: 10.6686770CurrentTrain: epoch 15, batch    54 | loss: 13.9605341CurrentTrain: epoch 15, batch    55 | loss: 11.9812905CurrentTrain: epoch 15, batch    56 | loss: 19.5657392CurrentTrain: epoch 15, batch    57 | loss: 10.6331443CurrentTrain: epoch 15, batch    58 | loss: 14.1557095CurrentTrain: epoch 15, batch    59 | loss: 12.5708529CurrentTrain: epoch 15, batch    60 | loss: 14.7605326CurrentTrain: epoch 15, batch    61 | loss: 9.9872038CurrentTrain: epoch  7, batch    62 | loss: 10.5611402CurrentTrain: epoch 15, batch     0 | loss: 11.8487791CurrentTrain: epoch 15, batch     1 | loss: 9.2153292CurrentTrain: epoch 15, batch     2 | loss: 16.2688672CurrentTrain: epoch 15, batch     3 | loss: 15.4838304CurrentTrain: epoch 15, batch     4 | loss: 14.3288100CurrentTrain: epoch 15, batch     5 | loss: 22.0046258CurrentTrain: epoch 15, batch     6 | loss: 15.4340758CurrentTrain: epoch 15, batch     7 | loss: 11.8601248CurrentTrain: epoch 15, batch     8 | loss: 12.4065158CurrentTrain: epoch 15, batch     9 | loss: 18.1235374CurrentTrain: epoch 15, batch    10 | loss: 11.4558489CurrentTrain: epoch 15, batch    11 | loss: 12.8596812CurrentTrain: epoch 15, batch    12 | loss: 29.1670766CurrentTrain: epoch 15, batch    13 | loss: 23.4883707CurrentTrain: epoch 15, batch    14 | loss: 22.0519098CurrentTrain: epoch 15, batch    15 | loss: 10.0866715CurrentTrain: epoch 15, batch    16 | loss: 10.3437455CurrentTrain: epoch 15, batch    17 | loss: 16.6359799CurrentTrain: epoch 15, batch    18 | loss: 17.5360503CurrentTrain: epoch 15, batch    19 | loss: 15.9385289CurrentTrain: epoch 15, batch    20 | loss: 11.8450963CurrentTrain: epoch 15, batch    21 | loss: 20.1315008CurrentTrain: epoch 15, batch    22 | loss: 17.9253462CurrentTrain: epoch 15, batch    23 | loss: 13.1314317CurrentTrain: epoch 15, batch    24 | loss: 16.8937461CurrentTrain: epoch 15, batch    25 | loss: 10.0819415CurrentTrain: epoch 15, batch    26 | loss: 9.8009430CurrentTrain: epoch 15, batch    27 | loss: 23.0334312CurrentTrain: epoch 15, batch    28 | loss: 9.7310162CurrentTrain: epoch 15, batch    29 | loss: 11.6202070CurrentTrain: epoch 15, batch    30 | loss: 10.7076586CurrentTrain: epoch 15, batch    31 | loss: 14.1646053CurrentTrain: epoch 15, batch    32 | loss: 24.7532188CurrentTrain: epoch 15, batch    33 | loss: 11.6420813CurrentTrain: epoch 15, batch    34 | loss: 13.6352020CurrentTrain: epoch 15, batch    35 | loss: 12.4191887CurrentTrain: epoch 15, batch    36 | loss: 9.7621622CurrentTrain: epoch 15, batch    37 | loss: 12.3058999CurrentTrain: epoch 15, batch    38 | loss: 10.5881781CurrentTrain: epoch 15, batch    39 | loss: 9.0595060CurrentTrain: epoch 15, batch    40 | loss: 10.0030040CurrentTrain: epoch 15, batch    41 | loss: 12.1757419CurrentTrain: epoch 15, batch    42 | loss: 11.3056120CurrentTrain: epoch 15, batch    43 | loss: 18.2390725CurrentTrain: epoch 15, batch    44 | loss: 10.6134259CurrentTrain: epoch 15, batch    45 | loss: 20.4719494CurrentTrain: epoch 15, batch    46 | loss: 13.4722114CurrentTrain: epoch 15, batch    47 | loss: 16.4783278CurrentTrain: epoch 15, batch    48 | loss: 10.5804112CurrentTrain: epoch 15, batch    49 | loss: 14.4857782CurrentTrain: epoch 15, batch    50 | loss: 14.9247969CurrentTrain: epoch 15, batch    51 | loss: 9.8966247CurrentTrain: epoch 15, batch    52 | loss: 16.3788074CurrentTrain: epoch 15, batch    53 | loss: 13.5637992CurrentTrain: epoch 15, batch    54 | loss: 14.5111357CurrentTrain: epoch 15, batch    55 | loss: 17.3819103CurrentTrain: epoch 15, batch    56 | loss: 14.7426485CurrentTrain: epoch 15, batch    57 | loss: 12.2048642CurrentTrain: epoch 15, batch    58 | loss: 13.6791742CurrentTrain: epoch 15, batch    59 | loss: 9.2448860CurrentTrain: epoch 15, batch    60 | loss: 26.7485823CurrentTrain: epoch 15, batch    61 | loss: 9.8301157CurrentTrain: epoch  7, batch    62 | loss: 8.6938560CurrentTrain: epoch 15, batch     0 | loss: 11.6111586CurrentTrain: epoch 15, batch     1 | loss: 12.6346228CurrentTrain: epoch 15, batch     2 | loss: 14.0855869CurrentTrain: epoch 15, batch     3 | loss: 15.4472195CurrentTrain: epoch 15, batch     4 | loss: 13.8427326CurrentTrain: epoch 15, batch     5 | loss: 20.7767900CurrentTrain: epoch 15, batch     6 | loss: 11.2462399CurrentTrain: epoch 15, batch     7 | loss: 12.3144446CurrentTrain: epoch 15, batch     8 | loss: 10.2066679CurrentTrain: epoch 15, batch     9 | loss: 9.1896051CurrentTrain: epoch 15, batch    10 | loss: 20.6959263CurrentTrain: epoch 15, batch    11 | loss: 9.4961294CurrentTrain: epoch 15, batch    12 | loss: 30.9731871CurrentTrain: epoch 15, batch    13 | loss: 15.7376823CurrentTrain: epoch 15, batch    14 | loss: 14.4162930CurrentTrain: epoch 15, batch    15 | loss: 8.5405823CurrentTrain: epoch 15, batch    16 | loss: 8.1818821CurrentTrain: epoch 15, batch    17 | loss: 12.4316926CurrentTrain: epoch 15, batch    18 | loss: 14.7068891CurrentTrain: epoch 15, batch    19 | loss: 13.0127833CurrentTrain: epoch 15, batch    20 | loss: 13.4810889CurrentTrain: epoch 15, batch    21 | loss: 23.6994896CurrentTrain: epoch 15, batch    22 | loss: 10.0951800CurrentTrain: epoch 15, batch    23 | loss: 19.7923543CurrentTrain: epoch 15, batch    24 | loss: 15.6532461CurrentTrain: epoch 15, batch    25 | loss: 21.8082796CurrentTrain: epoch 15, batch    26 | loss: 35.1525224CurrentTrain: epoch 15, batch    27 | loss: 30.2852424CurrentTrain: epoch 15, batch    28 | loss: 9.7621148CurrentTrain: epoch 15, batch    29 | loss: 16.3750940CurrentTrain: epoch 15, batch    30 | loss: 12.4983160CurrentTrain: epoch 15, batch    31 | loss: 19.3649343CurrentTrain: epoch 15, batch    32 | loss: 10.5737974CurrentTrain: epoch 15, batch    33 | loss: 12.6839309CurrentTrain: epoch 15, batch    34 | loss: 13.3448723CurrentTrain: epoch 15, batch    35 | loss: 19.7935484CurrentTrain: epoch 15, batch    36 | loss: 16.1247507CurrentTrain: epoch 15, batch    37 | loss: 16.2781990CurrentTrain: epoch 15, batch    38 | loss: 17.0544090CurrentTrain: epoch 15, batch    39 | loss: 18.4443102CurrentTrain: epoch 15, batch    40 | loss: 13.5292300CurrentTrain: epoch 15, batch    41 | loss: 15.4661256CurrentTrain: epoch 15, batch    42 | loss: 15.0127187CurrentTrain: epoch 15, batch    43 | loss: 16.2108712CurrentTrain: epoch 15, batch    44 | loss: 14.0195154CurrentTrain: epoch 15, batch    45 | loss: 24.7010059CurrentTrain: epoch 15, batch    46 | loss: 10.2427968CurrentTrain: epoch 15, batch    47 | loss: 12.0965452CurrentTrain: epoch 15, batch    48 | loss: 26.3952059CurrentTrain: epoch 15, batch    49 | loss: 14.4632428CurrentTrain: epoch 15, batch    50 | loss: 16.2455198CurrentTrain: epoch 15, batch    51 | loss: 18.4930868CurrentTrain: epoch 15, batch    52 | loss: 15.1723500CurrentTrain: epoch 15, batch    53 | loss: 13.7321092CurrentTrain: epoch 15, batch    54 | loss: 19.4935607CurrentTrain: epoch 15, batch    55 | loss: 16.9181705CurrentTrain: epoch 15, batch    56 | loss: 10.8598804CurrentTrain: epoch 15, batch    57 | loss: 9.3147518CurrentTrain: epoch 15, batch    58 | loss: 19.1817721CurrentTrain: epoch 15, batch    59 | loss: 9.9867516CurrentTrain: epoch 15, batch    60 | loss: 14.0237351CurrentTrain: epoch 15, batch    61 | loss: 15.2991193CurrentTrain: epoch  7, batch    62 | loss: 27.1292235CurrentTrain: epoch 15, batch     0 | loss: 18.3997979CurrentTrain: epoch 15, batch     1 | loss: 12.6128450CurrentTrain: epoch 15, batch     2 | loss: 13.2188982CurrentTrain: epoch 15, batch     3 | loss: 12.2690537CurrentTrain: epoch 15, batch     4 | loss: 9.6165118CurrentTrain: epoch 15, batch     5 | loss: 9.7986437CurrentTrain: epoch 15, batch     6 | loss: 9.3346996CurrentTrain: epoch 15, batch     7 | loss: 10.7074535CurrentTrain: epoch 15, batch     8 | loss: 12.3715239CurrentTrain: epoch 15, batch     9 | loss: 21.5577208CurrentTrain: epoch 15, batch    10 | loss: 12.7270619CurrentTrain: epoch 15, batch    11 | loss: 11.8726108CurrentTrain: epoch 15, batch    12 | loss: 15.4819944CurrentTrain: epoch 15, batch    13 | loss: 15.3970270CurrentTrain: epoch 15, batch    14 | loss: 18.1096449CurrentTrain: epoch 15, batch    15 | loss: 12.6191289CurrentTrain: epoch 15, batch    16 | loss: 11.6008804CurrentTrain: epoch 15, batch    17 | loss: 19.0665750CurrentTrain: epoch 15, batch    18 | loss: 10.7901461CurrentTrain: epoch 15, batch    19 | loss: 14.9670958CurrentTrain: epoch 15, batch    20 | loss: 12.8739099CurrentTrain: epoch 15, batch    21 | loss: 12.1103647CurrentTrain: epoch 15, batch    22 | loss: 25.4597627CurrentTrain: epoch 15, batch    23 | loss: 14.5062427CurrentTrain: epoch 15, batch    24 | loss: 12.1329254CurrentTrain: epoch 15, batch    25 | loss: 10.9343077CurrentTrain: epoch 15, batch    26 | loss: 10.3042754CurrentTrain: epoch 15, batch    27 | loss: 17.2128250CurrentTrain: epoch 15, batch    28 | loss: 11.4251270CurrentTrain: epoch 15, batch    29 | loss: 12.9701952CurrentTrain: epoch 15, batch    30 | loss: 16.8025225CurrentTrain: epoch 15, batch    31 | loss: 18.6776661CurrentTrain: epoch 15, batch    32 | loss: 11.4574908CurrentTrain: epoch 15, batch    33 | loss: 12.3619846CurrentTrain: epoch 15, batch    34 | loss: 15.0568652CurrentTrain: epoch 15, batch    35 | loss: 19.7010311CurrentTrain: epoch 15, batch    36 | loss: 11.1119227CurrentTrain: epoch 15, batch    37 | loss: 14.2430052CurrentTrain: epoch 15, batch    38 | loss: 10.4968038CurrentTrain: epoch 15, batch    39 | loss: 10.4403917CurrentTrain: epoch 15, batch    40 | loss: 11.8574404CurrentTrain: epoch 15, batch    41 | loss: 17.2381911CurrentTrain: epoch 15, batch    42 | loss: 16.4545793CurrentTrain: epoch 15, batch    43 | loss: 18.3062650CurrentTrain: epoch 15, batch    44 | loss: 8.3986021CurrentTrain: epoch 15, batch    45 | loss: 19.1783979CurrentTrain: epoch 15, batch    46 | loss: 11.5100095CurrentTrain: epoch 15, batch    47 | loss: 12.7895802CurrentTrain: epoch 15, batch    48 | loss: 10.3997003CurrentTrain: epoch 15, batch    49 | loss: 21.0307871CurrentTrain: epoch 15, batch    50 | loss: 17.6844517CurrentTrain: epoch 15, batch    51 | loss: 10.1168120CurrentTrain: epoch 15, batch    52 | loss: 14.7564568CurrentTrain: epoch 15, batch    53 | loss: 10.2350429CurrentTrain: epoch 15, batch    54 | loss: 11.8015298CurrentTrain: epoch 15, batch    55 | loss: 12.1723855CurrentTrain: epoch 15, batch    56 | loss: 11.2388976CurrentTrain: epoch 15, batch    57 | loss: 12.1758301CurrentTrain: epoch 15, batch    58 | loss: 19.2028778CurrentTrain: epoch 15, batch    59 | loss: 13.2475887CurrentTrain: epoch 15, batch    60 | loss: 19.1294147CurrentTrain: epoch 15, batch    61 | loss: 19.6748756CurrentTrain: epoch  7, batch    62 | loss: 5.8106623CurrentTrain: epoch 15, batch     0 | loss: 9.7106758CurrentTrain: epoch 15, batch     1 | loss: 15.7811256CurrentTrain: epoch 15, batch     2 | loss: 9.2504136CurrentTrain: epoch 15, batch     3 | loss: 14.1676223CurrentTrain: epoch 15, batch     4 | loss: 17.5894265CurrentTrain: epoch 15, batch     5 | loss: 16.8816354CurrentTrain: epoch 15, batch     6 | loss: 16.2081714CurrentTrain: epoch 15, batch     7 | loss: 16.3467635CurrentTrain: epoch 15, batch     8 | loss: 12.9646816CurrentTrain: epoch 15, batch     9 | loss: 9.4530626CurrentTrain: epoch 15, batch    10 | loss: 11.9924121CurrentTrain: epoch 15, batch    11 | loss: 25.4539886CurrentTrain: epoch 15, batch    12 | loss: 8.8999445CurrentTrain: epoch 15, batch    13 | loss: 16.9466452CurrentTrain: epoch 15, batch    14 | loss: 11.1810832CurrentTrain: epoch 15, batch    15 | loss: 13.6459144CurrentTrain: epoch 15, batch    16 | loss: 14.2515073CurrentTrain: epoch 15, batch    17 | loss: 20.0016910CurrentTrain: epoch 15, batch    18 | loss: 10.2125117CurrentTrain: epoch 15, batch    19 | loss: 9.6961318CurrentTrain: epoch 15, batch    20 | loss: 11.4750243CurrentTrain: epoch 15, batch    21 | loss: 18.3523190CurrentTrain: epoch 15, batch    22 | loss: 9.9346293CurrentTrain: epoch 15, batch    23 | loss: 17.9477856CurrentTrain: epoch 15, batch    24 | loss: 9.7168743CurrentTrain: epoch 15, batch    25 | loss: 18.2841210CurrentTrain: epoch 15, batch    26 | loss: 19.7786316CurrentTrain: epoch 15, batch    27 | loss: 20.0478302CurrentTrain: epoch 15, batch    28 | loss: 47.3668949CurrentTrain: epoch 15, batch    29 | loss: 13.0119618CurrentTrain: epoch 15, batch    30 | loss: 9.0068193CurrentTrain: epoch 15, batch    31 | loss: 9.9342930CurrentTrain: epoch 15, batch    32 | loss: 10.2090894CurrentTrain: epoch 15, batch    33 | loss: 10.6035728CurrentTrain: epoch 15, batch    34 | loss: 13.4625004CurrentTrain: epoch 15, batch    35 | loss: 11.3898954CurrentTrain: epoch 15, batch    36 | loss: 14.9622399CurrentTrain: epoch 15, batch    37 | loss: 11.3589003CurrentTrain: epoch 15, batch    38 | loss: 15.3934312CurrentTrain: epoch 15, batch    39 | loss: 17.3456394CurrentTrain: epoch 15, batch    40 | loss: 21.5386870CurrentTrain: epoch 15, batch    41 | loss: 10.0429432CurrentTrain: epoch 15, batch    42 | loss: 13.4675580CurrentTrain: epoch 15, batch    43 | loss: 9.7248684CurrentTrain: epoch 15, batch    44 | loss: 11.9041762CurrentTrain: epoch 15, batch    45 | loss: 10.2664153CurrentTrain: epoch 15, batch    46 | loss: 14.0930825CurrentTrain: epoch 15, batch    47 | loss: 19.5053447CurrentTrain: epoch 15, batch    48 | loss: 17.3826127CurrentTrain: epoch 15, batch    49 | loss: 21.2613152CurrentTrain: epoch 15, batch    50 | loss: 19.9630377CurrentTrain: epoch 15, batch    51 | loss: 14.0402651CurrentTrain: epoch 15, batch    52 | loss: 12.6669052CurrentTrain: epoch 15, batch    53 | loss: 13.8797059CurrentTrain: epoch 15, batch    54 | loss: 13.3089939CurrentTrain: epoch 15, batch    55 | loss: 9.1611896CurrentTrain: epoch 15, batch    56 | loss: 16.9143929CurrentTrain: epoch 15, batch    57 | loss: 13.9186711CurrentTrain: epoch 15, batch    58 | loss: 12.4885548CurrentTrain: epoch 15, batch    59 | loss: 14.6975627CurrentTrain: epoch 15, batch    60 | loss: 15.6324210CurrentTrain: epoch 15, batch    61 | loss: 9.6621979CurrentTrain: epoch  7, batch    62 | loss: 14.7459568CurrentTrain: epoch 15, batch     0 | loss: 12.0285118CurrentTrain: epoch 15, batch     1 | loss: 19.3084279CurrentTrain: epoch 15, batch     2 | loss: 24.7972411CurrentTrain: epoch 15, batch     3 | loss: 11.7599611CurrentTrain: epoch 15, batch     4 | loss: 8.8699722CurrentTrain: epoch 15, batch     5 | loss: 14.8522670CurrentTrain: epoch 15, batch     6 | loss: 14.6344213CurrentTrain: epoch 15, batch     7 | loss: 7.9733016CurrentTrain: epoch 15, batch     8 | loss: 10.9644573CurrentTrain: epoch 15, batch     9 | loss: 13.4358091CurrentTrain: epoch 15, batch    10 | loss: 9.9331089CurrentTrain: epoch 15, batch    11 | loss: 11.1995212CurrentTrain: epoch 15, batch    12 | loss: 11.6278050CurrentTrain: epoch 15, batch    13 | loss: 10.0544437CurrentTrain: epoch 15, batch    14 | loss: 15.9390633CurrentTrain: epoch 15, batch    15 | loss: 8.2541945CurrentTrain: epoch 15, batch    16 | loss: 12.5606062CurrentTrain: epoch 15, batch    17 | loss: 12.3169260CurrentTrain: epoch 15, batch    18 | loss: 17.9322402CurrentTrain: epoch 15, batch    19 | loss: 16.3853547CurrentTrain: epoch 15, batch    20 | loss: 17.7995144CurrentTrain: epoch 15, batch    21 | loss: 13.1129143CurrentTrain: epoch 15, batch    22 | loss: 10.9278892CurrentTrain: epoch 15, batch    23 | loss: 15.9740362CurrentTrain: epoch 15, batch    24 | loss: 12.7770277CurrentTrain: epoch 15, batch    25 | loss: 11.8824319CurrentTrain: epoch 15, batch    26 | loss: 16.6469990CurrentTrain: epoch 15, batch    27 | loss: 11.8143647CurrentTrain: epoch 15, batch    28 | loss: 16.7817098CurrentTrain: epoch 15, batch    29 | loss: 13.2548905CurrentTrain: epoch 15, batch    30 | loss: 13.1148759CurrentTrain: epoch 15, batch    31 | loss: 17.1533251CurrentTrain: epoch 15, batch    32 | loss: 12.0836412CurrentTrain: epoch 15, batch    33 | loss: 26.9330262CurrentTrain: epoch 15, batch    34 | loss: 11.2989781CurrentTrain: epoch 15, batch    35 | loss: 10.8710239CurrentTrain: epoch 15, batch    36 | loss: 8.6549649CurrentTrain: epoch 15, batch    37 | loss: 13.2056348CurrentTrain: epoch 15, batch    38 | loss: 15.4489745CurrentTrain: epoch 15, batch    39 | loss: 9.4641555CurrentTrain: epoch 15, batch    40 | loss: 13.7978620CurrentTrain: epoch 15, batch    41 | loss: 9.7629804CurrentTrain: epoch 15, batch    42 | loss: 11.1184055CurrentTrain: epoch 15, batch    43 | loss: 9.6214602CurrentTrain: epoch 15, batch    44 | loss: 12.0250033CurrentTrain: epoch 15, batch    45 | loss: 11.5004894CurrentTrain: epoch 15, batch    46 | loss: 11.7101584CurrentTrain: epoch 15, batch    47 | loss: 10.8110296CurrentTrain: epoch 15, batch    48 | loss: 42.2774034CurrentTrain: epoch 15, batch    49 | loss: 15.5408027CurrentTrain: epoch 15, batch    50 | loss: 16.3058391CurrentTrain: epoch 15, batch    51 | loss: 21.6626780CurrentTrain: epoch 15, batch    52 | loss: 15.1192910CurrentTrain: epoch 15, batch    53 | loss: 11.4376085CurrentTrain: epoch 15, batch    54 | loss: 12.9595068CurrentTrain: epoch 15, batch    55 | loss: 11.4561550CurrentTrain: epoch 15, batch    56 | loss: 9.7836235CurrentTrain: epoch 15, batch    57 | loss: 28.4390119CurrentTrain: epoch 15, batch    58 | loss: 13.4134480CurrentTrain: epoch 15, batch    59 | loss: 15.6611797CurrentTrain: epoch 15, batch    60 | loss: 22.0137917CurrentTrain: epoch 15, batch    61 | loss: 13.8542516CurrentTrain: epoch  7, batch    62 | loss: 14.9472697
#### movement ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "movement" means literary, artistic, scientific or philosophical movement or scene associated with this person or work.
Here is an example:
Relation: movement
Context: the theme of the loss of identity in crowds in this painting can be seen as a precursor to expressionist painting , particularly the work of james ensor .
Head Entity: james ensor
Tail Entity: expressionist
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "movement":
Relation: movement  
Context: The surrealist movement, which sought to challenge the conventional perception of reality, was significantly influenced by the works of Salvador Dalí.  
Head Entity: Salvador Dalí  
Tail Entity: surrealist  

Relation: movement  
Context: The emergence of the romantic movement in literature was marked by the profound impact of poets like William Wordsworth and Samuel Taylor Coleridge.  
Head Entity: William Wordsworth  
Tail Entity: romantic  
#### located in the administrative territorial entity ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "located in the administrative territorial entity" means the item is located on the territory of the following administrative entity.
Here is an example:
Relation: located in the administrative territorial entity
Context: lyons hill is a restored village , and former parish with church , now part of the community of ardclough in north county kildare .
Head Entity: ardclough
Tail Entity: county kildare
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "located in the administrative territorial entity":
Relation: located in the administrative territorial entity  
Context: The city of San Francisco is known for its iconic Golden Gate Bridge and is part of the larger San Francisco County in California.  
Head Entity: San Francisco  
Tail Entity: San Francisco County  

Relation: located in the administrative territorial entity  
Context: The village of Haverfordwest serves as the county town of Pembrokeshire, situated in the picturesque region of Wales.  
Head Entity: Haverfordwest  
Tail Entity: Pembrokeshire  
#### head of government ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "head of government" means head of the executive power of this town, city, municipality, state, country, or other governmental body.
Here is an example:
Relation: head of government
Context: on february 10 , 2010 , stefan mappus was elected minister - president of baden - württemberg , and asked that all ministers continue their roles .
Head Entity: baden - württemberg
Tail Entity: stefan mappus
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "head of government":
Relation: head of government  
Context: in 2021, jacinda ardern was re-elected as the prime minister of new zealand, leading the labor party to a significant victory in the general elections.  
Head Entity: new zealand  
Tail Entity: jacinda ardern  

Relation: head of government  
Context: during the annual summit, justin trudeau addressed the nation as the prime minister of canada, outlining his government's plans for the upcoming year.  
Head Entity: canada  
Tail Entity: justin trudeau  
#### member of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "member of" means organization, club or musical group to which the subject belongs.
Here is an example:
Relation: member of
Context: hank henshaw became a member of the sinestro corps during the sinestro corps war .
Head Entity: hank henshaw
Tail Entity: sinestro corps
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "member of":
Relation: member of  
Context: after years of dedication, she was finally recognized as a member of the national ballet company.  
Head Entity: she  
Tail Entity: national ballet company  

Relation: member of  
Context: the renowned scientist was proud to be a member of the international academy of sciences.  
Head Entity: the renowned scientist  
Tail Entity: international academy of sciences  
#### crosses ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "crosses" means obstacle which this bridge crosses over or this tunnel goes under.
Here is an example:
Relation: crosses
Context: the tobin bridge , a major regional transportation artery , carries route 1 from chelsea across the mystic river to charlestown .
Head Entity: tobin bridge
Tail Entity: mystic river
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "crosses":
Relation: crosses  
Context: the golden gate bridge spans the golden gate strait, connecting san francisco to marin county and allowing vehicles to cross over the water.  
Head Entity: golden gate bridge  
Tail Entity: golden gate strait  

Relation: crosses  
Context: the channel tunnel, also known as the chunnel, runs beneath the english channel, providing a rail link between the united kingdom and mainland europe.  
Head Entity: channel tunnel  
Tail Entity: english channel  
#### tributary ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "tributary" means watercourse that flows into an other one.
Here is an example:
Relation: tributary
Context: it is on the panj river just downstream from the mouth of the bartang river at the point where the panj briefly turns west before resuming its northerly course .
Head Entity: panj river
Tail Entity: bartang river
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "tributary":
Relation: tributary  
Context: The Missouri River, which is a major tributary of the Mississippi River, flows through several states before joining the larger watercourse.  
Head Entity: Missouri River  
Tail Entity: Mississippi River  

Relation: tributary  
Context: The Rio Grande serves as a natural border between the United States and Mexico, and it has several tributaries, including the Pecos River.  
Head Entity: Rio Grande  
Tail Entity: Pecos River  
#### position held ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "position held" means subject currently or formerly holds the object position or public office.
Here is an example:
Relation: position held
Context: his father robert docking and grandfather george docking both served as governor of kansas .
Head Entity: george docking
Tail Entity: governor of kansas
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "position held":
Relation: position held  
Context: After a successful career in politics, Sarah Thompson was appointed as the Secretary of State for New York.  
Head Entity: Sarah Thompson  
Tail Entity: Secretary of State for New York  

Relation: position held  
Context: John Smith, a renowned scientist, was the director of the National Institute of Health for over a decade.  
Head Entity: John Smith  
Tail Entity: director of the National Institute of Health  
#### developer ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "developer" means organization or person that developed the item.
Here is an example:
Relation: developer
Context: in january 2009 , it was revealed that raw thrills and specular interactive came together to produce a spiritual successor to the midway arcade racer " hydro thunder " with a game entitled h2overdrive .
Head Entity: h2overdrive
Tail Entity: specular interactive
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "developer":
Relation: developer  
Context: In 2015, the popular game "The Witcher 3: Wild Hunt" was released, developed by CD Projekt Red, which received critical acclaim for its storytelling and open-world design.  
Head Entity: The Witcher 3: Wild Hunt  
Tail Entity: CD Projekt Red  

Relation: developer  
Context: The innovative mobile game "Angry Birds" was created by Rovio Entertainment, which became a cultural phenomenon shortly after its launch in 2009.  
Head Entity: Angry Birds  
Tail Entity: Rovio Entertainment  
#### location of formation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "location of formation" means location where a group or organization was formed.
Here is an example:
Relation: location of formation
Context: holy molar are a san diego based band formed in 2001 .
Head Entity: holy molar
Tail Entity: san diego
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "location of formation":
Relation: location of formation  
Context: the tech startup was established in silicon valley in 2015.  
Head Entity: the tech startup  
Tail Entity: silicon valley  

Relation: location of formation  
Context: the famous rock band was formed in new york city during the late 1980s.  
Head Entity: the famous rock band  
Tail Entity: new york city  
#### country of origin ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "country of origin" means country of origin of this item (creative work, food, phrase, product, etc.).
Here is an example:
Relation: country of origin
Context: she has appeared in some american television shows , beginning with her regular role as snow white 's stepmother , evil queen lillian " lily " white in the series " the charmings " .
Head Entity: the charmings
Tail Entity: american
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "country of origin":
Relation: country of origin  
Context: The famous dish sushi is traditionally made with vinegared rice and often includes seafood, vegetables, and occasionally tropical fruits, originating from Japan.  
Head Entity: sushi  
Tail Entity: Japan  

Relation: country of origin  
Context: The iconic painting "Mona Lisa," known for its captivating smile, was created by the renowned artist Leonardo da Vinci during the Italian Renaissance in Italy.  
Head Entity: Mona Lisa  
Tail Entity: Italy  
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 85.94%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 86.25%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 86.46%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 89.06%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 90.28%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 91.25%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 91.48%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 92.19%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 92.31%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 92.41%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 92.50%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 92.97%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 93.38%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 93.40%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 94.06%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 94.35%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 94.32%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 94.57%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 94.79%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 94.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 94.71%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 94.68%   [EVAL] batch:   27 | acc: 87.50%,  total acc: 94.42%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 94.40%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 94.58%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 94.76%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 94.92%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 95.08%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 95.22%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 95.18%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 95.31%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 95.44%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 95.39%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 95.51%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 95.62%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 95.73%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 95.83%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 95.93%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 96.02%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 96.11%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 96.20%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 96.28%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 96.22%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 96.30%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 96.38%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 96.32%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 96.15%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 96.11%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 96.18%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 95.91%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 95.87%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 95.94%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 95.91%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 95.97%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 95.94%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 96.00%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 95.97%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 95.24%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 85.94%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 86.25%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 86.46%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 89.06%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 90.28%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 91.25%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 91.48%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 92.19%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 92.31%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 92.41%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 92.50%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 92.97%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 93.38%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 93.40%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 94.06%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 94.35%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 94.32%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 94.57%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 94.79%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 94.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 94.71%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 94.68%   [EVAL] batch:   27 | acc: 87.50%,  total acc: 94.42%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 94.40%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 94.58%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 94.76%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 94.92%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 95.08%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 95.22%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 95.18%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 95.31%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 95.44%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 95.39%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 95.51%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 95.62%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 95.73%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 95.83%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 95.93%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 96.02%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 96.11%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 96.20%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 96.28%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 96.22%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 96.30%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 96.38%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 96.32%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 96.15%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 96.11%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 96.18%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 95.91%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 95.87%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 95.94%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 95.91%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 95.97%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 95.94%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 96.00%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 95.97%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 95.24%   
cur_acc:  ['0.9524']
his_acc:  ['0.9524']
CurrentTrain: epoch 15, batch     0 | loss: 22.6097739CurrentTrain: epoch 15, batch     1 | loss: 31.3025728CurrentTrain: epoch 15, batch     2 | loss: 21.7722308CurrentTrain: epoch  1, batch     3 | loss: 7.8265199CurrentTrain: epoch 15, batch     0 | loss: 16.6916522CurrentTrain: epoch 15, batch     1 | loss: 13.3526825CurrentTrain: epoch 15, batch     2 | loss: 15.5536623CurrentTrain: epoch  1, batch     3 | loss: 9.3547677CurrentTrain: epoch 15, batch     0 | loss: 12.6288121CurrentTrain: epoch 15, batch     1 | loss: 20.0819546CurrentTrain: epoch 15, batch     2 | loss: 21.9298289CurrentTrain: epoch  1, batch     3 | loss: 8.7328087CurrentTrain: epoch 15, batch     0 | loss: 19.8334513CurrentTrain: epoch 15, batch     1 | loss: 14.1724708CurrentTrain: epoch 15, batch     2 | loss: 12.1467671CurrentTrain: epoch  1, batch     3 | loss: 11.4987121CurrentTrain: epoch 15, batch     0 | loss: 13.7999480CurrentTrain: epoch 15, batch     1 | loss: 9.5114283CurrentTrain: epoch 15, batch     2 | loss: 19.5045604CurrentTrain: epoch  1, batch     3 | loss: 6.8847314CurrentTrain: epoch 15, batch     0 | loss: 13.6888166CurrentTrain: epoch 15, batch     1 | loss: 13.9305589CurrentTrain: epoch 15, batch     2 | loss: 22.1060172CurrentTrain: epoch  1, batch     3 | loss: 8.2833034CurrentTrain: epoch 15, batch     0 | loss: 12.9059028CurrentTrain: epoch 15, batch     1 | loss: 11.1478448CurrentTrain: epoch 15, batch     2 | loss: 11.0462346CurrentTrain: epoch  1, batch     3 | loss: 21.4000180CurrentTrain: epoch 15, batch     0 | loss: 9.4275452CurrentTrain: epoch 15, batch     1 | loss: 9.7205932CurrentTrain: epoch 15, batch     2 | loss: 11.9933782CurrentTrain: epoch  1, batch     3 | loss: 7.5045220CurrentTrain: epoch 15, batch     0 | loss: 17.3639100CurrentTrain: epoch 15, batch     1 | loss: 9.8463208CurrentTrain: epoch 15, batch     2 | loss: 16.0695491CurrentTrain: epoch  1, batch     3 | loss: 8.4413851CurrentTrain: epoch 15, batch     0 | loss: 10.6642309CurrentTrain: epoch 15, batch     1 | loss: 9.9663724CurrentTrain: epoch 15, batch     2 | loss: 14.7136225CurrentTrain: epoch  1, batch     3 | loss: 9.6392923
#### applies to jurisdiction ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "applies to jurisdiction" means the item (institution, law, public office, public register...) or statement belongs to or has power over or applies to the value (a territorial jurisdiction: a country, state, municipality, ...).
Here is an example:
Relation: applies to jurisdiction
Context: in 2004 the catalan government gave him the george cross .
Head Entity: george cross
Tail Entity: catalan
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "applies to jurisdiction":
Relation: applies to jurisdiction  
Context: The new environmental regulations introduced by the European Union will affect all member states.  
Head Entity: environmental regulations  
Tail Entity: European Union  

Relation: applies to jurisdiction  
Context: The Supreme Court's ruling has significant implications for the state of California's water rights.  
Head Entity: Supreme Court  
Tail Entity: California  
#### performer ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "performer" means actor, musician, band or other performer associated with this role or musical work.
Here is an example:
Relation: performer
Context: " tenth avenue freeze - out " is the second song on bruce springsteen 's breakthrough album " born to run " , released in 1975 .
Head Entity: tenth avenue freeze - out
Tail Entity: bruce springsteen
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "performer":
Relation: performer  
Context: "Shape of You" is a popular song by Ed Sheeran that topped charts worldwide after its release in 2017.  
Head Entity: Shape of You  
Tail Entity: Ed Sheeran  

Relation: performer  
Context: The Beatles are widely regarded as one of the most influential bands in the history of music, with hits like "Hey Jude" and "Let It Be."  
Head Entity: The Beatles  
Tail Entity: Hey Jude  
#### manufacturer ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "manufacturer" means manufacturer or producer of this product.
Here is an example:
Relation: manufacturer
Context: the glasflügel h-201 standard " libelle " ( german : " dragonfly " ) is an early composite standard class single - seat sailplane produced by glasflügel from 1967 .
Head Entity: dragonfly
Tail Entity: glasflügel
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "manufacturer":
Relation: manufacturer  
Context: The iPhone 13, a smartphone developed by Apple, was released in September 2021 and features advanced camera technology and improved battery life.  
Head Entity: iPhone 13  
Tail Entity: Apple  

Relation: manufacturer  
Context: The Tesla Model S is an all-electric luxury sedan that was first introduced by Tesla, Inc. in 2012, showcasing innovative technology and performance.  
Head Entity: Tesla Model S  
Tail Entity: Tesla, Inc.  
#### participant ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "participant" means person, group of people or organization (object) that actively takes/took part in an event or process.
Here is an example:
Relation: participant
Context: in contrast to gwladys and roger 's allegiance to the house of lancaster , their three sons were staunch yorkists during the wars of the roses .
Head Entity: wars of the roses
Tail Entity: house of lancaster
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "participant":
Relation: participant  
Context: During the annual tech conference, several leading companies showcased their latest innovations, with Google and Microsoft being the most prominent participants.  
Head Entity: tech conference  
Tail Entity: Google  

Relation: participant  
Context: The local community organized a charity run, where various schools and organizations, including the Red Cross, were active participants in raising funds for the homeless.  
Head Entity: charity run  
Tail Entity: Red Cross  
#### operating system ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "operating system" means operating system (OS) on which a software works or the OS installed on hardware.
Here is an example:
Relation: operating system
Context: the software is written in python and works on all the platforms supported by blender : windows , osx and linux .
Head Entity: blender
Tail Entity: windows
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "operating system":
Relation: operating system  
Context: The latest version of the application is compatible with various operating systems, including macOS, Windows, and Ubuntu.  
Head Entity: application  
Tail Entity: macOS  

Relation: operating system  
Context: Many developers prefer to use Linux as their primary operating system due to its flexibility and open-source nature.  
Head Entity: developers  
Tail Entity: Linux  
#### competition class ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "competition class" means official classification by a regulating body under which the subject (events, teams, participants, or equipment) qualifies for inclusion.
Here is an example:
Relation: competition class
Context: in july 2007 , griffin lost by a technical knockout to light heavyweight champion danny green of the world boxing association at challenge stadium in perth , australia .
Head Entity: danny green
Tail Entity: light heavyweight
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "competition class":
Relation: competition class  
Context: during the 2020 olympics, the men's 100 meters final showcased athletes from various competition classes, with usain bolt representing the sprinting class.  
Head Entity: usain bolt  
Tail Entity: sprinting  

Relation: competition class  
Context: the formula one race in monaco is known for its high-speed competition class, where lewis hamilton competed fiercely against other top drivers.  
Head Entity: lewis hamilton  
Tail Entity: formula one
#### successful candidate ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "successful candidate" means person elected after the election.
Here is an example:
Relation: successful candidate
Context: the qup gained 3 seats from its previous election result , and won a 10 % swing to it in primary votes , but failed to defeat the incumbent labor government led by ted theodore .
Head Entity: previous election
Tail Entity: ted theodore
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "successful candidate":
Relation: successful candidate  
Context: After a closely contested race, the voters chose Sarah Johnson as their representative, marking a significant shift in the political landscape of the district.  
Head Entity: closely contested race  
Tail Entity: Sarah Johnson  

Relation: successful candidate  
Context: In the recent mayoral election, the community rallied behind Mark Thompson, who promised to bring new initiatives to the city.  
Head Entity: recent mayoral election  
Tail Entity: Mark Thompson  
#### sibling ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "sibling" means the subject and the object have the same parents.
Here is an example:
Relation: sibling
Context: although the latter 's eldest son , lǫgmaðr , appears to have succeeded to the kingship , he was soon forced to contend with factions supporting his younger brothers : haraldr , and óláfr .
Head Entity: lǫgmaðr
Tail Entity: óláfr
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "sibling":
Relation: sibling  
Context: In the royal family of the ancient kingdom, the two brothers, Eirik and Sigurd, were known for their fierce loyalty to each other, having grown up under the same roof and shared the same parents.  
Head Entity: Eirik  
Tail Entity: Sigurd  

Relation: sibling  
Context: During the family reunion, it was evident that both Clara and her brother, James, had inherited their parents' love for music, often performing together at local events.  
Head Entity: Clara  
Tail Entity: James  
#### military branch ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "military branch" means branch to which this military unit, award, office, or person belongs.
Here is an example:
Relation: military branch
Context: two of her sons , tunis and thomas tingey rose to prominence in the union navy during the civil war .
Head Entity: tunis
Tail Entity: union navy
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "military branch":
Relation: military branch  
Context: after serving in the army, he transitioned to a role in the air force where he excelled in various leadership positions.  
Head Entity: army  
Tail Entity: air force  

Relation: military branch  
Context: the general was proud of his service in the marine corps, which shaped his leadership style throughout his career.  
Head Entity: general  
Tail Entity: marine corps  
#### child ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "child" means subject has object as child.
Here is an example:
Relation: child
Context: developed on the east bank of the river near the former native village of otstonwakin , the borough is named for andrew montour , the french / native american and son of madame montour .
Head Entity: madame montour
Tail Entity: andrew montour
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "child":
Relation: child  
Context: In the small town of Maplewood, Sarah Johnson is known for her dedication to her family, especially her son, Michael, who is a talented musician.  
Head Entity: Sarah Johnson  
Tail Entity: Michael  

Relation: child  
Context: The famous author, J.K. Rowling, often draws inspiration from her experiences as a mother, particularly her relationship with her daughter, Jessica.  
Head Entity: J.K. Rowling  
Tail Entity: Jessica  
MemoryTrain:  epoch 15, batch     0 | loss: 8.4055116MemoryTrain:  epoch 15, batch     1 | loss: 7.7044249MemoryTrain:  epoch 15, batch     2 | loss: 5.6688901MemoryTrain:  epoch 11, batch     3 | loss: 7.7788971MemoryTrain:  epoch 15, batch     0 | loss: 7.9017103MemoryTrain:  epoch 15, batch     1 | loss: 7.7156103MemoryTrain:  epoch 15, batch     2 | loss: 7.6944465MemoryTrain:  epoch 11, batch     3 | loss: 6.7512739MemoryTrain:  epoch 15, batch     0 | loss: 5.1247150MemoryTrain:  epoch 15, batch     1 | loss: 11.5485929MemoryTrain:  epoch 15, batch     2 | loss: 7.1087460MemoryTrain:  epoch 11, batch     3 | loss: 4.6515654MemoryTrain:  epoch 15, batch     0 | loss: 7.6868283MemoryTrain:  epoch 15, batch     1 | loss: 4.2525571MemoryTrain:  epoch 15, batch     2 | loss: 3.1170538MemoryTrain:  epoch 11, batch     3 | loss: 6.7924500MemoryTrain:  epoch 15, batch     0 | loss: 8.8956147MemoryTrain:  epoch 15, batch     1 | loss: 5.6628571MemoryTrain:  epoch 15, batch     2 | loss: 4.9996852MemoryTrain:  epoch 11, batch     3 | loss: 9.0098699MemoryTrain:  epoch 15, batch     0 | loss: 6.1024040MemoryTrain:  epoch 15, batch     1 | loss: 3.5013414MemoryTrain:  epoch 15, batch     2 | loss: 6.3118241MemoryTrain:  epoch 11, batch     3 | loss: 7.2048320MemoryTrain:  epoch 15, batch     0 | loss: 3.4066654MemoryTrain:  epoch 15, batch     1 | loss: 4.3997245MemoryTrain:  epoch 15, batch     2 | loss: 9.4125238MemoryTrain:  epoch 11, batch     3 | loss: 2.9221093MemoryTrain:  epoch 15, batch     0 | loss: 5.5218357MemoryTrain:  epoch 15, batch     1 | loss: 5.4243857MemoryTrain:  epoch 15, batch     2 | loss: 3.4601545MemoryTrain:  epoch 11, batch     3 | loss: 4.1002334MemoryTrain:  epoch 15, batch     0 | loss: 8.0990879MemoryTrain:  epoch 15, batch     1 | loss: 4.5067311MemoryTrain:  epoch 15, batch     2 | loss: 3.4258967MemoryTrain:  epoch 11, batch     3 | loss: 4.7719967MemoryTrain:  epoch 15, batch     0 | loss: 3.8791950MemoryTrain:  epoch 15, batch     1 | loss: 7.6157953MemoryTrain:  epoch 15, batch     2 | loss: 2.6484287MemoryTrain:  epoch 11, batch     3 | loss: 3.2626682
[EVAL] batch:    0 | acc: 43.75%,  total acc: 43.75%   [EVAL] batch:    1 | acc: 62.50%,  total acc: 53.12%   [EVAL] batch:    2 | acc: 68.75%,  total acc: 58.33%   [EVAL] batch:    3 | acc: 62.50%,  total acc: 59.38%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 61.25%   [EVAL] batch:    5 | acc: 68.75%,  total acc: 62.50%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 65.18%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 69.53%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 70.83%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 72.50%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 74.43%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 75.00%   [EVAL] batch:   12 | acc: 100.00%,  total acc: 76.92%   [EVAL] batch:   13 | acc: 81.25%,  total acc: 77.23%   [EVAL] batch:   14 | acc: 87.50%,  total acc: 77.92%   [EVAL] batch:   15 | acc: 93.75%,  total acc: 78.91%   [EVAL] batch:   16 | acc: 87.50%,  total acc: 79.41%   [EVAL] batch:   17 | acc: 100.00%,  total acc: 80.56%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 81.58%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 81.56%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 81.85%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 82.39%   [EVAL] batch:   22 | acc: 81.25%,  total acc: 82.34%   [EVAL] batch:   23 | acc: 81.25%,  total acc: 82.29%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 82.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 83.17%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 83.80%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 84.38%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 84.91%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 85.42%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 85.89%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 86.13%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 86.55%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 86.95%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 87.32%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 87.67%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 88.01%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 88.32%   [EVAL] batch:   38 | acc: 87.50%,  total acc: 88.30%   [EVAL] batch:   39 | acc: 75.00%,  total acc: 87.97%   [EVAL] batch:   40 | acc: 75.00%,  total acc: 87.65%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 87.65%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 87.79%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 87.64%   [EVAL] batch:   44 | acc: 50.00%,  total acc: 86.81%   [EVAL] batch:   45 | acc: 37.50%,  total acc: 85.73%   [EVAL] batch:   46 | acc: 37.50%,  total acc: 84.71%   [EVAL] batch:   47 | acc: 62.50%,  total acc: 84.24%   [EVAL] batch:   48 | acc: 56.25%,  total acc: 83.67%   [EVAL] batch:   49 | acc: 37.50%,  total acc: 82.75%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 82.97%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 83.17%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 83.37%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 83.33%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 83.52%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 83.59%   [EVAL] batch:   56 | acc: 87.50%,  total acc: 83.66%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 83.73%   [EVAL] batch:   58 | acc: 81.25%,  total acc: 83.69%   [EVAL] batch:   59 | acc: 62.50%,  total acc: 83.33%   [EVAL] batch:   60 | acc: 81.25%,  total acc: 83.30%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 83.37%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 82.84%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 85.94%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 86.25%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 88.28%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 89.58%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 88.75%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 89.20%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 90.10%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 89.90%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 90.18%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 90.42%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 91.02%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 91.18%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 91.32%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 91.78%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 91.88%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 91.96%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 91.48%   [EVAL] batch:   22 | acc: 75.00%,  total acc: 90.76%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 91.15%   [EVAL] batch:   24 | acc: 81.25%,  total acc: 90.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 91.11%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 91.44%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 91.52%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 91.59%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 91.88%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 92.14%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 92.38%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 92.61%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 92.83%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 92.86%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 93.06%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 93.24%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 93.42%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 93.59%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 93.90%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 94.05%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 94.04%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 93.89%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 93.89%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 94.02%   [EVAL] batch:   46 | acc: 75.00%,  total acc: 93.62%   [EVAL] batch:   47 | acc: 81.25%,  total acc: 93.36%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 93.49%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 93.38%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 93.38%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 93.27%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 93.28%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 93.40%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 93.30%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 93.30%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 93.31%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 93.21%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 93.33%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 93.33%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 93.44%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 93.45%   [EVAL] batch:   62 | acc: 68.75%,  total acc: 93.06%   [EVAL] batch:   63 | acc: 56.25%,  total acc: 92.48%   [EVAL] batch:   64 | acc: 62.50%,  total acc: 92.02%   [EVAL] batch:   65 | acc: 68.75%,  total acc: 91.67%   [EVAL] batch:   66 | acc: 68.75%,  total acc: 91.32%   [EVAL] batch:   67 | acc: 68.75%,  total acc: 90.99%   [EVAL] batch:   68 | acc: 68.75%,  total acc: 90.67%   [EVAL] batch:   69 | acc: 93.75%,  total acc: 90.71%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 90.85%   [EVAL] batch:   71 | acc: 68.75%,  total acc: 90.54%   [EVAL] batch:   72 | acc: 93.75%,  total acc: 90.58%   [EVAL] batch:   73 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 90.58%   [EVAL] batch:   75 | acc: 87.50%,  total acc: 90.54%   [EVAL] batch:   76 | acc: 87.50%,  total acc: 90.50%   [EVAL] batch:   77 | acc: 87.50%,  total acc: 90.46%   [EVAL] batch:   78 | acc: 93.75%,  total acc: 90.51%   [EVAL] batch:   79 | acc: 93.75%,  total acc: 90.55%   [EVAL] batch:   80 | acc: 100.00%,  total acc: 90.66%   [EVAL] batch:   81 | acc: 93.75%,  total acc: 90.70%   [EVAL] batch:   82 | acc: 75.00%,  total acc: 90.51%   [EVAL] batch:   83 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:   84 | acc: 81.25%,  total acc: 90.51%   [EVAL] batch:   85 | acc: 87.50%,  total acc: 90.48%   [EVAL] batch:   86 | acc: 81.25%,  total acc: 90.37%   [EVAL] batch:   87 | acc: 93.75%,  total acc: 90.41%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 90.52%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:   90 | acc: 100.00%,  total acc: 90.73%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 90.83%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 90.93%   [EVAL] batch:   93 | acc: 93.75%,  total acc: 90.96%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 91.05%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 91.15%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 91.24%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 91.33%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 91.41%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 91.50%   [EVAL] batch:  100 | acc: 93.75%,  total acc: 91.52%   [EVAL] batch:  101 | acc: 87.50%,  total acc: 91.48%   [EVAL] batch:  102 | acc: 68.75%,  total acc: 91.26%   [EVAL] batch:  103 | acc: 81.25%,  total acc: 91.17%   [EVAL] batch:  104 | acc: 93.75%,  total acc: 91.19%   [EVAL] batch:  105 | acc: 87.50%,  total acc: 91.16%   [EVAL] batch:  106 | acc: 62.50%,  total acc: 90.89%   [EVAL] batch:  107 | acc: 31.25%,  total acc: 90.34%   [EVAL] batch:  108 | acc: 50.00%,  total acc: 89.97%   [EVAL] batch:  109 | acc: 43.75%,  total acc: 89.55%   [EVAL] batch:  110 | acc: 68.75%,  total acc: 89.36%   [EVAL] batch:  111 | acc: 37.50%,  total acc: 88.90%   [EVAL] batch:  112 | acc: 75.00%,  total acc: 88.77%   [EVAL] batch:  113 | acc: 87.50%,  total acc: 88.76%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 88.86%   [EVAL] batch:  115 | acc: 87.50%,  total acc: 88.85%   [EVAL] batch:  116 | acc: 87.50%,  total acc: 88.84%   [EVAL] batch:  117 | acc: 93.75%,  total acc: 88.88%   [EVAL] batch:  118 | acc: 87.50%,  total acc: 88.87%   [EVAL] batch:  119 | acc: 81.25%,  total acc: 88.80%   [EVAL] batch:  120 | acc: 87.50%,  total acc: 88.79%   [EVAL] batch:  121 | acc: 68.75%,  total acc: 88.63%   [EVAL] batch:  122 | acc: 68.75%,  total acc: 88.47%   [EVAL] batch:  123 | acc: 81.25%,  total acc: 88.41%   [EVAL] batch:  124 | acc: 100.00%,  total acc: 88.50%   
cur_acc:  ['0.9524', '0.8284']
his_acc:  ['0.9524', '0.8850']
CurrentTrain: epoch 15, batch     0 | loss: 17.7388307CurrentTrain: epoch 15, batch     1 | loss: 14.1776062CurrentTrain: epoch 15, batch     2 | loss: 16.5714715CurrentTrain: epoch  1, batch     3 | loss: 9.4871950CurrentTrain: epoch 15, batch     0 | loss: 22.7567664CurrentTrain: epoch 15, batch     1 | loss: 11.3927194CurrentTrain: epoch 15, batch     2 | loss: 19.1287982CurrentTrain: epoch  1, batch     3 | loss: 9.7628378CurrentTrain: epoch 15, batch     0 | loss: 10.1329336CurrentTrain: epoch 15, batch     1 | loss: 16.0131113CurrentTrain: epoch 15, batch     2 | loss: 12.2101200CurrentTrain: epoch  1, batch     3 | loss: 9.3663673CurrentTrain: epoch 15, batch     0 | loss: 11.5979684CurrentTrain: epoch 15, batch     1 | loss: 8.8588714CurrentTrain: epoch 15, batch     2 | loss: 9.4744426CurrentTrain: epoch  1, batch     3 | loss: 19.3326644CurrentTrain: epoch 15, batch     0 | loss: 9.1387666CurrentTrain: epoch 15, batch     1 | loss: 18.5723346CurrentTrain: epoch 15, batch     2 | loss: 16.7473867CurrentTrain: epoch  1, batch     3 | loss: 7.5326794CurrentTrain: epoch 15, batch     0 | loss: 7.0591067CurrentTrain: epoch 15, batch     1 | loss: 21.0544109CurrentTrain: epoch 15, batch     2 | loss: 8.8017693CurrentTrain: epoch  1, batch     3 | loss: 6.4415647CurrentTrain: epoch 15, batch     0 | loss: 15.1952507CurrentTrain: epoch 15, batch     1 | loss: 15.0916608CurrentTrain: epoch 15, batch     2 | loss: 10.9669443CurrentTrain: epoch  1, batch     3 | loss: 6.6766614CurrentTrain: epoch 15, batch     0 | loss: 10.8797489CurrentTrain: epoch 15, batch     1 | loss: 7.6684898CurrentTrain: epoch 15, batch     2 | loss: 13.5040133CurrentTrain: epoch  1, batch     3 | loss: 6.6672134CurrentTrain: epoch 15, batch     0 | loss: 7.4933429CurrentTrain: epoch 15, batch     1 | loss: 13.4355768CurrentTrain: epoch 15, batch     2 | loss: 15.6474491CurrentTrain: epoch  1, batch     3 | loss: 6.2481642CurrentTrain: epoch 15, batch     0 | loss: 12.3094620CurrentTrain: epoch 15, batch     1 | loss: 8.8819515CurrentTrain: epoch 15, batch     2 | loss: 8.5930878CurrentTrain: epoch  1, batch     3 | loss: 6.2005700
#### spouse ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "spouse" means the subject has the object as their spouse (husband, wife, partner, etc.).
Here is an example:
Relation: spouse
Context: rohan is the son of narayana murthy , founder of infosys , and his wife sudha murthy , an engineer and author of note .
Head Entity: narayana murthy
Tail Entity: sudha murthy
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "spouse":
Relation: spouse  
Context: elon musk, the CEO of spacex and tesla, was previously married to talulah riley, a talented actress and writer.  
Head Entity: elon musk  
Tail Entity: talulah riley  

Relation: spouse  
Context: barack obama, the 44th president of the united states, has been married to michelle obama since 1992, and they have two daughters together.  
Head Entity: barack obama  
Tail Entity: michelle obama  
#### record label ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "record label" means brand and trademark associated with the marketing of subject music recordings and music videos.
Here is an example:
Relation: record label
Context: it was first released on a single in the uk by parlophone in september 1967 , and on the group 's self - titled album " tomorrow " in february 1968 .
Head Entity: tomorrow
Tail Entity: parlophone
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "record label":
Relation: record label  
Context: The band's latest album was distributed by Atlantic Records, showcasing their unique sound and artistic vision.  
Head Entity: Atlantic Records  
Tail Entity: band  

Relation: record label  
Context: After signing with Universal Music Group, the artist released a series of hit singles that topped the charts.  
Head Entity: Universal Music Group  
Tail Entity: artist  
#### distributor ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "distributor" means distributor of a creative work; distributor for a record label; news agency; film distributor.
Here is an example:
Relation: distributor
Context: distributed by paramount pictures , " the italian job " was theatrically released in the united states on may 30 , 2003 , and grossed over $ 176 million worldwide .
Head Entity: the italian job
Tail Entity: paramount pictures
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "distributor":
Relation: distributor  
Context: the film was distributed by universal pictures and received critical acclaim upon its release in 2015.  
Head Entity: the film  
Tail Entity: universal pictures  

Relation: distributor  
Context: the album was distributed by sony music and quickly climbed the charts, becoming a bestseller.  
Head Entity: the album  
Tail Entity: sony music  
#### located in or next to body of water ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "located in or next to body of water" means body of water on or next to which a place is located.
Here is an example:
Relation: located in or next to body of water
Context: in 1796 carondelet commissioned pintado to conduct a survey of lands granted to ambrosio longue - épée in iberville county on the west bank of the mississippi river opposite baton rouge .
Head Entity: baton rouge
Tail Entity: mississippi river
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "located in or next to body of water":
Relation: located in or next to body of water  
Context: the city of new orleans is situated on the banks of the mississippi river, making it a vibrant hub for trade and culture.  
Head Entity: new orleans  
Tail Entity: mississippi river  

Relation: located in or next to body of water  
Context: the picturesque village of lake placid is nestled beside the serene waters of lake placid, attracting tourists year-round.  
Head Entity: lake placid  
Tail Entity: lake placid  
#### subsidiary ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "subsidiary" means subsidiary of a company or organization; generally a fully owned separate corporation.
Here is an example:
Relation: subsidiary
Context: cbs and viacom split again in 2005 , but both are still owned by national amusements .
Head Entity: national amusements
Tail Entity: viacom
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "subsidiary":
Sample 1:  
Relation: subsidiary  
Context: In 2018, Facebook acquired WhatsApp, which now operates as a subsidiary under the social media giant.  
Head Entity: Facebook  
Tail Entity: WhatsApp  

Sample 2:  
Relation: subsidiary  
Context: Toyota has several subsidiaries, including Lexus, which focuses on luxury vehicles.  
Head Entity: Toyota  
Tail Entity: Lexus  
#### country of citizenship ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "country of citizenship" means the object is a country that recognizes the subject as its citizen.
Here is an example:
Relation: country of citizenship
Context: he challenged seven other swimmers in heat four including ukraine 's rostyslav svanidze , a top 16 finalist in atlanta four years earlier .
Head Entity: rostyslav svanidze
Tail Entity: ukraine
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "country of citizenship":
Relation: country of citizenship  
Context: after years of living in the united states, the renowned artist decided to apply for dual citizenship, allowing him to maintain his ties to his homeland, italy.  
Head Entity: the renowned artist  
Tail Entity: italy  

Relation: country of citizenship  
Context: during the international conference, the speaker proudly mentioned that she had recently obtained her citizenship from canada, where she had lived for over a decade.  
Head Entity: the speaker  
Tail Entity: canada  
#### genre ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "genre" means creative work's genre or an artist's field of work.
Here is an example:
Relation: genre
Context: nina cites " duffy , muse , and other pop and alternative artists " as her major influences .
Head Entity: duffy
Tail Entity: pop
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "genre":
Relation: genre  
Context: the film "inception" is often categorized as a science fiction thriller that explores complex themes of dreams and reality.  
Head Entity: inception  
Tail Entity: science fiction  

Relation: genre  
Context: beethoven's symphonies are celebrated for their profound emotional depth and are considered masterpieces of classical music.  
Head Entity: beethoven  
Tail Entity: classical music  
#### mouth of the watercourse ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "mouth of the watercourse" means the body of water to which the watercourse drains.
Here is an example:
Relation: mouth of the watercourse
Context: san lázaro is 660 km far from asunción and is located at the exact site of the confluence of the apa river with paraguay river , bordering brazil - chaco .
Head Entity: apa river
Tail Entity: paraguay river
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "mouth of the watercourse":
Relation: mouth of the watercourse  
Context: the mississippi river flows into the gulf of mexico, creating a rich delta ecosystem that supports diverse wildlife and plant species.  
Head Entity: mississippi river  
Tail Entity: gulf of mexico  

Relation: mouth of the watercourse  
Context: the rhine river, known for its scenic beauty, empties into the north sea, providing a vital shipping route for trade in europe.  
Head Entity: rhine river  
Tail Entity: north sea  
#### position played on team / speciality ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "position played on team / speciality" means position or specialism of a player on a team.
Here is an example:
Relation: position played on team / speciality
Context: bradley halliday ( born 10 july 1995 ) is an english professional footballer who plays as a defender or a midfielder for league two club cambridge united .
Head Entity: bradley halliday
Tail Entity: defender
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "position played on team / speciality":
Relation: position played on team / speciality  
Context: alex morgan ( born 2 july 1989 ) is an american soccer player who plays as a forward for the national team and club orlando pride in the national women's soccer league.  
Head Entity: alex morgan  
Tail Entity: forward  

Relation: position played on team / speciality  
Context: lebron james ( born 30 december 1984 ) is an american professional basketball player who plays as a small forward for the los angeles lakers in the nba.  
Head Entity: lebron james  
Tail Entity: small forward  
#### sports season of league or competition ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "sports season of league or competition" means property that shows the competition of which the item is a season.
Here is an example:
Relation: sports season of league or competition
Context: the stade louis ii in monaco was the venue for the uefa super cup every year since 1998 .
Head Entity: 1998
Tail Entity: uefa super cup
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "sports season of league or competition":
Relation: sports season of league or competition  
Context: The 2020 NBA season was significantly impacted by the COVID-19 pandemic, leading to a unique playoff format.  
Head Entity: 2020  
Tail Entity: NBA  

Relation: sports season of league or competition  
Context: The 2019 Rugby World Cup took place in Japan, showcasing teams from around the globe.  
Head Entity: 2019  
Tail Entity: Rugby World Cup  
MemoryTrain:  epoch 15, batch     0 | loss: 6.9901380MemoryTrain:  epoch 15, batch     1 | loss: 5.1563365MemoryTrain:  epoch 15, batch     2 | loss: 5.8418158MemoryTrain:  epoch 15, batch     3 | loss: 4.7784981MemoryTrain:  epoch 15, batch     4 | loss: 7.9686254MemoryTrain:  epoch  9, batch     5 | loss: 6.4925400MemoryTrain:  epoch 15, batch     0 | loss: 5.5330489MemoryTrain:  epoch 15, batch     1 | loss: 3.8080839MemoryTrain:  epoch 15, batch     2 | loss: 6.1732658MemoryTrain:  epoch 15, batch     3 | loss: 6.8059758MemoryTrain:  epoch 15, batch     4 | loss: 6.1711777MemoryTrain:  epoch  9, batch     5 | loss: 9.4282273MemoryTrain:  epoch 15, batch     0 | loss: 8.6581958MemoryTrain:  epoch 15, batch     1 | loss: 4.6664147MemoryTrain:  epoch 15, batch     2 | loss: 7.4495020MemoryTrain:  epoch 15, batch     3 | loss: 5.3566051MemoryTrain:  epoch 15, batch     4 | loss: 4.2310291MemoryTrain:  epoch  9, batch     5 | loss: 3.3934110MemoryTrain:  epoch 15, batch     0 | loss: 5.1508043MemoryTrain:  epoch 15, batch     1 | loss: 4.3971845MemoryTrain:  epoch 15, batch     2 | loss: 3.5166987MemoryTrain:  epoch 15, batch     3 | loss: 4.5147130MemoryTrain:  epoch 15, batch     4 | loss: 4.6289778MemoryTrain:  epoch  9, batch     5 | loss: 7.6589965MemoryTrain:  epoch 15, batch     0 | loss: 3.9911432MemoryTrain:  epoch 15, batch     1 | loss: 3.8631365MemoryTrain:  epoch 15, batch     2 | loss: 3.3222062MemoryTrain:  epoch 15, batch     3 | loss: 7.1440241MemoryTrain:  epoch 15, batch     4 | loss: 5.7532746MemoryTrain:  epoch  9, batch     5 | loss: 4.4019076MemoryTrain:  epoch 15, batch     0 | loss: 2.4362878MemoryTrain:  epoch 15, batch     1 | loss: 3.5680413MemoryTrain:  epoch 15, batch     2 | loss: 6.8891934MemoryTrain:  epoch 15, batch     3 | loss: 2.5219417MemoryTrain:  epoch 15, batch     4 | loss: 5.2308132MemoryTrain:  epoch  9, batch     5 | loss: 4.6260356MemoryTrain:  epoch 15, batch     0 | loss: 2.5041093MemoryTrain:  epoch 15, batch     1 | loss: 4.4564940MemoryTrain:  epoch 15, batch     2 | loss: 5.0207779MemoryTrain:  epoch 15, batch     3 | loss: 3.0441880MemoryTrain:  epoch 15, batch     4 | loss: 7.2061876MemoryTrain:  epoch  9, batch     5 | loss: 5.0801425MemoryTrain:  epoch 15, batch     0 | loss: 6.3227016MemoryTrain:  epoch 15, batch     1 | loss: 4.6536353MemoryTrain:  epoch 15, batch     2 | loss: 7.8514336MemoryTrain:  epoch 15, batch     3 | loss: 2.8286403MemoryTrain:  epoch 15, batch     4 | loss: 2.2748092MemoryTrain:  epoch  9, batch     5 | loss: 2.5589529MemoryTrain:  epoch 15, batch     0 | loss: 4.9635163MemoryTrain:  epoch 15, batch     1 | loss: 3.4673722MemoryTrain:  epoch 15, batch     2 | loss: 3.0438979MemoryTrain:  epoch 15, batch     3 | loss: 2.7235756MemoryTrain:  epoch 15, batch     4 | loss: 5.3659739MemoryTrain:  epoch  9, batch     5 | loss: 3.0022175MemoryTrain:  epoch 15, batch     0 | loss: 2.6174862MemoryTrain:  epoch 15, batch     1 | loss: 5.3796642MemoryTrain:  epoch 15, batch     2 | loss: 10.3953048MemoryTrain:  epoch 15, batch     3 | loss: 2.7737611MemoryTrain:  epoch 15, batch     4 | loss: 6.2166766MemoryTrain:  epoch  9, batch     5 | loss: 3.4846061
[EVAL] batch:    0 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 93.75%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 91.67%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 90.62%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 90.00%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 91.67%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 91.07%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 92.19%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 90.97%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 88.75%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 88.07%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 87.02%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 85.71%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 85.00%   [EVAL] batch:   15 | acc: 87.50%,  total acc: 85.16%   [EVAL] batch:   16 | acc: 81.25%,  total acc: 84.93%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 84.03%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 84.87%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 85.00%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 85.12%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 85.23%   [EVAL] batch:   22 | acc: 81.25%,  total acc: 85.05%   [EVAL] batch:   23 | acc: 81.25%,  total acc: 84.90%   [EVAL] batch:   24 | acc: 81.25%,  total acc: 84.75%   [EVAL] batch:   25 | acc: 87.50%,  total acc: 84.86%   [EVAL] batch:   26 | acc: 75.00%,  total acc: 84.49%   [EVAL] batch:   27 | acc: 81.25%,  total acc: 84.38%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 84.70%   [EVAL] batch:   29 | acc: 75.00%,  total acc: 84.38%   [EVAL] batch:   30 | acc: 62.50%,  total acc: 83.67%   [EVAL] batch:   31 | acc: 68.75%,  total acc: 83.20%   [EVAL] batch:   32 | acc: 56.25%,  total acc: 82.39%   [EVAL] batch:   33 | acc: 62.50%,  total acc: 81.80%   [EVAL] batch:   34 | acc: 62.50%,  total acc: 81.25%   [EVAL] batch:   35 | acc: 75.00%,  total acc: 81.08%   [EVAL] batch:   36 | acc: 62.50%,  total acc: 80.57%   [EVAL] batch:   37 | acc: 75.00%,  total acc: 80.43%   [EVAL] batch:   38 | acc: 68.75%,  total acc: 80.13%   [EVAL] batch:   39 | acc: 81.25%,  total acc: 80.16%   [EVAL] batch:   40 | acc: 75.00%,  total acc: 80.03%   [EVAL] batch:   41 | acc: 68.75%,  total acc: 79.76%   [EVAL] batch:   42 | acc: 87.50%,  total acc: 79.94%   [EVAL] batch:   43 | acc: 62.50%,  total acc: 79.55%   [EVAL] batch:   44 | acc: 25.00%,  total acc: 78.33%   [EVAL] batch:   45 | acc: 31.25%,  total acc: 77.31%   [EVAL] batch:   46 | acc: 12.50%,  total acc: 75.93%   [EVAL] batch:   47 | acc: 37.50%,  total acc: 75.13%   [EVAL] batch:   48 | acc: 18.75%,  total acc: 73.98%   [EVAL] batch:   49 | acc: 50.00%,  total acc: 73.50%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 74.02%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 74.52%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 75.46%   [EVAL] batch:   54 | acc: 100.00%,  total acc: 75.91%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 76.34%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 76.75%   [EVAL] batch:   57 | acc: 100.00%,  total acc: 77.16%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 77.54%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 77.92%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 78.28%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 78.53%   [EVAL] batch:   62 | acc: 43.75%,  total acc: 77.98%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 85.94%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 85.00%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 85.42%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 84.82%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 85.16%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 86.11%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 86.25%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 85.80%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 86.98%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 87.02%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 87.92%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 88.67%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 88.97%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 89.24%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 89.80%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 90.00%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 90.48%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 90.06%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 90.22%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 90.36%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 90.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 90.97%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 91.29%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 91.59%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 91.88%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 92.14%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 92.19%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 92.42%   [EVAL] batch:   33 | acc: 75.00%,  total acc: 91.91%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 91.43%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 91.32%   [EVAL] batch:   36 | acc: 93.75%,  total acc: 91.39%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 91.45%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 91.67%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 91.88%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 92.07%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 92.26%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 92.30%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 92.33%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 92.36%   [EVAL] batch:   45 | acc: 93.75%,  total acc: 92.39%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 92.15%   [EVAL] batch:   47 | acc: 81.25%,  total acc: 91.93%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 92.09%   [EVAL] batch:   49 | acc: 81.25%,  total acc: 91.88%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 91.91%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 91.83%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 91.86%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 92.01%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 91.93%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 91.96%   [EVAL] batch:   56 | acc: 87.50%,  total acc: 91.89%   [EVAL] batch:   57 | acc: 75.00%,  total acc: 91.59%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 91.63%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:   60 | acc: 93.75%,  total acc: 91.70%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 91.63%   [EVAL] batch:   62 | acc: 75.00%,  total acc: 91.37%   [EVAL] batch:   63 | acc: 50.00%,  total acc: 90.72%   [EVAL] batch:   64 | acc: 62.50%,  total acc: 90.29%   [EVAL] batch:   65 | acc: 62.50%,  total acc: 89.87%   [EVAL] batch:   66 | acc: 50.00%,  total acc: 89.27%   [EVAL] batch:   67 | acc: 68.75%,  total acc: 88.97%   [EVAL] batch:   68 | acc: 62.50%,  total acc: 88.59%   [EVAL] batch:   69 | acc: 81.25%,  total acc: 88.48%   [EVAL] batch:   70 | acc: 68.75%,  total acc: 88.20%   [EVAL] batch:   71 | acc: 56.25%,  total acc: 87.76%   [EVAL] batch:   72 | acc: 81.25%,  total acc: 87.67%   [EVAL] batch:   73 | acc: 87.50%,  total acc: 87.67%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 87.67%   [EVAL] batch:   75 | acc: 81.25%,  total acc: 87.58%   [EVAL] batch:   76 | acc: 75.00%,  total acc: 87.42%   [EVAL] batch:   77 | acc: 81.25%,  total acc: 87.34%   [EVAL] batch:   78 | acc: 81.25%,  total acc: 87.26%   [EVAL] batch:   79 | acc: 75.00%,  total acc: 87.11%   [EVAL] batch:   80 | acc: 93.75%,  total acc: 87.19%   [EVAL] batch:   81 | acc: 62.50%,  total acc: 86.89%   [EVAL] batch:   82 | acc: 75.00%,  total acc: 86.75%   [EVAL] batch:   83 | acc: 75.00%,  total acc: 86.61%   [EVAL] batch:   84 | acc: 75.00%,  total acc: 86.47%   [EVAL] batch:   85 | acc: 56.25%,  total acc: 86.12%   [EVAL] batch:   86 | acc: 81.25%,  total acc: 86.06%   [EVAL] batch:   87 | acc: 68.75%,  total acc: 85.87%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 86.03%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 86.18%   [EVAL] batch:   90 | acc: 100.00%,  total acc: 86.33%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 86.48%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 86.63%   [EVAL] batch:   93 | acc: 93.75%,  total acc: 86.70%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 86.84%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 86.98%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 87.11%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 87.24%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 87.37%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:  100 | acc: 93.75%,  total acc: 87.56%   [EVAL] batch:  101 | acc: 93.75%,  total acc: 87.62%   [EVAL] batch:  102 | acc: 75.00%,  total acc: 87.50%   [EVAL] batch:  103 | acc: 81.25%,  total acc: 87.44%   [EVAL] batch:  104 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:  105 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:  106 | acc: 62.50%,  total acc: 87.27%   [EVAL] batch:  107 | acc: 43.75%,  total acc: 86.86%   [EVAL] batch:  108 | acc: 68.75%,  total acc: 86.70%   [EVAL] batch:  109 | acc: 56.25%,  total acc: 86.42%   [EVAL] batch:  110 | acc: 62.50%,  total acc: 86.20%   [EVAL] batch:  111 | acc: 56.25%,  total acc: 85.94%   [EVAL] batch:  112 | acc: 87.50%,  total acc: 85.95%   [EVAL] batch:  113 | acc: 87.50%,  total acc: 85.96%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 86.09%   [EVAL] batch:  115 | acc: 87.50%,  total acc: 86.10%   [EVAL] batch:  116 | acc: 93.75%,  total acc: 86.16%   [EVAL] batch:  117 | acc: 93.75%,  total acc: 86.23%   [EVAL] batch:  118 | acc: 81.25%,  total acc: 86.19%   [EVAL] batch:  119 | acc: 50.00%,  total acc: 85.89%   [EVAL] batch:  120 | acc: 37.50%,  total acc: 85.49%   [EVAL] batch:  121 | acc: 18.75%,  total acc: 84.94%   [EVAL] batch:  122 | acc: 43.75%,  total acc: 84.60%   [EVAL] batch:  123 | acc: 31.25%,  total acc: 84.17%   [EVAL] batch:  124 | acc: 56.25%,  total acc: 83.95%   [EVAL] batch:  125 | acc: 100.00%,  total acc: 84.08%   [EVAL] batch:  126 | acc: 87.50%,  total acc: 84.10%   [EVAL] batch:  127 | acc: 87.50%,  total acc: 84.13%   [EVAL] batch:  128 | acc: 87.50%,  total acc: 84.16%   [EVAL] batch:  129 | acc: 87.50%,  total acc: 84.18%   [EVAL] batch:  130 | acc: 100.00%,  total acc: 84.30%   [EVAL] batch:  131 | acc: 87.50%,  total acc: 84.33%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 84.45%   [EVAL] batch:  133 | acc: 81.25%,  total acc: 84.42%   [EVAL] batch:  134 | acc: 68.75%,  total acc: 84.31%   [EVAL] batch:  135 | acc: 81.25%,  total acc: 84.28%   [EVAL] batch:  136 | acc: 81.25%,  total acc: 84.26%   [EVAL] batch:  137 | acc: 81.25%,  total acc: 84.24%   [EVAL] batch:  138 | acc: 68.75%,  total acc: 84.13%   [EVAL] batch:  139 | acc: 75.00%,  total acc: 84.06%   [EVAL] batch:  140 | acc: 87.50%,  total acc: 84.09%   [EVAL] batch:  141 | acc: 81.25%,  total acc: 84.07%   [EVAL] batch:  142 | acc: 68.75%,  total acc: 83.96%   [EVAL] batch:  143 | acc: 100.00%,  total acc: 84.07%   [EVAL] batch:  144 | acc: 87.50%,  total acc: 84.09%   [EVAL] batch:  145 | acc: 87.50%,  total acc: 84.12%   [EVAL] batch:  146 | acc: 87.50%,  total acc: 84.14%   [EVAL] batch:  147 | acc: 81.25%,  total acc: 84.12%   [EVAL] batch:  148 | acc: 81.25%,  total acc: 84.10%   [EVAL] batch:  149 | acc: 81.25%,  total acc: 84.08%   [EVAL] batch:  150 | acc: 87.50%,  total acc: 84.11%   [EVAL] batch:  151 | acc: 75.00%,  total acc: 84.05%   [EVAL] batch:  152 | acc: 81.25%,  total acc: 84.03%   [EVAL] batch:  153 | acc: 93.75%,  total acc: 84.09%   [EVAL] batch:  154 | acc: 75.00%,  total acc: 84.03%   [EVAL] batch:  155 | acc: 62.50%,  total acc: 83.89%   [EVAL] batch:  156 | acc: 68.75%,  total acc: 83.80%   [EVAL] batch:  157 | acc: 56.25%,  total acc: 83.62%   [EVAL] batch:  158 | acc: 62.50%,  total acc: 83.49%   [EVAL] batch:  159 | acc: 62.50%,  total acc: 83.36%   [EVAL] batch:  160 | acc: 75.00%,  total acc: 83.31%   [EVAL] batch:  161 | acc: 62.50%,  total acc: 83.18%   [EVAL] batch:  162 | acc: 75.00%,  total acc: 83.13%   [EVAL] batch:  163 | acc: 68.75%,  total acc: 83.04%   [EVAL] batch:  164 | acc: 81.25%,  total acc: 83.03%   [EVAL] batch:  165 | acc: 75.00%,  total acc: 82.98%   [EVAL] batch:  166 | acc: 68.75%,  total acc: 82.90%   [EVAL] batch:  167 | acc: 87.50%,  total acc: 82.92%   [EVAL] batch:  168 | acc: 62.50%,  total acc: 82.80%   [EVAL] batch:  169 | acc: 25.00%,  total acc: 82.46%   [EVAL] batch:  170 | acc: 31.25%,  total acc: 82.16%   [EVAL] batch:  171 | acc: 12.50%,  total acc: 81.76%   [EVAL] batch:  172 | acc: 37.50%,  total acc: 81.50%   [EVAL] batch:  173 | acc: 18.75%,  total acc: 81.14%   [EVAL] batch:  174 | acc: 50.00%,  total acc: 80.96%   [EVAL] batch:  175 | acc: 100.00%,  total acc: 81.07%   [EVAL] batch:  176 | acc: 100.00%,  total acc: 81.18%   [EVAL] batch:  177 | acc: 100.00%,  total acc: 81.29%   [EVAL] batch:  178 | acc: 100.00%,  total acc: 81.39%   [EVAL] batch:  179 | acc: 100.00%,  total acc: 81.49%   [EVAL] batch:  180 | acc: 100.00%,  total acc: 81.60%   [EVAL] batch:  181 | acc: 100.00%,  total acc: 81.70%   [EVAL] batch:  182 | acc: 100.00%,  total acc: 81.80%   [EVAL] batch:  183 | acc: 100.00%,  total acc: 81.90%   [EVAL] batch:  184 | acc: 100.00%,  total acc: 81.99%   [EVAL] batch:  185 | acc: 100.00%,  total acc: 82.09%   [EVAL] batch:  186 | acc: 93.75%,  total acc: 82.15%   [EVAL] batch:  187 | acc: 43.75%,  total acc: 81.95%   
cur_acc:  ['0.9524', '0.8284', '0.7798']
his_acc:  ['0.9524', '0.8850', '0.8195']
CurrentTrain: epoch 15, batch     0 | loss: 20.1755644CurrentTrain: epoch 15, batch     1 | loss: 24.5415685CurrentTrain: epoch 15, batch     2 | loss: 14.1772165CurrentTrain: epoch  1, batch     3 | loss: 8.2583316CurrentTrain: epoch 15, batch     0 | loss: 16.7685530CurrentTrain: epoch 15, batch     1 | loss: 14.2061854CurrentTrain: epoch 15, batch     2 | loss: 11.7489416CurrentTrain: epoch  1, batch     3 | loss: 6.9496724CurrentTrain: epoch 15, batch     0 | loss: 16.0694801CurrentTrain: epoch 15, batch     1 | loss: 18.4674819CurrentTrain: epoch 15, batch     2 | loss: 11.0519150CurrentTrain: epoch  1, batch     3 | loss: 7.4117312CurrentTrain: epoch 15, batch     0 | loss: 13.4991469CurrentTrain: epoch 15, batch     1 | loss: 13.1133785CurrentTrain: epoch 15, batch     2 | loss: 12.8837984CurrentTrain: epoch  1, batch     3 | loss: 7.7849257CurrentTrain: epoch 15, batch     0 | loss: 12.7332174CurrentTrain: epoch 15, batch     1 | loss: 19.6420478CurrentTrain: epoch 15, batch     2 | loss: 12.1466204CurrentTrain: epoch  1, batch     3 | loss: 8.6586964CurrentTrain: epoch 15, batch     0 | loss: 14.6511869CurrentTrain: epoch 15, batch     1 | loss: 11.5287338CurrentTrain: epoch 15, batch     2 | loss: 12.5200332CurrentTrain: epoch  1, batch     3 | loss: 7.6876496CurrentTrain: epoch 15, batch     0 | loss: 10.5414542CurrentTrain: epoch 15, batch     1 | loss: 10.5146553CurrentTrain: epoch 15, batch     2 | loss: 11.3093147CurrentTrain: epoch  1, batch     3 | loss: 7.6654728CurrentTrain: epoch 15, batch     0 | loss: 8.8645002CurrentTrain: epoch 15, batch     1 | loss: 14.6062753CurrentTrain: epoch 15, batch     2 | loss: 15.1528897CurrentTrain: epoch  1, batch     3 | loss: 8.0401784CurrentTrain: epoch 15, batch     0 | loss: 10.4841498CurrentTrain: epoch 15, batch     1 | loss: 12.4983747CurrentTrain: epoch 15, batch     2 | loss: 8.6324169CurrentTrain: epoch  1, batch     3 | loss: 7.4142874CurrentTrain: epoch 15, batch     0 | loss: 10.0593396CurrentTrain: epoch 15, batch     1 | loss: 12.8706627CurrentTrain: epoch 15, batch     2 | loss: 15.0991337CurrentTrain: epoch  1, batch     3 | loss: 6.5106224
#### follows ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "follows" means immediately prior item in a series of which the subject is a part.
Here is an example:
Relation: follows
Context: ( 1988 ) , " night watch " ( 2004 ) , " day watch " ( 2006 ) and " legend № 17 " ( 2013 ) .
Head Entity: day watch
Tail Entity: night watch
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "follows":
Relation: follows  
Context: ( 1995 ) , " The Phantom Menace " ( 1999 ) , " Attack of the Clones " ( 2002 ) and " Revenge of the Sith " ( 2005 ) .  
Head Entity: Attack of the Clones  
Tail Entity: The Phantom Menace  

Relation: follows  
Context: ( 2010 ) , " The Hunger Games " ( 2012 ) , " Catching Fire " ( 2013 ) and " Mockingjay " ( 2014 ) .  
Head Entity: Catching Fire  
Tail Entity: The Hunger Games  
#### sport ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "sport" means sport that the subject participates or participated in or is associated with.
Here is an example:
Relation: sport
Context: it is primarily used for baseball and serves as the home stadium for yaquis de obregón .
Head Entity: yaquis de obregón
Tail Entity: baseball
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "sport":
Relation: sport  
Context: she has been a professional athlete in tennis since she was a teenager, competing in various international tournaments.  
Head Entity: she  
Tail Entity: tennis  

Relation: sport  
Context: the city is known for its vibrant soccer culture, with several teams competing in the national league.  
Head Entity: the city  
Tail Entity: soccer  
#### father ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "father" means male parent of the subject.
Here is an example:
Relation: father
Context: after the crowning of his nephew radoslav , the son of stefan , sava left the serbian maritime in 1229 for a trip to palestine .
Head Entity: radoslav
Tail Entity: stefan
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "father":
Relation: father  
Context: in the historical records, it is noted that king henry was the father of prince edward, who later became a significant figure in the kingdom.  
Head Entity: prince edward  
Tail Entity: king henry  

Relation: father  
Context: during the family reunion, it was revealed that michael, the youngest son, was the father of three children, making him the proud dad of the next generation.  
Head Entity: michael  
Tail Entity: unknown father (context does not specify, but implies a paternal relationship)  
#### original network ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "original network" means network or service that originally broadcast a radio or television program.
Here is an example:
Relation: original network
Context: since 2009 , culshaw has starred in the bbc one comedy sketch show " the impressions show " alongside debra stephenson .
Head Entity: the impressions show
Tail Entity: bbc one
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "original network":
Relation: original network  
Context: The popular series "Breaking Bad" first aired on AMC, captivating audiences with its intense storytelling and character development.  
Head Entity: Breaking Bad  
Tail Entity: AMC  

Relation: original network  
Context: "Friends" became a cultural phenomenon when it premiered on NBC, showcasing the lives and relationships of six friends living in New York City.  
Head Entity: Friends  
Tail Entity: NBC  
#### original language of film or TV show ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "original language of film or TV show" means language in which a film or a performance work was originally created.
Here is an example:
Relation: original language of film or TV show
Context: he was also nominated twice for the filmfare best telugu actor award , for the films bhale bhale magadivoy and " gentleman " .
Head Entity: gentleman
Tail Entity: telugu
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "original language of film or TV show":
Relation: original language of film or TV show  
Context: The movie "Parasite" received critical acclaim and was originally produced in Korean, showcasing the talents of its director Bong Joon-ho.  
Head Entity: Parasite  
Tail Entity: Korean  

Relation: original language of film or TV show  
Context: The animated series "Doraemon" has been beloved by children and adults alike, and it was first aired in Japanese, capturing the hearts of its audience.  
Head Entity: Doraemon  
Tail Entity: Japanese  
#### league ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "league" means league in which team or player plays or has played in.
Here is an example:
Relation: league
Context: dungannon swifts football club is a northern irish , semi - professional football club playing in the nifl premiership .
Head Entity: dungannon swifts football club
Tail Entity: nifl premiership
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "league":
Relation: league  
Context: the los angeles lakers are a professional basketball team based in los angeles , california , and they compete in the national basketball association (nba) .  
Head Entity: los angeles lakers  
Tail Entity: national basketball association  

Relation: league  
Context: manchester united football club is one of the most successful clubs in english football history and plays in the premier league .  
Head Entity: manchester united football club  
Tail Entity: premier league  
#### mother ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "mother" means female parent of the subject.
Here is an example:
Relation: mother
Context: she is supposedly the third and youngest child of seti i and tuya , and the younger sister of ramesses ii and tia .
Head Entity: tia
Tail Entity: tuya
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "mother":
Relation: mother  
Context: Cleopatra was the daughter of Ptolemy XII and his wife, who was also her mother, Cleopatra VI.  
Head Entity: Cleopatra  
Tail Entity: Cleopatra VI  

Relation: mother  
Context: In Norse mythology, Frigg is known as the mother of Baldr, the god of light and purity.  
Head Entity: Baldr  
Tail Entity: Frigg  
#### instrument ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "instrument" means musical instrument that a person plays or teaches or used in a music occupation.
Here is an example:
Relation: instrument
Context: peter bastian ( 25 august 1943 in copenhagen – 27 march 2017 ) was a danish musician . peter bastian bio and cv he was educated in theoretical physics and classical bassoon .
Head Entity: peter bastian
Tail Entity: bassoon
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "instrument":
Relation: instrument  
Context: johann sebastian bach (31 march 1685 – 28 july 1750) was a german composer and musician of the baroque period. he was known for his exceptional skill in playing the organ and the harpsichord.  
Head Entity: johann sebastian bach  
Tail Entity: organ  

Relation: instrument  
Context: yo-yo ma, born on october 7, 1955, is a world-renowned cellist who has performed with numerous orchestras and in various genres. he is particularly famous for his interpretations of classical cello concertos.  
Head Entity: yo-yo ma  
Tail Entity: cello  
#### country ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "country" means sovereign state of this item (not to be used for human beings).
Here is an example:
Relation: country
Context: ploegsteert is a village in belgium located in the municipality of comines - warneton in the hainaut province and is the most westerly settlement of the walloon region .
Head Entity: hainaut
Tail Entity: belgium
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "country":
Relation: country  
Context: the city of prague is the capital of the czech republic and is known for its beautiful architecture and rich history.  
Head Entity: prague  
Tail Entity: czech republic  

Relation: country  
Context: the great barrier reef is located off the coast of australia and is the largest coral reef system in the world.  
Head Entity: great barrier reef  
Tail Entity: australia  
#### characters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "characters" means characters which appear in this item (like plays, operas, operettas, books, comics, films, TV series, video games).
Here is an example:
Relation: characters
Context: klaus kinski , who would play dracula himself nine years later in " nosferatu the vampyre " , is also featured in the film as renfield .
Head Entity: nosferatu the vampyre
Tail Entity: renfield
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "characters":
Relation: characters  
Context: in the animated series "avatar: the last airbender," zuko is a complex character who struggles with his identity and family legacy.  
Head Entity: avatar: the last airbender  
Tail Entity: zuko  

Relation: characters  
Context: the novel "pride and prejudice" features elizabeth bennet, a strong-willed protagonist who navigates societal expectations and personal relationships.  
Head Entity: pride and prejudice  
Tail Entity: elizabeth bennet  
MemoryTrain:  epoch 15, batch     0 | loss: 4.2007112MemoryTrain:  epoch 15, batch     1 | loss: 3.5126400MemoryTrain:  epoch 15, batch     2 | loss: 5.8989209MemoryTrain:  epoch 15, batch     3 | loss: 4.3851085MemoryTrain:  epoch 15, batch     4 | loss: 4.9562173MemoryTrain:  epoch 15, batch     5 | loss: 4.0662239MemoryTrain:  epoch 15, batch     6 | loss: 5.6699805MemoryTrain:  epoch  7, batch     7 | loss: 3.8786771MemoryTrain:  epoch 15, batch     0 | loss: 3.5235827MemoryTrain:  epoch 15, batch     1 | loss: 4.0962933MemoryTrain:  epoch 15, batch     2 | loss: 5.7627352MemoryTrain:  epoch 15, batch     3 | loss: 4.1148963MemoryTrain:  epoch 15, batch     4 | loss: 4.5571246MemoryTrain:  epoch 15, batch     5 | loss: 6.3825122MemoryTrain:  epoch 15, batch     6 | loss: 3.6693931MemoryTrain:  epoch  7, batch     7 | loss: 4.8369326MemoryTrain:  epoch 15, batch     0 | loss: 8.5049740MemoryTrain:  epoch 15, batch     1 | loss: 3.7407150MemoryTrain:  epoch 15, batch     2 | loss: 5.2217613MemoryTrain:  epoch 15, batch     3 | loss: 3.0256691MemoryTrain:  epoch 15, batch     4 | loss: 4.2359062MemoryTrain:  epoch 15, batch     5 | loss: 3.4196567MemoryTrain:  epoch 15, batch     6 | loss: 5.0657595MemoryTrain:  epoch  7, batch     7 | loss: 1.9851498MemoryTrain:  epoch 15, batch     0 | loss: 2.3719997MemoryTrain:  epoch 15, batch     1 | loss: 2.5173472MemoryTrain:  epoch 15, batch     2 | loss: 5.6957356MemoryTrain:  epoch 15, batch     3 | loss: 2.7841643MemoryTrain:  epoch 15, batch     4 | loss: 2.7975365MemoryTrain:  epoch 15, batch     5 | loss: 2.6592449MemoryTrain:  epoch 15, batch     6 | loss: 5.5108884MemoryTrain:  epoch  7, batch     7 | loss: 1.6710719MemoryTrain:  epoch 15, batch     0 | loss: 3.0149707MemoryTrain:  epoch 15, batch     1 | loss: 4.7144354MemoryTrain:  epoch 15, batch     2 | loss: 3.0182281MemoryTrain:  epoch 15, batch     3 | loss: 2.8434452MemoryTrain:  epoch 15, batch     4 | loss: 2.2829647MemoryTrain:  epoch 15, batch     5 | loss: 5.5504803MemoryTrain:  epoch 15, batch     6 | loss: 9.1491802MemoryTrain:  epoch  7, batch     7 | loss: 6.4367569MemoryTrain:  epoch 15, batch     0 | loss: 2.6021585MemoryTrain:  epoch 15, batch     1 | loss: 1.6671556MemoryTrain:  epoch 15, batch     2 | loss: 6.9455740MemoryTrain:  epoch 15, batch     3 | loss: 2.2451729MemoryTrain:  epoch 15, batch     4 | loss: 2.5885879MemoryTrain:  epoch 15, batch     5 | loss: 2.0366348MemoryTrain:  epoch 15, batch     6 | loss: 2.8016056MemoryTrain:  epoch  7, batch     7 | loss: 10.1298202MemoryTrain:  epoch 15, batch     0 | loss: 2.0411435MemoryTrain:  epoch 15, batch     1 | loss: 2.2731281MemoryTrain:  epoch 15, batch     2 | loss: 2.5409736MemoryTrain:  epoch 15, batch     3 | loss: 4.3370264MemoryTrain:  epoch 15, batch     4 | loss: 3.1766888MemoryTrain:  epoch 15, batch     5 | loss: 2.2158529MemoryTrain:  epoch 15, batch     6 | loss: 3.7155514MemoryTrain:  epoch  7, batch     7 | loss: 4.6614931MemoryTrain:  epoch 15, batch     0 | loss: 5.3847110MemoryTrain:  epoch 15, batch     1 | loss: 1.9989464MemoryTrain:  epoch 15, batch     2 | loss: 4.6051650MemoryTrain:  epoch 15, batch     3 | loss: 4.8301958MemoryTrain:  epoch 15, batch     4 | loss: 3.6856703MemoryTrain:  epoch 15, batch     5 | loss: 3.6149373MemoryTrain:  epoch 15, batch     6 | loss: 2.2244828MemoryTrain:  epoch  7, batch     7 | loss: 1.4960351MemoryTrain:  epoch 15, batch     0 | loss: 2.0846069MemoryTrain:  epoch 15, batch     1 | loss: 3.9234707MemoryTrain:  epoch 15, batch     2 | loss: 2.8588064MemoryTrain:  epoch 15, batch     3 | loss: 5.8044489MemoryTrain:  epoch 15, batch     4 | loss: 2.2823215MemoryTrain:  epoch 15, batch     5 | loss: 4.2029523MemoryTrain:  epoch 15, batch     6 | loss: 2.7250420MemoryTrain:  epoch  7, batch     7 | loss: 4.2733135MemoryTrain:  epoch 15, batch     0 | loss: 1.6627845MemoryTrain:  epoch 15, batch     1 | loss: 6.6135175MemoryTrain:  epoch 15, batch     2 | loss: 1.8473216MemoryTrain:  epoch 15, batch     3 | loss: 1.5209647MemoryTrain:  epoch 15, batch     4 | loss: 2.1463502MemoryTrain:  epoch 15, batch     5 | loss: 5.4692556MemoryTrain:  epoch 15, batch     6 | loss: 2.1941210MemoryTrain:  epoch  7, batch     7 | loss: 2.7916442
[EVAL] batch:    0 | acc: 25.00%,  total acc: 25.00%   [EVAL] batch:    1 | acc: 31.25%,  total acc: 28.12%   [EVAL] batch:    2 | acc: 18.75%,  total acc: 25.00%   [EVAL] batch:    3 | acc: 31.25%,  total acc: 26.56%   [EVAL] batch:    4 | acc: 6.25%,  total acc: 22.50%   [EVAL] batch:    5 | acc: 25.00%,  total acc: 22.92%   [EVAL] batch:    6 | acc: 37.50%,  total acc: 25.00%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 31.25%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 35.42%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 39.38%   [EVAL] batch:   10 | acc: 68.75%,  total acc: 42.05%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 45.31%   [EVAL] batch:   12 | acc: 62.50%,  total acc: 46.63%   [EVAL] batch:   13 | acc: 43.75%,  total acc: 46.43%   [EVAL] batch:   14 | acc: 50.00%,  total acc: 46.67%   [EVAL] batch:   15 | acc: 25.00%,  total acc: 45.31%   [EVAL] batch:   16 | acc: 43.75%,  total acc: 45.22%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 46.53%   [EVAL] batch:   18 | acc: 56.25%,  total acc: 47.04%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 49.69%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 52.08%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 53.98%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 55.98%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 57.55%   [EVAL] batch:   24 | acc: 81.25%,  total acc: 58.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 60.10%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 61.57%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 62.95%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 64.22%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 65.42%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 66.53%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 67.58%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 68.56%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 69.30%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 70.00%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 70.66%   [EVAL] batch:   36 | acc: 93.75%,  total acc: 71.28%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 71.88%   [EVAL] batch:   38 | acc: 68.75%,  total acc: 71.79%   [EVAL] batch:   39 | acc: 68.75%,  total acc: 71.72%   [EVAL] batch:   40 | acc: 62.50%,  total acc: 71.49%   [EVAL] batch:   41 | acc: 43.75%,  total acc: 70.83%   [EVAL] batch:   42 | acc: 37.50%,  total acc: 70.06%   [EVAL] batch:   43 | acc: 75.00%,  total acc: 70.17%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 70.69%   [EVAL] batch:   45 | acc: 87.50%,  total acc: 71.06%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 71.68%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 72.27%   [EVAL] batch:   48 | acc: 93.75%,  total acc: 72.70%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 73.00%   [EVAL] batch:   50 | acc: 37.50%,  total acc: 72.30%   [EVAL] batch:   51 | acc: 62.50%,  total acc: 72.12%   [EVAL] batch:   52 | acc: 68.75%,  total acc: 72.05%   [EVAL] batch:   53 | acc: 68.75%,  total acc: 71.99%   [EVAL] batch:   54 | acc: 62.50%,  total acc: 71.82%   [EVAL] batch:   55 | acc: 31.25%,  total acc: 71.09%   [EVAL] batch:   56 | acc: 68.75%,  total acc: 71.05%   [EVAL] batch:   57 | acc: 81.25%,  total acc: 71.23%   [EVAL] batch:   58 | acc: 75.00%,  total acc: 71.29%   [EVAL] batch:   59 | acc: 87.50%,  total acc: 71.56%   [EVAL] batch:   60 | acc: 93.75%,  total acc: 71.93%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 72.38%   [EVAL] batch:   62 | acc: 43.75%,  total acc: 71.92%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 82.81%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 82.50%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 83.04%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 83.59%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 84.03%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 83.12%   [EVAL] batch:   10 | acc: 68.75%,  total acc: 81.82%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 82.81%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 83.17%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 83.93%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 84.58%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 85.55%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 86.40%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 86.81%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 87.17%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 86.56%   [EVAL] batch:   20 | acc: 81.25%,  total acc: 86.31%   [EVAL] batch:   21 | acc: 75.00%,  total acc: 85.80%   [EVAL] batch:   22 | acc: 81.25%,  total acc: 85.60%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 85.68%   [EVAL] batch:   24 | acc: 81.25%,  total acc: 85.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 86.06%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 86.57%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 87.05%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 87.92%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 88.31%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 88.48%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 88.83%   [EVAL] batch:   33 | acc: 62.50%,  total acc: 88.05%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 87.68%   [EVAL] batch:   35 | acc: 75.00%,  total acc: 87.33%   [EVAL] batch:   36 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 87.66%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 87.98%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 88.28%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 88.57%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 88.84%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 88.95%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 88.92%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 89.03%   [EVAL] batch:   45 | acc: 93.75%,  total acc: 89.13%   [EVAL] batch:   46 | acc: 75.00%,  total acc: 88.83%   [EVAL] batch:   47 | acc: 81.25%,  total acc: 88.67%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 88.90%   [EVAL] batch:   49 | acc: 81.25%,  total acc: 88.75%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 88.85%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 88.82%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 88.92%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 89.12%   [EVAL] batch:   54 | acc: 75.00%,  total acc: 88.86%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 88.95%   [EVAL] batch:   56 | acc: 81.25%,  total acc: 88.82%   [EVAL] batch:   57 | acc: 43.75%,  total acc: 88.04%   [EVAL] batch:   58 | acc: 56.25%,  total acc: 87.50%   [EVAL] batch:   59 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:   60 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:   61 | acc: 75.00%,  total acc: 87.30%   [EVAL] batch:   62 | acc: 75.00%,  total acc: 87.10%   [EVAL] batch:   63 | acc: 37.50%,  total acc: 86.33%   [EVAL] batch:   64 | acc: 56.25%,  total acc: 85.87%   [EVAL] batch:   65 | acc: 56.25%,  total acc: 85.42%   [EVAL] batch:   66 | acc: 37.50%,  total acc: 84.70%   [EVAL] batch:   67 | acc: 75.00%,  total acc: 84.56%   [EVAL] batch:   68 | acc: 56.25%,  total acc: 84.15%   [EVAL] batch:   69 | acc: 81.25%,  total acc: 84.11%   [EVAL] batch:   70 | acc: 62.50%,  total acc: 83.80%   [EVAL] batch:   71 | acc: 56.25%,  total acc: 83.42%   [EVAL] batch:   72 | acc: 75.00%,  total acc: 83.30%   [EVAL] batch:   73 | acc: 81.25%,  total acc: 83.28%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:   75 | acc: 87.50%,  total acc: 83.39%   [EVAL] batch:   76 | acc: 81.25%,  total acc: 83.36%   [EVAL] batch:   77 | acc: 81.25%,  total acc: 83.33%   [EVAL] batch:   78 | acc: 87.50%,  total acc: 83.39%   [EVAL] batch:   79 | acc: 75.00%,  total acc: 83.28%   [EVAL] batch:   80 | acc: 93.75%,  total acc: 83.41%   [EVAL] batch:   81 | acc: 31.25%,  total acc: 82.77%   [EVAL] batch:   82 | acc: 43.75%,  total acc: 82.30%   [EVAL] batch:   83 | acc: 25.00%,  total acc: 81.62%   [EVAL] batch:   84 | acc: 43.75%,  total acc: 81.18%   [EVAL] batch:   85 | acc: 25.00%,  total acc: 80.52%   [EVAL] batch:   86 | acc: 43.75%,  total acc: 80.10%   [EVAL] batch:   87 | acc: 62.50%,  total acc: 79.90%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 80.13%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 80.35%   [EVAL] batch:   90 | acc: 100.00%,  total acc: 80.56%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 80.77%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 80.98%   [EVAL] batch:   93 | acc: 100.00%,  total acc: 81.18%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 81.38%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 81.58%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 81.77%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 81.95%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 82.13%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 82.31%   [EVAL] batch:  100 | acc: 93.75%,  total acc: 82.43%   [EVAL] batch:  101 | acc: 93.75%,  total acc: 82.54%   [EVAL] batch:  102 | acc: 75.00%,  total acc: 82.46%   [EVAL] batch:  103 | acc: 81.25%,  total acc: 82.45%   [EVAL] batch:  104 | acc: 93.75%,  total acc: 82.56%   [EVAL] batch:  105 | acc: 93.75%,  total acc: 82.67%   [EVAL] batch:  106 | acc: 37.50%,  total acc: 82.24%   [EVAL] batch:  107 | acc: 18.75%,  total acc: 81.66%   [EVAL] batch:  108 | acc: 18.75%,  total acc: 81.08%   [EVAL] batch:  109 | acc: 18.75%,  total acc: 80.51%   [EVAL] batch:  110 | acc: 50.00%,  total acc: 80.24%   [EVAL] batch:  111 | acc: 31.25%,  total acc: 79.80%   [EVAL] batch:  112 | acc: 56.25%,  total acc: 79.59%   [EVAL] batch:  113 | acc: 87.50%,  total acc: 79.66%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 79.84%   [EVAL] batch:  115 | acc: 87.50%,  total acc: 79.90%   [EVAL] batch:  116 | acc: 93.75%,  total acc: 80.02%   [EVAL] batch:  117 | acc: 93.75%,  total acc: 80.14%   [EVAL] batch:  118 | acc: 62.50%,  total acc: 79.99%   [EVAL] batch:  119 | acc: 0.00%,  total acc: 79.32%   [EVAL] batch:  120 | acc: 18.75%,  total acc: 78.82%   [EVAL] batch:  121 | acc: 0.00%,  total acc: 78.18%   [EVAL] batch:  122 | acc: 12.50%,  total acc: 77.64%   [EVAL] batch:  123 | acc: 6.25%,  total acc: 77.07%   [EVAL] batch:  124 | acc: 6.25%,  total acc: 76.50%   [EVAL] batch:  125 | acc: 75.00%,  total acc: 76.49%   [EVAL] batch:  126 | acc: 75.00%,  total acc: 76.48%   [EVAL] batch:  127 | acc: 62.50%,  total acc: 76.37%   [EVAL] batch:  128 | acc: 50.00%,  total acc: 76.16%   [EVAL] batch:  129 | acc: 81.25%,  total acc: 76.20%   [EVAL] batch:  130 | acc: 81.25%,  total acc: 76.24%   [EVAL] batch:  131 | acc: 87.50%,  total acc: 76.33%   [EVAL] batch:  132 | acc: 93.75%,  total acc: 76.46%   [EVAL] batch:  133 | acc: 81.25%,  total acc: 76.49%   [EVAL] batch:  134 | acc: 68.75%,  total acc: 76.44%   [EVAL] batch:  135 | acc: 87.50%,  total acc: 76.52%   [EVAL] batch:  136 | acc: 87.50%,  total acc: 76.60%   [EVAL] batch:  137 | acc: 81.25%,  total acc: 76.63%   [EVAL] batch:  138 | acc: 56.25%,  total acc: 76.48%   [EVAL] batch:  139 | acc: 68.75%,  total acc: 76.43%   [EVAL] batch:  140 | acc: 87.50%,  total acc: 76.51%   [EVAL] batch:  141 | acc: 81.25%,  total acc: 76.54%   [EVAL] batch:  142 | acc: 75.00%,  total acc: 76.53%   [EVAL] batch:  143 | acc: 93.75%,  total acc: 76.65%   [EVAL] batch:  144 | acc: 87.50%,  total acc: 76.72%   [EVAL] batch:  145 | acc: 75.00%,  total acc: 76.71%   [EVAL] batch:  146 | acc: 75.00%,  total acc: 76.70%   [EVAL] batch:  147 | acc: 75.00%,  total acc: 76.69%   [EVAL] batch:  148 | acc: 93.75%,  total acc: 76.80%   [EVAL] batch:  149 | acc: 75.00%,  total acc: 76.79%   [EVAL] batch:  150 | acc: 87.50%,  total acc: 76.86%   [EVAL] batch:  151 | acc: 75.00%,  total acc: 76.85%   [EVAL] batch:  152 | acc: 87.50%,  total acc: 76.92%   [EVAL] batch:  153 | acc: 93.75%,  total acc: 77.03%   [EVAL] batch:  154 | acc: 68.75%,  total acc: 76.98%   [EVAL] batch:  155 | acc: 56.25%,  total acc: 76.84%   [EVAL] batch:  156 | acc: 56.25%,  total acc: 76.71%   [EVAL] batch:  157 | acc: 50.00%,  total acc: 76.54%   [EVAL] batch:  158 | acc: 50.00%,  total acc: 76.38%   [EVAL] batch:  159 | acc: 62.50%,  total acc: 76.29%   [EVAL] batch:  160 | acc: 62.50%,  total acc: 76.20%   [EVAL] batch:  161 | acc: 50.00%,  total acc: 76.04%   [EVAL] batch:  162 | acc: 62.50%,  total acc: 75.96%   [EVAL] batch:  163 | acc: 68.75%,  total acc: 75.91%   [EVAL] batch:  164 | acc: 81.25%,  total acc: 75.95%   [EVAL] batch:  165 | acc: 75.00%,  total acc: 75.94%   [EVAL] batch:  166 | acc: 75.00%,  total acc: 75.94%   [EVAL] batch:  167 | acc: 81.25%,  total acc: 75.97%   [EVAL] batch:  168 | acc: 68.75%,  total acc: 75.92%   [EVAL] batch:  169 | acc: 31.25%,  total acc: 75.66%   [EVAL] batch:  170 | acc: 37.50%,  total acc: 75.44%   [EVAL] batch:  171 | acc: 31.25%,  total acc: 75.18%   [EVAL] batch:  172 | acc: 62.50%,  total acc: 75.11%   [EVAL] batch:  173 | acc: 25.00%,  total acc: 74.82%   [EVAL] batch:  174 | acc: 50.00%,  total acc: 74.68%   [EVAL] batch:  175 | acc: 100.00%,  total acc: 74.82%   [EVAL] batch:  176 | acc: 100.00%,  total acc: 74.96%   [EVAL] batch:  177 | acc: 100.00%,  total acc: 75.11%   [EVAL] batch:  178 | acc: 100.00%,  total acc: 75.24%   [EVAL] batch:  179 | acc: 100.00%,  total acc: 75.38%   [EVAL] batch:  180 | acc: 100.00%,  total acc: 75.52%   [EVAL] batch:  181 | acc: 81.25%,  total acc: 75.55%   [EVAL] batch:  182 | acc: 56.25%,  total acc: 75.44%   [EVAL] batch:  183 | acc: 68.75%,  total acc: 75.41%   [EVAL] batch:  184 | acc: 68.75%,  total acc: 75.37%   [EVAL] batch:  185 | acc: 56.25%,  total acc: 75.27%   [EVAL] batch:  186 | acc: 75.00%,  total acc: 75.27%   [EVAL] batch:  187 | acc: 62.50%,  total acc: 75.20%   [EVAL] batch:  188 | acc: 12.50%,  total acc: 74.87%   [EVAL] batch:  189 | acc: 31.25%,  total acc: 74.64%   [EVAL] batch:  190 | acc: 18.75%,  total acc: 74.35%   [EVAL] batch:  191 | acc: 18.75%,  total acc: 74.06%   [EVAL] batch:  192 | acc: 31.25%,  total acc: 73.83%   [EVAL] batch:  193 | acc: 12.50%,  total acc: 73.52%   [EVAL] batch:  194 | acc: 68.75%,  total acc: 73.49%   [EVAL] batch:  195 | acc: 62.50%,  total acc: 73.44%   [EVAL] batch:  196 | acc: 75.00%,  total acc: 73.45%   [EVAL] batch:  197 | acc: 75.00%,  total acc: 73.45%   [EVAL] batch:  198 | acc: 75.00%,  total acc: 73.46%   [EVAL] batch:  199 | acc: 75.00%,  total acc: 73.47%   [EVAL] batch:  200 | acc: 62.50%,  total acc: 73.41%   [EVAL] batch:  201 | acc: 25.00%,  total acc: 73.17%   [EVAL] batch:  202 | acc: 31.25%,  total acc: 72.97%   [EVAL] batch:  203 | acc: 56.25%,  total acc: 72.89%   [EVAL] batch:  204 | acc: 56.25%,  total acc: 72.80%   [EVAL] batch:  205 | acc: 37.50%,  total acc: 72.63%   [EVAL] batch:  206 | acc: 93.75%,  total acc: 72.74%   [EVAL] batch:  207 | acc: 100.00%,  total acc: 72.87%   [EVAL] batch:  208 | acc: 100.00%,  total acc: 73.00%   [EVAL] batch:  209 | acc: 93.75%,  total acc: 73.10%   [EVAL] batch:  210 | acc: 93.75%,  total acc: 73.19%   [EVAL] batch:  211 | acc: 100.00%,  total acc: 73.32%   [EVAL] batch:  212 | acc: 81.25%,  total acc: 73.36%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 73.48%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 73.60%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 73.73%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 73.85%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 73.97%   [EVAL] batch:  218 | acc: 100.00%,  total acc: 74.09%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 74.20%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 74.32%   [EVAL] batch:  221 | acc: 87.50%,  total acc: 74.38%   [EVAL] batch:  222 | acc: 93.75%,  total acc: 74.47%   [EVAL] batch:  223 | acc: 93.75%,  total acc: 74.55%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 74.67%   [EVAL] batch:  225 | acc: 81.25%,  total acc: 74.70%   [EVAL] batch:  226 | acc: 62.50%,  total acc: 74.64%   [EVAL] batch:  227 | acc: 75.00%,  total acc: 74.64%   [EVAL] batch:  228 | acc: 50.00%,  total acc: 74.54%   [EVAL] batch:  229 | acc: 31.25%,  total acc: 74.35%   [EVAL] batch:  230 | acc: 56.25%,  total acc: 74.27%   [EVAL] batch:  231 | acc: 87.50%,  total acc: 74.33%   [EVAL] batch:  232 | acc: 93.75%,  total acc: 74.41%   [EVAL] batch:  233 | acc: 93.75%,  total acc: 74.49%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 74.60%   [EVAL] batch:  235 | acc: 100.00%,  total acc: 74.71%   [EVAL] batch:  236 | acc: 81.25%,  total acc: 74.74%   [EVAL] batch:  237 | acc: 62.50%,  total acc: 74.68%   [EVAL] batch:  238 | acc: 56.25%,  total acc: 74.61%   [EVAL] batch:  239 | acc: 62.50%,  total acc: 74.56%   [EVAL] batch:  240 | acc: 75.00%,  total acc: 74.56%   [EVAL] batch:  241 | acc: 68.75%,  total acc: 74.54%   [EVAL] batch:  242 | acc: 50.00%,  total acc: 74.43%   [EVAL] batch:  243 | acc: 31.25%,  total acc: 74.26%   [EVAL] batch:  244 | acc: 93.75%,  total acc: 74.34%   [EVAL] batch:  245 | acc: 75.00%,  total acc: 74.34%   [EVAL] batch:  246 | acc: 68.75%,  total acc: 74.32%   [EVAL] batch:  247 | acc: 100.00%,  total acc: 74.42%   [EVAL] batch:  248 | acc: 93.75%,  total acc: 74.50%   [EVAL] batch:  249 | acc: 93.75%,  total acc: 74.58%   
cur_acc:  ['0.9524', '0.8284', '0.7798', '0.7192']
his_acc:  ['0.9524', '0.8850', '0.8195', '0.7458']
CurrentTrain: epoch 15, batch     0 | loss: 17.6384868CurrentTrain: epoch 15, batch     1 | loss: 14.4199577CurrentTrain: epoch 15, batch     2 | loss: 10.6745287CurrentTrain: epoch  1, batch     3 | loss: 7.1711403CurrentTrain: epoch 15, batch     0 | loss: 10.5567092CurrentTrain: epoch 15, batch     1 | loss: 10.8244596CurrentTrain: epoch 15, batch     2 | loss: 12.0464826CurrentTrain: epoch  1, batch     3 | loss: 8.1838315CurrentTrain: epoch 15, batch     0 | loss: 9.5238530CurrentTrain: epoch 15, batch     1 | loss: 14.4707018CurrentTrain: epoch 15, batch     2 | loss: 9.9227618CurrentTrain: epoch  1, batch     3 | loss: 8.5393472CurrentTrain: epoch 15, batch     0 | loss: 7.7800410CurrentTrain: epoch 15, batch     1 | loss: 10.7361297CurrentTrain: epoch 15, batch     2 | loss: 12.6848891CurrentTrain: epoch  1, batch     3 | loss: 7.7351167CurrentTrain: epoch 15, batch     0 | loss: 9.7083844CurrentTrain: epoch 15, batch     1 | loss: 10.1688833CurrentTrain: epoch 15, batch     2 | loss: 7.4559239CurrentTrain: epoch  1, batch     3 | loss: 5.8308123CurrentTrain: epoch 15, batch     0 | loss: 10.0630049CurrentTrain: epoch 15, batch     1 | loss: 8.3539803CurrentTrain: epoch 15, batch     2 | loss: 7.6197724CurrentTrain: epoch  1, batch     3 | loss: 7.2126001CurrentTrain: epoch 15, batch     0 | loss: 4.9773508CurrentTrain: epoch 15, batch     1 | loss: 6.2791977CurrentTrain: epoch 15, batch     2 | loss: 9.9649697CurrentTrain: epoch  1, batch     3 | loss: 7.4134742CurrentTrain: epoch 15, batch     0 | loss: 8.6517428CurrentTrain: epoch 15, batch     1 | loss: 15.9861407CurrentTrain: epoch 15, batch     2 | loss: 21.6691905CurrentTrain: epoch  1, batch     3 | loss: 6.7031208CurrentTrain: epoch 15, batch     0 | loss: 13.0249874CurrentTrain: epoch 15, batch     1 | loss: 6.8685954CurrentTrain: epoch 15, batch     2 | loss: 16.4815135CurrentTrain: epoch  1, batch     3 | loss: 7.7660307CurrentTrain: epoch 15, batch     0 | loss: 23.4264637CurrentTrain: epoch 15, batch     1 | loss: 9.8362915CurrentTrain: epoch 15, batch     2 | loss: 11.5375659CurrentTrain: epoch  1, batch     3 | loss: 6.1698496
#### member of political party ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "member of political party" means the political party of which a person is or has been a member or otherwise affiliated.
Here is an example:
Relation: member of political party
Context: alongside evan durbin and hugh gaitskell , he brought the thinking of john maynard keynes to the labour party , especially in relation to price determination .
Head Entity: hugh gaitskell
Tail Entity: labour party
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "member of political party":
Relation: member of political party  
Context: After serving as the mayor of Chicago, he became a prominent figure in the Democratic Party, advocating for progressive policies and social justice.  
Head Entity: mayor of Chicago  
Tail Entity: Democratic Party  

Relation: member of political party  
Context: Throughout her career, she has been a staunch advocate for environmental issues while being a dedicated member of the Green Party, pushing for sustainable practices.  
Head Entity: she  
Tail Entity: Green Party  
#### after a work by ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "after a work by" means artist whose work strongly inspired/ was copied in this item.
Here is an example:
Relation: after a work by
Context: it is a partly fictional biography , adapted by shane connaughton and jim sheridan from the book " my left foot " by christy brown .
Head Entity: my left foot
Tail Entity: christy brown
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "after a work by":
Relation: after a work by  
Context: the film "the great gatsby" draws heavily from the classic novel of the same name written by f. scott fitzgerald.  
Head Entity: the great gatsby  
Tail Entity: f. scott fitzgerald  

Relation: after a work by  
Context: the musical "hamilton" is inspired by the biography "alexander hamilton" authored by ron chernow.  
Head Entity: hamilton  
Tail Entity: ron chernow  
#### notable work ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "notable work" means notable scientific, artistic or literary work, or other work of significance among subject's works.
Here is an example:
Relation: notable work
Context: the influential victorian art critic john ruskin expounded theories about social justice in " unto this last " ( 1860 ) .
Head Entity: john ruskin
Tail Entity: unto this last
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "notable work":
Relation: notable work  
Context: the renowned physicist albert einstein published his groundbreaking theory of relativity in the paper "on the electrodynamics of moving bodies" (1905).  
Head Entity: albert einstein  
Tail Entity: on the electrodynamics of moving bodies  

Relation: notable work  
Context: the celebrated author j.k. rowling captivated readers with her first book in the harry potter series, "harry potter and the philosopher's stone" (1997).  
Head Entity: j.k. rowling  
Tail Entity: harry potter and the philosopher's stone  
#### headquarters location ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "headquarters location" means city, where an organization's headquarters is or has been situated.
Here is an example:
Relation: headquarters location
Context: in 1986 fram traveled to poland to play against katowice in a very even duel fram eventually lost . sparta prague came to reykjavík 1987 to play against fram reykjavik at laugardalsvöllur stadium .
Head Entity: fram reykjavik
Tail Entity: reykjavík
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "headquarters location":
Relation: headquarters location  
Context: in 2001, the tech company apple inc. moved its headquarters to cupertino, california, where it has since developed numerous innovative products.  
Head Entity: apple inc.  
Tail Entity: cupertino  

Relation: headquarters location  
Context: the multinational corporation unilever established its headquarters in rotterdam, netherlands, in the early 20th century, which has been a key location for its global operations.  
Head Entity: unilever  
Tail Entity: rotterdam  
#### taxon rank ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "taxon rank" means level in a taxonomic hierarchy.
Here is an example:
Relation: taxon rank
Context: a number of species that were placed in the family phyllophoridae by thander in 1989 and 1990 are now included in cucumariidae .
Head Entity: phyllophoridae
Tail Entity: family
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "taxon rank":
Relation: taxon rank  
Context: The genus Canis includes several species, with Canis lupus being the most recognized as the wolf, which is classified under the family Canidae.  
Head Entity: Canis  
Tail Entity: genus  

Relation: taxon rank  
Context: In the classification of plants, the family Rosaceae encompasses a variety of species, including roses and strawberries, which are categorized under the order Rosales.  
Head Entity: Rosaceae  
Tail Entity: family  
#### field of work ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "field of work" means specialization of a person or organization.
Here is an example:
Relation: field of work
Context: elspeth frances garman is professor of molecular biophysics at the university of oxford and a former president of the british crystallographic association .
Head Entity: elspeth frances garman
Tail Entity: molecular biophysics
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "field of work":
Relation: field of work  
Context: dr. amanda jones is a leading researcher in the field of artificial intelligence and currently works at the tech innovation lab in silicon valley.  
Head Entity: dr. amanda jones  
Tail Entity: artificial intelligence  

Relation: field of work  
Context: the world health organization has appointed dr. rajesh kumar as the new director of global health initiatives, focusing on infectious diseases.  
Head Entity: dr. rajesh kumar  
Tail Entity: infectious diseases  
#### licensed to broadcast to ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "licensed to broadcast to" means place that a radio/TV station is licensed/required to broadcast to.
Here is an example:
Relation: licensed to broadcast to
Context: this early involvement in radio led him to buy many small , under performing radio stations beginning with wcos in columbia , south carolina , and turn them around with new formats and improved management .
Head Entity: wcos
Tail Entity: columbia , south carolina
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "licensed to broadcast to":
Relation: licensed to broadcast to  
Context: The new television station, KXYZ, received its broadcasting license and is set to serve the vibrant community of Austin, Texas, with a variety of local programming.  
Head Entity: KXYZ  
Tail Entity: Austin, Texas  

Relation: licensed to broadcast to  
Context: After acquiring the rights, the radio station WABC announced its plans to reach listeners in the bustling city of New York, ensuring a wide array of music and news coverage.  
Head Entity: WABC  
Tail Entity: New York
#### constellation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "constellation" means the area of the celestial sphere of which the subject is a part.
Here is an example:
Relation: constellation
Context: hd 32518 b is an extrasolar planet which orbits the k - type giant star hd 32518 , located approximately 383 light years away in the constellation camelopardalis .
Head Entity: hd 32518
Tail Entity: camelopardalis
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "constellation":
Relation: constellation  
Context: the star sirius, known as the brightest star in the night sky, is located in the constellation canis major and is approximately 8.6 light years away from Earth.  
Head Entity: sirius  
Tail Entity: canis major  

Relation: constellation  
Context: the andromeda galaxy, which is the nearest spiral galaxy to the Milky Way, can be found in the constellation andromeda, making it a prominent feature in the night sky.  
Head Entity: andromeda galaxy  
Tail Entity: andromeda  
#### platform ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "platform" means platform for which a work was developed or released, or the specific platform version of a software product.
Here is an example:
Relation: platform
Context: " toy soldiers : war chest " was released on august 11 , 2015 for windows , playstation 4 and xbox one .
Head Entity: toy soldiers : war chest
Tail Entity: xbox one
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "platform":
Relation: platform  
Context: "The Legend of Zelda: Breath of the Wild" was launched on March 3, 2017 for the Nintendo Switch and Wii U.  
Head Entity: The Legend of Zelda: Breath of the Wild  
Tail Entity: Nintendo Switch  

Relation: platform  
Context: "Cyberpunk 2077" was made available on December 10, 2020 for PC, PlayStation 4, and Xbox One.  
Head Entity: Cyberpunk 2077  
Tail Entity: PlayStation 4  
#### place served by transport hub ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "place served by transport hub" means territorial entity or entities served by this transport hub (airport, train station, etc.).
Here is an example:
Relation: place served by transport hub
Context: services to the island run from san juan 's luis muñoz marín international airport , ceiba airport or isla grande airport ( 20- to 30-minute flight ) .
Head Entity: isla grande airport
Tail Entity: san juan
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "place served by transport hub":
Relation: place served by transport hub  
Context: The city of Barcelona is well-connected to various destinations through the El Prat Airport, which offers flights to numerous European cities.  
Head Entity: El Prat Airport  
Tail Entity: Barcelona  

Relation: place served by transport hub  
Context: The central train station in Zurich provides access to multiple Swiss cities and international destinations, making it a key transport hub in the region.  
Head Entity: Zurich central train station  
Tail Entity: Zurich  
MemoryTrain:  epoch 15, batch     0 | loss: 6.7161874MemoryTrain:  epoch 15, batch     1 | loss: 2.6969875MemoryTrain:  epoch 15, batch     2 | loss: 2.8105072MemoryTrain:  epoch 15, batch     3 | loss: 3.6506187MemoryTrain:  epoch 15, batch     4 | loss: 2.9636996MemoryTrain:  epoch 15, batch     5 | loss: 4.8572374MemoryTrain:  epoch 15, batch     6 | loss: 3.6954782MemoryTrain:  epoch 15, batch     7 | loss: 2.8382847MemoryTrain:  epoch 15, batch     8 | loss: 3.1305244MemoryTrain:  epoch  5, batch     9 | loss: 9.3706064MemoryTrain:  epoch 15, batch     0 | loss: 5.2303801MemoryTrain:  epoch 15, batch     1 | loss: 3.6169826MemoryTrain:  epoch 15, batch     2 | loss: 3.3539667MemoryTrain:  epoch 15, batch     3 | loss: 5.7027400MemoryTrain:  epoch 15, batch     4 | loss: 3.7225002MemoryTrain:  epoch 15, batch     5 | loss: 4.7923440MemoryTrain:  epoch 15, batch     6 | loss: 3.3331985MemoryTrain:  epoch 15, batch     7 | loss: 2.7574027MemoryTrain:  epoch 15, batch     8 | loss: 2.2176520MemoryTrain:  epoch  5, batch     9 | loss: 8.4041683MemoryTrain:  epoch 15, batch     0 | loss: 2.4594084MemoryTrain:  epoch 15, batch     1 | loss: 1.5025298MemoryTrain:  epoch 15, batch     2 | loss: 4.5987215MemoryTrain:  epoch 15, batch     3 | loss: 2.2376828MemoryTrain:  epoch 15, batch     4 | loss: 2.9505209MemoryTrain:  epoch 15, batch     5 | loss: 2.7947208MemoryTrain:  epoch 15, batch     6 | loss: 4.9107035MemoryTrain:  epoch 15, batch     7 | loss: 5.0231857MemoryTrain:  epoch 15, batch     8 | loss: 2.8295803MemoryTrain:  epoch  5, batch     9 | loss: 8.2622946MemoryTrain:  epoch 15, batch     0 | loss: 2.4631952MemoryTrain:  epoch 15, batch     1 | loss: 1.7579724MemoryTrain:  epoch 15, batch     2 | loss: 2.0690471MemoryTrain:  epoch 15, batch     3 | loss: 2.3128346MemoryTrain:  epoch 15, batch     4 | loss: 4.1916442MemoryTrain:  epoch 15, batch     5 | loss: 1.9398438MemoryTrain:  epoch 15, batch     6 | loss: 2.1817751MemoryTrain:  epoch 15, batch     7 | loss: 3.1988855MemoryTrain:  epoch 15, batch     8 | loss: 2.5189472MemoryTrain:  epoch  5, batch     9 | loss: 8.2620703MemoryTrain:  epoch 15, batch     0 | loss: 1.9694323MemoryTrain:  epoch 15, batch     1 | loss: 2.0918628MemoryTrain:  epoch 15, batch     2 | loss: 1.7196257MemoryTrain:  epoch 15, batch     3 | loss: 2.5433187MemoryTrain:  epoch 15, batch     4 | loss: 3.5118853MemoryTrain:  epoch 15, batch     5 | loss: 2.4650836MemoryTrain:  epoch 15, batch     6 | loss: 2.1389005MemoryTrain:  epoch 15, batch     7 | loss: 1.9752798MemoryTrain:  epoch 15, batch     8 | loss: 2.6649797MemoryTrain:  epoch  5, batch     9 | loss: 8.5370333MemoryTrain:  epoch 15, batch     0 | loss: 2.1239789MemoryTrain:  epoch 15, batch     1 | loss: 2.7283620MemoryTrain:  epoch 15, batch     2 | loss: 2.2018740MemoryTrain:  epoch 15, batch     3 | loss: 4.6526600MemoryTrain:  epoch 15, batch     4 | loss: 1.4953061MemoryTrain:  epoch 15, batch     5 | loss: 2.2987769MemoryTrain:  epoch 15, batch     6 | loss: 1.8049707MemoryTrain:  epoch 15, batch     7 | loss: 1.4218440MemoryTrain:  epoch 15, batch     8 | loss: 1.8334381MemoryTrain:  epoch  5, batch     9 | loss: 7.5130110MemoryTrain:  epoch 15, batch     0 | loss: 1.4742247MemoryTrain:  epoch 15, batch     1 | loss: 2.6098845MemoryTrain:  epoch 15, batch     2 | loss: 4.6550139MemoryTrain:  epoch 15, batch     3 | loss: 2.4995699MemoryTrain:  epoch 15, batch     4 | loss: 1.7716321MemoryTrain:  epoch 15, batch     5 | loss: 2.6585182MemoryTrain:  epoch 15, batch     6 | loss: 2.7175181MemoryTrain:  epoch 15, batch     7 | loss: 1.7438315MemoryTrain:  epoch 15, batch     8 | loss: 2.2744755MemoryTrain:  epoch  5, batch     9 | loss: 7.5905146MemoryTrain:  epoch 15, batch     0 | loss: 1.8046692MemoryTrain:  epoch 15, batch     1 | loss: 3.8257681MemoryTrain:  epoch 15, batch     2 | loss: 4.1039766MemoryTrain:  epoch 15, batch     3 | loss: 1.3784099MemoryTrain:  epoch 15, batch     4 | loss: 2.8527807MemoryTrain:  epoch 15, batch     5 | loss: 1.4750126MemoryTrain:  epoch 15, batch     6 | loss: 1.8405325MemoryTrain:  epoch 15, batch     7 | loss: 1.7711567MemoryTrain:  epoch 15, batch     8 | loss: 2.2619244MemoryTrain:  epoch  5, batch     9 | loss: 8.1133080MemoryTrain:  epoch 15, batch     0 | loss: 1.8852362MemoryTrain:  epoch 15, batch     1 | loss: 1.3955017MemoryTrain:  epoch 15, batch     2 | loss: 4.8441752MemoryTrain:  epoch 15, batch     3 | loss: 2.2730368MemoryTrain:  epoch 15, batch     4 | loss: 2.0432638MemoryTrain:  epoch 15, batch     5 | loss: 2.8245059MemoryTrain:  epoch 15, batch     6 | loss: 2.0629182MemoryTrain:  epoch 15, batch     7 | loss: 1.7960583MemoryTrain:  epoch 15, batch     8 | loss: 3.9460261MemoryTrain:  epoch  5, batch     9 | loss: 7.8048136MemoryTrain:  epoch 15, batch     0 | loss: 3.1977264MemoryTrain:  epoch 15, batch     1 | loss: 7.1845858MemoryTrain:  epoch 15, batch     2 | loss: 2.1891864MemoryTrain:  epoch 15, batch     3 | loss: 5.3925577MemoryTrain:  epoch 15, batch     4 | loss: 2.0875315MemoryTrain:  epoch 15, batch     5 | loss: 1.7736067MemoryTrain:  epoch 15, batch     6 | loss: 1.7285832MemoryTrain:  epoch 15, batch     7 | loss: 4.6479795MemoryTrain:  epoch 15, batch     8 | loss: 2.0430938MemoryTrain:  epoch  5, batch     9 | loss: 7.9781781
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 92.19%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 92.50%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 92.86%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 92.97%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 92.71%   [EVAL] batch:   12 | acc: 100.00%,  total acc: 93.27%   [EVAL] batch:   13 | acc: 87.50%,  total acc: 92.86%   [EVAL] batch:   14 | acc: 81.25%,  total acc: 92.08%   [EVAL] batch:   15 | acc: 75.00%,  total acc: 91.02%   [EVAL] batch:   16 | acc: 62.50%,  total acc: 89.34%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 88.19%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 87.17%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 86.56%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 86.61%   [EVAL] batch:   21 | acc: 75.00%,  total acc: 86.08%   [EVAL] batch:   22 | acc: 81.25%,  total acc: 85.87%   [EVAL] batch:   23 | acc: 81.25%,  total acc: 85.68%   [EVAL] batch:   24 | acc: 50.00%,  total acc: 84.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 84.86%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 85.42%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 85.94%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 86.42%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 86.88%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 87.30%   [EVAL] batch:   31 | acc: 81.25%,  total acc: 87.11%   [EVAL] batch:   32 | acc: 87.50%,  total acc: 87.12%   [EVAL] batch:   33 | acc: 75.00%,  total acc: 86.76%   [EVAL] batch:   34 | acc: 87.50%,  total acc: 86.79%   [EVAL] batch:   35 | acc: 68.75%,  total acc: 86.28%   [EVAL] batch:   36 | acc: 75.00%,  total acc: 85.98%   [EVAL] batch:   37 | acc: 81.25%,  total acc: 85.86%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 86.22%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 86.56%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 86.89%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 87.20%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 87.64%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 87.92%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 88.18%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 88.43%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 88.67%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 88.90%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 89.12%   [EVAL] batch:   50 | acc: 87.50%,  total acc: 89.09%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 89.06%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 89.15%   [EVAL] batch:   53 | acc: 93.75%,  total acc: 89.24%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 89.09%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 89.17%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 89.25%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 89.33%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 89.30%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 89.48%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 89.65%   [EVAL] batch:   61 | acc: 81.25%,  total acc: 89.52%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 88.89%   
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 79.69%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 78.75%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 80.21%   [EVAL] batch:    6 | acc: 43.75%,  total acc: 75.00%   [EVAL] batch:    7 | acc: 62.50%,  total acc: 73.44%   [EVAL] batch:    8 | acc: 56.25%,  total acc: 71.53%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 71.25%   [EVAL] batch:   10 | acc: 50.00%,  total acc: 69.32%   [EVAL] batch:   11 | acc: 62.50%,  total acc: 68.75%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 69.71%   [EVAL] batch:   13 | acc: 87.50%,  total acc: 70.98%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 72.50%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 74.22%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 75.74%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 76.74%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 77.63%   [EVAL] batch:   19 | acc: 68.75%,  total acc: 77.19%   [EVAL] batch:   20 | acc: 81.25%,  total acc: 77.38%   [EVAL] batch:   21 | acc: 68.75%,  total acc: 76.99%   [EVAL] batch:   22 | acc: 56.25%,  total acc: 76.09%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 76.56%   [EVAL] batch:   24 | acc: 75.00%,  total acc: 76.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 77.40%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 78.24%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 79.02%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 79.74%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 80.42%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 81.05%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 81.45%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 82.01%   [EVAL] batch:   33 | acc: 75.00%,  total acc: 81.80%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 81.61%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 81.77%   [EVAL] batch:   36 | acc: 87.50%,  total acc: 81.93%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 82.24%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 82.69%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 83.12%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 83.54%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 83.93%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 84.16%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:   44 | acc: 81.25%,  total acc: 84.31%   [EVAL] batch:   45 | acc: 75.00%,  total acc: 84.10%   [EVAL] batch:   46 | acc: 62.50%,  total acc: 83.64%   [EVAL] batch:   47 | acc: 68.75%,  total acc: 83.33%   [EVAL] batch:   48 | acc: 93.75%,  total acc: 83.55%   [EVAL] batch:   49 | acc: 68.75%,  total acc: 83.25%   [EVAL] batch:   50 | acc: 81.25%,  total acc: 83.21%   [EVAL] batch:   51 | acc: 81.25%,  total acc: 83.17%   [EVAL] batch:   52 | acc: 87.50%,  total acc: 83.25%   [EVAL] batch:   53 | acc: 75.00%,  total acc: 83.10%   [EVAL] batch:   54 | acc: 56.25%,  total acc: 82.61%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 82.70%   [EVAL] batch:   56 | acc: 68.75%,  total acc: 82.46%   [EVAL] batch:   57 | acc: 43.75%,  total acc: 81.79%   [EVAL] batch:   58 | acc: 62.50%,  total acc: 81.46%   [EVAL] batch:   59 | acc: 87.50%,  total acc: 81.56%   [EVAL] batch:   60 | acc: 87.50%,  total acc: 81.66%   [EVAL] batch:   61 | acc: 75.00%,  total acc: 81.55%   [EVAL] batch:   62 | acc: 81.25%,  total acc: 81.55%   [EVAL] batch:   63 | acc: 43.75%,  total acc: 80.96%   [EVAL] batch:   64 | acc: 50.00%,  total acc: 80.48%   [EVAL] batch:   65 | acc: 56.25%,  total acc: 80.11%   [EVAL] batch:   66 | acc: 43.75%,  total acc: 79.57%   [EVAL] batch:   67 | acc: 62.50%,  total acc: 79.32%   [EVAL] batch:   68 | acc: 56.25%,  total acc: 78.99%   [EVAL] batch:   69 | acc: 75.00%,  total acc: 78.93%   [EVAL] batch:   70 | acc: 56.25%,  total acc: 78.61%   [EVAL] batch:   71 | acc: 50.00%,  total acc: 78.21%   [EVAL] batch:   72 | acc: 75.00%,  total acc: 78.17%   [EVAL] batch:   73 | acc: 81.25%,  total acc: 78.21%   [EVAL] batch:   74 | acc: 81.25%,  total acc: 78.25%   [EVAL] batch:   75 | acc: 81.25%,  total acc: 78.29%   [EVAL] batch:   76 | acc: 75.00%,  total acc: 78.25%   [EVAL] batch:   77 | acc: 81.25%,  total acc: 78.29%   [EVAL] batch:   78 | acc: 81.25%,  total acc: 78.32%   [EVAL] batch:   79 | acc: 81.25%,  total acc: 78.36%   [EVAL] batch:   80 | acc: 93.75%,  total acc: 78.55%   [EVAL] batch:   81 | acc: 50.00%,  total acc: 78.20%   [EVAL] batch:   82 | acc: 50.00%,  total acc: 77.86%   [EVAL] batch:   83 | acc: 50.00%,  total acc: 77.53%   [EVAL] batch:   84 | acc: 50.00%,  total acc: 77.21%   [EVAL] batch:   85 | acc: 43.75%,  total acc: 76.82%   [EVAL] batch:   86 | acc: 56.25%,  total acc: 76.58%   [EVAL] batch:   87 | acc: 68.75%,  total acc: 76.49%   [EVAL] batch:   88 | acc: 93.75%,  total acc: 76.69%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 76.94%   [EVAL] batch:   90 | acc: 100.00%,  total acc: 77.20%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 77.45%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 77.69%   [EVAL] batch:   93 | acc: 100.00%,  total acc: 77.93%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 78.16%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 78.39%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 78.61%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 78.83%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 79.04%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 79.25%   [EVAL] batch:  100 | acc: 93.75%,  total acc: 79.39%   [EVAL] batch:  101 | acc: 93.75%,  total acc: 79.53%   [EVAL] batch:  102 | acc: 75.00%,  total acc: 79.49%   [EVAL] batch:  103 | acc: 81.25%,  total acc: 79.51%   [EVAL] batch:  104 | acc: 93.75%,  total acc: 79.64%   [EVAL] batch:  105 | acc: 93.75%,  total acc: 79.78%   [EVAL] batch:  106 | acc: 37.50%,  total acc: 79.38%   [EVAL] batch:  107 | acc: 25.00%,  total acc: 78.88%   [EVAL] batch:  108 | acc: 18.75%,  total acc: 78.33%   [EVAL] batch:  109 | acc: 12.50%,  total acc: 77.73%   [EVAL] batch:  110 | acc: 50.00%,  total acc: 77.48%   [EVAL] batch:  111 | acc: 31.25%,  total acc: 77.06%   [EVAL] batch:  112 | acc: 56.25%,  total acc: 76.88%   [EVAL] batch:  113 | acc: 87.50%,  total acc: 76.97%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 77.17%   [EVAL] batch:  115 | acc: 87.50%,  total acc: 77.26%   [EVAL] batch:  116 | acc: 93.75%,  total acc: 77.40%   [EVAL] batch:  117 | acc: 93.75%,  total acc: 77.54%   [EVAL] batch:  118 | acc: 62.50%,  total acc: 77.42%   [EVAL] batch:  119 | acc: 0.00%,  total acc: 76.77%   [EVAL] batch:  120 | acc: 0.00%,  total acc: 76.14%   [EVAL] batch:  121 | acc: 0.00%,  total acc: 75.51%   [EVAL] batch:  122 | acc: 6.25%,  total acc: 74.95%   [EVAL] batch:  123 | acc: 0.00%,  total acc: 74.34%   [EVAL] batch:  124 | acc: 0.00%,  total acc: 73.75%   [EVAL] batch:  125 | acc: 87.50%,  total acc: 73.86%   [EVAL] batch:  126 | acc: 81.25%,  total acc: 73.92%   [EVAL] batch:  127 | acc: 68.75%,  total acc: 73.88%   [EVAL] batch:  128 | acc: 56.25%,  total acc: 73.74%   [EVAL] batch:  129 | acc: 81.25%,  total acc: 73.80%   [EVAL] batch:  130 | acc: 81.25%,  total acc: 73.85%   [EVAL] batch:  131 | acc: 93.75%,  total acc: 74.01%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 74.20%   [EVAL] batch:  133 | acc: 87.50%,  total acc: 74.30%   [EVAL] batch:  134 | acc: 68.75%,  total acc: 74.26%   [EVAL] batch:  135 | acc: 87.50%,  total acc: 74.36%   [EVAL] batch:  136 | acc: 100.00%,  total acc: 74.54%   [EVAL] batch:  137 | acc: 81.25%,  total acc: 74.59%   [EVAL] batch:  138 | acc: 56.25%,  total acc: 74.46%   [EVAL] batch:  139 | acc: 68.75%,  total acc: 74.42%   [EVAL] batch:  140 | acc: 81.25%,  total acc: 74.47%   [EVAL] batch:  141 | acc: 93.75%,  total acc: 74.60%   [EVAL] batch:  142 | acc: 68.75%,  total acc: 74.56%   [EVAL] batch:  143 | acc: 81.25%,  total acc: 74.61%   [EVAL] batch:  144 | acc: 56.25%,  total acc: 74.48%   [EVAL] batch:  145 | acc: 62.50%,  total acc: 74.40%   [EVAL] batch:  146 | acc: 62.50%,  total acc: 74.32%   [EVAL] batch:  147 | acc: 68.75%,  total acc: 74.28%   [EVAL] batch:  148 | acc: 68.75%,  total acc: 74.24%   [EVAL] batch:  149 | acc: 62.50%,  total acc: 74.17%   [EVAL] batch:  150 | acc: 87.50%,  total acc: 74.25%   [EVAL] batch:  151 | acc: 75.00%,  total acc: 74.26%   [EVAL] batch:  152 | acc: 75.00%,  total acc: 74.26%   [EVAL] batch:  153 | acc: 93.75%,  total acc: 74.39%   [EVAL] batch:  154 | acc: 68.75%,  total acc: 74.35%   [EVAL] batch:  155 | acc: 56.25%,  total acc: 74.24%   [EVAL] batch:  156 | acc: 43.75%,  total acc: 74.04%   [EVAL] batch:  157 | acc: 50.00%,  total acc: 73.89%   [EVAL] batch:  158 | acc: 50.00%,  total acc: 73.74%   [EVAL] batch:  159 | acc: 50.00%,  total acc: 73.59%   [EVAL] batch:  160 | acc: 62.50%,  total acc: 73.52%   [EVAL] batch:  161 | acc: 37.50%,  total acc: 73.30%   [EVAL] batch:  162 | acc: 62.50%,  total acc: 73.24%   [EVAL] batch:  163 | acc: 62.50%,  total acc: 73.17%   [EVAL] batch:  164 | acc: 68.75%,  total acc: 73.14%   [EVAL] batch:  165 | acc: 75.00%,  total acc: 73.16%   [EVAL] batch:  166 | acc: 75.00%,  total acc: 73.17%   [EVAL] batch:  167 | acc: 81.25%,  total acc: 73.21%   [EVAL] batch:  168 | acc: 68.75%,  total acc: 73.19%   [EVAL] batch:  169 | acc: 25.00%,  total acc: 72.90%   [EVAL] batch:  170 | acc: 37.50%,  total acc: 72.70%   [EVAL] batch:  171 | acc: 31.25%,  total acc: 72.46%   [EVAL] batch:  172 | acc: 56.25%,  total acc: 72.36%   [EVAL] batch:  173 | acc: 31.25%,  total acc: 72.13%   [EVAL] batch:  174 | acc: 50.00%,  total acc: 72.00%   [EVAL] batch:  175 | acc: 100.00%,  total acc: 72.16%   [EVAL] batch:  176 | acc: 100.00%,  total acc: 72.32%   [EVAL] batch:  177 | acc: 100.00%,  total acc: 72.47%   [EVAL] batch:  178 | acc: 100.00%,  total acc: 72.63%   [EVAL] batch:  179 | acc: 100.00%,  total acc: 72.78%   [EVAL] batch:  180 | acc: 100.00%,  total acc: 72.93%   [EVAL] batch:  181 | acc: 81.25%,  total acc: 72.97%   [EVAL] batch:  182 | acc: 56.25%,  total acc: 72.88%   [EVAL] batch:  183 | acc: 62.50%,  total acc: 72.83%   [EVAL] batch:  184 | acc: 68.75%,  total acc: 72.80%   [EVAL] batch:  185 | acc: 56.25%,  total acc: 72.72%   [EVAL] batch:  186 | acc: 68.75%,  total acc: 72.69%   [EVAL] batch:  187 | acc: 43.75%,  total acc: 72.54%   [EVAL] batch:  188 | acc: 12.50%,  total acc: 72.22%   [EVAL] batch:  189 | acc: 37.50%,  total acc: 72.04%   [EVAL] batch:  190 | acc: 18.75%,  total acc: 71.76%   [EVAL] batch:  191 | acc: 18.75%,  total acc: 71.48%   [EVAL] batch:  192 | acc: 25.00%,  total acc: 71.24%   [EVAL] batch:  193 | acc: 12.50%,  total acc: 70.94%   [EVAL] batch:  194 | acc: 62.50%,  total acc: 70.90%   [EVAL] batch:  195 | acc: 50.00%,  total acc: 70.79%   [EVAL] batch:  196 | acc: 75.00%,  total acc: 70.81%   [EVAL] batch:  197 | acc: 75.00%,  total acc: 70.83%   [EVAL] batch:  198 | acc: 68.75%,  total acc: 70.82%   [EVAL] batch:  199 | acc: 75.00%,  total acc: 70.84%   [EVAL] batch:  200 | acc: 68.75%,  total acc: 70.83%   [EVAL] batch:  201 | acc: 25.00%,  total acc: 70.61%   [EVAL] batch:  202 | acc: 50.00%,  total acc: 70.50%   [EVAL] batch:  203 | acc: 68.75%,  total acc: 70.50%   [EVAL] batch:  204 | acc: 68.75%,  total acc: 70.49%   [EVAL] batch:  205 | acc: 37.50%,  total acc: 70.33%   [EVAL] batch:  206 | acc: 93.75%,  total acc: 70.44%   [EVAL] batch:  207 | acc: 100.00%,  total acc: 70.58%   [EVAL] batch:  208 | acc: 100.00%,  total acc: 70.72%   [EVAL] batch:  209 | acc: 93.75%,  total acc: 70.83%   [EVAL] batch:  210 | acc: 100.00%,  total acc: 70.97%   [EVAL] batch:  211 | acc: 100.00%,  total acc: 71.11%   [EVAL] batch:  212 | acc: 81.25%,  total acc: 71.16%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 71.29%   [EVAL] batch:  214 | acc: 93.75%,  total acc: 71.40%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 71.53%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 71.66%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 71.79%   [EVAL] batch:  218 | acc: 100.00%,  total acc: 71.92%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 72.05%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 72.17%   [EVAL] batch:  221 | acc: 93.75%,  total acc: 72.27%   [EVAL] batch:  222 | acc: 93.75%,  total acc: 72.37%   [EVAL] batch:  223 | acc: 93.75%,  total acc: 72.46%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 72.58%   [EVAL] batch:  225 | acc: 68.75%,  total acc: 72.57%   [EVAL] batch:  226 | acc: 50.00%,  total acc: 72.47%   [EVAL] batch:  227 | acc: 75.00%,  total acc: 72.48%   [EVAL] batch:  228 | acc: 37.50%,  total acc: 72.33%   [EVAL] batch:  229 | acc: 31.25%,  total acc: 72.15%   [EVAL] batch:  230 | acc: 50.00%,  total acc: 72.05%   [EVAL] batch:  231 | acc: 93.75%,  total acc: 72.14%   [EVAL] batch:  232 | acc: 93.75%,  total acc: 72.24%   [EVAL] batch:  233 | acc: 93.75%,  total acc: 72.33%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 72.45%   [EVAL] batch:  235 | acc: 100.00%,  total acc: 72.56%   [EVAL] batch:  236 | acc: 81.25%,  total acc: 72.60%   [EVAL] batch:  237 | acc: 75.00%,  total acc: 72.61%   [EVAL] batch:  238 | acc: 56.25%,  total acc: 72.54%   [EVAL] batch:  239 | acc: 68.75%,  total acc: 72.53%   [EVAL] batch:  240 | acc: 62.50%,  total acc: 72.48%   [EVAL] batch:  241 | acc: 75.00%,  total acc: 72.49%   [EVAL] batch:  242 | acc: 62.50%,  total acc: 72.45%   [EVAL] batch:  243 | acc: 37.50%,  total acc: 72.31%   [EVAL] batch:  244 | acc: 87.50%,  total acc: 72.37%   [EVAL] batch:  245 | acc: 68.75%,  total acc: 72.36%   [EVAL] batch:  246 | acc: 56.25%,  total acc: 72.29%   [EVAL] batch:  247 | acc: 87.50%,  total acc: 72.35%   [EVAL] batch:  248 | acc: 81.25%,  total acc: 72.39%   [EVAL] batch:  249 | acc: 62.50%,  total acc: 72.35%   [EVAL] batch:  250 | acc: 93.75%,  total acc: 72.44%   [EVAL] batch:  251 | acc: 93.75%,  total acc: 72.52%   [EVAL] batch:  252 | acc: 93.75%,  total acc: 72.60%   [EVAL] batch:  253 | acc: 87.50%,  total acc: 72.66%   [EVAL] batch:  254 | acc: 93.75%,  total acc: 72.75%   [EVAL] batch:  255 | acc: 100.00%,  total acc: 72.85%   [EVAL] batch:  256 | acc: 87.50%,  total acc: 72.91%   [EVAL] batch:  257 | acc: 93.75%,  total acc: 72.99%   [EVAL] batch:  258 | acc: 100.00%,  total acc: 73.09%   [EVAL] batch:  259 | acc: 93.75%,  total acc: 73.17%   [EVAL] batch:  260 | acc: 93.75%,  total acc: 73.25%   [EVAL] batch:  261 | acc: 81.25%,  total acc: 73.28%   [EVAL] batch:  262 | acc: 100.00%,  total acc: 73.38%   [EVAL] batch:  263 | acc: 87.50%,  total acc: 73.44%   [EVAL] batch:  264 | acc: 81.25%,  total acc: 73.47%   [EVAL] batch:  265 | acc: 75.00%,  total acc: 73.47%   [EVAL] batch:  266 | acc: 62.50%,  total acc: 73.43%   [EVAL] batch:  267 | acc: 68.75%,  total acc: 73.41%   [EVAL] batch:  268 | acc: 68.75%,  total acc: 73.40%   [EVAL] batch:  269 | acc: 75.00%,  total acc: 73.40%   [EVAL] batch:  270 | acc: 87.50%,  total acc: 73.45%   [EVAL] batch:  271 | acc: 75.00%,  total acc: 73.46%   [EVAL] batch:  272 | acc: 81.25%,  total acc: 73.49%   [EVAL] batch:  273 | acc: 81.25%,  total acc: 73.52%   [EVAL] batch:  274 | acc: 50.00%,  total acc: 73.43%   [EVAL] batch:  275 | acc: 100.00%,  total acc: 73.53%   [EVAL] batch:  276 | acc: 100.00%,  total acc: 73.62%   [EVAL] batch:  277 | acc: 100.00%,  total acc: 73.72%   [EVAL] batch:  278 | acc: 100.00%,  total acc: 73.81%   [EVAL] batch:  279 | acc: 100.00%,  total acc: 73.91%   [EVAL] batch:  280 | acc: 100.00%,  total acc: 74.00%   [EVAL] batch:  281 | acc: 81.25%,  total acc: 74.02%   [EVAL] batch:  282 | acc: 87.50%,  total acc: 74.07%   [EVAL] batch:  283 | acc: 75.00%,  total acc: 74.08%   [EVAL] batch:  284 | acc: 87.50%,  total acc: 74.12%   [EVAL] batch:  285 | acc: 68.75%,  total acc: 74.10%   [EVAL] batch:  286 | acc: 75.00%,  total acc: 74.11%   [EVAL] batch:  287 | acc: 81.25%,  total acc: 74.13%   [EVAL] batch:  288 | acc: 100.00%,  total acc: 74.22%   [EVAL] batch:  289 | acc: 100.00%,  total acc: 74.31%   [EVAL] batch:  290 | acc: 100.00%,  total acc: 74.40%   [EVAL] batch:  291 | acc: 100.00%,  total acc: 74.49%   [EVAL] batch:  292 | acc: 100.00%,  total acc: 74.57%   [EVAL] batch:  293 | acc: 93.75%,  total acc: 74.64%   [EVAL] batch:  294 | acc: 100.00%,  total acc: 74.72%   [EVAL] batch:  295 | acc: 100.00%,  total acc: 74.81%   [EVAL] batch:  296 | acc: 100.00%,  total acc: 74.89%   [EVAL] batch:  297 | acc: 100.00%,  total acc: 74.98%   [EVAL] batch:  298 | acc: 100.00%,  total acc: 75.06%   [EVAL] batch:  299 | acc: 100.00%,  total acc: 75.15%   [EVAL] batch:  300 | acc: 87.50%,  total acc: 75.19%   [EVAL] batch:  301 | acc: 87.50%,  total acc: 75.23%   [EVAL] batch:  302 | acc: 93.75%,  total acc: 75.29%   [EVAL] batch:  303 | acc: 93.75%,  total acc: 75.35%   [EVAL] batch:  304 | acc: 81.25%,  total acc: 75.37%   [EVAL] batch:  305 | acc: 93.75%,  total acc: 75.43%   [EVAL] batch:  306 | acc: 93.75%,  total acc: 75.49%   [EVAL] batch:  307 | acc: 93.75%,  total acc: 75.55%   [EVAL] batch:  308 | acc: 87.50%,  total acc: 75.59%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 75.67%   [EVAL] batch:  310 | acc: 100.00%,  total acc: 75.74%   [EVAL] batch:  311 | acc: 81.25%,  total acc: 75.76%   [EVAL] batch:  312 | acc: 50.00%,  total acc: 75.68%   
cur_acc:  ['0.9524', '0.8284', '0.7798', '0.7192', '0.8889']
his_acc:  ['0.9524', '0.8850', '0.8195', '0.7458', '0.7568']
CurrentTrain: epoch 15, batch     0 | loss: 22.3143636CurrentTrain: epoch 15, batch     1 | loss: 25.3412702CurrentTrain: epoch 15, batch     2 | loss: 20.5067720CurrentTrain: epoch  1, batch     3 | loss: 8.5752586CurrentTrain: epoch 15, batch     0 | loss: 16.1725342CurrentTrain: epoch 15, batch     1 | loss: 18.6762996CurrentTrain: epoch 15, batch     2 | loss: 12.0235454CurrentTrain: epoch  1, batch     3 | loss: 7.0199351CurrentTrain: epoch 15, batch     0 | loss: 13.5284557CurrentTrain: epoch 15, batch     1 | loss: 11.8852344CurrentTrain: epoch 15, batch     2 | loss: 16.7166042CurrentTrain: epoch  1, batch     3 | loss: 8.5485758CurrentTrain: epoch 15, batch     0 | loss: 13.7925817CurrentTrain: epoch 15, batch     1 | loss: 19.4753439CurrentTrain: epoch 15, batch     2 | loss: 13.6632462CurrentTrain: epoch  1, batch     3 | loss: 7.3135880CurrentTrain: epoch 15, batch     0 | loss: 27.6476030CurrentTrain: epoch 15, batch     1 | loss: 13.6964170CurrentTrain: epoch 15, batch     2 | loss: 16.0465786CurrentTrain: epoch  1, batch     3 | loss: 8.3707876CurrentTrain: epoch 15, batch     0 | loss: 14.8330091CurrentTrain: epoch 15, batch     1 | loss: 13.5681299CurrentTrain: epoch 15, batch     2 | loss: 16.4172029CurrentTrain: epoch  1, batch     3 | loss: 7.7206162CurrentTrain: epoch 15, batch     0 | loss: 11.5086104CurrentTrain: epoch 15, batch     1 | loss: 15.4687445CurrentTrain: epoch 15, batch     2 | loss: 11.7405332CurrentTrain: epoch  1, batch     3 | loss: 8.1157542CurrentTrain: epoch 15, batch     0 | loss: 18.1501087CurrentTrain: epoch 15, batch     1 | loss: 10.1403247CurrentTrain: epoch 15, batch     2 | loss: 8.8862755CurrentTrain: epoch  1, batch     3 | loss: 9.3985465CurrentTrain: epoch 15, batch     0 | loss: 11.7414184CurrentTrain: epoch 15, batch     1 | loss: 17.9573376CurrentTrain: epoch 15, batch     2 | loss: 9.6722829CurrentTrain: epoch  1, batch     3 | loss: 7.1277556CurrentTrain: epoch 15, batch     0 | loss: 13.1702117CurrentTrain: epoch 15, batch     1 | loss: 12.0016892CurrentTrain: epoch 15, batch     2 | loss: 9.8883534CurrentTrain: epoch  1, batch     3 | loss: 6.5979360
#### located on terrain feature ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "located on terrain feature" means located on the specified physical/geophysical feature.
Here is an example:
Relation: located on terrain feature
Context: all of mainland australia 's alpine areas , including its highest mountain , mount kosciuszko ( ahd ) , are part of this range , called the main range .
Head Entity: mount kosciuszko
Tail Entity: main range
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "located on terrain feature":
Relation: located on terrain feature  
Context: The Great Barrier Reef, which is the world's largest coral reef system, is situated off the coast of Queensland, Australia, and is a UNESCO World Heritage site.  
Head Entity: Great Barrier Reef  
Tail Entity: Queensland  

Relation: located on terrain feature  
Context: The Rocky Mountains stretch from Canada down to New Mexico, providing a stunning backdrop for outdoor activities and wildlife.  
Head Entity: Rocky Mountains  
Tail Entity: New Mexico  
#### participant of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "participant of" means event in which a person or organization was/is a participant.
Here is an example:
Relation: participant of
Context: tiago venâncio ( born august 19 , 1987 , setúbal ) is a freestyle swimmer from portugal , who represented his home country at the 2004 summer olympics in athens , greece .
Head Entity: tiago venâncio
Tail Entity: 2004 summer olympics
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "participant of":
Relation: participant of  
Context: the united nations climate change conference, held in paris in 2015, saw numerous world leaders and organizations come together to discuss global warming and its impacts.  
Head Entity: united nations climate change conference  
Tail Entity: paris agreement  

Relation: participant of  
Context: the 2022 fifa world cup in qatar featured teams from around the globe competing for the prestigious trophy, showcasing the best of international football.  
Head Entity: 2022 fifa world cup  
Tail Entity: qatar
#### director ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "director" means director of film, TV-series, stageplay, video game or similar.
Here is an example:
Relation: director
Context: " runaway reptar " was written by ali marie matheson and jon cooksey and directed by john holmquist and jim duffy .
Head Entity: runaway reptar
Tail Entity: john holmquist
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "director":
Relation: director  
Context: "Inception" was directed by Christopher Nolan and features a complex narrative structure that keeps viewers on the edge of their seats.  
Head Entity: Inception  
Tail Entity: Christopher Nolan  

Relation: director  
Context: The acclaimed stage play "Hamilton" was directed by Thomas Kail, who brought Lin-Manuel Miranda's vision to life on Broadway.  
Head Entity: Hamilton  
Tail Entity: Thomas Kail  
#### owned by ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "owned by" means owner of the subject.
Here is an example:
Relation: owned by
Context: the station is owned by the hearst television subsidiary of the hearst corporation .
Head Entity: hearst television
Tail Entity: hearst corporation
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "owned by":
Relation: owned by  
Context: the famous painting is owned by the metropolitan museum of art in new york.  
Head Entity: metropolitan museum of art  
Tail Entity: famous painting  

Relation: owned by  
Context: the luxury car brand is owned by a major automotive conglomerate based in germany.  
Head Entity: major automotive conglomerate  
Tail Entity: luxury car brand  
#### part of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "part of" means object of which the subject is a part.
Here is an example:
Relation: part of
Context: thessaly was home to extensive neolithic and chalcolithic cultures around 6000 bc–2500 bc ( see cardium pottery , dimini and sesklo ) .
Head Entity: cardium pottery
Tail Entity: neolithic
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "part of":
Relation: part of  
Context: The engine is a crucial part of the car's overall functionality, providing the necessary power to drive.  
Head Entity: engine  
Tail Entity: car  

Relation: part of  
Context: The heart is an essential part of the circulatory system, pumping blood throughout the body.  
Head Entity: heart  
Tail Entity: circulatory system  
#### occupant ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "occupant" means person or organization occupying property.
Here is an example:
Relation: occupant
Context: it served as the home of k league side gyeongnam fc and national league side changwon city fc prior to the construction of the changwon football center in 2009 .
Head Entity: changwon football center
Tail Entity: gyeongnam fc
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "occupant":
Relation: occupant  
Context: The historic building was once the residence of the famous artist, and it now serves as a gallery showcasing his works, attracting visitors from all over the world.  
Head Entity: historic building  
Tail Entity: famous artist  

Relation: occupant  
Context: After the renovation, the old factory was transformed into a vibrant co-working space, where numerous startups and freelancers now thrive.  
Head Entity: old factory  
Tail Entity: startups
#### architect ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "architect" means person or architectural firm responsible for designing this building.
Here is an example:
Relation: architect
Context: he also commissioned renowned architect richard neutra to design the iconic desert kaufmann house ( 1946 ) in palm springs , california .
Head Entity: kaufmann house
Tail Entity: richard neutra
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "architect":
Relation: architect  
Context: the innovative design of the new city hall was created by the famous architect frank gehry, known for his unique and modern architectural style.  
Head Entity: city hall  
Tail Entity: frank gehry  

Relation: architect  
Context: after years of planning, the museum was finally built under the guidance of architect zaha hadid, whose work is celebrated for its futuristic designs.  
Head Entity: museum  
Tail Entity: zaha hadid  
#### residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "residence" means the place where the person is or has been, resident.
Here is an example:
Relation: residence
Context: in 2001 brian haw set up the parliament square peace campaign outside the houses of parliament in london .
Head Entity: brian haw
Tail Entity: london
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "residence":
Relation: residence  
Context: After moving to New York City in 2010, Sarah found her dream apartment in Brooklyn.  
Head Entity: Sarah  
Tail Entity: Brooklyn  

Relation: residence  
Context: The famous author lived in a quaint cottage in the countryside for many years before relocating to the city.  
Head Entity: The famous author  
Tail Entity: the city  
#### composer ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "composer" means person who wrote the music.
Here is an example:
Relation: composer
Context: at a young age he was an accomplished cellist , performing at age seventeen the solo part of richard strauss 's " don quixote " .
Head Entity: don quixote
Tail Entity: richard strauss
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "composer":
Relation: composer  
Context: The renowned composer Ludwig van Beethoven created some of the most celebrated symphonies in classical music history.  
Head Entity: symphonies  
Tail Entity: Ludwig van Beethoven  

Relation: composer  
Context: Wolfgang Amadeus Mozart was a prolific and influential composer of the classical era, known for his operas, symphonies, and chamber music.  
Head Entity: operas  
Tail Entity: Wolfgang Amadeus Mozart  
#### location ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "location" means location of the object, structure or event.
Here is an example:
Relation: location
Context: the exhibition was shown for the public at kulturen i lund , regionmuseet kristianstad and falkenbergs museum .
Head Entity: kulturen
Tail Entity: lund
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "location":
Relation: location  
Context: The conference will take place at the Grand Hotel in Stockholm, which is known for its luxurious accommodations.  
Head Entity: Grand Hotel  
Tail Entity: Stockholm  

Relation: location  
Context: The famous painting was created in the artist's studio located in the heart of Paris.  
Head Entity: artist's studio  
Tail Entity: Paris  
MemoryTrain:  epoch 15, batch     0 | loss: 5.2866168MemoryTrain:  epoch 15, batch     1 | loss: 3.6830655MemoryTrain:  epoch 15, batch     2 | loss: 2.5385670MemoryTrain:  epoch 15, batch     3 | loss: 3.3516581MemoryTrain:  epoch 15, batch     4 | loss: 6.8347834MemoryTrain:  epoch 15, batch     5 | loss: 3.5421998MemoryTrain:  epoch 15, batch     6 | loss: 3.6560874MemoryTrain:  epoch 15, batch     7 | loss: 5.4973150MemoryTrain:  epoch 15, batch     8 | loss: 3.4586302MemoryTrain:  epoch 15, batch     9 | loss: 3.5030253MemoryTrain:  epoch 15, batch    10 | loss: 4.1811970MemoryTrain:  epoch  3, batch    11 | loss: 10.9532202MemoryTrain:  epoch 15, batch     0 | loss: 1.6537159MemoryTrain:  epoch 15, batch     1 | loss: 2.9535416MemoryTrain:  epoch 15, batch     2 | loss: 2.9525843MemoryTrain:  epoch 15, batch     3 | loss: 4.2992951MemoryTrain:  epoch 15, batch     4 | loss: 3.3579392MemoryTrain:  epoch 15, batch     5 | loss: 9.8703678MemoryTrain:  epoch 15, batch     6 | loss: 4.8600633MemoryTrain:  epoch 15, batch     7 | loss: 7.3175627MemoryTrain:  epoch 15, batch     8 | loss: 4.2324301MemoryTrain:  epoch 15, batch     9 | loss: 2.8860436MemoryTrain:  epoch 15, batch    10 | loss: 4.3759974MemoryTrain:  epoch  3, batch    11 | loss: 12.2940987MemoryTrain:  epoch 15, batch     0 | loss: 3.5988685MemoryTrain:  epoch 15, batch     1 | loss: 3.5133923MemoryTrain:  epoch 15, batch     2 | loss: 4.2650049MemoryTrain:  epoch 15, batch     3 | loss: 4.9389896MemoryTrain:  epoch 15, batch     4 | loss: 1.8730702MemoryTrain:  epoch 15, batch     5 | loss: 5.8327787MemoryTrain:  epoch 15, batch     6 | loss: 3.7032037MemoryTrain:  epoch 15, batch     7 | loss: 2.6526835MemoryTrain:  epoch 15, batch     8 | loss: 1.9001538MemoryTrain:  epoch 15, batch     9 | loss: 5.2349494MemoryTrain:  epoch 15, batch    10 | loss: 2.4892330MemoryTrain:  epoch  3, batch    11 | loss: 10.6457583MemoryTrain:  epoch 15, batch     0 | loss: 2.5598598MemoryTrain:  epoch 15, batch     1 | loss: 3.6454697MemoryTrain:  epoch 15, batch     2 | loss: 8.0752678MemoryTrain:  epoch 15, batch     3 | loss: 2.6680922MemoryTrain:  epoch 15, batch     4 | loss: 3.1501012MemoryTrain:  epoch 15, batch     5 | loss: 2.7662621MemoryTrain:  epoch 15, batch     6 | loss: 2.2435250MemoryTrain:  epoch 15, batch     7 | loss: 2.1754513MemoryTrain:  epoch 15, batch     8 | loss: 4.4920461MemoryTrain:  epoch 15, batch     9 | loss: 4.7906793MemoryTrain:  epoch 15, batch    10 | loss: 6.3102159MemoryTrain:  epoch  3, batch    11 | loss: 11.3379189MemoryTrain:  epoch 15, batch     0 | loss: 3.8317500MemoryTrain:  epoch 15, batch     1 | loss: 2.5455906MemoryTrain:  epoch 15, batch     2 | loss: 2.7490972MemoryTrain:  epoch 15, batch     3 | loss: 2.3846675MemoryTrain:  epoch 15, batch     4 | loss: 2.9741416MemoryTrain:  epoch 15, batch     5 | loss: 2.3502462MemoryTrain:  epoch 15, batch     6 | loss: 4.7684281MemoryTrain:  epoch 15, batch     7 | loss: 2.1379925MemoryTrain:  epoch 15, batch     8 | loss: 3.4199662MemoryTrain:  epoch 15, batch     9 | loss: 1.9017139MemoryTrain:  epoch 15, batch    10 | loss: 6.9596118MemoryTrain:  epoch  3, batch    11 | loss: 10.3224533MemoryTrain:  epoch 15, batch     0 | loss: 2.7054433MemoryTrain:  epoch 15, batch     1 | loss: 1.8320270MemoryTrain:  epoch 15, batch     2 | loss: 2.2859423MemoryTrain:  epoch 15, batch     3 | loss: 1.9227852MemoryTrain:  epoch 15, batch     4 | loss: 4.2164686MemoryTrain:  epoch 15, batch     5 | loss: 2.3966679MemoryTrain:  epoch 15, batch     6 | loss: 1.8479307MemoryTrain:  epoch 15, batch     7 | loss: 2.2710681MemoryTrain:  epoch 15, batch     8 | loss: 2.6745859MemoryTrain:  epoch 15, batch     9 | loss: 3.1557091MemoryTrain:  epoch 15, batch    10 | loss: 3.1004499MemoryTrain:  epoch  3, batch    11 | loss: 10.3145410MemoryTrain:  epoch 15, batch     0 | loss: 2.0986263MemoryTrain:  epoch 15, batch     1 | loss: 1.6904097MemoryTrain:  epoch 15, batch     2 | loss: 3.8646882MemoryTrain:  epoch 15, batch     3 | loss: 2.3193188MemoryTrain:  epoch 15, batch     4 | loss: 2.2766248MemoryTrain:  epoch 15, batch     5 | loss: 2.2029052MemoryTrain:  epoch 15, batch     6 | loss: 2.2288671MemoryTrain:  epoch 15, batch     7 | loss: 1.9106899MemoryTrain:  epoch 15, batch     8 | loss: 2.0800601MemoryTrain:  epoch 15, batch     9 | loss: 1.8325002MemoryTrain:  epoch 15, batch    10 | loss: 3.8275851MemoryTrain:  epoch  3, batch    11 | loss: 9.8112507MemoryTrain:  epoch 15, batch     0 | loss: 2.3204783MemoryTrain:  epoch 15, batch     1 | loss: 1.9019654MemoryTrain:  epoch 15, batch     2 | loss: 2.4292998MemoryTrain:  epoch 15, batch     3 | loss: 1.4825882MemoryTrain:  epoch 15, batch     4 | loss: 2.0191034MemoryTrain:  epoch 15, batch     5 | loss: 2.4268466MemoryTrain:  epoch 15, batch     6 | loss: 2.0987716MemoryTrain:  epoch 15, batch     7 | loss: 1.6623274MemoryTrain:  epoch 15, batch     8 | loss: 2.0735213MemoryTrain:  epoch 15, batch     9 | loss: 2.6901641MemoryTrain:  epoch 15, batch    10 | loss: 1.8528995MemoryTrain:  epoch  3, batch    11 | loss: 9.9324216MemoryTrain:  epoch 15, batch     0 | loss: 2.6992397MemoryTrain:  epoch 15, batch     1 | loss: 2.0579713MemoryTrain:  epoch 15, batch     2 | loss: 1.4213928MemoryTrain:  epoch 15, batch     3 | loss: 1.9429748MemoryTrain:  epoch 15, batch     4 | loss: 4.5142336MemoryTrain:  epoch 15, batch     5 | loss: 2.3761064MemoryTrain:  epoch 15, batch     6 | loss: 2.4853495MemoryTrain:  epoch 15, batch     7 | loss: 2.0568321MemoryTrain:  epoch 15, batch     8 | loss: 4.9510875MemoryTrain:  epoch 15, batch     9 | loss: 4.6619615MemoryTrain:  epoch 15, batch    10 | loss: 1.4595691MemoryTrain:  epoch  3, batch    11 | loss: 10.7448673MemoryTrain:  epoch 15, batch     0 | loss: 1.9229041MemoryTrain:  epoch 15, batch     1 | loss: 2.4573007MemoryTrain:  epoch 15, batch     2 | loss: 1.6555346MemoryTrain:  epoch 15, batch     3 | loss: 4.2302857MemoryTrain:  epoch 15, batch     4 | loss: 4.3881870MemoryTrain:  epoch 15, batch     5 | loss: 1.4566716MemoryTrain:  epoch 15, batch     6 | loss: 3.1786656MemoryTrain:  epoch 15, batch     7 | loss: 1.9496398MemoryTrain:  epoch 15, batch     8 | loss: 1.3863129MemoryTrain:  epoch 15, batch     9 | loss: 1.5859901MemoryTrain:  epoch 15, batch    10 | loss: 3.7832978MemoryTrain:  epoch  3, batch    11 | loss: 10.5992142
[EVAL] batch:    0 | acc: 31.25%,  total acc: 31.25%   [EVAL] batch:    1 | acc: 31.25%,  total acc: 31.25%   [EVAL] batch:    2 | acc: 37.50%,  total acc: 33.33%   [EVAL] batch:    3 | acc: 31.25%,  total acc: 32.81%   [EVAL] batch:    4 | acc: 62.50%,  total acc: 38.75%   [EVAL] batch:    5 | acc: 43.75%,  total acc: 39.58%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 45.54%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 50.78%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 55.56%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 59.38%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 62.50%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 65.10%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 66.83%   [EVAL] batch:   13 | acc: 81.25%,  total acc: 67.86%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 69.58%   [EVAL] batch:   15 | acc: 93.75%,  total acc: 71.09%   [EVAL] batch:   16 | acc: 87.50%,  total acc: 72.06%   [EVAL] batch:   17 | acc: 75.00%,  total acc: 72.22%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 71.71%   [EVAL] batch:   19 | acc: 25.00%,  total acc: 69.38%   [EVAL] batch:   20 | acc: 6.25%,  total acc: 66.37%   [EVAL] batch:   21 | acc: 0.00%,  total acc: 63.35%   [EVAL] batch:   22 | acc: 31.25%,  total acc: 61.96%   [EVAL] batch:   23 | acc: 6.25%,  total acc: 59.64%   [EVAL] batch:   24 | acc: 31.25%,  total acc: 58.50%   [EVAL] batch:   25 | acc: 6.25%,  total acc: 56.49%   [EVAL] batch:   26 | acc: 12.50%,  total acc: 54.86%   [EVAL] batch:   27 | acc: 18.75%,  total acc: 53.57%   [EVAL] batch:   28 | acc: 6.25%,  total acc: 51.94%   [EVAL] batch:   29 | acc: 18.75%,  total acc: 50.83%   [EVAL] batch:   30 | acc: 6.25%,  total acc: 49.40%   [EVAL] batch:   31 | acc: 68.75%,  total acc: 50.00%   [EVAL] batch:   32 | acc: 87.50%,  total acc: 51.14%   [EVAL] batch:   33 | acc: 75.00%,  total acc: 51.84%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 53.21%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 54.17%   [EVAL] batch:   36 | acc: 75.00%,  total acc: 54.73%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 55.59%   [EVAL] batch:   38 | acc: 93.75%,  total acc: 56.57%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 57.66%   [EVAL] batch:   40 | acc: 87.50%,  total acc: 58.38%   [EVAL] batch:   41 | acc: 81.25%,  total acc: 58.93%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 59.88%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 60.65%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 61.39%   [EVAL] batch:   45 | acc: 81.25%,  total acc: 61.82%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 62.23%   [EVAL] batch:   47 | acc: 81.25%,  total acc: 62.63%   [EVAL] batch:   48 | acc: 81.25%,  total acc: 63.01%   [EVAL] batch:   49 | acc: 81.25%,  total acc: 63.38%   [EVAL] batch:   50 | acc: 43.75%,  total acc: 62.99%   [EVAL] batch:   51 | acc: 75.00%,  total acc: 63.22%   [EVAL] batch:   52 | acc: 43.75%,  total acc: 62.85%   [EVAL] batch:   53 | acc: 75.00%,  total acc: 63.08%   [EVAL] batch:   54 | acc: 62.50%,  total acc: 63.07%   [EVAL] batch:   55 | acc: 68.75%,  total acc: 63.17%   [EVAL] batch:   56 | acc: 31.25%,  total acc: 62.61%   [EVAL] batch:   57 | acc: 31.25%,  total acc: 62.07%   [EVAL] batch:   58 | acc: 37.50%,  total acc: 61.65%   [EVAL] batch:   59 | acc: 43.75%,  total acc: 61.35%   [EVAL] batch:   60 | acc: 50.00%,  total acc: 61.17%   [EVAL] batch:   61 | acc: 37.50%,  total acc: 60.79%   [EVAL] batch:   62 | acc: 18.75%,  total acc: 60.12%   
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 79.17%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 76.56%   [EVAL] batch:    4 | acc: 62.50%,  total acc: 73.75%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 76.04%   [EVAL] batch:    6 | acc: 37.50%,  total acc: 70.54%   [EVAL] batch:    7 | acc: 37.50%,  total acc: 66.41%   [EVAL] batch:    8 | acc: 56.25%,  total acc: 65.28%   [EVAL] batch:    9 | acc: 50.00%,  total acc: 63.75%   [EVAL] batch:   10 | acc: 37.50%,  total acc: 61.36%   [EVAL] batch:   11 | acc: 37.50%,  total acc: 59.38%   [EVAL] batch:   12 | acc: 62.50%,  total acc: 59.62%   [EVAL] batch:   13 | acc: 87.50%,  total acc: 61.61%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 63.75%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 66.02%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 68.01%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 69.44%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 70.72%   [EVAL] batch:   19 | acc: 68.75%,  total acc: 70.62%   [EVAL] batch:   20 | acc: 81.25%,  total acc: 71.13%   [EVAL] batch:   21 | acc: 68.75%,  total acc: 71.02%   [EVAL] batch:   22 | acc: 68.75%,  total acc: 70.92%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 71.61%   [EVAL] batch:   24 | acc: 75.00%,  total acc: 71.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 72.84%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 73.84%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 74.78%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 75.43%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 76.25%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 77.02%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 77.54%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 78.22%   [EVAL] batch:   33 | acc: 75.00%,  total acc: 78.12%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 78.04%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 78.30%   [EVAL] batch:   36 | acc: 93.75%,  total acc: 78.72%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 79.11%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 79.65%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 80.16%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 80.64%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 81.10%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 81.40%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 81.53%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 81.67%   [EVAL] batch:   45 | acc: 75.00%,  total acc: 81.52%   [EVAL] batch:   46 | acc: 62.50%,  total acc: 81.12%   [EVAL] batch:   47 | acc: 75.00%,  total acc: 80.99%   [EVAL] batch:   48 | acc: 87.50%,  total acc: 81.12%   [EVAL] batch:   49 | acc: 68.75%,  total acc: 80.88%   [EVAL] batch:   50 | acc: 81.25%,  total acc: 80.88%   [EVAL] batch:   51 | acc: 75.00%,  total acc: 80.77%   [EVAL] batch:   52 | acc: 87.50%,  total acc: 80.90%   [EVAL] batch:   53 | acc: 62.50%,  total acc: 80.56%   [EVAL] batch:   54 | acc: 56.25%,  total acc: 80.11%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 80.25%   [EVAL] batch:   56 | acc: 62.50%,  total acc: 79.93%   [EVAL] batch:   57 | acc: 43.75%,  total acc: 79.31%   [EVAL] batch:   58 | acc: 68.75%,  total acc: 79.13%   [EVAL] batch:   59 | acc: 87.50%,  total acc: 79.27%   [EVAL] batch:   60 | acc: 87.50%,  total acc: 79.41%   [EVAL] batch:   61 | acc: 75.00%,  total acc: 79.33%   [EVAL] batch:   62 | acc: 68.75%,  total acc: 79.17%   [EVAL] batch:   63 | acc: 43.75%,  total acc: 78.61%   [EVAL] batch:   64 | acc: 50.00%,  total acc: 78.17%   [EVAL] batch:   65 | acc: 50.00%,  total acc: 77.75%   [EVAL] batch:   66 | acc: 43.75%,  total acc: 77.24%   [EVAL] batch:   67 | acc: 50.00%,  total acc: 76.84%   [EVAL] batch:   68 | acc: 50.00%,  total acc: 76.45%   [EVAL] batch:   69 | acc: 75.00%,  total acc: 76.43%   [EVAL] batch:   70 | acc: 56.25%,  total acc: 76.14%   [EVAL] batch:   71 | acc: 50.00%,  total acc: 75.78%   [EVAL] batch:   72 | acc: 75.00%,  total acc: 75.77%   [EVAL] batch:   73 | acc: 68.75%,  total acc: 75.68%   [EVAL] batch:   74 | acc: 75.00%,  total acc: 75.67%   [EVAL] batch:   75 | acc: 93.75%,  total acc: 75.90%   [EVAL] batch:   76 | acc: 75.00%,  total acc: 75.89%   [EVAL] batch:   77 | acc: 68.75%,  total acc: 75.80%   [EVAL] batch:   78 | acc: 87.50%,  total acc: 75.95%   [EVAL] batch:   79 | acc: 87.50%,  total acc: 76.09%   [EVAL] batch:   80 | acc: 100.00%,  total acc: 76.39%   [EVAL] batch:   81 | acc: 31.25%,  total acc: 75.84%   [EVAL] batch:   82 | acc: 31.25%,  total acc: 75.30%   [EVAL] batch:   83 | acc: 31.25%,  total acc: 74.78%   [EVAL] batch:   84 | acc: 25.00%,  total acc: 74.19%   [EVAL] batch:   85 | acc: 37.50%,  total acc: 73.76%   [EVAL] batch:   86 | acc: 25.00%,  total acc: 73.20%   [EVAL] batch:   87 | acc: 50.00%,  total acc: 72.94%   [EVAL] batch:   88 | acc: 93.75%,  total acc: 73.17%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 73.47%   [EVAL] batch:   90 | acc: 93.75%,  total acc: 73.70%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 73.98%   [EVAL] batch:   92 | acc: 87.50%,  total acc: 74.13%   [EVAL] batch:   93 | acc: 100.00%,  total acc: 74.40%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 74.67%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 74.93%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 75.19%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 75.45%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 75.69%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 75.94%   [EVAL] batch:  100 | acc: 93.75%,  total acc: 76.11%   [EVAL] batch:  101 | acc: 93.75%,  total acc: 76.29%   [EVAL] batch:  102 | acc: 81.25%,  total acc: 76.33%   [EVAL] batch:  103 | acc: 81.25%,  total acc: 76.38%   [EVAL] batch:  104 | acc: 100.00%,  total acc: 76.61%   [EVAL] batch:  105 | acc: 93.75%,  total acc: 76.77%   [EVAL] batch:  106 | acc: 37.50%,  total acc: 76.40%   [EVAL] batch:  107 | acc: 18.75%,  total acc: 75.87%   [EVAL] batch:  108 | acc: 18.75%,  total acc: 75.34%   [EVAL] batch:  109 | acc: 18.75%,  total acc: 74.83%   [EVAL] batch:  110 | acc: 56.25%,  total acc: 74.66%   [EVAL] batch:  111 | acc: 25.00%,  total acc: 74.22%   [EVAL] batch:  112 | acc: 62.50%,  total acc: 74.12%   [EVAL] batch:  113 | acc: 87.50%,  total acc: 74.23%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 74.46%   [EVAL] batch:  115 | acc: 87.50%,  total acc: 74.57%   [EVAL] batch:  116 | acc: 93.75%,  total acc: 74.73%   [EVAL] batch:  117 | acc: 93.75%,  total acc: 74.89%   [EVAL] batch:  118 | acc: 62.50%,  total acc: 74.79%   [EVAL] batch:  119 | acc: 0.00%,  total acc: 74.17%   [EVAL] batch:  120 | acc: 12.50%,  total acc: 73.66%   [EVAL] batch:  121 | acc: 0.00%,  total acc: 73.05%   [EVAL] batch:  122 | acc: 0.00%,  total acc: 72.46%   [EVAL] batch:  123 | acc: 0.00%,  total acc: 71.88%   [EVAL] batch:  124 | acc: 0.00%,  total acc: 71.30%   [EVAL] batch:  125 | acc: 81.25%,  total acc: 71.38%   [EVAL] batch:  126 | acc: 75.00%,  total acc: 71.41%   [EVAL] batch:  127 | acc: 62.50%,  total acc: 71.34%   [EVAL] batch:  128 | acc: 50.00%,  total acc: 71.17%   [EVAL] batch:  129 | acc: 81.25%,  total acc: 71.25%   [EVAL] batch:  130 | acc: 75.00%,  total acc: 71.28%   [EVAL] batch:  131 | acc: 87.50%,  total acc: 71.40%   [EVAL] batch:  132 | acc: 93.75%,  total acc: 71.57%   [EVAL] batch:  133 | acc: 81.25%,  total acc: 71.64%   [EVAL] batch:  134 | acc: 68.75%,  total acc: 71.62%   [EVAL] batch:  135 | acc: 87.50%,  total acc: 71.74%   [EVAL] batch:  136 | acc: 87.50%,  total acc: 71.85%   [EVAL] batch:  137 | acc: 81.25%,  total acc: 71.92%   [EVAL] batch:  138 | acc: 56.25%,  total acc: 71.81%   [EVAL] batch:  139 | acc: 81.25%,  total acc: 71.88%   [EVAL] batch:  140 | acc: 87.50%,  total acc: 71.99%   [EVAL] batch:  141 | acc: 87.50%,  total acc: 72.10%   [EVAL] batch:  142 | acc: 68.75%,  total acc: 72.07%   [EVAL] batch:  143 | acc: 81.25%,  total acc: 72.14%   [EVAL] batch:  144 | acc: 31.25%,  total acc: 71.85%   [EVAL] batch:  145 | acc: 25.00%,  total acc: 71.53%   [EVAL] batch:  146 | acc: 25.00%,  total acc: 71.22%   [EVAL] batch:  147 | acc: 12.50%,  total acc: 70.82%   [EVAL] batch:  148 | acc: 37.50%,  total acc: 70.60%   [EVAL] batch:  149 | acc: 25.00%,  total acc: 70.29%   [EVAL] batch:  150 | acc: 81.25%,  total acc: 70.36%   [EVAL] batch:  151 | acc: 87.50%,  total acc: 70.48%   [EVAL] batch:  152 | acc: 75.00%,  total acc: 70.51%   [EVAL] batch:  153 | acc: 100.00%,  total acc: 70.70%   [EVAL] batch:  154 | acc: 75.00%,  total acc: 70.73%   [EVAL] batch:  155 | acc: 56.25%,  total acc: 70.63%   [EVAL] batch:  156 | acc: 56.25%,  total acc: 70.54%   [EVAL] batch:  157 | acc: 50.00%,  total acc: 70.41%   [EVAL] batch:  158 | acc: 50.00%,  total acc: 70.28%   [EVAL] batch:  159 | acc: 50.00%,  total acc: 70.16%   [EVAL] batch:  160 | acc: 62.50%,  total acc: 70.11%   [EVAL] batch:  161 | acc: 37.50%,  total acc: 69.91%   [EVAL] batch:  162 | acc: 62.50%,  total acc: 69.86%   [EVAL] batch:  163 | acc: 68.75%,  total acc: 69.86%   [EVAL] batch:  164 | acc: 75.00%,  total acc: 69.89%   [EVAL] batch:  165 | acc: 81.25%,  total acc: 69.95%   [EVAL] batch:  166 | acc: 75.00%,  total acc: 69.99%   [EVAL] batch:  167 | acc: 81.25%,  total acc: 70.05%   [EVAL] batch:  168 | acc: 68.75%,  total acc: 70.04%   [EVAL] batch:  169 | acc: 37.50%,  total acc: 69.85%   [EVAL] batch:  170 | acc: 37.50%,  total acc: 69.66%   [EVAL] batch:  171 | acc: 31.25%,  total acc: 69.44%   [EVAL] batch:  172 | acc: 50.00%,  total acc: 69.33%   [EVAL] batch:  173 | acc: 31.25%,  total acc: 69.11%   [EVAL] batch:  174 | acc: 50.00%,  total acc: 69.00%   [EVAL] batch:  175 | acc: 100.00%,  total acc: 69.18%   [EVAL] batch:  176 | acc: 100.00%,  total acc: 69.35%   [EVAL] batch:  177 | acc: 100.00%,  total acc: 69.52%   [EVAL] batch:  178 | acc: 100.00%,  total acc: 69.69%   [EVAL] batch:  179 | acc: 100.00%,  total acc: 69.86%   [EVAL] batch:  180 | acc: 100.00%,  total acc: 70.03%   [EVAL] batch:  181 | acc: 81.25%,  total acc: 70.09%   [EVAL] batch:  182 | acc: 43.75%,  total acc: 69.95%   [EVAL] batch:  183 | acc: 62.50%,  total acc: 69.90%   [EVAL] batch:  184 | acc: 68.75%,  total acc: 69.90%   [EVAL] batch:  185 | acc: 56.25%,  total acc: 69.83%   [EVAL] batch:  186 | acc: 75.00%,  total acc: 69.85%   [EVAL] batch:  187 | acc: 50.00%,  total acc: 69.75%   [EVAL] batch:  188 | acc: 18.75%,  total acc: 69.48%   [EVAL] batch:  189 | acc: 50.00%,  total acc: 69.38%   [EVAL] batch:  190 | acc: 25.00%,  total acc: 69.14%   [EVAL] batch:  191 | acc: 31.25%,  total acc: 68.95%   [EVAL] batch:  192 | acc: 37.50%,  total acc: 68.78%   [EVAL] batch:  193 | acc: 18.75%,  total acc: 68.52%   [EVAL] batch:  194 | acc: 68.75%,  total acc: 68.53%   [EVAL] batch:  195 | acc: 56.25%,  total acc: 68.46%   [EVAL] batch:  196 | acc: 81.25%,  total acc: 68.53%   [EVAL] batch:  197 | acc: 75.00%,  total acc: 68.56%   [EVAL] batch:  198 | acc: 68.75%,  total acc: 68.56%   [EVAL] batch:  199 | acc: 75.00%,  total acc: 68.59%   [EVAL] batch:  200 | acc: 68.75%,  total acc: 68.59%   [EVAL] batch:  201 | acc: 31.25%,  total acc: 68.41%   [EVAL] batch:  202 | acc: 56.25%,  total acc: 68.35%   [EVAL] batch:  203 | acc: 62.50%,  total acc: 68.32%   [EVAL] batch:  204 | acc: 68.75%,  total acc: 68.32%   [EVAL] batch:  205 | acc: 37.50%,  total acc: 68.17%   [EVAL] batch:  206 | acc: 93.75%,  total acc: 68.30%   [EVAL] batch:  207 | acc: 100.00%,  total acc: 68.45%   [EVAL] batch:  208 | acc: 100.00%,  total acc: 68.60%   [EVAL] batch:  209 | acc: 93.75%,  total acc: 68.72%   [EVAL] batch:  210 | acc: 100.00%,  total acc: 68.87%   [EVAL] batch:  211 | acc: 100.00%,  total acc: 69.02%   [EVAL] batch:  212 | acc: 87.50%,  total acc: 69.10%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 69.25%   [EVAL] batch:  214 | acc: 93.75%,  total acc: 69.36%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 69.50%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 69.64%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 69.78%   [EVAL] batch:  218 | acc: 100.00%,  total acc: 69.92%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 70.06%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 70.19%   [EVAL] batch:  221 | acc: 93.75%,  total acc: 70.30%   [EVAL] batch:  222 | acc: 93.75%,  total acc: 70.40%   [EVAL] batch:  223 | acc: 93.75%,  total acc: 70.51%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 70.64%   [EVAL] batch:  225 | acc: 75.00%,  total acc: 70.66%   [EVAL] batch:  226 | acc: 68.75%,  total acc: 70.65%   [EVAL] batch:  227 | acc: 62.50%,  total acc: 70.61%   [EVAL] batch:  228 | acc: 37.50%,  total acc: 70.47%   [EVAL] batch:  229 | acc: 31.25%,  total acc: 70.30%   [EVAL] batch:  230 | acc: 50.00%,  total acc: 70.21%   [EVAL] batch:  231 | acc: 93.75%,  total acc: 70.31%   [EVAL] batch:  232 | acc: 93.75%,  total acc: 70.41%   [EVAL] batch:  233 | acc: 100.00%,  total acc: 70.54%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 70.66%   [EVAL] batch:  235 | acc: 100.00%,  total acc: 70.79%   [EVAL] batch:  236 | acc: 81.25%,  total acc: 70.83%   [EVAL] batch:  237 | acc: 68.75%,  total acc: 70.82%   [EVAL] batch:  238 | acc: 56.25%,  total acc: 70.76%   [EVAL] batch:  239 | acc: 50.00%,  total acc: 70.68%   [EVAL] batch:  240 | acc: 50.00%,  total acc: 70.59%   [EVAL] batch:  241 | acc: 75.00%,  total acc: 70.61%   [EVAL] batch:  242 | acc: 50.00%,  total acc: 70.52%   [EVAL] batch:  243 | acc: 37.50%,  total acc: 70.39%   [EVAL] batch:  244 | acc: 81.25%,  total acc: 70.43%   [EVAL] batch:  245 | acc: 68.75%,  total acc: 70.43%   [EVAL] batch:  246 | acc: 56.25%,  total acc: 70.37%   [EVAL] batch:  247 | acc: 81.25%,  total acc: 70.41%   [EVAL] batch:  248 | acc: 87.50%,  total acc: 70.48%   [EVAL] batch:  249 | acc: 75.00%,  total acc: 70.50%   [EVAL] batch:  250 | acc: 93.75%,  total acc: 70.59%   [EVAL] batch:  251 | acc: 93.75%,  total acc: 70.68%   [EVAL] batch:  252 | acc: 93.75%,  total acc: 70.78%   [EVAL] batch:  253 | acc: 87.50%,  total acc: 70.84%   [EVAL] batch:  254 | acc: 93.75%,  total acc: 70.93%   [EVAL] batch:  255 | acc: 100.00%,  total acc: 71.04%   [EVAL] batch:  256 | acc: 68.75%,  total acc: 71.04%   [EVAL] batch:  257 | acc: 56.25%,  total acc: 70.98%   [EVAL] batch:  258 | acc: 81.25%,  total acc: 71.02%   [EVAL] batch:  259 | acc: 87.50%,  total acc: 71.08%   [EVAL] batch:  260 | acc: 43.75%,  total acc: 70.98%   [EVAL] batch:  261 | acc: 68.75%,  total acc: 70.97%   [EVAL] batch:  262 | acc: 68.75%,  total acc: 70.96%   [EVAL] batch:  263 | acc: 68.75%,  total acc: 70.95%   [EVAL] batch:  264 | acc: 68.75%,  total acc: 70.94%   [EVAL] batch:  265 | acc: 56.25%,  total acc: 70.89%   [EVAL] batch:  266 | acc: 56.25%,  total acc: 70.83%   [EVAL] batch:  267 | acc: 62.50%,  total acc: 70.80%   [EVAL] batch:  268 | acc: 56.25%,  total acc: 70.75%   [EVAL] batch:  269 | acc: 62.50%,  total acc: 70.72%   [EVAL] batch:  270 | acc: 75.00%,  total acc: 70.73%   [EVAL] batch:  271 | acc: 50.00%,  total acc: 70.66%   [EVAL] batch:  272 | acc: 68.75%,  total acc: 70.65%   [EVAL] batch:  273 | acc: 62.50%,  total acc: 70.62%   [EVAL] batch:  274 | acc: 37.50%,  total acc: 70.50%   [EVAL] batch:  275 | acc: 100.00%,  total acc: 70.61%   [EVAL] batch:  276 | acc: 100.00%,  total acc: 70.71%   [EVAL] batch:  277 | acc: 100.00%,  total acc: 70.82%   [EVAL] batch:  278 | acc: 100.00%,  total acc: 70.92%   [EVAL] batch:  279 | acc: 100.00%,  total acc: 71.03%   [EVAL] batch:  280 | acc: 100.00%,  total acc: 71.13%   [EVAL] batch:  281 | acc: 81.25%,  total acc: 71.17%   [EVAL] batch:  282 | acc: 87.50%,  total acc: 71.22%   [EVAL] batch:  283 | acc: 68.75%,  total acc: 71.21%   [EVAL] batch:  284 | acc: 87.50%,  total acc: 71.27%   [EVAL] batch:  285 | acc: 62.50%,  total acc: 71.24%   [EVAL] batch:  286 | acc: 75.00%,  total acc: 71.25%   [EVAL] batch:  287 | acc: 81.25%,  total acc: 71.29%   [EVAL] batch:  288 | acc: 100.00%,  total acc: 71.39%   [EVAL] batch:  289 | acc: 100.00%,  total acc: 71.49%   [EVAL] batch:  290 | acc: 100.00%,  total acc: 71.59%   [EVAL] batch:  291 | acc: 100.00%,  total acc: 71.68%   [EVAL] batch:  292 | acc: 100.00%,  total acc: 71.78%   [EVAL] batch:  293 | acc: 93.75%,  total acc: 71.85%   [EVAL] batch:  294 | acc: 100.00%,  total acc: 71.95%   [EVAL] batch:  295 | acc: 100.00%,  total acc: 72.04%   [EVAL] batch:  296 | acc: 100.00%,  total acc: 72.14%   [EVAL] batch:  297 | acc: 100.00%,  total acc: 72.23%   [EVAL] batch:  298 | acc: 100.00%,  total acc: 72.32%   [EVAL] batch:  299 | acc: 100.00%,  total acc: 72.42%   [EVAL] batch:  300 | acc: 81.25%,  total acc: 72.45%   [EVAL] batch:  301 | acc: 87.50%,  total acc: 72.50%   [EVAL] batch:  302 | acc: 93.75%,  total acc: 72.57%   [EVAL] batch:  303 | acc: 93.75%,  total acc: 72.64%   [EVAL] batch:  304 | acc: 81.25%,  total acc: 72.66%   [EVAL] batch:  305 | acc: 93.75%,  total acc: 72.73%   [EVAL] batch:  306 | acc: 87.50%,  total acc: 72.78%   [EVAL] batch:  307 | acc: 87.50%,  total acc: 72.83%   [EVAL] batch:  308 | acc: 87.50%,  total acc: 72.88%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 72.96%   [EVAL] batch:  310 | acc: 100.00%,  total acc: 73.05%   [EVAL] batch:  311 | acc: 87.50%,  total acc: 73.10%   [EVAL] batch:  312 | acc: 68.75%,  total acc: 73.08%   [EVAL] batch:  313 | acc: 31.25%,  total acc: 72.95%   [EVAL] batch:  314 | acc: 31.25%,  total acc: 72.82%   [EVAL] batch:  315 | acc: 37.50%,  total acc: 72.71%   [EVAL] batch:  316 | acc: 43.75%,  total acc: 72.61%   [EVAL] batch:  317 | acc: 37.50%,  total acc: 72.50%   [EVAL] batch:  318 | acc: 75.00%,  total acc: 72.51%   [EVAL] batch:  319 | acc: 87.50%,  total acc: 72.56%   [EVAL] batch:  320 | acc: 87.50%,  total acc: 72.61%   [EVAL] batch:  321 | acc: 93.75%,  total acc: 72.67%   [EVAL] batch:  322 | acc: 100.00%,  total acc: 72.76%   [EVAL] batch:  323 | acc: 87.50%,  total acc: 72.80%   [EVAL] batch:  324 | acc: 93.75%,  total acc: 72.87%   [EVAL] batch:  325 | acc: 93.75%,  total acc: 72.93%   [EVAL] batch:  326 | acc: 75.00%,  total acc: 72.94%   [EVAL] batch:  327 | acc: 93.75%,  total acc: 73.00%   [EVAL] batch:  328 | acc: 93.75%,  total acc: 73.06%   [EVAL] batch:  329 | acc: 75.00%,  total acc: 73.07%   [EVAL] batch:  330 | acc: 93.75%,  total acc: 73.13%   [EVAL] batch:  331 | acc: 25.00%,  total acc: 72.99%   [EVAL] batch:  332 | acc: 18.75%,  total acc: 72.82%   [EVAL] batch:  333 | acc: 0.00%,  total acc: 72.60%   [EVAL] batch:  334 | acc: 25.00%,  total acc: 72.46%   [EVAL] batch:  335 | acc: 6.25%,  total acc: 72.27%   [EVAL] batch:  336 | acc: 12.50%,  total acc: 72.09%   [EVAL] batch:  337 | acc: 31.25%,  total acc: 71.97%   [EVAL] batch:  338 | acc: 0.00%,  total acc: 71.76%   [EVAL] batch:  339 | acc: 31.25%,  total acc: 71.64%   [EVAL] batch:  340 | acc: 0.00%,  total acc: 71.43%   [EVAL] batch:  341 | acc: 18.75%,  total acc: 71.27%   [EVAL] batch:  342 | acc: 12.50%,  total acc: 71.10%   [EVAL] batch:  343 | acc: 25.00%,  total acc: 70.97%   [EVAL] batch:  344 | acc: 81.25%,  total acc: 71.00%   [EVAL] batch:  345 | acc: 87.50%,  total acc: 71.04%   [EVAL] batch:  346 | acc: 87.50%,  total acc: 71.09%   [EVAL] batch:  347 | acc: 93.75%,  total acc: 71.16%   [EVAL] batch:  348 | acc: 87.50%,  total acc: 71.20%   [EVAL] batch:  349 | acc: 68.75%,  total acc: 71.20%   [EVAL] batch:  350 | acc: 100.00%,  total acc: 71.28%   [EVAL] batch:  351 | acc: 93.75%,  total acc: 71.34%   [EVAL] batch:  352 | acc: 87.50%,  total acc: 71.39%   [EVAL] batch:  353 | acc: 87.50%,  total acc: 71.43%   [EVAL] batch:  354 | acc: 93.75%,  total acc: 71.50%   [EVAL] batch:  355 | acc: 100.00%,  total acc: 71.58%   [EVAL] batch:  356 | acc: 87.50%,  total acc: 71.62%   [EVAL] batch:  357 | acc: 93.75%,  total acc: 71.68%   [EVAL] batch:  358 | acc: 68.75%,  total acc: 71.67%   [EVAL] batch:  359 | acc: 93.75%,  total acc: 71.74%   [EVAL] batch:  360 | acc: 81.25%,  total acc: 71.76%   [EVAL] batch:  361 | acc: 87.50%,  total acc: 71.81%   [EVAL] batch:  362 | acc: 56.25%,  total acc: 71.76%   [EVAL] batch:  363 | acc: 56.25%,  total acc: 71.72%   [EVAL] batch:  364 | acc: 56.25%,  total acc: 71.68%   [EVAL] batch:  365 | acc: 56.25%,  total acc: 71.64%   [EVAL] batch:  366 | acc: 75.00%,  total acc: 71.65%   [EVAL] batch:  367 | acc: 62.50%,  total acc: 71.62%   [EVAL] batch:  368 | acc: 56.25%,  total acc: 71.58%   [EVAL] batch:  369 | acc: 31.25%,  total acc: 71.47%   [EVAL] batch:  370 | acc: 37.50%,  total acc: 71.38%   [EVAL] batch:  371 | acc: 31.25%,  total acc: 71.27%   [EVAL] batch:  372 | acc: 37.50%,  total acc: 71.18%   [EVAL] batch:  373 | acc: 56.25%,  total acc: 71.14%   [EVAL] batch:  374 | acc: 37.50%,  total acc: 71.05%   
cur_acc:  ['0.9524', '0.8284', '0.7798', '0.7192', '0.8889', '0.6012']
his_acc:  ['0.9524', '0.8850', '0.8195', '0.7458', '0.7568', '0.7105']
CurrentTrain: epoch 15, batch     0 | loss: 17.6841302CurrentTrain: epoch 15, batch     1 | loss: 23.5596510CurrentTrain: epoch 15, batch     2 | loss: 16.1859995CurrentTrain: epoch  1, batch     3 | loss: 9.8431279CurrentTrain: epoch 15, batch     0 | loss: 15.7421929CurrentTrain: epoch 15, batch     1 | loss: 14.9924252CurrentTrain: epoch 15, batch     2 | loss: 12.2019469CurrentTrain: epoch  1, batch     3 | loss: 12.4737529CurrentTrain: epoch 15, batch     0 | loss: 10.9671771CurrentTrain: epoch 15, batch     1 | loss: 16.7766538CurrentTrain: epoch 15, batch     2 | loss: 12.4706700CurrentTrain: epoch  1, batch     3 | loss: 9.1134667CurrentTrain: epoch 15, batch     0 | loss: 13.2158972CurrentTrain: epoch 15, batch     1 | loss: 16.8736465CurrentTrain: epoch 15, batch     2 | loss: 13.9881578CurrentTrain: epoch  1, batch     3 | loss: 29.6858637CurrentTrain: epoch 15, batch     0 | loss: 9.9889029CurrentTrain: epoch 15, batch     1 | loss: 10.9143419CurrentTrain: epoch 15, batch     2 | loss: 13.7940339CurrentTrain: epoch  1, batch     3 | loss: 6.2698275CurrentTrain: epoch 15, batch     0 | loss: 11.0811495CurrentTrain: epoch 15, batch     1 | loss: 10.0712981CurrentTrain: epoch 15, batch     2 | loss: 18.7959369CurrentTrain: epoch  1, batch     3 | loss: 6.5320853CurrentTrain: epoch 15, batch     0 | loss: 9.6705890CurrentTrain: epoch 15, batch     1 | loss: 9.6482173CurrentTrain: epoch 15, batch     2 | loss: 25.1240831CurrentTrain: epoch  1, batch     3 | loss: 23.4306128CurrentTrain: epoch 15, batch     0 | loss: 12.0214255CurrentTrain: epoch 15, batch     1 | loss: 11.1058458CurrentTrain: epoch 15, batch     2 | loss: 13.4377540CurrentTrain: epoch  1, batch     3 | loss: 17.5710720CurrentTrain: epoch 15, batch     0 | loss: 15.4280287CurrentTrain: epoch 15, batch     1 | loss: 9.8522039CurrentTrain: epoch 15, batch     2 | loss: 13.2776946CurrentTrain: epoch  1, batch     3 | loss: 5.8587253CurrentTrain: epoch 15, batch     0 | loss: 10.9211205CurrentTrain: epoch 15, batch     1 | loss: 11.9947338CurrentTrain: epoch 15, batch     2 | loss: 15.7050283CurrentTrain: epoch  1, batch     3 | loss: 7.0577417
#### main subject ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "main subject" means primary topic of a work.
Here is an example:
Relation: main subject
Context: bryson 's best known work is his 1985 book " evil angels " which chronicles the story of lindy chamberlain 's trial for murder , following the death of her baby daughter , azaria .
Head Entity: evil angels
Tail Entity: death
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "main subject":
Relation: main subject  
Context: in her groundbreaking 2010 novel "the help," kathryn stockett explores the lives of african american maids working in white households in jackson, mississippi during the early 1960s.  
Head Entity: the help  
Tail Entity: african american maids  

Relation: main subject  
Context: "sapiens: a brief history of humankind" by yuval noah harari provides an overview of the history of our species, examining how biology and history have defined us and enhanced our understanding of the world.  
Head Entity: sapiens: a brief history of humankind  
Tail Entity: biology and history  
#### participating team ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "participating team" means teams that actively takes/took part in an event or process.
Here is an example:
Relation: participating team
Context: he represented romania at euro 2000 , his first major tournament where they beat england 3–2 , and drew 1–1 against germany .
Head Entity: euro 2000
Tail Entity: germany
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "participating team":
Relation: participating team  
Context: The Brazilian national team showcased their skills at the 2014 FIFA World Cup, where they faced off against Germany in the semi-finals.  
Head Entity: 2014 FIFA World Cup  
Tail Entity: Germany  

Relation: participating team  
Context: During the 2021 Tokyo Olympics, the United States women's soccer team competed fiercely, ultimately playing against Canada in the semi-finals.  
Head Entity: 2021 Tokyo Olympics  
Tail Entity: Canada  
#### has part ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "has part" means part of this subject.
Here is an example:
Relation: has part
Context: among the competing artists was previous eurovision song contest entrant robin juhkental who represented estonia in 2010 as part of the group malcolm lincoln .
Head Entity: malcolm lincoln
Tail Entity: robin juhkental
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "has part":
Relation: has part  
Context: The famous painting "The Last Supper" features several apostles, including Judas Iscariot, who is depicted as part of the group gathered around Jesus.  
Head Entity: The Last Supper  
Tail Entity: Judas Iscariot  

Relation: has part  
Context: The smartphone model Galaxy S21 includes various components, such as the Exynos 2100 processor, which is an integral part of its performance.  
Head Entity: Galaxy S21  
Tail Entity: Exynos 2100 processor  
#### nominated for ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "nominated for" means award nomination received by a person, organisation or creative work.
Here is an example:
Relation: nominated for
Context: he was nominated for the academy award for best picture , along with steven spielberg , ian bryce , mark gordon for the film " saving private ryan " .
Head Entity: ian bryce
Tail Entity: academy award for best picture
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "nominated for":
Relation: nominated for  
Context: The film "The Shape of Water" received multiple nominations, including a nod for best director for Guillermo del Toro at the Academy Awards.  
Head Entity: Guillermo del Toro  
Tail Entity: Academy Awards  

Relation: nominated for  
Context: The popular band was nominated for the Grammy Award for Best New Artist after their debut album topped the charts.  
Head Entity: the popular band  
Tail Entity: Grammy Award for Best New Artist  
#### instance of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "instance of" means that class of which this subject is a particular example and member.
Here is an example:
Relation: instance of
Context: bell gothic was replaced by at&t ; with matthew carter 's typeface bell centennial in 1978 , the one hundredth anniversary of at&t ; 's founding .
Head Entity: bell centennial
Tail Entity: typeface
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "instance of":
Relation: instance of  
Context: the smartphone is a type of mobile phone that combines cellular and mobile computing functions into one device.  
Head Entity: smartphone  
Tail Entity: mobile phone  

Relation: instance of  
Context: the golden retriever is a popular breed known for its friendly and tolerant attitude.  
Head Entity: golden retriever  
Tail Entity: breed  
#### mountain range ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "mountain range" means range or subrange to which the geographical item belongs.
Here is an example:
Relation: mountain range
Context: the mount kapaz or kepez ( ) is a mountain in lesser caucasus near ganja city in central azerbaijan .
Head Entity: mount kapaz
Tail Entity: lesser caucasus
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "mountain range":
Relation: mountain range  
Context: the sierra nevada is a mountain range in the western united states, primarily located in the state of california.  
Head Entity: sierra nevada  
Tail Entity: california  

Relation: mountain range  
Context: the alps are a major mountain range in europe, stretching across eight countries including france, italy, and switzerland.  
Head Entity: alps  
Tail Entity: switzerland  
#### screenwriter ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "screenwriter" means person who wrote the script for subject item.
Here is an example:
Relation: screenwriter
Context: it would also be the last episode to feature a grounding , until " the marge - ian chronicles " in season 27 , six years later ( also written by brian kelley ) .
Head Entity: the marge - ian chronicles
Tail Entity: brian kelley
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "screenwriter":
Relation: screenwriter  
Context: The film "Inception" was a groundbreaking project that showcased the visionary talents of its creator, with the screenplay crafted by the brilliant Christopher Nolan.  
Head Entity: Inception  
Tail Entity: Christopher Nolan  

Relation: screenwriter  
Context: The beloved animated feature "Toy Story" was brought to life through the imaginative script penned by the talented Andrew Stanton.  
Head Entity: Toy Story  
Tail Entity: Andrew Stanton  
#### language of work or name ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "language of work or name" means language associated with this creative work (such as books, shows, songs, broadcasts or websites) or a name.
Here is an example:
Relation: language of work or name
Context: the publisher found success in 1993 with tarō gomi 's " everyone poops " , a japanese language import that sold over one million copies and is now their best - known title .
Head Entity: everyone poops
Tail Entity: japanese language
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "language of work or name":
Relation: language of work or name  
Context: The animated series "Avatar: The Last Airbender" features a rich narrative and is primarily produced in English, appealing to a global audience.  
Head Entity: Avatar: The Last Airbender  
Tail Entity: English  

Relation: language of work or name  
Context: The famous novel "Cien años de soledad" by Gabriel García Márquez is a cornerstone of Latin American literature and is originally written in Spanish.  
Head Entity: Cien años de soledad  
Tail Entity: Spanish  
#### operator ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "operator" means person, profession, or organization that operates the equipment, facility, or service.
Here is an example:
Relation: operator
Context: the aircraft was a boeing 707 - 321c that first entered service with pan american world airways ( pan am ) in 1963 .
Head Entity: boeing 707 - 321c
Tail Entity: pan american world airways
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "operator":
Relation: operator  
Context: the new electric bus fleet is managed by the city transportation authority, which ensures efficient public transit services.  
Head Entity: electric bus fleet  
Tail Entity: city transportation authority  

Relation: operator  
Context: the state-of-the-art research facility is operated by the national science foundation, providing resources for groundbreaking studies.  
Head Entity: research facility  
Tail Entity: national science foundation  
#### religion ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "religion" means religion of a person, organization or religious building, or associated with this subject.
Here is an example:
Relation: religion
Context: the most important lama of this series was the third changkya , rolpai dorje , who was preceptor to the qianlong emperor of china , and chief representative of tibetan buddhism at the qing court .
Head Entity: rolpai dorje
Tail Entity: tibetan buddhism
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "religion":
Relation: religion  
Context: the cathedral of notre-dame is a famous example of french gothic architecture and serves as a major center for the roman catholic faith in paris.  
Head Entity: cathedral of notre-dame  
Tail Entity: roman catholic faith  

Relation: religion  
Context: the dalai lama is a prominent figure in the gelug school of tibetan buddhism, advocating for peace and compassion worldwide.  
Head Entity: dalai lama  
Tail Entity: tibetan buddhism  
MemoryTrain:  epoch 15, batch     0 | loss: 2.7724522MemoryTrain:  epoch 15, batch     1 | loss: 2.9816911MemoryTrain:  epoch 15, batch     2 | loss: 2.5416069MemoryTrain:  epoch 15, batch     3 | loss: 2.9042970MemoryTrain:  epoch 15, batch     4 | loss: 4.2509925MemoryTrain:  epoch 15, batch     5 | loss: 3.7689344MemoryTrain:  epoch 15, batch     6 | loss: 6.1879313MemoryTrain:  epoch 15, batch     7 | loss: 3.1598303MemoryTrain:  epoch 15, batch     8 | loss: 2.6404694MemoryTrain:  epoch 15, batch     9 | loss: 3.9376719MemoryTrain:  epoch 15, batch    10 | loss: 4.1304901MemoryTrain:  epoch 15, batch    11 | loss: 5.5431678MemoryTrain:  epoch 15, batch    12 | loss: 5.2673513MemoryTrain:  epoch  1, batch    13 | loss: 6.4888870MemoryTrain:  epoch 15, batch     0 | loss: 2.6886637MemoryTrain:  epoch 15, batch     1 | loss: 2.0143194MemoryTrain:  epoch 15, batch     2 | loss: 3.5827301MemoryTrain:  epoch 15, batch     3 | loss: 4.2792655MemoryTrain:  epoch 15, batch     4 | loss: 2.1858046MemoryTrain:  epoch 15, batch     5 | loss: 3.6914954MemoryTrain:  epoch 15, batch     6 | loss: 2.6099631MemoryTrain:  epoch 15, batch     7 | loss: 4.5844766MemoryTrain:  epoch 15, batch     8 | loss: 4.0585326MemoryTrain:  epoch 15, batch     9 | loss: 2.5655691MemoryTrain:  epoch 15, batch    10 | loss: 3.7405866MemoryTrain:  epoch 15, batch    11 | loss: 2.6877576MemoryTrain:  epoch 15, batch    12 | loss: 5.0402488MemoryTrain:  epoch  1, batch    13 | loss: 6.2392800MemoryTrain:  epoch 15, batch     0 | loss: 4.9177033MemoryTrain:  epoch 15, batch     1 | loss: 3.1185562MemoryTrain:  epoch 15, batch     2 | loss: 2.0506050MemoryTrain:  epoch 15, batch     3 | loss: 2.3328067MemoryTrain:  epoch 15, batch     4 | loss: 2.5867911MemoryTrain:  epoch 15, batch     5 | loss: 2.2565078MemoryTrain:  epoch 15, batch     6 | loss: 2.2546031MemoryTrain:  epoch 15, batch     7 | loss: 5.0671910MemoryTrain:  epoch 15, batch     8 | loss: 2.5365469MemoryTrain:  epoch 15, batch     9 | loss: 3.0554820MemoryTrain:  epoch 15, batch    10 | loss: 4.2813177MemoryTrain:  epoch 15, batch    11 | loss: 2.9483626MemoryTrain:  epoch 15, batch    12 | loss: 2.1639120MemoryTrain:  epoch  1, batch    13 | loss: 7.2971432MemoryTrain:  epoch 15, batch     0 | loss: 2.3264463MemoryTrain:  epoch 15, batch     1 | loss: 1.6661649MemoryTrain:  epoch 15, batch     2 | loss: 3.1839848MemoryTrain:  epoch 15, batch     3 | loss: 2.3977809MemoryTrain:  epoch 15, batch     4 | loss: 1.9915900MemoryTrain:  epoch 15, batch     5 | loss: 2.0115096MemoryTrain:  epoch 15, batch     6 | loss: 1.7748917MemoryTrain:  epoch 15, batch     7 | loss: 3.1153264MemoryTrain:  epoch 15, batch     8 | loss: 2.4973581MemoryTrain:  epoch 15, batch     9 | loss: 2.4823727MemoryTrain:  epoch 15, batch    10 | loss: 2.6838615MemoryTrain:  epoch 15, batch    11 | loss: 1.8375723MemoryTrain:  epoch 15, batch    12 | loss: 10.1703959MemoryTrain:  epoch  1, batch    13 | loss: 9.6093246MemoryTrain:  epoch 15, batch     0 | loss: 2.1371874MemoryTrain:  epoch 15, batch     1 | loss: 1.6116845MemoryTrain:  epoch 15, batch     2 | loss: 2.0027218MemoryTrain:  epoch 15, batch     3 | loss: 2.0859946MemoryTrain:  epoch 15, batch     4 | loss: 3.0826606MemoryTrain:  epoch 15, batch     5 | loss: 2.9403538MemoryTrain:  epoch 15, batch     6 | loss: 1.8894973MemoryTrain:  epoch 15, batch     7 | loss: 3.2482609MemoryTrain:  epoch 15, batch     8 | loss: 3.5271433MemoryTrain:  epoch 15, batch     9 | loss: 1.8223227MemoryTrain:  epoch 15, batch    10 | loss: 1.8642113MemoryTrain:  epoch 15, batch    11 | loss: 1.6482260MemoryTrain:  epoch 15, batch    12 | loss: 2.2216985MemoryTrain:  epoch  1, batch    13 | loss: 6.4938961MemoryTrain:  epoch 15, batch     0 | loss: 2.1182761MemoryTrain:  epoch 15, batch     1 | loss: 2.0759298MemoryTrain:  epoch 15, batch     2 | loss: 2.5627651MemoryTrain:  epoch 15, batch     3 | loss: 3.4482341MemoryTrain:  epoch 15, batch     4 | loss: 1.9843356MemoryTrain:  epoch 15, batch     5 | loss: 5.2457092MemoryTrain:  epoch 15, batch     6 | loss: 4.1358670MemoryTrain:  epoch 15, batch     7 | loss: 1.5317255MemoryTrain:  epoch 15, batch     8 | loss: 4.3132572MemoryTrain:  epoch 15, batch     9 | loss: 2.4677759MemoryTrain:  epoch 15, batch    10 | loss: 1.8512673MemoryTrain:  epoch 15, batch    11 | loss: 1.5673460MemoryTrain:  epoch 15, batch    12 | loss: 4.2271100MemoryTrain:  epoch  1, batch    13 | loss: 4.7012571MemoryTrain:  epoch 15, batch     0 | loss: 2.3807062MemoryTrain:  epoch 15, batch     1 | loss: 4.3884795MemoryTrain:  epoch 15, batch     2 | loss: 1.9136399MemoryTrain:  epoch 15, batch     3 | loss: 3.8830350MemoryTrain:  epoch 15, batch     4 | loss: 2.1112128MemoryTrain:  epoch 15, batch     5 | loss: 2.1854336MemoryTrain:  epoch 15, batch     6 | loss: 2.1985180MemoryTrain:  epoch 15, batch     7 | loss: 1.6937296MemoryTrain:  epoch 15, batch     8 | loss: 1.9498718MemoryTrain:  epoch 15, batch     9 | loss: 2.8237251MemoryTrain:  epoch 15, batch    10 | loss: 1.9111672MemoryTrain:  epoch 15, batch    11 | loss: 4.7243886MemoryTrain:  epoch 15, batch    12 | loss: 1.4954383MemoryTrain:  epoch  1, batch    13 | loss: 9.6458774MemoryTrain:  epoch 15, batch     0 | loss: 1.7257622MemoryTrain:  epoch 15, batch     1 | loss: 4.1932504MemoryTrain:  epoch 15, batch     2 | loss: 1.6533714MemoryTrain:  epoch 15, batch     3 | loss: 2.3335607MemoryTrain:  epoch 15, batch     4 | loss: 1.8677391MemoryTrain:  epoch 15, batch     5 | loss: 2.3161666MemoryTrain:  epoch 15, batch     6 | loss: 1.4493514MemoryTrain:  epoch 15, batch     7 | loss: 1.8510328MemoryTrain:  epoch 15, batch     8 | loss: 4.6694200MemoryTrain:  epoch 15, batch     9 | loss: 1.6748829MemoryTrain:  epoch 15, batch    10 | loss: 2.1863059MemoryTrain:  epoch 15, batch    11 | loss: 4.0243169MemoryTrain:  epoch 15, batch    12 | loss: 3.0965242MemoryTrain:  epoch  1, batch    13 | loss: 5.9354310MemoryTrain:  epoch 15, batch     0 | loss: 1.4810177MemoryTrain:  epoch 15, batch     1 | loss: 1.8597337MemoryTrain:  epoch 15, batch     2 | loss: 2.7438872MemoryTrain:  epoch 15, batch     3 | loss: 2.0119830MemoryTrain:  epoch 15, batch     4 | loss: 1.6537848MemoryTrain:  epoch 15, batch     5 | loss: 1.7321930MemoryTrain:  epoch 15, batch     6 | loss: 3.8783742MemoryTrain:  epoch 15, batch     7 | loss: 2.5874572MemoryTrain:  epoch 15, batch     8 | loss: 3.8237367MemoryTrain:  epoch 15, batch     9 | loss: 1.8741912MemoryTrain:  epoch 15, batch    10 | loss: 2.1422251MemoryTrain:  epoch 15, batch    11 | loss: 1.9277355MemoryTrain:  epoch 15, batch    12 | loss: 1.4380436MemoryTrain:  epoch  1, batch    13 | loss: 5.5425057MemoryTrain:  epoch 15, batch     0 | loss: 2.3131358MemoryTrain:  epoch 15, batch     1 | loss: 4.2746582MemoryTrain:  epoch 15, batch     2 | loss: 2.0244509MemoryTrain:  epoch 15, batch     3 | loss: 1.5700214MemoryTrain:  epoch 15, batch     4 | loss: 3.8289621MemoryTrain:  epoch 15, batch     5 | loss: 1.7786244MemoryTrain:  epoch 15, batch     6 | loss: 1.8807979MemoryTrain:  epoch 15, batch     7 | loss: 4.5442738MemoryTrain:  epoch 15, batch     8 | loss: 1.5064745MemoryTrain:  epoch 15, batch     9 | loss: 1.5568138MemoryTrain:  epoch 15, batch    10 | loss: 1.9667174MemoryTrain:  epoch 15, batch    11 | loss: 1.5179740MemoryTrain:  epoch 15, batch    12 | loss: 1.5051761MemoryTrain:  epoch  1, batch    13 | loss: 6.1657249
[EVAL] batch:    0 | acc: 56.25%,  total acc: 56.25%   [EVAL] batch:    1 | acc: 37.50%,  total acc: 46.88%   [EVAL] batch:    2 | acc: 43.75%,  total acc: 45.83%   [EVAL] batch:    3 | acc: 56.25%,  total acc: 48.44%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 53.75%   [EVAL] batch:    5 | acc: 62.50%,  total acc: 55.21%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 58.04%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 60.94%   [EVAL] batch:    8 | acc: 56.25%,  total acc: 60.42%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 61.88%   [EVAL] batch:   10 | acc: 75.00%,  total acc: 63.07%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 64.06%   [EVAL] batch:   12 | acc: 50.00%,  total acc: 62.98%   [EVAL] batch:   13 | acc: 12.50%,  total acc: 59.38%   [EVAL] batch:   14 | acc: 18.75%,  total acc: 56.67%   [EVAL] batch:   15 | acc: 31.25%,  total acc: 55.08%   [EVAL] batch:   16 | acc: 50.00%,  total acc: 54.78%   [EVAL] batch:   17 | acc: 43.75%,  total acc: 54.17%   [EVAL] batch:   18 | acc: 37.50%,  total acc: 53.29%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 55.62%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 57.74%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 59.38%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 61.14%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 62.76%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 64.25%   [EVAL] batch:   25 | acc: 12.50%,  total acc: 62.26%   [EVAL] batch:   26 | acc: 6.25%,  total acc: 60.19%   [EVAL] batch:   27 | acc: 6.25%,  total acc: 58.26%   [EVAL] batch:   28 | acc: 37.50%,  total acc: 57.54%   [EVAL] batch:   29 | acc: 25.00%,  total acc: 56.46%   [EVAL] batch:   30 | acc: 6.25%,  total acc: 54.84%   [EVAL] batch:   31 | acc: 12.50%,  total acc: 53.52%   [EVAL] batch:   32 | acc: 18.75%,  total acc: 52.46%   [EVAL] batch:   33 | acc: 18.75%,  total acc: 51.47%   [EVAL] batch:   34 | acc: 6.25%,  total acc: 50.18%   [EVAL] batch:   35 | acc: 0.00%,  total acc: 48.78%   [EVAL] batch:   36 | acc: 12.50%,  total acc: 47.80%   [EVAL] batch:   37 | acc: 37.50%,  total acc: 47.53%   [EVAL] batch:   38 | acc: 50.00%,  total acc: 47.60%   [EVAL] batch:   39 | acc: 75.00%,  total acc: 48.28%   [EVAL] batch:   40 | acc: 43.75%,  total acc: 48.17%   [EVAL] batch:   41 | acc: 43.75%,  total acc: 48.07%   [EVAL] batch:   42 | acc: 56.25%,  total acc: 48.26%   [EVAL] batch:   43 | acc: 62.50%,  total acc: 48.58%   [EVAL] batch:   44 | acc: 81.25%,  total acc: 49.31%   [EVAL] batch:   45 | acc: 93.75%,  total acc: 50.27%   [EVAL] batch:   46 | acc: 75.00%,  total acc: 50.80%   [EVAL] batch:   47 | acc: 62.50%,  total acc: 51.04%   [EVAL] batch:   48 | acc: 87.50%,  total acc: 51.79%   [EVAL] batch:   49 | acc: 81.25%,  total acc: 52.38%   [EVAL] batch:   50 | acc: 43.75%,  total acc: 52.21%   [EVAL] batch:   51 | acc: 50.00%,  total acc: 52.16%   [EVAL] batch:   52 | acc: 43.75%,  total acc: 52.00%   [EVAL] batch:   53 | acc: 62.50%,  total acc: 52.20%   [EVAL] batch:   54 | acc: 75.00%,  total acc: 52.61%   [EVAL] batch:   55 | acc: 43.75%,  total acc: 52.46%   [EVAL] batch:   56 | acc: 75.00%,  total acc: 52.85%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 53.45%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 54.03%   [EVAL] batch:   59 | acc: 75.00%,  total acc: 54.37%   [EVAL] batch:   60 | acc: 87.50%,  total acc: 54.92%   [EVAL] batch:   61 | acc: 81.25%,  total acc: 55.34%   [EVAL] batch:   62 | acc: 37.50%,  total acc: 55.06%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 77.08%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 76.56%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 75.00%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 77.08%   [EVAL] batch:    6 | acc: 37.50%,  total acc: 71.43%   [EVAL] batch:    7 | acc: 31.25%,  total acc: 66.41%   [EVAL] batch:    8 | acc: 43.75%,  total acc: 63.89%   [EVAL] batch:    9 | acc: 50.00%,  total acc: 62.50%   [EVAL] batch:   10 | acc: 31.25%,  total acc: 59.66%   [EVAL] batch:   11 | acc: 37.50%,  total acc: 57.81%   [EVAL] batch:   12 | acc: 56.25%,  total acc: 57.69%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 60.27%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 62.50%   [EVAL] batch:   15 | acc: 93.75%,  total acc: 64.45%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 66.54%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 68.06%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 69.41%   [EVAL] batch:   19 | acc: 68.75%,  total acc: 69.38%   [EVAL] batch:   20 | acc: 81.25%,  total acc: 69.94%   [EVAL] batch:   21 | acc: 68.75%,  total acc: 69.89%   [EVAL] batch:   22 | acc: 56.25%,  total acc: 69.29%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 70.31%   [EVAL] batch:   24 | acc: 68.75%,  total acc: 70.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 71.39%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 72.45%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 73.44%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 74.35%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 75.21%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 76.01%   [EVAL] batch:   31 | acc: 87.50%,  total acc: 76.37%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 77.08%   [EVAL] batch:   33 | acc: 75.00%,  total acc: 77.02%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 76.96%   [EVAL] batch:   35 | acc: 75.00%,  total acc: 76.91%   [EVAL] batch:   36 | acc: 87.50%,  total acc: 77.20%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 77.63%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 78.21%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 78.75%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 79.27%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 79.61%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 79.94%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 80.26%   [EVAL] batch:   44 | acc: 81.25%,  total acc: 80.28%   [EVAL] batch:   45 | acc: 75.00%,  total acc: 80.16%   [EVAL] batch:   46 | acc: 56.25%,  total acc: 79.65%   [EVAL] batch:   47 | acc: 62.50%,  total acc: 79.30%   [EVAL] batch:   48 | acc: 87.50%,  total acc: 79.46%   [EVAL] batch:   49 | acc: 75.00%,  total acc: 79.38%   [EVAL] batch:   50 | acc: 81.25%,  total acc: 79.41%   [EVAL] batch:   51 | acc: 81.25%,  total acc: 79.45%   [EVAL] batch:   52 | acc: 81.25%,  total acc: 79.48%   [EVAL] batch:   53 | acc: 62.50%,  total acc: 79.17%   [EVAL] batch:   54 | acc: 56.25%,  total acc: 78.75%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 78.91%   [EVAL] batch:   56 | acc: 62.50%,  total acc: 78.62%   [EVAL] batch:   57 | acc: 43.75%,  total acc: 78.02%   [EVAL] batch:   58 | acc: 56.25%,  total acc: 77.65%   [EVAL] batch:   59 | acc: 81.25%,  total acc: 77.71%   [EVAL] batch:   60 | acc: 81.25%,  total acc: 77.77%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 77.62%   [EVAL] batch:   62 | acc: 68.75%,  total acc: 77.48%   [EVAL] batch:   63 | acc: 37.50%,  total acc: 76.86%   [EVAL] batch:   64 | acc: 50.00%,  total acc: 76.44%   [EVAL] batch:   65 | acc: 50.00%,  total acc: 76.04%   [EVAL] batch:   66 | acc: 43.75%,  total acc: 75.56%   [EVAL] batch:   67 | acc: 43.75%,  total acc: 75.09%   [EVAL] batch:   68 | acc: 50.00%,  total acc: 74.73%   [EVAL] batch:   69 | acc: 68.75%,  total acc: 74.64%   [EVAL] batch:   70 | acc: 68.75%,  total acc: 74.56%   [EVAL] batch:   71 | acc: 50.00%,  total acc: 74.22%   [EVAL] batch:   72 | acc: 62.50%,  total acc: 74.06%   [EVAL] batch:   73 | acc: 68.75%,  total acc: 73.99%   [EVAL] batch:   74 | acc: 68.75%,  total acc: 73.92%   [EVAL] batch:   75 | acc: 75.00%,  total acc: 73.93%   [EVAL] batch:   76 | acc: 87.50%,  total acc: 74.11%   [EVAL] batch:   77 | acc: 68.75%,  total acc: 74.04%   [EVAL] batch:   78 | acc: 93.75%,  total acc: 74.29%   [EVAL] batch:   79 | acc: 81.25%,  total acc: 74.38%   [EVAL] batch:   80 | acc: 87.50%,  total acc: 74.54%   [EVAL] batch:   81 | acc: 25.00%,  total acc: 73.93%   [EVAL] batch:   82 | acc: 18.75%,  total acc: 73.27%   [EVAL] batch:   83 | acc: 18.75%,  total acc: 72.62%   [EVAL] batch:   84 | acc: 6.25%,  total acc: 71.84%   [EVAL] batch:   85 | acc: 25.00%,  total acc: 71.29%   [EVAL] batch:   86 | acc: 18.75%,  total acc: 70.69%   [EVAL] batch:   87 | acc: 43.75%,  total acc: 70.38%   [EVAL] batch:   88 | acc: 93.75%,  total acc: 70.65%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 70.97%   [EVAL] batch:   90 | acc: 81.25%,  total acc: 71.09%   [EVAL] batch:   91 | acc: 93.75%,  total acc: 71.33%   [EVAL] batch:   92 | acc: 87.50%,  total acc: 71.51%   [EVAL] batch:   93 | acc: 93.75%,  total acc: 71.74%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 72.04%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 72.33%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 72.62%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 72.90%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 73.17%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 73.44%   [EVAL] batch:  100 | acc: 93.75%,  total acc: 73.64%   [EVAL] batch:  101 | acc: 93.75%,  total acc: 73.84%   [EVAL] batch:  102 | acc: 81.25%,  total acc: 73.91%   [EVAL] batch:  103 | acc: 87.50%,  total acc: 74.04%   [EVAL] batch:  104 | acc: 100.00%,  total acc: 74.29%   [EVAL] batch:  105 | acc: 93.75%,  total acc: 74.47%   [EVAL] batch:  106 | acc: 37.50%,  total acc: 74.12%   [EVAL] batch:  107 | acc: 18.75%,  total acc: 73.61%   [EVAL] batch:  108 | acc: 25.00%,  total acc: 73.17%   [EVAL] batch:  109 | acc: 18.75%,  total acc: 72.67%   [EVAL] batch:  110 | acc: 62.50%,  total acc: 72.58%   [EVAL] batch:  111 | acc: 37.50%,  total acc: 72.27%   [EVAL] batch:  112 | acc: 56.25%,  total acc: 72.12%   [EVAL] batch:  113 | acc: 87.50%,  total acc: 72.26%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 72.50%   [EVAL] batch:  115 | acc: 87.50%,  total acc: 72.63%   [EVAL] batch:  116 | acc: 93.75%,  total acc: 72.81%   [EVAL] batch:  117 | acc: 93.75%,  total acc: 72.99%   [EVAL] batch:  118 | acc: 62.50%,  total acc: 72.90%   [EVAL] batch:  119 | acc: 0.00%,  total acc: 72.29%   [EVAL] batch:  120 | acc: 12.50%,  total acc: 71.80%   [EVAL] batch:  121 | acc: 0.00%,  total acc: 71.21%   [EVAL] batch:  122 | acc: 0.00%,  total acc: 70.63%   [EVAL] batch:  123 | acc: 0.00%,  total acc: 70.06%   [EVAL] batch:  124 | acc: 0.00%,  total acc: 69.50%   [EVAL] batch:  125 | acc: 68.75%,  total acc: 69.49%   [EVAL] batch:  126 | acc: 50.00%,  total acc: 69.34%   [EVAL] batch:  127 | acc: 50.00%,  total acc: 69.19%   [EVAL] batch:  128 | acc: 31.25%,  total acc: 68.90%   [EVAL] batch:  129 | acc: 62.50%,  total acc: 68.85%   [EVAL] batch:  130 | acc: 62.50%,  total acc: 68.80%   [EVAL] batch:  131 | acc: 87.50%,  total acc: 68.94%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 69.17%   [EVAL] batch:  133 | acc: 81.25%,  total acc: 69.26%   [EVAL] batch:  134 | acc: 75.00%,  total acc: 69.31%   [EVAL] batch:  135 | acc: 87.50%,  total acc: 69.44%   [EVAL] batch:  136 | acc: 100.00%,  total acc: 69.66%   [EVAL] batch:  137 | acc: 81.25%,  total acc: 69.75%   [EVAL] batch:  138 | acc: 62.50%,  total acc: 69.69%   [EVAL] batch:  139 | acc: 75.00%,  total acc: 69.73%   [EVAL] batch:  140 | acc: 87.50%,  total acc: 69.86%   [EVAL] batch:  141 | acc: 87.50%,  total acc: 69.98%   [EVAL] batch:  142 | acc: 68.75%,  total acc: 69.97%   [EVAL] batch:  143 | acc: 81.25%,  total acc: 70.05%   [EVAL] batch:  144 | acc: 18.75%,  total acc: 69.70%   [EVAL] batch:  145 | acc: 25.00%,  total acc: 69.39%   [EVAL] batch:  146 | acc: 25.00%,  total acc: 69.09%   [EVAL] batch:  147 | acc: 0.00%,  total acc: 68.62%   [EVAL] batch:  148 | acc: 25.00%,  total acc: 68.33%   [EVAL] batch:  149 | acc: 18.75%,  total acc: 68.00%   [EVAL] batch:  150 | acc: 68.75%,  total acc: 68.00%   [EVAL] batch:  151 | acc: 75.00%,  total acc: 68.05%   [EVAL] batch:  152 | acc: 75.00%,  total acc: 68.10%   [EVAL] batch:  153 | acc: 100.00%,  total acc: 68.30%   [EVAL] batch:  154 | acc: 68.75%,  total acc: 68.31%   [EVAL] batch:  155 | acc: 43.75%,  total acc: 68.15%   [EVAL] batch:  156 | acc: 50.00%,  total acc: 68.03%   [EVAL] batch:  157 | acc: 50.00%,  total acc: 67.92%   [EVAL] batch:  158 | acc: 50.00%,  total acc: 67.81%   [EVAL] batch:  159 | acc: 43.75%,  total acc: 67.66%   [EVAL] batch:  160 | acc: 62.50%,  total acc: 67.62%   [EVAL] batch:  161 | acc: 43.75%,  total acc: 67.48%   [EVAL] batch:  162 | acc: 56.25%,  total acc: 67.41%   [EVAL] batch:  163 | acc: 62.50%,  total acc: 67.38%   [EVAL] batch:  164 | acc: 75.00%,  total acc: 67.42%   [EVAL] batch:  165 | acc: 68.75%,  total acc: 67.43%   [EVAL] batch:  166 | acc: 75.00%,  total acc: 67.48%   [EVAL] batch:  167 | acc: 81.25%,  total acc: 67.56%   [EVAL] batch:  168 | acc: 68.75%,  total acc: 67.57%   [EVAL] batch:  169 | acc: 37.50%,  total acc: 67.39%   [EVAL] batch:  170 | acc: 37.50%,  total acc: 67.21%   [EVAL] batch:  171 | acc: 31.25%,  total acc: 67.01%   [EVAL] batch:  172 | acc: 50.00%,  total acc: 66.91%   [EVAL] batch:  173 | acc: 31.25%,  total acc: 66.70%   [EVAL] batch:  174 | acc: 50.00%,  total acc: 66.61%   [EVAL] batch:  175 | acc: 100.00%,  total acc: 66.80%   [EVAL] batch:  176 | acc: 100.00%,  total acc: 66.98%   [EVAL] batch:  177 | acc: 100.00%,  total acc: 67.17%   [EVAL] batch:  178 | acc: 100.00%,  total acc: 67.35%   [EVAL] batch:  179 | acc: 100.00%,  total acc: 67.53%   [EVAL] batch:  180 | acc: 100.00%,  total acc: 67.71%   [EVAL] batch:  181 | acc: 81.25%,  total acc: 67.79%   [EVAL] batch:  182 | acc: 50.00%,  total acc: 67.69%   [EVAL] batch:  183 | acc: 62.50%,  total acc: 67.66%   [EVAL] batch:  184 | acc: 68.75%,  total acc: 67.67%   [EVAL] batch:  185 | acc: 62.50%,  total acc: 67.64%   [EVAL] batch:  186 | acc: 75.00%,  total acc: 67.68%   [EVAL] batch:  187 | acc: 56.25%,  total acc: 67.62%   [EVAL] batch:  188 | acc: 12.50%,  total acc: 67.33%   [EVAL] batch:  189 | acc: 37.50%,  total acc: 67.17%   [EVAL] batch:  190 | acc: 25.00%,  total acc: 66.95%   [EVAL] batch:  191 | acc: 25.00%,  total acc: 66.73%   [EVAL] batch:  192 | acc: 18.75%,  total acc: 66.48%   [EVAL] batch:  193 | acc: 18.75%,  total acc: 66.24%   [EVAL] batch:  194 | acc: 68.75%,  total acc: 66.25%   [EVAL] batch:  195 | acc: 62.50%,  total acc: 66.23%   [EVAL] batch:  196 | acc: 81.25%,  total acc: 66.31%   [EVAL] batch:  197 | acc: 75.00%,  total acc: 66.35%   [EVAL] batch:  198 | acc: 68.75%,  total acc: 66.36%   [EVAL] batch:  199 | acc: 75.00%,  total acc: 66.41%   [EVAL] batch:  200 | acc: 68.75%,  total acc: 66.42%   [EVAL] batch:  201 | acc: 31.25%,  total acc: 66.24%   [EVAL] batch:  202 | acc: 56.25%,  total acc: 66.19%   [EVAL] batch:  203 | acc: 62.50%,  total acc: 66.18%   [EVAL] batch:  204 | acc: 62.50%,  total acc: 66.16%   [EVAL] batch:  205 | acc: 37.50%,  total acc: 66.02%   [EVAL] batch:  206 | acc: 93.75%,  total acc: 66.15%   [EVAL] batch:  207 | acc: 100.00%,  total acc: 66.32%   [EVAL] batch:  208 | acc: 100.00%,  total acc: 66.48%   [EVAL] batch:  209 | acc: 87.50%,  total acc: 66.58%   [EVAL] batch:  210 | acc: 100.00%,  total acc: 66.74%   [EVAL] batch:  211 | acc: 100.00%,  total acc: 66.89%   [EVAL] batch:  212 | acc: 87.50%,  total acc: 66.99%   [EVAL] batch:  213 | acc: 81.25%,  total acc: 67.06%   [EVAL] batch:  214 | acc: 81.25%,  total acc: 67.12%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 67.27%   [EVAL] batch:  216 | acc: 75.00%,  total acc: 67.31%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 67.46%   [EVAL] batch:  218 | acc: 93.75%,  total acc: 67.58%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 67.73%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 67.87%   [EVAL] batch:  221 | acc: 93.75%,  total acc: 67.99%   [EVAL] batch:  222 | acc: 93.75%,  total acc: 68.11%   [EVAL] batch:  223 | acc: 93.75%,  total acc: 68.22%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 68.36%   [EVAL] batch:  225 | acc: 68.75%,  total acc: 68.36%   [EVAL] batch:  226 | acc: 68.75%,  total acc: 68.36%   [EVAL] batch:  227 | acc: 62.50%,  total acc: 68.34%   [EVAL] batch:  228 | acc: 43.75%,  total acc: 68.23%   [EVAL] batch:  229 | acc: 50.00%,  total acc: 68.15%   [EVAL] batch:  230 | acc: 68.75%,  total acc: 68.15%   [EVAL] batch:  231 | acc: 93.75%,  total acc: 68.27%   [EVAL] batch:  232 | acc: 93.75%,  total acc: 68.37%   [EVAL] batch:  233 | acc: 100.00%,  total acc: 68.51%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 68.64%   [EVAL] batch:  235 | acc: 100.00%,  total acc: 68.78%   [EVAL] batch:  236 | acc: 87.50%,  total acc: 68.86%   [EVAL] batch:  237 | acc: 62.50%,  total acc: 68.83%   [EVAL] batch:  238 | acc: 37.50%,  total acc: 68.70%   [EVAL] batch:  239 | acc: 25.00%,  total acc: 68.52%   [EVAL] batch:  240 | acc: 18.75%,  total acc: 68.31%   [EVAL] batch:  241 | acc: 62.50%,  total acc: 68.29%   [EVAL] batch:  242 | acc: 37.50%,  total acc: 68.16%   [EVAL] batch:  243 | acc: 37.50%,  total acc: 68.03%   [EVAL] batch:  244 | acc: 75.00%,  total acc: 68.06%   [EVAL] batch:  245 | acc: 56.25%,  total acc: 68.01%   [EVAL] batch:  246 | acc: 62.50%,  total acc: 67.99%   [EVAL] batch:  247 | acc: 68.75%,  total acc: 67.99%   [EVAL] batch:  248 | acc: 93.75%,  total acc: 68.10%   [EVAL] batch:  249 | acc: 81.25%,  total acc: 68.15%   [EVAL] batch:  250 | acc: 93.75%,  total acc: 68.25%   [EVAL] batch:  251 | acc: 93.75%,  total acc: 68.35%   [EVAL] batch:  252 | acc: 93.75%,  total acc: 68.45%   [EVAL] batch:  253 | acc: 87.50%,  total acc: 68.53%   [EVAL] batch:  254 | acc: 93.75%,  total acc: 68.63%   [EVAL] batch:  255 | acc: 100.00%,  total acc: 68.75%   [EVAL] batch:  256 | acc: 50.00%,  total acc: 68.68%   [EVAL] batch:  257 | acc: 18.75%,  total acc: 68.48%   [EVAL] batch:  258 | acc: 37.50%,  total acc: 68.36%   [EVAL] batch:  259 | acc: 56.25%,  total acc: 68.32%   [EVAL] batch:  260 | acc: 18.75%,  total acc: 68.13%   [EVAL] batch:  261 | acc: 43.75%,  total acc: 68.03%   [EVAL] batch:  262 | acc: 50.00%,  total acc: 67.97%   [EVAL] batch:  263 | acc: 68.75%,  total acc: 67.97%   [EVAL] batch:  264 | acc: 56.25%,  total acc: 67.92%   [EVAL] batch:  265 | acc: 43.75%,  total acc: 67.83%   [EVAL] batch:  266 | acc: 56.25%,  total acc: 67.79%   [EVAL] batch:  267 | acc: 56.25%,  total acc: 67.75%   [EVAL] batch:  268 | acc: 56.25%,  total acc: 67.70%   [EVAL] batch:  269 | acc: 56.25%,  total acc: 67.66%   [EVAL] batch:  270 | acc: 68.75%,  total acc: 67.67%   [EVAL] batch:  271 | acc: 43.75%,  total acc: 67.58%   [EVAL] batch:  272 | acc: 56.25%,  total acc: 67.54%   [EVAL] batch:  273 | acc: 75.00%,  total acc: 67.56%   [EVAL] batch:  274 | acc: 31.25%,  total acc: 67.43%   [EVAL] batch:  275 | acc: 100.00%,  total acc: 67.55%   [EVAL] batch:  276 | acc: 100.00%,  total acc: 67.67%   [EVAL] batch:  277 | acc: 100.00%,  total acc: 67.78%   [EVAL] batch:  278 | acc: 100.00%,  total acc: 67.90%   [EVAL] batch:  279 | acc: 100.00%,  total acc: 68.01%   [EVAL] batch:  280 | acc: 100.00%,  total acc: 68.13%   [EVAL] batch:  281 | acc: 87.50%,  total acc: 68.20%   [EVAL] batch:  282 | acc: 87.50%,  total acc: 68.26%   [EVAL] batch:  283 | acc: 62.50%,  total acc: 68.24%   [EVAL] batch:  284 | acc: 87.50%,  total acc: 68.31%   [EVAL] batch:  285 | acc: 62.50%,  total acc: 68.29%   [EVAL] batch:  286 | acc: 75.00%,  total acc: 68.31%   [EVAL] batch:  287 | acc: 81.25%,  total acc: 68.36%   [EVAL] batch:  288 | acc: 100.00%,  total acc: 68.47%   [EVAL] batch:  289 | acc: 100.00%,  total acc: 68.58%   [EVAL] batch:  290 | acc: 100.00%,  total acc: 68.69%   [EVAL] batch:  291 | acc: 100.00%,  total acc: 68.79%   [EVAL] batch:  292 | acc: 100.00%,  total acc: 68.90%   [EVAL] batch:  293 | acc: 93.75%,  total acc: 68.98%   [EVAL] batch:  294 | acc: 100.00%,  total acc: 69.09%   [EVAL] batch:  295 | acc: 100.00%,  total acc: 69.19%   [EVAL] batch:  296 | acc: 100.00%,  total acc: 69.30%   [EVAL] batch:  297 | acc: 100.00%,  total acc: 69.40%   [EVAL] batch:  298 | acc: 100.00%,  total acc: 69.50%   [EVAL] batch:  299 | acc: 100.00%,  total acc: 69.60%   [EVAL] batch:  300 | acc: 87.50%,  total acc: 69.66%   [EVAL] batch:  301 | acc: 87.50%,  total acc: 69.72%   [EVAL] batch:  302 | acc: 93.75%,  total acc: 69.80%   [EVAL] batch:  303 | acc: 93.75%,  total acc: 69.88%   [EVAL] batch:  304 | acc: 81.25%,  total acc: 69.92%   [EVAL] batch:  305 | acc: 93.75%,  total acc: 70.00%   [EVAL] batch:  306 | acc: 93.75%,  total acc: 70.07%   [EVAL] batch:  307 | acc: 93.75%,  total acc: 70.15%   [EVAL] batch:  308 | acc: 93.75%,  total acc: 70.23%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 70.32%   [EVAL] batch:  310 | acc: 100.00%,  total acc: 70.42%   [EVAL] batch:  311 | acc: 87.50%,  total acc: 70.47%   [EVAL] batch:  312 | acc: 62.50%,  total acc: 70.45%   [EVAL] batch:  313 | acc: 37.50%,  total acc: 70.34%   [EVAL] batch:  314 | acc: 31.25%,  total acc: 70.22%   [EVAL] batch:  315 | acc: 31.25%,  total acc: 70.09%   [EVAL] batch:  316 | acc: 31.25%,  total acc: 69.97%   [EVAL] batch:  317 | acc: 25.00%,  total acc: 69.83%   [EVAL] batch:  318 | acc: 68.75%,  total acc: 69.83%   [EVAL] batch:  319 | acc: 87.50%,  total acc: 69.88%   [EVAL] batch:  320 | acc: 87.50%,  total acc: 69.94%   [EVAL] batch:  321 | acc: 93.75%,  total acc: 70.01%   [EVAL] batch:  322 | acc: 100.00%,  total acc: 70.10%   [EVAL] batch:  323 | acc: 81.25%,  total acc: 70.14%   [EVAL] batch:  324 | acc: 93.75%,  total acc: 70.21%   [EVAL] batch:  325 | acc: 75.00%,  total acc: 70.23%   [EVAL] batch:  326 | acc: 56.25%,  total acc: 70.18%   [EVAL] batch:  327 | acc: 68.75%,  total acc: 70.18%   [EVAL] batch:  328 | acc: 81.25%,  total acc: 70.21%   [EVAL] batch:  329 | acc: 62.50%,  total acc: 70.19%   [EVAL] batch:  330 | acc: 81.25%,  total acc: 70.22%   [EVAL] batch:  331 | acc: 18.75%,  total acc: 70.07%   [EVAL] batch:  332 | acc: 6.25%,  total acc: 69.88%   [EVAL] batch:  333 | acc: 0.00%,  total acc: 69.67%   [EVAL] batch:  334 | acc: 25.00%,  total acc: 69.53%   [EVAL] batch:  335 | acc: 6.25%,  total acc: 69.35%   [EVAL] batch:  336 | acc: 12.50%,  total acc: 69.18%   [EVAL] batch:  337 | acc: 25.00%,  total acc: 69.05%   [EVAL] batch:  338 | acc: 0.00%,  total acc: 68.84%   [EVAL] batch:  339 | acc: 18.75%,  total acc: 68.69%   [EVAL] batch:  340 | acc: 0.00%,  total acc: 68.49%   [EVAL] batch:  341 | acc: 12.50%,  total acc: 68.33%   [EVAL] batch:  342 | acc: 6.25%,  total acc: 68.15%   [EVAL] batch:  343 | acc: 18.75%,  total acc: 68.01%   [EVAL] batch:  344 | acc: 81.25%,  total acc: 68.04%   [EVAL] batch:  345 | acc: 87.50%,  total acc: 68.10%   [EVAL] batch:  346 | acc: 93.75%,  total acc: 68.17%   [EVAL] batch:  347 | acc: 93.75%,  total acc: 68.25%   [EVAL] batch:  348 | acc: 81.25%,  total acc: 68.28%   [EVAL] batch:  349 | acc: 68.75%,  total acc: 68.29%   [EVAL] batch:  350 | acc: 93.75%,  total acc: 68.36%   [EVAL] batch:  351 | acc: 93.75%,  total acc: 68.43%   [EVAL] batch:  352 | acc: 87.50%,  total acc: 68.48%   [EVAL] batch:  353 | acc: 87.50%,  total acc: 68.54%   [EVAL] batch:  354 | acc: 93.75%,  total acc: 68.61%   [EVAL] batch:  355 | acc: 93.75%,  total acc: 68.68%   [EVAL] batch:  356 | acc: 93.75%,  total acc: 68.75%   [EVAL] batch:  357 | acc: 93.75%,  total acc: 68.82%   [EVAL] batch:  358 | acc: 68.75%,  total acc: 68.82%   [EVAL] batch:  359 | acc: 81.25%,  total acc: 68.85%   [EVAL] batch:  360 | acc: 81.25%,  total acc: 68.89%   [EVAL] batch:  361 | acc: 87.50%,  total acc: 68.94%   [EVAL] batch:  362 | acc: 56.25%,  total acc: 68.90%   [EVAL] batch:  363 | acc: 56.25%,  total acc: 68.87%   [EVAL] batch:  364 | acc: 43.75%,  total acc: 68.80%   [EVAL] batch:  365 | acc: 62.50%,  total acc: 68.78%   [EVAL] batch:  366 | acc: 75.00%,  total acc: 68.80%   [EVAL] batch:  367 | acc: 56.25%,  total acc: 68.77%   [EVAL] batch:  368 | acc: 56.25%,  total acc: 68.73%   [EVAL] batch:  369 | acc: 37.50%,  total acc: 68.65%   [EVAL] batch:  370 | acc: 37.50%,  total acc: 68.56%   [EVAL] batch:  371 | acc: 43.75%,  total acc: 68.50%   [EVAL] batch:  372 | acc: 37.50%,  total acc: 68.41%   [EVAL] batch:  373 | acc: 43.75%,  total acc: 68.35%   [EVAL] batch:  374 | acc: 37.50%,  total acc: 68.27%   [EVAL] batch:  375 | acc: 56.25%,  total acc: 68.23%   [EVAL] batch:  376 | acc: 37.50%,  total acc: 68.15%   [EVAL] batch:  377 | acc: 43.75%,  total acc: 68.09%   [EVAL] batch:  378 | acc: 56.25%,  total acc: 68.06%   [EVAL] batch:  379 | acc: 75.00%,  total acc: 68.08%   [EVAL] batch:  380 | acc: 62.50%,  total acc: 68.06%   [EVAL] batch:  381 | acc: 75.00%,  total acc: 68.08%   [EVAL] batch:  382 | acc: 81.25%,  total acc: 68.11%   [EVAL] batch:  383 | acc: 56.25%,  total acc: 68.08%   [EVAL] batch:  384 | acc: 75.00%,  total acc: 68.10%   [EVAL] batch:  385 | acc: 75.00%,  total acc: 68.12%   [EVAL] batch:  386 | acc: 75.00%,  total acc: 68.14%   [EVAL] batch:  387 | acc: 50.00%,  total acc: 68.09%   [EVAL] batch:  388 | acc: 12.50%,  total acc: 67.95%   [EVAL] batch:  389 | acc: 18.75%,  total acc: 67.82%   [EVAL] batch:  390 | acc: 31.25%,  total acc: 67.73%   [EVAL] batch:  391 | acc: 50.00%,  total acc: 67.68%   [EVAL] batch:  392 | acc: 43.75%,  total acc: 67.62%   [EVAL] batch:  393 | acc: 37.50%,  total acc: 67.54%   [EVAL] batch:  394 | acc: 100.00%,  total acc: 67.63%   [EVAL] batch:  395 | acc: 100.00%,  total acc: 67.71%   [EVAL] batch:  396 | acc: 93.75%,  total acc: 67.77%   [EVAL] batch:  397 | acc: 100.00%,  total acc: 67.85%   [EVAL] batch:  398 | acc: 100.00%,  total acc: 67.94%   [EVAL] batch:  399 | acc: 100.00%,  total acc: 68.02%   [EVAL] batch:  400 | acc: 12.50%,  total acc: 67.88%   [EVAL] batch:  401 | acc: 6.25%,  total acc: 67.72%   [EVAL] batch:  402 | acc: 6.25%,  total acc: 67.57%   [EVAL] batch:  403 | acc: 37.50%,  total acc: 67.50%   [EVAL] batch:  404 | acc: 25.00%,  total acc: 67.39%   [EVAL] batch:  405 | acc: 6.25%,  total acc: 67.24%   [EVAL] batch:  406 | acc: 12.50%,  total acc: 67.11%   [EVAL] batch:  407 | acc: 18.75%,  total acc: 66.99%   [EVAL] batch:  408 | acc: 18.75%,  total acc: 66.87%   [EVAL] batch:  409 | acc: 6.25%,  total acc: 66.72%   [EVAL] batch:  410 | acc: 0.00%,  total acc: 66.56%   [EVAL] batch:  411 | acc: 12.50%,  total acc: 66.43%   [EVAL] batch:  412 | acc: 37.50%,  total acc: 66.36%   [EVAL] batch:  413 | acc: 50.00%,  total acc: 66.32%   [EVAL] batch:  414 | acc: 75.00%,  total acc: 66.34%   [EVAL] batch:  415 | acc: 43.75%,  total acc: 66.29%   [EVAL] batch:  416 | acc: 43.75%,  total acc: 66.23%   [EVAL] batch:  417 | acc: 56.25%,  total acc: 66.21%   [EVAL] batch:  418 | acc: 62.50%,  total acc: 66.20%   [EVAL] batch:  419 | acc: 81.25%,  total acc: 66.24%   [EVAL] batch:  420 | acc: 93.75%,  total acc: 66.30%   [EVAL] batch:  421 | acc: 75.00%,  total acc: 66.32%   [EVAL] batch:  422 | acc: 62.50%,  total acc: 66.31%   [EVAL] batch:  423 | acc: 87.50%,  total acc: 66.36%   [EVAL] batch:  424 | acc: 81.25%,  total acc: 66.40%   [EVAL] batch:  425 | acc: 43.75%,  total acc: 66.34%   [EVAL] batch:  426 | acc: 50.00%,  total acc: 66.31%   [EVAL] batch:  427 | acc: 43.75%,  total acc: 66.25%   [EVAL] batch:  428 | acc: 62.50%,  total acc: 66.24%   [EVAL] batch:  429 | acc: 75.00%,  total acc: 66.26%   [EVAL] batch:  430 | acc: 43.75%,  total acc: 66.21%   [EVAL] batch:  431 | acc: 75.00%,  total acc: 66.23%   [EVAL] batch:  432 | acc: 87.50%,  total acc: 66.28%   [EVAL] batch:  433 | acc: 87.50%,  total acc: 66.33%   [EVAL] batch:  434 | acc: 75.00%,  total acc: 66.35%   [EVAL] batch:  435 | acc: 87.50%,  total acc: 66.40%   [EVAL] batch:  436 | acc: 81.25%,  total acc: 66.43%   [EVAL] batch:  437 | acc: 37.50%,  total acc: 66.37%   
cur_acc:  ['0.9524', '0.8284', '0.7798', '0.7192', '0.8889', '0.6012', '0.5506']
his_acc:  ['0.9524', '0.8850', '0.8195', '0.7458', '0.7568', '0.7105', '0.6637']
CurrentTrain: epoch 15, batch     0 | loss: 18.8848928CurrentTrain: epoch 15, batch     1 | loss: 14.4378111CurrentTrain: epoch 15, batch     2 | loss: 19.8076566CurrentTrain: epoch  1, batch     3 | loss: 7.2383724CurrentTrain: epoch 15, batch     0 | loss: 14.2463451CurrentTrain: epoch 15, batch     1 | loss: 14.0630264CurrentTrain: epoch 15, batch     2 | loss: 10.9230085CurrentTrain: epoch  1, batch     3 | loss: 7.9935882CurrentTrain: epoch 15, batch     0 | loss: 10.1306253CurrentTrain: epoch 15, batch     1 | loss: 7.9431590CurrentTrain: epoch 15, batch     2 | loss: 14.3235110CurrentTrain: epoch  1, batch     3 | loss: 10.4049120CurrentTrain: epoch 15, batch     0 | loss: 10.9037189CurrentTrain: epoch 15, batch     1 | loss: 19.6938904CurrentTrain: epoch 15, batch     2 | loss: 11.6582481CurrentTrain: epoch  1, batch     3 | loss: 8.4075211CurrentTrain: epoch 15, batch     0 | loss: 11.5308217CurrentTrain: epoch 15, batch     1 | loss: 9.2972231CurrentTrain: epoch 15, batch     2 | loss: 11.3239172CurrentTrain: epoch  1, batch     3 | loss: 6.7249320CurrentTrain: epoch 15, batch     0 | loss: 8.6685349CurrentTrain: epoch 15, batch     1 | loss: 13.4414565CurrentTrain: epoch 15, batch     2 | loss: 12.4338449CurrentTrain: epoch  1, batch     3 | loss: 7.1354823CurrentTrain: epoch 15, batch     0 | loss: 14.7136295CurrentTrain: epoch 15, batch     1 | loss: 16.1716866CurrentTrain: epoch 15, batch     2 | loss: 9.7923216CurrentTrain: epoch  1, batch     3 | loss: 8.3060374CurrentTrain: epoch 15, batch     0 | loss: 10.5171581CurrentTrain: epoch 15, batch     1 | loss: 8.0287228CurrentTrain: epoch 15, batch     2 | loss: 8.9401101CurrentTrain: epoch  1, batch     3 | loss: 6.2779268CurrentTrain: epoch 15, batch     0 | loss: 7.7090316CurrentTrain: epoch 15, batch     1 | loss: 13.0448722CurrentTrain: epoch 15, batch     2 | loss: 9.2585717CurrentTrain: epoch  1, batch     3 | loss: 6.0630044CurrentTrain: epoch 15, batch     0 | loss: 6.6586120CurrentTrain: epoch 15, batch     1 | loss: 15.6855136CurrentTrain: epoch 15, batch     2 | loss: 9.4483369CurrentTrain: epoch  1, batch     3 | loss: 5.9273872
#### contains administrative territorial entity ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "contains administrative territorial entity" means direct subdivisions of an administrative territorial entity.
Here is an example:
Relation: contains administrative territorial entity
Context: the sutagao are the chibcha - speaking indios sutagaos indigenous people from the region of fusagasugá , bogotá savanna , cundinamarca , colombia .
Head Entity: colombia
Tail Entity: cundinamarca
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "contains administrative territorial entity":
Relation: contains administrative territorial entity  
Context: The state of California is known for its diverse geography and is divided into several counties, including Los Angeles County, which is the most populous county in the United States.  
Head Entity: California  
Tail Entity: Los Angeles County  

Relation: contains administrative territorial entity  
Context: The country of Japan is made up of several prefectures, with Tokyo being the most prominent and the capital of the nation.  
Head Entity: Japan  
Tail Entity: Tokyo  
#### said to be the same as ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "said to be the same as" means this item is said to be the same as that item, but it's uncertain or disputed.
Here is an example:
Relation: said to be the same as
Context: enzo is an italian given name derivative of heinz , a form of henry .
Head Entity: henry
Tail Entity: heinz
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "said to be the same as":
Relation: said to be the same as  
Context: The two species, Canis lupus and Canis familiaris, are often said to be the same as each other due to their close genetic relationship.  
Head Entity: Canis lupus  
Tail Entity: Canis familiaris  

Relation: said to be the same as  
Context: Many historians argue that the ancient city of Byzantium is said to be the same as modern-day Istanbul, although the transition involved significant changes over time.  
Head Entity: Byzantium  
Tail Entity: Istanbul  
#### winner ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "winner" means winner of a competition or similar event.
Here is an example:
Relation: winner
Context: in 2010 , ty burrell received a nomination for the television critics association award for individual achievement in comedy , winning in 2011 .
Head Entity: television critics association award for individual achievement in comedy
Tail Entity: ty burrell
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "winner":
Relation: winner  
Context: In 2022, the Boston Red Sox triumphed in the World Series, marking their fourth championship in the last two decades.  
Head Entity: World Series  
Tail Entity: Boston Red Sox  

Relation: winner  
Context: At the annual film festival, the documentary "Voices of Change" was awarded the Best Picture, highlighting its impactful storytelling and direction.  
Head Entity: Best Picture  
Tail Entity: Voices of Change  
#### military rank ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "military rank" means military rank achieved by a person, or military rank associated with a position.
Here is an example:
Relation: military rank
Context: the calais region was defended by the 15th army under " generaloberst " ( colonel general ) hans von salmuth , and normandy by the 7th army commanded by " generaloberst " friedrich dollmann .
Head Entity: hans von salmuth
Tail Entity: generaloberst
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "military rank":
Relation: military rank  
Context: during the battle of stalingrad, the soviet forces were led by general vasily chuikov, who held the rank of major general, while the german troops were commanded by general friedrich paulus.  
Head Entity: vasily chuikov  
Tail Entity: major general  

Relation: military rank  
Context: admiral horatio nelson was a key figure in the british navy during the napoleonic wars, famously holding the rank of vice admiral at the time of the battle of trafalgar.  
Head Entity: horatio nelson  
Tail Entity: vice admiral  
#### publisher ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "publisher" means organization or person responsible for publishing books, periodicals, printed music, podcasts, games or software.
Here is an example:
Relation: publisher
Context: tokyopop also released a collection of karin short stories under the title : " chibi vampire : airmail " on august 31 , 2010 .
Head Entity: karin
Tail Entity: tokyopop
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "publisher":
Relation: publisher  
Context: Penguin Random House announced the release of a new novel by the acclaimed author John Grisham, set to hit the shelves on June 15, 2021.  
Head Entity: John Grisham  
Tail Entity: Penguin Random House  

Relation: publisher  
Context: The popular science magazine, Scientific American, published an article discussing the latest advancements in renewable energy technologies.  
Head Entity: Scientific American  
Tail Entity: renewable energy technologies  
#### followed by ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "followed by" means immediately following item in a series of which the subject is a part.
Here is an example:
Relation: followed by
Context: " muzai moratorium " and " shōso strip " were released on vinyl in 2008 , as a part of her 10-year anniversary celebrations .
Head Entity: muzai moratorium
Tail Entity: shōso strip
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "followed by":
Relation: followed by  
Context: The first chapter of the book introduces the main character, while the second chapter follows with a detailed backstory.  
Head Entity: first chapter  
Tail Entity: second chapter  

Relation: followed by  
Context: In the concert lineup, the opening act performed a lively set, followed by the headliner who captivated the audience with their hits.  
Head Entity: opening act  
Tail Entity: headliner  
#### work location ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "work location" means location where persons or organisations were actively participating in employment, business or other work.
Here is an example:
Relation: work location
Context: in 1694 , he found employment at the court chapel in weimar and was promoted to vice capellmaster ( " " ) in 1695 , succeeding august kühnel , with samuel drese as capellmaster .
Head Entity: august kühnel
Tail Entity: weimar
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "work location":
Relation: work location  
Context: After completing his studies, John accepted a position at the tech startup in Silicon Valley, where he contributed to several innovative projects.  
Head Entity: John  
Tail Entity: Silicon Valley  

Relation: work location  
Context: The renowned artist spent several years in Paris, where he created some of his most famous works and collaborated with other influential figures in the art world.  
Head Entity: the renowned artist  
Tail Entity: Paris  
#### occupation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "occupation" means occupation of a person.
Here is an example:
Relation: occupation
Context: andrei droznin ( russian : андрей дрознин ) is a russian theatre director and movement coach .
Head Entity: andrei droznin
Tail Entity: theatre director
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "occupation":
Relation: occupation  
Context: dr. emily thompson is a renowned biochemist known for her groundbreaking research in cancer treatment.  
Head Entity: dr. emily thompson  
Tail Entity: biochemist  

Relation: occupation  
Context: john smith, a talented software engineer, has been working at tech innovations for over five years.  
Head Entity: john smith  
Tail Entity: software engineer  
#### heritage designation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "heritage designation" means heritage designation of a cultural or natural site.
Here is an example:
Relation: heritage designation
Context: altamaha town , bluffton historic district , church of the cross , and rose hill plantation house are listed on the national register of historic places .
Head Entity: altamaha town
Tail Entity: national register of historic places
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "heritage designation":
Relation: heritage designation  
Context: The Great Wall of China is recognized as a UNESCO World Heritage Site due to its historical significance and architectural grandeur.  
Head Entity: Great Wall of China  
Tail Entity: UNESCO World Heritage Site  

Relation: heritage designation  
Context: The ancient city of Petra in Jordan is designated as a World Heritage Site, attracting tourists from around the globe.  
Head Entity: ancient city of Petra  
Tail Entity: World Heritage Site  
#### voice type ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "voice type" means person's voice type.
Here is an example:
Relation: voice type
Context: beverly bower ( september 30 , 1925 – march 24 , 2002 ) was an american operatic soprano who had an active international opera career from the mid-1950s through the early 1970s .
Head Entity: beverly bower
Tail Entity: soprano
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "voice type":
Relation: voice type  
Context: luciano pavarotti ( october 12 , 1935 – september 6 , 2007 ) was an italian operatic tenor who gained worldwide fame for his televised concerts and media appearances.  
Head Entity: luciano pavarotti  
Tail Entity: tenor  

Relation: voice type  
Context: mariah carey ( born march 27 , 1969 ) is an american singer, songwriter, and actress known for her five-octave vocal range and her use of the whistle register.  
Head Entity: mariah carey  
Tail Entity: soprano  
MemoryTrain:  epoch 15, batch     0 | loss: 3.7652115MemoryTrain:  epoch 15, batch     1 | loss: 3.7426447MemoryTrain:  epoch 15, batch     2 | loss: 3.3603118MemoryTrain:  epoch 15, batch     3 | loss: 2.8251704MemoryTrain:  epoch 15, batch     4 | loss: 2.9034691MemoryTrain:  epoch 15, batch     5 | loss: 4.8968164MemoryTrain:  epoch 15, batch     6 | loss: 3.0180626MemoryTrain:  epoch 15, batch     7 | loss: 2.9304199MemoryTrain:  epoch 15, batch     8 | loss: 2.2806894MemoryTrain:  epoch 15, batch     9 | loss: 2.2085010MemoryTrain:  epoch 15, batch    10 | loss: 2.4598560MemoryTrain:  epoch 15, batch    11 | loss: 2.9029420MemoryTrain:  epoch 15, batch    12 | loss: 4.0579563MemoryTrain:  epoch 15, batch    13 | loss: 4.3723239MemoryTrain:  epoch 15, batch    14 | loss: 3.6316634MemoryTrain:  epoch 15, batch     0 | loss: 3.0033280MemoryTrain:  epoch 15, batch     1 | loss: 2.3023586MemoryTrain:  epoch 15, batch     2 | loss: 3.4830106MemoryTrain:  epoch 15, batch     3 | loss: 4.8278297MemoryTrain:  epoch 15, batch     4 | loss: 2.1173960MemoryTrain:  epoch 15, batch     5 | loss: 3.7690482MemoryTrain:  epoch 15, batch     6 | loss: 2.5891177MemoryTrain:  epoch 15, batch     7 | loss: 3.1835716MemoryTrain:  epoch 15, batch     8 | loss: 5.7974641MemoryTrain:  epoch 15, batch     9 | loss: 2.1080359MemoryTrain:  epoch 15, batch    10 | loss: 2.8052433MemoryTrain:  epoch 15, batch    11 | loss: 3.0625251MemoryTrain:  epoch 15, batch    12 | loss: 2.1406928MemoryTrain:  epoch 15, batch    13 | loss: 2.8490205MemoryTrain:  epoch 15, batch    14 | loss: 3.1309884MemoryTrain:  epoch 15, batch     0 | loss: 2.5743604MemoryTrain:  epoch 15, batch     1 | loss: 2.0513704MemoryTrain:  epoch 15, batch     2 | loss: 2.2561931MemoryTrain:  epoch 15, batch     3 | loss: 2.3489463MemoryTrain:  epoch 15, batch     4 | loss: 1.9588849MemoryTrain:  epoch 15, batch     5 | loss: 2.6633054MemoryTrain:  epoch 15, batch     6 | loss: 4.3948652MemoryTrain:  epoch 15, batch     7 | loss: 1.8397937MemoryTrain:  epoch 15, batch     8 | loss: 1.8284141MemoryTrain:  epoch 15, batch     9 | loss: 3.8951376MemoryTrain:  epoch 15, batch    10 | loss: 1.7001694MemoryTrain:  epoch 15, batch    11 | loss: 1.8427581MemoryTrain:  epoch 15, batch    12 | loss: 4.6290275MemoryTrain:  epoch 15, batch    13 | loss: 2.0935665MemoryTrain:  epoch 15, batch    14 | loss: 2.6712888MemoryTrain:  epoch 15, batch     0 | loss: 2.0893773MemoryTrain:  epoch 15, batch     1 | loss: 2.4529009MemoryTrain:  epoch 15, batch     2 | loss: 5.0023342MemoryTrain:  epoch 15, batch     3 | loss: 1.8164444MemoryTrain:  epoch 15, batch     4 | loss: 1.4635877MemoryTrain:  epoch 15, batch     5 | loss: 1.9524684MemoryTrain:  epoch 15, batch     6 | loss: 2.6160879MemoryTrain:  epoch 15, batch     7 | loss: 3.0288195MemoryTrain:  epoch 15, batch     8 | loss: 1.9372631MemoryTrain:  epoch 15, batch     9 | loss: 2.2389609MemoryTrain:  epoch 15, batch    10 | loss: 1.9221001MemoryTrain:  epoch 15, batch    11 | loss: 2.1539978MemoryTrain:  epoch 15, batch    12 | loss: 2.4131159MemoryTrain:  epoch 15, batch    13 | loss: 2.2303133MemoryTrain:  epoch 15, batch    14 | loss: 3.1546269MemoryTrain:  epoch 15, batch     0 | loss: 1.7538106MemoryTrain:  epoch 15, batch     1 | loss: 1.6084740MemoryTrain:  epoch 15, batch     2 | loss: 2.1556884MemoryTrain:  epoch 15, batch     3 | loss: 1.5260113MemoryTrain:  epoch 15, batch     4 | loss: 2.4825245MemoryTrain:  epoch 15, batch     5 | loss: 1.7825629MemoryTrain:  epoch 15, batch     6 | loss: 1.8128159MemoryTrain:  epoch 15, batch     7 | loss: 1.8064798MemoryTrain:  epoch 15, batch     8 | loss: 2.0078157MemoryTrain:  epoch 15, batch     9 | loss: 2.7929250MemoryTrain:  epoch 15, batch    10 | loss: 2.2945564MemoryTrain:  epoch 15, batch    11 | loss: 1.6285498MemoryTrain:  epoch 15, batch    12 | loss: 1.3212177MemoryTrain:  epoch 15, batch    13 | loss: 1.6125373MemoryTrain:  epoch 15, batch    14 | loss: 2.0297281MemoryTrain:  epoch 15, batch     0 | loss: 3.1488584MemoryTrain:  epoch 15, batch     1 | loss: 1.4562033MemoryTrain:  epoch 15, batch     2 | loss: 1.5713206MemoryTrain:  epoch 15, batch     3 | loss: 1.6628554MemoryTrain:  epoch 15, batch     4 | loss: 1.8430681MemoryTrain:  epoch 15, batch     5 | loss: 8.9675057MemoryTrain:  epoch 15, batch     6 | loss: 1.6167438MemoryTrain:  epoch 15, batch     7 | loss: 1.7387302MemoryTrain:  epoch 15, batch     8 | loss: 4.4037628MemoryTrain:  epoch 15, batch     9 | loss: 1.7502344MemoryTrain:  epoch 15, batch    10 | loss: 1.7341416MemoryTrain:  epoch 15, batch    11 | loss: 2.1017483MemoryTrain:  epoch 15, batch    12 | loss: 1.5427883MemoryTrain:  epoch 15, batch    13 | loss: 1.8397990MemoryTrain:  epoch 15, batch    14 | loss: 2.2627338MemoryTrain:  epoch 15, batch     0 | loss: 1.5733906MemoryTrain:  epoch 15, batch     1 | loss: 1.6946773MemoryTrain:  epoch 15, batch     2 | loss: 4.9916453MemoryTrain:  epoch 15, batch     3 | loss: 2.6378577MemoryTrain:  epoch 15, batch     4 | loss: 4.2127077MemoryTrain:  epoch 15, batch     5 | loss: 1.5256464MemoryTrain:  epoch 15, batch     6 | loss: 3.8747740MemoryTrain:  epoch 15, batch     7 | loss: 1.4565730MemoryTrain:  epoch 15, batch     8 | loss: 1.5885738MemoryTrain:  epoch 15, batch     9 | loss: 1.7912424MemoryTrain:  epoch 15, batch    10 | loss: 1.7652228MemoryTrain:  epoch 15, batch    11 | loss: 2.0140910MemoryTrain:  epoch 15, batch    12 | loss: 1.7487456MemoryTrain:  epoch 15, batch    13 | loss: 3.2138908MemoryTrain:  epoch 15, batch    14 | loss: 2.8805005MemoryTrain:  epoch 15, batch     0 | loss: 2.1991413MemoryTrain:  epoch 15, batch     1 | loss: 4.3397610MemoryTrain:  epoch 15, batch     2 | loss: 2.8977969MemoryTrain:  epoch 15, batch     3 | loss: 2.0781245MemoryTrain:  epoch 15, batch     4 | loss: 1.4299379MemoryTrain:  epoch 15, batch     5 | loss: 1.8710145MemoryTrain:  epoch 15, batch     6 | loss: 1.4130123MemoryTrain:  epoch 15, batch     7 | loss: 2.1802050MemoryTrain:  epoch 15, batch     8 | loss: 1.8225121MemoryTrain:  epoch 15, batch     9 | loss: 1.8642610MemoryTrain:  epoch 15, batch    10 | loss: 1.7838499MemoryTrain:  epoch 15, batch    11 | loss: 1.6265055MemoryTrain:  epoch 15, batch    12 | loss: 3.6231214MemoryTrain:  epoch 15, batch    13 | loss: 4.5535242MemoryTrain:  epoch 15, batch    14 | loss: 1.3631746MemoryTrain:  epoch 15, batch     0 | loss: 4.6794777MemoryTrain:  epoch 15, batch     1 | loss: 1.6831888MemoryTrain:  epoch 15, batch     2 | loss: 4.6033240MemoryTrain:  epoch 15, batch     3 | loss: 1.8661332MemoryTrain:  epoch 15, batch     4 | loss: 1.6311906MemoryTrain:  epoch 15, batch     5 | loss: 4.7117355MemoryTrain:  epoch 15, batch     6 | loss: 1.3772072MemoryTrain:  epoch 15, batch     7 | loss: 1.3882556MemoryTrain:  epoch 15, batch     8 | loss: 2.1309570MemoryTrain:  epoch 15, batch     9 | loss: 2.9387836MemoryTrain:  epoch 15, batch    10 | loss: 3.9621427MemoryTrain:  epoch 15, batch    11 | loss: 3.9641927MemoryTrain:  epoch 15, batch    12 | loss: 1.6500911MemoryTrain:  epoch 15, batch    13 | loss: 1.7415698MemoryTrain:  epoch 15, batch    14 | loss: 1.3306762MemoryTrain:  epoch 15, batch     0 | loss: 1.9298475MemoryTrain:  epoch 15, batch     1 | loss: 1.4837545MemoryTrain:  epoch 15, batch     2 | loss: 1.3091152MemoryTrain:  epoch 15, batch     3 | loss: 4.2182530MemoryTrain:  epoch 15, batch     4 | loss: 1.7834482MemoryTrain:  epoch 15, batch     5 | loss: 1.4084157MemoryTrain:  epoch 15, batch     6 | loss: 1.7471526MemoryTrain:  epoch 15, batch     7 | loss: 3.4608423MemoryTrain:  epoch 15, batch     8 | loss: 1.4215123MemoryTrain:  epoch 15, batch     9 | loss: 2.0034699MemoryTrain:  epoch 15, batch    10 | loss: 1.7070897MemoryTrain:  epoch 15, batch    11 | loss: 4.3542341MemoryTrain:  epoch 15, batch    12 | loss: 2.0912646MemoryTrain:  epoch 15, batch    13 | loss: 1.2770586MemoryTrain:  epoch 15, batch    14 | loss: 2.2187753
[EVAL] batch:    0 | acc: 50.00%,  total acc: 50.00%   [EVAL] batch:    1 | acc: 50.00%,  total acc: 50.00%   [EVAL] batch:    2 | acc: 56.25%,  total acc: 52.08%   [EVAL] batch:    3 | acc: 62.50%,  total acc: 54.69%   [EVAL] batch:    4 | acc: 56.25%,  total acc: 55.00%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 58.33%   [EVAL] batch:    6 | acc: 50.00%,  total acc: 57.14%   [EVAL] batch:    7 | acc: 68.75%,  total acc: 58.59%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 59.72%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 60.00%   [EVAL] batch:   10 | acc: 43.75%,  total acc: 58.52%   [EVAL] batch:   11 | acc: 62.50%,  total acc: 58.85%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 61.54%   [EVAL] batch:   13 | acc: 81.25%,  total acc: 62.95%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 63.75%   [EVAL] batch:   15 | acc: 68.75%,  total acc: 64.06%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 64.71%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 66.32%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 68.09%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 69.06%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 70.54%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 71.88%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 73.10%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 73.96%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:   25 | acc: 37.50%,  total acc: 73.56%   [EVAL] batch:   26 | acc: 31.25%,  total acc: 71.99%   [EVAL] batch:   27 | acc: 43.75%,  total acc: 70.98%   [EVAL] batch:   28 | acc: 50.00%,  total acc: 70.26%   [EVAL] batch:   29 | acc: 18.75%,  total acc: 68.54%   [EVAL] batch:   30 | acc: 43.75%,  total acc: 67.74%   [EVAL] batch:   31 | acc: 56.25%,  total acc: 67.38%   [EVAL] batch:   32 | acc: 56.25%,  total acc: 67.05%   [EVAL] batch:   33 | acc: 31.25%,  total acc: 65.99%   [EVAL] batch:   34 | acc: 31.25%,  total acc: 65.00%   [EVAL] batch:   35 | acc: 12.50%,  total acc: 63.54%   [EVAL] batch:   36 | acc: 18.75%,  total acc: 62.33%   [EVAL] batch:   37 | acc: 62.50%,  total acc: 62.34%   [EVAL] batch:   38 | acc: 68.75%,  total acc: 62.50%   [EVAL] batch:   39 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:   40 | acc: 81.25%,  total acc: 62.96%   [EVAL] batch:   41 | acc: 81.25%,  total acc: 63.39%   [EVAL] batch:   42 | acc: 68.75%,  total acc: 63.52%   [EVAL] batch:   43 | acc: 68.75%,  total acc: 63.64%   [EVAL] batch:   44 | acc: 75.00%,  total acc: 63.89%   [EVAL] batch:   45 | acc: 56.25%,  total acc: 63.72%   [EVAL] batch:   46 | acc: 68.75%,  total acc: 63.83%   [EVAL] batch:   47 | acc: 62.50%,  total acc: 63.80%   [EVAL] batch:   48 | acc: 75.00%,  total acc: 64.03%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 64.50%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 65.20%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 65.87%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 66.51%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 67.13%   [EVAL] batch:   54 | acc: 100.00%,  total acc: 67.73%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 68.30%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 68.75%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 69.07%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 69.39%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 69.79%   [EVAL] batch:   60 | acc: 87.50%,  total acc: 70.08%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 70.56%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 70.24%   
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 75.00%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    4 | acc: 62.50%,  total acc: 72.50%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 75.00%   [EVAL] batch:    6 | acc: 31.25%,  total acc: 68.75%   [EVAL] batch:    7 | acc: 25.00%,  total acc: 63.28%   [EVAL] batch:    8 | acc: 43.75%,  total acc: 61.11%   [EVAL] batch:    9 | acc: 43.75%,  total acc: 59.38%   [EVAL] batch:   10 | acc: 31.25%,  total acc: 56.82%   [EVAL] batch:   11 | acc: 43.75%,  total acc: 55.73%   [EVAL] batch:   12 | acc: 50.00%,  total acc: 55.29%   [EVAL] batch:   13 | acc: 87.50%,  total acc: 57.59%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 60.00%   [EVAL] batch:   15 | acc: 87.50%,  total acc: 61.72%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 63.97%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 65.62%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 67.43%   [EVAL] batch:   19 | acc: 68.75%,  total acc: 67.50%   [EVAL] batch:   20 | acc: 81.25%,  total acc: 68.15%   [EVAL] batch:   21 | acc: 68.75%,  total acc: 68.18%   [EVAL] batch:   22 | acc: 62.50%,  total acc: 67.93%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 69.27%   [EVAL] batch:   24 | acc: 68.75%,  total acc: 69.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 70.43%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 71.53%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 72.54%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 73.28%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 74.17%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 74.80%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 75.39%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 76.14%   [EVAL] batch:   33 | acc: 81.25%,  total acc: 76.29%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 76.25%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 76.56%   [EVAL] batch:   36 | acc: 93.75%,  total acc: 77.03%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 77.63%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 78.21%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 78.75%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 79.27%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 79.61%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 79.94%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 79.97%   [EVAL] batch:   44 | acc: 75.00%,  total acc: 79.86%   [EVAL] batch:   45 | acc: 81.25%,  total acc: 79.89%   [EVAL] batch:   46 | acc: 50.00%,  total acc: 79.26%   [EVAL] batch:   47 | acc: 68.75%,  total acc: 79.04%   [EVAL] batch:   48 | acc: 81.25%,  total acc: 79.08%   [EVAL] batch:   49 | acc: 68.75%,  total acc: 78.88%   [EVAL] batch:   50 | acc: 81.25%,  total acc: 78.92%   [EVAL] batch:   51 | acc: 75.00%,  total acc: 78.85%   [EVAL] batch:   52 | acc: 75.00%,  total acc: 78.77%   [EVAL] batch:   53 | acc: 68.75%,  total acc: 78.59%   [EVAL] batch:   54 | acc: 56.25%,  total acc: 78.18%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 78.35%   [EVAL] batch:   56 | acc: 62.50%,  total acc: 78.07%   [EVAL] batch:   57 | acc: 43.75%,  total acc: 77.48%   [EVAL] batch:   58 | acc: 68.75%,  total acc: 77.33%   [EVAL] batch:   59 | acc: 75.00%,  total acc: 77.29%   [EVAL] batch:   60 | acc: 81.25%,  total acc: 77.36%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 77.22%   [EVAL] batch:   62 | acc: 56.25%,  total acc: 76.88%   [EVAL] batch:   63 | acc: 43.75%,  total acc: 76.37%   [EVAL] batch:   64 | acc: 50.00%,  total acc: 75.96%   [EVAL] batch:   65 | acc: 43.75%,  total acc: 75.47%   [EVAL] batch:   66 | acc: 25.00%,  total acc: 74.72%   [EVAL] batch:   67 | acc: 50.00%,  total acc: 74.36%   [EVAL] batch:   68 | acc: 43.75%,  total acc: 73.91%   [EVAL] batch:   69 | acc: 62.50%,  total acc: 73.75%   [EVAL] batch:   70 | acc: 56.25%,  total acc: 73.50%   [EVAL] batch:   71 | acc: 43.75%,  total acc: 73.09%   [EVAL] batch:   72 | acc: 62.50%,  total acc: 72.95%   [EVAL] batch:   73 | acc: 50.00%,  total acc: 72.64%   [EVAL] batch:   74 | acc: 68.75%,  total acc: 72.58%   [EVAL] batch:   75 | acc: 75.00%,  total acc: 72.62%   [EVAL] batch:   76 | acc: 75.00%,  total acc: 72.65%   [EVAL] batch:   77 | acc: 68.75%,  total acc: 72.60%   [EVAL] batch:   78 | acc: 93.75%,  total acc: 72.86%   [EVAL] batch:   79 | acc: 75.00%,  total acc: 72.89%   [EVAL] batch:   80 | acc: 87.50%,  total acc: 73.07%   [EVAL] batch:   81 | acc: 31.25%,  total acc: 72.56%   [EVAL] batch:   82 | acc: 6.25%,  total acc: 71.76%   [EVAL] batch:   83 | acc: 12.50%,  total acc: 71.06%   [EVAL] batch:   84 | acc: 12.50%,  total acc: 70.37%   [EVAL] batch:   85 | acc: 25.00%,  total acc: 69.84%   [EVAL] batch:   86 | acc: 12.50%,  total acc: 69.18%   [EVAL] batch:   87 | acc: 43.75%,  total acc: 68.89%   [EVAL] batch:   88 | acc: 93.75%,  total acc: 69.17%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 69.51%   [EVAL] batch:   90 | acc: 87.50%,  total acc: 69.71%   [EVAL] batch:   91 | acc: 93.75%,  total acc: 69.97%   [EVAL] batch:   92 | acc: 75.00%,  total acc: 70.03%   [EVAL] batch:   93 | acc: 93.75%,  total acc: 70.28%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 70.59%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 70.90%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 71.20%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 71.49%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 71.78%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 72.06%   [EVAL] batch:  100 | acc: 93.75%,  total acc: 72.28%   [EVAL] batch:  101 | acc: 93.75%,  total acc: 72.49%   [EVAL] batch:  102 | acc: 93.75%,  total acc: 72.69%   [EVAL] batch:  103 | acc: 93.75%,  total acc: 72.90%   [EVAL] batch:  104 | acc: 100.00%,  total acc: 73.15%   [EVAL] batch:  105 | acc: 93.75%,  total acc: 73.35%   [EVAL] batch:  106 | acc: 37.50%,  total acc: 73.01%   [EVAL] batch:  107 | acc: 18.75%,  total acc: 72.51%   [EVAL] batch:  108 | acc: 18.75%,  total acc: 72.02%   [EVAL] batch:  109 | acc: 18.75%,  total acc: 71.53%   [EVAL] batch:  110 | acc: 50.00%,  total acc: 71.34%   [EVAL] batch:  111 | acc: 31.25%,  total acc: 70.98%   [EVAL] batch:  112 | acc: 62.50%,  total acc: 70.91%   [EVAL] batch:  113 | acc: 87.50%,  total acc: 71.05%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 71.30%   [EVAL] batch:  115 | acc: 87.50%,  total acc: 71.44%   [EVAL] batch:  116 | acc: 87.50%,  total acc: 71.58%   [EVAL] batch:  117 | acc: 93.75%,  total acc: 71.77%   [EVAL] batch:  118 | acc: 62.50%,  total acc: 71.69%   [EVAL] batch:  119 | acc: 0.00%,  total acc: 71.09%   [EVAL] batch:  120 | acc: 12.50%,  total acc: 70.61%   [EVAL] batch:  121 | acc: 0.00%,  total acc: 70.03%   [EVAL] batch:  122 | acc: 0.00%,  total acc: 69.46%   [EVAL] batch:  123 | acc: 0.00%,  total acc: 68.90%   [EVAL] batch:  124 | acc: 0.00%,  total acc: 68.35%   [EVAL] batch:  125 | acc: 62.50%,  total acc: 68.30%   [EVAL] batch:  126 | acc: 62.50%,  total acc: 68.26%   [EVAL] batch:  127 | acc: 50.00%,  total acc: 68.12%   [EVAL] batch:  128 | acc: 37.50%,  total acc: 67.88%   [EVAL] batch:  129 | acc: 62.50%,  total acc: 67.84%   [EVAL] batch:  130 | acc: 68.75%,  total acc: 67.84%   [EVAL] batch:  131 | acc: 87.50%,  total acc: 67.99%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 68.23%   [EVAL] batch:  133 | acc: 75.00%,  total acc: 68.28%   [EVAL] batch:  134 | acc: 75.00%,  total acc: 68.33%   [EVAL] batch:  135 | acc: 87.50%,  total acc: 68.47%   [EVAL] batch:  136 | acc: 93.75%,  total acc: 68.66%   [EVAL] batch:  137 | acc: 75.00%,  total acc: 68.70%   [EVAL] batch:  138 | acc: 56.25%,  total acc: 68.62%   [EVAL] batch:  139 | acc: 62.50%,  total acc: 68.57%   [EVAL] batch:  140 | acc: 81.25%,  total acc: 68.66%   [EVAL] batch:  141 | acc: 75.00%,  total acc: 68.71%   [EVAL] batch:  142 | acc: 62.50%,  total acc: 68.66%   [EVAL] batch:  143 | acc: 81.25%,  total acc: 68.75%   [EVAL] batch:  144 | acc: 31.25%,  total acc: 68.49%   [EVAL] batch:  145 | acc: 25.00%,  total acc: 68.19%   [EVAL] batch:  146 | acc: 25.00%,  total acc: 67.90%   [EVAL] batch:  147 | acc: 0.00%,  total acc: 67.44%   [EVAL] batch:  148 | acc: 25.00%,  total acc: 67.16%   [EVAL] batch:  149 | acc: 18.75%,  total acc: 66.83%   [EVAL] batch:  150 | acc: 68.75%,  total acc: 66.85%   [EVAL] batch:  151 | acc: 75.00%,  total acc: 66.90%   [EVAL] batch:  152 | acc: 62.50%,  total acc: 66.87%   [EVAL] batch:  153 | acc: 87.50%,  total acc: 67.00%   [EVAL] batch:  154 | acc: 56.25%,  total acc: 66.94%   [EVAL] batch:  155 | acc: 50.00%,  total acc: 66.83%   [EVAL] batch:  156 | acc: 37.50%,  total acc: 66.64%   [EVAL] batch:  157 | acc: 43.75%,  total acc: 66.50%   [EVAL] batch:  158 | acc: 43.75%,  total acc: 66.35%   [EVAL] batch:  159 | acc: 43.75%,  total acc: 66.21%   [EVAL] batch:  160 | acc: 56.25%,  total acc: 66.15%   [EVAL] batch:  161 | acc: 43.75%,  total acc: 66.01%   [EVAL] batch:  162 | acc: 68.75%,  total acc: 66.03%   [EVAL] batch:  163 | acc: 62.50%,  total acc: 66.01%   [EVAL] batch:  164 | acc: 75.00%,  total acc: 66.06%   [EVAL] batch:  165 | acc: 62.50%,  total acc: 66.04%   [EVAL] batch:  166 | acc: 75.00%,  total acc: 66.09%   [EVAL] batch:  167 | acc: 81.25%,  total acc: 66.18%   [EVAL] batch:  168 | acc: 68.75%,  total acc: 66.20%   [EVAL] batch:  169 | acc: 37.50%,  total acc: 66.03%   [EVAL] batch:  170 | acc: 37.50%,  total acc: 65.86%   [EVAL] batch:  171 | acc: 31.25%,  total acc: 65.66%   [EVAL] batch:  172 | acc: 43.75%,  total acc: 65.53%   [EVAL] batch:  173 | acc: 31.25%,  total acc: 65.34%   [EVAL] batch:  174 | acc: 50.00%,  total acc: 65.25%   [EVAL] batch:  175 | acc: 100.00%,  total acc: 65.45%   [EVAL] batch:  176 | acc: 100.00%,  total acc: 65.64%   [EVAL] batch:  177 | acc: 100.00%,  total acc: 65.84%   [EVAL] batch:  178 | acc: 100.00%,  total acc: 66.03%   [EVAL] batch:  179 | acc: 100.00%,  total acc: 66.22%   [EVAL] batch:  180 | acc: 100.00%,  total acc: 66.40%   [EVAL] batch:  181 | acc: 81.25%,  total acc: 66.48%   [EVAL] batch:  182 | acc: 56.25%,  total acc: 66.43%   [EVAL] batch:  183 | acc: 62.50%,  total acc: 66.41%   [EVAL] batch:  184 | acc: 75.00%,  total acc: 66.45%   [EVAL] batch:  185 | acc: 68.75%,  total acc: 66.47%   [EVAL] batch:  186 | acc: 75.00%,  total acc: 66.51%   [EVAL] batch:  187 | acc: 31.25%,  total acc: 66.32%   [EVAL] batch:  188 | acc: 6.25%,  total acc: 66.01%   [EVAL] batch:  189 | acc: 18.75%,  total acc: 65.76%   [EVAL] batch:  190 | acc: 0.00%,  total acc: 65.41%   [EVAL] batch:  191 | acc: 0.00%,  total acc: 65.07%   [EVAL] batch:  192 | acc: 6.25%,  total acc: 64.77%   [EVAL] batch:  193 | acc: 18.75%,  total acc: 64.53%   [EVAL] batch:  194 | acc: 68.75%,  total acc: 64.55%   [EVAL] batch:  195 | acc: 81.25%,  total acc: 64.64%   [EVAL] batch:  196 | acc: 81.25%,  total acc: 64.72%   [EVAL] batch:  197 | acc: 75.00%,  total acc: 64.77%   [EVAL] batch:  198 | acc: 75.00%,  total acc: 64.82%   [EVAL] batch:  199 | acc: 75.00%,  total acc: 64.88%   [EVAL] batch:  200 | acc: 68.75%,  total acc: 64.89%   [EVAL] batch:  201 | acc: 31.25%,  total acc: 64.73%   [EVAL] batch:  202 | acc: 56.25%,  total acc: 64.69%   [EVAL] batch:  203 | acc: 68.75%,  total acc: 64.71%   [EVAL] batch:  204 | acc: 62.50%,  total acc: 64.70%   [EVAL] batch:  205 | acc: 37.50%,  total acc: 64.56%   [EVAL] batch:  206 | acc: 81.25%,  total acc: 64.64%   [EVAL] batch:  207 | acc: 93.75%,  total acc: 64.78%   [EVAL] batch:  208 | acc: 100.00%,  total acc: 64.95%   [EVAL] batch:  209 | acc: 87.50%,  total acc: 65.06%   [EVAL] batch:  210 | acc: 100.00%,  total acc: 65.23%   [EVAL] batch:  211 | acc: 100.00%,  total acc: 65.39%   [EVAL] batch:  212 | acc: 62.50%,  total acc: 65.38%   [EVAL] batch:  213 | acc: 68.75%,  total acc: 65.39%   [EVAL] batch:  214 | acc: 68.75%,  total acc: 65.41%   [EVAL] batch:  215 | acc: 81.25%,  total acc: 65.48%   [EVAL] batch:  216 | acc: 68.75%,  total acc: 65.50%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 65.65%   [EVAL] batch:  218 | acc: 81.25%,  total acc: 65.72%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 65.88%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 66.04%   [EVAL] batch:  221 | acc: 93.75%,  total acc: 66.16%   [EVAL] batch:  222 | acc: 93.75%,  total acc: 66.28%   [EVAL] batch:  223 | acc: 93.75%,  total acc: 66.41%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 66.56%   [EVAL] batch:  225 | acc: 68.75%,  total acc: 66.57%   [EVAL] batch:  226 | acc: 75.00%,  total acc: 66.60%   [EVAL] batch:  227 | acc: 68.75%,  total acc: 66.61%   [EVAL] batch:  228 | acc: 43.75%,  total acc: 66.51%   [EVAL] batch:  229 | acc: 50.00%,  total acc: 66.44%   [EVAL] batch:  230 | acc: 56.25%,  total acc: 66.40%   [EVAL] batch:  231 | acc: 81.25%,  total acc: 66.46%   [EVAL] batch:  232 | acc: 75.00%,  total acc: 66.50%   [EVAL] batch:  233 | acc: 87.50%,  total acc: 66.59%   [EVAL] batch:  234 | acc: 93.75%,  total acc: 66.70%   [EVAL] batch:  235 | acc: 87.50%,  total acc: 66.79%   [EVAL] batch:  236 | acc: 68.75%,  total acc: 66.80%   [EVAL] batch:  237 | acc: 62.50%,  total acc: 66.78%   [EVAL] batch:  238 | acc: 56.25%,  total acc: 66.74%   [EVAL] batch:  239 | acc: 62.50%,  total acc: 66.72%   [EVAL] batch:  240 | acc: 43.75%,  total acc: 66.62%   [EVAL] batch:  241 | acc: 75.00%,  total acc: 66.66%   [EVAL] batch:  242 | acc: 43.75%,  total acc: 66.56%   [EVAL] batch:  243 | acc: 37.50%,  total acc: 66.44%   [EVAL] batch:  244 | acc: 68.75%,  total acc: 66.45%   [EVAL] batch:  245 | acc: 56.25%,  total acc: 66.41%   [EVAL] batch:  246 | acc: 50.00%,  total acc: 66.35%   [EVAL] batch:  247 | acc: 68.75%,  total acc: 66.36%   [EVAL] batch:  248 | acc: 87.50%,  total acc: 66.44%   [EVAL] batch:  249 | acc: 68.75%,  total acc: 66.45%   [EVAL] batch:  250 | acc: 93.75%,  total acc: 66.56%   [EVAL] batch:  251 | acc: 93.75%,  total acc: 66.67%   [EVAL] batch:  252 | acc: 87.50%,  total acc: 66.75%   [EVAL] batch:  253 | acc: 81.25%,  total acc: 66.81%   [EVAL] batch:  254 | acc: 93.75%,  total acc: 66.91%   [EVAL] batch:  255 | acc: 100.00%,  total acc: 67.04%   [EVAL] batch:  256 | acc: 56.25%,  total acc: 67.00%   [EVAL] batch:  257 | acc: 25.00%,  total acc: 66.84%   [EVAL] batch:  258 | acc: 43.75%,  total acc: 66.75%   [EVAL] batch:  259 | acc: 56.25%,  total acc: 66.71%   [EVAL] batch:  260 | acc: 25.00%,  total acc: 66.55%   [EVAL] batch:  261 | acc: 37.50%,  total acc: 66.44%   [EVAL] batch:  262 | acc: 50.00%,  total acc: 66.37%   [EVAL] batch:  263 | acc: 68.75%,  total acc: 66.38%   [EVAL] batch:  264 | acc: 62.50%,  total acc: 66.37%   [EVAL] batch:  265 | acc: 50.00%,  total acc: 66.31%   [EVAL] batch:  266 | acc: 56.25%,  total acc: 66.27%   [EVAL] batch:  267 | acc: 62.50%,  total acc: 66.25%   [EVAL] batch:  268 | acc: 56.25%,  total acc: 66.22%   [EVAL] batch:  269 | acc: 62.50%,  total acc: 66.20%   [EVAL] batch:  270 | acc: 62.50%,  total acc: 66.19%   [EVAL] batch:  271 | acc: 50.00%,  total acc: 66.13%   [EVAL] batch:  272 | acc: 62.50%,  total acc: 66.12%   [EVAL] batch:  273 | acc: 68.75%,  total acc: 66.13%   [EVAL] batch:  274 | acc: 43.75%,  total acc: 66.05%   [EVAL] batch:  275 | acc: 100.00%,  total acc: 66.17%   [EVAL] batch:  276 | acc: 100.00%,  total acc: 66.29%   [EVAL] batch:  277 | acc: 100.00%,  total acc: 66.41%   [EVAL] batch:  278 | acc: 100.00%,  total acc: 66.53%   [EVAL] batch:  279 | acc: 100.00%,  total acc: 66.65%   [EVAL] batch:  280 | acc: 100.00%,  total acc: 66.77%   [EVAL] batch:  281 | acc: 75.00%,  total acc: 66.80%   [EVAL] batch:  282 | acc: 75.00%,  total acc: 66.83%   [EVAL] batch:  283 | acc: 62.50%,  total acc: 66.81%   [EVAL] batch:  284 | acc: 68.75%,  total acc: 66.82%   [EVAL] batch:  285 | acc: 62.50%,  total acc: 66.81%   [EVAL] batch:  286 | acc: 56.25%,  total acc: 66.77%   [EVAL] batch:  287 | acc: 68.75%,  total acc: 66.78%   [EVAL] batch:  288 | acc: 100.00%,  total acc: 66.89%   [EVAL] batch:  289 | acc: 100.00%,  total acc: 67.00%   [EVAL] batch:  290 | acc: 100.00%,  total acc: 67.12%   [EVAL] batch:  291 | acc: 100.00%,  total acc: 67.23%   [EVAL] batch:  292 | acc: 100.00%,  total acc: 67.34%   [EVAL] batch:  293 | acc: 93.75%,  total acc: 67.43%   [EVAL] batch:  294 | acc: 100.00%,  total acc: 67.54%   [EVAL] batch:  295 | acc: 100.00%,  total acc: 67.65%   [EVAL] batch:  296 | acc: 100.00%,  total acc: 67.76%   [EVAL] batch:  297 | acc: 100.00%,  total acc: 67.87%   [EVAL] batch:  298 | acc: 100.00%,  total acc: 67.98%   [EVAL] batch:  299 | acc: 93.75%,  total acc: 68.06%   [EVAL] batch:  300 | acc: 87.50%,  total acc: 68.13%   [EVAL] batch:  301 | acc: 87.50%,  total acc: 68.19%   [EVAL] batch:  302 | acc: 93.75%,  total acc: 68.28%   [EVAL] batch:  303 | acc: 93.75%,  total acc: 68.36%   [EVAL] batch:  304 | acc: 87.50%,  total acc: 68.42%   [EVAL] batch:  305 | acc: 93.75%,  total acc: 68.50%   [EVAL] batch:  306 | acc: 93.75%,  total acc: 68.59%   [EVAL] batch:  307 | acc: 93.75%,  total acc: 68.67%   [EVAL] batch:  308 | acc: 93.75%,  total acc: 68.75%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 68.85%   [EVAL] batch:  310 | acc: 100.00%,  total acc: 68.95%   [EVAL] batch:  311 | acc: 87.50%,  total acc: 69.01%   [EVAL] batch:  312 | acc: 62.50%,  total acc: 68.99%   [EVAL] batch:  313 | acc: 31.25%,  total acc: 68.87%   [EVAL] batch:  314 | acc: 31.25%,  total acc: 68.75%   [EVAL] batch:  315 | acc: 31.25%,  total acc: 68.63%   [EVAL] batch:  316 | acc: 43.75%,  total acc: 68.55%   [EVAL] batch:  317 | acc: 25.00%,  total acc: 68.42%   [EVAL] batch:  318 | acc: 68.75%,  total acc: 68.42%   [EVAL] batch:  319 | acc: 81.25%,  total acc: 68.46%   [EVAL] batch:  320 | acc: 87.50%,  total acc: 68.52%   [EVAL] batch:  321 | acc: 87.50%,  total acc: 68.58%   [EVAL] batch:  322 | acc: 100.00%,  total acc: 68.67%   [EVAL] batch:  323 | acc: 75.00%,  total acc: 68.69%   [EVAL] batch:  324 | acc: 87.50%,  total acc: 68.75%   [EVAL] batch:  325 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:  326 | acc: 50.00%,  total acc: 68.69%   [EVAL] batch:  327 | acc: 68.75%,  total acc: 68.69%   [EVAL] batch:  328 | acc: 75.00%,  total acc: 68.71%   [EVAL] batch:  329 | acc: 43.75%,  total acc: 68.64%   [EVAL] batch:  330 | acc: 68.75%,  total acc: 68.64%   [EVAL] batch:  331 | acc: 25.00%,  total acc: 68.51%   [EVAL] batch:  332 | acc: 12.50%,  total acc: 68.34%   [EVAL] batch:  333 | acc: 12.50%,  total acc: 68.17%   [EVAL] batch:  334 | acc: 18.75%,  total acc: 68.02%   [EVAL] batch:  335 | acc: 6.25%,  total acc: 67.84%   [EVAL] batch:  336 | acc: 18.75%,  total acc: 67.69%   [EVAL] batch:  337 | acc: 25.00%,  total acc: 67.57%   [EVAL] batch:  338 | acc: 0.00%,  total acc: 67.37%   [EVAL] batch:  339 | acc: 6.25%,  total acc: 67.19%   [EVAL] batch:  340 | acc: 0.00%,  total acc: 66.99%   [EVAL] batch:  341 | acc: 6.25%,  total acc: 66.81%   [EVAL] batch:  342 | acc: 6.25%,  total acc: 66.64%   [EVAL] batch:  343 | acc: 18.75%,  total acc: 66.50%   [EVAL] batch:  344 | acc: 81.25%,  total acc: 66.54%   [EVAL] batch:  345 | acc: 87.50%,  total acc: 66.60%   [EVAL] batch:  346 | acc: 87.50%,  total acc: 66.66%   [EVAL] batch:  347 | acc: 93.75%,  total acc: 66.74%   [EVAL] batch:  348 | acc: 87.50%,  total acc: 66.80%   [EVAL] batch:  349 | acc: 62.50%,  total acc: 66.79%   [EVAL] batch:  350 | acc: 93.75%,  total acc: 66.86%   [EVAL] batch:  351 | acc: 93.75%,  total acc: 66.94%   [EVAL] batch:  352 | acc: 87.50%,  total acc: 67.00%   [EVAL] batch:  353 | acc: 87.50%,  total acc: 67.06%   [EVAL] batch:  354 | acc: 93.75%,  total acc: 67.13%   [EVAL] batch:  355 | acc: 93.75%,  total acc: 67.21%   [EVAL] batch:  356 | acc: 43.75%,  total acc: 67.14%   [EVAL] batch:  357 | acc: 18.75%,  total acc: 67.00%   [EVAL] batch:  358 | acc: 31.25%,  total acc: 66.90%   [EVAL] batch:  359 | acc: 18.75%,  total acc: 66.77%   [EVAL] batch:  360 | acc: 25.00%,  total acc: 66.66%   [EVAL] batch:  361 | acc: 31.25%,  total acc: 66.56%   [EVAL] batch:  362 | acc: 25.00%,  total acc: 66.44%   [EVAL] batch:  363 | acc: 37.50%,  total acc: 66.36%   [EVAL] batch:  364 | acc: 50.00%,  total acc: 66.32%   [EVAL] batch:  365 | acc: 68.75%,  total acc: 66.33%   [EVAL] batch:  366 | acc: 56.25%,  total acc: 66.30%   [EVAL] batch:  367 | acc: 56.25%,  total acc: 66.27%   [EVAL] batch:  368 | acc: 56.25%,  total acc: 66.24%   [EVAL] batch:  369 | acc: 37.50%,  total acc: 66.17%   [EVAL] batch:  370 | acc: 31.25%,  total acc: 66.07%   [EVAL] batch:  371 | acc: 31.25%,  total acc: 65.98%   [EVAL] batch:  372 | acc: 37.50%,  total acc: 65.90%   [EVAL] batch:  373 | acc: 56.25%,  total acc: 65.88%   [EVAL] batch:  374 | acc: 31.25%,  total acc: 65.78%   [EVAL] batch:  375 | acc: 56.25%,  total acc: 65.76%   [EVAL] batch:  376 | acc: 37.50%,  total acc: 65.68%   [EVAL] batch:  377 | acc: 50.00%,  total acc: 65.64%   [EVAL] batch:  378 | acc: 50.00%,  total acc: 65.60%   [EVAL] batch:  379 | acc: 62.50%,  total acc: 65.59%   [EVAL] batch:  380 | acc: 50.00%,  total acc: 65.55%   [EVAL] batch:  381 | acc: 75.00%,  total acc: 65.58%   [EVAL] batch:  382 | acc: 56.25%,  total acc: 65.55%   [EVAL] batch:  383 | acc: 50.00%,  total acc: 65.51%   [EVAL] batch:  384 | acc: 56.25%,  total acc: 65.49%   [EVAL] batch:  385 | acc: 68.75%,  total acc: 65.50%   [EVAL] batch:  386 | acc: 56.25%,  total acc: 65.47%   [EVAL] batch:  387 | acc: 37.50%,  total acc: 65.40%   [EVAL] batch:  388 | acc: 12.50%,  total acc: 65.26%   [EVAL] batch:  389 | acc: 18.75%,  total acc: 65.14%   [EVAL] batch:  390 | acc: 31.25%,  total acc: 65.06%   [EVAL] batch:  391 | acc: 50.00%,  total acc: 65.02%   [EVAL] batch:  392 | acc: 37.50%,  total acc: 64.95%   [EVAL] batch:  393 | acc: 37.50%,  total acc: 64.88%   [EVAL] batch:  394 | acc: 93.75%,  total acc: 64.95%   [EVAL] batch:  395 | acc: 100.00%,  total acc: 65.04%   [EVAL] batch:  396 | acc: 93.75%,  total acc: 65.11%   [EVAL] batch:  397 | acc: 100.00%,  total acc: 65.20%   [EVAL] batch:  398 | acc: 100.00%,  total acc: 65.29%   [EVAL] batch:  399 | acc: 93.75%,  total acc: 65.36%   [EVAL] batch:  400 | acc: 6.25%,  total acc: 65.21%   [EVAL] batch:  401 | acc: 0.00%,  total acc: 65.05%   [EVAL] batch:  402 | acc: 6.25%,  total acc: 64.90%   [EVAL] batch:  403 | acc: 25.00%,  total acc: 64.81%   [EVAL] batch:  404 | acc: 12.50%,  total acc: 64.68%   [EVAL] batch:  405 | acc: 0.00%,  total acc: 64.52%   [EVAL] batch:  406 | acc: 12.50%,  total acc: 64.39%   [EVAL] batch:  407 | acc: 18.75%,  total acc: 64.28%   [EVAL] batch:  408 | acc: 18.75%,  total acc: 64.17%   [EVAL] batch:  409 | acc: 6.25%,  total acc: 64.02%   [EVAL] batch:  410 | acc: 0.00%,  total acc: 63.87%   [EVAL] batch:  411 | acc: 18.75%,  total acc: 63.76%   [EVAL] batch:  412 | acc: 37.50%,  total acc: 63.70%   [EVAL] batch:  413 | acc: 43.75%,  total acc: 63.65%   [EVAL] batch:  414 | acc: 75.00%,  total acc: 63.67%   [EVAL] batch:  415 | acc: 43.75%,  total acc: 63.63%   [EVAL] batch:  416 | acc: 37.50%,  total acc: 63.56%   [EVAL] batch:  417 | acc: 68.75%,  total acc: 63.58%   [EVAL] batch:  418 | acc: 62.50%,  total acc: 63.57%   [EVAL] batch:  419 | acc: 93.75%,  total acc: 63.65%   [EVAL] batch:  420 | acc: 87.50%,  total acc: 63.70%   [EVAL] batch:  421 | acc: 81.25%,  total acc: 63.74%   [EVAL] batch:  422 | acc: 62.50%,  total acc: 63.74%   [EVAL] batch:  423 | acc: 100.00%,  total acc: 63.83%   [EVAL] batch:  424 | acc: 93.75%,  total acc: 63.90%   [EVAL] batch:  425 | acc: 43.75%,  total acc: 63.85%   [EVAL] batch:  426 | acc: 50.00%,  total acc: 63.82%   [EVAL] batch:  427 | acc: 43.75%,  total acc: 63.77%   [EVAL] batch:  428 | acc: 56.25%,  total acc: 63.75%   [EVAL] batch:  429 | acc: 81.25%,  total acc: 63.79%   [EVAL] batch:  430 | acc: 43.75%,  total acc: 63.75%   [EVAL] batch:  431 | acc: 75.00%,  total acc: 63.77%   [EVAL] batch:  432 | acc: 87.50%,  total acc: 63.83%   [EVAL] batch:  433 | acc: 93.75%,  total acc: 63.90%   [EVAL] batch:  434 | acc: 68.75%,  total acc: 63.91%   [EVAL] batch:  435 | acc: 87.50%,  total acc: 63.96%   [EVAL] batch:  436 | acc: 75.00%,  total acc: 63.99%   [EVAL] batch:  437 | acc: 56.25%,  total acc: 63.97%   [EVAL] batch:  438 | acc: 62.50%,  total acc: 63.97%   [EVAL] batch:  439 | acc: 37.50%,  total acc: 63.91%   [EVAL] batch:  440 | acc: 68.75%,  total acc: 63.92%   [EVAL] batch:  441 | acc: 68.75%,  total acc: 63.93%   [EVAL] batch:  442 | acc: 56.25%,  total acc: 63.91%   [EVAL] batch:  443 | acc: 62.50%,  total acc: 63.91%   [EVAL] batch:  444 | acc: 62.50%,  total acc: 63.90%   [EVAL] batch:  445 | acc: 62.50%,  total acc: 63.90%   [EVAL] batch:  446 | acc: 68.75%,  total acc: 63.91%   [EVAL] batch:  447 | acc: 62.50%,  total acc: 63.91%   [EVAL] batch:  448 | acc: 25.00%,  total acc: 63.82%   [EVAL] batch:  449 | acc: 100.00%,  total acc: 63.90%   [EVAL] batch:  450 | acc: 87.50%,  total acc: 63.96%   [EVAL] batch:  451 | acc: 68.75%,  total acc: 63.97%   [EVAL] batch:  452 | acc: 75.00%,  total acc: 63.99%   [EVAL] batch:  453 | acc: 75.00%,  total acc: 64.01%   [EVAL] batch:  454 | acc: 81.25%,  total acc: 64.05%   [EVAL] batch:  455 | acc: 100.00%,  total acc: 64.13%   [EVAL] batch:  456 | acc: 93.75%,  total acc: 64.20%   [EVAL] batch:  457 | acc: 93.75%,  total acc: 64.26%   [EVAL] batch:  458 | acc: 100.00%,  total acc: 64.34%   [EVAL] batch:  459 | acc: 100.00%,  total acc: 64.42%   [EVAL] batch:  460 | acc: 93.75%,  total acc: 64.48%   [EVAL] batch:  461 | acc: 100.00%,  total acc: 64.56%   [EVAL] batch:  462 | acc: 68.75%,  total acc: 64.57%   [EVAL] batch:  463 | acc: 37.50%,  total acc: 64.51%   [EVAL] batch:  464 | acc: 31.25%,  total acc: 64.44%   [EVAL] batch:  465 | acc: 50.00%,  total acc: 64.40%   [EVAL] batch:  466 | acc: 25.00%,  total acc: 64.32%   [EVAL] batch:  467 | acc: 43.75%,  total acc: 64.28%   [EVAL] batch:  468 | acc: 43.75%,  total acc: 64.23%   [EVAL] batch:  469 | acc: 62.50%,  total acc: 64.23%   [EVAL] batch:  470 | acc: 31.25%,  total acc: 64.16%   [EVAL] batch:  471 | acc: 43.75%,  total acc: 64.12%   [EVAL] batch:  472 | acc: 18.75%,  total acc: 64.02%   [EVAL] batch:  473 | acc: 12.50%,  total acc: 63.91%   [EVAL] batch:  474 | acc: 37.50%,  total acc: 63.86%   [EVAL] batch:  475 | acc: 68.75%,  total acc: 63.87%   [EVAL] batch:  476 | acc: 68.75%,  total acc: 63.88%   [EVAL] batch:  477 | acc: 68.75%,  total acc: 63.89%   [EVAL] batch:  478 | acc: 87.50%,  total acc: 63.94%   [EVAL] batch:  479 | acc: 68.75%,  total acc: 63.95%   [EVAL] batch:  480 | acc: 75.00%,  total acc: 63.97%   [EVAL] batch:  481 | acc: 75.00%,  total acc: 63.99%   [EVAL] batch:  482 | acc: 50.00%,  total acc: 63.96%   [EVAL] batch:  483 | acc: 75.00%,  total acc: 63.99%   [EVAL] batch:  484 | acc: 62.50%,  total acc: 63.98%   [EVAL] batch:  485 | acc: 62.50%,  total acc: 63.98%   [EVAL] batch:  486 | acc: 87.50%,  total acc: 64.03%   [EVAL] batch:  487 | acc: 93.75%,  total acc: 64.09%   [EVAL] batch:  488 | acc: 100.00%,  total acc: 64.16%   [EVAL] batch:  489 | acc: 100.00%,  total acc: 64.23%   [EVAL] batch:  490 | acc: 100.00%,  total acc: 64.31%   [EVAL] batch:  491 | acc: 100.00%,  total acc: 64.38%   [EVAL] batch:  492 | acc: 100.00%,  total acc: 64.45%   [EVAL] batch:  493 | acc: 100.00%,  total acc: 64.52%   [EVAL] batch:  494 | acc: 93.75%,  total acc: 64.58%   [EVAL] batch:  495 | acc: 81.25%,  total acc: 64.62%   [EVAL] batch:  496 | acc: 93.75%,  total acc: 64.68%   [EVAL] batch:  497 | acc: 81.25%,  total acc: 64.71%   [EVAL] batch:  498 | acc: 100.00%,  total acc: 64.78%   [EVAL] batch:  499 | acc: 100.00%,  total acc: 64.85%   
cur_acc:  ['0.9524', '0.8284', '0.7798', '0.7192', '0.8889', '0.6012', '0.5506', '0.7024']
his_acc:  ['0.9524', '0.8850', '0.8195', '0.7458', '0.7568', '0.7105', '0.6637', '0.6485']
--------Round  3
seed:  400
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/train.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/valid.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/test.pkl
Task_order: [7 0 1 2 5 3 4 6]
prepared data!
CurrentTrain: epoch 15, batch     0 | loss: 29.6887266CurrentTrain: epoch 15, batch     1 | loss: 31.0878502CurrentTrain: epoch 15, batch     2 | loss: 29.2062851CurrentTrain: epoch 15, batch     3 | loss: 27.5758293CurrentTrain: epoch 15, batch     4 | loss: 20.4461666CurrentTrain: epoch 15, batch     5 | loss: 28.0195133CurrentTrain: epoch 15, batch     6 | loss: 30.5572484CurrentTrain: epoch 15, batch     7 | loss: 30.0052211CurrentTrain: epoch 15, batch     8 | loss: 36.3167508CurrentTrain: epoch 15, batch     9 | loss: 20.2848630CurrentTrain: epoch 15, batch    10 | loss: 21.4536348CurrentTrain: epoch 15, batch    11 | loss: 20.5856535CurrentTrain: epoch 15, batch    12 | loss: 18.2345430CurrentTrain: epoch 15, batch    13 | loss: 22.3457438CurrentTrain: epoch 15, batch    14 | loss: 20.0654216CurrentTrain: epoch 15, batch    15 | loss: 23.8938460CurrentTrain: epoch 15, batch    16 | loss: 18.6326508CurrentTrain: epoch 15, batch    17 | loss: 27.2423527CurrentTrain: epoch 15, batch    18 | loss: 17.2572666CurrentTrain: epoch 15, batch    19 | loss: 26.5560677CurrentTrain: epoch 15, batch    20 | loss: 18.6236413CurrentTrain: epoch 15, batch    21 | loss: 19.3084340CurrentTrain: epoch 15, batch    22 | loss: 28.1509279CurrentTrain: epoch 15, batch    23 | loss: 20.1454546CurrentTrain: epoch 15, batch    24 | loss: 24.9395425CurrentTrain: epoch 15, batch    25 | loss: 19.4020320CurrentTrain: epoch 15, batch    26 | loss: 20.7881447CurrentTrain: epoch 15, batch    27 | loss: 18.8631481CurrentTrain: epoch 15, batch    28 | loss: 21.5420274CurrentTrain: epoch 15, batch    29 | loss: 19.6366340CurrentTrain: epoch 15, batch    30 | loss: 16.3127451CurrentTrain: epoch 15, batch    31 | loss: 16.1648302CurrentTrain: epoch 15, batch    32 | loss: 23.3598957CurrentTrain: epoch 15, batch    33 | loss: 15.2058779CurrentTrain: epoch 15, batch    34 | loss: 16.0316913CurrentTrain: epoch 15, batch    35 | loss: 24.4734734CurrentTrain: epoch 15, batch    36 | loss: 21.6445837CurrentTrain: epoch 15, batch    37 | loss: 22.0715518CurrentTrain: epoch 15, batch    38 | loss: 15.9222790CurrentTrain: epoch 15, batch    39 | loss: 23.1824051CurrentTrain: epoch 15, batch    40 | loss: 14.8011733CurrentTrain: epoch 15, batch    41 | loss: 26.0694543CurrentTrain: epoch 15, batch    42 | loss: 22.6275213CurrentTrain: epoch 15, batch    43 | loss: 15.6250118CurrentTrain: epoch 15, batch    44 | loss: 15.6268991CurrentTrain: epoch 15, batch    45 | loss: 16.1304596CurrentTrain: epoch 15, batch    46 | loss: 22.3748608CurrentTrain: epoch 15, batch    47 | loss: 24.9370043CurrentTrain: epoch 15, batch    48 | loss: 21.6365962CurrentTrain: epoch 15, batch    49 | loss: 30.0735254CurrentTrain: epoch 15, batch    50 | loss: 16.1655560CurrentTrain: epoch 15, batch    51 | loss: 20.0766079CurrentTrain: epoch 15, batch    52 | loss: 24.3139781CurrentTrain: epoch 15, batch    53 | loss: 24.2580538CurrentTrain: epoch 15, batch    54 | loss: 16.4274795CurrentTrain: epoch 15, batch    55 | loss: 24.1414446CurrentTrain: epoch 15, batch    56 | loss: 18.8951374CurrentTrain: epoch 15, batch    57 | loss: 22.5342714CurrentTrain: epoch 15, batch    58 | loss: 20.2628913CurrentTrain: epoch 15, batch    59 | loss: 21.5500016CurrentTrain: epoch 15, batch    60 | loss: 21.0129834CurrentTrain: epoch 15, batch    61 | loss: 13.3481308CurrentTrain: epoch  7, batch    62 | loss: 28.4442204CurrentTrain: epoch 15, batch     0 | loss: 12.6677243CurrentTrain: epoch 15, batch     1 | loss: 14.7246560CurrentTrain: epoch 15, batch     2 | loss: 20.6898790CurrentTrain: epoch 15, batch     3 | loss: 15.4979894CurrentTrain: epoch 15, batch     4 | loss: 21.0785028CurrentTrain: epoch 15, batch     5 | loss: 18.0994136CurrentTrain: epoch 15, batch     6 | loss: 23.3546705CurrentTrain: epoch 15, batch     7 | loss: 21.2081907CurrentTrain: epoch 15, batch     8 | loss: 18.3794744CurrentTrain: epoch 15, batch     9 | loss: 17.8370827CurrentTrain: epoch 15, batch    10 | loss: 28.4080652CurrentTrain: epoch 15, batch    11 | loss: 15.0738866CurrentTrain: epoch 15, batch    12 | loss: 16.2578926CurrentTrain: epoch 15, batch    13 | loss: 12.4320699CurrentTrain: epoch 15, batch    14 | loss: 14.4204068CurrentTrain: epoch 15, batch    15 | loss: 16.9693380CurrentTrain: epoch 15, batch    16 | loss: 14.9131454CurrentTrain: epoch 15, batch    17 | loss: 12.1380387CurrentTrain: epoch 15, batch    18 | loss: 34.0889236CurrentTrain: epoch 15, batch    19 | loss: 18.3310428CurrentTrain: epoch 15, batch    20 | loss: 11.4489848CurrentTrain: epoch 15, batch    21 | loss: 17.6025284CurrentTrain: epoch 15, batch    22 | loss: 11.0961558CurrentTrain: epoch 15, batch    23 | loss: 30.3314936CurrentTrain: epoch 15, batch    24 | loss: 18.9301031CurrentTrain: epoch 15, batch    25 | loss: 18.3539353CurrentTrain: epoch 15, batch    26 | loss: 16.4221808CurrentTrain: epoch 15, batch    27 | loss: 21.2248338CurrentTrain: epoch 15, batch    28 | loss: 43.8537254CurrentTrain: epoch 15, batch    29 | loss: 21.3996790CurrentTrain: epoch 15, batch    30 | loss: 15.1534693CurrentTrain: epoch 15, batch    31 | loss: 11.2034036CurrentTrain: epoch 15, batch    32 | loss: 24.1524704CurrentTrain: epoch 15, batch    33 | loss: 14.6867078CurrentTrain: epoch 15, batch    34 | loss: 15.4878557CurrentTrain: epoch 15, batch    35 | loss: 20.9305957CurrentTrain: epoch 15, batch    36 | loss: 17.6079104CurrentTrain: epoch 15, batch    37 | loss: 19.0193307CurrentTrain: epoch 15, batch    38 | loss: 26.7309525CurrentTrain: epoch 15, batch    39 | loss: 16.4146132CurrentTrain: epoch 15, batch    40 | loss: 13.3577953CurrentTrain: epoch 15, batch    41 | loss: 13.7336718CurrentTrain: epoch 15, batch    42 | loss: 29.7136577CurrentTrain: epoch 15, batch    43 | loss: 14.8440019CurrentTrain: epoch 15, batch    44 | loss: 13.7782602CurrentTrain: epoch 15, batch    45 | loss: 13.2360055CurrentTrain: epoch 15, batch    46 | loss: 14.4096463CurrentTrain: epoch 15, batch    47 | loss: 24.7203635CurrentTrain: epoch 15, batch    48 | loss: 15.7787459CurrentTrain: epoch 15, batch    49 | loss: 20.9150640CurrentTrain: epoch 15, batch    50 | loss: 17.1369330CurrentTrain: epoch 15, batch    51 | loss: 18.9025082CurrentTrain: epoch 15, batch    52 | loss: 13.7615767CurrentTrain: epoch 15, batch    53 | loss: 15.5610457CurrentTrain: epoch 15, batch    54 | loss: 19.8375817CurrentTrain: epoch 15, batch    55 | loss: 16.8378921CurrentTrain: epoch 15, batch    56 | loss: 14.3261296CurrentTrain: epoch 15, batch    57 | loss: 14.8133540CurrentTrain: epoch 15, batch    58 | loss: 17.8447431CurrentTrain: epoch 15, batch    59 | loss: 16.9387394CurrentTrain: epoch 15, batch    60 | loss: 19.1326748CurrentTrain: epoch 15, batch    61 | loss: 29.1750927CurrentTrain: epoch  7, batch    62 | loss: 8.0348486CurrentTrain: epoch 15, batch     0 | loss: 27.9896483CurrentTrain: epoch 15, batch     1 | loss: 16.5528762CurrentTrain: epoch 15, batch     2 | loss: 17.1108933CurrentTrain: epoch 15, batch     3 | loss: 19.0291489CurrentTrain: epoch 15, batch     4 | loss: 20.8849424CurrentTrain: epoch 15, batch     5 | loss: 25.2271020CurrentTrain: epoch 15, batch     6 | loss: 14.7870802CurrentTrain: epoch 15, batch     7 | loss: 15.0177876CurrentTrain: epoch 15, batch     8 | loss: 17.0805953CurrentTrain: epoch 15, batch     9 | loss: 32.4655086CurrentTrain: epoch 15, batch    10 | loss: 20.4540498CurrentTrain: epoch 15, batch    11 | loss: 13.5989359CurrentTrain: epoch 15, batch    12 | loss: 13.4021846CurrentTrain: epoch 15, batch    13 | loss: 17.9176927CurrentTrain: epoch 15, batch    14 | loss: 14.7247360CurrentTrain: epoch 15, batch    15 | loss: 16.4999387CurrentTrain: epoch 15, batch    16 | loss: 19.1207064CurrentTrain: epoch 15, batch    17 | loss: 21.1510589CurrentTrain: epoch 15, batch    18 | loss: 19.8666470CurrentTrain: epoch 15, batch    19 | loss: 11.2577101CurrentTrain: epoch 15, batch    20 | loss: 29.8958082CurrentTrain: epoch 15, batch    21 | loss: 14.9582908CurrentTrain: epoch 15, batch    22 | loss: 14.6984799CurrentTrain: epoch 15, batch    23 | loss: 15.6635432CurrentTrain: epoch 15, batch    24 | loss: 14.0633300CurrentTrain: epoch 15, batch    25 | loss: 19.2989283CurrentTrain: epoch 15, batch    26 | loss: 17.2528322CurrentTrain: epoch 15, batch    27 | loss: 15.7178346CurrentTrain: epoch 15, batch    28 | loss: 12.1393974CurrentTrain: epoch 15, batch    29 | loss: 17.8639557CurrentTrain: epoch 15, batch    30 | loss: 12.8116154CurrentTrain: epoch 15, batch    31 | loss: 13.9227226CurrentTrain: epoch 15, batch    32 | loss: 16.7487579CurrentTrain: epoch 15, batch    33 | loss: 19.3232433CurrentTrain: epoch 15, batch    34 | loss: 15.0743937CurrentTrain: epoch 15, batch    35 | loss: 14.0059571CurrentTrain: epoch 15, batch    36 | loss: 17.3033242CurrentTrain: epoch 15, batch    37 | loss: 14.7912691CurrentTrain: epoch 15, batch    38 | loss: 13.4306322CurrentTrain: epoch 15, batch    39 | loss: 23.7029097CurrentTrain: epoch 15, batch    40 | loss: 16.1851387CurrentTrain: epoch 15, batch    41 | loss: 14.5595310CurrentTrain: epoch 15, batch    42 | loss: 16.8262282CurrentTrain: epoch 15, batch    43 | loss: 16.6300787CurrentTrain: epoch 15, batch    44 | loss: 13.5361139CurrentTrain: epoch 15, batch    45 | loss: 13.5788593CurrentTrain: epoch 15, batch    46 | loss: 25.3788228CurrentTrain: epoch 15, batch    47 | loss: 19.8048143CurrentTrain: epoch 15, batch    48 | loss: 14.7898317CurrentTrain: epoch 15, batch    49 | loss: 26.1523647CurrentTrain: epoch 15, batch    50 | loss: 26.7346006CurrentTrain: epoch 15, batch    51 | loss: 18.8101534CurrentTrain: epoch 15, batch    52 | loss: 13.0351005CurrentTrain: epoch 15, batch    53 | loss: 12.1449260CurrentTrain: epoch 15, batch    54 | loss: 28.7359497CurrentTrain: epoch 15, batch    55 | loss: 11.6642211CurrentTrain: epoch 15, batch    56 | loss: 15.7936943CurrentTrain: epoch 15, batch    57 | loss: 14.3515245CurrentTrain: epoch 15, batch    58 | loss: 14.3926357CurrentTrain: epoch 15, batch    59 | loss: 12.0040332CurrentTrain: epoch 15, batch    60 | loss: 19.1043879CurrentTrain: epoch 15, batch    61 | loss: 41.5173590CurrentTrain: epoch  7, batch    62 | loss: 9.2070726CurrentTrain: epoch 15, batch     0 | loss: 10.9795545CurrentTrain: epoch 15, batch     1 | loss: 34.3440522CurrentTrain: epoch 15, batch     2 | loss: 22.6892304CurrentTrain: epoch 15, batch     3 | loss: 16.9586220CurrentTrain: epoch 15, batch     4 | loss: 23.7079210CurrentTrain: epoch 15, batch     5 | loss: 14.7362239CurrentTrain: epoch 15, batch     6 | loss: 11.6739616CurrentTrain: epoch 15, batch     7 | loss: 23.5197411CurrentTrain: epoch 15, batch     8 | loss: 11.2836435CurrentTrain: epoch 15, batch     9 | loss: 12.7669752CurrentTrain: epoch 15, batch    10 | loss: 14.4666102CurrentTrain: epoch 15, batch    11 | loss: 12.9401051CurrentTrain: epoch 15, batch    12 | loss: 17.3778249CurrentTrain: epoch 15, batch    13 | loss: 11.4252209CurrentTrain: epoch 15, batch    14 | loss: 11.5402132CurrentTrain: epoch 15, batch    15 | loss: 17.3262721CurrentTrain: epoch 15, batch    16 | loss: 12.8661530CurrentTrain: epoch 15, batch    17 | loss: 15.1579837CurrentTrain: epoch 15, batch    18 | loss: 16.8033518CurrentTrain: epoch 15, batch    19 | loss: 10.9921310CurrentTrain: epoch 15, batch    20 | loss: 17.8940267CurrentTrain: epoch 15, batch    21 | loss: 17.1928849CurrentTrain: epoch 15, batch    22 | loss: 13.5745944CurrentTrain: epoch 15, batch    23 | loss: 17.5633906CurrentTrain: epoch 15, batch    24 | loss: 50.5831225CurrentTrain: epoch 15, batch    25 | loss: 19.7056604CurrentTrain: epoch 15, batch    26 | loss: 11.8838405CurrentTrain: epoch 15, batch    27 | loss: 12.7513055CurrentTrain: epoch 15, batch    28 | loss: 17.8253175CurrentTrain: epoch 15, batch    29 | loss: 14.6574533CurrentTrain: epoch 15, batch    30 | loss: 15.1056655CurrentTrain: epoch 15, batch    31 | loss: 11.4927314CurrentTrain: epoch 15, batch    32 | loss: 17.9691668CurrentTrain: epoch 15, batch    33 | loss: 13.0862298CurrentTrain: epoch 15, batch    34 | loss: 18.2938146CurrentTrain: epoch 15, batch    35 | loss: 13.1558107CurrentTrain: epoch 15, batch    36 | loss: 14.4447242CurrentTrain: epoch 15, batch    37 | loss: 12.3330334CurrentTrain: epoch 15, batch    38 | loss: 12.0055402CurrentTrain: epoch 15, batch    39 | loss: 12.6778047CurrentTrain: epoch 15, batch    40 | loss: 21.5938410CurrentTrain: epoch 15, batch    41 | loss: 14.3609302CurrentTrain: epoch 15, batch    42 | loss: 13.1165390CurrentTrain: epoch 15, batch    43 | loss: 19.0888000CurrentTrain: epoch 15, batch    44 | loss: 14.6037169CurrentTrain: epoch 15, batch    45 | loss: 13.2755510CurrentTrain: epoch 15, batch    46 | loss: 16.0730942CurrentTrain: epoch 15, batch    47 | loss: 13.5613980CurrentTrain: epoch 15, batch    48 | loss: 13.9249796CurrentTrain: epoch 15, batch    49 | loss: 15.3518093CurrentTrain: epoch 15, batch    50 | loss: 27.9557698CurrentTrain: epoch 15, batch    51 | loss: 10.3078730CurrentTrain: epoch 15, batch    52 | loss: 15.9014198CurrentTrain: epoch 15, batch    53 | loss: 22.4133370CurrentTrain: epoch 15, batch    54 | loss: 10.9425542CurrentTrain: epoch 15, batch    55 | loss: 10.0123793CurrentTrain: epoch 15, batch    56 | loss: 11.9470693CurrentTrain: epoch 15, batch    57 | loss: 15.7803827CurrentTrain: epoch 15, batch    58 | loss: 12.3601424CurrentTrain: epoch 15, batch    59 | loss: 13.5472666CurrentTrain: epoch 15, batch    60 | loss: 14.5624798CurrentTrain: epoch 15, batch    61 | loss: 17.9502699CurrentTrain: epoch  7, batch    62 | loss: 8.8237180CurrentTrain: epoch 15, batch     0 | loss: 13.1292779CurrentTrain: epoch 15, batch     1 | loss: 13.1076399CurrentTrain: epoch 15, batch     2 | loss: 11.6794921CurrentTrain: epoch 15, batch     3 | loss: 16.8847059CurrentTrain: epoch 15, batch     4 | loss: 10.1006808CurrentTrain: epoch 15, batch     5 | loss: 14.9204121CurrentTrain: epoch 15, batch     6 | loss: 19.1060382CurrentTrain: epoch 15, batch     7 | loss: 14.6636929CurrentTrain: epoch 15, batch     8 | loss: 11.2398426CurrentTrain: epoch 15, batch     9 | loss: 14.6878331CurrentTrain: epoch 15, batch    10 | loss: 15.6597557CurrentTrain: epoch 15, batch    11 | loss: 13.0933987CurrentTrain: epoch 15, batch    12 | loss: 8.8770344CurrentTrain: epoch 15, batch    13 | loss: 21.5209241CurrentTrain: epoch 15, batch    14 | loss: 13.1703943CurrentTrain: epoch 15, batch    15 | loss: 10.8393150CurrentTrain: epoch 15, batch    16 | loss: 13.2868684CurrentTrain: epoch 15, batch    17 | loss: 12.6519225CurrentTrain: epoch 15, batch    18 | loss: 25.9890751CurrentTrain: epoch 15, batch    19 | loss: 21.0701494CurrentTrain: epoch 15, batch    20 | loss: 14.0051342CurrentTrain: epoch 15, batch    21 | loss: 11.3463631CurrentTrain: epoch 15, batch    22 | loss: 13.3010735CurrentTrain: epoch 15, batch    23 | loss: 15.7103244CurrentTrain: epoch 15, batch    24 | loss: 25.1507201CurrentTrain: epoch 15, batch    25 | loss: 19.8754215CurrentTrain: epoch 15, batch    26 | loss: 19.5878248CurrentTrain: epoch 15, batch    27 | loss: 9.7410760CurrentTrain: epoch 15, batch    28 | loss: 23.7411751CurrentTrain: epoch 15, batch    29 | loss: 13.6787302CurrentTrain: epoch 15, batch    30 | loss: 14.4914996CurrentTrain: epoch 15, batch    31 | loss: 16.2075125CurrentTrain: epoch 15, batch    32 | loss: 16.8254890CurrentTrain: epoch 15, batch    33 | loss: 16.5055325CurrentTrain: epoch 15, batch    34 | loss: 14.1683521CurrentTrain: epoch 15, batch    35 | loss: 16.4478807CurrentTrain: epoch 15, batch    36 | loss: 13.0398697CurrentTrain: epoch 15, batch    37 | loss: 17.0799786CurrentTrain: epoch 15, batch    38 | loss: 18.5286293CurrentTrain: epoch 15, batch    39 | loss: 14.1175592CurrentTrain: epoch 15, batch    40 | loss: 20.8371178CurrentTrain: epoch 15, batch    41 | loss: 16.4854883CurrentTrain: epoch 15, batch    42 | loss: 13.8714542CurrentTrain: epoch 15, batch    43 | loss: 20.3874540CurrentTrain: epoch 15, batch    44 | loss: 12.5437828CurrentTrain: epoch 15, batch    45 | loss: 10.9897612CurrentTrain: epoch 15, batch    46 | loss: 13.3586065CurrentTrain: epoch 15, batch    47 | loss: 20.3253760CurrentTrain: epoch 15, batch    48 | loss: 38.0860953CurrentTrain: epoch 15, batch    49 | loss: 15.6446277CurrentTrain: epoch 15, batch    50 | loss: 18.3271852CurrentTrain: epoch 15, batch    51 | loss: 21.3415243CurrentTrain: epoch 15, batch    52 | loss: 14.0808484CurrentTrain: epoch 15, batch    53 | loss: 15.3344981CurrentTrain: epoch 15, batch    54 | loss: 14.5944601CurrentTrain: epoch 15, batch    55 | loss: 14.1523170CurrentTrain: epoch 15, batch    56 | loss: 29.7562604CurrentTrain: epoch 15, batch    57 | loss: 10.8468747CurrentTrain: epoch 15, batch    58 | loss: 10.9786434CurrentTrain: epoch 15, batch    59 | loss: 17.1907064CurrentTrain: epoch 15, batch    60 | loss: 16.3516703CurrentTrain: epoch 15, batch    61 | loss: 14.0697306CurrentTrain: epoch  7, batch    62 | loss: 10.1024385CurrentTrain: epoch 15, batch     0 | loss: 11.9819087CurrentTrain: epoch 15, batch     1 | loss: 16.3823167CurrentTrain: epoch 15, batch     2 | loss: 10.9356380CurrentTrain: epoch 15, batch     3 | loss: 29.4453302CurrentTrain: epoch 15, batch     4 | loss: 11.9309521CurrentTrain: epoch 15, batch     5 | loss: 16.8922916CurrentTrain: epoch 15, batch     6 | loss: 16.2681547CurrentTrain: epoch 15, batch     7 | loss: 11.8754164CurrentTrain: epoch 15, batch     8 | loss: 12.8477006CurrentTrain: epoch 15, batch     9 | loss: 12.5569699CurrentTrain: epoch 15, batch    10 | loss: 14.5169511CurrentTrain: epoch 15, batch    11 | loss: 16.1205471CurrentTrain: epoch 15, batch    12 | loss: 23.1544026CurrentTrain: epoch 15, batch    13 | loss: 12.7968334CurrentTrain: epoch 15, batch    14 | loss: 12.0289660CurrentTrain: epoch 15, batch    15 | loss: 26.6482141CurrentTrain: epoch 15, batch    16 | loss: 9.0339573CurrentTrain: epoch 15, batch    17 | loss: 17.9152470CurrentTrain: epoch 15, batch    18 | loss: 14.4212310CurrentTrain: epoch 15, batch    19 | loss: 22.7700663CurrentTrain: epoch 15, batch    20 | loss: 14.3039001CurrentTrain: epoch 15, batch    21 | loss: 14.6833174CurrentTrain: epoch 15, batch    22 | loss: 13.1815086CurrentTrain: epoch 15, batch    23 | loss: 9.3813075CurrentTrain: epoch 15, batch    24 | loss: 16.6703656CurrentTrain: epoch 15, batch    25 | loss: 15.8408752CurrentTrain: epoch 15, batch    26 | loss: 16.2921380CurrentTrain: epoch 15, batch    27 | loss: 14.4841190CurrentTrain: epoch 15, batch    28 | loss: 9.0457614CurrentTrain: epoch 15, batch    29 | loss: 11.6607417CurrentTrain: epoch 15, batch    30 | loss: 16.7501159CurrentTrain: epoch 15, batch    31 | loss: 8.7867256CurrentTrain: epoch 15, batch    32 | loss: 12.6721050CurrentTrain: epoch 15, batch    33 | loss: 20.9334153CurrentTrain: epoch 15, batch    34 | loss: 16.7112029CurrentTrain: epoch 15, batch    35 | loss: 14.4021290CurrentTrain: epoch 15, batch    36 | loss: 15.1759442CurrentTrain: epoch 15, batch    37 | loss: 17.8966737CurrentTrain: epoch 15, batch    38 | loss: 18.9818044CurrentTrain: epoch 15, batch    39 | loss: 15.3062051CurrentTrain: epoch 15, batch    40 | loss: 11.3280749CurrentTrain: epoch 15, batch    41 | loss: 11.0444269CurrentTrain: epoch 15, batch    42 | loss: 10.3681100CurrentTrain: epoch 15, batch    43 | loss: 15.7364025CurrentTrain: epoch 15, batch    44 | loss: 13.2282589CurrentTrain: epoch 15, batch    45 | loss: 15.7113840CurrentTrain: epoch 15, batch    46 | loss: 18.1195951CurrentTrain: epoch 15, batch    47 | loss: 20.5116959CurrentTrain: epoch 15, batch    48 | loss: 11.4347526CurrentTrain: epoch 15, batch    49 | loss: 13.2245202CurrentTrain: epoch 15, batch    50 | loss: 16.1222137CurrentTrain: epoch 15, batch    51 | loss: 8.7009890CurrentTrain: epoch 15, batch    52 | loss: 13.2523883CurrentTrain: epoch 15, batch    53 | loss: 9.9548751CurrentTrain: epoch 15, batch    54 | loss: 19.5491211CurrentTrain: epoch 15, batch    55 | loss: 12.0598737CurrentTrain: epoch 15, batch    56 | loss: 7.9428610CurrentTrain: epoch 15, batch    57 | loss: 17.2774067CurrentTrain: epoch 15, batch    58 | loss: 12.4925498CurrentTrain: epoch 15, batch    59 | loss: 22.7632387CurrentTrain: epoch 15, batch    60 | loss: 27.3889437CurrentTrain: epoch 15, batch    61 | loss: 12.0143887CurrentTrain: epoch  7, batch    62 | loss: 10.8897355CurrentTrain: epoch 15, batch     0 | loss: 17.4413015CurrentTrain: epoch 15, batch     1 | loss: 12.1000469CurrentTrain: epoch 15, batch     2 | loss: 16.2561842CurrentTrain: epoch 15, batch     3 | loss: 10.7101690CurrentTrain: epoch 15, batch     4 | loss: 9.8959400CurrentTrain: epoch 15, batch     5 | loss: 10.3802283CurrentTrain: epoch 15, batch     6 | loss: 11.9708461CurrentTrain: epoch 15, batch     7 | loss: 9.9395696CurrentTrain: epoch 15, batch     8 | loss: 14.0794342CurrentTrain: epoch 15, batch     9 | loss: 17.7595057CurrentTrain: epoch 15, batch    10 | loss: 9.6203801CurrentTrain: epoch 15, batch    11 | loss: 21.8983663CurrentTrain: epoch 15, batch    12 | loss: 15.1170125CurrentTrain: epoch 15, batch    13 | loss: 10.6457406CurrentTrain: epoch 15, batch    14 | loss: 11.5593544CurrentTrain: epoch 15, batch    15 | loss: 12.2190701CurrentTrain: epoch 15, batch    16 | loss: 9.4162594CurrentTrain: epoch 15, batch    17 | loss: 10.7719314CurrentTrain: epoch 15, batch    18 | loss: 24.2277092CurrentTrain: epoch 15, batch    19 | loss: 10.1571779CurrentTrain: epoch 15, batch    20 | loss: 14.3720778CurrentTrain: epoch 15, batch    21 | loss: 10.3377014CurrentTrain: epoch 15, batch    22 | loss: 9.9026092CurrentTrain: epoch 15, batch    23 | loss: 10.9566063CurrentTrain: epoch 15, batch    24 | loss: 19.0274984CurrentTrain: epoch 15, batch    25 | loss: 15.6659146CurrentTrain: epoch 15, batch    26 | loss: 14.6115628CurrentTrain: epoch 15, batch    27 | loss: 21.4951786CurrentTrain: epoch 15, batch    28 | loss: 8.2464994CurrentTrain: epoch 15, batch    29 | loss: 17.0034037CurrentTrain: epoch 15, batch    30 | loss: 11.7141538CurrentTrain: epoch 15, batch    31 | loss: 15.8320223CurrentTrain: epoch 15, batch    32 | loss: 17.8481176CurrentTrain: epoch 15, batch    33 | loss: 14.6057620CurrentTrain: epoch 15, batch    34 | loss: 12.0454555CurrentTrain: epoch 15, batch    35 | loss: 12.4050926CurrentTrain: epoch 15, batch    36 | loss: 19.4810401CurrentTrain: epoch 15, batch    37 | loss: 12.3215325CurrentTrain: epoch 15, batch    38 | loss: 17.3908700CurrentTrain: epoch 15, batch    39 | loss: 16.4132103CurrentTrain: epoch 15, batch    40 | loss: 13.9273670CurrentTrain: epoch 15, batch    41 | loss: 8.7998759CurrentTrain: epoch 15, batch    42 | loss: 10.1154963CurrentTrain: epoch 15, batch    43 | loss: 10.4220572CurrentTrain: epoch 15, batch    44 | loss: 11.0980972CurrentTrain: epoch 15, batch    45 | loss: 11.2730153CurrentTrain: epoch 15, batch    46 | loss: 28.6736599CurrentTrain: epoch 15, batch    47 | loss: 7.6215860CurrentTrain: epoch 15, batch    48 | loss: 19.3154741CurrentTrain: epoch 15, batch    49 | loss: 12.4333075CurrentTrain: epoch 15, batch    50 | loss: 11.5941227CurrentTrain: epoch 15, batch    51 | loss: 23.6173100CurrentTrain: epoch 15, batch    52 | loss: 24.2652473CurrentTrain: epoch 15, batch    53 | loss: 20.2193121CurrentTrain: epoch 15, batch    54 | loss: 15.1351406CurrentTrain: epoch 15, batch    55 | loss: 14.0479054CurrentTrain: epoch 15, batch    56 | loss: 10.4874972CurrentTrain: epoch 15, batch    57 | loss: 11.7307086CurrentTrain: epoch 15, batch    58 | loss: 19.0493410CurrentTrain: epoch 15, batch    59 | loss: 24.7958249CurrentTrain: epoch 15, batch    60 | loss: 18.4718788CurrentTrain: epoch 15, batch    61 | loss: 12.9939489CurrentTrain: epoch  7, batch    62 | loss: 13.0846680CurrentTrain: epoch 15, batch     0 | loss: 11.6759853CurrentTrain: epoch 15, batch     1 | loss: 12.1559422CurrentTrain: epoch 15, batch     2 | loss: 20.5465999CurrentTrain: epoch 15, batch     3 | loss: 10.1812318CurrentTrain: epoch 15, batch     4 | loss: 13.7083445CurrentTrain: epoch 15, batch     5 | loss: 11.7535791CurrentTrain: epoch 15, batch     6 | loss: 9.0677207CurrentTrain: epoch 15, batch     7 | loss: 12.4857006CurrentTrain: epoch 15, batch     8 | loss: 11.6066021CurrentTrain: epoch 15, batch     9 | loss: 12.7896835CurrentTrain: epoch 15, batch    10 | loss: 14.6179783CurrentTrain: epoch 15, batch    11 | loss: 13.4982246CurrentTrain: epoch 15, batch    12 | loss: 19.8636222CurrentTrain: epoch 15, batch    13 | loss: 11.2029315CurrentTrain: epoch 15, batch    14 | loss: 17.1831755CurrentTrain: epoch 15, batch    15 | loss: 16.7163327CurrentTrain: epoch 15, batch    16 | loss: 12.4032685CurrentTrain: epoch 15, batch    17 | loss: 16.5727254CurrentTrain: epoch 15, batch    18 | loss: 15.0540870CurrentTrain: epoch 15, batch    19 | loss: 17.5613720CurrentTrain: epoch 15, batch    20 | loss: 11.6325990CurrentTrain: epoch 15, batch    21 | loss: 12.8124634CurrentTrain: epoch 15, batch    22 | loss: 12.7129495CurrentTrain: epoch 15, batch    23 | loss: 18.5152789CurrentTrain: epoch 15, batch    24 | loss: 8.8590537CurrentTrain: epoch 15, batch    25 | loss: 10.1275512CurrentTrain: epoch 15, batch    26 | loss: 24.0583597CurrentTrain: epoch 15, batch    27 | loss: 11.4399764CurrentTrain: epoch 15, batch    28 | loss: 16.8372078CurrentTrain: epoch 15, batch    29 | loss: 11.7879979CurrentTrain: epoch 15, batch    30 | loss: 13.6535717CurrentTrain: epoch 15, batch    31 | loss: 11.5643485CurrentTrain: epoch 15, batch    32 | loss: 10.7922238CurrentTrain: epoch 15, batch    33 | loss: 16.2471182CurrentTrain: epoch 15, batch    34 | loss: 15.5162282CurrentTrain: epoch 15, batch    35 | loss: 15.2857348CurrentTrain: epoch 15, batch    36 | loss: 15.2330566CurrentTrain: epoch 15, batch    37 | loss: 11.3938619CurrentTrain: epoch 15, batch    38 | loss: 9.9515536CurrentTrain: epoch 15, batch    39 | loss: 15.4655938CurrentTrain: epoch 15, batch    40 | loss: 12.5183954CurrentTrain: epoch 15, batch    41 | loss: 26.3440638CurrentTrain: epoch 15, batch    42 | loss: 17.5838511CurrentTrain: epoch 15, batch    43 | loss: 16.3055565CurrentTrain: epoch 15, batch    44 | loss: 15.4480716CurrentTrain: epoch 15, batch    45 | loss: 10.1813733CurrentTrain: epoch 15, batch    46 | loss: 14.9904211CurrentTrain: epoch 15, batch    47 | loss: 13.4964031CurrentTrain: epoch 15, batch    48 | loss: 12.8681072CurrentTrain: epoch 15, batch    49 | loss: 8.1699038CurrentTrain: epoch 15, batch    50 | loss: 11.0857321CurrentTrain: epoch 15, batch    51 | loss: 19.4939299CurrentTrain: epoch 15, batch    52 | loss: 24.3617422CurrentTrain: epoch 15, batch    53 | loss: 11.0208595CurrentTrain: epoch 15, batch    54 | loss: 10.1271000CurrentTrain: epoch 15, batch    55 | loss: 10.2727881CurrentTrain: epoch 15, batch    56 | loss: 14.2284228CurrentTrain: epoch 15, batch    57 | loss: 15.0681472CurrentTrain: epoch 15, batch    58 | loss: 11.0544608CurrentTrain: epoch 15, batch    59 | loss: 10.6657576CurrentTrain: epoch 15, batch    60 | loss: 11.6687312CurrentTrain: epoch 15, batch    61 | loss: 15.0884046CurrentTrain: epoch  7, batch    62 | loss: 8.1982068CurrentTrain: epoch 15, batch     0 | loss: 28.1772628CurrentTrain: epoch 15, batch     1 | loss: 15.3229498CurrentTrain: epoch 15, batch     2 | loss: 16.8730104CurrentTrain: epoch 15, batch     3 | loss: 9.0214572CurrentTrain: epoch 15, batch     4 | loss: 12.6208744CurrentTrain: epoch 15, batch     5 | loss: 17.5971697CurrentTrain: epoch 15, batch     6 | loss: 20.7421675CurrentTrain: epoch 15, batch     7 | loss: 17.7937281CurrentTrain: epoch 15, batch     8 | loss: 16.4822998CurrentTrain: epoch 15, batch     9 | loss: 15.5923690CurrentTrain: epoch 15, batch    10 | loss: 10.9066565CurrentTrain: epoch 15, batch    11 | loss: 9.0673582CurrentTrain: epoch 15, batch    12 | loss: 20.2372869CurrentTrain: epoch 15, batch    13 | loss: 12.3798821CurrentTrain: epoch 15, batch    14 | loss: 7.3342321CurrentTrain: epoch 15, batch    15 | loss: 10.8553444CurrentTrain: epoch 15, batch    16 | loss: 12.1485730CurrentTrain: epoch 15, batch    17 | loss: 11.5747362CurrentTrain: epoch 15, batch    18 | loss: 14.8685585CurrentTrain: epoch 15, batch    19 | loss: 10.5167844CurrentTrain: epoch 15, batch    20 | loss: 10.5911876CurrentTrain: epoch 15, batch    21 | loss: 15.7915588CurrentTrain: epoch 15, batch    22 | loss: 9.4089588CurrentTrain: epoch 15, batch    23 | loss: 19.6769783CurrentTrain: epoch 15, batch    24 | loss: 14.2322335CurrentTrain: epoch 15, batch    25 | loss: 14.7114311CurrentTrain: epoch 15, batch    26 | loss: 18.6834667CurrentTrain: epoch 15, batch    27 | loss: 12.3200527CurrentTrain: epoch 15, batch    28 | loss: 12.2782850CurrentTrain: epoch 15, batch    29 | loss: 13.7246235CurrentTrain: epoch 15, batch    30 | loss: 28.1592597CurrentTrain: epoch 15, batch    31 | loss: 31.9154161CurrentTrain: epoch 15, batch    32 | loss: 19.4118697CurrentTrain: epoch 15, batch    33 | loss: 11.9872900CurrentTrain: epoch 15, batch    34 | loss: 12.5095252CurrentTrain: epoch 15, batch    35 | loss: 16.0355470CurrentTrain: epoch 15, batch    36 | loss: 11.0586427CurrentTrain: epoch 15, batch    37 | loss: 19.3478917CurrentTrain: epoch 15, batch    38 | loss: 16.8259035CurrentTrain: epoch 15, batch    39 | loss: 16.5624348CurrentTrain: epoch 15, batch    40 | loss: 10.6593998CurrentTrain: epoch 15, batch    41 | loss: 15.1129589CurrentTrain: epoch 15, batch    42 | loss: 9.9886440CurrentTrain: epoch 15, batch    43 | loss: 18.9145738CurrentTrain: epoch 15, batch    44 | loss: 13.1276922CurrentTrain: epoch 15, batch    45 | loss: 9.7955581CurrentTrain: epoch 15, batch    46 | loss: 12.2719752CurrentTrain: epoch 15, batch    47 | loss: 24.5590527CurrentTrain: epoch 15, batch    48 | loss: 7.8193912CurrentTrain: epoch 15, batch    49 | loss: 17.5936375CurrentTrain: epoch 15, batch    50 | loss: 11.1259774CurrentTrain: epoch 15, batch    51 | loss: 19.3761381CurrentTrain: epoch 15, batch    52 | loss: 17.2411449CurrentTrain: epoch 15, batch    53 | loss: 13.3230371CurrentTrain: epoch 15, batch    54 | loss: 13.9761901CurrentTrain: epoch 15, batch    55 | loss: 17.4483777CurrentTrain: epoch 15, batch    56 | loss: 33.8270875CurrentTrain: epoch 15, batch    57 | loss: 11.2267405CurrentTrain: epoch 15, batch    58 | loss: 26.9401593CurrentTrain: epoch 15, batch    59 | loss: 16.5511011CurrentTrain: epoch 15, batch    60 | loss: 11.8072813CurrentTrain: epoch 15, batch    61 | loss: 13.7000370CurrentTrain: epoch  7, batch    62 | loss: 8.0598591CurrentTrain: epoch 15, batch     0 | loss: 13.3754633CurrentTrain: epoch 15, batch     1 | loss: 14.8417635CurrentTrain: epoch 15, batch     2 | loss: 16.5425924CurrentTrain: epoch 15, batch     3 | loss: 11.3080030CurrentTrain: epoch 15, batch     4 | loss: 12.1841730CurrentTrain: epoch 15, batch     5 | loss: 16.0256999CurrentTrain: epoch 15, batch     6 | loss: 14.9938759CurrentTrain: epoch 15, batch     7 | loss: 20.4373025CurrentTrain: epoch 15, batch     8 | loss: 12.7598105CurrentTrain: epoch 15, batch     9 | loss: 14.0781120CurrentTrain: epoch 15, batch    10 | loss: 11.9820162CurrentTrain: epoch 15, batch    11 | loss: 11.6630580CurrentTrain: epoch 15, batch    12 | loss: 14.4463356CurrentTrain: epoch 15, batch    13 | loss: 17.9517525CurrentTrain: epoch 15, batch    14 | loss: 12.2839913CurrentTrain: epoch 15, batch    15 | loss: 22.3831597CurrentTrain: epoch 15, batch    16 | loss: 8.1428487CurrentTrain: epoch 15, batch    17 | loss: 15.7153029CurrentTrain: epoch 15, batch    18 | loss: 13.9912430CurrentTrain: epoch 15, batch    19 | loss: 19.0217641CurrentTrain: epoch 15, batch    20 | loss: 10.0597052CurrentTrain: epoch 15, batch    21 | loss: 18.2290292CurrentTrain: epoch 15, batch    22 | loss: 15.6837833CurrentTrain: epoch 15, batch    23 | loss: 16.0614783CurrentTrain: epoch 15, batch    24 | loss: 12.3442721CurrentTrain: epoch 15, batch    25 | loss: 14.3622141CurrentTrain: epoch 15, batch    26 | loss: 14.3857814CurrentTrain: epoch 15, batch    27 | loss: 10.4607693CurrentTrain: epoch 15, batch    28 | loss: 19.7529219CurrentTrain: epoch 15, batch    29 | loss: 15.3647038CurrentTrain: epoch 15, batch    30 | loss: 9.9263325CurrentTrain: epoch 15, batch    31 | loss: 10.5925901CurrentTrain: epoch 15, batch    32 | loss: 9.1402143CurrentTrain: epoch 15, batch    33 | loss: 8.7334252CurrentTrain: epoch 15, batch    34 | loss: 16.4236899CurrentTrain: epoch 15, batch    35 | loss: 13.3865971CurrentTrain: epoch 15, batch    36 | loss: 17.1916233CurrentTrain: epoch 15, batch    37 | loss: 15.3180096CurrentTrain: epoch 15, batch    38 | loss: 18.8230657CurrentTrain: epoch 15, batch    39 | loss: 18.7725147CurrentTrain: epoch 15, batch    40 | loss: 17.9363125CurrentTrain: epoch 15, batch    41 | loss: 20.6826663CurrentTrain: epoch 15, batch    42 | loss: 19.3955498CurrentTrain: epoch 15, batch    43 | loss: 17.0998434CurrentTrain: epoch 15, batch    44 | loss: 9.9682175CurrentTrain: epoch 15, batch    45 | loss: 13.8784467CurrentTrain: epoch 15, batch    46 | loss: 10.6217970CurrentTrain: epoch 15, batch    47 | loss: 11.9028515CurrentTrain: epoch 15, batch    48 | loss: 11.4327082CurrentTrain: epoch 15, batch    49 | loss: 10.9124690CurrentTrain: epoch 15, batch    50 | loss: 12.1990870CurrentTrain: epoch 15, batch    51 | loss: 15.9702325CurrentTrain: epoch 15, batch    52 | loss: 9.7055788CurrentTrain: epoch 15, batch    53 | loss: 16.0118786CurrentTrain: epoch 15, batch    54 | loss: 13.6237878CurrentTrain: epoch 15, batch    55 | loss: 11.0394387CurrentTrain: epoch 15, batch    56 | loss: 9.8589545CurrentTrain: epoch 15, batch    57 | loss: 19.2201679CurrentTrain: epoch 15, batch    58 | loss: 11.7971041CurrentTrain: epoch 15, batch    59 | loss: 9.9768908CurrentTrain: epoch 15, batch    60 | loss: 19.1673744CurrentTrain: epoch 15, batch    61 | loss: 9.7815563CurrentTrain: epoch  7, batch    62 | loss: 14.5693766
#### movement ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "movement" means literary, artistic, scientific or philosophical movement or scene associated with this person or work.
Here is an example:
Relation: movement
Context: the large workshops of peter paul rubens and jacob jordaens , and the influence of anthony van dyck , made antwerp the center of the flemish baroque .
Head Entity: jacob jordaens
Tail Entity: flemish baroque
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "movement":
Relation: movement  
Context: The emergence of surrealism in the early 20th century was significantly influenced by the works of Salvador Dalí and André Breton.  
Head Entity: Salvador Dalí  
Tail Entity: surrealism  

Relation: movement  
Context: The impact of the Harlem Renaissance on American literature and culture can be seen through the works of Langston Hughes and Zora Neale Hurston.  
Head Entity: Langston Hughes  
Tail Entity: Harlem Renaissance  
#### located in the administrative territorial entity ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "located in the administrative territorial entity" means the item is located on the territory of the following administrative entity.
Here is an example:
Relation: located in the administrative territorial entity
Context: lyons hill is a restored village , and former parish with church , now part of the community of ardclough in north county kildare .
Head Entity: ardclough
Tail Entity: county kildare
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "located in the administrative territorial entity":
Relation: located in the administrative territorial entity  
Context: The city of San Francisco is known for its iconic Golden Gate Bridge and is part of the larger San Francisco County in California.  
Head Entity: San Francisco  
Tail Entity: San Francisco County  

Relation: located in the administrative territorial entity  
Context: The historic town of Bath is famous for its Roman baths and is situated within the unitary authority of Bath and North East Somerset.  
Head Entity: Bath  
Tail Entity: Bath and North East Somerset  
#### head of government ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "head of government" means head of the executive power of this town, city, municipality, state, country, or other governmental body.
Here is an example:
Relation: head of government
Context: on february 10 , 2010 , stefan mappus was elected minister - president of baden - württemberg , and asked that all ministers continue their roles .
Head Entity: baden - württemberg
Tail Entity: stefan mappus
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "head of government":
Relation: head of government  
Context: in 2021, jacinda ardern was re-elected as the prime minister of new zealand, leading the labor party to a significant victory in the general elections.  
Head Entity: new zealand  
Tail Entity: jacinda ardern  

Relation: head of government  
Context: during the annual summit, justin trudeau addressed the attendees as the prime minister of canada, outlining his government's plans for the upcoming year.  
Head Entity: canada  
Tail Entity: justin trudeau  
#### member of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "member of" means organization, club or musical group to which the subject belongs.
Here is an example:
Relation: member of
Context: hank henshaw became a member of the sinestro corps during the sinestro corps war .
Head Entity: hank henshaw
Tail Entity: sinestro corps
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "member of":
Relation: member of  
Context: after years of dedication, she was finally recognized as a member of the national ballet company.  
Head Entity: she  
Tail Entity: national ballet company  

Relation: member of  
Context: the renowned scientist was proud to be a member of the international research consortium focused on climate change.  
Head Entity: the renowned scientist  
Tail Entity: international research consortium  
#### crosses ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "crosses" means obstacle which this bridge crosses over or this tunnel goes under.
Here is an example:
Relation: crosses
Context: the green island bridge crosses the hudson river in new york , connecting green island with troy .
Head Entity: green island bridge
Tail Entity: hudson river
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "crosses":
Relation: crosses  
Context: the golden gate bridge crosses the san francisco bay, linking san francisco to marin county.  
Head Entity: golden gate bridge  
Tail Entity: san francisco bay  

Relation: crosses  
Context: the london underground crosses the thames river, providing a vital transport link between the north and south of the city.  
Head Entity: london underground  
Tail Entity: thames river  
#### tributary ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "tributary" means watercourse that flows into an other one.
Here is an example:
Relation: tributary
Context: it is on the panj river just downstream from the mouth of the bartang river at the point where the panj briefly turns west before resuming its northerly course .
Head Entity: panj river
Tail Entity: bartang river
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "tributary":
Relation: tributary  
Context: The Missouri River, which is a major tributary of the Mississippi River, flows through several states before joining the larger watercourse.  
Head Entity: Missouri River  
Tail Entity: Mississippi River  

Relation: tributary  
Context: The Rio Grande serves as a natural border and has several tributaries, including the Pecos River, which contributes to its flow.  
Head Entity: Pecos River  
Tail Entity: Rio Grande  
#### position held ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "position held" means subject currently or formerly holds the object position or public office.
Here is an example:
Relation: position held
Context: in 1973 , the college was again hit by financial difficulties , but a direct appeal for assistance to the prime minister , norman kirk , secured te aute 's future .
Head Entity: norman kirk
Tail Entity: prime minister
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "position held":
Relation: position held  
Context: After serving as the mayor for over a decade, Sarah Thompson announced her retirement, paving the way for new leadership in the city council.  
Head Entity: Sarah Thompson  
Tail Entity: mayor  

Relation: position held  
Context: During his tenure as the governor, Mark Johnson implemented several reforms that significantly improved the education system in the state.  
Head Entity: Mark Johnson  
Tail Entity: governor  
#### developer ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "developer" means organization or person that developed the item.
Here is an example:
Relation: developer
Context: a headcrab is a fictional alien parasitoid found in the " half - life " video game series created by valve software .
Head Entity: half - life
Tail Entity: valve software
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "developer":
Relation: developer  
Context: The popular game "The Witcher 3: Wild Hunt" was developed by CD Projekt Red, a well-known Polish video game developer.  
Head Entity: The Witcher 3: Wild Hunt  
Tail Entity: CD Projekt Red  

Relation: developer  
Context: The innovative mobile game "Angry Birds" was created by Rovio Entertainment, a Finnish company that specializes in game development.  
Head Entity: Angry Birds  
Tail Entity: Rovio Entertainment  
#### location of formation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "location of formation" means location where a group or organization was formed.
Here is an example:
Relation: location of formation
Context: relentless mutation is the 3rd studio album from vancouver , british columbia - based technical death metal band archspire .
Head Entity: archspire
Tail Entity: vancouver
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "location of formation":
Relation: location of formation  
Context: the famous tech company was established in silicon valley, california, by a group of innovative engineers.  
Head Entity: the famous tech company  
Tail Entity: silicon valley  

Relation: location of formation  
Context: the renowned art collective originated in the vibrant streets of brooklyn, new york, where creativity flourished.  
Head Entity: the renowned art collective  
Tail Entity: brooklyn  
#### country of origin ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "country of origin" means country of origin of this item (creative work, food, phrase, product, etc.).
Here is an example:
Relation: country of origin
Context: she has appeared in some american television shows , beginning with her regular role as snow white 's stepmother , evil queen lillian " lily " white in the series " the charmings " .
Head Entity: the charmings
Tail Entity: american
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "country of origin":
Relation: country of origin  
Context: The famous dish sushi is traditionally made with vinegared rice and often includes seafood, vegetables, and occasionally tropical fruits, originating from Japan.  
Head Entity: sushi  
Tail Entity: Japan  

Relation: country of origin  
Context: The iconic painting "Mona Lisa," known for its captivating smile, was created by the renowned artist Leonardo da Vinci in Italy during the Renaissance period.  
Head Entity: Mona Lisa  
Tail Entity: Italy  
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 83.75%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 85.71%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 88.89%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 89.38%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 89.77%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 90.38%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 90.83%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 91.41%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 91.91%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 92.01%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 92.43%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 92.81%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 92.86%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 92.33%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 92.66%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 92.97%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 92.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 93.03%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 93.06%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 93.08%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 93.10%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 93.33%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 93.55%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 93.94%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 94.12%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 94.11%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 94.10%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 94.26%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 94.24%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 94.39%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 94.53%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 94.66%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 94.79%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 94.91%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 95.03%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 95.14%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 95.24%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 95.35%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 95.31%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 95.41%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 95.50%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 95.47%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 95.31%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 95.28%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 95.37%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 95.11%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 95.09%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 95.18%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 95.15%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 95.23%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 95.21%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 95.29%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 95.26%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 94.54%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 83.75%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 85.71%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 88.89%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 89.38%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 89.77%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 90.38%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 90.83%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 91.41%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 91.91%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 92.01%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 92.43%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 92.81%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 92.86%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 92.33%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 92.66%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 92.97%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 92.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 93.03%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 93.06%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 93.08%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 93.10%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 93.33%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 93.55%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 93.94%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 94.12%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 94.11%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 94.10%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 94.26%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 94.24%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 94.39%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 94.53%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 94.66%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 94.79%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 94.91%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 95.03%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 95.14%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 95.24%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 95.35%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 95.31%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 95.41%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 95.50%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 95.47%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 95.31%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 95.28%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 95.37%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 95.11%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 95.09%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 95.18%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 95.15%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 95.23%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 95.21%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 95.29%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 95.26%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 94.54%   
cur_acc:  ['0.9454']
his_acc:  ['0.9454']
CurrentTrain: epoch 15, batch     0 | loss: 22.6633073CurrentTrain: epoch 15, batch     1 | loss: 19.6721718CurrentTrain: epoch 15, batch     2 | loss: 23.5259712CurrentTrain: epoch  1, batch     3 | loss: 11.4885002CurrentTrain: epoch 15, batch     0 | loss: 17.7801441CurrentTrain: epoch 15, batch     1 | loss: 23.7390777CurrentTrain: epoch 15, batch     2 | loss: 18.3649185CurrentTrain: epoch  1, batch     3 | loss: 13.9314343CurrentTrain: epoch 15, batch     0 | loss: 19.1459972CurrentTrain: epoch 15, batch     1 | loss: 17.9503507CurrentTrain: epoch 15, batch     2 | loss: 17.0798877CurrentTrain: epoch  1, batch     3 | loss: 14.8368285CurrentTrain: epoch 15, batch     0 | loss: 13.1607408CurrentTrain: epoch 15, batch     1 | loss: 13.4081763CurrentTrain: epoch 15, batch     2 | loss: 14.0538444CurrentTrain: epoch  1, batch     3 | loss: 11.5843890CurrentTrain: epoch 15, batch     0 | loss: 24.1677770CurrentTrain: epoch 15, batch     1 | loss: 13.2958771CurrentTrain: epoch 15, batch     2 | loss: 16.8876313CurrentTrain: epoch  1, batch     3 | loss: 7.6636854CurrentTrain: epoch 15, batch     0 | loss: 11.0964840CurrentTrain: epoch 15, batch     1 | loss: 13.3495888CurrentTrain: epoch 15, batch     2 | loss: 10.6834205CurrentTrain: epoch  1, batch     3 | loss: 6.6510593CurrentTrain: epoch 15, batch     0 | loss: 10.9368261CurrentTrain: epoch 15, batch     1 | loss: 12.9951768CurrentTrain: epoch 15, batch     2 | loss: 13.8228006CurrentTrain: epoch  1, batch     3 | loss: 7.6080403CurrentTrain: epoch 15, batch     0 | loss: 15.9004856CurrentTrain: epoch 15, batch     1 | loss: 10.4825736CurrentTrain: epoch 15, batch     2 | loss: 18.5536601CurrentTrain: epoch  1, batch     3 | loss: 6.8523627CurrentTrain: epoch 15, batch     0 | loss: 14.7461076CurrentTrain: epoch 15, batch     1 | loss: 8.1178214CurrentTrain: epoch 15, batch     2 | loss: 11.2731322CurrentTrain: epoch  1, batch     3 | loss: 6.8276243CurrentTrain: epoch 15, batch     0 | loss: 9.1440944CurrentTrain: epoch 15, batch     1 | loss: 8.7515962CurrentTrain: epoch 15, batch     2 | loss: 10.0177436CurrentTrain: epoch  1, batch     3 | loss: 6.8310134
#### main subject ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "main subject" means primary topic of a work.
Here is an example:
Relation: main subject
Context: for more information on the grand mosque seizure itself , see " the siege of mecca " by yaroslav trofimov .
Head Entity: the siege of mecca
Tail Entity: grand mosque seizure
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "main subject":
Relation: main subject  
Context: in her book "the power of habit," charles duhigg explores the science behind why habits exist and how they can be changed.  
Head Entity: the power of habit  
Tail Entity: science of habits  

Relation: main subject  
Context: "sapiens: a brief history of humankind" by yuval noah harari provides a compelling overview of the history and impact of our species.  
Head Entity: sapiens: a brief history of humankind  
Tail Entity: history of humankind  
#### participating team ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "participating team" means teams that actively takes/took part in an event or process.
Here is an example:
Relation: participating team
Context: he represented romania at euro 2000 , his first major tournament where they beat england 3–2 , and drew 1–1 against germany .
Head Entity: euro 2000
Tail Entity: germany
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "participating team":
Relation: participating team  
Context: The Brazilian national team showcased their skills at the 2014 FIFA World Cup, where they faced off against Germany in the semi-finals.  
Head Entity: 2014 FIFA World Cup  
Tail Entity: Germany  

Relation: participating team  
Context: During the 2021 Tokyo Olympics, the United States women's soccer team competed fiercely, ultimately playing against Canada in the semi-finals.  
Head Entity: 2021 Tokyo Olympics  
Tail Entity: Canada  
#### has part ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "has part" means part of this subject.
Here is an example:
Relation: has part
Context: among the competing artists was previous eurovision song contest entrant robin juhkental who represented estonia in 2010 as part of the group malcolm lincoln .
Head Entity: malcolm lincoln
Tail Entity: robin juhkental
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "has part":
Relation: has part  
Context: The famous painting "The Last Supper" features several apostles, including Judas Iscariot, who is depicted as part of the group gathered around Jesus.  
Head Entity: The Last Supper  
Tail Entity: Judas Iscariot  

Relation: has part  
Context: The smartphone model Galaxy S21 comes with various components, including the Exynos 2100 processor, which is an integral part of its performance.  
Head Entity: Galaxy S21  
Tail Entity: Exynos 2100 processor  
#### nominated for ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "nominated for" means award nomination received by a person, organisation or creative work.
Here is an example:
Relation: nominated for
Context: he was nominated for the academy award for best picture , along with steven spielberg , ian bryce , mark gordon for the film " saving private ryan " .
Head Entity: ian bryce
Tail Entity: academy award for best picture
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "nominated for":
Relation: nominated for  
Context: The film "The Shape of Water" received multiple nominations, including a nod for best director for Guillermo del Toro at the Academy Awards.  
Head Entity: Guillermo del Toro  
Tail Entity: Academy Awards  

Relation: nominated for  
Context: The popular band was nominated for the Grammy Award for Best New Artist after their debut album topped the charts.  
Head Entity: the popular band  
Tail Entity: Grammy Award for Best New Artist  
#### instance of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "instance of" means that class of which this subject is a particular example and member.
Here is an example:
Relation: instance of
Context: on march 2015 , cube entertainment launched the multi - national girl group clc including original members seunghee , yujin , seungyeon , sorn and yeeun .
Head Entity: clc
Tail Entity: girl group
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "instance of":
Relation: instance of  
Context: The smartphone brand Apple released its latest model, the iPhone 14, which features advanced technology and improved camera capabilities.  
Head Entity: iPhone 14  
Tail Entity: smartphone  

Relation: instance of  
Context: The famous painting "Starry Night" was created by the artist Vincent van Gogh and is considered a masterpiece of post-impressionism.  
Head Entity: Starry Night  
Tail Entity: painting  
#### mountain range ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "mountain range" means range or subrange to which the geographical item belongs.
Here is an example:
Relation: mountain range
Context: brand peak is a sharp snow - covered peak located east - southeast of the eternity range and northwest of mount duemler , in palmer land .
Head Entity: mount duemler
Tail Entity: eternity range
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "mountain range":
Relation: mountain range  
Context: the sierra nevada is a major mountain range in the western united states, known for its stunning landscapes and diverse ecosystems, and it includes famous peaks like mount whitney and half dome.  
Head Entity: mount whitney  
Tail Entity: sierra nevada  

Relation: mountain range  
Context: the appalachian mountains stretch from georgia to maine, featuring a variety of subranges, including the great smoky mountains, which are renowned for their biodiversity and scenic beauty.  
Head Entity: great smoky mountains  
Tail Entity: appalachian mountains  
#### screenwriter ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "screenwriter" means person who wrote the script for subject item.
Here is an example:
Relation: screenwriter
Context: it would also be the last episode to feature a grounding , until " the marge - ian chronicles " in season 27 , six years later ( also written by brian kelley ) .
Head Entity: the marge - ian chronicles
Tail Entity: brian kelley
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "screenwriter":
Relation: screenwriter  
Context: The film "Inception" was a groundbreaking project that showcased the visionary talents of its creator, with the screenplay crafted by the brilliant Christopher Nolan.  
Head Entity: Inception  
Tail Entity: Christopher Nolan  

Relation: screenwriter  
Context: The beloved animated feature "Toy Story" was brought to life through the imaginative script penned by the talented Andrew Stanton.  
Head Entity: Toy Story  
Tail Entity: Andrew Stanton  
#### language of work or name ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "language of work or name" means language associated with this creative work (such as books, shows, songs, broadcasts or websites) or a name.
Here is an example:
Relation: language of work or name
Context: the publisher found success in 1993 with tarō gomi 's " everyone poops " , a japanese language import that sold over one million copies and is now their best - known title .
Head Entity: everyone poops
Tail Entity: japanese language
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "language of work or name":
Relation: language of work or name  
Context: The animated series "Avatar: The Last Airbender" features a rich narrative and is primarily produced in English, appealing to a global audience.  
Head Entity: Avatar: The Last Airbender  
Tail Entity: English  

Relation: language of work or name  
Context: The famous novel "Cien años de soledad" by Gabriel García Márquez is celebrated for its magical realism and is originally written in Spanish, captivating readers worldwide.  
Head Entity: Cien años de soledad  
Tail Entity: Spanish  
#### operator ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "operator" means person, profession, or organization that operates the equipment, facility, or service.
Here is an example:
Relation: operator
Context: the aircraft was a boeing 707 - 321c that first entered service with pan american world airways ( pan am ) in 1963 .
Head Entity: boeing 707 - 321c
Tail Entity: pan american world airways
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "operator":
Relation: operator  
Context: the new electric bus fleet is managed by the city transportation authority, which ensures efficient public transit services.  
Head Entity: electric bus fleet  
Tail Entity: city transportation authority  

Relation: operator  
Context: the state-of-the-art research facility is operated by the national science foundation, providing resources for groundbreaking studies.  
Head Entity: research facility  
Tail Entity: national science foundation  
#### religion ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "religion" means religion of a person, organization or religious building, or associated with this subject.
Here is an example:
Relation: religion
Context: the most important lama of this series was the third changkya , rolpai dorje , who was preceptor to the qianlong emperor of china , and chief representative of tibetan buddhism at the qing court .
Head Entity: rolpai dorje
Tail Entity: tibetan buddhism
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "religion":
Relation: religion  
Context: the cathedral of notre-dame is a famous example of french gothic architecture and serves as the seat of the archbishop of paris, representing the roman catholic faith in the heart of the city.  
Head Entity: cathedral of notre-dame  
Tail Entity: roman catholic faith  

Relation: religion  
Context: the dalai lama is a prominent figure in the world, known for his teachings on compassion and non-violence, and is the spiritual leader of tibetan buddhism.  
Head Entity: dalai lama  
Tail Entity: tibetan buddhism  
MemoryTrain:  epoch 15, batch     0 | loss: 5.8701529MemoryTrain:  epoch 15, batch     1 | loss: 8.8160776MemoryTrain:  epoch 15, batch     2 | loss: 5.3640072MemoryTrain:  epoch 11, batch     3 | loss: 6.1807489MemoryTrain:  epoch 15, batch     0 | loss: 8.1084670MemoryTrain:  epoch 15, batch     1 | loss: 11.0386526MemoryTrain:  epoch 15, batch     2 | loss: 9.5014388MemoryTrain:  epoch 11, batch     3 | loss: 6.2487095MemoryTrain:  epoch 15, batch     0 | loss: 9.0752569MemoryTrain:  epoch 15, batch     1 | loss: 9.1347341MemoryTrain:  epoch 15, batch     2 | loss: 9.9328171MemoryTrain:  epoch 11, batch     3 | loss: 6.7907106MemoryTrain:  epoch 15, batch     0 | loss: 7.0718798MemoryTrain:  epoch 15, batch     1 | loss: 5.8408577MemoryTrain:  epoch 15, batch     2 | loss: 4.7266009MemoryTrain:  epoch 11, batch     3 | loss: 6.3308933MemoryTrain:  epoch 15, batch     0 | loss: 6.2847957MemoryTrain:  epoch 15, batch     1 | loss: 4.8776066MemoryTrain:  epoch 15, batch     2 | loss: 6.0617551MemoryTrain:  epoch 11, batch     3 | loss: 2.9129692MemoryTrain:  epoch 15, batch     0 | loss: 5.7541687MemoryTrain:  epoch 15, batch     1 | loss: 8.5830608MemoryTrain:  epoch 15, batch     2 | loss: 3.4896198MemoryTrain:  epoch 11, batch     3 | loss: 5.6685448MemoryTrain:  epoch 15, batch     0 | loss: 6.2740740MemoryTrain:  epoch 15, batch     1 | loss: 6.5566148MemoryTrain:  epoch 15, batch     2 | loss: 5.2234489MemoryTrain:  epoch 11, batch     3 | loss: 5.8804267MemoryTrain:  epoch 15, batch     0 | loss: 2.6995164MemoryTrain:  epoch 15, batch     1 | loss: 5.7178589MemoryTrain:  epoch 15, batch     2 | loss: 3.0859106MemoryTrain:  epoch 11, batch     3 | loss: 4.4758190MemoryTrain:  epoch 15, batch     0 | loss: 3.7104593MemoryTrain:  epoch 15, batch     1 | loss: 4.6177999MemoryTrain:  epoch 15, batch     2 | loss: 4.0931232MemoryTrain:  epoch 11, batch     3 | loss: 10.5129948MemoryTrain:  epoch 15, batch     0 | loss: 8.3779838MemoryTrain:  epoch 15, batch     1 | loss: 3.5033219MemoryTrain:  epoch 15, batch     2 | loss: 7.5683052MemoryTrain:  epoch 11, batch     3 | loss: 10.6260637
[EVAL] batch:    0 | acc: 50.00%,  total acc: 50.00%   [EVAL] batch:    1 | acc: 50.00%,  total acc: 50.00%   [EVAL] batch:    2 | acc: 56.25%,  total acc: 52.08%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 57.81%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 61.25%   [EVAL] batch:    5 | acc: 62.50%,  total acc: 61.46%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 66.96%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 70.31%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 72.92%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 74.38%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 76.70%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 78.65%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 78.37%   [EVAL] batch:   13 | acc: 31.25%,  total acc: 75.00%   [EVAL] batch:   14 | acc: 31.25%,  total acc: 72.08%   [EVAL] batch:   15 | acc: 18.75%,  total acc: 68.75%   [EVAL] batch:   16 | acc: 56.25%,  total acc: 68.01%   [EVAL] batch:   17 | acc: 43.75%,  total acc: 66.67%   [EVAL] batch:   18 | acc: 37.50%,  total acc: 65.13%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 66.88%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 68.45%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 69.60%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 70.92%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 72.14%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 73.25%   [EVAL] batch:   25 | acc: 56.25%,  total acc: 72.60%   [EVAL] batch:   26 | acc: 50.00%,  total acc: 71.76%   [EVAL] batch:   27 | acc: 56.25%,  total acc: 71.21%   [EVAL] batch:   28 | acc: 81.25%,  total acc: 71.55%   [EVAL] batch:   29 | acc: 43.75%,  total acc: 70.62%   [EVAL] batch:   30 | acc: 68.75%,  total acc: 70.56%   [EVAL] batch:   31 | acc: 87.50%,  total acc: 71.09%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 71.78%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 72.61%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 73.39%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 74.13%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 74.83%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 75.49%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 76.12%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 76.72%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 77.29%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 77.83%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 78.34%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 78.84%   [EVAL] batch:   44 | acc: 81.25%,  total acc: 78.89%   [EVAL] batch:   45 | acc: 93.75%,  total acc: 79.21%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 79.52%   [EVAL] batch:   47 | acc: 81.25%,  total acc: 79.56%   [EVAL] batch:   48 | acc: 93.75%,  total acc: 79.85%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 80.12%   [EVAL] batch:   50 | acc: 81.25%,  total acc: 80.15%   [EVAL] batch:   51 | acc: 81.25%,  total acc: 80.17%   [EVAL] batch:   52 | acc: 81.25%,  total acc: 80.19%   [EVAL] batch:   53 | acc: 87.50%,  total acc: 80.32%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 80.45%   [EVAL] batch:   55 | acc: 75.00%,  total acc: 80.36%   [EVAL] batch:   56 | acc: 87.50%,  total acc: 80.48%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 80.60%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 80.83%   [EVAL] batch:   59 | acc: 75.00%,  total acc: 80.73%   [EVAL] batch:   60 | acc: 87.50%,  total acc: 80.84%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 80.65%   [EVAL] batch:   62 | acc: 37.50%,  total acc: 79.96%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 82.50%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 82.14%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 83.59%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 85.42%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 86.25%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 85.80%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 86.46%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 86.54%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 87.05%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 88.28%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 88.97%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 89.24%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 89.80%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 90.00%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 90.18%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 90.06%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 90.22%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 90.36%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 90.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 90.97%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 91.07%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 91.38%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 91.67%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 91.94%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 92.19%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 92.42%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 92.65%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 92.68%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 92.88%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 93.07%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 93.26%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 93.43%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 93.59%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 93.90%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 94.04%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 94.18%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 94.31%   [EVAL] batch:   45 | acc: 87.50%,  total acc: 94.16%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 94.15%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 94.14%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 94.26%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 94.25%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 94.24%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 94.11%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 94.10%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 94.21%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 94.09%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 94.08%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 94.08%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 93.97%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 93.86%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 93.85%   [EVAL] batch:   60 | acc: 93.75%,  total acc: 93.85%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 93.75%   [EVAL] batch:   62 | acc: 68.75%,  total acc: 93.35%   [EVAL] batch:   63 | acc: 56.25%,  total acc: 92.77%   [EVAL] batch:   64 | acc: 50.00%,  total acc: 92.12%   [EVAL] batch:   65 | acc: 68.75%,  total acc: 91.76%   [EVAL] batch:   66 | acc: 68.75%,  total acc: 91.42%   [EVAL] batch:   67 | acc: 75.00%,  total acc: 91.18%   [EVAL] batch:   68 | acc: 75.00%,  total acc: 90.94%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 91.07%   [EVAL] batch:   70 | acc: 87.50%,  total acc: 91.02%   [EVAL] batch:   71 | acc: 93.75%,  total acc: 91.06%   [EVAL] batch:   72 | acc: 93.75%,  total acc: 91.10%   [EVAL] batch:   73 | acc: 100.00%,  total acc: 91.22%   [EVAL] batch:   74 | acc: 100.00%,  total acc: 91.33%   [EVAL] batch:   75 | acc: 37.50%,  total acc: 90.62%   [EVAL] batch:   76 | acc: 37.50%,  total acc: 89.94%   [EVAL] batch:   77 | acc: 18.75%,  total acc: 89.02%   [EVAL] batch:   78 | acc: 37.50%,  total acc: 88.37%   [EVAL] batch:   79 | acc: 56.25%,  total acc: 87.97%   [EVAL] batch:   80 | acc: 25.00%,  total acc: 87.19%   [EVAL] batch:   81 | acc: 81.25%,  total acc: 87.12%   [EVAL] batch:   82 | acc: 100.00%,  total acc: 87.27%   [EVAL] batch:   83 | acc: 100.00%,  total acc: 87.43%   [EVAL] batch:   84 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   85 | acc: 100.00%,  total acc: 87.65%   [EVAL] batch:   86 | acc: 100.00%,  total acc: 87.79%   [EVAL] batch:   87 | acc: 81.25%,  total acc: 87.71%   [EVAL] batch:   88 | acc: 37.50%,  total acc: 87.15%   [EVAL] batch:   89 | acc: 68.75%,  total acc: 86.94%   [EVAL] batch:   90 | acc: 62.50%,  total acc: 86.68%   [EVAL] batch:   91 | acc: 68.75%,  total acc: 86.48%   [EVAL] batch:   92 | acc: 50.00%,  total acc: 86.09%   [EVAL] batch:   93 | acc: 75.00%,  total acc: 85.97%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 86.12%   [EVAL] batch:   95 | acc: 93.75%,  total acc: 86.20%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 86.34%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 86.48%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 86.62%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 86.75%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 86.88%   [EVAL] batch:  101 | acc: 100.00%,  total acc: 87.01%   [EVAL] batch:  102 | acc: 100.00%,  total acc: 87.14%   [EVAL] batch:  103 | acc: 100.00%,  total acc: 87.26%   [EVAL] batch:  104 | acc: 100.00%,  total acc: 87.38%   [EVAL] batch:  105 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:  106 | acc: 93.75%,  total acc: 87.56%   [EVAL] batch:  107 | acc: 87.50%,  total acc: 87.56%   [EVAL] batch:  108 | acc: 93.75%,  total acc: 87.61%   [EVAL] batch:  109 | acc: 93.75%,  total acc: 87.67%   [EVAL] batch:  110 | acc: 81.25%,  total acc: 87.61%   [EVAL] batch:  111 | acc: 87.50%,  total acc: 87.61%   [EVAL] batch:  112 | acc: 100.00%,  total acc: 87.72%   [EVAL] batch:  113 | acc: 68.75%,  total acc: 87.55%   [EVAL] batch:  114 | acc: 87.50%,  total acc: 87.55%   [EVAL] batch:  115 | acc: 87.50%,  total acc: 87.55%   [EVAL] batch:  116 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:  117 | acc: 75.00%,  total acc: 87.39%   [EVAL] batch:  118 | acc: 81.25%,  total acc: 87.34%   [EVAL] batch:  119 | acc: 93.75%,  total acc: 87.40%   [EVAL] batch:  120 | acc: 93.75%,  total acc: 87.45%   [EVAL] batch:  121 | acc: 81.25%,  total acc: 87.40%   [EVAL] batch:  122 | acc: 75.00%,  total acc: 87.30%   [EVAL] batch:  123 | acc: 81.25%,  total acc: 87.25%   [EVAL] batch:  124 | acc: 75.00%,  total acc: 87.15%   
cur_acc:  ['0.9454', '0.7996']
his_acc:  ['0.9454', '0.8715']
CurrentTrain: epoch 15, batch     0 | loss: 22.1368665CurrentTrain: epoch 15, batch     1 | loss: 15.6318880CurrentTrain: epoch 15, batch     2 | loss: 15.8082086CurrentTrain: epoch  1, batch     3 | loss: 11.6561397CurrentTrain: epoch 15, batch     0 | loss: 23.9465434CurrentTrain: epoch 15, batch     1 | loss: 16.2363870CurrentTrain: epoch 15, batch     2 | loss: 22.4297786CurrentTrain: epoch  1, batch     3 | loss: 7.7317080CurrentTrain: epoch 15, batch     0 | loss: 11.6271925CurrentTrain: epoch 15, batch     1 | loss: 9.8372027CurrentTrain: epoch 15, batch     2 | loss: 18.3161482CurrentTrain: epoch  1, batch     3 | loss: 7.2496798CurrentTrain: epoch 15, batch     0 | loss: 13.7681525CurrentTrain: epoch 15, batch     1 | loss: 25.9198784CurrentTrain: epoch 15, batch     2 | loss: 12.9805219CurrentTrain: epoch  1, batch     3 | loss: 7.1487277CurrentTrain: epoch 15, batch     0 | loss: 12.9767797CurrentTrain: epoch 15, batch     1 | loss: 13.1805126CurrentTrain: epoch 15, batch     2 | loss: 11.2430068CurrentTrain: epoch  1, batch     3 | loss: 10.3358551CurrentTrain: epoch 15, batch     0 | loss: 10.0766809CurrentTrain: epoch 15, batch     1 | loss: 9.8791340CurrentTrain: epoch 15, batch     2 | loss: 13.8569645CurrentTrain: epoch  1, batch     3 | loss: 8.1648832CurrentTrain: epoch 15, batch     0 | loss: 16.0053447CurrentTrain: epoch 15, batch     1 | loss: 8.9634604CurrentTrain: epoch 15, batch     2 | loss: 12.7792601CurrentTrain: epoch  1, batch     3 | loss: 6.4659397CurrentTrain: epoch 15, batch     0 | loss: 8.8137050CurrentTrain: epoch 15, batch     1 | loss: 14.8433060CurrentTrain: epoch 15, batch     2 | loss: 15.8617069CurrentTrain: epoch  1, batch     3 | loss: 7.5938609CurrentTrain: epoch 15, batch     0 | loss: 13.8004809CurrentTrain: epoch 15, batch     1 | loss: 9.9496162CurrentTrain: epoch 15, batch     2 | loss: 19.4040463CurrentTrain: epoch  1, batch     3 | loss: 5.9423322CurrentTrain: epoch 15, batch     0 | loss: 17.5087030CurrentTrain: epoch 15, batch     1 | loss: 13.8988982CurrentTrain: epoch 15, batch     2 | loss: 11.2400284CurrentTrain: epoch  1, batch     3 | loss: 7.2085962
#### follows ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "follows" means immediately prior item in a series of which the subject is a part.
Here is an example:
Relation: follows
Context: ( 1988 ) , " night watch " ( 2004 ) , " day watch " ( 2006 ) and " legend № 17 " ( 2013 ) .
Head Entity: day watch
Tail Entity: night watch
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "follows":
Relation: follows  
Context: ( 1995 ) , " The Phantom Menace " ( 1999 ) , " Attack of the Clones " ( 2002 ) and " Revenge of the Sith " ( 2005 ) .  
Head Entity: Attack of the Clones  
Tail Entity: The Phantom Menace  

Relation: follows  
Context: ( 2010 ) , " The Hunger Games " ( 2012 ) , " Catching Fire " ( 2013 ) and " Mockingjay " ( 2014 ) .  
Head Entity: Catching Fire  
Tail Entity: The Hunger Games  
#### sport ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "sport" means sport that the subject participates or participated in or is associated with.
Here is an example:
Relation: sport
Context: dick packer was a u.s. soccer center forward who was a member of the u.s. team at the 1956 summer olympics .
Head Entity: dick packer
Tail Entity: soccer
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "sport":
Relation: sport  
Context: serena williams is a professional tennis player who has won numerous grand slam titles throughout her career.  
Head Entity: serena williams  
Tail Entity: tennis  

Relation: sport  
Context: lebron james is known for his exceptional skills in basketball and has played for the los angeles lakers in the nba.  
Head Entity: lebron james  
Tail Entity: basketball  
#### father ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "father" means male parent of the subject.
Here is an example:
Relation: father
Context: after the crowning of his nephew radoslav , the son of stefan , sava left the serbian maritime in 1229 for a trip to palestine .
Head Entity: radoslav
Tail Entity: stefan
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "father":
Relation: father  
Context: in the historical records, it is noted that king henry was the father of prince edward, who later became a significant figure in the kingdom.  
Head Entity: prince edward  
Tail Entity: king henry  

Relation: father  
Context: during the family reunion, it was revealed that michael, the youngest son, was the father of three children, making him the proud parent in the family.  
Head Entity: michael  
Tail Entity: unknown parent
#### original network ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "original network" means network or service that originally broadcast a radio or television program.
Here is an example:
Relation: original network
Context: since 2009 , culshaw has starred in the bbc one comedy sketch show " the impressions show " alongside debra stephenson .
Head Entity: the impressions show
Tail Entity: bbc one
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "original network":
Relation: original network  
Context: The popular series "Breaking Bad" first aired on AMC, captivating audiences with its intense storytelling and character development.  
Head Entity: Breaking Bad  
Tail Entity: AMC  

Relation: original network  
Context: "Friends" became a cultural phenomenon when it premiered on NBC, showcasing the lives and relationships of six friends living in New York City.  
Head Entity: Friends  
Tail Entity: NBC  
#### original language of film or TV show ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "original language of film or TV show" means language in which a film or a performance work was originally created.
Here is an example:
Relation: original language of film or TV show
Context: he was also nominated twice for the filmfare best telugu actor award , for the films bhale bhale magadivoy and " gentleman " .
Head Entity: gentleman
Tail Entity: telugu
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "original language of film or TV show":
Relation: original language of film or TV show  
Context: The movie "Parasite" received critical acclaim and was originally produced in Korean, showcasing the talents of its director Bong Joon-ho.  
Head Entity: Parasite  
Tail Entity: Korean  

Relation: original language of film or TV show  
Context: The animated series "Avatar: The Last Airbender" was created in English and has gained a massive following since its release.  
Head Entity: Avatar: The Last Airbender  
Tail Entity: English  
#### league ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "league" means league in which team or player plays or has played in.
Here is an example:
Relation: league
Context: in the summer of 2013 he returned to montenegro and joined with fk rudar pljevlja , playing with the club for two seasons in the montenegrin first league .
Head Entity: fk rudar pljevlja
Tail Entity: montenegrin first league
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "league":
Relation: league  
Context: After a successful stint in the English Premier League, the player signed with FC Barcelona, competing in La Liga for the next three seasons.  
Head Entity: FC Barcelona  
Tail Entity: La Liga  

Relation: league  
Context: The young athlete made headlines when he joined the New York Yankees, showcasing his talent in Major League Baseball.  
Head Entity: New York Yankees  
Tail Entity: Major League Baseball  
#### mother ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "mother" means female parent of the subject.
Here is an example:
Relation: mother
Context: she is supposedly the third and youngest child of seti i and tuya , and the younger sister of ramesses ii and tia .
Head Entity: tia
Tail Entity: tuya
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "mother":
Relation: mother  
Context: Cleopatra was the daughter of Ptolemy XII and his wife, who was also her mother, Cleopatra VI.  
Head Entity: Cleopatra  
Tail Entity: Cleopatra VI  

Relation: mother  
Context: In Norse mythology, Frigg is known as the mother of Baldr, the god of light and purity.  
Head Entity: Baldr  
Tail Entity: Frigg  
#### instrument ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "instrument" means musical instrument that a person plays or teaches or used in a music occupation.
Here is an example:
Relation: instrument
Context: peter bastian ( 25 august 1943 in copenhagen – 27 march 2017 ) was a danish musician . peter bastian bio and cv he was educated in theoretical physics and classical bassoon .
Head Entity: peter bastian
Tail Entity: bassoon
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "instrument":
Relation: instrument  
Context: johann sebastian bach (31 march 1685 – 28 july 1750) was a german composer and musician of the baroque period. he was known for his exceptional skill in playing the organ and the harpsichord.  
Head Entity: johann sebastian bach  
Tail Entity: organ  

Relation: instrument  
Context: yo-yo ma, born on october 7, 1955, is a world-renowned cellist who has performed with numerous orchestras and in various genres. he is particularly famous for his interpretations of classical cello concertos.  
Head Entity: yo-yo ma  
Tail Entity: cello  
#### country ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "country" means sovereign state of this item (not to be used for human beings).
Here is an example:
Relation: country
Context: a u.s. government - funded $ 36 million bridge over the panj river connects sher khan bandar in afghanistan with nizhniy pyanzh in tajikistan , which transport more than 150 trucks or 1,000 cars daily .
Head Entity: sher khan bandar
Tail Entity: afghanistan
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "country":
Relation: country  
Context: the ancient city of petra, located in southern jordan, is famous for its rock-cut architecture and water conduit system, attracting thousands of tourists each year.  
Head Entity: petra  
Tail Entity: jordan  

Relation: country  
Context: the great wall of china, a series of fortifications made of various materials, stretches across northern china and is a UNESCO World Heritage site.  
Head Entity: great wall of china  
Tail Entity: china  
#### characters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "characters" means characters which appear in this item (like plays, operas, operettas, books, comics, films, TV series, video games).
Here is an example:
Relation: characters
Context: hagrid suggests in " harry potter and the chamber of secrets " that " " they 're startin ' ter think the job 's jinxed .
Head Entity: harry potter and the chamber of secrets
Tail Entity: hagrid
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "characters":
Relation: characters  
Context: In the animated series "Avatar: The Last Airbender," Aang, the last Airbender, teams up with Katara and Sokka to defeat the Fire Nation.  
Head Entity: Avatar: The Last Airbender  
Tail Entity: Aang  

Relation: characters  
Context: The novel "Pride and Prejudice" features Elizabeth Bennet as she navigates societal expectations and her relationship with Mr. Darcy.  
Head Entity: Pride and Prejudice  
Tail Entity: Elizabeth Bennet  
MemoryTrain:  epoch 15, batch     0 | loss: 5.2346540MemoryTrain:  epoch 15, batch     1 | loss: 5.0793316MemoryTrain:  epoch 15, batch     2 | loss: 5.6809537MemoryTrain:  epoch 15, batch     3 | loss: 4.1125306MemoryTrain:  epoch 15, batch     4 | loss: 4.1831802MemoryTrain:  epoch  9, batch     5 | loss: 4.3795226MemoryTrain:  epoch 15, batch     0 | loss: 4.6750494MemoryTrain:  epoch 15, batch     1 | loss: 4.8576057MemoryTrain:  epoch 15, batch     2 | loss: 5.4212054MemoryTrain:  epoch 15, batch     3 | loss: 3.8462426MemoryTrain:  epoch 15, batch     4 | loss: 7.8571723MemoryTrain:  epoch  9, batch     5 | loss: 5.2344375MemoryTrain:  epoch 15, batch     0 | loss: 6.6196729MemoryTrain:  epoch 15, batch     1 | loss: 3.5643930MemoryTrain:  epoch 15, batch     2 | loss: 3.9873074MemoryTrain:  epoch 15, batch     3 | loss: 4.0561172MemoryTrain:  epoch 15, batch     4 | loss: 6.9910426MemoryTrain:  epoch  9, batch     5 | loss: 5.0774257MemoryTrain:  epoch 15, batch     0 | loss: 7.0338827MemoryTrain:  epoch 15, batch     1 | loss: 3.0207930MemoryTrain:  epoch 15, batch     2 | loss: 3.8776083MemoryTrain:  epoch 15, batch     3 | loss: 3.5294551MemoryTrain:  epoch 15, batch     4 | loss: 4.8623083MemoryTrain:  epoch  9, batch     5 | loss: 3.1964078MemoryTrain:  epoch 15, batch     0 | loss: 6.1931003MemoryTrain:  epoch 15, batch     1 | loss: 6.9170993MemoryTrain:  epoch 15, batch     2 | loss: 6.5794908MemoryTrain:  epoch 15, batch     3 | loss: 2.8917677MemoryTrain:  epoch 15, batch     4 | loss: 2.8660798MemoryTrain:  epoch  9, batch     5 | loss: 4.1030420MemoryTrain:  epoch 15, batch     0 | loss: 3.4172543MemoryTrain:  epoch 15, batch     1 | loss: 4.5083528MemoryTrain:  epoch 15, batch     2 | loss: 2.5256023MemoryTrain:  epoch 15, batch     3 | loss: 4.8160498MemoryTrain:  epoch 15, batch     4 | loss: 2.3396514MemoryTrain:  epoch  9, batch     5 | loss: 3.7775316MemoryTrain:  epoch 15, batch     0 | loss: 5.5069605MemoryTrain:  epoch 15, batch     1 | loss: 4.7084997MemoryTrain:  epoch 15, batch     2 | loss: 7.4853196MemoryTrain:  epoch 15, batch     3 | loss: 3.1871289MemoryTrain:  epoch 15, batch     4 | loss: 4.8875100MemoryTrain:  epoch  9, batch     5 | loss: 1.8230219MemoryTrain:  epoch 15, batch     0 | loss: 3.3280227MemoryTrain:  epoch 15, batch     1 | loss: 6.0272026MemoryTrain:  epoch 15, batch     2 | loss: 4.7076570MemoryTrain:  epoch 15, batch     3 | loss: 3.3976223MemoryTrain:  epoch 15, batch     4 | loss: 2.7750626MemoryTrain:  epoch  9, batch     5 | loss: 4.1456802MemoryTrain:  epoch 15, batch     0 | loss: 3.9397597MemoryTrain:  epoch 15, batch     1 | loss: 3.0404734MemoryTrain:  epoch 15, batch     2 | loss: 4.7713467MemoryTrain:  epoch 15, batch     3 | loss: 2.4188598MemoryTrain:  epoch 15, batch     4 | loss: 1.7084295MemoryTrain:  epoch  9, batch     5 | loss: 4.1029069MemoryTrain:  epoch 15, batch     0 | loss: 2.1089999MemoryTrain:  epoch 15, batch     1 | loss: 2.5098590MemoryTrain:  epoch 15, batch     2 | loss: 8.2177721MemoryTrain:  epoch 15, batch     3 | loss: 2.7086729MemoryTrain:  epoch 15, batch     4 | loss: 3.5027797MemoryTrain:  epoch  9, batch     5 | loss: 3.3681543
[EVAL] batch:    0 | acc: 31.25%,  total acc: 31.25%   [EVAL] batch:    1 | acc: 43.75%,  total acc: 37.50%   [EVAL] batch:    2 | acc: 25.00%,  total acc: 33.33%   [EVAL] batch:    3 | acc: 50.00%,  total acc: 37.50%   [EVAL] batch:    4 | acc: 18.75%,  total acc: 33.75%   [EVAL] batch:    5 | acc: 37.50%,  total acc: 34.38%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 41.96%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 49.22%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 54.86%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 58.75%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 62.50%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 65.62%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 66.35%   [EVAL] batch:   13 | acc: 56.25%,  total acc: 65.62%   [EVAL] batch:   14 | acc: 62.50%,  total acc: 65.42%   [EVAL] batch:   15 | acc: 62.50%,  total acc: 65.23%   [EVAL] batch:   16 | acc: 56.25%,  total acc: 64.71%   [EVAL] batch:   17 | acc: 75.00%,  total acc: 65.28%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 65.46%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 67.19%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 68.75%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 69.89%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 71.20%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 72.40%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 73.00%   [EVAL] batch:   25 | acc: 87.50%,  total acc: 73.56%   [EVAL] batch:   26 | acc: 87.50%,  total acc: 74.07%   [EVAL] batch:   27 | acc: 75.00%,  total acc: 74.11%   [EVAL] batch:   28 | acc: 87.50%,  total acc: 74.57%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 74.79%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 75.20%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 75.78%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 76.52%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 77.02%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 77.50%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 77.95%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 78.55%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 78.78%   [EVAL] batch:   38 | acc: 75.00%,  total acc: 78.69%   [EVAL] batch:   39 | acc: 81.25%,  total acc: 78.75%   [EVAL] batch:   40 | acc: 87.50%,  total acc: 78.96%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 79.17%   [EVAL] batch:   42 | acc: 43.75%,  total acc: 78.34%   [EVAL] batch:   43 | acc: 75.00%,  total acc: 78.27%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 78.75%   [EVAL] batch:   45 | acc: 93.75%,  total acc: 79.08%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 79.52%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 79.95%   [EVAL] batch:   48 | acc: 93.75%,  total acc: 80.23%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 80.62%   [EVAL] batch:   50 | acc: 75.00%,  total acc: 80.51%   [EVAL] batch:   51 | acc: 62.50%,  total acc: 80.17%   [EVAL] batch:   52 | acc: 50.00%,  total acc: 79.60%   [EVAL] batch:   53 | acc: 68.75%,  total acc: 79.40%   [EVAL] batch:   54 | acc: 75.00%,  total acc: 79.32%   [EVAL] batch:   55 | acc: 31.25%,  total acc: 78.46%   [EVAL] batch:   56 | acc: 56.25%,  total acc: 78.07%   [EVAL] batch:   57 | acc: 75.00%,  total acc: 78.02%   [EVAL] batch:   58 | acc: 43.75%,  total acc: 77.44%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 77.71%   [EVAL] batch:   60 | acc: 87.50%,  total acc: 77.87%   [EVAL] batch:   61 | acc: 75.00%,  total acc: 77.82%   [EVAL] batch:   62 | acc: 37.50%,  total acc: 77.18%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 82.81%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 83.75%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 82.14%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 82.03%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 81.94%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:   10 | acc: 75.00%,  total acc: 80.68%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 80.73%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 82.14%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 82.92%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 83.98%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 84.93%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 85.42%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 86.18%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 86.25%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 86.61%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 86.36%   [EVAL] batch:   22 | acc: 68.75%,  total acc: 85.60%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 86.20%   [EVAL] batch:   24 | acc: 81.25%,  total acc: 86.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 86.54%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 87.04%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 87.28%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 87.92%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 88.31%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 88.67%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 89.02%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 89.34%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 89.46%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 89.76%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 90.03%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 90.30%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 90.54%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 90.78%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 91.01%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 91.22%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 91.28%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 91.48%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 91.67%   [EVAL] batch:   45 | acc: 87.50%,  total acc: 91.58%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 91.62%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 91.84%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 91.88%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 91.91%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 91.95%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 91.98%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 92.13%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 92.05%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 92.08%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 92.11%   [EVAL] batch:   57 | acc: 56.25%,  total acc: 91.49%   [EVAL] batch:   58 | acc: 81.25%,  total acc: 91.31%   [EVAL] batch:   59 | acc: 87.50%,  total acc: 91.25%   [EVAL] batch:   60 | acc: 87.50%,  total acc: 91.19%   [EVAL] batch:   61 | acc: 81.25%,  total acc: 91.03%   [EVAL] batch:   62 | acc: 68.75%,  total acc: 90.67%   [EVAL] batch:   63 | acc: 25.00%,  total acc: 89.65%   [EVAL] batch:   64 | acc: 43.75%,  total acc: 88.94%   [EVAL] batch:   65 | acc: 68.75%,  total acc: 88.64%   [EVAL] batch:   66 | acc: 43.75%,  total acc: 87.97%   [EVAL] batch:   67 | acc: 68.75%,  total acc: 87.68%   [EVAL] batch:   68 | acc: 68.75%,  total acc: 87.41%   [EVAL] batch:   69 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   70 | acc: 68.75%,  total acc: 87.24%   [EVAL] batch:   71 | acc: 75.00%,  total acc: 87.07%   [EVAL] batch:   72 | acc: 87.50%,  total acc: 87.07%   [EVAL] batch:   73 | acc: 87.50%,  total acc: 87.08%   [EVAL] batch:   74 | acc: 81.25%,  total acc: 87.00%   [EVAL] batch:   75 | acc: 37.50%,  total acc: 86.35%   [EVAL] batch:   76 | acc: 37.50%,  total acc: 85.71%   [EVAL] batch:   77 | acc: 37.50%,  total acc: 85.10%   [EVAL] batch:   78 | acc: 43.75%,  total acc: 84.57%   [EVAL] batch:   79 | acc: 56.25%,  total acc: 84.22%   [EVAL] batch:   80 | acc: 25.00%,  total acc: 83.49%   [EVAL] batch:   81 | acc: 81.25%,  total acc: 83.46%   [EVAL] batch:   82 | acc: 100.00%,  total acc: 83.66%   [EVAL] batch:   83 | acc: 100.00%,  total acc: 83.85%   [EVAL] batch:   84 | acc: 93.75%,  total acc: 83.97%   [EVAL] batch:   85 | acc: 100.00%,  total acc: 84.16%   [EVAL] batch:   86 | acc: 100.00%,  total acc: 84.34%   [EVAL] batch:   87 | acc: 75.00%,  total acc: 84.23%   [EVAL] batch:   88 | acc: 25.00%,  total acc: 83.57%   [EVAL] batch:   89 | acc: 37.50%,  total acc: 83.06%   [EVAL] batch:   90 | acc: 50.00%,  total acc: 82.69%   [EVAL] batch:   91 | acc: 50.00%,  total acc: 82.34%   [EVAL] batch:   92 | acc: 43.75%,  total acc: 81.92%   [EVAL] batch:   93 | acc: 68.75%,  total acc: 81.78%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 81.97%   [EVAL] batch:   95 | acc: 93.75%,  total acc: 82.10%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 82.28%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 82.46%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 82.64%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 82.81%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 82.98%   [EVAL] batch:  101 | acc: 93.75%,  total acc: 83.09%   [EVAL] batch:  102 | acc: 100.00%,  total acc: 83.25%   [EVAL] batch:  103 | acc: 100.00%,  total acc: 83.41%   [EVAL] batch:  104 | acc: 100.00%,  total acc: 83.57%   [EVAL] batch:  105 | acc: 100.00%,  total acc: 83.73%   [EVAL] batch:  106 | acc: 75.00%,  total acc: 83.64%   [EVAL] batch:  107 | acc: 62.50%,  total acc: 83.45%   [EVAL] batch:  108 | acc: 68.75%,  total acc: 83.31%   [EVAL] batch:  109 | acc: 56.25%,  total acc: 83.07%   [EVAL] batch:  110 | acc: 43.75%,  total acc: 82.71%   [EVAL] batch:  111 | acc: 62.50%,  total acc: 82.53%   [EVAL] batch:  112 | acc: 87.50%,  total acc: 82.58%   [EVAL] batch:  113 | acc: 75.00%,  total acc: 82.51%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 82.66%   [EVAL] batch:  115 | acc: 81.25%,  total acc: 82.65%   [EVAL] batch:  116 | acc: 87.50%,  total acc: 82.69%   [EVAL] batch:  117 | acc: 75.00%,  total acc: 82.63%   [EVAL] batch:  118 | acc: 81.25%,  total acc: 82.62%   [EVAL] batch:  119 | acc: 93.75%,  total acc: 82.71%   [EVAL] batch:  120 | acc: 93.75%,  total acc: 82.80%   [EVAL] batch:  121 | acc: 81.25%,  total acc: 82.79%   [EVAL] batch:  122 | acc: 81.25%,  total acc: 82.77%   [EVAL] batch:  123 | acc: 81.25%,  total acc: 82.76%   [EVAL] batch:  124 | acc: 93.75%,  total acc: 82.85%   [EVAL] batch:  125 | acc: 31.25%,  total acc: 82.44%   [EVAL] batch:  126 | acc: 43.75%,  total acc: 82.14%   [EVAL] batch:  127 | acc: 25.00%,  total acc: 81.69%   [EVAL] batch:  128 | acc: 50.00%,  total acc: 81.44%   [EVAL] batch:  129 | acc: 18.75%,  total acc: 80.96%   [EVAL] batch:  130 | acc: 37.50%,  total acc: 80.63%   [EVAL] batch:  131 | acc: 87.50%,  total acc: 80.68%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 80.83%   [EVAL] batch:  133 | acc: 100.00%,  total acc: 80.97%   [EVAL] batch:  134 | acc: 93.75%,  total acc: 81.06%   [EVAL] batch:  135 | acc: 100.00%,  total acc: 81.20%   [EVAL] batch:  136 | acc: 100.00%,  total acc: 81.34%   [EVAL] batch:  137 | acc: 75.00%,  total acc: 81.30%   [EVAL] batch:  138 | acc: 56.25%,  total acc: 81.12%   [EVAL] batch:  139 | acc: 62.50%,  total acc: 80.98%   [EVAL] batch:  140 | acc: 62.50%,  total acc: 80.85%   [EVAL] batch:  141 | acc: 56.25%,  total acc: 80.68%   [EVAL] batch:  142 | acc: 75.00%,  total acc: 80.64%   [EVAL] batch:  143 | acc: 68.75%,  total acc: 80.56%   [EVAL] batch:  144 | acc: 100.00%,  total acc: 80.69%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 80.82%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 80.91%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 81.04%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 81.17%   [EVAL] batch:  149 | acc: 87.50%,  total acc: 81.21%   [EVAL] batch:  150 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:  151 | acc: 87.50%,  total acc: 81.29%   [EVAL] batch:  152 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:  153 | acc: 87.50%,  total acc: 81.29%   [EVAL] batch:  154 | acc: 81.25%,  total acc: 81.29%   [EVAL] batch:  155 | acc: 87.50%,  total acc: 81.33%   [EVAL] batch:  156 | acc: 93.75%,  total acc: 81.41%   [EVAL] batch:  157 | acc: 100.00%,  total acc: 81.53%   [EVAL] batch:  158 | acc: 93.75%,  total acc: 81.60%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 81.68%   [EVAL] batch:  160 | acc: 93.75%,  total acc: 81.75%   [EVAL] batch:  161 | acc: 100.00%,  total acc: 81.87%   [EVAL] batch:  162 | acc: 87.50%,  total acc: 81.90%   [EVAL] batch:  163 | acc: 75.00%,  total acc: 81.86%   [EVAL] batch:  164 | acc: 81.25%,  total acc: 81.86%   [EVAL] batch:  165 | acc: 87.50%,  total acc: 81.89%   [EVAL] batch:  166 | acc: 87.50%,  total acc: 81.92%   [EVAL] batch:  167 | acc: 43.75%,  total acc: 81.70%   [EVAL] batch:  168 | acc: 75.00%,  total acc: 81.66%   [EVAL] batch:  169 | acc: 100.00%,  total acc: 81.76%   [EVAL] batch:  170 | acc: 93.75%,  total acc: 81.83%   [EVAL] batch:  171 | acc: 100.00%,  total acc: 81.94%   [EVAL] batch:  172 | acc: 100.00%,  total acc: 82.04%   [EVAL] batch:  173 | acc: 93.75%,  total acc: 82.11%   [EVAL] batch:  174 | acc: 100.00%,  total acc: 82.21%   [EVAL] batch:  175 | acc: 75.00%,  total acc: 82.17%   [EVAL] batch:  176 | acc: 62.50%,  total acc: 82.06%   [EVAL] batch:  177 | acc: 50.00%,  total acc: 81.88%   [EVAL] batch:  178 | acc: 68.75%,  total acc: 81.81%   [EVAL] batch:  179 | acc: 75.00%,  total acc: 81.77%   [EVAL] batch:  180 | acc: 31.25%,  total acc: 81.49%   [EVAL] batch:  181 | acc: 56.25%,  total acc: 81.35%   [EVAL] batch:  182 | acc: 75.00%,  total acc: 81.32%   [EVAL] batch:  183 | acc: 43.75%,  total acc: 81.11%   [EVAL] batch:  184 | acc: 93.75%,  total acc: 81.18%   [EVAL] batch:  185 | acc: 87.50%,  total acc: 81.22%   [EVAL] batch:  186 | acc: 75.00%,  total acc: 81.18%   [EVAL] batch:  187 | acc: 37.50%,  total acc: 80.95%   
cur_acc:  ['0.9454', '0.7996', '0.7718']
his_acc:  ['0.9454', '0.8715', '0.8095']
CurrentTrain: epoch 15, batch     0 | loss: 12.0700164CurrentTrain: epoch 15, batch     1 | loss: 30.5296577CurrentTrain: epoch 15, batch     2 | loss: 20.1294691CurrentTrain: epoch  1, batch     3 | loss: 9.5686661CurrentTrain: epoch 15, batch     0 | loss: 17.7906619CurrentTrain: epoch 15, batch     1 | loss: 14.5465566CurrentTrain: epoch 15, batch     2 | loss: 11.1606737CurrentTrain: epoch  1, batch     3 | loss: 8.1506016CurrentTrain: epoch 15, batch     0 | loss: 12.1386383CurrentTrain: epoch 15, batch     1 | loss: 16.3484607CurrentTrain: epoch 15, batch     2 | loss: 12.6553515CurrentTrain: epoch  1, batch     3 | loss: 8.1069823CurrentTrain: epoch 15, batch     0 | loss: 11.5825339CurrentTrain: epoch 15, batch     1 | loss: 16.4223926CurrentTrain: epoch 15, batch     2 | loss: 8.3838688CurrentTrain: epoch  1, batch     3 | loss: 10.0247125CurrentTrain: epoch 15, batch     0 | loss: 11.3368697CurrentTrain: epoch 15, batch     1 | loss: 20.6542471CurrentTrain: epoch 15, batch     2 | loss: 17.9962531CurrentTrain: epoch  1, batch     3 | loss: 5.8505958CurrentTrain: epoch 15, batch     0 | loss: 11.4320756CurrentTrain: epoch 15, batch     1 | loss: 14.6608432CurrentTrain: epoch 15, batch     2 | loss: 11.0596176CurrentTrain: epoch  1, batch     3 | loss: 5.6929678CurrentTrain: epoch 15, batch     0 | loss: 8.9058071CurrentTrain: epoch 15, batch     1 | loss: 9.8609834CurrentTrain: epoch 15, batch     2 | loss: 10.0517600CurrentTrain: epoch  1, batch     3 | loss: 10.5276396CurrentTrain: epoch 15, batch     0 | loss: 14.5233832CurrentTrain: epoch 15, batch     1 | loss: 16.0769933CurrentTrain: epoch 15, batch     2 | loss: 13.0902105CurrentTrain: epoch  1, batch     3 | loss: 21.6416113CurrentTrain: epoch 15, batch     0 | loss: 10.5311252CurrentTrain: epoch 15, batch     1 | loss: 11.0688092CurrentTrain: epoch 15, batch     2 | loss: 7.9269733CurrentTrain: epoch  1, batch     3 | loss: 6.0658766CurrentTrain: epoch 15, batch     0 | loss: 29.0092744CurrentTrain: epoch 15, batch     1 | loss: 8.1449576CurrentTrain: epoch 15, batch     2 | loss: 11.3035006CurrentTrain: epoch  1, batch     3 | loss: 6.9208427
#### applies to jurisdiction ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "applies to jurisdiction" means the item (institution, law, public office, public register...) or statement belongs to or has power over or applies to the value (a territorial jurisdiction: a country, state, municipality, ...).
Here is an example:
Relation: applies to jurisdiction
Context: george lawson sheldon ( may 31 , 1870 – april 4 , 1960 ) was 14th governor of nebraska with service from 1907 until 1909 .
Head Entity: governor
Tail Entity: nebraska
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "applies to jurisdiction":
Relation: applies to jurisdiction  
Context: The Environmental Protection Agency (EPA) is responsible for enforcing regulations related to environmental protection in the United States.  
Head Entity: Environmental Protection Agency  
Tail Entity: United States  

Relation: applies to jurisdiction  
Context: The California Civil Code governs various aspects of civil law within the state of California, including contracts and property rights.  
Head Entity: California Civil Code  
Tail Entity: California  
#### performer ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "performer" means actor, musician, band or other performer associated with this role or musical work.
Here is an example:
Relation: performer
Context: it is prominently featured on jean michel jarre 's albums " oxygène " ( 1977 ) and " équinoxe " ( 1978 ) .
Head Entity: équinoxe
Tail Entity: jean michel jarre
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "performer":
Relation: performer  
Context: The legendary band Queen is known for their iconic song "Bohemian Rhapsody," which showcases Freddie Mercury's extraordinary vocal talent.  
Head Entity: Bohemian Rhapsody  
Tail Entity: Freddie Mercury  

Relation: performer  
Context: Taylor Swift captivated audiences with her heartfelt performance of "Love Story" during the awards ceremony last night.  
Head Entity: Love Story  
Tail Entity: Taylor Swift  
#### manufacturer ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "manufacturer" means manufacturer or producer of this product.
Here is an example:
Relation: manufacturer
Context: the glasflügel h-201 standard " libelle " ( german : " dragonfly " ) is an early composite standard class single - seat sailplane produced by glasflügel from 1967 .
Head Entity: dragonfly
Tail Entity: glasflügel
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "manufacturer":
Relation: manufacturer  
Context: The iPhone 13, a smartphone developed by Apple, was released in September 2021 and features advanced camera technology and improved battery life.  
Head Entity: iPhone 13  
Tail Entity: Apple  

Relation: manufacturer  
Context: The Tesla Model S is an all-electric luxury sedan that was first introduced by Tesla, Inc. in 2012, showcasing innovative technology and performance.  
Head Entity: Tesla Model S  
Tail Entity: Tesla, Inc.  
#### participant ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "participant" means person, group of people or organization (object) that actively takes/took part in an event or process.
Here is an example:
Relation: participant
Context: norway was represented in the eurovision song contest 2005 by the song " in my dreams " performed by wig wam .
Head Entity: eurovision song contest 2005
Tail Entity: wig wam
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "participant":
Relation: participant  
Context: the united states was represented in the 2020 summer olympics by a team of athletes competing in various sports.  
Head Entity: 2020 summer olympics  
Tail Entity: united states  

Relation: participant  
Context: the annual tech conference featured several startups, including a presentation by innovative solutions inc.  
Head Entity: annual tech conference  
Tail Entity: innovative solutions inc.  
#### operating system ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "operating system" means operating system (OS) on which a software works or the OS installed on hardware.
Here is an example:
Relation: operating system
Context: the software is written in python and works on all the platforms supported by blender : windows , osx and linux .
Head Entity: blender
Tail Entity: windows
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "operating system":
Relation: operating system  
Context: The latest version of the application is compatible with various operating systems, including macOS, Windows, and Ubuntu.  
Head Entity: application  
Tail Entity: macOS  

Relation: operating system  
Context: Many developers prefer to use Linux as their operating system due to its flexibility and open-source nature.  
Head Entity: developers  
Tail Entity: Linux  
#### competition class ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "competition class" means official classification by a regulating body under which the subject (events, teams, participants, or equipment) qualifies for inclusion.
Here is an example:
Relation: competition class
Context: in his next bout , he won the wbc light welterweight title by defeating roger mayweather for a second time .
Head Entity: roger mayweather
Tail Entity: light welterweight
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "competition class":
Relation: competition class  
Context: The team secured their place in the tournament by qualifying under the UEFA Champions League regulations.  
Head Entity: UEFA Champions League  
Tail Entity: tournament  

Relation: competition class  
Context: The athlete competed in the 100 meters event, which is classified under the Olympic Games standards.  
Head Entity: Olympic Games  
Tail Entity: 100 meters event  
#### successful candidate ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "successful candidate" means person elected after the election.
Here is an example:
Relation: successful candidate
Context: the qup gained 3 seats from its previous election result , and won a 10 % swing to it in primary votes , but failed to defeat the incumbent labor government led by ted theodore .
Head Entity: previous election
Tail Entity: ted theodore
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "successful candidate":
Relation: successful candidate  
Context: After a closely contested campaign, the voters chose Sarah Johnson as their representative, marking a significant shift in the political landscape of the district.  
Head Entity: closely contested campaign  
Tail Entity: Sarah Johnson  

Relation: successful candidate  
Context: In the recent mayoral election, the community rallied behind Mark Thompson, who promised to bring new initiatives to the city, ultimately leading to his victory.  
Head Entity: recent mayoral election  
Tail Entity: Mark Thompson  
#### sibling ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "sibling" means the subject and the object have the same parents.
Here is an example:
Relation: sibling
Context: although the latter 's eldest son , lǫgmaðr , appears to have succeeded to the kingship , he was soon forced to contend with factions supporting his younger brothers : haraldr , and óláfr .
Head Entity: lǫgmaðr
Tail Entity: óláfr
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "sibling":
Relation: sibling  
Context: In the royal family of the ancient kingdom, the two brothers, Eirik and Sigurd, were known for their fierce loyalty to each other, having grown up under the same roof and shared the same parents.  
Head Entity: Eirik  
Tail Entity: Sigurd  

Relation: sibling  
Context: During the family reunion, it was evident that both Clara and her brother, James, inherited their parents' artistic talents, showcasing their skills in painting and music.  
Head Entity: Clara  
Tail Entity: James  
#### military branch ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "military branch" means branch to which this military unit, award, office, or person belongs.
Here is an example:
Relation: military branch
Context: dennis chalker is a retired navy seal , inventor and author who has written six books about the united states navy seals .
Head Entity: dennis chalker
Tail Entity: united states navy
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "military branch":
Relation: military branch  
Context: the general served in the air force for over twenty years before retiring and taking on a civilian role in defense consulting.  
Head Entity: the general  
Tail Entity: air force  

Relation: military branch  
Context: during the ceremony, the admiral was recognized for his service in the coast guard, where he led several important missions.  
Head Entity: the admiral  
Tail Entity: coast guard  
#### child ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "child" means subject has object as child.
Here is an example:
Relation: child
Context: in 1406 adolf married marie of burgundy , daughter of john the fearless and margaret of bavaria .
Head Entity: john the fearless
Tail Entity: marie of burgundy
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "child":
Relation: child  
Context: in 1980, michael and sarah welcomed their first child, a daughter named emily, into the world.  
Head Entity: michael  
Tail Entity: emily  

Relation: child  
Context: during the family reunion, it was revealed that elizabeth is the daughter of robert, making her the youngest child in the family.  
Head Entity: robert  
Tail Entity: elizabeth  
MemoryTrain:  epoch 15, batch     0 | loss: 6.3167550MemoryTrain:  epoch 15, batch     1 | loss: 4.9930313MemoryTrain:  epoch 15, batch     2 | loss: 9.6088196MemoryTrain:  epoch 15, batch     3 | loss: 6.4862794MemoryTrain:  epoch 15, batch     4 | loss: 4.2563070MemoryTrain:  epoch 15, batch     5 | loss: 6.9063328MemoryTrain:  epoch 15, batch     6 | loss: 3.9089351MemoryTrain:  epoch  7, batch     7 | loss: 4.5182341MemoryTrain:  epoch 15, batch     0 | loss: 4.4628388MemoryTrain:  epoch 15, batch     1 | loss: 6.1087417MemoryTrain:  epoch 15, batch     2 | loss: 3.6856272MemoryTrain:  epoch 15, batch     3 | loss: 4.8782862MemoryTrain:  epoch 15, batch     4 | loss: 5.3434879MemoryTrain:  epoch 15, batch     5 | loss: 4.5004731MemoryTrain:  epoch 15, batch     6 | loss: 3.3917361MemoryTrain:  epoch  7, batch     7 | loss: 5.3819503MemoryTrain:  epoch 15, batch     0 | loss: 5.0920060MemoryTrain:  epoch 15, batch     1 | loss: 3.5919974MemoryTrain:  epoch 15, batch     2 | loss: 6.2652376MemoryTrain:  epoch 15, batch     3 | loss: 6.8604730MemoryTrain:  epoch 15, batch     4 | loss: 5.7282485MemoryTrain:  epoch 15, batch     5 | loss: 6.1986400MemoryTrain:  epoch 15, batch     6 | loss: 2.9843407MemoryTrain:  epoch  7, batch     7 | loss: 4.0760246MemoryTrain:  epoch 15, batch     0 | loss: 2.4221323MemoryTrain:  epoch 15, batch     1 | loss: 3.2395963MemoryTrain:  epoch 15, batch     2 | loss: 5.9560526MemoryTrain:  epoch 15, batch     3 | loss: 3.2164965MemoryTrain:  epoch 15, batch     4 | loss: 2.5946903MemoryTrain:  epoch 15, batch     5 | loss: 5.0914610MemoryTrain:  epoch 15, batch     6 | loss: 2.6682335MemoryTrain:  epoch  7, batch     7 | loss: 6.2804369MemoryTrain:  epoch 15, batch     0 | loss: 3.3442252MemoryTrain:  epoch 15, batch     1 | loss: 2.5114846MemoryTrain:  epoch 15, batch     2 | loss: 3.1457336MemoryTrain:  epoch 15, batch     3 | loss: 2.5666614MemoryTrain:  epoch 15, batch     4 | loss: 2.4512137MemoryTrain:  epoch 15, batch     5 | loss: 2.5290892MemoryTrain:  epoch 15, batch     6 | loss: 4.5401660MemoryTrain:  epoch  7, batch     7 | loss: 11.5084380MemoryTrain:  epoch 15, batch     0 | loss: 2.4118254MemoryTrain:  epoch 15, batch     1 | loss: 2.6752260MemoryTrain:  epoch 15, batch     2 | loss: 5.0020888MemoryTrain:  epoch 15, batch     3 | loss: 3.2762855MemoryTrain:  epoch 15, batch     4 | loss: 2.3037710MemoryTrain:  epoch 15, batch     5 | loss: 3.0214861MemoryTrain:  epoch 15, batch     6 | loss: 3.6049082MemoryTrain:  epoch  7, batch     7 | loss: 3.2291501MemoryTrain:  epoch 15, batch     0 | loss: 5.0320212MemoryTrain:  epoch 15, batch     1 | loss: 2.0375784MemoryTrain:  epoch 15, batch     2 | loss: 4.5693928MemoryTrain:  epoch 15, batch     3 | loss: 2.9579115MemoryTrain:  epoch 15, batch     4 | loss: 2.0725481MemoryTrain:  epoch 15, batch     5 | loss: 3.5748754MemoryTrain:  epoch 15, batch     6 | loss: 3.6046476MemoryTrain:  epoch  7, batch     7 | loss: 1.7270198MemoryTrain:  epoch 15, batch     0 | loss: 4.1773222MemoryTrain:  epoch 15, batch     1 | loss: 1.7521778MemoryTrain:  epoch 15, batch     2 | loss: 1.9170958MemoryTrain:  epoch 15, batch     3 | loss: 4.5904628MemoryTrain:  epoch 15, batch     4 | loss: 5.2013546MemoryTrain:  epoch 15, batch     5 | loss: 4.2291999MemoryTrain:  epoch 15, batch     6 | loss: 2.7824145MemoryTrain:  epoch  7, batch     7 | loss: 1.8932175MemoryTrain:  epoch 15, batch     0 | loss: 4.2138961MemoryTrain:  epoch 15, batch     1 | loss: 2.1284769MemoryTrain:  epoch 15, batch     2 | loss: 4.8119781MemoryTrain:  epoch 15, batch     3 | loss: 1.8791253MemoryTrain:  epoch 15, batch     4 | loss: 2.5825810MemoryTrain:  epoch 15, batch     5 | loss: 6.1375012MemoryTrain:  epoch 15, batch     6 | loss: 2.8193212MemoryTrain:  epoch  7, batch     7 | loss: 1.5361577MemoryTrain:  epoch 15, batch     0 | loss: 3.2230654MemoryTrain:  epoch 15, batch     1 | loss: 10.4094408MemoryTrain:  epoch 15, batch     2 | loss: 2.7069030MemoryTrain:  epoch 15, batch     3 | loss: 1.3940523MemoryTrain:  epoch 15, batch     4 | loss: 1.8699941MemoryTrain:  epoch 15, batch     5 | loss: 5.0600694MemoryTrain:  epoch 15, batch     6 | loss: 1.7255246MemoryTrain:  epoch  7, batch     7 | loss: 1.2437355
[EVAL] batch:    0 | acc: 37.50%,  total acc: 37.50%   [EVAL] batch:    1 | acc: 75.00%,  total acc: 56.25%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 62.50%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 64.06%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 70.00%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 71.88%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 73.21%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 74.22%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 73.61%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 72.50%   [EVAL] batch:   10 | acc: 75.00%,  total acc: 72.73%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 72.92%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 74.04%   [EVAL] batch:   13 | acc: 87.50%,  total acc: 75.00%   [EVAL] batch:   14 | acc: 87.50%,  total acc: 75.83%   [EVAL] batch:   15 | acc: 87.50%,  total acc: 76.56%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 77.57%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 78.47%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 77.96%   [EVAL] batch:   19 | acc: 68.75%,  total acc: 77.50%   [EVAL] batch:   20 | acc: 31.25%,  total acc: 75.30%   [EVAL] batch:   21 | acc: 43.75%,  total acc: 73.86%   [EVAL] batch:   22 | acc: 37.50%,  total acc: 72.28%   [EVAL] batch:   23 | acc: 43.75%,  total acc: 71.09%   [EVAL] batch:   24 | acc: 62.50%,  total acc: 70.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 71.88%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 72.92%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 73.88%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 74.78%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 75.62%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 76.41%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 77.15%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 77.84%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 78.49%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 79.11%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 79.69%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 80.24%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 80.76%   [EVAL] batch:   38 | acc: 93.75%,  total acc: 81.09%   [EVAL] batch:   39 | acc: 81.25%,  total acc: 81.09%   [EVAL] batch:   40 | acc: 68.75%,  total acc: 80.79%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 80.95%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:   44 | acc: 50.00%,  total acc: 80.56%   [EVAL] batch:   45 | acc: 50.00%,  total acc: 79.89%   [EVAL] batch:   46 | acc: 56.25%,  total acc: 79.39%   [EVAL] batch:   47 | acc: 75.00%,  total acc: 79.30%   [EVAL] batch:   48 | acc: 56.25%,  total acc: 78.83%   [EVAL] batch:   49 | acc: 75.00%,  total acc: 78.75%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 79.04%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 79.33%   [EVAL] batch:   52 | acc: 81.25%,  total acc: 79.36%   [EVAL] batch:   53 | acc: 87.50%,  total acc: 79.51%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 79.77%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 79.91%   [EVAL] batch:   56 | acc: 31.25%,  total acc: 79.06%   [EVAL] batch:   57 | acc: 18.75%,  total acc: 78.02%   [EVAL] batch:   58 | acc: 25.00%,  total acc: 77.12%   [EVAL] batch:   59 | acc: 12.50%,  total acc: 76.04%   [EVAL] batch:   60 | acc: 31.25%,  total acc: 75.31%   [EVAL] batch:   61 | acc: 12.50%,  total acc: 74.29%   [EVAL] batch:   62 | acc: 18.75%,  total acc: 73.41%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 85.94%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 86.25%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 84.82%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 83.59%   [EVAL] batch:    8 | acc: 75.00%,  total acc: 82.64%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 81.88%   [EVAL] batch:   10 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 82.14%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 82.92%   [EVAL] batch:   15 | acc: 93.75%,  total acc: 83.59%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 84.19%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 84.72%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 85.53%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 85.31%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 85.42%   [EVAL] batch:   21 | acc: 75.00%,  total acc: 84.94%   [EVAL] batch:   22 | acc: 68.75%,  total acc: 84.24%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 84.90%   [EVAL] batch:   24 | acc: 75.00%,  total acc: 84.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 85.10%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 85.65%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 86.16%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 86.42%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 86.88%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 87.30%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 87.70%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 88.07%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 88.42%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 88.57%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 88.89%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 89.19%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 89.47%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 89.74%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 90.00%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 90.24%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 90.48%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 90.55%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 90.69%   [EVAL] batch:   45 | acc: 75.00%,  total acc: 90.35%   [EVAL] batch:   46 | acc: 56.25%,  total acc: 89.63%   [EVAL] batch:   47 | acc: 62.50%,  total acc: 89.06%   [EVAL] batch:   48 | acc: 87.50%,  total acc: 89.03%   [EVAL] batch:   49 | acc: 81.25%,  total acc: 88.88%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 88.97%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 88.94%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 89.03%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 89.24%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 89.20%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 89.29%   [EVAL] batch:   56 | acc: 87.50%,  total acc: 89.25%   [EVAL] batch:   57 | acc: 62.50%,  total acc: 88.79%   [EVAL] batch:   58 | acc: 75.00%,  total acc: 88.56%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 88.65%   [EVAL] batch:   60 | acc: 87.50%,  total acc: 88.63%   [EVAL] batch:   61 | acc: 81.25%,  total acc: 88.51%   [EVAL] batch:   62 | acc: 68.75%,  total acc: 88.19%   [EVAL] batch:   63 | acc: 18.75%,  total acc: 87.11%   [EVAL] batch:   64 | acc: 37.50%,  total acc: 86.35%   [EVAL] batch:   65 | acc: 68.75%,  total acc: 86.08%   [EVAL] batch:   66 | acc: 50.00%,  total acc: 85.54%   [EVAL] batch:   67 | acc: 56.25%,  total acc: 85.11%   [EVAL] batch:   68 | acc: 56.25%,  total acc: 84.69%   [EVAL] batch:   69 | acc: 93.75%,  total acc: 84.82%   [EVAL] batch:   70 | acc: 75.00%,  total acc: 84.68%   [EVAL] batch:   71 | acc: 75.00%,  total acc: 84.55%   [EVAL] batch:   72 | acc: 87.50%,  total acc: 84.59%   [EVAL] batch:   73 | acc: 87.50%,  total acc: 84.63%   [EVAL] batch:   74 | acc: 100.00%,  total acc: 84.83%   [EVAL] batch:   75 | acc: 31.25%,  total acc: 84.13%   [EVAL] batch:   76 | acc: 18.75%,  total acc: 83.28%   [EVAL] batch:   77 | acc: 37.50%,  total acc: 82.69%   [EVAL] batch:   78 | acc: 37.50%,  total acc: 82.12%   [EVAL] batch:   79 | acc: 43.75%,  total acc: 81.64%   [EVAL] batch:   80 | acc: 25.00%,  total acc: 80.94%   [EVAL] batch:   81 | acc: 75.00%,  total acc: 80.87%   [EVAL] batch:   82 | acc: 100.00%,  total acc: 81.10%   [EVAL] batch:   83 | acc: 100.00%,  total acc: 81.32%   [EVAL] batch:   84 | acc: 93.75%,  total acc: 81.47%   [EVAL] batch:   85 | acc: 100.00%,  total acc: 81.69%   [EVAL] batch:   86 | acc: 100.00%,  total acc: 81.90%   [EVAL] batch:   87 | acc: 81.25%,  total acc: 81.89%   [EVAL] batch:   88 | acc: 25.00%,  total acc: 81.25%   [EVAL] batch:   89 | acc: 31.25%,  total acc: 80.69%   [EVAL] batch:   90 | acc: 43.75%,  total acc: 80.29%   [EVAL] batch:   91 | acc: 50.00%,  total acc: 79.96%   [EVAL] batch:   92 | acc: 37.50%,  total acc: 79.50%   [EVAL] batch:   93 | acc: 56.25%,  total acc: 79.26%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 79.47%   [EVAL] batch:   95 | acc: 93.75%,  total acc: 79.62%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 79.83%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 80.04%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 80.24%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 80.44%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 80.63%   [EVAL] batch:  101 | acc: 93.75%,  total acc: 80.76%   [EVAL] batch:  102 | acc: 93.75%,  total acc: 80.89%   [EVAL] batch:  103 | acc: 100.00%,  total acc: 81.07%   [EVAL] batch:  104 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:  105 | acc: 100.00%,  total acc: 81.43%   [EVAL] batch:  106 | acc: 81.25%,  total acc: 81.43%   [EVAL] batch:  107 | acc: 68.75%,  total acc: 81.31%   [EVAL] batch:  108 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:  109 | acc: 62.50%,  total acc: 81.08%   [EVAL] batch:  110 | acc: 50.00%,  total acc: 80.80%   [EVAL] batch:  111 | acc: 68.75%,  total acc: 80.69%   [EVAL] batch:  112 | acc: 68.75%,  total acc: 80.59%   [EVAL] batch:  113 | acc: 50.00%,  total acc: 80.32%   [EVAL] batch:  114 | acc: 62.50%,  total acc: 80.16%   [EVAL] batch:  115 | acc: 75.00%,  total acc: 80.12%   [EVAL] batch:  116 | acc: 68.75%,  total acc: 80.02%   [EVAL] batch:  117 | acc: 68.75%,  total acc: 79.93%   [EVAL] batch:  118 | acc: 75.00%,  total acc: 79.88%   [EVAL] batch:  119 | acc: 93.75%,  total acc: 80.00%   [EVAL] batch:  120 | acc: 93.75%,  total acc: 80.11%   [EVAL] batch:  121 | acc: 81.25%,  total acc: 80.12%   [EVAL] batch:  122 | acc: 81.25%,  total acc: 80.13%   [EVAL] batch:  123 | acc: 81.25%,  total acc: 80.14%   [EVAL] batch:  124 | acc: 93.75%,  total acc: 80.25%   [EVAL] batch:  125 | acc: 31.25%,  total acc: 79.86%   [EVAL] batch:  126 | acc: 43.75%,  total acc: 79.58%   [EVAL] batch:  127 | acc: 18.75%,  total acc: 79.10%   [EVAL] batch:  128 | acc: 43.75%,  total acc: 78.83%   [EVAL] batch:  129 | acc: 25.00%,  total acc: 78.41%   [EVAL] batch:  130 | acc: 18.75%,  total acc: 77.96%   [EVAL] batch:  131 | acc: 62.50%,  total acc: 77.84%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 78.01%   [EVAL] batch:  133 | acc: 93.75%,  total acc: 78.12%   [EVAL] batch:  134 | acc: 93.75%,  total acc: 78.24%   [EVAL] batch:  135 | acc: 87.50%,  total acc: 78.31%   [EVAL] batch:  136 | acc: 100.00%,  total acc: 78.47%   [EVAL] batch:  137 | acc: 75.00%,  total acc: 78.44%   [EVAL] batch:  138 | acc: 25.00%,  total acc: 78.06%   [EVAL] batch:  139 | acc: 18.75%,  total acc: 77.63%   [EVAL] batch:  140 | acc: 37.50%,  total acc: 77.35%   [EVAL] batch:  141 | acc: 18.75%,  total acc: 76.94%   [EVAL] batch:  142 | acc: 37.50%,  total acc: 76.66%   [EVAL] batch:  143 | acc: 43.75%,  total acc: 76.43%   [EVAL] batch:  144 | acc: 100.00%,  total acc: 76.59%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 76.76%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 76.87%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 77.03%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 77.18%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 77.29%   [EVAL] batch:  150 | acc: 87.50%,  total acc: 77.36%   [EVAL] batch:  151 | acc: 87.50%,  total acc: 77.43%   [EVAL] batch:  152 | acc: 75.00%,  total acc: 77.41%   [EVAL] batch:  153 | acc: 87.50%,  total acc: 77.48%   [EVAL] batch:  154 | acc: 81.25%,  total acc: 77.50%   [EVAL] batch:  155 | acc: 87.50%,  total acc: 77.56%   [EVAL] batch:  156 | acc: 93.75%,  total acc: 77.67%   [EVAL] batch:  157 | acc: 100.00%,  total acc: 77.81%   [EVAL] batch:  158 | acc: 93.75%,  total acc: 77.91%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 78.01%   [EVAL] batch:  160 | acc: 93.75%,  total acc: 78.11%   [EVAL] batch:  161 | acc: 100.00%,  total acc: 78.24%   [EVAL] batch:  162 | acc: 68.75%,  total acc: 78.18%   [EVAL] batch:  163 | acc: 31.25%,  total acc: 77.90%   [EVAL] batch:  164 | acc: 31.25%,  total acc: 77.61%   [EVAL] batch:  165 | acc: 37.50%,  total acc: 77.37%   [EVAL] batch:  166 | acc: 18.75%,  total acc: 77.02%   [EVAL] batch:  167 | acc: 18.75%,  total acc: 76.67%   [EVAL] batch:  168 | acc: 56.25%,  total acc: 76.55%   [EVAL] batch:  169 | acc: 100.00%,  total acc: 76.69%   [EVAL] batch:  170 | acc: 93.75%,  total acc: 76.79%   [EVAL] batch:  171 | acc: 100.00%,  total acc: 76.93%   [EVAL] batch:  172 | acc: 100.00%,  total acc: 77.06%   [EVAL] batch:  173 | acc: 93.75%,  total acc: 77.16%   [EVAL] batch:  174 | acc: 100.00%,  total acc: 77.29%   [EVAL] batch:  175 | acc: 37.50%,  total acc: 77.06%   [EVAL] batch:  176 | acc: 50.00%,  total acc: 76.91%   [EVAL] batch:  177 | acc: 43.75%,  total acc: 76.72%   [EVAL] batch:  178 | acc: 62.50%,  total acc: 76.64%   [EVAL] batch:  179 | acc: 62.50%,  total acc: 76.56%   [EVAL] batch:  180 | acc: 25.00%,  total acc: 76.28%   [EVAL] batch:  181 | acc: 56.25%,  total acc: 76.17%   [EVAL] batch:  182 | acc: 62.50%,  total acc: 76.09%   [EVAL] batch:  183 | acc: 50.00%,  total acc: 75.95%   [EVAL] batch:  184 | acc: 87.50%,  total acc: 76.01%   [EVAL] batch:  185 | acc: 75.00%,  total acc: 76.01%   [EVAL] batch:  186 | acc: 75.00%,  total acc: 76.00%   [EVAL] batch:  187 | acc: 56.25%,  total acc: 75.90%   [EVAL] batch:  188 | acc: 62.50%,  total acc: 75.83%   [EVAL] batch:  189 | acc: 62.50%,  total acc: 75.76%   [EVAL] batch:  190 | acc: 81.25%,  total acc: 75.79%   [EVAL] batch:  191 | acc: 87.50%,  total acc: 75.85%   [EVAL] batch:  192 | acc: 93.75%,  total acc: 75.94%   [EVAL] batch:  193 | acc: 68.75%,  total acc: 75.90%   [EVAL] batch:  194 | acc: 93.75%,  total acc: 75.99%   [EVAL] batch:  195 | acc: 68.75%,  total acc: 75.96%   [EVAL] batch:  196 | acc: 56.25%,  total acc: 75.86%   [EVAL] batch:  197 | acc: 75.00%,  total acc: 75.85%   [EVAL] batch:  198 | acc: 75.00%,  total acc: 75.85%   [EVAL] batch:  199 | acc: 87.50%,  total acc: 75.91%   [EVAL] batch:  200 | acc: 87.50%,  total acc: 75.96%   [EVAL] batch:  201 | acc: 87.50%,  total acc: 76.02%   [EVAL] batch:  202 | acc: 81.25%,  total acc: 76.05%   [EVAL] batch:  203 | acc: 87.50%,  total acc: 76.10%   [EVAL] batch:  204 | acc: 93.75%,  total acc: 76.19%   [EVAL] batch:  205 | acc: 93.75%,  total acc: 76.27%   [EVAL] batch:  206 | acc: 68.75%,  total acc: 76.24%   [EVAL] batch:  207 | acc: 43.75%,  total acc: 76.08%   [EVAL] batch:  208 | acc: 25.00%,  total acc: 75.84%   [EVAL] batch:  209 | acc: 50.00%,  total acc: 75.71%   [EVAL] batch:  210 | acc: 37.50%,  total acc: 75.53%   [EVAL] batch:  211 | acc: 56.25%,  total acc: 75.44%   [EVAL] batch:  212 | acc: 81.25%,  total acc: 75.47%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 75.58%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 75.70%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 75.81%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 75.92%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 76.03%   [EVAL] batch:  218 | acc: 100.00%,  total acc: 76.14%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 76.25%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 76.36%   [EVAL] batch:  221 | acc: 100.00%,  total acc: 76.46%   [EVAL] batch:  222 | acc: 100.00%,  total acc: 76.57%   [EVAL] batch:  223 | acc: 100.00%,  total acc: 76.67%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 76.78%   [EVAL] batch:  225 | acc: 93.75%,  total acc: 76.85%   [EVAL] batch:  226 | acc: 87.50%,  total acc: 76.90%   [EVAL] batch:  227 | acc: 75.00%,  total acc: 76.89%   [EVAL] batch:  228 | acc: 75.00%,  total acc: 76.88%   [EVAL] batch:  229 | acc: 100.00%,  total acc: 76.98%   [EVAL] batch:  230 | acc: 87.50%,  total acc: 77.03%   [EVAL] batch:  231 | acc: 56.25%,  total acc: 76.94%   [EVAL] batch:  232 | acc: 50.00%,  total acc: 76.82%   [EVAL] batch:  233 | acc: 56.25%,  total acc: 76.74%   [EVAL] batch:  234 | acc: 68.75%,  total acc: 76.70%   [EVAL] batch:  235 | acc: 68.75%,  total acc: 76.67%   [EVAL] batch:  236 | acc: 68.75%,  total acc: 76.64%   [EVAL] batch:  237 | acc: 81.25%,  total acc: 76.65%   [EVAL] batch:  238 | acc: 87.50%,  total acc: 76.70%   [EVAL] batch:  239 | acc: 100.00%,  total acc: 76.80%   [EVAL] batch:  240 | acc: 75.00%,  total acc: 76.79%   [EVAL] batch:  241 | acc: 93.75%,  total acc: 76.86%   [EVAL] batch:  242 | acc: 93.75%,  total acc: 76.93%   [EVAL] batch:  243 | acc: 68.75%,  total acc: 76.90%   [EVAL] batch:  244 | acc: 12.50%,  total acc: 76.63%   [EVAL] batch:  245 | acc: 12.50%,  total acc: 76.37%   [EVAL] batch:  246 | acc: 31.25%,  total acc: 76.19%   [EVAL] batch:  247 | acc: 12.50%,  total acc: 75.93%   [EVAL] batch:  248 | acc: 18.75%,  total acc: 75.70%   [EVAL] batch:  249 | acc: 31.25%,  total acc: 75.52%   
cur_acc:  ['0.9454', '0.7996', '0.7718', '0.7341']
his_acc:  ['0.9454', '0.8715', '0.8095', '0.7552']
CurrentTrain: epoch 15, batch     0 | loss: 17.1296652CurrentTrain: epoch 15, batch     1 | loss: 13.6452318CurrentTrain: epoch 15, batch     2 | loss: 14.0882530CurrentTrain: epoch  1, batch     3 | loss: 11.9778518CurrentTrain: epoch 15, batch     0 | loss: 19.6722050CurrentTrain: epoch 15, batch     1 | loss: 17.7019227CurrentTrain: epoch 15, batch     2 | loss: 11.4393758CurrentTrain: epoch  1, batch     3 | loss: 5.6964866CurrentTrain: epoch 15, batch     0 | loss: 20.4342341CurrentTrain: epoch 15, batch     1 | loss: 12.2496747CurrentTrain: epoch 15, batch     2 | loss: 12.6277472CurrentTrain: epoch  1, batch     3 | loss: 6.3729835CurrentTrain: epoch 15, batch     0 | loss: 8.4215211CurrentTrain: epoch 15, batch     1 | loss: 14.8252794CurrentTrain: epoch 15, batch     2 | loss: 12.3606569CurrentTrain: epoch  1, batch     3 | loss: 7.2556952CurrentTrain: epoch 15, batch     0 | loss: 13.3654690CurrentTrain: epoch 15, batch     1 | loss: 18.6677118CurrentTrain: epoch 15, batch     2 | loss: 16.2532320CurrentTrain: epoch  1, batch     3 | loss: 6.2787745CurrentTrain: epoch 15, batch     0 | loss: 7.6478909CurrentTrain: epoch 15, batch     1 | loss: 15.8450990CurrentTrain: epoch 15, batch     2 | loss: 7.3095623CurrentTrain: epoch  1, batch     3 | loss: 6.2231107CurrentTrain: epoch 15, batch     0 | loss: 9.1255201CurrentTrain: epoch 15, batch     1 | loss: 12.3579709CurrentTrain: epoch 15, batch     2 | loss: 11.6960360CurrentTrain: epoch  1, batch     3 | loss: 5.6911699CurrentTrain: epoch 15, batch     0 | loss: 18.0726089CurrentTrain: epoch 15, batch     1 | loss: 9.0827169CurrentTrain: epoch 15, batch     2 | loss: 10.9136797CurrentTrain: epoch  1, batch     3 | loss: 5.9439550CurrentTrain: epoch 15, batch     0 | loss: 11.6269781CurrentTrain: epoch 15, batch     1 | loss: 9.3010082CurrentTrain: epoch 15, batch     2 | loss: 6.3104337CurrentTrain: epoch  1, batch     3 | loss: 5.6045107CurrentTrain: epoch 15, batch     0 | loss: 12.8159068CurrentTrain: epoch 15, batch     1 | loss: 12.6383152CurrentTrain: epoch 15, batch     2 | loss: 12.2525276CurrentTrain: epoch  1, batch     3 | loss: 14.4082061
#### member of political party ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "member of political party" means the political party of which a person is or has been a member or otherwise affiliated.
Here is an example:
Relation: member of political party
Context: alongside evan durbin and hugh gaitskell , he brought the thinking of john maynard keynes to the labour party , especially in relation to price determination .
Head Entity: hugh gaitskell
Tail Entity: labour party
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "member of political party":
Relation: member of political party  
Context: After serving as the mayor of Chicago, he became a prominent figure in the Democratic Party, advocating for social justice and economic reform.  
Head Entity: mayor of Chicago  
Tail Entity: Democratic Party  

Relation: member of political party  
Context: Throughout her career, she has been a staunch advocate for environmental policies as a member of the Green Party, pushing for sustainable practices at both local and national levels.  
Head Entity: she  
Tail Entity: Green Party  
#### after a work by ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "after a work by" means artist whose work strongly inspired/ was copied in this item.
Here is an example:
Relation: after a work by
Context: it is a partly fictional biography , adapted by shane connaughton and jim sheridan from the book " my left foot " by christy brown .
Head Entity: my left foot
Tail Entity: christy brown
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "after a work by":
Relation: after a work by  
Context: the film is based on the novel "the great gatsby" by f. scott fitzgerald, capturing the essence of the roaring twenties.  
Head Entity: the great gatsby  
Tail Entity: f. scott fitzgerald  

Relation: after a work by  
Context: this musical is inspired by the play "hamlet" written by william shakespeare, reimagining the classic tragedy in a contemporary setting.  
Head Entity: hamlet  
Tail Entity: william shakespeare  
#### notable work ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "notable work" means notable scientific, artistic or literary work, or other work of significance among subject's works.
Here is an example:
Relation: notable work
Context: the influential victorian art critic john ruskin expounded theories about social justice in " unto this last " ( 1860 ) .
Head Entity: john ruskin
Tail Entity: unto this last
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "notable work":
Relation: notable work  
Context: the renowned physicist albert einstein published his groundbreaking paper on the theory of relativity in "on the electrodynamics of moving bodies" (1905).  
Head Entity: albert einstein  
Tail Entity: on the electrodynamics of moving bodies  

Relation: notable work  
Context: the celebrated author j.k. rowling captivated readers with her first book in the harry potter series, "harry potter and the philosopher's stone" (1997).  
Head Entity: j.k. rowling  
Tail Entity: harry potter and the philosopher's stone  
#### headquarters location ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "headquarters location" means city, where an organization's headquarters is or has been situated.
Here is an example:
Relation: headquarters location
Context: in 1986 fram traveled to poland to play against katowice in a very even duel fram eventually lost . sparta prague came to reykjavík 1987 to play against fram reykjavik at laugardalsvöllur stadium .
Head Entity: fram reykjavik
Tail Entity: reykjavík
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "headquarters location":
Relation: headquarters location  
Context: in 2001, the tech company apple inc. moved its headquarters to cupertino, california, where it has since developed numerous innovative products.  
Head Entity: apple inc.  
Tail Entity: cupertino  

Relation: headquarters location  
Context: the multinational corporation unilever has its headquarters located in london, which serves as a central hub for its global operations.  
Head Entity: unilever  
Tail Entity: london  
#### taxon rank ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "taxon rank" means level in a taxonomic hierarchy.
Here is an example:
Relation: taxon rank
Context: a number of species that were placed in the family phyllophoridae by thander in 1989 and 1990 are now included in cucumariidae .
Head Entity: phyllophoridae
Tail Entity: family
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "taxon rank":
Relation: taxon rank  
Context: The genus Canis includes several species, with Canis lupus being the most recognized as the wolf, which belongs to the family Canidae.  
Head Entity: Canis  
Tail Entity: genus  

Relation: taxon rank  
Context: In the classification of plants, the family Rosaceae encompasses a variety of species, including roses and strawberries, which are categorized under the order Rosales.  
Head Entity: Rosaceae  
Tail Entity: family  
#### field of work ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "field of work" means specialization of a person or organization.
Here is an example:
Relation: field of work
Context: he later collaborated with vito volterra and explored laplace transforms and other parts of functional analysis .
Head Entity: vito volterra
Tail Entity: functional analysis
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "field of work":
Relation: field of work  
Context: Marie Curie conducted pioneering research on radioactivity, which laid the groundwork for advancements in nuclear physics.  
Head Entity: Marie Curie  
Tail Entity: nuclear physics  

Relation: field of work  
Context: The company specializes in artificial intelligence and machine learning, providing innovative solutions for various industries.  
Head Entity: The company  
Tail Entity: artificial intelligence  
#### licensed to broadcast to ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "licensed to broadcast to" means place that a radio/TV station is licensed/required to broadcast to.
Here is an example:
Relation: licensed to broadcast to
Context: this early involvement in radio led him to buy many small , under performing radio stations beginning with wcos in columbia , south carolina , and turn them around with new formats and improved management .
Head Entity: wcos
Tail Entity: columbia , south carolina
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "licensed to broadcast to":
Relation: licensed to broadcast to  
Context: The new regulations allowed the station KXYZ to expand its reach, enabling it to serve the community of Springfield more effectively.  
Head Entity: KXYZ  
Tail Entity: Springfield  

Relation: licensed to broadcast to  
Context: After acquiring the rights, the television network was able to transmit its shows to viewers in the greater Los Angeles area, significantly increasing its audience.  
Head Entity: television network  
Tail Entity: greater Los Angeles area  
#### constellation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "constellation" means the area of the celestial sphere of which the subject is a part.
Here is an example:
Relation: constellation
Context: hd 32518 b is an extrasolar planet which orbits the k - type giant star hd 32518 , located approximately 383 light years away in the constellation camelopardalis .
Head Entity: hd 32518
Tail Entity: camelopardalis
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "constellation":
Relation: constellation  
Context: the star sirius, known as the brightest star in the night sky, is located in the constellation canis major and is approximately 8.6 light years away from Earth.  
Head Entity: sirius  
Tail Entity: canis major  

Relation: constellation  
Context: the andromeda galaxy, which is the nearest spiral galaxy to the milky way, can be found in the constellation andromeda, making it a prominent feature in the night sky.  
Head Entity: andromeda galaxy  
Tail Entity: andromeda  
#### platform ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "platform" means platform for which a work was developed or released, or the specific platform version of a software product.
Here is an example:
Relation: platform
Context: " toy soldiers : war chest " was released on august 11 , 2015 for windows , playstation 4 and xbox one .
Head Entity: toy soldiers : war chest
Tail Entity: xbox one
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "platform":
Relation: platform  
Context: "The Legend of Zelda: Breath of the Wild" was launched on March 3, 2017 for the Nintendo Switch and Wii U.  
Head Entity: The Legend of Zelda: Breath of the Wild  
Tail Entity: Nintendo Switch  

Relation: platform  
Context: "Cyberpunk 2077" was made available on December 10, 2020 for PC, PlayStation 4, and Xbox One.  
Head Entity: Cyberpunk 2077  
Tail Entity: PlayStation 4  
#### place served by transport hub ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "place served by transport hub" means territorial entity or entities served by this transport hub (airport, train station, etc.).
Here is an example:
Relation: place served by transport hub
Context: services to the island run from san juan 's luis muñoz marín international airport , ceiba airport or isla grande airport ( 20- to 30-minute flight ) .
Head Entity: isla grande airport
Tail Entity: san juan
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "place served by transport hub":
Relation: place served by transport hub  
Context: The city of Barcelona is well-connected to the surrounding regions through the El Prat Airport, which offers numerous flights to various destinations.  
Head Entity: El Prat Airport  
Tail Entity: Barcelona  

Relation: place served by transport hub  
Context: The central train station in Zurich provides access to multiple Swiss cities and international destinations, making it a key transport hub for travelers.  
Head Entity: Zurich central train station  
Tail Entity: Zurich  
MemoryTrain:  epoch 15, batch     0 | loss: 3.8755406MemoryTrain:  epoch 15, batch     1 | loss: 5.6430025MemoryTrain:  epoch 15, batch     2 | loss: 4.7546041MemoryTrain:  epoch 15, batch     3 | loss: 5.1036454MemoryTrain:  epoch 15, batch     4 | loss: 3.7384171MemoryTrain:  epoch 15, batch     5 | loss: 6.3883143MemoryTrain:  epoch 15, batch     6 | loss: 3.5082036MemoryTrain:  epoch 15, batch     7 | loss: 3.8919848MemoryTrain:  epoch 15, batch     8 | loss: 3.2489692MemoryTrain:  epoch  5, batch     9 | loss: 8.7804114MemoryTrain:  epoch 15, batch     0 | loss: 2.6304400MemoryTrain:  epoch 15, batch     1 | loss: 4.2682880MemoryTrain:  epoch 15, batch     2 | loss: 3.5560838MemoryTrain:  epoch 15, batch     3 | loss: 2.6279799MemoryTrain:  epoch 15, batch     4 | loss: 2.6508715MemoryTrain:  epoch 15, batch     5 | loss: 2.4080540MemoryTrain:  epoch 15, batch     6 | loss: 3.3930137MemoryTrain:  epoch 15, batch     7 | loss: 3.0454632MemoryTrain:  epoch 15, batch     8 | loss: 4.1041476MemoryTrain:  epoch  5, batch     9 | loss: 8.7994995MemoryTrain:  epoch 15, batch     0 | loss: 4.7808357MemoryTrain:  epoch 15, batch     1 | loss: 2.2353227MemoryTrain:  epoch 15, batch     2 | loss: 3.5985805MemoryTrain:  epoch 15, batch     3 | loss: 3.0399528MemoryTrain:  epoch 15, batch     4 | loss: 4.6891003MemoryTrain:  epoch 15, batch     5 | loss: 2.4133491MemoryTrain:  epoch 15, batch     6 | loss: 2.0432126MemoryTrain:  epoch 15, batch     7 | loss: 5.6110289MemoryTrain:  epoch 15, batch     8 | loss: 1.6281565MemoryTrain:  epoch  5, batch     9 | loss: 8.2469321MemoryTrain:  epoch 15, batch     0 | loss: 2.1602684MemoryTrain:  epoch 15, batch     1 | loss: 1.5970506MemoryTrain:  epoch 15, batch     2 | loss: 2.7987849MemoryTrain:  epoch 15, batch     3 | loss: 2.4849764MemoryTrain:  epoch 15, batch     4 | loss: 2.1062600MemoryTrain:  epoch 15, batch     5 | loss: 2.5245466MemoryTrain:  epoch 15, batch     6 | loss: 2.0425284MemoryTrain:  epoch 15, batch     7 | loss: 3.4635924MemoryTrain:  epoch 15, batch     8 | loss: 2.1106829MemoryTrain:  epoch  5, batch     9 | loss: 8.8957677MemoryTrain:  epoch 15, batch     0 | loss: 5.2942898MemoryTrain:  epoch 15, batch     1 | loss: 1.9545902MemoryTrain:  epoch 15, batch     2 | loss: 5.3217131MemoryTrain:  epoch 15, batch     3 | loss: 2.7136676MemoryTrain:  epoch 15, batch     4 | loss: 5.0351296MemoryTrain:  epoch 15, batch     5 | loss: 1.5467702MemoryTrain:  epoch 15, batch     6 | loss: 2.4992707MemoryTrain:  epoch 15, batch     7 | loss: 2.6196612MemoryTrain:  epoch 15, batch     8 | loss: 2.4466159MemoryTrain:  epoch  5, batch     9 | loss: 8.1834728MemoryTrain:  epoch 15, batch     0 | loss: 1.8093735MemoryTrain:  epoch 15, batch     1 | loss: 2.0119071MemoryTrain:  epoch 15, batch     2 | loss: 1.9370657MemoryTrain:  epoch 15, batch     3 | loss: 4.3179474MemoryTrain:  epoch 15, batch     4 | loss: 6.6360514MemoryTrain:  epoch 15, batch     5 | loss: 1.7215399MemoryTrain:  epoch 15, batch     6 | loss: 2.1130379MemoryTrain:  epoch 15, batch     7 | loss: 2.4978500MemoryTrain:  epoch 15, batch     8 | loss: 1.6602379MemoryTrain:  epoch  5, batch     9 | loss: 7.8982391MemoryTrain:  epoch 15, batch     0 | loss: 2.1007172MemoryTrain:  epoch 15, batch     1 | loss: 2.2558613MemoryTrain:  epoch 15, batch     2 | loss: 3.7238287MemoryTrain:  epoch 15, batch     3 | loss: 2.1986109MemoryTrain:  epoch 15, batch     4 | loss: 1.8861196MemoryTrain:  epoch 15, batch     5 | loss: 2.5107534MemoryTrain:  epoch 15, batch     6 | loss: 1.7674942MemoryTrain:  epoch 15, batch     7 | loss: 2.8721319MemoryTrain:  epoch 15, batch     8 | loss: 2.3326291MemoryTrain:  epoch  5, batch     9 | loss: 7.5030793MemoryTrain:  epoch 15, batch     0 | loss: 2.5731816MemoryTrain:  epoch 15, batch     1 | loss: 2.2050108MemoryTrain:  epoch 15, batch     2 | loss: 3.1802332MemoryTrain:  epoch 15, batch     3 | loss: 4.7891675MemoryTrain:  epoch 15, batch     4 | loss: 2.2675242MemoryTrain:  epoch 15, batch     5 | loss: 2.2483722MemoryTrain:  epoch 15, batch     6 | loss: 1.8483046MemoryTrain:  epoch 15, batch     7 | loss: 2.2465167MemoryTrain:  epoch 15, batch     8 | loss: 2.5255389MemoryTrain:  epoch  5, batch     9 | loss: 7.8910351MemoryTrain:  epoch 15, batch     0 | loss: 1.9103499MemoryTrain:  epoch 15, batch     1 | loss: 2.3530052MemoryTrain:  epoch 15, batch     2 | loss: 4.9941818MemoryTrain:  epoch 15, batch     3 | loss: 1.8752197MemoryTrain:  epoch 15, batch     4 | loss: 3.7601290MemoryTrain:  epoch 15, batch     5 | loss: 1.4301207MemoryTrain:  epoch 15, batch     6 | loss: 5.9304824MemoryTrain:  epoch 15, batch     7 | loss: 4.6582670MemoryTrain:  epoch 15, batch     8 | loss: 1.3899181MemoryTrain:  epoch  5, batch     9 | loss: 8.2020511MemoryTrain:  epoch 15, batch     0 | loss: 2.1608329MemoryTrain:  epoch 15, batch     1 | loss: 2.4274014MemoryTrain:  epoch 15, batch     2 | loss: 2.0729149MemoryTrain:  epoch 15, batch     3 | loss: 1.6600550MemoryTrain:  epoch 15, batch     4 | loss: 3.1713949MemoryTrain:  epoch 15, batch     5 | loss: 2.1068103MemoryTrain:  epoch 15, batch     6 | loss: 3.2185243MemoryTrain:  epoch 15, batch     7 | loss: 4.2848312MemoryTrain:  epoch 15, batch     8 | loss: 4.1163021MemoryTrain:  epoch  5, batch     9 | loss: 8.9897533
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 92.19%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 92.50%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 92.97%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 92.36%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 90.62%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 89.77%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 88.54%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 88.46%   [EVAL] batch:   13 | acc: 81.25%,  total acc: 87.95%   [EVAL] batch:   14 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 85.55%   [EVAL] batch:   16 | acc: 62.50%,  total acc: 84.19%   [EVAL] batch:   17 | acc: 56.25%,  total acc: 82.64%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 81.58%   [EVAL] batch:   19 | acc: 56.25%,  total acc: 80.31%   [EVAL] batch:   20 | acc: 50.00%,  total acc: 78.87%   [EVAL] batch:   21 | acc: 50.00%,  total acc: 77.56%   [EVAL] batch:   22 | acc: 68.75%,  total acc: 77.17%   [EVAL] batch:   23 | acc: 68.75%,  total acc: 76.82%   [EVAL] batch:   24 | acc: 37.50%,  total acc: 75.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 76.20%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 77.08%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 77.90%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 78.66%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 79.38%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 80.04%   [EVAL] batch:   31 | acc: 87.50%,  total acc: 80.27%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 80.68%   [EVAL] batch:   33 | acc: 68.75%,  total acc: 80.33%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 80.71%   [EVAL] batch:   35 | acc: 68.75%,  total acc: 80.38%   [EVAL] batch:   36 | acc: 81.25%,  total acc: 80.41%   [EVAL] batch:   37 | acc: 75.00%,  total acc: 80.26%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 80.77%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 81.71%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 82.14%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 82.56%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 82.81%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 83.19%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 83.56%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 83.91%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 84.24%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 84.57%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 84.88%   [EVAL] batch:   50 | acc: 87.50%,  total acc: 84.93%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 85.10%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 85.26%   [EVAL] batch:   53 | acc: 93.75%,  total acc: 85.42%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 85.45%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 85.60%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 85.75%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 85.88%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 85.91%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 86.15%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 86.37%   [EVAL] batch:   61 | acc: 81.25%,  total acc: 86.29%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 85.71%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 84.38%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 85.00%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 85.42%   [EVAL] batch:    6 | acc: 50.00%,  total acc: 80.36%   [EVAL] batch:    7 | acc: 56.25%,  total acc: 77.34%   [EVAL] batch:    8 | acc: 62.50%,  total acc: 75.69%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 74.38%   [EVAL] batch:   10 | acc: 37.50%,  total acc: 71.02%   [EVAL] batch:   11 | acc: 56.25%,  total acc: 69.79%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 70.67%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 72.32%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 73.75%   [EVAL] batch:   15 | acc: 93.75%,  total acc: 75.00%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 76.10%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 77.08%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 78.29%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 78.12%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 78.57%   [EVAL] batch:   21 | acc: 68.75%,  total acc: 78.12%   [EVAL] batch:   22 | acc: 75.00%,  total acc: 77.99%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 78.91%   [EVAL] batch:   24 | acc: 81.25%,  total acc: 79.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 79.81%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 80.56%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 81.68%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 82.29%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 82.86%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 83.40%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 83.90%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 84.38%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 84.64%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 85.07%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 85.47%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 85.86%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 86.22%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 86.56%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 86.89%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 87.05%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 87.21%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 87.36%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 87.36%   [EVAL] batch:   45 | acc: 81.25%,  total acc: 87.23%   [EVAL] batch:   46 | acc: 56.25%,  total acc: 86.57%   [EVAL] batch:   47 | acc: 56.25%,  total acc: 85.94%   [EVAL] batch:   48 | acc: 93.75%,  total acc: 86.10%   [EVAL] batch:   49 | acc: 56.25%,  total acc: 85.50%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 85.66%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 85.70%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 85.85%   [EVAL] batch:   53 | acc: 87.50%,  total acc: 85.88%   [EVAL] batch:   54 | acc: 56.25%,  total acc: 85.34%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 85.49%   [EVAL] batch:   56 | acc: 68.75%,  total acc: 85.20%   [EVAL] batch:   57 | acc: 62.50%,  total acc: 84.81%   [EVAL] batch:   58 | acc: 62.50%,  total acc: 84.43%   [EVAL] batch:   59 | acc: 81.25%,  total acc: 84.38%   [EVAL] batch:   60 | acc: 87.50%,  total acc: 84.43%   [EVAL] batch:   61 | acc: 81.25%,  total acc: 84.38%   [EVAL] batch:   62 | acc: 68.75%,  total acc: 84.13%   [EVAL] batch:   63 | acc: 12.50%,  total acc: 83.01%   [EVAL] batch:   64 | acc: 25.00%,  total acc: 82.12%   [EVAL] batch:   65 | acc: 62.50%,  total acc: 81.82%   [EVAL] batch:   66 | acc: 25.00%,  total acc: 80.97%   [EVAL] batch:   67 | acc: 43.75%,  total acc: 80.42%   [EVAL] batch:   68 | acc: 43.75%,  total acc: 79.89%   [EVAL] batch:   69 | acc: 75.00%,  total acc: 79.82%   [EVAL] batch:   70 | acc: 81.25%,  total acc: 79.84%   [EVAL] batch:   71 | acc: 75.00%,  total acc: 79.77%   [EVAL] batch:   72 | acc: 87.50%,  total acc: 79.88%   [EVAL] batch:   73 | acc: 87.50%,  total acc: 79.98%   [EVAL] batch:   74 | acc: 81.25%,  total acc: 80.00%   [EVAL] batch:   75 | acc: 31.25%,  total acc: 79.36%   [EVAL] batch:   76 | acc: 25.00%,  total acc: 78.65%   [EVAL] batch:   77 | acc: 37.50%,  total acc: 78.12%   [EVAL] batch:   78 | acc: 43.75%,  total acc: 77.69%   [EVAL] batch:   79 | acc: 50.00%,  total acc: 77.34%   [EVAL] batch:   80 | acc: 25.00%,  total acc: 76.70%   [EVAL] batch:   81 | acc: 75.00%,  total acc: 76.68%   [EVAL] batch:   82 | acc: 100.00%,  total acc: 76.96%   [EVAL] batch:   83 | acc: 100.00%,  total acc: 77.23%   [EVAL] batch:   84 | acc: 93.75%,  total acc: 77.43%   [EVAL] batch:   85 | acc: 100.00%,  total acc: 77.69%   [EVAL] batch:   86 | acc: 100.00%,  total acc: 77.95%   [EVAL] batch:   87 | acc: 75.00%,  total acc: 77.91%   [EVAL] batch:   88 | acc: 6.25%,  total acc: 77.11%   [EVAL] batch:   89 | acc: 18.75%,  total acc: 76.46%   [EVAL] batch:   90 | acc: 31.25%,  total acc: 75.96%   [EVAL] batch:   91 | acc: 43.75%,  total acc: 75.61%   [EVAL] batch:   92 | acc: 25.00%,  total acc: 75.07%   [EVAL] batch:   93 | acc: 50.00%,  total acc: 74.80%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 75.07%   [EVAL] batch:   95 | acc: 93.75%,  total acc: 75.26%   [EVAL] batch:   96 | acc: 93.75%,  total acc: 75.45%   [EVAL] batch:   97 | acc: 93.75%,  total acc: 75.64%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 75.88%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 76.12%   [EVAL] batch:  100 | acc: 56.25%,  total acc: 75.93%   [EVAL] batch:  101 | acc: 93.75%,  total acc: 76.10%   [EVAL] batch:  102 | acc: 56.25%,  total acc: 75.91%   [EVAL] batch:  103 | acc: 62.50%,  total acc: 75.78%   [EVAL] batch:  104 | acc: 75.00%,  total acc: 75.77%   [EVAL] batch:  105 | acc: 81.25%,  total acc: 75.83%   [EVAL] batch:  106 | acc: 81.25%,  total acc: 75.88%   [EVAL] batch:  107 | acc: 81.25%,  total acc: 75.93%   [EVAL] batch:  108 | acc: 81.25%,  total acc: 75.97%   [EVAL] batch:  109 | acc: 75.00%,  total acc: 75.97%   [EVAL] batch:  110 | acc: 62.50%,  total acc: 75.84%   [EVAL] batch:  111 | acc: 75.00%,  total acc: 75.84%   [EVAL] batch:  112 | acc: 68.75%,  total acc: 75.77%   [EVAL] batch:  113 | acc: 37.50%,  total acc: 75.44%   [EVAL] batch:  114 | acc: 68.75%,  total acc: 75.38%   [EVAL] batch:  115 | acc: 68.75%,  total acc: 75.32%   [EVAL] batch:  116 | acc: 75.00%,  total acc: 75.32%   [EVAL] batch:  117 | acc: 62.50%,  total acc: 75.21%   [EVAL] batch:  118 | acc: 81.25%,  total acc: 75.26%   [EVAL] batch:  119 | acc: 93.75%,  total acc: 75.42%   [EVAL] batch:  120 | acc: 93.75%,  total acc: 75.57%   [EVAL] batch:  121 | acc: 87.50%,  total acc: 75.67%   [EVAL] batch:  122 | acc: 81.25%,  total acc: 75.71%   [EVAL] batch:  123 | acc: 81.25%,  total acc: 75.76%   [EVAL] batch:  124 | acc: 100.00%,  total acc: 75.95%   [EVAL] batch:  125 | acc: 18.75%,  total acc: 75.50%   [EVAL] batch:  126 | acc: 31.25%,  total acc: 75.15%   [EVAL] batch:  127 | acc: 18.75%,  total acc: 74.71%   [EVAL] batch:  128 | acc: 43.75%,  total acc: 74.47%   [EVAL] batch:  129 | acc: 25.00%,  total acc: 74.09%   [EVAL] batch:  130 | acc: 12.50%,  total acc: 73.62%   [EVAL] batch:  131 | acc: 68.75%,  total acc: 73.58%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 73.78%   [EVAL] batch:  133 | acc: 93.75%,  total acc: 73.93%   [EVAL] batch:  134 | acc: 93.75%,  total acc: 74.07%   [EVAL] batch:  135 | acc: 87.50%,  total acc: 74.17%   [EVAL] batch:  136 | acc: 100.00%,  total acc: 74.36%   [EVAL] batch:  137 | acc: 68.75%,  total acc: 74.32%   [EVAL] batch:  138 | acc: 25.00%,  total acc: 73.97%   [EVAL] batch:  139 | acc: 25.00%,  total acc: 73.62%   [EVAL] batch:  140 | acc: 43.75%,  total acc: 73.40%   [EVAL] batch:  141 | acc: 25.00%,  total acc: 73.06%   [EVAL] batch:  142 | acc: 37.50%,  total acc: 72.81%   [EVAL] batch:  143 | acc: 50.00%,  total acc: 72.66%   [EVAL] batch:  144 | acc: 100.00%,  total acc: 72.84%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 73.03%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 73.17%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 73.35%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 73.53%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 73.67%   [EVAL] batch:  150 | acc: 87.50%,  total acc: 73.76%   [EVAL] batch:  151 | acc: 87.50%,  total acc: 73.85%   [EVAL] batch:  152 | acc: 68.75%,  total acc: 73.82%   [EVAL] batch:  153 | acc: 87.50%,  total acc: 73.90%   [EVAL] batch:  154 | acc: 81.25%,  total acc: 73.95%   [EVAL] batch:  155 | acc: 87.50%,  total acc: 74.04%   [EVAL] batch:  156 | acc: 93.75%,  total acc: 74.16%   [EVAL] batch:  157 | acc: 100.00%,  total acc: 74.33%   [EVAL] batch:  158 | acc: 93.75%,  total acc: 74.45%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 74.57%   [EVAL] batch:  160 | acc: 93.75%,  total acc: 74.69%   [EVAL] batch:  161 | acc: 100.00%,  total acc: 74.85%   [EVAL] batch:  162 | acc: 62.50%,  total acc: 74.77%   [EVAL] batch:  163 | acc: 25.00%,  total acc: 74.47%   [EVAL] batch:  164 | acc: 25.00%,  total acc: 74.17%   [EVAL] batch:  165 | acc: 37.50%,  total acc: 73.95%   [EVAL] batch:  166 | acc: 18.75%,  total acc: 73.62%   [EVAL] batch:  167 | acc: 12.50%,  total acc: 73.25%   [EVAL] batch:  168 | acc: 50.00%,  total acc: 73.11%   [EVAL] batch:  169 | acc: 100.00%,  total acc: 73.27%   [EVAL] batch:  170 | acc: 93.75%,  total acc: 73.39%   [EVAL] batch:  171 | acc: 100.00%,  total acc: 73.55%   [EVAL] batch:  172 | acc: 100.00%,  total acc: 73.70%   [EVAL] batch:  173 | acc: 93.75%,  total acc: 73.81%   [EVAL] batch:  174 | acc: 93.75%,  total acc: 73.93%   [EVAL] batch:  175 | acc: 37.50%,  total acc: 73.72%   [EVAL] batch:  176 | acc: 50.00%,  total acc: 73.59%   [EVAL] batch:  177 | acc: 50.00%,  total acc: 73.46%   [EVAL] batch:  178 | acc: 56.25%,  total acc: 73.36%   [EVAL] batch:  179 | acc: 56.25%,  total acc: 73.26%   [EVAL] batch:  180 | acc: 18.75%,  total acc: 72.96%   [EVAL] batch:  181 | acc: 43.75%,  total acc: 72.80%   [EVAL] batch:  182 | acc: 68.75%,  total acc: 72.78%   [EVAL] batch:  183 | acc: 50.00%,  total acc: 72.66%   [EVAL] batch:  184 | acc: 75.00%,  total acc: 72.67%   [EVAL] batch:  185 | acc: 81.25%,  total acc: 72.72%   [EVAL] batch:  186 | acc: 68.75%,  total acc: 72.69%   [EVAL] batch:  187 | acc: 68.75%,  total acc: 72.67%   [EVAL] batch:  188 | acc: 75.00%,  total acc: 72.69%   [EVAL] batch:  189 | acc: 75.00%,  total acc: 72.70%   [EVAL] batch:  190 | acc: 81.25%,  total acc: 72.74%   [EVAL] batch:  191 | acc: 87.50%,  total acc: 72.82%   [EVAL] batch:  192 | acc: 87.50%,  total acc: 72.90%   [EVAL] batch:  193 | acc: 75.00%,  total acc: 72.91%   [EVAL] batch:  194 | acc: 93.75%,  total acc: 73.01%   [EVAL] batch:  195 | acc: 68.75%,  total acc: 72.99%   [EVAL] batch:  196 | acc: 56.25%,  total acc: 72.91%   [EVAL] batch:  197 | acc: 81.25%,  total acc: 72.95%   [EVAL] batch:  198 | acc: 75.00%,  total acc: 72.96%   [EVAL] batch:  199 | acc: 87.50%,  total acc: 73.03%   [EVAL] batch:  200 | acc: 87.50%,  total acc: 73.10%   [EVAL] batch:  201 | acc: 81.25%,  total acc: 73.14%   [EVAL] batch:  202 | acc: 75.00%,  total acc: 73.15%   [EVAL] batch:  203 | acc: 93.75%,  total acc: 73.25%   [EVAL] batch:  204 | acc: 87.50%,  total acc: 73.32%   [EVAL] batch:  205 | acc: 93.75%,  total acc: 73.42%   [EVAL] batch:  206 | acc: 68.75%,  total acc: 73.40%   [EVAL] batch:  207 | acc: 43.75%,  total acc: 73.26%   [EVAL] batch:  208 | acc: 31.25%,  total acc: 73.06%   [EVAL] batch:  209 | acc: 50.00%,  total acc: 72.95%   [EVAL] batch:  210 | acc: 50.00%,  total acc: 72.84%   [EVAL] batch:  211 | acc: 56.25%,  total acc: 72.76%   [EVAL] batch:  212 | acc: 68.75%,  total acc: 72.74%   [EVAL] batch:  213 | acc: 93.75%,  total acc: 72.84%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 72.97%   [EVAL] batch:  215 | acc: 68.75%,  total acc: 72.95%   [EVAL] batch:  216 | acc: 93.75%,  total acc: 73.04%   [EVAL] batch:  217 | acc: 81.25%,  total acc: 73.08%   [EVAL] batch:  218 | acc: 87.50%,  total acc: 73.14%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 73.27%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 73.39%   [EVAL] batch:  221 | acc: 100.00%,  total acc: 73.51%   [EVAL] batch:  222 | acc: 100.00%,  total acc: 73.63%   [EVAL] batch:  223 | acc: 100.00%,  total acc: 73.74%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 73.86%   [EVAL] batch:  225 | acc: 93.75%,  total acc: 73.95%   [EVAL] batch:  226 | acc: 87.50%,  total acc: 74.01%   [EVAL] batch:  227 | acc: 81.25%,  total acc: 74.04%   [EVAL] batch:  228 | acc: 81.25%,  total acc: 74.07%   [EVAL] batch:  229 | acc: 100.00%,  total acc: 74.18%   [EVAL] batch:  230 | acc: 93.75%,  total acc: 74.27%   [EVAL] batch:  231 | acc: 56.25%,  total acc: 74.19%   [EVAL] batch:  232 | acc: 43.75%,  total acc: 74.06%   [EVAL] batch:  233 | acc: 50.00%,  total acc: 73.96%   [EVAL] batch:  234 | acc: 68.75%,  total acc: 73.94%   [EVAL] batch:  235 | acc: 68.75%,  total acc: 73.91%   [EVAL] batch:  236 | acc: 62.50%,  total acc: 73.87%   [EVAL] batch:  237 | acc: 87.50%,  total acc: 73.92%   [EVAL] batch:  238 | acc: 87.50%,  total acc: 73.98%   [EVAL] batch:  239 | acc: 100.00%,  total acc: 74.09%   [EVAL] batch:  240 | acc: 81.25%,  total acc: 74.12%   [EVAL] batch:  241 | acc: 93.75%,  total acc: 74.20%   [EVAL] batch:  242 | acc: 93.75%,  total acc: 74.28%   [EVAL] batch:  243 | acc: 75.00%,  total acc: 74.28%   [EVAL] batch:  244 | acc: 12.50%,  total acc: 74.03%   [EVAL] batch:  245 | acc: 25.00%,  total acc: 73.83%   [EVAL] batch:  246 | acc: 31.25%,  total acc: 73.66%   [EVAL] batch:  247 | acc: 12.50%,  total acc: 73.41%   [EVAL] batch:  248 | acc: 18.75%,  total acc: 73.19%   [EVAL] batch:  249 | acc: 31.25%,  total acc: 73.02%   [EVAL] batch:  250 | acc: 93.75%,  total acc: 73.11%   [EVAL] batch:  251 | acc: 93.75%,  total acc: 73.19%   [EVAL] batch:  252 | acc: 93.75%,  total acc: 73.27%   [EVAL] batch:  253 | acc: 87.50%,  total acc: 73.33%   [EVAL] batch:  254 | acc: 93.75%,  total acc: 73.41%   [EVAL] batch:  255 | acc: 100.00%,  total acc: 73.51%   [EVAL] batch:  256 | acc: 93.75%,  total acc: 73.59%   [EVAL] batch:  257 | acc: 87.50%,  total acc: 73.64%   [EVAL] batch:  258 | acc: 87.50%,  total acc: 73.70%   [EVAL] batch:  259 | acc: 75.00%,  total acc: 73.70%   [EVAL] batch:  260 | acc: 81.25%,  total acc: 73.73%   [EVAL] batch:  261 | acc: 75.00%,  total acc: 73.74%   [EVAL] batch:  262 | acc: 87.50%,  total acc: 73.79%   [EVAL] batch:  263 | acc: 81.25%,  total acc: 73.82%   [EVAL] batch:  264 | acc: 81.25%,  total acc: 73.84%   [EVAL] batch:  265 | acc: 56.25%,  total acc: 73.78%   [EVAL] batch:  266 | acc: 62.50%,  total acc: 73.74%   [EVAL] batch:  267 | acc: 56.25%,  total acc: 73.67%   [EVAL] batch:  268 | acc: 62.50%,  total acc: 73.63%   [EVAL] batch:  269 | acc: 56.25%,  total acc: 73.56%   [EVAL] batch:  270 | acc: 50.00%,  total acc: 73.48%   [EVAL] batch:  271 | acc: 50.00%,  total acc: 73.39%   [EVAL] batch:  272 | acc: 68.75%,  total acc: 73.37%   [EVAL] batch:  273 | acc: 68.75%,  total acc: 73.36%   [EVAL] batch:  274 | acc: 37.50%,  total acc: 73.23%   [EVAL] batch:  275 | acc: 100.00%,  total acc: 73.32%   [EVAL] batch:  276 | acc: 100.00%,  total acc: 73.42%   [EVAL] batch:  277 | acc: 100.00%,  total acc: 73.52%   [EVAL] batch:  278 | acc: 100.00%,  total acc: 73.61%   [EVAL] batch:  279 | acc: 100.00%,  total acc: 73.71%   [EVAL] batch:  280 | acc: 100.00%,  total acc: 73.80%   [EVAL] batch:  281 | acc: 87.50%,  total acc: 73.85%   [EVAL] batch:  282 | acc: 93.75%,  total acc: 73.92%   [EVAL] batch:  283 | acc: 68.75%,  total acc: 73.90%   [EVAL] batch:  284 | acc: 93.75%,  total acc: 73.97%   [EVAL] batch:  285 | acc: 68.75%,  total acc: 73.95%   [EVAL] batch:  286 | acc: 81.25%,  total acc: 73.98%   [EVAL] batch:  287 | acc: 75.00%,  total acc: 73.98%   [EVAL] batch:  288 | acc: 100.00%,  total acc: 74.07%   [EVAL] batch:  289 | acc: 100.00%,  total acc: 74.16%   [EVAL] batch:  290 | acc: 100.00%,  total acc: 74.25%   [EVAL] batch:  291 | acc: 100.00%,  total acc: 74.34%   [EVAL] batch:  292 | acc: 100.00%,  total acc: 74.42%   [EVAL] batch:  293 | acc: 93.75%,  total acc: 74.49%   [EVAL] batch:  294 | acc: 100.00%,  total acc: 74.58%   [EVAL] batch:  295 | acc: 100.00%,  total acc: 74.66%   [EVAL] batch:  296 | acc: 100.00%,  total acc: 74.75%   [EVAL] batch:  297 | acc: 100.00%,  total acc: 74.83%   [EVAL] batch:  298 | acc: 100.00%,  total acc: 74.92%   [EVAL] batch:  299 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:  300 | acc: 87.50%,  total acc: 75.04%   [EVAL] batch:  301 | acc: 93.75%,  total acc: 75.10%   [EVAL] batch:  302 | acc: 93.75%,  total acc: 75.17%   [EVAL] batch:  303 | acc: 93.75%,  total acc: 75.23%   [EVAL] batch:  304 | acc: 87.50%,  total acc: 75.27%   [EVAL] batch:  305 | acc: 93.75%,  total acc: 75.33%   [EVAL] batch:  306 | acc: 93.75%,  total acc: 75.39%   [EVAL] batch:  307 | acc: 93.75%,  total acc: 75.45%   [EVAL] batch:  308 | acc: 87.50%,  total acc: 75.49%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 75.56%   [EVAL] batch:  310 | acc: 100.00%,  total acc: 75.64%   [EVAL] batch:  311 | acc: 81.25%,  total acc: 75.66%   [EVAL] batch:  312 | acc: 50.00%,  total acc: 75.58%   
cur_acc:  ['0.9454', '0.7996', '0.7718', '0.7341', '0.8571']
his_acc:  ['0.9454', '0.8715', '0.8095', '0.7552', '0.7558']
CurrentTrain: epoch 15, batch     0 | loss: 13.4463479CurrentTrain: epoch 15, batch     1 | loss: 15.8320949CurrentTrain: epoch 15, batch     2 | loss: 18.5912054CurrentTrain: epoch  1, batch     3 | loss: 10.2186240CurrentTrain: epoch 15, batch     0 | loss: 12.9327500CurrentTrain: epoch 15, batch     1 | loss: 11.7564101CurrentTrain: epoch 15, batch     2 | loss: 11.4972502CurrentTrain: epoch  1, batch     3 | loss: 8.7278917CurrentTrain: epoch 15, batch     0 | loss: 17.6104546CurrentTrain: epoch 15, batch     1 | loss: 11.8098567CurrentTrain: epoch 15, batch     2 | loss: 21.8519197CurrentTrain: epoch  1, batch     3 | loss: 7.5468594CurrentTrain: epoch 15, batch     0 | loss: 11.4789860CurrentTrain: epoch 15, batch     1 | loss: 10.7330534CurrentTrain: epoch 15, batch     2 | loss: 14.3676037CurrentTrain: epoch  1, batch     3 | loss: 18.8061865CurrentTrain: epoch 15, batch     0 | loss: 12.1257433CurrentTrain: epoch 15, batch     1 | loss: 14.5380270CurrentTrain: epoch 15, batch     2 | loss: 11.2188066CurrentTrain: epoch  1, batch     3 | loss: 6.7762382CurrentTrain: epoch 15, batch     0 | loss: 12.0391756CurrentTrain: epoch 15, batch     1 | loss: 9.9465604CurrentTrain: epoch 15, batch     2 | loss: 9.7804416CurrentTrain: epoch  1, batch     3 | loss: 9.4782638CurrentTrain: epoch 15, batch     0 | loss: 7.9143136CurrentTrain: epoch 15, batch     1 | loss: 10.5549403CurrentTrain: epoch 15, batch     2 | loss: 10.7791093CurrentTrain: epoch  1, batch     3 | loss: 7.5542111CurrentTrain: epoch 15, batch     0 | loss: 7.8868215CurrentTrain: epoch 15, batch     1 | loss: 10.5579565CurrentTrain: epoch 15, batch     2 | loss: 24.5958326CurrentTrain: epoch  1, batch     3 | loss: 7.3119895CurrentTrain: epoch 15, batch     0 | loss: 22.3819600CurrentTrain: epoch 15, batch     1 | loss: 10.6352592CurrentTrain: epoch 15, batch     2 | loss: 5.3323139CurrentTrain: epoch  1, batch     3 | loss: 6.2360452CurrentTrain: epoch 15, batch     0 | loss: 10.4454481CurrentTrain: epoch 15, batch     1 | loss: 8.7040857CurrentTrain: epoch 15, batch     2 | loss: 9.6838088CurrentTrain: epoch  1, batch     3 | loss: 6.4117419
#### spouse ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "spouse" means the subject has the object as their spouse (husband, wife, partner, etc.).
Here is an example:
Relation: spouse
Context: " clinton crazies " is a term in american politics of the 1990s and later that refers to intense criticism of united states president bill clinton and his wife hillary clinton .
Head Entity: bill clinton
Tail Entity: hillary clinton
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "spouse":
Relation: spouse  
Context: "In a lavish ceremony, actor Ryan Reynolds married actress Blake Lively, solidifying their status as one of Hollywood's power couples."  
Head Entity: Ryan Reynolds  
Tail Entity: Blake Lively  

Relation: spouse  
Context: "The couple celebrated their 25th wedding anniversary, reflecting on their journey together, with John Legend expressing his love for Chrissy Teigen in a heartfelt post."  
Head Entity: John Legend  
Tail Entity: Chrissy Teigen  
#### record label ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "record label" means brand and trademark associated with the marketing of subject music recordings and music videos.
Here is an example:
Relation: record label
Context: it was first released on a single in the uk by parlophone in september 1967 , and on the group 's self - titled album " tomorrow " in february 1968 .
Head Entity: tomorrow
Tail Entity: parlophone
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "record label":
Relation: record label  
Context: The band's latest album was distributed by Atlantic Records, showcasing their unique sound and artistic vision.  
Head Entity: Atlantic Records  
Tail Entity: band  

Relation: record label  
Context: After signing with Universal Music Group, the artist released a series of hit singles that topped the charts.  
Head Entity: Universal Music Group  
Tail Entity: artist  
#### distributor ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "distributor" means distributor of a creative work; distributor for a record label; news agency; film distributor.
Here is an example:
Relation: distributor
Context: " the many adventures of winnie the pooh " is a 1977 american animated buddy musical comedy film produced by walt disney productions and distributed by buena vista distribution .
Head Entity: the many adventures of winnie the pooh
Tail Entity: buena vista distribution
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "distributor":
Relation: distributor  
Context: "Inception" is a 2010 science fiction film directed by Christopher Nolan and distributed by Warner Bros. Pictures.  
Head Entity: Inception  
Tail Entity: Warner Bros. Pictures  

Relation: distributor  
Context: "The Dark Knight" is a 2008 superhero film directed by Christopher Nolan and distributed by Warner Bros. Pictures.  
Head Entity: The Dark Knight  
Tail Entity: Warner Bros. Pictures  
#### located in or next to body of water ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "located in or next to body of water" means body of water on or next to which a place is located.
Here is an example:
Relation: located in or next to body of water
Context: in 1796 carondelet commissioned pintado to conduct a survey of lands granted to ambrosio longue - épée in iberville county on the west bank of the mississippi river opposite baton rouge .
Head Entity: baton rouge
Tail Entity: mississippi river
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "located in or next to body of water":
Relation: located in or next to body of water  
Context: the city of new orleans is situated on the banks of the mississippi river, making it a vibrant hub for trade and culture.  
Head Entity: new orleans  
Tail Entity: mississippi river  

Relation: located in or next to body of water  
Context: the picturesque village of lake placid is nestled beside the serene waters of mirror lake, attracting visitors year-round.  
Head Entity: lake placid  
Tail Entity: mirror lake  
#### subsidiary ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "subsidiary" means subsidiary of a company or organization; generally a fully owned separate corporation.
Here is an example:
Relation: subsidiary
Context: cbs and viacom split again in 2005 , but both are still owned by national amusements .
Head Entity: national amusements
Tail Entity: viacom
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "subsidiary":
Sample 1:  
Relation: subsidiary  
Context: In 2018, Facebook acquired WhatsApp, making it a subsidiary of the social media giant.  
Head Entity: Facebook  
Tail Entity: WhatsApp  

Sample 2:  
Relation: subsidiary  
Context: Toyota has several subsidiaries, including Lexus, which focuses on luxury vehicles.  
Head Entity: Toyota  
Tail Entity: Lexus  
#### country of citizenship ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "country of citizenship" means the object is a country that recognizes the subject as its citizen.
Here is an example:
Relation: country of citizenship
Context: he challenged seven other swimmers in heat four including ukraine 's rostyslav svanidze , a top 16 finalist in atlanta four years earlier .
Head Entity: rostyslav svanidze
Tail Entity: ukraine
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "country of citizenship":
Relation: country of citizenship  
Context: after years of living in the united states, the renowned artist decided to apply for dual citizenship, allowing him to maintain his ties to his homeland, italy.  
Head Entity: the renowned artist  
Tail Entity: italy  

Relation: country of citizenship  
Context: during the international conference, the delegate from canada proudly spoke about the rights and privileges of being a citizen of his country, emphasizing the importance of citizenship in shaping national identity.  
Head Entity: the delegate from canada  
Tail Entity: canada  
#### genre ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "genre" means creative work's genre or an artist's field of work.
Here is an example:
Relation: genre
Context: buck moved to austin in the mid-1970s and joined the blues rock group , the fabulous thunderbirds , along with keith ferguson , jimmie vaughan , and kim wilson .
Head Entity: the fabulous thunderbirds
Tail Entity: blues rock
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "genre":
Relation: genre  
Context: the album features a mix of pop and electronic music, showcasing the artist's versatility and creativity.  
Head Entity: the album  
Tail Entity: pop and electronic music  

Relation: genre  
Context: the film is a thrilling blend of horror and science fiction, captivating audiences with its unique storyline and visual effects.  
Head Entity: the film  
Tail Entity: horror and science fiction  
#### mouth of the watercourse ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "mouth of the watercourse" means the body of water to which the watercourse drains.
Here is an example:
Relation: mouth of the watercourse
Context: san lázaro is 660 km far from asunción and is located at the exact site of the confluence of the apa river with paraguay river , bordering brazil - chaco .
Head Entity: apa river
Tail Entity: paraguay river
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "mouth of the watercourse":
Relation: mouth of the watercourse  
Context: the mississippi river flows into the gulf of mexico, creating a rich delta ecosystem that supports diverse wildlife and plant species.  
Head Entity: mississippi river  
Tail Entity: gulf of mexico  

Relation: mouth of the watercourse  
Context: the thames river empties into the north sea, serving as a vital waterway for trade and transportation in london.  
Head Entity: thames river  
Tail Entity: north sea  
#### position played on team / speciality ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "position played on team / speciality" means position or specialism of a player on a team.
Here is an example:
Relation: position played on team / speciality
Context: bradley halliday ( born 10 july 1995 ) is an english professional footballer who plays as a defender or a midfielder for league two club cambridge united .
Head Entity: bradley halliday
Tail Entity: defender
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "position played on team / speciality":
Relation: position played on team / speciality  
Context: alex morgan ( born 2 july 1989 ) is an american soccer player who plays as a forward for the national team and club orlando pride in the national women's soccer league.  
Head Entity: alex morgan  
Tail Entity: forward  

Relation: position played on team / speciality  
Context: lebron james ( born 30 december 1984 ) is an american professional basketball player who plays as a small forward for the los angeles lakers in the nba.  
Head Entity: lebron james  
Tail Entity: small forward  
#### sports season of league or competition ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "sports season of league or competition" means property that shows the competition of which the item is a season.
Here is an example:
Relation: sports season of league or competition
Context: the stade louis ii in monaco was the venue for the uefa super cup every year since 1998 .
Head Entity: 1998
Tail Entity: uefa super cup
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "sports season of league or competition":
Relation: sports season of league or competition  
Context: The 2020 NBA season was significantly impacted by the COVID-19 pandemic, leading to a unique playoff format.  
Head Entity: 2020  
Tail Entity: NBA  

Relation: sports season of league or competition  
Context: The 2019 Rugby World Cup took place in Japan, showcasing teams from around the globe.  
Head Entity: 2019  
Tail Entity: Rugby World Cup  
MemoryTrain:  epoch 15, batch     0 | loss: 3.2054978MemoryTrain:  epoch 15, batch     1 | loss: 6.4745340MemoryTrain:  epoch 15, batch     2 | loss: 5.2942034MemoryTrain:  epoch 15, batch     3 | loss: 5.0872044MemoryTrain:  epoch 15, batch     4 | loss: 2.3764340MemoryTrain:  epoch 15, batch     5 | loss: 3.4223722MemoryTrain:  epoch 15, batch     6 | loss: 2.7061312MemoryTrain:  epoch 15, batch     7 | loss: 2.7078037MemoryTrain:  epoch 15, batch     8 | loss: 3.9553782MemoryTrain:  epoch 15, batch     9 | loss: 5.5267014MemoryTrain:  epoch 15, batch    10 | loss: 3.3927042MemoryTrain:  epoch  3, batch    11 | loss: 12.2073978MemoryTrain:  epoch 15, batch     0 | loss: 2.6779674MemoryTrain:  epoch 15, batch     1 | loss: 2.2496032MemoryTrain:  epoch 15, batch     2 | loss: 4.0329967MemoryTrain:  epoch 15, batch     3 | loss: 10.1699219MemoryTrain:  epoch 15, batch     4 | loss: 2.9312594MemoryTrain:  epoch 15, batch     5 | loss: 2.8131787MemoryTrain:  epoch 15, batch     6 | loss: 2.3227565MemoryTrain:  epoch 15, batch     7 | loss: 2.9129231MemoryTrain:  epoch 15, batch     8 | loss: 3.7842452MemoryTrain:  epoch 15, batch     9 | loss: 3.3385429MemoryTrain:  epoch 15, batch    10 | loss: 2.7174718MemoryTrain:  epoch  3, batch    11 | loss: 24.9713531MemoryTrain:  epoch 15, batch     0 | loss: 2.4170389MemoryTrain:  epoch 15, batch     1 | loss: 2.7180769MemoryTrain:  epoch 15, batch     2 | loss: 2.3689389MemoryTrain:  epoch 15, batch     3 | loss: 4.2460978MemoryTrain:  epoch 15, batch     4 | loss: 4.0935153MemoryTrain:  epoch 15, batch     5 | loss: 5.3385739MemoryTrain:  epoch 15, batch     6 | loss: 2.4487239MemoryTrain:  epoch 15, batch     7 | loss: 2.5222540MemoryTrain:  epoch 15, batch     8 | loss: 2.4867333MemoryTrain:  epoch 15, batch     9 | loss: 2.5938938MemoryTrain:  epoch 15, batch    10 | loss: 1.4617537MemoryTrain:  epoch  3, batch    11 | loss: 10.9235053MemoryTrain:  epoch 15, batch     0 | loss: 2.6217662MemoryTrain:  epoch 15, batch     1 | loss: 1.8367212MemoryTrain:  epoch 15, batch     2 | loss: 2.4459804MemoryTrain:  epoch 15, batch     3 | loss: 5.0396745MemoryTrain:  epoch 15, batch     4 | loss: 2.2831259MemoryTrain:  epoch 15, batch     5 | loss: 2.3758199MemoryTrain:  epoch 15, batch     6 | loss: 2.1899183MemoryTrain:  epoch 15, batch     7 | loss: 2.4326683MemoryTrain:  epoch 15, batch     8 | loss: 3.8967794MemoryTrain:  epoch 15, batch     9 | loss: 2.3854242MemoryTrain:  epoch 15, batch    10 | loss: 2.3977412MemoryTrain:  epoch  3, batch    11 | loss: 10.6838362MemoryTrain:  epoch 15, batch     0 | loss: 1.9348932MemoryTrain:  epoch 15, batch     1 | loss: 2.2997436MemoryTrain:  epoch 15, batch     2 | loss: 2.0456706MemoryTrain:  epoch 15, batch     3 | loss: 4.1345799MemoryTrain:  epoch 15, batch     4 | loss: 2.2647481MemoryTrain:  epoch 15, batch     5 | loss: 1.8146269MemoryTrain:  epoch 15, batch     6 | loss: 2.1840832MemoryTrain:  epoch 15, batch     7 | loss: 1.9631400MemoryTrain:  epoch 15, batch     8 | loss: 1.8513884MemoryTrain:  epoch 15, batch     9 | loss: 2.0668926MemoryTrain:  epoch 15, batch    10 | loss: 3.0038919MemoryTrain:  epoch  3, batch    11 | loss: 10.7858855MemoryTrain:  epoch 15, batch     0 | loss: 2.5858998MemoryTrain:  epoch 15, batch     1 | loss: 2.0008807MemoryTrain:  epoch 15, batch     2 | loss: 1.4873895MemoryTrain:  epoch 15, batch     3 | loss: 2.2189566MemoryTrain:  epoch 15, batch     4 | loss: 1.9398326MemoryTrain:  epoch 15, batch     5 | loss: 1.8161128MemoryTrain:  epoch 15, batch     6 | loss: 1.6168107MemoryTrain:  epoch 15, batch     7 | loss: 3.7698474MemoryTrain:  epoch 15, batch     8 | loss: 1.5715603MemoryTrain:  epoch 15, batch     9 | loss: 2.3565122MemoryTrain:  epoch 15, batch    10 | loss: 1.9250749MemoryTrain:  epoch  3, batch    11 | loss: 10.1110390MemoryTrain:  epoch 15, batch     0 | loss: 1.8928133MemoryTrain:  epoch 15, batch     1 | loss: 1.5599388MemoryTrain:  epoch 15, batch     2 | loss: 1.4293886MemoryTrain:  epoch 15, batch     3 | loss: 1.7642438MemoryTrain:  epoch 15, batch     4 | loss: 2.6466437MemoryTrain:  epoch 15, batch     5 | loss: 3.0837880MemoryTrain:  epoch 15, batch     6 | loss: 2.0885883MemoryTrain:  epoch 15, batch     7 | loss: 2.6464755MemoryTrain:  epoch 15, batch     8 | loss: 2.6537514MemoryTrain:  epoch 15, batch     9 | loss: 2.9832580MemoryTrain:  epoch 15, batch    10 | loss: 2.7768677MemoryTrain:  epoch  3, batch    11 | loss: 9.8720592MemoryTrain:  epoch 15, batch     0 | loss: 2.2688449MemoryTrain:  epoch 15, batch     1 | loss: 4.3179134MemoryTrain:  epoch 15, batch     2 | loss: 4.4046888MemoryTrain:  epoch 15, batch     3 | loss: 1.3995835MemoryTrain:  epoch 15, batch     4 | loss: 2.2086623MemoryTrain:  epoch 15, batch     5 | loss: 3.1260370MemoryTrain:  epoch 15, batch     6 | loss: 1.3989471MemoryTrain:  epoch 15, batch     7 | loss: 3.9627133MemoryTrain:  epoch 15, batch     8 | loss: 4.0822679MemoryTrain:  epoch 15, batch     9 | loss: 2.2521131MemoryTrain:  epoch 15, batch    10 | loss: 1.7620278MemoryTrain:  epoch  3, batch    11 | loss: 10.3895292MemoryTrain:  epoch 15, batch     0 | loss: 1.3834779MemoryTrain:  epoch 15, batch     1 | loss: 1.5282338MemoryTrain:  epoch 15, batch     2 | loss: 2.6079079MemoryTrain:  epoch 15, batch     3 | loss: 2.2296105MemoryTrain:  epoch 15, batch     4 | loss: 1.7219832MemoryTrain:  epoch 15, batch     5 | loss: 4.3912363MemoryTrain:  epoch 15, batch     6 | loss: 1.3539385MemoryTrain:  epoch 15, batch     7 | loss: 2.4049778MemoryTrain:  epoch 15, batch     8 | loss: 2.1398539MemoryTrain:  epoch 15, batch     9 | loss: 3.6935262MemoryTrain:  epoch 15, batch    10 | loss: 1.4812111MemoryTrain:  epoch  3, batch    11 | loss: 10.4585162MemoryTrain:  epoch 15, batch     0 | loss: 1.7916244MemoryTrain:  epoch 15, batch     1 | loss: 2.2774749MemoryTrain:  epoch 15, batch     2 | loss: 2.1350808MemoryTrain:  epoch 15, batch     3 | loss: 2.1914857MemoryTrain:  epoch 15, batch     4 | loss: 2.9570514MemoryTrain:  epoch 15, batch     5 | loss: 2.4580247MemoryTrain:  epoch 15, batch     6 | loss: 1.4201767MemoryTrain:  epoch 15, batch     7 | loss: 2.0148394MemoryTrain:  epoch 15, batch     8 | loss: 2.0846021MemoryTrain:  epoch 15, batch     9 | loss: 1.8318734MemoryTrain:  epoch 15, batch    10 | loss: 1.4115832MemoryTrain:  epoch  3, batch    11 | loss: 10.6657807
[EVAL] batch:    0 | acc: 37.50%,  total acc: 37.50%   [EVAL] batch:    1 | acc: 18.75%,  total acc: 28.12%   [EVAL] batch:    2 | acc: 43.75%,  total acc: 33.33%   [EVAL] batch:    3 | acc: 31.25%,  total acc: 32.81%   [EVAL] batch:    4 | acc: 50.00%,  total acc: 36.25%   [EVAL] batch:    5 | acc: 43.75%,  total acc: 37.50%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 44.64%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 51.56%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 55.56%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 59.38%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 62.50%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 65.62%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 67.31%   [EVAL] batch:   13 | acc: 56.25%,  total acc: 66.52%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 66.67%   [EVAL] batch:   15 | acc: 81.25%,  total acc: 67.58%   [EVAL] batch:   16 | acc: 81.25%,  total acc: 68.38%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 68.40%   [EVAL] batch:   18 | acc: 75.00%,  total acc: 68.75%   [EVAL] batch:   19 | acc: 37.50%,  total acc: 67.19%   [EVAL] batch:   20 | acc: 50.00%,  total acc: 66.37%   [EVAL] batch:   21 | acc: 43.75%,  total acc: 65.34%   [EVAL] batch:   22 | acc: 31.25%,  total acc: 63.86%   [EVAL] batch:   23 | acc: 50.00%,  total acc: 63.28%   [EVAL] batch:   24 | acc: 56.25%,  total acc: 63.00%   [EVAL] batch:   25 | acc: 81.25%,  total acc: 63.70%   [EVAL] batch:   26 | acc: 68.75%,  total acc: 63.89%   [EVAL] batch:   27 | acc: 81.25%,  total acc: 64.51%   [EVAL] batch:   28 | acc: 87.50%,  total acc: 65.30%   [EVAL] batch:   29 | acc: 56.25%,  total acc: 65.00%   [EVAL] batch:   30 | acc: 50.00%,  total acc: 64.52%   [EVAL] batch:   31 | acc: 56.25%,  total acc: 64.26%   [EVAL] batch:   32 | acc: 50.00%,  total acc: 63.83%   [EVAL] batch:   33 | acc: 62.50%,  total acc: 63.79%   [EVAL] batch:   34 | acc: 62.50%,  total acc: 63.75%   [EVAL] batch:   35 | acc: 68.75%,  total acc: 63.89%   [EVAL] batch:   36 | acc: 68.75%,  total acc: 64.02%   [EVAL] batch:   37 | acc: 68.75%,  total acc: 64.14%   [EVAL] batch:   38 | acc: 62.50%,  total acc: 64.10%   [EVAL] batch:   39 | acc: 68.75%,  total acc: 64.22%   [EVAL] batch:   40 | acc: 56.25%,  total acc: 64.02%   [EVAL] batch:   41 | acc: 75.00%,  total acc: 64.29%   [EVAL] batch:   42 | acc: 68.75%,  total acc: 64.39%   [EVAL] batch:   43 | acc: 56.25%,  total acc: 64.20%   [EVAL] batch:   44 | acc: 37.50%,  total acc: 63.61%   [EVAL] batch:   45 | acc: 37.50%,  total acc: 63.04%   [EVAL] batch:   46 | acc: 25.00%,  total acc: 62.23%   [EVAL] batch:   47 | acc: 50.00%,  total acc: 61.98%   [EVAL] batch:   48 | acc: 31.25%,  total acc: 61.35%   [EVAL] batch:   49 | acc: 50.00%,  total acc: 61.12%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 61.89%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 62.62%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 63.33%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 64.00%   [EVAL] batch:   54 | acc: 100.00%,  total acc: 64.66%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 65.29%   [EVAL] batch:   56 | acc: 75.00%,  total acc: 65.46%   [EVAL] batch:   57 | acc: 56.25%,  total acc: 65.30%   [EVAL] batch:   58 | acc: 68.75%,  total acc: 65.36%   [EVAL] batch:   59 | acc: 81.25%,  total acc: 65.62%   [EVAL] batch:   60 | acc: 62.50%,  total acc: 65.57%   [EVAL] batch:   61 | acc: 75.00%,  total acc: 65.73%   [EVAL] batch:   62 | acc: 31.25%,  total acc: 65.18%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 82.81%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 83.75%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:    6 | acc: 50.00%,  total acc: 79.46%   [EVAL] batch:    7 | acc: 56.25%,  total acc: 76.56%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 75.69%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 75.00%   [EVAL] batch:   10 | acc: 37.50%,  total acc: 71.59%   [EVAL] batch:   11 | acc: 62.50%,  total acc: 70.83%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 71.15%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 72.77%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 74.17%   [EVAL] batch:   15 | acc: 93.75%,  total acc: 75.39%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 76.47%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 77.43%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 78.62%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 78.44%   [EVAL] batch:   20 | acc: 81.25%,  total acc: 78.57%   [EVAL] batch:   21 | acc: 68.75%,  total acc: 78.12%   [EVAL] batch:   22 | acc: 75.00%,  total acc: 77.99%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 78.39%   [EVAL] batch:   24 | acc: 81.25%,  total acc: 78.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 79.33%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 80.09%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 80.80%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 81.47%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 82.08%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 82.66%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 83.01%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 83.52%   [EVAL] batch:   33 | acc: 68.75%,  total acc: 83.09%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 82.86%   [EVAL] batch:   35 | acc: 75.00%,  total acc: 82.64%   [EVAL] batch:   36 | acc: 87.50%,  total acc: 82.77%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 82.89%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 83.33%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 83.75%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 84.15%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 84.59%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 84.80%   [EVAL] batch:   44 | acc: 81.25%,  total acc: 84.72%   [EVAL] batch:   45 | acc: 75.00%,  total acc: 84.51%   [EVAL] batch:   46 | acc: 56.25%,  total acc: 83.91%   [EVAL] batch:   47 | acc: 56.25%,  total acc: 83.33%   [EVAL] batch:   48 | acc: 93.75%,  total acc: 83.55%   [EVAL] batch:   49 | acc: 62.50%,  total acc: 83.12%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 83.33%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 83.41%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 83.61%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 83.56%   [EVAL] batch:   54 | acc: 50.00%,  total acc: 82.95%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 83.15%   [EVAL] batch:   56 | acc: 56.25%,  total acc: 82.68%   [EVAL] batch:   57 | acc: 43.75%,  total acc: 82.00%   [EVAL] batch:   58 | acc: 50.00%,  total acc: 81.46%   [EVAL] batch:   59 | acc: 68.75%,  total acc: 81.25%   [EVAL] batch:   60 | acc: 87.50%,  total acc: 81.35%   [EVAL] batch:   61 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:   62 | acc: 56.25%,  total acc: 80.85%   [EVAL] batch:   63 | acc: 18.75%,  total acc: 79.88%   [EVAL] batch:   64 | acc: 25.00%,  total acc: 79.04%   [EVAL] batch:   65 | acc: 62.50%,  total acc: 78.79%   [EVAL] batch:   66 | acc: 25.00%,  total acc: 77.99%   [EVAL] batch:   67 | acc: 43.75%,  total acc: 77.48%   [EVAL] batch:   68 | acc: 31.25%,  total acc: 76.81%   [EVAL] batch:   69 | acc: 81.25%,  total acc: 76.88%   [EVAL] batch:   70 | acc: 75.00%,  total acc: 76.85%   [EVAL] batch:   71 | acc: 81.25%,  total acc: 76.91%   [EVAL] batch:   72 | acc: 87.50%,  total acc: 77.05%   [EVAL] batch:   73 | acc: 87.50%,  total acc: 77.20%   [EVAL] batch:   74 | acc: 81.25%,  total acc: 77.25%   [EVAL] batch:   75 | acc: 37.50%,  total acc: 76.73%   [EVAL] batch:   76 | acc: 25.00%,  total acc: 76.06%   [EVAL] batch:   77 | acc: 37.50%,  total acc: 75.56%   [EVAL] batch:   78 | acc: 43.75%,  total acc: 75.16%   [EVAL] batch:   79 | acc: 50.00%,  total acc: 74.84%   [EVAL] batch:   80 | acc: 25.00%,  total acc: 74.23%   [EVAL] batch:   81 | acc: 81.25%,  total acc: 74.31%   [EVAL] batch:   82 | acc: 100.00%,  total acc: 74.62%   [EVAL] batch:   83 | acc: 100.00%,  total acc: 74.93%   [EVAL] batch:   84 | acc: 93.75%,  total acc: 75.15%   [EVAL] batch:   85 | acc: 100.00%,  total acc: 75.44%   [EVAL] batch:   86 | acc: 93.75%,  total acc: 75.65%   [EVAL] batch:   87 | acc: 75.00%,  total acc: 75.64%   [EVAL] batch:   88 | acc: 6.25%,  total acc: 74.86%   [EVAL] batch:   89 | acc: 25.00%,  total acc: 74.31%   [EVAL] batch:   90 | acc: 25.00%,  total acc: 73.76%   [EVAL] batch:   91 | acc: 43.75%,  total acc: 73.44%   [EVAL] batch:   92 | acc: 25.00%,  total acc: 72.92%   [EVAL] batch:   93 | acc: 43.75%,  total acc: 72.61%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 72.89%   [EVAL] batch:   95 | acc: 93.75%,  total acc: 73.11%   [EVAL] batch:   96 | acc: 87.50%,  total acc: 73.26%   [EVAL] batch:   97 | acc: 87.50%,  total acc: 73.41%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 73.67%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 73.94%   [EVAL] batch:  100 | acc: 56.25%,  total acc: 73.76%   [EVAL] batch:  101 | acc: 93.75%,  total acc: 73.96%   [EVAL] batch:  102 | acc: 56.25%,  total acc: 73.79%   [EVAL] batch:  103 | acc: 62.50%,  total acc: 73.68%   [EVAL] batch:  104 | acc: 81.25%,  total acc: 73.75%   [EVAL] batch:  105 | acc: 75.00%,  total acc: 73.76%   [EVAL] batch:  106 | acc: 81.25%,  total acc: 73.83%   [EVAL] batch:  107 | acc: 81.25%,  total acc: 73.90%   [EVAL] batch:  108 | acc: 81.25%,  total acc: 73.97%   [EVAL] batch:  109 | acc: 62.50%,  total acc: 73.86%   [EVAL] batch:  110 | acc: 56.25%,  total acc: 73.70%   [EVAL] batch:  111 | acc: 75.00%,  total acc: 73.72%   [EVAL] batch:  112 | acc: 75.00%,  total acc: 73.73%   [EVAL] batch:  113 | acc: 56.25%,  total acc: 73.57%   [EVAL] batch:  114 | acc: 62.50%,  total acc: 73.48%   [EVAL] batch:  115 | acc: 68.75%,  total acc: 73.44%   [EVAL] batch:  116 | acc: 87.50%,  total acc: 73.56%   [EVAL] batch:  117 | acc: 75.00%,  total acc: 73.57%   [EVAL] batch:  118 | acc: 68.75%,  total acc: 73.53%   [EVAL] batch:  119 | acc: 93.75%,  total acc: 73.70%   [EVAL] batch:  120 | acc: 93.75%,  total acc: 73.86%   [EVAL] batch:  121 | acc: 81.25%,  total acc: 73.92%   [EVAL] batch:  122 | acc: 81.25%,  total acc: 73.98%   [EVAL] batch:  123 | acc: 81.25%,  total acc: 74.04%   [EVAL] batch:  124 | acc: 100.00%,  total acc: 74.25%   [EVAL] batch:  125 | acc: 25.00%,  total acc: 73.86%   [EVAL] batch:  126 | acc: 31.25%,  total acc: 73.52%   [EVAL] batch:  127 | acc: 25.00%,  total acc: 73.14%   [EVAL] batch:  128 | acc: 37.50%,  total acc: 72.87%   [EVAL] batch:  129 | acc: 18.75%,  total acc: 72.45%   [EVAL] batch:  130 | acc: 18.75%,  total acc: 72.04%   [EVAL] batch:  131 | acc: 62.50%,  total acc: 71.97%   [EVAL] batch:  132 | acc: 93.75%,  total acc: 72.13%   [EVAL] batch:  133 | acc: 87.50%,  total acc: 72.25%   [EVAL] batch:  134 | acc: 93.75%,  total acc: 72.41%   [EVAL] batch:  135 | acc: 75.00%,  total acc: 72.43%   [EVAL] batch:  136 | acc: 93.75%,  total acc: 72.58%   [EVAL] batch:  137 | acc: 56.25%,  total acc: 72.46%   [EVAL] batch:  138 | acc: 18.75%,  total acc: 72.08%   [EVAL] batch:  139 | acc: 37.50%,  total acc: 71.83%   [EVAL] batch:  140 | acc: 43.75%,  total acc: 71.63%   [EVAL] batch:  141 | acc: 25.00%,  total acc: 71.30%   [EVAL] batch:  142 | acc: 43.75%,  total acc: 71.11%   [EVAL] batch:  143 | acc: 56.25%,  total acc: 71.01%   [EVAL] batch:  144 | acc: 100.00%,  total acc: 71.21%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 71.40%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 71.56%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 71.75%   [EVAL] batch:  148 | acc: 93.75%,  total acc: 71.90%   [EVAL] batch:  149 | acc: 87.50%,  total acc: 72.00%   [EVAL] batch:  150 | acc: 87.50%,  total acc: 72.10%   [EVAL] batch:  151 | acc: 87.50%,  total acc: 72.20%   [EVAL] batch:  152 | acc: 75.00%,  total acc: 72.22%   [EVAL] batch:  153 | acc: 87.50%,  total acc: 72.32%   [EVAL] batch:  154 | acc: 81.25%,  total acc: 72.38%   [EVAL] batch:  155 | acc: 87.50%,  total acc: 72.48%   [EVAL] batch:  156 | acc: 93.75%,  total acc: 72.61%   [EVAL] batch:  157 | acc: 100.00%,  total acc: 72.78%   [EVAL] batch:  158 | acc: 93.75%,  total acc: 72.92%   [EVAL] batch:  159 | acc: 87.50%,  total acc: 73.01%   [EVAL] batch:  160 | acc: 93.75%,  total acc: 73.14%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 73.26%   [EVAL] batch:  162 | acc: 68.75%,  total acc: 73.24%   [EVAL] batch:  163 | acc: 50.00%,  total acc: 73.09%   [EVAL] batch:  164 | acc: 56.25%,  total acc: 72.99%   [EVAL] batch:  165 | acc: 37.50%,  total acc: 72.78%   [EVAL] batch:  166 | acc: 12.50%,  total acc: 72.42%   [EVAL] batch:  167 | acc: 31.25%,  total acc: 72.17%   [EVAL] batch:  168 | acc: 50.00%,  total acc: 72.04%   [EVAL] batch:  169 | acc: 100.00%,  total acc: 72.21%   [EVAL] batch:  170 | acc: 93.75%,  total acc: 72.33%   [EVAL] batch:  171 | acc: 100.00%,  total acc: 72.49%   [EVAL] batch:  172 | acc: 100.00%,  total acc: 72.65%   [EVAL] batch:  173 | acc: 93.75%,  total acc: 72.77%   [EVAL] batch:  174 | acc: 93.75%,  total acc: 72.89%   [EVAL] batch:  175 | acc: 37.50%,  total acc: 72.69%   [EVAL] batch:  176 | acc: 56.25%,  total acc: 72.60%   [EVAL] batch:  177 | acc: 50.00%,  total acc: 72.47%   [EVAL] batch:  178 | acc: 43.75%,  total acc: 72.31%   [EVAL] batch:  179 | acc: 62.50%,  total acc: 72.26%   [EVAL] batch:  180 | acc: 25.00%,  total acc: 72.00%   [EVAL] batch:  181 | acc: 56.25%,  total acc: 71.91%   [EVAL] batch:  182 | acc: 75.00%,  total acc: 71.93%   [EVAL] batch:  183 | acc: 56.25%,  total acc: 71.84%   [EVAL] batch:  184 | acc: 75.00%,  total acc: 71.86%   [EVAL] batch:  185 | acc: 81.25%,  total acc: 71.91%   [EVAL] batch:  186 | acc: 68.75%,  total acc: 71.89%   [EVAL] batch:  187 | acc: 56.25%,  total acc: 71.81%   [EVAL] batch:  188 | acc: 75.00%,  total acc: 71.83%   [EVAL] batch:  189 | acc: 68.75%,  total acc: 71.81%   [EVAL] batch:  190 | acc: 81.25%,  total acc: 71.86%   [EVAL] batch:  191 | acc: 62.50%,  total acc: 71.81%   [EVAL] batch:  192 | acc: 81.25%,  total acc: 71.86%   [EVAL] batch:  193 | acc: 81.25%,  total acc: 71.91%   [EVAL] batch:  194 | acc: 87.50%,  total acc: 71.99%   [EVAL] batch:  195 | acc: 62.50%,  total acc: 71.94%   [EVAL] batch:  196 | acc: 50.00%,  total acc: 71.83%   [EVAL] batch:  197 | acc: 75.00%,  total acc: 71.84%   [EVAL] batch:  198 | acc: 62.50%,  total acc: 71.80%   [EVAL] batch:  199 | acc: 81.25%,  total acc: 71.84%   [EVAL] batch:  200 | acc: 75.00%,  total acc: 71.86%   [EVAL] batch:  201 | acc: 81.25%,  total acc: 71.91%   [EVAL] batch:  202 | acc: 81.25%,  total acc: 71.95%   [EVAL] batch:  203 | acc: 87.50%,  total acc: 72.03%   [EVAL] batch:  204 | acc: 87.50%,  total acc: 72.10%   [EVAL] batch:  205 | acc: 93.75%,  total acc: 72.21%   [EVAL] batch:  206 | acc: 68.75%,  total acc: 72.19%   [EVAL] batch:  207 | acc: 43.75%,  total acc: 72.06%   [EVAL] batch:  208 | acc: 25.00%,  total acc: 71.83%   [EVAL] batch:  209 | acc: 56.25%,  total acc: 71.76%   [EVAL] batch:  210 | acc: 37.50%,  total acc: 71.59%   [EVAL] batch:  211 | acc: 56.25%,  total acc: 71.52%   [EVAL] batch:  212 | acc: 68.75%,  total acc: 71.51%   [EVAL] batch:  213 | acc: 93.75%,  total acc: 71.61%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 71.74%   [EVAL] batch:  215 | acc: 81.25%,  total acc: 71.79%   [EVAL] batch:  216 | acc: 93.75%,  total acc: 71.89%   [EVAL] batch:  217 | acc: 87.50%,  total acc: 71.96%   [EVAL] batch:  218 | acc: 87.50%,  total acc: 72.03%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 72.16%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 72.29%   [EVAL] batch:  221 | acc: 100.00%,  total acc: 72.41%   [EVAL] batch:  222 | acc: 100.00%,  total acc: 72.53%   [EVAL] batch:  223 | acc: 100.00%,  total acc: 72.66%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 72.78%   [EVAL] batch:  225 | acc: 93.75%,  total acc: 72.87%   [EVAL] batch:  226 | acc: 87.50%,  total acc: 72.94%   [EVAL] batch:  227 | acc: 75.00%,  total acc: 72.94%   [EVAL] batch:  228 | acc: 81.25%,  total acc: 72.98%   [EVAL] batch:  229 | acc: 100.00%,  total acc: 73.10%   [EVAL] batch:  230 | acc: 93.75%,  total acc: 73.19%   [EVAL] batch:  231 | acc: 56.25%,  total acc: 73.11%   [EVAL] batch:  232 | acc: 37.50%,  total acc: 72.96%   [EVAL] batch:  233 | acc: 56.25%,  total acc: 72.89%   [EVAL] batch:  234 | acc: 56.25%,  total acc: 72.82%   [EVAL] batch:  235 | acc: 62.50%,  total acc: 72.78%   [EVAL] batch:  236 | acc: 50.00%,  total acc: 72.68%   [EVAL] batch:  237 | acc: 87.50%,  total acc: 72.74%   [EVAL] batch:  238 | acc: 87.50%,  total acc: 72.80%   [EVAL] batch:  239 | acc: 100.00%,  total acc: 72.92%   [EVAL] batch:  240 | acc: 68.75%,  total acc: 72.90%   [EVAL] batch:  241 | acc: 93.75%,  total acc: 72.99%   [EVAL] batch:  242 | acc: 93.75%,  total acc: 73.07%   [EVAL] batch:  243 | acc: 68.75%,  total acc: 73.05%   [EVAL] batch:  244 | acc: 6.25%,  total acc: 72.78%   [EVAL] batch:  245 | acc: 12.50%,  total acc: 72.54%   [EVAL] batch:  246 | acc: 18.75%,  total acc: 72.32%   [EVAL] batch:  247 | acc: 12.50%,  total acc: 72.08%   [EVAL] batch:  248 | acc: 6.25%,  total acc: 71.81%   [EVAL] batch:  249 | acc: 18.75%,  total acc: 71.60%   [EVAL] batch:  250 | acc: 93.75%,  total acc: 71.69%   [EVAL] batch:  251 | acc: 93.75%,  total acc: 71.78%   [EVAL] batch:  252 | acc: 93.75%,  total acc: 71.86%   [EVAL] batch:  253 | acc: 87.50%,  total acc: 71.92%   [EVAL] batch:  254 | acc: 93.75%,  total acc: 72.01%   [EVAL] batch:  255 | acc: 100.00%,  total acc: 72.12%   [EVAL] batch:  256 | acc: 93.75%,  total acc: 72.20%   [EVAL] batch:  257 | acc: 93.75%,  total acc: 72.29%   [EVAL] batch:  258 | acc: 81.25%,  total acc: 72.32%   [EVAL] batch:  259 | acc: 75.00%,  total acc: 72.33%   [EVAL] batch:  260 | acc: 87.50%,  total acc: 72.39%   [EVAL] batch:  261 | acc: 75.00%,  total acc: 72.40%   [EVAL] batch:  262 | acc: 93.75%,  total acc: 72.48%   [EVAL] batch:  263 | acc: 81.25%,  total acc: 72.51%   [EVAL] batch:  264 | acc: 75.00%,  total acc: 72.52%   [EVAL] batch:  265 | acc: 50.00%,  total acc: 72.44%   [EVAL] batch:  266 | acc: 62.50%,  total acc: 72.40%   [EVAL] batch:  267 | acc: 68.75%,  total acc: 72.39%   [EVAL] batch:  268 | acc: 62.50%,  total acc: 72.35%   [EVAL] batch:  269 | acc: 68.75%,  total acc: 72.34%   [EVAL] batch:  270 | acc: 68.75%,  total acc: 72.32%   [EVAL] batch:  271 | acc: 56.25%,  total acc: 72.27%   [EVAL] batch:  272 | acc: 75.00%,  total acc: 72.28%   [EVAL] batch:  273 | acc: 75.00%,  total acc: 72.29%   [EVAL] batch:  274 | acc: 50.00%,  total acc: 72.20%   [EVAL] batch:  275 | acc: 100.00%,  total acc: 72.31%   [EVAL] batch:  276 | acc: 100.00%,  total acc: 72.41%   [EVAL] batch:  277 | acc: 100.00%,  total acc: 72.50%   [EVAL] batch:  278 | acc: 100.00%,  total acc: 72.60%   [EVAL] batch:  279 | acc: 100.00%,  total acc: 72.70%   [EVAL] batch:  280 | acc: 100.00%,  total acc: 72.80%   [EVAL] batch:  281 | acc: 87.50%,  total acc: 72.85%   [EVAL] batch:  282 | acc: 87.50%,  total acc: 72.90%   [EVAL] batch:  283 | acc: 75.00%,  total acc: 72.91%   [EVAL] batch:  284 | acc: 87.50%,  total acc: 72.96%   [EVAL] batch:  285 | acc: 68.75%,  total acc: 72.95%   [EVAL] batch:  286 | acc: 75.00%,  total acc: 72.95%   [EVAL] batch:  287 | acc: 75.00%,  total acc: 72.96%   [EVAL] batch:  288 | acc: 100.00%,  total acc: 73.05%   [EVAL] batch:  289 | acc: 100.00%,  total acc: 73.15%   [EVAL] batch:  290 | acc: 100.00%,  total acc: 73.24%   [EVAL] batch:  291 | acc: 100.00%,  total acc: 73.33%   [EVAL] batch:  292 | acc: 100.00%,  total acc: 73.42%   [EVAL] batch:  293 | acc: 93.75%,  total acc: 73.49%   [EVAL] batch:  294 | acc: 100.00%,  total acc: 73.58%   [EVAL] batch:  295 | acc: 100.00%,  total acc: 73.67%   [EVAL] batch:  296 | acc: 100.00%,  total acc: 73.76%   [EVAL] batch:  297 | acc: 100.00%,  total acc: 73.85%   [EVAL] batch:  298 | acc: 100.00%,  total acc: 73.93%   [EVAL] batch:  299 | acc: 100.00%,  total acc: 74.02%   [EVAL] batch:  300 | acc: 93.75%,  total acc: 74.09%   [EVAL] batch:  301 | acc: 93.75%,  total acc: 74.15%   [EVAL] batch:  302 | acc: 93.75%,  total acc: 74.22%   [EVAL] batch:  303 | acc: 93.75%,  total acc: 74.28%   [EVAL] batch:  304 | acc: 87.50%,  total acc: 74.32%   [EVAL] batch:  305 | acc: 93.75%,  total acc: 74.39%   [EVAL] batch:  306 | acc: 93.75%,  total acc: 74.45%   [EVAL] batch:  307 | acc: 81.25%,  total acc: 74.47%   [EVAL] batch:  308 | acc: 87.50%,  total acc: 74.51%   [EVAL] batch:  309 | acc: 93.75%,  total acc: 74.58%   [EVAL] batch:  310 | acc: 100.00%,  total acc: 74.66%   [EVAL] batch:  311 | acc: 81.25%,  total acc: 74.68%   [EVAL] batch:  312 | acc: 56.25%,  total acc: 74.62%   [EVAL] batch:  313 | acc: 43.75%,  total acc: 74.52%   [EVAL] batch:  314 | acc: 18.75%,  total acc: 74.35%   [EVAL] batch:  315 | acc: 43.75%,  total acc: 74.25%   [EVAL] batch:  316 | acc: 43.75%,  total acc: 74.15%   [EVAL] batch:  317 | acc: 50.00%,  total acc: 74.08%   [EVAL] batch:  318 | acc: 56.25%,  total acc: 74.02%   [EVAL] batch:  319 | acc: 100.00%,  total acc: 74.10%   [EVAL] batch:  320 | acc: 100.00%,  total acc: 74.18%   [EVAL] batch:  321 | acc: 81.25%,  total acc: 74.20%   [EVAL] batch:  322 | acc: 100.00%,  total acc: 74.28%   [EVAL] batch:  323 | acc: 93.75%,  total acc: 74.34%   [EVAL] batch:  324 | acc: 93.75%,  total acc: 74.40%   [EVAL] batch:  325 | acc: 75.00%,  total acc: 74.41%   [EVAL] batch:  326 | acc: 62.50%,  total acc: 74.37%   [EVAL] batch:  327 | acc: 75.00%,  total acc: 74.37%   [EVAL] batch:  328 | acc: 75.00%,  total acc: 74.37%   [EVAL] batch:  329 | acc: 81.25%,  total acc: 74.39%   [EVAL] batch:  330 | acc: 68.75%,  total acc: 74.38%   [EVAL] batch:  331 | acc: 56.25%,  total acc: 74.32%   [EVAL] batch:  332 | acc: 50.00%,  total acc: 74.25%   [EVAL] batch:  333 | acc: 37.50%,  total acc: 74.14%   [EVAL] batch:  334 | acc: 37.50%,  total acc: 74.03%   [EVAL] batch:  335 | acc: 50.00%,  total acc: 73.96%   [EVAL] batch:  336 | acc: 43.75%,  total acc: 73.87%   [EVAL] batch:  337 | acc: 75.00%,  total acc: 73.87%   [EVAL] batch:  338 | acc: 75.00%,  total acc: 73.88%   [EVAL] batch:  339 | acc: 75.00%,  total acc: 73.88%   [EVAL] batch:  340 | acc: 81.25%,  total acc: 73.90%   [EVAL] batch:  341 | acc: 75.00%,  total acc: 73.90%   [EVAL] batch:  342 | acc: 56.25%,  total acc: 73.85%   [EVAL] batch:  343 | acc: 37.50%,  total acc: 73.75%   [EVAL] batch:  344 | acc: 62.50%,  total acc: 73.71%   [EVAL] batch:  345 | acc: 50.00%,  total acc: 73.65%   [EVAL] batch:  346 | acc: 62.50%,  total acc: 73.61%   [EVAL] batch:  347 | acc: 62.50%,  total acc: 73.58%   [EVAL] batch:  348 | acc: 75.00%,  total acc: 73.59%   [EVAL] batch:  349 | acc: 68.75%,  total acc: 73.57%   [EVAL] batch:  350 | acc: 62.50%,  total acc: 73.54%   [EVAL] batch:  351 | acc: 68.75%,  total acc: 73.53%   [EVAL] batch:  352 | acc: 56.25%,  total acc: 73.48%   [EVAL] batch:  353 | acc: 75.00%,  total acc: 73.48%   [EVAL] batch:  354 | acc: 75.00%,  total acc: 73.49%   [EVAL] batch:  355 | acc: 62.50%,  total acc: 73.46%   [EVAL] batch:  356 | acc: 50.00%,  total acc: 73.39%   [EVAL] batch:  357 | acc: 31.25%,  total acc: 73.27%   [EVAL] batch:  358 | acc: 25.00%,  total acc: 73.14%   [EVAL] batch:  359 | acc: 43.75%,  total acc: 73.06%   [EVAL] batch:  360 | acc: 37.50%,  total acc: 72.96%   [EVAL] batch:  361 | acc: 50.00%,  total acc: 72.89%   [EVAL] batch:  362 | acc: 68.75%,  total acc: 72.88%   [EVAL] batch:  363 | acc: 100.00%,  total acc: 72.96%   [EVAL] batch:  364 | acc: 100.00%,  total acc: 73.03%   [EVAL] batch:  365 | acc: 100.00%,  total acc: 73.10%   [EVAL] batch:  366 | acc: 100.00%,  total acc: 73.18%   [EVAL] batch:  367 | acc: 100.00%,  total acc: 73.25%   [EVAL] batch:  368 | acc: 87.50%,  total acc: 73.29%   [EVAL] batch:  369 | acc: 75.00%,  total acc: 73.29%   [EVAL] batch:  370 | acc: 62.50%,  total acc: 73.26%   [EVAL] batch:  371 | acc: 68.75%,  total acc: 73.25%   [EVAL] batch:  372 | acc: 68.75%,  total acc: 73.24%   [EVAL] batch:  373 | acc: 68.75%,  total acc: 73.23%   [EVAL] batch:  374 | acc: 68.75%,  total acc: 73.22%   
cur_acc:  ['0.9454', '0.7996', '0.7718', '0.7341', '0.8571', '0.6518']
his_acc:  ['0.9454', '0.8715', '0.8095', '0.7552', '0.7558', '0.7322']
CurrentTrain: epoch 15, batch     0 | loss: 15.7844572CurrentTrain: epoch 15, batch     1 | loss: 21.3413824CurrentTrain: epoch 15, batch     2 | loss: 16.5468853CurrentTrain: epoch  1, batch     3 | loss: 11.2635624CurrentTrain: epoch 15, batch     0 | loss: 11.2133447CurrentTrain: epoch 15, batch     1 | loss: 10.8916608CurrentTrain: epoch 15, batch     2 | loss: 13.9274988CurrentTrain: epoch  1, batch     3 | loss: 6.3611540CurrentTrain: epoch 15, batch     0 | loss: 15.5049415CurrentTrain: epoch 15, batch     1 | loss: 9.2246909CurrentTrain: epoch 15, batch     2 | loss: 10.0688994CurrentTrain: epoch  1, batch     3 | loss: 9.5861801CurrentTrain: epoch 15, batch     0 | loss: 12.8812820CurrentTrain: epoch 15, batch     1 | loss: 14.6608157CurrentTrain: epoch 15, batch     2 | loss: 9.5597572CurrentTrain: epoch  1, batch     3 | loss: 6.2629372CurrentTrain: epoch 15, batch     0 | loss: 8.9979172CurrentTrain: epoch 15, batch     1 | loss: 9.5292847CurrentTrain: epoch 15, batch     2 | loss: 13.2669360CurrentTrain: epoch  1, batch     3 | loss: 8.2397776CurrentTrain: epoch 15, batch     0 | loss: 17.1049540CurrentTrain: epoch 15, batch     1 | loss: 8.1443219CurrentTrain: epoch 15, batch     2 | loss: 8.7265532CurrentTrain: epoch  1, batch     3 | loss: 6.5474413CurrentTrain: epoch 15, batch     0 | loss: 11.7900397CurrentTrain: epoch 15, batch     1 | loss: 11.3560124CurrentTrain: epoch 15, batch     2 | loss: 7.6143138CurrentTrain: epoch  1, batch     3 | loss: 7.2931191CurrentTrain: epoch 15, batch     0 | loss: 9.3436756CurrentTrain: epoch 15, batch     1 | loss: 7.5070014CurrentTrain: epoch 15, batch     2 | loss: 10.4088218CurrentTrain: epoch  1, batch     3 | loss: 7.6042134CurrentTrain: epoch 15, batch     0 | loss: 8.8589025CurrentTrain: epoch 15, batch     1 | loss: 9.9623936CurrentTrain: epoch 15, batch     2 | loss: 7.8248248CurrentTrain: epoch  1, batch     3 | loss: 6.5114558CurrentTrain: epoch 15, batch     0 | loss: 5.7055494CurrentTrain: epoch 15, batch     1 | loss: 11.5067748CurrentTrain: epoch 15, batch     2 | loss: 9.4533795CurrentTrain: epoch  1, batch     3 | loss: 6.6279363
#### contains administrative territorial entity ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "contains administrative territorial entity" means direct subdivisions of an administrative territorial entity.
Here is an example:
Relation: contains administrative territorial entity
Context: the cantons of créteil are administrative divisions of the val - de - marne department , île - de - france region , northern france .
Head Entity: île - de - france
Tail Entity: val - de - marne
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "contains administrative territorial entity":
Relation: contains administrative territorial entity  
Context: The provinces of Canada are the primary administrative divisions, with Ontario being one of the largest provinces in the country.  
Head Entity: Canada  
Tail Entity: Ontario  

Relation: contains administrative territorial entity  
Context: The state of California is divided into several counties, including Los Angeles County, which is known for its entertainment industry.  
Head Entity: California  
Tail Entity: Los Angeles County  
#### said to be the same as ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "said to be the same as" means this item is said to be the same as that item, but it's uncertain or disputed.
Here is an example:
Relation: said to be the same as
Context: enzo is an italian given name derivative of heinz , a form of henry .
Head Entity: henry
Tail Entity: heinz
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "said to be the same as":
Relation: said to be the same as  
Context: The two species, Canis lupus and Canis familiaris, are often said to be the same as each other due to their close genetic relationship.  
Head Entity: Canis lupus  
Tail Entity: Canis familiaris  

Relation: said to be the same as  
Context: In some circles, the terms "artificial intelligence" and "machine learning" are said to be the same as each other, although experts argue there are key differences.  
Head Entity: artificial intelligence  
Tail Entity: machine learning  
#### winner ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "winner" means winner of a competition or similar event.
Here is an example:
Relation: winner
Context: in 2010 , ty burrell received a nomination for the television critics association award for individual achievement in comedy , winning in 2011 .
Head Entity: television critics association award for individual achievement in comedy
Tail Entity: ty burrell
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "winner":
Relation: winner  
Context: In 2022, the Boston Red Sox triumphed in the World Series, marking their fourth championship in the last two decades.  
Head Entity: World Series  
Tail Entity: Boston Red Sox  

Relation: winner  
Context: At the annual film festival, the documentary "Voices of Change" was awarded the Best Documentary prize, highlighting its impactful storytelling.  
Head Entity: Best Documentary prize  
Tail Entity: "Voices of Change"  
#### military rank ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "military rank" means military rank achieved by a person, or military rank associated with a position.
Here is an example:
Relation: military rank
Context: the calais region was defended by the 15th army under " generaloberst " ( colonel general ) hans von salmuth , and normandy by the 7th army commanded by " generaloberst " friedrich dollmann .
Head Entity: hans von salmuth
Tail Entity: generaloberst
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "military rank":
Relation: military rank  
Context: during the battle of stalingrad, the soviet forces were led by general vasily chuikov, who held the rank of major general, while the german troops were commanded by general friedrich paulus.  
Head Entity: vasily chuikov  
Tail Entity: major general  

Relation: military rank  
Context: admiral horatio nelson was a key figure in the british navy, famously known for his rank of vice admiral during the napoleonic wars.  
Head Entity: horatio nelson  
Tail Entity: vice admiral  
#### publisher ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "publisher" means organization or person responsible for publishing books, periodicals, printed music, podcasts, games or software.
Here is an example:
Relation: publisher
Context: she went on to the film school at san francisco state university , when she was hired as an artist for an atari game called " electrocop " .
Head Entity: electrocop
Tail Entity: atari
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "publisher":
Relation: publisher  
Context: The renowned author released her latest novel through a well-known publishing house that has been in the industry for decades.  
Head Entity: latest novel  
Tail Entity: publishing house  

Relation: publisher  
Context: After years of hard work, the independent game developer finally secured a deal with a major publisher to distribute their new title worldwide.  
Head Entity: new title  
Tail Entity: major publisher  
#### followed by ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "followed by" means immediately following item in a series of which the subject is a part.
Here is an example:
Relation: followed by
Context: " muzai moratorium " and " shōso strip " were released on vinyl in 2008 , as a part of her 10-year anniversary celebrations .
Head Entity: muzai moratorium
Tail Entity: shōso strip
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "followed by":
Relation: followed by  
Context: The first chapter of the book introduces the main character, while the second chapter follows with a detailed backstory.  
Head Entity: first chapter  
Tail Entity: second chapter  

Relation: followed by  
Context: In the concert lineup, the opening act performed a lively set, followed by the headliner who captivated the audience with their hits.  
Head Entity: opening act  
Tail Entity: headliner  
#### work location ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "work location" means location where persons or organisations were actively participating in employment, business or other work.
Here is an example:
Relation: work location
Context: there is a memoir of his life and work by gustav queck ( jena , 1852 ) .
Head Entity: gustav queck
Tail Entity: jena
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "work location":
Relation: work location  
Context: the headquarters of the company is situated in san francisco, where they have been operating since 2010.  
Head Entity: the company  
Tail Entity: san francisco  

Relation: work location  
Context: during her time at the university, she conducted research in boston, contributing significantly to her field.  
Head Entity: she  
Tail Entity: boston  
#### occupation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "occupation" means occupation of a person.
Here is an example:
Relation: occupation
Context: andrei droznin ( russian : андрей дрознин ) is a russian theatre director and movement coach .
Head Entity: andrei droznin
Tail Entity: theatre director
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "occupation":
Relation: occupation  
Context: dr. emily thompson is a renowned biochemist known for her groundbreaking research in enzyme catalysis.  
Head Entity: dr. emily thompson  
Tail Entity: biochemist  

Relation: occupation  
Context: john smith, a former professional athlete, now works as a sports commentator for various television networks.  
Head Entity: john smith  
Tail Entity: sports commentator  
#### heritage designation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "heritage designation" means heritage designation of a cultural or natural site.
Here is an example:
Relation: heritage designation
Context: altamaha town , bluffton historic district , church of the cross , and rose hill plantation house are listed on the national register of historic places .
Head Entity: altamaha town
Tail Entity: national register of historic places
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "heritage designation":
Relation: heritage designation  
Context: The Great Wall of China is recognized as a UNESCO World Heritage Site due to its historical significance and architectural grandeur.  
Head Entity: Great Wall of China  
Tail Entity: UNESCO World Heritage Site  

Relation: heritage designation  
Context: The ancient city of Petra in Jordan is designated as a World Heritage Site, attracting tourists from around the globe.  
Head Entity: Petra  
Tail Entity: World Heritage Site  
#### voice type ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "voice type" means person's voice type.
Here is an example:
Relation: voice type
Context: beverly bower ( september 30 , 1925 – march 24 , 2002 ) was an american operatic soprano who had an active international opera career from the mid-1950s through the early 1970s .
Head Entity: beverly bower
Tail Entity: soprano
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "voice type":
Relation: voice type  
Context: luciano pavarotti ( october 12 , 1935 – september 6 , 2007 ) was an italian operatic tenor who gained worldwide fame for his televised concerts and media appearances.  
Head Entity: luciano pavarotti  
Tail Entity: tenor  

Relation: voice type  
Context: mariah carey ( born march 27 , 1969 ) is an american singer, songwriter, and actress known for her five-octave vocal range and her use of the whistle register.  
Head Entity: mariah carey  
Tail Entity: singer  
MemoryTrain:  epoch 15, batch     0 | loss: 2.8402293MemoryTrain:  epoch 15, batch     1 | loss: 3.7735165MemoryTrain:  epoch 15, batch     2 | loss: 4.2649499MemoryTrain:  epoch 15, batch     3 | loss: 3.4616073MemoryTrain:  epoch 15, batch     4 | loss: 4.2105795MemoryTrain:  epoch 15, batch     5 | loss: 4.3901147MemoryTrain:  epoch 15, batch     6 | loss: 3.7908740MemoryTrain:  epoch 15, batch     7 | loss: 3.5319825MemoryTrain:  epoch 15, batch     8 | loss: 3.6812518MemoryTrain:  epoch 15, batch     9 | loss: 5.0702809MemoryTrain:  epoch 15, batch    10 | loss: 2.4542314MemoryTrain:  epoch 15, batch    11 | loss: 4.0713321MemoryTrain:  epoch 15, batch    12 | loss: 5.5122336MemoryTrain:  epoch  1, batch    13 | loss: 6.4387704MemoryTrain:  epoch 15, batch     0 | loss: 2.8182817MemoryTrain:  epoch 15, batch     1 | loss: 3.3965534MemoryTrain:  epoch 15, batch     2 | loss: 2.3743800MemoryTrain:  epoch 15, batch     3 | loss: 3.5945085MemoryTrain:  epoch 15, batch     4 | loss: 2.9007225MemoryTrain:  epoch 15, batch     5 | loss: 1.8050772MemoryTrain:  epoch 15, batch     6 | loss: 9.5911881MemoryTrain:  epoch 15, batch     7 | loss: 1.9588378MemoryTrain:  epoch 15, batch     8 | loss: 2.8842475MemoryTrain:  epoch 15, batch     9 | loss: 2.1741863MemoryTrain:  epoch 15, batch    10 | loss: 3.7527623MemoryTrain:  epoch 15, batch    11 | loss: 2.6322432MemoryTrain:  epoch 15, batch    12 | loss: 4.4708598MemoryTrain:  epoch  1, batch    13 | loss: 7.1083792MemoryTrain:  epoch 15, batch     0 | loss: 2.2484052MemoryTrain:  epoch 15, batch     1 | loss: 6.1289819MemoryTrain:  epoch 15, batch     2 | loss: 1.7901458MemoryTrain:  epoch 15, batch     3 | loss: 2.0613770MemoryTrain:  epoch 15, batch     4 | loss: 2.0856835MemoryTrain:  epoch 15, batch     5 | loss: 3.5025473MemoryTrain:  epoch 15, batch     6 | loss: 2.3476436MemoryTrain:  epoch 15, batch     7 | loss: 2.2910811MemoryTrain:  epoch 15, batch     8 | loss: 2.8809236MemoryTrain:  epoch 15, batch     9 | loss: 2.7859695MemoryTrain:  epoch 15, batch    10 | loss: 1.9683164MemoryTrain:  epoch 15, batch    11 | loss: 5.3582576MemoryTrain:  epoch 15, batch    12 | loss: 2.6085829MemoryTrain:  epoch  1, batch    13 | loss: 6.6272810MemoryTrain:  epoch 15, batch     0 | loss: 1.7597604MemoryTrain:  epoch 15, batch     1 | loss: 5.2950864MemoryTrain:  epoch 15, batch     2 | loss: 2.0828828MemoryTrain:  epoch 15, batch     3 | loss: 2.4743610MemoryTrain:  epoch 15, batch     4 | loss: 1.5393408MemoryTrain:  epoch 15, batch     5 | loss: 2.5109701MemoryTrain:  epoch 15, batch     6 | loss: 2.7369624MemoryTrain:  epoch 15, batch     7 | loss: 1.7439779MemoryTrain:  epoch 15, batch     8 | loss: 1.7617428MemoryTrain:  epoch 15, batch     9 | loss: 2.1669705MemoryTrain:  epoch 15, batch    10 | loss: 2.4545015MemoryTrain:  epoch 15, batch    11 | loss: 2.3992728MemoryTrain:  epoch 15, batch    12 | loss: 1.5826293MemoryTrain:  epoch  1, batch    13 | loss: 5.2317168MemoryTrain:  epoch 15, batch     0 | loss: 1.8237777MemoryTrain:  epoch 15, batch     1 | loss: 1.4541186MemoryTrain:  epoch 15, batch     2 | loss: 1.8765826MemoryTrain:  epoch 15, batch     3 | loss: 2.6233122MemoryTrain:  epoch 15, batch     4 | loss: 1.5354006MemoryTrain:  epoch 15, batch     5 | loss: 5.0507153MemoryTrain:  epoch 15, batch     6 | loss: 1.8717320MemoryTrain:  epoch 15, batch     7 | loss: 1.8887035MemoryTrain:  epoch 15, batch     8 | loss: 2.2753676MemoryTrain:  epoch 15, batch     9 | loss: 2.2627479MemoryTrain:  epoch 15, batch    10 | loss: 1.9995261MemoryTrain:  epoch 15, batch    11 | loss: 3.2723439MemoryTrain:  epoch 15, batch    12 | loss: 2.8909995MemoryTrain:  epoch  1, batch    13 | loss: 6.4023993MemoryTrain:  epoch 15, batch     0 | loss: 1.8203246MemoryTrain:  epoch 15, batch     1 | loss: 4.1903779MemoryTrain:  epoch 15, batch     2 | loss: 1.7344897MemoryTrain:  epoch 15, batch     3 | loss: 1.4185416MemoryTrain:  epoch 15, batch     4 | loss: 2.2248196MemoryTrain:  epoch 15, batch     5 | loss: 1.9718348MemoryTrain:  epoch 15, batch     6 | loss: 2.5246824MemoryTrain:  epoch 15, batch     7 | loss: 1.5151338MemoryTrain:  epoch 15, batch     8 | loss: 1.7421395MemoryTrain:  epoch 15, batch     9 | loss: 2.4691758MemoryTrain:  epoch 15, batch    10 | loss: 1.7622736MemoryTrain:  epoch 15, batch    11 | loss: 2.1664902MemoryTrain:  epoch 15, batch    12 | loss: 4.2160022MemoryTrain:  epoch  1, batch    13 | loss: 5.8950923MemoryTrain:  epoch 15, batch     0 | loss: 4.1418699MemoryTrain:  epoch 15, batch     1 | loss: 1.3812798MemoryTrain:  epoch 15, batch     2 | loss: 2.9834076MemoryTrain:  epoch 15, batch     3 | loss: 4.2167408MemoryTrain:  epoch 15, batch     4 | loss: 1.3678424MemoryTrain:  epoch 15, batch     5 | loss: 1.4302529MemoryTrain:  epoch 15, batch     6 | loss: 1.5793135MemoryTrain:  epoch 15, batch     7 | loss: 2.6256632MemoryTrain:  epoch 15, batch     8 | loss: 2.2830974MemoryTrain:  epoch 15, batch     9 | loss: 1.9600242MemoryTrain:  epoch 15, batch    10 | loss: 2.1069085MemoryTrain:  epoch 15, batch    11 | loss: 1.4390234MemoryTrain:  epoch 15, batch    12 | loss: 4.4952423MemoryTrain:  epoch  1, batch    13 | loss: 5.2633178MemoryTrain:  epoch 15, batch     0 | loss: 1.3245550MemoryTrain:  epoch 15, batch     1 | loss: 2.0906459MemoryTrain:  epoch 15, batch     2 | loss: 1.7642197MemoryTrain:  epoch 15, batch     3 | loss: 2.6496131MemoryTrain:  epoch 15, batch     4 | loss: 1.7693715MemoryTrain:  epoch 15, batch     5 | loss: 1.5352108MemoryTrain:  epoch 15, batch     6 | loss: 1.5363850MemoryTrain:  epoch 15, batch     7 | loss: 2.1812460MemoryTrain:  epoch 15, batch     8 | loss: 1.7804181MemoryTrain:  epoch 15, batch     9 | loss: 2.2735623MemoryTrain:  epoch 15, batch    10 | loss: 1.7271304MemoryTrain:  epoch 15, batch    11 | loss: 3.0221356MemoryTrain:  epoch 15, batch    12 | loss: 4.2168697MemoryTrain:  epoch  1, batch    13 | loss: 6.3418158MemoryTrain:  epoch 15, batch     0 | loss: 1.3939132MemoryTrain:  epoch 15, batch     1 | loss: 1.8871950MemoryTrain:  epoch 15, batch     2 | loss: 1.7322039MemoryTrain:  epoch 15, batch     3 | loss: 3.1680337MemoryTrain:  epoch 15, batch     4 | loss: 1.4974217MemoryTrain:  epoch 15, batch     5 | loss: 1.4369453MemoryTrain:  epoch 15, batch     6 | loss: 1.4454938MemoryTrain:  epoch 15, batch     7 | loss: 1.6125346MemoryTrain:  epoch 15, batch     8 | loss: 1.8910283MemoryTrain:  epoch 15, batch     9 | loss: 1.8147537MemoryTrain:  epoch 15, batch    10 | loss: 2.1930224MemoryTrain:  epoch 15, batch    11 | loss: 1.6591863MemoryTrain:  epoch 15, batch    12 | loss: 2.1111490MemoryTrain:  epoch  1, batch    13 | loss: 5.7900335MemoryTrain:  epoch 15, batch     0 | loss: 2.0574692MemoryTrain:  epoch 15, batch     1 | loss: 1.6384731MemoryTrain:  epoch 15, batch     2 | loss: 1.7029840MemoryTrain:  epoch 15, batch     3 | loss: 4.2212513MemoryTrain:  epoch 15, batch     4 | loss: 4.5255374MemoryTrain:  epoch 15, batch     5 | loss: 1.8128750MemoryTrain:  epoch 15, batch     6 | loss: 1.3595420MemoryTrain:  epoch 15, batch     7 | loss: 1.8871742MemoryTrain:  epoch 15, batch     8 | loss: 2.4189024MemoryTrain:  epoch 15, batch     9 | loss: 1.4474879MemoryTrain:  epoch 15, batch    10 | loss: 2.9318107MemoryTrain:  epoch 15, batch    11 | loss: 1.4054026MemoryTrain:  epoch 15, batch    12 | loss: 1.4251593MemoryTrain:  epoch  1, batch    13 | loss: 5.4648416
[EVAL] batch:    0 | acc: 25.00%,  total acc: 25.00%   [EVAL] batch:    1 | acc: 18.75%,  total acc: 21.88%   [EVAL] batch:    2 | acc: 25.00%,  total acc: 22.92%   [EVAL] batch:    3 | acc: 25.00%,  total acc: 23.44%   [EVAL] batch:    4 | acc: 37.50%,  total acc: 26.25%   [EVAL] batch:    5 | acc: 37.50%,  total acc: 28.12%   [EVAL] batch:    6 | acc: 43.75%,  total acc: 30.36%   [EVAL] batch:    7 | acc: 68.75%,  total acc: 35.16%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 38.89%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 41.88%   [EVAL] batch:   10 | acc: 50.00%,  total acc: 42.61%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 45.31%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 48.08%   [EVAL] batch:   13 | acc: 62.50%,  total acc: 49.11%   [EVAL] batch:   14 | acc: 50.00%,  total acc: 49.17%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 49.61%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 51.10%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 52.08%   [EVAL] batch:   18 | acc: 81.25%,  total acc: 53.62%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 55.62%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 57.74%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 59.66%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 61.41%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 62.76%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 64.25%   [EVAL] batch:   25 | acc: 68.75%,  total acc: 64.42%   [EVAL] batch:   26 | acc: 75.00%,  total acc: 64.81%   [EVAL] batch:   27 | acc: 62.50%,  total acc: 64.73%   [EVAL] batch:   28 | acc: 75.00%,  total acc: 65.09%   [EVAL] batch:   29 | acc: 68.75%,  total acc: 65.21%   [EVAL] batch:   30 | acc: 75.00%,  total acc: 65.52%   [EVAL] batch:   31 | acc: 68.75%,  total acc: 65.62%   [EVAL] batch:   32 | acc: 62.50%,  total acc: 65.53%   [EVAL] batch:   33 | acc: 18.75%,  total acc: 64.15%   [EVAL] batch:   34 | acc: 43.75%,  total acc: 63.57%   [EVAL] batch:   35 | acc: 25.00%,  total acc: 62.50%   [EVAL] batch:   36 | acc: 25.00%,  total acc: 61.49%   [EVAL] batch:   37 | acc: 68.75%,  total acc: 61.68%   [EVAL] batch:   38 | acc: 87.50%,  total acc: 62.34%   [EVAL] batch:   39 | acc: 87.50%,  total acc: 62.97%   [EVAL] batch:   40 | acc: 81.25%,  total acc: 63.41%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 64.14%   [EVAL] batch:   42 | acc: 75.00%,  total acc: 64.39%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 65.06%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 65.56%   [EVAL] batch:   45 | acc: 68.75%,  total acc: 65.62%   [EVAL] batch:   46 | acc: 62.50%,  total acc: 65.56%   [EVAL] batch:   47 | acc: 62.50%,  total acc: 65.49%   [EVAL] batch:   48 | acc: 81.25%,  total acc: 65.82%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 66.25%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 66.91%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 67.55%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 68.16%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 68.75%   [EVAL] batch:   54 | acc: 100.00%,  total acc: 69.32%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 69.87%   [EVAL] batch:   56 | acc: 87.50%,  total acc: 70.18%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 70.47%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 70.76%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 71.15%   [EVAL] batch:   60 | acc: 81.25%,  total acc: 71.31%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 71.77%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 71.43%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 82.81%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 82.50%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    6 | acc: 37.50%,  total acc: 76.79%   [EVAL] batch:    7 | acc: 37.50%,  total acc: 71.88%   [EVAL] batch:    8 | acc: 56.25%,  total acc: 70.14%   [EVAL] batch:    9 | acc: 37.50%,  total acc: 66.88%   [EVAL] batch:   10 | acc: 31.25%,  total acc: 63.64%   [EVAL] batch:   11 | acc: 43.75%,  total acc: 61.98%   [EVAL] batch:   12 | acc: 56.25%,  total acc: 61.54%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 63.84%   [EVAL] batch:   14 | acc: 87.50%,  total acc: 65.42%   [EVAL] batch:   15 | acc: 93.75%,  total acc: 67.19%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 68.75%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 70.14%   [EVAL] batch:   18 | acc: 81.25%,  total acc: 70.72%   [EVAL] batch:   19 | acc: 56.25%,  total acc: 70.00%   [EVAL] batch:   20 | acc: 81.25%,  total acc: 70.54%   [EVAL] batch:   21 | acc: 68.75%,  total acc: 70.45%   [EVAL] batch:   22 | acc: 62.50%,  total acc: 70.11%   [EVAL] batch:   23 | acc: 81.25%,  total acc: 70.57%   [EVAL] batch:   24 | acc: 68.75%,  total acc: 70.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 71.63%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 72.69%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 73.66%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 74.35%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 75.21%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 76.01%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 76.56%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 77.27%   [EVAL] batch:   33 | acc: 62.50%,  total acc: 76.84%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 76.79%   [EVAL] batch:   35 | acc: 75.00%,  total acc: 76.74%   [EVAL] batch:   36 | acc: 87.50%,  total acc: 77.03%   [EVAL] batch:   37 | acc: 81.25%,  total acc: 77.14%   [EVAL] batch:   38 | acc: 93.75%,  total acc: 77.56%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 78.12%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 78.66%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 78.87%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 79.22%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 79.26%   [EVAL] batch:   44 | acc: 75.00%,  total acc: 79.17%   [EVAL] batch:   45 | acc: 68.75%,  total acc: 78.94%   [EVAL] batch:   46 | acc: 37.50%,  total acc: 78.06%   [EVAL] batch:   47 | acc: 62.50%,  total acc: 77.73%   [EVAL] batch:   48 | acc: 81.25%,  total acc: 77.81%   [EVAL] batch:   49 | acc: 43.75%,  total acc: 77.12%   [EVAL] batch:   50 | acc: 75.00%,  total acc: 77.08%   [EVAL] batch:   51 | acc: 68.75%,  total acc: 76.92%   [EVAL] batch:   52 | acc: 62.50%,  total acc: 76.65%   [EVAL] batch:   53 | acc: 56.25%,  total acc: 76.27%   [EVAL] batch:   54 | acc: 37.50%,  total acc: 75.57%   [EVAL] batch:   55 | acc: 68.75%,  total acc: 75.45%   [EVAL] batch:   56 | acc: 56.25%,  total acc: 75.11%   [EVAL] batch:   57 | acc: 37.50%,  total acc: 74.46%   [EVAL] batch:   58 | acc: 50.00%,  total acc: 74.05%   [EVAL] batch:   59 | acc: 75.00%,  total acc: 74.06%   [EVAL] batch:   60 | acc: 75.00%,  total acc: 74.08%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 73.99%   [EVAL] batch:   62 | acc: 68.75%,  total acc: 73.91%   [EVAL] batch:   63 | acc: 43.75%,  total acc: 73.44%   [EVAL] batch:   64 | acc: 43.75%,  total acc: 72.98%   [EVAL] batch:   65 | acc: 68.75%,  total acc: 72.92%   [EVAL] batch:   66 | acc: 62.50%,  total acc: 72.76%   [EVAL] batch:   67 | acc: 62.50%,  total acc: 72.61%   [EVAL] batch:   68 | acc: 62.50%,  total acc: 72.46%   [EVAL] batch:   69 | acc: 56.25%,  total acc: 72.23%   [EVAL] batch:   70 | acc: 50.00%,  total acc: 71.92%   [EVAL] batch:   71 | acc: 50.00%,  total acc: 71.61%   [EVAL] batch:   72 | acc: 62.50%,  total acc: 71.49%   [EVAL] batch:   73 | acc: 62.50%,  total acc: 71.37%   [EVAL] batch:   74 | acc: 50.00%,  total acc: 71.08%   [EVAL] batch:   75 | acc: 25.00%,  total acc: 70.48%   [EVAL] batch:   76 | acc: 18.75%,  total acc: 69.81%   [EVAL] batch:   77 | acc: 31.25%,  total acc: 69.31%   [EVAL] batch:   78 | acc: 37.50%,  total acc: 68.91%   [EVAL] batch:   79 | acc: 43.75%,  total acc: 68.59%   [EVAL] batch:   80 | acc: 6.25%,  total acc: 67.82%   [EVAL] batch:   81 | acc: 68.75%,  total acc: 67.84%   [EVAL] batch:   82 | acc: 100.00%,  total acc: 68.22%   [EVAL] batch:   83 | acc: 100.00%,  total acc: 68.60%   [EVAL] batch:   84 | acc: 93.75%,  total acc: 68.90%   [EVAL] batch:   85 | acc: 100.00%,  total acc: 69.26%   [EVAL] batch:   86 | acc: 93.75%,  total acc: 69.54%   [EVAL] batch:   87 | acc: 75.00%,  total acc: 69.60%   [EVAL] batch:   88 | acc: 12.50%,  total acc: 68.96%   [EVAL] batch:   89 | acc: 25.00%,  total acc: 68.47%   [EVAL] batch:   90 | acc: 25.00%,  total acc: 67.99%   [EVAL] batch:   91 | acc: 43.75%,  total acc: 67.73%   [EVAL] batch:   92 | acc: 25.00%,  total acc: 67.27%   [EVAL] batch:   93 | acc: 43.75%,  total acc: 67.02%   [EVAL] batch:   94 | acc: 93.75%,  total acc: 67.30%   [EVAL] batch:   95 | acc: 93.75%,  total acc: 67.58%   [EVAL] batch:   96 | acc: 93.75%,  total acc: 67.85%   [EVAL] batch:   97 | acc: 87.50%,  total acc: 68.05%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 68.37%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 68.69%   [EVAL] batch:  100 | acc: 56.25%,  total acc: 68.56%   [EVAL] batch:  101 | acc: 87.50%,  total acc: 68.75%   [EVAL] batch:  102 | acc: 56.25%,  total acc: 68.63%   [EVAL] batch:  103 | acc: 62.50%,  total acc: 68.57%   [EVAL] batch:  104 | acc: 68.75%,  total acc: 68.57%   [EVAL] batch:  105 | acc: 75.00%,  total acc: 68.63%   [EVAL] batch:  106 | acc: 81.25%,  total acc: 68.75%   [EVAL] batch:  107 | acc: 81.25%,  total acc: 68.87%   [EVAL] batch:  108 | acc: 81.25%,  total acc: 68.98%   [EVAL] batch:  109 | acc: 62.50%,  total acc: 68.92%   [EVAL] batch:  110 | acc: 56.25%,  total acc: 68.81%   [EVAL] batch:  111 | acc: 68.75%,  total acc: 68.81%   [EVAL] batch:  112 | acc: 81.25%,  total acc: 68.92%   [EVAL] batch:  113 | acc: 56.25%,  total acc: 68.80%   [EVAL] batch:  114 | acc: 50.00%,  total acc: 68.64%   [EVAL] batch:  115 | acc: 68.75%,  total acc: 68.64%   [EVAL] batch:  116 | acc: 62.50%,  total acc: 68.59%   [EVAL] batch:  117 | acc: 68.75%,  total acc: 68.59%   [EVAL] batch:  118 | acc: 43.75%,  total acc: 68.38%   [EVAL] batch:  119 | acc: 93.75%,  total acc: 68.59%   [EVAL] batch:  120 | acc: 93.75%,  total acc: 68.80%   [EVAL] batch:  121 | acc: 81.25%,  total acc: 68.90%   [EVAL] batch:  122 | acc: 87.50%,  total acc: 69.05%   [EVAL] batch:  123 | acc: 81.25%,  total acc: 69.15%   [EVAL] batch:  124 | acc: 100.00%,  total acc: 69.40%   [EVAL] batch:  125 | acc: 6.25%,  total acc: 68.90%   [EVAL] batch:  126 | acc: 12.50%,  total acc: 68.45%   [EVAL] batch:  127 | acc: 12.50%,  total acc: 68.02%   [EVAL] batch:  128 | acc: 0.00%,  total acc: 67.49%   [EVAL] batch:  129 | acc: 0.00%,  total acc: 66.97%   [EVAL] batch:  130 | acc: 0.00%,  total acc: 66.46%   [EVAL] batch:  131 | acc: 50.00%,  total acc: 66.34%   [EVAL] batch:  132 | acc: 93.75%,  total acc: 66.54%   [EVAL] batch:  133 | acc: 87.50%,  total acc: 66.70%   [EVAL] batch:  134 | acc: 87.50%,  total acc: 66.85%   [EVAL] batch:  135 | acc: 75.00%,  total acc: 66.91%   [EVAL] batch:  136 | acc: 93.75%,  total acc: 67.11%   [EVAL] batch:  137 | acc: 50.00%,  total acc: 66.98%   [EVAL] batch:  138 | acc: 25.00%,  total acc: 66.68%   [EVAL] batch:  139 | acc: 37.50%,  total acc: 66.47%   [EVAL] batch:  140 | acc: 43.75%,  total acc: 66.31%   [EVAL] batch:  141 | acc: 25.00%,  total acc: 66.02%   [EVAL] batch:  142 | acc: 43.75%,  total acc: 65.87%   [EVAL] batch:  143 | acc: 56.25%,  total acc: 65.80%   [EVAL] batch:  144 | acc: 100.00%,  total acc: 66.03%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 66.27%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 66.45%   [EVAL] batch:  147 | acc: 93.75%,  total acc: 66.64%   [EVAL] batch:  148 | acc: 93.75%,  total acc: 66.82%   [EVAL] batch:  149 | acc: 87.50%,  total acc: 66.96%   [EVAL] batch:  150 | acc: 87.50%,  total acc: 67.09%   [EVAL] batch:  151 | acc: 87.50%,  total acc: 67.23%   [EVAL] batch:  152 | acc: 75.00%,  total acc: 67.28%   [EVAL] batch:  153 | acc: 87.50%,  total acc: 67.41%   [EVAL] batch:  154 | acc: 81.25%,  total acc: 67.50%   [EVAL] batch:  155 | acc: 87.50%,  total acc: 67.63%   [EVAL] batch:  156 | acc: 93.75%,  total acc: 67.79%   [EVAL] batch:  157 | acc: 100.00%,  total acc: 68.00%   [EVAL] batch:  158 | acc: 93.75%,  total acc: 68.16%   [EVAL] batch:  159 | acc: 87.50%,  total acc: 68.28%   [EVAL] batch:  160 | acc: 93.75%,  total acc: 68.44%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 68.60%   [EVAL] batch:  162 | acc: 68.75%,  total acc: 68.60%   [EVAL] batch:  163 | acc: 43.75%,  total acc: 68.45%   [EVAL] batch:  164 | acc: 43.75%,  total acc: 68.30%   [EVAL] batch:  165 | acc: 37.50%,  total acc: 68.11%   [EVAL] batch:  166 | acc: 6.25%,  total acc: 67.74%   [EVAL] batch:  167 | acc: 31.25%,  total acc: 67.52%   [EVAL] batch:  168 | acc: 50.00%,  total acc: 67.42%   [EVAL] batch:  169 | acc: 81.25%,  total acc: 67.50%   [EVAL] batch:  170 | acc: 87.50%,  total acc: 67.62%   [EVAL] batch:  171 | acc: 93.75%,  total acc: 67.77%   [EVAL] batch:  172 | acc: 100.00%,  total acc: 67.96%   [EVAL] batch:  173 | acc: 62.50%,  total acc: 67.92%   [EVAL] batch:  174 | acc: 75.00%,  total acc: 67.96%   [EVAL] batch:  175 | acc: 50.00%,  total acc: 67.86%   [EVAL] batch:  176 | acc: 56.25%,  total acc: 67.80%   [EVAL] batch:  177 | acc: 62.50%,  total acc: 67.77%   [EVAL] batch:  178 | acc: 56.25%,  total acc: 67.70%   [EVAL] batch:  179 | acc: 68.75%,  total acc: 67.71%   [EVAL] batch:  180 | acc: 18.75%,  total acc: 67.44%   [EVAL] batch:  181 | acc: 50.00%,  total acc: 67.34%   [EVAL] batch:  182 | acc: 75.00%,  total acc: 67.38%   [EVAL] batch:  183 | acc: 56.25%,  total acc: 67.32%   [EVAL] batch:  184 | acc: 81.25%,  total acc: 67.40%   [EVAL] batch:  185 | acc: 68.75%,  total acc: 67.41%   [EVAL] batch:  186 | acc: 75.00%,  total acc: 67.45%   [EVAL] batch:  187 | acc: 75.00%,  total acc: 67.49%   [EVAL] batch:  188 | acc: 62.50%,  total acc: 67.46%   [EVAL] batch:  189 | acc: 62.50%,  total acc: 67.43%   [EVAL] batch:  190 | acc: 68.75%,  total acc: 67.44%   [EVAL] batch:  191 | acc: 75.00%,  total acc: 67.48%   [EVAL] batch:  192 | acc: 81.25%,  total acc: 67.55%   [EVAL] batch:  193 | acc: 81.25%,  total acc: 67.62%   [EVAL] batch:  194 | acc: 68.75%,  total acc: 67.63%   [EVAL] batch:  195 | acc: 68.75%,  total acc: 67.63%   [EVAL] batch:  196 | acc: 50.00%,  total acc: 67.54%   [EVAL] batch:  197 | acc: 75.00%,  total acc: 67.58%   [EVAL] batch:  198 | acc: 62.50%,  total acc: 67.56%   [EVAL] batch:  199 | acc: 75.00%,  total acc: 67.59%   [EVAL] batch:  200 | acc: 75.00%,  total acc: 67.63%   [EVAL] batch:  201 | acc: 81.25%,  total acc: 67.70%   [EVAL] batch:  202 | acc: 81.25%,  total acc: 67.76%   [EVAL] batch:  203 | acc: 93.75%,  total acc: 67.89%   [EVAL] batch:  204 | acc: 81.25%,  total acc: 67.96%   [EVAL] batch:  205 | acc: 93.75%,  total acc: 68.08%   [EVAL] batch:  206 | acc: 56.25%,  total acc: 68.03%   [EVAL] batch:  207 | acc: 37.50%,  total acc: 67.88%   [EVAL] batch:  208 | acc: 25.00%,  total acc: 67.67%   [EVAL] batch:  209 | acc: 43.75%,  total acc: 67.56%   [EVAL] batch:  210 | acc: 31.25%,  total acc: 67.39%   [EVAL] batch:  211 | acc: 50.00%,  total acc: 67.31%   [EVAL] batch:  212 | acc: 68.75%,  total acc: 67.31%   [EVAL] batch:  213 | acc: 93.75%,  total acc: 67.44%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 67.59%   [EVAL] batch:  215 | acc: 87.50%,  total acc: 67.68%   [EVAL] batch:  216 | acc: 93.75%,  total acc: 67.80%   [EVAL] batch:  217 | acc: 87.50%,  total acc: 67.89%   [EVAL] batch:  218 | acc: 87.50%,  total acc: 67.98%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 68.12%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 68.27%   [EVAL] batch:  221 | acc: 100.00%,  total acc: 68.41%   [EVAL] batch:  222 | acc: 100.00%,  total acc: 68.55%   [EVAL] batch:  223 | acc: 100.00%,  total acc: 68.69%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 68.83%   [EVAL] batch:  225 | acc: 93.75%,  total acc: 68.94%   [EVAL] batch:  226 | acc: 87.50%,  total acc: 69.03%   [EVAL] batch:  227 | acc: 62.50%,  total acc: 69.00%   [EVAL] batch:  228 | acc: 81.25%,  total acc: 69.05%   [EVAL] batch:  229 | acc: 100.00%,  total acc: 69.18%   [EVAL] batch:  230 | acc: 93.75%,  total acc: 69.29%   [EVAL] batch:  231 | acc: 56.25%,  total acc: 69.23%   [EVAL] batch:  232 | acc: 37.50%,  total acc: 69.10%   [EVAL] batch:  233 | acc: 50.00%,  total acc: 69.02%   [EVAL] batch:  234 | acc: 62.50%,  total acc: 68.99%   [EVAL] batch:  235 | acc: 62.50%,  total acc: 68.96%   [EVAL] batch:  236 | acc: 56.25%,  total acc: 68.91%   [EVAL] batch:  237 | acc: 87.50%,  total acc: 68.99%   [EVAL] batch:  238 | acc: 87.50%,  total acc: 69.06%   [EVAL] batch:  239 | acc: 100.00%,  total acc: 69.19%   [EVAL] batch:  240 | acc: 87.50%,  total acc: 69.27%   [EVAL] batch:  241 | acc: 87.50%,  total acc: 69.34%   [EVAL] batch:  242 | acc: 87.50%,  total acc: 69.42%   [EVAL] batch:  243 | acc: 68.75%,  total acc: 69.42%   [EVAL] batch:  244 | acc: 6.25%,  total acc: 69.16%   [EVAL] batch:  245 | acc: 12.50%,  total acc: 68.93%   [EVAL] batch:  246 | acc: 18.75%,  total acc: 68.72%   [EVAL] batch:  247 | acc: 12.50%,  total acc: 68.50%   [EVAL] batch:  248 | acc: 6.25%,  total acc: 68.25%   [EVAL] batch:  249 | acc: 18.75%,  total acc: 68.05%   [EVAL] batch:  250 | acc: 93.75%,  total acc: 68.15%   [EVAL] batch:  251 | acc: 93.75%,  total acc: 68.25%   [EVAL] batch:  252 | acc: 93.75%,  total acc: 68.35%   [EVAL] batch:  253 | acc: 87.50%,  total acc: 68.43%   [EVAL] batch:  254 | acc: 93.75%,  total acc: 68.53%   [EVAL] batch:  255 | acc: 100.00%,  total acc: 68.65%   [EVAL] batch:  256 | acc: 100.00%,  total acc: 68.77%   [EVAL] batch:  257 | acc: 93.75%,  total acc: 68.87%   [EVAL] batch:  258 | acc: 87.50%,  total acc: 68.94%   [EVAL] batch:  259 | acc: 75.00%,  total acc: 68.97%   [EVAL] batch:  260 | acc: 87.50%,  total acc: 69.04%   [EVAL] batch:  261 | acc: 81.25%,  total acc: 69.08%   [EVAL] batch:  262 | acc: 87.50%,  total acc: 69.15%   [EVAL] batch:  263 | acc: 81.25%,  total acc: 69.20%   [EVAL] batch:  264 | acc: 75.00%,  total acc: 69.22%   [EVAL] batch:  265 | acc: 56.25%,  total acc: 69.17%   [EVAL] batch:  266 | acc: 62.50%,  total acc: 69.15%   [EVAL] batch:  267 | acc: 68.75%,  total acc: 69.15%   [EVAL] batch:  268 | acc: 56.25%,  total acc: 69.10%   [EVAL] batch:  269 | acc: 31.25%,  total acc: 68.96%   [EVAL] batch:  270 | acc: 50.00%,  total acc: 68.89%   [EVAL] batch:  271 | acc: 31.25%,  total acc: 68.75%   [EVAL] batch:  272 | acc: 25.00%,  total acc: 68.59%   [EVAL] batch:  273 | acc: 31.25%,  total acc: 68.45%   [EVAL] batch:  274 | acc: 25.00%,  total acc: 68.30%   [EVAL] batch:  275 | acc: 100.00%,  total acc: 68.41%   [EVAL] batch:  276 | acc: 100.00%,  total acc: 68.52%   [EVAL] batch:  277 | acc: 100.00%,  total acc: 68.64%   [EVAL] batch:  278 | acc: 100.00%,  total acc: 68.75%   [EVAL] batch:  279 | acc: 100.00%,  total acc: 68.86%   [EVAL] batch:  280 | acc: 100.00%,  total acc: 68.97%   [EVAL] batch:  281 | acc: 68.75%,  total acc: 68.97%   [EVAL] batch:  282 | acc: 68.75%,  total acc: 68.97%   [EVAL] batch:  283 | acc: 62.50%,  total acc: 68.95%   [EVAL] batch:  284 | acc: 75.00%,  total acc: 68.97%   [EVAL] batch:  285 | acc: 62.50%,  total acc: 68.95%   [EVAL] batch:  286 | acc: 56.25%,  total acc: 68.90%   [EVAL] batch:  287 | acc: 68.75%,  total acc: 68.90%   [EVAL] batch:  288 | acc: 100.00%,  total acc: 69.01%   [EVAL] batch:  289 | acc: 100.00%,  total acc: 69.12%   [EVAL] batch:  290 | acc: 100.00%,  total acc: 69.22%   [EVAL] batch:  291 | acc: 100.00%,  total acc: 69.33%   [EVAL] batch:  292 | acc: 100.00%,  total acc: 69.43%   [EVAL] batch:  293 | acc: 93.75%,  total acc: 69.52%   [EVAL] batch:  294 | acc: 100.00%,  total acc: 69.62%   [EVAL] batch:  295 | acc: 100.00%,  total acc: 69.72%   [EVAL] batch:  296 | acc: 100.00%,  total acc: 69.82%   [EVAL] batch:  297 | acc: 100.00%,  total acc: 69.92%   [EVAL] batch:  298 | acc: 100.00%,  total acc: 70.03%   [EVAL] batch:  299 | acc: 100.00%,  total acc: 70.12%   [EVAL] batch:  300 | acc: 87.50%,  total acc: 70.18%   [EVAL] batch:  301 | acc: 93.75%,  total acc: 70.26%   [EVAL] batch:  302 | acc: 93.75%,  total acc: 70.34%   [EVAL] batch:  303 | acc: 93.75%,  total acc: 70.42%   [EVAL] batch:  304 | acc: 87.50%,  total acc: 70.47%   [EVAL] batch:  305 | acc: 87.50%,  total acc: 70.53%   [EVAL] batch:  306 | acc: 87.50%,  total acc: 70.58%   [EVAL] batch:  307 | acc: 75.00%,  total acc: 70.60%   [EVAL] batch:  308 | acc: 87.50%,  total acc: 70.65%   [EVAL] batch:  309 | acc: 93.75%,  total acc: 70.73%   [EVAL] batch:  310 | acc: 93.75%,  total acc: 70.80%   [EVAL] batch:  311 | acc: 75.00%,  total acc: 70.81%   [EVAL] batch:  312 | acc: 56.25%,  total acc: 70.77%   [EVAL] batch:  313 | acc: 50.00%,  total acc: 70.70%   [EVAL] batch:  314 | acc: 18.75%,  total acc: 70.54%   [EVAL] batch:  315 | acc: 43.75%,  total acc: 70.45%   [EVAL] batch:  316 | acc: 37.50%,  total acc: 70.35%   [EVAL] batch:  317 | acc: 56.25%,  total acc: 70.30%   [EVAL] batch:  318 | acc: 50.00%,  total acc: 70.24%   [EVAL] batch:  319 | acc: 100.00%,  total acc: 70.33%   [EVAL] batch:  320 | acc: 93.75%,  total acc: 70.40%   [EVAL] batch:  321 | acc: 75.00%,  total acc: 70.42%   [EVAL] batch:  322 | acc: 81.25%,  total acc: 70.45%   [EVAL] batch:  323 | acc: 87.50%,  total acc: 70.51%   [EVAL] batch:  324 | acc: 81.25%,  total acc: 70.54%   [EVAL] batch:  325 | acc: 75.00%,  total acc: 70.55%   [EVAL] batch:  326 | acc: 50.00%,  total acc: 70.49%   [EVAL] batch:  327 | acc: 68.75%,  total acc: 70.48%   [EVAL] batch:  328 | acc: 68.75%,  total acc: 70.48%   [EVAL] batch:  329 | acc: 81.25%,  total acc: 70.51%   [EVAL] batch:  330 | acc: 81.25%,  total acc: 70.54%   [EVAL] batch:  331 | acc: 43.75%,  total acc: 70.46%   [EVAL] batch:  332 | acc: 56.25%,  total acc: 70.42%   [EVAL] batch:  333 | acc: 43.75%,  total acc: 70.34%   [EVAL] batch:  334 | acc: 56.25%,  total acc: 70.30%   [EVAL] batch:  335 | acc: 56.25%,  total acc: 70.26%   [EVAL] batch:  336 | acc: 37.50%,  total acc: 70.16%   [EVAL] batch:  337 | acc: 75.00%,  total acc: 70.17%   [EVAL] batch:  338 | acc: 75.00%,  total acc: 70.19%   [EVAL] batch:  339 | acc: 81.25%,  total acc: 70.22%   [EVAL] batch:  340 | acc: 100.00%,  total acc: 70.31%   [EVAL] batch:  341 | acc: 75.00%,  total acc: 70.32%   [EVAL] batch:  342 | acc: 62.50%,  total acc: 70.30%   [EVAL] batch:  343 | acc: 37.50%,  total acc: 70.20%   [EVAL] batch:  344 | acc: 62.50%,  total acc: 70.18%   [EVAL] batch:  345 | acc: 50.00%,  total acc: 70.12%   [EVAL] batch:  346 | acc: 62.50%,  total acc: 70.10%   [EVAL] batch:  347 | acc: 68.75%,  total acc: 70.10%   [EVAL] batch:  348 | acc: 75.00%,  total acc: 70.11%   [EVAL] batch:  349 | acc: 62.50%,  total acc: 70.09%   [EVAL] batch:  350 | acc: 62.50%,  total acc: 70.07%   [EVAL] batch:  351 | acc: 68.75%,  total acc: 70.06%   [EVAL] batch:  352 | acc: 50.00%,  total acc: 70.01%   [EVAL] batch:  353 | acc: 62.50%,  total acc: 69.99%   [EVAL] batch:  354 | acc: 62.50%,  total acc: 69.96%   [EVAL] batch:  355 | acc: 56.25%,  total acc: 69.93%   [EVAL] batch:  356 | acc: 56.25%,  total acc: 69.89%   [EVAL] batch:  357 | acc: 31.25%,  total acc: 69.78%   [EVAL] batch:  358 | acc: 31.25%,  total acc: 69.67%   [EVAL] batch:  359 | acc: 50.00%,  total acc: 69.62%   [EVAL] batch:  360 | acc: 37.50%,  total acc: 69.53%   [EVAL] batch:  361 | acc: 50.00%,  total acc: 69.48%   [EVAL] batch:  362 | acc: 68.75%,  total acc: 69.47%   [EVAL] batch:  363 | acc: 100.00%,  total acc: 69.56%   [EVAL] batch:  364 | acc: 100.00%,  total acc: 69.64%   [EVAL] batch:  365 | acc: 100.00%,  total acc: 69.72%   [EVAL] batch:  366 | acc: 100.00%,  total acc: 69.81%   [EVAL] batch:  367 | acc: 100.00%,  total acc: 69.89%   [EVAL] batch:  368 | acc: 87.50%,  total acc: 69.94%   [EVAL] batch:  369 | acc: 68.75%,  total acc: 69.93%   [EVAL] batch:  370 | acc: 62.50%,  total acc: 69.91%   [EVAL] batch:  371 | acc: 56.25%,  total acc: 69.88%   [EVAL] batch:  372 | acc: 68.75%,  total acc: 69.87%   [EVAL] batch:  373 | acc: 43.75%,  total acc: 69.80%   [EVAL] batch:  374 | acc: 62.50%,  total acc: 69.78%   [EVAL] batch:  375 | acc: 25.00%,  total acc: 69.66%   [EVAL] batch:  376 | acc: 18.75%,  total acc: 69.53%   [EVAL] batch:  377 | acc: 25.00%,  total acc: 69.41%   [EVAL] batch:  378 | acc: 25.00%,  total acc: 69.29%   [EVAL] batch:  379 | acc: 37.50%,  total acc: 69.21%   [EVAL] batch:  380 | acc: 37.50%,  total acc: 69.13%   [EVAL] batch:  381 | acc: 43.75%,  total acc: 69.06%   [EVAL] batch:  382 | acc: 68.75%,  total acc: 69.06%   [EVAL] batch:  383 | acc: 68.75%,  total acc: 69.06%   [EVAL] batch:  384 | acc: 68.75%,  total acc: 69.06%   [EVAL] batch:  385 | acc: 50.00%,  total acc: 69.01%   [EVAL] batch:  386 | acc: 75.00%,  total acc: 69.02%   [EVAL] batch:  387 | acc: 81.25%,  total acc: 69.06%   [EVAL] batch:  388 | acc: 62.50%,  total acc: 69.04%   [EVAL] batch:  389 | acc: 50.00%,  total acc: 68.99%   [EVAL] batch:  390 | acc: 56.25%,  total acc: 68.96%   [EVAL] batch:  391 | acc: 75.00%,  total acc: 68.97%   [EVAL] batch:  392 | acc: 68.75%,  total acc: 68.97%   [EVAL] batch:  393 | acc: 81.25%,  total acc: 69.00%   [EVAL] batch:  394 | acc: 93.75%,  total acc: 69.07%   [EVAL] batch:  395 | acc: 100.00%,  total acc: 69.14%   [EVAL] batch:  396 | acc: 100.00%,  total acc: 69.22%   [EVAL] batch:  397 | acc: 100.00%,  total acc: 69.30%   [EVAL] batch:  398 | acc: 93.75%,  total acc: 69.36%   [EVAL] batch:  399 | acc: 100.00%,  total acc: 69.44%   [EVAL] batch:  400 | acc: 68.75%,  total acc: 69.44%   [EVAL] batch:  401 | acc: 75.00%,  total acc: 69.45%   [EVAL] batch:  402 | acc: 62.50%,  total acc: 69.43%   [EVAL] batch:  403 | acc: 75.00%,  total acc: 69.45%   [EVAL] batch:  404 | acc: 68.75%,  total acc: 69.44%   [EVAL] batch:  405 | acc: 75.00%,  total acc: 69.46%   [EVAL] batch:  406 | acc: 68.75%,  total acc: 69.46%   [EVAL] batch:  407 | acc: 62.50%,  total acc: 69.44%   [EVAL] batch:  408 | acc: 18.75%,  total acc: 69.32%   [EVAL] batch:  409 | acc: 43.75%,  total acc: 69.25%   [EVAL] batch:  410 | acc: 25.00%,  total acc: 69.15%   [EVAL] batch:  411 | acc: 25.00%,  total acc: 69.04%   [EVAL] batch:  412 | acc: 68.75%,  total acc: 69.04%   [EVAL] batch:  413 | acc: 87.50%,  total acc: 69.08%   [EVAL] batch:  414 | acc: 87.50%,  total acc: 69.13%   [EVAL] batch:  415 | acc: 81.25%,  total acc: 69.16%   [EVAL] batch:  416 | acc: 93.75%,  total acc: 69.21%   [EVAL] batch:  417 | acc: 75.00%,  total acc: 69.23%   [EVAL] batch:  418 | acc: 93.75%,  total acc: 69.29%   [EVAL] batch:  419 | acc: 87.50%,  total acc: 69.33%   [EVAL] batch:  420 | acc: 68.75%,  total acc: 69.33%   [EVAL] batch:  421 | acc: 62.50%,  total acc: 69.31%   [EVAL] batch:  422 | acc: 62.50%,  total acc: 69.30%   [EVAL] batch:  423 | acc: 81.25%,  total acc: 69.32%   [EVAL] batch:  424 | acc: 87.50%,  total acc: 69.37%   [EVAL] batch:  425 | acc: 100.00%,  total acc: 69.44%   [EVAL] batch:  426 | acc: 100.00%,  total acc: 69.51%   [EVAL] batch:  427 | acc: 100.00%,  total acc: 69.58%   [EVAL] batch:  428 | acc: 100.00%,  total acc: 69.65%   [EVAL] batch:  429 | acc: 100.00%,  total acc: 69.72%   [EVAL] batch:  430 | acc: 100.00%,  total acc: 69.79%   [EVAL] batch:  431 | acc: 87.50%,  total acc: 69.84%   [EVAL] batch:  432 | acc: 87.50%,  total acc: 69.88%   [EVAL] batch:  433 | acc: 87.50%,  total acc: 69.92%   [EVAL] batch:  434 | acc: 93.75%,  total acc: 69.97%   [EVAL] batch:  435 | acc: 81.25%,  total acc: 70.00%   [EVAL] batch:  436 | acc: 100.00%,  total acc: 70.07%   [EVAL] batch:  437 | acc: 50.00%,  total acc: 70.02%   
cur_acc:  ['0.9454', '0.7996', '0.7718', '0.7341', '0.8571', '0.6518', '0.7143']
his_acc:  ['0.9454', '0.8715', '0.8095', '0.7552', '0.7558', '0.7322', '0.7002']
CurrentTrain: epoch 15, batch     0 | loss: 18.5255249CurrentTrain: epoch 15, batch     1 | loss: 14.8810750CurrentTrain: epoch 15, batch     2 | loss: 30.9763543CurrentTrain: epoch  1, batch     3 | loss: 13.9965417CurrentTrain: epoch 15, batch     0 | loss: 18.9692544CurrentTrain: epoch 15, batch     1 | loss: 17.5673145CurrentTrain: epoch 15, batch     2 | loss: 18.4265721CurrentTrain: epoch  1, batch     3 | loss: 9.7245442CurrentTrain: epoch 15, batch     0 | loss: 12.9100044CurrentTrain: epoch 15, batch     1 | loss: 16.8717553CurrentTrain: epoch 15, batch     2 | loss: 19.7361227CurrentTrain: epoch  1, batch     3 | loss: 7.9140072CurrentTrain: epoch 15, batch     0 | loss: 11.7182595CurrentTrain: epoch 15, batch     1 | loss: 19.4887243CurrentTrain: epoch 15, batch     2 | loss: 12.7416638CurrentTrain: epoch  1, batch     3 | loss: 7.6420331CurrentTrain: epoch 15, batch     0 | loss: 11.7342231CurrentTrain: epoch 15, batch     1 | loss: 18.1925670CurrentTrain: epoch 15, batch     2 | loss: 14.3008110CurrentTrain: epoch  1, batch     3 | loss: 8.3992714CurrentTrain: epoch 15, batch     0 | loss: 11.9467059CurrentTrain: epoch 15, batch     1 | loss: 10.7185614CurrentTrain: epoch 15, batch     2 | loss: 17.8518406CurrentTrain: epoch  1, batch     3 | loss: 8.8138436CurrentTrain: epoch 15, batch     0 | loss: 13.1160219CurrentTrain: epoch 15, batch     1 | loss: 13.1969078CurrentTrain: epoch 15, batch     2 | loss: 10.5945679CurrentTrain: epoch  1, batch     3 | loss: 7.8129089CurrentTrain: epoch 15, batch     0 | loss: 17.3497084CurrentTrain: epoch 15, batch     1 | loss: 17.8029097CurrentTrain: epoch 15, batch     2 | loss: 7.9318698CurrentTrain: epoch  1, batch     3 | loss: 7.0149901CurrentTrain: epoch 15, batch     0 | loss: 11.0140654CurrentTrain: epoch 15, batch     1 | loss: 11.7397422CurrentTrain: epoch 15, batch     2 | loss: 10.5074612CurrentTrain: epoch  1, batch     3 | loss: 5.6924725CurrentTrain: epoch 15, batch     0 | loss: 12.8214795CurrentTrain: epoch 15, batch     1 | loss: 11.6612198CurrentTrain: epoch 15, batch     2 | loss: 10.9630351CurrentTrain: epoch  1, batch     3 | loss: 8.4336233
#### located on terrain feature ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "located on terrain feature" means located on the specified physical/geophysical feature.
Here is an example:
Relation: located on terrain feature
Context: it is situated in the historic lower lusatia region , about south of beeskow , and north of cottbus .
Head Entity: cottbus
Tail Entity: lower lusatia
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "located on terrain feature":
Relation: located on terrain feature  
Context: The ancient ruins are found on the slopes of the majestic Andes mountains, providing a breathtaking view of the valley below.  
Head Entity: Andes mountains  
Tail Entity: ancient ruins  

Relation: located on terrain feature  
Context: The village is nestled in the heart of the lush Black Forest, surrounded by towering trees and serene lakes.  
Head Entity: Black Forest  
Tail Entity: village  
#### participant of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "participant of" means event in which a person or organization was/is a participant.
Here is an example:
Relation: participant of
Context: tiago venâncio ( born august 19 , 1987 , setúbal ) is a freestyle swimmer from portugal , who represented his home country at the 2004 summer olympics in athens , greece .
Head Entity: tiago venâncio
Tail Entity: 2004 summer olympics
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "participant of":
Relation: participant of  
Context: the united nations climate change conference, held in paris in 2015, saw numerous world leaders and activists come together to discuss global warming.  
Head Entity: united nations climate change conference  
Tail Entity: paris agreement  

Relation: participant of  
Context: the 2022 fifa world cup in qatar featured teams from around the globe competing for the prestigious trophy.  
Head Entity: 2022 fifa world cup  
Tail Entity: qatar
#### director ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "director" means director of film, TV-series, stageplay, video game or similar.
Here is an example:
Relation: director
Context: " runaway reptar " was written by ali marie matheson and jon cooksey and directed by john holmquist and jim duffy .
Head Entity: runaway reptar
Tail Entity: john holmquist
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "director":
Relation: director  
Context: "Inception" was directed by Christopher Nolan and features a complex narrative structure that keeps viewers on the edge of their seats.  
Head Entity: Inception  
Tail Entity: Christopher Nolan  

Relation: director  
Context: The acclaimed stage play "Hamlet" was directed by Kenneth Branagh, who brought a fresh perspective to the classic Shakespearean tale.  
Head Entity: Hamlet  
Tail Entity: Kenneth Branagh  
#### owned by ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "owned by" means owner of the subject.
Here is an example:
Relation: owned by
Context: the station is owned by the hearst television subsidiary of the hearst corporation .
Head Entity: hearst television
Tail Entity: hearst corporation
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "owned by":
Relation: owned by  
Context: the famous painting is owned by the metropolitan museum of art in new york.  
Head Entity: metropolitan museum of art  
Tail Entity: famous painting  

Relation: owned by  
Context: the luxury car brand is owned by a major automotive conglomerate based in germany.  
Head Entity: major automotive conglomerate  
Tail Entity: luxury car brand  
#### part of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "part of" means object of which the subject is a part.
Here is an example:
Relation: part of
Context: thessaly was home to extensive neolithic and chalcolithic cultures around 6000 bc–2500 bc ( see cardium pottery , dimini and sesklo ) .
Head Entity: cardium pottery
Tail Entity: neolithic
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "part of":
Relation: part of  
Context: The engine is a crucial part of the car's overall functionality, providing the necessary power to drive.  
Head Entity: engine  
Tail Entity: car  

Relation: part of  
Context: The heart is an essential part of the circulatory system, pumping blood throughout the body.  
Head Entity: heart  
Tail Entity: circulatory system  
#### occupant ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "occupant" means person or organization occupying property.
Here is an example:
Relation: occupant
Context: it served as the home of k league side gyeongnam fc and national league side changwon city fc prior to the construction of the changwon football center in 2009 .
Head Entity: changwon football center
Tail Entity: gyeongnam fc
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "occupant":
Relation: occupant  
Context: The historic building was once the residence of the famous artist, and it now serves as a gallery showcasing his works, attracting visitors from all over the country.  
Head Entity: historic building  
Tail Entity: famous artist  

Relation: occupant  
Context: After the renovation, the old factory was transformed into a vibrant co-working space, where numerous startups and freelancers now operate.  
Head Entity: old factory  
Tail Entity: startups
#### architect ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "architect" means person or architectural firm responsible for designing this building.
Here is an example:
Relation: architect
Context: he also commissioned renowned architect richard neutra to design the iconic desert kaufmann house ( 1946 ) in palm springs , california .
Head Entity: kaufmann house
Tail Entity: richard neutra
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "architect":
Relation: architect  
Context: the innovative design of the new city library was created by the famous architect zaha hadid, known for her futuristic structures.  
Head Entity: city library  
Tail Entity: zaha hadid  

Relation: architect  
Context: after years of planning, the historic renovation of the old courthouse was finally completed, thanks to the talented architect frank gehry.  
Head Entity: old courthouse  
Tail Entity: frank gehry  
#### residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "residence" means the place where the person is or has been, resident.
Here is an example:
Relation: residence
Context: in 2001 brian haw set up the parliament square peace campaign outside the houses of parliament in london .
Head Entity: brian haw
Tail Entity: london
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "residence":
Relation: residence  
Context: After moving to New York City in 2010, Sarah found her dream apartment in Brooklyn.  
Head Entity: Sarah  
Tail Entity: Brooklyn  

Relation: residence  
Context: The famous author lived in a quaint cottage in the countryside for many years before relocating to the city.  
Head Entity: The famous author  
Tail Entity: the city  
#### composer ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "composer" means person who wrote the music.
Here is an example:
Relation: composer
Context: at a young age he was an accomplished cellist , performing at age seventeen the solo part of richard strauss 's " don quixote " .
Head Entity: don quixote
Tail Entity: richard strauss
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "composer":
Relation: composer  
Context: The renowned composer Ludwig van Beethoven created some of the most celebrated symphonies in classical music history.  
Head Entity: symphonies  
Tail Entity: Ludwig van Beethoven  

Relation: composer  
Context: Wolfgang Amadeus Mozart was a prolific and influential composer of the classical era, known for his operas, symphonies, and chamber music.  
Head Entity: operas  
Tail Entity: Wolfgang Amadeus Mozart  
#### location ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "location" means location of the object, structure or event.
Here is an example:
Relation: location
Context: another attraction is engelsberg ironworks ( ) , an ironworks in ängelsberg .
Head Entity: engelsberg ironworks
Tail Entity: ängelsberg
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "location":
Relation: location  
Context: the famous landmark is the eiffel tower ( ) , located in the heart of paris .  
Head Entity: eiffel tower  
Tail Entity: paris  

Relation: location  
Context: the annual festival takes place at central park ( ) , a large public park in new york city .  
Head Entity: central park  
Tail Entity: new york city  
MemoryTrain:  epoch 15, batch     0 | loss: 3.5500237MemoryTrain:  epoch 15, batch     1 | loss: 6.8772578MemoryTrain:  epoch 15, batch     2 | loss: 3.0256713MemoryTrain:  epoch 15, batch     3 | loss: 4.7272957MemoryTrain:  epoch 15, batch     4 | loss: 2.7107497MemoryTrain:  epoch 15, batch     5 | loss: 2.3407518MemoryTrain:  epoch 15, batch     6 | loss: 2.0001919MemoryTrain:  epoch 15, batch     7 | loss: 2.6974684MemoryTrain:  epoch 15, batch     8 | loss: 4.0851167MemoryTrain:  epoch 15, batch     9 | loss: 3.5137801MemoryTrain:  epoch 15, batch    10 | loss: 2.3033727MemoryTrain:  epoch 15, batch    11 | loss: 3.9518913MemoryTrain:  epoch 15, batch    12 | loss: 3.6442057MemoryTrain:  epoch 15, batch    13 | loss: 3.3484688MemoryTrain:  epoch 15, batch    14 | loss: 2.4591748MemoryTrain:  epoch 15, batch     0 | loss: 3.7754531MemoryTrain:  epoch 15, batch     1 | loss: 7.6635676MemoryTrain:  epoch 15, batch     2 | loss: 3.2935590MemoryTrain:  epoch 15, batch     3 | loss: 3.5675558MemoryTrain:  epoch 15, batch     4 | loss: 3.4935383MemoryTrain:  epoch 15, batch     5 | loss: 2.1796984MemoryTrain:  epoch 15, batch     6 | loss: 2.7131182MemoryTrain:  epoch 15, batch     7 | loss: 3.0401794MemoryTrain:  epoch 15, batch     8 | loss: 2.2349292MemoryTrain:  epoch 15, batch     9 | loss: 2.0962552MemoryTrain:  epoch 15, batch    10 | loss: 3.1580385MemoryTrain:  epoch 15, batch    11 | loss: 5.1289133MemoryTrain:  epoch 15, batch    12 | loss: 2.0554441MemoryTrain:  epoch 15, batch    13 | loss: 3.4867309MemoryTrain:  epoch 15, batch    14 | loss: 1.9493471MemoryTrain:  epoch 15, batch     0 | loss: 3.6060661MemoryTrain:  epoch 15, batch     1 | loss: 3.1246752MemoryTrain:  epoch 15, batch     2 | loss: 3.9531946MemoryTrain:  epoch 15, batch     3 | loss: 2.2932206MemoryTrain:  epoch 15, batch     4 | loss: 2.4163087MemoryTrain:  epoch 15, batch     5 | loss: 2.3430770MemoryTrain:  epoch 15, batch     6 | loss: 2.2072865MemoryTrain:  epoch 15, batch     7 | loss: 2.0322404MemoryTrain:  epoch 15, batch     8 | loss: 2.8625244MemoryTrain:  epoch 15, batch     9 | loss: 3.3705381MemoryTrain:  epoch 15, batch    10 | loss: 3.6540222MemoryTrain:  epoch 15, batch    11 | loss: 4.5095076MemoryTrain:  epoch 15, batch    12 | loss: 2.7060516MemoryTrain:  epoch 15, batch    13 | loss: 4.3241233MemoryTrain:  epoch 15, batch    14 | loss: 3.3769864MemoryTrain:  epoch 15, batch     0 | loss: 2.4537535MemoryTrain:  epoch 15, batch     1 | loss: 1.7598728MemoryTrain:  epoch 15, batch     2 | loss: 1.4696688MemoryTrain:  epoch 15, batch     3 | loss: 1.6847393MemoryTrain:  epoch 15, batch     4 | loss: 1.8517761MemoryTrain:  epoch 15, batch     5 | loss: 3.3625673MemoryTrain:  epoch 15, batch     6 | loss: 2.8124888MemoryTrain:  epoch 15, batch     7 | loss: 3.7959568MemoryTrain:  epoch 15, batch     8 | loss: 2.8123715MemoryTrain:  epoch 15, batch     9 | loss: 2.4043613MemoryTrain:  epoch 15, batch    10 | loss: 3.2441037MemoryTrain:  epoch 15, batch    11 | loss: 5.5621822MemoryTrain:  epoch 15, batch    12 | loss: 1.9733913MemoryTrain:  epoch 15, batch    13 | loss: 2.0730318MemoryTrain:  epoch 15, batch    14 | loss: 2.0943205MemoryTrain:  epoch 15, batch     0 | loss: 2.8270793MemoryTrain:  epoch 15, batch     1 | loss: 3.9868119MemoryTrain:  epoch 15, batch     2 | loss: 2.8039470MemoryTrain:  epoch 15, batch     3 | loss: 1.6775705MemoryTrain:  epoch 15, batch     4 | loss: 1.8172622MemoryTrain:  epoch 15, batch     5 | loss: 1.5408098MemoryTrain:  epoch 15, batch     6 | loss: 1.7955104MemoryTrain:  epoch 15, batch     7 | loss: 2.3348309MemoryTrain:  epoch 15, batch     8 | loss: 2.4139062MemoryTrain:  epoch 15, batch     9 | loss: 1.9378284MemoryTrain:  epoch 15, batch    10 | loss: 2.4134300MemoryTrain:  epoch 15, batch    11 | loss: 1.9202256MemoryTrain:  epoch 15, batch    12 | loss: 2.0340866MemoryTrain:  epoch 15, batch    13 | loss: 1.8046008MemoryTrain:  epoch 15, batch    14 | loss: 2.2418746MemoryTrain:  epoch 15, batch     0 | loss: 1.7729594MemoryTrain:  epoch 15, batch     1 | loss: 3.2525116MemoryTrain:  epoch 15, batch     2 | loss: 1.5780469MemoryTrain:  epoch 15, batch     3 | loss: 1.6286957MemoryTrain:  epoch 15, batch     4 | loss: 4.0877218MemoryTrain:  epoch 15, batch     5 | loss: 2.2507973MemoryTrain:  epoch 15, batch     6 | loss: 1.9500984MemoryTrain:  epoch 15, batch     7 | loss: 2.8500762MemoryTrain:  epoch 15, batch     8 | loss: 2.4393498MemoryTrain:  epoch 15, batch     9 | loss: 2.2534232MemoryTrain:  epoch 15, batch    10 | loss: 1.3543437MemoryTrain:  epoch 15, batch    11 | loss: 4.6741454MemoryTrain:  epoch 15, batch    12 | loss: 1.5287575MemoryTrain:  epoch 15, batch    13 | loss: 4.8600472MemoryTrain:  epoch 15, batch    14 | loss: 1.6104620MemoryTrain:  epoch 15, batch     0 | loss: 1.8393870MemoryTrain:  epoch 15, batch     1 | loss: 1.4870857MemoryTrain:  epoch 15, batch     2 | loss: 3.8288230MemoryTrain:  epoch 15, batch     3 | loss: 3.8878013MemoryTrain:  epoch 15, batch     4 | loss: 1.7453283MemoryTrain:  epoch 15, batch     5 | loss: 1.3661416MemoryTrain:  epoch 15, batch     6 | loss: 1.4732853MemoryTrain:  epoch 15, batch     7 | loss: 1.8801615MemoryTrain:  epoch 15, batch     8 | loss: 1.8639011MemoryTrain:  epoch 15, batch     9 | loss: 2.7288197MemoryTrain:  epoch 15, batch    10 | loss: 1.8541031MemoryTrain:  epoch 15, batch    11 | loss: 4.3820678MemoryTrain:  epoch 15, batch    12 | loss: 1.7136423MemoryTrain:  epoch 15, batch    13 | loss: 2.1384170MemoryTrain:  epoch 15, batch    14 | loss: 2.1955127MemoryTrain:  epoch 15, batch     0 | loss: 2.6858974MemoryTrain:  epoch 15, batch     1 | loss: 1.7672628MemoryTrain:  epoch 15, batch     2 | loss: 1.4243397MemoryTrain:  epoch 15, batch     3 | loss: 1.7115380MemoryTrain:  epoch 15, batch     4 | loss: 1.9888241MemoryTrain:  epoch 15, batch     5 | loss: 4.3252102MemoryTrain:  epoch 15, batch     6 | loss: 3.6454935MemoryTrain:  epoch 15, batch     7 | loss: 1.7398369MemoryTrain:  epoch 15, batch     8 | loss: 1.2440237MemoryTrain:  epoch 15, batch     9 | loss: 1.4868740MemoryTrain:  epoch 15, batch    10 | loss: 3.1743056MemoryTrain:  epoch 15, batch    11 | loss: 1.9959224MemoryTrain:  epoch 15, batch    12 | loss: 1.5484520MemoryTrain:  epoch 15, batch    13 | loss: 1.9427298MemoryTrain:  epoch 15, batch    14 | loss: 2.2607748MemoryTrain:  epoch 15, batch     0 | loss: 1.3020246MemoryTrain:  epoch 15, batch     1 | loss: 2.1304263MemoryTrain:  epoch 15, batch     2 | loss: 1.4089708MemoryTrain:  epoch 15, batch     3 | loss: 2.1798900MemoryTrain:  epoch 15, batch     4 | loss: 1.6491260MemoryTrain:  epoch 15, batch     5 | loss: 2.1760575MemoryTrain:  epoch 15, batch     6 | loss: 2.7432744MemoryTrain:  epoch 15, batch     7 | loss: 2.2647299MemoryTrain:  epoch 15, batch     8 | loss: 1.6906295MemoryTrain:  epoch 15, batch     9 | loss: 1.6204300MemoryTrain:  epoch 15, batch    10 | loss: 1.4481357MemoryTrain:  epoch 15, batch    11 | loss: 1.5412524MemoryTrain:  epoch 15, batch    12 | loss: 2.7030082MemoryTrain:  epoch 15, batch    13 | loss: 1.4884994MemoryTrain:  epoch 15, batch    14 | loss: 1.8685102MemoryTrain:  epoch 15, batch     0 | loss: 1.7948594MemoryTrain:  epoch 15, batch     1 | loss: 1.6946372MemoryTrain:  epoch 15, batch     2 | loss: 2.0749646MemoryTrain:  epoch 15, batch     3 | loss: 2.7540572MemoryTrain:  epoch 15, batch     4 | loss: 4.3058087MemoryTrain:  epoch 15, batch     5 | loss: 1.7566237MemoryTrain:  epoch 15, batch     6 | loss: 3.7553327MemoryTrain:  epoch 15, batch     7 | loss: 4.1992819MemoryTrain:  epoch 15, batch     8 | loss: 2.8248596MemoryTrain:  epoch 15, batch     9 | loss: 1.9443588MemoryTrain:  epoch 15, batch    10 | loss: 4.1627941MemoryTrain:  epoch 15, batch    11 | loss: 4.3873456MemoryTrain:  epoch 15, batch    12 | loss: 1.7015233MemoryTrain:  epoch 15, batch    13 | loss: 2.3525622MemoryTrain:  epoch 15, batch    14 | loss: 1.7405825
[EVAL] batch:    0 | acc: 25.00%,  total acc: 25.00%   [EVAL] batch:    1 | acc: 37.50%,  total acc: 31.25%   [EVAL] batch:    2 | acc: 18.75%,  total acc: 27.08%   [EVAL] batch:    3 | acc: 18.75%,  total acc: 25.00%   [EVAL] batch:    4 | acc: 25.00%,  total acc: 25.00%   [EVAL] batch:    5 | acc: 37.50%,  total acc: 27.08%   [EVAL] batch:    6 | acc: 56.25%,  total acc: 31.25%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 37.50%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 43.75%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 48.12%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 51.14%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 53.65%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 56.25%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 57.14%   [EVAL] batch:   14 | acc: 87.50%,  total acc: 59.17%   [EVAL] batch:   15 | acc: 81.25%,  total acc: 60.55%   [EVAL] batch:   16 | acc: 81.25%,  total acc: 61.76%   [EVAL] batch:   17 | acc: 75.00%,  total acc: 62.50%   [EVAL] batch:   18 | acc: 75.00%,  total acc: 63.16%   [EVAL] batch:   19 | acc: 25.00%,  total acc: 61.25%   [EVAL] batch:   20 | acc: 6.25%,  total acc: 58.63%   [EVAL] batch:   21 | acc: 18.75%,  total acc: 56.82%   [EVAL] batch:   22 | acc: 37.50%,  total acc: 55.98%   [EVAL] batch:   23 | acc: 12.50%,  total acc: 54.17%   [EVAL] batch:   24 | acc: 18.75%,  total acc: 52.75%   [EVAL] batch:   25 | acc: 6.25%,  total acc: 50.96%   [EVAL] batch:   26 | acc: 12.50%,  total acc: 49.54%   [EVAL] batch:   27 | acc: 6.25%,  total acc: 47.99%   [EVAL] batch:   28 | acc: 6.25%,  total acc: 46.55%   [EVAL] batch:   29 | acc: 18.75%,  total acc: 45.62%   [EVAL] batch:   30 | acc: 6.25%,  total acc: 44.35%   [EVAL] batch:   31 | acc: 43.75%,  total acc: 44.34%   [EVAL] batch:   32 | acc: 87.50%,  total acc: 45.64%   [EVAL] batch:   33 | acc: 75.00%,  total acc: 46.51%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 47.86%   [EVAL] batch:   35 | acc: 81.25%,  total acc: 48.78%   [EVAL] batch:   36 | acc: 56.25%,  total acc: 48.99%   [EVAL] batch:   37 | acc: 81.25%,  total acc: 49.84%   [EVAL] batch:   38 | acc: 87.50%,  total acc: 50.80%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 52.03%   [EVAL] batch:   40 | acc: 81.25%,  total acc: 52.74%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 53.57%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 54.51%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 55.26%   [EVAL] batch:   44 | acc: 25.00%,  total acc: 54.58%   [EVAL] batch:   45 | acc: 37.50%,  total acc: 54.21%   [EVAL] batch:   46 | acc: 37.50%,  total acc: 53.86%   [EVAL] batch:   47 | acc: 18.75%,  total acc: 53.12%   [EVAL] batch:   48 | acc: 31.25%,  total acc: 52.68%   [EVAL] batch:   49 | acc: 31.25%,  total acc: 52.25%   [EVAL] batch:   50 | acc: 62.50%,  total acc: 52.45%   [EVAL] batch:   51 | acc: 81.25%,  total acc: 53.00%   [EVAL] batch:   52 | acc: 37.50%,  total acc: 52.71%   [EVAL] batch:   53 | acc: 62.50%,  total acc: 52.89%   [EVAL] batch:   54 | acc: 75.00%,  total acc: 53.30%   [EVAL] batch:   55 | acc: 68.75%,  total acc: 53.57%   [EVAL] batch:   56 | acc: 25.00%,  total acc: 53.07%   [EVAL] batch:   57 | acc: 25.00%,  total acc: 52.59%   [EVAL] batch:   58 | acc: 31.25%,  total acc: 52.22%   [EVAL] batch:   59 | acc: 31.25%,  total acc: 51.88%   [EVAL] batch:   60 | acc: 31.25%,  total acc: 51.54%   [EVAL] batch:   61 | acc: 18.75%,  total acc: 51.01%   [EVAL] batch:   62 | acc: 12.50%,  total acc: 50.40%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 80.00%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    6 | acc: 37.50%,  total acc: 75.00%   [EVAL] batch:    7 | acc: 25.00%,  total acc: 68.75%   [EVAL] batch:    8 | acc: 43.75%,  total acc: 65.97%   [EVAL] batch:    9 | acc: 37.50%,  total acc: 63.12%   [EVAL] batch:   10 | acc: 25.00%,  total acc: 59.66%   [EVAL] batch:   11 | acc: 43.75%,  total acc: 58.33%   [EVAL] batch:   12 | acc: 50.00%,  total acc: 57.69%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 60.27%   [EVAL] batch:   14 | acc: 87.50%,  total acc: 62.08%   [EVAL] batch:   15 | acc: 87.50%,  total acc: 63.67%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 65.44%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 67.01%   [EVAL] batch:   18 | acc: 81.25%,  total acc: 67.76%   [EVAL] batch:   19 | acc: 62.50%,  total acc: 67.50%   [EVAL] batch:   20 | acc: 81.25%,  total acc: 68.15%   [EVAL] batch:   21 | acc: 68.75%,  total acc: 68.18%   [EVAL] batch:   22 | acc: 62.50%,  total acc: 67.93%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 69.01%   [EVAL] batch:   24 | acc: 81.25%,  total acc: 69.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 70.67%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 71.76%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 72.77%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 73.49%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 74.38%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 75.20%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 75.78%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 76.52%   [EVAL] batch:   33 | acc: 62.50%,  total acc: 76.10%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 76.07%   [EVAL] batch:   35 | acc: 68.75%,  total acc: 75.87%   [EVAL] batch:   36 | acc: 75.00%,  total acc: 75.84%   [EVAL] batch:   37 | acc: 81.25%,  total acc: 75.99%   [EVAL] batch:   38 | acc: 93.75%,  total acc: 76.44%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 77.03%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 77.59%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 77.98%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 78.34%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 78.41%   [EVAL] batch:   44 | acc: 75.00%,  total acc: 78.33%   [EVAL] batch:   45 | acc: 62.50%,  total acc: 77.99%   [EVAL] batch:   46 | acc: 50.00%,  total acc: 77.39%   [EVAL] batch:   47 | acc: 62.50%,  total acc: 77.08%   [EVAL] batch:   48 | acc: 81.25%,  total acc: 77.17%   [EVAL] batch:   49 | acc: 43.75%,  total acc: 76.50%   [EVAL] batch:   50 | acc: 81.25%,  total acc: 76.59%   [EVAL] batch:   51 | acc: 81.25%,  total acc: 76.68%   [EVAL] batch:   52 | acc: 81.25%,  total acc: 76.77%   [EVAL] batch:   53 | acc: 68.75%,  total acc: 76.62%   [EVAL] batch:   54 | acc: 43.75%,  total acc: 76.02%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 76.23%   [EVAL] batch:   56 | acc: 56.25%,  total acc: 75.88%   [EVAL] batch:   57 | acc: 37.50%,  total acc: 75.22%   [EVAL] batch:   58 | acc: 56.25%,  total acc: 74.89%   [EVAL] batch:   59 | acc: 75.00%,  total acc: 74.90%   [EVAL] batch:   60 | acc: 81.25%,  total acc: 75.00%   [EVAL] batch:   61 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:   62 | acc: 56.25%,  total acc: 74.70%   [EVAL] batch:   63 | acc: 12.50%,  total acc: 73.73%   [EVAL] batch:   64 | acc: 12.50%,  total acc: 72.79%   [EVAL] batch:   65 | acc: 62.50%,  total acc: 72.63%   [EVAL] batch:   66 | acc: 25.00%,  total acc: 71.92%   [EVAL] batch:   67 | acc: 31.25%,  total acc: 71.32%   [EVAL] batch:   68 | acc: 31.25%,  total acc: 70.74%   [EVAL] batch:   69 | acc: 43.75%,  total acc: 70.36%   [EVAL] batch:   70 | acc: 43.75%,  total acc: 69.98%   [EVAL] batch:   71 | acc: 43.75%,  total acc: 69.62%   [EVAL] batch:   72 | acc: 50.00%,  total acc: 69.35%   [EVAL] batch:   73 | acc: 43.75%,  total acc: 69.00%   [EVAL] batch:   74 | acc: 50.00%,  total acc: 68.75%   [EVAL] batch:   75 | acc: 31.25%,  total acc: 68.26%   [EVAL] batch:   76 | acc: 18.75%,  total acc: 67.61%   [EVAL] batch:   77 | acc: 25.00%,  total acc: 67.07%   [EVAL] batch:   78 | acc: 37.50%,  total acc: 66.69%   [EVAL] batch:   79 | acc: 43.75%,  total acc: 66.41%   [EVAL] batch:   80 | acc: 18.75%,  total acc: 65.82%   [EVAL] batch:   81 | acc: 75.00%,  total acc: 65.93%   [EVAL] batch:   82 | acc: 100.00%,  total acc: 66.34%   [EVAL] batch:   83 | acc: 100.00%,  total acc: 66.74%   [EVAL] batch:   84 | acc: 93.75%,  total acc: 67.06%   [EVAL] batch:   85 | acc: 100.00%,  total acc: 67.44%   [EVAL] batch:   86 | acc: 93.75%,  total acc: 67.74%   [EVAL] batch:   87 | acc: 62.50%,  total acc: 67.68%   [EVAL] batch:   88 | acc: 12.50%,  total acc: 67.06%   [EVAL] batch:   89 | acc: 18.75%,  total acc: 66.53%   [EVAL] batch:   90 | acc: 6.25%,  total acc: 65.87%   [EVAL] batch:   91 | acc: 31.25%,  total acc: 65.49%   [EVAL] batch:   92 | acc: 25.00%,  total acc: 65.05%   [EVAL] batch:   93 | acc: 50.00%,  total acc: 64.89%   [EVAL] batch:   94 | acc: 81.25%,  total acc: 65.07%   [EVAL] batch:   95 | acc: 87.50%,  total acc: 65.30%   [EVAL] batch:   96 | acc: 87.50%,  total acc: 65.53%   [EVAL] batch:   97 | acc: 87.50%,  total acc: 65.75%   [EVAL] batch:   98 | acc: 87.50%,  total acc: 65.97%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 66.31%   [EVAL] batch:  100 | acc: 25.00%,  total acc: 65.90%   [EVAL] batch:  101 | acc: 37.50%,  total acc: 65.62%   [EVAL] batch:  102 | acc: 18.75%,  total acc: 65.17%   [EVAL] batch:  103 | acc: 12.50%,  total acc: 64.66%   [EVAL] batch:  104 | acc: 37.50%,  total acc: 64.40%   [EVAL] batch:  105 | acc: 37.50%,  total acc: 64.15%   [EVAL] batch:  106 | acc: 75.00%,  total acc: 64.25%   [EVAL] batch:  107 | acc: 81.25%,  total acc: 64.41%   [EVAL] batch:  108 | acc: 81.25%,  total acc: 64.56%   [EVAL] batch:  109 | acc: 68.75%,  total acc: 64.60%   [EVAL] batch:  110 | acc: 56.25%,  total acc: 64.53%   [EVAL] batch:  111 | acc: 68.75%,  total acc: 64.56%   [EVAL] batch:  112 | acc: 68.75%,  total acc: 64.60%   [EVAL] batch:  113 | acc: 62.50%,  total acc: 64.58%   [EVAL] batch:  114 | acc: 68.75%,  total acc: 64.62%   [EVAL] batch:  115 | acc: 68.75%,  total acc: 64.66%   [EVAL] batch:  116 | acc: 81.25%,  total acc: 64.80%   [EVAL] batch:  117 | acc: 68.75%,  total acc: 64.83%   [EVAL] batch:  118 | acc: 50.00%,  total acc: 64.71%   [EVAL] batch:  119 | acc: 93.75%,  total acc: 64.95%   [EVAL] batch:  120 | acc: 93.75%,  total acc: 65.19%   [EVAL] batch:  121 | acc: 87.50%,  total acc: 65.37%   [EVAL] batch:  122 | acc: 81.25%,  total acc: 65.50%   [EVAL] batch:  123 | acc: 87.50%,  total acc: 65.68%   [EVAL] batch:  124 | acc: 100.00%,  total acc: 65.95%   [EVAL] batch:  125 | acc: 6.25%,  total acc: 65.48%   [EVAL] batch:  126 | acc: 6.25%,  total acc: 65.01%   [EVAL] batch:  127 | acc: 12.50%,  total acc: 64.60%   [EVAL] batch:  128 | acc: 0.00%,  total acc: 64.10%   [EVAL] batch:  129 | acc: 0.00%,  total acc: 63.61%   [EVAL] batch:  130 | acc: 0.00%,  total acc: 63.12%   [EVAL] batch:  131 | acc: 50.00%,  total acc: 63.02%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 63.30%   [EVAL] batch:  133 | acc: 93.75%,  total acc: 63.53%   [EVAL] batch:  134 | acc: 100.00%,  total acc: 63.80%   [EVAL] batch:  135 | acc: 75.00%,  total acc: 63.88%   [EVAL] batch:  136 | acc: 93.75%,  total acc: 64.10%   [EVAL] batch:  137 | acc: 56.25%,  total acc: 64.04%   [EVAL] batch:  138 | acc: 25.00%,  total acc: 63.76%   [EVAL] batch:  139 | acc: 37.50%,  total acc: 63.57%   [EVAL] batch:  140 | acc: 37.50%,  total acc: 63.39%   [EVAL] batch:  141 | acc: 25.00%,  total acc: 63.12%   [EVAL] batch:  142 | acc: 43.75%,  total acc: 62.98%   [EVAL] batch:  143 | acc: 37.50%,  total acc: 62.80%   [EVAL] batch:  144 | acc: 93.75%,  total acc: 63.02%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 63.27%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 63.48%   [EVAL] batch:  147 | acc: 93.75%,  total acc: 63.68%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 63.93%   [EVAL] batch:  149 | acc: 81.25%,  total acc: 64.04%   [EVAL] batch:  150 | acc: 81.25%,  total acc: 64.16%   [EVAL] batch:  151 | acc: 87.50%,  total acc: 64.31%   [EVAL] batch:  152 | acc: 75.00%,  total acc: 64.38%   [EVAL] batch:  153 | acc: 87.50%,  total acc: 64.53%   [EVAL] batch:  154 | acc: 81.25%,  total acc: 64.64%   [EVAL] batch:  155 | acc: 87.50%,  total acc: 64.78%   [EVAL] batch:  156 | acc: 93.75%,  total acc: 64.97%   [EVAL] batch:  157 | acc: 100.00%,  total acc: 65.19%   [EVAL] batch:  158 | acc: 93.75%,  total acc: 65.37%   [EVAL] batch:  159 | acc: 87.50%,  total acc: 65.51%   [EVAL] batch:  160 | acc: 93.75%,  total acc: 65.68%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 65.86%   [EVAL] batch:  162 | acc: 62.50%,  total acc: 65.84%   [EVAL] batch:  163 | acc: 25.00%,  total acc: 65.59%   [EVAL] batch:  164 | acc: 31.25%,  total acc: 65.38%   [EVAL] batch:  165 | acc: 31.25%,  total acc: 65.17%   [EVAL] batch:  166 | acc: 12.50%,  total acc: 64.86%   [EVAL] batch:  167 | acc: 18.75%,  total acc: 64.58%   [EVAL] batch:  168 | acc: 43.75%,  total acc: 64.46%   [EVAL] batch:  169 | acc: 87.50%,  total acc: 64.60%   [EVAL] batch:  170 | acc: 93.75%,  total acc: 64.77%   [EVAL] batch:  171 | acc: 100.00%,  total acc: 64.97%   [EVAL] batch:  172 | acc: 100.00%,  total acc: 65.17%   [EVAL] batch:  173 | acc: 62.50%,  total acc: 65.16%   [EVAL] batch:  174 | acc: 87.50%,  total acc: 65.29%   [EVAL] batch:  175 | acc: 50.00%,  total acc: 65.20%   [EVAL] batch:  176 | acc: 43.75%,  total acc: 65.08%   [EVAL] batch:  177 | acc: 56.25%,  total acc: 65.03%   [EVAL] batch:  178 | acc: 50.00%,  total acc: 64.94%   [EVAL] batch:  179 | acc: 62.50%,  total acc: 64.93%   [EVAL] batch:  180 | acc: 25.00%,  total acc: 64.71%   [EVAL] batch:  181 | acc: 43.75%,  total acc: 64.59%   [EVAL] batch:  182 | acc: 50.00%,  total acc: 64.52%   [EVAL] batch:  183 | acc: 37.50%,  total acc: 64.37%   [EVAL] batch:  184 | acc: 81.25%,  total acc: 64.46%   [EVAL] batch:  185 | acc: 68.75%,  total acc: 64.48%   [EVAL] batch:  186 | acc: 68.75%,  total acc: 64.51%   [EVAL] batch:  187 | acc: 43.75%,  total acc: 64.39%   [EVAL] batch:  188 | acc: 56.25%,  total acc: 64.35%   [EVAL] batch:  189 | acc: 68.75%,  total acc: 64.38%   [EVAL] batch:  190 | acc: 68.75%,  total acc: 64.40%   [EVAL] batch:  191 | acc: 56.25%,  total acc: 64.36%   [EVAL] batch:  192 | acc: 81.25%,  total acc: 64.44%   [EVAL] batch:  193 | acc: 68.75%,  total acc: 64.47%   [EVAL] batch:  194 | acc: 75.00%,  total acc: 64.52%   [EVAL] batch:  195 | acc: 68.75%,  total acc: 64.54%   [EVAL] batch:  196 | acc: 50.00%,  total acc: 64.47%   [EVAL] batch:  197 | acc: 87.50%,  total acc: 64.58%   [EVAL] batch:  198 | acc: 68.75%,  total acc: 64.60%   [EVAL] batch:  199 | acc: 81.25%,  total acc: 64.69%   [EVAL] batch:  200 | acc: 68.75%,  total acc: 64.71%   [EVAL] batch:  201 | acc: 81.25%,  total acc: 64.79%   [EVAL] batch:  202 | acc: 75.00%,  total acc: 64.84%   [EVAL] batch:  203 | acc: 87.50%,  total acc: 64.95%   [EVAL] batch:  204 | acc: 81.25%,  total acc: 65.03%   [EVAL] batch:  205 | acc: 81.25%,  total acc: 65.11%   [EVAL] batch:  206 | acc: 75.00%,  total acc: 65.16%   [EVAL] batch:  207 | acc: 37.50%,  total acc: 65.02%   [EVAL] batch:  208 | acc: 25.00%,  total acc: 64.83%   [EVAL] batch:  209 | acc: 56.25%,  total acc: 64.79%   [EVAL] batch:  210 | acc: 31.25%,  total acc: 64.63%   [EVAL] batch:  211 | acc: 56.25%,  total acc: 64.59%   [EVAL] batch:  212 | acc: 68.75%,  total acc: 64.61%   [EVAL] batch:  213 | acc: 81.25%,  total acc: 64.69%   [EVAL] batch:  214 | acc: 93.75%,  total acc: 64.83%   [EVAL] batch:  215 | acc: 62.50%,  total acc: 64.81%   [EVAL] batch:  216 | acc: 93.75%,  total acc: 64.95%   [EVAL] batch:  217 | acc: 81.25%,  total acc: 65.02%   [EVAL] batch:  218 | acc: 87.50%,  total acc: 65.13%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 65.28%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 65.44%   [EVAL] batch:  221 | acc: 100.00%,  total acc: 65.60%   [EVAL] batch:  222 | acc: 100.00%,  total acc: 65.75%   [EVAL] batch:  223 | acc: 100.00%,  total acc: 65.90%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 66.06%   [EVAL] batch:  225 | acc: 93.75%,  total acc: 66.18%   [EVAL] batch:  226 | acc: 87.50%,  total acc: 66.27%   [EVAL] batch:  227 | acc: 75.00%,  total acc: 66.31%   [EVAL] batch:  228 | acc: 81.25%,  total acc: 66.38%   [EVAL] batch:  229 | acc: 100.00%,  total acc: 66.52%   [EVAL] batch:  230 | acc: 93.75%,  total acc: 66.64%   [EVAL] batch:  231 | acc: 56.25%,  total acc: 66.59%   [EVAL] batch:  232 | acc: 37.50%,  total acc: 66.47%   [EVAL] batch:  233 | acc: 43.75%,  total acc: 66.37%   [EVAL] batch:  234 | acc: 56.25%,  total acc: 66.33%   [EVAL] batch:  235 | acc: 62.50%,  total acc: 66.31%   [EVAL] batch:  236 | acc: 56.25%,  total acc: 66.27%   [EVAL] batch:  237 | acc: 87.50%,  total acc: 66.36%   [EVAL] batch:  238 | acc: 87.50%,  total acc: 66.45%   [EVAL] batch:  239 | acc: 100.00%,  total acc: 66.59%   [EVAL] batch:  240 | acc: 87.50%,  total acc: 66.68%   [EVAL] batch:  241 | acc: 87.50%,  total acc: 66.76%   [EVAL] batch:  242 | acc: 87.50%,  total acc: 66.85%   [EVAL] batch:  243 | acc: 75.00%,  total acc: 66.88%   [EVAL] batch:  244 | acc: 12.50%,  total acc: 66.66%   [EVAL] batch:  245 | acc: 18.75%,  total acc: 66.46%   [EVAL] batch:  246 | acc: 18.75%,  total acc: 66.27%   [EVAL] batch:  247 | acc: 12.50%,  total acc: 66.05%   [EVAL] batch:  248 | acc: 6.25%,  total acc: 65.81%   [EVAL] batch:  249 | acc: 18.75%,  total acc: 65.62%   [EVAL] batch:  250 | acc: 93.75%,  total acc: 65.74%   [EVAL] batch:  251 | acc: 93.75%,  total acc: 65.85%   [EVAL] batch:  252 | acc: 93.75%,  total acc: 65.96%   [EVAL] batch:  253 | acc: 81.25%,  total acc: 66.02%   [EVAL] batch:  254 | acc: 93.75%,  total acc: 66.13%   [EVAL] batch:  255 | acc: 100.00%,  total acc: 66.26%   [EVAL] batch:  256 | acc: 81.25%,  total acc: 66.32%   [EVAL] batch:  257 | acc: 75.00%,  total acc: 66.35%   [EVAL] batch:  258 | acc: 75.00%,  total acc: 66.39%   [EVAL] batch:  259 | acc: 75.00%,  total acc: 66.42%   [EVAL] batch:  260 | acc: 62.50%,  total acc: 66.40%   [EVAL] batch:  261 | acc: 81.25%,  total acc: 66.46%   [EVAL] batch:  262 | acc: 62.50%,  total acc: 66.44%   [EVAL] batch:  263 | acc: 81.25%,  total acc: 66.50%   [EVAL] batch:  264 | acc: 62.50%,  total acc: 66.49%   [EVAL] batch:  265 | acc: 50.00%,  total acc: 66.42%   [EVAL] batch:  266 | acc: 62.50%,  total acc: 66.41%   [EVAL] batch:  267 | acc: 56.25%,  total acc: 66.37%   [EVAL] batch:  268 | acc: 50.00%,  total acc: 66.31%   [EVAL] batch:  269 | acc: 31.25%,  total acc: 66.18%   [EVAL] batch:  270 | acc: 37.50%,  total acc: 66.07%   [EVAL] batch:  271 | acc: 18.75%,  total acc: 65.90%   [EVAL] batch:  272 | acc: 18.75%,  total acc: 65.73%   [EVAL] batch:  273 | acc: 25.00%,  total acc: 65.58%   [EVAL] batch:  274 | acc: 18.75%,  total acc: 65.41%   [EVAL] batch:  275 | acc: 100.00%,  total acc: 65.53%   [EVAL] batch:  276 | acc: 100.00%,  total acc: 65.66%   [EVAL] batch:  277 | acc: 100.00%,  total acc: 65.78%   [EVAL] batch:  278 | acc: 100.00%,  total acc: 65.91%   [EVAL] batch:  279 | acc: 100.00%,  total acc: 66.03%   [EVAL] batch:  280 | acc: 100.00%,  total acc: 66.15%   [EVAL] batch:  281 | acc: 75.00%,  total acc: 66.18%   [EVAL] batch:  282 | acc: 75.00%,  total acc: 66.21%   [EVAL] batch:  283 | acc: 68.75%,  total acc: 66.22%   [EVAL] batch:  284 | acc: 68.75%,  total acc: 66.23%   [EVAL] batch:  285 | acc: 62.50%,  total acc: 66.22%   [EVAL] batch:  286 | acc: 56.25%,  total acc: 66.18%   [EVAL] batch:  287 | acc: 68.75%,  total acc: 66.19%   [EVAL] batch:  288 | acc: 100.00%,  total acc: 66.31%   [EVAL] batch:  289 | acc: 100.00%,  total acc: 66.42%   [EVAL] batch:  290 | acc: 100.00%,  total acc: 66.54%   [EVAL] batch:  291 | acc: 100.00%,  total acc: 66.65%   [EVAL] batch:  292 | acc: 100.00%,  total acc: 66.77%   [EVAL] batch:  293 | acc: 93.75%,  total acc: 66.86%   [EVAL] batch:  294 | acc: 100.00%,  total acc: 66.97%   [EVAL] batch:  295 | acc: 100.00%,  total acc: 67.08%   [EVAL] batch:  296 | acc: 100.00%,  total acc: 67.19%   [EVAL] batch:  297 | acc: 100.00%,  total acc: 67.30%   [EVAL] batch:  298 | acc: 100.00%,  total acc: 67.41%   [EVAL] batch:  299 | acc: 100.00%,  total acc: 67.52%   [EVAL] batch:  300 | acc: 93.75%,  total acc: 67.61%   [EVAL] batch:  301 | acc: 93.75%,  total acc: 67.69%   [EVAL] batch:  302 | acc: 93.75%,  total acc: 67.78%   [EVAL] batch:  303 | acc: 93.75%,  total acc: 67.87%   [EVAL] batch:  304 | acc: 87.50%,  total acc: 67.93%   [EVAL] batch:  305 | acc: 93.75%,  total acc: 68.01%   [EVAL] batch:  306 | acc: 62.50%,  total acc: 68.00%   [EVAL] batch:  307 | acc: 68.75%,  total acc: 68.00%   [EVAL] batch:  308 | acc: 56.25%,  total acc: 67.96%   [EVAL] batch:  309 | acc: 68.75%,  total acc: 67.96%   [EVAL] batch:  310 | acc: 81.25%,  total acc: 68.01%   [EVAL] batch:  311 | acc: 56.25%,  total acc: 67.97%   [EVAL] batch:  312 | acc: 43.75%,  total acc: 67.89%   [EVAL] batch:  313 | acc: 43.75%,  total acc: 67.81%   [EVAL] batch:  314 | acc: 31.25%,  total acc: 67.70%   [EVAL] batch:  315 | acc: 43.75%,  total acc: 67.62%   [EVAL] batch:  316 | acc: 50.00%,  total acc: 67.57%   [EVAL] batch:  317 | acc: 50.00%,  total acc: 67.51%   [EVAL] batch:  318 | acc: 50.00%,  total acc: 67.46%   [EVAL] batch:  319 | acc: 100.00%,  total acc: 67.56%   [EVAL] batch:  320 | acc: 100.00%,  total acc: 67.66%   [EVAL] batch:  321 | acc: 87.50%,  total acc: 67.72%   [EVAL] batch:  322 | acc: 93.75%,  total acc: 67.80%   [EVAL] batch:  323 | acc: 100.00%,  total acc: 67.90%   [EVAL] batch:  324 | acc: 87.50%,  total acc: 67.96%   [EVAL] batch:  325 | acc: 68.75%,  total acc: 67.96%   [EVAL] batch:  326 | acc: 56.25%,  total acc: 67.93%   [EVAL] batch:  327 | acc: 75.00%,  total acc: 67.95%   [EVAL] batch:  328 | acc: 68.75%,  total acc: 67.95%   [EVAL] batch:  329 | acc: 81.25%,  total acc: 67.99%   [EVAL] batch:  330 | acc: 75.00%,  total acc: 68.01%   [EVAL] batch:  331 | acc: 50.00%,  total acc: 67.96%   [EVAL] batch:  332 | acc: 43.75%,  total acc: 67.89%   [EVAL] batch:  333 | acc: 56.25%,  total acc: 67.85%   [EVAL] batch:  334 | acc: 37.50%,  total acc: 67.76%   [EVAL] batch:  335 | acc: 50.00%,  total acc: 67.71%   [EVAL] batch:  336 | acc: 25.00%,  total acc: 67.58%   [EVAL] batch:  337 | acc: 87.50%,  total acc: 67.64%   [EVAL] batch:  338 | acc: 75.00%,  total acc: 67.66%   [EVAL] batch:  339 | acc: 68.75%,  total acc: 67.67%   [EVAL] batch:  340 | acc: 81.25%,  total acc: 67.71%   [EVAL] batch:  341 | acc: 68.75%,  total acc: 67.71%   [EVAL] batch:  342 | acc: 50.00%,  total acc: 67.66%   [EVAL] batch:  343 | acc: 37.50%,  total acc: 67.57%   [EVAL] batch:  344 | acc: 62.50%,  total acc: 67.55%   [EVAL] batch:  345 | acc: 56.25%,  total acc: 67.52%   [EVAL] batch:  346 | acc: 62.50%,  total acc: 67.51%   [EVAL] batch:  347 | acc: 68.75%,  total acc: 67.51%   [EVAL] batch:  348 | acc: 75.00%,  total acc: 67.53%   [EVAL] batch:  349 | acc: 62.50%,  total acc: 67.52%   [EVAL] batch:  350 | acc: 62.50%,  total acc: 67.50%   [EVAL] batch:  351 | acc: 62.50%,  total acc: 67.49%   [EVAL] batch:  352 | acc: 50.00%,  total acc: 67.44%   [EVAL] batch:  353 | acc: 68.75%,  total acc: 67.44%   [EVAL] batch:  354 | acc: 68.75%,  total acc: 67.45%   [EVAL] batch:  355 | acc: 62.50%,  total acc: 67.43%   [EVAL] batch:  356 | acc: 56.25%,  total acc: 67.40%   [EVAL] batch:  357 | acc: 31.25%,  total acc: 67.30%   [EVAL] batch:  358 | acc: 31.25%,  total acc: 67.20%   [EVAL] batch:  359 | acc: 50.00%,  total acc: 67.15%   [EVAL] batch:  360 | acc: 37.50%,  total acc: 67.07%   [EVAL] batch:  361 | acc: 50.00%,  total acc: 67.02%   [EVAL] batch:  362 | acc: 68.75%,  total acc: 67.03%   [EVAL] batch:  363 | acc: 100.00%,  total acc: 67.12%   [EVAL] batch:  364 | acc: 100.00%,  total acc: 67.21%   [EVAL] batch:  365 | acc: 100.00%,  total acc: 67.30%   [EVAL] batch:  366 | acc: 100.00%,  total acc: 67.39%   [EVAL] batch:  367 | acc: 100.00%,  total acc: 67.48%   [EVAL] batch:  368 | acc: 87.50%,  total acc: 67.53%   [EVAL] batch:  369 | acc: 68.75%,  total acc: 67.53%   [EVAL] batch:  370 | acc: 62.50%,  total acc: 67.52%   [EVAL] batch:  371 | acc: 75.00%,  total acc: 67.54%   [EVAL] batch:  372 | acc: 81.25%,  total acc: 67.58%   [EVAL] batch:  373 | acc: 62.50%,  total acc: 67.56%   [EVAL] batch:  374 | acc: 68.75%,  total acc: 67.57%   [EVAL] batch:  375 | acc: 18.75%,  total acc: 67.44%   [EVAL] batch:  376 | acc: 18.75%,  total acc: 67.31%   [EVAL] batch:  377 | acc: 31.25%,  total acc: 67.21%   [EVAL] batch:  378 | acc: 25.00%,  total acc: 67.10%   [EVAL] batch:  379 | acc: 31.25%,  total acc: 67.01%   [EVAL] batch:  380 | acc: 37.50%,  total acc: 66.93%   [EVAL] batch:  381 | acc: 43.75%,  total acc: 66.87%   [EVAL] batch:  382 | acc: 68.75%,  total acc: 66.87%   [EVAL] batch:  383 | acc: 68.75%,  total acc: 66.88%   [EVAL] batch:  384 | acc: 68.75%,  total acc: 66.88%   [EVAL] batch:  385 | acc: 50.00%,  total acc: 66.84%   [EVAL] batch:  386 | acc: 75.00%,  total acc: 66.86%   [EVAL] batch:  387 | acc: 93.75%,  total acc: 66.93%   [EVAL] batch:  388 | acc: 75.00%,  total acc: 66.95%   [EVAL] batch:  389 | acc: 68.75%,  total acc: 66.96%   [EVAL] batch:  390 | acc: 62.50%,  total acc: 66.94%   [EVAL] batch:  391 | acc: 68.75%,  total acc: 66.95%   [EVAL] batch:  392 | acc: 75.00%,  total acc: 66.97%   [EVAL] batch:  393 | acc: 81.25%,  total acc: 67.01%   [EVAL] batch:  394 | acc: 87.50%,  total acc: 67.06%   [EVAL] batch:  395 | acc: 100.00%,  total acc: 67.14%   [EVAL] batch:  396 | acc: 100.00%,  total acc: 67.22%   [EVAL] batch:  397 | acc: 93.75%,  total acc: 67.29%   [EVAL] batch:  398 | acc: 93.75%,  total acc: 67.36%   [EVAL] batch:  399 | acc: 100.00%,  total acc: 67.44%   [EVAL] batch:  400 | acc: 68.75%,  total acc: 67.44%   [EVAL] batch:  401 | acc: 75.00%,  total acc: 67.46%   [EVAL] batch:  402 | acc: 81.25%,  total acc: 67.49%   [EVAL] batch:  403 | acc: 81.25%,  total acc: 67.53%   [EVAL] batch:  404 | acc: 81.25%,  total acc: 67.56%   [EVAL] batch:  405 | acc: 87.50%,  total acc: 67.61%   [EVAL] batch:  406 | acc: 56.25%,  total acc: 67.58%   [EVAL] batch:  407 | acc: 62.50%,  total acc: 67.57%   [EVAL] batch:  408 | acc: 25.00%,  total acc: 67.47%   [EVAL] batch:  409 | acc: 37.50%,  total acc: 67.39%   [EVAL] batch:  410 | acc: 12.50%,  total acc: 67.26%   [EVAL] batch:  411 | acc: 18.75%,  total acc: 67.14%   [EVAL] batch:  412 | acc: 62.50%,  total acc: 67.13%   [EVAL] batch:  413 | acc: 56.25%,  total acc: 67.10%   [EVAL] batch:  414 | acc: 68.75%,  total acc: 67.11%   [EVAL] batch:  415 | acc: 75.00%,  total acc: 67.13%   [EVAL] batch:  416 | acc: 75.00%,  total acc: 67.15%   [EVAL] batch:  417 | acc: 68.75%,  total acc: 67.15%   [EVAL] batch:  418 | acc: 75.00%,  total acc: 67.17%   [EVAL] batch:  419 | acc: 87.50%,  total acc: 67.22%   [EVAL] batch:  420 | acc: 68.75%,  total acc: 67.22%   [EVAL] batch:  421 | acc: 56.25%,  total acc: 67.19%   [EVAL] batch:  422 | acc: 62.50%,  total acc: 67.18%   [EVAL] batch:  423 | acc: 75.00%,  total acc: 67.20%   [EVAL] batch:  424 | acc: 87.50%,  total acc: 67.25%   [EVAL] batch:  425 | acc: 100.00%,  total acc: 67.33%   [EVAL] batch:  426 | acc: 100.00%,  total acc: 67.40%   [EVAL] batch:  427 | acc: 100.00%,  total acc: 67.48%   [EVAL] batch:  428 | acc: 100.00%,  total acc: 67.56%   [EVAL] batch:  429 | acc: 100.00%,  total acc: 67.63%   [EVAL] batch:  430 | acc: 100.00%,  total acc: 67.71%   [EVAL] batch:  431 | acc: 87.50%,  total acc: 67.75%   [EVAL] batch:  432 | acc: 87.50%,  total acc: 67.80%   [EVAL] batch:  433 | acc: 87.50%,  total acc: 67.84%   [EVAL] batch:  434 | acc: 93.75%,  total acc: 67.90%   [EVAL] batch:  435 | acc: 81.25%,  total acc: 67.93%   [EVAL] batch:  436 | acc: 100.00%,  total acc: 68.01%   [EVAL] batch:  437 | acc: 62.50%,  total acc: 67.99%   [EVAL] batch:  438 | acc: 37.50%,  total acc: 67.92%   [EVAL] batch:  439 | acc: 25.00%,  total acc: 67.83%   [EVAL] batch:  440 | acc: 12.50%,  total acc: 67.70%   [EVAL] batch:  441 | acc: 25.00%,  total acc: 67.60%   [EVAL] batch:  442 | acc: 18.75%,  total acc: 67.49%   [EVAL] batch:  443 | acc: 56.25%,  total acc: 67.47%   [EVAL] batch:  444 | acc: 68.75%,  total acc: 67.47%   [EVAL] batch:  445 | acc: 87.50%,  total acc: 67.52%   [EVAL] batch:  446 | acc: 87.50%,  total acc: 67.56%   [EVAL] batch:  447 | acc: 100.00%,  total acc: 67.63%   [EVAL] batch:  448 | acc: 68.75%,  total acc: 67.64%   [EVAL] batch:  449 | acc: 81.25%,  total acc: 67.67%   [EVAL] batch:  450 | acc: 87.50%,  total acc: 67.71%   [EVAL] batch:  451 | acc: 75.00%,  total acc: 67.73%   [EVAL] batch:  452 | acc: 87.50%,  total acc: 67.77%   [EVAL] batch:  453 | acc: 81.25%,  total acc: 67.80%   [EVAL] batch:  454 | acc: 75.00%,  total acc: 67.82%   [EVAL] batch:  455 | acc: 87.50%,  total acc: 67.86%   [EVAL] batch:  456 | acc: 31.25%,  total acc: 67.78%   [EVAL] batch:  457 | acc: 18.75%,  total acc: 67.67%   [EVAL] batch:  458 | acc: 18.75%,  total acc: 67.57%   [EVAL] batch:  459 | acc: 31.25%,  total acc: 67.49%   [EVAL] batch:  460 | acc: 18.75%,  total acc: 67.38%   [EVAL] batch:  461 | acc: 12.50%,  total acc: 67.26%   [EVAL] batch:  462 | acc: 18.75%,  total acc: 67.16%   [EVAL] batch:  463 | acc: 0.00%,  total acc: 67.01%   [EVAL] batch:  464 | acc: 18.75%,  total acc: 66.91%   [EVAL] batch:  465 | acc: 0.00%,  total acc: 66.77%   [EVAL] batch:  466 | acc: 18.75%,  total acc: 66.66%   [EVAL] batch:  467 | acc: 12.50%,  total acc: 66.55%   [EVAL] batch:  468 | acc: 12.50%,  total acc: 66.43%   [EVAL] batch:  469 | acc: 68.75%,  total acc: 66.44%   [EVAL] batch:  470 | acc: 87.50%,  total acc: 66.48%   [EVAL] batch:  471 | acc: 81.25%,  total acc: 66.51%   [EVAL] batch:  472 | acc: 93.75%,  total acc: 66.57%   [EVAL] batch:  473 | acc: 68.75%,  total acc: 66.57%   [EVAL] batch:  474 | acc: 56.25%,  total acc: 66.55%   [EVAL] batch:  475 | acc: 93.75%,  total acc: 66.61%   [EVAL] batch:  476 | acc: 93.75%,  total acc: 66.67%   [EVAL] batch:  477 | acc: 87.50%,  total acc: 66.71%   [EVAL] batch:  478 | acc: 87.50%,  total acc: 66.75%   [EVAL] batch:  479 | acc: 93.75%,  total acc: 66.81%   [EVAL] batch:  480 | acc: 93.75%,  total acc: 66.87%   [EVAL] batch:  481 | acc: 50.00%,  total acc: 66.83%   [EVAL] batch:  482 | acc: 31.25%,  total acc: 66.76%   [EVAL] batch:  483 | acc: 37.50%,  total acc: 66.70%   [EVAL] batch:  484 | acc: 31.25%,  total acc: 66.62%   [EVAL] batch:  485 | acc: 25.00%,  total acc: 66.54%   [EVAL] batch:  486 | acc: 37.50%,  total acc: 66.48%   [EVAL] batch:  487 | acc: 43.75%,  total acc: 66.43%   [EVAL] batch:  488 | acc: 68.75%,  total acc: 66.44%   [EVAL] batch:  489 | acc: 50.00%,  total acc: 66.40%   [EVAL] batch:  490 | acc: 50.00%,  total acc: 66.37%   [EVAL] batch:  491 | acc: 68.75%,  total acc: 66.37%   [EVAL] batch:  492 | acc: 75.00%,  total acc: 66.39%   [EVAL] batch:  493 | acc: 56.25%,  total acc: 66.37%   [EVAL] batch:  494 | acc: 25.00%,  total acc: 66.29%   [EVAL] batch:  495 | acc: 31.25%,  total acc: 66.22%   [EVAL] batch:  496 | acc: 25.00%,  total acc: 66.13%   [EVAL] batch:  497 | acc: 25.00%,  total acc: 66.05%   [EVAL] batch:  498 | acc: 25.00%,  total acc: 65.97%   [EVAL] batch:  499 | acc: 25.00%,  total acc: 65.89%   
cur_acc:  ['0.9454', '0.7996', '0.7718', '0.7341', '0.8571', '0.6518', '0.7143', '0.5040']
his_acc:  ['0.9454', '0.8715', '0.8095', '0.7552', '0.7558', '0.7322', '0.7002', '0.6589']
--------Round  4
seed:  500
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/train.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/valid.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/test.pkl
Task_order: [7 5 6 4 2 1 3 0]
prepared data!
CurrentTrain: epoch 15, batch     0 | loss: 27.9895448CurrentTrain: epoch 15, batch     1 | loss: 26.4187489CurrentTrain: epoch 15, batch     2 | loss: 34.0378792CurrentTrain: epoch 15, batch     3 | loss: 22.9170851CurrentTrain: epoch 15, batch     4 | loss: 22.3142004CurrentTrain: epoch 15, batch     5 | loss: 24.6774138CurrentTrain: epoch 15, batch     6 | loss: 20.8560648CurrentTrain: epoch 15, batch     7 | loss: 34.1312563CurrentTrain: epoch 15, batch     8 | loss: 30.1974230CurrentTrain: epoch 15, batch     9 | loss: 26.6706864CurrentTrain: epoch 15, batch    10 | loss: 29.1617779CurrentTrain: epoch 15, batch    11 | loss: 32.5998659CurrentTrain: epoch 15, batch    12 | loss: 33.8173644CurrentTrain: epoch 15, batch    13 | loss: 25.6151053CurrentTrain: epoch 15, batch    14 | loss: 31.9843780CurrentTrain: epoch 15, batch    15 | loss: 22.5475770CurrentTrain: epoch 15, batch    16 | loss: 26.2791707CurrentTrain: epoch 15, batch    17 | loss: 34.2300547CurrentTrain: epoch 15, batch    18 | loss: 17.1452831CurrentTrain: epoch 15, batch    19 | loss: 25.8919739CurrentTrain: epoch 15, batch    20 | loss: 21.3024208CurrentTrain: epoch 15, batch    21 | loss: 18.4236134CurrentTrain: epoch 15, batch    22 | loss: 17.0134178CurrentTrain: epoch 15, batch    23 | loss: 23.2385148CurrentTrain: epoch 15, batch    24 | loss: 19.5569821CurrentTrain: epoch 15, batch    25 | loss: 15.5518714CurrentTrain: epoch 15, batch    26 | loss: 19.1134712CurrentTrain: epoch 15, batch    27 | loss: 21.9821652CurrentTrain: epoch 15, batch    28 | loss: 21.7243989CurrentTrain: epoch 15, batch    29 | loss: 18.7797374CurrentTrain: epoch 15, batch    30 | loss: 12.5821941CurrentTrain: epoch 15, batch    31 | loss: 16.8918550CurrentTrain: epoch 15, batch    32 | loss: 16.5763020CurrentTrain: epoch 15, batch    33 | loss: 15.2919228CurrentTrain: epoch 15, batch    34 | loss: 22.2869451CurrentTrain: epoch 15, batch    35 | loss: 21.7189778CurrentTrain: epoch 15, batch    36 | loss: 15.0360622CurrentTrain: epoch 15, batch    37 | loss: 20.9073049CurrentTrain: epoch 15, batch    38 | loss: 21.1691480CurrentTrain: epoch 15, batch    39 | loss: 18.2885907CurrentTrain: epoch 15, batch    40 | loss: 14.5445140CurrentTrain: epoch 15, batch    41 | loss: 15.0520639CurrentTrain: epoch 15, batch    42 | loss: 25.7296223CurrentTrain: epoch 15, batch    43 | loss: 19.3061787CurrentTrain: epoch 15, batch    44 | loss: 18.3403399CurrentTrain: epoch 15, batch    45 | loss: 23.1145843CurrentTrain: epoch 15, batch    46 | loss: 19.1622020CurrentTrain: epoch 15, batch    47 | loss: 15.2624384CurrentTrain: epoch 15, batch    48 | loss: 15.7363336CurrentTrain: epoch 15, batch    49 | loss: 17.1395834CurrentTrain: epoch 15, batch    50 | loss: 30.3024870CurrentTrain: epoch 15, batch    51 | loss: 16.8815928CurrentTrain: epoch 15, batch    52 | loss: 23.1455896CurrentTrain: epoch 15, batch    53 | loss: 18.0994637CurrentTrain: epoch 15, batch    54 | loss: 14.2993987CurrentTrain: epoch 15, batch    55 | loss: 12.0290708CurrentTrain: epoch 15, batch    56 | loss: 16.3811059CurrentTrain: epoch 15, batch    57 | loss: 19.6200512CurrentTrain: epoch 15, batch    58 | loss: 18.1843179CurrentTrain: epoch 15, batch    59 | loss: 19.2659794CurrentTrain: epoch 15, batch    60 | loss: 16.7245411CurrentTrain: epoch 15, batch    61 | loss: 17.2320022CurrentTrain: epoch  7, batch    62 | loss: 12.9165167CurrentTrain: epoch 15, batch     0 | loss: 16.0053315CurrentTrain: epoch 15, batch     1 | loss: 25.1783087CurrentTrain: epoch 15, batch     2 | loss: 13.5383870CurrentTrain: epoch 15, batch     3 | loss: 20.4696376CurrentTrain: epoch 15, batch     4 | loss: 22.7761346CurrentTrain: epoch 15, batch     5 | loss: 18.1156255CurrentTrain: epoch 15, batch     6 | loss: 27.1386743CurrentTrain: epoch 15, batch     7 | loss: 18.4998391CurrentTrain: epoch 15, batch     8 | loss: 22.1746621CurrentTrain: epoch 15, batch     9 | loss: 15.6600664CurrentTrain: epoch 15, batch    10 | loss: 13.5311496CurrentTrain: epoch 15, batch    11 | loss: 13.8199310CurrentTrain: epoch 15, batch    12 | loss: 17.5415689CurrentTrain: epoch 15, batch    13 | loss: 16.5954372CurrentTrain: epoch 15, batch    14 | loss: 21.5136166CurrentTrain: epoch 15, batch    15 | loss: 19.6630055CurrentTrain: epoch 15, batch    16 | loss: 23.3789491CurrentTrain: epoch 15, batch    17 | loss: 16.0152608CurrentTrain: epoch 15, batch    18 | loss: 20.4081328CurrentTrain: epoch 15, batch    19 | loss: 23.0013669CurrentTrain: epoch 15, batch    20 | loss: 18.1420402CurrentTrain: epoch 15, batch    21 | loss: 22.7268622CurrentTrain: epoch 15, batch    22 | loss: 14.0797815CurrentTrain: epoch 15, batch    23 | loss: 12.9388660CurrentTrain: epoch 15, batch    24 | loss: 18.4925661CurrentTrain: epoch 15, batch    25 | loss: 18.6224388CurrentTrain: epoch 15, batch    26 | loss: 19.3051397CurrentTrain: epoch 15, batch    27 | loss: 19.5433173CurrentTrain: epoch 15, batch    28 | loss: 18.7670633CurrentTrain: epoch 15, batch    29 | loss: 12.8532138CurrentTrain: epoch 15, batch    30 | loss: 18.3134601CurrentTrain: epoch 15, batch    31 | loss: 12.1046519CurrentTrain: epoch 15, batch    32 | loss: 15.1667432CurrentTrain: epoch 15, batch    33 | loss: 17.9495929CurrentTrain: epoch 15, batch    34 | loss: 14.7719750CurrentTrain: epoch 15, batch    35 | loss: 20.9185732CurrentTrain: epoch 15, batch    36 | loss: 18.4791026CurrentTrain: epoch 15, batch    37 | loss: 17.1293007CurrentTrain: epoch 15, batch    38 | loss: 18.5804571CurrentTrain: epoch 15, batch    39 | loss: 16.9662516CurrentTrain: epoch 15, batch    40 | loss: 22.4692692CurrentTrain: epoch 15, batch    41 | loss: 12.7698238CurrentTrain: epoch 15, batch    42 | loss: 12.7300228CurrentTrain: epoch 15, batch    43 | loss: 22.0389693CurrentTrain: epoch 15, batch    44 | loss: 12.0832174CurrentTrain: epoch 15, batch    45 | loss: 11.8555595CurrentTrain: epoch 15, batch    46 | loss: 14.3194685CurrentTrain: epoch 15, batch    47 | loss: 17.0375083CurrentTrain: epoch 15, batch    48 | loss: 16.7392548CurrentTrain: epoch 15, batch    49 | loss: 15.3962937CurrentTrain: epoch 15, batch    50 | loss: 15.2245404CurrentTrain: epoch 15, batch    51 | loss: 16.3926767CurrentTrain: epoch 15, batch    52 | loss: 14.5748356CurrentTrain: epoch 15, batch    53 | loss: 13.4632912CurrentTrain: epoch 15, batch    54 | loss: 13.6675359CurrentTrain: epoch 15, batch    55 | loss: 19.0683747CurrentTrain: epoch 15, batch    56 | loss: 15.6410430CurrentTrain: epoch 15, batch    57 | loss: 15.4069040CurrentTrain: epoch 15, batch    58 | loss: 13.3364957CurrentTrain: epoch 15, batch    59 | loss: 17.7018993CurrentTrain: epoch 15, batch    60 | loss: 21.3921572CurrentTrain: epoch 15, batch    61 | loss: 18.9172635CurrentTrain: epoch  7, batch    62 | loss: 10.4955484CurrentTrain: epoch 15, batch     0 | loss: 15.4784900CurrentTrain: epoch 15, batch     1 | loss: 14.1075023CurrentTrain: epoch 15, batch     2 | loss: 20.7609930CurrentTrain: epoch 15, batch     3 | loss: 15.5065466CurrentTrain: epoch 15, batch     4 | loss: 16.6749896CurrentTrain: epoch 15, batch     5 | loss: 22.2990463CurrentTrain: epoch 15, batch     6 | loss: 11.7335334CurrentTrain: epoch 15, batch     7 | loss: 16.7256165CurrentTrain: epoch 15, batch     8 | loss: 17.4104419CurrentTrain: epoch 15, batch     9 | loss: 18.2364223CurrentTrain: epoch 15, batch    10 | loss: 24.5809140CurrentTrain: epoch 15, batch    11 | loss: 15.3944479CurrentTrain: epoch 15, batch    12 | loss: 12.3631687CurrentTrain: epoch 15, batch    13 | loss: 16.0984023CurrentTrain: epoch 15, batch    14 | loss: 10.4094848CurrentTrain: epoch 15, batch    15 | loss: 17.9613985CurrentTrain: epoch 15, batch    16 | loss: 16.1192747CurrentTrain: epoch 15, batch    17 | loss: 13.0646628CurrentTrain: epoch 15, batch    18 | loss: 10.8745067CurrentTrain: epoch 15, batch    19 | loss: 19.0515739CurrentTrain: epoch 15, batch    20 | loss: 15.2691255CurrentTrain: epoch 15, batch    21 | loss: 13.6160625CurrentTrain: epoch 15, batch    22 | loss: 18.6858659CurrentTrain: epoch 15, batch    23 | loss: 19.7988249CurrentTrain: epoch 15, batch    24 | loss: 15.3181678CurrentTrain: epoch 15, batch    25 | loss: 14.8767466CurrentTrain: epoch 15, batch    26 | loss: 18.9753726CurrentTrain: epoch 15, batch    27 | loss: 17.2990810CurrentTrain: epoch 15, batch    28 | loss: 27.5071024CurrentTrain: epoch 15, batch    29 | loss: 18.5114005CurrentTrain: epoch 15, batch    30 | loss: 20.1793102CurrentTrain: epoch 15, batch    31 | loss: 22.7324576CurrentTrain: epoch 15, batch    32 | loss: 11.2768240CurrentTrain: epoch 15, batch    33 | loss: 20.4935845CurrentTrain: epoch 15, batch    34 | loss: 16.2509799CurrentTrain: epoch 15, batch    35 | loss: 12.1159732CurrentTrain: epoch 15, batch    36 | loss: 19.1933787CurrentTrain: epoch 15, batch    37 | loss: 20.1703320CurrentTrain: epoch 15, batch    38 | loss: 14.2541392CurrentTrain: epoch 15, batch    39 | loss: 14.8746305CurrentTrain: epoch 15, batch    40 | loss: 9.7362027CurrentTrain: epoch 15, batch    41 | loss: 10.0902726CurrentTrain: epoch 15, batch    42 | loss: 12.7943261CurrentTrain: epoch 15, batch    43 | loss: 13.8408437CurrentTrain: epoch 15, batch    44 | loss: 13.1048162CurrentTrain: epoch 15, batch    45 | loss: 19.7353844CurrentTrain: epoch 15, batch    46 | loss: 17.1701686CurrentTrain: epoch 15, batch    47 | loss: 12.5836118CurrentTrain: epoch 15, batch    48 | loss: 21.4553367CurrentTrain: epoch 15, batch    49 | loss: 14.0949414CurrentTrain: epoch 15, batch    50 | loss: 17.1664414CurrentTrain: epoch 15, batch    51 | loss: 9.6764736CurrentTrain: epoch 15, batch    52 | loss: 16.4771079CurrentTrain: epoch 15, batch    53 | loss: 16.2391196CurrentTrain: epoch 15, batch    54 | loss: 18.1361623CurrentTrain: epoch 15, batch    55 | loss: 15.2084355CurrentTrain: epoch 15, batch    56 | loss: 13.4851019CurrentTrain: epoch 15, batch    57 | loss: 17.4222003CurrentTrain: epoch 15, batch    58 | loss: 12.9514128CurrentTrain: epoch 15, batch    59 | loss: 16.6532805CurrentTrain: epoch 15, batch    60 | loss: 11.0036740CurrentTrain: epoch 15, batch    61 | loss: 20.6010055CurrentTrain: epoch  7, batch    62 | loss: 7.8122472CurrentTrain: epoch 15, batch     0 | loss: 14.4688206CurrentTrain: epoch 15, batch     1 | loss: 14.7964100CurrentTrain: epoch 15, batch     2 | loss: 18.9881869CurrentTrain: epoch 15, batch     3 | loss: 10.0563274CurrentTrain: epoch 15, batch     4 | loss: 12.3269977CurrentTrain: epoch 15, batch     5 | loss: 16.6807403CurrentTrain: epoch 15, batch     6 | loss: 17.7951814CurrentTrain: epoch 15, batch     7 | loss: 11.9778190CurrentTrain: epoch 15, batch     8 | loss: 21.8890346CurrentTrain: epoch 15, batch     9 | loss: 13.9527669CurrentTrain: epoch 15, batch    10 | loss: 20.7426314CurrentTrain: epoch 15, batch    11 | loss: 11.2558405CurrentTrain: epoch 15, batch    12 | loss: 18.8283019CurrentTrain: epoch 15, batch    13 | loss: 14.1150709CurrentTrain: epoch 15, batch    14 | loss: 14.3033996CurrentTrain: epoch 15, batch    15 | loss: 28.4266478CurrentTrain: epoch 15, batch    16 | loss: 14.4348518CurrentTrain: epoch 15, batch    17 | loss: 17.3973081CurrentTrain: epoch 15, batch    18 | loss: 14.2138761CurrentTrain: epoch 15, batch    19 | loss: 13.2157223CurrentTrain: epoch 15, batch    20 | loss: 10.7479070CurrentTrain: epoch 15, batch    21 | loss: 17.6325299CurrentTrain: epoch 15, batch    22 | loss: 10.4765064CurrentTrain: epoch 15, batch    23 | loss: 15.6589403CurrentTrain: epoch 15, batch    24 | loss: 20.7892602CurrentTrain: epoch 15, batch    25 | loss: 15.2255073CurrentTrain: epoch 15, batch    26 | loss: 12.5272857CurrentTrain: epoch 15, batch    27 | loss: 15.8155969CurrentTrain: epoch 15, batch    28 | loss: 16.7659506CurrentTrain: epoch 15, batch    29 | loss: 14.7458044CurrentTrain: epoch 15, batch    30 | loss: 14.0018950CurrentTrain: epoch 15, batch    31 | loss: 11.1446541CurrentTrain: epoch 15, batch    32 | loss: 14.2255628CurrentTrain: epoch 15, batch    33 | loss: 22.4735612CurrentTrain: epoch 15, batch    34 | loss: 13.6030859CurrentTrain: epoch 15, batch    35 | loss: 11.2716714CurrentTrain: epoch 15, batch    36 | loss: 25.7478548CurrentTrain: epoch 15, batch    37 | loss: 14.3951741CurrentTrain: epoch 15, batch    38 | loss: 20.8961950CurrentTrain: epoch 15, batch    39 | loss: 13.0445427CurrentTrain: epoch 15, batch    40 | loss: 9.9944497CurrentTrain: epoch 15, batch    41 | loss: 8.9258165CurrentTrain: epoch 15, batch    42 | loss: 15.2910726CurrentTrain: epoch 15, batch    43 | loss: 13.4894480CurrentTrain: epoch 15, batch    44 | loss: 12.3208566CurrentTrain: epoch 15, batch    45 | loss: 26.3293467CurrentTrain: epoch 15, batch    46 | loss: 13.0496822CurrentTrain: epoch 15, batch    47 | loss: 17.3733776CurrentTrain: epoch 15, batch    48 | loss: 10.2211889CurrentTrain: epoch 15, batch    49 | loss: 10.2558709CurrentTrain: epoch 15, batch    50 | loss: 28.4261155CurrentTrain: epoch 15, batch    51 | loss: 15.7678726CurrentTrain: epoch 15, batch    52 | loss: 17.4956080CurrentTrain: epoch 15, batch    53 | loss: 23.9875515CurrentTrain: epoch 15, batch    54 | loss: 13.9967675CurrentTrain: epoch 15, batch    55 | loss: 12.2516378CurrentTrain: epoch 15, batch    56 | loss: 9.9481792CurrentTrain: epoch 15, batch    57 | loss: 12.2869506CurrentTrain: epoch 15, batch    58 | loss: 17.5993597CurrentTrain: epoch 15, batch    59 | loss: 21.4320175CurrentTrain: epoch 15, batch    60 | loss: 15.0606299CurrentTrain: epoch 15, batch    61 | loss: 12.3076837CurrentTrain: epoch  7, batch    62 | loss: 11.1644007CurrentTrain: epoch 15, batch     0 | loss: 16.0112163CurrentTrain: epoch 15, batch     1 | loss: 16.9639569CurrentTrain: epoch 15, batch     2 | loss: 19.7141152CurrentTrain: epoch 15, batch     3 | loss: 19.1210611CurrentTrain: epoch 15, batch     4 | loss: 14.4921858CurrentTrain: epoch 15, batch     5 | loss: 14.1608518CurrentTrain: epoch 15, batch     6 | loss: 14.3855000CurrentTrain: epoch 15, batch     7 | loss: 16.9672618CurrentTrain: epoch 15, batch     8 | loss: 12.3439201CurrentTrain: epoch 15, batch     9 | loss: 19.4290423CurrentTrain: epoch 15, batch    10 | loss: 18.4831322CurrentTrain: epoch 15, batch    11 | loss: 15.6685988CurrentTrain: epoch 15, batch    12 | loss: 30.6010059CurrentTrain: epoch 15, batch    13 | loss: 17.4166895CurrentTrain: epoch 15, batch    14 | loss: 11.3600067CurrentTrain: epoch 15, batch    15 | loss: 12.0639485CurrentTrain: epoch 15, batch    16 | loss: 14.8055693CurrentTrain: epoch 15, batch    17 | loss: 11.7146771CurrentTrain: epoch 15, batch    18 | loss: 9.3425883CurrentTrain: epoch 15, batch    19 | loss: 10.8995226CurrentTrain: epoch 15, batch    20 | loss: 12.7413980CurrentTrain: epoch 15, batch    21 | loss: 15.4506603CurrentTrain: epoch 15, batch    22 | loss: 16.7268155CurrentTrain: epoch 15, batch    23 | loss: 18.4179235CurrentTrain: epoch 15, batch    24 | loss: 11.4004897CurrentTrain: epoch 15, batch    25 | loss: 25.7662639CurrentTrain: epoch 15, batch    26 | loss: 9.6039928CurrentTrain: epoch 15, batch    27 | loss: 13.9066655CurrentTrain: epoch 15, batch    28 | loss: 12.3401810CurrentTrain: epoch 15, batch    29 | loss: 15.2051419CurrentTrain: epoch 15, batch    30 | loss: 16.7191579CurrentTrain: epoch 15, batch    31 | loss: 10.5449562CurrentTrain: epoch 15, batch    32 | loss: 17.3602591CurrentTrain: epoch 15, batch    33 | loss: 10.1118167CurrentTrain: epoch 15, batch    34 | loss: 20.8655691CurrentTrain: epoch 15, batch    35 | loss: 14.3802538CurrentTrain: epoch 15, batch    36 | loss: 14.3434791CurrentTrain: epoch 15, batch    37 | loss: 18.4608334CurrentTrain: epoch 15, batch    38 | loss: 17.0450551CurrentTrain: epoch 15, batch    39 | loss: 12.3229245CurrentTrain: epoch 15, batch    40 | loss: 16.7597946CurrentTrain: epoch 15, batch    41 | loss: 10.3957855CurrentTrain: epoch 15, batch    42 | loss: 10.9704246CurrentTrain: epoch 15, batch    43 | loss: 16.0824734CurrentTrain: epoch 15, batch    44 | loss: 12.7483103CurrentTrain: epoch 15, batch    45 | loss: 14.6395221CurrentTrain: epoch 15, batch    46 | loss: 11.6187924CurrentTrain: epoch 15, batch    47 | loss: 16.1568581CurrentTrain: epoch 15, batch    48 | loss: 12.7750976CurrentTrain: epoch 15, batch    49 | loss: 13.6916517CurrentTrain: epoch 15, batch    50 | loss: 13.6906117CurrentTrain: epoch 15, batch    51 | loss: 19.6302754CurrentTrain: epoch 15, batch    52 | loss: 8.2086641CurrentTrain: epoch 15, batch    53 | loss: 16.7295268CurrentTrain: epoch 15, batch    54 | loss: 10.8141755CurrentTrain: epoch 15, batch    55 | loss: 17.8659349CurrentTrain: epoch 15, batch    56 | loss: 9.8953140CurrentTrain: epoch 15, batch    57 | loss: 11.5595079CurrentTrain: epoch 15, batch    58 | loss: 14.6220111CurrentTrain: epoch 15, batch    59 | loss: 8.0630862CurrentTrain: epoch 15, batch    60 | loss: 15.5863109CurrentTrain: epoch 15, batch    61 | loss: 11.3066127CurrentTrain: epoch  7, batch    62 | loss: 11.4847582CurrentTrain: epoch 15, batch     0 | loss: 13.6468064CurrentTrain: epoch 15, batch     1 | loss: 13.0819984CurrentTrain: epoch 15, batch     2 | loss: 11.5244533CurrentTrain: epoch 15, batch     3 | loss: 15.9014792CurrentTrain: epoch 15, batch     4 | loss: 12.1700762CurrentTrain: epoch 15, batch     5 | loss: 19.5452879CurrentTrain: epoch 15, batch     6 | loss: 13.1604732CurrentTrain: epoch 15, batch     7 | loss: 17.4091976CurrentTrain: epoch 15, batch     8 | loss: 25.5451088CurrentTrain: epoch 15, batch     9 | loss: 17.4118641CurrentTrain: epoch 15, batch    10 | loss: 18.2441071CurrentTrain: epoch 15, batch    11 | loss: 24.4305447CurrentTrain: epoch 15, batch    12 | loss: 15.4867995CurrentTrain: epoch 15, batch    13 | loss: 16.4337427CurrentTrain: epoch 15, batch    14 | loss: 10.4889675CurrentTrain: epoch 15, batch    15 | loss: 26.9963634CurrentTrain: epoch 15, batch    16 | loss: 19.0140760CurrentTrain: epoch 15, batch    17 | loss: 18.4406287CurrentTrain: epoch 15, batch    18 | loss: 17.8494993CurrentTrain: epoch 15, batch    19 | loss: 25.1433603CurrentTrain: epoch 15, batch    20 | loss: 11.1901086CurrentTrain: epoch 15, batch    21 | loss: 24.3325780CurrentTrain: epoch 15, batch    22 | loss: 13.0204935CurrentTrain: epoch 15, batch    23 | loss: 31.0546177CurrentTrain: epoch 15, batch    24 | loss: 13.7131339CurrentTrain: epoch 15, batch    25 | loss: 9.5006693CurrentTrain: epoch 15, batch    26 | loss: 12.9427566CurrentTrain: epoch 15, batch    27 | loss: 10.5441915CurrentTrain: epoch 15, batch    28 | loss: 16.9031761CurrentTrain: epoch 15, batch    29 | loss: 13.7979865CurrentTrain: epoch 15, batch    30 | loss: 17.4316283CurrentTrain: epoch 15, batch    31 | loss: 13.7422688CurrentTrain: epoch 15, batch    32 | loss: 18.2707953CurrentTrain: epoch 15, batch    33 | loss: 16.1360618CurrentTrain: epoch 15, batch    34 | loss: 16.0380490CurrentTrain: epoch 15, batch    35 | loss: 12.8008983CurrentTrain: epoch 15, batch    36 | loss: 15.0367451CurrentTrain: epoch 15, batch    37 | loss: 17.7812712CurrentTrain: epoch 15, batch    38 | loss: 13.0740400CurrentTrain: epoch 15, batch    39 | loss: 15.5689844CurrentTrain: epoch 15, batch    40 | loss: 16.6464795CurrentTrain: epoch 15, batch    41 | loss: 13.9569861CurrentTrain: epoch 15, batch    42 | loss: 11.5101269CurrentTrain: epoch 15, batch    43 | loss: 11.3206599CurrentTrain: epoch 15, batch    44 | loss: 17.6362897CurrentTrain: epoch 15, batch    45 | loss: 17.1343544CurrentTrain: epoch 15, batch    46 | loss: 17.7926232CurrentTrain: epoch 15, batch    47 | loss: 16.1245319CurrentTrain: epoch 15, batch    48 | loss: 17.8312610CurrentTrain: epoch 15, batch    49 | loss: 15.1708489CurrentTrain: epoch 15, batch    50 | loss: 13.6425847CurrentTrain: epoch 15, batch    51 | loss: 10.1598102CurrentTrain: epoch 15, batch    52 | loss: 12.4619980CurrentTrain: epoch 15, batch    53 | loss: 12.4290263CurrentTrain: epoch 15, batch    54 | loss: 15.5832131CurrentTrain: epoch 15, batch    55 | loss: 16.3414966CurrentTrain: epoch 15, batch    56 | loss: 15.1277871CurrentTrain: epoch 15, batch    57 | loss: 17.3855481CurrentTrain: epoch 15, batch    58 | loss: 23.4203814CurrentTrain: epoch 15, batch    59 | loss: 15.1498262CurrentTrain: epoch 15, batch    60 | loss: 14.3860453CurrentTrain: epoch 15, batch    61 | loss: 9.1661574CurrentTrain: epoch  7, batch    62 | loss: 15.1199700CurrentTrain: epoch 15, batch     0 | loss: 11.0079664CurrentTrain: epoch 15, batch     1 | loss: 20.7146079CurrentTrain: epoch 15, batch     2 | loss: 28.9220659CurrentTrain: epoch 15, batch     3 | loss: 37.7957237CurrentTrain: epoch 15, batch     4 | loss: 17.3227489CurrentTrain: epoch 15, batch     5 | loss: 13.6311720CurrentTrain: epoch 15, batch     6 | loss: 25.3736589CurrentTrain: epoch 15, batch     7 | loss: 16.1645726CurrentTrain: epoch 15, batch     8 | loss: 10.2953786CurrentTrain: epoch 15, batch     9 | loss: 9.9913145CurrentTrain: epoch 15, batch    10 | loss: 14.8047136CurrentTrain: epoch 15, batch    11 | loss: 18.4225065CurrentTrain: epoch 15, batch    12 | loss: 10.0406021CurrentTrain: epoch 15, batch    13 | loss: 17.3495924CurrentTrain: epoch 15, batch    14 | loss: 13.2841888CurrentTrain: epoch 15, batch    15 | loss: 15.9645733CurrentTrain: epoch 15, batch    16 | loss: 15.9021618CurrentTrain: epoch 15, batch    17 | loss: 20.1480358CurrentTrain: epoch 15, batch    18 | loss: 8.6621105CurrentTrain: epoch 15, batch    19 | loss: 10.7210196CurrentTrain: epoch 15, batch    20 | loss: 11.6874776CurrentTrain: epoch 15, batch    21 | loss: 15.4222021CurrentTrain: epoch 15, batch    22 | loss: 18.4579074CurrentTrain: epoch 15, batch    23 | loss: 26.5813273CurrentTrain: epoch 15, batch    24 | loss: 11.0264232CurrentTrain: epoch 15, batch    25 | loss: 27.4918721CurrentTrain: epoch 15, batch    26 | loss: 10.2729142CurrentTrain: epoch 15, batch    27 | loss: 16.1635244CurrentTrain: epoch 15, batch    28 | loss: 10.2142508CurrentTrain: epoch 15, batch    29 | loss: 10.3547167CurrentTrain: epoch 15, batch    30 | loss: 11.8503313CurrentTrain: epoch 15, batch    31 | loss: 16.2805547CurrentTrain: epoch 15, batch    32 | loss: 19.0189301CurrentTrain: epoch 15, batch    33 | loss: 27.3895205CurrentTrain: epoch 15, batch    34 | loss: 13.1376234CurrentTrain: epoch 15, batch    35 | loss: 15.8082720CurrentTrain: epoch 15, batch    36 | loss: 14.9155676CurrentTrain: epoch 15, batch    37 | loss: 12.2654613CurrentTrain: epoch 15, batch    38 | loss: 9.4189515CurrentTrain: epoch 15, batch    39 | loss: 11.3281063CurrentTrain: epoch 15, batch    40 | loss: 10.2266009CurrentTrain: epoch 15, batch    41 | loss: 12.2300084CurrentTrain: epoch 15, batch    42 | loss: 9.6736855CurrentTrain: epoch 15, batch    43 | loss: 12.5484111CurrentTrain: epoch 15, batch    44 | loss: 12.4321557CurrentTrain: epoch 15, batch    45 | loss: 17.4558391CurrentTrain: epoch 15, batch    46 | loss: 12.9118499CurrentTrain: epoch 15, batch    47 | loss: 9.0229440CurrentTrain: epoch 15, batch    48 | loss: 15.5923778CurrentTrain: epoch 15, batch    49 | loss: 11.9874374CurrentTrain: epoch 15, batch    50 | loss: 9.4468355CurrentTrain: epoch 15, batch    51 | loss: 12.1087761CurrentTrain: epoch 15, batch    52 | loss: 19.3720640CurrentTrain: epoch 15, batch    53 | loss: 10.0303451CurrentTrain: epoch 15, batch    54 | loss: 10.8493163CurrentTrain: epoch 15, batch    55 | loss: 18.8757199CurrentTrain: epoch 15, batch    56 | loss: 20.4023664CurrentTrain: epoch 15, batch    57 | loss: 16.8547328CurrentTrain: epoch 15, batch    58 | loss: 9.8906988CurrentTrain: epoch 15, batch    59 | loss: 12.7080021CurrentTrain: epoch 15, batch    60 | loss: 11.3721080CurrentTrain: epoch 15, batch    61 | loss: 10.3835562CurrentTrain: epoch  7, batch    62 | loss: 8.0540887CurrentTrain: epoch 15, batch     0 | loss: 29.3072765CurrentTrain: epoch 15, batch     1 | loss: 11.9115852CurrentTrain: epoch 15, batch     2 | loss: 11.6372106CurrentTrain: epoch 15, batch     3 | loss: 13.3500502CurrentTrain: epoch 15, batch     4 | loss: 10.1944773CurrentTrain: epoch 15, batch     5 | loss: 26.4078929CurrentTrain: epoch 15, batch     6 | loss: 14.9644827CurrentTrain: epoch 15, batch     7 | loss: 14.8784661CurrentTrain: epoch 15, batch     8 | loss: 17.2869692CurrentTrain: epoch 15, batch     9 | loss: 8.3495990CurrentTrain: epoch 15, batch    10 | loss: 34.0805315CurrentTrain: epoch 15, batch    11 | loss: 17.5880224CurrentTrain: epoch 15, batch    12 | loss: 12.4615230CurrentTrain: epoch 15, batch    13 | loss: 10.4342570CurrentTrain: epoch 15, batch    14 | loss: 9.6182723CurrentTrain: epoch 15, batch    15 | loss: 14.3399210CurrentTrain: epoch 15, batch    16 | loss: 8.8433555CurrentTrain: epoch 15, batch    17 | loss: 8.5903942CurrentTrain: epoch 15, batch    18 | loss: 10.6790923CurrentTrain: epoch 15, batch    19 | loss: 17.1218527CurrentTrain: epoch 15, batch    20 | loss: 10.9341425CurrentTrain: epoch 15, batch    21 | loss: 24.3965920CurrentTrain: epoch 15, batch    22 | loss: 18.2390726CurrentTrain: epoch 15, batch    23 | loss: 10.8837472CurrentTrain: epoch 15, batch    24 | loss: 16.9400659CurrentTrain: epoch 15, batch    25 | loss: 30.5535144CurrentTrain: epoch 15, batch    26 | loss: 9.5486753CurrentTrain: epoch 15, batch    27 | loss: 14.7027063CurrentTrain: epoch 15, batch    28 | loss: 14.2847577CurrentTrain: epoch 15, batch    29 | loss: 10.2322200CurrentTrain: epoch 15, batch    30 | loss: 12.6631191CurrentTrain: epoch 15, batch    31 | loss: 12.7332597CurrentTrain: epoch 15, batch    32 | loss: 9.8283312CurrentTrain: epoch 15, batch    33 | loss: 11.9191110CurrentTrain: epoch 15, batch    34 | loss: 13.6441236CurrentTrain: epoch 15, batch    35 | loss: 28.6249489CurrentTrain: epoch 15, batch    36 | loss: 17.9075521CurrentTrain: epoch 15, batch    37 | loss: 12.3633346CurrentTrain: epoch 15, batch    38 | loss: 16.5891701CurrentTrain: epoch 15, batch    39 | loss: 11.7978670CurrentTrain: epoch 15, batch    40 | loss: 24.9439738CurrentTrain: epoch 15, batch    41 | loss: 23.9948866CurrentTrain: epoch 15, batch    42 | loss: 15.7519079CurrentTrain: epoch 15, batch    43 | loss: 15.7448162CurrentTrain: epoch 15, batch    44 | loss: 9.2035828CurrentTrain: epoch 15, batch    45 | loss: 16.7451693CurrentTrain: epoch 15, batch    46 | loss: 12.5329172CurrentTrain: epoch 15, batch    47 | loss: 11.6947664CurrentTrain: epoch 15, batch    48 | loss: 11.4242908CurrentTrain: epoch 15, batch    49 | loss: 17.9058361CurrentTrain: epoch 15, batch    50 | loss: 11.4917802CurrentTrain: epoch 15, batch    51 | loss: 22.7284811CurrentTrain: epoch 15, batch    52 | loss: 12.8662009CurrentTrain: epoch 15, batch    53 | loss: 14.9147164CurrentTrain: epoch 15, batch    54 | loss: 17.7419395CurrentTrain: epoch 15, batch    55 | loss: 17.3961287CurrentTrain: epoch 15, batch    56 | loss: 15.9277369CurrentTrain: epoch 15, batch    57 | loss: 13.6229935CurrentTrain: epoch 15, batch    58 | loss: 14.0612641CurrentTrain: epoch 15, batch    59 | loss: 12.9139938CurrentTrain: epoch 15, batch    60 | loss: 11.2187558CurrentTrain: epoch 15, batch    61 | loss: 28.3133803CurrentTrain: epoch  7, batch    62 | loss: 8.2698400CurrentTrain: epoch 15, batch     0 | loss: 9.2126419CurrentTrain: epoch 15, batch     1 | loss: 12.3061591CurrentTrain: epoch 15, batch     2 | loss: 9.6155544CurrentTrain: epoch 15, batch     3 | loss: 12.1613457CurrentTrain: epoch 15, batch     4 | loss: 16.4583551CurrentTrain: epoch 15, batch     5 | loss: 18.0511643CurrentTrain: epoch 15, batch     6 | loss: 13.3381761CurrentTrain: epoch 15, batch     7 | loss: 13.5130822CurrentTrain: epoch 15, batch     8 | loss: 14.9134103CurrentTrain: epoch 15, batch     9 | loss: 10.8151660CurrentTrain: epoch 15, batch    10 | loss: 11.0178059CurrentTrain: epoch 15, batch    11 | loss: 18.7725833CurrentTrain: epoch 15, batch    12 | loss: 11.1449703CurrentTrain: epoch 15, batch    13 | loss: 20.1058932CurrentTrain: epoch 15, batch    14 | loss: 11.3163657CurrentTrain: epoch 15, batch    15 | loss: 12.3970797CurrentTrain: epoch 15, batch    16 | loss: 12.5074149CurrentTrain: epoch 15, batch    17 | loss: 9.0933830CurrentTrain: epoch 15, batch    18 | loss: 16.8122369CurrentTrain: epoch 15, batch    19 | loss: 10.4247936CurrentTrain: epoch 15, batch    20 | loss: 12.6871088CurrentTrain: epoch 15, batch    21 | loss: 12.5291907CurrentTrain: epoch 15, batch    22 | loss: 13.1302937CurrentTrain: epoch 15, batch    23 | loss: 18.3058234CurrentTrain: epoch 15, batch    24 | loss: 12.4261826CurrentTrain: epoch 15, batch    25 | loss: 19.3613106CurrentTrain: epoch 15, batch    26 | loss: 12.9314787CurrentTrain: epoch 15, batch    27 | loss: 12.0488018CurrentTrain: epoch 15, batch    28 | loss: 14.7173908CurrentTrain: epoch 15, batch    29 | loss: 21.8083085CurrentTrain: epoch 15, batch    30 | loss: 20.1793025CurrentTrain: epoch 15, batch    31 | loss: 16.1634429CurrentTrain: epoch 15, batch    32 | loss: 14.6167527CurrentTrain: epoch 15, batch    33 | loss: 10.0393075CurrentTrain: epoch 15, batch    34 | loss: 17.1937503CurrentTrain: epoch 15, batch    35 | loss: 11.2154120CurrentTrain: epoch 15, batch    36 | loss: 15.7520485CurrentTrain: epoch 15, batch    37 | loss: 17.4224650CurrentTrain: epoch 15, batch    38 | loss: 15.8174298CurrentTrain: epoch 15, batch    39 | loss: 12.0402666CurrentTrain: epoch 15, batch    40 | loss: 10.7704643CurrentTrain: epoch 15, batch    41 | loss: 8.7460824CurrentTrain: epoch 15, batch    42 | loss: 10.6544930CurrentTrain: epoch 15, batch    43 | loss: 10.6269567CurrentTrain: epoch 15, batch    44 | loss: 17.3630371CurrentTrain: epoch 15, batch    45 | loss: 18.6016051CurrentTrain: epoch 15, batch    46 | loss: 16.1424047CurrentTrain: epoch 15, batch    47 | loss: 9.5894953CurrentTrain: epoch 15, batch    48 | loss: 16.7151437CurrentTrain: epoch 15, batch    49 | loss: 18.8300440CurrentTrain: epoch 15, batch    50 | loss: 9.7358348CurrentTrain: epoch 15, batch    51 | loss: 15.6701631CurrentTrain: epoch 15, batch    52 | loss: 11.9247610CurrentTrain: epoch 15, batch    53 | loss: 15.4040602CurrentTrain: epoch 15, batch    54 | loss: 13.5876229CurrentTrain: epoch 15, batch    55 | loss: 14.7444766CurrentTrain: epoch 15, batch    56 | loss: 14.5478170CurrentTrain: epoch 15, batch    57 | loss: 15.0171941CurrentTrain: epoch 15, batch    58 | loss: 14.5343041CurrentTrain: epoch 15, batch    59 | loss: 11.0919481CurrentTrain: epoch 15, batch    60 | loss: 26.1282871CurrentTrain: epoch 15, batch    61 | loss: 13.0106933CurrentTrain: epoch  7, batch    62 | loss: 15.5280715CurrentTrain: epoch 15, batch     0 | loss: 10.5148076CurrentTrain: epoch 15, batch     1 | loss: 14.1243526CurrentTrain: epoch 15, batch     2 | loss: 8.1557551CurrentTrain: epoch 15, batch     3 | loss: 14.7245013CurrentTrain: epoch 15, batch     4 | loss: 22.7406390CurrentTrain: epoch 15, batch     5 | loss: 16.4034025CurrentTrain: epoch 15, batch     6 | loss: 10.3756009CurrentTrain: epoch 15, batch     7 | loss: 21.7320900CurrentTrain: epoch 15, batch     8 | loss: 10.1903838CurrentTrain: epoch 15, batch     9 | loss: 16.7324425CurrentTrain: epoch 15, batch    10 | loss: 17.1846150CurrentTrain: epoch 15, batch    11 | loss: 12.3509210CurrentTrain: epoch 15, batch    12 | loss: 8.1855212CurrentTrain: epoch 15, batch    13 | loss: 24.6067904CurrentTrain: epoch 15, batch    14 | loss: 10.4128446CurrentTrain: epoch 15, batch    15 | loss: 12.6404288CurrentTrain: epoch 15, batch    16 | loss: 17.8518308CurrentTrain: epoch 15, batch    17 | loss: 12.8865727CurrentTrain: epoch 15, batch    18 | loss: 17.1999603CurrentTrain: epoch 15, batch    19 | loss: 26.5737824CurrentTrain: epoch 15, batch    20 | loss: 9.6259178CurrentTrain: epoch 15, batch    21 | loss: 14.5608441CurrentTrain: epoch 15, batch    22 | loss: 10.5375395CurrentTrain: epoch 15, batch    23 | loss: 41.8084820CurrentTrain: epoch 15, batch    24 | loss: 14.8523625CurrentTrain: epoch 15, batch    25 | loss: 26.6841468CurrentTrain: epoch 15, batch    26 | loss: 15.2157023CurrentTrain: epoch 15, batch    27 | loss: 18.3189794CurrentTrain: epoch 15, batch    28 | loss: 11.1807689CurrentTrain: epoch 15, batch    29 | loss: 23.0933459CurrentTrain: epoch 15, batch    30 | loss: 22.3042377CurrentTrain: epoch 15, batch    31 | loss: 11.8003008CurrentTrain: epoch 15, batch    32 | loss: 9.3007842CurrentTrain: epoch 15, batch    33 | loss: 13.3170824CurrentTrain: epoch 15, batch    34 | loss: 15.8240328CurrentTrain: epoch 15, batch    35 | loss: 17.0947103CurrentTrain: epoch 15, batch    36 | loss: 12.7430998CurrentTrain: epoch 15, batch    37 | loss: 8.4756589CurrentTrain: epoch 15, batch    38 | loss: 13.0062330CurrentTrain: epoch 15, batch    39 | loss: 20.9392873CurrentTrain: epoch 15, batch    40 | loss: 11.3141844CurrentTrain: epoch 15, batch    41 | loss: 12.5209108CurrentTrain: epoch 15, batch    42 | loss: 14.2273284CurrentTrain: epoch 15, batch    43 | loss: 16.0824936CurrentTrain: epoch 15, batch    44 | loss: 16.1625436CurrentTrain: epoch 15, batch    45 | loss: 19.4287855CurrentTrain: epoch 15, batch    46 | loss: 11.9715330CurrentTrain: epoch 15, batch    47 | loss: 12.9049439CurrentTrain: epoch 15, batch    48 | loss: 14.1834519CurrentTrain: epoch 15, batch    49 | loss: 16.0221105CurrentTrain: epoch 15, batch    50 | loss: 11.7882194CurrentTrain: epoch 15, batch    51 | loss: 14.2321778CurrentTrain: epoch 15, batch    52 | loss: 13.8764967CurrentTrain: epoch 15, batch    53 | loss: 10.2961153CurrentTrain: epoch 15, batch    54 | loss: 18.5471773CurrentTrain: epoch 15, batch    55 | loss: 11.3696004CurrentTrain: epoch 15, batch    56 | loss: 17.3443343CurrentTrain: epoch 15, batch    57 | loss: 14.7243108CurrentTrain: epoch 15, batch    58 | loss: 20.7283661CurrentTrain: epoch 15, batch    59 | loss: 15.2106028CurrentTrain: epoch 15, batch    60 | loss: 14.5691241CurrentTrain: epoch 15, batch    61 | loss: 23.2846071CurrentTrain: epoch  7, batch    62 | loss: 9.3343582
#### movement ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "movement" means literary, artistic, scientific or philosophical movement or scene associated with this person or work.
Here is an example:
Relation: movement
Context: the theme of the loss of identity in crowds in this painting can be seen as a precursor to expressionist painting , particularly the work of james ensor .
Head Entity: james ensor
Tail Entity: expressionist
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "movement":
Relation: movement  
Context: The surrealist movement, which sought to challenge the conventional perception of reality, was significantly influenced by the works of Salvador Dalí.  
Head Entity: Salvador Dalí  
Tail Entity: surrealist  

Relation: movement  
Context: The emergence of the romantic movement in literature was marked by the profound impact of poets like William Wordsworth and Samuel Taylor Coleridge.  
Head Entity: William Wordsworth  
Tail Entity: romantic  
#### located in the administrative territorial entity ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "located in the administrative territorial entity" means the item is located on the territory of the following administrative entity.
Here is an example:
Relation: located in the administrative territorial entity
Context: lyons hill is a restored village , and former parish with church , now part of the community of ardclough in north county kildare .
Head Entity: ardclough
Tail Entity: county kildare
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "located in the administrative territorial entity":
Relation: located in the administrative territorial entity  
Context: The city of San Francisco is known for its iconic Golden Gate Bridge and is part of the larger San Francisco County in California.  
Head Entity: San Francisco  
Tail Entity: San Francisco County  

Relation: located in the administrative territorial entity  
Context: The historic town of Bath is famous for its Roman baths and is situated within the unitary authority of Bath and North East Somerset.  
Head Entity: Bath  
Tail Entity: Bath and North East Somerset  
#### head of government ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "head of government" means head of the executive power of this town, city, municipality, state, country, or other governmental body.
Here is an example:
Relation: head of government
Context: on february 10 , 2010 , stefan mappus was elected minister - president of baden - württemberg , and asked that all ministers continue their roles .
Head Entity: baden - württemberg
Tail Entity: stefan mappus
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "head of government":
Relation: head of government  
Context: in 2021, jacinda ardern was re-elected as the prime minister of new zealand, leading the labor party to a significant victory in the general elections.  
Head Entity: new zealand  
Tail Entity: jacinda ardern  

Relation: head of government  
Context: during the annual summit, justin trudeau addressed the nation as the prime minister of canada, outlining his government's plans for the upcoming year.  
Head Entity: canada  
Tail Entity: justin trudeau  
#### member of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "member of" means organization, club or musical group to which the subject belongs.
Here is an example:
Relation: member of
Context: hank henshaw became a member of the sinestro corps during the sinestro corps war .
Head Entity: hank henshaw
Tail Entity: sinestro corps
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "member of":
Relation: member of  
Context: after years of dedication, she was finally recognized as a member of the national ballet company.  
Head Entity: she  
Tail Entity: national ballet company  

Relation: member of  
Context: the renowned scientist was proud to be a member of the international research consortium focused on climate change.  
Head Entity: the renowned scientist  
Tail Entity: international research consortium  
#### crosses ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "crosses" means obstacle which this bridge crosses over or this tunnel goes under.
Here is an example:
Relation: crosses
Context: the belleville turnpike bridge ( also known as the rutgers street bridge ) crosses the passaic river , connecting belleville to north arlington .
Head Entity: belleville turnpike bridge
Tail Entity: passaic river
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "crosses":
Relation: crosses  
Context: the golden gate bridge crosses the golden gate strait, linking san francisco to marin county.  
Head Entity: golden gate bridge  
Tail Entity: golden gate strait  

Relation: crosses  
Context: the london bridge crosses the river thames, providing a route between the city of london and southwark.  
Head Entity: london bridge  
Tail Entity: river thames  
#### tributary ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "tributary" means watercourse that flows into an other one.
Here is an example:
Relation: tributary
Context: it is on the panj river just downstream from the mouth of the bartang river at the point where the panj briefly turns west before resuming its northerly course .
Head Entity: panj river
Tail Entity: bartang river
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "tributary":
Relation: tributary  
Context: The Missouri River, which is a major tributary of the Mississippi River, flows through several states before joining the larger watercourse.  
Head Entity: Missouri River  
Tail Entity: Mississippi River  

Relation: tributary  
Context: The Rio Grande serves as a natural border between the United States and Mexico, and it has several tributaries, including the Pecos River.  
Head Entity: Rio Grande  
Tail Entity: Pecos River  
#### position held ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "position held" means subject currently or formerly holds the object position or public office.
Here is an example:
Relation: position held
Context: the 16th g7 summit was the first for japanese prime minister toshiki kaifu .
Head Entity: toshiki kaifu
Tail Entity: japanese prime minister
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "position held":
Relation: position held  
Context: after serving as the mayor of new york city for eight years, bill de blasio announced his candidacy for governor of new york.  
Head Entity: bill de blasio  
Tail Entity: mayor of new york city  

Relation: position held  
Context: during her tenure as the secretary of state, hillary clinton played a crucial role in shaping u.s. foreign policy.  
Head Entity: hillary clinton  
Tail Entity: secretary of state  
#### developer ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "developer" means organization or person that developed the item.
Here is an example:
Relation: developer
Context: unlike its predecessor , " torchlight ii " features cinematic sequences , which are produced by klei entertainment , the developer of " do n't starve " , " eets " and " shank " .
Head Entity: shank
Tail Entity: klei entertainment
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "developer":
Relation: developer  
Context: The popular game "Hollow Knight" was created by Team Cherry, the developer known for its stunning art style and challenging gameplay.  
Head Entity: Hollow Knight  
Tail Entity: Team Cherry  

Relation: developer  
Context: "The Witcher 3: Wild Hunt" was developed by CD Projekt Red, a company that has gained acclaim for its immersive storytelling and expansive open world.  
Head Entity: The Witcher 3: Wild Hunt  
Tail Entity: CD Projekt Red  
#### location of formation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "location of formation" means location where a group or organization was formed.
Here is an example:
Relation: location of formation
Context: dean reis , meanwhile , performs with the heartaches , while rubalcaba plays in earthless and is part - owner of the independent record store thirsty moon records in the hillcrest area of san diego .
Head Entity: earthless
Tail Entity: san diego
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "location of formation":
Relation: location of formation  
Context: the tech startup was founded in a small garage in silicon valley, where innovative ideas flourished and many successful companies began their journey.  
Head Entity: tech startup  
Tail Entity: silicon valley  

Relation: location of formation  
Context: the famous rock band originated in a vibrant music scene in nashville, where they honed their skills and gained a loyal following.  
Head Entity: rock band  
Tail Entity: nashville  
#### country of origin ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "country of origin" means country of origin of this item (creative work, food, phrase, product, etc.).
Here is an example:
Relation: country of origin
Context: in the fall of 1996 , now consisting of trevor paglen , nicolas lampert , and sean thomas , noisegate embarked on their first major tour of the united states , with filmmaker laura klein .
Head Entity: noisegate
Tail Entity: united states
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "country of origin":
Relation: country of origin  
Context: The famous dish known as paella originated in the region of Valencia, where it has been a staple for centuries, showcasing the rich culinary traditions of Spain.  
Head Entity: paella  
Tail Entity: Spain  

Relation: country of origin  
Context: The iconic brand Rolex is renowned for its luxury watches, which are meticulously crafted in Switzerland, reflecting the country's long-standing reputation for precision and quality in watchmaking.  
Head Entity: Rolex  
Tail Entity: Switzerland  
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 89.06%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 88.75%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 88.54%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 89.29%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 91.67%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 91.25%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 90.91%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 91.67%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 91.83%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 91.96%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 92.08%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 92.58%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 93.01%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 93.06%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 93.42%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 93.47%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 94.01%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 93.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 93.99%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 93.98%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 93.97%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 93.97%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 94.17%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 94.35%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 94.53%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 94.70%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 94.85%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 94.82%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 94.62%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 94.76%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 94.74%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 94.87%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 95.00%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 95.12%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 95.24%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 95.35%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 95.45%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 95.56%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 95.65%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 95.61%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 95.57%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 95.66%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 95.75%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 95.71%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 95.55%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 95.52%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 95.60%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 95.34%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 95.31%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 95.39%   [EVAL] batch:   57 | acc: 100.00%,  total acc: 95.47%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 95.55%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 95.52%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 95.59%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 95.56%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 94.84%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 89.06%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 88.75%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 88.54%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 89.29%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 91.67%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 91.25%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 90.91%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 91.67%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 91.83%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 91.96%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 92.08%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 92.58%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 93.01%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 93.06%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 93.42%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 93.47%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 94.01%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 93.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 93.99%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 93.98%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 93.97%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 93.97%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 94.17%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 94.35%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 94.53%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 94.70%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 94.85%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 94.82%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 94.62%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 94.76%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 94.74%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 94.87%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 95.00%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 95.12%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 95.24%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 95.35%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 95.45%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 95.56%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 95.65%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 95.61%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 95.57%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 95.66%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 95.75%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 95.71%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 95.55%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 95.52%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 95.60%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 95.34%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 95.31%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 95.39%   [EVAL] batch:   57 | acc: 100.00%,  total acc: 95.47%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 95.55%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 95.52%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 95.59%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 95.56%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 94.84%   
cur_acc:  ['0.9484']
his_acc:  ['0.9484']
CurrentTrain: epoch 15, batch     0 | loss: 16.3075566CurrentTrain: epoch 15, batch     1 | loss: 24.5070891CurrentTrain: epoch 15, batch     2 | loss: 26.2216572CurrentTrain: epoch  1, batch     3 | loss: 10.8067085CurrentTrain: epoch 15, batch     0 | loss: 17.8465700CurrentTrain: epoch 15, batch     1 | loss: 14.4983070CurrentTrain: epoch 15, batch     2 | loss: 12.1383234CurrentTrain: epoch  1, batch     3 | loss: 7.2845150CurrentTrain: epoch 15, batch     0 | loss: 13.8103379CurrentTrain: epoch 15, batch     1 | loss: 11.9615150CurrentTrain: epoch 15, batch     2 | loss: 12.2288068CurrentTrain: epoch  1, batch     3 | loss: 5.6071692CurrentTrain: epoch 15, batch     0 | loss: 11.2080526CurrentTrain: epoch 15, batch     1 | loss: 9.9853163CurrentTrain: epoch 15, batch     2 | loss: 10.9724007CurrentTrain: epoch  1, batch     3 | loss: 6.1473689CurrentTrain: epoch 15, batch     0 | loss: 12.3087209CurrentTrain: epoch 15, batch     1 | loss: 15.0564927CurrentTrain: epoch 15, batch     2 | loss: 12.9753377CurrentTrain: epoch  1, batch     3 | loss: 7.7686149CurrentTrain: epoch 15, batch     0 | loss: 13.2166422CurrentTrain: epoch 15, batch     1 | loss: 15.7011690CurrentTrain: epoch 15, batch     2 | loss: 15.9096798CurrentTrain: epoch  1, batch     3 | loss: 7.0274432CurrentTrain: epoch 15, batch     0 | loss: 10.0910763CurrentTrain: epoch 15, batch     1 | loss: 13.6988359CurrentTrain: epoch 15, batch     2 | loss: 9.5476165CurrentTrain: epoch  1, batch     3 | loss: 6.6600983CurrentTrain: epoch 15, batch     0 | loss: 10.5495814CurrentTrain: epoch 15, batch     1 | loss: 8.0373827CurrentTrain: epoch 15, batch     2 | loss: 10.1083364CurrentTrain: epoch  1, batch     3 | loss: 5.5031269CurrentTrain: epoch 15, batch     0 | loss: 14.3645002CurrentTrain: epoch 15, batch     1 | loss: 8.9698526CurrentTrain: epoch 15, batch     2 | loss: 9.4933123CurrentTrain: epoch  1, batch     3 | loss: 7.7396561CurrentTrain: epoch 15, batch     0 | loss: 8.8358661CurrentTrain: epoch 15, batch     1 | loss: 7.4449515CurrentTrain: epoch 15, batch     2 | loss: 12.2248049CurrentTrain: epoch  1, batch     3 | loss: 6.1761804
#### member of political party ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "member of political party" means the political party of which a person is or has been a member or otherwise affiliated.
Here is an example:
Relation: member of political party
Context: dresser is an analog to anton drexler , the founder of the nazi party which was then hijacked by adolf hitler .
Head Entity: anton drexler
Tail Entity: nazi party
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "member of political party":
Relation: member of political party  
Context: After serving as a senator, john doe became a prominent member of the democratic party, advocating for various social reforms.  
Head Entity: john doe  
Tail Entity: democratic party  

Relation: member of political party  
Context: During his tenure as a governor, jane smith was an active member of the republican party, influencing key legislation in the state.  
Head Entity: jane smith  
Tail Entity: republican party  
#### after a work by ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "after a work by" means artist whose work strongly inspired/ was copied in this item.
Here is an example:
Relation: after a work by
Context: it is a partly fictional biography , adapted by shane connaughton and jim sheridan from the book " my left foot " by christy brown .
Head Entity: my left foot
Tail Entity: christy brown
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "after a work by":
Relation: after a work by  
Context: the film is based on the novel "the great gatsby" by f. scott fitzgerald, capturing the essence of the roaring twenties.  
Head Entity: the great gatsby  
Tail Entity: f. scott fitzgerald  

Relation: after a work by  
Context: this musical is inspired by the play "hamlet" written by william shakespeare, reimagining the classic tragedy in a contemporary setting.  
Head Entity: hamlet  
Tail Entity: william shakespeare  
#### notable work ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "notable work" means notable scientific, artistic or literary work, or other work of significance among subject's works.
Here is an example:
Relation: notable work
Context: the influential victorian art critic john ruskin expounded theories about social justice in " unto this last " ( 1860 ) .
Head Entity: john ruskin
Tail Entity: unto this last
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "notable work":
Relation: notable work  
Context: the renowned physicist albert einstein published his groundbreaking theory of relativity in "on the electrodynamics of moving bodies" (1905).  
Head Entity: albert einstein  
Tail Entity: on the electrodynamics of moving bodies  

Relation: notable work  
Context: the celebrated author j.k. rowling captivated readers with her fantasy series, particularly "harry potter and the philosopher's stone" (1997).  
Head Entity: j.k. rowling  
Tail Entity: harry potter and the philosopher's stone  
#### headquarters location ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "headquarters location" means city, where an organization's headquarters is or has been situated.
Here is an example:
Relation: headquarters location
Context: in 1986 fram traveled to poland to play against katowice in a very even duel fram eventually lost . sparta prague came to reykjavík 1987 to play against fram reykjavik at laugardalsvöllur stadium .
Head Entity: fram reykjavik
Tail Entity: reykjavík
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "headquarters location":
Relation: headquarters location  
Context: in 2001, the tech company apple inc. moved its headquarters to cupertino, california, where it has since developed numerous innovative products.  
Head Entity: apple inc.  
Tail Entity: cupertino  

Relation: headquarters location  
Context: after several years of expansion, the multinational corporation unilever established its headquarters in rotterdam, netherlands, to better manage its global operations.  
Head Entity: unilever  
Tail Entity: rotterdam  
#### taxon rank ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "taxon rank" means level in a taxonomic hierarchy.
Here is an example:
Relation: taxon rank
Context: a number of species that were placed in the family phyllophoridae by thander in 1989 and 1990 are now included in cucumariidae .
Head Entity: phyllophoridae
Tail Entity: family
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "taxon rank":
Relation: taxon rank  
Context: The genus Canis includes several species, with Canis lupus being the most recognized as the wolf, which belongs to the family Canidae.  
Head Entity: Canis  
Tail Entity: genus  

Relation: taxon rank  
Context: In the classification of plants, the family Rosaceae encompasses a variety of species, including roses and strawberries, which are categorized under the order Rosales.  
Head Entity: Rosaceae  
Tail Entity: family  
#### field of work ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "field of work" means specialization of a person or organization.
Here is an example:
Relation: field of work
Context: he later collaborated with vito volterra and explored laplace transforms and other parts of functional analysis .
Head Entity: vito volterra
Tail Entity: functional analysis
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "field of work":
Relation: field of work  
Context: Marie Curie conducted pioneering research on radioactivity, which laid the groundwork for advancements in nuclear physics.  
Head Entity: Marie Curie  
Tail Entity: nuclear physics  

Relation: field of work  
Context: The company specializes in artificial intelligence and machine learning, providing innovative solutions for various industries.  
Head Entity: The company  
Tail Entity: artificial intelligence
#### licensed to broadcast to ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "licensed to broadcast to" means place that a radio/TV station is licensed/required to broadcast to.
Here is an example:
Relation: licensed to broadcast to
Context: this early involvement in radio led him to buy many small , under performing radio stations beginning with wcos in columbia , south carolina , and turn them around with new formats and improved management .
Head Entity: wcos
Tail Entity: columbia , south carolina
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "licensed to broadcast to":
Relation: licensed to broadcast to  
Context: The new television station, KXYZ, has been granted permission to broadcast to the entire metropolitan area of San Francisco, ensuring that local viewers have access to a variety of programming.  
Head Entity: KXYZ  
Tail Entity: San Francisco  

Relation: licensed to broadcast to  
Context: After acquiring the rights, the radio station WABC can now officially broadcast to listeners in the greater New York City area, reaching millions of potential audience members.  
Head Entity: WABC  
Tail Entity: greater New York City area  
#### constellation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "constellation" means the area of the celestial sphere of which the subject is a part.
Here is an example:
Relation: constellation
Context: hd 32518 b is an extrasolar planet which orbits the k - type giant star hd 32518 , located approximately 383 light years away in the constellation camelopardalis .
Head Entity: hd 32518
Tail Entity: camelopardalis
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "constellation":
Relation: constellation  
Context: the star sirius, known as the brightest star in the night sky, is located in the constellation canis major and is approximately 8.6 light years away from Earth.  
Head Entity: sirius  
Tail Entity: canis major  

Relation: constellation  
Context: the andromeda galaxy, which is the nearest spiral galaxy to the milky way, can be found in the constellation andromeda, making it a prominent feature in the night sky.  
Head Entity: andromeda galaxy  
Tail Entity: andromeda  
#### platform ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "platform" means platform for which a work was developed or released, or the specific platform version of a software product.
Here is an example:
Relation: platform
Context: " toy soldiers : war chest " was released on august 11 , 2015 for windows , playstation 4 and xbox one .
Head Entity: toy soldiers : war chest
Tail Entity: xbox one
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "platform":
Relation: platform  
Context: "The Legend of Zelda: Breath of the Wild" was launched on March 3, 2017 for the Nintendo Switch and Wii U.  
Head Entity: The Legend of Zelda: Breath of the Wild  
Tail Entity: Nintendo Switch  

Relation: platform  
Context: "Cyberpunk 2077" was made available on December 10, 2020 for PC, PlayStation 4, and Xbox One.  
Head Entity: Cyberpunk 2077  
Tail Entity: PlayStation 4  
#### place served by transport hub ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "place served by transport hub" means territorial entity or entities served by this transport hub (airport, train station, etc.).
Here is an example:
Relation: place served by transport hub
Context: services to the island run from san juan 's luis muñoz marín international airport , ceiba airport or isla grande airport ( 20- to 30-minute flight ) .
Head Entity: isla grande airport
Tail Entity: san juan
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "place served by transport hub":
Relation: place served by transport hub  
Context: The city of Barcelona is well-connected to various destinations through the El Prat Airport, which offers flights to numerous European cities.  
Head Entity: El Prat Airport  
Tail Entity: Barcelona  

Relation: place served by transport hub  
Context: The central train station in Zurich provides access to multiple Swiss cities and international destinations, making it a key transport hub in the region.  
Head Entity: Zurich central train station  
Tail Entity: Zurich  
MemoryTrain:  epoch 15, batch     0 | loss: 10.6299725MemoryTrain:  epoch 15, batch     1 | loss: 7.9299384MemoryTrain:  epoch 15, batch     2 | loss: 5.6174896MemoryTrain:  epoch 11, batch     3 | loss: 14.9777203MemoryTrain:  epoch 15, batch     0 | loss: 11.4502085MemoryTrain:  epoch 15, batch     1 | loss: 8.1585352MemoryTrain:  epoch 15, batch     2 | loss: 11.5310370MemoryTrain:  epoch 11, batch     3 | loss: 6.1084100MemoryTrain:  epoch 15, batch     0 | loss: 8.6524331MemoryTrain:  epoch 15, batch     1 | loss: 5.0576116MemoryTrain:  epoch 15, batch     2 | loss: 6.8228970MemoryTrain:  epoch 11, batch     3 | loss: 8.1754114MemoryTrain:  epoch 15, batch     0 | loss: 3.0972407MemoryTrain:  epoch 15, batch     1 | loss: 4.6159180MemoryTrain:  epoch 15, batch     2 | loss: 4.6585467MemoryTrain:  epoch 11, batch     3 | loss: 4.5429488MemoryTrain:  epoch 15, batch     0 | loss: 4.0266336MemoryTrain:  epoch 15, batch     1 | loss: 5.7745255MemoryTrain:  epoch 15, batch     2 | loss: 4.4441444MemoryTrain:  epoch 11, batch     3 | loss: 3.5953527MemoryTrain:  epoch 15, batch     0 | loss: 3.6520395MemoryTrain:  epoch 15, batch     1 | loss: 4.2546560MemoryTrain:  epoch 15, batch     2 | loss: 2.7939567MemoryTrain:  epoch 11, batch     3 | loss: 6.9513552MemoryTrain:  epoch 15, batch     0 | loss: 4.0387078MemoryTrain:  epoch 15, batch     1 | loss: 4.9083016MemoryTrain:  epoch 15, batch     2 | loss: 4.6735586MemoryTrain:  epoch 11, batch     3 | loss: 2.4411477MemoryTrain:  epoch 15, batch     0 | loss: 4.8537560MemoryTrain:  epoch 15, batch     1 | loss: 3.0169464MemoryTrain:  epoch 15, batch     2 | loss: 10.4272695MemoryTrain:  epoch 11, batch     3 | loss: 2.3525544MemoryTrain:  epoch 15, batch     0 | loss: 5.8552274MemoryTrain:  epoch 15, batch     1 | loss: 7.4431696MemoryTrain:  epoch 15, batch     2 | loss: 7.7895355MemoryTrain:  epoch 11, batch     3 | loss: 2.5067561MemoryTrain:  epoch 15, batch     0 | loss: 4.0023553MemoryTrain:  epoch 15, batch     1 | loss: 4.7378486MemoryTrain:  epoch 15, batch     2 | loss: 8.7188597MemoryTrain:  epoch 11, batch     3 | loss: 2.0576341
[EVAL] batch:    0 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 96.88%   [EVAL] batch:    2 | acc: 100.00%,  total acc: 97.92%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 95.31%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 95.00%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 95.83%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 96.43%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 96.88%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 97.22%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 97.50%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 97.73%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 96.88%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 96.15%   [EVAL] batch:   13 | acc: 75.00%,  total acc: 94.64%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 92.92%   [EVAL] batch:   15 | acc: 62.50%,  total acc: 91.02%   [EVAL] batch:   16 | acc: 68.75%,  total acc: 89.71%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 88.19%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 86.84%   [EVAL] batch:   19 | acc: 56.25%,  total acc: 85.31%   [EVAL] batch:   20 | acc: 81.25%,  total acc: 85.12%   [EVAL] batch:   21 | acc: 62.50%,  total acc: 84.09%   [EVAL] batch:   22 | acc: 68.75%,  total acc: 83.42%   [EVAL] batch:   23 | acc: 50.00%,  total acc: 82.03%   [EVAL] batch:   24 | acc: 25.00%,  total acc: 79.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 80.53%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 81.92%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 82.54%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 83.12%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 83.67%   [EVAL] batch:   31 | acc: 87.50%,  total acc: 83.79%   [EVAL] batch:   32 | acc: 87.50%,  total acc: 83.90%   [EVAL] batch:   33 | acc: 68.75%,  total acc: 83.46%   [EVAL] batch:   34 | acc: 87.50%,  total acc: 83.57%   [EVAL] batch:   35 | acc: 62.50%,  total acc: 82.99%   [EVAL] batch:   36 | acc: 68.75%,  total acc: 82.60%   [EVAL] batch:   37 | acc: 75.00%,  total acc: 82.40%   [EVAL] batch:   38 | acc: 93.75%,  total acc: 82.69%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 83.12%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 83.54%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 83.93%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 84.30%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 84.52%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 84.86%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 85.19%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 85.51%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 85.81%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 86.10%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 86.38%   [EVAL] batch:   50 | acc: 81.25%,  total acc: 86.27%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 86.54%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 86.67%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 86.92%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 86.93%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 87.17%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 87.28%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 87.39%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 87.39%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 87.60%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 87.81%   [EVAL] batch:   61 | acc: 81.25%,  total acc: 87.70%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 87.10%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:    3 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 90.00%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 89.58%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 86.61%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 85.94%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 86.11%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 85.00%   [EVAL] batch:   10 | acc: 62.50%,  total acc: 82.95%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 82.81%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 83.17%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 83.93%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 84.58%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 85.55%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 86.40%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 86.81%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:   21 | acc: 75.00%,  total acc: 86.93%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 87.98%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 88.43%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 88.62%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 88.79%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 89.17%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 89.52%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 89.84%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 90.15%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 90.44%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 90.54%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 90.80%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 91.05%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 91.28%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 91.51%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 91.72%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 91.92%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 92.11%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 92.30%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 92.47%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 92.64%   [EVAL] batch:   45 | acc: 87.50%,  total acc: 92.53%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 92.29%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 92.32%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 92.47%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 92.38%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 92.40%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 92.43%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 92.45%   [EVAL] batch:   53 | acc: 87.50%,  total acc: 92.36%   [EVAL] batch:   54 | acc: 68.75%,  total acc: 91.93%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 91.85%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 91.89%   [EVAL] batch:   57 | acc: 100.00%,  total acc: 92.03%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 92.16%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 92.29%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 92.42%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 92.54%   [EVAL] batch:   62 | acc: 100.00%,  total acc: 92.66%   [EVAL] batch:   63 | acc: 93.75%,  total acc: 92.68%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 92.79%   [EVAL] batch:   65 | acc: 100.00%,  total acc: 92.90%   [EVAL] batch:   66 | acc: 81.25%,  total acc: 92.72%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 92.83%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 92.93%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 93.04%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 93.13%   [EVAL] batch:   71 | acc: 100.00%,  total acc: 93.23%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 93.32%   [EVAL] batch:   73 | acc: 93.75%,  total acc: 93.33%   [EVAL] batch:   74 | acc: 93.75%,  total acc: 93.33%   [EVAL] batch:   75 | acc: 68.75%,  total acc: 93.01%   [EVAL] batch:   76 | acc: 87.50%,  total acc: 92.94%   [EVAL] batch:   77 | acc: 68.75%,  total acc: 92.63%   [EVAL] batch:   78 | acc: 43.75%,  total acc: 92.01%   [EVAL] batch:   79 | acc: 68.75%,  total acc: 91.72%   [EVAL] batch:   80 | acc: 68.75%,  total acc: 91.44%   [EVAL] batch:   81 | acc: 56.25%,  total acc: 91.01%   [EVAL] batch:   82 | acc: 75.00%,  total acc: 90.81%   [EVAL] batch:   83 | acc: 62.50%,  total acc: 90.48%   [EVAL] batch:   84 | acc: 68.75%,  total acc: 90.22%   [EVAL] batch:   85 | acc: 56.25%,  total acc: 89.83%   [EVAL] batch:   86 | acc: 37.50%,  total acc: 89.22%   [EVAL] batch:   87 | acc: 68.75%,  total acc: 88.99%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 89.12%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 89.24%   [EVAL] batch:   90 | acc: 100.00%,  total acc: 89.35%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 89.47%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 89.58%   [EVAL] batch:   93 | acc: 100.00%,  total acc: 89.69%   [EVAL] batch:   94 | acc: 81.25%,  total acc: 89.61%   [EVAL] batch:   95 | acc: 81.25%,  total acc: 89.52%   [EVAL] batch:   96 | acc: 75.00%,  total acc: 89.37%   [EVAL] batch:   97 | acc: 68.75%,  total acc: 89.16%   [EVAL] batch:   98 | acc: 81.25%,  total acc: 89.08%   [EVAL] batch:   99 | acc: 50.00%,  total acc: 88.69%   [EVAL] batch:  100 | acc: 93.75%,  total acc: 88.74%   [EVAL] batch:  101 | acc: 100.00%,  total acc: 88.85%   [EVAL] batch:  102 | acc: 100.00%,  total acc: 88.96%   [EVAL] batch:  103 | acc: 100.00%,  total acc: 89.06%   [EVAL] batch:  104 | acc: 100.00%,  total acc: 89.17%   [EVAL] batch:  105 | acc: 93.75%,  total acc: 89.21%   [EVAL] batch:  106 | acc: 100.00%,  total acc: 89.31%   [EVAL] batch:  107 | acc: 100.00%,  total acc: 89.41%   [EVAL] batch:  108 | acc: 100.00%,  total acc: 89.51%   [EVAL] batch:  109 | acc: 100.00%,  total acc: 89.60%   [EVAL] batch:  110 | acc: 100.00%,  total acc: 89.70%   [EVAL] batch:  111 | acc: 100.00%,  total acc: 89.79%   [EVAL] batch:  112 | acc: 93.75%,  total acc: 89.82%   [EVAL] batch:  113 | acc: 87.50%,  total acc: 89.80%   [EVAL] batch:  114 | acc: 93.75%,  total acc: 89.84%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 89.92%   [EVAL] batch:  116 | acc: 93.75%,  total acc: 89.96%   [EVAL] batch:  117 | acc: 93.75%,  total acc: 89.99%   [EVAL] batch:  118 | acc: 93.75%,  total acc: 90.02%   [EVAL] batch:  119 | acc: 100.00%,  total acc: 90.10%   [EVAL] batch:  120 | acc: 87.50%,  total acc: 90.08%   [EVAL] batch:  121 | acc: 93.75%,  total acc: 90.11%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 90.19%   [EVAL] batch:  123 | acc: 87.50%,  total acc: 90.17%   [EVAL] batch:  124 | acc: 93.75%,  total acc: 90.20%   
cur_acc:  ['0.9484', '0.8710']
his_acc:  ['0.9484', '0.9020']
CurrentTrain: epoch 15, batch     0 | loss: 21.7091746CurrentTrain: epoch 15, batch     1 | loss: 27.8440457CurrentTrain: epoch 15, batch     2 | loss: 24.5744689CurrentTrain: epoch  1, batch     3 | loss: 13.5434940CurrentTrain: epoch 15, batch     0 | loss: 18.2026077CurrentTrain: epoch 15, batch     1 | loss: 22.9078139CurrentTrain: epoch 15, batch     2 | loss: 20.1804183CurrentTrain: epoch  1, batch     3 | loss: 8.9195022CurrentTrain: epoch 15, batch     0 | loss: 17.8232478CurrentTrain: epoch 15, batch     1 | loss: 13.7410998CurrentTrain: epoch 15, batch     2 | loss: 19.3570649CurrentTrain: epoch  1, batch     3 | loss: 9.1844300CurrentTrain: epoch 15, batch     0 | loss: 13.4860839CurrentTrain: epoch 15, batch     1 | loss: 17.2418582CurrentTrain: epoch 15, batch     2 | loss: 22.2415313CurrentTrain: epoch  1, batch     3 | loss: 9.1152511CurrentTrain: epoch 15, batch     0 | loss: 12.9287665CurrentTrain: epoch 15, batch     1 | loss: 12.9068531CurrentTrain: epoch 15, batch     2 | loss: 14.2872964CurrentTrain: epoch  1, batch     3 | loss: 17.8317209CurrentTrain: epoch 15, batch     0 | loss: 17.5571138CurrentTrain: epoch 15, batch     1 | loss: 12.3032880CurrentTrain: epoch 15, batch     2 | loss: 11.0936207CurrentTrain: epoch  1, batch     3 | loss: 8.1309903CurrentTrain: epoch 15, batch     0 | loss: 12.0597141CurrentTrain: epoch 15, batch     1 | loss: 20.2016868CurrentTrain: epoch 15, batch     2 | loss: 12.2575478CurrentTrain: epoch  1, batch     3 | loss: 8.3534175CurrentTrain: epoch 15, batch     0 | loss: 18.1033816CurrentTrain: epoch 15, batch     1 | loss: 12.2605330CurrentTrain: epoch 15, batch     2 | loss: 10.6900370CurrentTrain: epoch  1, batch     3 | loss: 6.5938233CurrentTrain: epoch 15, batch     0 | loss: 10.1137188CurrentTrain: epoch 15, batch     1 | loss: 14.6654847CurrentTrain: epoch 15, batch     2 | loss: 11.9987908CurrentTrain: epoch  1, batch     3 | loss: 8.2970266CurrentTrain: epoch 15, batch     0 | loss: 11.2977753CurrentTrain: epoch 15, batch     1 | loss: 10.7320833CurrentTrain: epoch 15, batch     2 | loss: 16.5122221CurrentTrain: epoch  1, batch     3 | loss: 7.4771096
#### located on terrain feature ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "located on terrain feature" means located on the specified physical/geophysical feature.
Here is an example:
Relation: located on terrain feature
Context: it is situated in the historic lower lusatia region , about south of beeskow , and north of cottbus .
Head Entity: cottbus
Tail Entity: lower lusatia
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "located on terrain feature":
Relation: located on terrain feature  
Context: The ancient ruins are found near the banks of the river Nile, surrounded by vast deserts and fertile lands.  
Head Entity: Nile  
Tail Entity: deserts  

Relation: located on terrain feature  
Context: The village is nestled in the foothills of the Rocky Mountains, offering stunning views of the surrounding landscape.  
Head Entity: village  
Tail Entity: Rocky Mountains  
#### participant of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "participant of" means event in which a person or organization was/is a participant.
Here is an example:
Relation: participant of
Context: tiago venâncio ( born august 19 , 1987 , setúbal ) is a freestyle swimmer from portugal , who represented his home country at the 2004 summer olympics in athens , greece .
Head Entity: tiago venâncio
Tail Entity: 2004 summer olympics
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "participant of":
Relation: participant of  
Context: the united nations climate change conference, held in paris in 2015, saw numerous world leaders and activists come together to discuss global warming.  
Head Entity: united nations climate change conference  
Tail Entity: paris agreement  

Relation: participant of  
Context: the 2022 fifa world cup in qatar featured teams from around the globe competing for the prestigious trophy.  
Head Entity: 2022 fifa world cup  
Tail Entity: qatar
#### director ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "director" means director of film, TV-series, stageplay, video game or similar.
Here is an example:
Relation: director
Context: " runaway reptar " was written by ali marie matheson and jon cooksey and directed by john holmquist and jim duffy .
Head Entity: runaway reptar
Tail Entity: john holmquist
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "director":
Relation: director  
Context: "Inception" was directed by Christopher Nolan and features a complex narrative structure that keeps viewers on the edge of their seats.  
Head Entity: Inception  
Tail Entity: Christopher Nolan  

Relation: director  
Context: The acclaimed stage play "Hamlet" was directed by Kenneth Branagh, who brought a fresh perspective to the classic Shakespearean tragedy.  
Head Entity: Hamlet  
Tail Entity: Kenneth Branagh  
#### owned by ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "owned by" means owner of the subject.
Here is an example:
Relation: owned by
Context: as of april 2013 , sony isp , known as so - net , released a new fibre service to tokyo for small businesses and homes and also made it available to six surrounding prefectures .
Head Entity: so - net
Tail Entity: sony
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "owned by":
Relation: owned by  
Context: In 2020, the popular video game company, Bungie, announced that it had become an independent studio, having previously been owned by Microsoft for several years.  
Head Entity: Bungie  
Tail Entity: Microsoft  

Relation: owned by  
Context: The luxury fashion brand Gucci, known for its high-end products, is owned by the French conglomerate Kering, which also owns several other prestigious labels.  
Head Entity: Gucci  
Tail Entity: Kering  
#### part of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "part of" means object of which the subject is a part.
Here is an example:
Relation: part of
Context: thessaly was home to extensive neolithic and chalcolithic cultures around 6000 bc–2500 bc ( see cardium pottery , dimini and sesklo ) .
Head Entity: cardium pottery
Tail Entity: neolithic
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "part of":
Relation: part of  
Context: The engine is a crucial part of the car's overall functionality, providing the necessary power to drive.  
Head Entity: engine  
Tail Entity: car  

Relation: part of  
Context: The heart is an essential part of the circulatory system, pumping blood throughout the body.  
Head Entity: heart  
Tail Entity: circulatory system  
#### occupant ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "occupant" means person or organization occupying property.
Here is an example:
Relation: occupant
Context: on 1 march , he made his senior debut in the first round of 2011 afc champions league group stage which tianjin teda beat k league side jeju united 1–0 at jeju world cup stadium .
Head Entity: jeju world cup stadium
Tail Entity: jeju united
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "occupant":
Relation: occupant  
Context: The Smith family has lived in the historic Victorian house on Maple Street for over a decade, making it their beloved home.  
Head Entity: Victorian house on Maple Street  
Tail Entity: Smith family  

Relation: occupant  
Context: The local community center serves as a hub for various activities, with the Greenfield Arts Organization using the space for their weekly workshops.  
Head Entity: local community center  
Tail Entity: Greenfield Arts Organization  
#### architect ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "architect" means person or architectural firm responsible for designing this building.
Here is an example:
Relation: architect
Context: reading town hall was built in several phases between 1786 and 1897 , although the principal facade was designed by alfred waterhouse in 1875 .
Head Entity: reading town hall
Tail Entity: alfred waterhouse
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "architect":
Relation: architect  
Context: the new art museum was designed by renowned architect zaha hadid, whose innovative style has transformed modern architecture.  
Head Entity: new art museum  
Tail Entity: zaha hadid  

Relation: architect  
Context: the iconic sydney opera house was the brainchild of architect jørn utzon, who won the design competition in 1957.  
Head Entity: sydney opera house  
Tail Entity: jørn utzon  
#### residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "residence" means the place where the person is or has been, resident.
Here is an example:
Relation: residence
Context: cyrus alexander was born in pennsylvania , and his family soon moved to illinois .
Head Entity: cyrus alexander
Tail Entity: illinois
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "residence":
Relation: residence  
Context: after living in new york for several years, emily decided to relocate to california for a fresh start.  
Head Entity: emily  
Tail Entity: california  

Relation: residence  
Context: during his childhood, michael spent most of his time in texas before moving to florida as a teenager.  
Head Entity: michael  
Tail Entity: florida  
#### composer ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "composer" means person who wrote the music.
Here is an example:
Relation: composer
Context: at a young age he was an accomplished cellist , performing at age seventeen the solo part of richard strauss 's " don quixote " .
Head Entity: don quixote
Tail Entity: richard strauss
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "composer":
Relation: composer  
Context: The renowned composer Ludwig van Beethoven created some of the most celebrated symphonies in classical music history.  
Head Entity: symphonies  
Tail Entity: Ludwig van Beethoven  

Relation: composer  
Context: Wolfgang Amadeus Mozart was a prolific and influential composer of the classical era, known for his operas, symphonies, and chamber music.  
Head Entity: operas  
Tail Entity: Wolfgang Amadeus Mozart  
#### location ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "location" means location of the object, structure or event.
Here is an example:
Relation: location
Context: one polish archer qualified for the women 's individual recurve by obtaining one of the eight olympic places available from the 2015 world archery championships in copenhagen , denmark .
Head Entity: 2015 world archery championships
Tail Entity: copenhagen
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "location":
Relation: location  
Context: The annual music festival will take place in the vibrant city of Austin, Texas, attracting thousands of fans from around the world.  
Head Entity: annual music festival  
Tail Entity: Austin, Texas  

Relation: location  
Context: The historic battle was fought near the banks of the river in Gettysburg, Pennsylvania, which is now a national park.  
Head Entity: historic battle  
Tail Entity: Gettysburg, Pennsylvania  
MemoryTrain:  epoch 15, batch     0 | loss: 8.5100460MemoryTrain:  epoch 15, batch     1 | loss: 4.9921567MemoryTrain:  epoch 15, batch     2 | loss: 7.9276945MemoryTrain:  epoch 15, batch     3 | loss: 4.6569208MemoryTrain:  epoch 15, batch     4 | loss: 7.9638589MemoryTrain:  epoch  9, batch     5 | loss: 3.8477732MemoryTrain:  epoch 15, batch     0 | loss: 5.7084341MemoryTrain:  epoch 15, batch     1 | loss: 13.5799529MemoryTrain:  epoch 15, batch     2 | loss: 4.7480576MemoryTrain:  epoch 15, batch     3 | loss: 8.9970893MemoryTrain:  epoch 15, batch     4 | loss: 4.8990039MemoryTrain:  epoch  9, batch     5 | loss: 2.3964654MemoryTrain:  epoch 15, batch     0 | loss: 8.0526095MemoryTrain:  epoch 15, batch     1 | loss: 5.8217913MemoryTrain:  epoch 15, batch     2 | loss: 6.8165816MemoryTrain:  epoch 15, batch     3 | loss: 4.3714009MemoryTrain:  epoch 15, batch     4 | loss: 4.7580874MemoryTrain:  epoch  9, batch     5 | loss: 5.2468726MemoryTrain:  epoch 15, batch     0 | loss: 2.9813613MemoryTrain:  epoch 15, batch     1 | loss: 4.0740766MemoryTrain:  epoch 15, batch     2 | loss: 4.0467847MemoryTrain:  epoch 15, batch     3 | loss: 6.3771517MemoryTrain:  epoch 15, batch     4 | loss: 5.3963928MemoryTrain:  epoch  9, batch     5 | loss: 3.2171646MemoryTrain:  epoch 15, batch     0 | loss: 4.0881157MemoryTrain:  epoch 15, batch     1 | loss: 2.4985892MemoryTrain:  epoch 15, batch     2 | loss: 5.0848781MemoryTrain:  epoch 15, batch     3 | loss: 3.6142493MemoryTrain:  epoch 15, batch     4 | loss: 2.7434831MemoryTrain:  epoch  9, batch     5 | loss: 3.1824976MemoryTrain:  epoch 15, batch     0 | loss: 9.2781625MemoryTrain:  epoch 15, batch     1 | loss: 2.1689742MemoryTrain:  epoch 15, batch     2 | loss: 2.7210848MemoryTrain:  epoch 15, batch     3 | loss: 4.3205280MemoryTrain:  epoch 15, batch     4 | loss: 5.0235056MemoryTrain:  epoch  9, batch     5 | loss: 4.1903992MemoryTrain:  epoch 15, batch     0 | loss: 2.4002328MemoryTrain:  epoch 15, batch     1 | loss: 6.2840431MemoryTrain:  epoch 15, batch     2 | loss: 2.1825523MemoryTrain:  epoch 15, batch     3 | loss: 2.3279491MemoryTrain:  epoch 15, batch     4 | loss: 4.3292399MemoryTrain:  epoch  9, batch     5 | loss: 3.9452015MemoryTrain:  epoch 15, batch     0 | loss: 3.3215626MemoryTrain:  epoch 15, batch     1 | loss: 3.2705824MemoryTrain:  epoch 15, batch     2 | loss: 3.3875774MemoryTrain:  epoch 15, batch     3 | loss: 5.0775646MemoryTrain:  epoch 15, batch     4 | loss: 2.5932825MemoryTrain:  epoch  9, batch     5 | loss: 1.4506395MemoryTrain:  epoch 15, batch     0 | loss: 3.6482584MemoryTrain:  epoch 15, batch     1 | loss: 2.5438386MemoryTrain:  epoch 15, batch     2 | loss: 5.4871753MemoryTrain:  epoch 15, batch     3 | loss: 5.0389792MemoryTrain:  epoch 15, batch     4 | loss: 2.3436456MemoryTrain:  epoch  9, batch     5 | loss: 2.6748289MemoryTrain:  epoch 15, batch     0 | loss: 2.3499315MemoryTrain:  epoch 15, batch     1 | loss: 3.0536820MemoryTrain:  epoch 15, batch     2 | loss: 2.3940061MemoryTrain:  epoch 15, batch     3 | loss: 2.7126972MemoryTrain:  epoch 15, batch     4 | loss: 2.4393571MemoryTrain:  epoch  9, batch     5 | loss: 4.1165192
[EVAL] batch:    0 | acc: 37.50%,  total acc: 37.50%   [EVAL] batch:    1 | acc: 56.25%,  total acc: 46.88%   [EVAL] batch:    2 | acc: 37.50%,  total acc: 43.75%   [EVAL] batch:    3 | acc: 50.00%,  total acc: 45.31%   [EVAL] batch:    4 | acc: 56.25%,  total acc: 47.50%   [EVAL] batch:    5 | acc: 50.00%,  total acc: 47.92%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 52.68%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 57.03%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 61.81%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 65.00%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 67.61%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 70.31%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 72.12%   [EVAL] batch:   13 | acc: 87.50%,  total acc: 73.21%   [EVAL] batch:   14 | acc: 87.50%,  total acc: 74.17%   [EVAL] batch:   15 | acc: 93.75%,  total acc: 75.39%   [EVAL] batch:   16 | acc: 87.50%,  total acc: 76.10%   [EVAL] batch:   17 | acc: 87.50%,  total acc: 76.74%   [EVAL] batch:   18 | acc: 75.00%,  total acc: 76.64%   [EVAL] batch:   19 | acc: 62.50%,  total acc: 75.94%   [EVAL] batch:   20 | acc: 56.25%,  total acc: 75.00%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 75.57%   [EVAL] batch:   22 | acc: 75.00%,  total acc: 75.54%   [EVAL] batch:   23 | acc: 56.25%,  total acc: 74.74%   [EVAL] batch:   24 | acc: 68.75%,  total acc: 74.50%   [EVAL] batch:   25 | acc: 18.75%,  total acc: 72.36%   [EVAL] batch:   26 | acc: 50.00%,  total acc: 71.53%   [EVAL] batch:   27 | acc: 25.00%,  total acc: 69.87%   [EVAL] batch:   28 | acc: 25.00%,  total acc: 68.32%   [EVAL] batch:   29 | acc: 37.50%,  total acc: 67.29%   [EVAL] batch:   30 | acc: 6.25%,  total acc: 65.32%   [EVAL] batch:   31 | acc: 81.25%,  total acc: 65.82%   [EVAL] batch:   32 | acc: 87.50%,  total acc: 66.48%   [EVAL] batch:   33 | acc: 75.00%,  total acc: 66.73%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 67.68%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 68.23%   [EVAL] batch:   36 | acc: 87.50%,  total acc: 68.75%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 69.41%   [EVAL] batch:   38 | acc: 87.50%,  total acc: 69.87%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 70.62%   [EVAL] batch:   40 | acc: 87.50%,  total acc: 71.04%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 71.58%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 72.09%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 72.59%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 72.92%   [EVAL] batch:   45 | acc: 68.75%,  total acc: 72.83%   [EVAL] batch:   46 | acc: 56.25%,  total acc: 72.47%   [EVAL] batch:   47 | acc: 62.50%,  total acc: 72.27%   [EVAL] batch:   48 | acc: 81.25%,  total acc: 72.45%   [EVAL] batch:   49 | acc: 75.00%,  total acc: 72.50%   [EVAL] batch:   50 | acc: 62.50%,  total acc: 72.30%   [EVAL] batch:   51 | acc: 81.25%,  total acc: 72.48%   [EVAL] batch:   52 | acc: 43.75%,  total acc: 71.93%   [EVAL] batch:   53 | acc: 75.00%,  total acc: 71.99%   [EVAL] batch:   54 | acc: 68.75%,  total acc: 71.93%   [EVAL] batch:   55 | acc: 75.00%,  total acc: 71.99%   [EVAL] batch:   56 | acc: 56.25%,  total acc: 71.71%   [EVAL] batch:   57 | acc: 50.00%,  total acc: 71.34%   [EVAL] batch:   58 | acc: 62.50%,  total acc: 71.19%   [EVAL] batch:   59 | acc: 75.00%,  total acc: 71.25%   [EVAL] batch:   60 | acc: 50.00%,  total acc: 70.90%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 70.87%   [EVAL] batch:   62 | acc: 25.00%,  total acc: 70.14%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 85.94%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 83.75%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:    6 | acc: 50.00%,  total acc: 79.46%   [EVAL] batch:    7 | acc: 62.50%,  total acc: 77.34%   [EVAL] batch:    8 | acc: 75.00%,  total acc: 77.08%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 76.25%   [EVAL] batch:   10 | acc: 43.75%,  total acc: 73.30%   [EVAL] batch:   11 | acc: 50.00%,  total acc: 71.35%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 72.60%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 74.11%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 75.42%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 76.95%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 78.31%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 79.17%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 79.93%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 80.62%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 80.95%   [EVAL] batch:   21 | acc: 75.00%,  total acc: 80.68%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 81.52%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 81.77%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 82.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 82.69%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 83.33%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 83.71%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 84.05%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 84.58%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 85.08%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 85.55%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 85.98%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 86.40%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 86.61%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 86.98%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 87.33%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 87.66%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 87.98%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 88.28%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 88.57%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 88.84%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 89.10%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 89.35%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 89.44%   [EVAL] batch:   45 | acc: 87.50%,  total acc: 89.40%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 89.23%   [EVAL] batch:   47 | acc: 87.50%,  total acc: 89.19%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 89.41%   [EVAL] batch:   49 | acc: 81.25%,  total acc: 89.25%   [EVAL] batch:   50 | acc: 75.00%,  total acc: 88.97%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 89.06%   [EVAL] batch:   52 | acc: 87.50%,  total acc: 89.03%   [EVAL] batch:   53 | acc: 62.50%,  total acc: 88.54%   [EVAL] batch:   54 | acc: 50.00%,  total acc: 87.84%   [EVAL] batch:   55 | acc: 81.25%,  total acc: 87.72%   [EVAL] batch:   56 | acc: 81.25%,  total acc: 87.61%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 87.72%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 87.92%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 88.12%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 88.32%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 88.51%   [EVAL] batch:   62 | acc: 100.00%,  total acc: 88.69%   [EVAL] batch:   63 | acc: 93.75%,  total acc: 88.77%   [EVAL] batch:   64 | acc: 93.75%,  total acc: 88.85%   [EVAL] batch:   65 | acc: 100.00%,  total acc: 89.02%   [EVAL] batch:   66 | acc: 87.50%,  total acc: 88.99%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 89.15%   [EVAL] batch:   68 | acc: 93.75%,  total acc: 89.22%   [EVAL] batch:   69 | acc: 75.00%,  total acc: 89.02%   [EVAL] batch:   70 | acc: 87.50%,  total acc: 89.00%   [EVAL] batch:   71 | acc: 81.25%,  total acc: 88.89%   [EVAL] batch:   72 | acc: 81.25%,  total acc: 88.78%   [EVAL] batch:   73 | acc: 75.00%,  total acc: 88.60%   [EVAL] batch:   74 | acc: 68.75%,  total acc: 88.33%   [EVAL] batch:   75 | acc: 62.50%,  total acc: 87.99%   [EVAL] batch:   76 | acc: 81.25%,  total acc: 87.91%   [EVAL] batch:   77 | acc: 62.50%,  total acc: 87.58%   [EVAL] batch:   78 | acc: 50.00%,  total acc: 87.10%   [EVAL] batch:   79 | acc: 62.50%,  total acc: 86.80%   [EVAL] batch:   80 | acc: 56.25%,  total acc: 86.42%   [EVAL] batch:   81 | acc: 62.50%,  total acc: 86.13%   [EVAL] batch:   82 | acc: 87.50%,  total acc: 86.14%   [EVAL] batch:   83 | acc: 75.00%,  total acc: 86.01%   [EVAL] batch:   84 | acc: 62.50%,  total acc: 85.74%   [EVAL] batch:   85 | acc: 68.75%,  total acc: 85.54%   [EVAL] batch:   86 | acc: 37.50%,  total acc: 84.99%   [EVAL] batch:   87 | acc: 81.25%,  total acc: 84.94%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 85.11%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 85.28%   [EVAL] batch:   90 | acc: 100.00%,  total acc: 85.44%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 85.60%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 85.75%   [EVAL] batch:   93 | acc: 100.00%,  total acc: 85.90%   [EVAL] batch:   94 | acc: 68.75%,  total acc: 85.72%   [EVAL] batch:   95 | acc: 81.25%,  total acc: 85.68%   [EVAL] batch:   96 | acc: 75.00%,  total acc: 85.57%   [EVAL] batch:   97 | acc: 68.75%,  total acc: 85.40%   [EVAL] batch:   98 | acc: 81.25%,  total acc: 85.35%   [EVAL] batch:   99 | acc: 62.50%,  total acc: 85.12%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 85.27%   [EVAL] batch:  101 | acc: 100.00%,  total acc: 85.42%   [EVAL] batch:  102 | acc: 100.00%,  total acc: 85.56%   [EVAL] batch:  103 | acc: 100.00%,  total acc: 85.70%   [EVAL] batch:  104 | acc: 100.00%,  total acc: 85.83%   [EVAL] batch:  105 | acc: 93.75%,  total acc: 85.91%   [EVAL] batch:  106 | acc: 100.00%,  total acc: 86.04%   [EVAL] batch:  107 | acc: 100.00%,  total acc: 86.17%   [EVAL] batch:  108 | acc: 100.00%,  total acc: 86.30%   [EVAL] batch:  109 | acc: 100.00%,  total acc: 86.42%   [EVAL] batch:  110 | acc: 100.00%,  total acc: 86.54%   [EVAL] batch:  111 | acc: 100.00%,  total acc: 86.66%   [EVAL] batch:  112 | acc: 93.75%,  total acc: 86.73%   [EVAL] batch:  113 | acc: 81.25%,  total acc: 86.68%   [EVAL] batch:  114 | acc: 93.75%,  total acc: 86.74%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 86.85%   [EVAL] batch:  116 | acc: 93.75%,  total acc: 86.91%   [EVAL] batch:  117 | acc: 93.75%,  total acc: 86.97%   [EVAL] batch:  118 | acc: 93.75%,  total acc: 87.03%   [EVAL] batch:  119 | acc: 100.00%,  total acc: 87.14%   [EVAL] batch:  120 | acc: 87.50%,  total acc: 87.14%   [EVAL] batch:  121 | acc: 93.75%,  total acc: 87.19%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 87.30%   [EVAL] batch:  123 | acc: 93.75%,  total acc: 87.35%   [EVAL] batch:  124 | acc: 93.75%,  total acc: 87.40%   [EVAL] batch:  125 | acc: 37.50%,  total acc: 87.00%   [EVAL] batch:  126 | acc: 56.25%,  total acc: 86.76%   [EVAL] batch:  127 | acc: 37.50%,  total acc: 86.38%   [EVAL] batch:  128 | acc: 50.00%,  total acc: 86.09%   [EVAL] batch:  129 | acc: 56.25%,  total acc: 85.87%   [EVAL] batch:  130 | acc: 50.00%,  total acc: 85.59%   [EVAL] batch:  131 | acc: 81.25%,  total acc: 85.56%   [EVAL] batch:  132 | acc: 87.50%,  total acc: 85.57%   [EVAL] batch:  133 | acc: 100.00%,  total acc: 85.68%   [EVAL] batch:  134 | acc: 93.75%,  total acc: 85.74%   [EVAL] batch:  135 | acc: 93.75%,  total acc: 85.80%   [EVAL] batch:  136 | acc: 100.00%,  total acc: 85.90%   [EVAL] batch:  137 | acc: 93.75%,  total acc: 85.96%   [EVAL] batch:  138 | acc: 87.50%,  total acc: 85.97%   [EVAL] batch:  139 | acc: 87.50%,  total acc: 85.98%   [EVAL] batch:  140 | acc: 93.75%,  total acc: 86.04%   [EVAL] batch:  141 | acc: 87.50%,  total acc: 86.05%   [EVAL] batch:  142 | acc: 87.50%,  total acc: 86.06%   [EVAL] batch:  143 | acc: 75.00%,  total acc: 85.98%   [EVAL] batch:  144 | acc: 62.50%,  total acc: 85.82%   [EVAL] batch:  145 | acc: 56.25%,  total acc: 85.62%   [EVAL] batch:  146 | acc: 87.50%,  total acc: 85.63%   [EVAL] batch:  147 | acc: 75.00%,  total acc: 85.56%   [EVAL] batch:  148 | acc: 56.25%,  total acc: 85.36%   [EVAL] batch:  149 | acc: 68.75%,  total acc: 85.25%   [EVAL] batch:  150 | acc: 18.75%,  total acc: 84.81%   [EVAL] batch:  151 | acc: 50.00%,  total acc: 84.58%   [EVAL] batch:  152 | acc: 25.00%,  total acc: 84.19%   [EVAL] batch:  153 | acc: 25.00%,  total acc: 83.81%   [EVAL] batch:  154 | acc: 37.50%,  total acc: 83.51%   [EVAL] batch:  155 | acc: 6.25%,  total acc: 83.01%   [EVAL] batch:  156 | acc: 81.25%,  total acc: 83.00%   [EVAL] batch:  157 | acc: 87.50%,  total acc: 83.03%   [EVAL] batch:  158 | acc: 75.00%,  total acc: 82.98%   [EVAL] batch:  159 | acc: 100.00%,  total acc: 83.09%   [EVAL] batch:  160 | acc: 87.50%,  total acc: 83.11%   [EVAL] batch:  161 | acc: 87.50%,  total acc: 83.14%   [EVAL] batch:  162 | acc: 93.75%,  total acc: 83.21%   [EVAL] batch:  163 | acc: 87.50%,  total acc: 83.23%   [EVAL] batch:  164 | acc: 100.00%,  total acc: 83.33%   [EVAL] batch:  165 | acc: 87.50%,  total acc: 83.36%   [EVAL] batch:  166 | acc: 93.75%,  total acc: 83.42%   [EVAL] batch:  167 | acc: 93.75%,  total acc: 83.48%   [EVAL] batch:  168 | acc: 93.75%,  total acc: 83.54%   [EVAL] batch:  169 | acc: 87.50%,  total acc: 83.57%   [EVAL] batch:  170 | acc: 68.75%,  total acc: 83.48%   [EVAL] batch:  171 | acc: 56.25%,  total acc: 83.32%   [EVAL] batch:  172 | acc: 62.50%,  total acc: 83.20%   [EVAL] batch:  173 | acc: 81.25%,  total acc: 83.19%   [EVAL] batch:  174 | acc: 75.00%,  total acc: 83.14%   [EVAL] batch:  175 | acc: 62.50%,  total acc: 83.03%   [EVAL] batch:  176 | acc: 81.25%,  total acc: 83.02%   [EVAL] batch:  177 | acc: 43.75%,  total acc: 82.79%   [EVAL] batch:  178 | acc: 75.00%,  total acc: 82.75%   [EVAL] batch:  179 | acc: 68.75%,  total acc: 82.67%   [EVAL] batch:  180 | acc: 75.00%,  total acc: 82.63%   [EVAL] batch:  181 | acc: 56.25%,  total acc: 82.49%   [EVAL] batch:  182 | acc: 50.00%,  total acc: 82.31%   [EVAL] batch:  183 | acc: 62.50%,  total acc: 82.20%   [EVAL] batch:  184 | acc: 75.00%,  total acc: 82.16%   [EVAL] batch:  185 | acc: 50.00%,  total acc: 81.99%   [EVAL] batch:  186 | acc: 68.75%,  total acc: 81.92%   [EVAL] batch:  187 | acc: 25.00%,  total acc: 81.62%   
cur_acc:  ['0.9484', '0.8710', '0.7014']
his_acc:  ['0.9484', '0.9020', '0.8162']
CurrentTrain: epoch 15, batch     0 | loss: 15.1656270CurrentTrain: epoch 15, batch     1 | loss: 22.0047131CurrentTrain: epoch 15, batch     2 | loss: 16.6666639CurrentTrain: epoch  1, batch     3 | loss: 6.8388420CurrentTrain: epoch 15, batch     0 | loss: 17.2433075CurrentTrain: epoch 15, batch     1 | loss: 16.6228645CurrentTrain: epoch 15, batch     2 | loss: 12.0407649CurrentTrain: epoch  1, batch     3 | loss: 6.9772198CurrentTrain: epoch 15, batch     0 | loss: 9.7045497CurrentTrain: epoch 15, batch     1 | loss: 26.3905367CurrentTrain: epoch 15, batch     2 | loss: 16.0944184CurrentTrain: epoch  1, batch     3 | loss: 7.4433327CurrentTrain: epoch 15, batch     0 | loss: 11.3710655CurrentTrain: epoch 15, batch     1 | loss: 11.2259378CurrentTrain: epoch 15, batch     2 | loss: 13.2755210CurrentTrain: epoch  1, batch     3 | loss: 7.3242796CurrentTrain: epoch 15, batch     0 | loss: 11.2244522CurrentTrain: epoch 15, batch     1 | loss: 14.4010161CurrentTrain: epoch 15, batch     2 | loss: 11.3053636CurrentTrain: epoch  1, batch     3 | loss: 8.0734281CurrentTrain: epoch 15, batch     0 | loss: 11.6356835CurrentTrain: epoch 15, batch     1 | loss: 13.0969706CurrentTrain: epoch 15, batch     2 | loss: 12.5800805CurrentTrain: epoch  1, batch     3 | loss: 5.7500994CurrentTrain: epoch 15, batch     0 | loss: 11.2808681CurrentTrain: epoch 15, batch     1 | loss: 16.5444808CurrentTrain: epoch 15, batch     2 | loss: 14.4467050CurrentTrain: epoch  1, batch     3 | loss: 6.0941515CurrentTrain: epoch 15, batch     0 | loss: 8.8747218CurrentTrain: epoch 15, batch     1 | loss: 9.7957994CurrentTrain: epoch 15, batch     2 | loss: 9.1600211CurrentTrain: epoch  1, batch     3 | loss: 5.7655361CurrentTrain: epoch 15, batch     0 | loss: 8.0037942CurrentTrain: epoch 15, batch     1 | loss: 10.8100909CurrentTrain: epoch 15, batch     2 | loss: 6.8116804CurrentTrain: epoch  1, batch     3 | loss: 8.2324904CurrentTrain: epoch 15, batch     0 | loss: 8.4399055CurrentTrain: epoch 15, batch     1 | loss: 8.4659890CurrentTrain: epoch 15, batch     2 | loss: 8.3543076CurrentTrain: epoch  1, batch     3 | loss: 6.6527668
#### contains administrative territorial entity ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "contains administrative territorial entity" means direct subdivisions of an administrative territorial entity.
Here is an example:
Relation: contains administrative territorial entity
Context: the sutagao are the chibcha - speaking indios sutagaos indigenous people from the region of fusagasugá , bogotá savanna , cundinamarca , colombia .
Head Entity: colombia
Tail Entity: cundinamarca
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "contains administrative territorial entity":
Relation: contains administrative territorial entity  
Context: The state of California is known for its diverse geography and is divided into several counties, including Los Angeles County, which is the most populous county in the United States.  
Head Entity: California  
Tail Entity: Los Angeles County  

Relation: contains administrative territorial entity  
Context: The country of Japan is made up of several prefectures, with Tokyo being the most famous and densely populated among them.  
Head Entity: Japan  
Tail Entity: Tokyo  
#### said to be the same as ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "said to be the same as" means this item is said to be the same as that item, but it's uncertain or disputed.
Here is an example:
Relation: said to be the same as
Context: enzo is an italian given name derivative of heinz , a form of henry .
Head Entity: henry
Tail Entity: heinz
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "said to be the same as":
Relation: said to be the same as  
Context: The two species, Canis lupus and Canis familiaris, are often said to be the same as each other due to their close genetic relationship.  
Head Entity: Canis lupus  
Tail Entity: Canis familiaris  

Relation: said to be the same as  
Context: Many historians argue that the ancient city of Byzantium is said to be the same as modern-day Istanbul, although the transition involved significant changes over time.  
Head Entity: Byzantium  
Tail Entity: Istanbul  
#### winner ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "winner" means winner of a competition or similar event.
Here is an example:
Relation: winner
Context: the filming was primarily held at pollachi . it fetched pawan kalyan the filmfare award for best actor – telugu award at 59th filmfare awards south .
Head Entity: filmfare award for best actor – telugu
Tail Entity: pawan kalyan
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "winner":
Relation: winner  
Context: in the thrilling finale of the championship, the underdog team triumphed, securing the title and making history as they celebrated their victory at the national sports arena. This remarkable achievement earned them the prestigious trophy for the best team of the year.  
Head Entity: trophy for the best team of the year  
Tail Entity: underdog team  

Relation: winner  
Context: during the annual music awards, the talented singer captivated the audience with her powerful performance, ultimately leading her to win the coveted title of best new artist. The event was a celebration of emerging talent in the music industry.  
Head Entity: title of best new artist  
Tail Entity: talented singer  
#### military rank ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "military rank" means military rank achieved by a person, or military rank associated with a position.
Here is an example:
Relation: military rank
Context: the calais region was defended by the 15th army under " generaloberst " ( colonel general ) hans von salmuth , and normandy by the 7th army commanded by " generaloberst " friedrich dollmann .
Head Entity: hans von salmuth
Tail Entity: generaloberst
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "military rank":
Relation: military rank  
Context: during the battle of stalingrad, the soviet forces were led by general vasily chuikov, who held the rank of major general, while the german troops were commanded by general friedrich paulus.  
Head Entity: vasily chuikov  
Tail Entity: major general  

Relation: military rank  
Context: admiral horatio nelson was a key figure in the british navy during the napoleonic wars, famously holding the rank of vice admiral at the time of the battle of trafalgar.  
Head Entity: horatio nelson  
Tail Entity: vice admiral  
#### publisher ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "publisher" means organization or person responsible for publishing books, periodicals, printed music, podcasts, games or software.
Here is an example:
Relation: publisher
Context: she went on to the film school at san francisco state university , when she was hired as an artist for an atari game called " electrocop " .
Head Entity: electrocop
Tail Entity: atari
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "publisher":
Relation: publisher  
Context: The renowned author released her latest novel through a well-known publishing house that has been in the industry for decades.  
Head Entity: latest novel  
Tail Entity: publishing house  

Relation: publisher  
Context: After years of hard work, the independent game developer finally secured a deal with a major publisher to distribute their new game worldwide.  
Head Entity: new game  
Tail Entity: major publisher  
#### followed by ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "followed by" means immediately following item in a series of which the subject is a part.
Here is an example:
Relation: followed by
Context: " muzai moratorium " and " shōso strip " were released on vinyl in 2008 , as a part of her 10-year anniversary celebrations .
Head Entity: muzai moratorium
Tail Entity: shōso strip
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "followed by":
Relation: followed by  
Context: The first book in the series, "The Enchanted Forest," was published in 2015, followed by "The Hidden Valley" in 2017.  
Head Entity: The Enchanted Forest  
Tail Entity: The Hidden Valley  

Relation: followed by  
Context: The concert began with a stunning performance of Beethoven's Fifth Symphony, followed by a captivating rendition of Mozart's Requiem.  
Head Entity: Beethoven's Fifth Symphony  
Tail Entity: Mozart's Requiem  
#### work location ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "work location" means location where persons or organisations were actively participating in employment, business or other work.
Here is an example:
Relation: work location
Context: in 1694 , he found employment at the court chapel in weimar and was promoted to vice capellmaster ( " " ) in 1695 , succeeding august kühnel , with samuel drese as capellmaster .
Head Entity: august kühnel
Tail Entity: weimar
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "work location":
Relation: work location  
Context: After completing his studies, John accepted a position at the tech startup in Silicon Valley, where he contributed to several innovative projects.  
Head Entity: John  
Tail Entity: Silicon Valley  

Relation: work location  
Context: The renowned architect designed several buildings in the bustling city of New York, where he gained significant recognition for his work.  
Head Entity: the renowned architect  
Tail Entity: New York  
#### occupation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "occupation" means occupation of a person.
Here is an example:
Relation: occupation
Context: andrei droznin ( russian : андрей дрознин ) is a russian theatre director and movement coach .
Head Entity: andrei droznin
Tail Entity: theatre director
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "occupation":
Relation: occupation  
Context: dr. emily thompson is a renowned biochemist known for her groundbreaking research in cancer treatment.  
Head Entity: dr. emily thompson  
Tail Entity: biochemist  

Relation: occupation  
Context: john smith, a talented software engineer, has been working at tech innovations for over five years.  
Head Entity: john smith  
Tail Entity: software engineer  
#### heritage designation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "heritage designation" means heritage designation of a cultural or natural site.
Here is an example:
Relation: heritage designation
Context: altamaha town , bluffton historic district , church of the cross , and rose hill plantation house are listed on the national register of historic places .
Head Entity: altamaha town
Tail Entity: national register of historic places
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "heritage designation":
Relation: heritage designation  
Context: The Great Wall of China is recognized as a UNESCO World Heritage Site due to its historical significance and architectural grandeur.  
Head Entity: Great Wall of China  
Tail Entity: UNESCO World Heritage Site  

Relation: heritage designation  
Context: The ancient city of Petra in Jordan is designated as a World Heritage Site, attracting tourists from around the globe.  
Head Entity: ancient city of Petra  
Tail Entity: World Heritage Site  
#### voice type ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "voice type" means person's voice type.
Here is an example:
Relation: voice type
Context: ada adini ( 1855 – february 1924 ) was an american operatic soprano who had an active international career from 1876 up into the first decade of the 20th century .
Head Entity: ada adini
Tail Entity: soprano
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "voice type":
Relation: voice type  
Context: luciano pavarotti (1935 – 2007) was an italian operatic tenor who gained worldwide fame for his televised concerts and media appearances.  
Head Entity: luciano pavarotti  
Tail Entity: tenor  

Relation: voice type  
Context: mariah carey, born in 1969, is an american singer, songwriter, and actress known for her five-octave vocal range and her use of the whistle register.  
Head Entity: mariah carey  
Tail Entity: singer  
MemoryTrain:  epoch 15, batch     0 | loss: 4.3446008MemoryTrain:  epoch 15, batch     1 | loss: 3.5456637MemoryTrain:  epoch 15, batch     2 | loss: 3.9299237MemoryTrain:  epoch 15, batch     3 | loss: 3.1014547MemoryTrain:  epoch 15, batch     4 | loss: 3.6550638MemoryTrain:  epoch 15, batch     5 | loss: 3.4771154MemoryTrain:  epoch 15, batch     6 | loss: 5.9277500MemoryTrain:  epoch  7, batch     7 | loss: 3.5125027MemoryTrain:  epoch 15, batch     0 | loss: 5.7627327MemoryTrain:  epoch 15, batch     1 | loss: 3.2675568MemoryTrain:  epoch 15, batch     2 | loss: 7.4410091MemoryTrain:  epoch 15, batch     3 | loss: 6.3051080MemoryTrain:  epoch 15, batch     4 | loss: 5.2016058MemoryTrain:  epoch 15, batch     5 | loss: 4.6859642MemoryTrain:  epoch 15, batch     6 | loss: 2.9435424MemoryTrain:  epoch  7, batch     7 | loss: 2.8713978MemoryTrain:  epoch 15, batch     0 | loss: 3.0730565MemoryTrain:  epoch 15, batch     1 | loss: 6.7437957MemoryTrain:  epoch 15, batch     2 | loss: 3.4670270MemoryTrain:  epoch 15, batch     3 | loss: 2.0894125MemoryTrain:  epoch 15, batch     4 | loss: 3.7080067MemoryTrain:  epoch 15, batch     5 | loss: 2.2061369MemoryTrain:  epoch 15, batch     6 | loss: 2.2976357MemoryTrain:  epoch  7, batch     7 | loss: 5.7102142MemoryTrain:  epoch 15, batch     0 | loss: 2.3891456MemoryTrain:  epoch 15, batch     1 | loss: 2.8118983MemoryTrain:  epoch 15, batch     2 | loss: 3.0170079MemoryTrain:  epoch 15, batch     3 | loss: 4.2537511MemoryTrain:  epoch 15, batch     4 | loss: 7.3579008MemoryTrain:  epoch 15, batch     5 | loss: 4.7869436MemoryTrain:  epoch 15, batch     6 | loss: 2.1843729MemoryTrain:  epoch  7, batch     7 | loss: 4.4266081MemoryTrain:  epoch 15, batch     0 | loss: 5.1698480MemoryTrain:  epoch 15, batch     1 | loss: 2.8381842MemoryTrain:  epoch 15, batch     2 | loss: 2.9361429MemoryTrain:  epoch 15, batch     3 | loss: 3.3577945MemoryTrain:  epoch 15, batch     4 | loss: 2.1002346MemoryTrain:  epoch 15, batch     5 | loss: 3.0061703MemoryTrain:  epoch 15, batch     6 | loss: 3.8446679MemoryTrain:  epoch  7, batch     7 | loss: 4.4055926MemoryTrain:  epoch 15, batch     0 | loss: 1.8297161MemoryTrain:  epoch 15, batch     1 | loss: 1.7745098MemoryTrain:  epoch 15, batch     2 | loss: 2.6493153MemoryTrain:  epoch 15, batch     3 | loss: 5.4481959MemoryTrain:  epoch 15, batch     4 | loss: 2.8206663MemoryTrain:  epoch 15, batch     5 | loss: 1.9582972MemoryTrain:  epoch 15, batch     6 | loss: 2.0984000MemoryTrain:  epoch  7, batch     7 | loss: 1.9123640MemoryTrain:  epoch 15, batch     0 | loss: 1.8716837MemoryTrain:  epoch 15, batch     1 | loss: 3.5058606MemoryTrain:  epoch 15, batch     2 | loss: 2.2163002MemoryTrain:  epoch 15, batch     3 | loss: 3.9069905MemoryTrain:  epoch 15, batch     4 | loss: 2.6586272MemoryTrain:  epoch 15, batch     5 | loss: 1.9469612MemoryTrain:  epoch 15, batch     6 | loss: 2.3999563MemoryTrain:  epoch  7, batch     7 | loss: 1.3808025MemoryTrain:  epoch 15, batch     0 | loss: 1.5658957MemoryTrain:  epoch 15, batch     1 | loss: 3.0101690MemoryTrain:  epoch 15, batch     2 | loss: 4.5084515MemoryTrain:  epoch 15, batch     3 | loss: 2.1713081MemoryTrain:  epoch 15, batch     4 | loss: 1.7674100MemoryTrain:  epoch 15, batch     5 | loss: 4.5313532MemoryTrain:  epoch 15, batch     6 | loss: 4.8933738MemoryTrain:  epoch  7, batch     7 | loss: 1.4475379MemoryTrain:  epoch 15, batch     0 | loss: 2.2377362MemoryTrain:  epoch 15, batch     1 | loss: 2.4313432MemoryTrain:  epoch 15, batch     2 | loss: 4.4327652MemoryTrain:  epoch 15, batch     3 | loss: 2.5207243MemoryTrain:  epoch 15, batch     4 | loss: 3.0543526MemoryTrain:  epoch 15, batch     5 | loss: 2.5517282MemoryTrain:  epoch 15, batch     6 | loss: 1.4047946MemoryTrain:  epoch  7, batch     7 | loss: 4.2017014MemoryTrain:  epoch 15, batch     0 | loss: 3.7693641MemoryTrain:  epoch 15, batch     1 | loss: 3.9162098MemoryTrain:  epoch 15, batch     2 | loss: 4.9263512MemoryTrain:  epoch 15, batch     3 | loss: 5.1967265MemoryTrain:  epoch 15, batch     4 | loss: 4.1693848MemoryTrain:  epoch 15, batch     5 | loss: 1.3824380MemoryTrain:  epoch 15, batch     6 | loss: 2.0567800MemoryTrain:  epoch  7, batch     7 | loss: 1.4318629
[EVAL] batch:    0 | acc: 43.75%,  total acc: 43.75%   [EVAL] batch:    1 | acc: 50.00%,  total acc: 46.88%   [EVAL] batch:    2 | acc: 31.25%,  total acc: 41.67%   [EVAL] batch:    3 | acc: 43.75%,  total acc: 42.19%   [EVAL] batch:    4 | acc: 50.00%,  total acc: 43.75%   [EVAL] batch:    5 | acc: 37.50%,  total acc: 42.71%   [EVAL] batch:    6 | acc: 56.25%,  total acc: 44.64%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 49.22%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 52.78%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 54.37%   [EVAL] batch:   10 | acc: 50.00%,  total acc: 53.98%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 55.73%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 57.69%   [EVAL] batch:   13 | acc: 62.50%,  total acc: 58.04%   [EVAL] batch:   14 | acc: 50.00%,  total acc: 57.50%   [EVAL] batch:   15 | acc: 43.75%,  total acc: 56.64%   [EVAL] batch:   16 | acc: 62.50%,  total acc: 56.99%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 57.29%   [EVAL] batch:   18 | acc: 87.50%,  total acc: 58.88%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 60.00%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 61.90%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 63.64%   [EVAL] batch:   22 | acc: 87.50%,  total acc: 64.67%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 65.62%   [EVAL] batch:   24 | acc: 93.75%,  total acc: 66.75%   [EVAL] batch:   25 | acc: 81.25%,  total acc: 67.31%   [EVAL] batch:   26 | acc: 81.25%,  total acc: 67.82%   [EVAL] batch:   27 | acc: 87.50%,  total acc: 68.53%   [EVAL] batch:   28 | acc: 56.25%,  total acc: 68.10%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 68.75%   [EVAL] batch:   30 | acc: 81.25%,  total acc: 69.15%   [EVAL] batch:   31 | acc: 75.00%,  total acc: 69.34%   [EVAL] batch:   32 | acc: 62.50%,  total acc: 69.13%   [EVAL] batch:   33 | acc: 31.25%,  total acc: 68.01%   [EVAL] batch:   34 | acc: 50.00%,  total acc: 67.50%   [EVAL] batch:   35 | acc: 25.00%,  total acc: 66.32%   [EVAL] batch:   36 | acc: 18.75%,  total acc: 65.03%   [EVAL] batch:   37 | acc: 81.25%,  total acc: 65.46%   [EVAL] batch:   38 | acc: 75.00%,  total acc: 65.71%   [EVAL] batch:   39 | acc: 87.50%,  total acc: 66.25%   [EVAL] batch:   40 | acc: 75.00%,  total acc: 66.46%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 66.96%   [EVAL] batch:   42 | acc: 75.00%,  total acc: 67.15%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 67.47%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 67.92%   [EVAL] batch:   45 | acc: 81.25%,  total acc: 68.21%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 68.48%   [EVAL] batch:   47 | acc: 68.75%,  total acc: 68.49%   [EVAL] batch:   48 | acc: 87.50%,  total acc: 68.88%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 69.25%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 69.85%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 70.43%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 70.99%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 71.53%   [EVAL] batch:   54 | acc: 100.00%,  total acc: 72.05%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 72.54%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 73.03%   [EVAL] batch:   57 | acc: 100.00%,  total acc: 73.49%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 73.94%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 74.38%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 74.80%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 75.20%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 74.80%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 78.12%   [EVAL] batch:    4 | acc: 62.50%,  total acc: 75.00%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 77.08%   [EVAL] batch:    6 | acc: 43.75%,  total acc: 72.32%   [EVAL] batch:    7 | acc: 56.25%,  total acc: 70.31%   [EVAL] batch:    8 | acc: 56.25%,  total acc: 68.75%   [EVAL] batch:    9 | acc: 56.25%,  total acc: 67.50%   [EVAL] batch:   10 | acc: 43.75%,  total acc: 65.34%   [EVAL] batch:   11 | acc: 37.50%,  total acc: 63.02%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 63.46%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 65.62%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 67.50%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 69.53%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 71.32%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 72.57%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 73.68%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 74.06%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 75.00%   [EVAL] batch:   21 | acc: 68.75%,  total acc: 74.72%   [EVAL] batch:   22 | acc: 75.00%,  total acc: 74.73%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 75.78%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 76.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 77.16%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 78.01%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 78.57%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 79.09%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 79.79%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 80.44%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 81.05%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 81.63%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 82.17%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 82.50%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 82.99%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 83.45%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 83.88%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 84.29%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 84.69%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 85.06%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 85.27%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 85.61%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 85.65%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 85.69%   [EVAL] batch:   45 | acc: 81.25%,  total acc: 85.60%   [EVAL] batch:   46 | acc: 56.25%,  total acc: 84.97%   [EVAL] batch:   47 | acc: 75.00%,  total acc: 84.77%   [EVAL] batch:   48 | acc: 75.00%,  total acc: 84.57%   [EVAL] batch:   49 | acc: 62.50%,  total acc: 84.12%   [EVAL] batch:   50 | acc: 75.00%,  total acc: 83.95%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 84.01%   [EVAL] batch:   52 | acc: 87.50%,  total acc: 84.08%   [EVAL] batch:   53 | acc: 62.50%,  total acc: 83.68%   [EVAL] batch:   54 | acc: 50.00%,  total acc: 83.07%   [EVAL] batch:   55 | acc: 68.75%,  total acc: 82.81%   [EVAL] batch:   56 | acc: 81.25%,  total acc: 82.79%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 82.87%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 83.16%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 83.33%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 83.61%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 83.87%   [EVAL] batch:   62 | acc: 93.75%,  total acc: 84.03%   [EVAL] batch:   63 | acc: 93.75%,  total acc: 84.18%   [EVAL] batch:   64 | acc: 93.75%,  total acc: 84.33%   [EVAL] batch:   65 | acc: 100.00%,  total acc: 84.56%   [EVAL] batch:   66 | acc: 81.25%,  total acc: 84.51%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 84.74%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 84.96%   [EVAL] batch:   69 | acc: 81.25%,  total acc: 84.91%   [EVAL] batch:   70 | acc: 93.75%,  total acc: 85.04%   [EVAL] batch:   71 | acc: 87.50%,  total acc: 85.07%   [EVAL] batch:   72 | acc: 93.75%,  total acc: 85.19%   [EVAL] batch:   73 | acc: 81.25%,  total acc: 85.14%   [EVAL] batch:   74 | acc: 81.25%,  total acc: 85.08%   [EVAL] batch:   75 | acc: 62.50%,  total acc: 84.79%   [EVAL] batch:   76 | acc: 87.50%,  total acc: 84.82%   [EVAL] batch:   77 | acc: 75.00%,  total acc: 84.70%   [EVAL] batch:   78 | acc: 50.00%,  total acc: 84.26%   [EVAL] batch:   79 | acc: 68.75%,  total acc: 84.06%   [EVAL] batch:   80 | acc: 56.25%,  total acc: 83.72%   [EVAL] batch:   81 | acc: 62.50%,  total acc: 83.46%   [EVAL] batch:   82 | acc: 75.00%,  total acc: 83.36%   [EVAL] batch:   83 | acc: 68.75%,  total acc: 83.18%   [EVAL] batch:   84 | acc: 68.75%,  total acc: 83.01%   [EVAL] batch:   85 | acc: 81.25%,  total acc: 82.99%   [EVAL] batch:   86 | acc: 50.00%,  total acc: 82.61%   [EVAL] batch:   87 | acc: 81.25%,  total acc: 82.60%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 82.79%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 82.99%   [EVAL] batch:   90 | acc: 100.00%,  total acc: 83.17%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 83.36%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 83.53%   [EVAL] batch:   93 | acc: 93.75%,  total acc: 83.64%   [EVAL] batch:   94 | acc: 50.00%,  total acc: 83.29%   [EVAL] batch:   95 | acc: 75.00%,  total acc: 83.20%   [EVAL] batch:   96 | acc: 62.50%,  total acc: 82.99%   [EVAL] batch:   97 | acc: 62.50%,  total acc: 82.78%   [EVAL] batch:   98 | acc: 75.00%,  total acc: 82.70%   [EVAL] batch:   99 | acc: 43.75%,  total acc: 82.31%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 82.49%   [EVAL] batch:  101 | acc: 100.00%,  total acc: 82.66%   [EVAL] batch:  102 | acc: 100.00%,  total acc: 82.83%   [EVAL] batch:  103 | acc: 100.00%,  total acc: 82.99%   [EVAL] batch:  104 | acc: 100.00%,  total acc: 83.15%   [EVAL] batch:  105 | acc: 87.50%,  total acc: 83.20%   [EVAL] batch:  106 | acc: 100.00%,  total acc: 83.35%   [EVAL] batch:  107 | acc: 100.00%,  total acc: 83.51%   [EVAL] batch:  108 | acc: 100.00%,  total acc: 83.66%   [EVAL] batch:  109 | acc: 100.00%,  total acc: 83.81%   [EVAL] batch:  110 | acc: 100.00%,  total acc: 83.95%   [EVAL] batch:  111 | acc: 93.75%,  total acc: 84.04%   [EVAL] batch:  112 | acc: 100.00%,  total acc: 84.18%   [EVAL] batch:  113 | acc: 93.75%,  total acc: 84.27%   [EVAL] batch:  114 | acc: 93.75%,  total acc: 84.35%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 84.48%   [EVAL] batch:  116 | acc: 93.75%,  total acc: 84.56%   [EVAL] batch:  117 | acc: 93.75%,  total acc: 84.64%   [EVAL] batch:  118 | acc: 93.75%,  total acc: 84.72%   [EVAL] batch:  119 | acc: 100.00%,  total acc: 84.84%   [EVAL] batch:  120 | acc: 87.50%,  total acc: 84.87%   [EVAL] batch:  121 | acc: 93.75%,  total acc: 84.94%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 85.06%   [EVAL] batch:  123 | acc: 93.75%,  total acc: 85.13%   [EVAL] batch:  124 | acc: 93.75%,  total acc: 85.20%   [EVAL] batch:  125 | acc: 25.00%,  total acc: 84.72%   [EVAL] batch:  126 | acc: 50.00%,  total acc: 84.45%   [EVAL] batch:  127 | acc: 37.50%,  total acc: 84.08%   [EVAL] batch:  128 | acc: 31.25%,  total acc: 83.67%   [EVAL] batch:  129 | acc: 56.25%,  total acc: 83.46%   [EVAL] batch:  130 | acc: 50.00%,  total acc: 83.21%   [EVAL] batch:  131 | acc: 75.00%,  total acc: 83.14%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 83.27%   [EVAL] batch:  133 | acc: 100.00%,  total acc: 83.40%   [EVAL] batch:  134 | acc: 93.75%,  total acc: 83.47%   [EVAL] batch:  135 | acc: 100.00%,  total acc: 83.59%   [EVAL] batch:  136 | acc: 93.75%,  total acc: 83.67%   [EVAL] batch:  137 | acc: 81.25%,  total acc: 83.65%   [EVAL] batch:  138 | acc: 81.25%,  total acc: 83.63%   [EVAL] batch:  139 | acc: 81.25%,  total acc: 83.62%   [EVAL] batch:  140 | acc: 87.50%,  total acc: 83.64%   [EVAL] batch:  141 | acc: 87.50%,  total acc: 83.67%   [EVAL] batch:  142 | acc: 62.50%,  total acc: 83.52%   [EVAL] batch:  143 | acc: 68.75%,  total acc: 83.42%   [EVAL] batch:  144 | acc: 56.25%,  total acc: 83.23%   [EVAL] batch:  145 | acc: 56.25%,  total acc: 83.05%   [EVAL] batch:  146 | acc: 81.25%,  total acc: 83.04%   [EVAL] batch:  147 | acc: 68.75%,  total acc: 82.94%   [EVAL] batch:  148 | acc: 50.00%,  total acc: 82.72%   [EVAL] batch:  149 | acc: 75.00%,  total acc: 82.67%   [EVAL] batch:  150 | acc: 6.25%,  total acc: 82.16%   [EVAL] batch:  151 | acc: 12.50%,  total acc: 81.70%   [EVAL] batch:  152 | acc: 18.75%,  total acc: 81.29%   [EVAL] batch:  153 | acc: 6.25%,  total acc: 80.80%   [EVAL] batch:  154 | acc: 25.00%,  total acc: 80.44%   [EVAL] batch:  155 | acc: 0.00%,  total acc: 79.93%   [EVAL] batch:  156 | acc: 75.00%,  total acc: 79.90%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 79.98%   [EVAL] batch:  158 | acc: 87.50%,  total acc: 80.03%   [EVAL] batch:  159 | acc: 100.00%,  total acc: 80.16%   [EVAL] batch:  160 | acc: 87.50%,  total acc: 80.20%   [EVAL] batch:  161 | acc: 81.25%,  total acc: 80.21%   [EVAL] batch:  162 | acc: 100.00%,  total acc: 80.33%   [EVAL] batch:  163 | acc: 93.75%,  total acc: 80.41%   [EVAL] batch:  164 | acc: 81.25%,  total acc: 80.42%   [EVAL] batch:  165 | acc: 87.50%,  total acc: 80.46%   [EVAL] batch:  166 | acc: 87.50%,  total acc: 80.50%   [EVAL] batch:  167 | acc: 87.50%,  total acc: 80.54%   [EVAL] batch:  168 | acc: 81.25%,  total acc: 80.55%   [EVAL] batch:  169 | acc: 6.25%,  total acc: 80.11%   [EVAL] batch:  170 | acc: 0.00%,  total acc: 79.64%   [EVAL] batch:  171 | acc: 6.25%,  total acc: 79.22%   [EVAL] batch:  172 | acc: 12.50%,  total acc: 78.83%   [EVAL] batch:  173 | acc: 0.00%,  total acc: 78.38%   [EVAL] batch:  174 | acc: 6.25%,  total acc: 77.96%   [EVAL] batch:  175 | acc: 68.75%,  total acc: 77.91%   [EVAL] batch:  176 | acc: 87.50%,  total acc: 77.97%   [EVAL] batch:  177 | acc: 43.75%,  total acc: 77.77%   [EVAL] batch:  178 | acc: 68.75%,  total acc: 77.72%   [EVAL] batch:  179 | acc: 75.00%,  total acc: 77.71%   [EVAL] batch:  180 | acc: 75.00%,  total acc: 77.69%   [EVAL] batch:  181 | acc: 56.25%,  total acc: 77.58%   [EVAL] batch:  182 | acc: 43.75%,  total acc: 77.39%   [EVAL] batch:  183 | acc: 50.00%,  total acc: 77.24%   [EVAL] batch:  184 | acc: 75.00%,  total acc: 77.23%   [EVAL] batch:  185 | acc: 37.50%,  total acc: 77.02%   [EVAL] batch:  186 | acc: 68.75%,  total acc: 76.97%   [EVAL] batch:  187 | acc: 37.50%,  total acc: 76.76%   [EVAL] batch:  188 | acc: 56.25%,  total acc: 76.65%   [EVAL] batch:  189 | acc: 31.25%,  total acc: 76.41%   [EVAL] batch:  190 | acc: 50.00%,  total acc: 76.28%   [EVAL] batch:  191 | acc: 37.50%,  total acc: 76.07%   [EVAL] batch:  192 | acc: 37.50%,  total acc: 75.87%   [EVAL] batch:  193 | acc: 43.75%,  total acc: 75.71%   [EVAL] batch:  194 | acc: 87.50%,  total acc: 75.77%   [EVAL] batch:  195 | acc: 62.50%,  total acc: 75.70%   [EVAL] batch:  196 | acc: 81.25%,  total acc: 75.73%   [EVAL] batch:  197 | acc: 75.00%,  total acc: 75.73%   [EVAL] batch:  198 | acc: 37.50%,  total acc: 75.53%   [EVAL] batch:  199 | acc: 100.00%,  total acc: 75.66%   [EVAL] batch:  200 | acc: 62.50%,  total acc: 75.59%   [EVAL] batch:  201 | acc: 50.00%,  total acc: 75.46%   [EVAL] batch:  202 | acc: 56.25%,  total acc: 75.37%   [EVAL] batch:  203 | acc: 50.00%,  total acc: 75.25%   [EVAL] batch:  204 | acc: 62.50%,  total acc: 75.18%   [EVAL] batch:  205 | acc: 75.00%,  total acc: 75.18%   [EVAL] batch:  206 | acc: 81.25%,  total acc: 75.21%   [EVAL] batch:  207 | acc: 93.75%,  total acc: 75.30%   [EVAL] batch:  208 | acc: 100.00%,  total acc: 75.42%   [EVAL] batch:  209 | acc: 93.75%,  total acc: 75.51%   [EVAL] batch:  210 | acc: 87.50%,  total acc: 75.56%   [EVAL] batch:  211 | acc: 87.50%,  total acc: 75.62%   [EVAL] batch:  212 | acc: 87.50%,  total acc: 75.67%   [EVAL] batch:  213 | acc: 75.00%,  total acc: 75.67%   [EVAL] batch:  214 | acc: 87.50%,  total acc: 75.73%   [EVAL] batch:  215 | acc: 81.25%,  total acc: 75.75%   [EVAL] batch:  216 | acc: 75.00%,  total acc: 75.75%   [EVAL] batch:  217 | acc: 87.50%,  total acc: 75.80%   [EVAL] batch:  218 | acc: 68.75%,  total acc: 75.77%   [EVAL] batch:  219 | acc: 68.75%,  total acc: 75.74%   [EVAL] batch:  220 | acc: 37.50%,  total acc: 75.57%   [EVAL] batch:  221 | acc: 50.00%,  total acc: 75.45%   [EVAL] batch:  222 | acc: 37.50%,  total acc: 75.28%   [EVAL] batch:  223 | acc: 18.75%,  total acc: 75.03%   [EVAL] batch:  224 | acc: 50.00%,  total acc: 74.92%   [EVAL] batch:  225 | acc: 75.00%,  total acc: 74.92%   [EVAL] batch:  226 | acc: 81.25%,  total acc: 74.94%   [EVAL] batch:  227 | acc: 75.00%,  total acc: 74.95%   [EVAL] batch:  228 | acc: 100.00%,  total acc: 75.05%   [EVAL] batch:  229 | acc: 68.75%,  total acc: 75.03%   [EVAL] batch:  230 | acc: 75.00%,  total acc: 75.03%   [EVAL] batch:  231 | acc: 93.75%,  total acc: 75.11%   [EVAL] batch:  232 | acc: 81.25%,  total acc: 75.13%   [EVAL] batch:  233 | acc: 87.50%,  total acc: 75.19%   [EVAL] batch:  234 | acc: 75.00%,  total acc: 75.19%   [EVAL] batch:  235 | acc: 75.00%,  total acc: 75.19%   [EVAL] batch:  236 | acc: 87.50%,  total acc: 75.24%   [EVAL] batch:  237 | acc: 93.75%,  total acc: 75.32%   [EVAL] batch:  238 | acc: 100.00%,  total acc: 75.42%   [EVAL] batch:  239 | acc: 100.00%,  total acc: 75.52%   [EVAL] batch:  240 | acc: 100.00%,  total acc: 75.62%   [EVAL] batch:  241 | acc: 100.00%,  total acc: 75.72%   [EVAL] batch:  242 | acc: 100.00%,  total acc: 75.82%   [EVAL] batch:  243 | acc: 100.00%,  total acc: 75.92%   [EVAL] batch:  244 | acc: 100.00%,  total acc: 76.02%   [EVAL] batch:  245 | acc: 100.00%,  total acc: 76.12%   [EVAL] batch:  246 | acc: 100.00%,  total acc: 76.21%   [EVAL] batch:  247 | acc: 100.00%,  total acc: 76.31%   [EVAL] batch:  248 | acc: 100.00%,  total acc: 76.41%   [EVAL] batch:  249 | acc: 100.00%,  total acc: 76.50%   
cur_acc:  ['0.9484', '0.8710', '0.7014', '0.7480']
his_acc:  ['0.9484', '0.9020', '0.8162', '0.7650']
CurrentTrain: epoch 15, batch     0 | loss: 19.2179387CurrentTrain: epoch 15, batch     1 | loss: 20.6233175CurrentTrain: epoch 15, batch     2 | loss: 16.2819321CurrentTrain: epoch  1, batch     3 | loss: 10.4025639CurrentTrain: epoch 15, batch     0 | loss: 14.1813027CurrentTrain: epoch 15, batch     1 | loss: 28.4632912CurrentTrain: epoch 15, batch     2 | loss: 17.6572339CurrentTrain: epoch  1, batch     3 | loss: 8.4770945CurrentTrain: epoch 15, batch     0 | loss: 10.0140093CurrentTrain: epoch 15, batch     1 | loss: 12.7469231CurrentTrain: epoch 15, batch     2 | loss: 17.4205375CurrentTrain: epoch  1, batch     3 | loss: 7.3054949CurrentTrain: epoch 15, batch     0 | loss: 14.6559826CurrentTrain: epoch 15, batch     1 | loss: 13.4886234CurrentTrain: epoch 15, batch     2 | loss: 27.8845545CurrentTrain: epoch  1, batch     3 | loss: 9.6554588CurrentTrain: epoch 15, batch     0 | loss: 12.1126967CurrentTrain: epoch 15, batch     1 | loss: 14.9977637CurrentTrain: epoch 15, batch     2 | loss: 16.1611546CurrentTrain: epoch  1, batch     3 | loss: 8.8458987CurrentTrain: epoch 15, batch     0 | loss: 13.9933576CurrentTrain: epoch 15, batch     1 | loss: 10.6723614CurrentTrain: epoch 15, batch     2 | loss: 14.0246125CurrentTrain: epoch  1, batch     3 | loss: 7.5702351CurrentTrain: epoch 15, batch     0 | loss: 11.4538713CurrentTrain: epoch 15, batch     1 | loss: 11.0772691CurrentTrain: epoch 15, batch     2 | loss: 10.0677577CurrentTrain: epoch  1, batch     3 | loss: 10.2498842CurrentTrain: epoch 15, batch     0 | loss: 13.5395715CurrentTrain: epoch 15, batch     1 | loss: 14.0845561CurrentTrain: epoch 15, batch     2 | loss: 13.3496720CurrentTrain: epoch  1, batch     3 | loss: 6.3248580CurrentTrain: epoch 15, batch     0 | loss: 13.9424980CurrentTrain: epoch 15, batch     1 | loss: 8.7098198CurrentTrain: epoch 15, batch     2 | loss: 10.8422606CurrentTrain: epoch  1, batch     3 | loss: 6.5134452CurrentTrain: epoch 15, batch     0 | loss: 30.9649013CurrentTrain: epoch 15, batch     1 | loss: 13.0491943CurrentTrain: epoch 15, batch     2 | loss: 10.9896619CurrentTrain: epoch  1, batch     3 | loss: 19.9754241
#### applies to jurisdiction ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "applies to jurisdiction" means the item (institution, law, public office, public register...) or statement belongs to or has power over or applies to the value (a territorial jurisdiction: a country, state, municipality, ...).
Here is an example:
Relation: applies to jurisdiction
Context: in 2004 the catalan government gave him the george cross .
Head Entity: george cross
Tail Entity: catalan
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "applies to jurisdiction":
Relation: applies to jurisdiction  
Context: The new environmental regulations introduced by the European Union will impact all member states significantly.  
Head Entity: European Union  
Tail Entity: member states  

Relation: applies to jurisdiction  
Context: The Supreme Court ruling clarified that federal laws take precedence over state laws in matters of immigration.  
Head Entity: federal laws  
Tail Entity: state laws  
#### performer ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "performer" means actor, musician, band or other performer associated with this role or musical work.
Here is an example:
Relation: performer
Context: it is prominently featured on jean michel jarre 's albums " oxygène " ( 1977 ) and " équinoxe " ( 1978 ) .
Head Entity: équinoxe
Tail Entity: jean michel jarre
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "performer":
Relation: performer  
Context: The legendary band Queen is known for their iconic song "Bohemian Rhapsody," which showcases Freddie Mercury's extraordinary vocal talent.  
Head Entity: Bohemian Rhapsody  
Tail Entity: Freddie Mercury  

Relation: performer  
Context: Taylor Swift captivated audiences with her heartfelt performance of "Love Story" during the awards ceremony last night.  
Head Entity: Love Story  
Tail Entity: Taylor Swift  
#### manufacturer ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "manufacturer" means manufacturer or producer of this product.
Here is an example:
Relation: manufacturer
Context: the glasflügel h-201 standard " libelle " ( german : " dragonfly " ) is an early composite standard class single - seat sailplane produced by glasflügel from 1967 .
Head Entity: dragonfly
Tail Entity: glasflügel
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "manufacturer":
Relation: manufacturer  
Context: The iPhone 13, a smartphone developed by Apple, was released in September 2021 and features advanced camera technology and improved battery life.  
Head Entity: iPhone 13  
Tail Entity: Apple  

Relation: manufacturer  
Context: The Tesla Model S is an all-electric luxury sedan that was first introduced by Tesla, Inc. in 2012, showcasing innovative technology and performance.  
Head Entity: Tesla Model S  
Tail Entity: Tesla, Inc.  
#### participant ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "participant" means person, group of people or organization (object) that actively takes/took part in an event or process.
Here is an example:
Relation: participant
Context: in contrast to gwladys and roger 's allegiance to the house of lancaster , their three sons were staunch yorkists during the wars of the roses .
Head Entity: wars of the roses
Tail Entity: house of lancaster
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "participant":
Relation: participant  
Context: During the annual tech conference, several leading companies showcased their latest innovations, with Google and Microsoft being the most prominent participants.  
Head Entity: tech conference  
Tail Entity: Google  

Relation: participant  
Context: The local community organized a charity run, where various schools and organizations, including the Red Cross, participated to raise funds for the homeless.  
Head Entity: charity run  
Tail Entity: Red Cross  
#### operating system ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "operating system" means operating system (OS) on which a software works or the OS installed on hardware.
Here is an example:
Relation: operating system
Context: the software is written in python and works on all the platforms supported by blender : windows , osx and linux .
Head Entity: blender
Tail Entity: windows
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "operating system":
Relation: operating system  
Context: The latest version of the application is compatible with various operating systems, including macOS, Windows, and Ubuntu.  
Head Entity: application  
Tail Entity: macOS  

Relation: operating system  
Context: Many developers prefer to use Linux as their operating system due to its flexibility and open-source nature.  
Head Entity: developers  
Tail Entity: Linux  
#### competition class ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "competition class" means official classification by a regulating body under which the subject (events, teams, participants, or equipment) qualifies for inclusion.
Here is an example:
Relation: competition class
Context: in july 2007 , griffin lost by a technical knockout to light heavyweight champion danny green of the world boxing association at challenge stadium in perth , australia .
Head Entity: danny green
Tail Entity: light heavyweight
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "competition class":
Relation: competition class  
Context: during the 2020 olympics, the men's 100 meters final showcased athletes from various competition classes, with usain bolt representing the sprinting class.  
Head Entity: usain bolt  
Tail Entity: sprinting  

Relation: competition class  
Context: the formula one race in monaco is known for its high-speed competition class, where lewis hamilton competed fiercely against other top drivers.  
Head Entity: lewis hamilton  
Tail Entity: formula one
#### successful candidate ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "successful candidate" means person elected after the election.
Here is an example:
Relation: successful candidate
Context: the qup gained 3 seats from its previous election result , and won a 10 % swing to it in primary votes , but failed to defeat the incumbent labor government led by ted theodore .
Head Entity: previous election
Tail Entity: ted theodore
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "successful candidate":
Relation: successful candidate  
Context: After a closely contested campaign, the voters chose Sarah Johnson as their representative, marking a significant shift in the political landscape of the district.  
Head Entity: closely contested campaign  
Tail Entity: Sarah Johnson  

Relation: successful candidate  
Context: In the recent mayoral election, the community rallied behind Mark Thompson, who promised to bring new initiatives to the city, ultimately leading to his victory.  
Head Entity: recent mayoral election  
Tail Entity: Mark Thompson  
#### sibling ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "sibling" means the subject and the object have the same parents.
Here is an example:
Relation: sibling
Context: the deaths of his brothers wenceslaus ii ( 1487 ) , casimir ii ( 1490 ) and władysław ( 1494 ) allowed jan v to reunificated the whole duchy of zator .
Head Entity: casimir ii
Tail Entity: wenceslaus ii
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "sibling":
Relation: sibling  
Context: both elizabeth and her brother, charles, were known for their contributions to the arts and culture of their time.  
Head Entity: elizabeth  
Tail Entity: charles  

Relation: sibling  
Context: during the family reunion, it was heartwarming to see how much john and his sister, sarah, resembled each other in both looks and personality.  
Head Entity: john  
Tail Entity: sarah  
#### military branch ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "military branch" means branch to which this military unit, award, office, or person belongs.
Here is an example:
Relation: military branch
Context: two of her sons , tunis and thomas tingey rose to prominence in the union navy during the civil war .
Head Entity: tunis
Tail Entity: union navy
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "military branch":
Relation: military branch  
Context: after serving in the army, he transitioned to a leadership role in the air force, where he implemented several key strategies.  
Head Entity: air force  
Tail Entity: army  

Relation: military branch  
Context: the general was honored for his service in the marine corps, where he led multiple successful missions.  
Head Entity: marine corps  
Tail Entity: general  
#### child ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "child" means subject has object as child.
Here is an example:
Relation: child
Context: in 1406 adolf married marie of burgundy , daughter of john the fearless and margaret of bavaria .
Head Entity: john the fearless
Tail Entity: marie of burgundy
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "child":
Relation: child  
Context: in 1980, michael and sarah welcomed their first child, a daughter named emily, into the world.  
Head Entity: michael  
Tail Entity: emily  

Relation: child  
Context: during the 19th century, queen victoria had nine children, including her beloved daughter, princess alice.  
Head Entity: queen victoria  
Tail Entity: princess alice  
MemoryTrain:  epoch 15, batch     0 | loss: 5.0797224MemoryTrain:  epoch 15, batch     1 | loss: 3.4531415MemoryTrain:  epoch 15, batch     2 | loss: 3.1036016MemoryTrain:  epoch 15, batch     3 | loss: 3.7122721MemoryTrain:  epoch 15, batch     4 | loss: 3.7792216MemoryTrain:  epoch 15, batch     5 | loss: 7.0922945MemoryTrain:  epoch 15, batch     6 | loss: 4.0787473MemoryTrain:  epoch 15, batch     7 | loss: 4.2249366MemoryTrain:  epoch 15, batch     8 | loss: 5.3437596MemoryTrain:  epoch  5, batch     9 | loss: 11.0276790MemoryTrain:  epoch 15, batch     0 | loss: 3.6832292MemoryTrain:  epoch 15, batch     1 | loss: 3.1968419MemoryTrain:  epoch 15, batch     2 | loss: 2.8187125MemoryTrain:  epoch 15, batch     3 | loss: 3.3558300MemoryTrain:  epoch 15, batch     4 | loss: 2.7638517MemoryTrain:  epoch 15, batch     5 | loss: 6.2959631MemoryTrain:  epoch 15, batch     6 | loss: 3.7272999MemoryTrain:  epoch 15, batch     7 | loss: 5.0591662MemoryTrain:  epoch 15, batch     8 | loss: 3.1995055MemoryTrain:  epoch  5, batch     9 | loss: 10.2919926MemoryTrain:  epoch 15, batch     0 | loss: 3.0410627MemoryTrain:  epoch 15, batch     1 | loss: 5.1451317MemoryTrain:  epoch 15, batch     2 | loss: 3.4183280MemoryTrain:  epoch 15, batch     3 | loss: 3.1141681MemoryTrain:  epoch 15, batch     4 | loss: 2.0663761MemoryTrain:  epoch 15, batch     5 | loss: 3.1187473MemoryTrain:  epoch 15, batch     6 | loss: 3.4912244MemoryTrain:  epoch 15, batch     7 | loss: 2.6674642MemoryTrain:  epoch 15, batch     8 | loss: 3.0267150MemoryTrain:  epoch  5, batch     9 | loss: 11.0019972MemoryTrain:  epoch 15, batch     0 | loss: 3.9974609MemoryTrain:  epoch 15, batch     1 | loss: 4.8729760MemoryTrain:  epoch 15, batch     2 | loss: 3.3508319MemoryTrain:  epoch 15, batch     3 | loss: 1.7622737MemoryTrain:  epoch 15, batch     4 | loss: 5.0535006MemoryTrain:  epoch 15, batch     5 | loss: 3.6637238MemoryTrain:  epoch 15, batch     6 | loss: 2.5780749MemoryTrain:  epoch 15, batch     7 | loss: 2.5186841MemoryTrain:  epoch 15, batch     8 | loss: 3.7679513MemoryTrain:  epoch  5, batch     9 | loss: 8.3429206MemoryTrain:  epoch 15, batch     0 | loss: 2.2580010MemoryTrain:  epoch 15, batch     1 | loss: 2.1010081MemoryTrain:  epoch 15, batch     2 | loss: 2.5544770MemoryTrain:  epoch 15, batch     3 | loss: 2.4676596MemoryTrain:  epoch 15, batch     4 | loss: 1.6707171MemoryTrain:  epoch 15, batch     5 | loss: 2.6583049MemoryTrain:  epoch 15, batch     6 | loss: 2.1079731MemoryTrain:  epoch 15, batch     7 | loss: 2.1772361MemoryTrain:  epoch 15, batch     8 | loss: 1.6926671MemoryTrain:  epoch  5, batch     9 | loss: 8.6105978MemoryTrain:  epoch 15, batch     0 | loss: 3.2579222MemoryTrain:  epoch 15, batch     1 | loss: 2.2969973MemoryTrain:  epoch 15, batch     2 | loss: 1.9656535MemoryTrain:  epoch 15, batch     3 | loss: 2.1058846MemoryTrain:  epoch 15, batch     4 | loss: 2.4163140MemoryTrain:  epoch 15, batch     5 | loss: 1.5296797MemoryTrain:  epoch 15, batch     6 | loss: 2.9128628MemoryTrain:  epoch 15, batch     7 | loss: 1.9720963MemoryTrain:  epoch 15, batch     8 | loss: 2.1385104MemoryTrain:  epoch  5, batch     9 | loss: 9.7023588MemoryTrain:  epoch 15, batch     0 | loss: 6.3319183MemoryTrain:  epoch 15, batch     1 | loss: 2.1017028MemoryTrain:  epoch 15, batch     2 | loss: 1.6739765MemoryTrain:  epoch 15, batch     3 | loss: 2.2196044MemoryTrain:  epoch 15, batch     4 | loss: 2.1486580MemoryTrain:  epoch 15, batch     5 | loss: 4.9026281MemoryTrain:  epoch 15, batch     6 | loss: 2.1807164MemoryTrain:  epoch 15, batch     7 | loss: 1.9147893MemoryTrain:  epoch 15, batch     8 | loss: 1.8570766MemoryTrain:  epoch  5, batch     9 | loss: 8.4531221MemoryTrain:  epoch 15, batch     0 | loss: 2.4359583MemoryTrain:  epoch 15, batch     1 | loss: 5.2721772MemoryTrain:  epoch 15, batch     2 | loss: 1.7725883MemoryTrain:  epoch 15, batch     3 | loss: 2.2784665MemoryTrain:  epoch 15, batch     4 | loss: 1.9845699MemoryTrain:  epoch 15, batch     5 | loss: 2.0719134MemoryTrain:  epoch 15, batch     6 | loss: 1.9079317MemoryTrain:  epoch 15, batch     7 | loss: 2.5940828MemoryTrain:  epoch 15, batch     8 | loss: 3.1039550MemoryTrain:  epoch  5, batch     9 | loss: 7.9461068MemoryTrain:  epoch 15, batch     0 | loss: 1.7178037MemoryTrain:  epoch 15, batch     1 | loss: 4.4167652MemoryTrain:  epoch 15, batch     2 | loss: 2.3447503MemoryTrain:  epoch 15, batch     3 | loss: 3.1803786MemoryTrain:  epoch 15, batch     4 | loss: 1.4794416MemoryTrain:  epoch 15, batch     5 | loss: 1.5204916MemoryTrain:  epoch 15, batch     6 | loss: 2.1551022MemoryTrain:  epoch 15, batch     7 | loss: 1.4931516MemoryTrain:  epoch 15, batch     8 | loss: 1.6168495MemoryTrain:  epoch  5, batch     9 | loss: 20.4973431MemoryTrain:  epoch 15, batch     0 | loss: 4.5519069MemoryTrain:  epoch 15, batch     1 | loss: 7.0377575MemoryTrain:  epoch 15, batch     2 | loss: 1.5885483MemoryTrain:  epoch 15, batch     3 | loss: 2.5348902MemoryTrain:  epoch 15, batch     4 | loss: 2.1394710MemoryTrain:  epoch 15, batch     5 | loss: 1.7795078MemoryTrain:  epoch 15, batch     6 | loss: 2.4452192MemoryTrain:  epoch 15, batch     7 | loss: 4.6849090MemoryTrain:  epoch 15, batch     8 | loss: 5.8358068MemoryTrain:  epoch  5, batch     9 | loss: 8.3994581
[EVAL] batch:    0 | acc: 50.00%,  total acc: 50.00%   [EVAL] batch:    1 | acc: 50.00%,  total acc: 50.00%   [EVAL] batch:    2 | acc: 56.25%,  total acc: 52.08%   [EVAL] batch:    3 | acc: 62.50%,  total acc: 54.69%   [EVAL] batch:    4 | acc: 50.00%,  total acc: 53.75%   [EVAL] batch:    5 | acc: 56.25%,  total acc: 54.17%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 56.25%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 60.16%   [EVAL] batch:    8 | acc: 75.00%,  total acc: 61.81%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 63.12%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 64.77%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 65.62%   [EVAL] batch:   12 | acc: 100.00%,  total acc: 68.27%   [EVAL] batch:   13 | acc: 87.50%,  total acc: 69.64%   [EVAL] batch:   14 | acc: 81.25%,  total acc: 70.42%   [EVAL] batch:   15 | acc: 75.00%,  total acc: 70.70%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 72.06%   [EVAL] batch:   17 | acc: 87.50%,  total acc: 72.92%   [EVAL] batch:   18 | acc: 81.25%,  total acc: 73.36%   [EVAL] batch:   19 | acc: 6.25%,  total acc: 70.00%   [EVAL] batch:   20 | acc: 43.75%,  total acc: 68.75%   [EVAL] batch:   21 | acc: 31.25%,  total acc: 67.05%   [EVAL] batch:   22 | acc: 31.25%,  total acc: 65.49%   [EVAL] batch:   23 | acc: 25.00%,  total acc: 63.80%   [EVAL] batch:   24 | acc: 18.75%,  total acc: 62.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 63.46%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 64.58%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 65.85%   [EVAL] batch:   28 | acc: 81.25%,  total acc: 66.38%   [EVAL] batch:   29 | acc: 93.75%,  total acc: 67.29%   [EVAL] batch:   30 | acc: 81.25%,  total acc: 67.74%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 68.75%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 69.70%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 70.59%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 71.43%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 72.22%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 72.97%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 73.68%   [EVAL] batch:   38 | acc: 93.75%,  total acc: 74.20%   [EVAL] batch:   39 | acc: 81.25%,  total acc: 74.38%   [EVAL] batch:   40 | acc: 81.25%,  total acc: 74.54%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 75.00%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 75.44%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 75.85%   [EVAL] batch:   44 | acc: 68.75%,  total acc: 75.69%   [EVAL] batch:   45 | acc: 62.50%,  total acc: 75.41%   [EVAL] batch:   46 | acc: 68.75%,  total acc: 75.27%   [EVAL] batch:   47 | acc: 81.25%,  total acc: 75.39%   [EVAL] batch:   48 | acc: 62.50%,  total acc: 75.13%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 75.50%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 75.86%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 76.20%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 76.53%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 76.62%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 76.93%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 77.12%   [EVAL] batch:   56 | acc: 43.75%,  total acc: 76.54%   [EVAL] batch:   57 | acc: 50.00%,  total acc: 76.08%   [EVAL] batch:   58 | acc: 56.25%,  total acc: 75.74%   [EVAL] batch:   59 | acc: 31.25%,  total acc: 75.00%   [EVAL] batch:   60 | acc: 62.50%,  total acc: 74.80%   [EVAL] batch:   61 | acc: 18.75%,  total acc: 73.89%   [EVAL] batch:   62 | acc: 37.50%,  total acc: 73.31%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 81.25%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 78.75%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 80.21%   [EVAL] batch:    6 | acc: 43.75%,  total acc: 75.00%   [EVAL] batch:    7 | acc: 56.25%,  total acc: 72.66%   [EVAL] batch:    8 | acc: 50.00%,  total acc: 70.14%   [EVAL] batch:    9 | acc: 56.25%,  total acc: 68.75%   [EVAL] batch:   10 | acc: 43.75%,  total acc: 66.48%   [EVAL] batch:   11 | acc: 37.50%,  total acc: 64.06%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 64.42%   [EVAL] batch:   13 | acc: 81.25%,  total acc: 65.62%   [EVAL] batch:   14 | acc: 87.50%,  total acc: 67.08%   [EVAL] batch:   15 | acc: 87.50%,  total acc: 68.36%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 70.22%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 71.53%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 72.70%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 72.81%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 73.51%   [EVAL] batch:   21 | acc: 68.75%,  total acc: 73.30%   [EVAL] batch:   22 | acc: 68.75%,  total acc: 73.10%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 74.22%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 74.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 75.72%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 76.62%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 77.46%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 78.02%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 78.75%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 79.44%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 80.08%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 80.68%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 81.61%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 82.12%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 82.60%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 83.06%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 83.49%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 83.91%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 84.30%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 84.52%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 84.88%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 84.80%   [EVAL] batch:   44 | acc: 68.75%,  total acc: 84.44%   [EVAL] batch:   45 | acc: 62.50%,  total acc: 83.97%   [EVAL] batch:   46 | acc: 43.75%,  total acc: 83.11%   [EVAL] batch:   47 | acc: 62.50%,  total acc: 82.68%   [EVAL] batch:   48 | acc: 62.50%,  total acc: 82.27%   [EVAL] batch:   49 | acc: 50.00%,  total acc: 81.62%   [EVAL] batch:   50 | acc: 75.00%,  total acc: 81.50%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 81.61%   [EVAL] batch:   52 | acc: 81.25%,  total acc: 81.60%   [EVAL] batch:   53 | acc: 62.50%,  total acc: 81.25%   [EVAL] batch:   54 | acc: 50.00%,  total acc: 80.68%   [EVAL] batch:   55 | acc: 62.50%,  total acc: 80.36%   [EVAL] batch:   56 | acc: 81.25%,  total acc: 80.37%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 80.60%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 80.83%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 81.04%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 81.35%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 81.65%   [EVAL] batch:   62 | acc: 93.75%,  total acc: 81.85%   [EVAL] batch:   63 | acc: 93.75%,  total acc: 82.03%   [EVAL] batch:   64 | acc: 93.75%,  total acc: 82.21%   [EVAL] batch:   65 | acc: 93.75%,  total acc: 82.39%   [EVAL] batch:   66 | acc: 81.25%,  total acc: 82.37%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 82.63%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 82.88%   [EVAL] batch:   69 | acc: 81.25%,  total acc: 82.86%   [EVAL] batch:   70 | acc: 87.50%,  total acc: 82.92%   [EVAL] batch:   71 | acc: 87.50%,  total acc: 82.99%   [EVAL] batch:   72 | acc: 93.75%,  total acc: 83.13%   [EVAL] batch:   73 | acc: 81.25%,  total acc: 83.11%   [EVAL] batch:   74 | acc: 81.25%,  total acc: 83.08%   [EVAL] batch:   75 | acc: 68.75%,  total acc: 82.89%   [EVAL] batch:   76 | acc: 81.25%,  total acc: 82.87%   [EVAL] batch:   77 | acc: 81.25%,  total acc: 82.85%   [EVAL] batch:   78 | acc: 50.00%,  total acc: 82.44%   [EVAL] batch:   79 | acc: 68.75%,  total acc: 82.27%   [EVAL] batch:   80 | acc: 56.25%,  total acc: 81.94%   [EVAL] batch:   81 | acc: 56.25%,  total acc: 81.63%   [EVAL] batch:   82 | acc: 68.75%,  total acc: 81.48%   [EVAL] batch:   83 | acc: 62.50%,  total acc: 81.25%   [EVAL] batch:   84 | acc: 62.50%,  total acc: 81.03%   [EVAL] batch:   85 | acc: 75.00%,  total acc: 80.96%   [EVAL] batch:   86 | acc: 50.00%,  total acc: 80.60%   [EVAL] batch:   87 | acc: 81.25%,  total acc: 80.61%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 80.83%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 81.04%   [EVAL] batch:   90 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 81.45%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 81.65%   [EVAL] batch:   93 | acc: 93.75%,  total acc: 81.78%   [EVAL] batch:   94 | acc: 50.00%,  total acc: 81.45%   [EVAL] batch:   95 | acc: 75.00%,  total acc: 81.38%   [EVAL] batch:   96 | acc: 62.50%,  total acc: 81.19%   [EVAL] batch:   97 | acc: 62.50%,  total acc: 80.99%   [EVAL] batch:   98 | acc: 75.00%,  total acc: 80.93%   [EVAL] batch:   99 | acc: 43.75%,  total acc: 80.56%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 80.75%   [EVAL] batch:  101 | acc: 100.00%,  total acc: 80.94%   [EVAL] batch:  102 | acc: 100.00%,  total acc: 81.13%   [EVAL] batch:  103 | acc: 100.00%,  total acc: 81.31%   [EVAL] batch:  104 | acc: 100.00%,  total acc: 81.49%   [EVAL] batch:  105 | acc: 87.50%,  total acc: 81.54%   [EVAL] batch:  106 | acc: 100.00%,  total acc: 81.72%   [EVAL] batch:  107 | acc: 100.00%,  total acc: 81.89%   [EVAL] batch:  108 | acc: 100.00%,  total acc: 82.05%   [EVAL] batch:  109 | acc: 100.00%,  total acc: 82.22%   [EVAL] batch:  110 | acc: 100.00%,  total acc: 82.38%   [EVAL] batch:  111 | acc: 93.75%,  total acc: 82.48%   [EVAL] batch:  112 | acc: 100.00%,  total acc: 82.63%   [EVAL] batch:  113 | acc: 81.25%,  total acc: 82.62%   [EVAL] batch:  114 | acc: 93.75%,  total acc: 82.72%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 82.87%   [EVAL] batch:  116 | acc: 87.50%,  total acc: 82.91%   [EVAL] batch:  117 | acc: 87.50%,  total acc: 82.94%   [EVAL] batch:  118 | acc: 93.75%,  total acc: 83.04%   [EVAL] batch:  119 | acc: 93.75%,  total acc: 83.12%   [EVAL] batch:  120 | acc: 87.50%,  total acc: 83.16%   [EVAL] batch:  121 | acc: 93.75%,  total acc: 83.25%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 83.38%   [EVAL] batch:  123 | acc: 93.75%,  total acc: 83.47%   [EVAL] batch:  124 | acc: 93.75%,  total acc: 83.55%   [EVAL] batch:  125 | acc: 25.00%,  total acc: 83.09%   [EVAL] batch:  126 | acc: 50.00%,  total acc: 82.82%   [EVAL] batch:  127 | acc: 50.00%,  total acc: 82.57%   [EVAL] batch:  128 | acc: 31.25%,  total acc: 82.17%   [EVAL] batch:  129 | acc: 56.25%,  total acc: 81.97%   [EVAL] batch:  130 | acc: 50.00%,  total acc: 81.73%   [EVAL] batch:  131 | acc: 81.25%,  total acc: 81.72%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 81.86%   [EVAL] batch:  133 | acc: 100.00%,  total acc: 82.00%   [EVAL] batch:  134 | acc: 93.75%,  total acc: 82.08%   [EVAL] batch:  135 | acc: 100.00%,  total acc: 82.22%   [EVAL] batch:  136 | acc: 100.00%,  total acc: 82.34%   [EVAL] batch:  137 | acc: 87.50%,  total acc: 82.38%   [EVAL] batch:  138 | acc: 87.50%,  total acc: 82.42%   [EVAL] batch:  139 | acc: 81.25%,  total acc: 82.41%   [EVAL] batch:  140 | acc: 93.75%,  total acc: 82.49%   [EVAL] batch:  141 | acc: 87.50%,  total acc: 82.53%   [EVAL] batch:  142 | acc: 68.75%,  total acc: 82.43%   [EVAL] batch:  143 | acc: 68.75%,  total acc: 82.34%   [EVAL] batch:  144 | acc: 43.75%,  total acc: 82.07%   [EVAL] batch:  145 | acc: 50.00%,  total acc: 81.85%   [EVAL] batch:  146 | acc: 81.25%,  total acc: 81.85%   [EVAL] batch:  147 | acc: 75.00%,  total acc: 81.80%   [EVAL] batch:  148 | acc: 43.75%,  total acc: 81.54%   [EVAL] batch:  149 | acc: 75.00%,  total acc: 81.50%   [EVAL] batch:  150 | acc: 6.25%,  total acc: 81.00%   [EVAL] batch:  151 | acc: 18.75%,  total acc: 80.59%   [EVAL] batch:  152 | acc: 12.50%,  total acc: 80.15%   [EVAL] batch:  153 | acc: 6.25%,  total acc: 79.67%   [EVAL] batch:  154 | acc: 25.00%,  total acc: 79.31%   [EVAL] batch:  155 | acc: 0.00%,  total acc: 78.81%   [EVAL] batch:  156 | acc: 68.75%,  total acc: 78.74%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 78.84%   [EVAL] batch:  158 | acc: 87.50%,  total acc: 78.89%   [EVAL] batch:  159 | acc: 100.00%,  total acc: 79.02%   [EVAL] batch:  160 | acc: 87.50%,  total acc: 79.08%   [EVAL] batch:  161 | acc: 81.25%,  total acc: 79.09%   [EVAL] batch:  162 | acc: 93.75%,  total acc: 79.18%   [EVAL] batch:  163 | acc: 87.50%,  total acc: 79.23%   [EVAL] batch:  164 | acc: 81.25%,  total acc: 79.24%   [EVAL] batch:  165 | acc: 87.50%,  total acc: 79.29%   [EVAL] batch:  166 | acc: 81.25%,  total acc: 79.30%   [EVAL] batch:  167 | acc: 81.25%,  total acc: 79.32%   [EVAL] batch:  168 | acc: 87.50%,  total acc: 79.36%   [EVAL] batch:  169 | acc: 37.50%,  total acc: 79.12%   [EVAL] batch:  170 | acc: 6.25%,  total acc: 78.69%   [EVAL] batch:  171 | acc: 18.75%,  total acc: 78.34%   [EVAL] batch:  172 | acc: 25.00%,  total acc: 78.03%   [EVAL] batch:  173 | acc: 31.25%,  total acc: 77.77%   [EVAL] batch:  174 | acc: 6.25%,  total acc: 77.36%   [EVAL] batch:  175 | acc: 31.25%,  total acc: 77.10%   [EVAL] batch:  176 | acc: 31.25%,  total acc: 76.84%   [EVAL] batch:  177 | acc: 25.00%,  total acc: 76.54%   [EVAL] batch:  178 | acc: 37.50%,  total acc: 76.33%   [EVAL] batch:  179 | acc: 37.50%,  total acc: 76.11%   [EVAL] batch:  180 | acc: 31.25%,  total acc: 75.86%   [EVAL] batch:  181 | acc: 43.75%,  total acc: 75.69%   [EVAL] batch:  182 | acc: 37.50%,  total acc: 75.48%   [EVAL] batch:  183 | acc: 50.00%,  total acc: 75.34%   [EVAL] batch:  184 | acc: 81.25%,  total acc: 75.37%   [EVAL] batch:  185 | acc: 50.00%,  total acc: 75.24%   [EVAL] batch:  186 | acc: 75.00%,  total acc: 75.23%   [EVAL] batch:  187 | acc: 50.00%,  total acc: 75.10%   [EVAL] batch:  188 | acc: 75.00%,  total acc: 75.10%   [EVAL] batch:  189 | acc: 43.75%,  total acc: 74.93%   [EVAL] batch:  190 | acc: 75.00%,  total acc: 74.93%   [EVAL] batch:  191 | acc: 56.25%,  total acc: 74.84%   [EVAL] batch:  192 | acc: 56.25%,  total acc: 74.74%   [EVAL] batch:  193 | acc: 56.25%,  total acc: 74.65%   [EVAL] batch:  194 | acc: 81.25%,  total acc: 74.68%   [EVAL] batch:  195 | acc: 62.50%,  total acc: 74.62%   [EVAL] batch:  196 | acc: 75.00%,  total acc: 74.62%   [EVAL] batch:  197 | acc: 62.50%,  total acc: 74.56%   [EVAL] batch:  198 | acc: 31.25%,  total acc: 74.34%   [EVAL] batch:  199 | acc: 100.00%,  total acc: 74.47%   [EVAL] batch:  200 | acc: 31.25%,  total acc: 74.25%   [EVAL] batch:  201 | acc: 18.75%,  total acc: 73.98%   [EVAL] batch:  202 | acc: 37.50%,  total acc: 73.80%   [EVAL] batch:  203 | acc: 31.25%,  total acc: 73.59%   [EVAL] batch:  204 | acc: 25.00%,  total acc: 73.35%   [EVAL] batch:  205 | acc: 50.00%,  total acc: 73.24%   [EVAL] batch:  206 | acc: 62.50%,  total acc: 73.19%   [EVAL] batch:  207 | acc: 87.50%,  total acc: 73.26%   [EVAL] batch:  208 | acc: 100.00%,  total acc: 73.39%   [EVAL] batch:  209 | acc: 93.75%,  total acc: 73.48%   [EVAL] batch:  210 | acc: 87.50%,  total acc: 73.55%   [EVAL] batch:  211 | acc: 81.25%,  total acc: 73.58%   [EVAL] batch:  212 | acc: 87.50%,  total acc: 73.65%   [EVAL] batch:  213 | acc: 75.00%,  total acc: 73.66%   [EVAL] batch:  214 | acc: 81.25%,  total acc: 73.69%   [EVAL] batch:  215 | acc: 75.00%,  total acc: 73.70%   [EVAL] batch:  216 | acc: 75.00%,  total acc: 73.70%   [EVAL] batch:  217 | acc: 81.25%,  total acc: 73.74%   [EVAL] batch:  218 | acc: 68.75%,  total acc: 73.72%   [EVAL] batch:  219 | acc: 68.75%,  total acc: 73.69%   [EVAL] batch:  220 | acc: 31.25%,  total acc: 73.50%   [EVAL] batch:  221 | acc: 37.50%,  total acc: 73.34%   [EVAL] batch:  222 | acc: 31.25%,  total acc: 73.15%   [EVAL] batch:  223 | acc: 18.75%,  total acc: 72.91%   [EVAL] batch:  224 | acc: 50.00%,  total acc: 72.81%   [EVAL] batch:  225 | acc: 75.00%,  total acc: 72.82%   [EVAL] batch:  226 | acc: 75.00%,  total acc: 72.82%   [EVAL] batch:  227 | acc: 75.00%,  total acc: 72.83%   [EVAL] batch:  228 | acc: 93.75%,  total acc: 72.93%   [EVAL] batch:  229 | acc: 68.75%,  total acc: 72.91%   [EVAL] batch:  230 | acc: 75.00%,  total acc: 72.92%   [EVAL] batch:  231 | acc: 93.75%,  total acc: 73.01%   [EVAL] batch:  232 | acc: 68.75%,  total acc: 72.99%   [EVAL] batch:  233 | acc: 68.75%,  total acc: 72.97%   [EVAL] batch:  234 | acc: 62.50%,  total acc: 72.93%   [EVAL] batch:  235 | acc: 56.25%,  total acc: 72.85%   [EVAL] batch:  236 | acc: 81.25%,  total acc: 72.89%   [EVAL] batch:  237 | acc: 93.75%,  total acc: 72.98%   [EVAL] batch:  238 | acc: 100.00%,  total acc: 73.09%   [EVAL] batch:  239 | acc: 100.00%,  total acc: 73.20%   [EVAL] batch:  240 | acc: 100.00%,  total acc: 73.31%   [EVAL] batch:  241 | acc: 100.00%,  total acc: 73.42%   [EVAL] batch:  242 | acc: 100.00%,  total acc: 73.53%   [EVAL] batch:  243 | acc: 100.00%,  total acc: 73.64%   [EVAL] batch:  244 | acc: 100.00%,  total acc: 73.75%   [EVAL] batch:  245 | acc: 93.75%,  total acc: 73.83%   [EVAL] batch:  246 | acc: 100.00%,  total acc: 73.94%   [EVAL] batch:  247 | acc: 100.00%,  total acc: 74.04%   [EVAL] batch:  248 | acc: 100.00%,  total acc: 74.15%   [EVAL] batch:  249 | acc: 100.00%,  total acc: 74.25%   [EVAL] batch:  250 | acc: 50.00%,  total acc: 74.15%   [EVAL] batch:  251 | acc: 50.00%,  total acc: 74.06%   [EVAL] batch:  252 | acc: 56.25%,  total acc: 73.99%   [EVAL] batch:  253 | acc: 62.50%,  total acc: 73.94%   [EVAL] batch:  254 | acc: 50.00%,  total acc: 73.85%   [EVAL] batch:  255 | acc: 56.25%,  total acc: 73.78%   [EVAL] batch:  256 | acc: 68.75%,  total acc: 73.76%   [EVAL] batch:  257 | acc: 87.50%,  total acc: 73.81%   [EVAL] batch:  258 | acc: 75.00%,  total acc: 73.82%   [EVAL] batch:  259 | acc: 75.00%,  total acc: 73.82%   [EVAL] batch:  260 | acc: 81.25%,  total acc: 73.85%   [EVAL] batch:  261 | acc: 75.00%,  total acc: 73.85%   [EVAL] batch:  262 | acc: 100.00%,  total acc: 73.95%   [EVAL] batch:  263 | acc: 87.50%,  total acc: 74.01%   [EVAL] batch:  264 | acc: 81.25%,  total acc: 74.03%   [EVAL] batch:  265 | acc: 75.00%,  total acc: 74.04%   [EVAL] batch:  266 | acc: 93.75%,  total acc: 74.11%   [EVAL] batch:  267 | acc: 87.50%,  total acc: 74.16%   [EVAL] batch:  268 | acc: 81.25%,  total acc: 74.19%   [EVAL] batch:  269 | acc: 6.25%,  total acc: 73.94%   [EVAL] batch:  270 | acc: 43.75%,  total acc: 73.82%   [EVAL] batch:  271 | acc: 31.25%,  total acc: 73.67%   [EVAL] batch:  272 | acc: 31.25%,  total acc: 73.51%   [EVAL] batch:  273 | acc: 25.00%,  total acc: 73.33%   [EVAL] batch:  274 | acc: 18.75%,  total acc: 73.14%   [EVAL] batch:  275 | acc: 100.00%,  total acc: 73.23%   [EVAL] batch:  276 | acc: 93.75%,  total acc: 73.31%   [EVAL] batch:  277 | acc: 100.00%,  total acc: 73.40%   [EVAL] batch:  278 | acc: 81.25%,  total acc: 73.43%   [EVAL] batch:  279 | acc: 93.75%,  total acc: 73.50%   [EVAL] batch:  280 | acc: 81.25%,  total acc: 73.53%   [EVAL] batch:  281 | acc: 100.00%,  total acc: 73.63%   [EVAL] batch:  282 | acc: 100.00%,  total acc: 73.72%   [EVAL] batch:  283 | acc: 100.00%,  total acc: 73.81%   [EVAL] batch:  284 | acc: 100.00%,  total acc: 73.90%   [EVAL] batch:  285 | acc: 100.00%,  total acc: 73.99%   [EVAL] batch:  286 | acc: 100.00%,  total acc: 74.09%   [EVAL] batch:  287 | acc: 100.00%,  total acc: 74.18%   [EVAL] batch:  288 | acc: 93.75%,  total acc: 74.24%   [EVAL] batch:  289 | acc: 81.25%,  total acc: 74.27%   [EVAL] batch:  290 | acc: 81.25%,  total acc: 74.29%   [EVAL] batch:  291 | acc: 93.75%,  total acc: 74.36%   [EVAL] batch:  292 | acc: 93.75%,  total acc: 74.42%   [EVAL] batch:  293 | acc: 93.75%,  total acc: 74.49%   [EVAL] batch:  294 | acc: 68.75%,  total acc: 74.47%   [EVAL] batch:  295 | acc: 62.50%,  total acc: 74.43%   [EVAL] batch:  296 | acc: 68.75%,  total acc: 74.41%   [EVAL] batch:  297 | acc: 81.25%,  total acc: 74.43%   [EVAL] batch:  298 | acc: 62.50%,  total acc: 74.39%   [EVAL] batch:  299 | acc: 93.75%,  total acc: 74.46%   [EVAL] batch:  300 | acc: 93.75%,  total acc: 74.52%   [EVAL] batch:  301 | acc: 93.75%,  total acc: 74.59%   [EVAL] batch:  302 | acc: 93.75%,  total acc: 74.65%   [EVAL] batch:  303 | acc: 81.25%,  total acc: 74.67%   [EVAL] batch:  304 | acc: 93.75%,  total acc: 74.73%   [EVAL] batch:  305 | acc: 87.50%,  total acc: 74.78%   [EVAL] batch:  306 | acc: 43.75%,  total acc: 74.67%   [EVAL] batch:  307 | acc: 50.00%,  total acc: 74.59%   [EVAL] batch:  308 | acc: 56.25%,  total acc: 74.53%   [EVAL] batch:  309 | acc: 31.25%,  total acc: 74.40%   [EVAL] batch:  310 | acc: 62.50%,  total acc: 74.36%   [EVAL] batch:  311 | acc: 18.75%,  total acc: 74.18%   [EVAL] batch:  312 | acc: 37.50%,  total acc: 74.06%   
cur_acc:  ['0.9484', '0.8710', '0.7014', '0.7480', '0.7331']
his_acc:  ['0.9484', '0.9020', '0.8162', '0.7650', '0.7406']
CurrentTrain: epoch 15, batch     0 | loss: 20.3309028CurrentTrain: epoch 15, batch     1 | loss: 13.6485010CurrentTrain: epoch 15, batch     2 | loss: 18.6984514CurrentTrain: epoch  1, batch     3 | loss: 8.7460178CurrentTrain: epoch 15, batch     0 | loss: 20.3062630CurrentTrain: epoch 15, batch     1 | loss: 17.3143170CurrentTrain: epoch 15, batch     2 | loss: 25.1992545CurrentTrain: epoch  1, batch     3 | loss: 8.7572638CurrentTrain: epoch 15, batch     0 | loss: 12.1538270CurrentTrain: epoch 15, batch     1 | loss: 10.7400019CurrentTrain: epoch 15, batch     2 | loss: 11.6862056CurrentTrain: epoch  1, batch     3 | loss: 8.0259580CurrentTrain: epoch 15, batch     0 | loss: 11.4539743CurrentTrain: epoch 15, batch     1 | loss: 10.1430300CurrentTrain: epoch 15, batch     2 | loss: 13.1650247CurrentTrain: epoch  1, batch     3 | loss: 9.2251450CurrentTrain: epoch 15, batch     0 | loss: 14.5135233CurrentTrain: epoch 15, batch     1 | loss: 15.7963551CurrentTrain: epoch 15, batch     2 | loss: 10.7911512CurrentTrain: epoch  1, batch     3 | loss: 6.5270146CurrentTrain: epoch 15, batch     0 | loss: 12.3620878CurrentTrain: epoch 15, batch     1 | loss: 9.9609757CurrentTrain: epoch 15, batch     2 | loss: 16.7881057CurrentTrain: epoch  1, batch     3 | loss: 6.8060656CurrentTrain: epoch 15, batch     0 | loss: 13.0830552CurrentTrain: epoch 15, batch     1 | loss: 12.0999247CurrentTrain: epoch 15, batch     2 | loss: 6.9810932CurrentTrain: epoch  1, batch     3 | loss: 7.3480178CurrentTrain: epoch 15, batch     0 | loss: 10.4011713CurrentTrain: epoch 15, batch     1 | loss: 8.8171060CurrentTrain: epoch 15, batch     2 | loss: 9.9472390CurrentTrain: epoch  1, batch     3 | loss: 6.5119928CurrentTrain: epoch 15, batch     0 | loss: 8.9968490CurrentTrain: epoch 15, batch     1 | loss: 7.7750507CurrentTrain: epoch 15, batch     2 | loss: 9.7177713CurrentTrain: epoch  1, batch     3 | loss: 7.3490891CurrentTrain: epoch 15, batch     0 | loss: 12.9011311CurrentTrain: epoch 15, batch     1 | loss: 8.8855993CurrentTrain: epoch 15, batch     2 | loss: 8.1314311CurrentTrain: epoch  1, batch     3 | loss: 6.2485160
#### follows ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "follows" means immediately prior item in a series of which the subject is a part.
Here is an example:
Relation: follows
Context: ( 1988 ) , " night watch " ( 2004 ) , " day watch " ( 2006 ) and " legend № 17 " ( 2013 ) .
Head Entity: day watch
Tail Entity: night watch
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "follows":
Relation: follows  
Context: ( 1995 ) , " The Phantom Menace " ( 1999 ) , " Attack of the Clones " ( 2002 ) and " Revenge of the Sith " ( 2005 ) .  
Head Entity: Attack of the Clones  
Tail Entity: The Phantom Menace  

Relation: follows  
Context: ( 2010 ) , " The Hunger Games " ( 2012 ) , " Catching Fire " ( 2013 ) and " Mockingjay " ( 2014 ) .  
Head Entity: Catching Fire  
Tail Entity: The Hunger Games  
#### sport ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "sport" means sport that the subject participates or participated in or is associated with.
Here is an example:
Relation: sport
Context: it is primarily used for baseball and serves as the home stadium for yaquis de obregón .
Head Entity: yaquis de obregón
Tail Entity: baseball
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "sport":
Relation: sport  
Context: she has been a professional athlete in tennis since she was a teenager and has won several championships.  
Head Entity: she  
Tail Entity: tennis  

Relation: sport  
Context: the city is known for its vibrant soccer culture and is home to several professional teams.  
Head Entity: the city  
Tail Entity: soccer  
#### father ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "father" means male parent of the subject.
Here is an example:
Relation: father
Context: after the crowning of his nephew radoslav , the son of stefan , sava left the serbian maritime in 1229 for a trip to palestine .
Head Entity: radoslav
Tail Entity: stefan
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "father":
Relation: father  
Context: in the historical records, it is noted that king henry was the father of prince edward, who later became a significant figure in the kingdom.  
Head Entity: prince edward  
Tail Entity: king henry  

Relation: father  
Context: during the family reunion, it was revealed that michael was the father of two daughters, emily and sarah, who both excelled in their studies.  
Head Entity: michael  
Tail Entity: emily  
#### original network ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "original network" means network or service that originally broadcast a radio or television program.
Here is an example:
Relation: original network
Context: since 2009 , culshaw has starred in the bbc one comedy sketch show " the impressions show " alongside debra stephenson .
Head Entity: the impressions show
Tail Entity: bbc one
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "original network":
Relation: original network  
Context: The popular series "Breaking Bad" first aired on AMC, captivating audiences with its intense storytelling and character development.  
Head Entity: Breaking Bad  
Tail Entity: AMC  

Relation: original network  
Context: "Friends" became a cultural phenomenon when it premiered on NBC, showcasing the lives of six friends living in New York City.  
Head Entity: Friends  
Tail Entity: NBC  
#### original language of film or TV show ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "original language of film or TV show" means language in which a film or a performance work was originally created.
Here is an example:
Relation: original language of film or TV show
Context: he was also nominated twice for the filmfare best telugu actor award , for the films bhale bhale magadivoy and " gentleman " .
Head Entity: gentleman
Tail Entity: telugu
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "original language of film or TV show":
Relation: original language of film or TV show  
Context: The movie "Parasite" received critical acclaim and was originally produced in Korean, showcasing the unique storytelling style of South Korean cinema.  
Head Entity: Parasite  
Tail Entity: Korean  

Relation: original language of film or TV show  
Context: The animated series "Avatar: The Last Airbender" was created in English, captivating audiences with its rich world-building and character development.  
Head Entity: Avatar: The Last Airbender  
Tail Entity: English  
#### league ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "league" means league in which team or player plays or has played in.
Here is an example:
Relation: league
Context: in the summer of 2013 he returned to montenegro and joined with fk rudar pljevlja , playing with the club for two seasons in the montenegrin first league .
Head Entity: fk rudar pljevlja
Tail Entity: montenegrin first league
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "league":
Relation: league  
Context: After a successful stint in the English Premier League, the player signed with FC Barcelona, competing in La Liga for the next three seasons.  
Head Entity: FC Barcelona  
Tail Entity: La Liga  

Relation: league  
Context: The young athlete made headlines when he joined the New York Yankees, showcasing his talent in Major League Baseball.  
Head Entity: New York Yankees  
Tail Entity: Major League Baseball  
#### mother ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "mother" means female parent of the subject.
Here is an example:
Relation: mother
Context: she is supposedly the third and youngest child of seti i and tuya , and the younger sister of ramesses ii and tia .
Head Entity: tia
Tail Entity: tuya
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "mother":
Relation: mother  
Context: Cleopatra was the daughter of Ptolemy XII and his wife, who was also her mother, Cleopatra VI.  
Head Entity: Cleopatra  
Tail Entity: Cleopatra VI  

Relation: mother  
Context: In Norse mythology, Loki is said to have fathered several children, including the goddess Hel, who is the daughter of the giantess Angerboda, her mother.  
Head Entity: Hel  
Tail Entity: Angerboda  
#### instrument ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "instrument" means musical instrument that a person plays or teaches or used in a music occupation.
Here is an example:
Relation: instrument
Context: peter bastian ( 25 august 1943 in copenhagen – 27 march 2017 ) was a danish musician . peter bastian bio and cv he was educated in theoretical physics and classical bassoon .
Head Entity: peter bastian
Tail Entity: bassoon
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "instrument":
Relation: instrument  
Context: johann sebastian bach (31 march 1685 – 28 july 1750) was a german composer and musician of the baroque period. he was known for his exceptional skill in playing the organ and the harpsichord.  
Head Entity: johann sebastian bach  
Tail Entity: organ  

Relation: instrument  
Context: yo-yo ma, born on october 7, 1955, is a world-renowned cellist who has performed with numerous orchestras and in various genres. he is particularly famous for his interpretations of classical cello concertos.  
Head Entity: yo-yo ma  
Tail Entity: cello  
#### country ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "country" means sovereign state of this item (not to be used for human beings).
Here is an example:
Relation: country
Context: a u.s. government - funded $ 36 million bridge over the panj river connects sher khan bandar in afghanistan with nizhniy pyanzh in tajikistan , which transport more than 150 trucks or 1,000 cars daily .
Head Entity: sher khan bandar
Tail Entity: afghanistan
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "country":
Relation: country  
Context: the ancient city of petra, located in southern jordan, is famous for its rock-cut architecture and water conduit system, attracting thousands of tourists each year.  
Head Entity: petra  
Tail Entity: jordan  

Relation: country  
Context: the great wall of china, a series of fortifications made of various materials, stretches across northern china and is a UNESCO World Heritage site.  
Head Entity: great wall of china  
Tail Entity: china  
#### characters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "characters" means characters which appear in this item (like plays, operas, operettas, books, comics, films, TV series, video games).
Here is an example:
Relation: characters
Context: hagrid suggests in " harry potter and the chamber of secrets " that " " they 're startin ' ter think the job 's jinxed .
Head Entity: harry potter and the chamber of secrets
Tail Entity: hagrid
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "characters":
Relation: characters  
Context: In the animated series "Avatar: The Last Airbender," Aang, the last Airbender, teams up with Katara and Sokka to defeat the Fire Nation.  
Head Entity: Avatar: The Last Airbender  
Tail Entity: Aang  

Relation: characters  
Context: The novel "Pride and Prejudice" features Elizabeth Bennet as she navigates societal expectations and her relationship with Mr. Darcy.  
Head Entity: Pride and Prejudice  
Tail Entity: Elizabeth Bennet  
MemoryTrain:  epoch 15, batch     0 | loss: 4.8729115MemoryTrain:  epoch 15, batch     1 | loss: 3.4091401MemoryTrain:  epoch 15, batch     2 | loss: 3.2206098MemoryTrain:  epoch 15, batch     3 | loss: 7.2762528MemoryTrain:  epoch 15, batch     4 | loss: 5.3129856MemoryTrain:  epoch 15, batch     5 | loss: 3.7902744MemoryTrain:  epoch 15, batch     6 | loss: 3.1195791MemoryTrain:  epoch 15, batch     7 | loss: 13.5524993MemoryTrain:  epoch 15, batch     8 | loss: 1.9461567MemoryTrain:  epoch 15, batch     9 | loss: 4.5804710MemoryTrain:  epoch 15, batch    10 | loss: 4.3340092MemoryTrain:  epoch  3, batch    11 | loss: 12.4832328MemoryTrain:  epoch 15, batch     0 | loss: 6.4461222MemoryTrain:  epoch 15, batch     1 | loss: 2.2005136MemoryTrain:  epoch 15, batch     2 | loss: 2.4286013MemoryTrain:  epoch 15, batch     3 | loss: 4.2604891MemoryTrain:  epoch 15, batch     4 | loss: 5.3834151MemoryTrain:  epoch 15, batch     5 | loss: 2.8274481MemoryTrain:  epoch 15, batch     6 | loss: 1.9999056MemoryTrain:  epoch 15, batch     7 | loss: 2.7358876MemoryTrain:  epoch 15, batch     8 | loss: 3.1923905MemoryTrain:  epoch 15, batch     9 | loss: 2.8846012MemoryTrain:  epoch 15, batch    10 | loss: 2.9054525MemoryTrain:  epoch  3, batch    11 | loss: 11.4661743MemoryTrain:  epoch 15, batch     0 | loss: 2.5575363MemoryTrain:  epoch 15, batch     1 | loss: 2.7065629MemoryTrain:  epoch 15, batch     2 | loss: 3.7164200MemoryTrain:  epoch 15, batch     3 | loss: 2.8854577MemoryTrain:  epoch 15, batch     4 | loss: 5.3110535MemoryTrain:  epoch 15, batch     5 | loss: 3.4725834MemoryTrain:  epoch 15, batch     6 | loss: 3.3523543MemoryTrain:  epoch 15, batch     7 | loss: 2.1956568MemoryTrain:  epoch 15, batch     8 | loss: 2.4104111MemoryTrain:  epoch 15, batch     9 | loss: 4.8465653MemoryTrain:  epoch 15, batch    10 | loss: 1.4648735MemoryTrain:  epoch  3, batch    11 | loss: 10.9993135MemoryTrain:  epoch 15, batch     0 | loss: 2.6027785MemoryTrain:  epoch 15, batch     1 | loss: 3.0914378MemoryTrain:  epoch 15, batch     2 | loss: 2.5635506MemoryTrain:  epoch 15, batch     3 | loss: 2.5216013MemoryTrain:  epoch 15, batch     4 | loss: 3.3464849MemoryTrain:  epoch 15, batch     5 | loss: 1.8482121MemoryTrain:  epoch 15, batch     6 | loss: 2.5730313MemoryTrain:  epoch 15, batch     7 | loss: 2.6907907MemoryTrain:  epoch 15, batch     8 | loss: 3.6987931MemoryTrain:  epoch 15, batch     9 | loss: 2.1742610MemoryTrain:  epoch 15, batch    10 | loss: 2.6315542MemoryTrain:  epoch  3, batch    11 | loss: 10.9485056MemoryTrain:  epoch 15, batch     0 | loss: 1.7082852MemoryTrain:  epoch 15, batch     1 | loss: 3.4597448MemoryTrain:  epoch 15, batch     2 | loss: 3.4224741MemoryTrain:  epoch 15, batch     3 | loss: 2.6337191MemoryTrain:  epoch 15, batch     4 | loss: 1.9202565MemoryTrain:  epoch 15, batch     5 | loss: 2.8349549MemoryTrain:  epoch 15, batch     6 | loss: 2.0850784MemoryTrain:  epoch 15, batch     7 | loss: 1.8604134MemoryTrain:  epoch 15, batch     8 | loss: 2.5420982MemoryTrain:  epoch 15, batch     9 | loss: 1.8970222MemoryTrain:  epoch 15, batch    10 | loss: 1.8229345MemoryTrain:  epoch  3, batch    11 | loss: 11.0507888MemoryTrain:  epoch 15, batch     0 | loss: 2.2194501MemoryTrain:  epoch 15, batch     1 | loss: 1.9834390MemoryTrain:  epoch 15, batch     2 | loss: 2.1231991MemoryTrain:  epoch 15, batch     3 | loss: 3.9548500MemoryTrain:  epoch 15, batch     4 | loss: 3.2130052MemoryTrain:  epoch 15, batch     5 | loss: 5.9451512MemoryTrain:  epoch 15, batch     6 | loss: 1.7394025MemoryTrain:  epoch 15, batch     7 | loss: 2.7601004MemoryTrain:  epoch 15, batch     8 | loss: 3.9945505MemoryTrain:  epoch 15, batch     9 | loss: 1.8146750MemoryTrain:  epoch 15, batch    10 | loss: 2.4841290MemoryTrain:  epoch  3, batch    11 | loss: 11.1731866MemoryTrain:  epoch 15, batch     0 | loss: 1.7993441MemoryTrain:  epoch 15, batch     1 | loss: 2.0368635MemoryTrain:  epoch 15, batch     2 | loss: 4.0502156MemoryTrain:  epoch 15, batch     3 | loss: 1.5279038MemoryTrain:  epoch 15, batch     4 | loss: 1.9179549MemoryTrain:  epoch 15, batch     5 | loss: 3.4802990MemoryTrain:  epoch 15, batch     6 | loss: 2.6077631MemoryTrain:  epoch 15, batch     7 | loss: 2.6922936MemoryTrain:  epoch 15, batch     8 | loss: 2.0692375MemoryTrain:  epoch 15, batch     9 | loss: 4.1691047MemoryTrain:  epoch 15, batch    10 | loss: 2.0752264MemoryTrain:  epoch  3, batch    11 | loss: 10.5151431MemoryTrain:  epoch 15, batch     0 | loss: 2.1603131MemoryTrain:  epoch 15, batch     1 | loss: 1.4553601MemoryTrain:  epoch 15, batch     2 | loss: 2.4015114MemoryTrain:  epoch 15, batch     3 | loss: 2.0048416MemoryTrain:  epoch 15, batch     4 | loss: 2.9457443MemoryTrain:  epoch 15, batch     5 | loss: 2.4447096MemoryTrain:  epoch 15, batch     6 | loss: 1.7429607MemoryTrain:  epoch 15, batch     7 | loss: 2.5344939MemoryTrain:  epoch 15, batch     8 | loss: 1.7954116MemoryTrain:  epoch 15, batch     9 | loss: 2.4902178MemoryTrain:  epoch 15, batch    10 | loss: 4.1213074MemoryTrain:  epoch  3, batch    11 | loss: 10.2998611MemoryTrain:  epoch 15, batch     0 | loss: 1.5537721MemoryTrain:  epoch 15, batch     1 | loss: 1.8405676MemoryTrain:  epoch 15, batch     2 | loss: 1.7138460MemoryTrain:  epoch 15, batch     3 | loss: 1.7041619MemoryTrain:  epoch 15, batch     4 | loss: 1.5255647MemoryTrain:  epoch 15, batch     5 | loss: 2.0386369MemoryTrain:  epoch 15, batch     6 | loss: 5.3515318MemoryTrain:  epoch 15, batch     7 | loss: 2.5301305MemoryTrain:  epoch 15, batch     8 | loss: 2.0361778MemoryTrain:  epoch 15, batch     9 | loss: 2.0255133MemoryTrain:  epoch 15, batch    10 | loss: 2.5322280MemoryTrain:  epoch  3, batch    11 | loss: 9.8703116MemoryTrain:  epoch 15, batch     0 | loss: 4.3045509MemoryTrain:  epoch 15, batch     1 | loss: 2.2719459MemoryTrain:  epoch 15, batch     2 | loss: 3.9501147MemoryTrain:  epoch 15, batch     3 | loss: 1.6948307MemoryTrain:  epoch 15, batch     4 | loss: 1.7187422MemoryTrain:  epoch 15, batch     5 | loss: 2.6219577MemoryTrain:  epoch 15, batch     6 | loss: 1.7140956MemoryTrain:  epoch 15, batch     7 | loss: 2.8866168MemoryTrain:  epoch 15, batch     8 | loss: 2.5638870MemoryTrain:  epoch 15, batch     9 | loss: 1.4083435MemoryTrain:  epoch 15, batch    10 | loss: 2.7127500MemoryTrain:  epoch  3, batch    11 | loss: 10.5759746
[EVAL] batch:    0 | acc: 18.75%,  total acc: 18.75%   [EVAL] batch:    1 | acc: 18.75%,  total acc: 18.75%   [EVAL] batch:    2 | acc: 18.75%,  total acc: 18.75%   [EVAL] batch:    3 | acc: 18.75%,  total acc: 18.75%   [EVAL] batch:    4 | acc: 18.75%,  total acc: 18.75%   [EVAL] batch:    5 | acc: 6.25%,  total acc: 16.67%   [EVAL] batch:    6 | acc: 50.00%,  total acc: 21.43%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 28.12%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 32.64%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 37.50%   [EVAL] batch:   10 | acc: 75.00%,  total acc: 40.91%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 44.27%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 46.15%   [EVAL] batch:   13 | acc: 37.50%,  total acc: 45.54%   [EVAL] batch:   14 | acc: 56.25%,  total acc: 46.25%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 46.88%   [EVAL] batch:   16 | acc: 56.25%,  total acc: 47.43%   [EVAL] batch:   17 | acc: 56.25%,  total acc: 47.92%   [EVAL] batch:   18 | acc: 56.25%,  total acc: 48.36%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 50.94%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 53.27%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 55.11%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 57.07%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 58.85%   [EVAL] batch:   24 | acc: 93.75%,  total acc: 60.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 61.78%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 63.19%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 64.51%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 65.73%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 66.88%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 67.74%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 68.75%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 69.70%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 70.40%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 71.07%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 71.70%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 72.47%   [EVAL] batch:   37 | acc: 56.25%,  total acc: 72.04%   [EVAL] batch:   38 | acc: 12.50%,  total acc: 70.51%   [EVAL] batch:   39 | acc: 43.75%,  total acc: 69.84%   [EVAL] batch:   40 | acc: 50.00%,  total acc: 69.36%   [EVAL] batch:   41 | acc: 25.00%,  total acc: 68.30%   [EVAL] batch:   42 | acc: 31.25%,  total acc: 67.44%   [EVAL] batch:   43 | acc: 62.50%,  total acc: 67.33%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 68.06%   [EVAL] batch:   45 | acc: 93.75%,  total acc: 68.61%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 69.28%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 69.79%   [EVAL] batch:   48 | acc: 87.50%,  total acc: 70.15%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 70.62%   [EVAL] batch:   50 | acc: 56.25%,  total acc: 70.34%   [EVAL] batch:   51 | acc: 62.50%,  total acc: 70.19%   [EVAL] batch:   52 | acc: 62.50%,  total acc: 70.05%   [EVAL] batch:   53 | acc: 62.50%,  total acc: 69.91%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 70.23%   [EVAL] batch:   55 | acc: 31.25%,  total acc: 69.53%   [EVAL] batch:   56 | acc: 56.25%,  total acc: 69.30%   [EVAL] batch:   57 | acc: 68.75%,  total acc: 69.29%   [EVAL] batch:   58 | acc: 62.50%,  total acc: 69.17%   [EVAL] batch:   59 | acc: 81.25%,  total acc: 69.38%   [EVAL] batch:   60 | acc: 75.00%,  total acc: 69.47%   [EVAL] batch:   61 | acc: 81.25%,  total acc: 69.66%   [EVAL] batch:   62 | acc: 31.25%,  total acc: 69.05%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 84.38%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 82.50%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    6 | acc: 43.75%,  total acc: 77.68%   [EVAL] batch:    7 | acc: 62.50%,  total acc: 75.78%   [EVAL] batch:    8 | acc: 50.00%,  total acc: 72.92%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 71.88%   [EVAL] batch:   10 | acc: 50.00%,  total acc: 69.89%   [EVAL] batch:   11 | acc: 43.75%,  total acc: 67.71%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 67.79%   [EVAL] batch:   13 | acc: 87.50%,  total acc: 69.20%   [EVAL] batch:   14 | acc: 87.50%,  total acc: 70.42%   [EVAL] batch:   15 | acc: 87.50%,  total acc: 71.48%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 73.16%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 74.31%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 75.33%   [EVAL] batch:   19 | acc: 68.75%,  total acc: 75.00%   [EVAL] batch:   20 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:   21 | acc: 68.75%,  total acc: 74.72%   [EVAL] batch:   22 | acc: 68.75%,  total acc: 74.46%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 75.52%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 76.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 76.92%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 77.78%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 78.57%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 79.09%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 79.79%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 80.44%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 81.05%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 81.63%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 82.17%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 82.50%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 82.99%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 83.45%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 83.88%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 84.29%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 84.69%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 85.06%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 85.27%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 85.47%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 85.51%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 85.56%   [EVAL] batch:   45 | acc: 68.75%,  total acc: 85.19%   [EVAL] batch:   46 | acc: 62.50%,  total acc: 84.71%   [EVAL] batch:   47 | acc: 62.50%,  total acc: 84.24%   [EVAL] batch:   48 | acc: 87.50%,  total acc: 84.31%   [EVAL] batch:   49 | acc: 68.75%,  total acc: 84.00%   [EVAL] batch:   50 | acc: 75.00%,  total acc: 83.82%   [EVAL] batch:   51 | acc: 81.25%,  total acc: 83.77%   [EVAL] batch:   52 | acc: 81.25%,  total acc: 83.73%   [EVAL] batch:   53 | acc: 62.50%,  total acc: 83.33%   [EVAL] batch:   54 | acc: 50.00%,  total acc: 82.73%   [EVAL] batch:   55 | acc: 62.50%,  total acc: 82.37%   [EVAL] batch:   56 | acc: 68.75%,  total acc: 82.13%   [EVAL] batch:   57 | acc: 56.25%,  total acc: 81.68%   [EVAL] batch:   58 | acc: 75.00%,  total acc: 81.57%   [EVAL] batch:   59 | acc: 81.25%,  total acc: 81.56%   [EVAL] batch:   60 | acc: 87.50%,  total acc: 81.66%   [EVAL] batch:   61 | acc: 75.00%,  total acc: 81.55%   [EVAL] batch:   62 | acc: 93.75%,  total acc: 81.75%   [EVAL] batch:   63 | acc: 93.75%,  total acc: 81.93%   [EVAL] batch:   64 | acc: 93.75%,  total acc: 82.12%   [EVAL] batch:   65 | acc: 100.00%,  total acc: 82.39%   [EVAL] batch:   66 | acc: 81.25%,  total acc: 82.37%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 82.63%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 82.88%   [EVAL] batch:   69 | acc: 68.75%,  total acc: 82.68%   [EVAL] batch:   70 | acc: 81.25%,  total acc: 82.66%   [EVAL] batch:   71 | acc: 75.00%,  total acc: 82.55%   [EVAL] batch:   72 | acc: 81.25%,  total acc: 82.53%   [EVAL] batch:   73 | acc: 75.00%,  total acc: 82.43%   [EVAL] batch:   74 | acc: 62.50%,  total acc: 82.17%   [EVAL] batch:   75 | acc: 68.75%,  total acc: 81.99%   [EVAL] batch:   76 | acc: 81.25%,  total acc: 81.98%   [EVAL] batch:   77 | acc: 68.75%,  total acc: 81.81%   [EVAL] batch:   78 | acc: 43.75%,  total acc: 81.33%   [EVAL] batch:   79 | acc: 68.75%,  total acc: 81.17%   [EVAL] batch:   80 | acc: 50.00%,  total acc: 80.79%   [EVAL] batch:   81 | acc: 68.75%,  total acc: 80.64%   [EVAL] batch:   82 | acc: 75.00%,  total acc: 80.57%   [EVAL] batch:   83 | acc: 81.25%,  total acc: 80.58%   [EVAL] batch:   84 | acc: 56.25%,  total acc: 80.29%   [EVAL] batch:   85 | acc: 75.00%,  total acc: 80.23%   [EVAL] batch:   86 | acc: 43.75%,  total acc: 79.81%   [EVAL] batch:   87 | acc: 87.50%,  total acc: 79.90%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 80.13%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 80.35%   [EVAL] batch:   90 | acc: 100.00%,  total acc: 80.56%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 80.77%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 80.98%   [EVAL] batch:   93 | acc: 93.75%,  total acc: 81.12%   [EVAL] batch:   94 | acc: 50.00%,  total acc: 80.79%   [EVAL] batch:   95 | acc: 81.25%,  total acc: 80.79%   [EVAL] batch:   96 | acc: 68.75%,  total acc: 80.67%   [EVAL] batch:   97 | acc: 68.75%,  total acc: 80.55%   [EVAL] batch:   98 | acc: 81.25%,  total acc: 80.56%   [EVAL] batch:   99 | acc: 43.75%,  total acc: 80.19%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 80.38%   [EVAL] batch:  101 | acc: 100.00%,  total acc: 80.58%   [EVAL] batch:  102 | acc: 100.00%,  total acc: 80.76%   [EVAL] batch:  103 | acc: 100.00%,  total acc: 80.95%   [EVAL] batch:  104 | acc: 100.00%,  total acc: 81.13%   [EVAL] batch:  105 | acc: 87.50%,  total acc: 81.19%   [EVAL] batch:  106 | acc: 100.00%,  total acc: 81.37%   [EVAL] batch:  107 | acc: 100.00%,  total acc: 81.54%   [EVAL] batch:  108 | acc: 100.00%,  total acc: 81.71%   [EVAL] batch:  109 | acc: 100.00%,  total acc: 81.88%   [EVAL] batch:  110 | acc: 100.00%,  total acc: 82.04%   [EVAL] batch:  111 | acc: 93.75%,  total acc: 82.14%   [EVAL] batch:  112 | acc: 100.00%,  total acc: 82.30%   [EVAL] batch:  113 | acc: 81.25%,  total acc: 82.29%   [EVAL] batch:  114 | acc: 93.75%,  total acc: 82.39%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 82.54%   [EVAL] batch:  116 | acc: 87.50%,  total acc: 82.59%   [EVAL] batch:  117 | acc: 93.75%,  total acc: 82.68%   [EVAL] batch:  118 | acc: 93.75%,  total acc: 82.77%   [EVAL] batch:  119 | acc: 93.75%,  total acc: 82.86%   [EVAL] batch:  120 | acc: 87.50%,  total acc: 82.90%   [EVAL] batch:  121 | acc: 87.50%,  total acc: 82.94%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 83.08%   [EVAL] batch:  123 | acc: 87.50%,  total acc: 83.11%   [EVAL] batch:  124 | acc: 93.75%,  total acc: 83.20%   [EVAL] batch:  125 | acc: 25.00%,  total acc: 82.74%   [EVAL] batch:  126 | acc: 37.50%,  total acc: 82.38%   [EVAL] batch:  127 | acc: 31.25%,  total acc: 81.98%   [EVAL] batch:  128 | acc: 37.50%,  total acc: 81.64%   [EVAL] batch:  129 | acc: 50.00%,  total acc: 81.39%   [EVAL] batch:  130 | acc: 50.00%,  total acc: 81.15%   [EVAL] batch:  131 | acc: 81.25%,  total acc: 81.16%   [EVAL] batch:  132 | acc: 87.50%,  total acc: 81.20%   [EVAL] batch:  133 | acc: 100.00%,  total acc: 81.34%   [EVAL] batch:  134 | acc: 93.75%,  total acc: 81.44%   [EVAL] batch:  135 | acc: 100.00%,  total acc: 81.57%   [EVAL] batch:  136 | acc: 93.75%,  total acc: 81.66%   [EVAL] batch:  137 | acc: 87.50%,  total acc: 81.70%   [EVAL] batch:  138 | acc: 75.00%,  total acc: 81.65%   [EVAL] batch:  139 | acc: 75.00%,  total acc: 81.61%   [EVAL] batch:  140 | acc: 87.50%,  total acc: 81.65%   [EVAL] batch:  141 | acc: 87.50%,  total acc: 81.69%   [EVAL] batch:  142 | acc: 68.75%,  total acc: 81.60%   [EVAL] batch:  143 | acc: 56.25%,  total acc: 81.42%   [EVAL] batch:  144 | acc: 43.75%,  total acc: 81.16%   [EVAL] batch:  145 | acc: 43.75%,  total acc: 80.91%   [EVAL] batch:  146 | acc: 81.25%,  total acc: 80.91%   [EVAL] batch:  147 | acc: 68.75%,  total acc: 80.83%   [EVAL] batch:  148 | acc: 43.75%,  total acc: 80.58%   [EVAL] batch:  149 | acc: 75.00%,  total acc: 80.54%   [EVAL] batch:  150 | acc: 6.25%,  total acc: 80.05%   [EVAL] batch:  151 | acc: 18.75%,  total acc: 79.65%   [EVAL] batch:  152 | acc: 6.25%,  total acc: 79.17%   [EVAL] batch:  153 | acc: 6.25%,  total acc: 78.69%   [EVAL] batch:  154 | acc: 25.00%,  total acc: 78.35%   [EVAL] batch:  155 | acc: 0.00%,  total acc: 77.84%   [EVAL] batch:  156 | acc: 68.75%,  total acc: 77.79%   [EVAL] batch:  157 | acc: 87.50%,  total acc: 77.85%   [EVAL] batch:  158 | acc: 87.50%,  total acc: 77.91%   [EVAL] batch:  159 | acc: 100.00%,  total acc: 78.05%   [EVAL] batch:  160 | acc: 93.75%,  total acc: 78.14%   [EVAL] batch:  161 | acc: 81.25%,  total acc: 78.16%   [EVAL] batch:  162 | acc: 87.50%,  total acc: 78.22%   [EVAL] batch:  163 | acc: 87.50%,  total acc: 78.28%   [EVAL] batch:  164 | acc: 93.75%,  total acc: 78.37%   [EVAL] batch:  165 | acc: 87.50%,  total acc: 78.43%   [EVAL] batch:  166 | acc: 87.50%,  total acc: 78.48%   [EVAL] batch:  167 | acc: 81.25%,  total acc: 78.50%   [EVAL] batch:  168 | acc: 87.50%,  total acc: 78.55%   [EVAL] batch:  169 | acc: 18.75%,  total acc: 78.20%   [EVAL] batch:  170 | acc: 6.25%,  total acc: 77.78%   [EVAL] batch:  171 | acc: 12.50%,  total acc: 77.40%   [EVAL] batch:  172 | acc: 25.00%,  total acc: 77.10%   [EVAL] batch:  173 | acc: 25.00%,  total acc: 76.80%   [EVAL] batch:  174 | acc: 6.25%,  total acc: 76.39%   [EVAL] batch:  175 | acc: 31.25%,  total acc: 76.14%   [EVAL] batch:  176 | acc: 50.00%,  total acc: 75.99%   [EVAL] batch:  177 | acc: 25.00%,  total acc: 75.70%   [EVAL] batch:  178 | acc: 43.75%,  total acc: 75.52%   [EVAL] batch:  179 | acc: 43.75%,  total acc: 75.35%   [EVAL] batch:  180 | acc: 50.00%,  total acc: 75.21%   [EVAL] batch:  181 | acc: 37.50%,  total acc: 75.00%   [EVAL] batch:  182 | acc: 43.75%,  total acc: 74.83%   [EVAL] batch:  183 | acc: 56.25%,  total acc: 74.73%   [EVAL] batch:  184 | acc: 75.00%,  total acc: 74.73%   [EVAL] batch:  185 | acc: 50.00%,  total acc: 74.60%   [EVAL] batch:  186 | acc: 68.75%,  total acc: 74.57%   [EVAL] batch:  187 | acc: 37.50%,  total acc: 74.37%   [EVAL] batch:  188 | acc: 56.25%,  total acc: 74.27%   [EVAL] batch:  189 | acc: 37.50%,  total acc: 74.08%   [EVAL] batch:  190 | acc: 75.00%,  total acc: 74.08%   [EVAL] batch:  191 | acc: 56.25%,  total acc: 73.99%   [EVAL] batch:  192 | acc: 56.25%,  total acc: 73.90%   [EVAL] batch:  193 | acc: 37.50%,  total acc: 73.71%   [EVAL] batch:  194 | acc: 75.00%,  total acc: 73.72%   [EVAL] batch:  195 | acc: 62.50%,  total acc: 73.66%   [EVAL] batch:  196 | acc: 75.00%,  total acc: 73.67%   [EVAL] batch:  197 | acc: 56.25%,  total acc: 73.58%   [EVAL] batch:  198 | acc: 31.25%,  total acc: 73.37%   [EVAL] batch:  199 | acc: 100.00%,  total acc: 73.50%   [EVAL] batch:  200 | acc: 37.50%,  total acc: 73.32%   [EVAL] batch:  201 | acc: 18.75%,  total acc: 73.05%   [EVAL] batch:  202 | acc: 43.75%,  total acc: 72.91%   [EVAL] batch:  203 | acc: 31.25%,  total acc: 72.70%   [EVAL] batch:  204 | acc: 31.25%,  total acc: 72.50%   [EVAL] batch:  205 | acc: 62.50%,  total acc: 72.45%   [EVAL] batch:  206 | acc: 68.75%,  total acc: 72.43%   [EVAL] batch:  207 | acc: 87.50%,  total acc: 72.51%   [EVAL] batch:  208 | acc: 100.00%,  total acc: 72.64%   [EVAL] batch:  209 | acc: 93.75%,  total acc: 72.74%   [EVAL] batch:  210 | acc: 87.50%,  total acc: 72.81%   [EVAL] batch:  211 | acc: 81.25%,  total acc: 72.85%   [EVAL] batch:  212 | acc: 93.75%,  total acc: 72.95%   [EVAL] batch:  213 | acc: 68.75%,  total acc: 72.93%   [EVAL] batch:  214 | acc: 56.25%,  total acc: 72.85%   [EVAL] batch:  215 | acc: 81.25%,  total acc: 72.89%   [EVAL] batch:  216 | acc: 56.25%,  total acc: 72.81%   [EVAL] batch:  217 | acc: 75.00%,  total acc: 72.82%   [EVAL] batch:  218 | acc: 62.50%,  total acc: 72.77%   [EVAL] batch:  219 | acc: 75.00%,  total acc: 72.78%   [EVAL] batch:  220 | acc: 31.25%,  total acc: 72.60%   [EVAL] batch:  221 | acc: 31.25%,  total acc: 72.41%   [EVAL] batch:  222 | acc: 31.25%,  total acc: 72.23%   [EVAL] batch:  223 | acc: 6.25%,  total acc: 71.93%   [EVAL] batch:  224 | acc: 37.50%,  total acc: 71.78%   [EVAL] batch:  225 | acc: 75.00%,  total acc: 71.79%   [EVAL] batch:  226 | acc: 81.25%,  total acc: 71.83%   [EVAL] batch:  227 | acc: 75.00%,  total acc: 71.85%   [EVAL] batch:  228 | acc: 100.00%,  total acc: 71.97%   [EVAL] batch:  229 | acc: 75.00%,  total acc: 71.98%   [EVAL] batch:  230 | acc: 75.00%,  total acc: 72.00%   [EVAL] batch:  231 | acc: 81.25%,  total acc: 72.04%   [EVAL] batch:  232 | acc: 37.50%,  total acc: 71.89%   [EVAL] batch:  233 | acc: 62.50%,  total acc: 71.85%   [EVAL] batch:  234 | acc: 37.50%,  total acc: 71.70%   [EVAL] batch:  235 | acc: 56.25%,  total acc: 71.64%   [EVAL] batch:  236 | acc: 62.50%,  total acc: 71.60%   [EVAL] batch:  237 | acc: 81.25%,  total acc: 71.64%   [EVAL] batch:  238 | acc: 100.00%,  total acc: 71.76%   [EVAL] batch:  239 | acc: 100.00%,  total acc: 71.88%   [EVAL] batch:  240 | acc: 100.00%,  total acc: 71.99%   [EVAL] batch:  241 | acc: 100.00%,  total acc: 72.11%   [EVAL] batch:  242 | acc: 100.00%,  total acc: 72.22%   [EVAL] batch:  243 | acc: 100.00%,  total acc: 72.34%   [EVAL] batch:  244 | acc: 93.75%,  total acc: 72.42%   [EVAL] batch:  245 | acc: 81.25%,  total acc: 72.46%   [EVAL] batch:  246 | acc: 87.50%,  total acc: 72.52%   [EVAL] batch:  247 | acc: 93.75%,  total acc: 72.61%   [EVAL] batch:  248 | acc: 100.00%,  total acc: 72.72%   [EVAL] batch:  249 | acc: 100.00%,  total acc: 72.82%   [EVAL] batch:  250 | acc: 43.75%,  total acc: 72.71%   [EVAL] batch:  251 | acc: 43.75%,  total acc: 72.59%   [EVAL] batch:  252 | acc: 43.75%,  total acc: 72.48%   [EVAL] batch:  253 | acc: 50.00%,  total acc: 72.39%   [EVAL] batch:  254 | acc: 56.25%,  total acc: 72.33%   [EVAL] batch:  255 | acc: 50.00%,  total acc: 72.24%   [EVAL] batch:  256 | acc: 62.50%,  total acc: 72.20%   [EVAL] batch:  257 | acc: 81.25%,  total acc: 72.24%   [EVAL] batch:  258 | acc: 68.75%,  total acc: 72.22%   [EVAL] batch:  259 | acc: 75.00%,  total acc: 72.24%   [EVAL] batch:  260 | acc: 81.25%,  total acc: 72.27%   [EVAL] batch:  261 | acc: 68.75%,  total acc: 72.26%   [EVAL] batch:  262 | acc: 100.00%,  total acc: 72.36%   [EVAL] batch:  263 | acc: 87.50%,  total acc: 72.42%   [EVAL] batch:  264 | acc: 81.25%,  total acc: 72.45%   [EVAL] batch:  265 | acc: 81.25%,  total acc: 72.49%   [EVAL] batch:  266 | acc: 93.75%,  total acc: 72.57%   [EVAL] batch:  267 | acc: 93.75%,  total acc: 72.64%   [EVAL] batch:  268 | acc: 81.25%,  total acc: 72.68%   [EVAL] batch:  269 | acc: 6.25%,  total acc: 72.43%   [EVAL] batch:  270 | acc: 25.00%,  total acc: 72.26%   [EVAL] batch:  271 | acc: 12.50%,  total acc: 72.04%   [EVAL] batch:  272 | acc: 43.75%,  total acc: 71.93%   [EVAL] batch:  273 | acc: 18.75%,  total acc: 71.74%   [EVAL] batch:  274 | acc: 18.75%,  total acc: 71.55%   [EVAL] batch:  275 | acc: 100.00%,  total acc: 71.65%   [EVAL] batch:  276 | acc: 93.75%,  total acc: 71.73%   [EVAL] batch:  277 | acc: 100.00%,  total acc: 71.83%   [EVAL] batch:  278 | acc: 81.25%,  total acc: 71.86%   [EVAL] batch:  279 | acc: 93.75%,  total acc: 71.94%   [EVAL] batch:  280 | acc: 81.25%,  total acc: 71.98%   [EVAL] batch:  281 | acc: 100.00%,  total acc: 72.07%   [EVAL] batch:  282 | acc: 100.00%,  total acc: 72.17%   [EVAL] batch:  283 | acc: 100.00%,  total acc: 72.27%   [EVAL] batch:  284 | acc: 100.00%,  total acc: 72.37%   [EVAL] batch:  285 | acc: 100.00%,  total acc: 72.47%   [EVAL] batch:  286 | acc: 100.00%,  total acc: 72.56%   [EVAL] batch:  287 | acc: 100.00%,  total acc: 72.66%   [EVAL] batch:  288 | acc: 93.75%,  total acc: 72.73%   [EVAL] batch:  289 | acc: 68.75%,  total acc: 72.72%   [EVAL] batch:  290 | acc: 75.00%,  total acc: 72.72%   [EVAL] batch:  291 | acc: 93.75%,  total acc: 72.80%   [EVAL] batch:  292 | acc: 93.75%,  total acc: 72.87%   [EVAL] batch:  293 | acc: 81.25%,  total acc: 72.90%   [EVAL] batch:  294 | acc: 37.50%,  total acc: 72.78%   [EVAL] batch:  295 | acc: 31.25%,  total acc: 72.64%   [EVAL] batch:  296 | acc: 37.50%,  total acc: 72.52%   [EVAL] batch:  297 | acc: 43.75%,  total acc: 72.42%   [EVAL] batch:  298 | acc: 56.25%,  total acc: 72.37%   [EVAL] batch:  299 | acc: 68.75%,  total acc: 72.35%   [EVAL] batch:  300 | acc: 93.75%,  total acc: 72.43%   [EVAL] batch:  301 | acc: 93.75%,  total acc: 72.50%   [EVAL] batch:  302 | acc: 93.75%,  total acc: 72.57%   [EVAL] batch:  303 | acc: 81.25%,  total acc: 72.59%   [EVAL] batch:  304 | acc: 93.75%,  total acc: 72.66%   [EVAL] batch:  305 | acc: 87.50%,  total acc: 72.71%   [EVAL] batch:  306 | acc: 31.25%,  total acc: 72.58%   [EVAL] batch:  307 | acc: 18.75%,  total acc: 72.40%   [EVAL] batch:  308 | acc: 18.75%,  total acc: 72.23%   [EVAL] batch:  309 | acc: 6.25%,  total acc: 72.02%   [EVAL] batch:  310 | acc: 25.00%,  total acc: 71.86%   [EVAL] batch:  311 | acc: 0.00%,  total acc: 71.63%   [EVAL] batch:  312 | acc: 12.50%,  total acc: 71.45%   [EVAL] batch:  313 | acc: 18.75%,  total acc: 71.28%   [EVAL] batch:  314 | acc: 25.00%,  total acc: 71.13%   [EVAL] batch:  315 | acc: 12.50%,  total acc: 70.95%   [EVAL] batch:  316 | acc: 25.00%,  total acc: 70.80%   [EVAL] batch:  317 | acc: 12.50%,  total acc: 70.62%   [EVAL] batch:  318 | acc: 18.75%,  total acc: 70.45%   [EVAL] batch:  319 | acc: 81.25%,  total acc: 70.49%   [EVAL] batch:  320 | acc: 56.25%,  total acc: 70.44%   [EVAL] batch:  321 | acc: 81.25%,  total acc: 70.48%   [EVAL] batch:  322 | acc: 75.00%,  total acc: 70.49%   [EVAL] batch:  323 | acc: 81.25%,  total acc: 70.52%   [EVAL] batch:  324 | acc: 81.25%,  total acc: 70.56%   [EVAL] batch:  325 | acc: 43.75%,  total acc: 70.48%   [EVAL] batch:  326 | acc: 43.75%,  total acc: 70.39%   [EVAL] batch:  327 | acc: 50.00%,  total acc: 70.33%   [EVAL] batch:  328 | acc: 68.75%,  total acc: 70.33%   [EVAL] batch:  329 | acc: 50.00%,  total acc: 70.27%   [EVAL] batch:  330 | acc: 43.75%,  total acc: 70.19%   [EVAL] batch:  331 | acc: 93.75%,  total acc: 70.26%   [EVAL] batch:  332 | acc: 100.00%,  total acc: 70.35%   [EVAL] batch:  333 | acc: 100.00%,  total acc: 70.43%   [EVAL] batch:  334 | acc: 93.75%,  total acc: 70.50%   [EVAL] batch:  335 | acc: 100.00%,  total acc: 70.59%   [EVAL] batch:  336 | acc: 100.00%,  total acc: 70.68%   [EVAL] batch:  337 | acc: 93.75%,  total acc: 70.75%   [EVAL] batch:  338 | acc: 100.00%,  total acc: 70.83%   [EVAL] batch:  339 | acc: 100.00%,  total acc: 70.92%   [EVAL] batch:  340 | acc: 100.00%,  total acc: 71.00%   [EVAL] batch:  341 | acc: 100.00%,  total acc: 71.09%   [EVAL] batch:  342 | acc: 100.00%,  total acc: 71.17%   [EVAL] batch:  343 | acc: 93.75%,  total acc: 71.24%   [EVAL] batch:  344 | acc: 100.00%,  total acc: 71.32%   [EVAL] batch:  345 | acc: 100.00%,  total acc: 71.41%   [EVAL] batch:  346 | acc: 87.50%,  total acc: 71.45%   [EVAL] batch:  347 | acc: 93.75%,  total acc: 71.52%   [EVAL] batch:  348 | acc: 100.00%,  total acc: 71.60%   [EVAL] batch:  349 | acc: 100.00%,  total acc: 71.68%   [EVAL] batch:  350 | acc: 18.75%,  total acc: 71.53%   [EVAL] batch:  351 | acc: 12.50%,  total acc: 71.36%   [EVAL] batch:  352 | acc: 56.25%,  total acc: 71.32%   [EVAL] batch:  353 | acc: 37.50%,  total acc: 71.22%   [EVAL] batch:  354 | acc: 25.00%,  total acc: 71.09%   [EVAL] batch:  355 | acc: 43.75%,  total acc: 71.01%   [EVAL] batch:  356 | acc: 87.50%,  total acc: 71.06%   [EVAL] batch:  357 | acc: 93.75%,  total acc: 71.12%   [EVAL] batch:  358 | acc: 100.00%,  total acc: 71.20%   [EVAL] batch:  359 | acc: 100.00%,  total acc: 71.28%   [EVAL] batch:  360 | acc: 87.50%,  total acc: 71.33%   [EVAL] batch:  361 | acc: 87.50%,  total acc: 71.37%   [EVAL] batch:  362 | acc: 68.75%,  total acc: 71.37%   [EVAL] batch:  363 | acc: 68.75%,  total acc: 71.36%   [EVAL] batch:  364 | acc: 68.75%,  total acc: 71.35%   [EVAL] batch:  365 | acc: 56.25%,  total acc: 71.31%   [EVAL] batch:  366 | acc: 81.25%,  total acc: 71.34%   [EVAL] batch:  367 | acc: 62.50%,  total acc: 71.31%   [EVAL] batch:  368 | acc: 31.25%,  total acc: 71.21%   [EVAL] batch:  369 | acc: 81.25%,  total acc: 71.23%   [EVAL] batch:  370 | acc: 62.50%,  total acc: 71.21%   [EVAL] batch:  371 | acc: 62.50%,  total acc: 71.19%   [EVAL] batch:  372 | acc: 81.25%,  total acc: 71.21%   [EVAL] batch:  373 | acc: 81.25%,  total acc: 71.24%   [EVAL] batch:  374 | acc: 62.50%,  total acc: 71.22%   
cur_acc:  ['0.9484', '0.8710', '0.7014', '0.7480', '0.7331', '0.6905']
his_acc:  ['0.9484', '0.9020', '0.8162', '0.7650', '0.7406', '0.7122']
CurrentTrain: epoch 15, batch     0 | loss: 21.9376542CurrentTrain: epoch 15, batch     1 | loss: 22.4446407CurrentTrain: epoch 15, batch     2 | loss: 18.3675599CurrentTrain: epoch  1, batch     3 | loss: 7.2573768CurrentTrain: epoch 15, batch     0 | loss: 12.3241490CurrentTrain: epoch 15, batch     1 | loss: 13.2914743CurrentTrain: epoch 15, batch     2 | loss: 19.6480887CurrentTrain: epoch  1, batch     3 | loss: 9.9561688CurrentTrain: epoch 15, batch     0 | loss: 17.9544854CurrentTrain: epoch 15, batch     1 | loss: 11.8002312CurrentTrain: epoch 15, batch     2 | loss: 10.9183194CurrentTrain: epoch  1, batch     3 | loss: 6.9748455CurrentTrain: epoch 15, batch     0 | loss: 21.2567559CurrentTrain: epoch 15, batch     1 | loss: 8.2586247CurrentTrain: epoch 15, batch     2 | loss: 12.1398265CurrentTrain: epoch  1, batch     3 | loss: 7.2388575CurrentTrain: epoch 15, batch     0 | loss: 18.6742390CurrentTrain: epoch 15, batch     1 | loss: 12.6912288CurrentTrain: epoch 15, batch     2 | loss: 18.4995066CurrentTrain: epoch  1, batch     3 | loss: 7.0978041CurrentTrain: epoch 15, batch     0 | loss: 8.6918649CurrentTrain: epoch 15, batch     1 | loss: 9.8462779CurrentTrain: epoch 15, batch     2 | loss: 9.6454686CurrentTrain: epoch  1, batch     3 | loss: 6.2262463CurrentTrain: epoch 15, batch     0 | loss: 10.4429772CurrentTrain: epoch 15, batch     1 | loss: 18.4389052CurrentTrain: epoch 15, batch     2 | loss: 14.0823210CurrentTrain: epoch  1, batch     3 | loss: 6.2717735CurrentTrain: epoch 15, batch     0 | loss: 13.8090011CurrentTrain: epoch 15, batch     1 | loss: 8.7070445CurrentTrain: epoch 15, batch     2 | loss: 12.3335022CurrentTrain: epoch  1, batch     3 | loss: 7.0770495CurrentTrain: epoch 15, batch     0 | loss: 17.6781058CurrentTrain: epoch 15, batch     1 | loss: 21.2982630CurrentTrain: epoch 15, batch     2 | loss: 9.9764045CurrentTrain: epoch  1, batch     3 | loss: 6.6125234CurrentTrain: epoch 15, batch     0 | loss: 9.9382428CurrentTrain: epoch 15, batch     1 | loss: 9.7508328CurrentTrain: epoch 15, batch     2 | loss: 15.6068202CurrentTrain: epoch  1, batch     3 | loss: 5.9184920
#### spouse ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "spouse" means the subject has the object as their spouse (husband, wife, partner, etc.).
Here is an example:
Relation: spouse
Context: " clinton crazies " is a term in american politics of the 1990s and later that refers to intense criticism of united states president bill clinton and his wife hillary clinton .
Head Entity: bill clinton
Tail Entity: hillary clinton
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "spouse":
Relation: spouse  
Context: "In a lavish ceremony, actor Ryan Reynolds married actress Blake Lively, solidifying their status as one of Hollywood's power couples."  
Head Entity: Ryan Reynolds  
Tail Entity: Blake Lively  

Relation: spouse  
Context: "The couple celebrated their 25th wedding anniversary, reflecting on their journey together, with John Legend expressing his love for Chrissy Teigen in a heartfelt post."  
Head Entity: John Legend  
Tail Entity: Chrissy Teigen  
#### record label ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "record label" means brand and trademark associated with the marketing of subject music recordings and music videos.
Here is an example:
Relation: record label
Context: they briefly reformed in 1992 , when sub pop re - released " primal rock therapy " on cd with five additional unreleased tracks .
Head Entity: primal rock therapy
Tail Entity: sub pop
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "record label":
Relation: record label  
Context: The album "Future Nostalgia" was released under the Warner Records label, showcasing Dua Lipa's unique style and sound.  
Head Entity: Future Nostalgia  
Tail Entity: Warner Records  

Relation: record label  
Context: After signing with Atlantic Records, the band released their highly anticipated debut album, which quickly climbed the charts.  
Head Entity: debut album  
Tail Entity: Atlantic Records  
#### distributor ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "distributor" means distributor of a creative work; distributor for a record label; news agency; film distributor.
Here is an example:
Relation: distributor
Context: " the many adventures of winnie the pooh " is a 1977 american animated buddy musical comedy film produced by walt disney productions and distributed by buena vista distribution .
Head Entity: the many adventures of winnie the pooh
Tail Entity: buena vista distribution
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "distributor":
Relation: distributor  
Context: "Inception" is a 2010 science fiction film directed by Christopher Nolan and distributed by Warner Bros. Pictures.  
Head Entity: Inception  
Tail Entity: Warner Bros. Pictures  

Relation: distributor  
Context: "The Dark Knight" is a 2008 superhero film directed by Christopher Nolan and distributed by Warner Bros. Pictures.  
Head Entity: The Dark Knight  
Tail Entity: Warner Bros. Pictures  
#### located in or next to body of water ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "located in or next to body of water" means body of water on or next to which a place is located.
Here is an example:
Relation: located in or next to body of water
Context: one account notes , alston and his son , peter also , practiced their counterfeiting operation , at stack island , in the lower mississippi river , about 170 miles upriver from natchez .
Head Entity: stack island
Tail Entity: mississippi river
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "located in or next to body of water":
Relation: located in or next to body of water  
Context: the quaint village of portsmouth is situated right by the shores of the picturesque solent, attracting tourists with its stunning views and maritime activities.  
Head Entity: portsmouth  
Tail Entity: solent  

Relation: located in or next to body of water  
Context: the historic city of venice is built on a series of islands in the venetian lagoon, making it a unique destination for travelers seeking a blend of culture and water.  
Head Entity: venice  
Tail Entity: venetian lagoon  
#### subsidiary ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "subsidiary" means subsidiary of a company or organization; generally a fully owned separate corporation.
Here is an example:
Relation: subsidiary
Context: following the sale of amstrad plc to rupert murdoch 's bskyb , viglen is now lord sugar 's sole it establishment .
Head Entity: bskyb
Tail Entity: amstrad
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "subsidiary":
Relation: subsidiary  
Context: In 2018, Facebook acquired WhatsApp, making it a subsidiary of the social media giant.  
Head Entity: Facebook  
Tail Entity: WhatsApp  

Relation: subsidiary  
Context: After the merger, the new company announced that its subsidiary, XYZ Corp, would continue to operate independently.  
Head Entity: new company  
Tail Entity: XYZ Corp  
#### country of citizenship ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "country of citizenship" means the object is a country that recognizes the subject as its citizen.
Here is an example:
Relation: country of citizenship
Context: he challenged seven other swimmers in heat four including ukraine 's rostyslav svanidze , a top 16 finalist in atlanta four years earlier .
Head Entity: rostyslav svanidze
Tail Entity: ukraine
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "country of citizenship":
Relation: country of citizenship  
Context: after years of living in the united states, the renowned artist decided to apply for dual citizenship, allowing him to maintain his ties to his homeland, italy.  
Head Entity: the renowned artist  
Tail Entity: italy  

Relation: country of citizenship  
Context: during the international conference, the delegate from canada proudly spoke about the rights and privileges of being a citizen of his country, emphasizing the importance of citizenship in shaping national identity.  
Head Entity: the delegate from canada  
Tail Entity: canada  
#### genre ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "genre" means creative work's genre or an artist's field of work.
Here is an example:
Relation: genre
Context: nina cites " duffy , muse , and other pop and alternative artists " as her major influences .
Head Entity: duffy
Tail Entity: pop
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "genre":
Relation: genre  
Context: the film "inception" is often categorized as a science fiction thriller, blending elements of action and psychological drama.  
Head Entity: inception  
Tail Entity: science fiction  

Relation: genre  
Context: the band "coldplay" is known for their alternative rock sound, which incorporates elements of pop and electronic music.  
Head Entity: coldplay  
Tail Entity: alternative rock  
#### mouth of the watercourse ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "mouth of the watercourse" means the body of water to which the watercourse drains.
Here is an example:
Relation: mouth of the watercourse
Context: san lázaro is 660 km far from asunción and is located at the exact site of the confluence of the apa river with paraguay river , bordering brazil - chaco .
Head Entity: apa river
Tail Entity: paraguay river
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "mouth of the watercourse":
Relation: mouth of the watercourse  
Context: the mississippi river flows into the gulf of mexico, creating a rich delta ecosystem that supports diverse wildlife and plant species.  
Head Entity: mississippi river  
Tail Entity: gulf of mexico  

Relation: mouth of the watercourse  
Context: the rhine river, known for its scenic beauty, empties into the north sea, providing a vital shipping route for trade in europe.  
Head Entity: rhine river  
Tail Entity: north sea  
#### position played on team / speciality ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "position played on team / speciality" means position or specialism of a player on a team.
Here is an example:
Relation: position played on team / speciality
Context: bradley halliday ( born 10 july 1995 ) is an english professional footballer who plays as a defender or a midfielder for league two club cambridge united .
Head Entity: bradley halliday
Tail Entity: defender
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "position played on team / speciality":
Relation: position played on team / speciality  
Context: alex morgan ( born 2 july 1989 ) is an american soccer player who plays as a forward for the national team and the club san diego wave fc in the nwsl.  
Head Entity: alex morgan  
Tail Entity: forward  

Relation: position played on team / speciality  
Context: lebron james ( born 30 december 1984 ) is an american professional basketball player who plays as a small forward for the los angeles lakers in the nba.  
Head Entity: lebron james  
Tail Entity: small forward  
#### sports season of league or competition ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "sports season of league or competition" means property that shows the competition of which the item is a season.
Here is an example:
Relation: sports season of league or competition
Context: hammami made his international debut for tunisia in 2007 , and represented them at the africa cup of nations in 2010 and 2013 .
Head Entity: 2010
Tail Entity: africa cup of nations
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "sports season of league or competition":
Relation: sports season of league or competition  
Context: The 2021 UEFA Champions League was won by Chelsea, who defeated Manchester City in the final held in Porto.  
Head Entity: 2021  
Tail Entity: UEFA Champions League  

Relation: sports season of league or competition  
Context: The 2019 Cricket World Cup took place in England and Wales, featuring ten teams competing for the title.  
Head Entity: 2019  
Tail Entity: Cricket World Cup  
MemoryTrain:  epoch 15, batch     0 | loss: 3.6793443MemoryTrain:  epoch 15, batch     1 | loss: 3.8181973MemoryTrain:  epoch 15, batch     2 | loss: 6.6421298MemoryTrain:  epoch 15, batch     3 | loss: 2.1658574MemoryTrain:  epoch 15, batch     4 | loss: 3.0301092MemoryTrain:  epoch 15, batch     5 | loss: 3.0115001MemoryTrain:  epoch 15, batch     6 | loss: 3.0077143MemoryTrain:  epoch 15, batch     7 | loss: 3.0821481MemoryTrain:  epoch 15, batch     8 | loss: 2.5659703MemoryTrain:  epoch 15, batch     9 | loss: 4.6018920MemoryTrain:  epoch 15, batch    10 | loss: 3.6147302MemoryTrain:  epoch 15, batch    11 | loss: 2.7614495MemoryTrain:  epoch 15, batch    12 | loss: 3.2697856MemoryTrain:  epoch  1, batch    13 | loss: 4.9413058MemoryTrain:  epoch 15, batch     0 | loss: 1.9692088MemoryTrain:  epoch 15, batch     1 | loss: 2.5190433MemoryTrain:  epoch 15, batch     2 | loss: 3.4282077MemoryTrain:  epoch 15, batch     3 | loss: 3.9424151MemoryTrain:  epoch 15, batch     4 | loss: 5.6574740MemoryTrain:  epoch 15, batch     5 | loss: 2.6024125MemoryTrain:  epoch 15, batch     6 | loss: 2.3713167MemoryTrain:  epoch 15, batch     7 | loss: 2.7475263MemoryTrain:  epoch 15, batch     8 | loss: 3.5330916MemoryTrain:  epoch 15, batch     9 | loss: 2.4346726MemoryTrain:  epoch 15, batch    10 | loss: 1.7554949MemoryTrain:  epoch 15, batch    11 | loss: 2.2686285MemoryTrain:  epoch 15, batch    12 | loss: 1.9168738MemoryTrain:  epoch  1, batch    13 | loss: 5.5806395MemoryTrain:  epoch 15, batch     0 | loss: 2.3282511MemoryTrain:  epoch 15, batch     1 | loss: 2.0469566MemoryTrain:  epoch 15, batch     2 | loss: 3.2139547MemoryTrain:  epoch 15, batch     3 | loss: 2.0525507MemoryTrain:  epoch 15, batch     4 | loss: 2.2277016MemoryTrain:  epoch 15, batch     5 | loss: 2.2551580MemoryTrain:  epoch 15, batch     6 | loss: 5.4829037MemoryTrain:  epoch 15, batch     7 | loss: 3.8885860MemoryTrain:  epoch 15, batch     8 | loss: 2.0761580MemoryTrain:  epoch 15, batch     9 | loss: 2.2025315MemoryTrain:  epoch 15, batch    10 | loss: 1.8438824MemoryTrain:  epoch 15, batch    11 | loss: 1.7692363MemoryTrain:  epoch 15, batch    12 | loss: 2.1803006MemoryTrain:  epoch  1, batch    13 | loss: 4.8173135MemoryTrain:  epoch 15, batch     0 | loss: 2.2134229MemoryTrain:  epoch 15, batch     1 | loss: 2.0367279MemoryTrain:  epoch 15, batch     2 | loss: 2.5042086MemoryTrain:  epoch 15, batch     3 | loss: 4.2038932MemoryTrain:  epoch 15, batch     4 | loss: 1.8640013MemoryTrain:  epoch 15, batch     5 | loss: 2.7949039MemoryTrain:  epoch 15, batch     6 | loss: 2.6729849MemoryTrain:  epoch 15, batch     7 | loss: 4.5059728MemoryTrain:  epoch 15, batch     8 | loss: 1.8015403MemoryTrain:  epoch 15, batch     9 | loss: 3.6319766MemoryTrain:  epoch 15, batch    10 | loss: 1.5082977MemoryTrain:  epoch 15, batch    11 | loss: 1.9389996MemoryTrain:  epoch 15, batch    12 | loss: 1.7630020MemoryTrain:  epoch  1, batch    13 | loss: 9.2558619MemoryTrain:  epoch 15, batch     0 | loss: 1.8992432MemoryTrain:  epoch 15, batch     1 | loss: 2.1727576MemoryTrain:  epoch 15, batch     2 | loss: 1.6270300MemoryTrain:  epoch 15, batch     3 | loss: 2.1458629MemoryTrain:  epoch 15, batch     4 | loss: 1.9372426MemoryTrain:  epoch 15, batch     5 | loss: 1.5546806MemoryTrain:  epoch 15, batch     6 | loss: 1.7930674MemoryTrain:  epoch 15, batch     7 | loss: 2.1622385MemoryTrain:  epoch 15, batch     8 | loss: 4.2558674MemoryTrain:  epoch 15, batch     9 | loss: 2.3374659MemoryTrain:  epoch 15, batch    10 | loss: 1.8304336MemoryTrain:  epoch 15, batch    11 | loss: 1.7243005MemoryTrain:  epoch 15, batch    12 | loss: 2.9942003MemoryTrain:  epoch  1, batch    13 | loss: 7.0138368MemoryTrain:  epoch 15, batch     0 | loss: 1.8912937MemoryTrain:  epoch 15, batch     1 | loss: 1.8380605MemoryTrain:  epoch 15, batch     2 | loss: 2.2436704MemoryTrain:  epoch 15, batch     3 | loss: 2.3988740MemoryTrain:  epoch 15, batch     4 | loss: 2.1383455MemoryTrain:  epoch 15, batch     5 | loss: 1.9450466MemoryTrain:  epoch 15, batch     6 | loss: 2.7403629MemoryTrain:  epoch 15, batch     7 | loss: 1.9861582MemoryTrain:  epoch 15, batch     8 | loss: 4.1101184MemoryTrain:  epoch 15, batch     9 | loss: 1.5301590MemoryTrain:  epoch 15, batch    10 | loss: 1.6818148MemoryTrain:  epoch 15, batch    11 | loss: 2.5587217MemoryTrain:  epoch 15, batch    12 | loss: 3.4489146MemoryTrain:  epoch  1, batch    13 | loss: 6.3884194MemoryTrain:  epoch 15, batch     0 | loss: 6.2178963MemoryTrain:  epoch 15, batch     1 | loss: 2.4679992MemoryTrain:  epoch 15, batch     2 | loss: 1.7095636MemoryTrain:  epoch 15, batch     3 | loss: 2.2572219MemoryTrain:  epoch 15, batch     4 | loss: 2.4633289MemoryTrain:  epoch 15, batch     5 | loss: 1.5789532MemoryTrain:  epoch 15, batch     6 | loss: 1.5695897MemoryTrain:  epoch 15, batch     7 | loss: 1.3297229MemoryTrain:  epoch 15, batch     8 | loss: 2.0794198MemoryTrain:  epoch 15, batch     9 | loss: 2.7779183MemoryTrain:  epoch 15, batch    10 | loss: 2.1989671MemoryTrain:  epoch 15, batch    11 | loss: 1.7426365MemoryTrain:  epoch 15, batch    12 | loss: 1.9862687MemoryTrain:  epoch  1, batch    13 | loss: 5.6627074MemoryTrain:  epoch 15, batch     0 | loss: 1.4587230MemoryTrain:  epoch 15, batch     1 | loss: 2.5681794MemoryTrain:  epoch 15, batch     2 | loss: 2.2065500MemoryTrain:  epoch 15, batch     3 | loss: 1.4192245MemoryTrain:  epoch 15, batch     4 | loss: 1.4059583MemoryTrain:  epoch 15, batch     5 | loss: 4.2349074MemoryTrain:  epoch 15, batch     6 | loss: 1.3894470MemoryTrain:  epoch 15, batch     7 | loss: 2.1500676MemoryTrain:  epoch 15, batch     8 | loss: 2.7852075MemoryTrain:  epoch 15, batch     9 | loss: 1.6937680MemoryTrain:  epoch 15, batch    10 | loss: 1.8561617MemoryTrain:  epoch 15, batch    11 | loss: 2.4115873MemoryTrain:  epoch 15, batch    12 | loss: 1.7043252MemoryTrain:  epoch  1, batch    13 | loss: 5.5875945MemoryTrain:  epoch 15, batch     0 | loss: 1.6673062MemoryTrain:  epoch 15, batch     1 | loss: 1.3330895MemoryTrain:  epoch 15, batch     2 | loss: 4.5071323MemoryTrain:  epoch 15, batch     3 | loss: 4.4159922MemoryTrain:  epoch 15, batch     4 | loss: 1.8227836MemoryTrain:  epoch 15, batch     5 | loss: 2.0203075MemoryTrain:  epoch 15, batch     6 | loss: 1.5369341MemoryTrain:  epoch 15, batch     7 | loss: 1.3913132MemoryTrain:  epoch 15, batch     8 | loss: 3.1811970MemoryTrain:  epoch 15, batch     9 | loss: 1.8152280MemoryTrain:  epoch 15, batch    10 | loss: 1.3426846MemoryTrain:  epoch 15, batch    11 | loss: 1.4262176MemoryTrain:  epoch 15, batch    12 | loss: 1.3436833MemoryTrain:  epoch  1, batch    13 | loss: 6.1537721MemoryTrain:  epoch 15, batch     0 | loss: 4.3987539MemoryTrain:  epoch 15, batch     1 | loss: 4.4365922MemoryTrain:  epoch 15, batch     2 | loss: 1.3790105MemoryTrain:  epoch 15, batch     3 | loss: 4.8534479MemoryTrain:  epoch 15, batch     4 | loss: 1.7818126MemoryTrain:  epoch 15, batch     5 | loss: 1.3699356MemoryTrain:  epoch 15, batch     6 | loss: 4.3767341MemoryTrain:  epoch 15, batch     7 | loss: 3.3093361MemoryTrain:  epoch 15, batch     8 | loss: 1.6459268MemoryTrain:  epoch 15, batch     9 | loss: 4.0996118MemoryTrain:  epoch 15, batch    10 | loss: 2.0907827MemoryTrain:  epoch 15, batch    11 | loss: 1.7501884MemoryTrain:  epoch 15, batch    12 | loss: 3.7613478MemoryTrain:  epoch  1, batch    13 | loss: 4.8093538
[EVAL] batch:    0 | acc: 50.00%,  total acc: 50.00%   [EVAL] batch:    1 | acc: 50.00%,  total acc: 50.00%   [EVAL] batch:    2 | acc: 50.00%,  total acc: 50.00%   [EVAL] batch:    3 | acc: 43.75%,  total acc: 48.44%   [EVAL] batch:    4 | acc: 56.25%,  total acc: 50.00%   [EVAL] batch:    5 | acc: 50.00%,  total acc: 50.00%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 56.25%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 61.72%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 63.89%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 64.38%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 65.91%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 68.75%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 69.23%   [EVAL] batch:   13 | acc: 50.00%,  total acc: 67.86%   [EVAL] batch:   14 | acc: 56.25%,  total acc: 67.08%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 66.41%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 66.91%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 66.67%   [EVAL] batch:   18 | acc: 81.25%,  total acc: 67.43%   [EVAL] batch:   19 | acc: 43.75%,  total acc: 66.25%   [EVAL] batch:   20 | acc: 62.50%,  total acc: 66.07%   [EVAL] batch:   21 | acc: 37.50%,  total acc: 64.77%   [EVAL] batch:   22 | acc: 56.25%,  total acc: 64.40%   [EVAL] batch:   23 | acc: 62.50%,  total acc: 64.32%   [EVAL] batch:   24 | acc: 56.25%,  total acc: 64.00%   [EVAL] batch:   25 | acc: 62.50%,  total acc: 63.94%   [EVAL] batch:   26 | acc: 62.50%,  total acc: 63.89%   [EVAL] batch:   27 | acc: 31.25%,  total acc: 62.72%   [EVAL] batch:   28 | acc: 62.50%,  total acc: 62.72%   [EVAL] batch:   29 | acc: 43.75%,  total acc: 62.08%   [EVAL] batch:   30 | acc: 25.00%,  total acc: 60.89%   [EVAL] batch:   31 | acc: 62.50%,  total acc: 60.94%   [EVAL] batch:   32 | acc: 56.25%,  total acc: 60.80%   [EVAL] batch:   33 | acc: 62.50%,  total acc: 60.85%   [EVAL] batch:   34 | acc: 68.75%,  total acc: 61.07%   [EVAL] batch:   35 | acc: 62.50%,  total acc: 61.11%   [EVAL] batch:   36 | acc: 68.75%,  total acc: 61.32%   [EVAL] batch:   37 | acc: 68.75%,  total acc: 61.51%   [EVAL] batch:   38 | acc: 62.50%,  total acc: 61.54%   [EVAL] batch:   39 | acc: 75.00%,  total acc: 61.88%   [EVAL] batch:   40 | acc: 75.00%,  total acc: 62.20%   [EVAL] batch:   41 | acc: 81.25%,  total acc: 62.65%   [EVAL] batch:   42 | acc: 81.25%,  total acc: 63.08%   [EVAL] batch:   43 | acc: 68.75%,  total acc: 63.21%   [EVAL] batch:   44 | acc: 37.50%,  total acc: 62.64%   [EVAL] batch:   45 | acc: 31.25%,  total acc: 61.96%   [EVAL] batch:   46 | acc: 31.25%,  total acc: 61.30%   [EVAL] batch:   47 | acc: 37.50%,  total acc: 60.81%   [EVAL] batch:   48 | acc: 25.00%,  total acc: 60.08%   [EVAL] batch:   49 | acc: 37.50%,  total acc: 59.62%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 60.42%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 61.18%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 61.91%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 62.62%   [EVAL] batch:   54 | acc: 100.00%,  total acc: 63.30%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 63.95%   [EVAL] batch:   56 | acc: 81.25%,  total acc: 64.25%   [EVAL] batch:   57 | acc: 75.00%,  total acc: 64.44%   [EVAL] batch:   58 | acc: 75.00%,  total acc: 64.62%   [EVAL] batch:   59 | acc: 87.50%,  total acc: 65.00%   [EVAL] batch:   60 | acc: 87.50%,  total acc: 65.37%   [EVAL] batch:   61 | acc: 81.25%,  total acc: 65.62%   [EVAL] batch:   62 | acc: 37.50%,  total acc: 65.18%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 79.17%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 78.12%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 76.25%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 78.12%   [EVAL] batch:    6 | acc: 43.75%,  total acc: 73.21%   [EVAL] batch:    7 | acc: 43.75%,  total acc: 69.53%   [EVAL] batch:    8 | acc: 43.75%,  total acc: 66.67%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 66.25%   [EVAL] batch:   10 | acc: 37.50%,  total acc: 63.64%   [EVAL] batch:   11 | acc: 43.75%,  total acc: 61.98%   [EVAL] batch:   12 | acc: 62.50%,  total acc: 62.02%   [EVAL] batch:   13 | acc: 87.50%,  total acc: 63.84%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 65.83%   [EVAL] batch:   15 | acc: 93.75%,  total acc: 67.58%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 69.49%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 70.83%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 72.37%   [EVAL] batch:   19 | acc: 68.75%,  total acc: 72.19%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 72.92%   [EVAL] batch:   21 | acc: 75.00%,  total acc: 73.01%   [EVAL] batch:   22 | acc: 75.00%,  total acc: 73.10%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 74.22%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 74.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 75.72%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 76.62%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 77.46%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 78.02%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 78.75%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 79.44%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 80.08%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 80.68%   [EVAL] batch:   33 | acc: 81.25%,  total acc: 80.70%   [EVAL] batch:   34 | acc: 81.25%,  total acc: 80.71%   [EVAL] batch:   35 | acc: 81.25%,  total acc: 80.73%   [EVAL] batch:   36 | acc: 93.75%,  total acc: 81.08%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 81.73%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 82.19%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 82.62%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 82.89%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 83.28%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 83.24%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:   45 | acc: 75.00%,  total acc: 83.15%   [EVAL] batch:   46 | acc: 75.00%,  total acc: 82.98%   [EVAL] batch:   47 | acc: 75.00%,  total acc: 82.81%   [EVAL] batch:   48 | acc: 93.75%,  total acc: 83.04%   [EVAL] batch:   49 | acc: 68.75%,  total acc: 82.75%   [EVAL] batch:   50 | acc: 75.00%,  total acc: 82.60%   [EVAL] batch:   51 | acc: 81.25%,  total acc: 82.57%   [EVAL] batch:   52 | acc: 87.50%,  total acc: 82.67%   [EVAL] batch:   53 | acc: 62.50%,  total acc: 82.29%   [EVAL] batch:   54 | acc: 50.00%,  total acc: 81.70%   [EVAL] batch:   55 | acc: 62.50%,  total acc: 81.36%   [EVAL] batch:   56 | acc: 68.75%,  total acc: 81.14%   [EVAL] batch:   57 | acc: 50.00%,  total acc: 80.60%   [EVAL] batch:   58 | acc: 62.50%,  total acc: 80.30%   [EVAL] batch:   59 | acc: 81.25%,  total acc: 80.31%   [EVAL] batch:   60 | acc: 87.50%,  total acc: 80.43%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 80.24%   [EVAL] batch:   62 | acc: 87.50%,  total acc: 80.36%   [EVAL] batch:   63 | acc: 93.75%,  total acc: 80.57%   [EVAL] batch:   64 | acc: 93.75%,  total acc: 80.77%   [EVAL] batch:   65 | acc: 100.00%,  total acc: 81.06%   [EVAL] batch:   66 | acc: 81.25%,  total acc: 81.06%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 81.34%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 81.61%   [EVAL] batch:   69 | acc: 68.75%,  total acc: 81.43%   [EVAL] batch:   70 | acc: 87.50%,  total acc: 81.51%   [EVAL] batch:   71 | acc: 81.25%,  total acc: 81.51%   [EVAL] batch:   72 | acc: 87.50%,  total acc: 81.59%   [EVAL] batch:   73 | acc: 68.75%,  total acc: 81.42%   [EVAL] batch:   74 | acc: 68.75%,  total acc: 81.25%   [EVAL] batch:   75 | acc: 68.75%,  total acc: 81.09%   [EVAL] batch:   76 | acc: 81.25%,  total acc: 81.09%   [EVAL] batch:   77 | acc: 81.25%,  total acc: 81.09%   [EVAL] batch:   78 | acc: 43.75%,  total acc: 80.62%   [EVAL] batch:   79 | acc: 68.75%,  total acc: 80.47%   [EVAL] batch:   80 | acc: 68.75%,  total acc: 80.32%   [EVAL] batch:   81 | acc: 75.00%,  total acc: 80.26%   [EVAL] batch:   82 | acc: 75.00%,  total acc: 80.20%   [EVAL] batch:   83 | acc: 81.25%,  total acc: 80.21%   [EVAL] batch:   84 | acc: 56.25%,  total acc: 79.93%   [EVAL] batch:   85 | acc: 81.25%,  total acc: 79.94%   [EVAL] batch:   86 | acc: 50.00%,  total acc: 79.60%   [EVAL] batch:   87 | acc: 81.25%,  total acc: 79.62%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 79.85%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 80.07%   [EVAL] batch:   90 | acc: 100.00%,  total acc: 80.29%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 80.50%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 80.71%   [EVAL] batch:   93 | acc: 93.75%,  total acc: 80.85%   [EVAL] batch:   94 | acc: 68.75%,  total acc: 80.72%   [EVAL] batch:   95 | acc: 81.25%,  total acc: 80.73%   [EVAL] batch:   96 | acc: 75.00%,  total acc: 80.67%   [EVAL] batch:   97 | acc: 68.75%,  total acc: 80.55%   [EVAL] batch:   98 | acc: 81.25%,  total acc: 80.56%   [EVAL] batch:   99 | acc: 43.75%,  total acc: 80.19%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 80.38%   [EVAL] batch:  101 | acc: 100.00%,  total acc: 80.58%   [EVAL] batch:  102 | acc: 100.00%,  total acc: 80.76%   [EVAL] batch:  103 | acc: 100.00%,  total acc: 80.95%   [EVAL] batch:  104 | acc: 100.00%,  total acc: 81.13%   [EVAL] batch:  105 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:  106 | acc: 100.00%,  total acc: 81.43%   [EVAL] batch:  107 | acc: 100.00%,  total acc: 81.60%   [EVAL] batch:  108 | acc: 100.00%,  total acc: 81.77%   [EVAL] batch:  109 | acc: 100.00%,  total acc: 81.93%   [EVAL] batch:  110 | acc: 100.00%,  total acc: 82.09%   [EVAL] batch:  111 | acc: 93.75%,  total acc: 82.20%   [EVAL] batch:  112 | acc: 100.00%,  total acc: 82.36%   [EVAL] batch:  113 | acc: 87.50%,  total acc: 82.40%   [EVAL] batch:  114 | acc: 93.75%,  total acc: 82.50%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 82.65%   [EVAL] batch:  116 | acc: 87.50%,  total acc: 82.69%   [EVAL] batch:  117 | acc: 93.75%,  total acc: 82.79%   [EVAL] batch:  118 | acc: 87.50%,  total acc: 82.83%   [EVAL] batch:  119 | acc: 93.75%,  total acc: 82.92%   [EVAL] batch:  120 | acc: 87.50%,  total acc: 82.95%   [EVAL] batch:  121 | acc: 93.75%,  total acc: 83.04%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 83.18%   [EVAL] batch:  123 | acc: 87.50%,  total acc: 83.22%   [EVAL] batch:  124 | acc: 93.75%,  total acc: 83.30%   [EVAL] batch:  125 | acc: 0.00%,  total acc: 82.64%   [EVAL] batch:  126 | acc: 12.50%,  total acc: 82.09%   [EVAL] batch:  127 | acc: 12.50%,  total acc: 81.54%   [EVAL] batch:  128 | acc: 0.00%,  total acc: 80.91%   [EVAL] batch:  129 | acc: 25.00%,  total acc: 80.48%   [EVAL] batch:  130 | acc: 25.00%,  total acc: 80.06%   [EVAL] batch:  131 | acc: 68.75%,  total acc: 79.97%   [EVAL] batch:  132 | acc: 81.25%,  total acc: 79.98%   [EVAL] batch:  133 | acc: 100.00%,  total acc: 80.13%   [EVAL] batch:  134 | acc: 93.75%,  total acc: 80.23%   [EVAL] batch:  135 | acc: 93.75%,  total acc: 80.33%   [EVAL] batch:  136 | acc: 93.75%,  total acc: 80.43%   [EVAL] batch:  137 | acc: 81.25%,  total acc: 80.43%   [EVAL] batch:  138 | acc: 75.00%,  total acc: 80.40%   [EVAL] batch:  139 | acc: 75.00%,  total acc: 80.36%   [EVAL] batch:  140 | acc: 93.75%,  total acc: 80.45%   [EVAL] batch:  141 | acc: 87.50%,  total acc: 80.50%   [EVAL] batch:  142 | acc: 68.75%,  total acc: 80.42%   [EVAL] batch:  143 | acc: 43.75%,  total acc: 80.16%   [EVAL] batch:  144 | acc: 37.50%,  total acc: 79.87%   [EVAL] batch:  145 | acc: 6.25%,  total acc: 79.37%   [EVAL] batch:  146 | acc: 62.50%,  total acc: 79.25%   [EVAL] batch:  147 | acc: 50.00%,  total acc: 79.05%   [EVAL] batch:  148 | acc: 43.75%,  total acc: 78.82%   [EVAL] batch:  149 | acc: 56.25%,  total acc: 78.67%   [EVAL] batch:  150 | acc: 6.25%,  total acc: 78.19%   [EVAL] batch:  151 | acc: 18.75%,  total acc: 77.80%   [EVAL] batch:  152 | acc: 12.50%,  total acc: 77.37%   [EVAL] batch:  153 | acc: 6.25%,  total acc: 76.91%   [EVAL] batch:  154 | acc: 25.00%,  total acc: 76.57%   [EVAL] batch:  155 | acc: 0.00%,  total acc: 76.08%   [EVAL] batch:  156 | acc: 68.75%,  total acc: 76.04%   [EVAL] batch:  157 | acc: 87.50%,  total acc: 76.11%   [EVAL] batch:  158 | acc: 87.50%,  total acc: 76.18%   [EVAL] batch:  159 | acc: 100.00%,  total acc: 76.33%   [EVAL] batch:  160 | acc: 87.50%,  total acc: 76.40%   [EVAL] batch:  161 | acc: 81.25%,  total acc: 76.43%   [EVAL] batch:  162 | acc: 87.50%,  total acc: 76.50%   [EVAL] batch:  163 | acc: 87.50%,  total acc: 76.56%   [EVAL] batch:  164 | acc: 87.50%,  total acc: 76.63%   [EVAL] batch:  165 | acc: 87.50%,  total acc: 76.69%   [EVAL] batch:  166 | acc: 93.75%,  total acc: 76.80%   [EVAL] batch:  167 | acc: 87.50%,  total acc: 76.86%   [EVAL] batch:  168 | acc: 87.50%,  total acc: 76.92%   [EVAL] batch:  169 | acc: 25.00%,  total acc: 76.62%   [EVAL] batch:  170 | acc: 25.00%,  total acc: 76.32%   [EVAL] batch:  171 | acc: 12.50%,  total acc: 75.94%   [EVAL] batch:  172 | acc: 18.75%,  total acc: 75.61%   [EVAL] batch:  173 | acc: 31.25%,  total acc: 75.36%   [EVAL] batch:  174 | acc: 12.50%,  total acc: 75.00%   [EVAL] batch:  175 | acc: 31.25%,  total acc: 74.75%   [EVAL] batch:  176 | acc: 62.50%,  total acc: 74.68%   [EVAL] batch:  177 | acc: 37.50%,  total acc: 74.47%   [EVAL] batch:  178 | acc: 50.00%,  total acc: 74.34%   [EVAL] batch:  179 | acc: 43.75%,  total acc: 74.17%   [EVAL] batch:  180 | acc: 43.75%,  total acc: 74.00%   [EVAL] batch:  181 | acc: 37.50%,  total acc: 73.80%   [EVAL] batch:  182 | acc: 43.75%,  total acc: 73.63%   [EVAL] batch:  183 | acc: 43.75%,  total acc: 73.47%   [EVAL] batch:  184 | acc: 75.00%,  total acc: 73.48%   [EVAL] batch:  185 | acc: 50.00%,  total acc: 73.35%   [EVAL] batch:  186 | acc: 75.00%,  total acc: 73.36%   [EVAL] batch:  187 | acc: 37.50%,  total acc: 73.17%   [EVAL] batch:  188 | acc: 43.75%,  total acc: 73.02%   [EVAL] batch:  189 | acc: 37.50%,  total acc: 72.83%   [EVAL] batch:  190 | acc: 62.50%,  total acc: 72.77%   [EVAL] batch:  191 | acc: 37.50%,  total acc: 72.59%   [EVAL] batch:  192 | acc: 43.75%,  total acc: 72.44%   [EVAL] batch:  193 | acc: 37.50%,  total acc: 72.26%   [EVAL] batch:  194 | acc: 81.25%,  total acc: 72.31%   [EVAL] batch:  195 | acc: 62.50%,  total acc: 72.26%   [EVAL] batch:  196 | acc: 75.00%,  total acc: 72.27%   [EVAL] batch:  197 | acc: 62.50%,  total acc: 72.22%   [EVAL] batch:  198 | acc: 31.25%,  total acc: 72.02%   [EVAL] batch:  199 | acc: 100.00%,  total acc: 72.16%   [EVAL] batch:  200 | acc: 31.25%,  total acc: 71.95%   [EVAL] batch:  201 | acc: 12.50%,  total acc: 71.66%   [EVAL] batch:  202 | acc: 37.50%,  total acc: 71.49%   [EVAL] batch:  203 | acc: 25.00%,  total acc: 71.26%   [EVAL] batch:  204 | acc: 18.75%,  total acc: 71.01%   [EVAL] batch:  205 | acc: 50.00%,  total acc: 70.90%   [EVAL] batch:  206 | acc: 68.75%,  total acc: 70.89%   [EVAL] batch:  207 | acc: 93.75%,  total acc: 71.00%   [EVAL] batch:  208 | acc: 100.00%,  total acc: 71.14%   [EVAL] batch:  209 | acc: 93.75%,  total acc: 71.25%   [EVAL] batch:  210 | acc: 93.75%,  total acc: 71.36%   [EVAL] batch:  211 | acc: 81.25%,  total acc: 71.40%   [EVAL] batch:  212 | acc: 81.25%,  total acc: 71.45%   [EVAL] batch:  213 | acc: 62.50%,  total acc: 71.41%   [EVAL] batch:  214 | acc: 43.75%,  total acc: 71.28%   [EVAL] batch:  215 | acc: 62.50%,  total acc: 71.24%   [EVAL] batch:  216 | acc: 50.00%,  total acc: 71.14%   [EVAL] batch:  217 | acc: 68.75%,  total acc: 71.13%   [EVAL] batch:  218 | acc: 56.25%,  total acc: 71.06%   [EVAL] batch:  219 | acc: 68.75%,  total acc: 71.05%   [EVAL] batch:  220 | acc: 25.00%,  total acc: 70.84%   [EVAL] batch:  221 | acc: 31.25%,  total acc: 70.66%   [EVAL] batch:  222 | acc: 18.75%,  total acc: 70.43%   [EVAL] batch:  223 | acc: 12.50%,  total acc: 70.17%   [EVAL] batch:  224 | acc: 43.75%,  total acc: 70.06%   [EVAL] batch:  225 | acc: 68.75%,  total acc: 70.05%   [EVAL] batch:  226 | acc: 87.50%,  total acc: 70.13%   [EVAL] batch:  227 | acc: 68.75%,  total acc: 70.12%   [EVAL] batch:  228 | acc: 100.00%,  total acc: 70.25%   [EVAL] batch:  229 | acc: 68.75%,  total acc: 70.24%   [EVAL] batch:  230 | acc: 75.00%,  total acc: 70.27%   [EVAL] batch:  231 | acc: 93.75%,  total acc: 70.37%   [EVAL] batch:  232 | acc: 68.75%,  total acc: 70.36%   [EVAL] batch:  233 | acc: 68.75%,  total acc: 70.35%   [EVAL] batch:  234 | acc: 50.00%,  total acc: 70.27%   [EVAL] batch:  235 | acc: 56.25%,  total acc: 70.21%   [EVAL] batch:  236 | acc: 62.50%,  total acc: 70.17%   [EVAL] batch:  237 | acc: 93.75%,  total acc: 70.27%   [EVAL] batch:  238 | acc: 100.00%,  total acc: 70.40%   [EVAL] batch:  239 | acc: 100.00%,  total acc: 70.52%   [EVAL] batch:  240 | acc: 100.00%,  total acc: 70.64%   [EVAL] batch:  241 | acc: 100.00%,  total acc: 70.76%   [EVAL] batch:  242 | acc: 100.00%,  total acc: 70.88%   [EVAL] batch:  243 | acc: 100.00%,  total acc: 71.00%   [EVAL] batch:  244 | acc: 93.75%,  total acc: 71.10%   [EVAL] batch:  245 | acc: 81.25%,  total acc: 71.14%   [EVAL] batch:  246 | acc: 87.50%,  total acc: 71.20%   [EVAL] batch:  247 | acc: 81.25%,  total acc: 71.24%   [EVAL] batch:  248 | acc: 100.00%,  total acc: 71.36%   [EVAL] batch:  249 | acc: 100.00%,  total acc: 71.47%   [EVAL] batch:  250 | acc: 31.25%,  total acc: 71.31%   [EVAL] batch:  251 | acc: 37.50%,  total acc: 71.18%   [EVAL] batch:  252 | acc: 25.00%,  total acc: 71.00%   [EVAL] batch:  253 | acc: 37.50%,  total acc: 70.87%   [EVAL] batch:  254 | acc: 31.25%,  total acc: 70.71%   [EVAL] batch:  255 | acc: 43.75%,  total acc: 70.61%   [EVAL] batch:  256 | acc: 56.25%,  total acc: 70.55%   [EVAL] batch:  257 | acc: 81.25%,  total acc: 70.59%   [EVAL] batch:  258 | acc: 68.75%,  total acc: 70.58%   [EVAL] batch:  259 | acc: 68.75%,  total acc: 70.58%   [EVAL] batch:  260 | acc: 68.75%,  total acc: 70.57%   [EVAL] batch:  261 | acc: 62.50%,  total acc: 70.54%   [EVAL] batch:  262 | acc: 100.00%,  total acc: 70.65%   [EVAL] batch:  263 | acc: 87.50%,  total acc: 70.71%   [EVAL] batch:  264 | acc: 81.25%,  total acc: 70.75%   [EVAL] batch:  265 | acc: 81.25%,  total acc: 70.79%   [EVAL] batch:  266 | acc: 93.75%,  total acc: 70.88%   [EVAL] batch:  267 | acc: 93.75%,  total acc: 70.97%   [EVAL] batch:  268 | acc: 81.25%,  total acc: 71.00%   [EVAL] batch:  269 | acc: 0.00%,  total acc: 70.74%   [EVAL] batch:  270 | acc: 12.50%,  total acc: 70.53%   [EVAL] batch:  271 | acc: 12.50%,  total acc: 70.31%   [EVAL] batch:  272 | acc: 25.00%,  total acc: 70.15%   [EVAL] batch:  273 | acc: 31.25%,  total acc: 70.00%   [EVAL] batch:  274 | acc: 12.50%,  total acc: 69.80%   [EVAL] batch:  275 | acc: 100.00%,  total acc: 69.90%   [EVAL] batch:  276 | acc: 93.75%,  total acc: 69.99%   [EVAL] batch:  277 | acc: 100.00%,  total acc: 70.10%   [EVAL] batch:  278 | acc: 81.25%,  total acc: 70.14%   [EVAL] batch:  279 | acc: 93.75%,  total acc: 70.22%   [EVAL] batch:  280 | acc: 81.25%,  total acc: 70.26%   [EVAL] batch:  281 | acc: 100.00%,  total acc: 70.37%   [EVAL] batch:  282 | acc: 100.00%,  total acc: 70.47%   [EVAL] batch:  283 | acc: 100.00%,  total acc: 70.58%   [EVAL] batch:  284 | acc: 100.00%,  total acc: 70.68%   [EVAL] batch:  285 | acc: 100.00%,  total acc: 70.78%   [EVAL] batch:  286 | acc: 100.00%,  total acc: 70.88%   [EVAL] batch:  287 | acc: 100.00%,  total acc: 70.99%   [EVAL] batch:  288 | acc: 93.75%,  total acc: 71.06%   [EVAL] batch:  289 | acc: 75.00%,  total acc: 71.08%   [EVAL] batch:  290 | acc: 81.25%,  total acc: 71.11%   [EVAL] batch:  291 | acc: 93.75%,  total acc: 71.19%   [EVAL] batch:  292 | acc: 93.75%,  total acc: 71.27%   [EVAL] batch:  293 | acc: 81.25%,  total acc: 71.30%   [EVAL] batch:  294 | acc: 31.25%,  total acc: 71.17%   [EVAL] batch:  295 | acc: 18.75%,  total acc: 70.99%   [EVAL] batch:  296 | acc: 37.50%,  total acc: 70.88%   [EVAL] batch:  297 | acc: 37.50%,  total acc: 70.76%   [EVAL] batch:  298 | acc: 50.00%,  total acc: 70.69%   [EVAL] batch:  299 | acc: 43.75%,  total acc: 70.60%   [EVAL] batch:  300 | acc: 93.75%,  total acc: 70.68%   [EVAL] batch:  301 | acc: 93.75%,  total acc: 70.76%   [EVAL] batch:  302 | acc: 93.75%,  total acc: 70.83%   [EVAL] batch:  303 | acc: 81.25%,  total acc: 70.87%   [EVAL] batch:  304 | acc: 93.75%,  total acc: 70.94%   [EVAL] batch:  305 | acc: 87.50%,  total acc: 71.00%   [EVAL] batch:  306 | acc: 25.00%,  total acc: 70.85%   [EVAL] batch:  307 | acc: 18.75%,  total acc: 70.68%   [EVAL] batch:  308 | acc: 18.75%,  total acc: 70.51%   [EVAL] batch:  309 | acc: 6.25%,  total acc: 70.30%   [EVAL] batch:  310 | acc: 18.75%,  total acc: 70.14%   [EVAL] batch:  311 | acc: 0.00%,  total acc: 69.91%   [EVAL] batch:  312 | acc: 12.50%,  total acc: 69.73%   [EVAL] batch:  313 | acc: 25.00%,  total acc: 69.59%   [EVAL] batch:  314 | acc: 25.00%,  total acc: 69.44%   [EVAL] batch:  315 | acc: 12.50%,  total acc: 69.26%   [EVAL] batch:  316 | acc: 25.00%,  total acc: 69.12%   [EVAL] batch:  317 | acc: 31.25%,  total acc: 69.01%   [EVAL] batch:  318 | acc: 25.00%,  total acc: 68.87%   [EVAL] batch:  319 | acc: 75.00%,  total acc: 68.89%   [EVAL] batch:  320 | acc: 75.00%,  total acc: 68.91%   [EVAL] batch:  321 | acc: 81.25%,  total acc: 68.94%   [EVAL] batch:  322 | acc: 81.25%,  total acc: 68.98%   [EVAL] batch:  323 | acc: 87.50%,  total acc: 69.04%   [EVAL] batch:  324 | acc: 87.50%,  total acc: 69.10%   [EVAL] batch:  325 | acc: 37.50%,  total acc: 69.00%   [EVAL] batch:  326 | acc: 43.75%,  total acc: 68.92%   [EVAL] batch:  327 | acc: 50.00%,  total acc: 68.86%   [EVAL] batch:  328 | acc: 50.00%,  total acc: 68.81%   [EVAL] batch:  329 | acc: 50.00%,  total acc: 68.75%   [EVAL] batch:  330 | acc: 43.75%,  total acc: 68.67%   [EVAL] batch:  331 | acc: 93.75%,  total acc: 68.75%   [EVAL] batch:  332 | acc: 100.00%,  total acc: 68.84%   [EVAL] batch:  333 | acc: 100.00%,  total acc: 68.94%   [EVAL] batch:  334 | acc: 93.75%,  total acc: 69.01%   [EVAL] batch:  335 | acc: 100.00%,  total acc: 69.10%   [EVAL] batch:  336 | acc: 100.00%,  total acc: 69.20%   [EVAL] batch:  337 | acc: 93.75%,  total acc: 69.27%   [EVAL] batch:  338 | acc: 100.00%,  total acc: 69.36%   [EVAL] batch:  339 | acc: 87.50%,  total acc: 69.41%   [EVAL] batch:  340 | acc: 100.00%,  total acc: 69.50%   [EVAL] batch:  341 | acc: 93.75%,  total acc: 69.57%   [EVAL] batch:  342 | acc: 100.00%,  total acc: 69.66%   [EVAL] batch:  343 | acc: 93.75%,  total acc: 69.73%   [EVAL] batch:  344 | acc: 100.00%,  total acc: 69.82%   [EVAL] batch:  345 | acc: 100.00%,  total acc: 69.91%   [EVAL] batch:  346 | acc: 87.50%,  total acc: 69.96%   [EVAL] batch:  347 | acc: 87.50%,  total acc: 70.01%   [EVAL] batch:  348 | acc: 100.00%,  total acc: 70.09%   [EVAL] batch:  349 | acc: 100.00%,  total acc: 70.18%   [EVAL] batch:  350 | acc: 25.00%,  total acc: 70.05%   [EVAL] batch:  351 | acc: 18.75%,  total acc: 69.90%   [EVAL] batch:  352 | acc: 43.75%,  total acc: 69.83%   [EVAL] batch:  353 | acc: 18.75%,  total acc: 69.69%   [EVAL] batch:  354 | acc: 12.50%,  total acc: 69.52%   [EVAL] batch:  355 | acc: 31.25%,  total acc: 69.42%   [EVAL] batch:  356 | acc: 87.50%,  total acc: 69.47%   [EVAL] batch:  357 | acc: 93.75%,  total acc: 69.54%   [EVAL] batch:  358 | acc: 93.75%,  total acc: 69.60%   [EVAL] batch:  359 | acc: 100.00%,  total acc: 69.69%   [EVAL] batch:  360 | acc: 87.50%,  total acc: 69.74%   [EVAL] batch:  361 | acc: 81.25%,  total acc: 69.77%   [EVAL] batch:  362 | acc: 68.75%,  total acc: 69.77%   [EVAL] batch:  363 | acc: 62.50%,  total acc: 69.75%   [EVAL] batch:  364 | acc: 62.50%,  total acc: 69.73%   [EVAL] batch:  365 | acc: 50.00%,  total acc: 69.67%   [EVAL] batch:  366 | acc: 62.50%,  total acc: 69.65%   [EVAL] batch:  367 | acc: 56.25%,  total acc: 69.62%   [EVAL] batch:  368 | acc: 37.50%,  total acc: 69.53%   [EVAL] batch:  369 | acc: 87.50%,  total acc: 69.58%   [EVAL] batch:  370 | acc: 68.75%,  total acc: 69.58%   [EVAL] batch:  371 | acc: 50.00%,  total acc: 69.52%   [EVAL] batch:  372 | acc: 87.50%,  total acc: 69.57%   [EVAL] batch:  373 | acc: 87.50%,  total acc: 69.62%   [EVAL] batch:  374 | acc: 68.75%,  total acc: 69.62%   [EVAL] batch:  375 | acc: 50.00%,  total acc: 69.56%   [EVAL] batch:  376 | acc: 50.00%,  total acc: 69.51%   [EVAL] batch:  377 | acc: 50.00%,  total acc: 69.46%   [EVAL] batch:  378 | acc: 43.75%,  total acc: 69.39%   [EVAL] batch:  379 | acc: 56.25%,  total acc: 69.36%   [EVAL] batch:  380 | acc: 50.00%,  total acc: 69.31%   [EVAL] batch:  381 | acc: 93.75%,  total acc: 69.37%   [EVAL] batch:  382 | acc: 100.00%,  total acc: 69.45%   [EVAL] batch:  383 | acc: 81.25%,  total acc: 69.48%   [EVAL] batch:  384 | acc: 68.75%,  total acc: 69.48%   [EVAL] batch:  385 | acc: 81.25%,  total acc: 69.51%   [EVAL] batch:  386 | acc: 100.00%,  total acc: 69.59%   [EVAL] batch:  387 | acc: 75.00%,  total acc: 69.60%   [EVAL] batch:  388 | acc: 50.00%,  total acc: 69.55%   [EVAL] batch:  389 | acc: 56.25%,  total acc: 69.52%   [EVAL] batch:  390 | acc: 56.25%,  total acc: 69.49%   [EVAL] batch:  391 | acc: 75.00%,  total acc: 69.50%   [EVAL] batch:  392 | acc: 62.50%,  total acc: 69.48%   [EVAL] batch:  393 | acc: 81.25%,  total acc: 69.51%   [EVAL] batch:  394 | acc: 43.75%,  total acc: 69.45%   [EVAL] batch:  395 | acc: 62.50%,  total acc: 69.43%   [EVAL] batch:  396 | acc: 37.50%,  total acc: 69.35%   [EVAL] batch:  397 | acc: 56.25%,  total acc: 69.32%   [EVAL] batch:  398 | acc: 62.50%,  total acc: 69.30%   [EVAL] batch:  399 | acc: 56.25%,  total acc: 69.27%   [EVAL] batch:  400 | acc: 62.50%,  total acc: 69.25%   [EVAL] batch:  401 | acc: 62.50%,  total acc: 69.23%   [EVAL] batch:  402 | acc: 31.25%,  total acc: 69.14%   [EVAL] batch:  403 | acc: 62.50%,  total acc: 69.12%   [EVAL] batch:  404 | acc: 43.75%,  total acc: 69.06%   [EVAL] batch:  405 | acc: 25.00%,  total acc: 68.95%   [EVAL] batch:  406 | acc: 62.50%,  total acc: 68.93%   [EVAL] batch:  407 | acc: 56.25%,  total acc: 68.90%   [EVAL] batch:  408 | acc: 62.50%,  total acc: 68.89%   [EVAL] batch:  409 | acc: 68.75%,  total acc: 68.89%   [EVAL] batch:  410 | acc: 62.50%,  total acc: 68.87%   [EVAL] batch:  411 | acc: 68.75%,  total acc: 68.87%   [EVAL] batch:  412 | acc: 68.75%,  total acc: 68.87%   [EVAL] batch:  413 | acc: 62.50%,  total acc: 68.86%   [EVAL] batch:  414 | acc: 75.00%,  total acc: 68.87%   [EVAL] batch:  415 | acc: 75.00%,  total acc: 68.89%   [EVAL] batch:  416 | acc: 81.25%,  total acc: 68.91%   [EVAL] batch:  417 | acc: 81.25%,  total acc: 68.94%   [EVAL] batch:  418 | acc: 68.75%,  total acc: 68.94%   [EVAL] batch:  419 | acc: 37.50%,  total acc: 68.87%   [EVAL] batch:  420 | acc: 31.25%,  total acc: 68.78%   [EVAL] batch:  421 | acc: 31.25%,  total acc: 68.69%   [EVAL] batch:  422 | acc: 37.50%,  total acc: 68.62%   [EVAL] batch:  423 | acc: 25.00%,  total acc: 68.51%   [EVAL] batch:  424 | acc: 37.50%,  total acc: 68.44%   [EVAL] batch:  425 | acc: 100.00%,  total acc: 68.52%   [EVAL] batch:  426 | acc: 100.00%,  total acc: 68.59%   [EVAL] batch:  427 | acc: 100.00%,  total acc: 68.66%   [EVAL] batch:  428 | acc: 100.00%,  total acc: 68.74%   [EVAL] batch:  429 | acc: 100.00%,  total acc: 68.81%   [EVAL] batch:  430 | acc: 100.00%,  total acc: 68.88%   [EVAL] batch:  431 | acc: 81.25%,  total acc: 68.91%   [EVAL] batch:  432 | acc: 75.00%,  total acc: 68.92%   [EVAL] batch:  433 | acc: 75.00%,  total acc: 68.94%   [EVAL] batch:  434 | acc: 87.50%,  total acc: 68.98%   [EVAL] batch:  435 | acc: 87.50%,  total acc: 69.02%   [EVAL] batch:  436 | acc: 81.25%,  total acc: 69.05%   [EVAL] batch:  437 | acc: 37.50%,  total acc: 68.98%   
cur_acc:  ['0.9484', '0.8710', '0.7014', '0.7480', '0.7331', '0.6905', '0.6518']
his_acc:  ['0.9484', '0.9020', '0.8162', '0.7650', '0.7406', '0.7122', '0.6898']
CurrentTrain: epoch 15, batch     0 | loss: 25.4719244CurrentTrain: epoch 15, batch     1 | loss: 15.0502774CurrentTrain: epoch 15, batch     2 | loss: 12.4824344CurrentTrain: epoch  1, batch     3 | loss: 11.4678925CurrentTrain: epoch 15, batch     0 | loss: 25.1422411CurrentTrain: epoch 15, batch     1 | loss: 13.5712352CurrentTrain: epoch 15, batch     2 | loss: 9.8140186CurrentTrain: epoch  1, batch     3 | loss: 12.3749853CurrentTrain: epoch 15, batch     0 | loss: 15.8729551CurrentTrain: epoch 15, batch     1 | loss: 16.0433025CurrentTrain: epoch 15, batch     2 | loss: 14.0629483CurrentTrain: epoch  1, batch     3 | loss: 8.4364197CurrentTrain: epoch 15, batch     0 | loss: 20.4699666CurrentTrain: epoch 15, batch     1 | loss: 16.3328426CurrentTrain: epoch 15, batch     2 | loss: 12.0521999CurrentTrain: epoch  1, batch     3 | loss: 8.1087274CurrentTrain: epoch 15, batch     0 | loss: 11.4571013CurrentTrain: epoch 15, batch     1 | loss: 12.1839869CurrentTrain: epoch 15, batch     2 | loss: 9.2620052CurrentTrain: epoch  1, batch     3 | loss: 6.0814891CurrentTrain: epoch 15, batch     0 | loss: 11.6477500CurrentTrain: epoch 15, batch     1 | loss: 16.8479265CurrentTrain: epoch 15, batch     2 | loss: 21.3420119CurrentTrain: epoch  1, batch     3 | loss: 8.0304866CurrentTrain: epoch 15, batch     0 | loss: 14.6017085CurrentTrain: epoch 15, batch     1 | loss: 13.0574571CurrentTrain: epoch 15, batch     2 | loss: 11.1920163CurrentTrain: epoch  1, batch     3 | loss: 6.4445612CurrentTrain: epoch 15, batch     0 | loss: 12.6648001CurrentTrain: epoch 15, batch     1 | loss: 11.6609244CurrentTrain: epoch 15, batch     2 | loss: 9.5634888CurrentTrain: epoch  1, batch     3 | loss: 9.5900480CurrentTrain: epoch 15, batch     0 | loss: 8.2517283CurrentTrain: epoch 15, batch     1 | loss: 12.7866414CurrentTrain: epoch 15, batch     2 | loss: 9.0914175CurrentTrain: epoch  1, batch     3 | loss: 6.1188009CurrentTrain: epoch 15, batch     0 | loss: 17.1567983CurrentTrain: epoch 15, batch     1 | loss: 9.3506714CurrentTrain: epoch 15, batch     2 | loss: 7.4527361CurrentTrain: epoch  1, batch     3 | loss: 7.9241993
#### main subject ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "main subject" means primary topic of a work.
Here is an example:
Relation: main subject
Context: auerbach is prominently featured in the documentary film , " the first basket " , about jewish basketball history .
Head Entity: the first basket
Tail Entity: basketball
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "main subject":
Relation: main subject  
Context: the novel "Pride and Prejudice" explores the themes of love and social class in early 19th century England.  
Head Entity: Pride and Prejudice  
Tail Entity: love  

Relation: main subject  
Context: the documentary "Our Planet" highlights the impact of climate change on various ecosystems around the world.  
Head Entity: Our Planet  
Tail Entity: climate change  
#### participating team ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "participating team" means teams that actively takes/took part in an event or process.
Here is an example:
Relation: participating team
Context: he represented romania at euro 2000 , his first major tournament where they beat england 3–2 , and drew 1–1 against germany .
Head Entity: euro 2000
Tail Entity: germany
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "participating team":
Relation: participating team  
Context: The Brazilian national team showcased their skills at the 2014 FIFA World Cup, where they faced off against Germany in the semi-finals.  
Head Entity: 2014 FIFA World Cup  
Tail Entity: Germany  

Relation: participating team  
Context: During the 2021 Tokyo Olympics, the United States women's soccer team competed fiercely, ultimately playing against Canada in the semi-finals.  
Head Entity: 2021 Tokyo Olympics  
Tail Entity: Canada  
#### has part ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "has part" means part of this subject.
Here is an example:
Relation: has part
Context: among the competing artists was previous eurovision song contest entrant robin juhkental who represented estonia in 2010 as part of the group malcolm lincoln .
Head Entity: malcolm lincoln
Tail Entity: robin juhkental
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "has part":
Relation: has part  
Context: The famous painting "The Last Supper" features several apostles, including Peter, who is depicted prominently on the left side of the artwork.  
Head Entity: The Last Supper  
Tail Entity: Peter  

Relation: has part  
Context: The smartphone model Galaxy S21 comes with a high-resolution camera that includes a wide-angle lens, allowing users to capture stunning landscapes.  
Head Entity: Galaxy S21  
Tail Entity: wide-angle lens  
#### nominated for ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "nominated for" means award nomination received by a person, organisation or creative work.
Here is an example:
Relation: nominated for
Context: he was nominated for the academy award for best picture , along with steven spielberg , ian bryce , mark gordon for the film " saving private ryan " .
Head Entity: ian bryce
Tail Entity: academy award for best picture
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "nominated for":
Relation: nominated for  
Context: The film "The Shape of Water" received multiple nominations, including a nod for best director for Guillermo del Toro at the Academy Awards.  
Head Entity: Guillermo del Toro  
Tail Entity: Academy Awards  

Relation: nominated for  
Context: Beyoncé was nominated for several Grammy Awards this year, showcasing her incredible talent and influence in the music industry.  
Head Entity: Beyoncé  
Tail Entity: Grammy Awards  
#### instance of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "instance of" means that class of which this subject is a particular example and member.
Here is an example:
Relation: instance of
Context: bell gothic was replaced by at&t ; with matthew carter 's typeface bell centennial in 1978 , the one hundredth anniversary of at&t ; 's founding .
Head Entity: bell centennial
Tail Entity: typeface
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "instance of":
Relation: instance of  
Context: the smartphone is a type of mobile phone that combines cellular and mobile computing functions into one device.  
Head Entity: smartphone  
Tail Entity: mobile phone  

Relation: instance of  
Context: the golden retriever is a popular breed known for its friendly and tolerant attitude.  
Head Entity: golden retriever  
Tail Entity: breed  
#### mountain range ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "mountain range" means range or subrange to which the geographical item belongs.
Here is an example:
Relation: mountain range
Context: abus valley ( ) is an ice - free valley southeast of turnstile ridge at the north end of britannia range .
Head Entity: turnstile ridge
Tail Entity: britannia range
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "mountain range":
Relation: mountain range  
Context: the sierra nevada ( ) is a mountain range in the western united states, primarily in the state of california, and it includes the famous yosemite national park, which is located within the range.  
Head Entity: sierra nevada  
Tail Entity: yosemite national park  

Relation: mountain range  
Context: the appalachian mountains ( ) extend from the canadian province of quebec down to alabama, making it one of the longest mountain ranges in north america.  
Head Entity: appalachian mountains  
Tail Entity: alabama  
#### screenwriter ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "screenwriter" means person who wrote the script for subject item.
Here is an example:
Relation: screenwriter
Context: it would also be the last episode to feature a grounding , until " the marge - ian chronicles " in season 27 , six years later ( also written by brian kelley ) .
Head Entity: the marge - ian chronicles
Tail Entity: brian kelley
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "screenwriter":
Relation: screenwriter  
Context: The film "Inception" was a groundbreaking project that showcased the visionary talents of its creator, with the screenplay crafted by the brilliant Christopher Nolan.  
Head Entity: Inception  
Tail Entity: Christopher Nolan  

Relation: screenwriter  
Context: The beloved animated feature "Toy Story" was brought to life through the imaginative script penned by the talented Andrew Stanton.  
Head Entity: Toy Story  
Tail Entity: Andrew Stanton  
#### language of work or name ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "language of work or name" means language associated with this creative work (such as books, shows, songs, broadcasts or websites) or a name.
Here is an example:
Relation: language of work or name
Context: the publisher found success in 1993 with tarō gomi 's " everyone poops " , a japanese language import that sold over one million copies and is now their best - known title .
Head Entity: everyone poops
Tail Entity: japanese language
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "language of work or name":
Relation: language of work or name  
Context: The animated series "Doraemon" has been translated into multiple languages, but the original version is in Japanese, which has captivated audiences worldwide.  
Head Entity: Doraemon  
Tail Entity: Japanese  

Relation: language of work or name  
Context: The novel "One Hundred Years of Solitude" by Gabriel García Márquez is originally written in Spanish and has been translated into numerous languages, making it a global literary phenomenon.  
Head Entity: One Hundred Years of Solitude  
Tail Entity: Spanish  
#### operator ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "operator" means person, profession, or organization that operates the equipment, facility, or service.
Here is an example:
Relation: operator
Context: saratov airlines ( saratov airlines joint stock company , , " saratovskije avialinii " ) is a russian airline headquartered in saratov and based at saratov tsentralny airport .
Head Entity: saratov tsentralny airport
Tail Entity: saratov airlines
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "operator":
Relation: operator  
Context: The New York City Transit Authority operates the subway system, providing essential transportation services to millions of commuters daily.  
Head Entity: New York City Transit Authority  
Tail Entity: subway system  

Relation: operator  
Context: Tesla, Inc. is known for its electric vehicles and operates several Gigafactories around the world to produce batteries and vehicles.  
Head Entity: Tesla, Inc.  
Tail Entity: Gigafactories
#### religion ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "religion" means religion of a person, organization or religious building, or associated with this subject.
Here is an example:
Relation: religion
Context: the most important lama of this series was the third changkya , rolpai dorje , who was preceptor to the qianlong emperor of china , and chief representative of tibetan buddhism at the qing court .
Head Entity: rolpai dorje
Tail Entity: tibetan buddhism
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "religion":
Relation: religion  
Context: the cathedral of notre-dame is a famous example of french gothic architecture and serves as a major center for the roman catholic faith in paris.  
Head Entity: notre-dame  
Tail Entity: roman catholic faith  

Relation: religion  
Context: the dalai lama is a prominent figure in the practice of tibetan buddhism, advocating for peace and compassion around the world.  
Head Entity: dalai lama  
Tail Entity: tibetan buddhism  
MemoryTrain:  epoch 15, batch     0 | loss: 2.4376466MemoryTrain:  epoch 15, batch     1 | loss: 2.0308013MemoryTrain:  epoch 15, batch     2 | loss: 4.4867234MemoryTrain:  epoch 15, batch     3 | loss: 3.2420406MemoryTrain:  epoch 15, batch     4 | loss: 3.4662592MemoryTrain:  epoch 15, batch     5 | loss: 2.4970136MemoryTrain:  epoch 15, batch     6 | loss: 3.8691300MemoryTrain:  epoch 15, batch     7 | loss: 2.2187383MemoryTrain:  epoch 15, batch     8 | loss: 4.7864925MemoryTrain:  epoch 15, batch     9 | loss: 2.4711280MemoryTrain:  epoch 15, batch    10 | loss: 3.0576183MemoryTrain:  epoch 15, batch    11 | loss: 2.2870007MemoryTrain:  epoch 15, batch    12 | loss: 2.9388758MemoryTrain:  epoch 15, batch    13 | loss: 3.6560559MemoryTrain:  epoch 15, batch    14 | loss: 2.9598892MemoryTrain:  epoch 15, batch     0 | loss: 3.7861615MemoryTrain:  epoch 15, batch     1 | loss: 2.4071563MemoryTrain:  epoch 15, batch     2 | loss: 2.1930434MemoryTrain:  epoch 15, batch     3 | loss: 3.4191660MemoryTrain:  epoch 15, batch     4 | loss: 3.6504340MemoryTrain:  epoch 15, batch     5 | loss: 4.2697931MemoryTrain:  epoch 15, batch     6 | loss: 1.7482364MemoryTrain:  epoch 15, batch     7 | loss: 3.0197211MemoryTrain:  epoch 15, batch     8 | loss: 3.5315406MemoryTrain:  epoch 15, batch     9 | loss: 1.8554195MemoryTrain:  epoch 15, batch    10 | loss: 6.1683066MemoryTrain:  epoch 15, batch    11 | loss: 1.9431419MemoryTrain:  epoch 15, batch    12 | loss: 1.6756751MemoryTrain:  epoch 15, batch    13 | loss: 2.3278011MemoryTrain:  epoch 15, batch    14 | loss: 3.4517308MemoryTrain:  epoch 15, batch     0 | loss: 2.7425293MemoryTrain:  epoch 15, batch     1 | loss: 2.3717947MemoryTrain:  epoch 15, batch     2 | loss: 4.7483286MemoryTrain:  epoch 15, batch     3 | loss: 2.9108528MemoryTrain:  epoch 15, batch     4 | loss: 2.7976131MemoryTrain:  epoch 15, batch     5 | loss: 2.7073432MemoryTrain:  epoch 15, batch     6 | loss: 4.1699450MemoryTrain:  epoch 15, batch     7 | loss: 1.8089741MemoryTrain:  epoch 15, batch     8 | loss: 6.0514819MemoryTrain:  epoch 15, batch     9 | loss: 2.7000725MemoryTrain:  epoch 15, batch    10 | loss: 4.4814929MemoryTrain:  epoch 15, batch    11 | loss: 1.8920638MemoryTrain:  epoch 15, batch    12 | loss: 1.4846781MemoryTrain:  epoch 15, batch    13 | loss: 4.1699522MemoryTrain:  epoch 15, batch    14 | loss: 3.0033720MemoryTrain:  epoch 15, batch     0 | loss: 2.2619469MemoryTrain:  epoch 15, batch     1 | loss: 1.6460866MemoryTrain:  epoch 15, batch     2 | loss: 2.6273702MemoryTrain:  epoch 15, batch     3 | loss: 2.2373910MemoryTrain:  epoch 15, batch     4 | loss: 2.8928628MemoryTrain:  epoch 15, batch     5 | loss: 1.7034379MemoryTrain:  epoch 15, batch     6 | loss: 3.0153254MemoryTrain:  epoch 15, batch     7 | loss: 1.8857543MemoryTrain:  epoch 15, batch     8 | loss: 1.9404772MemoryTrain:  epoch 15, batch     9 | loss: 1.3975235MemoryTrain:  epoch 15, batch    10 | loss: 2.7139687MemoryTrain:  epoch 15, batch    11 | loss: 1.5444739MemoryTrain:  epoch 15, batch    12 | loss: 2.1497162MemoryTrain:  epoch 15, batch    13 | loss: 2.8938157MemoryTrain:  epoch 15, batch    14 | loss: 2.2393545MemoryTrain:  epoch 15, batch     0 | loss: 2.2687620MemoryTrain:  epoch 15, batch     1 | loss: 2.5880063MemoryTrain:  epoch 15, batch     2 | loss: 2.0323263MemoryTrain:  epoch 15, batch     3 | loss: 1.7211249MemoryTrain:  epoch 15, batch     4 | loss: 1.4288764MemoryTrain:  epoch 15, batch     5 | loss: 3.3486502MemoryTrain:  epoch 15, batch     6 | loss: 1.2992220MemoryTrain:  epoch 15, batch     7 | loss: 2.6053228MemoryTrain:  epoch 15, batch     8 | loss: 1.4910450MemoryTrain:  epoch 15, batch     9 | loss: 3.2585019MemoryTrain:  epoch 15, batch    10 | loss: 1.8548608MemoryTrain:  epoch 15, batch    11 | loss: 1.9513691MemoryTrain:  epoch 15, batch    12 | loss: 2.1778582MemoryTrain:  epoch 15, batch    13 | loss: 2.4307283MemoryTrain:  epoch 15, batch    14 | loss: 4.3374143MemoryTrain:  epoch 15, batch     0 | loss: 1.9488891MemoryTrain:  epoch 15, batch     1 | loss: 1.9357043MemoryTrain:  epoch 15, batch     2 | loss: 1.2845604MemoryTrain:  epoch 15, batch     3 | loss: 3.9862318MemoryTrain:  epoch 15, batch     4 | loss: 1.7636331MemoryTrain:  epoch 15, batch     5 | loss: 2.4582931MemoryTrain:  epoch 15, batch     6 | loss: 1.5145332MemoryTrain:  epoch 15, batch     7 | loss: 2.0363273MemoryTrain:  epoch 15, batch     8 | loss: 3.2847139MemoryTrain:  epoch 15, batch     9 | loss: 1.3969152MemoryTrain:  epoch 15, batch    10 | loss: 1.9686297MemoryTrain:  epoch 15, batch    11 | loss: 1.7181604MemoryTrain:  epoch 15, batch    12 | loss: 1.6781405MemoryTrain:  epoch 15, batch    13 | loss: 4.2347199MemoryTrain:  epoch 15, batch    14 | loss: 1.4273384MemoryTrain:  epoch 15, batch     0 | loss: 1.9103402MemoryTrain:  epoch 15, batch     1 | loss: 2.0589561MemoryTrain:  epoch 15, batch     2 | loss: 1.7913477MemoryTrain:  epoch 15, batch     3 | loss: 1.3206535MemoryTrain:  epoch 15, batch     4 | loss: 1.3747209MemoryTrain:  epoch 15, batch     5 | loss: 2.4363892MemoryTrain:  epoch 15, batch     6 | loss: 4.4122304MemoryTrain:  epoch 15, batch     7 | loss: 2.2800723MemoryTrain:  epoch 15, batch     8 | loss: 1.4076968MemoryTrain:  epoch 15, batch     9 | loss: 1.3748572MemoryTrain:  epoch 15, batch    10 | loss: 1.7302771MemoryTrain:  epoch 15, batch    11 | loss: 2.8814397MemoryTrain:  epoch 15, batch    12 | loss: 4.2680086MemoryTrain:  epoch 15, batch    13 | loss: 1.6966215MemoryTrain:  epoch 15, batch    14 | loss: 1.5258247MemoryTrain:  epoch 15, batch     0 | loss: 1.3587727MemoryTrain:  epoch 15, batch     1 | loss: 3.7733270MemoryTrain:  epoch 15, batch     2 | loss: 2.7205212MemoryTrain:  epoch 15, batch     3 | loss: 1.7259306MemoryTrain:  epoch 15, batch     4 | loss: 3.9931675MemoryTrain:  epoch 15, batch     5 | loss: 1.9823952MemoryTrain:  epoch 15, batch     6 | loss: 3.7003642MemoryTrain:  epoch 15, batch     7 | loss: 1.3159493MemoryTrain:  epoch 15, batch     8 | loss: 2.0619198MemoryTrain:  epoch 15, batch     9 | loss: 4.2005846MemoryTrain:  epoch 15, batch    10 | loss: 1.6382027MemoryTrain:  epoch 15, batch    11 | loss: 1.7481403MemoryTrain:  epoch 15, batch    12 | loss: 1.7153422MemoryTrain:  epoch 15, batch    13 | loss: 1.4928407MemoryTrain:  epoch 15, batch    14 | loss: 1.8817447MemoryTrain:  epoch 15, batch     0 | loss: 1.5443833MemoryTrain:  epoch 15, batch     1 | loss: 1.7066234MemoryTrain:  epoch 15, batch     2 | loss: 3.5979375MemoryTrain:  epoch 15, batch     3 | loss: 1.3583586MemoryTrain:  epoch 15, batch     4 | loss: 1.7039824MemoryTrain:  epoch 15, batch     5 | loss: 1.3292729MemoryTrain:  epoch 15, batch     6 | loss: 1.6562553MemoryTrain:  epoch 15, batch     7 | loss: 1.3122349MemoryTrain:  epoch 15, batch     8 | loss: 1.2943038MemoryTrain:  epoch 15, batch     9 | loss: 1.8270594MemoryTrain:  epoch 15, batch    10 | loss: 1.7275936MemoryTrain:  epoch 15, batch    11 | loss: 1.4391359MemoryTrain:  epoch 15, batch    12 | loss: 1.4800790MemoryTrain:  epoch 15, batch    13 | loss: 1.3684783MemoryTrain:  epoch 15, batch    14 | loss: 3.9006158MemoryTrain:  epoch 15, batch     0 | loss: 1.3489953MemoryTrain:  epoch 15, batch     1 | loss: 1.6866075MemoryTrain:  epoch 15, batch     2 | loss: 2.3013071MemoryTrain:  epoch 15, batch     3 | loss: 1.3679916MemoryTrain:  epoch 15, batch     4 | loss: 4.4141980MemoryTrain:  epoch 15, batch     5 | loss: 3.9251372MemoryTrain:  epoch 15, batch     6 | loss: 1.3393824MemoryTrain:  epoch 15, batch     7 | loss: 2.0356902MemoryTrain:  epoch 15, batch     8 | loss: 1.7199782MemoryTrain:  epoch 15, batch     9 | loss: 1.7047405MemoryTrain:  epoch 15, batch    10 | loss: 1.7517797MemoryTrain:  epoch 15, batch    11 | loss: 1.7901884MemoryTrain:  epoch 15, batch    12 | loss: 3.0168781MemoryTrain:  epoch 15, batch    13 | loss: 3.5002649MemoryTrain:  epoch 15, batch    14 | loss: 3.9716021
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    1 | acc: 43.75%,  total acc: 53.12%   [EVAL] batch:    2 | acc: 62.50%,  total acc: 56.25%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 59.38%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 62.50%   [EVAL] batch:    5 | acc: 68.75%,  total acc: 63.54%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 66.07%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 68.75%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 70.62%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 71.59%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 72.92%   [EVAL] batch:   12 | acc: 62.50%,  total acc: 72.12%   [EVAL] batch:   13 | acc: 6.25%,  total acc: 67.41%   [EVAL] batch:   14 | acc: 31.25%,  total acc: 65.00%   [EVAL] batch:   15 | acc: 31.25%,  total acc: 62.89%   [EVAL] batch:   16 | acc: 50.00%,  total acc: 62.13%   [EVAL] batch:   17 | acc: 43.75%,  total acc: 61.11%   [EVAL] batch:   18 | acc: 37.50%,  total acc: 59.87%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 61.56%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 63.39%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 65.06%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 66.58%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 67.97%   [EVAL] batch:   24 | acc: 93.75%,  total acc: 69.00%   [EVAL] batch:   25 | acc: 6.25%,  total acc: 66.59%   [EVAL] batch:   26 | acc: 0.00%,  total acc: 64.12%   [EVAL] batch:   27 | acc: 6.25%,  total acc: 62.05%   [EVAL] batch:   28 | acc: 12.50%,  total acc: 60.34%   [EVAL] batch:   29 | acc: 18.75%,  total acc: 58.96%   [EVAL] batch:   30 | acc: 0.00%,  total acc: 57.06%   [EVAL] batch:   31 | acc: 68.75%,  total acc: 57.42%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 58.52%   [EVAL] batch:   33 | acc: 87.50%,  total acc: 59.38%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 60.36%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 61.11%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 62.16%   [EVAL] batch:   37 | acc: 81.25%,  total acc: 62.66%   [EVAL] batch:   38 | acc: 50.00%,  total acc: 62.34%   [EVAL] batch:   39 | acc: 68.75%,  total acc: 62.50%   [EVAL] batch:   40 | acc: 43.75%,  total acc: 62.04%   [EVAL] batch:   41 | acc: 43.75%,  total acc: 61.61%   [EVAL] batch:   42 | acc: 68.75%,  total acc: 61.77%   [EVAL] batch:   43 | acc: 75.00%,  total acc: 62.07%   [EVAL] batch:   44 | acc: 56.25%,  total acc: 61.94%   [EVAL] batch:   45 | acc: 68.75%,  total acc: 62.09%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 62.50%   [EVAL] batch:   47 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:   48 | acc: 81.25%,  total acc: 62.88%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 63.38%   [EVAL] batch:   50 | acc: 56.25%,  total acc: 63.24%   [EVAL] batch:   51 | acc: 56.25%,  total acc: 63.10%   [EVAL] batch:   52 | acc: 50.00%,  total acc: 62.85%   [EVAL] batch:   53 | acc: 56.25%,  total acc: 62.73%   [EVAL] batch:   54 | acc: 62.50%,  total acc: 62.73%   [EVAL] batch:   55 | acc: 75.00%,  total acc: 62.95%   [EVAL] batch:   56 | acc: 75.00%,  total acc: 63.16%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 63.58%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 64.09%   [EVAL] batch:   59 | acc: 81.25%,  total acc: 64.38%   [EVAL] batch:   60 | acc: 93.75%,  total acc: 64.86%   [EVAL] batch:   61 | acc: 75.00%,  total acc: 65.02%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 64.78%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 80.00%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    6 | acc: 43.75%,  total acc: 75.89%   [EVAL] batch:    7 | acc: 43.75%,  total acc: 71.88%   [EVAL] batch:    8 | acc: 56.25%,  total acc: 70.14%   [EVAL] batch:    9 | acc: 50.00%,  total acc: 68.12%   [EVAL] batch:   10 | acc: 31.25%,  total acc: 64.77%   [EVAL] batch:   11 | acc: 43.75%,  total acc: 63.02%   [EVAL] batch:   12 | acc: 56.25%,  total acc: 62.50%   [EVAL] batch:   13 | acc: 87.50%,  total acc: 64.29%   [EVAL] batch:   14 | acc: 87.50%,  total acc: 65.83%   [EVAL] batch:   15 | acc: 87.50%,  total acc: 67.19%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 69.12%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 70.49%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 71.71%   [EVAL] batch:   19 | acc: 68.75%,  total acc: 71.56%   [EVAL] batch:   20 | acc: 81.25%,  total acc: 72.02%   [EVAL] batch:   21 | acc: 68.75%,  total acc: 71.88%   [EVAL] batch:   22 | acc: 56.25%,  total acc: 71.20%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 72.14%   [EVAL] batch:   24 | acc: 81.25%,  total acc: 72.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 73.56%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 74.54%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 75.45%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 76.08%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 76.88%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 77.62%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 78.32%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 78.98%   [EVAL] batch:   33 | acc: 81.25%,  total acc: 79.04%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 78.93%   [EVAL] batch:   35 | acc: 75.00%,  total acc: 78.82%   [EVAL] batch:   36 | acc: 93.75%,  total acc: 79.22%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 79.44%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 79.97%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 80.47%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 80.95%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 81.54%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 81.53%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 81.67%   [EVAL] batch:   45 | acc: 75.00%,  total acc: 81.52%   [EVAL] batch:   46 | acc: 50.00%,  total acc: 80.85%   [EVAL] batch:   47 | acc: 62.50%,  total acc: 80.47%   [EVAL] batch:   48 | acc: 81.25%,  total acc: 80.48%   [EVAL] batch:   49 | acc: 62.50%,  total acc: 80.12%   [EVAL] batch:   50 | acc: 75.00%,  total acc: 80.02%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 80.17%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 80.42%   [EVAL] batch:   53 | acc: 68.75%,  total acc: 80.21%   [EVAL] batch:   54 | acc: 50.00%,  total acc: 79.66%   [EVAL] batch:   55 | acc: 81.25%,  total acc: 79.69%   [EVAL] batch:   56 | acc: 68.75%,  total acc: 79.50%   [EVAL] batch:   57 | acc: 43.75%,  total acc: 78.88%   [EVAL] batch:   58 | acc: 56.25%,  total acc: 78.50%   [EVAL] batch:   59 | acc: 81.25%,  total acc: 78.54%   [EVAL] batch:   60 | acc: 81.25%,  total acc: 78.59%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 78.43%   [EVAL] batch:   62 | acc: 87.50%,  total acc: 78.57%   [EVAL] batch:   63 | acc: 93.75%,  total acc: 78.81%   [EVAL] batch:   64 | acc: 93.75%,  total acc: 79.04%   [EVAL] batch:   65 | acc: 100.00%,  total acc: 79.36%   [EVAL] batch:   66 | acc: 81.25%,  total acc: 79.38%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 79.69%   [EVAL] batch:   68 | acc: 87.50%,  total acc: 79.80%   [EVAL] batch:   69 | acc: 50.00%,  total acc: 79.38%   [EVAL] batch:   70 | acc: 50.00%,  total acc: 78.96%   [EVAL] batch:   71 | acc: 68.75%,  total acc: 78.82%   [EVAL] batch:   72 | acc: 43.75%,  total acc: 78.34%   [EVAL] batch:   73 | acc: 50.00%,  total acc: 77.96%   [EVAL] batch:   74 | acc: 37.50%,  total acc: 77.42%   [EVAL] batch:   75 | acc: 68.75%,  total acc: 77.30%   [EVAL] batch:   76 | acc: 81.25%,  total acc: 77.35%   [EVAL] batch:   77 | acc: 62.50%,  total acc: 77.16%   [EVAL] batch:   78 | acc: 43.75%,  total acc: 76.74%   [EVAL] batch:   79 | acc: 75.00%,  total acc: 76.72%   [EVAL] batch:   80 | acc: 56.25%,  total acc: 76.47%   [EVAL] batch:   81 | acc: 56.25%,  total acc: 76.22%   [EVAL] batch:   82 | acc: 75.00%,  total acc: 76.20%   [EVAL] batch:   83 | acc: 81.25%,  total acc: 76.26%   [EVAL] batch:   84 | acc: 56.25%,  total acc: 76.03%   [EVAL] batch:   85 | acc: 81.25%,  total acc: 76.09%   [EVAL] batch:   86 | acc: 62.50%,  total acc: 75.93%   [EVAL] batch:   87 | acc: 87.50%,  total acc: 76.07%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 76.33%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 76.60%   [EVAL] batch:   90 | acc: 100.00%,  total acc: 76.85%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 77.11%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 77.35%   [EVAL] batch:   93 | acc: 93.75%,  total acc: 77.53%   [EVAL] batch:   94 | acc: 56.25%,  total acc: 77.30%   [EVAL] batch:   95 | acc: 75.00%,  total acc: 77.28%   [EVAL] batch:   96 | acc: 62.50%,  total acc: 77.13%   [EVAL] batch:   97 | acc: 62.50%,  total acc: 76.98%   [EVAL] batch:   98 | acc: 81.25%,  total acc: 77.02%   [EVAL] batch:   99 | acc: 43.75%,  total acc: 76.69%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 76.92%   [EVAL] batch:  101 | acc: 100.00%,  total acc: 77.14%   [EVAL] batch:  102 | acc: 100.00%,  total acc: 77.37%   [EVAL] batch:  103 | acc: 100.00%,  total acc: 77.58%   [EVAL] batch:  104 | acc: 100.00%,  total acc: 77.80%   [EVAL] batch:  105 | acc: 93.75%,  total acc: 77.95%   [EVAL] batch:  106 | acc: 100.00%,  total acc: 78.15%   [EVAL] batch:  107 | acc: 100.00%,  total acc: 78.36%   [EVAL] batch:  108 | acc: 100.00%,  total acc: 78.56%   [EVAL] batch:  109 | acc: 100.00%,  total acc: 78.75%   [EVAL] batch:  110 | acc: 100.00%,  total acc: 78.94%   [EVAL] batch:  111 | acc: 93.75%,  total acc: 79.07%   [EVAL] batch:  112 | acc: 100.00%,  total acc: 79.26%   [EVAL] batch:  113 | acc: 87.50%,  total acc: 79.33%   [EVAL] batch:  114 | acc: 93.75%,  total acc: 79.46%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 79.63%   [EVAL] batch:  116 | acc: 87.50%,  total acc: 79.70%   [EVAL] batch:  117 | acc: 93.75%,  total acc: 79.82%   [EVAL] batch:  118 | acc: 93.75%,  total acc: 79.94%   [EVAL] batch:  119 | acc: 93.75%,  total acc: 80.05%   [EVAL] batch:  120 | acc: 87.50%,  total acc: 80.11%   [EVAL] batch:  121 | acc: 93.75%,  total acc: 80.23%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 80.39%   [EVAL] batch:  123 | acc: 87.50%,  total acc: 80.44%   [EVAL] batch:  124 | acc: 93.75%,  total acc: 80.55%   [EVAL] batch:  125 | acc: 0.00%,  total acc: 79.91%   [EVAL] batch:  126 | acc: 6.25%,  total acc: 79.33%   [EVAL] batch:  127 | acc: 0.00%,  total acc: 78.71%   [EVAL] batch:  128 | acc: 0.00%,  total acc: 78.10%   [EVAL] batch:  129 | acc: 0.00%,  total acc: 77.50%   [EVAL] batch:  130 | acc: 6.25%,  total acc: 76.96%   [EVAL] batch:  131 | acc: 62.50%,  total acc: 76.85%   [EVAL] batch:  132 | acc: 81.25%,  total acc: 76.88%   [EVAL] batch:  133 | acc: 100.00%,  total acc: 77.05%   [EVAL] batch:  134 | acc: 93.75%,  total acc: 77.18%   [EVAL] batch:  135 | acc: 93.75%,  total acc: 77.30%   [EVAL] batch:  136 | acc: 81.25%,  total acc: 77.33%   [EVAL] batch:  137 | acc: 81.25%,  total acc: 77.36%   [EVAL] batch:  138 | acc: 50.00%,  total acc: 77.16%   [EVAL] batch:  139 | acc: 56.25%,  total acc: 77.01%   [EVAL] batch:  140 | acc: 75.00%,  total acc: 76.99%   [EVAL] batch:  141 | acc: 50.00%,  total acc: 76.80%   [EVAL] batch:  142 | acc: 56.25%,  total acc: 76.66%   [EVAL] batch:  143 | acc: 31.25%,  total acc: 76.35%   [EVAL] batch:  144 | acc: 25.00%,  total acc: 75.99%   [EVAL] batch:  145 | acc: 12.50%,  total acc: 75.56%   [EVAL] batch:  146 | acc: 56.25%,  total acc: 75.43%   [EVAL] batch:  147 | acc: 43.75%,  total acc: 75.21%   [EVAL] batch:  148 | acc: 31.25%,  total acc: 74.92%   [EVAL] batch:  149 | acc: 43.75%,  total acc: 74.71%   [EVAL] batch:  150 | acc: 6.25%,  total acc: 74.25%   [EVAL] batch:  151 | acc: 18.75%,  total acc: 73.89%   [EVAL] batch:  152 | acc: 6.25%,  total acc: 73.45%   [EVAL] batch:  153 | acc: 6.25%,  total acc: 73.01%   [EVAL] batch:  154 | acc: 18.75%,  total acc: 72.66%   [EVAL] batch:  155 | acc: 0.00%,  total acc: 72.20%   [EVAL] batch:  156 | acc: 68.75%,  total acc: 72.17%   [EVAL] batch:  157 | acc: 87.50%,  total acc: 72.27%   [EVAL] batch:  158 | acc: 81.25%,  total acc: 72.33%   [EVAL] batch:  159 | acc: 100.00%,  total acc: 72.50%   [EVAL] batch:  160 | acc: 93.75%,  total acc: 72.63%   [EVAL] batch:  161 | acc: 75.00%,  total acc: 72.65%   [EVAL] batch:  162 | acc: 87.50%,  total acc: 72.74%   [EVAL] batch:  163 | acc: 87.50%,  total acc: 72.83%   [EVAL] batch:  164 | acc: 93.75%,  total acc: 72.95%   [EVAL] batch:  165 | acc: 87.50%,  total acc: 73.04%   [EVAL] batch:  166 | acc: 93.75%,  total acc: 73.17%   [EVAL] batch:  167 | acc: 93.75%,  total acc: 73.29%   [EVAL] batch:  168 | acc: 87.50%,  total acc: 73.37%   [EVAL] batch:  169 | acc: 25.00%,  total acc: 73.09%   [EVAL] batch:  170 | acc: 18.75%,  total acc: 72.77%   [EVAL] batch:  171 | acc: 18.75%,  total acc: 72.46%   [EVAL] batch:  172 | acc: 18.75%,  total acc: 72.15%   [EVAL] batch:  173 | acc: 37.50%,  total acc: 71.95%   [EVAL] batch:  174 | acc: 12.50%,  total acc: 71.61%   [EVAL] batch:  175 | acc: 25.00%,  total acc: 71.34%   [EVAL] batch:  176 | acc: 50.00%,  total acc: 71.22%   [EVAL] batch:  177 | acc: 37.50%,  total acc: 71.03%   [EVAL] batch:  178 | acc: 56.25%,  total acc: 70.95%   [EVAL] batch:  179 | acc: 43.75%,  total acc: 70.80%   [EVAL] batch:  180 | acc: 56.25%,  total acc: 70.72%   [EVAL] batch:  181 | acc: 37.50%,  total acc: 70.54%   [EVAL] batch:  182 | acc: 43.75%,  total acc: 70.39%   [EVAL] batch:  183 | acc: 43.75%,  total acc: 70.24%   [EVAL] batch:  184 | acc: 62.50%,  total acc: 70.20%   [EVAL] batch:  185 | acc: 50.00%,  total acc: 70.09%   [EVAL] batch:  186 | acc: 75.00%,  total acc: 70.12%   [EVAL] batch:  187 | acc: 31.25%,  total acc: 69.91%   [EVAL] batch:  188 | acc: 31.25%,  total acc: 69.71%   [EVAL] batch:  189 | acc: 25.00%,  total acc: 69.47%   [EVAL] batch:  190 | acc: 56.25%,  total acc: 69.40%   [EVAL] batch:  191 | acc: 37.50%,  total acc: 69.24%   [EVAL] batch:  192 | acc: 50.00%,  total acc: 69.14%   [EVAL] batch:  193 | acc: 31.25%,  total acc: 68.94%   [EVAL] batch:  194 | acc: 75.00%,  total acc: 68.97%   [EVAL] batch:  195 | acc: 62.50%,  total acc: 68.94%   [EVAL] batch:  196 | acc: 75.00%,  total acc: 68.97%   [EVAL] batch:  197 | acc: 62.50%,  total acc: 68.94%   [EVAL] batch:  198 | acc: 37.50%,  total acc: 68.78%   [EVAL] batch:  199 | acc: 100.00%,  total acc: 68.94%   [EVAL] batch:  200 | acc: 37.50%,  total acc: 68.78%   [EVAL] batch:  201 | acc: 25.00%,  total acc: 68.56%   [EVAL] batch:  202 | acc: 43.75%,  total acc: 68.44%   [EVAL] batch:  203 | acc: 37.50%,  total acc: 68.29%   [EVAL] batch:  204 | acc: 37.50%,  total acc: 68.14%   [EVAL] batch:  205 | acc: 75.00%,  total acc: 68.17%   [EVAL] batch:  206 | acc: 68.75%,  total acc: 68.18%   [EVAL] batch:  207 | acc: 93.75%,  total acc: 68.30%   [EVAL] batch:  208 | acc: 100.00%,  total acc: 68.45%   [EVAL] batch:  209 | acc: 93.75%,  total acc: 68.57%   [EVAL] batch:  210 | acc: 93.75%,  total acc: 68.69%   [EVAL] batch:  211 | acc: 87.50%,  total acc: 68.78%   [EVAL] batch:  212 | acc: 87.50%,  total acc: 68.87%   [EVAL] batch:  213 | acc: 68.75%,  total acc: 68.87%   [EVAL] batch:  214 | acc: 62.50%,  total acc: 68.84%   [EVAL] batch:  215 | acc: 81.25%,  total acc: 68.89%   [EVAL] batch:  216 | acc: 87.50%,  total acc: 68.98%   [EVAL] batch:  217 | acc: 75.00%,  total acc: 69.01%   [EVAL] batch:  218 | acc: 62.50%,  total acc: 68.98%   [EVAL] batch:  219 | acc: 75.00%,  total acc: 69.01%   [EVAL] batch:  220 | acc: 25.00%,  total acc: 68.81%   [EVAL] batch:  221 | acc: 31.25%,  total acc: 68.64%   [EVAL] batch:  222 | acc: 25.00%,  total acc: 68.44%   [EVAL] batch:  223 | acc: 6.25%,  total acc: 68.16%   [EVAL] batch:  224 | acc: 43.75%,  total acc: 68.06%   [EVAL] batch:  225 | acc: 75.00%,  total acc: 68.09%   [EVAL] batch:  226 | acc: 81.25%,  total acc: 68.14%   [EVAL] batch:  227 | acc: 62.50%,  total acc: 68.12%   [EVAL] batch:  228 | acc: 93.75%,  total acc: 68.23%   [EVAL] batch:  229 | acc: 68.75%,  total acc: 68.23%   [EVAL] batch:  230 | acc: 81.25%,  total acc: 68.29%   [EVAL] batch:  231 | acc: 87.50%,  total acc: 68.37%   [EVAL] batch:  232 | acc: 56.25%,  total acc: 68.32%   [EVAL] batch:  233 | acc: 68.75%,  total acc: 68.32%   [EVAL] batch:  234 | acc: 37.50%,  total acc: 68.19%   [EVAL] batch:  235 | acc: 50.00%,  total acc: 68.11%   [EVAL] batch:  236 | acc: 62.50%,  total acc: 68.09%   [EVAL] batch:  237 | acc: 93.75%,  total acc: 68.20%   [EVAL] batch:  238 | acc: 100.00%,  total acc: 68.33%   [EVAL] batch:  239 | acc: 100.00%,  total acc: 68.46%   [EVAL] batch:  240 | acc: 100.00%,  total acc: 68.59%   [EVAL] batch:  241 | acc: 100.00%,  total acc: 68.72%   [EVAL] batch:  242 | acc: 100.00%,  total acc: 68.85%   [EVAL] batch:  243 | acc: 100.00%,  total acc: 68.98%   [EVAL] batch:  244 | acc: 93.75%,  total acc: 69.08%   [EVAL] batch:  245 | acc: 81.25%,  total acc: 69.13%   [EVAL] batch:  246 | acc: 87.50%,  total acc: 69.21%   [EVAL] batch:  247 | acc: 81.25%,  total acc: 69.25%   [EVAL] batch:  248 | acc: 100.00%,  total acc: 69.38%   [EVAL] batch:  249 | acc: 100.00%,  total acc: 69.50%   [EVAL] batch:  250 | acc: 31.25%,  total acc: 69.35%   [EVAL] batch:  251 | acc: 37.50%,  total acc: 69.22%   [EVAL] batch:  252 | acc: 31.25%,  total acc: 69.07%   [EVAL] batch:  253 | acc: 31.25%,  total acc: 68.92%   [EVAL] batch:  254 | acc: 43.75%,  total acc: 68.82%   [EVAL] batch:  255 | acc: 43.75%,  total acc: 68.73%   [EVAL] batch:  256 | acc: 56.25%,  total acc: 68.68%   [EVAL] batch:  257 | acc: 75.00%,  total acc: 68.70%   [EVAL] batch:  258 | acc: 75.00%,  total acc: 68.73%   [EVAL] batch:  259 | acc: 68.75%,  total acc: 68.73%   [EVAL] batch:  260 | acc: 68.75%,  total acc: 68.73%   [EVAL] batch:  261 | acc: 56.25%,  total acc: 68.68%   [EVAL] batch:  262 | acc: 100.00%,  total acc: 68.80%   [EVAL] batch:  263 | acc: 81.25%,  total acc: 68.84%   [EVAL] batch:  264 | acc: 81.25%,  total acc: 68.89%   [EVAL] batch:  265 | acc: 87.50%,  total acc: 68.96%   [EVAL] batch:  266 | acc: 93.75%,  total acc: 69.05%   [EVAL] batch:  267 | acc: 100.00%,  total acc: 69.17%   [EVAL] batch:  268 | acc: 68.75%,  total acc: 69.17%   [EVAL] batch:  269 | acc: 0.00%,  total acc: 68.91%   [EVAL] batch:  270 | acc: 6.25%,  total acc: 68.68%   [EVAL] batch:  271 | acc: 6.25%,  total acc: 68.45%   [EVAL] batch:  272 | acc: 12.50%,  total acc: 68.25%   [EVAL] batch:  273 | acc: 6.25%,  total acc: 68.02%   [EVAL] batch:  274 | acc: 6.25%,  total acc: 67.80%   [EVAL] batch:  275 | acc: 87.50%,  total acc: 67.87%   [EVAL] batch:  276 | acc: 93.75%,  total acc: 67.96%   [EVAL] batch:  277 | acc: 93.75%,  total acc: 68.05%   [EVAL] batch:  278 | acc: 81.25%,  total acc: 68.10%   [EVAL] batch:  279 | acc: 81.25%,  total acc: 68.15%   [EVAL] batch:  280 | acc: 81.25%,  total acc: 68.19%   [EVAL] batch:  281 | acc: 100.00%,  total acc: 68.31%   [EVAL] batch:  282 | acc: 100.00%,  total acc: 68.42%   [EVAL] batch:  283 | acc: 100.00%,  total acc: 68.53%   [EVAL] batch:  284 | acc: 100.00%,  total acc: 68.64%   [EVAL] batch:  285 | acc: 100.00%,  total acc: 68.75%   [EVAL] batch:  286 | acc: 100.00%,  total acc: 68.86%   [EVAL] batch:  287 | acc: 100.00%,  total acc: 68.97%   [EVAL] batch:  288 | acc: 93.75%,  total acc: 69.05%   [EVAL] batch:  289 | acc: 75.00%,  total acc: 69.07%   [EVAL] batch:  290 | acc: 81.25%,  total acc: 69.12%   [EVAL] batch:  291 | acc: 93.75%,  total acc: 69.20%   [EVAL] batch:  292 | acc: 93.75%,  total acc: 69.28%   [EVAL] batch:  293 | acc: 81.25%,  total acc: 69.32%   [EVAL] batch:  294 | acc: 31.25%,  total acc: 69.19%   [EVAL] batch:  295 | acc: 18.75%,  total acc: 69.02%   [EVAL] batch:  296 | acc: 18.75%,  total acc: 68.86%   [EVAL] batch:  297 | acc: 31.25%,  total acc: 68.73%   [EVAL] batch:  298 | acc: 50.00%,  total acc: 68.67%   [EVAL] batch:  299 | acc: 25.00%,  total acc: 68.52%   [EVAL] batch:  300 | acc: 93.75%,  total acc: 68.60%   [EVAL] batch:  301 | acc: 93.75%,  total acc: 68.69%   [EVAL] batch:  302 | acc: 93.75%,  total acc: 68.77%   [EVAL] batch:  303 | acc: 81.25%,  total acc: 68.81%   [EVAL] batch:  304 | acc: 93.75%,  total acc: 68.89%   [EVAL] batch:  305 | acc: 93.75%,  total acc: 68.97%   [EVAL] batch:  306 | acc: 31.25%,  total acc: 68.85%   [EVAL] batch:  307 | acc: 18.75%,  total acc: 68.69%   [EVAL] batch:  308 | acc: 18.75%,  total acc: 68.53%   [EVAL] batch:  309 | acc: 6.25%,  total acc: 68.33%   [EVAL] batch:  310 | acc: 18.75%,  total acc: 68.17%   [EVAL] batch:  311 | acc: 0.00%,  total acc: 67.95%   [EVAL] batch:  312 | acc: 12.50%,  total acc: 67.77%   [EVAL] batch:  313 | acc: 6.25%,  total acc: 67.58%   [EVAL] batch:  314 | acc: 18.75%,  total acc: 67.42%   [EVAL] batch:  315 | acc: 0.00%,  total acc: 67.21%   [EVAL] batch:  316 | acc: 12.50%,  total acc: 67.03%   [EVAL] batch:  317 | acc: 6.25%,  total acc: 66.84%   [EVAL] batch:  318 | acc: 25.00%,  total acc: 66.71%   [EVAL] batch:  319 | acc: 75.00%,  total acc: 66.74%   [EVAL] batch:  320 | acc: 87.50%,  total acc: 66.80%   [EVAL] batch:  321 | acc: 81.25%,  total acc: 66.85%   [EVAL] batch:  322 | acc: 81.25%,  total acc: 66.89%   [EVAL] batch:  323 | acc: 81.25%,  total acc: 66.94%   [EVAL] batch:  324 | acc: 87.50%,  total acc: 67.00%   [EVAL] batch:  325 | acc: 56.25%,  total acc: 66.97%   [EVAL] batch:  326 | acc: 50.00%,  total acc: 66.92%   [EVAL] batch:  327 | acc: 56.25%,  total acc: 66.88%   [EVAL] batch:  328 | acc: 68.75%,  total acc: 66.89%   [EVAL] batch:  329 | acc: 75.00%,  total acc: 66.91%   [EVAL] batch:  330 | acc: 43.75%,  total acc: 66.84%   [EVAL] batch:  331 | acc: 93.75%,  total acc: 66.92%   [EVAL] batch:  332 | acc: 100.00%,  total acc: 67.02%   [EVAL] batch:  333 | acc: 100.00%,  total acc: 67.12%   [EVAL] batch:  334 | acc: 93.75%,  total acc: 67.20%   [EVAL] batch:  335 | acc: 93.75%,  total acc: 67.28%   [EVAL] batch:  336 | acc: 100.00%,  total acc: 67.38%   [EVAL] batch:  337 | acc: 93.75%,  total acc: 67.46%   [EVAL] batch:  338 | acc: 93.75%,  total acc: 67.53%   [EVAL] batch:  339 | acc: 87.50%,  total acc: 67.59%   [EVAL] batch:  340 | acc: 100.00%,  total acc: 67.69%   [EVAL] batch:  341 | acc: 87.50%,  total acc: 67.74%   [EVAL] batch:  342 | acc: 100.00%,  total acc: 67.84%   [EVAL] batch:  343 | acc: 87.50%,  total acc: 67.90%   [EVAL] batch:  344 | acc: 93.75%,  total acc: 67.97%   [EVAL] batch:  345 | acc: 100.00%,  total acc: 68.06%   [EVAL] batch:  346 | acc: 87.50%,  total acc: 68.12%   [EVAL] batch:  347 | acc: 87.50%,  total acc: 68.18%   [EVAL] batch:  348 | acc: 93.75%,  total acc: 68.25%   [EVAL] batch:  349 | acc: 100.00%,  total acc: 68.34%   [EVAL] batch:  350 | acc: 18.75%,  total acc: 68.20%   [EVAL] batch:  351 | acc: 18.75%,  total acc: 68.06%   [EVAL] batch:  352 | acc: 56.25%,  total acc: 68.02%   [EVAL] batch:  353 | acc: 25.00%,  total acc: 67.90%   [EVAL] batch:  354 | acc: 18.75%,  total acc: 67.76%   [EVAL] batch:  355 | acc: 25.00%,  total acc: 67.64%   [EVAL] batch:  356 | acc: 87.50%,  total acc: 67.70%   [EVAL] batch:  357 | acc: 93.75%,  total acc: 67.77%   [EVAL] batch:  358 | acc: 87.50%,  total acc: 67.83%   [EVAL] batch:  359 | acc: 93.75%,  total acc: 67.90%   [EVAL] batch:  360 | acc: 87.50%,  total acc: 67.95%   [EVAL] batch:  361 | acc: 81.25%,  total acc: 67.99%   [EVAL] batch:  362 | acc: 75.00%,  total acc: 68.01%   [EVAL] batch:  363 | acc: 68.75%,  total acc: 68.01%   [EVAL] batch:  364 | acc: 50.00%,  total acc: 67.96%   [EVAL] batch:  365 | acc: 56.25%,  total acc: 67.93%   [EVAL] batch:  366 | acc: 62.50%,  total acc: 67.92%   [EVAL] batch:  367 | acc: 50.00%,  total acc: 67.87%   [EVAL] batch:  368 | acc: 31.25%,  total acc: 67.77%   [EVAL] batch:  369 | acc: 81.25%,  total acc: 67.80%   [EVAL] batch:  370 | acc: 62.50%,  total acc: 67.79%   [EVAL] batch:  371 | acc: 43.75%,  total acc: 67.73%   [EVAL] batch:  372 | acc: 75.00%,  total acc: 67.74%   [EVAL] batch:  373 | acc: 87.50%,  total acc: 67.80%   [EVAL] batch:  374 | acc: 62.50%,  total acc: 67.78%   [EVAL] batch:  375 | acc: 37.50%,  total acc: 67.70%   [EVAL] batch:  376 | acc: 25.00%,  total acc: 67.59%   [EVAL] batch:  377 | acc: 31.25%,  total acc: 67.49%   [EVAL] batch:  378 | acc: 25.00%,  total acc: 67.38%   [EVAL] batch:  379 | acc: 56.25%,  total acc: 67.35%   [EVAL] batch:  380 | acc: 31.25%,  total acc: 67.26%   [EVAL] batch:  381 | acc: 87.50%,  total acc: 67.31%   [EVAL] batch:  382 | acc: 100.00%,  total acc: 67.40%   [EVAL] batch:  383 | acc: 87.50%,  total acc: 67.45%   [EVAL] batch:  384 | acc: 75.00%,  total acc: 67.47%   [EVAL] batch:  385 | acc: 87.50%,  total acc: 67.52%   [EVAL] batch:  386 | acc: 100.00%,  total acc: 67.60%   [EVAL] batch:  387 | acc: 75.00%,  total acc: 67.62%   [EVAL] batch:  388 | acc: 50.00%,  total acc: 67.58%   [EVAL] batch:  389 | acc: 56.25%,  total acc: 67.55%   [EVAL] batch:  390 | acc: 62.50%,  total acc: 67.54%   [EVAL] batch:  391 | acc: 75.00%,  total acc: 67.55%   [EVAL] batch:  392 | acc: 68.75%,  total acc: 67.56%   [EVAL] batch:  393 | acc: 81.25%,  total acc: 67.59%   [EVAL] batch:  394 | acc: 25.00%,  total acc: 67.48%   [EVAL] batch:  395 | acc: 37.50%,  total acc: 67.41%   [EVAL] batch:  396 | acc: 25.00%,  total acc: 67.30%   [EVAL] batch:  397 | acc: 12.50%,  total acc: 67.16%   [EVAL] batch:  398 | acc: 50.00%,  total acc: 67.12%   [EVAL] batch:  399 | acc: 31.25%,  total acc: 67.03%   [EVAL] batch:  400 | acc: 25.00%,  total acc: 66.93%   [EVAL] batch:  401 | acc: 43.75%,  total acc: 66.87%   [EVAL] batch:  402 | acc: 25.00%,  total acc: 66.76%   [EVAL] batch:  403 | acc: 31.25%,  total acc: 66.68%   [EVAL] batch:  404 | acc: 31.25%,  total acc: 66.59%   [EVAL] batch:  405 | acc: 18.75%,  total acc: 66.47%   [EVAL] batch:  406 | acc: 56.25%,  total acc: 66.45%   [EVAL] batch:  407 | acc: 62.50%,  total acc: 66.44%   [EVAL] batch:  408 | acc: 68.75%,  total acc: 66.44%   [EVAL] batch:  409 | acc: 68.75%,  total acc: 66.45%   [EVAL] batch:  410 | acc: 75.00%,  total acc: 66.47%   [EVAL] batch:  411 | acc: 62.50%,  total acc: 66.46%   [EVAL] batch:  412 | acc: 75.00%,  total acc: 66.48%   [EVAL] batch:  413 | acc: 68.75%,  total acc: 66.49%   [EVAL] batch:  414 | acc: 68.75%,  total acc: 66.49%   [EVAL] batch:  415 | acc: 68.75%,  total acc: 66.50%   [EVAL] batch:  416 | acc: 75.00%,  total acc: 66.52%   [EVAL] batch:  417 | acc: 75.00%,  total acc: 66.54%   [EVAL] batch:  418 | acc: 68.75%,  total acc: 66.54%   [EVAL] batch:  419 | acc: 37.50%,  total acc: 66.47%   [EVAL] batch:  420 | acc: 31.25%,  total acc: 66.39%   [EVAL] batch:  421 | acc: 25.00%,  total acc: 66.29%   [EVAL] batch:  422 | acc: 37.50%,  total acc: 66.22%   [EVAL] batch:  423 | acc: 25.00%,  total acc: 66.13%   [EVAL] batch:  424 | acc: 31.25%,  total acc: 66.04%   [EVAL] batch:  425 | acc: 100.00%,  total acc: 66.12%   [EVAL] batch:  426 | acc: 100.00%,  total acc: 66.20%   [EVAL] batch:  427 | acc: 100.00%,  total acc: 66.28%   [EVAL] batch:  428 | acc: 100.00%,  total acc: 66.36%   [EVAL] batch:  429 | acc: 100.00%,  total acc: 66.44%   [EVAL] batch:  430 | acc: 100.00%,  total acc: 66.52%   [EVAL] batch:  431 | acc: 81.25%,  total acc: 66.55%   [EVAL] batch:  432 | acc: 81.25%,  total acc: 66.58%   [EVAL] batch:  433 | acc: 87.50%,  total acc: 66.63%   [EVAL] batch:  434 | acc: 87.50%,  total acc: 66.68%   [EVAL] batch:  435 | acc: 87.50%,  total acc: 66.73%   [EVAL] batch:  436 | acc: 87.50%,  total acc: 66.78%   [EVAL] batch:  437 | acc: 81.25%,  total acc: 66.81%   [EVAL] batch:  438 | acc: 50.00%,  total acc: 66.77%   [EVAL] batch:  439 | acc: 56.25%,  total acc: 66.75%   [EVAL] batch:  440 | acc: 68.75%,  total acc: 66.75%   [EVAL] batch:  441 | acc: 62.50%,  total acc: 66.74%   [EVAL] batch:  442 | acc: 81.25%,  total acc: 66.77%   [EVAL] batch:  443 | acc: 62.50%,  total acc: 66.77%   [EVAL] batch:  444 | acc: 93.75%,  total acc: 66.83%   [EVAL] batch:  445 | acc: 75.00%,  total acc: 66.84%   [EVAL] batch:  446 | acc: 75.00%,  total acc: 66.86%   [EVAL] batch:  447 | acc: 87.50%,  total acc: 66.91%   [EVAL] batch:  448 | acc: 87.50%,  total acc: 66.95%   [EVAL] batch:  449 | acc: 81.25%,  total acc: 66.99%   [EVAL] batch:  450 | acc: 18.75%,  total acc: 66.88%   [EVAL] batch:  451 | acc: 18.75%,  total acc: 66.77%   [EVAL] batch:  452 | acc: 31.25%,  total acc: 66.69%   [EVAL] batch:  453 | acc: 37.50%,  total acc: 66.63%   [EVAL] batch:  454 | acc: 56.25%,  total acc: 66.61%   [EVAL] batch:  455 | acc: 25.00%,  total acc: 66.52%   [EVAL] batch:  456 | acc: 75.00%,  total acc: 66.53%   [EVAL] batch:  457 | acc: 100.00%,  total acc: 66.61%   [EVAL] batch:  458 | acc: 100.00%,  total acc: 66.68%   [EVAL] batch:  459 | acc: 100.00%,  total acc: 66.75%   [EVAL] batch:  460 | acc: 100.00%,  total acc: 66.82%   [EVAL] batch:  461 | acc: 93.75%,  total acc: 66.88%   [EVAL] batch:  462 | acc: 56.25%,  total acc: 66.86%   [EVAL] batch:  463 | acc: 0.00%,  total acc: 66.72%   [EVAL] batch:  464 | acc: 0.00%,  total acc: 66.57%   [EVAL] batch:  465 | acc: 6.25%,  total acc: 66.44%   [EVAL] batch:  466 | acc: 18.75%,  total acc: 66.34%   [EVAL] batch:  467 | acc: 12.50%,  total acc: 66.23%   [EVAL] batch:  468 | acc: 25.00%,  total acc: 66.14%   [EVAL] batch:  469 | acc: 93.75%,  total acc: 66.20%   [EVAL] batch:  470 | acc: 93.75%,  total acc: 66.26%   [EVAL] batch:  471 | acc: 81.25%,  total acc: 66.29%   [EVAL] batch:  472 | acc: 87.50%,  total acc: 66.33%   [EVAL] batch:  473 | acc: 100.00%,  total acc: 66.40%   [EVAL] batch:  474 | acc: 100.00%,  total acc: 66.47%   [EVAL] batch:  475 | acc: 56.25%,  total acc: 66.45%   [EVAL] batch:  476 | acc: 62.50%,  total acc: 66.44%   [EVAL] batch:  477 | acc: 50.00%,  total acc: 66.41%   [EVAL] batch:  478 | acc: 37.50%,  total acc: 66.35%   [EVAL] batch:  479 | acc: 68.75%,  total acc: 66.35%   [EVAL] batch:  480 | acc: 62.50%,  total acc: 66.35%   [EVAL] batch:  481 | acc: 68.75%,  total acc: 66.35%   [EVAL] batch:  482 | acc: 56.25%,  total acc: 66.33%   [EVAL] batch:  483 | acc: 87.50%,  total acc: 66.37%   [EVAL] batch:  484 | acc: 81.25%,  total acc: 66.40%   [EVAL] batch:  485 | acc: 56.25%,  total acc: 66.38%   [EVAL] batch:  486 | acc: 87.50%,  total acc: 66.43%   [EVAL] batch:  487 | acc: 75.00%,  total acc: 66.44%   [EVAL] batch:  488 | acc: 43.75%,  total acc: 66.40%   [EVAL] batch:  489 | acc: 68.75%,  total acc: 66.40%   [EVAL] batch:  490 | acc: 43.75%,  total acc: 66.36%   [EVAL] batch:  491 | acc: 68.75%,  total acc: 66.36%   [EVAL] batch:  492 | acc: 50.00%,  total acc: 66.33%   [EVAL] batch:  493 | acc: 75.00%,  total acc: 66.35%   [EVAL] batch:  494 | acc: 93.75%,  total acc: 66.40%   [EVAL] batch:  495 | acc: 93.75%,  total acc: 66.46%   [EVAL] batch:  496 | acc: 81.25%,  total acc: 66.49%   [EVAL] batch:  497 | acc: 87.50%,  total acc: 66.53%   [EVAL] batch:  498 | acc: 81.25%,  total acc: 66.56%   [EVAL] batch:  499 | acc: 93.75%,  total acc: 66.61%   
cur_acc:  ['0.9484', '0.8710', '0.7014', '0.7480', '0.7331', '0.6905', '0.6518', '0.6478']
his_acc:  ['0.9484', '0.9020', '0.8162', '0.7650', '0.7406', '0.7122', '0.6898', '0.6661']
--------Round  5
seed:  600
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/train.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/valid.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/test.pkl
Task_order: [7 2 0 1 6 3 4 5]
prepared data!
CurrentTrain: epoch 15, batch     0 | loss: 26.3496968CurrentTrain: epoch 15, batch     1 | loss: 30.6368098CurrentTrain: epoch 15, batch     2 | loss: 22.7189407CurrentTrain: epoch 15, batch     3 | loss: 29.9399324CurrentTrain: epoch 15, batch     4 | loss: 24.1708838CurrentTrain: epoch 15, batch     5 | loss: 26.7507517CurrentTrain: epoch 15, batch     6 | loss: 23.6167043CurrentTrain: epoch 15, batch     7 | loss: 30.0590181CurrentTrain: epoch 15, batch     8 | loss: 23.1169995CurrentTrain: epoch 15, batch     9 | loss: 20.4710053CurrentTrain: epoch 15, batch    10 | loss: 31.5175567CurrentTrain: epoch 15, batch    11 | loss: 22.6744452CurrentTrain: epoch 15, batch    12 | loss: 21.3237923CurrentTrain: epoch 15, batch    13 | loss: 22.9797925CurrentTrain: epoch 15, batch    14 | loss: 26.1821956CurrentTrain: epoch 15, batch    15 | loss: 23.3548825CurrentTrain: epoch 15, batch    16 | loss: 24.0946199CurrentTrain: epoch 15, batch    17 | loss: 20.3295909CurrentTrain: epoch 15, batch    18 | loss: 16.4045470CurrentTrain: epoch 15, batch    19 | loss: 37.2552919CurrentTrain: epoch 15, batch    20 | loss: 19.9624720CurrentTrain: epoch 15, batch    21 | loss: 23.9407585CurrentTrain: epoch 15, batch    22 | loss: 22.2847683CurrentTrain: epoch 15, batch    23 | loss: 18.9119438CurrentTrain: epoch 15, batch    24 | loss: 18.0194499CurrentTrain: epoch 15, batch    25 | loss: 19.4094462CurrentTrain: epoch 15, batch    26 | loss: 17.3907309CurrentTrain: epoch 15, batch    27 | loss: 15.6995743CurrentTrain: epoch 15, batch    28 | loss: 17.2823998CurrentTrain: epoch 15, batch    29 | loss: 27.6887660CurrentTrain: epoch 15, batch    30 | loss: 23.1147399CurrentTrain: epoch 15, batch    31 | loss: 17.9433758CurrentTrain: epoch 15, batch    32 | loss: 18.8042516CurrentTrain: epoch 15, batch    33 | loss: 36.5545733CurrentTrain: epoch 15, batch    34 | loss: 21.2072143CurrentTrain: epoch 15, batch    35 | loss: 21.7572137CurrentTrain: epoch 15, batch    36 | loss: 20.9019364CurrentTrain: epoch 15, batch    37 | loss: 29.8177755CurrentTrain: epoch 15, batch    38 | loss: 17.8660142CurrentTrain: epoch 15, batch    39 | loss: 16.4897896CurrentTrain: epoch 15, batch    40 | loss: 22.7485693CurrentTrain: epoch 15, batch    41 | loss: 22.6211257CurrentTrain: epoch 15, batch    42 | loss: 15.9655949CurrentTrain: epoch 15, batch    43 | loss: 15.7095362CurrentTrain: epoch 15, batch    44 | loss: 19.4601108CurrentTrain: epoch 15, batch    45 | loss: 21.2146632CurrentTrain: epoch 15, batch    46 | loss: 16.7396199CurrentTrain: epoch 15, batch    47 | loss: 20.7701512CurrentTrain: epoch 15, batch    48 | loss: 23.6432839CurrentTrain: epoch 15, batch    49 | loss: 14.8496121CurrentTrain: epoch 15, batch    50 | loss: 23.9924967CurrentTrain: epoch 15, batch    51 | loss: 38.7874065CurrentTrain: epoch 15, batch    52 | loss: 16.1371999CurrentTrain: epoch 15, batch    53 | loss: 22.1654763CurrentTrain: epoch 15, batch    54 | loss: 16.7684230CurrentTrain: epoch 15, batch    55 | loss: 17.8538259CurrentTrain: epoch 15, batch    56 | loss: 20.5560423CurrentTrain: epoch 15, batch    57 | loss: 26.1666789CurrentTrain: epoch 15, batch    58 | loss: 17.8261569CurrentTrain: epoch 15, batch    59 | loss: 12.9951915CurrentTrain: epoch 15, batch    60 | loss: 18.8476839CurrentTrain: epoch 15, batch    61 | loss: 15.4761506CurrentTrain: epoch  7, batch    62 | loss: 12.7657067CurrentTrain: epoch 15, batch     0 | loss: 15.4415412CurrentTrain: epoch 15, batch     1 | loss: 14.9112498CurrentTrain: epoch 15, batch     2 | loss: 15.4386949CurrentTrain: epoch 15, batch     3 | loss: 18.6318792CurrentTrain: epoch 15, batch     4 | loss: 19.2637154CurrentTrain: epoch 15, batch     5 | loss: 15.9234780CurrentTrain: epoch 15, batch     6 | loss: 20.0118084CurrentTrain: epoch 15, batch     7 | loss: 22.0630581CurrentTrain: epoch 15, batch     8 | loss: 14.6335751CurrentTrain: epoch 15, batch     9 | loss: 26.0855315CurrentTrain: epoch 15, batch    10 | loss: 14.3181671CurrentTrain: epoch 15, batch    11 | loss: 12.3437546CurrentTrain: epoch 15, batch    12 | loss: 13.8375335CurrentTrain: epoch 15, batch    13 | loss: 13.8797172CurrentTrain: epoch 15, batch    14 | loss: 37.7988364CurrentTrain: epoch 15, batch    15 | loss: 21.8035105CurrentTrain: epoch 15, batch    16 | loss: 17.5342979CurrentTrain: epoch 15, batch    17 | loss: 22.9839824CurrentTrain: epoch 15, batch    18 | loss: 17.4140268CurrentTrain: epoch 15, batch    19 | loss: 17.3346355CurrentTrain: epoch 15, batch    20 | loss: 13.1146157CurrentTrain: epoch 15, batch    21 | loss: 18.9580380CurrentTrain: epoch 15, batch    22 | loss: 15.4637686CurrentTrain: epoch 15, batch    23 | loss: 19.6717884CurrentTrain: epoch 15, batch    24 | loss: 14.8244528CurrentTrain: epoch 15, batch    25 | loss: 15.6173495CurrentTrain: epoch 15, batch    26 | loss: 19.2265621CurrentTrain: epoch 15, batch    27 | loss: 17.3192876CurrentTrain: epoch 15, batch    28 | loss: 15.8988985CurrentTrain: epoch 15, batch    29 | loss: 24.5136689CurrentTrain: epoch 15, batch    30 | loss: 19.7858485CurrentTrain: epoch 15, batch    31 | loss: 14.9695799CurrentTrain: epoch 15, batch    32 | loss: 29.7905588CurrentTrain: epoch 15, batch    33 | loss: 20.8626974CurrentTrain: epoch 15, batch    34 | loss: 21.9617577CurrentTrain: epoch 15, batch    35 | loss: 12.2637137CurrentTrain: epoch 15, batch    36 | loss: 16.6746257CurrentTrain: epoch 15, batch    37 | loss: 13.1145688CurrentTrain: epoch 15, batch    38 | loss: 24.2062288CurrentTrain: epoch 15, batch    39 | loss: 11.5775326CurrentTrain: epoch 15, batch    40 | loss: 26.3116237CurrentTrain: epoch 15, batch    41 | loss: 20.2682085CurrentTrain: epoch 15, batch    42 | loss: 19.4199580CurrentTrain: epoch 15, batch    43 | loss: 16.0874925CurrentTrain: epoch 15, batch    44 | loss: 13.1452162CurrentTrain: epoch 15, batch    45 | loss: 26.2824323CurrentTrain: epoch 15, batch    46 | loss: 12.9033461CurrentTrain: epoch 15, batch    47 | loss: 16.3152565CurrentTrain: epoch 15, batch    48 | loss: 18.3588046CurrentTrain: epoch 15, batch    49 | loss: 13.1252064CurrentTrain: epoch 15, batch    50 | loss: 23.1536594CurrentTrain: epoch 15, batch    51 | loss: 13.3395088CurrentTrain: epoch 15, batch    52 | loss: 18.6222477CurrentTrain: epoch 15, batch    53 | loss: 11.0418155CurrentTrain: epoch 15, batch    54 | loss: 23.7964077CurrentTrain: epoch 15, batch    55 | loss: 14.5435410CurrentTrain: epoch 15, batch    56 | loss: 17.8271603CurrentTrain: epoch 15, batch    57 | loss: 16.6513009CurrentTrain: epoch 15, batch    58 | loss: 22.7364834CurrentTrain: epoch 15, batch    59 | loss: 13.9773714CurrentTrain: epoch 15, batch    60 | loss: 22.3169275CurrentTrain: epoch 15, batch    61 | loss: 24.9849223CurrentTrain: epoch  7, batch    62 | loss: 12.3833844CurrentTrain: epoch 15, batch     0 | loss: 20.4052501CurrentTrain: epoch 15, batch     1 | loss: 17.8736072CurrentTrain: epoch 15, batch     2 | loss: 12.7729452CurrentTrain: epoch 15, batch     3 | loss: 14.4704247CurrentTrain: epoch 15, batch     4 | loss: 14.7486874CurrentTrain: epoch 15, batch     5 | loss: 31.1111938CurrentTrain: epoch 15, batch     6 | loss: 11.4003038CurrentTrain: epoch 15, batch     7 | loss: 20.2632421CurrentTrain: epoch 15, batch     8 | loss: 45.9876655CurrentTrain: epoch 15, batch     9 | loss: 22.1491298CurrentTrain: epoch 15, batch    10 | loss: 18.6489901CurrentTrain: epoch 15, batch    11 | loss: 19.5405653CurrentTrain: epoch 15, batch    12 | loss: 15.3293245CurrentTrain: epoch 15, batch    13 | loss: 13.3939466CurrentTrain: epoch 15, batch    14 | loss: 20.5303588CurrentTrain: epoch 15, batch    15 | loss: 19.7429348CurrentTrain: epoch 15, batch    16 | loss: 16.5710833CurrentTrain: epoch 15, batch    17 | loss: 12.9429108CurrentTrain: epoch 15, batch    18 | loss: 22.1437133CurrentTrain: epoch 15, batch    19 | loss: 26.1526237CurrentTrain: epoch 15, batch    20 | loss: 13.1776640CurrentTrain: epoch 15, batch    21 | loss: 12.0491734CurrentTrain: epoch 15, batch    22 | loss: 14.7518071CurrentTrain: epoch 15, batch    23 | loss: 12.3797503CurrentTrain: epoch 15, batch    24 | loss: 19.1330773CurrentTrain: epoch 15, batch    25 | loss: 13.8758132CurrentTrain: epoch 15, batch    26 | loss: 25.9513211CurrentTrain: epoch 15, batch    27 | loss: 29.4215286CurrentTrain: epoch 15, batch    28 | loss: 11.1637610CurrentTrain: epoch 15, batch    29 | loss: 14.2177434CurrentTrain: epoch 15, batch    30 | loss: 13.0978403CurrentTrain: epoch 15, batch    31 | loss: 15.0123795CurrentTrain: epoch 15, batch    32 | loss: 15.1473537CurrentTrain: epoch 15, batch    33 | loss: 17.0471460CurrentTrain: epoch 15, batch    34 | loss: 13.5035810CurrentTrain: epoch 15, batch    35 | loss: 18.9895222CurrentTrain: epoch 15, batch    36 | loss: 12.3066049CurrentTrain: epoch 15, batch    37 | loss: 11.6656230CurrentTrain: epoch 15, batch    38 | loss: 12.7165776CurrentTrain: epoch 15, batch    39 | loss: 14.5823221CurrentTrain: epoch 15, batch    40 | loss: 12.5605523CurrentTrain: epoch 15, batch    41 | loss: 16.4919787CurrentTrain: epoch 15, batch    42 | loss: 10.8639154CurrentTrain: epoch 15, batch    43 | loss: 18.3647061CurrentTrain: epoch 15, batch    44 | loss: 26.9198946CurrentTrain: epoch 15, batch    45 | loss: 38.1501794CurrentTrain: epoch 15, batch    46 | loss: 22.7558862CurrentTrain: epoch 15, batch    47 | loss: 14.8351807CurrentTrain: epoch 15, batch    48 | loss: 11.5596819CurrentTrain: epoch 15, batch    49 | loss: 17.3414088CurrentTrain: epoch 15, batch    50 | loss: 17.1366715CurrentTrain: epoch 15, batch    51 | loss: 19.1153637CurrentTrain: epoch 15, batch    52 | loss: 11.2108369CurrentTrain: epoch 15, batch    53 | loss: 14.9786778CurrentTrain: epoch 15, batch    54 | loss: 16.6690075CurrentTrain: epoch 15, batch    55 | loss: 19.2575567CurrentTrain: epoch 15, batch    56 | loss: 13.4689420CurrentTrain: epoch 15, batch    57 | loss: 14.3844908CurrentTrain: epoch 15, batch    58 | loss: 19.7381488CurrentTrain: epoch 15, batch    59 | loss: 11.2919053CurrentTrain: epoch 15, batch    60 | loss: 15.1367075CurrentTrain: epoch 15, batch    61 | loss: 11.3081830CurrentTrain: epoch  7, batch    62 | loss: 8.9171501CurrentTrain: epoch 15, batch     0 | loss: 18.1040284CurrentTrain: epoch 15, batch     1 | loss: 14.8211749CurrentTrain: epoch 15, batch     2 | loss: 14.6006117CurrentTrain: epoch 15, batch     3 | loss: 12.6019348CurrentTrain: epoch 15, batch     4 | loss: 17.1634557CurrentTrain: epoch 15, batch     5 | loss: 25.6058399CurrentTrain: epoch 15, batch     6 | loss: 25.7111530CurrentTrain: epoch 15, batch     7 | loss: 11.8558176CurrentTrain: epoch 15, batch     8 | loss: 14.3950736CurrentTrain: epoch 15, batch     9 | loss: 13.6329423CurrentTrain: epoch 15, batch    10 | loss: 12.0711359CurrentTrain: epoch 15, batch    11 | loss: 17.1402556CurrentTrain: epoch 15, batch    12 | loss: 14.9047510CurrentTrain: epoch 15, batch    13 | loss: 12.6872295CurrentTrain: epoch 15, batch    14 | loss: 12.1251779CurrentTrain: epoch 15, batch    15 | loss: 12.0416438CurrentTrain: epoch 15, batch    16 | loss: 11.7514741CurrentTrain: epoch 15, batch    17 | loss: 26.3394001CurrentTrain: epoch 15, batch    18 | loss: 13.9556851CurrentTrain: epoch 15, batch    19 | loss: 17.1935492CurrentTrain: epoch 15, batch    20 | loss: 11.8462079CurrentTrain: epoch 15, batch    21 | loss: 12.7164832CurrentTrain: epoch 15, batch    22 | loss: 15.3409078CurrentTrain: epoch 15, batch    23 | loss: 14.6431341CurrentTrain: epoch 15, batch    24 | loss: 12.9650024CurrentTrain: epoch 15, batch    25 | loss: 14.3976972CurrentTrain: epoch 15, batch    26 | loss: 19.6081162CurrentTrain: epoch 15, batch    27 | loss: 27.1031901CurrentTrain: epoch 15, batch    28 | loss: 18.5188189CurrentTrain: epoch 15, batch    29 | loss: 12.2903050CurrentTrain: epoch 15, batch    30 | loss: 10.0348154CurrentTrain: epoch 15, batch    31 | loss: 13.7519586CurrentTrain: epoch 15, batch    32 | loss: 20.7521935CurrentTrain: epoch 15, batch    33 | loss: 12.5484451CurrentTrain: epoch 15, batch    34 | loss: 20.8520340CurrentTrain: epoch 15, batch    35 | loss: 16.0823687CurrentTrain: epoch 15, batch    36 | loss: 9.8111973CurrentTrain: epoch 15, batch    37 | loss: 14.5605550CurrentTrain: epoch 15, batch    38 | loss: 16.9622952CurrentTrain: epoch 15, batch    39 | loss: 21.8445854CurrentTrain: epoch 15, batch    40 | loss: 18.9065403CurrentTrain: epoch 15, batch    41 | loss: 22.1807808CurrentTrain: epoch 15, batch    42 | loss: 12.2424460CurrentTrain: epoch 15, batch    43 | loss: 23.2393215CurrentTrain: epoch 15, batch    44 | loss: 10.3692853CurrentTrain: epoch 15, batch    45 | loss: 12.9388680CurrentTrain: epoch 15, batch    46 | loss: 16.2564024CurrentTrain: epoch 15, batch    47 | loss: 12.2219928CurrentTrain: epoch 15, batch    48 | loss: 13.4471183CurrentTrain: epoch 15, batch    49 | loss: 13.7461144CurrentTrain: epoch 15, batch    50 | loss: 20.0660469CurrentTrain: epoch 15, batch    51 | loss: 13.3523782CurrentTrain: epoch 15, batch    52 | loss: 24.1526970CurrentTrain: epoch 15, batch    53 | loss: 14.0704844CurrentTrain: epoch 15, batch    54 | loss: 9.5387543CurrentTrain: epoch 15, batch    55 | loss: 27.6088113CurrentTrain: epoch 15, batch    56 | loss: 14.1973555CurrentTrain: epoch 15, batch    57 | loss: 10.1386258CurrentTrain: epoch 15, batch    58 | loss: 9.1597094CurrentTrain: epoch 15, batch    59 | loss: 15.0783311CurrentTrain: epoch 15, batch    60 | loss: 12.1324680CurrentTrain: epoch 15, batch    61 | loss: 13.2117850CurrentTrain: epoch  7, batch    62 | loss: 14.9958132CurrentTrain: epoch 15, batch     0 | loss: 12.1707201CurrentTrain: epoch 15, batch     1 | loss: 12.4367158CurrentTrain: epoch 15, batch     2 | loss: 17.6957949CurrentTrain: epoch 15, batch     3 | loss: 10.4387835CurrentTrain: epoch 15, batch     4 | loss: 26.1056891CurrentTrain: epoch 15, batch     5 | loss: 9.8025151CurrentTrain: epoch 15, batch     6 | loss: 17.1441764CurrentTrain: epoch 15, batch     7 | loss: 13.8584775CurrentTrain: epoch 15, batch     8 | loss: 13.5727635CurrentTrain: epoch 15, batch     9 | loss: 11.1807574CurrentTrain: epoch 15, batch    10 | loss: 15.3607361CurrentTrain: epoch 15, batch    11 | loss: 29.9350174CurrentTrain: epoch 15, batch    12 | loss: 20.0186691CurrentTrain: epoch 15, batch    13 | loss: 13.6938581CurrentTrain: epoch 15, batch    14 | loss: 12.8028642CurrentTrain: epoch 15, batch    15 | loss: 12.6497304CurrentTrain: epoch 15, batch    16 | loss: 27.0680934CurrentTrain: epoch 15, batch    17 | loss: 17.9456930CurrentTrain: epoch 15, batch    18 | loss: 18.6091429CurrentTrain: epoch 15, batch    19 | loss: 11.9332587CurrentTrain: epoch 15, batch    20 | loss: 12.6065324CurrentTrain: epoch 15, batch    21 | loss: 18.5829913CurrentTrain: epoch 15, batch    22 | loss: 11.6033019CurrentTrain: epoch 15, batch    23 | loss: 19.1939545CurrentTrain: epoch 15, batch    24 | loss: 15.6228381CurrentTrain: epoch 15, batch    25 | loss: 19.7529973CurrentTrain: epoch 15, batch    26 | loss: 11.8298559CurrentTrain: epoch 15, batch    27 | loss: 22.3397138CurrentTrain: epoch 15, batch    28 | loss: 22.1137413CurrentTrain: epoch 15, batch    29 | loss: 20.5026678CurrentTrain: epoch 15, batch    30 | loss: 13.8264461CurrentTrain: epoch 15, batch    31 | loss: 10.6535347CurrentTrain: epoch 15, batch    32 | loss: 13.8241080CurrentTrain: epoch 15, batch    33 | loss: 11.5878079CurrentTrain: epoch 15, batch    34 | loss: 13.7214831CurrentTrain: epoch 15, batch    35 | loss: 9.7654756CurrentTrain: epoch 15, batch    36 | loss: 17.9476692CurrentTrain: epoch 15, batch    37 | loss: 23.2132116CurrentTrain: epoch 15, batch    38 | loss: 11.2060477CurrentTrain: epoch 15, batch    39 | loss: 11.7152538CurrentTrain: epoch 15, batch    40 | loss: 24.1801446CurrentTrain: epoch 15, batch    41 | loss: 11.1692877CurrentTrain: epoch 15, batch    42 | loss: 12.0890745CurrentTrain: epoch 15, batch    43 | loss: 12.7364171CurrentTrain: epoch 15, batch    44 | loss: 12.4926575CurrentTrain: epoch 15, batch    45 | loss: 7.8782299CurrentTrain: epoch 15, batch    46 | loss: 13.2219244CurrentTrain: epoch 15, batch    47 | loss: 26.8321905CurrentTrain: epoch 15, batch    48 | loss: 48.8459951CurrentTrain: epoch 15, batch    49 | loss: 11.3400852CurrentTrain: epoch 15, batch    50 | loss: 9.2682458CurrentTrain: epoch 15, batch    51 | loss: 12.2611308CurrentTrain: epoch 15, batch    52 | loss: 20.2811619CurrentTrain: epoch 15, batch    53 | loss: 12.6551585CurrentTrain: epoch 15, batch    54 | loss: 18.4886673CurrentTrain: epoch 15, batch    55 | loss: 17.0507212CurrentTrain: epoch 15, batch    56 | loss: 10.9479782CurrentTrain: epoch 15, batch    57 | loss: 14.1963613CurrentTrain: epoch 15, batch    58 | loss: 11.8511857CurrentTrain: epoch 15, batch    59 | loss: 11.8058524CurrentTrain: epoch 15, batch    60 | loss: 11.5785804CurrentTrain: epoch 15, batch    61 | loss: 11.7175913CurrentTrain: epoch  7, batch    62 | loss: 10.2786439CurrentTrain: epoch 15, batch     0 | loss: 12.6650524CurrentTrain: epoch 15, batch     1 | loss: 13.1372056CurrentTrain: epoch 15, batch     2 | loss: 27.2163296CurrentTrain: epoch 15, batch     3 | loss: 12.1029956CurrentTrain: epoch 15, batch     4 | loss: 17.8763442CurrentTrain: epoch 15, batch     5 | loss: 29.9894644CurrentTrain: epoch 15, batch     6 | loss: 10.7137184CurrentTrain: epoch 15, batch     7 | loss: 17.4790787CurrentTrain: epoch 15, batch     8 | loss: 12.8132692CurrentTrain: epoch 15, batch     9 | loss: 10.9584329CurrentTrain: epoch 15, batch    10 | loss: 11.9253583CurrentTrain: epoch 15, batch    11 | loss: 13.3739642CurrentTrain: epoch 15, batch    12 | loss: 8.6542994CurrentTrain: epoch 15, batch    13 | loss: 8.7157573CurrentTrain: epoch 15, batch    14 | loss: 11.3825858CurrentTrain: epoch 15, batch    15 | loss: 14.6561580CurrentTrain: epoch 15, batch    16 | loss: 8.2628031CurrentTrain: epoch 15, batch    17 | loss: 7.9355865CurrentTrain: epoch 15, batch    18 | loss: 12.0327564CurrentTrain: epoch 15, batch    19 | loss: 11.1124099CurrentTrain: epoch 15, batch    20 | loss: 13.3248908CurrentTrain: epoch 15, batch    21 | loss: 10.8401576CurrentTrain: epoch 15, batch    22 | loss: 12.8540232CurrentTrain: epoch 15, batch    23 | loss: 8.5907467CurrentTrain: epoch 15, batch    24 | loss: 11.7103622CurrentTrain: epoch 15, batch    25 | loss: 9.9960842CurrentTrain: epoch 15, batch    26 | loss: 15.0517277CurrentTrain: epoch 15, batch    27 | loss: 24.9886226CurrentTrain: epoch 15, batch    28 | loss: 14.5099417CurrentTrain: epoch 15, batch    29 | loss: 11.0786365CurrentTrain: epoch 15, batch    30 | loss: 22.1504483CurrentTrain: epoch 15, batch    31 | loss: 12.7373423CurrentTrain: epoch 15, batch    32 | loss: 10.0170836CurrentTrain: epoch 15, batch    33 | loss: 17.4405516CurrentTrain: epoch 15, batch    34 | loss: 13.8019993CurrentTrain: epoch 15, batch    35 | loss: 10.6750836CurrentTrain: epoch 15, batch    36 | loss: 10.8956326CurrentTrain: epoch 15, batch    37 | loss: 21.0937738CurrentTrain: epoch 15, batch    38 | loss: 11.7293330CurrentTrain: epoch 15, batch    39 | loss: 16.6263728CurrentTrain: epoch 15, batch    40 | loss: 14.4525836CurrentTrain: epoch 15, batch    41 | loss: 11.5446106CurrentTrain: epoch 15, batch    42 | loss: 12.0855789CurrentTrain: epoch 15, batch    43 | loss: 18.4759627CurrentTrain: epoch 15, batch    44 | loss: 14.3779503CurrentTrain: epoch 15, batch    45 | loss: 11.3898030CurrentTrain: epoch 15, batch    46 | loss: 17.1172455CurrentTrain: epoch 15, batch    47 | loss: 19.3463952CurrentTrain: epoch 15, batch    48 | loss: 11.1560295CurrentTrain: epoch 15, batch    49 | loss: 19.7750209CurrentTrain: epoch 15, batch    50 | loss: 11.2939665CurrentTrain: epoch 15, batch    51 | loss: 19.0206482CurrentTrain: epoch 15, batch    52 | loss: 11.6883722CurrentTrain: epoch 15, batch    53 | loss: 8.4589805CurrentTrain: epoch 15, batch    54 | loss: 11.7146185CurrentTrain: epoch 15, batch    55 | loss: 9.1801521CurrentTrain: epoch 15, batch    56 | loss: 17.3106212CurrentTrain: epoch 15, batch    57 | loss: 15.0105166CurrentTrain: epoch 15, batch    58 | loss: 10.6125814CurrentTrain: epoch 15, batch    59 | loss: 11.0833971CurrentTrain: epoch 15, batch    60 | loss: 13.6913664CurrentTrain: epoch 15, batch    61 | loss: 14.5854193CurrentTrain: epoch  7, batch    62 | loss: 9.0596000CurrentTrain: epoch 15, batch     0 | loss: 10.1804173CurrentTrain: epoch 15, batch     1 | loss: 23.6410935CurrentTrain: epoch 15, batch     2 | loss: 10.3872125CurrentTrain: epoch 15, batch     3 | loss: 10.9664179CurrentTrain: epoch 15, batch     4 | loss: 9.6542848CurrentTrain: epoch 15, batch     5 | loss: 17.3049098CurrentTrain: epoch 15, batch     6 | loss: 12.6322578CurrentTrain: epoch 15, batch     7 | loss: 20.1841937CurrentTrain: epoch 15, batch     8 | loss: 17.5188335CurrentTrain: epoch 15, batch     9 | loss: 10.4705068CurrentTrain: epoch 15, batch    10 | loss: 11.9728803CurrentTrain: epoch 15, batch    11 | loss: 14.2855330CurrentTrain: epoch 15, batch    12 | loss: 14.7694835CurrentTrain: epoch 15, batch    13 | loss: 13.0351804CurrentTrain: epoch 15, batch    14 | loss: 39.1520111CurrentTrain: epoch 15, batch    15 | loss: 14.4311684CurrentTrain: epoch 15, batch    16 | loss: 13.0298006CurrentTrain: epoch 15, batch    17 | loss: 11.9774771CurrentTrain: epoch 15, batch    18 | loss: 11.2281156CurrentTrain: epoch 15, batch    19 | loss: 13.5213331CurrentTrain: epoch 15, batch    20 | loss: 21.3837721CurrentTrain: epoch 15, batch    21 | loss: 12.4524787CurrentTrain: epoch 15, batch    22 | loss: 8.5529616CurrentTrain: epoch 15, batch    23 | loss: 11.9056322CurrentTrain: epoch 15, batch    24 | loss: 17.7673222CurrentTrain: epoch 15, batch    25 | loss: 17.9493892CurrentTrain: epoch 15, batch    26 | loss: 41.9943394CurrentTrain: epoch 15, batch    27 | loss: 11.3785217CurrentTrain: epoch 15, batch    28 | loss: 13.6600593CurrentTrain: epoch 15, batch    29 | loss: 18.3540372CurrentTrain: epoch 15, batch    30 | loss: 14.5327583CurrentTrain: epoch 15, batch    31 | loss: 20.8995424CurrentTrain: epoch 15, batch    32 | loss: 13.4529051CurrentTrain: epoch 15, batch    33 | loss: 22.9398620CurrentTrain: epoch 15, batch    34 | loss: 8.1728335CurrentTrain: epoch 15, batch    35 | loss: 18.2681180CurrentTrain: epoch 15, batch    36 | loss: 13.8895975CurrentTrain: epoch 15, batch    37 | loss: 21.2338449CurrentTrain: epoch 15, batch    38 | loss: 15.9028740CurrentTrain: epoch 15, batch    39 | loss: 18.3593431CurrentTrain: epoch 15, batch    40 | loss: 36.3823561CurrentTrain: epoch 15, batch    41 | loss: 12.2016899CurrentTrain: epoch 15, batch    42 | loss: 12.9394302CurrentTrain: epoch 15, batch    43 | loss: 12.6773101CurrentTrain: epoch 15, batch    44 | loss: 12.8257513CurrentTrain: epoch 15, batch    45 | loss: 10.1348745CurrentTrain: epoch 15, batch    46 | loss: 10.9528105CurrentTrain: epoch 15, batch    47 | loss: 13.6554231CurrentTrain: epoch 15, batch    48 | loss: 15.1474151CurrentTrain: epoch 15, batch    49 | loss: 17.1256715CurrentTrain: epoch 15, batch    50 | loss: 23.6517490CurrentTrain: epoch 15, batch    51 | loss: 19.3319722CurrentTrain: epoch 15, batch    52 | loss: 9.7439179CurrentTrain: epoch 15, batch    53 | loss: 10.1564711CurrentTrain: epoch 15, batch    54 | loss: 11.3376068CurrentTrain: epoch 15, batch    55 | loss: 13.2643197CurrentTrain: epoch 15, batch    56 | loss: 9.5847582CurrentTrain: epoch 15, batch    57 | loss: 13.2916171CurrentTrain: epoch 15, batch    58 | loss: 14.7382766CurrentTrain: epoch 15, batch    59 | loss: 18.3724667CurrentTrain: epoch 15, batch    60 | loss: 13.5807268CurrentTrain: epoch 15, batch    61 | loss: 6.8425703CurrentTrain: epoch  7, batch    62 | loss: 16.5732583CurrentTrain: epoch 15, batch     0 | loss: 9.7170821CurrentTrain: epoch 15, batch     1 | loss: 10.7999483CurrentTrain: epoch 15, batch     2 | loss: 15.6683091CurrentTrain: epoch 15, batch     3 | loss: 13.1377651CurrentTrain: epoch 15, batch     4 | loss: 13.2368920CurrentTrain: epoch 15, batch     5 | loss: 15.4804402CurrentTrain: epoch 15, batch     6 | loss: 11.5303490CurrentTrain: epoch 15, batch     7 | loss: 19.4173407CurrentTrain: epoch 15, batch     8 | loss: 12.3381582CurrentTrain: epoch 15, batch     9 | loss: 15.5825153CurrentTrain: epoch 15, batch    10 | loss: 23.0884570CurrentTrain: epoch 15, batch    11 | loss: 15.7691039CurrentTrain: epoch 15, batch    12 | loss: 11.5505676CurrentTrain: epoch 15, batch    13 | loss: 12.0788716CurrentTrain: epoch 15, batch    14 | loss: 21.1514692CurrentTrain: epoch 15, batch    15 | loss: 17.0851909CurrentTrain: epoch 15, batch    16 | loss: 11.9483799CurrentTrain: epoch 15, batch    17 | loss: 18.2727802CurrentTrain: epoch 15, batch    18 | loss: 13.6927403CurrentTrain: epoch 15, batch    19 | loss: 18.1623147CurrentTrain: epoch 15, batch    20 | loss: 12.0586188CurrentTrain: epoch 15, batch    21 | loss: 11.7709243CurrentTrain: epoch 15, batch    22 | loss: 18.0181222CurrentTrain: epoch 15, batch    23 | loss: 20.5169695CurrentTrain: epoch 15, batch    24 | loss: 17.6260694CurrentTrain: epoch 15, batch    25 | loss: 27.5572675CurrentTrain: epoch 15, batch    26 | loss: 15.8640388CurrentTrain: epoch 15, batch    27 | loss: 11.8877104CurrentTrain: epoch 15, batch    28 | loss: 14.7571745CurrentTrain: epoch 15, batch    29 | loss: 20.5774826CurrentTrain: epoch 15, batch    30 | loss: 15.2003044CurrentTrain: epoch 15, batch    31 | loss: 10.8259986CurrentTrain: epoch 15, batch    32 | loss: 9.0995706CurrentTrain: epoch 15, batch    33 | loss: 17.7741464CurrentTrain: epoch 15, batch    34 | loss: 12.6626154CurrentTrain: epoch 15, batch    35 | loss: 22.1997412CurrentTrain: epoch 15, batch    36 | loss: 12.1069317CurrentTrain: epoch 15, batch    37 | loss: 20.3351374CurrentTrain: epoch 15, batch    38 | loss: 18.3478478CurrentTrain: epoch 15, batch    39 | loss: 10.5177155CurrentTrain: epoch 15, batch    40 | loss: 10.6437382CurrentTrain: epoch 15, batch    41 | loss: 11.2578870CurrentTrain: epoch 15, batch    42 | loss: 14.9072134CurrentTrain: epoch 15, batch    43 | loss: 14.4569742CurrentTrain: epoch 15, batch    44 | loss: 13.4087726CurrentTrain: epoch 15, batch    45 | loss: 11.3364683CurrentTrain: epoch 15, batch    46 | loss: 15.0814238CurrentTrain: epoch 15, batch    47 | loss: 15.0858106CurrentTrain: epoch 15, batch    48 | loss: 14.8534954CurrentTrain: epoch 15, batch    49 | loss: 19.4518756CurrentTrain: epoch 15, batch    50 | loss: 11.3078380CurrentTrain: epoch 15, batch    51 | loss: 17.8084142CurrentTrain: epoch 15, batch    52 | loss: 23.1728108CurrentTrain: epoch 15, batch    53 | loss: 17.6209736CurrentTrain: epoch 15, batch    54 | loss: 11.3819314CurrentTrain: epoch 15, batch    55 | loss: 17.7490036CurrentTrain: epoch 15, batch    56 | loss: 13.6617766CurrentTrain: epoch 15, batch    57 | loss: 16.0799919CurrentTrain: epoch 15, batch    58 | loss: 16.1824810CurrentTrain: epoch 15, batch    59 | loss: 18.0604989CurrentTrain: epoch 15, batch    60 | loss: 7.6683057CurrentTrain: epoch 15, batch    61 | loss: 14.1558631CurrentTrain: epoch  7, batch    62 | loss: 17.6107991CurrentTrain: epoch 15, batch     0 | loss: 18.3947174CurrentTrain: epoch 15, batch     1 | loss: 18.9437163CurrentTrain: epoch 15, batch     2 | loss: 12.1631982CurrentTrain: epoch 15, batch     3 | loss: 9.4678949CurrentTrain: epoch 15, batch     4 | loss: 18.5972706CurrentTrain: epoch 15, batch     5 | loss: 15.1214613CurrentTrain: epoch 15, batch     6 | loss: 11.3918340CurrentTrain: epoch 15, batch     7 | loss: 21.9652065CurrentTrain: epoch 15, batch     8 | loss: 14.0328319CurrentTrain: epoch 15, batch     9 | loss: 20.3962112CurrentTrain: epoch 15, batch    10 | loss: 19.1079706CurrentTrain: epoch 15, batch    11 | loss: 14.8774804CurrentTrain: epoch 15, batch    12 | loss: 16.9678774CurrentTrain: epoch 15, batch    13 | loss: 9.9487140CurrentTrain: epoch 15, batch    14 | loss: 14.3993135CurrentTrain: epoch 15, batch    15 | loss: 19.0963303CurrentTrain: epoch 15, batch    16 | loss: 14.0500345CurrentTrain: epoch 15, batch    17 | loss: 9.2936031CurrentTrain: epoch 15, batch    18 | loss: 12.9144353CurrentTrain: epoch 15, batch    19 | loss: 20.7646609CurrentTrain: epoch 15, batch    20 | loss: 17.1266049CurrentTrain: epoch 15, batch    21 | loss: 13.4613942CurrentTrain: epoch 15, batch    22 | loss: 12.7118510CurrentTrain: epoch 15, batch    23 | loss: 12.5265417CurrentTrain: epoch 15, batch    24 | loss: 17.5496248CurrentTrain: epoch 15, batch    25 | loss: 16.5791820CurrentTrain: epoch 15, batch    26 | loss: 13.5651223CurrentTrain: epoch 15, batch    27 | loss: 13.8651359CurrentTrain: epoch 15, batch    28 | loss: 10.1498087CurrentTrain: epoch 15, batch    29 | loss: 15.9663233CurrentTrain: epoch 15, batch    30 | loss: 12.4486745CurrentTrain: epoch 15, batch    31 | loss: 14.5675810CurrentTrain: epoch 15, batch    32 | loss: 13.8957826CurrentTrain: epoch 15, batch    33 | loss: 22.1888708CurrentTrain: epoch 15, batch    34 | loss: 12.0482039CurrentTrain: epoch 15, batch    35 | loss: 10.6654596CurrentTrain: epoch 15, batch    36 | loss: 13.1905527CurrentTrain: epoch 15, batch    37 | loss: 12.0919189CurrentTrain: epoch 15, batch    38 | loss: 13.7452779CurrentTrain: epoch 15, batch    39 | loss: 16.1817863CurrentTrain: epoch 15, batch    40 | loss: 11.3975496CurrentTrain: epoch 15, batch    41 | loss: 13.1943702CurrentTrain: epoch 15, batch    42 | loss: 11.8699803CurrentTrain: epoch 15, batch    43 | loss: 22.1193739CurrentTrain: epoch 15, batch    44 | loss: 11.0186701CurrentTrain: epoch 15, batch    45 | loss: 23.3889485CurrentTrain: epoch 15, batch    46 | loss: 11.5140064CurrentTrain: epoch 15, batch    47 | loss: 28.4084917CurrentTrain: epoch 15, batch    48 | loss: 18.3828013CurrentTrain: epoch 15, batch    49 | loss: 18.2961529CurrentTrain: epoch 15, batch    50 | loss: 9.6131013CurrentTrain: epoch 15, batch    51 | loss: 16.7855066CurrentTrain: epoch 15, batch    52 | loss: 8.4946995CurrentTrain: epoch 15, batch    53 | loss: 14.4961815CurrentTrain: epoch 15, batch    54 | loss: 11.4367880CurrentTrain: epoch 15, batch    55 | loss: 14.3346241CurrentTrain: epoch 15, batch    56 | loss: 10.6618392CurrentTrain: epoch 15, batch    57 | loss: 25.8078898CurrentTrain: epoch 15, batch    58 | loss: 14.7660448CurrentTrain: epoch 15, batch    59 | loss: 10.0893385CurrentTrain: epoch 15, batch    60 | loss: 7.0471802CurrentTrain: epoch 15, batch    61 | loss: 9.0126253CurrentTrain: epoch  7, batch    62 | loss: 8.1592922CurrentTrain: epoch 15, batch     0 | loss: 12.8936861CurrentTrain: epoch 15, batch     1 | loss: 11.9976203CurrentTrain: epoch 15, batch     2 | loss: 11.9917543CurrentTrain: epoch 15, batch     3 | loss: 14.8328739CurrentTrain: epoch 15, batch     4 | loss: 10.5315816CurrentTrain: epoch 15, batch     5 | loss: 14.1136297CurrentTrain: epoch 15, batch     6 | loss: 11.2185394CurrentTrain: epoch 15, batch     7 | loss: 8.0961203CurrentTrain: epoch 15, batch     8 | loss: 16.9391338CurrentTrain: epoch 15, batch     9 | loss: 15.8859288CurrentTrain: epoch 15, batch    10 | loss: 11.1454916CurrentTrain: epoch 15, batch    11 | loss: 15.0687808CurrentTrain: epoch 15, batch    12 | loss: 8.9479783CurrentTrain: epoch 15, batch    13 | loss: 11.4355348CurrentTrain: epoch 15, batch    14 | loss: 13.8085834CurrentTrain: epoch 15, batch    15 | loss: 16.4829409CurrentTrain: epoch 15, batch    16 | loss: 17.0971182CurrentTrain: epoch 15, batch    17 | loss: 15.9660499CurrentTrain: epoch 15, batch    18 | loss: 11.8679912CurrentTrain: epoch 15, batch    19 | loss: 14.4503417CurrentTrain: epoch 15, batch    20 | loss: 13.1723468CurrentTrain: epoch 15, batch    21 | loss: 14.6409471CurrentTrain: epoch 15, batch    22 | loss: 18.2485540CurrentTrain: epoch 15, batch    23 | loss: 9.9823769CurrentTrain: epoch 15, batch    24 | loss: 8.3679424CurrentTrain: epoch 15, batch    25 | loss: 13.9869020CurrentTrain: epoch 15, batch    26 | loss: 15.0174422CurrentTrain: epoch 15, batch    27 | loss: 13.2754722CurrentTrain: epoch 15, batch    28 | loss: 17.2588759CurrentTrain: epoch 15, batch    29 | loss: 10.5731829CurrentTrain: epoch 15, batch    30 | loss: 10.8936932CurrentTrain: epoch 15, batch    31 | loss: 10.8079443CurrentTrain: epoch 15, batch    32 | loss: 9.5004215CurrentTrain: epoch 15, batch    33 | loss: 11.3276982CurrentTrain: epoch 15, batch    34 | loss: 13.0038817CurrentTrain: epoch 15, batch    35 | loss: 12.5153476CurrentTrain: epoch 15, batch    36 | loss: 10.7426374CurrentTrain: epoch 15, batch    37 | loss: 14.9192872CurrentTrain: epoch 15, batch    38 | loss: 18.3906277CurrentTrain: epoch 15, batch    39 | loss: 15.2895529CurrentTrain: epoch 15, batch    40 | loss: 11.2209115CurrentTrain: epoch 15, batch    41 | loss: 11.2370265CurrentTrain: epoch 15, batch    42 | loss: 17.7027433CurrentTrain: epoch 15, batch    43 | loss: 14.8808522CurrentTrain: epoch 15, batch    44 | loss: 13.6830248CurrentTrain: epoch 15, batch    45 | loss: 22.3384937CurrentTrain: epoch 15, batch    46 | loss: 15.3350722CurrentTrain: epoch 15, batch    47 | loss: 14.1975878CurrentTrain: epoch 15, batch    48 | loss: 9.9847001CurrentTrain: epoch 15, batch    49 | loss: 10.8308091CurrentTrain: epoch 15, batch    50 | loss: 9.9405259CurrentTrain: epoch 15, batch    51 | loss: 23.7950756CurrentTrain: epoch 15, batch    52 | loss: 11.4091685CurrentTrain: epoch 15, batch    53 | loss: 9.1437646CurrentTrain: epoch 15, batch    54 | loss: 13.5885122CurrentTrain: epoch 15, batch    55 | loss: 10.7830843CurrentTrain: epoch 15, batch    56 | loss: 8.0196538CurrentTrain: epoch 15, batch    57 | loss: 22.2950200CurrentTrain: epoch 15, batch    58 | loss: 13.7410487CurrentTrain: epoch 15, batch    59 | loss: 8.8236216CurrentTrain: epoch 15, batch    60 | loss: 11.0774766CurrentTrain: epoch 15, batch    61 | loss: 17.2914879CurrentTrain: epoch  7, batch    62 | loss: 9.5872465
#### movement ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "movement" means literary, artistic, scientific or philosophical movement or scene associated with this person or work.
Here is an example:
Relation: movement
Context: the theme of the loss of identity in crowds in this painting can be seen as a precursor to expressionist painting , particularly the work of james ensor .
Head Entity: james ensor
Tail Entity: expressionist
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "movement":
Relation: movement  
Context: The surrealist movement, which sought to challenge the conventional perception of reality, was significantly influenced by the works of Salvador Dalí.  
Head Entity: Salvador Dalí  
Tail Entity: surrealist  

Relation: movement  
Context: The emergence of the romantic movement in literature was marked by the profound impact of poets like William Wordsworth and Samuel Taylor Coleridge.  
Head Entity: William Wordsworth  
Tail Entity: romantic  
#### located in the administrative territorial entity ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "located in the administrative territorial entity" means the item is located on the territory of the following administrative entity.
Here is an example:
Relation: located in the administrative territorial entity
Context: lyons hill is a restored village , and former parish with church , now part of the community of ardclough in north county kildare .
Head Entity: ardclough
Tail Entity: county kildare
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "located in the administrative territorial entity":
Relation: located in the administrative territorial entity  
Context: The city of San Francisco is known for its iconic Golden Gate Bridge and is part of the larger San Francisco County in California.  
Head Entity: San Francisco  
Tail Entity: San Francisco County  

Relation: located in the administrative territorial entity  
Context: The historic town of Bath is famous for its Roman baths and is situated within the unitary authority of Bath and North East Somerset.  
Head Entity: Bath  
Tail Entity: Bath and North East Somerset  
#### head of government ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "head of government" means head of the executive power of this town, city, municipality, state, country, or other governmental body.
Here is an example:
Relation: head of government
Context: on february 10 , 2010 , stefan mappus was elected minister - president of baden - württemberg , and asked that all ministers continue their roles .
Head Entity: baden - württemberg
Tail Entity: stefan mappus
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "head of government":
Relation: head of government  
Context: in 2021, jacinda ardern was re-elected as the prime minister of new zealand, leading the labor party to a significant victory in the general elections.  
Head Entity: new zealand  
Tail Entity: jacinda ardern  

Relation: head of government  
Context: during the annual summit, justin trudeau addressed the nation as the prime minister of canada, outlining his government's plans for the upcoming year.  
Head Entity: canada  
Tail Entity: justin trudeau  
#### member of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "member of" means organization, club or musical group to which the subject belongs.
Here is an example:
Relation: member of
Context: hank henshaw became a member of the sinestro corps during the sinestro corps war .
Head Entity: hank henshaw
Tail Entity: sinestro corps
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "member of":
Relation: member of  
Context: after years of dedication, she was finally recognized as a member of the national ballet company.  
Head Entity: she  
Tail Entity: national ballet company  

Relation: member of  
Context: the renowned scientist was proud to be a member of the international research consortium focused on climate change.  
Head Entity: the renowned scientist  
Tail Entity: international research consortium  
#### crosses ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "crosses" means obstacle which this bridge crosses over or this tunnel goes under.
Here is an example:
Relation: crosses
Context: the bosideng bridge ( ) is a highway bridge over the yangtze river in hejiang county , sichuan , china .
Head Entity: bosideng bridge
Tail Entity: yangtze river
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "crosses":
Relation: crosses  
Context: the golden gate bridge ( ) spans the golden gate strait, connecting san francisco to marin county in california.  
Head Entity: golden gate bridge  
Tail Entity: golden gate strait  

Relation: crosses  
Context: the channel tunnel ( ) runs beneath the english channel, linking the united kingdom with mainland europe.  
Head Entity: channel tunnel  
Tail Entity: english channel  
#### tributary ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "tributary" means watercourse that flows into an other one.
Here is an example:
Relation: tributary
Context: it is on the panj river just downstream from the mouth of the bartang river at the point where the panj briefly turns west before resuming its northerly course .
Head Entity: panj river
Tail Entity: bartang river
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "tributary":
Relation: tributary  
Context: The Missouri River, which is a major tributary of the Mississippi River, flows through several states before joining the larger watercourse.  
Head Entity: Missouri River  
Tail Entity: Mississippi River  

Relation: tributary  
Context: The Rio Grande serves as a natural border and has several tributaries, including the Pecos River, which contributes to its flow.  
Head Entity: Pecos River  
Tail Entity: Rio Grande  
#### position held ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "position held" means subject currently or formerly holds the object position or public office.
Here is an example:
Relation: position held
Context: in september 2001 , mimica became minister of european integration under prime minister ivica račan , position he held until december 2003 .
Head Entity: ivica račan
Tail Entity: prime minister
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "position held":
Relation: position held  
Context: in 2010, barack obama was elected as the 44th president of the united states, a position he held until 2017.  
Head Entity: barack obama  
Tail Entity: president of the united states  

Relation: position held  
Context: after serving as the governor of california, arnold schwarzenegger took on the role of chairman of the president's council on physical fitness and sports, a position he held for several years.  
Head Entity: arnold schwarzenegger  
Tail Entity: chairman of the president's council on physical fitness and sports  
#### developer ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "developer" means organization or person that developed the item.
Here is an example:
Relation: developer
Context: unlike its predecessor , " torchlight ii " features cinematic sequences , which are produced by klei entertainment , the developer of " do n't starve " , " eets " and " shank " .
Head Entity: shank
Tail Entity: klei entertainment
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "developer":
Relation: developer  
Context: The popular game "Celeste" was created by Maddy Makes Games, the developer known for its engaging gameplay and heartfelt storytelling.  
Head Entity: Celeste  
Tail Entity: Maddy Makes Games  

Relation: developer  
Context: "The Witcher 3: Wild Hunt" was developed by CD Projekt Red, a studio that has gained acclaim for its immersive open-world experiences.  
Head Entity: The Witcher 3: Wild Hunt  
Tail Entity: CD Projekt Red  
#### location of formation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "location of formation" means location where a group or organization was formed.
Here is an example:
Relation: location of formation
Context: the weakerthans formed in winnipeg in 1997 with the lineup of john k. samson ( vocals , guitar ) , john p. sutton ( bass guitar ) , and jason tait ( drums ) .
Head Entity: the weakerthans
Tail Entity: winnipeg
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "location of formation":
Relation: location of formation  
Context: the band coldplay was formed in london in 1996 by chris martin, jonny buckland, guy berryman, and will champion.  
Head Entity: coldplay  
Tail Entity: london  

Relation: location of formation  
Context: the tech startup airbnb was founded in san francisco in 2008 by brian chesky, joe gebbia, and nathan blecharczyk.  
Head Entity: airbnb  
Tail Entity: san francisco  
#### country of origin ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "country of origin" means country of origin of this item (creative work, food, phrase, product, etc.).
Here is an example:
Relation: country of origin
Context: she has appeared in some american television shows , beginning with her regular role as snow white 's stepmother , evil queen lillian " lily " white in the series " the charmings " .
Head Entity: the charmings
Tail Entity: american
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "country of origin":
Relation: country of origin  
Context: The famous dish known as paella is traditionally associated with the coastal region of Valencia, where it originated and has become a staple of Spanish cuisine.  
Head Entity: paella  
Tail Entity: Spain  

Relation: country of origin  
Context: The iconic brand of chocolate, Toblerone, is known for its unique triangular shape and is produced in Switzerland, where it has been made since 1908.  
Head Entity: Toblerone  
Tail Entity: Switzerland  
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 85.00%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 86.46%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 89.06%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 90.28%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 91.25%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 91.48%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 92.19%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 92.31%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 92.41%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 92.50%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 92.97%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 93.38%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 93.40%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 94.06%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 94.35%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 94.32%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 94.57%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 94.79%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 94.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 94.71%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 94.68%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 94.64%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 94.61%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 94.79%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 94.96%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 95.12%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 95.27%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 95.40%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 95.36%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 95.49%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 95.61%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 95.56%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 95.67%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 95.78%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 95.88%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 95.98%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 96.08%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 96.16%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 96.25%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 96.33%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 96.41%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 96.48%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 96.56%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 96.62%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 96.57%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 96.39%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 96.34%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 96.41%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 96.14%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 96.09%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 96.05%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 95.91%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 95.97%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 95.94%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 96.00%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 95.97%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 95.24%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 85.00%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 86.46%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 89.06%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 90.28%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 91.25%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 91.48%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 92.19%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 92.31%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 92.41%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 92.50%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 92.97%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 93.38%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 93.40%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 94.06%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 94.35%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 94.32%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 94.57%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 94.79%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 94.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 94.71%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 94.68%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 94.64%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 94.61%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 94.79%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 94.96%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 95.12%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 95.27%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 95.40%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 95.36%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 95.49%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 95.61%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 95.56%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 95.67%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 95.78%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 95.88%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 95.98%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 96.08%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 96.16%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 96.25%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 96.33%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 96.41%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 96.48%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 96.56%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 96.62%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 96.57%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 96.39%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 96.34%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 96.41%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 96.14%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 96.09%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 96.05%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 95.91%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 95.97%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 95.94%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 96.00%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 95.97%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 95.24%   
cur_acc:  ['0.9524']
his_acc:  ['0.9524']
CurrentTrain: epoch 15, batch     0 | loss: 21.9739580CurrentTrain: epoch 15, batch     1 | loss: 25.4732235CurrentTrain: epoch 15, batch     2 | loss: 17.8232762CurrentTrain: epoch  1, batch     3 | loss: 11.2832165CurrentTrain: epoch 15, batch     0 | loss: 13.1379919CurrentTrain: epoch 15, batch     1 | loss: 18.6717624CurrentTrain: epoch 15, batch     2 | loss: 16.0872924CurrentTrain: epoch  1, batch     3 | loss: 8.5809275CurrentTrain: epoch 15, batch     0 | loss: 20.6999724CurrentTrain: epoch 15, batch     1 | loss: 15.4561093CurrentTrain: epoch 15, batch     2 | loss: 14.3985366CurrentTrain: epoch  1, batch     3 | loss: 8.4596862CurrentTrain: epoch 15, batch     0 | loss: 13.1695040CurrentTrain: epoch 15, batch     1 | loss: 15.9736198CurrentTrain: epoch 15, batch     2 | loss: 14.4382088CurrentTrain: epoch  1, batch     3 | loss: 9.2462687CurrentTrain: epoch 15, batch     0 | loss: 19.8823659CurrentTrain: epoch 15, batch     1 | loss: 13.6059411CurrentTrain: epoch 15, batch     2 | loss: 13.9584824CurrentTrain: epoch  1, batch     3 | loss: 9.7149251CurrentTrain: epoch 15, batch     0 | loss: 15.6253809CurrentTrain: epoch 15, batch     1 | loss: 12.8590101CurrentTrain: epoch 15, batch     2 | loss: 17.9882662CurrentTrain: epoch  1, batch     3 | loss: 9.0580915CurrentTrain: epoch 15, batch     0 | loss: 11.8024846CurrentTrain: epoch 15, batch     1 | loss: 11.7087506CurrentTrain: epoch 15, batch     2 | loss: 15.1571254CurrentTrain: epoch  1, batch     3 | loss: 8.6605322CurrentTrain: epoch 15, batch     0 | loss: 9.0660028CurrentTrain: epoch 15, batch     1 | loss: 11.3473521CurrentTrain: epoch 15, batch     2 | loss: 15.8069806CurrentTrain: epoch  1, batch     3 | loss: 9.5353299CurrentTrain: epoch 15, batch     0 | loss: 15.6108314CurrentTrain: epoch 15, batch     1 | loss: 8.3725102CurrentTrain: epoch 15, batch     2 | loss: 9.2611315CurrentTrain: epoch  1, batch     3 | loss: 8.7287006CurrentTrain: epoch 15, batch     0 | loss: 12.0205666CurrentTrain: epoch 15, batch     1 | loss: 11.2046077CurrentTrain: epoch 15, batch     2 | loss: 11.4864440CurrentTrain: epoch  1, batch     3 | loss: 9.3723131
#### applies to jurisdiction ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "applies to jurisdiction" means the item (institution, law, public office, public register...) or statement belongs to or has power over or applies to the value (a territorial jurisdiction: a country, state, municipality, ...).
Here is an example:
Relation: applies to jurisdiction
Context: george lawson sheldon ( may 31 , 1870 – april 4 , 1960 ) was 14th governor of nebraska with service from 1907 until 1909 .
Head Entity: governor
Tail Entity: nebraska
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "applies to jurisdiction":
Relation: applies to jurisdiction  
Context: The Environmental Protection Agency (EPA) is responsible for enforcing regulations related to environmental protection in the United States.  
Head Entity: Environmental Protection Agency  
Tail Entity: United States  

Relation: applies to jurisdiction  
Context: The California Consumer Privacy Act (CCPA) provides residents of California with specific rights regarding their personal information.  
Head Entity: California Consumer Privacy Act  
Tail Entity: California  
#### performer ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "performer" means actor, musician, band or other performer associated with this role or musical work.
Here is an example:
Relation: performer
Context: it is prominently featured on jean michel jarre 's albums " oxygène " ( 1977 ) and " équinoxe " ( 1978 ) .
Head Entity: équinoxe
Tail Entity: jean michel jarre
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "performer":
Relation: performer  
Context: The legendary band Queen is known for their iconic song "Bohemian Rhapsody," which showcases Freddie Mercury's extraordinary vocal talent.  
Head Entity: Bohemian Rhapsody  
Tail Entity: Freddie Mercury  

Relation: performer  
Context: Taylor Swift captivated audiences with her heartfelt performance of "Love Story" during the awards ceremony last night.  
Head Entity: Love Story  
Tail Entity: Taylor Swift  
#### manufacturer ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "manufacturer" means manufacturer or producer of this product.
Here is an example:
Relation: manufacturer
Context: the glasflügel h-201 standard " libelle " ( german : " dragonfly " ) is an early composite standard class single - seat sailplane produced by glasflügel from 1967 .
Head Entity: dragonfly
Tail Entity: glasflügel
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "manufacturer":
Relation: manufacturer  
Context: The iPhone 13, a smartphone developed by Apple, was released in September 2021 and features advanced camera technology and improved battery life.  
Head Entity: iPhone 13  
Tail Entity: Apple  

Relation: manufacturer  
Context: The Tesla Model S is an all-electric luxury sedan that was first introduced by Tesla, Inc. in 2012, showcasing innovative technology and performance.  
Head Entity: Tesla Model S  
Tail Entity: Tesla, Inc.  
#### participant ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "participant" means person, group of people or organization (object) that actively takes/took part in an event or process.
Here is an example:
Relation: participant
Context: in contrast to gwladys and roger 's allegiance to the house of lancaster , their three sons were staunch yorkists during the wars of the roses .
Head Entity: wars of the roses
Tail Entity: house of lancaster
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "participant":
Relation: participant  
Context: During the annual tech conference, several leading companies showcased their latest innovations, with Google and Microsoft being the most prominent participants.  
Head Entity: tech conference  
Tail Entity: Google  

Relation: participant  
Context: The local community organized a charity run, where various schools and organizations, including the Red Cross, participated to raise funds for the homeless.  
Head Entity: charity run  
Tail Entity: Red Cross  
#### operating system ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "operating system" means operating system (OS) on which a software works or the OS installed on hardware.
Here is an example:
Relation: operating system
Context: the software is written in python and works on all the platforms supported by blender : windows , osx and linux .
Head Entity: blender
Tail Entity: windows
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "operating system":
Relation: operating system  
Context: The latest version of the application is compatible with various operating systems, including macOS, Windows, and Ubuntu.  
Head Entity: application  
Tail Entity: macOS  

Relation: operating system  
Context: Many developers prefer to use Linux as their primary operating system due to its flexibility and open-source nature.  
Head Entity: developers  
Tail Entity: Linux  
#### competition class ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "competition class" means official classification by a regulating body under which the subject (events, teams, participants, or equipment) qualifies for inclusion.
Here is an example:
Relation: competition class
Context: in his next bout , he won the wbc light welterweight title by defeating roger mayweather for a second time .
Head Entity: roger mayweather
Tail Entity: light welterweight
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "competition class":
Relation: competition class  
Context: The team secured their place in the tournament by qualifying under the UEFA Champions League regulations.  
Head Entity: UEFA Champions League  
Tail Entity: tournament  

Relation: competition class  
Context: The athlete competed in the 100 meters dash, which is classified under the Olympic Games standards.  
Head Entity: Olympic Games  
Tail Entity: 100 meters dash  
#### successful candidate ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "successful candidate" means person elected after the election.
Here is an example:
Relation: successful candidate
Context: the qup gained 3 seats from its previous election result , and won a 10 % swing to it in primary votes , but failed to defeat the incumbent labor government led by ted theodore .
Head Entity: previous election
Tail Entity: ted theodore
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "successful candidate":
Relation: successful candidate  
Context: After a closely contested campaign, the voters chose Sarah Johnson as their representative, marking a significant shift in the political landscape of the district.  
Head Entity: closely contested campaign  
Tail Entity: Sarah Johnson  

Relation: successful candidate  
Context: In the recent mayoral election, the community rallied behind Mark Thompson, who promised to bring new initiatives to the city, ultimately leading to his victory.  
Head Entity: recent mayoral election  
Tail Entity: Mark Thompson  
#### sibling ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "sibling" means the subject and the object have the same parents.
Here is an example:
Relation: sibling
Context: although the latter 's eldest son , lǫgmaðr , appears to have succeeded to the kingship , he was soon forced to contend with factions supporting his younger brothers : haraldr , and óláfr .
Head Entity: lǫgmaðr
Tail Entity: óláfr
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "sibling":
Relation: sibling  
Context: In the royal family of the ancient kingdom, the two brothers, Eirik and Sigurd, were known for their fierce rivalry, yet they shared a bond that only siblings could understand.  
Head Entity: Eirik  
Tail Entity: Sigurd  

Relation: sibling  
Context: During the family reunion, it became clear that the two sisters, Maria and Elena, had grown apart over the years, but their shared childhood memories reminded them of their strong sibling connection.  
Head Entity: Maria  
Tail Entity: Elena  
#### military branch ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "military branch" means branch to which this military unit, award, office, or person belongs.
Here is an example:
Relation: military branch
Context: iain harrison is a competitive shooter and former british army captain .
Head Entity: iain harrison
Tail Entity: british army
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "military branch":
Relation: military branch  
Context: the united states navy is known for its naval operations and maritime security.  
Head Entity: united states navy  
Tail Entity: naval operations  

Relation: military branch  
Context: general patton served in the united states army during world war ii.  
Head Entity: general patton  
Tail Entity: united states army  
#### child ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "child" means subject has object as child.
Here is an example:
Relation: child
Context: developed on the east bank of the river near the former native village of otstonwakin , the borough is named for andrew montour , the french / native american and son of madame montour .
Head Entity: madame montour
Tail Entity: andrew montour
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "child":
Relation: child  
Context: In the small town of Maplewood, Sarah Johnson is known for her dedication to her family, especially her son, Michael, who is a talented musician.  
Head Entity: Sarah Johnson  
Tail Entity: Michael  

Relation: child  
Context: The famous author, J.K. Rowling, often draws inspiration from her experiences as a mother, particularly her relationship with her daughter, Jessica.  
Head Entity: J.K. Rowling  
Tail Entity: Jessica  
MemoryTrain:  epoch 15, batch     0 | loss: 7.4422926MemoryTrain:  epoch 15, batch     1 | loss: 10.1167926MemoryTrain:  epoch 15, batch     2 | loss: 7.0552404MemoryTrain:  epoch 11, batch     3 | loss: 5.3999229MemoryTrain:  epoch 15, batch     0 | loss: 7.5487493MemoryTrain:  epoch 15, batch     1 | loss: 12.6643777MemoryTrain:  epoch 15, batch     2 | loss: 7.1407521MemoryTrain:  epoch 11, batch     3 | loss: 7.7528259MemoryTrain:  epoch 15, batch     0 | loss: 5.4090667MemoryTrain:  epoch 15, batch     1 | loss: 6.4781612MemoryTrain:  epoch 15, batch     2 | loss: 4.4550846MemoryTrain:  epoch 11, batch     3 | loss: 4.2866221MemoryTrain:  epoch 15, batch     0 | loss: 7.0755259MemoryTrain:  epoch 15, batch     1 | loss: 5.5428227MemoryTrain:  epoch 15, batch     2 | loss: 4.6914370MemoryTrain:  epoch 11, batch     3 | loss: 8.2595746MemoryTrain:  epoch 15, batch     0 | loss: 4.0521786MemoryTrain:  epoch 15, batch     1 | loss: 3.4722710MemoryTrain:  epoch 15, batch     2 | loss: 5.1021138MemoryTrain:  epoch 11, batch     3 | loss: 5.7940269MemoryTrain:  epoch 15, batch     0 | loss: 5.7749749MemoryTrain:  epoch 15, batch     1 | loss: 6.3105986MemoryTrain:  epoch 15, batch     2 | loss: 7.0089439MemoryTrain:  epoch 11, batch     3 | loss: 3.7830736MemoryTrain:  epoch 15, batch     0 | loss: 6.9521467MemoryTrain:  epoch 15, batch     1 | loss: 7.0379029MemoryTrain:  epoch 15, batch     2 | loss: 5.3359445MemoryTrain:  epoch 11, batch     3 | loss: 2.5763268MemoryTrain:  epoch 15, batch     0 | loss: 6.1695358MemoryTrain:  epoch 15, batch     1 | loss: 2.7596739MemoryTrain:  epoch 15, batch     2 | loss: 3.1848000MemoryTrain:  epoch 11, batch     3 | loss: 3.5460455MemoryTrain:  epoch 15, batch     0 | loss: 4.1368483MemoryTrain:  epoch 15, batch     1 | loss: 4.1350046MemoryTrain:  epoch 15, batch     2 | loss: 5.6622363MemoryTrain:  epoch 11, batch     3 | loss: 4.8389492MemoryTrain:  epoch 15, batch     0 | loss: 4.6033847MemoryTrain:  epoch 15, batch     1 | loss: 6.0290787MemoryTrain:  epoch 15, batch     2 | loss: 2.4742249MemoryTrain:  epoch 11, batch     3 | loss: 6.5107258
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    3 | acc: 62.50%,  total acc: 76.56%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 78.75%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 79.17%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 80.36%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 81.88%   [EVAL] batch:   10 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:   12 | acc: 100.00%,  total acc: 82.69%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 83.48%   [EVAL] batch:   14 | acc: 87.50%,  total acc: 83.75%   [EVAL] batch:   15 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 85.29%   [EVAL] batch:   17 | acc: 100.00%,  total acc: 86.11%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 86.51%   [EVAL] batch:   19 | acc: 62.50%,  total acc: 85.31%   [EVAL] batch:   20 | acc: 68.75%,  total acc: 84.52%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 84.94%   [EVAL] batch:   22 | acc: 68.75%,  total acc: 84.24%   [EVAL] batch:   23 | acc: 75.00%,  total acc: 83.85%   [EVAL] batch:   24 | acc: 68.75%,  total acc: 83.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 83.89%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 84.49%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 84.82%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 85.34%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 85.83%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 86.29%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 86.52%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 86.93%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 87.32%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 87.68%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 88.02%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 88.34%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 88.65%   [EVAL] batch:   38 | acc: 87.50%,  total acc: 88.62%   [EVAL] batch:   39 | acc: 75.00%,  total acc: 88.28%   [EVAL] batch:   40 | acc: 68.75%,  total acc: 87.80%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 87.80%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 87.94%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 87.93%   [EVAL] batch:   44 | acc: 50.00%,  total acc: 87.08%   [EVAL] batch:   45 | acc: 37.50%,  total acc: 86.01%   [EVAL] batch:   46 | acc: 43.75%,  total acc: 85.11%   [EVAL] batch:   47 | acc: 62.50%,  total acc: 84.64%   [EVAL] batch:   48 | acc: 50.00%,  total acc: 83.93%   [EVAL] batch:   49 | acc: 50.00%,  total acc: 83.25%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 83.46%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 83.65%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 83.84%   [EVAL] batch:   53 | acc: 87.50%,  total acc: 83.91%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 84.09%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 84.15%   [EVAL] batch:   56 | acc: 81.25%,  total acc: 84.10%   [EVAL] batch:   57 | acc: 81.25%,  total acc: 84.05%   [EVAL] batch:   58 | acc: 75.00%,  total acc: 83.90%   [EVAL] batch:   59 | acc: 68.75%,  total acc: 83.65%   [EVAL] batch:   60 | acc: 68.75%,  total acc: 83.40%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 83.17%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 82.64%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 85.94%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 86.25%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 88.28%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 88.89%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 88.75%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 88.64%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 89.58%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 89.42%   [EVAL] batch:   13 | acc: 87.50%,  total acc: 89.29%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 90.23%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 90.44%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 91.12%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 91.25%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 91.67%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 91.48%   [EVAL] batch:   22 | acc: 87.50%,  total acc: 91.30%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 91.67%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 91.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 91.83%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 92.13%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 92.41%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 92.46%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 92.71%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 92.94%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 93.16%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 93.37%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 93.57%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 93.57%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 93.92%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 94.08%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 94.23%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 94.38%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 94.51%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 94.49%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 94.48%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 94.32%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 94.31%   [EVAL] batch:   45 | acc: 75.00%,  total acc: 93.89%   [EVAL] batch:   46 | acc: 75.00%,  total acc: 93.48%   [EVAL] batch:   47 | acc: 68.75%,  total acc: 92.97%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 93.11%   [EVAL] batch:   49 | acc: 81.25%,  total acc: 92.88%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 92.89%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 92.79%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 92.81%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 92.94%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 92.73%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 92.75%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 92.76%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 92.67%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 92.80%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 92.81%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 92.93%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 92.94%   [EVAL] batch:   62 | acc: 87.50%,  total acc: 92.86%   [EVAL] batch:   63 | acc: 87.50%,  total acc: 92.77%   [EVAL] batch:   64 | acc: 68.75%,  total acc: 92.40%   [EVAL] batch:   65 | acc: 81.25%,  total acc: 92.23%   [EVAL] batch:   66 | acc: 75.00%,  total acc: 91.98%   [EVAL] batch:   67 | acc: 93.75%,  total acc: 92.00%   [EVAL] batch:   68 | acc: 75.00%,  total acc: 91.76%   [EVAL] batch:   69 | acc: 93.75%,  total acc: 91.79%   [EVAL] batch:   70 | acc: 87.50%,  total acc: 91.73%   [EVAL] batch:   71 | acc: 75.00%,  total acc: 91.49%   [EVAL] batch:   72 | acc: 81.25%,  total acc: 91.35%   [EVAL] batch:   73 | acc: 81.25%,  total acc: 91.22%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 91.17%   [EVAL] batch:   75 | acc: 100.00%,  total acc: 91.28%   [EVAL] batch:   76 | acc: 87.50%,  total acc: 91.23%   [EVAL] batch:   77 | acc: 87.50%,  total acc: 91.19%   [EVAL] batch:   78 | acc: 100.00%,  total acc: 91.30%   [EVAL] batch:   79 | acc: 100.00%,  total acc: 91.41%   [EVAL] batch:   80 | acc: 100.00%,  total acc: 91.51%   [EVAL] batch:   81 | acc: 81.25%,  total acc: 91.39%   [EVAL] batch:   82 | acc: 56.25%,  total acc: 90.96%   [EVAL] batch:   83 | acc: 87.50%,  total acc: 90.92%   [EVAL] batch:   84 | acc: 75.00%,  total acc: 90.74%   [EVAL] batch:   85 | acc: 81.25%,  total acc: 90.62%   [EVAL] batch:   86 | acc: 75.00%,  total acc: 90.45%   [EVAL] batch:   87 | acc: 75.00%,  total acc: 90.27%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 90.38%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 90.49%   [EVAL] batch:   90 | acc: 93.75%,  total acc: 90.52%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 90.73%   [EVAL] batch:   93 | acc: 93.75%,  total acc: 90.76%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 90.86%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 90.95%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 91.04%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 91.14%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 91.22%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 91.31%   [EVAL] batch:  100 | acc: 93.75%,  total acc: 91.34%   [EVAL] batch:  101 | acc: 87.50%,  total acc: 91.30%   [EVAL] batch:  102 | acc: 62.50%,  total acc: 91.02%   [EVAL] batch:  103 | acc: 81.25%,  total acc: 90.93%   [EVAL] batch:  104 | acc: 93.75%,  total acc: 90.95%   [EVAL] batch:  105 | acc: 93.75%,  total acc: 90.98%   [EVAL] batch:  106 | acc: 62.50%,  total acc: 90.71%   [EVAL] batch:  107 | acc: 37.50%,  total acc: 90.22%   [EVAL] batch:  108 | acc: 50.00%,  total acc: 89.85%   [EVAL] batch:  109 | acc: 43.75%,  total acc: 89.43%   [EVAL] batch:  110 | acc: 62.50%,  total acc: 89.19%   [EVAL] batch:  111 | acc: 50.00%,  total acc: 88.84%   [EVAL] batch:  112 | acc: 75.00%,  total acc: 88.72%   [EVAL] batch:  113 | acc: 87.50%,  total acc: 88.71%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 88.80%   [EVAL] batch:  115 | acc: 87.50%,  total acc: 88.79%   [EVAL] batch:  116 | acc: 93.75%,  total acc: 88.84%   [EVAL] batch:  117 | acc: 93.75%,  total acc: 88.88%   [EVAL] batch:  118 | acc: 87.50%,  total acc: 88.87%   [EVAL] batch:  119 | acc: 75.00%,  total acc: 88.75%   [EVAL] batch:  120 | acc: 81.25%,  total acc: 88.69%   [EVAL] batch:  121 | acc: 68.75%,  total acc: 88.52%   [EVAL] batch:  122 | acc: 56.25%,  total acc: 88.26%   [EVAL] batch:  123 | acc: 68.75%,  total acc: 88.10%   [EVAL] batch:  124 | acc: 93.75%,  total acc: 88.15%   
cur_acc:  ['0.9524', '0.8264']
his_acc:  ['0.9524', '0.8815']
CurrentTrain: epoch 15, batch     0 | loss: 23.3132978CurrentTrain: epoch 15, batch     1 | loss: 15.4365769CurrentTrain: epoch 15, batch     2 | loss: 20.9740841CurrentTrain: epoch  1, batch     3 | loss: 16.1350670CurrentTrain: epoch 15, batch     0 | loss: 16.3090235CurrentTrain: epoch 15, batch     1 | loss: 14.6940018CurrentTrain: epoch 15, batch     2 | loss: 16.1554366CurrentTrain: epoch  1, batch     3 | loss: 13.2052955CurrentTrain: epoch 15, batch     0 | loss: 15.3951441CurrentTrain: epoch 15, batch     1 | loss: 16.6662438CurrentTrain: epoch 15, batch     2 | loss: 16.4931232CurrentTrain: epoch  1, batch     3 | loss: 9.0115158CurrentTrain: epoch 15, batch     0 | loss: 10.8791187CurrentTrain: epoch 15, batch     1 | loss: 10.7925055CurrentTrain: epoch 15, batch     2 | loss: 14.6578116CurrentTrain: epoch  1, batch     3 | loss: 6.9141497CurrentTrain: epoch 15, batch     0 | loss: 12.9816592CurrentTrain: epoch 15, batch     1 | loss: 14.8673881CurrentTrain: epoch 15, batch     2 | loss: 13.5856989CurrentTrain: epoch  1, batch     3 | loss: 6.8843382CurrentTrain: epoch 15, batch     0 | loss: 12.1536183CurrentTrain: epoch 15, batch     1 | loss: 12.8083083CurrentTrain: epoch 15, batch     2 | loss: 10.0545719CurrentTrain: epoch  1, batch     3 | loss: 7.3351501CurrentTrain: epoch 15, batch     0 | loss: 13.9281611CurrentTrain: epoch 15, batch     1 | loss: 12.0911838CurrentTrain: epoch 15, batch     2 | loss: 11.3467764CurrentTrain: epoch  1, batch     3 | loss: 8.8250014CurrentTrain: epoch 15, batch     0 | loss: 26.9069093CurrentTrain: epoch 15, batch     1 | loss: 27.2039798CurrentTrain: epoch 15, batch     2 | loss: 11.7579477CurrentTrain: epoch  1, batch     3 | loss: 25.9287997CurrentTrain: epoch 15, batch     0 | loss: 16.9228593CurrentTrain: epoch 15, batch     1 | loss: 12.3278448CurrentTrain: epoch 15, batch     2 | loss: 9.5324923CurrentTrain: epoch  1, batch     3 | loss: 6.9798464CurrentTrain: epoch 15, batch     0 | loss: 22.6589244CurrentTrain: epoch 15, batch     1 | loss: 8.8067641CurrentTrain: epoch 15, batch     2 | loss: 10.5310740CurrentTrain: epoch  1, batch     3 | loss: 7.0124394
#### main subject ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "main subject" means primary topic of a work.
Here is an example:
Relation: main subject
Context: for more information on the grand mosque seizure itself , see " the siege of mecca " by yaroslav trofimov .
Head Entity: the siege of mecca
Tail Entity: grand mosque seizure
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "main subject":
Relation: main subject  
Context: in her book "the power of habit," charles duhigg explores the science behind why habits exist and how they can be changed.  
Head Entity: the power of habit  
Tail Entity: science of habits  

Relation: main subject  
Context: "sapiens: a brief history of humankind" by yuval noah harari provides a compelling overview of the history and impact of our species.  
Head Entity: sapiens: a brief history of humankind  
Tail Entity: history of humankind  
#### participating team ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "participating team" means teams that actively takes/took part in an event or process.
Here is an example:
Relation: participating team
Context: he represented romania at euro 2000 , his first major tournament where they beat england 3–2 , and drew 1–1 against germany .
Head Entity: euro 2000
Tail Entity: germany
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "participating team":
Relation: participating team  
Context: The Brazilian national team showcased their skills at the 2014 FIFA World Cup, where they faced off against Germany in the semi-finals.  
Head Entity: 2014 FIFA World Cup  
Tail Entity: Germany  

Relation: participating team  
Context: During the 2021 Tokyo Olympics, the United States women's soccer team competed fiercely, ultimately playing against Canada in the semi-finals.  
Head Entity: 2021 Tokyo Olympics  
Tail Entity: Canada  
#### has part ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "has part" means part of this subject.
Here is an example:
Relation: has part
Context: among the competing artists was previous eurovision song contest entrant robin juhkental who represented estonia in 2010 as part of the group malcolm lincoln .
Head Entity: malcolm lincoln
Tail Entity: robin juhkental
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "has part":
Relation: has part  
Context: The famous painting "The Last Supper" features several apostles, including Peter, who is depicted prominently on the left side of the artwork.  
Head Entity: The Last Supper  
Tail Entity: Peter  

Relation: has part  
Context: The smartphone model Galaxy S21 comes with a high-resolution camera that includes a wide-angle lens, allowing users to capture stunning landscapes.  
Head Entity: Galaxy S21  
Tail Entity: wide-angle lens  
#### nominated for ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "nominated for" means award nomination received by a person, organisation or creative work.
Here is an example:
Relation: nominated for
Context: for her performance in the musical - drama " crazy heart " ( 2009 ) , she was nominated for the academy award for best supporting actress .
Head Entity: crazy heart
Tail Entity: academy award for best supporting actress
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "nominated for":
Relation: nominated for  
Context: The film "The Shape of Water" received critical acclaim and was nominated for several prestigious awards, including the Academy Award for Best Picture.  
Head Entity: The Shape of Water  
Tail Entity: Academy Award for Best Picture  

Relation: nominated for  
Context: In 2021, the popular series "The Crown" was nominated for the Emmy Award for Outstanding Drama Series, showcasing its impact on television.  
Head Entity: The Crown  
Tail Entity: Emmy Award for Outstanding Drama Series  
#### instance of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "instance of" means that class of which this subject is a particular example and member.
Here is an example:
Relation: instance of
Context: on march 2015 , cube entertainment launched the multi - national girl group clc including original members seunghee , yujin , seungyeon , sorn and yeeun .
Head Entity: clc
Tail Entity: girl group
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "instance of":
Relation: instance of  
Context: The smartphone brand Apple released its latest model, the iPhone 14, which features advanced technology and improved camera capabilities.  
Head Entity: iPhone 14  
Tail Entity: smartphone  

Relation: instance of  
Context: The famous painting "Starry Night" was created by the artist Vincent van Gogh and is considered a masterpiece of post-impressionism.  
Head Entity: Starry Night  
Tail Entity: painting  
#### mountain range ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "mountain range" means range or subrange to which the geographical item belongs.
Here is an example:
Relation: mountain range
Context: brand peak is a sharp snow - covered peak located east - southeast of the eternity range and northwest of mount duemler , in palmer land .
Head Entity: mount duemler
Tail Entity: eternity range
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "mountain range":
Relation: mountain range  
Context: the sierra nevada is a major mountain range in the western united states, known for its stunning landscapes and diverse ecosystems, with mount whitney being the highest peak in the range.  
Head Entity: mount whitney  
Tail Entity: sierra nevada  

Relation: mountain range  
Context: the appalachian mountains stretch from georgia to maine, and the great smoky mountains are a prominent subrange within this vast mountain system.  
Head Entity: great smoky mountains  
Tail Entity: appalachian mountains  
#### screenwriter ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "screenwriter" means person who wrote the script for subject item.
Here is an example:
Relation: screenwriter
Context: it would also be the last episode to feature a grounding , until " the marge - ian chronicles " in season 27 , six years later ( also written by brian kelley ) .
Head Entity: the marge - ian chronicles
Tail Entity: brian kelley
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "screenwriter":
Relation: screenwriter  
Context: The film "Inception" was a groundbreaking project that showcased the visionary talents of its creator, with the screenplay crafted by the brilliant Christopher Nolan.  
Head Entity: Inception  
Tail Entity: Christopher Nolan  

Relation: screenwriter  
Context: The beloved animated feature "Toy Story" was brought to life through the imaginative storytelling of its screenwriter, Joss Whedon, who played a pivotal role in its success.  
Head Entity: Toy Story  
Tail Entity: Joss Whedon  
#### language of work or name ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "language of work or name" means language associated with this creative work (such as books, shows, songs, broadcasts or websites) or a name.
Here is an example:
Relation: language of work or name
Context: the publisher found success in 1993 with tarō gomi 's " everyone poops " , a japanese language import that sold over one million copies and is now their best - known title .
Head Entity: everyone poops
Tail Entity: japanese language
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "language of work or name":
Relation: language of work or name  
Context: The animated series "Avatar: The Last Airbender" features a rich narrative and is primarily produced in English, appealing to a global audience.  
Head Entity: Avatar: The Last Airbender  
Tail Entity: English  

Relation: language of work or name  
Context: The famous novel "Cien años de soledad" by Gabriel García Márquez is a cornerstone of Latin American literature and is originally written in Spanish.  
Head Entity: Cien años de soledad  
Tail Entity: Spanish  
#### operator ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "operator" means person, profession, or organization that operates the equipment, facility, or service.
Here is an example:
Relation: operator
Context: saratov airlines ( saratov airlines joint stock company , , " saratovskije avialinii " ) is a russian airline headquartered in saratov and based at saratov tsentralny airport .
Head Entity: saratov tsentralny airport
Tail Entity: saratov airlines
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "operator":
Relation: operator  
Context: The New York City Transit Authority operates the subway system, providing essential transportation services to millions of commuters every day.  
Head Entity: subway system  
Tail Entity: New York City Transit Authority  

Relation: operator  
Context: Tesla, Inc. is known for its electric vehicles and operates several Gigafactories around the world to produce batteries and vehicles.  
Head Entity: Gigafactories  
Tail Entity: Tesla, Inc.  
#### religion ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "religion" means religion of a person, organization or religious building, or associated with this subject.
Here is an example:
Relation: religion
Context: the most important lama of this series was the third changkya , rolpai dorje , who was preceptor to the qianlong emperor of china , and chief representative of tibetan buddhism at the qing court .
Head Entity: rolpai dorje
Tail Entity: tibetan buddhism
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "religion":
Relation: religion  
Context: the cathedral of notre-dame is a famous example of french gothic architecture and serves as a major center for the roman catholic faith in paris.  
Head Entity: notre-dame  
Tail Entity: roman catholic faith  

Relation: religion  
Context: the dalai lama is a prominent figure in the gelug school of tibetan buddhism, advocating for peace and compassion worldwide.  
Head Entity: dalai lama  
Tail Entity: tibetan buddhism  
MemoryTrain:  epoch 15, batch     0 | loss: 3.1935095MemoryTrain:  epoch 15, batch     1 | loss: 10.0152259MemoryTrain:  epoch 15, batch     2 | loss: 4.2825769MemoryTrain:  epoch 15, batch     3 | loss: 9.2681592MemoryTrain:  epoch 15, batch     4 | loss: 7.8616422MemoryTrain:  epoch  9, batch     5 | loss: 8.1735633MemoryTrain:  epoch 15, batch     0 | loss: 4.2790295MemoryTrain:  epoch 15, batch     1 | loss: 5.3731841MemoryTrain:  epoch 15, batch     2 | loss: 4.2761859MemoryTrain:  epoch 15, batch     3 | loss: 4.0148771MemoryTrain:  epoch 15, batch     4 | loss: 13.4353556MemoryTrain:  epoch  9, batch     5 | loss: 6.1102013MemoryTrain:  epoch 15, batch     0 | loss: 4.8938461MemoryTrain:  epoch 15, batch     1 | loss: 3.8952993MemoryTrain:  epoch 15, batch     2 | loss: 5.2317393MemoryTrain:  epoch 15, batch     3 | loss: 4.0396951MemoryTrain:  epoch 15, batch     4 | loss: 5.6515707MemoryTrain:  epoch  9, batch     5 | loss: 3.4823488MemoryTrain:  epoch 15, batch     0 | loss: 4.0212903MemoryTrain:  epoch 15, batch     1 | loss: 3.2536373MemoryTrain:  epoch 15, batch     2 | loss: 5.5360225MemoryTrain:  epoch 15, batch     3 | loss: 5.9081101MemoryTrain:  epoch 15, batch     4 | loss: 7.1959250MemoryTrain:  epoch  9, batch     5 | loss: 2.8507075MemoryTrain:  epoch 15, batch     0 | loss: 8.5438133MemoryTrain:  epoch 15, batch     1 | loss: 5.2595270MemoryTrain:  epoch 15, batch     2 | loss: 2.4553522MemoryTrain:  epoch 15, batch     3 | loss: 5.4541417MemoryTrain:  epoch 15, batch     4 | loss: 3.5903305MemoryTrain:  epoch  9, batch     5 | loss: 3.6623865MemoryTrain:  epoch 15, batch     0 | loss: 3.3228278MemoryTrain:  epoch 15, batch     1 | loss: 7.0224230MemoryTrain:  epoch 15, batch     2 | loss: 3.6188984MemoryTrain:  epoch 15, batch     3 | loss: 2.8891877MemoryTrain:  epoch 15, batch     4 | loss: 4.6339896MemoryTrain:  epoch  9, batch     5 | loss: 5.7349398MemoryTrain:  epoch 15, batch     0 | loss: 3.8560646MemoryTrain:  epoch 15, batch     1 | loss: 5.3578852MemoryTrain:  epoch 15, batch     2 | loss: 1.9390570MemoryTrain:  epoch 15, batch     3 | loss: 3.5460124MemoryTrain:  epoch 15, batch     4 | loss: 3.1534899MemoryTrain:  epoch  9, batch     5 | loss: 3.8352374MemoryTrain:  epoch 15, batch     0 | loss: 4.5512533MemoryTrain:  epoch 15, batch     1 | loss: 4.7305577MemoryTrain:  epoch 15, batch     2 | loss: 4.7178321MemoryTrain:  epoch 15, batch     3 | loss: 10.7693769MemoryTrain:  epoch 15, batch     4 | loss: 5.1420774MemoryTrain:  epoch  9, batch     5 | loss: 3.8419379MemoryTrain:  epoch 15, batch     0 | loss: 2.6104141MemoryTrain:  epoch 15, batch     1 | loss: 2.9794737MemoryTrain:  epoch 15, batch     2 | loss: 4.0642252MemoryTrain:  epoch 15, batch     3 | loss: 4.9249693MemoryTrain:  epoch 15, batch     4 | loss: 2.8660196MemoryTrain:  epoch  9, batch     5 | loss: 1.6390077MemoryTrain:  epoch 15, batch     0 | loss: 4.8394749MemoryTrain:  epoch 15, batch     1 | loss: 5.2541408MemoryTrain:  epoch 15, batch     2 | loss: 4.4260596MemoryTrain:  epoch 15, batch     3 | loss: 2.6515770MemoryTrain:  epoch 15, batch     4 | loss: 3.2600117MemoryTrain:  epoch  9, batch     5 | loss: 6.9772599
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    1 | acc: 50.00%,  total acc: 56.25%   [EVAL] batch:    2 | acc: 62.50%,  total acc: 58.33%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 64.06%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 68.75%   [EVAL] batch:    5 | acc: 62.50%,  total acc: 67.71%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 72.32%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 75.78%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 77.78%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 78.75%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 80.68%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 82.29%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 81.73%   [EVAL] batch:   13 | acc: 18.75%,  total acc: 77.23%   [EVAL] batch:   14 | acc: 25.00%,  total acc: 73.75%   [EVAL] batch:   15 | acc: 18.75%,  total acc: 70.31%   [EVAL] batch:   16 | acc: 50.00%,  total acc: 69.12%   [EVAL] batch:   17 | acc: 43.75%,  total acc: 67.71%   [EVAL] batch:   18 | acc: 37.50%,  total acc: 66.12%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 67.81%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 69.35%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 70.45%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 71.74%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 72.92%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 74.00%   [EVAL] batch:   25 | acc: 37.50%,  total acc: 72.60%   [EVAL] batch:   26 | acc: 43.75%,  total acc: 71.53%   [EVAL] batch:   27 | acc: 43.75%,  total acc: 70.54%   [EVAL] batch:   28 | acc: 75.00%,  total acc: 70.69%   [EVAL] batch:   29 | acc: 37.50%,  total acc: 69.58%   [EVAL] batch:   30 | acc: 50.00%,  total acc: 68.95%   [EVAL] batch:   31 | acc: 87.50%,  total acc: 69.53%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 70.27%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 71.14%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 71.96%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 72.57%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 73.31%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 74.01%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 74.68%   [EVAL] batch:   39 | acc: 93.75%,  total acc: 75.16%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 75.76%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 76.34%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 76.89%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 77.27%   [EVAL] batch:   44 | acc: 75.00%,  total acc: 77.22%   [EVAL] batch:   45 | acc: 81.25%,  total acc: 77.31%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 77.39%   [EVAL] batch:   47 | acc: 81.25%,  total acc: 77.47%   [EVAL] batch:   48 | acc: 87.50%,  total acc: 77.68%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 77.88%   [EVAL] batch:   50 | acc: 37.50%,  total acc: 77.08%   [EVAL] batch:   51 | acc: 43.75%,  total acc: 76.44%   [EVAL] batch:   52 | acc: 68.75%,  total acc: 76.30%   [EVAL] batch:   53 | acc: 37.50%,  total acc: 75.58%   [EVAL] batch:   54 | acc: 50.00%,  total acc: 75.11%   [EVAL] batch:   55 | acc: 56.25%,  total acc: 74.78%   [EVAL] batch:   56 | acc: 81.25%,  total acc: 74.89%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 75.11%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 75.32%   [EVAL] batch:   59 | acc: 68.75%,  total acc: 75.21%   [EVAL] batch:   60 | acc: 87.50%,  total acc: 75.41%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 75.30%   [EVAL] batch:   62 | acc: 37.50%,  total acc: 74.70%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 80.00%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 82.29%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 82.03%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 82.64%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 82.50%   [EVAL] batch:   10 | acc: 75.00%,  total acc: 81.82%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 81.73%   [EVAL] batch:   13 | acc: 87.50%,  total acc: 82.14%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 82.92%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 83.98%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 84.56%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 85.07%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 85.86%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 85.94%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 86.31%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 86.36%   [EVAL] batch:   22 | acc: 75.00%,  total acc: 85.87%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 85.94%   [EVAL] batch:   24 | acc: 81.25%,  total acc: 85.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 86.30%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 86.81%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 87.28%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 87.92%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 88.31%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 88.67%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 89.02%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 89.34%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 89.46%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 89.76%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 90.03%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 90.30%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 90.54%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 90.78%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 91.01%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 91.22%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 91.28%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 91.19%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 91.25%   [EVAL] batch:   45 | acc: 75.00%,  total acc: 90.90%   [EVAL] batch:   46 | acc: 68.75%,  total acc: 90.43%   [EVAL] batch:   47 | acc: 75.00%,  total acc: 90.10%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 90.31%   [EVAL] batch:   49 | acc: 81.25%,  total acc: 90.12%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 90.20%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 90.14%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 90.21%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 90.39%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 90.34%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 90.40%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 90.46%   [EVAL] batch:   57 | acc: 68.75%,  total acc: 90.09%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 90.04%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 90.10%   [EVAL] batch:   60 | acc: 93.75%,  total acc: 90.16%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 90.12%   [EVAL] batch:   62 | acc: 87.50%,  total acc: 90.08%   [EVAL] batch:   63 | acc: 87.50%,  total acc: 90.04%   [EVAL] batch:   64 | acc: 75.00%,  total acc: 89.81%   [EVAL] batch:   65 | acc: 81.25%,  total acc: 89.68%   [EVAL] batch:   66 | acc: 81.25%,  total acc: 89.55%   [EVAL] batch:   67 | acc: 81.25%,  total acc: 89.43%   [EVAL] batch:   68 | acc: 75.00%,  total acc: 89.22%   [EVAL] batch:   69 | acc: 87.50%,  total acc: 89.20%   [EVAL] batch:   70 | acc: 81.25%,  total acc: 89.08%   [EVAL] batch:   71 | acc: 62.50%,  total acc: 88.72%   [EVAL] batch:   72 | acc: 81.25%,  total acc: 88.61%   [EVAL] batch:   73 | acc: 81.25%,  total acc: 88.51%   [EVAL] batch:   74 | acc: 81.25%,  total acc: 88.42%   [EVAL] batch:   75 | acc: 100.00%,  total acc: 88.57%   [EVAL] batch:   76 | acc: 93.75%,  total acc: 88.64%   [EVAL] batch:   77 | acc: 81.25%,  total acc: 88.54%   [EVAL] batch:   78 | acc: 93.75%,  total acc: 88.61%   [EVAL] batch:   79 | acc: 93.75%,  total acc: 88.67%   [EVAL] batch:   80 | acc: 100.00%,  total acc: 88.81%   [EVAL] batch:   81 | acc: 31.25%,  total acc: 88.11%   [EVAL] batch:   82 | acc: 18.75%,  total acc: 87.27%   [EVAL] batch:   83 | acc: 25.00%,  total acc: 86.53%   [EVAL] batch:   84 | acc: 0.00%,  total acc: 85.51%   [EVAL] batch:   85 | acc: 25.00%,  total acc: 84.81%   [EVAL] batch:   86 | acc: 18.75%,  total acc: 84.05%   [EVAL] batch:   87 | acc: 50.00%,  total acc: 83.66%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 83.85%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 84.03%   [EVAL] batch:   90 | acc: 93.75%,  total acc: 84.13%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 84.31%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 84.48%   [EVAL] batch:   93 | acc: 87.50%,  total acc: 84.51%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 84.67%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 84.83%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 84.99%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 85.14%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 85.29%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 85.44%   [EVAL] batch:  100 | acc: 93.75%,  total acc: 85.52%   [EVAL] batch:  101 | acc: 93.75%,  total acc: 85.60%   [EVAL] batch:  102 | acc: 62.50%,  total acc: 85.38%   [EVAL] batch:  103 | acc: 81.25%,  total acc: 85.34%   [EVAL] batch:  104 | acc: 93.75%,  total acc: 85.42%   [EVAL] batch:  105 | acc: 93.75%,  total acc: 85.50%   [EVAL] batch:  106 | acc: 56.25%,  total acc: 85.22%   [EVAL] batch:  107 | acc: 37.50%,  total acc: 84.78%   [EVAL] batch:  108 | acc: 43.75%,  total acc: 84.40%   [EVAL] batch:  109 | acc: 31.25%,  total acc: 83.92%   [EVAL] batch:  110 | acc: 62.50%,  total acc: 83.73%   [EVAL] batch:  111 | acc: 43.75%,  total acc: 83.37%   [EVAL] batch:  112 | acc: 75.00%,  total acc: 83.30%   [EVAL] batch:  113 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 83.48%   [EVAL] batch:  115 | acc: 81.25%,  total acc: 83.46%   [EVAL] batch:  116 | acc: 93.75%,  total acc: 83.55%   [EVAL] batch:  117 | acc: 93.75%,  total acc: 83.63%   [EVAL] batch:  118 | acc: 93.75%,  total acc: 83.72%   [EVAL] batch:  119 | acc: 68.75%,  total acc: 83.59%   [EVAL] batch:  120 | acc: 75.00%,  total acc: 83.52%   [EVAL] batch:  121 | acc: 75.00%,  total acc: 83.45%   [EVAL] batch:  122 | acc: 62.50%,  total acc: 83.28%   [EVAL] batch:  123 | acc: 75.00%,  total acc: 83.22%   [EVAL] batch:  124 | acc: 87.50%,  total acc: 83.25%   [EVAL] batch:  125 | acc: 62.50%,  total acc: 83.09%   [EVAL] batch:  126 | acc: 50.00%,  total acc: 82.82%   [EVAL] batch:  127 | acc: 62.50%,  total acc: 82.67%   [EVAL] batch:  128 | acc: 81.25%,  total acc: 82.66%   [EVAL] batch:  129 | acc: 87.50%,  total acc: 82.69%   [EVAL] batch:  130 | acc: 62.50%,  total acc: 82.54%   [EVAL] batch:  131 | acc: 100.00%,  total acc: 82.67%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 82.80%   [EVAL] batch:  133 | acc: 93.75%,  total acc: 82.88%   [EVAL] batch:  134 | acc: 87.50%,  total acc: 82.92%   [EVAL] batch:  135 | acc: 100.00%,  total acc: 83.04%   [EVAL] batch:  136 | acc: 100.00%,  total acc: 83.17%   [EVAL] batch:  137 | acc: 75.00%,  total acc: 83.11%   [EVAL] batch:  138 | acc: 18.75%,  total acc: 82.64%   [EVAL] batch:  139 | acc: 25.00%,  total acc: 82.23%   [EVAL] batch:  140 | acc: 18.75%,  total acc: 81.78%   [EVAL] batch:  141 | acc: 50.00%,  total acc: 81.56%   [EVAL] batch:  142 | acc: 43.75%,  total acc: 81.29%   [EVAL] batch:  143 | acc: 37.50%,  total acc: 80.99%   [EVAL] batch:  144 | acc: 100.00%,  total acc: 81.12%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 81.34%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 81.46%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 81.59%   [EVAL] batch:  149 | acc: 100.00%,  total acc: 81.71%   [EVAL] batch:  150 | acc: 37.50%,  total acc: 81.42%   [EVAL] batch:  151 | acc: 43.75%,  total acc: 81.17%   [EVAL] batch:  152 | acc: 43.75%,  total acc: 80.92%   [EVAL] batch:  153 | acc: 75.00%,  total acc: 80.88%   [EVAL] batch:  154 | acc: 37.50%,  total acc: 80.60%   [EVAL] batch:  155 | acc: 50.00%,  total acc: 80.41%   [EVAL] batch:  156 | acc: 87.50%,  total acc: 80.45%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 80.54%   [EVAL] batch:  158 | acc: 100.00%,  total acc: 80.66%   [EVAL] batch:  159 | acc: 100.00%,  total acc: 80.78%   [EVAL] batch:  160 | acc: 93.75%,  total acc: 80.86%   [EVAL] batch:  161 | acc: 100.00%,  total acc: 80.98%   [EVAL] batch:  162 | acc: 100.00%,  total acc: 81.10%   [EVAL] batch:  163 | acc: 100.00%,  total acc: 81.21%   [EVAL] batch:  164 | acc: 93.75%,  total acc: 81.29%   [EVAL] batch:  165 | acc: 100.00%,  total acc: 81.40%   [EVAL] batch:  166 | acc: 100.00%,  total acc: 81.51%   [EVAL] batch:  167 | acc: 100.00%,  total acc: 81.62%   [EVAL] batch:  168 | acc: 93.75%,  total acc: 81.69%   [EVAL] batch:  169 | acc: 75.00%,  total acc: 81.65%   [EVAL] batch:  170 | acc: 81.25%,  total acc: 81.65%   [EVAL] batch:  171 | acc: 81.25%,  total acc: 81.65%   [EVAL] batch:  172 | acc: 81.25%,  total acc: 81.65%   [EVAL] batch:  173 | acc: 87.50%,  total acc: 81.68%   [EVAL] batch:  174 | acc: 87.50%,  total acc: 81.71%   [EVAL] batch:  175 | acc: 37.50%,  total acc: 81.46%   [EVAL] batch:  176 | acc: 43.75%,  total acc: 81.25%   [EVAL] batch:  177 | acc: 68.75%,  total acc: 81.18%   [EVAL] batch:  178 | acc: 37.50%,  total acc: 80.94%   [EVAL] batch:  179 | acc: 50.00%,  total acc: 80.76%   [EVAL] batch:  180 | acc: 56.25%,  total acc: 80.63%   [EVAL] batch:  181 | acc: 81.25%,  total acc: 80.63%   [EVAL] batch:  182 | acc: 87.50%,  total acc: 80.67%   [EVAL] batch:  183 | acc: 87.50%,  total acc: 80.71%   [EVAL] batch:  184 | acc: 68.75%,  total acc: 80.64%   [EVAL] batch:  185 | acc: 87.50%,  total acc: 80.68%   [EVAL] batch:  186 | acc: 68.75%,  total acc: 80.61%   [EVAL] batch:  187 | acc: 37.50%,  total acc: 80.39%   
cur_acc:  ['0.9524', '0.8264', '0.7470']
his_acc:  ['0.9524', '0.8815', '0.8039']
CurrentTrain: epoch 15, batch     0 | loss: 26.5955498CurrentTrain: epoch 15, batch     1 | loss: 31.5570877CurrentTrain: epoch 15, batch     2 | loss: 16.3131831CurrentTrain: epoch  1, batch     3 | loss: 8.9403202CurrentTrain: epoch 15, batch     0 | loss: 15.6448086CurrentTrain: epoch 15, batch     1 | loss: 11.9395053CurrentTrain: epoch 15, batch     2 | loss: 29.2588842CurrentTrain: epoch  1, batch     3 | loss: 6.3172726CurrentTrain: epoch 15, batch     0 | loss: 11.0247688CurrentTrain: epoch 15, batch     1 | loss: 26.9138146CurrentTrain: epoch 15, batch     2 | loss: 14.0910116CurrentTrain: epoch  1, batch     3 | loss: 8.3650703CurrentTrain: epoch 15, batch     0 | loss: 14.7679550CurrentTrain: epoch 15, batch     1 | loss: 10.9783712CurrentTrain: epoch 15, batch     2 | loss: 13.4734691CurrentTrain: epoch  1, batch     3 | loss: 8.9483759CurrentTrain: epoch 15, batch     0 | loss: 11.1325220CurrentTrain: epoch 15, batch     1 | loss: 18.8388324CurrentTrain: epoch 15, batch     2 | loss: 25.9466732CurrentTrain: epoch  1, batch     3 | loss: 8.1471383CurrentTrain: epoch 15, batch     0 | loss: 12.8840731CurrentTrain: epoch 15, batch     1 | loss: 10.2703245CurrentTrain: epoch 15, batch     2 | loss: 14.2195624CurrentTrain: epoch  1, batch     3 | loss: 11.5352018CurrentTrain: epoch 15, batch     0 | loss: 9.2208165CurrentTrain: epoch 15, batch     1 | loss: 7.4844664CurrentTrain: epoch 15, batch     2 | loss: 9.7938886CurrentTrain: epoch  1, batch     3 | loss: 8.6237504CurrentTrain: epoch 15, batch     0 | loss: 8.6313500CurrentTrain: epoch 15, batch     1 | loss: 12.4505925CurrentTrain: epoch 15, batch     2 | loss: 9.9240603CurrentTrain: epoch  1, batch     3 | loss: 6.1928189CurrentTrain: epoch 15, batch     0 | loss: 15.2753509CurrentTrain: epoch 15, batch     1 | loss: 11.6874771CurrentTrain: epoch 15, batch     2 | loss: 12.8976078CurrentTrain: epoch  1, batch     3 | loss: 14.5047346CurrentTrain: epoch 15, batch     0 | loss: 22.6291609CurrentTrain: epoch 15, batch     1 | loss: 6.8797843CurrentTrain: epoch 15, batch     2 | loss: 12.0307325CurrentTrain: epoch  1, batch     3 | loss: 6.4112654
#### follows ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "follows" means immediately prior item in a series of which the subject is a part.
Here is an example:
Relation: follows
Context: ( 1988 ) , " night watch " ( 2004 ) , " day watch " ( 2006 ) and " legend № 17 " ( 2013 ) .
Head Entity: day watch
Tail Entity: night watch
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "follows":
Relation: follows  
Context: ( 1995 ) , " The Phantom Menace " ( 1999 ) , " Attack of the Clones " ( 2002 ) and " Revenge of the Sith " ( 2005 ) .  
Head Entity: Attack of the Clones  
Tail Entity: The Phantom Menace  

Relation: follows  
Context: ( 2010 ) , " The Hunger Games " ( 2012 ) , " Catching Fire " ( 2013 ) and " Mockingjay " ( 2014 ) .  
Head Entity: Catching Fire  
Tail Entity: The Hunger Games  
#### sport ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "sport" means sport that the subject participates or participated in or is associated with.
Here is an example:
Relation: sport
Context: dick packer was a u.s. soccer center forward who was a member of the u.s. team at the 1956 summer olympics .
Head Entity: dick packer
Tail Entity: soccer
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "sport":
Relation: sport  
Context: serena williams is a professional tennis player who has won numerous grand slam titles throughout her career.  
Head Entity: serena williams  
Tail Entity: tennis  

Relation: sport  
Context: lebron james is known for his exceptional skills in basketball and has played for the los angeles lakers in the nba.  
Head Entity: lebron james  
Tail Entity: basketball  
#### father ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "father" means male parent of the subject.
Here is an example:
Relation: father
Context: after the crowning of his nephew radoslav , the son of stefan , sava left the serbian maritime in 1229 for a trip to palestine .
Head Entity: radoslav
Tail Entity: stefan
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "father":
Relation: father  
Context: in the historical records, it is noted that king henry was the father of prince edward, who later became a significant figure in the kingdom.  
Head Entity: prince edward  
Tail Entity: king henry  

Relation: father  
Context: during the family reunion, it was revealed that michael, the youngest son, was the father of three children, making him the proud parent in the family.  
Head Entity: michael  
Tail Entity: unknown parent
#### original network ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "original network" means network or service that originally broadcast a radio or television program.
Here is an example:
Relation: original network
Context: he also made a special appearance in kbs ' romantic comedy drama " fight for my way " .
Head Entity: fight for my way
Tail Entity: kbs
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "original network":
Relation: original network  
Context: the popular series "breaking bad" was first aired on amc, captivating audiences with its intense storytelling.  
Head Entity: breaking bad  
Tail Entity: amc  

Relation: original network  
Context: the animated show "the simpsons" has been a staple of fox's programming since its debut in 1989.  
Head Entity: the simpsons  
Tail Entity: fox  
#### original language of film or TV show ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "original language of film or TV show" means language in which a film or a performance work was originally created.
Here is an example:
Relation: original language of film or TV show
Context: he was also nominated twice for the filmfare best telugu actor award , for the films bhale bhale magadivoy and " gentleman " .
Head Entity: gentleman
Tail Entity: telugu
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "original language of film or TV show":
Relation: original language of film or TV show  
Context: The movie "Parasite" received critical acclaim and was originally produced in Korean, showcasing the unique storytelling style of South Korean cinema.  
Head Entity: Parasite  
Tail Entity: Korean  

Relation: original language of film or TV show  
Context: The animated series "Avatar: The Last Airbender" was created in English, captivating audiences with its rich world-building and character development.  
Head Entity: Avatar: The Last Airbender  
Tail Entity: English  
#### league ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "league" means league in which team or player plays or has played in.
Here is an example:
Relation: league
Context: in the summer of 2013 he returned to montenegro and joined with fk rudar pljevlja , playing with the club for two seasons in the montenegrin first league .
Head Entity: fk rudar pljevlja
Tail Entity: montenegrin first league
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "league":
Relation: league  
Context: After a successful stint in the English Premier League, the player signed with FC Barcelona, competing in La Liga for the next three seasons.  
Head Entity: FC Barcelona  
Tail Entity: La Liga  

Relation: league  
Context: The young athlete made headlines when he joined the New York Yankees, showcasing his talent in Major League Baseball.  
Head Entity: New York Yankees  
Tail Entity: Major League Baseball  
#### mother ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "mother" means female parent of the subject.
Here is an example:
Relation: mother
Context: the first stewart king of scotland and son of marjorie bruce and walter stewart , robert ii , is believed to have been born in the abbey .
Head Entity: robert ii
Tail Entity: marjorie bruce
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "mother":
Relation: mother  
Context: in the historical records, it is noted that queen elizabeth i was the daughter of anne boleyn, who served as her mother during her early years.  
Head Entity: queen elizabeth i  
Tail Entity: anne boleyn  

Relation: mother  
Context: the famous artist pablo picasso often credited his mother, maría ruiz, as a significant influence on his early artistic development.  
Head Entity: pablo picasso  
Tail Entity: maría ruiz  
#### instrument ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "instrument" means musical instrument that a person plays or teaches or used in a music occupation.
Here is an example:
Relation: instrument
Context: peter bastian ( 25 august 1943 in copenhagen – 27 march 2017 ) was a danish musician . peter bastian bio and cv he was educated in theoretical physics and classical bassoon .
Head Entity: peter bastian
Tail Entity: bassoon
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "instrument":
Relation: instrument  
Context: johann sebastian bach (31 march 1685 – 28 july 1750) was a german composer and musician of the baroque period. he was known for his exceptional skill in playing the organ and the harpsichord.  
Head Entity: johann sebastian bach  
Tail Entity: organ  

Relation: instrument  
Context: yo-yo ma, born on october 7, 1955, is a world-renowned cellist who has performed with numerous orchestras and in various genres. he is particularly famous for his interpretations of classical cello concertos.  
Head Entity: yo-yo ma  
Tail Entity: cello  
#### country ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "country" means sovereign state of this item (not to be used for human beings).
Here is an example:
Relation: country
Context: a u.s. government - funded $ 36 million bridge over the panj river connects sher khan bandar in afghanistan with nizhniy pyanzh in tajikistan , which transport more than 150 trucks or 1,000 cars daily .
Head Entity: sher khan bandar
Tail Entity: afghanistan
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "country":
Relation: country  
Context: the ancient city of carthage, located in present-day tunisia, was a major power in the mediterranean before its destruction in 146 bc.  
Head Entity: carthage  
Tail Entity: tunisia  

Relation: country  
Context: the famous pyramids of giza are located near cairo, which is the capital of egypt and one of the most populous cities in africa.  
Head Entity: giza  
Tail Entity: egypt  
#### characters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "characters" means characters which appear in this item (like plays, operas, operettas, books, comics, films, TV series, video games).
Here is an example:
Relation: characters
Context: klaus kinski , who would play dracula himself nine years later in " nosferatu the vampyre " , is also featured in the film as renfield .
Head Entity: nosferatu the vampyre
Tail Entity: renfield
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "characters":
Relation: characters  
Context: in the animated series "avatar: the last airbender," zuko is a complex character who struggles with his identity and family legacy.  
Head Entity: avatar: the last airbender  
Tail Entity: zuko  

Relation: characters  
Context: the novel "pride and prejudice" features elizabeth bennet, a strong-willed protagonist who navigates societal expectations and personal relationships.  
Head Entity: pride and prejudice  
Tail Entity: elizabeth bennet  
MemoryTrain:  epoch 15, batch     0 | loss: 5.9187267MemoryTrain:  epoch 15, batch     1 | loss: 3.7605726MemoryTrain:  epoch 15, batch     2 | loss: 4.5009657MemoryTrain:  epoch 15, batch     3 | loss: 4.4494827MemoryTrain:  epoch 15, batch     4 | loss: 5.8982500MemoryTrain:  epoch 15, batch     5 | loss: 11.5981171MemoryTrain:  epoch 15, batch     6 | loss: 4.0934054MemoryTrain:  epoch  7, batch     7 | loss: 4.1435778MemoryTrain:  epoch 15, batch     0 | loss: 4.2636808MemoryTrain:  epoch 15, batch     1 | loss: 3.4548831MemoryTrain:  epoch 15, batch     2 | loss: 4.8636536MemoryTrain:  epoch 15, batch     3 | loss: 5.0510467MemoryTrain:  epoch 15, batch     4 | loss: 11.7564953MemoryTrain:  epoch 15, batch     5 | loss: 4.8508899MemoryTrain:  epoch 15, batch     6 | loss: 7.1319702MemoryTrain:  epoch  7, batch     7 | loss: 5.0437330MemoryTrain:  epoch 15, batch     0 | loss: 2.9371605MemoryTrain:  epoch 15, batch     1 | loss: 3.4669299MemoryTrain:  epoch 15, batch     2 | loss: 3.5518715MemoryTrain:  epoch 15, batch     3 | loss: 3.5983934MemoryTrain:  epoch 15, batch     4 | loss: 5.6630600MemoryTrain:  epoch 15, batch     5 | loss: 5.4271243MemoryTrain:  epoch 15, batch     6 | loss: 3.7274603MemoryTrain:  epoch  7, batch     7 | loss: 2.7339450MemoryTrain:  epoch 15, batch     0 | loss: 3.1166932MemoryTrain:  epoch 15, batch     1 | loss: 5.1807470MemoryTrain:  epoch 15, batch     2 | loss: 3.4686488MemoryTrain:  epoch 15, batch     3 | loss: 1.8726713MemoryTrain:  epoch 15, batch     4 | loss: 2.9430751MemoryTrain:  epoch 15, batch     5 | loss: 2.1457519MemoryTrain:  epoch 15, batch     6 | loss: 2.5594093MemoryTrain:  epoch  7, batch     7 | loss: 3.4518993MemoryTrain:  epoch 15, batch     0 | loss: 3.0522372MemoryTrain:  epoch 15, batch     1 | loss: 3.7243374MemoryTrain:  epoch 15, batch     2 | loss: 2.8854313MemoryTrain:  epoch 15, batch     3 | loss: 2.1968209MemoryTrain:  epoch 15, batch     4 | loss: 4.6011163MemoryTrain:  epoch 15, batch     5 | loss: 2.2742246MemoryTrain:  epoch 15, batch     6 | loss: 3.5409111MemoryTrain:  epoch  7, batch     7 | loss: 1.9637935MemoryTrain:  epoch 15, batch     0 | loss: 5.4118696MemoryTrain:  epoch 15, batch     1 | loss: 3.1953550MemoryTrain:  epoch 15, batch     2 | loss: 3.4585001MemoryTrain:  epoch 15, batch     3 | loss: 3.1828205MemoryTrain:  epoch 15, batch     4 | loss: 2.3734879MemoryTrain:  epoch 15, batch     5 | loss: 3.4624333MemoryTrain:  epoch 15, batch     6 | loss: 2.6255397MemoryTrain:  epoch  7, batch     7 | loss: 3.1552717MemoryTrain:  epoch 15, batch     0 | loss: 2.1811753MemoryTrain:  epoch 15, batch     1 | loss: 2.8975930MemoryTrain:  epoch 15, batch     2 | loss: 2.0812500MemoryTrain:  epoch 15, batch     3 | loss: 3.5202343MemoryTrain:  epoch 15, batch     4 | loss: 3.1694076MemoryTrain:  epoch 15, batch     5 | loss: 2.6558791MemoryTrain:  epoch 15, batch     6 | loss: 2.1963144MemoryTrain:  epoch  7, batch     7 | loss: 2.4572200MemoryTrain:  epoch 15, batch     0 | loss: 2.4047323MemoryTrain:  epoch 15, batch     1 | loss: 3.4870373MemoryTrain:  epoch 15, batch     2 | loss: 2.1566221MemoryTrain:  epoch 15, batch     3 | loss: 5.1336879MemoryTrain:  epoch 15, batch     4 | loss: 4.9610487MemoryTrain:  epoch 15, batch     5 | loss: 2.4809008MemoryTrain:  epoch 15, batch     6 | loss: 5.0783073MemoryTrain:  epoch  7, batch     7 | loss: 4.4612344MemoryTrain:  epoch 15, batch     0 | loss: 5.8607307MemoryTrain:  epoch 15, batch     1 | loss: 3.0815435MemoryTrain:  epoch 15, batch     2 | loss: 1.7741827MemoryTrain:  epoch 15, batch     3 | loss: 5.2305100MemoryTrain:  epoch 15, batch     4 | loss: 3.7672920MemoryTrain:  epoch 15, batch     5 | loss: 7.6334388MemoryTrain:  epoch 15, batch     6 | loss: 3.7862437MemoryTrain:  epoch  7, batch     7 | loss: 1.4550643MemoryTrain:  epoch 15, batch     0 | loss: 5.3699513MemoryTrain:  epoch 15, batch     1 | loss: 2.4878962MemoryTrain:  epoch 15, batch     2 | loss: 4.6202386MemoryTrain:  epoch 15, batch     3 | loss: 1.9215365MemoryTrain:  epoch 15, batch     4 | loss: 3.1135016MemoryTrain:  epoch 15, batch     5 | loss: 5.0576617MemoryTrain:  epoch 15, batch     6 | loss: 2.2025974MemoryTrain:  epoch  7, batch     7 | loss: 4.4313893
[EVAL] batch:    0 | acc: 31.25%,  total acc: 31.25%   [EVAL] batch:    1 | acc: 37.50%,  total acc: 34.38%   [EVAL] batch:    2 | acc: 25.00%,  total acc: 31.25%   [EVAL] batch:    3 | acc: 50.00%,  total acc: 35.94%   [EVAL] batch:    4 | acc: 18.75%,  total acc: 32.50%   [EVAL] batch:    5 | acc: 43.75%,  total acc: 34.38%   [EVAL] batch:    6 | acc: 62.50%,  total acc: 38.39%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 45.31%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 50.00%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 53.75%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 56.25%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 58.85%   [EVAL] batch:   12 | acc: 50.00%,  total acc: 58.17%   [EVAL] batch:   13 | acc: 56.25%,  total acc: 58.04%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 58.75%   [EVAL] batch:   15 | acc: 43.75%,  total acc: 57.81%   [EVAL] batch:   16 | acc: 62.50%,  total acc: 58.09%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 58.68%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 58.88%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 60.94%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 62.80%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 64.20%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 65.76%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 67.19%   [EVAL] batch:   24 | acc: 93.75%,  total acc: 68.25%   [EVAL] batch:   25 | acc: 93.75%,  total acc: 69.23%   [EVAL] batch:   26 | acc: 87.50%,  total acc: 69.91%   [EVAL] batch:   27 | acc: 81.25%,  total acc: 70.31%   [EVAL] batch:   28 | acc: 87.50%,  total acc: 70.91%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 71.25%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 71.98%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 72.66%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 73.48%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 74.08%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 74.64%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 75.17%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 75.84%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 76.15%   [EVAL] batch:   38 | acc: 81.25%,  total acc: 76.28%   [EVAL] batch:   39 | acc: 68.75%,  total acc: 76.09%   [EVAL] batch:   40 | acc: 75.00%,  total acc: 76.07%   [EVAL] batch:   41 | acc: 62.50%,  total acc: 75.74%   [EVAL] batch:   42 | acc: 68.75%,  total acc: 75.58%   [EVAL] batch:   43 | acc: 75.00%,  total acc: 75.57%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 76.11%   [EVAL] batch:   45 | acc: 87.50%,  total acc: 76.36%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 76.86%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 77.34%   [EVAL] batch:   48 | acc: 93.75%,  total acc: 77.68%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 78.12%   [EVAL] batch:   50 | acc: 56.25%,  total acc: 77.70%   [EVAL] batch:   51 | acc: 62.50%,  total acc: 77.40%   [EVAL] batch:   52 | acc: 56.25%,  total acc: 77.00%   [EVAL] batch:   53 | acc: 68.75%,  total acc: 76.85%   [EVAL] batch:   54 | acc: 75.00%,  total acc: 76.82%   [EVAL] batch:   55 | acc: 31.25%,  total acc: 76.00%   [EVAL] batch:   56 | acc: 43.75%,  total acc: 75.44%   [EVAL] batch:   57 | acc: 81.25%,  total acc: 75.54%   [EVAL] batch:   58 | acc: 56.25%,  total acc: 75.21%   [EVAL] batch:   59 | acc: 75.00%,  total acc: 75.21%   [EVAL] batch:   60 | acc: 87.50%,  total acc: 75.41%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 75.71%   [EVAL] batch:   62 | acc: 43.75%,  total acc: 75.20%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 83.33%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 82.14%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 82.03%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 81.94%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:   10 | acc: 75.00%,  total acc: 80.68%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 80.73%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 82.14%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 82.92%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 83.98%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 84.56%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 85.07%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 85.86%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 85.62%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 86.01%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 86.08%   [EVAL] batch:   22 | acc: 68.75%,  total acc: 85.33%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 85.42%   [EVAL] batch:   24 | acc: 81.25%,  total acc: 85.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 85.82%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 86.34%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 86.83%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 87.07%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 87.90%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 88.28%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 88.64%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 88.97%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 89.11%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 89.41%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 89.70%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 89.97%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 90.22%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 90.47%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 90.70%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 90.77%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 90.84%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 90.77%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 90.69%   [EVAL] batch:   45 | acc: 75.00%,  total acc: 90.35%   [EVAL] batch:   46 | acc: 68.75%,  total acc: 89.89%   [EVAL] batch:   47 | acc: 75.00%,  total acc: 89.58%   [EVAL] batch:   48 | acc: 93.75%,  total acc: 89.67%   [EVAL] batch:   49 | acc: 81.25%,  total acc: 89.50%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 89.54%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 89.62%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 89.81%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 89.66%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 89.73%   [EVAL] batch:   56 | acc: 87.50%,  total acc: 89.69%   [EVAL] batch:   57 | acc: 62.50%,  total acc: 89.22%   [EVAL] batch:   58 | acc: 68.75%,  total acc: 88.88%   [EVAL] batch:   59 | acc: 87.50%,  total acc: 88.85%   [EVAL] batch:   60 | acc: 93.75%,  total acc: 88.93%   [EVAL] batch:   61 | acc: 81.25%,  total acc: 88.81%   [EVAL] batch:   62 | acc: 87.50%,  total acc: 88.79%   [EVAL] batch:   63 | acc: 68.75%,  total acc: 88.48%   [EVAL] batch:   64 | acc: 75.00%,  total acc: 88.27%   [EVAL] batch:   65 | acc: 81.25%,  total acc: 88.16%   [EVAL] batch:   66 | acc: 75.00%,  total acc: 87.97%   [EVAL] batch:   67 | acc: 93.75%,  total acc: 88.05%   [EVAL] batch:   68 | acc: 81.25%,  total acc: 87.95%   [EVAL] batch:   69 | acc: 87.50%,  total acc: 87.95%   [EVAL] batch:   70 | acc: 68.75%,  total acc: 87.68%   [EVAL] batch:   71 | acc: 56.25%,  total acc: 87.24%   [EVAL] batch:   72 | acc: 81.25%,  total acc: 87.16%   [EVAL] batch:   73 | acc: 75.00%,  total acc: 86.99%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 87.00%   [EVAL] batch:   75 | acc: 100.00%,  total acc: 87.17%   [EVAL] batch:   76 | acc: 93.75%,  total acc: 87.26%   [EVAL] batch:   77 | acc: 81.25%,  total acc: 87.18%   [EVAL] batch:   78 | acc: 93.75%,  total acc: 87.26%   [EVAL] batch:   79 | acc: 100.00%,  total acc: 87.42%   [EVAL] batch:   80 | acc: 100.00%,  total acc: 87.58%   [EVAL] batch:   81 | acc: 25.00%,  total acc: 86.81%   [EVAL] batch:   82 | acc: 12.50%,  total acc: 85.92%   [EVAL] batch:   83 | acc: 0.00%,  total acc: 84.90%   [EVAL] batch:   84 | acc: 0.00%,  total acc: 83.90%   [EVAL] batch:   85 | acc: 6.25%,  total acc: 82.99%   [EVAL] batch:   86 | acc: 12.50%,  total acc: 82.18%   [EVAL] batch:   87 | acc: 43.75%,  total acc: 81.75%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 81.95%   [EVAL] batch:   89 | acc: 93.75%,  total acc: 82.08%   [EVAL] batch:   90 | acc: 87.50%,  total acc: 82.14%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 82.34%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 82.53%   [EVAL] batch:   93 | acc: 87.50%,  total acc: 82.58%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 82.76%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 82.94%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 83.12%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 83.29%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 83.46%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 83.62%   [EVAL] batch:  100 | acc: 93.75%,  total acc: 83.73%   [EVAL] batch:  101 | acc: 93.75%,  total acc: 83.82%   [EVAL] batch:  102 | acc: 87.50%,  total acc: 83.86%   [EVAL] batch:  103 | acc: 87.50%,  total acc: 83.89%   [EVAL] batch:  104 | acc: 100.00%,  total acc: 84.05%   [EVAL] batch:  105 | acc: 93.75%,  total acc: 84.14%   [EVAL] batch:  106 | acc: 50.00%,  total acc: 83.82%   [EVAL] batch:  107 | acc: 25.00%,  total acc: 83.28%   [EVAL] batch:  108 | acc: 31.25%,  total acc: 82.80%   [EVAL] batch:  109 | acc: 37.50%,  total acc: 82.39%   [EVAL] batch:  110 | acc: 56.25%,  total acc: 82.15%   [EVAL] batch:  111 | acc: 37.50%,  total acc: 81.75%   [EVAL] batch:  112 | acc: 68.75%,  total acc: 81.64%   [EVAL] batch:  113 | acc: 87.50%,  total acc: 81.69%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 81.85%   [EVAL] batch:  115 | acc: 81.25%,  total acc: 81.84%   [EVAL] batch:  116 | acc: 93.75%,  total acc: 81.94%   [EVAL] batch:  117 | acc: 93.75%,  total acc: 82.04%   [EVAL] batch:  118 | acc: 68.75%,  total acc: 81.93%   [EVAL] batch:  119 | acc: 6.25%,  total acc: 81.30%   [EVAL] batch:  120 | acc: 31.25%,  total acc: 80.89%   [EVAL] batch:  121 | acc: 0.00%,  total acc: 80.23%   [EVAL] batch:  122 | acc: 12.50%,  total acc: 79.67%   [EVAL] batch:  123 | acc: 6.25%,  total acc: 79.08%   [EVAL] batch:  124 | acc: 0.00%,  total acc: 78.45%   [EVAL] batch:  125 | acc: 50.00%,  total acc: 78.22%   [EVAL] batch:  126 | acc: 37.50%,  total acc: 77.90%   [EVAL] batch:  127 | acc: 56.25%,  total acc: 77.73%   [EVAL] batch:  128 | acc: 81.25%,  total acc: 77.76%   [EVAL] batch:  129 | acc: 56.25%,  total acc: 77.60%   [EVAL] batch:  130 | acc: 62.50%,  total acc: 77.48%   [EVAL] batch:  131 | acc: 87.50%,  total acc: 77.56%   [EVAL] batch:  132 | acc: 81.25%,  total acc: 77.58%   [EVAL] batch:  133 | acc: 62.50%,  total acc: 77.47%   [EVAL] batch:  134 | acc: 87.50%,  total acc: 77.55%   [EVAL] batch:  135 | acc: 87.50%,  total acc: 77.62%   [EVAL] batch:  136 | acc: 87.50%,  total acc: 77.69%   [EVAL] batch:  137 | acc: 62.50%,  total acc: 77.58%   [EVAL] batch:  138 | acc: 18.75%,  total acc: 77.16%   [EVAL] batch:  139 | acc: 31.25%,  total acc: 76.83%   [EVAL] batch:  140 | acc: 25.00%,  total acc: 76.46%   [EVAL] batch:  141 | acc: 43.75%,  total acc: 76.23%   [EVAL] batch:  142 | acc: 43.75%,  total acc: 76.01%   [EVAL] batch:  143 | acc: 37.50%,  total acc: 75.74%   [EVAL] batch:  144 | acc: 100.00%,  total acc: 75.91%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 76.07%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 76.19%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 76.35%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 76.51%   [EVAL] batch:  149 | acc: 100.00%,  total acc: 76.67%   [EVAL] batch:  150 | acc: 31.25%,  total acc: 76.37%   [EVAL] batch:  151 | acc: 12.50%,  total acc: 75.95%   [EVAL] batch:  152 | acc: 25.00%,  total acc: 75.61%   [EVAL] batch:  153 | acc: 62.50%,  total acc: 75.53%   [EVAL] batch:  154 | acc: 37.50%,  total acc: 75.28%   [EVAL] batch:  155 | acc: 43.75%,  total acc: 75.08%   [EVAL] batch:  156 | acc: 87.50%,  total acc: 75.16%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 75.28%   [EVAL] batch:  158 | acc: 100.00%,  total acc: 75.43%   [EVAL] batch:  159 | acc: 100.00%,  total acc: 75.59%   [EVAL] batch:  160 | acc: 93.75%,  total acc: 75.70%   [EVAL] batch:  161 | acc: 100.00%,  total acc: 75.85%   [EVAL] batch:  162 | acc: 100.00%,  total acc: 76.00%   [EVAL] batch:  163 | acc: 87.50%,  total acc: 76.07%   [EVAL] batch:  164 | acc: 93.75%,  total acc: 76.17%   [EVAL] batch:  165 | acc: 87.50%,  total acc: 76.24%   [EVAL] batch:  166 | acc: 100.00%,  total acc: 76.38%   [EVAL] batch:  167 | acc: 100.00%,  total acc: 76.53%   [EVAL] batch:  168 | acc: 93.75%,  total acc: 76.63%   [EVAL] batch:  169 | acc: 56.25%,  total acc: 76.51%   [EVAL] batch:  170 | acc: 75.00%,  total acc: 76.50%   [EVAL] batch:  171 | acc: 50.00%,  total acc: 76.34%   [EVAL] batch:  172 | acc: 37.50%,  total acc: 76.12%   [EVAL] batch:  173 | acc: 56.25%,  total acc: 76.01%   [EVAL] batch:  174 | acc: 56.25%,  total acc: 75.89%   [EVAL] batch:  175 | acc: 43.75%,  total acc: 75.71%   [EVAL] batch:  176 | acc: 31.25%,  total acc: 75.46%   [EVAL] batch:  177 | acc: 56.25%,  total acc: 75.35%   [EVAL] batch:  178 | acc: 37.50%,  total acc: 75.14%   [EVAL] batch:  179 | acc: 50.00%,  total acc: 75.00%   [EVAL] batch:  180 | acc: 56.25%,  total acc: 74.90%   [EVAL] batch:  181 | acc: 87.50%,  total acc: 74.97%   [EVAL] batch:  182 | acc: 87.50%,  total acc: 75.03%   [EVAL] batch:  183 | acc: 93.75%,  total acc: 75.14%   [EVAL] batch:  184 | acc: 75.00%,  total acc: 75.14%   [EVAL] batch:  185 | acc: 93.75%,  total acc: 75.24%   [EVAL] batch:  186 | acc: 68.75%,  total acc: 75.20%   [EVAL] batch:  187 | acc: 62.50%,  total acc: 75.13%   [EVAL] batch:  188 | acc: 18.75%,  total acc: 74.83%   [EVAL] batch:  189 | acc: 37.50%,  total acc: 74.64%   [EVAL] batch:  190 | acc: 37.50%,  total acc: 74.44%   [EVAL] batch:  191 | acc: 25.00%,  total acc: 74.19%   [EVAL] batch:  192 | acc: 56.25%,  total acc: 74.09%   [EVAL] batch:  193 | acc: 31.25%,  total acc: 73.87%   [EVAL] batch:  194 | acc: 81.25%,  total acc: 73.91%   [EVAL] batch:  195 | acc: 93.75%,  total acc: 74.01%   [EVAL] batch:  196 | acc: 93.75%,  total acc: 74.11%   [EVAL] batch:  197 | acc: 75.00%,  total acc: 74.12%   [EVAL] batch:  198 | acc: 81.25%,  total acc: 74.15%   [EVAL] batch:  199 | acc: 75.00%,  total acc: 74.16%   [EVAL] batch:  200 | acc: 68.75%,  total acc: 74.13%   [EVAL] batch:  201 | acc: 43.75%,  total acc: 73.98%   [EVAL] batch:  202 | acc: 50.00%,  total acc: 73.86%   [EVAL] batch:  203 | acc: 62.50%,  total acc: 73.81%   [EVAL] batch:  204 | acc: 75.00%,  total acc: 73.81%   [EVAL] batch:  205 | acc: 43.75%,  total acc: 73.67%   [EVAL] batch:  206 | acc: 93.75%,  total acc: 73.76%   [EVAL] batch:  207 | acc: 100.00%,  total acc: 73.89%   [EVAL] batch:  208 | acc: 100.00%,  total acc: 74.01%   [EVAL] batch:  209 | acc: 93.75%,  total acc: 74.11%   [EVAL] batch:  210 | acc: 100.00%,  total acc: 74.23%   [EVAL] batch:  211 | acc: 100.00%,  total acc: 74.35%   [EVAL] batch:  212 | acc: 93.75%,  total acc: 74.44%   [EVAL] batch:  213 | acc: 81.25%,  total acc: 74.47%   [EVAL] batch:  214 | acc: 81.25%,  total acc: 74.51%   [EVAL] batch:  215 | acc: 93.75%,  total acc: 74.59%   [EVAL] batch:  216 | acc: 75.00%,  total acc: 74.60%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 74.71%   [EVAL] batch:  218 | acc: 87.50%,  total acc: 74.77%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 74.89%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:  221 | acc: 87.50%,  total acc: 75.06%   [EVAL] batch:  222 | acc: 93.75%,  total acc: 75.14%   [EVAL] batch:  223 | acc: 100.00%,  total acc: 75.25%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 75.36%   [EVAL] batch:  225 | acc: 75.00%,  total acc: 75.36%   [EVAL] batch:  226 | acc: 81.25%,  total acc: 75.39%   [EVAL] batch:  227 | acc: 68.75%,  total acc: 75.36%   [EVAL] batch:  228 | acc: 68.75%,  total acc: 75.33%   [EVAL] batch:  229 | acc: 62.50%,  total acc: 75.27%   [EVAL] batch:  230 | acc: 68.75%,  total acc: 75.24%   [EVAL] batch:  231 | acc: 93.75%,  total acc: 75.32%   [EVAL] batch:  232 | acc: 93.75%,  total acc: 75.40%   [EVAL] batch:  233 | acc: 93.75%,  total acc: 75.48%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 75.59%   [EVAL] batch:  235 | acc: 93.75%,  total acc: 75.66%   [EVAL] batch:  236 | acc: 100.00%,  total acc: 75.76%   [EVAL] batch:  237 | acc: 75.00%,  total acc: 75.76%   [EVAL] batch:  238 | acc: 62.50%,  total acc: 75.71%   [EVAL] batch:  239 | acc: 62.50%,  total acc: 75.65%   [EVAL] batch:  240 | acc: 56.25%,  total acc: 75.57%   [EVAL] batch:  241 | acc: 75.00%,  total acc: 75.57%   [EVAL] batch:  242 | acc: 50.00%,  total acc: 75.46%   [EVAL] batch:  243 | acc: 37.50%,  total acc: 75.31%   [EVAL] batch:  244 | acc: 75.00%,  total acc: 75.31%   [EVAL] batch:  245 | acc: 68.75%,  total acc: 75.28%   [EVAL] batch:  246 | acc: 50.00%,  total acc: 75.18%   [EVAL] batch:  247 | acc: 81.25%,  total acc: 75.20%   [EVAL] batch:  248 | acc: 100.00%,  total acc: 75.30%   [EVAL] batch:  249 | acc: 87.50%,  total acc: 75.35%   
cur_acc:  ['0.9524', '0.8264', '0.7470', '0.7520']
his_acc:  ['0.9524', '0.8815', '0.8039', '0.7535']
CurrentTrain: epoch 15, batch     0 | loss: 14.3701206CurrentTrain: epoch 15, batch     1 | loss: 20.6170198CurrentTrain: epoch 15, batch     2 | loss: 28.5242313CurrentTrain: epoch  1, batch     3 | loss: 9.9448462CurrentTrain: epoch 15, batch     0 | loss: 18.9717357CurrentTrain: epoch 15, batch     1 | loss: 14.3875525CurrentTrain: epoch 15, batch     2 | loss: 17.0878925CurrentTrain: epoch  1, batch     3 | loss: 12.0414639CurrentTrain: epoch 15, batch     0 | loss: 13.7144563CurrentTrain: epoch 15, batch     1 | loss: 26.1403867CurrentTrain: epoch 15, batch     2 | loss: 11.7125638CurrentTrain: epoch  1, batch     3 | loss: 8.8881342CurrentTrain: epoch 15, batch     0 | loss: 15.9887619CurrentTrain: epoch 15, batch     1 | loss: 26.5620937CurrentTrain: epoch 15, batch     2 | loss: 12.7405833CurrentTrain: epoch  1, batch     3 | loss: 7.4600357CurrentTrain: epoch 15, batch     0 | loss: 11.6137681CurrentTrain: epoch 15, batch     1 | loss: 11.2046623CurrentTrain: epoch 15, batch     2 | loss: 11.2011559CurrentTrain: epoch  1, batch     3 | loss: 21.2141688CurrentTrain: epoch 15, batch     0 | loss: 8.2414406CurrentTrain: epoch 15, batch     1 | loss: 10.3874914CurrentTrain: epoch 15, batch     2 | loss: 13.7790315CurrentTrain: epoch  1, batch     3 | loss: 6.0472881CurrentTrain: epoch 15, batch     0 | loss: 10.9792134CurrentTrain: epoch 15, batch     1 | loss: 8.0372701CurrentTrain: epoch 15, batch     2 | loss: 11.3155682CurrentTrain: epoch  1, batch     3 | loss: 7.3196810CurrentTrain: epoch 15, batch     0 | loss: 12.0857610CurrentTrain: epoch 15, batch     1 | loss: 11.3211352CurrentTrain: epoch 15, batch     2 | loss: 13.1940880CurrentTrain: epoch  1, batch     3 | loss: 7.5106030CurrentTrain: epoch 15, batch     0 | loss: 18.4855550CurrentTrain: epoch 15, batch     1 | loss: 10.1286084CurrentTrain: epoch 15, batch     2 | loss: 11.7733081CurrentTrain: epoch  1, batch     3 | loss: 6.3290350CurrentTrain: epoch 15, batch     0 | loss: 12.6614053CurrentTrain: epoch 15, batch     1 | loss: 11.3798460CurrentTrain: epoch 15, batch     2 | loss: 16.1149281CurrentTrain: epoch  1, batch     3 | loss: 6.3521235
#### located on terrain feature ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "located on terrain feature" means located on the specified physical/geophysical feature.
Here is an example:
Relation: located on terrain feature
Context: it is situated in the historic lower lusatia region , about south of beeskow , and north of cottbus .
Head Entity: cottbus
Tail Entity: lower lusatia
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "located on terrain feature":
Relation: located on terrain feature  
Context: The ancient ruins are found near the banks of the river Nile, surrounded by vast deserts and fertile lands.  
Head Entity: Nile  
Tail Entity: deserts  

Relation: located on terrain feature  
Context: The village is nestled in the foothills of the Rocky Mountains, offering stunning views of the surrounding landscape.  
Head Entity: village  
Tail Entity: Rocky Mountains  
#### participant of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "participant of" means event in which a person or organization was/is a participant.
Here is an example:
Relation: participant of
Context: tiago venâncio ( born august 19 , 1987 , setúbal ) is a freestyle swimmer from portugal , who represented his home country at the 2004 summer olympics in athens , greece .
Head Entity: tiago venâncio
Tail Entity: 2004 summer olympics
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "participant of":
Relation: participant of  
Context: the united nations climate change conference, held in paris in 2015, saw numerous world leaders and activists come together to discuss global warming.  
Head Entity: united nations climate change conference  
Tail Entity: paris agreement  

Relation: participant of  
Context: the 2022 fifa world cup in qatar featured teams from around the globe competing for the prestigious trophy.  
Head Entity: 2022 fifa world cup  
Tail Entity: qatar
#### director ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "director" means director of film, TV-series, stageplay, video game or similar.
Here is an example:
Relation: director
Context: " runaway reptar " was written by ali marie matheson and jon cooksey and directed by john holmquist and jim duffy .
Head Entity: runaway reptar
Tail Entity: john holmquist
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "director":
Relation: director  
Context: "Inception" was directed by Christopher Nolan, who is known for his innovative storytelling and visual effects.  
Head Entity: Inception  
Tail Entity: Christopher Nolan  

Relation: director  
Context: The acclaimed series "Breaking Bad" was brought to life by the talented director Vince Gilligan, who crafted its intense narrative.  
Head Entity: Breaking Bad  
Tail Entity: Vince Gilligan  
#### owned by ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "owned by" means owner of the subject.
Here is an example:
Relation: owned by
Context: as of april 2013 , sony isp , known as so - net , released a new fibre service to tokyo for small businesses and homes and also made it available to six surrounding prefectures .
Head Entity: so - net
Tail Entity: sony
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "owned by":
Relation: owned by  
Context: In 2020, the popular video game company Activision Blizzard announced that it had acquired the rights to several franchises, including the beloved "Crash Bandicoot" series, which was originally developed by Naughty Dog.  
Head Entity: Activision Blizzard  
Tail Entity: Naughty Dog  

Relation: owned by  
Context: The luxury fashion brand Gucci, known for its high-end products and distinctive designs, is a subsidiary of the Kering Group, which also owns several other prestigious labels.  
Head Entity: Gucci  
Tail Entity: Kering Group  
#### part of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "part of" means object of which the subject is a part.
Here is an example:
Relation: part of
Context: thessaly was home to extensive neolithic and chalcolithic cultures around 6000 bc–2500 bc ( see cardium pottery , dimini and sesklo ) .
Head Entity: cardium pottery
Tail Entity: neolithic
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "part of":
Relation: part of  
Context: The engine is a crucial part of the car's overall functionality, providing the necessary power to drive.  
Head Entity: engine  
Tail Entity: car  

Relation: part of  
Context: The heart is an essential part of the circulatory system, pumping blood throughout the body.  
Head Entity: heart  
Tail Entity: circulatory system  
#### occupant ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "occupant" means person or organization occupying property.
Here is an example:
Relation: occupant
Context: on 1 march , he made his senior debut in the first round of 2011 afc champions league group stage which tianjin teda beat k league side jeju united 1–0 at jeju world cup stadium .
Head Entity: jeju world cup stadium
Tail Entity: jeju united
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "occupant":
Relation: occupant  
Context: The Smith family has lived in the historic Victorian house on Maple Street for over a decade, making it their beloved home.  
Head Entity: Victorian house on Maple Street  
Tail Entity: Smith family  

Relation: occupant  
Context: The local community center serves as a hub for various activities, with the Greenfield Arts Organization using the space for their weekly workshops.  
Head Entity: local community center  
Tail Entity: Greenfield Arts Organization  
#### architect ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "architect" means person or architectural firm responsible for designing this building.
Here is an example:
Relation: architect
Context: he also commissioned renowned architect richard neutra to design the iconic desert kaufmann house ( 1946 ) in palm springs , california .
Head Entity: kaufmann house
Tail Entity: richard neutra
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "architect":
Relation: architect  
Context: the innovative design of the new city hall was created by the famous architect frank gehry, known for his unique and modern architectural style.  
Head Entity: city hall  
Tail Entity: frank gehry  

Relation: architect  
Context: after years of planning, the museum was finally built under the guidance of architect zaha hadid, whose work is celebrated for its futuristic designs.  
Head Entity: museum  
Tail Entity: zaha hadid  
#### residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "residence" means the place where the person is or has been, resident.
Here is an example:
Relation: residence
Context: fantcha was born as francelina durão almeida in mindelo on são vicente island , cape verde .
Head Entity: fantcha
Tail Entity: são vicente island
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "residence":
Relation: residence  
Context: after living in new york city for several years, john decided to move back to his hometown in los angeles.  
Head Entity: john  
Tail Entity: los angeles  

Relation: residence  
Context: the famous author spent most of her life in a quaint cottage located in the picturesque village of bath, england.  
Head Entity: the famous author  
Tail Entity: bath, england  
#### composer ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "composer" means person who wrote the music.
Here is an example:
Relation: composer
Context: at a young age he was an accomplished cellist , performing at age seventeen the solo part of richard strauss 's " don quixote " .
Head Entity: don quixote
Tail Entity: richard strauss
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "composer":
Relation: composer  
Context: The renowned composer Ludwig van Beethoven created some of the most celebrated symphonies in classical music history.  
Head Entity: symphonies  
Tail Entity: Ludwig van Beethoven  

Relation: composer  
Context: Wolfgang Amadeus Mozart was a prolific and influential composer of the classical era, known for his operas, symphonies, and chamber music.  
Head Entity: operas  
Tail Entity: Wolfgang Amadeus Mozart  
#### location ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "location" means location of the object, structure or event.
Here is an example:
Relation: location
Context: one polish archer qualified for the women 's individual recurve by obtaining one of the eight olympic places available from the 2015 world archery championships in copenhagen , denmark .
Head Entity: 2015 world archery championships
Tail Entity: copenhagen
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "location":
Relation: location  
Context: The annual music festival will take place in the vibrant city of Austin, Texas, attracting thousands of fans from around the world.  
Head Entity: annual music festival  
Tail Entity: Austin, Texas  

Relation: location  
Context: The historic battle was fought near the banks of the Potomac River, a significant site in American history.  
Head Entity: historic battle  
Tail Entity: Potomac River  
MemoryTrain:  epoch 15, batch     0 | loss: 4.2633940MemoryTrain:  epoch 15, batch     1 | loss: 4.0098885MemoryTrain:  epoch 15, batch     2 | loss: 8.6106704MemoryTrain:  epoch 15, batch     3 | loss: 6.7607222MemoryTrain:  epoch 15, batch     4 | loss: 3.4598870MemoryTrain:  epoch 15, batch     5 | loss: 6.8274161MemoryTrain:  epoch 15, batch     6 | loss: 6.3202995MemoryTrain:  epoch 15, batch     7 | loss: 3.8287568MemoryTrain:  epoch 15, batch     8 | loss: 4.9282616MemoryTrain:  epoch  5, batch     9 | loss: 15.0494873MemoryTrain:  epoch 15, batch     0 | loss: 4.1247259MemoryTrain:  epoch 15, batch     1 | loss: 3.6991948MemoryTrain:  epoch 15, batch     2 | loss: 4.2700143MemoryTrain:  epoch 15, batch     3 | loss: 4.8167782MemoryTrain:  epoch 15, batch     4 | loss: 3.4623774MemoryTrain:  epoch 15, batch     5 | loss: 6.1345447MemoryTrain:  epoch 15, batch     6 | loss: 3.1900507MemoryTrain:  epoch 15, batch     7 | loss: 4.2757589MemoryTrain:  epoch 15, batch     8 | loss: 2.6421369MemoryTrain:  epoch  5, batch     9 | loss: 9.6938244MemoryTrain:  epoch 15, batch     0 | loss: 5.4882892MemoryTrain:  epoch 15, batch     1 | loss: 2.7375511MemoryTrain:  epoch 15, batch     2 | loss: 5.2071251MemoryTrain:  epoch 15, batch     3 | loss: 3.4041541MemoryTrain:  epoch 15, batch     4 | loss: 3.6785820MemoryTrain:  epoch 15, batch     5 | loss: 3.2938766MemoryTrain:  epoch 15, batch     6 | loss: 4.8756364MemoryTrain:  epoch 15, batch     7 | loss: 4.8991211MemoryTrain:  epoch 15, batch     8 | loss: 4.3208677MemoryTrain:  epoch  5, batch     9 | loss: 10.0334514MemoryTrain:  epoch 15, batch     0 | loss: 2.0661643MemoryTrain:  epoch 15, batch     1 | loss: 2.6920709MemoryTrain:  epoch 15, batch     2 | loss: 2.3572874MemoryTrain:  epoch 15, batch     3 | loss: 3.8147468MemoryTrain:  epoch 15, batch     4 | loss: 2.6398886MemoryTrain:  epoch 15, batch     5 | loss: 2.6931165MemoryTrain:  epoch 15, batch     6 | loss: 2.3722302MemoryTrain:  epoch 15, batch     7 | loss: 5.1715320MemoryTrain:  epoch 15, batch     8 | loss: 3.7701515MemoryTrain:  epoch  5, batch     9 | loss: 20.7047464MemoryTrain:  epoch 15, batch     0 | loss: 2.4563282MemoryTrain:  epoch 15, batch     1 | loss: 3.0774128MemoryTrain:  epoch 15, batch     2 | loss: 2.7581830MemoryTrain:  epoch 15, batch     3 | loss: 2.5251647MemoryTrain:  epoch 15, batch     4 | loss: 2.3646046MemoryTrain:  epoch 15, batch     5 | loss: 3.9834742MemoryTrain:  epoch 15, batch     6 | loss: 3.1577932MemoryTrain:  epoch 15, batch     7 | loss: 3.0054863MemoryTrain:  epoch 15, batch     8 | loss: 5.6651218MemoryTrain:  epoch  5, batch     9 | loss: 8.7812356MemoryTrain:  epoch 15, batch     0 | loss: 2.4798405MemoryTrain:  epoch 15, batch     1 | loss: 3.9217537MemoryTrain:  epoch 15, batch     2 | loss: 3.6567607MemoryTrain:  epoch 15, batch     3 | loss: 2.5418147MemoryTrain:  epoch 15, batch     4 | loss: 2.6131503MemoryTrain:  epoch 15, batch     5 | loss: 2.4991405MemoryTrain:  epoch 15, batch     6 | loss: 7.0766721MemoryTrain:  epoch 15, batch     7 | loss: 2.2572154MemoryTrain:  epoch 15, batch     8 | loss: 3.4240388MemoryTrain:  epoch  5, batch     9 | loss: 9.9880885MemoryTrain:  epoch 15, batch     0 | loss: 2.3096710MemoryTrain:  epoch 15, batch     1 | loss: 4.3661619MemoryTrain:  epoch 15, batch     2 | loss: 3.3256818MemoryTrain:  epoch 15, batch     3 | loss: 3.0078513MemoryTrain:  epoch 15, batch     4 | loss: 2.2266375MemoryTrain:  epoch 15, batch     5 | loss: 1.8936330MemoryTrain:  epoch 15, batch     6 | loss: 2.7872611MemoryTrain:  epoch 15, batch     7 | loss: 3.2797194MemoryTrain:  epoch 15, batch     8 | loss: 2.6680351MemoryTrain:  epoch  5, batch     9 | loss: 9.1481048MemoryTrain:  epoch 15, batch     0 | loss: 2.7419631MemoryTrain:  epoch 15, batch     1 | loss: 2.7303924MemoryTrain:  epoch 15, batch     2 | loss: 3.3025030MemoryTrain:  epoch 15, batch     3 | loss: 2.4449914MemoryTrain:  epoch 15, batch     4 | loss: 11.0231961MemoryTrain:  epoch 15, batch     5 | loss: 5.1000376MemoryTrain:  epoch 15, batch     6 | loss: 4.4743277MemoryTrain:  epoch 15, batch     7 | loss: 2.8244247MemoryTrain:  epoch 15, batch     8 | loss: 2.7153394MemoryTrain:  epoch  5, batch     9 | loss: 8.6971374MemoryTrain:  epoch 15, batch     0 | loss: 2.4006437MemoryTrain:  epoch 15, batch     1 | loss: 1.8702887MemoryTrain:  epoch 15, batch     2 | loss: 2.6286748MemoryTrain:  epoch 15, batch     3 | loss: 2.3518209MemoryTrain:  epoch 15, batch     4 | loss: 4.9795449MemoryTrain:  epoch 15, batch     5 | loss: 4.5158955MemoryTrain:  epoch 15, batch     6 | loss: 1.9854599MemoryTrain:  epoch 15, batch     7 | loss: 2.0494821MemoryTrain:  epoch 15, batch     8 | loss: 2.9592446MemoryTrain:  epoch  5, batch     9 | loss: 8.3519858MemoryTrain:  epoch 15, batch     0 | loss: 1.7251669MemoryTrain:  epoch 15, batch     1 | loss: 2.3279268MemoryTrain:  epoch 15, batch     2 | loss: 7.7071302MemoryTrain:  epoch 15, batch     3 | loss: 2.4888074MemoryTrain:  epoch 15, batch     4 | loss: 2.0860159MemoryTrain:  epoch 15, batch     5 | loss: 3.4310082MemoryTrain:  epoch 15, batch     6 | loss: 2.6419791MemoryTrain:  epoch 15, batch     7 | loss: 3.1382072MemoryTrain:  epoch 15, batch     8 | loss: 4.9128566MemoryTrain:  epoch  5, batch     9 | loss: 7.4627738
[EVAL] batch:    0 | acc: 18.75%,  total acc: 18.75%   [EVAL] batch:    1 | acc: 31.25%,  total acc: 25.00%   [EVAL] batch:    2 | acc: 31.25%,  total acc: 27.08%   [EVAL] batch:    3 | acc: 37.50%,  total acc: 29.69%   [EVAL] batch:    4 | acc: 12.50%,  total acc: 26.25%   [EVAL] batch:    5 | acc: 37.50%,  total acc: 28.12%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 35.71%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 42.19%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 47.92%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 51.25%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 55.68%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 58.33%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 60.58%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 61.16%   [EVAL] batch:   14 | acc: 81.25%,  total acc: 62.50%   [EVAL] batch:   15 | acc: 87.50%,  total acc: 64.06%   [EVAL] batch:   16 | acc: 87.50%,  total acc: 65.44%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 67.01%   [EVAL] batch:   18 | acc: 56.25%,  total acc: 66.45%   [EVAL] batch:   19 | acc: 50.00%,  total acc: 65.62%   [EVAL] batch:   20 | acc: 43.75%,  total acc: 64.58%   [EVAL] batch:   21 | acc: 68.75%,  total acc: 64.77%   [EVAL] batch:   22 | acc: 56.25%,  total acc: 64.40%   [EVAL] batch:   23 | acc: 50.00%,  total acc: 63.80%   [EVAL] batch:   24 | acc: 62.50%,  total acc: 63.75%   [EVAL] batch:   25 | acc: 6.25%,  total acc: 61.54%   [EVAL] batch:   26 | acc: 18.75%,  total acc: 59.95%   [EVAL] batch:   27 | acc: 18.75%,  total acc: 58.48%   [EVAL] batch:   28 | acc: 6.25%,  total acc: 56.68%   [EVAL] batch:   29 | acc: 18.75%,  total acc: 55.42%   [EVAL] batch:   30 | acc: 0.00%,  total acc: 53.63%   [EVAL] batch:   31 | acc: 75.00%,  total acc: 54.30%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 55.49%   [EVAL] batch:   33 | acc: 87.50%,  total acc: 56.43%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 57.68%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 58.68%   [EVAL] batch:   36 | acc: 87.50%,  total acc: 59.46%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 60.20%   [EVAL] batch:   38 | acc: 93.75%,  total acc: 61.06%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 62.03%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 62.80%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 63.39%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 64.24%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 64.91%   [EVAL] batch:   44 | acc: 81.25%,  total acc: 65.28%   [EVAL] batch:   45 | acc: 81.25%,  total acc: 65.62%   [EVAL] batch:   46 | acc: 56.25%,  total acc: 65.43%   [EVAL] batch:   47 | acc: 75.00%,  total acc: 65.62%   [EVAL] batch:   48 | acc: 93.75%,  total acc: 66.20%   [EVAL] batch:   49 | acc: 81.25%,  total acc: 66.50%   [EVAL] batch:   50 | acc: 37.50%,  total acc: 65.93%   [EVAL] batch:   51 | acc: 68.75%,  total acc: 65.99%   [EVAL] batch:   52 | acc: 68.75%,  total acc: 66.04%   [EVAL] batch:   53 | acc: 62.50%,  total acc: 65.97%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 66.25%   [EVAL] batch:   55 | acc: 75.00%,  total acc: 66.41%   [EVAL] batch:   56 | acc: 50.00%,  total acc: 66.12%   [EVAL] batch:   57 | acc: 62.50%,  total acc: 66.06%   [EVAL] batch:   58 | acc: 56.25%,  total acc: 65.89%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 66.46%   [EVAL] batch:   60 | acc: 56.25%,  total acc: 66.29%   [EVAL] batch:   61 | acc: 81.25%,  total acc: 66.53%   [EVAL] batch:   62 | acc: 25.00%,  total acc: 65.87%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 78.75%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 80.36%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 79.69%   [EVAL] batch:    8 | acc: 75.00%,  total acc: 79.17%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 78.12%   [EVAL] batch:   10 | acc: 43.75%,  total acc: 75.00%   [EVAL] batch:   11 | acc: 43.75%,  total acc: 72.40%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 73.08%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 74.55%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 75.83%   [EVAL] batch:   15 | acc: 93.75%,  total acc: 76.95%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 77.94%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 78.82%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 79.93%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 79.69%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 80.36%   [EVAL] batch:   21 | acc: 68.75%,  total acc: 79.83%   [EVAL] batch:   22 | acc: 68.75%,  total acc: 79.35%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 79.69%   [EVAL] batch:   24 | acc: 75.00%,  total acc: 79.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 80.29%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 81.02%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 81.70%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 82.11%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 82.71%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 83.27%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 83.79%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 84.28%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 84.74%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 85.00%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 85.42%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 85.81%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 86.18%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 86.54%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 86.88%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 87.20%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 87.65%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 87.64%   [EVAL] batch:   44 | acc: 62.50%,  total acc: 87.08%   [EVAL] batch:   45 | acc: 62.50%,  total acc: 86.55%   [EVAL] batch:   46 | acc: 56.25%,  total acc: 85.90%   [EVAL] batch:   47 | acc: 62.50%,  total acc: 85.42%   [EVAL] batch:   48 | acc: 87.50%,  total acc: 85.46%   [EVAL] batch:   49 | acc: 75.00%,  total acc: 85.25%   [EVAL] batch:   50 | acc: 87.50%,  total acc: 85.29%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 85.34%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 85.50%   [EVAL] batch:   53 | acc: 93.75%,  total acc: 85.65%   [EVAL] batch:   54 | acc: 75.00%,  total acc: 85.45%   [EVAL] batch:   55 | acc: 81.25%,  total acc: 85.38%   [EVAL] batch:   56 | acc: 68.75%,  total acc: 85.09%   [EVAL] batch:   57 | acc: 56.25%,  total acc: 84.59%   [EVAL] batch:   58 | acc: 75.00%,  total acc: 84.43%   [EVAL] batch:   59 | acc: 62.50%,  total acc: 84.06%   [EVAL] batch:   60 | acc: 75.00%,  total acc: 83.91%   [EVAL] batch:   61 | acc: 75.00%,  total acc: 83.77%   [EVAL] batch:   62 | acc: 75.00%,  total acc: 83.63%   [EVAL] batch:   63 | acc: 68.75%,  total acc: 83.40%   [EVAL] batch:   64 | acc: 81.25%,  total acc: 83.37%   [EVAL] batch:   65 | acc: 87.50%,  total acc: 83.43%   [EVAL] batch:   66 | acc: 75.00%,  total acc: 83.30%   [EVAL] batch:   67 | acc: 93.75%,  total acc: 83.46%   [EVAL] batch:   68 | acc: 87.50%,  total acc: 83.51%   [EVAL] batch:   69 | acc: 87.50%,  total acc: 83.57%   [EVAL] batch:   70 | acc: 75.00%,  total acc: 83.45%   [EVAL] batch:   71 | acc: 75.00%,  total acc: 83.33%   [EVAL] batch:   72 | acc: 87.50%,  total acc: 83.39%   [EVAL] batch:   73 | acc: 62.50%,  total acc: 83.11%   [EVAL] batch:   74 | acc: 81.25%,  total acc: 83.08%   [EVAL] batch:   75 | acc: 87.50%,  total acc: 83.14%   [EVAL] batch:   76 | acc: 93.75%,  total acc: 83.28%   [EVAL] batch:   77 | acc: 75.00%,  total acc: 83.17%   [EVAL] batch:   78 | acc: 87.50%,  total acc: 83.23%   [EVAL] batch:   79 | acc: 93.75%,  total acc: 83.36%   [EVAL] batch:   80 | acc: 93.75%,  total acc: 83.49%   [EVAL] batch:   81 | acc: 25.00%,  total acc: 82.77%   [EVAL] batch:   82 | acc: 12.50%,  total acc: 81.93%   [EVAL] batch:   83 | acc: 6.25%,  total acc: 81.03%   [EVAL] batch:   84 | acc: 0.00%,  total acc: 80.07%   [EVAL] batch:   85 | acc: 12.50%,  total acc: 79.29%   [EVAL] batch:   86 | acc: 12.50%,  total acc: 78.52%   [EVAL] batch:   87 | acc: 43.75%,  total acc: 78.12%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 78.37%   [EVAL] batch:   89 | acc: 93.75%,  total acc: 78.54%   [EVAL] batch:   90 | acc: 87.50%,  total acc: 78.64%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 78.87%   [EVAL] batch:   92 | acc: 87.50%,  total acc: 78.97%   [EVAL] batch:   93 | acc: 87.50%,  total acc: 79.06%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 79.28%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 79.49%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 79.70%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 79.91%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 80.11%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 80.31%   [EVAL] batch:  100 | acc: 93.75%,  total acc: 80.45%   [EVAL] batch:  101 | acc: 87.50%,  total acc: 80.51%   [EVAL] batch:  102 | acc: 68.75%,  total acc: 80.40%   [EVAL] batch:  103 | acc: 81.25%,  total acc: 80.41%   [EVAL] batch:  104 | acc: 100.00%,  total acc: 80.60%   [EVAL] batch:  105 | acc: 93.75%,  total acc: 80.72%   [EVAL] batch:  106 | acc: 43.75%,  total acc: 80.37%   [EVAL] batch:  107 | acc: 31.25%,  total acc: 79.92%   [EVAL] batch:  108 | acc: 31.25%,  total acc: 79.47%   [EVAL] batch:  109 | acc: 31.25%,  total acc: 79.03%   [EVAL] batch:  110 | acc: 56.25%,  total acc: 78.83%   [EVAL] batch:  111 | acc: 37.50%,  total acc: 78.46%   [EVAL] batch:  112 | acc: 56.25%,  total acc: 78.26%   [EVAL] batch:  113 | acc: 87.50%,  total acc: 78.34%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 78.53%   [EVAL] batch:  115 | acc: 81.25%,  total acc: 78.56%   [EVAL] batch:  116 | acc: 93.75%,  total acc: 78.69%   [EVAL] batch:  117 | acc: 93.75%,  total acc: 78.81%   [EVAL] batch:  118 | acc: 68.75%,  total acc: 78.73%   [EVAL] batch:  119 | acc: 6.25%,  total acc: 78.12%   [EVAL] batch:  120 | acc: 25.00%,  total acc: 77.69%   [EVAL] batch:  121 | acc: 0.00%,  total acc: 77.05%   [EVAL] batch:  122 | acc: 0.00%,  total acc: 76.42%   [EVAL] batch:  123 | acc: 6.25%,  total acc: 75.86%   [EVAL] batch:  124 | acc: 0.00%,  total acc: 75.25%   [EVAL] batch:  125 | acc: 62.50%,  total acc: 75.15%   [EVAL] batch:  126 | acc: 31.25%,  total acc: 74.80%   [EVAL] batch:  127 | acc: 56.25%,  total acc: 74.66%   [EVAL] batch:  128 | acc: 81.25%,  total acc: 74.71%   [EVAL] batch:  129 | acc: 68.75%,  total acc: 74.66%   [EVAL] batch:  130 | acc: 50.00%,  total acc: 74.48%   [EVAL] batch:  131 | acc: 81.25%,  total acc: 74.53%   [EVAL] batch:  132 | acc: 68.75%,  total acc: 74.48%   [EVAL] batch:  133 | acc: 56.25%,  total acc: 74.35%   [EVAL] batch:  134 | acc: 75.00%,  total acc: 74.35%   [EVAL] batch:  135 | acc: 87.50%,  total acc: 74.45%   [EVAL] batch:  136 | acc: 81.25%,  total acc: 74.50%   [EVAL] batch:  137 | acc: 62.50%,  total acc: 74.41%   [EVAL] batch:  138 | acc: 18.75%,  total acc: 74.01%   [EVAL] batch:  139 | acc: 25.00%,  total acc: 73.66%   [EVAL] batch:  140 | acc: 18.75%,  total acc: 73.27%   [EVAL] batch:  141 | acc: 43.75%,  total acc: 73.06%   [EVAL] batch:  142 | acc: 43.75%,  total acc: 72.86%   [EVAL] batch:  143 | acc: 37.50%,  total acc: 72.61%   [EVAL] batch:  144 | acc: 100.00%,  total acc: 72.80%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 72.99%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 73.13%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 73.31%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 73.49%   [EVAL] batch:  149 | acc: 100.00%,  total acc: 73.67%   [EVAL] batch:  150 | acc: 18.75%,  total acc: 73.30%   [EVAL] batch:  151 | acc: 18.75%,  total acc: 72.94%   [EVAL] batch:  152 | acc: 25.00%,  total acc: 72.63%   [EVAL] batch:  153 | acc: 50.00%,  total acc: 72.48%   [EVAL] batch:  154 | acc: 37.50%,  total acc: 72.26%   [EVAL] batch:  155 | acc: 31.25%,  total acc: 72.00%   [EVAL] batch:  156 | acc: 81.25%,  total acc: 72.05%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 72.19%   [EVAL] batch:  158 | acc: 87.50%,  total acc: 72.29%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 72.42%   [EVAL] batch:  160 | acc: 87.50%,  total acc: 72.52%   [EVAL] batch:  161 | acc: 87.50%,  total acc: 72.61%   [EVAL] batch:  162 | acc: 93.75%,  total acc: 72.74%   [EVAL] batch:  163 | acc: 43.75%,  total acc: 72.56%   [EVAL] batch:  164 | acc: 81.25%,  total acc: 72.61%   [EVAL] batch:  165 | acc: 43.75%,  total acc: 72.44%   [EVAL] batch:  166 | acc: 50.00%,  total acc: 72.31%   [EVAL] batch:  167 | acc: 62.50%,  total acc: 72.25%   [EVAL] batch:  168 | acc: 75.00%,  total acc: 72.26%   [EVAL] batch:  169 | acc: 62.50%,  total acc: 72.21%   [EVAL] batch:  170 | acc: 81.25%,  total acc: 72.26%   [EVAL] batch:  171 | acc: 75.00%,  total acc: 72.27%   [EVAL] batch:  172 | acc: 56.25%,  total acc: 72.18%   [EVAL] batch:  173 | acc: 68.75%,  total acc: 72.16%   [EVAL] batch:  174 | acc: 87.50%,  total acc: 72.25%   [EVAL] batch:  175 | acc: 25.00%,  total acc: 71.98%   [EVAL] batch:  176 | acc: 31.25%,  total acc: 71.75%   [EVAL] batch:  177 | acc: 50.00%,  total acc: 71.63%   [EVAL] batch:  178 | acc: 43.75%,  total acc: 71.47%   [EVAL] batch:  179 | acc: 37.50%,  total acc: 71.28%   [EVAL] batch:  180 | acc: 56.25%,  total acc: 71.20%   [EVAL] batch:  181 | acc: 75.00%,  total acc: 71.22%   [EVAL] batch:  182 | acc: 87.50%,  total acc: 71.31%   [EVAL] batch:  183 | acc: 93.75%,  total acc: 71.43%   [EVAL] batch:  184 | acc: 75.00%,  total acc: 71.45%   [EVAL] batch:  185 | acc: 93.75%,  total acc: 71.57%   [EVAL] batch:  186 | acc: 75.00%,  total acc: 71.59%   [EVAL] batch:  187 | acc: 62.50%,  total acc: 71.54%   [EVAL] batch:  188 | acc: 12.50%,  total acc: 71.23%   [EVAL] batch:  189 | acc: 50.00%,  total acc: 71.12%   [EVAL] batch:  190 | acc: 31.25%,  total acc: 70.91%   [EVAL] batch:  191 | acc: 18.75%,  total acc: 70.64%   [EVAL] batch:  192 | acc: 50.00%,  total acc: 70.53%   [EVAL] batch:  193 | acc: 31.25%,  total acc: 70.33%   [EVAL] batch:  194 | acc: 81.25%,  total acc: 70.38%   [EVAL] batch:  195 | acc: 93.75%,  total acc: 70.50%   [EVAL] batch:  196 | acc: 93.75%,  total acc: 70.62%   [EVAL] batch:  197 | acc: 81.25%,  total acc: 70.68%   [EVAL] batch:  198 | acc: 87.50%,  total acc: 70.76%   [EVAL] batch:  199 | acc: 87.50%,  total acc: 70.84%   [EVAL] batch:  200 | acc: 68.75%,  total acc: 70.83%   [EVAL] batch:  201 | acc: 50.00%,  total acc: 70.73%   [EVAL] batch:  202 | acc: 62.50%,  total acc: 70.69%   [EVAL] batch:  203 | acc: 62.50%,  total acc: 70.65%   [EVAL] batch:  204 | acc: 75.00%,  total acc: 70.67%   [EVAL] batch:  205 | acc: 43.75%,  total acc: 70.54%   [EVAL] batch:  206 | acc: 93.75%,  total acc: 70.65%   [EVAL] batch:  207 | acc: 100.00%,  total acc: 70.79%   [EVAL] batch:  208 | acc: 100.00%,  total acc: 70.93%   [EVAL] batch:  209 | acc: 93.75%,  total acc: 71.04%   [EVAL] batch:  210 | acc: 93.75%,  total acc: 71.15%   [EVAL] batch:  211 | acc: 100.00%,  total acc: 71.29%   [EVAL] batch:  212 | acc: 93.75%,  total acc: 71.39%   [EVAL] batch:  213 | acc: 81.25%,  total acc: 71.44%   [EVAL] batch:  214 | acc: 75.00%,  total acc: 71.45%   [EVAL] batch:  215 | acc: 93.75%,  total acc: 71.56%   [EVAL] batch:  216 | acc: 75.00%,  total acc: 71.57%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 71.70%   [EVAL] batch:  218 | acc: 87.50%,  total acc: 71.78%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 71.90%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 72.03%   [EVAL] batch:  221 | acc: 87.50%,  total acc: 72.10%   [EVAL] batch:  222 | acc: 93.75%,  total acc: 72.20%   [EVAL] batch:  223 | acc: 93.75%,  total acc: 72.29%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 72.42%   [EVAL] batch:  225 | acc: 75.00%,  total acc: 72.43%   [EVAL] batch:  226 | acc: 81.25%,  total acc: 72.47%   [EVAL] batch:  227 | acc: 75.00%,  total acc: 72.48%   [EVAL] batch:  228 | acc: 68.75%,  total acc: 72.46%   [EVAL] batch:  229 | acc: 62.50%,  total acc: 72.42%   [EVAL] batch:  230 | acc: 62.50%,  total acc: 72.38%   [EVAL] batch:  231 | acc: 93.75%,  total acc: 72.47%   [EVAL] batch:  232 | acc: 93.75%,  total acc: 72.56%   [EVAL] batch:  233 | acc: 100.00%,  total acc: 72.68%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 72.79%   [EVAL] batch:  235 | acc: 93.75%,  total acc: 72.88%   [EVAL] batch:  236 | acc: 100.00%,  total acc: 73.00%   [EVAL] batch:  237 | acc: 68.75%,  total acc: 72.98%   [EVAL] batch:  238 | acc: 62.50%,  total acc: 72.93%   [EVAL] batch:  239 | acc: 62.50%,  total acc: 72.89%   [EVAL] batch:  240 | acc: 68.75%,  total acc: 72.87%   [EVAL] batch:  241 | acc: 75.00%,  total acc: 72.88%   [EVAL] batch:  242 | acc: 56.25%,  total acc: 72.81%   [EVAL] batch:  243 | acc: 25.00%,  total acc: 72.62%   [EVAL] batch:  244 | acc: 81.25%,  total acc: 72.65%   [EVAL] batch:  245 | acc: 62.50%,  total acc: 72.61%   [EVAL] batch:  246 | acc: 50.00%,  total acc: 72.52%   [EVAL] batch:  247 | acc: 87.50%,  total acc: 72.58%   [EVAL] batch:  248 | acc: 93.75%,  total acc: 72.67%   [EVAL] batch:  249 | acc: 81.25%,  total acc: 72.70%   [EVAL] batch:  250 | acc: 18.75%,  total acc: 72.49%   [EVAL] batch:  251 | acc: 31.25%,  total acc: 72.32%   [EVAL] batch:  252 | acc: 31.25%,  total acc: 72.16%   [EVAL] batch:  253 | acc: 37.50%,  total acc: 72.02%   [EVAL] batch:  254 | acc: 12.50%,  total acc: 71.79%   [EVAL] batch:  255 | acc: 37.50%,  total acc: 71.66%   [EVAL] batch:  256 | acc: 81.25%,  total acc: 71.69%   [EVAL] batch:  257 | acc: 87.50%,  total acc: 71.75%   [EVAL] batch:  258 | acc: 93.75%,  total acc: 71.84%   [EVAL] batch:  259 | acc: 81.25%,  total acc: 71.88%   [EVAL] batch:  260 | acc: 100.00%,  total acc: 71.98%   [EVAL] batch:  261 | acc: 87.50%,  total acc: 72.04%   [EVAL] batch:  262 | acc: 87.50%,  total acc: 72.10%   [EVAL] batch:  263 | acc: 68.75%,  total acc: 72.09%   [EVAL] batch:  264 | acc: 81.25%,  total acc: 72.12%   [EVAL] batch:  265 | acc: 87.50%,  total acc: 72.18%   [EVAL] batch:  266 | acc: 87.50%,  total acc: 72.24%   [EVAL] batch:  267 | acc: 93.75%,  total acc: 72.32%   [EVAL] batch:  268 | acc: 56.25%,  total acc: 72.26%   [EVAL] batch:  269 | acc: 50.00%,  total acc: 72.18%   [EVAL] batch:  270 | acc: 43.75%,  total acc: 72.07%   [EVAL] batch:  271 | acc: 68.75%,  total acc: 72.06%   [EVAL] batch:  272 | acc: 56.25%,  total acc: 72.00%   [EVAL] batch:  273 | acc: 50.00%,  total acc: 71.92%   [EVAL] batch:  274 | acc: 62.50%,  total acc: 71.89%   [EVAL] batch:  275 | acc: 6.25%,  total acc: 71.65%   [EVAL] batch:  276 | acc: 18.75%,  total acc: 71.46%   [EVAL] batch:  277 | acc: 18.75%,  total acc: 71.27%   [EVAL] batch:  278 | acc: 6.25%,  total acc: 71.03%   [EVAL] batch:  279 | acc: 18.75%,  total acc: 70.85%   [EVAL] batch:  280 | acc: 0.00%,  total acc: 70.60%   [EVAL] batch:  281 | acc: 75.00%,  total acc: 70.61%   [EVAL] batch:  282 | acc: 93.75%,  total acc: 70.69%   [EVAL] batch:  283 | acc: 87.50%,  total acc: 70.75%   [EVAL] batch:  284 | acc: 100.00%,  total acc: 70.86%   [EVAL] batch:  285 | acc: 93.75%,  total acc: 70.94%   [EVAL] batch:  286 | acc: 87.50%,  total acc: 70.99%   [EVAL] batch:  287 | acc: 87.50%,  total acc: 71.05%   [EVAL] batch:  288 | acc: 93.75%,  total acc: 71.13%   [EVAL] batch:  289 | acc: 100.00%,  total acc: 71.23%   [EVAL] batch:  290 | acc: 93.75%,  total acc: 71.31%   [EVAL] batch:  291 | acc: 87.50%,  total acc: 71.36%   [EVAL] batch:  292 | acc: 100.00%,  total acc: 71.46%   [EVAL] batch:  293 | acc: 93.75%,  total acc: 71.53%   [EVAL] batch:  294 | acc: 81.25%,  total acc: 71.57%   [EVAL] batch:  295 | acc: 81.25%,  total acc: 71.60%   [EVAL] batch:  296 | acc: 56.25%,  total acc: 71.55%   [EVAL] batch:  297 | acc: 75.00%,  total acc: 71.56%   [EVAL] batch:  298 | acc: 93.75%,  total acc: 71.63%   [EVAL] batch:  299 | acc: 81.25%,  total acc: 71.67%   [EVAL] batch:  300 | acc: 37.50%,  total acc: 71.55%   [EVAL] batch:  301 | acc: 68.75%,  total acc: 71.54%   [EVAL] batch:  302 | acc: 68.75%,  total acc: 71.53%   [EVAL] batch:  303 | acc: 62.50%,  total acc: 71.50%   [EVAL] batch:  304 | acc: 81.25%,  total acc: 71.54%   [EVAL] batch:  305 | acc: 75.00%,  total acc: 71.55%   [EVAL] batch:  306 | acc: 50.00%,  total acc: 71.48%   [EVAL] batch:  307 | acc: 62.50%,  total acc: 71.45%   [EVAL] batch:  308 | acc: 56.25%,  total acc: 71.40%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 71.49%   [EVAL] batch:  310 | acc: 56.25%,  total acc: 71.44%   [EVAL] batch:  311 | acc: 81.25%,  total acc: 71.47%   [EVAL] batch:  312 | acc: 25.00%,  total acc: 71.33%   
cur_acc:  ['0.9524', '0.8264', '0.7470', '0.7520', '0.6587']
his_acc:  ['0.9524', '0.8815', '0.8039', '0.7535', '0.7133']
CurrentTrain: epoch 15, batch     0 | loss: 19.7106563CurrentTrain: epoch 15, batch     1 | loss: 13.4640296CurrentTrain: epoch 15, batch     2 | loss: 15.3220891CurrentTrain: epoch  1, batch     3 | loss: 10.3793734CurrentTrain: epoch 15, batch     0 | loss: 12.0505370CurrentTrain: epoch 15, batch     1 | loss: 17.7003623CurrentTrain: epoch 15, batch     2 | loss: 20.1179994CurrentTrain: epoch  1, batch     3 | loss: 7.2812800CurrentTrain: epoch 15, batch     0 | loss: 13.0677313CurrentTrain: epoch 15, batch     1 | loss: 11.4183171CurrentTrain: epoch 15, batch     2 | loss: 8.2353504CurrentTrain: epoch  1, batch     3 | loss: 10.1909834CurrentTrain: epoch 15, batch     0 | loss: 10.5820261CurrentTrain: epoch 15, batch     1 | loss: 10.4603660CurrentTrain: epoch 15, batch     2 | loss: 13.7376976CurrentTrain: epoch  1, batch     3 | loss: 16.2790884CurrentTrain: epoch 15, batch     0 | loss: 8.4685081CurrentTrain: epoch 15, batch     1 | loss: 9.5069436CurrentTrain: epoch 15, batch     2 | loss: 9.9432296CurrentTrain: epoch  1, batch     3 | loss: 8.2708609CurrentTrain: epoch 15, batch     0 | loss: 10.7992001CurrentTrain: epoch 15, batch     1 | loss: 8.6592111CurrentTrain: epoch 15, batch     2 | loss: 11.5107206CurrentTrain: epoch  1, batch     3 | loss: 7.8563351CurrentTrain: epoch 15, batch     0 | loss: 8.8733690CurrentTrain: epoch 15, batch     1 | loss: 8.4819507CurrentTrain: epoch 15, batch     2 | loss: 17.4504366CurrentTrain: epoch  1, batch     3 | loss: 6.9229257CurrentTrain: epoch 15, batch     0 | loss: 11.8720348CurrentTrain: epoch 15, batch     1 | loss: 13.1153567CurrentTrain: epoch 15, batch     2 | loss: 17.9967357CurrentTrain: epoch  1, batch     3 | loss: 5.7116249CurrentTrain: epoch 15, batch     0 | loss: 8.6755847CurrentTrain: epoch 15, batch     1 | loss: 9.1312556CurrentTrain: epoch 15, batch     2 | loss: 14.5269599CurrentTrain: epoch  1, batch     3 | loss: 7.7756761CurrentTrain: epoch 15, batch     0 | loss: 9.0651590CurrentTrain: epoch 15, batch     1 | loss: 14.3719318CurrentTrain: epoch 15, batch     2 | loss: 14.2335460CurrentTrain: epoch  1, batch     3 | loss: 7.6941916
#### spouse ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "spouse" means the subject has the object as their spouse (husband, wife, partner, etc.).
Here is an example:
Relation: spouse
Context: " clinton crazies " is a term in american politics of the 1990s and later that refers to intense criticism of united states president bill clinton and his wife hillary clinton .
Head Entity: bill clinton
Tail Entity: hillary clinton
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "spouse":
Relation: spouse  
Context: "In a lavish ceremony, actor Ryan Reynolds married actress Blake Lively, solidifying their status as one of Hollywood's power couples."  
Head Entity: Ryan Reynolds  
Tail Entity: Blake Lively  

Relation: spouse  
Context: "The couple celebrated their 25th wedding anniversary, reflecting on their journey together, with John Legend expressing his love for Chrissy Teigen in a heartfelt post."  
Head Entity: John Legend  
Tail Entity: Chrissy Teigen  
#### record label ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "record label" means brand and trademark associated with the marketing of subject music recordings and music videos.
Here is an example:
Relation: record label
Context: " i 'm a cuckoo " was belle & sebastian 's second single from " dear catastrophe waitress " , released on rough trade records in 2004 .
Head Entity: belle & sebastian
Tail Entity: rough trade records
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "record label":
Relation: record label  
Context: The album "Random Access Memories" by Daft Punk was released under the Columbia Records label in 2013.  
Head Entity: Daft Punk  
Tail Entity: Columbia Records  

Relation: record label  
Context: Taylor Swift's latest album "Evermore" was produced by her own label, Republic Records, showcasing her evolution as an artist.  
Head Entity: Taylor Swift  
Tail Entity: Republic Records  
#### distributor ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "distributor" means distributor of a creative work; distributor for a record label; news agency; film distributor.
Here is an example:
Relation: distributor
Context: distributed by paramount pictures , " the italian job " was theatrically released in the united states on may 30 , 2003 , and grossed over $ 176 million worldwide .
Head Entity: the italian job
Tail Entity: paramount pictures
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "distributor":
Relation: distributor  
Context: the film was distributed by universal pictures and received critical acclaim upon its release in 2015.  
Head Entity: the film  
Tail Entity: universal pictures  

Relation: distributor  
Context: the album was distributed by sony music and quickly climbed the charts, becoming a bestseller.  
Head Entity: the album  
Tail Entity: sony music  
#### located in or next to body of water ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "located in or next to body of water" means body of water on or next to which a place is located.
Here is an example:
Relation: located in or next to body of water
Context: in 1796 carondelet commissioned pintado to conduct a survey of lands granted to ambrosio longue - épée in iberville county on the west bank of the mississippi river opposite baton rouge .
Head Entity: baton rouge
Tail Entity: mississippi river
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "located in or next to body of water":
Relation: located in or next to body of water  
Context: the city of new orleans is situated on the banks of the mississippi river, making it a vibrant hub for trade and culture.  
Head Entity: new orleans  
Tail Entity: mississippi river  

Relation: located in or next to body of water  
Context: the picturesque village of sausalito lies just across the bay from san francisco, offering stunning views of the golden gate bridge and the surrounding waters.  
Head Entity: sausalito  
Tail Entity: san francisco bay  
#### subsidiary ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "subsidiary" means subsidiary of a company or organization; generally a fully owned separate corporation.
Here is an example:
Relation: subsidiary
Context: cbs and viacom split again in 2005 , but both are still owned by national amusements .
Head Entity: national amusements
Tail Entity: viacom
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "subsidiary":
Sample 1:  
Relation: subsidiary  
Context: In 2018, Facebook acquired WhatsApp, making it a subsidiary of the social media giant.  
Head Entity: Facebook  
Tail Entity: WhatsApp  

Sample 2:  
Relation: subsidiary  
Context: Toyota's luxury vehicle division, Lexus, operates as a subsidiary under the main company.  
Head Entity: Toyota  
Tail Entity: Lexus  
#### country of citizenship ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "country of citizenship" means the object is a country that recognizes the subject as its citizen.
Here is an example:
Relation: country of citizenship
Context: he challenged seven other swimmers in heat four including ukraine 's rostyslav svanidze , a top 16 finalist in atlanta four years earlier .
Head Entity: rostyslav svanidze
Tail Entity: ukraine
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "country of citizenship":
Relation: country of citizenship  
Context: After years of hard work and dedication, Maria Gonzalez proudly received her citizenship certificate, officially recognizing her as a citizen of Spain.  
Head Entity: Maria Gonzalez  
Tail Entity: Spain  

Relation: country of citizenship  
Context: During the international conference, it was noted that Ahmed Al-Farsi, a prominent entrepreneur, holds dual citizenship in both Oman and the United States.  
Head Entity: Ahmed Al-Farsi  
Tail Entity: Oman  
#### genre ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "genre" means creative work's genre or an artist's field of work.
Here is an example:
Relation: genre
Context: buck moved to austin in the mid-1970s and joined the blues rock group , the fabulous thunderbirds , along with keith ferguson , jimmie vaughan , and kim wilson .
Head Entity: the fabulous thunderbirds
Tail Entity: blues rock
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "genre":
Relation: genre  
Context: the album "thriller" by michael jackson is widely regarded as a landmark in pop music history, blending elements of rock, funk, and soul.  
Head Entity: michael jackson  
Tail Entity: pop  

Relation: genre  
Context: the film "inception," directed by christopher nolan, is a complex narrative that combines science fiction with psychological thriller elements.  
Head Entity: inception  
Tail Entity: science fiction  
#### mouth of the watercourse ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "mouth of the watercourse" means the body of water to which the watercourse drains.
Here is an example:
Relation: mouth of the watercourse
Context: san lázaro is 660 km far from asunción and is located at the exact site of the confluence of the apa river with paraguay river , bordering brazil - chaco .
Head Entity: apa river
Tail Entity: paraguay river
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "mouth of the watercourse":
Relation: mouth of the watercourse  
Context: the mississippi river flows southward through the united states and empties into the gulf of mexico, making it a vital watercourse for trade and transportation.  
Head Entity: mississippi river  
Tail Entity: gulf of mexico  

Relation: mouth of the watercourse  
Context: the amazon river, known for its vast drainage basin, ultimately discharges into the atlantic ocean, serving as a crucial ecological and economic resource for the surrounding regions.  
Head Entity: amazon river  
Tail Entity: atlantic ocean  
#### position played on team / speciality ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "position played on team / speciality" means position or specialism of a player on a team.
Here is an example:
Relation: position played on team / speciality
Context: bradley halliday ( born 10 july 1995 ) is an english professional footballer who plays as a defender or a midfielder for league two club cambridge united .
Head Entity: bradley halliday
Tail Entity: defender
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "position played on team / speciality":
Relation: position played on team / speciality  
Context: alex morgan ( born 2 july 1989 ) is an american soccer player who plays as a forward for the national team and club san diego wave fc in the nwsl.  
Head Entity: alex morgan  
Tail Entity: forward  

Relation: position played on team / speciality  
Context: lebron james ( born 30 december 1984 ) is an american professional basketball player who plays as a small forward for the los angeles lakers in the nba.  
Head Entity: lebron james  
Tail Entity: small forward  
#### sports season of league or competition ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "sports season of league or competition" means property that shows the competition of which the item is a season.
Here is an example:
Relation: sports season of league or competition
Context: the stade louis ii in monaco was the venue for the uefa super cup every year since 1998 .
Head Entity: 1998
Tail Entity: uefa super cup
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "sports season of league or competition":
Relation: sports season of league or competition  
Context: The 2020 NBA season was significantly impacted by the COVID-19 pandemic, leading to a unique playoff format.  
Head Entity: 2020  
Tail Entity: NBA  

Relation: sports season of league or competition  
Context: The 2019 Rugby World Cup took place in Japan, showcasing teams from around the globe.  
Head Entity: 2019  
Tail Entity: Rugby World Cup  
MemoryTrain:  epoch 15, batch     0 | loss: 10.1819573MemoryTrain:  epoch 15, batch     1 | loss: 5.0480972MemoryTrain:  epoch 15, batch     2 | loss: 4.3598088MemoryTrain:  epoch 15, batch     3 | loss: 5.4026937MemoryTrain:  epoch 15, batch     4 | loss: 3.8346416MemoryTrain:  epoch 15, batch     5 | loss: 7.0373265MemoryTrain:  epoch 15, batch     6 | loss: 3.6651577MemoryTrain:  epoch 15, batch     7 | loss: 5.4816524MemoryTrain:  epoch 15, batch     8 | loss: 3.3692001MemoryTrain:  epoch 15, batch     9 | loss: 3.8999705MemoryTrain:  epoch 15, batch    10 | loss: 5.6428418MemoryTrain:  epoch  3, batch    11 | loss: 12.2442489MemoryTrain:  epoch 15, batch     0 | loss: 5.2212358MemoryTrain:  epoch 15, batch     1 | loss: 2.5402597MemoryTrain:  epoch 15, batch     2 | loss: 2.1528669MemoryTrain:  epoch 15, batch     3 | loss: 3.3913080MemoryTrain:  epoch 15, batch     4 | loss: 2.7017207MemoryTrain:  epoch 15, batch     5 | loss: 3.6667704MemoryTrain:  epoch 15, batch     6 | loss: 3.2832635MemoryTrain:  epoch 15, batch     7 | loss: 2.8548412MemoryTrain:  epoch 15, batch     8 | loss: 3.1986544MemoryTrain:  epoch 15, batch     9 | loss: 3.6521504MemoryTrain:  epoch 15, batch    10 | loss: 4.5980241MemoryTrain:  epoch  3, batch    11 | loss: 12.9334119MemoryTrain:  epoch 15, batch     0 | loss: 2.7944828MemoryTrain:  epoch 15, batch     1 | loss: 2.0542155MemoryTrain:  epoch 15, batch     2 | loss: 2.6641763MemoryTrain:  epoch 15, batch     3 | loss: 3.3195253MemoryTrain:  epoch 15, batch     4 | loss: 4.3097229MemoryTrain:  epoch 15, batch     5 | loss: 3.6365126MemoryTrain:  epoch 15, batch     6 | loss: 2.9198680MemoryTrain:  epoch 15, batch     7 | loss: 4.6278065MemoryTrain:  epoch 15, batch     8 | loss: 2.8002846MemoryTrain:  epoch 15, batch     9 | loss: 5.3081723MemoryTrain:  epoch 15, batch    10 | loss: 4.8435023MemoryTrain:  epoch  3, batch    11 | loss: 12.1488627MemoryTrain:  epoch 15, batch     0 | loss: 2.3689167MemoryTrain:  epoch 15, batch     1 | loss: 3.8908068MemoryTrain:  epoch 15, batch     2 | loss: 2.8827000MemoryTrain:  epoch 15, batch     3 | loss: 2.7138947MemoryTrain:  epoch 15, batch     4 | loss: 4.7856302MemoryTrain:  epoch 15, batch     5 | loss: 3.6094350MemoryTrain:  epoch 15, batch     6 | loss: 4.7774371MemoryTrain:  epoch 15, batch     7 | loss: 2.8867259MemoryTrain:  epoch 15, batch     8 | loss: 2.6498247MemoryTrain:  epoch 15, batch     9 | loss: 3.0135654MemoryTrain:  epoch 15, batch    10 | loss: 1.7944776MemoryTrain:  epoch  3, batch    11 | loss: 10.5161059MemoryTrain:  epoch 15, batch     0 | loss: 2.7478756MemoryTrain:  epoch 15, batch     1 | loss: 2.2029618MemoryTrain:  epoch 15, batch     2 | loss: 4.6088074MemoryTrain:  epoch 15, batch     3 | loss: 4.6757246MemoryTrain:  epoch 15, batch     4 | loss: 2.0505448MemoryTrain:  epoch 15, batch     5 | loss: 1.8898917MemoryTrain:  epoch 15, batch     6 | loss: 2.5801708MemoryTrain:  epoch 15, batch     7 | loss: 3.3427949MemoryTrain:  epoch 15, batch     8 | loss: 1.9548121MemoryTrain:  epoch 15, batch     9 | loss: 3.2880209MemoryTrain:  epoch 15, batch    10 | loss: 2.7360519MemoryTrain:  epoch  3, batch    11 | loss: 10.6090276MemoryTrain:  epoch 15, batch     0 | loss: 4.8902944MemoryTrain:  epoch 15, batch     1 | loss: 2.2221845MemoryTrain:  epoch 15, batch     2 | loss: 2.6715807MemoryTrain:  epoch 15, batch     3 | loss: 4.9314791MemoryTrain:  epoch 15, batch     4 | loss: 1.7809719MemoryTrain:  epoch 15, batch     5 | loss: 1.9460330MemoryTrain:  epoch 15, batch     6 | loss: 1.9203975MemoryTrain:  epoch 15, batch     7 | loss: 2.6223319MemoryTrain:  epoch 15, batch     8 | loss: 2.2681438MemoryTrain:  epoch 15, batch     9 | loss: 1.5441364MemoryTrain:  epoch 15, batch    10 | loss: 2.2044353MemoryTrain:  epoch  3, batch    11 | loss: 10.3930769MemoryTrain:  epoch 15, batch     0 | loss: 4.3832458MemoryTrain:  epoch 15, batch     1 | loss: 2.4683258MemoryTrain:  epoch 15, batch     2 | loss: 2.5749968MemoryTrain:  epoch 15, batch     3 | loss: 2.2093324MemoryTrain:  epoch 15, batch     4 | loss: 3.4100767MemoryTrain:  epoch 15, batch     5 | loss: 1.8801275MemoryTrain:  epoch 15, batch     6 | loss: 1.6523843MemoryTrain:  epoch 15, batch     7 | loss: 3.0282262MemoryTrain:  epoch 15, batch     8 | loss: 2.6802136MemoryTrain:  epoch 15, batch     9 | loss: 2.1056434MemoryTrain:  epoch 15, batch    10 | loss: 1.6609911MemoryTrain:  epoch  3, batch    11 | loss: 10.2683326MemoryTrain:  epoch 15, batch     0 | loss: 4.4129605MemoryTrain:  epoch 15, batch     1 | loss: 3.1443286MemoryTrain:  epoch 15, batch     2 | loss: 5.7851107MemoryTrain:  epoch 15, batch     3 | loss: 4.6247089MemoryTrain:  epoch 15, batch     4 | loss: 5.6044142MemoryTrain:  epoch 15, batch     5 | loss: 1.6039058MemoryTrain:  epoch 15, batch     6 | loss: 4.4595890MemoryTrain:  epoch 15, batch     7 | loss: 3.4170130MemoryTrain:  epoch 15, batch     8 | loss: 1.5826199MemoryTrain:  epoch 15, batch     9 | loss: 3.2923230MemoryTrain:  epoch 15, batch    10 | loss: 2.0036834MemoryTrain:  epoch  3, batch    11 | loss: 10.4779066MemoryTrain:  epoch 15, batch     0 | loss: 1.8632698MemoryTrain:  epoch 15, batch     1 | loss: 2.8031453MemoryTrain:  epoch 15, batch     2 | loss: 2.6796536MemoryTrain:  epoch 15, batch     3 | loss: 2.2173110MemoryTrain:  epoch 15, batch     4 | loss: 1.9800135MemoryTrain:  epoch 15, batch     5 | loss: 1.4874379MemoryTrain:  epoch 15, batch     6 | loss: 2.9020625MemoryTrain:  epoch 15, batch     7 | loss: 4.2721246MemoryTrain:  epoch 15, batch     8 | loss: 2.0378062MemoryTrain:  epoch 15, batch     9 | loss: 2.9986031MemoryTrain:  epoch 15, batch    10 | loss: 2.0799379MemoryTrain:  epoch  3, batch    11 | loss: 25.9699314MemoryTrain:  epoch 15, batch     0 | loss: 2.3481646MemoryTrain:  epoch 15, batch     1 | loss: 4.0110357MemoryTrain:  epoch 15, batch     2 | loss: 2.9165668MemoryTrain:  epoch 15, batch     3 | loss: 1.8748306MemoryTrain:  epoch 15, batch     4 | loss: 2.3010852MemoryTrain:  epoch 15, batch     5 | loss: 1.9432081MemoryTrain:  epoch 15, batch     6 | loss: 2.3833499MemoryTrain:  epoch 15, batch     7 | loss: 2.7483302MemoryTrain:  epoch 15, batch     8 | loss: 1.7982994MemoryTrain:  epoch 15, batch     9 | loss: 2.4594669MemoryTrain:  epoch 15, batch    10 | loss: 2.4065023MemoryTrain:  epoch  3, batch    11 | loss: 23.3452023
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    1 | acc: 50.00%,  total acc: 56.25%   [EVAL] batch:    2 | acc: 56.25%,  total acc: 56.25%   [EVAL] batch:    3 | acc: 56.25%,  total acc: 56.25%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 60.00%   [EVAL] batch:    5 | acc: 62.50%,  total acc: 60.42%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 65.18%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 69.53%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 72.22%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 73.75%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 75.00%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 77.08%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 77.88%   [EVAL] batch:   13 | acc: 56.25%,  total acc: 76.34%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 75.83%   [EVAL] batch:   15 | acc: 81.25%,  total acc: 76.17%   [EVAL] batch:   16 | acc: 87.50%,  total acc: 76.84%   [EVAL] batch:   17 | acc: 56.25%,  total acc: 75.69%   [EVAL] batch:   18 | acc: 81.25%,  total acc: 75.99%   [EVAL] batch:   19 | acc: 43.75%,  total acc: 74.38%   [EVAL] batch:   20 | acc: 50.00%,  total acc: 73.21%   [EVAL] batch:   21 | acc: 25.00%,  total acc: 71.02%   [EVAL] batch:   22 | acc: 43.75%,  total acc: 69.84%   [EVAL] batch:   23 | acc: 62.50%,  total acc: 69.53%   [EVAL] batch:   24 | acc: 37.50%,  total acc: 68.25%   [EVAL] batch:   25 | acc: 87.50%,  total acc: 68.99%   [EVAL] batch:   26 | acc: 87.50%,  total acc: 69.68%   [EVAL] batch:   27 | acc: 75.00%,  total acc: 69.87%   [EVAL] batch:   28 | acc: 68.75%,  total acc: 69.83%   [EVAL] batch:   29 | acc: 75.00%,  total acc: 70.00%   [EVAL] batch:   30 | acc: 50.00%,  total acc: 69.35%   [EVAL] batch:   31 | acc: 68.75%,  total acc: 69.34%   [EVAL] batch:   32 | acc: 62.50%,  total acc: 69.13%   [EVAL] batch:   33 | acc: 62.50%,  total acc: 68.93%   [EVAL] batch:   34 | acc: 62.50%,  total acc: 68.75%   [EVAL] batch:   35 | acc: 75.00%,  total acc: 68.92%   [EVAL] batch:   36 | acc: 56.25%,  total acc: 68.58%   [EVAL] batch:   37 | acc: 62.50%,  total acc: 68.42%   [EVAL] batch:   38 | acc: 68.75%,  total acc: 68.43%   [EVAL] batch:   39 | acc: 75.00%,  total acc: 68.59%   [EVAL] batch:   40 | acc: 62.50%,  total acc: 68.45%   [EVAL] batch:   41 | acc: 81.25%,  total acc: 68.75%   [EVAL] batch:   42 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:   43 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:   44 | acc: 31.25%,  total acc: 67.92%   [EVAL] batch:   45 | acc: 31.25%,  total acc: 67.12%   [EVAL] batch:   46 | acc: 25.00%,  total acc: 66.22%   [EVAL] batch:   47 | acc: 50.00%,  total acc: 65.89%   [EVAL] batch:   48 | acc: 31.25%,  total acc: 65.18%   [EVAL] batch:   49 | acc: 50.00%,  total acc: 64.88%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 65.56%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 66.23%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 66.86%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 67.48%   [EVAL] batch:   54 | acc: 100.00%,  total acc: 68.07%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 68.64%   [EVAL] batch:   56 | acc: 87.50%,  total acc: 68.97%   [EVAL] batch:   57 | acc: 81.25%,  total acc: 69.18%   [EVAL] batch:   58 | acc: 81.25%,  total acc: 69.39%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 69.79%   [EVAL] batch:   60 | acc: 87.50%,  total acc: 70.08%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 70.46%   [EVAL] batch:   62 | acc: 37.50%,  total acc: 69.94%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 75.00%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 77.08%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 76.56%   [EVAL] batch:    4 | acc: 62.50%,  total acc: 73.75%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 76.04%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 75.89%   [EVAL] batch:    7 | acc: 68.75%,  total acc: 75.00%   [EVAL] batch:    8 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 74.38%   [EVAL] batch:   10 | acc: 43.75%,  total acc: 71.59%   [EVAL] batch:   11 | acc: 50.00%,  total acc: 69.79%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 70.67%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 72.32%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 73.75%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 75.39%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 76.47%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 77.43%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 78.62%   [EVAL] batch:   19 | acc: 68.75%,  total acc: 78.12%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 78.57%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 78.69%   [EVAL] batch:   22 | acc: 62.50%,  total acc: 77.99%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 78.39%   [EVAL] batch:   24 | acc: 75.00%,  total acc: 78.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 79.09%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 79.86%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 80.58%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 81.88%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 82.46%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 83.01%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 83.52%   [EVAL] batch:   33 | acc: 81.25%,  total acc: 83.46%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 83.21%   [EVAL] batch:   35 | acc: 81.25%,  total acc: 83.16%   [EVAL] batch:   36 | acc: 93.75%,  total acc: 83.45%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 83.72%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 84.13%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 84.53%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 84.91%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 85.27%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 85.47%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 85.37%   [EVAL] batch:   44 | acc: 68.75%,  total acc: 85.00%   [EVAL] batch:   45 | acc: 56.25%,  total acc: 84.38%   [EVAL] batch:   46 | acc: 31.25%,  total acc: 83.24%   [EVAL] batch:   47 | acc: 56.25%,  total acc: 82.68%   [EVAL] batch:   48 | acc: 68.75%,  total acc: 82.40%   [EVAL] batch:   49 | acc: 75.00%,  total acc: 82.25%   [EVAL] batch:   50 | acc: 87.50%,  total acc: 82.35%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 82.45%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 82.67%   [EVAL] batch:   53 | acc: 93.75%,  total acc: 82.87%   [EVAL] batch:   54 | acc: 68.75%,  total acc: 82.61%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 82.70%   [EVAL] batch:   56 | acc: 56.25%,  total acc: 82.24%   [EVAL] batch:   57 | acc: 43.75%,  total acc: 81.57%   [EVAL] batch:   58 | acc: 62.50%,  total acc: 81.25%   [EVAL] batch:   59 | acc: 56.25%,  total acc: 80.83%   [EVAL] batch:   60 | acc: 62.50%,  total acc: 80.53%   [EVAL] batch:   61 | acc: 62.50%,  total acc: 80.24%   [EVAL] batch:   62 | acc: 62.50%,  total acc: 79.96%   [EVAL] batch:   63 | acc: 75.00%,  total acc: 79.88%   [EVAL] batch:   64 | acc: 75.00%,  total acc: 79.81%   [EVAL] batch:   65 | acc: 87.50%,  total acc: 79.92%   [EVAL] batch:   66 | acc: 62.50%,  total acc: 79.66%   [EVAL] batch:   67 | acc: 87.50%,  total acc: 79.78%   [EVAL] batch:   68 | acc: 87.50%,  total acc: 79.89%   [EVAL] batch:   69 | acc: 87.50%,  total acc: 80.00%   [EVAL] batch:   70 | acc: 68.75%,  total acc: 79.84%   [EVAL] batch:   71 | acc: 62.50%,  total acc: 79.60%   [EVAL] batch:   72 | acc: 75.00%,  total acc: 79.54%   [EVAL] batch:   73 | acc: 75.00%,  total acc: 79.48%   [EVAL] batch:   74 | acc: 81.25%,  total acc: 79.50%   [EVAL] batch:   75 | acc: 87.50%,  total acc: 79.61%   [EVAL] batch:   76 | acc: 75.00%,  total acc: 79.55%   [EVAL] batch:   77 | acc: 68.75%,  total acc: 79.41%   [EVAL] batch:   78 | acc: 87.50%,  total acc: 79.51%   [EVAL] batch:   79 | acc: 87.50%,  total acc: 79.61%   [EVAL] batch:   80 | acc: 87.50%,  total acc: 79.71%   [EVAL] batch:   81 | acc: 25.00%,  total acc: 79.04%   [EVAL] batch:   82 | acc: 6.25%,  total acc: 78.16%   [EVAL] batch:   83 | acc: 6.25%,  total acc: 77.31%   [EVAL] batch:   84 | acc: 0.00%,  total acc: 76.40%   [EVAL] batch:   85 | acc: 12.50%,  total acc: 75.65%   [EVAL] batch:   86 | acc: 18.75%,  total acc: 75.00%   [EVAL] batch:   87 | acc: 43.75%,  total acc: 74.64%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 74.93%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 75.21%   [EVAL] batch:   90 | acc: 87.50%,  total acc: 75.34%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 75.61%   [EVAL] batch:   92 | acc: 93.75%,  total acc: 75.81%   [EVAL] batch:   93 | acc: 93.75%,  total acc: 76.00%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 76.25%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 76.50%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 76.74%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 76.98%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 77.21%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 77.44%   [EVAL] batch:  100 | acc: 93.75%,  total acc: 77.60%   [EVAL] batch:  101 | acc: 93.75%,  total acc: 77.76%   [EVAL] batch:  102 | acc: 68.75%,  total acc: 77.67%   [EVAL] batch:  103 | acc: 81.25%,  total acc: 77.70%   [EVAL] batch:  104 | acc: 100.00%,  total acc: 77.92%   [EVAL] batch:  105 | acc: 93.75%,  total acc: 78.07%   [EVAL] batch:  106 | acc: 43.75%,  total acc: 77.75%   [EVAL] batch:  107 | acc: 37.50%,  total acc: 77.37%   [EVAL] batch:  108 | acc: 37.50%,  total acc: 77.01%   [EVAL] batch:  109 | acc: 31.25%,  total acc: 76.59%   [EVAL] batch:  110 | acc: 62.50%,  total acc: 76.46%   [EVAL] batch:  111 | acc: 37.50%,  total acc: 76.12%   [EVAL] batch:  112 | acc: 62.50%,  total acc: 76.00%   [EVAL] batch:  113 | acc: 87.50%,  total acc: 76.10%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 76.30%   [EVAL] batch:  115 | acc: 81.25%,  total acc: 76.35%   [EVAL] batch:  116 | acc: 93.75%,  total acc: 76.50%   [EVAL] batch:  117 | acc: 93.75%,  total acc: 76.64%   [EVAL] batch:  118 | acc: 68.75%,  total acc: 76.58%   [EVAL] batch:  119 | acc: 6.25%,  total acc: 75.99%   [EVAL] batch:  120 | acc: 18.75%,  total acc: 75.52%   [EVAL] batch:  121 | acc: 0.00%,  total acc: 74.90%   [EVAL] batch:  122 | acc: 0.00%,  total acc: 74.29%   [EVAL] batch:  123 | acc: 0.00%,  total acc: 73.69%   [EVAL] batch:  124 | acc: 0.00%,  total acc: 73.10%   [EVAL] batch:  125 | acc: 43.75%,  total acc: 72.87%   [EVAL] batch:  126 | acc: 31.25%,  total acc: 72.54%   [EVAL] batch:  127 | acc: 50.00%,  total acc: 72.36%   [EVAL] batch:  128 | acc: 81.25%,  total acc: 72.43%   [EVAL] batch:  129 | acc: 62.50%,  total acc: 72.36%   [EVAL] batch:  130 | acc: 50.00%,  total acc: 72.19%   [EVAL] batch:  131 | acc: 75.00%,  total acc: 72.21%   [EVAL] batch:  132 | acc: 75.00%,  total acc: 72.23%   [EVAL] batch:  133 | acc: 62.50%,  total acc: 72.15%   [EVAL] batch:  134 | acc: 87.50%,  total acc: 72.27%   [EVAL] batch:  135 | acc: 87.50%,  total acc: 72.38%   [EVAL] batch:  136 | acc: 87.50%,  total acc: 72.49%   [EVAL] batch:  137 | acc: 56.25%,  total acc: 72.37%   [EVAL] batch:  138 | acc: 12.50%,  total acc: 71.94%   [EVAL] batch:  139 | acc: 25.00%,  total acc: 71.61%   [EVAL] batch:  140 | acc: 18.75%,  total acc: 71.23%   [EVAL] batch:  141 | acc: 43.75%,  total acc: 71.04%   [EVAL] batch:  142 | acc: 43.75%,  total acc: 70.85%   [EVAL] batch:  143 | acc: 37.50%,  total acc: 70.62%   [EVAL] batch:  144 | acc: 100.00%,  total acc: 70.82%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 71.02%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 71.17%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 71.37%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 71.56%   [EVAL] batch:  149 | acc: 100.00%,  total acc: 71.75%   [EVAL] batch:  150 | acc: 37.50%,  total acc: 71.52%   [EVAL] batch:  151 | acc: 18.75%,  total acc: 71.18%   [EVAL] batch:  152 | acc: 25.00%,  total acc: 70.87%   [EVAL] batch:  153 | acc: 31.25%,  total acc: 70.62%   [EVAL] batch:  154 | acc: 37.50%,  total acc: 70.40%   [EVAL] batch:  155 | acc: 31.25%,  total acc: 70.15%   [EVAL] batch:  156 | acc: 81.25%,  total acc: 70.22%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 70.37%   [EVAL] batch:  158 | acc: 81.25%,  total acc: 70.44%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 70.59%   [EVAL] batch:  160 | acc: 87.50%,  total acc: 70.69%   [EVAL] batch:  161 | acc: 81.25%,  total acc: 70.76%   [EVAL] batch:  162 | acc: 93.75%,  total acc: 70.90%   [EVAL] batch:  163 | acc: 50.00%,  total acc: 70.77%   [EVAL] batch:  164 | acc: 81.25%,  total acc: 70.83%   [EVAL] batch:  165 | acc: 62.50%,  total acc: 70.78%   [EVAL] batch:  166 | acc: 62.50%,  total acc: 70.73%   [EVAL] batch:  167 | acc: 62.50%,  total acc: 70.68%   [EVAL] batch:  168 | acc: 75.00%,  total acc: 70.71%   [EVAL] batch:  169 | acc: 56.25%,  total acc: 70.62%   [EVAL] batch:  170 | acc: 81.25%,  total acc: 70.69%   [EVAL] batch:  171 | acc: 68.75%,  total acc: 70.68%   [EVAL] batch:  172 | acc: 56.25%,  total acc: 70.59%   [EVAL] batch:  173 | acc: 75.00%,  total acc: 70.62%   [EVAL] batch:  174 | acc: 87.50%,  total acc: 70.71%   [EVAL] batch:  175 | acc: 31.25%,  total acc: 70.49%   [EVAL] batch:  176 | acc: 31.25%,  total acc: 70.27%   [EVAL] batch:  177 | acc: 43.75%,  total acc: 70.12%   [EVAL] batch:  178 | acc: 37.50%,  total acc: 69.94%   [EVAL] batch:  179 | acc: 31.25%,  total acc: 69.72%   [EVAL] batch:  180 | acc: 43.75%,  total acc: 69.58%   [EVAL] batch:  181 | acc: 68.75%,  total acc: 69.57%   [EVAL] batch:  182 | acc: 81.25%,  total acc: 69.64%   [EVAL] batch:  183 | acc: 93.75%,  total acc: 69.77%   [EVAL] batch:  184 | acc: 75.00%,  total acc: 69.80%   [EVAL] batch:  185 | acc: 87.50%,  total acc: 69.89%   [EVAL] batch:  186 | acc: 75.00%,  total acc: 69.92%   [EVAL] batch:  187 | acc: 62.50%,  total acc: 69.88%   [EVAL] batch:  188 | acc: 12.50%,  total acc: 69.58%   [EVAL] batch:  189 | acc: 43.75%,  total acc: 69.44%   [EVAL] batch:  190 | acc: 31.25%,  total acc: 69.24%   [EVAL] batch:  191 | acc: 25.00%,  total acc: 69.01%   [EVAL] batch:  192 | acc: 43.75%,  total acc: 68.88%   [EVAL] batch:  193 | acc: 25.00%,  total acc: 68.65%   [EVAL] batch:  194 | acc: 75.00%,  total acc: 68.69%   [EVAL] batch:  195 | acc: 87.50%,  total acc: 68.78%   [EVAL] batch:  196 | acc: 81.25%,  total acc: 68.85%   [EVAL] batch:  197 | acc: 81.25%,  total acc: 68.91%   [EVAL] batch:  198 | acc: 75.00%,  total acc: 68.94%   [EVAL] batch:  199 | acc: 68.75%,  total acc: 68.94%   [EVAL] batch:  200 | acc: 68.75%,  total acc: 68.94%   [EVAL] batch:  201 | acc: 37.50%,  total acc: 68.78%   [EVAL] batch:  202 | acc: 50.00%,  total acc: 68.69%   [EVAL] batch:  203 | acc: 56.25%,  total acc: 68.63%   [EVAL] batch:  204 | acc: 68.75%,  total acc: 68.63%   [EVAL] batch:  205 | acc: 37.50%,  total acc: 68.48%   [EVAL] batch:  206 | acc: 93.75%,  total acc: 68.60%   [EVAL] batch:  207 | acc: 100.00%,  total acc: 68.75%   [EVAL] batch:  208 | acc: 100.00%,  total acc: 68.90%   [EVAL] batch:  209 | acc: 93.75%,  total acc: 69.02%   [EVAL] batch:  210 | acc: 93.75%,  total acc: 69.14%   [EVAL] batch:  211 | acc: 100.00%,  total acc: 69.28%   [EVAL] batch:  212 | acc: 93.75%,  total acc: 69.40%   [EVAL] batch:  213 | acc: 81.25%,  total acc: 69.45%   [EVAL] batch:  214 | acc: 75.00%,  total acc: 69.48%   [EVAL] batch:  215 | acc: 93.75%,  total acc: 69.59%   [EVAL] batch:  216 | acc: 75.00%,  total acc: 69.61%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 69.75%   [EVAL] batch:  218 | acc: 87.50%,  total acc: 69.83%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 69.97%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 70.11%   [EVAL] batch:  221 | acc: 87.50%,  total acc: 70.19%   [EVAL] batch:  222 | acc: 87.50%,  total acc: 70.26%   [EVAL] batch:  223 | acc: 93.75%,  total acc: 70.37%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 70.50%   [EVAL] batch:  225 | acc: 68.75%,  total acc: 70.49%   [EVAL] batch:  226 | acc: 56.25%,  total acc: 70.43%   [EVAL] batch:  227 | acc: 50.00%,  total acc: 70.34%   [EVAL] batch:  228 | acc: 56.25%,  total acc: 70.28%   [EVAL] batch:  229 | acc: 43.75%,  total acc: 70.16%   [EVAL] batch:  230 | acc: 50.00%,  total acc: 70.08%   [EVAL] batch:  231 | acc: 87.50%,  total acc: 70.15%   [EVAL] batch:  232 | acc: 93.75%,  total acc: 70.25%   [EVAL] batch:  233 | acc: 93.75%,  total acc: 70.35%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 70.48%   [EVAL] batch:  235 | acc: 100.00%,  total acc: 70.60%   [EVAL] batch:  236 | acc: 93.75%,  total acc: 70.70%   [EVAL] batch:  237 | acc: 56.25%,  total acc: 70.64%   [EVAL] batch:  238 | acc: 50.00%,  total acc: 70.55%   [EVAL] batch:  239 | acc: 43.75%,  total acc: 70.44%   [EVAL] batch:  240 | acc: 50.00%,  total acc: 70.36%   [EVAL] batch:  241 | acc: 37.50%,  total acc: 70.22%   [EVAL] batch:  242 | acc: 37.50%,  total acc: 70.09%   [EVAL] batch:  243 | acc: 25.00%,  total acc: 69.90%   [EVAL] batch:  244 | acc: 75.00%,  total acc: 69.92%   [EVAL] batch:  245 | acc: 62.50%,  total acc: 69.89%   [EVAL] batch:  246 | acc: 50.00%,  total acc: 69.81%   [EVAL] batch:  247 | acc: 68.75%,  total acc: 69.81%   [EVAL] batch:  248 | acc: 87.50%,  total acc: 69.88%   [EVAL] batch:  249 | acc: 62.50%,  total acc: 69.85%   [EVAL] batch:  250 | acc: 0.00%,  total acc: 69.57%   [EVAL] batch:  251 | acc: 31.25%,  total acc: 69.42%   [EVAL] batch:  252 | acc: 12.50%,  total acc: 69.19%   [EVAL] batch:  253 | acc: 0.00%,  total acc: 68.92%   [EVAL] batch:  254 | acc: 6.25%,  total acc: 68.68%   [EVAL] batch:  255 | acc: 12.50%,  total acc: 68.46%   [EVAL] batch:  256 | acc: 75.00%,  total acc: 68.48%   [EVAL] batch:  257 | acc: 75.00%,  total acc: 68.51%   [EVAL] batch:  258 | acc: 81.25%,  total acc: 68.56%   [EVAL] batch:  259 | acc: 75.00%,  total acc: 68.58%   [EVAL] batch:  260 | acc: 75.00%,  total acc: 68.61%   [EVAL] batch:  261 | acc: 87.50%,  total acc: 68.68%   [EVAL] batch:  262 | acc: 87.50%,  total acc: 68.75%   [EVAL] batch:  263 | acc: 75.00%,  total acc: 68.77%   [EVAL] batch:  264 | acc: 81.25%,  total acc: 68.82%   [EVAL] batch:  265 | acc: 81.25%,  total acc: 68.87%   [EVAL] batch:  266 | acc: 87.50%,  total acc: 68.94%   [EVAL] batch:  267 | acc: 87.50%,  total acc: 69.01%   [EVAL] batch:  268 | acc: 62.50%,  total acc: 68.98%   [EVAL] batch:  269 | acc: 25.00%,  total acc: 68.82%   [EVAL] batch:  270 | acc: 12.50%,  total acc: 68.61%   [EVAL] batch:  271 | acc: 6.25%,  total acc: 68.38%   [EVAL] batch:  272 | acc: 6.25%,  total acc: 68.15%   [EVAL] batch:  273 | acc: 12.50%,  total acc: 67.95%   [EVAL] batch:  274 | acc: 18.75%,  total acc: 67.77%   [EVAL] batch:  275 | acc: 6.25%,  total acc: 67.55%   [EVAL] batch:  276 | acc: 12.50%,  total acc: 67.35%   [EVAL] batch:  277 | acc: 18.75%,  total acc: 67.18%   [EVAL] batch:  278 | acc: 6.25%,  total acc: 66.96%   [EVAL] batch:  279 | acc: 25.00%,  total acc: 66.81%   [EVAL] batch:  280 | acc: 0.00%,  total acc: 66.57%   [EVAL] batch:  281 | acc: 68.75%,  total acc: 66.58%   [EVAL] batch:  282 | acc: 93.75%,  total acc: 66.67%   [EVAL] batch:  283 | acc: 87.50%,  total acc: 66.75%   [EVAL] batch:  284 | acc: 100.00%,  total acc: 66.86%   [EVAL] batch:  285 | acc: 87.50%,  total acc: 66.94%   [EVAL] batch:  286 | acc: 87.50%,  total acc: 67.01%   [EVAL] batch:  287 | acc: 100.00%,  total acc: 67.12%   [EVAL] batch:  288 | acc: 87.50%,  total acc: 67.19%   [EVAL] batch:  289 | acc: 100.00%,  total acc: 67.31%   [EVAL] batch:  290 | acc: 93.75%,  total acc: 67.40%   [EVAL] batch:  291 | acc: 81.25%,  total acc: 67.44%   [EVAL] batch:  292 | acc: 100.00%,  total acc: 67.56%   [EVAL] batch:  293 | acc: 100.00%,  total acc: 67.67%   [EVAL] batch:  294 | acc: 81.25%,  total acc: 67.71%   [EVAL] batch:  295 | acc: 81.25%,  total acc: 67.76%   [EVAL] batch:  296 | acc: 68.75%,  total acc: 67.76%   [EVAL] batch:  297 | acc: 75.00%,  total acc: 67.79%   [EVAL] batch:  298 | acc: 100.00%,  total acc: 67.89%   [EVAL] batch:  299 | acc: 81.25%,  total acc: 67.94%   [EVAL] batch:  300 | acc: 37.50%,  total acc: 67.84%   [EVAL] batch:  301 | acc: 56.25%,  total acc: 67.80%   [EVAL] batch:  302 | acc: 68.75%,  total acc: 67.80%   [EVAL] batch:  303 | acc: 43.75%,  total acc: 67.72%   [EVAL] batch:  304 | acc: 81.25%,  total acc: 67.77%   [EVAL] batch:  305 | acc: 68.75%,  total acc: 67.77%   [EVAL] batch:  306 | acc: 37.50%,  total acc: 67.67%   [EVAL] batch:  307 | acc: 50.00%,  total acc: 67.61%   [EVAL] batch:  308 | acc: 50.00%,  total acc: 67.56%   [EVAL] batch:  309 | acc: 81.25%,  total acc: 67.60%   [EVAL] batch:  310 | acc: 56.25%,  total acc: 67.56%   [EVAL] batch:  311 | acc: 87.50%,  total acc: 67.63%   [EVAL] batch:  312 | acc: 50.00%,  total acc: 67.57%   [EVAL] batch:  313 | acc: 50.00%,  total acc: 67.52%   [EVAL] batch:  314 | acc: 68.75%,  total acc: 67.52%   [EVAL] batch:  315 | acc: 50.00%,  total acc: 67.46%   [EVAL] batch:  316 | acc: 62.50%,  total acc: 67.45%   [EVAL] batch:  317 | acc: 75.00%,  total acc: 67.47%   [EVAL] batch:  318 | acc: 75.00%,  total acc: 67.50%   [EVAL] batch:  319 | acc: 100.00%,  total acc: 67.60%   [EVAL] batch:  320 | acc: 100.00%,  total acc: 67.70%   [EVAL] batch:  321 | acc: 81.25%,  total acc: 67.74%   [EVAL] batch:  322 | acc: 100.00%,  total acc: 67.84%   [EVAL] batch:  323 | acc: 87.50%,  total acc: 67.90%   [EVAL] batch:  324 | acc: 87.50%,  total acc: 67.96%   [EVAL] batch:  325 | acc: 81.25%,  total acc: 68.00%   [EVAL] batch:  326 | acc: 56.25%,  total acc: 67.97%   [EVAL] batch:  327 | acc: 81.25%,  total acc: 68.01%   [EVAL] batch:  328 | acc: 75.00%,  total acc: 68.03%   [EVAL] batch:  329 | acc: 81.25%,  total acc: 68.07%   [EVAL] batch:  330 | acc: 68.75%,  total acc: 68.07%   [EVAL] batch:  331 | acc: 56.25%,  total acc: 68.03%   [EVAL] batch:  332 | acc: 50.00%,  total acc: 67.98%   [EVAL] batch:  333 | acc: 43.75%,  total acc: 67.91%   [EVAL] batch:  334 | acc: 31.25%,  total acc: 67.80%   [EVAL] batch:  335 | acc: 50.00%,  total acc: 67.75%   [EVAL] batch:  336 | acc: 43.75%,  total acc: 67.67%   [EVAL] batch:  337 | acc: 75.00%,  total acc: 67.70%   [EVAL] batch:  338 | acc: 87.50%,  total acc: 67.75%   [EVAL] batch:  339 | acc: 68.75%,  total acc: 67.76%   [EVAL] batch:  340 | acc: 75.00%,  total acc: 67.78%   [EVAL] batch:  341 | acc: 68.75%,  total acc: 67.78%   [EVAL] batch:  342 | acc: 75.00%,  total acc: 67.80%   [EVAL] batch:  343 | acc: 37.50%,  total acc: 67.71%   [EVAL] batch:  344 | acc: 87.50%,  total acc: 67.77%   [EVAL] batch:  345 | acc: 43.75%,  total acc: 67.70%   [EVAL] batch:  346 | acc: 68.75%,  total acc: 67.71%   [EVAL] batch:  347 | acc: 68.75%,  total acc: 67.71%   [EVAL] batch:  348 | acc: 75.00%,  total acc: 67.73%   [EVAL] batch:  349 | acc: 50.00%,  total acc: 67.68%   [EVAL] batch:  350 | acc: 68.75%,  total acc: 67.68%   [EVAL] batch:  351 | acc: 75.00%,  total acc: 67.70%   [EVAL] batch:  352 | acc: 62.50%,  total acc: 67.69%   [EVAL] batch:  353 | acc: 75.00%,  total acc: 67.71%   [EVAL] batch:  354 | acc: 81.25%,  total acc: 67.75%   [EVAL] batch:  355 | acc: 68.75%,  total acc: 67.75%   [EVAL] batch:  356 | acc: 50.00%,  total acc: 67.70%   [EVAL] batch:  357 | acc: 25.00%,  total acc: 67.58%   [EVAL] batch:  358 | acc: 31.25%,  total acc: 67.48%   [EVAL] batch:  359 | acc: 37.50%,  total acc: 67.40%   [EVAL] batch:  360 | acc: 37.50%,  total acc: 67.31%   [EVAL] batch:  361 | acc: 50.00%,  total acc: 67.27%   [EVAL] batch:  362 | acc: 68.75%,  total acc: 67.27%   [EVAL] batch:  363 | acc: 100.00%,  total acc: 67.36%   [EVAL] batch:  364 | acc: 100.00%,  total acc: 67.45%   [EVAL] batch:  365 | acc: 100.00%,  total acc: 67.54%   [EVAL] batch:  366 | acc: 100.00%,  total acc: 67.63%   [EVAL] batch:  367 | acc: 100.00%,  total acc: 67.71%   [EVAL] batch:  368 | acc: 87.50%,  total acc: 67.77%   [EVAL] batch:  369 | acc: 93.75%,  total acc: 67.84%   [EVAL] batch:  370 | acc: 81.25%,  total acc: 67.87%   [EVAL] batch:  371 | acc: 87.50%,  total acc: 67.93%   [EVAL] batch:  372 | acc: 93.75%,  total acc: 68.00%   [EVAL] batch:  373 | acc: 87.50%,  total acc: 68.05%   [EVAL] batch:  374 | acc: 81.25%,  total acc: 68.08%   
cur_acc:  ['0.9524', '0.8264', '0.7470', '0.7520', '0.6587', '0.6994']
his_acc:  ['0.9524', '0.8815', '0.8039', '0.7535', '0.7133', '0.6808']
CurrentTrain: epoch 15, batch     0 | loss: 16.0883612CurrentTrain: epoch 15, batch     1 | loss: 14.8375462CurrentTrain: epoch 15, batch     2 | loss: 16.8844019CurrentTrain: epoch  1, batch     3 | loss: 10.4629514CurrentTrain: epoch 15, batch     0 | loss: 13.2070144CurrentTrain: epoch 15, batch     1 | loss: 12.7704367CurrentTrain: epoch 15, batch     2 | loss: 14.8581783CurrentTrain: epoch  1, batch     3 | loss: 9.6718117CurrentTrain: epoch 15, batch     0 | loss: 11.1691390CurrentTrain: epoch 15, batch     1 | loss: 11.9737774CurrentTrain: epoch 15, batch     2 | loss: 11.8061043CurrentTrain: epoch  1, batch     3 | loss: 9.3053164CurrentTrain: epoch 15, batch     0 | loss: 13.7019889CurrentTrain: epoch 15, batch     1 | loss: 12.5213364CurrentTrain: epoch 15, batch     2 | loss: 13.8330006CurrentTrain: epoch  1, batch     3 | loss: 5.9371643CurrentTrain: epoch 15, batch     0 | loss: 14.5402194CurrentTrain: epoch 15, batch     1 | loss: 6.0979700CurrentTrain: epoch 15, batch     2 | loss: 10.2217731CurrentTrain: epoch  1, batch     3 | loss: 7.4824955CurrentTrain: epoch 15, batch     0 | loss: 12.1538195CurrentTrain: epoch 15, batch     1 | loss: 11.1303057CurrentTrain: epoch 15, batch     2 | loss: 17.6976290CurrentTrain: epoch  1, batch     3 | loss: 7.0177840CurrentTrain: epoch 15, batch     0 | loss: 16.9053028CurrentTrain: epoch 15, batch     1 | loss: 8.9474707CurrentTrain: epoch 15, batch     2 | loss: 10.3040053CurrentTrain: epoch  1, batch     3 | loss: 5.8169562CurrentTrain: epoch 15, batch     0 | loss: 9.9834319CurrentTrain: epoch 15, batch     1 | loss: 14.8261739CurrentTrain: epoch 15, batch     2 | loss: 6.8775283CurrentTrain: epoch  1, batch     3 | loss: 6.0496550CurrentTrain: epoch 15, batch     0 | loss: 10.3247844CurrentTrain: epoch 15, batch     1 | loss: 15.8936259CurrentTrain: epoch 15, batch     2 | loss: 9.9850219CurrentTrain: epoch  1, batch     3 | loss: 5.5971234CurrentTrain: epoch 15, batch     0 | loss: 7.6359654CurrentTrain: epoch 15, batch     1 | loss: 9.8312267CurrentTrain: epoch 15, batch     2 | loss: 7.1364204CurrentTrain: epoch  1, batch     3 | loss: 5.4020508
#### contains administrative territorial entity ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "contains administrative territorial entity" means direct subdivisions of an administrative territorial entity.
Here is an example:
Relation: contains administrative territorial entity
Context: the sutagao are the chibcha - speaking indios sutagaos indigenous people from the region of fusagasugá , bogotá savanna , cundinamarca , colombia .
Head Entity: colombia
Tail Entity: cundinamarca
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "contains administrative territorial entity":
Relation: contains administrative territorial entity  
Context: The state of California is known for its diverse geography and is divided into several counties, including Los Angeles County, which is the most populous county in the United States.  
Head Entity: California  
Tail Entity: Los Angeles County  

Relation: contains administrative territorial entity  
Context: The country of Japan is made up of several prefectures, with Tokyo being the most famous and densely populated among them.  
Head Entity: Japan  
Tail Entity: Tokyo  
#### said to be the same as ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "said to be the same as" means this item is said to be the same as that item, but it's uncertain or disputed.
Here is an example:
Relation: said to be the same as
Context: enzo is an italian given name derivative of heinz , a form of henry .
Head Entity: henry
Tail Entity: heinz
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "said to be the same as":
Relation: said to be the same as  
Context: The two species, Canis lupus and Canis familiaris, are often said to be the same as each other due to their close genetic relationship.  
Head Entity: Canis lupus  
Tail Entity: Canis familiaris  

Relation: said to be the same as  
Context: In some circles, the terms "artificial intelligence" and "machine learning" are said to be the same as each other, although experts argue there are key differences.  
Head Entity: artificial intelligence  
Tail Entity: machine learning  
#### winner ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "winner" means winner of a competition or similar event.
Here is an example:
Relation: winner
Context: in 2010 , ty burrell received a nomination for the television critics association award for individual achievement in comedy , winning in 2011 .
Head Entity: television critics association award for individual achievement in comedy
Tail Entity: ty burrell
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "winner":
Relation: winner  
Context: In 2022, the Boston Red Sox triumphed in the World Series, marking their fourth championship in the last two decades.  
Head Entity: World Series  
Tail Entity: Boston Red Sox  

Relation: winner  
Context: At the annual film festival, the documentary "Voices of Change" was awarded the Best Picture, highlighting its impactful storytelling and direction.  
Head Entity: Best Picture  
Tail Entity: Voices of Change  
#### military rank ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "military rank" means military rank achieved by a person, or military rank associated with a position.
Here is an example:
Relation: military rank
Context: among notable relatives were his maternal grandfather charles ready , a u.s. representative from tennessee and his cousin confederate states army brigadier general john hunt morgan .
Head Entity: john hunt morgan
Tail Entity: brigadier general
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "military rank":
Relation: military rank  
Context: During the ceremony, General Sarah Thompson was promoted to the rank of major general, making her one of the highest-ranking women in the army.  
Head Entity: Sarah Thompson  
Tail Entity: major general  

Relation: military rank  
Context: The historical records indicate that Colonel James Anderson served with distinction during the war, eventually rising to the rank of colonel.  
Head Entity: James Anderson  
Tail Entity: colonel  
#### publisher ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "publisher" means organization or person responsible for publishing books, periodicals, printed music, podcasts, games or software.
Here is an example:
Relation: publisher
Context: there are seven different nations that are allied or in conflict : prior to its north american release , " vanguard bandits " was titled " detonator gauntlet " by working designs .
Head Entity: vanguard bandits
Tail Entity: working designs
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "publisher":
Relation: publisher  
Context: The latest novel by the acclaimed author was released by Penguin Random House, a well-known publishing house in the industry.  
Head Entity: latest novel  
Tail Entity: Penguin Random House  

Relation: publisher  
Context: After years of hard work, the game developer finally secured a deal with Electronic Arts to publish their new sports game.  
Head Entity: new sports game  
Tail Entity: Electronic Arts  
#### followed by ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "followed by" means immediately following item in a series of which the subject is a part.
Here is an example:
Relation: followed by
Context: " muzai moratorium " and " shōso strip " were released on vinyl in 2008 , as a part of her 10-year anniversary celebrations .
Head Entity: muzai moratorium
Tail Entity: shōso strip
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "followed by":
Relation: followed by  
Context: The first chapter of the book introduces the main character, while the second chapter follows with a detailed backstory.  
Head Entity: first chapter  
Tail Entity: second chapter  

Relation: followed by  
Context: In the concert lineup, the opening act performed a lively set, followed by the headliner who captivated the audience with their hits.  
Head Entity: opening act  
Tail Entity: headliner  
#### work location ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "work location" means location where persons or organisations were actively participating in employment, business or other work.
Here is an example:
Relation: work location
Context: he was also associated with robert wilkinson in producing " londina illustrata " , an illustrated account of ancient buildings in london and westminster in two volumes ( 1819–25 ) .
Head Entity: robert wilkinson
Tail Entity: london
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "work location":
Relation: work location  
Context: The headquarters of the company is situated in the bustling city of San Francisco, where it has been operating since its inception in 2010.  
Head Entity: the company  
Tail Entity: San Francisco  

Relation: work location  
Context: During her tenure at the university, she conducted groundbreaking research in neuroscience, primarily based in the labs located in Cambridge.  
Head Entity: she  
Tail Entity: Cambridge  
#### occupation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "occupation" means occupation of a person.
Here is an example:
Relation: occupation
Context: andrei droznin ( russian : андрей дрознин ) is a russian theatre director and movement coach .
Head Entity: andrei droznin
Tail Entity: theatre director
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "occupation":
Relation: occupation  
Context: dr. emily thompson is a renowned biochemist known for her groundbreaking research in cancer treatment.  
Head Entity: dr. emily thompson  
Tail Entity: biochemist  

Relation: occupation  
Context: john smith, a talented software engineer, has been working at tech innovations for over five years.  
Head Entity: john smith  
Tail Entity: software engineer  
#### heritage designation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "heritage designation" means heritage designation of a cultural or natural site.
Here is an example:
Relation: heritage designation
Context: altamaha town , bluffton historic district , church of the cross , and rose hill plantation house are listed on the national register of historic places .
Head Entity: altamaha town
Tail Entity: national register of historic places
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "heritage designation":
Relation: heritage designation  
Context: The Great Wall of China is recognized as a UNESCO World Heritage Site due to its historical significance and architectural grandeur.  
Head Entity: Great Wall of China  
Tail Entity: UNESCO World Heritage Site  

Relation: heritage designation  
Context: The ancient city of Petra in Jordan is designated as a World Heritage Site, attracting tourists from around the globe.  
Head Entity: Petra  
Tail Entity: World Heritage Site  
#### voice type ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "voice type" means person's voice type.
Here is an example:
Relation: voice type
Context: beverly bower ( september 30 , 1925 – march 24 , 2002 ) was an american operatic soprano who had an active international opera career from the mid-1950s through the early 1970s .
Head Entity: beverly bower
Tail Entity: soprano
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "voice type":
Relation: voice type  
Context: luciano pavarotti ( october 12 , 1935 – september 6 , 2007 ) was an italian tenor who gained worldwide fame for his televised concerts and media appearances.  
Head Entity: luciano pavarotti  
Tail Entity: tenor  

Relation: voice type  
Context: mariah carey ( born march 27 , 1969 ) is an american singer, songwriter, and actress known for her five-octave vocal range and her use of the whistle register.  
Head Entity: mariah carey  
Tail Entity: singer  
MemoryTrain:  epoch 15, batch     0 | loss: 3.4049586MemoryTrain:  epoch 15, batch     1 | loss: 3.6043260MemoryTrain:  epoch 15, batch     2 | loss: 2.7720216MemoryTrain:  epoch 15, batch     3 | loss: 2.5002183MemoryTrain:  epoch 15, batch     4 | loss: 2.3751071MemoryTrain:  epoch 15, batch     5 | loss: 5.7035446MemoryTrain:  epoch 15, batch     6 | loss: 4.7007399MemoryTrain:  epoch 15, batch     7 | loss: 5.3754383MemoryTrain:  epoch 15, batch     8 | loss: 3.5330991MemoryTrain:  epoch 15, batch     9 | loss: 2.2364990MemoryTrain:  epoch 15, batch    10 | loss: 8.7891781MemoryTrain:  epoch 15, batch    11 | loss: 3.2777571MemoryTrain:  epoch 15, batch    12 | loss: 4.0932103MemoryTrain:  epoch  1, batch    13 | loss: 5.3429918MemoryTrain:  epoch 15, batch     0 | loss: 2.7653319MemoryTrain:  epoch 15, batch     1 | loss: 4.9958417MemoryTrain:  epoch 15, batch     2 | loss: 3.9071165MemoryTrain:  epoch 15, batch     3 | loss: 4.4344509MemoryTrain:  epoch 15, batch     4 | loss: 2.8596017MemoryTrain:  epoch 15, batch     5 | loss: 2.8144596MemoryTrain:  epoch 15, batch     6 | loss: 2.5262097MemoryTrain:  epoch 15, batch     7 | loss: 3.2276940MemoryTrain:  epoch 15, batch     8 | loss: 2.0510534MemoryTrain:  epoch 15, batch     9 | loss: 2.4566182MemoryTrain:  epoch 15, batch    10 | loss: 3.3059415MemoryTrain:  epoch 15, batch    11 | loss: 3.6081689MemoryTrain:  epoch 15, batch    12 | loss: 3.5339366MemoryTrain:  epoch  1, batch    13 | loss: 6.7933530MemoryTrain:  epoch 15, batch     0 | loss: 2.0174271MemoryTrain:  epoch 15, batch     1 | loss: 3.1756069MemoryTrain:  epoch 15, batch     2 | loss: 5.5005471MemoryTrain:  epoch 15, batch     3 | loss: 2.2648040MemoryTrain:  epoch 15, batch     4 | loss: 7.3974107MemoryTrain:  epoch 15, batch     5 | loss: 2.2380795MemoryTrain:  epoch 15, batch     6 | loss: 1.8693204MemoryTrain:  epoch 15, batch     7 | loss: 4.1497584MemoryTrain:  epoch 15, batch     8 | loss: 3.7442222MemoryTrain:  epoch 15, batch     9 | loss: 2.7034313MemoryTrain:  epoch 15, batch    10 | loss: 4.6220082MemoryTrain:  epoch 15, batch    11 | loss: 2.0718518MemoryTrain:  epoch 15, batch    12 | loss: 2.2384466MemoryTrain:  epoch  1, batch    13 | loss: 6.7641395MemoryTrain:  epoch 15, batch     0 | loss: 3.1294711MemoryTrain:  epoch 15, batch     1 | loss: 3.3158108MemoryTrain:  epoch 15, batch     2 | loss: 2.5820974MemoryTrain:  epoch 15, batch     3 | loss: 2.7288793MemoryTrain:  epoch 15, batch     4 | loss: 3.9201850MemoryTrain:  epoch 15, batch     5 | loss: 4.6316810MemoryTrain:  epoch 15, batch     6 | loss: 1.8105234MemoryTrain:  epoch 15, batch     7 | loss: 5.0886778MemoryTrain:  epoch 15, batch     8 | loss: 3.3955650MemoryTrain:  epoch 15, batch     9 | loss: 2.4658727MemoryTrain:  epoch 15, batch    10 | loss: 2.9849940MemoryTrain:  epoch 15, batch    11 | loss: 4.1549330MemoryTrain:  epoch 15, batch    12 | loss: 1.5866442MemoryTrain:  epoch  1, batch    13 | loss: 5.3084685MemoryTrain:  epoch 15, batch     0 | loss: 2.0232939MemoryTrain:  epoch 15, batch     1 | loss: 2.2328749MemoryTrain:  epoch 15, batch     2 | loss: 2.1220969MemoryTrain:  epoch 15, batch     3 | loss: 1.8065851MemoryTrain:  epoch 15, batch     4 | loss: 2.7006142MemoryTrain:  epoch 15, batch     5 | loss: 1.8787760MemoryTrain:  epoch 15, batch     6 | loss: 1.6268348MemoryTrain:  epoch 15, batch     7 | loss: 2.1245789MemoryTrain:  epoch 15, batch     8 | loss: 1.7430643MemoryTrain:  epoch 15, batch     9 | loss: 2.1930551MemoryTrain:  epoch 15, batch    10 | loss: 2.9277491MemoryTrain:  epoch 15, batch    11 | loss: 2.7564254MemoryTrain:  epoch 15, batch    12 | loss: 1.4780223MemoryTrain:  epoch  1, batch    13 | loss: 8.8079132MemoryTrain:  epoch 15, batch     0 | loss: 2.1589611MemoryTrain:  epoch 15, batch     1 | loss: 1.7765368MemoryTrain:  epoch 15, batch     2 | loss: 3.0213255MemoryTrain:  epoch 15, batch     3 | loss: 2.1825817MemoryTrain:  epoch 15, batch     4 | loss: 2.1084517MemoryTrain:  epoch 15, batch     5 | loss: 6.7641777MemoryTrain:  epoch 15, batch     6 | loss: 1.8666409MemoryTrain:  epoch 15, batch     7 | loss: 2.6823409MemoryTrain:  epoch 15, batch     8 | loss: 1.5289262MemoryTrain:  epoch 15, batch     9 | loss: 2.7096088MemoryTrain:  epoch 15, batch    10 | loss: 1.8703029MemoryTrain:  epoch 15, batch    11 | loss: 2.8929404MemoryTrain:  epoch 15, batch    12 | loss: 1.8725410MemoryTrain:  epoch  1, batch    13 | loss: 5.4825855MemoryTrain:  epoch 15, batch     0 | loss: 2.0522708MemoryTrain:  epoch 15, batch     1 | loss: 4.9095531MemoryTrain:  epoch 15, batch     2 | loss: 4.4040061MemoryTrain:  epoch 15, batch     3 | loss: 1.7827855MemoryTrain:  epoch 15, batch     4 | loss: 2.3785821MemoryTrain:  epoch 15, batch     5 | loss: 2.0601631MemoryTrain:  epoch 15, batch     6 | loss: 1.9236089MemoryTrain:  epoch 15, batch     7 | loss: 2.7486368MemoryTrain:  epoch 15, batch     8 | loss: 1.6929073MemoryTrain:  epoch 15, batch     9 | loss: 3.8594838MemoryTrain:  epoch 15, batch    10 | loss: 4.0087983MemoryTrain:  epoch 15, batch    11 | loss: 2.5447882MemoryTrain:  epoch 15, batch    12 | loss: 1.7123708MemoryTrain:  epoch  1, batch    13 | loss: 7.2739125MemoryTrain:  epoch 15, batch     0 | loss: 4.6312591MemoryTrain:  epoch 15, batch     1 | loss: 1.4538232MemoryTrain:  epoch 15, batch     2 | loss: 1.7870110MemoryTrain:  epoch 15, batch     3 | loss: 1.4687557MemoryTrain:  epoch 15, batch     4 | loss: 4.0435274MemoryTrain:  epoch 15, batch     5 | loss: 4.5725941MemoryTrain:  epoch 15, batch     6 | loss: 2.4498510MemoryTrain:  epoch 15, batch     7 | loss: 2.6557642MemoryTrain:  epoch 15, batch     8 | loss: 2.0781139MemoryTrain:  epoch 15, batch     9 | loss: 1.7010414MemoryTrain:  epoch 15, batch    10 | loss: 1.6922283MemoryTrain:  epoch 15, batch    11 | loss: 2.2747366MemoryTrain:  epoch 15, batch    12 | loss: 1.5448262MemoryTrain:  epoch  1, batch    13 | loss: 7.3893276MemoryTrain:  epoch 15, batch     0 | loss: 1.9428584MemoryTrain:  epoch 15, batch     1 | loss: 4.1373652MemoryTrain:  epoch 15, batch     2 | loss: 2.7380485MemoryTrain:  epoch 15, batch     3 | loss: 1.8168739MemoryTrain:  epoch 15, batch     4 | loss: 5.1177825MemoryTrain:  epoch 15, batch     5 | loss: 5.1508529MemoryTrain:  epoch 15, batch     6 | loss: 4.7097508MemoryTrain:  epoch 15, batch     7 | loss: 2.1130460MemoryTrain:  epoch 15, batch     8 | loss: 2.0884282MemoryTrain:  epoch 15, batch     9 | loss: 1.3841082MemoryTrain:  epoch 15, batch    10 | loss: 1.5387815MemoryTrain:  epoch 15, batch    11 | loss: 2.1785691MemoryTrain:  epoch 15, batch    12 | loss: 1.4717176MemoryTrain:  epoch  1, batch    13 | loss: 5.2042091MemoryTrain:  epoch 15, batch     0 | loss: 1.7127638MemoryTrain:  epoch 15, batch     1 | loss: 1.8748535MemoryTrain:  epoch 15, batch     2 | loss: 1.9956641MemoryTrain:  epoch 15, batch     3 | loss: 2.5518482MemoryTrain:  epoch 15, batch     4 | loss: 2.0583130MemoryTrain:  epoch 15, batch     5 | loss: 2.5124864MemoryTrain:  epoch 15, batch     6 | loss: 1.9337435MemoryTrain:  epoch 15, batch     7 | loss: 3.2292386MemoryTrain:  epoch 15, batch     8 | loss: 2.0500075MemoryTrain:  epoch 15, batch     9 | loss: 1.7679106MemoryTrain:  epoch 15, batch    10 | loss: 3.0738140MemoryTrain:  epoch 15, batch    11 | loss: 1.3901968MemoryTrain:  epoch 15, batch    12 | loss: 1.5634648MemoryTrain:  epoch  1, batch    13 | loss: 6.0561036
[EVAL] batch:    0 | acc: 56.25%,  total acc: 56.25%   [EVAL] batch:    1 | acc: 56.25%,  total acc: 56.25%   [EVAL] batch:    2 | acc: 68.75%,  total acc: 60.42%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 64.06%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 65.00%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 69.79%   [EVAL] batch:    6 | acc: 43.75%,  total acc: 66.07%   [EVAL] batch:    7 | acc: 62.50%,  total acc: 65.62%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 65.97%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 67.50%   [EVAL] batch:   10 | acc: 56.25%,  total acc: 66.48%   [EVAL] batch:   11 | acc: 56.25%,  total acc: 65.62%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 66.83%   [EVAL] batch:   13 | acc: 81.25%,  total acc: 67.86%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 68.33%   [EVAL] batch:   15 | acc: 62.50%,  total acc: 67.97%   [EVAL] batch:   16 | acc: 87.50%,  total acc: 69.12%   [EVAL] batch:   17 | acc: 100.00%,  total acc: 70.83%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 72.37%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 73.12%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 74.40%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 75.57%   [EVAL] batch:   22 | acc: 87.50%,  total acc: 76.09%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 76.56%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 77.50%   [EVAL] batch:   25 | acc: 62.50%,  total acc: 76.92%   [EVAL] batch:   26 | acc: 62.50%,  total acc: 76.39%   [EVAL] batch:   27 | acc: 87.50%,  total acc: 76.79%   [EVAL] batch:   28 | acc: 75.00%,  total acc: 76.72%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 76.88%   [EVAL] batch:   30 | acc: 81.25%,  total acc: 77.02%   [EVAL] batch:   31 | acc: 62.50%,  total acc: 76.56%   [EVAL] batch:   32 | acc: 50.00%,  total acc: 75.76%   [EVAL] batch:   33 | acc: 18.75%,  total acc: 74.08%   [EVAL] batch:   34 | acc: 31.25%,  total acc: 72.86%   [EVAL] batch:   35 | acc: 18.75%,  total acc: 71.35%   [EVAL] batch:   36 | acc: 18.75%,  total acc: 69.93%   [EVAL] batch:   37 | acc: 50.00%,  total acc: 69.41%   [EVAL] batch:   38 | acc: 62.50%,  total acc: 69.23%   [EVAL] batch:   39 | acc: 75.00%,  total acc: 69.38%   [EVAL] batch:   40 | acc: 50.00%,  total acc: 68.90%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 69.35%   [EVAL] batch:   42 | acc: 62.50%,  total acc: 69.19%   [EVAL] batch:   43 | acc: 62.50%,  total acc: 69.03%   [EVAL] batch:   44 | acc: 75.00%,  total acc: 69.17%   [EVAL] batch:   45 | acc: 68.75%,  total acc: 69.16%   [EVAL] batch:   46 | acc: 68.75%,  total acc: 69.15%   [EVAL] batch:   47 | acc: 56.25%,  total acc: 68.88%   [EVAL] batch:   48 | acc: 62.50%,  total acc: 68.75%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 69.12%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 69.73%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 70.31%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 70.87%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 71.41%   [EVAL] batch:   54 | acc: 100.00%,  total acc: 71.93%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 72.43%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 72.81%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 73.06%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 73.31%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 73.65%   [EVAL] batch:   60 | acc: 87.50%,  total acc: 73.87%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 74.29%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 73.91%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 75.00%,  total acc: 71.88%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 72.92%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 73.44%   [EVAL] batch:    4 | acc: 62.50%,  total acc: 71.25%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 72.92%   [EVAL] batch:    6 | acc: 50.00%,  total acc: 69.64%   [EVAL] batch:    7 | acc: 43.75%,  total acc: 66.41%   [EVAL] batch:    8 | acc: 56.25%,  total acc: 65.28%   [EVAL] batch:    9 | acc: 56.25%,  total acc: 64.38%   [EVAL] batch:   10 | acc: 43.75%,  total acc: 62.50%   [EVAL] batch:   11 | acc: 43.75%,  total acc: 60.94%   [EVAL] batch:   12 | acc: 62.50%,  total acc: 61.06%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 63.39%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 65.42%   [EVAL] batch:   15 | acc: 93.75%,  total acc: 67.19%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 68.75%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 70.14%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 71.38%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 71.56%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 72.32%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 73.01%   [EVAL] batch:   22 | acc: 68.75%,  total acc: 72.83%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 73.44%   [EVAL] batch:   24 | acc: 68.75%,  total acc: 73.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 74.28%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 75.23%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 76.12%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 76.72%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 77.50%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 78.23%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 78.91%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 79.55%   [EVAL] batch:   33 | acc: 75.00%,  total acc: 79.41%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 79.29%   [EVAL] batch:   35 | acc: 81.25%,  total acc: 79.34%   [EVAL] batch:   36 | acc: 93.75%,  total acc: 79.73%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 80.10%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 80.61%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 81.09%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 81.55%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 81.70%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 81.98%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 81.96%   [EVAL] batch:   44 | acc: 68.75%,  total acc: 81.67%   [EVAL] batch:   45 | acc: 56.25%,  total acc: 81.11%   [EVAL] batch:   46 | acc: 37.50%,  total acc: 80.19%   [EVAL] batch:   47 | acc: 50.00%,  total acc: 79.56%   [EVAL] batch:   48 | acc: 62.50%,  total acc: 79.21%   [EVAL] batch:   49 | acc: 62.50%,  total acc: 78.88%   [EVAL] batch:   50 | acc: 75.00%,  total acc: 78.80%   [EVAL] batch:   51 | acc: 75.00%,  total acc: 78.73%   [EVAL] batch:   52 | acc: 87.50%,  total acc: 78.89%   [EVAL] batch:   53 | acc: 68.75%,  total acc: 78.70%   [EVAL] batch:   54 | acc: 56.25%,  total acc: 78.30%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 78.46%   [EVAL] batch:   56 | acc: 68.75%,  total acc: 78.29%   [EVAL] batch:   57 | acc: 37.50%,  total acc: 77.59%   [EVAL] batch:   58 | acc: 50.00%,  total acc: 77.12%   [EVAL] batch:   59 | acc: 62.50%,  total acc: 76.88%   [EVAL] batch:   60 | acc: 81.25%,  total acc: 76.95%   [EVAL] batch:   61 | acc: 62.50%,  total acc: 76.71%   [EVAL] batch:   62 | acc: 68.75%,  total acc: 76.59%   [EVAL] batch:   63 | acc: 62.50%,  total acc: 76.37%   [EVAL] batch:   64 | acc: 75.00%,  total acc: 76.35%   [EVAL] batch:   65 | acc: 87.50%,  total acc: 76.52%   [EVAL] batch:   66 | acc: 68.75%,  total acc: 76.40%   [EVAL] batch:   67 | acc: 93.75%,  total acc: 76.65%   [EVAL] batch:   68 | acc: 87.50%,  total acc: 76.81%   [EVAL] batch:   69 | acc: 81.25%,  total acc: 76.88%   [EVAL] batch:   70 | acc: 68.75%,  total acc: 76.76%   [EVAL] batch:   71 | acc: 56.25%,  total acc: 76.48%   [EVAL] batch:   72 | acc: 75.00%,  total acc: 76.46%   [EVAL] batch:   73 | acc: 68.75%,  total acc: 76.35%   [EVAL] batch:   74 | acc: 81.25%,  total acc: 76.42%   [EVAL] batch:   75 | acc: 81.25%,  total acc: 76.48%   [EVAL] batch:   76 | acc: 75.00%,  total acc: 76.46%   [EVAL] batch:   77 | acc: 68.75%,  total acc: 76.36%   [EVAL] batch:   78 | acc: 93.75%,  total acc: 76.58%   [EVAL] batch:   79 | acc: 87.50%,  total acc: 76.72%   [EVAL] batch:   80 | acc: 93.75%,  total acc: 76.93%   [EVAL] batch:   81 | acc: 31.25%,  total acc: 76.37%   [EVAL] batch:   82 | acc: 12.50%,  total acc: 75.60%   [EVAL] batch:   83 | acc: 0.00%,  total acc: 74.70%   [EVAL] batch:   84 | acc: 0.00%,  total acc: 73.82%   [EVAL] batch:   85 | acc: 12.50%,  total acc: 73.11%   [EVAL] batch:   86 | acc: 12.50%,  total acc: 72.41%   [EVAL] batch:   87 | acc: 43.75%,  total acc: 72.09%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 72.40%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 72.71%   [EVAL] batch:   90 | acc: 93.75%,  total acc: 72.94%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 73.23%   [EVAL] batch:   92 | acc: 93.75%,  total acc: 73.45%   [EVAL] batch:   93 | acc: 93.75%,  total acc: 73.67%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 73.95%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 74.22%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 74.48%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 74.74%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 75.25%   [EVAL] batch:  100 | acc: 93.75%,  total acc: 75.43%   [EVAL] batch:  101 | acc: 93.75%,  total acc: 75.61%   [EVAL] batch:  102 | acc: 75.00%,  total acc: 75.61%   [EVAL] batch:  103 | acc: 87.50%,  total acc: 75.72%   [EVAL] batch:  104 | acc: 100.00%,  total acc: 75.95%   [EVAL] batch:  105 | acc: 93.75%,  total acc: 76.12%   [EVAL] batch:  106 | acc: 43.75%,  total acc: 75.82%   [EVAL] batch:  107 | acc: 25.00%,  total acc: 75.35%   [EVAL] batch:  108 | acc: 25.00%,  total acc: 74.89%   [EVAL] batch:  109 | acc: 31.25%,  total acc: 74.49%   [EVAL] batch:  110 | acc: 56.25%,  total acc: 74.32%   [EVAL] batch:  111 | acc: 43.75%,  total acc: 74.05%   [EVAL] batch:  112 | acc: 62.50%,  total acc: 73.95%   [EVAL] batch:  113 | acc: 87.50%,  total acc: 74.07%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 74.29%   [EVAL] batch:  115 | acc: 81.25%,  total acc: 74.35%   [EVAL] batch:  116 | acc: 87.50%,  total acc: 74.47%   [EVAL] batch:  117 | acc: 93.75%,  total acc: 74.63%   [EVAL] batch:  118 | acc: 68.75%,  total acc: 74.58%   [EVAL] batch:  119 | acc: 6.25%,  total acc: 74.01%   [EVAL] batch:  120 | acc: 31.25%,  total acc: 73.66%   [EVAL] batch:  121 | acc: 0.00%,  total acc: 73.05%   [EVAL] batch:  122 | acc: 6.25%,  total acc: 72.51%   [EVAL] batch:  123 | acc: 6.25%,  total acc: 71.98%   [EVAL] batch:  124 | acc: 0.00%,  total acc: 71.40%   [EVAL] batch:  125 | acc: 43.75%,  total acc: 71.18%   [EVAL] batch:  126 | acc: 37.50%,  total acc: 70.92%   [EVAL] batch:  127 | acc: 50.00%,  total acc: 70.75%   [EVAL] batch:  128 | acc: 75.00%,  total acc: 70.78%   [EVAL] batch:  129 | acc: 75.00%,  total acc: 70.82%   [EVAL] batch:  130 | acc: 50.00%,  total acc: 70.66%   [EVAL] batch:  131 | acc: 62.50%,  total acc: 70.60%   [EVAL] batch:  132 | acc: 62.50%,  total acc: 70.54%   [EVAL] batch:  133 | acc: 43.75%,  total acc: 70.34%   [EVAL] batch:  134 | acc: 62.50%,  total acc: 70.28%   [EVAL] batch:  135 | acc: 75.00%,  total acc: 70.31%   [EVAL] batch:  136 | acc: 68.75%,  total acc: 70.30%   [EVAL] batch:  137 | acc: 43.75%,  total acc: 70.11%   [EVAL] batch:  138 | acc: 18.75%,  total acc: 69.74%   [EVAL] batch:  139 | acc: 25.00%,  total acc: 69.42%   [EVAL] batch:  140 | acc: 12.50%,  total acc: 69.02%   [EVAL] batch:  141 | acc: 43.75%,  total acc: 68.84%   [EVAL] batch:  142 | acc: 37.50%,  total acc: 68.62%   [EVAL] batch:  143 | acc: 37.50%,  total acc: 68.40%   [EVAL] batch:  144 | acc: 93.75%,  total acc: 68.58%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 68.79%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 68.96%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 69.17%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 69.38%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 69.54%   [EVAL] batch:  150 | acc: 31.25%,  total acc: 69.29%   [EVAL] batch:  151 | acc: 12.50%,  total acc: 68.91%   [EVAL] batch:  152 | acc: 25.00%,  total acc: 68.63%   [EVAL] batch:  153 | acc: 37.50%,  total acc: 68.43%   [EVAL] batch:  154 | acc: 31.25%,  total acc: 68.19%   [EVAL] batch:  155 | acc: 31.25%,  total acc: 67.95%   [EVAL] batch:  156 | acc: 68.75%,  total acc: 67.95%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 68.12%   [EVAL] batch:  158 | acc: 81.25%,  total acc: 68.20%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 68.36%   [EVAL] batch:  160 | acc: 87.50%,  total acc: 68.48%   [EVAL] batch:  161 | acc: 81.25%,  total acc: 68.56%   [EVAL] batch:  162 | acc: 93.75%,  total acc: 68.71%   [EVAL] batch:  163 | acc: 50.00%,  total acc: 68.60%   [EVAL] batch:  164 | acc: 81.25%,  total acc: 68.67%   [EVAL] batch:  165 | acc: 56.25%,  total acc: 68.60%   [EVAL] batch:  166 | acc: 50.00%,  total acc: 68.49%   [EVAL] batch:  167 | acc: 62.50%,  total acc: 68.45%   [EVAL] batch:  168 | acc: 75.00%,  total acc: 68.49%   [EVAL] batch:  169 | acc: 62.50%,  total acc: 68.46%   [EVAL] batch:  170 | acc: 68.75%,  total acc: 68.46%   [EVAL] batch:  171 | acc: 62.50%,  total acc: 68.42%   [EVAL] batch:  172 | acc: 62.50%,  total acc: 68.39%   [EVAL] batch:  173 | acc: 75.00%,  total acc: 68.43%   [EVAL] batch:  174 | acc: 93.75%,  total acc: 68.57%   [EVAL] batch:  175 | acc: 37.50%,  total acc: 68.39%   [EVAL] batch:  176 | acc: 31.25%,  total acc: 68.19%   [EVAL] batch:  177 | acc: 56.25%,  total acc: 68.12%   [EVAL] batch:  178 | acc: 37.50%,  total acc: 67.95%   [EVAL] batch:  179 | acc: 31.25%,  total acc: 67.74%   [EVAL] batch:  180 | acc: 56.25%,  total acc: 67.68%   [EVAL] batch:  181 | acc: 75.00%,  total acc: 67.72%   [EVAL] batch:  182 | acc: 81.25%,  total acc: 67.79%   [EVAL] batch:  183 | acc: 93.75%,  total acc: 67.93%   [EVAL] batch:  184 | acc: 75.00%,  total acc: 67.97%   [EVAL] batch:  185 | acc: 93.75%,  total acc: 68.11%   [EVAL] batch:  186 | acc: 75.00%,  total acc: 68.15%   [EVAL] batch:  187 | acc: 43.75%,  total acc: 68.02%   [EVAL] batch:  188 | acc: 12.50%,  total acc: 67.72%   [EVAL] batch:  189 | acc: 18.75%,  total acc: 67.47%   [EVAL] batch:  190 | acc: 0.00%,  total acc: 67.11%   [EVAL] batch:  191 | acc: 0.00%,  total acc: 66.76%   [EVAL] batch:  192 | acc: 12.50%,  total acc: 66.48%   [EVAL] batch:  193 | acc: 18.75%,  total acc: 66.24%   [EVAL] batch:  194 | acc: 81.25%,  total acc: 66.31%   [EVAL] batch:  195 | acc: 100.00%,  total acc: 66.49%   [EVAL] batch:  196 | acc: 81.25%,  total acc: 66.56%   [EVAL] batch:  197 | acc: 81.25%,  total acc: 66.64%   [EVAL] batch:  198 | acc: 81.25%,  total acc: 66.71%   [EVAL] batch:  199 | acc: 75.00%,  total acc: 66.75%   [EVAL] batch:  200 | acc: 75.00%,  total acc: 66.79%   [EVAL] batch:  201 | acc: 37.50%,  total acc: 66.65%   [EVAL] batch:  202 | acc: 50.00%,  total acc: 66.56%   [EVAL] batch:  203 | acc: 62.50%,  total acc: 66.54%   [EVAL] batch:  204 | acc: 75.00%,  total acc: 66.59%   [EVAL] batch:  205 | acc: 37.50%,  total acc: 66.44%   [EVAL] batch:  206 | acc: 93.75%,  total acc: 66.58%   [EVAL] batch:  207 | acc: 100.00%,  total acc: 66.74%   [EVAL] batch:  208 | acc: 100.00%,  total acc: 66.90%   [EVAL] batch:  209 | acc: 93.75%,  total acc: 67.02%   [EVAL] batch:  210 | acc: 93.75%,  total acc: 67.15%   [EVAL] batch:  211 | acc: 100.00%,  total acc: 67.31%   [EVAL] batch:  212 | acc: 93.75%,  total acc: 67.43%   [EVAL] batch:  213 | acc: 81.25%,  total acc: 67.49%   [EVAL] batch:  214 | acc: 75.00%,  total acc: 67.53%   [EVAL] batch:  215 | acc: 93.75%,  total acc: 67.65%   [EVAL] batch:  216 | acc: 75.00%,  total acc: 67.68%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 67.83%   [EVAL] batch:  218 | acc: 87.50%,  total acc: 67.92%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 68.07%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 68.21%   [EVAL] batch:  221 | acc: 87.50%,  total acc: 68.30%   [EVAL] batch:  222 | acc: 87.50%,  total acc: 68.39%   [EVAL] batch:  223 | acc: 93.75%,  total acc: 68.50%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 68.64%   [EVAL] batch:  225 | acc: 75.00%,  total acc: 68.67%   [EVAL] batch:  226 | acc: 62.50%,  total acc: 68.64%   [EVAL] batch:  227 | acc: 56.25%,  total acc: 68.59%   [EVAL] batch:  228 | acc: 68.75%,  total acc: 68.59%   [EVAL] batch:  229 | acc: 50.00%,  total acc: 68.51%   [EVAL] batch:  230 | acc: 68.75%,  total acc: 68.51%   [EVAL] batch:  231 | acc: 87.50%,  total acc: 68.59%   [EVAL] batch:  232 | acc: 87.50%,  total acc: 68.67%   [EVAL] batch:  233 | acc: 81.25%,  total acc: 68.72%   [EVAL] batch:  234 | acc: 93.75%,  total acc: 68.83%   [EVAL] batch:  235 | acc: 68.75%,  total acc: 68.83%   [EVAL] batch:  236 | acc: 75.00%,  total acc: 68.86%   [EVAL] batch:  237 | acc: 62.50%,  total acc: 68.83%   [EVAL] batch:  238 | acc: 62.50%,  total acc: 68.80%   [EVAL] batch:  239 | acc: 56.25%,  total acc: 68.75%   [EVAL] batch:  240 | acc: 56.25%,  total acc: 68.70%   [EVAL] batch:  241 | acc: 43.75%,  total acc: 68.60%   [EVAL] batch:  242 | acc: 43.75%,  total acc: 68.49%   [EVAL] batch:  243 | acc: 25.00%,  total acc: 68.31%   [EVAL] batch:  244 | acc: 68.75%,  total acc: 68.32%   [EVAL] batch:  245 | acc: 75.00%,  total acc: 68.34%   [EVAL] batch:  246 | acc: 62.50%,  total acc: 68.32%   [EVAL] batch:  247 | acc: 81.25%,  total acc: 68.37%   [EVAL] batch:  248 | acc: 87.50%,  total acc: 68.45%   [EVAL] batch:  249 | acc: 75.00%,  total acc: 68.47%   [EVAL] batch:  250 | acc: 0.00%,  total acc: 68.20%   [EVAL] batch:  251 | acc: 31.25%,  total acc: 68.06%   [EVAL] batch:  252 | acc: 6.25%,  total acc: 67.81%   [EVAL] batch:  253 | acc: 6.25%,  total acc: 67.57%   [EVAL] batch:  254 | acc: 25.00%,  total acc: 67.40%   [EVAL] batch:  255 | acc: 31.25%,  total acc: 67.26%   [EVAL] batch:  256 | acc: 62.50%,  total acc: 67.24%   [EVAL] batch:  257 | acc: 68.75%,  total acc: 67.25%   [EVAL] batch:  258 | acc: 81.25%,  total acc: 67.30%   [EVAL] batch:  259 | acc: 75.00%,  total acc: 67.33%   [EVAL] batch:  260 | acc: 75.00%,  total acc: 67.36%   [EVAL] batch:  261 | acc: 87.50%,  total acc: 67.44%   [EVAL] batch:  262 | acc: 87.50%,  total acc: 67.51%   [EVAL] batch:  263 | acc: 62.50%,  total acc: 67.50%   [EVAL] batch:  264 | acc: 75.00%,  total acc: 67.52%   [EVAL] batch:  265 | acc: 87.50%,  total acc: 67.60%   [EVAL] batch:  266 | acc: 87.50%,  total acc: 67.67%   [EVAL] batch:  267 | acc: 93.75%,  total acc: 67.77%   [EVAL] batch:  268 | acc: 56.25%,  total acc: 67.73%   [EVAL] batch:  269 | acc: 25.00%,  total acc: 67.57%   [EVAL] batch:  270 | acc: 12.50%,  total acc: 67.37%   [EVAL] batch:  271 | acc: 12.50%,  total acc: 67.16%   [EVAL] batch:  272 | acc: 18.75%,  total acc: 66.99%   [EVAL] batch:  273 | acc: 25.00%,  total acc: 66.83%   [EVAL] batch:  274 | acc: 18.75%,  total acc: 66.66%   [EVAL] batch:  275 | acc: 6.25%,  total acc: 66.44%   [EVAL] batch:  276 | acc: 12.50%,  total acc: 66.25%   [EVAL] batch:  277 | acc: 18.75%,  total acc: 66.07%   [EVAL] batch:  278 | acc: 6.25%,  total acc: 65.86%   [EVAL] batch:  279 | acc: 18.75%,  total acc: 65.69%   [EVAL] batch:  280 | acc: 0.00%,  total acc: 65.46%   [EVAL] batch:  281 | acc: 68.75%,  total acc: 65.47%   [EVAL] batch:  282 | acc: 87.50%,  total acc: 65.55%   [EVAL] batch:  283 | acc: 87.50%,  total acc: 65.62%   [EVAL] batch:  284 | acc: 100.00%,  total acc: 65.75%   [EVAL] batch:  285 | acc: 93.75%,  total acc: 65.84%   [EVAL] batch:  286 | acc: 87.50%,  total acc: 65.92%   [EVAL] batch:  287 | acc: 100.00%,  total acc: 66.04%   [EVAL] batch:  288 | acc: 93.75%,  total acc: 66.13%   [EVAL] batch:  289 | acc: 93.75%,  total acc: 66.23%   [EVAL] batch:  290 | acc: 87.50%,  total acc: 66.30%   [EVAL] batch:  291 | acc: 87.50%,  total acc: 66.37%   [EVAL] batch:  292 | acc: 100.00%,  total acc: 66.49%   [EVAL] batch:  293 | acc: 87.50%,  total acc: 66.56%   [EVAL] batch:  294 | acc: 62.50%,  total acc: 66.55%   [EVAL] batch:  295 | acc: 37.50%,  total acc: 66.45%   [EVAL] batch:  296 | acc: 25.00%,  total acc: 66.31%   [EVAL] batch:  297 | acc: 43.75%,  total acc: 66.23%   [EVAL] batch:  298 | acc: 81.25%,  total acc: 66.28%   [EVAL] batch:  299 | acc: 31.25%,  total acc: 66.17%   [EVAL] batch:  300 | acc: 31.25%,  total acc: 66.05%   [EVAL] batch:  301 | acc: 62.50%,  total acc: 66.04%   [EVAL] batch:  302 | acc: 62.50%,  total acc: 66.03%   [EVAL] batch:  303 | acc: 50.00%,  total acc: 65.97%   [EVAL] batch:  304 | acc: 75.00%,  total acc: 66.00%   [EVAL] batch:  305 | acc: 75.00%,  total acc: 66.03%   [EVAL] batch:  306 | acc: 43.75%,  total acc: 65.96%   [EVAL] batch:  307 | acc: 43.75%,  total acc: 65.89%   [EVAL] batch:  308 | acc: 43.75%,  total acc: 65.82%   [EVAL] batch:  309 | acc: 75.00%,  total acc: 65.85%   [EVAL] batch:  310 | acc: 43.75%,  total acc: 65.78%   [EVAL] batch:  311 | acc: 87.50%,  total acc: 65.85%   [EVAL] batch:  312 | acc: 56.25%,  total acc: 65.81%   [EVAL] batch:  313 | acc: 43.75%,  total acc: 65.74%   [EVAL] batch:  314 | acc: 50.00%,  total acc: 65.69%   [EVAL] batch:  315 | acc: 37.50%,  total acc: 65.61%   [EVAL] batch:  316 | acc: 56.25%,  total acc: 65.58%   [EVAL] batch:  317 | acc: 68.75%,  total acc: 65.59%   [EVAL] batch:  318 | acc: 75.00%,  total acc: 65.62%   [EVAL] batch:  319 | acc: 100.00%,  total acc: 65.72%   [EVAL] batch:  320 | acc: 100.00%,  total acc: 65.83%   [EVAL] batch:  321 | acc: 81.25%,  total acc: 65.88%   [EVAL] batch:  322 | acc: 93.75%,  total acc: 65.96%   [EVAL] batch:  323 | acc: 87.50%,  total acc: 66.03%   [EVAL] batch:  324 | acc: 87.50%,  total acc: 66.10%   [EVAL] batch:  325 | acc: 75.00%,  total acc: 66.12%   [EVAL] batch:  326 | acc: 50.00%,  total acc: 66.07%   [EVAL] batch:  327 | acc: 62.50%,  total acc: 66.06%   [EVAL] batch:  328 | acc: 56.25%,  total acc: 66.03%   [EVAL] batch:  329 | acc: 75.00%,  total acc: 66.06%   [EVAL] batch:  330 | acc: 68.75%,  total acc: 66.07%   [EVAL] batch:  331 | acc: 56.25%,  total acc: 66.04%   [EVAL] batch:  332 | acc: 31.25%,  total acc: 65.93%   [EVAL] batch:  333 | acc: 31.25%,  total acc: 65.83%   [EVAL] batch:  334 | acc: 31.25%,  total acc: 65.73%   [EVAL] batch:  335 | acc: 43.75%,  total acc: 65.66%   [EVAL] batch:  336 | acc: 25.00%,  total acc: 65.54%   [EVAL] batch:  337 | acc: 62.50%,  total acc: 65.53%   [EVAL] batch:  338 | acc: 75.00%,  total acc: 65.56%   [EVAL] batch:  339 | acc: 56.25%,  total acc: 65.53%   [EVAL] batch:  340 | acc: 56.25%,  total acc: 65.51%   [EVAL] batch:  341 | acc: 62.50%,  total acc: 65.50%   [EVAL] batch:  342 | acc: 56.25%,  total acc: 65.47%   [EVAL] batch:  343 | acc: 37.50%,  total acc: 65.39%   [EVAL] batch:  344 | acc: 68.75%,  total acc: 65.40%   [EVAL] batch:  345 | acc: 43.75%,  total acc: 65.34%   [EVAL] batch:  346 | acc: 62.50%,  total acc: 65.33%   [EVAL] batch:  347 | acc: 68.75%,  total acc: 65.34%   [EVAL] batch:  348 | acc: 75.00%,  total acc: 65.37%   [EVAL] batch:  349 | acc: 50.00%,  total acc: 65.32%   [EVAL] batch:  350 | acc: 68.75%,  total acc: 65.33%   [EVAL] batch:  351 | acc: 81.25%,  total acc: 65.38%   [EVAL] batch:  352 | acc: 56.25%,  total acc: 65.35%   [EVAL] batch:  353 | acc: 81.25%,  total acc: 65.40%   [EVAL] batch:  354 | acc: 81.25%,  total acc: 65.44%   [EVAL] batch:  355 | acc: 75.00%,  total acc: 65.47%   [EVAL] batch:  356 | acc: 56.25%,  total acc: 65.44%   [EVAL] batch:  357 | acc: 25.00%,  total acc: 65.33%   [EVAL] batch:  358 | acc: 31.25%,  total acc: 65.23%   [EVAL] batch:  359 | acc: 37.50%,  total acc: 65.16%   [EVAL] batch:  360 | acc: 37.50%,  total acc: 65.08%   [EVAL] batch:  361 | acc: 50.00%,  total acc: 65.04%   [EVAL] batch:  362 | acc: 68.75%,  total acc: 65.05%   [EVAL] batch:  363 | acc: 100.00%,  total acc: 65.14%   [EVAL] batch:  364 | acc: 100.00%,  total acc: 65.24%   [EVAL] batch:  365 | acc: 100.00%,  total acc: 65.33%   [EVAL] batch:  366 | acc: 100.00%,  total acc: 65.43%   [EVAL] batch:  367 | acc: 100.00%,  total acc: 65.52%   [EVAL] batch:  368 | acc: 93.75%,  total acc: 65.60%   [EVAL] batch:  369 | acc: 100.00%,  total acc: 65.69%   [EVAL] batch:  370 | acc: 81.25%,  total acc: 65.73%   [EVAL] batch:  371 | acc: 93.75%,  total acc: 65.81%   [EVAL] batch:  372 | acc: 100.00%,  total acc: 65.90%   [EVAL] batch:  373 | acc: 87.50%,  total acc: 65.96%   [EVAL] batch:  374 | acc: 87.50%,  total acc: 66.02%   [EVAL] batch:  375 | acc: 56.25%,  total acc: 65.99%   [EVAL] batch:  376 | acc: 56.25%,  total acc: 65.96%   [EVAL] batch:  377 | acc: 68.75%,  total acc: 65.97%   [EVAL] batch:  378 | acc: 75.00%,  total acc: 66.00%   [EVAL] batch:  379 | acc: 68.75%,  total acc: 66.00%   [EVAL] batch:  380 | acc: 93.75%,  total acc: 66.08%   [EVAL] batch:  381 | acc: 43.75%,  total acc: 66.02%   [EVAL] batch:  382 | acc: 62.50%,  total acc: 66.01%   [EVAL] batch:  383 | acc: 68.75%,  total acc: 66.02%   [EVAL] batch:  384 | acc: 81.25%,  total acc: 66.06%   [EVAL] batch:  385 | acc: 56.25%,  total acc: 66.03%   [EVAL] batch:  386 | acc: 56.25%,  total acc: 66.00%   [EVAL] batch:  387 | acc: 81.25%,  total acc: 66.04%   [EVAL] batch:  388 | acc: 81.25%,  total acc: 66.08%   [EVAL] batch:  389 | acc: 75.00%,  total acc: 66.11%   [EVAL] batch:  390 | acc: 62.50%,  total acc: 66.10%   [EVAL] batch:  391 | acc: 87.50%,  total acc: 66.15%   [EVAL] batch:  392 | acc: 100.00%,  total acc: 66.24%   [EVAL] batch:  393 | acc: 100.00%,  total acc: 66.32%   [EVAL] batch:  394 | acc: 87.50%,  total acc: 66.38%   [EVAL] batch:  395 | acc: 100.00%,  total acc: 66.46%   [EVAL] batch:  396 | acc: 100.00%,  total acc: 66.55%   [EVAL] batch:  397 | acc: 87.50%,  total acc: 66.60%   [EVAL] batch:  398 | acc: 87.50%,  total acc: 66.65%   [EVAL] batch:  399 | acc: 100.00%,  total acc: 66.73%   [EVAL] batch:  400 | acc: 62.50%,  total acc: 66.72%   [EVAL] batch:  401 | acc: 62.50%,  total acc: 66.71%   [EVAL] batch:  402 | acc: 87.50%,  total acc: 66.76%   [EVAL] batch:  403 | acc: 75.00%,  total acc: 66.79%   [EVAL] batch:  404 | acc: 81.25%,  total acc: 66.82%   [EVAL] batch:  405 | acc: 81.25%,  total acc: 66.86%   [EVAL] batch:  406 | acc: 62.50%,  total acc: 66.85%   [EVAL] batch:  407 | acc: 50.00%,  total acc: 66.80%   [EVAL] batch:  408 | acc: 18.75%,  total acc: 66.69%   [EVAL] batch:  409 | acc: 31.25%,  total acc: 66.60%   [EVAL] batch:  410 | acc: 18.75%,  total acc: 66.48%   [EVAL] batch:  411 | acc: 18.75%,  total acc: 66.37%   [EVAL] batch:  412 | acc: 50.00%,  total acc: 66.33%   [EVAL] batch:  413 | acc: 62.50%,  total acc: 66.32%   [EVAL] batch:  414 | acc: 75.00%,  total acc: 66.34%   [EVAL] batch:  415 | acc: 50.00%,  total acc: 66.30%   [EVAL] batch:  416 | acc: 87.50%,  total acc: 66.35%   [EVAL] batch:  417 | acc: 62.50%,  total acc: 66.34%   [EVAL] batch:  418 | acc: 62.50%,  total acc: 66.33%   [EVAL] batch:  419 | acc: 75.00%,  total acc: 66.35%   [EVAL] batch:  420 | acc: 68.75%,  total acc: 66.36%   [EVAL] batch:  421 | acc: 68.75%,  total acc: 66.37%   [EVAL] batch:  422 | acc: 56.25%,  total acc: 66.34%   [EVAL] batch:  423 | acc: 62.50%,  total acc: 66.33%   [EVAL] batch:  424 | acc: 87.50%,  total acc: 66.38%   [EVAL] batch:  425 | acc: 100.00%,  total acc: 66.46%   [EVAL] batch:  426 | acc: 100.00%,  total acc: 66.54%   [EVAL] batch:  427 | acc: 100.00%,  total acc: 66.62%   [EVAL] batch:  428 | acc: 100.00%,  total acc: 66.70%   [EVAL] batch:  429 | acc: 100.00%,  total acc: 66.77%   [EVAL] batch:  430 | acc: 100.00%,  total acc: 66.85%   [EVAL] batch:  431 | acc: 93.75%,  total acc: 66.91%   [EVAL] batch:  432 | acc: 87.50%,  total acc: 66.96%   [EVAL] batch:  433 | acc: 87.50%,  total acc: 67.01%   [EVAL] batch:  434 | acc: 93.75%,  total acc: 67.07%   [EVAL] batch:  435 | acc: 87.50%,  total acc: 67.12%   [EVAL] batch:  436 | acc: 100.00%,  total acc: 67.19%   [EVAL] batch:  437 | acc: 50.00%,  total acc: 67.15%   
cur_acc:  ['0.9524', '0.8264', '0.7470', '0.7520', '0.6587', '0.6994', '0.7391']
his_acc:  ['0.9524', '0.8815', '0.8039', '0.7535', '0.7133', '0.6808', '0.6715']
CurrentTrain: epoch 15, batch     0 | loss: 19.3473571CurrentTrain: epoch 15, batch     1 | loss: 16.3243509CurrentTrain: epoch 15, batch     2 | loss: 13.2913611CurrentTrain: epoch  1, batch     3 | loss: 9.3296215CurrentTrain: epoch 15, batch     0 | loss: 14.8496891CurrentTrain: epoch 15, batch     1 | loss: 12.3230159CurrentTrain: epoch 15, batch     2 | loss: 17.0751259CurrentTrain: epoch  1, batch     3 | loss: 17.1236671CurrentTrain: epoch 15, batch     0 | loss: 12.8542488CurrentTrain: epoch 15, batch     1 | loss: 12.5533398CurrentTrain: epoch 15, batch     2 | loss: 11.4617974CurrentTrain: epoch  1, batch     3 | loss: 7.5378856CurrentTrain: epoch 15, batch     0 | loss: 17.8755119CurrentTrain: epoch 15, batch     1 | loss: 14.4561219CurrentTrain: epoch 15, batch     2 | loss: 8.9249074CurrentTrain: epoch  1, batch     3 | loss: 8.0080125CurrentTrain: epoch 15, batch     0 | loss: 9.0641445CurrentTrain: epoch 15, batch     1 | loss: 18.4591219CurrentTrain: epoch 15, batch     2 | loss: 10.1302680CurrentTrain: epoch  1, batch     3 | loss: 7.7031491CurrentTrain: epoch 15, batch     0 | loss: 9.3663937CurrentTrain: epoch 15, batch     1 | loss: 8.8682903CurrentTrain: epoch 15, batch     2 | loss: 9.0005608CurrentTrain: epoch  1, batch     3 | loss: 6.6309125CurrentTrain: epoch 15, batch     0 | loss: 12.8524292CurrentTrain: epoch 15, batch     1 | loss: 9.1959575CurrentTrain: epoch 15, batch     2 | loss: 9.1333572CurrentTrain: epoch  1, batch     3 | loss: 6.6768316CurrentTrain: epoch 15, batch     0 | loss: 9.9760566CurrentTrain: epoch 15, batch     1 | loss: 14.6919773CurrentTrain: epoch 15, batch     2 | loss: 5.8483547CurrentTrain: epoch  1, batch     3 | loss: 7.0949533CurrentTrain: epoch 15, batch     0 | loss: 9.7826878CurrentTrain: epoch 15, batch     1 | loss: 8.1562487CurrentTrain: epoch 15, batch     2 | loss: 9.1353358CurrentTrain: epoch  1, batch     3 | loss: 6.4800680CurrentTrain: epoch 15, batch     0 | loss: 10.7200549CurrentTrain: epoch 15, batch     1 | loss: 7.6908093CurrentTrain: epoch 15, batch     2 | loss: 9.5163908CurrentTrain: epoch  1, batch     3 | loss: 7.1764950
#### member of political party ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "member of political party" means the political party of which a person is or has been a member or otherwise affiliated.
Here is an example:
Relation: member of political party
Context: alongside evan durbin and hugh gaitskell , he brought the thinking of john maynard keynes to the labour party , especially in relation to price determination .
Head Entity: hugh gaitskell
Tail Entity: labour party
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "member of political party":
Relation: member of political party  
Context: After serving as the mayor of Chicago, he became a prominent figure in the Democratic Party, advocating for social justice and economic reform.  
Head Entity: mayor of Chicago  
Tail Entity: Democratic Party  

Relation: member of political party  
Context: Throughout her career, she has been a staunch advocate for environmental policies as a member of the Green Party, pushing for sustainable practices in government.  
Head Entity: she  
Tail Entity: Green Party  
#### after a work by ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "after a work by" means artist whose work strongly inspired/ was copied in this item.
Here is an example:
Relation: after a work by
Context: the original story of real life escape of betty mahmoody is depicted in the movie " not without my daughter " which itself was based on betty mahmoody 's book of the same name .
Head Entity: not without my daughter
Tail Entity: betty mahmoody
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "after a work by":
Relation: after a work by  
Context: the film "the great gatsby" draws heavily from f. scott fitzgerald's classic novel, capturing the essence of the roaring twenties and the complexities of love and ambition.  
Head Entity: the great gatsby  
Tail Entity: f. scott fitzgerald  

Relation: after a work by  
Context: the animated feature "the lion king" was inspired by shakespeare's play "hamlet," incorporating themes of betrayal, revenge, and the struggle for power.  
Head Entity: the lion king  
Tail Entity: shakespeare
#### notable work ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "notable work" means notable scientific, artistic or literary work, or other work of significance among subject's works.
Here is an example:
Relation: notable work
Context: the influential victorian art critic john ruskin expounded theories about social justice in " unto this last " ( 1860 ) .
Head Entity: john ruskin
Tail Entity: unto this last
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "notable work":
Relation: notable work  
Context: the renowned physicist albert einstein published his groundbreaking paper on the theory of relativity in "on the electrodynamics of moving bodies" (1905).  
Head Entity: albert einstein  
Tail Entity: on the electrodynamics of moving bodies  

Relation: notable work  
Context: the celebrated author j.k. rowling captivated readers with her first book in the harry potter series, "harry potter and the philosopher's stone" (1997).  
Head Entity: j.k. rowling  
Tail Entity: harry potter and the philosopher's stone  
#### headquarters location ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "headquarters location" means city, where an organization's headquarters is or has been situated.
Here is an example:
Relation: headquarters location
Context: at the request of the stockholm institute for bible translation , mirza khazar translated the bible – the new testament and old testament – into the azerbaijani language .
Head Entity: institute for bible translation
Tail Entity: stockholm
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "headquarters location":
Relation: headquarters location  
Context: the global technology company, headquartered in san francisco, has been a leader in software development for over two decades.  
Head Entity: global technology company  
Tail Entity: san francisco  

Relation: headquarters location  
Context: after years of expansion, the non-profit organization moved its headquarters to new york city to better serve its growing membership.  
Head Entity: non-profit organization  
Tail Entity: new york city  
#### taxon rank ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "taxon rank" means level in a taxonomic hierarchy.
Here is an example:
Relation: taxon rank
Context: a number of species that were placed in the family phyllophoridae by thander in 1989 and 1990 are now included in cucumariidae .
Head Entity: phyllophoridae
Tail Entity: family
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "taxon rank":
Relation: taxon rank  
Context: The genus Canis includes several species, with Canis lupus being the most recognized as the wolf, which belongs to the family Canidae.  
Head Entity: Canis  
Tail Entity: genus  

Relation: taxon rank  
Context: In the classification of plants, the family Rosaceae encompasses a variety of species, including roses and strawberries, which are categorized under the order Rosales.  
Head Entity: Rosaceae  
Tail Entity: family  
#### field of work ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "field of work" means specialization of a person or organization.
Here is an example:
Relation: field of work
Context: elspeth frances garman is professor of molecular biophysics at the university of oxford and a former president of the british crystallographic association .
Head Entity: elspeth frances garman
Tail Entity: molecular biophysics
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "field of work":
Relation: field of work  
Context: dr. amanda jones is a leading researcher in artificial intelligence and currently works at the tech innovation lab in silicon valley.  
Head Entity: dr. amanda jones  
Tail Entity: artificial intelligence  

Relation: field of work  
Context: the world health organization has appointed dr. rajesh kumar as the new director of global health initiatives, focusing on infectious diseases.  
Head Entity: dr. rajesh kumar  
Tail Entity: infectious diseases  
#### licensed to broadcast to ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "licensed to broadcast to" means place that a radio/TV station is licensed/required to broadcast to.
Here is an example:
Relation: licensed to broadcast to
Context: this early involvement in radio led him to buy many small , under performing radio stations beginning with wcos in columbia , south carolina , and turn them around with new formats and improved management .
Head Entity: wcos
Tail Entity: columbia , south carolina
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "licensed to broadcast to":
Relation: licensed to broadcast to  
Context: The new regulations allowed the station KXYZ to expand its reach, enabling it to serve the community of Springfield more effectively.  
Head Entity: KXYZ  
Tail Entity: Springfield  

Relation: licensed to broadcast to  
Context: After acquiring the rights, the television network was able to broadcast its popular shows to the audience in the city of Denver, significantly increasing its viewership.  
Head Entity: television network  
Tail Entity: Denver  
#### constellation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "constellation" means the area of the celestial sphere of which the subject is a part.
Here is an example:
Relation: constellation
Context: ζ microscopii , latinised as zeta microscopii , is a solitary , yellow - white hued star in the southern constellation of microscopium .
Head Entity: zeta microscopii
Tail Entity: microscopium
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "constellation":
Relation: constellation  
Context: The star Betelgeuse is located in the constellation of Orion, which is one of the most recognizable constellations in the night sky.  
Head Entity: Betelgeuse  
Tail Entity: Orion  

Relation: constellation  
Context: The Andromeda Galaxy is part of the Andromeda constellation, which is named after a princess in Greek mythology.  
Head Entity: Andromeda Galaxy  
Tail Entity: Andromeda
#### platform ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "platform" means platform for which a work was developed or released, or the specific platform version of a software product.
Here is an example:
Relation: platform
Context: " toy soldiers : war chest " was released on august 11 , 2015 for windows , playstation 4 and xbox one .
Head Entity: toy soldiers : war chest
Tail Entity: xbox one
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "platform":
Relation: platform  
Context: "The Legend of Zelda: Breath of the Wild" was launched on March 3, 2017 for the Nintendo Switch and Wii U.  
Head Entity: The Legend of Zelda: Breath of the Wild  
Tail Entity: Nintendo Switch  

Relation: platform  
Context: "Cyberpunk 2077" was made available on December 10, 2020 for PC, PlayStation 4, and Xbox One.  
Head Entity: Cyberpunk 2077  
Tail Entity: PlayStation 4  
#### place served by transport hub ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "place served by transport hub" means territorial entity or entities served by this transport hub (airport, train station, etc.).
Here is an example:
Relation: place served by transport hub
Context: services to the island run from san juan 's luis muñoz marín international airport , ceiba airport or isla grande airport ( 20- to 30-minute flight ) .
Head Entity: isla grande airport
Tail Entity: san juan
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "place served by transport hub":
Relation: place served by transport hub  
Context: The city of Barcelona is well-connected to the surrounding regions through the El Prat Airport, which offers numerous flights to various destinations.  
Head Entity: El Prat Airport  
Tail Entity: Barcelona  

Relation: place served by transport hub  
Context: The central train station in Zurich provides access to multiple Swiss cities and international destinations, making it a key transport hub in the region.  
Head Entity: Zurich central train station  
Tail Entity: Zurich  
MemoryTrain:  epoch 15, batch     0 | loss: 2.9656390MemoryTrain:  epoch 15, batch     1 | loss: 2.6798655MemoryTrain:  epoch 15, batch     2 | loss: 2.1712637MemoryTrain:  epoch 15, batch     3 | loss: 2.4936253MemoryTrain:  epoch 15, batch     4 | loss: 2.5757505MemoryTrain:  epoch 15, batch     5 | loss: 2.4715849MemoryTrain:  epoch 15, batch     6 | loss: 2.1288474MemoryTrain:  epoch 15, batch     7 | loss: 2.7971743MemoryTrain:  epoch 15, batch     8 | loss: 3.8737177MemoryTrain:  epoch 15, batch     9 | loss: 2.4260139MemoryTrain:  epoch 15, batch    10 | loss: 3.6366262MemoryTrain:  epoch 15, batch    11 | loss: 2.3289149MemoryTrain:  epoch 15, batch    12 | loss: 1.9772580MemoryTrain:  epoch 15, batch    13 | loss: 2.6365175MemoryTrain:  epoch 15, batch    14 | loss: 5.0118929MemoryTrain:  epoch 15, batch     0 | loss: 4.9546701MemoryTrain:  epoch 15, batch     1 | loss: 1.5598130MemoryTrain:  epoch 15, batch     2 | loss: 2.1870950MemoryTrain:  epoch 15, batch     3 | loss: 2.2721692MemoryTrain:  epoch 15, batch     4 | loss: 2.8769685MemoryTrain:  epoch 15, batch     5 | loss: 2.5772921MemoryTrain:  epoch 15, batch     6 | loss: 4.2913169MemoryTrain:  epoch 15, batch     7 | loss: 2.0074887MemoryTrain:  epoch 15, batch     8 | loss: 4.6180043MemoryTrain:  epoch 15, batch     9 | loss: 3.8445562MemoryTrain:  epoch 15, batch    10 | loss: 2.3798402MemoryTrain:  epoch 15, batch    11 | loss: 1.9938342MemoryTrain:  epoch 15, batch    12 | loss: 3.3735700MemoryTrain:  epoch 15, batch    13 | loss: 2.4711882MemoryTrain:  epoch 15, batch    14 | loss: 2.3168298MemoryTrain:  epoch 15, batch     0 | loss: 2.7097569MemoryTrain:  epoch 15, batch     1 | loss: 3.4789781MemoryTrain:  epoch 15, batch     2 | loss: 2.1210297MemoryTrain:  epoch 15, batch     3 | loss: 4.9185944MemoryTrain:  epoch 15, batch     4 | loss: 1.9273034MemoryTrain:  epoch 15, batch     5 | loss: 2.8174350MemoryTrain:  epoch 15, batch     6 | loss: 2.0536809MemoryTrain:  epoch 15, batch     7 | loss: 3.5052267MemoryTrain:  epoch 15, batch     8 | loss: 4.7798766MemoryTrain:  epoch 15, batch     9 | loss: 1.6763397MemoryTrain:  epoch 15, batch    10 | loss: 1.3952620MemoryTrain:  epoch 15, batch    11 | loss: 3.8890839MemoryTrain:  epoch 15, batch    12 | loss: 1.3726785MemoryTrain:  epoch 15, batch    13 | loss: 1.8323377MemoryTrain:  epoch 15, batch    14 | loss: 4.5602535MemoryTrain:  epoch 15, batch     0 | loss: 3.3307098MemoryTrain:  epoch 15, batch     1 | loss: 1.9078047MemoryTrain:  epoch 15, batch     2 | loss: 1.4465236MemoryTrain:  epoch 15, batch     3 | loss: 1.8008249MemoryTrain:  epoch 15, batch     4 | loss: 1.7129155MemoryTrain:  epoch 15, batch     5 | loss: 4.6700669MemoryTrain:  epoch 15, batch     6 | loss: 1.8415658MemoryTrain:  epoch 15, batch     7 | loss: 3.1888435MemoryTrain:  epoch 15, batch     8 | loss: 1.4343798MemoryTrain:  epoch 15, batch     9 | loss: 1.6661539MemoryTrain:  epoch 15, batch    10 | loss: 2.4112719MemoryTrain:  epoch 15, batch    11 | loss: 1.5139637MemoryTrain:  epoch 15, batch    12 | loss: 2.1033614MemoryTrain:  epoch 15, batch    13 | loss: 1.8285871MemoryTrain:  epoch 15, batch    14 | loss: 1.3609559MemoryTrain:  epoch 15, batch     0 | loss: 3.0748986MemoryTrain:  epoch 15, batch     1 | loss: 2.1171758MemoryTrain:  epoch 15, batch     2 | loss: 1.4182698MemoryTrain:  epoch 15, batch     3 | loss: 2.0615711MemoryTrain:  epoch 15, batch     4 | loss: 1.8160513MemoryTrain:  epoch 15, batch     5 | loss: 1.9832042MemoryTrain:  epoch 15, batch     6 | loss: 4.2614997MemoryTrain:  epoch 15, batch     7 | loss: 4.3948857MemoryTrain:  epoch 15, batch     8 | loss: 3.7953243MemoryTrain:  epoch 15, batch     9 | loss: 1.3479602MemoryTrain:  epoch 15, batch    10 | loss: 1.5236887MemoryTrain:  epoch 15, batch    11 | loss: 2.6864288MemoryTrain:  epoch 15, batch    12 | loss: 2.3879044MemoryTrain:  epoch 15, batch    13 | loss: 2.4684666MemoryTrain:  epoch 15, batch    14 | loss: 2.5160219MemoryTrain:  epoch 15, batch     0 | loss: 4.5107788MemoryTrain:  epoch 15, batch     1 | loss: 1.3904519MemoryTrain:  epoch 15, batch     2 | loss: 1.5403896MemoryTrain:  epoch 15, batch     3 | loss: 2.2114198MemoryTrain:  epoch 15, batch     4 | loss: 1.3319838MemoryTrain:  epoch 15, batch     5 | loss: 1.3146161MemoryTrain:  epoch 15, batch     6 | loss: 1.4434094MemoryTrain:  epoch 15, batch     7 | loss: 2.3980628MemoryTrain:  epoch 15, batch     8 | loss: 1.9680408MemoryTrain:  epoch 15, batch     9 | loss: 1.9092471MemoryTrain:  epoch 15, batch    10 | loss: 1.3889071MemoryTrain:  epoch 15, batch    11 | loss: 4.2648822MemoryTrain:  epoch 15, batch    12 | loss: 1.6669224MemoryTrain:  epoch 15, batch    13 | loss: 1.3198600MemoryTrain:  epoch 15, batch    14 | loss: 2.8620941MemoryTrain:  epoch 15, batch     0 | loss: 2.1369959MemoryTrain:  epoch 15, batch     1 | loss: 1.5583671MemoryTrain:  epoch 15, batch     2 | loss: 1.4137530MemoryTrain:  epoch 15, batch     3 | loss: 1.6661703MemoryTrain:  epoch 15, batch     4 | loss: 4.6468868MemoryTrain:  epoch 15, batch     5 | loss: 1.5287135MemoryTrain:  epoch 15, batch     6 | loss: 1.4186699MemoryTrain:  epoch 15, batch     7 | loss: 1.9882731MemoryTrain:  epoch 15, batch     8 | loss: 1.6789157MemoryTrain:  epoch 15, batch     9 | loss: 2.3638250MemoryTrain:  epoch 15, batch    10 | loss: 1.4429184MemoryTrain:  epoch 15, batch    11 | loss: 1.3134626MemoryTrain:  epoch 15, batch    12 | loss: 1.6973784MemoryTrain:  epoch 15, batch    13 | loss: 1.6593633MemoryTrain:  epoch 15, batch    14 | loss: 1.6864101MemoryTrain:  epoch 15, batch     0 | loss: 1.6728487MemoryTrain:  epoch 15, batch     1 | loss: 2.2309908MemoryTrain:  epoch 15, batch     2 | loss: 1.6958032MemoryTrain:  epoch 15, batch     3 | loss: 2.4015831MemoryTrain:  epoch 15, batch     4 | loss: 2.2185587MemoryTrain:  epoch 15, batch     5 | loss: 1.2928140MemoryTrain:  epoch 15, batch     6 | loss: 5.6281916MemoryTrain:  epoch 15, batch     7 | loss: 1.3260382MemoryTrain:  epoch 15, batch     8 | loss: 1.6759370MemoryTrain:  epoch 15, batch     9 | loss: 1.3739895MemoryTrain:  epoch 15, batch    10 | loss: 1.5321327MemoryTrain:  epoch 15, batch    11 | loss: 1.7451138MemoryTrain:  epoch 15, batch    12 | loss: 1.8635004MemoryTrain:  epoch 15, batch    13 | loss: 1.3792562MemoryTrain:  epoch 15, batch    14 | loss: 1.3920384MemoryTrain:  epoch 15, batch     0 | loss: 3.1562777MemoryTrain:  epoch 15, batch     1 | loss: 1.3386452MemoryTrain:  epoch 15, batch     2 | loss: 1.7386368MemoryTrain:  epoch 15, batch     3 | loss: 1.6608238MemoryTrain:  epoch 15, batch     4 | loss: 1.4311612MemoryTrain:  epoch 15, batch     5 | loss: 1.7872044MemoryTrain:  epoch 15, batch     6 | loss: 1.6984867MemoryTrain:  epoch 15, batch     7 | loss: 2.3224491MemoryTrain:  epoch 15, batch     8 | loss: 2.1379505MemoryTrain:  epoch 15, batch     9 | loss: 1.9909744MemoryTrain:  epoch 15, batch    10 | loss: 1.4109497MemoryTrain:  epoch 15, batch    11 | loss: 1.3325261MemoryTrain:  epoch 15, batch    12 | loss: 2.1329507MemoryTrain:  epoch 15, batch    13 | loss: 1.9666849MemoryTrain:  epoch 15, batch    14 | loss: 1.7339554MemoryTrain:  epoch 15, batch     0 | loss: 1.6545455MemoryTrain:  epoch 15, batch     1 | loss: 1.8003565MemoryTrain:  epoch 15, batch     2 | loss: 2.5600130MemoryTrain:  epoch 15, batch     3 | loss: 1.6447277MemoryTrain:  epoch 15, batch     4 | loss: 1.3219545MemoryTrain:  epoch 15, batch     5 | loss: 2.6093835MemoryTrain:  epoch 15, batch     6 | loss: 2.3880617MemoryTrain:  epoch 15, batch     7 | loss: 1.4149270MemoryTrain:  epoch 15, batch     8 | loss: 2.8162840MemoryTrain:  epoch 15, batch     9 | loss: 1.7529767MemoryTrain:  epoch 15, batch    10 | loss: 2.5250500MemoryTrain:  epoch 15, batch    11 | loss: 3.7855655MemoryTrain:  epoch 15, batch    12 | loss: 1.2762847MemoryTrain:  epoch 15, batch    13 | loss: 1.3156551MemoryTrain:  epoch 15, batch    14 | loss: 1.4099019
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 92.19%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 92.50%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 92.36%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 91.88%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 90.91%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 89.58%   [EVAL] batch:   12 | acc: 62.50%,  total acc: 87.50%   [EVAL] batch:   13 | acc: 75.00%,  total acc: 86.61%   [EVAL] batch:   14 | acc: 62.50%,  total acc: 85.00%   [EVAL] batch:   15 | acc: 50.00%,  total acc: 82.81%   [EVAL] batch:   16 | acc: 56.25%,  total acc: 81.25%   [EVAL] batch:   17 | acc: 50.00%,  total acc: 79.51%   [EVAL] batch:   18 | acc: 56.25%,  total acc: 78.29%   [EVAL] batch:   19 | acc: 25.00%,  total acc: 75.62%   [EVAL] batch:   20 | acc: 37.50%,  total acc: 73.81%   [EVAL] batch:   21 | acc: 50.00%,  total acc: 72.73%   [EVAL] batch:   22 | acc: 25.00%,  total acc: 70.65%   [EVAL] batch:   23 | acc: 50.00%,  total acc: 69.79%   [EVAL] batch:   24 | acc: 25.00%,  total acc: 68.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 69.23%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 70.37%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 71.43%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 72.41%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 73.33%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 74.19%   [EVAL] batch:   31 | acc: 75.00%,  total acc: 74.22%   [EVAL] batch:   32 | acc: 68.75%,  total acc: 74.05%   [EVAL] batch:   33 | acc: 62.50%,  total acc: 73.71%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 73.75%   [EVAL] batch:   35 | acc: 62.50%,  total acc: 73.44%   [EVAL] batch:   36 | acc: 56.25%,  total acc: 72.97%   [EVAL] batch:   37 | acc: 68.75%,  total acc: 72.86%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 73.56%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 74.22%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 74.85%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 75.45%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 76.02%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 76.42%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 76.94%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 77.45%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 77.93%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 78.39%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 78.83%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 79.25%   [EVAL] batch:   50 | acc: 81.25%,  total acc: 79.29%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 79.57%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 79.83%   [EVAL] batch:   53 | acc: 93.75%,  total acc: 80.09%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 80.23%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 80.47%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 80.70%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 80.93%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 81.04%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 81.35%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 81.66%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 81.75%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 81.25%   
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 71.88%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 72.92%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 71.88%   [EVAL] batch:    4 | acc: 56.25%,  total acc: 68.75%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 70.83%   [EVAL] batch:    6 | acc: 37.50%,  total acc: 66.07%   [EVAL] batch:    7 | acc: 43.75%,  total acc: 63.28%   [EVAL] batch:    8 | acc: 43.75%,  total acc: 61.11%   [EVAL] batch:    9 | acc: 43.75%,  total acc: 59.38%   [EVAL] batch:   10 | acc: 37.50%,  total acc: 57.39%   [EVAL] batch:   11 | acc: 37.50%,  total acc: 55.73%   [EVAL] batch:   12 | acc: 56.25%,  total acc: 55.77%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 58.48%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 60.83%   [EVAL] batch:   15 | acc: 93.75%,  total acc: 62.89%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 64.71%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 66.32%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 67.76%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 68.12%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 69.05%   [EVAL] batch:   21 | acc: 75.00%,  total acc: 69.32%   [EVAL] batch:   22 | acc: 62.50%,  total acc: 69.02%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 69.79%   [EVAL] batch:   24 | acc: 62.50%,  total acc: 69.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 70.67%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 71.76%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 72.77%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 73.49%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 74.38%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 75.20%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 75.78%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 76.52%   [EVAL] batch:   33 | acc: 62.50%,  total acc: 76.10%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 76.07%   [EVAL] batch:   35 | acc: 81.25%,  total acc: 76.22%   [EVAL] batch:   36 | acc: 93.75%,  total acc: 76.69%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 76.97%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 77.56%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 78.12%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 78.66%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 79.02%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 79.36%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 79.40%   [EVAL] batch:   44 | acc: 62.50%,  total acc: 79.03%   [EVAL] batch:   45 | acc: 56.25%,  total acc: 78.53%   [EVAL] batch:   46 | acc: 43.75%,  total acc: 77.79%   [EVAL] batch:   47 | acc: 56.25%,  total acc: 77.34%   [EVAL] batch:   48 | acc: 50.00%,  total acc: 76.79%   [EVAL] batch:   49 | acc: 68.75%,  total acc: 76.62%   [EVAL] batch:   50 | acc: 75.00%,  total acc: 76.59%   [EVAL] batch:   51 | acc: 75.00%,  total acc: 76.56%   [EVAL] batch:   52 | acc: 81.25%,  total acc: 76.65%   [EVAL] batch:   53 | acc: 62.50%,  total acc: 76.39%   [EVAL] batch:   54 | acc: 56.25%,  total acc: 76.02%   [EVAL] batch:   55 | acc: 81.25%,  total acc: 76.12%   [EVAL] batch:   56 | acc: 56.25%,  total acc: 75.77%   [EVAL] batch:   57 | acc: 43.75%,  total acc: 75.22%   [EVAL] batch:   58 | acc: 50.00%,  total acc: 74.79%   [EVAL] batch:   59 | acc: 56.25%,  total acc: 74.48%   [EVAL] batch:   60 | acc: 62.50%,  total acc: 74.28%   [EVAL] batch:   61 | acc: 62.50%,  total acc: 74.09%   [EVAL] batch:   62 | acc: 62.50%,  total acc: 73.91%   [EVAL] batch:   63 | acc: 62.50%,  total acc: 73.73%   [EVAL] batch:   64 | acc: 75.00%,  total acc: 73.75%   [EVAL] batch:   65 | acc: 87.50%,  total acc: 73.96%   [EVAL] batch:   66 | acc: 68.75%,  total acc: 73.88%   [EVAL] batch:   67 | acc: 81.25%,  total acc: 73.99%   [EVAL] batch:   68 | acc: 81.25%,  total acc: 74.09%   [EVAL] batch:   69 | acc: 81.25%,  total acc: 74.20%   [EVAL] batch:   70 | acc: 68.75%,  total acc: 74.12%   [EVAL] batch:   71 | acc: 56.25%,  total acc: 73.87%   [EVAL] batch:   72 | acc: 87.50%,  total acc: 74.06%   [EVAL] batch:   73 | acc: 75.00%,  total acc: 74.07%   [EVAL] batch:   74 | acc: 81.25%,  total acc: 74.17%   [EVAL] batch:   75 | acc: 81.25%,  total acc: 74.26%   [EVAL] batch:   76 | acc: 68.75%,  total acc: 74.19%   [EVAL] batch:   77 | acc: 68.75%,  total acc: 74.12%   [EVAL] batch:   78 | acc: 81.25%,  total acc: 74.21%   [EVAL] batch:   79 | acc: 87.50%,  total acc: 74.38%   [EVAL] batch:   80 | acc: 93.75%,  total acc: 74.61%   [EVAL] batch:   81 | acc: 31.25%,  total acc: 74.09%   [EVAL] batch:   82 | acc: 12.50%,  total acc: 73.34%   [EVAL] batch:   83 | acc: 6.25%,  total acc: 72.54%   [EVAL] batch:   84 | acc: 6.25%,  total acc: 71.76%   [EVAL] batch:   85 | acc: 12.50%,  total acc: 71.08%   [EVAL] batch:   86 | acc: 18.75%,  total acc: 70.47%   [EVAL] batch:   87 | acc: 43.75%,  total acc: 70.17%   [EVAL] batch:   88 | acc: 93.75%,  total acc: 70.44%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 70.76%   [EVAL] batch:   90 | acc: 81.25%,  total acc: 70.88%   [EVAL] batch:   91 | acc: 93.75%,  total acc: 71.13%   [EVAL] batch:   92 | acc: 81.25%,  total acc: 71.24%   [EVAL] batch:   93 | acc: 93.75%,  total acc: 71.48%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 71.78%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 72.07%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 72.36%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 72.64%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 72.92%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 73.19%   [EVAL] batch:  100 | acc: 93.75%,  total acc: 73.39%   [EVAL] batch:  101 | acc: 93.75%,  total acc: 73.59%   [EVAL] batch:  102 | acc: 75.00%,  total acc: 73.60%   [EVAL] batch:  103 | acc: 87.50%,  total acc: 73.74%   [EVAL] batch:  104 | acc: 100.00%,  total acc: 73.99%   [EVAL] batch:  105 | acc: 93.75%,  total acc: 74.17%   [EVAL] batch:  106 | acc: 43.75%,  total acc: 73.89%   [EVAL] batch:  107 | acc: 25.00%,  total acc: 73.44%   [EVAL] batch:  108 | acc: 25.00%,  total acc: 72.99%   [EVAL] batch:  109 | acc: 31.25%,  total acc: 72.61%   [EVAL] batch:  110 | acc: 62.50%,  total acc: 72.52%   [EVAL] batch:  111 | acc: 43.75%,  total acc: 72.27%   [EVAL] batch:  112 | acc: 62.50%,  total acc: 72.18%   [EVAL] batch:  113 | acc: 87.50%,  total acc: 72.31%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 72.55%   [EVAL] batch:  115 | acc: 81.25%,  total acc: 72.63%   [EVAL] batch:  116 | acc: 87.50%,  total acc: 72.76%   [EVAL] batch:  117 | acc: 93.75%,  total acc: 72.93%   [EVAL] batch:  118 | acc: 68.75%,  total acc: 72.90%   [EVAL] batch:  119 | acc: 6.25%,  total acc: 72.34%   [EVAL] batch:  120 | acc: 31.25%,  total acc: 72.00%   [EVAL] batch:  121 | acc: 0.00%,  total acc: 71.41%   [EVAL] batch:  122 | acc: 6.25%,  total acc: 70.88%   [EVAL] batch:  123 | acc: 0.00%,  total acc: 70.31%   [EVAL] batch:  124 | acc: 0.00%,  total acc: 69.75%   [EVAL] batch:  125 | acc: 37.50%,  total acc: 69.49%   [EVAL] batch:  126 | acc: 37.50%,  total acc: 69.24%   [EVAL] batch:  127 | acc: 50.00%,  total acc: 69.09%   [EVAL] batch:  128 | acc: 56.25%,  total acc: 68.99%   [EVAL] batch:  129 | acc: 62.50%,  total acc: 68.94%   [EVAL] batch:  130 | acc: 43.75%,  total acc: 68.75%   [EVAL] batch:  131 | acc: 62.50%,  total acc: 68.70%   [EVAL] batch:  132 | acc: 56.25%,  total acc: 68.61%   [EVAL] batch:  133 | acc: 43.75%,  total acc: 68.42%   [EVAL] batch:  134 | acc: 56.25%,  total acc: 68.33%   [EVAL] batch:  135 | acc: 68.75%,  total acc: 68.34%   [EVAL] batch:  136 | acc: 56.25%,  total acc: 68.25%   [EVAL] batch:  137 | acc: 50.00%,  total acc: 68.12%   [EVAL] batch:  138 | acc: 12.50%,  total acc: 67.72%   [EVAL] batch:  139 | acc: 31.25%,  total acc: 67.46%   [EVAL] batch:  140 | acc: 12.50%,  total acc: 67.07%   [EVAL] batch:  141 | acc: 43.75%,  total acc: 66.90%   [EVAL] batch:  142 | acc: 43.75%,  total acc: 66.74%   [EVAL] batch:  143 | acc: 37.50%,  total acc: 66.54%   [EVAL] batch:  144 | acc: 93.75%,  total acc: 66.72%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 66.95%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 67.13%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 67.36%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 67.58%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 67.75%   [EVAL] batch:  150 | acc: 12.50%,  total acc: 67.38%   [EVAL] batch:  151 | acc: 12.50%,  total acc: 67.02%   [EVAL] batch:  152 | acc: 18.75%,  total acc: 66.71%   [EVAL] batch:  153 | acc: 12.50%,  total acc: 66.36%   [EVAL] batch:  154 | acc: 18.75%,  total acc: 66.05%   [EVAL] batch:  155 | acc: 25.00%,  total acc: 65.79%   [EVAL] batch:  156 | acc: 75.00%,  total acc: 65.84%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 66.02%   [EVAL] batch:  158 | acc: 81.25%,  total acc: 66.12%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 66.29%   [EVAL] batch:  160 | acc: 87.50%,  total acc: 66.42%   [EVAL] batch:  161 | acc: 81.25%,  total acc: 66.51%   [EVAL] batch:  162 | acc: 68.75%,  total acc: 66.53%   [EVAL] batch:  163 | acc: 18.75%,  total acc: 66.23%   [EVAL] batch:  164 | acc: 56.25%,  total acc: 66.17%   [EVAL] batch:  165 | acc: 25.00%,  total acc: 65.93%   [EVAL] batch:  166 | acc: 31.25%,  total acc: 65.72%   [EVAL] batch:  167 | acc: 50.00%,  total acc: 65.62%   [EVAL] batch:  168 | acc: 62.50%,  total acc: 65.61%   [EVAL] batch:  169 | acc: 75.00%,  total acc: 65.66%   [EVAL] batch:  170 | acc: 87.50%,  total acc: 65.79%   [EVAL] batch:  171 | acc: 81.25%,  total acc: 65.88%   [EVAL] batch:  172 | acc: 68.75%,  total acc: 65.90%   [EVAL] batch:  173 | acc: 81.25%,  total acc: 65.98%   [EVAL] batch:  174 | acc: 93.75%,  total acc: 66.14%   [EVAL] batch:  175 | acc: 37.50%,  total acc: 65.98%   [EVAL] batch:  176 | acc: 25.00%,  total acc: 65.75%   [EVAL] batch:  177 | acc: 43.75%,  total acc: 65.62%   [EVAL] batch:  178 | acc: 31.25%,  total acc: 65.43%   [EVAL] batch:  179 | acc: 37.50%,  total acc: 65.28%   [EVAL] batch:  180 | acc: 50.00%,  total acc: 65.19%   [EVAL] batch:  181 | acc: 75.00%,  total acc: 65.25%   [EVAL] batch:  182 | acc: 87.50%,  total acc: 65.37%   [EVAL] batch:  183 | acc: 87.50%,  total acc: 65.49%   [EVAL] batch:  184 | acc: 81.25%,  total acc: 65.57%   [EVAL] batch:  185 | acc: 93.75%,  total acc: 65.73%   [EVAL] batch:  186 | acc: 75.00%,  total acc: 65.78%   [EVAL] batch:  187 | acc: 50.00%,  total acc: 65.69%   [EVAL] batch:  188 | acc: 6.25%,  total acc: 65.38%   [EVAL] batch:  189 | acc: 18.75%,  total acc: 65.13%   [EVAL] batch:  190 | acc: 0.00%,  total acc: 64.79%   [EVAL] batch:  191 | acc: 0.00%,  total acc: 64.45%   [EVAL] batch:  192 | acc: 6.25%,  total acc: 64.15%   [EVAL] batch:  193 | acc: 18.75%,  total acc: 63.92%   [EVAL] batch:  194 | acc: 81.25%,  total acc: 64.01%   [EVAL] batch:  195 | acc: 93.75%,  total acc: 64.16%   [EVAL] batch:  196 | acc: 81.25%,  total acc: 64.24%   [EVAL] batch:  197 | acc: 81.25%,  total acc: 64.33%   [EVAL] batch:  198 | acc: 87.50%,  total acc: 64.45%   [EVAL] batch:  199 | acc: 75.00%,  total acc: 64.50%   [EVAL] batch:  200 | acc: 68.75%,  total acc: 64.52%   [EVAL] batch:  201 | acc: 37.50%,  total acc: 64.39%   [EVAL] batch:  202 | acc: 50.00%,  total acc: 64.32%   [EVAL] batch:  203 | acc: 56.25%,  total acc: 64.28%   [EVAL] batch:  204 | acc: 75.00%,  total acc: 64.33%   [EVAL] batch:  205 | acc: 37.50%,  total acc: 64.20%   [EVAL] batch:  206 | acc: 87.50%,  total acc: 64.31%   [EVAL] batch:  207 | acc: 100.00%,  total acc: 64.48%   [EVAL] batch:  208 | acc: 100.00%,  total acc: 64.65%   [EVAL] batch:  209 | acc: 93.75%,  total acc: 64.79%   [EVAL] batch:  210 | acc: 93.75%,  total acc: 64.93%   [EVAL] batch:  211 | acc: 100.00%,  total acc: 65.09%   [EVAL] batch:  212 | acc: 87.50%,  total acc: 65.20%   [EVAL] batch:  213 | acc: 81.25%,  total acc: 65.27%   [EVAL] batch:  214 | acc: 75.00%,  total acc: 65.32%   [EVAL] batch:  215 | acc: 93.75%,  total acc: 65.45%   [EVAL] batch:  216 | acc: 68.75%,  total acc: 65.47%   [EVAL] batch:  217 | acc: 93.75%,  total acc: 65.60%   [EVAL] batch:  218 | acc: 81.25%,  total acc: 65.67%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 65.82%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 65.98%   [EVAL] batch:  221 | acc: 87.50%,  total acc: 66.08%   [EVAL] batch:  222 | acc: 87.50%,  total acc: 66.17%   [EVAL] batch:  223 | acc: 93.75%,  total acc: 66.29%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 66.44%   [EVAL] batch:  225 | acc: 75.00%,  total acc: 66.48%   [EVAL] batch:  226 | acc: 75.00%,  total acc: 66.52%   [EVAL] batch:  227 | acc: 68.75%,  total acc: 66.53%   [EVAL] batch:  228 | acc: 75.00%,  total acc: 66.57%   [EVAL] batch:  229 | acc: 56.25%,  total acc: 66.52%   [EVAL] batch:  230 | acc: 68.75%,  total acc: 66.53%   [EVAL] batch:  231 | acc: 87.50%,  total acc: 66.62%   [EVAL] batch:  232 | acc: 87.50%,  total acc: 66.71%   [EVAL] batch:  233 | acc: 81.25%,  total acc: 66.77%   [EVAL] batch:  234 | acc: 93.75%,  total acc: 66.89%   [EVAL] batch:  235 | acc: 68.75%,  total acc: 66.90%   [EVAL] batch:  236 | acc: 68.75%,  total acc: 66.90%   [EVAL] batch:  237 | acc: 56.25%,  total acc: 66.86%   [EVAL] batch:  238 | acc: 50.00%,  total acc: 66.79%   [EVAL] batch:  239 | acc: 37.50%,  total acc: 66.67%   [EVAL] batch:  240 | acc: 37.50%,  total acc: 66.55%   [EVAL] batch:  241 | acc: 37.50%,  total acc: 66.43%   [EVAL] batch:  242 | acc: 43.75%,  total acc: 66.33%   [EVAL] batch:  243 | acc: 18.75%,  total acc: 66.14%   [EVAL] batch:  244 | acc: 62.50%,  total acc: 66.12%   [EVAL] batch:  245 | acc: 68.75%,  total acc: 66.13%   [EVAL] batch:  246 | acc: 50.00%,  total acc: 66.07%   [EVAL] batch:  247 | acc: 68.75%,  total acc: 66.08%   [EVAL] batch:  248 | acc: 75.00%,  total acc: 66.11%   [EVAL] batch:  249 | acc: 43.75%,  total acc: 66.03%   [EVAL] batch:  250 | acc: 0.00%,  total acc: 65.76%   [EVAL] batch:  251 | acc: 25.00%,  total acc: 65.60%   [EVAL] batch:  252 | acc: 12.50%,  total acc: 65.39%   [EVAL] batch:  253 | acc: 6.25%,  total acc: 65.16%   [EVAL] batch:  254 | acc: 25.00%,  total acc: 65.00%   [EVAL] batch:  255 | acc: 31.25%,  total acc: 64.87%   [EVAL] batch:  256 | acc: 62.50%,  total acc: 64.86%   [EVAL] batch:  257 | acc: 68.75%,  total acc: 64.87%   [EVAL] batch:  258 | acc: 87.50%,  total acc: 64.96%   [EVAL] batch:  259 | acc: 75.00%,  total acc: 65.00%   [EVAL] batch:  260 | acc: 75.00%,  total acc: 65.04%   [EVAL] batch:  261 | acc: 81.25%,  total acc: 65.10%   [EVAL] batch:  262 | acc: 81.25%,  total acc: 65.16%   [EVAL] batch:  263 | acc: 56.25%,  total acc: 65.13%   [EVAL] batch:  264 | acc: 62.50%,  total acc: 65.12%   [EVAL] batch:  265 | acc: 81.25%,  total acc: 65.18%   [EVAL] batch:  266 | acc: 68.75%,  total acc: 65.19%   [EVAL] batch:  267 | acc: 68.75%,  total acc: 65.21%   [EVAL] batch:  268 | acc: 37.50%,  total acc: 65.10%   [EVAL] batch:  269 | acc: 25.00%,  total acc: 64.95%   [EVAL] batch:  270 | acc: 12.50%,  total acc: 64.76%   [EVAL] batch:  271 | acc: 12.50%,  total acc: 64.57%   [EVAL] batch:  272 | acc: 25.00%,  total acc: 64.42%   [EVAL] batch:  273 | acc: 25.00%,  total acc: 64.28%   [EVAL] batch:  274 | acc: 25.00%,  total acc: 64.14%   [EVAL] batch:  275 | acc: 6.25%,  total acc: 63.93%   [EVAL] batch:  276 | acc: 6.25%,  total acc: 63.72%   [EVAL] batch:  277 | acc: 18.75%,  total acc: 63.56%   [EVAL] batch:  278 | acc: 6.25%,  total acc: 63.35%   [EVAL] batch:  279 | acc: 18.75%,  total acc: 63.19%   [EVAL] batch:  280 | acc: 0.00%,  total acc: 62.97%   [EVAL] batch:  281 | acc: 68.75%,  total acc: 62.99%   [EVAL] batch:  282 | acc: 81.25%,  total acc: 63.05%   [EVAL] batch:  283 | acc: 81.25%,  total acc: 63.12%   [EVAL] batch:  284 | acc: 100.00%,  total acc: 63.25%   [EVAL] batch:  285 | acc: 87.50%,  total acc: 63.33%   [EVAL] batch:  286 | acc: 93.75%,  total acc: 63.44%   [EVAL] batch:  287 | acc: 93.75%,  total acc: 63.54%   [EVAL] batch:  288 | acc: 87.50%,  total acc: 63.62%   [EVAL] batch:  289 | acc: 100.00%,  total acc: 63.75%   [EVAL] batch:  290 | acc: 87.50%,  total acc: 63.83%   [EVAL] batch:  291 | acc: 87.50%,  total acc: 63.91%   [EVAL] batch:  292 | acc: 100.00%,  total acc: 64.04%   [EVAL] batch:  293 | acc: 93.75%,  total acc: 64.14%   [EVAL] batch:  294 | acc: 62.50%,  total acc: 64.13%   [EVAL] batch:  295 | acc: 43.75%,  total acc: 64.06%   [EVAL] batch:  296 | acc: 31.25%,  total acc: 63.95%   [EVAL] batch:  297 | acc: 43.75%,  total acc: 63.88%   [EVAL] batch:  298 | acc: 81.25%,  total acc: 63.94%   [EVAL] batch:  299 | acc: 37.50%,  total acc: 63.85%   [EVAL] batch:  300 | acc: 31.25%,  total acc: 63.75%   [EVAL] batch:  301 | acc: 62.50%,  total acc: 63.74%   [EVAL] batch:  302 | acc: 43.75%,  total acc: 63.68%   [EVAL] batch:  303 | acc: 56.25%,  total acc: 63.65%   [EVAL] batch:  304 | acc: 68.75%,  total acc: 63.67%   [EVAL] batch:  305 | acc: 68.75%,  total acc: 63.68%   [EVAL] batch:  306 | acc: 50.00%,  total acc: 63.64%   [EVAL] batch:  307 | acc: 43.75%,  total acc: 63.58%   [EVAL] batch:  308 | acc: 43.75%,  total acc: 63.51%   [EVAL] batch:  309 | acc: 75.00%,  total acc: 63.55%   [EVAL] batch:  310 | acc: 50.00%,  total acc: 63.50%   [EVAL] batch:  311 | acc: 81.25%,  total acc: 63.56%   [EVAL] batch:  312 | acc: 56.25%,  total acc: 63.54%   [EVAL] batch:  313 | acc: 31.25%,  total acc: 63.44%   [EVAL] batch:  314 | acc: 50.00%,  total acc: 63.39%   [EVAL] batch:  315 | acc: 37.50%,  total acc: 63.31%   [EVAL] batch:  316 | acc: 50.00%,  total acc: 63.27%   [EVAL] batch:  317 | acc: 62.50%,  total acc: 63.27%   [EVAL] batch:  318 | acc: 62.50%,  total acc: 63.26%   [EVAL] batch:  319 | acc: 100.00%,  total acc: 63.38%   [EVAL] batch:  320 | acc: 100.00%,  total acc: 63.49%   [EVAL] batch:  321 | acc: 81.25%,  total acc: 63.55%   [EVAL] batch:  322 | acc: 100.00%,  total acc: 63.66%   [EVAL] batch:  323 | acc: 93.75%,  total acc: 63.75%   [EVAL] batch:  324 | acc: 87.50%,  total acc: 63.83%   [EVAL] batch:  325 | acc: 75.00%,  total acc: 63.86%   [EVAL] batch:  326 | acc: 50.00%,  total acc: 63.82%   [EVAL] batch:  327 | acc: 62.50%,  total acc: 63.81%   [EVAL] batch:  328 | acc: 56.25%,  total acc: 63.79%   [EVAL] batch:  329 | acc: 68.75%,  total acc: 63.81%   [EVAL] batch:  330 | acc: 75.00%,  total acc: 63.84%   [EVAL] batch:  331 | acc: 50.00%,  total acc: 63.80%   [EVAL] batch:  332 | acc: 18.75%,  total acc: 63.66%   [EVAL] batch:  333 | acc: 25.00%,  total acc: 63.55%   [EVAL] batch:  334 | acc: 12.50%,  total acc: 63.40%   [EVAL] batch:  335 | acc: 18.75%,  total acc: 63.26%   [EVAL] batch:  336 | acc: 12.50%,  total acc: 63.11%   [EVAL] batch:  337 | acc: 56.25%,  total acc: 63.09%   [EVAL] batch:  338 | acc: 68.75%,  total acc: 63.11%   [EVAL] batch:  339 | acc: 56.25%,  total acc: 63.09%   [EVAL] batch:  340 | acc: 50.00%,  total acc: 63.05%   [EVAL] batch:  341 | acc: 56.25%,  total acc: 63.03%   [EVAL] batch:  342 | acc: 56.25%,  total acc: 63.01%   [EVAL] batch:  343 | acc: 31.25%,  total acc: 62.92%   [EVAL] batch:  344 | acc: 87.50%,  total acc: 62.99%   [EVAL] batch:  345 | acc: 50.00%,  total acc: 62.95%   [EVAL] batch:  346 | acc: 68.75%,  total acc: 62.97%   [EVAL] batch:  347 | acc: 68.75%,  total acc: 62.98%   [EVAL] batch:  348 | acc: 75.00%,  total acc: 63.02%   [EVAL] batch:  349 | acc: 50.00%,  total acc: 62.98%   [EVAL] batch:  350 | acc: 75.00%,  total acc: 63.02%   [EVAL] batch:  351 | acc: 75.00%,  total acc: 63.05%   [EVAL] batch:  352 | acc: 56.25%,  total acc: 63.03%   [EVAL] batch:  353 | acc: 87.50%,  total acc: 63.10%   [EVAL] batch:  354 | acc: 81.25%,  total acc: 63.15%   [EVAL] batch:  355 | acc: 75.00%,  total acc: 63.18%   [EVAL] batch:  356 | acc: 62.50%,  total acc: 63.18%   [EVAL] batch:  357 | acc: 31.25%,  total acc: 63.09%   [EVAL] batch:  358 | acc: 31.25%,  total acc: 63.00%   [EVAL] batch:  359 | acc: 50.00%,  total acc: 62.97%   [EVAL] batch:  360 | acc: 37.50%,  total acc: 62.90%   [EVAL] batch:  361 | acc: 50.00%,  total acc: 62.86%   [EVAL] batch:  362 | acc: 68.75%,  total acc: 62.88%   [EVAL] batch:  363 | acc: 100.00%,  total acc: 62.98%   [EVAL] batch:  364 | acc: 100.00%,  total acc: 63.08%   [EVAL] batch:  365 | acc: 100.00%,  total acc: 63.18%   [EVAL] batch:  366 | acc: 100.00%,  total acc: 63.28%   [EVAL] batch:  367 | acc: 100.00%,  total acc: 63.38%   [EVAL] batch:  368 | acc: 93.75%,  total acc: 63.47%   [EVAL] batch:  369 | acc: 93.75%,  total acc: 63.55%   [EVAL] batch:  370 | acc: 81.25%,  total acc: 63.60%   [EVAL] batch:  371 | acc: 93.75%,  total acc: 63.68%   [EVAL] batch:  372 | acc: 93.75%,  total acc: 63.76%   [EVAL] batch:  373 | acc: 87.50%,  total acc: 63.82%   [EVAL] batch:  374 | acc: 81.25%,  total acc: 63.87%   [EVAL] batch:  375 | acc: 56.25%,  total acc: 63.85%   [EVAL] batch:  376 | acc: 50.00%,  total acc: 63.81%   [EVAL] batch:  377 | acc: 68.75%,  total acc: 63.82%   [EVAL] batch:  378 | acc: 68.75%,  total acc: 63.84%   [EVAL] batch:  379 | acc: 62.50%,  total acc: 63.83%   [EVAL] batch:  380 | acc: 87.50%,  total acc: 63.89%   [EVAL] batch:  381 | acc: 43.75%,  total acc: 63.84%   [EVAL] batch:  382 | acc: 56.25%,  total acc: 63.82%   [EVAL] batch:  383 | acc: 68.75%,  total acc: 63.83%   [EVAL] batch:  384 | acc: 75.00%,  total acc: 63.86%   [EVAL] batch:  385 | acc: 56.25%,  total acc: 63.84%   [EVAL] batch:  386 | acc: 56.25%,  total acc: 63.82%   [EVAL] batch:  387 | acc: 93.75%,  total acc: 63.90%   [EVAL] batch:  388 | acc: 87.50%,  total acc: 63.96%   [EVAL] batch:  389 | acc: 75.00%,  total acc: 63.99%   [EVAL] batch:  390 | acc: 81.25%,  total acc: 64.03%   [EVAL] batch:  391 | acc: 87.50%,  total acc: 64.09%   [EVAL] batch:  392 | acc: 100.00%,  total acc: 64.19%   [EVAL] batch:  393 | acc: 100.00%,  total acc: 64.28%   [EVAL] batch:  394 | acc: 81.25%,  total acc: 64.32%   [EVAL] batch:  395 | acc: 100.00%,  total acc: 64.41%   [EVAL] batch:  396 | acc: 100.00%,  total acc: 64.50%   [EVAL] batch:  397 | acc: 93.75%,  total acc: 64.57%   [EVAL] batch:  398 | acc: 81.25%,  total acc: 64.61%   [EVAL] batch:  399 | acc: 93.75%,  total acc: 64.69%   [EVAL] batch:  400 | acc: 62.50%,  total acc: 64.68%   [EVAL] batch:  401 | acc: 50.00%,  total acc: 64.65%   [EVAL] batch:  402 | acc: 93.75%,  total acc: 64.72%   [EVAL] batch:  403 | acc: 81.25%,  total acc: 64.76%   [EVAL] batch:  404 | acc: 62.50%,  total acc: 64.75%   [EVAL] batch:  405 | acc: 81.25%,  total acc: 64.79%   [EVAL] batch:  406 | acc: 50.00%,  total acc: 64.76%   [EVAL] batch:  407 | acc: 50.00%,  total acc: 64.72%   [EVAL] batch:  408 | acc: 18.75%,  total acc: 64.61%   [EVAL] batch:  409 | acc: 31.25%,  total acc: 64.53%   [EVAL] batch:  410 | acc: 12.50%,  total acc: 64.40%   [EVAL] batch:  411 | acc: 18.75%,  total acc: 64.29%   [EVAL] batch:  412 | acc: 50.00%,  total acc: 64.26%   [EVAL] batch:  413 | acc: 56.25%,  total acc: 64.24%   [EVAL] batch:  414 | acc: 68.75%,  total acc: 64.25%   [EVAL] batch:  415 | acc: 43.75%,  total acc: 64.20%   [EVAL] batch:  416 | acc: 68.75%,  total acc: 64.21%   [EVAL] batch:  417 | acc: 56.25%,  total acc: 64.19%   [EVAL] batch:  418 | acc: 62.50%,  total acc: 64.19%   [EVAL] batch:  419 | acc: 81.25%,  total acc: 64.23%   [EVAL] batch:  420 | acc: 68.75%,  total acc: 64.24%   [EVAL] batch:  421 | acc: 56.25%,  total acc: 64.22%   [EVAL] batch:  422 | acc: 56.25%,  total acc: 64.20%   [EVAL] batch:  423 | acc: 68.75%,  total acc: 64.21%   [EVAL] batch:  424 | acc: 87.50%,  total acc: 64.26%   [EVAL] batch:  425 | acc: 100.00%,  total acc: 64.35%   [EVAL] batch:  426 | acc: 100.00%,  total acc: 64.43%   [EVAL] batch:  427 | acc: 100.00%,  total acc: 64.52%   [EVAL] batch:  428 | acc: 100.00%,  total acc: 64.60%   [EVAL] batch:  429 | acc: 100.00%,  total acc: 64.68%   [EVAL] batch:  430 | acc: 100.00%,  total acc: 64.76%   [EVAL] batch:  431 | acc: 93.75%,  total acc: 64.83%   [EVAL] batch:  432 | acc: 87.50%,  total acc: 64.88%   [EVAL] batch:  433 | acc: 87.50%,  total acc: 64.93%   [EVAL] batch:  434 | acc: 93.75%,  total acc: 65.00%   [EVAL] batch:  435 | acc: 87.50%,  total acc: 65.05%   [EVAL] batch:  436 | acc: 100.00%,  total acc: 65.13%   [EVAL] batch:  437 | acc: 93.75%,  total acc: 65.20%   [EVAL] batch:  438 | acc: 93.75%,  total acc: 65.26%   [EVAL] batch:  439 | acc: 93.75%,  total acc: 65.33%   [EVAL] batch:  440 | acc: 100.00%,  total acc: 65.41%   [EVAL] batch:  441 | acc: 81.25%,  total acc: 65.44%   [EVAL] batch:  442 | acc: 100.00%,  total acc: 65.52%   [EVAL] batch:  443 | acc: 100.00%,  total acc: 65.60%   [EVAL] batch:  444 | acc: 87.50%,  total acc: 65.65%   [EVAL] batch:  445 | acc: 93.75%,  total acc: 65.71%   [EVAL] batch:  446 | acc: 81.25%,  total acc: 65.74%   [EVAL] batch:  447 | acc: 81.25%,  total acc: 65.78%   [EVAL] batch:  448 | acc: 81.25%,  total acc: 65.81%   [EVAL] batch:  449 | acc: 75.00%,  total acc: 65.83%   [EVAL] batch:  450 | acc: 56.25%,  total acc: 65.81%   [EVAL] batch:  451 | acc: 75.00%,  total acc: 65.83%   [EVAL] batch:  452 | acc: 62.50%,  total acc: 65.83%   [EVAL] batch:  453 | acc: 43.75%,  total acc: 65.78%   [EVAL] batch:  454 | acc: 50.00%,  total acc: 65.74%   [EVAL] batch:  455 | acc: 50.00%,  total acc: 65.71%   [EVAL] batch:  456 | acc: 50.00%,  total acc: 65.67%   [EVAL] batch:  457 | acc: 43.75%,  total acc: 65.62%   [EVAL] batch:  458 | acc: 31.25%,  total acc: 65.55%   [EVAL] batch:  459 | acc: 37.50%,  total acc: 65.49%   [EVAL] batch:  460 | acc: 43.75%,  total acc: 65.44%   [EVAL] batch:  461 | acc: 25.00%,  total acc: 65.35%   [EVAL] batch:  462 | acc: 68.75%,  total acc: 65.36%   [EVAL] batch:  463 | acc: 100.00%,  total acc: 65.44%   [EVAL] batch:  464 | acc: 100.00%,  total acc: 65.51%   [EVAL] batch:  465 | acc: 100.00%,  total acc: 65.58%   [EVAL] batch:  466 | acc: 100.00%,  total acc: 65.66%   [EVAL] batch:  467 | acc: 100.00%,  total acc: 65.73%   [EVAL] batch:  468 | acc: 93.75%,  total acc: 65.79%   [EVAL] batch:  469 | acc: 62.50%,  total acc: 65.78%   [EVAL] batch:  470 | acc: 75.00%,  total acc: 65.80%   [EVAL] batch:  471 | acc: 68.75%,  total acc: 65.81%   [EVAL] batch:  472 | acc: 56.25%,  total acc: 65.79%   [EVAL] batch:  473 | acc: 75.00%,  total acc: 65.81%   [EVAL] batch:  474 | acc: 37.50%,  total acc: 65.75%   [EVAL] batch:  475 | acc: 100.00%,  total acc: 65.82%   [EVAL] batch:  476 | acc: 100.00%,  total acc: 65.89%   [EVAL] batch:  477 | acc: 100.00%,  total acc: 65.96%   [EVAL] batch:  478 | acc: 100.00%,  total acc: 66.04%   [EVAL] batch:  479 | acc: 100.00%,  total acc: 66.11%   [EVAL] batch:  480 | acc: 93.75%,  total acc: 66.16%   [EVAL] batch:  481 | acc: 100.00%,  total acc: 66.23%   [EVAL] batch:  482 | acc: 100.00%,  total acc: 66.30%   [EVAL] batch:  483 | acc: 100.00%,  total acc: 66.37%   [EVAL] batch:  484 | acc: 100.00%,  total acc: 66.44%   [EVAL] batch:  485 | acc: 100.00%,  total acc: 66.51%   [EVAL] batch:  486 | acc: 100.00%,  total acc: 66.58%   [EVAL] batch:  487 | acc: 87.50%,  total acc: 66.62%   [EVAL] batch:  488 | acc: 87.50%,  total acc: 66.67%   [EVAL] batch:  489 | acc: 93.75%,  total acc: 66.72%   [EVAL] batch:  490 | acc: 100.00%,  total acc: 66.79%   [EVAL] batch:  491 | acc: 87.50%,  total acc: 66.83%   [EVAL] batch:  492 | acc: 93.75%,  total acc: 66.89%   [EVAL] batch:  493 | acc: 87.50%,  total acc: 66.93%   [EVAL] batch:  494 | acc: 100.00%,  total acc: 66.99%   [EVAL] batch:  495 | acc: 87.50%,  total acc: 67.04%   [EVAL] batch:  496 | acc: 93.75%,  total acc: 67.09%   [EVAL] batch:  497 | acc: 100.00%,  total acc: 67.16%   [EVAL] batch:  498 | acc: 93.75%,  total acc: 67.21%   [EVAL] batch:  499 | acc: 93.75%,  total acc: 67.26%   
cur_acc:  ['0.9524', '0.8264', '0.7470', '0.7520', '0.6587', '0.6994', '0.7391', '0.8125']
his_acc:  ['0.9524', '0.8815', '0.8039', '0.7535', '0.7133', '0.6808', '0.6715', '0.6726']
----------END
his_acc mean:  [0.9501 0.8712 0.8056 0.7622 0.7429 0.7099 0.6815 0.6609]
