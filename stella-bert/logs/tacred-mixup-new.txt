#############params############
cuda:0
Task=Tacred, 5-shot
Encoding model: bert
pattern=hybridprompt
mem=1, margin=0.3, gen=1, gen_num=2
#############params############
--------Round  0
seed:  100
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/test.pkl
Task_order: [7 3 0 5 4 1 6 2]
prepared data!
CurrentTrain: epoch  0, batch     0 | loss: 13.3724613CurrentTrain: epoch  0, batch     1 | loss: 13.1578608CurrentTrain: epoch  0, batch     2 | loss: 13.0914288CurrentTrain: epoch  0, batch     3 | loss: 12.9665594CurrentTrain: epoch  0, batch     4 | loss: 12.8442793CurrentTrain: epoch  0, batch     5 | loss: 12.5822926CurrentTrain: epoch  0, batch     6 | loss: 12.5752811CurrentTrain: epoch  0, batch     7 | loss: 12.3465538CurrentTrain: epoch  0, batch     8 | loss: 12.2965069CurrentTrain: epoch  0, batch     9 | loss: 12.1475677CurrentTrain: epoch  0, batch    10 | loss: 12.0400219CurrentTrain: epoch  0, batch    11 | loss: 11.9480858CurrentTrain: epoch  0, batch    12 | loss: 12.0638752CurrentTrain: epoch  0, batch    13 | loss: 11.8167038CurrentTrain: epoch  0, batch    14 | loss: 11.6889286CurrentTrain: epoch  0, batch    15 | loss: 11.6254883CurrentTrain: epoch  0, batch    16 | loss: 11.0940533CurrentTrain: epoch  0, batch    17 | loss: 11.2020321CurrentTrain: epoch  0, batch    18 | loss: 11.2489090CurrentTrain: epoch  0, batch    19 | loss: 11.4948883CurrentTrain: epoch  0, batch    20 | loss: 11.0849552CurrentTrain: epoch  0, batch    21 | loss: 11.3767891CurrentTrain: epoch  0, batch    22 | loss: 11.4738388CurrentTrain: epoch  0, batch    23 | loss: 11.0327606CurrentTrain: epoch  0, batch    24 | loss: 11.3491087CurrentTrain: epoch  0, batch    25 | loss: 11.3099518CurrentTrain: epoch  0, batch    26 | loss: 10.8578224CurrentTrain: epoch  0, batch    27 | loss: 10.2841034CurrentTrain: epoch  0, batch    28 | loss: 10.7051201CurrentTrain: epoch  0, batch    29 | loss: 10.7940836CurrentTrain: epoch  0, batch    30 | loss: 10.4195404CurrentTrain: epoch  0, batch    31 | loss: 10.7609634CurrentTrain: epoch  0, batch    32 | loss: 10.3042355CurrentTrain: epoch  0, batch    33 | loss: 10.3129387CurrentTrain: epoch  0, batch    34 | loss: 10.0250797CurrentTrain: epoch  0, batch    35 | loss: 10.2144766CurrentTrain: epoch  0, batch    36 | loss: 10.2306309CurrentTrain: epoch  0, batch    37 | loss: 10.0852337CurrentTrain: epoch  1, batch     0 | loss: 10.2014656CurrentTrain: epoch  1, batch     1 | loss: 10.6530342CurrentTrain: epoch  1, batch     2 | loss: 9.7890358CurrentTrain: epoch  1, batch     3 | loss: 9.7257767CurrentTrain: epoch  1, batch     4 | loss: 9.4564104CurrentTrain: epoch  1, batch     5 | loss: 10.1626291CurrentTrain: epoch  1, batch     6 | loss: 9.8237648CurrentTrain: epoch  1, batch     7 | loss: 9.4988441CurrentTrain: epoch  1, batch     8 | loss: 9.4991245CurrentTrain: epoch  1, batch     9 | loss: 9.6452885CurrentTrain: epoch  1, batch    10 | loss: 9.9743004CurrentTrain: epoch  1, batch    11 | loss: 9.7963219CurrentTrain: epoch  1, batch    12 | loss: 9.6937313CurrentTrain: epoch  1, batch    13 | loss: 9.1966829CurrentTrain: epoch  1, batch    14 | loss: 9.5502548CurrentTrain: epoch  1, batch    15 | loss: 9.2058496CurrentTrain: epoch  1, batch    16 | loss: 9.1888971CurrentTrain: epoch  1, batch    17 | loss: 9.5938950CurrentTrain: epoch  1, batch    18 | loss: 9.4860392CurrentTrain: epoch  1, batch    19 | loss: 9.5833225CurrentTrain: epoch  1, batch    20 | loss: 9.5087862CurrentTrain: epoch  1, batch    21 | loss: 9.1296339CurrentTrain: epoch  1, batch    22 | loss: 9.2063999CurrentTrain: epoch  1, batch    23 | loss: 9.0002327CurrentTrain: epoch  1, batch    24 | loss: 9.2855091CurrentTrain: epoch  1, batch    25 | loss: 8.4403152CurrentTrain: epoch  1, batch    26 | loss: 9.3554955CurrentTrain: epoch  1, batch    27 | loss: 8.4032183CurrentTrain: epoch  1, batch    28 | loss: 8.6823807CurrentTrain: epoch  1, batch    29 | loss: 8.0479479CurrentTrain: epoch  1, batch    30 | loss: 8.6466703CurrentTrain: epoch  1, batch    31 | loss: 9.0862045CurrentTrain: epoch  1, batch    32 | loss: 8.8169594CurrentTrain: epoch  1, batch    33 | loss: 8.2732611CurrentTrain: epoch  1, batch    34 | loss: 8.6010847CurrentTrain: epoch  1, batch    35 | loss: 8.1362267CurrentTrain: epoch  1, batch    36 | loss: 8.8653030CurrentTrain: epoch  1, batch    37 | loss: 8.5752344CurrentTrain: epoch  2, batch     0 | loss: 7.7833271CurrentTrain: epoch  2, batch     1 | loss: 8.4801159CurrentTrain: epoch  2, batch     2 | loss: 8.7112026CurrentTrain: epoch  2, batch     3 | loss: 8.3481588CurrentTrain: epoch  2, batch     4 | loss: 9.3701601CurrentTrain: epoch  2, batch     5 | loss: 9.5443459CurrentTrain: epoch  2, batch     6 | loss: 8.6426182CurrentTrain: epoch  2, batch     7 | loss: 8.3060379CurrentTrain: epoch  2, batch     8 | loss: 8.6485052CurrentTrain: epoch  2, batch     9 | loss: 8.4162703CurrentTrain: epoch  2, batch    10 | loss: 8.0237541CurrentTrain: epoch  2, batch    11 | loss: 8.5527153CurrentTrain: epoch  2, batch    12 | loss: 8.2105560CurrentTrain: epoch  2, batch    13 | loss: 7.9770279CurrentTrain: epoch  2, batch    14 | loss: 7.3642979CurrentTrain: epoch  2, batch    15 | loss: 7.9607420CurrentTrain: epoch  2, batch    16 | loss: 8.3288574CurrentTrain: epoch  2, batch    17 | loss: 8.4686508CurrentTrain: epoch  2, batch    18 | loss: 8.0185461CurrentTrain: epoch  2, batch    19 | loss: 7.8661394CurrentTrain: epoch  2, batch    20 | loss: 7.6271992CurrentTrain: epoch  2, batch    21 | loss: 7.9648695CurrentTrain: epoch  2, batch    22 | loss: 7.4678106CurrentTrain: epoch  2, batch    23 | loss: 7.4899635CurrentTrain: epoch  2, batch    24 | loss: 7.8132114CurrentTrain: epoch  2, batch    25 | loss: 8.1924648CurrentTrain: epoch  2, batch    26 | loss: 7.2895327CurrentTrain: epoch  2, batch    27 | loss: 8.5118685CurrentTrain: epoch  2, batch    28 | loss: 6.8255653CurrentTrain: epoch  2, batch    29 | loss: 7.8991265CurrentTrain: epoch  2, batch    30 | loss: 7.5748453CurrentTrain: epoch  2, batch    31 | loss: 7.1347771CurrentTrain: epoch  2, batch    32 | loss: 7.6812344CurrentTrain: epoch  2, batch    33 | loss: 7.5430918CurrentTrain: epoch  2, batch    34 | loss: 8.4325619CurrentTrain: epoch  2, batch    35 | loss: 7.4143143CurrentTrain: epoch  2, batch    36 | loss: 7.8018699CurrentTrain: epoch  2, batch    37 | loss: 7.7435017CurrentTrain: epoch  3, batch     0 | loss: 7.5232515CurrentTrain: epoch  3, batch     1 | loss: 7.8361187CurrentTrain: epoch  3, batch     2 | loss: 7.8839965CurrentTrain: epoch  3, batch     3 | loss: 7.9287844CurrentTrain: epoch  3, batch     4 | loss: 7.4654551CurrentTrain: epoch  3, batch     5 | loss: 7.8325772CurrentTrain: epoch  3, batch     6 | loss: 8.4771061CurrentTrain: epoch  3, batch     7 | loss: 6.9571447CurrentTrain: epoch  3, batch     8 | loss: 8.0827675CurrentTrain: epoch  3, batch     9 | loss: 7.7508407CurrentTrain: epoch  3, batch    10 | loss: 7.0086536CurrentTrain: epoch  3, batch    11 | loss: 6.6070518CurrentTrain: epoch  3, batch    12 | loss: 7.7624445CurrentTrain: epoch  3, batch    13 | loss: 8.1287346CurrentTrain: epoch  3, batch    14 | loss: 7.2786741CurrentTrain: epoch  3, batch    15 | loss: 7.4686642CurrentTrain: epoch  3, batch    16 | loss: 8.3591795CurrentTrain: epoch  3, batch    17 | loss: 7.4006367CurrentTrain: epoch  3, batch    18 | loss: 7.4785576CurrentTrain: epoch  3, batch    19 | loss: 7.9789510CurrentTrain: epoch  3, batch    20 | loss: 7.5735807CurrentTrain: epoch  3, batch    21 | loss: 7.4664931CurrentTrain: epoch  3, batch    22 | loss: 7.6469474CurrentTrain: epoch  3, batch    23 | loss: 7.7438240CurrentTrain: epoch  3, batch    24 | loss: 6.4601979CurrentTrain: epoch  3, batch    25 | loss: 6.9148345CurrentTrain: epoch  3, batch    26 | loss: 6.8151793CurrentTrain: epoch  3, batch    27 | loss: 7.9257421CurrentTrain: epoch  3, batch    28 | loss: 7.3877330CurrentTrain: epoch  3, batch    29 | loss: 6.2554440CurrentTrain: epoch  3, batch    30 | loss: 7.2944155CurrentTrain: epoch  3, batch    31 | loss: 6.9774604CurrentTrain: epoch  3, batch    32 | loss: 6.2400208CurrentTrain: epoch  3, batch    33 | loss: 6.4645548CurrentTrain: epoch  3, batch    34 | loss: 6.7722120CurrentTrain: epoch  3, batch    35 | loss: 6.4985266CurrentTrain: epoch  3, batch    36 | loss: 6.8035321CurrentTrain: epoch  3, batch    37 | loss: 6.8702993CurrentTrain: epoch  4, batch     0 | loss: 7.0334969CurrentTrain: epoch  4, batch     1 | loss: 7.0049124CurrentTrain: epoch  4, batch     2 | loss: 5.8247557CurrentTrain: epoch  4, batch     3 | loss: 6.8830619CurrentTrain: epoch  4, batch     4 | loss: 6.9699078CurrentTrain: epoch  4, batch     5 | loss: 6.9527235CurrentTrain: epoch  4, batch     6 | loss: 6.2295132CurrentTrain: epoch  4, batch     7 | loss: 6.9355526CurrentTrain: epoch  4, batch     8 | loss: 7.7385564CurrentTrain: epoch  4, batch     9 | loss: 7.0533419CurrentTrain: epoch  4, batch    10 | loss: 7.2315435CurrentTrain: epoch  4, batch    11 | loss: 6.2178564CurrentTrain: epoch  4, batch    12 | loss: 6.9987230CurrentTrain: epoch  4, batch    13 | loss: 6.5305729CurrentTrain: epoch  4, batch    14 | loss: 6.8219061CurrentTrain: epoch  4, batch    15 | loss: 7.0248032CurrentTrain: epoch  4, batch    16 | loss: 6.7223358CurrentTrain: epoch  4, batch    17 | loss: 6.6297302CurrentTrain: epoch  4, batch    18 | loss: 6.2468553CurrentTrain: epoch  4, batch    19 | loss: 6.3774834CurrentTrain: epoch  4, batch    20 | loss: 6.7463589CurrentTrain: epoch  4, batch    21 | loss: 7.4178104CurrentTrain: epoch  4, batch    22 | loss: 6.9252653CurrentTrain: epoch  4, batch    23 | loss: 6.1513681CurrentTrain: epoch  4, batch    24 | loss: 7.1817818CurrentTrain: epoch  4, batch    25 | loss: 7.3936548CurrentTrain: epoch  4, batch    26 | loss: 6.3802161CurrentTrain: epoch  4, batch    27 | loss: 8.6171103CurrentTrain: epoch  4, batch    28 | loss: 6.6908755CurrentTrain: epoch  4, batch    29 | loss: 6.6532860CurrentTrain: epoch  4, batch    30 | loss: 6.7164712CurrentTrain: epoch  4, batch    31 | loss: 6.5105209CurrentTrain: epoch  4, batch    32 | loss: 7.6631536CurrentTrain: epoch  4, batch    33 | loss: 6.2924786CurrentTrain: epoch  4, batch    34 | loss: 7.8369579CurrentTrain: epoch  4, batch    35 | loss: 7.2543831CurrentTrain: epoch  4, batch    36 | loss: 6.5398402CurrentTrain: epoch  4, batch    37 | loss: 7.5059304CurrentTrain: epoch  5, batch     0 | loss: 6.1030679CurrentTrain: epoch  5, batch     1 | loss: 6.8386374CurrentTrain: epoch  5, batch     2 | loss: 7.0808506CurrentTrain: epoch  5, batch     3 | loss: 7.1512556CurrentTrain: epoch  5, batch     4 | loss: 6.9177895CurrentTrain: epoch  5, batch     5 | loss: 6.7688484CurrentTrain: epoch  5, batch     6 | loss: 6.7772846CurrentTrain: epoch  5, batch     7 | loss: 6.8151073CurrentTrain: epoch  5, batch     8 | loss: 6.7718267CurrentTrain: epoch  5, batch     9 | loss: 6.6284990CurrentTrain: epoch  5, batch    10 | loss: 6.2529025CurrentTrain: epoch  5, batch    11 | loss: 6.5486612CurrentTrain: epoch  5, batch    12 | loss: 6.1802988CurrentTrain: epoch  5, batch    13 | loss: 6.8904743CurrentTrain: epoch  5, batch    14 | loss: 6.4440236CurrentTrain: epoch  5, batch    15 | loss: 6.5432043CurrentTrain: epoch  5, batch    16 | loss: 5.7127781CurrentTrain: epoch  5, batch    17 | loss: 5.8208084CurrentTrain: epoch  5, batch    18 | loss: 7.2228193CurrentTrain: epoch  5, batch    19 | loss: 6.8538017CurrentTrain: epoch  5, batch    20 | loss: 6.0343790CurrentTrain: epoch  5, batch    21 | loss: 6.0485401CurrentTrain: epoch  5, batch    22 | loss: 6.1135545CurrentTrain: epoch  5, batch    23 | loss: 5.8876462CurrentTrain: epoch  5, batch    24 | loss: 6.7454958CurrentTrain: epoch  5, batch    25 | loss: 5.8460865CurrentTrain: epoch  5, batch    26 | loss: 5.9473586CurrentTrain: epoch  5, batch    27 | loss: 6.7466745CurrentTrain: epoch  5, batch    28 | loss: 8.3776693CurrentTrain: epoch  5, batch    29 | loss: 6.3754187CurrentTrain: epoch  5, batch    30 | loss: 6.4071417CurrentTrain: epoch  5, batch    31 | loss: 6.0694742CurrentTrain: epoch  5, batch    32 | loss: 5.5056829CurrentTrain: epoch  5, batch    33 | loss: 6.3093948CurrentTrain: epoch  5, batch    34 | loss: 6.6475391CurrentTrain: epoch  5, batch    35 | loss: 5.9693222CurrentTrain: epoch  5, batch    36 | loss: 6.5377998CurrentTrain: epoch  5, batch    37 | loss: 6.0111084CurrentTrain: epoch  6, batch     0 | loss: 6.0047464CurrentTrain: epoch  6, batch     1 | loss: 6.6310277CurrentTrain: epoch  6, batch     2 | loss: 6.6020212CurrentTrain: epoch  6, batch     3 | loss: 6.2475824CurrentTrain: epoch  6, batch     4 | loss: 6.0720997CurrentTrain: epoch  6, batch     5 | loss: 6.0006638CurrentTrain: epoch  6, batch     6 | loss: 6.3973942CurrentTrain: epoch  6, batch     7 | loss: 5.9961662CurrentTrain: epoch  6, batch     8 | loss: 5.7649665CurrentTrain: epoch  6, batch     9 | loss: 6.0096731CurrentTrain: epoch  6, batch    10 | loss: 5.9515557CurrentTrain: epoch  6, batch    11 | loss: 5.9529152CurrentTrain: epoch  6, batch    12 | loss: 5.9199295CurrentTrain: epoch  6, batch    13 | loss: 5.6818867CurrentTrain: epoch  6, batch    14 | loss: 5.8865380CurrentTrain: epoch  6, batch    15 | loss: 5.3489008CurrentTrain: epoch  6, batch    16 | loss: 6.4938850CurrentTrain: epoch  6, batch    17 | loss: 5.9322629CurrentTrain: epoch  6, batch    18 | loss: 6.0616775CurrentTrain: epoch  6, batch    19 | loss: 5.8435102CurrentTrain: epoch  6, batch    20 | loss: 6.5467153CurrentTrain: epoch  6, batch    21 | loss: 6.8287182CurrentTrain: epoch  6, batch    22 | loss: 5.8638916CurrentTrain: epoch  6, batch    23 | loss: 5.8143373CurrentTrain: epoch  6, batch    24 | loss: 5.5679169CurrentTrain: epoch  6, batch    25 | loss: 6.4695663CurrentTrain: epoch  6, batch    26 | loss: 6.4372053CurrentTrain: epoch  6, batch    27 | loss: 5.6673498CurrentTrain: epoch  6, batch    28 | loss: 5.9043579CurrentTrain: epoch  6, batch    29 | loss: 6.1868839CurrentTrain: epoch  6, batch    30 | loss: 6.6498880CurrentTrain: epoch  6, batch    31 | loss: 6.2947693CurrentTrain: epoch  6, batch    32 | loss: 5.6921754CurrentTrain: epoch  6, batch    33 | loss: 6.2850089CurrentTrain: epoch  6, batch    34 | loss: 5.9845924CurrentTrain: epoch  6, batch    35 | loss: 6.4355583CurrentTrain: epoch  6, batch    36 | loss: 6.2763124CurrentTrain: epoch  6, batch    37 | loss: 5.9351416CurrentTrain: epoch  7, batch     0 | loss: 6.6529021CurrentTrain: epoch  7, batch     1 | loss: 5.8108978CurrentTrain: epoch  7, batch     2 | loss: 5.5417938CurrentTrain: epoch  7, batch     3 | loss: 5.5101213CurrentTrain: epoch  7, batch     4 | loss: 6.3973951CurrentTrain: epoch  7, batch     5 | loss: 5.6384888CurrentTrain: epoch  7, batch     6 | loss: 5.6699238CurrentTrain: epoch  7, batch     7 | loss: 5.6597505CurrentTrain: epoch  7, batch     8 | loss: 5.6804357CurrentTrain: epoch  7, batch     9 | loss: 5.5073752CurrentTrain: epoch  7, batch    10 | loss: 5.9107246CurrentTrain: epoch  7, batch    11 | loss: 5.8943958CurrentTrain: epoch  7, batch    12 | loss: 5.7086897CurrentTrain: epoch  7, batch    13 | loss: 5.8933516CurrentTrain: epoch  7, batch    14 | loss: 5.8622904CurrentTrain: epoch  7, batch    15 | loss: 6.0675993CurrentTrain: epoch  7, batch    16 | loss: 5.7433643CurrentTrain: epoch  7, batch    17 | loss: 5.4421778CurrentTrain: epoch  7, batch    18 | loss: 5.6944370CurrentTrain: epoch  7, batch    19 | loss: 5.5842218CurrentTrain: epoch  7, batch    20 | loss: 5.5729561CurrentTrain: epoch  7, batch    21 | loss: 5.3309689CurrentTrain: epoch  7, batch    22 | loss: 6.9911366CurrentTrain: epoch  7, batch    23 | loss: 6.2403383CurrentTrain: epoch  7, batch    24 | loss: 6.1066108CurrentTrain: epoch  7, batch    25 | loss: 5.3430290CurrentTrain: epoch  7, batch    26 | loss: 5.5711193CurrentTrain: epoch  7, batch    27 | loss: 5.4540119CurrentTrain: epoch  7, batch    28 | loss: 5.4332466CurrentTrain: epoch  7, batch    29 | loss: 5.3174152CurrentTrain: epoch  7, batch    30 | loss: 5.4019990CurrentTrain: epoch  7, batch    31 | loss: 5.3367624CurrentTrain: epoch  7, batch    32 | loss: 5.6065183CurrentTrain: epoch  7, batch    33 | loss: 5.8639040CurrentTrain: epoch  7, batch    34 | loss: 5.5942111CurrentTrain: epoch  7, batch    35 | loss: 5.7962923CurrentTrain: epoch  7, batch    36 | loss: 5.5112915CurrentTrain: epoch  7, batch    37 | loss: 4.9952893CurrentTrain: epoch  8, batch     0 | loss: 5.4854412CurrentTrain: epoch  8, batch     1 | loss: 5.6333909CurrentTrain: epoch  8, batch     2 | loss: 5.1421604CurrentTrain: epoch  8, batch     3 | loss: 5.3651385CurrentTrain: epoch  8, batch     4 | loss: 5.3226509CurrentTrain: epoch  8, batch     5 | loss: 5.3203096CurrentTrain: epoch  8, batch     6 | loss: 5.3517303CurrentTrain: epoch  8, batch     7 | loss: 5.4473085CurrentTrain: epoch  8, batch     8 | loss: 5.3135548CurrentTrain: epoch  8, batch     9 | loss: 5.2105174CurrentTrain: epoch  8, batch    10 | loss: 5.6048307CurrentTrain: epoch  8, batch    11 | loss: 5.3615904CurrentTrain: epoch  8, batch    12 | loss: 5.7865286CurrentTrain: epoch  8, batch    13 | loss: 5.3026810CurrentTrain: epoch  8, batch    14 | loss: 5.6099992CurrentTrain: epoch  8, batch    15 | loss: 5.3994226CurrentTrain: epoch  8, batch    16 | loss: 5.7551336CurrentTrain: epoch  8, batch    17 | loss: 5.0666037CurrentTrain: epoch  8, batch    18 | loss: 5.2056007CurrentTrain: epoch  8, batch    19 | loss: 5.7251682CurrentTrain: epoch  8, batch    20 | loss: 5.5557652CurrentTrain: epoch  8, batch    21 | loss: 5.4007759CurrentTrain: epoch  8, batch    22 | loss: 5.9072752CurrentTrain: epoch  8, batch    23 | loss: 5.6366529CurrentTrain: epoch  8, batch    24 | loss: 5.2677555CurrentTrain: epoch  8, batch    25 | loss: 5.3671660CurrentTrain: epoch  8, batch    26 | loss: 6.1590004CurrentTrain: epoch  8, batch    27 | loss: 5.1991730CurrentTrain: epoch  8, batch    28 | loss: 5.2172470CurrentTrain: epoch  8, batch    29 | loss: 5.6280775CurrentTrain: epoch  8, batch    30 | loss: 5.2850456CurrentTrain: epoch  8, batch    31 | loss: 5.3826280CurrentTrain: epoch  8, batch    32 | loss: 4.9245048CurrentTrain: epoch  8, batch    33 | loss: 5.1530337CurrentTrain: epoch  8, batch    34 | loss: 5.0754180CurrentTrain: epoch  8, batch    35 | loss: 5.2098951CurrentTrain: epoch  8, batch    36 | loss: 5.0569038CurrentTrain: epoch  8, batch    37 | loss: 5.1459870CurrentTrain: epoch  9, batch     0 | loss: 5.3714628CurrentTrain: epoch  9, batch     1 | loss: 5.2017689CurrentTrain: epoch  9, batch     2 | loss: 5.2213621CurrentTrain: epoch  9, batch     3 | loss: 5.0896473CurrentTrain: epoch  9, batch     4 | loss: 5.1074209CurrentTrain: epoch  9, batch     5 | loss: 5.1780319CurrentTrain: epoch  9, batch     6 | loss: 5.4686308CurrentTrain: epoch  9, batch     7 | loss: 5.1841688CurrentTrain: epoch  9, batch     8 | loss: 5.1631765CurrentTrain: epoch  9, batch     9 | loss: 5.0416660CurrentTrain: epoch  9, batch    10 | loss: 5.0971518CurrentTrain: epoch  9, batch    11 | loss: 5.0610533CurrentTrain: epoch  9, batch    12 | loss: 5.1421871CurrentTrain: epoch  9, batch    13 | loss: 5.2279015CurrentTrain: epoch  9, batch    14 | loss: 5.3229456CurrentTrain: epoch  9, batch    15 | loss: 5.0906010CurrentTrain: epoch  9, batch    16 | loss: 5.0985069CurrentTrain: epoch  9, batch    17 | loss: 5.8242164CurrentTrain: epoch  9, batch    18 | loss: 5.5273147CurrentTrain: epoch  9, batch    19 | loss: 5.1797643CurrentTrain: epoch  9, batch    20 | loss: 5.3398924CurrentTrain: epoch  9, batch    21 | loss: 4.9716997CurrentTrain: epoch  9, batch    22 | loss: 5.1110754CurrentTrain: epoch  9, batch    23 | loss: 5.2672491CurrentTrain: epoch  9, batch    24 | loss: 5.1435614CurrentTrain: epoch  9, batch    25 | loss: 5.7747397CurrentTrain: epoch  9, batch    26 | loss: 5.6257625CurrentTrain: epoch  9, batch    27 | loss: 5.3424568CurrentTrain: epoch  9, batch    28 | loss: 5.0072770CurrentTrain: epoch  9, batch    29 | loss: 5.5512266CurrentTrain: epoch  9, batch    30 | loss: 4.9825134CurrentTrain: epoch  9, batch    31 | loss: 5.0399342CurrentTrain: epoch  9, batch    32 | loss: 5.1282659CurrentTrain: epoch  9, batch    33 | loss: 5.3177009CurrentTrain: epoch  9, batch    34 | loss: 5.1026111CurrentTrain: epoch  9, batch    35 | loss: 4.9330521CurrentTrain: epoch  9, batch    36 | loss: 4.9632597CurrentTrain: epoch  9, batch    37 | loss: 5.1368017
#### person countries of residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person countries of residence" means a person is living in a country.
Here is an example:
Relation: person countries of residence
Context: khamenei , 67 , has final say on all state matters in iran as supreme leader , a post he has held since 1989 .
Head Entity: khamenei
Tail Entity: iran
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person countries of residence":
Relation: person countries of residence  
Context: After years of living in the United States, Maria decided to return to her homeland, spain, where she felt a deep connection to her roots.  
Head Entity: Maria  
Tail Entity: spain  

Relation: person countries of residence  
Context: Following his successful career in technology, Raj moved to canada to enjoy a quieter life surrounded by nature.  
Head Entity: Raj  
Tail Entity: canada  
#### organization top members employees ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization top members employees" means an organization 's employer is a person.
Here is an example:
Relation: organization top members employees
Context: kao charng , vice chairman of the cabinet-level mainland affairs council -lrb- mac -rrb- , explained that the termination clause was added to give both sides an option after the implementation of the tariff-cutting economic cooperation framework agreement -lrb- ecfa -rrb- .
Head Entity: mainland affairs council
Tail Entity: kao charng
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization top members employees":
Relation: organization top members employees  
Context: elon musk, the CEO of spacex, announced the successful launch of the latest satellite into orbit, marking another milestone for the company.  
Head Entity: spacex  
Tail Entity: elon musk  

Relation: organization top members employees  
Context: sheryl sandberg, the chief operating officer of facebook, spoke at the conference about the importance of data privacy and user trust in the digital age.  
Head Entity: facebook  
Tail Entity: sheryl sandberg  
#### organization member of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization member of" means an organization is the member of an organization.
Here is an example:
Relation: organization member of
Context: south africa 1-5 leyds cnr biccard streets cosatu 's concern has been that the scorpions ' existence as a branch of the national directorate of public prosecutions blurred the necessary separation of functions between those investigating crime and those prosecuting the criminals .
Head Entity: scorpions
Tail Entity: national directorate of public prosecutions
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization member of":
Relation: organization member of  
Context: The International Federation of Red Cross and Red Crescent Societies (IFRC) is a global humanitarian organization that coordinates the activities of national Red Cross and Red Crescent societies, ensuring that they work together effectively in times of crisis.  
Head Entity: IFRC  
Tail Entity: national Red Cross and Red Crescent societies  

Relation: organization member of  
Context: The European Union is a political and economic union of member states that are located primarily in Europe, working together to promote peace, stability, and prosperity across the continent.  
Head Entity: European Union  
Tail Entity: member states
#### person origin ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person origin" means a person 's nationality is a country.
Here is an example:
Relation: person origin
Context: iranian nuclear negotiator ali larijani said thursday in ankara that talks on settling the iranian nuclear crisis had made some progress towards a `` united view . ''
Head Entity: ali larijani
Tail Entity: iranian
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person origin":
Relation: person origin  
Context: The famous physicist Albert Einstein was born in the Kingdom of Württemberg in the German Empire, which is now part of modern-day Germany.  
Head Entity: Albert Einstein  
Tail Entity: German  

Relation: person origin  
Context: The renowned author Chimamanda Ngozi Adichie often speaks about her Nigerian heritage and how it influences her writing.  
Head Entity: Chimamanda Ngozi Adichie  
Tail Entity: Nigerian  
#### person title ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person title" means a person 's title is a title.
Here is an example:
Relation: person title
Context: `` it has to have some tempo , it has to have some energy , '' said sean ross , vice president of music and programming at edison media research , which surveys the radio industry .
Head Entity: sean ross
Tail Entity: vice president
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person title":
Relation: person title  
Context: "After years of hard work and dedication, Maria Lopez was finally recognized as the chief executive officer of the company, leading it to new heights."  
Head Entity: Maria Lopez  
Tail Entity: chief executive officer  

Relation: person title  
Context: "During the conference, Dr. James Smith, the lead researcher at the institute, presented groundbreaking findings in the field of renewable energy."  
Head Entity: Dr. James Smith  
Tail Entity: lead researcher  
#### organization country of headquarters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization country of headquarters" means an organization is located in a country.
Here is an example:
Relation: organization country of headquarters
Context: the credit crisis spread to the largest us bond insurer thursday , sending shares of mbia inc plunging and calling into question the safety of tens of billions of dollars of company and local government debt held by investors .
Head Entity: mbia
Tail Entity: us
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization country of headquarters":
Relation: organization country of headquarters  
Context: after years of rapid growth, the tech giant apple inc announced plans to expand its operations in ireland, taking advantage of the favorable tax environment.  
Head Entity: apple  
Tail Entity: ireland  

Relation: organization country of headquarters  
Context: the renowned automotive manufacturer toyota has established its main production facility in japan, contributing significantly to the local economy.  
Head Entity: toyota  
Tail Entity: japan  
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 75.00%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    3 | acc: 62.50%,  total acc: 76.56%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 77.50%   [EVAL] batch:    5 | acc: 68.75%,  total acc: 76.04%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 78.57%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 82.64%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 83.75%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 85.23%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 85.94%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 86.54%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 85.27%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 84.58%   [EVAL] batch:   15 | acc: 62.50%,  total acc: 83.20%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 82.72%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 81.60%   [EVAL] batch:   18 | acc: 81.25%,  total acc: 81.58%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 81.56%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 82.44%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 83.24%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 83.97%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 84.64%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 85.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 85.82%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 86.11%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 86.61%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 87.07%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 86.88%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 86.90%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 87.11%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 85.61%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 75.00%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    3 | acc: 62.50%,  total acc: 76.56%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 77.50%   [EVAL] batch:    5 | acc: 68.75%,  total acc: 76.04%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 78.57%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 82.64%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 83.75%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 85.23%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 85.94%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 86.54%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 85.27%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 84.58%   [EVAL] batch:   15 | acc: 62.50%,  total acc: 83.20%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 82.72%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 81.60%   [EVAL] batch:   18 | acc: 81.25%,  total acc: 81.58%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 81.56%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 82.44%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 83.24%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 83.97%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 84.64%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 85.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 85.82%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 86.11%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 86.61%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 87.07%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 86.88%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 86.90%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 87.11%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 85.61%   
cur_acc:  ['0.8561']
his_acc:  ['0.8561']
CurrentTrain: epoch  0, batch     0 | loss: 6.7105479CurrentTrain: epoch  0, batch     1 | loss: 7.8216114CurrentTrain: epoch  1, batch     0 | loss: 6.5109053CurrentTrain: epoch  1, batch     1 | loss: 5.2046876CurrentTrain: epoch  2, batch     0 | loss: 5.8193226CurrentTrain: epoch  2, batch     1 | loss: 5.3728476CurrentTrain: epoch  3, batch     0 | loss: 5.3471889CurrentTrain: epoch  3, batch     1 | loss: 4.4202156CurrentTrain: epoch  4, batch     0 | loss: 4.8331256CurrentTrain: epoch  4, batch     1 | loss: 4.8644309CurrentTrain: epoch  5, batch     0 | loss: 4.9063978CurrentTrain: epoch  5, batch     1 | loss: 3.9061246CurrentTrain: epoch  6, batch     0 | loss: 4.2706718CurrentTrain: epoch  6, batch     1 | loss: 3.8399904CurrentTrain: epoch  7, batch     0 | loss: 4.1974335CurrentTrain: epoch  7, batch     1 | loss: 4.0149384CurrentTrain: epoch  8, batch     0 | loss: 3.6917348CurrentTrain: epoch  8, batch     1 | loss: 3.3621042CurrentTrain: epoch  9, batch     0 | loss: 3.4238887CurrentTrain: epoch  9, batch     1 | loss: 3.0295978
#### person date of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person date of birth" means a person was born in a date.
Here is an example:
Relation: person date of birth
Context: kirkaldy , born irene morgan in baltimore , maryland , in 1917 , was arrested in 1944 for refusing to give up her seat on a greyhound bus heading from gloucester to baltimore , and for resisting arrest .
Head Entity: irene morgan
Tail Entity: 1917
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person date of birth":
Relation: person date of birth  
Context: the famous physicist albert einstein was born in ulm, in the kingdom of wurttemberg in the german empire, on march 14, 1879.  
Head Entity: albert einstein  
Tail Entity: march 14, 1879  

Relation: person date of birth  
Context: the renowned author jane austen was born on december 16, 1775, in steventon, hampshire, england.  
Head Entity: jane austen  
Tail Entity: december 16, 1775  
#### person stateorprovince of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person stateorprovince of birth" means a person was died in a state or province.
Here is an example:
Relation: person stateorprovince of birth
Context: jane matilda bolin was born on april 11 , 1908 , in poughkeepsie , ny .
Head Entity: jane matilda bolin
Tail Entity: ny
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person stateorprovince of birth":
Relation: person stateorprovince of birth  
Context: albert einstein was born on march 14, 1879, in ulm, württemberg, germany.  
Head Entity: albert einstein  
Tail Entity: württemberg  

Relation: person stateorprovince of birth  
Context: oprah winfrey was born on january 29, 1954, in kosciusko, mississippi.  
Head Entity: oprah winfrey  
Tail Entity: mississippi  
#### person parents ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person parents" means a person 's parent is a person.
Here is an example:
Relation: person parents
Context: lynne spears told the court that lutfi had treated her daughter like a hostage in her own home , drugged her and took over her finances .
Head Entity: her
Tail Entity: lynne spears
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person parents":
Sample 1:  
Relation: person parents  
Context: After years of hard work, Sarah finally made her parents proud by graduating at the top of her class.  
Head Entity: her  
Tail Entity: Sarah  

Sample 2:  
Relation: person parents  
Context: During the family reunion, Michael shared stories about how his father taught him valuable life lessons.  
Head Entity: his  
Tail Entity: Michael  
#### person employee of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person employee of" means a person is an employee of an organization.
Here is an example:
Relation: person employee of
Context: seeking revenge , axel reunites with old pal sgt. billy rosewood -lrb- judge reinhold -rrb- and jon flint -lrb- hector elizondo -rrb- of the beverly hills police department .
Head Entity: hector elizondo
Tail Entity: beverly hills police department
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person employee of":
Relation: person employee of  
Context: After years of hard work, Maria finally secured a position at the prestigious tech firm, Innovatech, where she could showcase her skills.  
Head Entity: Maria  
Tail Entity: Innovatech  

Relation: person employee of  
Context: John has been with the global consulting firm, Stratagem, for over a decade, helping clients navigate complex business challenges.  
Head Entity: John  
Tail Entity: Stratagem  
#### person stateorprovince of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person stateorprovince of death" means a person was died in a state or province.
Here is an example:
Relation: person stateorprovince of death
Context: irene morgan kirkaldy , 90 , who died of alzheimer 's disease aug. 10 at her home in gloucester , va. , quietly changed history in 1944 when she refused to give up her seat on a crowded greyhound bus to a white couple .
Head Entity: irene morgan kirkaldy
Tail Entity: va.
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person stateorprovince of death":
Relation: person stateorprovince of death  
Context: john smith, a renowned author, passed away on march 5 in his residence located in los angeles, california, leaving behind a legacy of literary works that inspired many.  
Head Entity: john smith  
Tail Entity: california  

Relation: person stateorprovince of death  
Context: the famous musician, elena rodriguez, died tragically in a car accident on july 12 while traveling through the scenic routes of oregon, where she had spent her childhood.  
Head Entity: elena rodriguez  
Tail Entity: oregon  
Mixup data size:  1495
MixupTrain:  epoch  0, batch     0 | loss: 7.0381384MixupTrain:  epoch  0, batch     1 | loss: 6.7714033MixupTrain:  epoch  0, batch     2 | loss: 6.5151596MixupTrain:  epoch  0, batch     3 | loss: 6.3253698MixupTrain:  epoch  0, batch     4 | loss: 6.0300312MixupTrain:  epoch  0, batch     5 | loss: 5.9534149MixupTrain:  epoch  0, batch     6 | loss: 5.7328839MixupTrain:  epoch  0, batch     7 | loss: 5.7006507MixupTrain:  epoch  0, batch     8 | loss: 5.6177511MixupTrain:  epoch  0, batch     9 | loss: 5.5412407MixupTrain:  epoch  0, batch    10 | loss: 5.5403929MixupTrain:  epoch  0, batch    11 | loss: 5.5066404MixupTrain:  epoch  0, batch    12 | loss: 5.4768238MixupTrain:  epoch  0, batch    13 | loss: 5.4537468MixupTrain:  epoch  0, batch    14 | loss: 5.3969402MixupTrain:  epoch  0, batch    15 | loss: 5.3503113MixupTrain:  epoch  0, batch    16 | loss: 5.3478022MixupTrain:  epoch  0, batch    17 | loss: 5.2782683MixupTrain:  epoch  0, batch    18 | loss: 5.2234483MixupTrain:  epoch  0, batch    19 | loss: 5.2662444MixupTrain:  epoch  0, batch    20 | loss: 5.1970348MixupTrain:  epoch  0, batch    21 | loss: 5.2025356MixupTrain:  epoch  0, batch    22 | loss: 5.1701241MixupTrain:  epoch  0, batch    23 | loss: 5.1027918MixupTrain:  epoch  0, batch    24 | loss: 5.1056643MixupTrain:  epoch  0, batch    25 | loss: 5.0691018MixupTrain:  epoch  0, batch    26 | loss: 5.0025702MixupTrain:  epoch  0, batch    27 | loss: 4.9559498MixupTrain:  epoch  0, batch    28 | loss: 4.9389744MixupTrain:  epoch  0, batch    29 | loss: 4.9750009MixupTrain:  epoch  0, batch    30 | loss: 4.8796124MixupTrain:  epoch  0, batch    31 | loss: 4.8541651MixupTrain:  epoch  0, batch    32 | loss: 4.8431921MixupTrain:  epoch  0, batch    33 | loss: 4.8173475MixupTrain:  epoch  0, batch    34 | loss: 4.7781363MixupTrain:  epoch  0, batch    35 | loss: 4.7718830MixupTrain:  epoch  0, batch    36 | loss: 4.7168722MixupTrain:  epoch  0, batch    37 | loss: 4.7250757MixupTrain:  epoch  0, batch    38 | loss: 4.6889257MixupTrain:  epoch  0, batch    39 | loss: 4.6609235MixupTrain:  epoch  0, batch    40 | loss: 4.6260200MixupTrain:  epoch  0, batch    41 | loss: 4.6109705MixupTrain:  epoch  0, batch    42 | loss: 4.5960331MixupTrain:  epoch  0, batch    43 | loss: 4.5584288MixupTrain:  epoch  0, batch    44 | loss: 4.5390587MixupTrain:  epoch  0, batch    45 | loss: 4.5079565MixupTrain:  epoch  0, batch    46 | loss: 4.4902716MixupTrain:  epoch  0, batch    47 | loss: 4.4809484MixupTrain:  epoch  0, batch    48 | loss: 4.4686584MixupTrain:  epoch  0, batch    49 | loss: 4.4385529MixupTrain:  epoch  0, batch    50 | loss: 4.4257784MixupTrain:  epoch  0, batch    51 | loss: 4.4105215MixupTrain:  epoch  0, batch    52 | loss: 4.3960075MixupTrain:  epoch  0, batch    53 | loss: 4.3358879MixupTrain:  epoch  0, batch    54 | loss: 4.3298430MixupTrain:  epoch  0, batch    55 | loss: 4.3439169MixupTrain:  epoch  0, batch    56 | loss: 4.3187656MixupTrain:  epoch  0, batch    57 | loss: 4.2970304MixupTrain:  epoch  0, batch    58 | loss: 4.2765846MixupTrain:  epoch  0, batch    59 | loss: 4.2623892MixupTrain:  epoch  0, batch    60 | loss: 4.2558775MixupTrain:  epoch  0, batch    61 | loss: 4.2423544MixupTrain:  epoch  0, batch    62 | loss: 4.2269697MixupTrain:  epoch  0, batch    63 | loss: 4.2163200MixupTrain:  epoch  0, batch    64 | loss: 4.2036681MixupTrain:  epoch  0, batch    65 | loss: 4.1838846MixupTrain:  epoch  0, batch    66 | loss: 4.1770926MixupTrain:  epoch  0, batch    67 | loss: 4.1813841MixupTrain:  epoch  0, batch    68 | loss: 4.1608229MixupTrain:  epoch  0, batch    69 | loss: 4.1415882MixupTrain:  epoch  0, batch    70 | loss: 4.1272573MixupTrain:  epoch  0, batch    71 | loss: 4.1249647MixupTrain:  epoch  0, batch    72 | loss: 4.1163778MixupTrain:  epoch  0, batch    73 | loss: 4.0918283MixupTrain:  epoch  0, batch    74 | loss: 4.1122732MixupTrain:  epoch  0, batch    75 | loss: 4.0777626MixupTrain:  epoch  0, batch    76 | loss: 4.0758343MixupTrain:  epoch  0, batch    77 | loss: 4.0753918MixupTrain:  epoch  0, batch    78 | loss: 4.0688810MixupTrain:  epoch  0, batch    79 | loss: 4.0611520MixupTrain:  epoch  0, batch    80 | loss: 4.0485716MixupTrain:  epoch  0, batch    81 | loss: 4.0566072MixupTrain:  epoch  0, batch    82 | loss: 4.0356240MixupTrain:  epoch  0, batch    83 | loss: 4.0318604MixupTrain:  epoch  0, batch    84 | loss: 4.0215945MixupTrain:  epoch  0, batch    85 | loss: 4.0137229MixupTrain:  epoch  0, batch    86 | loss: 4.0042729MixupTrain:  epoch  0, batch    87 | loss: 4.0134478MixupTrain:  epoch  0, batch    88 | loss: 3.9978814MixupTrain:  epoch  0, batch    89 | loss: 3.9953613MixupTrain:  epoch  0, batch    90 | loss: 3.9940009MixupTrain:  epoch  0, batch    91 | loss: 4.0006852MixupTrain:  epoch  0, batch    92 | loss: 3.9875014MixupTrain:  epoch  0, batch    93 | loss: 3.9690678
MemoryTrain:  epoch  0, batch     0 | loss: 7.7958107MemoryTrain:  epoch  0, batch     1 | loss: 6.3955688MemoryTrain:  epoch  0, batch     2 | loss: 7.4508591MemoryTrain:  epoch  1, batch     0 | loss: 6.3619370MemoryTrain:  epoch  1, batch     1 | loss: 4.9034142MemoryTrain:  epoch  1, batch     2 | loss: 5.5356202MemoryTrain:  epoch  2, batch     0 | loss: 3.3407724MemoryTrain:  epoch  2, batch     1 | loss: 4.6465278MemoryTrain:  epoch  2, batch     2 | loss: 5.3265843MemoryTrain:  epoch  3, batch     0 | loss: 3.9203501MemoryTrain:  epoch  3, batch     1 | loss: 3.2478163MemoryTrain:  epoch  3, batch     2 | loss: 5.4294200MemoryTrain:  epoch  4, batch     0 | loss: 3.0144846MemoryTrain:  epoch  4, batch     1 | loss: 3.5540688MemoryTrain:  epoch  4, batch     2 | loss: 2.1695473MemoryTrain:  epoch  5, batch     0 | loss: 3.1601248MemoryTrain:  epoch  5, batch     1 | loss: 2.7638698MemoryTrain:  epoch  5, batch     2 | loss: 4.4942551MemoryTrain:  epoch  6, batch     0 | loss: 2.1448743MemoryTrain:  epoch  6, batch     1 | loss: 3.4222999MemoryTrain:  epoch  6, batch     2 | loss: 1.4720627MemoryTrain:  epoch  7, batch     0 | loss: 2.3616848MemoryTrain:  epoch  7, batch     1 | loss: 2.6757536MemoryTrain:  epoch  7, batch     2 | loss: 2.4578674MemoryTrain:  epoch  8, batch     0 | loss: 2.2408955MemoryTrain:  epoch  8, batch     1 | loss: 2.4110405MemoryTrain:  epoch  8, batch     2 | loss: 1.3985682MemoryTrain:  epoch  9, batch     0 | loss: 2.2793300MemoryTrain:  epoch  9, batch     1 | loss: 2.1704757MemoryTrain:  epoch  9, batch     2 | loss: 2.0187938
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 83.75%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 86.46%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 88.28%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 89.58%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 90.00%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 89.20%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 88.54%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 88.46%   [EVAL] batch:   13 | acc: 50.00%,  total acc: 85.71%   
[EVAL] batch:    0 | acc: 50.00%,  total acc: 50.00%   [EVAL] batch:    1 | acc: 62.50%,  total acc: 56.25%   [EVAL] batch:    2 | acc: 68.75%,  total acc: 60.42%   [EVAL] batch:    3 | acc: 43.75%,  total acc: 56.25%   [EVAL] batch:    4 | acc: 43.75%,  total acc: 53.75%   [EVAL] batch:    5 | acc: 50.00%,  total acc: 53.12%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 58.93%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 64.06%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 67.36%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 70.00%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 72.73%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 73.96%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 75.48%   [EVAL] batch:   13 | acc: 75.00%,  total acc: 75.45%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 75.00%   [EVAL] batch:   15 | acc: 62.50%,  total acc: 74.22%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 74.26%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 73.96%   [EVAL] batch:   18 | acc: 87.50%,  total acc: 74.67%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 75.31%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 76.49%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 77.56%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 78.53%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 79.43%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 80.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 81.01%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 81.48%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 82.14%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 82.76%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 82.92%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 83.47%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 83.79%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 84.09%   [EVAL] batch:   33 | acc: 68.75%,  total acc: 83.64%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 83.93%   [EVAL] batch:   35 | acc: 81.25%,  total acc: 83.85%   [EVAL] batch:   36 | acc: 87.50%,  total acc: 83.95%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 84.21%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 84.62%   [EVAL] batch:   39 | acc: 87.50%,  total acc: 84.69%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 85.06%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 85.42%   [EVAL] batch:   42 | acc: 87.50%,  total acc: 85.47%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 85.37%   [EVAL] batch:   44 | acc: 81.25%,  total acc: 85.28%   [EVAL] batch:   45 | acc: 75.00%,  total acc: 85.05%   [EVAL] batch:   46 | acc: 6.25%,  total acc: 83.38%   
cur_acc:  ['0.8561', '0.8571']
his_acc:  ['0.8561', '0.8338']
CurrentTrain: epoch  0, batch     0 | loss: 5.8504248CurrentTrain: epoch  0, batch     1 | loss: 6.8113351CurrentTrain: epoch  1, batch     0 | loss: 5.6757526CurrentTrain: epoch  1, batch     1 | loss: 4.8506646CurrentTrain: epoch  2, batch     0 | loss: 4.7830763CurrentTrain: epoch  2, batch     1 | loss: 4.4512849CurrentTrain: epoch  3, batch     0 | loss: 4.2209110CurrentTrain: epoch  3, batch     1 | loss: 4.2425756CurrentTrain: epoch  4, batch     0 | loss: 3.5564959CurrentTrain: epoch  4, batch     1 | loss: 4.6355524CurrentTrain: epoch  5, batch     0 | loss: 3.6075709CurrentTrain: epoch  5, batch     1 | loss: 3.4360323CurrentTrain: epoch  6, batch     0 | loss: 3.5241616CurrentTrain: epoch  6, batch     1 | loss: 2.7754450CurrentTrain: epoch  7, batch     0 | loss: 3.3550339CurrentTrain: epoch  7, batch     1 | loss: 2.8177595CurrentTrain: epoch  8, batch     0 | loss: 2.7079055CurrentTrain: epoch  8, batch     1 | loss: 3.6281548CurrentTrain: epoch  9, batch     0 | loss: 2.4379196CurrentTrain: epoch  9, batch     1 | loss: 3.6398058
#### person country of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person country of birth" means a person was born in a country.
Here is an example:
Relation: person country of birth
Context: in testimony by satellite link from germany to a house of representatives ' panel , murat kurnaz recounted his five-year detention , alleging a wide range of torture and abuse .
Head Entity: murat kurnaz
Tail Entity: germany
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person country of birth":
Relation: person country of birth  
Context: During the interview, the renowned artist shared his experiences growing up in the vibrant streets of Barcelona, reflecting on how the city shaped his creative vision.  
Head Entity: the renowned artist  
Tail Entity: Barcelona  

Relation: person country of birth  
Context: In her autobiography, the famous scientist detailed her early life in the lush landscapes of New Zealand, which inspired her passion for environmental research.  
Head Entity: the famous scientist  
Tail Entity: New Zealand  
#### organization website ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization website" means an organization 's website is an url.
Here is an example:
Relation: organization website
Context: http://www.pentaxsailaway.com/ paul j. sail away with pentax sweepstakes
Head Entity: pentax
Tail Entity: http://www.pentaxsailaway.com/
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization website":
Relation: organization website  
Context: Visit the official site at https://www.nike.com for the latest sports gear and apparel.  
Head Entity: Nike  
Tail Entity: https://www.nike.com  

Relation: organization website  
Context: For more information about their services, check out http://www.tesla.com.  
Head Entity: Tesla  
Tail Entity: http://www.tesla.com  
#### organization shareholders ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization shareholders" means an organization was invested by a person.
Here is an example:
Relation: organization shareholders
Context: marsans already owns the madrid-based air comet and 95 percent of aerolineas argentinas .
Head Entity: aerolineas argentinas
Tail Entity: marsans
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization shareholders":
Relation: organization shareholders  
Context: tech giant apple inc. has recently acquired a significant stake in the innovative startup nextdoor.  
Head Entity: nextdoor  
Tail Entity: apple inc.  

Relation: organization shareholders  
Context: the investment firm blackrock has increased its holdings in the renewable energy company sunrun.  
Head Entity: sunrun  
Tail Entity: blackrock  
#### organization dissolved ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization dissolved" means an organization was dissolved in a date.
Here is an example:
Relation: organization dissolved
Context: the tse suffered its worst-ever system crash in november 2005 which paralyzed the world 's second largest bourse and forced it to shelve plan for a listing of its own .
Head Entity: tse
Tail Entity: november 2005
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization dissolved":
Relation: organization dissolved  
Context: The once-prominent tech startup, Innovatech, officially ceased operations in March 2020 after struggling to secure funding.  
Head Entity: Innovatech  
Tail Entity: March 2020  

Relation: organization dissolved  
Context: After years of financial difficulties, the local arts council announced its dissolution in January 2019, leaving many community projects in limbo.  
Head Entity: local arts council  
Tail Entity: January 2019  
#### organization founded by ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization founded by" means an organization was found by a person.
Here is an example:
Relation: organization founded by
Context: `` i have no greater obligation than to ensure the safely of airline travelers in this country , '' transportation secretary ray lahood said in a joint statement with j. randolph babbitt , administrator of the federal aviation administration , that was issued on the eve of a senate hearing on aviation safety .
Head Entity: federal aviation administration
Tail Entity: j. randolph babbitt
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization founded by":
Relation: organization founded by  
Context: In 1975, the renowned physicist and entrepreneur, Dr. Jane Smith, established Quantum Innovations, a company dedicated to advancing quantum computing technologies.  
Head Entity: Quantum Innovations  
Tail Entity: Dr. Jane Smith  

Relation: organization founded by  
Context: The charity organization, Hope for Tomorrow, was created in 2010 by the well-known philanthropist, John Doe, to support underprivileged children around the world.  
Head Entity: Hope for Tomorrow  
Tail Entity: John Doe  
Mixup data size:  2455
MixupTrain:  epoch  0, batch     0 | loss: 6.2348180MixupTrain:  epoch  0, batch     1 | loss: 6.0658255MixupTrain:  epoch  0, batch     2 | loss: 6.4150748MixupTrain:  epoch  0, batch     3 | loss: 6.0657778MixupTrain:  epoch  0, batch     4 | loss: 5.9036665MixupTrain:  epoch  0, batch     5 | loss: 5.6960793MixupTrain:  epoch  0, batch     6 | loss: 5.9132633MixupTrain:  epoch  0, batch     7 | loss: 5.8690801MixupTrain:  epoch  0, batch     8 | loss: 5.8644924MixupTrain:  epoch  0, batch     9 | loss: 5.4645290MixupTrain:  epoch  0, batch    10 | loss: 5.9167433MixupTrain:  epoch  0, batch    11 | loss: 5.4368072MixupTrain:  epoch  0, batch    12 | loss: 5.6713696MixupTrain:  epoch  0, batch    13 | loss: 5.3670778MixupTrain:  epoch  0, batch    14 | loss: 5.3519039MixupTrain:  epoch  0, batch    15 | loss: 5.5337386MixupTrain:  epoch  0, batch    16 | loss: 5.6111369MixupTrain:  epoch  0, batch    17 | loss: 5.4360447MixupTrain:  epoch  0, batch    18 | loss: 5.3444357MixupTrain:  epoch  0, batch    19 | loss: 4.9983330MixupTrain:  epoch  0, batch    20 | loss: 4.9801722MixupTrain:  epoch  0, batch    21 | loss: 5.0903654MixupTrain:  epoch  0, batch    22 | loss: 5.3942747MixupTrain:  epoch  0, batch    23 | loss: 5.3437319MixupTrain:  epoch  0, batch    24 | loss: 5.2437472MixupTrain:  epoch  0, batch    25 | loss: 5.2907372MixupTrain:  epoch  0, batch    26 | loss: 5.3422856MixupTrain:  epoch  0, batch    27 | loss: 5.1641879MixupTrain:  epoch  0, batch    28 | loss: 5.3515272MixupTrain:  epoch  0, batch    29 | loss: 5.1937866MixupTrain:  epoch  0, batch    30 | loss: 5.3576903MixupTrain:  epoch  0, batch    31 | loss: 5.1249971MixupTrain:  epoch  0, batch    32 | loss: 5.1488347MixupTrain:  epoch  0, batch    33 | loss: 5.0952907MixupTrain:  epoch  0, batch    34 | loss: 5.2451725MixupTrain:  epoch  0, batch    35 | loss: 5.2436113MixupTrain:  epoch  0, batch    36 | loss: 4.8495698MixupTrain:  epoch  0, batch    37 | loss: 4.9542489MixupTrain:  epoch  0, batch    38 | loss: 5.1311541MixupTrain:  epoch  0, batch    39 | loss: 5.0496516MixupTrain:  epoch  0, batch    40 | loss: 4.8716860MixupTrain:  epoch  0, batch    41 | loss: 5.0779800MixupTrain:  epoch  0, batch    42 | loss: 4.9051013MixupTrain:  epoch  0, batch    43 | loss: 5.0572186MixupTrain:  epoch  0, batch    44 | loss: 5.1548381MixupTrain:  epoch  0, batch    45 | loss: 4.9833283MixupTrain:  epoch  0, batch    46 | loss: 4.9651175MixupTrain:  epoch  0, batch    47 | loss: 5.1921339MixupTrain:  epoch  0, batch    48 | loss: 5.0675430MixupTrain:  epoch  0, batch    49 | loss: 4.8894396MixupTrain:  epoch  0, batch    50 | loss: 5.1169734MixupTrain:  epoch  0, batch    51 | loss: 4.6671324MixupTrain:  epoch  0, batch    52 | loss: 4.8766241MixupTrain:  epoch  0, batch    53 | loss: 4.9375758MixupTrain:  epoch  0, batch    54 | loss: 4.8815060MixupTrain:  epoch  0, batch    55 | loss: 4.8910713MixupTrain:  epoch  0, batch    56 | loss: 4.8771505MixupTrain:  epoch  0, batch    57 | loss: 4.7112665MixupTrain:  epoch  0, batch    58 | loss: 4.7184038MixupTrain:  epoch  0, batch    59 | loss: 4.7442641MixupTrain:  epoch  0, batch    60 | loss: 4.6647072MixupTrain:  epoch  0, batch    61 | loss: 4.6936989MixupTrain:  epoch  0, batch    62 | loss: 4.7946863MixupTrain:  epoch  0, batch    63 | loss: 4.7711787MixupTrain:  epoch  0, batch    64 | loss: 4.7599621MixupTrain:  epoch  0, batch    65 | loss: 4.8444147MixupTrain:  epoch  0, batch    66 | loss: 4.7633018MixupTrain:  epoch  0, batch    67 | loss: 4.6476326MixupTrain:  epoch  0, batch    68 | loss: 4.8021622MixupTrain:  epoch  0, batch    69 | loss: 4.7883177MixupTrain:  epoch  0, batch    70 | loss: 4.8585863MixupTrain:  epoch  0, batch    71 | loss: 4.8015137MixupTrain:  epoch  0, batch    72 | loss: 4.7120695MixupTrain:  epoch  0, batch    73 | loss: 4.7183118MixupTrain:  epoch  0, batch    74 | loss: 4.8574486MixupTrain:  epoch  0, batch    75 | loss: 4.6868949MixupTrain:  epoch  0, batch    76 | loss: 4.6285753MixupTrain:  epoch  0, batch    77 | loss: 4.7581334MixupTrain:  epoch  0, batch    78 | loss: 4.6218367MixupTrain:  epoch  0, batch    79 | loss: 4.7322183MixupTrain:  epoch  0, batch    80 | loss: 4.7533374MixupTrain:  epoch  0, batch    81 | loss: 4.7260180MixupTrain:  epoch  0, batch    82 | loss: 4.6494493MixupTrain:  epoch  0, batch    83 | loss: 4.5049744MixupTrain:  epoch  0, batch    84 | loss: 4.4991693MixupTrain:  epoch  0, batch    85 | loss: 4.6029925MixupTrain:  epoch  0, batch    86 | loss: 4.3724356MixupTrain:  epoch  0, batch    87 | loss: 4.3572478MixupTrain:  epoch  0, batch    88 | loss: 4.5306554MixupTrain:  epoch  0, batch    89 | loss: 4.5643458MixupTrain:  epoch  0, batch    90 | loss: 4.5217600MixupTrain:  epoch  0, batch    91 | loss: 4.5637121MixupTrain:  epoch  0, batch    92 | loss: 4.6049914MixupTrain:  epoch  0, batch    93 | loss: 4.3494129MixupTrain:  epoch  0, batch    94 | loss: 4.6299748MixupTrain:  epoch  0, batch    95 | loss: 4.6539574MixupTrain:  epoch  0, batch    96 | loss: 4.4204884MixupTrain:  epoch  0, batch    97 | loss: 4.5454626MixupTrain:  epoch  0, batch    98 | loss: 4.5696998MixupTrain:  epoch  0, batch    99 | loss: 4.4294658MixupTrain:  epoch  0, batch   100 | loss: 4.7343140MixupTrain:  epoch  0, batch   101 | loss: 4.4846077MixupTrain:  epoch  0, batch   102 | loss: 4.4279284MixupTrain:  epoch  0, batch   103 | loss: 4.5911150MixupTrain:  epoch  0, batch   104 | loss: 4.5886974MixupTrain:  epoch  0, batch   105 | loss: 4.4421978MixupTrain:  epoch  0, batch   106 | loss: 4.6779733MixupTrain:  epoch  0, batch   107 | loss: 4.3492889MixupTrain:  epoch  0, batch   108 | loss: 4.5910673MixupTrain:  epoch  0, batch   109 | loss: 4.4897594MixupTrain:  epoch  0, batch   110 | loss: 4.3024125MixupTrain:  epoch  0, batch   111 | loss: 4.3782692MixupTrain:  epoch  0, batch   112 | loss: 4.3909016MixupTrain:  epoch  0, batch   113 | loss: 4.5690355MixupTrain:  epoch  0, batch   114 | loss: 4.4872084MixupTrain:  epoch  0, batch   115 | loss: 4.3995047MixupTrain:  epoch  0, batch   116 | loss: 4.4403853MixupTrain:  epoch  0, batch   117 | loss: 4.4605775MixupTrain:  epoch  0, batch   118 | loss: 4.4166260MixupTrain:  epoch  0, batch   119 | loss: 4.5879068MixupTrain:  epoch  0, batch   120 | loss: 4.7041373MixupTrain:  epoch  0, batch   121 | loss: 4.3031979MixupTrain:  epoch  0, batch   122 | loss: 4.3746529MixupTrain:  epoch  0, batch   123 | loss: 4.2957664MixupTrain:  epoch  0, batch   124 | loss: 4.4619217MixupTrain:  epoch  0, batch   125 | loss: 4.3358355MixupTrain:  epoch  0, batch   126 | loss: 4.3366990MixupTrain:  epoch  0, batch   127 | loss: 4.2740593MixupTrain:  epoch  0, batch   128 | loss: 4.3073769MixupTrain:  epoch  0, batch   129 | loss: 4.2563396MixupTrain:  epoch  0, batch   130 | loss: 4.4415255MixupTrain:  epoch  0, batch   131 | loss: 4.3668203MixupTrain:  epoch  0, batch   132 | loss: 4.4868717MixupTrain:  epoch  0, batch   133 | loss: 4.3898935MixupTrain:  epoch  0, batch   134 | loss: 4.2558980MixupTrain:  epoch  0, batch   135 | loss: 4.5065794MixupTrain:  epoch  0, batch   136 | loss: 4.4362679MixupTrain:  epoch  0, batch   137 | loss: 4.2912016MixupTrain:  epoch  0, batch   138 | loss: 4.1593819MixupTrain:  epoch  0, batch   139 | loss: 4.2810345MixupTrain:  epoch  0, batch   140 | loss: 4.2546840MixupTrain:  epoch  0, batch   141 | loss: 4.2595310MixupTrain:  epoch  0, batch   142 | loss: 4.3034573MixupTrain:  epoch  0, batch   143 | loss: 4.1950150MixupTrain:  epoch  0, batch   144 | loss: 4.2114048MixupTrain:  epoch  0, batch   145 | loss: 4.2382622MixupTrain:  epoch  0, batch   146 | loss: 4.2514544MixupTrain:  epoch  0, batch   147 | loss: 4.1719670MixupTrain:  epoch  0, batch   148 | loss: 4.3481741MixupTrain:  epoch  0, batch   149 | loss: 4.2213893MixupTrain:  epoch  0, batch   150 | loss: 4.1621046MixupTrain:  epoch  0, batch   151 | loss: 4.3201771MixupTrain:  epoch  0, batch   152 | loss: 4.2456484MixupTrain:  epoch  0, batch   153 | loss: 4.3703914
MemoryTrain:  epoch  0, batch     0 | loss: 4.2183075MemoryTrain:  epoch  0, batch     1 | loss: 4.0262232MemoryTrain:  epoch  0, batch     2 | loss: 2.9571784MemoryTrain:  epoch  1, batch     0 | loss: 2.6427743MemoryTrain:  epoch  1, batch     1 | loss: 4.2847443MemoryTrain:  epoch  1, batch     2 | loss: 3.4544859MemoryTrain:  epoch  2, batch     0 | loss: 2.7470417MemoryTrain:  epoch  2, batch     1 | loss: 2.3991868MemoryTrain:  epoch  2, batch     2 | loss: 3.6727839MemoryTrain:  epoch  3, batch     0 | loss: 2.3725238MemoryTrain:  epoch  3, batch     1 | loss: 2.4878292MemoryTrain:  epoch  3, batch     2 | loss: 3.4554231MemoryTrain:  epoch  4, batch     0 | loss: 2.1817851MemoryTrain:  epoch  4, batch     1 | loss: 2.5632215MemoryTrain:  epoch  4, batch     2 | loss: 3.1656241MemoryTrain:  epoch  5, batch     0 | loss: 2.5945411MemoryTrain:  epoch  5, batch     1 | loss: 2.3459854MemoryTrain:  epoch  5, batch     2 | loss: 1.9837625MemoryTrain:  epoch  6, batch     0 | loss: 2.1697502MemoryTrain:  epoch  6, batch     1 | loss: 1.7484651MemoryTrain:  epoch  6, batch     2 | loss: 2.3107038MemoryTrain:  epoch  7, batch     0 | loss: 2.1808410MemoryTrain:  epoch  7, batch     1 | loss: 1.8346121MemoryTrain:  epoch  7, batch     2 | loss: 1.6575577MemoryTrain:  epoch  8, batch     0 | loss: 1.5463042MemoryTrain:  epoch  8, batch     1 | loss: 1.7442406MemoryTrain:  epoch  8, batch     2 | loss: 2.0778735MemoryTrain:  epoch  9, batch     0 | loss: 1.8331246MemoryTrain:  epoch  9, batch     1 | loss: 1.6189148MemoryTrain:  epoch  9, batch     2 | loss: 1.7112873
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 77.08%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 78.12%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 78.75%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 82.29%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 83.93%   [EVAL] batch:    7 | acc: 6.25%,  total acc: 74.22%   
[EVAL] batch:    0 | acc: 31.25%,  total acc: 31.25%   [EVAL] batch:    1 | acc: 56.25%,  total acc: 43.75%   [EVAL] batch:    2 | acc: 62.50%,  total acc: 50.00%   [EVAL] batch:    3 | acc: 25.00%,  total acc: 43.75%   [EVAL] batch:    4 | acc: 43.75%,  total acc: 43.75%   [EVAL] batch:    5 | acc: 50.00%,  total acc: 44.79%   [EVAL] batch:    6 | acc: 18.75%,  total acc: 41.07%   [EVAL] batch:    7 | acc: 25.00%,  total acc: 39.06%   [EVAL] batch:    8 | acc: 18.75%,  total acc: 36.81%   [EVAL] batch:    9 | acc: 25.00%,  total acc: 35.62%   [EVAL] batch:   10 | acc: 31.25%,  total acc: 35.23%   [EVAL] batch:   11 | acc: 6.25%,  total acc: 32.81%   [EVAL] batch:   12 | acc: 25.00%,  total acc: 32.21%   [EVAL] batch:   13 | acc: 56.25%,  total acc: 33.93%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 36.67%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 37.89%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 40.07%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 41.32%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 42.76%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 44.38%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 47.02%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 49.43%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 51.63%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 53.65%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 55.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 57.21%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 58.56%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 60.04%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 61.42%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 62.29%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 63.51%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 64.45%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 65.34%   [EVAL] batch:   33 | acc: 68.75%,  total acc: 65.44%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 66.43%   [EVAL] batch:   35 | acc: 81.25%,  total acc: 66.84%   [EVAL] batch:   36 | acc: 87.50%,  total acc: 67.40%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 68.26%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 69.07%   [EVAL] batch:   39 | acc: 81.25%,  total acc: 69.38%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 70.12%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 70.83%   [EVAL] batch:   42 | acc: 87.50%,  total acc: 71.22%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 71.45%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 71.94%   [EVAL] batch:   45 | acc: 75.00%,  total acc: 72.01%   [EVAL] batch:   46 | acc: 56.25%,  total acc: 71.68%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 72.27%   [EVAL] batch:   48 | acc: 68.75%,  total acc: 72.19%   [EVAL] batch:   49 | acc: 81.25%,  total acc: 72.38%   [EVAL] batch:   50 | acc: 81.25%,  total acc: 72.55%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 73.08%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 73.47%   [EVAL] batch:   53 | acc: 12.50%,  total acc: 72.34%   
cur_acc:  ['0.8561', '0.8571', '0.7422']
his_acc:  ['0.8561', '0.8338', '0.7234']
CurrentTrain: epoch  0, batch     0 | loss: 4.4315763CurrentTrain: epoch  0, batch     1 | loss: 5.2232461CurrentTrain: epoch  1, batch     0 | loss: 3.9792981CurrentTrain: epoch  1, batch     1 | loss: 3.9012635CurrentTrain: epoch  2, batch     0 | loss: 3.4734321CurrentTrain: epoch  2, batch     1 | loss: 3.0163841CurrentTrain: epoch  3, batch     0 | loss: 2.9469316CurrentTrain: epoch  3, batch     1 | loss: 2.7579186CurrentTrain: epoch  4, batch     0 | loss: 2.9349799CurrentTrain: epoch  4, batch     1 | loss: 2.4817216CurrentTrain: epoch  5, batch     0 | loss: 2.3502274CurrentTrain: epoch  5, batch     1 | loss: 2.4541075CurrentTrain: epoch  6, batch     0 | loss: 2.2741642CurrentTrain: epoch  6, batch     1 | loss: 2.3216584CurrentTrain: epoch  7, batch     0 | loss: 2.2829957CurrentTrain: epoch  7, batch     1 | loss: 1.9547353CurrentTrain: epoch  8, batch     0 | loss: 2.0519757CurrentTrain: epoch  8, batch     1 | loss: 2.1070604CurrentTrain: epoch  9, batch     0 | loss: 2.0251441CurrentTrain: epoch  9, batch     1 | loss: 2.1429820
#### person cause of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person cause of death" means a person was died of an event.
Here is an example:
Relation: person cause of death
Context: mr scheider had suffered from multiple myeloma for several years , and died of complications from a staph infection , his wife , brenda siemer , said .
Head Entity: mr scheider
Tail Entity: complications from a staph infection
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person cause of death":
Relation: person cause of death  
Context: after battling with lung cancer for over a year, john doe passed away peacefully in his sleep, surrounded by family.  
Head Entity: john doe  
Tail Entity: lung cancer  

Relation: person cause of death  
Context: the renowned scientist, dr. emily white, tragically lost her life due to a car accident while returning from a conference.  
Head Entity: dr. emily white  
Tail Entity: car accident  
#### organization political religious affiliation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization political religious affiliation" means an organization is the member of a religion.
Here is an example:
Relation: organization political religious affiliation
Context: the mnlf used to be the largest muslim group fighting for a separate islamic homeland in the southern philippines until it settled for limited autonomy and signed a peace agreement with manila in 1996 .
Head Entity: mnlf
Tail Entity: islamic
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization political religious affiliation":
Relation: organization political religious affiliation  
Context: the national council of churches in the philippines has been a significant advocate for social justice and has ties to various religious groups across the country.  
Head Entity: national council of churches in the philippines  
Tail Entity: religious groups  

Relation: organization political religious affiliation  
Context: the interfaith alliance works to promote understanding and cooperation among different faiths, influencing political discourse in the region.  
Head Entity: interfaith alliance  
Tail Entity: faiths  
#### organization stateorprovince of headquarters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization stateorprovince of headquarters" means an organization is located in a state or province.
Here is an example:
Relation: organization stateorprovince of headquarters
Context: based in armonk , new york , mbia insures $ 670 billion -lrb- euro452 .18 billion -rrb- in debt .
Head Entity: mbia
Tail Entity: new york
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization stateorprovince of headquarters":
Relation: organization stateorprovince of headquarters  
Context: the tech giant apple inc. has its headquarters in cupertino, california, where it develops innovative products.  
Head Entity: apple inc.  
Tail Entity: california  

Relation: organization stateorprovince of headquarters  
Context: the multinational corporation samsung electronics is headquartered in suwon, south korea, and is a leader in consumer electronics.  
Head Entity: samsung electronics  
Tail Entity: south korea  
#### person other family ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person other family" means a person 's relative is a person.
Here is an example:
Relation: person other family
Context: parren mitchell 's sister-in-law , juanita jackson mitchell , was the long - time head and legal counsel of the maryland naacp .
Head Entity: parren mitchell
Tail Entity: juanita jackson mitchell
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person other family":
Relation: person other family  
Context: During the family reunion, it was revealed that Sarah's cousin, Michael, had recently graduated from college with honors.  
Head Entity: Sarah  
Tail Entity: Michael  

Relation: person other family  
Context: In her memoir, Lisa shared stories about her aunt, who played a significant role in her upbringing and inspired her to pursue a career in art.  
Head Entity: Lisa  
Tail Entity: her aunt  
#### person city of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person city of death" means a person was died in a city.
Here is an example:
Relation: person city of death
Context: verity died wednesday , jan 3 , 2007 , in beaufort memorial hospital .
Head Entity: verity
Tail Entity: beaufort
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person city of death":
Relation: person city of death  
Context: after a long battle with illness, john passed away in the quiet town of springfield.  
Head Entity: john  
Tail Entity: springfield  

Relation: person city of death  
Context: the renowned author tragically died in a car accident on the streets of los angeles.  
Head Entity: the renowned author  
Tail Entity: los angeles  
Mixup data size:  3640
MixupTrain:  epoch  0, batch     0 | loss: 5.0433559MixupTrain:  epoch  0, batch     1 | loss: 4.4387350MixupTrain:  epoch  0, batch     2 | loss: 4.0327921MixupTrain:  epoch  0, batch     3 | loss: 4.3902702MixupTrain:  epoch  0, batch     4 | loss: 4.4723029MixupTrain:  epoch  0, batch     5 | loss: 4.6750121MixupTrain:  epoch  0, batch     6 | loss: 4.9282131MixupTrain:  epoch  0, batch     7 | loss: 4.0997529MixupTrain:  epoch  0, batch     8 | loss: 4.5996046MixupTrain:  epoch  0, batch     9 | loss: 4.2905049MixupTrain:  epoch  0, batch    10 | loss: 4.4953208MixupTrain:  epoch  0, batch    11 | loss: 4.0574269MixupTrain:  epoch  0, batch    12 | loss: 4.1154490MixupTrain:  epoch  0, batch    13 | loss: 4.2143955MixupTrain:  epoch  0, batch    14 | loss: 4.3430972MixupTrain:  epoch  0, batch    15 | loss: 4.2145362MixupTrain:  epoch  0, batch    16 | loss: 4.2081399MixupTrain:  epoch  0, batch    17 | loss: 4.0010033MixupTrain:  epoch  0, batch    18 | loss: 3.9298322MixupTrain:  epoch  0, batch    19 | loss: 4.1082163MixupTrain:  epoch  0, batch    20 | loss: 3.9945512MixupTrain:  epoch  0, batch    21 | loss: 4.2005663MixupTrain:  epoch  0, batch    22 | loss: 3.7290154MixupTrain:  epoch  0, batch    23 | loss: 3.9569302MixupTrain:  epoch  0, batch    24 | loss: 3.9375589MixupTrain:  epoch  0, batch    25 | loss: 3.8745816MixupTrain:  epoch  0, batch    26 | loss: 4.0330200MixupTrain:  epoch  0, batch    27 | loss: 3.6192222MixupTrain:  epoch  0, batch    28 | loss: 4.2498255MixupTrain:  epoch  0, batch    29 | loss: 4.1274023MixupTrain:  epoch  0, batch    30 | loss: 3.8265743MixupTrain:  epoch  0, batch    31 | loss: 4.1059427MixupTrain:  epoch  0, batch    32 | loss: 3.9003477MixupTrain:  epoch  0, batch    33 | loss: 3.4707761MixupTrain:  epoch  0, batch    34 | loss: 3.9961605MixupTrain:  epoch  0, batch    35 | loss: 3.5657892MixupTrain:  epoch  0, batch    36 | loss: 4.0535116MixupTrain:  epoch  0, batch    37 | loss: 3.6349659MixupTrain:  epoch  0, batch    38 | loss: 4.1837263MixupTrain:  epoch  0, batch    39 | loss: 3.6881227MixupTrain:  epoch  0, batch    40 | loss: 3.7926831MixupTrain:  epoch  0, batch    41 | loss: 3.8699641MixupTrain:  epoch  0, batch    42 | loss: 3.6265917MixupTrain:  epoch  0, batch    43 | loss: 3.7255013MixupTrain:  epoch  0, batch    44 | loss: 3.5964651MixupTrain:  epoch  0, batch    45 | loss: 4.0236773MixupTrain:  epoch  0, batch    46 | loss: 3.4718962MixupTrain:  epoch  0, batch    47 | loss: 3.6982780MixupTrain:  epoch  0, batch    48 | loss: 3.9058232MixupTrain:  epoch  0, batch    49 | loss: 4.0868368MixupTrain:  epoch  0, batch    50 | loss: 3.9652734MixupTrain:  epoch  0, batch    51 | loss: 3.7797382MixupTrain:  epoch  0, batch    52 | loss: 3.5672915MixupTrain:  epoch  0, batch    53 | loss: 3.5736880MixupTrain:  epoch  0, batch    54 | loss: 3.5889077MixupTrain:  epoch  0, batch    55 | loss: 3.6198716MixupTrain:  epoch  0, batch    56 | loss: 3.5150113MixupTrain:  epoch  0, batch    57 | loss: 3.5458946MixupTrain:  epoch  0, batch    58 | loss: 3.5069723MixupTrain:  epoch  0, batch    59 | loss: 3.9093087MixupTrain:  epoch  0, batch    60 | loss: 3.6066141MixupTrain:  epoch  0, batch    61 | loss: 3.7157183MixupTrain:  epoch  0, batch    62 | loss: 3.5721869MixupTrain:  epoch  0, batch    63 | loss: 3.7161674MixupTrain:  epoch  0, batch    64 | loss: 3.5234990MixupTrain:  epoch  0, batch    65 | loss: 3.8327293MixupTrain:  epoch  0, batch    66 | loss: 3.4036949MixupTrain:  epoch  0, batch    67 | loss: 3.7885065MixupTrain:  epoch  0, batch    68 | loss: 3.4325881MixupTrain:  epoch  0, batch    69 | loss: 3.5122237MixupTrain:  epoch  0, batch    70 | loss: 3.6471765MixupTrain:  epoch  0, batch    71 | loss: 3.4569192MixupTrain:  epoch  0, batch    72 | loss: 3.7207875MixupTrain:  epoch  0, batch    73 | loss: 3.5935855MixupTrain:  epoch  0, batch    74 | loss: 3.4301219MixupTrain:  epoch  0, batch    75 | loss: 3.4557104MixupTrain:  epoch  0, batch    76 | loss: 3.4563079MixupTrain:  epoch  0, batch    77 | loss: 3.5537379MixupTrain:  epoch  0, batch    78 | loss: 3.4449682MixupTrain:  epoch  0, batch    79 | loss: 3.4644065MixupTrain:  epoch  0, batch    80 | loss: 3.3861370MixupTrain:  epoch  0, batch    81 | loss: 3.1645346MixupTrain:  epoch  0, batch    82 | loss: 3.3898478MixupTrain:  epoch  0, batch    83 | loss: 3.5448568MixupTrain:  epoch  0, batch    84 | loss: 3.4000981MixupTrain:  epoch  0, batch    85 | loss: 3.7316356MixupTrain:  epoch  0, batch    86 | loss: 3.6930151MixupTrain:  epoch  0, batch    87 | loss: 3.5826159MixupTrain:  epoch  0, batch    88 | loss: 3.5048985MixupTrain:  epoch  0, batch    89 | loss: 3.4901810MixupTrain:  epoch  0, batch    90 | loss: 3.4040656MixupTrain:  epoch  0, batch    91 | loss: 3.2792182MixupTrain:  epoch  0, batch    92 | loss: 3.3555713MixupTrain:  epoch  0, batch    93 | loss: 3.2803836MixupTrain:  epoch  0, batch    94 | loss: 3.6206772MixupTrain:  epoch  0, batch    95 | loss: 3.2112272MixupTrain:  epoch  0, batch    96 | loss: 3.4675577MixupTrain:  epoch  0, batch    97 | loss: 3.3060377MixupTrain:  epoch  0, batch    98 | loss: 3.5145669MixupTrain:  epoch  0, batch    99 | loss: 3.3226984MixupTrain:  epoch  0, batch   100 | loss: 3.5275345MixupTrain:  epoch  0, batch   101 | loss: 3.5934463MixupTrain:  epoch  0, batch   102 | loss: 3.2228031MixupTrain:  epoch  0, batch   103 | loss: 3.3461761MixupTrain:  epoch  0, batch   104 | loss: 3.6392503MixupTrain:  epoch  0, batch   105 | loss: 3.2089028MixupTrain:  epoch  0, batch   106 | loss: 3.2503190MixupTrain:  epoch  0, batch   107 | loss: 3.2707012MixupTrain:  epoch  0, batch   108 | loss: 3.4794273MixupTrain:  epoch  0, batch   109 | loss: 3.2886267MixupTrain:  epoch  0, batch   110 | loss: 3.3337257MixupTrain:  epoch  0, batch   111 | loss: 3.7705503MixupTrain:  epoch  0, batch   112 | loss: 3.4071405MixupTrain:  epoch  0, batch   113 | loss: 3.3892877MixupTrain:  epoch  0, batch   114 | loss: 3.4110940MixupTrain:  epoch  0, batch   115 | loss: 3.3827381MixupTrain:  epoch  0, batch   116 | loss: 3.1365798MixupTrain:  epoch  0, batch   117 | loss: 3.4222655MixupTrain:  epoch  0, batch   118 | loss: 3.4860249MixupTrain:  epoch  0, batch   119 | loss: 3.2543952MixupTrain:  epoch  0, batch   120 | loss: 3.4452367MixupTrain:  epoch  0, batch   121 | loss: 3.4991403MixupTrain:  epoch  0, batch   122 | loss: 3.3121264MixupTrain:  epoch  0, batch   123 | loss: 3.2751491MixupTrain:  epoch  0, batch   124 | loss: 3.2268367MixupTrain:  epoch  0, batch   125 | loss: 3.2277477MixupTrain:  epoch  0, batch   126 | loss: 3.3388221MixupTrain:  epoch  0, batch   127 | loss: 3.2437546MixupTrain:  epoch  0, batch   128 | loss: 3.2940054MixupTrain:  epoch  0, batch   129 | loss: 3.2141538MixupTrain:  epoch  0, batch   130 | loss: 3.5804820MixupTrain:  epoch  0, batch   131 | loss: 3.3679214MixupTrain:  epoch  0, batch   132 | loss: 3.2447035MixupTrain:  epoch  0, batch   133 | loss: 3.3455334MixupTrain:  epoch  0, batch   134 | loss: 3.2812171MixupTrain:  epoch  0, batch   135 | loss: 3.1536093MixupTrain:  epoch  0, batch   136 | loss: 3.2636781MixupTrain:  epoch  0, batch   137 | loss: 3.5006554MixupTrain:  epoch  0, batch   138 | loss: 3.3368521MixupTrain:  epoch  0, batch   139 | loss: 3.2938933MixupTrain:  epoch  0, batch   140 | loss: 3.2616966MixupTrain:  epoch  0, batch   141 | loss: 3.1273885MixupTrain:  epoch  0, batch   142 | loss: 3.2681808MixupTrain:  epoch  0, batch   143 | loss: 3.2361727MixupTrain:  epoch  0, batch   144 | loss: 3.1834724MixupTrain:  epoch  0, batch   145 | loss: 3.3344474MixupTrain:  epoch  0, batch   146 | loss: 3.2812285MixupTrain:  epoch  0, batch   147 | loss: 3.4213471MixupTrain:  epoch  0, batch   148 | loss: 3.1935318MixupTrain:  epoch  0, batch   149 | loss: 3.1309357MixupTrain:  epoch  0, batch   150 | loss: 3.2440901MixupTrain:  epoch  0, batch   151 | loss: 3.2699916MixupTrain:  epoch  0, batch   152 | loss: 3.3270550MixupTrain:  epoch  0, batch   153 | loss: 3.1139119MixupTrain:  epoch  0, batch   154 | loss: 3.0828335MixupTrain:  epoch  0, batch   155 | loss: 3.0590973MixupTrain:  epoch  0, batch   156 | loss: 3.2074265MixupTrain:  epoch  0, batch   157 | loss: 3.1510119MixupTrain:  epoch  0, batch   158 | loss: 3.2210097MixupTrain:  epoch  0, batch   159 | loss: 3.2537355MixupTrain:  epoch  0, batch   160 | loss: 3.3528714MixupTrain:  epoch  0, batch   161 | loss: 3.3411832MixupTrain:  epoch  0, batch   162 | loss: 3.2970190MixupTrain:  epoch  0, batch   163 | loss: 3.2118278MixupTrain:  epoch  0, batch   164 | loss: 3.1294556MixupTrain:  epoch  0, batch   165 | loss: 3.0771163MixupTrain:  epoch  0, batch   166 | loss: 3.1828082MixupTrain:  epoch  0, batch   167 | loss: 3.1158032MixupTrain:  epoch  0, batch   168 | loss: 3.3532310MixupTrain:  epoch  0, batch   169 | loss: 3.1131580MixupTrain:  epoch  0, batch   170 | loss: 3.1904948MixupTrain:  epoch  0, batch   171 | loss: 3.2028251MixupTrain:  epoch  0, batch   172 | loss: 3.2939768MixupTrain:  epoch  0, batch   173 | loss: 3.1393185MixupTrain:  epoch  0, batch   174 | loss: 3.1853797MixupTrain:  epoch  0, batch   175 | loss: 3.3917346MixupTrain:  epoch  0, batch   176 | loss: 3.2011156MixupTrain:  epoch  0, batch   177 | loss: 3.0048749MixupTrain:  epoch  0, batch   178 | loss: 3.3960242MixupTrain:  epoch  0, batch   179 | loss: 3.3602533MixupTrain:  epoch  0, batch   180 | loss: 3.1744921MixupTrain:  epoch  0, batch   181 | loss: 3.1520071MixupTrain:  epoch  0, batch   182 | loss: 3.1792030MixupTrain:  epoch  0, batch   183 | loss: 3.1542883MixupTrain:  epoch  0, batch   184 | loss: 3.1182032MixupTrain:  epoch  0, batch   185 | loss: 3.1989913MixupTrain:  epoch  0, batch   186 | loss: 3.1115847MixupTrain:  epoch  0, batch   187 | loss: 3.1401846MixupTrain:  epoch  0, batch   188 | loss: 2.9933867MixupTrain:  epoch  0, batch   189 | loss: 3.3105261MixupTrain:  epoch  0, batch   190 | loss: 3.1043613MixupTrain:  epoch  0, batch   191 | loss: 3.0726399MixupTrain:  epoch  0, batch   192 | loss: 3.0429211MixupTrain:  epoch  0, batch   193 | loss: 3.3730321MixupTrain:  epoch  0, batch   194 | loss: 3.1482337MixupTrain:  epoch  0, batch   195 | loss: 3.1617601MixupTrain:  epoch  0, batch   196 | loss: 3.0826411MixupTrain:  epoch  0, batch   197 | loss: 3.1307707MixupTrain:  epoch  0, batch   198 | loss: 3.0408430MixupTrain:  epoch  0, batch   199 | loss: 3.3210742MixupTrain:  epoch  0, batch   200 | loss: 3.0283947MixupTrain:  epoch  0, batch   201 | loss: 3.0870924MixupTrain:  epoch  0, batch   202 | loss: 3.0522547MixupTrain:  epoch  0, batch   203 | loss: 3.0678239MixupTrain:  epoch  0, batch   204 | loss: 3.1203547MixupTrain:  epoch  0, batch   205 | loss: 3.0859885MixupTrain:  epoch  0, batch   206 | loss: 3.2447190MixupTrain:  epoch  0, batch   207 | loss: 3.1731272MixupTrain:  epoch  0, batch   208 | loss: 3.0327730MixupTrain:  epoch  0, batch   209 | loss: 3.2549682MixupTrain:  epoch  0, batch   210 | loss: 3.1891816MixupTrain:  epoch  0, batch   211 | loss: 3.2059891MixupTrain:  epoch  0, batch   212 | loss: 3.0625930MixupTrain:  epoch  0, batch   213 | loss: 3.2782042MixupTrain:  epoch  0, batch   214 | loss: 3.1167884MixupTrain:  epoch  0, batch   215 | loss: 3.2078879MixupTrain:  epoch  0, batch   216 | loss: 3.3128347MixupTrain:  epoch  0, batch   217 | loss: 3.0602407MixupTrain:  epoch  0, batch   218 | loss: 3.1943979MixupTrain:  epoch  0, batch   219 | loss: 3.2907672MixupTrain:  epoch  0, batch   220 | loss: 3.2316132MixupTrain:  epoch  0, batch   221 | loss: 3.1285777MixupTrain:  epoch  0, batch   222 | loss: 3.2007279MixupTrain:  epoch  0, batch   223 | loss: 2.9529729MixupTrain:  epoch  0, batch   224 | loss: 3.3230107MixupTrain:  epoch  0, batch   225 | loss: 3.2809734MixupTrain:  epoch  0, batch   226 | loss: 3.1619983MixupTrain:  epoch  0, batch   227 | loss: 3.2307644
MemoryTrain:  epoch  0, batch     0 | loss: 1.5743994MemoryTrain:  epoch  0, batch     1 | loss: 1.9951081MemoryTrain:  epoch  0, batch     2 | loss: 2.0097282MemoryTrain:  epoch  0, batch     3 | loss: 2.3826203MemoryTrain:  epoch  1, batch     0 | loss: 1.4759533MemoryTrain:  epoch  1, batch     1 | loss: 1.4976951MemoryTrain:  epoch  1, batch     2 | loss: 1.6243705MemoryTrain:  epoch  1, batch     3 | loss: 1.3093504MemoryTrain:  epoch  2, batch     0 | loss: 1.3177469MemoryTrain:  epoch  2, batch     1 | loss: 1.3110350MemoryTrain:  epoch  2, batch     2 | loss: 1.4036890MemoryTrain:  epoch  2, batch     3 | loss: 1.3061745MemoryTrain:  epoch  3, batch     0 | loss: 1.4122882MemoryTrain:  epoch  3, batch     1 | loss: 1.3504007MemoryTrain:  epoch  3, batch     2 | loss: 1.3544912MemoryTrain:  epoch  3, batch     3 | loss: 1.3051322MemoryTrain:  epoch  4, batch     0 | loss: 1.4010004MemoryTrain:  epoch  4, batch     1 | loss: 1.2359729MemoryTrain:  epoch  4, batch     2 | loss: 1.2451818MemoryTrain:  epoch  4, batch     3 | loss: 1.2995958MemoryTrain:  epoch  5, batch     0 | loss: 1.2878953MemoryTrain:  epoch  5, batch     1 | loss: 1.3055873MemoryTrain:  epoch  5, batch     2 | loss: 1.1962783MemoryTrain:  epoch  5, batch     3 | loss: 1.3210994MemoryTrain:  epoch  6, batch     0 | loss: 1.2899804MemoryTrain:  epoch  6, batch     1 | loss: 1.3606057MemoryTrain:  epoch  6, batch     2 | loss: 1.2596403MemoryTrain:  epoch  6, batch     3 | loss: 1.2065210MemoryTrain:  epoch  7, batch     0 | loss: 1.2191699MemoryTrain:  epoch  7, batch     1 | loss: 1.3006139MemoryTrain:  epoch  7, batch     2 | loss: 1.2518761MemoryTrain:  epoch  7, batch     3 | loss: 1.2162019MemoryTrain:  epoch  8, batch     0 | loss: 1.2159412MemoryTrain:  epoch  8, batch     1 | loss: 1.2444862MemoryTrain:  epoch  8, batch     2 | loss: 1.2614329MemoryTrain:  epoch  8, batch     3 | loss: 1.1953266MemoryTrain:  epoch  9, batch     0 | loss: 1.2437240MemoryTrain:  epoch  9, batch     1 | loss: 1.2142200MemoryTrain:  epoch  9, batch     2 | loss: 1.2241070MemoryTrain:  epoch  9, batch     3 | loss: 1.2908486
[EVAL] batch:    0 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    1 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 97.92%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 95.31%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 95.00%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 94.79%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 94.64%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 92.19%   [EVAL] batch:    8 | acc: 18.75%,  total acc: 84.03%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 82.50%   [EVAL] batch:   10 | acc: 75.00%,  total acc: 81.82%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:   12 | acc: 43.75%,  total acc: 78.37%   
[EVAL] batch:    0 | acc: 31.25%,  total acc: 31.25%   [EVAL] batch:    1 | acc: 50.00%,  total acc: 40.62%   [EVAL] batch:    2 | acc: 62.50%,  total acc: 47.92%   [EVAL] batch:    3 | acc: 25.00%,  total acc: 42.19%   [EVAL] batch:    4 | acc: 31.25%,  total acc: 40.00%   [EVAL] batch:    5 | acc: 37.50%,  total acc: 39.58%   [EVAL] batch:    6 | acc: 37.50%,  total acc: 39.29%   [EVAL] batch:    7 | acc: 50.00%,  total acc: 40.62%   [EVAL] batch:    8 | acc: 37.50%,  total acc: 40.28%   [EVAL] batch:    9 | acc: 37.50%,  total acc: 40.00%   [EVAL] batch:   10 | acc: 43.75%,  total acc: 40.34%   [EVAL] batch:   11 | acc: 6.25%,  total acc: 37.50%   [EVAL] batch:   12 | acc: 37.50%,  total acc: 37.50%   [EVAL] batch:   13 | acc: 37.50%,  total acc: 37.50%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 39.58%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 40.62%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 42.65%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 43.75%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 45.07%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 46.88%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 49.40%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 51.42%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 53.53%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 55.47%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 57.25%   [EVAL] batch:   25 | acc: 93.75%,  total acc: 58.65%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 59.95%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 61.38%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 62.72%   [EVAL] batch:   29 | acc: 68.75%,  total acc: 62.92%   [EVAL] batch:   30 | acc: 81.25%,  total acc: 63.51%   [EVAL] batch:   31 | acc: 87.50%,  total acc: 64.26%   [EVAL] batch:   32 | acc: 87.50%,  total acc: 64.96%   [EVAL] batch:   33 | acc: 31.25%,  total acc: 63.97%   [EVAL] batch:   34 | acc: 37.50%,  total acc: 63.21%   [EVAL] batch:   35 | acc: 12.50%,  total acc: 61.81%   [EVAL] batch:   36 | acc: 0.00%,  total acc: 60.14%   [EVAL] batch:   37 | acc: 25.00%,  total acc: 59.21%   [EVAL] batch:   38 | acc: 12.50%,  total acc: 58.01%   [EVAL] batch:   39 | acc: 81.25%,  total acc: 58.59%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 59.60%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 60.57%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 61.34%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 61.79%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 62.50%   [EVAL] batch:   45 | acc: 87.50%,  total acc: 63.04%   [EVAL] batch:   46 | acc: 62.50%,  total acc: 63.03%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 63.67%   [EVAL] batch:   48 | acc: 87.50%,  total acc: 64.16%   [EVAL] batch:   49 | acc: 75.00%,  total acc: 64.38%   [EVAL] batch:   50 | acc: 75.00%,  total acc: 64.58%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 65.26%   [EVAL] batch:   52 | acc: 81.25%,  total acc: 65.57%   [EVAL] batch:   53 | acc: 93.75%,  total acc: 66.09%   [EVAL] batch:   54 | acc: 100.00%,  total acc: 66.70%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 67.19%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 67.65%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 68.00%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 68.43%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 68.85%   [EVAL] batch:   60 | acc: 81.25%,  total acc: 69.06%   [EVAL] batch:   61 | acc: 25.00%,  total acc: 68.35%   [EVAL] batch:   62 | acc: 62.50%,  total acc: 68.25%   [EVAL] batch:   63 | acc: 75.00%,  total acc: 68.36%   [EVAL] batch:   64 | acc: 75.00%,  total acc: 68.46%   [EVAL] batch:   65 | acc: 56.25%,  total acc: 68.28%   
cur_acc:  ['0.8561', '0.8571', '0.7422', '0.7837']
his_acc:  ['0.8561', '0.8338', '0.7234', '0.6828']
CurrentTrain: epoch  0, batch     0 | loss: 7.1153145CurrentTrain: epoch  0, batch     1 | loss: 7.7457833CurrentTrain: epoch  1, batch     0 | loss: 6.3476954CurrentTrain: epoch  1, batch     1 | loss: 7.2604446CurrentTrain: epoch  2, batch     0 | loss: 6.3192010CurrentTrain: epoch  2, batch     1 | loss: 5.4774876CurrentTrain: epoch  3, batch     0 | loss: 6.1067224CurrentTrain: epoch  3, batch     1 | loss: 4.9405847CurrentTrain: epoch  4, batch     0 | loss: 5.3977814CurrentTrain: epoch  4, batch     1 | loss: 5.1358404CurrentTrain: epoch  5, batch     0 | loss: 4.3213158CurrentTrain: epoch  5, batch     1 | loss: 6.5706730CurrentTrain: epoch  6, batch     0 | loss: 5.2314386CurrentTrain: epoch  6, batch     1 | loss: 4.2509131CurrentTrain: epoch  7, batch     0 | loss: 4.6950960CurrentTrain: epoch  7, batch     1 | loss: 4.3949227CurrentTrain: epoch  8, batch     0 | loss: 4.2042942CurrentTrain: epoch  8, batch     1 | loss: 5.1473031CurrentTrain: epoch  9, batch     0 | loss: 4.8469315CurrentTrain: epoch  9, batch     1 | loss: 3.5213201
#### organization subsidiaries ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization subsidiaries" means an organization 's subsidiary is an organization.
Here is an example:
Relation: organization subsidiaries
Context: venture fund buys sporting chain highland capital 's consumer fund includes lululemon athletica , a yoga retailer , and o beverages , a flavored water company developed by tom first , one of the two `` juice guys '' who cofounded nantucket nectars .
Head Entity: highland capital
Tail Entity: o beverages
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization subsidiaries":
Relation: organization subsidiaries  
Context: The tech giant Alphabet Inc. has expanded its portfolio by acquiring Fitbit, a company known for its fitness tracking devices, which now operates as a subsidiary under the Google brand.  
Head Entity: Alphabet Inc.  
Tail Entity: Fitbit  

Relation: organization subsidiaries  
Context: The automotive manufacturer General Motors has announced that its electric vehicle division, Chevrolet, will be launching a new line of electric cars, further solidifying its position in the market as a subsidiary of the larger corporation.  
Head Entity: General Motors  
Tail Entity: Chevrolet  
#### organization parents ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization parents" means an organization 's parent is an organization.
Here is an example:
Relation: organization parents
Context: aseel kami , tim cocks , abdul-rahman taher , tim cocks and michael christie -lrb- reuters -rrb- add an intriguing note in the speculation over who 's responsible for the recent bombings : `` but a senior iraqi intelligence source , who declined to be named , said there was evidence the bombs could be the work of the militant badr organisation , the armed wing of the supreme islamic iraqi council -lrb- isci -rrb- .
Head Entity: badr organisation
Tail Entity: supreme islamic iraqi council
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization parents":
Relation: organization parents  
Context: In a recent merger, the tech giant Alphabet Inc. announced that it would acquire the innovative startup DeepMind Technologies, which has been a subsidiary of Alphabet since 2015. This acquisition is expected to enhance Alphabet's capabilities in artificial intelligence.  
Head Entity: DeepMind Technologies  
Tail Entity: Alphabet Inc.  

Relation: organization parents  
Context: The historical records indicate that the renowned publishing house Penguin Random House was formed through the merger of Penguin Group and Random House, which were both significant players in the literary world. This merger has allowed them to dominate the market for decades.  
Head Entity: Penguin Random House  
Tail Entity: Random House  
#### organization alternate names ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization alternate names" means a person 's alias is a person.
Here is an example:
Relation: organization alternate names
Context: the talks between graham and pak ui-chun were held in a `` friendly '' manner , the official korean central news agency -lrb- kcna -rrb- said , without giving details .
Head Entity: kcna
Tail Entity: korean central news agency
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization alternate names":
Relation: organization alternate names  
Context: The International Business Machines Corporation, commonly known as IBM, has been a leader in technology for decades.  
Head Entity: IBM  
Tail Entity: International Business Machines Corporation  

Relation: organization alternate names  
Context: The World Health Organization, often referred to as WHO, has been instrumental in addressing global health issues.  
Head Entity: WHO  
Tail Entity: World Health Organization  
#### organization city of headquarters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization city of headquarters" means an organization is located in a city.
Here is an example:
Relation: organization city of headquarters
Context: ------ london 2008-05-20 07:23:45 utc enodis plc endorses sweetened takeover bid by us company manitowoc illinois tool works of glenville , illinois , which had offered 282 pence -lrb- us$ 551 euro3 54 -rrb- per share , said monday that it was considering its position .
Head Entity: illinois tool works
Tail Entity: glenville
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization city of headquarters":
Relation: organization city of headquarters  
Context: ------ in 2015, tech giant apple inc. announced plans to expand its operations in cupertino, california, where it has been headquartered since its founding.  
Head Entity: apple inc.  
Tail Entity: cupertino  

Relation: organization city of headquarters  
Context: ------ the multinational corporation samsung electronics is based in suwon, south korea, and has been a leader in the technology sector for decades.  
Head Entity: samsung electronics  
Tail Entity: suwon  
#### person siblings ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person siblings" means a person 's sibling is a person.
Here is an example:
Relation: person siblings
Context: `` holly and sanjaya are headed to -lsb- the hawaiian island of -rsb- kauai tomorrow morning so she can meet his parents . ''
Head Entity: she
Tail Entity: sanjaya
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person siblings":
Relation: person siblings  
Context: `` during the family reunion, john introduced his sister to everyone, and they all had a great time reminiscing about their childhood. ''  
Head Entity: his sister  
Tail Entity: john  

Relation: person siblings  
Context: `` after the graduation ceremony, emily celebrated with her brother, who had just completed his degree in engineering. ''  
Head Entity: her brother  
Tail Entity: emily  
Mixup data size:  5050
MixupTrain:  epoch  0, batch     0 | loss: 4.6928940MixupTrain:  epoch  0, batch     1 | loss: 4.4578428MixupTrain:  epoch  0, batch     2 | loss: 4.1805286MixupTrain:  epoch  0, batch     3 | loss: 4.5316072MixupTrain:  epoch  0, batch     4 | loss: 4.0368776MixupTrain:  epoch  0, batch     5 | loss: 4.2894654MixupTrain:  epoch  0, batch     6 | loss: 4.1095161MixupTrain:  epoch  0, batch     7 | loss: 4.8346987MixupTrain:  epoch  0, batch     8 | loss: 3.9726920MixupTrain:  epoch  0, batch     9 | loss: 4.0406284MixupTrain:  epoch  0, batch    10 | loss: 4.1733341MixupTrain:  epoch  0, batch    11 | loss: 3.8655252MixupTrain:  epoch  0, batch    12 | loss: 4.2716179MixupTrain:  epoch  0, batch    13 | loss: 4.2669592MixupTrain:  epoch  0, batch    14 | loss: 3.8572388MixupTrain:  epoch  0, batch    15 | loss: 4.3362226MixupTrain:  epoch  0, batch    16 | loss: 3.8689246MixupTrain:  epoch  0, batch    17 | loss: 3.7310481MixupTrain:  epoch  0, batch    18 | loss: 4.2380843MixupTrain:  epoch  0, batch    19 | loss: 3.6613936MixupTrain:  epoch  0, batch    20 | loss: 3.6193385MixupTrain:  epoch  0, batch    21 | loss: 3.8308952MixupTrain:  epoch  0, batch    22 | loss: 3.9210451MixupTrain:  epoch  0, batch    23 | loss: 3.9071460MixupTrain:  epoch  0, batch    24 | loss: 3.4815550MixupTrain:  epoch  0, batch    25 | loss: 3.5176420MixupTrain:  epoch  0, batch    26 | loss: 3.8925409MixupTrain:  epoch  0, batch    27 | loss: 4.1703820MixupTrain:  epoch  0, batch    28 | loss: 3.9646440MixupTrain:  epoch  0, batch    29 | loss: 3.7978907MixupTrain:  epoch  0, batch    30 | loss: 3.9599986MixupTrain:  epoch  0, batch    31 | loss: 3.3075290MixupTrain:  epoch  0, batch    32 | loss: 3.4775541MixupTrain:  epoch  0, batch    33 | loss: 3.7204471MixupTrain:  epoch  0, batch    34 | loss: 3.5566204MixupTrain:  epoch  0, batch    35 | loss: 3.9266334MixupTrain:  epoch  0, batch    36 | loss: 3.4703674MixupTrain:  epoch  0, batch    37 | loss: 3.7286921MixupTrain:  epoch  0, batch    38 | loss: 3.7934375MixupTrain:  epoch  0, batch    39 | loss: 3.3841372MixupTrain:  epoch  0, batch    40 | loss: 3.3214316MixupTrain:  epoch  0, batch    41 | loss: 3.5709248MixupTrain:  epoch  0, batch    42 | loss: 3.1958246MixupTrain:  epoch  0, batch    43 | loss: 4.0510283MixupTrain:  epoch  0, batch    44 | loss: 3.4282107MixupTrain:  epoch  0, batch    45 | loss: 3.5637784MixupTrain:  epoch  0, batch    46 | loss: 4.1040325MixupTrain:  epoch  0, batch    47 | loss: 3.6585915MixupTrain:  epoch  0, batch    48 | loss: 3.8382907MixupTrain:  epoch  0, batch    49 | loss: 3.6381056MixupTrain:  epoch  0, batch    50 | loss: 3.6627455MixupTrain:  epoch  0, batch    51 | loss: 3.6811223MixupTrain:  epoch  0, batch    52 | loss: 3.9716494MixupTrain:  epoch  0, batch    53 | loss: 3.7514272MixupTrain:  epoch  0, batch    54 | loss: 3.5888219MixupTrain:  epoch  0, batch    55 | loss: 3.5858893MixupTrain:  epoch  0, batch    56 | loss: 3.6143436MixupTrain:  epoch  0, batch    57 | loss: 2.9582267MixupTrain:  epoch  0, batch    58 | loss: 3.3504548MixupTrain:  epoch  0, batch    59 | loss: 3.4575095MixupTrain:  epoch  0, batch    60 | loss: 3.2970245MixupTrain:  epoch  0, batch    61 | loss: 3.2832770MixupTrain:  epoch  0, batch    62 | loss: 3.3947895MixupTrain:  epoch  0, batch    63 | loss: 3.4202495MixupTrain:  epoch  0, batch    64 | loss: 3.3946338MixupTrain:  epoch  0, batch    65 | loss: 3.5173080MixupTrain:  epoch  0, batch    66 | loss: 3.4084373MixupTrain:  epoch  0, batch    67 | loss: 3.9481823MixupTrain:  epoch  0, batch    68 | loss: 3.7037141MixupTrain:  epoch  0, batch    69 | loss: 3.7101011MixupTrain:  epoch  0, batch    70 | loss: 3.7563655MixupTrain:  epoch  0, batch    71 | loss: 3.3768992MixupTrain:  epoch  0, batch    72 | loss: 3.0550737MixupTrain:  epoch  0, batch    73 | loss: 3.4391189MixupTrain:  epoch  0, batch    74 | loss: 3.2913661MixupTrain:  epoch  0, batch    75 | loss: 3.3175998MixupTrain:  epoch  0, batch    76 | loss: 3.5180840MixupTrain:  epoch  0, batch    77 | loss: 3.8897457MixupTrain:  epoch  0, batch    78 | loss: 3.1969075MixupTrain:  epoch  0, batch    79 | loss: 3.6100273MixupTrain:  epoch  0, batch    80 | loss: 3.4011025MixupTrain:  epoch  0, batch    81 | loss: 3.6381512MixupTrain:  epoch  0, batch    82 | loss: 3.4077151MixupTrain:  epoch  0, batch    83 | loss: 2.9570522MixupTrain:  epoch  0, batch    84 | loss: 3.1859250MixupTrain:  epoch  0, batch    85 | loss: 3.4682460MixupTrain:  epoch  0, batch    86 | loss: 3.4961135MixupTrain:  epoch  0, batch    87 | loss: 3.3188624MixupTrain:  epoch  0, batch    88 | loss: 3.5923533MixupTrain:  epoch  0, batch    89 | loss: 3.2693243MixupTrain:  epoch  0, batch    90 | loss: 3.3299215MixupTrain:  epoch  0, batch    91 | loss: 3.2604179MixupTrain:  epoch  0, batch    92 | loss: 3.3328204MixupTrain:  epoch  0, batch    93 | loss: 3.1037521MixupTrain:  epoch  0, batch    94 | loss: 3.5592771MixupTrain:  epoch  0, batch    95 | loss: 3.2926636MixupTrain:  epoch  0, batch    96 | loss: 3.4027472MixupTrain:  epoch  0, batch    97 | loss: 3.4778094MixupTrain:  epoch  0, batch    98 | loss: 3.5161448MixupTrain:  epoch  0, batch    99 | loss: 3.1913490MixupTrain:  epoch  0, batch   100 | loss: 3.2992861MixupTrain:  epoch  0, batch   101 | loss: 3.4386368MixupTrain:  epoch  0, batch   102 | loss: 3.2062612MixupTrain:  epoch  0, batch   103 | loss: 3.2138762MixupTrain:  epoch  0, batch   104 | loss: 3.3897002MixupTrain:  epoch  0, batch   105 | loss: 3.3547297MixupTrain:  epoch  0, batch   106 | loss: 3.2994246MixupTrain:  epoch  0, batch   107 | loss: 3.3904212MixupTrain:  epoch  0, batch   108 | loss: 3.2064648MixupTrain:  epoch  0, batch   109 | loss: 3.0017834MixupTrain:  epoch  0, batch   110 | loss: 3.0669279MixupTrain:  epoch  0, batch   111 | loss: 3.7164712MixupTrain:  epoch  0, batch   112 | loss: 3.4340901MixupTrain:  epoch  0, batch   113 | loss: 3.7434287MixupTrain:  epoch  0, batch   114 | loss: 3.4198947MixupTrain:  epoch  0, batch   115 | loss: 3.2798452MixupTrain:  epoch  0, batch   116 | loss: 3.2211566MixupTrain:  epoch  0, batch   117 | loss: 3.0541449MixupTrain:  epoch  0, batch   118 | loss: 3.3477063MixupTrain:  epoch  0, batch   119 | loss: 3.1059127MixupTrain:  epoch  0, batch   120 | loss: 3.1328166MixupTrain:  epoch  0, batch   121 | loss: 3.4165645MixupTrain:  epoch  0, batch   122 | loss: 3.4115243MixupTrain:  epoch  0, batch   123 | loss: 3.2981787MixupTrain:  epoch  0, batch   124 | loss: 3.4366117MixupTrain:  epoch  0, batch   125 | loss: 3.0788894MixupTrain:  epoch  0, batch   126 | loss: 3.5013533MixupTrain:  epoch  0, batch   127 | loss: 3.0954719MixupTrain:  epoch  0, batch   128 | loss: 3.1892309MixupTrain:  epoch  0, batch   129 | loss: 3.6561151MixupTrain:  epoch  0, batch   130 | loss: 3.1206477MixupTrain:  epoch  0, batch   131 | loss: 3.2927790MixupTrain:  epoch  0, batch   132 | loss: 3.2783065MixupTrain:  epoch  0, batch   133 | loss: 3.3224397MixupTrain:  epoch  0, batch   134 | loss: 3.0685298MixupTrain:  epoch  0, batch   135 | loss: 3.1137109MixupTrain:  epoch  0, batch   136 | loss: 3.2723444MixupTrain:  epoch  0, batch   137 | loss: 3.2593575MixupTrain:  epoch  0, batch   138 | loss: 3.2383685MixupTrain:  epoch  0, batch   139 | loss: 3.1456690MixupTrain:  epoch  0, batch   140 | loss: 3.0072737MixupTrain:  epoch  0, batch   141 | loss: 2.9883103MixupTrain:  epoch  0, batch   142 | loss: 3.3756394MixupTrain:  epoch  0, batch   143 | loss: 3.3898072MixupTrain:  epoch  0, batch   144 | loss: 3.0071535MixupTrain:  epoch  0, batch   145 | loss: 3.1686294MixupTrain:  epoch  0, batch   146 | loss: 3.0875573MixupTrain:  epoch  0, batch   147 | loss: 3.2623348MixupTrain:  epoch  0, batch   148 | loss: 3.2929716MixupTrain:  epoch  0, batch   149 | loss: 3.1931682MixupTrain:  epoch  0, batch   150 | loss: 3.2588234MixupTrain:  epoch  0, batch   151 | loss: 3.1023560MixupTrain:  epoch  0, batch   152 | loss: 3.0233722MixupTrain:  epoch  0, batch   153 | loss: 3.1925473MixupTrain:  epoch  0, batch   154 | loss: 3.1648407MixupTrain:  epoch  0, batch   155 | loss: 3.4103107MixupTrain:  epoch  0, batch   156 | loss: 3.0000179MixupTrain:  epoch  0, batch   157 | loss: 3.3074422MixupTrain:  epoch  0, batch   158 | loss: 3.3248107MixupTrain:  epoch  0, batch   159 | loss: 2.9857211MixupTrain:  epoch  0, batch   160 | loss: 2.9680603MixupTrain:  epoch  0, batch   161 | loss: 3.2466254MixupTrain:  epoch  0, batch   162 | loss: 3.1639807MixupTrain:  epoch  0, batch   163 | loss: 3.1342549MixupTrain:  epoch  0, batch   164 | loss: 3.2476945MixupTrain:  epoch  0, batch   165 | loss: 3.1766901MixupTrain:  epoch  0, batch   166 | loss: 3.1392694MixupTrain:  epoch  0, batch   167 | loss: 3.2347021MixupTrain:  epoch  0, batch   168 | loss: 3.2403407MixupTrain:  epoch  0, batch   169 | loss: 3.1516266MixupTrain:  epoch  0, batch   170 | loss: 3.4782379MixupTrain:  epoch  0, batch   171 | loss: 3.1045823MixupTrain:  epoch  0, batch   172 | loss: 3.1879606MixupTrain:  epoch  0, batch   173 | loss: 3.0719869MixupTrain:  epoch  0, batch   174 | loss: 3.0032082MixupTrain:  epoch  0, batch   175 | loss: 3.2898116MixupTrain:  epoch  0, batch   176 | loss: 3.1344063MixupTrain:  epoch  0, batch   177 | loss: 3.2095680MixupTrain:  epoch  0, batch   178 | loss: 3.2203856MixupTrain:  epoch  0, batch   179 | loss: 3.2842598MixupTrain:  epoch  0, batch   180 | loss: 3.2329938MixupTrain:  epoch  0, batch   181 | loss: 3.4609070MixupTrain:  epoch  0, batch   182 | loss: 3.1524880MixupTrain:  epoch  0, batch   183 | loss: 3.0725608MixupTrain:  epoch  0, batch   184 | loss: 3.4921801MixupTrain:  epoch  0, batch   185 | loss: 3.2524881MixupTrain:  epoch  0, batch   186 | loss: 3.3108516MixupTrain:  epoch  0, batch   187 | loss: 3.1469798MixupTrain:  epoch  0, batch   188 | loss: 2.9822383MixupTrain:  epoch  0, batch   189 | loss: 2.9834692MixupTrain:  epoch  0, batch   190 | loss: 3.1226482MixupTrain:  epoch  0, batch   191 | loss: 3.2521744MixupTrain:  epoch  0, batch   192 | loss: 3.1162143MixupTrain:  epoch  0, batch   193 | loss: 3.1509905MixupTrain:  epoch  0, batch   194 | loss: 3.1689548MixupTrain:  epoch  0, batch   195 | loss: 3.1755047MixupTrain:  epoch  0, batch   196 | loss: 3.2764871MixupTrain:  epoch  0, batch   197 | loss: 2.9783340MixupTrain:  epoch  0, batch   198 | loss: 3.1856501MixupTrain:  epoch  0, batch   199 | loss: 3.5283923MixupTrain:  epoch  0, batch   200 | loss: 3.2176914MixupTrain:  epoch  0, batch   201 | loss: 3.0722947MixupTrain:  epoch  0, batch   202 | loss: 3.0204048MixupTrain:  epoch  0, batch   203 | loss: 3.0963199MixupTrain:  epoch  0, batch   204 | loss: 3.0095959MixupTrain:  epoch  0, batch   205 | loss: 3.3648186MixupTrain:  epoch  0, batch   206 | loss: 3.2859342MixupTrain:  epoch  0, batch   207 | loss: 3.2399526MixupTrain:  epoch  0, batch   208 | loss: 2.9426184MixupTrain:  epoch  0, batch   209 | loss: 2.8515482MixupTrain:  epoch  0, batch   210 | loss: 3.1514692MixupTrain:  epoch  0, batch   211 | loss: 3.0450854MixupTrain:  epoch  0, batch   212 | loss: 3.0192249MixupTrain:  epoch  0, batch   213 | loss: 3.0283332MixupTrain:  epoch  0, batch   214 | loss: 2.9105315MixupTrain:  epoch  0, batch   215 | loss: 2.9749758MixupTrain:  epoch  0, batch   216 | loss: 2.7804880MixupTrain:  epoch  0, batch   217 | loss: 3.1182995MixupTrain:  epoch  0, batch   218 | loss: 3.3157210MixupTrain:  epoch  0, batch   219 | loss: 2.6432579MixupTrain:  epoch  0, batch   220 | loss: 2.9185672MixupTrain:  epoch  0, batch   221 | loss: 3.1549716MixupTrain:  epoch  0, batch   222 | loss: 3.0207767MixupTrain:  epoch  0, batch   223 | loss: 3.1864347MixupTrain:  epoch  0, batch   224 | loss: 3.1991093MixupTrain:  epoch  0, batch   225 | loss: 3.0158415MixupTrain:  epoch  0, batch   226 | loss: 3.1679244MixupTrain:  epoch  0, batch   227 | loss: 2.8748193MixupTrain:  epoch  0, batch   228 | loss: 3.0491362MixupTrain:  epoch  0, batch   229 | loss: 3.1422758MixupTrain:  epoch  0, batch   230 | loss: 2.7266717MixupTrain:  epoch  0, batch   231 | loss: 3.3529868MixupTrain:  epoch  0, batch   232 | loss: 3.1427965MixupTrain:  epoch  0, batch   233 | loss: 3.1201684MixupTrain:  epoch  0, batch   234 | loss: 2.9409616MixupTrain:  epoch  0, batch   235 | loss: 3.0408592MixupTrain:  epoch  0, batch   236 | loss: 3.0399923MixupTrain:  epoch  0, batch   237 | loss: 3.0975885MixupTrain:  epoch  0, batch   238 | loss: 3.3320117MixupTrain:  epoch  0, batch   239 | loss: 3.0380023MixupTrain:  epoch  0, batch   240 | loss: 3.2177548MixupTrain:  epoch  0, batch   241 | loss: 3.0884032MixupTrain:  epoch  0, batch   242 | loss: 3.1334968MixupTrain:  epoch  0, batch   243 | loss: 3.0758612MixupTrain:  epoch  0, batch   244 | loss: 3.3700182MixupTrain:  epoch  0, batch   245 | loss: 3.1239274MixupTrain:  epoch  0, batch   246 | loss: 3.1152177MixupTrain:  epoch  0, batch   247 | loss: 3.1469259MixupTrain:  epoch  0, batch   248 | loss: 3.2367649MixupTrain:  epoch  0, batch   249 | loss: 3.2679167MixupTrain:  epoch  0, batch   250 | loss: 2.9797268MixupTrain:  epoch  0, batch   251 | loss: 3.0516176MixupTrain:  epoch  0, batch   252 | loss: 3.0930977MixupTrain:  epoch  0, batch   253 | loss: 3.1817136MixupTrain:  epoch  0, batch   254 | loss: 3.0721660MixupTrain:  epoch  0, batch   255 | loss: 3.0834842MixupTrain:  epoch  0, batch   256 | loss: 3.2371387MixupTrain:  epoch  0, batch   257 | loss: 3.3325081MixupTrain:  epoch  0, batch   258 | loss: 3.0799165MixupTrain:  epoch  0, batch   259 | loss: 3.0112362MixupTrain:  epoch  0, batch   260 | loss: 3.1166430MixupTrain:  epoch  0, batch   261 | loss: 3.0803485MixupTrain:  epoch  0, batch   262 | loss: 2.7337506MixupTrain:  epoch  0, batch   263 | loss: 3.1249597MixupTrain:  epoch  0, batch   264 | loss: 3.0684891MixupTrain:  epoch  0, batch   265 | loss: 3.1539454MixupTrain:  epoch  0, batch   266 | loss: 3.1909971MixupTrain:  epoch  0, batch   267 | loss: 3.0511484MixupTrain:  epoch  0, batch   268 | loss: 3.0183654MixupTrain:  epoch  0, batch   269 | loss: 2.9895477MixupTrain:  epoch  0, batch   270 | loss: 3.1403215MixupTrain:  epoch  0, batch   271 | loss: 2.9560480MixupTrain:  epoch  0, batch   272 | loss: 3.1421390MixupTrain:  epoch  0, batch   273 | loss: 2.9239144MixupTrain:  epoch  0, batch   274 | loss: 3.0845234MixupTrain:  epoch  0, batch   275 | loss: 3.1900847MixupTrain:  epoch  0, batch   276 | loss: 3.0180137MixupTrain:  epoch  0, batch   277 | loss: 2.9547181MixupTrain:  epoch  0, batch   278 | loss: 3.1858292MixupTrain:  epoch  0, batch   279 | loss: 2.8290021MixupTrain:  epoch  0, batch   280 | loss: 2.9423566MixupTrain:  epoch  0, batch   281 | loss: 2.7492301MixupTrain:  epoch  0, batch   282 | loss: 3.2500806MixupTrain:  epoch  0, batch   283 | loss: 2.9629130MixupTrain:  epoch  0, batch   284 | loss: 2.7302322MixupTrain:  epoch  0, batch   285 | loss: 2.9380364MixupTrain:  epoch  0, batch   286 | loss: 3.0241365MixupTrain:  epoch  0, batch   287 | loss: 2.8627923MixupTrain:  epoch  0, batch   288 | loss: 2.8864098MixupTrain:  epoch  0, batch   289 | loss: 3.2219253MixupTrain:  epoch  0, batch   290 | loss: 3.1637483MixupTrain:  epoch  0, batch   291 | loss: 2.9317672MixupTrain:  epoch  0, batch   292 | loss: 2.9571717MixupTrain:  epoch  0, batch   293 | loss: 3.0071778MixupTrain:  epoch  0, batch   294 | loss: 3.1552680MixupTrain:  epoch  0, batch   295 | loss: 2.9792724MixupTrain:  epoch  0, batch   296 | loss: 2.9790802MixupTrain:  epoch  0, batch   297 | loss: 2.8772941MixupTrain:  epoch  0, batch   298 | loss: 3.0669842MixupTrain:  epoch  0, batch   299 | loss: 3.0093982MixupTrain:  epoch  0, batch   300 | loss: 3.0375240MixupTrain:  epoch  0, batch   301 | loss: 3.1445158MixupTrain:  epoch  0, batch   302 | loss: 3.0403900MixupTrain:  epoch  0, batch   303 | loss: 3.2339711MixupTrain:  epoch  0, batch   304 | loss: 3.1053014MixupTrain:  epoch  0, batch   305 | loss: 3.0636666MixupTrain:  epoch  0, batch   306 | loss: 3.0957637MixupTrain:  epoch  0, batch   307 | loss: 3.0213270MixupTrain:  epoch  0, batch   308 | loss: 3.0989666MixupTrain:  epoch  0, batch   309 | loss: 3.0403705MixupTrain:  epoch  0, batch   310 | loss: 3.0280430MixupTrain:  epoch  0, batch   311 | loss: 3.1176670MixupTrain:  epoch  0, batch   312 | loss: 3.1820014MixupTrain:  epoch  0, batch   313 | loss: 3.0883651MixupTrain:  epoch  0, batch   314 | loss: 3.0285282MixupTrain:  epoch  0, batch   315 | loss: 2.8867052
MemoryTrain:  epoch  0, batch     0 | loss: 1.2903607MemoryTrain:  epoch  0, batch     1 | loss: 1.4113970MemoryTrain:  epoch  0, batch     2 | loss: 1.9071555MemoryTrain:  epoch  0, batch     3 | loss: 1.7690396MemoryTrain:  epoch  0, batch     4 | loss: 1.6815363MemoryTrain:  epoch  1, batch     0 | loss: 1.2737887MemoryTrain:  epoch  1, batch     1 | loss: 1.2974952MemoryTrain:  epoch  1, batch     2 | loss: 1.5220109MemoryTrain:  epoch  1, batch     3 | loss: 1.7112978MemoryTrain:  epoch  1, batch     4 | loss: 1.5195218MemoryTrain:  epoch  2, batch     0 | loss: 1.3802210MemoryTrain:  epoch  2, batch     1 | loss: 1.4272475MemoryTrain:  epoch  2, batch     2 | loss: 1.3703671MemoryTrain:  epoch  2, batch     3 | loss: 1.3617543MemoryTrain:  epoch  2, batch     4 | loss: 1.2633204MemoryTrain:  epoch  3, batch     0 | loss: 1.2563157MemoryTrain:  epoch  3, batch     1 | loss: 1.3484300MemoryTrain:  epoch  3, batch     2 | loss: 1.2970588MemoryTrain:  epoch  3, batch     3 | loss: 1.2747545MemoryTrain:  epoch  3, batch     4 | loss: 1.2866131MemoryTrain:  epoch  4, batch     0 | loss: 1.3142262MemoryTrain:  epoch  4, batch     1 | loss: 1.2211335MemoryTrain:  epoch  4, batch     2 | loss: 1.2384622MemoryTrain:  epoch  4, batch     3 | loss: 1.4675442MemoryTrain:  epoch  4, batch     4 | loss: 1.2244499MemoryTrain:  epoch  5, batch     0 | loss: 1.2879525MemoryTrain:  epoch  5, batch     1 | loss: 1.2864580MemoryTrain:  epoch  5, batch     2 | loss: 1.2493749MemoryTrain:  epoch  5, batch     3 | loss: 1.2437937MemoryTrain:  epoch  5, batch     4 | loss: 1.3177018MemoryTrain:  epoch  6, batch     0 | loss: 1.2015967MemoryTrain:  epoch  6, batch     1 | loss: 1.2512238MemoryTrain:  epoch  6, batch     2 | loss: 1.2590203MemoryTrain:  epoch  6, batch     3 | loss: 1.2400000MemoryTrain:  epoch  6, batch     4 | loss: 1.2549747MemoryTrain:  epoch  7, batch     0 | loss: 1.3122288MemoryTrain:  epoch  7, batch     1 | loss: 1.2281857MemoryTrain:  epoch  7, batch     2 | loss: 1.2279329MemoryTrain:  epoch  7, batch     3 | loss: 1.2752917MemoryTrain:  epoch  7, batch     4 | loss: 1.2716430MemoryTrain:  epoch  8, batch     0 | loss: 1.2776077MemoryTrain:  epoch  8, batch     1 | loss: 1.2074028MemoryTrain:  epoch  8, batch     2 | loss: 1.2568249MemoryTrain:  epoch  8, batch     3 | loss: 1.2307026MemoryTrain:  epoch  8, batch     4 | loss: 1.2186954MemoryTrain:  epoch  9, batch     0 | loss: 1.2311692MemoryTrain:  epoch  9, batch     1 | loss: 1.2382865MemoryTrain:  epoch  9, batch     2 | loss: 1.2085664MemoryTrain:  epoch  9, batch     3 | loss: 1.2364988MemoryTrain:  epoch  9, batch     4 | loss: 1.1899168
[EVAL] batch:    0 | acc: 12.50%,  total acc: 12.50%   [EVAL] batch:    1 | acc: 31.25%,  total acc: 21.88%   [EVAL] batch:    2 | acc: 12.50%,  total acc: 18.75%   [EVAL] batch:    3 | acc: 25.00%,  total acc: 20.31%   [EVAL] batch:    4 | acc: 25.00%,  total acc: 21.25%   [EVAL] batch:    5 | acc: 6.25%,  total acc: 18.75%   [EVAL] batch:    6 | acc: 31.25%,  total acc: 20.54%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 30.47%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 37.50%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 42.50%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 47.16%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 51.04%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 53.37%   [EVAL] batch:   13 | acc: 100.00%,  total acc: 56.70%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 59.17%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 61.72%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 63.60%   [EVAL] batch:   17 | acc: 81.25%,  total acc: 64.58%   [EVAL] batch:   18 | acc: 37.50%,  total acc: 63.16%   [EVAL] batch:   19 | acc: 31.25%,  total acc: 61.56%   [EVAL] batch:   20 | acc: 50.00%,  total acc: 61.01%   [EVAL] batch:   21 | acc: 0.00%,  total acc: 58.24%   
[EVAL] batch:    0 | acc: 31.25%,  total acc: 31.25%   [EVAL] batch:    1 | acc: 50.00%,  total acc: 40.62%   [EVAL] batch:    2 | acc: 56.25%,  total acc: 45.83%   [EVAL] batch:    3 | acc: 25.00%,  total acc: 40.62%   [EVAL] batch:    4 | acc: 31.25%,  total acc: 38.75%   [EVAL] batch:    5 | acc: 31.25%,  total acc: 37.50%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 41.96%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 46.09%   [EVAL] batch:    8 | acc: 75.00%,  total acc: 49.31%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 51.25%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 54.55%   [EVAL] batch:   11 | acc: 37.50%,  total acc: 53.12%   [EVAL] batch:   12 | acc: 43.75%,  total acc: 52.40%   [EVAL] batch:   13 | acc: 25.00%,  total acc: 50.45%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 52.08%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 52.34%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 53.68%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 54.17%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 54.93%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 56.56%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 58.63%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 60.23%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 61.68%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 63.28%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 64.75%   [EVAL] batch:   25 | acc: 93.75%,  total acc: 65.87%   [EVAL] batch:   26 | acc: 81.25%,  total acc: 66.44%   [EVAL] batch:   27 | acc: 87.50%,  total acc: 67.19%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 68.10%   [EVAL] batch:   29 | acc: 75.00%,  total acc: 68.33%   [EVAL] batch:   30 | acc: 81.25%,  total acc: 68.75%   [EVAL] batch:   31 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:   32 | acc: 87.50%,  total acc: 69.32%   [EVAL] batch:   33 | acc: 31.25%,  total acc: 68.20%   [EVAL] batch:   34 | acc: 18.75%,  total acc: 66.79%   [EVAL] batch:   35 | acc: 12.50%,  total acc: 65.28%   [EVAL] batch:   36 | acc: 0.00%,  total acc: 63.51%   [EVAL] batch:   37 | acc: 6.25%,  total acc: 62.01%   [EVAL] batch:   38 | acc: 6.25%,  total acc: 60.58%   [EVAL] batch:   39 | acc: 87.50%,  total acc: 61.25%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 62.20%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 63.10%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 63.81%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 64.20%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 64.86%   [EVAL] batch:   45 | acc: 87.50%,  total acc: 65.35%   [EVAL] batch:   46 | acc: 56.25%,  total acc: 65.16%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 65.76%   [EVAL] batch:   48 | acc: 31.25%,  total acc: 65.05%   [EVAL] batch:   49 | acc: 68.75%,  total acc: 65.12%   [EVAL] batch:   50 | acc: 62.50%,  total acc: 65.07%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 65.62%   [EVAL] batch:   52 | acc: 75.00%,  total acc: 65.80%   [EVAL] batch:   53 | acc: 87.50%,  total acc: 66.20%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 66.70%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 67.19%   [EVAL] batch:   56 | acc: 87.50%,  total acc: 67.54%   [EVAL] batch:   57 | acc: 81.25%,  total acc: 67.78%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 68.33%   [EVAL] batch:   59 | acc: 87.50%,  total acc: 68.65%   [EVAL] batch:   60 | acc: 43.75%,  total acc: 68.24%   [EVAL] batch:   61 | acc: 18.75%,  total acc: 67.44%   [EVAL] batch:   62 | acc: 31.25%,  total acc: 66.87%   [EVAL] batch:   63 | acc: 62.50%,  total acc: 66.80%   [EVAL] batch:   64 | acc: 62.50%,  total acc: 66.73%   [EVAL] batch:   65 | acc: 37.50%,  total acc: 66.29%   [EVAL] batch:   66 | acc: 25.00%,  total acc: 65.67%   [EVAL] batch:   67 | acc: 18.75%,  total acc: 64.98%   [EVAL] batch:   68 | acc: 18.75%,  total acc: 64.31%   [EVAL] batch:   69 | acc: 18.75%,  total acc: 63.66%   [EVAL] batch:   70 | acc: 25.00%,  total acc: 63.12%   [EVAL] batch:   71 | acc: 6.25%,  total acc: 62.33%   [EVAL] batch:   72 | acc: 56.25%,  total acc: 62.24%   [EVAL] batch:   73 | acc: 100.00%,  total acc: 62.75%   [EVAL] batch:   74 | acc: 93.75%,  total acc: 63.17%   [EVAL] batch:   75 | acc: 87.50%,  total acc: 63.49%   [EVAL] batch:   76 | acc: 87.50%,  total acc: 63.80%   [EVAL] batch:   77 | acc: 87.50%,  total acc: 64.10%   [EVAL] batch:   78 | acc: 93.75%,  total acc: 64.48%   [EVAL] batch:   79 | acc: 100.00%,  total acc: 64.92%   [EVAL] batch:   80 | acc: 93.75%,  total acc: 65.28%   [EVAL] batch:   81 | acc: 100.00%,  total acc: 65.70%   [EVAL] batch:   82 | acc: 87.50%,  total acc: 65.96%   [EVAL] batch:   83 | acc: 81.25%,  total acc: 66.15%   [EVAL] batch:   84 | acc: 25.00%,  total acc: 65.66%   [EVAL] batch:   85 | acc: 37.50%,  total acc: 65.33%   [EVAL] batch:   86 | acc: 37.50%,  total acc: 65.01%   [EVAL] batch:   87 | acc: 0.00%,  total acc: 64.28%   
cur_acc:  ['0.8561', '0.8571', '0.7422', '0.7837', '0.5824']
his_acc:  ['0.8561', '0.8338', '0.7234', '0.6828', '0.6428']
CurrentTrain: epoch  0, batch     0 | loss: 4.9513369CurrentTrain: epoch  0, batch     1 | loss: 4.2830133CurrentTrain: epoch  1, batch     0 | loss: 3.5903766CurrentTrain: epoch  1, batch     1 | loss: 3.0028327CurrentTrain: epoch  2, batch     0 | loss: 2.9387517CurrentTrain: epoch  2, batch     1 | loss: 2.4556286CurrentTrain: epoch  3, batch     0 | loss: 2.4722061CurrentTrain: epoch  3, batch     1 | loss: 2.1322925CurrentTrain: epoch  4, batch     0 | loss: 2.1746092CurrentTrain: epoch  4, batch     1 | loss: 2.2707388CurrentTrain: epoch  5, batch     0 | loss: 2.0259299CurrentTrain: epoch  5, batch     1 | loss: 2.1068175CurrentTrain: epoch  6, batch     0 | loss: 1.9765470CurrentTrain: epoch  6, batch     1 | loss: 1.9282502CurrentTrain: epoch  7, batch     0 | loss: 1.9973294CurrentTrain: epoch  7, batch     1 | loss: 1.9962997CurrentTrain: epoch  8, batch     0 | loss: 1.9695796CurrentTrain: epoch  8, batch     1 | loss: 1.9616048CurrentTrain: epoch  9, batch     0 | loss: 1.9407032CurrentTrain: epoch  9, batch     1 | loss: 1.9105042
#### organization founded ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization founded" means an organization was found in a date.
Here is an example:
Relation: organization founded
Context: the jnf was founded in 1901 to buy plots in palestine , then ruled by the ottomans .
Head Entity: jnf
Tail Entity: 1901
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization founded":
Relation: organization founded  
Context: the united nations was established in 1945 to promote international cooperation and peace.  
Head Entity: united nations  
Tail Entity: 1945  

Relation: organization founded  
Context: the world health organization was created in 1948 to coordinate global health efforts.  
Head Entity: world health organization  
Tail Entity: 1948  
#### person age ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person age" means a person 's age is a number.
Here is an example:
Relation: person age
Context: paris , feb 23 -lrb- xinhua -rrb- yoadimnadji , 56 , died of a cardiovascular problem at midnight .
Head Entity: yoadimnadji
Tail Entity: 56
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person age":
Relation: person age  
Context: In a recent interview, the renowned author, Jane Doe, revealed that she is 34 years old and still has many stories to tell.  
Head Entity: Jane Doe  
Tail Entity: 34  

Relation: person age  
Context: During the family reunion, Uncle Bob proudly announced that he had just turned 70, sharing his plans for retirement.  
Head Entity: Uncle Bob  
Tail Entity: 70  
#### person city of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person city of birth" means a person was born in a city.
Here is an example:
Relation: person city of birth
Context: rothman was born in san francisco in 1932 in an orthodox jewish family .
Head Entity: rothman
Tail Entity: san francisco
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person city of birth":
Relation: person city of birth  
Context: after spending his early years in new york, he moved to los angeles where he began his career.  
Head Entity: he  
Tail Entity: los angeles  

Relation: person city of birth  
Context: the famous author was born in a small town near boston, which greatly influenced her writing.  
Head Entity: the famous author  
Tail Entity: boston  
#### organization members ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization members" means an organization 's member is an organization.
Here is an example:
Relation: organization members
Context: sun plays for the grand rapids flight of the international basketball league after toiling for the maryland nighthawks of the american basketball association , both development leagues for those who dream of an nba career .
Head Entity: american basketball association
Tail Entity: maryland nighthawks
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization members":
Relation: organization members  
Context: The New York Philharmonic is one of the oldest orchestras in the United States, and it has had many notable musicians, including members from the Boston Symphony Orchestra.  
Head Entity: Boston Symphony Orchestra  
Tail Entity: New York Philharmonic  

Relation: organization members  
Context: The National Football League has various teams, and the Dallas Cowboys are one of its most famous members, alongside the Green Bay Packers.  
Head Entity: Green Bay Packers  
Tail Entity: Dallas Cowboys  
#### person religion ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person religion" means an person is the member of a religion.
Here is an example:
Relation: person religion
Context: the pope defended his action on the grounds that he could not refuse an audience to a head of state from a country with a strong catholic tradition unless he had clear-cut proof of the allegations against him .
Head Entity: he
Tail Entity: catholic
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person religion":
Relation: person religion  
Context: During the ceremony, the rabbi spoke about the importance of faith and community in Judaism, emphasizing how each member contributes to the collective spirit.  
Head Entity: rabbi  
Tail Entity: Judaism  

Relation: person religion  
Context: The imam led the prayers at the mosque, reminding the congregation of their duties as followers of Islam and the significance of their beliefs in daily life.  
Head Entity: imam  
Tail Entity: Islam  
Mixup data size:  6685
MixupTrain:  epoch  0, batch     0 | loss: 3.7755923MixupTrain:  epoch  0, batch     1 | loss: 3.8241260MixupTrain:  epoch  0, batch     2 | loss: 3.7418725MixupTrain:  epoch  0, batch     3 | loss: 3.6472583MixupTrain:  epoch  0, batch     4 | loss: 3.8678231MixupTrain:  epoch  0, batch     5 | loss: 4.3175402MixupTrain:  epoch  0, batch     6 | loss: 3.7381265MixupTrain:  epoch  0, batch     7 | loss: 3.5701225MixupTrain:  epoch  0, batch     8 | loss: 3.6165848MixupTrain:  epoch  0, batch     9 | loss: 3.4325411MixupTrain:  epoch  0, batch    10 | loss: 3.9718001MixupTrain:  epoch  0, batch    11 | loss: 3.6842356MixupTrain:  epoch  0, batch    12 | loss: 3.6431332MixupTrain:  epoch  0, batch    13 | loss: 3.7521572MixupTrain:  epoch  0, batch    14 | loss: 3.8932245MixupTrain:  epoch  0, batch    15 | loss: 3.9697366MixupTrain:  epoch  0, batch    16 | loss: 3.7482958MixupTrain:  epoch  0, batch    17 | loss: 3.6438384MixupTrain:  epoch  0, batch    18 | loss: 3.4821820MixupTrain:  epoch  0, batch    19 | loss: 3.5329680MixupTrain:  epoch  0, batch    20 | loss: 3.1515870MixupTrain:  epoch  0, batch    21 | loss: 3.7661438MixupTrain:  epoch  0, batch    22 | loss: 4.0182276MixupTrain:  epoch  0, batch    23 | loss: 3.6687326MixupTrain:  epoch  0, batch    24 | loss: 3.3096352MixupTrain:  epoch  0, batch    25 | loss: 3.5812404MixupTrain:  epoch  0, batch    26 | loss: 3.0973265MixupTrain:  epoch  0, batch    27 | loss: 3.6729269MixupTrain:  epoch  0, batch    28 | loss: 3.2928391MixupTrain:  epoch  0, batch    29 | loss: 3.5308661MixupTrain:  epoch  0, batch    30 | loss: 3.1470871MixupTrain:  epoch  0, batch    31 | loss: 3.5493572MixupTrain:  epoch  0, batch    32 | loss: 3.4233313MixupTrain:  epoch  0, batch    33 | loss: 3.0651031MixupTrain:  epoch  0, batch    34 | loss: 3.5112216MixupTrain:  epoch  0, batch    35 | loss: 3.8328393MixupTrain:  epoch  0, batch    36 | loss: 3.7570362MixupTrain:  epoch  0, batch    37 | loss: 3.0408454MixupTrain:  epoch  0, batch    38 | loss: 3.5405498MixupTrain:  epoch  0, batch    39 | loss: 3.2491322MixupTrain:  epoch  0, batch    40 | loss: 3.4449935MixupTrain:  epoch  0, batch    41 | loss: 3.3045177MixupTrain:  epoch  0, batch    42 | loss: 3.0740247MixupTrain:  epoch  0, batch    43 | loss: 3.3222623MixupTrain:  epoch  0, batch    44 | loss: 3.6104484MixupTrain:  epoch  0, batch    45 | loss: 3.4768066MixupTrain:  epoch  0, batch    46 | loss: 3.3605723MixupTrain:  epoch  0, batch    47 | loss: 3.5257096MixupTrain:  epoch  0, batch    48 | loss: 3.0803366MixupTrain:  epoch  0, batch    49 | loss: 3.2627459MixupTrain:  epoch  0, batch    50 | loss: 3.2114890MixupTrain:  epoch  0, batch    51 | loss: 3.2719448MixupTrain:  epoch  0, batch    52 | loss: 3.4086189MixupTrain:  epoch  0, batch    53 | loss: 3.4418669MixupTrain:  epoch  0, batch    54 | loss: 3.8514137MixupTrain:  epoch  0, batch    55 | loss: 3.6850672MixupTrain:  epoch  0, batch    56 | loss: 3.3850865MixupTrain:  epoch  0, batch    57 | loss: 3.0404863MixupTrain:  epoch  0, batch    58 | loss: 2.9562497MixupTrain:  epoch  0, batch    59 | loss: 3.5891733MixupTrain:  epoch  0, batch    60 | loss: 3.2624116MixupTrain:  epoch  0, batch    61 | loss: 3.2563529MixupTrain:  epoch  0, batch    62 | loss: 3.8527088MixupTrain:  epoch  0, batch    63 | loss: 3.3217292MixupTrain:  epoch  0, batch    64 | loss: 3.4023070MixupTrain:  epoch  0, batch    65 | loss: 3.4501019MixupTrain:  epoch  0, batch    66 | loss: 3.2796926MixupTrain:  epoch  0, batch    67 | loss: 3.2912600MixupTrain:  epoch  0, batch    68 | loss: 3.3463054MixupTrain:  epoch  0, batch    69 | loss: 3.3357306MixupTrain:  epoch  0, batch    70 | loss: 3.3221116MixupTrain:  epoch  0, batch    71 | loss: 3.3409655MixupTrain:  epoch  0, batch    72 | loss: 3.0159335MixupTrain:  epoch  0, batch    73 | loss: 3.5300617MixupTrain:  epoch  0, batch    74 | loss: 3.1381273MixupTrain:  epoch  0, batch    75 | loss: 3.5158091MixupTrain:  epoch  0, batch    76 | loss: 3.3007479MixupTrain:  epoch  0, batch    77 | loss: 3.0555005MixupTrain:  epoch  0, batch    78 | loss: 3.4559124MixupTrain:  epoch  0, batch    79 | loss: 3.6117210MixupTrain:  epoch  0, batch    80 | loss: 3.4283681MixupTrain:  epoch  0, batch    81 | loss: 3.2101698MixupTrain:  epoch  0, batch    82 | loss: 3.2282281MixupTrain:  epoch  0, batch    83 | loss: 3.2669878MixupTrain:  epoch  0, batch    84 | loss: 3.1247528MixupTrain:  epoch  0, batch    85 | loss: 3.0809004MixupTrain:  epoch  0, batch    86 | loss: 3.0846698MixupTrain:  epoch  0, batch    87 | loss: 3.2019272MixupTrain:  epoch  0, batch    88 | loss: 3.0676429MixupTrain:  epoch  0, batch    89 | loss: 3.4990373MixupTrain:  epoch  0, batch    90 | loss: 3.4160738MixupTrain:  epoch  0, batch    91 | loss: 3.4908175MixupTrain:  epoch  0, batch    92 | loss: 3.1969051MixupTrain:  epoch  0, batch    93 | loss: 3.2932405MixupTrain:  epoch  0, batch    94 | loss: 3.2357812MixupTrain:  epoch  0, batch    95 | loss: 3.2859588MixupTrain:  epoch  0, batch    96 | loss: 3.1983981MixupTrain:  epoch  0, batch    97 | loss: 2.9980788MixupTrain:  epoch  0, batch    98 | loss: 3.1567593MixupTrain:  epoch  0, batch    99 | loss: 3.3270478MixupTrain:  epoch  0, batch   100 | loss: 3.1818876MixupTrain:  epoch  0, batch   101 | loss: 3.2617545MixupTrain:  epoch  0, batch   102 | loss: 3.1173220MixupTrain:  epoch  0, batch   103 | loss: 3.0153098MixupTrain:  epoch  0, batch   104 | loss: 3.3548303MixupTrain:  epoch  0, batch   105 | loss: 3.2625422MixupTrain:  epoch  0, batch   106 | loss: 3.2092273MixupTrain:  epoch  0, batch   107 | loss: 2.9426646MixupTrain:  epoch  0, batch   108 | loss: 3.1192679MixupTrain:  epoch  0, batch   109 | loss: 3.1717947MixupTrain:  epoch  0, batch   110 | loss: 2.9479928MixupTrain:  epoch  0, batch   111 | loss: 3.1040149MixupTrain:  epoch  0, batch   112 | loss: 3.1274772MixupTrain:  epoch  0, batch   113 | loss: 3.1163812MixupTrain:  epoch  0, batch   114 | loss: 3.1579118MixupTrain:  epoch  0, batch   115 | loss: 3.0858850MixupTrain:  epoch  0, batch   116 | loss: 3.0532684MixupTrain:  epoch  0, batch   117 | loss: 3.0937760MixupTrain:  epoch  0, batch   118 | loss: 3.3699713MixupTrain:  epoch  0, batch   119 | loss: 3.2446973MixupTrain:  epoch  0, batch   120 | loss: 2.9098172MixupTrain:  epoch  0, batch   121 | loss: 3.1553924MixupTrain:  epoch  0, batch   122 | loss: 2.8762813MixupTrain:  epoch  0, batch   123 | loss: 3.1991973MixupTrain:  epoch  0, batch   124 | loss: 3.5817227MixupTrain:  epoch  0, batch   125 | loss: 3.0963259MixupTrain:  epoch  0, batch   126 | loss: 3.0281658MixupTrain:  epoch  0, batch   127 | loss: 3.0847609MixupTrain:  epoch  0, batch   128 | loss: 3.2264931MixupTrain:  epoch  0, batch   129 | loss: 3.1569591MixupTrain:  epoch  0, batch   130 | loss: 3.0017638MixupTrain:  epoch  0, batch   131 | loss: 3.0481420MixupTrain:  epoch  0, batch   132 | loss: 2.9801919MixupTrain:  epoch  0, batch   133 | loss: 3.1948936MixupTrain:  epoch  0, batch   134 | loss: 3.0775528MixupTrain:  epoch  0, batch   135 | loss: 2.9403658MixupTrain:  epoch  0, batch   136 | loss: 2.9949660MixupTrain:  epoch  0, batch   137 | loss: 3.2598820MixupTrain:  epoch  0, batch   138 | loss: 3.1816347MixupTrain:  epoch  0, batch   139 | loss: 3.1380162MixupTrain:  epoch  0, batch   140 | loss: 3.3190694MixupTrain:  epoch  0, batch   141 | loss: 3.2324998MixupTrain:  epoch  0, batch   142 | loss: 3.2051342MixupTrain:  epoch  0, batch   143 | loss: 3.0478926MixupTrain:  epoch  0, batch   144 | loss: 3.0491199MixupTrain:  epoch  0, batch   145 | loss: 3.0847673MixupTrain:  epoch  0, batch   146 | loss: 3.3497124MixupTrain:  epoch  0, batch   147 | loss: 3.0745883MixupTrain:  epoch  0, batch   148 | loss: 3.2823901MixupTrain:  epoch  0, batch   149 | loss: 3.0714929MixupTrain:  epoch  0, batch   150 | loss: 3.2329869MixupTrain:  epoch  0, batch   151 | loss: 2.9687800MixupTrain:  epoch  0, batch   152 | loss: 3.2658467MixupTrain:  epoch  0, batch   153 | loss: 3.1386786MixupTrain:  epoch  0, batch   154 | loss: 2.9808915MixupTrain:  epoch  0, batch   155 | loss: 3.2781029MixupTrain:  epoch  0, batch   156 | loss: 2.9335105MixupTrain:  epoch  0, batch   157 | loss: 3.1850755MixupTrain:  epoch  0, batch   158 | loss: 2.9228230MixupTrain:  epoch  0, batch   159 | loss: 3.3269706MixupTrain:  epoch  0, batch   160 | loss: 3.1199427MixupTrain:  epoch  0, batch   161 | loss: 2.9895086MixupTrain:  epoch  0, batch   162 | loss: 3.0652704MixupTrain:  epoch  0, batch   163 | loss: 3.0936391MixupTrain:  epoch  0, batch   164 | loss: 3.1080432MixupTrain:  epoch  0, batch   165 | loss: 3.2240427MixupTrain:  epoch  0, batch   166 | loss: 2.9527292MixupTrain:  epoch  0, batch   167 | loss: 3.1812544MixupTrain:  epoch  0, batch   168 | loss: 3.1318598MixupTrain:  epoch  0, batch   169 | loss: 2.9099841MixupTrain:  epoch  0, batch   170 | loss: 3.2348485MixupTrain:  epoch  0, batch   171 | loss: 3.0142050MixupTrain:  epoch  0, batch   172 | loss: 3.0564938MixupTrain:  epoch  0, batch   173 | loss: 3.0324643MixupTrain:  epoch  0, batch   174 | loss: 3.1814556MixupTrain:  epoch  0, batch   175 | loss: 2.8973908MixupTrain:  epoch  0, batch   176 | loss: 3.1384439MixupTrain:  epoch  0, batch   177 | loss: 3.2472467MixupTrain:  epoch  0, batch   178 | loss: 2.9669914MixupTrain:  epoch  0, batch   179 | loss: 3.0484581MixupTrain:  epoch  0, batch   180 | loss: 3.0644479MixupTrain:  epoch  0, batch   181 | loss: 3.0570531MixupTrain:  epoch  0, batch   182 | loss: 3.2673578MixupTrain:  epoch  0, batch   183 | loss: 3.2181602MixupTrain:  epoch  0, batch   184 | loss: 2.9066794MixupTrain:  epoch  0, batch   185 | loss: 3.0534315MixupTrain:  epoch  0, batch   186 | loss: 3.0795264MixupTrain:  epoch  0, batch   187 | loss: 3.0282741MixupTrain:  epoch  0, batch   188 | loss: 3.1787338MixupTrain:  epoch  0, batch   189 | loss: 3.2502480MixupTrain:  epoch  0, batch   190 | loss: 3.1647038MixupTrain:  epoch  0, batch   191 | loss: 3.2088454MixupTrain:  epoch  0, batch   192 | loss: 3.0560145MixupTrain:  epoch  0, batch   193 | loss: 2.9841268MixupTrain:  epoch  0, batch   194 | loss: 2.9378362MixupTrain:  epoch  0, batch   195 | loss: 2.8480866MixupTrain:  epoch  0, batch   196 | loss: 2.8670051MixupTrain:  epoch  0, batch   197 | loss: 3.1125350MixupTrain:  epoch  0, batch   198 | loss: 3.0490353MixupTrain:  epoch  0, batch   199 | loss: 2.9514937MixupTrain:  epoch  0, batch   200 | loss: 3.0834088MixupTrain:  epoch  0, batch   201 | loss: 3.1941249MixupTrain:  epoch  0, batch   202 | loss: 3.2556205MixupTrain:  epoch  0, batch   203 | loss: 2.8865628MixupTrain:  epoch  0, batch   204 | loss: 2.9723566MixupTrain:  epoch  0, batch   205 | loss: 3.2366657MixupTrain:  epoch  0, batch   206 | loss: 3.2764215MixupTrain:  epoch  0, batch   207 | loss: 2.8702302MixupTrain:  epoch  0, batch   208 | loss: 3.0461555MixupTrain:  epoch  0, batch   209 | loss: 3.0632403MixupTrain:  epoch  0, batch   210 | loss: 3.0994563MixupTrain:  epoch  0, batch   211 | loss: 2.8894494MixupTrain:  epoch  0, batch   212 | loss: 2.8394642MixupTrain:  epoch  0, batch   213 | loss: 3.0462723MixupTrain:  epoch  0, batch   214 | loss: 2.9549990MixupTrain:  epoch  0, batch   215 | loss: 2.8767104MixupTrain:  epoch  0, batch   216 | loss: 2.8462930MixupTrain:  epoch  0, batch   217 | loss: 3.1782963MixupTrain:  epoch  0, batch   218 | loss: 2.9737792MixupTrain:  epoch  0, batch   219 | loss: 3.0637338MixupTrain:  epoch  0, batch   220 | loss: 3.2630713MixupTrain:  epoch  0, batch   221 | loss: 3.0177269MixupTrain:  epoch  0, batch   222 | loss: 2.9780779MixupTrain:  epoch  0, batch   223 | loss: 2.9882889MixupTrain:  epoch  0, batch   224 | loss: 3.1569118MixupTrain:  epoch  0, batch   225 | loss: 3.2901177MixupTrain:  epoch  0, batch   226 | loss: 2.9477568MixupTrain:  epoch  0, batch   227 | loss: 2.9727747MixupTrain:  epoch  0, batch   228 | loss: 3.0926337MixupTrain:  epoch  0, batch   229 | loss: 2.9517469MixupTrain:  epoch  0, batch   230 | loss: 2.9967220MixupTrain:  epoch  0, batch   231 | loss: 2.9775219MixupTrain:  epoch  0, batch   232 | loss: 3.1650853MixupTrain:  epoch  0, batch   233 | loss: 3.3043988MixupTrain:  epoch  0, batch   234 | loss: 3.0604255MixupTrain:  epoch  0, batch   235 | loss: 3.0176737MixupTrain:  epoch  0, batch   236 | loss: 3.1497617MixupTrain:  epoch  0, batch   237 | loss: 2.8600483MixupTrain:  epoch  0, batch   238 | loss: 2.8690848MixupTrain:  epoch  0, batch   239 | loss: 3.1554005MixupTrain:  epoch  0, batch   240 | loss: 2.9689851MixupTrain:  epoch  0, batch   241 | loss: 2.9579768MixupTrain:  epoch  0, batch   242 | loss: 2.9442334MixupTrain:  epoch  0, batch   243 | loss: 3.0486276MixupTrain:  epoch  0, batch   244 | loss: 3.1911173MixupTrain:  epoch  0, batch   245 | loss: 3.1374443MixupTrain:  epoch  0, batch   246 | loss: 3.2201200MixupTrain:  epoch  0, batch   247 | loss: 3.0241110MixupTrain:  epoch  0, batch   248 | loss: 3.0010400MixupTrain:  epoch  0, batch   249 | loss: 2.9388986MixupTrain:  epoch  0, batch   250 | loss: 2.9735026MixupTrain:  epoch  0, batch   251 | loss: 2.8572426MixupTrain:  epoch  0, batch   252 | loss: 3.0377460MixupTrain:  epoch  0, batch   253 | loss: 3.0478897MixupTrain:  epoch  0, batch   254 | loss: 3.0401664MixupTrain:  epoch  0, batch   255 | loss: 2.9061384MixupTrain:  epoch  0, batch   256 | loss: 3.0676999MixupTrain:  epoch  0, batch   257 | loss: 2.8628449MixupTrain:  epoch  0, batch   258 | loss: 2.8825109MixupTrain:  epoch  0, batch   259 | loss: 3.0422130MixupTrain:  epoch  0, batch   260 | loss: 3.0493634MixupTrain:  epoch  0, batch   261 | loss: 3.2447209MixupTrain:  epoch  0, batch   262 | loss: 3.0173216MixupTrain:  epoch  0, batch   263 | loss: 3.1397305MixupTrain:  epoch  0, batch   264 | loss: 2.9173951MixupTrain:  epoch  0, batch   265 | loss: 3.1139095MixupTrain:  epoch  0, batch   266 | loss: 3.0414853MixupTrain:  epoch  0, batch   267 | loss: 2.8993764MixupTrain:  epoch  0, batch   268 | loss: 2.9503832MixupTrain:  epoch  0, batch   269 | loss: 2.8132167MixupTrain:  epoch  0, batch   270 | loss: 2.9474454MixupTrain:  epoch  0, batch   271 | loss: 3.0387239MixupTrain:  epoch  0, batch   272 | loss: 3.0689816MixupTrain:  epoch  0, batch   273 | loss: 3.0623455MixupTrain:  epoch  0, batch   274 | loss: 2.9941525MixupTrain:  epoch  0, batch   275 | loss: 3.0578637MixupTrain:  epoch  0, batch   276 | loss: 3.0061371MixupTrain:  epoch  0, batch   277 | loss: 2.9383783MixupTrain:  epoch  0, batch   278 | loss: 3.0325723MixupTrain:  epoch  0, batch   279 | loss: 3.0502729MixupTrain:  epoch  0, batch   280 | loss: 3.1605926MixupTrain:  epoch  0, batch   281 | loss: 2.9328697MixupTrain:  epoch  0, batch   282 | loss: 3.0668216MixupTrain:  epoch  0, batch   283 | loss: 2.8974900MixupTrain:  epoch  0, batch   284 | loss: 3.0101299MixupTrain:  epoch  0, batch   285 | loss: 3.1202137MixupTrain:  epoch  0, batch   286 | loss: 2.9343402MixupTrain:  epoch  0, batch   287 | loss: 2.9102116MixupTrain:  epoch  0, batch   288 | loss: 2.9983625MixupTrain:  epoch  0, batch   289 | loss: 2.9304807MixupTrain:  epoch  0, batch   290 | loss: 2.9586568MixupTrain:  epoch  0, batch   291 | loss: 2.9446623MixupTrain:  epoch  0, batch   292 | loss: 2.9990556MixupTrain:  epoch  0, batch   293 | loss: 2.7219553MixupTrain:  epoch  0, batch   294 | loss: 2.9165926MixupTrain:  epoch  0, batch   295 | loss: 2.8724399MixupTrain:  epoch  0, batch   296 | loss: 3.0430713MixupTrain:  epoch  0, batch   297 | loss: 3.0109720MixupTrain:  epoch  0, batch   298 | loss: 3.1166141MixupTrain:  epoch  0, batch   299 | loss: 3.0440745MixupTrain:  epoch  0, batch   300 | loss: 2.7254853MixupTrain:  epoch  0, batch   301 | loss: 2.9004035MixupTrain:  epoch  0, batch   302 | loss: 2.9228616MixupTrain:  epoch  0, batch   303 | loss: 3.1306572MixupTrain:  epoch  0, batch   304 | loss: 3.0221820MixupTrain:  epoch  0, batch   305 | loss: 3.0682933MixupTrain:  epoch  0, batch   306 | loss: 2.9700537MixupTrain:  epoch  0, batch   307 | loss: 3.0294530MixupTrain:  epoch  0, batch   308 | loss: 2.7419002MixupTrain:  epoch  0, batch   309 | loss: 2.9668331MixupTrain:  epoch  0, batch   310 | loss: 3.0031319MixupTrain:  epoch  0, batch   311 | loss: 3.0181003MixupTrain:  epoch  0, batch   312 | loss: 2.9544044MixupTrain:  epoch  0, batch   313 | loss: 3.1278505MixupTrain:  epoch  0, batch   314 | loss: 2.8959169MixupTrain:  epoch  0, batch   315 | loss: 3.1086621MixupTrain:  epoch  0, batch   316 | loss: 3.0561767MixupTrain:  epoch  0, batch   317 | loss: 2.8250380MixupTrain:  epoch  0, batch   318 | loss: 2.8607426MixupTrain:  epoch  0, batch   319 | loss: 3.0034366MixupTrain:  epoch  0, batch   320 | loss: 2.8803604MixupTrain:  epoch  0, batch   321 | loss: 2.8358920MixupTrain:  epoch  0, batch   322 | loss: 3.1060538MixupTrain:  epoch  0, batch   323 | loss: 2.9350567MixupTrain:  epoch  0, batch   324 | loss: 3.0172777MixupTrain:  epoch  0, batch   325 | loss: 2.9787185MixupTrain:  epoch  0, batch   326 | loss: 3.0162592MixupTrain:  epoch  0, batch   327 | loss: 3.0074067MixupTrain:  epoch  0, batch   328 | loss: 3.0973220MixupTrain:  epoch  0, batch   329 | loss: 2.9035790MixupTrain:  epoch  0, batch   330 | loss: 3.1943569MixupTrain:  epoch  0, batch   331 | loss: 2.9960346MixupTrain:  epoch  0, batch   332 | loss: 2.9794841MixupTrain:  epoch  0, batch   333 | loss: 2.9328694MixupTrain:  epoch  0, batch   334 | loss: 2.8739276MixupTrain:  epoch  0, batch   335 | loss: 2.9859877MixupTrain:  epoch  0, batch   336 | loss: 2.9305086MixupTrain:  epoch  0, batch   337 | loss: 2.8735628MixupTrain:  epoch  0, batch   338 | loss: 2.9446065MixupTrain:  epoch  0, batch   339 | loss: 2.9358201MixupTrain:  epoch  0, batch   340 | loss: 2.9742076MixupTrain:  epoch  0, batch   341 | loss: 2.9363775MixupTrain:  epoch  0, batch   342 | loss: 2.9248636MixupTrain:  epoch  0, batch   343 | loss: 2.9259539MixupTrain:  epoch  0, batch   344 | loss: 3.0027258MixupTrain:  epoch  0, batch   345 | loss: 3.0229077MixupTrain:  epoch  0, batch   346 | loss: 3.0186942MixupTrain:  epoch  0, batch   347 | loss: 2.8972507MixupTrain:  epoch  0, batch   348 | loss: 2.9473884MixupTrain:  epoch  0, batch   349 | loss: 3.0484362MixupTrain:  epoch  0, batch   350 | loss: 2.9185848MixupTrain:  epoch  0, batch   351 | loss: 3.0534301MixupTrain:  epoch  0, batch   352 | loss: 2.7430711MixupTrain:  epoch  0, batch   353 | loss: 3.0642447MixupTrain:  epoch  0, batch   354 | loss: 3.0796263MixupTrain:  epoch  0, batch   355 | loss: 3.0639505MixupTrain:  epoch  0, batch   356 | loss: 2.9587131MixupTrain:  epoch  0, batch   357 | loss: 3.0198123MixupTrain:  epoch  0, batch   358 | loss: 3.0996804MixupTrain:  epoch  0, batch   359 | loss: 3.1400862MixupTrain:  epoch  0, batch   360 | loss: 2.9120374MixupTrain:  epoch  0, batch   361 | loss: 3.1180363MixupTrain:  epoch  0, batch   362 | loss: 2.9550362MixupTrain:  epoch  0, batch   363 | loss: 2.9283137MixupTrain:  epoch  0, batch   364 | loss: 3.0649681MixupTrain:  epoch  0, batch   365 | loss: 2.7428350MixupTrain:  epoch  0, batch   366 | loss: 3.2290742MixupTrain:  epoch  0, batch   367 | loss: 2.9986637MixupTrain:  epoch  0, batch   368 | loss: 2.8941197MixupTrain:  epoch  0, batch   369 | loss: 2.9809051MixupTrain:  epoch  0, batch   370 | loss: 2.8064053MixupTrain:  epoch  0, batch   371 | loss: 3.0328100MixupTrain:  epoch  0, batch   372 | loss: 2.9317355MixupTrain:  epoch  0, batch   373 | loss: 3.3640313MixupTrain:  epoch  0, batch   374 | loss: 2.9340031MixupTrain:  epoch  0, batch   375 | loss: 3.1075120MixupTrain:  epoch  0, batch   376 | loss: 2.8779070MixupTrain:  epoch  0, batch   377 | loss: 2.8288159MixupTrain:  epoch  0, batch   378 | loss: 2.8862979MixupTrain:  epoch  0, batch   379 | loss: 2.8475420MixupTrain:  epoch  0, batch   380 | loss: 2.9521224MixupTrain:  epoch  0, batch   381 | loss: 2.9539165MixupTrain:  epoch  0, batch   382 | loss: 3.0063171MixupTrain:  epoch  0, batch   383 | loss: 2.9049556MixupTrain:  epoch  0, batch   384 | loss: 2.7985015MixupTrain:  epoch  0, batch   385 | loss: 2.8190446MixupTrain:  epoch  0, batch   386 | loss: 2.8912282MixupTrain:  epoch  0, batch   387 | loss: 2.8147800MixupTrain:  epoch  0, batch   388 | loss: 2.9985852MixupTrain:  epoch  0, batch   389 | loss: 2.9330938MixupTrain:  epoch  0, batch   390 | loss: 3.0505168MixupTrain:  epoch  0, batch   391 | loss: 3.0930605MixupTrain:  epoch  0, batch   392 | loss: 3.1037750MixupTrain:  epoch  0, batch   393 | loss: 2.8554645MixupTrain:  epoch  0, batch   394 | loss: 2.8590102MixupTrain:  epoch  0, batch   395 | loss: 2.9359927MixupTrain:  epoch  0, batch   396 | loss: 2.9176152MixupTrain:  epoch  0, batch   397 | loss: 3.0312641MixupTrain:  epoch  0, batch   398 | loss: 3.0361090MixupTrain:  epoch  0, batch   399 | loss: 2.9741147MixupTrain:  epoch  0, batch   400 | loss: 2.8516805MixupTrain:  epoch  0, batch   401 | loss: 3.0687785MixupTrain:  epoch  0, batch   402 | loss: 3.0118160MixupTrain:  epoch  0, batch   403 | loss: 2.8358989MixupTrain:  epoch  0, batch   404 | loss: 2.9259069MixupTrain:  epoch  0, batch   405 | loss: 3.2295616MixupTrain:  epoch  0, batch   406 | loss: 2.9439259MixupTrain:  epoch  0, batch   407 | loss: 3.0476241MixupTrain:  epoch  0, batch   408 | loss: 3.0921769MixupTrain:  epoch  0, batch   409 | loss: 2.8608100MixupTrain:  epoch  0, batch   410 | loss: 2.9814029MixupTrain:  epoch  0, batch   411 | loss: 2.8563957MixupTrain:  epoch  0, batch   412 | loss: 2.8860226MixupTrain:  epoch  0, batch   413 | loss: 2.9269969MixupTrain:  epoch  0, batch   414 | loss: 2.8834090MixupTrain:  epoch  0, batch   415 | loss: 2.9364452MixupTrain:  epoch  0, batch   416 | loss: 2.9098883MixupTrain:  epoch  0, batch   417 | loss: 3.0167973
MemoryTrain:  epoch  0, batch     0 | loss: 1.1930603MemoryTrain:  epoch  0, batch     1 | loss: 1.4529557MemoryTrain:  epoch  0, batch     2 | loss: 1.4549648MemoryTrain:  epoch  0, batch     3 | loss: 1.7257733MemoryTrain:  epoch  0, batch     4 | loss: 1.7512519MemoryTrain:  epoch  0, batch     5 | loss: 1.7505130MemoryTrain:  epoch  1, batch     0 | loss: 1.3403964MemoryTrain:  epoch  1, batch     1 | loss: 1.2545525MemoryTrain:  epoch  1, batch     2 | loss: 1.3243976MemoryTrain:  epoch  1, batch     3 | loss: 1.2940995MemoryTrain:  epoch  1, batch     4 | loss: 1.2872924MemoryTrain:  epoch  1, batch     5 | loss: 1.3415334MemoryTrain:  epoch  2, batch     0 | loss: 1.2621448MemoryTrain:  epoch  2, batch     1 | loss: 1.2682805MemoryTrain:  epoch  2, batch     2 | loss: 1.2874945MemoryTrain:  epoch  2, batch     3 | loss: 1.2598190MemoryTrain:  epoch  2, batch     4 | loss: 1.2092097MemoryTrain:  epoch  2, batch     5 | loss: 1.2693976MemoryTrain:  epoch  3, batch     0 | loss: 1.2653296MemoryTrain:  epoch  3, batch     1 | loss: 1.2605703MemoryTrain:  epoch  3, batch     2 | loss: 1.2002404MemoryTrain:  epoch  3, batch     3 | loss: 1.2309550MemoryTrain:  epoch  3, batch     4 | loss: 1.2158830MemoryTrain:  epoch  3, batch     5 | loss: 1.2751386MemoryTrain:  epoch  4, batch     0 | loss: 1.1897556MemoryTrain:  epoch  4, batch     1 | loss: 1.2461048MemoryTrain:  epoch  4, batch     2 | loss: 1.2681800MemoryTrain:  epoch  4, batch     3 | loss: 1.2326169MemoryTrain:  epoch  4, batch     4 | loss: 1.2132689MemoryTrain:  epoch  4, batch     5 | loss: 1.2417130MemoryTrain:  epoch  5, batch     0 | loss: 1.2122205MemoryTrain:  epoch  5, batch     1 | loss: 1.2761402MemoryTrain:  epoch  5, batch     2 | loss: 1.2189357MemoryTrain:  epoch  5, batch     3 | loss: 1.2076192MemoryTrain:  epoch  5, batch     4 | loss: 1.2483723MemoryTrain:  epoch  5, batch     5 | loss: 1.2747347MemoryTrain:  epoch  6, batch     0 | loss: 1.2120589MemoryTrain:  epoch  6, batch     1 | loss: 1.2364219MemoryTrain:  epoch  6, batch     2 | loss: 1.1829824MemoryTrain:  epoch  6, batch     3 | loss: 1.2060728MemoryTrain:  epoch  6, batch     4 | loss: 1.2062391MemoryTrain:  epoch  6, batch     5 | loss: 1.2014498MemoryTrain:  epoch  7, batch     0 | loss: 1.2126307MemoryTrain:  epoch  7, batch     1 | loss: 1.1968739MemoryTrain:  epoch  7, batch     2 | loss: 1.1806056MemoryTrain:  epoch  7, batch     3 | loss: 1.2034779MemoryTrain:  epoch  7, batch     4 | loss: 1.2269185MemoryTrain:  epoch  7, batch     5 | loss: 1.2687842MemoryTrain:  epoch  8, batch     0 | loss: 1.1971521MemoryTrain:  epoch  8, batch     1 | loss: 1.2376235MemoryTrain:  epoch  8, batch     2 | loss: 1.1816990MemoryTrain:  epoch  8, batch     3 | loss: 1.2054715MemoryTrain:  epoch  8, batch     4 | loss: 1.1695013MemoryTrain:  epoch  8, batch     5 | loss: 1.2873110MemoryTrain:  epoch  9, batch     0 | loss: 1.1859140MemoryTrain:  epoch  9, batch     1 | loss: 1.1714731MemoryTrain:  epoch  9, batch     2 | loss: 1.1899006MemoryTrain:  epoch  9, batch     3 | loss: 1.2098724MemoryTrain:  epoch  9, batch     4 | loss: 1.1764736MemoryTrain:  epoch  9, batch     5 | loss: 1.1961401
[EVAL] batch:    0 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 96.88%   [EVAL] batch:    2 | acc: 100.00%,  total acc: 97.92%   [EVAL] batch:    3 | acc: 100.00%,  total acc: 98.44%   [EVAL] batch:    4 | acc: 100.00%,  total acc: 98.75%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 98.96%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 99.11%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 99.22%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 97.92%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 94.38%   [EVAL] batch:   10 | acc: 68.75%,  total acc: 92.05%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 92.19%   [EVAL] batch:   12 | acc: 100.00%,  total acc: 92.79%   [EVAL] batch:   13 | acc: 75.00%,  total acc: 91.52%   
[EVAL] batch:    0 | acc: 31.25%,  total acc: 31.25%   [EVAL] batch:    1 | acc: 50.00%,  total acc: 40.62%   [EVAL] batch:    2 | acc: 56.25%,  total acc: 45.83%   [EVAL] batch:    3 | acc: 12.50%,  total acc: 37.50%   [EVAL] batch:    4 | acc: 18.75%,  total acc: 33.75%   [EVAL] batch:    5 | acc: 31.25%,  total acc: 33.33%   [EVAL] batch:    6 | acc: 56.25%,  total acc: 36.61%   [EVAL] batch:    7 | acc: 56.25%,  total acc: 39.06%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 42.36%   [EVAL] batch:    9 | acc: 56.25%,  total acc: 43.75%   [EVAL] batch:   10 | acc: 75.00%,  total acc: 46.59%   [EVAL] batch:   11 | acc: 25.00%,  total acc: 44.79%   [EVAL] batch:   12 | acc: 31.25%,  total acc: 43.75%   [EVAL] batch:   13 | acc: 25.00%,  total acc: 42.41%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 44.17%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 44.92%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 46.69%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 47.57%   [EVAL] batch:   18 | acc: 81.25%,  total acc: 49.34%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 51.25%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 53.57%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 55.40%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 57.07%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 58.59%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 60.25%   [EVAL] batch:   25 | acc: 93.75%,  total acc: 61.54%   [EVAL] batch:   26 | acc: 75.00%,  total acc: 62.04%   [EVAL] batch:   27 | acc: 87.50%,  total acc: 62.95%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 64.01%   [EVAL] batch:   29 | acc: 75.00%,  total acc: 64.38%   [EVAL] batch:   30 | acc: 75.00%,  total acc: 64.72%   [EVAL] batch:   31 | acc: 62.50%,  total acc: 64.65%   [EVAL] batch:   32 | acc: 81.25%,  total acc: 65.15%   [EVAL] batch:   33 | acc: 43.75%,  total acc: 64.52%   [EVAL] batch:   34 | acc: 31.25%,  total acc: 63.57%   [EVAL] batch:   35 | acc: 12.50%,  total acc: 62.15%   [EVAL] batch:   36 | acc: 0.00%,  total acc: 60.47%   [EVAL] batch:   37 | acc: 6.25%,  total acc: 59.05%   [EVAL] batch:   38 | acc: 6.25%,  total acc: 57.69%   [EVAL] batch:   39 | acc: 87.50%,  total acc: 58.44%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 59.30%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 60.27%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 61.05%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 61.51%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 62.22%   [EVAL] batch:   45 | acc: 87.50%,  total acc: 62.77%   [EVAL] batch:   46 | acc: 56.25%,  total acc: 62.63%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 63.28%   [EVAL] batch:   48 | acc: 25.00%,  total acc: 62.50%   [EVAL] batch:   49 | acc: 75.00%,  total acc: 62.75%   [EVAL] batch:   50 | acc: 68.75%,  total acc: 62.87%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 63.46%   [EVAL] batch:   52 | acc: 75.00%,  total acc: 63.68%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 64.00%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 64.55%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 64.96%   [EVAL] batch:   56 | acc: 87.50%,  total acc: 65.35%   [EVAL] batch:   57 | acc: 75.00%,  total acc: 65.52%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 66.10%   [EVAL] batch:   59 | acc: 87.50%,  total acc: 66.46%   [EVAL] batch:   60 | acc: 43.75%,  total acc: 66.09%   [EVAL] batch:   61 | acc: 12.50%,  total acc: 65.22%   [EVAL] batch:   62 | acc: 18.75%,  total acc: 64.48%   [EVAL] batch:   63 | acc: 43.75%,  total acc: 64.16%   [EVAL] batch:   64 | acc: 56.25%,  total acc: 64.04%   [EVAL] batch:   65 | acc: 56.25%,  total acc: 63.92%   [EVAL] batch:   66 | acc: 0.00%,  total acc: 62.97%   [EVAL] batch:   67 | acc: 0.00%,  total acc: 62.04%   [EVAL] batch:   68 | acc: 18.75%,  total acc: 61.41%   [EVAL] batch:   69 | acc: 6.25%,  total acc: 60.62%   [EVAL] batch:   70 | acc: 25.00%,  total acc: 60.12%   [EVAL] batch:   71 | acc: 6.25%,  total acc: 59.38%   [EVAL] batch:   72 | acc: 37.50%,  total acc: 59.08%   [EVAL] batch:   73 | acc: 62.50%,  total acc: 59.12%   [EVAL] batch:   74 | acc: 43.75%,  total acc: 58.92%   [EVAL] batch:   75 | acc: 87.50%,  total acc: 59.29%   [EVAL] batch:   76 | acc: 56.25%,  total acc: 59.25%   [EVAL] batch:   77 | acc: 68.75%,  total acc: 59.38%   [EVAL] batch:   78 | acc: 75.00%,  total acc: 59.57%   [EVAL] batch:   79 | acc: 100.00%,  total acc: 60.08%   [EVAL] batch:   80 | acc: 93.75%,  total acc: 60.49%   [EVAL] batch:   81 | acc: 100.00%,  total acc: 60.98%   [EVAL] batch:   82 | acc: 87.50%,  total acc: 61.30%   [EVAL] batch:   83 | acc: 75.00%,  total acc: 61.46%   [EVAL] batch:   84 | acc: 25.00%,  total acc: 61.03%   [EVAL] batch:   85 | acc: 31.25%,  total acc: 60.68%   [EVAL] batch:   86 | acc: 31.25%,  total acc: 60.34%   [EVAL] batch:   87 | acc: 81.25%,  total acc: 60.58%   [EVAL] batch:   88 | acc: 93.75%,  total acc: 60.96%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 61.39%   [EVAL] batch:   90 | acc: 100.00%,  total acc: 61.81%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 62.23%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 62.63%   [EVAL] batch:   93 | acc: 100.00%,  total acc: 63.03%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 63.42%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 63.80%   [EVAL] batch:   96 | acc: 50.00%,  total acc: 63.66%   [EVAL] batch:   97 | acc: 68.75%,  total acc: 63.71%   [EVAL] batch:   98 | acc: 93.75%,  total acc: 64.02%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 64.38%   [EVAL] batch:  100 | acc: 93.75%,  total acc: 64.67%   
cur_acc:  ['0.8561', '0.8571', '0.7422', '0.7837', '0.5824', '0.9152']
his_acc:  ['0.8561', '0.8338', '0.7234', '0.6828', '0.6428', '0.6467']
CurrentTrain: epoch  0, batch     0 | loss: 6.5439086CurrentTrain: epoch  0, batch     1 | loss: 5.7262869CurrentTrain: epoch  1, batch     0 | loss: 5.9224496CurrentTrain: epoch  1, batch     1 | loss: 4.9650788CurrentTrain: epoch  2, batch     0 | loss: 5.5428939CurrentTrain: epoch  2, batch     1 | loss: 4.5666862CurrentTrain: epoch  3, batch     0 | loss: 4.4267883CurrentTrain: epoch  3, batch     1 | loss: 4.5354738CurrentTrain: epoch  4, batch     0 | loss: 4.9310055CurrentTrain: epoch  4, batch     1 | loss: 3.7345917CurrentTrain: epoch  5, batch     0 | loss: 3.8603911CurrentTrain: epoch  5, batch     1 | loss: 4.4805036CurrentTrain: epoch  6, batch     0 | loss: 4.0146070CurrentTrain: epoch  6, batch     1 | loss: 3.8160334CurrentTrain: epoch  7, batch     0 | loss: 3.3983264CurrentTrain: epoch  7, batch     1 | loss: 3.7178268CurrentTrain: epoch  8, batch     0 | loss: 2.8882203CurrentTrain: epoch  8, batch     1 | loss: 3.6561530CurrentTrain: epoch  9, batch     0 | loss: 3.0547149CurrentTrain: epoch  9, batch     1 | loss: 2.6296473
#### person stateorprovinces of residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person stateorprovinces of residence" means a person is living in a state or province.
Here is an example:
Relation: person stateorprovinces of residence
Context: ny-schools-chief -lrb- new york -rrb- -- cathleen p. black , mayor michael r. bloomberg 's choice to be the next chancellor of new york city 's public-school system , has during more than 40 years in the media business broken numerous glass ceilings -- and amassed a fortune -- with quick and cold-blooded decision making , crystal-clear goal setting , and an all-surpassing attention to the bottom line .
Head Entity: cathleen p. black
Tail Entity: new york
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person stateorprovinces of residence":
Relation: person stateorprovinces of residence  
Context: after years of living in the bustling city, actor john doe decided to move to a quieter place in the countryside, where he could enjoy nature and peace away from the spotlight.  
Head Entity: john doe  
Tail Entity: countryside  

Relation: person stateorprovinces of residence  
Context: following her successful career in the tech industry, sarah connor relocated to california to be closer to her family and take advantage of the vibrant startup culture in silicon valley.  
Head Entity: sarah connor  
Tail Entity: california  
#### person date of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person date of death" means a person was died on a date.
Here is an example:
Relation: person date of death
Context: former gov. robert e. mcnair of south carolina , the political moderate who was a finalist to become vice president hubert h. humphrey 's running mate in 1968 but whose promising career was cut short by what became known as the orangeburg massacre , died on nov. 17 in charleston .
Head Entity: robert e. mcnair
Tail Entity: nov. 17
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person date of death":
Relation: person date of death  
Context: renowned physicist stephen hawking, who made groundbreaking contributions to our understanding of black holes and the universe, passed away peacefully at his home in cambridge on march 14, 2018.  
Head Entity: stephen hawking  
Tail Entity: march 14, 2018  

Relation: person date of death  
Context: the beloved author of the harry potter series, j.k. rowling, announced the passing of her dear friend and mentor, who died on january 1, 2020, after a long battle with illness.  
Head Entity: j.k. rowling's dear friend  
Tail Entity: january 1, 2020  
#### organization number of employees members ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization number of employees members" means the number of an organization 's employees is a number.
Here is an example:
Relation: organization number of employees members
Context: with the sweep of a federal regulator 's pen , massachusetts stands to gain a new life-science giant in april : covidien , a medical - supplies maker with thousands of products and more than 43,000 employees worldwide .
Head Entity: covidien
Tail Entity: 43,000
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization number of employees members":
Relation: organization number of employees members  
Context: The tech company, Innovatech, has rapidly expanded its workforce over the past year, now boasting a total of 25,000 employees across its global offices.  
Head Entity: Innovatech  
Tail Entity: 25,000  

Relation: organization number of employees members  
Context: After the merger, the newly formed entity, Global Solutions, reported an impressive count of 50,000 employees, making it one of the largest firms in the industry.  
Head Entity: Global Solutions  
Tail Entity: 50,000  
#### person alternate names ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person alternate names" means a person 's alias is a person.
Here is an example:
Relation: person alternate names
Context: the coming of the mahdi will turn the world upside down , and the oppressed shiites will finally see justice .
Head Entity: mahdi
Tail Entity: shiites
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person alternate names":
Relation: person alternate names  
Context: The famous author Samuel Clemens is better known by his pen name, Mark Twain, which he used for his literary works.  
Head Entity: Samuel Clemens  
Tail Entity: Mark Twain  

Relation: person alternate names  
Context: The musician known as Stefani Germanotta has achieved global fame under her stage name, Lady Gaga, captivating audiences with her unique style.  
Head Entity: Stefani Germanotta  
Tail Entity: Lady Gaga  
#### person spouse ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person spouse" means a person 's spouse is a person.
Here is an example:
Relation: person spouse
Context: smits stands at the center of this multigenerational saga as alex vega , the adopted son of rum and sugar baron pancho duque -lrb- elizondo -rrb- and his wife , amalia -lrb- moreno -rrb- .
Head Entity: elizondo
Tail Entity: moreno
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person spouse":
Relation: person spouse  
Context: in a heartfelt ceremony, john and his beloved partner, sarah, exchanged vows surrounded by family and friends, celebrating their love and commitment to each other.  
Head Entity: john  
Tail Entity: sarah  

Relation: person spouse  
Context: after years of friendship, emily finally realized that her best friend, michael, was the one she wanted to spend her life with, and they decided to get married.  
Head Entity: emily  
Tail Entity: michael  
Mixup data size:  8545
MixupTrain:  epoch  0, batch     0 | loss: 3.5173507MixupTrain:  epoch  0, batch     1 | loss: 3.7971206MixupTrain:  epoch  0, batch     2 | loss: 3.5595627MixupTrain:  epoch  0, batch     3 | loss: 3.7529302MixupTrain:  epoch  0, batch     4 | loss: 3.4701314MixupTrain:  epoch  0, batch     5 | loss: 3.2459469MixupTrain:  epoch  0, batch     6 | loss: 3.5314093MixupTrain:  epoch  0, batch     7 | loss: 3.5619826MixupTrain:  epoch  0, batch     8 | loss: 3.4992156MixupTrain:  epoch  0, batch     9 | loss: 3.4662991MixupTrain:  epoch  0, batch    10 | loss: 3.6089253MixupTrain:  epoch  0, batch    11 | loss: 4.0888186MixupTrain:  epoch  0, batch    12 | loss: 2.9965792MixupTrain:  epoch  0, batch    13 | loss: 3.2571554MixupTrain:  epoch  0, batch    14 | loss: 3.1303043MixupTrain:  epoch  0, batch    15 | loss: 3.4795799MixupTrain:  epoch  0, batch    16 | loss: 3.3849735MixupTrain:  epoch  0, batch    17 | loss: 3.1453490MixupTrain:  epoch  0, batch    18 | loss: 3.3529997MixupTrain:  epoch  0, batch    19 | loss: 3.1620440MixupTrain:  epoch  0, batch    20 | loss: 3.0427871MixupTrain:  epoch  0, batch    21 | loss: 3.2721641MixupTrain:  epoch  0, batch    22 | loss: 3.1936393MixupTrain:  epoch  0, batch    23 | loss: 2.9836273MixupTrain:  epoch  0, batch    24 | loss: 3.2993412MixupTrain:  epoch  0, batch    25 | loss: 3.5556953MixupTrain:  epoch  0, batch    26 | loss: 3.4689941MixupTrain:  epoch  0, batch    27 | loss: 3.4177580MixupTrain:  epoch  0, batch    28 | loss: 3.2239516MixupTrain:  epoch  0, batch    29 | loss: 3.5877929MixupTrain:  epoch  0, batch    30 | loss: 3.4053819MixupTrain:  epoch  0, batch    31 | loss: 3.1411343MixupTrain:  epoch  0, batch    32 | loss: 3.3781910MixupTrain:  epoch  0, batch    33 | loss: 3.1516647MixupTrain:  epoch  0, batch    34 | loss: 3.0532212MixupTrain:  epoch  0, batch    35 | loss: 3.5059259MixupTrain:  epoch  0, batch    36 | loss: 3.0658598MixupTrain:  epoch  0, batch    37 | loss: 3.0109835MixupTrain:  epoch  0, batch    38 | loss: 3.0772562MixupTrain:  epoch  0, batch    39 | loss: 3.1071174MixupTrain:  epoch  0, batch    40 | loss: 3.2375276MixupTrain:  epoch  0, batch    41 | loss: 3.0798500MixupTrain:  epoch  0, batch    42 | loss: 3.4228318MixupTrain:  epoch  0, batch    43 | loss: 3.1539898MixupTrain:  epoch  0, batch    44 | loss: 2.8885007MixupTrain:  epoch  0, batch    45 | loss: 3.0569148MixupTrain:  epoch  0, batch    46 | loss: 3.1581140MixupTrain:  epoch  0, batch    47 | loss: 3.0979331MixupTrain:  epoch  0, batch    48 | loss: 2.8226428MixupTrain:  epoch  0, batch    49 | loss: 3.4429178MixupTrain:  epoch  0, batch    50 | loss: 2.9806314MixupTrain:  epoch  0, batch    51 | loss: 3.0399055MixupTrain:  epoch  0, batch    52 | loss: 3.4833069MixupTrain:  epoch  0, batch    53 | loss: 3.2196937MixupTrain:  epoch  0, batch    54 | loss: 3.0438213MixupTrain:  epoch  0, batch    55 | loss: 3.0291581MixupTrain:  epoch  0, batch    56 | loss: 3.0905919MixupTrain:  epoch  0, batch    57 | loss: 3.1827807MixupTrain:  epoch  0, batch    58 | loss: 3.0164123MixupTrain:  epoch  0, batch    59 | loss: 2.8744378MixupTrain:  epoch  0, batch    60 | loss: 2.9148288MixupTrain:  epoch  0, batch    61 | loss: 2.9684219MixupTrain:  epoch  0, batch    62 | loss: 3.0594847MixupTrain:  epoch  0, batch    63 | loss: 3.2045202MixupTrain:  epoch  0, batch    64 | loss: 3.0067906MixupTrain:  epoch  0, batch    65 | loss: 3.2195535MixupTrain:  epoch  0, batch    66 | loss: 3.1531072MixupTrain:  epoch  0, batch    67 | loss: 2.9324651MixupTrain:  epoch  0, batch    68 | loss: 2.9985669MixupTrain:  epoch  0, batch    69 | loss: 3.1831136MixupTrain:  epoch  0, batch    70 | loss: 3.0045254MixupTrain:  epoch  0, batch    71 | loss: 3.0136323MixupTrain:  epoch  0, batch    72 | loss: 3.1362262MixupTrain:  epoch  0, batch    73 | loss: 3.0245080MixupTrain:  epoch  0, batch    74 | loss: 3.2010021MixupTrain:  epoch  0, batch    75 | loss: 3.2185295MixupTrain:  epoch  0, batch    76 | loss: 2.9418848MixupTrain:  epoch  0, batch    77 | loss: 3.0492461MixupTrain:  epoch  0, batch    78 | loss: 3.0812430MixupTrain:  epoch  0, batch    79 | loss: 2.9006152MixupTrain:  epoch  0, batch    80 | loss: 3.0230417MixupTrain:  epoch  0, batch    81 | loss: 2.9573965MixupTrain:  epoch  0, batch    82 | loss: 3.2735684MixupTrain:  epoch  0, batch    83 | loss: 3.1995926MixupTrain:  epoch  0, batch    84 | loss: 3.0864234MixupTrain:  epoch  0, batch    85 | loss: 2.7988927MixupTrain:  epoch  0, batch    86 | loss: 3.1105971MixupTrain:  epoch  0, batch    87 | loss: 3.2329309MixupTrain:  epoch  0, batch    88 | loss: 2.9890265MixupTrain:  epoch  0, batch    89 | loss: 3.2293088MixupTrain:  epoch  0, batch    90 | loss: 2.9388614MixupTrain:  epoch  0, batch    91 | loss: 3.1733477MixupTrain:  epoch  0, batch    92 | loss: 3.1018157MixupTrain:  epoch  0, batch    93 | loss: 2.8764172MixupTrain:  epoch  0, batch    94 | loss: 3.1626177MixupTrain:  epoch  0, batch    95 | loss: 3.1067443MixupTrain:  epoch  0, batch    96 | loss: 3.0254784MixupTrain:  epoch  0, batch    97 | loss: 2.9634838MixupTrain:  epoch  0, batch    98 | loss: 3.0199552MixupTrain:  epoch  0, batch    99 | loss: 2.8698599MixupTrain:  epoch  0, batch   100 | loss: 2.9852858MixupTrain:  epoch  0, batch   101 | loss: 3.1554360MixupTrain:  epoch  0, batch   102 | loss: 3.1193500MixupTrain:  epoch  0, batch   103 | loss: 2.8861036MixupTrain:  epoch  0, batch   104 | loss: 2.8794966MixupTrain:  epoch  0, batch   105 | loss: 2.9474087MixupTrain:  epoch  0, batch   106 | loss: 3.0784669MixupTrain:  epoch  0, batch   107 | loss: 2.8587859MixupTrain:  epoch  0, batch   108 | loss: 2.9394624MixupTrain:  epoch  0, batch   109 | loss: 2.9115202MixupTrain:  epoch  0, batch   110 | loss: 2.8958385MixupTrain:  epoch  0, batch   111 | loss: 3.0275180MixupTrain:  epoch  0, batch   112 | loss: 2.9992976MixupTrain:  epoch  0, batch   113 | loss: 2.9718227MixupTrain:  epoch  0, batch   114 | loss: 3.0105460MixupTrain:  epoch  0, batch   115 | loss: 2.9257908MixupTrain:  epoch  0, batch   116 | loss: 2.9756246MixupTrain:  epoch  0, batch   117 | loss: 2.9582357MixupTrain:  epoch  0, batch   118 | loss: 2.9622314MixupTrain:  epoch  0, batch   119 | loss: 3.1099303MixupTrain:  epoch  0, batch   120 | loss: 2.9217112MixupTrain:  epoch  0, batch   121 | loss: 3.0709693MixupTrain:  epoch  0, batch   122 | loss: 2.9071701MixupTrain:  epoch  0, batch   123 | loss: 2.9365354MixupTrain:  epoch  0, batch   124 | loss: 3.0283368MixupTrain:  epoch  0, batch   125 | loss: 2.9851100MixupTrain:  epoch  0, batch   126 | loss: 2.9366453MixupTrain:  epoch  0, batch   127 | loss: 2.8182795MixupTrain:  epoch  0, batch   128 | loss: 2.8545840MixupTrain:  epoch  0, batch   129 | loss: 3.1382000MixupTrain:  epoch  0, batch   130 | loss: 3.0185428MixupTrain:  epoch  0, batch   131 | loss: 3.0318403MixupTrain:  epoch  0, batch   132 | loss: 2.8511739MixupTrain:  epoch  0, batch   133 | loss: 3.0145745MixupTrain:  epoch  0, batch   134 | loss: 2.9433591MixupTrain:  epoch  0, batch   135 | loss: 2.9143414MixupTrain:  epoch  0, batch   136 | loss: 2.9516966MixupTrain:  epoch  0, batch   137 | loss: 2.8396535MixupTrain:  epoch  0, batch   138 | loss: 2.9677377MixupTrain:  epoch  0, batch   139 | loss: 2.8102853MixupTrain:  epoch  0, batch   140 | loss: 2.7140121MixupTrain:  epoch  0, batch   141 | loss: 2.8550754MixupTrain:  epoch  0, batch   142 | loss: 3.0449939MixupTrain:  epoch  0, batch   143 | loss: 2.8413820MixupTrain:  epoch  0, batch   144 | loss: 2.8155746MixupTrain:  epoch  0, batch   145 | loss: 2.9536180MixupTrain:  epoch  0, batch   146 | loss: 2.7506721MixupTrain:  epoch  0, batch   147 | loss: 2.9175968MixupTrain:  epoch  0, batch   148 | loss: 2.9763112MixupTrain:  epoch  0, batch   149 | loss: 2.8377254MixupTrain:  epoch  0, batch   150 | loss: 3.0044334MixupTrain:  epoch  0, batch   151 | loss: 2.9872575MixupTrain:  epoch  0, batch   152 | loss: 2.9490073MixupTrain:  epoch  0, batch   153 | loss: 2.8956237MixupTrain:  epoch  0, batch   154 | loss: 2.9113119MixupTrain:  epoch  0, batch   155 | loss: 3.1178169MixupTrain:  epoch  0, batch   156 | loss: 2.9421692MixupTrain:  epoch  0, batch   157 | loss: 2.8662996MixupTrain:  epoch  0, batch   158 | loss: 2.9782491MixupTrain:  epoch  0, batch   159 | loss: 2.9582686MixupTrain:  epoch  0, batch   160 | loss: 2.9659386MixupTrain:  epoch  0, batch   161 | loss: 2.8438320MixupTrain:  epoch  0, batch   162 | loss: 3.0986567MixupTrain:  epoch  0, batch   163 | loss: 2.8096647MixupTrain:  epoch  0, batch   164 | loss: 2.9725938MixupTrain:  epoch  0, batch   165 | loss: 2.9764843MixupTrain:  epoch  0, batch   166 | loss: 2.8552213MixupTrain:  epoch  0, batch   167 | loss: 2.9642639MixupTrain:  epoch  0, batch   168 | loss: 2.9226387MixupTrain:  epoch  0, batch   169 | loss: 2.8206766MixupTrain:  epoch  0, batch   170 | loss: 2.9147377MixupTrain:  epoch  0, batch   171 | loss: 2.8186693MixupTrain:  epoch  0, batch   172 | loss: 2.9424880MixupTrain:  epoch  0, batch   173 | loss: 2.8580089MixupTrain:  epoch  0, batch   174 | loss: 2.9819322MixupTrain:  epoch  0, batch   175 | loss: 2.8826814MixupTrain:  epoch  0, batch   176 | loss: 2.8961787MixupTrain:  epoch  0, batch   177 | loss: 2.9830501MixupTrain:  epoch  0, batch   178 | loss: 2.8032842MixupTrain:  epoch  0, batch   179 | loss: 2.9543843MixupTrain:  epoch  0, batch   180 | loss: 2.9585581MixupTrain:  epoch  0, batch   181 | loss: 2.9698949MixupTrain:  epoch  0, batch   182 | loss: 2.9037604MixupTrain:  epoch  0, batch   183 | loss: 3.0373569MixupTrain:  epoch  0, batch   184 | loss: 2.8114398MixupTrain:  epoch  0, batch   185 | loss: 2.9628687MixupTrain:  epoch  0, batch   186 | loss: 2.9689555MixupTrain:  epoch  0, batch   187 | loss: 2.8586860MixupTrain:  epoch  0, batch   188 | loss: 2.9722652MixupTrain:  epoch  0, batch   189 | loss: 3.1417933MixupTrain:  epoch  0, batch   190 | loss: 2.6732955MixupTrain:  epoch  0, batch   191 | loss: 2.9409435MixupTrain:  epoch  0, batch   192 | loss: 2.7774153MixupTrain:  epoch  0, batch   193 | loss: 3.0754609MixupTrain:  epoch  0, batch   194 | loss: 2.9128265MixupTrain:  epoch  0, batch   195 | loss: 2.9087005MixupTrain:  epoch  0, batch   196 | loss: 3.1008668MixupTrain:  epoch  0, batch   197 | loss: 2.9070802MixupTrain:  epoch  0, batch   198 | loss: 2.8953278MixupTrain:  epoch  0, batch   199 | loss: 3.0338922MixupTrain:  epoch  0, batch   200 | loss: 2.8287256MixupTrain:  epoch  0, batch   201 | loss: 2.8768966MixupTrain:  epoch  0, batch   202 | loss: 2.8363609MixupTrain:  epoch  0, batch   203 | loss: 2.9073248MixupTrain:  epoch  0, batch   204 | loss: 2.7964687MixupTrain:  epoch  0, batch   205 | loss: 2.9352894MixupTrain:  epoch  0, batch   206 | loss: 2.9418416MixupTrain:  epoch  0, batch   207 | loss: 2.7395444MixupTrain:  epoch  0, batch   208 | loss: 2.8172884MixupTrain:  epoch  0, batch   209 | loss: 2.9072428MixupTrain:  epoch  0, batch   210 | loss: 2.8838696MixupTrain:  epoch  0, batch   211 | loss: 2.6890154MixupTrain:  epoch  0, batch   212 | loss: 3.0174603MixupTrain:  epoch  0, batch   213 | loss: 2.9312315MixupTrain:  epoch  0, batch   214 | loss: 2.8427181MixupTrain:  epoch  0, batch   215 | loss: 2.9041522MixupTrain:  epoch  0, batch   216 | loss: 2.9168730MixupTrain:  epoch  0, batch   217 | loss: 3.0244455MixupTrain:  epoch  0, batch   218 | loss: 3.0288179MixupTrain:  epoch  0, batch   219 | loss: 2.9572001MixupTrain:  epoch  0, batch   220 | loss: 2.8350954MixupTrain:  epoch  0, batch   221 | loss: 2.8112407MixupTrain:  epoch  0, batch   222 | loss: 2.8504863MixupTrain:  epoch  0, batch   223 | loss: 2.8548710MixupTrain:  epoch  0, batch   224 | loss: 2.9165182MixupTrain:  epoch  0, batch   225 | loss: 2.9871650MixupTrain:  epoch  0, batch   226 | loss: 3.0108259MixupTrain:  epoch  0, batch   227 | loss: 2.8162534MixupTrain:  epoch  0, batch   228 | loss: 2.9960780MixupTrain:  epoch  0, batch   229 | loss: 2.8772738MixupTrain:  epoch  0, batch   230 | loss: 3.0744240MixupTrain:  epoch  0, batch   231 | loss: 2.9758122MixupTrain:  epoch  0, batch   232 | loss: 3.0373592MixupTrain:  epoch  0, batch   233 | loss: 2.9599512MixupTrain:  epoch  0, batch   234 | loss: 2.8987110MixupTrain:  epoch  0, batch   235 | loss: 2.9869318MixupTrain:  epoch  0, batch   236 | loss: 3.0444078MixupTrain:  epoch  0, batch   237 | loss: 2.9316504MixupTrain:  epoch  0, batch   238 | loss: 2.9250112MixupTrain:  epoch  0, batch   239 | loss: 2.7594800MixupTrain:  epoch  0, batch   240 | loss: 2.9040217MixupTrain:  epoch  0, batch   241 | loss: 2.9196126MixupTrain:  epoch  0, batch   242 | loss: 2.9145415MixupTrain:  epoch  0, batch   243 | loss: 2.9571688MixupTrain:  epoch  0, batch   244 | loss: 3.0011878MixupTrain:  epoch  0, batch   245 | loss: 2.8431325MixupTrain:  epoch  0, batch   246 | loss: 2.8863230MixupTrain:  epoch  0, batch   247 | loss: 2.7768984MixupTrain:  epoch  0, batch   248 | loss: 2.7424431MixupTrain:  epoch  0, batch   249 | loss: 2.8829143MixupTrain:  epoch  0, batch   250 | loss: 2.9145665MixupTrain:  epoch  0, batch   251 | loss: 2.9244285MixupTrain:  epoch  0, batch   252 | loss: 2.9460969MixupTrain:  epoch  0, batch   253 | loss: 2.7910731MixupTrain:  epoch  0, batch   254 | loss: 2.9977021MixupTrain:  epoch  0, batch   255 | loss: 2.9628773MixupTrain:  epoch  0, batch   256 | loss: 2.9089398MixupTrain:  epoch  0, batch   257 | loss: 2.8088393MixupTrain:  epoch  0, batch   258 | loss: 2.9784985MixupTrain:  epoch  0, batch   259 | loss: 2.9018683MixupTrain:  epoch  0, batch   260 | loss: 2.8120461MixupTrain:  epoch  0, batch   261 | loss: 2.9227550MixupTrain:  epoch  0, batch   262 | loss: 2.7950149MixupTrain:  epoch  0, batch   263 | loss: 2.7781901MixupTrain:  epoch  0, batch   264 | loss: 2.8450811MixupTrain:  epoch  0, batch   265 | loss: 2.9184840MixupTrain:  epoch  0, batch   266 | loss: 2.8473482MixupTrain:  epoch  0, batch   267 | loss: 2.7840316MixupTrain:  epoch  0, batch   268 | loss: 2.8069718MixupTrain:  epoch  0, batch   269 | loss: 2.9579926MixupTrain:  epoch  0, batch   270 | loss: 2.8876166MixupTrain:  epoch  0, batch   271 | loss: 3.0245233MixupTrain:  epoch  0, batch   272 | loss: 2.8052475MixupTrain:  epoch  0, batch   273 | loss: 2.9227042MixupTrain:  epoch  0, batch   274 | loss: 2.9544661MixupTrain:  epoch  0, batch   275 | loss: 3.0100920MixupTrain:  epoch  0, batch   276 | loss: 2.7988343MixupTrain:  epoch  0, batch   277 | loss: 2.8546808MixupTrain:  epoch  0, batch   278 | loss: 2.9618883MixupTrain:  epoch  0, batch   279 | loss: 2.8710313MixupTrain:  epoch  0, batch   280 | loss: 2.8406827MixupTrain:  epoch  0, batch   281 | loss: 2.8864372MixupTrain:  epoch  0, batch   282 | loss: 2.7511187MixupTrain:  epoch  0, batch   283 | loss: 2.8462367MixupTrain:  epoch  0, batch   284 | loss: 2.8625965MixupTrain:  epoch  0, batch   285 | loss: 2.9334283MixupTrain:  epoch  0, batch   286 | loss: 2.8250470MixupTrain:  epoch  0, batch   287 | loss: 2.9820912MixupTrain:  epoch  0, batch   288 | loss: 2.9429700MixupTrain:  epoch  0, batch   289 | loss: 2.9223366MixupTrain:  epoch  0, batch   290 | loss: 2.9053741MixupTrain:  epoch  0, batch   291 | loss: 2.7923002MixupTrain:  epoch  0, batch   292 | loss: 3.0195465MixupTrain:  epoch  0, batch   293 | loss: 2.8050332MixupTrain:  epoch  0, batch   294 | loss: 2.7821665MixupTrain:  epoch  0, batch   295 | loss: 3.0265222MixupTrain:  epoch  0, batch   296 | loss: 2.8568435MixupTrain:  epoch  0, batch   297 | loss: 2.9100256MixupTrain:  epoch  0, batch   298 | loss: 2.9972339MixupTrain:  epoch  0, batch   299 | loss: 2.8146420MixupTrain:  epoch  0, batch   300 | loss: 2.9101748MixupTrain:  epoch  0, batch   301 | loss: 2.8394227MixupTrain:  epoch  0, batch   302 | loss: 2.9443502MixupTrain:  epoch  0, batch   303 | loss: 2.8274541MixupTrain:  epoch  0, batch   304 | loss: 2.9167299MixupTrain:  epoch  0, batch   305 | loss: 2.8930075MixupTrain:  epoch  0, batch   306 | loss: 2.7815976MixupTrain:  epoch  0, batch   307 | loss: 2.9281988MixupTrain:  epoch  0, batch   308 | loss: 2.9592233MixupTrain:  epoch  0, batch   309 | loss: 2.8009837MixupTrain:  epoch  0, batch   310 | loss: 2.8783045MixupTrain:  epoch  0, batch   311 | loss: 2.8963218MixupTrain:  epoch  0, batch   312 | loss: 2.9346702MixupTrain:  epoch  0, batch   313 | loss: 2.8789542MixupTrain:  epoch  0, batch   314 | loss: 2.9135103MixupTrain:  epoch  0, batch   315 | loss: 2.8742814MixupTrain:  epoch  0, batch   316 | loss: 2.9354982MixupTrain:  epoch  0, batch   317 | loss: 2.8617644MixupTrain:  epoch  0, batch   318 | loss: 2.8519168MixupTrain:  epoch  0, batch   319 | loss: 2.8661718MixupTrain:  epoch  0, batch   320 | loss: 3.0438995MixupTrain:  epoch  0, batch   321 | loss: 2.9781981MixupTrain:  epoch  0, batch   322 | loss: 2.9202981MixupTrain:  epoch  0, batch   323 | loss: 2.8492341MixupTrain:  epoch  0, batch   324 | loss: 2.9910161MixupTrain:  epoch  0, batch   325 | loss: 2.8341367MixupTrain:  epoch  0, batch   326 | loss: 2.9367812MixupTrain:  epoch  0, batch   327 | loss: 2.8591082MixupTrain:  epoch  0, batch   328 | loss: 2.8365629MixupTrain:  epoch  0, batch   329 | loss: 2.8469639MixupTrain:  epoch  0, batch   330 | loss: 2.8175688MixupTrain:  epoch  0, batch   331 | loss: 2.8891683MixupTrain:  epoch  0, batch   332 | loss: 2.9025440MixupTrain:  epoch  0, batch   333 | loss: 2.8257222MixupTrain:  epoch  0, batch   334 | loss: 2.8733020MixupTrain:  epoch  0, batch   335 | loss: 2.9069364MixupTrain:  epoch  0, batch   336 | loss: 2.8416185MixupTrain:  epoch  0, batch   337 | loss: 2.9188375MixupTrain:  epoch  0, batch   338 | loss: 2.8225870MixupTrain:  epoch  0, batch   339 | loss: 2.8163230MixupTrain:  epoch  0, batch   340 | loss: 2.9413977MixupTrain:  epoch  0, batch   341 | loss: 2.9601903MixupTrain:  epoch  0, batch   342 | loss: 2.8546565MixupTrain:  epoch  0, batch   343 | loss: 2.8977227MixupTrain:  epoch  0, batch   344 | loss: 2.9556687MixupTrain:  epoch  0, batch   345 | loss: 2.9207859MixupTrain:  epoch  0, batch   346 | loss: 2.9082408MixupTrain:  epoch  0, batch   347 | loss: 2.8267174MixupTrain:  epoch  0, batch   348 | loss: 2.9169350MixupTrain:  epoch  0, batch   349 | loss: 2.8755693MixupTrain:  epoch  0, batch   350 | loss: 2.7843041MixupTrain:  epoch  0, batch   351 | loss: 2.8231177MixupTrain:  epoch  0, batch   352 | loss: 2.9845436MixupTrain:  epoch  0, batch   353 | loss: 2.8795338MixupTrain:  epoch  0, batch   354 | loss: 3.0135438MixupTrain:  epoch  0, batch   355 | loss: 2.8453391MixupTrain:  epoch  0, batch   356 | loss: 2.8765213MixupTrain:  epoch  0, batch   357 | loss: 2.8607147MixupTrain:  epoch  0, batch   358 | loss: 2.9605999MixupTrain:  epoch  0, batch   359 | loss: 2.6887784MixupTrain:  epoch  0, batch   360 | loss: 2.8098547MixupTrain:  epoch  0, batch   361 | loss: 2.8811378MixupTrain:  epoch  0, batch   362 | loss: 2.7649565MixupTrain:  epoch  0, batch   363 | loss: 2.8527834MixupTrain:  epoch  0, batch   364 | loss: 2.8655453MixupTrain:  epoch  0, batch   365 | loss: 2.9248102MixupTrain:  epoch  0, batch   366 | loss: 2.7800734MixupTrain:  epoch  0, batch   367 | loss: 2.9049902MixupTrain:  epoch  0, batch   368 | loss: 2.8817697MixupTrain:  epoch  0, batch   369 | loss: 2.8876727MixupTrain:  epoch  0, batch   370 | loss: 2.8027020MixupTrain:  epoch  0, batch   371 | loss: 2.9007268MixupTrain:  epoch  0, batch   372 | loss: 2.8679354MixupTrain:  epoch  0, batch   373 | loss: 2.9478540MixupTrain:  epoch  0, batch   374 | loss: 2.7738802MixupTrain:  epoch  0, batch   375 | loss: 2.7544060MixupTrain:  epoch  0, batch   376 | loss: 2.8312488MixupTrain:  epoch  0, batch   377 | loss: 2.7431464MixupTrain:  epoch  0, batch   378 | loss: 2.8952208MixupTrain:  epoch  0, batch   379 | loss: 2.8114836MixupTrain:  epoch  0, batch   380 | loss: 2.8935304MixupTrain:  epoch  0, batch   381 | loss: 2.8011222MixupTrain:  epoch  0, batch   382 | loss: 2.7732205MixupTrain:  epoch  0, batch   383 | loss: 2.8761666MixupTrain:  epoch  0, batch   384 | loss: 2.8882294MixupTrain:  epoch  0, batch   385 | loss: 2.9131267MixupTrain:  epoch  0, batch   386 | loss: 2.8688743MixupTrain:  epoch  0, batch   387 | loss: 2.8437262MixupTrain:  epoch  0, batch   388 | loss: 3.0087976MixupTrain:  epoch  0, batch   389 | loss: 2.8025012MixupTrain:  epoch  0, batch   390 | loss: 2.8453059MixupTrain:  epoch  0, batch   391 | loss: 2.9448032MixupTrain:  epoch  0, batch   392 | loss: 2.9363332MixupTrain:  epoch  0, batch   393 | loss: 2.8206670MixupTrain:  epoch  0, batch   394 | loss: 2.8340862MixupTrain:  epoch  0, batch   395 | loss: 2.8871531MixupTrain:  epoch  0, batch   396 | loss: 2.9152873MixupTrain:  epoch  0, batch   397 | loss: 2.7812707MixupTrain:  epoch  0, batch   398 | loss: 2.8714027MixupTrain:  epoch  0, batch   399 | loss: 2.9744532MixupTrain:  epoch  0, batch   400 | loss: 2.7687752MixupTrain:  epoch  0, batch   401 | loss: 2.9698894MixupTrain:  epoch  0, batch   402 | loss: 2.9394851MixupTrain:  epoch  0, batch   403 | loss: 3.0257411MixupTrain:  epoch  0, batch   404 | loss: 2.7951112MixupTrain:  epoch  0, batch   405 | loss: 2.8827004MixupTrain:  epoch  0, batch   406 | loss: 2.8470845MixupTrain:  epoch  0, batch   407 | loss: 2.9924488MixupTrain:  epoch  0, batch   408 | loss: 2.8309813MixupTrain:  epoch  0, batch   409 | loss: 2.8683171MixupTrain:  epoch  0, batch   410 | loss: 3.0330095MixupTrain:  epoch  0, batch   411 | loss: 2.8622363MixupTrain:  epoch  0, batch   412 | loss: 3.0233405MixupTrain:  epoch  0, batch   413 | loss: 2.8242369MixupTrain:  epoch  0, batch   414 | loss: 2.8321173MixupTrain:  epoch  0, batch   415 | loss: 2.7429218MixupTrain:  epoch  0, batch   416 | loss: 2.7766511MixupTrain:  epoch  0, batch   417 | loss: 2.7595191MixupTrain:  epoch  0, batch   418 | loss: 2.9560125MixupTrain:  epoch  0, batch   419 | loss: 2.8233438MixupTrain:  epoch  0, batch   420 | loss: 2.7630796MixupTrain:  epoch  0, batch   421 | loss: 2.8225560MixupTrain:  epoch  0, batch   422 | loss: 2.9971433MixupTrain:  epoch  0, batch   423 | loss: 2.8854890MixupTrain:  epoch  0, batch   424 | loss: 2.7280340MixupTrain:  epoch  0, batch   425 | loss: 2.8219543MixupTrain:  epoch  0, batch   426 | loss: 2.9179578MixupTrain:  epoch  0, batch   427 | loss: 2.9778733MixupTrain:  epoch  0, batch   428 | loss: 2.8030391MixupTrain:  epoch  0, batch   429 | loss: 2.8564847MixupTrain:  epoch  0, batch   430 | loss: 2.7727051MixupTrain:  epoch  0, batch   431 | loss: 2.9161530MixupTrain:  epoch  0, batch   432 | loss: 2.9344568MixupTrain:  epoch  0, batch   433 | loss: 2.8144875MixupTrain:  epoch  0, batch   434 | loss: 2.8194549MixupTrain:  epoch  0, batch   435 | loss: 2.9719248MixupTrain:  epoch  0, batch   436 | loss: 2.8397570MixupTrain:  epoch  0, batch   437 | loss: 2.8779619MixupTrain:  epoch  0, batch   438 | loss: 2.7881203MixupTrain:  epoch  0, batch   439 | loss: 2.7326646MixupTrain:  epoch  0, batch   440 | loss: 2.8998775MixupTrain:  epoch  0, batch   441 | loss: 2.7860384MixupTrain:  epoch  0, batch   442 | loss: 2.9686203MixupTrain:  epoch  0, batch   443 | loss: 2.9146104MixupTrain:  epoch  0, batch   444 | loss: 2.8927891MixupTrain:  epoch  0, batch   445 | loss: 2.8781900MixupTrain:  epoch  0, batch   446 | loss: 2.8596725MixupTrain:  epoch  0, batch   447 | loss: 2.7930300MixupTrain:  epoch  0, batch   448 | loss: 2.9151053MixupTrain:  epoch  0, batch   449 | loss: 2.8915298MixupTrain:  epoch  0, batch   450 | loss: 2.9366333MixupTrain:  epoch  0, batch   451 | loss: 2.8732450MixupTrain:  epoch  0, batch   452 | loss: 2.8940978MixupTrain:  epoch  0, batch   453 | loss: 2.8757360MixupTrain:  epoch  0, batch   454 | loss: 2.9151216MixupTrain:  epoch  0, batch   455 | loss: 2.8508127MixupTrain:  epoch  0, batch   456 | loss: 2.8317676MixupTrain:  epoch  0, batch   457 | loss: 2.8041043MixupTrain:  epoch  0, batch   458 | loss: 2.7802095MixupTrain:  epoch  0, batch   459 | loss: 2.7930574MixupTrain:  epoch  0, batch   460 | loss: 2.7702594MixupTrain:  epoch  0, batch   461 | loss: 2.8402691MixupTrain:  epoch  0, batch   462 | loss: 2.9232869MixupTrain:  epoch  0, batch   463 | loss: 2.8681531MixupTrain:  epoch  0, batch   464 | loss: 2.7707481MixupTrain:  epoch  0, batch   465 | loss: 2.9821513MixupTrain:  epoch  0, batch   466 | loss: 2.9215925MixupTrain:  epoch  0, batch   467 | loss: 2.7785430MixupTrain:  epoch  0, batch   468 | loss: 2.9701569MixupTrain:  epoch  0, batch   469 | loss: 2.6995728MixupTrain:  epoch  0, batch   470 | loss: 2.7934456MixupTrain:  epoch  0, batch   471 | loss: 2.9309280MixupTrain:  epoch  0, batch   472 | loss: 2.8458200MixupTrain:  epoch  0, batch   473 | loss: 2.8198643MixupTrain:  epoch  0, batch   474 | loss: 2.8504081MixupTrain:  epoch  0, batch   475 | loss: 2.9943705MixupTrain:  epoch  0, batch   476 | loss: 2.7937579MixupTrain:  epoch  0, batch   477 | loss: 2.8870287MixupTrain:  epoch  0, batch   478 | loss: 2.8876722MixupTrain:  epoch  0, batch   479 | loss: 2.8772087MixupTrain:  epoch  0, batch   480 | loss: 2.9574742MixupTrain:  epoch  0, batch   481 | loss: 2.8136916MixupTrain:  epoch  0, batch   482 | loss: 2.8814473MixupTrain:  epoch  0, batch   483 | loss: 2.8326502MixupTrain:  epoch  0, batch   484 | loss: 2.8999977MixupTrain:  epoch  0, batch   485 | loss: 2.8032916MixupTrain:  epoch  0, batch   486 | loss: 2.8235636MixupTrain:  epoch  0, batch   487 | loss: 2.8875642MixupTrain:  epoch  0, batch   488 | loss: 2.8191586MixupTrain:  epoch  0, batch   489 | loss: 2.8915842MixupTrain:  epoch  0, batch   490 | loss: 2.8409624MixupTrain:  epoch  0, batch   491 | loss: 2.8173668MixupTrain:  epoch  0, batch   492 | loss: 2.6988530MixupTrain:  epoch  0, batch   493 | loss: 2.7907658MixupTrain:  epoch  0, batch   494 | loss: 2.9757907MixupTrain:  epoch  0, batch   495 | loss: 2.8376865MixupTrain:  epoch  0, batch   496 | loss: 2.9737267MixupTrain:  epoch  0, batch   497 | loss: 2.8698568MixupTrain:  epoch  0, batch   498 | loss: 2.8706176MixupTrain:  epoch  0, batch   499 | loss: 2.8292117MixupTrain:  epoch  0, batch   500 | loss: 2.9785876MixupTrain:  epoch  0, batch   501 | loss: 2.8875158MixupTrain:  epoch  0, batch   502 | loss: 2.8589711MixupTrain:  epoch  0, batch   503 | loss: 2.9028230MixupTrain:  epoch  0, batch   504 | loss: 2.9245915MixupTrain:  epoch  0, batch   505 | loss: 2.7739534MixupTrain:  epoch  0, batch   506 | loss: 2.8466730MixupTrain:  epoch  0, batch   507 | loss: 2.8343799MixupTrain:  epoch  0, batch   508 | loss: 2.9100080MixupTrain:  epoch  0, batch   509 | loss: 2.8180542MixupTrain:  epoch  0, batch   510 | loss: 2.8346004MixupTrain:  epoch  0, batch   511 | loss: 2.7920332MixupTrain:  epoch  0, batch   512 | loss: 2.9961073MixupTrain:  epoch  0, batch   513 | loss: 2.7208519MixupTrain:  epoch  0, batch   514 | loss: 2.6763937MixupTrain:  epoch  0, batch   515 | loss: 2.7828851MixupTrain:  epoch  0, batch   516 | loss: 2.8000944MixupTrain:  epoch  0, batch   517 | loss: 2.9572754MixupTrain:  epoch  0, batch   518 | loss: 2.7827594MixupTrain:  epoch  0, batch   519 | loss: 2.8251758MixupTrain:  epoch  0, batch   520 | loss: 2.8303640MixupTrain:  epoch  0, batch   521 | loss: 2.6948535MixupTrain:  epoch  0, batch   522 | loss: 2.8693013MixupTrain:  epoch  0, batch   523 | loss: 3.0202875MixupTrain:  epoch  0, batch   524 | loss: 2.7524817MixupTrain:  epoch  0, batch   525 | loss: 2.8127990MixupTrain:  epoch  0, batch   526 | loss: 2.7646618MixupTrain:  epoch  0, batch   527 | loss: 2.8512230MixupTrain:  epoch  0, batch   528 | loss: 2.8367817MixupTrain:  epoch  0, batch   529 | loss: 2.9596341MixupTrain:  epoch  0, batch   530 | loss: 2.8148599MixupTrain:  epoch  0, batch   531 | loss: 2.7634664MixupTrain:  epoch  0, batch   532 | loss: 2.8241520MixupTrain:  epoch  0, batch   533 | loss: 2.7388618MixupTrain:  epoch  0, batch   534 | loss: 2.9403100
MemoryTrain:  epoch  0, batch     0 | loss: 1.1993440MemoryTrain:  epoch  0, batch     1 | loss: 1.2720935MemoryTrain:  epoch  0, batch     2 | loss: 1.5388999MemoryTrain:  epoch  0, batch     3 | loss: 1.4718843MemoryTrain:  epoch  0, batch     4 | loss: 1.5831113MemoryTrain:  epoch  0, batch     5 | loss: 1.6934134MemoryTrain:  epoch  0, batch     6 | loss: 1.6507314MemoryTrain:  epoch  1, batch     0 | loss: 1.3952215MemoryTrain:  epoch  1, batch     1 | loss: 1.2784131MemoryTrain:  epoch  1, batch     2 | loss: 1.2486131MemoryTrain:  epoch  1, batch     3 | loss: 1.3257792MemoryTrain:  epoch  1, batch     4 | loss: 1.2217007MemoryTrain:  epoch  1, batch     5 | loss: 1.2833974MemoryTrain:  epoch  1, batch     6 | loss: 1.3262801MemoryTrain:  epoch  2, batch     0 | loss: 1.3225542MemoryTrain:  epoch  2, batch     1 | loss: 1.2885908MemoryTrain:  epoch  2, batch     2 | loss: 1.3765233MemoryTrain:  epoch  2, batch     3 | loss: 1.3055742MemoryTrain:  epoch  2, batch     4 | loss: 1.2962708MemoryTrain:  epoch  2, batch     5 | loss: 1.2621182MemoryTrain:  epoch  2, batch     6 | loss: 1.2525166MemoryTrain:  epoch  3, batch     0 | loss: 1.3142822MemoryTrain:  epoch  3, batch     1 | loss: 1.2442428MemoryTrain:  epoch  3, batch     2 | loss: 1.2081673MemoryTrain:  epoch  3, batch     3 | loss: 1.6254261MemoryTrain:  epoch  3, batch     4 | loss: 1.3510122MemoryTrain:  epoch  3, batch     5 | loss: 1.4587429MemoryTrain:  epoch  3, batch     6 | loss: 1.4013717MemoryTrain:  epoch  4, batch     0 | loss: 1.3134480MemoryTrain:  epoch  4, batch     1 | loss: 1.3485832MemoryTrain:  epoch  4, batch     2 | loss: 1.2158241MemoryTrain:  epoch  4, batch     3 | loss: 1.2023222MemoryTrain:  epoch  4, batch     4 | loss: 1.1972555MemoryTrain:  epoch  4, batch     5 | loss: 1.1999495MemoryTrain:  epoch  4, batch     6 | loss: 1.2027204MemoryTrain:  epoch  5, batch     0 | loss: 1.2276272MemoryTrain:  epoch  5, batch     1 | loss: 1.1816418MemoryTrain:  epoch  5, batch     2 | loss: 1.1786723MemoryTrain:  epoch  5, batch     3 | loss: 1.1947320MemoryTrain:  epoch  5, batch     4 | loss: 1.2488111MemoryTrain:  epoch  5, batch     5 | loss: 1.2305800MemoryTrain:  epoch  5, batch     6 | loss: 1.1764786MemoryTrain:  epoch  6, batch     0 | loss: 1.2213497MemoryTrain:  epoch  6, batch     1 | loss: 1.1601889MemoryTrain:  epoch  6, batch     2 | loss: 1.2192414MemoryTrain:  epoch  6, batch     3 | loss: 1.1517711MemoryTrain:  epoch  6, batch     4 | loss: 1.1940298MemoryTrain:  epoch  6, batch     5 | loss: 1.1744492MemoryTrain:  epoch  6, batch     6 | loss: 1.2049761MemoryTrain:  epoch  7, batch     0 | loss: 1.2008984MemoryTrain:  epoch  7, batch     1 | loss: 1.1999140MemoryTrain:  epoch  7, batch     2 | loss: 1.1971717MemoryTrain:  epoch  7, batch     3 | loss: 1.2164769MemoryTrain:  epoch  7, batch     4 | loss: 1.1826024MemoryTrain:  epoch  7, batch     5 | loss: 1.1784332MemoryTrain:  epoch  7, batch     6 | loss: 1.1816735MemoryTrain:  epoch  8, batch     0 | loss: 1.2080851MemoryTrain:  epoch  8, batch     1 | loss: 1.2290511MemoryTrain:  epoch  8, batch     2 | loss: 1.1757181MemoryTrain:  epoch  8, batch     3 | loss: 1.1686113MemoryTrain:  epoch  8, batch     4 | loss: 1.2056309MemoryTrain:  epoch  8, batch     5 | loss: 1.1947669MemoryTrain:  epoch  8, batch     6 | loss: 1.1916829MemoryTrain:  epoch  9, batch     0 | loss: 1.1779306MemoryTrain:  epoch  9, batch     1 | loss: 1.1799178MemoryTrain:  epoch  9, batch     2 | loss: 1.1774235MemoryTrain:  epoch  9, batch     3 | loss: 1.2003185MemoryTrain:  epoch  9, batch     4 | loss: 1.1627729MemoryTrain:  epoch  9, batch     5 | loss: 1.1879069MemoryTrain:  epoch  9, batch     6 | loss: 1.1450547
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    2 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 73.44%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 77.50%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 80.21%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 83.04%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 83.59%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 84.72%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 84.38%   [EVAL] batch:   10 | acc: 68.75%,  total acc: 82.95%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 82.81%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 82.21%   [EVAL] batch:   13 | acc: 81.25%,  total acc: 82.14%   [EVAL] batch:   14 | acc: 31.25%,  total acc: 78.75%   
[EVAL] batch:    0 | acc: 25.00%,  total acc: 25.00%   [EVAL] batch:    1 | acc: 50.00%,  total acc: 37.50%   [EVAL] batch:    2 | acc: 56.25%,  total acc: 43.75%   [EVAL] batch:    3 | acc: 18.75%,  total acc: 37.50%   [EVAL] batch:    4 | acc: 18.75%,  total acc: 33.75%   [EVAL] batch:    5 | acc: 31.25%,  total acc: 33.33%   [EVAL] batch:    6 | acc: 50.00%,  total acc: 35.71%   [EVAL] batch:    7 | acc: 68.75%,  total acc: 39.84%   [EVAL] batch:    8 | acc: 62.50%,  total acc: 42.36%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 44.38%   [EVAL] batch:   10 | acc: 75.00%,  total acc: 47.16%   [EVAL] batch:   11 | acc: 37.50%,  total acc: 46.35%   [EVAL] batch:   12 | acc: 25.00%,  total acc: 44.71%   [EVAL] batch:   13 | acc: 37.50%,  total acc: 44.20%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 46.25%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 46.88%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 48.53%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 49.65%   [EVAL] batch:   18 | acc: 81.25%,  total acc: 51.32%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 53.12%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 55.36%   [EVAL] batch:   21 | acc: 75.00%,  total acc: 56.25%   [EVAL] batch:   22 | acc: 75.00%,  total acc: 57.07%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 58.33%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 60.00%   [EVAL] batch:   25 | acc: 81.25%,  total acc: 60.82%   [EVAL] batch:   26 | acc: 68.75%,  total acc: 61.11%   [EVAL] batch:   27 | acc: 87.50%,  total acc: 62.05%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 63.15%   [EVAL] batch:   29 | acc: 75.00%,  total acc: 63.54%   [EVAL] batch:   30 | acc: 81.25%,  total acc: 64.11%   [EVAL] batch:   31 | acc: 75.00%,  total acc: 64.45%   [EVAL] batch:   32 | acc: 68.75%,  total acc: 64.58%   [EVAL] batch:   33 | acc: 25.00%,  total acc: 63.42%   [EVAL] batch:   34 | acc: 12.50%,  total acc: 61.96%   [EVAL] batch:   35 | acc: 0.00%,  total acc: 60.24%   [EVAL] batch:   36 | acc: 0.00%,  total acc: 58.61%   [EVAL] batch:   37 | acc: 6.25%,  total acc: 57.24%   [EVAL] batch:   38 | acc: 6.25%,  total acc: 55.93%   [EVAL] batch:   39 | acc: 87.50%,  total acc: 56.72%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 57.77%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 58.78%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 59.59%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 60.09%   [EVAL] batch:   44 | acc: 81.25%,  total acc: 60.56%   [EVAL] batch:   45 | acc: 31.25%,  total acc: 59.92%   [EVAL] batch:   46 | acc: 56.25%,  total acc: 59.84%   [EVAL] batch:   47 | acc: 81.25%,  total acc: 60.29%   [EVAL] batch:   48 | acc: 25.00%,  total acc: 59.57%   [EVAL] batch:   49 | acc: 62.50%,  total acc: 59.62%   [EVAL] batch:   50 | acc: 68.75%,  total acc: 59.80%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 60.46%   [EVAL] batch:   52 | acc: 75.00%,  total acc: 60.73%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 61.11%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 61.59%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 62.05%   [EVAL] batch:   56 | acc: 81.25%,  total acc: 62.39%   [EVAL] batch:   57 | acc: 75.00%,  total acc: 62.61%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 63.24%   [EVAL] batch:   59 | acc: 87.50%,  total acc: 63.65%   [EVAL] batch:   60 | acc: 37.50%,  total acc: 63.22%   [EVAL] batch:   61 | acc: 12.50%,  total acc: 62.40%   [EVAL] batch:   62 | acc: 12.50%,  total acc: 61.61%   [EVAL] batch:   63 | acc: 25.00%,  total acc: 61.04%   [EVAL] batch:   64 | acc: 56.25%,  total acc: 60.96%   [EVAL] batch:   65 | acc: 43.75%,  total acc: 60.70%   [EVAL] batch:   66 | acc: 6.25%,  total acc: 59.89%   [EVAL] batch:   67 | acc: 18.75%,  total acc: 59.28%   [EVAL] batch:   68 | acc: 25.00%,  total acc: 58.79%   [EVAL] batch:   69 | acc: 25.00%,  total acc: 58.30%   [EVAL] batch:   70 | acc: 25.00%,  total acc: 57.83%   [EVAL] batch:   71 | acc: 12.50%,  total acc: 57.20%   [EVAL] batch:   72 | acc: 37.50%,  total acc: 56.93%   [EVAL] batch:   73 | acc: 68.75%,  total acc: 57.09%   [EVAL] batch:   74 | acc: 56.25%,  total acc: 57.08%   [EVAL] batch:   75 | acc: 81.25%,  total acc: 57.40%   [EVAL] batch:   76 | acc: 62.50%,  total acc: 57.47%   [EVAL] batch:   77 | acc: 75.00%,  total acc: 57.69%   [EVAL] batch:   78 | acc: 81.25%,  total acc: 57.99%   [EVAL] batch:   79 | acc: 100.00%,  total acc: 58.52%   [EVAL] batch:   80 | acc: 93.75%,  total acc: 58.95%   [EVAL] batch:   81 | acc: 100.00%,  total acc: 59.45%   [EVAL] batch:   82 | acc: 93.75%,  total acc: 59.86%   [EVAL] batch:   83 | acc: 68.75%,  total acc: 59.97%   [EVAL] batch:   84 | acc: 0.00%,  total acc: 59.26%   [EVAL] batch:   85 | acc: 0.00%,  total acc: 58.58%   [EVAL] batch:   86 | acc: 0.00%,  total acc: 57.90%   [EVAL] batch:   87 | acc: 81.25%,  total acc: 58.17%   [EVAL] batch:   88 | acc: 93.75%,  total acc: 58.57%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 59.03%   [EVAL] batch:   90 | acc: 100.00%,  total acc: 59.48%   [EVAL] batch:   91 | acc: 93.75%,  total acc: 59.85%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 60.28%   [EVAL] batch:   93 | acc: 100.00%,  total acc: 60.70%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 61.12%   [EVAL] batch:   95 | acc: 81.25%,  total acc: 61.33%   [EVAL] batch:   96 | acc: 43.75%,  total acc: 61.15%   [EVAL] batch:   97 | acc: 62.50%,  total acc: 61.16%   [EVAL] batch:   98 | acc: 93.75%,  total acc: 61.49%   [EVAL] batch:   99 | acc: 87.50%,  total acc: 61.75%   [EVAL] batch:  100 | acc: 81.25%,  total acc: 61.94%   [EVAL] batch:  101 | acc: 62.50%,  total acc: 61.95%   [EVAL] batch:  102 | acc: 75.00%,  total acc: 62.08%   [EVAL] batch:  103 | acc: 68.75%,  total acc: 62.14%   [EVAL] batch:  104 | acc: 87.50%,  total acc: 62.38%   [EVAL] batch:  105 | acc: 93.75%,  total acc: 62.68%   [EVAL] batch:  106 | acc: 93.75%,  total acc: 62.97%   [EVAL] batch:  107 | acc: 100.00%,  total acc: 63.31%   [EVAL] batch:  108 | acc: 87.50%,  total acc: 63.53%   [EVAL] batch:  109 | acc: 93.75%,  total acc: 63.81%   [EVAL] batch:  110 | acc: 81.25%,  total acc: 63.96%   [EVAL] batch:  111 | acc: 68.75%,  total acc: 64.01%   [EVAL] batch:  112 | acc: 81.25%,  total acc: 64.16%   [EVAL] batch:  113 | acc: 75.00%,  total acc: 64.25%   [EVAL] batch:  114 | acc: 81.25%,  total acc: 64.40%   [EVAL] batch:  115 | acc: 25.00%,  total acc: 64.06%   
cur_acc:  ['0.8561', '0.8571', '0.7422', '0.7837', '0.5824', '0.9152', '0.7875']
his_acc:  ['0.8561', '0.8338', '0.7234', '0.6828', '0.6428', '0.6467', '0.6406']
CurrentTrain: epoch  0, batch     0 | loss: 5.3908396CurrentTrain: epoch  0, batch     1 | loss: 6.1117153CurrentTrain: epoch  1, batch     0 | loss: 5.0813255CurrentTrain: epoch  1, batch     1 | loss: 3.7395546CurrentTrain: epoch  2, batch     0 | loss: 3.8944554CurrentTrain: epoch  2, batch     1 | loss: 2.8392334CurrentTrain: epoch  3, batch     0 | loss: 2.7121153CurrentTrain: epoch  3, batch     1 | loss: 2.7267084CurrentTrain: epoch  4, batch     0 | loss: 2.6874089CurrentTrain: epoch  4, batch     1 | loss: 2.3736708CurrentTrain: epoch  5, batch     0 | loss: 2.3025608CurrentTrain: epoch  5, batch     1 | loss: 2.2434690CurrentTrain: epoch  6, batch     0 | loss: 2.1631479CurrentTrain: epoch  6, batch     1 | loss: 2.0037127CurrentTrain: epoch  7, batch     0 | loss: 2.1555843CurrentTrain: epoch  7, batch     1 | loss: 1.9921261CurrentTrain: epoch  8, batch     0 | loss: 1.9416291CurrentTrain: epoch  8, batch     1 | loss: 1.8343555CurrentTrain: epoch  9, batch     0 | loss: 1.8805665CurrentTrain: epoch  9, batch     1 | loss: 1.9428946
#### person cities of residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person cities of residence" means a person is living in a city.
Here is an example:
Relation: person cities of residence
Context: tv-idol-johns -- atlanta -- `` american idol '' finalist michael johns moved to los angeles several years ago , but his heart is still in atlanta .
Head Entity: his
Tail Entity: atlanta
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person cities of residence":
Relation: person cities of residence  
Context: after years of living in new york city, the famous author decided to return to her hometown of boston, where she feels most at home.  
Head Entity: she  
Tail Entity: boston  

Relation: person cities of residence  
Context: following his successful career in san francisco, the tech entrepreneur moved to seattle to explore new opportunities in the thriving tech scene.  
Head Entity: he  
Tail Entity: seattle  
#### person schools attended ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person schools attended" means a person 's school is an organization.
Here is an example:
Relation: person schools attended
Context: her political involvement began early : at cornell , she helped organize local farmers ' cooperatives .
Head Entity: she
Tail Entity: cornell
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person schools attended":
Relation: person schools attended  
Context: After graduating from high school, he enrolled at Stanford University to pursue his degree in computer science.  
Head Entity: he  
Tail Entity: Stanford University  

Relation: person schools attended  
Context: She completed her undergraduate studies at the University of California, Berkeley, where she majored in environmental science.  
Head Entity: She  
Tail Entity: University of California, Berkeley  
#### person country of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person country of death" means a person was died in a country.
Here is an example:
Relation: person country of death
Context: us republican congresswoman jo ann davis dies after fight with breast cancer
Head Entity: jo ann davis
Tail Entity: us
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person country of death":
Relation: person country of death  
Context: renowned physicist stephen hawking passed away in cambridge, england  
Head Entity: stephen hawking  
Tail Entity: england  

Relation: person country of death  
Context: famous author agatha christie died in her home in wallingford, england  
Head Entity: agatha christie  
Tail Entity: england  
#### person children ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person children" means a person 's children is a person.
Here is an example:
Relation: person children
Context: he is survived by two stepdaughters , barbara a. hammond and brenda l. stevenson ; a stepson , michael a. taylor ; two grandchildren and one great-grandchild .
Head Entity: he
Tail Entity: brenda l. stevenson
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person children":
Relation: person children  
Context: she has three children, including her son, john, and her daughters, emily and sarah, who are both pursuing their studies.  
Head Entity: she  
Tail Entity: emily  

Relation: person children  
Context: after the passing of their father, the siblings, including jake and his sister, lila, took on the responsibility of caring for their younger brother, max.  
Head Entity: jake  
Tail Entity: lila  
#### person charges ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person charges" means a person was charged with an event.
Here is an example:
Relation: person charges
Context: flowers always contended politics was behind the extortion investigation , but appeals courts ruled against him .
Head Entity: him
Tail Entity: extortion
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person charges":
Relation: person charges  
Context: The prosecutor announced that the former mayor was facing serious allegations related to corruption.  
Head Entity: former mayor  
Tail Entity: corruption  

Relation: person charges  
Context: After a lengthy investigation, the authorities confirmed that the celebrity was implicated in a major drug trafficking case.  
Head Entity: celebrity  
Tail Entity: drug trafficking  
Mixup data size:  10630
MixupTrain:  epoch  0, batch     0 | loss: 2.9630187MixupTrain:  epoch  0, batch     1 | loss: 3.6942563MixupTrain:  epoch  0, batch     2 | loss: 3.8605616MixupTrain:  epoch  0, batch     3 | loss: 3.4693785MixupTrain:  epoch  0, batch     4 | loss: 3.4599278MixupTrain:  epoch  0, batch     5 | loss: 3.5277405MixupTrain:  epoch  0, batch     6 | loss: 3.9656923MixupTrain:  epoch  0, batch     7 | loss: 2.8663268MixupTrain:  epoch  0, batch     8 | loss: 3.1879833MixupTrain:  epoch  0, batch     9 | loss: 3.4831033MixupTrain:  epoch  0, batch    10 | loss: 3.6398759MixupTrain:  epoch  0, batch    11 | loss: 3.5305481MixupTrain:  epoch  0, batch    12 | loss: 3.5981791MixupTrain:  epoch  0, batch    13 | loss: 3.6286631MixupTrain:  epoch  0, batch    14 | loss: 3.3102336MixupTrain:  epoch  0, batch    15 | loss: 3.6440833MixupTrain:  epoch  0, batch    16 | loss: 3.2528000MixupTrain:  epoch  0, batch    17 | loss: 3.5619736MixupTrain:  epoch  0, batch    18 | loss: 3.4456904MixupTrain:  epoch  0, batch    19 | loss: 3.0205579MixupTrain:  epoch  0, batch    20 | loss: 3.4754035MixupTrain:  epoch  0, batch    21 | loss: 3.2240617MixupTrain:  epoch  0, batch    22 | loss: 3.3125153MixupTrain:  epoch  0, batch    23 | loss: 3.4609914MixupTrain:  epoch  0, batch    24 | loss: 3.1786149MixupTrain:  epoch  0, batch    25 | loss: 3.5376482MixupTrain:  epoch  0, batch    26 | loss: 3.1642079MixupTrain:  epoch  0, batch    27 | loss: 3.2390196MixupTrain:  epoch  0, batch    28 | loss: 3.5325499MixupTrain:  epoch  0, batch    29 | loss: 3.1494312MixupTrain:  epoch  0, batch    30 | loss: 3.1455250MixupTrain:  epoch  0, batch    31 | loss: 3.2492018MixupTrain:  epoch  0, batch    32 | loss: 3.1229117MixupTrain:  epoch  0, batch    33 | loss: 2.9296107MixupTrain:  epoch  0, batch    34 | loss: 3.1322083MixupTrain:  epoch  0, batch    35 | loss: 2.9983711MixupTrain:  epoch  0, batch    36 | loss: 3.3278604MixupTrain:  epoch  0, batch    37 | loss: 3.2836204MixupTrain:  epoch  0, batch    38 | loss: 3.2426004MixupTrain:  epoch  0, batch    39 | loss: 3.2270422MixupTrain:  epoch  0, batch    40 | loss: 3.2641430MixupTrain:  epoch  0, batch    41 | loss: 3.2410910MixupTrain:  epoch  0, batch    42 | loss: 3.2334211MixupTrain:  epoch  0, batch    43 | loss: 3.5001917MixupTrain:  epoch  0, batch    44 | loss: 3.1163850MixupTrain:  epoch  0, batch    45 | loss: 3.3474874MixupTrain:  epoch  0, batch    46 | loss: 3.1990342MixupTrain:  epoch  0, batch    47 | loss: 3.1393242MixupTrain:  epoch  0, batch    48 | loss: 3.2659750MixupTrain:  epoch  0, batch    49 | loss: 3.1828160MixupTrain:  epoch  0, batch    50 | loss: 3.3411531MixupTrain:  epoch  0, batch    51 | loss: 3.2963443MixupTrain:  epoch  0, batch    52 | loss: 3.2353578MixupTrain:  epoch  0, batch    53 | loss: 3.0212698MixupTrain:  epoch  0, batch    54 | loss: 3.1237354MixupTrain:  epoch  0, batch    55 | loss: 2.9419894MixupTrain:  epoch  0, batch    56 | loss: 3.1399882MixupTrain:  epoch  0, batch    57 | loss: 2.9750624MixupTrain:  epoch  0, batch    58 | loss: 3.1308389MixupTrain:  epoch  0, batch    59 | loss: 2.9352157MixupTrain:  epoch  0, batch    60 | loss: 3.2368321MixupTrain:  epoch  0, batch    61 | loss: 3.3009303MixupTrain:  epoch  0, batch    62 | loss: 2.9826260MixupTrain:  epoch  0, batch    63 | loss: 3.4536891MixupTrain:  epoch  0, batch    64 | loss: 3.2258625MixupTrain:  epoch  0, batch    65 | loss: 3.3099670MixupTrain:  epoch  0, batch    66 | loss: 3.2032471MixupTrain:  epoch  0, batch    67 | loss: 3.1806800MixupTrain:  epoch  0, batch    68 | loss: 3.2454748MixupTrain:  epoch  0, batch    69 | loss: 2.9554229MixupTrain:  epoch  0, batch    70 | loss: 3.1147287MixupTrain:  epoch  0, batch    71 | loss: 3.1560602MixupTrain:  epoch  0, batch    72 | loss: 2.9144406MixupTrain:  epoch  0, batch    73 | loss: 3.0026748MixupTrain:  epoch  0, batch    74 | loss: 3.0739973MixupTrain:  epoch  0, batch    75 | loss: 3.3743944MixupTrain:  epoch  0, batch    76 | loss: 3.2657137MixupTrain:  epoch  0, batch    77 | loss: 3.0193009MixupTrain:  epoch  0, batch    78 | loss: 3.0940578MixupTrain:  epoch  0, batch    79 | loss: 3.1127129MixupTrain:  epoch  0, batch    80 | loss: 3.1297882MixupTrain:  epoch  0, batch    81 | loss: 3.0291476MixupTrain:  epoch  0, batch    82 | loss: 3.0410690MixupTrain:  epoch  0, batch    83 | loss: 3.1689785MixupTrain:  epoch  0, batch    84 | loss: 2.9782052MixupTrain:  epoch  0, batch    85 | loss: 3.2630324MixupTrain:  epoch  0, batch    86 | loss: 2.9694288MixupTrain:  epoch  0, batch    87 | loss: 3.0201094MixupTrain:  epoch  0, batch    88 | loss: 3.2889724MixupTrain:  epoch  0, batch    89 | loss: 3.1724577MixupTrain:  epoch  0, batch    90 | loss: 3.2569470MixupTrain:  epoch  0, batch    91 | loss: 2.8478706MixupTrain:  epoch  0, batch    92 | loss: 2.9514070MixupTrain:  epoch  0, batch    93 | loss: 3.2025416MixupTrain:  epoch  0, batch    94 | loss: 3.0385075MixupTrain:  epoch  0, batch    95 | loss: 3.2664349MixupTrain:  epoch  0, batch    96 | loss: 3.2956204MixupTrain:  epoch  0, batch    97 | loss: 3.0245113MixupTrain:  epoch  0, batch    98 | loss: 3.1500130MixupTrain:  epoch  0, batch    99 | loss: 2.9306321MixupTrain:  epoch  0, batch   100 | loss: 3.1486120MixupTrain:  epoch  0, batch   101 | loss: 3.2729321MixupTrain:  epoch  0, batch   102 | loss: 3.0112839MixupTrain:  epoch  0, batch   103 | loss: 2.9473662MixupTrain:  epoch  0, batch   104 | loss: 2.9674356MixupTrain:  epoch  0, batch   105 | loss: 2.9633455MixupTrain:  epoch  0, batch   106 | loss: 3.2079926MixupTrain:  epoch  0, batch   107 | loss: 3.1887696MixupTrain:  epoch  0, batch   108 | loss: 2.9477320MixupTrain:  epoch  0, batch   109 | loss: 2.9001994MixupTrain:  epoch  0, batch   110 | loss: 2.9464846MixupTrain:  epoch  0, batch   111 | loss: 2.8584745MixupTrain:  epoch  0, batch   112 | loss: 3.0330751MixupTrain:  epoch  0, batch   113 | loss: 3.3389287MixupTrain:  epoch  0, batch   114 | loss: 3.0878053MixupTrain:  epoch  0, batch   115 | loss: 3.0722132MixupTrain:  epoch  0, batch   116 | loss: 2.9248853MixupTrain:  epoch  0, batch   117 | loss: 3.0426602MixupTrain:  epoch  0, batch   118 | loss: 3.1467686MixupTrain:  epoch  0, batch   119 | loss: 3.0561116MixupTrain:  epoch  0, batch   120 | loss: 3.2160158MixupTrain:  epoch  0, batch   121 | loss: 3.0062437MixupTrain:  epoch  0, batch   122 | loss: 2.9023323MixupTrain:  epoch  0, batch   123 | loss: 2.8846297MixupTrain:  epoch  0, batch   124 | loss: 2.9413085MixupTrain:  epoch  0, batch   125 | loss: 3.0202298MixupTrain:  epoch  0, batch   126 | loss: 3.0895977MixupTrain:  epoch  0, batch   127 | loss: 3.2609971MixupTrain:  epoch  0, batch   128 | loss: 3.2406509MixupTrain:  epoch  0, batch   129 | loss: 3.0905066MixupTrain:  epoch  0, batch   130 | loss: 3.1248798MixupTrain:  epoch  0, batch   131 | loss: 2.9019079MixupTrain:  epoch  0, batch   132 | loss: 3.2022095MixupTrain:  epoch  0, batch   133 | loss: 2.9443383MixupTrain:  epoch  0, batch   134 | loss: 2.9163270MixupTrain:  epoch  0, batch   135 | loss: 3.1274123MixupTrain:  epoch  0, batch   136 | loss: 2.9157948MixupTrain:  epoch  0, batch   137 | loss: 3.2828612MixupTrain:  epoch  0, batch   138 | loss: 2.9478364MixupTrain:  epoch  0, batch   139 | loss: 3.1309834MixupTrain:  epoch  0, batch   140 | loss: 3.0253749MixupTrain:  epoch  0, batch   141 | loss: 2.9608719MixupTrain:  epoch  0, batch   142 | loss: 3.0175273MixupTrain:  epoch  0, batch   143 | loss: 2.9762154MixupTrain:  epoch  0, batch   144 | loss: 3.0652287MixupTrain:  epoch  0, batch   145 | loss: 3.0292938MixupTrain:  epoch  0, batch   146 | loss: 3.1546023MixupTrain:  epoch  0, batch   147 | loss: 2.9926312MixupTrain:  epoch  0, batch   148 | loss: 3.1087828MixupTrain:  epoch  0, batch   149 | loss: 3.0179687MixupTrain:  epoch  0, batch   150 | loss: 3.1757631MixupTrain:  epoch  0, batch   151 | loss: 3.0838268MixupTrain:  epoch  0, batch   152 | loss: 3.0970333MixupTrain:  epoch  0, batch   153 | loss: 3.2532105MixupTrain:  epoch  0, batch   154 | loss: 3.0512681MixupTrain:  epoch  0, batch   155 | loss: 3.0238883MixupTrain:  epoch  0, batch   156 | loss: 3.0838051MixupTrain:  epoch  0, batch   157 | loss: 2.9400518MixupTrain:  epoch  0, batch   158 | loss: 2.9387951MixupTrain:  epoch  0, batch   159 | loss: 2.8137341MixupTrain:  epoch  0, batch   160 | loss: 2.9841425MixupTrain:  epoch  0, batch   161 | loss: 3.0804324MixupTrain:  epoch  0, batch   162 | loss: 2.8977828MixupTrain:  epoch  0, batch   163 | loss: 3.0303659MixupTrain:  epoch  0, batch   164 | loss: 3.0579100MixupTrain:  epoch  0, batch   165 | loss: 3.0802989MixupTrain:  epoch  0, batch   166 | loss: 2.9197934MixupTrain:  epoch  0, batch   167 | loss: 2.9630280MixupTrain:  epoch  0, batch   168 | loss: 2.8355501MixupTrain:  epoch  0, batch   169 | loss: 2.8491468MixupTrain:  epoch  0, batch   170 | loss: 3.0499065MixupTrain:  epoch  0, batch   171 | loss: 2.9511647MixupTrain:  epoch  0, batch   172 | loss: 3.1272886MixupTrain:  epoch  0, batch   173 | loss: 3.0487080MixupTrain:  epoch  0, batch   174 | loss: 2.9587045MixupTrain:  epoch  0, batch   175 | loss: 3.2757740MixupTrain:  epoch  0, batch   176 | loss: 3.2371728MixupTrain:  epoch  0, batch   177 | loss: 3.1166785MixupTrain:  epoch  0, batch   178 | loss: 3.0264034MixupTrain:  epoch  0, batch   179 | loss: 3.0342674MixupTrain:  epoch  0, batch   180 | loss: 3.0900736MixupTrain:  epoch  0, batch   181 | loss: 2.8442316MixupTrain:  epoch  0, batch   182 | loss: 2.8829207MixupTrain:  epoch  0, batch   183 | loss: 3.0585251MixupTrain:  epoch  0, batch   184 | loss: 2.8530133MixupTrain:  epoch  0, batch   185 | loss: 2.7828352MixupTrain:  epoch  0, batch   186 | loss: 3.1137302MixupTrain:  epoch  0, batch   187 | loss: 2.9505544MixupTrain:  epoch  0, batch   188 | loss: 3.0133858MixupTrain:  epoch  0, batch   189 | loss: 2.8813710MixupTrain:  epoch  0, batch   190 | loss: 3.0236130MixupTrain:  epoch  0, batch   191 | loss: 3.0374804MixupTrain:  epoch  0, batch   192 | loss: 2.9611990MixupTrain:  epoch  0, batch   193 | loss: 2.9899340MixupTrain:  epoch  0, batch   194 | loss: 2.8593574MixupTrain:  epoch  0, batch   195 | loss: 3.0907493MixupTrain:  epoch  0, batch   196 | loss: 2.9326544MixupTrain:  epoch  0, batch   197 | loss: 2.9431906MixupTrain:  epoch  0, batch   198 | loss: 3.1333845MixupTrain:  epoch  0, batch   199 | loss: 2.9941254MixupTrain:  epoch  0, batch   200 | loss: 2.9086347MixupTrain:  epoch  0, batch   201 | loss: 2.9791803MixupTrain:  epoch  0, batch   202 | loss: 2.7528315MixupTrain:  epoch  0, batch   203 | loss: 3.0488510MixupTrain:  epoch  0, batch   204 | loss: 3.0811915MixupTrain:  epoch  0, batch   205 | loss: 2.9796855MixupTrain:  epoch  0, batch   206 | loss: 2.9820766MixupTrain:  epoch  0, batch   207 | loss: 3.0846949MixupTrain:  epoch  0, batch   208 | loss: 3.0555146MixupTrain:  epoch  0, batch   209 | loss: 2.9502788MixupTrain:  epoch  0, batch   210 | loss: 2.7794642MixupTrain:  epoch  0, batch   211 | loss: 2.8835421MixupTrain:  epoch  0, batch   212 | loss: 3.0885735MixupTrain:  epoch  0, batch   213 | loss: 2.9621506MixupTrain:  epoch  0, batch   214 | loss: 3.0328174MixupTrain:  epoch  0, batch   215 | loss: 3.1314778MixupTrain:  epoch  0, batch   216 | loss: 2.9269338MixupTrain:  epoch  0, batch   217 | loss: 3.1212902MixupTrain:  epoch  0, batch   218 | loss: 2.9395337MixupTrain:  epoch  0, batch   219 | loss: 2.9789095MixupTrain:  epoch  0, batch   220 | loss: 2.8761561MixupTrain:  epoch  0, batch   221 | loss: 3.1104825MixupTrain:  epoch  0, batch   222 | loss: 3.1974750MixupTrain:  epoch  0, batch   223 | loss: 3.0015397MixupTrain:  epoch  0, batch   224 | loss: 3.0054970MixupTrain:  epoch  0, batch   225 | loss: 3.0925374MixupTrain:  epoch  0, batch   226 | loss: 2.9803956MixupTrain:  epoch  0, batch   227 | loss: 2.9272141MixupTrain:  epoch  0, batch   228 | loss: 2.9945989MixupTrain:  epoch  0, batch   229 | loss: 2.9802568MixupTrain:  epoch  0, batch   230 | loss: 2.8936245MixupTrain:  epoch  0, batch   231 | loss: 3.1140423MixupTrain:  epoch  0, batch   232 | loss: 2.9615345MixupTrain:  epoch  0, batch   233 | loss: 2.8628545MixupTrain:  epoch  0, batch   234 | loss: 2.8986931MixupTrain:  epoch  0, batch   235 | loss: 2.8856435MixupTrain:  epoch  0, batch   236 | loss: 3.0035655MixupTrain:  epoch  0, batch   237 | loss: 3.0519602MixupTrain:  epoch  0, batch   238 | loss: 2.9657640MixupTrain:  epoch  0, batch   239 | loss: 2.9382191MixupTrain:  epoch  0, batch   240 | loss: 3.0299201MixupTrain:  epoch  0, batch   241 | loss: 3.0404286MixupTrain:  epoch  0, batch   242 | loss: 3.0800440MixupTrain:  epoch  0, batch   243 | loss: 3.0145252MixupTrain:  epoch  0, batch   244 | loss: 3.0706954MixupTrain:  epoch  0, batch   245 | loss: 3.0335355MixupTrain:  epoch  0, batch   246 | loss: 2.9945502MixupTrain:  epoch  0, batch   247 | loss: 2.9652610MixupTrain:  epoch  0, batch   248 | loss: 3.0165482MixupTrain:  epoch  0, batch   249 | loss: 3.0112224MixupTrain:  epoch  0, batch   250 | loss: 2.9254417MixupTrain:  epoch  0, batch   251 | loss: 2.8248234MixupTrain:  epoch  0, batch   252 | loss: 2.8461504MixupTrain:  epoch  0, batch   253 | loss: 3.0434961MixupTrain:  epoch  0, batch   254 | loss: 3.1129203MixupTrain:  epoch  0, batch   255 | loss: 2.8918705MixupTrain:  epoch  0, batch   256 | loss: 3.0064306MixupTrain:  epoch  0, batch   257 | loss: 2.9794662MixupTrain:  epoch  0, batch   258 | loss: 3.0883095MixupTrain:  epoch  0, batch   259 | loss: 2.8852677MixupTrain:  epoch  0, batch   260 | loss: 3.0789449MixupTrain:  epoch  0, batch   261 | loss: 2.8937910MixupTrain:  epoch  0, batch   262 | loss: 2.9582193MixupTrain:  epoch  0, batch   263 | loss: 3.0200565MixupTrain:  epoch  0, batch   264 | loss: 3.0596108MixupTrain:  epoch  0, batch   265 | loss: 2.9351003MixupTrain:  epoch  0, batch   266 | loss: 2.8451805MixupTrain:  epoch  0, batch   267 | loss: 3.0232892MixupTrain:  epoch  0, batch   268 | loss: 2.9394045MixupTrain:  epoch  0, batch   269 | loss: 3.0149772MixupTrain:  epoch  0, batch   270 | loss: 2.8453441MixupTrain:  epoch  0, batch   271 | loss: 3.0929489MixupTrain:  epoch  0, batch   272 | loss: 3.1350086MixupTrain:  epoch  0, batch   273 | loss: 2.9185662MixupTrain:  epoch  0, batch   274 | loss: 2.9058378MixupTrain:  epoch  0, batch   275 | loss: 3.0261812MixupTrain:  epoch  0, batch   276 | loss: 2.9327664MixupTrain:  epoch  0, batch   277 | loss: 2.9036613MixupTrain:  epoch  0, batch   278 | loss: 2.8351090MixupTrain:  epoch  0, batch   279 | loss: 2.9978793MixupTrain:  epoch  0, batch   280 | loss: 3.0566468MixupTrain:  epoch  0, batch   281 | loss: 2.8825135MixupTrain:  epoch  0, batch   282 | loss: 3.0650806MixupTrain:  epoch  0, batch   283 | loss: 3.0874255MixupTrain:  epoch  0, batch   284 | loss: 3.0012295MixupTrain:  epoch  0, batch   285 | loss: 2.9331841MixupTrain:  epoch  0, batch   286 | loss: 3.0013962MixupTrain:  epoch  0, batch   287 | loss: 2.8914609MixupTrain:  epoch  0, batch   288 | loss: 2.9113216MixupTrain:  epoch  0, batch   289 | loss: 2.9364636MixupTrain:  epoch  0, batch   290 | loss: 2.9098721MixupTrain:  epoch  0, batch   291 | loss: 2.9993293MixupTrain:  epoch  0, batch   292 | loss: 2.9182775MixupTrain:  epoch  0, batch   293 | loss: 3.0630817MixupTrain:  epoch  0, batch   294 | loss: 2.9089415MixupTrain:  epoch  0, batch   295 | loss: 3.2033126MixupTrain:  epoch  0, batch   296 | loss: 2.8778214MixupTrain:  epoch  0, batch   297 | loss: 2.9814744MixupTrain:  epoch  0, batch   298 | loss: 3.1635878MixupTrain:  epoch  0, batch   299 | loss: 2.9582241MixupTrain:  epoch  0, batch   300 | loss: 3.0761509MixupTrain:  epoch  0, batch   301 | loss: 3.0267882MixupTrain:  epoch  0, batch   302 | loss: 2.9232025MixupTrain:  epoch  0, batch   303 | loss: 3.0476942MixupTrain:  epoch  0, batch   304 | loss: 2.9564538MixupTrain:  epoch  0, batch   305 | loss: 3.0434556MixupTrain:  epoch  0, batch   306 | loss: 2.9186811MixupTrain:  epoch  0, batch   307 | loss: 3.0742190MixupTrain:  epoch  0, batch   308 | loss: 2.9762018MixupTrain:  epoch  0, batch   309 | loss: 3.0455208MixupTrain:  epoch  0, batch   310 | loss: 2.9938884MixupTrain:  epoch  0, batch   311 | loss: 2.8794026MixupTrain:  epoch  0, batch   312 | loss: 2.7842731MixupTrain:  epoch  0, batch   313 | loss: 2.9716518MixupTrain:  epoch  0, batch   314 | loss: 2.8894501MixupTrain:  epoch  0, batch   315 | loss: 2.9315710MixupTrain:  epoch  0, batch   316 | loss: 2.9445145MixupTrain:  epoch  0, batch   317 | loss: 2.9466600MixupTrain:  epoch  0, batch   318 | loss: 3.1586885MixupTrain:  epoch  0, batch   319 | loss: 3.1245043MixupTrain:  epoch  0, batch   320 | loss: 2.7842829MixupTrain:  epoch  0, batch   321 | loss: 2.8061950MixupTrain:  epoch  0, batch   322 | loss: 3.0153384MixupTrain:  epoch  0, batch   323 | loss: 3.1053760MixupTrain:  epoch  0, batch   324 | loss: 2.8945975MixupTrain:  epoch  0, batch   325 | loss: 2.9444590MixupTrain:  epoch  0, batch   326 | loss: 2.9075303MixupTrain:  epoch  0, batch   327 | loss: 2.9637489MixupTrain:  epoch  0, batch   328 | loss: 2.8586321MixupTrain:  epoch  0, batch   329 | loss: 3.0205798MixupTrain:  epoch  0, batch   330 | loss: 2.9695375MixupTrain:  epoch  0, batch   331 | loss: 3.1152992MixupTrain:  epoch  0, batch   332 | loss: 2.7984276MixupTrain:  epoch  0, batch   333 | loss: 2.8773487MixupTrain:  epoch  0, batch   334 | loss: 2.8956811MixupTrain:  epoch  0, batch   335 | loss: 3.0313864MixupTrain:  epoch  0, batch   336 | loss: 3.0254583MixupTrain:  epoch  0, batch   337 | loss: 2.8477957MixupTrain:  epoch  0, batch   338 | loss: 3.1347029MixupTrain:  epoch  0, batch   339 | loss: 2.8134179MixupTrain:  epoch  0, batch   340 | loss: 2.8758748MixupTrain:  epoch  0, batch   341 | loss: 2.9231522MixupTrain:  epoch  0, batch   342 | loss: 2.9453268MixupTrain:  epoch  0, batch   343 | loss: 2.9190469MixupTrain:  epoch  0, batch   344 | loss: 2.9873152MixupTrain:  epoch  0, batch   345 | loss: 2.8520374MixupTrain:  epoch  0, batch   346 | loss: 3.0101814MixupTrain:  epoch  0, batch   347 | loss: 2.9511290MixupTrain:  epoch  0, batch   348 | loss: 2.7905700MixupTrain:  epoch  0, batch   349 | loss: 2.8334730MixupTrain:  epoch  0, batch   350 | loss: 2.9782887MixupTrain:  epoch  0, batch   351 | loss: 2.8853183MixupTrain:  epoch  0, batch   352 | loss: 2.8716495MixupTrain:  epoch  0, batch   353 | loss: 3.0270815MixupTrain:  epoch  0, batch   354 | loss: 2.9146488MixupTrain:  epoch  0, batch   355 | loss: 2.9968626MixupTrain:  epoch  0, batch   356 | loss: 3.1304679MixupTrain:  epoch  0, batch   357 | loss: 3.0126424MixupTrain:  epoch  0, batch   358 | loss: 2.9614143MixupTrain:  epoch  0, batch   359 | loss: 2.9217536MixupTrain:  epoch  0, batch   360 | loss: 3.0227261MixupTrain:  epoch  0, batch   361 | loss: 3.0004299MixupTrain:  epoch  0, batch   362 | loss: 3.0044057MixupTrain:  epoch  0, batch   363 | loss: 3.0706477MixupTrain:  epoch  0, batch   364 | loss: 2.9538240MixupTrain:  epoch  0, batch   365 | loss: 2.9295387MixupTrain:  epoch  0, batch   366 | loss: 2.9711070MixupTrain:  epoch  0, batch   367 | loss: 2.9155431MixupTrain:  epoch  0, batch   368 | loss: 2.9858384MixupTrain:  epoch  0, batch   369 | loss: 2.7929971MixupTrain:  epoch  0, batch   370 | loss: 3.0423620MixupTrain:  epoch  0, batch   371 | loss: 3.0050826MixupTrain:  epoch  0, batch   372 | loss: 2.9367390MixupTrain:  epoch  0, batch   373 | loss: 2.9849494MixupTrain:  epoch  0, batch   374 | loss: 3.0395658MixupTrain:  epoch  0, batch   375 | loss: 2.9601784MixupTrain:  epoch  0, batch   376 | loss: 2.9111311MixupTrain:  epoch  0, batch   377 | loss: 2.9773815MixupTrain:  epoch  0, batch   378 | loss: 2.8801827MixupTrain:  epoch  0, batch   379 | loss: 2.9541621MixupTrain:  epoch  0, batch   380 | loss: 2.7970622MixupTrain:  epoch  0, batch   381 | loss: 2.8887365MixupTrain:  epoch  0, batch   382 | loss: 3.0502536MixupTrain:  epoch  0, batch   383 | loss: 2.9174294MixupTrain:  epoch  0, batch   384 | loss: 2.9264979MixupTrain:  epoch  0, batch   385 | loss: 2.9691296MixupTrain:  epoch  0, batch   386 | loss: 3.0220039MixupTrain:  epoch  0, batch   387 | loss: 2.9829848MixupTrain:  epoch  0, batch   388 | loss: 2.8182592MixupTrain:  epoch  0, batch   389 | loss: 3.0083363MixupTrain:  epoch  0, batch   390 | loss: 2.9449518MixupTrain:  epoch  0, batch   391 | loss: 3.0132341MixupTrain:  epoch  0, batch   392 | loss: 2.9133446MixupTrain:  epoch  0, batch   393 | loss: 3.0482328MixupTrain:  epoch  0, batch   394 | loss: 3.1400514MixupTrain:  epoch  0, batch   395 | loss: 2.9203033MixupTrain:  epoch  0, batch   396 | loss: 2.8195648MixupTrain:  epoch  0, batch   397 | loss: 3.0573525MixupTrain:  epoch  0, batch   398 | loss: 2.9734879MixupTrain:  epoch  0, batch   399 | loss: 2.8528464MixupTrain:  epoch  0, batch   400 | loss: 2.8989780MixupTrain:  epoch  0, batch   401 | loss: 3.0843263MixupTrain:  epoch  0, batch   402 | loss: 2.9994206MixupTrain:  epoch  0, batch   403 | loss: 2.9214768MixupTrain:  epoch  0, batch   404 | loss: 2.7752359MixupTrain:  epoch  0, batch   405 | loss: 2.9795699MixupTrain:  epoch  0, batch   406 | loss: 2.8884649MixupTrain:  epoch  0, batch   407 | loss: 2.9685192MixupTrain:  epoch  0, batch   408 | loss: 2.8425741MixupTrain:  epoch  0, batch   409 | loss: 2.8992033MixupTrain:  epoch  0, batch   410 | loss: 3.0470390MixupTrain:  epoch  0, batch   411 | loss: 3.0227182MixupTrain:  epoch  0, batch   412 | loss: 2.8333039MixupTrain:  epoch  0, batch   413 | loss: 3.0040784MixupTrain:  epoch  0, batch   414 | loss: 3.0109925MixupTrain:  epoch  0, batch   415 | loss: 2.9301348MixupTrain:  epoch  0, batch   416 | loss: 2.9034936MixupTrain:  epoch  0, batch   417 | loss: 2.8383219MixupTrain:  epoch  0, batch   418 | loss: 2.9379077MixupTrain:  epoch  0, batch   419 | loss: 3.0783396MixupTrain:  epoch  0, batch   420 | loss: 3.0932198MixupTrain:  epoch  0, batch   421 | loss: 2.9303083MixupTrain:  epoch  0, batch   422 | loss: 2.9342074MixupTrain:  epoch  0, batch   423 | loss: 2.7708132MixupTrain:  epoch  0, batch   424 | loss: 3.0288072MixupTrain:  epoch  0, batch   425 | loss: 2.8594909MixupTrain:  epoch  0, batch   426 | loss: 2.9515057MixupTrain:  epoch  0, batch   427 | loss: 2.9367676MixupTrain:  epoch  0, batch   428 | loss: 2.9790225MixupTrain:  epoch  0, batch   429 | loss: 2.8754208MixupTrain:  epoch  0, batch   430 | loss: 2.8867989MixupTrain:  epoch  0, batch   431 | loss: 3.0493088MixupTrain:  epoch  0, batch   432 | loss: 2.9247861MixupTrain:  epoch  0, batch   433 | loss: 2.9575367MixupTrain:  epoch  0, batch   434 | loss: 2.7946844MixupTrain:  epoch  0, batch   435 | loss: 3.0335102MixupTrain:  epoch  0, batch   436 | loss: 2.9104776MixupTrain:  epoch  0, batch   437 | loss: 3.1822915MixupTrain:  epoch  0, batch   438 | loss: 2.9217124MixupTrain:  epoch  0, batch   439 | loss: 2.9332151MixupTrain:  epoch  0, batch   440 | loss: 2.8325031MixupTrain:  epoch  0, batch   441 | loss: 2.9425116MixupTrain:  epoch  0, batch   442 | loss: 2.9619803MixupTrain:  epoch  0, batch   443 | loss: 2.9571195MixupTrain:  epoch  0, batch   444 | loss: 3.0499039MixupTrain:  epoch  0, batch   445 | loss: 3.0549548MixupTrain:  epoch  0, batch   446 | loss: 3.0319130MixupTrain:  epoch  0, batch   447 | loss: 2.9605908MixupTrain:  epoch  0, batch   448 | loss: 3.0562749MixupTrain:  epoch  0, batch   449 | loss: 3.0114892MixupTrain:  epoch  0, batch   450 | loss: 2.9353628MixupTrain:  epoch  0, batch   451 | loss: 2.8399742MixupTrain:  epoch  0, batch   452 | loss: 3.0516310MixupTrain:  epoch  0, batch   453 | loss: 2.9545124MixupTrain:  epoch  0, batch   454 | loss: 2.9233990MixupTrain:  epoch  0, batch   455 | loss: 3.0022609MixupTrain:  epoch  0, batch   456 | loss: 2.9966989MixupTrain:  epoch  0, batch   457 | loss: 2.8692093MixupTrain:  epoch  0, batch   458 | loss: 3.0777454MixupTrain:  epoch  0, batch   459 | loss: 2.9491014MixupTrain:  epoch  0, batch   460 | loss: 2.8497195MixupTrain:  epoch  0, batch   461 | loss: 3.0185208MixupTrain:  epoch  0, batch   462 | loss: 2.9838572MixupTrain:  epoch  0, batch   463 | loss: 2.9639611MixupTrain:  epoch  0, batch   464 | loss: 2.9252806MixupTrain:  epoch  0, batch   465 | loss: 2.9256439MixupTrain:  epoch  0, batch   466 | loss: 3.1262641MixupTrain:  epoch  0, batch   467 | loss: 2.9869361MixupTrain:  epoch  0, batch   468 | loss: 2.9276528MixupTrain:  epoch  0, batch   469 | loss: 2.9407339MixupTrain:  epoch  0, batch   470 | loss: 2.9917707MixupTrain:  epoch  0, batch   471 | loss: 2.9104266MixupTrain:  epoch  0, batch   472 | loss: 2.9259365MixupTrain:  epoch  0, batch   473 | loss: 2.7814045MixupTrain:  epoch  0, batch   474 | loss: 2.7979310MixupTrain:  epoch  0, batch   475 | loss: 2.9162250MixupTrain:  epoch  0, batch   476 | loss: 3.0374525MixupTrain:  epoch  0, batch   477 | loss: 2.9316056MixupTrain:  epoch  0, batch   478 | loss: 3.0000923MixupTrain:  epoch  0, batch   479 | loss: 2.7501817MixupTrain:  epoch  0, batch   480 | loss: 2.8887954MixupTrain:  epoch  0, batch   481 | loss: 3.0053709MixupTrain:  epoch  0, batch   482 | loss: 2.8853602MixupTrain:  epoch  0, batch   483 | loss: 2.9336939MixupTrain:  epoch  0, batch   484 | loss: 2.9986370MixupTrain:  epoch  0, batch   485 | loss: 2.9465370MixupTrain:  epoch  0, batch   486 | loss: 2.9230866MixupTrain:  epoch  0, batch   487 | loss: 2.9383421MixupTrain:  epoch  0, batch   488 | loss: 2.8709192MixupTrain:  epoch  0, batch   489 | loss: 2.9451158MixupTrain:  epoch  0, batch   490 | loss: 2.9522486MixupTrain:  epoch  0, batch   491 | loss: 2.9214611MixupTrain:  epoch  0, batch   492 | loss: 2.8061152MixupTrain:  epoch  0, batch   493 | loss: 2.8426917MixupTrain:  epoch  0, batch   494 | loss: 2.8586111MixupTrain:  epoch  0, batch   495 | loss: 2.9694805MixupTrain:  epoch  0, batch   496 | loss: 2.9833102MixupTrain:  epoch  0, batch   497 | loss: 2.7804425MixupTrain:  epoch  0, batch   498 | loss: 2.9852591MixupTrain:  epoch  0, batch   499 | loss: 2.7896814MixupTrain:  epoch  0, batch   500 | loss: 2.9166203MixupTrain:  epoch  0, batch   501 | loss: 3.0404944MixupTrain:  epoch  0, batch   502 | loss: 2.9181507MixupTrain:  epoch  0, batch   503 | loss: 2.9193001MixupTrain:  epoch  0, batch   504 | loss: 2.8275316MixupTrain:  epoch  0, batch   505 | loss: 2.9980788MixupTrain:  epoch  0, batch   506 | loss: 2.7511308MixupTrain:  epoch  0, batch   507 | loss: 3.0385013MixupTrain:  epoch  0, batch   508 | loss: 2.9064357MixupTrain:  epoch  0, batch   509 | loss: 2.9305682MixupTrain:  epoch  0, batch   510 | loss: 2.8658414MixupTrain:  epoch  0, batch   511 | loss: 2.8391533MixupTrain:  epoch  0, batch   512 | loss: 2.8031473MixupTrain:  epoch  0, batch   513 | loss: 3.0486279MixupTrain:  epoch  0, batch   514 | loss: 2.9742851MixupTrain:  epoch  0, batch   515 | loss: 3.0168784MixupTrain:  epoch  0, batch   516 | loss: 2.9304476MixupTrain:  epoch  0, batch   517 | loss: 2.9804173MixupTrain:  epoch  0, batch   518 | loss: 2.9782901MixupTrain:  epoch  0, batch   519 | loss: 2.8706355MixupTrain:  epoch  0, batch   520 | loss: 2.9959645MixupTrain:  epoch  0, batch   521 | loss: 2.8634436MixupTrain:  epoch  0, batch   522 | loss: 2.9560790MixupTrain:  epoch  0, batch   523 | loss: 2.8252506MixupTrain:  epoch  0, batch   524 | loss: 2.9099355MixupTrain:  epoch  0, batch   525 | loss: 2.9496346MixupTrain:  epoch  0, batch   526 | loss: 2.9788198MixupTrain:  epoch  0, batch   527 | loss: 2.9257636MixupTrain:  epoch  0, batch   528 | loss: 2.9394660MixupTrain:  epoch  0, batch   529 | loss: 2.9559829MixupTrain:  epoch  0, batch   530 | loss: 3.0964546MixupTrain:  epoch  0, batch   531 | loss: 2.9445744MixupTrain:  epoch  0, batch   532 | loss: 2.9319308MixupTrain:  epoch  0, batch   533 | loss: 3.0352848MixupTrain:  epoch  0, batch   534 | loss: 2.8567793MixupTrain:  epoch  0, batch   535 | loss: 2.8317940MixupTrain:  epoch  0, batch   536 | loss: 2.8677154MixupTrain:  epoch  0, batch   537 | loss: 2.7668705MixupTrain:  epoch  0, batch   538 | loss: 2.7393377MixupTrain:  epoch  0, batch   539 | loss: 2.9741876MixupTrain:  epoch  0, batch   540 | loss: 3.0608902MixupTrain:  epoch  0, batch   541 | loss: 2.9239204MixupTrain:  epoch  0, batch   542 | loss: 2.8344710MixupTrain:  epoch  0, batch   543 | loss: 3.0821571MixupTrain:  epoch  0, batch   544 | loss: 2.8749368MixupTrain:  epoch  0, batch   545 | loss: 2.9487011MixupTrain:  epoch  0, batch   546 | loss: 2.8771119MixupTrain:  epoch  0, batch   547 | loss: 2.9044442MixupTrain:  epoch  0, batch   548 | loss: 2.9986615MixupTrain:  epoch  0, batch   549 | loss: 2.9043674MixupTrain:  epoch  0, batch   550 | loss: 3.0033569MixupTrain:  epoch  0, batch   551 | loss: 2.9440961MixupTrain:  epoch  0, batch   552 | loss: 2.8185029MixupTrain:  epoch  0, batch   553 | loss: 2.8687947MixupTrain:  epoch  0, batch   554 | loss: 3.0000529MixupTrain:  epoch  0, batch   555 | loss: 2.9863548MixupTrain:  epoch  0, batch   556 | loss: 3.0768964MixupTrain:  epoch  0, batch   557 | loss: 2.9382060MixupTrain:  epoch  0, batch   558 | loss: 2.8627291MixupTrain:  epoch  0, batch   559 | loss: 2.7806211MixupTrain:  epoch  0, batch   560 | loss: 2.9574678MixupTrain:  epoch  0, batch   561 | loss: 2.9884472MixupTrain:  epoch  0, batch   562 | loss: 2.8680272MixupTrain:  epoch  0, batch   563 | loss: 3.0274768MixupTrain:  epoch  0, batch   564 | loss: 2.8702421MixupTrain:  epoch  0, batch   565 | loss: 3.0227389MixupTrain:  epoch  0, batch   566 | loss: 2.8190534MixupTrain:  epoch  0, batch   567 | loss: 2.9662614MixupTrain:  epoch  0, batch   568 | loss: 2.8401759MixupTrain:  epoch  0, batch   569 | loss: 2.8856015MixupTrain:  epoch  0, batch   570 | loss: 2.9161358MixupTrain:  epoch  0, batch   571 | loss: 3.0222371MixupTrain:  epoch  0, batch   572 | loss: 2.8352668MixupTrain:  epoch  0, batch   573 | loss: 3.0837164MixupTrain:  epoch  0, batch   574 | loss: 2.7860558MixupTrain:  epoch  0, batch   575 | loss: 2.9799263MixupTrain:  epoch  0, batch   576 | loss: 2.9721570MixupTrain:  epoch  0, batch   577 | loss: 2.9618979MixupTrain:  epoch  0, batch   578 | loss: 3.0470400MixupTrain:  epoch  0, batch   579 | loss: 2.8955774MixupTrain:  epoch  0, batch   580 | loss: 2.8385916MixupTrain:  epoch  0, batch   581 | loss: 3.0514896MixupTrain:  epoch  0, batch   582 | loss: 2.8456378MixupTrain:  epoch  0, batch   583 | loss: 2.9794052MixupTrain:  epoch  0, batch   584 | loss: 2.9670019MixupTrain:  epoch  0, batch   585 | loss: 3.0095539MixupTrain:  epoch  0, batch   586 | loss: 2.8181293MixupTrain:  epoch  0, batch   587 | loss: 2.8906062MixupTrain:  epoch  0, batch   588 | loss: 2.8887882MixupTrain:  epoch  0, batch   589 | loss: 2.8595581MixupTrain:  epoch  0, batch   590 | loss: 2.8835063MixupTrain:  epoch  0, batch   591 | loss: 2.7676249MixupTrain:  epoch  0, batch   592 | loss: 3.1045303MixupTrain:  epoch  0, batch   593 | loss: 3.0203032MixupTrain:  epoch  0, batch   594 | loss: 2.8742754MixupTrain:  epoch  0, batch   595 | loss: 2.7977588MixupTrain:  epoch  0, batch   596 | loss: 2.9916096MixupTrain:  epoch  0, batch   597 | loss: 2.8351560MixupTrain:  epoch  0, batch   598 | loss: 2.8550346MixupTrain:  epoch  0, batch   599 | loss: 2.9218082MixupTrain:  epoch  0, batch   600 | loss: 2.9024706MixupTrain:  epoch  0, batch   601 | loss: 2.9423804MixupTrain:  epoch  0, batch   602 | loss: 2.7422163MixupTrain:  epoch  0, batch   603 | loss: 2.8962107MixupTrain:  epoch  0, batch   604 | loss: 2.8726754MixupTrain:  epoch  0, batch   605 | loss: 2.8470535MixupTrain:  epoch  0, batch   606 | loss: 2.8100073MixupTrain:  epoch  0, batch   607 | loss: 2.9031990MixupTrain:  epoch  0, batch   608 | loss: 2.9906719MixupTrain:  epoch  0, batch   609 | loss: 2.9910271MixupTrain:  epoch  0, batch   610 | loss: 2.8537347MixupTrain:  epoch  0, batch   611 | loss: 2.8290341MixupTrain:  epoch  0, batch   612 | loss: 3.0821574MixupTrain:  epoch  0, batch   613 | loss: 3.0536933MixupTrain:  epoch  0, batch   614 | loss: 2.8576045MixupTrain:  epoch  0, batch   615 | loss: 2.9889164MixupTrain:  epoch  0, batch   616 | loss: 2.9785445MixupTrain:  epoch  0, batch   617 | loss: 2.7454696MixupTrain:  epoch  0, batch   618 | loss: 2.8612018MixupTrain:  epoch  0, batch   619 | loss: 2.8970323MixupTrain:  epoch  0, batch   620 | loss: 2.8517351MixupTrain:  epoch  0, batch   621 | loss: 3.0924232MixupTrain:  epoch  0, batch   622 | loss: 2.9608808MixupTrain:  epoch  0, batch   623 | loss: 3.0759993MixupTrain:  epoch  0, batch   624 | loss: 2.9890411MixupTrain:  epoch  0, batch   625 | loss: 2.8605251MixupTrain:  epoch  0, batch   626 | loss: 2.8465395MixupTrain:  epoch  0, batch   627 | loss: 3.0012355MixupTrain:  epoch  0, batch   628 | loss: 3.0081382MixupTrain:  epoch  0, batch   629 | loss: 2.8033133MixupTrain:  epoch  0, batch   630 | loss: 3.0655913MixupTrain:  epoch  0, batch   631 | loss: 3.0478168MixupTrain:  epoch  0, batch   632 | loss: 2.9330888MixupTrain:  epoch  0, batch   633 | loss: 2.9382920MixupTrain:  epoch  0, batch   634 | loss: 2.8707232MixupTrain:  epoch  0, batch   635 | loss: 2.8599133MixupTrain:  epoch  0, batch   636 | loss: 2.9508090MixupTrain:  epoch  0, batch   637 | loss: 2.8666177MixupTrain:  epoch  0, batch   638 | loss: 2.8436446MixupTrain:  epoch  0, batch   639 | loss: 2.9916449MixupTrain:  epoch  0, batch   640 | loss: 2.8572295MixupTrain:  epoch  0, batch   641 | loss: 3.0206132MixupTrain:  epoch  0, batch   642 | loss: 2.8521471MixupTrain:  epoch  0, batch   643 | loss: 2.9317682MixupTrain:  epoch  0, batch   644 | loss: 2.9354222MixupTrain:  epoch  0, batch   645 | loss: 2.9238286MixupTrain:  epoch  0, batch   646 | loss: 2.9335356MixupTrain:  epoch  0, batch   647 | loss: 2.9966977MixupTrain:  epoch  0, batch   648 | loss: 3.0659723MixupTrain:  epoch  0, batch   649 | loss: 2.8950367MixupTrain:  epoch  0, batch   650 | loss: 2.8401532MixupTrain:  epoch  0, batch   651 | loss: 2.8118997MixupTrain:  epoch  0, batch   652 | loss: 2.8657556MixupTrain:  epoch  0, batch   653 | loss: 2.9085295MixupTrain:  epoch  0, batch   654 | loss: 2.7557197MixupTrain:  epoch  0, batch   655 | loss: 2.9215250MixupTrain:  epoch  0, batch   656 | loss: 2.9185920MixupTrain:  epoch  0, batch   657 | loss: 2.8569605MixupTrain:  epoch  0, batch   658 | loss: 2.8300850MixupTrain:  epoch  0, batch   659 | loss: 2.9220581MixupTrain:  epoch  0, batch   660 | loss: 2.9108987MixupTrain:  epoch  0, batch   661 | loss: 2.9293690MixupTrain:  epoch  0, batch   662 | loss: 2.8571479MixupTrain:  epoch  0, batch   663 | loss: 3.2625148MixupTrain:  epoch  0, batch   664 | loss: 2.7873504
MemoryTrain:  epoch  0, batch     0 | loss: 1.1491642MemoryTrain:  epoch  0, batch     1 | loss: 1.2403785MemoryTrain:  epoch  0, batch     2 | loss: 1.3762977MemoryTrain:  epoch  0, batch     3 | loss: 1.3740733MemoryTrain:  epoch  0, batch     4 | loss: 1.3918247MemoryTrain:  epoch  0, batch     5 | loss: 1.4810903MemoryTrain:  epoch  0, batch     6 | loss: 1.6119817MemoryTrain:  epoch  0, batch     7 | loss: 1.5618454MemoryTrain:  epoch  1, batch     0 | loss: 1.3274314MemoryTrain:  epoch  1, batch     1 | loss: 1.1893504MemoryTrain:  epoch  1, batch     2 | loss: 1.2727630MemoryTrain:  epoch  1, batch     3 | loss: 1.2001798MemoryTrain:  epoch  1, batch     4 | loss: 1.2176747MemoryTrain:  epoch  1, batch     5 | loss: 1.3726765MemoryTrain:  epoch  1, batch     6 | loss: 1.2434453MemoryTrain:  epoch  1, batch     7 | loss: 1.2592268MemoryTrain:  epoch  2, batch     0 | loss: 1.1953064MemoryTrain:  epoch  2, batch     1 | loss: 1.2543844MemoryTrain:  epoch  2, batch     2 | loss: 1.2074075MemoryTrain:  epoch  2, batch     3 | loss: 1.2653056MemoryTrain:  epoch  2, batch     4 | loss: 1.2660117MemoryTrain:  epoch  2, batch     5 | loss: 1.1989129MemoryTrain:  epoch  2, batch     6 | loss: 1.3056326MemoryTrain:  epoch  2, batch     7 | loss: 1.1891682MemoryTrain:  epoch  3, batch     0 | loss: 1.1703398MemoryTrain:  epoch  3, batch     1 | loss: 1.1828871MemoryTrain:  epoch  3, batch     2 | loss: 1.2126975MemoryTrain:  epoch  3, batch     3 | loss: 1.2016730MemoryTrain:  epoch  3, batch     4 | loss: 1.2267874MemoryTrain:  epoch  3, batch     5 | loss: 1.1758313MemoryTrain:  epoch  3, batch     6 | loss: 1.2285051MemoryTrain:  epoch  3, batch     7 | loss: 1.1990541MemoryTrain:  epoch  4, batch     0 | loss: 1.2033539MemoryTrain:  epoch  4, batch     1 | loss: 1.1967545MemoryTrain:  epoch  4, batch     2 | loss: 1.1928095MemoryTrain:  epoch  4, batch     3 | loss: 1.1559088MemoryTrain:  epoch  4, batch     4 | loss: 1.2017143MemoryTrain:  epoch  4, batch     5 | loss: 1.1737690MemoryTrain:  epoch  4, batch     6 | loss: 1.1905994MemoryTrain:  epoch  4, batch     7 | loss: 1.1892874MemoryTrain:  epoch  5, batch     0 | loss: 1.1772288MemoryTrain:  epoch  5, batch     1 | loss: 1.1575854MemoryTrain:  epoch  5, batch     2 | loss: 1.2156069MemoryTrain:  epoch  5, batch     3 | loss: 1.1844403MemoryTrain:  epoch  5, batch     4 | loss: 1.1873980MemoryTrain:  epoch  5, batch     5 | loss: 1.1764491MemoryTrain:  epoch  5, batch     6 | loss: 1.1608057MemoryTrain:  epoch  5, batch     7 | loss: 1.2171319MemoryTrain:  epoch  6, batch     0 | loss: 1.5587367MemoryTrain:  epoch  6, batch     1 | loss: 1.1651025MemoryTrain:  epoch  6, batch     2 | loss: 1.2074256MemoryTrain:  epoch  6, batch     3 | loss: 1.1790239MemoryTrain:  epoch  6, batch     4 | loss: 1.3204266MemoryTrain:  epoch  6, batch     5 | loss: 1.2104894MemoryTrain:  epoch  6, batch     6 | loss: 1.1761405MemoryTrain:  epoch  6, batch     7 | loss: 1.3414489MemoryTrain:  epoch  7, batch     0 | loss: 1.3479474MemoryTrain:  epoch  7, batch     1 | loss: 1.1641154MemoryTrain:  epoch  7, batch     2 | loss: 1.1827037MemoryTrain:  epoch  7, batch     3 | loss: 1.1648372MemoryTrain:  epoch  7, batch     4 | loss: 1.4699819MemoryTrain:  epoch  7, batch     5 | loss: 1.1578128MemoryTrain:  epoch  7, batch     6 | loss: 1.1609783MemoryTrain:  epoch  7, batch     7 | loss: 1.1940868MemoryTrain:  epoch  8, batch     0 | loss: 1.1957278MemoryTrain:  epoch  8, batch     1 | loss: 1.1912282MemoryTrain:  epoch  8, batch     2 | loss: 1.1751521MemoryTrain:  epoch  8, batch     3 | loss: 1.1503835MemoryTrain:  epoch  8, batch     4 | loss: 1.1458440MemoryTrain:  epoch  8, batch     5 | loss: 1.1563714MemoryTrain:  epoch  8, batch     6 | loss: 1.1594809MemoryTrain:  epoch  8, batch     7 | loss: 1.1902050MemoryTrain:  epoch  9, batch     0 | loss: 1.2087784MemoryTrain:  epoch  9, batch     1 | loss: 1.1715503MemoryTrain:  epoch  9, batch     2 | loss: 1.1845622MemoryTrain:  epoch  9, batch     3 | loss: 1.1542614MemoryTrain:  epoch  9, batch     4 | loss: 1.1639212MemoryTrain:  epoch  9, batch     5 | loss: 1.1648778MemoryTrain:  epoch  9, batch     6 | loss: 1.1623731MemoryTrain:  epoch  9, batch     7 | loss: 1.1538650
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 85.94%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 83.75%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 84.82%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 85.16%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 86.81%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   10 | acc: 75.00%,  total acc: 86.36%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   12 | acc: 100.00%,  total acc: 88.46%   [EVAL] batch:   13 | acc: 100.00%,  total acc: 89.29%   [EVAL] batch:   14 | acc: 100.00%,  total acc: 90.00%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 91.18%   [EVAL] batch:   17 | acc: 25.00%,  total acc: 87.50%   
[EVAL] batch:    0 | acc: 6.25%,  total acc: 6.25%   [EVAL] batch:    1 | acc: 6.25%,  total acc: 6.25%   [EVAL] batch:    2 | acc: 0.00%,  total acc: 4.17%   [EVAL] batch:    3 | acc: 6.25%,  total acc: 4.69%   [EVAL] batch:    4 | acc: 6.25%,  total acc: 5.00%   [EVAL] batch:    5 | acc: 18.75%,  total acc: 7.29%   [EVAL] batch:    6 | acc: 31.25%,  total acc: 10.71%   [EVAL] batch:    7 | acc: 68.75%,  total acc: 17.97%   [EVAL] batch:    8 | acc: 62.50%,  total acc: 22.92%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 26.88%   [EVAL] batch:   10 | acc: 68.75%,  total acc: 30.68%   [EVAL] batch:   11 | acc: 25.00%,  total acc: 30.21%   [EVAL] batch:   12 | acc: 25.00%,  total acc: 29.81%   [EVAL] batch:   13 | acc: 31.25%,  total acc: 29.91%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 32.50%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 33.98%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 36.40%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 37.85%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 39.14%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 40.94%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 43.45%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 45.17%   [EVAL] batch:   22 | acc: 75.00%,  total acc: 46.47%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 48.18%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 50.25%   [EVAL] batch:   25 | acc: 81.25%,  total acc: 51.44%   [EVAL] batch:   26 | acc: 87.50%,  total acc: 52.78%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 54.24%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 55.82%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 56.67%   [EVAL] batch:   30 | acc: 81.25%,  total acc: 57.46%   [EVAL] batch:   31 | acc: 81.25%,  total acc: 58.20%   [EVAL] batch:   32 | acc: 68.75%,  total acc: 58.52%   [EVAL] batch:   33 | acc: 25.00%,  total acc: 57.54%   [EVAL] batch:   34 | acc: 12.50%,  total acc: 56.25%   [EVAL] batch:   35 | acc: 0.00%,  total acc: 54.69%   [EVAL] batch:   36 | acc: 0.00%,  total acc: 53.21%   [EVAL] batch:   37 | acc: 6.25%,  total acc: 51.97%   [EVAL] batch:   38 | acc: 6.25%,  total acc: 50.80%   [EVAL] batch:   39 | acc: 75.00%,  total acc: 51.41%   [EVAL] batch:   40 | acc: 81.25%,  total acc: 52.13%   [EVAL] batch:   41 | acc: 81.25%,  total acc: 52.83%   [EVAL] batch:   42 | acc: 81.25%,  total acc: 53.49%   [EVAL] batch:   43 | acc: 68.75%,  total acc: 53.84%   [EVAL] batch:   44 | acc: 75.00%,  total acc: 54.31%   [EVAL] batch:   45 | acc: 56.25%,  total acc: 54.35%   [EVAL] batch:   46 | acc: 43.75%,  total acc: 54.12%   [EVAL] batch:   47 | acc: 81.25%,  total acc: 54.69%   [EVAL] batch:   48 | acc: 37.50%,  total acc: 54.34%   [EVAL] batch:   49 | acc: 68.75%,  total acc: 54.62%   [EVAL] batch:   50 | acc: 81.25%,  total acc: 55.15%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 55.89%   [EVAL] batch:   52 | acc: 75.00%,  total acc: 56.25%   [EVAL] batch:   53 | acc: 25.00%,  total acc: 55.67%   [EVAL] batch:   54 | acc: 43.75%,  total acc: 55.45%   [EVAL] batch:   55 | acc: 62.50%,  total acc: 55.58%   [EVAL] batch:   56 | acc: 62.50%,  total acc: 55.70%   [EVAL] batch:   57 | acc: 75.00%,  total acc: 56.03%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 56.67%   [EVAL] batch:   59 | acc: 81.25%,  total acc: 57.08%   [EVAL] batch:   60 | acc: 31.25%,  total acc: 56.66%   [EVAL] batch:   61 | acc: 6.25%,  total acc: 55.85%   [EVAL] batch:   62 | acc: 0.00%,  total acc: 54.96%   [EVAL] batch:   63 | acc: 12.50%,  total acc: 54.30%   [EVAL] batch:   64 | acc: 37.50%,  total acc: 54.04%   [EVAL] batch:   65 | acc: 18.75%,  total acc: 53.50%   [EVAL] batch:   66 | acc: 0.00%,  total acc: 52.71%   [EVAL] batch:   67 | acc: 0.00%,  total acc: 51.93%   [EVAL] batch:   68 | acc: 12.50%,  total acc: 51.36%   [EVAL] batch:   69 | acc: 12.50%,  total acc: 50.80%   [EVAL] batch:   70 | acc: 18.75%,  total acc: 50.35%   [EVAL] batch:   71 | acc: 0.00%,  total acc: 49.65%   [EVAL] batch:   72 | acc: 56.25%,  total acc: 49.74%   [EVAL] batch:   73 | acc: 75.00%,  total acc: 50.08%   [EVAL] batch:   74 | acc: 68.75%,  total acc: 50.33%   [EVAL] batch:   75 | acc: 87.50%,  total acc: 50.82%   [EVAL] batch:   76 | acc: 68.75%,  total acc: 51.06%   [EVAL] batch:   77 | acc: 75.00%,  total acc: 51.36%   [EVAL] batch:   78 | acc: 75.00%,  total acc: 51.66%   [EVAL] batch:   79 | acc: 93.75%,  total acc: 52.19%   [EVAL] batch:   80 | acc: 93.75%,  total acc: 52.70%   [EVAL] batch:   81 | acc: 100.00%,  total acc: 53.28%   [EVAL] batch:   82 | acc: 87.50%,  total acc: 53.69%   [EVAL] batch:   83 | acc: 62.50%,  total acc: 53.79%   [EVAL] batch:   84 | acc: 0.00%,  total acc: 53.16%   [EVAL] batch:   85 | acc: 0.00%,  total acc: 52.54%   [EVAL] batch:   86 | acc: 0.00%,  total acc: 51.94%   [EVAL] batch:   87 | acc: 81.25%,  total acc: 52.27%   [EVAL] batch:   88 | acc: 93.75%,  total acc: 52.74%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 53.26%   [EVAL] batch:   90 | acc: 100.00%,  total acc: 53.78%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 54.28%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 54.77%   [EVAL] batch:   93 | acc: 100.00%,  total acc: 55.25%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 55.72%   [EVAL] batch:   95 | acc: 81.25%,  total acc: 55.99%   [EVAL] batch:   96 | acc: 31.25%,  total acc: 55.73%   [EVAL] batch:   97 | acc: 62.50%,  total acc: 55.80%   [EVAL] batch:   98 | acc: 93.75%,  total acc: 56.19%   [EVAL] batch:   99 | acc: 81.25%,  total acc: 56.44%   [EVAL] batch:  100 | acc: 75.00%,  total acc: 56.62%   [EVAL] batch:  101 | acc: 56.25%,  total acc: 56.62%   [EVAL] batch:  102 | acc: 81.25%,  total acc: 56.86%   [EVAL] batch:  103 | acc: 56.25%,  total acc: 56.85%   [EVAL] batch:  104 | acc: 87.50%,  total acc: 57.14%   [EVAL] batch:  105 | acc: 75.00%,  total acc: 57.31%   [EVAL] batch:  106 | acc: 100.00%,  total acc: 57.71%   [EVAL] batch:  107 | acc: 100.00%,  total acc: 58.10%   [EVAL] batch:  108 | acc: 87.50%,  total acc: 58.37%   [EVAL] batch:  109 | acc: 100.00%,  total acc: 58.75%   [EVAL] batch:  110 | acc: 75.00%,  total acc: 58.90%   [EVAL] batch:  111 | acc: 18.75%,  total acc: 58.54%   [EVAL] batch:  112 | acc: 6.25%,  total acc: 58.08%   [EVAL] batch:  113 | acc: 6.25%,  total acc: 57.62%   [EVAL] batch:  114 | acc: 0.00%,  total acc: 57.12%   [EVAL] batch:  115 | acc: 62.50%,  total acc: 57.17%   [EVAL] batch:  116 | acc: 81.25%,  total acc: 57.37%   [EVAL] batch:  117 | acc: 87.50%,  total acc: 57.63%   [EVAL] batch:  118 | acc: 81.25%,  total acc: 57.83%   [EVAL] batch:  119 | acc: 81.25%,  total acc: 58.02%   [EVAL] batch:  120 | acc: 87.50%,  total acc: 58.26%   [EVAL] batch:  121 | acc: 81.25%,  total acc: 58.45%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 58.79%   [EVAL] batch:  123 | acc: 87.50%,  total acc: 59.02%   [EVAL] batch:  124 | acc: 100.00%,  total acc: 59.35%   [EVAL] batch:  125 | acc: 81.25%,  total acc: 59.52%   [EVAL] batch:  126 | acc: 87.50%,  total acc: 59.74%   [EVAL] batch:  127 | acc: 100.00%,  total acc: 60.06%   [EVAL] batch:  128 | acc: 100.00%,  total acc: 60.37%   [EVAL] batch:  129 | acc: 100.00%,  total acc: 60.67%   [EVAL] batch:  130 | acc: 100.00%,  total acc: 60.97%   [EVAL] batch:  131 | acc: 100.00%,  total acc: 61.27%   [EVAL] batch:  132 | acc: 62.50%,  total acc: 61.28%   
cur_acc:  ['0.8561', '0.8571', '0.7422', '0.7837', '0.5824', '0.9152', '0.7875', '0.8750']
his_acc:  ['0.8561', '0.8338', '0.7234', '0.6828', '0.6428', '0.6467', '0.6406', '0.6128']
--------Round  1
seed:  200
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/test.pkl
Task_order: [7 6 3 2 4 0 5 1]
prepared data!
CurrentTrain: epoch  0, batch     0 | loss: 13.2802010CurrentTrain: epoch  0, batch     1 | loss: 13.1349955CurrentTrain: epoch  0, batch     2 | loss: 12.8535433CurrentTrain: epoch  0, batch     3 | loss: 12.7593260CurrentTrain: epoch  0, batch     4 | loss: 12.5562077CurrentTrain: epoch  0, batch     5 | loss: 12.3904190CurrentTrain: epoch  0, batch     6 | loss: 12.6205006CurrentTrain: epoch  0, batch     7 | loss: 12.2283916CurrentTrain: epoch  0, batch     8 | loss: 12.3876419CurrentTrain: epoch  0, batch     9 | loss: 12.1698198CurrentTrain: epoch  0, batch    10 | loss: 11.9389248CurrentTrain: epoch  0, batch    11 | loss: 11.7447548CurrentTrain: epoch  0, batch    12 | loss: 11.7351961CurrentTrain: epoch  0, batch    13 | loss: 11.4284248CurrentTrain: epoch  0, batch    14 | loss: 11.8203602CurrentTrain: epoch  0, batch    15 | loss: 11.5768623CurrentTrain: epoch  0, batch    16 | loss: 11.1595383CurrentTrain: epoch  0, batch    17 | loss: 11.6565123CurrentTrain: epoch  0, batch    18 | loss: 11.2405529CurrentTrain: epoch  0, batch    19 | loss: 10.9821968CurrentTrain: epoch  0, batch    20 | loss: 11.2366266CurrentTrain: epoch  0, batch    21 | loss: 11.1120310CurrentTrain: epoch  0, batch    22 | loss: 11.1243448CurrentTrain: epoch  0, batch    23 | loss: 10.5962372CurrentTrain: epoch  0, batch    24 | loss: 11.3327875CurrentTrain: epoch  0, batch    25 | loss: 10.4282646CurrentTrain: epoch  0, batch    26 | loss: 11.0734892CurrentTrain: epoch  0, batch    27 | loss: 10.7823658CurrentTrain: epoch  0, batch    28 | loss: 11.1045742CurrentTrain: epoch  0, batch    29 | loss: 10.6497116CurrentTrain: epoch  0, batch    30 | loss: 11.0069180CurrentTrain: epoch  0, batch    31 | loss: 10.7062941CurrentTrain: epoch  0, batch    32 | loss: 10.4087543CurrentTrain: epoch  0, batch    33 | loss: 10.6291437CurrentTrain: epoch  0, batch    34 | loss: 10.4319420CurrentTrain: epoch  0, batch    35 | loss: 10.2932320CurrentTrain: epoch  0, batch    36 | loss: 10.3294687CurrentTrain: epoch  0, batch    37 | loss: 10.6886921CurrentTrain: epoch  1, batch     0 | loss: 10.0274220CurrentTrain: epoch  1, batch     1 | loss: 10.3605671CurrentTrain: epoch  1, batch     2 | loss: 9.3831139CurrentTrain: epoch  1, batch     3 | loss: 9.9566097CurrentTrain: epoch  1, batch     4 | loss: 10.1696930CurrentTrain: epoch  1, batch     5 | loss: 9.2270870CurrentTrain: epoch  1, batch     6 | loss: 9.9473782CurrentTrain: epoch  1, batch     7 | loss: 9.5648832CurrentTrain: epoch  1, batch     8 | loss: 10.4515524CurrentTrain: epoch  1, batch     9 | loss: 9.6059303CurrentTrain: epoch  1, batch    10 | loss: 10.1541948CurrentTrain: epoch  1, batch    11 | loss: 9.9902744CurrentTrain: epoch  1, batch    12 | loss: 9.4070539CurrentTrain: epoch  1, batch    13 | loss: 9.3520918CurrentTrain: epoch  1, batch    14 | loss: 9.4026279CurrentTrain: epoch  1, batch    15 | loss: 8.5980301CurrentTrain: epoch  1, batch    16 | loss: 9.2435780CurrentTrain: epoch  1, batch    17 | loss: 8.8028927CurrentTrain: epoch  1, batch    18 | loss: 9.3838978CurrentTrain: epoch  1, batch    19 | loss: 8.7484388CurrentTrain: epoch  1, batch    20 | loss: 9.2054653CurrentTrain: epoch  1, batch    21 | loss: 8.9022350CurrentTrain: epoch  1, batch    22 | loss: 9.1228895CurrentTrain: epoch  1, batch    23 | loss: 8.9608364CurrentTrain: epoch  1, batch    24 | loss: 9.0797548CurrentTrain: epoch  1, batch    25 | loss: 8.8821726CurrentTrain: epoch  1, batch    26 | loss: 8.6187983CurrentTrain: epoch  1, batch    27 | loss: 8.8972969CurrentTrain: epoch  1, batch    28 | loss: 9.1453152CurrentTrain: epoch  1, batch    29 | loss: 9.6284580CurrentTrain: epoch  1, batch    30 | loss: 9.0746851CurrentTrain: epoch  1, batch    31 | loss: 8.6957960CurrentTrain: epoch  1, batch    32 | loss: 8.4105644CurrentTrain: epoch  1, batch    33 | loss: 8.3571301CurrentTrain: epoch  1, batch    34 | loss: 8.3048668CurrentTrain: epoch  1, batch    35 | loss: 8.1186781CurrentTrain: epoch  1, batch    36 | loss: 8.6023245CurrentTrain: epoch  1, batch    37 | loss: 9.2198334CurrentTrain: epoch  2, batch     0 | loss: 7.9115930CurrentTrain: epoch  2, batch     1 | loss: 8.1513596CurrentTrain: epoch  2, batch     2 | loss: 8.7652540CurrentTrain: epoch  2, batch     3 | loss: 7.6508551CurrentTrain: epoch  2, batch     4 | loss: 7.4840450CurrentTrain: epoch  2, batch     5 | loss: 8.8183422CurrentTrain: epoch  2, batch     6 | loss: 8.1857052CurrentTrain: epoch  2, batch     7 | loss: 8.8885574CurrentTrain: epoch  2, batch     8 | loss: 8.4472828CurrentTrain: epoch  2, batch     9 | loss: 8.7860470CurrentTrain: epoch  2, batch    10 | loss: 8.3565512CurrentTrain: epoch  2, batch    11 | loss: 8.0827093CurrentTrain: epoch  2, batch    12 | loss: 7.4907031CurrentTrain: epoch  2, batch    13 | loss: 7.8509359CurrentTrain: epoch  2, batch    14 | loss: 8.5059528CurrentTrain: epoch  2, batch    15 | loss: 8.2803631CurrentTrain: epoch  2, batch    16 | loss: 7.9751143CurrentTrain: epoch  2, batch    17 | loss: 8.5223141CurrentTrain: epoch  2, batch    18 | loss: 8.3344469CurrentTrain: epoch  2, batch    19 | loss: 8.5263338CurrentTrain: epoch  2, batch    20 | loss: 8.4650478CurrentTrain: epoch  2, batch    21 | loss: 7.7884769CurrentTrain: epoch  2, batch    22 | loss: 8.5135641CurrentTrain: epoch  2, batch    23 | loss: 7.8034697CurrentTrain: epoch  2, batch    24 | loss: 7.8297262CurrentTrain: epoch  2, batch    25 | loss: 7.7552643CurrentTrain: epoch  2, batch    26 | loss: 7.6751842CurrentTrain: epoch  2, batch    27 | loss: 8.1470509CurrentTrain: epoch  2, batch    28 | loss: 7.5798874CurrentTrain: epoch  2, batch    29 | loss: 7.7476630CurrentTrain: epoch  2, batch    30 | loss: 6.9305363CurrentTrain: epoch  2, batch    31 | loss: 7.3527551CurrentTrain: epoch  2, batch    32 | loss: 7.3495054CurrentTrain: epoch  2, batch    33 | loss: 7.3708644CurrentTrain: epoch  2, batch    34 | loss: 8.0199623CurrentTrain: epoch  2, batch    35 | loss: 7.1278238CurrentTrain: epoch  2, batch    36 | loss: 8.0304947CurrentTrain: epoch  2, batch    37 | loss: 8.1530895CurrentTrain: epoch  3, batch     0 | loss: 7.9919777CurrentTrain: epoch  3, batch     1 | loss: 7.0634346CurrentTrain: epoch  3, batch     2 | loss: 7.9929681CurrentTrain: epoch  3, batch     3 | loss: 7.4478388CurrentTrain: epoch  3, batch     4 | loss: 7.2146778CurrentTrain: epoch  3, batch     5 | loss: 6.6581697CurrentTrain: epoch  3, batch     6 | loss: 7.6002264CurrentTrain: epoch  3, batch     7 | loss: 6.3198609CurrentTrain: epoch  3, batch     8 | loss: 8.2176771CurrentTrain: epoch  3, batch     9 | loss: 6.8330741CurrentTrain: epoch  3, batch    10 | loss: 7.5612659CurrentTrain: epoch  3, batch    11 | loss: 6.7674894CurrentTrain: epoch  3, batch    12 | loss: 7.1080904CurrentTrain: epoch  3, batch    13 | loss: 6.7774429CurrentTrain: epoch  3, batch    14 | loss: 7.7298717CurrentTrain: epoch  3, batch    15 | loss: 6.9157739CurrentTrain: epoch  3, batch    16 | loss: 7.5558996CurrentTrain: epoch  3, batch    17 | loss: 6.5049462CurrentTrain: epoch  3, batch    18 | loss: 6.6028433CurrentTrain: epoch  3, batch    19 | loss: 6.8423963CurrentTrain: epoch  3, batch    20 | loss: 6.1856627CurrentTrain: epoch  3, batch    21 | loss: 7.5518608CurrentTrain: epoch  3, batch    22 | loss: 6.8488960CurrentTrain: epoch  3, batch    23 | loss: 7.2033234CurrentTrain: epoch  3, batch    24 | loss: 8.2578735CurrentTrain: epoch  3, batch    25 | loss: 7.0632348CurrentTrain: epoch  3, batch    26 | loss: 7.3757458CurrentTrain: epoch  3, batch    27 | loss: 6.0445309CurrentTrain: epoch  3, batch    28 | loss: 8.9760151CurrentTrain: epoch  3, batch    29 | loss: 7.7080750CurrentTrain: epoch  3, batch    30 | loss: 7.1876688CurrentTrain: epoch  3, batch    31 | loss: 6.8349247CurrentTrain: epoch  3, batch    32 | loss: 7.6860561CurrentTrain: epoch  3, batch    33 | loss: 7.9990501CurrentTrain: epoch  3, batch    34 | loss: 7.8640647CurrentTrain: epoch  3, batch    35 | loss: 7.1472502CurrentTrain: epoch  3, batch    36 | loss: 7.1710143CurrentTrain: epoch  3, batch    37 | loss: 6.3860636CurrentTrain: epoch  4, batch     0 | loss: 6.9832926CurrentTrain: epoch  4, batch     1 | loss: 7.2314768CurrentTrain: epoch  4, batch     2 | loss: 7.5010943CurrentTrain: epoch  4, batch     3 | loss: 7.1646175CurrentTrain: epoch  4, batch     4 | loss: 6.9670916CurrentTrain: epoch  4, batch     5 | loss: 7.5047970CurrentTrain: epoch  4, batch     6 | loss: 6.6741734CurrentTrain: epoch  4, batch     7 | loss: 7.0021296CurrentTrain: epoch  4, batch     8 | loss: 7.0118618CurrentTrain: epoch  4, batch     9 | loss: 7.3959436CurrentTrain: epoch  4, batch    10 | loss: 5.7335539CurrentTrain: epoch  4, batch    11 | loss: 6.8795500CurrentTrain: epoch  4, batch    12 | loss: 6.5106115CurrentTrain: epoch  4, batch    13 | loss: 6.2971792CurrentTrain: epoch  4, batch    14 | loss: 6.8815899CurrentTrain: epoch  4, batch    15 | loss: 7.2562542CurrentTrain: epoch  4, batch    16 | loss: 7.6452436CurrentTrain: epoch  4, batch    17 | loss: 6.7229471CurrentTrain: epoch  4, batch    18 | loss: 7.8008537CurrentTrain: epoch  4, batch    19 | loss: 6.3723660CurrentTrain: epoch  4, batch    20 | loss: 6.4975872CurrentTrain: epoch  4, batch    21 | loss: 6.5179658CurrentTrain: epoch  4, batch    22 | loss: 7.7382116CurrentTrain: epoch  4, batch    23 | loss: 6.3585639CurrentTrain: epoch  4, batch    24 | loss: 6.5111389CurrentTrain: epoch  4, batch    25 | loss: 6.9061174CurrentTrain: epoch  4, batch    26 | loss: 7.4992447CurrentTrain: epoch  4, batch    27 | loss: 6.2905216CurrentTrain: epoch  4, batch    28 | loss: 6.7171593CurrentTrain: epoch  4, batch    29 | loss: 6.4646015CurrentTrain: epoch  4, batch    30 | loss: 6.0314865CurrentTrain: epoch  4, batch    31 | loss: 6.9433403CurrentTrain: epoch  4, batch    32 | loss: 7.5051522CurrentTrain: epoch  4, batch    33 | loss: 7.8230677CurrentTrain: epoch  4, batch    34 | loss: 6.1751699CurrentTrain: epoch  4, batch    35 | loss: 6.0209470CurrentTrain: epoch  4, batch    36 | loss: 6.5517120CurrentTrain: epoch  4, batch    37 | loss: 7.1280808CurrentTrain: epoch  5, batch     0 | loss: 6.0833960CurrentTrain: epoch  5, batch     1 | loss: 7.3361759CurrentTrain: epoch  5, batch     2 | loss: 6.4904757CurrentTrain: epoch  5, batch     3 | loss: 6.8945618CurrentTrain: epoch  5, batch     4 | loss: 6.5567756CurrentTrain: epoch  5, batch     5 | loss: 6.6046276CurrentTrain: epoch  5, batch     6 | loss: 6.6634016CurrentTrain: epoch  5, batch     7 | loss: 6.0954432CurrentTrain: epoch  5, batch     8 | loss: 6.7984543CurrentTrain: epoch  5, batch     9 | loss: 7.2363191CurrentTrain: epoch  5, batch    10 | loss: 6.4388485CurrentTrain: epoch  5, batch    11 | loss: 6.7644253CurrentTrain: epoch  5, batch    12 | loss: 6.1658735CurrentTrain: epoch  5, batch    13 | loss: 6.5088096CurrentTrain: epoch  5, batch    14 | loss: 6.0094748CurrentTrain: epoch  5, batch    15 | loss: 7.1649895CurrentTrain: epoch  5, batch    16 | loss: 6.6348181CurrentTrain: epoch  5, batch    17 | loss: 6.9743586CurrentTrain: epoch  5, batch    18 | loss: 6.5736294CurrentTrain: epoch  5, batch    19 | loss: 6.3347836CurrentTrain: epoch  5, batch    20 | loss: 5.9500222CurrentTrain: epoch  5, batch    21 | loss: 6.3778958CurrentTrain: epoch  5, batch    22 | loss: 6.9609060CurrentTrain: epoch  5, batch    23 | loss: 6.4806757CurrentTrain: epoch  5, batch    24 | loss: 5.7881370CurrentTrain: epoch  5, batch    25 | loss: 6.6556287CurrentTrain: epoch  5, batch    26 | loss: 6.3058505CurrentTrain: epoch  5, batch    27 | loss: 6.1629601CurrentTrain: epoch  5, batch    28 | loss: 6.6966095CurrentTrain: epoch  5, batch    29 | loss: 6.2729626CurrentTrain: epoch  5, batch    30 | loss: 7.1454935CurrentTrain: epoch  5, batch    31 | loss: 7.1009192CurrentTrain: epoch  5, batch    32 | loss: 6.3483148CurrentTrain: epoch  5, batch    33 | loss: 6.2577152CurrentTrain: epoch  5, batch    34 | loss: 5.9635215CurrentTrain: epoch  5, batch    35 | loss: 6.3042617CurrentTrain: epoch  5, batch    36 | loss: 6.2796874CurrentTrain: epoch  5, batch    37 | loss: 6.9707670CurrentTrain: epoch  6, batch     0 | loss: 6.5013580CurrentTrain: epoch  6, batch     1 | loss: 6.6169977CurrentTrain: epoch  6, batch     2 | loss: 6.3493214CurrentTrain: epoch  6, batch     3 | loss: 5.7356715CurrentTrain: epoch  6, batch     4 | loss: 6.0400085CurrentTrain: epoch  6, batch     5 | loss: 6.4937878CurrentTrain: epoch  6, batch     6 | loss: 6.4186697CurrentTrain: epoch  6, batch     7 | loss: 5.9666777CurrentTrain: epoch  6, batch     8 | loss: 5.7081084CurrentTrain: epoch  6, batch     9 | loss: 6.3238177CurrentTrain: epoch  6, batch    10 | loss: 5.8721495CurrentTrain: epoch  6, batch    11 | loss: 5.6340542CurrentTrain: epoch  6, batch    12 | loss: 6.0375528CurrentTrain: epoch  6, batch    13 | loss: 6.0690765CurrentTrain: epoch  6, batch    14 | loss: 5.8627453CurrentTrain: epoch  6, batch    15 | loss: 5.7513885CurrentTrain: epoch  6, batch    16 | loss: 5.8665733CurrentTrain: epoch  6, batch    17 | loss: 6.1913600CurrentTrain: epoch  6, batch    18 | loss: 6.2294722CurrentTrain: epoch  6, batch    19 | loss: 5.7066860CurrentTrain: epoch  6, batch    20 | loss: 6.3204298CurrentTrain: epoch  6, batch    21 | loss: 5.8985815CurrentTrain: epoch  6, batch    22 | loss: 5.6156740CurrentTrain: epoch  6, batch    23 | loss: 6.3429537CurrentTrain: epoch  6, batch    24 | loss: 5.9274578CurrentTrain: epoch  6, batch    25 | loss: 6.0186138CurrentTrain: epoch  6, batch    26 | loss: 6.3320160CurrentTrain: epoch  6, batch    27 | loss: 5.8505263CurrentTrain: epoch  6, batch    28 | loss: 5.9235129CurrentTrain: epoch  6, batch    29 | loss: 6.9317207CurrentTrain: epoch  6, batch    30 | loss: 6.3005133CurrentTrain: epoch  6, batch    31 | loss: 6.6660266CurrentTrain: epoch  6, batch    32 | loss: 6.4551473CurrentTrain: epoch  6, batch    33 | loss: 5.7569952CurrentTrain: epoch  6, batch    34 | loss: 5.5971513CurrentTrain: epoch  6, batch    35 | loss: 6.6459103CurrentTrain: epoch  6, batch    36 | loss: 6.0601206CurrentTrain: epoch  6, batch    37 | loss: 5.6527200CurrentTrain: epoch  7, batch     0 | loss: 5.9147763CurrentTrain: epoch  7, batch     1 | loss: 6.6560221CurrentTrain: epoch  7, batch     2 | loss: 5.7639699CurrentTrain: epoch  7, batch     3 | loss: 6.7172852CurrentTrain: epoch  7, batch     4 | loss: 5.3524275CurrentTrain: epoch  7, batch     5 | loss: 5.6810889CurrentTrain: epoch  7, batch     6 | loss: 5.5397310CurrentTrain: epoch  7, batch     7 | loss: 6.8823538CurrentTrain: epoch  7, batch     8 | loss: 5.8663378CurrentTrain: epoch  7, batch     9 | loss: 5.9696116CurrentTrain: epoch  7, batch    10 | loss: 5.9785347CurrentTrain: epoch  7, batch    11 | loss: 5.5797195CurrentTrain: epoch  7, batch    12 | loss: 6.3873587CurrentTrain: epoch  7, batch    13 | loss: 5.2493525CurrentTrain: epoch  7, batch    14 | loss: 5.8773270CurrentTrain: epoch  7, batch    15 | loss: 5.1019535CurrentTrain: epoch  7, batch    16 | loss: 5.2589927CurrentTrain: epoch  7, batch    17 | loss: 5.6192894CurrentTrain: epoch  7, batch    18 | loss: 5.9998703CurrentTrain: epoch  7, batch    19 | loss: 5.5079513CurrentTrain: epoch  7, batch    20 | loss: 6.0957985CurrentTrain: epoch  7, batch    21 | loss: 6.3054371CurrentTrain: epoch  7, batch    22 | loss: 5.5577707CurrentTrain: epoch  7, batch    23 | loss: 5.4725075CurrentTrain: epoch  7, batch    24 | loss: 5.8252001CurrentTrain: epoch  7, batch    25 | loss: 5.6671000CurrentTrain: epoch  7, batch    26 | loss: 5.4472437CurrentTrain: epoch  7, batch    27 | loss: 5.5402231CurrentTrain: epoch  7, batch    28 | loss: 5.2967792CurrentTrain: epoch  7, batch    29 | loss: 5.7896690CurrentTrain: epoch  7, batch    30 | loss: 5.2447500CurrentTrain: epoch  7, batch    31 | loss: 5.8165655CurrentTrain: epoch  7, batch    32 | loss: 5.2711825CurrentTrain: epoch  7, batch    33 | loss: 5.8786683CurrentTrain: epoch  7, batch    34 | loss: 6.0796375CurrentTrain: epoch  7, batch    35 | loss: 5.4314795CurrentTrain: epoch  7, batch    36 | loss: 5.4015923CurrentTrain: epoch  7, batch    37 | loss: 5.2279620CurrentTrain: epoch  8, batch     0 | loss: 5.7651100CurrentTrain: epoch  8, batch     1 | loss: 5.5463328CurrentTrain: epoch  8, batch     2 | loss: 5.6352644CurrentTrain: epoch  8, batch     3 | loss: 5.4617023CurrentTrain: epoch  8, batch     4 | loss: 5.3868985CurrentTrain: epoch  8, batch     5 | loss: 5.2759304CurrentTrain: epoch  8, batch     6 | loss: 5.4585609CurrentTrain: epoch  8, batch     7 | loss: 5.2740984CurrentTrain: epoch  8, batch     8 | loss: 5.2903099CurrentTrain: epoch  8, batch     9 | loss: 5.2408276CurrentTrain: epoch  8, batch    10 | loss: 5.1950755CurrentTrain: epoch  8, batch    11 | loss: 5.2687807CurrentTrain: epoch  8, batch    12 | loss: 5.1533937CurrentTrain: epoch  8, batch    13 | loss: 5.2864199CurrentTrain: epoch  8, batch    14 | loss: 5.0697298CurrentTrain: epoch  8, batch    15 | loss: 5.2984962CurrentTrain: epoch  8, batch    16 | loss: 5.5476913CurrentTrain: epoch  8, batch    17 | loss: 5.4338136CurrentTrain: epoch  8, batch    18 | loss: 5.4137597CurrentTrain: epoch  8, batch    19 | loss: 5.7288036CurrentTrain: epoch  8, batch    20 | loss: 5.3349481CurrentTrain: epoch  8, batch    21 | loss: 5.2418222CurrentTrain: epoch  8, batch    22 | loss: 5.4384861CurrentTrain: epoch  8, batch    23 | loss: 5.2596416CurrentTrain: epoch  8, batch    24 | loss: 4.9609900CurrentTrain: epoch  8, batch    25 | loss: 5.0900707CurrentTrain: epoch  8, batch    26 | loss: 5.3500009CurrentTrain: epoch  8, batch    27 | loss: 5.1826162CurrentTrain: epoch  8, batch    28 | loss: 5.0278215CurrentTrain: epoch  8, batch    29 | loss: 5.1849627CurrentTrain: epoch  8, batch    30 | loss: 5.6713986CurrentTrain: epoch  8, batch    31 | loss: 5.2621274CurrentTrain: epoch  8, batch    32 | loss: 5.1813807CurrentTrain: epoch  8, batch    33 | loss: 5.0310068CurrentTrain: epoch  8, batch    34 | loss: 5.7943745CurrentTrain: epoch  8, batch    35 | loss: 5.1644030CurrentTrain: epoch  8, batch    36 | loss: 5.2162132CurrentTrain: epoch  8, batch    37 | loss: 5.1813245CurrentTrain: epoch  9, batch     0 | loss: 4.9590979CurrentTrain: epoch  9, batch     1 | loss: 5.5985775CurrentTrain: epoch  9, batch     2 | loss: 5.7749796CurrentTrain: epoch  9, batch     3 | loss: 5.2126770CurrentTrain: epoch  9, batch     4 | loss: 5.1679702CurrentTrain: epoch  9, batch     5 | loss: 5.2355785CurrentTrain: epoch  9, batch     6 | loss: 5.0929294CurrentTrain: epoch  9, batch     7 | loss: 5.2581539CurrentTrain: epoch  9, batch     8 | loss: 5.0894132CurrentTrain: epoch  9, batch     9 | loss: 5.1172247CurrentTrain: epoch  9, batch    10 | loss: 5.2053318CurrentTrain: epoch  9, batch    11 | loss: 5.1939783CurrentTrain: epoch  9, batch    12 | loss: 5.0255156CurrentTrain: epoch  9, batch    13 | loss: 5.0848608CurrentTrain: epoch  9, batch    14 | loss: 5.0442362CurrentTrain: epoch  9, batch    15 | loss: 5.1826906CurrentTrain: epoch  9, batch    16 | loss: 5.2034988CurrentTrain: epoch  9, batch    17 | loss: 5.0356255CurrentTrain: epoch  9, batch    18 | loss: 4.8979101CurrentTrain: epoch  9, batch    19 | loss: 4.9665632CurrentTrain: epoch  9, batch    20 | loss: 5.0905142CurrentTrain: epoch  9, batch    21 | loss: 4.9941010CurrentTrain: epoch  9, batch    22 | loss: 5.1560440CurrentTrain: epoch  9, batch    23 | loss: 5.2060556CurrentTrain: epoch  9, batch    24 | loss: 5.0347013CurrentTrain: epoch  9, batch    25 | loss: 5.0389137CurrentTrain: epoch  9, batch    26 | loss: 4.9953737CurrentTrain: epoch  9, batch    27 | loss: 5.1345081CurrentTrain: epoch  9, batch    28 | loss: 5.2083936CurrentTrain: epoch  9, batch    29 | loss: 5.0580378CurrentTrain: epoch  9, batch    30 | loss: 5.1392102CurrentTrain: epoch  9, batch    31 | loss: 5.1486464CurrentTrain: epoch  9, batch    32 | loss: 5.3934546CurrentTrain: epoch  9, batch    33 | loss: 5.4659247CurrentTrain: epoch  9, batch    34 | loss: 5.3416600CurrentTrain: epoch  9, batch    35 | loss: 4.9406276CurrentTrain: epoch  9, batch    36 | loss: 4.9448872CurrentTrain: epoch  9, batch    37 | loss: 4.9273853
#### person countries of residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person countries of residence" means a person is living in a country.
Here is an example:
Relation: person countries of residence
Context: the head of iran 's atomic energy organisation , ali akbar salehi , told domestic news agencies the new proposal had come in response to iran 's move last week to begin enriching uranium itself to the 20 percent level required for a tehran medical research rector after rejecting a previous offer .
Head Entity: ali akbar salehi
Tail Entity: iran
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person countries of residence":
Relation: person countries of residence  
Context: After years of living in the bustling city of New York, Maria Gonzalez decided to return to her hometown in Mexico, where she felt a stronger connection to her roots and family.  
Head Entity: Maria Gonzalez  
Tail Entity: Mexico  

Relation: person countries of residence  
Context: Following his successful career in London, James Smith relocated to Australia, seeking a more relaxed lifestyle by the beach.  
Head Entity: James Smith  
Tail Entity: Australia  
#### organization top members employees ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization top members employees" means an organization 's employer is a person.
Here is an example:
Relation: organization top members employees
Context: as for any gay or lesbian people who enlisted with the military this week , `` they may be told they can not join '' if the pentagon decides to enforce the law , said aubrey sarvis , executive director of the servicemembers legal defense network , a group representing service members affected by the law .
Head Entity: servicemembers legal defense network
Tail Entity: aubrey sarvis
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization top members employees":
Relation: organization top members employees  
Context: The board of directors at Tech Innovations Inc. recently appointed Sarah Johnson as the new Chief Technology Officer, highlighting her extensive experience in the tech industry and her leadership skills.  
Head Entity: Tech Innovations Inc.  
Tail Entity: Sarah Johnson  

Relation: organization top members employees  
Context: During the annual meeting, the CEO of Green Earth Solutions, Mark Thompson, announced the promotion of Lisa Chen to the position of Chief Financial Officer, recognizing her contributions to the company's growth.  
Head Entity: Green Earth Solutions  
Tail Entity: Lisa Chen  
#### organization member of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization member of" means an organization is the member of an organization.
Here is an example:
Relation: organization member of
Context: south africa 1-5 leyds cnr biccard streets cosatu 's concern has been that the scorpions ' existence as a branch of the national directorate of public prosecutions blurred the necessary separation of functions between those investigating crime and those prosecuting the criminals .
Head Entity: scorpions
Tail Entity: national directorate of public prosecutions
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization member of":
Relation: organization member of  
Context: The International Federation of Red Cross and Red Crescent Societies (IFRC) is a global humanitarian organization that coordinates the activities of national Red Cross and Red Crescent societies around the world.  
Head Entity: International Federation of Red Cross and Red Crescent Societies  
Tail Entity: national Red Cross and Red Crescent societies  

Relation: organization member of  
Context: The European Union (EU) is a political and economic union of member states that are located primarily in Europe, working together to promote peace and stability in the region.  
Head Entity: European Union  
Tail Entity: member states  
#### person origin ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person origin" means a person 's nationality is a country.
Here is an example:
Relation: person origin
Context: the talks will aim to thrash out a common approach ahead of new negotiations between eu foreign policy chief javier solana and iranian official ali larijani , state department spokesman sean mccormack said .
Head Entity: ali larijani
Tail Entity: iranian
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person origin":
Relation: person origin  
Context: The renowned physicist Albert Einstein was born in the Kingdom of Württemberg in the German Empire, which is now part of modern-day Germany.  
Head Entity: Albert Einstein  
Tail Entity: German  

Relation: person origin  
Context: The famous author Chimamanda Ngozi Adichie hails from Nigeria, where she was born and raised before moving to the United States.  
Head Entity: Chimamanda Ngozi Adichie  
Tail Entity: Nigerian  
#### person title ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person title" means a person 's title is a title.
Here is an example:
Relation: person title
Context: senate banking committee chairman chris dodd agreed that `` clean and simple '' bill was necessary , but the democrat also called for changes to the proposal to ensure accountability and assistance for homeowners .
Head Entity: chris dodd
Tail Entity: chairman
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person title":
Relation: person title  
Context: After years of dedicated service, the board of directors appointed Sarah Johnson as the new chief executive officer, marking a significant milestone in her career.  
Head Entity: Sarah Johnson  
Tail Entity: chief executive officer  

Relation: person title  
Context: During the award ceremony, the renowned scientist Dr. Emily Carter was recognized for her groundbreaking research and was honored with the title of lead researcher in the environmental studies department.  
Head Entity: Dr. Emily Carter  
Tail Entity: lead researcher  
#### organization country of headquarters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization country of headquarters" means an organization is located in a country.
Here is an example:
Relation: organization country of headquarters
Context: commander viliame naupoto , chairman of the fiji pine limited announced the woodchips exports target here tuesday after signing a woodchip sale agreement with japan 's itochu corporation .
Head Entity: itochu corporation
Tail Entity: japan
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization country of headquarters":
Relation: organization country of headquarters  
Context: the headquarters of the multinational technology company apple inc. is located in cupertino, california, where it has been a significant player in the tech industry.  
Head Entity: apple inc.  
Tail Entity: california  

Relation: organization country of headquarters  
Context: the famous automobile manufacturer toyota motor corporation has its main office in toyota city, a key location in japan's automotive sector.  
Head Entity: toyota motor corporation  
Tail Entity: japan  
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 50.00%,  total acc: 65.62%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 72.92%   [EVAL] batch:    3 | acc: 56.25%,  total acc: 68.75%   [EVAL] batch:    4 | acc: 62.50%,  total acc: 67.50%   [EVAL] batch:    5 | acc: 68.75%,  total acc: 67.71%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 71.43%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 77.08%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 78.75%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 80.68%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 81.77%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 82.69%   [EVAL] batch:   13 | acc: 75.00%,  total acc: 82.14%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 81.67%   [EVAL] batch:   15 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:   16 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:   17 | acc: 75.00%,  total acc: 80.90%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 81.58%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 81.88%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 82.74%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 83.52%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 84.24%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 84.64%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 85.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 85.82%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 86.11%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 86.61%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 87.07%   [EVAL] batch:   29 | acc: 75.00%,  total acc: 86.67%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 86.90%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 87.30%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 85.80%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 50.00%,  total acc: 65.62%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 72.92%   [EVAL] batch:    3 | acc: 56.25%,  total acc: 68.75%   [EVAL] batch:    4 | acc: 62.50%,  total acc: 67.50%   [EVAL] batch:    5 | acc: 68.75%,  total acc: 67.71%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 71.43%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 77.08%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 78.75%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 80.68%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 81.77%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 82.69%   [EVAL] batch:   13 | acc: 75.00%,  total acc: 82.14%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 81.67%   [EVAL] batch:   15 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:   16 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:   17 | acc: 75.00%,  total acc: 80.90%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 81.58%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 81.88%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 82.74%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 83.52%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 84.24%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 84.64%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 85.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 85.82%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 86.11%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 86.61%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 87.07%   [EVAL] batch:   29 | acc: 75.00%,  total acc: 86.67%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 86.90%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 87.30%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 85.80%   
cur_acc:  ['0.8580']
his_acc:  ['0.8580']
CurrentTrain: epoch  0, batch     0 | loss: 5.8424301CurrentTrain: epoch  0, batch     1 | loss: 6.7802753CurrentTrain: epoch  1, batch     0 | loss: 5.5416875CurrentTrain: epoch  1, batch     1 | loss: 5.1429358CurrentTrain: epoch  2, batch     0 | loss: 4.9725399CurrentTrain: epoch  2, batch     1 | loss: 5.2198701CurrentTrain: epoch  3, batch     0 | loss: 5.0250702CurrentTrain: epoch  3, batch     1 | loss: 4.0776119CurrentTrain: epoch  4, batch     0 | loss: 3.9728217CurrentTrain: epoch  4, batch     1 | loss: 4.3750763CurrentTrain: epoch  5, batch     0 | loss: 3.7476020CurrentTrain: epoch  5, batch     1 | loss: 4.0451331CurrentTrain: epoch  6, batch     0 | loss: 3.4468160CurrentTrain: epoch  6, batch     1 | loss: 4.0957627CurrentTrain: epoch  7, batch     0 | loss: 3.3394785CurrentTrain: epoch  7, batch     1 | loss: 3.5227256CurrentTrain: epoch  8, batch     0 | loss: 3.5470490CurrentTrain: epoch  8, batch     1 | loss: 2.8136432CurrentTrain: epoch  9, batch     0 | loss: 2.9029033CurrentTrain: epoch  9, batch     1 | loss: 2.9860859
#### person stateorprovinces of residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person stateorprovinces of residence" means a person is living in a state or province.
Here is an example:
Relation: person stateorprovinces of residence
Context: ny-schools-chief -lrb- new york -rrb- -- cathleen p. black , mayor michael r. bloomberg 's choice to be the next chancellor of new york city 's public-school system , has during more than 40 years in the media business broken numerous glass ceilings -- and amassed a fortune -- with quick and cold-blooded decision making , crystal-clear goal setting , and an all-surpassing attention to the bottom line .
Head Entity: cathleen p. black
Tail Entity: new york
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person stateorprovinces of residence":
Relation: person stateorprovinces of residence  
Context: after years of living in the bustling city, actor john doe has decided to settle down in the serene landscapes of oregon, where he can enjoy a quieter lifestyle away from the spotlight.  
Head Entity: john doe  
Tail Entity: oregon  

Relation: person stateorprovinces of residence  
Context: renowned author jane smith has recently moved to the vibrant city of los angeles, drawn by its rich cultural scene and opportunities for collaboration with other creatives.  
Head Entity: jane smith  
Tail Entity: los angeles  
#### person date of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person date of death" means a person was died on a date.
Here is an example:
Relation: person date of death
Context: he was taken off life support on feb. 14 .
Head Entity: he
Tail Entity: feb. 14
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person date of death":
Relation: person date of death  
Context: The famous author passed away on July 10, 2020, after a long illness.  
Head Entity: The famous author  
Tail Entity: July 10, 2020  

Relation: person date of death  
Context: She left this world on March 5, 2018, surrounded by her family.  
Head Entity: She  
Tail Entity: March 5, 2018  
#### organization number of employees members ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization number of employees members" means the number of an organization 's employees is a number.
Here is an example:
Relation: organization number of employees members
Context: with the sweep of a federal regulator 's pen , massachusetts stands to gain a new life-science giant in april : covidien , a medical - supplies maker with thousands of products and more than 43,000 employees worldwide .
Head Entity: covidien
Tail Entity: 43,000
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization number of employees members":
Relation: organization number of employees members  
Context: The tech company, Innovatech, has rapidly expanded its workforce over the past year, now boasting a total of 25,000 employees across its global offices.  
Head Entity: Innovatech  
Tail Entity: 25,000  

Relation: organization number of employees members  
Context: After the merger, the newly formed entity, Global Finance Corp, reported an impressive headcount of 15,000 employees, making it one of the largest financial institutions in the region.  
Head Entity: Global Finance Corp  
Tail Entity: 15,000  
#### person alternate names ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person alternate names" means a person 's alias is a person.
Here is an example:
Relation: person alternate names
Context: they represent a continuation of the company 's jerome robbins celebration that began in the spring at lincoln center to mark the 10th anniversary of robbins ' death .
Head Entity: jerome robbins
Tail Entity: robbins
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person alternate names":
Relation: person alternate names  
Context: The famous author Samuel Clemens is better known by his pen name, Mark Twain, which he adopted during his writing career.  
Head Entity: Samuel Clemens  
Tail Entity: Mark Twain  

Relation: person alternate names  
Context: The musician known as Prince was born as Prince Rogers Nelson, a name that reflects his heritage and family background.  
Head Entity: Prince Rogers Nelson  
Tail Entity: Prince  
#### person spouse ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person spouse" means a person 's spouse is a person.
Here is an example:
Relation: person spouse
Context: beverly hills , california 2008-08-17 21:15:39 utc ------ there was much dancing : ellen degeneres and portia de rossi are married , according to reports .
Head Entity: ellen degeneres
Tail Entity: portia de rossi
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person spouse":
Relation: person spouse  
Context: in a lavish ceremony held in new york city, the couple exchanged vows in front of family and friends: john legend and chrissy teigen celebrated their love with a beautiful wedding.  
Head Entity: john legend  
Tail Entity: chrissy teigen  

Relation: person spouse  
Context: during the annual charity gala, it was announced that the famous actor and his long-time partner have tied the knot: ben affleck and jennifer garner are now officially married.  
Head Entity: ben affleck  
Tail Entity: jennifer garner  
Mixup data size:  1495
MixupTrain:  epoch  0, batch     0 | loss: 6.8438606MixupTrain:  epoch  0, batch     1 | loss: 6.6218376MixupTrain:  epoch  0, batch     2 | loss: 6.4623833MixupTrain:  epoch  0, batch     3 | loss: 6.3183756MixupTrain:  epoch  0, batch     4 | loss: 6.0796680MixupTrain:  epoch  0, batch     5 | loss: 6.1122680MixupTrain:  epoch  0, batch     6 | loss: 5.8177381MixupTrain:  epoch  0, batch     7 | loss: 5.7202621MixupTrain:  epoch  0, batch     8 | loss: 5.6239805MixupTrain:  epoch  0, batch     9 | loss: 5.5861959MixupTrain:  epoch  0, batch    10 | loss: 5.5411177MixupTrain:  epoch  0, batch    11 | loss: 5.4155984MixupTrain:  epoch  0, batch    12 | loss: 5.5422316MixupTrain:  epoch  0, batch    13 | loss: 5.4420981MixupTrain:  epoch  0, batch    14 | loss: 5.3941650MixupTrain:  epoch  0, batch    15 | loss: 5.3373113MixupTrain:  epoch  0, batch    16 | loss: 5.3215847MixupTrain:  epoch  0, batch    17 | loss: 5.3244247MixupTrain:  epoch  0, batch    18 | loss: 5.2771740MixupTrain:  epoch  0, batch    19 | loss: 5.1975961MixupTrain:  epoch  0, batch    20 | loss: 5.2560854MixupTrain:  epoch  0, batch    21 | loss: 5.2071710MixupTrain:  epoch  0, batch    22 | loss: 5.1662025MixupTrain:  epoch  0, batch    23 | loss: 5.0847235MixupTrain:  epoch  0, batch    24 | loss: 5.0914392MixupTrain:  epoch  0, batch    25 | loss: 5.0957322MixupTrain:  epoch  0, batch    26 | loss: 5.0824041MixupTrain:  epoch  0, batch    27 | loss: 5.0460658MixupTrain:  epoch  0, batch    28 | loss: 5.0081906MixupTrain:  epoch  0, batch    29 | loss: 5.0015616MixupTrain:  epoch  0, batch    30 | loss: 4.9677105MixupTrain:  epoch  0, batch    31 | loss: 4.9346094MixupTrain:  epoch  0, batch    32 | loss: 4.9312553MixupTrain:  epoch  0, batch    33 | loss: 4.8952723MixupTrain:  epoch  0, batch    34 | loss: 4.8891792MixupTrain:  epoch  0, batch    35 | loss: 4.8453679MixupTrain:  epoch  0, batch    36 | loss: 4.8372016MixupTrain:  epoch  0, batch    37 | loss: 4.8335590MixupTrain:  epoch  0, batch    38 | loss: 4.7781143MixupTrain:  epoch  0, batch    39 | loss: 4.7623224MixupTrain:  epoch  0, batch    40 | loss: 4.7350111MixupTrain:  epoch  0, batch    41 | loss: 4.7097149MixupTrain:  epoch  0, batch    42 | loss: 4.6496067MixupTrain:  epoch  0, batch    43 | loss: 4.6854696MixupTrain:  epoch  0, batch    44 | loss: 4.6424780MixupTrain:  epoch  0, batch    45 | loss: 4.6070948MixupTrain:  epoch  0, batch    46 | loss: 4.6118584MixupTrain:  epoch  0, batch    47 | loss: 4.5693779MixupTrain:  epoch  0, batch    48 | loss: 4.5545030MixupTrain:  epoch  0, batch    49 | loss: 4.5553904MixupTrain:  epoch  0, batch    50 | loss: 4.5477228MixupTrain:  epoch  0, batch    51 | loss: 4.5290461MixupTrain:  epoch  0, batch    52 | loss: 4.4809813MixupTrain:  epoch  0, batch    53 | loss: 4.4710131MixupTrain:  epoch  0, batch    54 | loss: 4.4551325MixupTrain:  epoch  0, batch    55 | loss: 4.4599180MixupTrain:  epoch  0, batch    56 | loss: 4.4496393MixupTrain:  epoch  0, batch    57 | loss: 4.4249763MixupTrain:  epoch  0, batch    58 | loss: 4.4018793MixupTrain:  epoch  0, batch    59 | loss: 4.3941598MixupTrain:  epoch  0, batch    60 | loss: 4.3692913MixupTrain:  epoch  0, batch    61 | loss: 4.3739753MixupTrain:  epoch  0, batch    62 | loss: 4.3191032MixupTrain:  epoch  0, batch    63 | loss: 4.3457804MixupTrain:  epoch  0, batch    64 | loss: 4.3150187MixupTrain:  epoch  0, batch    65 | loss: 4.3011589MixupTrain:  epoch  0, batch    66 | loss: 4.2927990MixupTrain:  epoch  0, batch    67 | loss: 4.2840118MixupTrain:  epoch  0, batch    68 | loss: 4.2600193MixupTrain:  epoch  0, batch    69 | loss: 4.2316775MixupTrain:  epoch  0, batch    70 | loss: 4.2282848MixupTrain:  epoch  0, batch    71 | loss: 4.2414889MixupTrain:  epoch  0, batch    72 | loss: 4.2227039MixupTrain:  epoch  0, batch    73 | loss: 4.2185001MixupTrain:  epoch  0, batch    74 | loss: 4.2038193MixupTrain:  epoch  0, batch    75 | loss: 4.1901574MixupTrain:  epoch  0, batch    76 | loss: 4.1691599MixupTrain:  epoch  0, batch    77 | loss: 4.1706977MixupTrain:  epoch  0, batch    78 | loss: 4.1698074MixupTrain:  epoch  0, batch    79 | loss: 4.1529293MixupTrain:  epoch  0, batch    80 | loss: 4.1519103MixupTrain:  epoch  0, batch    81 | loss: 4.1322632MixupTrain:  epoch  0, batch    82 | loss: 4.1490412MixupTrain:  epoch  0, batch    83 | loss: 4.1309366MixupTrain:  epoch  0, batch    84 | loss: 4.1238565MixupTrain:  epoch  0, batch    85 | loss: 4.1201992MixupTrain:  epoch  0, batch    86 | loss: 4.1210475MixupTrain:  epoch  0, batch    87 | loss: 4.1044168MixupTrain:  epoch  0, batch    88 | loss: 4.1027951MixupTrain:  epoch  0, batch    89 | loss: 4.1060905MixupTrain:  epoch  0, batch    90 | loss: 4.0781331MixupTrain:  epoch  0, batch    91 | loss: 4.0797973MixupTrain:  epoch  0, batch    92 | loss: 4.0806608MixupTrain:  epoch  0, batch    93 | loss: 4.0486426
MemoryTrain:  epoch  0, batch     0 | loss: 7.9075079MemoryTrain:  epoch  0, batch     1 | loss: 7.2956376MemoryTrain:  epoch  0, batch     2 | loss: 4.8700557MemoryTrain:  epoch  1, batch     0 | loss: 6.5443840MemoryTrain:  epoch  1, batch     1 | loss: 5.3283024MemoryTrain:  epoch  1, batch     2 | loss: 4.9426608MemoryTrain:  epoch  2, batch     0 | loss: 3.9041791MemoryTrain:  epoch  2, batch     1 | loss: 3.7832978MemoryTrain:  epoch  2, batch     2 | loss: 4.8279533MemoryTrain:  epoch  3, batch     0 | loss: 2.9338720MemoryTrain:  epoch  3, batch     1 | loss: 3.2457402MemoryTrain:  epoch  3, batch     2 | loss: 1.6406070MemoryTrain:  epoch  4, batch     0 | loss: 2.4333823MemoryTrain:  epoch  4, batch     1 | loss: 3.0687201MemoryTrain:  epoch  4, batch     2 | loss: 1.3587127MemoryTrain:  epoch  5, batch     0 | loss: 3.0350761MemoryTrain:  epoch  5, batch     1 | loss: 2.0180306MemoryTrain:  epoch  5, batch     2 | loss: 1.2990534MemoryTrain:  epoch  6, batch     0 | loss: 1.8731672MemoryTrain:  epoch  6, batch     1 | loss: 2.4276478MemoryTrain:  epoch  6, batch     2 | loss: 1.7996234MemoryTrain:  epoch  7, batch     0 | loss: 2.0046241MemoryTrain:  epoch  7, batch     1 | loss: 2.1389420MemoryTrain:  epoch  7, batch     2 | loss: 3.4796154MemoryTrain:  epoch  8, batch     0 | loss: 1.7900155MemoryTrain:  epoch  8, batch     1 | loss: 2.1457539MemoryTrain:  epoch  8, batch     2 | loss: 1.8504771MemoryTrain:  epoch  9, batch     0 | loss: 1.6232553MemoryTrain:  epoch  9, batch     1 | loss: 2.0702217MemoryTrain:  epoch  9, batch     2 | loss: 2.8417816
[EVAL] batch:    0 | acc: 31.25%,  total acc: 31.25%   [EVAL] batch:    1 | acc: 37.50%,  total acc: 34.38%   [EVAL] batch:    2 | acc: 43.75%,  total acc: 37.50%   [EVAL] batch:    3 | acc: 56.25%,  total acc: 42.19%   [EVAL] batch:    4 | acc: 31.25%,  total acc: 40.00%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 48.96%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 56.25%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 60.94%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 63.89%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 64.38%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 65.91%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 68.75%   [EVAL] batch:   12 | acc: 100.00%,  total acc: 71.15%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 72.77%   [EVAL] batch:   14 | acc: 43.75%,  total acc: 70.83%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 68.75%,  total acc: 75.00%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 73.44%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 75.00%   [EVAL] batch:    5 | acc: 56.25%,  total acc: 71.88%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 75.00%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 78.12%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 80.56%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 81.88%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 83.52%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 83.85%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 84.13%   [EVAL] batch:   13 | acc: 75.00%,  total acc: 83.48%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 82.92%   [EVAL] batch:   15 | acc: 62.50%,  total acc: 81.64%   [EVAL] batch:   16 | acc: 81.25%,  total acc: 81.62%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 80.56%   [EVAL] batch:   18 | acc: 81.25%,  total acc: 80.59%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 80.94%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 81.85%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 82.67%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 83.42%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 83.85%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 84.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 85.10%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 85.42%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 85.94%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 86.42%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 86.46%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 86.90%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 87.30%   [EVAL] batch:   32 | acc: 62.50%,  total acc: 86.55%   [EVAL] batch:   33 | acc: 25.00%,  total acc: 84.74%   [EVAL] batch:   34 | acc: 43.75%,  total acc: 83.57%   [EVAL] batch:   35 | acc: 50.00%,  total acc: 82.64%   [EVAL] batch:   36 | acc: 37.50%,  total acc: 81.42%   [EVAL] batch:   37 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 81.73%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 82.19%   [EVAL] batch:   40 | acc: 87.50%,  total acc: 82.32%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 82.44%   [EVAL] batch:   42 | acc: 56.25%,  total acc: 81.83%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 82.24%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 82.64%   [EVAL] batch:   45 | acc: 93.75%,  total acc: 82.88%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 82.85%   
cur_acc:  ['0.8580', '0.7083']
his_acc:  ['0.8580', '0.8285']
CurrentTrain: epoch  0, batch     0 | loss: 6.8481541CurrentTrain: epoch  0, batch     1 | loss: 6.8309007CurrentTrain: epoch  1, batch     0 | loss: 6.2745991CurrentTrain: epoch  1, batch     1 | loss: 4.6922474CurrentTrain: epoch  2, batch     0 | loss: 5.3492169CurrentTrain: epoch  2, batch     1 | loss: 5.0344391CurrentTrain: epoch  3, batch     0 | loss: 4.8750010CurrentTrain: epoch  3, batch     1 | loss: 4.0351734CurrentTrain: epoch  4, batch     0 | loss: 3.9948978CurrentTrain: epoch  4, batch     1 | loss: 4.1673484CurrentTrain: epoch  5, batch     0 | loss: 3.8425226CurrentTrain: epoch  5, batch     1 | loss: 3.7294769CurrentTrain: epoch  6, batch     0 | loss: 3.6108470CurrentTrain: epoch  6, batch     1 | loss: 3.1506016CurrentTrain: epoch  7, batch     0 | loss: 3.0061827CurrentTrain: epoch  7, batch     1 | loss: 3.0022135CurrentTrain: epoch  8, batch     0 | loss: 2.8245342CurrentTrain: epoch  8, batch     1 | loss: 2.6344960CurrentTrain: epoch  9, batch     0 | loss: 2.7602377CurrentTrain: epoch  9, batch     1 | loss: 2.7181833
#### person date of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person date of birth" means a person was born in a date.
Here is an example:
Relation: person date of birth
Context: kirkaldy , born irene morgan in baltimore , maryland , in 1917 , was arrested in 1944 for refusing to give up her seat on a greyhound bus heading from gloucester to baltimore , and for resisting arrest .
Head Entity: irene morgan
Tail Entity: 1917
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person date of birth":
Relation: person date of birth  
Context: the famous physicist albert einstein was born in ulm, in the kingdom of wurttemberg in the german empire, on march 14, 1879.  
Head Entity: albert einstein  
Tail Entity: march 14, 1879  

Relation: person date of birth  
Context: the renowned author mark twain, whose real name was samuel langhorne clemens, was born on november 30, 1835, in florida, missouri.  
Head Entity: mark twain  
Tail Entity: november 30, 1835  
#### person stateorprovince of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person stateorprovince of birth" means a person was died in a state or province.
Here is an example:
Relation: person stateorprovince of birth
Context: jane matilda bolin was born on april 11 , 1908 , in poughkeepsie , ny .
Head Entity: jane matilda bolin
Tail Entity: ny
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person stateorprovince of birth":
Relation: person stateorprovince of birth  
Context: albert einstein was born on march 14, 1879, in ulm, baden-württemberg, germany.  
Head Entity: albert einstein  
Tail Entity: baden-württemberg  

Relation: person stateorprovince of birth  
Context: marilyn monroe was born on june 1, 1926, in los angeles, california.  
Head Entity: marilyn monroe  
Tail Entity: california  
#### person parents ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person parents" means a person 's parent is a person.
Here is an example:
Relation: person parents
Context: lynne spears told the court that lutfi had treated her daughter like a hostage in her own home , drugged her and took over her finances .
Head Entity: her
Tail Entity: lynne spears
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person parents":
Relation: person parents  
Context: During the family reunion, Sarah introduced her father, John, to her friends, highlighting how much he has influenced her life.  
Head Entity: her father  
Tail Entity: John  

Relation: person parents  
Context: Emily often shares stories about her mother, who has always been her biggest supporter and role model throughout her life.  
Head Entity: her mother  
Tail Entity: Emily
#### person employee of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person employee of" means a person is an employee of an organization.
Here is an example:
Relation: person employee of
Context: kell hath no fury : publicist and mtv reality star kelly cutrone is wasting no time in kicking her brands -lrb- including her p.r. firm people 's revolution and , increasingly , kelly cutrone herself -rrb- into high gear in 2010 .
Head Entity: kelly cutrone
Tail Entity: mtv
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person employee of":
Relation: person employee of  
Context: After years of hard work and dedication, Sarah Thompson has finally landed a position at one of the top tech companies in Silicon Valley, where she will be contributing to innovative projects.  
Head Entity: Sarah Thompson  
Tail Entity: tech company  

Relation: person employee of  
Context: John Smith, a talented graphic designer, has been working for Creative Solutions for over five years, helping to shape the visual identity of numerous brands.  
Head Entity: John Smith  
Tail Entity: Creative Solutions  
#### person stateorprovince of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person stateorprovince of death" means a person was died in a state or province.
Here is an example:
Relation: person stateorprovince of death
Context: herrera died monday of age-related causes at his home in glendale , ariz. , said mona kempfer , his youngest daughter .
Head Entity: herrera
Tail Entity: ariz.
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person stateorprovince of death":
Relation: person stateorprovince of death  
Context: after a long battle with illness, johnson passed away peacefully in his sleep at his residence in springfield, illinois, surrounded by family.  
Head Entity: johnson  
Tail Entity: illinois  

Relation: person stateorprovince of death  
Context: the renowned author, who had lived in new york for many years, succumbed to his illness last week in a hospital in albany, new york.  
Head Entity: the renowned author  
Tail Entity: new york  
Mixup data size:  2455
MixupTrain:  epoch  0, batch     0 | loss: 5.6942496MixupTrain:  epoch  0, batch     1 | loss: 5.5415325MixupTrain:  epoch  0, batch     2 | loss: 5.4990101MixupTrain:  epoch  0, batch     3 | loss: 4.8716965MixupTrain:  epoch  0, batch     4 | loss: 5.0674887MixupTrain:  epoch  0, batch     5 | loss: 5.1420498MixupTrain:  epoch  0, batch     6 | loss: 4.6139293MixupTrain:  epoch  0, batch     7 | loss: 4.8405509MixupTrain:  epoch  0, batch     8 | loss: 4.7624731MixupTrain:  epoch  0, batch     9 | loss: 5.1130280MixupTrain:  epoch  0, batch    10 | loss: 5.2409177MixupTrain:  epoch  0, batch    11 | loss: 4.6536098MixupTrain:  epoch  0, batch    12 | loss: 5.1901708MixupTrain:  epoch  0, batch    13 | loss: 4.6979070MixupTrain:  epoch  0, batch    14 | loss: 4.9861007MixupTrain:  epoch  0, batch    15 | loss: 4.7490344MixupTrain:  epoch  0, batch    16 | loss: 4.8729062MixupTrain:  epoch  0, batch    17 | loss: 4.8813801MixupTrain:  epoch  0, batch    18 | loss: 4.6855788MixupTrain:  epoch  0, batch    19 | loss: 4.7258282MixupTrain:  epoch  0, batch    20 | loss: 4.6416235MixupTrain:  epoch  0, batch    21 | loss: 4.8659792MixupTrain:  epoch  0, batch    22 | loss: 4.4517813MixupTrain:  epoch  0, batch    23 | loss: 4.4846449MixupTrain:  epoch  0, batch    24 | loss: 4.3877420MixupTrain:  epoch  0, batch    25 | loss: 4.5976553MixupTrain:  epoch  0, batch    26 | loss: 4.4948473MixupTrain:  epoch  0, batch    27 | loss: 4.7046666MixupTrain:  epoch  0, batch    28 | loss: 4.2308283MixupTrain:  epoch  0, batch    29 | loss: 4.3993373MixupTrain:  epoch  0, batch    30 | loss: 4.0544062MixupTrain:  epoch  0, batch    31 | loss: 4.3812809MixupTrain:  epoch  0, batch    32 | loss: 4.5240211MixupTrain:  epoch  0, batch    33 | loss: 4.1787252MixupTrain:  epoch  0, batch    34 | loss: 4.4241037MixupTrain:  epoch  0, batch    35 | loss: 4.0641441MixupTrain:  epoch  0, batch    36 | loss: 4.6929102MixupTrain:  epoch  0, batch    37 | loss: 4.7999897MixupTrain:  epoch  0, batch    38 | loss: 4.4781728MixupTrain:  epoch  0, batch    39 | loss: 4.3700953MixupTrain:  epoch  0, batch    40 | loss: 4.2410307MixupTrain:  epoch  0, batch    41 | loss: 4.1211982MixupTrain:  epoch  0, batch    42 | loss: 4.4704771MixupTrain:  epoch  0, batch    43 | loss: 4.2154689MixupTrain:  epoch  0, batch    44 | loss: 4.0565877MixupTrain:  epoch  0, batch    45 | loss: 4.2695208MixupTrain:  epoch  0, batch    46 | loss: 4.3780718MixupTrain:  epoch  0, batch    47 | loss: 3.8952367MixupTrain:  epoch  0, batch    48 | loss: 4.3484945MixupTrain:  epoch  0, batch    49 | loss: 4.2489552MixupTrain:  epoch  0, batch    50 | loss: 4.1513853MixupTrain:  epoch  0, batch    51 | loss: 4.4631901MixupTrain:  epoch  0, batch    52 | loss: 3.9912798MixupTrain:  epoch  0, batch    53 | loss: 4.2739987MixupTrain:  epoch  0, batch    54 | loss: 4.1747050MixupTrain:  epoch  0, batch    55 | loss: 4.0281272MixupTrain:  epoch  0, batch    56 | loss: 4.2758136MixupTrain:  epoch  0, batch    57 | loss: 4.2686977MixupTrain:  epoch  0, batch    58 | loss: 4.1540513MixupTrain:  epoch  0, batch    59 | loss: 4.5777159MixupTrain:  epoch  0, batch    60 | loss: 4.3882713MixupTrain:  epoch  0, batch    61 | loss: 4.1533809MixupTrain:  epoch  0, batch    62 | loss: 4.0898933MixupTrain:  epoch  0, batch    63 | loss: 4.2872162MixupTrain:  epoch  0, batch    64 | loss: 3.9852154MixupTrain:  epoch  0, batch    65 | loss: 4.1934805MixupTrain:  epoch  0, batch    66 | loss: 4.0349088MixupTrain:  epoch  0, batch    67 | loss: 3.9821961MixupTrain:  epoch  0, batch    68 | loss: 3.9931395MixupTrain:  epoch  0, batch    69 | loss: 4.1132574MixupTrain:  epoch  0, batch    70 | loss: 4.0000582MixupTrain:  epoch  0, batch    71 | loss: 4.2029791MixupTrain:  epoch  0, batch    72 | loss: 4.0433602MixupTrain:  epoch  0, batch    73 | loss: 4.0557184MixupTrain:  epoch  0, batch    74 | loss: 4.0266881MixupTrain:  epoch  0, batch    75 | loss: 3.7804289MixupTrain:  epoch  0, batch    76 | loss: 4.0599165MixupTrain:  epoch  0, batch    77 | loss: 3.7859085MixupTrain:  epoch  0, batch    78 | loss: 3.9106631MixupTrain:  epoch  0, batch    79 | loss: 4.0613670MixupTrain:  epoch  0, batch    80 | loss: 4.0948558MixupTrain:  epoch  0, batch    81 | loss: 3.7550681MixupTrain:  epoch  0, batch    82 | loss: 4.2594161MixupTrain:  epoch  0, batch    83 | loss: 3.9707234MixupTrain:  epoch  0, batch    84 | loss: 4.2703090MixupTrain:  epoch  0, batch    85 | loss: 4.1306319MixupTrain:  epoch  0, batch    86 | loss: 3.8844304MixupTrain:  epoch  0, batch    87 | loss: 3.7118356MixupTrain:  epoch  0, batch    88 | loss: 4.1468487MixupTrain:  epoch  0, batch    89 | loss: 3.8704014MixupTrain:  epoch  0, batch    90 | loss: 3.9111536MixupTrain:  epoch  0, batch    91 | loss: 3.9704962MixupTrain:  epoch  0, batch    92 | loss: 4.0463467MixupTrain:  epoch  0, batch    93 | loss: 3.9402366MixupTrain:  epoch  0, batch    94 | loss: 4.0011473MixupTrain:  epoch  0, batch    95 | loss: 3.9929662MixupTrain:  epoch  0, batch    96 | loss: 3.6346798MixupTrain:  epoch  0, batch    97 | loss: 3.9061623MixupTrain:  epoch  0, batch    98 | loss: 4.0570583MixupTrain:  epoch  0, batch    99 | loss: 4.1131516MixupTrain:  epoch  0, batch   100 | loss: 3.8140621MixupTrain:  epoch  0, batch   101 | loss: 3.7668753MixupTrain:  epoch  0, batch   102 | loss: 4.0921340MixupTrain:  epoch  0, batch   103 | loss: 3.8112602MixupTrain:  epoch  0, batch   104 | loss: 3.8042693MixupTrain:  epoch  0, batch   105 | loss: 3.8054016MixupTrain:  epoch  0, batch   106 | loss: 4.0070796MixupTrain:  epoch  0, batch   107 | loss: 3.8991530MixupTrain:  epoch  0, batch   108 | loss: 3.8998926MixupTrain:  epoch  0, batch   109 | loss: 3.7342892MixupTrain:  epoch  0, batch   110 | loss: 3.5373232MixupTrain:  epoch  0, batch   111 | loss: 3.7585611MixupTrain:  epoch  0, batch   112 | loss: 3.8610525MixupTrain:  epoch  0, batch   113 | loss: 3.9072847MixupTrain:  epoch  0, batch   114 | loss: 3.8874552MixupTrain:  epoch  0, batch   115 | loss: 3.9618223MixupTrain:  epoch  0, batch   116 | loss: 3.7466021MixupTrain:  epoch  0, batch   117 | loss: 3.8589272MixupTrain:  epoch  0, batch   118 | loss: 3.6259880MixupTrain:  epoch  0, batch   119 | loss: 3.5783677MixupTrain:  epoch  0, batch   120 | loss: 3.9764786MixupTrain:  epoch  0, batch   121 | loss: 3.9072204MixupTrain:  epoch  0, batch   122 | loss: 4.0376358MixupTrain:  epoch  0, batch   123 | loss: 3.7681932MixupTrain:  epoch  0, batch   124 | loss: 3.7766182MixupTrain:  epoch  0, batch   125 | loss: 3.8272676MixupTrain:  epoch  0, batch   126 | loss: 3.8438115MixupTrain:  epoch  0, batch   127 | loss: 3.8385704MixupTrain:  epoch  0, batch   128 | loss: 3.5877583MixupTrain:  epoch  0, batch   129 | loss: 3.7809389MixupTrain:  epoch  0, batch   130 | loss: 3.9590540MixupTrain:  epoch  0, batch   131 | loss: 3.7052662MixupTrain:  epoch  0, batch   132 | loss: 3.5818906MixupTrain:  epoch  0, batch   133 | loss: 3.7177896MixupTrain:  epoch  0, batch   134 | loss: 3.8418841MixupTrain:  epoch  0, batch   135 | loss: 3.7825699MixupTrain:  epoch  0, batch   136 | loss: 3.6119545MixupTrain:  epoch  0, batch   137 | loss: 3.9297333MixupTrain:  epoch  0, batch   138 | loss: 3.8420763MixupTrain:  epoch  0, batch   139 | loss: 3.5972378MixupTrain:  epoch  0, batch   140 | loss: 3.7158427MixupTrain:  epoch  0, batch   141 | loss: 3.7800112MixupTrain:  epoch  0, batch   142 | loss: 3.8326421MixupTrain:  epoch  0, batch   143 | loss: 3.8571424MixupTrain:  epoch  0, batch   144 | loss: 3.6892948MixupTrain:  epoch  0, batch   145 | loss: 3.6209188MixupTrain:  epoch  0, batch   146 | loss: 3.8104167MixupTrain:  epoch  0, batch   147 | loss: 3.5798249MixupTrain:  epoch  0, batch   148 | loss: 3.8838677MixupTrain:  epoch  0, batch   149 | loss: 3.6852655MixupTrain:  epoch  0, batch   150 | loss: 3.8979201MixupTrain:  epoch  0, batch   151 | loss: 3.5203278MixupTrain:  epoch  0, batch   152 | loss: 3.6283734MixupTrain:  epoch  0, batch   153 | loss: 3.8989549
MemoryTrain:  epoch  0, batch     0 | loss: 1.9837122MemoryTrain:  epoch  0, batch     1 | loss: 2.7793736MemoryTrain:  epoch  0, batch     2 | loss: 2.6625659MemoryTrain:  epoch  1, batch     0 | loss: 1.7341217MemoryTrain:  epoch  1, batch     1 | loss: 2.2400715MemoryTrain:  epoch  1, batch     2 | loss: 1.6692131MemoryTrain:  epoch  2, batch     0 | loss: 1.5895138MemoryTrain:  epoch  2, batch     1 | loss: 1.6566907MemoryTrain:  epoch  2, batch     2 | loss: 1.4808463MemoryTrain:  epoch  3, batch     0 | loss: 1.5996368MemoryTrain:  epoch  3, batch     1 | loss: 1.8007650MemoryTrain:  epoch  3, batch     2 | loss: 1.4487973MemoryTrain:  epoch  4, batch     0 | loss: 1.6189129MemoryTrain:  epoch  4, batch     1 | loss: 1.6230130MemoryTrain:  epoch  4, batch     2 | loss: 1.4725304MemoryTrain:  epoch  5, batch     0 | loss: 1.5884466MemoryTrain:  epoch  5, batch     1 | loss: 1.4523585MemoryTrain:  epoch  5, batch     2 | loss: 1.3570924MemoryTrain:  epoch  6, batch     0 | loss: 1.5145338MemoryTrain:  epoch  6, batch     1 | loss: 1.2956640MemoryTrain:  epoch  6, batch     2 | loss: 1.4127650MemoryTrain:  epoch  7, batch     0 | loss: 1.4866921MemoryTrain:  epoch  7, batch     1 | loss: 1.3825338MemoryTrain:  epoch  7, batch     2 | loss: 1.2493858MemoryTrain:  epoch  8, batch     0 | loss: 1.4071929MemoryTrain:  epoch  8, batch     1 | loss: 1.3857518MemoryTrain:  epoch  8, batch     2 | loss: 1.2917583MemoryTrain:  epoch  9, batch     0 | loss: 1.2802099MemoryTrain:  epoch  9, batch     1 | loss: 1.3605164MemoryTrain:  epoch  9, batch     2 | loss: 1.3767955
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 56.25%,  total acc: 68.75%   [EVAL] batch:    2 | acc: 56.25%,  total acc: 64.58%   [EVAL] batch:    3 | acc: 50.00%,  total acc: 60.94%   [EVAL] batch:    4 | acc: 25.00%,  total acc: 53.75%   [EVAL] batch:    5 | acc: 68.75%,  total acc: 56.25%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 58.04%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 62.50%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 65.97%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 68.12%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 69.32%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 71.35%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 73.08%   [EVAL] batch:   13 | acc: 62.50%,  total acc: 72.32%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 75.00%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 77.08%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 75.00%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 77.50%   [EVAL] batch:    5 | acc: 62.50%,  total acc: 75.00%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 77.68%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 80.47%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 82.64%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 83.75%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 85.23%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 85.42%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 85.58%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 84.38%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 83.75%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 82.03%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 81.62%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 80.56%   [EVAL] batch:   18 | acc: 81.25%,  total acc: 80.59%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 80.94%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 81.85%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 82.39%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 83.15%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 83.59%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 84.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 84.86%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 85.19%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 85.71%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 86.21%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 86.04%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 86.49%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 86.91%   [EVAL] batch:   32 | acc: 56.25%,  total acc: 85.98%   [EVAL] batch:   33 | acc: 12.50%,  total acc: 83.82%   [EVAL] batch:   34 | acc: 18.75%,  total acc: 81.96%   [EVAL] batch:   35 | acc: 12.50%,  total acc: 80.03%   [EVAL] batch:   36 | acc: 37.50%,  total acc: 78.89%   [EVAL] batch:   37 | acc: 75.00%,  total acc: 78.78%   [EVAL] batch:   38 | acc: 75.00%,  total acc: 78.69%   [EVAL] batch:   39 | acc: 93.75%,  total acc: 79.06%   [EVAL] batch:   40 | acc: 75.00%,  total acc: 78.96%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 79.17%   [EVAL] batch:   42 | acc: 43.75%,  total acc: 78.34%   [EVAL] batch:   43 | acc: 43.75%,  total acc: 77.56%   [EVAL] batch:   44 | acc: 68.75%,  total acc: 77.36%   [EVAL] batch:   45 | acc: 37.50%,  total acc: 76.49%   [EVAL] batch:   46 | acc: 62.50%,  total acc: 76.20%   [EVAL] batch:   47 | acc: 68.75%,  total acc: 76.04%   [EVAL] batch:   48 | acc: 68.75%,  total acc: 75.89%   [EVAL] batch:   49 | acc: 50.00%,  total acc: 75.38%   [EVAL] batch:   50 | acc: 37.50%,  total acc: 74.63%   [EVAL] batch:   51 | acc: 43.75%,  total acc: 74.04%   [EVAL] batch:   52 | acc: 62.50%,  total acc: 73.82%   [EVAL] batch:   53 | acc: 75.00%,  total acc: 73.84%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 74.20%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 74.55%   [EVAL] batch:   56 | acc: 87.50%,  total acc: 74.78%   [EVAL] batch:   57 | acc: 75.00%,  total acc: 74.78%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 75.21%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 75.52%   [EVAL] batch:   60 | acc: 43.75%,  total acc: 75.00%   
cur_acc:  ['0.8580', '0.7083', '0.7232']
his_acc:  ['0.8580', '0.8285', '0.7500']
CurrentTrain: epoch  0, batch     0 | loss: 5.0321479CurrentTrain: epoch  0, batch     1 | loss: 5.9463716CurrentTrain: epoch  1, batch     0 | loss: 4.7636185CurrentTrain: epoch  1, batch     1 | loss: 4.5841722CurrentTrain: epoch  2, batch     0 | loss: 4.6622319CurrentTrain: epoch  2, batch     1 | loss: 3.5938344CurrentTrain: epoch  3, batch     0 | loss: 3.6105828CurrentTrain: epoch  3, batch     1 | loss: 3.7369936CurrentTrain: epoch  4, batch     0 | loss: 3.2432923CurrentTrain: epoch  4, batch     1 | loss: 3.0729592CurrentTrain: epoch  5, batch     0 | loss: 2.9903789CurrentTrain: epoch  5, batch     1 | loss: 2.8150511CurrentTrain: epoch  6, batch     0 | loss: 2.7604222CurrentTrain: epoch  6, batch     1 | loss: 2.4810829CurrentTrain: epoch  7, batch     0 | loss: 2.5348954CurrentTrain: epoch  7, batch     1 | loss: 2.7508805CurrentTrain: epoch  8, batch     0 | loss: 2.5619860CurrentTrain: epoch  8, batch     1 | loss: 2.4700985CurrentTrain: epoch  9, batch     0 | loss: 2.2603579CurrentTrain: epoch  9, batch     1 | loss: 2.3293169
#### person cities of residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person cities of residence" means a person is living in a city.
Here is an example:
Relation: person cities of residence
Context: -lrb- now there are two : andre carson , a democrat from indianapolis , was elected to the house last year . -rrb-
Head Entity: andre carson
Tail Entity: indianapolis
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person cities of residence":
Relation: person cities of residence  
Context: -lrb- after moving to the west coast, sarah jones found her new home in san francisco, where she works as a software engineer. -rrb-  
Head Entity: sarah jones  
Tail Entity: san francisco  

Relation: person cities of residence  
Context: -lrb- during his college years, michael smith spent a lot of time in boston, which he now considers his second home. -rrb-  
Head Entity: michael smith  
Tail Entity: boston  
#### person schools attended ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person schools attended" means a person 's school is an organization.
Here is an example:
Relation: person schools attended
Context: after world war ii , he attended the university of southern california , where he became editor of a college magazine .
Head Entity: he
Tail Entity: university of southern california
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person schools attended":
Relation: person schools attended  
Context: She graduated from Harvard University with a degree in psychology before pursuing her career in clinical research.  
Head Entity: She  
Tail Entity: Harvard University  

Relation: person schools attended  
Context: After completing his high school education, he enrolled at Stanford University to study computer science.  
Head Entity: he  
Tail Entity: Stanford University  
#### person country of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person country of death" means a person was died in a country.
Here is an example:
Relation: person country of death
Context: us republican congresswoman jo ann davis dies after fight with breast cancer
Head Entity: jo ann davis
Tail Entity: us
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person country of death":
Relation: person country of death  
Context: renowned physicist stephen hawking passed away in cambridge, england  
Head Entity: stephen hawking  
Tail Entity: england  

Relation: person country of death  
Context: famous author agatha christie died in her home in wallingford, england  
Head Entity: agatha christie  
Tail Entity: england  
#### person children ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person children" means a person 's children is a person.
Here is an example:
Relation: person children
Context: he is survived by two stepdaughters , barbara a. hammond and brenda l. stevenson ; a stepson , michael a. taylor ; two grandchildren and one great-grandchild .
Head Entity: he
Tail Entity: brenda l. stevenson
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person children":
Relation: person children  
Context: she has three children, including her son, john, and her daughters, emily and sarah, who are both pursuing their studies.  
Head Entity: she  
Tail Entity: emily  

Relation: person children  
Context: after the passing of their father, the siblings, including jake and his sister, lila, took on the responsibility of caring for their younger brother, max.  
Head Entity: jake  
Tail Entity: lila  
#### person charges ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person charges" means a person was charged with an event.
Here is an example:
Relation: person charges
Context: ferrara said he was innocent of limoli 's slaying , but he pleaded guilty in 1992 to murder , along with racketeering charges , under a deal that sent him to prison for 22 years , rather than go to trial and risk a conviction that could lead to life in prison .
Head Entity: ferrara
Tail Entity: racketeering
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person charges":
Relation: person charges  
Context: After a lengthy investigation, the authorities announced that Johnson was charged with embezzlement, a crime that could result in significant prison time if convicted.  
Head Entity: Johnson  
Tail Entity: embezzlement  

Relation: person charges  
Context: The district attorney confirmed that Smith was charged with assault following the altercation that took place outside the nightclub last weekend.  
Head Entity: Smith  
Tail Entity: assault  
Mixup data size:  3640
MixupTrain:  epoch  0, batch     0 | loss: 4.2641764MixupTrain:  epoch  0, batch     1 | loss: 4.3594079MixupTrain:  epoch  0, batch     2 | loss: 4.0860167MixupTrain:  epoch  0, batch     3 | loss: 4.3212547MixupTrain:  epoch  0, batch     4 | loss: 4.2482262MixupTrain:  epoch  0, batch     5 | loss: 4.1221070MixupTrain:  epoch  0, batch     6 | loss: 3.9251714MixupTrain:  epoch  0, batch     7 | loss: 4.3083162MixupTrain:  epoch  0, batch     8 | loss: 3.8954768MixupTrain:  epoch  0, batch     9 | loss: 3.7061646MixupTrain:  epoch  0, batch    10 | loss: 3.6334605MixupTrain:  epoch  0, batch    11 | loss: 3.7362633MixupTrain:  epoch  0, batch    12 | loss: 3.6265295MixupTrain:  epoch  0, batch    13 | loss: 3.6108093MixupTrain:  epoch  0, batch    14 | loss: 4.1714134MixupTrain:  epoch  0, batch    15 | loss: 3.8305273MixupTrain:  epoch  0, batch    16 | loss: 3.6236551MixupTrain:  epoch  0, batch    17 | loss: 4.0011215MixupTrain:  epoch  0, batch    18 | loss: 3.7247224MixupTrain:  epoch  0, batch    19 | loss: 3.7043297MixupTrain:  epoch  0, batch    20 | loss: 3.7257249MixupTrain:  epoch  0, batch    21 | loss: 3.6654615MixupTrain:  epoch  0, batch    22 | loss: 3.7398548MixupTrain:  epoch  0, batch    23 | loss: 3.4889698MixupTrain:  epoch  0, batch    24 | loss: 3.6935258MixupTrain:  epoch  0, batch    25 | loss: 3.8773637MixupTrain:  epoch  0, batch    26 | loss: 3.5554547MixupTrain:  epoch  0, batch    27 | loss: 3.6754565MixupTrain:  epoch  0, batch    28 | loss: 3.2282000MixupTrain:  epoch  0, batch    29 | loss: 3.8126354MixupTrain:  epoch  0, batch    30 | loss: 3.6302094MixupTrain:  epoch  0, batch    31 | loss: 3.6351576MixupTrain:  epoch  0, batch    32 | loss: 3.5744548MixupTrain:  epoch  0, batch    33 | loss: 3.3770614MixupTrain:  epoch  0, batch    34 | loss: 3.4708977MixupTrain:  epoch  0, batch    35 | loss: 3.5234334MixupTrain:  epoch  0, batch    36 | loss: 3.6188407MixupTrain:  epoch  0, batch    37 | loss: 3.3919492MixupTrain:  epoch  0, batch    38 | loss: 3.5106595MixupTrain:  epoch  0, batch    39 | loss: 3.6346190MixupTrain:  epoch  0, batch    40 | loss: 3.3348079MixupTrain:  epoch  0, batch    41 | loss: 3.6360161MixupTrain:  epoch  0, batch    42 | loss: 3.1763418MixupTrain:  epoch  0, batch    43 | loss: 3.5729556MixupTrain:  epoch  0, batch    44 | loss: 3.4836237MixupTrain:  epoch  0, batch    45 | loss: 3.2014060MixupTrain:  epoch  0, batch    46 | loss: 3.4574442MixupTrain:  epoch  0, batch    47 | loss: 3.4879785MixupTrain:  epoch  0, batch    48 | loss: 3.6066833MixupTrain:  epoch  0, batch    49 | loss: 3.0162222MixupTrain:  epoch  0, batch    50 | loss: 3.4781408MixupTrain:  epoch  0, batch    51 | loss: 3.3089514MixupTrain:  epoch  0, batch    52 | loss: 3.6116476MixupTrain:  epoch  0, batch    53 | loss: 3.3322034MixupTrain:  epoch  0, batch    54 | loss: 3.4352226MixupTrain:  epoch  0, batch    55 | loss: 3.6696210MixupTrain:  epoch  0, batch    56 | loss: 3.5220642MixupTrain:  epoch  0, batch    57 | loss: 3.2601399MixupTrain:  epoch  0, batch    58 | loss: 3.6073599MixupTrain:  epoch  0, batch    59 | loss: 3.2371817MixupTrain:  epoch  0, batch    60 | loss: 3.3075261MixupTrain:  epoch  0, batch    61 | loss: 3.2046113MixupTrain:  epoch  0, batch    62 | loss: 3.2110453MixupTrain:  epoch  0, batch    63 | loss: 3.1968393MixupTrain:  epoch  0, batch    64 | loss: 3.4883366MixupTrain:  epoch  0, batch    65 | loss: 3.2842348MixupTrain:  epoch  0, batch    66 | loss: 3.4331663MixupTrain:  epoch  0, batch    67 | loss: 3.0896654MixupTrain:  epoch  0, batch    68 | loss: 3.5951095MixupTrain:  epoch  0, batch    69 | loss: 3.1205571MixupTrain:  epoch  0, batch    70 | loss: 3.2837591MixupTrain:  epoch  0, batch    71 | loss: 3.4749610MixupTrain:  epoch  0, batch    72 | loss: 2.9786992MixupTrain:  epoch  0, batch    73 | loss: 3.2983704MixupTrain:  epoch  0, batch    74 | loss: 3.3377759MixupTrain:  epoch  0, batch    75 | loss: 3.4899340MixupTrain:  epoch  0, batch    76 | loss: 3.3728673MixupTrain:  epoch  0, batch    77 | loss: 3.1161618MixupTrain:  epoch  0, batch    78 | loss: 3.2002296MixupTrain:  epoch  0, batch    79 | loss: 3.3461015MixupTrain:  epoch  0, batch    80 | loss: 3.2006457MixupTrain:  epoch  0, batch    81 | loss: 3.2138984MixupTrain:  epoch  0, batch    82 | loss: 3.3257856MixupTrain:  epoch  0, batch    83 | loss: 3.4899919MixupTrain:  epoch  0, batch    84 | loss: 3.2996781MixupTrain:  epoch  0, batch    85 | loss: 3.2893841MixupTrain:  epoch  0, batch    86 | loss: 3.3266764MixupTrain:  epoch  0, batch    87 | loss: 3.3254762MixupTrain:  epoch  0, batch    88 | loss: 3.1874208MixupTrain:  epoch  0, batch    89 | loss: 3.0926993MixupTrain:  epoch  0, batch    90 | loss: 3.1867642MixupTrain:  epoch  0, batch    91 | loss: 3.3691659MixupTrain:  epoch  0, batch    92 | loss: 3.2384582MixupTrain:  epoch  0, batch    93 | loss: 3.0823026MixupTrain:  epoch  0, batch    94 | loss: 3.3233190MixupTrain:  epoch  0, batch    95 | loss: 3.2955635MixupTrain:  epoch  0, batch    96 | loss: 3.0390005MixupTrain:  epoch  0, batch    97 | loss: 3.2137218MixupTrain:  epoch  0, batch    98 | loss: 3.1615257MixupTrain:  epoch  0, batch    99 | loss: 3.0125740MixupTrain:  epoch  0, batch   100 | loss: 3.0469558MixupTrain:  epoch  0, batch   101 | loss: 3.2496924MixupTrain:  epoch  0, batch   102 | loss: 3.0989141MixupTrain:  epoch  0, batch   103 | loss: 3.2826717MixupTrain:  epoch  0, batch   104 | loss: 3.2881174MixupTrain:  epoch  0, batch   105 | loss: 3.2410743MixupTrain:  epoch  0, batch   106 | loss: 3.0422344MixupTrain:  epoch  0, batch   107 | loss: 3.2148242MixupTrain:  epoch  0, batch   108 | loss: 3.2792454MixupTrain:  epoch  0, batch   109 | loss: 3.3723552MixupTrain:  epoch  0, batch   110 | loss: 3.0962505MixupTrain:  epoch  0, batch   111 | loss: 3.1275892MixupTrain:  epoch  0, batch   112 | loss: 3.1899490MixupTrain:  epoch  0, batch   113 | loss: 2.9744949MixupTrain:  epoch  0, batch   114 | loss: 3.3108516MixupTrain:  epoch  0, batch   115 | loss: 3.1333604MixupTrain:  epoch  0, batch   116 | loss: 2.8270688MixupTrain:  epoch  0, batch   117 | loss: 3.0634708MixupTrain:  epoch  0, batch   118 | loss: 3.2789249MixupTrain:  epoch  0, batch   119 | loss: 3.1292551MixupTrain:  epoch  0, batch   120 | loss: 3.2235508MixupTrain:  epoch  0, batch   121 | loss: 3.1869192MixupTrain:  epoch  0, batch   122 | loss: 2.9930034MixupTrain:  epoch  0, batch   123 | loss: 3.0285516MixupTrain:  epoch  0, batch   124 | loss: 3.1342974MixupTrain:  epoch  0, batch   125 | loss: 3.2850056MixupTrain:  epoch  0, batch   126 | loss: 3.0630641MixupTrain:  epoch  0, batch   127 | loss: 3.2045336MixupTrain:  epoch  0, batch   128 | loss: 3.2532938MixupTrain:  epoch  0, batch   129 | loss: 3.0388923MixupTrain:  epoch  0, batch   130 | loss: 3.1188750MixupTrain:  epoch  0, batch   131 | loss: 3.0162821MixupTrain:  epoch  0, batch   132 | loss: 3.3712759MixupTrain:  epoch  0, batch   133 | loss: 3.1023898MixupTrain:  epoch  0, batch   134 | loss: 2.9411335MixupTrain:  epoch  0, batch   135 | loss: 3.1845634MixupTrain:  epoch  0, batch   136 | loss: 3.0041461MixupTrain:  epoch  0, batch   137 | loss: 3.3657730MixupTrain:  epoch  0, batch   138 | loss: 3.1565528MixupTrain:  epoch  0, batch   139 | loss: 3.1720705MixupTrain:  epoch  0, batch   140 | loss: 2.9945397MixupTrain:  epoch  0, batch   141 | loss: 2.9686353MixupTrain:  epoch  0, batch   142 | loss: 3.0456696MixupTrain:  epoch  0, batch   143 | loss: 3.0569015MixupTrain:  epoch  0, batch   144 | loss: 3.2432945MixupTrain:  epoch  0, batch   145 | loss: 2.9958711MixupTrain:  epoch  0, batch   146 | loss: 2.9770489MixupTrain:  epoch  0, batch   147 | loss: 3.1673121MixupTrain:  epoch  0, batch   148 | loss: 3.0557432MixupTrain:  epoch  0, batch   149 | loss: 3.1822162MixupTrain:  epoch  0, batch   150 | loss: 2.9704776MixupTrain:  epoch  0, batch   151 | loss: 3.0715523MixupTrain:  epoch  0, batch   152 | loss: 3.0559504MixupTrain:  epoch  0, batch   153 | loss: 3.0742435MixupTrain:  epoch  0, batch   154 | loss: 3.0078733MixupTrain:  epoch  0, batch   155 | loss: 3.1730270MixupTrain:  epoch  0, batch   156 | loss: 3.2473860MixupTrain:  epoch  0, batch   157 | loss: 3.0496478MixupTrain:  epoch  0, batch   158 | loss: 3.1060638MixupTrain:  epoch  0, batch   159 | loss: 3.1982408MixupTrain:  epoch  0, batch   160 | loss: 3.2083354MixupTrain:  epoch  0, batch   161 | loss: 3.0841961MixupTrain:  epoch  0, batch   162 | loss: 3.0639188MixupTrain:  epoch  0, batch   163 | loss: 2.9711051MixupTrain:  epoch  0, batch   164 | loss: 3.1361690MixupTrain:  epoch  0, batch   165 | loss: 3.1899104MixupTrain:  epoch  0, batch   166 | loss: 3.1213264MixupTrain:  epoch  0, batch   167 | loss: 3.0091734MixupTrain:  epoch  0, batch   168 | loss: 2.9768028MixupTrain:  epoch  0, batch   169 | loss: 3.0700970MixupTrain:  epoch  0, batch   170 | loss: 3.0562825MixupTrain:  epoch  0, batch   171 | loss: 3.0704255MixupTrain:  epoch  0, batch   172 | loss: 3.1052113MixupTrain:  epoch  0, batch   173 | loss: 3.0666792MixupTrain:  epoch  0, batch   174 | loss: 2.9900036MixupTrain:  epoch  0, batch   175 | loss: 3.0749528MixupTrain:  epoch  0, batch   176 | loss: 3.0171759MixupTrain:  epoch  0, batch   177 | loss: 2.8962548MixupTrain:  epoch  0, batch   178 | loss: 2.9828055MixupTrain:  epoch  0, batch   179 | loss: 2.9401565MixupTrain:  epoch  0, batch   180 | loss: 3.0010815MixupTrain:  epoch  0, batch   181 | loss: 3.2080307MixupTrain:  epoch  0, batch   182 | loss: 3.1225615MixupTrain:  epoch  0, batch   183 | loss: 2.9583921MixupTrain:  epoch  0, batch   184 | loss: 3.1306963MixupTrain:  epoch  0, batch   185 | loss: 3.1257002MixupTrain:  epoch  0, batch   186 | loss: 2.8501024MixupTrain:  epoch  0, batch   187 | loss: 3.0398059MixupTrain:  epoch  0, batch   188 | loss: 3.0473740MixupTrain:  epoch  0, batch   189 | loss: 2.9781561MixupTrain:  epoch  0, batch   190 | loss: 2.8931768MixupTrain:  epoch  0, batch   191 | loss: 3.0939898MixupTrain:  epoch  0, batch   192 | loss: 2.9807048MixupTrain:  epoch  0, batch   193 | loss: 3.0747962MixupTrain:  epoch  0, batch   194 | loss: 2.9881945MixupTrain:  epoch  0, batch   195 | loss: 2.9074242MixupTrain:  epoch  0, batch   196 | loss: 3.0661337MixupTrain:  epoch  0, batch   197 | loss: 3.0470619MixupTrain:  epoch  0, batch   198 | loss: 3.0694454MixupTrain:  epoch  0, batch   199 | loss: 3.0330865MixupTrain:  epoch  0, batch   200 | loss: 3.0786874MixupTrain:  epoch  0, batch   201 | loss: 2.9752891MixupTrain:  epoch  0, batch   202 | loss: 3.1555967MixupTrain:  epoch  0, batch   203 | loss: 3.0921931MixupTrain:  epoch  0, batch   204 | loss: 2.8392072MixupTrain:  epoch  0, batch   205 | loss: 3.0368032MixupTrain:  epoch  0, batch   206 | loss: 2.9066484MixupTrain:  epoch  0, batch   207 | loss: 2.9870529MixupTrain:  epoch  0, batch   208 | loss: 2.9661841MixupTrain:  epoch  0, batch   209 | loss: 2.9442487MixupTrain:  epoch  0, batch   210 | loss: 3.2063437MixupTrain:  epoch  0, batch   211 | loss: 2.9108191MixupTrain:  epoch  0, batch   212 | loss: 2.9283299MixupTrain:  epoch  0, batch   213 | loss: 2.8954506MixupTrain:  epoch  0, batch   214 | loss: 3.0197463MixupTrain:  epoch  0, batch   215 | loss: 2.9176488MixupTrain:  epoch  0, batch   216 | loss: 3.1403205MixupTrain:  epoch  0, batch   217 | loss: 3.0578182MixupTrain:  epoch  0, batch   218 | loss: 3.1721330MixupTrain:  epoch  0, batch   219 | loss: 3.1281414MixupTrain:  epoch  0, batch   220 | loss: 2.9739008MixupTrain:  epoch  0, batch   221 | loss: 3.1314979MixupTrain:  epoch  0, batch   222 | loss: 2.9723122MixupTrain:  epoch  0, batch   223 | loss: 3.0187445MixupTrain:  epoch  0, batch   224 | loss: 2.8855939MixupTrain:  epoch  0, batch   225 | loss: 2.8214164MixupTrain:  epoch  0, batch   226 | loss: 3.0865781MixupTrain:  epoch  0, batch   227 | loss: 3.0656772
MemoryTrain:  epoch  0, batch     0 | loss: 1.4050791MemoryTrain:  epoch  0, batch     1 | loss: 2.0517213MemoryTrain:  epoch  0, batch     2 | loss: 1.9191756MemoryTrain:  epoch  0, batch     3 | loss: 1.7598006MemoryTrain:  epoch  1, batch     0 | loss: 1.3813736MemoryTrain:  epoch  1, batch     1 | loss: 1.5789578MemoryTrain:  epoch  1, batch     2 | loss: 1.5997101MemoryTrain:  epoch  1, batch     3 | loss: 1.6112540MemoryTrain:  epoch  2, batch     0 | loss: 1.3335100MemoryTrain:  epoch  2, batch     1 | loss: 1.6220131MemoryTrain:  epoch  2, batch     2 | loss: 1.4101553MemoryTrain:  epoch  2, batch     3 | loss: 1.3771265MemoryTrain:  epoch  3, batch     0 | loss: 1.3555105MemoryTrain:  epoch  3, batch     1 | loss: 1.3585324MemoryTrain:  epoch  3, batch     2 | loss: 1.2904487MemoryTrain:  epoch  3, batch     3 | loss: 1.3124546MemoryTrain:  epoch  4, batch     0 | loss: 1.3354957MemoryTrain:  epoch  4, batch     1 | loss: 1.2795448MemoryTrain:  epoch  4, batch     2 | loss: 1.2982211MemoryTrain:  epoch  4, batch     3 | loss: 1.3757368MemoryTrain:  epoch  5, batch     0 | loss: 1.2205759MemoryTrain:  epoch  5, batch     1 | loss: 1.3128304MemoryTrain:  epoch  5, batch     2 | loss: 1.2955402MemoryTrain:  epoch  5, batch     3 | loss: 1.2833453MemoryTrain:  epoch  6, batch     0 | loss: 1.2604866MemoryTrain:  epoch  6, batch     1 | loss: 1.2674454MemoryTrain:  epoch  6, batch     2 | loss: 1.3152370MemoryTrain:  epoch  6, batch     3 | loss: 1.2602293MemoryTrain:  epoch  7, batch     0 | loss: 1.3241761MemoryTrain:  epoch  7, batch     1 | loss: 1.2520307MemoryTrain:  epoch  7, batch     2 | loss: 1.2586131MemoryTrain:  epoch  7, batch     3 | loss: 1.2262568MemoryTrain:  epoch  8, batch     0 | loss: 1.2490816MemoryTrain:  epoch  8, batch     1 | loss: 1.2408297MemoryTrain:  epoch  8, batch     2 | loss: 1.2052922MemoryTrain:  epoch  8, batch     3 | loss: 1.2233816MemoryTrain:  epoch  9, batch     0 | loss: 1.2329526MemoryTrain:  epoch  9, batch     1 | loss: 1.2217605MemoryTrain:  epoch  9, batch     2 | loss: 1.2245641MemoryTrain:  epoch  9, batch     3 | loss: 1.1960778
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 82.81%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:    5 | acc: 68.75%,  total acc: 79.17%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 79.46%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 78.91%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 79.86%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 80.62%   [EVAL] batch:   10 | acc: 68.75%,  total acc: 79.55%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 80.73%   [EVAL] batch:   12 | acc: 100.00%,  total acc: 82.21%   [EVAL] batch:   13 | acc: 100.00%,  total acc: 83.48%   [EVAL] batch:   14 | acc: 100.00%,  total acc: 84.58%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 85.55%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 86.40%   [EVAL] batch:   17 | acc: 25.00%,  total acc: 82.99%   
[EVAL] batch:    0 | acc: 43.75%,  total acc: 43.75%   [EVAL] batch:    1 | acc: 43.75%,  total acc: 43.75%   [EVAL] batch:    2 | acc: 50.00%,  total acc: 45.83%   [EVAL] batch:    3 | acc: 37.50%,  total acc: 43.75%   [EVAL] batch:    4 | acc: 50.00%,  total acc: 45.00%   [EVAL] batch:    5 | acc: 37.50%,  total acc: 43.75%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 49.11%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 55.47%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 60.42%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 63.75%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 67.05%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 68.75%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 70.19%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 70.09%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 70.42%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 69.53%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 69.85%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 69.44%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 69.08%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 69.69%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 71.13%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 72.16%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 73.37%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 74.48%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 75.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 76.44%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 77.08%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 77.90%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 78.66%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 78.75%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 79.44%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 80.08%   [EVAL] batch:   32 | acc: 43.75%,  total acc: 78.98%   [EVAL] batch:   33 | acc: 0.00%,  total acc: 76.65%   [EVAL] batch:   34 | acc: 0.00%,  total acc: 74.46%   [EVAL] batch:   35 | acc: 6.25%,  total acc: 72.57%   [EVAL] batch:   36 | acc: 0.00%,  total acc: 70.61%   [EVAL] batch:   37 | acc: 62.50%,  total acc: 70.39%   [EVAL] batch:   38 | acc: 75.00%,  total acc: 70.51%   [EVAL] batch:   39 | acc: 87.50%,  total acc: 70.94%   [EVAL] batch:   40 | acc: 75.00%,  total acc: 71.04%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 71.58%   [EVAL] batch:   42 | acc: 37.50%,  total acc: 70.78%   [EVAL] batch:   43 | acc: 0.00%,  total acc: 69.18%   [EVAL] batch:   44 | acc: 12.50%,  total acc: 67.92%   [EVAL] batch:   45 | acc: 6.25%,  total acc: 66.58%   [EVAL] batch:   46 | acc: 25.00%,  total acc: 65.69%   [EVAL] batch:   47 | acc: 68.75%,  total acc: 65.76%   [EVAL] batch:   48 | acc: 18.75%,  total acc: 64.80%   [EVAL] batch:   49 | acc: 12.50%,  total acc: 63.75%   [EVAL] batch:   50 | acc: 0.00%,  total acc: 62.50%   [EVAL] batch:   51 | acc: 18.75%,  total acc: 61.66%   [EVAL] batch:   52 | acc: 0.00%,  total acc: 60.50%   [EVAL] batch:   53 | acc: 43.75%,  total acc: 60.19%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 60.80%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 61.27%   [EVAL] batch:   56 | acc: 81.25%,  total acc: 61.62%   [EVAL] batch:   57 | acc: 68.75%,  total acc: 61.75%   [EVAL] batch:   58 | acc: 81.25%,  total acc: 62.08%   [EVAL] batch:   59 | acc: 75.00%,  total acc: 62.29%   [EVAL] batch:   60 | acc: 62.50%,  total acc: 62.30%   [EVAL] batch:   61 | acc: 75.00%,  total acc: 62.50%   [EVAL] batch:   62 | acc: 87.50%,  total acc: 62.90%   [EVAL] batch:   63 | acc: 81.25%,  total acc: 63.18%   [EVAL] batch:   64 | acc: 68.75%,  total acc: 63.27%   [EVAL] batch:   65 | acc: 87.50%,  total acc: 63.64%   [EVAL] batch:   66 | acc: 62.50%,  total acc: 63.62%   [EVAL] batch:   67 | acc: 87.50%,  total acc: 63.97%   [EVAL] batch:   68 | acc: 81.25%,  total acc: 64.22%   [EVAL] batch:   69 | acc: 75.00%,  total acc: 64.38%   [EVAL] batch:   70 | acc: 93.75%,  total acc: 64.79%   [EVAL] batch:   71 | acc: 68.75%,  total acc: 64.84%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 65.33%   [EVAL] batch:   73 | acc: 100.00%,  total acc: 65.79%   [EVAL] batch:   74 | acc: 100.00%,  total acc: 66.25%   [EVAL] batch:   75 | acc: 100.00%,  total acc: 66.69%   [EVAL] batch:   76 | acc: 100.00%,  total acc: 67.13%   [EVAL] batch:   77 | acc: 75.00%,  total acc: 67.23%   
cur_acc:  ['0.8580', '0.7083', '0.7232', '0.8299']
his_acc:  ['0.8580', '0.8285', '0.7500', '0.6723']
CurrentTrain: epoch  0, batch     0 | loss: 7.9266381CurrentTrain: epoch  0, batch     1 | loss: 8.8847666CurrentTrain: epoch  1, batch     0 | loss: 7.8395271CurrentTrain: epoch  1, batch     1 | loss: 7.2560358CurrentTrain: epoch  2, batch     0 | loss: 7.2145481CurrentTrain: epoch  2, batch     1 | loss: 7.9157224CurrentTrain: epoch  3, batch     0 | loss: 7.1356535CurrentTrain: epoch  3, batch     1 | loss: 6.0766597CurrentTrain: epoch  4, batch     0 | loss: 6.5992947CurrentTrain: epoch  4, batch     1 | loss: 5.3449516CurrentTrain: epoch  5, batch     0 | loss: 5.8856411CurrentTrain: epoch  5, batch     1 | loss: 6.5758109CurrentTrain: epoch  6, batch     0 | loss: 6.3026700CurrentTrain: epoch  6, batch     1 | loss: 5.4114394CurrentTrain: epoch  7, batch     0 | loss: 5.5658150CurrentTrain: epoch  7, batch     1 | loss: 5.6311059CurrentTrain: epoch  8, batch     0 | loss: 5.0781436CurrentTrain: epoch  8, batch     1 | loss: 5.5114813CurrentTrain: epoch  9, batch     0 | loss: 5.2140760CurrentTrain: epoch  9, batch     1 | loss: 4.0652008
#### organization subsidiaries ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization subsidiaries" means an organization 's subsidiary is an organization.
Here is an example:
Relation: organization subsidiaries
Context: venture fund buys sporting chain highland capital 's consumer fund includes lululemon athletica , a yoga retailer , and o beverages , a flavored water company developed by tom first , one of the two `` juice guys '' who cofounded nantucket nectars .
Head Entity: highland capital
Tail Entity: o beverages
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization subsidiaries":
Relation: organization subsidiaries  
Context: The tech giant Alphabet Inc. has several subsidiaries, including YouTube, which has become a leading platform for video content, and Waymo, a company focused on self-driving technology.  
Head Entity: Alphabet Inc.  
Tail Entity: Waymo  

Relation: organization subsidiaries  
Context: The automotive manufacturer Ford Motor Company has expanded its reach by acquiring several companies, including the electric vehicle startup Rivian, which aims to revolutionize the EV market.  
Head Entity: Ford Motor Company  
Tail Entity: Rivian  
#### organization parents ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization parents" means an organization 's parent is an organization.
Here is an example:
Relation: organization parents
Context: aseel kami , tim cocks , abdul-rahman taher , tim cocks and michael christie -lrb- reuters -rrb- add an intriguing note in the speculation over who 's responsible for the recent bombings : `` but a senior iraqi intelligence source , who declined to be named , said there was evidence the bombs could be the work of the militant badr organisation , the armed wing of the supreme islamic iraqi council -lrb- isci -rrb- .
Head Entity: badr organisation
Tail Entity: supreme islamic iraqi council
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization parents":
Relation: organization parents  
Context: in a recent announcement, the tech giant apple inc. revealed its plans to acquire the smaller company beats electronics, which is known for its high-quality audio products. this acquisition is expected to enhance apple's music streaming service and expand its product offerings.  
Head Entity: apple inc.  
Tail Entity: beats electronics  

Relation: organization parents  
Context: the merger between the two pharmaceutical companies, pfizer and allergan, has created a new entity that is set to dominate the market. this strategic move is seen as a way to leverage their combined resources and expertise in drug development.  
Head Entity: pfizer  
Tail Entity: allergan  
#### organization alternate names ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization alternate names" means a person 's alias is a person.
Here is an example:
Relation: organization alternate names
Context: while section 106 of the hyde act openly bans indian testing , and the agreement upholds reinforces that test ban by upholding the applicability of domestic laws , washington has already recommended that the nuclear suppliers group -lrb- nsg -rrb- link its proposed exemption for india to a similar test ban .
Head Entity: nuclear suppliers group
Tail Entity: nsg
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization alternate names":
Relation: organization alternate names  
Context: The International Monetary Fund, often referred to as the IMF, plays a crucial role in global economic stability by providing financial assistance and advice to member countries.  
Head Entity: International Monetary Fund  
Tail Entity: IMF  

Relation: organization alternate names  
Context: The World Health Organization, commonly known as WHO, has been at the forefront of global health initiatives, especially during the COVID-19 pandemic.  
Head Entity: World Health Organization  
Tail Entity: WHO  
#### organization city of headquarters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization city of headquarters" means an organization is located in a city.
Here is an example:
Relation: organization city of headquarters
Context: guy-sheftall entered spelman college in atlanta at age 16 and later earned a master 's in english with a thesis titled `` faulkner 's treatment of women in his major novels . ''
Head Entity: spelman college
Tail Entity: atlanta
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization city of headquarters":
Relation: organization city of headquarters  
Context: the headquarters of tech giant apple inc. is located in cupertino, california, where it has been since 1993.  
Head Entity: apple inc.  
Tail Entity: cupertino  

Relation: organization city of headquarters  
Context: the united nations has its main office in new york city, serving as a hub for international diplomacy and cooperation.  
Head Entity: united nations  
Tail Entity: new york city  
#### person siblings ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person siblings" means a person 's sibling is a person.
Here is an example:
Relation: person siblings
Context: more than three decades ago , kerry 's work against the vietnam war set him on course to the senate - and , he often hoped , on to the presidency .
Head Entity: kerry
Tail Entity: he
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person siblings":
Relation: person siblings  
Context: During their childhood, Emma and her brother often played together in the backyard, creating memories that would last a lifetime.  
Head Entity: Emma  
Tail Entity: her brother  

Relation: person siblings  
Context: After years of living in different cities, Sarah finally reunited with her sister at their family home for the holidays, reminiscing about their childhood adventures.  
Head Entity: Sarah  
Tail Entity: her sister  
Mixup data size:  5050
MixupTrain:  epoch  0, batch     0 | loss: 4.2934642MixupTrain:  epoch  0, batch     1 | loss: 4.2195873MixupTrain:  epoch  0, batch     2 | loss: 4.2484088MixupTrain:  epoch  0, batch     3 | loss: 4.6087370MixupTrain:  epoch  0, batch     4 | loss: 3.7532477MixupTrain:  epoch  0, batch     5 | loss: 4.3253260MixupTrain:  epoch  0, batch     6 | loss: 4.2204103MixupTrain:  epoch  0, batch     7 | loss: 3.9395144MixupTrain:  epoch  0, batch     8 | loss: 4.0131330MixupTrain:  epoch  0, batch     9 | loss: 4.4988995MixupTrain:  epoch  0, batch    10 | loss: 4.2169795MixupTrain:  epoch  0, batch    11 | loss: 3.6537368MixupTrain:  epoch  0, batch    12 | loss: 3.6705887MixupTrain:  epoch  0, batch    13 | loss: 3.8657241MixupTrain:  epoch  0, batch    14 | loss: 3.4805374MixupTrain:  epoch  0, batch    15 | loss: 4.2078104MixupTrain:  epoch  0, batch    16 | loss: 3.7758465MixupTrain:  epoch  0, batch    17 | loss: 4.2632341MixupTrain:  epoch  0, batch    18 | loss: 3.7753692MixupTrain:  epoch  0, batch    19 | loss: 3.8880854MixupTrain:  epoch  0, batch    20 | loss: 3.4168773MixupTrain:  epoch  0, batch    21 | loss: 3.5431118MixupTrain:  epoch  0, batch    22 | loss: 3.7785535MixupTrain:  epoch  0, batch    23 | loss: 3.9406965MixupTrain:  epoch  0, batch    24 | loss: 3.5356276MixupTrain:  epoch  0, batch    25 | loss: 3.6211941MixupTrain:  epoch  0, batch    26 | loss: 3.5837655MixupTrain:  epoch  0, batch    27 | loss: 3.5465131MixupTrain:  epoch  0, batch    28 | loss: 4.1889830MixupTrain:  epoch  0, batch    29 | loss: 3.8753417MixupTrain:  epoch  0, batch    30 | loss: 3.6119454MixupTrain:  epoch  0, batch    31 | loss: 3.7437935MixupTrain:  epoch  0, batch    32 | loss: 3.3288658MixupTrain:  epoch  0, batch    33 | loss: 3.9056938MixupTrain:  epoch  0, batch    34 | loss: 3.5183620MixupTrain:  epoch  0, batch    35 | loss: 3.8530066MixupTrain:  epoch  0, batch    36 | loss: 3.8908300MixupTrain:  epoch  0, batch    37 | loss: 3.5823903MixupTrain:  epoch  0, batch    38 | loss: 3.8036895MixupTrain:  epoch  0, batch    39 | loss: 3.5386777MixupTrain:  epoch  0, batch    40 | loss: 3.4252205MixupTrain:  epoch  0, batch    41 | loss: 3.6370430MixupTrain:  epoch  0, batch    42 | loss: 3.7562468MixupTrain:  epoch  0, batch    43 | loss: 3.5197744MixupTrain:  epoch  0, batch    44 | loss: 3.7946939MixupTrain:  epoch  0, batch    45 | loss: 3.7016230MixupTrain:  epoch  0, batch    46 | loss: 3.7415612MixupTrain:  epoch  0, batch    47 | loss: 3.4345160MixupTrain:  epoch  0, batch    48 | loss: 3.5771132MixupTrain:  epoch  0, batch    49 | loss: 3.3427992MixupTrain:  epoch  0, batch    50 | loss: 3.6414957MixupTrain:  epoch  0, batch    51 | loss: 3.6242175MixupTrain:  epoch  0, batch    52 | loss: 3.7583489MixupTrain:  epoch  0, batch    53 | loss: 3.5037537MixupTrain:  epoch  0, batch    54 | loss: 3.4345574MixupTrain:  epoch  0, batch    55 | loss: 3.6741509MixupTrain:  epoch  0, batch    56 | loss: 3.4420912MixupTrain:  epoch  0, batch    57 | loss: 3.3920565MixupTrain:  epoch  0, batch    58 | loss: 3.3135581MixupTrain:  epoch  0, batch    59 | loss: 3.5222003MixupTrain:  epoch  0, batch    60 | loss: 3.6676474MixupTrain:  epoch  0, batch    61 | loss: 3.4514356MixupTrain:  epoch  0, batch    62 | loss: 3.4645383MixupTrain:  epoch  0, batch    63 | loss: 3.6166601MixupTrain:  epoch  0, batch    64 | loss: 3.3526549MixupTrain:  epoch  0, batch    65 | loss: 3.6010919MixupTrain:  epoch  0, batch    66 | loss: 3.6100829MixupTrain:  epoch  0, batch    67 | loss: 3.3521404MixupTrain:  epoch  0, batch    68 | loss: 3.5435963MixupTrain:  epoch  0, batch    69 | loss: 3.5711026MixupTrain:  epoch  0, batch    70 | loss: 3.2886503MixupTrain:  epoch  0, batch    71 | loss: 3.3473780MixupTrain:  epoch  0, batch    72 | loss: 3.3022203MixupTrain:  epoch  0, batch    73 | loss: 3.0994196MixupTrain:  epoch  0, batch    74 | loss: 3.4316473MixupTrain:  epoch  0, batch    75 | loss: 3.6302643MixupTrain:  epoch  0, batch    76 | loss: 3.5228167MixupTrain:  epoch  0, batch    77 | loss: 3.4513049MixupTrain:  epoch  0, batch    78 | loss: 3.1719234MixupTrain:  epoch  0, batch    79 | loss: 3.4825053MixupTrain:  epoch  0, batch    80 | loss: 3.4643364MixupTrain:  epoch  0, batch    81 | loss: 3.2800860MixupTrain:  epoch  0, batch    82 | loss: 3.2477722MixupTrain:  epoch  0, batch    83 | loss: 3.3980093MixupTrain:  epoch  0, batch    84 | loss: 3.4522095MixupTrain:  epoch  0, batch    85 | loss: 3.4865608MixupTrain:  epoch  0, batch    86 | loss: 3.5359735MixupTrain:  epoch  0, batch    87 | loss: 3.4647174MixupTrain:  epoch  0, batch    88 | loss: 3.3157392MixupTrain:  epoch  0, batch    89 | loss: 3.4020512MixupTrain:  epoch  0, batch    90 | loss: 3.4226778MixupTrain:  epoch  0, batch    91 | loss: 3.2930319MixupTrain:  epoch  0, batch    92 | loss: 3.5290082MixupTrain:  epoch  0, batch    93 | loss: 3.5845962MixupTrain:  epoch  0, batch    94 | loss: 3.3496642MixupTrain:  epoch  0, batch    95 | loss: 3.1557150MixupTrain:  epoch  0, batch    96 | loss: 3.0180774MixupTrain:  epoch  0, batch    97 | loss: 3.1506848MixupTrain:  epoch  0, batch    98 | loss: 3.0079894MixupTrain:  epoch  0, batch    99 | loss: 3.7126348MixupTrain:  epoch  0, batch   100 | loss: 3.0624485MixupTrain:  epoch  0, batch   101 | loss: 3.3480215MixupTrain:  epoch  0, batch   102 | loss: 3.2243745MixupTrain:  epoch  0, batch   103 | loss: 3.1746712MixupTrain:  epoch  0, batch   104 | loss: 3.1872015MixupTrain:  epoch  0, batch   105 | loss: 3.3595228MixupTrain:  epoch  0, batch   106 | loss: 3.2468157MixupTrain:  epoch  0, batch   107 | loss: 3.2098384MixupTrain:  epoch  0, batch   108 | loss: 3.6944547MixupTrain:  epoch  0, batch   109 | loss: 3.2872231MixupTrain:  epoch  0, batch   110 | loss: 3.0452771MixupTrain:  epoch  0, batch   111 | loss: 3.1428232MixupTrain:  epoch  0, batch   112 | loss: 3.2910788MixupTrain:  epoch  0, batch   113 | loss: 2.9484074MixupTrain:  epoch  0, batch   114 | loss: 3.2336140MixupTrain:  epoch  0, batch   115 | loss: 3.3693283MixupTrain:  epoch  0, batch   116 | loss: 3.4079823MixupTrain:  epoch  0, batch   117 | loss: 3.1411090MixupTrain:  epoch  0, batch   118 | loss: 3.2433012MixupTrain:  epoch  0, batch   119 | loss: 3.3784418MixupTrain:  epoch  0, batch   120 | loss: 3.2412949MixupTrain:  epoch  0, batch   121 | loss: 3.1045308MixupTrain:  epoch  0, batch   122 | loss: 3.3441858MixupTrain:  epoch  0, batch   123 | loss: 3.2428446MixupTrain:  epoch  0, batch   124 | loss: 3.0004461MixupTrain:  epoch  0, batch   125 | loss: 3.5251436MixupTrain:  epoch  0, batch   126 | loss: 3.0952821MixupTrain:  epoch  0, batch   127 | loss: 3.1599703MixupTrain:  epoch  0, batch   128 | loss: 3.3273792MixupTrain:  epoch  0, batch   129 | loss: 3.1225796MixupTrain:  epoch  0, batch   130 | loss: 3.2633014MixupTrain:  epoch  0, batch   131 | loss: 3.1334219MixupTrain:  epoch  0, batch   132 | loss: 3.2554445MixupTrain:  epoch  0, batch   133 | loss: 3.4385180MixupTrain:  epoch  0, batch   134 | loss: 3.1828246MixupTrain:  epoch  0, batch   135 | loss: 3.0664048MixupTrain:  epoch  0, batch   136 | loss: 3.3219993MixupTrain:  epoch  0, batch   137 | loss: 3.3553443MixupTrain:  epoch  0, batch   138 | loss: 2.7634146MixupTrain:  epoch  0, batch   139 | loss: 3.2639720MixupTrain:  epoch  0, batch   140 | loss: 3.0089388MixupTrain:  epoch  0, batch   141 | loss: 3.3412304MixupTrain:  epoch  0, batch   142 | loss: 3.0586786MixupTrain:  epoch  0, batch   143 | loss: 3.2508647MixupTrain:  epoch  0, batch   144 | loss: 3.3428221MixupTrain:  epoch  0, batch   145 | loss: 3.3145361MixupTrain:  epoch  0, batch   146 | loss: 3.1682811MixupTrain:  epoch  0, batch   147 | loss: 3.3022099MixupTrain:  epoch  0, batch   148 | loss: 3.3025312MixupTrain:  epoch  0, batch   149 | loss: 3.1489468MixupTrain:  epoch  0, batch   150 | loss: 3.4869764MixupTrain:  epoch  0, batch   151 | loss: 3.1217518MixupTrain:  epoch  0, batch   152 | loss: 3.0827560MixupTrain:  epoch  0, batch   153 | loss: 3.1099360MixupTrain:  epoch  0, batch   154 | loss: 3.0800624MixupTrain:  epoch  0, batch   155 | loss: 3.0890369MixupTrain:  epoch  0, batch   156 | loss: 3.3312800MixupTrain:  epoch  0, batch   157 | loss: 2.9566443MixupTrain:  epoch  0, batch   158 | loss: 3.1400919MixupTrain:  epoch  0, batch   159 | loss: 3.0962956MixupTrain:  epoch  0, batch   160 | loss: 3.2673850MixupTrain:  epoch  0, batch   161 | loss: 3.1266227MixupTrain:  epoch  0, batch   162 | loss: 3.5750537MixupTrain:  epoch  0, batch   163 | loss: 3.0409446MixupTrain:  epoch  0, batch   164 | loss: 3.2384143MixupTrain:  epoch  0, batch   165 | loss: 3.0862253MixupTrain:  epoch  0, batch   166 | loss: 3.1480722MixupTrain:  epoch  0, batch   167 | loss: 3.2191350MixupTrain:  epoch  0, batch   168 | loss: 3.1356339MixupTrain:  epoch  0, batch   169 | loss: 2.8532906MixupTrain:  epoch  0, batch   170 | loss: 3.0798354MixupTrain:  epoch  0, batch   171 | loss: 3.1041026MixupTrain:  epoch  0, batch   172 | loss: 3.1591067MixupTrain:  epoch  0, batch   173 | loss: 3.3068569MixupTrain:  epoch  0, batch   174 | loss: 3.1611900MixupTrain:  epoch  0, batch   175 | loss: 3.0549779MixupTrain:  epoch  0, batch   176 | loss: 3.1104183MixupTrain:  epoch  0, batch   177 | loss: 3.0876081MixupTrain:  epoch  0, batch   178 | loss: 3.2304225MixupTrain:  epoch  0, batch   179 | loss: 3.2504382MixupTrain:  epoch  0, batch   180 | loss: 3.1003716MixupTrain:  epoch  0, batch   181 | loss: 2.9731677MixupTrain:  epoch  0, batch   182 | loss: 2.9640527MixupTrain:  epoch  0, batch   183 | loss: 3.2081361MixupTrain:  epoch  0, batch   184 | loss: 3.0102139MixupTrain:  epoch  0, batch   185 | loss: 3.0815187MixupTrain:  epoch  0, batch   186 | loss: 2.8236079MixupTrain:  epoch  0, batch   187 | loss: 3.2570505MixupTrain:  epoch  0, batch   188 | loss: 3.0733128MixupTrain:  epoch  0, batch   189 | loss: 3.2579601MixupTrain:  epoch  0, batch   190 | loss: 3.2821639MixupTrain:  epoch  0, batch   191 | loss: 3.1093102MixupTrain:  epoch  0, batch   192 | loss: 2.9031663MixupTrain:  epoch  0, batch   193 | loss: 3.0040770MixupTrain:  epoch  0, batch   194 | loss: 3.1791162MixupTrain:  epoch  0, batch   195 | loss: 3.3367956MixupTrain:  epoch  0, batch   196 | loss: 3.2588778MixupTrain:  epoch  0, batch   197 | loss: 3.2699270MixupTrain:  epoch  0, batch   198 | loss: 3.2769642MixupTrain:  epoch  0, batch   199 | loss: 3.3112564MixupTrain:  epoch  0, batch   200 | loss: 2.9729590MixupTrain:  epoch  0, batch   201 | loss: 3.0851569MixupTrain:  epoch  0, batch   202 | loss: 3.1047311MixupTrain:  epoch  0, batch   203 | loss: 3.1000919MixupTrain:  epoch  0, batch   204 | loss: 2.9915981MixupTrain:  epoch  0, batch   205 | loss: 3.0810409MixupTrain:  epoch  0, batch   206 | loss: 3.1957610MixupTrain:  epoch  0, batch   207 | loss: 2.9844377MixupTrain:  epoch  0, batch   208 | loss: 3.0895844MixupTrain:  epoch  0, batch   209 | loss: 3.2158933MixupTrain:  epoch  0, batch   210 | loss: 3.1499748MixupTrain:  epoch  0, batch   211 | loss: 3.1650519MixupTrain:  epoch  0, batch   212 | loss: 3.0919704MixupTrain:  epoch  0, batch   213 | loss: 3.0978661MixupTrain:  epoch  0, batch   214 | loss: 3.0941286MixupTrain:  epoch  0, batch   215 | loss: 2.9498434MixupTrain:  epoch  0, batch   216 | loss: 3.1847160MixupTrain:  epoch  0, batch   217 | loss: 3.0893497MixupTrain:  epoch  0, batch   218 | loss: 3.0631464MixupTrain:  epoch  0, batch   219 | loss: 2.9554803MixupTrain:  epoch  0, batch   220 | loss: 2.9201412MixupTrain:  epoch  0, batch   221 | loss: 3.0852499MixupTrain:  epoch  0, batch   222 | loss: 3.1847875MixupTrain:  epoch  0, batch   223 | loss: 2.9474108MixupTrain:  epoch  0, batch   224 | loss: 3.3549523MixupTrain:  epoch  0, batch   225 | loss: 3.1919241MixupTrain:  epoch  0, batch   226 | loss: 3.1265073MixupTrain:  epoch  0, batch   227 | loss: 3.5014358MixupTrain:  epoch  0, batch   228 | loss: 3.0802240MixupTrain:  epoch  0, batch   229 | loss: 2.9979362MixupTrain:  epoch  0, batch   230 | loss: 3.1258578MixupTrain:  epoch  0, batch   231 | loss: 3.2929530MixupTrain:  epoch  0, batch   232 | loss: 3.0202146MixupTrain:  epoch  0, batch   233 | loss: 3.1126490MixupTrain:  epoch  0, batch   234 | loss: 2.9869707MixupTrain:  epoch  0, batch   235 | loss: 2.8682899MixupTrain:  epoch  0, batch   236 | loss: 3.2123144MixupTrain:  epoch  0, batch   237 | loss: 3.0854378MixupTrain:  epoch  0, batch   238 | loss: 3.1742268MixupTrain:  epoch  0, batch   239 | loss: 2.9382427MixupTrain:  epoch  0, batch   240 | loss: 2.9880006MixupTrain:  epoch  0, batch   241 | loss: 3.1718340MixupTrain:  epoch  0, batch   242 | loss: 3.0827610MixupTrain:  epoch  0, batch   243 | loss: 3.0494328MixupTrain:  epoch  0, batch   244 | loss: 3.2424531MixupTrain:  epoch  0, batch   245 | loss: 3.0470684MixupTrain:  epoch  0, batch   246 | loss: 3.2459078MixupTrain:  epoch  0, batch   247 | loss: 3.0018470MixupTrain:  epoch  0, batch   248 | loss: 2.9873896MixupTrain:  epoch  0, batch   249 | loss: 2.9636469MixupTrain:  epoch  0, batch   250 | loss: 3.0730915MixupTrain:  epoch  0, batch   251 | loss: 2.9677958MixupTrain:  epoch  0, batch   252 | loss: 2.7805295MixupTrain:  epoch  0, batch   253 | loss: 3.0084507MixupTrain:  epoch  0, batch   254 | loss: 3.0066485MixupTrain:  epoch  0, batch   255 | loss: 2.8488834MixupTrain:  epoch  0, batch   256 | loss: 3.1124926MixupTrain:  epoch  0, batch   257 | loss: 3.0827374MixupTrain:  epoch  0, batch   258 | loss: 3.0171487MixupTrain:  epoch  0, batch   259 | loss: 3.0320482MixupTrain:  epoch  0, batch   260 | loss: 3.0402098MixupTrain:  epoch  0, batch   261 | loss: 3.2247202MixupTrain:  epoch  0, batch   262 | loss: 2.9499078MixupTrain:  epoch  0, batch   263 | loss: 3.1286826MixupTrain:  epoch  0, batch   264 | loss: 2.9656074MixupTrain:  epoch  0, batch   265 | loss: 3.0495527MixupTrain:  epoch  0, batch   266 | loss: 3.1506226MixupTrain:  epoch  0, batch   267 | loss: 3.1665456MixupTrain:  epoch  0, batch   268 | loss: 3.0417275MixupTrain:  epoch  0, batch   269 | loss: 2.9480431MixupTrain:  epoch  0, batch   270 | loss: 3.2892499MixupTrain:  epoch  0, batch   271 | loss: 3.1094899MixupTrain:  epoch  0, batch   272 | loss: 2.8607860MixupTrain:  epoch  0, batch   273 | loss: 3.2605948MixupTrain:  epoch  0, batch   274 | loss: 2.9718275MixupTrain:  epoch  0, batch   275 | loss: 3.0245757MixupTrain:  epoch  0, batch   276 | loss: 3.1516871MixupTrain:  epoch  0, batch   277 | loss: 2.9970765MixupTrain:  epoch  0, batch   278 | loss: 2.9372230MixupTrain:  epoch  0, batch   279 | loss: 3.0430455MixupTrain:  epoch  0, batch   280 | loss: 3.1410098MixupTrain:  epoch  0, batch   281 | loss: 3.0305963MixupTrain:  epoch  0, batch   282 | loss: 3.0762997MixupTrain:  epoch  0, batch   283 | loss: 3.0045543MixupTrain:  epoch  0, batch   284 | loss: 3.1719418MixupTrain:  epoch  0, batch   285 | loss: 2.9286747MixupTrain:  epoch  0, batch   286 | loss: 3.1015711MixupTrain:  epoch  0, batch   287 | loss: 3.1140738MixupTrain:  epoch  0, batch   288 | loss: 2.9482429MixupTrain:  epoch  0, batch   289 | loss: 2.9721951MixupTrain:  epoch  0, batch   290 | loss: 2.8662744MixupTrain:  epoch  0, batch   291 | loss: 2.8995700MixupTrain:  epoch  0, batch   292 | loss: 3.1424723MixupTrain:  epoch  0, batch   293 | loss: 3.3863339MixupTrain:  epoch  0, batch   294 | loss: 3.0208187MixupTrain:  epoch  0, batch   295 | loss: 3.1193240MixupTrain:  epoch  0, batch   296 | loss: 3.1222382MixupTrain:  epoch  0, batch   297 | loss: 3.0686452MixupTrain:  epoch  0, batch   298 | loss: 2.9900036MixupTrain:  epoch  0, batch   299 | loss: 3.1627536MixupTrain:  epoch  0, batch   300 | loss: 3.2391720MixupTrain:  epoch  0, batch   301 | loss: 3.0490367MixupTrain:  epoch  0, batch   302 | loss: 3.1516356MixupTrain:  epoch  0, batch   303 | loss: 3.1455204MixupTrain:  epoch  0, batch   304 | loss: 2.7638221MixupTrain:  epoch  0, batch   305 | loss: 3.1433258MixupTrain:  epoch  0, batch   306 | loss: 3.0238118MixupTrain:  epoch  0, batch   307 | loss: 2.8946033MixupTrain:  epoch  0, batch   308 | loss: 3.0660830MixupTrain:  epoch  0, batch   309 | loss: 2.7126572MixupTrain:  epoch  0, batch   310 | loss: 3.0169451MixupTrain:  epoch  0, batch   311 | loss: 3.0154195MixupTrain:  epoch  0, batch   312 | loss: 3.0686090MixupTrain:  epoch  0, batch   313 | loss: 3.1576617MixupTrain:  epoch  0, batch   314 | loss: 2.9936957MixupTrain:  epoch  0, batch   315 | loss: 2.8845956
MemoryTrain:  epoch  0, batch     0 | loss: 1.1993806MemoryTrain:  epoch  0, batch     1 | loss: 1.8151218MemoryTrain:  epoch  0, batch     2 | loss: 1.9915828MemoryTrain:  epoch  0, batch     3 | loss: 1.7237914MemoryTrain:  epoch  0, batch     4 | loss: 1.8468667MemoryTrain:  epoch  1, batch     0 | loss: 1.7418413MemoryTrain:  epoch  1, batch     1 | loss: 1.3247790MemoryTrain:  epoch  1, batch     2 | loss: 1.3483087MemoryTrain:  epoch  1, batch     3 | loss: 1.3782675MemoryTrain:  epoch  1, batch     4 | loss: 1.7871149MemoryTrain:  epoch  2, batch     0 | loss: 1.3559954MemoryTrain:  epoch  2, batch     1 | loss: 1.2852994MemoryTrain:  epoch  2, batch     2 | loss: 1.4341984MemoryTrain:  epoch  2, batch     3 | loss: 1.5905621MemoryTrain:  epoch  2, batch     4 | loss: 1.3958665MemoryTrain:  epoch  3, batch     0 | loss: 1.2503949MemoryTrain:  epoch  3, batch     1 | loss: 1.4200810MemoryTrain:  epoch  3, batch     2 | loss: 1.2641863MemoryTrain:  epoch  3, batch     3 | loss: 1.4619001MemoryTrain:  epoch  3, batch     4 | loss: 1.2881424MemoryTrain:  epoch  4, batch     0 | loss: 1.2656614MemoryTrain:  epoch  4, batch     1 | loss: 1.2965713MemoryTrain:  epoch  4, batch     2 | loss: 1.2373457MemoryTrain:  epoch  4, batch     3 | loss: 1.3148274MemoryTrain:  epoch  4, batch     4 | loss: 1.3228660MemoryTrain:  epoch  5, batch     0 | loss: 1.2672688MemoryTrain:  epoch  5, batch     1 | loss: 1.2140101MemoryTrain:  epoch  5, batch     2 | loss: 1.2294613MemoryTrain:  epoch  5, batch     3 | loss: 1.2104101MemoryTrain:  epoch  5, batch     4 | loss: 1.3194013MemoryTrain:  epoch  6, batch     0 | loss: 1.2717623MemoryTrain:  epoch  6, batch     1 | loss: 1.2125142MemoryTrain:  epoch  6, batch     2 | loss: 1.3248453MemoryTrain:  epoch  6, batch     3 | loss: 1.3007104MemoryTrain:  epoch  6, batch     4 | loss: 1.2152599MemoryTrain:  epoch  7, batch     0 | loss: 1.2734005MemoryTrain:  epoch  7, batch     1 | loss: 1.2786684MemoryTrain:  epoch  7, batch     2 | loss: 1.2401333MemoryTrain:  epoch  7, batch     3 | loss: 1.2262466MemoryTrain:  epoch  7, batch     4 | loss: 1.2018105MemoryTrain:  epoch  8, batch     0 | loss: 1.2224598MemoryTrain:  epoch  8, batch     1 | loss: 1.2494605MemoryTrain:  epoch  8, batch     2 | loss: 1.2492099MemoryTrain:  epoch  8, batch     3 | loss: 1.2044966MemoryTrain:  epoch  8, batch     4 | loss: 1.2117659MemoryTrain:  epoch  9, batch     0 | loss: 1.2473768MemoryTrain:  epoch  9, batch     1 | loss: 1.2143836MemoryTrain:  epoch  9, batch     2 | loss: 1.2423782MemoryTrain:  epoch  9, batch     3 | loss: 1.2115195MemoryTrain:  epoch  9, batch     4 | loss: 1.1961272
[EVAL] batch:    0 | acc: 37.50%,  total acc: 37.50%   [EVAL] batch:    1 | acc: 50.00%,  total acc: 43.75%   [EVAL] batch:    2 | acc: 37.50%,  total acc: 41.67%   [EVAL] batch:    3 | acc: 56.25%,  total acc: 45.31%   [EVAL] batch:    4 | acc: 56.25%,  total acc: 47.50%   [EVAL] batch:    5 | acc: 50.00%,  total acc: 47.92%   [EVAL] batch:    6 | acc: 62.50%,  total acc: 50.00%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 53.12%   [EVAL] batch:    8 | acc: 50.00%,  total acc: 52.78%   [EVAL] batch:    9 | acc: 56.25%,  total acc: 53.12%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 56.25%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 58.85%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 60.10%   [EVAL] batch:   13 | acc: 100.00%,  total acc: 62.95%   [EVAL] batch:   14 | acc: 100.00%,  total acc: 65.42%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 67.58%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 69.12%   [EVAL] batch:   17 | acc: 81.25%,  total acc: 69.79%   [EVAL] batch:   18 | acc: 43.75%,  total acc: 68.42%   [EVAL] batch:   19 | acc: 68.75%,  total acc: 68.44%   [EVAL] batch:   20 | acc: 50.00%,  total acc: 67.56%   [EVAL] batch:   21 | acc: 31.25%,  total acc: 65.91%   
[EVAL] batch:    0 | acc: 6.25%,  total acc: 6.25%   [EVAL] batch:    1 | acc: 31.25%,  total acc: 18.75%   [EVAL] batch:    2 | acc: 37.50%,  total acc: 25.00%   [EVAL] batch:    3 | acc: 12.50%,  total acc: 21.88%   [EVAL] batch:    4 | acc: 18.75%,  total acc: 21.25%   [EVAL] batch:    5 | acc: 6.25%,  total acc: 18.75%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 26.79%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 35.94%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 43.06%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 48.12%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 52.84%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 55.73%   [EVAL] batch:   12 | acc: 56.25%,  total acc: 55.77%   [EVAL] batch:   13 | acc: 31.25%,  total acc: 54.02%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 55.00%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 55.08%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 56.25%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 56.60%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 56.91%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 58.13%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 60.12%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 61.65%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 63.32%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 64.58%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 66.00%   [EVAL] batch:   25 | acc: 93.75%,  total acc: 67.07%   [EVAL] batch:   26 | acc: 87.50%,  total acc: 67.82%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 68.97%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 70.04%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 70.42%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 71.17%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 71.88%   [EVAL] batch:   32 | acc: 56.25%,  total acc: 71.40%   [EVAL] batch:   33 | acc: 12.50%,  total acc: 69.67%   [EVAL] batch:   34 | acc: 12.50%,  total acc: 68.04%   [EVAL] batch:   35 | acc: 12.50%,  total acc: 66.49%   [EVAL] batch:   36 | acc: 6.25%,  total acc: 64.86%   [EVAL] batch:   37 | acc: 68.75%,  total acc: 64.97%   [EVAL] batch:   38 | acc: 75.00%,  total acc: 65.22%   [EVAL] batch:   39 | acc: 93.75%,  total acc: 65.94%   [EVAL] batch:   40 | acc: 75.00%,  total acc: 66.16%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 66.82%   [EVAL] batch:   42 | acc: 25.00%,  total acc: 65.84%   [EVAL] batch:   43 | acc: 0.00%,  total acc: 64.35%   [EVAL] batch:   44 | acc: 0.00%,  total acc: 62.92%   [EVAL] batch:   45 | acc: 0.00%,  total acc: 61.55%   [EVAL] batch:   46 | acc: 25.00%,  total acc: 60.77%   [EVAL] batch:   47 | acc: 81.25%,  total acc: 61.20%   [EVAL] batch:   48 | acc: 18.75%,  total acc: 60.33%   [EVAL] batch:   49 | acc: 12.50%,  total acc: 59.38%   [EVAL] batch:   50 | acc: 0.00%,  total acc: 58.21%   [EVAL] batch:   51 | acc: 6.25%,  total acc: 57.21%   [EVAL] batch:   52 | acc: 0.00%,  total acc: 56.13%   [EVAL] batch:   53 | acc: 25.00%,  total acc: 55.56%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 56.25%   [EVAL] batch:   55 | acc: 68.75%,  total acc: 56.47%   [EVAL] batch:   56 | acc: 62.50%,  total acc: 56.58%   [EVAL] batch:   57 | acc: 56.25%,  total acc: 56.57%   [EVAL] batch:   58 | acc: 68.75%,  total acc: 56.78%   [EVAL] batch:   59 | acc: 62.50%,  total acc: 56.88%   [EVAL] batch:   60 | acc: 56.25%,  total acc: 56.86%   [EVAL] batch:   61 | acc: 75.00%,  total acc: 57.16%   [EVAL] batch:   62 | acc: 68.75%,  total acc: 57.34%   [EVAL] batch:   63 | acc: 68.75%,  total acc: 57.52%   [EVAL] batch:   64 | acc: 56.25%,  total acc: 57.50%   [EVAL] batch:   65 | acc: 81.25%,  total acc: 57.86%   [EVAL] batch:   66 | acc: 62.50%,  total acc: 57.93%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 58.55%   [EVAL] batch:   68 | acc: 87.50%,  total acc: 58.97%   [EVAL] batch:   69 | acc: 56.25%,  total acc: 58.93%   [EVAL] batch:   70 | acc: 62.50%,  total acc: 58.98%   [EVAL] batch:   71 | acc: 62.50%,  total acc: 59.03%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 59.59%   [EVAL] batch:   73 | acc: 100.00%,  total acc: 60.14%   [EVAL] batch:   74 | acc: 100.00%,  total acc: 60.67%   [EVAL] batch:   75 | acc: 100.00%,  total acc: 61.18%   [EVAL] batch:   76 | acc: 100.00%,  total acc: 61.69%   [EVAL] batch:   77 | acc: 87.50%,  total acc: 62.02%   [EVAL] batch:   78 | acc: 31.25%,  total acc: 61.63%   [EVAL] batch:   79 | acc: 56.25%,  total acc: 61.56%   [EVAL] batch:   80 | acc: 50.00%,  total acc: 61.42%   [EVAL] batch:   81 | acc: 43.75%,  total acc: 61.20%   [EVAL] batch:   82 | acc: 56.25%,  total acc: 61.14%   [EVAL] batch:   83 | acc: 50.00%,  total acc: 61.01%   [EVAL] batch:   84 | acc: 62.50%,  total acc: 61.03%   [EVAL] batch:   85 | acc: 75.00%,  total acc: 61.19%   [EVAL] batch:   86 | acc: 50.00%,  total acc: 61.06%   [EVAL] batch:   87 | acc: 68.75%,  total acc: 61.15%   [EVAL] batch:   88 | acc: 81.25%,  total acc: 61.38%   [EVAL] batch:   89 | acc: 87.50%,  total acc: 61.67%   [EVAL] batch:   90 | acc: 81.25%,  total acc: 61.88%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 62.30%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 62.70%   [EVAL] batch:   93 | acc: 100.00%,  total acc: 63.10%   [EVAL] batch:   94 | acc: 87.50%,  total acc: 63.36%   [EVAL] batch:   95 | acc: 81.25%,  total acc: 63.54%   [EVAL] batch:   96 | acc: 43.75%,  total acc: 63.34%   [EVAL] batch:   97 | acc: 68.75%,  total acc: 63.39%   [EVAL] batch:   98 | acc: 50.00%,  total acc: 63.26%   [EVAL] batch:   99 | acc: 12.50%,  total acc: 62.75%   
cur_acc:  ['0.8580', '0.7083', '0.7232', '0.8299', '0.6591']
his_acc:  ['0.8580', '0.8285', '0.7500', '0.6723', '0.6275']
CurrentTrain: epoch  0, batch     0 | loss: 5.4524651CurrentTrain: epoch  0, batch     1 | loss: 6.9782157CurrentTrain: epoch  1, batch     0 | loss: 5.1394105CurrentTrain: epoch  1, batch     1 | loss: 3.1492450CurrentTrain: epoch  2, batch     0 | loss: 4.2489595CurrentTrain: epoch  2, batch     1 | loss: 2.9210291CurrentTrain: epoch  3, batch     0 | loss: 3.4714475CurrentTrain: epoch  3, batch     1 | loss: 3.7209313CurrentTrain: epoch  4, batch     0 | loss: 3.0462945CurrentTrain: epoch  4, batch     1 | loss: 2.9461627CurrentTrain: epoch  5, batch     0 | loss: 3.2292435CurrentTrain: epoch  5, batch     1 | loss: 2.3116887CurrentTrain: epoch  6, batch     0 | loss: 2.5216360CurrentTrain: epoch  6, batch     1 | loss: 3.4549627CurrentTrain: epoch  7, batch     0 | loss: 2.7958469CurrentTrain: epoch  7, batch     1 | loss: 2.6798787CurrentTrain: epoch  8, batch     0 | loss: 2.7624593CurrentTrain: epoch  8, batch     1 | loss: 2.1677022CurrentTrain: epoch  9, batch     0 | loss: 2.3858614CurrentTrain: epoch  9, batch     1 | loss: 2.2951889
#### person country of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person country of birth" means a person was born in a country.
Here is an example:
Relation: person country of birth
Context: in testimony by satellite link from germany to a house of representatives ' panel , murat kurnaz recounted his five-year detention , alleging a wide range of torture and abuse .
Head Entity: murat kurnaz
Tail Entity: germany
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person country of birth":
Relation: person country of birth  
Context: During the interview, the renowned artist shared his experiences growing up in the vibrant streets of Barcelona, which he credits as the foundation of his creative journey.  
Head Entity: the renowned artist  
Tail Entity: Barcelona  

Relation: person country of birth  
Context: In her autobiography, the famous scientist details her early life in Tokyo, where she developed a passion for technology and innovation.  
Head Entity: the famous scientist  
Tail Entity: Tokyo  
#### organization website ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization website" means an organization 's website is an url.
Here is an example:
Relation: organization website
Context: http://www.pentaxsailaway.com/ paul j. sail away with pentax sweepstakes
Head Entity: pentax
Tail Entity: http://www.pentaxsailaway.com/
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization website":
Relation: organization website  
Context: Visit the official site at https://www.techinnovators.com for more information on their latest products.  
Head Entity: Tech Innovators  
Tail Entity: https://www.techinnovators.com  

Relation: organization website  
Context: For updates and news, check out the website of Green Earth Initiative at http://www.greenearth.org.  
Head Entity: Green Earth Initiative  
Tail Entity: http://www.greenearth.org  
#### organization shareholders ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization shareholders" means an organization was invested by a person.
Here is an example:
Relation: organization shareholders
Context: marsans already owns the madrid-based air comet and 95 percent of aerolineas argentinas .
Head Entity: aerolineas argentinas
Tail Entity: marsans
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization shareholders":
Relation: organization shareholders  
Context: tech giant Apple has seen significant investments from Warren Buffett's Berkshire Hathaway, which now holds a substantial stake in the company.  
Head Entity: Apple  
Tail Entity: Berkshire Hathaway  

Relation: organization shareholders  
Context: The investment firm Vanguard Group has increased its holdings in the multinational corporation Procter & Gamble, reflecting confidence in its long-term growth.  
Head Entity: Procter & Gamble  
Tail Entity: Vanguard Group  
#### organization dissolved ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization dissolved" means an organization was dissolved in a date.
Here is an example:
Relation: organization dissolved
Context: the tse suffered its worst-ever system crash in november 2005 which paralyzed the world 's second largest bourse and forced it to shelve plan for a listing of its own .
Head Entity: tse
Tail Entity: november 2005
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization dissolved":
Relation: organization dissolved  
Context: the once-prominent tech startup, Innovatech, officially ceased operations in March 2020 after struggling to secure funding.  
Head Entity: Innovatech  
Tail Entity: March 2020  

Relation: organization dissolved  
Context: after years of financial difficulties, the local arts council announced its closure in January 2019, leaving many artists without support.  
Head Entity: local arts council  
Tail Entity: January 2019  
#### organization founded by ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization founded by" means an organization was found by a person.
Here is an example:
Relation: organization founded by
Context: born in new york , she inherited most of her fortune from her father , ted arison , who founded carnival cruise lines and also owned the miami heat basketball team .
Head Entity: carnival cruise lines
Tail Entity: ted arison
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization founded by":
Relation: organization founded by  
Context: in 1975, steve jobs and steve wozniak started apple inc., which revolutionized the personal computer industry and changed the way people interact with technology.  
Head Entity: apple inc.  
Tail Entity: steve jobs  

Relation: organization founded by  
Context: the famous fashion brand gucci was established in 1921 by guccio gucci in florence, italy, and has since become a symbol of luxury and style.  
Head Entity: gucci  
Tail Entity: guccio gucci  
Mixup data size:  6685
MixupTrain:  epoch  0, batch     0 | loss: 4.2922435MixupTrain:  epoch  0, batch     1 | loss: 4.2271757MixupTrain:  epoch  0, batch     2 | loss: 3.7594314MixupTrain:  epoch  0, batch     3 | loss: 4.0052910MixupTrain:  epoch  0, batch     4 | loss: 4.2200947MixupTrain:  epoch  0, batch     5 | loss: 3.8907132MixupTrain:  epoch  0, batch     6 | loss: 3.7881527MixupTrain:  epoch  0, batch     7 | loss: 3.4345918MixupTrain:  epoch  0, batch     8 | loss: 3.4794769MixupTrain:  epoch  0, batch     9 | loss: 4.0976763MixupTrain:  epoch  0, batch    10 | loss: 3.7552485MixupTrain:  epoch  0, batch    11 | loss: 3.4922023MixupTrain:  epoch  0, batch    12 | loss: 3.5786991MixupTrain:  epoch  0, batch    13 | loss: 3.2724741MixupTrain:  epoch  0, batch    14 | loss: 3.1764615MixupTrain:  epoch  0, batch    15 | loss: 3.8731060MixupTrain:  epoch  0, batch    16 | loss: 3.4445465MixupTrain:  epoch  0, batch    17 | loss: 3.3947134MixupTrain:  epoch  0, batch    18 | loss: 3.3671582MixupTrain:  epoch  0, batch    19 | loss: 3.9562807MixupTrain:  epoch  0, batch    20 | loss: 3.5761557MixupTrain:  epoch  0, batch    21 | loss: 3.9108806MixupTrain:  epoch  0, batch    22 | loss: 3.4122353MixupTrain:  epoch  0, batch    23 | loss: 3.5702939MixupTrain:  epoch  0, batch    24 | loss: 3.1643476MixupTrain:  epoch  0, batch    25 | loss: 3.1704385MixupTrain:  epoch  0, batch    26 | loss: 3.4000292MixupTrain:  epoch  0, batch    27 | loss: 3.6267152MixupTrain:  epoch  0, batch    28 | loss: 3.3624678MixupTrain:  epoch  0, batch    29 | loss: 3.2011018MixupTrain:  epoch  0, batch    30 | loss: 3.1296389MixupTrain:  epoch  0, batch    31 | loss: 3.6290679MixupTrain:  epoch  0, batch    32 | loss: 2.9623528MixupTrain:  epoch  0, batch    33 | loss: 3.4446244MixupTrain:  epoch  0, batch    34 | loss: 3.4987445MixupTrain:  epoch  0, batch    35 | loss: 3.3201859MixupTrain:  epoch  0, batch    36 | loss: 3.5293837MixupTrain:  epoch  0, batch    37 | loss: 3.3764272MixupTrain:  epoch  0, batch    38 | loss: 3.0947714MixupTrain:  epoch  0, batch    39 | loss: 3.5786052MixupTrain:  epoch  0, batch    40 | loss: 3.5651879MixupTrain:  epoch  0, batch    41 | loss: 3.3731761MixupTrain:  epoch  0, batch    42 | loss: 3.4222450MixupTrain:  epoch  0, batch    43 | loss: 3.4781649MixupTrain:  epoch  0, batch    44 | loss: 3.3723736MixupTrain:  epoch  0, batch    45 | loss: 3.3933790MixupTrain:  epoch  0, batch    46 | loss: 3.6098776MixupTrain:  epoch  0, batch    47 | loss: 3.3628659MixupTrain:  epoch  0, batch    48 | loss: 3.4113386MixupTrain:  epoch  0, batch    49 | loss: 3.3029487MixupTrain:  epoch  0, batch    50 | loss: 3.0996919MixupTrain:  epoch  0, batch    51 | loss: 2.9651999MixupTrain:  epoch  0, batch    52 | loss: 3.4162092MixupTrain:  epoch  0, batch    53 | loss: 3.2320824MixupTrain:  epoch  0, batch    54 | loss: 2.9731400MixupTrain:  epoch  0, batch    55 | loss: 2.9717500MixupTrain:  epoch  0, batch    56 | loss: 3.1776948MixupTrain:  epoch  0, batch    57 | loss: 3.0691729MixupTrain:  epoch  0, batch    58 | loss: 3.3161955MixupTrain:  epoch  0, batch    59 | loss: 3.0444555MixupTrain:  epoch  0, batch    60 | loss: 3.4748387MixupTrain:  epoch  0, batch    61 | loss: 3.2636194MixupTrain:  epoch  0, batch    62 | loss: 3.4358492MixupTrain:  epoch  0, batch    63 | loss: 3.0649626MixupTrain:  epoch  0, batch    64 | loss: 3.1054168MixupTrain:  epoch  0, batch    65 | loss: 3.1488130MixupTrain:  epoch  0, batch    66 | loss: 3.2086077MixupTrain:  epoch  0, batch    67 | loss: 3.4283781MixupTrain:  epoch  0, batch    68 | loss: 3.3138890MixupTrain:  epoch  0, batch    69 | loss: 3.3629384MixupTrain:  epoch  0, batch    70 | loss: 3.3358099MixupTrain:  epoch  0, batch    71 | loss: 3.0387816MixupTrain:  epoch  0, batch    72 | loss: 3.0193257MixupTrain:  epoch  0, batch    73 | loss: 3.1668570MixupTrain:  epoch  0, batch    74 | loss: 3.1281195MixupTrain:  epoch  0, batch    75 | loss: 2.9869926MixupTrain:  epoch  0, batch    76 | loss: 3.1934433MixupTrain:  epoch  0, batch    77 | loss: 3.2767220MixupTrain:  epoch  0, batch    78 | loss: 3.2733469MixupTrain:  epoch  0, batch    79 | loss: 3.2691488MixupTrain:  epoch  0, batch    80 | loss: 3.1490226MixupTrain:  epoch  0, batch    81 | loss: 3.4616618MixupTrain:  epoch  0, batch    82 | loss: 3.1102362MixupTrain:  epoch  0, batch    83 | loss: 3.3406425MixupTrain:  epoch  0, batch    84 | loss: 3.3296180MixupTrain:  epoch  0, batch    85 | loss: 2.8712034MixupTrain:  epoch  0, batch    86 | loss: 3.0862129MixupTrain:  epoch  0, batch    87 | loss: 3.3231363MixupTrain:  epoch  0, batch    88 | loss: 3.0935550MixupTrain:  epoch  0, batch    89 | loss: 3.1554098MixupTrain:  epoch  0, batch    90 | loss: 3.3831930MixupTrain:  epoch  0, batch    91 | loss: 3.1856728MixupTrain:  epoch  0, batch    92 | loss: 3.7740488MixupTrain:  epoch  0, batch    93 | loss: 3.2934120MixupTrain:  epoch  0, batch    94 | loss: 3.0825171MixupTrain:  epoch  0, batch    95 | loss: 3.0701971MixupTrain:  epoch  0, batch    96 | loss: 3.1747651MixupTrain:  epoch  0, batch    97 | loss: 3.0826182MixupTrain:  epoch  0, batch    98 | loss: 3.2577279MixupTrain:  epoch  0, batch    99 | loss: 3.2125525MixupTrain:  epoch  0, batch   100 | loss: 3.1512392MixupTrain:  epoch  0, batch   101 | loss: 2.8566432MixupTrain:  epoch  0, batch   102 | loss: 3.2195969MixupTrain:  epoch  0, batch   103 | loss: 3.0710859MixupTrain:  epoch  0, batch   104 | loss: 3.3189182MixupTrain:  epoch  0, batch   105 | loss: 3.4232802MixupTrain:  epoch  0, batch   106 | loss: 3.1106360MixupTrain:  epoch  0, batch   107 | loss: 2.9017029MixupTrain:  epoch  0, batch   108 | loss: 3.0413969MixupTrain:  epoch  0, batch   109 | loss: 3.2517397MixupTrain:  epoch  0, batch   110 | loss: 3.2447906MixupTrain:  epoch  0, batch   111 | loss: 2.8390126MixupTrain:  epoch  0, batch   112 | loss: 3.2508366MixupTrain:  epoch  0, batch   113 | loss: 3.2092566MixupTrain:  epoch  0, batch   114 | loss: 3.1829309MixupTrain:  epoch  0, batch   115 | loss: 2.8344464MixupTrain:  epoch  0, batch   116 | loss: 3.1326509MixupTrain:  epoch  0, batch   117 | loss: 2.9508791MixupTrain:  epoch  0, batch   118 | loss: 3.0530491MixupTrain:  epoch  0, batch   119 | loss: 2.9791493MixupTrain:  epoch  0, batch   120 | loss: 3.0464325MixupTrain:  epoch  0, batch   121 | loss: 2.9233198MixupTrain:  epoch  0, batch   122 | loss: 3.1385574MixupTrain:  epoch  0, batch   123 | loss: 3.2063200MixupTrain:  epoch  0, batch   124 | loss: 3.1229534MixupTrain:  epoch  0, batch   125 | loss: 3.2262216MixupTrain:  epoch  0, batch   126 | loss: 3.2980027MixupTrain:  epoch  0, batch   127 | loss: 3.1120584MixupTrain:  epoch  0, batch   128 | loss: 2.9958725MixupTrain:  epoch  0, batch   129 | loss: 3.0112414MixupTrain:  epoch  0, batch   130 | loss: 3.0970244MixupTrain:  epoch  0, batch   131 | loss: 3.1546285MixupTrain:  epoch  0, batch   132 | loss: 2.8631973MixupTrain:  epoch  0, batch   133 | loss: 3.0876012MixupTrain:  epoch  0, batch   134 | loss: 3.1454115MixupTrain:  epoch  0, batch   135 | loss: 3.1681602MixupTrain:  epoch  0, batch   136 | loss: 3.0751510MixupTrain:  epoch  0, batch   137 | loss: 3.0478697MixupTrain:  epoch  0, batch   138 | loss: 2.8891401MixupTrain:  epoch  0, batch   139 | loss: 3.0484428MixupTrain:  epoch  0, batch   140 | loss: 3.0077386MixupTrain:  epoch  0, batch   141 | loss: 3.0216761MixupTrain:  epoch  0, batch   142 | loss: 3.3412018MixupTrain:  epoch  0, batch   143 | loss: 3.1655507MixupTrain:  epoch  0, batch   144 | loss: 2.8174593MixupTrain:  epoch  0, batch   145 | loss: 3.3109341MixupTrain:  epoch  0, batch   146 | loss: 2.9565442MixupTrain:  epoch  0, batch   147 | loss: 3.2333360MixupTrain:  epoch  0, batch   148 | loss: 2.9691324MixupTrain:  epoch  0, batch   149 | loss: 2.9308224MixupTrain:  epoch  0, batch   150 | loss: 3.2866776MixupTrain:  epoch  0, batch   151 | loss: 2.9613872MixupTrain:  epoch  0, batch   152 | loss: 3.1456077MixupTrain:  epoch  0, batch   153 | loss: 2.9080195MixupTrain:  epoch  0, batch   154 | loss: 3.0810294MixupTrain:  epoch  0, batch   155 | loss: 2.8523457MixupTrain:  epoch  0, batch   156 | loss: 3.1584191MixupTrain:  epoch  0, batch   157 | loss: 3.2444649MixupTrain:  epoch  0, batch   158 | loss: 3.2165804MixupTrain:  epoch  0, batch   159 | loss: 3.1875339MixupTrain:  epoch  0, batch   160 | loss: 2.7859211MixupTrain:  epoch  0, batch   161 | loss: 2.8154502MixupTrain:  epoch  0, batch   162 | loss: 2.9207926MixupTrain:  epoch  0, batch   163 | loss: 3.1076207MixupTrain:  epoch  0, batch   164 | loss: 2.9970465MixupTrain:  epoch  0, batch   165 | loss: 2.9427338MixupTrain:  epoch  0, batch   166 | loss: 2.9387722MixupTrain:  epoch  0, batch   167 | loss: 3.1075342MixupTrain:  epoch  0, batch   168 | loss: 3.1804581MixupTrain:  epoch  0, batch   169 | loss: 3.1659408MixupTrain:  epoch  0, batch   170 | loss: 3.0432889MixupTrain:  epoch  0, batch   171 | loss: 3.0816231MixupTrain:  epoch  0, batch   172 | loss: 3.0441337MixupTrain:  epoch  0, batch   173 | loss: 3.0098543MixupTrain:  epoch  0, batch   174 | loss: 3.0326874MixupTrain:  epoch  0, batch   175 | loss: 3.1195807MixupTrain:  epoch  0, batch   176 | loss: 3.0160456MixupTrain:  epoch  0, batch   177 | loss: 3.0642853MixupTrain:  epoch  0, batch   178 | loss: 2.9071677MixupTrain:  epoch  0, batch   179 | loss: 3.0079098MixupTrain:  epoch  0, batch   180 | loss: 3.1171503MixupTrain:  epoch  0, batch   181 | loss: 3.2153945MixupTrain:  epoch  0, batch   182 | loss: 3.0136106MixupTrain:  epoch  0, batch   183 | loss: 3.2820916MixupTrain:  epoch  0, batch   184 | loss: 2.8959606MixupTrain:  epoch  0, batch   185 | loss: 2.9849141MixupTrain:  epoch  0, batch   186 | loss: 3.1467431MixupTrain:  epoch  0, batch   187 | loss: 3.0533800MixupTrain:  epoch  0, batch   188 | loss: 2.9546144MixupTrain:  epoch  0, batch   189 | loss: 3.1968699MixupTrain:  epoch  0, batch   190 | loss: 2.7555346MixupTrain:  epoch  0, batch   191 | loss: 2.9396710MixupTrain:  epoch  0, batch   192 | loss: 2.9796338MixupTrain:  epoch  0, batch   193 | loss: 3.1053247MixupTrain:  epoch  0, batch   194 | loss: 3.1438282MixupTrain:  epoch  0, batch   195 | loss: 3.0031898MixupTrain:  epoch  0, batch   196 | loss: 3.1092947MixupTrain:  epoch  0, batch   197 | loss: 3.0146589MixupTrain:  epoch  0, batch   198 | loss: 2.7157650MixupTrain:  epoch  0, batch   199 | loss: 3.0667357MixupTrain:  epoch  0, batch   200 | loss: 3.0100918MixupTrain:  epoch  0, batch   201 | loss: 2.9543164MixupTrain:  epoch  0, batch   202 | loss: 3.1557312MixupTrain:  epoch  0, batch   203 | loss: 3.1322172MixupTrain:  epoch  0, batch   204 | loss: 3.2257836MixupTrain:  epoch  0, batch   205 | loss: 2.9787838MixupTrain:  epoch  0, batch   206 | loss: 3.1094806MixupTrain:  epoch  0, batch   207 | loss: 2.9326873MixupTrain:  epoch  0, batch   208 | loss: 2.8970437MixupTrain:  epoch  0, batch   209 | loss: 3.1544223MixupTrain:  epoch  0, batch   210 | loss: 2.8607783MixupTrain:  epoch  0, batch   211 | loss: 3.0820317MixupTrain:  epoch  0, batch   212 | loss: 2.9869075MixupTrain:  epoch  0, batch   213 | loss: 2.8360391MixupTrain:  epoch  0, batch   214 | loss: 3.0401235MixupTrain:  epoch  0, batch   215 | loss: 3.0513449MixupTrain:  epoch  0, batch   216 | loss: 3.0538766MixupTrain:  epoch  0, batch   217 | loss: 2.9343569MixupTrain:  epoch  0, batch   218 | loss: 3.0262227MixupTrain:  epoch  0, batch   219 | loss: 2.9020698MixupTrain:  epoch  0, batch   220 | loss: 3.0035286MixupTrain:  epoch  0, batch   221 | loss: 3.0276127MixupTrain:  epoch  0, batch   222 | loss: 3.1368804MixupTrain:  epoch  0, batch   223 | loss: 3.0405316MixupTrain:  epoch  0, batch   224 | loss: 3.3844128MixupTrain:  epoch  0, batch   225 | loss: 3.1122937MixupTrain:  epoch  0, batch   226 | loss: 3.0256457MixupTrain:  epoch  0, batch   227 | loss: 3.1091385MixupTrain:  epoch  0, batch   228 | loss: 3.0179186MixupTrain:  epoch  0, batch   229 | loss: 3.0782061MixupTrain:  epoch  0, batch   230 | loss: 2.9086986MixupTrain:  epoch  0, batch   231 | loss: 2.9033108MixupTrain:  epoch  0, batch   232 | loss: 2.9971516MixupTrain:  epoch  0, batch   233 | loss: 3.1110339MixupTrain:  epoch  0, batch   234 | loss: 2.9542747MixupTrain:  epoch  0, batch   235 | loss: 2.9640493MixupTrain:  epoch  0, batch   236 | loss: 3.0658941MixupTrain:  epoch  0, batch   237 | loss: 2.9475729MixupTrain:  epoch  0, batch   238 | loss: 2.8919110MixupTrain:  epoch  0, batch   239 | loss: 2.9667480MixupTrain:  epoch  0, batch   240 | loss: 2.9059112MixupTrain:  epoch  0, batch   241 | loss: 2.9583111MixupTrain:  epoch  0, batch   242 | loss: 2.9157181MixupTrain:  epoch  0, batch   243 | loss: 2.8949127MixupTrain:  epoch  0, batch   244 | loss: 3.2114649MixupTrain:  epoch  0, batch   245 | loss: 2.9026432MixupTrain:  epoch  0, batch   246 | loss: 2.9566219MixupTrain:  epoch  0, batch   247 | loss: 2.8793147MixupTrain:  epoch  0, batch   248 | loss: 3.0029306MixupTrain:  epoch  0, batch   249 | loss: 3.0264249MixupTrain:  epoch  0, batch   250 | loss: 3.0668910MixupTrain:  epoch  0, batch   251 | loss: 2.7041974MixupTrain:  epoch  0, batch   252 | loss: 3.1127446MixupTrain:  epoch  0, batch   253 | loss: 2.7883363MixupTrain:  epoch  0, batch   254 | loss: 2.9912095MixupTrain:  epoch  0, batch   255 | loss: 2.8366537MixupTrain:  epoch  0, batch   256 | loss: 2.8958330MixupTrain:  epoch  0, batch   257 | loss: 2.7686231MixupTrain:  epoch  0, batch   258 | loss: 2.9942241MixupTrain:  epoch  0, batch   259 | loss: 2.9713950MixupTrain:  epoch  0, batch   260 | loss: 2.9480176MixupTrain:  epoch  0, batch   261 | loss: 3.0705566MixupTrain:  epoch  0, batch   262 | loss: 2.8478847MixupTrain:  epoch  0, batch   263 | loss: 2.9653411MixupTrain:  epoch  0, batch   264 | loss: 2.8689704MixupTrain:  epoch  0, batch   265 | loss: 2.8351579MixupTrain:  epoch  0, batch   266 | loss: 3.1818817MixupTrain:  epoch  0, batch   267 | loss: 2.8620758MixupTrain:  epoch  0, batch   268 | loss: 2.8936784MixupTrain:  epoch  0, batch   269 | loss: 3.0182428MixupTrain:  epoch  0, batch   270 | loss: 3.0574389MixupTrain:  epoch  0, batch   271 | loss: 2.9120984MixupTrain:  epoch  0, batch   272 | loss: 3.0044620MixupTrain:  epoch  0, batch   273 | loss: 3.2829914MixupTrain:  epoch  0, batch   274 | loss: 2.9797270MixupTrain:  epoch  0, batch   275 | loss: 2.9045429MixupTrain:  epoch  0, batch   276 | loss: 2.9010201MixupTrain:  epoch  0, batch   277 | loss: 3.0538931MixupTrain:  epoch  0, batch   278 | loss: 3.0694892MixupTrain:  epoch  0, batch   279 | loss: 3.2225490MixupTrain:  epoch  0, batch   280 | loss: 2.9927180MixupTrain:  epoch  0, batch   281 | loss: 2.9589586MixupTrain:  epoch  0, batch   282 | loss: 2.8577194MixupTrain:  epoch  0, batch   283 | loss: 2.8509510MixupTrain:  epoch  0, batch   284 | loss: 2.9794536MixupTrain:  epoch  0, batch   285 | loss: 2.7943006MixupTrain:  epoch  0, batch   286 | loss: 3.1350579MixupTrain:  epoch  0, batch   287 | loss: 2.9637465MixupTrain:  epoch  0, batch   288 | loss: 3.2258964MixupTrain:  epoch  0, batch   289 | loss: 2.9330831MixupTrain:  epoch  0, batch   290 | loss: 3.0767822MixupTrain:  epoch  0, batch   291 | loss: 2.8813546MixupTrain:  epoch  0, batch   292 | loss: 3.2007120MixupTrain:  epoch  0, batch   293 | loss: 3.0990114MixupTrain:  epoch  0, batch   294 | loss: 2.8199952MixupTrain:  epoch  0, batch   295 | loss: 2.9307003MixupTrain:  epoch  0, batch   296 | loss: 3.1685333MixupTrain:  epoch  0, batch   297 | loss: 2.9604712MixupTrain:  epoch  0, batch   298 | loss: 2.8320680MixupTrain:  epoch  0, batch   299 | loss: 2.8071251MixupTrain:  epoch  0, batch   300 | loss: 2.9024482MixupTrain:  epoch  0, batch   301 | loss: 2.9746249MixupTrain:  epoch  0, batch   302 | loss: 3.0714731MixupTrain:  epoch  0, batch   303 | loss: 2.8393154MixupTrain:  epoch  0, batch   304 | loss: 2.9192619MixupTrain:  epoch  0, batch   305 | loss: 2.9163213MixupTrain:  epoch  0, batch   306 | loss: 2.7924848MixupTrain:  epoch  0, batch   307 | loss: 2.8376107MixupTrain:  epoch  0, batch   308 | loss: 2.8600788MixupTrain:  epoch  0, batch   309 | loss: 3.0685985MixupTrain:  epoch  0, batch   310 | loss: 2.8297269MixupTrain:  epoch  0, batch   311 | loss: 3.0465198MixupTrain:  epoch  0, batch   312 | loss: 2.9991994MixupTrain:  epoch  0, batch   313 | loss: 3.0119872MixupTrain:  epoch  0, batch   314 | loss: 2.9953265MixupTrain:  epoch  0, batch   315 | loss: 3.1443019MixupTrain:  epoch  0, batch   316 | loss: 3.0077140MixupTrain:  epoch  0, batch   317 | loss: 3.1575065MixupTrain:  epoch  0, batch   318 | loss: 2.8843660MixupTrain:  epoch  0, batch   319 | loss: 3.1741514MixupTrain:  epoch  0, batch   320 | loss: 2.8504632MixupTrain:  epoch  0, batch   321 | loss: 3.0595195MixupTrain:  epoch  0, batch   322 | loss: 3.0800650MixupTrain:  epoch  0, batch   323 | loss: 3.0159411MixupTrain:  epoch  0, batch   324 | loss: 2.9545455MixupTrain:  epoch  0, batch   325 | loss: 3.1207495MixupTrain:  epoch  0, batch   326 | loss: 3.0153985MixupTrain:  epoch  0, batch   327 | loss: 3.1029358MixupTrain:  epoch  0, batch   328 | loss: 2.9350204MixupTrain:  epoch  0, batch   329 | loss: 2.8032646MixupTrain:  epoch  0, batch   330 | loss: 2.7764363MixupTrain:  epoch  0, batch   331 | loss: 2.9535816MixupTrain:  epoch  0, batch   332 | loss: 2.7808881MixupTrain:  epoch  0, batch   333 | loss: 3.0612109MixupTrain:  epoch  0, batch   334 | loss: 2.9794478MixupTrain:  epoch  0, batch   335 | loss: 3.0556550MixupTrain:  epoch  0, batch   336 | loss: 2.9603653MixupTrain:  epoch  0, batch   337 | loss: 3.0316610MixupTrain:  epoch  0, batch   338 | loss: 3.0450978MixupTrain:  epoch  0, batch   339 | loss: 2.8436365MixupTrain:  epoch  0, batch   340 | loss: 3.0468464MixupTrain:  epoch  0, batch   341 | loss: 2.9271140MixupTrain:  epoch  0, batch   342 | loss: 3.0955906MixupTrain:  epoch  0, batch   343 | loss: 3.0322456MixupTrain:  epoch  0, batch   344 | loss: 2.9245150MixupTrain:  epoch  0, batch   345 | loss: 2.9161947MixupTrain:  epoch  0, batch   346 | loss: 2.9426241MixupTrain:  epoch  0, batch   347 | loss: 2.9882226MixupTrain:  epoch  0, batch   348 | loss: 2.9585648MixupTrain:  epoch  0, batch   349 | loss: 2.9052277MixupTrain:  epoch  0, batch   350 | loss: 2.9076724MixupTrain:  epoch  0, batch   351 | loss: 2.8449724MixupTrain:  epoch  0, batch   352 | loss: 3.0977285MixupTrain:  epoch  0, batch   353 | loss: 2.7916000MixupTrain:  epoch  0, batch   354 | loss: 2.8663614MixupTrain:  epoch  0, batch   355 | loss: 2.8437223MixupTrain:  epoch  0, batch   356 | loss: 2.8737006MixupTrain:  epoch  0, batch   357 | loss: 3.0308483MixupTrain:  epoch  0, batch   358 | loss: 3.0681536MixupTrain:  epoch  0, batch   359 | loss: 2.8166585MixupTrain:  epoch  0, batch   360 | loss: 2.6909237MixupTrain:  epoch  0, batch   361 | loss: 2.8340602MixupTrain:  epoch  0, batch   362 | loss: 2.9636102MixupTrain:  epoch  0, batch   363 | loss: 2.9000268MixupTrain:  epoch  0, batch   364 | loss: 2.9006209MixupTrain:  epoch  0, batch   365 | loss: 2.8809376MixupTrain:  epoch  0, batch   366 | loss: 2.9448819MixupTrain:  epoch  0, batch   367 | loss: 2.8520155MixupTrain:  epoch  0, batch   368 | loss: 3.0029685MixupTrain:  epoch  0, batch   369 | loss: 2.9499748MixupTrain:  epoch  0, batch   370 | loss: 2.7529063MixupTrain:  epoch  0, batch   371 | loss: 2.7339578MixupTrain:  epoch  0, batch   372 | loss: 2.9827185MixupTrain:  epoch  0, batch   373 | loss: 2.9301169MixupTrain:  epoch  0, batch   374 | loss: 3.0779655MixupTrain:  epoch  0, batch   375 | loss: 2.9175754MixupTrain:  epoch  0, batch   376 | loss: 3.0077305MixupTrain:  epoch  0, batch   377 | loss: 2.8903954MixupTrain:  epoch  0, batch   378 | loss: 3.0075364MixupTrain:  epoch  0, batch   379 | loss: 2.7826874MixupTrain:  epoch  0, batch   380 | loss: 3.0097170MixupTrain:  epoch  0, batch   381 | loss: 2.8260231MixupTrain:  epoch  0, batch   382 | loss: 2.9499321MixupTrain:  epoch  0, batch   383 | loss: 2.7099223MixupTrain:  epoch  0, batch   384 | loss: 2.9441757MixupTrain:  epoch  0, batch   385 | loss: 2.8493230MixupTrain:  epoch  0, batch   386 | loss: 3.0108612MixupTrain:  epoch  0, batch   387 | loss: 3.0489373MixupTrain:  epoch  0, batch   388 | loss: 2.7473557MixupTrain:  epoch  0, batch   389 | loss: 2.9766190MixupTrain:  epoch  0, batch   390 | loss: 2.8386164MixupTrain:  epoch  0, batch   391 | loss: 2.9580483MixupTrain:  epoch  0, batch   392 | loss: 3.1076393MixupTrain:  epoch  0, batch   393 | loss: 2.8812029MixupTrain:  epoch  0, batch   394 | loss: 2.7996812MixupTrain:  epoch  0, batch   395 | loss: 2.9799559MixupTrain:  epoch  0, batch   396 | loss: 2.9472063MixupTrain:  epoch  0, batch   397 | loss: 2.7548947MixupTrain:  epoch  0, batch   398 | loss: 2.9915614MixupTrain:  epoch  0, batch   399 | loss: 2.9263620MixupTrain:  epoch  0, batch   400 | loss: 3.0208380MixupTrain:  epoch  0, batch   401 | loss: 2.9699383MixupTrain:  epoch  0, batch   402 | loss: 3.0315433MixupTrain:  epoch  0, batch   403 | loss: 3.0396442MixupTrain:  epoch  0, batch   404 | loss: 2.9887118MixupTrain:  epoch  0, batch   405 | loss: 2.8016505MixupTrain:  epoch  0, batch   406 | loss: 2.7179182MixupTrain:  epoch  0, batch   407 | loss: 2.7498765MixupTrain:  epoch  0, batch   408 | loss: 2.9450784MixupTrain:  epoch  0, batch   409 | loss: 2.8025179MixupTrain:  epoch  0, batch   410 | loss: 2.9433146MixupTrain:  epoch  0, batch   411 | loss: 2.8519843MixupTrain:  epoch  0, batch   412 | loss: 3.1144607MixupTrain:  epoch  0, batch   413 | loss: 2.9754028MixupTrain:  epoch  0, batch   414 | loss: 2.8699903MixupTrain:  epoch  0, batch   415 | loss: 3.0802121MixupTrain:  epoch  0, batch   416 | loss: 2.9444520MixupTrain:  epoch  0, batch   417 | loss: 2.7983823
MemoryTrain:  epoch  0, batch     0 | loss: 1.2237697MemoryTrain:  epoch  0, batch     1 | loss: 1.6563612MemoryTrain:  epoch  0, batch     2 | loss: 1.4254711MemoryTrain:  epoch  0, batch     3 | loss: 1.7490020MemoryTrain:  epoch  0, batch     4 | loss: 1.7616562MemoryTrain:  epoch  0, batch     5 | loss: 1.8368648MemoryTrain:  epoch  1, batch     0 | loss: 1.3573427MemoryTrain:  epoch  1, batch     1 | loss: 1.5744188MemoryTrain:  epoch  1, batch     2 | loss: 1.2812762MemoryTrain:  epoch  1, batch     3 | loss: 1.4277737MemoryTrain:  epoch  1, batch     4 | loss: 1.3143256MemoryTrain:  epoch  1, batch     5 | loss: 1.6579667MemoryTrain:  epoch  2, batch     0 | loss: 1.3511777MemoryTrain:  epoch  2, batch     1 | loss: 1.3931615MemoryTrain:  epoch  2, batch     2 | loss: 1.6005940MemoryTrain:  epoch  2, batch     3 | loss: 1.2218268MemoryTrain:  epoch  2, batch     4 | loss: 1.3375088MemoryTrain:  epoch  2, batch     5 | loss: 1.2883134MemoryTrain:  epoch  3, batch     0 | loss: 1.2525114MemoryTrain:  epoch  3, batch     1 | loss: 1.3900793MemoryTrain:  epoch  3, batch     2 | loss: 1.5410395MemoryTrain:  epoch  3, batch     3 | loss: 1.2549276MemoryTrain:  epoch  3, batch     4 | loss: 1.3143725MemoryTrain:  epoch  3, batch     5 | loss: 1.2337773MemoryTrain:  epoch  4, batch     0 | loss: 1.2157581MemoryTrain:  epoch  4, batch     1 | loss: 1.2261828MemoryTrain:  epoch  4, batch     2 | loss: 1.2971926MemoryTrain:  epoch  4, batch     3 | loss: 1.3577025MemoryTrain:  epoch  4, batch     4 | loss: 1.2877698MemoryTrain:  epoch  4, batch     5 | loss: 1.2554387MemoryTrain:  epoch  5, batch     0 | loss: 1.2121429MemoryTrain:  epoch  5, batch     1 | loss: 1.2836666MemoryTrain:  epoch  5, batch     2 | loss: 1.2473578MemoryTrain:  epoch  5, batch     3 | loss: 1.2429686MemoryTrain:  epoch  5, batch     4 | loss: 1.2459514MemoryTrain:  epoch  5, batch     5 | loss: 1.2560844MemoryTrain:  epoch  6, batch     0 | loss: 1.2471673MemoryTrain:  epoch  6, batch     1 | loss: 1.2177219MemoryTrain:  epoch  6, batch     2 | loss: 1.2015743MemoryTrain:  epoch  6, batch     3 | loss: 1.1721861MemoryTrain:  epoch  6, batch     4 | loss: 1.2132878MemoryTrain:  epoch  6, batch     5 | loss: 1.2365679MemoryTrain:  epoch  7, batch     0 | loss: 1.1946912MemoryTrain:  epoch  7, batch     1 | loss: 1.2017214MemoryTrain:  epoch  7, batch     2 | loss: 1.2255472MemoryTrain:  epoch  7, batch     3 | loss: 1.2223785MemoryTrain:  epoch  7, batch     4 | loss: 1.3189043MemoryTrain:  epoch  7, batch     5 | loss: 1.2261727MemoryTrain:  epoch  8, batch     0 | loss: 1.1933606MemoryTrain:  epoch  8, batch     1 | loss: 1.2090553MemoryTrain:  epoch  8, batch     2 | loss: 1.2009249MemoryTrain:  epoch  8, batch     3 | loss: 1.2040224MemoryTrain:  epoch  8, batch     4 | loss: 1.2395903MemoryTrain:  epoch  8, batch     5 | loss: 1.2093294MemoryTrain:  epoch  9, batch     0 | loss: 1.1946415MemoryTrain:  epoch  9, batch     1 | loss: 1.1981981MemoryTrain:  epoch  9, batch     2 | loss: 1.1909072MemoryTrain:  epoch  9, batch     3 | loss: 1.2184380MemoryTrain:  epoch  9, batch     4 | loss: 1.2197405MemoryTrain:  epoch  9, batch     5 | loss: 1.1956221
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 85.94%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 82.50%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 83.04%   [EVAL] batch:    7 | acc: 6.25%,  total acc: 73.44%   
[EVAL] batch:    0 | acc: 6.25%,  total acc: 6.25%   [EVAL] batch:    1 | acc: 25.00%,  total acc: 15.62%   [EVAL] batch:    2 | acc: 12.50%,  total acc: 14.58%   [EVAL] batch:    3 | acc: 6.25%,  total acc: 12.50%   [EVAL] batch:    4 | acc: 12.50%,  total acc: 12.50%   [EVAL] batch:    5 | acc: 18.75%,  total acc: 13.54%   [EVAL] batch:    6 | acc: 12.50%,  total acc: 13.39%   [EVAL] batch:    7 | acc: 12.50%,  total acc: 13.28%   [EVAL] batch:    8 | acc: 6.25%,  total acc: 12.50%   [EVAL] batch:    9 | acc: 12.50%,  total acc: 12.50%   [EVAL] batch:   10 | acc: 12.50%,  total acc: 12.50%   [EVAL] batch:   11 | acc: 37.50%,  total acc: 14.58%   [EVAL] batch:   12 | acc: 18.75%,  total acc: 14.90%   [EVAL] batch:   13 | acc: 31.25%,  total acc: 16.07%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 20.00%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 22.27%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 25.37%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 27.43%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 29.28%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 32.19%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 35.42%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 38.07%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 40.76%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 42.97%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 45.25%   [EVAL] batch:   25 | acc: 93.75%,  total acc: 47.12%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 48.84%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 50.67%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 52.16%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 53.12%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 54.23%   [EVAL] batch:   31 | acc: 87.50%,  total acc: 55.27%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 54.73%   [EVAL] batch:   33 | acc: 0.00%,  total acc: 53.12%   [EVAL] batch:   34 | acc: 0.00%,  total acc: 51.61%   [EVAL] batch:   35 | acc: 0.00%,  total acc: 50.17%   [EVAL] batch:   36 | acc: 0.00%,  total acc: 48.82%   [EVAL] batch:   37 | acc: 43.75%,  total acc: 48.68%   [EVAL] batch:   38 | acc: 75.00%,  total acc: 49.36%   [EVAL] batch:   39 | acc: 93.75%,  total acc: 50.47%   [EVAL] batch:   40 | acc: 68.75%,  total acc: 50.91%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 51.93%   [EVAL] batch:   42 | acc: 25.00%,  total acc: 51.31%   [EVAL] batch:   43 | acc: 0.00%,  total acc: 50.14%   [EVAL] batch:   44 | acc: 6.25%,  total acc: 49.17%   [EVAL] batch:   45 | acc: 0.00%,  total acc: 48.10%   [EVAL] batch:   46 | acc: 18.75%,  total acc: 47.47%   [EVAL] batch:   47 | acc: 75.00%,  total acc: 48.05%   [EVAL] batch:   48 | acc: 18.75%,  total acc: 47.45%   [EVAL] batch:   49 | acc: 12.50%,  total acc: 46.75%   [EVAL] batch:   50 | acc: 0.00%,  total acc: 45.83%   [EVAL] batch:   51 | acc: 6.25%,  total acc: 45.07%   [EVAL] batch:   52 | acc: 0.00%,  total acc: 44.22%   [EVAL] batch:   53 | acc: 25.00%,  total acc: 43.87%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 44.66%   [EVAL] batch:   55 | acc: 56.25%,  total acc: 44.87%   [EVAL] batch:   56 | acc: 50.00%,  total acc: 44.96%   [EVAL] batch:   57 | acc: 43.75%,  total acc: 44.94%   [EVAL] batch:   58 | acc: 50.00%,  total acc: 45.02%   [EVAL] batch:   59 | acc: 50.00%,  total acc: 45.10%   [EVAL] batch:   60 | acc: 43.75%,  total acc: 45.08%   [EVAL] batch:   61 | acc: 43.75%,  total acc: 45.06%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 45.14%   [EVAL] batch:   63 | acc: 56.25%,  total acc: 45.31%   [EVAL] batch:   64 | acc: 31.25%,  total acc: 45.10%   [EVAL] batch:   65 | acc: 62.50%,  total acc: 45.36%   [EVAL] batch:   66 | acc: 56.25%,  total acc: 45.52%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 46.32%   [EVAL] batch:   68 | acc: 75.00%,  total acc: 46.74%   [EVAL] batch:   69 | acc: 50.00%,  total acc: 46.79%   [EVAL] batch:   70 | acc: 62.50%,  total acc: 47.01%   [EVAL] batch:   71 | acc: 62.50%,  total acc: 47.22%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 47.95%   [EVAL] batch:   73 | acc: 100.00%,  total acc: 48.65%   [EVAL] batch:   74 | acc: 100.00%,  total acc: 49.33%   [EVAL] batch:   75 | acc: 100.00%,  total acc: 50.00%   [EVAL] batch:   76 | acc: 100.00%,  total acc: 50.65%   [EVAL] batch:   77 | acc: 75.00%,  total acc: 50.96%   [EVAL] batch:   78 | acc: 6.25%,  total acc: 50.40%   [EVAL] batch:   79 | acc: 12.50%,  total acc: 49.92%   [EVAL] batch:   80 | acc: 18.75%,  total acc: 49.54%   [EVAL] batch:   81 | acc: 25.00%,  total acc: 49.24%   [EVAL] batch:   82 | acc: 37.50%,  total acc: 49.10%   [EVAL] batch:   83 | acc: 0.00%,  total acc: 48.51%   [EVAL] batch:   84 | acc: 43.75%,  total acc: 48.46%   [EVAL] batch:   85 | acc: 68.75%,  total acc: 48.69%   [EVAL] batch:   86 | acc: 50.00%,  total acc: 48.71%   [EVAL] batch:   87 | acc: 68.75%,  total acc: 48.93%   [EVAL] batch:   88 | acc: 68.75%,  total acc: 49.16%   [EVAL] batch:   89 | acc: 87.50%,  total acc: 49.58%   [EVAL] batch:   90 | acc: 68.75%,  total acc: 49.79%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 50.34%   [EVAL] batch:   92 | acc: 93.75%,  total acc: 50.81%   [EVAL] batch:   93 | acc: 87.50%,  total acc: 51.20%   [EVAL] batch:   94 | acc: 87.50%,  total acc: 51.58%   [EVAL] batch:   95 | acc: 93.75%,  total acc: 52.02%   [EVAL] batch:   96 | acc: 50.00%,  total acc: 52.00%   [EVAL] batch:   97 | acc: 50.00%,  total acc: 51.98%   [EVAL] batch:   98 | acc: 43.75%,  total acc: 51.89%   [EVAL] batch:   99 | acc: 75.00%,  total acc: 52.12%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 52.60%   [EVAL] batch:  101 | acc: 81.25%,  total acc: 52.88%   [EVAL] batch:  102 | acc: 87.50%,  total acc: 53.22%   [EVAL] batch:  103 | acc: 62.50%,  total acc: 53.31%   [EVAL] batch:  104 | acc: 81.25%,  total acc: 53.57%   [EVAL] batch:  105 | acc: 87.50%,  total acc: 53.89%   [EVAL] batch:  106 | acc: 25.00%,  total acc: 53.62%   
cur_acc:  ['0.8580', '0.7083', '0.7232', '0.8299', '0.6591', '0.7344']
his_acc:  ['0.8580', '0.8285', '0.7500', '0.6723', '0.6275', '0.5362']
CurrentTrain: epoch  0, batch     0 | loss: 4.8921523CurrentTrain: epoch  0, batch     1 | loss: 5.1066885CurrentTrain: epoch  1, batch     0 | loss: 3.9243503CurrentTrain: epoch  1, batch     1 | loss: 3.9271495CurrentTrain: epoch  2, batch     0 | loss: 3.5633664CurrentTrain: epoch  2, batch     1 | loss: 2.6273715CurrentTrain: epoch  3, batch     0 | loss: 2.4354625CurrentTrain: epoch  3, batch     1 | loss: 2.6894798CurrentTrain: epoch  4, batch     0 | loss: 2.5127230CurrentTrain: epoch  4, batch     1 | loss: 2.0956602CurrentTrain: epoch  5, batch     0 | loss: 2.2764962CurrentTrain: epoch  5, batch     1 | loss: 2.1797612CurrentTrain: epoch  6, batch     0 | loss: 2.1521502CurrentTrain: epoch  6, batch     1 | loss: 2.0877481CurrentTrain: epoch  7, batch     0 | loss: 2.0574293CurrentTrain: epoch  7, batch     1 | loss: 2.0908823CurrentTrain: epoch  8, batch     0 | loss: 2.0515306CurrentTrain: epoch  8, batch     1 | loss: 2.0554624CurrentTrain: epoch  9, batch     0 | loss: 1.9721184CurrentTrain: epoch  9, batch     1 | loss: 1.9612694
#### person cause of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person cause of death" means a person was died of an event.
Here is an example:
Relation: person cause of death
Context: mr scheider had suffered from multiple myeloma for several years , and died of complications from a staph infection , his wife , brenda siemer , said .
Head Entity: mr scheider
Tail Entity: complications from a staph infection
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person cause of death":
Relation: person cause of death  
Context: after battling with lung cancer for over a year, john doe passed away peacefully in his sleep, surrounded by family.  
Head Entity: john doe  
Tail Entity: lung cancer  

Relation: person cause of death  
Context: the renowned scientist, dr. emily white, tragically lost her life due to a car accident while returning from a conference.  
Head Entity: dr. emily white  
Tail Entity: car accident  
#### organization political religious affiliation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization political religious affiliation" means an organization is the member of a religion.
Here is an example:
Relation: organization political religious affiliation
Context: the militia has also suffered from allegations of being involved in clashes with rival shiite militias , particularly the badr corp , the military wing linked to supreme iraqi islamic council , the party led by powerful politician abdel aziz al-hakim .
Head Entity: badr corp
Tail Entity: shiite
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization political religious affiliation":
Relation: organization political religious affiliation  
Context: The Christian Democratic Union has been a significant player in German politics, often aligning itself with various religious groups to promote policies that reflect its Christian values.  
Head Entity: Christian Democratic Union  
Tail Entity: Christian  

Relation: organization political religious affiliation  
Context: The Islamic Society of North America has been actively involved in advocating for Muslim rights and has formed alliances with political parties that support religious freedom.  
Head Entity: Islamic Society of North America  
Tail Entity: Muslim  
#### organization stateorprovince of headquarters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization stateorprovince of headquarters" means an organization is located in a state or province.
Here is an example:
Relation: organization stateorprovince of headquarters
Context: new york-based veteran taiwan journalist chang chao-ying and the washington - based formosan association for public affairs -lrb- fapa -rrb- have jointly established a `` freedom of speech prize '' aimed at helping taiwan media improve their professional quality and ethics .
Head Entity: formosan association for public affairs
Tail Entity: washington
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization stateorprovince of headquarters":
Relation: organization stateorprovince of headquarters  
Context: The tech giant Google has its headquarters in Mountain View, California, where it continues to innovate and expand its services globally.  
Head Entity: Google  
Tail Entity: California  

Relation: organization stateorprovince of headquarters  
Context: The multinational corporation Unilever operates its North American headquarters in Englewood Cliffs, New Jersey, focusing on sustainable living and consumer goods.  
Head Entity: Unilever  
Tail Entity: New Jersey  
#### person other family ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person other family" means a person 's relative is a person.
Here is an example:
Relation: person other family
Context: former american idol contestant , sanjaya malakar and hills starlet , holly montag , have been getting close since she replaced her sister and brother-in-law , heidi montag and spencer pratt , on i 'm a celebrity .
Head Entity: spencer pratt ,
Tail Entity: holly montag
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person other family":
Relation: person other family  
Context: in a recent interview, actress jennifer aniston spoke fondly of her brother, alex aniston, and their close bond, highlighting how family ties remain strong despite their busy careers.  
Head Entity: alex aniston  
Tail Entity: jennifer aniston  

Relation: person other family  
Context: during the family reunion, uncle bob shared stories about his adventures with his niece, emily, emphasizing the importance of family connections and shared experiences.  
Head Entity: emily  
Tail Entity: uncle bob  
#### person city of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person city of death" means a person was died in a city.
Here is an example:
Relation: person city of death
Context: millender-mcdonald , who was 68 , died late saturday at her home in carson , california , said her chief of staff , bandele mcqueen .
Head Entity: millender-mcdonald
Tail Entity: carson
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person city of death":
Relation: person city of death  
Context: renowned author and activist, james baldwin, passed away in 1987 in the city of saint-paul, minnesota, where he spent his final years.  
Head Entity: james baldwin  
Tail Entity: saint-paul  

Relation: person city of death  
Context: the famous physicist, albert einstein, died in 1955 in the city of princeton, new jersey, where he had lived for many years.  
Head Entity: albert einstein  
Tail Entity: princeton  
Mixup data size:  8545
MixupTrain:  epoch  0, batch     0 | loss: 3.4099355MixupTrain:  epoch  0, batch     1 | loss: 3.6465042MixupTrain:  epoch  0, batch     2 | loss: 3.4232411MixupTrain:  epoch  0, batch     3 | loss: 3.5310636MixupTrain:  epoch  0, batch     4 | loss: 3.5951188MixupTrain:  epoch  0, batch     5 | loss: 3.5871274MixupTrain:  epoch  0, batch     6 | loss: 3.8131583MixupTrain:  epoch  0, batch     7 | loss: 3.7085013MixupTrain:  epoch  0, batch     8 | loss: 3.5448403MixupTrain:  epoch  0, batch     9 | loss: 3.4120440MixupTrain:  epoch  0, batch    10 | loss: 3.3398466MixupTrain:  epoch  0, batch    11 | loss: 3.2997842MixupTrain:  epoch  0, batch    12 | loss: 3.5700145MixupTrain:  epoch  0, batch    13 | loss: 3.5803139MixupTrain:  epoch  0, batch    14 | loss: 3.0514948MixupTrain:  epoch  0, batch    15 | loss: 3.4603481MixupTrain:  epoch  0, batch    16 | loss: 3.2960143MixupTrain:  epoch  0, batch    17 | loss: 3.1835365MixupTrain:  epoch  0, batch    18 | loss: 3.4545479MixupTrain:  epoch  0, batch    19 | loss: 3.2485723MixupTrain:  epoch  0, batch    20 | loss: 3.3711691MixupTrain:  epoch  0, batch    21 | loss: 3.3010654MixupTrain:  epoch  0, batch    22 | loss: 3.4185815MixupTrain:  epoch  0, batch    23 | loss: 3.4787536MixupTrain:  epoch  0, batch    24 | loss: 3.6688466MixupTrain:  epoch  0, batch    25 | loss: 3.3345113MixupTrain:  epoch  0, batch    26 | loss: 3.3233225MixupTrain:  epoch  0, batch    27 | loss: 3.4555902MixupTrain:  epoch  0, batch    28 | loss: 3.3103871MixupTrain:  epoch  0, batch    29 | loss: 3.3408082MixupTrain:  epoch  0, batch    30 | loss: 3.0964689MixupTrain:  epoch  0, batch    31 | loss: 3.0219355MixupTrain:  epoch  0, batch    32 | loss: 3.3840094MixupTrain:  epoch  0, batch    33 | loss: 3.4636426MixupTrain:  epoch  0, batch    34 | loss: 3.1779113MixupTrain:  epoch  0, batch    35 | loss: 3.4197106MixupTrain:  epoch  0, batch    36 | loss: 3.0825210MixupTrain:  epoch  0, batch    37 | loss: 3.0510530MixupTrain:  epoch  0, batch    38 | loss: 3.4217424MixupTrain:  epoch  0, batch    39 | loss: 3.4114063MixupTrain:  epoch  0, batch    40 | loss: 3.2928531MixupTrain:  epoch  0, batch    41 | loss: 3.2507038MixupTrain:  epoch  0, batch    42 | loss: 3.5844219MixupTrain:  epoch  0, batch    43 | loss: 3.3987865MixupTrain:  epoch  0, batch    44 | loss: 3.2985291MixupTrain:  epoch  0, batch    45 | loss: 3.1521580MixupTrain:  epoch  0, batch    46 | loss: 2.8027234MixupTrain:  epoch  0, batch    47 | loss: 3.1077642MixupTrain:  epoch  0, batch    48 | loss: 3.1128771MixupTrain:  epoch  0, batch    49 | loss: 3.1392243MixupTrain:  epoch  0, batch    50 | loss: 3.2339776MixupTrain:  epoch  0, batch    51 | loss: 3.1968954MixupTrain:  epoch  0, batch    52 | loss: 3.0158205MixupTrain:  epoch  0, batch    53 | loss: 3.0896711MixupTrain:  epoch  0, batch    54 | loss: 2.9455690MixupTrain:  epoch  0, batch    55 | loss: 3.2035844MixupTrain:  epoch  0, batch    56 | loss: 3.1361222MixupTrain:  epoch  0, batch    57 | loss: 3.0467603MixupTrain:  epoch  0, batch    58 | loss: 3.3555632MixupTrain:  epoch  0, batch    59 | loss: 3.0030260MixupTrain:  epoch  0, batch    60 | loss: 3.2117610MixupTrain:  epoch  0, batch    61 | loss: 3.1429529MixupTrain:  epoch  0, batch    62 | loss: 2.8213737MixupTrain:  epoch  0, batch    63 | loss: 3.4949031MixupTrain:  epoch  0, batch    64 | loss: 3.2153599MixupTrain:  epoch  0, batch    65 | loss: 3.0526152MixupTrain:  epoch  0, batch    66 | loss: 3.0287023MixupTrain:  epoch  0, batch    67 | loss: 3.1950834MixupTrain:  epoch  0, batch    68 | loss: 3.2708571MixupTrain:  epoch  0, batch    69 | loss: 2.9982429MixupTrain:  epoch  0, batch    70 | loss: 3.0184641MixupTrain:  epoch  0, batch    71 | loss: 3.1502008MixupTrain:  epoch  0, batch    72 | loss: 3.0328469MixupTrain:  epoch  0, batch    73 | loss: 3.0747585MixupTrain:  epoch  0, batch    74 | loss: 2.9582205MixupTrain:  epoch  0, batch    75 | loss: 3.0719123MixupTrain:  epoch  0, batch    76 | loss: 3.1061506MixupTrain:  epoch  0, batch    77 | loss: 3.0280111MixupTrain:  epoch  0, batch    78 | loss: 3.2149091MixupTrain:  epoch  0, batch    79 | loss: 3.2194338MixupTrain:  epoch  0, batch    80 | loss: 3.2161248MixupTrain:  epoch  0, batch    81 | loss: 3.0913539MixupTrain:  epoch  0, batch    82 | loss: 3.1805077MixupTrain:  epoch  0, batch    83 | loss: 3.0979557MixupTrain:  epoch  0, batch    84 | loss: 2.9251595MixupTrain:  epoch  0, batch    85 | loss: 3.1081610MixupTrain:  epoch  0, batch    86 | loss: 3.0694413MixupTrain:  epoch  0, batch    87 | loss: 3.2064164MixupTrain:  epoch  0, batch    88 | loss: 2.8706751MixupTrain:  epoch  0, batch    89 | loss: 3.1100154MixupTrain:  epoch  0, batch    90 | loss: 3.1533985MixupTrain:  epoch  0, batch    91 | loss: 3.0105236MixupTrain:  epoch  0, batch    92 | loss: 3.2896662MixupTrain:  epoch  0, batch    93 | loss: 3.0124745MixupTrain:  epoch  0, batch    94 | loss: 3.1198678MixupTrain:  epoch  0, batch    95 | loss: 2.9668756MixupTrain:  epoch  0, batch    96 | loss: 2.9981074MixupTrain:  epoch  0, batch    97 | loss: 3.2995133MixupTrain:  epoch  0, batch    98 | loss: 3.1125753MixupTrain:  epoch  0, batch    99 | loss: 2.8998270MixupTrain:  epoch  0, batch   100 | loss: 3.0870581MixupTrain:  epoch  0, batch   101 | loss: 2.9140201MixupTrain:  epoch  0, batch   102 | loss: 2.9401131MixupTrain:  epoch  0, batch   103 | loss: 3.2008014MixupTrain:  epoch  0, batch   104 | loss: 3.0731523MixupTrain:  epoch  0, batch   105 | loss: 2.8324156MixupTrain:  epoch  0, batch   106 | loss: 3.1386206MixupTrain:  epoch  0, batch   107 | loss: 3.0540242MixupTrain:  epoch  0, batch   108 | loss: 3.0128894MixupTrain:  epoch  0, batch   109 | loss: 2.9772134MixupTrain:  epoch  0, batch   110 | loss: 2.8371525MixupTrain:  epoch  0, batch   111 | loss: 2.9162054MixupTrain:  epoch  0, batch   112 | loss: 3.1484275MixupTrain:  epoch  0, batch   113 | loss: 2.9926178MixupTrain:  epoch  0, batch   114 | loss: 3.1377339MixupTrain:  epoch  0, batch   115 | loss: 3.2062023MixupTrain:  epoch  0, batch   116 | loss: 2.9749060MixupTrain:  epoch  0, batch   117 | loss: 2.9399025MixupTrain:  epoch  0, batch   118 | loss: 3.1091814MixupTrain:  epoch  0, batch   119 | loss: 3.1184006MixupTrain:  epoch  0, batch   120 | loss: 3.0539458MixupTrain:  epoch  0, batch   121 | loss: 3.0422864MixupTrain:  epoch  0, batch   122 | loss: 3.0148077MixupTrain:  epoch  0, batch   123 | loss: 3.1337595MixupTrain:  epoch  0, batch   124 | loss: 2.9896364MixupTrain:  epoch  0, batch   125 | loss: 2.8951135MixupTrain:  epoch  0, batch   126 | loss: 2.9460914MixupTrain:  epoch  0, batch   127 | loss: 3.0714407MixupTrain:  epoch  0, batch   128 | loss: 2.9233267MixupTrain:  epoch  0, batch   129 | loss: 3.0131991MixupTrain:  epoch  0, batch   130 | loss: 3.0693879MixupTrain:  epoch  0, batch   131 | loss: 3.0138106MixupTrain:  epoch  0, batch   132 | loss: 2.9068301MixupTrain:  epoch  0, batch   133 | loss: 3.0038662MixupTrain:  epoch  0, batch   134 | loss: 2.9089103MixupTrain:  epoch  0, batch   135 | loss: 3.0716128MixupTrain:  epoch  0, batch   136 | loss: 2.8867276MixupTrain:  epoch  0, batch   137 | loss: 3.0875220MixupTrain:  epoch  0, batch   138 | loss: 3.2269642MixupTrain:  epoch  0, batch   139 | loss: 2.9427924MixupTrain:  epoch  0, batch   140 | loss: 2.9827614MixupTrain:  epoch  0, batch   141 | loss: 3.0058727MixupTrain:  epoch  0, batch   142 | loss: 3.0026016MixupTrain:  epoch  0, batch   143 | loss: 2.8002334MixupTrain:  epoch  0, batch   144 | loss: 2.8854375MixupTrain:  epoch  0, batch   145 | loss: 2.9991455MixupTrain:  epoch  0, batch   146 | loss: 3.0579615MixupTrain:  epoch  0, batch   147 | loss: 3.0543232MixupTrain:  epoch  0, batch   148 | loss: 2.9818058MixupTrain:  epoch  0, batch   149 | loss: 2.9414401MixupTrain:  epoch  0, batch   150 | loss: 2.9737408MixupTrain:  epoch  0, batch   151 | loss: 2.9245951MixupTrain:  epoch  0, batch   152 | loss: 3.0129876MixupTrain:  epoch  0, batch   153 | loss: 2.9686174MixupTrain:  epoch  0, batch   154 | loss: 2.9404018MixupTrain:  epoch  0, batch   155 | loss: 3.0962481MixupTrain:  epoch  0, batch   156 | loss: 2.9134579MixupTrain:  epoch  0, batch   157 | loss: 2.9661603MixupTrain:  epoch  0, batch   158 | loss: 2.8646693MixupTrain:  epoch  0, batch   159 | loss: 2.8933978MixupTrain:  epoch  0, batch   160 | loss: 2.8685422MixupTrain:  epoch  0, batch   161 | loss: 3.0173862MixupTrain:  epoch  0, batch   162 | loss: 2.9951196MixupTrain:  epoch  0, batch   163 | loss: 2.8967242MixupTrain:  epoch  0, batch   164 | loss: 3.1024861MixupTrain:  epoch  0, batch   165 | loss: 2.9993947MixupTrain:  epoch  0, batch   166 | loss: 2.9517362MixupTrain:  epoch  0, batch   167 | loss: 2.9144754MixupTrain:  epoch  0, batch   168 | loss: 2.9924855MixupTrain:  epoch  0, batch   169 | loss: 2.9443979MixupTrain:  epoch  0, batch   170 | loss: 2.8372293MixupTrain:  epoch  0, batch   171 | loss: 3.0887871MixupTrain:  epoch  0, batch   172 | loss: 2.8136778MixupTrain:  epoch  0, batch   173 | loss: 2.9892988MixupTrain:  epoch  0, batch   174 | loss: 3.0241008MixupTrain:  epoch  0, batch   175 | loss: 2.9421682MixupTrain:  epoch  0, batch   176 | loss: 2.8475466MixupTrain:  epoch  0, batch   177 | loss: 2.8903189MixupTrain:  epoch  0, batch   178 | loss: 2.9874485MixupTrain:  epoch  0, batch   179 | loss: 2.9603276MixupTrain:  epoch  0, batch   180 | loss: 2.9653137MixupTrain:  epoch  0, batch   181 | loss: 3.0373316MixupTrain:  epoch  0, batch   182 | loss: 2.9627438MixupTrain:  epoch  0, batch   183 | loss: 2.9272206MixupTrain:  epoch  0, batch   184 | loss: 2.8632169MixupTrain:  epoch  0, batch   185 | loss: 2.8489609MixupTrain:  epoch  0, batch   186 | loss: 2.9706116MixupTrain:  epoch  0, batch   187 | loss: 2.8382289MixupTrain:  epoch  0, batch   188 | loss: 3.0539455MixupTrain:  epoch  0, batch   189 | loss: 3.0010853MixupTrain:  epoch  0, batch   190 | loss: 2.8511767MixupTrain:  epoch  0, batch   191 | loss: 2.8524444MixupTrain:  epoch  0, batch   192 | loss: 2.8827190MixupTrain:  epoch  0, batch   193 | loss: 3.0431633MixupTrain:  epoch  0, batch   194 | loss: 2.9529784MixupTrain:  epoch  0, batch   195 | loss: 2.9878559MixupTrain:  epoch  0, batch   196 | loss: 2.9611955MixupTrain:  epoch  0, batch   197 | loss: 2.8092964MixupTrain:  epoch  0, batch   198 | loss: 2.8289418MixupTrain:  epoch  0, batch   199 | loss: 2.9999862MixupTrain:  epoch  0, batch   200 | loss: 2.8417850MixupTrain:  epoch  0, batch   201 | loss: 2.8673661MixupTrain:  epoch  0, batch   202 | loss: 3.0129123MixupTrain:  epoch  0, batch   203 | loss: 2.9453137MixupTrain:  epoch  0, batch   204 | loss: 2.8351271MixupTrain:  epoch  0, batch   205 | loss: 2.9716465MixupTrain:  epoch  0, batch   206 | loss: 2.7164283MixupTrain:  epoch  0, batch   207 | loss: 2.9298272MixupTrain:  epoch  0, batch   208 | loss: 2.9711149MixupTrain:  epoch  0, batch   209 | loss: 3.0085230MixupTrain:  epoch  0, batch   210 | loss: 3.1669812MixupTrain:  epoch  0, batch   211 | loss: 2.9656849MixupTrain:  epoch  0, batch   212 | loss: 2.8217177MixupTrain:  epoch  0, batch   213 | loss: 2.8439732MixupTrain:  epoch  0, batch   214 | loss: 2.8471534MixupTrain:  epoch  0, batch   215 | loss: 2.9146757MixupTrain:  epoch  0, batch   216 | loss: 3.0313041MixupTrain:  epoch  0, batch   217 | loss: 2.9082978MixupTrain:  epoch  0, batch   218 | loss: 3.0529966MixupTrain:  epoch  0, batch   219 | loss: 2.9638581MixupTrain:  epoch  0, batch   220 | loss: 3.0077820MixupTrain:  epoch  0, batch   221 | loss: 2.9770215MixupTrain:  epoch  0, batch   222 | loss: 3.0556734MixupTrain:  epoch  0, batch   223 | loss: 2.9730864MixupTrain:  epoch  0, batch   224 | loss: 3.0827291MixupTrain:  epoch  0, batch   225 | loss: 2.9233849MixupTrain:  epoch  0, batch   226 | loss: 2.8955848MixupTrain:  epoch  0, batch   227 | loss: 2.9654224MixupTrain:  epoch  0, batch   228 | loss: 2.8922253MixupTrain:  epoch  0, batch   229 | loss: 2.9216669MixupTrain:  epoch  0, batch   230 | loss: 2.9009838MixupTrain:  epoch  0, batch   231 | loss: 2.9165187MixupTrain:  epoch  0, batch   232 | loss: 3.0202632MixupTrain:  epoch  0, batch   233 | loss: 2.9481606MixupTrain:  epoch  0, batch   234 | loss: 3.1180453MixupTrain:  epoch  0, batch   235 | loss: 2.9167645MixupTrain:  epoch  0, batch   236 | loss: 2.8303430MixupTrain:  epoch  0, batch   237 | loss: 2.9715636MixupTrain:  epoch  0, batch   238 | loss: 3.1301875MixupTrain:  epoch  0, batch   239 | loss: 2.8128505MixupTrain:  epoch  0, batch   240 | loss: 3.0352268MixupTrain:  epoch  0, batch   241 | loss: 3.1147468MixupTrain:  epoch  0, batch   242 | loss: 2.9295452MixupTrain:  epoch  0, batch   243 | loss: 3.1500232MixupTrain:  epoch  0, batch   244 | loss: 2.8670371MixupTrain:  epoch  0, batch   245 | loss: 2.8952723MixupTrain:  epoch  0, batch   246 | loss: 2.9754548MixupTrain:  epoch  0, batch   247 | loss: 2.9062557MixupTrain:  epoch  0, batch   248 | loss: 2.8882539MixupTrain:  epoch  0, batch   249 | loss: 2.8916664MixupTrain:  epoch  0, batch   250 | loss: 2.9558952MixupTrain:  epoch  0, batch   251 | loss: 2.8691978MixupTrain:  epoch  0, batch   252 | loss: 3.0532861MixupTrain:  epoch  0, batch   253 | loss: 2.9774637MixupTrain:  epoch  0, batch   254 | loss: 2.9212008MixupTrain:  epoch  0, batch   255 | loss: 2.9107120MixupTrain:  epoch  0, batch   256 | loss: 3.0391254MixupTrain:  epoch  0, batch   257 | loss: 2.9370973MixupTrain:  epoch  0, batch   258 | loss: 2.8195641MixupTrain:  epoch  0, batch   259 | loss: 2.8881373MixupTrain:  epoch  0, batch   260 | loss: 2.9319503MixupTrain:  epoch  0, batch   261 | loss: 2.8526192MixupTrain:  epoch  0, batch   262 | loss: 3.1280446MixupTrain:  epoch  0, batch   263 | loss: 2.8539605MixupTrain:  epoch  0, batch   264 | loss: 3.1788356MixupTrain:  epoch  0, batch   265 | loss: 3.0061364MixupTrain:  epoch  0, batch   266 | loss: 3.0020092MixupTrain:  epoch  0, batch   267 | loss: 3.1837385MixupTrain:  epoch  0, batch   268 | loss: 2.8998239MixupTrain:  epoch  0, batch   269 | loss: 2.9009390MixupTrain:  epoch  0, batch   270 | loss: 2.9553580MixupTrain:  epoch  0, batch   271 | loss: 2.9770291MixupTrain:  epoch  0, batch   272 | loss: 3.0641789MixupTrain:  epoch  0, batch   273 | loss: 2.9626970MixupTrain:  epoch  0, batch   274 | loss: 2.8848929MixupTrain:  epoch  0, batch   275 | loss: 2.9345212MixupTrain:  epoch  0, batch   276 | loss: 2.8246365MixupTrain:  epoch  0, batch   277 | loss: 2.9751830MixupTrain:  epoch  0, batch   278 | loss: 2.8622591MixupTrain:  epoch  0, batch   279 | loss: 2.8689704MixupTrain:  epoch  0, batch   280 | loss: 3.0142305MixupTrain:  epoch  0, batch   281 | loss: 2.9170175MixupTrain:  epoch  0, batch   282 | loss: 2.8789191MixupTrain:  epoch  0, batch   283 | loss: 3.0269227MixupTrain:  epoch  0, batch   284 | loss: 2.9927642MixupTrain:  epoch  0, batch   285 | loss: 3.0629125MixupTrain:  epoch  0, batch   286 | loss: 3.0764506MixupTrain:  epoch  0, batch   287 | loss: 2.8155468MixupTrain:  epoch  0, batch   288 | loss: 2.8752193MixupTrain:  epoch  0, batch   289 | loss: 2.9522409MixupTrain:  epoch  0, batch   290 | loss: 3.0037234MixupTrain:  epoch  0, batch   291 | loss: 2.9833093MixupTrain:  epoch  0, batch   292 | loss: 2.9371879MixupTrain:  epoch  0, batch   293 | loss: 3.1035013MixupTrain:  epoch  0, batch   294 | loss: 2.8116298MixupTrain:  epoch  0, batch   295 | loss: 2.8958271MixupTrain:  epoch  0, batch   296 | loss: 2.8540289MixupTrain:  epoch  0, batch   297 | loss: 2.8187594MixupTrain:  epoch  0, batch   298 | loss: 2.9482555MixupTrain:  epoch  0, batch   299 | loss: 3.0507793MixupTrain:  epoch  0, batch   300 | loss: 2.9511299MixupTrain:  epoch  0, batch   301 | loss: 2.9302216MixupTrain:  epoch  0, batch   302 | loss: 2.7911577MixupTrain:  epoch  0, batch   303 | loss: 2.9749475MixupTrain:  epoch  0, batch   304 | loss: 2.8703163MixupTrain:  epoch  0, batch   305 | loss: 3.0457783MixupTrain:  epoch  0, batch   306 | loss: 2.8792646MixupTrain:  epoch  0, batch   307 | loss: 2.8462749MixupTrain:  epoch  0, batch   308 | loss: 2.8642950MixupTrain:  epoch  0, batch   309 | loss: 2.8014016MixupTrain:  epoch  0, batch   310 | loss: 2.8128734MixupTrain:  epoch  0, batch   311 | loss: 2.9291945MixupTrain:  epoch  0, batch   312 | loss: 2.9695113MixupTrain:  epoch  0, batch   313 | loss: 3.0107358MixupTrain:  epoch  0, batch   314 | loss: 2.9501553MixupTrain:  epoch  0, batch   315 | loss: 2.9498758MixupTrain:  epoch  0, batch   316 | loss: 3.0319147MixupTrain:  epoch  0, batch   317 | loss: 2.9185991MixupTrain:  epoch  0, batch   318 | loss: 2.8215137MixupTrain:  epoch  0, batch   319 | loss: 2.9582777MixupTrain:  epoch  0, batch   320 | loss: 2.8694131MixupTrain:  epoch  0, batch   321 | loss: 3.0485623MixupTrain:  epoch  0, batch   322 | loss: 2.9118619MixupTrain:  epoch  0, batch   323 | loss: 2.9902122MixupTrain:  epoch  0, batch   324 | loss: 2.8965225MixupTrain:  epoch  0, batch   325 | loss: 2.9908338MixupTrain:  epoch  0, batch   326 | loss: 2.8883138MixupTrain:  epoch  0, batch   327 | loss: 2.7282095MixupTrain:  epoch  0, batch   328 | loss: 2.9166238MixupTrain:  epoch  0, batch   329 | loss: 2.9089837MixupTrain:  epoch  0, batch   330 | loss: 2.8931634MixupTrain:  epoch  0, batch   331 | loss: 2.8683767MixupTrain:  epoch  0, batch   332 | loss: 2.9249229MixupTrain:  epoch  0, batch   333 | loss: 2.8528113MixupTrain:  epoch  0, batch   334 | loss: 2.9402089MixupTrain:  epoch  0, batch   335 | loss: 2.9084551MixupTrain:  epoch  0, batch   336 | loss: 2.7611184MixupTrain:  epoch  0, batch   337 | loss: 3.0394928MixupTrain:  epoch  0, batch   338 | loss: 2.9606180MixupTrain:  epoch  0, batch   339 | loss: 2.7741649MixupTrain:  epoch  0, batch   340 | loss: 2.9201677MixupTrain:  epoch  0, batch   341 | loss: 2.9300709MixupTrain:  epoch  0, batch   342 | loss: 2.7955837MixupTrain:  epoch  0, batch   343 | loss: 2.9894509MixupTrain:  epoch  0, batch   344 | loss: 2.8764920MixupTrain:  epoch  0, batch   345 | loss: 2.9643502MixupTrain:  epoch  0, batch   346 | loss: 2.9739323MixupTrain:  epoch  0, batch   347 | loss: 3.0550613MixupTrain:  epoch  0, batch   348 | loss: 2.9965115MixupTrain:  epoch  0, batch   349 | loss: 2.7495465MixupTrain:  epoch  0, batch   350 | loss: 2.8082578MixupTrain:  epoch  0, batch   351 | loss: 2.9755914MixupTrain:  epoch  0, batch   352 | loss: 2.7779369MixupTrain:  epoch  0, batch   353 | loss: 2.8790185MixupTrain:  epoch  0, batch   354 | loss: 2.9417758MixupTrain:  epoch  0, batch   355 | loss: 2.9214454MixupTrain:  epoch  0, batch   356 | loss: 2.9150076MixupTrain:  epoch  0, batch   357 | loss: 3.0261116MixupTrain:  epoch  0, batch   358 | loss: 2.8364840MixupTrain:  epoch  0, batch   359 | loss: 2.9760094MixupTrain:  epoch  0, batch   360 | loss: 2.7969334MixupTrain:  epoch  0, batch   361 | loss: 2.9270930MixupTrain:  epoch  0, batch   362 | loss: 2.9876044MixupTrain:  epoch  0, batch   363 | loss: 3.0376418MixupTrain:  epoch  0, batch   364 | loss: 2.9514470MixupTrain:  epoch  0, batch   365 | loss: 2.9363194MixupTrain:  epoch  0, batch   366 | loss: 2.9603500MixupTrain:  epoch  0, batch   367 | loss: 2.9423399MixupTrain:  epoch  0, batch   368 | loss: 2.9124696MixupTrain:  epoch  0, batch   369 | loss: 2.9568839MixupTrain:  epoch  0, batch   370 | loss: 2.9926851MixupTrain:  epoch  0, batch   371 | loss: 2.7830825MixupTrain:  epoch  0, batch   372 | loss: 2.8446441MixupTrain:  epoch  0, batch   373 | loss: 2.8637681MixupTrain:  epoch  0, batch   374 | loss: 2.7558551MixupTrain:  epoch  0, batch   375 | loss: 2.9568491MixupTrain:  epoch  0, batch   376 | loss: 2.8082705MixupTrain:  epoch  0, batch   377 | loss: 2.9564655MixupTrain:  epoch  0, batch   378 | loss: 2.7984161MixupTrain:  epoch  0, batch   379 | loss: 2.8935385MixupTrain:  epoch  0, batch   380 | loss: 2.9222801MixupTrain:  epoch  0, batch   381 | loss: 2.8857276MixupTrain:  epoch  0, batch   382 | loss: 2.8916488MixupTrain:  epoch  0, batch   383 | loss: 2.7756176MixupTrain:  epoch  0, batch   384 | loss: 3.0059071MixupTrain:  epoch  0, batch   385 | loss: 2.9646199MixupTrain:  epoch  0, batch   386 | loss: 3.0022876MixupTrain:  epoch  0, batch   387 | loss: 3.0318789MixupTrain:  epoch  0, batch   388 | loss: 2.8112619MixupTrain:  epoch  0, batch   389 | loss: 2.8857930MixupTrain:  epoch  0, batch   390 | loss: 2.9450898MixupTrain:  epoch  0, batch   391 | loss: 2.8290851MixupTrain:  epoch  0, batch   392 | loss: 2.7973804MixupTrain:  epoch  0, batch   393 | loss: 2.9701219MixupTrain:  epoch  0, batch   394 | loss: 2.8784697MixupTrain:  epoch  0, batch   395 | loss: 2.9570384MixupTrain:  epoch  0, batch   396 | loss: 2.8338099MixupTrain:  epoch  0, batch   397 | loss: 2.8651853MixupTrain:  epoch  0, batch   398 | loss: 2.9092782MixupTrain:  epoch  0, batch   399 | loss: 2.9730306MixupTrain:  epoch  0, batch   400 | loss: 2.8918571MixupTrain:  epoch  0, batch   401 | loss: 2.8350754MixupTrain:  epoch  0, batch   402 | loss: 3.0728717MixupTrain:  epoch  0, batch   403 | loss: 2.8600898MixupTrain:  epoch  0, batch   404 | loss: 2.8060894MixupTrain:  epoch  0, batch   405 | loss: 2.8342848MixupTrain:  epoch  0, batch   406 | loss: 2.7229815MixupTrain:  epoch  0, batch   407 | loss: 3.0614080MixupTrain:  epoch  0, batch   408 | loss: 2.9764283MixupTrain:  epoch  0, batch   409 | loss: 2.9874740MixupTrain:  epoch  0, batch   410 | loss: 3.0365663MixupTrain:  epoch  0, batch   411 | loss: 2.8506629MixupTrain:  epoch  0, batch   412 | loss: 2.8155637MixupTrain:  epoch  0, batch   413 | loss: 2.9606557MixupTrain:  epoch  0, batch   414 | loss: 2.9318147MixupTrain:  epoch  0, batch   415 | loss: 2.8076155MixupTrain:  epoch  0, batch   416 | loss: 2.9283724MixupTrain:  epoch  0, batch   417 | loss: 2.7813609MixupTrain:  epoch  0, batch   418 | loss: 2.8211384MixupTrain:  epoch  0, batch   419 | loss: 3.0154643MixupTrain:  epoch  0, batch   420 | loss: 3.0316033MixupTrain:  epoch  0, batch   421 | loss: 2.8838160MixupTrain:  epoch  0, batch   422 | loss: 2.9426603MixupTrain:  epoch  0, batch   423 | loss: 2.9287953MixupTrain:  epoch  0, batch   424 | loss: 2.9000175MixupTrain:  epoch  0, batch   425 | loss: 2.8393149MixupTrain:  epoch  0, batch   426 | loss: 2.9602621MixupTrain:  epoch  0, batch   427 | loss: 3.1104426MixupTrain:  epoch  0, batch   428 | loss: 3.1001916MixupTrain:  epoch  0, batch   429 | loss: 2.8721251MixupTrain:  epoch  0, batch   430 | loss: 2.9377162MixupTrain:  epoch  0, batch   431 | loss: 2.8305912MixupTrain:  epoch  0, batch   432 | loss: 2.9170055MixupTrain:  epoch  0, batch   433 | loss: 2.9058299MixupTrain:  epoch  0, batch   434 | loss: 3.0703177MixupTrain:  epoch  0, batch   435 | loss: 2.7474055MixupTrain:  epoch  0, batch   436 | loss: 3.0392766MixupTrain:  epoch  0, batch   437 | loss: 2.9678783MixupTrain:  epoch  0, batch   438 | loss: 2.7909260MixupTrain:  epoch  0, batch   439 | loss: 2.9388814MixupTrain:  epoch  0, batch   440 | loss: 2.9101648MixupTrain:  epoch  0, batch   441 | loss: 2.8316066MixupTrain:  epoch  0, batch   442 | loss: 2.9527001MixupTrain:  epoch  0, batch   443 | loss: 2.8278143MixupTrain:  epoch  0, batch   444 | loss: 2.8844099MixupTrain:  epoch  0, batch   445 | loss: 2.9214654MixupTrain:  epoch  0, batch   446 | loss: 2.8741183MixupTrain:  epoch  0, batch   447 | loss: 2.8673863MixupTrain:  epoch  0, batch   448 | loss: 2.9574928MixupTrain:  epoch  0, batch   449 | loss: 2.8999710MixupTrain:  epoch  0, batch   450 | loss: 2.9436519MixupTrain:  epoch  0, batch   451 | loss: 2.9195518MixupTrain:  epoch  0, batch   452 | loss: 2.8364086MixupTrain:  epoch  0, batch   453 | loss: 2.8021698MixupTrain:  epoch  0, batch   454 | loss: 2.8123589MixupTrain:  epoch  0, batch   455 | loss: 2.8728249MixupTrain:  epoch  0, batch   456 | loss: 2.8788700MixupTrain:  epoch  0, batch   457 | loss: 2.9339445MixupTrain:  epoch  0, batch   458 | loss: 2.9180646MixupTrain:  epoch  0, batch   459 | loss: 2.9437864MixupTrain:  epoch  0, batch   460 | loss: 3.0420871MixupTrain:  epoch  0, batch   461 | loss: 2.8623371MixupTrain:  epoch  0, batch   462 | loss: 2.8506255MixupTrain:  epoch  0, batch   463 | loss: 2.8410053MixupTrain:  epoch  0, batch   464 | loss: 2.8015738MixupTrain:  epoch  0, batch   465 | loss: 2.8554778MixupTrain:  epoch  0, batch   466 | loss: 2.9343164MixupTrain:  epoch  0, batch   467 | loss: 2.9269462MixupTrain:  epoch  0, batch   468 | loss: 2.8905833MixupTrain:  epoch  0, batch   469 | loss: 2.9008408MixupTrain:  epoch  0, batch   470 | loss: 2.9383523MixupTrain:  epoch  0, batch   471 | loss: 2.9522128MixupTrain:  epoch  0, batch   472 | loss: 2.9614909MixupTrain:  epoch  0, batch   473 | loss: 2.8216772MixupTrain:  epoch  0, batch   474 | loss: 2.9671955MixupTrain:  epoch  0, batch   475 | loss: 2.8590531MixupTrain:  epoch  0, batch   476 | loss: 3.0242281MixupTrain:  epoch  0, batch   477 | loss: 2.8518376MixupTrain:  epoch  0, batch   478 | loss: 2.8698723MixupTrain:  epoch  0, batch   479 | loss: 2.8957734MixupTrain:  epoch  0, batch   480 | loss: 2.8972239MixupTrain:  epoch  0, batch   481 | loss: 2.9551001MixupTrain:  epoch  0, batch   482 | loss: 2.8531332MixupTrain:  epoch  0, batch   483 | loss: 2.9492590MixupTrain:  epoch  0, batch   484 | loss: 3.1376784MixupTrain:  epoch  0, batch   485 | loss: 2.8621721MixupTrain:  epoch  0, batch   486 | loss: 2.8906035MixupTrain:  epoch  0, batch   487 | loss: 2.8823504MixupTrain:  epoch  0, batch   488 | loss: 2.9679241MixupTrain:  epoch  0, batch   489 | loss: 2.9360223MixupTrain:  epoch  0, batch   490 | loss: 2.9374392MixupTrain:  epoch  0, batch   491 | loss: 2.9492023MixupTrain:  epoch  0, batch   492 | loss: 2.9225097MixupTrain:  epoch  0, batch   493 | loss: 2.9052083MixupTrain:  epoch  0, batch   494 | loss: 2.8523242MixupTrain:  epoch  0, batch   495 | loss: 2.8487511MixupTrain:  epoch  0, batch   496 | loss: 2.8094206MixupTrain:  epoch  0, batch   497 | loss: 2.8813601MixupTrain:  epoch  0, batch   498 | loss: 2.9411609MixupTrain:  epoch  0, batch   499 | loss: 3.0055602MixupTrain:  epoch  0, batch   500 | loss: 2.9545045MixupTrain:  epoch  0, batch   501 | loss: 3.0376987MixupTrain:  epoch  0, batch   502 | loss: 2.8663673MixupTrain:  epoch  0, batch   503 | loss: 2.9186616MixupTrain:  epoch  0, batch   504 | loss: 3.0627067MixupTrain:  epoch  0, batch   505 | loss: 2.8203933MixupTrain:  epoch  0, batch   506 | loss: 2.8068955MixupTrain:  epoch  0, batch   507 | loss: 2.9382634MixupTrain:  epoch  0, batch   508 | loss: 2.8694117MixupTrain:  epoch  0, batch   509 | loss: 2.9057014MixupTrain:  epoch  0, batch   510 | loss: 2.9424169MixupTrain:  epoch  0, batch   511 | loss: 2.8285336MixupTrain:  epoch  0, batch   512 | loss: 2.8398268MixupTrain:  epoch  0, batch   513 | loss: 2.8574309MixupTrain:  epoch  0, batch   514 | loss: 2.7616868MixupTrain:  epoch  0, batch   515 | loss: 2.8664358MixupTrain:  epoch  0, batch   516 | loss: 3.0486293MixupTrain:  epoch  0, batch   517 | loss: 3.0812550MixupTrain:  epoch  0, batch   518 | loss: 3.0123785MixupTrain:  epoch  0, batch   519 | loss: 2.8132601MixupTrain:  epoch  0, batch   520 | loss: 3.0069423MixupTrain:  epoch  0, batch   521 | loss: 2.8669136MixupTrain:  epoch  0, batch   522 | loss: 3.0658896MixupTrain:  epoch  0, batch   523 | loss: 2.9835701MixupTrain:  epoch  0, batch   524 | loss: 2.9762778MixupTrain:  epoch  0, batch   525 | loss: 2.8261549MixupTrain:  epoch  0, batch   526 | loss: 2.8723750MixupTrain:  epoch  0, batch   527 | loss: 2.9104459MixupTrain:  epoch  0, batch   528 | loss: 2.9367299MixupTrain:  epoch  0, batch   529 | loss: 2.8389676MixupTrain:  epoch  0, batch   530 | loss: 2.8786509MixupTrain:  epoch  0, batch   531 | loss: 2.9670677MixupTrain:  epoch  0, batch   532 | loss: 2.9224336MixupTrain:  epoch  0, batch   533 | loss: 2.9867687MixupTrain:  epoch  0, batch   534 | loss: 2.5157580
MemoryTrain:  epoch  0, batch     0 | loss: 1.2696986MemoryTrain:  epoch  0, batch     1 | loss: 1.2657200MemoryTrain:  epoch  0, batch     2 | loss: 1.4113418MemoryTrain:  epoch  0, batch     3 | loss: 1.5254951MemoryTrain:  epoch  0, batch     4 | loss: 1.7035201MemoryTrain:  epoch  0, batch     5 | loss: 1.6201601MemoryTrain:  epoch  0, batch     6 | loss: 1.9341211MemoryTrain:  epoch  1, batch     0 | loss: 1.3925641MemoryTrain:  epoch  1, batch     1 | loss: 1.2837408MemoryTrain:  epoch  1, batch     2 | loss: 1.2580922MemoryTrain:  epoch  1, batch     3 | loss: 1.2862570MemoryTrain:  epoch  1, batch     4 | loss: 1.2496747MemoryTrain:  epoch  1, batch     5 | loss: 1.2178954MemoryTrain:  epoch  1, batch     6 | loss: 1.3311704MemoryTrain:  epoch  2, batch     0 | loss: 1.2319263MemoryTrain:  epoch  2, batch     1 | loss: 1.2569172MemoryTrain:  epoch  2, batch     2 | loss: 1.2482554MemoryTrain:  epoch  2, batch     3 | loss: 1.2962924MemoryTrain:  epoch  2, batch     4 | loss: 1.2882231MemoryTrain:  epoch  2, batch     5 | loss: 1.2487113MemoryTrain:  epoch  2, batch     6 | loss: 1.2244864MemoryTrain:  epoch  3, batch     0 | loss: 1.2991545MemoryTrain:  epoch  3, batch     1 | loss: 1.2342081MemoryTrain:  epoch  3, batch     2 | loss: 1.2336123MemoryTrain:  epoch  3, batch     3 | loss: 1.2021627MemoryTrain:  epoch  3, batch     4 | loss: 1.1994174MemoryTrain:  epoch  3, batch     5 | loss: 1.3229092MemoryTrain:  epoch  3, batch     6 | loss: 1.2038726MemoryTrain:  epoch  4, batch     0 | loss: 1.2013689MemoryTrain:  epoch  4, batch     1 | loss: 1.2341161MemoryTrain:  epoch  4, batch     2 | loss: 1.1905451MemoryTrain:  epoch  4, batch     3 | loss: 1.1888843MemoryTrain:  epoch  4, batch     4 | loss: 1.2320678MemoryTrain:  epoch  4, batch     5 | loss: 1.1872844MemoryTrain:  epoch  4, batch     6 | loss: 1.2171268MemoryTrain:  epoch  5, batch     0 | loss: 1.1995730MemoryTrain:  epoch  5, batch     1 | loss: 1.1764920MemoryTrain:  epoch  5, batch     2 | loss: 1.1856856MemoryTrain:  epoch  5, batch     3 | loss: 1.2088337MemoryTrain:  epoch  5, batch     4 | loss: 1.2177225MemoryTrain:  epoch  5, batch     5 | loss: 1.2250738MemoryTrain:  epoch  5, batch     6 | loss: 1.2994306MemoryTrain:  epoch  6, batch     0 | loss: 1.2382810MemoryTrain:  epoch  6, batch     1 | loss: 1.1942538MemoryTrain:  epoch  6, batch     2 | loss: 1.1786988MemoryTrain:  epoch  6, batch     3 | loss: 1.1719793MemoryTrain:  epoch  6, batch     4 | loss: 1.1636558MemoryTrain:  epoch  6, batch     5 | loss: 1.2817730MemoryTrain:  epoch  6, batch     6 | loss: 1.2079561MemoryTrain:  epoch  7, batch     0 | loss: 1.1633234MemoryTrain:  epoch  7, batch     1 | loss: 1.2126831MemoryTrain:  epoch  7, batch     2 | loss: 1.1749191MemoryTrain:  epoch  7, batch     3 | loss: 1.2122817MemoryTrain:  epoch  7, batch     4 | loss: 1.2348776MemoryTrain:  epoch  7, batch     5 | loss: 1.1982229MemoryTrain:  epoch  7, batch     6 | loss: 1.1655302MemoryTrain:  epoch  8, batch     0 | loss: 1.2225771MemoryTrain:  epoch  8, batch     1 | loss: 1.1916339MemoryTrain:  epoch  8, batch     2 | loss: 1.1820631MemoryTrain:  epoch  8, batch     3 | loss: 1.1801639MemoryTrain:  epoch  8, batch     4 | loss: 1.2197618MemoryTrain:  epoch  8, batch     5 | loss: 1.2060270MemoryTrain:  epoch  8, batch     6 | loss: 1.2093532MemoryTrain:  epoch  9, batch     0 | loss: 1.1793101MemoryTrain:  epoch  9, batch     1 | loss: 1.1879039MemoryTrain:  epoch  9, batch     2 | loss: 1.2131944MemoryTrain:  epoch  9, batch     3 | loss: 1.1888125MemoryTrain:  epoch  9, batch     4 | loss: 1.1960951MemoryTrain:  epoch  9, batch     5 | loss: 1.1733708MemoryTrain:  epoch  9, batch     6 | loss: 1.2212644
[EVAL] batch:    0 | acc: 50.00%,  total acc: 50.00%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 65.62%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 68.75%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 71.88%   [EVAL] batch:    4 | acc: 100.00%,  total acc: 77.50%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 83.93%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 83.59%   [EVAL] batch:    8 | acc: 37.50%,  total acc: 78.47%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 76.88%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 77.27%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 78.12%   [EVAL] batch:   12 | acc: 50.00%,  total acc: 75.96%   
[EVAL] batch:    0 | acc: 6.25%,  total acc: 6.25%   [EVAL] batch:    1 | acc: 25.00%,  total acc: 15.62%   [EVAL] batch:    2 | acc: 18.75%,  total acc: 16.67%   [EVAL] batch:    3 | acc: 6.25%,  total acc: 14.06%   [EVAL] batch:    4 | acc: 12.50%,  total acc: 13.75%   [EVAL] batch:    5 | acc: 18.75%,  total acc: 14.58%   [EVAL] batch:    6 | acc: 56.25%,  total acc: 20.54%   [EVAL] batch:    7 | acc: 43.75%,  total acc: 23.44%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 28.47%   [EVAL] batch:    9 | acc: 50.00%,  total acc: 30.63%   [EVAL] batch:   10 | acc: 62.50%,  total acc: 33.52%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 36.98%   [EVAL] batch:   12 | acc: 31.25%,  total acc: 36.54%   [EVAL] batch:   13 | acc: 37.50%,  total acc: 36.61%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 39.17%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 40.23%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 42.28%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 43.40%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 44.41%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 46.25%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 48.81%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 50.85%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 52.99%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 54.69%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 56.50%   [EVAL] batch:   25 | acc: 93.75%,  total acc: 57.93%   [EVAL] batch:   26 | acc: 81.25%,  total acc: 58.80%   [EVAL] batch:   27 | acc: 87.50%,  total acc: 59.82%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 60.99%   [EVAL] batch:   29 | acc: 62.50%,  total acc: 61.04%   [EVAL] batch:   30 | acc: 62.50%,  total acc: 61.09%   [EVAL] batch:   31 | acc: 75.00%,  total acc: 61.52%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 60.80%   [EVAL] batch:   33 | acc: 0.00%,  total acc: 59.01%   [EVAL] batch:   34 | acc: 6.25%,  total acc: 57.50%   [EVAL] batch:   35 | acc: 6.25%,  total acc: 56.08%   [EVAL] batch:   36 | acc: 0.00%,  total acc: 54.56%   [EVAL] batch:   37 | acc: 50.00%,  total acc: 54.44%   [EVAL] batch:   38 | acc: 75.00%,  total acc: 54.97%   [EVAL] batch:   39 | acc: 93.75%,  total acc: 55.94%   [EVAL] batch:   40 | acc: 75.00%,  total acc: 56.40%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 57.29%   [EVAL] batch:   42 | acc: 12.50%,  total acc: 56.25%   [EVAL] batch:   43 | acc: 0.00%,  total acc: 54.97%   [EVAL] batch:   44 | acc: 0.00%,  total acc: 53.75%   [EVAL] batch:   45 | acc: 0.00%,  total acc: 52.58%   [EVAL] batch:   46 | acc: 18.75%,  total acc: 51.86%   [EVAL] batch:   47 | acc: 62.50%,  total acc: 52.08%   [EVAL] batch:   48 | acc: 6.25%,  total acc: 51.15%   [EVAL] batch:   49 | acc: 6.25%,  total acc: 50.25%   [EVAL] batch:   50 | acc: 0.00%,  total acc: 49.26%   [EVAL] batch:   51 | acc: 0.00%,  total acc: 48.32%   [EVAL] batch:   52 | acc: 0.00%,  total acc: 47.41%   [EVAL] batch:   53 | acc: 37.50%,  total acc: 47.22%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 47.95%   [EVAL] batch:   55 | acc: 81.25%,  total acc: 48.55%   [EVAL] batch:   56 | acc: 62.50%,  total acc: 48.79%   [EVAL] batch:   57 | acc: 62.50%,  total acc: 49.03%   [EVAL] batch:   58 | acc: 75.00%,  total acc: 49.47%   [EVAL] batch:   59 | acc: 50.00%,  total acc: 49.48%   [EVAL] batch:   60 | acc: 12.50%,  total acc: 48.87%   [EVAL] batch:   61 | acc: 0.00%,  total acc: 48.08%   [EVAL] batch:   62 | acc: 18.75%,  total acc: 47.62%   [EVAL] batch:   63 | acc: 12.50%,  total acc: 47.07%   [EVAL] batch:   64 | acc: 0.00%,  total acc: 46.35%   [EVAL] batch:   65 | acc: 12.50%,  total acc: 45.83%   [EVAL] batch:   66 | acc: 12.50%,  total acc: 45.34%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 46.14%   [EVAL] batch:   68 | acc: 81.25%,  total acc: 46.65%   [EVAL] batch:   69 | acc: 43.75%,  total acc: 46.61%   [EVAL] batch:   70 | acc: 62.50%,  total acc: 46.83%   [EVAL] batch:   71 | acc: 62.50%,  total acc: 47.05%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 47.77%   [EVAL] batch:   73 | acc: 100.00%,  total acc: 48.48%   [EVAL] batch:   74 | acc: 100.00%,  total acc: 49.17%   [EVAL] batch:   75 | acc: 100.00%,  total acc: 49.84%   [EVAL] batch:   76 | acc: 100.00%,  total acc: 50.49%   [EVAL] batch:   77 | acc: 75.00%,  total acc: 50.80%   [EVAL] batch:   78 | acc: 18.75%,  total acc: 50.40%   [EVAL] batch:   79 | acc: 25.00%,  total acc: 50.08%   [EVAL] batch:   80 | acc: 18.75%,  total acc: 49.69%   [EVAL] batch:   81 | acc: 25.00%,  total acc: 49.39%   [EVAL] batch:   82 | acc: 25.00%,  total acc: 49.10%   [EVAL] batch:   83 | acc: 0.00%,  total acc: 48.51%   [EVAL] batch:   84 | acc: 43.75%,  total acc: 48.46%   [EVAL] batch:   85 | acc: 68.75%,  total acc: 48.69%   [EVAL] batch:   86 | acc: 50.00%,  total acc: 48.71%   [EVAL] batch:   87 | acc: 62.50%,  total acc: 48.86%   [EVAL] batch:   88 | acc: 75.00%,  total acc: 49.16%   [EVAL] batch:   89 | acc: 87.50%,  total acc: 49.58%   [EVAL] batch:   90 | acc: 62.50%,  total acc: 49.73%   [EVAL] batch:   91 | acc: 75.00%,  total acc: 50.00%   [EVAL] batch:   92 | acc: 50.00%,  total acc: 50.00%   [EVAL] batch:   93 | acc: 56.25%,  total acc: 50.07%   [EVAL] batch:   94 | acc: 50.00%,  total acc: 50.07%   [EVAL] batch:   95 | acc: 50.00%,  total acc: 50.07%   [EVAL] batch:   96 | acc: 6.25%,  total acc: 49.61%   [EVAL] batch:   97 | acc: 6.25%,  total acc: 49.17%   [EVAL] batch:   98 | acc: 6.25%,  total acc: 48.74%   [EVAL] batch:   99 | acc: 62.50%,  total acc: 48.88%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 49.38%   [EVAL] batch:  101 | acc: 62.50%,  total acc: 49.51%   [EVAL] batch:  102 | acc: 56.25%,  total acc: 49.58%   [EVAL] batch:  103 | acc: 37.50%,  total acc: 49.46%   [EVAL] batch:  104 | acc: 43.75%,  total acc: 49.40%   [EVAL] batch:  105 | acc: 37.50%,  total acc: 49.29%   [EVAL] batch:  106 | acc: 43.75%,  total acc: 49.24%   [EVAL] batch:  107 | acc: 75.00%,  total acc: 49.48%   [EVAL] batch:  108 | acc: 75.00%,  total acc: 49.71%   [EVAL] batch:  109 | acc: 75.00%,  total acc: 49.94%   [EVAL] batch:  110 | acc: 100.00%,  total acc: 50.39%   [EVAL] batch:  111 | acc: 100.00%,  total acc: 50.84%   [EVAL] batch:  112 | acc: 100.00%,  total acc: 51.27%   [EVAL] batch:  113 | acc: 93.75%,  total acc: 51.64%   [EVAL] batch:  114 | acc: 50.00%,  total acc: 51.63%   [EVAL] batch:  115 | acc: 50.00%,  total acc: 51.62%   [EVAL] batch:  116 | acc: 68.75%,  total acc: 51.76%   [EVAL] batch:  117 | acc: 93.75%,  total acc: 52.12%   [EVAL] batch:  118 | acc: 75.00%,  total acc: 52.31%   
cur_acc:  ['0.8580', '0.7083', '0.7232', '0.8299', '0.6591', '0.7344', '0.7596']
his_acc:  ['0.8580', '0.8285', '0.7500', '0.6723', '0.6275', '0.5362', '0.5231']
CurrentTrain: epoch  0, batch     0 | loss: 4.5534611CurrentTrain: epoch  0, batch     1 | loss: 4.3132548CurrentTrain: epoch  1, batch     0 | loss: 3.4098606CurrentTrain: epoch  1, batch     1 | loss: 3.0753870CurrentTrain: epoch  2, batch     0 | loss: 2.8561044CurrentTrain: epoch  2, batch     1 | loss: 2.6244016CurrentTrain: epoch  3, batch     0 | loss: 2.6168363CurrentTrain: epoch  3, batch     1 | loss: 2.2845333CurrentTrain: epoch  4, batch     0 | loss: 2.3942380CurrentTrain: epoch  4, batch     1 | loss: 2.0007412CurrentTrain: epoch  5, batch     0 | loss: 1.9602816CurrentTrain: epoch  5, batch     1 | loss: 2.1241343CurrentTrain: epoch  6, batch     0 | loss: 1.9430168CurrentTrain: epoch  6, batch     1 | loss: 2.0851700CurrentTrain: epoch  7, batch     0 | loss: 1.9088866CurrentTrain: epoch  7, batch     1 | loss: 1.9048373CurrentTrain: epoch  8, batch     0 | loss: 1.8486345CurrentTrain: epoch  8, batch     1 | loss: 1.9527841CurrentTrain: epoch  9, batch     0 | loss: 1.8008300CurrentTrain: epoch  9, batch     1 | loss: 1.7985325
#### organization founded ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization founded" means an organization was found in a date.
Here is an example:
Relation: organization founded
Context: the jnf was founded in 1901 to buy plots in palestine , then ruled by the ottomans .
Head Entity: jnf
Tail Entity: 1901
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization founded":
Relation: organization founded  
Context: the united nations was established in 1945 to promote international cooperation and peace.  
Head Entity: united nations  
Tail Entity: 1945  

Relation: organization founded  
Context: the world health organization was created in 1948 to coordinate global health efforts.  
Head Entity: world health organization  
Tail Entity: 1948  
#### person age ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person age" means a person 's age is a number.
Here is an example:
Relation: person age
Context: paris , feb 23 -lrb- xinhua -rrb- yoadimnadji , 56 , died of a cardiovascular problem at midnight .
Head Entity: yoadimnadji
Tail Entity: 56
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person age":
Relation: person age  
Context: In a recent interview, the renowned author, Jane Doe, revealed that she is 34 years old and still has many stories to tell.  
Head Entity: Jane Doe  
Tail Entity: 34  

Relation: person age  
Context: During the family reunion, Uncle Bob proudly announced that he turned 70 last month, celebrating a lifetime of memories.  
Head Entity: Uncle Bob  
Tail Entity: 70  
#### person city of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person city of birth" means a person was born in a city.
Here is an example:
Relation: person city of birth
Context: born belle miriam silverman in brooklyn , she quickly became bubbles , an endearment coined by the doctor who delivered her , noting that she was born blowing a bubble of spit from her little mouth .
Head Entity: she
Tail Entity: brooklyn
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person city of birth":
Relation: person city of birth  
Context: born in the vibrant city of los angeles, the actor quickly rose to fame, captivating audiences with his performances.  
Head Entity: the actor  
Tail Entity: los angeles  

Relation: person city of birth  
Context: she was born in the historic city of rome, where her family has lived for generations, steeped in rich culture and tradition.  
Head Entity: she  
Tail Entity: rome  
#### organization members ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization members" means an organization 's member is an organization.
Here is an example:
Relation: organization members
Context: sun plays for the grand rapids flight of the international basketball league after toiling for the maryland nighthawks of the american basketball association , both development leagues for those who dream of an nba career .
Head Entity: american basketball association
Tail Entity: maryland nighthawks
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization members":
Relation: organization members  
Context: The New York Philharmonic is one of the oldest orchestras in the United States, and it has had many notable musicians as members, including the famous conductor Leonard Bernstein.  
Head Entity: New York Philharmonic  
Tail Entity: Leonard Bernstein  

Relation: organization members  
Context: The National Football League has a long history of legendary players, and one of its most famous members is Joe Montana, who played for the San Francisco 49ers.  
Head Entity: National Football League  
Tail Entity: Joe Montana  
#### person religion ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person religion" means an person is the member of a religion.
Here is an example:
Relation: person religion
Context: the pope defended his action on the grounds that he could not refuse an audience to a head of state from a country with a strong catholic tradition unless he had clear-cut proof of the allegations against him .
Head Entity: he
Tail Entity: catholic
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person religion":
Relation: person religion  
Context: After years of study and reflection, Maria decided to embrace Buddhism, finding peace and purpose in its teachings.  
Head Entity: Maria  
Tail Entity: Buddhism  

Relation: person religion  
Context: During the festival, Ahmed proudly wore his traditional attire, celebrating his deep connection to Islam and its rich cultural heritage.  
Head Entity: Ahmed  
Tail Entity: Islam  
Mixup data size:  10630
MixupTrain:  epoch  0, batch     0 | loss: 3.6048734MixupTrain:  epoch  0, batch     1 | loss: 3.8480506MixupTrain:  epoch  0, batch     2 | loss: 3.8985133MixupTrain:  epoch  0, batch     3 | loss: 3.6388586MixupTrain:  epoch  0, batch     4 | loss: 3.4487388MixupTrain:  epoch  0, batch     5 | loss: 3.3083189MixupTrain:  epoch  0, batch     6 | loss: 3.4362574MixupTrain:  epoch  0, batch     7 | loss: 3.6367159MixupTrain:  epoch  0, batch     8 | loss: 3.4923248MixupTrain:  epoch  0, batch     9 | loss: 3.9030063MixupTrain:  epoch  0, batch    10 | loss: 3.2769673MixupTrain:  epoch  0, batch    11 | loss: 3.3528419MixupTrain:  epoch  0, batch    12 | loss: 3.4216294MixupTrain:  epoch  0, batch    13 | loss: 3.1858301MixupTrain:  epoch  0, batch    14 | loss: 3.3445063MixupTrain:  epoch  0, batch    15 | loss: 3.3473392MixupTrain:  epoch  0, batch    16 | loss: 3.3855872MixupTrain:  epoch  0, batch    17 | loss: 3.4553065MixupTrain:  epoch  0, batch    18 | loss: 3.2728243MixupTrain:  epoch  0, batch    19 | loss: 3.6508803MixupTrain:  epoch  0, batch    20 | loss: 3.2901232MixupTrain:  epoch  0, batch    21 | loss: 3.4378858MixupTrain:  epoch  0, batch    22 | loss: 3.0719669MixupTrain:  epoch  0, batch    23 | loss: 3.1465111MixupTrain:  epoch  0, batch    24 | loss: 3.1338809MixupTrain:  epoch  0, batch    25 | loss: 3.6555557MixupTrain:  epoch  0, batch    26 | loss: 3.2997327MixupTrain:  epoch  0, batch    27 | loss: 3.2724607MixupTrain:  epoch  0, batch    28 | loss: 3.4735045MixupTrain:  epoch  0, batch    29 | loss: 3.2766223MixupTrain:  epoch  0, batch    30 | loss: 3.0588470MixupTrain:  epoch  0, batch    31 | loss: 3.0490589MixupTrain:  epoch  0, batch    32 | loss: 3.4156518MixupTrain:  epoch  0, batch    33 | loss: 3.0162768MixupTrain:  epoch  0, batch    34 | loss: 3.4418221MixupTrain:  epoch  0, batch    35 | loss: 3.1868813MixupTrain:  epoch  0, batch    36 | loss: 3.1144943MixupTrain:  epoch  0, batch    37 | loss: 3.5700645MixupTrain:  epoch  0, batch    38 | loss: 3.3865190MixupTrain:  epoch  0, batch    39 | loss: 3.5330877MixupTrain:  epoch  0, batch    40 | loss: 3.3304701MixupTrain:  epoch  0, batch    41 | loss: 3.2994118MixupTrain:  epoch  0, batch    42 | loss: 2.9573560MixupTrain:  epoch  0, batch    43 | loss: 3.2003207MixupTrain:  epoch  0, batch    44 | loss: 3.2420132MixupTrain:  epoch  0, batch    45 | loss: 3.2351561MixupTrain:  epoch  0, batch    46 | loss: 3.1535904MixupTrain:  epoch  0, batch    47 | loss: 3.0109890MixupTrain:  epoch  0, batch    48 | loss: 3.0069387MixupTrain:  epoch  0, batch    49 | loss: 3.0334604MixupTrain:  epoch  0, batch    50 | loss: 3.2104752MixupTrain:  epoch  0, batch    51 | loss: 3.0744023MixupTrain:  epoch  0, batch    52 | loss: 3.4359136MixupTrain:  epoch  0, batch    53 | loss: 2.9314897MixupTrain:  epoch  0, batch    54 | loss: 3.3540287MixupTrain:  epoch  0, batch    55 | loss: 3.6591311MixupTrain:  epoch  0, batch    56 | loss: 3.1075349MixupTrain:  epoch  0, batch    57 | loss: 3.1038389MixupTrain:  epoch  0, batch    58 | loss: 3.1095276MixupTrain:  epoch  0, batch    59 | loss: 3.0303068MixupTrain:  epoch  0, batch    60 | loss: 3.1836033MixupTrain:  epoch  0, batch    61 | loss: 3.5910399MixupTrain:  epoch  0, batch    62 | loss: 2.9941320MixupTrain:  epoch  0, batch    63 | loss: 3.0973094MixupTrain:  epoch  0, batch    64 | loss: 3.0044699MixupTrain:  epoch  0, batch    65 | loss: 3.2760673MixupTrain:  epoch  0, batch    66 | loss: 3.0099783MixupTrain:  epoch  0, batch    67 | loss: 3.2348204MixupTrain:  epoch  0, batch    68 | loss: 3.1057730MixupTrain:  epoch  0, batch    69 | loss: 3.0590334MixupTrain:  epoch  0, batch    70 | loss: 2.8705888MixupTrain:  epoch  0, batch    71 | loss: 2.9870162MixupTrain:  epoch  0, batch    72 | loss: 2.9976296MixupTrain:  epoch  0, batch    73 | loss: 3.1741390MixupTrain:  epoch  0, batch    74 | loss: 3.0978973MixupTrain:  epoch  0, batch    75 | loss: 3.4016526MixupTrain:  epoch  0, batch    76 | loss: 3.3163786MixupTrain:  epoch  0, batch    77 | loss: 3.0864890MixupTrain:  epoch  0, batch    78 | loss: 2.9304180MixupTrain:  epoch  0, batch    79 | loss: 3.3048761MixupTrain:  epoch  0, batch    80 | loss: 3.1113117MixupTrain:  epoch  0, batch    81 | loss: 3.2405376MixupTrain:  epoch  0, batch    82 | loss: 3.2848239MixupTrain:  epoch  0, batch    83 | loss: 3.1159253MixupTrain:  epoch  0, batch    84 | loss: 2.9789917MixupTrain:  epoch  0, batch    85 | loss: 3.0820637MixupTrain:  epoch  0, batch    86 | loss: 3.0652063MixupTrain:  epoch  0, batch    87 | loss: 3.0952852MixupTrain:  epoch  0, batch    88 | loss: 3.1470780MixupTrain:  epoch  0, batch    89 | loss: 3.3024211MixupTrain:  epoch  0, batch    90 | loss: 2.9997203MixupTrain:  epoch  0, batch    91 | loss: 2.9599094MixupTrain:  epoch  0, batch    92 | loss: 3.0480857MixupTrain:  epoch  0, batch    93 | loss: 2.9842215MixupTrain:  epoch  0, batch    94 | loss: 3.1216977MixupTrain:  epoch  0, batch    95 | loss: 3.1850848MixupTrain:  epoch  0, batch    96 | loss: 2.8215237MixupTrain:  epoch  0, batch    97 | loss: 3.2609468MixupTrain:  epoch  0, batch    98 | loss: 3.0973744MixupTrain:  epoch  0, batch    99 | loss: 2.9953079MixupTrain:  epoch  0, batch   100 | loss: 3.0492892MixupTrain:  epoch  0, batch   101 | loss: 3.1060305MixupTrain:  epoch  0, batch   102 | loss: 3.1573851MixupTrain:  epoch  0, batch   103 | loss: 3.1120872MixupTrain:  epoch  0, batch   104 | loss: 3.0177224MixupTrain:  epoch  0, batch   105 | loss: 3.1832776MixupTrain:  epoch  0, batch   106 | loss: 3.0876675MixupTrain:  epoch  0, batch   107 | loss: 2.9334016MixupTrain:  epoch  0, batch   108 | loss: 3.1130662MixupTrain:  epoch  0, batch   109 | loss: 3.2022047MixupTrain:  epoch  0, batch   110 | loss: 2.8525796MixupTrain:  epoch  0, batch   111 | loss: 3.0462523MixupTrain:  epoch  0, batch   112 | loss: 3.0387383MixupTrain:  epoch  0, batch   113 | loss: 3.0831242MixupTrain:  epoch  0, batch   114 | loss: 3.3515344MixupTrain:  epoch  0, batch   115 | loss: 3.1505771MixupTrain:  epoch  0, batch   116 | loss: 3.1022537MixupTrain:  epoch  0, batch   117 | loss: 2.9845676MixupTrain:  epoch  0, batch   118 | loss: 3.0017848MixupTrain:  epoch  0, batch   119 | loss: 2.9985805MixupTrain:  epoch  0, batch   120 | loss: 3.0358067MixupTrain:  epoch  0, batch   121 | loss: 3.1175349MixupTrain:  epoch  0, batch   122 | loss: 3.0757384MixupTrain:  epoch  0, batch   123 | loss: 3.1019022MixupTrain:  epoch  0, batch   124 | loss: 2.8925750MixupTrain:  epoch  0, batch   125 | loss: 3.1459556MixupTrain:  epoch  0, batch   126 | loss: 3.0160007MixupTrain:  epoch  0, batch   127 | loss: 3.3690767MixupTrain:  epoch  0, batch   128 | loss: 3.0178132MixupTrain:  epoch  0, batch   129 | loss: 3.1469712MixupTrain:  epoch  0, batch   130 | loss: 2.9441183MixupTrain:  epoch  0, batch   131 | loss: 3.1829398MixupTrain:  epoch  0, batch   132 | loss: 3.0217104MixupTrain:  epoch  0, batch   133 | loss: 3.0284462MixupTrain:  epoch  0, batch   134 | loss: 3.0892627MixupTrain:  epoch  0, batch   135 | loss: 2.9358058MixupTrain:  epoch  0, batch   136 | loss: 2.9815183MixupTrain:  epoch  0, batch   137 | loss: 3.2601814MixupTrain:  epoch  0, batch   138 | loss: 2.9394562MixupTrain:  epoch  0, batch   139 | loss: 2.9493878MixupTrain:  epoch  0, batch   140 | loss: 3.0375962MixupTrain:  epoch  0, batch   141 | loss: 3.1746867MixupTrain:  epoch  0, batch   142 | loss: 2.9230371MixupTrain:  epoch  0, batch   143 | loss: 3.0591958MixupTrain:  epoch  0, batch   144 | loss: 3.0768003MixupTrain:  epoch  0, batch   145 | loss: 2.8513951MixupTrain:  epoch  0, batch   146 | loss: 3.0907409MixupTrain:  epoch  0, batch   147 | loss: 2.9521358MixupTrain:  epoch  0, batch   148 | loss: 3.2988613MixupTrain:  epoch  0, batch   149 | loss: 2.9317212MixupTrain:  epoch  0, batch   150 | loss: 2.9845490MixupTrain:  epoch  0, batch   151 | loss: 2.9736922MixupTrain:  epoch  0, batch   152 | loss: 2.9771643MixupTrain:  epoch  0, batch   153 | loss: 3.0261750MixupTrain:  epoch  0, batch   154 | loss: 2.9688191MixupTrain:  epoch  0, batch   155 | loss: 2.9395220MixupTrain:  epoch  0, batch   156 | loss: 3.2340746MixupTrain:  epoch  0, batch   157 | loss: 3.0642648MixupTrain:  epoch  0, batch   158 | loss: 2.8943768MixupTrain:  epoch  0, batch   159 | loss: 2.9961290MixupTrain:  epoch  0, batch   160 | loss: 3.0951495MixupTrain:  epoch  0, batch   161 | loss: 2.9159536MixupTrain:  epoch  0, batch   162 | loss: 2.8615232MixupTrain:  epoch  0, batch   163 | loss: 3.0416532MixupTrain:  epoch  0, batch   164 | loss: 3.1081798MixupTrain:  epoch  0, batch   165 | loss: 2.8998294MixupTrain:  epoch  0, batch   166 | loss: 2.9712517MixupTrain:  epoch  0, batch   167 | loss: 3.0933027MixupTrain:  epoch  0, batch   168 | loss: 3.3478765MixupTrain:  epoch  0, batch   169 | loss: 3.0464916MixupTrain:  epoch  0, batch   170 | loss: 2.8751974MixupTrain:  epoch  0, batch   171 | loss: 2.8939462MixupTrain:  epoch  0, batch   172 | loss: 3.0149393MixupTrain:  epoch  0, batch   173 | loss: 3.1188934MixupTrain:  epoch  0, batch   174 | loss: 3.2050195MixupTrain:  epoch  0, batch   175 | loss: 2.9208913MixupTrain:  epoch  0, batch   176 | loss: 2.9408708MixupTrain:  epoch  0, batch   177 | loss: 3.0634551MixupTrain:  epoch  0, batch   178 | loss: 3.1349292MixupTrain:  epoch  0, batch   179 | loss: 2.9709182MixupTrain:  epoch  0, batch   180 | loss: 3.2306647MixupTrain:  epoch  0, batch   181 | loss: 3.1111593MixupTrain:  epoch  0, batch   182 | loss: 3.0883808MixupTrain:  epoch  0, batch   183 | loss: 3.2114453MixupTrain:  epoch  0, batch   184 | loss: 3.3308020MixupTrain:  epoch  0, batch   185 | loss: 3.0178857MixupTrain:  epoch  0, batch   186 | loss: 2.9580612MixupTrain:  epoch  0, batch   187 | loss: 2.9337468MixupTrain:  epoch  0, batch   188 | loss: 2.9555101MixupTrain:  epoch  0, batch   189 | loss: 3.0732961MixupTrain:  epoch  0, batch   190 | loss: 3.0641618MixupTrain:  epoch  0, batch   191 | loss: 2.9656186MixupTrain:  epoch  0, batch   192 | loss: 3.0729256MixupTrain:  epoch  0, batch   193 | loss: 3.1366439MixupTrain:  epoch  0, batch   194 | loss: 3.0327158MixupTrain:  epoch  0, batch   195 | loss: 3.2743874MixupTrain:  epoch  0, batch   196 | loss: 3.0903537MixupTrain:  epoch  0, batch   197 | loss: 2.9683771MixupTrain:  epoch  0, batch   198 | loss: 2.9674430MixupTrain:  epoch  0, batch   199 | loss: 3.0986898MixupTrain:  epoch  0, batch   200 | loss: 2.8525381MixupTrain:  epoch  0, batch   201 | loss: 3.0508347MixupTrain:  epoch  0, batch   202 | loss: 3.2017703MixupTrain:  epoch  0, batch   203 | loss: 3.2043386MixupTrain:  epoch  0, batch   204 | loss: 3.0492654MixupTrain:  epoch  0, batch   205 | loss: 2.8840494MixupTrain:  epoch  0, batch   206 | loss: 3.1263542MixupTrain:  epoch  0, batch   207 | loss: 2.9977856MixupTrain:  epoch  0, batch   208 | loss: 3.0154364MixupTrain:  epoch  0, batch   209 | loss: 3.0126882MixupTrain:  epoch  0, batch   210 | loss: 3.0702825MixupTrain:  epoch  0, batch   211 | loss: 2.9991145MixupTrain:  epoch  0, batch   212 | loss: 3.0536473MixupTrain:  epoch  0, batch   213 | loss: 2.9559929MixupTrain:  epoch  0, batch   214 | loss: 2.9551685MixupTrain:  epoch  0, batch   215 | loss: 2.8613434MixupTrain:  epoch  0, batch   216 | loss: 3.1524732MixupTrain:  epoch  0, batch   217 | loss: 3.0214198MixupTrain:  epoch  0, batch   218 | loss: 3.1967232MixupTrain:  epoch  0, batch   219 | loss: 2.9859490MixupTrain:  epoch  0, batch   220 | loss: 3.0712414MixupTrain:  epoch  0, batch   221 | loss: 3.0040245MixupTrain:  epoch  0, batch   222 | loss: 2.9587312MixupTrain:  epoch  0, batch   223 | loss: 2.9680846MixupTrain:  epoch  0, batch   224 | loss: 3.0021915MixupTrain:  epoch  0, batch   225 | loss: 2.9191689MixupTrain:  epoch  0, batch   226 | loss: 2.9406283MixupTrain:  epoch  0, batch   227 | loss: 2.8885908MixupTrain:  epoch  0, batch   228 | loss: 2.9919348MixupTrain:  epoch  0, batch   229 | loss: 3.0278802MixupTrain:  epoch  0, batch   230 | loss: 3.0656660MixupTrain:  epoch  0, batch   231 | loss: 3.0226946MixupTrain:  epoch  0, batch   232 | loss: 3.0384052MixupTrain:  epoch  0, batch   233 | loss: 2.8010492MixupTrain:  epoch  0, batch   234 | loss: 3.0829396MixupTrain:  epoch  0, batch   235 | loss: 2.9217892MixupTrain:  epoch  0, batch   236 | loss: 2.9660723MixupTrain:  epoch  0, batch   237 | loss: 3.1169484MixupTrain:  epoch  0, batch   238 | loss: 2.9462113MixupTrain:  epoch  0, batch   239 | loss: 3.0091033MixupTrain:  epoch  0, batch   240 | loss: 3.0359511MixupTrain:  epoch  0, batch   241 | loss: 3.2501435MixupTrain:  epoch  0, batch   242 | loss: 3.0022361MixupTrain:  epoch  0, batch   243 | loss: 3.0928414MixupTrain:  epoch  0, batch   244 | loss: 2.9136233MixupTrain:  epoch  0, batch   245 | loss: 2.9811490MixupTrain:  epoch  0, batch   246 | loss: 3.2587833MixupTrain:  epoch  0, batch   247 | loss: 2.9951243MixupTrain:  epoch  0, batch   248 | loss: 3.0360675MixupTrain:  epoch  0, batch   249 | loss: 3.0726147MixupTrain:  epoch  0, batch   250 | loss: 2.8828704MixupTrain:  epoch  0, batch   251 | loss: 2.9943361MixupTrain:  epoch  0, batch   252 | loss: 3.0042148MixupTrain:  epoch  0, batch   253 | loss: 3.1584263MixupTrain:  epoch  0, batch   254 | loss: 3.0951762MixupTrain:  epoch  0, batch   255 | loss: 2.8735998MixupTrain:  epoch  0, batch   256 | loss: 2.9244912MixupTrain:  epoch  0, batch   257 | loss: 3.0557513MixupTrain:  epoch  0, batch   258 | loss: 3.0649552MixupTrain:  epoch  0, batch   259 | loss: 3.0158496MixupTrain:  epoch  0, batch   260 | loss: 2.9710553MixupTrain:  epoch  0, batch   261 | loss: 2.9277415MixupTrain:  epoch  0, batch   262 | loss: 2.9964755MixupTrain:  epoch  0, batch   263 | loss: 2.8342879MixupTrain:  epoch  0, batch   264 | loss: 3.0371189MixupTrain:  epoch  0, batch   265 | loss: 2.9679918MixupTrain:  epoch  0, batch   266 | loss: 2.9935894MixupTrain:  epoch  0, batch   267 | loss: 2.9128320MixupTrain:  epoch  0, batch   268 | loss: 2.9489903MixupTrain:  epoch  0, batch   269 | loss: 3.0187302MixupTrain:  epoch  0, batch   270 | loss: 2.9068866MixupTrain:  epoch  0, batch   271 | loss: 3.1765199MixupTrain:  epoch  0, batch   272 | loss: 2.9928663MixupTrain:  epoch  0, batch   273 | loss: 3.0759678MixupTrain:  epoch  0, batch   274 | loss: 3.0436578MixupTrain:  epoch  0, batch   275 | loss: 2.9979656MixupTrain:  epoch  0, batch   276 | loss: 3.1636631MixupTrain:  epoch  0, batch   277 | loss: 3.0650477MixupTrain:  epoch  0, batch   278 | loss: 2.9580610MixupTrain:  epoch  0, batch   279 | loss: 3.0287006MixupTrain:  epoch  0, batch   280 | loss: 2.9828401MixupTrain:  epoch  0, batch   281 | loss: 3.0866492MixupTrain:  epoch  0, batch   282 | loss: 2.9234402MixupTrain:  epoch  0, batch   283 | loss: 2.9460387MixupTrain:  epoch  0, batch   284 | loss: 3.0311193MixupTrain:  epoch  0, batch   285 | loss: 2.9666505MixupTrain:  epoch  0, batch   286 | loss: 2.9118433MixupTrain:  epoch  0, batch   287 | loss: 3.0278625MixupTrain:  epoch  0, batch   288 | loss: 3.0877275MixupTrain:  epoch  0, batch   289 | loss: 3.1044202MixupTrain:  epoch  0, batch   290 | loss: 3.0281792MixupTrain:  epoch  0, batch   291 | loss: 3.0937219MixupTrain:  epoch  0, batch   292 | loss: 3.0655034MixupTrain:  epoch  0, batch   293 | loss: 2.8746541MixupTrain:  epoch  0, batch   294 | loss: 2.8882656MixupTrain:  epoch  0, batch   295 | loss: 2.9296606MixupTrain:  epoch  0, batch   296 | loss: 2.9113445MixupTrain:  epoch  0, batch   297 | loss: 2.8654366MixupTrain:  epoch  0, batch   298 | loss: 3.1045825MixupTrain:  epoch  0, batch   299 | loss: 2.7944129MixupTrain:  epoch  0, batch   300 | loss: 2.8297539MixupTrain:  epoch  0, batch   301 | loss: 3.0735011MixupTrain:  epoch  0, batch   302 | loss: 3.0498006MixupTrain:  epoch  0, batch   303 | loss: 2.9369650MixupTrain:  epoch  0, batch   304 | loss: 3.1513700MixupTrain:  epoch  0, batch   305 | loss: 2.9893804MixupTrain:  epoch  0, batch   306 | loss: 2.9404159MixupTrain:  epoch  0, batch   307 | loss: 3.0890002MixupTrain:  epoch  0, batch   308 | loss: 3.1193764MixupTrain:  epoch  0, batch   309 | loss: 3.0519071MixupTrain:  epoch  0, batch   310 | loss: 2.7892804MixupTrain:  epoch  0, batch   311 | loss: 2.9841785MixupTrain:  epoch  0, batch   312 | loss: 3.1257873MixupTrain:  epoch  0, batch   313 | loss: 3.0093880MixupTrain:  epoch  0, batch   314 | loss: 3.0895298MixupTrain:  epoch  0, batch   315 | loss: 2.8310981MixupTrain:  epoch  0, batch   316 | loss: 2.8771410MixupTrain:  epoch  0, batch   317 | loss: 3.0314198MixupTrain:  epoch  0, batch   318 | loss: 2.7695022MixupTrain:  epoch  0, batch   319 | loss: 2.9075861MixupTrain:  epoch  0, batch   320 | loss: 2.8649220MixupTrain:  epoch  0, batch   321 | loss: 2.8749137MixupTrain:  epoch  0, batch   322 | loss: 2.9649305MixupTrain:  epoch  0, batch   323 | loss: 3.0610268MixupTrain:  epoch  0, batch   324 | loss: 3.0996723MixupTrain:  epoch  0, batch   325 | loss: 3.0171976MixupTrain:  epoch  0, batch   326 | loss: 3.0211558MixupTrain:  epoch  0, batch   327 | loss: 3.1003041MixupTrain:  epoch  0, batch   328 | loss: 2.9773655MixupTrain:  epoch  0, batch   329 | loss: 2.9153428MixupTrain:  epoch  0, batch   330 | loss: 2.8649831MixupTrain:  epoch  0, batch   331 | loss: 3.0168691MixupTrain:  epoch  0, batch   332 | loss: 2.9698000MixupTrain:  epoch  0, batch   333 | loss: 3.1447225MixupTrain:  epoch  0, batch   334 | loss: 2.8219419MixupTrain:  epoch  0, batch   335 | loss: 2.9699979MixupTrain:  epoch  0, batch   336 | loss: 2.8582377MixupTrain:  epoch  0, batch   337 | loss: 2.8285480MixupTrain:  epoch  0, batch   338 | loss: 2.9486153MixupTrain:  epoch  0, batch   339 | loss: 2.8593855MixupTrain:  epoch  0, batch   340 | loss: 2.9556856MixupTrain:  epoch  0, batch   341 | loss: 3.0477340MixupTrain:  epoch  0, batch   342 | loss: 2.9634509MixupTrain:  epoch  0, batch   343 | loss: 3.0262151MixupTrain:  epoch  0, batch   344 | loss: 2.8451691MixupTrain:  epoch  0, batch   345 | loss: 2.9752140MixupTrain:  epoch  0, batch   346 | loss: 2.9444039MixupTrain:  epoch  0, batch   347 | loss: 3.0267441MixupTrain:  epoch  0, batch   348 | loss: 3.0911677MixupTrain:  epoch  0, batch   349 | loss: 3.0500772MixupTrain:  epoch  0, batch   350 | loss: 3.0590057MixupTrain:  epoch  0, batch   351 | loss: 3.2228367MixupTrain:  epoch  0, batch   352 | loss: 2.9705408MixupTrain:  epoch  0, batch   353 | loss: 2.9496379MixupTrain:  epoch  0, batch   354 | loss: 2.9931917MixupTrain:  epoch  0, batch   355 | loss: 2.9360113MixupTrain:  epoch  0, batch   356 | loss: 2.9381282MixupTrain:  epoch  0, batch   357 | loss: 2.9711533MixupTrain:  epoch  0, batch   358 | loss: 2.9658446MixupTrain:  epoch  0, batch   359 | loss: 2.9215066MixupTrain:  epoch  0, batch   360 | loss: 3.0345473MixupTrain:  epoch  0, batch   361 | loss: 2.8158517MixupTrain:  epoch  0, batch   362 | loss: 3.1323562MixupTrain:  epoch  0, batch   363 | loss: 3.0991824MixupTrain:  epoch  0, batch   364 | loss: 3.0155480MixupTrain:  epoch  0, batch   365 | loss: 2.8946137MixupTrain:  epoch  0, batch   366 | loss: 3.0912154MixupTrain:  epoch  0, batch   367 | loss: 2.9155588MixupTrain:  epoch  0, batch   368 | loss: 2.9989853MixupTrain:  epoch  0, batch   369 | loss: 2.9758072MixupTrain:  epoch  0, batch   370 | loss: 2.8250031MixupTrain:  epoch  0, batch   371 | loss: 2.8442268MixupTrain:  epoch  0, batch   372 | loss: 2.9265628MixupTrain:  epoch  0, batch   373 | loss: 2.9244237MixupTrain:  epoch  0, batch   374 | loss: 2.9781582MixupTrain:  epoch  0, batch   375 | loss: 2.9977531MixupTrain:  epoch  0, batch   376 | loss: 2.8976433MixupTrain:  epoch  0, batch   377 | loss: 2.9715729MixupTrain:  epoch  0, batch   378 | loss: 2.9460073MixupTrain:  epoch  0, batch   379 | loss: 2.9986284MixupTrain:  epoch  0, batch   380 | loss: 2.9959526MixupTrain:  epoch  0, batch   381 | loss: 2.7372699MixupTrain:  epoch  0, batch   382 | loss: 2.8521409MixupTrain:  epoch  0, batch   383 | loss: 2.9274807MixupTrain:  epoch  0, batch   384 | loss: 2.9075398MixupTrain:  epoch  0, batch   385 | loss: 2.8533192MixupTrain:  epoch  0, batch   386 | loss: 3.0162954MixupTrain:  epoch  0, batch   387 | loss: 3.1166854MixupTrain:  epoch  0, batch   388 | loss: 2.9123816MixupTrain:  epoch  0, batch   389 | loss: 2.9603033MixupTrain:  epoch  0, batch   390 | loss: 2.9893444MixupTrain:  epoch  0, batch   391 | loss: 2.8860278MixupTrain:  epoch  0, batch   392 | loss: 2.9716847MixupTrain:  epoch  0, batch   393 | loss: 2.9943528MixupTrain:  epoch  0, batch   394 | loss: 3.0463154MixupTrain:  epoch  0, batch   395 | loss: 3.1424744MixupTrain:  epoch  0, batch   396 | loss: 2.7943039MixupTrain:  epoch  0, batch   397 | loss: 2.9531131MixupTrain:  epoch  0, batch   398 | loss: 2.9925623MixupTrain:  epoch  0, batch   399 | loss: 3.0632567MixupTrain:  epoch  0, batch   400 | loss: 3.0293403MixupTrain:  epoch  0, batch   401 | loss: 3.0475638MixupTrain:  epoch  0, batch   402 | loss: 2.9865551MixupTrain:  epoch  0, batch   403 | loss: 3.0257006MixupTrain:  epoch  0, batch   404 | loss: 3.0320852MixupTrain:  epoch  0, batch   405 | loss: 2.9771099MixupTrain:  epoch  0, batch   406 | loss: 3.0265603MixupTrain:  epoch  0, batch   407 | loss: 2.9221225MixupTrain:  epoch  0, batch   408 | loss: 2.9064083MixupTrain:  epoch  0, batch   409 | loss: 2.9059584MixupTrain:  epoch  0, batch   410 | loss: 2.8793766MixupTrain:  epoch  0, batch   411 | loss: 2.9535556MixupTrain:  epoch  0, batch   412 | loss: 2.9900670MixupTrain:  epoch  0, batch   413 | loss: 2.9945714MixupTrain:  epoch  0, batch   414 | loss: 3.0276985MixupTrain:  epoch  0, batch   415 | loss: 3.0265746MixupTrain:  epoch  0, batch   416 | loss: 2.7550726MixupTrain:  epoch  0, batch   417 | loss: 3.0007548MixupTrain:  epoch  0, batch   418 | loss: 3.0166855MixupTrain:  epoch  0, batch   419 | loss: 2.9882345MixupTrain:  epoch  0, batch   420 | loss: 2.8079085MixupTrain:  epoch  0, batch   421 | loss: 3.0451684MixupTrain:  epoch  0, batch   422 | loss: 2.7418556MixupTrain:  epoch  0, batch   423 | loss: 2.9506042MixupTrain:  epoch  0, batch   424 | loss: 2.9869423MixupTrain:  epoch  0, batch   425 | loss: 2.9193532MixupTrain:  epoch  0, batch   426 | loss: 2.9664226MixupTrain:  epoch  0, batch   427 | loss: 3.1501451MixupTrain:  epoch  0, batch   428 | loss: 2.8368850MixupTrain:  epoch  0, batch   429 | loss: 2.8283153MixupTrain:  epoch  0, batch   430 | loss: 2.8784847MixupTrain:  epoch  0, batch   431 | loss: 2.9451866MixupTrain:  epoch  0, batch   432 | loss: 2.8703341MixupTrain:  epoch  0, batch   433 | loss: 2.7177560MixupTrain:  epoch  0, batch   434 | loss: 3.1314464MixupTrain:  epoch  0, batch   435 | loss: 2.8967905MixupTrain:  epoch  0, batch   436 | loss: 2.9983149MixupTrain:  epoch  0, batch   437 | loss: 2.9718378MixupTrain:  epoch  0, batch   438 | loss: 2.8896794MixupTrain:  epoch  0, batch   439 | loss: 2.8333316MixupTrain:  epoch  0, batch   440 | loss: 2.8540411MixupTrain:  epoch  0, batch   441 | loss: 3.0432270MixupTrain:  epoch  0, batch   442 | loss: 3.0260372MixupTrain:  epoch  0, batch   443 | loss: 2.9558642MixupTrain:  epoch  0, batch   444 | loss: 2.8825788MixupTrain:  epoch  0, batch   445 | loss: 2.8989537MixupTrain:  epoch  0, batch   446 | loss: 2.9567912MixupTrain:  epoch  0, batch   447 | loss: 3.0163713MixupTrain:  epoch  0, batch   448 | loss: 3.0632820MixupTrain:  epoch  0, batch   449 | loss: 2.8150601MixupTrain:  epoch  0, batch   450 | loss: 3.0761988MixupTrain:  epoch  0, batch   451 | loss: 2.9458947MixupTrain:  epoch  0, batch   452 | loss: 3.1886854MixupTrain:  epoch  0, batch   453 | loss: 2.9707932MixupTrain:  epoch  0, batch   454 | loss: 2.8110690MixupTrain:  epoch  0, batch   455 | loss: 3.0930698MixupTrain:  epoch  0, batch   456 | loss: 2.9898589MixupTrain:  epoch  0, batch   457 | loss: 2.8120408MixupTrain:  epoch  0, batch   458 | loss: 2.9661627MixupTrain:  epoch  0, batch   459 | loss: 3.0603213MixupTrain:  epoch  0, batch   460 | loss: 2.8674111MixupTrain:  epoch  0, batch   461 | loss: 3.1793199MixupTrain:  epoch  0, batch   462 | loss: 3.0582104MixupTrain:  epoch  0, batch   463 | loss: 2.7458675MixupTrain:  epoch  0, batch   464 | loss: 2.9187450MixupTrain:  epoch  0, batch   465 | loss: 2.9710364MixupTrain:  epoch  0, batch   466 | loss: 2.9083810MixupTrain:  epoch  0, batch   467 | loss: 2.9398751MixupTrain:  epoch  0, batch   468 | loss: 2.9968915MixupTrain:  epoch  0, batch   469 | loss: 2.9661438MixupTrain:  epoch  0, batch   470 | loss: 2.9636531MixupTrain:  epoch  0, batch   471 | loss: 2.9674344MixupTrain:  epoch  0, batch   472 | loss: 2.8319623MixupTrain:  epoch  0, batch   473 | loss: 2.9875777MixupTrain:  epoch  0, batch   474 | loss: 2.9157705MixupTrain:  epoch  0, batch   475 | loss: 3.0473976MixupTrain:  epoch  0, batch   476 | loss: 2.8021452MixupTrain:  epoch  0, batch   477 | loss: 2.8676867MixupTrain:  epoch  0, batch   478 | loss: 2.9527926MixupTrain:  epoch  0, batch   479 | loss: 3.0529561MixupTrain:  epoch  0, batch   480 | loss: 2.9615264MixupTrain:  epoch  0, batch   481 | loss: 2.9972661MixupTrain:  epoch  0, batch   482 | loss: 2.9002934MixupTrain:  epoch  0, batch   483 | loss: 2.9999971MixupTrain:  epoch  0, batch   484 | loss: 2.9768114MixupTrain:  epoch  0, batch   485 | loss: 3.0295990MixupTrain:  epoch  0, batch   486 | loss: 3.0500073MixupTrain:  epoch  0, batch   487 | loss: 2.9724555MixupTrain:  epoch  0, batch   488 | loss: 3.0465217MixupTrain:  epoch  0, batch   489 | loss: 3.0773826MixupTrain:  epoch  0, batch   490 | loss: 3.0209632MixupTrain:  epoch  0, batch   491 | loss: 2.9796524MixupTrain:  epoch  0, batch   492 | loss: 2.9072976MixupTrain:  epoch  0, batch   493 | loss: 2.9419947MixupTrain:  epoch  0, batch   494 | loss: 2.8825612MixupTrain:  epoch  0, batch   495 | loss: 2.9685397MixupTrain:  epoch  0, batch   496 | loss: 2.9295666MixupTrain:  epoch  0, batch   497 | loss: 3.1459651MixupTrain:  epoch  0, batch   498 | loss: 3.0418460MixupTrain:  epoch  0, batch   499 | loss: 2.8458495MixupTrain:  epoch  0, batch   500 | loss: 2.9211392MixupTrain:  epoch  0, batch   501 | loss: 2.8684154MixupTrain:  epoch  0, batch   502 | loss: 2.9659290MixupTrain:  epoch  0, batch   503 | loss: 2.9181795MixupTrain:  epoch  0, batch   504 | loss: 2.9458179MixupTrain:  epoch  0, batch   505 | loss: 2.9896698MixupTrain:  epoch  0, batch   506 | loss: 3.0090494MixupTrain:  epoch  0, batch   507 | loss: 2.8789804MixupTrain:  epoch  0, batch   508 | loss: 3.0410118MixupTrain:  epoch  0, batch   509 | loss: 2.9093347MixupTrain:  epoch  0, batch   510 | loss: 2.9077954MixupTrain:  epoch  0, batch   511 | loss: 2.9318717MixupTrain:  epoch  0, batch   512 | loss: 2.8674207MixupTrain:  epoch  0, batch   513 | loss: 2.9462314MixupTrain:  epoch  0, batch   514 | loss: 3.0990515MixupTrain:  epoch  0, batch   515 | loss: 2.8800187MixupTrain:  epoch  0, batch   516 | loss: 2.9985204MixupTrain:  epoch  0, batch   517 | loss: 3.0043058MixupTrain:  epoch  0, batch   518 | loss: 2.8686914MixupTrain:  epoch  0, batch   519 | loss: 2.9645376MixupTrain:  epoch  0, batch   520 | loss: 2.9654298MixupTrain:  epoch  0, batch   521 | loss: 3.0456378MixupTrain:  epoch  0, batch   522 | loss: 2.9030304MixupTrain:  epoch  0, batch   523 | loss: 3.0398800MixupTrain:  epoch  0, batch   524 | loss: 2.9469934MixupTrain:  epoch  0, batch   525 | loss: 2.9043612MixupTrain:  epoch  0, batch   526 | loss: 2.9534907MixupTrain:  epoch  0, batch   527 | loss: 3.0407312MixupTrain:  epoch  0, batch   528 | loss: 3.0159216MixupTrain:  epoch  0, batch   529 | loss: 2.9765296MixupTrain:  epoch  0, batch   530 | loss: 3.0290833MixupTrain:  epoch  0, batch   531 | loss: 2.9139266MixupTrain:  epoch  0, batch   532 | loss: 3.0019712MixupTrain:  epoch  0, batch   533 | loss: 2.9812021MixupTrain:  epoch  0, batch   534 | loss: 3.0893545MixupTrain:  epoch  0, batch   535 | loss: 2.8821576MixupTrain:  epoch  0, batch   536 | loss: 2.9421194MixupTrain:  epoch  0, batch   537 | loss: 2.8576651MixupTrain:  epoch  0, batch   538 | loss: 2.8920288MixupTrain:  epoch  0, batch   539 | loss: 2.9897110MixupTrain:  epoch  0, batch   540 | loss: 3.1053295MixupTrain:  epoch  0, batch   541 | loss: 2.9452744MixupTrain:  epoch  0, batch   542 | loss: 2.8393450MixupTrain:  epoch  0, batch   543 | loss: 2.8689017MixupTrain:  epoch  0, batch   544 | loss: 2.8701253MixupTrain:  epoch  0, batch   545 | loss: 3.1065397MixupTrain:  epoch  0, batch   546 | loss: 3.0831180MixupTrain:  epoch  0, batch   547 | loss: 2.8657341MixupTrain:  epoch  0, batch   548 | loss: 3.0832903MixupTrain:  epoch  0, batch   549 | loss: 2.9216509MixupTrain:  epoch  0, batch   550 | loss: 2.9675207MixupTrain:  epoch  0, batch   551 | loss: 2.8667421MixupTrain:  epoch  0, batch   552 | loss: 2.9156761MixupTrain:  epoch  0, batch   553 | loss: 2.9744473MixupTrain:  epoch  0, batch   554 | loss: 2.9724071MixupTrain:  epoch  0, batch   555 | loss: 2.9551640MixupTrain:  epoch  0, batch   556 | loss: 3.0525932MixupTrain:  epoch  0, batch   557 | loss: 2.9169815MixupTrain:  epoch  0, batch   558 | loss: 2.9769368MixupTrain:  epoch  0, batch   559 | loss: 2.9092538MixupTrain:  epoch  0, batch   560 | loss: 3.0134017MixupTrain:  epoch  0, batch   561 | loss: 2.8941691MixupTrain:  epoch  0, batch   562 | loss: 3.1661012MixupTrain:  epoch  0, batch   563 | loss: 3.1649284MixupTrain:  epoch  0, batch   564 | loss: 2.8718262MixupTrain:  epoch  0, batch   565 | loss: 3.0085452MixupTrain:  epoch  0, batch   566 | loss: 3.1369829MixupTrain:  epoch  0, batch   567 | loss: 2.9639125MixupTrain:  epoch  0, batch   568 | loss: 3.1112342MixupTrain:  epoch  0, batch   569 | loss: 2.8578470MixupTrain:  epoch  0, batch   570 | loss: 3.0232384MixupTrain:  epoch  0, batch   571 | loss: 2.8924179MixupTrain:  epoch  0, batch   572 | loss: 3.1308849MixupTrain:  epoch  0, batch   573 | loss: 2.9879880MixupTrain:  epoch  0, batch   574 | loss: 3.0035067MixupTrain:  epoch  0, batch   575 | loss: 2.9376945MixupTrain:  epoch  0, batch   576 | loss: 3.0014629MixupTrain:  epoch  0, batch   577 | loss: 2.8416195MixupTrain:  epoch  0, batch   578 | loss: 2.9730425MixupTrain:  epoch  0, batch   579 | loss: 2.7857499MixupTrain:  epoch  0, batch   580 | loss: 2.8261685MixupTrain:  epoch  0, batch   581 | loss: 2.8742118MixupTrain:  epoch  0, batch   582 | loss: 2.9563651MixupTrain:  epoch  0, batch   583 | loss: 2.9267387MixupTrain:  epoch  0, batch   584 | loss: 2.9052627MixupTrain:  epoch  0, batch   585 | loss: 2.9108458MixupTrain:  epoch  0, batch   586 | loss: 2.9465504MixupTrain:  epoch  0, batch   587 | loss: 2.8698986MixupTrain:  epoch  0, batch   588 | loss: 2.8762803MixupTrain:  epoch  0, batch   589 | loss: 2.8610382MixupTrain:  epoch  0, batch   590 | loss: 2.9096942MixupTrain:  epoch  0, batch   591 | loss: 3.0318742MixupTrain:  epoch  0, batch   592 | loss: 3.0365682MixupTrain:  epoch  0, batch   593 | loss: 2.9913073MixupTrain:  epoch  0, batch   594 | loss: 2.9304311MixupTrain:  epoch  0, batch   595 | loss: 2.9456463MixupTrain:  epoch  0, batch   596 | loss: 3.0341473MixupTrain:  epoch  0, batch   597 | loss: 2.9399111MixupTrain:  epoch  0, batch   598 | loss: 3.0710950MixupTrain:  epoch  0, batch   599 | loss: 3.0672579MixupTrain:  epoch  0, batch   600 | loss: 3.1367469MixupTrain:  epoch  0, batch   601 | loss: 2.9074578MixupTrain:  epoch  0, batch   602 | loss: 3.1406937MixupTrain:  epoch  0, batch   603 | loss: 2.8128965MixupTrain:  epoch  0, batch   604 | loss: 2.9203432MixupTrain:  epoch  0, batch   605 | loss: 2.9914882MixupTrain:  epoch  0, batch   606 | loss: 2.8703871MixupTrain:  epoch  0, batch   607 | loss: 2.9607000MixupTrain:  epoch  0, batch   608 | loss: 2.9632511MixupTrain:  epoch  0, batch   609 | loss: 2.9236231MixupTrain:  epoch  0, batch   610 | loss: 3.0013790MixupTrain:  epoch  0, batch   611 | loss: 2.8892832MixupTrain:  epoch  0, batch   612 | loss: 2.8737254MixupTrain:  epoch  0, batch   613 | loss: 2.8846264MixupTrain:  epoch  0, batch   614 | loss: 2.9368179MixupTrain:  epoch  0, batch   615 | loss: 3.0085068MixupTrain:  epoch  0, batch   616 | loss: 2.9216621MixupTrain:  epoch  0, batch   617 | loss: 2.9618270MixupTrain:  epoch  0, batch   618 | loss: 2.8986053MixupTrain:  epoch  0, batch   619 | loss: 2.9567604MixupTrain:  epoch  0, batch   620 | loss: 2.9783025MixupTrain:  epoch  0, batch   621 | loss: 3.0227332MixupTrain:  epoch  0, batch   622 | loss: 2.9683797MixupTrain:  epoch  0, batch   623 | loss: 3.0178022MixupTrain:  epoch  0, batch   624 | loss: 3.0402999MixupTrain:  epoch  0, batch   625 | loss: 3.0169342MixupTrain:  epoch  0, batch   626 | loss: 2.9883213MixupTrain:  epoch  0, batch   627 | loss: 2.8775930MixupTrain:  epoch  0, batch   628 | loss: 2.8796165MixupTrain:  epoch  0, batch   629 | loss: 2.9566407MixupTrain:  epoch  0, batch   630 | loss: 3.0742831MixupTrain:  epoch  0, batch   631 | loss: 2.7862101MixupTrain:  epoch  0, batch   632 | loss: 2.8010526MixupTrain:  epoch  0, batch   633 | loss: 2.9328623MixupTrain:  epoch  0, batch   634 | loss: 2.9259686MixupTrain:  epoch  0, batch   635 | loss: 3.0366781MixupTrain:  epoch  0, batch   636 | loss: 2.8937016MixupTrain:  epoch  0, batch   637 | loss: 3.1717093MixupTrain:  epoch  0, batch   638 | loss: 3.0318680MixupTrain:  epoch  0, batch   639 | loss: 2.9927421MixupTrain:  epoch  0, batch   640 | loss: 2.8468468MixupTrain:  epoch  0, batch   641 | loss: 3.1382246MixupTrain:  epoch  0, batch   642 | loss: 2.9386768MixupTrain:  epoch  0, batch   643 | loss: 2.9071884MixupTrain:  epoch  0, batch   644 | loss: 2.9821208MixupTrain:  epoch  0, batch   645 | loss: 2.9370141MixupTrain:  epoch  0, batch   646 | loss: 3.0465813MixupTrain:  epoch  0, batch   647 | loss: 3.0113056MixupTrain:  epoch  0, batch   648 | loss: 2.8490043MixupTrain:  epoch  0, batch   649 | loss: 3.0042551MixupTrain:  epoch  0, batch   650 | loss: 3.1759753MixupTrain:  epoch  0, batch   651 | loss: 3.0002749MixupTrain:  epoch  0, batch   652 | loss: 3.1248000MixupTrain:  epoch  0, batch   653 | loss: 2.9419065MixupTrain:  epoch  0, batch   654 | loss: 3.0114396MixupTrain:  epoch  0, batch   655 | loss: 2.9228673MixupTrain:  epoch  0, batch   656 | loss: 2.8044496MixupTrain:  epoch  0, batch   657 | loss: 3.1197617MixupTrain:  epoch  0, batch   658 | loss: 2.9666741MixupTrain:  epoch  0, batch   659 | loss: 2.8736887MixupTrain:  epoch  0, batch   660 | loss: 3.0082641MixupTrain:  epoch  0, batch   661 | loss: 2.8638401MixupTrain:  epoch  0, batch   662 | loss: 3.0238466MixupTrain:  epoch  0, batch   663 | loss: 2.9329429MixupTrain:  epoch  0, batch   664 | loss: 2.7772787
MemoryTrain:  epoch  0, batch     0 | loss: 1.2683719MemoryTrain:  epoch  0, batch     1 | loss: 1.5210004MemoryTrain:  epoch  0, batch     2 | loss: 1.3913852MemoryTrain:  epoch  0, batch     3 | loss: 1.4200387MemoryTrain:  epoch  0, batch     4 | loss: 1.5259060MemoryTrain:  epoch  0, batch     5 | loss: 1.5940878MemoryTrain:  epoch  0, batch     6 | loss: 1.5750287MemoryTrain:  epoch  0, batch     7 | loss: 1.9100207MemoryTrain:  epoch  1, batch     0 | loss: 1.2504942MemoryTrain:  epoch  1, batch     1 | loss: 1.5093161MemoryTrain:  epoch  1, batch     2 | loss: 1.2982543MemoryTrain:  epoch  1, batch     3 | loss: 1.2273757MemoryTrain:  epoch  1, batch     4 | loss: 1.2795467MemoryTrain:  epoch  1, batch     5 | loss: 1.3491894MemoryTrain:  epoch  1, batch     6 | loss: 1.4047182MemoryTrain:  epoch  1, batch     7 | loss: 1.4130877MemoryTrain:  epoch  2, batch     0 | loss: 1.2584569MemoryTrain:  epoch  2, batch     1 | loss: 1.3041484MemoryTrain:  epoch  2, batch     2 | loss: 1.2550435MemoryTrain:  epoch  2, batch     3 | loss: 1.4396377MemoryTrain:  epoch  2, batch     4 | loss: 1.1909789MemoryTrain:  epoch  2, batch     5 | loss: 1.2281961MemoryTrain:  epoch  2, batch     6 | loss: 1.2999527MemoryTrain:  epoch  2, batch     7 | loss: 1.2455378MemoryTrain:  epoch  3, batch     0 | loss: 1.2232710MemoryTrain:  epoch  3, batch     1 | loss: 1.2497299MemoryTrain:  epoch  3, batch     2 | loss: 1.1956346MemoryTrain:  epoch  3, batch     3 | loss: 1.2415146MemoryTrain:  epoch  3, batch     4 | loss: 1.1896211MemoryTrain:  epoch  3, batch     5 | loss: 1.2444212MemoryTrain:  epoch  3, batch     6 | loss: 1.2026381MemoryTrain:  epoch  3, batch     7 | loss: 1.2908266MemoryTrain:  epoch  4, batch     0 | loss: 1.1767712MemoryTrain:  epoch  4, batch     1 | loss: 1.2179825MemoryTrain:  epoch  4, batch     2 | loss: 1.2349617MemoryTrain:  epoch  4, batch     3 | loss: 1.1631597MemoryTrain:  epoch  4, batch     4 | loss: 1.2128048MemoryTrain:  epoch  4, batch     5 | loss: 1.2135247MemoryTrain:  epoch  4, batch     6 | loss: 1.1815212MemoryTrain:  epoch  4, batch     7 | loss: 1.1825206MemoryTrain:  epoch  5, batch     0 | loss: 1.2105505MemoryTrain:  epoch  5, batch     1 | loss: 1.2148038MemoryTrain:  epoch  5, batch     2 | loss: 1.1769723MemoryTrain:  epoch  5, batch     3 | loss: 1.1701343MemoryTrain:  epoch  5, batch     4 | loss: 1.1983359MemoryTrain:  epoch  5, batch     5 | loss: 1.1503347MemoryTrain:  epoch  5, batch     6 | loss: 1.2101347MemoryTrain:  epoch  5, batch     7 | loss: 1.1576842MemoryTrain:  epoch  6, batch     0 | loss: 1.1658710MemoryTrain:  epoch  6, batch     1 | loss: 1.2046192MemoryTrain:  epoch  6, batch     2 | loss: 1.2072351MemoryTrain:  epoch  6, batch     3 | loss: 1.1928229MemoryTrain:  epoch  6, batch     4 | loss: 1.1803856MemoryTrain:  epoch  6, batch     5 | loss: 1.1711464MemoryTrain:  epoch  6, batch     6 | loss: 1.1915007MemoryTrain:  epoch  6, batch     7 | loss: 1.2035948MemoryTrain:  epoch  7, batch     0 | loss: 1.2036786MemoryTrain:  epoch  7, batch     1 | loss: 1.1776221MemoryTrain:  epoch  7, batch     2 | loss: 1.2143254MemoryTrain:  epoch  7, batch     3 | loss: 1.1898437MemoryTrain:  epoch  7, batch     4 | loss: 1.1603243MemoryTrain:  epoch  7, batch     5 | loss: 1.2205770MemoryTrain:  epoch  7, batch     6 | loss: 1.2058645MemoryTrain:  epoch  7, batch     7 | loss: 1.1807700MemoryTrain:  epoch  8, batch     0 | loss: 1.1584988MemoryTrain:  epoch  8, batch     1 | loss: 1.1770813MemoryTrain:  epoch  8, batch     2 | loss: 1.1690766MemoryTrain:  epoch  8, batch     3 | loss: 1.1977384MemoryTrain:  epoch  8, batch     4 | loss: 1.1991111MemoryTrain:  epoch  8, batch     5 | loss: 1.1741085MemoryTrain:  epoch  8, batch     6 | loss: 1.1796219MemoryTrain:  epoch  8, batch     7 | loss: 1.1439059MemoryTrain:  epoch  9, batch     0 | loss: 1.1471515MemoryTrain:  epoch  9, batch     1 | loss: 1.3008535MemoryTrain:  epoch  9, batch     2 | loss: 1.1655120MemoryTrain:  epoch  9, batch     3 | loss: 1.1517105MemoryTrain:  epoch  9, batch     4 | loss: 1.1888635MemoryTrain:  epoch  9, batch     5 | loss: 1.1814895MemoryTrain:  epoch  9, batch     6 | loss: 1.1506875MemoryTrain:  epoch  9, batch     7 | loss: 1.1943536
[EVAL] batch:    0 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 96.88%   [EVAL] batch:    2 | acc: 100.00%,  total acc: 97.92%   [EVAL] batch:    3 | acc: 100.00%,  total acc: 98.44%   [EVAL] batch:    4 | acc: 100.00%,  total acc: 98.75%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 98.96%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 99.11%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 99.22%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 97.92%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 94.38%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 94.32%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 94.27%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 93.27%   [EVAL] batch:   13 | acc: 56.25%,  total acc: 90.62%   
[EVAL] batch:    0 | acc: 12.50%,  total acc: 12.50%   [EVAL] batch:    1 | acc: 18.75%,  total acc: 15.62%   [EVAL] batch:    2 | acc: 12.50%,  total acc: 14.58%   [EVAL] batch:    3 | acc: 6.25%,  total acc: 12.50%   [EVAL] batch:    4 | acc: 12.50%,  total acc: 12.50%   [EVAL] batch:    5 | acc: 18.75%,  total acc: 13.54%   [EVAL] batch:    6 | acc: 50.00%,  total acc: 18.75%   [EVAL] batch:    7 | acc: 50.00%,  total acc: 22.66%   [EVAL] batch:    8 | acc: 43.75%,  total acc: 25.00%   [EVAL] batch:    9 | acc: 31.25%,  total acc: 25.62%   [EVAL] batch:   10 | acc: 43.75%,  total acc: 27.27%   [EVAL] batch:   11 | acc: 68.75%,  total acc: 30.73%   [EVAL] batch:   12 | acc: 31.25%,  total acc: 30.77%   [EVAL] batch:   13 | acc: 31.25%,  total acc: 30.80%   [EVAL] batch:   14 | acc: 56.25%,  total acc: 32.50%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 33.98%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 36.40%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 37.85%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 39.14%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 41.25%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 43.75%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 46.02%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 48.10%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 50.00%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 52.00%   [EVAL] batch:   25 | acc: 93.75%,  total acc: 53.61%   [EVAL] batch:   26 | acc: 75.00%,  total acc: 54.40%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 55.80%   [EVAL] batch:   28 | acc: 87.50%,  total acc: 56.90%   [EVAL] batch:   29 | acc: 68.75%,  total acc: 57.29%   [EVAL] batch:   30 | acc: 68.75%,  total acc: 57.66%   [EVAL] batch:   31 | acc: 81.25%,  total acc: 58.40%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 57.77%   [EVAL] batch:   33 | acc: 0.00%,  total acc: 56.07%   [EVAL] batch:   34 | acc: 0.00%,  total acc: 54.46%   [EVAL] batch:   35 | acc: 0.00%,  total acc: 52.95%   [EVAL] batch:   36 | acc: 0.00%,  total acc: 51.52%   [EVAL] batch:   37 | acc: 56.25%,  total acc: 51.64%   [EVAL] batch:   38 | acc: 75.00%,  total acc: 52.24%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 53.44%   [EVAL] batch:   40 | acc: 81.25%,  total acc: 54.12%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 55.06%   [EVAL] batch:   42 | acc: 0.00%,  total acc: 53.78%   [EVAL] batch:   43 | acc: 0.00%,  total acc: 52.56%   [EVAL] batch:   44 | acc: 0.00%,  total acc: 51.39%   [EVAL] batch:   45 | acc: 0.00%,  total acc: 50.27%   [EVAL] batch:   46 | acc: 18.75%,  total acc: 49.60%   [EVAL] batch:   47 | acc: 43.75%,  total acc: 49.48%   [EVAL] batch:   48 | acc: 12.50%,  total acc: 48.72%   [EVAL] batch:   49 | acc: 18.75%,  total acc: 48.12%   [EVAL] batch:   50 | acc: 0.00%,  total acc: 47.18%   [EVAL] batch:   51 | acc: 0.00%,  total acc: 46.27%   [EVAL] batch:   52 | acc: 0.00%,  total acc: 45.40%   [EVAL] batch:   53 | acc: 25.00%,  total acc: 45.02%   [EVAL] batch:   54 | acc: 68.75%,  total acc: 45.45%   [EVAL] batch:   55 | acc: 56.25%,  total acc: 45.65%   [EVAL] batch:   56 | acc: 43.75%,  total acc: 45.61%   [EVAL] batch:   57 | acc: 43.75%,  total acc: 45.58%   [EVAL] batch:   58 | acc: 62.50%,  total acc: 45.87%   [EVAL] batch:   59 | acc: 50.00%,  total acc: 45.94%   [EVAL] batch:   60 | acc: 12.50%,  total acc: 45.39%   [EVAL] batch:   61 | acc: 0.00%,  total acc: 44.66%   [EVAL] batch:   62 | acc: 18.75%,  total acc: 44.25%   [EVAL] batch:   63 | acc: 12.50%,  total acc: 43.75%   [EVAL] batch:   64 | acc: 0.00%,  total acc: 43.08%   [EVAL] batch:   65 | acc: 12.50%,  total acc: 42.61%   [EVAL] batch:   66 | acc: 18.75%,  total acc: 42.26%   [EVAL] batch:   67 | acc: 93.75%,  total acc: 43.01%   [EVAL] batch:   68 | acc: 75.00%,  total acc: 43.48%   [EVAL] batch:   69 | acc: 43.75%,  total acc: 43.48%   [EVAL] batch:   70 | acc: 62.50%,  total acc: 43.75%   [EVAL] batch:   71 | acc: 62.50%,  total acc: 44.01%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 44.78%   [EVAL] batch:   73 | acc: 100.00%,  total acc: 45.52%   [EVAL] batch:   74 | acc: 100.00%,  total acc: 46.25%   [EVAL] batch:   75 | acc: 100.00%,  total acc: 46.96%   [EVAL] batch:   76 | acc: 100.00%,  total acc: 47.65%   [EVAL] batch:   77 | acc: 75.00%,  total acc: 48.00%   [EVAL] batch:   78 | acc: 0.00%,  total acc: 47.39%   [EVAL] batch:   79 | acc: 0.00%,  total acc: 46.80%   [EVAL] batch:   80 | acc: 0.00%,  total acc: 46.22%   [EVAL] batch:   81 | acc: 6.25%,  total acc: 45.73%   [EVAL] batch:   82 | acc: 6.25%,  total acc: 45.26%   [EVAL] batch:   83 | acc: 0.00%,  total acc: 44.72%   [EVAL] batch:   84 | acc: 25.00%,  total acc: 44.49%   [EVAL] batch:   85 | acc: 43.75%,  total acc: 44.48%   [EVAL] batch:   86 | acc: 50.00%,  total acc: 44.54%   [EVAL] batch:   87 | acc: 37.50%,  total acc: 44.46%   [EVAL] batch:   88 | acc: 50.00%,  total acc: 44.52%   [EVAL] batch:   89 | acc: 56.25%,  total acc: 44.65%   [EVAL] batch:   90 | acc: 56.25%,  total acc: 44.78%   [EVAL] batch:   91 | acc: 68.75%,  total acc: 45.04%   [EVAL] batch:   92 | acc: 62.50%,  total acc: 45.23%   [EVAL] batch:   93 | acc: 68.75%,  total acc: 45.48%   [EVAL] batch:   94 | acc: 56.25%,  total acc: 45.59%   [EVAL] batch:   95 | acc: 56.25%,  total acc: 45.70%   [EVAL] batch:   96 | acc: 6.25%,  total acc: 45.30%   [EVAL] batch:   97 | acc: 12.50%,  total acc: 44.96%   [EVAL] batch:   98 | acc: 6.25%,  total acc: 44.57%   [EVAL] batch:   99 | acc: 68.75%,  total acc: 44.81%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 45.36%   [EVAL] batch:  101 | acc: 31.25%,  total acc: 45.22%   [EVAL] batch:  102 | acc: 43.75%,  total acc: 45.21%   [EVAL] batch:  103 | acc: 37.50%,  total acc: 45.13%   [EVAL] batch:  104 | acc: 43.75%,  total acc: 45.12%   [EVAL] batch:  105 | acc: 37.50%,  total acc: 45.05%   [EVAL] batch:  106 | acc: 50.00%,  total acc: 45.09%   [EVAL] batch:  107 | acc: 75.00%,  total acc: 45.37%   [EVAL] batch:  108 | acc: 75.00%,  total acc: 45.64%   [EVAL] batch:  109 | acc: 68.75%,  total acc: 45.85%   [EVAL] batch:  110 | acc: 93.75%,  total acc: 46.28%   [EVAL] batch:  111 | acc: 100.00%,  total acc: 46.76%   [EVAL] batch:  112 | acc: 100.00%,  total acc: 47.23%   [EVAL] batch:  113 | acc: 87.50%,  total acc: 47.59%   [EVAL] batch:  114 | acc: 37.50%,  total acc: 47.50%   [EVAL] batch:  115 | acc: 43.75%,  total acc: 47.47%   [EVAL] batch:  116 | acc: 68.75%,  total acc: 47.65%   [EVAL] batch:  117 | acc: 50.00%,  total acc: 47.67%   [EVAL] batch:  118 | acc: 56.25%,  total acc: 47.74%   [EVAL] batch:  119 | acc: 100.00%,  total acc: 48.18%   [EVAL] batch:  120 | acc: 93.75%,  total acc: 48.55%   [EVAL] batch:  121 | acc: 100.00%,  total acc: 48.98%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 49.39%   [EVAL] batch:  123 | acc: 100.00%,  total acc: 49.80%   [EVAL] batch:  124 | acc: 100.00%,  total acc: 50.20%   [EVAL] batch:  125 | acc: 100.00%,  total acc: 50.60%   [EVAL] batch:  126 | acc: 100.00%,  total acc: 50.98%   [EVAL] batch:  127 | acc: 81.25%,  total acc: 51.22%   [EVAL] batch:  128 | acc: 68.75%,  total acc: 51.36%   [EVAL] batch:  129 | acc: 93.75%,  total acc: 51.68%   [EVAL] batch:  130 | acc: 93.75%,  total acc: 52.00%   [EVAL] batch:  131 | acc: 81.25%,  total acc: 52.23%   [EVAL] batch:  132 | acc: 43.75%,  total acc: 52.16%   
cur_acc:  ['0.8580', '0.7083', '0.7232', '0.8299', '0.6591', '0.7344', '0.7596', '0.9062']
his_acc:  ['0.8580', '0.8285', '0.7500', '0.6723', '0.6275', '0.5362', '0.5231', '0.5216']
--------Round  2
seed:  300
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/test.pkl
Task_order: [7 2 3 1 5 6 0 4]
prepared data!
CurrentTrain: epoch  0, batch     0 | loss: 13.2087269CurrentTrain: epoch  0, batch     1 | loss: 13.0199137CurrentTrain: epoch  0, batch     2 | loss: 12.9507532CurrentTrain: epoch  0, batch     3 | loss: 12.9761171CurrentTrain: epoch  0, batch     4 | loss: 12.7495098CurrentTrain: epoch  0, batch     5 | loss: 12.6696224CurrentTrain: epoch  0, batch     6 | loss: 12.5769510CurrentTrain: epoch  0, batch     7 | loss: 12.3734856CurrentTrain: epoch  0, batch     8 | loss: 12.1756287CurrentTrain: epoch  0, batch     9 | loss: 12.1970425CurrentTrain: epoch  0, batch    10 | loss: 11.7653656CurrentTrain: epoch  0, batch    11 | loss: 11.9477577CurrentTrain: epoch  0, batch    12 | loss: 11.8912954CurrentTrain: epoch  0, batch    13 | loss: 11.6467667CurrentTrain: epoch  0, batch    14 | loss: 11.8122826CurrentTrain: epoch  0, batch    15 | loss: 11.4353065CurrentTrain: epoch  0, batch    16 | loss: 10.9647694CurrentTrain: epoch  0, batch    17 | loss: 11.1076756CurrentTrain: epoch  0, batch    18 | loss: 11.2894936CurrentTrain: epoch  0, batch    19 | loss: 11.0927696CurrentTrain: epoch  0, batch    20 | loss: 11.3817110CurrentTrain: epoch  0, batch    21 | loss: 10.7818146CurrentTrain: epoch  0, batch    22 | loss: 11.4332819CurrentTrain: epoch  0, batch    23 | loss: 10.9831028CurrentTrain: epoch  0, batch    24 | loss: 11.0063896CurrentTrain: epoch  0, batch    25 | loss: 11.1643925CurrentTrain: epoch  0, batch    26 | loss: 11.0537434CurrentTrain: epoch  0, batch    27 | loss: 11.3541059CurrentTrain: epoch  0, batch    28 | loss: 10.8199501CurrentTrain: epoch  0, batch    29 | loss: 10.6863194CurrentTrain: epoch  0, batch    30 | loss: 10.0413303CurrentTrain: epoch  0, batch    31 | loss: 10.6086655CurrentTrain: epoch  0, batch    32 | loss: 10.4057312CurrentTrain: epoch  0, batch    33 | loss: 10.5034266CurrentTrain: epoch  0, batch    34 | loss: 10.8645554CurrentTrain: epoch  0, batch    35 | loss: 10.2836971CurrentTrain: epoch  0, batch    36 | loss: 10.9565506CurrentTrain: epoch  0, batch    37 | loss: 9.8085461CurrentTrain: epoch  1, batch     0 | loss: 9.8185816CurrentTrain: epoch  1, batch     1 | loss: 9.8147831CurrentTrain: epoch  1, batch     2 | loss: 9.6312332CurrentTrain: epoch  1, batch     3 | loss: 9.8634815CurrentTrain: epoch  1, batch     4 | loss: 9.5187492CurrentTrain: epoch  1, batch     5 | loss: 9.6676426CurrentTrain: epoch  1, batch     6 | loss: 9.7539749CurrentTrain: epoch  1, batch     7 | loss: 9.6661797CurrentTrain: epoch  1, batch     8 | loss: 10.0596447CurrentTrain: epoch  1, batch     9 | loss: 9.0199070CurrentTrain: epoch  1, batch    10 | loss: 9.1203289CurrentTrain: epoch  1, batch    11 | loss: 9.6704865CurrentTrain: epoch  1, batch    12 | loss: 9.3800087CurrentTrain: epoch  1, batch    13 | loss: 9.2762136CurrentTrain: epoch  1, batch    14 | loss: 9.4696026CurrentTrain: epoch  1, batch    15 | loss: 9.5647688CurrentTrain: epoch  1, batch    16 | loss: 10.1065178CurrentTrain: epoch  1, batch    17 | loss: 9.1685410CurrentTrain: epoch  1, batch    18 | loss: 9.5913010CurrentTrain: epoch  1, batch    19 | loss: 8.9354973CurrentTrain: epoch  1, batch    20 | loss: 9.4953499CurrentTrain: epoch  1, batch    21 | loss: 9.2778196CurrentTrain: epoch  1, batch    22 | loss: 9.9682455CurrentTrain: epoch  1, batch    23 | loss: 8.7892504CurrentTrain: epoch  1, batch    24 | loss: 9.1099186CurrentTrain: epoch  1, batch    25 | loss: 9.3512154CurrentTrain: epoch  1, batch    26 | loss: 9.1993484CurrentTrain: epoch  1, batch    27 | loss: 8.9436426CurrentTrain: epoch  1, batch    28 | loss: 8.7912445CurrentTrain: epoch  1, batch    29 | loss: 9.3720875CurrentTrain: epoch  1, batch    30 | loss: 8.0983925CurrentTrain: epoch  1, batch    31 | loss: 8.8172836CurrentTrain: epoch  1, batch    32 | loss: 8.3275070CurrentTrain: epoch  1, batch    33 | loss: 9.5964775CurrentTrain: epoch  1, batch    34 | loss: 9.2709389CurrentTrain: epoch  1, batch    35 | loss: 8.2293615CurrentTrain: epoch  1, batch    36 | loss: 8.5118189CurrentTrain: epoch  1, batch    37 | loss: 7.9557390CurrentTrain: epoch  2, batch     0 | loss: 8.7053566CurrentTrain: epoch  2, batch     1 | loss: 8.8010311CurrentTrain: epoch  2, batch     2 | loss: 8.8548851CurrentTrain: epoch  2, batch     3 | loss: 8.4463291CurrentTrain: epoch  2, batch     4 | loss: 8.1630373CurrentTrain: epoch  2, batch     5 | loss: 8.1222191CurrentTrain: epoch  2, batch     6 | loss: 8.4969311CurrentTrain: epoch  2, batch     7 | loss: 8.4461756CurrentTrain: epoch  2, batch     8 | loss: 8.0245724CurrentTrain: epoch  2, batch     9 | loss: 8.1865635CurrentTrain: epoch  2, batch    10 | loss: 8.2028828CurrentTrain: epoch  2, batch    11 | loss: 8.1254892CurrentTrain: epoch  2, batch    12 | loss: 7.2692280CurrentTrain: epoch  2, batch    13 | loss: 8.9866390CurrentTrain: epoch  2, batch    14 | loss: 8.0113049CurrentTrain: epoch  2, batch    15 | loss: 8.8053017CurrentTrain: epoch  2, batch    16 | loss: 7.9154873CurrentTrain: epoch  2, batch    17 | loss: 8.4287491CurrentTrain: epoch  2, batch    18 | loss: 7.8917398CurrentTrain: epoch  2, batch    19 | loss: 7.2855177CurrentTrain: epoch  2, batch    20 | loss: 8.9045582CurrentTrain: epoch  2, batch    21 | loss: 7.2932310CurrentTrain: epoch  2, batch    22 | loss: 7.9779377CurrentTrain: epoch  2, batch    23 | loss: 7.3568144CurrentTrain: epoch  2, batch    24 | loss: 8.1652098CurrentTrain: epoch  2, batch    25 | loss: 8.0188408CurrentTrain: epoch  2, batch    26 | loss: 8.2196026CurrentTrain: epoch  2, batch    27 | loss: 8.1800299CurrentTrain: epoch  2, batch    28 | loss: 7.6191506CurrentTrain: epoch  2, batch    29 | loss: 7.7923894CurrentTrain: epoch  2, batch    30 | loss: 8.2442551CurrentTrain: epoch  2, batch    31 | loss: 8.4937057CurrentTrain: epoch  2, batch    32 | loss: 7.4409509CurrentTrain: epoch  2, batch    33 | loss: 7.5322537CurrentTrain: epoch  2, batch    34 | loss: 6.8028126CurrentTrain: epoch  2, batch    35 | loss: 7.5230656CurrentTrain: epoch  2, batch    36 | loss: 7.1648421CurrentTrain: epoch  2, batch    37 | loss: 7.2699237CurrentTrain: epoch  3, batch     0 | loss: 7.3906484CurrentTrain: epoch  3, batch     1 | loss: 7.5065727CurrentTrain: epoch  3, batch     2 | loss: 7.0198584CurrentTrain: epoch  3, batch     3 | loss: 6.8391638CurrentTrain: epoch  3, batch     4 | loss: 7.2671208CurrentTrain: epoch  3, batch     5 | loss: 7.2593451CurrentTrain: epoch  3, batch     6 | loss: 7.5184364CurrentTrain: epoch  3, batch     7 | loss: 7.4409895CurrentTrain: epoch  3, batch     8 | loss: 7.0618391CurrentTrain: epoch  3, batch     9 | loss: 7.4576893CurrentTrain: epoch  3, batch    10 | loss: 7.6951628CurrentTrain: epoch  3, batch    11 | loss: 7.1350307CurrentTrain: epoch  3, batch    12 | loss: 6.9143333CurrentTrain: epoch  3, batch    13 | loss: 7.0880671CurrentTrain: epoch  3, batch    14 | loss: 7.6411009CurrentTrain: epoch  3, batch    15 | loss: 7.1863065CurrentTrain: epoch  3, batch    16 | loss: 7.7457542CurrentTrain: epoch  3, batch    17 | loss: 7.7029123CurrentTrain: epoch  3, batch    18 | loss: 7.3374228CurrentTrain: epoch  3, batch    19 | loss: 7.5289326CurrentTrain: epoch  3, batch    20 | loss: 7.1878595CurrentTrain: epoch  3, batch    21 | loss: 6.8936243CurrentTrain: epoch  3, batch    22 | loss: 6.9770875CurrentTrain: epoch  3, batch    23 | loss: 7.1128721CurrentTrain: epoch  3, batch    24 | loss: 7.1199274CurrentTrain: epoch  3, batch    25 | loss: 7.1438313CurrentTrain: epoch  3, batch    26 | loss: 7.1360049CurrentTrain: epoch  3, batch    27 | loss: 7.0865440CurrentTrain: epoch  3, batch    28 | loss: 6.8647313CurrentTrain: epoch  3, batch    29 | loss: 6.6826029CurrentTrain: epoch  3, batch    30 | loss: 7.9656429CurrentTrain: epoch  3, batch    31 | loss: 6.5522575CurrentTrain: epoch  3, batch    32 | loss: 7.3124948CurrentTrain: epoch  3, batch    33 | loss: 8.0758648CurrentTrain: epoch  3, batch    34 | loss: 7.3459740CurrentTrain: epoch  3, batch    35 | loss: 7.5827231CurrentTrain: epoch  3, batch    36 | loss: 7.1043916CurrentTrain: epoch  3, batch    37 | loss: 6.0459509CurrentTrain: epoch  4, batch     0 | loss: 6.3185363CurrentTrain: epoch  4, batch     1 | loss: 6.9835091CurrentTrain: epoch  4, batch     2 | loss: 6.1622972CurrentTrain: epoch  4, batch     3 | loss: 6.7811193CurrentTrain: epoch  4, batch     4 | loss: 6.9425020CurrentTrain: epoch  4, batch     5 | loss: 6.7272038CurrentTrain: epoch  4, batch     6 | loss: 7.2155972CurrentTrain: epoch  4, batch     7 | loss: 6.7061043CurrentTrain: epoch  4, batch     8 | loss: 6.3631277CurrentTrain: epoch  4, batch     9 | loss: 6.2532196CurrentTrain: epoch  4, batch    10 | loss: 7.1032529CurrentTrain: epoch  4, batch    11 | loss: 6.5674658CurrentTrain: epoch  4, batch    12 | loss: 6.5408397CurrentTrain: epoch  4, batch    13 | loss: 6.4219708CurrentTrain: epoch  4, batch    14 | loss: 7.2576098CurrentTrain: epoch  4, batch    15 | loss: 7.2377000CurrentTrain: epoch  4, batch    16 | loss: 6.7303133CurrentTrain: epoch  4, batch    17 | loss: 6.4520226CurrentTrain: epoch  4, batch    18 | loss: 6.2032681CurrentTrain: epoch  4, batch    19 | loss: 7.8418236CurrentTrain: epoch  4, batch    20 | loss: 6.1300001CurrentTrain: epoch  4, batch    21 | loss: 7.0789666CurrentTrain: epoch  4, batch    22 | loss: 7.0667696CurrentTrain: epoch  4, batch    23 | loss: 6.5345945CurrentTrain: epoch  4, batch    24 | loss: 6.7781086CurrentTrain: epoch  4, batch    25 | loss: 6.8168764CurrentTrain: epoch  4, batch    26 | loss: 6.0638132CurrentTrain: epoch  4, batch    27 | loss: 7.7091932CurrentTrain: epoch  4, batch    28 | loss: 6.3812513CurrentTrain: epoch  4, batch    29 | loss: 5.8591337CurrentTrain: epoch  4, batch    30 | loss: 7.0597486CurrentTrain: epoch  4, batch    31 | loss: 6.0953522CurrentTrain: epoch  4, batch    32 | loss: 7.5002551CurrentTrain: epoch  4, batch    33 | loss: 7.7359066CurrentTrain: epoch  4, batch    34 | loss: 6.5935516CurrentTrain: epoch  4, batch    35 | loss: 6.0427046CurrentTrain: epoch  4, batch    36 | loss: 6.2027988CurrentTrain: epoch  4, batch    37 | loss: 6.6911030CurrentTrain: epoch  5, batch     0 | loss: 6.7439809CurrentTrain: epoch  5, batch     1 | loss: 6.7152395CurrentTrain: epoch  5, batch     2 | loss: 6.2848411CurrentTrain: epoch  5, batch     3 | loss: 7.0535135CurrentTrain: epoch  5, batch     4 | loss: 6.5710001CurrentTrain: epoch  5, batch     5 | loss: 6.0350399CurrentTrain: epoch  5, batch     6 | loss: 7.8211527CurrentTrain: epoch  5, batch     7 | loss: 6.7686205CurrentTrain: epoch  5, batch     8 | loss: 7.5818777CurrentTrain: epoch  5, batch     9 | loss: 6.8462939CurrentTrain: epoch  5, batch    10 | loss: 6.1610661CurrentTrain: epoch  5, batch    11 | loss: 6.3940387CurrentTrain: epoch  5, batch    12 | loss: 7.0571899CurrentTrain: epoch  5, batch    13 | loss: 5.8307428CurrentTrain: epoch  5, batch    14 | loss: 6.0162973CurrentTrain: epoch  5, batch    15 | loss: 6.1628122CurrentTrain: epoch  5, batch    16 | loss: 5.8750391CurrentTrain: epoch  5, batch    17 | loss: 6.6922064CurrentTrain: epoch  5, batch    18 | loss: 5.5378733CurrentTrain: epoch  5, batch    19 | loss: 6.2626376CurrentTrain: epoch  5, batch    20 | loss: 6.2003875CurrentTrain: epoch  5, batch    21 | loss: 6.3515501CurrentTrain: epoch  5, batch    22 | loss: 5.6333055CurrentTrain: epoch  5, batch    23 | loss: 6.3369565CurrentTrain: epoch  5, batch    24 | loss: 6.1197600CurrentTrain: epoch  5, batch    25 | loss: 6.0775461CurrentTrain: epoch  5, batch    26 | loss: 5.5950022CurrentTrain: epoch  5, batch    27 | loss: 6.7338409CurrentTrain: epoch  5, batch    28 | loss: 6.3486199CurrentTrain: epoch  5, batch    29 | loss: 5.7662468CurrentTrain: epoch  5, batch    30 | loss: 6.5373678CurrentTrain: epoch  5, batch    31 | loss: 6.0416017CurrentTrain: epoch  5, batch    32 | loss: 5.5506792CurrentTrain: epoch  5, batch    33 | loss: 6.4052229CurrentTrain: epoch  5, batch    34 | loss: 5.9848375CurrentTrain: epoch  5, batch    35 | loss: 6.5998545CurrentTrain: epoch  5, batch    36 | loss: 5.8248491CurrentTrain: epoch  5, batch    37 | loss: 5.8896990CurrentTrain: epoch  6, batch     0 | loss: 6.2299685CurrentTrain: epoch  6, batch     1 | loss: 6.7775826CurrentTrain: epoch  6, batch     2 | loss: 5.8970594CurrentTrain: epoch  6, batch     3 | loss: 6.2942076CurrentTrain: epoch  6, batch     4 | loss: 5.8459249CurrentTrain: epoch  6, batch     5 | loss: 5.5401521CurrentTrain: epoch  6, batch     6 | loss: 6.0893326CurrentTrain: epoch  6, batch     7 | loss: 6.0586872CurrentTrain: epoch  6, batch     8 | loss: 5.3553090CurrentTrain: epoch  6, batch     9 | loss: 5.9092321CurrentTrain: epoch  6, batch    10 | loss: 5.5965433CurrentTrain: epoch  6, batch    11 | loss: 5.9160490CurrentTrain: epoch  6, batch    12 | loss: 6.0284495CurrentTrain: epoch  6, batch    13 | loss: 5.9493947CurrentTrain: epoch  6, batch    14 | loss: 6.2402687CurrentTrain: epoch  6, batch    15 | loss: 5.5478745CurrentTrain: epoch  6, batch    16 | loss: 5.3964357CurrentTrain: epoch  6, batch    17 | loss: 5.4435048CurrentTrain: epoch  6, batch    18 | loss: 5.5948944CurrentTrain: epoch  6, batch    19 | loss: 6.2832623CurrentTrain: epoch  6, batch    20 | loss: 5.2958879CurrentTrain: epoch  6, batch    21 | loss: 5.4202542CurrentTrain: epoch  6, batch    22 | loss: 5.4804287CurrentTrain: epoch  6, batch    23 | loss: 6.2045622CurrentTrain: epoch  6, batch    24 | loss: 5.2531867CurrentTrain: epoch  6, batch    25 | loss: 6.2566509CurrentTrain: epoch  6, batch    26 | loss: 6.0783968CurrentTrain: epoch  6, batch    27 | loss: 5.5099497CurrentTrain: epoch  6, batch    28 | loss: 5.7548194CurrentTrain: epoch  6, batch    29 | loss: 5.5913587CurrentTrain: epoch  6, batch    30 | loss: 6.4806685CurrentTrain: epoch  6, batch    31 | loss: 6.4139104CurrentTrain: epoch  6, batch    32 | loss: 5.6625357CurrentTrain: epoch  6, batch    33 | loss: 5.9305649CurrentTrain: epoch  6, batch    34 | loss: 5.3749981CurrentTrain: epoch  6, batch    35 | loss: 5.6221008CurrentTrain: epoch  6, batch    36 | loss: 5.4905119CurrentTrain: epoch  6, batch    37 | loss: 5.2476726CurrentTrain: epoch  7, batch     0 | loss: 6.3646564CurrentTrain: epoch  7, batch     1 | loss: 5.4444380CurrentTrain: epoch  7, batch     2 | loss: 5.8164406CurrentTrain: epoch  7, batch     3 | loss: 5.3898706CurrentTrain: epoch  7, batch     4 | loss: 5.6883888CurrentTrain: epoch  7, batch     5 | loss: 5.3783941CurrentTrain: epoch  7, batch     6 | loss: 5.4361773CurrentTrain: epoch  7, batch     7 | loss: 5.6634436CurrentTrain: epoch  7, batch     8 | loss: 5.2012043CurrentTrain: epoch  7, batch     9 | loss: 5.5845456CurrentTrain: epoch  7, batch    10 | loss: 5.2617483CurrentTrain: epoch  7, batch    11 | loss: 5.6387072CurrentTrain: epoch  7, batch    12 | loss: 5.3742485CurrentTrain: epoch  7, batch    13 | loss: 5.2166958CurrentTrain: epoch  7, batch    14 | loss: 5.4109554CurrentTrain: epoch  7, batch    15 | loss: 5.5269022CurrentTrain: epoch  7, batch    16 | loss: 5.0750217CurrentTrain: epoch  7, batch    17 | loss: 5.4896774CurrentTrain: epoch  7, batch    18 | loss: 5.3176212CurrentTrain: epoch  7, batch    19 | loss: 5.4763999CurrentTrain: epoch  7, batch    20 | loss: 5.0982575CurrentTrain: epoch  7, batch    21 | loss: 5.2663565CurrentTrain: epoch  7, batch    22 | loss: 5.5836043CurrentTrain: epoch  7, batch    23 | loss: 5.3055763CurrentTrain: epoch  7, batch    24 | loss: 4.9684057CurrentTrain: epoch  7, batch    25 | loss: 5.2441607CurrentTrain: epoch  7, batch    26 | loss: 5.7259889CurrentTrain: epoch  7, batch    27 | loss: 5.1034317CurrentTrain: epoch  7, batch    28 | loss: 5.1184263CurrentTrain: epoch  7, batch    29 | loss: 5.4596052CurrentTrain: epoch  7, batch    30 | loss: 5.2180176CurrentTrain: epoch  7, batch    31 | loss: 5.4878325CurrentTrain: epoch  7, batch    32 | loss: 5.2589340CurrentTrain: epoch  7, batch    33 | loss: 5.7667403CurrentTrain: epoch  7, batch    34 | loss: 5.2362404CurrentTrain: epoch  7, batch    35 | loss: 5.0365944CurrentTrain: epoch  7, batch    36 | loss: 5.1385751CurrentTrain: epoch  7, batch    37 | loss: 5.5000725CurrentTrain: epoch  8, batch     0 | loss: 5.1004448CurrentTrain: epoch  8, batch     1 | loss: 5.0339775CurrentTrain: epoch  8, batch     2 | loss: 5.4610081CurrentTrain: epoch  8, batch     3 | loss: 5.2123232CurrentTrain: epoch  8, batch     4 | loss: 6.7545738CurrentTrain: epoch  8, batch     5 | loss: 5.2014389CurrentTrain: epoch  8, batch     6 | loss: 5.1533704CurrentTrain: epoch  8, batch     7 | loss: 5.4867959CurrentTrain: epoch  8, batch     8 | loss: 4.9965563CurrentTrain: epoch  8, batch     9 | loss: 5.1309757CurrentTrain: epoch  8, batch    10 | loss: 5.7033992CurrentTrain: epoch  8, batch    11 | loss: 5.1993170CurrentTrain: epoch  8, batch    12 | loss: 5.2849789CurrentTrain: epoch  8, batch    13 | loss: 5.3247051CurrentTrain: epoch  8, batch    14 | loss: 5.8556709CurrentTrain: epoch  8, batch    15 | loss: 5.4150391CurrentTrain: epoch  8, batch    16 | loss: 5.3953357CurrentTrain: epoch  8, batch    17 | loss: 5.2108974CurrentTrain: epoch  8, batch    18 | loss: 5.1859922CurrentTrain: epoch  8, batch    19 | loss: 5.1883640CurrentTrain: epoch  8, batch    20 | loss: 5.1928997CurrentTrain: epoch  8, batch    21 | loss: 5.2991409CurrentTrain: epoch  8, batch    22 | loss: 5.4565306CurrentTrain: epoch  8, batch    23 | loss: 5.8816538CurrentTrain: epoch  8, batch    24 | loss: 5.0117102CurrentTrain: epoch  8, batch    25 | loss: 5.2132063CurrentTrain: epoch  8, batch    26 | loss: 5.1491680CurrentTrain: epoch  8, batch    27 | loss: 5.2450914CurrentTrain: epoch  8, batch    28 | loss: 5.2344971CurrentTrain: epoch  8, batch    29 | loss: 5.6617932CurrentTrain: epoch  8, batch    30 | loss: 4.8912096CurrentTrain: epoch  8, batch    31 | loss: 5.5423136CurrentTrain: epoch  8, batch    32 | loss: 5.2055902CurrentTrain: epoch  8, batch    33 | loss: 5.0451822CurrentTrain: epoch  8, batch    34 | loss: 4.9207592CurrentTrain: epoch  8, batch    35 | loss: 5.2537947CurrentTrain: epoch  8, batch    36 | loss: 5.1521540CurrentTrain: epoch  8, batch    37 | loss: 5.3708439CurrentTrain: epoch  9, batch     0 | loss: 5.0651379CurrentTrain: epoch  9, batch     1 | loss: 5.2041245CurrentTrain: epoch  9, batch     2 | loss: 5.1486673CurrentTrain: epoch  9, batch     3 | loss: 5.3884363CurrentTrain: epoch  9, batch     4 | loss: 5.1734037CurrentTrain: epoch  9, batch     5 | loss: 5.2683287CurrentTrain: epoch  9, batch     6 | loss: 5.1564941CurrentTrain: epoch  9, batch     7 | loss: 4.9178915CurrentTrain: epoch  9, batch     8 | loss: 5.5452099CurrentTrain: epoch  9, batch     9 | loss: 5.4646358CurrentTrain: epoch  9, batch    10 | loss: 4.9728060CurrentTrain: epoch  9, batch    11 | loss: 5.2902603CurrentTrain: epoch  9, batch    12 | loss: 4.7671599CurrentTrain: epoch  9, batch    13 | loss: 5.0248809CurrentTrain: epoch  9, batch    14 | loss: 5.2817597CurrentTrain: epoch  9, batch    15 | loss: 5.1829705CurrentTrain: epoch  9, batch    16 | loss: 5.3796248CurrentTrain: epoch  9, batch    17 | loss: 5.3599305CurrentTrain: epoch  9, batch    18 | loss: 5.1962638CurrentTrain: epoch  9, batch    19 | loss: 4.9278779CurrentTrain: epoch  9, batch    20 | loss: 5.0773144CurrentTrain: epoch  9, batch    21 | loss: 4.9320908CurrentTrain: epoch  9, batch    22 | loss: 5.0288858CurrentTrain: epoch  9, batch    23 | loss: 5.1755042CurrentTrain: epoch  9, batch    24 | loss: 5.2058334CurrentTrain: epoch  9, batch    25 | loss: 5.0178351CurrentTrain: epoch  9, batch    26 | loss: 4.8972778CurrentTrain: epoch  9, batch    27 | loss: 5.3965902CurrentTrain: epoch  9, batch    28 | loss: 4.8876524CurrentTrain: epoch  9, batch    29 | loss: 4.9514642CurrentTrain: epoch  9, batch    30 | loss: 4.9825726CurrentTrain: epoch  9, batch    31 | loss: 4.8663821CurrentTrain: epoch  9, batch    32 | loss: 4.9631619CurrentTrain: epoch  9, batch    33 | loss: 5.3709507CurrentTrain: epoch  9, batch    34 | loss: 4.8301649CurrentTrain: epoch  9, batch    35 | loss: 5.0057335CurrentTrain: epoch  9, batch    36 | loss: 4.8597403CurrentTrain: epoch  9, batch    37 | loss: 4.8599663
#### person countries of residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person countries of residence" means a person is living in a country.
Here is an example:
Relation: person countries of residence
Context: germany 's biggest industrial union , ig metall , on monday nominated berthold huber , widely viewed as a modernizing figure , to take over as its new leader later this year .
Head Entity: berthold huber
Tail Entity: germany
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person countries of residence":
Relation: person countries of residence  
Context: after spending several years in the united states, the renowned artist decided to return to his roots in spain, where he felt a deep connection to the culture.  
Head Entity: the renowned artist  
Tail Entity: spain  

Relation: person countries of residence  
Context: following her successful career in the uk, the actress moved to australia to enjoy a more relaxed lifestyle and to be closer to her family.  
Head Entity: the actress  
Tail Entity: australia  
#### organization top members employees ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization top members employees" means an organization 's employer is a person.
Here is an example:
Relation: organization top members employees
Context: kao charng , vice chairman of the cabinet-level mainland affairs council -lrb- mac -rrb- , explained that the termination clause was added to give both sides an option after the implementation of the tariff-cutting economic cooperation framework agreement -lrb- ecfa -rrb- .
Head Entity: mainland affairs council
Tail Entity: kao charng
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization top members employees":
Relation: organization top members employees  
Context: elon musk, the CEO of spacex, announced the successful launch of the latest satellite into orbit, marking another milestone for the company.  
Head Entity: spacex  
Tail Entity: elon musk  

Relation: organization top members employees  
Context: sheryl sandberg, the chief operating officer of facebook, spoke at the conference about the importance of data privacy and user trust in the digital age.  
Head Entity: facebook  
Tail Entity: sheryl sandberg  
#### organization member of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization member of" means an organization is the member of an organization.
Here is an example:
Relation: organization member of
Context: south africa 1-5 leyds cnr biccard streets cosatu 's concern has been that the scorpions ' existence as a branch of the national directorate of public prosecutions blurred the necessary separation of functions between those investigating crime and those prosecuting the criminals .
Head Entity: scorpions
Tail Entity: national directorate of public prosecutions
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization member of":
Relation: organization member of  
Context: The International Federation of Red Cross and Red Crescent Societies (IFRC) is a global humanitarian organization that coordinates the activities of national Red Cross and Red Crescent societies around the world.  
Head Entity: International Federation of Red Cross and Red Crescent Societies  
Tail Entity: national Red Cross and Red Crescent societies  

Relation: organization member of  
Context: The European Union (EU) is a political and economic union of member states that are located primarily in Europe, working together to promote peace and stability in the region.  
Head Entity: European Union  
Tail Entity: member states  
#### person origin ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person origin" means a person 's nationality is a country.
Here is an example:
Relation: person origin
Context: receiving two yellow cards in the previous game against south korea , chinese skipper li jie were sidelined in this east asian top-level clash .
Head Entity: li jie
Tail Entity: chinese
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person origin":
Relation: person origin  
Context: The renowned physicist Albert Einstein was born in Germany before moving to Switzerland and later becoming a citizen of the United States.  
Head Entity: Albert Einstein  
Tail Entity: Germany  

Relation: person origin  
Context: The famous artist Frida Kahlo was born in Mexico and is celebrated for her unique contributions to Mexican culture and art.  
Head Entity: Frida Kahlo  
Tail Entity: Mexico  
#### person title ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person title" means a person 's title is a title.
Here is an example:
Relation: person title
Context: `` it has to have some tempo , it has to have some energy , '' said sean ross , vice president of music and programming at edison media research , which surveys the radio industry .
Head Entity: sean ross
Tail Entity: vice president
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person title":
Relation: person title  
Context: "After years of hard work and dedication, Maria Gonzalez was promoted to Chief Financial Officer at Tech Innovations, where she has been instrumental in driving the company's financial strategy."  
Head Entity: Maria Gonzalez  
Tail Entity: Chief Financial Officer  

Relation: person title  
Context: "During the annual conference, Dr. James Lee, the lead researcher at the National Institute of Health, presented groundbreaking findings in the field of genetics."  
Head Entity: Dr. James Lee  
Tail Entity: lead researcher  
#### organization country of headquarters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization country of headquarters" means an organization is located in a country.
Here is an example:
Relation: organization country of headquarters
Context: covidien had $ 1.7 billion in cash on its balance sheet at the end of march , and has told analysts it plans to spend about $ 750 million a year on acquisitions - though some believe that amount could increase if the company found the right candidate for a purchase , especially outside of the united states .
Head Entity: covidien
Tail Entity: united states
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization country of headquarters":
Relation: organization country of headquarters  
Context: Samsung Electronics, a leading technology company, has its headquarters in South Korea, where it continues to innovate and expand its global market presence.  
Head Entity: Samsung Electronics  
Tail Entity: South Korea  

Relation: organization country of headquarters  
Context: Nestlé, the world's largest food and beverage company, operates its main office in Switzerland, overseeing its numerous brands and products worldwide.  
Head Entity: Nestlé  
Tail Entity: Switzerland  
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 84.38%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 83.75%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 82.29%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 84.82%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 86.72%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 88.19%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 88.75%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 89.77%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 90.10%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 90.38%   [EVAL] batch:   13 | acc: 75.00%,  total acc: 89.29%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 88.33%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 86.33%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 85.66%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 84.72%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 83.88%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 83.44%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 84.23%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 84.94%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 85.60%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 86.20%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 86.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 87.26%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 87.95%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 88.36%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 88.33%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 88.51%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 88.67%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 87.12%   
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 84.38%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 83.75%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 82.29%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 84.82%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 86.72%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 88.19%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 88.75%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 89.77%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 90.10%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 90.38%   [EVAL] batch:   13 | acc: 75.00%,  total acc: 89.29%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 88.33%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 86.33%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 85.66%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 84.72%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 83.88%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 83.44%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 84.23%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 84.94%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 85.60%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 86.20%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 86.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 87.26%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 87.95%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 88.36%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 88.33%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 88.51%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 88.67%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 87.12%   
cur_acc:  ['0.8712']
his_acc:  ['0.8712']
CurrentTrain: epoch  0, batch     0 | loss: 6.8613453CurrentTrain: epoch  0, batch     1 | loss: 6.4257975CurrentTrain: epoch  1, batch     0 | loss: 6.0042400CurrentTrain: epoch  1, batch     1 | loss: 5.2039752CurrentTrain: epoch  2, batch     0 | loss: 5.2630882CurrentTrain: epoch  2, batch     1 | loss: 4.7064323CurrentTrain: epoch  3, batch     0 | loss: 4.3410349CurrentTrain: epoch  3, batch     1 | loss: 5.6018100CurrentTrain: epoch  4, batch     0 | loss: 4.4732785CurrentTrain: epoch  4, batch     1 | loss: 5.5644574CurrentTrain: epoch  5, batch     0 | loss: 4.2808990CurrentTrain: epoch  5, batch     1 | loss: 4.4947124CurrentTrain: epoch  6, batch     0 | loss: 3.9946146CurrentTrain: epoch  6, batch     1 | loss: 3.9249234CurrentTrain: epoch  7, batch     0 | loss: 3.2631283CurrentTrain: epoch  7, batch     1 | loss: 3.9332352CurrentTrain: epoch  8, batch     0 | loss: 3.2059627CurrentTrain: epoch  8, batch     1 | loss: 3.5168374CurrentTrain: epoch  9, batch     0 | loss: 3.2768483CurrentTrain: epoch  9, batch     1 | loss: 3.1085188
#### person cities of residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person cities of residence" means a person is living in a city.
Here is an example:
Relation: person cities of residence
Context: -lrb- now there are two : andre carson , a democrat from indianapolis , was elected to the house last year . -rrb-
Head Entity: andre carson
Tail Entity: indianapolis
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person cities of residence":
Relation: person cities of residence  
Context: -lrb- after moving to the west coast, sarah jones settled in san francisco, where she found a job in tech. -rrb-  
Head Entity: sarah jones  
Tail Entity: san francisco  

Relation: person cities of residence  
Context: -lrb- during his college years, michael smith lived in boston, enjoying the vibrant culture and history of the city. -rrb-  
Head Entity: michael smith  
Tail Entity: boston  
#### person schools attended ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person schools attended" means a person 's school is an organization.
Here is an example:
Relation: person schools attended
Context: after world war ii , he attended the university of southern california , where he became editor of a college magazine .
Head Entity: he
Tail Entity: university of southern california
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person schools attended":
Relation: person schools attended  
Context: She graduated from Harvard University with a degree in economics before pursuing her career in finance.  
Head Entity: She  
Tail Entity: Harvard University  

Relation: person schools attended  
Context: After completing his high school education, John enrolled at Stanford University to study computer science.  
Head Entity: John  
Tail Entity: Stanford University  
#### person country of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person country of death" means a person was died in a country.
Here is an example:
Relation: person country of death
Context: u.s. rep. parren mitchell , founding member of congressional black caucus , dies at 85
Head Entity: parren mitchell
Tail Entity: u.s.
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person country of death":
Relation: person country of death  
Context: renowned physicist stephen hawking passed away in cambridge, england at the age of 76  
Head Entity: stephen hawking  
Tail Entity: england  

Relation: person country of death  
Context: famous author gabriel garcia marquez died in mexico city, mexico, leaving behind a legacy of magical realism  
Head Entity: gabriel garcia marquez  
Tail Entity: mexico  
#### person children ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person children" means a person 's children is a person.
Here is an example:
Relation: person children
Context: he is survived by two stepdaughters , barbara a. hammond and brenda l. stevenson ; a stepson , michael a. taylor ; two grandchildren and one great-grandchild .
Head Entity: he
Tail Entity: brenda l. stevenson
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person children":
Relation: person children  
Context: she has three children, including her son, john, and her daughters, emily and sarah, who are both pursuing their studies.  
Head Entity: she  
Tail Entity: emily  

Relation: person children  
Context: after the passing of their father, the siblings, including jake and his sister, lila, took on the responsibility of caring for their younger brother, max.  
Head Entity: jake  
Tail Entity: lila  
#### person charges ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person charges" means a person was charged with an event.
Here is an example:
Relation: person charges
Context: previously , al-khawinay was sentenced to one year in jail for supporting the country 's minority shiite rebels and defaming the president , but was later pardoned by president ali abdullah saleh .
Head Entity: al-khawinay
Tail Entity: defaming the president
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person charges":
Relation: person charges  
Context: In a shocking turn of events, the local council announced that mayor Thompson was charged with embezzlement after an extensive investigation revealed misuse of public funds.  
Head Entity: mayor Thompson  
Tail Entity: embezzlement  

Relation: person charges  
Context: After a lengthy trial, it was determined that journalist Sarah Lee was charged with libel for publishing false information about a prominent businessman.  
Head Entity: journalist Sarah Lee  
Tail Entity: libel  
Mixup data size:  1495
MixupTrain:  epoch  0, batch     0 | loss: 7.0145960MixupTrain:  epoch  0, batch     1 | loss: 6.7311707MixupTrain:  epoch  0, batch     2 | loss: 6.5424271MixupTrain:  epoch  0, batch     3 | loss: 6.4128923MixupTrain:  epoch  0, batch     4 | loss: 5.9940453MixupTrain:  epoch  0, batch     5 | loss: 5.9102573MixupTrain:  epoch  0, batch     6 | loss: 5.8561926MixupTrain:  epoch  0, batch     7 | loss: 5.7041402MixupTrain:  epoch  0, batch     8 | loss: 5.6090088MixupTrain:  epoch  0, batch     9 | loss: 5.5411930MixupTrain:  epoch  0, batch    10 | loss: 5.5534849MixupTrain:  epoch  0, batch    11 | loss: 5.4659085MixupTrain:  epoch  0, batch    12 | loss: 5.4609709MixupTrain:  epoch  0, batch    13 | loss: 5.4255524MixupTrain:  epoch  0, batch    14 | loss: 5.4064798MixupTrain:  epoch  0, batch    15 | loss: 5.3669858MixupTrain:  epoch  0, batch    16 | loss: 5.3588734MixupTrain:  epoch  0, batch    17 | loss: 5.2720451MixupTrain:  epoch  0, batch    18 | loss: 5.2777610MixupTrain:  epoch  0, batch    19 | loss: 5.2825174MixupTrain:  epoch  0, batch    20 | loss: 5.1956797MixupTrain:  epoch  0, batch    21 | loss: 5.2146125MixupTrain:  epoch  0, batch    22 | loss: 5.1426101MixupTrain:  epoch  0, batch    23 | loss: 5.1608591MixupTrain:  epoch  0, batch    24 | loss: 5.1604047MixupTrain:  epoch  0, batch    25 | loss: 5.1296091MixupTrain:  epoch  0, batch    26 | loss: 5.0462303MixupTrain:  epoch  0, batch    27 | loss: 5.0996647MixupTrain:  epoch  0, batch    28 | loss: 5.0308571MixupTrain:  epoch  0, batch    29 | loss: 5.0261393MixupTrain:  epoch  0, batch    30 | loss: 4.9647417MixupTrain:  epoch  0, batch    31 | loss: 5.0000501MixupTrain:  epoch  0, batch    32 | loss: 4.9774709MixupTrain:  epoch  0, batch    33 | loss: 4.9420309MixupTrain:  epoch  0, batch    34 | loss: 4.8438644MixupTrain:  epoch  0, batch    35 | loss: 4.8527079MixupTrain:  epoch  0, batch    36 | loss: 4.8333588MixupTrain:  epoch  0, batch    37 | loss: 4.7688503MixupTrain:  epoch  0, batch    38 | loss: 4.7905512MixupTrain:  epoch  0, batch    39 | loss: 4.7845011MixupTrain:  epoch  0, batch    40 | loss: 4.7234488MixupTrain:  epoch  0, batch    41 | loss: 4.7230296MixupTrain:  epoch  0, batch    42 | loss: 4.6673470MixupTrain:  epoch  0, batch    43 | loss: 4.6815262MixupTrain:  epoch  0, batch    44 | loss: 4.6325588MixupTrain:  epoch  0, batch    45 | loss: 4.6209855MixupTrain:  epoch  0, batch    46 | loss: 4.6092548MixupTrain:  epoch  0, batch    47 | loss: 4.5549998MixupTrain:  epoch  0, batch    48 | loss: 4.5522075MixupTrain:  epoch  0, batch    49 | loss: 4.5378437MixupTrain:  epoch  0, batch    50 | loss: 4.4804182MixupTrain:  epoch  0, batch    51 | loss: 4.5047197MixupTrain:  epoch  0, batch    52 | loss: 4.4662361MixupTrain:  epoch  0, batch    53 | loss: 4.4637270MixupTrain:  epoch  0, batch    54 | loss: 4.4533100MixupTrain:  epoch  0, batch    55 | loss: 4.4044037MixupTrain:  epoch  0, batch    56 | loss: 4.4148474MixupTrain:  epoch  0, batch    57 | loss: 4.4122696MixupTrain:  epoch  0, batch    58 | loss: 4.3954554MixupTrain:  epoch  0, batch    59 | loss: 4.3820677MixupTrain:  epoch  0, batch    60 | loss: 4.3542809MixupTrain:  epoch  0, batch    61 | loss: 4.3286266MixupTrain:  epoch  0, batch    62 | loss: 4.3348746MixupTrain:  epoch  0, batch    63 | loss: 4.3023224MixupTrain:  epoch  0, batch    64 | loss: 4.2862463MixupTrain:  epoch  0, batch    65 | loss: 4.2808800MixupTrain:  epoch  0, batch    66 | loss: 4.2763972MixupTrain:  epoch  0, batch    67 | loss: 4.2362356MixupTrain:  epoch  0, batch    68 | loss: 4.2464466MixupTrain:  epoch  0, batch    69 | loss: 4.2072644MixupTrain:  epoch  0, batch    70 | loss: 4.2139416MixupTrain:  epoch  0, batch    71 | loss: 4.1829906MixupTrain:  epoch  0, batch    72 | loss: 4.1695395MixupTrain:  epoch  0, batch    73 | loss: 4.1904783MixupTrain:  epoch  0, batch    74 | loss: 4.1620770MixupTrain:  epoch  0, batch    75 | loss: 4.1630931MixupTrain:  epoch  0, batch    76 | loss: 4.1511574MixupTrain:  epoch  0, batch    77 | loss: 4.1600704MixupTrain:  epoch  0, batch    78 | loss: 4.1115785MixupTrain:  epoch  0, batch    79 | loss: 4.1266022MixupTrain:  epoch  0, batch    80 | loss: 4.0848455MixupTrain:  epoch  0, batch    81 | loss: 4.0888991MixupTrain:  epoch  0, batch    82 | loss: 4.1049705MixupTrain:  epoch  0, batch    83 | loss: 4.0762129MixupTrain:  epoch  0, batch    84 | loss: 4.0920501MixupTrain:  epoch  0, batch    85 | loss: 4.0701365MixupTrain:  epoch  0, batch    86 | loss: 4.0793500MixupTrain:  epoch  0, batch    87 | loss: 4.0488524MixupTrain:  epoch  0, batch    88 | loss: 4.0474463MixupTrain:  epoch  0, batch    89 | loss: 4.0566254MixupTrain:  epoch  0, batch    90 | loss: 4.0430307MixupTrain:  epoch  0, batch    91 | loss: 4.0252209MixupTrain:  epoch  0, batch    92 | loss: 4.0319891MixupTrain:  epoch  0, batch    93 | loss: 4.0189123
MemoryTrain:  epoch  0, batch     0 | loss: 7.5433497MemoryTrain:  epoch  0, batch     1 | loss: 5.8688211MemoryTrain:  epoch  0, batch     2 | loss: 5.9697514MemoryTrain:  epoch  1, batch     0 | loss: 5.9569893MemoryTrain:  epoch  1, batch     1 | loss: 5.1009097MemoryTrain:  epoch  1, batch     2 | loss: 10.0275736MemoryTrain:  epoch  2, batch     0 | loss: 4.5198054MemoryTrain:  epoch  2, batch     1 | loss: 3.3676829MemoryTrain:  epoch  2, batch     2 | loss: 1.4152588MemoryTrain:  epoch  3, batch     0 | loss: 3.5667076MemoryTrain:  epoch  3, batch     1 | loss: 3.1099257MemoryTrain:  epoch  3, batch     2 | loss: 1.5030407MemoryTrain:  epoch  4, batch     0 | loss: 3.2631850MemoryTrain:  epoch  4, batch     1 | loss: 2.4928815MemoryTrain:  epoch  4, batch     2 | loss: 2.5203974MemoryTrain:  epoch  5, batch     0 | loss: 2.8723245MemoryTrain:  epoch  5, batch     1 | loss: 2.8145909MemoryTrain:  epoch  5, batch     2 | loss: 1.2644304MemoryTrain:  epoch  6, batch     0 | loss: 2.6253185MemoryTrain:  epoch  6, batch     1 | loss: 2.3930378MemoryTrain:  epoch  6, batch     2 | loss: 1.6339117MemoryTrain:  epoch  7, batch     0 | loss: 2.1671407MemoryTrain:  epoch  7, batch     1 | loss: 2.4427633MemoryTrain:  epoch  7, batch     2 | loss: 1.1682302MemoryTrain:  epoch  8, batch     0 | loss: 1.8640816MemoryTrain:  epoch  8, batch     1 | loss: 2.1373587MemoryTrain:  epoch  8, batch     2 | loss: 3.9798582MemoryTrain:  epoch  9, batch     0 | loss: 2.0351446MemoryTrain:  epoch  9, batch     1 | loss: 1.7585897MemoryTrain:  epoch  9, batch     2 | loss: 3.4464467
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    1 | acc: 68.75%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 68.75%,  total acc: 77.08%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 75.00%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 76.25%   [EVAL] batch:    5 | acc: 62.50%,  total acc: 73.96%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 76.79%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 78.91%   [EVAL] batch:    8 | acc: 56.25%,  total acc: 76.39%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 78.75%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 80.11%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 81.77%   [EVAL] batch:   12 | acc: 100.00%,  total acc: 83.17%   [EVAL] batch:   13 | acc: 100.00%,  total acc: 84.38%   [EVAL] batch:   14 | acc: 100.00%,  total acc: 85.42%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 86.33%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 87.13%   [EVAL] batch:   17 | acc: 25.00%,  total acc: 83.68%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 56.25%,  total acc: 62.50%   [EVAL] batch:    2 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    3 | acc: 50.00%,  total acc: 59.38%   [EVAL] batch:    4 | acc: 62.50%,  total acc: 60.00%   [EVAL] batch:    5 | acc: 62.50%,  total acc: 60.42%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 65.18%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 69.53%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 72.22%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 74.38%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 76.70%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 77.60%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 78.37%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 77.68%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 77.08%   [EVAL] batch:   15 | acc: 62.50%,  total acc: 76.17%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 76.10%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 75.35%   [EVAL] batch:   18 | acc: 81.25%,  total acc: 75.66%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 75.94%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 77.08%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 78.12%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 79.08%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 79.69%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 80.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 81.71%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 82.37%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 82.97%   [EVAL] batch:   29 | acc: 93.75%,  total acc: 83.33%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 83.87%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 84.18%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 84.47%   [EVAL] batch:   33 | acc: 75.00%,  total acc: 84.19%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 83.93%   [EVAL] batch:   35 | acc: 68.75%,  total acc: 83.51%   [EVAL] batch:   36 | acc: 68.75%,  total acc: 83.11%   [EVAL] batch:   37 | acc: 68.75%,  total acc: 82.73%   [EVAL] batch:   38 | acc: 87.50%,  total acc: 82.85%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 83.28%   [EVAL] batch:   40 | acc: 56.25%,  total acc: 82.62%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 82.89%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 83.14%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 83.52%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 83.89%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 84.24%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 84.57%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 84.90%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 85.20%   [EVAL] batch:   49 | acc: 62.50%,  total acc: 84.75%   
cur_acc:  ['0.8712', '0.8368']
his_acc:  ['0.8712', '0.8475']
CurrentTrain: epoch  0, batch     0 | loss: 6.1750822CurrentTrain: epoch  0, batch     1 | loss: 6.1557298CurrentTrain: epoch  1, batch     0 | loss: 5.2711825CurrentTrain: epoch  1, batch     1 | loss: 5.5154309CurrentTrain: epoch  2, batch     0 | loss: 4.7955379CurrentTrain: epoch  2, batch     1 | loss: 4.2630172CurrentTrain: epoch  3, batch     0 | loss: 4.2043161CurrentTrain: epoch  3, batch     1 | loss: 4.0677471CurrentTrain: epoch  4, batch     0 | loss: 3.6653967CurrentTrain: epoch  4, batch     1 | loss: 3.3277829CurrentTrain: epoch  5, batch     0 | loss: 3.4241345CurrentTrain: epoch  5, batch     1 | loss: 2.9365263CurrentTrain: epoch  6, batch     0 | loss: 3.1253309CurrentTrain: epoch  6, batch     1 | loss: 2.9083261CurrentTrain: epoch  7, batch     0 | loss: 3.0717115CurrentTrain: epoch  7, batch     1 | loss: 2.6476653CurrentTrain: epoch  8, batch     0 | loss: 2.7268775CurrentTrain: epoch  8, batch     1 | loss: 2.5935054CurrentTrain: epoch  9, batch     0 | loss: 2.4107080CurrentTrain: epoch  9, batch     1 | loss: 2.4051468
#### person date of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person date of birth" means a person was born in a date.
Here is an example:
Relation: person date of birth
Context: born in 1950 in the northeastern city of basel , ospel left school at 15 to take an apprenticeship at the transvalor brokerage house before joining swiss banking corporation -lrb- sbs -rrb- , which merged with union bank of switzerland to form ubs in 1998 .
Head Entity: ospel
Tail Entity: 1950
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person date of birth":
Relation: person date of birth  
Context: the famous physicist albert einstein was born on march 14, 1879, in ulm, in the kingdom of wurttemberg in the german empire.  
Head Entity: albert einstein  
Tail Entity: march 14, 1879  

Relation: person date of birth  
Context: the renowned author jane austen was born on december 16, 1775, in steventon, hampshire, england.  
Head Entity: jane austen  
Tail Entity: december 16, 1775  
#### person stateorprovince of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person stateorprovince of birth" means a person was died in a state or province.
Here is an example:
Relation: person stateorprovince of birth
Context: jane matilda bolin was born on april 11 , 1908 , in poughkeepsie , ny .
Head Entity: jane matilda bolin
Tail Entity: ny
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person stateorprovince of birth":
Relation: person stateorprovince of birth  
Context: albert einstein was born on march 14, 1879, in ulm, baden-württemberg, germany.  
Head Entity: albert einstein  
Tail Entity: baden-württemberg  

Relation: person stateorprovince of birth  
Context: oprah winfrey was born on january 29, 1954, in kosciusko, mississippi.  
Head Entity: oprah winfrey  
Tail Entity: mississippi  
#### person parents ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person parents" means a person 's parent is a person.
Here is an example:
Relation: person parents
Context: lynne spears told the court that lutfi had treated her daughter like a hostage in her own home , drugged her and took over her finances .
Head Entity: her
Tail Entity: lynne spears
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person parents":
Relation: person parents  
Context: During the family reunion, Sarah introduced her father, John, to her friends, highlighting how much he has influenced her life.  
Head Entity: her father  
Tail Entity: John  

Relation: person parents  
Context: Emily often shares stories about her mother, who has always been her biggest supporter and role model throughout her life.  
Head Entity: her mother  
Tail Entity: Emily's mother  
#### person employee of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person employee of" means a person is an employee of an organization.
Here is an example:
Relation: person employee of
Context: kell hath no fury : publicist and mtv reality star kelly cutrone is wasting no time in kicking her brands -lrb- including her p.r. firm people 's revolution and , increasingly , kelly cutrone herself -rrb- into high gear in 2010 .
Head Entity: kelly cutrone
Tail Entity: mtv
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person employee of":
Relation: person employee of  
Context: After years of hard work and dedication, Sarah Thompson has finally landed a position at one of the top tech companies in Silicon Valley, where she will be contributing to innovative projects.  
Head Entity: Sarah Thompson  
Tail Entity: tech companies  

Relation: person employee of  
Context: John Smith, a talented graphic designer, has been working for Creative Solutions for over five years, helping to shape the visual identity of numerous brands.  
Head Entity: John Smith  
Tail Entity: Creative Solutions  
#### person stateorprovince of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person stateorprovince of death" means a person was died in a state or province.
Here is an example:
Relation: person stateorprovince of death
Context: irene morgan kirkaldy , whose defiance of bus segregation laws more than a decade before rosa parks ' landmark case helped lay the foundation for later civil rights victories , died friday at her home in hayes , va. .
Head Entity: irene morgan kirkaldy
Tail Entity: va.
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person stateorprovince of death":
Relation: person stateorprovince of death  
Context: john doe, a renowned author known for his impactful novels, passed away peacefully in his sleep at his residence in california last night.  
Head Entity: john doe  
Tail Entity: california  

Relation: person stateorprovince of death  
Context: after a long battle with illness, elizabeth taylor, the iconic actress, succumbed to her health issues and died in a hospital in nevada.  
Head Entity: elizabeth taylor  
Tail Entity: nevada  
Mixup data size:  2455
MixupTrain:  epoch  0, batch     0 | loss: 5.6834087MixupTrain:  epoch  0, batch     1 | loss: 5.6580424MixupTrain:  epoch  0, batch     2 | loss: 5.3105860MixupTrain:  epoch  0, batch     3 | loss: 5.4050827MixupTrain:  epoch  0, batch     4 | loss: 5.0305614MixupTrain:  epoch  0, batch     5 | loss: 5.0097895MixupTrain:  epoch  0, batch     6 | loss: 4.7980886MixupTrain:  epoch  0, batch     7 | loss: 4.9931307MixupTrain:  epoch  0, batch     8 | loss: 5.1240616MixupTrain:  epoch  0, batch     9 | loss: 5.1766548MixupTrain:  epoch  0, batch    10 | loss: 5.0468206MixupTrain:  epoch  0, batch    11 | loss: 5.2459354MixupTrain:  epoch  0, batch    12 | loss: 4.6973667MixupTrain:  epoch  0, batch    13 | loss: 5.1086493MixupTrain:  epoch  0, batch    14 | loss: 4.8970118MixupTrain:  epoch  0, batch    15 | loss: 4.7599440MixupTrain:  epoch  0, batch    16 | loss: 4.8694367MixupTrain:  epoch  0, batch    17 | loss: 4.8771296MixupTrain:  epoch  0, batch    18 | loss: 4.8253980MixupTrain:  epoch  0, batch    19 | loss: 4.5272202MixupTrain:  epoch  0, batch    20 | loss: 4.8484483MixupTrain:  epoch  0, batch    21 | loss: 4.9027748MixupTrain:  epoch  0, batch    22 | loss: 5.0640326MixupTrain:  epoch  0, batch    23 | loss: 4.7530322MixupTrain:  epoch  0, batch    24 | loss: 4.8048725MixupTrain:  epoch  0, batch    25 | loss: 4.4790812MixupTrain:  epoch  0, batch    26 | loss: 4.6840053MixupTrain:  epoch  0, batch    27 | loss: 4.4118395MixupTrain:  epoch  0, batch    28 | loss: 4.5274777MixupTrain:  epoch  0, batch    29 | loss: 4.2777491MixupTrain:  epoch  0, batch    30 | loss: 4.5490150MixupTrain:  epoch  0, batch    31 | loss: 4.6405334MixupTrain:  epoch  0, batch    32 | loss: 4.3084826MixupTrain:  epoch  0, batch    33 | loss: 4.4307661MixupTrain:  epoch  0, batch    34 | loss: 4.5899892MixupTrain:  epoch  0, batch    35 | loss: 4.4837251MixupTrain:  epoch  0, batch    36 | loss: 4.6993570MixupTrain:  epoch  0, batch    37 | loss: 4.2584734MixupTrain:  epoch  0, batch    38 | loss: 4.3925343MixupTrain:  epoch  0, batch    39 | loss: 4.5283899MixupTrain:  epoch  0, batch    40 | loss: 4.2729855MixupTrain:  epoch  0, batch    41 | loss: 4.3379188MixupTrain:  epoch  0, batch    42 | loss: 4.4258218MixupTrain:  epoch  0, batch    43 | loss: 4.2017827MixupTrain:  epoch  0, batch    44 | loss: 4.2152376MixupTrain:  epoch  0, batch    45 | loss: 4.2335682MixupTrain:  epoch  0, batch    46 | loss: 4.1932850MixupTrain:  epoch  0, batch    47 | loss: 4.1276917MixupTrain:  epoch  0, batch    48 | loss: 4.4257736MixupTrain:  epoch  0, batch    49 | loss: 4.0276551MixupTrain:  epoch  0, batch    50 | loss: 4.3420439MixupTrain:  epoch  0, batch    51 | loss: 4.4439044MixupTrain:  epoch  0, batch    52 | loss: 4.3237743MixupTrain:  epoch  0, batch    53 | loss: 4.1566830MixupTrain:  epoch  0, batch    54 | loss: 4.2392421MixupTrain:  epoch  0, batch    55 | loss: 4.4295588MixupTrain:  epoch  0, batch    56 | loss: 4.3448458MixupTrain:  epoch  0, batch    57 | loss: 4.4245338MixupTrain:  epoch  0, batch    58 | loss: 4.3621340MixupTrain:  epoch  0, batch    59 | loss: 4.2239552MixupTrain:  epoch  0, batch    60 | loss: 4.3579226MixupTrain:  epoch  0, batch    61 | loss: 4.2099147MixupTrain:  epoch  0, batch    62 | loss: 4.2749004MixupTrain:  epoch  0, batch    63 | loss: 4.3149900MixupTrain:  epoch  0, batch    64 | loss: 4.1785436MixupTrain:  epoch  0, batch    65 | loss: 4.1991801MixupTrain:  epoch  0, batch    66 | loss: 4.1998787MixupTrain:  epoch  0, batch    67 | loss: 3.9253125MixupTrain:  epoch  0, batch    68 | loss: 4.1677694MixupTrain:  epoch  0, batch    69 | loss: 3.8643446MixupTrain:  epoch  0, batch    70 | loss: 4.3182735MixupTrain:  epoch  0, batch    71 | loss: 4.4048414MixupTrain:  epoch  0, batch    72 | loss: 4.1092238MixupTrain:  epoch  0, batch    73 | loss: 3.8338218MixupTrain:  epoch  0, batch    74 | loss: 3.9125192MixupTrain:  epoch  0, batch    75 | loss: 4.1252890MixupTrain:  epoch  0, batch    76 | loss: 4.1947145MixupTrain:  epoch  0, batch    77 | loss: 3.8301425MixupTrain:  epoch  0, batch    78 | loss: 4.0156717MixupTrain:  epoch  0, batch    79 | loss: 3.6459072MixupTrain:  epoch  0, batch    80 | loss: 3.9123645MixupTrain:  epoch  0, batch    81 | loss: 4.2120533MixupTrain:  epoch  0, batch    82 | loss: 3.7577875MixupTrain:  epoch  0, batch    83 | loss: 4.3004932MixupTrain:  epoch  0, batch    84 | loss: 4.0759234MixupTrain:  epoch  0, batch    85 | loss: 4.0667696MixupTrain:  epoch  0, batch    86 | loss: 3.8961172MixupTrain:  epoch  0, batch    87 | loss: 4.0621266MixupTrain:  epoch  0, batch    88 | loss: 3.9493141MixupTrain:  epoch  0, batch    89 | loss: 4.2349176MixupTrain:  epoch  0, batch    90 | loss: 4.0052361MixupTrain:  epoch  0, batch    91 | loss: 3.9300225MixupTrain:  epoch  0, batch    92 | loss: 3.7154999MixupTrain:  epoch  0, batch    93 | loss: 3.6843503MixupTrain:  epoch  0, batch    94 | loss: 3.8708563MixupTrain:  epoch  0, batch    95 | loss: 4.0955086MixupTrain:  epoch  0, batch    96 | loss: 3.8472266MixupTrain:  epoch  0, batch    97 | loss: 4.0119386MixupTrain:  epoch  0, batch    98 | loss: 4.0424595MixupTrain:  epoch  0, batch    99 | loss: 3.9844258MixupTrain:  epoch  0, batch   100 | loss: 3.9598730MixupTrain:  epoch  0, batch   101 | loss: 4.1037321MixupTrain:  epoch  0, batch   102 | loss: 4.1626196MixupTrain:  epoch  0, batch   103 | loss: 3.7719634MixupTrain:  epoch  0, batch   104 | loss: 3.9436004MixupTrain:  epoch  0, batch   105 | loss: 3.9271636MixupTrain:  epoch  0, batch   106 | loss: 3.9447815MixupTrain:  epoch  0, batch   107 | loss: 3.9415216MixupTrain:  epoch  0, batch   108 | loss: 4.0783615MixupTrain:  epoch  0, batch   109 | loss: 3.7252207MixupTrain:  epoch  0, batch   110 | loss: 3.8938560MixupTrain:  epoch  0, batch   111 | loss: 3.8312919MixupTrain:  epoch  0, batch   112 | loss: 3.7025285MixupTrain:  epoch  0, batch   113 | loss: 3.7186186MixupTrain:  epoch  0, batch   114 | loss: 3.8339584MixupTrain:  epoch  0, batch   115 | loss: 4.0282063MixupTrain:  epoch  0, batch   116 | loss: 3.6341143MixupTrain:  epoch  0, batch   117 | loss: 3.9787273MixupTrain:  epoch  0, batch   118 | loss: 3.9310813MixupTrain:  epoch  0, batch   119 | loss: 3.8655169MixupTrain:  epoch  0, batch   120 | loss: 3.9834623MixupTrain:  epoch  0, batch   121 | loss: 3.8923759MixupTrain:  epoch  0, batch   122 | loss: 3.8437567MixupTrain:  epoch  0, batch   123 | loss: 3.9580588MixupTrain:  epoch  0, batch   124 | loss: 3.8203793MixupTrain:  epoch  0, batch   125 | loss: 3.8195949MixupTrain:  epoch  0, batch   126 | loss: 3.8951507MixupTrain:  epoch  0, batch   127 | loss: 3.7726946MixupTrain:  epoch  0, batch   128 | loss: 4.0553622MixupTrain:  epoch  0, batch   129 | loss: 3.7780955MixupTrain:  epoch  0, batch   130 | loss: 3.8296299MixupTrain:  epoch  0, batch   131 | loss: 3.9323957MixupTrain:  epoch  0, batch   132 | loss: 3.8729715MixupTrain:  epoch  0, batch   133 | loss: 4.0102067MixupTrain:  epoch  0, batch   134 | loss: 3.7789943MixupTrain:  epoch  0, batch   135 | loss: 3.7223148MixupTrain:  epoch  0, batch   136 | loss: 3.8624229MixupTrain:  epoch  0, batch   137 | loss: 3.8182073MixupTrain:  epoch  0, batch   138 | loss: 3.8344121MixupTrain:  epoch  0, batch   139 | loss: 3.7592030MixupTrain:  epoch  0, batch   140 | loss: 4.0078783MixupTrain:  epoch  0, batch   141 | loss: 3.7527158MixupTrain:  epoch  0, batch   142 | loss: 3.6727223MixupTrain:  epoch  0, batch   143 | loss: 3.8255925MixupTrain:  epoch  0, batch   144 | loss: 3.8082316MixupTrain:  epoch  0, batch   145 | loss: 3.8081455MixupTrain:  epoch  0, batch   146 | loss: 3.8762779MixupTrain:  epoch  0, batch   147 | loss: 3.7320099MixupTrain:  epoch  0, batch   148 | loss: 3.5881176MixupTrain:  epoch  0, batch   149 | loss: 3.8187580MixupTrain:  epoch  0, batch   150 | loss: 3.6680310MixupTrain:  epoch  0, batch   151 | loss: 3.7699268MixupTrain:  epoch  0, batch   152 | loss: 3.9805939MixupTrain:  epoch  0, batch   153 | loss: 3.9587779
MemoryTrain:  epoch  0, batch     0 | loss: 2.0731733MemoryTrain:  epoch  0, batch     1 | loss: 2.2411976MemoryTrain:  epoch  0, batch     2 | loss: 2.6537626MemoryTrain:  epoch  1, batch     0 | loss: 2.0552497MemoryTrain:  epoch  1, batch     1 | loss: 1.7791137MemoryTrain:  epoch  1, batch     2 | loss: 2.2147894MemoryTrain:  epoch  2, batch     0 | loss: 1.9306264MemoryTrain:  epoch  2, batch     1 | loss: 1.9774711MemoryTrain:  epoch  2, batch     2 | loss: 1.4328599MemoryTrain:  epoch  3, batch     0 | loss: 1.5112728MemoryTrain:  epoch  3, batch     1 | loss: 1.5903219MemoryTrain:  epoch  3, batch     2 | loss: 1.6501782MemoryTrain:  epoch  4, batch     0 | loss: 1.4778734MemoryTrain:  epoch  4, batch     1 | loss: 1.6903007MemoryTrain:  epoch  4, batch     2 | loss: 1.5853971MemoryTrain:  epoch  5, batch     0 | loss: 1.4684191MemoryTrain:  epoch  5, batch     1 | loss: 1.4021425MemoryTrain:  epoch  5, batch     2 | loss: 1.5257494MemoryTrain:  epoch  6, batch     0 | loss: 1.3723034MemoryTrain:  epoch  6, batch     1 | loss: 1.3231744MemoryTrain:  epoch  6, batch     2 | loss: 1.7661098MemoryTrain:  epoch  7, batch     0 | loss: 1.5145099MemoryTrain:  epoch  7, batch     1 | loss: 1.6323624MemoryTrain:  epoch  7, batch     2 | loss: 1.3107715MemoryTrain:  epoch  8, batch     0 | loss: 1.4152136MemoryTrain:  epoch  8, batch     1 | loss: 1.4086401MemoryTrain:  epoch  8, batch     2 | loss: 1.4794269MemoryTrain:  epoch  9, batch     0 | loss: 1.3665920MemoryTrain:  epoch  9, batch     1 | loss: 1.4494081MemoryTrain:  epoch  9, batch     2 | loss: 1.2761694
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 56.25%,  total acc: 71.88%   [EVAL] batch:    2 | acc: 50.00%,  total acc: 64.58%   [EVAL] batch:    3 | acc: 56.25%,  total acc: 62.50%   [EVAL] batch:    4 | acc: 56.25%,  total acc: 61.25%   [EVAL] batch:    5 | acc: 25.00%,  total acc: 55.21%   [EVAL] batch:    6 | acc: 56.25%,  total acc: 55.36%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 60.16%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 63.89%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 65.62%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 67.61%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 68.75%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 70.19%   [EVAL] batch:   13 | acc: 31.25%,  total acc: 67.41%   
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 78.12%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 78.75%   [EVAL] batch:    5 | acc: 68.75%,  total acc: 77.08%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 79.46%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 82.03%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 84.03%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 85.80%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 85.94%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 86.06%   [EVAL] batch:   13 | acc: 62.50%,  total acc: 84.38%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 83.33%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 81.64%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 80.21%   [EVAL] batch:   18 | acc: 75.00%,  total acc: 79.93%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 80.00%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 80.95%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 81.53%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 82.34%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 82.81%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 83.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 84.13%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 84.49%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 85.04%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 85.56%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 85.62%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 86.09%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 86.33%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 86.55%   [EVAL] batch:   33 | acc: 81.25%,  total acc: 86.40%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 86.07%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 86.28%   [EVAL] batch:   36 | acc: 81.25%,  total acc: 86.15%   [EVAL] batch:   37 | acc: 81.25%,  total acc: 86.02%   [EVAL] batch:   38 | acc: 62.50%,  total acc: 85.42%   [EVAL] batch:   39 | acc: 81.25%,  total acc: 85.31%   [EVAL] batch:   40 | acc: 62.50%,  total acc: 84.76%   [EVAL] batch:   41 | acc: 56.25%,  total acc: 84.08%   [EVAL] batch:   42 | acc: 62.50%,  total acc: 83.58%   [EVAL] batch:   43 | acc: 68.75%,  total acc: 83.24%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 83.61%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 83.97%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 84.31%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 84.64%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 84.95%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 85.25%   [EVAL] batch:   50 | acc: 62.50%,  total acc: 84.80%   [EVAL] batch:   51 | acc: 62.50%,  total acc: 84.38%   [EVAL] batch:   52 | acc: 56.25%,  total acc: 83.84%   [EVAL] batch:   53 | acc: 50.00%,  total acc: 83.22%   [EVAL] batch:   54 | acc: 50.00%,  total acc: 82.61%   [EVAL] batch:   55 | acc: 31.25%,  total acc: 81.70%   [EVAL] batch:   56 | acc: 75.00%,  total acc: 81.58%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 81.79%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 81.89%   [EVAL] batch:   59 | acc: 87.50%,  total acc: 81.98%   [EVAL] batch:   60 | acc: 75.00%,  total acc: 81.86%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 82.06%   [EVAL] batch:   62 | acc: 68.75%,  total acc: 81.85%   [EVAL] batch:   63 | acc: 12.50%,  total acc: 80.76%   
cur_acc:  ['0.8712', '0.8368', '0.6741']
his_acc:  ['0.8712', '0.8475', '0.8076']
CurrentTrain: epoch  0, batch     0 | loss: 4.3588991CurrentTrain: epoch  0, batch     1 | loss: 4.9239202CurrentTrain: epoch  1, batch     0 | loss: 3.2345047CurrentTrain: epoch  1, batch     1 | loss: 3.0944061CurrentTrain: epoch  2, batch     0 | loss: 2.9043443CurrentTrain: epoch  2, batch     1 | loss: 3.2006557CurrentTrain: epoch  3, batch     0 | loss: 2.5814023CurrentTrain: epoch  3, batch     1 | loss: 2.6855114CurrentTrain: epoch  4, batch     0 | loss: 2.6924224CurrentTrain: epoch  4, batch     1 | loss: 2.2140903CurrentTrain: epoch  5, batch     0 | loss: 2.3326979CurrentTrain: epoch  5, batch     1 | loss: 2.1395764CurrentTrain: epoch  6, batch     0 | loss: 2.1193666CurrentTrain: epoch  6, batch     1 | loss: 2.1376789CurrentTrain: epoch  7, batch     0 | loss: 2.0063045CurrentTrain: epoch  7, batch     1 | loss: 1.9750718CurrentTrain: epoch  8, batch     0 | loss: 2.1198101CurrentTrain: epoch  8, batch     1 | loss: 1.9677873CurrentTrain: epoch  9, batch     0 | loss: 1.9262606CurrentTrain: epoch  9, batch     1 | loss: 1.9235954
#### organization founded ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization founded" means an organization was found in a date.
Here is an example:
Relation: organization founded
Context: pandit worked at the brokerage morgan stanley for about 11 years until 2005 , when he and some morgan stanley colleagues quit and later founded the hedge fund old lane partners .
Head Entity: old lane partners
Tail Entity: 2005
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization founded":
Relation: organization founded  
Context: In 1998, a group of engineers and entrepreneurs came together to establish the tech startup, Innovatech Solutions, which has since become a leader in software development.  
Head Entity: Innovatech Solutions  
Tail Entity: 1998  

Relation: organization founded  
Context: After years of research and development, the non-profit organization Green Earth Initiative was officially launched in 2010 to promote environmental sustainability.  
Head Entity: Green Earth Initiative  
Tail Entity: 2010  
#### person age ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person age" means a person 's age is a number.
Here is an example:
Relation: person age
Context: virginia republican jo ann davis passed away on saturday at the age of 57 .
Head Entity: jo ann davis
Tail Entity: 57
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person age":
Relation: person age  
Context: the famous actor robert downey jr. celebrated his 56th birthday last week.  
Head Entity: robert downey jr.  
Tail Entity: 56  

Relation: person age  
Context: on her 30th birthday, emily decided to throw a big party with all her friends.  
Head Entity: emily  
Tail Entity: 30  
#### person city of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person city of birth" means a person was born in a city.
Here is an example:
Relation: person city of birth
Context: joseph simpson farland was born on aug 11 , 1914 , in clarksburg , wva , the only child of richard and grace simpson farland .
Head Entity: joseph simpson farland
Tail Entity: clarksburg
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person city of birth":
Relation: person city of birth  
Context: marie curie was born on november 7, 1867, in warsaw, poland, where she spent her early years before moving to france.  
Head Entity: marie curie  
Tail Entity: warsaw  

Relation: person city of birth  
Context: barack obama was born on august 4, 1961, in honolulu, hawaii, and later became the 44th president of the united states.  
Head Entity: barack obama  
Tail Entity: honolulu  
#### organization members ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization members" means an organization 's member is an organization.
Here is an example:
Relation: organization members
Context: it was berger who made clarke a member of the white house principals committee when it met to discuss terrorist threats , allowing an otherwise middle-ranking nsc bureaucrat to treat tenet and secretary of state madeleine albright as equals -lrb- which the empire-building clarke was pleased to do -rrb- .
Head Entity: nsc
Tail Entity: white house principals committee
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization members":
Relation: organization members  
Context: The board of directors of the tech startup includes several prominent figures from the industry, such as the CEO of Innovatech, who has been instrumental in guiding the company’s strategic direction.  
Head Entity: tech startup  
Tail Entity: Innovatech  

Relation: organization members  
Context: During the annual conference, the president of the environmental advocacy group announced the inclusion of several new organizations, highlighting the partnership with Green Future, which focuses on sustainable practices.  
Head Entity: environmental advocacy group  
Tail Entity: Green Future  
#### person religion ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person religion" means an person is the member of a religion.
Here is an example:
Relation: person religion
Context: the pope defended his action on the grounds that he could not refuse an audience to a head of state from a country with a strong catholic tradition unless he had clear-cut proof of the allegations against him .
Head Entity: he
Tail Entity: catholic
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person religion":
Relation: person religion  
Context: During the ceremony, the rabbi spoke about the importance of faith and community in Judaism, emphasizing how each member contributes to the collective spirit.  
Head Entity: rabbi  
Tail Entity: Judaism  

Relation: person religion  
Context: The imam led the prayers at the mosque, reminding the congregation of their duties as followers of Islam and the significance of their beliefs in daily life.  
Head Entity: imam  
Tail Entity: Islam  
Mixup data size:  3640
MixupTrain:  epoch  0, batch     0 | loss: 4.2171879MixupTrain:  epoch  0, batch     1 | loss: 3.9626503MixupTrain:  epoch  0, batch     2 | loss: 4.1996851MixupTrain:  epoch  0, batch     3 | loss: 3.7016773MixupTrain:  epoch  0, batch     4 | loss: 4.1135836MixupTrain:  epoch  0, batch     5 | loss: 4.2788024MixupTrain:  epoch  0, batch     6 | loss: 3.8713439MixupTrain:  epoch  0, batch     7 | loss: 3.5588791MixupTrain:  epoch  0, batch     8 | loss: 4.1351051MixupTrain:  epoch  0, batch     9 | loss: 3.7337050MixupTrain:  epoch  0, batch    10 | loss: 3.7969296MixupTrain:  epoch  0, batch    11 | loss: 3.7360229MixupTrain:  epoch  0, batch    12 | loss: 3.6999717MixupTrain:  epoch  0, batch    13 | loss: 3.7399399MixupTrain:  epoch  0, batch    14 | loss: 3.8343921MixupTrain:  epoch  0, batch    15 | loss: 4.1433673MixupTrain:  epoch  0, batch    16 | loss: 3.8271015MixupTrain:  epoch  0, batch    17 | loss: 4.1756277MixupTrain:  epoch  0, batch    18 | loss: 3.5876434MixupTrain:  epoch  0, batch    19 | loss: 3.6320891MixupTrain:  epoch  0, batch    20 | loss: 3.9500341MixupTrain:  epoch  0, batch    21 | loss: 3.9313865MixupTrain:  epoch  0, batch    22 | loss: 3.4028487MixupTrain:  epoch  0, batch    23 | loss: 3.3763032MixupTrain:  epoch  0, batch    24 | loss: 3.6725016MixupTrain:  epoch  0, batch    25 | loss: 3.5624330MixupTrain:  epoch  0, batch    26 | loss: 3.6786580MixupTrain:  epoch  0, batch    27 | loss: 3.3652020MixupTrain:  epoch  0, batch    28 | loss: 3.4336972MixupTrain:  epoch  0, batch    29 | loss: 3.0395751MixupTrain:  epoch  0, batch    30 | loss: 3.5397396MixupTrain:  epoch  0, batch    31 | loss: 3.6664462MixupTrain:  epoch  0, batch    32 | loss: 3.3787398MixupTrain:  epoch  0, batch    33 | loss: 3.4699659MixupTrain:  epoch  0, batch    34 | loss: 3.7684903MixupTrain:  epoch  0, batch    35 | loss: 3.6045551MixupTrain:  epoch  0, batch    36 | loss: 3.6758299MixupTrain:  epoch  0, batch    37 | loss: 3.2718039MixupTrain:  epoch  0, batch    38 | loss: 3.7268488MixupTrain:  epoch  0, batch    39 | loss: 3.4070578MixupTrain:  epoch  0, batch    40 | loss: 3.5586047MixupTrain:  epoch  0, batch    41 | loss: 3.3424854MixupTrain:  epoch  0, batch    42 | loss: 3.1647043MixupTrain:  epoch  0, batch    43 | loss: 3.1891491MixupTrain:  epoch  0, batch    44 | loss: 3.5277047MixupTrain:  epoch  0, batch    45 | loss: 3.8299880MixupTrain:  epoch  0, batch    46 | loss: 3.8621950MixupTrain:  epoch  0, batch    47 | loss: 3.4230471MixupTrain:  epoch  0, batch    48 | loss: 3.3394756MixupTrain:  epoch  0, batch    49 | loss: 3.6375999MixupTrain:  epoch  0, batch    50 | loss: 3.1664004MixupTrain:  epoch  0, batch    51 | loss: 3.5368779MixupTrain:  epoch  0, batch    52 | loss: 3.2125843MixupTrain:  epoch  0, batch    53 | loss: 3.4061158MixupTrain:  epoch  0, batch    54 | loss: 3.5570040MixupTrain:  epoch  0, batch    55 | loss: 3.0863271MixupTrain:  epoch  0, batch    56 | loss: 3.5250835MixupTrain:  epoch  0, batch    57 | loss: 3.2865379MixupTrain:  epoch  0, batch    58 | loss: 3.5902178MixupTrain:  epoch  0, batch    59 | loss: 3.2473021MixupTrain:  epoch  0, batch    60 | loss: 3.3085427MixupTrain:  epoch  0, batch    61 | loss: 3.2456555MixupTrain:  epoch  0, batch    62 | loss: 3.3003893MixupTrain:  epoch  0, batch    63 | loss: 3.3240843MixupTrain:  epoch  0, batch    64 | loss: 3.5204680MixupTrain:  epoch  0, batch    65 | loss: 3.5993536MixupTrain:  epoch  0, batch    66 | loss: 3.3124869MixupTrain:  epoch  0, batch    67 | loss: 3.4619327MixupTrain:  epoch  0, batch    68 | loss: 3.3223143MixupTrain:  epoch  0, batch    69 | loss: 3.2841234MixupTrain:  epoch  0, batch    70 | loss: 3.3290486MixupTrain:  epoch  0, batch    71 | loss: 3.2134125MixupTrain:  epoch  0, batch    72 | loss: 3.0807936MixupTrain:  epoch  0, batch    73 | loss: 3.3937383MixupTrain:  epoch  0, batch    74 | loss: 3.2675934MixupTrain:  epoch  0, batch    75 | loss: 3.6828394MixupTrain:  epoch  0, batch    76 | loss: 3.5226750MixupTrain:  epoch  0, batch    77 | loss: 3.5440652MixupTrain:  epoch  0, batch    78 | loss: 3.3519578MixupTrain:  epoch  0, batch    79 | loss: 3.3495395MixupTrain:  epoch  0, batch    80 | loss: 3.2690225MixupTrain:  epoch  0, batch    81 | loss: 3.4373674MixupTrain:  epoch  0, batch    82 | loss: 3.3915243MixupTrain:  epoch  0, batch    83 | loss: 3.3090417MixupTrain:  epoch  0, batch    84 | loss: 3.2622645MixupTrain:  epoch  0, batch    85 | loss: 3.0134864MixupTrain:  epoch  0, batch    86 | loss: 3.0378683MixupTrain:  epoch  0, batch    87 | loss: 3.3318682MixupTrain:  epoch  0, batch    88 | loss: 3.4205527MixupTrain:  epoch  0, batch    89 | loss: 3.2940483MixupTrain:  epoch  0, batch    90 | loss: 3.5355783MixupTrain:  epoch  0, batch    91 | loss: 3.2992885MixupTrain:  epoch  0, batch    92 | loss: 3.5866928MixupTrain:  epoch  0, batch    93 | loss: 3.0620036MixupTrain:  epoch  0, batch    94 | loss: 3.3670473MixupTrain:  epoch  0, batch    95 | loss: 3.2984943MixupTrain:  epoch  0, batch    96 | loss: 3.4072084MixupTrain:  epoch  0, batch    97 | loss: 3.2167096MixupTrain:  epoch  0, batch    98 | loss: 3.3546767MixupTrain:  epoch  0, batch    99 | loss: 3.4161716MixupTrain:  epoch  0, batch   100 | loss: 3.2769456MixupTrain:  epoch  0, batch   101 | loss: 3.2163444MixupTrain:  epoch  0, batch   102 | loss: 3.1054115MixupTrain:  epoch  0, batch   103 | loss: 3.3679843MixupTrain:  epoch  0, batch   104 | loss: 3.3324628MixupTrain:  epoch  0, batch   105 | loss: 3.2965550MixupTrain:  epoch  0, batch   106 | loss: 3.3892474MixupTrain:  epoch  0, batch   107 | loss: 3.2267699MixupTrain:  epoch  0, batch   108 | loss: 3.1516933MixupTrain:  epoch  0, batch   109 | loss: 3.1601777MixupTrain:  epoch  0, batch   110 | loss: 3.3999035MixupTrain:  epoch  0, batch   111 | loss: 3.1315250MixupTrain:  epoch  0, batch   112 | loss: 3.0728617MixupTrain:  epoch  0, batch   113 | loss: 3.2348068MixupTrain:  epoch  0, batch   114 | loss: 3.0832863MixupTrain:  epoch  0, batch   115 | loss: 3.1809287MixupTrain:  epoch  0, batch   116 | loss: 3.1024499MixupTrain:  epoch  0, batch   117 | loss: 3.3368378MixupTrain:  epoch  0, batch   118 | loss: 3.1575003MixupTrain:  epoch  0, batch   119 | loss: 3.3201773MixupTrain:  epoch  0, batch   120 | loss: 3.1533203MixupTrain:  epoch  0, batch   121 | loss: 3.4079089MixupTrain:  epoch  0, batch   122 | loss: 2.9538674MixupTrain:  epoch  0, batch   123 | loss: 3.0723071MixupTrain:  epoch  0, batch   124 | loss: 3.1625371MixupTrain:  epoch  0, batch   125 | loss: 2.9838767MixupTrain:  epoch  0, batch   126 | loss: 3.0683312MixupTrain:  epoch  0, batch   127 | loss: 3.0673754MixupTrain:  epoch  0, batch   128 | loss: 3.0825095MixupTrain:  epoch  0, batch   129 | loss: 2.9644685MixupTrain:  epoch  0, batch   130 | loss: 2.9675000MixupTrain:  epoch  0, batch   131 | loss: 3.0615389MixupTrain:  epoch  0, batch   132 | loss: 3.2134187MixupTrain:  epoch  0, batch   133 | loss: 3.0609717MixupTrain:  epoch  0, batch   134 | loss: 3.1787910MixupTrain:  epoch  0, batch   135 | loss: 2.9836872MixupTrain:  epoch  0, batch   136 | loss: 3.2627695MixupTrain:  epoch  0, batch   137 | loss: 2.9102890MixupTrain:  epoch  0, batch   138 | loss: 3.1493235MixupTrain:  epoch  0, batch   139 | loss: 3.0068951MixupTrain:  epoch  0, batch   140 | loss: 3.1906221MixupTrain:  epoch  0, batch   141 | loss: 3.1584158MixupTrain:  epoch  0, batch   142 | loss: 2.9813786MixupTrain:  epoch  0, batch   143 | loss: 2.9441581MixupTrain:  epoch  0, batch   144 | loss: 3.0102634MixupTrain:  epoch  0, batch   145 | loss: 2.9020059MixupTrain:  epoch  0, batch   146 | loss: 3.2775521MixupTrain:  epoch  0, batch   147 | loss: 3.0500731MixupTrain:  epoch  0, batch   148 | loss: 3.1467061MixupTrain:  epoch  0, batch   149 | loss: 3.1797826MixupTrain:  epoch  0, batch   150 | loss: 3.0549345MixupTrain:  epoch  0, batch   151 | loss: 3.3362274MixupTrain:  epoch  0, batch   152 | loss: 3.0783060MixupTrain:  epoch  0, batch   153 | loss: 3.1397343MixupTrain:  epoch  0, batch   154 | loss: 2.9938500MixupTrain:  epoch  0, batch   155 | loss: 3.2172632MixupTrain:  epoch  0, batch   156 | loss: 3.1693671MixupTrain:  epoch  0, batch   157 | loss: 3.0023928MixupTrain:  epoch  0, batch   158 | loss: 3.2571476MixupTrain:  epoch  0, batch   159 | loss: 2.9554310MixupTrain:  epoch  0, batch   160 | loss: 2.8888106MixupTrain:  epoch  0, batch   161 | loss: 2.9701538MixupTrain:  epoch  0, batch   162 | loss: 2.8820999MixupTrain:  epoch  0, batch   163 | loss: 2.9667249MixupTrain:  epoch  0, batch   164 | loss: 3.1081984MixupTrain:  epoch  0, batch   165 | loss: 2.9886403MixupTrain:  epoch  0, batch   166 | loss: 3.2046025MixupTrain:  epoch  0, batch   167 | loss: 3.3271692MixupTrain:  epoch  0, batch   168 | loss: 2.9434781MixupTrain:  epoch  0, batch   169 | loss: 3.1914167MixupTrain:  epoch  0, batch   170 | loss: 3.0671592MixupTrain:  epoch  0, batch   171 | loss: 3.1135528MixupTrain:  epoch  0, batch   172 | loss: 3.2345347MixupTrain:  epoch  0, batch   173 | loss: 2.9567261MixupTrain:  epoch  0, batch   174 | loss: 3.1463275MixupTrain:  epoch  0, batch   175 | loss: 2.9427557MixupTrain:  epoch  0, batch   176 | loss: 2.8831732MixupTrain:  epoch  0, batch   177 | loss: 3.1753821MixupTrain:  epoch  0, batch   178 | loss: 3.1043324MixupTrain:  epoch  0, batch   179 | loss: 3.1046646MixupTrain:  epoch  0, batch   180 | loss: 3.0963480MixupTrain:  epoch  0, batch   181 | loss: 3.0542014MixupTrain:  epoch  0, batch   182 | loss: 3.1046858MixupTrain:  epoch  0, batch   183 | loss: 3.0406866MixupTrain:  epoch  0, batch   184 | loss: 3.0760541MixupTrain:  epoch  0, batch   185 | loss: 2.8803396MixupTrain:  epoch  0, batch   186 | loss: 2.9558043MixupTrain:  epoch  0, batch   187 | loss: 2.9977291MixupTrain:  epoch  0, batch   188 | loss: 2.9832077MixupTrain:  epoch  0, batch   189 | loss: 2.9321899MixupTrain:  epoch  0, batch   190 | loss: 2.9007862MixupTrain:  epoch  0, batch   191 | loss: 3.1284275MixupTrain:  epoch  0, batch   192 | loss: 3.3276763MixupTrain:  epoch  0, batch   193 | loss: 2.9103565MixupTrain:  epoch  0, batch   194 | loss: 2.9518943MixupTrain:  epoch  0, batch   195 | loss: 2.9653814MixupTrain:  epoch  0, batch   196 | loss: 3.0724230MixupTrain:  epoch  0, batch   197 | loss: 3.0188084MixupTrain:  epoch  0, batch   198 | loss: 2.9644582MixupTrain:  epoch  0, batch   199 | loss: 3.0054855MixupTrain:  epoch  0, batch   200 | loss: 3.0346413MixupTrain:  epoch  0, batch   201 | loss: 2.9729893MixupTrain:  epoch  0, batch   202 | loss: 2.9299285MixupTrain:  epoch  0, batch   203 | loss: 3.0461240MixupTrain:  epoch  0, batch   204 | loss: 3.0402648MixupTrain:  epoch  0, batch   205 | loss: 3.3242745MixupTrain:  epoch  0, batch   206 | loss: 2.9525003MixupTrain:  epoch  0, batch   207 | loss: 3.0092058MixupTrain:  epoch  0, batch   208 | loss: 3.0097895MixupTrain:  epoch  0, batch   209 | loss: 3.0972419MixupTrain:  epoch  0, batch   210 | loss: 3.0392327MixupTrain:  epoch  0, batch   211 | loss: 3.0510304MixupTrain:  epoch  0, batch   212 | loss: 3.1066976MixupTrain:  epoch  0, batch   213 | loss: 2.9505138MixupTrain:  epoch  0, batch   214 | loss: 3.0894418MixupTrain:  epoch  0, batch   215 | loss: 2.7368388MixupTrain:  epoch  0, batch   216 | loss: 3.0175934MixupTrain:  epoch  0, batch   217 | loss: 3.0157297MixupTrain:  epoch  0, batch   218 | loss: 3.0778699MixupTrain:  epoch  0, batch   219 | loss: 3.1802373MixupTrain:  epoch  0, batch   220 | loss: 2.7911005MixupTrain:  epoch  0, batch   221 | loss: 2.9176536MixupTrain:  epoch  0, batch   222 | loss: 3.0494094MixupTrain:  epoch  0, batch   223 | loss: 2.9111376MixupTrain:  epoch  0, batch   224 | loss: 3.0544467MixupTrain:  epoch  0, batch   225 | loss: 3.0529304MixupTrain:  epoch  0, batch   226 | loss: 2.9755974MixupTrain:  epoch  0, batch   227 | loss: 3.2540631
MemoryTrain:  epoch  0, batch     0 | loss: 1.4741267MemoryTrain:  epoch  0, batch     1 | loss: 1.7143273MemoryTrain:  epoch  0, batch     2 | loss: 1.7391582MemoryTrain:  epoch  0, batch     3 | loss: 2.2535126MemoryTrain:  epoch  1, batch     0 | loss: 1.3583608MemoryTrain:  epoch  1, batch     1 | loss: 1.5408454MemoryTrain:  epoch  1, batch     2 | loss: 1.4076021MemoryTrain:  epoch  1, batch     3 | loss: 1.4952091MemoryTrain:  epoch  2, batch     0 | loss: 1.3563864MemoryTrain:  epoch  2, batch     1 | loss: 1.2656903MemoryTrain:  epoch  2, batch     2 | loss: 1.3560405MemoryTrain:  epoch  2, batch     3 | loss: 1.3423585MemoryTrain:  epoch  3, batch     0 | loss: 1.3539361MemoryTrain:  epoch  3, batch     1 | loss: 1.2323135MemoryTrain:  epoch  3, batch     2 | loss: 1.2624447MemoryTrain:  epoch  3, batch     3 | loss: 1.3404572MemoryTrain:  epoch  4, batch     0 | loss: 1.2985593MemoryTrain:  epoch  4, batch     1 | loss: 1.2738419MemoryTrain:  epoch  4, batch     2 | loss: 1.3274066MemoryTrain:  epoch  4, batch     3 | loss: 1.3240459MemoryTrain:  epoch  5, batch     0 | loss: 1.3260267MemoryTrain:  epoch  5, batch     1 | loss: 1.3099836MemoryTrain:  epoch  5, batch     2 | loss: 1.2477255MemoryTrain:  epoch  5, batch     3 | loss: 1.3359116MemoryTrain:  epoch  6, batch     0 | loss: 1.2991266MemoryTrain:  epoch  6, batch     1 | loss: 1.2373618MemoryTrain:  epoch  6, batch     2 | loss: 1.2874844MemoryTrain:  epoch  6, batch     3 | loss: 1.2936740MemoryTrain:  epoch  7, batch     0 | loss: 1.2625513MemoryTrain:  epoch  7, batch     1 | loss: 1.2924211MemoryTrain:  epoch  7, batch     2 | loss: 1.2440072MemoryTrain:  epoch  7, batch     3 | loss: 1.1761789MemoryTrain:  epoch  8, batch     0 | loss: 1.2683451MemoryTrain:  epoch  8, batch     1 | loss: 1.2878633MemoryTrain:  epoch  8, batch     2 | loss: 1.2057400MemoryTrain:  epoch  8, batch     3 | loss: 1.2807490MemoryTrain:  epoch  9, batch     0 | loss: 1.2549853MemoryTrain:  epoch  9, batch     1 | loss: 1.2335355MemoryTrain:  epoch  9, batch     2 | loss: 1.3104191MemoryTrain:  epoch  9, batch     3 | loss: 1.2271361
[EVAL] batch:    0 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 96.88%   [EVAL] batch:    2 | acc: 100.00%,  total acc: 97.92%   [EVAL] batch:    3 | acc: 100.00%,  total acc: 98.44%   [EVAL] batch:    4 | acc: 100.00%,  total acc: 98.75%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 98.96%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 99.11%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 99.22%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 97.22%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 93.75%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 93.18%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 93.23%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 91.35%   [EVAL] batch:   13 | acc: 62.50%,  total acc: 89.29%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    2 | acc: 68.75%,  total acc: 72.92%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 71.88%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 72.50%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 72.92%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 75.89%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 78.91%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 81.88%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 83.52%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 83.85%   [EVAL] batch:   12 | acc: 56.25%,  total acc: 81.73%   [EVAL] batch:   13 | acc: 31.25%,  total acc: 78.12%   [EVAL] batch:   14 | acc: 56.25%,  total acc: 76.67%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 75.39%   [EVAL] batch:   16 | acc: 68.75%,  total acc: 75.00%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 74.31%   [EVAL] batch:   18 | acc: 87.50%,  total acc: 75.00%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 75.31%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 76.19%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 76.99%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 77.72%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 78.39%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 79.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 80.05%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 80.56%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 81.90%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 82.08%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 82.66%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 83.01%   [EVAL] batch:   32 | acc: 56.25%,  total acc: 82.20%   [EVAL] batch:   33 | acc: 6.25%,  total acc: 79.96%   [EVAL] batch:   34 | acc: 12.50%,  total acc: 78.04%   [EVAL] batch:   35 | acc: 6.25%,  total acc: 76.04%   [EVAL] batch:   36 | acc: 12.50%,  total acc: 74.32%   [EVAL] batch:   37 | acc: 6.25%,  total acc: 72.53%   [EVAL] batch:   38 | acc: 31.25%,  total acc: 71.47%   [EVAL] batch:   39 | acc: 87.50%,  total acc: 71.88%   [EVAL] batch:   40 | acc: 68.75%,  total acc: 71.80%   [EVAL] batch:   41 | acc: 50.00%,  total acc: 71.28%   [EVAL] batch:   42 | acc: 62.50%,  total acc: 71.08%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 71.31%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 71.94%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 72.55%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 73.14%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 73.70%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 74.23%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 74.75%   [EVAL] batch:   50 | acc: 68.75%,  total acc: 74.63%   [EVAL] batch:   51 | acc: 56.25%,  total acc: 74.28%   [EVAL] batch:   52 | acc: 56.25%,  total acc: 73.94%   [EVAL] batch:   53 | acc: 50.00%,  total acc: 73.50%   [EVAL] batch:   54 | acc: 50.00%,  total acc: 73.07%   [EVAL] batch:   55 | acc: 31.25%,  total acc: 72.32%   [EVAL] batch:   56 | acc: 75.00%,  total acc: 72.37%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 72.74%   [EVAL] batch:   58 | acc: 81.25%,  total acc: 72.88%   [EVAL] batch:   59 | acc: 87.50%,  total acc: 73.12%   [EVAL] batch:   60 | acc: 68.75%,  total acc: 73.05%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 73.39%   [EVAL] batch:   62 | acc: 81.25%,  total acc: 73.51%   [EVAL] batch:   63 | acc: 75.00%,  total acc: 73.54%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 73.94%   [EVAL] batch:   65 | acc: 93.75%,  total acc: 74.24%   [EVAL] batch:   66 | acc: 100.00%,  total acc: 74.63%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 75.36%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 75.71%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 76.06%   [EVAL] batch:   71 | acc: 100.00%,  total acc: 76.39%   [EVAL] batch:   72 | acc: 43.75%,  total acc: 75.94%   [EVAL] batch:   73 | acc: 87.50%,  total acc: 76.10%   [EVAL] batch:   74 | acc: 93.75%,  total acc: 76.33%   [EVAL] batch:   75 | acc: 75.00%,  total acc: 76.32%   [EVAL] batch:   76 | acc: 81.25%,  total acc: 76.38%   [EVAL] batch:   77 | acc: 6.25%,  total acc: 75.48%   
cur_acc:  ['0.8712', '0.8368', '0.6741', '0.8929']
his_acc:  ['0.8712', '0.8475', '0.8076', '0.7548']
CurrentTrain: epoch  0, batch     0 | loss: 3.9763858CurrentTrain: epoch  0, batch     1 | loss: 4.1561441CurrentTrain: epoch  1, batch     0 | loss: 3.0778148CurrentTrain: epoch  1, batch     1 | loss: 2.9444551CurrentTrain: epoch  2, batch     0 | loss: 2.6032858CurrentTrain: epoch  2, batch     1 | loss: 3.4277496CurrentTrain: epoch  3, batch     0 | loss: 3.1889052CurrentTrain: epoch  3, batch     1 | loss: 2.3256853CurrentTrain: epoch  4, batch     0 | loss: 2.4834015CurrentTrain: epoch  4, batch     1 | loss: 2.1301343CurrentTrain: epoch  5, batch     0 | loss: 2.2670946CurrentTrain: epoch  5, batch     1 | loss: 1.9758314CurrentTrain: epoch  6, batch     0 | loss: 2.2428067CurrentTrain: epoch  6, batch     1 | loss: 1.9753560CurrentTrain: epoch  7, batch     0 | loss: 2.0814769CurrentTrain: epoch  7, batch     1 | loss: 1.9346205CurrentTrain: epoch  8, batch     0 | loss: 2.0151453CurrentTrain: epoch  8, batch     1 | loss: 1.9829384CurrentTrain: epoch  9, batch     0 | loss: 1.9298635CurrentTrain: epoch  9, batch     1 | loss: 1.8984829
#### person cause of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person cause of death" means a person was died of an event.
Here is an example:
Relation: person cause of death
Context: mr scheider had suffered from multiple myeloma for several years , and died of complications from a staph infection , his wife , brenda siemer , said .
Head Entity: mr scheider
Tail Entity: complications from a staph infection
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person cause of death":
Relation: person cause of death  
Context: after battling with lung cancer for over a year, john doe passed away peacefully in his sleep, surrounded by family.  
Head Entity: john doe  
Tail Entity: lung cancer  

Relation: person cause of death  
Context: the renowned scientist, dr. emily white, tragically lost her life due to a car accident while returning from a conference.  
Head Entity: dr. emily white  
Tail Entity: car accident  
#### organization political religious affiliation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization political religious affiliation" means an organization is the member of a religion.
Here is an example:
Relation: organization political religious affiliation
Context: the militia has also suffered from allegations of being involved in clashes with rival shiite militias , particularly the badr corp , the military wing linked to supreme iraqi islamic council , the party led by powerful politician abdel aziz al-hakim .
Head Entity: badr corp
Tail Entity: shiite
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization political religious affiliation":
Relation: organization political religious affiliation  
Context: The Christian Democratic Union has been a significant player in German politics, often aligning itself with various religious groups to promote policies that reflect its Christian values.  
Head Entity: Christian Democratic Union  
Tail Entity: Christian  

Relation: organization political religious affiliation  
Context: The Islamic Society of North America has been actively involved in advocating for Muslim rights and has formed alliances with political parties to ensure representation of Islamic values in legislation.  
Head Entity: Islamic Society of North America  
Tail Entity: Muslim  
#### organization stateorprovince of headquarters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization stateorprovince of headquarters" means an organization is located in a state or province.
Here is an example:
Relation: organization stateorprovince of headquarters
Context: andrew lebow , an oil trader with mf global in new york , said investors have been discouraged by lower-than-expected oil imports in china and the disappointing growth in the u.s. economy .
Head Entity: mf global
Tail Entity: new york
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization stateorprovince of headquarters":
Relation: organization stateorprovince of headquarters  
Context: the headquarters of tech giant apple inc. is located in cupertino, california, where it has been a hub for innovation and development.  
Head Entity: apple inc.  
Tail Entity: california  

Relation: organization stateorprovince of headquarters  
Context: the multinational corporation unilever has its main office situated in rotterdam, netherlands, overseeing operations across various countries.  
Head Entity: unilever  
Tail Entity: netherlands  
#### person other family ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person other family" means a person 's relative is a person.
Here is an example:
Relation: person other family
Context: first there was the residents ' contest , in which meredith -lrb- ellen pompeo -rrb- , cristina -lrb- sandra oh -rrb- , alex -lrb- justin chambers -rrb- and izzie -lrb- katherine heigl -rrb- earned points for things like number of sutures and surgeries scrubbed in on .
Head Entity: ellen pompeo
Tail Entity: izzie
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person other family":
Relation: person other family  
Context: During the family reunion, Sarah -lrb- the eldest daughter -rrb- introduced her cousin Jake -lrb- her uncle's son -rrb- to everyone, sharing stories of their childhood adventures together.  
Head Entity: Sarah  
Tail Entity: Jake  

Relation: person other family  
Context: At the wedding, Emily -lrb- the bride -rrb- was surrounded by her siblings, including her brother Tom -lrb- who was the best man -rrb- and her sister Lisa -lrb- who helped with the planning.  
Head Entity: Emily  
Tail Entity: Tom  
#### person city of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person city of death" means a person was died in a city.
Here is an example:
Relation: person city of death
Context: irene morgan kirkaldy , 90 , who died of alzheimer 's disease aug. 10 at her home in gloucester , va. , quietly changed history in 1944 when she refused to give up her seat on a crowded greyhound bus to a white couple .
Head Entity: her
Tail Entity: gloucester
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person city of death":
Relation: person city of death  
Context: john smith, a renowned author, passed away on march 5 in his apartment located in new york city, leaving behind a legacy of literary works that inspired many.  
Head Entity: john smith  
Tail Entity: new york city  

Relation: person city of death  
Context: after a long battle with cancer, elizabeth taylor died on march 23 at a hospital in los angeles, where she had spent her final days surrounded by family and friends.  
Head Entity: elizabeth taylor  
Tail Entity: los angeles  
Mixup data size:  5050
MixupTrain:  epoch  0, batch     0 | loss: 3.8575106MixupTrain:  epoch  0, batch     1 | loss: 3.9809740MixupTrain:  epoch  0, batch     2 | loss: 3.7158222MixupTrain:  epoch  0, batch     3 | loss: 3.8436382MixupTrain:  epoch  0, batch     4 | loss: 3.8056870MixupTrain:  epoch  0, batch     5 | loss: 3.8541467MixupTrain:  epoch  0, batch     6 | loss: 3.5819592MixupTrain:  epoch  0, batch     7 | loss: 3.6516051MixupTrain:  epoch  0, batch     8 | loss: 3.6941419MixupTrain:  epoch  0, batch     9 | loss: 3.3699181MixupTrain:  epoch  0, batch    10 | loss: 3.9388270MixupTrain:  epoch  0, batch    11 | loss: 3.6217890MixupTrain:  epoch  0, batch    12 | loss: 3.3207040MixupTrain:  epoch  0, batch    13 | loss: 3.5413103MixupTrain:  epoch  0, batch    14 | loss: 3.6066871MixupTrain:  epoch  0, batch    15 | loss: 3.5153069MixupTrain:  epoch  0, batch    16 | loss: 3.3343875MixupTrain:  epoch  0, batch    17 | loss: 3.5658741MixupTrain:  epoch  0, batch    18 | loss: 3.3730392MixupTrain:  epoch  0, batch    19 | loss: 3.4709063MixupTrain:  epoch  0, batch    20 | loss: 3.3974643MixupTrain:  epoch  0, batch    21 | loss: 3.9439778MixupTrain:  epoch  0, batch    22 | loss: 3.8082376MixupTrain:  epoch  0, batch    23 | loss: 3.4414892MixupTrain:  epoch  0, batch    24 | loss: 3.6807518MixupTrain:  epoch  0, batch    25 | loss: 3.6055815MixupTrain:  epoch  0, batch    26 | loss: 3.7916105MixupTrain:  epoch  0, batch    27 | loss: 3.3201532MixupTrain:  epoch  0, batch    28 | loss: 3.3910475MixupTrain:  epoch  0, batch    29 | loss: 3.6341968MixupTrain:  epoch  0, batch    30 | loss: 3.2964084MixupTrain:  epoch  0, batch    31 | loss: 3.6597190MixupTrain:  epoch  0, batch    32 | loss: 3.4318528MixupTrain:  epoch  0, batch    33 | loss: 3.4860077MixupTrain:  epoch  0, batch    34 | loss: 3.6049201MixupTrain:  epoch  0, batch    35 | loss: 3.3691933MixupTrain:  epoch  0, batch    36 | loss: 3.5060053MixupTrain:  epoch  0, batch    37 | loss: 3.4209638MixupTrain:  epoch  0, batch    38 | loss: 3.3944216MixupTrain:  epoch  0, batch    39 | loss: 3.3772964MixupTrain:  epoch  0, batch    40 | loss: 3.8831177MixupTrain:  epoch  0, batch    41 | loss: 3.6407998MixupTrain:  epoch  0, batch    42 | loss: 3.2250876MixupTrain:  epoch  0, batch    43 | loss: 3.5437653MixupTrain:  epoch  0, batch    44 | loss: 3.3299685MixupTrain:  epoch  0, batch    45 | loss: 3.2651410MixupTrain:  epoch  0, batch    46 | loss: 3.0335231MixupTrain:  epoch  0, batch    47 | loss: 3.1950128MixupTrain:  epoch  0, batch    48 | loss: 3.3821244MixupTrain:  epoch  0, batch    49 | loss: 3.4783130MixupTrain:  epoch  0, batch    50 | loss: 3.4973707MixupTrain:  epoch  0, batch    51 | loss: 3.3488655MixupTrain:  epoch  0, batch    52 | loss: 3.2840703MixupTrain:  epoch  0, batch    53 | loss: 3.4146054MixupTrain:  epoch  0, batch    54 | loss: 3.1588821MixupTrain:  epoch  0, batch    55 | loss: 3.5042992MixupTrain:  epoch  0, batch    56 | loss: 3.0287261MixupTrain:  epoch  0, batch    57 | loss: 3.3463962MixupTrain:  epoch  0, batch    58 | loss: 3.5217156MixupTrain:  epoch  0, batch    59 | loss: 3.6665323MixupTrain:  epoch  0, batch    60 | loss: 3.1202705MixupTrain:  epoch  0, batch    61 | loss: 3.1100705MixupTrain:  epoch  0, batch    62 | loss: 3.5998302MixupTrain:  epoch  0, batch    63 | loss: 3.2336287MixupTrain:  epoch  0, batch    64 | loss: 3.4461474MixupTrain:  epoch  0, batch    65 | loss: 3.2949259MixupTrain:  epoch  0, batch    66 | loss: 3.3897014MixupTrain:  epoch  0, batch    67 | loss: 3.2066588MixupTrain:  epoch  0, batch    68 | loss: 3.1541965MixupTrain:  epoch  0, batch    69 | loss: 3.2895515MixupTrain:  epoch  0, batch    70 | loss: 3.3030643MixupTrain:  epoch  0, batch    71 | loss: 3.1756346MixupTrain:  epoch  0, batch    72 | loss: 2.7810850MixupTrain:  epoch  0, batch    73 | loss: 3.2858591MixupTrain:  epoch  0, batch    74 | loss: 3.4829736MixupTrain:  epoch  0, batch    75 | loss: 3.4649739MixupTrain:  epoch  0, batch    76 | loss: 3.2339544MixupTrain:  epoch  0, batch    77 | loss: 3.0037644MixupTrain:  epoch  0, batch    78 | loss: 3.0989838MixupTrain:  epoch  0, batch    79 | loss: 3.2746441MixupTrain:  epoch  0, batch    80 | loss: 3.2016215MixupTrain:  epoch  0, batch    81 | loss: 3.2607195MixupTrain:  epoch  0, batch    82 | loss: 3.2762766MixupTrain:  epoch  0, batch    83 | loss: 3.1798768MixupTrain:  epoch  0, batch    84 | loss: 3.3337710MixupTrain:  epoch  0, batch    85 | loss: 3.1002159MixupTrain:  epoch  0, batch    86 | loss: 3.2559915MixupTrain:  epoch  0, batch    87 | loss: 3.2381949MixupTrain:  epoch  0, batch    88 | loss: 3.2324514MixupTrain:  epoch  0, batch    89 | loss: 3.0781891MixupTrain:  epoch  0, batch    90 | loss: 3.2930489MixupTrain:  epoch  0, batch    91 | loss: 3.4448857MixupTrain:  epoch  0, batch    92 | loss: 3.1365633MixupTrain:  epoch  0, batch    93 | loss: 3.1151831MixupTrain:  epoch  0, batch    94 | loss: 3.1920476MixupTrain:  epoch  0, batch    95 | loss: 3.3827686MixupTrain:  epoch  0, batch    96 | loss: 3.1629672MixupTrain:  epoch  0, batch    97 | loss: 3.1977212MixupTrain:  epoch  0, batch    98 | loss: 3.2294283MixupTrain:  epoch  0, batch    99 | loss: 3.2313161MixupTrain:  epoch  0, batch   100 | loss: 3.1008713MixupTrain:  epoch  0, batch   101 | loss: 3.3265922MixupTrain:  epoch  0, batch   102 | loss: 3.2631960MixupTrain:  epoch  0, batch   103 | loss: 3.2389627MixupTrain:  epoch  0, batch   104 | loss: 3.1605003MixupTrain:  epoch  0, batch   105 | loss: 3.1201038MixupTrain:  epoch  0, batch   106 | loss: 3.0402894MixupTrain:  epoch  0, batch   107 | loss: 3.1471093MixupTrain:  epoch  0, batch   108 | loss: 3.0810871MixupTrain:  epoch  0, batch   109 | loss: 3.2220135MixupTrain:  epoch  0, batch   110 | loss: 3.1633451MixupTrain:  epoch  0, batch   111 | loss: 3.1332183MixupTrain:  epoch  0, batch   112 | loss: 3.2691441MixupTrain:  epoch  0, batch   113 | loss: 3.4644921MixupTrain:  epoch  0, batch   114 | loss: 3.0000782MixupTrain:  epoch  0, batch   115 | loss: 3.0418782MixupTrain:  epoch  0, batch   116 | loss: 3.3268781MixupTrain:  epoch  0, batch   117 | loss: 3.0890279MixupTrain:  epoch  0, batch   118 | loss: 3.2819743MixupTrain:  epoch  0, batch   119 | loss: 3.1033397MixupTrain:  epoch  0, batch   120 | loss: 2.9787917MixupTrain:  epoch  0, batch   121 | loss: 2.8995359MixupTrain:  epoch  0, batch   122 | loss: 3.0437794MixupTrain:  epoch  0, batch   123 | loss: 3.0896840MixupTrain:  epoch  0, batch   124 | loss: 3.0957644MixupTrain:  epoch  0, batch   125 | loss: 3.1370575MixupTrain:  epoch  0, batch   126 | loss: 3.1520698MixupTrain:  epoch  0, batch   127 | loss: 3.2211103MixupTrain:  epoch  0, batch   128 | loss: 2.8833835MixupTrain:  epoch  0, batch   129 | loss: 3.2446196MixupTrain:  epoch  0, batch   130 | loss: 3.0627408MixupTrain:  epoch  0, batch   131 | loss: 3.3712733MixupTrain:  epoch  0, batch   132 | loss: 3.2850194MixupTrain:  epoch  0, batch   133 | loss: 3.2023611MixupTrain:  epoch  0, batch   134 | loss: 3.1133380MixupTrain:  epoch  0, batch   135 | loss: 3.1030512MixupTrain:  epoch  0, batch   136 | loss: 3.0174975MixupTrain:  epoch  0, batch   137 | loss: 3.0125735MixupTrain:  epoch  0, batch   138 | loss: 3.1797075MixupTrain:  epoch  0, batch   139 | loss: 3.0108907MixupTrain:  epoch  0, batch   140 | loss: 3.1069422MixupTrain:  epoch  0, batch   141 | loss: 3.0916579MixupTrain:  epoch  0, batch   142 | loss: 3.0315194MixupTrain:  epoch  0, batch   143 | loss: 3.4498260MixupTrain:  epoch  0, batch   144 | loss: 3.1074798MixupTrain:  epoch  0, batch   145 | loss: 2.9713216MixupTrain:  epoch  0, batch   146 | loss: 3.1134212MixupTrain:  epoch  0, batch   147 | loss: 3.1079130MixupTrain:  epoch  0, batch   148 | loss: 2.9905119MixupTrain:  epoch  0, batch   149 | loss: 3.1855686MixupTrain:  epoch  0, batch   150 | loss: 3.0680835MixupTrain:  epoch  0, batch   151 | loss: 3.1718678MixupTrain:  epoch  0, batch   152 | loss: 3.0783529MixupTrain:  epoch  0, batch   153 | loss: 3.0592971MixupTrain:  epoch  0, batch   154 | loss: 2.9722486MixupTrain:  epoch  0, batch   155 | loss: 3.0861521MixupTrain:  epoch  0, batch   156 | loss: 3.1973693MixupTrain:  epoch  0, batch   157 | loss: 2.9871569MixupTrain:  epoch  0, batch   158 | loss: 3.1602669MixupTrain:  epoch  0, batch   159 | loss: 3.1149669MixupTrain:  epoch  0, batch   160 | loss: 3.1245193MixupTrain:  epoch  0, batch   161 | loss: 3.0669780MixupTrain:  epoch  0, batch   162 | loss: 3.1421990MixupTrain:  epoch  0, batch   163 | loss: 3.0187612MixupTrain:  epoch  0, batch   164 | loss: 3.1318545MixupTrain:  epoch  0, batch   165 | loss: 3.1537187MixupTrain:  epoch  0, batch   166 | loss: 3.1548152MixupTrain:  epoch  0, batch   167 | loss: 3.1911521MixupTrain:  epoch  0, batch   168 | loss: 3.1221673MixupTrain:  epoch  0, batch   169 | loss: 3.1426804MixupTrain:  epoch  0, batch   170 | loss: 3.2204828MixupTrain:  epoch  0, batch   171 | loss: 2.8607876MixupTrain:  epoch  0, batch   172 | loss: 3.1027386MixupTrain:  epoch  0, batch   173 | loss: 3.0673606MixupTrain:  epoch  0, batch   174 | loss: 3.0498278MixupTrain:  epoch  0, batch   175 | loss: 3.0934286MixupTrain:  epoch  0, batch   176 | loss: 3.0180235MixupTrain:  epoch  0, batch   177 | loss: 2.8378172MixupTrain:  epoch  0, batch   178 | loss: 3.0778260MixupTrain:  epoch  0, batch   179 | loss: 2.9414148MixupTrain:  epoch  0, batch   180 | loss: 3.2714071MixupTrain:  epoch  0, batch   181 | loss: 2.9695671MixupTrain:  epoch  0, batch   182 | loss: 3.0309706MixupTrain:  epoch  0, batch   183 | loss: 3.1223397MixupTrain:  epoch  0, batch   184 | loss: 2.8884492MixupTrain:  epoch  0, batch   185 | loss: 2.9461167MixupTrain:  epoch  0, batch   186 | loss: 3.1373458MixupTrain:  epoch  0, batch   187 | loss: 3.3175616MixupTrain:  epoch  0, batch   188 | loss: 3.0917437MixupTrain:  epoch  0, batch   189 | loss: 2.8916240MixupTrain:  epoch  0, batch   190 | loss: 3.0431709MixupTrain:  epoch  0, batch   191 | loss: 3.2102680MixupTrain:  epoch  0, batch   192 | loss: 2.9600897MixupTrain:  epoch  0, batch   193 | loss: 3.1790466MixupTrain:  epoch  0, batch   194 | loss: 2.9750803MixupTrain:  epoch  0, batch   195 | loss: 2.9456947MixupTrain:  epoch  0, batch   196 | loss: 2.9501543MixupTrain:  epoch  0, batch   197 | loss: 3.1688728MixupTrain:  epoch  0, batch   198 | loss: 2.9679193MixupTrain:  epoch  0, batch   199 | loss: 3.1979897MixupTrain:  epoch  0, batch   200 | loss: 2.9967546MixupTrain:  epoch  0, batch   201 | loss: 2.9198203MixupTrain:  epoch  0, batch   202 | loss: 2.9412754MixupTrain:  epoch  0, batch   203 | loss: 2.8864765MixupTrain:  epoch  0, batch   204 | loss: 2.8948402MixupTrain:  epoch  0, batch   205 | loss: 3.0060654MixupTrain:  epoch  0, batch   206 | loss: 2.8799887MixupTrain:  epoch  0, batch   207 | loss: 3.0287161MixupTrain:  epoch  0, batch   208 | loss: 3.0661149MixupTrain:  epoch  0, batch   209 | loss: 2.8432291MixupTrain:  epoch  0, batch   210 | loss: 3.0762749MixupTrain:  epoch  0, batch   211 | loss: 3.0518527MixupTrain:  epoch  0, batch   212 | loss: 3.0027082MixupTrain:  epoch  0, batch   213 | loss: 2.8765764MixupTrain:  epoch  0, batch   214 | loss: 2.8287673MixupTrain:  epoch  0, batch   215 | loss: 3.2271707MixupTrain:  epoch  0, batch   216 | loss: 3.1957083MixupTrain:  epoch  0, batch   217 | loss: 3.1075521MixupTrain:  epoch  0, batch   218 | loss: 3.0760880MixupTrain:  epoch  0, batch   219 | loss: 3.0538464MixupTrain:  epoch  0, batch   220 | loss: 3.1448941MixupTrain:  epoch  0, batch   221 | loss: 3.0564065MixupTrain:  epoch  0, batch   222 | loss: 2.9362049MixupTrain:  epoch  0, batch   223 | loss: 2.9996710MixupTrain:  epoch  0, batch   224 | loss: 2.9130211MixupTrain:  epoch  0, batch   225 | loss: 3.0662484MixupTrain:  epoch  0, batch   226 | loss: 2.8444819MixupTrain:  epoch  0, batch   227 | loss: 3.1970687MixupTrain:  epoch  0, batch   228 | loss: 2.9837384MixupTrain:  epoch  0, batch   229 | loss: 3.0094333MixupTrain:  epoch  0, batch   230 | loss: 2.8984137MixupTrain:  epoch  0, batch   231 | loss: 3.0200901MixupTrain:  epoch  0, batch   232 | loss: 3.1322503MixupTrain:  epoch  0, batch   233 | loss: 3.0641296MixupTrain:  epoch  0, batch   234 | loss: 3.0231607MixupTrain:  epoch  0, batch   235 | loss: 3.0251536MixupTrain:  epoch  0, batch   236 | loss: 2.8959074MixupTrain:  epoch  0, batch   237 | loss: 3.0856977MixupTrain:  epoch  0, batch   238 | loss: 2.8896828MixupTrain:  epoch  0, batch   239 | loss: 3.1398416MixupTrain:  epoch  0, batch   240 | loss: 3.0823703MixupTrain:  epoch  0, batch   241 | loss: 2.7565041MixupTrain:  epoch  0, batch   242 | loss: 2.9193039MixupTrain:  epoch  0, batch   243 | loss: 2.8486295MixupTrain:  epoch  0, batch   244 | loss: 3.0472295MixupTrain:  epoch  0, batch   245 | loss: 3.1889381MixupTrain:  epoch  0, batch   246 | loss: 2.9187207MixupTrain:  epoch  0, batch   247 | loss: 2.9088879MixupTrain:  epoch  0, batch   248 | loss: 3.0427792MixupTrain:  epoch  0, batch   249 | loss: 3.1514008MixupTrain:  epoch  0, batch   250 | loss: 2.9042501MixupTrain:  epoch  0, batch   251 | loss: 2.9302588MixupTrain:  epoch  0, batch   252 | loss: 3.0399041MixupTrain:  epoch  0, batch   253 | loss: 2.7879150MixupTrain:  epoch  0, batch   254 | loss: 2.7823124MixupTrain:  epoch  0, batch   255 | loss: 2.8938603MixupTrain:  epoch  0, batch   256 | loss: 2.9419491MixupTrain:  epoch  0, batch   257 | loss: 3.0883913MixupTrain:  epoch  0, batch   258 | loss: 3.1052308MixupTrain:  epoch  0, batch   259 | loss: 3.0378065MixupTrain:  epoch  0, batch   260 | loss: 2.9332647MixupTrain:  epoch  0, batch   261 | loss: 3.2039135MixupTrain:  epoch  0, batch   262 | loss: 2.9383802MixupTrain:  epoch  0, batch   263 | loss: 3.0145590MixupTrain:  epoch  0, batch   264 | loss: 3.0649972MixupTrain:  epoch  0, batch   265 | loss: 3.0802453MixupTrain:  epoch  0, batch   266 | loss: 3.0829487MixupTrain:  epoch  0, batch   267 | loss: 2.8321235MixupTrain:  epoch  0, batch   268 | loss: 2.9397073MixupTrain:  epoch  0, batch   269 | loss: 3.0538940MixupTrain:  epoch  0, batch   270 | loss: 3.0292706MixupTrain:  epoch  0, batch   271 | loss: 3.0180526MixupTrain:  epoch  0, batch   272 | loss: 2.9613395MixupTrain:  epoch  0, batch   273 | loss: 3.1250730MixupTrain:  epoch  0, batch   274 | loss: 2.9605522MixupTrain:  epoch  0, batch   275 | loss: 3.1376572MixupTrain:  epoch  0, batch   276 | loss: 3.1004627MixupTrain:  epoch  0, batch   277 | loss: 3.1110029MixupTrain:  epoch  0, batch   278 | loss: 3.0230210MixupTrain:  epoch  0, batch   279 | loss: 2.9590731MixupTrain:  epoch  0, batch   280 | loss: 2.8955221MixupTrain:  epoch  0, batch   281 | loss: 3.0514379MixupTrain:  epoch  0, batch   282 | loss: 3.0940113MixupTrain:  epoch  0, batch   283 | loss: 3.0692854MixupTrain:  epoch  0, batch   284 | loss: 2.8342061MixupTrain:  epoch  0, batch   285 | loss: 3.0519428MixupTrain:  epoch  0, batch   286 | loss: 3.0144339MixupTrain:  epoch  0, batch   287 | loss: 3.0633445MixupTrain:  epoch  0, batch   288 | loss: 2.9199777MixupTrain:  epoch  0, batch   289 | loss: 3.2110701MixupTrain:  epoch  0, batch   290 | loss: 3.0556741MixupTrain:  epoch  0, batch   291 | loss: 2.9539745MixupTrain:  epoch  0, batch   292 | loss: 3.0786500MixupTrain:  epoch  0, batch   293 | loss: 2.8674157MixupTrain:  epoch  0, batch   294 | loss: 3.0218322MixupTrain:  epoch  0, batch   295 | loss: 2.9475169MixupTrain:  epoch  0, batch   296 | loss: 2.9424591MixupTrain:  epoch  0, batch   297 | loss: 2.9292998MixupTrain:  epoch  0, batch   298 | loss: 3.0319114MixupTrain:  epoch  0, batch   299 | loss: 2.9580822MixupTrain:  epoch  0, batch   300 | loss: 2.9633756MixupTrain:  epoch  0, batch   301 | loss: 3.0186975MixupTrain:  epoch  0, batch   302 | loss: 3.0205894MixupTrain:  epoch  0, batch   303 | loss: 2.9117010MixupTrain:  epoch  0, batch   304 | loss: 2.8790977MixupTrain:  epoch  0, batch   305 | loss: 2.8169994MixupTrain:  epoch  0, batch   306 | loss: 3.0353651MixupTrain:  epoch  0, batch   307 | loss: 2.9239414MixupTrain:  epoch  0, batch   308 | loss: 3.0532746MixupTrain:  epoch  0, batch   309 | loss: 3.0794516MixupTrain:  epoch  0, batch   310 | loss: 2.7782350MixupTrain:  epoch  0, batch   311 | loss: 3.0874071MixupTrain:  epoch  0, batch   312 | loss: 3.0015039MixupTrain:  epoch  0, batch   313 | loss: 2.9321337MixupTrain:  epoch  0, batch   314 | loss: 3.0717440MixupTrain:  epoch  0, batch   315 | loss: 2.8657448
MemoryTrain:  epoch  0, batch     0 | loss: 1.4081578MemoryTrain:  epoch  0, batch     1 | loss: 1.4762726MemoryTrain:  epoch  0, batch     2 | loss: 1.6333543MemoryTrain:  epoch  0, batch     3 | loss: 1.9903809MemoryTrain:  epoch  0, batch     4 | loss: 1.7755828MemoryTrain:  epoch  1, batch     0 | loss: 1.3704281MemoryTrain:  epoch  1, batch     1 | loss: 1.4950832MemoryTrain:  epoch  1, batch     2 | loss: 1.3651421MemoryTrain:  epoch  1, batch     3 | loss: 1.2506993MemoryTrain:  epoch  1, batch     4 | loss: 1.3630700MemoryTrain:  epoch  2, batch     0 | loss: 1.3183432MemoryTrain:  epoch  2, batch     1 | loss: 1.3247766MemoryTrain:  epoch  2, batch     2 | loss: 1.3066864MemoryTrain:  epoch  2, batch     3 | loss: 1.3032504MemoryTrain:  epoch  2, batch     4 | loss: 1.2744696MemoryTrain:  epoch  3, batch     0 | loss: 1.3172739MemoryTrain:  epoch  3, batch     1 | loss: 1.3212725MemoryTrain:  epoch  3, batch     2 | loss: 1.2654043MemoryTrain:  epoch  3, batch     3 | loss: 1.2736866MemoryTrain:  epoch  3, batch     4 | loss: 1.3203340MemoryTrain:  epoch  4, batch     0 | loss: 1.2962861MemoryTrain:  epoch  4, batch     1 | loss: 1.3078362MemoryTrain:  epoch  4, batch     2 | loss: 1.2673236MemoryTrain:  epoch  4, batch     3 | loss: 1.2365837MemoryTrain:  epoch  4, batch     4 | loss: 1.2308061MemoryTrain:  epoch  5, batch     0 | loss: 1.2537253MemoryTrain:  epoch  5, batch     1 | loss: 1.2623632MemoryTrain:  epoch  5, batch     2 | loss: 1.2613516MemoryTrain:  epoch  5, batch     3 | loss: 1.2719007MemoryTrain:  epoch  5, batch     4 | loss: 1.2034878MemoryTrain:  epoch  6, batch     0 | loss: 1.2534318MemoryTrain:  epoch  6, batch     1 | loss: 1.2447615MemoryTrain:  epoch  6, batch     2 | loss: 1.2743109MemoryTrain:  epoch  6, batch     3 | loss: 1.2191145MemoryTrain:  epoch  6, batch     4 | loss: 1.3031125MemoryTrain:  epoch  7, batch     0 | loss: 1.2332335MemoryTrain:  epoch  7, batch     1 | loss: 1.2197064MemoryTrain:  epoch  7, batch     2 | loss: 1.2608336MemoryTrain:  epoch  7, batch     3 | loss: 1.3013891MemoryTrain:  epoch  7, batch     4 | loss: 1.2118163MemoryTrain:  epoch  8, batch     0 | loss: 1.1955355MemoryTrain:  epoch  8, batch     1 | loss: 1.2441351MemoryTrain:  epoch  8, batch     2 | loss: 1.2096342MemoryTrain:  epoch  8, batch     3 | loss: 1.1729536MemoryTrain:  epoch  8, batch     4 | loss: 1.2312323MemoryTrain:  epoch  9, batch     0 | loss: 1.1883314MemoryTrain:  epoch  9, batch     1 | loss: 1.2683046MemoryTrain:  epoch  9, batch     2 | loss: 1.2344069MemoryTrain:  epoch  9, batch     3 | loss: 1.1973242MemoryTrain:  epoch  9, batch     4 | loss: 1.2272137
[EVAL] batch:    0 | acc: 37.50%,  total acc: 37.50%   [EVAL] batch:    1 | acc: 62.50%,  total acc: 50.00%   [EVAL] batch:    2 | acc: 50.00%,  total acc: 50.00%   [EVAL] batch:    3 | acc: 62.50%,  total acc: 53.12%   [EVAL] batch:    4 | acc: 100.00%,  total acc: 62.50%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 67.71%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 72.32%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 74.22%   [EVAL] batch:    8 | acc: 25.00%,  total acc: 68.75%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 69.38%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 70.45%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 71.88%   [EVAL] batch:   12 | acc: 50.00%,  total acc: 70.19%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 62.50%,  total acc: 68.75%   [EVAL] batch:    2 | acc: 62.50%,  total acc: 66.67%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 67.19%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 67.50%   [EVAL] batch:    5 | acc: 68.75%,  total acc: 67.71%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 71.43%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 77.78%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 79.38%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 81.77%   [EVAL] batch:   12 | acc: 50.00%,  total acc: 79.33%   [EVAL] batch:   13 | acc: 31.25%,  total acc: 75.89%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 75.42%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 74.22%   [EVAL] batch:   16 | acc: 68.75%,  total acc: 73.90%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 73.26%   [EVAL] batch:   18 | acc: 87.50%,  total acc: 74.01%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 75.00%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 75.89%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 76.70%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 77.45%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 78.12%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 79.00%   [EVAL] batch:   25 | acc: 93.75%,  total acc: 79.57%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 80.09%   [EVAL] batch:   27 | acc: 81.25%,  total acc: 80.13%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 80.60%   [EVAL] batch:   29 | acc: 75.00%,  total acc: 80.42%   [EVAL] batch:   30 | acc: 81.25%,  total acc: 80.44%   [EVAL] batch:   31 | acc: 75.00%,  total acc: 80.27%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 78.98%   [EVAL] batch:   33 | acc: 0.00%,  total acc: 76.65%   [EVAL] batch:   34 | acc: 0.00%,  total acc: 74.46%   [EVAL] batch:   35 | acc: 0.00%,  total acc: 72.40%   [EVAL] batch:   36 | acc: 0.00%,  total acc: 70.44%   [EVAL] batch:   37 | acc: 0.00%,  total acc: 68.59%   [EVAL] batch:   38 | acc: 25.00%,  total acc: 67.47%   [EVAL] batch:   39 | acc: 87.50%,  total acc: 67.97%   [EVAL] batch:   40 | acc: 75.00%,  total acc: 68.14%   [EVAL] batch:   41 | acc: 25.00%,  total acc: 67.11%   [EVAL] batch:   42 | acc: 37.50%,  total acc: 66.42%   [EVAL] batch:   43 | acc: 68.75%,  total acc: 66.48%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 67.22%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 67.93%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 68.62%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 69.27%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 69.90%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 70.50%   [EVAL] batch:   50 | acc: 56.25%,  total acc: 70.22%   [EVAL] batch:   51 | acc: 25.00%,  total acc: 69.35%   [EVAL] batch:   52 | acc: 12.50%,  total acc: 68.28%   [EVAL] batch:   53 | acc: 0.00%,  total acc: 67.01%   [EVAL] batch:   54 | acc: 6.25%,  total acc: 65.91%   [EVAL] batch:   55 | acc: 0.00%,  total acc: 64.73%   [EVAL] batch:   56 | acc: 62.50%,  total acc: 64.69%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 65.19%   [EVAL] batch:   58 | acc: 81.25%,  total acc: 65.47%   [EVAL] batch:   59 | acc: 87.50%,  total acc: 65.83%   [EVAL] batch:   60 | acc: 75.00%,  total acc: 65.98%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 66.33%   [EVAL] batch:   62 | acc: 56.25%,  total acc: 66.17%   [EVAL] batch:   63 | acc: 68.75%,  total acc: 66.21%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 66.73%   [EVAL] batch:   65 | acc: 93.75%,  total acc: 67.14%   [EVAL] batch:   66 | acc: 100.00%,  total acc: 67.63%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 68.11%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 68.57%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 69.02%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 69.45%   [EVAL] batch:   71 | acc: 93.75%,  total acc: 69.79%   [EVAL] batch:   72 | acc: 50.00%,  total acc: 69.52%   [EVAL] batch:   73 | acc: 75.00%,  total acc: 69.59%   [EVAL] batch:   74 | acc: 56.25%,  total acc: 69.42%   [EVAL] batch:   75 | acc: 62.50%,  total acc: 69.33%   [EVAL] batch:   76 | acc: 43.75%,  total acc: 68.99%   [EVAL] batch:   77 | acc: 31.25%,  total acc: 68.51%   [EVAL] batch:   78 | acc: 62.50%,  total acc: 68.43%   [EVAL] batch:   79 | acc: 50.00%,  total acc: 68.20%   [EVAL] batch:   80 | acc: 62.50%,  total acc: 68.13%   [EVAL] batch:   81 | acc: 100.00%,  total acc: 68.52%   [EVAL] batch:   82 | acc: 93.75%,  total acc: 68.83%   [EVAL] batch:   83 | acc: 100.00%,  total acc: 69.20%   [EVAL] batch:   84 | acc: 87.50%,  total acc: 69.41%   [EVAL] batch:   85 | acc: 31.25%,  total acc: 68.97%   [EVAL] batch:   86 | acc: 75.00%,  total acc: 69.04%   [EVAL] batch:   87 | acc: 75.00%,  total acc: 69.11%   [EVAL] batch:   88 | acc: 87.50%,  total acc: 69.31%   [EVAL] batch:   89 | acc: 56.25%,  total acc: 69.17%   
cur_acc:  ['0.8712', '0.8368', '0.6741', '0.8929', '0.7019']
his_acc:  ['0.8712', '0.8475', '0.8076', '0.7548', '0.6917']
CurrentTrain: epoch  0, batch     0 | loss: 5.8670058CurrentTrain: epoch  0, batch     1 | loss: 7.4454684CurrentTrain: epoch  1, batch     0 | loss: 5.0615511CurrentTrain: epoch  1, batch     1 | loss: 5.5886793CurrentTrain: epoch  2, batch     0 | loss: 4.5262241CurrentTrain: epoch  2, batch     1 | loss: 3.7995398CurrentTrain: epoch  3, batch     0 | loss: 3.9842591CurrentTrain: epoch  3, batch     1 | loss: 3.3363283CurrentTrain: epoch  4, batch     0 | loss: 3.5706141CurrentTrain: epoch  4, batch     1 | loss: 3.6566095CurrentTrain: epoch  5, batch     0 | loss: 3.5202260CurrentTrain: epoch  5, batch     1 | loss: 2.6511028CurrentTrain: epoch  6, batch     0 | loss: 3.1336026CurrentTrain: epoch  6, batch     1 | loss: 2.6699696CurrentTrain: epoch  7, batch     0 | loss: 2.6739774CurrentTrain: epoch  7, batch     1 | loss: 2.6127896CurrentTrain: epoch  8, batch     0 | loss: 2.9301429CurrentTrain: epoch  8, batch     1 | loss: 2.3675029CurrentTrain: epoch  9, batch     0 | loss: 2.6158271CurrentTrain: epoch  9, batch     1 | loss: 2.2640362
#### person stateorprovinces of residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person stateorprovinces of residence" means a person is living in a state or province.
Here is an example:
Relation: person stateorprovinces of residence
Context: the chairman of the senate foreign relations committee , massachusetts democrat john kerry , and the panel 's top republican , richard lugar of indiana , were at the white house meeting , which was led by vice president joe biden , a former chairman of the foreign relations panel .
Head Entity: john kerry
Tail Entity: massachusetts
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person stateorprovinces of residence":
Relation: person stateorprovinces of residence  
Context: After moving from New York to California, actress Emma Stone quickly adapted to the vibrant lifestyle of Los Angeles, where she now resides.  
Head Entity: Emma Stone  
Tail Entity: California  

Relation: person stateorprovinces of residence  
Context: Following his successful career in the tech industry, Mark Zuckerberg decided to settle in Palo Alto, a city known for its innovation and proximity to Silicon Valley.  
Head Entity: Mark Zuckerberg  
Tail Entity: Palo Alto  
#### person date of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person date of death" means a person was died on a date.
Here is an example:
Relation: person date of death
Context: he was taken off life support on feb. 14 .
Head Entity: he
Tail Entity: feb. 14
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person date of death":
Relation: person date of death  
Context: The renowned author passed away on July 20, 2021, after a long illness.  
Head Entity: The renowned author  
Tail Entity: July 20, 2021  

Relation: person date of death  
Context: She left this world peacefully in her sleep on March 5, 2019.  
Head Entity: She  
Tail Entity: March 5, 2019  
#### organization number of employees members ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization number of employees members" means the number of an organization 's employees is a number.
Here is an example:
Relation: organization number of employees members
Context: with the sweep of a federal regulator 's pen , massachusetts stands to gain a new life-science giant in april : covidien , a medical - supplies maker with thousands of products and more than 43,000 employees worldwide .
Head Entity: covidien
Tail Entity: 43,000
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization number of employees members":
Relation: organization number of employees members  
Context: The tech company, Innovatech, has rapidly expanded its workforce over the past year, now boasting a total of 25,000 employees across its global offices.  
Head Entity: Innovatech  
Tail Entity: 25,000  

Relation: organization number of employees members  
Context: After the merger, the newly formed entity, Global Finance Corp, reported an impressive headcount of 15,000 employees, making it one of the largest firms in the industry.  
Head Entity: Global Finance Corp  
Tail Entity: 15,000  
#### person alternate names ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person alternate names" means a person 's alias is a person.
Here is an example:
Relation: person alternate names
Context: a judge in new york city said remy ma , whose real name is remy smith , said thursday that the hip-hopper could not leave the united states for a five-country european concert tour .
Head Entity: remy smith
Tail Entity: remy ma
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person alternate names":
Relation: person alternate names  
Context: The famous author J.K. Rowling, known for her pen name Robert Galbraith, released a new mystery novel this week.  
Head Entity: Robert Galbraith  
Tail Entity: J.K. Rowling  

Relation: person alternate names  
Context: The musician known as Lady Gaga was born Stefani Germanotta, and she has become a global icon in the pop music industry.  
Head Entity: Stefani Germanotta  
Tail Entity: Lady Gaga  
#### person spouse ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person spouse" means a person 's spouse is a person.
Here is an example:
Relation: person spouse
Context: in addition to his wife , meskill is survived by two daughters , eileen gallup of new britain and maureen heneghan of haddon heights , n.j. ; three sons , john , of kensington , conn. ; peter , of east hartford , conn. ; and thomas , of branford , conn. ; two sisters , ruth prior of naples , fla. , and sister laura marie of portland , conn. ; five grandchildren , and two step-grandchildren .
Head Entity: his
Tail Entity: meskill
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person spouse":
Relation: person spouse  
Context: after a long and happy marriage, john and his wife, sarah, decided to celebrate their 50th wedding anniversary with family and friends.  
Head Entity: his  
Tail Entity: wife  

Relation: person spouse  
Context: during the ceremony, emily was seen smiling at her husband, mark, as they exchanged vows in front of their loved ones.  
Head Entity: her  
Tail Entity: husband  
Mixup data size:  6685
MixupTrain:  epoch  0, batch     0 | loss: 3.7404208MixupTrain:  epoch  0, batch     1 | loss: 3.4825032MixupTrain:  epoch  0, batch     2 | loss: 3.3386221MixupTrain:  epoch  0, batch     3 | loss: 3.6199067MixupTrain:  epoch  0, batch     4 | loss: 3.4831238MixupTrain:  epoch  0, batch     5 | loss: 3.4898353MixupTrain:  epoch  0, batch     6 | loss: 3.5863118MixupTrain:  epoch  0, batch     7 | loss: 3.6468692MixupTrain:  epoch  0, batch     8 | loss: 3.5772645MixupTrain:  epoch  0, batch     9 | loss: 4.1337109MixupTrain:  epoch  0, batch    10 | loss: 3.4875536MixupTrain:  epoch  0, batch    11 | loss: 3.1221023MixupTrain:  epoch  0, batch    12 | loss: 3.0715566MixupTrain:  epoch  0, batch    13 | loss: 3.6915407MixupTrain:  epoch  0, batch    14 | loss: 3.3255837MixupTrain:  epoch  0, batch    15 | loss: 3.5106025MixupTrain:  epoch  0, batch    16 | loss: 3.2283955MixupTrain:  epoch  0, batch    17 | loss: 3.3124871MixupTrain:  epoch  0, batch    18 | loss: 3.2377028MixupTrain:  epoch  0, batch    19 | loss: 3.2242165MixupTrain:  epoch  0, batch    20 | loss: 3.0981774MixupTrain:  epoch  0, batch    21 | loss: 3.3850830MixupTrain:  epoch  0, batch    22 | loss: 3.2911263MixupTrain:  epoch  0, batch    23 | loss: 3.1296382MixupTrain:  epoch  0, batch    24 | loss: 3.3140917MixupTrain:  epoch  0, batch    25 | loss: 2.9955688MixupTrain:  epoch  0, batch    26 | loss: 3.1284325MixupTrain:  epoch  0, batch    27 | loss: 3.3617237MixupTrain:  epoch  0, batch    28 | loss: 3.2392499MixupTrain:  epoch  0, batch    29 | loss: 3.2972131MixupTrain:  epoch  0, batch    30 | loss: 3.1669149MixupTrain:  epoch  0, batch    31 | loss: 3.0764523MixupTrain:  epoch  0, batch    32 | loss: 3.0651863MixupTrain:  epoch  0, batch    33 | loss: 3.0079322MixupTrain:  epoch  0, batch    34 | loss: 3.0560665MixupTrain:  epoch  0, batch    35 | loss: 2.9889855MixupTrain:  epoch  0, batch    36 | loss: 3.4761946MixupTrain:  epoch  0, batch    37 | loss: 3.2878284MixupTrain:  epoch  0, batch    38 | loss: 3.3608725MixupTrain:  epoch  0, batch    39 | loss: 3.0554857MixupTrain:  epoch  0, batch    40 | loss: 3.3001037MixupTrain:  epoch  0, batch    41 | loss: 2.9617815MixupTrain:  epoch  0, batch    42 | loss: 3.3015189MixupTrain:  epoch  0, batch    43 | loss: 3.1568632MixupTrain:  epoch  0, batch    44 | loss: 3.1452699MixupTrain:  epoch  0, batch    45 | loss: 2.8774900MixupTrain:  epoch  0, batch    46 | loss: 3.1373785MixupTrain:  epoch  0, batch    47 | loss: 3.0807827MixupTrain:  epoch  0, batch    48 | loss: 3.1192269MixupTrain:  epoch  0, batch    49 | loss: 3.0946019MixupTrain:  epoch  0, batch    50 | loss: 3.0504751MixupTrain:  epoch  0, batch    51 | loss: 2.9650617MixupTrain:  epoch  0, batch    52 | loss: 2.9945030MixupTrain:  epoch  0, batch    53 | loss: 3.4147880MixupTrain:  epoch  0, batch    54 | loss: 2.8995876MixupTrain:  epoch  0, batch    55 | loss: 3.0688710MixupTrain:  epoch  0, batch    56 | loss: 3.1940475MixupTrain:  epoch  0, batch    57 | loss: 3.0713720MixupTrain:  epoch  0, batch    58 | loss: 2.9868076MixupTrain:  epoch  0, batch    59 | loss: 2.9132214MixupTrain:  epoch  0, batch    60 | loss: 2.9132540MixupTrain:  epoch  0, batch    61 | loss: 2.9619365MixupTrain:  epoch  0, batch    62 | loss: 3.0827384MixupTrain:  epoch  0, batch    63 | loss: 2.8809633MixupTrain:  epoch  0, batch    64 | loss: 2.9719710MixupTrain:  epoch  0, batch    65 | loss: 3.1807494MixupTrain:  epoch  0, batch    66 | loss: 2.8150754MixupTrain:  epoch  0, batch    67 | loss: 3.2336302MixupTrain:  epoch  0, batch    68 | loss: 3.0860112MixupTrain:  epoch  0, batch    69 | loss: 3.0527015MixupTrain:  epoch  0, batch    70 | loss: 3.0977328MixupTrain:  epoch  0, batch    71 | loss: 3.1382957MixupTrain:  epoch  0, batch    72 | loss: 3.0284777MixupTrain:  epoch  0, batch    73 | loss: 3.0875776MixupTrain:  epoch  0, batch    74 | loss: 2.9537568MixupTrain:  epoch  0, batch    75 | loss: 2.9962358MixupTrain:  epoch  0, batch    76 | loss: 3.0555308MixupTrain:  epoch  0, batch    77 | loss: 3.0127392MixupTrain:  epoch  0, batch    78 | loss: 3.2346315MixupTrain:  epoch  0, batch    79 | loss: 3.0131736MixupTrain:  epoch  0, batch    80 | loss: 2.8722944MixupTrain:  epoch  0, batch    81 | loss: 3.0666251MixupTrain:  epoch  0, batch    82 | loss: 2.9968691MixupTrain:  epoch  0, batch    83 | loss: 3.0263789MixupTrain:  epoch  0, batch    84 | loss: 2.9260645MixupTrain:  epoch  0, batch    85 | loss: 3.0725303MixupTrain:  epoch  0, batch    86 | loss: 2.9493399MixupTrain:  epoch  0, batch    87 | loss: 2.9797232MixupTrain:  epoch  0, batch    88 | loss: 3.1247096MixupTrain:  epoch  0, batch    89 | loss: 2.7809329MixupTrain:  epoch  0, batch    90 | loss: 3.0136995MixupTrain:  epoch  0, batch    91 | loss: 3.1220746MixupTrain:  epoch  0, batch    92 | loss: 2.8022189MixupTrain:  epoch  0, batch    93 | loss: 2.9002514MixupTrain:  epoch  0, batch    94 | loss: 3.0215139MixupTrain:  epoch  0, batch    95 | loss: 2.8843076MixupTrain:  epoch  0, batch    96 | loss: 2.8621349MixupTrain:  epoch  0, batch    97 | loss: 3.0170178MixupTrain:  epoch  0, batch    98 | loss: 2.9072161MixupTrain:  epoch  0, batch    99 | loss: 3.0111904MixupTrain:  epoch  0, batch   100 | loss: 2.9632077MixupTrain:  epoch  0, batch   101 | loss: 2.8108191MixupTrain:  epoch  0, batch   102 | loss: 2.9469831MixupTrain:  epoch  0, batch   103 | loss: 2.9764791MixupTrain:  epoch  0, batch   104 | loss: 3.0310528MixupTrain:  epoch  0, batch   105 | loss: 2.8680959MixupTrain:  epoch  0, batch   106 | loss: 2.9439354MixupTrain:  epoch  0, batch   107 | loss: 2.8905764MixupTrain:  epoch  0, batch   108 | loss: 2.9800687MixupTrain:  epoch  0, batch   109 | loss: 3.0157492MixupTrain:  epoch  0, batch   110 | loss: 2.8821154MixupTrain:  epoch  0, batch   111 | loss: 2.8287091MixupTrain:  epoch  0, batch   112 | loss: 2.8843260MixupTrain:  epoch  0, batch   113 | loss: 3.0665064MixupTrain:  epoch  0, batch   114 | loss: 3.0074971MixupTrain:  epoch  0, batch   115 | loss: 2.7578106MixupTrain:  epoch  0, batch   116 | loss: 2.7370403MixupTrain:  epoch  0, batch   117 | loss: 2.6684999MixupTrain:  epoch  0, batch   118 | loss: 2.9802558MixupTrain:  epoch  0, batch   119 | loss: 2.8728914MixupTrain:  epoch  0, batch   120 | loss: 2.9618032MixupTrain:  epoch  0, batch   121 | loss: 2.9470730MixupTrain:  epoch  0, batch   122 | loss: 2.9628124MixupTrain:  epoch  0, batch   123 | loss: 2.9386654MixupTrain:  epoch  0, batch   124 | loss: 3.0467594MixupTrain:  epoch  0, batch   125 | loss: 3.1259007MixupTrain:  epoch  0, batch   126 | loss: 2.9381709MixupTrain:  epoch  0, batch   127 | loss: 2.9408145MixupTrain:  epoch  0, batch   128 | loss: 3.0346417MixupTrain:  epoch  0, batch   129 | loss: 3.0180345MixupTrain:  epoch  0, batch   130 | loss: 2.7769146MixupTrain:  epoch  0, batch   131 | loss: 2.9963799MixupTrain:  epoch  0, batch   132 | loss: 3.0444446MixupTrain:  epoch  0, batch   133 | loss: 2.7970943MixupTrain:  epoch  0, batch   134 | loss: 2.9416990MixupTrain:  epoch  0, batch   135 | loss: 2.9422531MixupTrain:  epoch  0, batch   136 | loss: 2.8431878MixupTrain:  epoch  0, batch   137 | loss: 2.8623486MixupTrain:  epoch  0, batch   138 | loss: 2.9522111MixupTrain:  epoch  0, batch   139 | loss: 2.9632280MixupTrain:  epoch  0, batch   140 | loss: 2.9632349MixupTrain:  epoch  0, batch   141 | loss: 2.7604787MixupTrain:  epoch  0, batch   142 | loss: 2.8417110MixupTrain:  epoch  0, batch   143 | loss: 2.9996185MixupTrain:  epoch  0, batch   144 | loss: 2.9306192MixupTrain:  epoch  0, batch   145 | loss: 2.9943821MixupTrain:  epoch  0, batch   146 | loss: 2.7831507MixupTrain:  epoch  0, batch   147 | loss: 2.8868461MixupTrain:  epoch  0, batch   148 | loss: 2.7561460MixupTrain:  epoch  0, batch   149 | loss: 2.7761097MixupTrain:  epoch  0, batch   150 | loss: 2.9838693MixupTrain:  epoch  0, batch   151 | loss: 2.8885322MixupTrain:  epoch  0, batch   152 | loss: 2.8685198MixupTrain:  epoch  0, batch   153 | loss: 2.8661056MixupTrain:  epoch  0, batch   154 | loss: 2.9215126MixupTrain:  epoch  0, batch   155 | loss: 2.9106174MixupTrain:  epoch  0, batch   156 | loss: 2.8996739MixupTrain:  epoch  0, batch   157 | loss: 2.8647974MixupTrain:  epoch  0, batch   158 | loss: 2.9877286MixupTrain:  epoch  0, batch   159 | loss: 2.8835831MixupTrain:  epoch  0, batch   160 | loss: 3.0023921MixupTrain:  epoch  0, batch   161 | loss: 2.9687130MixupTrain:  epoch  0, batch   162 | loss: 2.8324103MixupTrain:  epoch  0, batch   163 | loss: 2.9577227MixupTrain:  epoch  0, batch   164 | loss: 2.8940592MixupTrain:  epoch  0, batch   165 | loss: 2.8077714MixupTrain:  epoch  0, batch   166 | loss: 2.9161935MixupTrain:  epoch  0, batch   167 | loss: 2.9006538MixupTrain:  epoch  0, batch   168 | loss: 2.9960020MixupTrain:  epoch  0, batch   169 | loss: 2.9437776MixupTrain:  epoch  0, batch   170 | loss: 2.9161539MixupTrain:  epoch  0, batch   171 | loss: 2.9119608MixupTrain:  epoch  0, batch   172 | loss: 2.9219747MixupTrain:  epoch  0, batch   173 | loss: 2.8950610MixupTrain:  epoch  0, batch   174 | loss: 2.8946133MixupTrain:  epoch  0, batch   175 | loss: 2.8727198MixupTrain:  epoch  0, batch   176 | loss: 2.9612126MixupTrain:  epoch  0, batch   177 | loss: 2.8686545MixupTrain:  epoch  0, batch   178 | loss: 3.0167994MixupTrain:  epoch  0, batch   179 | loss: 2.8160443MixupTrain:  epoch  0, batch   180 | loss: 2.9706697MixupTrain:  epoch  0, batch   181 | loss: 2.8458717MixupTrain:  epoch  0, batch   182 | loss: 2.7516921MixupTrain:  epoch  0, batch   183 | loss: 2.7170208MixupTrain:  epoch  0, batch   184 | loss: 2.7644868MixupTrain:  epoch  0, batch   185 | loss: 2.9790525MixupTrain:  epoch  0, batch   186 | loss: 2.9390776MixupTrain:  epoch  0, batch   187 | loss: 2.9465575MixupTrain:  epoch  0, batch   188 | loss: 2.8164668MixupTrain:  epoch  0, batch   189 | loss: 2.8231587MixupTrain:  epoch  0, batch   190 | loss: 2.9704084MixupTrain:  epoch  0, batch   191 | loss: 2.8337121MixupTrain:  epoch  0, batch   192 | loss: 2.7820511MixupTrain:  epoch  0, batch   193 | loss: 2.8031595MixupTrain:  epoch  0, batch   194 | loss: 2.8044410MixupTrain:  epoch  0, batch   195 | loss: 2.9147861MixupTrain:  epoch  0, batch   196 | loss: 2.8289170MixupTrain:  epoch  0, batch   197 | loss: 2.7973809MixupTrain:  epoch  0, batch   198 | loss: 2.9261701MixupTrain:  epoch  0, batch   199 | loss: 2.8370438MixupTrain:  epoch  0, batch   200 | loss: 2.6917014MixupTrain:  epoch  0, batch   201 | loss: 2.9341059MixupTrain:  epoch  0, batch   202 | loss: 2.9622226MixupTrain:  epoch  0, batch   203 | loss: 2.8731949MixupTrain:  epoch  0, batch   204 | loss: 2.9043176MixupTrain:  epoch  0, batch   205 | loss: 2.8680778MixupTrain:  epoch  0, batch   206 | loss: 2.7849181MixupTrain:  epoch  0, batch   207 | loss: 3.0228846MixupTrain:  epoch  0, batch   208 | loss: 2.8141460MixupTrain:  epoch  0, batch   209 | loss: 2.8009205MixupTrain:  epoch  0, batch   210 | loss: 2.8116956MixupTrain:  epoch  0, batch   211 | loss: 2.9927568MixupTrain:  epoch  0, batch   212 | loss: 2.7913737MixupTrain:  epoch  0, batch   213 | loss: 2.7605081MixupTrain:  epoch  0, batch   214 | loss: 2.7753005MixupTrain:  epoch  0, batch   215 | loss: 2.8666482MixupTrain:  epoch  0, batch   216 | loss: 2.8834443MixupTrain:  epoch  0, batch   217 | loss: 2.9153359MixupTrain:  epoch  0, batch   218 | loss: 2.7414916MixupTrain:  epoch  0, batch   219 | loss: 2.9504387MixupTrain:  epoch  0, batch   220 | loss: 2.7849836MixupTrain:  epoch  0, batch   221 | loss: 2.8099899MixupTrain:  epoch  0, batch   222 | loss: 2.7493086MixupTrain:  epoch  0, batch   223 | loss: 2.9362454MixupTrain:  epoch  0, batch   224 | loss: 2.9314823MixupTrain:  epoch  0, batch   225 | loss: 2.8133969MixupTrain:  epoch  0, batch   226 | loss: 2.8449726MixupTrain:  epoch  0, batch   227 | loss: 2.6830783MixupTrain:  epoch  0, batch   228 | loss: 2.9417810MixupTrain:  epoch  0, batch   229 | loss: 2.6439555MixupTrain:  epoch  0, batch   230 | loss: 2.8646922MixupTrain:  epoch  0, batch   231 | loss: 2.8987570MixupTrain:  epoch  0, batch   232 | loss: 2.8290424MixupTrain:  epoch  0, batch   233 | loss: 2.8715312MixupTrain:  epoch  0, batch   234 | loss: 2.8873572MixupTrain:  epoch  0, batch   235 | loss: 2.9106705MixupTrain:  epoch  0, batch   236 | loss: 2.8012664MixupTrain:  epoch  0, batch   237 | loss: 2.9159057MixupTrain:  epoch  0, batch   238 | loss: 2.7116156MixupTrain:  epoch  0, batch   239 | loss: 2.7766299MixupTrain:  epoch  0, batch   240 | loss: 2.8205879MixupTrain:  epoch  0, batch   241 | loss: 2.7231925MixupTrain:  epoch  0, batch   242 | loss: 2.8062067MixupTrain:  epoch  0, batch   243 | loss: 2.7608862MixupTrain:  epoch  0, batch   244 | loss: 2.9437828MixupTrain:  epoch  0, batch   245 | loss: 2.8909688MixupTrain:  epoch  0, batch   246 | loss: 2.8967047MixupTrain:  epoch  0, batch   247 | loss: 2.6668589MixupTrain:  epoch  0, batch   248 | loss: 2.7900119MixupTrain:  epoch  0, batch   249 | loss: 2.7815804MixupTrain:  epoch  0, batch   250 | loss: 2.8467851MixupTrain:  epoch  0, batch   251 | loss: 2.7173944MixupTrain:  epoch  0, batch   252 | loss: 2.9305086MixupTrain:  epoch  0, batch   253 | loss: 2.8995805MixupTrain:  epoch  0, batch   254 | loss: 2.8146155MixupTrain:  epoch  0, batch   255 | loss: 2.8590655MixupTrain:  epoch  0, batch   256 | loss: 2.8059125MixupTrain:  epoch  0, batch   257 | loss: 2.7411706MixupTrain:  epoch  0, batch   258 | loss: 2.7693186MixupTrain:  epoch  0, batch   259 | loss: 2.8610730MixupTrain:  epoch  0, batch   260 | loss: 2.9466610MixupTrain:  epoch  0, batch   261 | loss: 2.8800063MixupTrain:  epoch  0, batch   262 | loss: 2.9030404MixupTrain:  epoch  0, batch   263 | loss: 2.9229512MixupTrain:  epoch  0, batch   264 | loss: 2.8916423MixupTrain:  epoch  0, batch   265 | loss: 2.8273544MixupTrain:  epoch  0, batch   266 | loss: 2.7427764MixupTrain:  epoch  0, batch   267 | loss: 2.8484516MixupTrain:  epoch  0, batch   268 | loss: 2.8119202MixupTrain:  epoch  0, batch   269 | loss: 2.7061367MixupTrain:  epoch  0, batch   270 | loss: 2.8555520MixupTrain:  epoch  0, batch   271 | loss: 2.7971895MixupTrain:  epoch  0, batch   272 | loss: 2.8238878MixupTrain:  epoch  0, batch   273 | loss: 2.8738353MixupTrain:  epoch  0, batch   274 | loss: 2.8774533MixupTrain:  epoch  0, batch   275 | loss: 2.8566098MixupTrain:  epoch  0, batch   276 | loss: 2.8886502MixupTrain:  epoch  0, batch   277 | loss: 2.8393445MixupTrain:  epoch  0, batch   278 | loss: 2.8005023MixupTrain:  epoch  0, batch   279 | loss: 2.8163943MixupTrain:  epoch  0, batch   280 | loss: 2.7968030MixupTrain:  epoch  0, batch   281 | loss: 2.8199425MixupTrain:  epoch  0, batch   282 | loss: 2.8472776MixupTrain:  epoch  0, batch   283 | loss: 2.7540507MixupTrain:  epoch  0, batch   284 | loss: 2.7963791MixupTrain:  epoch  0, batch   285 | loss: 2.8429658MixupTrain:  epoch  0, batch   286 | loss: 2.9861870MixupTrain:  epoch  0, batch   287 | loss: 2.8514848MixupTrain:  epoch  0, batch   288 | loss: 2.7670705MixupTrain:  epoch  0, batch   289 | loss: 2.9043922MixupTrain:  epoch  0, batch   290 | loss: 2.8320460MixupTrain:  epoch  0, batch   291 | loss: 2.7054617MixupTrain:  epoch  0, batch   292 | loss: 2.9254117MixupTrain:  epoch  0, batch   293 | loss: 2.8090193MixupTrain:  epoch  0, batch   294 | loss: 2.6542695MixupTrain:  epoch  0, batch   295 | loss: 2.6947157MixupTrain:  epoch  0, batch   296 | loss: 2.9227841MixupTrain:  epoch  0, batch   297 | loss: 2.7951381MixupTrain:  epoch  0, batch   298 | loss: 2.8434961MixupTrain:  epoch  0, batch   299 | loss: 2.8801849MixupTrain:  epoch  0, batch   300 | loss: 2.7146916MixupTrain:  epoch  0, batch   301 | loss: 2.8105278MixupTrain:  epoch  0, batch   302 | loss: 2.7327650MixupTrain:  epoch  0, batch   303 | loss: 2.7541606MixupTrain:  epoch  0, batch   304 | loss: 2.8760538MixupTrain:  epoch  0, batch   305 | loss: 2.7902477MixupTrain:  epoch  0, batch   306 | loss: 2.7650018MixupTrain:  epoch  0, batch   307 | loss: 2.7289715MixupTrain:  epoch  0, batch   308 | loss: 2.7089586MixupTrain:  epoch  0, batch   309 | loss: 2.9543071MixupTrain:  epoch  0, batch   310 | loss: 2.9079194MixupTrain:  epoch  0, batch   311 | loss: 2.8423359MixupTrain:  epoch  0, batch   312 | loss: 2.7600293MixupTrain:  epoch  0, batch   313 | loss: 2.7362859MixupTrain:  epoch  0, batch   314 | loss: 2.7084122MixupTrain:  epoch  0, batch   315 | loss: 2.8482451MixupTrain:  epoch  0, batch   316 | loss: 2.7121348MixupTrain:  epoch  0, batch   317 | loss: 2.7307627MixupTrain:  epoch  0, batch   318 | loss: 2.8889101MixupTrain:  epoch  0, batch   319 | loss: 2.8473902MixupTrain:  epoch  0, batch   320 | loss: 2.7907572MixupTrain:  epoch  0, batch   321 | loss: 3.0009527MixupTrain:  epoch  0, batch   322 | loss: 2.8400578MixupTrain:  epoch  0, batch   323 | loss: 2.9237523MixupTrain:  epoch  0, batch   324 | loss: 2.8433294MixupTrain:  epoch  0, batch   325 | loss: 2.9875846MixupTrain:  epoch  0, batch   326 | loss: 2.8202856MixupTrain:  epoch  0, batch   327 | loss: 2.7328987MixupTrain:  epoch  0, batch   328 | loss: 2.8362727MixupTrain:  epoch  0, batch   329 | loss: 2.7131379MixupTrain:  epoch  0, batch   330 | loss: 2.8042536MixupTrain:  epoch  0, batch   331 | loss: 2.7523546MixupTrain:  epoch  0, batch   332 | loss: 2.9064164MixupTrain:  epoch  0, batch   333 | loss: 3.0133271MixupTrain:  epoch  0, batch   334 | loss: 2.9581180MixupTrain:  epoch  0, batch   335 | loss: 2.8186293MixupTrain:  epoch  0, batch   336 | loss: 2.7023711MixupTrain:  epoch  0, batch   337 | loss: 2.8431904MixupTrain:  epoch  0, batch   338 | loss: 2.7560334MixupTrain:  epoch  0, batch   339 | loss: 2.7929733MixupTrain:  epoch  0, batch   340 | loss: 2.8531370MixupTrain:  epoch  0, batch   341 | loss: 2.7421465MixupTrain:  epoch  0, batch   342 | loss: 2.7704568MixupTrain:  epoch  0, batch   343 | loss: 2.8608508MixupTrain:  epoch  0, batch   344 | loss: 2.7923968MixupTrain:  epoch  0, batch   345 | loss: 2.7827206MixupTrain:  epoch  0, batch   346 | loss: 2.9101813MixupTrain:  epoch  0, batch   347 | loss: 2.7269287MixupTrain:  epoch  0, batch   348 | loss: 2.7671797MixupTrain:  epoch  0, batch   349 | loss: 2.7897148MixupTrain:  epoch  0, batch   350 | loss: 2.7556167MixupTrain:  epoch  0, batch   351 | loss: 2.8360066MixupTrain:  epoch  0, batch   352 | loss: 2.8605864MixupTrain:  epoch  0, batch   353 | loss: 2.8098412MixupTrain:  epoch  0, batch   354 | loss: 2.8475583MixupTrain:  epoch  0, batch   355 | loss: 2.7737813MixupTrain:  epoch  0, batch   356 | loss: 2.8645828MixupTrain:  epoch  0, batch   357 | loss: 3.0276272MixupTrain:  epoch  0, batch   358 | loss: 2.7087355MixupTrain:  epoch  0, batch   359 | loss: 2.8189919MixupTrain:  epoch  0, batch   360 | loss: 2.8082500MixupTrain:  epoch  0, batch   361 | loss: 2.9074454MixupTrain:  epoch  0, batch   362 | loss: 2.8147914MixupTrain:  epoch  0, batch   363 | loss: 2.7025690MixupTrain:  epoch  0, batch   364 | loss: 2.7731380MixupTrain:  epoch  0, batch   365 | loss: 2.7598937MixupTrain:  epoch  0, batch   366 | loss: 2.7210481MixupTrain:  epoch  0, batch   367 | loss: 2.7623277MixupTrain:  epoch  0, batch   368 | loss: 2.8815298MixupTrain:  epoch  0, batch   369 | loss: 2.7283125MixupTrain:  epoch  0, batch   370 | loss: 2.7331638MixupTrain:  epoch  0, batch   371 | loss: 2.8899171MixupTrain:  epoch  0, batch   372 | loss: 2.8957419MixupTrain:  epoch  0, batch   373 | loss: 2.7940950MixupTrain:  epoch  0, batch   374 | loss: 2.7904136MixupTrain:  epoch  0, batch   375 | loss: 2.7996488MixupTrain:  epoch  0, batch   376 | loss: 2.8212624MixupTrain:  epoch  0, batch   377 | loss: 2.8754473MixupTrain:  epoch  0, batch   378 | loss: 2.9683714MixupTrain:  epoch  0, batch   379 | loss: 2.9309914MixupTrain:  epoch  0, batch   380 | loss: 2.8287368MixupTrain:  epoch  0, batch   381 | loss: 2.7642653MixupTrain:  epoch  0, batch   382 | loss: 2.7396638MixupTrain:  epoch  0, batch   383 | loss: 2.8123600MixupTrain:  epoch  0, batch   384 | loss: 2.7150161MixupTrain:  epoch  0, batch   385 | loss: 2.6381085MixupTrain:  epoch  0, batch   386 | loss: 2.7719855MixupTrain:  epoch  0, batch   387 | loss: 2.8621550MixupTrain:  epoch  0, batch   388 | loss: 2.8095250MixupTrain:  epoch  0, batch   389 | loss: 2.7493329MixupTrain:  epoch  0, batch   390 | loss: 2.7633309MixupTrain:  epoch  0, batch   391 | loss: 2.7150044MixupTrain:  epoch  0, batch   392 | loss: 2.7911065MixupTrain:  epoch  0, batch   393 | loss: 2.7496016MixupTrain:  epoch  0, batch   394 | loss: 2.6758723MixupTrain:  epoch  0, batch   395 | loss: 2.8420863MixupTrain:  epoch  0, batch   396 | loss: 2.9526079MixupTrain:  epoch  0, batch   397 | loss: 2.9635987MixupTrain:  epoch  0, batch   398 | loss: 2.8547544MixupTrain:  epoch  0, batch   399 | loss: 2.6999002MixupTrain:  epoch  0, batch   400 | loss: 2.6927223MixupTrain:  epoch  0, batch   401 | loss: 2.8032904MixupTrain:  epoch  0, batch   402 | loss: 2.8649359MixupTrain:  epoch  0, batch   403 | loss: 2.9422741MixupTrain:  epoch  0, batch   404 | loss: 2.7519870MixupTrain:  epoch  0, batch   405 | loss: 2.8119473MixupTrain:  epoch  0, batch   406 | loss: 2.7933440MixupTrain:  epoch  0, batch   407 | loss: 2.7968709MixupTrain:  epoch  0, batch   408 | loss: 2.8289168MixupTrain:  epoch  0, batch   409 | loss: 2.7284915MixupTrain:  epoch  0, batch   410 | loss: 2.8281708MixupTrain:  epoch  0, batch   411 | loss: 2.8568377MixupTrain:  epoch  0, batch   412 | loss: 3.0119548MixupTrain:  epoch  0, batch   413 | loss: 2.8694580MixupTrain:  epoch  0, batch   414 | loss: 2.7638941MixupTrain:  epoch  0, batch   415 | loss: 2.8542743MixupTrain:  epoch  0, batch   416 | loss: 2.7857852MixupTrain:  epoch  0, batch   417 | loss: 2.8612792
MemoryTrain:  epoch  0, batch     0 | loss: 1.2594464MemoryTrain:  epoch  0, batch     1 | loss: 1.3847895MemoryTrain:  epoch  0, batch     2 | loss: 1.6111813MemoryTrain:  epoch  0, batch     3 | loss: 1.7817225MemoryTrain:  epoch  0, batch     4 | loss: 1.7499094MemoryTrain:  epoch  0, batch     5 | loss: 1.7064694MemoryTrain:  epoch  1, batch     0 | loss: 1.4661616MemoryTrain:  epoch  1, batch     1 | loss: 1.2427144MemoryTrain:  epoch  1, batch     2 | loss: 1.2279774MemoryTrain:  epoch  1, batch     3 | loss: 1.3961589MemoryTrain:  epoch  1, batch     4 | loss: 1.3737581MemoryTrain:  epoch  1, batch     5 | loss: 1.4043673MemoryTrain:  epoch  2, batch     0 | loss: 1.2704947MemoryTrain:  epoch  2, batch     1 | loss: 1.2331884MemoryTrain:  epoch  2, batch     2 | loss: 1.2430691MemoryTrain:  epoch  2, batch     3 | loss: 1.2845173MemoryTrain:  epoch  2, batch     4 | loss: 1.4120282MemoryTrain:  epoch  2, batch     5 | loss: 1.4669777MemoryTrain:  epoch  3, batch     0 | loss: 1.2432816MemoryTrain:  epoch  3, batch     1 | loss: 1.3157163MemoryTrain:  epoch  3, batch     2 | loss: 1.2238050MemoryTrain:  epoch  3, batch     3 | loss: 1.2901614MemoryTrain:  epoch  3, batch     4 | loss: 1.2584116MemoryTrain:  epoch  3, batch     5 | loss: 1.2006484MemoryTrain:  epoch  4, batch     0 | loss: 1.2905173MemoryTrain:  epoch  4, batch     1 | loss: 1.2630529MemoryTrain:  epoch  4, batch     2 | loss: 1.2704055MemoryTrain:  epoch  4, batch     3 | loss: 1.2697487MemoryTrain:  epoch  4, batch     4 | loss: 1.2363044MemoryTrain:  epoch  4, batch     5 | loss: 1.2334281MemoryTrain:  epoch  5, batch     0 | loss: 1.2264118MemoryTrain:  epoch  5, batch     1 | loss: 1.1871945MemoryTrain:  epoch  5, batch     2 | loss: 1.2944236MemoryTrain:  epoch  5, batch     3 | loss: 1.2242981MemoryTrain:  epoch  5, batch     4 | loss: 1.1928372MemoryTrain:  epoch  5, batch     5 | loss: 1.2502668MemoryTrain:  epoch  6, batch     0 | loss: 1.2321072MemoryTrain:  epoch  6, batch     1 | loss: 1.2581000MemoryTrain:  epoch  6, batch     2 | loss: 1.2004087MemoryTrain:  epoch  6, batch     3 | loss: 1.2052306MemoryTrain:  epoch  6, batch     4 | loss: 1.2038844MemoryTrain:  epoch  6, batch     5 | loss: 1.2392420MemoryTrain:  epoch  7, batch     0 | loss: 1.1992221MemoryTrain:  epoch  7, batch     1 | loss: 1.2647523MemoryTrain:  epoch  7, batch     2 | loss: 1.2507496MemoryTrain:  epoch  7, batch     3 | loss: 1.2530594MemoryTrain:  epoch  7, batch     4 | loss: 1.2265555MemoryTrain:  epoch  7, batch     5 | loss: 1.2186626MemoryTrain:  epoch  8, batch     0 | loss: 1.2558177MemoryTrain:  epoch  8, batch     1 | loss: 1.2237374MemoryTrain:  epoch  8, batch     2 | loss: 1.2306626MemoryTrain:  epoch  8, batch     3 | loss: 1.1707900MemoryTrain:  epoch  8, batch     4 | loss: 1.2265282MemoryTrain:  epoch  8, batch     5 | loss: 1.2304487MemoryTrain:  epoch  9, batch     0 | loss: 1.2287689MemoryTrain:  epoch  9, batch     1 | loss: 1.2265098MemoryTrain:  epoch  9, batch     2 | loss: 1.1848228MemoryTrain:  epoch  9, batch     3 | loss: 1.2312021MemoryTrain:  epoch  9, batch     4 | loss: 1.1673428MemoryTrain:  epoch  9, batch     5 | loss: 1.2916526
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 75.00%,  total acc: 71.88%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 72.92%   [EVAL] batch:    3 | acc: 93.75%,  total acc: 78.12%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 77.50%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 83.93%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 85.16%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 85.42%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 86.88%   [EVAL] batch:   10 | acc: 68.75%,  total acc: 85.23%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 84.38%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 83.17%   [EVAL] batch:   13 | acc: 56.25%,  total acc: 81.25%   [EVAL] batch:   14 | acc: 31.25%,  total acc: 77.92%   
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    1 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    2 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    3 | acc: 50.00%,  total acc: 59.38%   [EVAL] batch:    4 | acc: 62.50%,  total acc: 60.00%   [EVAL] batch:    5 | acc: 50.00%,  total acc: 58.33%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 62.50%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 67.19%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 70.83%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 72.50%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 76.04%   [EVAL] batch:   12 | acc: 50.00%,  total acc: 74.04%   [EVAL] batch:   13 | acc: 31.25%,  total acc: 70.98%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 70.83%   [EVAL] batch:   15 | acc: 62.50%,  total acc: 70.31%   [EVAL] batch:   16 | acc: 68.75%,  total acc: 70.22%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 69.79%   [EVAL] batch:   18 | acc: 87.50%,  total acc: 70.72%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 71.88%   [EVAL] batch:   20 | acc: 81.25%,  total acc: 72.32%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 72.73%   [EVAL] batch:   22 | acc: 87.50%,  total acc: 73.37%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 73.96%   [EVAL] batch:   24 | acc: 93.75%,  total acc: 74.75%   [EVAL] batch:   25 | acc: 93.75%,  total acc: 75.48%   [EVAL] batch:   26 | acc: 87.50%,  total acc: 75.93%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 76.79%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 77.59%   [EVAL] batch:   29 | acc: 75.00%,  total acc: 77.50%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 77.82%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 78.32%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 77.08%   [EVAL] batch:   33 | acc: 0.00%,  total acc: 74.82%   [EVAL] batch:   34 | acc: 0.00%,  total acc: 72.68%   [EVAL] batch:   35 | acc: 0.00%,  total acc: 70.66%   [EVAL] batch:   36 | acc: 0.00%,  total acc: 68.75%   [EVAL] batch:   37 | acc: 0.00%,  total acc: 66.94%   [EVAL] batch:   38 | acc: 18.75%,  total acc: 65.71%   [EVAL] batch:   39 | acc: 93.75%,  total acc: 66.41%   [EVAL] batch:   40 | acc: 75.00%,  total acc: 66.62%   [EVAL] batch:   41 | acc: 6.25%,  total acc: 65.18%   [EVAL] batch:   42 | acc: 12.50%,  total acc: 63.95%   [EVAL] batch:   43 | acc: 62.50%,  total acc: 63.92%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 64.72%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 65.49%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 66.22%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 66.93%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 67.60%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 68.00%   [EVAL] batch:   50 | acc: 37.50%,  total acc: 67.40%   [EVAL] batch:   51 | acc: 18.75%,  total acc: 66.47%   [EVAL] batch:   52 | acc: 18.75%,  total acc: 65.57%   [EVAL] batch:   53 | acc: 0.00%,  total acc: 64.35%   [EVAL] batch:   54 | acc: 6.25%,  total acc: 63.30%   [EVAL] batch:   55 | acc: 0.00%,  total acc: 62.17%   [EVAL] batch:   56 | acc: 56.25%,  total acc: 62.06%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 62.61%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 63.03%   [EVAL] batch:   59 | acc: 87.50%,  total acc: 63.44%   [EVAL] batch:   60 | acc: 75.00%,  total acc: 63.63%   [EVAL] batch:   61 | acc: 81.25%,  total acc: 63.91%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 63.69%   [EVAL] batch:   63 | acc: 68.75%,  total acc: 63.77%   [EVAL] batch:   64 | acc: 93.75%,  total acc: 64.23%   [EVAL] batch:   65 | acc: 93.75%,  total acc: 64.68%   [EVAL] batch:   66 | acc: 100.00%,  total acc: 65.21%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 65.72%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 66.21%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 66.70%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 67.17%   [EVAL] batch:   71 | acc: 93.75%,  total acc: 67.53%   [EVAL] batch:   72 | acc: 31.25%,  total acc: 67.04%   [EVAL] batch:   73 | acc: 75.00%,  total acc: 67.15%   [EVAL] batch:   74 | acc: 50.00%,  total acc: 66.92%   [EVAL] batch:   75 | acc: 50.00%,  total acc: 66.69%   [EVAL] batch:   76 | acc: 43.75%,  total acc: 66.40%   [EVAL] batch:   77 | acc: 37.50%,  total acc: 66.03%   [EVAL] batch:   78 | acc: 75.00%,  total acc: 66.14%   [EVAL] batch:   79 | acc: 56.25%,  total acc: 66.02%   [EVAL] batch:   80 | acc: 68.75%,  total acc: 66.05%   [EVAL] batch:   81 | acc: 100.00%,  total acc: 66.46%   [EVAL] batch:   82 | acc: 87.50%,  total acc: 66.72%   [EVAL] batch:   83 | acc: 93.75%,  total acc: 67.04%   [EVAL] batch:   84 | acc: 12.50%,  total acc: 66.40%   [EVAL] batch:   85 | acc: 0.00%,  total acc: 65.62%   [EVAL] batch:   86 | acc: 6.25%,  total acc: 64.94%   [EVAL] batch:   87 | acc: 18.75%,  total acc: 64.42%   [EVAL] batch:   88 | acc: 81.25%,  total acc: 64.61%   [EVAL] batch:   89 | acc: 75.00%,  total acc: 64.72%   [EVAL] batch:   90 | acc: 68.75%,  total acc: 64.77%   [EVAL] batch:   91 | acc: 68.75%,  total acc: 64.81%   [EVAL] batch:   92 | acc: 87.50%,  total acc: 65.05%   [EVAL] batch:   93 | acc: 81.25%,  total acc: 65.23%   [EVAL] batch:   94 | acc: 87.50%,  total acc: 65.46%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 65.82%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 66.17%   [EVAL] batch:   97 | acc: 93.75%,  total acc: 66.45%   [EVAL] batch:   98 | acc: 87.50%,  total acc: 66.67%   [EVAL] batch:   99 | acc: 93.75%,  total acc: 66.94%   [EVAL] batch:  100 | acc: 62.50%,  total acc: 66.89%   [EVAL] batch:  101 | acc: 75.00%,  total acc: 66.97%   [EVAL] batch:  102 | acc: 75.00%,  total acc: 67.05%   [EVAL] batch:  103 | acc: 50.00%,  total acc: 66.89%   [EVAL] batch:  104 | acc: 6.25%,  total acc: 66.31%   
cur_acc:  ['0.8712', '0.8368', '0.6741', '0.8929', '0.7019', '0.7792']
his_acc:  ['0.8712', '0.8475', '0.8076', '0.7548', '0.6917', '0.6631']
CurrentTrain: epoch  0, batch     0 | loss: 4.7597837CurrentTrain: epoch  0, batch     1 | loss: 4.8794270CurrentTrain: epoch  1, batch     0 | loss: 3.8588326CurrentTrain: epoch  1, batch     1 | loss: 4.3302507CurrentTrain: epoch  2, batch     0 | loss: 2.9317238CurrentTrain: epoch  2, batch     1 | loss: 4.1625719CurrentTrain: epoch  3, batch     0 | loss: 2.7721882CurrentTrain: epoch  3, batch     1 | loss: 4.0810404CurrentTrain: epoch  4, batch     0 | loss: 3.4786625CurrentTrain: epoch  4, batch     1 | loss: 2.6641800CurrentTrain: epoch  5, batch     0 | loss: 3.0254653CurrentTrain: epoch  5, batch     1 | loss: 2.8746228CurrentTrain: epoch  6, batch     0 | loss: 3.1075299CurrentTrain: epoch  6, batch     1 | loss: 2.2041941CurrentTrain: epoch  7, batch     0 | loss: 2.8903637CurrentTrain: epoch  7, batch     1 | loss: 2.1808476CurrentTrain: epoch  8, batch     0 | loss: 2.5707493CurrentTrain: epoch  8, batch     1 | loss: 2.5896699CurrentTrain: epoch  9, batch     0 | loss: 2.6246965CurrentTrain: epoch  9, batch     1 | loss: 2.0265586
#### person country of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person country of birth" means a person was born in a country.
Here is an example:
Relation: person country of birth
Context: in testimony by satellite link from germany to a house of representatives ' panel , murat kurnaz recounted his five-year detention , alleging a wide range of torture and abuse .
Head Entity: murat kurnaz
Tail Entity: germany
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person country of birth":
Relation: person country of birth  
Context: During the interview, it was revealed that Maria Gomez was born in Spain before moving to the United States for her studies.  
Head Entity: Maria Gomez  
Tail Entity: Spain  

Relation: person country of birth  
Context: The documentary highlighted the life of Akira Tanaka, who shared his experiences growing up in Japan and later relocating to Canada.  
Head Entity: Akira Tanaka  
Tail Entity: Japan  
#### organization website ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization website" means an organization 's website is an url.
Here is an example:
Relation: organization website
Context: barnes foundation : http://www.barnesfoundation.org
Head Entity: barnes foundation
Tail Entity: http://www.barnesfoundation.org
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization website":
Relation: organization website  
Context: national geographic society : https://www.nationalgeographic.com  
Head Entity: national geographic society  
Tail Entity: https://www.nationalgeographic.com  

Relation: organization website  
Context: world wildlife fund : https://www.worldwildlife.org  
Head Entity: world wildlife fund  
Tail Entity: https://www.worldwildlife.org  
#### organization shareholders ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization shareholders" means an organization was invested by a person.
Here is an example:
Relation: organization shareholders
Context: marsans already owns the madrid-based air comet and 95 percent of aerolineas argentinas .
Head Entity: aerolineas argentinas
Tail Entity: marsans
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization shareholders":
Relation: organization shareholders  
Context: tech giant Apple has seen significant investments from Warren Buffett's Berkshire Hathaway, which now holds a substantial stake in the company.  
Head Entity: Apple  
Tail Entity: Berkshire Hathaway  

Relation: organization shareholders  
Context: The investment firm Vanguard Group has increased its holdings in the multinational corporation Procter & Gamble, reflecting confidence in its long-term growth.  
Head Entity: Procter & Gamble  
Tail Entity: Vanguard Group  
#### organization dissolved ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization dissolved" means an organization was dissolved in a date.
Here is an example:
Relation: organization dissolved
Context: the tse suffered its worst-ever system crash in november 2005 which paralyzed the world 's second largest bourse and forced it to shelve plan for a listing of its own .
Head Entity: tse
Tail Entity: november 2005
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization dissolved":
Relation: organization dissolved  
Context: the once-prominent tech startup, Innovatech, officially ceased operations in early 2020 after struggling to secure funding.  
Head Entity: Innovatech  
Tail Entity: early 2020  

Relation: organization dissolved  
Context: after years of financial difficulties, the local arts council announced its closure in the spring of 2018, leaving many artists without support.  
Head Entity: local arts council  
Tail Entity: spring of 2018  
#### organization founded by ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization founded by" means an organization was found by a person.
Here is an example:
Relation: organization founded by
Context: born in new york , she inherited most of her fortune from her father , ted arison , who founded carnival cruise lines and also owned the miami heat basketball team .
Head Entity: carnival cruise lines
Tail Entity: ted arison
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization founded by":
Relation: organization founded by  
Context: in 1975, steve jobs and steve wozniak started apple inc., which revolutionized the personal computer industry and changed the way people interact with technology.  
Head Entity: apple inc.  
Tail Entity: steve jobs  

Relation: organization founded by  
Context: the famous fashion brand gucci was established in 1921 by guccio gucci in florence, italy, and has since become a symbol of luxury and style.  
Head Entity: gucci  
Tail Entity: guccio gucci  
Mixup data size:  8545
MixupTrain:  epoch  0, batch     0 | loss: 3.7500758MixupTrain:  epoch  0, batch     1 | loss: 4.1610107MixupTrain:  epoch  0, batch     2 | loss: 3.8923011MixupTrain:  epoch  0, batch     3 | loss: 3.8612583MixupTrain:  epoch  0, batch     4 | loss: 4.3066063MixupTrain:  epoch  0, batch     5 | loss: 3.9587469MixupTrain:  epoch  0, batch     6 | loss: 3.9200621MixupTrain:  epoch  0, batch     7 | loss: 4.3862114MixupTrain:  epoch  0, batch     8 | loss: 3.7191582MixupTrain:  epoch  0, batch     9 | loss: 4.2932181MixupTrain:  epoch  0, batch    10 | loss: 4.2764115MixupTrain:  epoch  0, batch    11 | loss: 3.4753470MixupTrain:  epoch  0, batch    12 | loss: 3.8441100MixupTrain:  epoch  0, batch    13 | loss: 4.0084238MixupTrain:  epoch  0, batch    14 | loss: 3.7586193MixupTrain:  epoch  0, batch    15 | loss: 3.9063203MixupTrain:  epoch  0, batch    16 | loss: 3.8943663MixupTrain:  epoch  0, batch    17 | loss: 4.1664634MixupTrain:  epoch  0, batch    18 | loss: 3.7278948MixupTrain:  epoch  0, batch    19 | loss: 3.5962794MixupTrain:  epoch  0, batch    20 | loss: 3.5225172MixupTrain:  epoch  0, batch    21 | loss: 3.5584230MixupTrain:  epoch  0, batch    22 | loss: 3.7834692MixupTrain:  epoch  0, batch    23 | loss: 4.3056545MixupTrain:  epoch  0, batch    24 | loss: 4.1255031MixupTrain:  epoch  0, batch    25 | loss: 3.9748418MixupTrain:  epoch  0, batch    26 | loss: 3.9216270MixupTrain:  epoch  0, batch    27 | loss: 3.7056923MixupTrain:  epoch  0, batch    28 | loss: 3.5966082MixupTrain:  epoch  0, batch    29 | loss: 4.0422249MixupTrain:  epoch  0, batch    30 | loss: 3.9189005MixupTrain:  epoch  0, batch    31 | loss: 3.2643571MixupTrain:  epoch  0, batch    32 | loss: 3.8143518MixupTrain:  epoch  0, batch    33 | loss: 4.0358353MixupTrain:  epoch  0, batch    34 | loss: 3.6825674MixupTrain:  epoch  0, batch    35 | loss: 3.6110373MixupTrain:  epoch  0, batch    36 | loss: 3.9107761MixupTrain:  epoch  0, batch    37 | loss: 3.0472710MixupTrain:  epoch  0, batch    38 | loss: 3.3005719MixupTrain:  epoch  0, batch    39 | loss: 3.4629760MixupTrain:  epoch  0, batch    40 | loss: 3.7519155MixupTrain:  epoch  0, batch    41 | loss: 3.7421925MixupTrain:  epoch  0, batch    42 | loss: 3.5725312MixupTrain:  epoch  0, batch    43 | loss: 3.6436834MixupTrain:  epoch  0, batch    44 | loss: 3.2507019MixupTrain:  epoch  0, batch    45 | loss: 3.3216982MixupTrain:  epoch  0, batch    46 | loss: 3.3804421MixupTrain:  epoch  0, batch    47 | loss: 3.9631133MixupTrain:  epoch  0, batch    48 | loss: 3.6642716MixupTrain:  epoch  0, batch    49 | loss: 3.4325466MixupTrain:  epoch  0, batch    50 | loss: 3.4276330MixupTrain:  epoch  0, batch    51 | loss: 3.2431450MixupTrain:  epoch  0, batch    52 | loss: 3.7020123MixupTrain:  epoch  0, batch    53 | loss: 3.9923425MixupTrain:  epoch  0, batch    54 | loss: 3.3165298MixupTrain:  epoch  0, batch    55 | loss: 3.5204701MixupTrain:  epoch  0, batch    56 | loss: 2.9793253MixupTrain:  epoch  0, batch    57 | loss: 3.6836922MixupTrain:  epoch  0, batch    58 | loss: 3.4445431MixupTrain:  epoch  0, batch    59 | loss: 3.7216704MixupTrain:  epoch  0, batch    60 | loss: 3.4602895MixupTrain:  epoch  0, batch    61 | loss: 3.7092984MixupTrain:  epoch  0, batch    62 | loss: 3.5568600MixupTrain:  epoch  0, batch    63 | loss: 3.5161409MixupTrain:  epoch  0, batch    64 | loss: 3.6552243MixupTrain:  epoch  0, batch    65 | loss: 3.7720699MixupTrain:  epoch  0, batch    66 | loss: 3.5829821MixupTrain:  epoch  0, batch    67 | loss: 3.7535510MixupTrain:  epoch  0, batch    68 | loss: 3.5636094MixupTrain:  epoch  0, batch    69 | loss: 3.3757391MixupTrain:  epoch  0, batch    70 | loss: 3.5244389MixupTrain:  epoch  0, batch    71 | loss: 3.4326186MixupTrain:  epoch  0, batch    72 | loss: 3.4922862MixupTrain:  epoch  0, batch    73 | loss: 3.3904340MixupTrain:  epoch  0, batch    74 | loss: 3.4918065MixupTrain:  epoch  0, batch    75 | loss: 3.1829395MixupTrain:  epoch  0, batch    76 | loss: 3.5552187MixupTrain:  epoch  0, batch    77 | loss: 3.2892251MixupTrain:  epoch  0, batch    78 | loss: 3.5292790MixupTrain:  epoch  0, batch    79 | loss: 3.4399536MixupTrain:  epoch  0, batch    80 | loss: 3.2933953MixupTrain:  epoch  0, batch    81 | loss: 3.1648359MixupTrain:  epoch  0, batch    82 | loss: 3.1984909MixupTrain:  epoch  0, batch    83 | loss: 3.5566075MixupTrain:  epoch  0, batch    84 | loss: 3.6728780MixupTrain:  epoch  0, batch    85 | loss: 3.3344824MixupTrain:  epoch  0, batch    86 | loss: 3.5762544MixupTrain:  epoch  0, batch    87 | loss: 3.3211479MixupTrain:  epoch  0, batch    88 | loss: 3.4173334MixupTrain:  epoch  0, batch    89 | loss: 3.5002899MixupTrain:  epoch  0, batch    90 | loss: 3.7175274MixupTrain:  epoch  0, batch    91 | loss: 3.5684581MixupTrain:  epoch  0, batch    92 | loss: 3.4709721MixupTrain:  epoch  0, batch    93 | loss: 3.2961791MixupTrain:  epoch  0, batch    94 | loss: 3.6287236MixupTrain:  epoch  0, batch    95 | loss: 3.7227645MixupTrain:  epoch  0, batch    96 | loss: 3.4156442MixupTrain:  epoch  0, batch    97 | loss: 3.4217219MixupTrain:  epoch  0, batch    98 | loss: 3.3047252MixupTrain:  epoch  0, batch    99 | loss: 3.3942866MixupTrain:  epoch  0, batch   100 | loss: 3.5094566MixupTrain:  epoch  0, batch   101 | loss: 3.1066060MixupTrain:  epoch  0, batch   102 | loss: 3.3525670MixupTrain:  epoch  0, batch   103 | loss: 3.6749809MixupTrain:  epoch  0, batch   104 | loss: 3.3135052MixupTrain:  epoch  0, batch   105 | loss: 3.7481675MixupTrain:  epoch  0, batch   106 | loss: 3.1298256MixupTrain:  epoch  0, batch   107 | loss: 3.7081122MixupTrain:  epoch  0, batch   108 | loss: 3.1694658MixupTrain:  epoch  0, batch   109 | loss: 3.4892118MixupTrain:  epoch  0, batch   110 | loss: 3.2128406MixupTrain:  epoch  0, batch   111 | loss: 3.4675231MixupTrain:  epoch  0, batch   112 | loss: 3.5554123MixupTrain:  epoch  0, batch   113 | loss: 3.3727283MixupTrain:  epoch  0, batch   114 | loss: 3.6680307MixupTrain:  epoch  0, batch   115 | loss: 3.0358634MixupTrain:  epoch  0, batch   116 | loss: 3.2126064MixupTrain:  epoch  0, batch   117 | loss: 3.5479925MixupTrain:  epoch  0, batch   118 | loss: 3.0742941MixupTrain:  epoch  0, batch   119 | loss: 3.4519515MixupTrain:  epoch  0, batch   120 | loss: 3.5063968MixupTrain:  epoch  0, batch   121 | loss: 3.4627039MixupTrain:  epoch  0, batch   122 | loss: 3.2170191MixupTrain:  epoch  0, batch   123 | loss: 3.3372946MixupTrain:  epoch  0, batch   124 | loss: 3.3403804MixupTrain:  epoch  0, batch   125 | loss: 3.0176110MixupTrain:  epoch  0, batch   126 | loss: 3.4351063MixupTrain:  epoch  0, batch   127 | loss: 3.6159139MixupTrain:  epoch  0, batch   128 | loss: 3.3069494MixupTrain:  epoch  0, batch   129 | loss: 3.4177828MixupTrain:  epoch  0, batch   130 | loss: 3.3581727MixupTrain:  epoch  0, batch   131 | loss: 3.4768324MixupTrain:  epoch  0, batch   132 | loss: 3.1823010MixupTrain:  epoch  0, batch   133 | loss: 3.1521611MixupTrain:  epoch  0, batch   134 | loss: 3.2987897MixupTrain:  epoch  0, batch   135 | loss: 3.2495894MixupTrain:  epoch  0, batch   136 | loss: 3.4497242MixupTrain:  epoch  0, batch   137 | loss: 3.3200362MixupTrain:  epoch  0, batch   138 | loss: 3.3206415MixupTrain:  epoch  0, batch   139 | loss: 3.3226545MixupTrain:  epoch  0, batch   140 | loss: 2.9557121MixupTrain:  epoch  0, batch   141 | loss: 3.3191462MixupTrain:  epoch  0, batch   142 | loss: 3.4658389MixupTrain:  epoch  0, batch   143 | loss: 3.3218970MixupTrain:  epoch  0, batch   144 | loss: 3.1675408MixupTrain:  epoch  0, batch   145 | loss: 3.1688194MixupTrain:  epoch  0, batch   146 | loss: 3.4099836MixupTrain:  epoch  0, batch   147 | loss: 3.3307457MixupTrain:  epoch  0, batch   148 | loss: 3.4310546MixupTrain:  epoch  0, batch   149 | loss: 3.3328433MixupTrain:  epoch  0, batch   150 | loss: 3.4207604MixupTrain:  epoch  0, batch   151 | loss: 3.6198585MixupTrain:  epoch  0, batch   152 | loss: 3.4870408MixupTrain:  epoch  0, batch   153 | loss: 2.9840841MixupTrain:  epoch  0, batch   154 | loss: 3.2594442MixupTrain:  epoch  0, batch   155 | loss: 3.1666560MixupTrain:  epoch  0, batch   156 | loss: 3.1445141MixupTrain:  epoch  0, batch   157 | loss: 3.5001280MixupTrain:  epoch  0, batch   158 | loss: 3.1868975MixupTrain:  epoch  0, batch   159 | loss: 3.1476033MixupTrain:  epoch  0, batch   160 | loss: 3.2285798MixupTrain:  epoch  0, batch   161 | loss: 3.0834146MixupTrain:  epoch  0, batch   162 | loss: 3.3915586MixupTrain:  epoch  0, batch   163 | loss: 3.0650127MixupTrain:  epoch  0, batch   164 | loss: 3.3921950MixupTrain:  epoch  0, batch   165 | loss: 3.1982117MixupTrain:  epoch  0, batch   166 | loss: 3.2086289MixupTrain:  epoch  0, batch   167 | loss: 3.4177225MixupTrain:  epoch  0, batch   168 | loss: 3.2188120MixupTrain:  epoch  0, batch   169 | loss: 3.2943759MixupTrain:  epoch  0, batch   170 | loss: 3.2107115MixupTrain:  epoch  0, batch   171 | loss: 3.3263073MixupTrain:  epoch  0, batch   172 | loss: 3.4101324MixupTrain:  epoch  0, batch   173 | loss: 3.3918054MixupTrain:  epoch  0, batch   174 | loss: 3.3150921MixupTrain:  epoch  0, batch   175 | loss: 3.2608666MixupTrain:  epoch  0, batch   176 | loss: 3.3117726MixupTrain:  epoch  0, batch   177 | loss: 3.1891356MixupTrain:  epoch  0, batch   178 | loss: 3.0540464MixupTrain:  epoch  0, batch   179 | loss: 3.2455621MixupTrain:  epoch  0, batch   180 | loss: 3.3724294MixupTrain:  epoch  0, batch   181 | loss: 3.3082771MixupTrain:  epoch  0, batch   182 | loss: 3.3276639MixupTrain:  epoch  0, batch   183 | loss: 2.9234385MixupTrain:  epoch  0, batch   184 | loss: 3.1064346MixupTrain:  epoch  0, batch   185 | loss: 3.4277852MixupTrain:  epoch  0, batch   186 | loss: 3.0118845MixupTrain:  epoch  0, batch   187 | loss: 3.3953276MixupTrain:  epoch  0, batch   188 | loss: 3.3140070MixupTrain:  epoch  0, batch   189 | loss: 3.3585963MixupTrain:  epoch  0, batch   190 | loss: 3.5162723MixupTrain:  epoch  0, batch   191 | loss: 3.1794415MixupTrain:  epoch  0, batch   192 | loss: 3.2832401MixupTrain:  epoch  0, batch   193 | loss: 3.1731381MixupTrain:  epoch  0, batch   194 | loss: 3.3912137MixupTrain:  epoch  0, batch   195 | loss: 3.0945990MixupTrain:  epoch  0, batch   196 | loss: 3.1216817MixupTrain:  epoch  0, batch   197 | loss: 3.0674267MixupTrain:  epoch  0, batch   198 | loss: 3.3986340MixupTrain:  epoch  0, batch   199 | loss: 3.2282207MixupTrain:  epoch  0, batch   200 | loss: 3.1962175MixupTrain:  epoch  0, batch   201 | loss: 3.4307628MixupTrain:  epoch  0, batch   202 | loss: 3.3377299MixupTrain:  epoch  0, batch   203 | loss: 3.2054553MixupTrain:  epoch  0, batch   204 | loss: 3.2408206MixupTrain:  epoch  0, batch   205 | loss: 3.2968869MixupTrain:  epoch  0, batch   206 | loss: 3.0490360MixupTrain:  epoch  0, batch   207 | loss: 3.3037815MixupTrain:  epoch  0, batch   208 | loss: 3.1012530MixupTrain:  epoch  0, batch   209 | loss: 3.2273908MixupTrain:  epoch  0, batch   210 | loss: 3.2199817MixupTrain:  epoch  0, batch   211 | loss: 2.9627619MixupTrain:  epoch  0, batch   212 | loss: 3.3678930MixupTrain:  epoch  0, batch   213 | loss: 3.1601191MixupTrain:  epoch  0, batch   214 | loss: 3.1435747MixupTrain:  epoch  0, batch   215 | loss: 3.2633600MixupTrain:  epoch  0, batch   216 | loss: 3.3854363MixupTrain:  epoch  0, batch   217 | loss: 3.3728311MixupTrain:  epoch  0, batch   218 | loss: 3.1816144MixupTrain:  epoch  0, batch   219 | loss: 2.9201405MixupTrain:  epoch  0, batch   220 | loss: 3.1184502MixupTrain:  epoch  0, batch   221 | loss: 3.1751025MixupTrain:  epoch  0, batch   222 | loss: 3.0106354MixupTrain:  epoch  0, batch   223 | loss: 3.5050130MixupTrain:  epoch  0, batch   224 | loss: 3.3154678MixupTrain:  epoch  0, batch   225 | loss: 3.2286952MixupTrain:  epoch  0, batch   226 | loss: 3.2157128MixupTrain:  epoch  0, batch   227 | loss: 3.1706367MixupTrain:  epoch  0, batch   228 | loss: 3.2764525MixupTrain:  epoch  0, batch   229 | loss: 3.3132639MixupTrain:  epoch  0, batch   230 | loss: 3.3674316MixupTrain:  epoch  0, batch   231 | loss: 3.0535903MixupTrain:  epoch  0, batch   232 | loss: 3.2654200MixupTrain:  epoch  0, batch   233 | loss: 3.3284893MixupTrain:  epoch  0, batch   234 | loss: 3.4181855MixupTrain:  epoch  0, batch   235 | loss: 3.1098897MixupTrain:  epoch  0, batch   236 | loss: 3.2476702MixupTrain:  epoch  0, batch   237 | loss: 3.3090837MixupTrain:  epoch  0, batch   238 | loss: 3.0273638MixupTrain:  epoch  0, batch   239 | loss: 3.0443218MixupTrain:  epoch  0, batch   240 | loss: 3.3692834MixupTrain:  epoch  0, batch   241 | loss: 2.9869704MixupTrain:  epoch  0, batch   242 | loss: 3.1540527MixupTrain:  epoch  0, batch   243 | loss: 2.9636014MixupTrain:  epoch  0, batch   244 | loss: 3.0196476MixupTrain:  epoch  0, batch   245 | loss: 2.9887309MixupTrain:  epoch  0, batch   246 | loss: 3.3598900MixupTrain:  epoch  0, batch   247 | loss: 3.2886400MixupTrain:  epoch  0, batch   248 | loss: 3.4383712MixupTrain:  epoch  0, batch   249 | loss: 3.2264652MixupTrain:  epoch  0, batch   250 | loss: 3.2626801MixupTrain:  epoch  0, batch   251 | loss: 3.0288684MixupTrain:  epoch  0, batch   252 | loss: 3.3717937MixupTrain:  epoch  0, batch   253 | loss: 3.1095839MixupTrain:  epoch  0, batch   254 | loss: 3.1573253MixupTrain:  epoch  0, batch   255 | loss: 3.0837204MixupTrain:  epoch  0, batch   256 | loss: 3.2327423MixupTrain:  epoch  0, batch   257 | loss: 3.1448178MixupTrain:  epoch  0, batch   258 | loss: 3.1997080MixupTrain:  epoch  0, batch   259 | loss: 3.1029971MixupTrain:  epoch  0, batch   260 | loss: 3.3332860MixupTrain:  epoch  0, batch   261 | loss: 3.3416159MixupTrain:  epoch  0, batch   262 | loss: 3.2443912MixupTrain:  epoch  0, batch   263 | loss: 3.3019614MixupTrain:  epoch  0, batch   264 | loss: 3.1862063MixupTrain:  epoch  0, batch   265 | loss: 3.0587683MixupTrain:  epoch  0, batch   266 | loss: 3.1605942MixupTrain:  epoch  0, batch   267 | loss: 3.1674151MixupTrain:  epoch  0, batch   268 | loss: 3.1735048MixupTrain:  epoch  0, batch   269 | loss: 3.1312122MixupTrain:  epoch  0, batch   270 | loss: 3.1744204MixupTrain:  epoch  0, batch   271 | loss: 3.2071552MixupTrain:  epoch  0, batch   272 | loss: 3.0751543MixupTrain:  epoch  0, batch   273 | loss: 3.0918694MixupTrain:  epoch  0, batch   274 | loss: 3.0916510MixupTrain:  epoch  0, batch   275 | loss: 3.0920243MixupTrain:  epoch  0, batch   276 | loss: 3.3195441MixupTrain:  epoch  0, batch   277 | loss: 3.1891863MixupTrain:  epoch  0, batch   278 | loss: 3.2643495MixupTrain:  epoch  0, batch   279 | loss: 3.0178838MixupTrain:  epoch  0, batch   280 | loss: 3.2254837MixupTrain:  epoch  0, batch   281 | loss: 3.1109529MixupTrain:  epoch  0, batch   282 | loss: 3.1118093MixupTrain:  epoch  0, batch   283 | loss: 3.2612290MixupTrain:  epoch  0, batch   284 | loss: 3.0474839MixupTrain:  epoch  0, batch   285 | loss: 3.1797063MixupTrain:  epoch  0, batch   286 | loss: 3.0694897MixupTrain:  epoch  0, batch   287 | loss: 3.1757464MixupTrain:  epoch  0, batch   288 | loss: 2.8979211MixupTrain:  epoch  0, batch   289 | loss: 3.0649128MixupTrain:  epoch  0, batch   290 | loss: 3.2971587MixupTrain:  epoch  0, batch   291 | loss: 3.2232733MixupTrain:  epoch  0, batch   292 | loss: 3.0847969MixupTrain:  epoch  0, batch   293 | loss: 3.1378295MixupTrain:  epoch  0, batch   294 | loss: 2.8364520MixupTrain:  epoch  0, batch   295 | loss: 3.1508217MixupTrain:  epoch  0, batch   296 | loss: 3.0947766MixupTrain:  epoch  0, batch   297 | loss: 3.0018620MixupTrain:  epoch  0, batch   298 | loss: 3.0995572MixupTrain:  epoch  0, batch   299 | loss: 3.3068397MixupTrain:  epoch  0, batch   300 | loss: 3.0265987MixupTrain:  epoch  0, batch   301 | loss: 3.1312323MixupTrain:  epoch  0, batch   302 | loss: 3.1498218MixupTrain:  epoch  0, batch   303 | loss: 3.2035766MixupTrain:  epoch  0, batch   304 | loss: 3.1284256MixupTrain:  epoch  0, batch   305 | loss: 3.1839161MixupTrain:  epoch  0, batch   306 | loss: 3.2481475MixupTrain:  epoch  0, batch   307 | loss: 3.0425625MixupTrain:  epoch  0, batch   308 | loss: 3.1825933MixupTrain:  epoch  0, batch   309 | loss: 3.2474422MixupTrain:  epoch  0, batch   310 | loss: 3.1697903MixupTrain:  epoch  0, batch   311 | loss: 2.9140263MixupTrain:  epoch  0, batch   312 | loss: 2.9220531MixupTrain:  epoch  0, batch   313 | loss: 3.1124668MixupTrain:  epoch  0, batch   314 | loss: 3.1082973MixupTrain:  epoch  0, batch   315 | loss: 3.4879827MixupTrain:  epoch  0, batch   316 | loss: 3.2548940MixupTrain:  epoch  0, batch   317 | loss: 3.3272266MixupTrain:  epoch  0, batch   318 | loss: 3.2545300MixupTrain:  epoch  0, batch   319 | loss: 2.9514613MixupTrain:  epoch  0, batch   320 | loss: 3.0850534MixupTrain:  epoch  0, batch   321 | loss: 3.1560254MixupTrain:  epoch  0, batch   322 | loss: 3.1061051MixupTrain:  epoch  0, batch   323 | loss: 3.0222611MixupTrain:  epoch  0, batch   324 | loss: 3.0500813MixupTrain:  epoch  0, batch   325 | loss: 3.3990266MixupTrain:  epoch  0, batch   326 | loss: 3.3237574MixupTrain:  epoch  0, batch   327 | loss: 3.2931514MixupTrain:  epoch  0, batch   328 | loss: 3.1374846MixupTrain:  epoch  0, batch   329 | loss: 3.1873248MixupTrain:  epoch  0, batch   330 | loss: 3.2296472MixupTrain:  epoch  0, batch   331 | loss: 3.2030010MixupTrain:  epoch  0, batch   332 | loss: 3.2405903MixupTrain:  epoch  0, batch   333 | loss: 3.1144691MixupTrain:  epoch  0, batch   334 | loss: 3.0838118MixupTrain:  epoch  0, batch   335 | loss: 3.0923924MixupTrain:  epoch  0, batch   336 | loss: 3.0657802MixupTrain:  epoch  0, batch   337 | loss: 3.2973001MixupTrain:  epoch  0, batch   338 | loss: 3.1867118MixupTrain:  epoch  0, batch   339 | loss: 3.0419226MixupTrain:  epoch  0, batch   340 | loss: 3.1573958MixupTrain:  epoch  0, batch   341 | loss: 3.2727594MixupTrain:  epoch  0, batch   342 | loss: 2.8626108MixupTrain:  epoch  0, batch   343 | loss: 3.0009866MixupTrain:  epoch  0, batch   344 | loss: 3.4419465MixupTrain:  epoch  0, batch   345 | loss: 3.2651105MixupTrain:  epoch  0, batch   346 | loss: 3.0839100MixupTrain:  epoch  0, batch   347 | loss: 3.0859864MixupTrain:  epoch  0, batch   348 | loss: 3.0498638MixupTrain:  epoch  0, batch   349 | loss: 3.0661995MixupTrain:  epoch  0, batch   350 | loss: 3.0228534MixupTrain:  epoch  0, batch   351 | loss: 3.0879736MixupTrain:  epoch  0, batch   352 | loss: 3.2273951MixupTrain:  epoch  0, batch   353 | loss: 3.0012326MixupTrain:  epoch  0, batch   354 | loss: 3.0987151MixupTrain:  epoch  0, batch   355 | loss: 3.1369491MixupTrain:  epoch  0, batch   356 | loss: 3.0501330MixupTrain:  epoch  0, batch   357 | loss: 3.0370531MixupTrain:  epoch  0, batch   358 | loss: 3.3013217MixupTrain:  epoch  0, batch   359 | loss: 3.2708359MixupTrain:  epoch  0, batch   360 | loss: 3.1531153MixupTrain:  epoch  0, batch   361 | loss: 2.9268684MixupTrain:  epoch  0, batch   362 | loss: 3.1565588MixupTrain:  epoch  0, batch   363 | loss: 3.3025277MixupTrain:  epoch  0, batch   364 | loss: 3.0935318MixupTrain:  epoch  0, batch   365 | loss: 3.0062585MixupTrain:  epoch  0, batch   366 | loss: 3.1676171MixupTrain:  epoch  0, batch   367 | loss: 3.1970906MixupTrain:  epoch  0, batch   368 | loss: 3.0611229MixupTrain:  epoch  0, batch   369 | loss: 3.1299655MixupTrain:  epoch  0, batch   370 | loss: 2.9537432MixupTrain:  epoch  0, batch   371 | loss: 3.0326743MixupTrain:  epoch  0, batch   372 | loss: 3.4198384MixupTrain:  epoch  0, batch   373 | loss: 3.0783014MixupTrain:  epoch  0, batch   374 | loss: 3.0650823MixupTrain:  epoch  0, batch   375 | loss: 3.3024516MixupTrain:  epoch  0, batch   376 | loss: 3.1098778MixupTrain:  epoch  0, batch   377 | loss: 3.0800254MixupTrain:  epoch  0, batch   378 | loss: 3.1033468MixupTrain:  epoch  0, batch   379 | loss: 3.1853809MixupTrain:  epoch  0, batch   380 | loss: 3.0696287MixupTrain:  epoch  0, batch   381 | loss: 2.9671619MixupTrain:  epoch  0, batch   382 | loss: 3.0902104MixupTrain:  epoch  0, batch   383 | loss: 2.9937716MixupTrain:  epoch  0, batch   384 | loss: 3.1233006MixupTrain:  epoch  0, batch   385 | loss: 3.1024950MixupTrain:  epoch  0, batch   386 | loss: 3.1544633MixupTrain:  epoch  0, batch   387 | loss: 3.0138574MixupTrain:  epoch  0, batch   388 | loss: 3.3065312MixupTrain:  epoch  0, batch   389 | loss: 3.1476212MixupTrain:  epoch  0, batch   390 | loss: 3.1216722MixupTrain:  epoch  0, batch   391 | loss: 3.0847144MixupTrain:  epoch  0, batch   392 | loss: 2.8628769MixupTrain:  epoch  0, batch   393 | loss: 3.1183639MixupTrain:  epoch  0, batch   394 | loss: 2.9974151MixupTrain:  epoch  0, batch   395 | loss: 3.1916161MixupTrain:  epoch  0, batch   396 | loss: 3.0664444MixupTrain:  epoch  0, batch   397 | loss: 3.1861985MixupTrain:  epoch  0, batch   398 | loss: 3.0456393MixupTrain:  epoch  0, batch   399 | loss: 3.0891390MixupTrain:  epoch  0, batch   400 | loss: 3.1645610MixupTrain:  epoch  0, batch   401 | loss: 3.2051272MixupTrain:  epoch  0, batch   402 | loss: 3.1121616MixupTrain:  epoch  0, batch   403 | loss: 3.1961350MixupTrain:  epoch  0, batch   404 | loss: 3.2375522MixupTrain:  epoch  0, batch   405 | loss: 3.2606523MixupTrain:  epoch  0, batch   406 | loss: 3.0147398MixupTrain:  epoch  0, batch   407 | loss: 3.3556433MixupTrain:  epoch  0, batch   408 | loss: 2.8858831MixupTrain:  epoch  0, batch   409 | loss: 3.0767694MixupTrain:  epoch  0, batch   410 | loss: 3.0002351MixupTrain:  epoch  0, batch   411 | loss: 3.0850608MixupTrain:  epoch  0, batch   412 | loss: 3.0663848MixupTrain:  epoch  0, batch   413 | loss: 3.0456920MixupTrain:  epoch  0, batch   414 | loss: 3.4777732MixupTrain:  epoch  0, batch   415 | loss: 3.1372674MixupTrain:  epoch  0, batch   416 | loss: 3.0476773MixupTrain:  epoch  0, batch   417 | loss: 3.1778784MixupTrain:  epoch  0, batch   418 | loss: 3.0220509MixupTrain:  epoch  0, batch   419 | loss: 3.1336908MixupTrain:  epoch  0, batch   420 | loss: 3.1708088MixupTrain:  epoch  0, batch   421 | loss: 3.2093472MixupTrain:  epoch  0, batch   422 | loss: 3.0670600MixupTrain:  epoch  0, batch   423 | loss: 2.7205739MixupTrain:  epoch  0, batch   424 | loss: 3.1060832MixupTrain:  epoch  0, batch   425 | loss: 3.0294628MixupTrain:  epoch  0, batch   426 | loss: 3.1722751MixupTrain:  epoch  0, batch   427 | loss: 3.0467801MixupTrain:  epoch  0, batch   428 | loss: 3.3469272MixupTrain:  epoch  0, batch   429 | loss: 2.9699929MixupTrain:  epoch  0, batch   430 | loss: 2.9291127MixupTrain:  epoch  0, batch   431 | loss: 3.4403372MixupTrain:  epoch  0, batch   432 | loss: 3.1784134MixupTrain:  epoch  0, batch   433 | loss: 3.1491899MixupTrain:  epoch  0, batch   434 | loss: 3.1598377MixupTrain:  epoch  0, batch   435 | loss: 3.3902087MixupTrain:  epoch  0, batch   436 | loss: 3.2045035MixupTrain:  epoch  0, batch   437 | loss: 3.3248301MixupTrain:  epoch  0, batch   438 | loss: 3.0858874MixupTrain:  epoch  0, batch   439 | loss: 3.1559405MixupTrain:  epoch  0, batch   440 | loss: 3.3604755MixupTrain:  epoch  0, batch   441 | loss: 3.2250881MixupTrain:  epoch  0, batch   442 | loss: 3.1417978MixupTrain:  epoch  0, batch   443 | loss: 3.1173110MixupTrain:  epoch  0, batch   444 | loss: 3.1565232MixupTrain:  epoch  0, batch   445 | loss: 3.0952849MixupTrain:  epoch  0, batch   446 | loss: 3.3018546MixupTrain:  epoch  0, batch   447 | loss: 3.3691821MixupTrain:  epoch  0, batch   448 | loss: 3.0507250MixupTrain:  epoch  0, batch   449 | loss: 3.1188116MixupTrain:  epoch  0, batch   450 | loss: 3.0837021MixupTrain:  epoch  0, batch   451 | loss: 3.0036733MixupTrain:  epoch  0, batch   452 | loss: 3.0059254MixupTrain:  epoch  0, batch   453 | loss: 3.2340722MixupTrain:  epoch  0, batch   454 | loss: 3.1858354MixupTrain:  epoch  0, batch   455 | loss: 3.2214034MixupTrain:  epoch  0, batch   456 | loss: 3.1723707MixupTrain:  epoch  0, batch   457 | loss: 3.0087681MixupTrain:  epoch  0, batch   458 | loss: 3.1058550MixupTrain:  epoch  0, batch   459 | loss: 3.1445971MixupTrain:  epoch  0, batch   460 | loss: 3.1250932MixupTrain:  epoch  0, batch   461 | loss: 3.0594232MixupTrain:  epoch  0, batch   462 | loss: 3.1330943MixupTrain:  epoch  0, batch   463 | loss: 3.1269631MixupTrain:  epoch  0, batch   464 | loss: 3.0064814MixupTrain:  epoch  0, batch   465 | loss: 3.1422753MixupTrain:  epoch  0, batch   466 | loss: 3.1944766MixupTrain:  epoch  0, batch   467 | loss: 3.0377765MixupTrain:  epoch  0, batch   468 | loss: 3.0625286MixupTrain:  epoch  0, batch   469 | loss: 3.1886420MixupTrain:  epoch  0, batch   470 | loss: 2.9290414MixupTrain:  epoch  0, batch   471 | loss: 3.2246122MixupTrain:  epoch  0, batch   472 | loss: 3.1683207MixupTrain:  epoch  0, batch   473 | loss: 3.1495724MixupTrain:  epoch  0, batch   474 | loss: 2.9187486MixupTrain:  epoch  0, batch   475 | loss: 3.0255644MixupTrain:  epoch  0, batch   476 | loss: 3.0837646MixupTrain:  epoch  0, batch   477 | loss: 3.1110511MixupTrain:  epoch  0, batch   478 | loss: 3.1420631MixupTrain:  epoch  0, batch   479 | loss: 2.9487813MixupTrain:  epoch  0, batch   480 | loss: 2.9826708MixupTrain:  epoch  0, batch   481 | loss: 2.8814857MixupTrain:  epoch  0, batch   482 | loss: 3.2126145MixupTrain:  epoch  0, batch   483 | loss: 3.2301702MixupTrain:  epoch  0, batch   484 | loss: 3.2068324MixupTrain:  epoch  0, batch   485 | loss: 3.1160960MixupTrain:  epoch  0, batch   486 | loss: 3.2457404MixupTrain:  epoch  0, batch   487 | loss: 3.0283515MixupTrain:  epoch  0, batch   488 | loss: 2.8707347MixupTrain:  epoch  0, batch   489 | loss: 3.0344591MixupTrain:  epoch  0, batch   490 | loss: 3.1712456MixupTrain:  epoch  0, batch   491 | loss: 3.1602390MixupTrain:  epoch  0, batch   492 | loss: 3.1755824MixupTrain:  epoch  0, batch   493 | loss: 3.1906018MixupTrain:  epoch  0, batch   494 | loss: 3.1046677MixupTrain:  epoch  0, batch   495 | loss: 3.1808724MixupTrain:  epoch  0, batch   496 | loss: 3.1533899MixupTrain:  epoch  0, batch   497 | loss: 3.3488364MixupTrain:  epoch  0, batch   498 | loss: 3.1812158MixupTrain:  epoch  0, batch   499 | loss: 3.1566629MixupTrain:  epoch  0, batch   500 | loss: 3.1065974MixupTrain:  epoch  0, batch   501 | loss: 3.2685828MixupTrain:  epoch  0, batch   502 | loss: 3.1263807MixupTrain:  epoch  0, batch   503 | loss: 3.0376956MixupTrain:  epoch  0, batch   504 | loss: 3.0300951MixupTrain:  epoch  0, batch   505 | loss: 3.1001911MixupTrain:  epoch  0, batch   506 | loss: 3.0470033MixupTrain:  epoch  0, batch   507 | loss: 3.0596645MixupTrain:  epoch  0, batch   508 | loss: 3.2617795MixupTrain:  epoch  0, batch   509 | loss: 3.0847292MixupTrain:  epoch  0, batch   510 | loss: 2.7040429MixupTrain:  epoch  0, batch   511 | loss: 3.1203556MixupTrain:  epoch  0, batch   512 | loss: 3.0635812MixupTrain:  epoch  0, batch   513 | loss: 3.1033289MixupTrain:  epoch  0, batch   514 | loss: 3.0040412MixupTrain:  epoch  0, batch   515 | loss: 2.9744306MixupTrain:  epoch  0, batch   516 | loss: 3.3803029MixupTrain:  epoch  0, batch   517 | loss: 3.3389096MixupTrain:  epoch  0, batch   518 | loss: 2.9878154MixupTrain:  epoch  0, batch   519 | loss: 2.9854579MixupTrain:  epoch  0, batch   520 | loss: 3.0946364MixupTrain:  epoch  0, batch   521 | loss: 3.1829443MixupTrain:  epoch  0, batch   522 | loss: 3.2208252MixupTrain:  epoch  0, batch   523 | loss: 3.0854516MixupTrain:  epoch  0, batch   524 | loss: 3.2734179MixupTrain:  epoch  0, batch   525 | loss: 3.1507249MixupTrain:  epoch  0, batch   526 | loss: 2.8789692MixupTrain:  epoch  0, batch   527 | loss: 3.2833633MixupTrain:  epoch  0, batch   528 | loss: 3.0866380MixupTrain:  epoch  0, batch   529 | loss: 3.1567819MixupTrain:  epoch  0, batch   530 | loss: 3.2372444MixupTrain:  epoch  0, batch   531 | loss: 3.0749857MixupTrain:  epoch  0, batch   532 | loss: 3.0780621MixupTrain:  epoch  0, batch   533 | loss: 3.0568905MixupTrain:  epoch  0, batch   534 | loss: 2.9590826
MemoryTrain:  epoch  0, batch     0 | loss: 1.5305926MemoryTrain:  epoch  0, batch     1 | loss: 1.2487271MemoryTrain:  epoch  0, batch     2 | loss: 1.7630658MemoryTrain:  epoch  0, batch     3 | loss: 1.7277570MemoryTrain:  epoch  0, batch     4 | loss: 1.5789742MemoryTrain:  epoch  0, batch     5 | loss: 1.6481131MemoryTrain:  epoch  0, batch     6 | loss: 1.7905458MemoryTrain:  epoch  1, batch     0 | loss: 1.3315585MemoryTrain:  epoch  1, batch     1 | loss: 1.4383900MemoryTrain:  epoch  1, batch     2 | loss: 1.3707392MemoryTrain:  epoch  1, batch     3 | loss: 1.3455932MemoryTrain:  epoch  1, batch     4 | loss: 1.1814076MemoryTrain:  epoch  1, batch     5 | loss: 1.2749871MemoryTrain:  epoch  1, batch     6 | loss: 1.2644570MemoryTrain:  epoch  2, batch     0 | loss: 1.2926271MemoryTrain:  epoch  2, batch     1 | loss: 1.2592667MemoryTrain:  epoch  2, batch     2 | loss: 1.3683935MemoryTrain:  epoch  2, batch     3 | loss: 1.2799222MemoryTrain:  epoch  2, batch     4 | loss: 1.2267320MemoryTrain:  epoch  2, batch     5 | loss: 1.2867093MemoryTrain:  epoch  2, batch     6 | loss: 1.2221659MemoryTrain:  epoch  3, batch     0 | loss: 1.2267890MemoryTrain:  epoch  3, batch     1 | loss: 1.2101256MemoryTrain:  epoch  3, batch     2 | loss: 1.2789060MemoryTrain:  epoch  3, batch     3 | loss: 1.2589118MemoryTrain:  epoch  3, batch     4 | loss: 1.2001113MemoryTrain:  epoch  3, batch     5 | loss: 1.2892023MemoryTrain:  epoch  3, batch     6 | loss: 1.2288089MemoryTrain:  epoch  4, batch     0 | loss: 1.2189873MemoryTrain:  epoch  4, batch     1 | loss: 1.2040176MemoryTrain:  epoch  4, batch     2 | loss: 1.1984053MemoryTrain:  epoch  4, batch     3 | loss: 1.2314035MemoryTrain:  epoch  4, batch     4 | loss: 1.1990272MemoryTrain:  epoch  4, batch     5 | loss: 1.2187381MemoryTrain:  epoch  4, batch     6 | loss: 1.2913779MemoryTrain:  epoch  5, batch     0 | loss: 1.2068077MemoryTrain:  epoch  5, batch     1 | loss: 1.2000750MemoryTrain:  epoch  5, batch     2 | loss: 1.2032983MemoryTrain:  epoch  5, batch     3 | loss: 1.2158574MemoryTrain:  epoch  5, batch     4 | loss: 1.1937497MemoryTrain:  epoch  5, batch     5 | loss: 1.2265515MemoryTrain:  epoch  5, batch     6 | loss: 1.1896629MemoryTrain:  epoch  6, batch     0 | loss: 1.2186363MemoryTrain:  epoch  6, batch     1 | loss: 1.1879966MemoryTrain:  epoch  6, batch     2 | loss: 1.2586665MemoryTrain:  epoch  6, batch     3 | loss: 1.1886573MemoryTrain:  epoch  6, batch     4 | loss: 1.2005310MemoryTrain:  epoch  6, batch     5 | loss: 1.2268044MemoryTrain:  epoch  6, batch     6 | loss: 1.2006233MemoryTrain:  epoch  7, batch     0 | loss: 1.2528327MemoryTrain:  epoch  7, batch     1 | loss: 1.1877636MemoryTrain:  epoch  7, batch     2 | loss: 1.1702420MemoryTrain:  epoch  7, batch     3 | loss: 1.2018760MemoryTrain:  epoch  7, batch     4 | loss: 1.1975510MemoryTrain:  epoch  7, batch     5 | loss: 1.1810780MemoryTrain:  epoch  7, batch     6 | loss: 1.2090862MemoryTrain:  epoch  8, batch     0 | loss: 1.1748629MemoryTrain:  epoch  8, batch     1 | loss: 1.2082777MemoryTrain:  epoch  8, batch     2 | loss: 1.2211808MemoryTrain:  epoch  8, batch     3 | loss: 1.1991823MemoryTrain:  epoch  8, batch     4 | loss: 1.2364461MemoryTrain:  epoch  8, batch     5 | loss: 1.1795685MemoryTrain:  epoch  8, batch     6 | loss: 1.2014060MemoryTrain:  epoch  9, batch     0 | loss: 1.1701763MemoryTrain:  epoch  9, batch     1 | loss: 1.1866872MemoryTrain:  epoch  9, batch     2 | loss: 1.1808110MemoryTrain:  epoch  9, batch     3 | loss: 1.1921322MemoryTrain:  epoch  9, batch     4 | loss: 1.1940410MemoryTrain:  epoch  9, batch     5 | loss: 1.1841279MemoryTrain:  epoch  9, batch     6 | loss: 1.1979673
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:    3 | acc: 56.25%,  total acc: 79.69%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 78.75%   [EVAL] batch:    5 | acc: 62.50%,  total acc: 76.04%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 75.00%   [EVAL] batch:    7 | acc: 6.25%,  total acc: 66.41%   
[EVAL] batch:    0 | acc: 12.50%,  total acc: 12.50%   [EVAL] batch:    1 | acc: 31.25%,  total acc: 21.88%   [EVAL] batch:    2 | acc: 18.75%,  total acc: 20.83%   [EVAL] batch:    3 | acc: 0.00%,  total acc: 15.62%   [EVAL] batch:    4 | acc: 18.75%,  total acc: 16.25%   [EVAL] batch:    5 | acc: 12.50%,  total acc: 15.62%   [EVAL] batch:    6 | acc: 37.50%,  total acc: 18.75%   [EVAL] batch:    7 | acc: 62.50%,  total acc: 24.22%   [EVAL] batch:    8 | acc: 62.50%,  total acc: 28.47%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 31.87%   [EVAL] batch:   10 | acc: 43.75%,  total acc: 32.95%   [EVAL] batch:   11 | acc: 37.50%,  total acc: 33.33%   [EVAL] batch:   12 | acc: 12.50%,  total acc: 31.73%   [EVAL] batch:   13 | acc: 31.25%,  total acc: 31.70%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 34.17%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 35.55%   [EVAL] batch:   16 | acc: 68.75%,  total acc: 37.50%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 38.89%   [EVAL] batch:   18 | acc: 75.00%,  total acc: 40.79%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 43.12%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 45.24%   [EVAL] batch:   21 | acc: 62.50%,  total acc: 46.02%   [EVAL] batch:   22 | acc: 87.50%,  total acc: 47.83%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 49.48%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 51.50%   [EVAL] batch:   25 | acc: 81.25%,  total acc: 52.64%   [EVAL] batch:   26 | acc: 87.50%,  total acc: 53.94%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 55.36%   [EVAL] batch:   28 | acc: 87.50%,  total acc: 56.47%   [EVAL] batch:   29 | acc: 68.75%,  total acc: 56.88%   [EVAL] batch:   30 | acc: 81.25%,  total acc: 57.66%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 58.79%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 58.14%   [EVAL] batch:   33 | acc: 0.00%,  total acc: 56.43%   [EVAL] batch:   34 | acc: 0.00%,  total acc: 54.82%   [EVAL] batch:   35 | acc: 0.00%,  total acc: 53.30%   [EVAL] batch:   36 | acc: 0.00%,  total acc: 51.86%   [EVAL] batch:   37 | acc: 0.00%,  total acc: 50.49%   [EVAL] batch:   38 | acc: 18.75%,  total acc: 49.68%   [EVAL] batch:   39 | acc: 81.25%,  total acc: 50.47%   [EVAL] batch:   40 | acc: 56.25%,  total acc: 50.61%   [EVAL] batch:   41 | acc: 6.25%,  total acc: 49.55%   [EVAL] batch:   42 | acc: 12.50%,  total acc: 48.69%   [EVAL] batch:   43 | acc: 62.50%,  total acc: 49.01%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 50.14%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 51.22%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 52.26%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 53.26%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 54.21%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 54.87%   [EVAL] batch:   50 | acc: 31.25%,  total acc: 54.41%   [EVAL] batch:   51 | acc: 6.25%,  total acc: 53.49%   [EVAL] batch:   52 | acc: 18.75%,  total acc: 52.83%   [EVAL] batch:   53 | acc: 0.00%,  total acc: 51.85%   [EVAL] batch:   54 | acc: 6.25%,  total acc: 51.02%   [EVAL] batch:   55 | acc: 0.00%,  total acc: 50.11%   [EVAL] batch:   56 | acc: 62.50%,  total acc: 50.33%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 51.08%   [EVAL] batch:   58 | acc: 81.25%,  total acc: 51.59%   [EVAL] batch:   59 | acc: 87.50%,  total acc: 52.19%   [EVAL] batch:   60 | acc: 68.75%,  total acc: 52.46%   [EVAL] batch:   61 | acc: 81.25%,  total acc: 52.92%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 52.88%   [EVAL] batch:   63 | acc: 43.75%,  total acc: 52.73%   [EVAL] batch:   64 | acc: 68.75%,  total acc: 52.98%   [EVAL] batch:   65 | acc: 56.25%,  total acc: 53.03%   [EVAL] batch:   66 | acc: 100.00%,  total acc: 53.73%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 54.41%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 55.07%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 55.71%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 56.34%   [EVAL] batch:   71 | acc: 93.75%,  total acc: 56.86%   [EVAL] batch:   72 | acc: 6.25%,  total acc: 56.16%   [EVAL] batch:   73 | acc: 12.50%,  total acc: 55.57%   [EVAL] batch:   74 | acc: 37.50%,  total acc: 55.33%   [EVAL] batch:   75 | acc: 43.75%,  total acc: 55.18%   [EVAL] batch:   76 | acc: 37.50%,  total acc: 54.95%   [EVAL] batch:   77 | acc: 31.25%,  total acc: 54.65%   [EVAL] batch:   78 | acc: 75.00%,  total acc: 54.91%   [EVAL] batch:   79 | acc: 56.25%,  total acc: 54.92%   [EVAL] batch:   80 | acc: 68.75%,  total acc: 55.09%   [EVAL] batch:   81 | acc: 75.00%,  total acc: 55.34%   [EVAL] batch:   82 | acc: 87.50%,  total acc: 55.72%   [EVAL] batch:   83 | acc: 81.25%,  total acc: 56.03%   [EVAL] batch:   84 | acc: 12.50%,  total acc: 55.51%   [EVAL] batch:   85 | acc: 0.00%,  total acc: 54.87%   [EVAL] batch:   86 | acc: 6.25%,  total acc: 54.31%   [EVAL] batch:   87 | acc: 18.75%,  total acc: 53.91%   [EVAL] batch:   88 | acc: 81.25%,  total acc: 54.21%   [EVAL] batch:   89 | acc: 75.00%,  total acc: 54.44%   [EVAL] batch:   90 | acc: 62.50%,  total acc: 54.53%   [EVAL] batch:   91 | acc: 75.00%,  total acc: 54.76%   [EVAL] batch:   92 | acc: 87.50%,  total acc: 55.11%   [EVAL] batch:   93 | acc: 68.75%,  total acc: 55.25%   [EVAL] batch:   94 | acc: 68.75%,  total acc: 55.39%   [EVAL] batch:   95 | acc: 87.50%,  total acc: 55.73%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 56.19%   [EVAL] batch:   97 | acc: 87.50%,  total acc: 56.51%   [EVAL] batch:   98 | acc: 93.75%,  total acc: 56.88%   [EVAL] batch:   99 | acc: 87.50%,  total acc: 57.19%   [EVAL] batch:  100 | acc: 75.00%,  total acc: 57.36%   [EVAL] batch:  101 | acc: 68.75%,  total acc: 57.48%   [EVAL] batch:  102 | acc: 81.25%,  total acc: 57.71%   [EVAL] batch:  103 | acc: 62.50%,  total acc: 57.75%   [EVAL] batch:  104 | acc: 81.25%,  total acc: 57.98%   [EVAL] batch:  105 | acc: 100.00%,  total acc: 58.37%   [EVAL] batch:  106 | acc: 81.25%,  total acc: 58.59%   [EVAL] batch:  107 | acc: 56.25%,  total acc: 58.56%   [EVAL] batch:  108 | acc: 75.00%,  total acc: 58.72%   [EVAL] batch:  109 | acc: 68.75%,  total acc: 58.81%   [EVAL] batch:  110 | acc: 68.75%,  total acc: 58.90%   [EVAL] batch:  111 | acc: 6.25%,  total acc: 58.43%   
cur_acc:  ['0.8712', '0.8368', '0.6741', '0.8929', '0.7019', '0.7792', '0.6641']
his_acc:  ['0.8712', '0.8475', '0.8076', '0.7548', '0.6917', '0.6631', '0.5843']
CurrentTrain: epoch  0, batch     0 | loss: 7.5050349CurrentTrain: epoch  0, batch     1 | loss: 8.9206972CurrentTrain: epoch  1, batch     0 | loss: 7.9017859CurrentTrain: epoch  1, batch     1 | loss: 6.5473175CurrentTrain: epoch  2, batch     0 | loss: 6.6023064CurrentTrain: epoch  2, batch     1 | loss: 6.6243973CurrentTrain: epoch  3, batch     0 | loss: 5.8614354CurrentTrain: epoch  3, batch     1 | loss: 5.7501788CurrentTrain: epoch  4, batch     0 | loss: 5.2653222CurrentTrain: epoch  4, batch     1 | loss: 5.2324123CurrentTrain: epoch  5, batch     0 | loss: 5.0347457CurrentTrain: epoch  5, batch     1 | loss: 4.7336187CurrentTrain: epoch  6, batch     0 | loss: 4.7253008CurrentTrain: epoch  6, batch     1 | loss: 4.1259298CurrentTrain: epoch  7, batch     0 | loss: 4.4185886CurrentTrain: epoch  7, batch     1 | loss: 4.5731111CurrentTrain: epoch  8, batch     0 | loss: 4.4722714CurrentTrain: epoch  8, batch     1 | loss: 3.6747801CurrentTrain: epoch  9, batch     0 | loss: 3.5657721CurrentTrain: epoch  9, batch     1 | loss: 4.7837648
#### organization subsidiaries ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization subsidiaries" means an organization 's subsidiary is an organization.
Here is an example:
Relation: organization subsidiaries
Context: venture fund buys sporting chain highland capital 's consumer fund includes lululemon athletica , a yoga retailer , and o beverages , a flavored water company developed by tom first , one of the two `` juice guys '' who cofounded nantucket nectars .
Head Entity: highland capital
Tail Entity: o beverages
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization subsidiaries":
Relation: organization subsidiaries  
Context: The tech giant Alphabet Inc. has several subsidiaries, including YouTube, which has become a leading platform for video sharing and streaming.  
Head Entity: Alphabet Inc.  
Tail Entity: YouTube  

Relation: organization subsidiaries  
Context: The automotive manufacturer General Motors has expanded its portfolio by acquiring Cruise Automation, a company focused on self-driving technology.  
Head Entity: General Motors  
Tail Entity: Cruise Automation  
#### organization parents ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization parents" means an organization 's parent is an organization.
Here is an example:
Relation: organization parents
Context: aseel kami , tim cocks , abdul-rahman taher , tim cocks and michael christie -lrb- reuters -rrb- add an intriguing note in the speculation over who 's responsible for the recent bombings : `` but a senior iraqi intelligence source , who declined to be named , said there was evidence the bombs could be the work of the militant badr organisation , the armed wing of the supreme islamic iraqi council -lrb- isci -rrb- .
Head Entity: badr organisation
Tail Entity: supreme islamic iraqi council
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization parents":
Relation: organization parents  
Context: In a recent merger, the tech giant Alphabet Inc. announced that it would acquire the popular video-sharing platform YouTube, which has been a subsidiary of Google since 2006. This acquisition is expected to enhance the company's digital advertising capabilities.  
Head Entity: YouTube  
Tail Entity: Alphabet Inc.  

Relation: organization parents  
Context: The historic partnership between the Ford Motor Company and the Lincoln Motor Company has led to significant advancements in automotive technology, with Lincoln being a luxury vehicle division of Ford. This relationship has allowed Lincoln to benefit from Ford's extensive research and development resources.  
Head Entity: Lincoln Motor Company  
Tail Entity: Ford Motor Company  
#### organization alternate names ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization alternate names" means a person 's alias is a person.
Here is an example:
Relation: organization alternate names
Context: the talks between graham and pak ui-chun were held in a `` friendly '' manner , the official korean central news agency -lrb- kcna -rrb- said , without giving details .
Head Entity: kcna
Tail Entity: korean central news agency
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization alternate names":
Relation: organization alternate names  
Context: The International Business Machines Corporation, commonly known as IBM, has been a leader in technology for decades.  
Head Entity: IBM  
Tail Entity: International Business Machines Corporation  

Relation: organization alternate names  
Context: The World Health Organization, often referred to as WHO, has been instrumental in addressing global health issues.  
Head Entity: WHO  
Tail Entity: World Health Organization  
#### organization city of headquarters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization city of headquarters" means an organization is located in a city.
Here is an example:
Relation: organization city of headquarters
Context: ------ london 2008-05-20 07:23:45 utc enodis plc endorses sweetened takeover bid by us company manitowoc illinois tool works of glenville , illinois , which had offered 282 pence -lrb- us$ 551 euro3 54 -rrb- per share , said monday that it was considering its position .
Head Entity: illinois tool works
Tail Entity: glenville
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization city of headquarters":
Relation: organization city of headquarters  
Context: ------ in 2015, tech giant apple inc. announced plans to expand its operations in cupertino, california, where it has been headquartered since its founding.  
Head Entity: apple inc.  
Tail Entity: cupertino  

Relation: organization city of headquarters  
Context: ------ the multinational corporation samsung electronics is based in suwon, south korea, and has been a leader in the technology sector for decades.  
Head Entity: samsung electronics  
Tail Entity: suwon  
#### person siblings ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person siblings" means a person 's sibling is a person.
Here is an example:
Relation: person siblings
Context: forsberg , a political science professor at city college of new york , died oct. 19 in a bronx hospital of cancer , said her sister , celia seupel .
Head Entity: forsberg
Tail Entity: celia seupel
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person siblings":
Relation: person siblings  
Context: during the family reunion, john introduced his sister, emily, who had just returned from studying abroad.  
Head Entity: john  
Tail Entity: emily  

Relation: person siblings  
Context: at the award ceremony, sarah proudly accepted her trophy while her brother, michael, cheered from the audience.  
Head Entity: sarah  
Tail Entity: michael  
Mixup data size:  10630
MixupTrain:  epoch  0, batch     0 | loss: 3.2505252MixupTrain:  epoch  0, batch     1 | loss: 3.7531781MixupTrain:  epoch  0, batch     2 | loss: 3.7188289MixupTrain:  epoch  0, batch     3 | loss: 3.9568222MixupTrain:  epoch  0, batch     4 | loss: 3.4431434MixupTrain:  epoch  0, batch     5 | loss: 3.6176324MixupTrain:  epoch  0, batch     6 | loss: 3.9158602MixupTrain:  epoch  0, batch     7 | loss: 3.3840325MixupTrain:  epoch  0, batch     8 | loss: 3.3577116MixupTrain:  epoch  0, batch     9 | loss: 3.8547149MixupTrain:  epoch  0, batch    10 | loss: 3.5295873MixupTrain:  epoch  0, batch    11 | loss: 3.0405691MixupTrain:  epoch  0, batch    12 | loss: 3.7225783MixupTrain:  epoch  0, batch    13 | loss: 3.7369208MixupTrain:  epoch  0, batch    14 | loss: 3.6979828MixupTrain:  epoch  0, batch    15 | loss: 3.6192298MixupTrain:  epoch  0, batch    16 | loss: 3.9716163MixupTrain:  epoch  0, batch    17 | loss: 3.6650171MixupTrain:  epoch  0, batch    18 | loss: 3.5283709MixupTrain:  epoch  0, batch    19 | loss: 3.5471361MixupTrain:  epoch  0, batch    20 | loss: 3.3267164MixupTrain:  epoch  0, batch    21 | loss: 3.6207852MixupTrain:  epoch  0, batch    22 | loss: 3.8001590MixupTrain:  epoch  0, batch    23 | loss: 2.9460411MixupTrain:  epoch  0, batch    24 | loss: 3.6753054MixupTrain:  epoch  0, batch    25 | loss: 3.3184094MixupTrain:  epoch  0, batch    26 | loss: 3.6450124MixupTrain:  epoch  0, batch    27 | loss: 3.1996393MixupTrain:  epoch  0, batch    28 | loss: 3.3744793MixupTrain:  epoch  0, batch    29 | loss: 3.1283407MixupTrain:  epoch  0, batch    30 | loss: 3.4884939MixupTrain:  epoch  0, batch    31 | loss: 3.1257319MixupTrain:  epoch  0, batch    32 | loss: 3.0547943MixupTrain:  epoch  0, batch    33 | loss: 3.4834032MixupTrain:  epoch  0, batch    34 | loss: 3.3239062MixupTrain:  epoch  0, batch    35 | loss: 3.1696558MixupTrain:  epoch  0, batch    36 | loss: 3.2996409MixupTrain:  epoch  0, batch    37 | loss: 3.4231756MixupTrain:  epoch  0, batch    38 | loss: 3.2207875MixupTrain:  epoch  0, batch    39 | loss: 3.3812099MixupTrain:  epoch  0, batch    40 | loss: 3.1878276MixupTrain:  epoch  0, batch    41 | loss: 3.1421056MixupTrain:  epoch  0, batch    42 | loss: 3.7467928MixupTrain:  epoch  0, batch    43 | loss: 3.1916480MixupTrain:  epoch  0, batch    44 | loss: 3.4875412MixupTrain:  epoch  0, batch    45 | loss: 3.3012297MixupTrain:  epoch  0, batch    46 | loss: 3.2759109MixupTrain:  epoch  0, batch    47 | loss: 2.9401898MixupTrain:  epoch  0, batch    48 | loss: 3.4145918MixupTrain:  epoch  0, batch    49 | loss: 3.3647897MixupTrain:  epoch  0, batch    50 | loss: 3.2836251MixupTrain:  epoch  0, batch    51 | loss: 3.0191083MixupTrain:  epoch  0, batch    52 | loss: 3.0980973MixupTrain:  epoch  0, batch    53 | loss: 3.2272470MixupTrain:  epoch  0, batch    54 | loss: 3.3418627MixupTrain:  epoch  0, batch    55 | loss: 3.3696179MixupTrain:  epoch  0, batch    56 | loss: 3.3565092MixupTrain:  epoch  0, batch    57 | loss: 3.1769190MixupTrain:  epoch  0, batch    58 | loss: 3.3428016MixupTrain:  epoch  0, batch    59 | loss: 3.1121724MixupTrain:  epoch  0, batch    60 | loss: 3.1600032MixupTrain:  epoch  0, batch    61 | loss: 3.1530819MixupTrain:  epoch  0, batch    62 | loss: 3.2113152MixupTrain:  epoch  0, batch    63 | loss: 3.4726224MixupTrain:  epoch  0, batch    64 | loss: 3.1327033MixupTrain:  epoch  0, batch    65 | loss: 3.1691566MixupTrain:  epoch  0, batch    66 | loss: 3.2470598MixupTrain:  epoch  0, batch    67 | loss: 3.1125951MixupTrain:  epoch  0, batch    68 | loss: 3.3097749MixupTrain:  epoch  0, batch    69 | loss: 3.1982732MixupTrain:  epoch  0, batch    70 | loss: 3.0820961MixupTrain:  epoch  0, batch    71 | loss: 3.0907397MixupTrain:  epoch  0, batch    72 | loss: 3.1875381MixupTrain:  epoch  0, batch    73 | loss: 3.0820260MixupTrain:  epoch  0, batch    74 | loss: 3.5941579MixupTrain:  epoch  0, batch    75 | loss: 3.1435156MixupTrain:  epoch  0, batch    76 | loss: 3.0441813MixupTrain:  epoch  0, batch    77 | loss: 3.0990257MixupTrain:  epoch  0, batch    78 | loss: 3.0390835MixupTrain:  epoch  0, batch    79 | loss: 3.4295468MixupTrain:  epoch  0, batch    80 | loss: 3.2897205MixupTrain:  epoch  0, batch    81 | loss: 3.1322129MixupTrain:  epoch  0, batch    82 | loss: 3.2182822MixupTrain:  epoch  0, batch    83 | loss: 3.1260002MixupTrain:  epoch  0, batch    84 | loss: 2.9951317MixupTrain:  epoch  0, batch    85 | loss: 3.1838069MixupTrain:  epoch  0, batch    86 | loss: 2.9476719MixupTrain:  epoch  0, batch    87 | loss: 3.2297144MixupTrain:  epoch  0, batch    88 | loss: 2.9962778MixupTrain:  epoch  0, batch    89 | loss: 3.1645305MixupTrain:  epoch  0, batch    90 | loss: 3.2467141MixupTrain:  epoch  0, batch    91 | loss: 3.4136999MixupTrain:  epoch  0, batch    92 | loss: 3.1425016MixupTrain:  epoch  0, batch    93 | loss: 2.8415949MixupTrain:  epoch  0, batch    94 | loss: 3.2391214MixupTrain:  epoch  0, batch    95 | loss: 3.0602689MixupTrain:  epoch  0, batch    96 | loss: 3.4151576MixupTrain:  epoch  0, batch    97 | loss: 3.5027356MixupTrain:  epoch  0, batch    98 | loss: 3.0617986MixupTrain:  epoch  0, batch    99 | loss: 3.0539565MixupTrain:  epoch  0, batch   100 | loss: 3.0410500MixupTrain:  epoch  0, batch   101 | loss: 3.1417389MixupTrain:  epoch  0, batch   102 | loss: 3.1007333MixupTrain:  epoch  0, batch   103 | loss: 3.1924691MixupTrain:  epoch  0, batch   104 | loss: 3.1913710MixupTrain:  epoch  0, batch   105 | loss: 3.1212175MixupTrain:  epoch  0, batch   106 | loss: 3.1122475MixupTrain:  epoch  0, batch   107 | loss: 3.0331120MixupTrain:  epoch  0, batch   108 | loss: 2.9209685MixupTrain:  epoch  0, batch   109 | loss: 3.1367412MixupTrain:  epoch  0, batch   110 | loss: 3.0903854MixupTrain:  epoch  0, batch   111 | loss: 3.0390835MixupTrain:  epoch  0, batch   112 | loss: 3.2742910MixupTrain:  epoch  0, batch   113 | loss: 3.1934652MixupTrain:  epoch  0, batch   114 | loss: 3.2106004MixupTrain:  epoch  0, batch   115 | loss: 2.9444516MixupTrain:  epoch  0, batch   116 | loss: 3.0856657MixupTrain:  epoch  0, batch   117 | loss: 2.7857838MixupTrain:  epoch  0, batch   118 | loss: 3.0325370MixupTrain:  epoch  0, batch   119 | loss: 3.0048497MixupTrain:  epoch  0, batch   120 | loss: 3.0140901MixupTrain:  epoch  0, batch   121 | loss: 3.1087720MixupTrain:  epoch  0, batch   122 | loss: 3.0221214MixupTrain:  epoch  0, batch   123 | loss: 3.1053181MixupTrain:  epoch  0, batch   124 | loss: 3.1766522MixupTrain:  epoch  0, batch   125 | loss: 2.8933523MixupTrain:  epoch  0, batch   126 | loss: 3.0126956MixupTrain:  epoch  0, batch   127 | loss: 3.1292863MixupTrain:  epoch  0, batch   128 | loss: 2.9683857MixupTrain:  epoch  0, batch   129 | loss: 3.0352955MixupTrain:  epoch  0, batch   130 | loss: 3.1766233MixupTrain:  epoch  0, batch   131 | loss: 2.9199319MixupTrain:  epoch  0, batch   132 | loss: 3.1496472MixupTrain:  epoch  0, batch   133 | loss: 2.9183946MixupTrain:  epoch  0, batch   134 | loss: 3.0248852MixupTrain:  epoch  0, batch   135 | loss: 3.2206357MixupTrain:  epoch  0, batch   136 | loss: 3.0240741MixupTrain:  epoch  0, batch   137 | loss: 2.9958909MixupTrain:  epoch  0, batch   138 | loss: 2.9717035MixupTrain:  epoch  0, batch   139 | loss: 2.9765444MixupTrain:  epoch  0, batch   140 | loss: 3.0440457MixupTrain:  epoch  0, batch   141 | loss: 2.9686499MixupTrain:  epoch  0, batch   142 | loss: 3.0076888MixupTrain:  epoch  0, batch   143 | loss: 3.1579275MixupTrain:  epoch  0, batch   144 | loss: 3.0740948MixupTrain:  epoch  0, batch   145 | loss: 3.2554722MixupTrain:  epoch  0, batch   146 | loss: 3.0685680MixupTrain:  epoch  0, batch   147 | loss: 3.2416444MixupTrain:  epoch  0, batch   148 | loss: 3.2091360MixupTrain:  epoch  0, batch   149 | loss: 3.0554852MixupTrain:  epoch  0, batch   150 | loss: 2.9591208MixupTrain:  epoch  0, batch   151 | loss: 3.0516386MixupTrain:  epoch  0, batch   152 | loss: 2.9708710MixupTrain:  epoch  0, batch   153 | loss: 2.9863009MixupTrain:  epoch  0, batch   154 | loss: 3.1799893MixupTrain:  epoch  0, batch   155 | loss: 2.8950369MixupTrain:  epoch  0, batch   156 | loss: 2.9544015MixupTrain:  epoch  0, batch   157 | loss: 3.0498195MixupTrain:  epoch  0, batch   158 | loss: 3.1657410MixupTrain:  epoch  0, batch   159 | loss: 3.0560756MixupTrain:  epoch  0, batch   160 | loss: 3.0722156MixupTrain:  epoch  0, batch   161 | loss: 3.0392270MixupTrain:  epoch  0, batch   162 | loss: 3.0755684MixupTrain:  epoch  0, batch   163 | loss: 2.9905257MixupTrain:  epoch  0, batch   164 | loss: 3.1780543MixupTrain:  epoch  0, batch   165 | loss: 3.1836360MixupTrain:  epoch  0, batch   166 | loss: 3.0388572MixupTrain:  epoch  0, batch   167 | loss: 3.2914038MixupTrain:  epoch  0, batch   168 | loss: 2.9316814MixupTrain:  epoch  0, batch   169 | loss: 2.8582067MixupTrain:  epoch  0, batch   170 | loss: 2.8585584MixupTrain:  epoch  0, batch   171 | loss: 2.9877796MixupTrain:  epoch  0, batch   172 | loss: 3.2503109MixupTrain:  epoch  0, batch   173 | loss: 3.0171666MixupTrain:  epoch  0, batch   174 | loss: 3.1096647MixupTrain:  epoch  0, batch   175 | loss: 3.0246015MixupTrain:  epoch  0, batch   176 | loss: 2.9831078MixupTrain:  epoch  0, batch   177 | loss: 2.9575236MixupTrain:  epoch  0, batch   178 | loss: 3.2448113MixupTrain:  epoch  0, batch   179 | loss: 2.8306601MixupTrain:  epoch  0, batch   180 | loss: 3.0049291MixupTrain:  epoch  0, batch   181 | loss: 3.0602131MixupTrain:  epoch  0, batch   182 | loss: 2.8482456MixupTrain:  epoch  0, batch   183 | loss: 3.0261383MixupTrain:  epoch  0, batch   184 | loss: 3.0835638MixupTrain:  epoch  0, batch   185 | loss: 2.9623871MixupTrain:  epoch  0, batch   186 | loss: 2.9915829MixupTrain:  epoch  0, batch   187 | loss: 2.9690454MixupTrain:  epoch  0, batch   188 | loss: 2.9936483MixupTrain:  epoch  0, batch   189 | loss: 3.0356424MixupTrain:  epoch  0, batch   190 | loss: 2.8698592MixupTrain:  epoch  0, batch   191 | loss: 3.0114956MixupTrain:  epoch  0, batch   192 | loss: 3.1469314MixupTrain:  epoch  0, batch   193 | loss: 3.2381477MixupTrain:  epoch  0, batch   194 | loss: 3.1519823MixupTrain:  epoch  0, batch   195 | loss: 2.8646033MixupTrain:  epoch  0, batch   196 | loss: 3.0789590MixupTrain:  epoch  0, batch   197 | loss: 2.8462529MixupTrain:  epoch  0, batch   198 | loss: 2.9307799MixupTrain:  epoch  0, batch   199 | loss: 3.0339785MixupTrain:  epoch  0, batch   200 | loss: 3.1921849MixupTrain:  epoch  0, batch   201 | loss: 3.0198627MixupTrain:  epoch  0, batch   202 | loss: 2.9445982MixupTrain:  epoch  0, batch   203 | loss: 3.1022656MixupTrain:  epoch  0, batch   204 | loss: 3.0052373MixupTrain:  epoch  0, batch   205 | loss: 2.9299386MixupTrain:  epoch  0, batch   206 | loss: 2.9714112MixupTrain:  epoch  0, batch   207 | loss: 3.0753736MixupTrain:  epoch  0, batch   208 | loss: 3.1193938MixupTrain:  epoch  0, batch   209 | loss: 2.9663682MixupTrain:  epoch  0, batch   210 | loss: 3.1209514MixupTrain:  epoch  0, batch   211 | loss: 3.2268128MixupTrain:  epoch  0, batch   212 | loss: 3.0823424MixupTrain:  epoch  0, batch   213 | loss: 2.9691253MixupTrain:  epoch  0, batch   214 | loss: 3.1386909MixupTrain:  epoch  0, batch   215 | loss: 2.8906956MixupTrain:  epoch  0, batch   216 | loss: 2.9119062MixupTrain:  epoch  0, batch   217 | loss: 2.9658513MixupTrain:  epoch  0, batch   218 | loss: 3.0720263MixupTrain:  epoch  0, batch   219 | loss: 3.0898759MixupTrain:  epoch  0, batch   220 | loss: 2.9086857MixupTrain:  epoch  0, batch   221 | loss: 3.0789280MixupTrain:  epoch  0, batch   222 | loss: 3.0444622MixupTrain:  epoch  0, batch   223 | loss: 2.9413953MixupTrain:  epoch  0, batch   224 | loss: 3.1680260MixupTrain:  epoch  0, batch   225 | loss: 2.9434099MixupTrain:  epoch  0, batch   226 | loss: 2.8709574MixupTrain:  epoch  0, batch   227 | loss: 2.9330189MixupTrain:  epoch  0, batch   228 | loss: 3.0779319MixupTrain:  epoch  0, batch   229 | loss: 3.0460443MixupTrain:  epoch  0, batch   230 | loss: 2.9956901MixupTrain:  epoch  0, batch   231 | loss: 2.8460169MixupTrain:  epoch  0, batch   232 | loss: 2.9154630MixupTrain:  epoch  0, batch   233 | loss: 2.8545659MixupTrain:  epoch  0, batch   234 | loss: 3.0123389MixupTrain:  epoch  0, batch   235 | loss: 3.1501064MixupTrain:  epoch  0, batch   236 | loss: 2.7753000MixupTrain:  epoch  0, batch   237 | loss: 2.9721377MixupTrain:  epoch  0, batch   238 | loss: 3.0165734MixupTrain:  epoch  0, batch   239 | loss: 3.0241685MixupTrain:  epoch  0, batch   240 | loss: 3.0291033MixupTrain:  epoch  0, batch   241 | loss: 2.8964183MixupTrain:  epoch  0, batch   242 | loss: 2.9108338MixupTrain:  epoch  0, batch   243 | loss: 3.0547454MixupTrain:  epoch  0, batch   244 | loss: 2.9711723MixupTrain:  epoch  0, batch   245 | loss: 2.7609544MixupTrain:  epoch  0, batch   246 | loss: 2.9833150MixupTrain:  epoch  0, batch   247 | loss: 3.1427236MixupTrain:  epoch  0, batch   248 | loss: 3.0580187MixupTrain:  epoch  0, batch   249 | loss: 2.9873214MixupTrain:  epoch  0, batch   250 | loss: 2.9830887MixupTrain:  epoch  0, batch   251 | loss: 2.9565320MixupTrain:  epoch  0, batch   252 | loss: 2.8369999MixupTrain:  epoch  0, batch   253 | loss: 3.0753043MixupTrain:  epoch  0, batch   254 | loss: 3.0723231MixupTrain:  epoch  0, batch   255 | loss: 2.9164181MixupTrain:  epoch  0, batch   256 | loss: 2.8961079MixupTrain:  epoch  0, batch   257 | loss: 3.0648732MixupTrain:  epoch  0, batch   258 | loss: 3.1214418MixupTrain:  epoch  0, batch   259 | loss: 2.9103303MixupTrain:  epoch  0, batch   260 | loss: 2.8118045MixupTrain:  epoch  0, batch   261 | loss: 3.0696528MixupTrain:  epoch  0, batch   262 | loss: 3.2277319MixupTrain:  epoch  0, batch   263 | loss: 2.9349318MixupTrain:  epoch  0, batch   264 | loss: 3.0155721MixupTrain:  epoch  0, batch   265 | loss: 2.8831220MixupTrain:  epoch  0, batch   266 | loss: 3.0160267MixupTrain:  epoch  0, batch   267 | loss: 2.8343058MixupTrain:  epoch  0, batch   268 | loss: 3.0502396MixupTrain:  epoch  0, batch   269 | loss: 2.9712913MixupTrain:  epoch  0, batch   270 | loss: 2.8703821MixupTrain:  epoch  0, batch   271 | loss: 2.9674892MixupTrain:  epoch  0, batch   272 | loss: 2.8243835MixupTrain:  epoch  0, batch   273 | loss: 3.0476971MixupTrain:  epoch  0, batch   274 | loss: 3.0277472MixupTrain:  epoch  0, batch   275 | loss: 3.0824089MixupTrain:  epoch  0, batch   276 | loss: 3.1613121MixupTrain:  epoch  0, batch   277 | loss: 2.8414798MixupTrain:  epoch  0, batch   278 | loss: 2.9375496MixupTrain:  epoch  0, batch   279 | loss: 3.0662014MixupTrain:  epoch  0, batch   280 | loss: 2.7541425MixupTrain:  epoch  0, batch   281 | loss: 2.8049898MixupTrain:  epoch  0, batch   282 | loss: 3.0900226MixupTrain:  epoch  0, batch   283 | loss: 2.9833512MixupTrain:  epoch  0, batch   284 | loss: 2.9045191MixupTrain:  epoch  0, batch   285 | loss: 3.0662682MixupTrain:  epoch  0, batch   286 | loss: 3.0129912MixupTrain:  epoch  0, batch   287 | loss: 2.8146513MixupTrain:  epoch  0, batch   288 | loss: 2.9737821MixupTrain:  epoch  0, batch   289 | loss: 2.9290028MixupTrain:  epoch  0, batch   290 | loss: 2.8368957MixupTrain:  epoch  0, batch   291 | loss: 2.8077333MixupTrain:  epoch  0, batch   292 | loss: 2.9778934MixupTrain:  epoch  0, batch   293 | loss: 2.7802975MixupTrain:  epoch  0, batch   294 | loss: 2.7283211MixupTrain:  epoch  0, batch   295 | loss: 2.9304540MixupTrain:  epoch  0, batch   296 | loss: 2.9482577MixupTrain:  epoch  0, batch   297 | loss: 2.8808529MixupTrain:  epoch  0, batch   298 | loss: 2.8572786MixupTrain:  epoch  0, batch   299 | loss: 2.9142919MixupTrain:  epoch  0, batch   300 | loss: 2.9624875MixupTrain:  epoch  0, batch   301 | loss: 2.9578054MixupTrain:  epoch  0, batch   302 | loss: 3.1234598MixupTrain:  epoch  0, batch   303 | loss: 3.0550070MixupTrain:  epoch  0, batch   304 | loss: 3.0493741MixupTrain:  epoch  0, batch   305 | loss: 3.0464168MixupTrain:  epoch  0, batch   306 | loss: 3.0933685MixupTrain:  epoch  0, batch   307 | loss: 3.0548265MixupTrain:  epoch  0, batch   308 | loss: 2.8961499MixupTrain:  epoch  0, batch   309 | loss: 2.9837852MixupTrain:  epoch  0, batch   310 | loss: 2.9485598MixupTrain:  epoch  0, batch   311 | loss: 2.8905835MixupTrain:  epoch  0, batch   312 | loss: 3.0269318MixupTrain:  epoch  0, batch   313 | loss: 3.0123551MixupTrain:  epoch  0, batch   314 | loss: 3.0612760MixupTrain:  epoch  0, batch   315 | loss: 2.9097953MixupTrain:  epoch  0, batch   316 | loss: 3.0367374MixupTrain:  epoch  0, batch   317 | loss: 2.9306526MixupTrain:  epoch  0, batch   318 | loss: 3.0133951MixupTrain:  epoch  0, batch   319 | loss: 2.9011221MixupTrain:  epoch  0, batch   320 | loss: 2.8854575MixupTrain:  epoch  0, batch   321 | loss: 2.9722583MixupTrain:  epoch  0, batch   322 | loss: 2.9035006MixupTrain:  epoch  0, batch   323 | loss: 3.0601056MixupTrain:  epoch  0, batch   324 | loss: 3.0517797MixupTrain:  epoch  0, batch   325 | loss: 2.9566998MixupTrain:  epoch  0, batch   326 | loss: 3.0806508MixupTrain:  epoch  0, batch   327 | loss: 2.9184163MixupTrain:  epoch  0, batch   328 | loss: 2.9423203MixupTrain:  epoch  0, batch   329 | loss: 3.2644672MixupTrain:  epoch  0, batch   330 | loss: 2.9259415MixupTrain:  epoch  0, batch   331 | loss: 2.9645824MixupTrain:  epoch  0, batch   332 | loss: 2.9102349MixupTrain:  epoch  0, batch   333 | loss: 2.8012269MixupTrain:  epoch  0, batch   334 | loss: 2.9296997MixupTrain:  epoch  0, batch   335 | loss: 3.0333672MixupTrain:  epoch  0, batch   336 | loss: 2.9890790MixupTrain:  epoch  0, batch   337 | loss: 3.0117083MixupTrain:  epoch  0, batch   338 | loss: 3.0022871MixupTrain:  epoch  0, batch   339 | loss: 2.8605921MixupTrain:  epoch  0, batch   340 | loss: 2.8091841MixupTrain:  epoch  0, batch   341 | loss: 2.9039383MixupTrain:  epoch  0, batch   342 | loss: 3.0344987MixupTrain:  epoch  0, batch   343 | loss: 2.9611657MixupTrain:  epoch  0, batch   344 | loss: 3.0833058MixupTrain:  epoch  0, batch   345 | loss: 2.9561031MixupTrain:  epoch  0, batch   346 | loss: 2.8890631MixupTrain:  epoch  0, batch   347 | loss: 2.8609076MixupTrain:  epoch  0, batch   348 | loss: 2.9018130MixupTrain:  epoch  0, batch   349 | loss: 2.9002337MixupTrain:  epoch  0, batch   350 | loss: 3.0042040MixupTrain:  epoch  0, batch   351 | loss: 2.9141536MixupTrain:  epoch  0, batch   352 | loss: 2.9214687MixupTrain:  epoch  0, batch   353 | loss: 2.9039042MixupTrain:  epoch  0, batch   354 | loss: 2.9988289MixupTrain:  epoch  0, batch   355 | loss: 2.9471788MixupTrain:  epoch  0, batch   356 | loss: 2.9254344MixupTrain:  epoch  0, batch   357 | loss: 2.8091028MixupTrain:  epoch  0, batch   358 | loss: 3.0128992MixupTrain:  epoch  0, batch   359 | loss: 2.8004410MixupTrain:  epoch  0, batch   360 | loss: 2.8791814MixupTrain:  epoch  0, batch   361 | loss: 2.8084838MixupTrain:  epoch  0, batch   362 | loss: 2.8212624MixupTrain:  epoch  0, batch   363 | loss: 2.8887875MixupTrain:  epoch  0, batch   364 | loss: 2.9595788MixupTrain:  epoch  0, batch   365 | loss: 3.0215878MixupTrain:  epoch  0, batch   366 | loss: 3.0496135MixupTrain:  epoch  0, batch   367 | loss: 2.8770690MixupTrain:  epoch  0, batch   368 | loss: 2.8439038MixupTrain:  epoch  0, batch   369 | loss: 3.0023463MixupTrain:  epoch  0, batch   370 | loss: 3.0058315MixupTrain:  epoch  0, batch   371 | loss: 2.9054475MixupTrain:  epoch  0, batch   372 | loss: 2.8394265MixupTrain:  epoch  0, batch   373 | loss: 2.8319323MixupTrain:  epoch  0, batch   374 | loss: 2.9643483MixupTrain:  epoch  0, batch   375 | loss: 2.9500561MixupTrain:  epoch  0, batch   376 | loss: 3.0730829MixupTrain:  epoch  0, batch   377 | loss: 2.9653041MixupTrain:  epoch  0, batch   378 | loss: 2.9537597MixupTrain:  epoch  0, batch   379 | loss: 2.9802122MixupTrain:  epoch  0, batch   380 | loss: 2.8645654MixupTrain:  epoch  0, batch   381 | loss: 2.9448140MixupTrain:  epoch  0, batch   382 | loss: 3.0566282MixupTrain:  epoch  0, batch   383 | loss: 2.9232254MixupTrain:  epoch  0, batch   384 | loss: 2.8756511MixupTrain:  epoch  0, batch   385 | loss: 2.8889737MixupTrain:  epoch  0, batch   386 | loss: 2.8203270MixupTrain:  epoch  0, batch   387 | loss: 2.9732552MixupTrain:  epoch  0, batch   388 | loss: 2.8613493MixupTrain:  epoch  0, batch   389 | loss: 2.8081746MixupTrain:  epoch  0, batch   390 | loss: 2.8377261MixupTrain:  epoch  0, batch   391 | loss: 2.8282654MixupTrain:  epoch  0, batch   392 | loss: 2.8739076MixupTrain:  epoch  0, batch   393 | loss: 2.8861313MixupTrain:  epoch  0, batch   394 | loss: 2.8926857MixupTrain:  epoch  0, batch   395 | loss: 2.9766216MixupTrain:  epoch  0, batch   396 | loss: 2.9590201MixupTrain:  epoch  0, batch   397 | loss: 2.9470766MixupTrain:  epoch  0, batch   398 | loss: 2.9727883MixupTrain:  epoch  0, batch   399 | loss: 2.8625252MixupTrain:  epoch  0, batch   400 | loss: 2.9174299MixupTrain:  epoch  0, batch   401 | loss: 2.8858945MixupTrain:  epoch  0, batch   402 | loss: 2.9199870MixupTrain:  epoch  0, batch   403 | loss: 2.9688373MixupTrain:  epoch  0, batch   404 | loss: 2.8869200MixupTrain:  epoch  0, batch   405 | loss: 2.9282775MixupTrain:  epoch  0, batch   406 | loss: 2.8979635MixupTrain:  epoch  0, batch   407 | loss: 3.0296931MixupTrain:  epoch  0, batch   408 | loss: 2.8532443MixupTrain:  epoch  0, batch   409 | loss: 2.8659296MixupTrain:  epoch  0, batch   410 | loss: 2.8956776MixupTrain:  epoch  0, batch   411 | loss: 2.8052495MixupTrain:  epoch  0, batch   412 | loss: 2.8336785MixupTrain:  epoch  0, batch   413 | loss: 2.9842710MixupTrain:  epoch  0, batch   414 | loss: 3.0033078MixupTrain:  epoch  0, batch   415 | loss: 2.9610620MixupTrain:  epoch  0, batch   416 | loss: 2.9008663MixupTrain:  epoch  0, batch   417 | loss: 2.7992857MixupTrain:  epoch  0, batch   418 | loss: 3.0136085MixupTrain:  epoch  0, batch   419 | loss: 2.9911041MixupTrain:  epoch  0, batch   420 | loss: 2.9298823MixupTrain:  epoch  0, batch   421 | loss: 2.8485827MixupTrain:  epoch  0, batch   422 | loss: 2.9680459MixupTrain:  epoch  0, batch   423 | loss: 2.9663177MixupTrain:  epoch  0, batch   424 | loss: 2.8998299MixupTrain:  epoch  0, batch   425 | loss: 3.0543003MixupTrain:  epoch  0, batch   426 | loss: 2.7416861MixupTrain:  epoch  0, batch   427 | loss: 2.9555209MixupTrain:  epoch  0, batch   428 | loss: 2.8165200MixupTrain:  epoch  0, batch   429 | loss: 3.0825067MixupTrain:  epoch  0, batch   430 | loss: 3.0392344MixupTrain:  epoch  0, batch   431 | loss: 2.8180664MixupTrain:  epoch  0, batch   432 | loss: 2.8460445MixupTrain:  epoch  0, batch   433 | loss: 2.7999911MixupTrain:  epoch  0, batch   434 | loss: 2.8222444MixupTrain:  epoch  0, batch   435 | loss: 2.8883848MixupTrain:  epoch  0, batch   436 | loss: 2.8234701MixupTrain:  epoch  0, batch   437 | loss: 2.9656277MixupTrain:  epoch  0, batch   438 | loss: 2.8520873MixupTrain:  epoch  0, batch   439 | loss: 2.9725850MixupTrain:  epoch  0, batch   440 | loss: 2.8981824MixupTrain:  epoch  0, batch   441 | loss: 2.9345655MixupTrain:  epoch  0, batch   442 | loss: 2.9823546MixupTrain:  epoch  0, batch   443 | loss: 2.9165497MixupTrain:  epoch  0, batch   444 | loss: 2.8925550MixupTrain:  epoch  0, batch   445 | loss: 3.0468237MixupTrain:  epoch  0, batch   446 | loss: 2.9268794MixupTrain:  epoch  0, batch   447 | loss: 3.1675220MixupTrain:  epoch  0, batch   448 | loss: 2.8681407MixupTrain:  epoch  0, batch   449 | loss: 2.8997297MixupTrain:  epoch  0, batch   450 | loss: 2.9211774MixupTrain:  epoch  0, batch   451 | loss: 2.9678237MixupTrain:  epoch  0, batch   452 | loss: 2.8400979MixupTrain:  epoch  0, batch   453 | loss: 3.0588639MixupTrain:  epoch  0, batch   454 | loss: 2.7953422MixupTrain:  epoch  0, batch   455 | loss: 2.9236994MixupTrain:  epoch  0, batch   456 | loss: 2.9347713MixupTrain:  epoch  0, batch   457 | loss: 2.8031998MixupTrain:  epoch  0, batch   458 | loss: 2.9293418MixupTrain:  epoch  0, batch   459 | loss: 3.0257049MixupTrain:  epoch  0, batch   460 | loss: 3.0598330MixupTrain:  epoch  0, batch   461 | loss: 2.8327453MixupTrain:  epoch  0, batch   462 | loss: 2.9867601MixupTrain:  epoch  0, batch   463 | loss: 2.9314959MixupTrain:  epoch  0, batch   464 | loss: 2.8386426MixupTrain:  epoch  0, batch   465 | loss: 2.9534168MixupTrain:  epoch  0, batch   466 | loss: 2.9208140MixupTrain:  epoch  0, batch   467 | loss: 2.9790022MixupTrain:  epoch  0, batch   468 | loss: 3.0120263MixupTrain:  epoch  0, batch   469 | loss: 2.8717430MixupTrain:  epoch  0, batch   470 | loss: 2.8940778MixupTrain:  epoch  0, batch   471 | loss: 2.8561068MixupTrain:  epoch  0, batch   472 | loss: 3.0539174MixupTrain:  epoch  0, batch   473 | loss: 2.7658391MixupTrain:  epoch  0, batch   474 | loss: 2.8084970MixupTrain:  epoch  0, batch   475 | loss: 2.9236941MixupTrain:  epoch  0, batch   476 | loss: 2.9272881MixupTrain:  epoch  0, batch   477 | loss: 2.8416185MixupTrain:  epoch  0, batch   478 | loss: 2.8299592MixupTrain:  epoch  0, batch   479 | loss: 2.8214080MixupTrain:  epoch  0, batch   480 | loss: 2.9146290MixupTrain:  epoch  0, batch   481 | loss: 3.0385389MixupTrain:  epoch  0, batch   482 | loss: 2.8975663MixupTrain:  epoch  0, batch   483 | loss: 2.9564824MixupTrain:  epoch  0, batch   484 | loss: 2.8856897MixupTrain:  epoch  0, batch   485 | loss: 2.9804487MixupTrain:  epoch  0, batch   486 | loss: 2.8497095MixupTrain:  epoch  0, batch   487 | loss: 2.9584022MixupTrain:  epoch  0, batch   488 | loss: 2.9365082MixupTrain:  epoch  0, batch   489 | loss: 2.9349952MixupTrain:  epoch  0, batch   490 | loss: 2.7070961MixupTrain:  epoch  0, batch   491 | loss: 2.8245640MixupTrain:  epoch  0, batch   492 | loss: 2.8619437MixupTrain:  epoch  0, batch   493 | loss: 2.8727908MixupTrain:  epoch  0, batch   494 | loss: 2.9135065MixupTrain:  epoch  0, batch   495 | loss: 2.9028802MixupTrain:  epoch  0, batch   496 | loss: 2.9792337MixupTrain:  epoch  0, batch   497 | loss: 2.8710961MixupTrain:  epoch  0, batch   498 | loss: 2.8657653MixupTrain:  epoch  0, batch   499 | loss: 2.8569379MixupTrain:  epoch  0, batch   500 | loss: 2.8899107MixupTrain:  epoch  0, batch   501 | loss: 2.8527718MixupTrain:  epoch  0, batch   502 | loss: 2.8451385MixupTrain:  epoch  0, batch   503 | loss: 2.9382911MixupTrain:  epoch  0, batch   504 | loss: 2.9510069MixupTrain:  epoch  0, batch   505 | loss: 2.8324497MixupTrain:  epoch  0, batch   506 | loss: 2.7780104MixupTrain:  epoch  0, batch   507 | loss: 2.9223990MixupTrain:  epoch  0, batch   508 | loss: 2.8390379MixupTrain:  epoch  0, batch   509 | loss: 2.9682248MixupTrain:  epoch  0, batch   510 | loss: 2.9784858MixupTrain:  epoch  0, batch   511 | loss: 2.8732526MixupTrain:  epoch  0, batch   512 | loss: 2.7577240MixupTrain:  epoch  0, batch   513 | loss: 2.8351717MixupTrain:  epoch  0, batch   514 | loss: 2.8319535MixupTrain:  epoch  0, batch   515 | loss: 2.9013953MixupTrain:  epoch  0, batch   516 | loss: 2.9689584MixupTrain:  epoch  0, batch   517 | loss: 2.9097915MixupTrain:  epoch  0, batch   518 | loss: 2.8130512MixupTrain:  epoch  0, batch   519 | loss: 2.9037905MixupTrain:  epoch  0, batch   520 | loss: 2.8769403MixupTrain:  epoch  0, batch   521 | loss: 2.9242511MixupTrain:  epoch  0, batch   522 | loss: 2.8492913MixupTrain:  epoch  0, batch   523 | loss: 2.8304851MixupTrain:  epoch  0, batch   524 | loss: 3.0302486MixupTrain:  epoch  0, batch   525 | loss: 2.7894125MixupTrain:  epoch  0, batch   526 | loss: 2.9302018MixupTrain:  epoch  0, batch   527 | loss: 2.9062512MixupTrain:  epoch  0, batch   528 | loss: 2.8746059MixupTrain:  epoch  0, batch   529 | loss: 2.9096084MixupTrain:  epoch  0, batch   530 | loss: 3.0689147MixupTrain:  epoch  0, batch   531 | loss: 3.0069740MixupTrain:  epoch  0, batch   532 | loss: 2.8949022MixupTrain:  epoch  0, batch   533 | loss: 2.9514890MixupTrain:  epoch  0, batch   534 | loss: 2.9538736MixupTrain:  epoch  0, batch   535 | loss: 2.9497914MixupTrain:  epoch  0, batch   536 | loss: 3.0301666MixupTrain:  epoch  0, batch   537 | loss: 2.9344566MixupTrain:  epoch  0, batch   538 | loss: 3.0200877MixupTrain:  epoch  0, batch   539 | loss: 2.9891763MixupTrain:  epoch  0, batch   540 | loss: 2.8578575MixupTrain:  epoch  0, batch   541 | loss: 2.9280899MixupTrain:  epoch  0, batch   542 | loss: 2.9368904MixupTrain:  epoch  0, batch   543 | loss: 2.8746471MixupTrain:  epoch  0, batch   544 | loss: 2.9952395MixupTrain:  epoch  0, batch   545 | loss: 2.9674273MixupTrain:  epoch  0, batch   546 | loss: 2.8486419MixupTrain:  epoch  0, batch   547 | loss: 2.9509828MixupTrain:  epoch  0, batch   548 | loss: 2.9333274MixupTrain:  epoch  0, batch   549 | loss: 3.0689321MixupTrain:  epoch  0, batch   550 | loss: 2.9665561MixupTrain:  epoch  0, batch   551 | loss: 2.9275382MixupTrain:  epoch  0, batch   552 | loss: 2.8433363MixupTrain:  epoch  0, batch   553 | loss: 2.9811642MixupTrain:  epoch  0, batch   554 | loss: 2.9981527MixupTrain:  epoch  0, batch   555 | loss: 2.9443626MixupTrain:  epoch  0, batch   556 | loss: 2.7515268MixupTrain:  epoch  0, batch   557 | loss: 2.8784945MixupTrain:  epoch  0, batch   558 | loss: 2.8922377MixupTrain:  epoch  0, batch   559 | loss: 2.9618702MixupTrain:  epoch  0, batch   560 | loss: 2.8917665MixupTrain:  epoch  0, batch   561 | loss: 2.8170302MixupTrain:  epoch  0, batch   562 | loss: 2.8390775MixupTrain:  epoch  0, batch   563 | loss: 2.6838770MixupTrain:  epoch  0, batch   564 | loss: 3.0071092MixupTrain:  epoch  0, batch   565 | loss: 2.9560828MixupTrain:  epoch  0, batch   566 | loss: 2.8254690MixupTrain:  epoch  0, batch   567 | loss: 2.8912342MixupTrain:  epoch  0, batch   568 | loss: 2.9492929MixupTrain:  epoch  0, batch   569 | loss: 2.8502920MixupTrain:  epoch  0, batch   570 | loss: 2.9010098MixupTrain:  epoch  0, batch   571 | loss: 3.0140254MixupTrain:  epoch  0, batch   572 | loss: 2.9948471MixupTrain:  epoch  0, batch   573 | loss: 2.9730172MixupTrain:  epoch  0, batch   574 | loss: 2.9090707MixupTrain:  epoch  0, batch   575 | loss: 2.9079733MixupTrain:  epoch  0, batch   576 | loss: 2.8075452MixupTrain:  epoch  0, batch   577 | loss: 2.8063035MixupTrain:  epoch  0, batch   578 | loss: 3.0074630MixupTrain:  epoch  0, batch   579 | loss: 3.0215175MixupTrain:  epoch  0, batch   580 | loss: 3.1049638MixupTrain:  epoch  0, batch   581 | loss: 2.9912276MixupTrain:  epoch  0, batch   582 | loss: 2.8799334MixupTrain:  epoch  0, batch   583 | loss: 2.8347282MixupTrain:  epoch  0, batch   584 | loss: 2.8900392MixupTrain:  epoch  0, batch   585 | loss: 2.8846025MixupTrain:  epoch  0, batch   586 | loss: 2.9090781MixupTrain:  epoch  0, batch   587 | loss: 2.9171269MixupTrain:  epoch  0, batch   588 | loss: 2.8683400MixupTrain:  epoch  0, batch   589 | loss: 2.8948207MixupTrain:  epoch  0, batch   590 | loss: 2.9934316MixupTrain:  epoch  0, batch   591 | loss: 2.8877559MixupTrain:  epoch  0, batch   592 | loss: 2.9371300MixupTrain:  epoch  0, batch   593 | loss: 2.8822217MixupTrain:  epoch  0, batch   594 | loss: 2.9497740MixupTrain:  epoch  0, batch   595 | loss: 2.9978123MixupTrain:  epoch  0, batch   596 | loss: 2.9495783MixupTrain:  epoch  0, batch   597 | loss: 2.8742197MixupTrain:  epoch  0, batch   598 | loss: 2.9649501MixupTrain:  epoch  0, batch   599 | loss: 3.0438888MixupTrain:  epoch  0, batch   600 | loss: 2.8644161MixupTrain:  epoch  0, batch   601 | loss: 2.7915306MixupTrain:  epoch  0, batch   602 | loss: 2.8736372MixupTrain:  epoch  0, batch   603 | loss: 2.8941429MixupTrain:  epoch  0, batch   604 | loss: 2.9836388MixupTrain:  epoch  0, batch   605 | loss: 2.9272094MixupTrain:  epoch  0, batch   606 | loss: 2.8741860MixupTrain:  epoch  0, batch   607 | loss: 2.9567962MixupTrain:  epoch  0, batch   608 | loss: 2.8571327MixupTrain:  epoch  0, batch   609 | loss: 2.9820313MixupTrain:  epoch  0, batch   610 | loss: 2.8627715MixupTrain:  epoch  0, batch   611 | loss: 2.9154644MixupTrain:  epoch  0, batch   612 | loss: 3.0425990MixupTrain:  epoch  0, batch   613 | loss: 2.8471477MixupTrain:  epoch  0, batch   614 | loss: 2.9263809MixupTrain:  epoch  0, batch   615 | loss: 2.8793371MixupTrain:  epoch  0, batch   616 | loss: 2.8432479MixupTrain:  epoch  0, batch   617 | loss: 2.9021804MixupTrain:  epoch  0, batch   618 | loss: 2.8660159MixupTrain:  epoch  0, batch   619 | loss: 2.8979139MixupTrain:  epoch  0, batch   620 | loss: 2.8241606MixupTrain:  epoch  0, batch   621 | loss: 2.9564457MixupTrain:  epoch  0, batch   622 | loss: 2.8637471MixupTrain:  epoch  0, batch   623 | loss: 2.9497325MixupTrain:  epoch  0, batch   624 | loss: 2.8969998MixupTrain:  epoch  0, batch   625 | loss: 2.9390450MixupTrain:  epoch  0, batch   626 | loss: 2.8969915MixupTrain:  epoch  0, batch   627 | loss: 2.9028199MixupTrain:  epoch  0, batch   628 | loss: 2.9643013MixupTrain:  epoch  0, batch   629 | loss: 2.9128234MixupTrain:  epoch  0, batch   630 | loss: 2.9115338MixupTrain:  epoch  0, batch   631 | loss: 2.9537539MixupTrain:  epoch  0, batch   632 | loss: 2.8231282MixupTrain:  epoch  0, batch   633 | loss: 2.9739830MixupTrain:  epoch  0, batch   634 | loss: 2.9283538MixupTrain:  epoch  0, batch   635 | loss: 2.8143945MixupTrain:  epoch  0, batch   636 | loss: 2.9235973MixupTrain:  epoch  0, batch   637 | loss: 3.0285795MixupTrain:  epoch  0, batch   638 | loss: 2.9585114MixupTrain:  epoch  0, batch   639 | loss: 2.8758807MixupTrain:  epoch  0, batch   640 | loss: 2.8476756MixupTrain:  epoch  0, batch   641 | loss: 2.9342718MixupTrain:  epoch  0, batch   642 | loss: 2.8708754MixupTrain:  epoch  0, batch   643 | loss: 2.8849630MixupTrain:  epoch  0, batch   644 | loss: 2.9018040MixupTrain:  epoch  0, batch   645 | loss: 2.8825266MixupTrain:  epoch  0, batch   646 | loss: 2.9478483MixupTrain:  epoch  0, batch   647 | loss: 2.9638233MixupTrain:  epoch  0, batch   648 | loss: 2.8852487MixupTrain:  epoch  0, batch   649 | loss: 2.9569192MixupTrain:  epoch  0, batch   650 | loss: 2.8835087MixupTrain:  epoch  0, batch   651 | loss: 2.8595476MixupTrain:  epoch  0, batch   652 | loss: 2.9654288MixupTrain:  epoch  0, batch   653 | loss: 2.8580303MixupTrain:  epoch  0, batch   654 | loss: 2.8622561MixupTrain:  epoch  0, batch   655 | loss: 2.8649621MixupTrain:  epoch  0, batch   656 | loss: 2.8710568MixupTrain:  epoch  0, batch   657 | loss: 3.0283394MixupTrain:  epoch  0, batch   658 | loss: 2.8980913MixupTrain:  epoch  0, batch   659 | loss: 2.9962566MixupTrain:  epoch  0, batch   660 | loss: 2.8828623MixupTrain:  epoch  0, batch   661 | loss: 2.8964181MixupTrain:  epoch  0, batch   662 | loss: 3.0956488MixupTrain:  epoch  0, batch   663 | loss: 3.0251858MixupTrain:  epoch  0, batch   664 | loss: 2.8178649
MemoryTrain:  epoch  0, batch     0 | loss: 1.2907054MemoryTrain:  epoch  0, batch     1 | loss: 1.2541335MemoryTrain:  epoch  0, batch     2 | loss: 1.4620419MemoryTrain:  epoch  0, batch     3 | loss: 2.0225101MemoryTrain:  epoch  0, batch     4 | loss: 1.4605956MemoryTrain:  epoch  0, batch     5 | loss: 1.7282686MemoryTrain:  epoch  0, batch     6 | loss: 1.7002113MemoryTrain:  epoch  0, batch     7 | loss: 1.8624842MemoryTrain:  epoch  1, batch     0 | loss: 1.4279971MemoryTrain:  epoch  1, batch     1 | loss: 1.4435056MemoryTrain:  epoch  1, batch     2 | loss: 1.3319519MemoryTrain:  epoch  1, batch     3 | loss: 1.2224083MemoryTrain:  epoch  1, batch     4 | loss: 1.3883832MemoryTrain:  epoch  1, batch     5 | loss: 1.4784698MemoryTrain:  epoch  1, batch     6 | loss: 1.3631561MemoryTrain:  epoch  1, batch     7 | loss: 1.2256583MemoryTrain:  epoch  2, batch     0 | loss: 1.2361729MemoryTrain:  epoch  2, batch     1 | loss: 1.2411798MemoryTrain:  epoch  2, batch     2 | loss: 1.2512890MemoryTrain:  epoch  2, batch     3 | loss: 1.1970457MemoryTrain:  epoch  2, batch     4 | loss: 1.2788799MemoryTrain:  epoch  2, batch     5 | loss: 1.2439654MemoryTrain:  epoch  2, batch     6 | loss: 1.2145152MemoryTrain:  epoch  2, batch     7 | loss: 1.2125596MemoryTrain:  epoch  3, batch     0 | loss: 1.1940002MemoryTrain:  epoch  3, batch     1 | loss: 1.2363465MemoryTrain:  epoch  3, batch     2 | loss: 1.1885923MemoryTrain:  epoch  3, batch     3 | loss: 1.2389375MemoryTrain:  epoch  3, batch     4 | loss: 1.2182732MemoryTrain:  epoch  3, batch     5 | loss: 1.2058015MemoryTrain:  epoch  3, batch     6 | loss: 1.2276340MemoryTrain:  epoch  3, batch     7 | loss: 1.1932688MemoryTrain:  epoch  4, batch     0 | loss: 1.2731518MemoryTrain:  epoch  4, batch     1 | loss: 1.1979811MemoryTrain:  epoch  4, batch     2 | loss: 1.1837907MemoryTrain:  epoch  4, batch     3 | loss: 1.2072397MemoryTrain:  epoch  4, batch     4 | loss: 1.2392416MemoryTrain:  epoch  4, batch     5 | loss: 1.2037762MemoryTrain:  epoch  4, batch     6 | loss: 1.2122223MemoryTrain:  epoch  4, batch     7 | loss: 1.1830585MemoryTrain:  epoch  5, batch     0 | loss: 1.1912677MemoryTrain:  epoch  5, batch     1 | loss: 1.2024949MemoryTrain:  epoch  5, batch     2 | loss: 1.1916869MemoryTrain:  epoch  5, batch     3 | loss: 1.2044797MemoryTrain:  epoch  5, batch     4 | loss: 1.2066377MemoryTrain:  epoch  5, batch     5 | loss: 1.2577021MemoryTrain:  epoch  5, batch     6 | loss: 1.3154886MemoryTrain:  epoch  5, batch     7 | loss: 1.2509726MemoryTrain:  epoch  6, batch     0 | loss: 1.1737988MemoryTrain:  epoch  6, batch     1 | loss: 1.1892573MemoryTrain:  epoch  6, batch     2 | loss: 1.2057316MemoryTrain:  epoch  6, batch     3 | loss: 1.1945256MemoryTrain:  epoch  6, batch     4 | loss: 1.1786828MemoryTrain:  epoch  6, batch     5 | loss: 1.1928160MemoryTrain:  epoch  6, batch     6 | loss: 1.1940482MemoryTrain:  epoch  6, batch     7 | loss: 1.1691833MemoryTrain:  epoch  7, batch     0 | loss: 1.1825888MemoryTrain:  epoch  7, batch     1 | loss: 1.1751214MemoryTrain:  epoch  7, batch     2 | loss: 1.1761966MemoryTrain:  epoch  7, batch     3 | loss: 1.1938328MemoryTrain:  epoch  7, batch     4 | loss: 1.1921818MemoryTrain:  epoch  7, batch     5 | loss: 1.1817379MemoryTrain:  epoch  7, batch     6 | loss: 1.1884748MemoryTrain:  epoch  7, batch     7 | loss: 1.2114131MemoryTrain:  epoch  8, batch     0 | loss: 1.1788992MemoryTrain:  epoch  8, batch     1 | loss: 1.2235854MemoryTrain:  epoch  8, batch     2 | loss: 1.2236879MemoryTrain:  epoch  8, batch     3 | loss: 1.1744385MemoryTrain:  epoch  8, batch     4 | loss: 1.1819595MemoryTrain:  epoch  8, batch     5 | loss: 1.1445144MemoryTrain:  epoch  8, batch     6 | loss: 1.2014060MemoryTrain:  epoch  8, batch     7 | loss: 1.1903546MemoryTrain:  epoch  9, batch     0 | loss: 1.1718040MemoryTrain:  epoch  9, batch     1 | loss: 1.1776383MemoryTrain:  epoch  9, batch     2 | loss: 1.2085564MemoryTrain:  epoch  9, batch     3 | loss: 1.1677599MemoryTrain:  epoch  9, batch     4 | loss: 1.1772742MemoryTrain:  epoch  9, batch     5 | loss: 1.2263989MemoryTrain:  epoch  9, batch     6 | loss: 1.2179240MemoryTrain:  epoch  9, batch     7 | loss: 1.2325914
[EVAL] batch:    0 | acc: 12.50%,  total acc: 12.50%   [EVAL] batch:    1 | acc: 31.25%,  total acc: 21.88%   [EVAL] batch:    2 | acc: 25.00%,  total acc: 22.92%   [EVAL] batch:    3 | acc: 43.75%,  total acc: 28.12%   [EVAL] batch:    4 | acc: 18.75%,  total acc: 26.25%   [EVAL] batch:    5 | acc: 37.50%,  total acc: 28.12%   [EVAL] batch:    6 | acc: 43.75%,  total acc: 30.36%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 36.72%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 41.67%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 45.62%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 49.43%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 53.12%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 56.25%   [EVAL] batch:   13 | acc: 100.00%,  total acc: 59.38%   [EVAL] batch:   14 | acc: 100.00%,  total acc: 62.08%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 64.45%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 66.54%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 68.06%   [EVAL] batch:   18 | acc: 87.50%,  total acc: 69.08%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 69.69%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 70.83%   [EVAL] batch:   21 | acc: 37.50%,  total acc: 69.32%   
[EVAL] batch:    0 | acc: 0.00%,  total acc: 0.00%   [EVAL] batch:    1 | acc: 18.75%,  total acc: 9.38%   [EVAL] batch:    2 | acc: 12.50%,  total acc: 10.42%   [EVAL] batch:    3 | acc: 0.00%,  total acc: 7.81%   [EVAL] batch:    4 | acc: 12.50%,  total acc: 8.75%   [EVAL] batch:    5 | acc: 12.50%,  total acc: 9.38%   [EVAL] batch:    6 | acc: 31.25%,  total acc: 12.50%   [EVAL] batch:    7 | acc: 68.75%,  total acc: 19.53%   [EVAL] batch:    8 | acc: 62.50%,  total acc: 24.31%   [EVAL] batch:    9 | acc: 56.25%,  total acc: 27.50%   [EVAL] batch:   10 | acc: 68.75%,  total acc: 31.25%   [EVAL] batch:   11 | acc: 56.25%,  total acc: 33.33%   [EVAL] batch:   12 | acc: 12.50%,  total acc: 31.73%   [EVAL] batch:   13 | acc: 25.00%,  total acc: 31.25%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 33.75%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 35.16%   [EVAL] batch:   16 | acc: 68.75%,  total acc: 37.13%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 38.54%   [EVAL] batch:   18 | acc: 81.25%,  total acc: 40.79%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 43.44%   [EVAL] batch:   20 | acc: 75.00%,  total acc: 44.94%   [EVAL] batch:   21 | acc: 43.75%,  total acc: 44.89%   [EVAL] batch:   22 | acc: 56.25%,  total acc: 45.38%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 47.14%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 48.75%   [EVAL] batch:   25 | acc: 75.00%,  total acc: 49.76%   [EVAL] batch:   26 | acc: 62.50%,  total acc: 50.23%   [EVAL] batch:   27 | acc: 43.75%,  total acc: 50.00%   [EVAL] batch:   28 | acc: 56.25%,  total acc: 50.22%   [EVAL] batch:   29 | acc: 56.25%,  total acc: 50.42%   [EVAL] batch:   30 | acc: 43.75%,  total acc: 50.20%   [EVAL] batch:   31 | acc: 50.00%,  total acc: 50.20%   [EVAL] batch:   32 | acc: 18.75%,  total acc: 49.24%   [EVAL] batch:   33 | acc: 0.00%,  total acc: 47.79%   [EVAL] batch:   34 | acc: 0.00%,  total acc: 46.43%   [EVAL] batch:   35 | acc: 0.00%,  total acc: 45.14%   [EVAL] batch:   36 | acc: 0.00%,  total acc: 43.92%   [EVAL] batch:   37 | acc: 0.00%,  total acc: 42.76%   [EVAL] batch:   38 | acc: 18.75%,  total acc: 42.15%   [EVAL] batch:   39 | acc: 75.00%,  total acc: 42.97%   [EVAL] batch:   40 | acc: 50.00%,  total acc: 43.14%   [EVAL] batch:   41 | acc: 18.75%,  total acc: 42.56%   [EVAL] batch:   42 | acc: 25.00%,  total acc: 42.15%   [EVAL] batch:   43 | acc: 62.50%,  total acc: 42.61%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 43.89%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 45.11%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 46.28%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 47.40%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 48.47%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 49.38%   [EVAL] batch:   50 | acc: 37.50%,  total acc: 49.14%   [EVAL] batch:   51 | acc: 12.50%,  total acc: 48.44%   [EVAL] batch:   52 | acc: 0.00%,  total acc: 47.52%   [EVAL] batch:   53 | acc: 0.00%,  total acc: 46.64%   [EVAL] batch:   54 | acc: 0.00%,  total acc: 45.80%   [EVAL] batch:   55 | acc: 0.00%,  total acc: 44.98%   [EVAL] batch:   56 | acc: 50.00%,  total acc: 45.07%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 45.80%   [EVAL] batch:   58 | acc: 81.25%,  total acc: 46.40%   [EVAL] batch:   59 | acc: 87.50%,  total acc: 47.08%   [EVAL] batch:   60 | acc: 62.50%,  total acc: 47.34%   [EVAL] batch:   61 | acc: 75.00%,  total acc: 47.78%   [EVAL] batch:   62 | acc: 56.25%,  total acc: 47.92%   [EVAL] batch:   63 | acc: 62.50%,  total acc: 48.14%   [EVAL] batch:   64 | acc: 75.00%,  total acc: 48.56%   [EVAL] batch:   65 | acc: 81.25%,  total acc: 49.05%   [EVAL] batch:   66 | acc: 100.00%,  total acc: 49.81%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 50.55%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 51.27%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 51.96%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 52.64%   [EVAL] batch:   71 | acc: 93.75%,  total acc: 53.21%   [EVAL] batch:   72 | acc: 6.25%,  total acc: 52.57%   [EVAL] batch:   73 | acc: 0.00%,  total acc: 51.86%   [EVAL] batch:   74 | acc: 43.75%,  total acc: 51.75%   [EVAL] batch:   75 | acc: 68.75%,  total acc: 51.97%   [EVAL] batch:   76 | acc: 37.50%,  total acc: 51.79%   [EVAL] batch:   77 | acc: 37.50%,  total acc: 51.60%   [EVAL] batch:   78 | acc: 68.75%,  total acc: 51.82%   [EVAL] batch:   79 | acc: 56.25%,  total acc: 51.88%   [EVAL] batch:   80 | acc: 68.75%,  total acc: 52.08%   [EVAL] batch:   81 | acc: 56.25%,  total acc: 52.13%   [EVAL] batch:   82 | acc: 81.25%,  total acc: 52.48%   [EVAL] batch:   83 | acc: 81.25%,  total acc: 52.83%   [EVAL] batch:   84 | acc: 6.25%,  total acc: 52.28%   [EVAL] batch:   85 | acc: 0.00%,  total acc: 51.67%   [EVAL] batch:   86 | acc: 0.00%,  total acc: 51.08%   [EVAL] batch:   87 | acc: 12.50%,  total acc: 50.64%   [EVAL] batch:   88 | acc: 81.25%,  total acc: 50.98%   [EVAL] batch:   89 | acc: 75.00%,  total acc: 51.25%   [EVAL] batch:   90 | acc: 50.00%,  total acc: 51.24%   [EVAL] batch:   91 | acc: 68.75%,  total acc: 51.43%   [EVAL] batch:   92 | acc: 68.75%,  total acc: 51.61%   [EVAL] batch:   93 | acc: 62.50%,  total acc: 51.73%   [EVAL] batch:   94 | acc: 75.00%,  total acc: 51.97%   [EVAL] batch:   95 | acc: 93.75%,  total acc: 52.41%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 52.90%   [EVAL] batch:   97 | acc: 87.50%,  total acc: 53.25%   [EVAL] batch:   98 | acc: 93.75%,  total acc: 53.66%   [EVAL] batch:   99 | acc: 37.50%,  total acc: 53.50%   [EVAL] batch:  100 | acc: 0.00%,  total acc: 52.97%   [EVAL] batch:  101 | acc: 0.00%,  total acc: 52.45%   [EVAL] batch:  102 | acc: 12.50%,  total acc: 52.06%   [EVAL] batch:  103 | acc: 18.75%,  total acc: 51.74%   [EVAL] batch:  104 | acc: 75.00%,  total acc: 51.96%   [EVAL] batch:  105 | acc: 93.75%,  total acc: 52.36%   [EVAL] batch:  106 | acc: 43.75%,  total acc: 52.28%   [EVAL] batch:  107 | acc: 62.50%,  total acc: 52.37%   [EVAL] batch:  108 | acc: 68.75%,  total acc: 52.52%   [EVAL] batch:  109 | acc: 62.50%,  total acc: 52.61%   [EVAL] batch:  110 | acc: 68.75%,  total acc: 52.76%   [EVAL] batch:  111 | acc: 18.75%,  total acc: 52.46%   [EVAL] batch:  112 | acc: 31.25%,  total acc: 52.27%   [EVAL] batch:  113 | acc: 18.75%,  total acc: 51.97%   [EVAL] batch:  114 | acc: 43.75%,  total acc: 51.90%   [EVAL] batch:  115 | acc: 18.75%,  total acc: 51.62%   [EVAL] batch:  116 | acc: 37.50%,  total acc: 51.50%   [EVAL] batch:  117 | acc: 31.25%,  total acc: 51.32%   [EVAL] batch:  118 | acc: 87.50%,  total acc: 51.63%   [EVAL] batch:  119 | acc: 75.00%,  total acc: 51.82%   [EVAL] batch:  120 | acc: 87.50%,  total acc: 52.12%   [EVAL] batch:  121 | acc: 81.25%,  total acc: 52.36%   [EVAL] batch:  122 | acc: 93.75%,  total acc: 52.69%   [EVAL] batch:  123 | acc: 93.75%,  total acc: 53.02%   [EVAL] batch:  124 | acc: 100.00%,  total acc: 53.40%   [EVAL] batch:  125 | acc: 100.00%,  total acc: 53.77%   [EVAL] batch:  126 | acc: 100.00%,  total acc: 54.13%   [EVAL] batch:  127 | acc: 100.00%,  total acc: 54.49%   [EVAL] batch:  128 | acc: 93.75%,  total acc: 54.80%   [EVAL] batch:  129 | acc: 93.75%,  total acc: 55.10%   [EVAL] batch:  130 | acc: 81.25%,  total acc: 55.30%   [EVAL] batch:  131 | acc: 87.50%,  total acc: 55.54%   [EVAL] batch:  132 | acc: 56.25%,  total acc: 55.55%   
cur_acc:  ['0.8712', '0.8368', '0.6741', '0.8929', '0.7019', '0.7792', '0.6641', '0.6932']
his_acc:  ['0.8712', '0.8475', '0.8076', '0.7548', '0.6917', '0.6631', '0.5843', '0.5555']
--------Round  3
seed:  400
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/test.pkl
Task_order: [7 0 1 2 5 3 4 6]
prepared data!
CurrentTrain: epoch  0, batch     0 | loss: 13.2847309CurrentTrain: epoch  0, batch     1 | loss: 13.0654793CurrentTrain: epoch  0, batch     2 | loss: 13.0602684CurrentTrain: epoch  0, batch     3 | loss: 12.8467712CurrentTrain: epoch  0, batch     4 | loss: 12.7184286CurrentTrain: epoch  0, batch     5 | loss: 12.7891693CurrentTrain: epoch  0, batch     6 | loss: 12.6818733CurrentTrain: epoch  0, batch     7 | loss: 12.3705120CurrentTrain: epoch  0, batch     8 | loss: 12.2915745CurrentTrain: epoch  0, batch     9 | loss: 12.2425079CurrentTrain: epoch  0, batch    10 | loss: 12.3455753CurrentTrain: epoch  0, batch    11 | loss: 11.9345894CurrentTrain: epoch  0, batch    12 | loss: 11.7195072CurrentTrain: epoch  0, batch    13 | loss: 11.4466705CurrentTrain: epoch  0, batch    14 | loss: 11.8823109CurrentTrain: epoch  0, batch    15 | loss: 11.6174355CurrentTrain: epoch  0, batch    16 | loss: 11.5683403CurrentTrain: epoch  0, batch    17 | loss: 11.3633032CurrentTrain: epoch  0, batch    18 | loss: 11.2391567CurrentTrain: epoch  0, batch    19 | loss: 11.1376295CurrentTrain: epoch  0, batch    20 | loss: 10.7151222CurrentTrain: epoch  0, batch    21 | loss: 11.2138157CurrentTrain: epoch  0, batch    22 | loss: 11.1371155CurrentTrain: epoch  0, batch    23 | loss: 11.5400944CurrentTrain: epoch  0, batch    24 | loss: 11.4176540CurrentTrain: epoch  0, batch    25 | loss: 11.1072712CurrentTrain: epoch  0, batch    26 | loss: 10.9390039CurrentTrain: epoch  0, batch    27 | loss: 11.1076527CurrentTrain: epoch  0, batch    28 | loss: 10.8956451CurrentTrain: epoch  0, batch    29 | loss: 10.5845385CurrentTrain: epoch  0, batch    30 | loss: 11.1783648CurrentTrain: epoch  0, batch    31 | loss: 11.0318556CurrentTrain: epoch  0, batch    32 | loss: 10.4600382CurrentTrain: epoch  0, batch    33 | loss: 10.8635902CurrentTrain: epoch  0, batch    34 | loss: 10.3801479CurrentTrain: epoch  0, batch    35 | loss: 10.5589447CurrentTrain: epoch  0, batch    36 | loss: 10.4160366CurrentTrain: epoch  0, batch    37 | loss: 10.8421173CurrentTrain: epoch  1, batch     0 | loss: 9.6221857CurrentTrain: epoch  1, batch     1 | loss: 9.9582281CurrentTrain: epoch  1, batch     2 | loss: 10.5672703CurrentTrain: epoch  1, batch     3 | loss: 10.1291962CurrentTrain: epoch  1, batch     4 | loss: 10.0491695CurrentTrain: epoch  1, batch     5 | loss: 10.0448475CurrentTrain: epoch  1, batch     6 | loss: 9.9490051CurrentTrain: epoch  1, batch     7 | loss: 9.6319885CurrentTrain: epoch  1, batch     8 | loss: 9.5466499CurrentTrain: epoch  1, batch     9 | loss: 9.7618151CurrentTrain: epoch  1, batch    10 | loss: 9.8207312CurrentTrain: epoch  1, batch    11 | loss: 10.0385981CurrentTrain: epoch  1, batch    12 | loss: 10.1160221CurrentTrain: epoch  1, batch    13 | loss: 9.5842209CurrentTrain: epoch  1, batch    14 | loss: 9.8823662CurrentTrain: epoch  1, batch    15 | loss: 8.9867229CurrentTrain: epoch  1, batch    16 | loss: 9.3629589CurrentTrain: epoch  1, batch    17 | loss: 8.8373394CurrentTrain: epoch  1, batch    18 | loss: 9.3530197CurrentTrain: epoch  1, batch    19 | loss: 9.5269966CurrentTrain: epoch  1, batch    20 | loss: 9.5057163CurrentTrain: epoch  1, batch    21 | loss: 8.9414082CurrentTrain: epoch  1, batch    22 | loss: 9.6907396CurrentTrain: epoch  1, batch    23 | loss: 9.2670441CurrentTrain: epoch  1, batch    24 | loss: 9.7144413CurrentTrain: epoch  1, batch    25 | loss: 9.3308926CurrentTrain: epoch  1, batch    26 | loss: 8.8267193CurrentTrain: epoch  1, batch    27 | loss: 9.4979153CurrentTrain: epoch  1, batch    28 | loss: 8.2073174CurrentTrain: epoch  1, batch    29 | loss: 8.4685268CurrentTrain: epoch  1, batch    30 | loss: 8.6007729CurrentTrain: epoch  1, batch    31 | loss: 8.3434410CurrentTrain: epoch  1, batch    32 | loss: 8.3764668CurrentTrain: epoch  1, batch    33 | loss: 8.1280518CurrentTrain: epoch  1, batch    34 | loss: 9.0519018CurrentTrain: epoch  1, batch    35 | loss: 7.9296837CurrentTrain: epoch  1, batch    36 | loss: 8.7114353CurrentTrain: epoch  1, batch    37 | loss: 6.8525310CurrentTrain: epoch  2, batch     0 | loss: 7.1741939CurrentTrain: epoch  2, batch     1 | loss: 8.6103783CurrentTrain: epoch  2, batch     2 | loss: 9.1000671CurrentTrain: epoch  2, batch     3 | loss: 8.1225576CurrentTrain: epoch  2, batch     4 | loss: 7.7544060CurrentTrain: epoch  2, batch     5 | loss: 8.8685331CurrentTrain: epoch  2, batch     6 | loss: 9.0322151CurrentTrain: epoch  2, batch     7 | loss: 8.9018087CurrentTrain: epoch  2, batch     8 | loss: 8.4944906CurrentTrain: epoch  2, batch     9 | loss: 8.0621929CurrentTrain: epoch  2, batch    10 | loss: 8.1464424CurrentTrain: epoch  2, batch    11 | loss: 8.4504786CurrentTrain: epoch  2, batch    12 | loss: 8.8165874CurrentTrain: epoch  2, batch    13 | loss: 9.0585918CurrentTrain: epoch  2, batch    14 | loss: 7.8291197CurrentTrain: epoch  2, batch    15 | loss: 7.5953898CurrentTrain: epoch  2, batch    16 | loss: 8.3865290CurrentTrain: epoch  2, batch    17 | loss: 7.8016300CurrentTrain: epoch  2, batch    18 | loss: 7.7510653CurrentTrain: epoch  2, batch    19 | loss: 8.0406933CurrentTrain: epoch  2, batch    20 | loss: 8.1036272CurrentTrain: epoch  2, batch    21 | loss: 7.8735309CurrentTrain: epoch  2, batch    22 | loss: 6.9352913CurrentTrain: epoch  2, batch    23 | loss: 8.2709589CurrentTrain: epoch  2, batch    24 | loss: 8.2417870CurrentTrain: epoch  2, batch    25 | loss: 7.5193892CurrentTrain: epoch  2, batch    26 | loss: 8.0626354CurrentTrain: epoch  2, batch    27 | loss: 7.4856801CurrentTrain: epoch  2, batch    28 | loss: 7.0462027CurrentTrain: epoch  2, batch    29 | loss: 7.8837657CurrentTrain: epoch  2, batch    30 | loss: 7.8445759CurrentTrain: epoch  2, batch    31 | loss: 8.3521433CurrentTrain: epoch  2, batch    32 | loss: 7.9478049CurrentTrain: epoch  2, batch    33 | loss: 8.0097761CurrentTrain: epoch  2, batch    34 | loss: 8.1032772CurrentTrain: epoch  2, batch    35 | loss: 7.0554104CurrentTrain: epoch  2, batch    36 | loss: 7.9180126CurrentTrain: epoch  2, batch    37 | loss: 7.6392989CurrentTrain: epoch  3, batch     0 | loss: 7.1544695CurrentTrain: epoch  3, batch     1 | loss: 7.3695459CurrentTrain: epoch  3, batch     2 | loss: 7.3956404CurrentTrain: epoch  3, batch     3 | loss: 7.3663306CurrentTrain: epoch  3, batch     4 | loss: 7.7700906CurrentTrain: epoch  3, batch     5 | loss: 7.8070207CurrentTrain: epoch  3, batch     6 | loss: 8.2846527CurrentTrain: epoch  3, batch     7 | loss: 7.8453951CurrentTrain: epoch  3, batch     8 | loss: 6.2491198CurrentTrain: epoch  3, batch     9 | loss: 6.9009881CurrentTrain: epoch  3, batch    10 | loss: 8.6502171CurrentTrain: epoch  3, batch    11 | loss: 6.7812176CurrentTrain: epoch  3, batch    12 | loss: 6.9301767CurrentTrain: epoch  3, batch    13 | loss: 6.4181671CurrentTrain: epoch  3, batch    14 | loss: 7.2899551CurrentTrain: epoch  3, batch    15 | loss: 6.9360828CurrentTrain: epoch  3, batch    16 | loss: 7.5385165CurrentTrain: epoch  3, batch    17 | loss: 7.0317383CurrentTrain: epoch  3, batch    18 | loss: 6.9773827CurrentTrain: epoch  3, batch    19 | loss: 7.7817492CurrentTrain: epoch  3, batch    20 | loss: 6.7549667CurrentTrain: epoch  3, batch    21 | loss: 8.6674566CurrentTrain: epoch  3, batch    22 | loss: 7.5026650CurrentTrain: epoch  3, batch    23 | loss: 7.9937968CurrentTrain: epoch  3, batch    24 | loss: 7.7119298CurrentTrain: epoch  3, batch    25 | loss: 7.6349688CurrentTrain: epoch  3, batch    26 | loss: 6.7957115CurrentTrain: epoch  3, batch    27 | loss: 6.7870612CurrentTrain: epoch  3, batch    28 | loss: 7.5315557CurrentTrain: epoch  3, batch    29 | loss: 6.9428549CurrentTrain: epoch  3, batch    30 | loss: 6.9865808CurrentTrain: epoch  3, batch    31 | loss: 7.4897962CurrentTrain: epoch  3, batch    32 | loss: 6.7192311CurrentTrain: epoch  3, batch    33 | loss: 8.0521946CurrentTrain: epoch  3, batch    34 | loss: 7.1659565CurrentTrain: epoch  3, batch    35 | loss: 7.3226409CurrentTrain: epoch  3, batch    36 | loss: 6.6434870CurrentTrain: epoch  3, batch    37 | loss: 7.1740427CurrentTrain: epoch  4, batch     0 | loss: 6.9441533CurrentTrain: epoch  4, batch     1 | loss: 6.8740263CurrentTrain: epoch  4, batch     2 | loss: 6.6107664CurrentTrain: epoch  4, batch     3 | loss: 7.0493698CurrentTrain: epoch  4, batch     4 | loss: 6.7218437CurrentTrain: epoch  4, batch     5 | loss: 7.2991872CurrentTrain: epoch  4, batch     6 | loss: 6.1680007CurrentTrain: epoch  4, batch     7 | loss: 5.8548055CurrentTrain: epoch  4, batch     8 | loss: 6.3033257CurrentTrain: epoch  4, batch     9 | loss: 7.2647057CurrentTrain: epoch  4, batch    10 | loss: 6.8886709CurrentTrain: epoch  4, batch    11 | loss: 7.3196664CurrentTrain: epoch  4, batch    12 | loss: 7.3579969CurrentTrain: epoch  4, batch    13 | loss: 6.4919548CurrentTrain: epoch  4, batch    14 | loss: 6.9153299CurrentTrain: epoch  4, batch    15 | loss: 6.3536673CurrentTrain: epoch  4, batch    16 | loss: 7.1371918CurrentTrain: epoch  4, batch    17 | loss: 6.6140995CurrentTrain: epoch  4, batch    18 | loss: 7.1906729CurrentTrain: epoch  4, batch    19 | loss: 5.6746011CurrentTrain: epoch  4, batch    20 | loss: 6.8376684CurrentTrain: epoch  4, batch    21 | loss: 6.0346074CurrentTrain: epoch  4, batch    22 | loss: 6.0248227CurrentTrain: epoch  4, batch    23 | loss: 6.2749753CurrentTrain: epoch  4, batch    24 | loss: 6.5058928CurrentTrain: epoch  4, batch    25 | loss: 6.8559051CurrentTrain: epoch  4, batch    26 | loss: 6.9561319CurrentTrain: epoch  4, batch    27 | loss: 8.5961208CurrentTrain: epoch  4, batch    28 | loss: 6.7926493CurrentTrain: epoch  4, batch    29 | loss: 7.1160097CurrentTrain: epoch  4, batch    30 | loss: 7.7643952CurrentTrain: epoch  4, batch    31 | loss: 8.2309027CurrentTrain: epoch  4, batch    32 | loss: 6.8689780CurrentTrain: epoch  4, batch    33 | loss: 6.3760605CurrentTrain: epoch  4, batch    34 | loss: 6.7243910CurrentTrain: epoch  4, batch    35 | loss: 6.4004450CurrentTrain: epoch  4, batch    36 | loss: 6.7725544CurrentTrain: epoch  4, batch    37 | loss: 6.8315611CurrentTrain: epoch  5, batch     0 | loss: 6.9695163CurrentTrain: epoch  5, batch     1 | loss: 6.6054258CurrentTrain: epoch  5, batch     2 | loss: 6.9875402CurrentTrain: epoch  5, batch     3 | loss: 6.4249210CurrentTrain: epoch  5, batch     4 | loss: 6.0854678CurrentTrain: epoch  5, batch     5 | loss: 5.9471807CurrentTrain: epoch  5, batch     6 | loss: 6.1702442CurrentTrain: epoch  5, batch     7 | loss: 6.3957653CurrentTrain: epoch  5, batch     8 | loss: 6.3200655CurrentTrain: epoch  5, batch     9 | loss: 5.7862296CurrentTrain: epoch  5, batch    10 | loss: 6.1513357CurrentTrain: epoch  5, batch    11 | loss: 6.3329735CurrentTrain: epoch  5, batch    12 | loss: 6.7414522CurrentTrain: epoch  5, batch    13 | loss: 6.9005575CurrentTrain: epoch  5, batch    14 | loss: 5.7661395CurrentTrain: epoch  5, batch    15 | loss: 6.1084228CurrentTrain: epoch  5, batch    16 | loss: 5.9540863CurrentTrain: epoch  5, batch    17 | loss: 6.3048944CurrentTrain: epoch  5, batch    18 | loss: 6.7538500CurrentTrain: epoch  5, batch    19 | loss: 5.7313604CurrentTrain: epoch  5, batch    20 | loss: 6.1345882CurrentTrain: epoch  5, batch    21 | loss: 5.8374972CurrentTrain: epoch  5, batch    22 | loss: 6.8072972CurrentTrain: epoch  5, batch    23 | loss: 6.5311661CurrentTrain: epoch  5, batch    24 | loss: 7.0445356CurrentTrain: epoch  5, batch    25 | loss: 6.4228764CurrentTrain: epoch  5, batch    26 | loss: 7.4539356CurrentTrain: epoch  5, batch    27 | loss: 7.0945187CurrentTrain: epoch  5, batch    28 | loss: 6.1847982CurrentTrain: epoch  5, batch    29 | loss: 6.1389055CurrentTrain: epoch  5, batch    30 | loss: 6.3009462CurrentTrain: epoch  5, batch    31 | loss: 5.7961864CurrentTrain: epoch  5, batch    32 | loss: 5.8153110CurrentTrain: epoch  5, batch    33 | loss: 5.9109650CurrentTrain: epoch  5, batch    34 | loss: 6.6058674CurrentTrain: epoch  5, batch    35 | loss: 6.4500852CurrentTrain: epoch  5, batch    36 | loss: 6.1988382CurrentTrain: epoch  5, batch    37 | loss: 8.8292599CurrentTrain: epoch  6, batch     0 | loss: 6.1608534CurrentTrain: epoch  6, batch     1 | loss: 6.1167269CurrentTrain: epoch  6, batch     2 | loss: 6.1323719CurrentTrain: epoch  6, batch     3 | loss: 5.6949015CurrentTrain: epoch  6, batch     4 | loss: 5.6699290CurrentTrain: epoch  6, batch     5 | loss: 6.1212282CurrentTrain: epoch  6, batch     6 | loss: 6.0025930CurrentTrain: epoch  6, batch     7 | loss: 6.2696671CurrentTrain: epoch  6, batch     8 | loss: 5.6254392CurrentTrain: epoch  6, batch     9 | loss: 6.1295805CurrentTrain: epoch  6, batch    10 | loss: 5.9081364CurrentTrain: epoch  6, batch    11 | loss: 5.9047976CurrentTrain: epoch  6, batch    12 | loss: 6.3043480CurrentTrain: epoch  6, batch    13 | loss: 6.3102884CurrentTrain: epoch  6, batch    14 | loss: 5.5393572CurrentTrain: epoch  6, batch    15 | loss: 5.7676582CurrentTrain: epoch  6, batch    16 | loss: 6.2909718CurrentTrain: epoch  6, batch    17 | loss: 6.4261427CurrentTrain: epoch  6, batch    18 | loss: 6.5391197CurrentTrain: epoch  6, batch    19 | loss: 5.7015429CurrentTrain: epoch  6, batch    20 | loss: 5.7391920CurrentTrain: epoch  6, batch    21 | loss: 5.5328569CurrentTrain: epoch  6, batch    22 | loss: 5.4499454CurrentTrain: epoch  6, batch    23 | loss: 5.4704957CurrentTrain: epoch  6, batch    24 | loss: 5.9225707CurrentTrain: epoch  6, batch    25 | loss: 5.4476576CurrentTrain: epoch  6, batch    26 | loss: 5.6345968CurrentTrain: epoch  6, batch    27 | loss: 5.5592752CurrentTrain: epoch  6, batch    28 | loss: 5.4524231CurrentTrain: epoch  6, batch    29 | loss: 5.6042304CurrentTrain: epoch  6, batch    30 | loss: 5.8343253CurrentTrain: epoch  6, batch    31 | loss: 6.0328693CurrentTrain: epoch  6, batch    32 | loss: 5.3261786CurrentTrain: epoch  6, batch    33 | loss: 5.8189220CurrentTrain: epoch  6, batch    34 | loss: 5.4246545CurrentTrain: epoch  6, batch    35 | loss: 6.2119398CurrentTrain: epoch  6, batch    36 | loss: 5.9045568CurrentTrain: epoch  6, batch    37 | loss: 5.8312573CurrentTrain: epoch  7, batch     0 | loss: 5.5250831CurrentTrain: epoch  7, batch     1 | loss: 5.2550707CurrentTrain: epoch  7, batch     2 | loss: 5.7418680CurrentTrain: epoch  7, batch     3 | loss: 5.3616076CurrentTrain: epoch  7, batch     4 | loss: 5.2787547CurrentTrain: epoch  7, batch     5 | loss: 5.5153956CurrentTrain: epoch  7, batch     6 | loss: 5.3702478CurrentTrain: epoch  7, batch     7 | loss: 5.8329687CurrentTrain: epoch  7, batch     8 | loss: 5.2707682CurrentTrain: epoch  7, batch     9 | loss: 5.4597960CurrentTrain: epoch  7, batch    10 | loss: 5.3136001CurrentTrain: epoch  7, batch    11 | loss: 5.3140402CurrentTrain: epoch  7, batch    12 | loss: 6.0651984CurrentTrain: epoch  7, batch    13 | loss: 5.3410940CurrentTrain: epoch  7, batch    14 | loss: 5.4021482CurrentTrain: epoch  7, batch    15 | loss: 5.3932428CurrentTrain: epoch  7, batch    16 | loss: 5.4769659CurrentTrain: epoch  7, batch    17 | loss: 5.3753691CurrentTrain: epoch  7, batch    18 | loss: 5.2969813CurrentTrain: epoch  7, batch    19 | loss: 5.4363899CurrentTrain: epoch  7, batch    20 | loss: 5.2138309CurrentTrain: epoch  7, batch    21 | loss: 5.5915155CurrentTrain: epoch  7, batch    22 | loss: 5.5390215CurrentTrain: epoch  7, batch    23 | loss: 5.3811707CurrentTrain: epoch  7, batch    24 | loss: 5.4542031CurrentTrain: epoch  7, batch    25 | loss: 5.5998039CurrentTrain: epoch  7, batch    26 | loss: 5.5934525CurrentTrain: epoch  7, batch    27 | loss: 5.3990951CurrentTrain: epoch  7, batch    28 | loss: 5.0029993CurrentTrain: epoch  7, batch    29 | loss: 5.1215420CurrentTrain: epoch  7, batch    30 | loss: 5.1189241CurrentTrain: epoch  7, batch    31 | loss: 6.1536093CurrentTrain: epoch  7, batch    32 | loss: 5.0790262CurrentTrain: epoch  7, batch    33 | loss: 5.2553558CurrentTrain: epoch  7, batch    34 | loss: 5.5976534CurrentTrain: epoch  7, batch    35 | loss: 4.9085417CurrentTrain: epoch  7, batch    36 | loss: 5.1690922CurrentTrain: epoch  7, batch    37 | loss: 6.2308011CurrentTrain: epoch  8, batch     0 | loss: 5.4794121CurrentTrain: epoch  8, batch     1 | loss: 4.9787712CurrentTrain: epoch  8, batch     2 | loss: 5.2311010CurrentTrain: epoch  8, batch     3 | loss: 5.0094271CurrentTrain: epoch  8, batch     4 | loss: 5.4913507CurrentTrain: epoch  8, batch     5 | loss: 5.6201992CurrentTrain: epoch  8, batch     6 | loss: 5.1246843CurrentTrain: epoch  8, batch     7 | loss: 5.1552606CurrentTrain: epoch  8, batch     8 | loss: 5.1120071CurrentTrain: epoch  8, batch     9 | loss: 5.5170059CurrentTrain: epoch  8, batch    10 | loss: 5.1708755CurrentTrain: epoch  8, batch    11 | loss: 4.9560881CurrentTrain: epoch  8, batch    12 | loss: 5.4310236CurrentTrain: epoch  8, batch    13 | loss: 4.9691114CurrentTrain: epoch  8, batch    14 | loss: 5.0742521CurrentTrain: epoch  8, batch    15 | loss: 5.1402435CurrentTrain: epoch  8, batch    16 | loss: 5.2018156CurrentTrain: epoch  8, batch    17 | loss: 5.0962543CurrentTrain: epoch  8, batch    18 | loss: 5.3896160CurrentTrain: epoch  8, batch    19 | loss: 5.2309055CurrentTrain: epoch  8, batch    20 | loss: 4.9797392CurrentTrain: epoch  8, batch    21 | loss: 5.1582308CurrentTrain: epoch  8, batch    22 | loss: 5.5311694CurrentTrain: epoch  8, batch    23 | loss: 5.1527643CurrentTrain: epoch  8, batch    24 | loss: 5.0071564CurrentTrain: epoch  8, batch    25 | loss: 4.9832406CurrentTrain: epoch  8, batch    26 | loss: 5.0783615CurrentTrain: epoch  8, batch    27 | loss: 5.0071926CurrentTrain: epoch  8, batch    28 | loss: 4.9267788CurrentTrain: epoch  8, batch    29 | loss: 5.0756593CurrentTrain: epoch  8, batch    30 | loss: 5.0786052CurrentTrain: epoch  8, batch    31 | loss: 5.2942648CurrentTrain: epoch  8, batch    32 | loss: 5.2634888CurrentTrain: epoch  8, batch    33 | loss: 4.9391232CurrentTrain: epoch  8, batch    34 | loss: 4.9872189CurrentTrain: epoch  8, batch    35 | loss: 5.3119040CurrentTrain: epoch  8, batch    36 | loss: 5.0415812CurrentTrain: epoch  8, batch    37 | loss: 4.9313707CurrentTrain: epoch  9, batch     0 | loss: 5.3723083CurrentTrain: epoch  9, batch     1 | loss: 4.9921522CurrentTrain: epoch  9, batch     2 | loss: 5.6037760CurrentTrain: epoch  9, batch     3 | loss: 5.0057344CurrentTrain: epoch  9, batch     4 | loss: 5.0826988CurrentTrain: epoch  9, batch     5 | loss: 5.1185288CurrentTrain: epoch  9, batch     6 | loss: 5.4970369CurrentTrain: epoch  9, batch     7 | loss: 5.0231075CurrentTrain: epoch  9, batch     8 | loss: 5.0851488CurrentTrain: epoch  9, batch     9 | loss: 4.9228449CurrentTrain: epoch  9, batch    10 | loss: 4.9995618CurrentTrain: epoch  9, batch    11 | loss: 4.9537492CurrentTrain: epoch  9, batch    12 | loss: 5.0030150CurrentTrain: epoch  9, batch    13 | loss: 4.8847094CurrentTrain: epoch  9, batch    14 | loss: 4.8788500CurrentTrain: epoch  9, batch    15 | loss: 4.9453030CurrentTrain: epoch  9, batch    16 | loss: 4.9044328CurrentTrain: epoch  9, batch    17 | loss: 5.1644740CurrentTrain: epoch  9, batch    18 | loss: 4.9933338CurrentTrain: epoch  9, batch    19 | loss: 5.0318432CurrentTrain: epoch  9, batch    20 | loss: 4.8291411CurrentTrain: epoch  9, batch    21 | loss: 4.9313908CurrentTrain: epoch  9, batch    22 | loss: 4.8472042CurrentTrain: epoch  9, batch    23 | loss: 5.2486224CurrentTrain: epoch  9, batch    24 | loss: 5.0476146CurrentTrain: epoch  9, batch    25 | loss: 5.0393686CurrentTrain: epoch  9, batch    26 | loss: 4.9168124CurrentTrain: epoch  9, batch    27 | loss: 4.9073410CurrentTrain: epoch  9, batch    28 | loss: 4.9823217CurrentTrain: epoch  9, batch    29 | loss: 5.3015671CurrentTrain: epoch  9, batch    30 | loss: 4.9273458CurrentTrain: epoch  9, batch    31 | loss: 5.5534468CurrentTrain: epoch  9, batch    32 | loss: 5.1611042CurrentTrain: epoch  9, batch    33 | loss: 4.8956461CurrentTrain: epoch  9, batch    34 | loss: 4.9383440CurrentTrain: epoch  9, batch    35 | loss: 5.0931587CurrentTrain: epoch  9, batch    36 | loss: 5.4035707CurrentTrain: epoch  9, batch    37 | loss: 5.4196939
#### person countries of residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person countries of residence" means a person is living in a country.
Here is an example:
Relation: person countries of residence
Context: the head of iran 's atomic energy organisation , ali akbar salehi , told domestic news agencies the new proposal had come in response to iran 's move last week to begin enriching uranium itself to the 20 percent level required for a tehran medical research rector after rejecting a previous offer .
Head Entity: ali akbar salehi
Tail Entity: iran
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person countries of residence":
Relation: person countries of residence  
Context: After years of living in the bustling city of New York, Maria Gonzalez decided to return to her hometown in Mexico, where she felt a stronger connection to her roots and family.  
Head Entity: Maria Gonzalez  
Tail Entity: Mexico  

Relation: person countries of residence  
Context: Following his successful career in London, David Chen moved to Canada to enjoy a quieter lifestyle and to be closer to nature.  
Head Entity: David Chen  
Tail Entity: Canada  
#### organization top members employees ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization top members employees" means an organization 's employer is a person.
Here is an example:
Relation: organization top members employees
Context: as for any gay or lesbian people who enlisted with the military this week , `` they may be told they can not join '' if the pentagon decides to enforce the law , said aubrey sarvis , executive director of the servicemembers legal defense network , a group representing service members affected by the law .
Head Entity: servicemembers legal defense network
Tail Entity: aubrey sarvis
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization top members employees":
Relation: organization top members employees  
Context: The board of directors at Tech Innovations Inc. recently appointed Sarah Johnson as the new Chief Technology Officer, highlighting her extensive experience in the tech industry and her leadership skills.  
Head Entity: Tech Innovations Inc.  
Tail Entity: Sarah Johnson  

Relation: organization top members employees  
Context: During the annual meeting, the CEO of Green Earth Solutions, Mark Thompson, announced the promotion of Lisa Chen to the position of Chief Financial Officer, recognizing her contributions to the company's growth.  
Head Entity: Green Earth Solutions  
Tail Entity: Lisa Chen  
#### organization member of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization member of" means an organization is the member of an organization.
Here is an example:
Relation: organization member of
Context: ipsco shares rose $ 1506 , or 12 percent , to close at $ 14596 on the new york stock exchange .
Head Entity: ipsco
Tail Entity: new york stock exchange
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization member of":
Relation: organization member of  
Context: The American Medical Association has announced its new initiatives to improve healthcare access across the country.  
Head Entity: American Medical Association  
Tail Entity: healthcare access  

Relation: organization member of  
Context: The United Nations has launched a new program aimed at promoting sustainable development in member countries.  
Head Entity: United Nations  
Tail Entity: sustainable development  
#### person origin ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person origin" means a person 's nationality is a country.
Here is an example:
Relation: person origin
Context: receiving two yellow cards in the previous game against south korea , chinese skipper li jie were sidelined in this east asian top-level clash .
Head Entity: li jie
Tail Entity: chinese
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person origin":
Relation: person origin  
Context: The renowned physicist Albert Einstein was born in Germany before moving to Switzerland and later becoming a citizen of the United States.  
Head Entity: Albert Einstein  
Tail Entity: German  

Relation: person origin  
Context: The famous artist Frida Kahlo was born in Mexico and is celebrated for her unique contributions to Mexican culture and art.  
Head Entity: Frida Kahlo  
Tail Entity: Mexican  
#### person title ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person title" means a person 's title is a title.
Here is an example:
Relation: person title
Context: earlier , in jerusalem , he spoke at the state funeral for the city 's fabled former mayor , teddy kollek , who died tuesday at 95 and was buried in the area of the mount herzl cemetery reserved for israel 's leaders .
Head Entity: teddy kollek
Tail Entity: mayor
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person title":
Relation: person title  
Context: During the award ceremony, the renowned scientist was honored for his groundbreaking research in genetics, and the audience celebrated the achievements of Dr. Jane Smith, who has been a leading figure in the field.  
Head Entity: Dr. Jane Smith  
Tail Entity: scientist  

Relation: person title  
Context: At the annual conference, the keynote speaker, a prominent author, captivated the audience with her insights on modern literature, and many attendees were eager to hear from Professor Emily Johnson, a celebrated novelist.  
Head Entity: Professor Emily Johnson  
Tail Entity: novelist  
#### organization country of headquarters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization country of headquarters" means an organization is located in a country.
Here is an example:
Relation: organization country of headquarters
Context: troubled us bond insurer mbia said it had gained a one billion - dollar capital injection from warburg pincus , a private equity firm , to help boost its finances following losses on mortgage - related securities .
Head Entity: mbia
Tail Entity: us
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization country of headquarters":
Relation: organization country of headquarters  
Context: the multinational technology company apple inc. has its headquarters in cupertino, california, where it designs and manufactures consumer electronics and software.  
Head Entity: apple inc.  
Tail Entity: california  

Relation: organization country of headquarters  
Context: the famous automotive manufacturer toyota motor corporation is headquartered in toyota city, japan, and is known for its innovative production techniques.  
Head Entity: toyota motor corporation  
Tail Entity: japan  
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 79.17%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 79.69%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 80.00%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 79.17%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 82.14%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 84.38%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 86.11%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 86.88%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 88.07%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 88.02%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 88.46%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 87.05%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 86.25%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 84.38%   [EVAL] batch:   16 | acc: 81.25%,  total acc: 84.19%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 83.33%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 82.57%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 82.81%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 83.63%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 84.38%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 85.05%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 85.68%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 86.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 86.78%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 87.04%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 87.93%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 87.71%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 87.70%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 87.89%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 86.36%   
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 79.17%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 79.69%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 80.00%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 79.17%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 82.14%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 84.38%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 86.11%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 86.88%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 88.07%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 88.02%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 88.46%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 87.05%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 86.25%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 84.38%   [EVAL] batch:   16 | acc: 81.25%,  total acc: 84.19%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 83.33%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 82.57%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 82.81%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 83.63%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 84.38%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 85.05%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 85.68%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 86.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 86.78%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 87.04%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 87.93%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 87.71%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 87.70%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 87.89%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 86.36%   
cur_acc:  ['0.8636']
his_acc:  ['0.8636']
CurrentTrain: epoch  0, batch     0 | loss: 6.0795898CurrentTrain: epoch  0, batch     1 | loss: 5.5748267CurrentTrain: epoch  1, batch     0 | loss: 5.3410764CurrentTrain: epoch  1, batch     1 | loss: 4.5117917CurrentTrain: epoch  2, batch     0 | loss: 4.7967134CurrentTrain: epoch  2, batch     1 | loss: 3.9993923CurrentTrain: epoch  3, batch     0 | loss: 4.3462553CurrentTrain: epoch  3, batch     1 | loss: 4.0914927CurrentTrain: epoch  4, batch     0 | loss: 3.8114500CurrentTrain: epoch  4, batch     1 | loss: 3.0481639CurrentTrain: epoch  5, batch     0 | loss: 3.5358310CurrentTrain: epoch  5, batch     1 | loss: 2.7952449CurrentTrain: epoch  6, batch     0 | loss: 2.9968181CurrentTrain: epoch  6, batch     1 | loss: 3.2744687CurrentTrain: epoch  7, batch     0 | loss: 3.1035142CurrentTrain: epoch  7, batch     1 | loss: 2.5038865CurrentTrain: epoch  8, batch     0 | loss: 2.6665583CurrentTrain: epoch  8, batch     1 | loss: 3.0331502CurrentTrain: epoch  9, batch     0 | loss: 2.4494319CurrentTrain: epoch  9, batch     1 | loss: 3.4616125
#### person country of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person country of birth" means a person was born in a country.
Here is an example:
Relation: person country of birth
Context: de maiziere noted that germany took in another former inmate from guantanamo in 2006 -- murat kurnaz , a turkish national who was born and grew up in germany .
Head Entity: murat kurnaz
Tail Entity: germany
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person country of birth":
Relation: person country of birth  
Context: born in 1985 in the bustling city of new delhi, arjun kapoor has always been proud of his indian heritage and culture.  
Head Entity: arjun kapoor  
Tail Entity: india  

Relation: person country of birth  
Context: during the interview, the renowned physicist shared his experiences growing up in the picturesque town of cambridge, where he was born and nurtured by a family of scholars.  
Head Entity: the renowned physicist  
Tail Entity: england  
#### organization website ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization website" means an organization 's website is an url.
Here is an example:
Relation: organization website
Context: http://www.pentaxsailaway.com/ paul j. sail away with pentax sweepstakes
Head Entity: pentax
Tail Entity: http://www.pentaxsailaway.com/
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization website":
Relation: organization website  
Context: Visit the official site at https://www.nike.com for the latest sports gear and apparel.  
Head Entity: Nike  
Tail Entity: https://www.nike.com  

Relation: organization website  
Context: For more information about their services, check out http://www.tesla.com.  
Head Entity: Tesla  
Tail Entity: http://www.tesla.com  
#### organization shareholders ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization shareholders" means an organization was invested by a person.
Here is an example:
Relation: organization shareholders
Context: ------ liberty media acquired a 41 percent stake in directv in late february by exchanging it for a 16 percent stake in news corp plus $ 625 million -lrb- euro402 5 million -rrb- in cash .
Head Entity: directv
Tail Entity: liberty media
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization shareholders":
Relation: organization shareholders  
Context: In 2020, Amazon announced that it had acquired a significant share in the electric vehicle startup Rivian, marking a strategic investment in the future of transportation.  
Head Entity: Rivian  
Tail Entity: Amazon  

Relation: organization shareholders  
Context: The recent merger between Disney and 21st Century Fox resulted in Disney becoming the majority shareholder of the entertainment giant, expanding its portfolio significantly.  
Head Entity: 21st Century Fox  
Tail Entity: Disney  
#### organization dissolved ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization dissolved" means an organization was dissolved in a date.
Here is an example:
Relation: organization dissolved
Context: the tse suffered its worst-ever system crash in november 2005 which paralyzed the world 's second largest bourse and forced it to shelve plan for a listing of its own .
Head Entity: tse
Tail Entity: november 2005
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization dissolved":
Relation: organization dissolved  
Context: The once-prominent tech startup, Innovatech, officially ceased operations in March 2020 after struggling to secure funding.  
Head Entity: Innovatech  
Tail Entity: March 2020  

Relation: organization dissolved  
Context: After years of financial difficulties, the local arts council announced its dissolution in January 2019, leaving many community projects in limbo.  
Head Entity: local arts council  
Tail Entity: January 2019  
#### organization founded by ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization founded by" means an organization was found by a person.
Here is an example:
Relation: organization founded by
Context: `` i have no greater obligation than to ensure the safely of airline travelers in this country , '' transportation secretary ray lahood said in a joint statement with j. randolph babbitt , administrator of the federal aviation administration , that was issued on the eve of a senate hearing on aviation safety .
Head Entity: federal aviation administration
Tail Entity: j. randolph babbitt
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization founded by":
Relation: organization founded by  
Context: In 1975, the renowned physicist and entrepreneur, Dr. Jane Smith, established Quantum Innovations, a company dedicated to advancing quantum computing technologies.  
Head Entity: Quantum Innovations  
Tail Entity: Dr. Jane Smith  

Relation: organization founded by  
Context: The charity organization, Hope for Tomorrow, was created in 2010 by the well-known philanthropist, Michael Johnson, to support underprivileged children.  
Head Entity: Hope for Tomorrow  
Tail Entity: Michael Johnson  
Mixup data size:  1495
MixupTrain:  epoch  0, batch     0 | loss: 6.9226313MixupTrain:  epoch  0, batch     1 | loss: 6.8455181MixupTrain:  epoch  0, batch     2 | loss: 6.6412315MixupTrain:  epoch  0, batch     3 | loss: 6.4526043MixupTrain:  epoch  0, batch     4 | loss: 6.2868671MixupTrain:  epoch  0, batch     5 | loss: 5.9813414MixupTrain:  epoch  0, batch     6 | loss: 5.9098625MixupTrain:  epoch  0, batch     7 | loss: 5.7845221MixupTrain:  epoch  0, batch     8 | loss: 5.6602325MixupTrain:  epoch  0, batch     9 | loss: 5.5686340MixupTrain:  epoch  0, batch    10 | loss: 5.5480814MixupTrain:  epoch  0, batch    11 | loss: 5.4520817MixupTrain:  epoch  0, batch    12 | loss: 5.4652624MixupTrain:  epoch  0, batch    13 | loss: 5.4439173MixupTrain:  epoch  0, batch    14 | loss: 5.4160848MixupTrain:  epoch  0, batch    15 | loss: 5.4154897MixupTrain:  epoch  0, batch    16 | loss: 5.3523722MixupTrain:  epoch  0, batch    17 | loss: 5.3478346MixupTrain:  epoch  0, batch    18 | loss: 5.2693930MixupTrain:  epoch  0, batch    19 | loss: 5.2745361MixupTrain:  epoch  0, batch    20 | loss: 5.2342491MixupTrain:  epoch  0, batch    21 | loss: 5.1784949MixupTrain:  epoch  0, batch    22 | loss: 5.2051020MixupTrain:  epoch  0, batch    23 | loss: 5.0880399MixupTrain:  epoch  0, batch    24 | loss: 5.1370225MixupTrain:  epoch  0, batch    25 | loss: 5.0721359MixupTrain:  epoch  0, batch    26 | loss: 5.0479956MixupTrain:  epoch  0, batch    27 | loss: 4.9606338MixupTrain:  epoch  0, batch    28 | loss: 4.9557776MixupTrain:  epoch  0, batch    29 | loss: 4.9308019MixupTrain:  epoch  0, batch    30 | loss: 4.8840647MixupTrain:  epoch  0, batch    31 | loss: 4.8531685MixupTrain:  epoch  0, batch    32 | loss: 4.8588829MixupTrain:  epoch  0, batch    33 | loss: 4.8346653MixupTrain:  epoch  0, batch    34 | loss: 4.7707157MixupTrain:  epoch  0, batch    35 | loss: 4.7926521MixupTrain:  epoch  0, batch    36 | loss: 4.7813568MixupTrain:  epoch  0, batch    37 | loss: 4.7469778MixupTrain:  epoch  0, batch    38 | loss: 4.7180433MixupTrain:  epoch  0, batch    39 | loss: 4.6979213MixupTrain:  epoch  0, batch    40 | loss: 4.6975231MixupTrain:  epoch  0, batch    41 | loss: 4.6689215MixupTrain:  epoch  0, batch    42 | loss: 4.6760488MixupTrain:  epoch  0, batch    43 | loss: 4.6209860MixupTrain:  epoch  0, batch    44 | loss: 4.6232595MixupTrain:  epoch  0, batch    45 | loss: 4.5994277MixupTrain:  epoch  0, batch    46 | loss: 4.5690937MixupTrain:  epoch  0, batch    47 | loss: 4.5105205MixupTrain:  epoch  0, batch    48 | loss: 4.5045156MixupTrain:  epoch  0, batch    49 | loss: 4.5198050MixupTrain:  epoch  0, batch    50 | loss: 4.4677200MixupTrain:  epoch  0, batch    51 | loss: 4.4601746MixupTrain:  epoch  0, batch    52 | loss: 4.4207306MixupTrain:  epoch  0, batch    53 | loss: 4.4171371MixupTrain:  epoch  0, batch    54 | loss: 4.3947706MixupTrain:  epoch  0, batch    55 | loss: 4.3894949MixupTrain:  epoch  0, batch    56 | loss: 4.3880477MixupTrain:  epoch  0, batch    57 | loss: 4.3734198MixupTrain:  epoch  0, batch    58 | loss: 4.3385634MixupTrain:  epoch  0, batch    59 | loss: 4.3458271MixupTrain:  epoch  0, batch    60 | loss: 4.3269939MixupTrain:  epoch  0, batch    61 | loss: 4.3145304MixupTrain:  epoch  0, batch    62 | loss: 4.2768965MixupTrain:  epoch  0, batch    63 | loss: 4.2682686MixupTrain:  epoch  0, batch    64 | loss: 4.2606382MixupTrain:  epoch  0, batch    65 | loss: 4.2466869MixupTrain:  epoch  0, batch    66 | loss: 4.2486372MixupTrain:  epoch  0, batch    67 | loss: 4.2123923MixupTrain:  epoch  0, batch    68 | loss: 4.2356300MixupTrain:  epoch  0, batch    69 | loss: 4.2091546MixupTrain:  epoch  0, batch    70 | loss: 4.1852751MixupTrain:  epoch  0, batch    71 | loss: 4.1714907MixupTrain:  epoch  0, batch    72 | loss: 4.1789942MixupTrain:  epoch  0, batch    73 | loss: 4.1668520MixupTrain:  epoch  0, batch    74 | loss: 4.1454248MixupTrain:  epoch  0, batch    75 | loss: 4.1270881MixupTrain:  epoch  0, batch    76 | loss: 4.1428394MixupTrain:  epoch  0, batch    77 | loss: 4.1131306MixupTrain:  epoch  0, batch    78 | loss: 4.1112595MixupTrain:  epoch  0, batch    79 | loss: 4.1064382MixupTrain:  epoch  0, batch    80 | loss: 4.0836897MixupTrain:  epoch  0, batch    81 | loss: 4.0638456MixupTrain:  epoch  0, batch    82 | loss: 4.0755320MixupTrain:  epoch  0, batch    83 | loss: 4.0492883MixupTrain:  epoch  0, batch    84 | loss: 4.0626984MixupTrain:  epoch  0, batch    85 | loss: 4.0412140MixupTrain:  epoch  0, batch    86 | loss: 4.0299540MixupTrain:  epoch  0, batch    87 | loss: 4.0291305MixupTrain:  epoch  0, batch    88 | loss: 4.0303750MixupTrain:  epoch  0, batch    89 | loss: 4.0202117MixupTrain:  epoch  0, batch    90 | loss: 4.0183277MixupTrain:  epoch  0, batch    91 | loss: 4.0193701MixupTrain:  epoch  0, batch    92 | loss: 4.0081224MixupTrain:  epoch  0, batch    93 | loss: 3.9984550
MemoryTrain:  epoch  0, batch     0 | loss: 7.0828004MemoryTrain:  epoch  0, batch     1 | loss: 7.8379431MemoryTrain:  epoch  0, batch     2 | loss: 7.2465701MemoryTrain:  epoch  1, batch     0 | loss: 6.4035554MemoryTrain:  epoch  1, batch     1 | loss: 6.1336412MemoryTrain:  epoch  1, batch     2 | loss: 3.4597909MemoryTrain:  epoch  2, batch     0 | loss: 5.1850038MemoryTrain:  epoch  2, batch     1 | loss: 5.2234488MemoryTrain:  epoch  2, batch     2 | loss: 2.3838377MemoryTrain:  epoch  3, batch     0 | loss: 4.8214941MemoryTrain:  epoch  3, batch     1 | loss: 4.6173134MemoryTrain:  epoch  3, batch     2 | loss: 1.2452698MemoryTrain:  epoch  4, batch     0 | loss: 3.5014174MemoryTrain:  epoch  4, batch     1 | loss: 4.5252934MemoryTrain:  epoch  4, batch     2 | loss: 4.8865819MemoryTrain:  epoch  5, batch     0 | loss: 3.7614970MemoryTrain:  epoch  5, batch     1 | loss: 4.3871403MemoryTrain:  epoch  5, batch     2 | loss: 1.7182411MemoryTrain:  epoch  6, batch     0 | loss: 3.4282949MemoryTrain:  epoch  6, batch     1 | loss: 4.2742887MemoryTrain:  epoch  6, batch     2 | loss: 1.2524083MemoryTrain:  epoch  7, batch     0 | loss: 3.2036843MemoryTrain:  epoch  7, batch     1 | loss: 3.6183877MemoryTrain:  epoch  7, batch     2 | loss: 7.7670045MemoryTrain:  epoch  8, batch     0 | loss: 3.5432789MemoryTrain:  epoch  8, batch     1 | loss: 3.2045414MemoryTrain:  epoch  8, batch     2 | loss: 1.3390576MemoryTrain:  epoch  9, batch     0 | loss: 3.1549616MemoryTrain:  epoch  9, batch     1 | loss: 3.2708240MemoryTrain:  epoch  9, batch     2 | loss: 6.4789581
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 68.75%,  total acc: 77.08%   [EVAL] batch:    3 | acc: 18.75%,  total acc: 62.50%   [EVAL] batch:    4 | acc: 12.50%,  total acc: 52.50%   [EVAL] batch:    5 | acc: 31.25%,  total acc: 48.96%   [EVAL] batch:    6 | acc: 6.25%,  total acc: 42.86%   [EVAL] batch:    7 | acc: 0.00%,  total acc: 37.50%   
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    1 | acc: 56.25%,  total acc: 59.38%   [EVAL] batch:    2 | acc: 68.75%,  total acc: 62.50%   [EVAL] batch:    3 | acc: 56.25%,  total acc: 60.94%   [EVAL] batch:    4 | acc: 62.50%,  total acc: 61.25%   [EVAL] batch:    5 | acc: 62.50%,  total acc: 61.46%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 63.39%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 65.62%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 68.75%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 70.62%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 72.73%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 73.44%   [EVAL] batch:   12 | acc: 56.25%,  total acc: 72.12%   [EVAL] batch:   13 | acc: 56.25%,  total acc: 70.98%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 71.25%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 70.31%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 70.59%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 70.14%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 69.74%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 70.00%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 71.43%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 72.73%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 73.91%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 76.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 76.92%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 77.55%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 78.35%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 79.09%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 79.38%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 80.04%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 80.47%   [EVAL] batch:   32 | acc: 68.75%,  total acc: 80.11%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 80.70%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 80.54%   [EVAL] batch:   35 | acc: 37.50%,  total acc: 79.34%   [EVAL] batch:   36 | acc: 12.50%,  total acc: 77.53%   [EVAL] batch:   37 | acc: 31.25%,  total acc: 76.32%   [EVAL] batch:   38 | acc: 12.50%,  total acc: 74.68%   [EVAL] batch:   39 | acc: 0.00%,  total acc: 72.81%   
cur_acc:  ['0.8636', '0.3750']
his_acc:  ['0.8636', '0.7281']
CurrentTrain: epoch  0, batch     0 | loss: 4.8733635CurrentTrain: epoch  0, batch     1 | loss: 5.2548175CurrentTrain: epoch  1, batch     0 | loss: 4.0755863CurrentTrain: epoch  1, batch     1 | loss: 4.1505113CurrentTrain: epoch  2, batch     0 | loss: 3.8830495CurrentTrain: epoch  2, batch     1 | loss: 2.9225764CurrentTrain: epoch  3, batch     0 | loss: 3.2257721CurrentTrain: epoch  3, batch     1 | loss: 3.1430266CurrentTrain: epoch  4, batch     0 | loss: 2.9823818CurrentTrain: epoch  4, batch     1 | loss: 2.8090498CurrentTrain: epoch  5, batch     0 | loss: 2.7102594CurrentTrain: epoch  5, batch     1 | loss: 2.6769643CurrentTrain: epoch  6, batch     0 | loss: 2.5537856CurrentTrain: epoch  6, batch     1 | loss: 2.7447870CurrentTrain: epoch  7, batch     0 | loss: 2.4575872CurrentTrain: epoch  7, batch     1 | loss: 2.8060048CurrentTrain: epoch  8, batch     0 | loss: 2.3346848CurrentTrain: epoch  8, batch     1 | loss: 2.0846310CurrentTrain: epoch  9, batch     0 | loss: 2.3406672CurrentTrain: epoch  9, batch     1 | loss: 2.1767704
#### organization founded ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization founded" means an organization was found in a date.
Here is an example:
Relation: organization founded
Context: the company was founded as a hobby in 1979 by the husband and wife team of tim and nina zagat , a pair of lawyers who started recuiting friends to provide their own ratings of the food , decor and service of restaurants they frequented .
Head Entity: zagat
Tail Entity: 1979
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization founded":
Relation: organization founded  
Context: In 1995, the tech startup was established by a group of engineers who aimed to revolutionize the software industry with innovative solutions.  
Head Entity: tech startup  
Tail Entity: 1995  

Relation: organization founded  
Context: The nonprofit organization was created in 2001 to support local artists and promote cultural events in the community.  
Head Entity: nonprofit organization  
Tail Entity: 2001  
#### person age ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person age" means a person 's age is a number.
Here is an example:
Relation: person age
Context: virginia republican jo ann davis passed away on saturday at the age of 57 .
Head Entity: jo ann davis
Tail Entity: 57
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person age":
Relation: person age  
Context: the famous actor robert downey jr. celebrated his 56th birthday last week.  
Head Entity: robert downey jr.  
Tail Entity: 56  

Relation: person age  
Context: on her 30th birthday, singer taylor swift released a new album to mark the occasion.  
Head Entity: taylor swift  
Tail Entity: 30  
#### person city of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person city of birth" means a person was born in a city.
Here is an example:
Relation: person city of birth
Context: joseph simpson farland was born on aug 11 , 1914 , in clarksburg , wva , the only child of richard and grace simpson farland .
Head Entity: joseph simpson farland
Tail Entity: clarksburg
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person city of birth":
Relation: person city of birth  
Context: marie curie was born on november 7, 1867, in warsaw, poland, where she spent her early years before moving to france.  
Head Entity: marie curie  
Tail Entity: warsaw  

Relation: person city of birth  
Context: barack obama was born on august 4, 1961, in honolulu, hawaii, and later became the 44th president of the united states.  
Head Entity: barack obama  
Tail Entity: honolulu  
#### organization members ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization members" means an organization 's member is an organization.
Here is an example:
Relation: organization members
Context: sun plays for the grand rapids flight of the international basketball league after toiling for the maryland nighthawks of the american basketball association , both development leagues for those who dream of an nba career .
Head Entity: american basketball association
Tail Entity: maryland nighthawks
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization members":
Relation: organization members  
Context: The New York Philharmonic is one of the oldest orchestras in the United States, and it has had many notable musicians as members, including the famous conductor Leonard Bernstein.  
Head Entity: New York Philharmonic  
Tail Entity: Leonard Bernstein  

Relation: organization members  
Context: The National Geographic Society has a long history of exploration and education, with many prominent explorers and scientists, such as Jane Goodall, being members of the organization.  
Head Entity: National Geographic Society  
Tail Entity: Jane Goodall  
#### person religion ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person religion" means an person is the member of a religion.
Here is an example:
Relation: person religion
Context: he fought attempts by zealous jews to move into the muslim quarter of the walled old city , but defended the practice of developing jewish suburbs around the eastern arab sector to prevent it from ever escaping israel 's rule .
Head Entity: he
Tail Entity: jewish
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person religion":
Relation: person religion  
Context: After years of studying various philosophies, she finally embraced Buddhism, finding peace and purpose in its teachings.  
Head Entity: she  
Tail Entity: Buddhism  

Relation: person religion  
Context: The community celebrated the festival of Diwali, where he proudly showcased his Hindu heritage and traditions.  
Head Entity: he  
Tail Entity: Hindu
Mixup data size:  2455
MixupTrain:  epoch  0, batch     0 | loss: 6.2817783MixupTrain:  epoch  0, batch     1 | loss: 5.9308004MixupTrain:  epoch  0, batch     2 | loss: 5.9071331MixupTrain:  epoch  0, batch     3 | loss: 5.7270083MixupTrain:  epoch  0, batch     4 | loss: 5.7731953MixupTrain:  epoch  0, batch     5 | loss: 5.2364798MixupTrain:  epoch  0, batch     6 | loss: 5.7637181MixupTrain:  epoch  0, batch     7 | loss: 5.4338522MixupTrain:  epoch  0, batch     8 | loss: 5.7536159MixupTrain:  epoch  0, batch     9 | loss: 5.4554577MixupTrain:  epoch  0, batch    10 | loss: 5.0668902MixupTrain:  epoch  0, batch    11 | loss: 5.1561546MixupTrain:  epoch  0, batch    12 | loss: 5.2272806MixupTrain:  epoch  0, batch    13 | loss: 5.2903094MixupTrain:  epoch  0, batch    14 | loss: 4.9648333MixupTrain:  epoch  0, batch    15 | loss: 5.3141160MixupTrain:  epoch  0, batch    16 | loss: 4.9918418MixupTrain:  epoch  0, batch    17 | loss: 5.0180712MixupTrain:  epoch  0, batch    18 | loss: 5.3483644MixupTrain:  epoch  0, batch    19 | loss: 5.0503674MixupTrain:  epoch  0, batch    20 | loss: 5.0848961MixupTrain:  epoch  0, batch    21 | loss: 5.1275806MixupTrain:  epoch  0, batch    22 | loss: 4.8002181MixupTrain:  epoch  0, batch    23 | loss: 5.1623878MixupTrain:  epoch  0, batch    24 | loss: 5.0254030MixupTrain:  epoch  0, batch    25 | loss: 5.0589867MixupTrain:  epoch  0, batch    26 | loss: 4.9561191MixupTrain:  epoch  0, batch    27 | loss: 5.0754652MixupTrain:  epoch  0, batch    28 | loss: 5.3240376MixupTrain:  epoch  0, batch    29 | loss: 4.9051003MixupTrain:  epoch  0, batch    30 | loss: 5.0151339MixupTrain:  epoch  0, batch    31 | loss: 4.9784479MixupTrain:  epoch  0, batch    32 | loss: 4.8036928MixupTrain:  epoch  0, batch    33 | loss: 4.9619808MixupTrain:  epoch  0, batch    34 | loss: 4.7063398MixupTrain:  epoch  0, batch    35 | loss: 4.7726188MixupTrain:  epoch  0, batch    36 | loss: 4.9101763MixupTrain:  epoch  0, batch    37 | loss: 4.8668261MixupTrain:  epoch  0, batch    38 | loss: 4.2972050MixupTrain:  epoch  0, batch    39 | loss: 4.8450975MixupTrain:  epoch  0, batch    40 | loss: 4.6659584MixupTrain:  epoch  0, batch    41 | loss: 4.7414093MixupTrain:  epoch  0, batch    42 | loss: 4.7988076MixupTrain:  epoch  0, batch    43 | loss: 4.4677277MixupTrain:  epoch  0, batch    44 | loss: 4.8372078MixupTrain:  epoch  0, batch    45 | loss: 4.5624852MixupTrain:  epoch  0, batch    46 | loss: 4.5419612MixupTrain:  epoch  0, batch    47 | loss: 4.4225092MixupTrain:  epoch  0, batch    48 | loss: 4.5315828MixupTrain:  epoch  0, batch    49 | loss: 4.5620127MixupTrain:  epoch  0, batch    50 | loss: 4.5505180MixupTrain:  epoch  0, batch    51 | loss: 4.5359879MixupTrain:  epoch  0, batch    52 | loss: 4.2827687MixupTrain:  epoch  0, batch    53 | loss: 4.4473982MixupTrain:  epoch  0, batch    54 | loss: 4.1528254MixupTrain:  epoch  0, batch    55 | loss: 4.5227299MixupTrain:  epoch  0, batch    56 | loss: 4.2993965MixupTrain:  epoch  0, batch    57 | loss: 4.5100331MixupTrain:  epoch  0, batch    58 | loss: 4.6585469MixupTrain:  epoch  0, batch    59 | loss: 4.3288937MixupTrain:  epoch  0, batch    60 | loss: 4.6545238MixupTrain:  epoch  0, batch    61 | loss: 4.4273529MixupTrain:  epoch  0, batch    62 | loss: 4.4545536MixupTrain:  epoch  0, batch    63 | loss: 4.3502154MixupTrain:  epoch  0, batch    64 | loss: 4.3654246MixupTrain:  epoch  0, batch    65 | loss: 4.3841705MixupTrain:  epoch  0, batch    66 | loss: 4.2286701MixupTrain:  epoch  0, batch    67 | loss: 4.3474970MixupTrain:  epoch  0, batch    68 | loss: 4.3872252MixupTrain:  epoch  0, batch    69 | loss: 4.5998769MixupTrain:  epoch  0, batch    70 | loss: 4.2098370MixupTrain:  epoch  0, batch    71 | loss: 4.3676615MixupTrain:  epoch  0, batch    72 | loss: 4.4283800MixupTrain:  epoch  0, batch    73 | loss: 4.4153638MixupTrain:  epoch  0, batch    74 | loss: 4.5518436MixupTrain:  epoch  0, batch    75 | loss: 4.4177008MixupTrain:  epoch  0, batch    76 | loss: 4.2510767MixupTrain:  epoch  0, batch    77 | loss: 4.2616997MixupTrain:  epoch  0, batch    78 | loss: 4.3971233MixupTrain:  epoch  0, batch    79 | loss: 4.4033108MixupTrain:  epoch  0, batch    80 | loss: 4.1027021MixupTrain:  epoch  0, batch    81 | loss: 4.3944597MixupTrain:  epoch  0, batch    82 | loss: 4.1660967MixupTrain:  epoch  0, batch    83 | loss: 4.2191143MixupTrain:  epoch  0, batch    84 | loss: 4.3928614MixupTrain:  epoch  0, batch    85 | loss: 4.1352930MixupTrain:  epoch  0, batch    86 | loss: 4.0566821MixupTrain:  epoch  0, batch    87 | loss: 4.1735864MixupTrain:  epoch  0, batch    88 | loss: 4.2868886MixupTrain:  epoch  0, batch    89 | loss: 4.2648187MixupTrain:  epoch  0, batch    90 | loss: 4.3920717MixupTrain:  epoch  0, batch    91 | loss: 4.1580377MixupTrain:  epoch  0, batch    92 | loss: 4.1881952MixupTrain:  epoch  0, batch    93 | loss: 4.2275071MixupTrain:  epoch  0, batch    94 | loss: 4.2816200MixupTrain:  epoch  0, batch    95 | loss: 4.3237286MixupTrain:  epoch  0, batch    96 | loss: 4.2133384MixupTrain:  epoch  0, batch    97 | loss: 4.2139344MixupTrain:  epoch  0, batch    98 | loss: 4.0868473MixupTrain:  epoch  0, batch    99 | loss: 4.2214775MixupTrain:  epoch  0, batch   100 | loss: 4.2768893MixupTrain:  epoch  0, batch   101 | loss: 4.1818781MixupTrain:  epoch  0, batch   102 | loss: 4.0713954MixupTrain:  epoch  0, batch   103 | loss: 4.1718760MixupTrain:  epoch  0, batch   104 | loss: 4.0828218MixupTrain:  epoch  0, batch   105 | loss: 4.1549311MixupTrain:  epoch  0, batch   106 | loss: 4.3339415MixupTrain:  epoch  0, batch   107 | loss: 4.1534948MixupTrain:  epoch  0, batch   108 | loss: 3.9888873MixupTrain:  epoch  0, batch   109 | loss: 4.2275057MixupTrain:  epoch  0, batch   110 | loss: 4.1757646MixupTrain:  epoch  0, batch   111 | loss: 4.2188950MixupTrain:  epoch  0, batch   112 | loss: 4.1852283MixupTrain:  epoch  0, batch   113 | loss: 4.2680869MixupTrain:  epoch  0, batch   114 | loss: 4.0729470MixupTrain:  epoch  0, batch   115 | loss: 4.0514126MixupTrain:  epoch  0, batch   116 | loss: 4.1978641MixupTrain:  epoch  0, batch   117 | loss: 4.1701837MixupTrain:  epoch  0, batch   118 | loss: 4.0983219MixupTrain:  epoch  0, batch   119 | loss: 4.0345435MixupTrain:  epoch  0, batch   120 | loss: 4.2045074MixupTrain:  epoch  0, batch   121 | loss: 3.9357290MixupTrain:  epoch  0, batch   122 | loss: 4.0730934MixupTrain:  epoch  0, batch   123 | loss: 4.0796385MixupTrain:  epoch  0, batch   124 | loss: 4.1515918MixupTrain:  epoch  0, batch   125 | loss: 4.0959053MixupTrain:  epoch  0, batch   126 | loss: 4.0421257MixupTrain:  epoch  0, batch   127 | loss: 3.8095670MixupTrain:  epoch  0, batch   128 | loss: 3.9791055MixupTrain:  epoch  0, batch   129 | loss: 4.0526180MixupTrain:  epoch  0, batch   130 | loss: 4.1290636MixupTrain:  epoch  0, batch   131 | loss: 4.1892776MixupTrain:  epoch  0, batch   132 | loss: 4.1139874MixupTrain:  epoch  0, batch   133 | loss: 3.9150074MixupTrain:  epoch  0, batch   134 | loss: 4.0873299MixupTrain:  epoch  0, batch   135 | loss: 4.0155315MixupTrain:  epoch  0, batch   136 | loss: 4.0959363MixupTrain:  epoch  0, batch   137 | loss: 3.9589944MixupTrain:  epoch  0, batch   138 | loss: 3.8970051MixupTrain:  epoch  0, batch   139 | loss: 3.8145418MixupTrain:  epoch  0, batch   140 | loss: 4.0738850MixupTrain:  epoch  0, batch   141 | loss: 4.0532546MixupTrain:  epoch  0, batch   142 | loss: 4.0627117MixupTrain:  epoch  0, batch   143 | loss: 4.1879630MixupTrain:  epoch  0, batch   144 | loss: 4.0604115MixupTrain:  epoch  0, batch   145 | loss: 3.9235697MixupTrain:  epoch  0, batch   146 | loss: 4.0959558MixupTrain:  epoch  0, batch   147 | loss: 4.0644770MixupTrain:  epoch  0, batch   148 | loss: 4.0082989MixupTrain:  epoch  0, batch   149 | loss: 4.0681534MixupTrain:  epoch  0, batch   150 | loss: 4.0533314MixupTrain:  epoch  0, batch   151 | loss: 3.9992304MixupTrain:  epoch  0, batch   152 | loss: 3.9643612MixupTrain:  epoch  0, batch   153 | loss: 3.8803084
MemoryTrain:  epoch  0, batch     0 | loss: 3.4577141MemoryTrain:  epoch  0, batch     1 | loss: 5.1908827MemoryTrain:  epoch  0, batch     2 | loss: 3.4056830MemoryTrain:  epoch  1, batch     0 | loss: 3.4395337MemoryTrain:  epoch  1, batch     1 | loss: 3.7758453MemoryTrain:  epoch  1, batch     2 | loss: 3.6191993MemoryTrain:  epoch  2, batch     0 | loss: 3.6260219MemoryTrain:  epoch  2, batch     1 | loss: 2.3239994MemoryTrain:  epoch  2, batch     2 | loss: 3.3346040MemoryTrain:  epoch  3, batch     0 | loss: 2.7005727MemoryTrain:  epoch  3, batch     1 | loss: 3.2621360MemoryTrain:  epoch  3, batch     2 | loss: 2.6719904MemoryTrain:  epoch  4, batch     0 | loss: 2.5760798MemoryTrain:  epoch  4, batch     1 | loss: 2.5274317MemoryTrain:  epoch  4, batch     2 | loss: 2.7430577MemoryTrain:  epoch  5, batch     0 | loss: 2.0416002MemoryTrain:  epoch  5, batch     1 | loss: 2.7164023MemoryTrain:  epoch  5, batch     2 | loss: 2.6391678MemoryTrain:  epoch  6, batch     0 | loss: 2.3696544MemoryTrain:  epoch  6, batch     1 | loss: 2.0456192MemoryTrain:  epoch  6, batch     2 | loss: 1.8831429MemoryTrain:  epoch  7, batch     0 | loss: 2.0221033MemoryTrain:  epoch  7, batch     1 | loss: 1.6006348MemoryTrain:  epoch  7, batch     2 | loss: 2.0372083MemoryTrain:  epoch  8, batch     0 | loss: 1.7047713MemoryTrain:  epoch  8, batch     1 | loss: 1.6912923MemoryTrain:  epoch  8, batch     2 | loss: 1.5446475MemoryTrain:  epoch  9, batch     0 | loss: 1.6666310MemoryTrain:  epoch  9, batch     1 | loss: 1.6540868MemoryTrain:  epoch  9, batch     2 | loss: 1.5889449
[EVAL] batch:    0 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 96.88%   [EVAL] batch:    2 | acc: 100.00%,  total acc: 97.92%   [EVAL] batch:    3 | acc: 100.00%,  total acc: 98.44%   [EVAL] batch:    4 | acc: 100.00%,  total acc: 98.75%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 98.96%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 99.11%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 99.22%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 98.61%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 95.00%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 94.89%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 93.23%   [EVAL] batch:   12 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   13 | acc: 75.00%,  total acc: 92.41%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 56.25%,  total acc: 62.50%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 66.67%   [EVAL] batch:    3 | acc: 56.25%,  total acc: 64.06%   [EVAL] batch:    4 | acc: 56.25%,  total acc: 62.50%   [EVAL] batch:    5 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 64.29%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 67.19%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 68.75%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 70.62%   [EVAL] batch:   10 | acc: 68.75%,  total acc: 70.45%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 70.83%   [EVAL] batch:   12 | acc: 56.25%,  total acc: 69.71%   [EVAL] batch:   13 | acc: 25.00%,  total acc: 66.52%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 66.67%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 66.02%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 66.54%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 66.32%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 66.12%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 66.56%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 68.15%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 69.60%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 70.65%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 71.61%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 72.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 73.80%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 74.54%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 75.45%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 76.29%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 76.67%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 77.42%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 77.93%   [EVAL] batch:   32 | acc: 62.50%,  total acc: 77.46%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 78.12%   [EVAL] batch:   34 | acc: 56.25%,  total acc: 77.50%   [EVAL] batch:   35 | acc: 18.75%,  total acc: 75.87%   [EVAL] batch:   36 | acc: 0.00%,  total acc: 73.82%   [EVAL] batch:   37 | acc: 25.00%,  total acc: 72.53%   [EVAL] batch:   38 | acc: 25.00%,  total acc: 71.31%   [EVAL] batch:   39 | acc: 50.00%,  total acc: 70.78%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 71.34%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 72.02%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 72.67%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 73.30%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 73.89%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 74.46%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 75.52%   [EVAL] batch:   48 | acc: 62.50%,  total acc: 75.26%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 75.50%   [EVAL] batch:   50 | acc: 87.50%,  total acc: 75.74%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 75.96%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 76.42%   [EVAL] batch:   53 | acc: 25.00%,  total acc: 75.46%   
cur_acc:  ['0.8636', '0.3750', '0.9241']
his_acc:  ['0.8636', '0.7281', '0.7546']
CurrentTrain: epoch  0, batch     0 | loss: 5.9076591CurrentTrain: epoch  0, batch     1 | loss: 6.1995664CurrentTrain: epoch  1, batch     0 | loss: 5.2472506CurrentTrain: epoch  1, batch     1 | loss: 4.6033306CurrentTrain: epoch  2, batch     0 | loss: 4.6011553CurrentTrain: epoch  2, batch     1 | loss: 5.0579519CurrentTrain: epoch  3, batch     0 | loss: 4.4356418CurrentTrain: epoch  3, batch     1 | loss: 4.4386945CurrentTrain: epoch  4, batch     0 | loss: 3.9057524CurrentTrain: epoch  4, batch     1 | loss: 4.3076544CurrentTrain: epoch  5, batch     0 | loss: 3.7593637CurrentTrain: epoch  5, batch     1 | loss: 3.8631334CurrentTrain: epoch  6, batch     0 | loss: 3.8120859CurrentTrain: epoch  6, batch     1 | loss: 3.2780783CurrentTrain: epoch  7, batch     0 | loss: 3.4256854CurrentTrain: epoch  7, batch     1 | loss: 2.7280366CurrentTrain: epoch  8, batch     0 | loss: 3.0896585CurrentTrain: epoch  8, batch     1 | loss: 2.7178950CurrentTrain: epoch  9, batch     0 | loss: 2.9799967CurrentTrain: epoch  9, batch     1 | loss: 2.6391814
#### person cities of residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person cities of residence" means a person is living in a city.
Here is an example:
Relation: person cities of residence
Context: tv-idol-johns -- atlanta -- `` american idol '' finalist michael johns moved to los angeles several years ago , but his heart is still in atlanta .
Head Entity: his
Tail Entity: atlanta
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person cities of residence":
Relation: person cities of residence  
Context: after years of living in new york city, the famous author decided to return to her hometown of boston, where she feels most at home.  
Head Entity: she  
Tail Entity: boston  

Relation: person cities of residence  
Context: despite being born in chicago, the renowned chef has spent most of his life in san francisco, where he opened his first restaurant.  
Head Entity: he  
Tail Entity: san francisco  
#### person schools attended ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person schools attended" means a person 's school is an organization.
Here is an example:
Relation: person schools attended
Context: her political involvement began early : at cornell , she helped organize local farmers ' cooperatives .
Head Entity: she
Tail Entity: cornell
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person schools attended":
Relation: person schools attended  
Context: After graduating from high school, he enrolled at Stanford University to pursue his degree in computer science.  
Head Entity: he  
Tail Entity: Stanford University  

Relation: person schools attended  
Context: She often reminisces about her time at Harvard, where she developed a passion for literature and writing.  
Head Entity: She  
Tail Entity: Harvard  
#### person country of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person country of death" means a person was died in a country.
Here is an example:
Relation: person country of death
Context: beirut , lebanon -lrb- ap -rrb- sheik abbas musawi , hezbollah 's secretary-general , his wife and son were killed in february 1992 when israeli helicopters fired rockets at his car in southern lebanon .
Head Entity: abbas musawi
Tail Entity: southern lebanon
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person country of death":
Relation: person country of death  
Context: in 1945, the renowned physicist albert einstein passed away in his home in princeton, new jersey, after a long and illustrious career in science.  
Head Entity: albert einstein  
Tail Entity: new jersey  

Relation: person country of death  
Context: the famous author ernest hemingway died in 1961, taking his last breath in a small town in idaho, where he had sought solace from the world.  
Head Entity: ernest hemingway  
Tail Entity: idaho  
#### person children ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person children" means a person 's children is a person.
Here is an example:
Relation: person children
Context: he is survived by his wife of 63 years , josephine robinson mcnair , of columbia ; a son , robert e. jr. , of columbia ; three daughters , robin lee howell and corinne godshall , of myrtle beach , s.c. , and claudia crawford mcnair , of jamestown , s.c. ; six grandchildren ; and one great-grandchild .
Head Entity: he
Tail Entity: claudia crawford mcnair
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person children":
Relation: person children  
Context: After the passing of her husband, she dedicated her life to raising their three children, including her youngest, emily, who is now a successful artist in new york.  
Head Entity: she  
Tail Entity: emily  

Relation: person children  
Context: The famous actor often shares stories about his childhood and the lessons he learned from his father, who was a significant influence on his two sons, michael and josh.  
Head Entity: his father  
Tail Entity: michael  
#### person charges ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person charges" means a person was charged with an event.
Here is an example:
Relation: person charges
Context: ferrara said he was innocent of limoli 's slaying , but he pleaded guilty in 1992 to murder , along with racketeering charges , under a deal that sent him to prison for 22 years , rather than go to trial and risk a conviction that could lead to life in prison .
Head Entity: ferrara
Tail Entity: racketeering
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person charges":
Relation: person charges  
Context: After a lengthy investigation, the authorities announced that Johnson was charged with embezzlement, a crime that could result in significant prison time if convicted.  
Head Entity: Johnson  
Tail Entity: embezzlement  

Relation: person charges  
Context: The district attorney confirmed that Smith was charged with assault following the altercation that took place last month at the downtown bar.  
Head Entity: Smith  
Tail Entity: assault  
Mixup data size:  3640
MixupTrain:  epoch  0, batch     0 | loss: 4.6104050MixupTrain:  epoch  0, batch     1 | loss: 4.3700113MixupTrain:  epoch  0, batch     2 | loss: 4.1012993MixupTrain:  epoch  0, batch     3 | loss: 3.9664986MixupTrain:  epoch  0, batch     4 | loss: 4.2453547MixupTrain:  epoch  0, batch     5 | loss: 3.8263826MixupTrain:  epoch  0, batch     6 | loss: 4.1017981MixupTrain:  epoch  0, batch     7 | loss: 4.0204339MixupTrain:  epoch  0, batch     8 | loss: 3.9801984MixupTrain:  epoch  0, batch     9 | loss: 3.7760675MixupTrain:  epoch  0, batch    10 | loss: 3.7543299MixupTrain:  epoch  0, batch    11 | loss: 3.8851128MixupTrain:  epoch  0, batch    12 | loss: 4.4248395MixupTrain:  epoch  0, batch    13 | loss: 3.9536319MixupTrain:  epoch  0, batch    14 | loss: 3.5223832MixupTrain:  epoch  0, batch    15 | loss: 4.0074692MixupTrain:  epoch  0, batch    16 | loss: 4.0799789MixupTrain:  epoch  0, batch    17 | loss: 3.7909665MixupTrain:  epoch  0, batch    18 | loss: 3.6705980MixupTrain:  epoch  0, batch    19 | loss: 3.9009972MixupTrain:  epoch  0, batch    20 | loss: 3.6783633MixupTrain:  epoch  0, batch    21 | loss: 3.5072427MixupTrain:  epoch  0, batch    22 | loss: 3.7297378MixupTrain:  epoch  0, batch    23 | loss: 3.5302191MixupTrain:  epoch  0, batch    24 | loss: 3.5141063MixupTrain:  epoch  0, batch    25 | loss: 3.7584138MixupTrain:  epoch  0, batch    26 | loss: 3.8246832MixupTrain:  epoch  0, batch    27 | loss: 3.5366721MixupTrain:  epoch  0, batch    28 | loss: 3.7477014MixupTrain:  epoch  0, batch    29 | loss: 3.6802325MixupTrain:  epoch  0, batch    30 | loss: 3.5904315MixupTrain:  epoch  0, batch    31 | loss: 3.2832608MixupTrain:  epoch  0, batch    32 | loss: 3.6868901MixupTrain:  epoch  0, batch    33 | loss: 3.5472167MixupTrain:  epoch  0, batch    34 | loss: 3.5550904MixupTrain:  epoch  0, batch    35 | loss: 3.3594432MixupTrain:  epoch  0, batch    36 | loss: 3.5465117MixupTrain:  epoch  0, batch    37 | loss: 3.2510791MixupTrain:  epoch  0, batch    38 | loss: 2.9838262MixupTrain:  epoch  0, batch    39 | loss: 3.4119210MixupTrain:  epoch  0, batch    40 | loss: 3.4061007MixupTrain:  epoch  0, batch    41 | loss: 3.4771614MixupTrain:  epoch  0, batch    42 | loss: 3.3494201MixupTrain:  epoch  0, batch    43 | loss: 3.3376594MixupTrain:  epoch  0, batch    44 | loss: 3.6587288MixupTrain:  epoch  0, batch    45 | loss: 3.3076797MixupTrain:  epoch  0, batch    46 | loss: 3.2048278MixupTrain:  epoch  0, batch    47 | loss: 3.5916555MixupTrain:  epoch  0, batch    48 | loss: 3.3227253MixupTrain:  epoch  0, batch    49 | loss: 3.1281719MixupTrain:  epoch  0, batch    50 | loss: 3.1194634MixupTrain:  epoch  0, batch    51 | loss: 3.2300570MixupTrain:  epoch  0, batch    52 | loss: 3.5248396MixupTrain:  epoch  0, batch    53 | loss: 3.2300768MixupTrain:  epoch  0, batch    54 | loss: 3.4012661MixupTrain:  epoch  0, batch    55 | loss: 3.2942178MixupTrain:  epoch  0, batch    56 | loss: 3.4796171MixupTrain:  epoch  0, batch    57 | loss: 3.3401594MixupTrain:  epoch  0, batch    58 | loss: 3.3267140MixupTrain:  epoch  0, batch    59 | loss: 3.2122436MixupTrain:  epoch  0, batch    60 | loss: 3.1539996MixupTrain:  epoch  0, batch    61 | loss: 3.2538924MixupTrain:  epoch  0, batch    62 | loss: 3.3094010MixupTrain:  epoch  0, batch    63 | loss: 3.2426763MixupTrain:  epoch  0, batch    64 | loss: 3.3761048MixupTrain:  epoch  0, batch    65 | loss: 3.2259488MixupTrain:  epoch  0, batch    66 | loss: 3.2764907MixupTrain:  epoch  0, batch    67 | loss: 3.2493899MixupTrain:  epoch  0, batch    68 | loss: 3.2931433MixupTrain:  epoch  0, batch    69 | loss: 3.1639979MixupTrain:  epoch  0, batch    70 | loss: 3.6264653MixupTrain:  epoch  0, batch    71 | loss: 3.2997422MixupTrain:  epoch  0, batch    72 | loss: 3.1833091MixupTrain:  epoch  0, batch    73 | loss: 3.2651646MixupTrain:  epoch  0, batch    74 | loss: 3.3977664MixupTrain:  epoch  0, batch    75 | loss: 3.2866061MixupTrain:  epoch  0, batch    76 | loss: 3.0053093MixupTrain:  epoch  0, batch    77 | loss: 3.2110538MixupTrain:  epoch  0, batch    78 | loss: 3.0441718MixupTrain:  epoch  0, batch    79 | loss: 3.1870599MixupTrain:  epoch  0, batch    80 | loss: 3.2147157MixupTrain:  epoch  0, batch    81 | loss: 3.0419416MixupTrain:  epoch  0, batch    82 | loss: 3.4773493MixupTrain:  epoch  0, batch    83 | loss: 3.2038782MixupTrain:  epoch  0, batch    84 | loss: 3.3446865MixupTrain:  epoch  0, batch    85 | loss: 3.1005025MixupTrain:  epoch  0, batch    86 | loss: 3.1797283MixupTrain:  epoch  0, batch    87 | loss: 3.1945560MixupTrain:  epoch  0, batch    88 | loss: 3.3141024MixupTrain:  epoch  0, batch    89 | loss: 3.2276824MixupTrain:  epoch  0, batch    90 | loss: 3.2151456MixupTrain:  epoch  0, batch    91 | loss: 3.2081289MixupTrain:  epoch  0, batch    92 | loss: 3.1698122MixupTrain:  epoch  0, batch    93 | loss: 3.0344081MixupTrain:  epoch  0, batch    94 | loss: 3.2915621MixupTrain:  epoch  0, batch    95 | loss: 3.3183665MixupTrain:  epoch  0, batch    96 | loss: 3.1159945MixupTrain:  epoch  0, batch    97 | loss: 3.0468597MixupTrain:  epoch  0, batch    98 | loss: 3.3102593MixupTrain:  epoch  0, batch    99 | loss: 3.1407166MixupTrain:  epoch  0, batch   100 | loss: 3.0101433MixupTrain:  epoch  0, batch   101 | loss: 3.0732801MixupTrain:  epoch  0, batch   102 | loss: 3.0247922MixupTrain:  epoch  0, batch   103 | loss: 3.0720606MixupTrain:  epoch  0, batch   104 | loss: 3.0541639MixupTrain:  epoch  0, batch   105 | loss: 3.0348916MixupTrain:  epoch  0, batch   106 | loss: 3.1960862MixupTrain:  epoch  0, batch   107 | loss: 3.0138750MixupTrain:  epoch  0, batch   108 | loss: 3.2807024MixupTrain:  epoch  0, batch   109 | loss: 2.9869759MixupTrain:  epoch  0, batch   110 | loss: 3.0766277MixupTrain:  epoch  0, batch   111 | loss: 2.9557257MixupTrain:  epoch  0, batch   112 | loss: 2.9357779MixupTrain:  epoch  0, batch   113 | loss: 3.0874958MixupTrain:  epoch  0, batch   114 | loss: 2.9736652MixupTrain:  epoch  0, batch   115 | loss: 3.0919394MixupTrain:  epoch  0, batch   116 | loss: 3.0723314MixupTrain:  epoch  0, batch   117 | loss: 2.9591780MixupTrain:  epoch  0, batch   118 | loss: 3.1670680MixupTrain:  epoch  0, batch   119 | loss: 3.0579784MixupTrain:  epoch  0, batch   120 | loss: 2.9673743MixupTrain:  epoch  0, batch   121 | loss: 3.2734811MixupTrain:  epoch  0, batch   122 | loss: 3.1008716MixupTrain:  epoch  0, batch   123 | loss: 3.0451498MixupTrain:  epoch  0, batch   124 | loss: 3.0219071MixupTrain:  epoch  0, batch   125 | loss: 2.9502523MixupTrain:  epoch  0, batch   126 | loss: 2.8714640MixupTrain:  epoch  0, batch   127 | loss: 3.0629709MixupTrain:  epoch  0, batch   128 | loss: 2.8630550MixupTrain:  epoch  0, batch   129 | loss: 3.0958362MixupTrain:  epoch  0, batch   130 | loss: 2.9977188MixupTrain:  epoch  0, batch   131 | loss: 2.8197997MixupTrain:  epoch  0, batch   132 | loss: 3.1068940MixupTrain:  epoch  0, batch   133 | loss: 2.7944093MixupTrain:  epoch  0, batch   134 | loss: 3.1119206MixupTrain:  epoch  0, batch   135 | loss: 3.0309324MixupTrain:  epoch  0, batch   136 | loss: 3.1027451MixupTrain:  epoch  0, batch   137 | loss: 3.0266542MixupTrain:  epoch  0, batch   138 | loss: 3.2393479MixupTrain:  epoch  0, batch   139 | loss: 2.9374852MixupTrain:  epoch  0, batch   140 | loss: 2.9061716MixupTrain:  epoch  0, batch   141 | loss: 3.1411824MixupTrain:  epoch  0, batch   142 | loss: 3.0005169MixupTrain:  epoch  0, batch   143 | loss: 3.1589510MixupTrain:  epoch  0, batch   144 | loss: 2.9571877MixupTrain:  epoch  0, batch   145 | loss: 3.0941582MixupTrain:  epoch  0, batch   146 | loss: 3.0074253MixupTrain:  epoch  0, batch   147 | loss: 2.9953461MixupTrain:  epoch  0, batch   148 | loss: 2.9831655MixupTrain:  epoch  0, batch   149 | loss: 3.0154133MixupTrain:  epoch  0, batch   150 | loss: 2.7953806MixupTrain:  epoch  0, batch   151 | loss: 2.9721627MixupTrain:  epoch  0, batch   152 | loss: 2.8742738MixupTrain:  epoch  0, batch   153 | loss: 2.7839570MixupTrain:  epoch  0, batch   154 | loss: 3.0811980MixupTrain:  epoch  0, batch   155 | loss: 3.0745587MixupTrain:  epoch  0, batch   156 | loss: 2.8294384MixupTrain:  epoch  0, batch   157 | loss: 2.8420527MixupTrain:  epoch  0, batch   158 | loss: 2.8192902MixupTrain:  epoch  0, batch   159 | loss: 3.0552707MixupTrain:  epoch  0, batch   160 | loss: 2.9706955MixupTrain:  epoch  0, batch   161 | loss: 3.1239085MixupTrain:  epoch  0, batch   162 | loss: 2.9331162MixupTrain:  epoch  0, batch   163 | loss: 2.9849794MixupTrain:  epoch  0, batch   164 | loss: 2.8980851MixupTrain:  epoch  0, batch   165 | loss: 2.9208632MixupTrain:  epoch  0, batch   166 | loss: 2.9781020MixupTrain:  epoch  0, batch   167 | loss: 3.0073361MixupTrain:  epoch  0, batch   168 | loss: 3.0451455MixupTrain:  epoch  0, batch   169 | loss: 2.9377694MixupTrain:  epoch  0, batch   170 | loss: 2.8594275MixupTrain:  epoch  0, batch   171 | loss: 2.9475944MixupTrain:  epoch  0, batch   172 | loss: 2.9217792MixupTrain:  epoch  0, batch   173 | loss: 2.9932783MixupTrain:  epoch  0, batch   174 | loss: 2.8943305MixupTrain:  epoch  0, batch   175 | loss: 2.9817247MixupTrain:  epoch  0, batch   176 | loss: 3.0091567MixupTrain:  epoch  0, batch   177 | loss: 3.0121102MixupTrain:  epoch  0, batch   178 | loss: 2.9299903MixupTrain:  epoch  0, batch   179 | loss: 3.0437994MixupTrain:  epoch  0, batch   180 | loss: 2.7195990MixupTrain:  epoch  0, batch   181 | loss: 3.0707045MixupTrain:  epoch  0, batch   182 | loss: 2.9207993MixupTrain:  epoch  0, batch   183 | loss: 2.9379153MixupTrain:  epoch  0, batch   184 | loss: 2.8537130MixupTrain:  epoch  0, batch   185 | loss: 2.9201283MixupTrain:  epoch  0, batch   186 | loss: 3.0266535MixupTrain:  epoch  0, batch   187 | loss: 3.1636038MixupTrain:  epoch  0, batch   188 | loss: 2.9296761MixupTrain:  epoch  0, batch   189 | loss: 3.1042130MixupTrain:  epoch  0, batch   190 | loss: 3.0050948MixupTrain:  epoch  0, batch   191 | loss: 3.0343056MixupTrain:  epoch  0, batch   192 | loss: 2.8227530MixupTrain:  epoch  0, batch   193 | loss: 2.8885620MixupTrain:  epoch  0, batch   194 | loss: 2.7637892MixupTrain:  epoch  0, batch   195 | loss: 2.8367472MixupTrain:  epoch  0, batch   196 | loss: 2.9069052MixupTrain:  epoch  0, batch   197 | loss: 2.7900414MixupTrain:  epoch  0, batch   198 | loss: 2.8265958MixupTrain:  epoch  0, batch   199 | loss: 2.8761756MixupTrain:  epoch  0, batch   200 | loss: 2.9869485MixupTrain:  epoch  0, batch   201 | loss: 2.9180422MixupTrain:  epoch  0, batch   202 | loss: 2.8805380MixupTrain:  epoch  0, batch   203 | loss: 2.9428186MixupTrain:  epoch  0, batch   204 | loss: 2.9368148MixupTrain:  epoch  0, batch   205 | loss: 2.9144387MixupTrain:  epoch  0, batch   206 | loss: 3.0459909MixupTrain:  epoch  0, batch   207 | loss: 3.1737704MixupTrain:  epoch  0, batch   208 | loss: 2.7609792MixupTrain:  epoch  0, batch   209 | loss: 2.9416294MixupTrain:  epoch  0, batch   210 | loss: 2.8797832MixupTrain:  epoch  0, batch   211 | loss: 2.8650064MixupTrain:  epoch  0, batch   212 | loss: 2.9422977MixupTrain:  epoch  0, batch   213 | loss: 3.0450761MixupTrain:  epoch  0, batch   214 | loss: 3.0081582MixupTrain:  epoch  0, batch   215 | loss: 2.9613690MixupTrain:  epoch  0, batch   216 | loss: 2.9374452MixupTrain:  epoch  0, batch   217 | loss: 3.0229969MixupTrain:  epoch  0, batch   218 | loss: 3.0533876MixupTrain:  epoch  0, batch   219 | loss: 2.9393568MixupTrain:  epoch  0, batch   220 | loss: 3.0152650MixupTrain:  epoch  0, batch   221 | loss: 2.8752747MixupTrain:  epoch  0, batch   222 | loss: 3.0748916MixupTrain:  epoch  0, batch   223 | loss: 2.9629111MixupTrain:  epoch  0, batch   224 | loss: 3.0811367MixupTrain:  epoch  0, batch   225 | loss: 2.8362706MixupTrain:  epoch  0, batch   226 | loss: 2.8575277MixupTrain:  epoch  0, batch   227 | loss: 2.9732251
MemoryTrain:  epoch  0, batch     0 | loss: 1.5449231MemoryTrain:  epoch  0, batch     1 | loss: 1.8076818MemoryTrain:  epoch  0, batch     2 | loss: 2.0789809MemoryTrain:  epoch  0, batch     3 | loss: 2.0063210MemoryTrain:  epoch  1, batch     0 | loss: 1.5794429MemoryTrain:  epoch  1, batch     1 | loss: 1.4822454MemoryTrain:  epoch  1, batch     2 | loss: 1.3982172MemoryTrain:  epoch  1, batch     3 | loss: 1.3945547MemoryTrain:  epoch  2, batch     0 | loss: 1.3846960MemoryTrain:  epoch  2, batch     1 | loss: 1.3532908MemoryTrain:  epoch  2, batch     2 | loss: 1.5222573MemoryTrain:  epoch  2, batch     3 | loss: 1.3569528MemoryTrain:  epoch  3, batch     0 | loss: 1.2845230MemoryTrain:  epoch  3, batch     1 | loss: 1.2761147MemoryTrain:  epoch  3, batch     2 | loss: 1.4019678MemoryTrain:  epoch  3, batch     3 | loss: 1.3629812MemoryTrain:  epoch  4, batch     0 | loss: 1.3575084MemoryTrain:  epoch  4, batch     1 | loss: 1.2771003MemoryTrain:  epoch  4, batch     2 | loss: 1.3185740MemoryTrain:  epoch  4, batch     3 | loss: 1.3203857MemoryTrain:  epoch  5, batch     0 | loss: 1.3272476MemoryTrain:  epoch  5, batch     1 | loss: 1.2914232MemoryTrain:  epoch  5, batch     2 | loss: 1.3709828MemoryTrain:  epoch  5, batch     3 | loss: 1.2692963MemoryTrain:  epoch  6, batch     0 | loss: 1.3112025MemoryTrain:  epoch  6, batch     1 | loss: 1.3303297MemoryTrain:  epoch  6, batch     2 | loss: 1.3002445MemoryTrain:  epoch  6, batch     3 | loss: 1.3042922MemoryTrain:  epoch  7, batch     0 | loss: 1.2682440MemoryTrain:  epoch  7, batch     1 | loss: 1.3114576MemoryTrain:  epoch  7, batch     2 | loss: 1.2107072MemoryTrain:  epoch  7, batch     3 | loss: 1.3047509MemoryTrain:  epoch  8, batch     0 | loss: 1.2423627MemoryTrain:  epoch  8, batch     1 | loss: 1.3106065MemoryTrain:  epoch  8, batch     2 | loss: 1.2417786MemoryTrain:  epoch  8, batch     3 | loss: 1.2572907MemoryTrain:  epoch  9, batch     0 | loss: 1.3811691MemoryTrain:  epoch  9, batch     1 | loss: 1.2433633MemoryTrain:  epoch  9, batch     2 | loss: 1.2748460MemoryTrain:  epoch  9, batch     3 | loss: 1.2729554
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 91.67%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 90.62%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 87.50%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 85.42%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 88.28%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 88.75%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 88.64%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 89.06%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 89.42%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 89.73%   [EVAL] batch:   14 | acc: 100.00%,  total acc: 90.42%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 91.02%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 91.54%   [EVAL] batch:   17 | acc: 18.75%,  total acc: 87.50%   
[EVAL] batch:    0 | acc: 37.50%,  total acc: 37.50%   [EVAL] batch:    1 | acc: 37.50%,  total acc: 37.50%   [EVAL] batch:    2 | acc: 43.75%,  total acc: 39.58%   [EVAL] batch:    3 | acc: 12.50%,  total acc: 32.81%   [EVAL] batch:    4 | acc: 43.75%,  total acc: 35.00%   [EVAL] batch:    5 | acc: 50.00%,  total acc: 37.50%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 42.86%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 50.00%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 54.17%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 57.50%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 59.66%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 61.46%   [EVAL] batch:   12 | acc: 56.25%,  total acc: 61.06%   [EVAL] batch:   13 | acc: 25.00%,  total acc: 58.48%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 59.17%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 58.98%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 59.93%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 60.07%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 60.20%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 60.94%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 62.80%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 64.49%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 66.03%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 67.45%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 68.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 69.95%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 70.83%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 71.88%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 72.84%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 73.33%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 74.19%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 74.80%   [EVAL] batch:   32 | acc: 62.50%,  total acc: 74.43%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 75.18%   [EVAL] batch:   34 | acc: 62.50%,  total acc: 74.82%   [EVAL] batch:   35 | acc: 31.25%,  total acc: 73.61%   [EVAL] batch:   36 | acc: 12.50%,  total acc: 71.96%   [EVAL] batch:   37 | acc: 37.50%,  total acc: 71.05%   [EVAL] batch:   38 | acc: 18.75%,  total acc: 69.71%   [EVAL] batch:   39 | acc: 56.25%,  total acc: 69.38%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 69.97%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 70.68%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 71.37%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 72.02%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 72.64%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 73.23%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 73.80%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 74.35%   [EVAL] batch:   48 | acc: 43.75%,  total acc: 73.72%   [EVAL] batch:   49 | acc: 68.75%,  total acc: 73.62%   [EVAL] batch:   50 | acc: 81.25%,  total acc: 73.77%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 74.28%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 74.76%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 75.23%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 75.45%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 75.67%   [EVAL] batch:   56 | acc: 87.50%,  total acc: 75.88%   [EVAL] batch:   57 | acc: 81.25%,  total acc: 75.97%   [EVAL] batch:   58 | acc: 68.75%,  total acc: 75.85%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 76.25%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 76.64%   [EVAL] batch:   61 | acc: 75.00%,  total acc: 76.61%   [EVAL] batch:   62 | acc: 100.00%,  total acc: 76.98%   [EVAL] batch:   63 | acc: 93.75%,  total acc: 77.25%   [EVAL] batch:   64 | acc: 87.50%,  total acc: 77.40%   [EVAL] batch:   65 | acc: 93.75%,  total acc: 77.65%   [EVAL] batch:   66 | acc: 100.00%,  total acc: 77.99%   [EVAL] batch:   67 | acc: 93.75%,  total acc: 78.22%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 78.53%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 78.84%   [EVAL] batch:   70 | acc: 43.75%,  total acc: 78.35%   
cur_acc:  ['0.8636', '0.3750', '0.9241', '0.8750']
his_acc:  ['0.8636', '0.7281', '0.7546', '0.7835']
CurrentTrain: epoch  0, batch     0 | loss: 4.0995302CurrentTrain: epoch  0, batch     1 | loss: 4.5462871CurrentTrain: epoch  1, batch     0 | loss: 3.2939291CurrentTrain: epoch  1, batch     1 | loss: 2.5921102CurrentTrain: epoch  2, batch     0 | loss: 2.5792549CurrentTrain: epoch  2, batch     1 | loss: 2.3953640CurrentTrain: epoch  3, batch     0 | loss: 2.4311919CurrentTrain: epoch  3, batch     1 | loss: 2.2422824CurrentTrain: epoch  4, batch     0 | loss: 2.2208371CurrentTrain: epoch  4, batch     1 | loss: 2.3091691CurrentTrain: epoch  5, batch     0 | loss: 2.1994879CurrentTrain: epoch  5, batch     1 | loss: 2.0766165CurrentTrain: epoch  6, batch     0 | loss: 2.1300268CurrentTrain: epoch  6, batch     1 | loss: 2.0880699CurrentTrain: epoch  7, batch     0 | loss: 1.9902124CurrentTrain: epoch  7, batch     1 | loss: 1.9698141CurrentTrain: epoch  8, batch     0 | loss: 2.1179054CurrentTrain: epoch  8, batch     1 | loss: 1.9558220CurrentTrain: epoch  9, batch     0 | loss: 1.9865680CurrentTrain: epoch  9, batch     1 | loss: 1.8921988
#### person cause of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person cause of death" means a person was died of an event.
Here is an example:
Relation: person cause of death
Context: mr scheider had suffered from multiple myeloma for several years , and died of complications from a staph infection , his wife , brenda siemer , said .
Head Entity: mr scheider
Tail Entity: complications from a staph infection
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person cause of death":
Relation: person cause of death  
Context: after battling with lung cancer for over a year, john doe passed away peacefully in his sleep, surrounded by family.  
Head Entity: john doe  
Tail Entity: lung cancer  

Relation: person cause of death  
Context: the renowned scientist, dr. emily white, tragically lost her life due to a car accident while returning from a conference.  
Head Entity: dr. emily white  
Tail Entity: car accident  
#### organization political religious affiliation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization political religious affiliation" means an organization is the member of a religion.
Here is an example:
Relation: organization political religious affiliation
Context: the militia has also suffered from allegations of being involved in clashes with rival shiite militias , particularly the badr corp , the military wing linked to supreme iraqi islamic council , the party led by powerful politician abdel aziz al-hakim .
Head Entity: badr corp
Tail Entity: shiite
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization political religious affiliation":
Relation: organization political religious affiliation  
Context: The Christian Democratic Union has been a significant player in German politics, often aligning itself with various religious groups to promote policies that reflect its Christian values.  
Head Entity: Christian Democratic Union  
Tail Entity: Christian  

Relation: organization political religious affiliation  
Context: The Islamic Society of North America has been actively involved in advocating for Muslim rights and representation in the political landscape of the United States.  
Head Entity: Islamic Society of North America  
Tail Entity: Muslim  
#### organization stateorprovince of headquarters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization stateorprovince of headquarters" means an organization is located in a state or province.
Here is an example:
Relation: organization stateorprovince of headquarters
Context: andrew lebow , an oil trader with mf global in new york , said investors have been discouraged by lower-than-expected oil imports in china and the disappointing growth in the u.s. economy .
Head Entity: mf global
Tail Entity: new york
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization stateorprovince of headquarters":
Relation: organization stateorprovince of headquarters  
Context: the headquarters of tech giant apple inc. is located in cupertino, california, where it has been a hub for innovation and development.  
Head Entity: apple inc.  
Tail Entity: cupertino, california  

Relation: organization stateorprovince of headquarters  
Context: the multinational corporation samsung electronics has its main office situated in suwon, south korea, which plays a crucial role in its global operations.  
Head Entity: samsung electronics  
Tail Entity: suwon, south korea  
#### person other family ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person other family" means a person 's relative is a person.
Here is an example:
Relation: person other family
Context: first there was the residents ' contest , in which meredith -lrb- ellen pompeo -rrb- , cristina -lrb- sandra oh -rrb- , alex -lrb- justin chambers -rrb- and izzie -lrb- katherine heigl -rrb- earned points for things like number of sutures and surgeries scrubbed in on .
Head Entity: ellen pompeo
Tail Entity: izzie
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person other family":
Relation: person other family  
Context: During the family reunion, Sarah introduced her cousin, Michael, to everyone, sharing stories about their childhood adventures and how they used to spend summers together at their grandparents' house.  
Head Entity: Sarah  
Tail Entity: Michael  

Relation: person other family  
Context: At the wedding, Emily was thrilled to see her aunt, who had traveled from across the country to celebrate the special day with her niece and the rest of the family.  
Head Entity: Emily  
Tail Entity: aunt  
#### person city of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person city of death" means a person was died in a city.
Here is an example:
Relation: person city of death
Context: irene morgan kirkaldy , 90 , who died of alzheimer 's disease aug. 10 at her home in gloucester , va. , quietly changed history in 1944 when she refused to give up her seat on a crowded greyhound bus to a white couple .
Head Entity: her
Tail Entity: gloucester
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person city of death":
Relation: person city of death  
Context: john smith, a renowned author, passed away on march 5 in his apartment located in new york city, leaving behind a legacy of literary works that inspired many.  
Head Entity: john smith  
Tail Entity: new york city  

Relation: person city of death  
Context: after a long battle with cancer, elena rodriguez died on july 15 in the vibrant city of los angeles, where she had spent most of her life advocating for social justice.  
Head Entity: elena rodriguez  
Tail Entity: los angeles  
Mixup data size:  5050
MixupTrain:  epoch  0, batch     0 | loss: 3.4873769MixupTrain:  epoch  0, batch     1 | loss: 4.0238557MixupTrain:  epoch  0, batch     2 | loss: 3.8062727MixupTrain:  epoch  0, batch     3 | loss: 3.3654861MixupTrain:  epoch  0, batch     4 | loss: 4.0964079MixupTrain:  epoch  0, batch     5 | loss: 4.0784831MixupTrain:  epoch  0, batch     6 | loss: 4.0818081MixupTrain:  epoch  0, batch     7 | loss: 3.9880559MixupTrain:  epoch  0, batch     8 | loss: 3.6462896MixupTrain:  epoch  0, batch     9 | loss: 3.6848736MixupTrain:  epoch  0, batch    10 | loss: 4.2583833MixupTrain:  epoch  0, batch    11 | loss: 4.1006508MixupTrain:  epoch  0, batch    12 | loss: 3.7568486MixupTrain:  epoch  0, batch    13 | loss: 3.6367149MixupTrain:  epoch  0, batch    14 | loss: 4.0714345MixupTrain:  epoch  0, batch    15 | loss: 3.9829526MixupTrain:  epoch  0, batch    16 | loss: 3.6994371MixupTrain:  epoch  0, batch    17 | loss: 3.7187796MixupTrain:  epoch  0, batch    18 | loss: 3.7496924MixupTrain:  epoch  0, batch    19 | loss: 3.8933806MixupTrain:  epoch  0, batch    20 | loss: 3.6374393MixupTrain:  epoch  0, batch    21 | loss: 3.7827001MixupTrain:  epoch  0, batch    22 | loss: 3.8360410MixupTrain:  epoch  0, batch    23 | loss: 3.6319721MixupTrain:  epoch  0, batch    24 | loss: 3.6922700MixupTrain:  epoch  0, batch    25 | loss: 3.8294001MixupTrain:  epoch  0, batch    26 | loss: 3.7820113MixupTrain:  epoch  0, batch    27 | loss: 3.3995113MixupTrain:  epoch  0, batch    28 | loss: 3.7176495MixupTrain:  epoch  0, batch    29 | loss: 4.5520887MixupTrain:  epoch  0, batch    30 | loss: 3.0214214MixupTrain:  epoch  0, batch    31 | loss: 3.2674856MixupTrain:  epoch  0, batch    32 | loss: 3.7145252MixupTrain:  epoch  0, batch    33 | loss: 3.7275786MixupTrain:  epoch  0, batch    34 | loss: 3.7399306MixupTrain:  epoch  0, batch    35 | loss: 3.4281464MixupTrain:  epoch  0, batch    36 | loss: 3.3379459MixupTrain:  epoch  0, batch    37 | loss: 3.4756966MixupTrain:  epoch  0, batch    38 | loss: 3.6497660MixupTrain:  epoch  0, batch    39 | loss: 3.5593851MixupTrain:  epoch  0, batch    40 | loss: 3.6266639MixupTrain:  epoch  0, batch    41 | loss: 3.2994494MixupTrain:  epoch  0, batch    42 | loss: 3.6914682MixupTrain:  epoch  0, batch    43 | loss: 3.4920659MixupTrain:  epoch  0, batch    44 | loss: 3.2658100MixupTrain:  epoch  0, batch    45 | loss: 3.5773361MixupTrain:  epoch  0, batch    46 | loss: 3.3637857MixupTrain:  epoch  0, batch    47 | loss: 4.1753316MixupTrain:  epoch  0, batch    48 | loss: 3.5100732MixupTrain:  epoch  0, batch    49 | loss: 3.4840016MixupTrain:  epoch  0, batch    50 | loss: 3.3597705MixupTrain:  epoch  0, batch    51 | loss: 3.2224925MixupTrain:  epoch  0, batch    52 | loss: 3.4246383MixupTrain:  epoch  0, batch    53 | loss: 3.4544706MixupTrain:  epoch  0, batch    54 | loss: 3.1992147MixupTrain:  epoch  0, batch    55 | loss: 3.3273473MixupTrain:  epoch  0, batch    56 | loss: 3.2860172MixupTrain:  epoch  0, batch    57 | loss: 3.4829078MixupTrain:  epoch  0, batch    58 | loss: 3.4425759MixupTrain:  epoch  0, batch    59 | loss: 3.3254657MixupTrain:  epoch  0, batch    60 | loss: 3.4136195MixupTrain:  epoch  0, batch    61 | loss: 3.5376773MixupTrain:  epoch  0, batch    62 | loss: 3.6852050MixupTrain:  epoch  0, batch    63 | loss: 3.3649120MixupTrain:  epoch  0, batch    64 | loss: 3.6436667MixupTrain:  epoch  0, batch    65 | loss: 3.5067940MixupTrain:  epoch  0, batch    66 | loss: 3.5182037MixupTrain:  epoch  0, batch    67 | loss: 3.9132719MixupTrain:  epoch  0, batch    68 | loss: 3.7042580MixupTrain:  epoch  0, batch    69 | loss: 3.5014277MixupTrain:  epoch  0, batch    70 | loss: 3.8230546MixupTrain:  epoch  0, batch    71 | loss: 3.3391945MixupTrain:  epoch  0, batch    72 | loss: 3.5348089MixupTrain:  epoch  0, batch    73 | loss: 3.0789273MixupTrain:  epoch  0, batch    74 | loss: 3.1483514MixupTrain:  epoch  0, batch    75 | loss: 3.4259467MixupTrain:  epoch  0, batch    76 | loss: 3.4335432MixupTrain:  epoch  0, batch    77 | loss: 3.2493787MixupTrain:  epoch  0, batch    78 | loss: 3.7539456MixupTrain:  epoch  0, batch    79 | loss: 3.0332484MixupTrain:  epoch  0, batch    80 | loss: 3.3891153MixupTrain:  epoch  0, batch    81 | loss: 3.4382071MixupTrain:  epoch  0, batch    82 | loss: 3.2704334MixupTrain:  epoch  0, batch    83 | loss: 3.2874773MixupTrain:  epoch  0, batch    84 | loss: 3.5241978MixupTrain:  epoch  0, batch    85 | loss: 3.1210306MixupTrain:  epoch  0, batch    86 | loss: 3.4893336MixupTrain:  epoch  0, batch    87 | loss: 3.7548475MixupTrain:  epoch  0, batch    88 | loss: 3.3683677MixupTrain:  epoch  0, batch    89 | loss: 3.4162273MixupTrain:  epoch  0, batch    90 | loss: 3.3628731MixupTrain:  epoch  0, batch    91 | loss: 3.3108780MixupTrain:  epoch  0, batch    92 | loss: 3.3511105MixupTrain:  epoch  0, batch    93 | loss: 3.3210552MixupTrain:  epoch  0, batch    94 | loss: 3.2546718MixupTrain:  epoch  0, batch    95 | loss: 3.3031750MixupTrain:  epoch  0, batch    96 | loss: 3.5945396MixupTrain:  epoch  0, batch    97 | loss: 3.2344732MixupTrain:  epoch  0, batch    98 | loss: 3.5676553MixupTrain:  epoch  0, batch    99 | loss: 3.5064721MixupTrain:  epoch  0, batch   100 | loss: 3.6428223MixupTrain:  epoch  0, batch   101 | loss: 3.1720033MixupTrain:  epoch  0, batch   102 | loss: 3.1547871MixupTrain:  epoch  0, batch   103 | loss: 3.1042190MixupTrain:  epoch  0, batch   104 | loss: 3.4810627MixupTrain:  epoch  0, batch   105 | loss: 3.3380053MixupTrain:  epoch  0, batch   106 | loss: 3.3524804MixupTrain:  epoch  0, batch   107 | loss: 3.1161895MixupTrain:  epoch  0, batch   108 | loss: 3.3920925MixupTrain:  epoch  0, batch   109 | loss: 3.3580363MixupTrain:  epoch  0, batch   110 | loss: 3.0590699MixupTrain:  epoch  0, batch   111 | loss: 3.1162033MixupTrain:  epoch  0, batch   112 | loss: 3.3535583MixupTrain:  epoch  0, batch   113 | loss: 3.2886477MixupTrain:  epoch  0, batch   114 | loss: 3.5471787MixupTrain:  epoch  0, batch   115 | loss: 3.2959704MixupTrain:  epoch  0, batch   116 | loss: 3.3525622MixupTrain:  epoch  0, batch   117 | loss: 3.3970335MixupTrain:  epoch  0, batch   118 | loss: 3.4196713MixupTrain:  epoch  0, batch   119 | loss: 3.4431295MixupTrain:  epoch  0, batch   120 | loss: 3.5592785MixupTrain:  epoch  0, batch   121 | loss: 3.0389216MixupTrain:  epoch  0, batch   122 | loss: 3.2758162MixupTrain:  epoch  0, batch   123 | loss: 3.2771173MixupTrain:  epoch  0, batch   124 | loss: 3.1700730MixupTrain:  epoch  0, batch   125 | loss: 3.3217335MixupTrain:  epoch  0, batch   126 | loss: 3.4406610MixupTrain:  epoch  0, batch   127 | loss: 3.2339616MixupTrain:  epoch  0, batch   128 | loss: 3.1781049MixupTrain:  epoch  0, batch   129 | loss: 3.0336261MixupTrain:  epoch  0, batch   130 | loss: 3.1400685MixupTrain:  epoch  0, batch   131 | loss: 3.2834473MixupTrain:  epoch  0, batch   132 | loss: 3.1220789MixupTrain:  epoch  0, batch   133 | loss: 3.2683764MixupTrain:  epoch  0, batch   134 | loss: 3.2164917MixupTrain:  epoch  0, batch   135 | loss: 3.5067647MixupTrain:  epoch  0, batch   136 | loss: 3.1157694MixupTrain:  epoch  0, batch   137 | loss: 3.3890710MixupTrain:  epoch  0, batch   138 | loss: 3.6578388MixupTrain:  epoch  0, batch   139 | loss: 2.9717450MixupTrain:  epoch  0, batch   140 | loss: 3.2950535MixupTrain:  epoch  0, batch   141 | loss: 3.1082215MixupTrain:  epoch  0, batch   142 | loss: 3.0514514MixupTrain:  epoch  0, batch   143 | loss: 3.4239550MixupTrain:  epoch  0, batch   144 | loss: 3.0219893MixupTrain:  epoch  0, batch   145 | loss: 3.0556884MixupTrain:  epoch  0, batch   146 | loss: 3.3406553MixupTrain:  epoch  0, batch   147 | loss: 3.1297574MixupTrain:  epoch  0, batch   148 | loss: 3.2425370MixupTrain:  epoch  0, batch   149 | loss: 3.0882721MixupTrain:  epoch  0, batch   150 | loss: 3.3321693MixupTrain:  epoch  0, batch   151 | loss: 3.0605602MixupTrain:  epoch  0, batch   152 | loss: 3.0522230MixupTrain:  epoch  0, batch   153 | loss: 3.3481526MixupTrain:  epoch  0, batch   154 | loss: 3.1689336MixupTrain:  epoch  0, batch   155 | loss: 3.1968865MixupTrain:  epoch  0, batch   156 | loss: 2.9428854MixupTrain:  epoch  0, batch   157 | loss: 3.2822900MixupTrain:  epoch  0, batch   158 | loss: 3.3391614MixupTrain:  epoch  0, batch   159 | loss: 3.2542605MixupTrain:  epoch  0, batch   160 | loss: 3.0445743MixupTrain:  epoch  0, batch   161 | loss: 3.0466428MixupTrain:  epoch  0, batch   162 | loss: 3.2010121MixupTrain:  epoch  0, batch   163 | loss: 3.0883675MixupTrain:  epoch  0, batch   164 | loss: 3.0255084MixupTrain:  epoch  0, batch   165 | loss: 3.0999794MixupTrain:  epoch  0, batch   166 | loss: 3.1736155MixupTrain:  epoch  0, batch   167 | loss: 3.3298097MixupTrain:  epoch  0, batch   168 | loss: 3.2304523MixupTrain:  epoch  0, batch   169 | loss: 3.1508164MixupTrain:  epoch  0, batch   170 | loss: 3.0514073MixupTrain:  epoch  0, batch   171 | loss: 3.2349010MixupTrain:  epoch  0, batch   172 | loss: 3.1849315MixupTrain:  epoch  0, batch   173 | loss: 3.2173085MixupTrain:  epoch  0, batch   174 | loss: 3.3837445MixupTrain:  epoch  0, batch   175 | loss: 3.0802126MixupTrain:  epoch  0, batch   176 | loss: 3.4299431MixupTrain:  epoch  0, batch   177 | loss: 3.1570106MixupTrain:  epoch  0, batch   178 | loss: 3.1765409MixupTrain:  epoch  0, batch   179 | loss: 3.2801003MixupTrain:  epoch  0, batch   180 | loss: 3.1279747MixupTrain:  epoch  0, batch   181 | loss: 3.5762975MixupTrain:  epoch  0, batch   182 | loss: 3.2222228MixupTrain:  epoch  0, batch   183 | loss: 3.2435927MixupTrain:  epoch  0, batch   184 | loss: 2.9161515MixupTrain:  epoch  0, batch   185 | loss: 3.1016397MixupTrain:  epoch  0, batch   186 | loss: 2.9495542MixupTrain:  epoch  0, batch   187 | loss: 3.0703795MixupTrain:  epoch  0, batch   188 | loss: 3.1652164MixupTrain:  epoch  0, batch   189 | loss: 3.0808825MixupTrain:  epoch  0, batch   190 | loss: 3.3503141MixupTrain:  epoch  0, batch   191 | loss: 3.3781810MixupTrain:  epoch  0, batch   192 | loss: 2.9069402MixupTrain:  epoch  0, batch   193 | loss: 3.4598136MixupTrain:  epoch  0, batch   194 | loss: 3.5001745MixupTrain:  epoch  0, batch   195 | loss: 3.1567523MixupTrain:  epoch  0, batch   196 | loss: 3.1545799MixupTrain:  epoch  0, batch   197 | loss: 3.0962338MixupTrain:  epoch  0, batch   198 | loss: 3.1850572MixupTrain:  epoch  0, batch   199 | loss: 2.9313471MixupTrain:  epoch  0, batch   200 | loss: 3.0977154MixupTrain:  epoch  0, batch   201 | loss: 3.2323637MixupTrain:  epoch  0, batch   202 | loss: 3.0856237MixupTrain:  epoch  0, batch   203 | loss: 2.9909911MixupTrain:  epoch  0, batch   204 | loss: 3.1519508MixupTrain:  epoch  0, batch   205 | loss: 3.2734270MixupTrain:  epoch  0, batch   206 | loss: 3.0138307MixupTrain:  epoch  0, batch   207 | loss: 3.0862520MixupTrain:  epoch  0, batch   208 | loss: 2.9505599MixupTrain:  epoch  0, batch   209 | loss: 3.2060144MixupTrain:  epoch  0, batch   210 | loss: 3.1055119MixupTrain:  epoch  0, batch   211 | loss: 3.3097029MixupTrain:  epoch  0, batch   212 | loss: 3.4300659MixupTrain:  epoch  0, batch   213 | loss: 3.0126019MixupTrain:  epoch  0, batch   214 | loss: 3.2715979MixupTrain:  epoch  0, batch   215 | loss: 3.2047234MixupTrain:  epoch  0, batch   216 | loss: 3.0678792MixupTrain:  epoch  0, batch   217 | loss: 2.9736366MixupTrain:  epoch  0, batch   218 | loss: 3.2402906MixupTrain:  epoch  0, batch   219 | loss: 2.9459851MixupTrain:  epoch  0, batch   220 | loss: 3.2532725MixupTrain:  epoch  0, batch   221 | loss: 3.0970545MixupTrain:  epoch  0, batch   222 | loss: 3.0379212MixupTrain:  epoch  0, batch   223 | loss: 3.1440740MixupTrain:  epoch  0, batch   224 | loss: 3.0072913MixupTrain:  epoch  0, batch   225 | loss: 3.3221612MixupTrain:  epoch  0, batch   226 | loss: 2.9555595MixupTrain:  epoch  0, batch   227 | loss: 3.0927544MixupTrain:  epoch  0, batch   228 | loss: 3.5637221MixupTrain:  epoch  0, batch   229 | loss: 2.9117274MixupTrain:  epoch  0, batch   230 | loss: 3.2096958MixupTrain:  epoch  0, batch   231 | loss: 3.0890245MixupTrain:  epoch  0, batch   232 | loss: 3.0605638MixupTrain:  epoch  0, batch   233 | loss: 3.1446595MixupTrain:  epoch  0, batch   234 | loss: 3.3782473MixupTrain:  epoch  0, batch   235 | loss: 3.0318444MixupTrain:  epoch  0, batch   236 | loss: 3.2349238MixupTrain:  epoch  0, batch   237 | loss: 3.0326569MixupTrain:  epoch  0, batch   238 | loss: 3.0877523MixupTrain:  epoch  0, batch   239 | loss: 3.0842030MixupTrain:  epoch  0, batch   240 | loss: 2.9013290MixupTrain:  epoch  0, batch   241 | loss: 3.1477985MixupTrain:  epoch  0, batch   242 | loss: 3.1926963MixupTrain:  epoch  0, batch   243 | loss: 3.0183454MixupTrain:  epoch  0, batch   244 | loss: 3.3030550MixupTrain:  epoch  0, batch   245 | loss: 3.2224219MixupTrain:  epoch  0, batch   246 | loss: 3.0591762MixupTrain:  epoch  0, batch   247 | loss: 3.1424716MixupTrain:  epoch  0, batch   248 | loss: 3.0146534MixupTrain:  epoch  0, batch   249 | loss: 3.2380548MixupTrain:  epoch  0, batch   250 | loss: 3.0920935MixupTrain:  epoch  0, batch   251 | loss: 3.0163376MixupTrain:  epoch  0, batch   252 | loss: 3.2627392MixupTrain:  epoch  0, batch   253 | loss: 3.0382445MixupTrain:  epoch  0, batch   254 | loss: 3.0668797MixupTrain:  epoch  0, batch   255 | loss: 3.1772318MixupTrain:  epoch  0, batch   256 | loss: 3.1823044MixupTrain:  epoch  0, batch   257 | loss: 3.0762234MixupTrain:  epoch  0, batch   258 | loss: 3.4525938MixupTrain:  epoch  0, batch   259 | loss: 3.3673782MixupTrain:  epoch  0, batch   260 | loss: 3.1040096MixupTrain:  epoch  0, batch   261 | loss: 3.1456580MixupTrain:  epoch  0, batch   262 | loss: 3.1653101MixupTrain:  epoch  0, batch   263 | loss: 3.1838040MixupTrain:  epoch  0, batch   264 | loss: 3.0568690MixupTrain:  epoch  0, batch   265 | loss: 3.2322202MixupTrain:  epoch  0, batch   266 | loss: 2.8316302MixupTrain:  epoch  0, batch   267 | loss: 3.0268776MixupTrain:  epoch  0, batch   268 | loss: 3.3693621MixupTrain:  epoch  0, batch   269 | loss: 3.1651690MixupTrain:  epoch  0, batch   270 | loss: 3.0954187MixupTrain:  epoch  0, batch   271 | loss: 3.2681060MixupTrain:  epoch  0, batch   272 | loss: 3.1320744MixupTrain:  epoch  0, batch   273 | loss: 2.9316890MixupTrain:  epoch  0, batch   274 | loss: 3.3057938MixupTrain:  epoch  0, batch   275 | loss: 3.0033631MixupTrain:  epoch  0, batch   276 | loss: 3.0530660MixupTrain:  epoch  0, batch   277 | loss: 3.2262225MixupTrain:  epoch  0, batch   278 | loss: 3.0887723MixupTrain:  epoch  0, batch   279 | loss: 3.1313894MixupTrain:  epoch  0, batch   280 | loss: 2.9882002MixupTrain:  epoch  0, batch   281 | loss: 3.0032153MixupTrain:  epoch  0, batch   282 | loss: 3.2347932MixupTrain:  epoch  0, batch   283 | loss: 3.2664859MixupTrain:  epoch  0, batch   284 | loss: 2.8658650MixupTrain:  epoch  0, batch   285 | loss: 2.9884679MixupTrain:  epoch  0, batch   286 | loss: 3.1121607MixupTrain:  epoch  0, batch   287 | loss: 3.2007170MixupTrain:  epoch  0, batch   288 | loss: 2.9014955MixupTrain:  epoch  0, batch   289 | loss: 2.9815035MixupTrain:  epoch  0, batch   290 | loss: 2.8510025MixupTrain:  epoch  0, batch   291 | loss: 2.9757600MixupTrain:  epoch  0, batch   292 | loss: 2.9579484MixupTrain:  epoch  0, batch   293 | loss: 2.9995370MixupTrain:  epoch  0, batch   294 | loss: 3.2634034MixupTrain:  epoch  0, batch   295 | loss: 3.0411470MixupTrain:  epoch  0, batch   296 | loss: 2.9290075MixupTrain:  epoch  0, batch   297 | loss: 2.9844429MixupTrain:  epoch  0, batch   298 | loss: 3.0165138MixupTrain:  epoch  0, batch   299 | loss: 3.1674652MixupTrain:  epoch  0, batch   300 | loss: 3.2729278MixupTrain:  epoch  0, batch   301 | loss: 3.2188888MixupTrain:  epoch  0, batch   302 | loss: 3.0522518MixupTrain:  epoch  0, batch   303 | loss: 3.0907171MixupTrain:  epoch  0, batch   304 | loss: 2.9477530MixupTrain:  epoch  0, batch   305 | loss: 2.9707868MixupTrain:  epoch  0, batch   306 | loss: 3.0299613MixupTrain:  epoch  0, batch   307 | loss: 3.1940098MixupTrain:  epoch  0, batch   308 | loss: 3.1207399MixupTrain:  epoch  0, batch   309 | loss: 2.8656182MixupTrain:  epoch  0, batch   310 | loss: 3.0520594MixupTrain:  epoch  0, batch   311 | loss: 2.9856138MixupTrain:  epoch  0, batch   312 | loss: 3.1210814MixupTrain:  epoch  0, batch   313 | loss: 3.2223215MixupTrain:  epoch  0, batch   314 | loss: 3.1349988MixupTrain:  epoch  0, batch   315 | loss: 3.0692294
MemoryTrain:  epoch  0, batch     0 | loss: 1.4130416MemoryTrain:  epoch  0, batch     1 | loss: 1.4054643MemoryTrain:  epoch  0, batch     2 | loss: 1.8315845MemoryTrain:  epoch  0, batch     3 | loss: 2.1653543MemoryTrain:  epoch  0, batch     4 | loss: 1.8776594MemoryTrain:  epoch  1, batch     0 | loss: 1.3420084MemoryTrain:  epoch  1, batch     1 | loss: 1.6377362MemoryTrain:  epoch  1, batch     2 | loss: 1.4006400MemoryTrain:  epoch  1, batch     3 | loss: 1.7519546MemoryTrain:  epoch  1, batch     4 | loss: 1.2195190MemoryTrain:  epoch  2, batch     0 | loss: 1.4866025MemoryTrain:  epoch  2, batch     1 | loss: 1.2777669MemoryTrain:  epoch  2, batch     2 | loss: 1.3726215MemoryTrain:  epoch  2, batch     3 | loss: 1.3095024MemoryTrain:  epoch  2, batch     4 | loss: 1.4668318MemoryTrain:  epoch  3, batch     0 | loss: 1.4243410MemoryTrain:  epoch  3, batch     1 | loss: 1.4272025MemoryTrain:  epoch  3, batch     2 | loss: 1.3637072MemoryTrain:  epoch  3, batch     3 | loss: 1.2442341MemoryTrain:  epoch  3, batch     4 | loss: 1.2302690MemoryTrain:  epoch  4, batch     0 | loss: 1.2765850MemoryTrain:  epoch  4, batch     1 | loss: 1.3156128MemoryTrain:  epoch  4, batch     2 | loss: 1.3491859MemoryTrain:  epoch  4, batch     3 | loss: 1.3029971MemoryTrain:  epoch  4, batch     4 | loss: 1.2537687MemoryTrain:  epoch  5, batch     0 | loss: 1.3620527MemoryTrain:  epoch  5, batch     1 | loss: 1.2934115MemoryTrain:  epoch  5, batch     2 | loss: 1.2127254MemoryTrain:  epoch  5, batch     3 | loss: 1.2327139MemoryTrain:  epoch  5, batch     4 | loss: 1.3996425MemoryTrain:  epoch  6, batch     0 | loss: 1.2439095MemoryTrain:  epoch  6, batch     1 | loss: 1.2290816MemoryTrain:  epoch  6, batch     2 | loss: 1.1952322MemoryTrain:  epoch  6, batch     3 | loss: 1.1933017MemoryTrain:  epoch  6, batch     4 | loss: 1.2034009MemoryTrain:  epoch  7, batch     0 | loss: 1.2009218MemoryTrain:  epoch  7, batch     1 | loss: 1.2270966MemoryTrain:  epoch  7, batch     2 | loss: 1.2227223MemoryTrain:  epoch  7, batch     3 | loss: 1.2429979MemoryTrain:  epoch  7, batch     4 | loss: 1.2088670MemoryTrain:  epoch  8, batch     0 | loss: 1.2349007MemoryTrain:  epoch  8, batch     1 | loss: 1.2301230MemoryTrain:  epoch  8, batch     2 | loss: 1.2136199MemoryTrain:  epoch  8, batch     3 | loss: 1.2505983MemoryTrain:  epoch  8, batch     4 | loss: 1.2510068MemoryTrain:  epoch  9, batch     0 | loss: 1.1985409MemoryTrain:  epoch  9, batch     1 | loss: 1.2496775MemoryTrain:  epoch  9, batch     2 | loss: 1.2208436MemoryTrain:  epoch  9, batch     3 | loss: 1.2195265MemoryTrain:  epoch  9, batch     4 | loss: 1.2650846
[EVAL] batch:    0 | acc: 50.00%,  total acc: 50.00%   [EVAL] batch:    1 | acc: 75.00%,  total acc: 62.50%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 66.67%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 67.19%   [EVAL] batch:    4 | acc: 43.75%,  total acc: 62.50%   [EVAL] batch:    5 | acc: 50.00%,  total acc: 60.42%   [EVAL] batch:    6 | acc: 37.50%,  total acc: 57.14%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 61.72%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 62.50%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 64.38%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 67.05%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 69.27%   [EVAL] batch:   12 | acc: 50.00%,  total acc: 67.79%   
[EVAL] batch:    0 | acc: 43.75%,  total acc: 43.75%   [EVAL] batch:    1 | acc: 43.75%,  total acc: 43.75%   [EVAL] batch:    2 | acc: 50.00%,  total acc: 45.83%   [EVAL] batch:    3 | acc: 31.25%,  total acc: 42.19%   [EVAL] batch:    4 | acc: 43.75%,  total acc: 42.50%   [EVAL] batch:    5 | acc: 50.00%,  total acc: 43.75%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 48.21%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 54.69%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 58.33%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 61.25%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 64.77%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 66.15%   [EVAL] batch:   12 | acc: 50.00%,  total acc: 64.90%   [EVAL] batch:   13 | acc: 25.00%,  total acc: 62.05%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 62.50%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 62.11%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 62.87%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 62.85%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 62.83%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 63.44%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 65.18%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 66.48%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 67.93%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 69.01%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 70.25%   [EVAL] batch:   25 | acc: 87.50%,  total acc: 70.91%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 71.76%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 72.77%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 73.71%   [EVAL] batch:   29 | acc: 75.00%,  total acc: 73.75%   [EVAL] batch:   30 | acc: 81.25%,  total acc: 73.99%   [EVAL] batch:   31 | acc: 87.50%,  total acc: 74.41%   [EVAL] batch:   32 | acc: 62.50%,  total acc: 74.05%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 74.82%   [EVAL] batch:   34 | acc: 68.75%,  total acc: 74.64%   [EVAL] batch:   35 | acc: 37.50%,  total acc: 73.61%   [EVAL] batch:   36 | acc: 12.50%,  total acc: 71.96%   [EVAL] batch:   37 | acc: 37.50%,  total acc: 71.05%   [EVAL] batch:   38 | acc: 6.25%,  total acc: 69.39%   [EVAL] batch:   39 | acc: 56.25%,  total acc: 69.06%   [EVAL] batch:   40 | acc: 87.50%,  total acc: 69.51%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 70.24%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 70.93%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 71.59%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 72.22%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 72.83%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 73.40%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 73.96%   [EVAL] batch:   48 | acc: 43.75%,  total acc: 73.34%   [EVAL] batch:   49 | acc: 50.00%,  total acc: 72.88%   [EVAL] batch:   50 | acc: 31.25%,  total acc: 72.06%   [EVAL] batch:   51 | acc: 37.50%,  total acc: 71.39%   [EVAL] batch:   52 | acc: 50.00%,  total acc: 70.99%   [EVAL] batch:   53 | acc: 25.00%,  total acc: 70.14%   [EVAL] batch:   54 | acc: 6.25%,  total acc: 68.98%   [EVAL] batch:   55 | acc: 25.00%,  total acc: 68.19%   [EVAL] batch:   56 | acc: 12.50%,  total acc: 67.21%   [EVAL] batch:   57 | acc: 18.75%,  total acc: 66.38%   [EVAL] batch:   58 | acc: 12.50%,  total acc: 65.47%   [EVAL] batch:   59 | acc: 50.00%,  total acc: 65.21%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 65.78%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 65.83%   [EVAL] batch:   62 | acc: 37.50%,  total acc: 65.38%   [EVAL] batch:   63 | acc: 50.00%,  total acc: 65.14%   [EVAL] batch:   64 | acc: 81.25%,  total acc: 65.38%   [EVAL] batch:   65 | acc: 93.75%,  total acc: 65.81%   [EVAL] batch:   66 | acc: 100.00%,  total acc: 66.32%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 66.82%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 67.30%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 67.77%   [EVAL] batch:   70 | acc: 75.00%,  total acc: 67.87%   [EVAL] batch:   71 | acc: 62.50%,  total acc: 67.80%   [EVAL] batch:   72 | acc: 75.00%,  total acc: 67.89%   [EVAL] batch:   73 | acc: 68.75%,  total acc: 67.91%   [EVAL] batch:   74 | acc: 56.25%,  total acc: 67.75%   [EVAL] batch:   75 | acc: 50.00%,  total acc: 67.52%   [EVAL] batch:   76 | acc: 37.50%,  total acc: 67.13%   [EVAL] batch:   77 | acc: 75.00%,  total acc: 67.23%   [EVAL] batch:   78 | acc: 87.50%,  total acc: 67.48%   [EVAL] batch:   79 | acc: 62.50%,  total acc: 67.42%   [EVAL] batch:   80 | acc: 87.50%,  total acc: 67.67%   [EVAL] batch:   81 | acc: 100.00%,  total acc: 68.06%   [EVAL] batch:   82 | acc: 87.50%,  total acc: 68.30%   [EVAL] batch:   83 | acc: 6.25%,  total acc: 67.56%   
cur_acc:  ['0.8636', '0.3750', '0.9241', '0.8750', '0.6779']
his_acc:  ['0.8636', '0.7281', '0.7546', '0.7835', '0.6756']
CurrentTrain: epoch  0, batch     0 | loss: 5.6418524CurrentTrain: epoch  0, batch     1 | loss: 6.6747251CurrentTrain: epoch  1, batch     0 | loss: 4.8064804CurrentTrain: epoch  1, batch     1 | loss: 4.7275577CurrentTrain: epoch  2, batch     0 | loss: 3.6905427CurrentTrain: epoch  2, batch     1 | loss: 3.3914096CurrentTrain: epoch  3, batch     0 | loss: 3.3334374CurrentTrain: epoch  3, batch     1 | loss: 2.6026046CurrentTrain: epoch  4, batch     0 | loss: 2.8755522CurrentTrain: epoch  4, batch     1 | loss: 2.7138398CurrentTrain: epoch  5, batch     0 | loss: 2.7615023CurrentTrain: epoch  5, batch     1 | loss: 2.7159834CurrentTrain: epoch  6, batch     0 | loss: 2.4426541CurrentTrain: epoch  6, batch     1 | loss: 2.4539762CurrentTrain: epoch  7, batch     0 | loss: 2.2620003CurrentTrain: epoch  7, batch     1 | loss: 2.2268856CurrentTrain: epoch  8, batch     0 | loss: 2.2147388CurrentTrain: epoch  8, batch     1 | loss: 2.1365099CurrentTrain: epoch  9, batch     0 | loss: 2.1990232CurrentTrain: epoch  9, batch     1 | loss: 1.9893777
#### person date of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person date of birth" means a person was born in a date.
Here is an example:
Relation: person date of birth
Context: born of schoolteacher parents in the western town of sabaneta on july 28 , 1954 , chavez studied at the military academy of venezuela in caracas .
Head Entity: chavez
Tail Entity: july 28 , 1954
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person date of birth":
Relation: person date of birth  
Context: the famous physicist albert einstein was born in ulm, germany, on march 14, 1879, and later developed the theory of relativity.  
Head Entity: albert einstein  
Tail Entity: march 14, 1879  

Relation: person date of birth  
Context: the renowned author mark twain, known for his classic novels, was born in florida, missouri, on november 30, 1835.  
Head Entity: mark twain  
Tail Entity: november 30, 1835  
#### person stateorprovince of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person stateorprovince of birth" means a person was died in a state or province.
Here is an example:
Relation: person stateorprovince of birth
Context: joseph simpson farland was born on aug 11 , 1914 , in clarksburg , wva , the only child of richard and grace simpson farland .
Head Entity: joseph simpson farland
Tail Entity: wva
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person stateorprovince of birth":
Relation: person stateorprovince of birth  
Context: martha stewart was born on august 3, 1941, in jersey city, new jersey, where she spent her early years before moving to connecticut.  
Head Entity: martha stewart  
Tail Entity: new jersey  

Relation: person stateorprovince of birth  
Context: barack obama was born on august 4, 1961, in honolulu, hawaii, and later became the 44th president of the united states.  
Head Entity: barack obama  
Tail Entity: hawaii  
#### person parents ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person parents" means a person 's parent is a person.
Here is an example:
Relation: person parents
Context: as the case developed , sandy 's mother , denise sandy , quietly made herself a spectral but central figure , by faithfully attending pretrial hearings .
Head Entity: sandy
Tail Entity: denise sandy
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person parents":
Relation: person parents  
Context: After the ceremony, Emily's father, John Smith, shared heartfelt stories about her childhood, bringing tears to many eyes.  
Head Entity: Emily  
Tail Entity: John Smith  

Relation: person parents  
Context: During the family reunion, Michael's mother, Sarah Johnson, prepared her famous lasagna, which everyone eagerly devoured.  
Head Entity: Michael  
Tail Entity: Sarah Johnson  
#### person employee of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person employee of" means a person is an employee of an organization.
Here is an example:
Relation: person employee of
Context: seeking revenge , axel reunites with old pal sgt. billy rosewood -lrb- judge reinhold -rrb- and jon flint -lrb- hector elizondo -rrb- of the beverly hills police department .
Head Entity: hector elizondo
Tail Entity: beverly hills police department
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person employee of":
Relation: person employee of  
Context: After years of hard work and dedication, Sarah Thompson finally received a promotion at the tech company where she has been employed since college.  
Head Entity: Sarah Thompson  
Tail Entity: tech company  

Relation: person employee of  
Context: John Smith has been a loyal employee at Greenfield Farms for over a decade, contributing to the growth and success of the organization.  
Head Entity: John Smith  
Tail Entity: Greenfield Farms  
#### person stateorprovince of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person stateorprovince of death" means a person was died in a state or province.
Here is an example:
Relation: person stateorprovince of death
Context: irene morgan kirkaldy , whose defiance of bus segregation laws more than a decade before rosa parks ' landmark case helped lay the foundation for later civil rights victories , died friday at her home in hayes , va. .
Head Entity: irene morgan kirkaldy
Tail Entity: va.
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person stateorprovince of death":
Relation: person stateorprovince of death  
Context: john doe, a renowned author known for his impactful novels, passed away peacefully in his sleep at his residence in california last night.  
Head Entity: john doe  
Tail Entity: california  

Relation: person stateorprovince of death  
Context: after a long battle with illness, elizabeth taylor, the iconic actress, succumbed to her health issues and died in a hospital in nevada.  
Head Entity: elizabeth taylor  
Tail Entity: nevada  
Mixup data size:  6685
MixupTrain:  epoch  0, batch     0 | loss: 3.7736685MixupTrain:  epoch  0, batch     1 | loss: 3.8538687MixupTrain:  epoch  0, batch     2 | loss: 4.1132374MixupTrain:  epoch  0, batch     3 | loss: 3.9606924MixupTrain:  epoch  0, batch     4 | loss: 4.3135624MixupTrain:  epoch  0, batch     5 | loss: 3.8499675MixupTrain:  epoch  0, batch     6 | loss: 3.9366367MixupTrain:  epoch  0, batch     7 | loss: 3.8552992MixupTrain:  epoch  0, batch     8 | loss: 4.0464478MixupTrain:  epoch  0, batch     9 | loss: 3.8134830MixupTrain:  epoch  0, batch    10 | loss: 4.1399403MixupTrain:  epoch  0, batch    11 | loss: 4.4176264MixupTrain:  epoch  0, batch    12 | loss: 3.9828010MixupTrain:  epoch  0, batch    13 | loss: 4.1619596MixupTrain:  epoch  0, batch    14 | loss: 3.9074028MixupTrain:  epoch  0, batch    15 | loss: 3.3837249MixupTrain:  epoch  0, batch    16 | loss: 4.0050640MixupTrain:  epoch  0, batch    17 | loss: 3.4970961MixupTrain:  epoch  0, batch    18 | loss: 3.3525174MixupTrain:  epoch  0, batch    19 | loss: 3.6608605MixupTrain:  epoch  0, batch    20 | loss: 3.5566592MixupTrain:  epoch  0, batch    21 | loss: 3.4081392MixupTrain:  epoch  0, batch    22 | loss: 3.5234463MixupTrain:  epoch  0, batch    23 | loss: 3.5993991MixupTrain:  epoch  0, batch    24 | loss: 3.6455116MixupTrain:  epoch  0, batch    25 | loss: 3.8260913MixupTrain:  epoch  0, batch    26 | loss: 3.6928844MixupTrain:  epoch  0, batch    27 | loss: 3.3672514MixupTrain:  epoch  0, batch    28 | loss: 3.9770136MixupTrain:  epoch  0, batch    29 | loss: 3.6439059MixupTrain:  epoch  0, batch    30 | loss: 3.6516712MixupTrain:  epoch  0, batch    31 | loss: 3.2375212MixupTrain:  epoch  0, batch    32 | loss: 3.2304430MixupTrain:  epoch  0, batch    33 | loss: 3.4767728MixupTrain:  epoch  0, batch    34 | loss: 3.4604826MixupTrain:  epoch  0, batch    35 | loss: 3.2680445MixupTrain:  epoch  0, batch    36 | loss: 3.5501380MixupTrain:  epoch  0, batch    37 | loss: 3.4326160MixupTrain:  epoch  0, batch    38 | loss: 3.6313543MixupTrain:  epoch  0, batch    39 | loss: 3.2493095MixupTrain:  epoch  0, batch    40 | loss: 3.6543508MixupTrain:  epoch  0, batch    41 | loss: 3.5954041MixupTrain:  epoch  0, batch    42 | loss: 3.7267809MixupTrain:  epoch  0, batch    43 | loss: 3.5461311MixupTrain:  epoch  0, batch    44 | loss: 3.1482425MixupTrain:  epoch  0, batch    45 | loss: 3.4737382MixupTrain:  epoch  0, batch    46 | loss: 3.2751398MixupTrain:  epoch  0, batch    47 | loss: 3.5696087MixupTrain:  epoch  0, batch    48 | loss: 3.4031124MixupTrain:  epoch  0, batch    49 | loss: 3.5003786MixupTrain:  epoch  0, batch    50 | loss: 3.1777034MixupTrain:  epoch  0, batch    51 | loss: 3.5846810MixupTrain:  epoch  0, batch    52 | loss: 3.3536434MixupTrain:  epoch  0, batch    53 | loss: 3.2524891MixupTrain:  epoch  0, batch    54 | loss: 3.4682009MixupTrain:  epoch  0, batch    55 | loss: 3.1093469MixupTrain:  epoch  0, batch    56 | loss: 3.2235038MixupTrain:  epoch  0, batch    57 | loss: 3.5742633MixupTrain:  epoch  0, batch    58 | loss: 3.2604551MixupTrain:  epoch  0, batch    59 | loss: 3.4714973MixupTrain:  epoch  0, batch    60 | loss: 3.4022479MixupTrain:  epoch  0, batch    61 | loss: 3.3789344MixupTrain:  epoch  0, batch    62 | loss: 3.2614942MixupTrain:  epoch  0, batch    63 | loss: 3.5622020MixupTrain:  epoch  0, batch    64 | loss: 3.1752386MixupTrain:  epoch  0, batch    65 | loss: 3.3197546MixupTrain:  epoch  0, batch    66 | loss: 3.1445143MixupTrain:  epoch  0, batch    67 | loss: 3.0739207MixupTrain:  epoch  0, batch    68 | loss: 3.2755537MixupTrain:  epoch  0, batch    69 | loss: 3.3996284MixupTrain:  epoch  0, batch    70 | loss: 3.1387830MixupTrain:  epoch  0, batch    71 | loss: 3.3305430MixupTrain:  epoch  0, batch    72 | loss: 3.0434804MixupTrain:  epoch  0, batch    73 | loss: 3.3584058MixupTrain:  epoch  0, batch    74 | loss: 3.3597333MixupTrain:  epoch  0, batch    75 | loss: 3.1721377MixupTrain:  epoch  0, batch    76 | loss: 3.4233310MixupTrain:  epoch  0, batch    77 | loss: 3.2741327MixupTrain:  epoch  0, batch    78 | loss: 3.1996531MixupTrain:  epoch  0, batch    79 | loss: 3.3033957MixupTrain:  epoch  0, batch    80 | loss: 3.6496637MixupTrain:  epoch  0, batch    81 | loss: 3.3142152MixupTrain:  epoch  0, batch    82 | loss: 3.2455580MixupTrain:  epoch  0, batch    83 | loss: 3.2839911MixupTrain:  epoch  0, batch    84 | loss: 3.2369738MixupTrain:  epoch  0, batch    85 | loss: 3.1158502MixupTrain:  epoch  0, batch    86 | loss: 3.2284789MixupTrain:  epoch  0, batch    87 | loss: 3.4073977MixupTrain:  epoch  0, batch    88 | loss: 3.1171608MixupTrain:  epoch  0, batch    89 | loss: 3.3563805MixupTrain:  epoch  0, batch    90 | loss: 3.3304515MixupTrain:  epoch  0, batch    91 | loss: 3.3021493MixupTrain:  epoch  0, batch    92 | loss: 3.0381289MixupTrain:  epoch  0, batch    93 | loss: 3.2322221MixupTrain:  epoch  0, batch    94 | loss: 3.3706522MixupTrain:  epoch  0, batch    95 | loss: 3.1818216MixupTrain:  epoch  0, batch    96 | loss: 3.2729774MixupTrain:  epoch  0, batch    97 | loss: 3.1730437MixupTrain:  epoch  0, batch    98 | loss: 3.2288694MixupTrain:  epoch  0, batch    99 | loss: 3.1142704MixupTrain:  epoch  0, batch   100 | loss: 2.9548678MixupTrain:  epoch  0, batch   101 | loss: 3.1612182MixupTrain:  epoch  0, batch   102 | loss: 2.8791604MixupTrain:  epoch  0, batch   103 | loss: 3.3025718MixupTrain:  epoch  0, batch   104 | loss: 3.3608279MixupTrain:  epoch  0, batch   105 | loss: 3.0293531MixupTrain:  epoch  0, batch   106 | loss: 3.3870595MixupTrain:  epoch  0, batch   107 | loss: 3.1905599MixupTrain:  epoch  0, batch   108 | loss: 3.1512313MixupTrain:  epoch  0, batch   109 | loss: 3.2513928MixupTrain:  epoch  0, batch   110 | loss: 3.3602114MixupTrain:  epoch  0, batch   111 | loss: 3.1781964MixupTrain:  epoch  0, batch   112 | loss: 3.3027344MixupTrain:  epoch  0, batch   113 | loss: 3.4946775MixupTrain:  epoch  0, batch   114 | loss: 3.1495495MixupTrain:  epoch  0, batch   115 | loss: 3.0862813MixupTrain:  epoch  0, batch   116 | loss: 3.1931617MixupTrain:  epoch  0, batch   117 | loss: 3.3178217MixupTrain:  epoch  0, batch   118 | loss: 3.3587551MixupTrain:  epoch  0, batch   119 | loss: 3.2278106MixupTrain:  epoch  0, batch   120 | loss: 3.0434551MixupTrain:  epoch  0, batch   121 | loss: 3.2102122MixupTrain:  epoch  0, batch   122 | loss: 3.1158862MixupTrain:  epoch  0, batch   123 | loss: 2.9347143MixupTrain:  epoch  0, batch   124 | loss: 3.1803567MixupTrain:  epoch  0, batch   125 | loss: 3.1379199MixupTrain:  epoch  0, batch   126 | loss: 3.0917578MixupTrain:  epoch  0, batch   127 | loss: 3.0478027MixupTrain:  epoch  0, batch   128 | loss: 3.2703681MixupTrain:  epoch  0, batch   129 | loss: 3.1413503MixupTrain:  epoch  0, batch   130 | loss: 3.2747514MixupTrain:  epoch  0, batch   131 | loss: 3.0564120MixupTrain:  epoch  0, batch   132 | loss: 3.0140965MixupTrain:  epoch  0, batch   133 | loss: 3.1272783MixupTrain:  epoch  0, batch   134 | loss: 3.2823050MixupTrain:  epoch  0, batch   135 | loss: 3.0838821MixupTrain:  epoch  0, batch   136 | loss: 3.1184366MixupTrain:  epoch  0, batch   137 | loss: 3.2742891MixupTrain:  epoch  0, batch   138 | loss: 3.1482499MixupTrain:  epoch  0, batch   139 | loss: 3.0306320MixupTrain:  epoch  0, batch   140 | loss: 2.9279046MixupTrain:  epoch  0, batch   141 | loss: 2.9944410MixupTrain:  epoch  0, batch   142 | loss: 3.0936465MixupTrain:  epoch  0, batch   143 | loss: 2.7965674MixupTrain:  epoch  0, batch   144 | loss: 2.9939475MixupTrain:  epoch  0, batch   145 | loss: 3.0825307MixupTrain:  epoch  0, batch   146 | loss: 3.1181903MixupTrain:  epoch  0, batch   147 | loss: 3.2031522MixupTrain:  epoch  0, batch   148 | loss: 3.2896817MixupTrain:  epoch  0, batch   149 | loss: 3.1552682MixupTrain:  epoch  0, batch   150 | loss: 3.2145286MixupTrain:  epoch  0, batch   151 | loss: 3.0417013MixupTrain:  epoch  0, batch   152 | loss: 3.1567488MixupTrain:  epoch  0, batch   153 | loss: 2.8925157MixupTrain:  epoch  0, batch   154 | loss: 3.1457438MixupTrain:  epoch  0, batch   155 | loss: 2.9331026MixupTrain:  epoch  0, batch   156 | loss: 3.1911216MixupTrain:  epoch  0, batch   157 | loss: 3.3257902MixupTrain:  epoch  0, batch   158 | loss: 3.0946550MixupTrain:  epoch  0, batch   159 | loss: 3.1903238MixupTrain:  epoch  0, batch   160 | loss: 3.1057901MixupTrain:  epoch  0, batch   161 | loss: 3.1280491MixupTrain:  epoch  0, batch   162 | loss: 3.0876908MixupTrain:  epoch  0, batch   163 | loss: 3.1922455MixupTrain:  epoch  0, batch   164 | loss: 3.1511035MixupTrain:  epoch  0, batch   165 | loss: 3.1736946MixupTrain:  epoch  0, batch   166 | loss: 3.2913117MixupTrain:  epoch  0, batch   167 | loss: 3.0970256MixupTrain:  epoch  0, batch   168 | loss: 2.8885183MixupTrain:  epoch  0, batch   169 | loss: 3.0211413MixupTrain:  epoch  0, batch   170 | loss: 2.9599497MixupTrain:  epoch  0, batch   171 | loss: 3.3400722MixupTrain:  epoch  0, batch   172 | loss: 2.9855521MixupTrain:  epoch  0, batch   173 | loss: 2.8786993MixupTrain:  epoch  0, batch   174 | loss: 3.0638065MixupTrain:  epoch  0, batch   175 | loss: 3.1368978MixupTrain:  epoch  0, batch   176 | loss: 3.0684166MixupTrain:  epoch  0, batch   177 | loss: 3.1794372MixupTrain:  epoch  0, batch   178 | loss: 3.0464129MixupTrain:  epoch  0, batch   179 | loss: 3.2575006MixupTrain:  epoch  0, batch   180 | loss: 2.9694200MixupTrain:  epoch  0, batch   181 | loss: 3.2436359MixupTrain:  epoch  0, batch   182 | loss: 3.0033894MixupTrain:  epoch  0, batch   183 | loss: 3.0385456MixupTrain:  epoch  0, batch   184 | loss: 2.9826603MixupTrain:  epoch  0, batch   185 | loss: 3.0747101MixupTrain:  epoch  0, batch   186 | loss: 2.9811447MixupTrain:  epoch  0, batch   187 | loss: 2.9299548MixupTrain:  epoch  0, batch   188 | loss: 3.0927231MixupTrain:  epoch  0, batch   189 | loss: 2.9405756MixupTrain:  epoch  0, batch   190 | loss: 2.9404969MixupTrain:  epoch  0, batch   191 | loss: 3.0820618MixupTrain:  epoch  0, batch   192 | loss: 2.8800273MixupTrain:  epoch  0, batch   193 | loss: 3.1290133MixupTrain:  epoch  0, batch   194 | loss: 2.9296498MixupTrain:  epoch  0, batch   195 | loss: 2.9837952MixupTrain:  epoch  0, batch   196 | loss: 3.0951571MixupTrain:  epoch  0, batch   197 | loss: 3.0621781MixupTrain:  epoch  0, batch   198 | loss: 3.1953835MixupTrain:  epoch  0, batch   199 | loss: 3.1138549MixupTrain:  epoch  0, batch   200 | loss: 2.9867165MixupTrain:  epoch  0, batch   201 | loss: 3.0979393MixupTrain:  epoch  0, batch   202 | loss: 2.8761010MixupTrain:  epoch  0, batch   203 | loss: 3.0086935MixupTrain:  epoch  0, batch   204 | loss: 3.0237784MixupTrain:  epoch  0, batch   205 | loss: 3.0227735MixupTrain:  epoch  0, batch   206 | loss: 3.2806661MixupTrain:  epoch  0, batch   207 | loss: 3.0671482MixupTrain:  epoch  0, batch   208 | loss: 3.0516131MixupTrain:  epoch  0, batch   209 | loss: 3.0469997MixupTrain:  epoch  0, batch   210 | loss: 3.1102295MixupTrain:  epoch  0, batch   211 | loss: 2.9812593MixupTrain:  epoch  0, batch   212 | loss: 3.3020320MixupTrain:  epoch  0, batch   213 | loss: 3.2107277MixupTrain:  epoch  0, batch   214 | loss: 2.8759575MixupTrain:  epoch  0, batch   215 | loss: 3.1878026MixupTrain:  epoch  0, batch   216 | loss: 3.1956646MixupTrain:  epoch  0, batch   217 | loss: 2.9974232MixupTrain:  epoch  0, batch   218 | loss: 3.0012233MixupTrain:  epoch  0, batch   219 | loss: 3.0596976MixupTrain:  epoch  0, batch   220 | loss: 3.2308183MixupTrain:  epoch  0, batch   221 | loss: 3.0196376MixupTrain:  epoch  0, batch   222 | loss: 2.9902425MixupTrain:  epoch  0, batch   223 | loss: 3.0276823MixupTrain:  epoch  0, batch   224 | loss: 3.0836720MixupTrain:  epoch  0, batch   225 | loss: 2.9643507MixupTrain:  epoch  0, batch   226 | loss: 3.2148581MixupTrain:  epoch  0, batch   227 | loss: 2.7729595MixupTrain:  epoch  0, batch   228 | loss: 3.0704021MixupTrain:  epoch  0, batch   229 | loss: 2.9844804MixupTrain:  epoch  0, batch   230 | loss: 3.0569363MixupTrain:  epoch  0, batch   231 | loss: 2.9439073MixupTrain:  epoch  0, batch   232 | loss: 3.1260350MixupTrain:  epoch  0, batch   233 | loss: 3.2107229MixupTrain:  epoch  0, batch   234 | loss: 2.8828683MixupTrain:  epoch  0, batch   235 | loss: 3.1164093MixupTrain:  epoch  0, batch   236 | loss: 3.1445937MixupTrain:  epoch  0, batch   237 | loss: 2.7542734MixupTrain:  epoch  0, batch   238 | loss: 2.9976785MixupTrain:  epoch  0, batch   239 | loss: 3.1870465MixupTrain:  epoch  0, batch   240 | loss: 2.9721410MixupTrain:  epoch  0, batch   241 | loss: 2.9468508MixupTrain:  epoch  0, batch   242 | loss: 3.0567000MixupTrain:  epoch  0, batch   243 | loss: 3.0178916MixupTrain:  epoch  0, batch   244 | loss: 3.1978140MixupTrain:  epoch  0, batch   245 | loss: 3.0051744MixupTrain:  epoch  0, batch   246 | loss: 3.0271425MixupTrain:  epoch  0, batch   247 | loss: 2.9736128MixupTrain:  epoch  0, batch   248 | loss: 3.0191212MixupTrain:  epoch  0, batch   249 | loss: 2.9952488MixupTrain:  epoch  0, batch   250 | loss: 3.0135424MixupTrain:  epoch  0, batch   251 | loss: 3.2223797MixupTrain:  epoch  0, batch   252 | loss: 3.1037703MixupTrain:  epoch  0, batch   253 | loss: 3.0717850MixupTrain:  epoch  0, batch   254 | loss: 3.0576053MixupTrain:  epoch  0, batch   255 | loss: 3.1545811MixupTrain:  epoch  0, batch   256 | loss: 3.0768557MixupTrain:  epoch  0, batch   257 | loss: 3.0063379MixupTrain:  epoch  0, batch   258 | loss: 2.9267263MixupTrain:  epoch  0, batch   259 | loss: 3.0815420MixupTrain:  epoch  0, batch   260 | loss: 2.8743460MixupTrain:  epoch  0, batch   261 | loss: 3.0372360MixupTrain:  epoch  0, batch   262 | loss: 3.0497527MixupTrain:  epoch  0, batch   263 | loss: 3.1625183MixupTrain:  epoch  0, batch   264 | loss: 2.9283056MixupTrain:  epoch  0, batch   265 | loss: 2.8404717MixupTrain:  epoch  0, batch   266 | loss: 3.1122487MixupTrain:  epoch  0, batch   267 | loss: 2.9911506MixupTrain:  epoch  0, batch   268 | loss: 2.9485898MixupTrain:  epoch  0, batch   269 | loss: 2.8404095MixupTrain:  epoch  0, batch   270 | loss: 3.2001584MixupTrain:  epoch  0, batch   271 | loss: 2.7947035MixupTrain:  epoch  0, batch   272 | loss: 2.7621968MixupTrain:  epoch  0, batch   273 | loss: 3.0192418MixupTrain:  epoch  0, batch   274 | loss: 2.9638555MixupTrain:  epoch  0, batch   275 | loss: 3.0798113MixupTrain:  epoch  0, batch   276 | loss: 3.1405611MixupTrain:  epoch  0, batch   277 | loss: 2.9965105MixupTrain:  epoch  0, batch   278 | loss: 3.0065408MixupTrain:  epoch  0, batch   279 | loss: 3.0373139MixupTrain:  epoch  0, batch   280 | loss: 2.7626495MixupTrain:  epoch  0, batch   281 | loss: 3.1772072MixupTrain:  epoch  0, batch   282 | loss: 3.0191877MixupTrain:  epoch  0, batch   283 | loss: 2.8556886MixupTrain:  epoch  0, batch   284 | loss: 3.0370021MixupTrain:  epoch  0, batch   285 | loss: 3.0564101MixupTrain:  epoch  0, batch   286 | loss: 3.0153484MixupTrain:  epoch  0, batch   287 | loss: 2.9553757MixupTrain:  epoch  0, batch   288 | loss: 2.9814959MixupTrain:  epoch  0, batch   289 | loss: 3.0506105MixupTrain:  epoch  0, batch   290 | loss: 2.9774168MixupTrain:  epoch  0, batch   291 | loss: 3.0162742MixupTrain:  epoch  0, batch   292 | loss: 3.1309831MixupTrain:  epoch  0, batch   293 | loss: 3.0760012MixupTrain:  epoch  0, batch   294 | loss: 2.9945474MixupTrain:  epoch  0, batch   295 | loss: 2.8762679MixupTrain:  epoch  0, batch   296 | loss: 3.0233843MixupTrain:  epoch  0, batch   297 | loss: 2.8978138MixupTrain:  epoch  0, batch   298 | loss: 2.9407454MixupTrain:  epoch  0, batch   299 | loss: 2.9137051MixupTrain:  epoch  0, batch   300 | loss: 2.8710029MixupTrain:  epoch  0, batch   301 | loss: 2.8230739MixupTrain:  epoch  0, batch   302 | loss: 2.9341130MixupTrain:  epoch  0, batch   303 | loss: 3.1257491MixupTrain:  epoch  0, batch   304 | loss: 3.0268593MixupTrain:  epoch  0, batch   305 | loss: 2.9379864MixupTrain:  epoch  0, batch   306 | loss: 2.9775386MixupTrain:  epoch  0, batch   307 | loss: 2.9724879MixupTrain:  epoch  0, batch   308 | loss: 3.1401141MixupTrain:  epoch  0, batch   309 | loss: 2.9544473MixupTrain:  epoch  0, batch   310 | loss: 3.1644430MixupTrain:  epoch  0, batch   311 | loss: 3.2619586MixupTrain:  epoch  0, batch   312 | loss: 2.9379377MixupTrain:  epoch  0, batch   313 | loss: 3.0437338MixupTrain:  epoch  0, batch   314 | loss: 3.0873389MixupTrain:  epoch  0, batch   315 | loss: 3.1098425MixupTrain:  epoch  0, batch   316 | loss: 2.9071488MixupTrain:  epoch  0, batch   317 | loss: 3.0612631MixupTrain:  epoch  0, batch   318 | loss: 3.0021749MixupTrain:  epoch  0, batch   319 | loss: 3.0210500MixupTrain:  epoch  0, batch   320 | loss: 2.9722652MixupTrain:  epoch  0, batch   321 | loss: 3.0259070MixupTrain:  epoch  0, batch   322 | loss: 2.8831034MixupTrain:  epoch  0, batch   323 | loss: 2.9480867MixupTrain:  epoch  0, batch   324 | loss: 3.0233016MixupTrain:  epoch  0, batch   325 | loss: 2.9735470MixupTrain:  epoch  0, batch   326 | loss: 3.0548759MixupTrain:  epoch  0, batch   327 | loss: 2.9850807MixupTrain:  epoch  0, batch   328 | loss: 3.0327449MixupTrain:  epoch  0, batch   329 | loss: 3.1042991MixupTrain:  epoch  0, batch   330 | loss: 2.9452057MixupTrain:  epoch  0, batch   331 | loss: 2.7957103MixupTrain:  epoch  0, batch   332 | loss: 2.9411926MixupTrain:  epoch  0, batch   333 | loss: 3.1511989MixupTrain:  epoch  0, batch   334 | loss: 3.0657163MixupTrain:  epoch  0, batch   335 | loss: 2.9651196MixupTrain:  epoch  0, batch   336 | loss: 2.9339113MixupTrain:  epoch  0, batch   337 | loss: 2.8699470MixupTrain:  epoch  0, batch   338 | loss: 2.8387573MixupTrain:  epoch  0, batch   339 | loss: 2.9909554MixupTrain:  epoch  0, batch   340 | loss: 2.9069023MixupTrain:  epoch  0, batch   341 | loss: 2.9318538MixupTrain:  epoch  0, batch   342 | loss: 3.1092980MixupTrain:  epoch  0, batch   343 | loss: 3.0219259MixupTrain:  epoch  0, batch   344 | loss: 2.7766371MixupTrain:  epoch  0, batch   345 | loss: 2.8218365MixupTrain:  epoch  0, batch   346 | loss: 3.0755990MixupTrain:  epoch  0, batch   347 | loss: 2.9677458MixupTrain:  epoch  0, batch   348 | loss: 3.0140057MixupTrain:  epoch  0, batch   349 | loss: 3.0863075MixupTrain:  epoch  0, batch   350 | loss: 2.8056507MixupTrain:  epoch  0, batch   351 | loss: 3.0783584MixupTrain:  epoch  0, batch   352 | loss: 3.1007063MixupTrain:  epoch  0, batch   353 | loss: 3.1676195MixupTrain:  epoch  0, batch   354 | loss: 2.7694244MixupTrain:  epoch  0, batch   355 | loss: 3.1763225MixupTrain:  epoch  0, batch   356 | loss: 2.8904159MixupTrain:  epoch  0, batch   357 | loss: 2.9795215MixupTrain:  epoch  0, batch   358 | loss: 3.0652637MixupTrain:  epoch  0, batch   359 | loss: 2.8840625MixupTrain:  epoch  0, batch   360 | loss: 2.9368849MixupTrain:  epoch  0, batch   361 | loss: 2.8913198MixupTrain:  epoch  0, batch   362 | loss: 3.0700843MixupTrain:  epoch  0, batch   363 | loss: 3.0229061MixupTrain:  epoch  0, batch   364 | loss: 3.0895858MixupTrain:  epoch  0, batch   365 | loss: 2.9898694MixupTrain:  epoch  0, batch   366 | loss: 2.9105968MixupTrain:  epoch  0, batch   367 | loss: 3.1016414MixupTrain:  epoch  0, batch   368 | loss: 2.8244562MixupTrain:  epoch  0, batch   369 | loss: 3.0447655MixupTrain:  epoch  0, batch   370 | loss: 3.0344372MixupTrain:  epoch  0, batch   371 | loss: 2.9471409MixupTrain:  epoch  0, batch   372 | loss: 3.0939305MixupTrain:  epoch  0, batch   373 | loss: 2.9666243MixupTrain:  epoch  0, batch   374 | loss: 2.8581884MixupTrain:  epoch  0, batch   375 | loss: 2.8273954MixupTrain:  epoch  0, batch   376 | loss: 3.0262585MixupTrain:  epoch  0, batch   377 | loss: 2.9830282MixupTrain:  epoch  0, batch   378 | loss: 2.9225018MixupTrain:  epoch  0, batch   379 | loss: 3.1325197MixupTrain:  epoch  0, batch   380 | loss: 2.9433138MixupTrain:  epoch  0, batch   381 | loss: 2.8681490MixupTrain:  epoch  0, batch   382 | loss: 2.8723741MixupTrain:  epoch  0, batch   383 | loss: 3.0469365MixupTrain:  epoch  0, batch   384 | loss: 2.9092708MixupTrain:  epoch  0, batch   385 | loss: 2.8493304MixupTrain:  epoch  0, batch   386 | loss: 2.8565118MixupTrain:  epoch  0, batch   387 | loss: 2.8427157MixupTrain:  epoch  0, batch   388 | loss: 3.0346518MixupTrain:  epoch  0, batch   389 | loss: 2.9736962MixupTrain:  epoch  0, batch   390 | loss: 2.9567633MixupTrain:  epoch  0, batch   391 | loss: 2.9017954MixupTrain:  epoch  0, batch   392 | loss: 3.0617714MixupTrain:  epoch  0, batch   393 | loss: 2.9987409MixupTrain:  epoch  0, batch   394 | loss: 2.9543300MixupTrain:  epoch  0, batch   395 | loss: 3.0485611MixupTrain:  epoch  0, batch   396 | loss: 2.8466213MixupTrain:  epoch  0, batch   397 | loss: 2.9447026MixupTrain:  epoch  0, batch   398 | loss: 2.8460574MixupTrain:  epoch  0, batch   399 | loss: 3.0929084MixupTrain:  epoch  0, batch   400 | loss: 2.8353102MixupTrain:  epoch  0, batch   401 | loss: 3.0154481MixupTrain:  epoch  0, batch   402 | loss: 2.8945255MixupTrain:  epoch  0, batch   403 | loss: 2.8234038MixupTrain:  epoch  0, batch   404 | loss: 2.8595672MixupTrain:  epoch  0, batch   405 | loss: 2.9733531MixupTrain:  epoch  0, batch   406 | loss: 2.7903714MixupTrain:  epoch  0, batch   407 | loss: 2.9249680MixupTrain:  epoch  0, batch   408 | loss: 3.0193644MixupTrain:  epoch  0, batch   409 | loss: 2.9903731MixupTrain:  epoch  0, batch   410 | loss: 2.9461627MixupTrain:  epoch  0, batch   411 | loss: 3.0218720MixupTrain:  epoch  0, batch   412 | loss: 2.9916139MixupTrain:  epoch  0, batch   413 | loss: 2.8474946MixupTrain:  epoch  0, batch   414 | loss: 3.0910530MixupTrain:  epoch  0, batch   415 | loss: 2.9764485MixupTrain:  epoch  0, batch   416 | loss: 2.9974644MixupTrain:  epoch  0, batch   417 | loss: 2.9651141
MemoryTrain:  epoch  0, batch     0 | loss: 1.2803619MemoryTrain:  epoch  0, batch     1 | loss: 1.5265694MemoryTrain:  epoch  0, batch     2 | loss: 1.5360999MemoryTrain:  epoch  0, batch     3 | loss: 1.7288961MemoryTrain:  epoch  0, batch     4 | loss: 1.9203774MemoryTrain:  epoch  0, batch     5 | loss: 2.0152392MemoryTrain:  epoch  1, batch     0 | loss: 1.4240102MemoryTrain:  epoch  1, batch     1 | loss: 1.2546986MemoryTrain:  epoch  1, batch     2 | loss: 1.2954185MemoryTrain:  epoch  1, batch     3 | loss: 1.3523053MemoryTrain:  epoch  1, batch     4 | loss: 1.3011076MemoryTrain:  epoch  1, batch     5 | loss: 1.3501800MemoryTrain:  epoch  2, batch     0 | loss: 1.2444130MemoryTrain:  epoch  2, batch     1 | loss: 1.3535122MemoryTrain:  epoch  2, batch     2 | loss: 1.2543743MemoryTrain:  epoch  2, batch     3 | loss: 1.2709848MemoryTrain:  epoch  2, batch     4 | loss: 1.2910390MemoryTrain:  epoch  2, batch     5 | loss: 1.3061523MemoryTrain:  epoch  3, batch     0 | loss: 1.3295165MemoryTrain:  epoch  3, batch     1 | loss: 1.3259211MemoryTrain:  epoch  3, batch     2 | loss: 1.2761627MemoryTrain:  epoch  3, batch     3 | loss: 1.2721038MemoryTrain:  epoch  3, batch     4 | loss: 1.2354531MemoryTrain:  epoch  3, batch     5 | loss: 1.2514311MemoryTrain:  epoch  4, batch     0 | loss: 1.2682447MemoryTrain:  epoch  4, batch     1 | loss: 1.2971312MemoryTrain:  epoch  4, batch     2 | loss: 1.2475377MemoryTrain:  epoch  4, batch     3 | loss: 1.2431694MemoryTrain:  epoch  4, batch     4 | loss: 1.2452350MemoryTrain:  epoch  4, batch     5 | loss: 1.2231833MemoryTrain:  epoch  5, batch     0 | loss: 1.2781696MemoryTrain:  epoch  5, batch     1 | loss: 1.2131135MemoryTrain:  epoch  5, batch     2 | loss: 1.2663934MemoryTrain:  epoch  5, batch     3 | loss: 1.1899698MemoryTrain:  epoch  5, batch     4 | loss: 1.2090713MemoryTrain:  epoch  5, batch     5 | loss: 1.1995405MemoryTrain:  epoch  6, batch     0 | loss: 1.2353022MemoryTrain:  epoch  6, batch     1 | loss: 1.2024605MemoryTrain:  epoch  6, batch     2 | loss: 1.1972249MemoryTrain:  epoch  6, batch     3 | loss: 1.2305517MemoryTrain:  epoch  6, batch     4 | loss: 1.2012634MemoryTrain:  epoch  6, batch     5 | loss: 1.2112175MemoryTrain:  epoch  7, batch     0 | loss: 1.2259030MemoryTrain:  epoch  7, batch     1 | loss: 1.2280929MemoryTrain:  epoch  7, batch     2 | loss: 1.1891420MemoryTrain:  epoch  7, batch     3 | loss: 1.2519057MemoryTrain:  epoch  7, batch     4 | loss: 1.2887294MemoryTrain:  epoch  7, batch     5 | loss: 1.2338756MemoryTrain:  epoch  8, batch     0 | loss: 1.1871601MemoryTrain:  epoch  8, batch     1 | loss: 1.1888235MemoryTrain:  epoch  8, batch     2 | loss: 1.2039168MemoryTrain:  epoch  8, batch     3 | loss: 1.1671594MemoryTrain:  epoch  8, batch     4 | loss: 1.1700857MemoryTrain:  epoch  8, batch     5 | loss: 1.2560740MemoryTrain:  epoch  9, batch     0 | loss: 1.2211056MemoryTrain:  epoch  9, batch     1 | loss: 1.2318509MemoryTrain:  epoch  9, batch     2 | loss: 1.2294248MemoryTrain:  epoch  9, batch     3 | loss: 1.2447993MemoryTrain:  epoch  9, batch     4 | loss: 1.2572618MemoryTrain:  epoch  9, batch     5 | loss: 1.1843717
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:    3 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 91.25%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 89.58%   [EVAL] batch:    6 | acc: 62.50%,  total acc: 85.71%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 86.72%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 88.19%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 89.38%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 88.64%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 88.02%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:   13 | acc: 25.00%,  total acc: 83.04%   
[EVAL] batch:    0 | acc: 50.00%,  total acc: 50.00%   [EVAL] batch:    1 | acc: 50.00%,  total acc: 50.00%   [EVAL] batch:    2 | acc: 50.00%,  total acc: 50.00%   [EVAL] batch:    3 | acc: 37.50%,  total acc: 46.88%   [EVAL] batch:    4 | acc: 43.75%,  total acc: 46.25%   [EVAL] batch:    5 | acc: 43.75%,  total acc: 45.83%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 50.89%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 57.03%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 61.11%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 63.75%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 67.05%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 68.23%   [EVAL] batch:   12 | acc: 56.25%,  total acc: 67.31%   [EVAL] batch:   13 | acc: 25.00%,  total acc: 64.29%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 65.00%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 64.45%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 65.07%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 64.93%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 64.80%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 65.94%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 67.26%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 68.47%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 69.84%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 70.83%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 72.00%   [EVAL] batch:   25 | acc: 87.50%,  total acc: 72.60%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 73.38%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 74.33%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 75.22%   [EVAL] batch:   29 | acc: 75.00%,  total acc: 75.21%   [EVAL] batch:   30 | acc: 81.25%,  total acc: 75.40%   [EVAL] batch:   31 | acc: 87.50%,  total acc: 75.78%   [EVAL] batch:   32 | acc: 62.50%,  total acc: 75.38%   [EVAL] batch:   33 | acc: 87.50%,  total acc: 75.74%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 75.71%   [EVAL] batch:   35 | acc: 25.00%,  total acc: 74.31%   [EVAL] batch:   36 | acc: 12.50%,  total acc: 72.64%   [EVAL] batch:   37 | acc: 31.25%,  total acc: 71.55%   [EVAL] batch:   38 | acc: 0.00%,  total acc: 69.71%   [EVAL] batch:   39 | acc: 50.00%,  total acc: 69.22%   [EVAL] batch:   40 | acc: 87.50%,  total acc: 69.66%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 70.39%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 71.08%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 71.73%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 72.36%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 72.96%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 73.54%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 74.09%   [EVAL] batch:   48 | acc: 37.50%,  total acc: 73.34%   [EVAL] batch:   49 | acc: 62.50%,  total acc: 73.12%   [EVAL] batch:   50 | acc: 25.00%,  total acc: 72.18%   [EVAL] batch:   51 | acc: 18.75%,  total acc: 71.15%   [EVAL] batch:   52 | acc: 12.50%,  total acc: 70.05%   [EVAL] batch:   53 | acc: 12.50%,  total acc: 68.98%   [EVAL] batch:   54 | acc: 18.75%,  total acc: 68.07%   [EVAL] batch:   55 | acc: 25.00%,  total acc: 67.30%   [EVAL] batch:   56 | acc: 12.50%,  total acc: 66.34%   [EVAL] batch:   57 | acc: 25.00%,  total acc: 65.62%   [EVAL] batch:   58 | acc: 12.50%,  total acc: 64.72%   [EVAL] batch:   59 | acc: 25.00%,  total acc: 64.06%   [EVAL] batch:   60 | acc: 56.25%,  total acc: 63.93%   [EVAL] batch:   61 | acc: 25.00%,  total acc: 63.31%   [EVAL] batch:   62 | acc: 31.25%,  total acc: 62.80%   [EVAL] batch:   63 | acc: 43.75%,  total acc: 62.50%   [EVAL] batch:   64 | acc: 75.00%,  total acc: 62.69%   [EVAL] batch:   65 | acc: 93.75%,  total acc: 63.16%   [EVAL] batch:   66 | acc: 100.00%,  total acc: 63.71%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 64.25%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 64.76%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 65.27%   [EVAL] batch:   70 | acc: 68.75%,  total acc: 65.32%   [EVAL] batch:   71 | acc: 68.75%,  total acc: 65.36%   [EVAL] batch:   72 | acc: 75.00%,  total acc: 65.50%   [EVAL] batch:   73 | acc: 68.75%,  total acc: 65.54%   [EVAL] batch:   74 | acc: 43.75%,  total acc: 65.25%   [EVAL] batch:   75 | acc: 50.00%,  total acc: 65.05%   [EVAL] batch:   76 | acc: 37.50%,  total acc: 64.69%   [EVAL] batch:   77 | acc: 50.00%,  total acc: 64.50%   [EVAL] batch:   78 | acc: 37.50%,  total acc: 64.16%   [EVAL] batch:   79 | acc: 25.00%,  total acc: 63.67%   [EVAL] batch:   80 | acc: 43.75%,  total acc: 63.43%   [EVAL] batch:   81 | acc: 87.50%,  total acc: 63.72%   [EVAL] batch:   82 | acc: 87.50%,  total acc: 64.01%   [EVAL] batch:   83 | acc: 93.75%,  total acc: 64.36%   [EVAL] batch:   84 | acc: 81.25%,  total acc: 64.56%   [EVAL] batch:   85 | acc: 93.75%,  total acc: 64.90%   [EVAL] batch:   86 | acc: 93.75%,  total acc: 65.23%   [EVAL] batch:   87 | acc: 93.75%,  total acc: 65.55%   [EVAL] batch:   88 | acc: 87.50%,  total acc: 65.80%   [EVAL] batch:   89 | acc: 62.50%,  total acc: 65.76%   [EVAL] batch:   90 | acc: 87.50%,  total acc: 66.00%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 66.37%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 66.73%   [EVAL] batch:   93 | acc: 87.50%,  total acc: 66.95%   [EVAL] batch:   94 | acc: 75.00%,  total acc: 67.04%   [EVAL] batch:   95 | acc: 87.50%,  total acc: 67.25%   [EVAL] batch:   96 | acc: 25.00%,  total acc: 66.82%   
cur_acc:  ['0.8636', '0.3750', '0.9241', '0.8750', '0.6779', '0.8304']
his_acc:  ['0.8636', '0.7281', '0.7546', '0.7835', '0.6756', '0.6682']
CurrentTrain: epoch  0, batch     0 | loss: 8.0508232CurrentTrain: epoch  0, batch     1 | loss: 8.4077759CurrentTrain: epoch  1, batch     0 | loss: 7.4422355CurrentTrain: epoch  1, batch     1 | loss: 6.8598089CurrentTrain: epoch  2, batch     0 | loss: 7.0040283CurrentTrain: epoch  2, batch     1 | loss: 6.3586783CurrentTrain: epoch  3, batch     0 | loss: 6.6215038CurrentTrain: epoch  3, batch     1 | loss: 5.3536963CurrentTrain: epoch  4, batch     0 | loss: 6.3997393CurrentTrain: epoch  4, batch     1 | loss: 5.8125501CurrentTrain: epoch  5, batch     0 | loss: 5.8969202CurrentTrain: epoch  5, batch     1 | loss: 5.2358937CurrentTrain: epoch  6, batch     0 | loss: 5.5275788CurrentTrain: epoch  6, batch     1 | loss: 4.6571732CurrentTrain: epoch  7, batch     0 | loss: 4.6876507CurrentTrain: epoch  7, batch     1 | loss: 4.8865595CurrentTrain: epoch  8, batch     0 | loss: 4.7198668CurrentTrain: epoch  8, batch     1 | loss: 4.1816740CurrentTrain: epoch  9, batch     0 | loss: 4.2977009CurrentTrain: epoch  9, batch     1 | loss: 3.6477687
#### organization subsidiaries ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization subsidiaries" means an organization 's subsidiary is an organization.
Here is an example:
Relation: organization subsidiaries
Context: `` this newly opened center aims to be at the forefront of knowledge on the subjects of public finance , fiscal policy , privatization , and regional economic cooperation , '' said bindu n lohani , director general and chief compliance officer of the adb 's regional and sustainable development department .
Head Entity: adb
Tail Entity: regional and sustainable development department
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization subsidiaries":
Relation: organization subsidiaries  
Context: "The recent acquisition of XYZ Technologies by Global Innovations has allowed the latter to expand its portfolio and enhance its capabilities in the tech sector," stated the CEO of Global Innovations during the press conference.  
Head Entity: Global Innovations  
Tail Entity: XYZ Technologies  

Relation: organization subsidiaries  
Context: "As part of its growth strategy, MegaCorp has launched a new division focused on renewable energy solutions, which will operate under the name EcoPower," announced the company's spokesperson.  
Head Entity: MegaCorp  
Tail Entity: EcoPower  
#### organization parents ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization parents" means an organization 's parent is an organization.
Here is an example:
Relation: organization parents
Context: aseel kami , tim cocks , abdul-rahman taher , tim cocks and michael christie -lrb- reuters -rrb- add an intriguing note in the speculation over who 's responsible for the recent bombings : `` but a senior iraqi intelligence source , who declined to be named , said there was evidence the bombs could be the work of the militant badr organisation , the armed wing of the supreme islamic iraqi council -lrb- isci -rrb- .
Head Entity: badr organisation
Tail Entity: supreme islamic iraqi council
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization parents":
Relation: organization parents  
Context: In a recent merger, the tech giant Alphabet Inc. announced that it would acquire the innovative startup DeepMind Technologies, which has been a subsidiary of Alphabet since 2015. This acquisition is expected to enhance Alphabet's capabilities in artificial intelligence.  
Head Entity: DeepMind Technologies  
Tail Entity: Alphabet Inc.  

Relation: organization parents  
Context: The historical archives reveal that the renowned publishing house Penguin Random House was formed through the merger of two major companies, Penguin Group and Random House, which were both influential in the literary world.  
Head Entity: Penguin Random House  
Tail Entity: Penguin Group  
#### organization alternate names ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization alternate names" means a person 's alias is a person.
Here is an example:
Relation: organization alternate names
Context: it also needs the green light from the 45-nation nuclear suppliers group -lrb- nsg -rrb- , which regulates global civilian nuclear trade , before it can begin buying nuclear reactors and fuel .
Head Entity: nsg
Tail Entity: nuclear suppliers group
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization alternate names":
Relation: organization alternate names  
Context: The International Monetary Fund, often referred to as the IMF, plays a crucial role in global economic stability.  
Head Entity: IMF  
Tail Entity: International Monetary Fund  

Relation: organization alternate names  
Context: The World Health Organization, commonly known as WHO, has been at the forefront of the global response to health crises.  
Head Entity: WHO  
Tail Entity: World Health Organization  
#### organization city of headquarters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization city of headquarters" means an organization is located in a city.
Here is an example:
Relation: organization city of headquarters
Context: sts-127 space shuttle endeavour launches from launch pad 39a at kennedy space center july 15 , in cape canaveral , florida .
Head Entity: kennedy space center
Tail Entity: cape canaveral
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization city of headquarters":
Relation: organization city of headquarters  
Context: the headquarters of the tech giant apple inc. is located in cupertino, california, where it has been since 1993.  
Head Entity: apple inc.  
Tail Entity: cupertino  

Relation: organization city of headquarters  
Context: the united nations has its main headquarters in new york city, which serves as a hub for international diplomacy.  
Head Entity: united nations  
Tail Entity: new york city  
#### person siblings ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person siblings" means a person 's sibling is a person.
Here is an example:
Relation: person siblings
Context: more than three decades ago , kerry 's work against the vietnam war set him on course to the senate - and , he often hoped , on to the presidency .
Head Entity: kerry
Tail Entity: he
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person siblings":
Relation: person siblings  
Context: During the family reunion, Sarah introduced her brother, Tom, to her friends, highlighting their close bond and shared childhood memories.  
Head Entity: Sarah  
Tail Entity: Tom  

Relation: person siblings  
Context: After the game, Emily celebrated her victory with her sister, who had always been her biggest supporter and cheerleader.  
Head Entity: Emily  
Tail Entity: her sister  
Mixup data size:  8545
MixupTrain:  epoch  0, batch     0 | loss: 3.4889429MixupTrain:  epoch  0, batch     1 | loss: 3.6967325MixupTrain:  epoch  0, batch     2 | loss: 3.8472614MixupTrain:  epoch  0, batch     3 | loss: 3.7214553MixupTrain:  epoch  0, batch     4 | loss: 4.0998383MixupTrain:  epoch  0, batch     5 | loss: 4.0821257MixupTrain:  epoch  0, batch     6 | loss: 4.0217400MixupTrain:  epoch  0, batch     7 | loss: 4.2351084MixupTrain:  epoch  0, batch     8 | loss: 3.9336631MixupTrain:  epoch  0, batch     9 | loss: 4.0859170MixupTrain:  epoch  0, batch    10 | loss: 4.0805526MixupTrain:  epoch  0, batch    11 | loss: 4.4593043MixupTrain:  epoch  0, batch    12 | loss: 4.0343981MixupTrain:  epoch  0, batch    13 | loss: 3.4438272MixupTrain:  epoch  0, batch    14 | loss: 4.1314235MixupTrain:  epoch  0, batch    15 | loss: 4.0579967MixupTrain:  epoch  0, batch    16 | loss: 3.3845325MixupTrain:  epoch  0, batch    17 | loss: 4.4343858MixupTrain:  epoch  0, batch    18 | loss: 3.7000890MixupTrain:  epoch  0, batch    19 | loss: 3.6493564MixupTrain:  epoch  0, batch    20 | loss: 3.4411616MixupTrain:  epoch  0, batch    21 | loss: 3.3100255MixupTrain:  epoch  0, batch    22 | loss: 3.3040338MixupTrain:  epoch  0, batch    23 | loss: 3.6741521MixupTrain:  epoch  0, batch    24 | loss: 3.6105959MixupTrain:  epoch  0, batch    25 | loss: 3.8549433MixupTrain:  epoch  0, batch    26 | loss: 3.6222205MixupTrain:  epoch  0, batch    27 | loss: 3.6459045MixupTrain:  epoch  0, batch    28 | loss: 3.7793584MixupTrain:  epoch  0, batch    29 | loss: 3.2518950MixupTrain:  epoch  0, batch    30 | loss: 3.8938236MixupTrain:  epoch  0, batch    31 | loss: 3.9367807MixupTrain:  epoch  0, batch    32 | loss: 3.6364362MixupTrain:  epoch  0, batch    33 | loss: 3.5507631MixupTrain:  epoch  0, batch    34 | loss: 3.6831479MixupTrain:  epoch  0, batch    35 | loss: 3.9021840MixupTrain:  epoch  0, batch    36 | loss: 3.7602327MixupTrain:  epoch  0, batch    37 | loss: 2.9313540MixupTrain:  epoch  0, batch    38 | loss: 3.5067039MixupTrain:  epoch  0, batch    39 | loss: 3.7510381MixupTrain:  epoch  0, batch    40 | loss: 3.4335046MixupTrain:  epoch  0, batch    41 | loss: 3.4966235MixupTrain:  epoch  0, batch    42 | loss: 3.6399817MixupTrain:  epoch  0, batch    43 | loss: 3.6205821MixupTrain:  epoch  0, batch    44 | loss: 3.7252111MixupTrain:  epoch  0, batch    45 | loss: 3.8610516MixupTrain:  epoch  0, batch    46 | loss: 3.5694382MixupTrain:  epoch  0, batch    47 | loss: 3.6228061MixupTrain:  epoch  0, batch    48 | loss: 3.8598862MixupTrain:  epoch  0, batch    49 | loss: 3.8026404MixupTrain:  epoch  0, batch    50 | loss: 3.5104592MixupTrain:  epoch  0, batch    51 | loss: 3.4908977MixupTrain:  epoch  0, batch    52 | loss: 3.6474154MixupTrain:  epoch  0, batch    53 | loss: 3.7215543MixupTrain:  epoch  0, batch    54 | loss: 3.5041375MixupTrain:  epoch  0, batch    55 | loss: 3.5424829MixupTrain:  epoch  0, batch    56 | loss: 3.3527188MixupTrain:  epoch  0, batch    57 | loss: 3.6072736MixupTrain:  epoch  0, batch    58 | loss: 3.5079093MixupTrain:  epoch  0, batch    59 | loss: 3.1923954MixupTrain:  epoch  0, batch    60 | loss: 3.3260322MixupTrain:  epoch  0, batch    61 | loss: 3.6669788MixupTrain:  epoch  0, batch    62 | loss: 3.2556324MixupTrain:  epoch  0, batch    63 | loss: 3.3828843MixupTrain:  epoch  0, batch    64 | loss: 3.2681994MixupTrain:  epoch  0, batch    65 | loss: 3.2976429MixupTrain:  epoch  0, batch    66 | loss: 3.4747620MixupTrain:  epoch  0, batch    67 | loss: 3.4570308MixupTrain:  epoch  0, batch    68 | loss: 3.5429897MixupTrain:  epoch  0, batch    69 | loss: 2.9934664MixupTrain:  epoch  0, batch    70 | loss: 3.3331270MixupTrain:  epoch  0, batch    71 | loss: 3.6250415MixupTrain:  epoch  0, batch    72 | loss: 3.3388062MixupTrain:  epoch  0, batch    73 | loss: 3.2698848MixupTrain:  epoch  0, batch    74 | loss: 2.9567947MixupTrain:  epoch  0, batch    75 | loss: 3.3576732MixupTrain:  epoch  0, batch    76 | loss: 3.1556768MixupTrain:  epoch  0, batch    77 | loss: 3.3456385MixupTrain:  epoch  0, batch    78 | loss: 3.1898351MixupTrain:  epoch  0, batch    79 | loss: 3.2213502MixupTrain:  epoch  0, batch    80 | loss: 3.2495227MixupTrain:  epoch  0, batch    81 | loss: 3.3218312MixupTrain:  epoch  0, batch    82 | loss: 3.1244476MixupTrain:  epoch  0, batch    83 | loss: 3.2558722MixupTrain:  epoch  0, batch    84 | loss: 3.3226724MixupTrain:  epoch  0, batch    85 | loss: 3.4127839MixupTrain:  epoch  0, batch    86 | loss: 3.5846460MixupTrain:  epoch  0, batch    87 | loss: 3.4927127MixupTrain:  epoch  0, batch    88 | loss: 3.3923631MixupTrain:  epoch  0, batch    89 | loss: 3.6022177MixupTrain:  epoch  0, batch    90 | loss: 3.4015000MixupTrain:  epoch  0, batch    91 | loss: 3.5640047MixupTrain:  epoch  0, batch    92 | loss: 3.3038383MixupTrain:  epoch  0, batch    93 | loss: 3.5678873MixupTrain:  epoch  0, batch    94 | loss: 3.4058318MixupTrain:  epoch  0, batch    95 | loss: 3.1464746MixupTrain:  epoch  0, batch    96 | loss: 3.1860714MixupTrain:  epoch  0, batch    97 | loss: 3.0626793MixupTrain:  epoch  0, batch    98 | loss: 3.5570664MixupTrain:  epoch  0, batch    99 | loss: 3.2509422MixupTrain:  epoch  0, batch   100 | loss: 3.1431925MixupTrain:  epoch  0, batch   101 | loss: 3.5733762MixupTrain:  epoch  0, batch   102 | loss: 3.4116023MixupTrain:  epoch  0, batch   103 | loss: 3.1817489MixupTrain:  epoch  0, batch   104 | loss: 3.5406344MixupTrain:  epoch  0, batch   105 | loss: 3.3343096MixupTrain:  epoch  0, batch   106 | loss: 3.4775081MixupTrain:  epoch  0, batch   107 | loss: 3.5437667MixupTrain:  epoch  0, batch   108 | loss: 3.3564038MixupTrain:  epoch  0, batch   109 | loss: 3.1291137MixupTrain:  epoch  0, batch   110 | loss: 3.3455842MixupTrain:  epoch  0, batch   111 | loss: 3.1168900MixupTrain:  epoch  0, batch   112 | loss: 3.3433847MixupTrain:  epoch  0, batch   113 | loss: 3.0386901MixupTrain:  epoch  0, batch   114 | loss: 3.1267180MixupTrain:  epoch  0, batch   115 | loss: 3.1281810MixupTrain:  epoch  0, batch   116 | loss: 3.4816935MixupTrain:  epoch  0, batch   117 | loss: 2.9887991MixupTrain:  epoch  0, batch   118 | loss: 3.8091745MixupTrain:  epoch  0, batch   119 | loss: 3.3666334MixupTrain:  epoch  0, batch   120 | loss: 2.9663305MixupTrain:  epoch  0, batch   121 | loss: 3.4084494MixupTrain:  epoch  0, batch   122 | loss: 3.2782025MixupTrain:  epoch  0, batch   123 | loss: 3.5295398MixupTrain:  epoch  0, batch   124 | loss: 3.6841121MixupTrain:  epoch  0, batch   125 | loss: 3.1528831MixupTrain:  epoch  0, batch   126 | loss: 3.3996403MixupTrain:  epoch  0, batch   127 | loss: 3.3187144MixupTrain:  epoch  0, batch   128 | loss: 3.1419842MixupTrain:  epoch  0, batch   129 | loss: 3.0073700MixupTrain:  epoch  0, batch   130 | loss: 3.1721163MixupTrain:  epoch  0, batch   131 | loss: 2.9377117MixupTrain:  epoch  0, batch   132 | loss: 3.2362852MixupTrain:  epoch  0, batch   133 | loss: 3.2258754MixupTrain:  epoch  0, batch   134 | loss: 3.1641555MixupTrain:  epoch  0, batch   135 | loss: 3.2342029MixupTrain:  epoch  0, batch   136 | loss: 3.3859866MixupTrain:  epoch  0, batch   137 | loss: 3.3125169MixupTrain:  epoch  0, batch   138 | loss: 3.1202826MixupTrain:  epoch  0, batch   139 | loss: 3.2324080MixupTrain:  epoch  0, batch   140 | loss: 3.3174300MixupTrain:  epoch  0, batch   141 | loss: 3.2976446MixupTrain:  epoch  0, batch   142 | loss: 3.2586188MixupTrain:  epoch  0, batch   143 | loss: 3.1603808MixupTrain:  epoch  0, batch   144 | loss: 3.3241844MixupTrain:  epoch  0, batch   145 | loss: 3.1675279MixupTrain:  epoch  0, batch   146 | loss: 3.4400156MixupTrain:  epoch  0, batch   147 | loss: 3.1301370MixupTrain:  epoch  0, batch   148 | loss: 3.2257531MixupTrain:  epoch  0, batch   149 | loss: 3.3304377MixupTrain:  epoch  0, batch   150 | loss: 3.0455656MixupTrain:  epoch  0, batch   151 | loss: 3.0622511MixupTrain:  epoch  0, batch   152 | loss: 2.8366208MixupTrain:  epoch  0, batch   153 | loss: 3.1034894MixupTrain:  epoch  0, batch   154 | loss: 3.2249713MixupTrain:  epoch  0, batch   155 | loss: 3.0286322MixupTrain:  epoch  0, batch   156 | loss: 3.0398767MixupTrain:  epoch  0, batch   157 | loss: 3.0632615MixupTrain:  epoch  0, batch   158 | loss: 3.4141183MixupTrain:  epoch  0, batch   159 | loss: 3.4883938MixupTrain:  epoch  0, batch   160 | loss: 3.1194341MixupTrain:  epoch  0, batch   161 | loss: 3.0757995MixupTrain:  epoch  0, batch   162 | loss: 3.1256361MixupTrain:  epoch  0, batch   163 | loss: 3.1368005MixupTrain:  epoch  0, batch   164 | loss: 3.0629792MixupTrain:  epoch  0, batch   165 | loss: 3.3115132MixupTrain:  epoch  0, batch   166 | loss: 3.1045065MixupTrain:  epoch  0, batch   167 | loss: 3.2241445MixupTrain:  epoch  0, batch   168 | loss: 3.2289541MixupTrain:  epoch  0, batch   169 | loss: 3.3816929MixupTrain:  epoch  0, batch   170 | loss: 3.3871205MixupTrain:  epoch  0, batch   171 | loss: 3.1627216MixupTrain:  epoch  0, batch   172 | loss: 3.0351262MixupTrain:  epoch  0, batch   173 | loss: 3.0961759MixupTrain:  epoch  0, batch   174 | loss: 3.1341486MixupTrain:  epoch  0, batch   175 | loss: 3.2578480MixupTrain:  epoch  0, batch   176 | loss: 3.0563648MixupTrain:  epoch  0, batch   177 | loss: 3.4387026MixupTrain:  epoch  0, batch   178 | loss: 3.2948532MixupTrain:  epoch  0, batch   179 | loss: 3.2985756MixupTrain:  epoch  0, batch   180 | loss: 3.0323510MixupTrain:  epoch  0, batch   181 | loss: 3.2646492MixupTrain:  epoch  0, batch   182 | loss: 3.2683434MixupTrain:  epoch  0, batch   183 | loss: 3.0517631MixupTrain:  epoch  0, batch   184 | loss: 3.5420988MixupTrain:  epoch  0, batch   185 | loss: 3.1089001MixupTrain:  epoch  0, batch   186 | loss: 3.1509762MixupTrain:  epoch  0, batch   187 | loss: 3.1128626MixupTrain:  epoch  0, batch   188 | loss: 3.2285569MixupTrain:  epoch  0, batch   189 | loss: 3.2887604MixupTrain:  epoch  0, batch   190 | loss: 3.0124092MixupTrain:  epoch  0, batch   191 | loss: 3.0091906MixupTrain:  epoch  0, batch   192 | loss: 3.1681342MixupTrain:  epoch  0, batch   193 | loss: 3.2527375MixupTrain:  epoch  0, batch   194 | loss: 3.1134419MixupTrain:  epoch  0, batch   195 | loss: 3.1935375MixupTrain:  epoch  0, batch   196 | loss: 2.9492428MixupTrain:  epoch  0, batch   197 | loss: 3.0546677MixupTrain:  epoch  0, batch   198 | loss: 3.2537603MixupTrain:  epoch  0, batch   199 | loss: 3.2157059MixupTrain:  epoch  0, batch   200 | loss: 3.0306363MixupTrain:  epoch  0, batch   201 | loss: 2.8502727MixupTrain:  epoch  0, batch   202 | loss: 3.0537729MixupTrain:  epoch  0, batch   203 | loss: 3.0980258MixupTrain:  epoch  0, batch   204 | loss: 3.1665478MixupTrain:  epoch  0, batch   205 | loss: 3.1951079MixupTrain:  epoch  0, batch   206 | loss: 2.9995835MixupTrain:  epoch  0, batch   207 | loss: 3.1972494MixupTrain:  epoch  0, batch   208 | loss: 3.4477453MixupTrain:  epoch  0, batch   209 | loss: 3.0368881MixupTrain:  epoch  0, batch   210 | loss: 3.1110144MixupTrain:  epoch  0, batch   211 | loss: 3.1326599MixupTrain:  epoch  0, batch   212 | loss: 2.9653287MixupTrain:  epoch  0, batch   213 | loss: 2.9951127MixupTrain:  epoch  0, batch   214 | loss: 3.3373289MixupTrain:  epoch  0, batch   215 | loss: 2.9587440MixupTrain:  epoch  0, batch   216 | loss: 3.2248192MixupTrain:  epoch  0, batch   217 | loss: 3.1558979MixupTrain:  epoch  0, batch   218 | loss: 2.9933529MixupTrain:  epoch  0, batch   219 | loss: 3.5942168MixupTrain:  epoch  0, batch   220 | loss: 3.0074215MixupTrain:  epoch  0, batch   221 | loss: 2.8720174MixupTrain:  epoch  0, batch   222 | loss: 3.1828628MixupTrain:  epoch  0, batch   223 | loss: 3.1465974MixupTrain:  epoch  0, batch   224 | loss: 3.1304164MixupTrain:  epoch  0, batch   225 | loss: 3.2554557MixupTrain:  epoch  0, batch   226 | loss: 3.0695715MixupTrain:  epoch  0, batch   227 | loss: 3.1279299MixupTrain:  epoch  0, batch   228 | loss: 3.1557770MixupTrain:  epoch  0, batch   229 | loss: 3.0862978MixupTrain:  epoch  0, batch   230 | loss: 3.0329289MixupTrain:  epoch  0, batch   231 | loss: 3.4820457MixupTrain:  epoch  0, batch   232 | loss: 3.2461071MixupTrain:  epoch  0, batch   233 | loss: 3.2908964MixupTrain:  epoch  0, batch   234 | loss: 3.2357793MixupTrain:  epoch  0, batch   235 | loss: 2.9059889MixupTrain:  epoch  0, batch   236 | loss: 3.1756144MixupTrain:  epoch  0, batch   237 | loss: 3.3798957MixupTrain:  epoch  0, batch   238 | loss: 2.7934985MixupTrain:  epoch  0, batch   239 | loss: 3.0587683MixupTrain:  epoch  0, batch   240 | loss: 3.1409025MixupTrain:  epoch  0, batch   241 | loss: 2.9211533MixupTrain:  epoch  0, batch   242 | loss: 3.1814976MixupTrain:  epoch  0, batch   243 | loss: 3.0190272MixupTrain:  epoch  0, batch   244 | loss: 3.2852769MixupTrain:  epoch  0, batch   245 | loss: 3.1818383MixupTrain:  epoch  0, batch   246 | loss: 3.0857675MixupTrain:  epoch  0, batch   247 | loss: 2.9306421MixupTrain:  epoch  0, batch   248 | loss: 2.9570947MixupTrain:  epoch  0, batch   249 | loss: 3.1450174MixupTrain:  epoch  0, batch   250 | loss: 3.1590734MixupTrain:  epoch  0, batch   251 | loss: 3.4246345MixupTrain:  epoch  0, batch   252 | loss: 3.2354889MixupTrain:  epoch  0, batch   253 | loss: 2.9505570MixupTrain:  epoch  0, batch   254 | loss: 2.9677377MixupTrain:  epoch  0, batch   255 | loss: 3.2596436MixupTrain:  epoch  0, batch   256 | loss: 3.1485496MixupTrain:  epoch  0, batch   257 | loss: 2.9543486MixupTrain:  epoch  0, batch   258 | loss: 3.1066403MixupTrain:  epoch  0, batch   259 | loss: 2.9028876MixupTrain:  epoch  0, batch   260 | loss: 3.0775642MixupTrain:  epoch  0, batch   261 | loss: 3.2413976MixupTrain:  epoch  0, batch   262 | loss: 2.9547458MixupTrain:  epoch  0, batch   263 | loss: 3.1425524MixupTrain:  epoch  0, batch   264 | loss: 2.9478345MixupTrain:  epoch  0, batch   265 | loss: 3.2710555MixupTrain:  epoch  0, batch   266 | loss: 3.0532176MixupTrain:  epoch  0, batch   267 | loss: 2.9427543MixupTrain:  epoch  0, batch   268 | loss: 2.9860501MixupTrain:  epoch  0, batch   269 | loss: 2.9179289MixupTrain:  epoch  0, batch   270 | loss: 3.1436050MixupTrain:  epoch  0, batch   271 | loss: 3.1487865MixupTrain:  epoch  0, batch   272 | loss: 3.0980413MixupTrain:  epoch  0, batch   273 | loss: 2.9259570MixupTrain:  epoch  0, batch   274 | loss: 3.0372894MixupTrain:  epoch  0, batch   275 | loss: 3.1104035MixupTrain:  epoch  0, batch   276 | loss: 3.0312998MixupTrain:  epoch  0, batch   277 | loss: 2.9949660MixupTrain:  epoch  0, batch   278 | loss: 2.8747072MixupTrain:  epoch  0, batch   279 | loss: 2.9607697MixupTrain:  epoch  0, batch   280 | loss: 3.2987862MixupTrain:  epoch  0, batch   281 | loss: 3.1253123MixupTrain:  epoch  0, batch   282 | loss: 3.0898600MixupTrain:  epoch  0, batch   283 | loss: 3.3511224MixupTrain:  epoch  0, batch   284 | loss: 3.1901445MixupTrain:  epoch  0, batch   285 | loss: 3.0552974MixupTrain:  epoch  0, batch   286 | loss: 3.0061426MixupTrain:  epoch  0, batch   287 | loss: 3.1140981MixupTrain:  epoch  0, batch   288 | loss: 2.9478717MixupTrain:  epoch  0, batch   289 | loss: 3.1146984MixupTrain:  epoch  0, batch   290 | loss: 3.1704447MixupTrain:  epoch  0, batch   291 | loss: 3.0915475MixupTrain:  epoch  0, batch   292 | loss: 3.1989946MixupTrain:  epoch  0, batch   293 | loss: 3.2380171MixupTrain:  epoch  0, batch   294 | loss: 2.9718988MixupTrain:  epoch  0, batch   295 | loss: 2.9549894MixupTrain:  epoch  0, batch   296 | loss: 3.0511410MixupTrain:  epoch  0, batch   297 | loss: 2.9844666MixupTrain:  epoch  0, batch   298 | loss: 3.0734992MixupTrain:  epoch  0, batch   299 | loss: 3.0257058MixupTrain:  epoch  0, batch   300 | loss: 2.9567032MixupTrain:  epoch  0, batch   301 | loss: 2.9667118MixupTrain:  epoch  0, batch   302 | loss: 2.9221668MixupTrain:  epoch  0, batch   303 | loss: 3.0554433MixupTrain:  epoch  0, batch   304 | loss: 3.2276211MixupTrain:  epoch  0, batch   305 | loss: 3.0029058MixupTrain:  epoch  0, batch   306 | loss: 3.1143999MixupTrain:  epoch  0, batch   307 | loss: 2.9496398MixupTrain:  epoch  0, batch   308 | loss: 3.1466715MixupTrain:  epoch  0, batch   309 | loss: 3.1779122MixupTrain:  epoch  0, batch   310 | loss: 3.1241317MixupTrain:  epoch  0, batch   311 | loss: 2.9590573MixupTrain:  epoch  0, batch   312 | loss: 2.9708481MixupTrain:  epoch  0, batch   313 | loss: 3.0852776MixupTrain:  epoch  0, batch   314 | loss: 3.1976602MixupTrain:  epoch  0, batch   315 | loss: 3.1290898MixupTrain:  epoch  0, batch   316 | loss: 2.9880140MixupTrain:  epoch  0, batch   317 | loss: 3.1332254MixupTrain:  epoch  0, batch   318 | loss: 3.1418955MixupTrain:  epoch  0, batch   319 | loss: 3.0816567MixupTrain:  epoch  0, batch   320 | loss: 3.1065927MixupTrain:  epoch  0, batch   321 | loss: 3.0981412MixupTrain:  epoch  0, batch   322 | loss: 3.1742008MixupTrain:  epoch  0, batch   323 | loss: 3.0977597MixupTrain:  epoch  0, batch   324 | loss: 3.0082340MixupTrain:  epoch  0, batch   325 | loss: 2.9186072MixupTrain:  epoch  0, batch   326 | loss: 3.2075834MixupTrain:  epoch  0, batch   327 | loss: 2.8793268MixupTrain:  epoch  0, batch   328 | loss: 2.8796411MixupTrain:  epoch  0, batch   329 | loss: 2.9443729MixupTrain:  epoch  0, batch   330 | loss: 2.9955163MixupTrain:  epoch  0, batch   331 | loss: 3.1561890MixupTrain:  epoch  0, batch   332 | loss: 2.9468155MixupTrain:  epoch  0, batch   333 | loss: 2.9455330MixupTrain:  epoch  0, batch   334 | loss: 2.9988923MixupTrain:  epoch  0, batch   335 | loss: 2.9869967MixupTrain:  epoch  0, batch   336 | loss: 3.1277771MixupTrain:  epoch  0, batch   337 | loss: 3.0039163MixupTrain:  epoch  0, batch   338 | loss: 3.2488973MixupTrain:  epoch  0, batch   339 | loss: 3.0330713MixupTrain:  epoch  0, batch   340 | loss: 3.1704712MixupTrain:  epoch  0, batch   341 | loss: 3.0318959MixupTrain:  epoch  0, batch   342 | loss: 3.5123343MixupTrain:  epoch  0, batch   343 | loss: 3.1045086MixupTrain:  epoch  0, batch   344 | loss: 2.8867671MixupTrain:  epoch  0, batch   345 | loss: 3.2154534MixupTrain:  epoch  0, batch   346 | loss: 3.2146950MixupTrain:  epoch  0, batch   347 | loss: 3.0105603MixupTrain:  epoch  0, batch   348 | loss: 3.1521935MixupTrain:  epoch  0, batch   349 | loss: 3.0236833MixupTrain:  epoch  0, batch   350 | loss: 3.0877304MixupTrain:  epoch  0, batch   351 | loss: 3.2715185MixupTrain:  epoch  0, batch   352 | loss: 3.0224299MixupTrain:  epoch  0, batch   353 | loss: 2.9265327MixupTrain:  epoch  0, batch   354 | loss: 3.2639458MixupTrain:  epoch  0, batch   355 | loss: 2.9691086MixupTrain:  epoch  0, batch   356 | loss: 2.9667249MixupTrain:  epoch  0, batch   357 | loss: 3.0411932MixupTrain:  epoch  0, batch   358 | loss: 2.9642673MixupTrain:  epoch  0, batch   359 | loss: 2.8824508MixupTrain:  epoch  0, batch   360 | loss: 2.9930153MixupTrain:  epoch  0, batch   361 | loss: 3.1252241MixupTrain:  epoch  0, batch   362 | loss: 2.9488485MixupTrain:  epoch  0, batch   363 | loss: 2.9342217MixupTrain:  epoch  0, batch   364 | loss: 2.8054621MixupTrain:  epoch  0, batch   365 | loss: 2.9559250MixupTrain:  epoch  0, batch   366 | loss: 3.1383736MixupTrain:  epoch  0, batch   367 | loss: 2.9891202MixupTrain:  epoch  0, batch   368 | loss: 2.8679667MixupTrain:  epoch  0, batch   369 | loss: 3.0430532MixupTrain:  epoch  0, batch   370 | loss: 2.9884868MixupTrain:  epoch  0, batch   371 | loss: 3.2201896MixupTrain:  epoch  0, batch   372 | loss: 3.0475574MixupTrain:  epoch  0, batch   373 | loss: 3.0658791MixupTrain:  epoch  0, batch   374 | loss: 3.1258862MixupTrain:  epoch  0, batch   375 | loss: 3.1309867MixupTrain:  epoch  0, batch   376 | loss: 2.9133193MixupTrain:  epoch  0, batch   377 | loss: 2.7645741MixupTrain:  epoch  0, batch   378 | loss: 3.0274920MixupTrain:  epoch  0, batch   379 | loss: 2.9908137MixupTrain:  epoch  0, batch   380 | loss: 3.0013506MixupTrain:  epoch  0, batch   381 | loss: 2.9817960MixupTrain:  epoch  0, batch   382 | loss: 3.1609330MixupTrain:  epoch  0, batch   383 | loss: 2.8592429MixupTrain:  epoch  0, batch   384 | loss: 3.1172025MixupTrain:  epoch  0, batch   385 | loss: 3.0144455MixupTrain:  epoch  0, batch   386 | loss: 3.0718808MixupTrain:  epoch  0, batch   387 | loss: 3.1540139MixupTrain:  epoch  0, batch   388 | loss: 3.0366733MixupTrain:  epoch  0, batch   389 | loss: 3.0645843MixupTrain:  epoch  0, batch   390 | loss: 2.8396297MixupTrain:  epoch  0, batch   391 | loss: 2.9087758MixupTrain:  epoch  0, batch   392 | loss: 3.2547274MixupTrain:  epoch  0, batch   393 | loss: 3.2256098MixupTrain:  epoch  0, batch   394 | loss: 3.0543613MixupTrain:  epoch  0, batch   395 | loss: 2.8385608MixupTrain:  epoch  0, batch   396 | loss: 2.9915037MixupTrain:  epoch  0, batch   397 | loss: 3.0956686MixupTrain:  epoch  0, batch   398 | loss: 3.2711334MixupTrain:  epoch  0, batch   399 | loss: 3.1462994MixupTrain:  epoch  0, batch   400 | loss: 2.8657165MixupTrain:  epoch  0, batch   401 | loss: 3.0115194MixupTrain:  epoch  0, batch   402 | loss: 3.1180081MixupTrain:  epoch  0, batch   403 | loss: 3.0266855MixupTrain:  epoch  0, batch   404 | loss: 3.0305510MixupTrain:  epoch  0, batch   405 | loss: 2.9774771MixupTrain:  epoch  0, batch   406 | loss: 2.8010962MixupTrain:  epoch  0, batch   407 | loss: 2.9653683MixupTrain:  epoch  0, batch   408 | loss: 2.8249273MixupTrain:  epoch  0, batch   409 | loss: 2.9779997MixupTrain:  epoch  0, batch   410 | loss: 3.2084465MixupTrain:  epoch  0, batch   411 | loss: 2.9897332MixupTrain:  epoch  0, batch   412 | loss: 2.9381690MixupTrain:  epoch  0, batch   413 | loss: 2.8684692MixupTrain:  epoch  0, batch   414 | loss: 3.0206437MixupTrain:  epoch  0, batch   415 | loss: 2.9896665MixupTrain:  epoch  0, batch   416 | loss: 3.0987730MixupTrain:  epoch  0, batch   417 | loss: 3.0173388MixupTrain:  epoch  0, batch   418 | loss: 3.1778736MixupTrain:  epoch  0, batch   419 | loss: 2.8691955MixupTrain:  epoch  0, batch   420 | loss: 3.0840383MixupTrain:  epoch  0, batch   421 | loss: 3.0812285MixupTrain:  epoch  0, batch   422 | loss: 2.9955893MixupTrain:  epoch  0, batch   423 | loss: 2.7587826MixupTrain:  epoch  0, batch   424 | loss: 3.0445254MixupTrain:  epoch  0, batch   425 | loss: 3.1944270MixupTrain:  epoch  0, batch   426 | loss: 2.8729343MixupTrain:  epoch  0, batch   427 | loss: 3.0157578MixupTrain:  epoch  0, batch   428 | loss: 2.8652315MixupTrain:  epoch  0, batch   429 | loss: 3.1019270MixupTrain:  epoch  0, batch   430 | loss: 3.1116848MixupTrain:  epoch  0, batch   431 | loss: 2.8936090MixupTrain:  epoch  0, batch   432 | loss: 3.0198069MixupTrain:  epoch  0, batch   433 | loss: 2.8296001MixupTrain:  epoch  0, batch   434 | loss: 3.0933833MixupTrain:  epoch  0, batch   435 | loss: 2.8162103MixupTrain:  epoch  0, batch   436 | loss: 2.9590409MixupTrain:  epoch  0, batch   437 | loss: 2.8956518MixupTrain:  epoch  0, batch   438 | loss: 2.8456919MixupTrain:  epoch  0, batch   439 | loss: 2.9872923MixupTrain:  epoch  0, batch   440 | loss: 3.0253782MixupTrain:  epoch  0, batch   441 | loss: 3.2190475MixupTrain:  epoch  0, batch   442 | loss: 3.0149088MixupTrain:  epoch  0, batch   443 | loss: 2.9728904MixupTrain:  epoch  0, batch   444 | loss: 3.0235302MixupTrain:  epoch  0, batch   445 | loss: 2.9683785MixupTrain:  epoch  0, batch   446 | loss: 2.9288499MixupTrain:  epoch  0, batch   447 | loss: 3.0000048MixupTrain:  epoch  0, batch   448 | loss: 3.0276594MixupTrain:  epoch  0, batch   449 | loss: 2.9397898MixupTrain:  epoch  0, batch   450 | loss: 3.0637584MixupTrain:  epoch  0, batch   451 | loss: 2.8407123MixupTrain:  epoch  0, batch   452 | loss: 3.0638168MixupTrain:  epoch  0, batch   453 | loss: 2.8521526MixupTrain:  epoch  0, batch   454 | loss: 3.0485559MixupTrain:  epoch  0, batch   455 | loss: 3.1031537MixupTrain:  epoch  0, batch   456 | loss: 2.9906554MixupTrain:  epoch  0, batch   457 | loss: 3.0679808MixupTrain:  epoch  0, batch   458 | loss: 2.9315343MixupTrain:  epoch  0, batch   459 | loss: 2.9711337MixupTrain:  epoch  0, batch   460 | loss: 2.9792471MixupTrain:  epoch  0, batch   461 | loss: 3.1546431MixupTrain:  epoch  0, batch   462 | loss: 2.9761596MixupTrain:  epoch  0, batch   463 | loss: 3.1480267MixupTrain:  epoch  0, batch   464 | loss: 2.8035994MixupTrain:  epoch  0, batch   465 | loss: 3.0547214MixupTrain:  epoch  0, batch   466 | loss: 3.1934838MixupTrain:  epoch  0, batch   467 | loss: 3.0089107MixupTrain:  epoch  0, batch   468 | loss: 2.9483666MixupTrain:  epoch  0, batch   469 | loss: 3.0392089MixupTrain:  epoch  0, batch   470 | loss: 2.8666568MixupTrain:  epoch  0, batch   471 | loss: 2.9485707MixupTrain:  epoch  0, batch   472 | loss: 2.9328203MixupTrain:  epoch  0, batch   473 | loss: 3.0096893MixupTrain:  epoch  0, batch   474 | loss: 3.0848944MixupTrain:  epoch  0, batch   475 | loss: 2.9834635MixupTrain:  epoch  0, batch   476 | loss: 3.2076983MixupTrain:  epoch  0, batch   477 | loss: 3.0177739MixupTrain:  epoch  0, batch   478 | loss: 3.0894494MixupTrain:  epoch  0, batch   479 | loss: 3.1479721MixupTrain:  epoch  0, batch   480 | loss: 2.8740611MixupTrain:  epoch  0, batch   481 | loss: 2.9680619MixupTrain:  epoch  0, batch   482 | loss: 2.9838142MixupTrain:  epoch  0, batch   483 | loss: 3.0348625MixupTrain:  epoch  0, batch   484 | loss: 3.0211563MixupTrain:  epoch  0, batch   485 | loss: 2.9640222MixupTrain:  epoch  0, batch   486 | loss: 3.2013369MixupTrain:  epoch  0, batch   487 | loss: 2.8468900MixupTrain:  epoch  0, batch   488 | loss: 2.8597913MixupTrain:  epoch  0, batch   489 | loss: 3.0421143MixupTrain:  epoch  0, batch   490 | loss: 2.9353046MixupTrain:  epoch  0, batch   491 | loss: 3.0074744MixupTrain:  epoch  0, batch   492 | loss: 2.9428718MixupTrain:  epoch  0, batch   493 | loss: 2.9825995MixupTrain:  epoch  0, batch   494 | loss: 3.0267668MixupTrain:  epoch  0, batch   495 | loss: 3.1141338MixupTrain:  epoch  0, batch   496 | loss: 2.9470878MixupTrain:  epoch  0, batch   497 | loss: 3.0885139MixupTrain:  epoch  0, batch   498 | loss: 2.8325148MixupTrain:  epoch  0, batch   499 | loss: 3.0913200MixupTrain:  epoch  0, batch   500 | loss: 2.9021978MixupTrain:  epoch  0, batch   501 | loss: 3.0628047MixupTrain:  epoch  0, batch   502 | loss: 3.0954595MixupTrain:  epoch  0, batch   503 | loss: 2.8448310MixupTrain:  epoch  0, batch   504 | loss: 2.8453631MixupTrain:  epoch  0, batch   505 | loss: 2.9297974MixupTrain:  epoch  0, batch   506 | loss: 2.8758037MixupTrain:  epoch  0, batch   507 | loss: 2.8241441MixupTrain:  epoch  0, batch   508 | loss: 3.1112263MixupTrain:  epoch  0, batch   509 | loss: 3.1642146MixupTrain:  epoch  0, batch   510 | loss: 2.8182058MixupTrain:  epoch  0, batch   511 | loss: 3.1081002MixupTrain:  epoch  0, batch   512 | loss: 3.0783858MixupTrain:  epoch  0, batch   513 | loss: 2.8135850MixupTrain:  epoch  0, batch   514 | loss: 3.0460372MixupTrain:  epoch  0, batch   515 | loss: 2.9312220MixupTrain:  epoch  0, batch   516 | loss: 2.9281731MixupTrain:  epoch  0, batch   517 | loss: 2.9275260MixupTrain:  epoch  0, batch   518 | loss: 2.9534144MixupTrain:  epoch  0, batch   519 | loss: 2.9033542MixupTrain:  epoch  0, batch   520 | loss: 3.1487637MixupTrain:  epoch  0, batch   521 | loss: 3.0385184MixupTrain:  epoch  0, batch   522 | loss: 3.0496943MixupTrain:  epoch  0, batch   523 | loss: 3.0597739MixupTrain:  epoch  0, batch   524 | loss: 3.2123404MixupTrain:  epoch  0, batch   525 | loss: 3.1052232MixupTrain:  epoch  0, batch   526 | loss: 3.2044291MixupTrain:  epoch  0, batch   527 | loss: 2.9683790MixupTrain:  epoch  0, batch   528 | loss: 2.9333062MixupTrain:  epoch  0, batch   529 | loss: 2.9809208MixupTrain:  epoch  0, batch   530 | loss: 2.9829450MixupTrain:  epoch  0, batch   531 | loss: 2.9649799MixupTrain:  epoch  0, batch   532 | loss: 3.2091360MixupTrain:  epoch  0, batch   533 | loss: 3.0755668MixupTrain:  epoch  0, batch   534 | loss: 2.9413671
MemoryTrain:  epoch  0, batch     0 | loss: 1.1675744MemoryTrain:  epoch  0, batch     1 | loss: 1.4010568MemoryTrain:  epoch  0, batch     2 | loss: 1.5551703MemoryTrain:  epoch  0, batch     3 | loss: 1.6130362MemoryTrain:  epoch  0, batch     4 | loss: 1.5801433MemoryTrain:  epoch  0, batch     5 | loss: 1.7323937MemoryTrain:  epoch  0, batch     6 | loss: 1.7791681MemoryTrain:  epoch  1, batch     0 | loss: 1.2643126MemoryTrain:  epoch  1, batch     1 | loss: 1.3488157MemoryTrain:  epoch  1, batch     2 | loss: 1.3695545MemoryTrain:  epoch  1, batch     3 | loss: 1.3242972MemoryTrain:  epoch  1, batch     4 | loss: 1.3370185MemoryTrain:  epoch  1, batch     5 | loss: 1.3008775MemoryTrain:  epoch  1, batch     6 | loss: 1.3379610MemoryTrain:  epoch  2, batch     0 | loss: 1.2904823MemoryTrain:  epoch  2, batch     1 | loss: 1.2986846MemoryTrain:  epoch  2, batch     2 | loss: 1.2353311MemoryTrain:  epoch  2, batch     3 | loss: 1.3249137MemoryTrain:  epoch  2, batch     4 | loss: 1.2500225MemoryTrain:  epoch  2, batch     5 | loss: 1.3195236MemoryTrain:  epoch  2, batch     6 | loss: 1.2732875MemoryTrain:  epoch  3, batch     0 | loss: 1.2662886MemoryTrain:  epoch  3, batch     1 | loss: 1.2712862MemoryTrain:  epoch  3, batch     2 | loss: 1.2365329MemoryTrain:  epoch  3, batch     3 | loss: 1.2184075MemoryTrain:  epoch  3, batch     4 | loss: 1.2310007MemoryTrain:  epoch  3, batch     5 | loss: 1.3309610MemoryTrain:  epoch  3, batch     6 | loss: 1.2269561MemoryTrain:  epoch  4, batch     0 | loss: 1.2273095MemoryTrain:  epoch  4, batch     1 | loss: 1.1976752MemoryTrain:  epoch  4, batch     2 | loss: 1.1910076MemoryTrain:  epoch  4, batch     3 | loss: 1.3416693MemoryTrain:  epoch  4, batch     4 | loss: 1.2152597MemoryTrain:  epoch  4, batch     5 | loss: 1.3304291MemoryTrain:  epoch  4, batch     6 | loss: 1.2308834MemoryTrain:  epoch  5, batch     0 | loss: 1.3109050MemoryTrain:  epoch  5, batch     1 | loss: 1.2845509MemoryTrain:  epoch  5, batch     2 | loss: 1.2433510MemoryTrain:  epoch  5, batch     3 | loss: 1.1949601MemoryTrain:  epoch  5, batch     4 | loss: 1.1639374MemoryTrain:  epoch  5, batch     5 | loss: 1.1873142MemoryTrain:  epoch  5, batch     6 | loss: 1.2947030MemoryTrain:  epoch  6, batch     0 | loss: 1.2024258MemoryTrain:  epoch  6, batch     1 | loss: 1.2541244MemoryTrain:  epoch  6, batch     2 | loss: 1.2105743MemoryTrain:  epoch  6, batch     3 | loss: 1.2368606MemoryTrain:  epoch  6, batch     4 | loss: 1.1934931MemoryTrain:  epoch  6, batch     5 | loss: 1.1923027MemoryTrain:  epoch  6, batch     6 | loss: 1.2139640MemoryTrain:  epoch  7, batch     0 | loss: 1.2268176MemoryTrain:  epoch  7, batch     1 | loss: 1.1851164MemoryTrain:  epoch  7, batch     2 | loss: 1.2641718MemoryTrain:  epoch  7, batch     3 | loss: 1.1753197MemoryTrain:  epoch  7, batch     4 | loss: 1.2107530MemoryTrain:  epoch  7, batch     5 | loss: 1.1772498MemoryTrain:  epoch  7, batch     6 | loss: 1.2122501MemoryTrain:  epoch  8, batch     0 | loss: 1.1997528MemoryTrain:  epoch  8, batch     1 | loss: 1.2106082MemoryTrain:  epoch  8, batch     2 | loss: 1.1643827MemoryTrain:  epoch  8, batch     3 | loss: 1.1609626MemoryTrain:  epoch  8, batch     4 | loss: 1.1957049MemoryTrain:  epoch  8, batch     5 | loss: 1.1701190MemoryTrain:  epoch  8, batch     6 | loss: 1.2278688MemoryTrain:  epoch  9, batch     0 | loss: 1.1602235MemoryTrain:  epoch  9, batch     1 | loss: 1.1651391MemoryTrain:  epoch  9, batch     2 | loss: 1.2150966MemoryTrain:  epoch  9, batch     3 | loss: 1.1670456MemoryTrain:  epoch  9, batch     4 | loss: 1.1933601MemoryTrain:  epoch  9, batch     5 | loss: 1.1766109MemoryTrain:  epoch  9, batch     6 | loss: 1.2368366
[EVAL] batch:    0 | acc: 31.25%,  total acc: 31.25%   [EVAL] batch:    1 | acc: 37.50%,  total acc: 34.38%   [EVAL] batch:    2 | acc: 25.00%,  total acc: 31.25%   [EVAL] batch:    3 | acc: 12.50%,  total acc: 26.56%   [EVAL] batch:    4 | acc: 12.50%,  total acc: 23.75%   [EVAL] batch:    5 | acc: 25.00%,  total acc: 23.96%   [EVAL] batch:    6 | acc: 56.25%,  total acc: 28.57%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 36.72%   [EVAL] batch:    8 | acc: 56.25%,  total acc: 38.89%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 41.88%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 45.45%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 48.96%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 51.92%   [EVAL] batch:   13 | acc: 87.50%,  total acc: 54.46%   [EVAL] batch:   14 | acc: 56.25%,  total acc: 54.58%   [EVAL] batch:   15 | acc: 81.25%,  total acc: 56.25%   [EVAL] batch:   16 | acc: 81.25%,  total acc: 57.72%   [EVAL] batch:   17 | acc: 75.00%,  total acc: 58.68%   [EVAL] batch:   18 | acc: 31.25%,  total acc: 57.24%   [EVAL] batch:   19 | acc: 43.75%,  total acc: 56.56%   [EVAL] batch:   20 | acc: 31.25%,  total acc: 55.36%   [EVAL] batch:   21 | acc: 37.50%,  total acc: 54.55%   
[EVAL] batch:    0 | acc: 50.00%,  total acc: 50.00%   [EVAL] batch:    1 | acc: 62.50%,  total acc: 56.25%   [EVAL] batch:    2 | acc: 50.00%,  total acc: 54.17%   [EVAL] batch:    3 | acc: 37.50%,  total acc: 50.00%   [EVAL] batch:    4 | acc: 50.00%,  total acc: 50.00%   [EVAL] batch:    5 | acc: 50.00%,  total acc: 50.00%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 54.46%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 59.38%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 62.50%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 64.38%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 66.48%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 67.19%   [EVAL] batch:   12 | acc: 43.75%,  total acc: 65.38%   [EVAL] batch:   13 | acc: 25.00%,  total acc: 62.50%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 63.33%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 62.89%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 63.60%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 63.54%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 63.49%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 64.69%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 66.37%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 67.61%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 69.02%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 70.05%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 71.25%   [EVAL] batch:   25 | acc: 87.50%,  total acc: 71.88%   [EVAL] batch:   26 | acc: 81.25%,  total acc: 72.22%   [EVAL] batch:   27 | acc: 87.50%,  total acc: 72.77%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 73.49%   [EVAL] batch:   29 | acc: 68.75%,  total acc: 73.33%   [EVAL] batch:   30 | acc: 75.00%,  total acc: 73.39%   [EVAL] batch:   31 | acc: 81.25%,  total acc: 73.63%   [EVAL] batch:   32 | acc: 62.50%,  total acc: 73.30%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 74.08%   [EVAL] batch:   34 | acc: 37.50%,  total acc: 73.04%   [EVAL] batch:   35 | acc: 25.00%,  total acc: 71.70%   [EVAL] batch:   36 | acc: 25.00%,  total acc: 70.44%   [EVAL] batch:   37 | acc: 37.50%,  total acc: 69.57%   [EVAL] batch:   38 | acc: 18.75%,  total acc: 68.27%   [EVAL] batch:   39 | acc: 56.25%,  total acc: 67.97%   [EVAL] batch:   40 | acc: 87.50%,  total acc: 68.45%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 69.05%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 69.77%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 70.45%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 71.11%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 71.74%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 72.34%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 72.92%   [EVAL] batch:   48 | acc: 12.50%,  total acc: 71.68%   [EVAL] batch:   49 | acc: 0.00%,  total acc: 70.25%   [EVAL] batch:   50 | acc: 18.75%,  total acc: 69.24%   [EVAL] batch:   51 | acc: 25.00%,  total acc: 68.39%   [EVAL] batch:   52 | acc: 18.75%,  total acc: 67.45%   [EVAL] batch:   53 | acc: 25.00%,  total acc: 66.67%   [EVAL] batch:   54 | acc: 18.75%,  total acc: 65.80%   [EVAL] batch:   55 | acc: 25.00%,  total acc: 65.07%   [EVAL] batch:   56 | acc: 18.75%,  total acc: 64.25%   [EVAL] batch:   57 | acc: 18.75%,  total acc: 63.47%   [EVAL] batch:   58 | acc: 12.50%,  total acc: 62.61%   [EVAL] batch:   59 | acc: 37.50%,  total acc: 62.19%   [EVAL] batch:   60 | acc: 62.50%,  total acc: 62.19%   [EVAL] batch:   61 | acc: 31.25%,  total acc: 61.69%   [EVAL] batch:   62 | acc: 25.00%,  total acc: 61.11%   [EVAL] batch:   63 | acc: 43.75%,  total acc: 60.84%   [EVAL] batch:   64 | acc: 81.25%,  total acc: 61.15%   [EVAL] batch:   65 | acc: 93.75%,  total acc: 61.65%   [EVAL] batch:   66 | acc: 100.00%,  total acc: 62.22%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 62.78%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 63.32%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 63.84%   [EVAL] batch:   70 | acc: 68.75%,  total acc: 63.91%   [EVAL] batch:   71 | acc: 62.50%,  total acc: 63.89%   [EVAL] batch:   72 | acc: 75.00%,  total acc: 64.04%   [EVAL] batch:   73 | acc: 68.75%,  total acc: 64.10%   [EVAL] batch:   74 | acc: 68.75%,  total acc: 64.17%   [EVAL] batch:   75 | acc: 87.50%,  total acc: 64.47%   [EVAL] batch:   76 | acc: 68.75%,  total acc: 64.53%   [EVAL] batch:   77 | acc: 50.00%,  total acc: 64.34%   [EVAL] batch:   78 | acc: 0.00%,  total acc: 63.53%   [EVAL] batch:   79 | acc: 0.00%,  total acc: 62.73%   [EVAL] batch:   80 | acc: 0.00%,  total acc: 61.96%   [EVAL] batch:   81 | acc: 62.50%,  total acc: 61.97%   [EVAL] batch:   82 | acc: 87.50%,  total acc: 62.27%   [EVAL] batch:   83 | acc: 93.75%,  total acc: 62.65%   [EVAL] batch:   84 | acc: 50.00%,  total acc: 62.50%   [EVAL] batch:   85 | acc: 50.00%,  total acc: 62.35%   [EVAL] batch:   86 | acc: 62.50%,  total acc: 62.36%   [EVAL] batch:   87 | acc: 68.75%,  total acc: 62.43%   [EVAL] batch:   88 | acc: 43.75%,  total acc: 62.22%   [EVAL] batch:   89 | acc: 50.00%,  total acc: 62.08%   [EVAL] batch:   90 | acc: 81.25%,  total acc: 62.29%   [EVAL] batch:   91 | acc: 93.75%,  total acc: 62.64%   [EVAL] batch:   92 | acc: 81.25%,  total acc: 62.84%   [EVAL] batch:   93 | acc: 93.75%,  total acc: 63.16%   [EVAL] batch:   94 | acc: 75.00%,  total acc: 63.29%   [EVAL] batch:   95 | acc: 87.50%,  total acc: 63.54%   [EVAL] batch:   96 | acc: 37.50%,  total acc: 63.27%   [EVAL] batch:   97 | acc: 37.50%,  total acc: 63.01%   [EVAL] batch:   98 | acc: 37.50%,  total acc: 62.75%   [EVAL] batch:   99 | acc: 18.75%,  total acc: 62.31%   [EVAL] batch:  100 | acc: 6.25%,  total acc: 61.76%   [EVAL] batch:  101 | acc: 18.75%,  total acc: 61.34%   [EVAL] batch:  102 | acc: 18.75%,  total acc: 60.92%   [EVAL] batch:  103 | acc: 81.25%,  total acc: 61.12%   [EVAL] batch:  104 | acc: 81.25%,  total acc: 61.31%   [EVAL] batch:  105 | acc: 56.25%,  total acc: 61.26%   [EVAL] batch:  106 | acc: 81.25%,  total acc: 61.45%   [EVAL] batch:  107 | acc: 75.00%,  total acc: 61.57%   [EVAL] batch:  108 | acc: 87.50%,  total acc: 61.81%   [EVAL] batch:  109 | acc: 87.50%,  total acc: 62.05%   [EVAL] batch:  110 | acc: 87.50%,  total acc: 62.27%   [EVAL] batch:  111 | acc: 62.50%,  total acc: 62.28%   [EVAL] batch:  112 | acc: 68.75%,  total acc: 62.33%   [EVAL] batch:  113 | acc: 81.25%,  total acc: 62.50%   [EVAL] batch:  114 | acc: 75.00%,  total acc: 62.61%   [EVAL] batch:  115 | acc: 37.50%,  total acc: 62.39%   [EVAL] batch:  116 | acc: 31.25%,  total acc: 62.13%   [EVAL] batch:  117 | acc: 50.00%,  total acc: 62.02%   [EVAL] batch:  118 | acc: 12.50%,  total acc: 61.61%   
cur_acc:  ['0.8636', '0.3750', '0.9241', '0.8750', '0.6779', '0.8304', '0.5455']
his_acc:  ['0.8636', '0.7281', '0.7546', '0.7835', '0.6756', '0.6682', '0.6161']
CurrentTrain: epoch  0, batch     0 | loss: 6.2874403CurrentTrain: epoch  0, batch     1 | loss: 6.9744663CurrentTrain: epoch  1, batch     0 | loss: 5.3527346CurrentTrain: epoch  1, batch     1 | loss: 5.8364019CurrentTrain: epoch  2, batch     0 | loss: 4.9603271CurrentTrain: epoch  2, batch     1 | loss: 4.9309192CurrentTrain: epoch  3, batch     0 | loss: 4.7655869CurrentTrain: epoch  3, batch     1 | loss: 3.5148284CurrentTrain: epoch  4, batch     0 | loss: 4.1592655CurrentTrain: epoch  4, batch     1 | loss: 3.3684912CurrentTrain: epoch  5, batch     0 | loss: 3.4394753CurrentTrain: epoch  5, batch     1 | loss: 3.2813337CurrentTrain: epoch  6, batch     0 | loss: 3.3924103CurrentTrain: epoch  6, batch     1 | loss: 2.9445939CurrentTrain: epoch  7, batch     0 | loss: 2.9743114CurrentTrain: epoch  7, batch     1 | loss: 3.0765049CurrentTrain: epoch  8, batch     0 | loss: 3.1523318CurrentTrain: epoch  8, batch     1 | loss: 2.3881695CurrentTrain: epoch  9, batch     0 | loss: 2.6442287CurrentTrain: epoch  9, batch     1 | loss: 2.3983417
#### person stateorprovinces of residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person stateorprovinces of residence" means a person is living in a state or province.
Here is an example:
Relation: person stateorprovinces of residence
Context: the chairman of the senate foreign relations committee , massachusetts democrat john kerry , and the panel 's top republican , richard lugar of indiana , were at the white house meeting , which was led by vice president joe biden , a former chairman of the foreign relations panel .
Head Entity: john kerry
Tail Entity: massachusetts
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person stateorprovinces of residence":
Relation: person stateorprovinces of residence  
Context: After moving to California for his new job, actor Tom Hanks quickly fell in love with the vibrant culture and beautiful landscapes of the state.  
Head Entity: Tom Hanks  
Tail Entity: California  

Relation: person stateorprovinces of residence  
Context: Following her graduation, singer Taylor Swift decided to settle in Nashville, where she began her music career and made many lifelong friends.  
Head Entity: Taylor Swift  
Tail Entity: Nashville  
#### person date of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person date of death" means a person was died on a date.
Here is an example:
Relation: person date of death
Context: he was taken off life support on feb. 14 .
Head Entity: he
Tail Entity: feb. 14
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person date of death":
Relation: person date of death  
Context: The renowned author passed away peacefully on March 5, 2020.  
Head Entity: The renowned author  
Tail Entity: March 5, 2020  

Relation: person date of death  
Context: She left this world on July 22, 2018, surrounded by her loved ones.  
Head Entity: She  
Tail Entity: July 22, 2018  
#### organization number of employees members ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization number of employees members" means the number of an organization 's employees is a number.
Here is an example:
Relation: organization number of employees members
Context: with the sweep of a federal regulator 's pen , massachusetts stands to gain a new life-science giant in april : covidien , a medical - supplies maker with thousands of products and more than 43,000 employees worldwide .
Head Entity: covidien
Tail Entity: 43,000
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization number of employees members":
Relation: organization number of employees members  
Context: The tech company, Innovatech, has rapidly expanded its workforce over the past year, now boasting a total of 25,000 employees across its global offices.  
Head Entity: Innovatech  
Tail Entity: 25,000  

Relation: organization number of employees members  
Context: After the merger, the newly formed entity, Global Finance Corp, reported an impressive workforce of 15,000 employees, making it one of the largest financial institutions in the region.  
Head Entity: Global Finance Corp  
Tail Entity: 15,000  
#### person alternate names ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person alternate names" means a person 's alias is a person.
Here is an example:
Relation: person alternate names
Context: `` i am known in the hospice as the man who would n't die , '' buchwald wrote in march .
Head Entity: buchwald
Tail Entity: man who would n't die
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person alternate names":
Relation: person alternate names  
Context: `` the famous author often referred to himself as the bard of Avon, '' said the literary critic during the lecture.  
Head Entity: author  
Tail Entity: bard of Avon  

Relation: person alternate names  
Context: `` in the world of music, she is often called the queen of pop, '' remarked the journalist in her article.  
Head Entity: she  
Tail Entity: queen of pop  
#### person spouse ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person spouse" means a person 's spouse is a person.
Here is an example:
Relation: person spouse
Context: beverly hills , california 2008-08-17 21:15:39 utc ------ there was much dancing : ellen degeneres and portia de rossi are married , according to reports .
Head Entity: ellen degeneres
Tail Entity: portia de rossi
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person spouse":
Relation: person spouse  
Context: in a lavish ceremony held in new york city, 2015-06-20 15:30:00 utc ------ the couple exchanged vows: john legend and chrissy teigen celebrated their love with family and friends.  
Head Entity: john legend  
Tail Entity: chrissy teigen  

Relation: person spouse  
Context: during a quiet beach wedding in malibu, 2019-07-12 17:45:00 utc ------ they promised to support each other: ben affleck and jennifer lopez tied the knot once again after years apart.  
Head Entity: ben affleck  
Tail Entity: jennifer lopez  
Mixup data size:  10630
MixupTrain:  epoch  0, batch     0 | loss: 4.0883808MixupTrain:  epoch  0, batch     1 | loss: 4.0141158MixupTrain:  epoch  0, batch     2 | loss: 3.3076138MixupTrain:  epoch  0, batch     3 | loss: 3.2938347MixupTrain:  epoch  0, batch     4 | loss: 3.6668139MixupTrain:  epoch  0, batch     5 | loss: 3.7356944MixupTrain:  epoch  0, batch     6 | loss: 3.2267251MixupTrain:  epoch  0, batch     7 | loss: 3.3937631MixupTrain:  epoch  0, batch     8 | loss: 3.4746304MixupTrain:  epoch  0, batch     9 | loss: 3.7808123MixupTrain:  epoch  0, batch    10 | loss: 3.2159815MixupTrain:  epoch  0, batch    11 | loss: 3.4202790MixupTrain:  epoch  0, batch    12 | loss: 3.6352475MixupTrain:  epoch  0, batch    13 | loss: 3.2647836MixupTrain:  epoch  0, batch    14 | loss: 3.4770045MixupTrain:  epoch  0, batch    15 | loss: 3.6066234MixupTrain:  epoch  0, batch    16 | loss: 3.6042426MixupTrain:  epoch  0, batch    17 | loss: 3.1930923MixupTrain:  epoch  0, batch    18 | loss: 3.4127359MixupTrain:  epoch  0, batch    19 | loss: 3.3879490MixupTrain:  epoch  0, batch    20 | loss: 3.2301102MixupTrain:  epoch  0, batch    21 | loss: 3.4375930MixupTrain:  epoch  0, batch    22 | loss: 3.2899547MixupTrain:  epoch  0, batch    23 | loss: 3.1619599MixupTrain:  epoch  0, batch    24 | loss: 3.3741188MixupTrain:  epoch  0, batch    25 | loss: 3.3757801MixupTrain:  epoch  0, batch    26 | loss: 3.3877516MixupTrain:  epoch  0, batch    27 | loss: 3.4545875MixupTrain:  epoch  0, batch    28 | loss: 3.2204723MixupTrain:  epoch  0, batch    29 | loss: 3.0069513MixupTrain:  epoch  0, batch    30 | loss: 3.3504019MixupTrain:  epoch  0, batch    31 | loss: 3.5729375MixupTrain:  epoch  0, batch    32 | loss: 3.3195326MixupTrain:  epoch  0, batch    33 | loss: 3.5015762MixupTrain:  epoch  0, batch    34 | loss: 3.2686555MixupTrain:  epoch  0, batch    35 | loss: 3.5684564MixupTrain:  epoch  0, batch    36 | loss: 3.0207729MixupTrain:  epoch  0, batch    37 | loss: 3.1038704MixupTrain:  epoch  0, batch    38 | loss: 3.0289412MixupTrain:  epoch  0, batch    39 | loss: 3.3369918MixupTrain:  epoch  0, batch    40 | loss: 3.2547865MixupTrain:  epoch  0, batch    41 | loss: 3.3213954MixupTrain:  epoch  0, batch    42 | loss: 3.1708298MixupTrain:  epoch  0, batch    43 | loss: 3.5478063MixupTrain:  epoch  0, batch    44 | loss: 3.0902922MixupTrain:  epoch  0, batch    45 | loss: 3.3132093MixupTrain:  epoch  0, batch    46 | loss: 3.5208921MixupTrain:  epoch  0, batch    47 | loss: 3.1219220MixupTrain:  epoch  0, batch    48 | loss: 3.4814863MixupTrain:  epoch  0, batch    49 | loss: 3.5508313MixupTrain:  epoch  0, batch    50 | loss: 3.0854325MixupTrain:  epoch  0, batch    51 | loss: 2.9523764MixupTrain:  epoch  0, batch    52 | loss: 3.0297217MixupTrain:  epoch  0, batch    53 | loss: 3.3910913MixupTrain:  epoch  0, batch    54 | loss: 2.9908929MixupTrain:  epoch  0, batch    55 | loss: 3.0662661MixupTrain:  epoch  0, batch    56 | loss: 3.1303201MixupTrain:  epoch  0, batch    57 | loss: 3.2220919MixupTrain:  epoch  0, batch    58 | loss: 3.5062811MixupTrain:  epoch  0, batch    59 | loss: 3.5148487MixupTrain:  epoch  0, batch    60 | loss: 2.9889741MixupTrain:  epoch  0, batch    61 | loss: 3.0055535MixupTrain:  epoch  0, batch    62 | loss: 3.3603020MixupTrain:  epoch  0, batch    63 | loss: 3.2669597MixupTrain:  epoch  0, batch    64 | loss: 3.1291206MixupTrain:  epoch  0, batch    65 | loss: 3.3589468MixupTrain:  epoch  0, batch    66 | loss: 2.9999743MixupTrain:  epoch  0, batch    67 | loss: 3.1731868MixupTrain:  epoch  0, batch    68 | loss: 3.3372242MixupTrain:  epoch  0, batch    69 | loss: 2.9780817MixupTrain:  epoch  0, batch    70 | loss: 2.9882672MixupTrain:  epoch  0, batch    71 | loss: 3.3392870MixupTrain:  epoch  0, batch    72 | loss: 3.1011634MixupTrain:  epoch  0, batch    73 | loss: 3.2883039MixupTrain:  epoch  0, batch    74 | loss: 3.0337563MixupTrain:  epoch  0, batch    75 | loss: 3.3572631MixupTrain:  epoch  0, batch    76 | loss: 3.0879269MixupTrain:  epoch  0, batch    77 | loss: 3.2113397MixupTrain:  epoch  0, batch    78 | loss: 3.2704282MixupTrain:  epoch  0, batch    79 | loss: 3.0275936MixupTrain:  epoch  0, batch    80 | loss: 3.3761392MixupTrain:  epoch  0, batch    81 | loss: 2.9286065MixupTrain:  epoch  0, batch    82 | loss: 3.4583793MixupTrain:  epoch  0, batch    83 | loss: 3.2988076MixupTrain:  epoch  0, batch    84 | loss: 2.9996109MixupTrain:  epoch  0, batch    85 | loss: 3.1282806MixupTrain:  epoch  0, batch    86 | loss: 2.9844348MixupTrain:  epoch  0, batch    87 | loss: 2.9473834MixupTrain:  epoch  0, batch    88 | loss: 2.9767313MixupTrain:  epoch  0, batch    89 | loss: 3.1975102MixupTrain:  epoch  0, batch    90 | loss: 2.9737344MixupTrain:  epoch  0, batch    91 | loss: 2.9898438MixupTrain:  epoch  0, batch    92 | loss: 3.1073306MixupTrain:  epoch  0, batch    93 | loss: 3.0134926MixupTrain:  epoch  0, batch    94 | loss: 3.0713286MixupTrain:  epoch  0, batch    95 | loss: 3.1459577MixupTrain:  epoch  0, batch    96 | loss: 2.9161301MixupTrain:  epoch  0, batch    97 | loss: 3.1965685MixupTrain:  epoch  0, batch    98 | loss: 3.1033013MixupTrain:  epoch  0, batch    99 | loss: 3.0231996MixupTrain:  epoch  0, batch   100 | loss: 2.9676304MixupTrain:  epoch  0, batch   101 | loss: 3.3011971MixupTrain:  epoch  0, batch   102 | loss: 3.1003795MixupTrain:  epoch  0, batch   103 | loss: 3.1610970MixupTrain:  epoch  0, batch   104 | loss: 3.1047952MixupTrain:  epoch  0, batch   105 | loss: 3.2632697MixupTrain:  epoch  0, batch   106 | loss: 2.8483262MixupTrain:  epoch  0, batch   107 | loss: 3.0503211MixupTrain:  epoch  0, batch   108 | loss: 3.2827220MixupTrain:  epoch  0, batch   109 | loss: 3.1954064MixupTrain:  epoch  0, batch   110 | loss: 3.0970833MixupTrain:  epoch  0, batch   111 | loss: 3.1768770MixupTrain:  epoch  0, batch   112 | loss: 2.9458570MixupTrain:  epoch  0, batch   113 | loss: 2.9881613MixupTrain:  epoch  0, batch   114 | loss: 2.9863732MixupTrain:  epoch  0, batch   115 | loss: 2.8255715MixupTrain:  epoch  0, batch   116 | loss: 3.2177474MixupTrain:  epoch  0, batch   117 | loss: 3.1424215MixupTrain:  epoch  0, batch   118 | loss: 3.2156694MixupTrain:  epoch  0, batch   119 | loss: 3.0530741MixupTrain:  epoch  0, batch   120 | loss: 3.0656855MixupTrain:  epoch  0, batch   121 | loss: 3.0827880MixupTrain:  epoch  0, batch   122 | loss: 3.0275598MixupTrain:  epoch  0, batch   123 | loss: 3.1188517MixupTrain:  epoch  0, batch   124 | loss: 3.2595062MixupTrain:  epoch  0, batch   125 | loss: 3.0686355MixupTrain:  epoch  0, batch   126 | loss: 3.0271783MixupTrain:  epoch  0, batch   127 | loss: 2.9740958MixupTrain:  epoch  0, batch   128 | loss: 2.9638829MixupTrain:  epoch  0, batch   129 | loss: 3.0631514MixupTrain:  epoch  0, batch   130 | loss: 2.8250549MixupTrain:  epoch  0, batch   131 | loss: 3.1430709MixupTrain:  epoch  0, batch   132 | loss: 3.0926695MixupTrain:  epoch  0, batch   133 | loss: 2.9977810MixupTrain:  epoch  0, batch   134 | loss: 2.9538274MixupTrain:  epoch  0, batch   135 | loss: 3.0105543MixupTrain:  epoch  0, batch   136 | loss: 2.8929236MixupTrain:  epoch  0, batch   137 | loss: 3.0708740MixupTrain:  epoch  0, batch   138 | loss: 3.1206946MixupTrain:  epoch  0, batch   139 | loss: 3.2125692MixupTrain:  epoch  0, batch   140 | loss: 2.9819870MixupTrain:  epoch  0, batch   141 | loss: 3.2118526MixupTrain:  epoch  0, batch   142 | loss: 3.1412277MixupTrain:  epoch  0, batch   143 | loss: 3.1077275MixupTrain:  epoch  0, batch   144 | loss: 3.0982397MixupTrain:  epoch  0, batch   145 | loss: 2.9842582MixupTrain:  epoch  0, batch   146 | loss: 2.9804888MixupTrain:  epoch  0, batch   147 | loss: 3.2310076MixupTrain:  epoch  0, batch   148 | loss: 3.0172286MixupTrain:  epoch  0, batch   149 | loss: 3.0615060MixupTrain:  epoch  0, batch   150 | loss: 2.9632387MixupTrain:  epoch  0, batch   151 | loss: 3.0334249MixupTrain:  epoch  0, batch   152 | loss: 3.0442166MixupTrain:  epoch  0, batch   153 | loss: 2.9306481MixupTrain:  epoch  0, batch   154 | loss: 3.0897169MixupTrain:  epoch  0, batch   155 | loss: 3.0748429MixupTrain:  epoch  0, batch   156 | loss: 2.9239976MixupTrain:  epoch  0, batch   157 | loss: 3.1141109MixupTrain:  epoch  0, batch   158 | loss: 2.9519844MixupTrain:  epoch  0, batch   159 | loss: 3.2335706MixupTrain:  epoch  0, batch   160 | loss: 3.1611300MixupTrain:  epoch  0, batch   161 | loss: 2.8685362MixupTrain:  epoch  0, batch   162 | loss: 2.8855715MixupTrain:  epoch  0, batch   163 | loss: 2.9888902MixupTrain:  epoch  0, batch   164 | loss: 2.9573374MixupTrain:  epoch  0, batch   165 | loss: 2.9640899MixupTrain:  epoch  0, batch   166 | loss: 2.7470462MixupTrain:  epoch  0, batch   167 | loss: 3.1550033MixupTrain:  epoch  0, batch   168 | loss: 2.9324098MixupTrain:  epoch  0, batch   169 | loss: 3.1875458MixupTrain:  epoch  0, batch   170 | loss: 3.0085607MixupTrain:  epoch  0, batch   171 | loss: 2.9475188MixupTrain:  epoch  0, batch   172 | loss: 3.0135293MixupTrain:  epoch  0, batch   173 | loss: 3.0356328MixupTrain:  epoch  0, batch   174 | loss: 3.2138219MixupTrain:  epoch  0, batch   175 | loss: 2.9599566MixupTrain:  epoch  0, batch   176 | loss: 2.9087524MixupTrain:  epoch  0, batch   177 | loss: 3.0103555MixupTrain:  epoch  0, batch   178 | loss: 2.9157221MixupTrain:  epoch  0, batch   179 | loss: 2.9174080MixupTrain:  epoch  0, batch   180 | loss: 2.9799652MixupTrain:  epoch  0, batch   181 | loss: 3.0426826MixupTrain:  epoch  0, batch   182 | loss: 2.8714051MixupTrain:  epoch  0, batch   183 | loss: 3.0464234MixupTrain:  epoch  0, batch   184 | loss: 3.0629196MixupTrain:  epoch  0, batch   185 | loss: 2.9157896MixupTrain:  epoch  0, batch   186 | loss: 2.9810340MixupTrain:  epoch  0, batch   187 | loss: 2.8373346MixupTrain:  epoch  0, batch   188 | loss: 3.0609760MixupTrain:  epoch  0, batch   189 | loss: 2.8090265MixupTrain:  epoch  0, batch   190 | loss: 3.0373349MixupTrain:  epoch  0, batch   191 | loss: 2.9590178MixupTrain:  epoch  0, batch   192 | loss: 2.8367896MixupTrain:  epoch  0, batch   193 | loss: 3.0305758MixupTrain:  epoch  0, batch   194 | loss: 3.0352817MixupTrain:  epoch  0, batch   195 | loss: 3.0382454MixupTrain:  epoch  0, batch   196 | loss: 3.1280966MixupTrain:  epoch  0, batch   197 | loss: 2.9577615MixupTrain:  epoch  0, batch   198 | loss: 2.9743543MixupTrain:  epoch  0, batch   199 | loss: 2.9730718MixupTrain:  epoch  0, batch   200 | loss: 3.0939798MixupTrain:  epoch  0, batch   201 | loss: 2.9087338MixupTrain:  epoch  0, batch   202 | loss: 3.0309296MixupTrain:  epoch  0, batch   203 | loss: 2.9423294MixupTrain:  epoch  0, batch   204 | loss: 3.1554999MixupTrain:  epoch  0, batch   205 | loss: 2.9686413MixupTrain:  epoch  0, batch   206 | loss: 2.9260652MixupTrain:  epoch  0, batch   207 | loss: 2.8266790MixupTrain:  epoch  0, batch   208 | loss: 3.0671754MixupTrain:  epoch  0, batch   209 | loss: 2.8912568MixupTrain:  epoch  0, batch   210 | loss: 3.0346208MixupTrain:  epoch  0, batch   211 | loss: 3.0032969MixupTrain:  epoch  0, batch   212 | loss: 2.9028971MixupTrain:  epoch  0, batch   213 | loss: 3.0918765MixupTrain:  epoch  0, batch   214 | loss: 2.8733513MixupTrain:  epoch  0, batch   215 | loss: 2.9308319MixupTrain:  epoch  0, batch   216 | loss: 2.9774399MixupTrain:  epoch  0, batch   217 | loss: 2.9037011MixupTrain:  epoch  0, batch   218 | loss: 3.0086288MixupTrain:  epoch  0, batch   219 | loss: 2.9912992MixupTrain:  epoch  0, batch   220 | loss: 2.9531386MixupTrain:  epoch  0, batch   221 | loss: 3.0146053MixupTrain:  epoch  0, batch   222 | loss: 3.2188129MixupTrain:  epoch  0, batch   223 | loss: 3.0563493MixupTrain:  epoch  0, batch   224 | loss: 3.0815096MixupTrain:  epoch  0, batch   225 | loss: 3.0674043MixupTrain:  epoch  0, batch   226 | loss: 2.9623790MixupTrain:  epoch  0, batch   227 | loss: 2.9637241MixupTrain:  epoch  0, batch   228 | loss: 3.2900290MixupTrain:  epoch  0, batch   229 | loss: 2.9957728MixupTrain:  epoch  0, batch   230 | loss: 2.9748344MixupTrain:  epoch  0, batch   231 | loss: 2.9144437MixupTrain:  epoch  0, batch   232 | loss: 3.0485029MixupTrain:  epoch  0, batch   233 | loss: 3.0247295MixupTrain:  epoch  0, batch   234 | loss: 2.9455948MixupTrain:  epoch  0, batch   235 | loss: 2.8563213MixupTrain:  epoch  0, batch   236 | loss: 3.0185719MixupTrain:  epoch  0, batch   237 | loss: 2.8243294MixupTrain:  epoch  0, batch   238 | loss: 3.0550330MixupTrain:  epoch  0, batch   239 | loss: 3.0998518MixupTrain:  epoch  0, batch   240 | loss: 3.0903068MixupTrain:  epoch  0, batch   241 | loss: 2.9050944MixupTrain:  epoch  0, batch   242 | loss: 3.0165815MixupTrain:  epoch  0, batch   243 | loss: 2.9103549MixupTrain:  epoch  0, batch   244 | loss: 2.9832785MixupTrain:  epoch  0, batch   245 | loss: 2.9929926MixupTrain:  epoch  0, batch   246 | loss: 3.0077958MixupTrain:  epoch  0, batch   247 | loss: 3.0186059MixupTrain:  epoch  0, batch   248 | loss: 2.9455292MixupTrain:  epoch  0, batch   249 | loss: 2.8846579MixupTrain:  epoch  0, batch   250 | loss: 2.8905685MixupTrain:  epoch  0, batch   251 | loss: 3.0550120MixupTrain:  epoch  0, batch   252 | loss: 3.0684123MixupTrain:  epoch  0, batch   253 | loss: 2.8862545MixupTrain:  epoch  0, batch   254 | loss: 2.8713498MixupTrain:  epoch  0, batch   255 | loss: 2.9997306MixupTrain:  epoch  0, batch   256 | loss: 2.8523953MixupTrain:  epoch  0, batch   257 | loss: 2.8533030MixupTrain:  epoch  0, batch   258 | loss: 3.1003535MixupTrain:  epoch  0, batch   259 | loss: 2.9093695MixupTrain:  epoch  0, batch   260 | loss: 2.9661613MixupTrain:  epoch  0, batch   261 | loss: 2.9256771MixupTrain:  epoch  0, batch   262 | loss: 2.8503923MixupTrain:  epoch  0, batch   263 | loss: 2.8650584MixupTrain:  epoch  0, batch   264 | loss: 3.0740442MixupTrain:  epoch  0, batch   265 | loss: 2.9687147MixupTrain:  epoch  0, batch   266 | loss: 3.0177045MixupTrain:  epoch  0, batch   267 | loss: 2.9806898MixupTrain:  epoch  0, batch   268 | loss: 2.8584876MixupTrain:  epoch  0, batch   269 | loss: 2.8496811MixupTrain:  epoch  0, batch   270 | loss: 2.9416206MixupTrain:  epoch  0, batch   271 | loss: 2.8611946MixupTrain:  epoch  0, batch   272 | loss: 2.8971841MixupTrain:  epoch  0, batch   273 | loss: 2.8453202MixupTrain:  epoch  0, batch   274 | loss: 3.0321534MixupTrain:  epoch  0, batch   275 | loss: 3.0074711MixupTrain:  epoch  0, batch   276 | loss: 3.1326358MixupTrain:  epoch  0, batch   277 | loss: 3.0050585MixupTrain:  epoch  0, batch   278 | loss: 2.9456148MixupTrain:  epoch  0, batch   279 | loss: 3.1642561MixupTrain:  epoch  0, batch   280 | loss: 2.9114723MixupTrain:  epoch  0, batch   281 | loss: 3.1584272MixupTrain:  epoch  0, batch   282 | loss: 2.9677744MixupTrain:  epoch  0, batch   283 | loss: 3.0536590MixupTrain:  epoch  0, batch   284 | loss: 2.8300757MixupTrain:  epoch  0, batch   285 | loss: 2.8600273MixupTrain:  epoch  0, batch   286 | loss: 2.9330480MixupTrain:  epoch  0, batch   287 | loss: 2.9908018MixupTrain:  epoch  0, batch   288 | loss: 3.0118876MixupTrain:  epoch  0, batch   289 | loss: 2.9973812MixupTrain:  epoch  0, batch   290 | loss: 2.9475091MixupTrain:  epoch  0, batch   291 | loss: 3.0073071MixupTrain:  epoch  0, batch   292 | loss: 2.8554611MixupTrain:  epoch  0, batch   293 | loss: 3.0739412MixupTrain:  epoch  0, batch   294 | loss: 2.9923682MixupTrain:  epoch  0, batch   295 | loss: 3.0125611MixupTrain:  epoch  0, batch   296 | loss: 3.0719819MixupTrain:  epoch  0, batch   297 | loss: 2.9694896MixupTrain:  epoch  0, batch   298 | loss: 2.8837039MixupTrain:  epoch  0, batch   299 | loss: 2.8564777MixupTrain:  epoch  0, batch   300 | loss: 3.0453801MixupTrain:  epoch  0, batch   301 | loss: 3.1141520MixupTrain:  epoch  0, batch   302 | loss: 3.0322256MixupTrain:  epoch  0, batch   303 | loss: 3.0161459MixupTrain:  epoch  0, batch   304 | loss: 3.0087085MixupTrain:  epoch  0, batch   305 | loss: 2.8954420MixupTrain:  epoch  0, batch   306 | loss: 2.9868898MixupTrain:  epoch  0, batch   307 | loss: 3.0948734MixupTrain:  epoch  0, batch   308 | loss: 2.9961853MixupTrain:  epoch  0, batch   309 | loss: 2.8337259MixupTrain:  epoch  0, batch   310 | loss: 2.9748321MixupTrain:  epoch  0, batch   311 | loss: 2.9792874MixupTrain:  epoch  0, batch   312 | loss: 2.9142280MixupTrain:  epoch  0, batch   313 | loss: 2.8376024MixupTrain:  epoch  0, batch   314 | loss: 3.0796809MixupTrain:  epoch  0, batch   315 | loss: 2.9503317MixupTrain:  epoch  0, batch   316 | loss: 3.0325809MixupTrain:  epoch  0, batch   317 | loss: 3.0150638MixupTrain:  epoch  0, batch   318 | loss: 3.0675628MixupTrain:  epoch  0, batch   319 | loss: 2.8391216MixupTrain:  epoch  0, batch   320 | loss: 2.9013724MixupTrain:  epoch  0, batch   321 | loss: 2.9189725MixupTrain:  epoch  0, batch   322 | loss: 2.8531742MixupTrain:  epoch  0, batch   323 | loss: 3.0574441MixupTrain:  epoch  0, batch   324 | loss: 2.8624654MixupTrain:  epoch  0, batch   325 | loss: 2.9684675MixupTrain:  epoch  0, batch   326 | loss: 2.8755136MixupTrain:  epoch  0, batch   327 | loss: 2.9124241MixupTrain:  epoch  0, batch   328 | loss: 3.1076703MixupTrain:  epoch  0, batch   329 | loss: 2.9167416MixupTrain:  epoch  0, batch   330 | loss: 3.0493958MixupTrain:  epoch  0, batch   331 | loss: 2.9293759MixupTrain:  epoch  0, batch   332 | loss: 2.9404225MixupTrain:  epoch  0, batch   333 | loss: 2.9904647MixupTrain:  epoch  0, batch   334 | loss: 2.9651539MixupTrain:  epoch  0, batch   335 | loss: 2.8649955MixupTrain:  epoch  0, batch   336 | loss: 2.9678719MixupTrain:  epoch  0, batch   337 | loss: 2.9087720MixupTrain:  epoch  0, batch   338 | loss: 2.9619648MixupTrain:  epoch  0, batch   339 | loss: 2.9979658MixupTrain:  epoch  0, batch   340 | loss: 2.9029281MixupTrain:  epoch  0, batch   341 | loss: 2.8254147MixupTrain:  epoch  0, batch   342 | loss: 2.9982057MixupTrain:  epoch  0, batch   343 | loss: 2.9608335MixupTrain:  epoch  0, batch   344 | loss: 2.9136529MixupTrain:  epoch  0, batch   345 | loss: 2.9462585MixupTrain:  epoch  0, batch   346 | loss: 3.0265179MixupTrain:  epoch  0, batch   347 | loss: 2.9457188MixupTrain:  epoch  0, batch   348 | loss: 2.9744747MixupTrain:  epoch  0, batch   349 | loss: 2.9984694MixupTrain:  epoch  0, batch   350 | loss: 2.9285517MixupTrain:  epoch  0, batch   351 | loss: 3.0640173MixupTrain:  epoch  0, batch   352 | loss: 2.8835106MixupTrain:  epoch  0, batch   353 | loss: 2.9089527MixupTrain:  epoch  0, batch   354 | loss: 2.9201422MixupTrain:  epoch  0, batch   355 | loss: 2.8429468MixupTrain:  epoch  0, batch   356 | loss: 2.9090197MixupTrain:  epoch  0, batch   357 | loss: 3.1509399MixupTrain:  epoch  0, batch   358 | loss: 3.0551805MixupTrain:  epoch  0, batch   359 | loss: 3.0645254MixupTrain:  epoch  0, batch   360 | loss: 2.9497714MixupTrain:  epoch  0, batch   361 | loss: 2.9669752MixupTrain:  epoch  0, batch   362 | loss: 2.9253798MixupTrain:  epoch  0, batch   363 | loss: 2.9807703MixupTrain:  epoch  0, batch   364 | loss: 2.9480677MixupTrain:  epoch  0, batch   365 | loss: 3.0626402MixupTrain:  epoch  0, batch   366 | loss: 2.8482199MixupTrain:  epoch  0, batch   367 | loss: 2.9370041MixupTrain:  epoch  0, batch   368 | loss: 2.9210014MixupTrain:  epoch  0, batch   369 | loss: 2.9283600MixupTrain:  epoch  0, batch   370 | loss: 2.9658475MixupTrain:  epoch  0, batch   371 | loss: 2.9095778MixupTrain:  epoch  0, batch   372 | loss: 3.0996227MixupTrain:  epoch  0, batch   373 | loss: 2.8950183MixupTrain:  epoch  0, batch   374 | loss: 2.9265575MixupTrain:  epoch  0, batch   375 | loss: 2.8609755MixupTrain:  epoch  0, batch   376 | loss: 2.8361444MixupTrain:  epoch  0, batch   377 | loss: 2.8568254MixupTrain:  epoch  0, batch   378 | loss: 3.0025730MixupTrain:  epoch  0, batch   379 | loss: 2.8665783MixupTrain:  epoch  0, batch   380 | loss: 2.8762012MixupTrain:  epoch  0, batch   381 | loss: 3.0227237MixupTrain:  epoch  0, batch   382 | loss: 3.0315719MixupTrain:  epoch  0, batch   383 | loss: 2.9423494MixupTrain:  epoch  0, batch   384 | loss: 2.8786058MixupTrain:  epoch  0, batch   385 | loss: 3.0092349MixupTrain:  epoch  0, batch   386 | loss: 3.0442791MixupTrain:  epoch  0, batch   387 | loss: 2.8776870MixupTrain:  epoch  0, batch   388 | loss: 2.9563918MixupTrain:  epoch  0, batch   389 | loss: 2.8587182MixupTrain:  epoch  0, batch   390 | loss: 2.8363690MixupTrain:  epoch  0, batch   391 | loss: 2.8059545MixupTrain:  epoch  0, batch   392 | loss: 3.0486255MixupTrain:  epoch  0, batch   393 | loss: 2.8306758MixupTrain:  epoch  0, batch   394 | loss: 3.0211039MixupTrain:  epoch  0, batch   395 | loss: 2.8787878MixupTrain:  epoch  0, batch   396 | loss: 2.8721023MixupTrain:  epoch  0, batch   397 | loss: 2.8963354MixupTrain:  epoch  0, batch   398 | loss: 3.0426834MixupTrain:  epoch  0, batch   399 | loss: 2.8269503MixupTrain:  epoch  0, batch   400 | loss: 2.9369278MixupTrain:  epoch  0, batch   401 | loss: 2.9905133MixupTrain:  epoch  0, batch   402 | loss: 2.9883046MixupTrain:  epoch  0, batch   403 | loss: 2.9275866MixupTrain:  epoch  0, batch   404 | loss: 3.0644846MixupTrain:  epoch  0, batch   405 | loss: 2.9025016MixupTrain:  epoch  0, batch   406 | loss: 2.8982856MixupTrain:  epoch  0, batch   407 | loss: 2.9414527MixupTrain:  epoch  0, batch   408 | loss: 2.8653369MixupTrain:  epoch  0, batch   409 | loss: 3.0379844MixupTrain:  epoch  0, batch   410 | loss: 2.9125276MixupTrain:  epoch  0, batch   411 | loss: 2.9029324MixupTrain:  epoch  0, batch   412 | loss: 2.7382755MixupTrain:  epoch  0, batch   413 | loss: 2.9011366MixupTrain:  epoch  0, batch   414 | loss: 3.0297241MixupTrain:  epoch  0, batch   415 | loss: 3.0499048MixupTrain:  epoch  0, batch   416 | loss: 2.9421411MixupTrain:  epoch  0, batch   417 | loss: 2.8941305MixupTrain:  epoch  0, batch   418 | loss: 2.9706597MixupTrain:  epoch  0, batch   419 | loss: 2.8901815MixupTrain:  epoch  0, batch   420 | loss: 2.9278750MixupTrain:  epoch  0, batch   421 | loss: 3.0463927MixupTrain:  epoch  0, batch   422 | loss: 2.9204030MixupTrain:  epoch  0, batch   423 | loss: 2.9446440MixupTrain:  epoch  0, batch   424 | loss: 2.8916588MixupTrain:  epoch  0, batch   425 | loss: 2.9471111MixupTrain:  epoch  0, batch   426 | loss: 2.9544544MixupTrain:  epoch  0, batch   427 | loss: 2.7956650MixupTrain:  epoch  0, batch   428 | loss: 2.9146173MixupTrain:  epoch  0, batch   429 | loss: 2.8886194MixupTrain:  epoch  0, batch   430 | loss: 2.9524562MixupTrain:  epoch  0, batch   431 | loss: 3.0573673MixupTrain:  epoch  0, batch   432 | loss: 2.8321500MixupTrain:  epoch  0, batch   433 | loss: 2.9032886MixupTrain:  epoch  0, batch   434 | loss: 2.9500594MixupTrain:  epoch  0, batch   435 | loss: 2.8222003MixupTrain:  epoch  0, batch   436 | loss: 2.9929838MixupTrain:  epoch  0, batch   437 | loss: 2.9536786MixupTrain:  epoch  0, batch   438 | loss: 2.8466530MixupTrain:  epoch  0, batch   439 | loss: 2.8457623MixupTrain:  epoch  0, batch   440 | loss: 2.9421539MixupTrain:  epoch  0, batch   441 | loss: 2.9428859MixupTrain:  epoch  0, batch   442 | loss: 2.8454614MixupTrain:  epoch  0, batch   443 | loss: 3.0561452MixupTrain:  epoch  0, batch   444 | loss: 2.9865274MixupTrain:  epoch  0, batch   445 | loss: 2.9700508MixupTrain:  epoch  0, batch   446 | loss: 2.8285151MixupTrain:  epoch  0, batch   447 | loss: 2.9006152MixupTrain:  epoch  0, batch   448 | loss: 2.8060975MixupTrain:  epoch  0, batch   449 | loss: 2.8790693MixupTrain:  epoch  0, batch   450 | loss: 2.9555283MixupTrain:  epoch  0, batch   451 | loss: 2.9913061MixupTrain:  epoch  0, batch   452 | loss: 2.8375218MixupTrain:  epoch  0, batch   453 | loss: 2.9776421MixupTrain:  epoch  0, batch   454 | loss: 3.0231948MixupTrain:  epoch  0, batch   455 | loss: 2.9148808MixupTrain:  epoch  0, batch   456 | loss: 3.0218496MixupTrain:  epoch  0, batch   457 | loss: 2.8755007MixupTrain:  epoch  0, batch   458 | loss: 2.9438651MixupTrain:  epoch  0, batch   459 | loss: 2.9898221MixupTrain:  epoch  0, batch   460 | loss: 2.8847766MixupTrain:  epoch  0, batch   461 | loss: 2.8896444MixupTrain:  epoch  0, batch   462 | loss: 2.8780608MixupTrain:  epoch  0, batch   463 | loss: 2.9183588MixupTrain:  epoch  0, batch   464 | loss: 2.9553914MixupTrain:  epoch  0, batch   465 | loss: 2.8798091MixupTrain:  epoch  0, batch   466 | loss: 2.9988699MixupTrain:  epoch  0, batch   467 | loss: 2.8486934MixupTrain:  epoch  0, batch   468 | loss: 2.8179140MixupTrain:  epoch  0, batch   469 | loss: 2.9106975MixupTrain:  epoch  0, batch   470 | loss: 2.7847733MixupTrain:  epoch  0, batch   471 | loss: 2.9875154MixupTrain:  epoch  0, batch   472 | loss: 2.7571273MixupTrain:  epoch  0, batch   473 | loss: 2.8665249MixupTrain:  epoch  0, batch   474 | loss: 2.9481194MixupTrain:  epoch  0, batch   475 | loss: 3.0188928MixupTrain:  epoch  0, batch   476 | loss: 2.9007740MixupTrain:  epoch  0, batch   477 | loss: 2.9332504MixupTrain:  epoch  0, batch   478 | loss: 2.9702260MixupTrain:  epoch  0, batch   479 | loss: 2.9698768MixupTrain:  epoch  0, batch   480 | loss: 3.0482717MixupTrain:  epoch  0, batch   481 | loss: 3.0074358MixupTrain:  epoch  0, batch   482 | loss: 2.8748322MixupTrain:  epoch  0, batch   483 | loss: 2.9665420MixupTrain:  epoch  0, batch   484 | loss: 2.8478336MixupTrain:  epoch  0, batch   485 | loss: 2.8902371MixupTrain:  epoch  0, batch   486 | loss: 2.8985958MixupTrain:  epoch  0, batch   487 | loss: 2.7835140MixupTrain:  epoch  0, batch   488 | loss: 3.0432718MixupTrain:  epoch  0, batch   489 | loss: 2.8405161MixupTrain:  epoch  0, batch   490 | loss: 2.9018326MixupTrain:  epoch  0, batch   491 | loss: 2.8818574MixupTrain:  epoch  0, batch   492 | loss: 2.9923530MixupTrain:  epoch  0, batch   493 | loss: 2.9778383MixupTrain:  epoch  0, batch   494 | loss: 2.9020121MixupTrain:  epoch  0, batch   495 | loss: 3.0291851MixupTrain:  epoch  0, batch   496 | loss: 2.8454919MixupTrain:  epoch  0, batch   497 | loss: 2.8830178MixupTrain:  epoch  0, batch   498 | loss: 2.9656572MixupTrain:  epoch  0, batch   499 | loss: 2.9255776MixupTrain:  epoch  0, batch   500 | loss: 2.9036160MixupTrain:  epoch  0, batch   501 | loss: 2.9034402MixupTrain:  epoch  0, batch   502 | loss: 2.8755269MixupTrain:  epoch  0, batch   503 | loss: 2.9415050MixupTrain:  epoch  0, batch   504 | loss: 2.9044662MixupTrain:  epoch  0, batch   505 | loss: 2.7701707MixupTrain:  epoch  0, batch   506 | loss: 2.9795065MixupTrain:  epoch  0, batch   507 | loss: 2.9443178MixupTrain:  epoch  0, batch   508 | loss: 3.0461147MixupTrain:  epoch  0, batch   509 | loss: 2.9049463MixupTrain:  epoch  0, batch   510 | loss: 3.0046749MixupTrain:  epoch  0, batch   511 | loss: 2.9181197MixupTrain:  epoch  0, batch   512 | loss: 3.0712323MixupTrain:  epoch  0, batch   513 | loss: 2.9944742MixupTrain:  epoch  0, batch   514 | loss: 2.9267743MixupTrain:  epoch  0, batch   515 | loss: 2.9202590MixupTrain:  epoch  0, batch   516 | loss: 2.8715527MixupTrain:  epoch  0, batch   517 | loss: 2.8076935MixupTrain:  epoch  0, batch   518 | loss: 2.9473844MixupTrain:  epoch  0, batch   519 | loss: 2.9500432MixupTrain:  epoch  0, batch   520 | loss: 3.0680523MixupTrain:  epoch  0, batch   521 | loss: 3.0622144MixupTrain:  epoch  0, batch   522 | loss: 2.9291677MixupTrain:  epoch  0, batch   523 | loss: 2.8959761MixupTrain:  epoch  0, batch   524 | loss: 2.9881742MixupTrain:  epoch  0, batch   525 | loss: 2.8890727MixupTrain:  epoch  0, batch   526 | loss: 2.9338572MixupTrain:  epoch  0, batch   527 | loss: 2.8632431MixupTrain:  epoch  0, batch   528 | loss: 2.9566271MixupTrain:  epoch  0, batch   529 | loss: 2.8665357MixupTrain:  epoch  0, batch   530 | loss: 2.9394145MixupTrain:  epoch  0, batch   531 | loss: 2.7628748MixupTrain:  epoch  0, batch   532 | loss: 2.9323659MixupTrain:  epoch  0, batch   533 | loss: 2.9349861MixupTrain:  epoch  0, batch   534 | loss: 2.8888738MixupTrain:  epoch  0, batch   535 | loss: 2.8369553MixupTrain:  epoch  0, batch   536 | loss: 2.9660034MixupTrain:  epoch  0, batch   537 | loss: 3.0535908MixupTrain:  epoch  0, batch   538 | loss: 2.8223579MixupTrain:  epoch  0, batch   539 | loss: 2.8181279MixupTrain:  epoch  0, batch   540 | loss: 2.8789878MixupTrain:  epoch  0, batch   541 | loss: 2.9034433MixupTrain:  epoch  0, batch   542 | loss: 2.9498773MixupTrain:  epoch  0, batch   543 | loss: 2.9263132MixupTrain:  epoch  0, batch   544 | loss: 2.9509487MixupTrain:  epoch  0, batch   545 | loss: 3.0564892MixupTrain:  epoch  0, batch   546 | loss: 2.8088472MixupTrain:  epoch  0, batch   547 | loss: 3.0719697MixupTrain:  epoch  0, batch   548 | loss: 2.9384742MixupTrain:  epoch  0, batch   549 | loss: 2.9271812MixupTrain:  epoch  0, batch   550 | loss: 2.8910470MixupTrain:  epoch  0, batch   551 | loss: 2.9724977MixupTrain:  epoch  0, batch   552 | loss: 2.9356413MixupTrain:  epoch  0, batch   553 | loss: 2.9366362MixupTrain:  epoch  0, batch   554 | loss: 2.8875985MixupTrain:  epoch  0, batch   555 | loss: 3.1027508MixupTrain:  epoch  0, batch   556 | loss: 2.8229206MixupTrain:  epoch  0, batch   557 | loss: 2.8747559MixupTrain:  epoch  0, batch   558 | loss: 2.9006362MixupTrain:  epoch  0, batch   559 | loss: 2.8940268MixupTrain:  epoch  0, batch   560 | loss: 2.8556032MixupTrain:  epoch  0, batch   561 | loss: 2.8755724MixupTrain:  epoch  0, batch   562 | loss: 2.9383125MixupTrain:  epoch  0, batch   563 | loss: 3.0603802MixupTrain:  epoch  0, batch   564 | loss: 2.8395982MixupTrain:  epoch  0, batch   565 | loss: 3.0130119MixupTrain:  epoch  0, batch   566 | loss: 2.8199248MixupTrain:  epoch  0, batch   567 | loss: 3.0563610MixupTrain:  epoch  0, batch   568 | loss: 2.9549055MixupTrain:  epoch  0, batch   569 | loss: 2.9496796MixupTrain:  epoch  0, batch   570 | loss: 3.0076268MixupTrain:  epoch  0, batch   571 | loss: 2.9938562MixupTrain:  epoch  0, batch   572 | loss: 2.8767037MixupTrain:  epoch  0, batch   573 | loss: 2.9079134MixupTrain:  epoch  0, batch   574 | loss: 2.7712421MixupTrain:  epoch  0, batch   575 | loss: 2.8851471MixupTrain:  epoch  0, batch   576 | loss: 2.8595312MixupTrain:  epoch  0, batch   577 | loss: 2.8317833MixupTrain:  epoch  0, batch   578 | loss: 2.7712836MixupTrain:  epoch  0, batch   579 | loss: 2.9368856MixupTrain:  epoch  0, batch   580 | loss: 2.9798784MixupTrain:  epoch  0, batch   581 | loss: 2.8794618MixupTrain:  epoch  0, batch   582 | loss: 2.8662643MixupTrain:  epoch  0, batch   583 | loss: 2.9595828MixupTrain:  epoch  0, batch   584 | loss: 3.0524611MixupTrain:  epoch  0, batch   585 | loss: 2.8747301MixupTrain:  epoch  0, batch   586 | loss: 2.8887608MixupTrain:  epoch  0, batch   587 | loss: 2.9758933MixupTrain:  epoch  0, batch   588 | loss: 2.9207850MixupTrain:  epoch  0, batch   589 | loss: 2.8310938MixupTrain:  epoch  0, batch   590 | loss: 2.9357650MixupTrain:  epoch  0, batch   591 | loss: 3.0817356MixupTrain:  epoch  0, batch   592 | loss: 2.9769154MixupTrain:  epoch  0, batch   593 | loss: 3.0475216MixupTrain:  epoch  0, batch   594 | loss: 2.8971624MixupTrain:  epoch  0, batch   595 | loss: 2.9058342MixupTrain:  epoch  0, batch   596 | loss: 3.0129330MixupTrain:  epoch  0, batch   597 | loss: 2.8540177MixupTrain:  epoch  0, batch   598 | loss: 2.8941031MixupTrain:  epoch  0, batch   599 | loss: 2.8611054MixupTrain:  epoch  0, batch   600 | loss: 2.9115720MixupTrain:  epoch  0, batch   601 | loss: 2.9712071MixupTrain:  epoch  0, batch   602 | loss: 2.8175459MixupTrain:  epoch  0, batch   603 | loss: 2.9813430MixupTrain:  epoch  0, batch   604 | loss: 2.8300090MixupTrain:  epoch  0, batch   605 | loss: 3.1425338MixupTrain:  epoch  0, batch   606 | loss: 3.0515461MixupTrain:  epoch  0, batch   607 | loss: 2.7849455MixupTrain:  epoch  0, batch   608 | loss: 3.0068967MixupTrain:  epoch  0, batch   609 | loss: 3.0315416MixupTrain:  epoch  0, batch   610 | loss: 2.8606229MixupTrain:  epoch  0, batch   611 | loss: 3.1007919MixupTrain:  epoch  0, batch   612 | loss: 2.8136916MixupTrain:  epoch  0, batch   613 | loss: 2.9447196MixupTrain:  epoch  0, batch   614 | loss: 2.9103255MixupTrain:  epoch  0, batch   615 | loss: 2.9376702MixupTrain:  epoch  0, batch   616 | loss: 2.9104931MixupTrain:  epoch  0, batch   617 | loss: 2.9977927MixupTrain:  epoch  0, batch   618 | loss: 2.8013003MixupTrain:  epoch  0, batch   619 | loss: 2.9747231MixupTrain:  epoch  0, batch   620 | loss: 2.8121862MixupTrain:  epoch  0, batch   621 | loss: 3.0983891MixupTrain:  epoch  0, batch   622 | loss: 2.9510629MixupTrain:  epoch  0, batch   623 | loss: 2.7801297MixupTrain:  epoch  0, batch   624 | loss: 2.9154394MixupTrain:  epoch  0, batch   625 | loss: 2.8720307MixupTrain:  epoch  0, batch   626 | loss: 2.9103298MixupTrain:  epoch  0, batch   627 | loss: 2.9496362MixupTrain:  epoch  0, batch   628 | loss: 2.9540009MixupTrain:  epoch  0, batch   629 | loss: 2.9171922MixupTrain:  epoch  0, batch   630 | loss: 2.7958045MixupTrain:  epoch  0, batch   631 | loss: 2.8378911MixupTrain:  epoch  0, batch   632 | loss: 2.9026551MixupTrain:  epoch  0, batch   633 | loss: 2.7768950MixupTrain:  epoch  0, batch   634 | loss: 2.9316893MixupTrain:  epoch  0, batch   635 | loss: 2.9273000MixupTrain:  epoch  0, batch   636 | loss: 2.9093964MixupTrain:  epoch  0, batch   637 | loss: 2.8266478MixupTrain:  epoch  0, batch   638 | loss: 2.9856315MixupTrain:  epoch  0, batch   639 | loss: 3.0334835MixupTrain:  epoch  0, batch   640 | loss: 2.9839849MixupTrain:  epoch  0, batch   641 | loss: 2.8781099MixupTrain:  epoch  0, batch   642 | loss: 2.9019675MixupTrain:  epoch  0, batch   643 | loss: 2.8367176MixupTrain:  epoch  0, batch   644 | loss: 2.9342358MixupTrain:  epoch  0, batch   645 | loss: 2.8608179MixupTrain:  epoch  0, batch   646 | loss: 2.8060057MixupTrain:  epoch  0, batch   647 | loss: 2.9320705MixupTrain:  epoch  0, batch   648 | loss: 3.0346491MixupTrain:  epoch  0, batch   649 | loss: 2.8615017MixupTrain:  epoch  0, batch   650 | loss: 2.8827591MixupTrain:  epoch  0, batch   651 | loss: 2.8773022MixupTrain:  epoch  0, batch   652 | loss: 2.9336729MixupTrain:  epoch  0, batch   653 | loss: 2.9323044MixupTrain:  epoch  0, batch   654 | loss: 2.9629891MixupTrain:  epoch  0, batch   655 | loss: 2.8736193MixupTrain:  epoch  0, batch   656 | loss: 3.0270677MixupTrain:  epoch  0, batch   657 | loss: 2.8412671MixupTrain:  epoch  0, batch   658 | loss: 3.0503626MixupTrain:  epoch  0, batch   659 | loss: 2.8413649MixupTrain:  epoch  0, batch   660 | loss: 2.8944561MixupTrain:  epoch  0, batch   661 | loss: 2.9299023MixupTrain:  epoch  0, batch   662 | loss: 2.8548403MixupTrain:  epoch  0, batch   663 | loss: 2.9618840MixupTrain:  epoch  0, batch   664 | loss: 2.8562927
MemoryTrain:  epoch  0, batch     0 | loss: 1.3241274MemoryTrain:  epoch  0, batch     1 | loss: 1.2322195MemoryTrain:  epoch  0, batch     2 | loss: 1.3950758MemoryTrain:  epoch  0, batch     3 | loss: 1.5688608MemoryTrain:  epoch  0, batch     4 | loss: 1.7017167MemoryTrain:  epoch  0, batch     5 | loss: 1.5662134MemoryTrain:  epoch  0, batch     6 | loss: 1.6851113MemoryTrain:  epoch  0, batch     7 | loss: 1.8667829MemoryTrain:  epoch  1, batch     0 | loss: 1.3295295MemoryTrain:  epoch  1, batch     1 | loss: 1.3840002MemoryTrain:  epoch  1, batch     2 | loss: 1.3365544MemoryTrain:  epoch  1, batch     3 | loss: 1.2982373MemoryTrain:  epoch  1, batch     4 | loss: 1.3620446MemoryTrain:  epoch  1, batch     5 | loss: 1.3496171MemoryTrain:  epoch  1, batch     6 | loss: 1.3730991MemoryTrain:  epoch  1, batch     7 | loss: 1.3020046MemoryTrain:  epoch  2, batch     0 | loss: 1.5022663MemoryTrain:  epoch  2, batch     1 | loss: 1.2674885MemoryTrain:  epoch  2, batch     2 | loss: 1.3447471MemoryTrain:  epoch  2, batch     3 | loss: 1.2960675MemoryTrain:  epoch  2, batch     4 | loss: 1.2075689MemoryTrain:  epoch  2, batch     5 | loss: 1.2190759MemoryTrain:  epoch  2, batch     6 | loss: 1.3343611MemoryTrain:  epoch  2, batch     7 | loss: 1.2326984MemoryTrain:  epoch  3, batch     0 | loss: 1.3172719MemoryTrain:  epoch  3, batch     1 | loss: 1.2044716MemoryTrain:  epoch  3, batch     2 | loss: 1.2576561MemoryTrain:  epoch  3, batch     3 | loss: 1.3709838MemoryTrain:  epoch  3, batch     4 | loss: 1.2780511MemoryTrain:  epoch  3, batch     5 | loss: 1.2171048MemoryTrain:  epoch  3, batch     6 | loss: 1.2117544MemoryTrain:  epoch  3, batch     7 | loss: 1.3768070MemoryTrain:  epoch  4, batch     0 | loss: 1.2154486MemoryTrain:  epoch  4, batch     1 | loss: 1.2437563MemoryTrain:  epoch  4, batch     2 | loss: 1.2875865MemoryTrain:  epoch  4, batch     3 | loss: 1.2112561MemoryTrain:  epoch  4, batch     4 | loss: 1.2377942MemoryTrain:  epoch  4, batch     5 | loss: 1.1880454MemoryTrain:  epoch  4, batch     6 | loss: 1.2193530MemoryTrain:  epoch  4, batch     7 | loss: 1.2312721MemoryTrain:  epoch  5, batch     0 | loss: 1.1960382MemoryTrain:  epoch  5, batch     1 | loss: 1.2258714MemoryTrain:  epoch  5, batch     2 | loss: 1.2038668MemoryTrain:  epoch  5, batch     3 | loss: 1.2984477MemoryTrain:  epoch  5, batch     4 | loss: 1.2157218MemoryTrain:  epoch  5, batch     5 | loss: 1.2051878MemoryTrain:  epoch  5, batch     6 | loss: 1.2307889MemoryTrain:  epoch  5, batch     7 | loss: 1.2315309MemoryTrain:  epoch  6, batch     0 | loss: 1.2167311MemoryTrain:  epoch  6, batch     1 | loss: 1.2997434MemoryTrain:  epoch  6, batch     2 | loss: 1.1837814MemoryTrain:  epoch  6, batch     3 | loss: 1.2053696MemoryTrain:  epoch  6, batch     4 | loss: 1.2098575MemoryTrain:  epoch  6, batch     5 | loss: 1.2430906MemoryTrain:  epoch  6, batch     6 | loss: 1.2053081MemoryTrain:  epoch  6, batch     7 | loss: 1.2021177MemoryTrain:  epoch  7, batch     0 | loss: 1.1937263MemoryTrain:  epoch  7, batch     1 | loss: 1.2037597MemoryTrain:  epoch  7, batch     2 | loss: 1.1766632MemoryTrain:  epoch  7, batch     3 | loss: 1.1790428MemoryTrain:  epoch  7, batch     4 | loss: 1.2029349MemoryTrain:  epoch  7, batch     5 | loss: 1.1806902MemoryTrain:  epoch  7, batch     6 | loss: 1.2276912MemoryTrain:  epoch  7, batch     7 | loss: 1.1581870MemoryTrain:  epoch  8, batch     0 | loss: 1.1618285MemoryTrain:  epoch  8, batch     1 | loss: 1.1901169MemoryTrain:  epoch  8, batch     2 | loss: 1.1995521MemoryTrain:  epoch  8, batch     3 | loss: 1.1730585MemoryTrain:  epoch  8, batch     4 | loss: 1.2344613MemoryTrain:  epoch  8, batch     5 | loss: 1.1958078MemoryTrain:  epoch  8, batch     6 | loss: 1.1857328MemoryTrain:  epoch  8, batch     7 | loss: 1.2142081MemoryTrain:  epoch  9, batch     0 | loss: 1.1905251MemoryTrain:  epoch  9, batch     1 | loss: 1.1761515MemoryTrain:  epoch  9, batch     2 | loss: 1.2206610MemoryTrain:  epoch  9, batch     3 | loss: 1.1776619MemoryTrain:  epoch  9, batch     4 | loss: 1.1725111MemoryTrain:  epoch  9, batch     5 | loss: 1.1761661MemoryTrain:  epoch  9, batch     6 | loss: 1.1621077MemoryTrain:  epoch  9, batch     7 | loss: 1.1705754
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 62.50%,  total acc: 65.62%   [EVAL] batch:    2 | acc: 43.75%,  total acc: 58.33%   [EVAL] batch:    3 | acc: 62.50%,  total acc: 59.38%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 61.25%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 65.62%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 69.64%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 71.88%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 72.92%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 73.75%   [EVAL] batch:   10 | acc: 43.75%,  total acc: 71.02%   [EVAL] batch:   11 | acc: 43.75%,  total acc: 68.75%   [EVAL] batch:   12 | acc: 50.00%,  total acc: 67.31%   [EVAL] batch:   13 | acc: 25.00%,  total acc: 64.29%   [EVAL] batch:   14 | acc: 18.75%,  total acc: 61.25%   
[EVAL] batch:    0 | acc: 37.50%,  total acc: 37.50%   [EVAL] batch:    1 | acc: 43.75%,  total acc: 40.62%   [EVAL] batch:    2 | acc: 37.50%,  total acc: 39.58%   [EVAL] batch:    3 | acc: 25.00%,  total acc: 35.94%   [EVAL] batch:    4 | acc: 25.00%,  total acc: 33.75%   [EVAL] batch:    5 | acc: 25.00%,  total acc: 32.29%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 38.39%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 46.09%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 51.39%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 53.75%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 56.25%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 58.33%   [EVAL] batch:   12 | acc: 37.50%,  total acc: 56.73%   [EVAL] batch:   13 | acc: 25.00%,  total acc: 54.46%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 55.83%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 55.86%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 56.99%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 57.29%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 57.57%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 59.06%   [EVAL] batch:   20 | acc: 81.25%,  total acc: 60.12%   [EVAL] batch:   21 | acc: 62.50%,  total acc: 60.23%   [EVAL] batch:   22 | acc: 81.25%,  total acc: 61.14%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 62.50%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 63.50%   [EVAL] batch:   25 | acc: 81.25%,  total acc: 64.18%   [EVAL] batch:   26 | acc: 87.50%,  total acc: 65.05%   [EVAL] batch:   27 | acc: 87.50%,  total acc: 65.85%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 67.03%   [EVAL] batch:   29 | acc: 68.75%,  total acc: 67.08%   [EVAL] batch:   30 | acc: 75.00%,  total acc: 67.34%   [EVAL] batch:   31 | acc: 81.25%,  total acc: 67.77%   [EVAL] batch:   32 | acc: 62.50%,  total acc: 67.61%   [EVAL] batch:   33 | acc: 81.25%,  total acc: 68.01%   [EVAL] batch:   34 | acc: 31.25%,  total acc: 66.96%   [EVAL] batch:   35 | acc: 12.50%,  total acc: 65.45%   [EVAL] batch:   36 | acc: 12.50%,  total acc: 64.02%   [EVAL] batch:   37 | acc: 31.25%,  total acc: 63.16%   [EVAL] batch:   38 | acc: 12.50%,  total acc: 61.86%   [EVAL] batch:   39 | acc: 50.00%,  total acc: 61.56%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 62.35%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 63.10%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 63.95%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 64.77%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 65.56%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 66.30%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 67.02%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 67.71%   [EVAL] batch:   48 | acc: 12.50%,  total acc: 66.58%   [EVAL] batch:   49 | acc: 0.00%,  total acc: 65.25%   [EVAL] batch:   50 | acc: 6.25%,  total acc: 64.09%   [EVAL] batch:   51 | acc: 18.75%,  total acc: 63.22%   [EVAL] batch:   52 | acc: 12.50%,  total acc: 62.26%   [EVAL] batch:   53 | acc: 12.50%,  total acc: 61.34%   [EVAL] batch:   54 | acc: 0.00%,  total acc: 60.23%   [EVAL] batch:   55 | acc: 12.50%,  total acc: 59.38%   [EVAL] batch:   56 | acc: 6.25%,  total acc: 58.44%   [EVAL] batch:   57 | acc: 6.25%,  total acc: 57.54%   [EVAL] batch:   58 | acc: 6.25%,  total acc: 56.67%   [EVAL] batch:   59 | acc: 37.50%,  total acc: 56.35%   [EVAL] batch:   60 | acc: 62.50%,  total acc: 56.45%   [EVAL] batch:   61 | acc: 25.00%,  total acc: 55.95%   [EVAL] batch:   62 | acc: 18.75%,  total acc: 55.36%   [EVAL] batch:   63 | acc: 37.50%,  total acc: 55.08%   [EVAL] batch:   64 | acc: 81.25%,  total acc: 55.48%   [EVAL] batch:   65 | acc: 93.75%,  total acc: 56.06%   [EVAL] batch:   66 | acc: 100.00%,  total acc: 56.72%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 57.35%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 57.97%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 58.57%   [EVAL] batch:   70 | acc: 75.00%,  total acc: 58.80%   [EVAL] batch:   71 | acc: 56.25%,  total acc: 58.77%   [EVAL] batch:   72 | acc: 75.00%,  total acc: 58.99%   [EVAL] batch:   73 | acc: 56.25%,  total acc: 58.95%   [EVAL] batch:   74 | acc: 68.75%,  total acc: 59.08%   [EVAL] batch:   75 | acc: 75.00%,  total acc: 59.29%   [EVAL] batch:   76 | acc: 50.00%,  total acc: 59.17%   [EVAL] batch:   77 | acc: 56.25%,  total acc: 59.13%   [EVAL] batch:   78 | acc: 0.00%,  total acc: 58.39%   [EVAL] batch:   79 | acc: 0.00%,  total acc: 57.66%   [EVAL] batch:   80 | acc: 0.00%,  total acc: 56.94%   [EVAL] batch:   81 | acc: 56.25%,  total acc: 56.94%   [EVAL] batch:   82 | acc: 62.50%,  total acc: 57.00%   [EVAL] batch:   83 | acc: 62.50%,  total acc: 57.07%   [EVAL] batch:   84 | acc: 43.75%,  total acc: 56.91%   [EVAL] batch:   85 | acc: 50.00%,  total acc: 56.83%   [EVAL] batch:   86 | acc: 62.50%,  total acc: 56.90%   [EVAL] batch:   87 | acc: 75.00%,  total acc: 57.10%   [EVAL] batch:   88 | acc: 56.25%,  total acc: 57.09%   [EVAL] batch:   89 | acc: 50.00%,  total acc: 57.01%   [EVAL] batch:   90 | acc: 81.25%,  total acc: 57.28%   [EVAL] batch:   91 | acc: 93.75%,  total acc: 57.68%   [EVAL] batch:   92 | acc: 75.00%,  total acc: 57.86%   [EVAL] batch:   93 | acc: 93.75%,  total acc: 58.24%   [EVAL] batch:   94 | acc: 68.75%,  total acc: 58.36%   [EVAL] batch:   95 | acc: 87.50%,  total acc: 58.66%   [EVAL] batch:   96 | acc: 18.75%,  total acc: 58.25%   [EVAL] batch:   97 | acc: 31.25%,  total acc: 57.97%   [EVAL] batch:   98 | acc: 25.00%,  total acc: 57.64%   [EVAL] batch:   99 | acc: 18.75%,  total acc: 57.25%   [EVAL] batch:  100 | acc: 6.25%,  total acc: 56.75%   [EVAL] batch:  101 | acc: 37.50%,  total acc: 56.56%   [EVAL] batch:  102 | acc: 18.75%,  total acc: 56.19%   [EVAL] batch:  103 | acc: 62.50%,  total acc: 56.25%   [EVAL] batch:  104 | acc: 81.25%,  total acc: 56.49%   [EVAL] batch:  105 | acc: 56.25%,  total acc: 56.49%   [EVAL] batch:  106 | acc: 81.25%,  total acc: 56.72%   [EVAL] batch:  107 | acc: 68.75%,  total acc: 56.83%   [EVAL] batch:  108 | acc: 87.50%,  total acc: 57.11%   [EVAL] batch:  109 | acc: 87.50%,  total acc: 57.39%   [EVAL] batch:  110 | acc: 56.25%,  total acc: 57.38%   [EVAL] batch:  111 | acc: 62.50%,  total acc: 57.42%   [EVAL] batch:  112 | acc: 50.00%,  total acc: 57.36%   [EVAL] batch:  113 | acc: 81.25%,  total acc: 57.57%   [EVAL] batch:  114 | acc: 50.00%,  total acc: 57.50%   [EVAL] batch:  115 | acc: 6.25%,  total acc: 57.06%   [EVAL] batch:  116 | acc: 6.25%,  total acc: 56.62%   [EVAL] batch:  117 | acc: 6.25%,  total acc: 56.20%   [EVAL] batch:  118 | acc: 56.25%,  total acc: 56.20%   [EVAL] batch:  119 | acc: 62.50%,  total acc: 56.25%   [EVAL] batch:  120 | acc: 50.00%,  total acc: 56.20%   [EVAL] batch:  121 | acc: 56.25%,  total acc: 56.20%   [EVAL] batch:  122 | acc: 68.75%,  total acc: 56.30%   [EVAL] batch:  123 | acc: 81.25%,  total acc: 56.50%   [EVAL] batch:  124 | acc: 93.75%,  total acc: 56.80%   [EVAL] batch:  125 | acc: 100.00%,  total acc: 57.14%   [EVAL] batch:  126 | acc: 68.75%,  total acc: 57.23%   [EVAL] batch:  127 | acc: 87.50%,  total acc: 57.47%   [EVAL] batch:  128 | acc: 50.00%,  total acc: 57.41%   [EVAL] batch:  129 | acc: 43.75%,  total acc: 57.31%   [EVAL] batch:  130 | acc: 50.00%,  total acc: 57.25%   [EVAL] batch:  131 | acc: 25.00%,  total acc: 57.01%   [EVAL] batch:  132 | acc: 25.00%,  total acc: 56.77%   
cur_acc:  ['0.8636', '0.3750', '0.9241', '0.8750', '0.6779', '0.8304', '0.5455', '0.6125']
his_acc:  ['0.8636', '0.7281', '0.7546', '0.7835', '0.6756', '0.6682', '0.6161', '0.5677']
--------Round  4
seed:  500
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/test.pkl
Task_order: [7 5 6 4 2 1 3 0]
prepared data!
CurrentTrain: epoch  0, batch     0 | loss: 13.4019890CurrentTrain: epoch  0, batch     1 | loss: 13.0827122CurrentTrain: epoch  0, batch     2 | loss: 12.9882603CurrentTrain: epoch  0, batch     3 | loss: 13.2246170CurrentTrain: epoch  0, batch     4 | loss: 12.7392473CurrentTrain: epoch  0, batch     5 | loss: 12.8655500CurrentTrain: epoch  0, batch     6 | loss: 12.6493101CurrentTrain: epoch  0, batch     7 | loss: 12.5278740CurrentTrain: epoch  0, batch     8 | loss: 12.2578068CurrentTrain: epoch  0, batch     9 | loss: 12.3140392CurrentTrain: epoch  0, batch    10 | loss: 12.0688591CurrentTrain: epoch  0, batch    11 | loss: 12.0963440CurrentTrain: epoch  0, batch    12 | loss: 12.0753641CurrentTrain: epoch  0, batch    13 | loss: 11.9682636CurrentTrain: epoch  0, batch    14 | loss: 11.9490137CurrentTrain: epoch  0, batch    15 | loss: 11.8067188CurrentTrain: epoch  0, batch    16 | loss: 11.5885601CurrentTrain: epoch  0, batch    17 | loss: 11.5466986CurrentTrain: epoch  0, batch    18 | loss: 11.4556866CurrentTrain: epoch  0, batch    19 | loss: 11.3058758CurrentTrain: epoch  0, batch    20 | loss: 11.3782234CurrentTrain: epoch  0, batch    21 | loss: 11.2084446CurrentTrain: epoch  0, batch    22 | loss: 11.1774120CurrentTrain: epoch  0, batch    23 | loss: 11.3950357CurrentTrain: epoch  0, batch    24 | loss: 11.1142492CurrentTrain: epoch  0, batch    25 | loss: 11.0981684CurrentTrain: epoch  0, batch    26 | loss: 10.6763000CurrentTrain: epoch  0, batch    27 | loss: 10.6721859CurrentTrain: epoch  0, batch    28 | loss: 10.8175106CurrentTrain: epoch  0, batch    29 | loss: 10.6007147CurrentTrain: epoch  0, batch    30 | loss: 10.6792660CurrentTrain: epoch  0, batch    31 | loss: 10.9762096CurrentTrain: epoch  0, batch    32 | loss: 10.2608557CurrentTrain: epoch  0, batch    33 | loss: 10.7019863CurrentTrain: epoch  0, batch    34 | loss: 10.2991571CurrentTrain: epoch  0, batch    35 | loss: 10.1644316CurrentTrain: epoch  0, batch    36 | loss: 10.4940157CurrentTrain: epoch  0, batch    37 | loss: 9.6853981CurrentTrain: epoch  1, batch     0 | loss: 10.3469391CurrentTrain: epoch  1, batch     1 | loss: 10.2650127CurrentTrain: epoch  1, batch     2 | loss: 10.2819071CurrentTrain: epoch  1, batch     3 | loss: 9.3556995CurrentTrain: epoch  1, batch     4 | loss: 9.5773716CurrentTrain: epoch  1, batch     5 | loss: 10.0088253CurrentTrain: epoch  1, batch     6 | loss: 10.0587654CurrentTrain: epoch  1, batch     7 | loss: 9.8503513CurrentTrain: epoch  1, batch     8 | loss: 9.2439137CurrentTrain: epoch  1, batch     9 | loss: 10.0887184CurrentTrain: epoch  1, batch    10 | loss: 10.6581192CurrentTrain: epoch  1, batch    11 | loss: 10.0456352CurrentTrain: epoch  1, batch    12 | loss: 9.6615667CurrentTrain: epoch  1, batch    13 | loss: 9.8576918CurrentTrain: epoch  1, batch    14 | loss: 9.1787434CurrentTrain: epoch  1, batch    15 | loss: 9.4571419CurrentTrain: epoch  1, batch    16 | loss: 9.3478003CurrentTrain: epoch  1, batch    17 | loss: 9.2411385CurrentTrain: epoch  1, batch    18 | loss: 9.3222160CurrentTrain: epoch  1, batch    19 | loss: 9.2675562CurrentTrain: epoch  1, batch    20 | loss: 9.5606632CurrentTrain: epoch  1, batch    21 | loss: 9.3292284CurrentTrain: epoch  1, batch    22 | loss: 9.2765017CurrentTrain: epoch  1, batch    23 | loss: 9.3692398CurrentTrain: epoch  1, batch    24 | loss: 9.5323668CurrentTrain: epoch  1, batch    25 | loss: 9.0339766CurrentTrain: epoch  1, batch    26 | loss: 8.1991692CurrentTrain: epoch  1, batch    27 | loss: 9.4317322CurrentTrain: epoch  1, batch    28 | loss: 9.0229378CurrentTrain: epoch  1, batch    29 | loss: 8.2617912CurrentTrain: epoch  1, batch    30 | loss: 9.3089771CurrentTrain: epoch  1, batch    31 | loss: 8.8577843CurrentTrain: epoch  1, batch    32 | loss: 9.4453011CurrentTrain: epoch  1, batch    33 | loss: 9.5014305CurrentTrain: epoch  1, batch    34 | loss: 8.1549664CurrentTrain: epoch  1, batch    35 | loss: 8.2463245CurrentTrain: epoch  1, batch    36 | loss: 8.7876587CurrentTrain: epoch  1, batch    37 | loss: 8.6026831CurrentTrain: epoch  2, batch     0 | loss: 8.1535273CurrentTrain: epoch  2, batch     1 | loss: 8.1066952CurrentTrain: epoch  2, batch     2 | loss: 8.2043037CurrentTrain: epoch  2, batch     3 | loss: 8.2739983CurrentTrain: epoch  2, batch     4 | loss: 8.1644783CurrentTrain: epoch  2, batch     5 | loss: 8.6390057CurrentTrain: epoch  2, batch     6 | loss: 9.2300282CurrentTrain: epoch  2, batch     7 | loss: 8.2383709CurrentTrain: epoch  2, batch     8 | loss: 8.6082001CurrentTrain: epoch  2, batch     9 | loss: 7.9222364CurrentTrain: epoch  2, batch    10 | loss: 8.5692053CurrentTrain: epoch  2, batch    11 | loss: 8.4907188CurrentTrain: epoch  2, batch    12 | loss: 7.6994772CurrentTrain: epoch  2, batch    13 | loss: 7.8899002CurrentTrain: epoch  2, batch    14 | loss: 8.8318186CurrentTrain: epoch  2, batch    15 | loss: 8.1875496CurrentTrain: epoch  2, batch    16 | loss: 8.5859690CurrentTrain: epoch  2, batch    17 | loss: 7.9120727CurrentTrain: epoch  2, batch    18 | loss: 8.4923153CurrentTrain: epoch  2, batch    19 | loss: 8.1523018CurrentTrain: epoch  2, batch    20 | loss: 7.7898059CurrentTrain: epoch  2, batch    21 | loss: 7.2716331CurrentTrain: epoch  2, batch    22 | loss: 7.6701088CurrentTrain: epoch  2, batch    23 | loss: 7.9315748CurrentTrain: epoch  2, batch    24 | loss: 7.2235126CurrentTrain: epoch  2, batch    25 | loss: 7.8275986CurrentTrain: epoch  2, batch    26 | loss: 7.9974546CurrentTrain: epoch  2, batch    27 | loss: 7.2852879CurrentTrain: epoch  2, batch    28 | loss: 7.4326801CurrentTrain: epoch  2, batch    29 | loss: 8.3191910CurrentTrain: epoch  2, batch    30 | loss: 8.4355755CurrentTrain: epoch  2, batch    31 | loss: 7.0872779CurrentTrain: epoch  2, batch    32 | loss: 7.2563086CurrentTrain: epoch  2, batch    33 | loss: 8.2587299CurrentTrain: epoch  2, batch    34 | loss: 8.8634605CurrentTrain: epoch  2, batch    35 | loss: 7.3567157CurrentTrain: epoch  2, batch    36 | loss: 7.9365358CurrentTrain: epoch  2, batch    37 | loss: 7.9394999CurrentTrain: epoch  3, batch     0 | loss: 7.4126778CurrentTrain: epoch  3, batch     1 | loss: 7.7536068CurrentTrain: epoch  3, batch     2 | loss: 8.2285175CurrentTrain: epoch  3, batch     3 | loss: 7.9057231CurrentTrain: epoch  3, batch     4 | loss: 6.7182088CurrentTrain: epoch  3, batch     5 | loss: 7.8840647CurrentTrain: epoch  3, batch     6 | loss: 7.8364034CurrentTrain: epoch  3, batch     7 | loss: 8.1186562CurrentTrain: epoch  3, batch     8 | loss: 6.8608274CurrentTrain: epoch  3, batch     9 | loss: 7.4172153CurrentTrain: epoch  3, batch    10 | loss: 7.8042622CurrentTrain: epoch  3, batch    11 | loss: 6.9313798CurrentTrain: epoch  3, batch    12 | loss: 7.2859406CurrentTrain: epoch  3, batch    13 | loss: 7.5289230CurrentTrain: epoch  3, batch    14 | loss: 6.8452444CurrentTrain: epoch  3, batch    15 | loss: 7.4403839CurrentTrain: epoch  3, batch    16 | loss: 7.0662456CurrentTrain: epoch  3, batch    17 | loss: 6.5590749CurrentTrain: epoch  3, batch    18 | loss: 7.4119692CurrentTrain: epoch  3, batch    19 | loss: 7.7860618CurrentTrain: epoch  3, batch    20 | loss: 7.0560379CurrentTrain: epoch  3, batch    21 | loss: 7.3197231CurrentTrain: epoch  3, batch    22 | loss: 6.0950150CurrentTrain: epoch  3, batch    23 | loss: 8.2481947CurrentTrain: epoch  3, batch    24 | loss: 7.6680670CurrentTrain: epoch  3, batch    25 | loss: 7.5862398CurrentTrain: epoch  3, batch    26 | loss: 7.0689135CurrentTrain: epoch  3, batch    27 | loss: 6.6976767CurrentTrain: epoch  3, batch    28 | loss: 7.3408871CurrentTrain: epoch  3, batch    29 | loss: 7.2192650CurrentTrain: epoch  3, batch    30 | loss: 7.1817703CurrentTrain: epoch  3, batch    31 | loss: 7.3887873CurrentTrain: epoch  3, batch    32 | loss: 6.6555586CurrentTrain: epoch  3, batch    33 | loss: 7.8802123CurrentTrain: epoch  3, batch    34 | loss: 7.0269723CurrentTrain: epoch  3, batch    35 | loss: 6.4596443CurrentTrain: epoch  3, batch    36 | loss: 7.2215023CurrentTrain: epoch  3, batch    37 | loss: 7.9262638CurrentTrain: epoch  4, batch     0 | loss: 7.0005856CurrentTrain: epoch  4, batch     1 | loss: 6.8991165CurrentTrain: epoch  4, batch     2 | loss: 6.4042253CurrentTrain: epoch  4, batch     3 | loss: 6.5254498CurrentTrain: epoch  4, batch     4 | loss: 6.9704676CurrentTrain: epoch  4, batch     5 | loss: 6.8684568CurrentTrain: epoch  4, batch     6 | loss: 8.4162140CurrentTrain: epoch  4, batch     7 | loss: 7.2850733CurrentTrain: epoch  4, batch     8 | loss: 7.6255054CurrentTrain: epoch  4, batch     9 | loss: 7.0074530CurrentTrain: epoch  4, batch    10 | loss: 6.8118863CurrentTrain: epoch  4, batch    11 | loss: 6.5833874CurrentTrain: epoch  4, batch    12 | loss: 7.5157537CurrentTrain: epoch  4, batch    13 | loss: 7.1056666CurrentTrain: epoch  4, batch    14 | loss: 6.4267769CurrentTrain: epoch  4, batch    15 | loss: 7.6033144CurrentTrain: epoch  4, batch    16 | loss: 7.8608651CurrentTrain: epoch  4, batch    17 | loss: 6.6038156CurrentTrain: epoch  4, batch    18 | loss: 6.9142818CurrentTrain: epoch  4, batch    19 | loss: 6.4965487CurrentTrain: epoch  4, batch    20 | loss: 6.6370296CurrentTrain: epoch  4, batch    21 | loss: 6.7117519CurrentTrain: epoch  4, batch    22 | loss: 6.9724832CurrentTrain: epoch  4, batch    23 | loss: 7.2385092CurrentTrain: epoch  4, batch    24 | loss: 6.7313881CurrentTrain: epoch  4, batch    25 | loss: 6.6695976CurrentTrain: epoch  4, batch    26 | loss: 6.6950970CurrentTrain: epoch  4, batch    27 | loss: 6.7519293CurrentTrain: epoch  4, batch    28 | loss: 6.7687798CurrentTrain: epoch  4, batch    29 | loss: 7.9078565CurrentTrain: epoch  4, batch    30 | loss: 6.9642482CurrentTrain: epoch  4, batch    31 | loss: 6.9400835CurrentTrain: epoch  4, batch    32 | loss: 6.2329454CurrentTrain: epoch  4, batch    33 | loss: 6.7720571CurrentTrain: epoch  4, batch    34 | loss: 5.6632271CurrentTrain: epoch  4, batch    35 | loss: 7.3702717CurrentTrain: epoch  4, batch    36 | loss: 6.0559335CurrentTrain: epoch  4, batch    37 | loss: 7.1051226CurrentTrain: epoch  5, batch     0 | loss: 6.7273960CurrentTrain: epoch  5, batch     1 | loss: 6.5923615CurrentTrain: epoch  5, batch     2 | loss: 6.3600411CurrentTrain: epoch  5, batch     3 | loss: 6.2589111CurrentTrain: epoch  5, batch     4 | loss: 7.2720919CurrentTrain: epoch  5, batch     5 | loss: 6.6753435CurrentTrain: epoch  5, batch     6 | loss: 6.9608302CurrentTrain: epoch  5, batch     7 | loss: 7.1728115CurrentTrain: epoch  5, batch     8 | loss: 6.6586637CurrentTrain: epoch  5, batch     9 | loss: 6.3482094CurrentTrain: epoch  5, batch    10 | loss: 6.1632414CurrentTrain: epoch  5, batch    11 | loss: 6.1507578CurrentTrain: epoch  5, batch    12 | loss: 6.4933720CurrentTrain: epoch  5, batch    13 | loss: 6.3792486CurrentTrain: epoch  5, batch    14 | loss: 6.4897251CurrentTrain: epoch  5, batch    15 | loss: 6.3984480CurrentTrain: epoch  5, batch    16 | loss: 6.3145223CurrentTrain: epoch  5, batch    17 | loss: 5.9582253CurrentTrain: epoch  5, batch    18 | loss: 6.6726866CurrentTrain: epoch  5, batch    19 | loss: 6.3279357CurrentTrain: epoch  5, batch    20 | loss: 6.8400507CurrentTrain: epoch  5, batch    21 | loss: 6.1683965CurrentTrain: epoch  5, batch    22 | loss: 5.4058785CurrentTrain: epoch  5, batch    23 | loss: 7.1180134CurrentTrain: epoch  5, batch    24 | loss: 5.9998074CurrentTrain: epoch  5, batch    25 | loss: 5.9732008CurrentTrain: epoch  5, batch    26 | loss: 6.7497501CurrentTrain: epoch  5, batch    27 | loss: 7.5040073CurrentTrain: epoch  5, batch    28 | loss: 7.7486196CurrentTrain: epoch  5, batch    29 | loss: 6.6440010CurrentTrain: epoch  5, batch    30 | loss: 6.8315015CurrentTrain: epoch  5, batch    31 | loss: 6.5471611CurrentTrain: epoch  5, batch    32 | loss: 5.7828088CurrentTrain: epoch  5, batch    33 | loss: 5.8116951CurrentTrain: epoch  5, batch    34 | loss: 6.2330236CurrentTrain: epoch  5, batch    35 | loss: 6.6195793CurrentTrain: epoch  5, batch    36 | loss: 6.5642519CurrentTrain: epoch  5, batch    37 | loss: 7.4338098CurrentTrain: epoch  6, batch     0 | loss: 6.5482264CurrentTrain: epoch  6, batch     1 | loss: 5.6440425CurrentTrain: epoch  6, batch     2 | loss: 6.4900370CurrentTrain: epoch  6, batch     3 | loss: 6.4631882CurrentTrain: epoch  6, batch     4 | loss: 6.1257548CurrentTrain: epoch  6, batch     5 | loss: 5.9590883CurrentTrain: epoch  6, batch     6 | loss: 6.0907803CurrentTrain: epoch  6, batch     7 | loss: 6.0411110CurrentTrain: epoch  6, batch     8 | loss: 6.1296434CurrentTrain: epoch  6, batch     9 | loss: 6.6790252CurrentTrain: epoch  6, batch    10 | loss: 5.5909543CurrentTrain: epoch  6, batch    11 | loss: 6.0385413CurrentTrain: epoch  6, batch    12 | loss: 5.4790945CurrentTrain: epoch  6, batch    13 | loss: 6.1514263CurrentTrain: epoch  6, batch    14 | loss: 6.1460285CurrentTrain: epoch  6, batch    15 | loss: 6.3636808CurrentTrain: epoch  6, batch    16 | loss: 6.3321300CurrentTrain: epoch  6, batch    17 | loss: 6.2772126CurrentTrain: epoch  6, batch    18 | loss: 6.0779209CurrentTrain: epoch  6, batch    19 | loss: 6.3173170CurrentTrain: epoch  6, batch    20 | loss: 5.8679733CurrentTrain: epoch  6, batch    21 | loss: 6.0393286CurrentTrain: epoch  6, batch    22 | loss: 5.5706677CurrentTrain: epoch  6, batch    23 | loss: 6.1163411CurrentTrain: epoch  6, batch    24 | loss: 5.9603791CurrentTrain: epoch  6, batch    25 | loss: 6.0858941CurrentTrain: epoch  6, batch    26 | loss: 5.8152914CurrentTrain: epoch  6, batch    27 | loss: 6.1199379CurrentTrain: epoch  6, batch    28 | loss: 5.9069843CurrentTrain: epoch  6, batch    29 | loss: 6.3247700CurrentTrain: epoch  6, batch    30 | loss: 5.9039502CurrentTrain: epoch  6, batch    31 | loss: 5.8615246CurrentTrain: epoch  6, batch    32 | loss: 6.3490295CurrentTrain: epoch  6, batch    33 | loss: 5.7250094CurrentTrain: epoch  6, batch    34 | loss: 6.1135869CurrentTrain: epoch  6, batch    35 | loss: 5.7505617CurrentTrain: epoch  6, batch    36 | loss: 6.3465528CurrentTrain: epoch  6, batch    37 | loss: 6.0750132CurrentTrain: epoch  7, batch     0 | loss: 5.9105616CurrentTrain: epoch  7, batch     1 | loss: 6.3183432CurrentTrain: epoch  7, batch     2 | loss: 5.5576735CurrentTrain: epoch  7, batch     3 | loss: 5.7737980CurrentTrain: epoch  7, batch     4 | loss: 6.0242672CurrentTrain: epoch  7, batch     5 | loss: 6.5256901CurrentTrain: epoch  7, batch     6 | loss: 5.5657034CurrentTrain: epoch  7, batch     7 | loss: 6.1147566CurrentTrain: epoch  7, batch     8 | loss: 5.5313969CurrentTrain: epoch  7, batch     9 | loss: 5.7586670CurrentTrain: epoch  7, batch    10 | loss: 5.7903919CurrentTrain: epoch  7, batch    11 | loss: 5.2445936CurrentTrain: epoch  7, batch    12 | loss: 5.5876517CurrentTrain: epoch  7, batch    13 | loss: 5.2334919CurrentTrain: epoch  7, batch    14 | loss: 5.7190342CurrentTrain: epoch  7, batch    15 | loss: 5.9632568CurrentTrain: epoch  7, batch    16 | loss: 6.2123804CurrentTrain: epoch  7, batch    17 | loss: 5.9164553CurrentTrain: epoch  7, batch    18 | loss: 5.4126058CurrentTrain: epoch  7, batch    19 | loss: 5.6118059CurrentTrain: epoch  7, batch    20 | loss: 5.4785242CurrentTrain: epoch  7, batch    21 | loss: 5.8719406CurrentTrain: epoch  7, batch    22 | loss: 5.7030907CurrentTrain: epoch  7, batch    23 | loss: 5.4166403CurrentTrain: epoch  7, batch    24 | loss: 6.0444689CurrentTrain: epoch  7, batch    25 | loss: 5.9076061CurrentTrain: epoch  7, batch    26 | loss: 5.2443795CurrentTrain: epoch  7, batch    27 | loss: 5.3820047CurrentTrain: epoch  7, batch    28 | loss: 6.4296403CurrentTrain: epoch  7, batch    29 | loss: 5.4219136CurrentTrain: epoch  7, batch    30 | loss: 5.3198662CurrentTrain: epoch  7, batch    31 | loss: 5.5793858CurrentTrain: epoch  7, batch    32 | loss: 5.4665084CurrentTrain: epoch  7, batch    33 | loss: 5.3541951CurrentTrain: epoch  7, batch    34 | loss: 5.3647661CurrentTrain: epoch  7, batch    35 | loss: 5.5517297CurrentTrain: epoch  7, batch    36 | loss: 5.4589214CurrentTrain: epoch  7, batch    37 | loss: 6.4131165CurrentTrain: epoch  8, batch     0 | loss: 5.6198521CurrentTrain: epoch  8, batch     1 | loss: 5.3000202CurrentTrain: epoch  8, batch     2 | loss: 5.5791302CurrentTrain: epoch  8, batch     3 | loss: 5.0230856CurrentTrain: epoch  8, batch     4 | loss: 5.4152589CurrentTrain: epoch  8, batch     5 | loss: 5.1903934CurrentTrain: epoch  8, batch     6 | loss: 5.5272579CurrentTrain: epoch  8, batch     7 | loss: 5.6745300CurrentTrain: epoch  8, batch     8 | loss: 5.2649698CurrentTrain: epoch  8, batch     9 | loss: 5.6030502CurrentTrain: epoch  8, batch    10 | loss: 5.4953785CurrentTrain: epoch  8, batch    11 | loss: 5.8452673CurrentTrain: epoch  8, batch    12 | loss: 5.0409822CurrentTrain: epoch  8, batch    13 | loss: 5.0895543CurrentTrain: epoch  8, batch    14 | loss: 5.0825210CurrentTrain: epoch  8, batch    15 | loss: 5.5179396CurrentTrain: epoch  8, batch    16 | loss: 5.4649930CurrentTrain: epoch  8, batch    17 | loss: 5.8188796CurrentTrain: epoch  8, batch    18 | loss: 5.3466954CurrentTrain: epoch  8, batch    19 | loss: 5.4015102CurrentTrain: epoch  8, batch    20 | loss: 5.5061913CurrentTrain: epoch  8, batch    21 | loss: 5.4135017CurrentTrain: epoch  8, batch    22 | loss: 5.7632256CurrentTrain: epoch  8, batch    23 | loss: 5.8368211CurrentTrain: epoch  8, batch    24 | loss: 5.5843143CurrentTrain: epoch  8, batch    25 | loss: 5.1716175CurrentTrain: epoch  8, batch    26 | loss: 5.2259789CurrentTrain: epoch  8, batch    27 | loss: 5.4649925CurrentTrain: epoch  8, batch    28 | loss: 5.1480694CurrentTrain: epoch  8, batch    29 | loss: 5.0890245CurrentTrain: epoch  8, batch    30 | loss: 6.1910057CurrentTrain: epoch  8, batch    31 | loss: 5.2369428CurrentTrain: epoch  8, batch    32 | loss: 5.5624146CurrentTrain: epoch  8, batch    33 | loss: 5.2851925CurrentTrain: epoch  8, batch    34 | loss: 5.2661085CurrentTrain: epoch  8, batch    35 | loss: 5.3177905CurrentTrain: epoch  8, batch    36 | loss: 5.7077456CurrentTrain: epoch  8, batch    37 | loss: 5.0890503CurrentTrain: epoch  9, batch     0 | loss: 5.3989868CurrentTrain: epoch  9, batch     1 | loss: 5.3264899CurrentTrain: epoch  9, batch     2 | loss: 5.0553708CurrentTrain: epoch  9, batch     3 | loss: 5.0131216CurrentTrain: epoch  9, batch     4 | loss: 5.4327259CurrentTrain: epoch  9, batch     5 | loss: 5.0642147CurrentTrain: epoch  9, batch     6 | loss: 5.2771177CurrentTrain: epoch  9, batch     7 | loss: 5.0388069CurrentTrain: epoch  9, batch     8 | loss: 5.8042245CurrentTrain: epoch  9, batch     9 | loss: 5.3090253CurrentTrain: epoch  9, batch    10 | loss: 4.9920263CurrentTrain: epoch  9, batch    11 | loss: 5.3048725CurrentTrain: epoch  9, batch    12 | loss: 4.9719057CurrentTrain: epoch  9, batch    13 | loss: 5.0643158CurrentTrain: epoch  9, batch    14 | loss: 5.0665479CurrentTrain: epoch  9, batch    15 | loss: 5.2301912CurrentTrain: epoch  9, batch    16 | loss: 4.9578943CurrentTrain: epoch  9, batch    17 | loss: 5.3337927CurrentTrain: epoch  9, batch    18 | loss: 5.1413684CurrentTrain: epoch  9, batch    19 | loss: 5.1629238CurrentTrain: epoch  9, batch    20 | loss: 5.0277052CurrentTrain: epoch  9, batch    21 | loss: 5.5086021CurrentTrain: epoch  9, batch    22 | loss: 5.0199037CurrentTrain: epoch  9, batch    23 | loss: 4.9995203CurrentTrain: epoch  9, batch    24 | loss: 4.9563255CurrentTrain: epoch  9, batch    25 | loss: 4.9689741CurrentTrain: epoch  9, batch    26 | loss: 5.1923866CurrentTrain: epoch  9, batch    27 | loss: 4.7800837CurrentTrain: epoch  9, batch    28 | loss: 5.0097685CurrentTrain: epoch  9, batch    29 | loss: 4.8617048CurrentTrain: epoch  9, batch    30 | loss: 4.8571782CurrentTrain: epoch  9, batch    31 | loss: 5.2471733CurrentTrain: epoch  9, batch    32 | loss: 4.9173923CurrentTrain: epoch  9, batch    33 | loss: 4.9250255CurrentTrain: epoch  9, batch    34 | loss: 5.0129099CurrentTrain: epoch  9, batch    35 | loss: 4.9180870CurrentTrain: epoch  9, batch    36 | loss: 4.8368664CurrentTrain: epoch  9, batch    37 | loss: 4.8675985
#### person countries of residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person countries of residence" means a person is living in a country.
Here is an example:
Relation: person countries of residence
Context: head of the atomic energy organization of iran -lrb- aeoi -rrb- ali akbar salehi said thursday that iran has proposed russia to establish a joint nuclear consortium to supply fuel for the bushehr plant and plants to be built in iran , the english language satellite press tv reported .
Head Entity: ali akbar salehi
Tail Entity: iran
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person countries of residence":
Relation: person countries of residence  
Context: After years of living in the United States, the renowned artist Frida Kahlo returned to Mexico, where she found inspiration in her homeland's vibrant culture and landscapes.  
Head Entity: Frida Kahlo  
Tail Entity: Mexico  

Relation: person countries of residence  
Context: Following his successful career in the tech industry, Elon Musk decided to move to Texas, where he could enjoy a more relaxed lifestyle and focus on his various ventures.  
Head Entity: Elon Musk  
Tail Entity: Texas  
#### organization top members employees ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization top members employees" means an organization 's employer is a person.
Here is an example:
Relation: organization top members employees
Context: kao charng , vice chairman of the cabinet-level mainland affairs council -lrb- mac -rrb- , explained that the termination clause was added to give both sides an option after the implementation of the tariff-cutting economic cooperation framework agreement -lrb- ecfa -rrb- .
Head Entity: mainland affairs council
Tail Entity: kao charng
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization top members employees":
Relation: organization top members employees  
Context: elon musk, the CEO of spacex, announced the successful launch of the latest satellite into orbit, marking another milestone for the company.  
Head Entity: spacex  
Tail Entity: elon musk  

Relation: organization top members employees  
Context: sheryl sandberg, the chief operating officer of facebook, spoke at the conference about the importance of leadership in the tech industry.  
Head Entity: facebook  
Tail Entity: sheryl sandberg  
#### organization member of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization member of" means an organization is the member of an organization.
Here is an example:
Relation: organization member of
Context: south africa 1-5 leyds cnr biccard streets cosatu 's concern has been that the scorpions ' existence as a branch of the national directorate of public prosecutions blurred the necessary separation of functions between those investigating crime and those prosecuting the criminals .
Head Entity: scorpions
Tail Entity: national directorate of public prosecutions
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization member of":
Relation: organization member of  
Context: The International Federation of Red Cross and Red Crescent Societies (IFRC) is a global humanitarian organization that coordinates the activities of national Red Cross and Red Crescent societies around the world.  
Head Entity: International Federation of Red Cross and Red Crescent Societies  
Tail Entity: national Red Cross and Red Crescent societies  

Relation: organization member of  
Context: The European Union is a political and economic union of member states that are located primarily in Europe, working together to promote peace and stability in the region.  
Head Entity: European Union  
Tail Entity: member states  
#### person origin ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person origin" means a person 's nationality is a country.
Here is an example:
Relation: person origin
Context: -- pascal yoadimnadji , 56 , the chadian prime minister , following a brain hemorrhage in paris .
Head Entity: pascal yoadimnadji
Tail Entity: chadian
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person origin":
Relation: person origin  
Context: -- emma watson, known for her role in harry potter, was born in paris, france.  
Head Entity: emma watson  
Tail Entity: french  

Relation: person origin  
Context: -- akira kurosawa, a legendary filmmaker, hailed from the land of the rising sun, japan.  
Head Entity: akira kurosawa  
Tail Entity: japanese  
#### person title ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person title" means a person 's title is a title.
Here is an example:
Relation: person title
Context: earlier , in jerusalem , he spoke at the state funeral for the city 's fabled former mayor , teddy kollek , who died tuesday at 95 and was buried in the area of the mount herzl cemetery reserved for israel 's leaders .
Head Entity: teddy kollek
Tail Entity: mayor
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person title":
Relation: person title  
Context: After years of dedicated service, the renowned scientist was honored with the title of Chief Research Officer at the prestigious institute.  
Head Entity: renowned scientist  
Tail Entity: Chief Research Officer  

Relation: person title  
Context: During the award ceremony, the actress received the title of Best Actress for her outstanding performance in the film.  
Head Entity: actress  
Tail Entity: Best Actress  
#### organization country of headquarters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization country of headquarters" means an organization is located in a country.
Here is an example:
Relation: organization country of headquarters
Context: commander viliame naupoto , chairman of the fiji pine limited announced the woodchips exports target here tuesday after signing a woodchip sale agreement with japan 's itochu corporation .
Head Entity: itochu corporation
Tail Entity: japan
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization country of headquarters":
Relation: organization country of headquarters  
Context: the headquarters of the multinational technology company apple inc. is located in cupertino, california, where it has been a significant player in the tech industry.  
Head Entity: apple inc.  
Tail Entity: california  

Relation: organization country of headquarters  
Context: the famous automobile manufacturer toyota motor corporation has its main office in toyota city, a key location in japan's automotive sector.  
Head Entity: toyota motor corporation  
Tail Entity: japan  
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 79.69%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 78.75%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 78.12%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 80.36%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 82.81%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 84.72%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 85.62%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 86.93%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 87.98%   [EVAL] batch:   13 | acc: 75.00%,  total acc: 87.05%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 86.25%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 84.38%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 83.82%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 82.64%   [EVAL] batch:   18 | acc: 87.50%,  total acc: 82.89%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 83.12%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 83.93%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 84.66%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 85.33%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 85.68%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 86.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 86.78%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 87.04%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 87.93%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 87.71%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 87.70%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 88.09%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 86.55%   
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 79.69%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 78.75%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 78.12%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 80.36%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 82.81%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 84.72%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 85.62%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 86.93%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 87.98%   [EVAL] batch:   13 | acc: 75.00%,  total acc: 87.05%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 86.25%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 84.38%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 83.82%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 82.64%   [EVAL] batch:   18 | acc: 87.50%,  total acc: 82.89%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 83.12%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 83.93%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 84.66%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 85.33%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 85.68%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 86.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 86.78%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 87.04%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 87.93%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 87.71%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 87.70%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 88.09%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 86.55%   
cur_acc:  ['0.8655']
his_acc:  ['0.8655']
CurrentTrain: epoch  0, batch     0 | loss: 5.2135973CurrentTrain: epoch  0, batch     1 | loss: 5.1405101CurrentTrain: epoch  1, batch     0 | loss: 4.3200312CurrentTrain: epoch  1, batch     1 | loss: 4.3572984CurrentTrain: epoch  2, batch     0 | loss: 3.9464569CurrentTrain: epoch  2, batch     1 | loss: 3.6531296CurrentTrain: epoch  3, batch     0 | loss: 3.1660500CurrentTrain: epoch  3, batch     1 | loss: 3.5222533CurrentTrain: epoch  4, batch     0 | loss: 3.1344156CurrentTrain: epoch  4, batch     1 | loss: 2.6173923CurrentTrain: epoch  5, batch     0 | loss: 2.8870707CurrentTrain: epoch  5, batch     1 | loss: 2.7878952CurrentTrain: epoch  6, batch     0 | loss: 2.8682191CurrentTrain: epoch  6, batch     1 | loss: 2.7527862CurrentTrain: epoch  7, batch     0 | loss: 2.6293406CurrentTrain: epoch  7, batch     1 | loss: 2.9248278CurrentTrain: epoch  8, batch     0 | loss: 2.5076828CurrentTrain: epoch  8, batch     1 | loss: 2.3255572CurrentTrain: epoch  9, batch     0 | loss: 2.3969457CurrentTrain: epoch  9, batch     1 | loss: 2.2494938
#### person cause of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person cause of death" means a person was died of an event.
Here is an example:
Relation: person cause of death
Context: mr scheider had suffered from multiple myeloma for several years , and died of complications from a staph infection , his wife , brenda siemer , said .
Head Entity: mr scheider
Tail Entity: complications from a staph infection
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person cause of death":
Relation: person cause of death  
Context: after battling with lung cancer for over a year, john doe passed away peacefully in his sleep, surrounded by family.  
Head Entity: john doe  
Tail Entity: lung cancer  

Relation: person cause of death  
Context: the renowned scientist, dr. emily white, tragically lost her life due to a car accident while returning from a conference.  
Head Entity: dr. emily white  
Tail Entity: car accident  
#### organization political religious affiliation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization political religious affiliation" means an organization is the member of a religion.
Here is an example:
Relation: organization political religious affiliation
Context: the militia has also suffered from allegations of being involved in clashes with rival shiite militias , particularly the badr corp , the military wing linked to supreme iraqi islamic council , the party led by powerful politician abdel aziz al-hakim .
Head Entity: badr corp
Tail Entity: shiite
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization political religious affiliation":
Relation: organization political religious affiliation  
Context: The Christian Democratic Union has been a significant player in German politics, often aligning itself with various religious groups to promote policies that reflect its Christian values.  
Head Entity: Christian Democratic Union  
Tail Entity: Christian  

Relation: organization political religious affiliation  
Context: The Islamic Society of North America has been actively involved in advocating for Muslim rights and has formed alliances with political parties to ensure that the interests of the Muslim community are represented.  
Head Entity: Islamic Society of North America  
Tail Entity: Muslim  
#### organization stateorprovince of headquarters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization stateorprovince of headquarters" means an organization is located in a state or province.
Here is an example:
Relation: organization stateorprovince of headquarters
Context: based in armonk , new york , mbia insures $ 670 billion -lrb- euro452 .18 billion -rrb- in debt .
Head Entity: mbia
Tail Entity: new york
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization stateorprovince of headquarters":
Relation: organization stateorprovince of headquarters  
Context: the tech giant apple inc. has its headquarters in cupertino, california, where it develops innovative products.  
Head Entity: apple inc.  
Tail Entity: california  

Relation: organization stateorprovince of headquarters  
Context: the multinational corporation samsung electronics is headquartered in suwon, south korea, and is a leader in consumer electronics.  
Head Entity: samsung electronics  
Tail Entity: south korea  
#### person other family ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person other family" means a person 's relative is a person.
Here is an example:
Relation: person other family
Context: former american idol contestant , sanjaya malakar and hills starlet , holly montag , have been getting close since she replaced her sister and brother-in-law , heidi montag and spencer pratt , on i 'm a celebrity .
Head Entity: spencer pratt ,
Tail Entity: holly montag
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person other family":
Relation: person other family  
Context: in a recent interview, actress jennifer aniston spoke fondly of her brother, alex aniston, and their close bond, highlighting how family ties remain strong despite their busy careers.  
Head Entity: alex aniston  
Tail Entity: jennifer aniston  

Relation: person other family  
Context: during the family reunion, cousins emily and josh shared stories about their grandparents, emphasizing the importance of family connections and the legacy they left behind.  
Head Entity: josh  
Tail Entity: emily  
#### person city of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person city of death" means a person was died in a city.
Here is an example:
Relation: person city of death
Context: millender-mcdonald , who was 68 , died late saturday at her home in carson , california , said her chief of staff , bandele mcqueen .
Head Entity: millender-mcdonald
Tail Entity: carson
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person city of death":
Relation: person city of death  
Context: renowned author and activist, james baldwin, passed away in 1987 in the city of paris, france, where he had spent many years of his life.  
Head Entity: james baldwin  
Tail Entity: paris  

Relation: person city of death  
Context: the famous musician, freddie mercury, died in 1991 at his home in london, england, leaving behind a legacy of unforgettable music.  
Head Entity: freddie mercury  
Tail Entity: london  
Mixup data size:  1495
MixupTrain:  epoch  0, batch     0 | loss: 6.7322798MixupTrain:  epoch  0, batch     1 | loss: 6.4684286MixupTrain:  epoch  0, batch     2 | loss: 6.3615351MixupTrain:  epoch  0, batch     3 | loss: 6.1545143MixupTrain:  epoch  0, batch     4 | loss: 5.8730922MixupTrain:  epoch  0, batch     5 | loss: 5.6899958MixupTrain:  epoch  0, batch     6 | loss: 5.5795140MixupTrain:  epoch  0, batch     7 | loss: 5.4927063MixupTrain:  epoch  0, batch     8 | loss: 5.4705715MixupTrain:  epoch  0, batch     9 | loss: 5.4573970MixupTrain:  epoch  0, batch    10 | loss: 5.3460541MixupTrain:  epoch  0, batch    11 | loss: 5.3690891MixupTrain:  epoch  0, batch    12 | loss: 5.3459373MixupTrain:  epoch  0, batch    13 | loss: 5.3469324MixupTrain:  epoch  0, batch    14 | loss: 5.3209462MixupTrain:  epoch  0, batch    15 | loss: 5.2722955MixupTrain:  epoch  0, batch    16 | loss: 5.2394972MixupTrain:  epoch  0, batch    17 | loss: 5.2177253MixupTrain:  epoch  0, batch    18 | loss: 5.2303209MixupTrain:  epoch  0, batch    19 | loss: 5.2135277MixupTrain:  epoch  0, batch    20 | loss: 5.2151041MixupTrain:  epoch  0, batch    21 | loss: 5.1147518MixupTrain:  epoch  0, batch    22 | loss: 5.1130385MixupTrain:  epoch  0, batch    23 | loss: 5.0785294MixupTrain:  epoch  0, batch    24 | loss: 5.0795398MixupTrain:  epoch  0, batch    25 | loss: 5.0146265MixupTrain:  epoch  0, batch    26 | loss: 5.0709677MixupTrain:  epoch  0, batch    27 | loss: 5.0654259MixupTrain:  epoch  0, batch    28 | loss: 5.0137053MixupTrain:  epoch  0, batch    29 | loss: 4.9345226MixupTrain:  epoch  0, batch    30 | loss: 4.9441934MixupTrain:  epoch  0, batch    31 | loss: 4.8564463MixupTrain:  epoch  0, batch    32 | loss: 4.9092226MixupTrain:  epoch  0, batch    33 | loss: 4.9085755MixupTrain:  epoch  0, batch    34 | loss: 4.8290448MixupTrain:  epoch  0, batch    35 | loss: 4.8148737MixupTrain:  epoch  0, batch    36 | loss: 4.8287139MixupTrain:  epoch  0, batch    37 | loss: 4.7906961MixupTrain:  epoch  0, batch    38 | loss: 4.7656422MixupTrain:  epoch  0, batch    39 | loss: 4.7363672MixupTrain:  epoch  0, batch    40 | loss: 4.7638097MixupTrain:  epoch  0, batch    41 | loss: 4.7525754MixupTrain:  epoch  0, batch    42 | loss: 4.7117214MixupTrain:  epoch  0, batch    43 | loss: 4.6928072MixupTrain:  epoch  0, batch    44 | loss: 4.6536365MixupTrain:  epoch  0, batch    45 | loss: 4.6444273MixupTrain:  epoch  0, batch    46 | loss: 4.6121917MixupTrain:  epoch  0, batch    47 | loss: 4.6214561MixupTrain:  epoch  0, batch    48 | loss: 4.5822773MixupTrain:  epoch  0, batch    49 | loss: 4.6115851MixupTrain:  epoch  0, batch    50 | loss: 4.5564146MixupTrain:  epoch  0, batch    51 | loss: 4.5749607MixupTrain:  epoch  0, batch    52 | loss: 4.5295010MixupTrain:  epoch  0, batch    53 | loss: 4.5371146MixupTrain:  epoch  0, batch    54 | loss: 4.4758353MixupTrain:  epoch  0, batch    55 | loss: 4.4573717MixupTrain:  epoch  0, batch    56 | loss: 4.4858637MixupTrain:  epoch  0, batch    57 | loss: 4.4514394MixupTrain:  epoch  0, batch    58 | loss: 4.4495831MixupTrain:  epoch  0, batch    59 | loss: 4.4195147MixupTrain:  epoch  0, batch    60 | loss: 4.4079380MixupTrain:  epoch  0, batch    61 | loss: 4.4090071MixupTrain:  epoch  0, batch    62 | loss: 4.3760409MixupTrain:  epoch  0, batch    63 | loss: 4.3979249MixupTrain:  epoch  0, batch    64 | loss: 4.3992653MixupTrain:  epoch  0, batch    65 | loss: 4.3424892MixupTrain:  epoch  0, batch    66 | loss: 4.3148026MixupTrain:  epoch  0, batch    67 | loss: 4.3087964MixupTrain:  epoch  0, batch    68 | loss: 4.3183150MixupTrain:  epoch  0, batch    69 | loss: 4.3224726MixupTrain:  epoch  0, batch    70 | loss: 4.2935810MixupTrain:  epoch  0, batch    71 | loss: 4.3002062MixupTrain:  epoch  0, batch    72 | loss: 4.2803273MixupTrain:  epoch  0, batch    73 | loss: 4.2890429MixupTrain:  epoch  0, batch    74 | loss: 4.2448902MixupTrain:  epoch  0, batch    75 | loss: 4.2619963MixupTrain:  epoch  0, batch    76 | loss: 4.2261152MixupTrain:  epoch  0, batch    77 | loss: 4.2397060MixupTrain:  epoch  0, batch    78 | loss: 4.2120976MixupTrain:  epoch  0, batch    79 | loss: 4.2138281MixupTrain:  epoch  0, batch    80 | loss: 4.2234812MixupTrain:  epoch  0, batch    81 | loss: 4.1827722MixupTrain:  epoch  0, batch    82 | loss: 4.1740046MixupTrain:  epoch  0, batch    83 | loss: 4.1757860MixupTrain:  epoch  0, batch    84 | loss: 4.1753559MixupTrain:  epoch  0, batch    85 | loss: 4.1801929MixupTrain:  epoch  0, batch    86 | loss: 4.1573143MixupTrain:  epoch  0, batch    87 | loss: 4.1538239MixupTrain:  epoch  0, batch    88 | loss: 4.1293774MixupTrain:  epoch  0, batch    89 | loss: 4.1457729MixupTrain:  epoch  0, batch    90 | loss: 4.1534886MixupTrain:  epoch  0, batch    91 | loss: 4.1241541MixupTrain:  epoch  0, batch    92 | loss: 4.1392918MixupTrain:  epoch  0, batch    93 | loss: 4.1398802
MemoryTrain:  epoch  0, batch     0 | loss: 7.5521441MemoryTrain:  epoch  0, batch     1 | loss: 7.0485430MemoryTrain:  epoch  0, batch     2 | loss: 5.3113046MemoryTrain:  epoch  1, batch     0 | loss: 5.8316956MemoryTrain:  epoch  1, batch     1 | loss: 6.4291906MemoryTrain:  epoch  1, batch     2 | loss: 6.6287255MemoryTrain:  epoch  2, batch     0 | loss: 5.0534563MemoryTrain:  epoch  2, batch     1 | loss: 4.3570614MemoryTrain:  epoch  2, batch     2 | loss: 3.4235103MemoryTrain:  epoch  3, batch     0 | loss: 3.3328266MemoryTrain:  epoch  3, batch     1 | loss: 4.4488726MemoryTrain:  epoch  3, batch     2 | loss: 5.6253700MemoryTrain:  epoch  4, batch     0 | loss: 3.2096865MemoryTrain:  epoch  4, batch     1 | loss: 4.2105236MemoryTrain:  epoch  4, batch     2 | loss: 1.8024992MemoryTrain:  epoch  5, batch     0 | loss: 3.7925668MemoryTrain:  epoch  5, batch     1 | loss: 3.0972705MemoryTrain:  epoch  5, batch     2 | loss: 1.2540849MemoryTrain:  epoch  6, batch     0 | loss: 3.7229810MemoryTrain:  epoch  6, batch     1 | loss: 3.2353306MemoryTrain:  epoch  6, batch     2 | loss: 1.2722321MemoryTrain:  epoch  7, batch     0 | loss: 3.4015245MemoryTrain:  epoch  7, batch     1 | loss: 2.7215326MemoryTrain:  epoch  7, batch     2 | loss: 1.3349372MemoryTrain:  epoch  8, batch     0 | loss: 3.1819282MemoryTrain:  epoch  8, batch     1 | loss: 2.9096193MemoryTrain:  epoch  8, batch     2 | loss: 1.6247755MemoryTrain:  epoch  9, batch     0 | loss: 2.7976356MemoryTrain:  epoch  9, batch     1 | loss: 2.9883745MemoryTrain:  epoch  9, batch     2 | loss: 4.6705003
[EVAL] batch:    0 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    1 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 97.92%   [EVAL] batch:    3 | acc: 62.50%,  total acc: 89.06%   [EVAL] batch:    4 | acc: 37.50%,  total acc: 78.75%   [EVAL] batch:    5 | acc: 68.75%,  total acc: 77.08%   [EVAL] batch:    6 | acc: 50.00%,  total acc: 73.21%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 75.78%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 77.78%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 78.12%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 79.55%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 80.73%   [EVAL] batch:   12 | acc: 43.75%,  total acc: 77.88%   
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 78.12%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 77.50%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 77.08%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 78.57%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 83.33%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 85.80%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 86.46%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 86.54%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 85.27%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 84.58%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 82.81%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 82.35%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 81.25%   [EVAL] batch:   18 | acc: 75.00%,  total acc: 80.92%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 82.14%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 82.95%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 83.70%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 84.11%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 84.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 85.34%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 85.65%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 86.16%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 86.64%   [EVAL] batch:   29 | acc: 75.00%,  total acc: 86.25%   [EVAL] batch:   30 | acc: 81.25%,  total acc: 86.09%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 86.33%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 86.74%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 87.13%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 87.32%   [EVAL] batch:   35 | acc: 75.00%,  total acc: 86.98%   [EVAL] batch:   36 | acc: 43.75%,  total acc: 85.81%   [EVAL] batch:   37 | acc: 62.50%,  total acc: 85.20%   [EVAL] batch:   38 | acc: 50.00%,  total acc: 84.29%   [EVAL] batch:   39 | acc: 81.25%,  total acc: 84.22%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 84.60%   [EVAL] batch:   41 | acc: 81.25%,  total acc: 84.52%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 84.74%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 84.80%   [EVAL] batch:   44 | acc: 81.25%,  total acc: 84.72%   
cur_acc:  ['0.8655', '0.7788']
his_acc:  ['0.8655', '0.8472']
CurrentTrain: epoch  0, batch     0 | loss: 5.7519064CurrentTrain: epoch  0, batch     1 | loss: 7.4001703CurrentTrain: epoch  1, batch     0 | loss: 6.4699965CurrentTrain: epoch  1, batch     1 | loss: 4.9688544CurrentTrain: epoch  2, batch     0 | loss: 5.3126593CurrentTrain: epoch  2, batch     1 | loss: 5.4916339CurrentTrain: epoch  3, batch     0 | loss: 4.8298616CurrentTrain: epoch  3, batch     1 | loss: 4.2980247CurrentTrain: epoch  4, batch     0 | loss: 4.4337826CurrentTrain: epoch  4, batch     1 | loss: 3.7796605CurrentTrain: epoch  5, batch     0 | loss: 3.7812786CurrentTrain: epoch  5, batch     1 | loss: 4.2500067CurrentTrain: epoch  6, batch     0 | loss: 3.8234739CurrentTrain: epoch  6, batch     1 | loss: 3.8309395CurrentTrain: epoch  7, batch     0 | loss: 3.5981355CurrentTrain: epoch  7, batch     1 | loss: 3.2971427CurrentTrain: epoch  8, batch     0 | loss: 3.1291449CurrentTrain: epoch  8, batch     1 | loss: 3.5829043CurrentTrain: epoch  9, batch     0 | loss: 2.9336960CurrentTrain: epoch  9, batch     1 | loss: 3.7950330
#### person stateorprovinces of residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person stateorprovinces of residence" means a person is living in a state or province.
Here is an example:
Relation: person stateorprovinces of residence
Context: ny-schools-chief -lrb- new york -rrb- -- cathleen p. black , mayor michael r. bloomberg 's choice to be the next chancellor of new york city 's public-school system , has during more than 40 years in the media business broken numerous glass ceilings -- and amassed a fortune -- with quick and cold-blooded decision making , crystal-clear goal setting , and an all-surpassing attention to the bottom line .
Head Entity: cathleen p. black
Tail Entity: new york
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person stateorprovinces of residence":
Relation: person stateorprovinces of residence  
Context: after years of living in the bustling city, actor john doe has decided to settle down in the serene landscapes of oregon, where he can enjoy a quieter lifestyle away from the spotlight.  
Head Entity: john doe  
Tail Entity: oregon  

Relation: person stateorprovinces of residence  
Context: renowned author jane smith has recently moved to the vibrant city of san francisco, drawn by its rich culture and artistic community, which she believes will inspire her next novel.  
Head Entity: jane smith  
Tail Entity: san francisco  
#### person date of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person date of death" means a person was died on a date.
Here is an example:
Relation: person date of death
Context: he was taken off life support on feb. 14 .
Head Entity: he
Tail Entity: feb. 14
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person date of death":
Relation: person date of death  
Context: The famous author passed away on July 10, 2020, after a long illness.  
Head Entity: The famous author  
Tail Entity: July 10, 2020  

Relation: person date of death  
Context: She left this world on March 5, 2018, surrounded by her family.  
Head Entity: She  
Tail Entity: March 5, 2018  
#### organization number of employees members ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization number of employees members" means the number of an organization 's employees is a number.
Here is an example:
Relation: organization number of employees members
Context: with the sweep of a federal regulator 's pen , massachusetts stands to gain a new life-science giant in april : covidien , a medical - supplies maker with thousands of products and more than 43,000 employees worldwide .
Head Entity: covidien
Tail Entity: 43,000
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization number of employees members":
Relation: organization number of employees members  
Context: The tech company, Innovatech, has rapidly expanded its workforce over the past year, now boasting a total of 25,000 employees across its global offices.  
Head Entity: Innovatech  
Tail Entity: 25,000  

Relation: organization number of employees members  
Context: After the merger, the newly formed entity, Global Finance Corp, reported an impressive headcount of 15,000 employees, making it one of the largest firms in the financial sector.  
Head Entity: Global Finance Corp  
Tail Entity: 15,000  
#### person alternate names ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person alternate names" means a person 's alias is a person.
Here is an example:
Relation: person alternate names
Context: `` i am known in the hospice as the man who would n't die , '' buchwald wrote in march .
Head Entity: buchwald
Tail Entity: man who would n't die
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person alternate names":
Relation: person alternate names  
Context: `` the famous author often referred to as the bard of Avon, is known for his timeless plays and sonnets. ''  
Head Entity: William Shakespeare  
Tail Entity: bard of Avon  

Relation: person alternate names  
Context: `` during his career, he was affectionately called the king of pop, captivating millions with his music and dance moves. ''  
Head Entity: Michael Jackson  
Tail Entity: king of pop  
#### person spouse ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person spouse" means a person 's spouse is a person.
Here is an example:
Relation: person spouse
Context: beverly hills , california 2008-08-17 21:15:39 utc ------ there was much dancing : ellen degeneres and portia de rossi are married , according to reports .
Head Entity: ellen degeneres
Tail Entity: portia de rossi
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person spouse":
Relation: person spouse  
Context: in a lavish ceremony held in new york city, the couple exchanged vows in front of family and friends: john legend and chrissy teigen celebrated their love with a beautiful wedding.  
Head Entity: john legend  
Tail Entity: chrissy teigen  

Relation: person spouse  
Context: during the annual charity gala, it was announced that the famous actor and his long-time partner have tied the knot: ben affleck and jennifer garner are now officially married.  
Head Entity: ben affleck  
Tail Entity: jennifer garner  
Mixup data size:  2455
MixupTrain:  epoch  0, batch     0 | loss: 5.3060989MixupTrain:  epoch  0, batch     1 | loss: 4.9435849MixupTrain:  epoch  0, batch     2 | loss: 5.0149307MixupTrain:  epoch  0, batch     3 | loss: 4.2545877MixupTrain:  epoch  0, batch     4 | loss: 4.9259653MixupTrain:  epoch  0, batch     5 | loss: 4.4718838MixupTrain:  epoch  0, batch     6 | loss: 4.8095574MixupTrain:  epoch  0, batch     7 | loss: 4.9098234MixupTrain:  epoch  0, batch     8 | loss: 4.9519415MixupTrain:  epoch  0, batch     9 | loss: 4.9475937MixupTrain:  epoch  0, batch    10 | loss: 4.5701790MixupTrain:  epoch  0, batch    11 | loss: 4.5192966MixupTrain:  epoch  0, batch    12 | loss: 4.4187293MixupTrain:  epoch  0, batch    13 | loss: 4.8936591MixupTrain:  epoch  0, batch    14 | loss: 4.6475244MixupTrain:  epoch  0, batch    15 | loss: 4.9719696MixupTrain:  epoch  0, batch    16 | loss: 4.4956799MixupTrain:  epoch  0, batch    17 | loss: 4.2668352MixupTrain:  epoch  0, batch    18 | loss: 4.3699951MixupTrain:  epoch  0, batch    19 | loss: 4.6786823MixupTrain:  epoch  0, batch    20 | loss: 4.6994524MixupTrain:  epoch  0, batch    21 | loss: 4.4151840MixupTrain:  epoch  0, batch    22 | loss: 4.2547712MixupTrain:  epoch  0, batch    23 | loss: 4.0846672MixupTrain:  epoch  0, batch    24 | loss: 4.4272771MixupTrain:  epoch  0, batch    25 | loss: 4.3510180MixupTrain:  epoch  0, batch    26 | loss: 4.7293282MixupTrain:  epoch  0, batch    27 | loss: 4.5742283MixupTrain:  epoch  0, batch    28 | loss: 4.5240574MixupTrain:  epoch  0, batch    29 | loss: 4.3364611MixupTrain:  epoch  0, batch    30 | loss: 4.3568029MixupTrain:  epoch  0, batch    31 | loss: 4.4394779MixupTrain:  epoch  0, batch    32 | loss: 4.4808216MixupTrain:  epoch  0, batch    33 | loss: 4.5133476MixupTrain:  epoch  0, batch    34 | loss: 4.2601061MixupTrain:  epoch  0, batch    35 | loss: 4.3131485MixupTrain:  epoch  0, batch    36 | loss: 4.4021368MixupTrain:  epoch  0, batch    37 | loss: 4.2569427MixupTrain:  epoch  0, batch    38 | loss: 4.2585039MixupTrain:  epoch  0, batch    39 | loss: 4.5258098MixupTrain:  epoch  0, batch    40 | loss: 4.2889366MixupTrain:  epoch  0, batch    41 | loss: 4.5709190MixupTrain:  epoch  0, batch    42 | loss: 4.4070125MixupTrain:  epoch  0, batch    43 | loss: 4.3998151MixupTrain:  epoch  0, batch    44 | loss: 4.1324072MixupTrain:  epoch  0, batch    45 | loss: 4.3380795MixupTrain:  epoch  0, batch    46 | loss: 4.6447673MixupTrain:  epoch  0, batch    47 | loss: 3.9288547MixupTrain:  epoch  0, batch    48 | loss: 4.1514988MixupTrain:  epoch  0, batch    49 | loss: 4.1983738MixupTrain:  epoch  0, batch    50 | loss: 4.4135723MixupTrain:  epoch  0, batch    51 | loss: 4.4108353MixupTrain:  epoch  0, batch    52 | loss: 4.3781967MixupTrain:  epoch  0, batch    53 | loss: 4.3975668MixupTrain:  epoch  0, batch    54 | loss: 4.2665596MixupTrain:  epoch  0, batch    55 | loss: 4.2885880MixupTrain:  epoch  0, batch    56 | loss: 4.1345243MixupTrain:  epoch  0, batch    57 | loss: 4.3219991MixupTrain:  epoch  0, batch    58 | loss: 4.4781437MixupTrain:  epoch  0, batch    59 | loss: 3.8284085MixupTrain:  epoch  0, batch    60 | loss: 3.9169714MixupTrain:  epoch  0, batch    61 | loss: 4.0649424MixupTrain:  epoch  0, batch    62 | loss: 3.9682055MixupTrain:  epoch  0, batch    63 | loss: 4.1347332MixupTrain:  epoch  0, batch    64 | loss: 4.1641693MixupTrain:  epoch  0, batch    65 | loss: 3.9321284MixupTrain:  epoch  0, batch    66 | loss: 4.4217267MixupTrain:  epoch  0, batch    67 | loss: 3.8565283MixupTrain:  epoch  0, batch    68 | loss: 3.8142776MixupTrain:  epoch  0, batch    69 | loss: 4.0462799MixupTrain:  epoch  0, batch    70 | loss: 4.1164355MixupTrain:  epoch  0, batch    71 | loss: 4.2978458MixupTrain:  epoch  0, batch    72 | loss: 3.6170430MixupTrain:  epoch  0, batch    73 | loss: 3.8610020MixupTrain:  epoch  0, batch    74 | loss: 4.0366030MixupTrain:  epoch  0, batch    75 | loss: 3.6639545MixupTrain:  epoch  0, batch    76 | loss: 3.9421229MixupTrain:  epoch  0, batch    77 | loss: 3.9375067MixupTrain:  epoch  0, batch    78 | loss: 3.9225011MixupTrain:  epoch  0, batch    79 | loss: 3.8854718MixupTrain:  epoch  0, batch    80 | loss: 3.9962237MixupTrain:  epoch  0, batch    81 | loss: 3.6128118MixupTrain:  epoch  0, batch    82 | loss: 3.8374338MixupTrain:  epoch  0, batch    83 | loss: 3.7910466MixupTrain:  epoch  0, batch    84 | loss: 3.7741535MixupTrain:  epoch  0, batch    85 | loss: 3.9760237MixupTrain:  epoch  0, batch    86 | loss: 3.8559873MixupTrain:  epoch  0, batch    87 | loss: 3.9932394MixupTrain:  epoch  0, batch    88 | loss: 3.9899807MixupTrain:  epoch  0, batch    89 | loss: 3.7410631MixupTrain:  epoch  0, batch    90 | loss: 3.7972922MixupTrain:  epoch  0, batch    91 | loss: 3.9006214MixupTrain:  epoch  0, batch    92 | loss: 3.7901952MixupTrain:  epoch  0, batch    93 | loss: 3.6758065MixupTrain:  epoch  0, batch    94 | loss: 3.8363218MixupTrain:  epoch  0, batch    95 | loss: 4.0731015MixupTrain:  epoch  0, batch    96 | loss: 4.1049705MixupTrain:  epoch  0, batch    97 | loss: 3.7922764MixupTrain:  epoch  0, batch    98 | loss: 3.9712825MixupTrain:  epoch  0, batch    99 | loss: 3.6566200MixupTrain:  epoch  0, batch   100 | loss: 3.7050791MixupTrain:  epoch  0, batch   101 | loss: 3.9177513MixupTrain:  epoch  0, batch   102 | loss: 3.7063813MixupTrain:  epoch  0, batch   103 | loss: 3.9246974MixupTrain:  epoch  0, batch   104 | loss: 3.7803640MixupTrain:  epoch  0, batch   105 | loss: 3.8639483MixupTrain:  epoch  0, batch   106 | loss: 3.7423983MixupTrain:  epoch  0, batch   107 | loss: 3.9709716MixupTrain:  epoch  0, batch   108 | loss: 3.8785830MixupTrain:  epoch  0, batch   109 | loss: 3.7523739MixupTrain:  epoch  0, batch   110 | loss: 3.7629852MixupTrain:  epoch  0, batch   111 | loss: 3.7224722MixupTrain:  epoch  0, batch   112 | loss: 3.7432780MixupTrain:  epoch  0, batch   113 | loss: 3.8011065MixupTrain:  epoch  0, batch   114 | loss: 3.4833264MixupTrain:  epoch  0, batch   115 | loss: 3.7825172MixupTrain:  epoch  0, batch   116 | loss: 3.7927418MixupTrain:  epoch  0, batch   117 | loss: 3.8007817MixupTrain:  epoch  0, batch   118 | loss: 3.5637147MixupTrain:  epoch  0, batch   119 | loss: 4.0198369MixupTrain:  epoch  0, batch   120 | loss: 3.7957058MixupTrain:  epoch  0, batch   121 | loss: 3.9196107MixupTrain:  epoch  0, batch   122 | loss: 3.5494621MixupTrain:  epoch  0, batch   123 | loss: 3.8160291MixupTrain:  epoch  0, batch   124 | loss: 3.7480412MixupTrain:  epoch  0, batch   125 | loss: 3.6993876MixupTrain:  epoch  0, batch   126 | loss: 3.7189302MixupTrain:  epoch  0, batch   127 | loss: 3.8755546MixupTrain:  epoch  0, batch   128 | loss: 3.5417745MixupTrain:  epoch  0, batch   129 | loss: 3.7164190MixupTrain:  epoch  0, batch   130 | loss: 3.5438571MixupTrain:  epoch  0, batch   131 | loss: 3.8079064MixupTrain:  epoch  0, batch   132 | loss: 3.6266966MixupTrain:  epoch  0, batch   133 | loss: 3.6650357MixupTrain:  epoch  0, batch   134 | loss: 4.0041723MixupTrain:  epoch  0, batch   135 | loss: 3.5845802MixupTrain:  epoch  0, batch   136 | loss: 3.5557728MixupTrain:  epoch  0, batch   137 | loss: 3.6228480MixupTrain:  epoch  0, batch   138 | loss: 3.6708331MixupTrain:  epoch  0, batch   139 | loss: 3.6065423MixupTrain:  epoch  0, batch   140 | loss: 3.4207630MixupTrain:  epoch  0, batch   141 | loss: 3.4142334MixupTrain:  epoch  0, batch   142 | loss: 3.5479808MixupTrain:  epoch  0, batch   143 | loss: 3.7967734MixupTrain:  epoch  0, batch   144 | loss: 3.4100800MixupTrain:  epoch  0, batch   145 | loss: 3.8156135MixupTrain:  epoch  0, batch   146 | loss: 3.5127492MixupTrain:  epoch  0, batch   147 | loss: 3.8604994MixupTrain:  epoch  0, batch   148 | loss: 3.7125077MixupTrain:  epoch  0, batch   149 | loss: 3.4747262MixupTrain:  epoch  0, batch   150 | loss: 3.6566703MixupTrain:  epoch  0, batch   151 | loss: 3.4587212MixupTrain:  epoch  0, batch   152 | loss: 3.6351278MixupTrain:  epoch  0, batch   153 | loss: 4.0916924
MemoryTrain:  epoch  0, batch     0 | loss: 3.3618505MemoryTrain:  epoch  0, batch     1 | loss: 3.2031250MemoryTrain:  epoch  0, batch     2 | loss: 3.2257891MemoryTrain:  epoch  1, batch     0 | loss: 2.3894801MemoryTrain:  epoch  1, batch     1 | loss: 3.7409649MemoryTrain:  epoch  1, batch     2 | loss: 2.5682750MemoryTrain:  epoch  2, batch     0 | loss: 2.2267003MemoryTrain:  epoch  2, batch     1 | loss: 2.2028136MemoryTrain:  epoch  2, batch     2 | loss: 2.6740913MemoryTrain:  epoch  3, batch     0 | loss: 2.0925012MemoryTrain:  epoch  3, batch     1 | loss: 2.2934284MemoryTrain:  epoch  3, batch     2 | loss: 2.3460002MemoryTrain:  epoch  4, batch     0 | loss: 2.1821795MemoryTrain:  epoch  4, batch     1 | loss: 2.0597713MemoryTrain:  epoch  4, batch     2 | loss: 1.6763233MemoryTrain:  epoch  5, batch     0 | loss: 1.6924666MemoryTrain:  epoch  5, batch     1 | loss: 2.1118329MemoryTrain:  epoch  5, batch     2 | loss: 1.7982825MemoryTrain:  epoch  6, batch     0 | loss: 1.9799154MemoryTrain:  epoch  6, batch     1 | loss: 1.5098562MemoryTrain:  epoch  6, batch     2 | loss: 1.6798955MemoryTrain:  epoch  7, batch     0 | loss: 1.5446687MemoryTrain:  epoch  7, batch     1 | loss: 1.8053982MemoryTrain:  epoch  7, batch     2 | loss: 1.5963545MemoryTrain:  epoch  8, batch     0 | loss: 1.5159280MemoryTrain:  epoch  8, batch     1 | loss: 1.6464095MemoryTrain:  epoch  8, batch     2 | loss: 1.6702189MemoryTrain:  epoch  9, batch     0 | loss: 1.4406743MemoryTrain:  epoch  9, batch     1 | loss: 1.5866343MemoryTrain:  epoch  9, batch     2 | loss: 1.7896144
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    1 | acc: 56.25%,  total acc: 59.38%   [EVAL] batch:    2 | acc: 62.50%,  total acc: 60.42%   [EVAL] batch:    3 | acc: 93.75%,  total acc: 68.75%   [EVAL] batch:    4 | acc: 56.25%,  total acc: 66.25%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 71.88%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 75.89%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 78.12%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 79.86%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 78.12%   [EVAL] batch:   10 | acc: 50.00%,  total acc: 75.57%   [EVAL] batch:   11 | acc: 62.50%,  total acc: 74.48%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 74.04%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 73.66%   [EVAL] batch:   14 | acc: 18.75%,  total acc: 70.00%   
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    1 | acc: 75.00%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 79.69%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    5 | acc: 68.75%,  total acc: 79.17%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 80.36%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 82.81%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 84.72%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 85.62%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 86.93%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 86.16%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 85.42%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 83.59%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 83.09%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 81.94%   [EVAL] batch:   18 | acc: 75.00%,  total acc: 81.58%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 81.88%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 82.74%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 83.52%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 84.24%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 84.64%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 85.25%   [EVAL] batch:   25 | acc: 93.75%,  total acc: 85.58%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 85.88%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 86.38%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 86.85%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 86.88%   [EVAL] batch:   30 | acc: 81.25%,  total acc: 86.69%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 86.91%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 87.12%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 87.32%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   35 | acc: 81.25%,  total acc: 87.33%   [EVAL] batch:   36 | acc: 62.50%,  total acc: 86.66%   [EVAL] batch:   37 | acc: 75.00%,  total acc: 86.35%   [EVAL] batch:   38 | acc: 37.50%,  total acc: 85.10%   [EVAL] batch:   39 | acc: 81.25%,  total acc: 85.00%   [EVAL] batch:   40 | acc: 37.50%,  total acc: 83.84%   [EVAL] batch:   41 | acc: 43.75%,  total acc: 82.89%   [EVAL] batch:   42 | acc: 62.50%,  total acc: 82.41%   [EVAL] batch:   43 | acc: 50.00%,  total acc: 81.68%   [EVAL] batch:   44 | acc: 62.50%,  total acc: 81.25%   [EVAL] batch:   45 | acc: 62.50%,  total acc: 80.84%   [EVAL] batch:   46 | acc: 50.00%,  total acc: 80.19%   [EVAL] batch:   47 | acc: 68.75%,  total acc: 79.95%   [EVAL] batch:   48 | acc: 87.50%,  total acc: 80.10%   [EVAL] batch:   49 | acc: 62.50%,  total acc: 79.75%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 80.15%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 80.53%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 80.78%   [EVAL] batch:   53 | acc: 93.75%,  total acc: 81.02%   [EVAL] batch:   54 | acc: 56.25%,  total acc: 80.57%   [EVAL] batch:   55 | acc: 56.25%,  total acc: 80.13%   [EVAL] batch:   56 | acc: 56.25%,  total acc: 79.71%   [EVAL] batch:   57 | acc: 68.75%,  total acc: 79.53%   [EVAL] batch:   58 | acc: 68.75%,  total acc: 79.34%   [EVAL] batch:   59 | acc: 18.75%,  total acc: 78.33%   
cur_acc:  ['0.8655', '0.7788', '0.7000']
his_acc:  ['0.8655', '0.8472', '0.7833']
CurrentTrain: epoch  0, batch     0 | loss: 8.3496857CurrentTrain: epoch  0, batch     1 | loss: 7.7980304CurrentTrain: epoch  1, batch     0 | loss: 7.7296548CurrentTrain: epoch  1, batch     1 | loss: 6.9038968CurrentTrain: epoch  2, batch     0 | loss: 6.7976494CurrentTrain: epoch  2, batch     1 | loss: 6.4477592CurrentTrain: epoch  3, batch     0 | loss: 6.6722674CurrentTrain: epoch  3, batch     1 | loss: 5.5949621CurrentTrain: epoch  4, batch     0 | loss: 6.0606155CurrentTrain: epoch  4, batch     1 | loss: 5.3828692CurrentTrain: epoch  5, batch     0 | loss: 5.8311124CurrentTrain: epoch  5, batch     1 | loss: 5.7782397CurrentTrain: epoch  6, batch     0 | loss: 5.3235583CurrentTrain: epoch  6, batch     1 | loss: 4.8132935CurrentTrain: epoch  7, batch     0 | loss: 4.8586979CurrentTrain: epoch  7, batch     1 | loss: 4.7550254CurrentTrain: epoch  8, batch     0 | loss: 4.8378372CurrentTrain: epoch  8, batch     1 | loss: 4.3913956CurrentTrain: epoch  9, batch     0 | loss: 4.2459278CurrentTrain: epoch  9, batch     1 | loss: 4.7252793
#### organization subsidiaries ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization subsidiaries" means an organization 's subsidiary is an organization.
Here is an example:
Relation: organization subsidiaries
Context: the state department of conservation and recreation closed eight campgrounds thursday night and friday , including boston harbor islands national park , nickerson state park in brewster , myles standish state forest in south carver , and shawme-crowell state forest in sandwich .
Head Entity: department of conservation and recreation
Tail Entity: boston harbor islands national park
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization subsidiaries":
Relation: organization subsidiaries  
Context: The tech giant Google has announced the acquisition of Fitbit, a company known for its fitness tracking devices, to enhance its health and wellness offerings.  
Head Entity: Google  
Tail Entity: Fitbit  

Relation: organization subsidiaries  
Context: The multinational beverage corporation Coca-Cola owns several brands, including Fanta, Sprite, and Dasani, which are popular worldwide.  
Head Entity: Coca-Cola  
Tail Entity: Fanta  
#### organization parents ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization parents" means an organization 's parent is an organization.
Here is an example:
Relation: organization parents
Context: aseel kami , tim cocks , abdul-rahman taher , tim cocks and michael christie -lrb- reuters -rrb- add an intriguing note in the speculation over who 's responsible for the recent bombings : `` but a senior iraqi intelligence source , who declined to be named , said there was evidence the bombs could be the work of the militant badr organisation , the armed wing of the supreme islamic iraqi council -lrb- isci -rrb- .
Head Entity: badr organisation
Tail Entity: supreme islamic iraqi council
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization parents":
Relation: organization parents  
Context: In a recent merger, the tech giant Alphabet Inc. announced that it would acquire the innovative startup DeepMind Technologies, which has been a subsidiary of Alphabet since 2014. This acquisition is expected to enhance Alphabet's capabilities in artificial intelligence.  
Head Entity: DeepMind Technologies  
Tail Entity: Alphabet Inc.  

Relation: organization parents  
Context: The historical records indicate that the renowned publishing house Penguin Random House was formed through the merger of two major companies, Penguin Group and Random House, which were both influential in the literary world.  
Head Entity: Penguin Random House  
Tail Entity: Penguin Group  
#### organization alternate names ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization alternate names" means a person 's alias is a person.
Here is an example:
Relation: organization alternate names
Context: it also needs the green light from the 45-nation nuclear suppliers group -lrb- nsg -rrb- , which regulates global civilian nuclear trade , before it can begin buying nuclear reactors and fuel .
Head Entity: nsg
Tail Entity: nuclear suppliers group
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization alternate names":
Relation: organization alternate names  
Context: The International Monetary Fund, often referred to as the IMF, plays a crucial role in global economic stability.  
Head Entity: IMF  
Tail Entity: International Monetary Fund  

Relation: organization alternate names  
Context: The World Health Organization, commonly known as WHO, has been at the forefront of the global response to health crises.  
Head Entity: WHO  
Tail Entity: World Health Organization  
#### organization city of headquarters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization city of headquarters" means an organization is located in a city.
Here is an example:
Relation: organization city of headquarters
Context: ------ london 2008-05-20 07:23:45 utc enodis plc endorses sweetened takeover bid by us company manitowoc illinois tool works of glenville , illinois , which had offered 282 pence -lrb- us$ 551 euro3 54 -rrb- per share , said monday that it was considering its position .
Head Entity: illinois tool works
Tail Entity: glenville
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization city of headquarters":
Relation: organization city of headquarters  
Context: ------ in 2015, tech giant apple inc. announced plans to expand its operations in cupertino, california, where it has been headquartered since its founding.  
Head Entity: apple inc.  
Tail Entity: cupertino  

Relation: organization city of headquarters  
Context: ------ the multinational corporation samsung electronics is based in suwon, south korea, and has been a leader in the technology sector for decades.  
Head Entity: samsung electronics  
Tail Entity: suwon  
#### person siblings ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person siblings" means a person 's sibling is a person.
Here is an example:
Relation: person siblings
Context: forsberg , a political science professor at city college of new york , died oct. 19 in a bronx hospital of cancer , said her sister , celia seupel .
Head Entity: forsberg
Tail Entity: celia seupel
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person siblings":
Relation: person siblings  
Context: during the family reunion, john introduced his sister, emily, who had just returned from studying abroad.  
Head Entity: john  
Tail Entity: emily  

Relation: person siblings  
Context: after the ceremony, lucas shared a heartfelt moment with his brother, michael, reminiscing about their childhood adventures.  
Head Entity: lucas  
Tail Entity: michael  
Mixup data size:  3640
MixupTrain:  epoch  0, batch     0 | loss: 4.9701538MixupTrain:  epoch  0, batch     1 | loss: 4.3741012MixupTrain:  epoch  0, batch     2 | loss: 3.8297081MixupTrain:  epoch  0, batch     3 | loss: 4.7037334MixupTrain:  epoch  0, batch     4 | loss: 4.5286016MixupTrain:  epoch  0, batch     5 | loss: 4.7549534MixupTrain:  epoch  0, batch     6 | loss: 3.9899957MixupTrain:  epoch  0, batch     7 | loss: 4.5001030MixupTrain:  epoch  0, batch     8 | loss: 4.4549341MixupTrain:  epoch  0, batch     9 | loss: 4.9274530MixupTrain:  epoch  0, batch    10 | loss: 4.6994257MixupTrain:  epoch  0, batch    11 | loss: 4.6746950MixupTrain:  epoch  0, batch    12 | loss: 3.9567776MixupTrain:  epoch  0, batch    13 | loss: 4.4405651MixupTrain:  epoch  0, batch    14 | loss: 4.1092973MixupTrain:  epoch  0, batch    15 | loss: 4.3373232MixupTrain:  epoch  0, batch    16 | loss: 3.8120890MixupTrain:  epoch  0, batch    17 | loss: 4.1557360MixupTrain:  epoch  0, batch    18 | loss: 4.0210719MixupTrain:  epoch  0, batch    19 | loss: 3.8412025MixupTrain:  epoch  0, batch    20 | loss: 4.6262836MixupTrain:  epoch  0, batch    21 | loss: 3.8812904MixupTrain:  epoch  0, batch    22 | loss: 4.1875887MixupTrain:  epoch  0, batch    23 | loss: 3.7959101MixupTrain:  epoch  0, batch    24 | loss: 4.3441772MixupTrain:  epoch  0, batch    25 | loss: 3.9520311MixupTrain:  epoch  0, batch    26 | loss: 4.2493625MixupTrain:  epoch  0, batch    27 | loss: 4.5126381MixupTrain:  epoch  0, batch    28 | loss: 3.5545135MixupTrain:  epoch  0, batch    29 | loss: 4.2201281MixupTrain:  epoch  0, batch    30 | loss: 4.2655401MixupTrain:  epoch  0, batch    31 | loss: 4.3226719MixupTrain:  epoch  0, batch    32 | loss: 4.2531686MixupTrain:  epoch  0, batch    33 | loss: 4.3008628MixupTrain:  epoch  0, batch    34 | loss: 4.1596861MixupTrain:  epoch  0, batch    35 | loss: 4.0886016MixupTrain:  epoch  0, batch    36 | loss: 3.8183970MixupTrain:  epoch  0, batch    37 | loss: 3.9432416MixupTrain:  epoch  0, batch    38 | loss: 4.0650716MixupTrain:  epoch  0, batch    39 | loss: 4.0380483MixupTrain:  epoch  0, batch    40 | loss: 4.0999689MixupTrain:  epoch  0, batch    41 | loss: 3.8837919MixupTrain:  epoch  0, batch    42 | loss: 3.9986205MixupTrain:  epoch  0, batch    43 | loss: 3.7434661MixupTrain:  epoch  0, batch    44 | loss: 3.5895658MixupTrain:  epoch  0, batch    45 | loss: 3.8856196MixupTrain:  epoch  0, batch    46 | loss: 3.8261690MixupTrain:  epoch  0, batch    47 | loss: 3.5489569MixupTrain:  epoch  0, batch    48 | loss: 4.3790789MixupTrain:  epoch  0, batch    49 | loss: 4.1956558MixupTrain:  epoch  0, batch    50 | loss: 3.8233879MixupTrain:  epoch  0, batch    51 | loss: 3.9421628MixupTrain:  epoch  0, batch    52 | loss: 3.6470613MixupTrain:  epoch  0, batch    53 | loss: 3.8879886MixupTrain:  epoch  0, batch    54 | loss: 3.8392336MixupTrain:  epoch  0, batch    55 | loss: 3.7482438MixupTrain:  epoch  0, batch    56 | loss: 3.8146224MixupTrain:  epoch  0, batch    57 | loss: 3.7033091MixupTrain:  epoch  0, batch    58 | loss: 3.3854947MixupTrain:  epoch  0, batch    59 | loss: 3.3492637MixupTrain:  epoch  0, batch    60 | loss: 3.6669905MixupTrain:  epoch  0, batch    61 | loss: 4.0817871MixupTrain:  epoch  0, batch    62 | loss: 3.5547297MixupTrain:  epoch  0, batch    63 | loss: 3.5794554MixupTrain:  epoch  0, batch    64 | loss: 4.0195427MixupTrain:  epoch  0, batch    65 | loss: 3.6894817MixupTrain:  epoch  0, batch    66 | loss: 3.7067926MixupTrain:  epoch  0, batch    67 | loss: 3.4588943MixupTrain:  epoch  0, batch    68 | loss: 3.6377518MixupTrain:  epoch  0, batch    69 | loss: 3.7578173MixupTrain:  epoch  0, batch    70 | loss: 3.7156539MixupTrain:  epoch  0, batch    71 | loss: 3.3270478MixupTrain:  epoch  0, batch    72 | loss: 3.4181762MixupTrain:  epoch  0, batch    73 | loss: 3.5777225MixupTrain:  epoch  0, batch    74 | loss: 3.8334124MixupTrain:  epoch  0, batch    75 | loss: 3.6458826MixupTrain:  epoch  0, batch    76 | loss: 3.3703957MixupTrain:  epoch  0, batch    77 | loss: 3.4747496MixupTrain:  epoch  0, batch    78 | loss: 3.9943326MixupTrain:  epoch  0, batch    79 | loss: 3.6569023MixupTrain:  epoch  0, batch    80 | loss: 3.5079827MixupTrain:  epoch  0, batch    81 | loss: 3.7185633MixupTrain:  epoch  0, batch    82 | loss: 3.3179612MixupTrain:  epoch  0, batch    83 | loss: 3.6010709MixupTrain:  epoch  0, batch    84 | loss: 3.4376857MixupTrain:  epoch  0, batch    85 | loss: 3.6269073MixupTrain:  epoch  0, batch    86 | loss: 3.4353976MixupTrain:  epoch  0, batch    87 | loss: 3.8222723MixupTrain:  epoch  0, batch    88 | loss: 3.3153055MixupTrain:  epoch  0, batch    89 | loss: 3.8762989MixupTrain:  epoch  0, batch    90 | loss: 3.8004432MixupTrain:  epoch  0, batch    91 | loss: 3.4971685MixupTrain:  epoch  0, batch    92 | loss: 3.7021337MixupTrain:  epoch  0, batch    93 | loss: 3.7225318MixupTrain:  epoch  0, batch    94 | loss: 3.5704527MixupTrain:  epoch  0, batch    95 | loss: 3.4686649MixupTrain:  epoch  0, batch    96 | loss: 3.6946120MixupTrain:  epoch  0, batch    97 | loss: 3.2490244MixupTrain:  epoch  0, batch    98 | loss: 3.3736339MixupTrain:  epoch  0, batch    99 | loss: 3.3612900MixupTrain:  epoch  0, batch   100 | loss: 3.9403999MixupTrain:  epoch  0, batch   101 | loss: 3.4309721MixupTrain:  epoch  0, batch   102 | loss: 3.4634156MixupTrain:  epoch  0, batch   103 | loss: 3.5619144MixupTrain:  epoch  0, batch   104 | loss: 3.6330140MixupTrain:  epoch  0, batch   105 | loss: 3.7303038MixupTrain:  epoch  0, batch   106 | loss: 3.4776070MixupTrain:  epoch  0, batch   107 | loss: 3.7880630MixupTrain:  epoch  0, batch   108 | loss: 3.8335047MixupTrain:  epoch  0, batch   109 | loss: 3.2435760MixupTrain:  epoch  0, batch   110 | loss: 3.2234573MixupTrain:  epoch  0, batch   111 | loss: 3.1914601MixupTrain:  epoch  0, batch   112 | loss: 3.4162714MixupTrain:  epoch  0, batch   113 | loss: 3.2466407MixupTrain:  epoch  0, batch   114 | loss: 3.6335254MixupTrain:  epoch  0, batch   115 | loss: 3.6434221MixupTrain:  epoch  0, batch   116 | loss: 3.4835420MixupTrain:  epoch  0, batch   117 | loss: 3.3013895MixupTrain:  epoch  0, batch   118 | loss: 3.2334790MixupTrain:  epoch  0, batch   119 | loss: 3.7534633MixupTrain:  epoch  0, batch   120 | loss: 3.4022913MixupTrain:  epoch  0, batch   121 | loss: 3.4628382MixupTrain:  epoch  0, batch   122 | loss: 3.3292978MixupTrain:  epoch  0, batch   123 | loss: 3.1296391MixupTrain:  epoch  0, batch   124 | loss: 3.5076103MixupTrain:  epoch  0, batch   125 | loss: 3.4146209MixupTrain:  epoch  0, batch   126 | loss: 3.3934526MixupTrain:  epoch  0, batch   127 | loss: 3.4852881MixupTrain:  epoch  0, batch   128 | loss: 3.7139392MixupTrain:  epoch  0, batch   129 | loss: 3.5992002MixupTrain:  epoch  0, batch   130 | loss: 3.4469461MixupTrain:  epoch  0, batch   131 | loss: 3.4210148MixupTrain:  epoch  0, batch   132 | loss: 3.4004829MixupTrain:  epoch  0, batch   133 | loss: 3.2930183MixupTrain:  epoch  0, batch   134 | loss: 3.3998148MixupTrain:  epoch  0, batch   135 | loss: 3.6121364MixupTrain:  epoch  0, batch   136 | loss: 3.3388052MixupTrain:  epoch  0, batch   137 | loss: 3.4595568MixupTrain:  epoch  0, batch   138 | loss: 3.3288114MixupTrain:  epoch  0, batch   139 | loss: 3.1766009MixupTrain:  epoch  0, batch   140 | loss: 3.6812119MixupTrain:  epoch  0, batch   141 | loss: 3.4287162MixupTrain:  epoch  0, batch   142 | loss: 3.5113790MixupTrain:  epoch  0, batch   143 | loss: 3.4324656MixupTrain:  epoch  0, batch   144 | loss: 3.4986076MixupTrain:  epoch  0, batch   145 | loss: 3.5155294MixupTrain:  epoch  0, batch   146 | loss: 3.1467259MixupTrain:  epoch  0, batch   147 | loss: 3.1591783MixupTrain:  epoch  0, batch   148 | loss: 3.1384723MixupTrain:  epoch  0, batch   149 | loss: 3.2033348MixupTrain:  epoch  0, batch   150 | loss: 3.1312070MixupTrain:  epoch  0, batch   151 | loss: 3.3548667MixupTrain:  epoch  0, batch   152 | loss: 3.5448546MixupTrain:  epoch  0, batch   153 | loss: 3.4867036MixupTrain:  epoch  0, batch   154 | loss: 3.2969620MixupTrain:  epoch  0, batch   155 | loss: 3.4044201MixupTrain:  epoch  0, batch   156 | loss: 3.4798720MixupTrain:  epoch  0, batch   157 | loss: 3.1642709MixupTrain:  epoch  0, batch   158 | loss: 3.3276410MixupTrain:  epoch  0, batch   159 | loss: 3.3858323MixupTrain:  epoch  0, batch   160 | loss: 3.3237100MixupTrain:  epoch  0, batch   161 | loss: 3.3554866MixupTrain:  epoch  0, batch   162 | loss: 3.4492698MixupTrain:  epoch  0, batch   163 | loss: 3.2146096MixupTrain:  epoch  0, batch   164 | loss: 3.2074931MixupTrain:  epoch  0, batch   165 | loss: 3.3473990MixupTrain:  epoch  0, batch   166 | loss: 3.4402571MixupTrain:  epoch  0, batch   167 | loss: 3.4384136MixupTrain:  epoch  0, batch   168 | loss: 3.1589708MixupTrain:  epoch  0, batch   169 | loss: 3.3250008MixupTrain:  epoch  0, batch   170 | loss: 3.2914691MixupTrain:  epoch  0, batch   171 | loss: 3.3981557MixupTrain:  epoch  0, batch   172 | loss: 3.1827445MixupTrain:  epoch  0, batch   173 | loss: 3.2726715MixupTrain:  epoch  0, batch   174 | loss: 3.1559997MixupTrain:  epoch  0, batch   175 | loss: 3.4069901MixupTrain:  epoch  0, batch   176 | loss: 3.3843064MixupTrain:  epoch  0, batch   177 | loss: 3.2122812MixupTrain:  epoch  0, batch   178 | loss: 3.1595252MixupTrain:  epoch  0, batch   179 | loss: 3.2382083MixupTrain:  epoch  0, batch   180 | loss: 3.3721085MixupTrain:  epoch  0, batch   181 | loss: 3.4039495MixupTrain:  epoch  0, batch   182 | loss: 3.4781096MixupTrain:  epoch  0, batch   183 | loss: 3.3359873MixupTrain:  epoch  0, batch   184 | loss: 3.2640836MixupTrain:  epoch  0, batch   185 | loss: 3.0342607MixupTrain:  epoch  0, batch   186 | loss: 3.2792368MixupTrain:  epoch  0, batch   187 | loss: 3.2114019MixupTrain:  epoch  0, batch   188 | loss: 3.3754590MixupTrain:  epoch  0, batch   189 | loss: 3.5164459MixupTrain:  epoch  0, batch   190 | loss: 3.3054748MixupTrain:  epoch  0, batch   191 | loss: 3.0838675MixupTrain:  epoch  0, batch   192 | loss: 3.3254130MixupTrain:  epoch  0, batch   193 | loss: 3.2137015MixupTrain:  epoch  0, batch   194 | loss: 3.2884345MixupTrain:  epoch  0, batch   195 | loss: 3.1368847MixupTrain:  epoch  0, batch   196 | loss: 3.2931786MixupTrain:  epoch  0, batch   197 | loss: 2.9620209MixupTrain:  epoch  0, batch   198 | loss: 3.1262534MixupTrain:  epoch  0, batch   199 | loss: 3.5020742MixupTrain:  epoch  0, batch   200 | loss: 3.1715631MixupTrain:  epoch  0, batch   201 | loss: 3.3429549MixupTrain:  epoch  0, batch   202 | loss: 3.4347415MixupTrain:  epoch  0, batch   203 | loss: 3.3806036MixupTrain:  epoch  0, batch   204 | loss: 3.2896633MixupTrain:  epoch  0, batch   205 | loss: 3.0792933MixupTrain:  epoch  0, batch   206 | loss: 2.9597535MixupTrain:  epoch  0, batch   207 | loss: 3.4270365MixupTrain:  epoch  0, batch   208 | loss: 3.2110348MixupTrain:  epoch  0, batch   209 | loss: 3.2905416MixupTrain:  epoch  0, batch   210 | loss: 3.2145460MixupTrain:  epoch  0, batch   211 | loss: 3.0807061MixupTrain:  epoch  0, batch   212 | loss: 3.1421707MixupTrain:  epoch  0, batch   213 | loss: 3.0866504MixupTrain:  epoch  0, batch   214 | loss: 3.3224897MixupTrain:  epoch  0, batch   215 | loss: 3.2223003MixupTrain:  epoch  0, batch   216 | loss: 3.1703207MixupTrain:  epoch  0, batch   217 | loss: 3.2849803MixupTrain:  epoch  0, batch   218 | loss: 3.2006381MixupTrain:  epoch  0, batch   219 | loss: 3.2666953MixupTrain:  epoch  0, batch   220 | loss: 3.1027205MixupTrain:  epoch  0, batch   221 | loss: 3.1292250MixupTrain:  epoch  0, batch   222 | loss: 3.1454592MixupTrain:  epoch  0, batch   223 | loss: 3.2713876MixupTrain:  epoch  0, batch   224 | loss: 3.2786412MixupTrain:  epoch  0, batch   225 | loss: 3.1855965MixupTrain:  epoch  0, batch   226 | loss: 3.3012931MixupTrain:  epoch  0, batch   227 | loss: 3.2051725
MemoryTrain:  epoch  0, batch     0 | loss: 1.8286790MemoryTrain:  epoch  0, batch     1 | loss: 1.9875205MemoryTrain:  epoch  0, batch     2 | loss: 2.5299277MemoryTrain:  epoch  0, batch     3 | loss: 2.2317452MemoryTrain:  epoch  1, batch     0 | loss: 2.0993855MemoryTrain:  epoch  1, batch     1 | loss: 1.5851110MemoryTrain:  epoch  1, batch     2 | loss: 1.6944828MemoryTrain:  epoch  1, batch     3 | loss: 1.8924471MemoryTrain:  epoch  2, batch     0 | loss: 1.8828528MemoryTrain:  epoch  2, batch     1 | loss: 1.5810050MemoryTrain:  epoch  2, batch     2 | loss: 1.4776763MemoryTrain:  epoch  2, batch     3 | loss: 1.3804089MemoryTrain:  epoch  3, batch     0 | loss: 1.4195931MemoryTrain:  epoch  3, batch     1 | loss: 1.4822276MemoryTrain:  epoch  3, batch     2 | loss: 1.4973366MemoryTrain:  epoch  3, batch     3 | loss: 1.5650336MemoryTrain:  epoch  4, batch     0 | loss: 1.3921582MemoryTrain:  epoch  4, batch     1 | loss: 1.3956550MemoryTrain:  epoch  4, batch     2 | loss: 1.4528770MemoryTrain:  epoch  4, batch     3 | loss: 1.3289821MemoryTrain:  epoch  5, batch     0 | loss: 1.3953156MemoryTrain:  epoch  5, batch     1 | loss: 1.3503076MemoryTrain:  epoch  5, batch     2 | loss: 1.3801489MemoryTrain:  epoch  5, batch     3 | loss: 1.2521842MemoryTrain:  epoch  6, batch     0 | loss: 1.2938362MemoryTrain:  epoch  6, batch     1 | loss: 1.3059845MemoryTrain:  epoch  6, batch     2 | loss: 1.3735031MemoryTrain:  epoch  6, batch     3 | loss: 1.3330202MemoryTrain:  epoch  7, batch     0 | loss: 1.2907190MemoryTrain:  epoch  7, batch     1 | loss: 1.3474870MemoryTrain:  epoch  7, batch     2 | loss: 1.2694988MemoryTrain:  epoch  7, batch     3 | loss: 1.2456976MemoryTrain:  epoch  8, batch     0 | loss: 1.2406404MemoryTrain:  epoch  8, batch     1 | loss: 1.3094616MemoryTrain:  epoch  8, batch     2 | loss: 1.2682275MemoryTrain:  epoch  8, batch     3 | loss: 1.3641071MemoryTrain:  epoch  9, batch     0 | loss: 1.2758641MemoryTrain:  epoch  9, batch     1 | loss: 1.3818411MemoryTrain:  epoch  9, batch     2 | loss: 1.2472227MemoryTrain:  epoch  9, batch     3 | loss: 1.2963288
[EVAL] batch:    0 | acc: 25.00%,  total acc: 25.00%   [EVAL] batch:    1 | acc: 37.50%,  total acc: 31.25%   [EVAL] batch:    2 | acc: 12.50%,  total acc: 25.00%   [EVAL] batch:    3 | acc: 43.75%,  total acc: 29.69%   [EVAL] batch:    4 | acc: 50.00%,  total acc: 33.75%   [EVAL] batch:    5 | acc: 37.50%,  total acc: 34.38%   [EVAL] batch:    6 | acc: 56.25%,  total acc: 37.50%   [EVAL] batch:    7 | acc: 50.00%,  total acc: 39.06%   [EVAL] batch:    8 | acc: 12.50%,  total acc: 36.11%   [EVAL] batch:    9 | acc: 56.25%,  total acc: 38.12%   [EVAL] batch:   10 | acc: 68.75%,  total acc: 40.91%   [EVAL] batch:   11 | acc: 50.00%,  total acc: 41.67%   [EVAL] batch:   12 | acc: 43.75%,  total acc: 41.83%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 45.54%   [EVAL] batch:   14 | acc: 100.00%,  total acc: 49.17%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 52.34%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 54.78%   [EVAL] batch:   17 | acc: 87.50%,  total acc: 56.60%   [EVAL] batch:   18 | acc: 81.25%,  total acc: 57.89%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 59.69%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 61.31%   [EVAL] batch:   21 | acc: 37.50%,  total acc: 60.23%   
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 82.81%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 85.00%   [EVAL] batch:    5 | acc: 68.75%,  total acc: 82.29%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 83.04%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 85.16%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 86.81%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 86.88%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 88.07%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 88.54%   [EVAL] batch:   12 | acc: 56.25%,  total acc: 86.06%   [EVAL] batch:   13 | acc: 31.25%,  total acc: 82.14%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 81.67%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 80.08%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 79.78%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 78.82%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 78.29%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 78.75%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 79.76%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 80.68%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 81.52%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 82.03%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 82.75%   [EVAL] batch:   25 | acc: 93.75%,  total acc: 83.17%   [EVAL] batch:   26 | acc: 75.00%,  total acc: 82.87%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 83.48%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 84.05%   [EVAL] batch:   29 | acc: 68.75%,  total acc: 83.54%   [EVAL] batch:   30 | acc: 81.25%,  total acc: 83.47%   [EVAL] batch:   31 | acc: 87.50%,  total acc: 83.59%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 83.90%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 84.38%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 84.64%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 84.72%   [EVAL] batch:   36 | acc: 62.50%,  total acc: 84.12%   [EVAL] batch:   37 | acc: 75.00%,  total acc: 83.88%   [EVAL] batch:   38 | acc: 37.50%,  total acc: 82.69%   [EVAL] batch:   39 | acc: 25.00%,  total acc: 81.25%   [EVAL] batch:   40 | acc: 0.00%,  total acc: 79.27%   [EVAL] batch:   41 | acc: 0.00%,  total acc: 77.38%   [EVAL] batch:   42 | acc: 0.00%,  total acc: 75.58%   [EVAL] batch:   43 | acc: 43.75%,  total acc: 74.86%   [EVAL] batch:   44 | acc: 75.00%,  total acc: 74.86%   [EVAL] batch:   45 | acc: 50.00%,  total acc: 74.32%   [EVAL] batch:   46 | acc: 37.50%,  total acc: 73.54%   [EVAL] batch:   47 | acc: 75.00%,  total acc: 73.57%   [EVAL] batch:   48 | acc: 81.25%,  total acc: 73.72%   [EVAL] batch:   49 | acc: 56.25%,  total acc: 73.38%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 73.90%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 74.40%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 74.76%   [EVAL] batch:   53 | acc: 87.50%,  total acc: 75.00%   [EVAL] batch:   54 | acc: 56.25%,  total acc: 74.66%   [EVAL] batch:   55 | acc: 0.00%,  total acc: 73.33%   [EVAL] batch:   56 | acc: 0.00%,  total acc: 72.04%   [EVAL] batch:   57 | acc: 6.25%,  total acc: 70.91%   [EVAL] batch:   58 | acc: 12.50%,  total acc: 69.92%   [EVAL] batch:   59 | acc: 12.50%,  total acc: 68.96%   [EVAL] batch:   60 | acc: 37.50%,  total acc: 68.44%   [EVAL] batch:   61 | acc: 25.00%,  total acc: 67.74%   [EVAL] batch:   62 | acc: 25.00%,  total acc: 67.06%   [EVAL] batch:   63 | acc: 43.75%,  total acc: 66.70%   [EVAL] batch:   64 | acc: 43.75%,  total acc: 66.35%   [EVAL] batch:   65 | acc: 50.00%,  total acc: 66.10%   [EVAL] batch:   66 | acc: 56.25%,  total acc: 65.95%   [EVAL] batch:   67 | acc: 25.00%,  total acc: 65.35%   [EVAL] batch:   68 | acc: 43.75%,  total acc: 65.04%   [EVAL] batch:   69 | acc: 68.75%,  total acc: 65.09%   [EVAL] batch:   70 | acc: 50.00%,  total acc: 64.88%   [EVAL] batch:   71 | acc: 50.00%,  total acc: 64.67%   [EVAL] batch:   72 | acc: 75.00%,  total acc: 64.81%   [EVAL] batch:   73 | acc: 93.75%,  total acc: 65.20%   [EVAL] batch:   74 | acc: 100.00%,  total acc: 65.67%   [EVAL] batch:   75 | acc: 100.00%,  total acc: 66.12%   [EVAL] batch:   76 | acc: 81.25%,  total acc: 66.31%   [EVAL] batch:   77 | acc: 87.50%,  total acc: 66.59%   [EVAL] batch:   78 | acc: 87.50%,  total acc: 66.85%   [EVAL] batch:   79 | acc: 93.75%,  total acc: 67.19%   [EVAL] batch:   80 | acc: 75.00%,  total acc: 67.28%   
cur_acc:  ['0.8655', '0.7788', '0.7000', '0.6023']
his_acc:  ['0.8655', '0.8472', '0.7833', '0.6728']
CurrentTrain: epoch  0, batch     0 | loss: 4.8245735CurrentTrain: epoch  0, batch     1 | loss: 5.7805676CurrentTrain: epoch  1, batch     0 | loss: 4.1783066CurrentTrain: epoch  1, batch     1 | loss: 4.2488475CurrentTrain: epoch  2, batch     0 | loss: 3.5549669CurrentTrain: epoch  2, batch     1 | loss: 4.6009007CurrentTrain: epoch  3, batch     0 | loss: 3.6696148CurrentTrain: epoch  3, batch     1 | loss: 2.8041332CurrentTrain: epoch  4, batch     0 | loss: 2.8680084CurrentTrain: epoch  4, batch     1 | loss: 2.6248980CurrentTrain: epoch  5, batch     0 | loss: 2.6827502CurrentTrain: epoch  5, batch     1 | loss: 2.4861989CurrentTrain: epoch  6, batch     0 | loss: 2.3401277CurrentTrain: epoch  6, batch     1 | loss: 2.3483281CurrentTrain: epoch  7, batch     0 | loss: 2.1601245CurrentTrain: epoch  7, batch     1 | loss: 2.0908966CurrentTrain: epoch  8, batch     0 | loss: 2.1676393CurrentTrain: epoch  8, batch     1 | loss: 2.2077391CurrentTrain: epoch  9, batch     0 | loss: 2.1659274CurrentTrain: epoch  9, batch     1 | loss: 2.0343788
#### person cities of residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person cities of residence" means a person is living in a city.
Here is an example:
Relation: person cities of residence
Context: in 1974 , divorced and the mother of a 5-year-old , forsberg moved to boston and began studying arms control at mit ; she received her ph.d. in 1980 .
Head Entity: forsberg
Tail Entity: boston
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person cities of residence":
Relation: person cities of residence  
Context: After spending several years in New York, she decided to relocate to San Francisco to pursue her career in tech.  
Head Entity: she  
Tail Entity: San Francisco  

Relation: person cities of residence  
Context: Following his graduation, John moved to Seattle where he found a job in a leading software company.  
Head Entity: John  
Tail Entity: Seattle  
#### person schools attended ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person schools attended" means a person 's school is an organization.
Here is an example:
Relation: person schools attended
Context: after world war ii , he attended the university of southern california , where he became editor of a college magazine .
Head Entity: he
Tail Entity: university of southern california
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person schools attended":
Relation: person schools attended  
Context: She graduated from Harvard University with a degree in psychology before pursuing her career in clinical research.  
Head Entity: She  
Tail Entity: Harvard University  

Relation: person schools attended  
Context: After completing his high school education, John enrolled at Stanford University to study computer science.  
Head Entity: John  
Tail Entity: Stanford University  
#### person country of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person country of death" means a person was died in a country.
Here is an example:
Relation: person country of death
Context: us republican congresswoman jo ann davis dies after fight with breast cancer
Head Entity: jo ann davis
Tail Entity: us
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person country of death":
Relation: person country of death  
Context: renowned physicist stephen hawking passed away in cambridge, england  
Head Entity: stephen hawking  
Tail Entity: england  

Relation: person country of death  
Context: famous author agatha christie died in her home in wallingford, england  
Head Entity: agatha christie  
Tail Entity: england  
#### person children ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person children" means a person 's children is a person.
Here is an example:
Relation: person children
Context: he is survived by two stepdaughters , barbara a. hammond and brenda l. stevenson ; a stepson , michael a. taylor ; two grandchildren and one great-grandchild .
Head Entity: he
Tail Entity: brenda l. stevenson
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person children":
Relation: person children  
Context: she has three children, including her son, john, and her daughters, emily and sarah, who are both pursuing their studies.  
Head Entity: she  
Tail Entity: emily  

Relation: person children  
Context: after the passing of their father, the siblings, including jake and his sister, lila, took on the responsibility of caring for their younger brother, max.  
Head Entity: jake  
Tail Entity: lila  
#### person charges ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person charges" means a person was charged with an event.
Here is an example:
Relation: person charges
Context: flowers always contended politics was behind the extortion investigation , but appeals courts ruled against him .
Head Entity: him
Tail Entity: extortion
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person charges":
Relation: person charges  
Context: The prosecutor announced that the former mayor was facing serious allegations related to corruption.  
Head Entity: former mayor  
Tail Entity: corruption  

Relation: person charges  
Context: After a lengthy investigation, the authorities confirmed that the celebrity was implicated in a major drug trafficking case.  
Head Entity: celebrity  
Tail Entity: drug trafficking  
Mixup data size:  5050
MixupTrain:  epoch  0, batch     0 | loss: 3.5613718MixupTrain:  epoch  0, batch     1 | loss: 3.6316259MixupTrain:  epoch  0, batch     2 | loss: 3.5888367MixupTrain:  epoch  0, batch     3 | loss: 3.8087866MixupTrain:  epoch  0, batch     4 | loss: 4.1479592MixupTrain:  epoch  0, batch     5 | loss: 3.3287365MixupTrain:  epoch  0, batch     6 | loss: 3.3837235MixupTrain:  epoch  0, batch     7 | loss: 3.4172988MixupTrain:  epoch  0, batch     8 | loss: 3.7312598MixupTrain:  epoch  0, batch     9 | loss: 3.4897079MixupTrain:  epoch  0, batch    10 | loss: 3.2218676MixupTrain:  epoch  0, batch    11 | loss: 3.7069795MixupTrain:  epoch  0, batch    12 | loss: 3.2495420MixupTrain:  epoch  0, batch    13 | loss: 3.4125419MixupTrain:  epoch  0, batch    14 | loss: 3.1216011MixupTrain:  epoch  0, batch    15 | loss: 3.7115746MixupTrain:  epoch  0, batch    16 | loss: 3.7161503MixupTrain:  epoch  0, batch    17 | loss: 3.1306000MixupTrain:  epoch  0, batch    18 | loss: 3.4574170MixupTrain:  epoch  0, batch    19 | loss: 3.6526461MixupTrain:  epoch  0, batch    20 | loss: 3.5056944MixupTrain:  epoch  0, batch    21 | loss: 3.4870248MixupTrain:  epoch  0, batch    22 | loss: 3.4510679MixupTrain:  epoch  0, batch    23 | loss: 3.4583449MixupTrain:  epoch  0, batch    24 | loss: 3.2290521MixupTrain:  epoch  0, batch    25 | loss: 3.4954245MixupTrain:  epoch  0, batch    26 | loss: 3.2412622MixupTrain:  epoch  0, batch    27 | loss: 3.2911603MixupTrain:  epoch  0, batch    28 | loss: 3.6422486MixupTrain:  epoch  0, batch    29 | loss: 3.5241950MixupTrain:  epoch  0, batch    30 | loss: 3.4869261MixupTrain:  epoch  0, batch    31 | loss: 3.5274165MixupTrain:  epoch  0, batch    32 | loss: 3.2263489MixupTrain:  epoch  0, batch    33 | loss: 2.9713144MixupTrain:  epoch  0, batch    34 | loss: 3.5546317MixupTrain:  epoch  0, batch    35 | loss: 3.6249449MixupTrain:  epoch  0, batch    36 | loss: 3.3329680MixupTrain:  epoch  0, batch    37 | loss: 3.4208519MixupTrain:  epoch  0, batch    38 | loss: 3.0974319MixupTrain:  epoch  0, batch    39 | loss: 3.3296349MixupTrain:  epoch  0, batch    40 | loss: 3.6488755MixupTrain:  epoch  0, batch    41 | loss: 3.0562756MixupTrain:  epoch  0, batch    42 | loss: 3.2355113MixupTrain:  epoch  0, batch    43 | loss: 3.3190141MixupTrain:  epoch  0, batch    44 | loss: 3.2003036MixupTrain:  epoch  0, batch    45 | loss: 2.9638760MixupTrain:  epoch  0, batch    46 | loss: 3.1712985MixupTrain:  epoch  0, batch    47 | loss: 3.4346757MixupTrain:  epoch  0, batch    48 | loss: 3.1726413MixupTrain:  epoch  0, batch    49 | loss: 3.2583609MixupTrain:  epoch  0, batch    50 | loss: 3.4479141MixupTrain:  epoch  0, batch    51 | loss: 3.3372259MixupTrain:  epoch  0, batch    52 | loss: 3.3137569MixupTrain:  epoch  0, batch    53 | loss: 3.3258796MixupTrain:  epoch  0, batch    54 | loss: 3.1680872MixupTrain:  epoch  0, batch    55 | loss: 3.4927998MixupTrain:  epoch  0, batch    56 | loss: 3.2959080MixupTrain:  epoch  0, batch    57 | loss: 3.3993618MixupTrain:  epoch  0, batch    58 | loss: 3.1410773MixupTrain:  epoch  0, batch    59 | loss: 3.1305261MixupTrain:  epoch  0, batch    60 | loss: 3.1376479MixupTrain:  epoch  0, batch    61 | loss: 3.1044602MixupTrain:  epoch  0, batch    62 | loss: 3.2358866MixupTrain:  epoch  0, batch    63 | loss: 3.1058276MixupTrain:  epoch  0, batch    64 | loss: 3.0862308MixupTrain:  epoch  0, batch    65 | loss: 3.3480110MixupTrain:  epoch  0, batch    66 | loss: 3.4292822MixupTrain:  epoch  0, batch    67 | loss: 3.3003178MixupTrain:  epoch  0, batch    68 | loss: 3.1175380MixupTrain:  epoch  0, batch    69 | loss: 3.1430674MixupTrain:  epoch  0, batch    70 | loss: 3.0136197MixupTrain:  epoch  0, batch    71 | loss: 3.1693068MixupTrain:  epoch  0, batch    72 | loss: 3.0120244MixupTrain:  epoch  0, batch    73 | loss: 3.0791166MixupTrain:  epoch  0, batch    74 | loss: 3.3403668MixupTrain:  epoch  0, batch    75 | loss: 3.0076745MixupTrain:  epoch  0, batch    76 | loss: 3.1360362MixupTrain:  epoch  0, batch    77 | loss: 3.2244458MixupTrain:  epoch  0, batch    78 | loss: 2.8733821MixupTrain:  epoch  0, batch    79 | loss: 2.7753873MixupTrain:  epoch  0, batch    80 | loss: 3.1523266MixupTrain:  epoch  0, batch    81 | loss: 3.0952959MixupTrain:  epoch  0, batch    82 | loss: 3.1002231MixupTrain:  epoch  0, batch    83 | loss: 3.1838746MixupTrain:  epoch  0, batch    84 | loss: 2.9416451MixupTrain:  epoch  0, batch    85 | loss: 3.2204270MixupTrain:  epoch  0, batch    86 | loss: 2.8568630MixupTrain:  epoch  0, batch    87 | loss: 2.9919004MixupTrain:  epoch  0, batch    88 | loss: 3.2475820MixupTrain:  epoch  0, batch    89 | loss: 2.8350739MixupTrain:  epoch  0, batch    90 | loss: 3.1694005MixupTrain:  epoch  0, batch    91 | loss: 3.0438709MixupTrain:  epoch  0, batch    92 | loss: 2.9035170MixupTrain:  epoch  0, batch    93 | loss: 2.8657749MixupTrain:  epoch  0, batch    94 | loss: 2.9624524MixupTrain:  epoch  0, batch    95 | loss: 3.0267835MixupTrain:  epoch  0, batch    96 | loss: 3.1241863MixupTrain:  epoch  0, batch    97 | loss: 2.8886662MixupTrain:  epoch  0, batch    98 | loss: 3.2200832MixupTrain:  epoch  0, batch    99 | loss: 2.8605611MixupTrain:  epoch  0, batch   100 | loss: 2.9321785MixupTrain:  epoch  0, batch   101 | loss: 2.8710966MixupTrain:  epoch  0, batch   102 | loss: 3.1676297MixupTrain:  epoch  0, batch   103 | loss: 3.2440906MixupTrain:  epoch  0, batch   104 | loss: 3.0985751MixupTrain:  epoch  0, batch   105 | loss: 3.1397948MixupTrain:  epoch  0, batch   106 | loss: 3.1542578MixupTrain:  epoch  0, batch   107 | loss: 3.1549835MixupTrain:  epoch  0, batch   108 | loss: 3.1832738MixupTrain:  epoch  0, batch   109 | loss: 2.9531143MixupTrain:  epoch  0, batch   110 | loss: 2.9262013MixupTrain:  epoch  0, batch   111 | loss: 3.0034256MixupTrain:  epoch  0, batch   112 | loss: 2.7466063MixupTrain:  epoch  0, batch   113 | loss: 2.8980272MixupTrain:  epoch  0, batch   114 | loss: 3.1363041MixupTrain:  epoch  0, batch   115 | loss: 2.9930561MixupTrain:  epoch  0, batch   116 | loss: 2.9052596MixupTrain:  epoch  0, batch   117 | loss: 3.1892138MixupTrain:  epoch  0, batch   118 | loss: 2.9247105MixupTrain:  epoch  0, batch   119 | loss: 3.1101663MixupTrain:  epoch  0, batch   120 | loss: 2.8291309MixupTrain:  epoch  0, batch   121 | loss: 3.2509472MixupTrain:  epoch  0, batch   122 | loss: 3.1063669MixupTrain:  epoch  0, batch   123 | loss: 3.0410922MixupTrain:  epoch  0, batch   124 | loss: 2.9050941MixupTrain:  epoch  0, batch   125 | loss: 2.9387314MixupTrain:  epoch  0, batch   126 | loss: 2.9681671MixupTrain:  epoch  0, batch   127 | loss: 3.0440068MixupTrain:  epoch  0, batch   128 | loss: 2.9837160MixupTrain:  epoch  0, batch   129 | loss: 3.0503273MixupTrain:  epoch  0, batch   130 | loss: 3.0696468MixupTrain:  epoch  0, batch   131 | loss: 2.9749780MixupTrain:  epoch  0, batch   132 | loss: 2.6354594MixupTrain:  epoch  0, batch   133 | loss: 3.0029714MixupTrain:  epoch  0, batch   134 | loss: 2.9611468MixupTrain:  epoch  0, batch   135 | loss: 2.8925581MixupTrain:  epoch  0, batch   136 | loss: 2.9116094MixupTrain:  epoch  0, batch   137 | loss: 3.1046443MixupTrain:  epoch  0, batch   138 | loss: 3.0757349MixupTrain:  epoch  0, batch   139 | loss: 2.9903367MixupTrain:  epoch  0, batch   140 | loss: 3.2164090MixupTrain:  epoch  0, batch   141 | loss: 3.0870762MixupTrain:  epoch  0, batch   142 | loss: 3.1715384MixupTrain:  epoch  0, batch   143 | loss: 3.0488410MixupTrain:  epoch  0, batch   144 | loss: 3.1194453MixupTrain:  epoch  0, batch   145 | loss: 2.9644103MixupTrain:  epoch  0, batch   146 | loss: 3.0383348MixupTrain:  epoch  0, batch   147 | loss: 2.9398785MixupTrain:  epoch  0, batch   148 | loss: 2.9285631MixupTrain:  epoch  0, batch   149 | loss: 3.1934638MixupTrain:  epoch  0, batch   150 | loss: 2.8932924MixupTrain:  epoch  0, batch   151 | loss: 2.9240263MixupTrain:  epoch  0, batch   152 | loss: 2.9053402MixupTrain:  epoch  0, batch   153 | loss: 3.0092120MixupTrain:  epoch  0, batch   154 | loss: 3.0679889MixupTrain:  epoch  0, batch   155 | loss: 2.7985566MixupTrain:  epoch  0, batch   156 | loss: 2.8456998MixupTrain:  epoch  0, batch   157 | loss: 3.1124797MixupTrain:  epoch  0, batch   158 | loss: 3.0196729MixupTrain:  epoch  0, batch   159 | loss: 2.7507095MixupTrain:  epoch  0, batch   160 | loss: 3.0364308MixupTrain:  epoch  0, batch   161 | loss: 2.9324429MixupTrain:  epoch  0, batch   162 | loss: 2.9152250MixupTrain:  epoch  0, batch   163 | loss: 3.0022697MixupTrain:  epoch  0, batch   164 | loss: 3.1067083MixupTrain:  epoch  0, batch   165 | loss: 3.0906148MixupTrain:  epoch  0, batch   166 | loss: 2.8724213MixupTrain:  epoch  0, batch   167 | loss: 2.8807817MixupTrain:  epoch  0, batch   168 | loss: 2.8146882MixupTrain:  epoch  0, batch   169 | loss: 2.9208493MixupTrain:  epoch  0, batch   170 | loss: 2.9140491MixupTrain:  epoch  0, batch   171 | loss: 2.7849841MixupTrain:  epoch  0, batch   172 | loss: 2.7529495MixupTrain:  epoch  0, batch   173 | loss: 2.7116437MixupTrain:  epoch  0, batch   174 | loss: 2.8406236MixupTrain:  epoch  0, batch   175 | loss: 2.9709287MixupTrain:  epoch  0, batch   176 | loss: 2.8194165MixupTrain:  epoch  0, batch   177 | loss: 2.8580289MixupTrain:  epoch  0, batch   178 | loss: 3.0302296MixupTrain:  epoch  0, batch   179 | loss: 2.9398901MixupTrain:  epoch  0, batch   180 | loss: 3.1167252MixupTrain:  epoch  0, batch   181 | loss: 2.9524336MixupTrain:  epoch  0, batch   182 | loss: 2.9168158MixupTrain:  epoch  0, batch   183 | loss: 2.9850700MixupTrain:  epoch  0, batch   184 | loss: 3.0715055MixupTrain:  epoch  0, batch   185 | loss: 2.7940869MixupTrain:  epoch  0, batch   186 | loss: 2.9475517MixupTrain:  epoch  0, batch   187 | loss: 2.8936601MixupTrain:  epoch  0, batch   188 | loss: 2.9388108MixupTrain:  epoch  0, batch   189 | loss: 2.9054155MixupTrain:  epoch  0, batch   190 | loss: 2.9383545MixupTrain:  epoch  0, batch   191 | loss: 3.1480742MixupTrain:  epoch  0, batch   192 | loss: 2.9762902MixupTrain:  epoch  0, batch   193 | loss: 2.9438095MixupTrain:  epoch  0, batch   194 | loss: 3.0245361MixupTrain:  epoch  0, batch   195 | loss: 3.0746136MixupTrain:  epoch  0, batch   196 | loss: 2.9755373MixupTrain:  epoch  0, batch   197 | loss: 3.0567698MixupTrain:  epoch  0, batch   198 | loss: 3.0399961MixupTrain:  epoch  0, batch   199 | loss: 2.8784168MixupTrain:  epoch  0, batch   200 | loss: 2.7890270MixupTrain:  epoch  0, batch   201 | loss: 2.9117310MixupTrain:  epoch  0, batch   202 | loss: 2.9471936MixupTrain:  epoch  0, batch   203 | loss: 2.9584968MixupTrain:  epoch  0, batch   204 | loss: 3.0720365MixupTrain:  epoch  0, batch   205 | loss: 2.9560208MixupTrain:  epoch  0, batch   206 | loss: 2.7733393MixupTrain:  epoch  0, batch   207 | loss: 2.9415641MixupTrain:  epoch  0, batch   208 | loss: 3.0010533MixupTrain:  epoch  0, batch   209 | loss: 3.0395517MixupTrain:  epoch  0, batch   210 | loss: 2.9908946MixupTrain:  epoch  0, batch   211 | loss: 2.9907935MixupTrain:  epoch  0, batch   212 | loss: 3.1122317MixupTrain:  epoch  0, batch   213 | loss: 2.9402628MixupTrain:  epoch  0, batch   214 | loss: 2.9863281MixupTrain:  epoch  0, batch   215 | loss: 2.8610005MixupTrain:  epoch  0, batch   216 | loss: 3.0109141MixupTrain:  epoch  0, batch   217 | loss: 2.9175999MixupTrain:  epoch  0, batch   218 | loss: 2.8919580MixupTrain:  epoch  0, batch   219 | loss: 2.7910037MixupTrain:  epoch  0, batch   220 | loss: 2.8446164MixupTrain:  epoch  0, batch   221 | loss: 2.9993086MixupTrain:  epoch  0, batch   222 | loss: 2.8298905MixupTrain:  epoch  0, batch   223 | loss: 2.9256797MixupTrain:  epoch  0, batch   224 | loss: 2.9710579MixupTrain:  epoch  0, batch   225 | loss: 3.1222601MixupTrain:  epoch  0, batch   226 | loss: 2.9067814MixupTrain:  epoch  0, batch   227 | loss: 2.8205714MixupTrain:  epoch  0, batch   228 | loss: 2.8018885MixupTrain:  epoch  0, batch   229 | loss: 2.9147429MixupTrain:  epoch  0, batch   230 | loss: 2.8310838MixupTrain:  epoch  0, batch   231 | loss: 2.8857398MixupTrain:  epoch  0, batch   232 | loss: 2.8526609MixupTrain:  epoch  0, batch   233 | loss: 3.0608020MixupTrain:  epoch  0, batch   234 | loss: 2.9094307MixupTrain:  epoch  0, batch   235 | loss: 2.9223561MixupTrain:  epoch  0, batch   236 | loss: 2.9750905MixupTrain:  epoch  0, batch   237 | loss: 3.0386014MixupTrain:  epoch  0, batch   238 | loss: 2.9792199MixupTrain:  epoch  0, batch   239 | loss: 2.7569709MixupTrain:  epoch  0, batch   240 | loss: 3.0874729MixupTrain:  epoch  0, batch   241 | loss: 2.8162737MixupTrain:  epoch  0, batch   242 | loss: 3.0697663MixupTrain:  epoch  0, batch   243 | loss: 2.9050968MixupTrain:  epoch  0, batch   244 | loss: 3.0975437MixupTrain:  epoch  0, batch   245 | loss: 2.9497473MixupTrain:  epoch  0, batch   246 | loss: 2.9183295MixupTrain:  epoch  0, batch   247 | loss: 2.8985279MixupTrain:  epoch  0, batch   248 | loss: 2.8460574MixupTrain:  epoch  0, batch   249 | loss: 2.9611073MixupTrain:  epoch  0, batch   250 | loss: 2.9033432MixupTrain:  epoch  0, batch   251 | loss: 2.8156445MixupTrain:  epoch  0, batch   252 | loss: 2.8684130MixupTrain:  epoch  0, batch   253 | loss: 2.9771953MixupTrain:  epoch  0, batch   254 | loss: 2.9142919MixupTrain:  epoch  0, batch   255 | loss: 3.0282617MixupTrain:  epoch  0, batch   256 | loss: 2.8051429MixupTrain:  epoch  0, batch   257 | loss: 2.9991789MixupTrain:  epoch  0, batch   258 | loss: 2.8685842MixupTrain:  epoch  0, batch   259 | loss: 2.9934788MixupTrain:  epoch  0, batch   260 | loss: 2.8252108MixupTrain:  epoch  0, batch   261 | loss: 2.8048859MixupTrain:  epoch  0, batch   262 | loss: 2.9532175MixupTrain:  epoch  0, batch   263 | loss: 2.8310773MixupTrain:  epoch  0, batch   264 | loss: 2.9440084MixupTrain:  epoch  0, batch   265 | loss: 2.7829778MixupTrain:  epoch  0, batch   266 | loss: 2.8881257MixupTrain:  epoch  0, batch   267 | loss: 2.9591551MixupTrain:  epoch  0, batch   268 | loss: 3.0103726MixupTrain:  epoch  0, batch   269 | loss: 2.9071333MixupTrain:  epoch  0, batch   270 | loss: 2.9452002MixupTrain:  epoch  0, batch   271 | loss: 2.8576155MixupTrain:  epoch  0, batch   272 | loss: 3.0573130MixupTrain:  epoch  0, batch   273 | loss: 3.0562067MixupTrain:  epoch  0, batch   274 | loss: 2.8249416MixupTrain:  epoch  0, batch   275 | loss: 2.8246729MixupTrain:  epoch  0, batch   276 | loss: 2.9868112MixupTrain:  epoch  0, batch   277 | loss: 2.9982028MixupTrain:  epoch  0, batch   278 | loss: 2.9202991MixupTrain:  epoch  0, batch   279 | loss: 2.9011269MixupTrain:  epoch  0, batch   280 | loss: 2.9688249MixupTrain:  epoch  0, batch   281 | loss: 2.9045677MixupTrain:  epoch  0, batch   282 | loss: 2.8912940MixupTrain:  epoch  0, batch   283 | loss: 2.7907848MixupTrain:  epoch  0, batch   284 | loss: 2.8454442MixupTrain:  epoch  0, batch   285 | loss: 2.9294195MixupTrain:  epoch  0, batch   286 | loss: 3.0759726MixupTrain:  epoch  0, batch   287 | loss: 2.9207742MixupTrain:  epoch  0, batch   288 | loss: 2.7822359MixupTrain:  epoch  0, batch   289 | loss: 2.8264637MixupTrain:  epoch  0, batch   290 | loss: 2.7663848MixupTrain:  epoch  0, batch   291 | loss: 2.9253576MixupTrain:  epoch  0, batch   292 | loss: 2.8802195MixupTrain:  epoch  0, batch   293 | loss: 2.8216882MixupTrain:  epoch  0, batch   294 | loss: 2.9315581MixupTrain:  epoch  0, batch   295 | loss: 2.9397917MixupTrain:  epoch  0, batch   296 | loss: 2.8938670MixupTrain:  epoch  0, batch   297 | loss: 2.7654769MixupTrain:  epoch  0, batch   298 | loss: 2.8027420MixupTrain:  epoch  0, batch   299 | loss: 2.8441992MixupTrain:  epoch  0, batch   300 | loss: 2.7069757MixupTrain:  epoch  0, batch   301 | loss: 2.9541907MixupTrain:  epoch  0, batch   302 | loss: 2.7411201MixupTrain:  epoch  0, batch   303 | loss: 2.9150333MixupTrain:  epoch  0, batch   304 | loss: 2.9000754MixupTrain:  epoch  0, batch   305 | loss: 2.6986589MixupTrain:  epoch  0, batch   306 | loss: 3.0132613MixupTrain:  epoch  0, batch   307 | loss: 2.9317851MixupTrain:  epoch  0, batch   308 | loss: 2.9148095MixupTrain:  epoch  0, batch   309 | loss: 2.8517685MixupTrain:  epoch  0, batch   310 | loss: 3.0430222MixupTrain:  epoch  0, batch   311 | loss: 3.0159879MixupTrain:  epoch  0, batch   312 | loss: 2.8803592MixupTrain:  epoch  0, batch   313 | loss: 2.8567657MixupTrain:  epoch  0, batch   314 | loss: 2.9580286MixupTrain:  epoch  0, batch   315 | loss: 2.7805781
MemoryTrain:  epoch  0, batch     0 | loss: 1.2533792MemoryTrain:  epoch  0, batch     1 | loss: 1.7617360MemoryTrain:  epoch  0, batch     2 | loss: 1.7529644MemoryTrain:  epoch  0, batch     3 | loss: 2.1247616MemoryTrain:  epoch  0, batch     4 | loss: 2.0073521MemoryTrain:  epoch  1, batch     0 | loss: 1.6390949MemoryTrain:  epoch  1, batch     1 | loss: 1.6604859MemoryTrain:  epoch  1, batch     2 | loss: 1.4907346MemoryTrain:  epoch  1, batch     3 | loss: 1.5097106MemoryTrain:  epoch  1, batch     4 | loss: 1.6297742MemoryTrain:  epoch  2, batch     0 | loss: 1.2923198MemoryTrain:  epoch  2, batch     1 | loss: 1.3237238MemoryTrain:  epoch  2, batch     2 | loss: 1.3835027MemoryTrain:  epoch  2, batch     3 | loss: 1.3553762MemoryTrain:  epoch  2, batch     4 | loss: 1.3113134MemoryTrain:  epoch  3, batch     0 | loss: 1.3201873MemoryTrain:  epoch  3, batch     1 | loss: 1.3687162MemoryTrain:  epoch  3, batch     2 | loss: 1.2455101MemoryTrain:  epoch  3, batch     3 | loss: 1.2535605MemoryTrain:  epoch  3, batch     4 | loss: 1.3522084MemoryTrain:  epoch  4, batch     0 | loss: 1.3840144MemoryTrain:  epoch  4, batch     1 | loss: 1.3627748MemoryTrain:  epoch  4, batch     2 | loss: 1.2433403MemoryTrain:  epoch  4, batch     3 | loss: 1.2570550MemoryTrain:  epoch  4, batch     4 | loss: 1.2241662MemoryTrain:  epoch  5, batch     0 | loss: 1.2293589MemoryTrain:  epoch  5, batch     1 | loss: 1.2797312MemoryTrain:  epoch  5, batch     2 | loss: 1.3150344MemoryTrain:  epoch  5, batch     3 | loss: 1.2579150MemoryTrain:  epoch  5, batch     4 | loss: 1.2553557MemoryTrain:  epoch  6, batch     0 | loss: 1.2493041MemoryTrain:  epoch  6, batch     1 | loss: 1.2674366MemoryTrain:  epoch  6, batch     2 | loss: 1.2066880MemoryTrain:  epoch  6, batch     3 | loss: 1.2449788MemoryTrain:  epoch  6, batch     4 | loss: 1.1972315MemoryTrain:  epoch  7, batch     0 | loss: 1.3137549MemoryTrain:  epoch  7, batch     1 | loss: 1.2555437MemoryTrain:  epoch  7, batch     2 | loss: 1.2151302MemoryTrain:  epoch  7, batch     3 | loss: 1.2638526MemoryTrain:  epoch  7, batch     4 | loss: 1.2752246MemoryTrain:  epoch  8, batch     0 | loss: 1.2419353MemoryTrain:  epoch  8, batch     1 | loss: 1.2201989MemoryTrain:  epoch  8, batch     2 | loss: 1.2772453MemoryTrain:  epoch  8, batch     3 | loss: 1.2401797MemoryTrain:  epoch  8, batch     4 | loss: 1.1657616MemoryTrain:  epoch  9, batch     0 | loss: 1.2234998MemoryTrain:  epoch  9, batch     1 | loss: 1.1952950MemoryTrain:  epoch  9, batch     2 | loss: 1.2288737MemoryTrain:  epoch  9, batch     3 | loss: 1.2048869MemoryTrain:  epoch  9, batch     4 | loss: 1.2046084
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 82.81%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 82.50%   [EVAL] batch:    5 | acc: 56.25%,  total acc: 78.12%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 79.46%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 81.94%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 83.12%   [EVAL] batch:   10 | acc: 75.00%,  total acc: 82.39%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 83.85%   [EVAL] batch:   12 | acc: 100.00%,  total acc: 85.10%   [EVAL] batch:   13 | acc: 100.00%,  total acc: 86.16%   [EVAL] batch:   14 | acc: 100.00%,  total acc: 87.08%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 87.89%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 88.60%   [EVAL] batch:   17 | acc: 25.00%,  total acc: 85.07%   
[EVAL] batch:    0 | acc: 25.00%,  total acc: 25.00%   [EVAL] batch:    1 | acc: 37.50%,  total acc: 31.25%   [EVAL] batch:    2 | acc: 31.25%,  total acc: 31.25%   [EVAL] batch:    3 | acc: 18.75%,  total acc: 28.12%   [EVAL] batch:    4 | acc: 25.00%,  total acc: 27.50%   [EVAL] batch:    5 | acc: 31.25%,  total acc: 28.12%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 34.82%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 42.97%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 49.31%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 53.12%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 57.39%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 60.42%   [EVAL] batch:   12 | acc: 56.25%,  total acc: 60.10%   [EVAL] batch:   13 | acc: 37.50%,  total acc: 58.48%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 59.58%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 59.38%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 60.29%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 60.42%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 60.53%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 61.88%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 63.69%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 64.77%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 66.03%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 67.19%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 68.50%   [EVAL] batch:   25 | acc: 87.50%,  total acc: 69.23%   [EVAL] batch:   26 | acc: 75.00%,  total acc: 69.44%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 70.54%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 71.34%   [EVAL] batch:   29 | acc: 68.75%,  total acc: 71.25%   [EVAL] batch:   30 | acc: 81.25%,  total acc: 71.57%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 72.27%   [EVAL] batch:   32 | acc: 43.75%,  total acc: 71.40%   [EVAL] batch:   33 | acc: 43.75%,  total acc: 70.59%   [EVAL] batch:   34 | acc: 50.00%,  total acc: 70.00%   [EVAL] batch:   35 | acc: 37.50%,  total acc: 69.10%   [EVAL] batch:   36 | acc: 75.00%,  total acc: 69.26%   [EVAL] batch:   37 | acc: 68.75%,  total acc: 69.24%   [EVAL] batch:   38 | acc: 43.75%,  total acc: 68.59%   [EVAL] batch:   39 | acc: 31.25%,  total acc: 67.66%   [EVAL] batch:   40 | acc: 0.00%,  total acc: 66.01%   [EVAL] batch:   41 | acc: 0.00%,  total acc: 64.43%   [EVAL] batch:   42 | acc: 0.00%,  total acc: 62.94%   [EVAL] batch:   43 | acc: 0.00%,  total acc: 61.51%   [EVAL] batch:   44 | acc: 31.25%,  total acc: 60.83%   [EVAL] batch:   45 | acc: 43.75%,  total acc: 60.46%   [EVAL] batch:   46 | acc: 25.00%,  total acc: 59.71%   [EVAL] batch:   47 | acc: 18.75%,  total acc: 58.85%   [EVAL] batch:   48 | acc: 62.50%,  total acc: 58.93%   [EVAL] batch:   49 | acc: 25.00%,  total acc: 58.25%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 59.07%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 59.86%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 60.50%   [EVAL] batch:   53 | acc: 87.50%,  total acc: 61.00%   [EVAL] batch:   54 | acc: 68.75%,  total acc: 61.14%   [EVAL] batch:   55 | acc: 12.50%,  total acc: 60.27%   [EVAL] batch:   56 | acc: 0.00%,  total acc: 59.21%   [EVAL] batch:   57 | acc: 0.00%,  total acc: 58.19%   [EVAL] batch:   58 | acc: 0.00%,  total acc: 57.20%   [EVAL] batch:   59 | acc: 12.50%,  total acc: 56.46%   [EVAL] batch:   60 | acc: 37.50%,  total acc: 56.15%   [EVAL] batch:   61 | acc: 25.00%,  total acc: 55.65%   [EVAL] batch:   62 | acc: 25.00%,  total acc: 55.16%   [EVAL] batch:   63 | acc: 25.00%,  total acc: 54.69%   [EVAL] batch:   64 | acc: 31.25%,  total acc: 54.33%   [EVAL] batch:   65 | acc: 43.75%,  total acc: 54.17%   [EVAL] batch:   66 | acc: 50.00%,  total acc: 54.10%   [EVAL] batch:   67 | acc: 25.00%,  total acc: 53.68%   [EVAL] batch:   68 | acc: 43.75%,  total acc: 53.53%   [EVAL] batch:   69 | acc: 68.75%,  total acc: 53.75%   [EVAL] batch:   70 | acc: 50.00%,  total acc: 53.70%   [EVAL] batch:   71 | acc: 56.25%,  total acc: 53.73%   [EVAL] batch:   72 | acc: 62.50%,  total acc: 53.85%   [EVAL] batch:   73 | acc: 100.00%,  total acc: 54.48%   [EVAL] batch:   74 | acc: 93.75%,  total acc: 55.00%   [EVAL] batch:   75 | acc: 100.00%,  total acc: 55.59%   [EVAL] batch:   76 | acc: 81.25%,  total acc: 55.93%   [EVAL] batch:   77 | acc: 37.50%,  total acc: 55.69%   [EVAL] batch:   78 | acc: 25.00%,  total acc: 55.30%   [EVAL] batch:   79 | acc: 31.25%,  total acc: 55.00%   [EVAL] batch:   80 | acc: 37.50%,  total acc: 54.78%   [EVAL] batch:   81 | acc: 87.50%,  total acc: 55.18%   [EVAL] batch:   82 | acc: 62.50%,  total acc: 55.27%   [EVAL] batch:   83 | acc: 93.75%,  total acc: 55.73%   [EVAL] batch:   84 | acc: 75.00%,  total acc: 55.96%   [EVAL] batch:   85 | acc: 81.25%,  total acc: 56.25%   [EVAL] batch:   86 | acc: 62.50%,  total acc: 56.32%   [EVAL] batch:   87 | acc: 93.75%,  total acc: 56.75%   [EVAL] batch:   88 | acc: 93.75%,  total acc: 57.16%   [EVAL] batch:   89 | acc: 87.50%,  total acc: 57.50%   [EVAL] batch:   90 | acc: 87.50%,  total acc: 57.83%   [EVAL] batch:   91 | acc: 81.25%,  total acc: 58.08%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 58.53%   [EVAL] batch:   93 | acc: 100.00%,  total acc: 58.98%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 59.41%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 59.83%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 60.24%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 60.65%   [EVAL] batch:   98 | acc: 6.25%,  total acc: 60.10%   
cur_acc:  ['0.8655', '0.7788', '0.7000', '0.6023', '0.8507']
his_acc:  ['0.8655', '0.8472', '0.7833', '0.6728', '0.6010']
CurrentTrain: epoch  0, batch     0 | loss: 3.9669976CurrentTrain: epoch  0, batch     1 | loss: 5.4642210CurrentTrain: epoch  1, batch     0 | loss: 3.6305718CurrentTrain: epoch  1, batch     1 | loss: 3.9457414CurrentTrain: epoch  2, batch     0 | loss: 3.1185799CurrentTrain: epoch  2, batch     1 | loss: 2.7072339CurrentTrain: epoch  3, batch     0 | loss: 2.5391717CurrentTrain: epoch  3, batch     1 | loss: 2.3038182CurrentTrain: epoch  4, batch     0 | loss: 2.3274701CurrentTrain: epoch  4, batch     1 | loss: 2.1724367CurrentTrain: epoch  5, batch     0 | loss: 2.1236377CurrentTrain: epoch  5, batch     1 | loss: 2.1342080CurrentTrain: epoch  6, batch     0 | loss: 2.1095228CurrentTrain: epoch  6, batch     1 | loss: 1.9977608CurrentTrain: epoch  7, batch     0 | loss: 2.0012162CurrentTrain: epoch  7, batch     1 | loss: 2.0821042CurrentTrain: epoch  8, batch     0 | loss: 1.9454386CurrentTrain: epoch  8, batch     1 | loss: 1.8806577CurrentTrain: epoch  9, batch     0 | loss: 1.8270659CurrentTrain: epoch  9, batch     1 | loss: 1.8936669
#### organization founded ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization founded" means an organization was found in a date.
Here is an example:
Relation: organization founded
Context: the jnf was founded in 1901 to buy plots in palestine , then ruled by the ottomans .
Head Entity: jnf
Tail Entity: 1901
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization founded":
Relation: organization founded  
Context: the united nations was established in 1945 to promote international cooperation and peace.  
Head Entity: united nations  
Tail Entity: 1945  

Relation: organization founded  
Context: the world health organization was created in 1948 to coordinate global health efforts.  
Head Entity: world health organization  
Tail Entity: 1948  
#### person age ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person age" means a person 's age is a number.
Here is an example:
Relation: person age
Context: paris , feb 23 -lrb- xinhua -rrb- yoadimnadji , 56 , died of a cardiovascular problem at midnight .
Head Entity: yoadimnadji
Tail Entity: 56
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person age":
Relation: person age  
Context: In a recent interview, the renowned author, Jane Doe, revealed that she is 34 years old and still has many stories to tell.  
Head Entity: Jane Doe  
Tail Entity: 34  

Relation: person age  
Context: During the family reunion, Uncle Bob proudly announced that he had just turned 70, sharing his plans for retirement.  
Head Entity: Uncle Bob  
Tail Entity: 70  
#### person city of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person city of birth" means a person was born in a city.
Here is an example:
Relation: person city of birth
Context: rothman was born in san francisco in 1932 in an orthodox jewish family .
Head Entity: rothman
Tail Entity: san francisco
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person city of birth":
Relation: person city of birth  
Context: after spending his early years in new york, he moved to los angeles where he began his career.  
Head Entity: he  
Tail Entity: los angeles  

Relation: person city of birth  
Context: the famous author was born in a small town near boston, which greatly influenced his writing.  
Head Entity: the famous author  
Tail Entity: boston  
#### organization members ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization members" means an organization 's member is an organization.
Here is an example:
Relation: organization members
Context: sun plays for the grand rapids flight of the international basketball league after toiling for the maryland nighthawks of the american basketball association , both development leagues for those who dream of an nba career .
Head Entity: american basketball association
Tail Entity: maryland nighthawks
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization members":
Relation: organization members  
Context: The New York Philharmonic is one of the oldest orchestras in the United States, and it has had many notable musicians as members, including the famous conductor Leonard Bernstein.  
Head Entity: New York Philharmonic  
Tail Entity: Leonard Bernstein  

Relation: organization members  
Context: The National Football League has a long history of legendary players, and one of its most famous members is Joe Montana, who played for the San Francisco 49ers.  
Head Entity: National Football League  
Tail Entity: Joe Montana  
#### person religion ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person religion" means an person is the member of a religion.
Here is an example:
Relation: person religion
Context: the pope defended his action on the grounds that he could not refuse an audience to a head of state from a country with a strong catholic tradition unless he had clear-cut proof of the allegations against him .
Head Entity: he
Tail Entity: catholic
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person religion":
Relation: person religion  
Context: During the ceremony, the rabbi spoke about the importance of faith and community in Judaism, emphasizing how each member contributes to the collective spirit.  
Head Entity: rabbi  
Tail Entity: Judaism  

Relation: person religion  
Context: The imam led the prayers at the mosque, inspiring the congregation with his deep knowledge of the Quran and the teachings of Islam.  
Head Entity: imam  
Tail Entity: Islam  
Mixup data size:  6685
MixupTrain:  epoch  0, batch     0 | loss: 3.6162353MixupTrain:  epoch  0, batch     1 | loss: 3.4991970MixupTrain:  epoch  0, batch     2 | loss: 3.5388155MixupTrain:  epoch  0, batch     3 | loss: 3.6234567MixupTrain:  epoch  0, batch     4 | loss: 3.4740396MixupTrain:  epoch  0, batch     5 | loss: 3.0908995MixupTrain:  epoch  0, batch     6 | loss: 2.9510889MixupTrain:  epoch  0, batch     7 | loss: 3.2506208MixupTrain:  epoch  0, batch     8 | loss: 3.3433151MixupTrain:  epoch  0, batch     9 | loss: 3.2811780MixupTrain:  epoch  0, batch    10 | loss: 3.4022706MixupTrain:  epoch  0, batch    11 | loss: 2.9942403MixupTrain:  epoch  0, batch    12 | loss: 3.4232566MixupTrain:  epoch  0, batch    13 | loss: 3.3979239MixupTrain:  epoch  0, batch    14 | loss: 3.1731410MixupTrain:  epoch  0, batch    15 | loss: 3.2833040MixupTrain:  epoch  0, batch    16 | loss: 3.3771400MixupTrain:  epoch  0, batch    17 | loss: 3.3184309MixupTrain:  epoch  0, batch    18 | loss: 3.2724414MixupTrain:  epoch  0, batch    19 | loss: 3.1104431MixupTrain:  epoch  0, batch    20 | loss: 3.2322323MixupTrain:  epoch  0, batch    21 | loss: 3.2093139MixupTrain:  epoch  0, batch    22 | loss: 3.2952967MixupTrain:  epoch  0, batch    23 | loss: 3.3056326MixupTrain:  epoch  0, batch    24 | loss: 2.9311986MixupTrain:  epoch  0, batch    25 | loss: 3.2927444MixupTrain:  epoch  0, batch    26 | loss: 2.9856474MixupTrain:  epoch  0, batch    27 | loss: 2.9594936MixupTrain:  epoch  0, batch    28 | loss: 3.1725285MixupTrain:  epoch  0, batch    29 | loss: 3.2226202MixupTrain:  epoch  0, batch    30 | loss: 3.1920395MixupTrain:  epoch  0, batch    31 | loss: 3.3086541MixupTrain:  epoch  0, batch    32 | loss: 3.0268369MixupTrain:  epoch  0, batch    33 | loss: 2.8766210MixupTrain:  epoch  0, batch    34 | loss: 3.1086287MixupTrain:  epoch  0, batch    35 | loss: 3.2045083MixupTrain:  epoch  0, batch    36 | loss: 3.2314804MixupTrain:  epoch  0, batch    37 | loss: 3.0748591MixupTrain:  epoch  0, batch    38 | loss: 3.0524499MixupTrain:  epoch  0, batch    39 | loss: 2.9873745MixupTrain:  epoch  0, batch    40 | loss: 3.2162137MixupTrain:  epoch  0, batch    41 | loss: 2.8912420MixupTrain:  epoch  0, batch    42 | loss: 2.8468447MixupTrain:  epoch  0, batch    43 | loss: 2.8362799MixupTrain:  epoch  0, batch    44 | loss: 3.1541262MixupTrain:  epoch  0, batch    45 | loss: 3.0465550MixupTrain:  epoch  0, batch    46 | loss: 3.1162221MixupTrain:  epoch  0, batch    47 | loss: 2.8669813MixupTrain:  epoch  0, batch    48 | loss: 2.7916102MixupTrain:  epoch  0, batch    49 | loss: 2.9470630MixupTrain:  epoch  0, batch    50 | loss: 3.1864262MixupTrain:  epoch  0, batch    51 | loss: 3.0549073MixupTrain:  epoch  0, batch    52 | loss: 2.9001365MixupTrain:  epoch  0, batch    53 | loss: 3.0121121MixupTrain:  epoch  0, batch    54 | loss: 2.9600921MixupTrain:  epoch  0, batch    55 | loss: 3.0805109MixupTrain:  epoch  0, batch    56 | loss: 2.7846520MixupTrain:  epoch  0, batch    57 | loss: 3.0178857MixupTrain:  epoch  0, batch    58 | loss: 2.8272982MixupTrain:  epoch  0, batch    59 | loss: 2.8777194MixupTrain:  epoch  0, batch    60 | loss: 2.9991505MixupTrain:  epoch  0, batch    61 | loss: 2.9698558MixupTrain:  epoch  0, batch    62 | loss: 3.1176424MixupTrain:  epoch  0, batch    63 | loss: 2.8411880MixupTrain:  epoch  0, batch    64 | loss: 2.8813987MixupTrain:  epoch  0, batch    65 | loss: 3.0747964MixupTrain:  epoch  0, batch    66 | loss: 3.0491176MixupTrain:  epoch  0, batch    67 | loss: 3.0225592MixupTrain:  epoch  0, batch    68 | loss: 3.1215720MixupTrain:  epoch  0, batch    69 | loss: 3.0780554MixupTrain:  epoch  0, batch    70 | loss: 3.0620191MixupTrain:  epoch  0, batch    71 | loss: 3.2148581MixupTrain:  epoch  0, batch    72 | loss: 2.9293244MixupTrain:  epoch  0, batch    73 | loss: 3.0650461MixupTrain:  epoch  0, batch    74 | loss: 3.0235319MixupTrain:  epoch  0, batch    75 | loss: 2.9428558MixupTrain:  epoch  0, batch    76 | loss: 2.7639241MixupTrain:  epoch  0, batch    77 | loss: 2.9197221MixupTrain:  epoch  0, batch    78 | loss: 2.7827244MixupTrain:  epoch  0, batch    79 | loss: 2.9657323MixupTrain:  epoch  0, batch    80 | loss: 2.9716504MixupTrain:  epoch  0, batch    81 | loss: 2.8843651MixupTrain:  epoch  0, batch    82 | loss: 2.8728335MixupTrain:  epoch  0, batch    83 | loss: 2.9097798MixupTrain:  epoch  0, batch    84 | loss: 3.0030665MixupTrain:  epoch  0, batch    85 | loss: 2.9781508MixupTrain:  epoch  0, batch    86 | loss: 2.9754989MixupTrain:  epoch  0, batch    87 | loss: 3.1052089MixupTrain:  epoch  0, batch    88 | loss: 3.0170577MixupTrain:  epoch  0, batch    89 | loss: 2.7466900MixupTrain:  epoch  0, batch    90 | loss: 3.1608026MixupTrain:  epoch  0, batch    91 | loss: 2.8631089MixupTrain:  epoch  0, batch    92 | loss: 2.9365051MixupTrain:  epoch  0, batch    93 | loss: 2.9301205MixupTrain:  epoch  0, batch    94 | loss: 2.9456167MixupTrain:  epoch  0, batch    95 | loss: 2.9859204MixupTrain:  epoch  0, batch    96 | loss: 3.0641534MixupTrain:  epoch  0, batch    97 | loss: 2.9079175MixupTrain:  epoch  0, batch    98 | loss: 3.0044909MixupTrain:  epoch  0, batch    99 | loss: 3.0538249MixupTrain:  epoch  0, batch   100 | loss: 2.7663724MixupTrain:  epoch  0, batch   101 | loss: 2.8088601MixupTrain:  epoch  0, batch   102 | loss: 2.9513569MixupTrain:  epoch  0, batch   103 | loss: 2.8014627MixupTrain:  epoch  0, batch   104 | loss: 3.0105364MixupTrain:  epoch  0, batch   105 | loss: 2.7793250MixupTrain:  epoch  0, batch   106 | loss: 2.8380957MixupTrain:  epoch  0, batch   107 | loss: 2.9004703MixupTrain:  epoch  0, batch   108 | loss: 2.7876482MixupTrain:  epoch  0, batch   109 | loss: 2.8518441MixupTrain:  epoch  0, batch   110 | loss: 2.9232893MixupTrain:  epoch  0, batch   111 | loss: 2.9426227MixupTrain:  epoch  0, batch   112 | loss: 2.8737938MixupTrain:  epoch  0, batch   113 | loss: 2.9565330MixupTrain:  epoch  0, batch   114 | loss: 2.9631851MixupTrain:  epoch  0, batch   115 | loss: 2.8516121MixupTrain:  epoch  0, batch   116 | loss: 2.8341637MixupTrain:  epoch  0, batch   117 | loss: 2.9827867MixupTrain:  epoch  0, batch   118 | loss: 2.8426905MixupTrain:  epoch  0, batch   119 | loss: 2.9602461MixupTrain:  epoch  0, batch   120 | loss: 2.9505892MixupTrain:  epoch  0, batch   121 | loss: 2.7586687MixupTrain:  epoch  0, batch   122 | loss: 2.9088206MixupTrain:  epoch  0, batch   123 | loss: 2.8518524MixupTrain:  epoch  0, batch   124 | loss: 2.8923059MixupTrain:  epoch  0, batch   125 | loss: 2.9939556MixupTrain:  epoch  0, batch   126 | loss: 2.9235382MixupTrain:  epoch  0, batch   127 | loss: 2.7837377MixupTrain:  epoch  0, batch   128 | loss: 2.9550726MixupTrain:  epoch  0, batch   129 | loss: 2.7999203MixupTrain:  epoch  0, batch   130 | loss: 2.9106503MixupTrain:  epoch  0, batch   131 | loss: 2.9205992MixupTrain:  epoch  0, batch   132 | loss: 3.0314732MixupTrain:  epoch  0, batch   133 | loss: 2.9185147MixupTrain:  epoch  0, batch   134 | loss: 2.8875155MixupTrain:  epoch  0, batch   135 | loss: 2.8166845MixupTrain:  epoch  0, batch   136 | loss: 2.9607482MixupTrain:  epoch  0, batch   137 | loss: 2.8564205MixupTrain:  epoch  0, batch   138 | loss: 2.8635807MixupTrain:  epoch  0, batch   139 | loss: 2.9160106MixupTrain:  epoch  0, batch   140 | loss: 2.9603605MixupTrain:  epoch  0, batch   141 | loss: 2.8882449MixupTrain:  epoch  0, batch   142 | loss: 2.6865952MixupTrain:  epoch  0, batch   143 | loss: 2.7113063MixupTrain:  epoch  0, batch   144 | loss: 2.9039214MixupTrain:  epoch  0, batch   145 | loss: 2.9708309MixupTrain:  epoch  0, batch   146 | loss: 2.8846653MixupTrain:  epoch  0, batch   147 | loss: 2.7294478MixupTrain:  epoch  0, batch   148 | loss: 2.9174199MixupTrain:  epoch  0, batch   149 | loss: 2.9143271MixupTrain:  epoch  0, batch   150 | loss: 3.1502669MixupTrain:  epoch  0, batch   151 | loss: 2.8331928MixupTrain:  epoch  0, batch   152 | loss: 3.0586658MixupTrain:  epoch  0, batch   153 | loss: 2.9230556MixupTrain:  epoch  0, batch   154 | loss: 2.8748438MixupTrain:  epoch  0, batch   155 | loss: 2.9075489MixupTrain:  epoch  0, batch   156 | loss: 2.7975864MixupTrain:  epoch  0, batch   157 | loss: 2.8026819MixupTrain:  epoch  0, batch   158 | loss: 3.0248709MixupTrain:  epoch  0, batch   159 | loss: 2.8879559MixupTrain:  epoch  0, batch   160 | loss: 2.8926873MixupTrain:  epoch  0, batch   161 | loss: 2.9213223MixupTrain:  epoch  0, batch   162 | loss: 2.9067183MixupTrain:  epoch  0, batch   163 | loss: 2.8993630MixupTrain:  epoch  0, batch   164 | loss: 2.8301532MixupTrain:  epoch  0, batch   165 | loss: 2.9047680MixupTrain:  epoch  0, batch   166 | loss: 2.8583269MixupTrain:  epoch  0, batch   167 | loss: 2.8115168MixupTrain:  epoch  0, batch   168 | loss: 2.8414378MixupTrain:  epoch  0, batch   169 | loss: 2.9531260MixupTrain:  epoch  0, batch   170 | loss: 2.9068396MixupTrain:  epoch  0, batch   171 | loss: 2.9510734MixupTrain:  epoch  0, batch   172 | loss: 2.8365157MixupTrain:  epoch  0, batch   173 | loss: 2.8158302MixupTrain:  epoch  0, batch   174 | loss: 2.8047452MixupTrain:  epoch  0, batch   175 | loss: 2.8570080MixupTrain:  epoch  0, batch   176 | loss: 2.8324060MixupTrain:  epoch  0, batch   177 | loss: 2.9223464MixupTrain:  epoch  0, batch   178 | loss: 2.8968291MixupTrain:  epoch  0, batch   179 | loss: 2.9829574MixupTrain:  epoch  0, batch   180 | loss: 2.8526459MixupTrain:  epoch  0, batch   181 | loss: 2.7931447MixupTrain:  epoch  0, batch   182 | loss: 2.9244418MixupTrain:  epoch  0, batch   183 | loss: 2.9234595MixupTrain:  epoch  0, batch   184 | loss: 2.7853808MixupTrain:  epoch  0, batch   185 | loss: 2.9295633MixupTrain:  epoch  0, batch   186 | loss: 2.7884333MixupTrain:  epoch  0, batch   187 | loss: 2.8835211MixupTrain:  epoch  0, batch   188 | loss: 2.9685690MixupTrain:  epoch  0, batch   189 | loss: 2.7891655MixupTrain:  epoch  0, batch   190 | loss: 2.7737150MixupTrain:  epoch  0, batch   191 | loss: 2.8887579MixupTrain:  epoch  0, batch   192 | loss: 2.7761843MixupTrain:  epoch  0, batch   193 | loss: 2.8927665MixupTrain:  epoch  0, batch   194 | loss: 2.7801991MixupTrain:  epoch  0, batch   195 | loss: 2.8055050MixupTrain:  epoch  0, batch   196 | loss: 2.9673820MixupTrain:  epoch  0, batch   197 | loss: 2.7050753MixupTrain:  epoch  0, batch   198 | loss: 2.8390012MixupTrain:  epoch  0, batch   199 | loss: 2.7893438MixupTrain:  epoch  0, batch   200 | loss: 2.8605359MixupTrain:  epoch  0, batch   201 | loss: 2.7432075MixupTrain:  epoch  0, batch   202 | loss: 2.8168032MixupTrain:  epoch  0, batch   203 | loss: 2.8334215MixupTrain:  epoch  0, batch   204 | loss: 2.8154569MixupTrain:  epoch  0, batch   205 | loss: 2.9754953MixupTrain:  epoch  0, batch   206 | loss: 2.7462935MixupTrain:  epoch  0, batch   207 | loss: 2.8979750MixupTrain:  epoch  0, batch   208 | loss: 2.9812965MixupTrain:  epoch  0, batch   209 | loss: 2.9134083MixupTrain:  epoch  0, batch   210 | loss: 2.7775393MixupTrain:  epoch  0, batch   211 | loss: 2.8070569MixupTrain:  epoch  0, batch   212 | loss: 2.8531101MixupTrain:  epoch  0, batch   213 | loss: 2.8453188MixupTrain:  epoch  0, batch   214 | loss: 2.8165751MixupTrain:  epoch  0, batch   215 | loss: 2.8245866MixupTrain:  epoch  0, batch   216 | loss: 2.7606015MixupTrain:  epoch  0, batch   217 | loss: 2.8289142MixupTrain:  epoch  0, batch   218 | loss: 2.7233882MixupTrain:  epoch  0, batch   219 | loss: 2.9471648MixupTrain:  epoch  0, batch   220 | loss: 2.8040078MixupTrain:  epoch  0, batch   221 | loss: 2.7849293MixupTrain:  epoch  0, batch   222 | loss: 2.7653546MixupTrain:  epoch  0, batch   223 | loss: 2.8338752MixupTrain:  epoch  0, batch   224 | loss: 2.7476864MixupTrain:  epoch  0, batch   225 | loss: 2.9320178MixupTrain:  epoch  0, batch   226 | loss: 2.7841883MixupTrain:  epoch  0, batch   227 | loss: 2.7523928MixupTrain:  epoch  0, batch   228 | loss: 2.7716677MixupTrain:  epoch  0, batch   229 | loss: 2.7312341MixupTrain:  epoch  0, batch   230 | loss: 2.9387426MixupTrain:  epoch  0, batch   231 | loss: 2.9286604MixupTrain:  epoch  0, batch   232 | loss: 2.7173719MixupTrain:  epoch  0, batch   233 | loss: 2.7785668MixupTrain:  epoch  0, batch   234 | loss: 2.8666754MixupTrain:  epoch  0, batch   235 | loss: 2.8458614MixupTrain:  epoch  0, batch   236 | loss: 2.7925251MixupTrain:  epoch  0, batch   237 | loss: 2.7269511MixupTrain:  epoch  0, batch   238 | loss: 2.8558428MixupTrain:  epoch  0, batch   239 | loss: 2.8063548MixupTrain:  epoch  0, batch   240 | loss: 2.9473984MixupTrain:  epoch  0, batch   241 | loss: 2.8625050MixupTrain:  epoch  0, batch   242 | loss: 2.8410993MixupTrain:  epoch  0, batch   243 | loss: 2.8360820MixupTrain:  epoch  0, batch   244 | loss: 2.6357741MixupTrain:  epoch  0, batch   245 | loss: 2.9024878MixupTrain:  epoch  0, batch   246 | loss: 2.8991270MixupTrain:  epoch  0, batch   247 | loss: 2.7430639MixupTrain:  epoch  0, batch   248 | loss: 2.8888135MixupTrain:  epoch  0, batch   249 | loss: 2.7522266MixupTrain:  epoch  0, batch   250 | loss: 2.8705800MixupTrain:  epoch  0, batch   251 | loss: 2.7719188MixupTrain:  epoch  0, batch   252 | loss: 2.8212242MixupTrain:  epoch  0, batch   253 | loss: 2.8651206MixupTrain:  epoch  0, batch   254 | loss: 2.7639890MixupTrain:  epoch  0, batch   255 | loss: 2.8557162MixupTrain:  epoch  0, batch   256 | loss: 2.8804340MixupTrain:  epoch  0, batch   257 | loss: 2.7922192MixupTrain:  epoch  0, batch   258 | loss: 2.8233080MixupTrain:  epoch  0, batch   259 | loss: 2.8000724MixupTrain:  epoch  0, batch   260 | loss: 2.8661489MixupTrain:  epoch  0, batch   261 | loss: 2.7186577MixupTrain:  epoch  0, batch   262 | loss: 2.7398663MixupTrain:  epoch  0, batch   263 | loss: 2.8184791MixupTrain:  epoch  0, batch   264 | loss: 2.8708372MixupTrain:  epoch  0, batch   265 | loss: 2.8206277MixupTrain:  epoch  0, batch   266 | loss: 2.7906561MixupTrain:  epoch  0, batch   267 | loss: 2.9519396MixupTrain:  epoch  0, batch   268 | loss: 2.8533063MixupTrain:  epoch  0, batch   269 | loss: 2.7725415MixupTrain:  epoch  0, batch   270 | loss: 2.9589396MixupTrain:  epoch  0, batch   271 | loss: 2.9204235MixupTrain:  epoch  0, batch   272 | loss: 2.7800095MixupTrain:  epoch  0, batch   273 | loss: 2.9417157MixupTrain:  epoch  0, batch   274 | loss: 2.7670207MixupTrain:  epoch  0, batch   275 | loss: 2.9159741MixupTrain:  epoch  0, batch   276 | loss: 2.7558608MixupTrain:  epoch  0, batch   277 | loss: 2.8523486MixupTrain:  epoch  0, batch   278 | loss: 2.7397914MixupTrain:  epoch  0, batch   279 | loss: 2.8463805MixupTrain:  epoch  0, batch   280 | loss: 2.8051703MixupTrain:  epoch  0, batch   281 | loss: 2.8767395MixupTrain:  epoch  0, batch   282 | loss: 2.7746332MixupTrain:  epoch  0, batch   283 | loss: 2.8285875MixupTrain:  epoch  0, batch   284 | loss: 2.8041754MixupTrain:  epoch  0, batch   285 | loss: 2.9165425MixupTrain:  epoch  0, batch   286 | loss: 2.8637180MixupTrain:  epoch  0, batch   287 | loss: 2.8380139MixupTrain:  epoch  0, batch   288 | loss: 2.8127484MixupTrain:  epoch  0, batch   289 | loss: 2.8418889MixupTrain:  epoch  0, batch   290 | loss: 2.8026748MixupTrain:  epoch  0, batch   291 | loss: 2.7216418MixupTrain:  epoch  0, batch   292 | loss: 2.8059552MixupTrain:  epoch  0, batch   293 | loss: 2.7828054MixupTrain:  epoch  0, batch   294 | loss: 2.8287015MixupTrain:  epoch  0, batch   295 | loss: 2.8454542MixupTrain:  epoch  0, batch   296 | loss: 2.7682898MixupTrain:  epoch  0, batch   297 | loss: 2.9452598MixupTrain:  epoch  0, batch   298 | loss: 2.8693891MixupTrain:  epoch  0, batch   299 | loss: 2.7791219MixupTrain:  epoch  0, batch   300 | loss: 2.8848605MixupTrain:  epoch  0, batch   301 | loss: 2.8171954MixupTrain:  epoch  0, batch   302 | loss: 2.7804353MixupTrain:  epoch  0, batch   303 | loss: 2.6369190MixupTrain:  epoch  0, batch   304 | loss: 2.8510466MixupTrain:  epoch  0, batch   305 | loss: 2.7656956MixupTrain:  epoch  0, batch   306 | loss: 2.8018765MixupTrain:  epoch  0, batch   307 | loss: 2.8782353MixupTrain:  epoch  0, batch   308 | loss: 2.8911381MixupTrain:  epoch  0, batch   309 | loss: 2.8127906MixupTrain:  epoch  0, batch   310 | loss: 2.7800550MixupTrain:  epoch  0, batch   311 | loss: 2.9467905MixupTrain:  epoch  0, batch   312 | loss: 2.8421223MixupTrain:  epoch  0, batch   313 | loss: 2.8206434MixupTrain:  epoch  0, batch   314 | loss: 2.8312845MixupTrain:  epoch  0, batch   315 | loss: 2.9343228MixupTrain:  epoch  0, batch   316 | loss: 2.8432684MixupTrain:  epoch  0, batch   317 | loss: 2.8643689MixupTrain:  epoch  0, batch   318 | loss: 2.7374573MixupTrain:  epoch  0, batch   319 | loss: 2.7823646MixupTrain:  epoch  0, batch   320 | loss: 2.9530692MixupTrain:  epoch  0, batch   321 | loss: 2.8489411MixupTrain:  epoch  0, batch   322 | loss: 2.9154396MixupTrain:  epoch  0, batch   323 | loss: 2.8049641MixupTrain:  epoch  0, batch   324 | loss: 2.7280610MixupTrain:  epoch  0, batch   325 | loss: 2.6562953MixupTrain:  epoch  0, batch   326 | loss: 2.8981233MixupTrain:  epoch  0, batch   327 | loss: 2.9075000MixupTrain:  epoch  0, batch   328 | loss: 2.8783269MixupTrain:  epoch  0, batch   329 | loss: 2.8178189MixupTrain:  epoch  0, batch   330 | loss: 2.7962499MixupTrain:  epoch  0, batch   331 | loss: 2.8156557MixupTrain:  epoch  0, batch   332 | loss: 2.7599907MixupTrain:  epoch  0, batch   333 | loss: 2.8147483MixupTrain:  epoch  0, batch   334 | loss: 2.7295389MixupTrain:  epoch  0, batch   335 | loss: 2.8134987MixupTrain:  epoch  0, batch   336 | loss: 2.8233435MixupTrain:  epoch  0, batch   337 | loss: 2.8233390MixupTrain:  epoch  0, batch   338 | loss: 2.9134221MixupTrain:  epoch  0, batch   339 | loss: 2.7846346MixupTrain:  epoch  0, batch   340 | loss: 2.7603242MixupTrain:  epoch  0, batch   341 | loss: 2.8546343MixupTrain:  epoch  0, batch   342 | loss: 2.7317500MixupTrain:  epoch  0, batch   343 | loss: 2.7628012MixupTrain:  epoch  0, batch   344 | loss: 2.8080995MixupTrain:  epoch  0, batch   345 | loss: 2.8223825MixupTrain:  epoch  0, batch   346 | loss: 2.7952182MixupTrain:  epoch  0, batch   347 | loss: 2.8210511MixupTrain:  epoch  0, batch   348 | loss: 2.8546405MixupTrain:  epoch  0, batch   349 | loss: 2.9773240MixupTrain:  epoch  0, batch   350 | loss: 2.8371305MixupTrain:  epoch  0, batch   351 | loss: 2.9264829MixupTrain:  epoch  0, batch   352 | loss: 2.9057109MixupTrain:  epoch  0, batch   353 | loss: 2.7884543MixupTrain:  epoch  0, batch   354 | loss: 2.9206583MixupTrain:  epoch  0, batch   355 | loss: 2.8093209MixupTrain:  epoch  0, batch   356 | loss: 2.8853917MixupTrain:  epoch  0, batch   357 | loss: 2.7976203MixupTrain:  epoch  0, batch   358 | loss: 2.8926957MixupTrain:  epoch  0, batch   359 | loss: 2.7290442MixupTrain:  epoch  0, batch   360 | loss: 2.8797545MixupTrain:  epoch  0, batch   361 | loss: 2.8412223MixupTrain:  epoch  0, batch   362 | loss: 2.8197813MixupTrain:  epoch  0, batch   363 | loss: 2.8419209MixupTrain:  epoch  0, batch   364 | loss: 2.7971087MixupTrain:  epoch  0, batch   365 | loss: 2.8429058MixupTrain:  epoch  0, batch   366 | loss: 2.7121229MixupTrain:  epoch  0, batch   367 | loss: 2.7845428MixupTrain:  epoch  0, batch   368 | loss: 2.6538959MixupTrain:  epoch  0, batch   369 | loss: 2.7965109MixupTrain:  epoch  0, batch   370 | loss: 2.7656453MixupTrain:  epoch  0, batch   371 | loss: 2.8289151MixupTrain:  epoch  0, batch   372 | loss: 2.8612690MixupTrain:  epoch  0, batch   373 | loss: 2.7967305MixupTrain:  epoch  0, batch   374 | loss: 2.7205005MixupTrain:  epoch  0, batch   375 | loss: 2.8041978MixupTrain:  epoch  0, batch   376 | loss: 2.6484137MixupTrain:  epoch  0, batch   377 | loss: 2.8582969MixupTrain:  epoch  0, batch   378 | loss: 2.7445655MixupTrain:  epoch  0, batch   379 | loss: 2.8499665MixupTrain:  epoch  0, batch   380 | loss: 2.7550778MixupTrain:  epoch  0, batch   381 | loss: 2.7719762MixupTrain:  epoch  0, batch   382 | loss: 2.8780289MixupTrain:  epoch  0, batch   383 | loss: 2.8031268MixupTrain:  epoch  0, batch   384 | loss: 2.9903786MixupTrain:  epoch  0, batch   385 | loss: 2.8067012MixupTrain:  epoch  0, batch   386 | loss: 2.8975844MixupTrain:  epoch  0, batch   387 | loss: 2.8342760MixupTrain:  epoch  0, batch   388 | loss: 2.7911172MixupTrain:  epoch  0, batch   389 | loss: 2.7962651MixupTrain:  epoch  0, batch   390 | loss: 2.8699274MixupTrain:  epoch  0, batch   391 | loss: 2.8141408MixupTrain:  epoch  0, batch   392 | loss: 2.7946749MixupTrain:  epoch  0, batch   393 | loss: 2.8216741MixupTrain:  epoch  0, batch   394 | loss: 2.8856378MixupTrain:  epoch  0, batch   395 | loss: 2.6978977MixupTrain:  epoch  0, batch   396 | loss: 2.9001245MixupTrain:  epoch  0, batch   397 | loss: 2.8041368MixupTrain:  epoch  0, batch   398 | loss: 2.6824036MixupTrain:  epoch  0, batch   399 | loss: 2.8902023MixupTrain:  epoch  0, batch   400 | loss: 2.9144020MixupTrain:  epoch  0, batch   401 | loss: 2.8966022MixupTrain:  epoch  0, batch   402 | loss: 2.7684939MixupTrain:  epoch  0, batch   403 | loss: 2.8103790MixupTrain:  epoch  0, batch   404 | loss: 2.7265544MixupTrain:  epoch  0, batch   405 | loss: 2.7053332MixupTrain:  epoch  0, batch   406 | loss: 2.7643847MixupTrain:  epoch  0, batch   407 | loss: 2.7785273MixupTrain:  epoch  0, batch   408 | loss: 2.8397274MixupTrain:  epoch  0, batch   409 | loss: 2.7635465MixupTrain:  epoch  0, batch   410 | loss: 2.7557263MixupTrain:  epoch  0, batch   411 | loss: 2.7848043MixupTrain:  epoch  0, batch   412 | loss: 2.8103814MixupTrain:  epoch  0, batch   413 | loss: 2.9030166MixupTrain:  epoch  0, batch   414 | loss: 2.8100176MixupTrain:  epoch  0, batch   415 | loss: 2.9055326MixupTrain:  epoch  0, batch   416 | loss: 2.9595430MixupTrain:  epoch  0, batch   417 | loss: 2.7167325
MemoryTrain:  epoch  0, batch     0 | loss: 1.2374532MemoryTrain:  epoch  0, batch     1 | loss: 1.3845115MemoryTrain:  epoch  0, batch     2 | loss: 1.5872490MemoryTrain:  epoch  0, batch     3 | loss: 1.5354543MemoryTrain:  epoch  0, batch     4 | loss: 1.8282229MemoryTrain:  epoch  0, batch     5 | loss: 1.7823613MemoryTrain:  epoch  1, batch     0 | loss: 1.2910051MemoryTrain:  epoch  1, batch     1 | loss: 1.4978004MemoryTrain:  epoch  1, batch     2 | loss: 1.2649224MemoryTrain:  epoch  1, batch     3 | loss: 1.3221470MemoryTrain:  epoch  1, batch     4 | loss: 1.3439443MemoryTrain:  epoch  1, batch     5 | loss: 1.3286123MemoryTrain:  epoch  2, batch     0 | loss: 1.2372119MemoryTrain:  epoch  2, batch     1 | loss: 1.1853716MemoryTrain:  epoch  2, batch     2 | loss: 1.2901747MemoryTrain:  epoch  2, batch     3 | loss: 1.3075489MemoryTrain:  epoch  2, batch     4 | loss: 1.2868153MemoryTrain:  epoch  2, batch     5 | loss: 1.2363100MemoryTrain:  epoch  3, batch     0 | loss: 1.2586882MemoryTrain:  epoch  3, batch     1 | loss: 1.2143708MemoryTrain:  epoch  3, batch     2 | loss: 1.2280704MemoryTrain:  epoch  3, batch     3 | loss: 1.2202551MemoryTrain:  epoch  3, batch     4 | loss: 1.2369165MemoryTrain:  epoch  3, batch     5 | loss: 1.2084664MemoryTrain:  epoch  4, batch     0 | loss: 1.2483428MemoryTrain:  epoch  4, batch     1 | loss: 1.1776260MemoryTrain:  epoch  4, batch     2 | loss: 1.2516716MemoryTrain:  epoch  4, batch     3 | loss: 1.3884454MemoryTrain:  epoch  4, batch     4 | loss: 1.2378595MemoryTrain:  epoch  4, batch     5 | loss: 1.1873991MemoryTrain:  epoch  5, batch     0 | loss: 1.2051191MemoryTrain:  epoch  5, batch     1 | loss: 1.2075870MemoryTrain:  epoch  5, batch     2 | loss: 1.3251965MemoryTrain:  epoch  5, batch     3 | loss: 1.2418926MemoryTrain:  epoch  5, batch     4 | loss: 1.2252671MemoryTrain:  epoch  5, batch     5 | loss: 1.2111434MemoryTrain:  epoch  6, batch     0 | loss: 1.2016687MemoryTrain:  epoch  6, batch     1 | loss: 1.1822412MemoryTrain:  epoch  6, batch     2 | loss: 1.2444271MemoryTrain:  epoch  6, batch     3 | loss: 1.2122867MemoryTrain:  epoch  6, batch     4 | loss: 1.1742225MemoryTrain:  epoch  6, batch     5 | loss: 1.1657223MemoryTrain:  epoch  7, batch     0 | loss: 1.1748140MemoryTrain:  epoch  7, batch     1 | loss: 1.1813962MemoryTrain:  epoch  7, batch     2 | loss: 1.2085458MemoryTrain:  epoch  7, batch     3 | loss: 1.2375555MemoryTrain:  epoch  7, batch     4 | loss: 1.1990876MemoryTrain:  epoch  7, batch     5 | loss: 1.2090729MemoryTrain:  epoch  8, batch     0 | loss: 1.2031856MemoryTrain:  epoch  8, batch     1 | loss: 1.1808999MemoryTrain:  epoch  8, batch     2 | loss: 1.2766387MemoryTrain:  epoch  8, batch     3 | loss: 1.1788549MemoryTrain:  epoch  8, batch     4 | loss: 1.1659604MemoryTrain:  epoch  8, batch     5 | loss: 1.2002389MemoryTrain:  epoch  9, batch     0 | loss: 1.2231359MemoryTrain:  epoch  9, batch     1 | loss: 1.1890700MemoryTrain:  epoch  9, batch     2 | loss: 1.1771758MemoryTrain:  epoch  9, batch     3 | loss: 1.1995220MemoryTrain:  epoch  9, batch     4 | loss: 1.2065581MemoryTrain:  epoch  9, batch     5 | loss: 1.1449598
[EVAL] batch:    0 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    1 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    2 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    3 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    4 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 98.96%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 99.11%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 99.22%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 97.22%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 93.75%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 92.61%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 92.71%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 92.79%   [EVAL] batch:   13 | acc: 56.25%,  total acc: 90.18%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 56.25%,  total acc: 62.50%   [EVAL] batch:    2 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    3 | acc: 50.00%,  total acc: 59.38%   [EVAL] batch:    4 | acc: 56.25%,  total acc: 58.75%   [EVAL] batch:    5 | acc: 43.75%,  total acc: 56.25%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 59.82%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 64.84%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 68.06%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 70.00%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 72.16%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 73.96%   [EVAL] batch:   12 | acc: 43.75%,  total acc: 71.63%   [EVAL] batch:   13 | acc: 25.00%,  total acc: 68.30%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 68.33%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 67.58%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 68.01%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 67.71%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 67.43%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 68.44%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 69.94%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 71.02%   [EVAL] batch:   22 | acc: 87.50%,  total acc: 71.74%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 72.66%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 73.75%   [EVAL] batch:   25 | acc: 87.50%,  total acc: 74.28%   [EVAL] batch:   26 | acc: 81.25%,  total acc: 74.54%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 75.45%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 76.29%   [EVAL] batch:   29 | acc: 75.00%,  total acc: 76.25%   [EVAL] batch:   30 | acc: 81.25%,  total acc: 76.41%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 76.95%   [EVAL] batch:   32 | acc: 50.00%,  total acc: 76.14%   [EVAL] batch:   33 | acc: 62.50%,  total acc: 75.74%   [EVAL] batch:   34 | acc: 62.50%,  total acc: 75.36%   [EVAL] batch:   35 | acc: 43.75%,  total acc: 74.48%   [EVAL] batch:   36 | acc: 68.75%,  total acc: 74.32%   [EVAL] batch:   37 | acc: 68.75%,  total acc: 74.18%   [EVAL] batch:   38 | acc: 25.00%,  total acc: 72.92%   [EVAL] batch:   39 | acc: 25.00%,  total acc: 71.72%   [EVAL] batch:   40 | acc: 0.00%,  total acc: 69.97%   [EVAL] batch:   41 | acc: 0.00%,  total acc: 68.30%   [EVAL] batch:   42 | acc: 0.00%,  total acc: 66.72%   [EVAL] batch:   43 | acc: 0.00%,  total acc: 65.20%   [EVAL] batch:   44 | acc: 25.00%,  total acc: 64.31%   [EVAL] batch:   45 | acc: 25.00%,  total acc: 63.45%   [EVAL] batch:   46 | acc: 18.75%,  total acc: 62.50%   [EVAL] batch:   47 | acc: 37.50%,  total acc: 61.98%   [EVAL] batch:   48 | acc: 50.00%,  total acc: 61.73%   [EVAL] batch:   49 | acc: 12.50%,  total acc: 60.75%   [EVAL] batch:   50 | acc: 87.50%,  total acc: 61.27%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 61.90%   [EVAL] batch:   52 | acc: 81.25%,  total acc: 62.26%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 62.62%   [EVAL] batch:   54 | acc: 56.25%,  total acc: 62.50%   [EVAL] batch:   55 | acc: 6.25%,  total acc: 61.50%   [EVAL] batch:   56 | acc: 0.00%,  total acc: 60.42%   [EVAL] batch:   57 | acc: 0.00%,  total acc: 59.38%   [EVAL] batch:   58 | acc: 0.00%,  total acc: 58.37%   [EVAL] batch:   59 | acc: 0.00%,  total acc: 57.40%   [EVAL] batch:   60 | acc: 0.00%,  total acc: 56.45%   [EVAL] batch:   61 | acc: 0.00%,  total acc: 55.54%   [EVAL] batch:   62 | acc: 12.50%,  total acc: 54.86%   [EVAL] batch:   63 | acc: 12.50%,  total acc: 54.20%   [EVAL] batch:   64 | acc: 6.25%,  total acc: 53.46%   [EVAL] batch:   65 | acc: 25.00%,  total acc: 53.03%   [EVAL] batch:   66 | acc: 37.50%,  total acc: 52.80%   [EVAL] batch:   67 | acc: 18.75%,  total acc: 52.30%   [EVAL] batch:   68 | acc: 37.50%,  total acc: 52.08%   [EVAL] batch:   69 | acc: 56.25%,  total acc: 52.14%   [EVAL] batch:   70 | acc: 43.75%,  total acc: 52.02%   [EVAL] batch:   71 | acc: 37.50%,  total acc: 51.82%   [EVAL] batch:   72 | acc: 75.00%,  total acc: 52.14%   [EVAL] batch:   73 | acc: 100.00%,  total acc: 52.79%   [EVAL] batch:   74 | acc: 93.75%,  total acc: 53.33%   [EVAL] batch:   75 | acc: 100.00%,  total acc: 53.95%   [EVAL] batch:   76 | acc: 81.25%,  total acc: 54.30%   [EVAL] batch:   77 | acc: 37.50%,  total acc: 54.09%   [EVAL] batch:   78 | acc: 25.00%,  total acc: 53.72%   [EVAL] batch:   79 | acc: 31.25%,  total acc: 53.44%   [EVAL] batch:   80 | acc: 31.25%,  total acc: 53.16%   [EVAL] batch:   81 | acc: 43.75%,  total acc: 53.05%   [EVAL] batch:   82 | acc: 50.00%,  total acc: 53.01%   [EVAL] batch:   83 | acc: 50.00%,  total acc: 52.98%   [EVAL] batch:   84 | acc: 37.50%,  total acc: 52.79%   [EVAL] batch:   85 | acc: 37.50%,  total acc: 52.62%   [EVAL] batch:   86 | acc: 37.50%,  total acc: 52.44%   [EVAL] batch:   87 | acc: 93.75%,  total acc: 52.91%   [EVAL] batch:   88 | acc: 93.75%,  total acc: 53.37%   [EVAL] batch:   89 | acc: 75.00%,  total acc: 53.61%   [EVAL] batch:   90 | acc: 87.50%,  total acc: 53.98%   [EVAL] batch:   91 | acc: 81.25%,  total acc: 54.28%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 54.77%   [EVAL] batch:   93 | acc: 100.00%,  total acc: 55.25%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 55.72%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 56.18%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 56.64%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 57.08%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 57.51%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 57.94%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 58.35%   [EVAL] batch:  101 | acc: 100.00%,  total acc: 58.76%   [EVAL] batch:  102 | acc: 100.00%,  total acc: 59.16%   [EVAL] batch:  103 | acc: 93.75%,  total acc: 59.50%   [EVAL] batch:  104 | acc: 100.00%,  total acc: 59.88%   [EVAL] batch:  105 | acc: 100.00%,  total acc: 60.26%   [EVAL] batch:  106 | acc: 81.25%,  total acc: 60.46%   [EVAL] batch:  107 | acc: 62.50%,  total acc: 60.47%   [EVAL] batch:  108 | acc: 81.25%,  total acc: 60.67%   [EVAL] batch:  109 | acc: 93.75%,  total acc: 60.97%   [EVAL] batch:  110 | acc: 93.75%,  total acc: 61.26%   [EVAL] batch:  111 | acc: 62.50%,  total acc: 61.27%   
cur_acc:  ['0.8655', '0.7788', '0.7000', '0.6023', '0.8507', '0.9018']
his_acc:  ['0.8655', '0.8472', '0.7833', '0.6728', '0.6010', '0.6127']
CurrentTrain: epoch  0, batch     0 | loss: 6.0973625CurrentTrain: epoch  0, batch     1 | loss: 5.3609052CurrentTrain: epoch  1, batch     0 | loss: 5.6389270CurrentTrain: epoch  1, batch     1 | loss: 4.9528403CurrentTrain: epoch  2, batch     0 | loss: 4.9590988CurrentTrain: epoch  2, batch     1 | loss: 5.3237834CurrentTrain: epoch  3, batch     0 | loss: 4.4027138CurrentTrain: epoch  3, batch     1 | loss: 4.7788959CurrentTrain: epoch  4, batch     0 | loss: 4.6654749CurrentTrain: epoch  4, batch     1 | loss: 4.2752175CurrentTrain: epoch  5, batch     0 | loss: 3.9147696CurrentTrain: epoch  5, batch     1 | loss: 4.4147544CurrentTrain: epoch  6, batch     0 | loss: 3.9088860CurrentTrain: epoch  6, batch     1 | loss: 3.6911523CurrentTrain: epoch  7, batch     0 | loss: 3.5077705CurrentTrain: epoch  7, batch     1 | loss: 3.3283341CurrentTrain: epoch  8, batch     0 | loss: 2.9096107CurrentTrain: epoch  8, batch     1 | loss: 2.5982933CurrentTrain: epoch  9, batch     0 | loss: 2.5850027CurrentTrain: epoch  9, batch     1 | loss: 3.0376070
#### person date of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person date of birth" means a person was born in a date.
Here is an example:
Relation: person date of birth
Context: kirkaldy , born irene morgan in baltimore , maryland , in 1917 , was arrested in 1944 for refusing to give up her seat on a greyhound bus heading from gloucester to baltimore , and for resisting arrest .
Head Entity: irene morgan
Tail Entity: 1917
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person date of birth":
Relation: person date of birth  
Context: born in 1985 in a small town in Texas, Sarah Johnson grew up with a passion for music and later became a renowned singer.  
Head Entity: Sarah Johnson  
Tail Entity: 1985  

Relation: person date of birth  
Context: Albert Einstein, who was born in Ulm, Germany, on March 14, 1879, revolutionized the field of physics with his theories of relativity.  
Head Entity: Albert Einstein  
Tail Entity: March 14, 1879  
#### person stateorprovince of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person stateorprovince of birth" means a person was died in a state or province.
Here is an example:
Relation: person stateorprovince of birth
Context: jane matilda bolin was born on april 11 , 1908 , in poughkeepsie , ny .
Head Entity: jane matilda bolin
Tail Entity: ny
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person stateorprovince of birth":
Relation: person stateorprovince of birth  
Context: albert einstein was born on march 14, 1879, in ulm, germany.  
Head Entity: albert einstein  
Tail Entity: germany  

Relation: person stateorprovince of birth  
Context: marilyn monroe was born on june 1, 1926, in los angeles, california.  
Head Entity: marilyn monroe  
Tail Entity: california  
#### person parents ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person parents" means a person 's parent is a person.
Here is an example:
Relation: person parents
Context: lynne spears told the court that lutfi had treated her daughter like a hostage in her own home , drugged her and took over her finances .
Head Entity: her
Tail Entity: lynne spears
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person parents":
Relation: person parents  
Context: During the family reunion, Sarah introduced her father, John, to her friends, highlighting how much he has influenced her life choices.  
Head Entity: her father  
Tail Entity: John  

Relation: person parents  
Context: Emily often shares stories about her mother, who was a strong influence in her decision to pursue a career in medicine.  
Head Entity: her mother  
Tail Entity: Emily
#### person employee of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person employee of" means a person is an employee of an organization.
Here is an example:
Relation: person employee of
Context: seeking revenge , axel reunites with old pal sgt. billy rosewood -lrb- judge reinhold -rrb- and jon flint -lrb- hector elizondo -rrb- of the beverly hills police department .
Head Entity: hector elizondo
Tail Entity: beverly hills police department
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person employee of":
Relation: person employee of  
Context: After years of hard work and dedication, Sarah Thompson finally received a promotion at Tech Innovations, where she has been a key player in the development team.  
Head Entity: Sarah Thompson  
Tail Entity: Tech Innovations  

Relation: person employee of  
Context: John Smith has been with Global Finance for over a decade, where he has climbed the ranks to become the senior analyst in the investment division.  
Head Entity: John Smith  
Tail Entity: Global Finance  
#### person stateorprovince of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person stateorprovince of death" means a person was died in a state or province.
Here is an example:
Relation: person stateorprovince of death
Context: millender-mcdonald , who was 68 , died late saturday at her home in carson , california , said her chief of staff , bandele mcqueen .
Head Entity: millender-mcdonald
Tail Entity: california
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person stateorprovince of death":
Relation: person stateorprovince of death  
Context: johnson, a renowned author, passed away peacefully in his sleep at his residence in new york, new york, surrounded by family and friends.  
Head Entity: johnson  
Tail Entity: new york  

Relation: person stateorprovince of death  
Context: the famous musician, who was 45, died unexpectedly in a hospital in nashville, tennessee, after a brief illness.  
Head Entity: the famous musician  
Tail Entity: tennessee  
Mixup data size:  8545
MixupTrain:  epoch  0, batch     0 | loss: 3.6714418MixupTrain:  epoch  0, batch     1 | loss: 3.6922629MixupTrain:  epoch  0, batch     2 | loss: 3.8983791MixupTrain:  epoch  0, batch     3 | loss: 3.9104090MixupTrain:  epoch  0, batch     4 | loss: 3.3025317MixupTrain:  epoch  0, batch     5 | loss: 3.8032720MixupTrain:  epoch  0, batch     6 | loss: 3.7509379MixupTrain:  epoch  0, batch     7 | loss: 3.4843705MixupTrain:  epoch  0, batch     8 | loss: 3.6741865MixupTrain:  epoch  0, batch     9 | loss: 3.4298460MixupTrain:  epoch  0, batch    10 | loss: 3.5263801MixupTrain:  epoch  0, batch    11 | loss: 3.5827589MixupTrain:  epoch  0, batch    12 | loss: 3.8222146MixupTrain:  epoch  0, batch    13 | loss: 3.5246778MixupTrain:  epoch  0, batch    14 | loss: 3.7075372MixupTrain:  epoch  0, batch    15 | loss: 3.9061081MixupTrain:  epoch  0, batch    16 | loss: 3.6202750MixupTrain:  epoch  0, batch    17 | loss: 3.6407123MixupTrain:  epoch  0, batch    18 | loss: 3.7635391MixupTrain:  epoch  0, batch    19 | loss: 3.7375212MixupTrain:  epoch  0, batch    20 | loss: 3.7960143MixupTrain:  epoch  0, batch    21 | loss: 3.8693547MixupTrain:  epoch  0, batch    22 | loss: 3.6073201MixupTrain:  epoch  0, batch    23 | loss: 3.5236368MixupTrain:  epoch  0, batch    24 | loss: 3.3745914MixupTrain:  epoch  0, batch    25 | loss: 3.4228883MixupTrain:  epoch  0, batch    26 | loss: 3.5637672MixupTrain:  epoch  0, batch    27 | loss: 3.3060257MixupTrain:  epoch  0, batch    28 | loss: 3.4678097MixupTrain:  epoch  0, batch    29 | loss: 3.4700089MixupTrain:  epoch  0, batch    30 | loss: 3.3298442MixupTrain:  epoch  0, batch    31 | loss: 3.5608146MixupTrain:  epoch  0, batch    32 | loss: 3.3313632MixupTrain:  epoch  0, batch    33 | loss: 3.5915332MixupTrain:  epoch  0, batch    34 | loss: 3.3614197MixupTrain:  epoch  0, batch    35 | loss: 3.2754581MixupTrain:  epoch  0, batch    36 | loss: 3.0609360MixupTrain:  epoch  0, batch    37 | loss: 3.1002731MixupTrain:  epoch  0, batch    38 | loss: 3.2593617MixupTrain:  epoch  0, batch    39 | loss: 3.1312404MixupTrain:  epoch  0, batch    40 | loss: 2.8991058MixupTrain:  epoch  0, batch    41 | loss: 3.2843375MixupTrain:  epoch  0, batch    42 | loss: 3.1999750MixupTrain:  epoch  0, batch    43 | loss: 3.5879292MixupTrain:  epoch  0, batch    44 | loss: 3.2864938MixupTrain:  epoch  0, batch    45 | loss: 3.3438559MixupTrain:  epoch  0, batch    46 | loss: 3.7638597MixupTrain:  epoch  0, batch    47 | loss: 3.3099532MixupTrain:  epoch  0, batch    48 | loss: 3.2332239MixupTrain:  epoch  0, batch    49 | loss: 3.5124223MixupTrain:  epoch  0, batch    50 | loss: 3.1525369MixupTrain:  epoch  0, batch    51 | loss: 3.4675646MixupTrain:  epoch  0, batch    52 | loss: 3.3800986MixupTrain:  epoch  0, batch    53 | loss: 3.5601926MixupTrain:  epoch  0, batch    54 | loss: 3.1181164MixupTrain:  epoch  0, batch    55 | loss: 3.4833207MixupTrain:  epoch  0, batch    56 | loss: 3.1705358MixupTrain:  epoch  0, batch    57 | loss: 3.3308179MixupTrain:  epoch  0, batch    58 | loss: 3.1937385MixupTrain:  epoch  0, batch    59 | loss: 3.2808447MixupTrain:  epoch  0, batch    60 | loss: 3.4016643MixupTrain:  epoch  0, batch    61 | loss: 3.2420044MixupTrain:  epoch  0, batch    62 | loss: 3.3775110MixupTrain:  epoch  0, batch    63 | loss: 3.3229866MixupTrain:  epoch  0, batch    64 | loss: 3.2476845MixupTrain:  epoch  0, batch    65 | loss: 3.0639231MixupTrain:  epoch  0, batch    66 | loss: 3.4825344MixupTrain:  epoch  0, batch    67 | loss: 3.1237321MixupTrain:  epoch  0, batch    68 | loss: 3.2085953MixupTrain:  epoch  0, batch    69 | loss: 3.1541348MixupTrain:  epoch  0, batch    70 | loss: 3.2629538MixupTrain:  epoch  0, batch    71 | loss: 3.3997526MixupTrain:  epoch  0, batch    72 | loss: 3.3614502MixupTrain:  epoch  0, batch    73 | loss: 3.3509665MixupTrain:  epoch  0, batch    74 | loss: 3.1832857MixupTrain:  epoch  0, batch    75 | loss: 3.3464553MixupTrain:  epoch  0, batch    76 | loss: 3.2607188MixupTrain:  epoch  0, batch    77 | loss: 3.5163856MixupTrain:  epoch  0, batch    78 | loss: 3.1971707MixupTrain:  epoch  0, batch    79 | loss: 3.1197755MixupTrain:  epoch  0, batch    80 | loss: 3.4088709MixupTrain:  epoch  0, batch    81 | loss: 3.1959262MixupTrain:  epoch  0, batch    82 | loss: 2.8220196MixupTrain:  epoch  0, batch    83 | loss: 3.0593073MixupTrain:  epoch  0, batch    84 | loss: 3.1969955MixupTrain:  epoch  0, batch    85 | loss: 3.2539861MixupTrain:  epoch  0, batch    86 | loss: 3.3785298MixupTrain:  epoch  0, batch    87 | loss: 3.2040319MixupTrain:  epoch  0, batch    88 | loss: 3.1449103MixupTrain:  epoch  0, batch    89 | loss: 3.2517281MixupTrain:  epoch  0, batch    90 | loss: 3.1861312MixupTrain:  epoch  0, batch    91 | loss: 3.3944187MixupTrain:  epoch  0, batch    92 | loss: 2.9739289MixupTrain:  epoch  0, batch    93 | loss: 3.1561289MixupTrain:  epoch  0, batch    94 | loss: 3.0330429MixupTrain:  epoch  0, batch    95 | loss: 3.1360765MixupTrain:  epoch  0, batch    96 | loss: 3.3062258MixupTrain:  epoch  0, batch    97 | loss: 3.1223729MixupTrain:  epoch  0, batch    98 | loss: 3.1156301MixupTrain:  epoch  0, batch    99 | loss: 3.1046047MixupTrain:  epoch  0, batch   100 | loss: 3.2292843MixupTrain:  epoch  0, batch   101 | loss: 3.1561370MixupTrain:  epoch  0, batch   102 | loss: 3.1009297MixupTrain:  epoch  0, batch   103 | loss: 3.0996015MixupTrain:  epoch  0, batch   104 | loss: 3.2249620MixupTrain:  epoch  0, batch   105 | loss: 3.2742047MixupTrain:  epoch  0, batch   106 | loss: 3.1480808MixupTrain:  epoch  0, batch   107 | loss: 3.1033554MixupTrain:  epoch  0, batch   108 | loss: 3.2669449MixupTrain:  epoch  0, batch   109 | loss: 2.9824128MixupTrain:  epoch  0, batch   110 | loss: 2.8044515MixupTrain:  epoch  0, batch   111 | loss: 3.1151040MixupTrain:  epoch  0, batch   112 | loss: 3.0759413MixupTrain:  epoch  0, batch   113 | loss: 3.3581185MixupTrain:  epoch  0, batch   114 | loss: 2.9762769MixupTrain:  epoch  0, batch   115 | loss: 3.2683570MixupTrain:  epoch  0, batch   116 | loss: 3.0232515MixupTrain:  epoch  0, batch   117 | loss: 3.0202332MixupTrain:  epoch  0, batch   118 | loss: 3.1413536MixupTrain:  epoch  0, batch   119 | loss: 3.2624006MixupTrain:  epoch  0, batch   120 | loss: 3.3337047MixupTrain:  epoch  0, batch   121 | loss: 3.3027823MixupTrain:  epoch  0, batch   122 | loss: 3.2263293MixupTrain:  epoch  0, batch   123 | loss: 3.2544460MixupTrain:  epoch  0, batch   124 | loss: 2.9571435MixupTrain:  epoch  0, batch   125 | loss: 3.2106280MixupTrain:  epoch  0, batch   126 | loss: 3.0269089MixupTrain:  epoch  0, batch   127 | loss: 2.9821520MixupTrain:  epoch  0, batch   128 | loss: 3.0437472MixupTrain:  epoch  0, batch   129 | loss: 3.3674819MixupTrain:  epoch  0, batch   130 | loss: 2.9900122MixupTrain:  epoch  0, batch   131 | loss: 3.2693405MixupTrain:  epoch  0, batch   132 | loss: 3.1522384MixupTrain:  epoch  0, batch   133 | loss: 3.1181090MixupTrain:  epoch  0, batch   134 | loss: 3.1605415MixupTrain:  epoch  0, batch   135 | loss: 2.9376683MixupTrain:  epoch  0, batch   136 | loss: 3.1168213MixupTrain:  epoch  0, batch   137 | loss: 3.1675081MixupTrain:  epoch  0, batch   138 | loss: 3.0738239MixupTrain:  epoch  0, batch   139 | loss: 2.9973202MixupTrain:  epoch  0, batch   140 | loss: 2.9481997MixupTrain:  epoch  0, batch   141 | loss: 3.1685057MixupTrain:  epoch  0, batch   142 | loss: 3.1022310MixupTrain:  epoch  0, batch   143 | loss: 3.1345448MixupTrain:  epoch  0, batch   144 | loss: 3.1332345MixupTrain:  epoch  0, batch   145 | loss: 2.8261309MixupTrain:  epoch  0, batch   146 | loss: 3.0112038MixupTrain:  epoch  0, batch   147 | loss: 3.2444048MixupTrain:  epoch  0, batch   148 | loss: 3.0134249MixupTrain:  epoch  0, batch   149 | loss: 3.0198293MixupTrain:  epoch  0, batch   150 | loss: 3.0110712MixupTrain:  epoch  0, batch   151 | loss: 3.0644710MixupTrain:  epoch  0, batch   152 | loss: 2.9782362MixupTrain:  epoch  0, batch   153 | loss: 2.9898541MixupTrain:  epoch  0, batch   154 | loss: 3.1014237MixupTrain:  epoch  0, batch   155 | loss: 3.3365264MixupTrain:  epoch  0, batch   156 | loss: 2.8895752MixupTrain:  epoch  0, batch   157 | loss: 2.9554501MixupTrain:  epoch  0, batch   158 | loss: 3.0651660MixupTrain:  epoch  0, batch   159 | loss: 2.9770291MixupTrain:  epoch  0, batch   160 | loss: 3.2059741MixupTrain:  epoch  0, batch   161 | loss: 3.1065679MixupTrain:  epoch  0, batch   162 | loss: 2.9587808MixupTrain:  epoch  0, batch   163 | loss: 2.9378843MixupTrain:  epoch  0, batch   164 | loss: 2.9784312MixupTrain:  epoch  0, batch   165 | loss: 3.2019508MixupTrain:  epoch  0, batch   166 | loss: 3.1176839MixupTrain:  epoch  0, batch   167 | loss: 2.8289385MixupTrain:  epoch  0, batch   168 | loss: 3.1808424MixupTrain:  epoch  0, batch   169 | loss: 2.9175367MixupTrain:  epoch  0, batch   170 | loss: 3.0538833MixupTrain:  epoch  0, batch   171 | loss: 3.1045942MixupTrain:  epoch  0, batch   172 | loss: 2.9235897MixupTrain:  epoch  0, batch   173 | loss: 3.0148523MixupTrain:  epoch  0, batch   174 | loss: 2.9535148MixupTrain:  epoch  0, batch   175 | loss: 2.9951639MixupTrain:  epoch  0, batch   176 | loss: 3.1138248MixupTrain:  epoch  0, batch   177 | loss: 3.0547168MixupTrain:  epoch  0, batch   178 | loss: 2.9576406MixupTrain:  epoch  0, batch   179 | loss: 2.9026985MixupTrain:  epoch  0, batch   180 | loss: 2.9520001MixupTrain:  epoch  0, batch   181 | loss: 3.1235287MixupTrain:  epoch  0, batch   182 | loss: 3.0897272MixupTrain:  epoch  0, batch   183 | loss: 3.0904148MixupTrain:  epoch  0, batch   184 | loss: 3.0512927MixupTrain:  epoch  0, batch   185 | loss: 2.9100819MixupTrain:  epoch  0, batch   186 | loss: 2.8312991MixupTrain:  epoch  0, batch   187 | loss: 3.2245860MixupTrain:  epoch  0, batch   188 | loss: 2.9578207MixupTrain:  epoch  0, batch   189 | loss: 3.1052203MixupTrain:  epoch  0, batch   190 | loss: 2.9422040MixupTrain:  epoch  0, batch   191 | loss: 3.2951736MixupTrain:  epoch  0, batch   192 | loss: 3.3444841MixupTrain:  epoch  0, batch   193 | loss: 3.0031152MixupTrain:  epoch  0, batch   194 | loss: 2.9889617MixupTrain:  epoch  0, batch   195 | loss: 3.0736284MixupTrain:  epoch  0, batch   196 | loss: 3.0215449MixupTrain:  epoch  0, batch   197 | loss: 2.8597138MixupTrain:  epoch  0, batch   198 | loss: 3.0893650MixupTrain:  epoch  0, batch   199 | loss: 3.1728544MixupTrain:  epoch  0, batch   200 | loss: 2.8627756MixupTrain:  epoch  0, batch   201 | loss: 2.8928282MixupTrain:  epoch  0, batch   202 | loss: 2.8789554MixupTrain:  epoch  0, batch   203 | loss: 2.9884236MixupTrain:  epoch  0, batch   204 | loss: 3.1126657MixupTrain:  epoch  0, batch   205 | loss: 3.2612820MixupTrain:  epoch  0, batch   206 | loss: 3.0632014MixupTrain:  epoch  0, batch   207 | loss: 2.9724479MixupTrain:  epoch  0, batch   208 | loss: 2.8723192MixupTrain:  epoch  0, batch   209 | loss: 3.1775994MixupTrain:  epoch  0, batch   210 | loss: 2.9378564MixupTrain:  epoch  0, batch   211 | loss: 2.9451513MixupTrain:  epoch  0, batch   212 | loss: 2.7808905MixupTrain:  epoch  0, batch   213 | loss: 3.1857557MixupTrain:  epoch  0, batch   214 | loss: 2.9694428MixupTrain:  epoch  0, batch   215 | loss: 2.9628906MixupTrain:  epoch  0, batch   216 | loss: 3.0655885MixupTrain:  epoch  0, batch   217 | loss: 2.8722823MixupTrain:  epoch  0, batch   218 | loss: 2.7546453MixupTrain:  epoch  0, batch   219 | loss: 3.1583261MixupTrain:  epoch  0, batch   220 | loss: 2.9001284MixupTrain:  epoch  0, batch   221 | loss: 2.8794823MixupTrain:  epoch  0, batch   222 | loss: 3.1365886MixupTrain:  epoch  0, batch   223 | loss: 2.9900746MixupTrain:  epoch  0, batch   224 | loss: 2.9540381MixupTrain:  epoch  0, batch   225 | loss: 2.8213949MixupTrain:  epoch  0, batch   226 | loss: 2.9560184MixupTrain:  epoch  0, batch   227 | loss: 2.9386158MixupTrain:  epoch  0, batch   228 | loss: 3.0226994MixupTrain:  epoch  0, batch   229 | loss: 3.1204319MixupTrain:  epoch  0, batch   230 | loss: 2.9592354MixupTrain:  epoch  0, batch   231 | loss: 2.9561758MixupTrain:  epoch  0, batch   232 | loss: 2.9533677MixupTrain:  epoch  0, batch   233 | loss: 3.1573453MixupTrain:  epoch  0, batch   234 | loss: 3.1495771MixupTrain:  epoch  0, batch   235 | loss: 2.9686692MixupTrain:  epoch  0, batch   236 | loss: 3.0234473MixupTrain:  epoch  0, batch   237 | loss: 3.0271912MixupTrain:  epoch  0, batch   238 | loss: 2.8847518MixupTrain:  epoch  0, batch   239 | loss: 3.0196869MixupTrain:  epoch  0, batch   240 | loss: 2.8905032MixupTrain:  epoch  0, batch   241 | loss: 3.0655556MixupTrain:  epoch  0, batch   242 | loss: 2.8156462MixupTrain:  epoch  0, batch   243 | loss: 3.0672188MixupTrain:  epoch  0, batch   244 | loss: 3.0077655MixupTrain:  epoch  0, batch   245 | loss: 3.1412005MixupTrain:  epoch  0, batch   246 | loss: 2.8394527MixupTrain:  epoch  0, batch   247 | loss: 3.0647621MixupTrain:  epoch  0, batch   248 | loss: 2.8464015MixupTrain:  epoch  0, batch   249 | loss: 3.1573415MixupTrain:  epoch  0, batch   250 | loss: 3.1030493MixupTrain:  epoch  0, batch   251 | loss: 2.9010015MixupTrain:  epoch  0, batch   252 | loss: 2.9349988MixupTrain:  epoch  0, batch   253 | loss: 3.0456069MixupTrain:  epoch  0, batch   254 | loss: 2.9966516MixupTrain:  epoch  0, batch   255 | loss: 3.0023847MixupTrain:  epoch  0, batch   256 | loss: 2.8846278MixupTrain:  epoch  0, batch   257 | loss: 2.8946748MixupTrain:  epoch  0, batch   258 | loss: 3.2298949MixupTrain:  epoch  0, batch   259 | loss: 2.9063563MixupTrain:  epoch  0, batch   260 | loss: 3.0605662MixupTrain:  epoch  0, batch   261 | loss: 3.1573074MixupTrain:  epoch  0, batch   262 | loss: 3.2383738MixupTrain:  epoch  0, batch   263 | loss: 2.8546805MixupTrain:  epoch  0, batch   264 | loss: 3.0596578MixupTrain:  epoch  0, batch   265 | loss: 2.8766763MixupTrain:  epoch  0, batch   266 | loss: 2.8572357MixupTrain:  epoch  0, batch   267 | loss: 3.1456122MixupTrain:  epoch  0, batch   268 | loss: 3.0310502MixupTrain:  epoch  0, batch   269 | loss: 3.0163078MixupTrain:  epoch  0, batch   270 | loss: 2.8318408MixupTrain:  epoch  0, batch   271 | loss: 3.0360193MixupTrain:  epoch  0, batch   272 | loss: 2.8777413MixupTrain:  epoch  0, batch   273 | loss: 2.8939698MixupTrain:  epoch  0, batch   274 | loss: 2.9934068MixupTrain:  epoch  0, batch   275 | loss: 3.0429897MixupTrain:  epoch  0, batch   276 | loss: 3.1322305MixupTrain:  epoch  0, batch   277 | loss: 3.0897877MixupTrain:  epoch  0, batch   278 | loss: 2.9946837MixupTrain:  epoch  0, batch   279 | loss: 3.1904614MixupTrain:  epoch  0, batch   280 | loss: 2.9603372MixupTrain:  epoch  0, batch   281 | loss: 2.8032479MixupTrain:  epoch  0, batch   282 | loss: 2.8758292MixupTrain:  epoch  0, batch   283 | loss: 2.9303470MixupTrain:  epoch  0, batch   284 | loss: 3.0437174MixupTrain:  epoch  0, batch   285 | loss: 3.0581391MixupTrain:  epoch  0, batch   286 | loss: 3.0244989MixupTrain:  epoch  0, batch   287 | loss: 3.2180364MixupTrain:  epoch  0, batch   288 | loss: 3.0406995MixupTrain:  epoch  0, batch   289 | loss: 2.9952755MixupTrain:  epoch  0, batch   290 | loss: 2.9431493MixupTrain:  epoch  0, batch   291 | loss: 3.1039376MixupTrain:  epoch  0, batch   292 | loss: 3.2610068MixupTrain:  epoch  0, batch   293 | loss: 2.8833275MixupTrain:  epoch  0, batch   294 | loss: 3.0185871MixupTrain:  epoch  0, batch   295 | loss: 2.9406548MixupTrain:  epoch  0, batch   296 | loss: 2.9175649MixupTrain:  epoch  0, batch   297 | loss: 3.0167913MixupTrain:  epoch  0, batch   298 | loss: 2.8518634MixupTrain:  epoch  0, batch   299 | loss: 3.1166549MixupTrain:  epoch  0, batch   300 | loss: 2.9405942MixupTrain:  epoch  0, batch   301 | loss: 2.9498138MixupTrain:  epoch  0, batch   302 | loss: 2.9360347MixupTrain:  epoch  0, batch   303 | loss: 3.0334439MixupTrain:  epoch  0, batch   304 | loss: 2.8919342MixupTrain:  epoch  0, batch   305 | loss: 3.0557168MixupTrain:  epoch  0, batch   306 | loss: 2.8643160MixupTrain:  epoch  0, batch   307 | loss: 2.9449811MixupTrain:  epoch  0, batch   308 | loss: 2.9511766MixupTrain:  epoch  0, batch   309 | loss: 2.9350572MixupTrain:  epoch  0, batch   310 | loss: 2.9380605MixupTrain:  epoch  0, batch   311 | loss: 3.0294235MixupTrain:  epoch  0, batch   312 | loss: 3.1090014MixupTrain:  epoch  0, batch   313 | loss: 2.9603100MixupTrain:  epoch  0, batch   314 | loss: 2.9905419MixupTrain:  epoch  0, batch   315 | loss: 3.0489879MixupTrain:  epoch  0, batch   316 | loss: 2.9930325MixupTrain:  epoch  0, batch   317 | loss: 3.0104785MixupTrain:  epoch  0, batch   318 | loss: 3.1067491MixupTrain:  epoch  0, batch   319 | loss: 2.8393188MixupTrain:  epoch  0, batch   320 | loss: 3.1690550MixupTrain:  epoch  0, batch   321 | loss: 3.1607795MixupTrain:  epoch  0, batch   322 | loss: 2.9397917MixupTrain:  epoch  0, batch   323 | loss: 2.9419041MixupTrain:  epoch  0, batch   324 | loss: 2.8965914MixupTrain:  epoch  0, batch   325 | loss: 2.9436648MixupTrain:  epoch  0, batch   326 | loss: 2.9579630MixupTrain:  epoch  0, batch   327 | loss: 2.9524236MixupTrain:  epoch  0, batch   328 | loss: 2.9874792MixupTrain:  epoch  0, batch   329 | loss: 3.0471251MixupTrain:  epoch  0, batch   330 | loss: 3.0436602MixupTrain:  epoch  0, batch   331 | loss: 2.8743932MixupTrain:  epoch  0, batch   332 | loss: 3.0438755MixupTrain:  epoch  0, batch   333 | loss: 2.9453878MixupTrain:  epoch  0, batch   334 | loss: 2.8674660MixupTrain:  epoch  0, batch   335 | loss: 3.0961497MixupTrain:  epoch  0, batch   336 | loss: 2.8420806MixupTrain:  epoch  0, batch   337 | loss: 2.9919465MixupTrain:  epoch  0, batch   338 | loss: 3.0583756MixupTrain:  epoch  0, batch   339 | loss: 2.9073710MixupTrain:  epoch  0, batch   340 | loss: 3.0438406MixupTrain:  epoch  0, batch   341 | loss: 2.8939986MixupTrain:  epoch  0, batch   342 | loss: 3.0938768MixupTrain:  epoch  0, batch   343 | loss: 3.0185518MixupTrain:  epoch  0, batch   344 | loss: 3.0337615MixupTrain:  epoch  0, batch   345 | loss: 2.8376789MixupTrain:  epoch  0, batch   346 | loss: 2.9298668MixupTrain:  epoch  0, batch   347 | loss: 2.9564486MixupTrain:  epoch  0, batch   348 | loss: 3.0136013MixupTrain:  epoch  0, batch   349 | loss: 2.9779506MixupTrain:  epoch  0, batch   350 | loss: 2.8189883MixupTrain:  epoch  0, batch   351 | loss: 3.2041671MixupTrain:  epoch  0, batch   352 | loss: 3.0648727MixupTrain:  epoch  0, batch   353 | loss: 2.8598404MixupTrain:  epoch  0, batch   354 | loss: 2.9533861MixupTrain:  epoch  0, batch   355 | loss: 2.8789346MixupTrain:  epoch  0, batch   356 | loss: 2.7977972MixupTrain:  epoch  0, batch   357 | loss: 2.8605769MixupTrain:  epoch  0, batch   358 | loss: 2.9906864MixupTrain:  epoch  0, batch   359 | loss: 2.8727765MixupTrain:  epoch  0, batch   360 | loss: 2.8564656MixupTrain:  epoch  0, batch   361 | loss: 2.9770701MixupTrain:  epoch  0, batch   362 | loss: 3.0368481MixupTrain:  epoch  0, batch   363 | loss: 2.9951296MixupTrain:  epoch  0, batch   364 | loss: 2.8024929MixupTrain:  epoch  0, batch   365 | loss: 2.8939047MixupTrain:  epoch  0, batch   366 | loss: 2.9729500MixupTrain:  epoch  0, batch   367 | loss: 2.8891098MixupTrain:  epoch  0, batch   368 | loss: 2.9818296MixupTrain:  epoch  0, batch   369 | loss: 2.8901749MixupTrain:  epoch  0, batch   370 | loss: 2.8816981MixupTrain:  epoch  0, batch   371 | loss: 2.8333738MixupTrain:  epoch  0, batch   372 | loss: 3.0952926MixupTrain:  epoch  0, batch   373 | loss: 3.0344086MixupTrain:  epoch  0, batch   374 | loss: 3.0106976MixupTrain:  epoch  0, batch   375 | loss: 2.9682870MixupTrain:  epoch  0, batch   376 | loss: 3.0212545MixupTrain:  epoch  0, batch   377 | loss: 2.8697612MixupTrain:  epoch  0, batch   378 | loss: 2.9899225MixupTrain:  epoch  0, batch   379 | loss: 3.0365524MixupTrain:  epoch  0, batch   380 | loss: 2.9881783MixupTrain:  epoch  0, batch   381 | loss: 2.7963281MixupTrain:  epoch  0, batch   382 | loss: 2.9163613MixupTrain:  epoch  0, batch   383 | loss: 2.9394569MixupTrain:  epoch  0, batch   384 | loss: 2.9991903MixupTrain:  epoch  0, batch   385 | loss: 2.9804430MixupTrain:  epoch  0, batch   386 | loss: 3.0179625MixupTrain:  epoch  0, batch   387 | loss: 2.9827480MixupTrain:  epoch  0, batch   388 | loss: 2.9332628MixupTrain:  epoch  0, batch   389 | loss: 2.9243350MixupTrain:  epoch  0, batch   390 | loss: 2.9801311MixupTrain:  epoch  0, batch   391 | loss: 2.9122629MixupTrain:  epoch  0, batch   392 | loss: 2.9138384MixupTrain:  epoch  0, batch   393 | loss: 3.0940661MixupTrain:  epoch  0, batch   394 | loss: 3.0254171MixupTrain:  epoch  0, batch   395 | loss: 3.0273476MixupTrain:  epoch  0, batch   396 | loss: 2.9510727MixupTrain:  epoch  0, batch   397 | loss: 3.0198309MixupTrain:  epoch  0, batch   398 | loss: 3.0108626MixupTrain:  epoch  0, batch   399 | loss: 3.0378685MixupTrain:  epoch  0, batch   400 | loss: 3.0790882MixupTrain:  epoch  0, batch   401 | loss: 3.0104303MixupTrain:  epoch  0, batch   402 | loss: 2.8702068MixupTrain:  epoch  0, batch   403 | loss: 3.0247362MixupTrain:  epoch  0, batch   404 | loss: 2.9881852MixupTrain:  epoch  0, batch   405 | loss: 3.0400276MixupTrain:  epoch  0, batch   406 | loss: 2.9575610MixupTrain:  epoch  0, batch   407 | loss: 2.8991067MixupTrain:  epoch  0, batch   408 | loss: 2.9315791MixupTrain:  epoch  0, batch   409 | loss: 2.8991361MixupTrain:  epoch  0, batch   410 | loss: 2.8857646MixupTrain:  epoch  0, batch   411 | loss: 2.9771712MixupTrain:  epoch  0, batch   412 | loss: 2.9559460MixupTrain:  epoch  0, batch   413 | loss: 2.9871659MixupTrain:  epoch  0, batch   414 | loss: 2.8403256MixupTrain:  epoch  0, batch   415 | loss: 3.0251241MixupTrain:  epoch  0, batch   416 | loss: 2.8021216MixupTrain:  epoch  0, batch   417 | loss: 2.9035392MixupTrain:  epoch  0, batch   418 | loss: 3.0809901MixupTrain:  epoch  0, batch   419 | loss: 3.2057257MixupTrain:  epoch  0, batch   420 | loss: 2.9260855MixupTrain:  epoch  0, batch   421 | loss: 2.8838127MixupTrain:  epoch  0, batch   422 | loss: 2.7898226MixupTrain:  epoch  0, batch   423 | loss: 2.8818033MixupTrain:  epoch  0, batch   424 | loss: 3.0461357MixupTrain:  epoch  0, batch   425 | loss: 3.0378370MixupTrain:  epoch  0, batch   426 | loss: 2.8660254MixupTrain:  epoch  0, batch   427 | loss: 2.9046767MixupTrain:  epoch  0, batch   428 | loss: 2.9477282MixupTrain:  epoch  0, batch   429 | loss: 2.9315591MixupTrain:  epoch  0, batch   430 | loss: 2.8265581MixupTrain:  epoch  0, batch   431 | loss: 3.0076630MixupTrain:  epoch  0, batch   432 | loss: 3.0712395MixupTrain:  epoch  0, batch   433 | loss: 2.9960403MixupTrain:  epoch  0, batch   434 | loss: 2.9683728MixupTrain:  epoch  0, batch   435 | loss: 2.7400630MixupTrain:  epoch  0, batch   436 | loss: 3.1002607MixupTrain:  epoch  0, batch   437 | loss: 2.8064961MixupTrain:  epoch  0, batch   438 | loss: 2.8745351MixupTrain:  epoch  0, batch   439 | loss: 3.0440340MixupTrain:  epoch  0, batch   440 | loss: 2.8446095MixupTrain:  epoch  0, batch   441 | loss: 2.9899035MixupTrain:  epoch  0, batch   442 | loss: 2.8715556MixupTrain:  epoch  0, batch   443 | loss: 2.9332786MixupTrain:  epoch  0, batch   444 | loss: 3.0337791MixupTrain:  epoch  0, batch   445 | loss: 2.9982939MixupTrain:  epoch  0, batch   446 | loss: 2.9951890MixupTrain:  epoch  0, batch   447 | loss: 2.7656343MixupTrain:  epoch  0, batch   448 | loss: 2.9924519MixupTrain:  epoch  0, batch   449 | loss: 2.8502405MixupTrain:  epoch  0, batch   450 | loss: 2.9504104MixupTrain:  epoch  0, batch   451 | loss: 2.8962734MixupTrain:  epoch  0, batch   452 | loss: 2.8189368MixupTrain:  epoch  0, batch   453 | loss: 3.0637507MixupTrain:  epoch  0, batch   454 | loss: 3.0313454MixupTrain:  epoch  0, batch   455 | loss: 2.9845662MixupTrain:  epoch  0, batch   456 | loss: 2.8343275MixupTrain:  epoch  0, batch   457 | loss: 2.9756331MixupTrain:  epoch  0, batch   458 | loss: 2.9304833MixupTrain:  epoch  0, batch   459 | loss: 2.8353574MixupTrain:  epoch  0, batch   460 | loss: 2.9188323MixupTrain:  epoch  0, batch   461 | loss: 2.8405786MixupTrain:  epoch  0, batch   462 | loss: 2.8831496MixupTrain:  epoch  0, batch   463 | loss: 3.0070124MixupTrain:  epoch  0, batch   464 | loss: 2.9271812MixupTrain:  epoch  0, batch   465 | loss: 3.0292368MixupTrain:  epoch  0, batch   466 | loss: 2.8144369MixupTrain:  epoch  0, batch   467 | loss: 2.9500227MixupTrain:  epoch  0, batch   468 | loss: 3.1141963MixupTrain:  epoch  0, batch   469 | loss: 2.7888882MixupTrain:  epoch  0, batch   470 | loss: 3.0762520MixupTrain:  epoch  0, batch   471 | loss: 2.9406993MixupTrain:  epoch  0, batch   472 | loss: 2.9254551MixupTrain:  epoch  0, batch   473 | loss: 3.0018582MixupTrain:  epoch  0, batch   474 | loss: 3.0737863MixupTrain:  epoch  0, batch   475 | loss: 2.9568737MixupTrain:  epoch  0, batch   476 | loss: 2.7717099MixupTrain:  epoch  0, batch   477 | loss: 2.9574924MixupTrain:  epoch  0, batch   478 | loss: 2.8847868MixupTrain:  epoch  0, batch   479 | loss: 2.8848288MixupTrain:  epoch  0, batch   480 | loss: 2.8458972MixupTrain:  epoch  0, batch   481 | loss: 2.9586360MixupTrain:  epoch  0, batch   482 | loss: 2.9935575MixupTrain:  epoch  0, batch   483 | loss: 3.1951261MixupTrain:  epoch  0, batch   484 | loss: 2.9966862MixupTrain:  epoch  0, batch   485 | loss: 3.0207658MixupTrain:  epoch  0, batch   486 | loss: 3.0636458MixupTrain:  epoch  0, batch   487 | loss: 2.8867440MixupTrain:  epoch  0, batch   488 | loss: 2.8744860MixupTrain:  epoch  0, batch   489 | loss: 3.0087299MixupTrain:  epoch  0, batch   490 | loss: 2.8504024MixupTrain:  epoch  0, batch   491 | loss: 3.0674367MixupTrain:  epoch  0, batch   492 | loss: 3.0081897MixupTrain:  epoch  0, batch   493 | loss: 2.9301019MixupTrain:  epoch  0, batch   494 | loss: 2.8407884MixupTrain:  epoch  0, batch   495 | loss: 3.0224388MixupTrain:  epoch  0, batch   496 | loss: 2.9339924MixupTrain:  epoch  0, batch   497 | loss: 2.8410931MixupTrain:  epoch  0, batch   498 | loss: 2.9903903MixupTrain:  epoch  0, batch   499 | loss: 2.9613972MixupTrain:  epoch  0, batch   500 | loss: 2.9035065MixupTrain:  epoch  0, batch   501 | loss: 2.9352627MixupTrain:  epoch  0, batch   502 | loss: 2.9762683MixupTrain:  epoch  0, batch   503 | loss: 3.0009894MixupTrain:  epoch  0, batch   504 | loss: 2.9487834MixupTrain:  epoch  0, batch   505 | loss: 2.9196503MixupTrain:  epoch  0, batch   506 | loss: 2.9257178MixupTrain:  epoch  0, batch   507 | loss: 2.9567122MixupTrain:  epoch  0, batch   508 | loss: 2.7603099MixupTrain:  epoch  0, batch   509 | loss: 2.8629551MixupTrain:  epoch  0, batch   510 | loss: 2.8457170MixupTrain:  epoch  0, batch   511 | loss: 2.8748226MixupTrain:  epoch  0, batch   512 | loss: 2.9087784MixupTrain:  epoch  0, batch   513 | loss: 2.9314840MixupTrain:  epoch  0, batch   514 | loss: 3.1387675MixupTrain:  epoch  0, batch   515 | loss: 2.9674356MixupTrain:  epoch  0, batch   516 | loss: 2.8836401MixupTrain:  epoch  0, batch   517 | loss: 3.0099783MixupTrain:  epoch  0, batch   518 | loss: 2.8478279MixupTrain:  epoch  0, batch   519 | loss: 2.8909087MixupTrain:  epoch  0, batch   520 | loss: 2.9500108MixupTrain:  epoch  0, batch   521 | loss: 2.9659946MixupTrain:  epoch  0, batch   522 | loss: 2.9823174MixupTrain:  epoch  0, batch   523 | loss: 2.9750736MixupTrain:  epoch  0, batch   524 | loss: 2.8177152MixupTrain:  epoch  0, batch   525 | loss: 2.9822352MixupTrain:  epoch  0, batch   526 | loss: 3.0193262MixupTrain:  epoch  0, batch   527 | loss: 2.9125457MixupTrain:  epoch  0, batch   528 | loss: 2.9106183MixupTrain:  epoch  0, batch   529 | loss: 2.8529916MixupTrain:  epoch  0, batch   530 | loss: 2.9706349MixupTrain:  epoch  0, batch   531 | loss: 2.9119878MixupTrain:  epoch  0, batch   532 | loss: 2.8397026MixupTrain:  epoch  0, batch   533 | loss: 2.8319016MixupTrain:  epoch  0, batch   534 | loss: 2.5124216
MemoryTrain:  epoch  0, batch     0 | loss: 1.2298543MemoryTrain:  epoch  0, batch     1 | loss: 1.3219935MemoryTrain:  epoch  0, batch     2 | loss: 1.5948100MemoryTrain:  epoch  0, batch     3 | loss: 1.3447559MemoryTrain:  epoch  0, batch     4 | loss: 1.5971844MemoryTrain:  epoch  0, batch     5 | loss: 1.7307143MemoryTrain:  epoch  0, batch     6 | loss: 1.6150250MemoryTrain:  epoch  1, batch     0 | loss: 1.3025143MemoryTrain:  epoch  1, batch     1 | loss: 1.3671080MemoryTrain:  epoch  1, batch     2 | loss: 1.2768625MemoryTrain:  epoch  1, batch     3 | loss: 1.2694122MemoryTrain:  epoch  1, batch     4 | loss: 1.5438914MemoryTrain:  epoch  1, batch     5 | loss: 1.3104715MemoryTrain:  epoch  1, batch     6 | loss: 1.3020333MemoryTrain:  epoch  2, batch     0 | loss: 1.2183495MemoryTrain:  epoch  2, batch     1 | loss: 1.2842203MemoryTrain:  epoch  2, batch     2 | loss: 1.2369654MemoryTrain:  epoch  2, batch     3 | loss: 1.2354543MemoryTrain:  epoch  2, batch     4 | loss: 1.2751219MemoryTrain:  epoch  2, batch     5 | loss: 1.2041050MemoryTrain:  epoch  2, batch     6 | loss: 1.3363893MemoryTrain:  epoch  3, batch     0 | loss: 1.2299790MemoryTrain:  epoch  3, batch     1 | loss: 1.2966924MemoryTrain:  epoch  3, batch     2 | loss: 1.1947784MemoryTrain:  epoch  3, batch     3 | loss: 1.1984134MemoryTrain:  epoch  3, batch     4 | loss: 1.1816499MemoryTrain:  epoch  3, batch     5 | loss: 1.2198143MemoryTrain:  epoch  3, batch     6 | loss: 1.2724519MemoryTrain:  epoch  4, batch     0 | loss: 1.2334337MemoryTrain:  epoch  4, batch     1 | loss: 1.2464401MemoryTrain:  epoch  4, batch     2 | loss: 1.2021078MemoryTrain:  epoch  4, batch     3 | loss: 1.2129009MemoryTrain:  epoch  4, batch     4 | loss: 1.2095422MemoryTrain:  epoch  4, batch     5 | loss: 1.2165447MemoryTrain:  epoch  4, batch     6 | loss: 1.2716140MemoryTrain:  epoch  5, batch     0 | loss: 1.1846533MemoryTrain:  epoch  5, batch     1 | loss: 1.1930722MemoryTrain:  epoch  5, batch     2 | loss: 1.2344848MemoryTrain:  epoch  5, batch     3 | loss: 1.2058578MemoryTrain:  epoch  5, batch     4 | loss: 1.1652064MemoryTrain:  epoch  5, batch     5 | loss: 1.2537574MemoryTrain:  epoch  5, batch     6 | loss: 1.1588964MemoryTrain:  epoch  6, batch     0 | loss: 1.1786214MemoryTrain:  epoch  6, batch     1 | loss: 1.1849080MemoryTrain:  epoch  6, batch     2 | loss: 1.1868434MemoryTrain:  epoch  6, batch     3 | loss: 1.1909238MemoryTrain:  epoch  6, batch     4 | loss: 1.1825666MemoryTrain:  epoch  6, batch     5 | loss: 1.2005532MemoryTrain:  epoch  6, batch     6 | loss: 1.2177720MemoryTrain:  epoch  7, batch     0 | loss: 1.1662827MemoryTrain:  epoch  7, batch     1 | loss: 1.1623631MemoryTrain:  epoch  7, batch     2 | loss: 1.2031486MemoryTrain:  epoch  7, batch     3 | loss: 1.1824058MemoryTrain:  epoch  7, batch     4 | loss: 1.2022094MemoryTrain:  epoch  7, batch     5 | loss: 1.2128756MemoryTrain:  epoch  7, batch     6 | loss: 1.1893057MemoryTrain:  epoch  8, batch     0 | loss: 1.1848440MemoryTrain:  epoch  8, batch     1 | loss: 1.1994538MemoryTrain:  epoch  8, batch     2 | loss: 1.1995347MemoryTrain:  epoch  8, batch     3 | loss: 1.1607796MemoryTrain:  epoch  8, batch     4 | loss: 1.1950203MemoryTrain:  epoch  8, batch     5 | loss: 1.1721985MemoryTrain:  epoch  8, batch     6 | loss: 1.1711791MemoryTrain:  epoch  9, batch     0 | loss: 1.1864738MemoryTrain:  epoch  9, batch     1 | loss: 1.1980968MemoryTrain:  epoch  9, batch     2 | loss: 1.1816800MemoryTrain:  epoch  9, batch     3 | loss: 1.1891589MemoryTrain:  epoch  9, batch     4 | loss: 1.1558546MemoryTrain:  epoch  9, batch     5 | loss: 1.1938311MemoryTrain:  epoch  9, batch     6 | loss: 1.1762803
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    4 | acc: 100.00%,  total acc: 90.00%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 88.54%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 85.71%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 86.72%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 86.88%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 86.93%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 85.94%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 86.06%   [EVAL] batch:   13 | acc: 50.00%,  total acc: 83.48%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 56.25%,  total acc: 62.50%   [EVAL] batch:    2 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    3 | acc: 50.00%,  total acc: 59.38%   [EVAL] batch:    4 | acc: 56.25%,  total acc: 58.75%   [EVAL] batch:    5 | acc: 43.75%,  total acc: 56.25%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 58.93%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 63.28%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 66.67%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 68.12%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 70.45%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 72.40%   [EVAL] batch:   12 | acc: 43.75%,  total acc: 70.19%   [EVAL] batch:   13 | acc: 25.00%,  total acc: 66.96%   [EVAL] batch:   14 | acc: 50.00%,  total acc: 65.83%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 65.23%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 65.81%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 65.62%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 65.46%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 66.56%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 67.86%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 68.75%   [EVAL] batch:   22 | acc: 87.50%,  total acc: 69.57%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 70.57%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 71.75%   [EVAL] batch:   25 | acc: 87.50%,  total acc: 72.36%   [EVAL] batch:   26 | acc: 87.50%,  total acc: 72.92%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 73.88%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 74.78%   [EVAL] batch:   29 | acc: 75.00%,  total acc: 74.79%   [EVAL] batch:   30 | acc: 81.25%,  total acc: 75.00%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 75.59%   [EVAL] batch:   32 | acc: 62.50%,  total acc: 75.19%   [EVAL] batch:   33 | acc: 56.25%,  total acc: 74.63%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 74.64%   [EVAL] batch:   35 | acc: 56.25%,  total acc: 74.13%   [EVAL] batch:   36 | acc: 43.75%,  total acc: 73.31%   [EVAL] batch:   37 | acc: 56.25%,  total acc: 72.86%   [EVAL] batch:   38 | acc: 25.00%,  total acc: 71.63%   [EVAL] batch:   39 | acc: 25.00%,  total acc: 70.47%   [EVAL] batch:   40 | acc: 0.00%,  total acc: 68.75%   [EVAL] batch:   41 | acc: 0.00%,  total acc: 67.11%   [EVAL] batch:   42 | acc: 0.00%,  total acc: 65.55%   [EVAL] batch:   43 | acc: 0.00%,  total acc: 64.06%   [EVAL] batch:   44 | acc: 6.25%,  total acc: 62.78%   [EVAL] batch:   45 | acc: 12.50%,  total acc: 61.68%   [EVAL] batch:   46 | acc: 0.00%,  total acc: 60.37%   [EVAL] batch:   47 | acc: 6.25%,  total acc: 59.24%   [EVAL] batch:   48 | acc: 31.25%,  total acc: 58.67%   [EVAL] batch:   49 | acc: 18.75%,  total acc: 57.88%   [EVAL] batch:   50 | acc: 81.25%,  total acc: 58.33%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 59.01%   [EVAL] batch:   52 | acc: 75.00%,  total acc: 59.32%   [EVAL] batch:   53 | acc: 68.75%,  total acc: 59.49%   [EVAL] batch:   54 | acc: 56.25%,  total acc: 59.43%   [EVAL] batch:   55 | acc: 0.00%,  total acc: 58.37%   [EVAL] batch:   56 | acc: 0.00%,  total acc: 57.35%   [EVAL] batch:   57 | acc: 0.00%,  total acc: 56.36%   [EVAL] batch:   58 | acc: 0.00%,  total acc: 55.40%   [EVAL] batch:   59 | acc: 0.00%,  total acc: 54.48%   [EVAL] batch:   60 | acc: 0.00%,  total acc: 53.59%   [EVAL] batch:   61 | acc: 0.00%,  total acc: 52.72%   [EVAL] batch:   62 | acc: 6.25%,  total acc: 51.98%   [EVAL] batch:   63 | acc: 6.25%,  total acc: 51.27%   [EVAL] batch:   64 | acc: 12.50%,  total acc: 50.67%   [EVAL] batch:   65 | acc: 25.00%,  total acc: 50.28%   [EVAL] batch:   66 | acc: 68.75%,  total acc: 50.56%   [EVAL] batch:   67 | acc: 37.50%,  total acc: 50.37%   [EVAL] batch:   68 | acc: 37.50%,  total acc: 50.18%   [EVAL] batch:   69 | acc: 81.25%,  total acc: 50.62%   [EVAL] batch:   70 | acc: 62.50%,  total acc: 50.79%   [EVAL] batch:   71 | acc: 62.50%,  total acc: 50.95%   [EVAL] batch:   72 | acc: 81.25%,  total acc: 51.37%   [EVAL] batch:   73 | acc: 100.00%,  total acc: 52.03%   [EVAL] batch:   74 | acc: 93.75%,  total acc: 52.58%   [EVAL] batch:   75 | acc: 100.00%,  total acc: 53.21%   [EVAL] batch:   76 | acc: 81.25%,  total acc: 53.57%   [EVAL] batch:   77 | acc: 31.25%,  total acc: 53.29%   [EVAL] batch:   78 | acc: 18.75%,  total acc: 52.85%   [EVAL] batch:   79 | acc: 25.00%,  total acc: 52.50%   [EVAL] batch:   80 | acc: 18.75%,  total acc: 52.08%   [EVAL] batch:   81 | acc: 56.25%,  total acc: 52.13%   [EVAL] batch:   82 | acc: 43.75%,  total acc: 52.03%   [EVAL] batch:   83 | acc: 56.25%,  total acc: 52.08%   [EVAL] batch:   84 | acc: 31.25%,  total acc: 51.84%   [EVAL] batch:   85 | acc: 56.25%,  total acc: 51.89%   [EVAL] batch:   86 | acc: 18.75%,  total acc: 51.51%   [EVAL] batch:   87 | acc: 68.75%,  total acc: 51.70%   [EVAL] batch:   88 | acc: 81.25%,  total acc: 52.04%   [EVAL] batch:   89 | acc: 37.50%,  total acc: 51.88%   [EVAL] batch:   90 | acc: 43.75%,  total acc: 51.79%   [EVAL] batch:   91 | acc: 50.00%,  total acc: 51.77%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 52.28%   [EVAL] batch:   93 | acc: 100.00%,  total acc: 52.79%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 53.29%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 53.78%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 54.25%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 54.72%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 55.18%   [EVAL] batch:   99 | acc: 93.75%,  total acc: 55.56%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 56.00%   [EVAL] batch:  101 | acc: 100.00%,  total acc: 56.43%   [EVAL] batch:  102 | acc: 100.00%,  total acc: 56.86%   [EVAL] batch:  103 | acc: 100.00%,  total acc: 57.27%   [EVAL] batch:  104 | acc: 100.00%,  total acc: 57.68%   [EVAL] batch:  105 | acc: 100.00%,  total acc: 58.08%   [EVAL] batch:  106 | acc: 68.75%,  total acc: 58.18%   [EVAL] batch:  107 | acc: 68.75%,  total acc: 58.28%   [EVAL] batch:  108 | acc: 68.75%,  total acc: 58.37%   [EVAL] batch:  109 | acc: 81.25%,  total acc: 58.58%   [EVAL] batch:  110 | acc: 62.50%,  total acc: 58.61%   [EVAL] batch:  111 | acc: 81.25%,  total acc: 58.82%   [EVAL] batch:  112 | acc: 75.00%,  total acc: 58.96%   [EVAL] batch:  113 | acc: 93.75%,  total acc: 59.27%   [EVAL] batch:  114 | acc: 87.50%,  total acc: 59.51%   [EVAL] batch:  115 | acc: 93.75%,  total acc: 59.81%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 60.15%   [EVAL] batch:  117 | acc: 75.00%,  total acc: 60.28%   [EVAL] batch:  118 | acc: 75.00%,  total acc: 60.40%   [EVAL] batch:  119 | acc: 93.75%,  total acc: 60.68%   [EVAL] batch:  120 | acc: 93.75%,  total acc: 60.95%   [EVAL] batch:  121 | acc: 81.25%,  total acc: 61.12%   [EVAL] batch:  122 | acc: 81.25%,  total acc: 61.28%   [EVAL] batch:  123 | acc: 81.25%,  total acc: 61.44%   [EVAL] batch:  124 | acc: 87.50%,  total acc: 61.65%   [EVAL] batch:  125 | acc: 31.25%,  total acc: 61.41%   
cur_acc:  ['0.8655', '0.7788', '0.7000', '0.6023', '0.8507', '0.9018', '0.8348']
his_acc:  ['0.8655', '0.8472', '0.7833', '0.6728', '0.6010', '0.6127', '0.6141']
CurrentTrain: epoch  0, batch     0 | loss: 5.2657447CurrentTrain: epoch  0, batch     1 | loss: 5.9193430CurrentTrain: epoch  1, batch     0 | loss: 3.7415926CurrentTrain: epoch  1, batch     1 | loss: 5.6645365CurrentTrain: epoch  2, batch     0 | loss: 4.0070195CurrentTrain: epoch  2, batch     1 | loss: 3.9574459CurrentTrain: epoch  3, batch     0 | loss: 3.4206176CurrentTrain: epoch  3, batch     1 | loss: 2.5295334CurrentTrain: epoch  4, batch     0 | loss: 2.5541127CurrentTrain: epoch  4, batch     1 | loss: 3.3937311CurrentTrain: epoch  5, batch     0 | loss: 2.8431854CurrentTrain: epoch  5, batch     1 | loss: 2.1055658CurrentTrain: epoch  6, batch     0 | loss: 2.1831901CurrentTrain: epoch  6, batch     1 | loss: 2.9582760CurrentTrain: epoch  7, batch     0 | loss: 2.8375640CurrentTrain: epoch  7, batch     1 | loss: 2.1516798CurrentTrain: epoch  8, batch     0 | loss: 2.3837900CurrentTrain: epoch  8, batch     1 | loss: 2.3062973CurrentTrain: epoch  9, batch     0 | loss: 2.0974717CurrentTrain: epoch  9, batch     1 | loss: 2.3335056
#### person country of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person country of birth" means a person was born in a country.
Here is an example:
Relation: person country of birth
Context: de maiziere noted that germany took in another former inmate from guantanamo in 2006 -- murat kurnaz , a turkish national who was born and grew up in germany .
Head Entity: murat kurnaz
Tail Entity: germany
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person country of birth":
Relation: person country of birth  
Context: born in 1985 in the bustling city of new delhi, arjun kapoor has always been proud of his indian heritage and culture.  
Head Entity: arjun kapoor  
Tail Entity: india  

Relation: person country of birth  
Context: during the interview, she revealed that she was born in the picturesque town of florence, which has greatly influenced her artistic style.  
Head Entity: she  
Tail Entity: italy  
#### organization website ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization website" means an organization 's website is an url.
Here is an example:
Relation: organization website
Context: http://www.pentaxsailaway.com/ paul j. sail away with pentax sweepstakes
Head Entity: pentax
Tail Entity: http://www.pentaxsailaway.com/
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization website":
Relation: organization website  
Context: Visit our official site at https://www.techinnovators.com for the latest updates on our projects.  
Head Entity: Tech Innovators  
Tail Entity: https://www.techinnovators.com  

Relation: organization website  
Context: For more information about our services, check out our website at http://www.greenearthsolutions.org.  
Head Entity: Green Earth Solutions  
Tail Entity: http://www.greenearthsolutions.org  
#### organization shareholders ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization shareholders" means an organization was invested by a person.
Here is an example:
Relation: organization shareholders
Context: marsans already owns the madrid-based air comet and 95 percent of aerolineas argentinas .
Head Entity: aerolineas argentinas
Tail Entity: marsans
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization shareholders":
Relation: organization shareholders  
Context: tech giant apple has recently acquired a significant stake in the innovative startup nextdoor.  
Head Entity: nextdoor  
Tail Entity: apple  

Relation: organization shareholders  
Context: the investment firm blackrock has increased its holdings in the renewable energy company sunrun.  
Head Entity: sunrun  
Tail Entity: blackrock  
#### organization dissolved ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization dissolved" means an organization was dissolved in a date.
Here is an example:
Relation: organization dissolved
Context: in 1953 , five years after the state was established , the jnf was dissolved and re-organized as an israeli company .
Head Entity: jnf
Tail Entity: 1953
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization dissolved":
Relation: organization dissolved  
Context: The board of directors announced that the nonprofit organization was officially dissolved in 2020 due to financial difficulties.  
Head Entity: nonprofit organization  
Tail Entity: 2020  

Relation: organization dissolved  
Context: After decades of service, the local charity was dissolved in 2018, marking the end of an era for community support.  
Head Entity: local charity  
Tail Entity: 2018  
#### organization founded by ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization founded by" means an organization was found by a person.
Here is an example:
Relation: organization founded by
Context: `` i have no greater obligation than to ensure the safely of airline travelers in this country , '' transportation secretary ray lahood said in a joint statement with j. randolph babbitt , administrator of the federal aviation administration , that was issued on the eve of a senate hearing on aviation safety .
Head Entity: federal aviation administration
Tail Entity: j. randolph babbitt
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization founded by":
Relation: organization founded by  
Context: In 1975, the renowned physicist and entrepreneur, Dr. Jane Smith, established Quantum Innovations, a company dedicated to advancing quantum computing technologies.  
Head Entity: Quantum Innovations  
Tail Entity: Dr. Jane Smith  

Relation: organization founded by  
Context: The charity organization, Hope for Tomorrow, was created in 2010 by the well-known philanthropist, Michael Johnson, to support underprivileged children around the world.  
Head Entity: Hope for Tomorrow  
Tail Entity: Michael Johnson  
Mixup data size:  10630
MixupTrain:  epoch  0, batch     0 | loss: 3.8558583MixupTrain:  epoch  0, batch     1 | loss: 3.9669919MixupTrain:  epoch  0, batch     2 | loss: 4.2706747MixupTrain:  epoch  0, batch     3 | loss: 3.1637101MixupTrain:  epoch  0, batch     4 | loss: 3.9312167MixupTrain:  epoch  0, batch     5 | loss: 3.7976460MixupTrain:  epoch  0, batch     6 | loss: 3.6941438MixupTrain:  epoch  0, batch     7 | loss: 3.7330751MixupTrain:  epoch  0, batch     8 | loss: 3.6546226MixupTrain:  epoch  0, batch     9 | loss: 3.3905280MixupTrain:  epoch  0, batch    10 | loss: 3.5462985MixupTrain:  epoch  0, batch    11 | loss: 3.5299892MixupTrain:  epoch  0, batch    12 | loss: 3.7673128MixupTrain:  epoch  0, batch    13 | loss: 3.7688649MixupTrain:  epoch  0, batch    14 | loss: 3.7224879MixupTrain:  epoch  0, batch    15 | loss: 3.6404569MixupTrain:  epoch  0, batch    16 | loss: 3.8119898MixupTrain:  epoch  0, batch    17 | loss: 3.5524883MixupTrain:  epoch  0, batch    18 | loss: 3.1757264MixupTrain:  epoch  0, batch    19 | loss: 3.3556001MixupTrain:  epoch  0, batch    20 | loss: 3.3589566MixupTrain:  epoch  0, batch    21 | loss: 3.3515253MixupTrain:  epoch  0, batch    22 | loss: 3.2962260MixupTrain:  epoch  0, batch    23 | loss: 3.5152049MixupTrain:  epoch  0, batch    24 | loss: 3.3720481MixupTrain:  epoch  0, batch    25 | loss: 3.3517625MixupTrain:  epoch  0, batch    26 | loss: 3.2181363MixupTrain:  epoch  0, batch    27 | loss: 3.1850755MixupTrain:  epoch  0, batch    28 | loss: 3.4257240MixupTrain:  epoch  0, batch    29 | loss: 3.5056386MixupTrain:  epoch  0, batch    30 | loss: 3.2352180MixupTrain:  epoch  0, batch    31 | loss: 3.3495021MixupTrain:  epoch  0, batch    32 | loss: 3.4544597MixupTrain:  epoch  0, batch    33 | loss: 3.3347340MixupTrain:  epoch  0, batch    34 | loss: 3.4402347MixupTrain:  epoch  0, batch    35 | loss: 3.1683455MixupTrain:  epoch  0, batch    36 | loss: 3.3233609MixupTrain:  epoch  0, batch    37 | loss: 2.8419905MixupTrain:  epoch  0, batch    38 | loss: 3.3566999MixupTrain:  epoch  0, batch    39 | loss: 3.5320365MixupTrain:  epoch  0, batch    40 | loss: 3.5167642MixupTrain:  epoch  0, batch    41 | loss: 3.2465975MixupTrain:  epoch  0, batch    42 | loss: 3.3000584MixupTrain:  epoch  0, batch    43 | loss: 3.3536410MixupTrain:  epoch  0, batch    44 | loss: 3.2769923MixupTrain:  epoch  0, batch    45 | loss: 3.7170448MixupTrain:  epoch  0, batch    46 | loss: 3.5757151MixupTrain:  epoch  0, batch    47 | loss: 3.2740116MixupTrain:  epoch  0, batch    48 | loss: 3.1522787MixupTrain:  epoch  0, batch    49 | loss: 3.3507371MixupTrain:  epoch  0, batch    50 | loss: 3.1004441MixupTrain:  epoch  0, batch    51 | loss: 3.2242002MixupTrain:  epoch  0, batch    52 | loss: 2.9647472MixupTrain:  epoch  0, batch    53 | loss: 3.1573706MixupTrain:  epoch  0, batch    54 | loss: 3.3509970MixupTrain:  epoch  0, batch    55 | loss: 3.3549738MixupTrain:  epoch  0, batch    56 | loss: 3.3793809MixupTrain:  epoch  0, batch    57 | loss: 3.3287110MixupTrain:  epoch  0, batch    58 | loss: 3.1244071MixupTrain:  epoch  0, batch    59 | loss: 2.8641624MixupTrain:  epoch  0, batch    60 | loss: 3.0727000MixupTrain:  epoch  0, batch    61 | loss: 3.0454993MixupTrain:  epoch  0, batch    62 | loss: 3.4151726MixupTrain:  epoch  0, batch    63 | loss: 3.2389750MixupTrain:  epoch  0, batch    64 | loss: 3.3357067MixupTrain:  epoch  0, batch    65 | loss: 3.3366594MixupTrain:  epoch  0, batch    66 | loss: 3.3773179MixupTrain:  epoch  0, batch    67 | loss: 3.3472886MixupTrain:  epoch  0, batch    68 | loss: 3.1328640MixupTrain:  epoch  0, batch    69 | loss: 3.0345511MixupTrain:  epoch  0, batch    70 | loss: 3.0308638MixupTrain:  epoch  0, batch    71 | loss: 3.0067525MixupTrain:  epoch  0, batch    72 | loss: 3.0375245MixupTrain:  epoch  0, batch    73 | loss: 3.0512474MixupTrain:  epoch  0, batch    74 | loss: 2.9673333MixupTrain:  epoch  0, batch    75 | loss: 3.2904491MixupTrain:  epoch  0, batch    76 | loss: 3.0155606MixupTrain:  epoch  0, batch    77 | loss: 3.2395706MixupTrain:  epoch  0, batch    78 | loss: 3.0883474MixupTrain:  epoch  0, batch    79 | loss: 3.1086273MixupTrain:  epoch  0, batch    80 | loss: 3.1574175MixupTrain:  epoch  0, batch    81 | loss: 3.1234269MixupTrain:  epoch  0, batch    82 | loss: 3.0501113MixupTrain:  epoch  0, batch    83 | loss: 3.0887141MixupTrain:  epoch  0, batch    84 | loss: 3.3384898MixupTrain:  epoch  0, batch    85 | loss: 3.4940956MixupTrain:  epoch  0, batch    86 | loss: 3.1581874MixupTrain:  epoch  0, batch    87 | loss: 3.3427362MixupTrain:  epoch  0, batch    88 | loss: 3.2141991MixupTrain:  epoch  0, batch    89 | loss: 3.0857096MixupTrain:  epoch  0, batch    90 | loss: 3.2606936MixupTrain:  epoch  0, batch    91 | loss: 3.3269315MixupTrain:  epoch  0, batch    92 | loss: 2.8149357MixupTrain:  epoch  0, batch    93 | loss: 3.0453117MixupTrain:  epoch  0, batch    94 | loss: 2.9744673MixupTrain:  epoch  0, batch    95 | loss: 3.2300353MixupTrain:  epoch  0, batch    96 | loss: 3.0647428MixupTrain:  epoch  0, batch    97 | loss: 3.0363250MixupTrain:  epoch  0, batch    98 | loss: 3.1883485MixupTrain:  epoch  0, batch    99 | loss: 2.9713049MixupTrain:  epoch  0, batch   100 | loss: 3.2109318MixupTrain:  epoch  0, batch   101 | loss: 3.3031523MixupTrain:  epoch  0, batch   102 | loss: 3.0889034MixupTrain:  epoch  0, batch   103 | loss: 3.1922386MixupTrain:  epoch  0, batch   104 | loss: 3.2777042MixupTrain:  epoch  0, batch   105 | loss: 3.0597110MixupTrain:  epoch  0, batch   106 | loss: 2.9484725MixupTrain:  epoch  0, batch   107 | loss: 3.3641615MixupTrain:  epoch  0, batch   108 | loss: 3.3202643MixupTrain:  epoch  0, batch   109 | loss: 3.2162261MixupTrain:  epoch  0, batch   110 | loss: 2.8496001MixupTrain:  epoch  0, batch   111 | loss: 3.0864997MixupTrain:  epoch  0, batch   112 | loss: 3.2425990MixupTrain:  epoch  0, batch   113 | loss: 3.2084105MixupTrain:  epoch  0, batch   114 | loss: 3.1986930MixupTrain:  epoch  0, batch   115 | loss: 3.0640888MixupTrain:  epoch  0, batch   116 | loss: 3.0690670MixupTrain:  epoch  0, batch   117 | loss: 3.3104966MixupTrain:  epoch  0, batch   118 | loss: 3.0812702MixupTrain:  epoch  0, batch   119 | loss: 3.0336199MixupTrain:  epoch  0, batch   120 | loss: 3.1461098MixupTrain:  epoch  0, batch   121 | loss: 3.0534935MixupTrain:  epoch  0, batch   122 | loss: 3.2280746MixupTrain:  epoch  0, batch   123 | loss: 3.2280416MixupTrain:  epoch  0, batch   124 | loss: 2.8802934MixupTrain:  epoch  0, batch   125 | loss: 3.0846362MixupTrain:  epoch  0, batch   126 | loss: 3.2538905MixupTrain:  epoch  0, batch   127 | loss: 3.0264792MixupTrain:  epoch  0, batch   128 | loss: 2.9669185MixupTrain:  epoch  0, batch   129 | loss: 3.1641974MixupTrain:  epoch  0, batch   130 | loss: 2.9941156MixupTrain:  epoch  0, batch   131 | loss: 2.9000385MixupTrain:  epoch  0, batch   132 | loss: 3.0597322MixupTrain:  epoch  0, batch   133 | loss: 3.0347271MixupTrain:  epoch  0, batch   134 | loss: 3.1513288MixupTrain:  epoch  0, batch   135 | loss: 2.9034100MixupTrain:  epoch  0, batch   136 | loss: 3.0246258MixupTrain:  epoch  0, batch   137 | loss: 3.3279061MixupTrain:  epoch  0, batch   138 | loss: 2.9579878MixupTrain:  epoch  0, batch   139 | loss: 2.9569278MixupTrain:  epoch  0, batch   140 | loss: 2.9938793MixupTrain:  epoch  0, batch   141 | loss: 3.0380125MixupTrain:  epoch  0, batch   142 | loss: 2.9654760MixupTrain:  epoch  0, batch   143 | loss: 3.0057290MixupTrain:  epoch  0, batch   144 | loss: 3.0797663MixupTrain:  epoch  0, batch   145 | loss: 3.0189784MixupTrain:  epoch  0, batch   146 | loss: 2.9090273MixupTrain:  epoch  0, batch   147 | loss: 2.8446121MixupTrain:  epoch  0, batch   148 | loss: 3.0839319MixupTrain:  epoch  0, batch   149 | loss: 2.8732748MixupTrain:  epoch  0, batch   150 | loss: 3.4090185MixupTrain:  epoch  0, batch   151 | loss: 2.8546920MixupTrain:  epoch  0, batch   152 | loss: 2.9605389MixupTrain:  epoch  0, batch   153 | loss: 3.1546700MixupTrain:  epoch  0, batch   154 | loss: 2.9990025MixupTrain:  epoch  0, batch   155 | loss: 3.1991909MixupTrain:  epoch  0, batch   156 | loss: 3.0503912MixupTrain:  epoch  0, batch   157 | loss: 3.0745084MixupTrain:  epoch  0, batch   158 | loss: 2.9559474MixupTrain:  epoch  0, batch   159 | loss: 3.0635543MixupTrain:  epoch  0, batch   160 | loss: 2.9736443MixupTrain:  epoch  0, batch   161 | loss: 3.1926885MixupTrain:  epoch  0, batch   162 | loss: 3.0886986MixupTrain:  epoch  0, batch   163 | loss: 2.9626372MixupTrain:  epoch  0, batch   164 | loss: 2.9230893MixupTrain:  epoch  0, batch   165 | loss: 2.9515142MixupTrain:  epoch  0, batch   166 | loss: 3.0221534MixupTrain:  epoch  0, batch   167 | loss: 3.0956206MixupTrain:  epoch  0, batch   168 | loss: 2.9454074MixupTrain:  epoch  0, batch   169 | loss: 3.0557215MixupTrain:  epoch  0, batch   170 | loss: 2.9125733MixupTrain:  epoch  0, batch   171 | loss: 3.0670404MixupTrain:  epoch  0, batch   172 | loss: 2.9839814MixupTrain:  epoch  0, batch   173 | loss: 3.0582533MixupTrain:  epoch  0, batch   174 | loss: 3.1826682MixupTrain:  epoch  0, batch   175 | loss: 3.0534983MixupTrain:  epoch  0, batch   176 | loss: 2.9421225MixupTrain:  epoch  0, batch   177 | loss: 3.0643792MixupTrain:  epoch  0, batch   178 | loss: 2.8559120MixupTrain:  epoch  0, batch   179 | loss: 2.8296242MixupTrain:  epoch  0, batch   180 | loss: 3.0298457MixupTrain:  epoch  0, batch   181 | loss: 2.9286842MixupTrain:  epoch  0, batch   182 | loss: 3.2078104MixupTrain:  epoch  0, batch   183 | loss: 2.9498775MixupTrain:  epoch  0, batch   184 | loss: 3.1627281MixupTrain:  epoch  0, batch   185 | loss: 2.9373055MixupTrain:  epoch  0, batch   186 | loss: 3.0812783MixupTrain:  epoch  0, batch   187 | loss: 3.1745048MixupTrain:  epoch  0, batch   188 | loss: 2.9753678MixupTrain:  epoch  0, batch   189 | loss: 3.1476643MixupTrain:  epoch  0, batch   190 | loss: 3.0038409MixupTrain:  epoch  0, batch   191 | loss: 3.1570551MixupTrain:  epoch  0, batch   192 | loss: 2.9970479MixupTrain:  epoch  0, batch   193 | loss: 2.9751277MixupTrain:  epoch  0, batch   194 | loss: 2.8989954MixupTrain:  epoch  0, batch   195 | loss: 2.9375966MixupTrain:  epoch  0, batch   196 | loss: 3.0443001MixupTrain:  epoch  0, batch   197 | loss: 2.8922253MixupTrain:  epoch  0, batch   198 | loss: 2.9872298MixupTrain:  epoch  0, batch   199 | loss: 3.0582123MixupTrain:  epoch  0, batch   200 | loss: 3.1436658MixupTrain:  epoch  0, batch   201 | loss: 3.0646949MixupTrain:  epoch  0, batch   202 | loss: 2.9226451MixupTrain:  epoch  0, batch   203 | loss: 2.7788570MixupTrain:  epoch  0, batch   204 | loss: 2.8783560MixupTrain:  epoch  0, batch   205 | loss: 2.9189601MixupTrain:  epoch  0, batch   206 | loss: 2.9569073MixupTrain:  epoch  0, batch   207 | loss: 2.9603395MixupTrain:  epoch  0, batch   208 | loss: 3.0709133MixupTrain:  epoch  0, batch   209 | loss: 2.8508568MixupTrain:  epoch  0, batch   210 | loss: 2.9107714MixupTrain:  epoch  0, batch   211 | loss: 2.9379621MixupTrain:  epoch  0, batch   212 | loss: 3.0390511MixupTrain:  epoch  0, batch   213 | loss: 3.1148648MixupTrain:  epoch  0, batch   214 | loss: 3.0131485MixupTrain:  epoch  0, batch   215 | loss: 3.0195060MixupTrain:  epoch  0, batch   216 | loss: 3.1093192MixupTrain:  epoch  0, batch   217 | loss: 2.9230955MixupTrain:  epoch  0, batch   218 | loss: 2.9763372MixupTrain:  epoch  0, batch   219 | loss: 3.1016252MixupTrain:  epoch  0, batch   220 | loss: 3.1200721MixupTrain:  epoch  0, batch   221 | loss: 3.0181751MixupTrain:  epoch  0, batch   222 | loss: 2.9292853MixupTrain:  epoch  0, batch   223 | loss: 3.1307015MixupTrain:  epoch  0, batch   224 | loss: 3.0329649MixupTrain:  epoch  0, batch   225 | loss: 3.2943566MixupTrain:  epoch  0, batch   226 | loss: 2.9675283MixupTrain:  epoch  0, batch   227 | loss: 3.0426309MixupTrain:  epoch  0, batch   228 | loss: 3.0736573MixupTrain:  epoch  0, batch   229 | loss: 3.1594763MixupTrain:  epoch  0, batch   230 | loss: 3.0288010MixupTrain:  epoch  0, batch   231 | loss: 3.2254019MixupTrain:  epoch  0, batch   232 | loss: 3.0603426MixupTrain:  epoch  0, batch   233 | loss: 3.1052599MixupTrain:  epoch  0, batch   234 | loss: 2.9366984MixupTrain:  epoch  0, batch   235 | loss: 3.0117736MixupTrain:  epoch  0, batch   236 | loss: 2.9839439MixupTrain:  epoch  0, batch   237 | loss: 2.9285302MixupTrain:  epoch  0, batch   238 | loss: 2.8890123MixupTrain:  epoch  0, batch   239 | loss: 3.0286026MixupTrain:  epoch  0, batch   240 | loss: 3.1639051MixupTrain:  epoch  0, batch   241 | loss: 2.9780941MixupTrain:  epoch  0, batch   242 | loss: 3.0005040MixupTrain:  epoch  0, batch   243 | loss: 3.0714109MixupTrain:  epoch  0, batch   244 | loss: 3.0748501MixupTrain:  epoch  0, batch   245 | loss: 3.0854940MixupTrain:  epoch  0, batch   246 | loss: 3.0901678MixupTrain:  epoch  0, batch   247 | loss: 3.0883532MixupTrain:  epoch  0, batch   248 | loss: 2.8597646MixupTrain:  epoch  0, batch   249 | loss: 2.8692558MixupTrain:  epoch  0, batch   250 | loss: 2.9264176MixupTrain:  epoch  0, batch   251 | loss: 2.9693274MixupTrain:  epoch  0, batch   252 | loss: 2.8622162MixupTrain:  epoch  0, batch   253 | loss: 3.0650849MixupTrain:  epoch  0, batch   254 | loss: 3.1729403MixupTrain:  epoch  0, batch   255 | loss: 2.9327190MixupTrain:  epoch  0, batch   256 | loss: 2.9741132MixupTrain:  epoch  0, batch   257 | loss: 2.9803600MixupTrain:  epoch  0, batch   258 | loss: 3.1272078MixupTrain:  epoch  0, batch   259 | loss: 2.9877381MixupTrain:  epoch  0, batch   260 | loss: 2.9833536MixupTrain:  epoch  0, batch   261 | loss: 2.8496423MixupTrain:  epoch  0, batch   262 | loss: 3.0250733MixupTrain:  epoch  0, batch   263 | loss: 2.9130616MixupTrain:  epoch  0, batch   264 | loss: 3.2131774MixupTrain:  epoch  0, batch   265 | loss: 2.8109865MixupTrain:  epoch  0, batch   266 | loss: 2.9675894MixupTrain:  epoch  0, batch   267 | loss: 3.0019631MixupTrain:  epoch  0, batch   268 | loss: 3.0299966MixupTrain:  epoch  0, batch   269 | loss: 3.1443241MixupTrain:  epoch  0, batch   270 | loss: 2.8582768MixupTrain:  epoch  0, batch   271 | loss: 2.8794570MixupTrain:  epoch  0, batch   272 | loss: 2.9489436MixupTrain:  epoch  0, batch   273 | loss: 2.7535334MixupTrain:  epoch  0, batch   274 | loss: 2.8459225MixupTrain:  epoch  0, batch   275 | loss: 2.9157541MixupTrain:  epoch  0, batch   276 | loss: 2.7498574MixupTrain:  epoch  0, batch   277 | loss: 2.8425655MixupTrain:  epoch  0, batch   278 | loss: 3.0546679MixupTrain:  epoch  0, batch   279 | loss: 3.0918887MixupTrain:  epoch  0, batch   280 | loss: 2.8410122MixupTrain:  epoch  0, batch   281 | loss: 3.0303960MixupTrain:  epoch  0, batch   282 | loss: 3.2252438MixupTrain:  epoch  0, batch   283 | loss: 3.1351724MixupTrain:  epoch  0, batch   284 | loss: 2.9578614MixupTrain:  epoch  0, batch   285 | loss: 3.0632770MixupTrain:  epoch  0, batch   286 | loss: 3.0103459MixupTrain:  epoch  0, batch   287 | loss: 3.1386526MixupTrain:  epoch  0, batch   288 | loss: 2.9838929MixupTrain:  epoch  0, batch   289 | loss: 3.0031128MixupTrain:  epoch  0, batch   290 | loss: 2.9308505MixupTrain:  epoch  0, batch   291 | loss: 3.1743798MixupTrain:  epoch  0, batch   292 | loss: 3.0604343MixupTrain:  epoch  0, batch   293 | loss: 3.0105224MixupTrain:  epoch  0, batch   294 | loss: 2.9569407MixupTrain:  epoch  0, batch   295 | loss: 2.8975534MixupTrain:  epoch  0, batch   296 | loss: 3.0085294MixupTrain:  epoch  0, batch   297 | loss: 2.8402596MixupTrain:  epoch  0, batch   298 | loss: 3.0181036MixupTrain:  epoch  0, batch   299 | loss: 2.9357519MixupTrain:  epoch  0, batch   300 | loss: 2.8168833MixupTrain:  epoch  0, batch   301 | loss: 2.7996635MixupTrain:  epoch  0, batch   302 | loss: 3.0852270MixupTrain:  epoch  0, batch   303 | loss: 3.0974689MixupTrain:  epoch  0, batch   304 | loss: 3.0652938MixupTrain:  epoch  0, batch   305 | loss: 3.1257548MixupTrain:  epoch  0, batch   306 | loss: 2.9013491MixupTrain:  epoch  0, batch   307 | loss: 2.8034043MixupTrain:  epoch  0, batch   308 | loss: 2.9687171MixupTrain:  epoch  0, batch   309 | loss: 3.0684614MixupTrain:  epoch  0, batch   310 | loss: 2.8439741MixupTrain:  epoch  0, batch   311 | loss: 2.9483371MixupTrain:  epoch  0, batch   312 | loss: 3.0099730MixupTrain:  epoch  0, batch   313 | loss: 2.9777679MixupTrain:  epoch  0, batch   314 | loss: 3.1414473MixupTrain:  epoch  0, batch   315 | loss: 2.9307580MixupTrain:  epoch  0, batch   316 | loss: 2.8324049MixupTrain:  epoch  0, batch   317 | loss: 3.0299609MixupTrain:  epoch  0, batch   318 | loss: 3.0995293MixupTrain:  epoch  0, batch   319 | loss: 2.9152877MixupTrain:  epoch  0, batch   320 | loss: 2.7842727MixupTrain:  epoch  0, batch   321 | loss: 3.1014738MixupTrain:  epoch  0, batch   322 | loss: 2.7913067MixupTrain:  epoch  0, batch   323 | loss: 2.8166769MixupTrain:  epoch  0, batch   324 | loss: 2.9203756MixupTrain:  epoch  0, batch   325 | loss: 2.8106549MixupTrain:  epoch  0, batch   326 | loss: 2.8429716MixupTrain:  epoch  0, batch   327 | loss: 3.0000861MixupTrain:  epoch  0, batch   328 | loss: 2.9832320MixupTrain:  epoch  0, batch   329 | loss: 2.8427343MixupTrain:  epoch  0, batch   330 | loss: 3.0345650MixupTrain:  epoch  0, batch   331 | loss: 3.0217795MixupTrain:  epoch  0, batch   332 | loss: 3.0391893MixupTrain:  epoch  0, batch   333 | loss: 2.8230004MixupTrain:  epoch  0, batch   334 | loss: 2.8811524MixupTrain:  epoch  0, batch   335 | loss: 2.8353982MixupTrain:  epoch  0, batch   336 | loss: 2.9767137MixupTrain:  epoch  0, batch   337 | loss: 2.9573283MixupTrain:  epoch  0, batch   338 | loss: 2.9016490MixupTrain:  epoch  0, batch   339 | loss: 2.7844033MixupTrain:  epoch  0, batch   340 | loss: 2.9409430MixupTrain:  epoch  0, batch   341 | loss: 2.9226646MixupTrain:  epoch  0, batch   342 | loss: 2.9582338MixupTrain:  epoch  0, batch   343 | loss: 2.9524536MixupTrain:  epoch  0, batch   344 | loss: 3.0845845MixupTrain:  epoch  0, batch   345 | loss: 3.1026177MixupTrain:  epoch  0, batch   346 | loss: 2.8871398MixupTrain:  epoch  0, batch   347 | loss: 2.9221611MixupTrain:  epoch  0, batch   348 | loss: 2.8909817MixupTrain:  epoch  0, batch   349 | loss: 3.1488128MixupTrain:  epoch  0, batch   350 | loss: 3.0575624MixupTrain:  epoch  0, batch   351 | loss: 2.8714385MixupTrain:  epoch  0, batch   352 | loss: 3.0402026MixupTrain:  epoch  0, batch   353 | loss: 3.0634522MixupTrain:  epoch  0, batch   354 | loss: 2.9535427MixupTrain:  epoch  0, batch   355 | loss: 3.0076747MixupTrain:  epoch  0, batch   356 | loss: 2.9267702MixupTrain:  epoch  0, batch   357 | loss: 2.9362807MixupTrain:  epoch  0, batch   358 | loss: 3.1539788MixupTrain:  epoch  0, batch   359 | loss: 2.8612475MixupTrain:  epoch  0, batch   360 | loss: 2.9433296MixupTrain:  epoch  0, batch   361 | loss: 2.9683685MixupTrain:  epoch  0, batch   362 | loss: 2.9710875MixupTrain:  epoch  0, batch   363 | loss: 3.1778312MixupTrain:  epoch  0, batch   364 | loss: 3.0157518MixupTrain:  epoch  0, batch   365 | loss: 2.8905411MixupTrain:  epoch  0, batch   366 | loss: 3.0179381MixupTrain:  epoch  0, batch   367 | loss: 2.9149358MixupTrain:  epoch  0, batch   368 | loss: 3.0380254MixupTrain:  epoch  0, batch   369 | loss: 3.0211782MixupTrain:  epoch  0, batch   370 | loss: 2.8185298MixupTrain:  epoch  0, batch   371 | loss: 2.8896968MixupTrain:  epoch  0, batch   372 | loss: 2.8231091MixupTrain:  epoch  0, batch   373 | loss: 3.0004206MixupTrain:  epoch  0, batch   374 | loss: 2.9585991MixupTrain:  epoch  0, batch   375 | loss: 3.0160098MixupTrain:  epoch  0, batch   376 | loss: 3.0023322MixupTrain:  epoch  0, batch   377 | loss: 2.8215218MixupTrain:  epoch  0, batch   378 | loss: 2.9669449MixupTrain:  epoch  0, batch   379 | loss: 3.0083983MixupTrain:  epoch  0, batch   380 | loss: 2.9584262MixupTrain:  epoch  0, batch   381 | loss: 3.0046012MixupTrain:  epoch  0, batch   382 | loss: 2.9392109MixupTrain:  epoch  0, batch   383 | loss: 2.9105968MixupTrain:  epoch  0, batch   384 | loss: 2.9846306MixupTrain:  epoch  0, batch   385 | loss: 2.9702764MixupTrain:  epoch  0, batch   386 | loss: 3.0469790MixupTrain:  epoch  0, batch   387 | loss: 2.7805858MixupTrain:  epoch  0, batch   388 | loss: 3.0172825MixupTrain:  epoch  0, batch   389 | loss: 2.9869151MixupTrain:  epoch  0, batch   390 | loss: 2.9096518MixupTrain:  epoch  0, batch   391 | loss: 2.9841700MixupTrain:  epoch  0, batch   392 | loss: 2.9780719MixupTrain:  epoch  0, batch   393 | loss: 3.1185169MixupTrain:  epoch  0, batch   394 | loss: 2.8846540MixupTrain:  epoch  0, batch   395 | loss: 2.9079173MixupTrain:  epoch  0, batch   396 | loss: 3.0321422MixupTrain:  epoch  0, batch   397 | loss: 2.8774104MixupTrain:  epoch  0, batch   398 | loss: 2.8576000MixupTrain:  epoch  0, batch   399 | loss: 3.0084701MixupTrain:  epoch  0, batch   400 | loss: 3.0512927MixupTrain:  epoch  0, batch   401 | loss: 2.8613362MixupTrain:  epoch  0, batch   402 | loss: 3.0400417MixupTrain:  epoch  0, batch   403 | loss: 3.1347351MixupTrain:  epoch  0, batch   404 | loss: 3.0978632MixupTrain:  epoch  0, batch   405 | loss: 2.8527818MixupTrain:  epoch  0, batch   406 | loss: 2.9159555MixupTrain:  epoch  0, batch   407 | loss: 2.9400120MixupTrain:  epoch  0, batch   408 | loss: 2.9098139MixupTrain:  epoch  0, batch   409 | loss: 2.9632883MixupTrain:  epoch  0, batch   410 | loss: 3.0247011MixupTrain:  epoch  0, batch   411 | loss: 2.8773336MixupTrain:  epoch  0, batch   412 | loss: 3.0098407MixupTrain:  epoch  0, batch   413 | loss: 2.9987357MixupTrain:  epoch  0, batch   414 | loss: 3.1575723MixupTrain:  epoch  0, batch   415 | loss: 2.9129882MixupTrain:  epoch  0, batch   416 | loss: 3.0658684MixupTrain:  epoch  0, batch   417 | loss: 3.1005628MixupTrain:  epoch  0, batch   418 | loss: 2.9678030MixupTrain:  epoch  0, batch   419 | loss: 3.0001338MixupTrain:  epoch  0, batch   420 | loss: 3.1018291MixupTrain:  epoch  0, batch   421 | loss: 2.8745375MixupTrain:  epoch  0, batch   422 | loss: 2.8042061MixupTrain:  epoch  0, batch   423 | loss: 3.0123272MixupTrain:  epoch  0, batch   424 | loss: 2.8835359MixupTrain:  epoch  0, batch   425 | loss: 2.8368847MixupTrain:  epoch  0, batch   426 | loss: 2.8913054MixupTrain:  epoch  0, batch   427 | loss: 2.9405665MixupTrain:  epoch  0, batch   428 | loss: 3.0123992MixupTrain:  epoch  0, batch   429 | loss: 3.0391111MixupTrain:  epoch  0, batch   430 | loss: 2.8714774MixupTrain:  epoch  0, batch   431 | loss: 2.9024868MixupTrain:  epoch  0, batch   432 | loss: 2.9714322MixupTrain:  epoch  0, batch   433 | loss: 3.1475148MixupTrain:  epoch  0, batch   434 | loss: 2.8495536MixupTrain:  epoch  0, batch   435 | loss: 2.8357506MixupTrain:  epoch  0, batch   436 | loss: 2.9212368MixupTrain:  epoch  0, batch   437 | loss: 3.0441704MixupTrain:  epoch  0, batch   438 | loss: 2.9894645MixupTrain:  epoch  0, batch   439 | loss: 2.7998309MixupTrain:  epoch  0, batch   440 | loss: 3.0219245MixupTrain:  epoch  0, batch   441 | loss: 2.9821792MixupTrain:  epoch  0, batch   442 | loss: 3.0947752MixupTrain:  epoch  0, batch   443 | loss: 3.0739279MixupTrain:  epoch  0, batch   444 | loss: 2.9409690MixupTrain:  epoch  0, batch   445 | loss: 2.8622508MixupTrain:  epoch  0, batch   446 | loss: 2.8886747MixupTrain:  epoch  0, batch   447 | loss: 3.0230436MixupTrain:  epoch  0, batch   448 | loss: 3.0021348MixupTrain:  epoch  0, batch   449 | loss: 2.9537115MixupTrain:  epoch  0, batch   450 | loss: 2.8533244MixupTrain:  epoch  0, batch   451 | loss: 2.9742889MixupTrain:  epoch  0, batch   452 | loss: 3.0145805MixupTrain:  epoch  0, batch   453 | loss: 3.0213699MixupTrain:  epoch  0, batch   454 | loss: 2.9074302MixupTrain:  epoch  0, batch   455 | loss: 2.8621306MixupTrain:  epoch  0, batch   456 | loss: 3.0472836MixupTrain:  epoch  0, batch   457 | loss: 2.9261441MixupTrain:  epoch  0, batch   458 | loss: 3.0617828MixupTrain:  epoch  0, batch   459 | loss: 2.9056973MixupTrain:  epoch  0, batch   460 | loss: 3.0170555MixupTrain:  epoch  0, batch   461 | loss: 3.0678060MixupTrain:  epoch  0, batch   462 | loss: 3.0307388MixupTrain:  epoch  0, batch   463 | loss: 3.1082449MixupTrain:  epoch  0, batch   464 | loss: 2.9617548MixupTrain:  epoch  0, batch   465 | loss: 2.9728646MixupTrain:  epoch  0, batch   466 | loss: 2.8308601MixupTrain:  epoch  0, batch   467 | loss: 2.8629737MixupTrain:  epoch  0, batch   468 | loss: 2.9186203MixupTrain:  epoch  0, batch   469 | loss: 2.8968740MixupTrain:  epoch  0, batch   470 | loss: 2.9614525MixupTrain:  epoch  0, batch   471 | loss: 2.9097705MixupTrain:  epoch  0, batch   472 | loss: 2.8766491MixupTrain:  epoch  0, batch   473 | loss: 3.0573044MixupTrain:  epoch  0, batch   474 | loss: 3.1468272MixupTrain:  epoch  0, batch   475 | loss: 2.9510629MixupTrain:  epoch  0, batch   476 | loss: 3.0337048MixupTrain:  epoch  0, batch   477 | loss: 3.0396786MixupTrain:  epoch  0, batch   478 | loss: 3.0730088MixupTrain:  epoch  0, batch   479 | loss: 3.0698559MixupTrain:  epoch  0, batch   480 | loss: 3.0862801MixupTrain:  epoch  0, batch   481 | loss: 2.9955678MixupTrain:  epoch  0, batch   482 | loss: 3.0999064MixupTrain:  epoch  0, batch   483 | loss: 2.7488024MixupTrain:  epoch  0, batch   484 | loss: 2.9610448MixupTrain:  epoch  0, batch   485 | loss: 2.8838973MixupTrain:  epoch  0, batch   486 | loss: 2.9012959MixupTrain:  epoch  0, batch   487 | loss: 2.9158003MixupTrain:  epoch  0, batch   488 | loss: 2.9498391MixupTrain:  epoch  0, batch   489 | loss: 2.9301658MixupTrain:  epoch  0, batch   490 | loss: 2.7927248MixupTrain:  epoch  0, batch   491 | loss: 3.0055420MixupTrain:  epoch  0, batch   492 | loss: 3.0146253MixupTrain:  epoch  0, batch   493 | loss: 2.8963695MixupTrain:  epoch  0, batch   494 | loss: 3.0435748MixupTrain:  epoch  0, batch   495 | loss: 2.9452116MixupTrain:  epoch  0, batch   496 | loss: 2.9348397MixupTrain:  epoch  0, batch   497 | loss: 2.8449285MixupTrain:  epoch  0, batch   498 | loss: 2.9730773MixupTrain:  epoch  0, batch   499 | loss: 3.0220618MixupTrain:  epoch  0, batch   500 | loss: 2.9343281MixupTrain:  epoch  0, batch   501 | loss: 2.9715483MixupTrain:  epoch  0, batch   502 | loss: 2.9372709MixupTrain:  epoch  0, batch   503 | loss: 2.9374642MixupTrain:  epoch  0, batch   504 | loss: 2.9442365MixupTrain:  epoch  0, batch   505 | loss: 2.9507194MixupTrain:  epoch  0, batch   506 | loss: 2.8355448MixupTrain:  epoch  0, batch   507 | loss: 3.0759888MixupTrain:  epoch  0, batch   508 | loss: 2.8266475MixupTrain:  epoch  0, batch   509 | loss: 2.8496466MixupTrain:  epoch  0, batch   510 | loss: 3.0992041MixupTrain:  epoch  0, batch   511 | loss: 2.9390216MixupTrain:  epoch  0, batch   512 | loss: 2.9768510MixupTrain:  epoch  0, batch   513 | loss: 2.9061642MixupTrain:  epoch  0, batch   514 | loss: 2.9124827MixupTrain:  epoch  0, batch   515 | loss: 3.0251787MixupTrain:  epoch  0, batch   516 | loss: 2.8563282MixupTrain:  epoch  0, batch   517 | loss: 2.8559382MixupTrain:  epoch  0, batch   518 | loss: 2.9045618MixupTrain:  epoch  0, batch   519 | loss: 2.8584754MixupTrain:  epoch  0, batch   520 | loss: 2.8787389MixupTrain:  epoch  0, batch   521 | loss: 3.0977774MixupTrain:  epoch  0, batch   522 | loss: 3.0366862MixupTrain:  epoch  0, batch   523 | loss: 3.0858006MixupTrain:  epoch  0, batch   524 | loss: 2.9091630MixupTrain:  epoch  0, batch   525 | loss: 2.9128337MixupTrain:  epoch  0, batch   526 | loss: 3.0738931MixupTrain:  epoch  0, batch   527 | loss: 2.9227896MixupTrain:  epoch  0, batch   528 | loss: 3.0456290MixupTrain:  epoch  0, batch   529 | loss: 3.1120486MixupTrain:  epoch  0, batch   530 | loss: 3.0021443MixupTrain:  epoch  0, batch   531 | loss: 3.1295228MixupTrain:  epoch  0, batch   532 | loss: 2.9983635MixupTrain:  epoch  0, batch   533 | loss: 2.8293324MixupTrain:  epoch  0, batch   534 | loss: 2.9136746MixupTrain:  epoch  0, batch   535 | loss: 2.9831793MixupTrain:  epoch  0, batch   536 | loss: 2.9804039MixupTrain:  epoch  0, batch   537 | loss: 3.1001132MixupTrain:  epoch  0, batch   538 | loss: 2.8720722MixupTrain:  epoch  0, batch   539 | loss: 2.9479520MixupTrain:  epoch  0, batch   540 | loss: 2.9557948MixupTrain:  epoch  0, batch   541 | loss: 2.9874072MixupTrain:  epoch  0, batch   542 | loss: 3.0640652MixupTrain:  epoch  0, batch   543 | loss: 2.9507723MixupTrain:  epoch  0, batch   544 | loss: 2.7981954MixupTrain:  epoch  0, batch   545 | loss: 2.9889302MixupTrain:  epoch  0, batch   546 | loss: 3.0261416MixupTrain:  epoch  0, batch   547 | loss: 2.8229866MixupTrain:  epoch  0, batch   548 | loss: 2.9930530MixupTrain:  epoch  0, batch   549 | loss: 2.9002357MixupTrain:  epoch  0, batch   550 | loss: 2.8416078MixupTrain:  epoch  0, batch   551 | loss: 3.0649791MixupTrain:  epoch  0, batch   552 | loss: 2.9511862MixupTrain:  epoch  0, batch   553 | loss: 3.0286019MixupTrain:  epoch  0, batch   554 | loss: 2.8394372MixupTrain:  epoch  0, batch   555 | loss: 2.8636873MixupTrain:  epoch  0, batch   556 | loss: 3.0362172MixupTrain:  epoch  0, batch   557 | loss: 2.9509888MixupTrain:  epoch  0, batch   558 | loss: 3.0449014MixupTrain:  epoch  0, batch   559 | loss: 3.0449715MixupTrain:  epoch  0, batch   560 | loss: 2.8369136MixupTrain:  epoch  0, batch   561 | loss: 2.9747703MixupTrain:  epoch  0, batch   562 | loss: 2.9087536MixupTrain:  epoch  0, batch   563 | loss: 2.9915318MixupTrain:  epoch  0, batch   564 | loss: 2.9586558MixupTrain:  epoch  0, batch   565 | loss: 3.1153283MixupTrain:  epoch  0, batch   566 | loss: 2.9043286MixupTrain:  epoch  0, batch   567 | loss: 2.9294920MixupTrain:  epoch  0, batch   568 | loss: 2.8393476MixupTrain:  epoch  0, batch   569 | loss: 2.9052501MixupTrain:  epoch  0, batch   570 | loss: 3.0086503MixupTrain:  epoch  0, batch   571 | loss: 2.8777087MixupTrain:  epoch  0, batch   572 | loss: 2.9388800MixupTrain:  epoch  0, batch   573 | loss: 2.9846807MixupTrain:  epoch  0, batch   574 | loss: 2.8719912MixupTrain:  epoch  0, batch   575 | loss: 2.7777746MixupTrain:  epoch  0, batch   576 | loss: 3.0064838MixupTrain:  epoch  0, batch   577 | loss: 3.0128992MixupTrain:  epoch  0, batch   578 | loss: 2.7866983MixupTrain:  epoch  0, batch   579 | loss: 3.0185311MixupTrain:  epoch  0, batch   580 | loss: 2.9311163MixupTrain:  epoch  0, batch   581 | loss: 3.1914687MixupTrain:  epoch  0, batch   582 | loss: 2.8819389MixupTrain:  epoch  0, batch   583 | loss: 3.0394175MixupTrain:  epoch  0, batch   584 | loss: 2.9533243MixupTrain:  epoch  0, batch   585 | loss: 2.8594894MixupTrain:  epoch  0, batch   586 | loss: 2.9451261MixupTrain:  epoch  0, batch   587 | loss: 2.9771762MixupTrain:  epoch  0, batch   588 | loss: 3.0507169MixupTrain:  epoch  0, batch   589 | loss: 3.0560832MixupTrain:  epoch  0, batch   590 | loss: 2.9143033MixupTrain:  epoch  0, batch   591 | loss: 3.1126471MixupTrain:  epoch  0, batch   592 | loss: 2.8524208MixupTrain:  epoch  0, batch   593 | loss: 3.0993366MixupTrain:  epoch  0, batch   594 | loss: 2.9705074MixupTrain:  epoch  0, batch   595 | loss: 2.8680358MixupTrain:  epoch  0, batch   596 | loss: 3.0114269MixupTrain:  epoch  0, batch   597 | loss: 2.9245358MixupTrain:  epoch  0, batch   598 | loss: 2.7685056MixupTrain:  epoch  0, batch   599 | loss: 3.0907502MixupTrain:  epoch  0, batch   600 | loss: 3.0391498MixupTrain:  epoch  0, batch   601 | loss: 2.9677570MixupTrain:  epoch  0, batch   602 | loss: 2.8181858MixupTrain:  epoch  0, batch   603 | loss: 2.8797433MixupTrain:  epoch  0, batch   604 | loss: 2.9520547MixupTrain:  epoch  0, batch   605 | loss: 2.9976645MixupTrain:  epoch  0, batch   606 | loss: 3.0119300MixupTrain:  epoch  0, batch   607 | loss: 2.9131966MixupTrain:  epoch  0, batch   608 | loss: 3.0066528MixupTrain:  epoch  0, batch   609 | loss: 3.0139990MixupTrain:  epoch  0, batch   610 | loss: 2.8566284MixupTrain:  epoch  0, batch   611 | loss: 2.9666743MixupTrain:  epoch  0, batch   612 | loss: 2.9679999MixupTrain:  epoch  0, batch   613 | loss: 2.9713445MixupTrain:  epoch  0, batch   614 | loss: 2.9486597MixupTrain:  epoch  0, batch   615 | loss: 2.9323626MixupTrain:  epoch  0, batch   616 | loss: 3.0252433MixupTrain:  epoch  0, batch   617 | loss: 2.9720144MixupTrain:  epoch  0, batch   618 | loss: 2.8338227MixupTrain:  epoch  0, batch   619 | loss: 2.9271445MixupTrain:  epoch  0, batch   620 | loss: 3.0640640MixupTrain:  epoch  0, batch   621 | loss: 2.9371767MixupTrain:  epoch  0, batch   622 | loss: 2.9865997MixupTrain:  epoch  0, batch   623 | loss: 2.9422781MixupTrain:  epoch  0, batch   624 | loss: 3.0090783MixupTrain:  epoch  0, batch   625 | loss: 2.9577320MixupTrain:  epoch  0, batch   626 | loss: 2.8797760MixupTrain:  epoch  0, batch   627 | loss: 2.9555163MixupTrain:  epoch  0, batch   628 | loss: 2.8817835MixupTrain:  epoch  0, batch   629 | loss: 2.9772301MixupTrain:  epoch  0, batch   630 | loss: 2.9200425MixupTrain:  epoch  0, batch   631 | loss: 2.8669345MixupTrain:  epoch  0, batch   632 | loss: 2.9612269MixupTrain:  epoch  0, batch   633 | loss: 2.9373322MixupTrain:  epoch  0, batch   634 | loss: 2.9909174MixupTrain:  epoch  0, batch   635 | loss: 3.1407762MixupTrain:  epoch  0, batch   636 | loss: 2.9566731MixupTrain:  epoch  0, batch   637 | loss: 2.9709969MixupTrain:  epoch  0, batch   638 | loss: 2.9533985MixupTrain:  epoch  0, batch   639 | loss: 3.0353363MixupTrain:  epoch  0, batch   640 | loss: 2.8952303MixupTrain:  epoch  0, batch   641 | loss: 2.8270721MixupTrain:  epoch  0, batch   642 | loss: 2.8836503MixupTrain:  epoch  0, batch   643 | loss: 2.8455505MixupTrain:  epoch  0, batch   644 | loss: 2.9698880MixupTrain:  epoch  0, batch   645 | loss: 2.9144340MixupTrain:  epoch  0, batch   646 | loss: 2.9232163MixupTrain:  epoch  0, batch   647 | loss: 2.8559377MixupTrain:  epoch  0, batch   648 | loss: 2.8393958MixupTrain:  epoch  0, batch   649 | loss: 3.0820708MixupTrain:  epoch  0, batch   650 | loss: 2.8754470MixupTrain:  epoch  0, batch   651 | loss: 2.8252635MixupTrain:  epoch  0, batch   652 | loss: 3.0213881MixupTrain:  epoch  0, batch   653 | loss: 2.8533170MixupTrain:  epoch  0, batch   654 | loss: 2.9747744MixupTrain:  epoch  0, batch   655 | loss: 2.9050319MixupTrain:  epoch  0, batch   656 | loss: 2.9817781MixupTrain:  epoch  0, batch   657 | loss: 2.8964427MixupTrain:  epoch  0, batch   658 | loss: 2.7399182MixupTrain:  epoch  0, batch   659 | loss: 3.0172324MixupTrain:  epoch  0, batch   660 | loss: 2.9245768MixupTrain:  epoch  0, batch   661 | loss: 2.8934276MixupTrain:  epoch  0, batch   662 | loss: 2.8535838MixupTrain:  epoch  0, batch   663 | loss: 2.8418100MixupTrain:  epoch  0, batch   664 | loss: 2.8868499
MemoryTrain:  epoch  0, batch     0 | loss: 1.1463075MemoryTrain:  epoch  0, batch     1 | loss: 1.2099074MemoryTrain:  epoch  0, batch     2 | loss: 1.2747509MemoryTrain:  epoch  0, batch     3 | loss: 1.5026855MemoryTrain:  epoch  0, batch     4 | loss: 1.4470721MemoryTrain:  epoch  0, batch     5 | loss: 1.6022494MemoryTrain:  epoch  0, batch     6 | loss: 1.5988097MemoryTrain:  epoch  0, batch     7 | loss: 1.5834948MemoryTrain:  epoch  1, batch     0 | loss: 1.2368664MemoryTrain:  epoch  1, batch     1 | loss: 1.2202332MemoryTrain:  epoch  1, batch     2 | loss: 1.2571490MemoryTrain:  epoch  1, batch     3 | loss: 1.3784372MemoryTrain:  epoch  1, batch     4 | loss: 1.5733573MemoryTrain:  epoch  1, batch     5 | loss: 1.2193031MemoryTrain:  epoch  1, batch     6 | loss: 1.2666961MemoryTrain:  epoch  1, batch     7 | loss: 1.3114132MemoryTrain:  epoch  2, batch     0 | loss: 1.2960787MemoryTrain:  epoch  2, batch     1 | loss: 1.4335297MemoryTrain:  epoch  2, batch     2 | loss: 1.3702852MemoryTrain:  epoch  2, batch     3 | loss: 1.2700800MemoryTrain:  epoch  2, batch     4 | loss: 1.1775756MemoryTrain:  epoch  2, batch     5 | loss: 1.3069565MemoryTrain:  epoch  2, batch     6 | loss: 1.2672958MemoryTrain:  epoch  2, batch     7 | loss: 1.2027358MemoryTrain:  epoch  3, batch     0 | loss: 1.2125545MemoryTrain:  epoch  3, batch     1 | loss: 1.2056037MemoryTrain:  epoch  3, batch     2 | loss: 1.1958716MemoryTrain:  epoch  3, batch     3 | loss: 1.2580745MemoryTrain:  epoch  3, batch     4 | loss: 1.1856914MemoryTrain:  epoch  3, batch     5 | loss: 1.1710076MemoryTrain:  epoch  3, batch     6 | loss: 1.2047033MemoryTrain:  epoch  3, batch     7 | loss: 1.2671638MemoryTrain:  epoch  4, batch     0 | loss: 1.1936473MemoryTrain:  epoch  4, batch     1 | loss: 1.1731558MemoryTrain:  epoch  4, batch     2 | loss: 1.1977603MemoryTrain:  epoch  4, batch     3 | loss: 1.1775768MemoryTrain:  epoch  4, batch     4 | loss: 1.1949904MemoryTrain:  epoch  4, batch     5 | loss: 1.2223753MemoryTrain:  epoch  4, batch     6 | loss: 1.1913775MemoryTrain:  epoch  4, batch     7 | loss: 1.1571547MemoryTrain:  epoch  5, batch     0 | loss: 1.2132673MemoryTrain:  epoch  5, batch     1 | loss: 1.1792650MemoryTrain:  epoch  5, batch     2 | loss: 1.1863701MemoryTrain:  epoch  5, batch     3 | loss: 1.1614786MemoryTrain:  epoch  5, batch     4 | loss: 1.1725032MemoryTrain:  epoch  5, batch     5 | loss: 1.1818950MemoryTrain:  epoch  5, batch     6 | loss: 1.1654150MemoryTrain:  epoch  5, batch     7 | loss: 1.1481732MemoryTrain:  epoch  6, batch     0 | loss: 1.1453869MemoryTrain:  epoch  6, batch     1 | loss: 1.1721524MemoryTrain:  epoch  6, batch     2 | loss: 1.1822410MemoryTrain:  epoch  6, batch     3 | loss: 1.1564820MemoryTrain:  epoch  6, batch     4 | loss: 1.1950493MemoryTrain:  epoch  6, batch     5 | loss: 1.1595179MemoryTrain:  epoch  6, batch     6 | loss: 1.1877985MemoryTrain:  epoch  6, batch     7 | loss: 1.1743174MemoryTrain:  epoch  7, batch     0 | loss: 1.2193127MemoryTrain:  epoch  7, batch     1 | loss: 1.1914614MemoryTrain:  epoch  7, batch     2 | loss: 1.1657263MemoryTrain:  epoch  7, batch     3 | loss: 1.2093692MemoryTrain:  epoch  7, batch     4 | loss: 1.1654668MemoryTrain:  epoch  7, batch     5 | loss: 1.1833773MemoryTrain:  epoch  7, batch     6 | loss: 1.1674302MemoryTrain:  epoch  7, batch     7 | loss: 1.1395276MemoryTrain:  epoch  8, batch     0 | loss: 1.1704539MemoryTrain:  epoch  8, batch     1 | loss: 1.2015588MemoryTrain:  epoch  8, batch     2 | loss: 1.1673882MemoryTrain:  epoch  8, batch     3 | loss: 1.1385058MemoryTrain:  epoch  8, batch     4 | loss: 1.1844032MemoryTrain:  epoch  8, batch     5 | loss: 1.1479003MemoryTrain:  epoch  8, batch     6 | loss: 1.1844797MemoryTrain:  epoch  8, batch     7 | loss: 1.1740031MemoryTrain:  epoch  9, batch     0 | loss: 1.1685264MemoryTrain:  epoch  9, batch     1 | loss: 1.1473870MemoryTrain:  epoch  9, batch     2 | loss: 1.1730636MemoryTrain:  epoch  9, batch     3 | loss: 1.1569557MemoryTrain:  epoch  9, batch     4 | loss: 1.1484184MemoryTrain:  epoch  9, batch     5 | loss: 1.1873527MemoryTrain:  epoch  9, batch     6 | loss: 1.1750590MemoryTrain:  epoch  9, batch     7 | loss: 1.1909479
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 89.58%   [EVAL] batch:    3 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:    4 | acc: 100.00%,  total acc: 92.50%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 91.67%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 91.96%   [EVAL] batch:    7 | acc: 6.25%,  total acc: 81.25%   
[EVAL] batch:    0 | acc: 12.50%,  total acc: 12.50%   [EVAL] batch:    1 | acc: 31.25%,  total acc: 21.88%   [EVAL] batch:    2 | acc: 12.50%,  total acc: 18.75%   [EVAL] batch:    3 | acc: 6.25%,  total acc: 15.62%   [EVAL] batch:    4 | acc: 12.50%,  total acc: 15.00%   [EVAL] batch:    5 | acc: 12.50%,  total acc: 14.58%   [EVAL] batch:    6 | acc: 12.50%,  total acc: 14.29%   [EVAL] batch:    7 | acc: 12.50%,  total acc: 14.06%   [EVAL] batch:    8 | acc: 25.00%,  total acc: 15.28%   [EVAL] batch:    9 | acc: 25.00%,  total acc: 16.25%   [EVAL] batch:   10 | acc: 0.00%,  total acc: 14.77%   [EVAL] batch:   11 | acc: 6.25%,  total acc: 14.06%   [EVAL] batch:   12 | acc: 0.00%,  total acc: 12.98%   [EVAL] batch:   13 | acc: 25.00%,  total acc: 13.84%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 17.50%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 19.92%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 23.16%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 25.35%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 27.30%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 30.00%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 32.74%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 35.23%   [EVAL] batch:   22 | acc: 87.50%,  total acc: 37.50%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 39.84%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 42.25%   [EVAL] batch:   25 | acc: 87.50%,  total acc: 43.99%   [EVAL] batch:   26 | acc: 87.50%,  total acc: 45.60%   [EVAL] batch:   27 | acc: 87.50%,  total acc: 47.10%   [EVAL] batch:   28 | acc: 87.50%,  total acc: 48.49%   [EVAL] batch:   29 | acc: 68.75%,  total acc: 49.17%   [EVAL] batch:   30 | acc: 68.75%,  total acc: 49.80%   [EVAL] batch:   31 | acc: 87.50%,  total acc: 50.98%   [EVAL] batch:   32 | acc: 56.25%,  total acc: 51.14%   [EVAL] batch:   33 | acc: 56.25%,  total acc: 51.29%   [EVAL] batch:   34 | acc: 68.75%,  total acc: 51.79%   [EVAL] batch:   35 | acc: 50.00%,  total acc: 51.74%   [EVAL] batch:   36 | acc: 31.25%,  total acc: 51.18%   [EVAL] batch:   37 | acc: 31.25%,  total acc: 50.66%   [EVAL] batch:   38 | acc: 12.50%,  total acc: 49.68%   [EVAL] batch:   39 | acc: 6.25%,  total acc: 48.59%   [EVAL] batch:   40 | acc: 0.00%,  total acc: 47.41%   [EVAL] batch:   41 | acc: 0.00%,  total acc: 46.28%   [EVAL] batch:   42 | acc: 0.00%,  total acc: 45.20%   [EVAL] batch:   43 | acc: 0.00%,  total acc: 44.18%   [EVAL] batch:   44 | acc: 6.25%,  total acc: 43.33%   [EVAL] batch:   45 | acc: 0.00%,  total acc: 42.39%   [EVAL] batch:   46 | acc: 0.00%,  total acc: 41.49%   [EVAL] batch:   47 | acc: 6.25%,  total acc: 40.76%   [EVAL] batch:   48 | acc: 6.25%,  total acc: 40.05%   [EVAL] batch:   49 | acc: 6.25%,  total acc: 39.38%   [EVAL] batch:   50 | acc: 81.25%,  total acc: 40.20%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 41.23%   [EVAL] batch:   52 | acc: 75.00%,  total acc: 41.86%   [EVAL] batch:   53 | acc: 75.00%,  total acc: 42.48%   [EVAL] batch:   54 | acc: 56.25%,  total acc: 42.73%   [EVAL] batch:   55 | acc: 6.25%,  total acc: 42.08%   [EVAL] batch:   56 | acc: 0.00%,  total acc: 41.34%   [EVAL] batch:   57 | acc: 0.00%,  total acc: 40.62%   [EVAL] batch:   58 | acc: 0.00%,  total acc: 39.94%   [EVAL] batch:   59 | acc: 0.00%,  total acc: 39.27%   [EVAL] batch:   60 | acc: 0.00%,  total acc: 38.63%   [EVAL] batch:   61 | acc: 0.00%,  total acc: 38.00%   [EVAL] batch:   62 | acc: 12.50%,  total acc: 37.60%   [EVAL] batch:   63 | acc: 6.25%,  total acc: 37.11%   [EVAL] batch:   64 | acc: 6.25%,  total acc: 36.63%   [EVAL] batch:   65 | acc: 6.25%,  total acc: 36.17%   [EVAL] batch:   66 | acc: 31.25%,  total acc: 36.10%   [EVAL] batch:   67 | acc: 18.75%,  total acc: 35.85%   [EVAL] batch:   68 | acc: 31.25%,  total acc: 35.78%   [EVAL] batch:   69 | acc: 50.00%,  total acc: 35.98%   [EVAL] batch:   70 | acc: 31.25%,  total acc: 35.92%   [EVAL] batch:   71 | acc: 25.00%,  total acc: 35.76%   [EVAL] batch:   72 | acc: 81.25%,  total acc: 36.39%   [EVAL] batch:   73 | acc: 100.00%,  total acc: 37.25%   [EVAL] batch:   74 | acc: 100.00%,  total acc: 38.08%   [EVAL] batch:   75 | acc: 93.75%,  total acc: 38.82%   [EVAL] batch:   76 | acc: 75.00%,  total acc: 39.29%   [EVAL] batch:   77 | acc: 31.25%,  total acc: 39.18%   [EVAL] batch:   78 | acc: 18.75%,  total acc: 38.92%   [EVAL] batch:   79 | acc: 25.00%,  total acc: 38.75%   [EVAL] batch:   80 | acc: 18.75%,  total acc: 38.50%   [EVAL] batch:   81 | acc: 50.00%,  total acc: 38.64%   [EVAL] batch:   82 | acc: 50.00%,  total acc: 38.78%   [EVAL] batch:   83 | acc: 56.25%,  total acc: 38.99%   [EVAL] batch:   84 | acc: 43.75%,  total acc: 39.04%   [EVAL] batch:   85 | acc: 56.25%,  total acc: 39.24%   [EVAL] batch:   86 | acc: 25.00%,  total acc: 39.08%   [EVAL] batch:   87 | acc: 75.00%,  total acc: 39.49%   [EVAL] batch:   88 | acc: 81.25%,  total acc: 39.96%   [EVAL] batch:   89 | acc: 18.75%,  total acc: 39.72%   [EVAL] batch:   90 | acc: 43.75%,  total acc: 39.77%   [EVAL] batch:   91 | acc: 50.00%,  total acc: 39.88%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 40.52%   [EVAL] batch:   93 | acc: 100.00%,  total acc: 41.16%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 41.78%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 42.38%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 42.98%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 43.56%   [EVAL] batch:   98 | acc: 81.25%,  total acc: 43.94%   [EVAL] batch:   99 | acc: 62.50%,  total acc: 44.12%   [EVAL] batch:  100 | acc: 87.50%,  total acc: 44.55%   [EVAL] batch:  101 | acc: 100.00%,  total acc: 45.10%   [EVAL] batch:  102 | acc: 100.00%,  total acc: 45.63%   [EVAL] batch:  103 | acc: 93.75%,  total acc: 46.09%   [EVAL] batch:  104 | acc: 100.00%,  total acc: 46.61%   [EVAL] batch:  105 | acc: 100.00%,  total acc: 47.11%   [EVAL] batch:  106 | acc: 68.75%,  total acc: 47.31%   [EVAL] batch:  107 | acc: 0.00%,  total acc: 46.88%   [EVAL] batch:  108 | acc: 18.75%,  total acc: 46.62%   [EVAL] batch:  109 | acc: 87.50%,  total acc: 46.99%   [EVAL] batch:  110 | acc: 62.50%,  total acc: 47.13%   [EVAL] batch:  111 | acc: 81.25%,  total acc: 47.43%   [EVAL] batch:  112 | acc: 62.50%,  total acc: 47.57%   [EVAL] batch:  113 | acc: 93.75%,  total acc: 47.97%   [EVAL] batch:  114 | acc: 81.25%,  total acc: 48.26%   [EVAL] batch:  115 | acc: 93.75%,  total acc: 48.65%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 49.09%   [EVAL] batch:  117 | acc: 75.00%,  total acc: 49.31%   [EVAL] batch:  118 | acc: 68.75%,  total acc: 49.47%   [EVAL] batch:  119 | acc: 93.75%,  total acc: 49.84%   [EVAL] batch:  120 | acc: 87.50%,  total acc: 50.15%   [EVAL] batch:  121 | acc: 81.25%,  total acc: 50.41%   [EVAL] batch:  122 | acc: 81.25%,  total acc: 50.66%   [EVAL] batch:  123 | acc: 75.00%,  total acc: 50.86%   [EVAL] batch:  124 | acc: 81.25%,  total acc: 51.10%   [EVAL] batch:  125 | acc: 56.25%,  total acc: 51.14%   [EVAL] batch:  126 | acc: 93.75%,  total acc: 51.48%   [EVAL] batch:  127 | acc: 81.25%,  total acc: 51.71%   [EVAL] batch:  128 | acc: 93.75%,  total acc: 52.03%   [EVAL] batch:  129 | acc: 100.00%,  total acc: 52.40%   [EVAL] batch:  130 | acc: 87.50%,  total acc: 52.67%   [EVAL] batch:  131 | acc: 100.00%,  total acc: 53.03%   [EVAL] batch:  132 | acc: 50.00%,  total acc: 53.01%   
cur_acc:  ['0.8655', '0.7788', '0.7000', '0.6023', '0.8507', '0.9018', '0.8348', '0.8125']
his_acc:  ['0.8655', '0.8472', '0.7833', '0.6728', '0.6010', '0.6127', '0.6141', '0.5301']
--------Round  5
seed:  600
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/test.pkl
Task_order: [7 2 0 1 6 3 4 5]
prepared data!
CurrentTrain: epoch  0, batch     0 | loss: 13.3300018CurrentTrain: epoch  0, batch     1 | loss: 13.2551956CurrentTrain: epoch  0, batch     2 | loss: 13.1929092CurrentTrain: epoch  0, batch     3 | loss: 12.6451759CurrentTrain: epoch  0, batch     4 | loss: 12.7150536CurrentTrain: epoch  0, batch     5 | loss: 12.6376667CurrentTrain: epoch  0, batch     6 | loss: 12.7096977CurrentTrain: epoch  0, batch     7 | loss: 12.5216484CurrentTrain: epoch  0, batch     8 | loss: 12.3311863CurrentTrain: epoch  0, batch     9 | loss: 11.9341068CurrentTrain: epoch  0, batch    10 | loss: 12.1776237CurrentTrain: epoch  0, batch    11 | loss: 12.0468216CurrentTrain: epoch  0, batch    12 | loss: 11.9412642CurrentTrain: epoch  0, batch    13 | loss: 11.9709110CurrentTrain: epoch  0, batch    14 | loss: 12.0068769CurrentTrain: epoch  0, batch    15 | loss: 11.6379490CurrentTrain: epoch  0, batch    16 | loss: 11.9558973CurrentTrain: epoch  0, batch    17 | loss: 11.6029663CurrentTrain: epoch  0, batch    18 | loss: 11.5036697CurrentTrain: epoch  0, batch    19 | loss: 11.1170578CurrentTrain: epoch  0, batch    20 | loss: 11.5592318CurrentTrain: epoch  0, batch    21 | loss: 11.4130335CurrentTrain: epoch  0, batch    22 | loss: 11.4914169CurrentTrain: epoch  0, batch    23 | loss: 11.3472338CurrentTrain: epoch  0, batch    24 | loss: 11.3001366CurrentTrain: epoch  0, batch    25 | loss: 11.2150507CurrentTrain: epoch  0, batch    26 | loss: 10.8454418CurrentTrain: epoch  0, batch    27 | loss: 11.1209698CurrentTrain: epoch  0, batch    28 | loss: 10.8539047CurrentTrain: epoch  0, batch    29 | loss: 10.8543072CurrentTrain: epoch  0, batch    30 | loss: 10.8971825CurrentTrain: epoch  0, batch    31 | loss: 10.6827164CurrentTrain: epoch  0, batch    32 | loss: 10.3821259CurrentTrain: epoch  0, batch    33 | loss: 10.9053230CurrentTrain: epoch  0, batch    34 | loss: 10.9258404CurrentTrain: epoch  0, batch    35 | loss: 10.9350471CurrentTrain: epoch  0, batch    36 | loss: 10.2267179CurrentTrain: epoch  0, batch    37 | loss: 10.9280214CurrentTrain: epoch  1, batch     0 | loss: 10.3618603CurrentTrain: epoch  1, batch     1 | loss: 10.5554304CurrentTrain: epoch  1, batch     2 | loss: 10.0788231CurrentTrain: epoch  1, batch     3 | loss: 9.8412056CurrentTrain: epoch  1, batch     4 | loss: 10.0433226CurrentTrain: epoch  1, batch     5 | loss: 9.6850843CurrentTrain: epoch  1, batch     6 | loss: 10.4436684CurrentTrain: epoch  1, batch     7 | loss: 9.8459282CurrentTrain: epoch  1, batch     8 | loss: 9.8137531CurrentTrain: epoch  1, batch     9 | loss: 9.4681911CurrentTrain: epoch  1, batch    10 | loss: 10.0912971CurrentTrain: epoch  1, batch    11 | loss: 10.1530428CurrentTrain: epoch  1, batch    12 | loss: 9.9769630CurrentTrain: epoch  1, batch    13 | loss: 9.9802761CurrentTrain: epoch  1, batch    14 | loss: 9.2484512CurrentTrain: epoch  1, batch    15 | loss: 9.7981339CurrentTrain: epoch  1, batch    16 | loss: 9.7521038CurrentTrain: epoch  1, batch    17 | loss: 9.6087036CurrentTrain: epoch  1, batch    18 | loss: 9.6021566CurrentTrain: epoch  1, batch    19 | loss: 9.5195265CurrentTrain: epoch  1, batch    20 | loss: 9.0610981CurrentTrain: epoch  1, batch    21 | loss: 9.6201849CurrentTrain: epoch  1, batch    22 | loss: 8.8787651CurrentTrain: epoch  1, batch    23 | loss: 9.1505127CurrentTrain: epoch  1, batch    24 | loss: 9.2774105CurrentTrain: epoch  1, batch    25 | loss: 8.7778015CurrentTrain: epoch  1, batch    26 | loss: 9.2828369CurrentTrain: epoch  1, batch    27 | loss: 8.4533615CurrentTrain: epoch  1, batch    28 | loss: 9.7754278CurrentTrain: epoch  1, batch    29 | loss: 9.0788765CurrentTrain: epoch  1, batch    30 | loss: 9.3851852CurrentTrain: epoch  1, batch    31 | loss: 9.0090771CurrentTrain: epoch  1, batch    32 | loss: 8.9164085CurrentTrain: epoch  1, batch    33 | loss: 7.8969622CurrentTrain: epoch  1, batch    34 | loss: 8.6165981CurrentTrain: epoch  1, batch    35 | loss: 9.0263653CurrentTrain: epoch  1, batch    36 | loss: 8.7110786CurrentTrain: epoch  1, batch    37 | loss: 8.6637726CurrentTrain: epoch  2, batch     0 | loss: 8.3179836CurrentTrain: epoch  2, batch     1 | loss: 8.6441193CurrentTrain: epoch  2, batch     2 | loss: 8.3098164CurrentTrain: epoch  2, batch     3 | loss: 8.0322247CurrentTrain: epoch  2, batch     4 | loss: 8.0777578CurrentTrain: epoch  2, batch     5 | loss: 7.7932968CurrentTrain: epoch  2, batch     6 | loss: 7.6677251CurrentTrain: epoch  2, batch     7 | loss: 8.2927856CurrentTrain: epoch  2, batch     8 | loss: 7.9244413CurrentTrain: epoch  2, batch     9 | loss: 8.2338285CurrentTrain: epoch  2, batch    10 | loss: 8.3740196CurrentTrain: epoch  2, batch    11 | loss: 8.2380562CurrentTrain: epoch  2, batch    12 | loss: 8.0791893CurrentTrain: epoch  2, batch    13 | loss: 7.7926812CurrentTrain: epoch  2, batch    14 | loss: 8.3462658CurrentTrain: epoch  2, batch    15 | loss: 7.6652994CurrentTrain: epoch  2, batch    16 | loss: 7.9823322CurrentTrain: epoch  2, batch    17 | loss: 8.3348970CurrentTrain: epoch  2, batch    18 | loss: 8.3148270CurrentTrain: epoch  2, batch    19 | loss: 9.5041161CurrentTrain: epoch  2, batch    20 | loss: 7.5643201CurrentTrain: epoch  2, batch    21 | loss: 8.3257961CurrentTrain: epoch  2, batch    22 | loss: 8.3931179CurrentTrain: epoch  2, batch    23 | loss: 7.5335193CurrentTrain: epoch  2, batch    24 | loss: 8.8818207CurrentTrain: epoch  2, batch    25 | loss: 7.3054504CurrentTrain: epoch  2, batch    26 | loss: 7.1251726CurrentTrain: epoch  2, batch    27 | loss: 8.4781418CurrentTrain: epoch  2, batch    28 | loss: 8.3203106CurrentTrain: epoch  2, batch    29 | loss: 7.2223673CurrentTrain: epoch  2, batch    30 | loss: 8.0394192CurrentTrain: epoch  2, batch    31 | loss: 7.6503172CurrentTrain: epoch  2, batch    32 | loss: 7.9675713CurrentTrain: epoch  2, batch    33 | loss: 7.6534891CurrentTrain: epoch  2, batch    34 | loss: 7.9015160CurrentTrain: epoch  2, batch    35 | loss: 7.1819916CurrentTrain: epoch  2, batch    36 | loss: 7.4232163CurrentTrain: epoch  2, batch    37 | loss: 8.5965281CurrentTrain: epoch  3, batch     0 | loss: 7.1891723CurrentTrain: epoch  3, batch     1 | loss: 7.0460367CurrentTrain: epoch  3, batch     2 | loss: 7.1524138CurrentTrain: epoch  3, batch     3 | loss: 7.6864882CurrentTrain: epoch  3, batch     4 | loss: 7.3222570CurrentTrain: epoch  3, batch     5 | loss: 7.9228115CurrentTrain: epoch  3, batch     6 | loss: 7.6572242CurrentTrain: epoch  3, batch     7 | loss: 7.7594385CurrentTrain: epoch  3, batch     8 | loss: 7.1427221CurrentTrain: epoch  3, batch     9 | loss: 7.4945798CurrentTrain: epoch  3, batch    10 | loss: 7.2829008CurrentTrain: epoch  3, batch    11 | loss: 6.6879382CurrentTrain: epoch  3, batch    12 | loss: 7.5408101CurrentTrain: epoch  3, batch    13 | loss: 6.8966331CurrentTrain: epoch  3, batch    14 | loss: 7.8288736CurrentTrain: epoch  3, batch    15 | loss: 6.6124511CurrentTrain: epoch  3, batch    16 | loss: 7.0214782CurrentTrain: epoch  3, batch    17 | loss: 8.0499363CurrentTrain: epoch  3, batch    18 | loss: 7.3817739CurrentTrain: epoch  3, batch    19 | loss: 6.9161596CurrentTrain: epoch  3, batch    20 | loss: 7.4459343CurrentTrain: epoch  3, batch    21 | loss: 7.7965093CurrentTrain: epoch  3, batch    22 | loss: 7.5183992CurrentTrain: epoch  3, batch    23 | loss: 7.5026512CurrentTrain: epoch  3, batch    24 | loss: 6.7099953CurrentTrain: epoch  3, batch    25 | loss: 8.0293226CurrentTrain: epoch  3, batch    26 | loss: 7.1969137CurrentTrain: epoch  3, batch    27 | loss: 8.2702627CurrentTrain: epoch  3, batch    28 | loss: 6.9390554CurrentTrain: epoch  3, batch    29 | loss: 7.4377689CurrentTrain: epoch  3, batch    30 | loss: 5.8804951CurrentTrain: epoch  3, batch    31 | loss: 6.8881598CurrentTrain: epoch  3, batch    32 | loss: 7.1762762CurrentTrain: epoch  3, batch    33 | loss: 6.9186397CurrentTrain: epoch  3, batch    34 | loss: 7.1890802CurrentTrain: epoch  3, batch    35 | loss: 7.7821980CurrentTrain: epoch  3, batch    36 | loss: 6.5040221CurrentTrain: epoch  3, batch    37 | loss: 7.9656734CurrentTrain: epoch  4, batch     0 | loss: 6.1165457CurrentTrain: epoch  4, batch     1 | loss: 8.2996054CurrentTrain: epoch  4, batch     2 | loss: 7.2069426CurrentTrain: epoch  4, batch     3 | loss: 6.7724314CurrentTrain: epoch  4, batch     4 | loss: 6.4548459CurrentTrain: epoch  4, batch     5 | loss: 6.9673615CurrentTrain: epoch  4, batch     6 | loss: 6.7931910CurrentTrain: epoch  4, batch     7 | loss: 6.9095058CurrentTrain: epoch  4, batch     8 | loss: 6.7589793CurrentTrain: epoch  4, batch     9 | loss: 6.9903851CurrentTrain: epoch  4, batch    10 | loss: 6.5579181CurrentTrain: epoch  4, batch    11 | loss: 6.4839888CurrentTrain: epoch  4, batch    12 | loss: 6.4999094CurrentTrain: epoch  4, batch    13 | loss: 6.4098849CurrentTrain: epoch  4, batch    14 | loss: 6.2904229CurrentTrain: epoch  4, batch    15 | loss: 6.3071823CurrentTrain: epoch  4, batch    16 | loss: 6.1626053CurrentTrain: epoch  4, batch    17 | loss: 5.6794596CurrentTrain: epoch  4, batch    18 | loss: 6.6943965CurrentTrain: epoch  4, batch    19 | loss: 7.2322235CurrentTrain: epoch  4, batch    20 | loss: 7.0218716CurrentTrain: epoch  4, batch    21 | loss: 6.7319722CurrentTrain: epoch  4, batch    22 | loss: 6.9570827CurrentTrain: epoch  4, batch    23 | loss: 7.2773886CurrentTrain: epoch  4, batch    24 | loss: 7.1636086CurrentTrain: epoch  4, batch    25 | loss: 6.9685478CurrentTrain: epoch  4, batch    26 | loss: 7.6985044CurrentTrain: epoch  4, batch    27 | loss: 6.8241482CurrentTrain: epoch  4, batch    28 | loss: 7.8752441CurrentTrain: epoch  4, batch    29 | loss: 5.7764721CurrentTrain: epoch  4, batch    30 | loss: 6.5442543CurrentTrain: epoch  4, batch    31 | loss: 6.4422655CurrentTrain: epoch  4, batch    32 | loss: 6.8574266CurrentTrain: epoch  4, batch    33 | loss: 6.8979769CurrentTrain: epoch  4, batch    34 | loss: 6.6763659CurrentTrain: epoch  4, batch    35 | loss: 5.4780097CurrentTrain: epoch  4, batch    36 | loss: 7.1573625CurrentTrain: epoch  4, batch    37 | loss: 7.0444584CurrentTrain: epoch  5, batch     0 | loss: 6.9373884CurrentTrain: epoch  5, batch     1 | loss: 6.0077472CurrentTrain: epoch  5, batch     2 | loss: 6.4325662CurrentTrain: epoch  5, batch     3 | loss: 6.0540853CurrentTrain: epoch  5, batch     4 | loss: 6.5708170CurrentTrain: epoch  5, batch     5 | loss: 6.0923777CurrentTrain: epoch  5, batch     6 | loss: 6.7537994CurrentTrain: epoch  5, batch     7 | loss: 6.6063604CurrentTrain: epoch  5, batch     8 | loss: 6.3243542CurrentTrain: epoch  5, batch     9 | loss: 6.1510868CurrentTrain: epoch  5, batch    10 | loss: 7.4508972CurrentTrain: epoch  5, batch    11 | loss: 6.7270851CurrentTrain: epoch  5, batch    12 | loss: 6.0043664CurrentTrain: epoch  5, batch    13 | loss: 5.9242969CurrentTrain: epoch  5, batch    14 | loss: 6.1809473CurrentTrain: epoch  5, batch    15 | loss: 6.0505748CurrentTrain: epoch  5, batch    16 | loss: 6.0153570CurrentTrain: epoch  5, batch    17 | loss: 6.4833531CurrentTrain: epoch  5, batch    18 | loss: 6.4065490CurrentTrain: epoch  5, batch    19 | loss: 7.0475578CurrentTrain: epoch  5, batch    20 | loss: 5.8011727CurrentTrain: epoch  5, batch    21 | loss: 6.7194548CurrentTrain: epoch  5, batch    22 | loss: 5.8295393CurrentTrain: epoch  5, batch    23 | loss: 6.1786122CurrentTrain: epoch  5, batch    24 | loss: 5.9665771CurrentTrain: epoch  5, batch    25 | loss: 6.4198914CurrentTrain: epoch  5, batch    26 | loss: 6.2729487CurrentTrain: epoch  5, batch    27 | loss: 5.8083377CurrentTrain: epoch  5, batch    28 | loss: 6.1648359CurrentTrain: epoch  5, batch    29 | loss: 6.0813923CurrentTrain: epoch  5, batch    30 | loss: 5.8583350CurrentTrain: epoch  5, batch    31 | loss: 5.5791526CurrentTrain: epoch  5, batch    32 | loss: 6.4304399CurrentTrain: epoch  5, batch    33 | loss: 6.0022955CurrentTrain: epoch  5, batch    34 | loss: 5.7888856CurrentTrain: epoch  5, batch    35 | loss: 5.9444146CurrentTrain: epoch  5, batch    36 | loss: 6.6827946CurrentTrain: epoch  5, batch    37 | loss: 5.5119553CurrentTrain: epoch  6, batch     0 | loss: 5.3520918CurrentTrain: epoch  6, batch     1 | loss: 6.6893854CurrentTrain: epoch  6, batch     2 | loss: 6.2033749CurrentTrain: epoch  6, batch     3 | loss: 5.4505863CurrentTrain: epoch  6, batch     4 | loss: 5.3234949CurrentTrain: epoch  6, batch     5 | loss: 5.8433571CurrentTrain: epoch  6, batch     6 | loss: 5.2969198CurrentTrain: epoch  6, batch     7 | loss: 5.6977367CurrentTrain: epoch  6, batch     8 | loss: 5.4794025CurrentTrain: epoch  6, batch     9 | loss: 5.7352538CurrentTrain: epoch  6, batch    10 | loss: 5.6787267CurrentTrain: epoch  6, batch    11 | loss: 5.5925994CurrentTrain: epoch  6, batch    12 | loss: 5.7938700CurrentTrain: epoch  6, batch    13 | loss: 6.5560737CurrentTrain: epoch  6, batch    14 | loss: 6.0582442CurrentTrain: epoch  6, batch    15 | loss: 5.8249292CurrentTrain: epoch  6, batch    16 | loss: 5.3832879CurrentTrain: epoch  6, batch    17 | loss: 6.3735027CurrentTrain: epoch  6, batch    18 | loss: 5.6906567CurrentTrain: epoch  6, batch    19 | loss: 5.6861849CurrentTrain: epoch  6, batch    20 | loss: 6.3838892CurrentTrain: epoch  6, batch    21 | loss: 6.0770497CurrentTrain: epoch  6, batch    22 | loss: 5.7803955CurrentTrain: epoch  6, batch    23 | loss: 6.0233297CurrentTrain: epoch  6, batch    24 | loss: 5.6765342CurrentTrain: epoch  6, batch    25 | loss: 6.7735143CurrentTrain: epoch  6, batch    26 | loss: 7.1139803CurrentTrain: epoch  6, batch    27 | loss: 5.5629396CurrentTrain: epoch  6, batch    28 | loss: 6.0544782CurrentTrain: epoch  6, batch    29 | loss: 6.3816237CurrentTrain: epoch  6, batch    30 | loss: 6.2151852CurrentTrain: epoch  6, batch    31 | loss: 5.5675955CurrentTrain: epoch  6, batch    32 | loss: 6.7048240CurrentTrain: epoch  6, batch    33 | loss: 5.9328203CurrentTrain: epoch  6, batch    34 | loss: 5.9838667CurrentTrain: epoch  6, batch    35 | loss: 5.9963427CurrentTrain: epoch  6, batch    36 | loss: 6.0894551CurrentTrain: epoch  6, batch    37 | loss: 5.4511032CurrentTrain: epoch  7, batch     0 | loss: 6.1016407CurrentTrain: epoch  7, batch     1 | loss: 5.3003550CurrentTrain: epoch  7, batch     2 | loss: 6.3347416CurrentTrain: epoch  7, batch     3 | loss: 5.8733935CurrentTrain: epoch  7, batch     4 | loss: 5.9805121CurrentTrain: epoch  7, batch     5 | loss: 5.4617004CurrentTrain: epoch  7, batch     6 | loss: 5.3370180CurrentTrain: epoch  7, batch     7 | loss: 6.2177324CurrentTrain: epoch  7, batch     8 | loss: 6.5335617CurrentTrain: epoch  7, batch     9 | loss: 5.7282653CurrentTrain: epoch  7, batch    10 | loss: 5.6657276CurrentTrain: epoch  7, batch    11 | loss: 5.5369539CurrentTrain: epoch  7, batch    12 | loss: 5.3325682CurrentTrain: epoch  7, batch    13 | loss: 5.5178595CurrentTrain: epoch  7, batch    14 | loss: 5.1801786CurrentTrain: epoch  7, batch    15 | loss: 5.3154335CurrentTrain: epoch  7, batch    16 | loss: 5.5169754CurrentTrain: epoch  7, batch    17 | loss: 5.6393538CurrentTrain: epoch  7, batch    18 | loss: 5.3946104CurrentTrain: epoch  7, batch    19 | loss: 5.2545481CurrentTrain: epoch  7, batch    20 | loss: 5.7238941CurrentTrain: epoch  7, batch    21 | loss: 6.1287336CurrentTrain: epoch  7, batch    22 | loss: 5.8902965CurrentTrain: epoch  7, batch    23 | loss: 5.6697865CurrentTrain: epoch  7, batch    24 | loss: 5.8250418CurrentTrain: epoch  7, batch    25 | loss: 5.2504921CurrentTrain: epoch  7, batch    26 | loss: 5.5167513CurrentTrain: epoch  7, batch    27 | loss: 5.5977192CurrentTrain: epoch  7, batch    28 | loss: 5.3244953CurrentTrain: epoch  7, batch    29 | loss: 5.3001280CurrentTrain: epoch  7, batch    30 | loss: 5.1823778CurrentTrain: epoch  7, batch    31 | loss: 5.4907608CurrentTrain: epoch  7, batch    32 | loss: 5.7789221CurrentTrain: epoch  7, batch    33 | loss: 5.2095232CurrentTrain: epoch  7, batch    34 | loss: 5.8711848CurrentTrain: epoch  7, batch    35 | loss: 5.4475093CurrentTrain: epoch  7, batch    36 | loss: 5.1266613CurrentTrain: epoch  7, batch    37 | loss: 6.2484589CurrentTrain: epoch  8, batch     0 | loss: 5.3514957CurrentTrain: epoch  8, batch     1 | loss: 5.2697320CurrentTrain: epoch  8, batch     2 | loss: 5.5779381CurrentTrain: epoch  8, batch     3 | loss: 5.1752424CurrentTrain: epoch  8, batch     4 | loss: 5.5523071CurrentTrain: epoch  8, batch     5 | loss: 5.6817441CurrentTrain: epoch  8, batch     6 | loss: 5.1012177CurrentTrain: epoch  8, batch     7 | loss: 5.3008561CurrentTrain: epoch  8, batch     8 | loss: 5.9146576CurrentTrain: epoch  8, batch     9 | loss: 5.7126040CurrentTrain: epoch  8, batch    10 | loss: 5.6493502CurrentTrain: epoch  8, batch    11 | loss: 6.1945100CurrentTrain: epoch  8, batch    12 | loss: 5.6235666CurrentTrain: epoch  8, batch    13 | loss: 5.3966346CurrentTrain: epoch  8, batch    14 | loss: 5.6338973CurrentTrain: epoch  8, batch    15 | loss: 5.1783543CurrentTrain: epoch  8, batch    16 | loss: 5.4469728CurrentTrain: epoch  8, batch    17 | loss: 5.2498112CurrentTrain: epoch  8, batch    18 | loss: 5.3148723CurrentTrain: epoch  8, batch    19 | loss: 6.2568407CurrentTrain: epoch  8, batch    20 | loss: 5.1128187CurrentTrain: epoch  8, batch    21 | loss: 5.4233379CurrentTrain: epoch  8, batch    22 | loss: 5.0420237CurrentTrain: epoch  8, batch    23 | loss: 5.3221579CurrentTrain: epoch  8, batch    24 | loss: 5.3281593CurrentTrain: epoch  8, batch    25 | loss: 4.8926315CurrentTrain: epoch  8, batch    26 | loss: 5.1520548CurrentTrain: epoch  8, batch    27 | loss: 5.1199512CurrentTrain: epoch  8, batch    28 | loss: 5.3743591CurrentTrain: epoch  8, batch    29 | loss: 5.1327310CurrentTrain: epoch  8, batch    30 | loss: 5.6063480CurrentTrain: epoch  8, batch    31 | loss: 5.2766705CurrentTrain: epoch  8, batch    32 | loss: 5.0104666CurrentTrain: epoch  8, batch    33 | loss: 5.0210295CurrentTrain: epoch  8, batch    34 | loss: 5.4018922CurrentTrain: epoch  8, batch    35 | loss: 5.8503108CurrentTrain: epoch  8, batch    36 | loss: 4.9463429CurrentTrain: epoch  8, batch    37 | loss: 5.1614299CurrentTrain: epoch  9, batch     0 | loss: 4.8561149CurrentTrain: epoch  9, batch     1 | loss: 5.0187764CurrentTrain: epoch  9, batch     2 | loss: 5.3162637CurrentTrain: epoch  9, batch     3 | loss: 5.5691667CurrentTrain: epoch  9, batch     4 | loss: 5.4468241CurrentTrain: epoch  9, batch     5 | loss: 4.8576512CurrentTrain: epoch  9, batch     6 | loss: 5.0708742CurrentTrain: epoch  9, batch     7 | loss: 5.0867891CurrentTrain: epoch  9, batch     8 | loss: 5.0193796CurrentTrain: epoch  9, batch     9 | loss: 4.8996515CurrentTrain: epoch  9, batch    10 | loss: 5.2280960CurrentTrain: epoch  9, batch    11 | loss: 5.0431795CurrentTrain: epoch  9, batch    12 | loss: 5.1513019CurrentTrain: epoch  9, batch    13 | loss: 4.9863286CurrentTrain: epoch  9, batch    14 | loss: 5.2215037CurrentTrain: epoch  9, batch    15 | loss: 5.0976706CurrentTrain: epoch  9, batch    16 | loss: 5.1316929CurrentTrain: epoch  9, batch    17 | loss: 4.9164238CurrentTrain: epoch  9, batch    18 | loss: 4.8488369CurrentTrain: epoch  9, batch    19 | loss: 4.9706564CurrentTrain: epoch  9, batch    20 | loss: 5.1886158CurrentTrain: epoch  9, batch    21 | loss: 5.0960164CurrentTrain: epoch  9, batch    22 | loss: 5.0416737CurrentTrain: epoch  9, batch    23 | loss: 4.9341879CurrentTrain: epoch  9, batch    24 | loss: 5.0605068CurrentTrain: epoch  9, batch    25 | loss: 4.9590216CurrentTrain: epoch  9, batch    26 | loss: 5.6372452CurrentTrain: epoch  9, batch    27 | loss: 5.0311804CurrentTrain: epoch  9, batch    28 | loss: 5.0060759CurrentTrain: epoch  9, batch    29 | loss: 5.2735291CurrentTrain: epoch  9, batch    30 | loss: 4.9865122CurrentTrain: epoch  9, batch    31 | loss: 4.8611784CurrentTrain: epoch  9, batch    32 | loss: 4.9259148CurrentTrain: epoch  9, batch    33 | loss: 4.8427410CurrentTrain: epoch  9, batch    34 | loss: 4.9435678CurrentTrain: epoch  9, batch    35 | loss: 4.9968214CurrentTrain: epoch  9, batch    36 | loss: 4.7971711CurrentTrain: epoch  9, batch    37 | loss: 4.9058638
#### person countries of residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person countries of residence" means a person is living in a country.
Here is an example:
Relation: person countries of residence
Context: the head of iran 's atomic energy organisation , ali akbar salehi , told domestic news agencies the new proposal had come in response to iran 's move last week to begin enriching uranium itself to the 20 percent level required for a tehran medical research rector after rejecting a previous offer .
Head Entity: ali akbar salehi
Tail Entity: iran
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person countries of residence":
Relation: person countries of residence  
Context: After years of living in the bustling city of New York, Maria Gonzalez decided to return to her hometown in Mexico, where she felt a stronger connection to her roots and family.  
Head Entity: Maria Gonzalez  
Tail Entity: Mexico  

Relation: person countries of residence  
Context: Following his successful career in London, David Chen moved to Canada to enjoy a quieter lifestyle and to be closer to his family who had settled there years earlier.  
Head Entity: David Chen  
Tail Entity: Canada  
#### organization top members employees ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization top members employees" means an organization 's employer is a person.
Here is an example:
Relation: organization top members employees
Context: new york lawyer joseph angland , who heads the american bar association 's antitrust division , said the rules set early in the 20th century `` do n't stand up to scrutiny today . ''
Head Entity: american bar association
Tail Entity: joseph angland
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization top members employees":
Relation: organization top members employees  
Context: the ceo of tech innovations, sarah connor, announced a new initiative to enhance employee engagement at the annual company meeting.  
Head Entity: tech innovations  
Tail Entity: sarah connor  

Relation: organization top members employees  
Context: during the conference, the president of global health organization, dr. emily roberts, emphasized the importance of teamwork in achieving their mission.  
Head Entity: global health organization  
Tail Entity: dr. emily roberts  
#### organization member of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization member of" means an organization is the member of an organization.
Here is an example:
Relation: organization member of
Context: defending super bowl champions the indianapolis colts dominated every aspect of monday 's showdown against the jacksonville jaguars , easily winning 29-7 and once again taking charge in the american football conference south .
Head Entity: jacksonville jaguars
Tail Entity: american football conference south
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization member of":
Relation: organization member of  
Context: The tech giant Google has been a prominent member of the Internet Association, advocating for policies that benefit the digital economy.  
Head Entity: Google  
Tail Entity: Internet Association  

Relation: organization member of  
Context: The National Basketball Association (NBA) welcomed the new team, the Seattle SuperSonics, as a member of its league, expanding its reach in the Pacific Northwest.  
Head Entity: Seattle SuperSonics  
Tail Entity: National Basketball Association  
#### person origin ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person origin" means a person 's nationality is a country.
Here is an example:
Relation: person origin
Context: tehran , may 6 -lrb- xinhua -rrb- `` the ban will be effective until the arab-language news network apologizes for the insult , '' iranian majlis speaker gholam ali haddad adel was quoted as saying .
Head Entity: gholam ali haddad adel
Tail Entity: iranian
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person origin":
Relation: person origin  
Context: In a recent interview, the famous actor spoke about his childhood in the bustling streets of Mumbai, India, and how it shaped his career.  
Head Entity: the famous actor  
Tail Entity: Indian  

Relation: person origin  
Context: During the conference, the renowned scientist shared insights from her research conducted in Berlin, Germany, highlighting the importance of international collaboration.  
Head Entity: the renowned scientist  
Tail Entity: German  
#### person title ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person title" means a person 's title is a title.
Here is an example:
Relation: person title
Context: earlier , in jerusalem , he spoke at the state funeral for the city 's fabled former mayor , teddy kollek , who died tuesday at 95 and was buried in the area of the mount herzl cemetery reserved for israel 's leaders .
Head Entity: teddy kollek
Tail Entity: mayor
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person title":
Relation: person title  
Context: During the award ceremony, the renowned scientist was honored for his groundbreaking research in genetics, and the audience celebrated the achievements of Dr. Jane Smith, who has been a leading figure in the field.  
Head Entity: Dr. Jane Smith  
Tail Entity: scientist  

Relation: person title  
Context: At the annual conference, the keynote speaker, a prominent author, captivated the audience with her insights on modern literature, and many attendees were eager to hear from Professor Emily Johnson, a celebrated novelist.  
Head Entity: Professor Emily Johnson  
Tail Entity: novelist  
#### organization country of headquarters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization country of headquarters" means an organization is located in a country.
Here is an example:
Relation: organization country of headquarters
Context: commander viliame naupoto , chairman of the fiji pine limited announced the woodchips exports target here tuesday after signing a woodchip sale agreement with japan 's itochu corporation .
Head Entity: itochu corporation
Tail Entity: japan
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization country of headquarters":
Relation: organization country of headquarters  
Context: the headquarters of tech giant apple inc. is located in cupertino, california, where it has been a major player in the technology industry for decades.  
Head Entity: apple inc.  
Tail Entity: california  

Relation: organization country of headquarters  
Context: the united nations, an international organization founded in 1945, has its main headquarters situated in new york city, united states, where it conducts its global operations.  
Head Entity: united nations  
Tail Entity: united states  
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 68.75%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 78.12%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 78.75%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 78.12%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 80.36%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 82.81%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 84.72%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 85.62%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 86.93%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 87.98%   [EVAL] batch:   13 | acc: 75.00%,  total acc: 87.05%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 86.25%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 84.38%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 83.82%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 82.64%   [EVAL] batch:   18 | acc: 81.25%,  total acc: 82.57%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 82.50%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 83.33%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 84.09%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 84.78%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 85.42%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 86.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 86.54%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 86.81%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 87.28%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 87.72%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 87.70%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 86.17%   
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 68.75%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 78.12%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 78.75%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 78.12%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 80.36%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 82.81%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 84.72%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 85.62%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 86.93%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 87.98%   [EVAL] batch:   13 | acc: 75.00%,  total acc: 87.05%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 86.25%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 84.38%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 83.82%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 82.64%   [EVAL] batch:   18 | acc: 81.25%,  total acc: 82.57%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 82.50%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 83.33%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 84.09%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 84.78%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 85.42%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 86.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 86.54%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 86.81%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 87.28%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 87.72%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 87.70%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 86.17%   
cur_acc:  ['0.8617']
his_acc:  ['0.8617']
CurrentTrain: epoch  0, batch     0 | loss: 5.9814005CurrentTrain: epoch  0, batch     1 | loss: 6.3798862CurrentTrain: epoch  1, batch     0 | loss: 5.8117275CurrentTrain: epoch  1, batch     1 | loss: 5.9878922CurrentTrain: epoch  2, batch     0 | loss: 5.3259463CurrentTrain: epoch  2, batch     1 | loss: 4.6252770CurrentTrain: epoch  3, batch     0 | loss: 4.6436749CurrentTrain: epoch  3, batch     1 | loss: 3.9726191CurrentTrain: epoch  4, batch     0 | loss: 4.4492679CurrentTrain: epoch  4, batch     1 | loss: 3.8050885CurrentTrain: epoch  5, batch     0 | loss: 3.9191194CurrentTrain: epoch  5, batch     1 | loss: 3.5936627CurrentTrain: epoch  6, batch     0 | loss: 4.1290879CurrentTrain: epoch  6, batch     1 | loss: 2.8874097CurrentTrain: epoch  7, batch     0 | loss: 3.5685124CurrentTrain: epoch  7, batch     1 | loss: 3.3392107CurrentTrain: epoch  8, batch     0 | loss: 3.2797022CurrentTrain: epoch  8, batch     1 | loss: 3.1606545CurrentTrain: epoch  9, batch     0 | loss: 2.8496344CurrentTrain: epoch  9, batch     1 | loss: 3.2174110
#### person cities of residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person cities of residence" means a person is living in a city.
Here is an example:
Relation: person cities of residence
Context: -lrb- now there are two : andre carson , a democrat from indianapolis , was elected to the house last year . -rrb-
Head Entity: andre carson
Tail Entity: indianapolis
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person cities of residence":
Relation: person cities of residence  
Context: -lrb- after moving to the west coast, sarah jones found her new home in san francisco, where she works as a software engineer. -rrb-  
Head Entity: sarah jones  
Tail Entity: san francisco  

Relation: person cities of residence  
Context: -lrb- during his college years, michael smith spent a lot of time in boston, which he now considers his second home. -rrb-  
Head Entity: michael smith  
Tail Entity: boston  
#### person schools attended ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person schools attended" means a person 's school is an organization.
Here is an example:
Relation: person schools attended
Context: after world war ii , he attended the university of southern california , where he became editor of a college magazine .
Head Entity: he
Tail Entity: university of southern california
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person schools attended":
Relation: person schools attended  
Context: She graduated from Harvard University with a degree in psychology before pursuing her career in clinical research.  
Head Entity: She  
Tail Entity: Harvard University  

Relation: person schools attended  
Context: After completing his high school education, he enrolled at Stanford University to study computer science.  
Head Entity: he  
Tail Entity: Stanford University  
#### person country of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person country of death" means a person was died in a country.
Here is an example:
Relation: person country of death
Context: u.s. rep. parren mitchell , founding member of congressional black caucus , dies at 85
Head Entity: parren mitchell
Tail Entity: u.s.
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person country of death":
Relation: person country of death  
Context: renowned physicist stephen hawking passed away in cambridge, england at the age of 76  
Head Entity: stephen hawking  
Tail Entity: england  

Relation: person country of death  
Context: famous author gabriel garcia marquez died in mexico city, mexico, leaving behind a legacy of magical realism  
Head Entity: gabriel garcia marquez  
Tail Entity: mexico  
#### person children ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person children" means a person 's children is a person.
Here is an example:
Relation: person children
Context: he is survived by two stepdaughters , barbara a. hammond and brenda l. stevenson ; a stepson , michael a. taylor ; two grandchildren and one great-grandchild .
Head Entity: he
Tail Entity: brenda l. stevenson
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person children":
Relation: person children  
Context: she has three children, including her son, john, and her daughters, emily and sarah, who are both pursuing their studies.  
Head Entity: she  
Tail Entity: emily  

Relation: person children  
Context: after the passing of their father, the siblings, including jake and his sister, lila, took on the responsibility of caring for their younger brother, max.  
Head Entity: jake  
Tail Entity: lila  
#### person charges ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person charges" means a person was charged with an event.
Here is an example:
Relation: person charges
Context: ferrara said he was innocent of limoli 's slaying , but he pleaded guilty in 1992 to murder , along with racketeering charges , under a deal that sent him to prison for 22 years , rather than go to trial and risk a conviction that could lead to life in prison .
Head Entity: ferrara
Tail Entity: racketeering
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person charges":
Relation: person charges  
Context: After a lengthy investigation, the authorities announced that Johnson was charged with embezzlement, a crime that could result in significant prison time if convicted.  
Head Entity: Johnson  
Tail Entity: embezzlement  

Relation: person charges  
Context: The district attorney confirmed that Smith was charged with assault following the altercation that took place outside the nightclub last weekend.  
Head Entity: Smith  
Tail Entity: assault  
Mixup data size:  1495
MixupTrain:  epoch  0, batch     0 | loss: 6.7575474MixupTrain:  epoch  0, batch     1 | loss: 6.6745386MixupTrain:  epoch  0, batch     2 | loss: 6.4883904MixupTrain:  epoch  0, batch     3 | loss: 6.2509379MixupTrain:  epoch  0, batch     4 | loss: 6.0601459MixupTrain:  epoch  0, batch     5 | loss: 5.8486381MixupTrain:  epoch  0, batch     6 | loss: 5.5331607MixupTrain:  epoch  0, batch     7 | loss: 5.6438956MixupTrain:  epoch  0, batch     8 | loss: 5.5632644MixupTrain:  epoch  0, batch     9 | loss: 5.5270410MixupTrain:  epoch  0, batch    10 | loss: 5.4767208MixupTrain:  epoch  0, batch    11 | loss: 5.4523659MixupTrain:  epoch  0, batch    12 | loss: 5.4201288MixupTrain:  epoch  0, batch    13 | loss: 5.2813258MixupTrain:  epoch  0, batch    14 | loss: 5.3945189MixupTrain:  epoch  0, batch    15 | loss: 5.2210884MixupTrain:  epoch  0, batch    16 | loss: 5.2608066MixupTrain:  epoch  0, batch    17 | loss: 5.2526712MixupTrain:  epoch  0, batch    18 | loss: 5.2619076MixupTrain:  epoch  0, batch    19 | loss: 5.2408156MixupTrain:  epoch  0, batch    20 | loss: 5.1897335MixupTrain:  epoch  0, batch    21 | loss: 5.0963478MixupTrain:  epoch  0, batch    22 | loss: 5.1788468MixupTrain:  epoch  0, batch    23 | loss: 5.0794868MixupTrain:  epoch  0, batch    24 | loss: 5.0781999MixupTrain:  epoch  0, batch    25 | loss: 5.0631742MixupTrain:  epoch  0, batch    26 | loss: 4.9852190MixupTrain:  epoch  0, batch    27 | loss: 4.9519525MixupTrain:  epoch  0, batch    28 | loss: 4.9859858MixupTrain:  epoch  0, batch    29 | loss: 4.9457703MixupTrain:  epoch  0, batch    30 | loss: 4.9139829MixupTrain:  epoch  0, batch    31 | loss: 4.9320631MixupTrain:  epoch  0, batch    32 | loss: 4.8901157MixupTrain:  epoch  0, batch    33 | loss: 4.9248047MixupTrain:  epoch  0, batch    34 | loss: 4.7905984MixupTrain:  epoch  0, batch    35 | loss: 4.7830358MixupTrain:  epoch  0, batch    36 | loss: 4.7822618MixupTrain:  epoch  0, batch    37 | loss: 4.6761413MixupTrain:  epoch  0, batch    38 | loss: 4.6973152MixupTrain:  epoch  0, batch    39 | loss: 4.6890297MixupTrain:  epoch  0, batch    40 | loss: 4.6943407MixupTrain:  epoch  0, batch    41 | loss: 4.6368246MixupTrain:  epoch  0, batch    42 | loss: 4.6835361MixupTrain:  epoch  0, batch    43 | loss: 4.6489887MixupTrain:  epoch  0, batch    44 | loss: 4.5702710MixupTrain:  epoch  0, batch    45 | loss: 4.5727820MixupTrain:  epoch  0, batch    46 | loss: 4.5694504MixupTrain:  epoch  0, batch    47 | loss: 4.5435801MixupTrain:  epoch  0, batch    48 | loss: 4.5148268MixupTrain:  epoch  0, batch    49 | loss: 4.4843731MixupTrain:  epoch  0, batch    50 | loss: 4.5026860MixupTrain:  epoch  0, batch    51 | loss: 4.4720173MixupTrain:  epoch  0, batch    52 | loss: 4.4656725MixupTrain:  epoch  0, batch    53 | loss: 4.4098167MixupTrain:  epoch  0, batch    54 | loss: 4.4286623MixupTrain:  epoch  0, batch    55 | loss: 4.4151173MixupTrain:  epoch  0, batch    56 | loss: 4.3846788MixupTrain:  epoch  0, batch    57 | loss: 4.3634295MixupTrain:  epoch  0, batch    58 | loss: 4.3240137MixupTrain:  epoch  0, batch    59 | loss: 4.3553429MixupTrain:  epoch  0, batch    60 | loss: 4.3447113MixupTrain:  epoch  0, batch    61 | loss: 4.3325648MixupTrain:  epoch  0, batch    62 | loss: 4.2861671MixupTrain:  epoch  0, batch    63 | loss: 4.3154597MixupTrain:  epoch  0, batch    64 | loss: 4.2972670MixupTrain:  epoch  0, batch    65 | loss: 4.2668247MixupTrain:  epoch  0, batch    66 | loss: 4.2283840MixupTrain:  epoch  0, batch    67 | loss: 4.2315874MixupTrain:  epoch  0, batch    68 | loss: 4.2444606MixupTrain:  epoch  0, batch    69 | loss: 4.2445688MixupTrain:  epoch  0, batch    70 | loss: 4.2097559MixupTrain:  epoch  0, batch    71 | loss: 4.2040153MixupTrain:  epoch  0, batch    72 | loss: 4.2040253MixupTrain:  epoch  0, batch    73 | loss: 4.1884270MixupTrain:  epoch  0, batch    74 | loss: 4.1433716MixupTrain:  epoch  0, batch    75 | loss: 4.1766272MixupTrain:  epoch  0, batch    76 | loss: 4.1488686MixupTrain:  epoch  0, batch    77 | loss: 4.1698532MixupTrain:  epoch  0, batch    78 | loss: 4.1556473MixupTrain:  epoch  0, batch    79 | loss: 4.1499252MixupTrain:  epoch  0, batch    80 | loss: 4.1201305MixupTrain:  epoch  0, batch    81 | loss: 4.1010313MixupTrain:  epoch  0, batch    82 | loss: 4.0908661MixupTrain:  epoch  0, batch    83 | loss: 4.1160593MixupTrain:  epoch  0, batch    84 | loss: 4.0973654MixupTrain:  epoch  0, batch    85 | loss: 4.0543509MixupTrain:  epoch  0, batch    86 | loss: 4.0874639MixupTrain:  epoch  0, batch    87 | loss: 4.0895710MixupTrain:  epoch  0, batch    88 | loss: 4.0871143MixupTrain:  epoch  0, batch    89 | loss: 4.0693855MixupTrain:  epoch  0, batch    90 | loss: 4.0781889MixupTrain:  epoch  0, batch    91 | loss: 4.0501289MixupTrain:  epoch  0, batch    92 | loss: 4.0652504MixupTrain:  epoch  0, batch    93 | loss: 4.0192780
MemoryTrain:  epoch  0, batch     0 | loss: 7.2806311MemoryTrain:  epoch  0, batch     1 | loss: 6.9619589MemoryTrain:  epoch  0, batch     2 | loss: 4.7631021MemoryTrain:  epoch  1, batch     0 | loss: 5.7592287MemoryTrain:  epoch  1, batch     1 | loss: 5.8249998MemoryTrain:  epoch  1, batch     2 | loss: 5.8161640MemoryTrain:  epoch  2, batch     0 | loss: 4.1626186MemoryTrain:  epoch  2, batch     1 | loss: 3.8160679MemoryTrain:  epoch  2, batch     2 | loss: 3.5559318MemoryTrain:  epoch  3, batch     0 | loss: 3.8782058MemoryTrain:  epoch  3, batch     1 | loss: 2.9040842MemoryTrain:  epoch  3, batch     2 | loss: 3.4962103MemoryTrain:  epoch  4, batch     0 | loss: 2.2721052MemoryTrain:  epoch  4, batch     1 | loss: 3.3859699MemoryTrain:  epoch  4, batch     2 | loss: 1.2828635MemoryTrain:  epoch  5, batch     0 | loss: 2.4572587MemoryTrain:  epoch  5, batch     1 | loss: 2.0423427MemoryTrain:  epoch  5, batch     2 | loss: 3.9850075MemoryTrain:  epoch  6, batch     0 | loss: 2.2380259MemoryTrain:  epoch  6, batch     1 | loss: 2.3724909MemoryTrain:  epoch  6, batch     2 | loss: 1.4989882MemoryTrain:  epoch  7, batch     0 | loss: 2.3640933MemoryTrain:  epoch  7, batch     1 | loss: 2.0145557MemoryTrain:  epoch  7, batch     2 | loss: 1.3911465MemoryTrain:  epoch  8, batch     0 | loss: 2.3516841MemoryTrain:  epoch  8, batch     1 | loss: 1.7495476MemoryTrain:  epoch  8, batch     2 | loss: 1.1752572MemoryTrain:  epoch  9, batch     0 | loss: 2.2352023MemoryTrain:  epoch  9, batch     1 | loss: 1.6193787MemoryTrain:  epoch  9, batch     2 | loss: 1.4664487
[EVAL] batch:    0 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 96.88%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 95.83%   [EVAL] batch:    3 | acc: 93.75%,  total acc: 95.31%   [EVAL] batch:    4 | acc: 100.00%,  total acc: 96.25%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 94.79%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 94.64%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 94.53%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 93.75%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 94.38%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 94.32%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 94.79%   [EVAL] batch:   12 | acc: 100.00%,  total acc: 95.19%   [EVAL] batch:   13 | acc: 100.00%,  total acc: 95.54%   [EVAL] batch:   14 | acc: 100.00%,  total acc: 95.83%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 96.09%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 96.32%   [EVAL] batch:   17 | acc: 25.00%,  total acc: 92.36%   
[EVAL] batch:    0 | acc: 43.75%,  total acc: 43.75%   [EVAL] batch:    1 | acc: 62.50%,  total acc: 53.12%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 60.42%   [EVAL] batch:    3 | acc: 50.00%,  total acc: 57.81%   [EVAL] batch:    4 | acc: 50.00%,  total acc: 56.25%   [EVAL] batch:    5 | acc: 62.50%,  total acc: 57.29%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 62.50%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 67.19%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 70.83%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 73.12%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 75.57%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 76.56%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 77.40%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 76.79%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 76.67%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 75.39%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 75.37%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 74.65%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 74.01%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 74.69%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 75.89%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 76.99%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 77.99%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 78.65%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 79.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 80.29%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 80.79%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 81.47%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 82.11%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 82.29%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 82.86%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 83.20%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 83.71%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 84.01%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 84.29%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 84.55%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 84.97%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 85.03%   [EVAL] batch:   38 | acc: 93.75%,  total acc: 85.26%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 85.62%   [EVAL] batch:   40 | acc: 81.25%,  total acc: 85.52%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 85.86%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 86.05%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 86.36%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 86.67%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 86.96%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 87.23%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 87.76%   [EVAL] batch:   49 | acc: 62.50%,  total acc: 87.25%   
cur_acc:  ['0.8617', '0.9236']
his_acc:  ['0.8617', '0.8725']
CurrentTrain: epoch  0, batch     0 | loss: 5.6610146CurrentTrain: epoch  0, batch     1 | loss: 6.2104735CurrentTrain: epoch  1, batch     0 | loss: 4.0906391CurrentTrain: epoch  1, batch     1 | loss: 5.0933795CurrentTrain: epoch  2, batch     0 | loss: 4.7895484CurrentTrain: epoch  2, batch     1 | loss: 2.8981252CurrentTrain: epoch  3, batch     0 | loss: 3.6867385CurrentTrain: epoch  3, batch     1 | loss: 3.4875119CurrentTrain: epoch  4, batch     0 | loss: 3.7902277CurrentTrain: epoch  4, batch     1 | loss: 2.7811465CurrentTrain: epoch  5, batch     0 | loss: 2.9108853CurrentTrain: epoch  5, batch     1 | loss: 3.9597125CurrentTrain: epoch  6, batch     0 | loss: 3.1610026CurrentTrain: epoch  6, batch     1 | loss: 3.2147400CurrentTrain: epoch  7, batch     0 | loss: 2.9184508CurrentTrain: epoch  7, batch     1 | loss: 3.1925750CurrentTrain: epoch  8, batch     0 | loss: 2.7467942CurrentTrain: epoch  8, batch     1 | loss: 3.6394870CurrentTrain: epoch  9, batch     0 | loss: 3.3250351CurrentTrain: epoch  9, batch     1 | loss: 2.4332962
#### person country of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person country of birth" means a person was born in a country.
Here is an example:
Relation: person country of birth
Context: in testimony by satellite link from germany to a house of representatives ' panel , murat kurnaz recounted his five-year detention , alleging a wide range of torture and abuse .
Head Entity: murat kurnaz
Tail Entity: germany
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person country of birth":
Relation: person country of birth  
Context: During the interview, the renowned artist shared his experiences growing up in the vibrant streets of Barcelona, emphasizing how the culture influenced his work.  
Head Entity: the renowned artist  
Tail Entity: Barcelona  

Relation: person country of birth  
Context: In her autobiography, the famous scientist detailed her early life in the lush landscapes of New Zealand, which sparked her interest in environmental studies.  
Head Entity: the famous scientist  
Tail Entity: New Zealand  
#### organization website ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization website" means an organization 's website is an url.
Here is an example:
Relation: organization website
Context: barnes foundation : http://www.barnesfoundation.org
Head Entity: barnes foundation
Tail Entity: http://www.barnesfoundation.org
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization website":
Relation: organization website  
Context: national geographic society : https://www.nationalgeographic.com  
Head Entity: national geographic society  
Tail Entity: https://www.nationalgeographic.com  

Relation: organization website  
Context: world wildlife fund : https://www.worldwildlife.org  
Head Entity: world wildlife fund  
Tail Entity: https://www.worldwildlife.org  
#### organization shareholders ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization shareholders" means an organization was invested by a person.
Here is an example:
Relation: organization shareholders
Context: marsans already owns the madrid-based air comet and 95 percent of aerolineas argentinas .
Head Entity: aerolineas argentinas
Tail Entity: marsans
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization shareholders":
Relation: organization shareholders  
Context: tech giant apple inc. has recently acquired a significant stake in the innovative startup nextdoor.  
Head Entity: nextdoor  
Tail Entity: apple inc.  

Relation: organization shareholders  
Context: the investment firm blackrock has increased its holdings in the renewable energy company sunrun.  
Head Entity: sunrun  
Tail Entity: blackrock  
#### organization dissolved ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization dissolved" means an organization was dissolved in a date.
Here is an example:
Relation: organization dissolved
Context: the tse suffered its worst-ever system crash in november 2005 which paralyzed the world 's second largest bourse and forced it to shelve plan for a listing of its own .
Head Entity: tse
Tail Entity: november 2005
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization dissolved":
Relation: organization dissolved  
Context: the once-prominent tech startup, Innovatech, officially ceased operations in early 2020 after struggling to secure funding.  
Head Entity: Innovatech  
Tail Entity: early 2020  

Relation: organization dissolved  
Context: after years of financial difficulties, the local arts council announced its closure in the spring of 2018, leaving many artists without support.  
Head Entity: local arts council  
Tail Entity: spring 2018  
#### organization founded by ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization founded by" means an organization was found by a person.
Here is an example:
Relation: organization founded by
Context: `` i have no greater obligation than to ensure the safely of airline travelers in this country , '' transportation secretary ray lahood said in a joint statement with j. randolph babbitt , administrator of the federal aviation administration , that was issued on the eve of a senate hearing on aviation safety .
Head Entity: federal aviation administration
Tail Entity: j. randolph babbitt
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization founded by":
Relation: organization founded by  
Context: In 1975, the renowned physicist and entrepreneur, Dr. Jane Smith, established Quantum Innovations, a company dedicated to advancing quantum computing technologies.  
Head Entity: Quantum Innovations  
Tail Entity: Dr. Jane Smith  

Relation: organization founded by  
Context: The charity organization, Hope for Tomorrow, was created in 2010 by the well-known philanthropist, Michael Johnson, to support underprivileged children.  
Head Entity: Hope for Tomorrow  
Tail Entity: Michael Johnson  
Mixup data size:  2455
MixupTrain:  epoch  0, batch     0 | loss: 5.3828125MixupTrain:  epoch  0, batch     1 | loss: 5.2908654MixupTrain:  epoch  0, batch     2 | loss: 5.3024912MixupTrain:  epoch  0, batch     3 | loss: 5.2151713MixupTrain:  epoch  0, batch     4 | loss: 5.2482786MixupTrain:  epoch  0, batch     5 | loss: 4.6409054MixupTrain:  epoch  0, batch     6 | loss: 4.7807608MixupTrain:  epoch  0, batch     7 | loss: 5.4591351MixupTrain:  epoch  0, batch     8 | loss: 5.1789818MixupTrain:  epoch  0, batch     9 | loss: 4.9987488MixupTrain:  epoch  0, batch    10 | loss: 5.2134628MixupTrain:  epoch  0, batch    11 | loss: 4.6744933MixupTrain:  epoch  0, batch    12 | loss: 4.5557556MixupTrain:  epoch  0, batch    13 | loss: 4.5472608MixupTrain:  epoch  0, batch    14 | loss: 4.5484476MixupTrain:  epoch  0, batch    15 | loss: 4.5548306MixupTrain:  epoch  0, batch    16 | loss: 4.3764439MixupTrain:  epoch  0, batch    17 | loss: 4.6749597MixupTrain:  epoch  0, batch    18 | loss: 4.4813824MixupTrain:  epoch  0, batch    19 | loss: 4.5352249MixupTrain:  epoch  0, batch    20 | loss: 4.2089329MixupTrain:  epoch  0, batch    21 | loss: 4.2953062MixupTrain:  epoch  0, batch    22 | loss: 4.2902298MixupTrain:  epoch  0, batch    23 | loss: 4.3748875MixupTrain:  epoch  0, batch    24 | loss: 4.3317795MixupTrain:  epoch  0, batch    25 | loss: 4.2661448MixupTrain:  epoch  0, batch    26 | loss: 4.7036295MixupTrain:  epoch  0, batch    27 | loss: 4.4836197MixupTrain:  epoch  0, batch    28 | loss: 4.8100986MixupTrain:  epoch  0, batch    29 | loss: 4.4629717MixupTrain:  epoch  0, batch    30 | loss: 4.3857632MixupTrain:  epoch  0, batch    31 | loss: 4.3571978MixupTrain:  epoch  0, batch    32 | loss: 4.3135490MixupTrain:  epoch  0, batch    33 | loss: 4.3271289MixupTrain:  epoch  0, batch    34 | loss: 4.1681948MixupTrain:  epoch  0, batch    35 | loss: 4.6254067MixupTrain:  epoch  0, batch    36 | loss: 4.0834775MixupTrain:  epoch  0, batch    37 | loss: 4.1000500MixupTrain:  epoch  0, batch    38 | loss: 4.2372742MixupTrain:  epoch  0, batch    39 | loss: 4.3782158MixupTrain:  epoch  0, batch    40 | loss: 3.6848540MixupTrain:  epoch  0, batch    41 | loss: 4.2473278MixupTrain:  epoch  0, batch    42 | loss: 4.3073187MixupTrain:  epoch  0, batch    43 | loss: 4.1756868MixupTrain:  epoch  0, batch    44 | loss: 4.3564239MixupTrain:  epoch  0, batch    45 | loss: 4.5103464MixupTrain:  epoch  0, batch    46 | loss: 4.4348249MixupTrain:  epoch  0, batch    47 | loss: 3.7183867MixupTrain:  epoch  0, batch    48 | loss: 4.1761265MixupTrain:  epoch  0, batch    49 | loss: 4.2527423MixupTrain:  epoch  0, batch    50 | loss: 4.3435888MixupTrain:  epoch  0, batch    51 | loss: 4.0183744MixupTrain:  epoch  0, batch    52 | loss: 4.0788598MixupTrain:  epoch  0, batch    53 | loss: 4.4064522MixupTrain:  epoch  0, batch    54 | loss: 4.4402657MixupTrain:  epoch  0, batch    55 | loss: 4.1068134MixupTrain:  epoch  0, batch    56 | loss: 4.2756734MixupTrain:  epoch  0, batch    57 | loss: 4.2774162MixupTrain:  epoch  0, batch    58 | loss: 4.2253766MixupTrain:  epoch  0, batch    59 | loss: 4.3495874MixupTrain:  epoch  0, batch    60 | loss: 4.2053556MixupTrain:  epoch  0, batch    61 | loss: 4.0306692MixupTrain:  epoch  0, batch    62 | loss: 3.9962122MixupTrain:  epoch  0, batch    63 | loss: 3.8445706MixupTrain:  epoch  0, batch    64 | loss: 4.1589870MixupTrain:  epoch  0, batch    65 | loss: 3.8752656MixupTrain:  epoch  0, batch    66 | loss: 4.2453465MixupTrain:  epoch  0, batch    67 | loss: 3.9079969MixupTrain:  epoch  0, batch    68 | loss: 4.1661510MixupTrain:  epoch  0, batch    69 | loss: 3.9700665MixupTrain:  epoch  0, batch    70 | loss: 3.6316566MixupTrain:  epoch  0, batch    71 | loss: 4.0250344MixupTrain:  epoch  0, batch    72 | loss: 4.0343103MixupTrain:  epoch  0, batch    73 | loss: 3.7755616MixupTrain:  epoch  0, batch    74 | loss: 4.2062645MixupTrain:  epoch  0, batch    75 | loss: 3.8596940MixupTrain:  epoch  0, batch    76 | loss: 4.1452599MixupTrain:  epoch  0, batch    77 | loss: 3.7759984MixupTrain:  epoch  0, batch    78 | loss: 3.6954813MixupTrain:  epoch  0, batch    79 | loss: 4.0366025MixupTrain:  epoch  0, batch    80 | loss: 3.9838512MixupTrain:  epoch  0, batch    81 | loss: 3.9589739MixupTrain:  epoch  0, batch    82 | loss: 3.8193502MixupTrain:  epoch  0, batch    83 | loss: 3.8470399MixupTrain:  epoch  0, batch    84 | loss: 4.0936627MixupTrain:  epoch  0, batch    85 | loss: 3.8647556MixupTrain:  epoch  0, batch    86 | loss: 3.7348363MixupTrain:  epoch  0, batch    87 | loss: 3.4819121MixupTrain:  epoch  0, batch    88 | loss: 3.5597839MixupTrain:  epoch  0, batch    89 | loss: 3.9301915MixupTrain:  epoch  0, batch    90 | loss: 4.0531678MixupTrain:  epoch  0, batch    91 | loss: 3.8139253MixupTrain:  epoch  0, batch    92 | loss: 3.6899445MixupTrain:  epoch  0, batch    93 | loss: 3.6714120MixupTrain:  epoch  0, batch    94 | loss: 3.8932695MixupTrain:  epoch  0, batch    95 | loss: 3.7715416MixupTrain:  epoch  0, batch    96 | loss: 4.0041895MixupTrain:  epoch  0, batch    97 | loss: 3.8478997MixupTrain:  epoch  0, batch    98 | loss: 3.6342859MixupTrain:  epoch  0, batch    99 | loss: 3.9396534MixupTrain:  epoch  0, batch   100 | loss: 3.5048540MixupTrain:  epoch  0, batch   101 | loss: 3.5773034MixupTrain:  epoch  0, batch   102 | loss: 3.8472672MixupTrain:  epoch  0, batch   103 | loss: 3.8969448MixupTrain:  epoch  0, batch   104 | loss: 4.0259733MixupTrain:  epoch  0, batch   105 | loss: 3.7652826MixupTrain:  epoch  0, batch   106 | loss: 3.7972729MixupTrain:  epoch  0, batch   107 | loss: 3.6431556MixupTrain:  epoch  0, batch   108 | loss: 3.6944888MixupTrain:  epoch  0, batch   109 | loss: 3.8336203MixupTrain:  epoch  0, batch   110 | loss: 3.6919796MixupTrain:  epoch  0, batch   111 | loss: 4.0768132MixupTrain:  epoch  0, batch   112 | loss: 3.8996038MixupTrain:  epoch  0, batch   113 | loss: 3.8202457MixupTrain:  epoch  0, batch   114 | loss: 3.6548755MixupTrain:  epoch  0, batch   115 | loss: 3.5897703MixupTrain:  epoch  0, batch   116 | loss: 3.7619216MixupTrain:  epoch  0, batch   117 | loss: 3.9549384MixupTrain:  epoch  0, batch   118 | loss: 3.8084064MixupTrain:  epoch  0, batch   119 | loss: 3.8231673MixupTrain:  epoch  0, batch   120 | loss: 3.8957324MixupTrain:  epoch  0, batch   121 | loss: 3.7301469MixupTrain:  epoch  0, batch   122 | loss: 3.3587360MixupTrain:  epoch  0, batch   123 | loss: 3.6485448MixupTrain:  epoch  0, batch   124 | loss: 3.5116892MixupTrain:  epoch  0, batch   125 | loss: 3.5939071MixupTrain:  epoch  0, batch   126 | loss: 3.4924083MixupTrain:  epoch  0, batch   127 | loss: 3.7834997MixupTrain:  epoch  0, batch   128 | loss: 3.8031731MixupTrain:  epoch  0, batch   129 | loss: 3.6553402MixupTrain:  epoch  0, batch   130 | loss: 3.6630592MixupTrain:  epoch  0, batch   131 | loss: 3.7243080MixupTrain:  epoch  0, batch   132 | loss: 3.8234110MixupTrain:  epoch  0, batch   133 | loss: 3.6855590MixupTrain:  epoch  0, batch   134 | loss: 3.7238491MixupTrain:  epoch  0, batch   135 | loss: 3.7741671MixupTrain:  epoch  0, batch   136 | loss: 3.7301674MixupTrain:  epoch  0, batch   137 | loss: 3.5910575MixupTrain:  epoch  0, batch   138 | loss: 3.7344484MixupTrain:  epoch  0, batch   139 | loss: 3.8643327MixupTrain:  epoch  0, batch   140 | loss: 3.7164974MixupTrain:  epoch  0, batch   141 | loss: 3.6123490MixupTrain:  epoch  0, batch   142 | loss: 3.5578058MixupTrain:  epoch  0, batch   143 | loss: 3.3276751MixupTrain:  epoch  0, batch   144 | loss: 3.5873773MixupTrain:  epoch  0, batch   145 | loss: 3.6648397MixupTrain:  epoch  0, batch   146 | loss: 3.5597515MixupTrain:  epoch  0, batch   147 | loss: 3.9472761MixupTrain:  epoch  0, batch   148 | loss: 3.5096853MixupTrain:  epoch  0, batch   149 | loss: 3.7866859MixupTrain:  epoch  0, batch   150 | loss: 3.6878231MixupTrain:  epoch  0, batch   151 | loss: 3.5795836MixupTrain:  epoch  0, batch   152 | loss: 3.7544632MixupTrain:  epoch  0, batch   153 | loss: 3.8726974
MemoryTrain:  epoch  0, batch     0 | loss: 2.2252188MemoryTrain:  epoch  0, batch     1 | loss: 3.1979771MemoryTrain:  epoch  0, batch     2 | loss: 3.7750969MemoryTrain:  epoch  1, batch     0 | loss: 2.8948412MemoryTrain:  epoch  1, batch     1 | loss: 2.3791933MemoryTrain:  epoch  1, batch     2 | loss: 2.6632464MemoryTrain:  epoch  2, batch     0 | loss: 2.1602154MemoryTrain:  epoch  2, batch     1 | loss: 2.5600193MemoryTrain:  epoch  2, batch     2 | loss: 1.9311218MemoryTrain:  epoch  3, batch     0 | loss: 1.9247965MemoryTrain:  epoch  3, batch     1 | loss: 1.9009181MemoryTrain:  epoch  3, batch     2 | loss: 2.0671101MemoryTrain:  epoch  4, batch     0 | loss: 1.9124627MemoryTrain:  epoch  4, batch     1 | loss: 1.5450443MemoryTrain:  epoch  4, batch     2 | loss: 1.9522063MemoryTrain:  epoch  5, batch     0 | loss: 1.8127134MemoryTrain:  epoch  5, batch     1 | loss: 1.9462862MemoryTrain:  epoch  5, batch     2 | loss: 1.6417185MemoryTrain:  epoch  6, batch     0 | loss: 1.6363310MemoryTrain:  epoch  6, batch     1 | loss: 1.6570718MemoryTrain:  epoch  6, batch     2 | loss: 1.8165731MemoryTrain:  epoch  7, batch     0 | loss: 1.5868158MemoryTrain:  epoch  7, batch     1 | loss: 1.5913082MemoryTrain:  epoch  7, batch     2 | loss: 1.5769560MemoryTrain:  epoch  8, batch     0 | loss: 1.7048714MemoryTrain:  epoch  8, batch     1 | loss: 1.5470594MemoryTrain:  epoch  8, batch     2 | loss: 1.3990099MemoryTrain:  epoch  9, batch     0 | loss: 1.5901916MemoryTrain:  epoch  9, batch     1 | loss: 1.3785795MemoryTrain:  epoch  9, batch     2 | loss: 1.3694749
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 91.67%   [EVAL] batch:    3 | acc: 93.75%,  total acc: 92.19%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 90.00%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 91.07%   [EVAL] batch:    7 | acc: 12.50%,  total acc: 81.25%   
[EVAL] batch:    0 | acc: 43.75%,  total acc: 43.75%   [EVAL] batch:    1 | acc: 31.25%,  total acc: 37.50%   [EVAL] batch:    2 | acc: 43.75%,  total acc: 39.58%   [EVAL] batch:    3 | acc: 37.50%,  total acc: 39.06%   [EVAL] batch:    4 | acc: 43.75%,  total acc: 40.00%   [EVAL] batch:    5 | acc: 37.50%,  total acc: 39.58%   [EVAL] batch:    6 | acc: 18.75%,  total acc: 36.61%   [EVAL] batch:    7 | acc: 6.25%,  total acc: 32.81%   [EVAL] batch:    8 | acc: 6.25%,  total acc: 29.86%   [EVAL] batch:    9 | acc: 18.75%,  total acc: 28.75%   [EVAL] batch:   10 | acc: 18.75%,  total acc: 27.84%   [EVAL] batch:   11 | acc: 0.00%,  total acc: 25.52%   [EVAL] batch:   12 | acc: 18.75%,  total acc: 25.00%   [EVAL] batch:   13 | acc: 37.50%,  total acc: 25.89%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 29.17%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 30.86%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 33.46%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 35.07%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 36.51%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 39.06%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 41.96%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 44.60%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 46.74%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 48.70%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 50.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 52.64%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 54.17%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 55.80%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 57.33%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 58.33%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 59.68%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 60.94%   [EVAL] batch:   32 | acc: 81.25%,  total acc: 61.55%   [EVAL] batch:   33 | acc: 75.00%,  total acc: 61.95%   [EVAL] batch:   34 | acc: 56.25%,  total acc: 61.79%   [EVAL] batch:   35 | acc: 75.00%,  total acc: 62.15%   [EVAL] batch:   36 | acc: 62.50%,  total acc: 62.16%   [EVAL] batch:   37 | acc: 81.25%,  total acc: 62.66%   [EVAL] batch:   38 | acc: 81.25%,  total acc: 63.14%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 64.06%   [EVAL] batch:   40 | acc: 50.00%,  total acc: 63.72%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 64.43%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 65.12%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 65.91%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 66.67%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 67.39%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 68.09%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 68.75%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 69.39%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 69.75%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 70.34%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 70.79%   [EVAL] batch:   52 | acc: 87.50%,  total acc: 71.11%   [EVAL] batch:   53 | acc: 93.75%,  total acc: 71.53%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 71.70%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 72.21%   [EVAL] batch:   56 | acc: 68.75%,  total acc: 72.15%   
cur_acc:  ['0.8617', '0.9236', '0.8125']
his_acc:  ['0.8617', '0.8725', '0.7215']
CurrentTrain: epoch  0, batch     0 | loss: 4.9802065CurrentTrain: epoch  0, batch     1 | loss: 5.3367252CurrentTrain: epoch  1, batch     0 | loss: 3.7555110CurrentTrain: epoch  1, batch     1 | loss: 4.2130270CurrentTrain: epoch  2, batch     0 | loss: 3.4996700CurrentTrain: epoch  2, batch     1 | loss: 3.1955473CurrentTrain: epoch  3, batch     0 | loss: 2.8357565CurrentTrain: epoch  3, batch     1 | loss: 3.0132000CurrentTrain: epoch  4, batch     0 | loss: 2.8870826CurrentTrain: epoch  4, batch     1 | loss: 2.4684260CurrentTrain: epoch  5, batch     0 | loss: 2.4852881CurrentTrain: epoch  5, batch     1 | loss: 2.8659213CurrentTrain: epoch  6, batch     0 | loss: 2.4593141CurrentTrain: epoch  6, batch     1 | loss: 2.4178009CurrentTrain: epoch  7, batch     0 | loss: 2.2607226CurrentTrain: epoch  7, batch     1 | loss: 2.1526864CurrentTrain: epoch  8, batch     0 | loss: 2.1130941CurrentTrain: epoch  8, batch     1 | loss: 2.1522462CurrentTrain: epoch  9, batch     0 | loss: 2.0766201CurrentTrain: epoch  9, batch     1 | loss: 1.9987921
#### organization founded ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization founded" means an organization was found in a date.
Here is an example:
Relation: organization founded
Context: the jnf was founded in 1901 to buy plots in palestine , then ruled by the ottomans .
Head Entity: jnf
Tail Entity: 1901
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization founded":
Relation: organization founded  
Context: the united nations was established in 1945 to promote international cooperation and peace.  
Head Entity: united nations  
Tail Entity: 1945  

Relation: organization founded  
Context: the world health organization was created in 1948 to address global health issues.  
Head Entity: world health organization  
Tail Entity: 1948  
#### person age ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person age" means a person 's age is a number.
Here is an example:
Relation: person age
Context: virginia republican jo ann davis passed away on saturday at the age of 57 .
Head Entity: jo ann davis
Tail Entity: 57
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person age":
Relation: person age  
Context: the famous actor robert downey jr. celebrated his 56th birthday last week.  
Head Entity: robert downey jr.  
Tail Entity: 56  

Relation: person age  
Context: on her 30th birthday, singer taylor swift announced her new album release.  
Head Entity: taylor swift  
Tail Entity: 30  
#### person city of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person city of birth" means a person was born in a city.
Here is an example:
Relation: person city of birth
Context: rothman was born in san francisco in 1932 in an orthodox jewish family .
Head Entity: rothman
Tail Entity: san francisco
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person city of birth":
Relation: person city of birth  
Context: after spending his early years in new york, he moved to los angeles where he began his career.  
Head Entity: he  
Tail Entity: los angeles  

Relation: person city of birth  
Context: the famous author was born in a small town near boston, which greatly influenced his writing.  
Head Entity: the famous author  
Tail Entity: boston  
#### organization members ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization members" means an organization 's member is an organization.
Here is an example:
Relation: organization members
Context: it was berger who made clarke a member of the white house principals committee when it met to discuss terrorist threats , allowing an otherwise middle-ranking nsc bureaucrat to treat tenet and secretary of state madeleine albright as equals -lrb- which the empire-building clarke was pleased to do -rrb- .
Head Entity: nsc
Tail Entity: white house principals committee
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization members":
Relation: organization members  
Context: The board of directors of the tech startup includes several prominent figures from the industry, such as the CEO of Innovatech, who has been instrumental in guiding the company’s strategic direction.  
Head Entity: tech startup  
Tail Entity: Innovatech  

Relation: organization members  
Context: During the annual conference, the president of the environmental advocacy group announced the inclusion of several new organizations, highlighting the partnership with Green Future, which focuses on sustainable practices.  
Head Entity: environmental advocacy group  
Tail Entity: Green Future  
#### person religion ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person religion" means an person is the member of a religion.
Here is an example:
Relation: person religion
Context: though not a household name , wildmon has considerable clout ; his group has a vast mailing list and a proven ability to mobilize christian conservatives by the hundreds of thousands .
Head Entity: his
Tail Entity: christian
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person religion":
Relation: person religion  
Context: The community center hosted an event where many attendees shared their experiences of faith, and Sarah, a devoted member of the local mosque, spoke passionately about her beliefs.  
Head Entity: Sarah  
Tail Entity: mosque  

Relation: person religion  
Context: During the interfaith dialogue, John expressed his views on spirituality, highlighting how his upbringing in a Jewish family shaped his understanding of the world.  
Head Entity: John  
Tail Entity: Jewish  
Mixup data size:  3640
MixupTrain:  epoch  0, batch     0 | loss: 4.3956857MixupTrain:  epoch  0, batch     1 | loss: 4.2271118MixupTrain:  epoch  0, batch     2 | loss: 4.4156413MixupTrain:  epoch  0, batch     3 | loss: 3.6822472MixupTrain:  epoch  0, batch     4 | loss: 3.8052042MixupTrain:  epoch  0, batch     5 | loss: 3.9847178MixupTrain:  epoch  0, batch     6 | loss: 4.5215726MixupTrain:  epoch  0, batch     7 | loss: 4.0676661MixupTrain:  epoch  0, batch     8 | loss: 3.8243165MixupTrain:  epoch  0, batch     9 | loss: 3.8350697MixupTrain:  epoch  0, batch    10 | loss: 3.9315374MixupTrain:  epoch  0, batch    11 | loss: 3.8397870MixupTrain:  epoch  0, batch    12 | loss: 3.8586049MixupTrain:  epoch  0, batch    13 | loss: 3.7611685MixupTrain:  epoch  0, batch    14 | loss: 3.9162250MixupTrain:  epoch  0, batch    15 | loss: 3.3904376MixupTrain:  epoch  0, batch    16 | loss: 3.9169681MixupTrain:  epoch  0, batch    17 | loss: 3.5102954MixupTrain:  epoch  0, batch    18 | loss: 3.6951170MixupTrain:  epoch  0, batch    19 | loss: 3.9963975MixupTrain:  epoch  0, batch    20 | loss: 4.0166578MixupTrain:  epoch  0, batch    21 | loss: 3.8101709MixupTrain:  epoch  0, batch    22 | loss: 3.7449191MixupTrain:  epoch  0, batch    23 | loss: 3.6563120MixupTrain:  epoch  0, batch    24 | loss: 3.6023579MixupTrain:  epoch  0, batch    25 | loss: 3.3235390MixupTrain:  epoch  0, batch    26 | loss: 3.2829232MixupTrain:  epoch  0, batch    27 | loss: 3.7749445MixupTrain:  epoch  0, batch    28 | loss: 3.5178041MixupTrain:  epoch  0, batch    29 | loss: 3.4661107MixupTrain:  epoch  0, batch    30 | loss: 3.4402370MixupTrain:  epoch  0, batch    31 | loss: 3.4745579MixupTrain:  epoch  0, batch    32 | loss: 3.0839860MixupTrain:  epoch  0, batch    33 | loss: 3.2834654MixupTrain:  epoch  0, batch    34 | loss: 3.6455383MixupTrain:  epoch  0, batch    35 | loss: 3.7034609MixupTrain:  epoch  0, batch    36 | loss: 3.4439766MixupTrain:  epoch  0, batch    37 | loss: 3.6632943MixupTrain:  epoch  0, batch    38 | loss: 3.3243346MixupTrain:  epoch  0, batch    39 | loss: 3.4296048MixupTrain:  epoch  0, batch    40 | loss: 3.3965845MixupTrain:  epoch  0, batch    41 | loss: 3.6491928MixupTrain:  epoch  0, batch    42 | loss: 3.6533442MixupTrain:  epoch  0, batch    43 | loss: 3.7238793MixupTrain:  epoch  0, batch    44 | loss: 3.5237498MixupTrain:  epoch  0, batch    45 | loss: 3.2488382MixupTrain:  epoch  0, batch    46 | loss: 3.3114588MixupTrain:  epoch  0, batch    47 | loss: 3.1621318MixupTrain:  epoch  0, batch    48 | loss: 3.1278534MixupTrain:  epoch  0, batch    49 | loss: 3.3259573MixupTrain:  epoch  0, batch    50 | loss: 3.8566544MixupTrain:  epoch  0, batch    51 | loss: 3.3682714MixupTrain:  epoch  0, batch    52 | loss: 3.4277420MixupTrain:  epoch  0, batch    53 | loss: 3.6378908MixupTrain:  epoch  0, batch    54 | loss: 3.2903652MixupTrain:  epoch  0, batch    55 | loss: 3.5668545MixupTrain:  epoch  0, batch    56 | loss: 3.5101137MixupTrain:  epoch  0, batch    57 | loss: 3.6746116MixupTrain:  epoch  0, batch    58 | loss: 3.5180016MixupTrain:  epoch  0, batch    59 | loss: 3.3592887MixupTrain:  epoch  0, batch    60 | loss: 3.4842710MixupTrain:  epoch  0, batch    61 | loss: 3.1953311MixupTrain:  epoch  0, batch    62 | loss: 3.0553434MixupTrain:  epoch  0, batch    63 | loss: 3.2219019MixupTrain:  epoch  0, batch    64 | loss: 2.8855734MixupTrain:  epoch  0, batch    65 | loss: 3.1648705MixupTrain:  epoch  0, batch    66 | loss: 3.4133921MixupTrain:  epoch  0, batch    67 | loss: 3.3166633MixupTrain:  epoch  0, batch    68 | loss: 3.9262829MixupTrain:  epoch  0, batch    69 | loss: 3.6512878MixupTrain:  epoch  0, batch    70 | loss: 3.5834410MixupTrain:  epoch  0, batch    71 | loss: 3.3031113MixupTrain:  epoch  0, batch    72 | loss: 3.3724058MixupTrain:  epoch  0, batch    73 | loss: 3.1445661MixupTrain:  epoch  0, batch    74 | loss: 3.8274977MixupTrain:  epoch  0, batch    75 | loss: 3.4634073MixupTrain:  epoch  0, batch    76 | loss: 3.3328087MixupTrain:  epoch  0, batch    77 | loss: 3.4348602MixupTrain:  epoch  0, batch    78 | loss: 3.5429497MixupTrain:  epoch  0, batch    79 | loss: 3.3329163MixupTrain:  epoch  0, batch    80 | loss: 2.9645896MixupTrain:  epoch  0, batch    81 | loss: 3.3555522MixupTrain:  epoch  0, batch    82 | loss: 3.1175444MixupTrain:  epoch  0, batch    83 | loss: 3.3571706MixupTrain:  epoch  0, batch    84 | loss: 3.3619466MixupTrain:  epoch  0, batch    85 | loss: 3.2952302MixupTrain:  epoch  0, batch    86 | loss: 3.3299003MixupTrain:  epoch  0, batch    87 | loss: 3.3077457MixupTrain:  epoch  0, batch    88 | loss: 3.0037699MixupTrain:  epoch  0, batch    89 | loss: 3.1879034MixupTrain:  epoch  0, batch    90 | loss: 3.1901517MixupTrain:  epoch  0, batch    91 | loss: 3.4104695MixupTrain:  epoch  0, batch    92 | loss: 3.1855226MixupTrain:  epoch  0, batch    93 | loss: 3.2195053MixupTrain:  epoch  0, batch    94 | loss: 3.5211053MixupTrain:  epoch  0, batch    95 | loss: 3.2072380MixupTrain:  epoch  0, batch    96 | loss: 2.8669014MixupTrain:  epoch  0, batch    97 | loss: 3.1409140MixupTrain:  epoch  0, batch    98 | loss: 3.1991324MixupTrain:  epoch  0, batch    99 | loss: 3.0468633MixupTrain:  epoch  0, batch   100 | loss: 3.1560514MixupTrain:  epoch  0, batch   101 | loss: 3.2039466MixupTrain:  epoch  0, batch   102 | loss: 3.0501893MixupTrain:  epoch  0, batch   103 | loss: 3.1283259MixupTrain:  epoch  0, batch   104 | loss: 3.0540576MixupTrain:  epoch  0, batch   105 | loss: 3.3122888MixupTrain:  epoch  0, batch   106 | loss: 3.4753644MixupTrain:  epoch  0, batch   107 | loss: 3.3547487MixupTrain:  epoch  0, batch   108 | loss: 3.0410950MixupTrain:  epoch  0, batch   109 | loss: 3.1196835MixupTrain:  epoch  0, batch   110 | loss: 2.9861257MixupTrain:  epoch  0, batch   111 | loss: 3.0313177MixupTrain:  epoch  0, batch   112 | loss: 3.1232598MixupTrain:  epoch  0, batch   113 | loss: 3.0700226MixupTrain:  epoch  0, batch   114 | loss: 3.1164398MixupTrain:  epoch  0, batch   115 | loss: 3.3226964MixupTrain:  epoch  0, batch   116 | loss: 2.9491730MixupTrain:  epoch  0, batch   117 | loss: 3.2326481MixupTrain:  epoch  0, batch   118 | loss: 3.0478683MixupTrain:  epoch  0, batch   119 | loss: 2.9239883MixupTrain:  epoch  0, batch   120 | loss: 3.1930916MixupTrain:  epoch  0, batch   121 | loss: 3.1235726MixupTrain:  epoch  0, batch   122 | loss: 3.0509195MixupTrain:  epoch  0, batch   123 | loss: 3.1581039MixupTrain:  epoch  0, batch   124 | loss: 2.9340420MixupTrain:  epoch  0, batch   125 | loss: 2.9024167MixupTrain:  epoch  0, batch   126 | loss: 3.0661042MixupTrain:  epoch  0, batch   127 | loss: 3.0446594MixupTrain:  epoch  0, batch   128 | loss: 3.0725863MixupTrain:  epoch  0, batch   129 | loss: 3.1239581MixupTrain:  epoch  0, batch   130 | loss: 3.1434131MixupTrain:  epoch  0, batch   131 | loss: 3.0587230MixupTrain:  epoch  0, batch   132 | loss: 2.9228950MixupTrain:  epoch  0, batch   133 | loss: 2.9641063MixupTrain:  epoch  0, batch   134 | loss: 3.1547463MixupTrain:  epoch  0, batch   135 | loss: 3.1496751MixupTrain:  epoch  0, batch   136 | loss: 3.0680771MixupTrain:  epoch  0, batch   137 | loss: 3.0898333MixupTrain:  epoch  0, batch   138 | loss: 3.0756261MixupTrain:  epoch  0, batch   139 | loss: 3.1917129MixupTrain:  epoch  0, batch   140 | loss: 2.9225161MixupTrain:  epoch  0, batch   141 | loss: 2.8931723MixupTrain:  epoch  0, batch   142 | loss: 3.0829301MixupTrain:  epoch  0, batch   143 | loss: 2.9532356MixupTrain:  epoch  0, batch   144 | loss: 3.2115793MixupTrain:  epoch  0, batch   145 | loss: 2.8981042MixupTrain:  epoch  0, batch   146 | loss: 3.1070442MixupTrain:  epoch  0, batch   147 | loss: 3.1180892MixupTrain:  epoch  0, batch   148 | loss: 3.0620399MixupTrain:  epoch  0, batch   149 | loss: 3.1700940MixupTrain:  epoch  0, batch   150 | loss: 3.1145291MixupTrain:  epoch  0, batch   151 | loss: 3.1612945MixupTrain:  epoch  0, batch   152 | loss: 3.0289159MixupTrain:  epoch  0, batch   153 | loss: 3.1909008MixupTrain:  epoch  0, batch   154 | loss: 3.1495659MixupTrain:  epoch  0, batch   155 | loss: 2.9486372MixupTrain:  epoch  0, batch   156 | loss: 3.1589670MixupTrain:  epoch  0, batch   157 | loss: 3.0691228MixupTrain:  epoch  0, batch   158 | loss: 3.0378580MixupTrain:  epoch  0, batch   159 | loss: 3.1368225MixupTrain:  epoch  0, batch   160 | loss: 2.6990163MixupTrain:  epoch  0, batch   161 | loss: 3.3512511MixupTrain:  epoch  0, batch   162 | loss: 3.1880591MixupTrain:  epoch  0, batch   163 | loss: 2.9708347MixupTrain:  epoch  0, batch   164 | loss: 3.1318254MixupTrain:  epoch  0, batch   165 | loss: 3.1971807MixupTrain:  epoch  0, batch   166 | loss: 2.8404927MixupTrain:  epoch  0, batch   167 | loss: 3.2340598MixupTrain:  epoch  0, batch   168 | loss: 2.9013784MixupTrain:  epoch  0, batch   169 | loss: 2.9041991MixupTrain:  epoch  0, batch   170 | loss: 3.0519886MixupTrain:  epoch  0, batch   171 | loss: 3.0342844MixupTrain:  epoch  0, batch   172 | loss: 3.0636675MixupTrain:  epoch  0, batch   173 | loss: 3.0979297MixupTrain:  epoch  0, batch   174 | loss: 2.9080083MixupTrain:  epoch  0, batch   175 | loss: 2.9415374MixupTrain:  epoch  0, batch   176 | loss: 2.8691778MixupTrain:  epoch  0, batch   177 | loss: 2.9919841MixupTrain:  epoch  0, batch   178 | loss: 3.1662283MixupTrain:  epoch  0, batch   179 | loss: 3.0326753MixupTrain:  epoch  0, batch   180 | loss: 2.9500833MixupTrain:  epoch  0, batch   181 | loss: 3.1017344MixupTrain:  epoch  0, batch   182 | loss: 2.9412947MixupTrain:  epoch  0, batch   183 | loss: 2.6945882MixupTrain:  epoch  0, batch   184 | loss: 2.9587123MixupTrain:  epoch  0, batch   185 | loss: 3.1866274MixupTrain:  epoch  0, batch   186 | loss: 3.0959151MixupTrain:  epoch  0, batch   187 | loss: 3.2525313MixupTrain:  epoch  0, batch   188 | loss: 2.9536343MixupTrain:  epoch  0, batch   189 | loss: 3.0234945MixupTrain:  epoch  0, batch   190 | loss: 2.9988146MixupTrain:  epoch  0, batch   191 | loss: 3.0696754MixupTrain:  epoch  0, batch   192 | loss: 2.9607048MixupTrain:  epoch  0, batch   193 | loss: 2.9290233MixupTrain:  epoch  0, batch   194 | loss: 3.0654306MixupTrain:  epoch  0, batch   195 | loss: 3.0081306MixupTrain:  epoch  0, batch   196 | loss: 3.0637875MixupTrain:  epoch  0, batch   197 | loss: 2.9031610MixupTrain:  epoch  0, batch   198 | loss: 3.2585073MixupTrain:  epoch  0, batch   199 | loss: 2.9929819MixupTrain:  epoch  0, batch   200 | loss: 2.9841590MixupTrain:  epoch  0, batch   201 | loss: 3.1560683MixupTrain:  epoch  0, batch   202 | loss: 2.9968436MixupTrain:  epoch  0, batch   203 | loss: 2.9863486MixupTrain:  epoch  0, batch   204 | loss: 3.2050281MixupTrain:  epoch  0, batch   205 | loss: 2.9227819MixupTrain:  epoch  0, batch   206 | loss: 3.1008120MixupTrain:  epoch  0, batch   207 | loss: 2.9510705MixupTrain:  epoch  0, batch   208 | loss: 3.0023320MixupTrain:  epoch  0, batch   209 | loss: 2.9221702MixupTrain:  epoch  0, batch   210 | loss: 3.1933951MixupTrain:  epoch  0, batch   211 | loss: 3.0683608MixupTrain:  epoch  0, batch   212 | loss: 2.9799843MixupTrain:  epoch  0, batch   213 | loss: 3.1031184MixupTrain:  epoch  0, batch   214 | loss: 3.0870709MixupTrain:  epoch  0, batch   215 | loss: 2.9310219MixupTrain:  epoch  0, batch   216 | loss: 2.9990907MixupTrain:  epoch  0, batch   217 | loss: 3.1330810MixupTrain:  epoch  0, batch   218 | loss: 3.0964084MixupTrain:  epoch  0, batch   219 | loss: 2.8924627MixupTrain:  epoch  0, batch   220 | loss: 3.0029142MixupTrain:  epoch  0, batch   221 | loss: 3.0045395MixupTrain:  epoch  0, batch   222 | loss: 2.8763371MixupTrain:  epoch  0, batch   223 | loss: 2.7265043MixupTrain:  epoch  0, batch   224 | loss: 2.8993771MixupTrain:  epoch  0, batch   225 | loss: 2.8441908MixupTrain:  epoch  0, batch   226 | loss: 2.8893094MixupTrain:  epoch  0, batch   227 | loss: 3.1590068
MemoryTrain:  epoch  0, batch     0 | loss: 1.6724362MemoryTrain:  epoch  0, batch     1 | loss: 1.7872831MemoryTrain:  epoch  0, batch     2 | loss: 1.8407692MemoryTrain:  epoch  0, batch     3 | loss: 1.9382898MemoryTrain:  epoch  1, batch     0 | loss: 1.3958664MemoryTrain:  epoch  1, batch     1 | loss: 1.4381018MemoryTrain:  epoch  1, batch     2 | loss: 1.3288908MemoryTrain:  epoch  1, batch     3 | loss: 1.4197531MemoryTrain:  epoch  2, batch     0 | loss: 1.4356303MemoryTrain:  epoch  2, batch     1 | loss: 1.3263543MemoryTrain:  epoch  2, batch     2 | loss: 1.3421493MemoryTrain:  epoch  2, batch     3 | loss: 1.4677445MemoryTrain:  epoch  3, batch     0 | loss: 1.5155635MemoryTrain:  epoch  3, batch     1 | loss: 1.3943641MemoryTrain:  epoch  3, batch     2 | loss: 1.2349291MemoryTrain:  epoch  3, batch     3 | loss: 1.2455049MemoryTrain:  epoch  4, batch     0 | loss: 1.3573589MemoryTrain:  epoch  4, batch     1 | loss: 1.3201221MemoryTrain:  epoch  4, batch     2 | loss: 1.3135358MemoryTrain:  epoch  4, batch     3 | loss: 1.2646035MemoryTrain:  epoch  5, batch     0 | loss: 1.3071055MemoryTrain:  epoch  5, batch     1 | loss: 1.2334144MemoryTrain:  epoch  5, batch     2 | loss: 1.2528149MemoryTrain:  epoch  5, batch     3 | loss: 1.2868818MemoryTrain:  epoch  6, batch     0 | loss: 1.2678628MemoryTrain:  epoch  6, batch     1 | loss: 1.2138095MemoryTrain:  epoch  6, batch     2 | loss: 1.2231641MemoryTrain:  epoch  6, batch     3 | loss: 1.2531352MemoryTrain:  epoch  7, batch     0 | loss: 1.2050574MemoryTrain:  epoch  7, batch     1 | loss: 1.2859437MemoryTrain:  epoch  7, batch     2 | loss: 1.3159919MemoryTrain:  epoch  7, batch     3 | loss: 1.2696110MemoryTrain:  epoch  8, batch     0 | loss: 1.2577065MemoryTrain:  epoch  8, batch     1 | loss: 1.2038569MemoryTrain:  epoch  8, batch     2 | loss: 1.2175914MemoryTrain:  epoch  8, batch     3 | loss: 1.2616779MemoryTrain:  epoch  9, batch     0 | loss: 1.1916046MemoryTrain:  epoch  9, batch     1 | loss: 1.2938280MemoryTrain:  epoch  9, batch     2 | loss: 1.2457387MemoryTrain:  epoch  9, batch     3 | loss: 1.2307775
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    1 | acc: 75.00%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 100.00%,  total acc: 89.58%   [EVAL] batch:    3 | acc: 100.00%,  total acc: 92.19%   [EVAL] batch:    4 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 94.79%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 95.54%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 96.09%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 95.83%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 92.50%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 92.61%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 92.71%   [EVAL] batch:   12 | acc: 100.00%,  total acc: 93.27%   [EVAL] batch:   13 | acc: 75.00%,  total acc: 91.96%   
[EVAL] batch:    0 | acc: 31.25%,  total acc: 31.25%   [EVAL] batch:    1 | acc: 25.00%,  total acc: 28.12%   [EVAL] batch:    2 | acc: 31.25%,  total acc: 29.17%   [EVAL] batch:    3 | acc: 31.25%,  total acc: 29.69%   [EVAL] batch:    4 | acc: 18.75%,  total acc: 27.50%   [EVAL] batch:    5 | acc: 25.00%,  total acc: 27.08%   [EVAL] batch:    6 | acc: 25.00%,  total acc: 26.79%   [EVAL] batch:    7 | acc: 18.75%,  total acc: 25.78%   [EVAL] batch:    8 | acc: 31.25%,  total acc: 26.39%   [EVAL] batch:    9 | acc: 37.50%,  total acc: 27.50%   [EVAL] batch:   10 | acc: 25.00%,  total acc: 27.27%   [EVAL] batch:   11 | acc: 31.25%,  total acc: 27.60%   [EVAL] batch:   12 | acc: 12.50%,  total acc: 26.44%   [EVAL] batch:   13 | acc: 31.25%,  total acc: 26.79%   [EVAL] batch:   14 | acc: 56.25%,  total acc: 28.75%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 30.47%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 33.09%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 34.72%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 36.18%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 38.44%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 41.37%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 44.03%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 46.20%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 48.18%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 50.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 52.16%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 53.70%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 55.36%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 56.90%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 57.92%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 59.27%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 60.35%   [EVAL] batch:   32 | acc: 43.75%,  total acc: 59.85%   [EVAL] batch:   33 | acc: 0.00%,  total acc: 58.09%   [EVAL] batch:   34 | acc: 6.25%,  total acc: 56.61%   [EVAL] batch:   35 | acc: 12.50%,  total acc: 55.38%   [EVAL] batch:   36 | acc: 0.00%,  total acc: 53.89%   [EVAL] batch:   37 | acc: 12.50%,  total acc: 52.80%   [EVAL] batch:   38 | acc: 31.25%,  total acc: 52.24%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 53.44%   [EVAL] batch:   40 | acc: 43.75%,  total acc: 53.20%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 54.17%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 55.09%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 55.97%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 56.94%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 57.88%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 58.78%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 59.64%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 60.46%   [EVAL] batch:   49 | acc: 81.25%,  total acc: 60.88%   [EVAL] batch:   50 | acc: 81.25%,  total acc: 61.27%   [EVAL] batch:   51 | acc: 75.00%,  total acc: 61.54%   [EVAL] batch:   52 | acc: 62.50%,  total acc: 61.56%   [EVAL] batch:   53 | acc: 75.00%,  total acc: 61.81%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 62.16%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 62.72%   [EVAL] batch:   56 | acc: 87.50%,  total acc: 63.16%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 63.58%   [EVAL] batch:   58 | acc: 81.25%,  total acc: 63.88%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 64.48%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 65.06%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 65.62%   [EVAL] batch:   62 | acc: 100.00%,  total acc: 66.17%   [EVAL] batch:   63 | acc: 100.00%,  total acc: 66.70%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 67.21%   [EVAL] batch:   65 | acc: 81.25%,  total acc: 67.42%   [EVAL] batch:   66 | acc: 75.00%,  total acc: 67.54%   [EVAL] batch:   67 | acc: 87.50%,  total acc: 67.83%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 68.30%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 68.75%   [EVAL] batch:   70 | acc: 50.00%,  total acc: 68.49%   
cur_acc:  ['0.8617', '0.9236', '0.8125', '0.9196']
his_acc:  ['0.8617', '0.8725', '0.7215', '0.6849']
CurrentTrain: epoch  0, batch     0 | loss: 5.6207056CurrentTrain: epoch  0, batch     1 | loss: 6.5976801CurrentTrain: epoch  1, batch     0 | loss: 5.0820494CurrentTrain: epoch  1, batch     1 | loss: 4.8605738CurrentTrain: epoch  2, batch     0 | loss: 4.3929200CurrentTrain: epoch  2, batch     1 | loss: 3.7222795CurrentTrain: epoch  3, batch     0 | loss: 4.0289254CurrentTrain: epoch  3, batch     1 | loss: 3.5757306CurrentTrain: epoch  4, batch     0 | loss: 3.2217927CurrentTrain: epoch  4, batch     1 | loss: 3.7629557CurrentTrain: epoch  5, batch     0 | loss: 3.2477064CurrentTrain: epoch  5, batch     1 | loss: 3.1926188CurrentTrain: epoch  6, batch     0 | loss: 3.2065599CurrentTrain: epoch  6, batch     1 | loss: 2.8290112CurrentTrain: epoch  7, batch     0 | loss: 2.9634247CurrentTrain: epoch  7, batch     1 | loss: 2.1034491CurrentTrain: epoch  8, batch     0 | loss: 2.4858818CurrentTrain: epoch  8, batch     1 | loss: 2.6018496CurrentTrain: epoch  9, batch     0 | loss: 2.5490565CurrentTrain: epoch  9, batch     1 | loss: 2.3333905
#### person stateorprovinces of residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person stateorprovinces of residence" means a person is living in a state or province.
Here is an example:
Relation: person stateorprovinces of residence
Context: the chairman of the senate foreign relations committee , massachusetts democrat john kerry , and the panel 's top republican , richard lugar of indiana , were at the white house meeting , which was led by vice president joe biden , a former chairman of the foreign relations panel .
Head Entity: john kerry
Tail Entity: massachusetts
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person stateorprovinces of residence":
Relation: person stateorprovinces of residence  
Context: After moving from New York to California, actress Emma Stone quickly adapted to the vibrant lifestyle of Los Angeles, where she now resides.  
Head Entity: Emma Stone  
Tail Entity: California  

Relation: person stateorprovinces of residence  
Context: Following his successful career in the tech industry, Mark Zuckerberg decided to settle in Palo Alto, a city known for its innovation and proximity to Silicon Valley.  
Head Entity: Mark Zuckerberg  
Tail Entity: Palo Alto  
#### person date of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person date of death" means a person was died on a date.
Here is an example:
Relation: person date of death
Context: he was taken off life support on feb. 14 .
Head Entity: he
Tail Entity: feb. 14
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person date of death":
Relation: person date of death  
Context: The famous author passed away on July 10, 2020, after a long illness.  
Head Entity: The famous author  
Tail Entity: July 10, 2020  

Relation: person date of death  
Context: She left this world on March 5, 2018, surrounded by her family.  
Head Entity: She  
Tail Entity: March 5, 2018  
#### organization number of employees members ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization number of employees members" means the number of an organization 's employees is a number.
Here is an example:
Relation: organization number of employees members
Context: covidien , which posted revenue of more than $ 10 billion last year , has about 42,000 employees worldwide .
Head Entity: covidien
Tail Entity: 42,000
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization number of employees members":
Relation: organization number of employees members  
Context: TechCorp, a leading software development company, employs approximately 5,500 people across its global offices.  
Head Entity: TechCorp  
Tail Entity: 5,500  

Relation: organization number of employees members  
Context: GreenEarth Nonprofit has grown significantly over the past year and now has a staff of around 1,200 dedicated volunteers and employees.  
Head Entity: GreenEarth Nonprofit  
Tail Entity: 1,200  
#### person alternate names ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person alternate names" means a person 's alias is a person.
Here is an example:
Relation: person alternate names
Context: a judge in new york city said remy ma , whose real name is remy smith , said thursday that the hip-hopper could not leave the united states for a five-country european concert tour .
Head Entity: remy smith
Tail Entity: remy ma
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person alternate names":
Relation: person alternate names  
Context: The famous author Samuel Langhorne Clemens, better known by his pen name Mark Twain, is celebrated for his novels like "The Adventures of Tom Sawyer."  
Head Entity: Samuel Langhorne Clemens  
Tail Entity: Mark Twain  

Relation: person alternate names  
Context: The musician Stefani Joanne Angelina Germanotta, who is widely recognized as Lady Gaga, has made a significant impact on the pop music scene.  
Head Entity: Stefani Joanne Angelina Germanotta  
Tail Entity: Lady Gaga  
#### person spouse ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person spouse" means a person 's spouse is a person.
Here is an example:
Relation: person spouse
Context: beverly hills , california 2008-08-17 21:15:39 utc ------ there was much dancing : ellen degeneres and portia de rossi are married , according to reports .
Head Entity: ellen degeneres
Tail Entity: portia de rossi
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person spouse":
Relation: person spouse  
Context: in a lavish ceremony held in new york city, the couple exchanged vows in front of family and friends: john legend and chrissy teigen celebrated their love with a beautiful wedding.  
Head Entity: john legend  
Tail Entity: chrissy teigen  

Relation: person spouse  
Context: during the annual charity gala, it was announced that the famous actor and his long-time partner have tied the knot: ben affleck and jennifer garner are now officially husband and wife.  
Head Entity: ben affleck  
Tail Entity: jennifer garner  
Mixup data size:  5050
MixupTrain:  epoch  0, batch     0 | loss: 4.0183821MixupTrain:  epoch  0, batch     1 | loss: 4.1945610MixupTrain:  epoch  0, batch     2 | loss: 4.2465749MixupTrain:  epoch  0, batch     3 | loss: 4.0149255MixupTrain:  epoch  0, batch     4 | loss: 3.4047806MixupTrain:  epoch  0, batch     5 | loss: 3.6837277MixupTrain:  epoch  0, batch     6 | loss: 3.7395864MixupTrain:  epoch  0, batch     7 | loss: 3.7427118MixupTrain:  epoch  0, batch     8 | loss: 3.8661575MixupTrain:  epoch  0, batch     9 | loss: 3.6746502MixupTrain:  epoch  0, batch    10 | loss: 3.6906691MixupTrain:  epoch  0, batch    11 | loss: 3.7261190MixupTrain:  epoch  0, batch    12 | loss: 3.4570630MixupTrain:  epoch  0, batch    13 | loss: 3.8288069MixupTrain:  epoch  0, batch    14 | loss: 3.4323907MixupTrain:  epoch  0, batch    15 | loss: 3.8389082MixupTrain:  epoch  0, batch    16 | loss: 3.4143052MixupTrain:  epoch  0, batch    17 | loss: 4.2280445MixupTrain:  epoch  0, batch    18 | loss: 3.4037697MixupTrain:  epoch  0, batch    19 | loss: 3.2755494MixupTrain:  epoch  0, batch    20 | loss: 3.4806828MixupTrain:  epoch  0, batch    21 | loss: 3.6742785MixupTrain:  epoch  0, batch    22 | loss: 3.3620548MixupTrain:  epoch  0, batch    23 | loss: 3.7075472MixupTrain:  epoch  0, batch    24 | loss: 3.8262606MixupTrain:  epoch  0, batch    25 | loss: 3.5630155MixupTrain:  epoch  0, batch    26 | loss: 3.4614506MixupTrain:  epoch  0, batch    27 | loss: 3.3302307MixupTrain:  epoch  0, batch    28 | loss: 3.5392878MixupTrain:  epoch  0, batch    29 | loss: 3.4000304MixupTrain:  epoch  0, batch    30 | loss: 3.3981254MixupTrain:  epoch  0, batch    31 | loss: 3.4029450MixupTrain:  epoch  0, batch    32 | loss: 3.6741900MixupTrain:  epoch  0, batch    33 | loss: 3.8441863MixupTrain:  epoch  0, batch    34 | loss: 3.5118778MixupTrain:  epoch  0, batch    35 | loss: 3.2493501MixupTrain:  epoch  0, batch    36 | loss: 3.2246609MixupTrain:  epoch  0, batch    37 | loss: 3.4126265MixupTrain:  epoch  0, batch    38 | loss: 3.6040511MixupTrain:  epoch  0, batch    39 | loss: 3.5134101MixupTrain:  epoch  0, batch    40 | loss: 3.4693918MixupTrain:  epoch  0, batch    41 | loss: 3.7039304MixupTrain:  epoch  0, batch    42 | loss: 3.6339817MixupTrain:  epoch  0, batch    43 | loss: 3.1531436MixupTrain:  epoch  0, batch    44 | loss: 3.2508974MixupTrain:  epoch  0, batch    45 | loss: 3.3895130MixupTrain:  epoch  0, batch    46 | loss: 3.1899772MixupTrain:  epoch  0, batch    47 | loss: 3.1644650MixupTrain:  epoch  0, batch    48 | loss: 3.1743183MixupTrain:  epoch  0, batch    49 | loss: 3.3601248MixupTrain:  epoch  0, batch    50 | loss: 3.0626111MixupTrain:  epoch  0, batch    51 | loss: 3.5584660MixupTrain:  epoch  0, batch    52 | loss: 3.0779991MixupTrain:  epoch  0, batch    53 | loss: 3.5305364MixupTrain:  epoch  0, batch    54 | loss: 3.4290361MixupTrain:  epoch  0, batch    55 | loss: 3.1939673MixupTrain:  epoch  0, batch    56 | loss: 3.2255223MixupTrain:  epoch  0, batch    57 | loss: 3.0819829MixupTrain:  epoch  0, batch    58 | loss: 3.4320087MixupTrain:  epoch  0, batch    59 | loss: 3.0622830MixupTrain:  epoch  0, batch    60 | loss: 3.0286698MixupTrain:  epoch  0, batch    61 | loss: 3.3343811MixupTrain:  epoch  0, batch    62 | loss: 3.2163110MixupTrain:  epoch  0, batch    63 | loss: 3.6431568MixupTrain:  epoch  0, batch    64 | loss: 3.2077866MixupTrain:  epoch  0, batch    65 | loss: 3.2557976MixupTrain:  epoch  0, batch    66 | loss: 3.6033940MixupTrain:  epoch  0, batch    67 | loss: 3.2413735MixupTrain:  epoch  0, batch    68 | loss: 3.4158082MixupTrain:  epoch  0, batch    69 | loss: 3.3728244MixupTrain:  epoch  0, batch    70 | loss: 3.4434326MixupTrain:  epoch  0, batch    71 | loss: 3.4977627MixupTrain:  epoch  0, batch    72 | loss: 3.1995351MixupTrain:  epoch  0, batch    73 | loss: 3.0447211MixupTrain:  epoch  0, batch    74 | loss: 3.3129022MixupTrain:  epoch  0, batch    75 | loss: 3.2623768MixupTrain:  epoch  0, batch    76 | loss: 3.3402696MixupTrain:  epoch  0, batch    77 | loss: 3.4431043MixupTrain:  epoch  0, batch    78 | loss: 3.1836071MixupTrain:  epoch  0, batch    79 | loss: 3.3563633MixupTrain:  epoch  0, batch    80 | loss: 3.2249393MixupTrain:  epoch  0, batch    81 | loss: 3.0740213MixupTrain:  epoch  0, batch    82 | loss: 3.2246366MixupTrain:  epoch  0, batch    83 | loss: 3.1242747MixupTrain:  epoch  0, batch    84 | loss: 3.3132451MixupTrain:  epoch  0, batch    85 | loss: 3.1048679MixupTrain:  epoch  0, batch    86 | loss: 3.1185656MixupTrain:  epoch  0, batch    87 | loss: 2.9801641MixupTrain:  epoch  0, batch    88 | loss: 3.3288970MixupTrain:  epoch  0, batch    89 | loss: 3.3168259MixupTrain:  epoch  0, batch    90 | loss: 3.1769867MixupTrain:  epoch  0, batch    91 | loss: 3.3404999MixupTrain:  epoch  0, batch    92 | loss: 2.9014654MixupTrain:  epoch  0, batch    93 | loss: 3.3414719MixupTrain:  epoch  0, batch    94 | loss: 3.0883157MixupTrain:  epoch  0, batch    95 | loss: 3.5010743MixupTrain:  epoch  0, batch    96 | loss: 3.0430117MixupTrain:  epoch  0, batch    97 | loss: 3.3174200MixupTrain:  epoch  0, batch    98 | loss: 3.2757063MixupTrain:  epoch  0, batch    99 | loss: 3.4107416MixupTrain:  epoch  0, batch   100 | loss: 3.1061440MixupTrain:  epoch  0, batch   101 | loss: 3.0572989MixupTrain:  epoch  0, batch   102 | loss: 3.0709071MixupTrain:  epoch  0, batch   103 | loss: 3.0774453MixupTrain:  epoch  0, batch   104 | loss: 3.3904314MixupTrain:  epoch  0, batch   105 | loss: 3.1767237MixupTrain:  epoch  0, batch   106 | loss: 3.2072577MixupTrain:  epoch  0, batch   107 | loss: 3.3709288MixupTrain:  epoch  0, batch   108 | loss: 3.1621971MixupTrain:  epoch  0, batch   109 | loss: 3.2814257MixupTrain:  epoch  0, batch   110 | loss: 3.1886506MixupTrain:  epoch  0, batch   111 | loss: 3.0177693MixupTrain:  epoch  0, batch   112 | loss: 3.2759533MixupTrain:  epoch  0, batch   113 | loss: 3.2927611MixupTrain:  epoch  0, batch   114 | loss: 3.2687936MixupTrain:  epoch  0, batch   115 | loss: 3.0250158MixupTrain:  epoch  0, batch   116 | loss: 3.0216095MixupTrain:  epoch  0, batch   117 | loss: 3.4087534MixupTrain:  epoch  0, batch   118 | loss: 3.1081903MixupTrain:  epoch  0, batch   119 | loss: 2.9127989MixupTrain:  epoch  0, batch   120 | loss: 3.1737864MixupTrain:  epoch  0, batch   121 | loss: 2.9808569MixupTrain:  epoch  0, batch   122 | loss: 3.1520834MixupTrain:  epoch  0, batch   123 | loss: 3.1910636MixupTrain:  epoch  0, batch   124 | loss: 3.1929755MixupTrain:  epoch  0, batch   125 | loss: 3.1387393MixupTrain:  epoch  0, batch   126 | loss: 3.0691524MixupTrain:  epoch  0, batch   127 | loss: 3.0571840MixupTrain:  epoch  0, batch   128 | loss: 3.1926339MixupTrain:  epoch  0, batch   129 | loss: 3.0669346MixupTrain:  epoch  0, batch   130 | loss: 2.9102767MixupTrain:  epoch  0, batch   131 | loss: 3.0637081MixupTrain:  epoch  0, batch   132 | loss: 3.1359854MixupTrain:  epoch  0, batch   133 | loss: 3.1518197MixupTrain:  epoch  0, batch   134 | loss: 3.1720405MixupTrain:  epoch  0, batch   135 | loss: 3.2160707MixupTrain:  epoch  0, batch   136 | loss: 3.1190495MixupTrain:  epoch  0, batch   137 | loss: 3.2816136MixupTrain:  epoch  0, batch   138 | loss: 2.9071550MixupTrain:  epoch  0, batch   139 | loss: 2.9897017MixupTrain:  epoch  0, batch   140 | loss: 2.9556413MixupTrain:  epoch  0, batch   141 | loss: 3.0610812MixupTrain:  epoch  0, batch   142 | loss: 2.9715791MixupTrain:  epoch  0, batch   143 | loss: 2.9808731MixupTrain:  epoch  0, batch   144 | loss: 3.1405032MixupTrain:  epoch  0, batch   145 | loss: 3.2135444MixupTrain:  epoch  0, batch   146 | loss: 3.1199446MixupTrain:  epoch  0, batch   147 | loss: 2.9923010MixupTrain:  epoch  0, batch   148 | loss: 3.1376448MixupTrain:  epoch  0, batch   149 | loss: 3.1033416MixupTrain:  epoch  0, batch   150 | loss: 3.1022801MixupTrain:  epoch  0, batch   151 | loss: 3.0018361MixupTrain:  epoch  0, batch   152 | loss: 3.3475122MixupTrain:  epoch  0, batch   153 | loss: 3.1465223MixupTrain:  epoch  0, batch   154 | loss: 2.9907665MixupTrain:  epoch  0, batch   155 | loss: 2.9159064MixupTrain:  epoch  0, batch   156 | loss: 2.9088333MixupTrain:  epoch  0, batch   157 | loss: 3.1511204MixupTrain:  epoch  0, batch   158 | loss: 2.9001658MixupTrain:  epoch  0, batch   159 | loss: 3.1650577MixupTrain:  epoch  0, batch   160 | loss: 3.1157115MixupTrain:  epoch  0, batch   161 | loss: 3.1818709MixupTrain:  epoch  0, batch   162 | loss: 2.8305624MixupTrain:  epoch  0, batch   163 | loss: 2.9558678MixupTrain:  epoch  0, batch   164 | loss: 3.2215393MixupTrain:  epoch  0, batch   165 | loss: 3.1696930MixupTrain:  epoch  0, batch   166 | loss: 3.1982536MixupTrain:  epoch  0, batch   167 | loss: 3.0124931MixupTrain:  epoch  0, batch   168 | loss: 3.1035635MixupTrain:  epoch  0, batch   169 | loss: 3.0719583MixupTrain:  epoch  0, batch   170 | loss: 2.9779713MixupTrain:  epoch  0, batch   171 | loss: 2.8804336MixupTrain:  epoch  0, batch   172 | loss: 3.0174167MixupTrain:  epoch  0, batch   173 | loss: 2.9648485MixupTrain:  epoch  0, batch   174 | loss: 2.9319303MixupTrain:  epoch  0, batch   175 | loss: 3.0231595MixupTrain:  epoch  0, batch   176 | loss: 2.9553418MixupTrain:  epoch  0, batch   177 | loss: 3.1067772MixupTrain:  epoch  0, batch   178 | loss: 3.2756264MixupTrain:  epoch  0, batch   179 | loss: 3.1943884MixupTrain:  epoch  0, batch   180 | loss: 2.7854929MixupTrain:  epoch  0, batch   181 | loss: 3.1645622MixupTrain:  epoch  0, batch   182 | loss: 3.1355324MixupTrain:  epoch  0, batch   183 | loss: 3.0928192MixupTrain:  epoch  0, batch   184 | loss: 3.1269929MixupTrain:  epoch  0, batch   185 | loss: 3.0859628MixupTrain:  epoch  0, batch   186 | loss: 3.0718787MixupTrain:  epoch  0, batch   187 | loss: 3.1868336MixupTrain:  epoch  0, batch   188 | loss: 2.9908290MixupTrain:  epoch  0, batch   189 | loss: 2.9740262MixupTrain:  epoch  0, batch   190 | loss: 3.1119053MixupTrain:  epoch  0, batch   191 | loss: 3.0572755MixupTrain:  epoch  0, batch   192 | loss: 2.9775147MixupTrain:  epoch  0, batch   193 | loss: 3.0397477MixupTrain:  epoch  0, batch   194 | loss: 3.1143980MixupTrain:  epoch  0, batch   195 | loss: 2.9600787MixupTrain:  epoch  0, batch   196 | loss: 2.8872871MixupTrain:  epoch  0, batch   197 | loss: 3.1829906MixupTrain:  epoch  0, batch   198 | loss: 2.9736834MixupTrain:  epoch  0, batch   199 | loss: 2.9284670MixupTrain:  epoch  0, batch   200 | loss: 2.8735454MixupTrain:  epoch  0, batch   201 | loss: 2.8064153MixupTrain:  epoch  0, batch   202 | loss: 3.1525812MixupTrain:  epoch  0, batch   203 | loss: 3.0747778MixupTrain:  epoch  0, batch   204 | loss: 2.8755755MixupTrain:  epoch  0, batch   205 | loss: 3.1888063MixupTrain:  epoch  0, batch   206 | loss: 3.2216187MixupTrain:  epoch  0, batch   207 | loss: 3.1013625MixupTrain:  epoch  0, batch   208 | loss: 2.9167275MixupTrain:  epoch  0, batch   209 | loss: 3.1582942MixupTrain:  epoch  0, batch   210 | loss: 3.1656265MixupTrain:  epoch  0, batch   211 | loss: 2.9900136MixupTrain:  epoch  0, batch   212 | loss: 2.9771318MixupTrain:  epoch  0, batch   213 | loss: 3.0916946MixupTrain:  epoch  0, batch   214 | loss: 3.0232325MixupTrain:  epoch  0, batch   215 | loss: 3.0515661MixupTrain:  epoch  0, batch   216 | loss: 2.9919562MixupTrain:  epoch  0, batch   217 | loss: 2.9773598MixupTrain:  epoch  0, batch   218 | loss: 3.0629725MixupTrain:  epoch  0, batch   219 | loss: 2.9539471MixupTrain:  epoch  0, batch   220 | loss: 2.9727221MixupTrain:  epoch  0, batch   221 | loss: 3.2433009MixupTrain:  epoch  0, batch   222 | loss: 3.0769079MixupTrain:  epoch  0, batch   223 | loss: 3.1129341MixupTrain:  epoch  0, batch   224 | loss: 3.2355404MixupTrain:  epoch  0, batch   225 | loss: 2.9285243MixupTrain:  epoch  0, batch   226 | loss: 2.8852248MixupTrain:  epoch  0, batch   227 | loss: 3.0377469MixupTrain:  epoch  0, batch   228 | loss: 3.1619489MixupTrain:  epoch  0, batch   229 | loss: 3.2047508MixupTrain:  epoch  0, batch   230 | loss: 3.1106429MixupTrain:  epoch  0, batch   231 | loss: 2.8211074MixupTrain:  epoch  0, batch   232 | loss: 2.9910319MixupTrain:  epoch  0, batch   233 | loss: 3.1595759MixupTrain:  epoch  0, batch   234 | loss: 2.8982029MixupTrain:  epoch  0, batch   235 | loss: 3.0842957MixupTrain:  epoch  0, batch   236 | loss: 3.0546644MixupTrain:  epoch  0, batch   237 | loss: 2.9044514MixupTrain:  epoch  0, batch   238 | loss: 3.1236174MixupTrain:  epoch  0, batch   239 | loss: 2.9649189MixupTrain:  epoch  0, batch   240 | loss: 3.1912501MixupTrain:  epoch  0, batch   241 | loss: 2.9261589MixupTrain:  epoch  0, batch   242 | loss: 3.0667753MixupTrain:  epoch  0, batch   243 | loss: 3.0263522MixupTrain:  epoch  0, batch   244 | loss: 3.0232053MixupTrain:  epoch  0, batch   245 | loss: 2.9873521MixupTrain:  epoch  0, batch   246 | loss: 2.9841878MixupTrain:  epoch  0, batch   247 | loss: 3.0987906MixupTrain:  epoch  0, batch   248 | loss: 3.1094456MixupTrain:  epoch  0, batch   249 | loss: 2.7956657MixupTrain:  epoch  0, batch   250 | loss: 2.9148359MixupTrain:  epoch  0, batch   251 | loss: 2.9207332MixupTrain:  epoch  0, batch   252 | loss: 2.9919817MixupTrain:  epoch  0, batch   253 | loss: 3.2073040MixupTrain:  epoch  0, batch   254 | loss: 3.0365069MixupTrain:  epoch  0, batch   255 | loss: 3.0803399MixupTrain:  epoch  0, batch   256 | loss: 2.8739095MixupTrain:  epoch  0, batch   257 | loss: 3.0825243MixupTrain:  epoch  0, batch   258 | loss: 3.0168087MixupTrain:  epoch  0, batch   259 | loss: 3.1149626MixupTrain:  epoch  0, batch   260 | loss: 3.1156285MixupTrain:  epoch  0, batch   261 | loss: 2.8781199MixupTrain:  epoch  0, batch   262 | loss: 2.8957481MixupTrain:  epoch  0, batch   263 | loss: 3.0224061MixupTrain:  epoch  0, batch   264 | loss: 2.9732533MixupTrain:  epoch  0, batch   265 | loss: 2.9304023MixupTrain:  epoch  0, batch   266 | loss: 3.0409677MixupTrain:  epoch  0, batch   267 | loss: 3.1371164MixupTrain:  epoch  0, batch   268 | loss: 3.2617288MixupTrain:  epoch  0, batch   269 | loss: 2.9796183MixupTrain:  epoch  0, batch   270 | loss: 3.1694305MixupTrain:  epoch  0, batch   271 | loss: 2.8577981MixupTrain:  epoch  0, batch   272 | loss: 3.0558825MixupTrain:  epoch  0, batch   273 | loss: 2.8572364MixupTrain:  epoch  0, batch   274 | loss: 2.9684289MixupTrain:  epoch  0, batch   275 | loss: 2.9274714MixupTrain:  epoch  0, batch   276 | loss: 2.9823322MixupTrain:  epoch  0, batch   277 | loss: 2.8375449MixupTrain:  epoch  0, batch   278 | loss: 2.9865227MixupTrain:  epoch  0, batch   279 | loss: 2.9604783MixupTrain:  epoch  0, batch   280 | loss: 3.0127459MixupTrain:  epoch  0, batch   281 | loss: 3.1258469MixupTrain:  epoch  0, batch   282 | loss: 2.9027138MixupTrain:  epoch  0, batch   283 | loss: 3.1108470MixupTrain:  epoch  0, batch   284 | loss: 2.9932227MixupTrain:  epoch  0, batch   285 | loss: 2.9263940MixupTrain:  epoch  0, batch   286 | loss: 2.9430757MixupTrain:  epoch  0, batch   287 | loss: 3.0617466MixupTrain:  epoch  0, batch   288 | loss: 2.9023347MixupTrain:  epoch  0, batch   289 | loss: 3.0047340MixupTrain:  epoch  0, batch   290 | loss: 3.1185222MixupTrain:  epoch  0, batch   291 | loss: 2.9730983MixupTrain:  epoch  0, batch   292 | loss: 2.9550312MixupTrain:  epoch  0, batch   293 | loss: 3.1037691MixupTrain:  epoch  0, batch   294 | loss: 3.0346122MixupTrain:  epoch  0, batch   295 | loss: 3.0474384MixupTrain:  epoch  0, batch   296 | loss: 2.9100761MixupTrain:  epoch  0, batch   297 | loss: 2.9774742MixupTrain:  epoch  0, batch   298 | loss: 2.7965136MixupTrain:  epoch  0, batch   299 | loss: 3.1125350MixupTrain:  epoch  0, batch   300 | loss: 2.9145041MixupTrain:  epoch  0, batch   301 | loss: 3.0100617MixupTrain:  epoch  0, batch   302 | loss: 2.9548664MixupTrain:  epoch  0, batch   303 | loss: 2.9335442MixupTrain:  epoch  0, batch   304 | loss: 2.9607494MixupTrain:  epoch  0, batch   305 | loss: 2.9203751MixupTrain:  epoch  0, batch   306 | loss: 2.8785272MixupTrain:  epoch  0, batch   307 | loss: 3.0205438MixupTrain:  epoch  0, batch   308 | loss: 2.7408125MixupTrain:  epoch  0, batch   309 | loss: 2.9963617MixupTrain:  epoch  0, batch   310 | loss: 2.8016999MixupTrain:  epoch  0, batch   311 | loss: 2.9360743MixupTrain:  epoch  0, batch   312 | loss: 2.9156277MixupTrain:  epoch  0, batch   313 | loss: 2.9733224MixupTrain:  epoch  0, batch   314 | loss: 3.2238734MixupTrain:  epoch  0, batch   315 | loss: 2.9975984
MemoryTrain:  epoch  0, batch     0 | loss: 1.2040269MemoryTrain:  epoch  0, batch     1 | loss: 1.3347874MemoryTrain:  epoch  0, batch     2 | loss: 1.7537909MemoryTrain:  epoch  0, batch     3 | loss: 1.7964408MemoryTrain:  epoch  0, batch     4 | loss: 1.8407297MemoryTrain:  epoch  1, batch     0 | loss: 1.3072257MemoryTrain:  epoch  1, batch     1 | loss: 1.3456864MemoryTrain:  epoch  1, batch     2 | loss: 1.3712113MemoryTrain:  epoch  1, batch     3 | loss: 1.2118385MemoryTrain:  epoch  1, batch     4 | loss: 1.2513750MemoryTrain:  epoch  2, batch     0 | loss: 1.2757320MemoryTrain:  epoch  2, batch     1 | loss: 1.2795874MemoryTrain:  epoch  2, batch     2 | loss: 1.2424208MemoryTrain:  epoch  2, batch     3 | loss: 1.3146718MemoryTrain:  epoch  2, batch     4 | loss: 1.2419840MemoryTrain:  epoch  3, batch     0 | loss: 1.2756541MemoryTrain:  epoch  3, batch     1 | loss: 1.2525194MemoryTrain:  epoch  3, batch     2 | loss: 1.2996490MemoryTrain:  epoch  3, batch     3 | loss: 1.2259189MemoryTrain:  epoch  3, batch     4 | loss: 1.2486066MemoryTrain:  epoch  4, batch     0 | loss: 1.2159549MemoryTrain:  epoch  4, batch     1 | loss: 1.2077386MemoryTrain:  epoch  4, batch     2 | loss: 1.2130842MemoryTrain:  epoch  4, batch     3 | loss: 1.2224683MemoryTrain:  epoch  4, batch     4 | loss: 1.2164574MemoryTrain:  epoch  5, batch     0 | loss: 1.2478786MemoryTrain:  epoch  5, batch     1 | loss: 1.2102492MemoryTrain:  epoch  5, batch     2 | loss: 1.2270848MemoryTrain:  epoch  5, batch     3 | loss: 1.1951571MemoryTrain:  epoch  5, batch     4 | loss: 1.1939304MemoryTrain:  epoch  6, batch     0 | loss: 1.1878841MemoryTrain:  epoch  6, batch     1 | loss: 1.2352738MemoryTrain:  epoch  6, batch     2 | loss: 1.2177733MemoryTrain:  epoch  6, batch     3 | loss: 1.1881317MemoryTrain:  epoch  6, batch     4 | loss: 1.2422986MemoryTrain:  epoch  7, batch     0 | loss: 1.2163792MemoryTrain:  epoch  7, batch     1 | loss: 1.2330319MemoryTrain:  epoch  7, batch     2 | loss: 1.1946876MemoryTrain:  epoch  7, batch     3 | loss: 1.1691202MemoryTrain:  epoch  7, batch     4 | loss: 1.1831554MemoryTrain:  epoch  8, batch     0 | loss: 1.2188758MemoryTrain:  epoch  8, batch     1 | loss: 1.2550405MemoryTrain:  epoch  8, batch     2 | loss: 1.2312443MemoryTrain:  epoch  8, batch     3 | loss: 1.2176853MemoryTrain:  epoch  8, batch     4 | loss: 1.1831900MemoryTrain:  epoch  9, batch     0 | loss: 1.2600913MemoryTrain:  epoch  9, batch     1 | loss: 1.2047477MemoryTrain:  epoch  9, batch     2 | loss: 1.1928046MemoryTrain:  epoch  9, batch     3 | loss: 1.1863087MemoryTrain:  epoch  9, batch     4 | loss: 1.1978729
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    3 | acc: 93.75%,  total acc: 89.06%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 90.00%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 91.96%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 92.19%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 92.36%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 91.25%   [EVAL] batch:   10 | acc: 75.00%,  total acc: 89.77%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:   12 | acc: 100.00%,  total acc: 91.35%   [EVAL] batch:   13 | acc: 81.25%,  total acc: 90.62%   [EVAL] batch:   14 | acc: 37.50%,  total acc: 87.08%   
[EVAL] batch:    0 | acc: 25.00%,  total acc: 25.00%   [EVAL] batch:    1 | acc: 18.75%,  total acc: 21.88%   [EVAL] batch:    2 | acc: 18.75%,  total acc: 20.83%   [EVAL] batch:    3 | acc: 6.25%,  total acc: 17.19%   [EVAL] batch:    4 | acc: 6.25%,  total acc: 15.00%   [EVAL] batch:    5 | acc: 12.50%,  total acc: 14.58%   [EVAL] batch:    6 | acc: 0.00%,  total acc: 12.50%   [EVAL] batch:    7 | acc: 12.50%,  total acc: 12.50%   [EVAL] batch:    8 | acc: 18.75%,  total acc: 13.19%   [EVAL] batch:    9 | acc: 25.00%,  total acc: 14.37%   [EVAL] batch:   10 | acc: 12.50%,  total acc: 14.20%   [EVAL] batch:   11 | acc: 6.25%,  total acc: 13.54%   [EVAL] batch:   12 | acc: 0.00%,  total acc: 12.50%   [EVAL] batch:   13 | acc: 31.25%,  total acc: 13.84%   [EVAL] batch:   14 | acc: 56.25%,  total acc: 16.67%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 19.14%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 22.43%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 24.65%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 26.64%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 29.69%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 33.04%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 36.08%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 38.59%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 40.89%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 43.25%   [EVAL] batch:   25 | acc: 93.75%,  total acc: 45.19%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 46.99%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 48.88%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 50.65%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 51.88%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 53.43%   [EVAL] batch:   31 | acc: 87.50%,  total acc: 54.49%   [EVAL] batch:   32 | acc: 43.75%,  total acc: 54.17%   [EVAL] batch:   33 | acc: 0.00%,  total acc: 52.57%   [EVAL] batch:   34 | acc: 6.25%,  total acc: 51.25%   [EVAL] batch:   35 | acc: 12.50%,  total acc: 50.17%   [EVAL] batch:   36 | acc: 0.00%,  total acc: 48.82%   [EVAL] batch:   37 | acc: 6.25%,  total acc: 47.70%   [EVAL] batch:   38 | acc: 31.25%,  total acc: 47.28%   [EVAL] batch:   39 | acc: 93.75%,  total acc: 48.44%   [EVAL] batch:   40 | acc: 43.75%,  total acc: 48.32%   [EVAL] batch:   41 | acc: 31.25%,  total acc: 47.92%   [EVAL] batch:   42 | acc: 37.50%,  total acc: 47.67%   [EVAL] batch:   43 | acc: 75.00%,  total acc: 48.30%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 49.44%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 50.54%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 51.60%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 52.60%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 53.57%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 54.25%   [EVAL] batch:   50 | acc: 81.25%,  total acc: 54.78%   [EVAL] batch:   51 | acc: 68.75%,  total acc: 55.05%   [EVAL] batch:   52 | acc: 75.00%,  total acc: 55.42%   [EVAL] batch:   53 | acc: 87.50%,  total acc: 56.02%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 56.59%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 57.37%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 58.00%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 58.51%   [EVAL] batch:   58 | acc: 75.00%,  total acc: 58.79%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 59.48%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 60.14%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 60.79%   [EVAL] batch:   62 | acc: 100.00%,  total acc: 61.41%   [EVAL] batch:   63 | acc: 100.00%,  total acc: 62.01%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 62.60%   [EVAL] batch:   65 | acc: 56.25%,  total acc: 62.50%   [EVAL] batch:   66 | acc: 81.25%,  total acc: 62.78%   [EVAL] batch:   67 | acc: 87.50%,  total acc: 63.14%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 63.68%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 64.20%   [EVAL] batch:   70 | acc: 93.75%,  total acc: 64.61%   [EVAL] batch:   71 | acc: 81.25%,  total acc: 64.84%   [EVAL] batch:   72 | acc: 93.75%,  total acc: 65.24%   [EVAL] batch:   73 | acc: 87.50%,  total acc: 65.54%   [EVAL] batch:   74 | acc: 93.75%,  total acc: 65.92%   [EVAL] batch:   75 | acc: 100.00%,  total acc: 66.37%   [EVAL] batch:   76 | acc: 93.75%,  total acc: 66.72%   [EVAL] batch:   77 | acc: 100.00%,  total acc: 67.15%   [EVAL] batch:   78 | acc: 87.50%,  total acc: 67.41%   [EVAL] batch:   79 | acc: 100.00%,  total acc: 67.81%   [EVAL] batch:   80 | acc: 75.00%,  total acc: 67.90%   [EVAL] batch:   81 | acc: 81.25%,  total acc: 68.06%   [EVAL] batch:   82 | acc: 100.00%,  total acc: 68.45%   [EVAL] batch:   83 | acc: 81.25%,  total acc: 68.60%   [EVAL] batch:   84 | acc: 87.50%,  total acc: 68.82%   
cur_acc:  ['0.8617', '0.9236', '0.8125', '0.9196', '0.8708']
his_acc:  ['0.8617', '0.8725', '0.7215', '0.6849', '0.6882']
CurrentTrain: epoch  0, batch     0 | loss: 6.3490791CurrentTrain: epoch  0, batch     1 | loss: 5.9064584CurrentTrain: epoch  1, batch     0 | loss: 5.3292480CurrentTrain: epoch  1, batch     1 | loss: 4.3943548CurrentTrain: epoch  2, batch     0 | loss: 4.5282726CurrentTrain: epoch  2, batch     1 | loss: 3.9450836CurrentTrain: epoch  3, batch     0 | loss: 3.7981257CurrentTrain: epoch  3, batch     1 | loss: 3.6704915CurrentTrain: epoch  4, batch     0 | loss: 3.4601893CurrentTrain: epoch  4, batch     1 | loss: 3.4965236CurrentTrain: epoch  5, batch     0 | loss: 3.1124983CurrentTrain: epoch  5, batch     1 | loss: 2.7340589CurrentTrain: epoch  6, batch     0 | loss: 2.6716943CurrentTrain: epoch  6, batch     1 | loss: 2.9044542CurrentTrain: epoch  7, batch     0 | loss: 2.6735415CurrentTrain: epoch  7, batch     1 | loss: 2.3721766CurrentTrain: epoch  8, batch     0 | loss: 2.8104575CurrentTrain: epoch  8, batch     1 | loss: 2.1197236CurrentTrain: epoch  9, batch     0 | loss: 2.6051545CurrentTrain: epoch  9, batch     1 | loss: 2.1167586
#### person date of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person date of birth" means a person was born in a date.
Here is an example:
Relation: person date of birth
Context: born of schoolteacher parents in the western town of sabaneta on july 28 , 1954 , chavez studied at the military academy of venezuela in caracas .
Head Entity: chavez
Tail Entity: july 28 , 1954
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person date of birth":
Relation: person date of birth  
Context: the famous physicist albert einstein was born in ulm, in the kingdom of wurttemberg in the german empire on march 14, 1879.  
Head Entity: albert einstein  
Tail Entity: march 14, 1879  

Relation: person date of birth  
Context: the renowned author jane austen was born on december 16, 1775, in steventon, hampshire, england.  
Head Entity: jane austen  
Tail Entity: december 16, 1775  
#### person stateorprovince of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person stateorprovince of birth" means a person was died in a state or province.
Here is an example:
Relation: person stateorprovince of birth
Context: jane matilda bolin was born on april 11 , 1908 , in poughkeepsie , ny .
Head Entity: jane matilda bolin
Tail Entity: ny
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person stateorprovince of birth":
Relation: person stateorprovince of birth  
Context: albert einstein was born on march 14, 1879, in ulm, germany.  
Head Entity: albert einstein  
Tail Entity: germany  

Relation: person stateorprovince of birth  
Context: marilyn monroe was born on june 1, 1926, in los angeles, california.  
Head Entity: marilyn monroe  
Tail Entity: california  
#### person parents ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person parents" means a person 's parent is a person.
Here is an example:
Relation: person parents
Context: as the case developed , sandy 's mother , denise sandy , quietly made herself a spectral but central figure , by faithfully attending pretrial hearings .
Head Entity: sandy
Tail Entity: denise sandy
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person parents":
Relation: person parents  
Context: During the family reunion, it was heartwarming to see how much Sarah resembled her father, John, in both looks and mannerisms.  
Head Entity: Sarah  
Tail Entity: John  

Relation: person parents  
Context: After the ceremony, Emily shared stories about her childhood and how her mother, Lisa, always encouraged her to pursue her dreams.  
Head Entity: Emily  
Tail Entity: Lisa  
#### person employee of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person employee of" means a person is an employee of an organization.
Here is an example:
Relation: person employee of
Context: seeking revenge , axel reunites with old pal sgt. billy rosewood -lrb- judge reinhold -rrb- and jon flint -lrb- hector elizondo -rrb- of the beverly hills police department .
Head Entity: hector elizondo
Tail Entity: beverly hills police department
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person employee of":
Relation: person employee of  
Context: After years of hard work and dedication, Sarah Thompson finally received a promotion at Tech Innovations, where she has been a key player in the development team.  
Head Entity: Sarah Thompson  
Tail Entity: Tech Innovations  

Relation: person employee of  
Context: John Smith has been with Global Finance for over a decade, where he has climbed the ranks to become the senior analyst in the investment division.  
Head Entity: John Smith  
Tail Entity: Global Finance  
#### person stateorprovince of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person stateorprovince of death" means a person was died in a state or province.
Here is an example:
Relation: person stateorprovince of death
Context: irene morgan kirkaldy , whose defiance of bus segregation laws more than a decade before rosa parks ' landmark case helped lay the foundation for later civil rights victories , died friday at her home in hayes , va. .
Head Entity: irene morgan kirkaldy
Tail Entity: va.
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person stateorprovince of death":
Relation: person stateorprovince of death  
Context: john doe, a renowned author known for his impactful novels, passed away peacefully in his sleep at his residence in california last week.  
Head Entity: john doe  
Tail Entity: california  

Relation: person stateorprovince of death  
Context: after a long battle with illness, elizabeth taylor, the iconic actress, succumbed to her health issues and died in a hospital in nevada.  
Head Entity: elizabeth taylor  
Tail Entity: nevada  
Mixup data size:  6685
MixupTrain:  epoch  0, batch     0 | loss: 4.1151857MixupTrain:  epoch  0, batch     1 | loss: 3.9433680MixupTrain:  epoch  0, batch     2 | loss: 3.7599015MixupTrain:  epoch  0, batch     3 | loss: 3.5528579MixupTrain:  epoch  0, batch     4 | loss: 3.5463982MixupTrain:  epoch  0, batch     5 | loss: 3.4318833MixupTrain:  epoch  0, batch     6 | loss: 3.4494848MixupTrain:  epoch  0, batch     7 | loss: 3.5061221MixupTrain:  epoch  0, batch     8 | loss: 3.4086871MixupTrain:  epoch  0, batch     9 | loss: 3.7945378MixupTrain:  epoch  0, batch    10 | loss: 3.4890141MixupTrain:  epoch  0, batch    11 | loss: 3.5546165MixupTrain:  epoch  0, batch    12 | loss: 3.3808188MixupTrain:  epoch  0, batch    13 | loss: 3.4342318MixupTrain:  epoch  0, batch    14 | loss: 3.5041401MixupTrain:  epoch  0, batch    15 | loss: 3.7274008MixupTrain:  epoch  0, batch    16 | loss: 3.3021951MixupTrain:  epoch  0, batch    17 | loss: 3.1820729MixupTrain:  epoch  0, batch    18 | loss: 3.2300894MixupTrain:  epoch  0, batch    19 | loss: 3.1874168MixupTrain:  epoch  0, batch    20 | loss: 3.5847192MixupTrain:  epoch  0, batch    21 | loss: 3.4226246MixupTrain:  epoch  0, batch    22 | loss: 3.4475007MixupTrain:  epoch  0, batch    23 | loss: 3.1277776MixupTrain:  epoch  0, batch    24 | loss: 3.2371840MixupTrain:  epoch  0, batch    25 | loss: 3.0535836MixupTrain:  epoch  0, batch    26 | loss: 3.3114133MixupTrain:  epoch  0, batch    27 | loss: 3.1252718MixupTrain:  epoch  0, batch    28 | loss: 3.1947756MixupTrain:  epoch  0, batch    29 | loss: 3.1257179MixupTrain:  epoch  0, batch    30 | loss: 3.3058462MixupTrain:  epoch  0, batch    31 | loss: 3.3137958MixupTrain:  epoch  0, batch    32 | loss: 3.1588018MixupTrain:  epoch  0, batch    33 | loss: 3.2101607MixupTrain:  epoch  0, batch    34 | loss: 3.1094937MixupTrain:  epoch  0, batch    35 | loss: 3.2710948MixupTrain:  epoch  0, batch    36 | loss: 3.1682661MixupTrain:  epoch  0, batch    37 | loss: 2.9799511MixupTrain:  epoch  0, batch    38 | loss: 3.3178439MixupTrain:  epoch  0, batch    39 | loss: 3.2302728MixupTrain:  epoch  0, batch    40 | loss: 3.2443852MixupTrain:  epoch  0, batch    41 | loss: 3.2463746MixupTrain:  epoch  0, batch    42 | loss: 3.5022166MixupTrain:  epoch  0, batch    43 | loss: 3.2201006MixupTrain:  epoch  0, batch    44 | loss: 3.2198570MixupTrain:  epoch  0, batch    45 | loss: 3.2857375MixupTrain:  epoch  0, batch    46 | loss: 3.2806363MixupTrain:  epoch  0, batch    47 | loss: 2.9867477MixupTrain:  epoch  0, batch    48 | loss: 3.0376608MixupTrain:  epoch  0, batch    49 | loss: 2.9970520MixupTrain:  epoch  0, batch    50 | loss: 2.9536242MixupTrain:  epoch  0, batch    51 | loss: 3.2555423MixupTrain:  epoch  0, batch    52 | loss: 3.2543776MixupTrain:  epoch  0, batch    53 | loss: 3.0703027MixupTrain:  epoch  0, batch    54 | loss: 2.9762006MixupTrain:  epoch  0, batch    55 | loss: 2.8834620MixupTrain:  epoch  0, batch    56 | loss: 3.2545068MixupTrain:  epoch  0, batch    57 | loss: 3.2283874MixupTrain:  epoch  0, batch    58 | loss: 2.8887315MixupTrain:  epoch  0, batch    59 | loss: 2.9520798MixupTrain:  epoch  0, batch    60 | loss: 3.2039635MixupTrain:  epoch  0, batch    61 | loss: 3.0042520MixupTrain:  epoch  0, batch    62 | loss: 3.1011338MixupTrain:  epoch  0, batch    63 | loss: 3.2192459MixupTrain:  epoch  0, batch    64 | loss: 3.1247339MixupTrain:  epoch  0, batch    65 | loss: 2.8866706MixupTrain:  epoch  0, batch    66 | loss: 2.8756061MixupTrain:  epoch  0, batch    67 | loss: 3.0503850MixupTrain:  epoch  0, batch    68 | loss: 2.8126030MixupTrain:  epoch  0, batch    69 | loss: 3.1048703MixupTrain:  epoch  0, batch    70 | loss: 3.1290720MixupTrain:  epoch  0, batch    71 | loss: 3.0559542MixupTrain:  epoch  0, batch    72 | loss: 3.0169768MixupTrain:  epoch  0, batch    73 | loss: 3.1909757MixupTrain:  epoch  0, batch    74 | loss: 3.1748772MixupTrain:  epoch  0, batch    75 | loss: 2.9430184MixupTrain:  epoch  0, batch    76 | loss: 2.8863854MixupTrain:  epoch  0, batch    77 | loss: 2.9794118MixupTrain:  epoch  0, batch    78 | loss: 2.9576774MixupTrain:  epoch  0, batch    79 | loss: 2.9509249MixupTrain:  epoch  0, batch    80 | loss: 3.1622882MixupTrain:  epoch  0, batch    81 | loss: 2.8969550MixupTrain:  epoch  0, batch    82 | loss: 2.9720254MixupTrain:  epoch  0, batch    83 | loss: 2.9545751MixupTrain:  epoch  0, batch    84 | loss: 3.0338330MixupTrain:  epoch  0, batch    85 | loss: 2.8732319MixupTrain:  epoch  0, batch    86 | loss: 2.8352671MixupTrain:  epoch  0, batch    87 | loss: 2.9468007MixupTrain:  epoch  0, batch    88 | loss: 2.9614220MixupTrain:  epoch  0, batch    89 | loss: 2.9478602MixupTrain:  epoch  0, batch    90 | loss: 2.8807924MixupTrain:  epoch  0, batch    91 | loss: 3.0988326MixupTrain:  epoch  0, batch    92 | loss: 3.0889406MixupTrain:  epoch  0, batch    93 | loss: 2.9116383MixupTrain:  epoch  0, batch    94 | loss: 3.0266488MixupTrain:  epoch  0, batch    95 | loss: 2.9340446MixupTrain:  epoch  0, batch    96 | loss: 3.0047369MixupTrain:  epoch  0, batch    97 | loss: 3.0112252MixupTrain:  epoch  0, batch    98 | loss: 2.9180694MixupTrain:  epoch  0, batch    99 | loss: 2.8613372MixupTrain:  epoch  0, batch   100 | loss: 3.0115252MixupTrain:  epoch  0, batch   101 | loss: 3.0945189MixupTrain:  epoch  0, batch   102 | loss: 2.9482772MixupTrain:  epoch  0, batch   103 | loss: 3.0702929MixupTrain:  epoch  0, batch   104 | loss: 2.8925645MixupTrain:  epoch  0, batch   105 | loss: 3.0842083MixupTrain:  epoch  0, batch   106 | loss: 3.0157123MixupTrain:  epoch  0, batch   107 | loss: 2.8910630MixupTrain:  epoch  0, batch   108 | loss: 2.9623382MixupTrain:  epoch  0, batch   109 | loss: 2.7705595MixupTrain:  epoch  0, batch   110 | loss: 2.9178779MixupTrain:  epoch  0, batch   111 | loss: 3.0406971MixupTrain:  epoch  0, batch   112 | loss: 2.9815602MixupTrain:  epoch  0, batch   113 | loss: 2.8400354MixupTrain:  epoch  0, batch   114 | loss: 2.9530358MixupTrain:  epoch  0, batch   115 | loss: 3.0491595MixupTrain:  epoch  0, batch   116 | loss: 2.9147043MixupTrain:  epoch  0, batch   117 | loss: 2.9474578MixupTrain:  epoch  0, batch   118 | loss: 3.0262101MixupTrain:  epoch  0, batch   119 | loss: 2.8983183MixupTrain:  epoch  0, batch   120 | loss: 3.0093622MixupTrain:  epoch  0, batch   121 | loss: 2.9275112MixupTrain:  epoch  0, batch   122 | loss: 2.9907408MixupTrain:  epoch  0, batch   123 | loss: 2.9384637MixupTrain:  epoch  0, batch   124 | loss: 2.9731712MixupTrain:  epoch  0, batch   125 | loss: 3.0598702MixupTrain:  epoch  0, batch   126 | loss: 2.8931346MixupTrain:  epoch  0, batch   127 | loss: 3.0085840MixupTrain:  epoch  0, batch   128 | loss: 3.0302849MixupTrain:  epoch  0, batch   129 | loss: 2.8931777MixupTrain:  epoch  0, batch   130 | loss: 2.9955359MixupTrain:  epoch  0, batch   131 | loss: 2.9238243MixupTrain:  epoch  0, batch   132 | loss: 3.0351350MixupTrain:  epoch  0, batch   133 | loss: 2.9082146MixupTrain:  epoch  0, batch   134 | loss: 2.7760878MixupTrain:  epoch  0, batch   135 | loss: 2.8396230MixupTrain:  epoch  0, batch   136 | loss: 2.9996142MixupTrain:  epoch  0, batch   137 | loss: 2.9600592MixupTrain:  epoch  0, batch   138 | loss: 2.7948244MixupTrain:  epoch  0, batch   139 | loss: 2.9540935MixupTrain:  epoch  0, batch   140 | loss: 2.8395462MixupTrain:  epoch  0, batch   141 | loss: 2.8037648MixupTrain:  epoch  0, batch   142 | loss: 2.9405065MixupTrain:  epoch  0, batch   143 | loss: 2.9854207MixupTrain:  epoch  0, batch   144 | loss: 3.0886991MixupTrain:  epoch  0, batch   145 | loss: 2.7961090MixupTrain:  epoch  0, batch   146 | loss: 2.7806094MixupTrain:  epoch  0, batch   147 | loss: 2.9841776MixupTrain:  epoch  0, batch   148 | loss: 3.1272645MixupTrain:  epoch  0, batch   149 | loss: 2.9647191MixupTrain:  epoch  0, batch   150 | loss: 2.8504434MixupTrain:  epoch  0, batch   151 | loss: 2.9778080MixupTrain:  epoch  0, batch   152 | loss: 2.7463143MixupTrain:  epoch  0, batch   153 | loss: 2.8233800MixupTrain:  epoch  0, batch   154 | loss: 3.0303202MixupTrain:  epoch  0, batch   155 | loss: 3.0219491MixupTrain:  epoch  0, batch   156 | loss: 2.8807189MixupTrain:  epoch  0, batch   157 | loss: 2.8365612MixupTrain:  epoch  0, batch   158 | loss: 2.9953392MixupTrain:  epoch  0, batch   159 | loss: 2.8625922MixupTrain:  epoch  0, batch   160 | loss: 2.7418113MixupTrain:  epoch  0, batch   161 | loss: 3.0197582MixupTrain:  epoch  0, batch   162 | loss: 2.8041155MixupTrain:  epoch  0, batch   163 | loss: 3.0337727MixupTrain:  epoch  0, batch   164 | loss: 2.8318439MixupTrain:  epoch  0, batch   165 | loss: 2.9129720MixupTrain:  epoch  0, batch   166 | loss: 2.9130692MixupTrain:  epoch  0, batch   167 | loss: 2.9119587MixupTrain:  epoch  0, batch   168 | loss: 3.0451927MixupTrain:  epoch  0, batch   169 | loss: 2.9646020MixupTrain:  epoch  0, batch   170 | loss: 2.8854785MixupTrain:  epoch  0, batch   171 | loss: 2.9227290MixupTrain:  epoch  0, batch   172 | loss: 2.9791360MixupTrain:  epoch  0, batch   173 | loss: 2.8620639MixupTrain:  epoch  0, batch   174 | loss: 2.9066253MixupTrain:  epoch  0, batch   175 | loss: 2.7687635MixupTrain:  epoch  0, batch   176 | loss: 2.8621304MixupTrain:  epoch  0, batch   177 | loss: 2.8015528MixupTrain:  epoch  0, batch   178 | loss: 2.8934541MixupTrain:  epoch  0, batch   179 | loss: 2.9205465MixupTrain:  epoch  0, batch   180 | loss: 2.9223657MixupTrain:  epoch  0, batch   181 | loss: 3.0261381MixupTrain:  epoch  0, batch   182 | loss: 2.9219003MixupTrain:  epoch  0, batch   183 | loss: 2.9070935MixupTrain:  epoch  0, batch   184 | loss: 2.8883967MixupTrain:  epoch  0, batch   185 | loss: 2.7329881MixupTrain:  epoch  0, batch   186 | loss: 2.8849373MixupTrain:  epoch  0, batch   187 | loss: 2.8965459MixupTrain:  epoch  0, batch   188 | loss: 2.8570378MixupTrain:  epoch  0, batch   189 | loss: 2.8731885MixupTrain:  epoch  0, batch   190 | loss: 2.9020381MixupTrain:  epoch  0, batch   191 | loss: 2.9977033MixupTrain:  epoch  0, batch   192 | loss: 2.9266491MixupTrain:  epoch  0, batch   193 | loss: 2.8306303MixupTrain:  epoch  0, batch   194 | loss: 2.9373050MixupTrain:  epoch  0, batch   195 | loss: 2.8035319MixupTrain:  epoch  0, batch   196 | loss: 2.8881853MixupTrain:  epoch  0, batch   197 | loss: 2.9563634MixupTrain:  epoch  0, batch   198 | loss: 2.8462176MixupTrain:  epoch  0, batch   199 | loss: 2.9278865MixupTrain:  epoch  0, batch   200 | loss: 2.8835030MixupTrain:  epoch  0, batch   201 | loss: 2.9759669MixupTrain:  epoch  0, batch   202 | loss: 2.9800172MixupTrain:  epoch  0, batch   203 | loss: 3.0281096MixupTrain:  epoch  0, batch   204 | loss: 2.8814571MixupTrain:  epoch  0, batch   205 | loss: 2.7637970MixupTrain:  epoch  0, batch   206 | loss: 2.7094035MixupTrain:  epoch  0, batch   207 | loss: 2.7862983MixupTrain:  epoch  0, batch   208 | loss: 2.9243350MixupTrain:  epoch  0, batch   209 | loss: 2.8988121MixupTrain:  epoch  0, batch   210 | loss: 2.7992864MixupTrain:  epoch  0, batch   211 | loss: 2.8900795MixupTrain:  epoch  0, batch   212 | loss: 2.8877907MixupTrain:  epoch  0, batch   213 | loss: 2.8926289MixupTrain:  epoch  0, batch   214 | loss: 2.8894045MixupTrain:  epoch  0, batch   215 | loss: 2.8249931MixupTrain:  epoch  0, batch   216 | loss: 2.9349215MixupTrain:  epoch  0, batch   217 | loss: 3.0039022MixupTrain:  epoch  0, batch   218 | loss: 2.9186780MixupTrain:  epoch  0, batch   219 | loss: 2.9453781MixupTrain:  epoch  0, batch   220 | loss: 2.8611765MixupTrain:  epoch  0, batch   221 | loss: 2.9378374MixupTrain:  epoch  0, batch   222 | loss: 2.8430910MixupTrain:  epoch  0, batch   223 | loss: 2.9011106MixupTrain:  epoch  0, batch   224 | loss: 2.9423828MixupTrain:  epoch  0, batch   225 | loss: 2.9491630MixupTrain:  epoch  0, batch   226 | loss: 2.9577856MixupTrain:  epoch  0, batch   227 | loss: 2.7853849MixupTrain:  epoch  0, batch   228 | loss: 3.0596983MixupTrain:  epoch  0, batch   229 | loss: 2.6869743MixupTrain:  epoch  0, batch   230 | loss: 2.8333640MixupTrain:  epoch  0, batch   231 | loss: 2.7632723MixupTrain:  epoch  0, batch   232 | loss: 2.9079459MixupTrain:  epoch  0, batch   233 | loss: 2.9051247MixupTrain:  epoch  0, batch   234 | loss: 2.8898144MixupTrain:  epoch  0, batch   235 | loss: 2.8867669MixupTrain:  epoch  0, batch   236 | loss: 2.8618531MixupTrain:  epoch  0, batch   237 | loss: 2.7997410MixupTrain:  epoch  0, batch   238 | loss: 2.9153438MixupTrain:  epoch  0, batch   239 | loss: 2.8367419MixupTrain:  epoch  0, batch   240 | loss: 2.9116666MixupTrain:  epoch  0, batch   241 | loss: 2.9554937MixupTrain:  epoch  0, batch   242 | loss: 2.8506162MixupTrain:  epoch  0, batch   243 | loss: 2.8671620MixupTrain:  epoch  0, batch   244 | loss: 2.8968682MixupTrain:  epoch  0, batch   245 | loss: 2.9853921MixupTrain:  epoch  0, batch   246 | loss: 2.8356519MixupTrain:  epoch  0, batch   247 | loss: 2.7767601MixupTrain:  epoch  0, batch   248 | loss: 2.9039359MixupTrain:  epoch  0, batch   249 | loss: 2.8590212MixupTrain:  epoch  0, batch   250 | loss: 2.9171057MixupTrain:  epoch  0, batch   251 | loss: 3.0774581MixupTrain:  epoch  0, batch   252 | loss: 2.9033046MixupTrain:  epoch  0, batch   253 | loss: 2.8328819MixupTrain:  epoch  0, batch   254 | loss: 2.8672705MixupTrain:  epoch  0, batch   255 | loss: 2.8797207MixupTrain:  epoch  0, batch   256 | loss: 2.8483195MixupTrain:  epoch  0, batch   257 | loss: 2.7976646MixupTrain:  epoch  0, batch   258 | loss: 2.9553118MixupTrain:  epoch  0, batch   259 | loss: 2.9345174MixupTrain:  epoch  0, batch   260 | loss: 2.7404714MixupTrain:  epoch  0, batch   261 | loss: 2.9324825MixupTrain:  epoch  0, batch   262 | loss: 2.8802664MixupTrain:  epoch  0, batch   263 | loss: 2.8906446MixupTrain:  epoch  0, batch   264 | loss: 2.8389983MixupTrain:  epoch  0, batch   265 | loss: 2.9053307MixupTrain:  epoch  0, batch   266 | loss: 2.9558115MixupTrain:  epoch  0, batch   267 | loss: 2.8768525MixupTrain:  epoch  0, batch   268 | loss: 2.9159150MixupTrain:  epoch  0, batch   269 | loss: 2.7736087MixupTrain:  epoch  0, batch   270 | loss: 2.9335709MixupTrain:  epoch  0, batch   271 | loss: 2.7835178MixupTrain:  epoch  0, batch   272 | loss: 2.7697215MixupTrain:  epoch  0, batch   273 | loss: 2.9720352MixupTrain:  epoch  0, batch   274 | loss: 2.7660968MixupTrain:  epoch  0, batch   275 | loss: 2.8639407MixupTrain:  epoch  0, batch   276 | loss: 2.8882444MixupTrain:  epoch  0, batch   277 | loss: 2.7813706MixupTrain:  epoch  0, batch   278 | loss: 2.8655844MixupTrain:  epoch  0, batch   279 | loss: 2.8999906MixupTrain:  epoch  0, batch   280 | loss: 2.9625320MixupTrain:  epoch  0, batch   281 | loss: 2.7502275MixupTrain:  epoch  0, batch   282 | loss: 2.9319952MixupTrain:  epoch  0, batch   283 | loss: 2.9157352MixupTrain:  epoch  0, batch   284 | loss: 2.8069968MixupTrain:  epoch  0, batch   285 | loss: 2.9239028MixupTrain:  epoch  0, batch   286 | loss: 2.7515798MixupTrain:  epoch  0, batch   287 | loss: 2.7755668MixupTrain:  epoch  0, batch   288 | loss: 2.8307161MixupTrain:  epoch  0, batch   289 | loss: 2.8765051MixupTrain:  epoch  0, batch   290 | loss: 2.8479900MixupTrain:  epoch  0, batch   291 | loss: 2.8420916MixupTrain:  epoch  0, batch   292 | loss: 2.9112215MixupTrain:  epoch  0, batch   293 | loss: 2.7407751MixupTrain:  epoch  0, batch   294 | loss: 2.8227897MixupTrain:  epoch  0, batch   295 | loss: 2.9488401MixupTrain:  epoch  0, batch   296 | loss: 2.9179921MixupTrain:  epoch  0, batch   297 | loss: 2.7436578MixupTrain:  epoch  0, batch   298 | loss: 2.7932968MixupTrain:  epoch  0, batch   299 | loss: 2.7823224MixupTrain:  epoch  0, batch   300 | loss: 2.8858669MixupTrain:  epoch  0, batch   301 | loss: 2.9378672MixupTrain:  epoch  0, batch   302 | loss: 2.7287395MixupTrain:  epoch  0, batch   303 | loss: 2.8749425MixupTrain:  epoch  0, batch   304 | loss: 2.8152316MixupTrain:  epoch  0, batch   305 | loss: 2.7940879MixupTrain:  epoch  0, batch   306 | loss: 2.9169810MixupTrain:  epoch  0, batch   307 | loss: 2.7851305MixupTrain:  epoch  0, batch   308 | loss: 2.9678216MixupTrain:  epoch  0, batch   309 | loss: 2.9980807MixupTrain:  epoch  0, batch   310 | loss: 2.7987397MixupTrain:  epoch  0, batch   311 | loss: 2.9519889MixupTrain:  epoch  0, batch   312 | loss: 2.7349589MixupTrain:  epoch  0, batch   313 | loss: 2.7753248MixupTrain:  epoch  0, batch   314 | loss: 2.8935659MixupTrain:  epoch  0, batch   315 | loss: 2.8859413MixupTrain:  epoch  0, batch   316 | loss: 2.9275777MixupTrain:  epoch  0, batch   317 | loss: 2.7696190MixupTrain:  epoch  0, batch   318 | loss: 2.8207412MixupTrain:  epoch  0, batch   319 | loss: 2.8223801MixupTrain:  epoch  0, batch   320 | loss: 2.8392496MixupTrain:  epoch  0, batch   321 | loss: 2.9518459MixupTrain:  epoch  0, batch   322 | loss: 2.7978272MixupTrain:  epoch  0, batch   323 | loss: 2.9102237MixupTrain:  epoch  0, batch   324 | loss: 2.8957851MixupTrain:  epoch  0, batch   325 | loss: 2.9636300MixupTrain:  epoch  0, batch   326 | loss: 2.8233054MixupTrain:  epoch  0, batch   327 | loss: 2.8649704MixupTrain:  epoch  0, batch   328 | loss: 2.8350043MixupTrain:  epoch  0, batch   329 | loss: 2.8290939MixupTrain:  epoch  0, batch   330 | loss: 2.7962482MixupTrain:  epoch  0, batch   331 | loss: 2.8835883MixupTrain:  epoch  0, batch   332 | loss: 2.7536917MixupTrain:  epoch  0, batch   333 | loss: 2.7941263MixupTrain:  epoch  0, batch   334 | loss: 2.7436748MixupTrain:  epoch  0, batch   335 | loss: 2.8690720MixupTrain:  epoch  0, batch   336 | loss: 2.8201470MixupTrain:  epoch  0, batch   337 | loss: 2.9112797MixupTrain:  epoch  0, batch   338 | loss: 2.8785930MixupTrain:  epoch  0, batch   339 | loss: 2.8308859MixupTrain:  epoch  0, batch   340 | loss: 2.8515422MixupTrain:  epoch  0, batch   341 | loss: 2.8875327MixupTrain:  epoch  0, batch   342 | loss: 2.8019185MixupTrain:  epoch  0, batch   343 | loss: 2.9449415MixupTrain:  epoch  0, batch   344 | loss: 2.8392258MixupTrain:  epoch  0, batch   345 | loss: 2.8614464MixupTrain:  epoch  0, batch   346 | loss: 2.7690063MixupTrain:  epoch  0, batch   347 | loss: 2.7737660MixupTrain:  epoch  0, batch   348 | loss: 2.7559192MixupTrain:  epoch  0, batch   349 | loss: 2.8613601MixupTrain:  epoch  0, batch   350 | loss: 2.9741585MixupTrain:  epoch  0, batch   351 | loss: 2.8033719MixupTrain:  epoch  0, batch   352 | loss: 2.8241982MixupTrain:  epoch  0, batch   353 | loss: 2.8811131MixupTrain:  epoch  0, batch   354 | loss: 2.7353127MixupTrain:  epoch  0, batch   355 | loss: 2.9220114MixupTrain:  epoch  0, batch   356 | loss: 2.7952940MixupTrain:  epoch  0, batch   357 | loss: 2.7205157MixupTrain:  epoch  0, batch   358 | loss: 2.8883233MixupTrain:  epoch  0, batch   359 | loss: 2.7606697MixupTrain:  epoch  0, batch   360 | loss: 2.7921071MixupTrain:  epoch  0, batch   361 | loss: 2.9291582MixupTrain:  epoch  0, batch   362 | loss: 2.9870136MixupTrain:  epoch  0, batch   363 | loss: 2.9025660MixupTrain:  epoch  0, batch   364 | loss: 2.7923212MixupTrain:  epoch  0, batch   365 | loss: 2.7288938MixupTrain:  epoch  0, batch   366 | loss: 2.7332959MixupTrain:  epoch  0, batch   367 | loss: 2.8458700MixupTrain:  epoch  0, batch   368 | loss: 2.8779438MixupTrain:  epoch  0, batch   369 | loss: 2.8215921MixupTrain:  epoch  0, batch   370 | loss: 2.7727332MixupTrain:  epoch  0, batch   371 | loss: 2.8689952MixupTrain:  epoch  0, batch   372 | loss: 2.8056741MixupTrain:  epoch  0, batch   373 | loss: 2.8933599MixupTrain:  epoch  0, batch   374 | loss: 2.7371755MixupTrain:  epoch  0, batch   375 | loss: 2.9409413MixupTrain:  epoch  0, batch   376 | loss: 2.9168897MixupTrain:  epoch  0, batch   377 | loss: 2.9063265MixupTrain:  epoch  0, batch   378 | loss: 2.8865433MixupTrain:  epoch  0, batch   379 | loss: 2.9459040MixupTrain:  epoch  0, batch   380 | loss: 2.9738395MixupTrain:  epoch  0, batch   381 | loss: 2.8763883MixupTrain:  epoch  0, batch   382 | loss: 2.8182385MixupTrain:  epoch  0, batch   383 | loss: 2.8269880MixupTrain:  epoch  0, batch   384 | loss: 3.0035331MixupTrain:  epoch  0, batch   385 | loss: 2.8900933MixupTrain:  epoch  0, batch   386 | loss: 2.7600884MixupTrain:  epoch  0, batch   387 | loss: 2.9940839MixupTrain:  epoch  0, batch   388 | loss: 2.7879796MixupTrain:  epoch  0, batch   389 | loss: 2.7752447MixupTrain:  epoch  0, batch   390 | loss: 2.8372736MixupTrain:  epoch  0, batch   391 | loss: 2.9267070MixupTrain:  epoch  0, batch   392 | loss: 2.9250317MixupTrain:  epoch  0, batch   393 | loss: 2.7950809MixupTrain:  epoch  0, batch   394 | loss: 2.7701221MixupTrain:  epoch  0, batch   395 | loss: 2.8542976MixupTrain:  epoch  0, batch   396 | loss: 2.8789110MixupTrain:  epoch  0, batch   397 | loss: 2.7426429MixupTrain:  epoch  0, batch   398 | loss: 2.9775922MixupTrain:  epoch  0, batch   399 | loss: 2.7586954MixupTrain:  epoch  0, batch   400 | loss: 2.7876377MixupTrain:  epoch  0, batch   401 | loss: 2.8364816MixupTrain:  epoch  0, batch   402 | loss: 2.8667347MixupTrain:  epoch  0, batch   403 | loss: 2.9463758MixupTrain:  epoch  0, batch   404 | loss: 2.8330374MixupTrain:  epoch  0, batch   405 | loss: 2.8819871MixupTrain:  epoch  0, batch   406 | loss: 2.7492259MixupTrain:  epoch  0, batch   407 | loss: 2.7328331MixupTrain:  epoch  0, batch   408 | loss: 2.8787088MixupTrain:  epoch  0, batch   409 | loss: 2.7153399MixupTrain:  epoch  0, batch   410 | loss: 2.8108811MixupTrain:  epoch  0, batch   411 | loss: 2.7442448MixupTrain:  epoch  0, batch   412 | loss: 2.8359156MixupTrain:  epoch  0, batch   413 | loss: 2.9324179MixupTrain:  epoch  0, batch   414 | loss: 2.8511167MixupTrain:  epoch  0, batch   415 | loss: 2.8788695MixupTrain:  epoch  0, batch   416 | loss: 2.8268263MixupTrain:  epoch  0, batch   417 | loss: 2.7233629
MemoryTrain:  epoch  0, batch     0 | loss: 1.1416028MemoryTrain:  epoch  0, batch     1 | loss: 1.3795314MemoryTrain:  epoch  0, batch     2 | loss: 1.5646133MemoryTrain:  epoch  0, batch     3 | loss: 1.6198869MemoryTrain:  epoch  0, batch     4 | loss: 1.6243548MemoryTrain:  epoch  0, batch     5 | loss: 1.6355449MemoryTrain:  epoch  1, batch     0 | loss: 1.3744559MemoryTrain:  epoch  1, batch     1 | loss: 1.2921033MemoryTrain:  epoch  1, batch     2 | loss: 1.1996841MemoryTrain:  epoch  1, batch     3 | loss: 1.3009491MemoryTrain:  epoch  1, batch     4 | loss: 1.3554484MemoryTrain:  epoch  1, batch     5 | loss: 1.2230124MemoryTrain:  epoch  2, batch     0 | loss: 1.2469687MemoryTrain:  epoch  2, batch     1 | loss: 1.2250061MemoryTrain:  epoch  2, batch     2 | loss: 1.2323081MemoryTrain:  epoch  2, batch     3 | loss: 1.2679632MemoryTrain:  epoch  2, batch     4 | loss: 1.2639623MemoryTrain:  epoch  2, batch     5 | loss: 1.3300233MemoryTrain:  epoch  3, batch     0 | loss: 1.2444913MemoryTrain:  epoch  3, batch     1 | loss: 1.2071997MemoryTrain:  epoch  3, batch     2 | loss: 1.2105241MemoryTrain:  epoch  3, batch     3 | loss: 1.1953084MemoryTrain:  epoch  3, batch     4 | loss: 1.1814059MemoryTrain:  epoch  3, batch     5 | loss: 1.2058136MemoryTrain:  epoch  4, batch     0 | loss: 1.2124903MemoryTrain:  epoch  4, batch     1 | loss: 1.1986841MemoryTrain:  epoch  4, batch     2 | loss: 1.2112010MemoryTrain:  epoch  4, batch     3 | loss: 1.2178861MemoryTrain:  epoch  4, batch     4 | loss: 1.1820092MemoryTrain:  epoch  4, batch     5 | loss: 1.1749177MemoryTrain:  epoch  5, batch     0 | loss: 1.1913532MemoryTrain:  epoch  5, batch     1 | loss: 1.1760153MemoryTrain:  epoch  5, batch     2 | loss: 1.1673845MemoryTrain:  epoch  5, batch     3 | loss: 1.1941999MemoryTrain:  epoch  5, batch     4 | loss: 1.1960900MemoryTrain:  epoch  5, batch     5 | loss: 1.1596330MemoryTrain:  epoch  6, batch     0 | loss: 1.1635890MemoryTrain:  epoch  6, batch     1 | loss: 1.2146455MemoryTrain:  epoch  6, batch     2 | loss: 1.1832533MemoryTrain:  epoch  6, batch     3 | loss: 1.1761801MemoryTrain:  epoch  6, batch     4 | loss: 1.1961677MemoryTrain:  epoch  6, batch     5 | loss: 1.1774359MemoryTrain:  epoch  7, batch     0 | loss: 1.1837707MemoryTrain:  epoch  7, batch     1 | loss: 1.1722848MemoryTrain:  epoch  7, batch     2 | loss: 1.1740496MemoryTrain:  epoch  7, batch     3 | loss: 1.1980596MemoryTrain:  epoch  7, batch     4 | loss: 1.2100399MemoryTrain:  epoch  7, batch     5 | loss: 1.1825316MemoryTrain:  epoch  8, batch     0 | loss: 1.2063724MemoryTrain:  epoch  8, batch     1 | loss: 1.1795418MemoryTrain:  epoch  8, batch     2 | loss: 1.1962792MemoryTrain:  epoch  8, batch     3 | loss: 1.1775119MemoryTrain:  epoch  8, batch     4 | loss: 1.1771461MemoryTrain:  epoch  8, batch     5 | loss: 1.1601652MemoryTrain:  epoch  9, batch     0 | loss: 1.1752189MemoryTrain:  epoch  9, batch     1 | loss: 1.1871760MemoryTrain:  epoch  9, batch     2 | loss: 1.1972413MemoryTrain:  epoch  9, batch     3 | loss: 1.1817578MemoryTrain:  epoch  9, batch     4 | loss: 1.1591077MemoryTrain:  epoch  9, batch     5 | loss: 1.1841837
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 75.00%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 93.75%,  total acc: 85.94%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 86.25%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 84.82%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 85.94%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 86.98%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   13 | acc: 37.50%,  total acc: 83.93%   
[EVAL] batch:    0 | acc: 43.75%,  total acc: 43.75%   [EVAL] batch:    1 | acc: 37.50%,  total acc: 40.62%   [EVAL] batch:    2 | acc: 31.25%,  total acc: 37.50%   [EVAL] batch:    3 | acc: 43.75%,  total acc: 39.06%   [EVAL] batch:    4 | acc: 31.25%,  total acc: 37.50%   [EVAL] batch:    5 | acc: 25.00%,  total acc: 35.42%   [EVAL] batch:    6 | acc: 18.75%,  total acc: 33.04%   [EVAL] batch:    7 | acc: 12.50%,  total acc: 30.47%   [EVAL] batch:    8 | acc: 43.75%,  total acc: 31.94%   [EVAL] batch:    9 | acc: 31.25%,  total acc: 31.87%   [EVAL] batch:   10 | acc: 25.00%,  total acc: 31.25%   [EVAL] batch:   11 | acc: 31.25%,  total acc: 31.25%   [EVAL] batch:   12 | acc: 12.50%,  total acc: 29.81%   [EVAL] batch:   13 | acc: 31.25%,  total acc: 29.91%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 32.92%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 34.38%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 36.76%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 38.19%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 39.47%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 41.88%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 44.64%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 46.88%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 48.91%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 50.78%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 52.75%   [EVAL] batch:   25 | acc: 93.75%,  total acc: 54.33%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 55.79%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 57.37%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 58.84%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 59.79%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 61.09%   [EVAL] batch:   31 | acc: 87.50%,  total acc: 61.91%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 61.17%   [EVAL] batch:   33 | acc: 6.25%,  total acc: 59.56%   [EVAL] batch:   34 | acc: 6.25%,  total acc: 58.04%   [EVAL] batch:   35 | acc: 25.00%,  total acc: 57.12%   [EVAL] batch:   36 | acc: 6.25%,  total acc: 55.74%   [EVAL] batch:   37 | acc: 6.25%,  total acc: 54.44%   [EVAL] batch:   38 | acc: 12.50%,  total acc: 53.37%   [EVAL] batch:   39 | acc: 68.75%,  total acc: 53.75%   [EVAL] batch:   40 | acc: 31.25%,  total acc: 53.20%   [EVAL] batch:   41 | acc: 31.25%,  total acc: 52.68%   [EVAL] batch:   42 | acc: 37.50%,  total acc: 52.33%   [EVAL] batch:   43 | acc: 68.75%,  total acc: 52.70%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 53.75%   [EVAL] batch:   45 | acc: 93.75%,  total acc: 54.62%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 55.59%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 56.51%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 57.40%   [EVAL] batch:   49 | acc: 75.00%,  total acc: 57.75%   [EVAL] batch:   50 | acc: 87.50%,  total acc: 58.33%   [EVAL] batch:   51 | acc: 68.75%,  total acc: 58.53%   [EVAL] batch:   52 | acc: 81.25%,  total acc: 58.96%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 59.38%   [EVAL] batch:   54 | acc: 68.75%,  total acc: 59.55%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 60.27%   [EVAL] batch:   56 | acc: 87.50%,  total acc: 60.75%   [EVAL] batch:   57 | acc: 100.00%,  total acc: 61.42%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 61.86%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 62.50%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 63.11%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 63.71%   [EVAL] batch:   62 | acc: 100.00%,  total acc: 64.29%   [EVAL] batch:   63 | acc: 100.00%,  total acc: 64.84%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 65.38%   [EVAL] batch:   65 | acc: 43.75%,  total acc: 65.06%   [EVAL] batch:   66 | acc: 68.75%,  total acc: 65.11%   [EVAL] batch:   67 | acc: 75.00%,  total acc: 65.26%   [EVAL] batch:   68 | acc: 81.25%,  total acc: 65.49%   [EVAL] batch:   69 | acc: 75.00%,  total acc: 65.62%   [EVAL] batch:   70 | acc: 62.50%,  total acc: 65.58%   [EVAL] batch:   71 | acc: 56.25%,  total acc: 65.45%   [EVAL] batch:   72 | acc: 50.00%,  total acc: 65.24%   [EVAL] batch:   73 | acc: 62.50%,  total acc: 65.20%   [EVAL] batch:   74 | acc: 50.00%,  total acc: 65.00%   [EVAL] batch:   75 | acc: 87.50%,  total acc: 65.30%   [EVAL] batch:   76 | acc: 81.25%,  total acc: 65.50%   [EVAL] batch:   77 | acc: 93.75%,  total acc: 65.87%   [EVAL] batch:   78 | acc: 75.00%,  total acc: 65.98%   [EVAL] batch:   79 | acc: 100.00%,  total acc: 66.41%   [EVAL] batch:   80 | acc: 43.75%,  total acc: 66.13%   [EVAL] batch:   81 | acc: 25.00%,  total acc: 65.62%   [EVAL] batch:   82 | acc: 43.75%,  total acc: 65.36%   [EVAL] batch:   83 | acc: 62.50%,  total acc: 65.33%   [EVAL] batch:   84 | acc: 50.00%,  total acc: 65.15%   [EVAL] batch:   85 | acc: 75.00%,  total acc: 65.26%   [EVAL] batch:   86 | acc: 81.25%,  total acc: 65.45%   [EVAL] batch:   87 | acc: 93.75%,  total acc: 65.77%   [EVAL] batch:   88 | acc: 87.50%,  total acc: 66.01%   [EVAL] batch:   89 | acc: 93.75%,  total acc: 66.32%   [EVAL] batch:   90 | acc: 87.50%,  total acc: 66.55%   [EVAL] batch:   91 | acc: 75.00%,  total acc: 66.64%   [EVAL] batch:   92 | acc: 93.75%,  total acc: 66.94%   [EVAL] batch:   93 | acc: 100.00%,  total acc: 67.29%   [EVAL] batch:   94 | acc: 87.50%,  total acc: 67.50%   [EVAL] batch:   95 | acc: 87.50%,  total acc: 67.71%   [EVAL] batch:   96 | acc: 81.25%,  total acc: 67.85%   [EVAL] batch:   97 | acc: 93.75%,  total acc: 68.11%   [EVAL] batch:   98 | acc: 31.25%,  total acc: 67.74%   
cur_acc:  ['0.8617', '0.9236', '0.8125', '0.9196', '0.8708', '0.8393']
his_acc:  ['0.8617', '0.8725', '0.7215', '0.6849', '0.6882', '0.6774']
CurrentTrain: epoch  0, batch     0 | loss: 8.5300007CurrentTrain: epoch  0, batch     1 | loss: 8.7693062CurrentTrain: epoch  1, batch     0 | loss: 7.9783278CurrentTrain: epoch  1, batch     1 | loss: 7.5802598CurrentTrain: epoch  2, batch     0 | loss: 7.1587992CurrentTrain: epoch  2, batch     1 | loss: 7.2708273CurrentTrain: epoch  3, batch     0 | loss: 7.0149479CurrentTrain: epoch  3, batch     1 | loss: 6.3425727CurrentTrain: epoch  4, batch     0 | loss: 5.9907017CurrentTrain: epoch  4, batch     1 | loss: 5.9455795CurrentTrain: epoch  5, batch     0 | loss: 5.4469347CurrentTrain: epoch  5, batch     1 | loss: 6.1246614CurrentTrain: epoch  6, batch     0 | loss: 5.3245816CurrentTrain: epoch  6, batch     1 | loss: 4.8445072CurrentTrain: epoch  7, batch     0 | loss: 4.6974163CurrentTrain: epoch  7, batch     1 | loss: 5.1697330CurrentTrain: epoch  8, batch     0 | loss: 4.0080223CurrentTrain: epoch  8, batch     1 | loss: 4.8064890CurrentTrain: epoch  9, batch     0 | loss: 4.6429729CurrentTrain: epoch  9, batch     1 | loss: 4.2902198
#### organization subsidiaries ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization subsidiaries" means an organization 's subsidiary is an organization.
Here is an example:
Relation: organization subsidiaries
Context: venture fund buys sporting chain highland capital 's consumer fund includes lululemon athletica , a yoga retailer , and o beverages , a flavored water company developed by tom first , one of the two `` juice guys '' who cofounded nantucket nectars .
Head Entity: highland capital
Tail Entity: o beverages
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization subsidiaries":
Relation: organization subsidiaries  
Context: The tech giant Alphabet Inc. has expanded its portfolio by acquiring Fitbit, a company known for its fitness tracking devices, which will operate as a subsidiary under the Google brand.  
Head Entity: Alphabet Inc.  
Tail Entity: Fitbit  

Relation: organization subsidiaries  
Context: The automotive manufacturer General Motors announced that its electric vehicle division, which includes the newly formed subsidiary Cruise, will focus on developing self-driving technology.  
Head Entity: General Motors  
Tail Entity: Cruise  
#### organization parents ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization parents" means an organization 's parent is an organization.
Here is an example:
Relation: organization parents
Context: those who try to salvage possessions from the debris of their homes can easily turn into victims too , said dr. irwin redlener , director of the national center for disaster preparedness at columbia university mailman school of medicine .
Head Entity: national center for disaster preparedness
Tail Entity: columbia university mailman school of medicine
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization parents":
Relation: organization parents  
Context: The Massachusetts Institute of Technology, known for its cutting-edge research and innovation, is part of a larger network of educational institutions that includes Harvard University.  
Head Entity: Massachusetts Institute of Technology  
Tail Entity: Harvard University  

Relation: organization parents  
Context: The World Wildlife Fund, a leading organization in conservation efforts, operates under the umbrella of the World Wide Fund for Nature, which has a broader international focus.  
Head Entity: World Wildlife Fund  
Tail Entity: World Wide Fund for Nature  
#### organization alternate names ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization alternate names" means a person 's alias is a person.
Here is an example:
Relation: organization alternate names
Context: it also needs the green light from the 45-nation nuclear suppliers group -lrb- nsg -rrb- , which regulates global civilian nuclear trade , before it can begin buying nuclear reactors and fuel .
Head Entity: nsg
Tail Entity: nuclear suppliers group
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization alternate names":
Relation: organization alternate names  
Context: The International Monetary Fund, often referred to as the IMF, plays a crucial role in global economic stability.  
Head Entity: IMF  
Tail Entity: International Monetary Fund  

Relation: organization alternate names  
Context: The World Health Organization, commonly known as WHO, has been at the forefront of the global response to health crises.  
Head Entity: WHO  
Tail Entity: World Health Organization  
#### organization city of headquarters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization city of headquarters" means an organization is located in a city.
Here is an example:
Relation: organization city of headquarters
Context: ------ london 2008-05-20 07:23:45 utc enodis plc endorses sweetened takeover bid by us company manitowoc illinois tool works of glenville , illinois , which had offered 282 pence -lrb- us$ 551 euro3 54 -rrb- per share , said monday that it was considering its position .
Head Entity: illinois tool works
Tail Entity: glenville
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization city of headquarters":
Relation: organization city of headquarters  
Context: ------ in 2015, tech giant apple inc. announced plans to expand its operations in cupertino, california, where it has been headquartered since its founding.  
Head Entity: apple inc.  
Tail Entity: cupertino  

Relation: organization city of headquarters  
Context: ------ the multinational corporation samsung electronics is based in suwon, south korea, and has been a leader in the technology sector for decades.  
Head Entity: samsung electronics  
Tail Entity: suwon  
#### person siblings ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person siblings" means a person 's sibling is a person.
Here is an example:
Relation: person siblings
Context: `` holly and sanjaya are headed to -lsb- the hawaiian island of -rsb- kauai tomorrow morning so she can meet his parents . ''
Head Entity: she
Tail Entity: sanjaya
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person siblings":
Relation: person siblings  
Context: `` during the family reunion, john introduced his sister to everyone, and they all had a great time reminiscing about their childhood. ''  
Head Entity: his sister  
Tail Entity: john  

Relation: person siblings  
Context: `` after the graduation ceremony, emily celebrated with her brother, who had just completed his degree in engineering. ''  
Head Entity: her brother  
Tail Entity: emily  
Mixup data size:  8545
MixupTrain:  epoch  0, batch     0 | loss: 3.9680698MixupTrain:  epoch  0, batch     1 | loss: 4.0197535MixupTrain:  epoch  0, batch     2 | loss: 3.9645443MixupTrain:  epoch  0, batch     3 | loss: 3.4303339MixupTrain:  epoch  0, batch     4 | loss: 4.0528593MixupTrain:  epoch  0, batch     5 | loss: 3.7473261MixupTrain:  epoch  0, batch     6 | loss: 3.9415216MixupTrain:  epoch  0, batch     7 | loss: 3.9599257MixupTrain:  epoch  0, batch     8 | loss: 4.1167221MixupTrain:  epoch  0, batch     9 | loss: 3.7895486MixupTrain:  epoch  0, batch    10 | loss: 3.6166689MixupTrain:  epoch  0, batch    11 | loss: 3.7405374MixupTrain:  epoch  0, batch    12 | loss: 3.7998023MixupTrain:  epoch  0, batch    13 | loss: 4.2027225MixupTrain:  epoch  0, batch    14 | loss: 3.9482346MixupTrain:  epoch  0, batch    15 | loss: 3.6313035MixupTrain:  epoch  0, batch    16 | loss: 3.7580385MixupTrain:  epoch  0, batch    17 | loss: 3.8605709MixupTrain:  epoch  0, batch    18 | loss: 3.5818980MixupTrain:  epoch  0, batch    19 | loss: 4.0737743MixupTrain:  epoch  0, batch    20 | loss: 3.9776626MixupTrain:  epoch  0, batch    21 | loss: 3.6246161MixupTrain:  epoch  0, batch    22 | loss: 3.9034905MixupTrain:  epoch  0, batch    23 | loss: 3.8686957MixupTrain:  epoch  0, batch    24 | loss: 3.6237986MixupTrain:  epoch  0, batch    25 | loss: 3.0774872MixupTrain:  epoch  0, batch    26 | loss: 3.5045276MixupTrain:  epoch  0, batch    27 | loss: 3.4371839MixupTrain:  epoch  0, batch    28 | loss: 3.7414975MixupTrain:  epoch  0, batch    29 | loss: 3.7207403MixupTrain:  epoch  0, batch    30 | loss: 3.6733692MixupTrain:  epoch  0, batch    31 | loss: 3.6948743MixupTrain:  epoch  0, batch    32 | loss: 3.4685211MixupTrain:  epoch  0, batch    33 | loss: 3.0542026MixupTrain:  epoch  0, batch    34 | loss: 3.8284445MixupTrain:  epoch  0, batch    35 | loss: 3.6325972MixupTrain:  epoch  0, batch    36 | loss: 3.6503813MixupTrain:  epoch  0, batch    37 | loss: 3.1375077MixupTrain:  epoch  0, batch    38 | loss: 3.7084625MixupTrain:  epoch  0, batch    39 | loss: 3.4952140MixupTrain:  epoch  0, batch    40 | loss: 3.4207771MixupTrain:  epoch  0, batch    41 | loss: 3.3273640MixupTrain:  epoch  0, batch    42 | loss: 3.2319665MixupTrain:  epoch  0, batch    43 | loss: 3.2476819MixupTrain:  epoch  0, batch    44 | loss: 3.7057123MixupTrain:  epoch  0, batch    45 | loss: 3.3027968MixupTrain:  epoch  0, batch    46 | loss: 3.1440265MixupTrain:  epoch  0, batch    47 | loss: 3.7914884MixupTrain:  epoch  0, batch    48 | loss: 3.7138650MixupTrain:  epoch  0, batch    49 | loss: 3.8606083MixupTrain:  epoch  0, batch    50 | loss: 3.6702790MixupTrain:  epoch  0, batch    51 | loss: 3.1377094MixupTrain:  epoch  0, batch    52 | loss: 3.2862301MixupTrain:  epoch  0, batch    53 | loss: 3.9060097MixupTrain:  epoch  0, batch    54 | loss: 3.4589248MixupTrain:  epoch  0, batch    55 | loss: 3.7589471MixupTrain:  epoch  0, batch    56 | loss: 3.2927487MixupTrain:  epoch  0, batch    57 | loss: 3.6672544MixupTrain:  epoch  0, batch    58 | loss: 3.4026465MixupTrain:  epoch  0, batch    59 | loss: 3.4037158MixupTrain:  epoch  0, batch    60 | loss: 3.5964260MixupTrain:  epoch  0, batch    61 | loss: 3.6273170MixupTrain:  epoch  0, batch    62 | loss: 3.4361496MixupTrain:  epoch  0, batch    63 | loss: 3.6010377MixupTrain:  epoch  0, batch    64 | loss: 3.1354909MixupTrain:  epoch  0, batch    65 | loss: 4.0802555MixupTrain:  epoch  0, batch    66 | loss: 3.6713161MixupTrain:  epoch  0, batch    67 | loss: 3.5451789MixupTrain:  epoch  0, batch    68 | loss: 3.3006725MixupTrain:  epoch  0, batch    69 | loss: 3.6308336MixupTrain:  epoch  0, batch    70 | loss: 3.4614921MixupTrain:  epoch  0, batch    71 | loss: 3.1897092MixupTrain:  epoch  0, batch    72 | loss: 3.3892138MixupTrain:  epoch  0, batch    73 | loss: 3.4707298MixupTrain:  epoch  0, batch    74 | loss: 3.4801695MixupTrain:  epoch  0, batch    75 | loss: 3.5963478MixupTrain:  epoch  0, batch    76 | loss: 3.1460345MixupTrain:  epoch  0, batch    77 | loss: 3.6848722MixupTrain:  epoch  0, batch    78 | loss: 3.2924860MixupTrain:  epoch  0, batch    79 | loss: 3.0371728MixupTrain:  epoch  0, batch    80 | loss: 3.5788291MixupTrain:  epoch  0, batch    81 | loss: 3.8586226MixupTrain:  epoch  0, batch    82 | loss: 3.0179024MixupTrain:  epoch  0, batch    83 | loss: 3.2201908MixupTrain:  epoch  0, batch    84 | loss: 3.7478228MixupTrain:  epoch  0, batch    85 | loss: 3.6166761MixupTrain:  epoch  0, batch    86 | loss: 3.2807336MixupTrain:  epoch  0, batch    87 | loss: 3.6996837MixupTrain:  epoch  0, batch    88 | loss: 3.3635359MixupTrain:  epoch  0, batch    89 | loss: 3.5541496MixupTrain:  epoch  0, batch    90 | loss: 3.1256683MixupTrain:  epoch  0, batch    91 | loss: 3.4421763MixupTrain:  epoch  0, batch    92 | loss: 3.4998214MixupTrain:  epoch  0, batch    93 | loss: 3.2247593MixupTrain:  epoch  0, batch    94 | loss: 3.2488129MixupTrain:  epoch  0, batch    95 | loss: 3.1706126MixupTrain:  epoch  0, batch    96 | loss: 3.1752005MixupTrain:  epoch  0, batch    97 | loss: 3.3133049MixupTrain:  epoch  0, batch    98 | loss: 2.9294901MixupTrain:  epoch  0, batch    99 | loss: 3.4011841MixupTrain:  epoch  0, batch   100 | loss: 3.2551594MixupTrain:  epoch  0, batch   101 | loss: 3.3230314MixupTrain:  epoch  0, batch   102 | loss: 3.3131132MixupTrain:  epoch  0, batch   103 | loss: 3.2807655MixupTrain:  epoch  0, batch   104 | loss: 3.1099770MixupTrain:  epoch  0, batch   105 | loss: 3.3477206MixupTrain:  epoch  0, batch   106 | loss: 2.9998479MixupTrain:  epoch  0, batch   107 | loss: 3.4011259MixupTrain:  epoch  0, batch   108 | loss: 3.3324871MixupTrain:  epoch  0, batch   109 | loss: 3.5513387MixupTrain:  epoch  0, batch   110 | loss: 3.3816447MixupTrain:  epoch  0, batch   111 | loss: 2.9880955MixupTrain:  epoch  0, batch   112 | loss: 3.3834558MixupTrain:  epoch  0, batch   113 | loss: 3.4580154MixupTrain:  epoch  0, batch   114 | loss: 3.2728279MixupTrain:  epoch  0, batch   115 | loss: 3.0251622MixupTrain:  epoch  0, batch   116 | loss: 3.5757670MixupTrain:  epoch  0, batch   117 | loss: 3.2354169MixupTrain:  epoch  0, batch   118 | loss: 3.3491449MixupTrain:  epoch  0, batch   119 | loss: 3.3812573MixupTrain:  epoch  0, batch   120 | loss: 3.2571840MixupTrain:  epoch  0, batch   121 | loss: 3.2658398MixupTrain:  epoch  0, batch   122 | loss: 3.1010971MixupTrain:  epoch  0, batch   123 | loss: 3.4042354MixupTrain:  epoch  0, batch   124 | loss: 2.9675975MixupTrain:  epoch  0, batch   125 | loss: 3.2290335MixupTrain:  epoch  0, batch   126 | loss: 3.3126967MixupTrain:  epoch  0, batch   127 | loss: 3.3650999MixupTrain:  epoch  0, batch   128 | loss: 3.2286148MixupTrain:  epoch  0, batch   129 | loss: 3.4030404MixupTrain:  epoch  0, batch   130 | loss: 3.3898144MixupTrain:  epoch  0, batch   131 | loss: 3.2646375MixupTrain:  epoch  0, batch   132 | loss: 3.0654073MixupTrain:  epoch  0, batch   133 | loss: 3.4147694MixupTrain:  epoch  0, batch   134 | loss: 3.2909055MixupTrain:  epoch  0, batch   135 | loss: 3.4371104MixupTrain:  epoch  0, batch   136 | loss: 3.1531329MixupTrain:  epoch  0, batch   137 | loss: 3.1947753MixupTrain:  epoch  0, batch   138 | loss: 3.0749648MixupTrain:  epoch  0, batch   139 | loss: 3.0298936MixupTrain:  epoch  0, batch   140 | loss: 3.2739496MixupTrain:  epoch  0, batch   141 | loss: 3.2147865MixupTrain:  epoch  0, batch   142 | loss: 3.3469381MixupTrain:  epoch  0, batch   143 | loss: 3.3707964MixupTrain:  epoch  0, batch   144 | loss: 3.2460489MixupTrain:  epoch  0, batch   145 | loss: 3.2119627MixupTrain:  epoch  0, batch   146 | loss: 3.2454720MixupTrain:  epoch  0, batch   147 | loss: 3.0686226MixupTrain:  epoch  0, batch   148 | loss: 3.2236743MixupTrain:  epoch  0, batch   149 | loss: 3.5172949MixupTrain:  epoch  0, batch   150 | loss: 3.1636803MixupTrain:  epoch  0, batch   151 | loss: 3.1362314MixupTrain:  epoch  0, batch   152 | loss: 3.2173998MixupTrain:  epoch  0, batch   153 | loss: 3.3951473MixupTrain:  epoch  0, batch   154 | loss: 3.2343202MixupTrain:  epoch  0, batch   155 | loss: 3.0869448MixupTrain:  epoch  0, batch   156 | loss: 3.3393929MixupTrain:  epoch  0, batch   157 | loss: 3.1126254MixupTrain:  epoch  0, batch   158 | loss: 3.2680008MixupTrain:  epoch  0, batch   159 | loss: 3.2735953MixupTrain:  epoch  0, batch   160 | loss: 3.2040284MixupTrain:  epoch  0, batch   161 | loss: 3.2833183MixupTrain:  epoch  0, batch   162 | loss: 3.1372967MixupTrain:  epoch  0, batch   163 | loss: 3.1343746MixupTrain:  epoch  0, batch   164 | loss: 3.3079784MixupTrain:  epoch  0, batch   165 | loss: 3.2839932MixupTrain:  epoch  0, batch   166 | loss: 3.1245980MixupTrain:  epoch  0, batch   167 | loss: 2.8469880MixupTrain:  epoch  0, batch   168 | loss: 3.1966815MixupTrain:  epoch  0, batch   169 | loss: 3.1570816MixupTrain:  epoch  0, batch   170 | loss: 3.0482752MixupTrain:  epoch  0, batch   171 | loss: 3.1385684MixupTrain:  epoch  0, batch   172 | loss: 3.2671289MixupTrain:  epoch  0, batch   173 | loss: 3.2425194MixupTrain:  epoch  0, batch   174 | loss: 3.2735887MixupTrain:  epoch  0, batch   175 | loss: 3.1933074MixupTrain:  epoch  0, batch   176 | loss: 3.2063789MixupTrain:  epoch  0, batch   177 | loss: 3.0976517MixupTrain:  epoch  0, batch   178 | loss: 3.1868017MixupTrain:  epoch  0, batch   179 | loss: 3.4193602MixupTrain:  epoch  0, batch   180 | loss: 3.4581740MixupTrain:  epoch  0, batch   181 | loss: 3.2006211MixupTrain:  epoch  0, batch   182 | loss: 3.1028101MixupTrain:  epoch  0, batch   183 | loss: 2.9718940MixupTrain:  epoch  0, batch   184 | loss: 3.0892777MixupTrain:  epoch  0, batch   185 | loss: 3.2262449MixupTrain:  epoch  0, batch   186 | loss: 3.0335493MixupTrain:  epoch  0, batch   187 | loss: 3.1226969MixupTrain:  epoch  0, batch   188 | loss: 3.3075738MixupTrain:  epoch  0, batch   189 | loss: 2.9753430MixupTrain:  epoch  0, batch   190 | loss: 3.0218341MixupTrain:  epoch  0, batch   191 | loss: 3.2051136MixupTrain:  epoch  0, batch   192 | loss: 3.1681657MixupTrain:  epoch  0, batch   193 | loss: 3.1790195MixupTrain:  epoch  0, batch   194 | loss: 3.0365381MixupTrain:  epoch  0, batch   195 | loss: 3.0721931MixupTrain:  epoch  0, batch   196 | loss: 3.2535720MixupTrain:  epoch  0, batch   197 | loss: 3.2476449MixupTrain:  epoch  0, batch   198 | loss: 3.5794640MixupTrain:  epoch  0, batch   199 | loss: 3.0513086MixupTrain:  epoch  0, batch   200 | loss: 2.8858931MixupTrain:  epoch  0, batch   201 | loss: 3.1785774MixupTrain:  epoch  0, batch   202 | loss: 3.1577806MixupTrain:  epoch  0, batch   203 | loss: 2.9315233MixupTrain:  epoch  0, batch   204 | loss: 3.2104793MixupTrain:  epoch  0, batch   205 | loss: 3.2191048MixupTrain:  epoch  0, batch   206 | loss: 3.3770535MixupTrain:  epoch  0, batch   207 | loss: 3.2083340MixupTrain:  epoch  0, batch   208 | loss: 3.0876207MixupTrain:  epoch  0, batch   209 | loss: 2.8400452MixupTrain:  epoch  0, batch   210 | loss: 3.1525836MixupTrain:  epoch  0, batch   211 | loss: 3.4000409MixupTrain:  epoch  0, batch   212 | loss: 3.1358941MixupTrain:  epoch  0, batch   213 | loss: 2.7612915MixupTrain:  epoch  0, batch   214 | loss: 3.1259708MixupTrain:  epoch  0, batch   215 | loss: 3.0071311MixupTrain:  epoch  0, batch   216 | loss: 3.0888736MixupTrain:  epoch  0, batch   217 | loss: 3.1611466MixupTrain:  epoch  0, batch   218 | loss: 3.2967904MixupTrain:  epoch  0, batch   219 | loss: 2.9821734MixupTrain:  epoch  0, batch   220 | loss: 3.1688457MixupTrain:  epoch  0, batch   221 | loss: 3.1002016MixupTrain:  epoch  0, batch   222 | loss: 3.1571138MixupTrain:  epoch  0, batch   223 | loss: 3.2072146MixupTrain:  epoch  0, batch   224 | loss: 3.2152326MixupTrain:  epoch  0, batch   225 | loss: 3.1413603MixupTrain:  epoch  0, batch   226 | loss: 3.2192054MixupTrain:  epoch  0, batch   227 | loss: 3.1667376MixupTrain:  epoch  0, batch   228 | loss: 2.7670455MixupTrain:  epoch  0, batch   229 | loss: 3.0042753MixupTrain:  epoch  0, batch   230 | loss: 3.2947290MixupTrain:  epoch  0, batch   231 | loss: 2.9829016MixupTrain:  epoch  0, batch   232 | loss: 3.0448484MixupTrain:  epoch  0, batch   233 | loss: 3.0658627MixupTrain:  epoch  0, batch   234 | loss: 3.2623825MixupTrain:  epoch  0, batch   235 | loss: 3.0799608MixupTrain:  epoch  0, batch   236 | loss: 2.8876352MixupTrain:  epoch  0, batch   237 | loss: 3.0385623MixupTrain:  epoch  0, batch   238 | loss: 3.1030154MixupTrain:  epoch  0, batch   239 | loss: 2.9990213MixupTrain:  epoch  0, batch   240 | loss: 2.9641194MixupTrain:  epoch  0, batch   241 | loss: 3.1380458MixupTrain:  epoch  0, batch   242 | loss: 3.4093494MixupTrain:  epoch  0, batch   243 | loss: 3.3737955MixupTrain:  epoch  0, batch   244 | loss: 3.2600517MixupTrain:  epoch  0, batch   245 | loss: 2.8560882MixupTrain:  epoch  0, batch   246 | loss: 3.0462947MixupTrain:  epoch  0, batch   247 | loss: 3.1098399MixupTrain:  epoch  0, batch   248 | loss: 3.1792040MixupTrain:  epoch  0, batch   249 | loss: 2.9263186MixupTrain:  epoch  0, batch   250 | loss: 3.0084858MixupTrain:  epoch  0, batch   251 | loss: 3.2421379MixupTrain:  epoch  0, batch   252 | loss: 3.0426838MixupTrain:  epoch  0, batch   253 | loss: 2.9432864MixupTrain:  epoch  0, batch   254 | loss: 3.2865863MixupTrain:  epoch  0, batch   255 | loss: 2.7966757MixupTrain:  epoch  0, batch   256 | loss: 3.3240743MixupTrain:  epoch  0, batch   257 | loss: 2.8552322MixupTrain:  epoch  0, batch   258 | loss: 3.1771669MixupTrain:  epoch  0, batch   259 | loss: 2.9393830MixupTrain:  epoch  0, batch   260 | loss: 2.9895420MixupTrain:  epoch  0, batch   261 | loss: 3.2041457MixupTrain:  epoch  0, batch   262 | loss: 3.0725822MixupTrain:  epoch  0, batch   263 | loss: 3.1036596MixupTrain:  epoch  0, batch   264 | loss: 3.2950847MixupTrain:  epoch  0, batch   265 | loss: 3.1013145MixupTrain:  epoch  0, batch   266 | loss: 3.0021298MixupTrain:  epoch  0, batch   267 | loss: 3.2348638MixupTrain:  epoch  0, batch   268 | loss: 3.4155006MixupTrain:  epoch  0, batch   269 | loss: 3.1801109MixupTrain:  epoch  0, batch   270 | loss: 3.0347548MixupTrain:  epoch  0, batch   271 | loss: 3.2866955MixupTrain:  epoch  0, batch   272 | loss: 3.2420473MixupTrain:  epoch  0, batch   273 | loss: 2.9485288MixupTrain:  epoch  0, batch   274 | loss: 3.1508541MixupTrain:  epoch  0, batch   275 | loss: 3.1286044MixupTrain:  epoch  0, batch   276 | loss: 3.1253707MixupTrain:  epoch  0, batch   277 | loss: 3.0165114MixupTrain:  epoch  0, batch   278 | loss: 3.0391662MixupTrain:  epoch  0, batch   279 | loss: 3.1766660MixupTrain:  epoch  0, batch   280 | loss: 3.0099993MixupTrain:  epoch  0, batch   281 | loss: 3.3642159MixupTrain:  epoch  0, batch   282 | loss: 3.1051853MixupTrain:  epoch  0, batch   283 | loss: 3.1990404MixupTrain:  epoch  0, batch   284 | loss: 3.2161505MixupTrain:  epoch  0, batch   285 | loss: 3.1481276MixupTrain:  epoch  0, batch   286 | loss: 3.2073624MixupTrain:  epoch  0, batch   287 | loss: 2.8287477MixupTrain:  epoch  0, batch   288 | loss: 3.0075877MixupTrain:  epoch  0, batch   289 | loss: 3.1206825MixupTrain:  epoch  0, batch   290 | loss: 3.2679172MixupTrain:  epoch  0, batch   291 | loss: 3.1900923MixupTrain:  epoch  0, batch   292 | loss: 2.9494295MixupTrain:  epoch  0, batch   293 | loss: 3.2059274MixupTrain:  epoch  0, batch   294 | loss: 3.1561358MixupTrain:  epoch  0, batch   295 | loss: 3.1497631MixupTrain:  epoch  0, batch   296 | loss: 3.1085298MixupTrain:  epoch  0, batch   297 | loss: 3.1339703MixupTrain:  epoch  0, batch   298 | loss: 2.9522333MixupTrain:  epoch  0, batch   299 | loss: 3.0964351MixupTrain:  epoch  0, batch   300 | loss: 3.1387424MixupTrain:  epoch  0, batch   301 | loss: 3.2079439MixupTrain:  epoch  0, batch   302 | loss: 3.0113199MixupTrain:  epoch  0, batch   303 | loss: 3.0975580MixupTrain:  epoch  0, batch   304 | loss: 3.0163209MixupTrain:  epoch  0, batch   305 | loss: 2.9188490MixupTrain:  epoch  0, batch   306 | loss: 2.9030685MixupTrain:  epoch  0, batch   307 | loss: 3.1102657MixupTrain:  epoch  0, batch   308 | loss: 3.1283731MixupTrain:  epoch  0, batch   309 | loss: 3.0634863MixupTrain:  epoch  0, batch   310 | loss: 3.0261297MixupTrain:  epoch  0, batch   311 | loss: 3.1051872MixupTrain:  epoch  0, batch   312 | loss: 2.9109907MixupTrain:  epoch  0, batch   313 | loss: 3.1168466MixupTrain:  epoch  0, batch   314 | loss: 3.1509984MixupTrain:  epoch  0, batch   315 | loss: 3.0397511MixupTrain:  epoch  0, batch   316 | loss: 3.4480376MixupTrain:  epoch  0, batch   317 | loss: 3.1141536MixupTrain:  epoch  0, batch   318 | loss: 3.0045471MixupTrain:  epoch  0, batch   319 | loss: 3.0869026MixupTrain:  epoch  0, batch   320 | loss: 2.9508016MixupTrain:  epoch  0, batch   321 | loss: 2.7560787MixupTrain:  epoch  0, batch   322 | loss: 3.1032856MixupTrain:  epoch  0, batch   323 | loss: 3.0339072MixupTrain:  epoch  0, batch   324 | loss: 3.3984840MixupTrain:  epoch  0, batch   325 | loss: 2.7916002MixupTrain:  epoch  0, batch   326 | loss: 3.0780730MixupTrain:  epoch  0, batch   327 | loss: 3.1743751MixupTrain:  epoch  0, batch   328 | loss: 3.0016878MixupTrain:  epoch  0, batch   329 | loss: 3.3093286MixupTrain:  epoch  0, batch   330 | loss: 2.9707603MixupTrain:  epoch  0, batch   331 | loss: 2.8373363MixupTrain:  epoch  0, batch   332 | loss: 3.2284245MixupTrain:  epoch  0, batch   333 | loss: 3.0459051MixupTrain:  epoch  0, batch   334 | loss: 3.1640239MixupTrain:  epoch  0, batch   335 | loss: 2.8390179MixupTrain:  epoch  0, batch   336 | loss: 3.2148912MixupTrain:  epoch  0, batch   337 | loss: 3.0813141MixupTrain:  epoch  0, batch   338 | loss: 3.0445576MixupTrain:  epoch  0, batch   339 | loss: 3.0906725MixupTrain:  epoch  0, batch   340 | loss: 3.0609863MixupTrain:  epoch  0, batch   341 | loss: 2.8872674MixupTrain:  epoch  0, batch   342 | loss: 3.1465969MixupTrain:  epoch  0, batch   343 | loss: 3.4008145MixupTrain:  epoch  0, batch   344 | loss: 2.9479153MixupTrain:  epoch  0, batch   345 | loss: 3.1582484MixupTrain:  epoch  0, batch   346 | loss: 3.1422179MixupTrain:  epoch  0, batch   347 | loss: 3.1528006MixupTrain:  epoch  0, batch   348 | loss: 2.8826976MixupTrain:  epoch  0, batch   349 | loss: 2.9425788MixupTrain:  epoch  0, batch   350 | loss: 2.9749355MixupTrain:  epoch  0, batch   351 | loss: 2.9276223MixupTrain:  epoch  0, batch   352 | loss: 2.9592566MixupTrain:  epoch  0, batch   353 | loss: 3.0397179MixupTrain:  epoch  0, batch   354 | loss: 3.1005421MixupTrain:  epoch  0, batch   355 | loss: 3.1692595MixupTrain:  epoch  0, batch   356 | loss: 2.9113860MixupTrain:  epoch  0, batch   357 | loss: 3.1121578MixupTrain:  epoch  0, batch   358 | loss: 2.9967477MixupTrain:  epoch  0, batch   359 | loss: 3.1003799MixupTrain:  epoch  0, batch   360 | loss: 3.0806131MixupTrain:  epoch  0, batch   361 | loss: 3.2203484MixupTrain:  epoch  0, batch   362 | loss: 3.1485543MixupTrain:  epoch  0, batch   363 | loss: 3.1136732MixupTrain:  epoch  0, batch   364 | loss: 3.1513903MixupTrain:  epoch  0, batch   365 | loss: 3.0529618MixupTrain:  epoch  0, batch   366 | loss: 3.0076842MixupTrain:  epoch  0, batch   367 | loss: 3.1696079MixupTrain:  epoch  0, batch   368 | loss: 2.9385984MixupTrain:  epoch  0, batch   369 | loss: 3.0198784MixupTrain:  epoch  0, batch   370 | loss: 2.9928865MixupTrain:  epoch  0, batch   371 | loss: 3.2087247MixupTrain:  epoch  0, batch   372 | loss: 3.1417263MixupTrain:  epoch  0, batch   373 | loss: 2.8763604MixupTrain:  epoch  0, batch   374 | loss: 3.1121149MixupTrain:  epoch  0, batch   375 | loss: 3.0354939MixupTrain:  epoch  0, batch   376 | loss: 2.8532586MixupTrain:  epoch  0, batch   377 | loss: 2.9828095MixupTrain:  epoch  0, batch   378 | loss: 3.0526686MixupTrain:  epoch  0, batch   379 | loss: 2.8552999MixupTrain:  epoch  0, batch   380 | loss: 3.0167181MixupTrain:  epoch  0, batch   381 | loss: 2.7865419MixupTrain:  epoch  0, batch   382 | loss: 3.1259229MixupTrain:  epoch  0, batch   383 | loss: 3.0600412MixupTrain:  epoch  0, batch   384 | loss: 2.9989915MixupTrain:  epoch  0, batch   385 | loss: 3.3285007MixupTrain:  epoch  0, batch   386 | loss: 2.8957651MixupTrain:  epoch  0, batch   387 | loss: 3.1901412MixupTrain:  epoch  0, batch   388 | loss: 2.9940085MixupTrain:  epoch  0, batch   389 | loss: 2.9755082MixupTrain:  epoch  0, batch   390 | loss: 3.0476418MixupTrain:  epoch  0, batch   391 | loss: 2.9442184MixupTrain:  epoch  0, batch   392 | loss: 3.1299956MixupTrain:  epoch  0, batch   393 | loss: 2.8237414MixupTrain:  epoch  0, batch   394 | loss: 2.9697924MixupTrain:  epoch  0, batch   395 | loss: 2.8281152MixupTrain:  epoch  0, batch   396 | loss: 2.9670458MixupTrain:  epoch  0, batch   397 | loss: 3.0166991MixupTrain:  epoch  0, batch   398 | loss: 3.0118873MixupTrain:  epoch  0, batch   399 | loss: 2.9687073MixupTrain:  epoch  0, batch   400 | loss: 2.8188541MixupTrain:  epoch  0, batch   401 | loss: 2.9801943MixupTrain:  epoch  0, batch   402 | loss: 2.9794207MixupTrain:  epoch  0, batch   403 | loss: 2.9149492MixupTrain:  epoch  0, batch   404 | loss: 3.1869824MixupTrain:  epoch  0, batch   405 | loss: 2.9991832MixupTrain:  epoch  0, batch   406 | loss: 3.0354958MixupTrain:  epoch  0, batch   407 | loss: 2.9699554MixupTrain:  epoch  0, batch   408 | loss: 2.9177070MixupTrain:  epoch  0, batch   409 | loss: 2.9163153MixupTrain:  epoch  0, batch   410 | loss: 3.1415887MixupTrain:  epoch  0, batch   411 | loss: 3.2641575MixupTrain:  epoch  0, batch   412 | loss: 3.1271796MixupTrain:  epoch  0, batch   413 | loss: 3.0291467MixupTrain:  epoch  0, batch   414 | loss: 2.9688787MixupTrain:  epoch  0, batch   415 | loss: 2.9197860MixupTrain:  epoch  0, batch   416 | loss: 3.1442947MixupTrain:  epoch  0, batch   417 | loss: 3.0588307MixupTrain:  epoch  0, batch   418 | loss: 3.1360707MixupTrain:  epoch  0, batch   419 | loss: 2.8840380MixupTrain:  epoch  0, batch   420 | loss: 3.1958256MixupTrain:  epoch  0, batch   421 | loss: 3.0187063MixupTrain:  epoch  0, batch   422 | loss: 2.8150148MixupTrain:  epoch  0, batch   423 | loss: 2.9912431MixupTrain:  epoch  0, batch   424 | loss: 3.0752077MixupTrain:  epoch  0, batch   425 | loss: 3.0061598MixupTrain:  epoch  0, batch   426 | loss: 3.0117671MixupTrain:  epoch  0, batch   427 | loss: 2.8321991MixupTrain:  epoch  0, batch   428 | loss: 2.9061279MixupTrain:  epoch  0, batch   429 | loss: 2.9029753MixupTrain:  epoch  0, batch   430 | loss: 2.9843867MixupTrain:  epoch  0, batch   431 | loss: 3.1219916MixupTrain:  epoch  0, batch   432 | loss: 3.0384555MixupTrain:  epoch  0, batch   433 | loss: 3.2018974MixupTrain:  epoch  0, batch   434 | loss: 3.0663505MixupTrain:  epoch  0, batch   435 | loss: 3.0791690MixupTrain:  epoch  0, batch   436 | loss: 2.7982264MixupTrain:  epoch  0, batch   437 | loss: 3.2063522MixupTrain:  epoch  0, batch   438 | loss: 3.1272435MixupTrain:  epoch  0, batch   439 | loss: 2.9849033MixupTrain:  epoch  0, batch   440 | loss: 3.1204238MixupTrain:  epoch  0, batch   441 | loss: 3.2139554MixupTrain:  epoch  0, batch   442 | loss: 2.9210200MixupTrain:  epoch  0, batch   443 | loss: 3.0622823MixupTrain:  epoch  0, batch   444 | loss: 3.0754056MixupTrain:  epoch  0, batch   445 | loss: 3.2418699MixupTrain:  epoch  0, batch   446 | loss: 3.0074029MixupTrain:  epoch  0, batch   447 | loss: 3.2252254MixupTrain:  epoch  0, batch   448 | loss: 2.8699265MixupTrain:  epoch  0, batch   449 | loss: 3.1064353MixupTrain:  epoch  0, batch   450 | loss: 3.0603557MixupTrain:  epoch  0, batch   451 | loss: 3.1591439MixupTrain:  epoch  0, batch   452 | loss: 3.0346718MixupTrain:  epoch  0, batch   453 | loss: 2.9049642MixupTrain:  epoch  0, batch   454 | loss: 3.0730433MixupTrain:  epoch  0, batch   455 | loss: 2.9402227MixupTrain:  epoch  0, batch   456 | loss: 3.1274118MixupTrain:  epoch  0, batch   457 | loss: 2.9876552MixupTrain:  epoch  0, batch   458 | loss: 3.0389004MixupTrain:  epoch  0, batch   459 | loss: 3.1790938MixupTrain:  epoch  0, batch   460 | loss: 2.9430199MixupTrain:  epoch  0, batch   461 | loss: 3.0608757MixupTrain:  epoch  0, batch   462 | loss: 3.1265182MixupTrain:  epoch  0, batch   463 | loss: 2.9982388MixupTrain:  epoch  0, batch   464 | loss: 2.8975358MixupTrain:  epoch  0, batch   465 | loss: 2.9297192MixupTrain:  epoch  0, batch   466 | loss: 3.1666949MixupTrain:  epoch  0, batch   467 | loss: 3.2144318MixupTrain:  epoch  0, batch   468 | loss: 2.9917884MixupTrain:  epoch  0, batch   469 | loss: 2.9519038MixupTrain:  epoch  0, batch   470 | loss: 3.2570124MixupTrain:  epoch  0, batch   471 | loss: 2.8884482MixupTrain:  epoch  0, batch   472 | loss: 3.0060410MixupTrain:  epoch  0, batch   473 | loss: 3.0270925MixupTrain:  epoch  0, batch   474 | loss: 3.0282125MixupTrain:  epoch  0, batch   475 | loss: 2.9633212MixupTrain:  epoch  0, batch   476 | loss: 3.0558896MixupTrain:  epoch  0, batch   477 | loss: 2.9590762MixupTrain:  epoch  0, batch   478 | loss: 2.9001803MixupTrain:  epoch  0, batch   479 | loss: 3.0292802MixupTrain:  epoch  0, batch   480 | loss: 2.9827023MixupTrain:  epoch  0, batch   481 | loss: 3.0700865MixupTrain:  epoch  0, batch   482 | loss: 3.2502217MixupTrain:  epoch  0, batch   483 | loss: 3.1090891MixupTrain:  epoch  0, batch   484 | loss: 2.8220062MixupTrain:  epoch  0, batch   485 | loss: 2.9452178MixupTrain:  epoch  0, batch   486 | loss: 3.0331402MixupTrain:  epoch  0, batch   487 | loss: 2.9725974MixupTrain:  epoch  0, batch   488 | loss: 3.1380663MixupTrain:  epoch  0, batch   489 | loss: 2.9102402MixupTrain:  epoch  0, batch   490 | loss: 3.0222392MixupTrain:  epoch  0, batch   491 | loss: 3.0018401MixupTrain:  epoch  0, batch   492 | loss: 3.1287451MixupTrain:  epoch  0, batch   493 | loss: 3.0061750MixupTrain:  epoch  0, batch   494 | loss: 3.0601940MixupTrain:  epoch  0, batch   495 | loss: 3.1678703MixupTrain:  epoch  0, batch   496 | loss: 2.9716339MixupTrain:  epoch  0, batch   497 | loss: 2.7988875MixupTrain:  epoch  0, batch   498 | loss: 3.0735102MixupTrain:  epoch  0, batch   499 | loss: 3.0056772MixupTrain:  epoch  0, batch   500 | loss: 2.9059675MixupTrain:  epoch  0, batch   501 | loss: 2.9491565MixupTrain:  epoch  0, batch   502 | loss: 3.0553141MixupTrain:  epoch  0, batch   503 | loss: 3.0066957MixupTrain:  epoch  0, batch   504 | loss: 2.9265482MixupTrain:  epoch  0, batch   505 | loss: 2.9605923MixupTrain:  epoch  0, batch   506 | loss: 2.8324540MixupTrain:  epoch  0, batch   507 | loss: 3.0253651MixupTrain:  epoch  0, batch   508 | loss: 3.1001449MixupTrain:  epoch  0, batch   509 | loss: 3.2324731MixupTrain:  epoch  0, batch   510 | loss: 2.9618230MixupTrain:  epoch  0, batch   511 | loss: 3.1677480MixupTrain:  epoch  0, batch   512 | loss: 2.8807421MixupTrain:  epoch  0, batch   513 | loss: 2.9839792MixupTrain:  epoch  0, batch   514 | loss: 2.9927192MixupTrain:  epoch  0, batch   515 | loss: 3.0359383MixupTrain:  epoch  0, batch   516 | loss: 3.0387237MixupTrain:  epoch  0, batch   517 | loss: 3.1503782MixupTrain:  epoch  0, batch   518 | loss: 2.8890657MixupTrain:  epoch  0, batch   519 | loss: 3.1533237MixupTrain:  epoch  0, batch   520 | loss: 3.0860658MixupTrain:  epoch  0, batch   521 | loss: 3.0942512MixupTrain:  epoch  0, batch   522 | loss: 3.0909629MixupTrain:  epoch  0, batch   523 | loss: 3.1428509MixupTrain:  epoch  0, batch   524 | loss: 2.8835158MixupTrain:  epoch  0, batch   525 | loss: 3.0644894MixupTrain:  epoch  0, batch   526 | loss: 2.9698548MixupTrain:  epoch  0, batch   527 | loss: 3.0366802MixupTrain:  epoch  0, batch   528 | loss: 2.9114738MixupTrain:  epoch  0, batch   529 | loss: 2.8742881MixupTrain:  epoch  0, batch   530 | loss: 3.1690612MixupTrain:  epoch  0, batch   531 | loss: 2.9159629MixupTrain:  epoch  0, batch   532 | loss: 2.9092538MixupTrain:  epoch  0, batch   533 | loss: 2.8832045MixupTrain:  epoch  0, batch   534 | loss: 2.6018512
MemoryTrain:  epoch  0, batch     0 | loss: 1.1545218MemoryTrain:  epoch  0, batch     1 | loss: 1.3247108MemoryTrain:  epoch  0, batch     2 | loss: 1.3872993MemoryTrain:  epoch  0, batch     3 | loss: 1.3827145MemoryTrain:  epoch  0, batch     4 | loss: 1.5126216MemoryTrain:  epoch  0, batch     5 | loss: 1.6526041MemoryTrain:  epoch  0, batch     6 | loss: 1.7941031MemoryTrain:  epoch  1, batch     0 | loss: 1.2516725MemoryTrain:  epoch  1, batch     1 | loss: 1.2247732MemoryTrain:  epoch  1, batch     2 | loss: 1.3002257MemoryTrain:  epoch  1, batch     3 | loss: 1.1970894MemoryTrain:  epoch  1, batch     4 | loss: 1.2996278MemoryTrain:  epoch  1, batch     5 | loss: 1.3483031MemoryTrain:  epoch  1, batch     6 | loss: 1.2395867MemoryTrain:  epoch  2, batch     0 | loss: 1.2673894MemoryTrain:  epoch  2, batch     1 | loss: 1.2466176MemoryTrain:  epoch  2, batch     2 | loss: 1.2026211MemoryTrain:  epoch  2, batch     3 | loss: 1.1868122MemoryTrain:  epoch  2, batch     4 | loss: 1.2390145MemoryTrain:  epoch  2, batch     5 | loss: 1.2192345MemoryTrain:  epoch  2, batch     6 | loss: 1.2072520MemoryTrain:  epoch  3, batch     0 | loss: 1.1950935MemoryTrain:  epoch  3, batch     1 | loss: 1.1824365MemoryTrain:  epoch  3, batch     2 | loss: 1.2233638MemoryTrain:  epoch  3, batch     3 | loss: 1.1815155MemoryTrain:  epoch  3, batch     4 | loss: 1.2028816MemoryTrain:  epoch  3, batch     5 | loss: 1.1825302MemoryTrain:  epoch  3, batch     6 | loss: 1.2117509MemoryTrain:  epoch  4, batch     0 | loss: 1.1696892MemoryTrain:  epoch  4, batch     1 | loss: 1.1821637MemoryTrain:  epoch  4, batch     2 | loss: 1.1704392MemoryTrain:  epoch  4, batch     3 | loss: 1.1644104MemoryTrain:  epoch  4, batch     4 | loss: 1.1815794MemoryTrain:  epoch  4, batch     5 | loss: 1.1714916MemoryTrain:  epoch  4, batch     6 | loss: 1.1866368MemoryTrain:  epoch  5, batch     0 | loss: 1.1782458MemoryTrain:  epoch  5, batch     1 | loss: 1.1829033MemoryTrain:  epoch  5, batch     2 | loss: 1.1900284MemoryTrain:  epoch  5, batch     3 | loss: 1.1937144MemoryTrain:  epoch  5, batch     4 | loss: 1.1972067MemoryTrain:  epoch  5, batch     5 | loss: 1.1551058MemoryTrain:  epoch  5, batch     6 | loss: 1.1723293MemoryTrain:  epoch  6, batch     0 | loss: 1.1672438MemoryTrain:  epoch  6, batch     1 | loss: 1.1569868MemoryTrain:  epoch  6, batch     2 | loss: 1.1753355MemoryTrain:  epoch  6, batch     3 | loss: 1.1554344MemoryTrain:  epoch  6, batch     4 | loss: 1.1792809MemoryTrain:  epoch  6, batch     5 | loss: 1.1688542MemoryTrain:  epoch  6, batch     6 | loss: 1.1736512MemoryTrain:  epoch  7, batch     0 | loss: 1.1933199MemoryTrain:  epoch  7, batch     1 | loss: 1.1689901MemoryTrain:  epoch  7, batch     2 | loss: 1.1668627MemoryTrain:  epoch  7, batch     3 | loss: 1.1650586MemoryTrain:  epoch  7, batch     4 | loss: 1.1532353MemoryTrain:  epoch  7, batch     5 | loss: 1.1637766MemoryTrain:  epoch  7, batch     6 | loss: 1.1559734MemoryTrain:  epoch  8, batch     0 | loss: 1.1575271MemoryTrain:  epoch  8, batch     1 | loss: 1.1822617MemoryTrain:  epoch  8, batch     2 | loss: 1.1780262MemoryTrain:  epoch  8, batch     3 | loss: 1.1766045MemoryTrain:  epoch  8, batch     4 | loss: 1.1795114MemoryTrain:  epoch  8, batch     5 | loss: 1.1884136MemoryTrain:  epoch  8, batch     6 | loss: 1.1348040MemoryTrain:  epoch  9, batch     0 | loss: 1.1535604MemoryTrain:  epoch  9, batch     1 | loss: 1.1600389MemoryTrain:  epoch  9, batch     2 | loss: 1.1708679MemoryTrain:  epoch  9, batch     3 | loss: 1.1521380MemoryTrain:  epoch  9, batch     4 | loss: 1.1550071MemoryTrain:  epoch  9, batch     5 | loss: 1.1393113MemoryTrain:  epoch  9, batch     6 | loss: 1.1414422
[EVAL] batch:    0 | acc: 25.00%,  total acc: 25.00%   [EVAL] batch:    1 | acc: 50.00%,  total acc: 37.50%   [EVAL] batch:    2 | acc: 31.25%,  total acc: 35.42%   [EVAL] batch:    3 | acc: 31.25%,  total acc: 34.38%   [EVAL] batch:    4 | acc: 18.75%,  total acc: 31.25%   [EVAL] batch:    5 | acc: 18.75%,  total acc: 29.17%   [EVAL] batch:    6 | acc: 25.00%,  total acc: 28.57%   [EVAL] batch:    7 | acc: 37.50%,  total acc: 29.69%   [EVAL] batch:    8 | acc: 12.50%,  total acc: 27.78%   [EVAL] batch:    9 | acc: 56.25%,  total acc: 30.63%   [EVAL] batch:   10 | acc: 62.50%,  total acc: 33.52%   [EVAL] batch:   11 | acc: 50.00%,  total acc: 34.90%   [EVAL] batch:   12 | acc: 50.00%,  total acc: 36.06%   [EVAL] batch:   13 | acc: 100.00%,  total acc: 40.62%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 44.17%   [EVAL] batch:   15 | acc: 93.75%,  total acc: 47.27%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 50.00%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 52.43%   [EVAL] batch:   18 | acc: 75.00%,  total acc: 53.62%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 55.31%   [EVAL] batch:   20 | acc: 81.25%,  total acc: 56.55%   [EVAL] batch:   21 | acc: 43.75%,  total acc: 55.97%   
[EVAL] batch:    0 | acc: 37.50%,  total acc: 37.50%   [EVAL] batch:    1 | acc: 25.00%,  total acc: 31.25%   [EVAL] batch:    2 | acc: 25.00%,  total acc: 29.17%   [EVAL] batch:    3 | acc: 31.25%,  total acc: 29.69%   [EVAL] batch:    4 | acc: 6.25%,  total acc: 25.00%   [EVAL] batch:    5 | acc: 25.00%,  total acc: 25.00%   [EVAL] batch:    6 | acc: 18.75%,  total acc: 24.11%   [EVAL] batch:    7 | acc: 12.50%,  total acc: 22.66%   [EVAL] batch:    8 | acc: 25.00%,  total acc: 22.92%   [EVAL] batch:    9 | acc: 25.00%,  total acc: 23.12%   [EVAL] batch:   10 | acc: 0.00%,  total acc: 21.02%   [EVAL] batch:   11 | acc: 12.50%,  total acc: 20.31%   [EVAL] batch:   12 | acc: 6.25%,  total acc: 19.23%   [EVAL] batch:   13 | acc: 31.25%,  total acc: 20.09%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 23.75%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 25.78%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 28.68%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 30.56%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 32.24%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 35.00%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 38.10%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 40.62%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 42.93%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 45.05%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 47.25%   [EVAL] batch:   25 | acc: 93.75%,  total acc: 49.04%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 50.69%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 52.46%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 54.09%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 55.00%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 56.25%   [EVAL] batch:   31 | acc: 81.25%,  total acc: 57.03%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 56.44%   [EVAL] batch:   33 | acc: 0.00%,  total acc: 54.78%   [EVAL] batch:   34 | acc: 0.00%,  total acc: 53.21%   [EVAL] batch:   35 | acc: 12.50%,  total acc: 52.08%   [EVAL] batch:   36 | acc: 0.00%,  total acc: 50.68%   [EVAL] batch:   37 | acc: 6.25%,  total acc: 49.51%   [EVAL] batch:   38 | acc: 12.50%,  total acc: 48.56%   [EVAL] batch:   39 | acc: 75.00%,  total acc: 49.22%   [EVAL] batch:   40 | acc: 31.25%,  total acc: 48.78%   [EVAL] batch:   41 | acc: 31.25%,  total acc: 48.36%   [EVAL] batch:   42 | acc: 31.25%,  total acc: 47.97%   [EVAL] batch:   43 | acc: 62.50%,  total acc: 48.30%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 49.44%   [EVAL] batch:   45 | acc: 93.75%,  total acc: 50.41%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 51.46%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 52.47%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 53.44%   [EVAL] batch:   49 | acc: 81.25%,  total acc: 54.00%   [EVAL] batch:   50 | acc: 81.25%,  total acc: 54.53%   [EVAL] batch:   51 | acc: 62.50%,  total acc: 54.69%   [EVAL] batch:   52 | acc: 56.25%,  total acc: 54.72%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 55.21%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 55.68%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 56.47%   [EVAL] batch:   56 | acc: 87.50%,  total acc: 57.02%   [EVAL] batch:   57 | acc: 100.00%,  total acc: 57.76%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 58.26%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 58.96%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 59.63%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 60.28%   [EVAL] batch:   62 | acc: 100.00%,  total acc: 60.91%   [EVAL] batch:   63 | acc: 100.00%,  total acc: 61.52%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 62.12%   [EVAL] batch:   65 | acc: 31.25%,  total acc: 61.65%   [EVAL] batch:   66 | acc: 6.25%,  total acc: 60.82%   [EVAL] batch:   67 | acc: 37.50%,  total acc: 60.48%   [EVAL] batch:   68 | acc: 81.25%,  total acc: 60.78%   [EVAL] batch:   69 | acc: 75.00%,  total acc: 60.98%   [EVAL] batch:   70 | acc: 68.75%,  total acc: 61.09%   [EVAL] batch:   71 | acc: 50.00%,  total acc: 60.94%   [EVAL] batch:   72 | acc: 75.00%,  total acc: 61.13%   [EVAL] batch:   73 | acc: 75.00%,  total acc: 61.32%   [EVAL] batch:   74 | acc: 62.50%,  total acc: 61.33%   [EVAL] batch:   75 | acc: 87.50%,  total acc: 61.68%   [EVAL] batch:   76 | acc: 81.25%,  total acc: 61.93%   [EVAL] batch:   77 | acc: 93.75%,  total acc: 62.34%   [EVAL] batch:   78 | acc: 68.75%,  total acc: 62.42%   [EVAL] batch:   79 | acc: 100.00%,  total acc: 62.89%   [EVAL] batch:   80 | acc: 25.00%,  total acc: 62.42%   [EVAL] batch:   81 | acc: 6.25%,  total acc: 61.74%   [EVAL] batch:   82 | acc: 0.00%,  total acc: 60.99%   [EVAL] batch:   83 | acc: 6.25%,  total acc: 60.34%   [EVAL] batch:   84 | acc: 6.25%,  total acc: 59.71%   [EVAL] batch:   85 | acc: 68.75%,  total acc: 59.81%   [EVAL] batch:   86 | acc: 18.75%,  total acc: 59.34%   [EVAL] batch:   87 | acc: 6.25%,  total acc: 58.74%   [EVAL] batch:   88 | acc: 6.25%,  total acc: 58.15%   [EVAL] batch:   89 | acc: 0.00%,  total acc: 57.50%   [EVAL] batch:   90 | acc: 18.75%,  total acc: 57.07%   [EVAL] batch:   91 | acc: 37.50%,  total acc: 56.86%   [EVAL] batch:   92 | acc: 93.75%,  total acc: 57.26%   [EVAL] batch:   93 | acc: 93.75%,  total acc: 57.65%   [EVAL] batch:   94 | acc: 81.25%,  total acc: 57.89%   [EVAL] batch:   95 | acc: 87.50%,  total acc: 58.20%   [EVAL] batch:   96 | acc: 81.25%,  total acc: 58.44%   [EVAL] batch:   97 | acc: 81.25%,  total acc: 58.67%   [EVAL] batch:   98 | acc: 31.25%,  total acc: 58.40%   [EVAL] batch:   99 | acc: 43.75%,  total acc: 58.25%   [EVAL] batch:  100 | acc: 56.25%,  total acc: 58.23%   [EVAL] batch:  101 | acc: 12.50%,  total acc: 57.78%   [EVAL] batch:  102 | acc: 25.00%,  total acc: 57.46%   [EVAL] batch:  103 | acc: 25.00%,  total acc: 57.15%   [EVAL] batch:  104 | acc: 18.75%,  total acc: 56.79%   [EVAL] batch:  105 | acc: 25.00%,  total acc: 56.49%   [EVAL] batch:  106 | acc: 31.25%,  total acc: 56.25%   [EVAL] batch:  107 | acc: 43.75%,  total acc: 56.13%   [EVAL] batch:  108 | acc: 56.25%,  total acc: 56.14%   [EVAL] batch:  109 | acc: 43.75%,  total acc: 56.02%   [EVAL] batch:  110 | acc: 56.25%,  total acc: 56.02%   [EVAL] batch:  111 | acc: 68.75%,  total acc: 56.14%   [EVAL] batch:  112 | acc: 93.75%,  total acc: 56.47%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 56.85%   [EVAL] batch:  114 | acc: 93.75%,  total acc: 57.17%   [EVAL] batch:  115 | acc: 87.50%,  total acc: 57.44%   [EVAL] batch:  116 | acc: 93.75%,  total acc: 57.75%   [EVAL] batch:  117 | acc: 75.00%,  total acc: 57.89%   [EVAL] batch:  118 | acc: 87.50%,  total acc: 58.14%   [EVAL] batch:  119 | acc: 87.50%,  total acc: 58.39%   [EVAL] batch:  120 | acc: 6.25%,  total acc: 57.95%   
cur_acc:  ['0.8617', '0.9236', '0.8125', '0.9196', '0.8708', '0.8393', '0.5597']
his_acc:  ['0.8617', '0.8725', '0.7215', '0.6849', '0.6882', '0.6774', '0.5795']
CurrentTrain: epoch  0, batch     0 | loss: 3.5333815CurrentTrain: epoch  0, batch     1 | loss: 4.5994515CurrentTrain: epoch  1, batch     0 | loss: 3.2470160CurrentTrain: epoch  1, batch     1 | loss: 3.0704355CurrentTrain: epoch  2, batch     0 | loss: 2.6685467CurrentTrain: epoch  2, batch     1 | loss: 2.1808476CurrentTrain: epoch  3, batch     0 | loss: 2.2937071CurrentTrain: epoch  3, batch     1 | loss: 1.9789031CurrentTrain: epoch  4, batch     0 | loss: 2.1398878CurrentTrain: epoch  4, batch     1 | loss: 1.9929079CurrentTrain: epoch  5, batch     0 | loss: 1.9881544CurrentTrain: epoch  5, batch     1 | loss: 1.9119812CurrentTrain: epoch  6, batch     0 | loss: 1.9033709CurrentTrain: epoch  6, batch     1 | loss: 1.9301888CurrentTrain: epoch  7, batch     0 | loss: 1.8843514CurrentTrain: epoch  7, batch     1 | loss: 1.8458712CurrentTrain: epoch  8, batch     0 | loss: 1.8230817CurrentTrain: epoch  8, batch     1 | loss: 1.8430562CurrentTrain: epoch  9, batch     0 | loss: 2.6706815CurrentTrain: epoch  9, batch     1 | loss: 2.2518675
#### person cause of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person cause of death" means a person was died of an event.
Here is an example:
Relation: person cause of death
Context: mr scheider had suffered from multiple myeloma for several years , and died of complications from a staph infection , his wife , brenda siemer , said .
Head Entity: mr scheider
Tail Entity: complications from a staph infection
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person cause of death":
Relation: person cause of death  
Context: after battling with lung cancer for over a year, john doe passed away peacefully in his sleep, surrounded by family.  
Head Entity: john doe  
Tail Entity: lung cancer  

Relation: person cause of death  
Context: the renowned scientist, dr. emily white, tragically lost her life due to a car accident while returning from a conference.  
Head Entity: dr. emily white  
Tail Entity: car accident  
#### organization political religious affiliation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization political religious affiliation" means an organization is the member of a religion.
Here is an example:
Relation: organization political religious affiliation
Context: the militia has also suffered from allegations of being involved in clashes with rival shiite militias , particularly the badr corp , the military wing linked to supreme iraqi islamic council , the party led by powerful politician abdel aziz al-hakim .
Head Entity: badr corp
Tail Entity: shiite
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization political religious affiliation":
Relation: organization political religious affiliation  
Context: The Christian Democratic Union has been a significant player in German politics, often aligning itself with various religious groups to promote policies that reflect its Christian values.  
Head Entity: Christian Democratic Union  
Tail Entity: Christian  

Relation: organization political religious affiliation  
Context: The Islamic Society of North America has been actively involved in advocating for Muslim rights and representation in the political landscape of the United States.  
Head Entity: Islamic Society of North America  
Tail Entity: Muslim  
#### organization stateorprovince of headquarters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization stateorprovince of headquarters" means an organization is located in a state or province.
Here is an example:
Relation: organization stateorprovince of headquarters
Context: andrew lebow , an oil trader with mf global in new york , said investors have been discouraged by lower-than-expected oil imports in china and the disappointing growth in the u.s. economy .
Head Entity: mf global
Tail Entity: new york
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization stateorprovince of headquarters":
Relation: organization stateorprovince of headquarters  
Context: the headquarters of tech giant apple inc. is located in cupertino, california, where it has been a hub for innovation and development.  
Head Entity: apple inc.  
Tail Entity: cupertino, california  

Relation: organization stateorprovince of headquarters  
Context: the multinational corporation samsung electronics has its main office situated in suwon, south korea, which plays a crucial role in its global operations.  
Head Entity: samsung electronics  
Tail Entity: suwon, south korea  
#### person other family ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person other family" means a person 's relative is a person.
Here is an example:
Relation: person other family
Context: former american idol contestant , sanjaya malakar and hills starlet , holly montag , have been getting close since she replaced her sister and brother-in-law , heidi montag and spencer pratt , on i 'm a celebrity .
Head Entity: spencer pratt ,
Tail Entity: holly montag
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person other family":
Relation: person other family  
Context: in a recent interview, actress jennifer aniston spoke fondly of her brother, alex aniston, highlighting their close bond and shared childhood memories.  
Head Entity: alex aniston  
Tail Entity: jennifer aniston  

Relation: person other family  
Context: during the family reunion, uncle bob introduced his niece, sarah, to the rest of the family, emphasizing how proud he was of her accomplishments.  
Head Entity: uncle bob  
Tail Entity: sarah  
#### person city of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person city of death" means a person was died in a city.
Here is an example:
Relation: person city of death
Context: irene morgan kirkaldy , 90 , who died of alzheimer 's disease aug. 10 at her home in gloucester , va. , quietly changed history in 1944 when she refused to give up her seat on a crowded greyhound bus to a white couple .
Head Entity: her
Tail Entity: gloucester
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person city of death":
Relation: person city of death  
Context: john smith, a renowned author, passed away on march 5 in his apartment located in new york city, leaving behind a legacy of literary works that inspired many.  
Head Entity: john smith  
Tail Entity: new york city  

Relation: person city of death  
Context: after a long battle with cancer, elizabeth taylor died on march 23 at a hospital in los angeles, where she had spent her final days surrounded by family and friends.  
Head Entity: elizabeth taylor  
Tail Entity: los angeles  
Mixup data size:  10630
MixupTrain:  epoch  0, batch     0 | loss: 3.8362546MixupTrain:  epoch  0, batch     1 | loss: 3.6811471MixupTrain:  epoch  0, batch     2 | loss: 3.2345765MixupTrain:  epoch  0, batch     3 | loss: 3.5734444MixupTrain:  epoch  0, batch     4 | loss: 3.6078770MixupTrain:  epoch  0, batch     5 | loss: 3.3158054MixupTrain:  epoch  0, batch     6 | loss: 3.3148484MixupTrain:  epoch  0, batch     7 | loss: 3.2478924MixupTrain:  epoch  0, batch     8 | loss: 3.8960621MixupTrain:  epoch  0, batch     9 | loss: 3.4889104MixupTrain:  epoch  0, batch    10 | loss: 3.4134607MixupTrain:  epoch  0, batch    11 | loss: 3.4130881MixupTrain:  epoch  0, batch    12 | loss: 3.6457367MixupTrain:  epoch  0, batch    13 | loss: 3.5644202MixupTrain:  epoch  0, batch    14 | loss: 3.0454979MixupTrain:  epoch  0, batch    15 | loss: 3.5966253MixupTrain:  epoch  0, batch    16 | loss: 3.5102527MixupTrain:  epoch  0, batch    17 | loss: 3.1560495MixupTrain:  epoch  0, batch    18 | loss: 3.8329654MixupTrain:  epoch  0, batch    19 | loss: 3.1683583MixupTrain:  epoch  0, batch    20 | loss: 3.6355233MixupTrain:  epoch  0, batch    21 | loss: 3.6068931MixupTrain:  epoch  0, batch    22 | loss: 3.1972086MixupTrain:  epoch  0, batch    23 | loss: 3.5273995MixupTrain:  epoch  0, batch    24 | loss: 3.1586821MixupTrain:  epoch  0, batch    25 | loss: 3.0231397MixupTrain:  epoch  0, batch    26 | loss: 3.3710890MixupTrain:  epoch  0, batch    27 | loss: 3.3229456MixupTrain:  epoch  0, batch    28 | loss: 3.4486685MixupTrain:  epoch  0, batch    29 | loss: 3.3432446MixupTrain:  epoch  0, batch    30 | loss: 3.3101058MixupTrain:  epoch  0, batch    31 | loss: 3.3070598MixupTrain:  epoch  0, batch    32 | loss: 3.2761941MixupTrain:  epoch  0, batch    33 | loss: 3.3986111MixupTrain:  epoch  0, batch    34 | loss: 3.2656677MixupTrain:  epoch  0, batch    35 | loss: 3.4138386MixupTrain:  epoch  0, batch    36 | loss: 3.1037965MixupTrain:  epoch  0, batch    37 | loss: 3.2679157MixupTrain:  epoch  0, batch    38 | loss: 3.1878176MixupTrain:  epoch  0, batch    39 | loss: 3.4073229MixupTrain:  epoch  0, batch    40 | loss: 3.4066443MixupTrain:  epoch  0, batch    41 | loss: 3.7863393MixupTrain:  epoch  0, batch    42 | loss: 3.1244533MixupTrain:  epoch  0, batch    43 | loss: 3.3784962MixupTrain:  epoch  0, batch    44 | loss: 3.2408757MixupTrain:  epoch  0, batch    45 | loss: 3.1981525MixupTrain:  epoch  0, batch    46 | loss: 3.2400393MixupTrain:  epoch  0, batch    47 | loss: 3.0082541MixupTrain:  epoch  0, batch    48 | loss: 3.1691434MixupTrain:  epoch  0, batch    49 | loss: 3.4462032MixupTrain:  epoch  0, batch    50 | loss: 3.2099972MixupTrain:  epoch  0, batch    51 | loss: 3.1553738MixupTrain:  epoch  0, batch    52 | loss: 3.1482294MixupTrain:  epoch  0, batch    53 | loss: 3.1238406MixupTrain:  epoch  0, batch    54 | loss: 3.1004956MixupTrain:  epoch  0, batch    55 | loss: 3.2996268MixupTrain:  epoch  0, batch    56 | loss: 3.3417311MixupTrain:  epoch  0, batch    57 | loss: 3.3181942MixupTrain:  epoch  0, batch    58 | loss: 3.6664667MixupTrain:  epoch  0, batch    59 | loss: 3.1318984MixupTrain:  epoch  0, batch    60 | loss: 3.2550359MixupTrain:  epoch  0, batch    61 | loss: 2.9864061MixupTrain:  epoch  0, batch    62 | loss: 3.1954267MixupTrain:  epoch  0, batch    63 | loss: 3.3110909MixupTrain:  epoch  0, batch    64 | loss: 2.9599185MixupTrain:  epoch  0, batch    65 | loss: 3.3801038MixupTrain:  epoch  0, batch    66 | loss: 2.9263623MixupTrain:  epoch  0, batch    67 | loss: 3.2779899MixupTrain:  epoch  0, batch    68 | loss: 3.1473441MixupTrain:  epoch  0, batch    69 | loss: 3.1128979MixupTrain:  epoch  0, batch    70 | loss: 3.4473960MixupTrain:  epoch  0, batch    71 | loss: 3.1885841MixupTrain:  epoch  0, batch    72 | loss: 3.1628222MixupTrain:  epoch  0, batch    73 | loss: 2.9800322MixupTrain:  epoch  0, batch    74 | loss: 3.1382499MixupTrain:  epoch  0, batch    75 | loss: 3.1837342MixupTrain:  epoch  0, batch    76 | loss: 3.3909755MixupTrain:  epoch  0, batch    77 | loss: 3.2996397MixupTrain:  epoch  0, batch    78 | loss: 3.2797606MixupTrain:  epoch  0, batch    79 | loss: 3.1386440MixupTrain:  epoch  0, batch    80 | loss: 3.1664734MixupTrain:  epoch  0, batch    81 | loss: 3.0617445MixupTrain:  epoch  0, batch    82 | loss: 3.2399759MixupTrain:  epoch  0, batch    83 | loss: 3.0732412MixupTrain:  epoch  0, batch    84 | loss: 3.0720177MixupTrain:  epoch  0, batch    85 | loss: 3.0967712MixupTrain:  epoch  0, batch    86 | loss: 3.1358080MixupTrain:  epoch  0, batch    87 | loss: 2.9546158MixupTrain:  epoch  0, batch    88 | loss: 3.0422575MixupTrain:  epoch  0, batch    89 | loss: 3.2781796MixupTrain:  epoch  0, batch    90 | loss: 3.0949244MixupTrain:  epoch  0, batch    91 | loss: 3.1514616MixupTrain:  epoch  0, batch    92 | loss: 3.1681347MixupTrain:  epoch  0, batch    93 | loss: 2.9948869MixupTrain:  epoch  0, batch    94 | loss: 3.1691532MixupTrain:  epoch  0, batch    95 | loss: 3.0973105MixupTrain:  epoch  0, batch    96 | loss: 3.1343992MixupTrain:  epoch  0, batch    97 | loss: 3.0711722MixupTrain:  epoch  0, batch    98 | loss: 3.0667191MixupTrain:  epoch  0, batch    99 | loss: 3.0536034MixupTrain:  epoch  0, batch   100 | loss: 3.0084863MixupTrain:  epoch  0, batch   101 | loss: 3.1773400MixupTrain:  epoch  0, batch   102 | loss: 3.2455962MixupTrain:  epoch  0, batch   103 | loss: 3.1020699MixupTrain:  epoch  0, batch   104 | loss: 3.0833764MixupTrain:  epoch  0, batch   105 | loss: 2.9780302MixupTrain:  epoch  0, batch   106 | loss: 3.1369202MixupTrain:  epoch  0, batch   107 | loss: 3.3577693MixupTrain:  epoch  0, batch   108 | loss: 3.1341002MixupTrain:  epoch  0, batch   109 | loss: 3.1183293MixupTrain:  epoch  0, batch   110 | loss: 3.2176421MixupTrain:  epoch  0, batch   111 | loss: 3.0457587MixupTrain:  epoch  0, batch   112 | loss: 3.2211308MixupTrain:  epoch  0, batch   113 | loss: 3.1119547MixupTrain:  epoch  0, batch   114 | loss: 2.9991913MixupTrain:  epoch  0, batch   115 | loss: 3.0619080MixupTrain:  epoch  0, batch   116 | loss: 3.1138365MixupTrain:  epoch  0, batch   117 | loss: 3.3199468MixupTrain:  epoch  0, batch   118 | loss: 2.9943519MixupTrain:  epoch  0, batch   119 | loss: 3.0267472MixupTrain:  epoch  0, batch   120 | loss: 3.0956268MixupTrain:  epoch  0, batch   121 | loss: 3.2163842MixupTrain:  epoch  0, batch   122 | loss: 3.1922579MixupTrain:  epoch  0, batch   123 | loss: 3.2037523MixupTrain:  epoch  0, batch   124 | loss: 3.0859971MixupTrain:  epoch  0, batch   125 | loss: 3.0100868MixupTrain:  epoch  0, batch   126 | loss: 2.8231590MixupTrain:  epoch  0, batch   127 | loss: 2.9846954MixupTrain:  epoch  0, batch   128 | loss: 2.9875329MixupTrain:  epoch  0, batch   129 | loss: 2.9403105MixupTrain:  epoch  0, batch   130 | loss: 3.0050881MixupTrain:  epoch  0, batch   131 | loss: 3.0845504MixupTrain:  epoch  0, batch   132 | loss: 3.1508822MixupTrain:  epoch  0, batch   133 | loss: 3.0023222MixupTrain:  epoch  0, batch   134 | loss: 3.0334349MixupTrain:  epoch  0, batch   135 | loss: 3.0287395MixupTrain:  epoch  0, batch   136 | loss: 2.9968467MixupTrain:  epoch  0, batch   137 | loss: 3.0917230MixupTrain:  epoch  0, batch   138 | loss: 3.0267742MixupTrain:  epoch  0, batch   139 | loss: 2.9867702MixupTrain:  epoch  0, batch   140 | loss: 3.1351480MixupTrain:  epoch  0, batch   141 | loss: 2.9880776MixupTrain:  epoch  0, batch   142 | loss: 3.1372461MixupTrain:  epoch  0, batch   143 | loss: 2.9279964MixupTrain:  epoch  0, batch   144 | loss: 2.8856211MixupTrain:  epoch  0, batch   145 | loss: 3.1201525MixupTrain:  epoch  0, batch   146 | loss: 3.1765156MixupTrain:  epoch  0, batch   147 | loss: 2.9494250MixupTrain:  epoch  0, batch   148 | loss: 2.9404521MixupTrain:  epoch  0, batch   149 | loss: 3.2379084MixupTrain:  epoch  0, batch   150 | loss: 2.9879885MixupTrain:  epoch  0, batch   151 | loss: 2.9521658MixupTrain:  epoch  0, batch   152 | loss: 3.0455580MixupTrain:  epoch  0, batch   153 | loss: 2.9700058MixupTrain:  epoch  0, batch   154 | loss: 2.9405966MixupTrain:  epoch  0, batch   155 | loss: 3.0258851MixupTrain:  epoch  0, batch   156 | loss: 2.9261689MixupTrain:  epoch  0, batch   157 | loss: 2.9995058MixupTrain:  epoch  0, batch   158 | loss: 3.0439324MixupTrain:  epoch  0, batch   159 | loss: 3.3272738MixupTrain:  epoch  0, batch   160 | loss: 3.1635590MixupTrain:  epoch  0, batch   161 | loss: 3.2104731MixupTrain:  epoch  0, batch   162 | loss: 3.0161028MixupTrain:  epoch  0, batch   163 | loss: 3.0081217MixupTrain:  epoch  0, batch   164 | loss: 3.2423837MixupTrain:  epoch  0, batch   165 | loss: 2.9527347MixupTrain:  epoch  0, batch   166 | loss: 3.2113385MixupTrain:  epoch  0, batch   167 | loss: 2.9365335MixupTrain:  epoch  0, batch   168 | loss: 2.9346519MixupTrain:  epoch  0, batch   169 | loss: 2.9944434MixupTrain:  epoch  0, batch   170 | loss: 2.9211206MixupTrain:  epoch  0, batch   171 | loss: 3.0825129MixupTrain:  epoch  0, batch   172 | loss: 2.9348204MixupTrain:  epoch  0, batch   173 | loss: 3.0515046MixupTrain:  epoch  0, batch   174 | loss: 2.9972205MixupTrain:  epoch  0, batch   175 | loss: 2.8164878MixupTrain:  epoch  0, batch   176 | loss: 2.9992480MixupTrain:  epoch  0, batch   177 | loss: 3.0828695MixupTrain:  epoch  0, batch   178 | loss: 2.9515648MixupTrain:  epoch  0, batch   179 | loss: 2.9549623MixupTrain:  epoch  0, batch   180 | loss: 3.0022979MixupTrain:  epoch  0, batch   181 | loss: 2.9036102MixupTrain:  epoch  0, batch   182 | loss: 2.9747558MixupTrain:  epoch  0, batch   183 | loss: 2.9408631MixupTrain:  epoch  0, batch   184 | loss: 2.9835494MixupTrain:  epoch  0, batch   185 | loss: 2.8022461MixupTrain:  epoch  0, batch   186 | loss: 2.9765921MixupTrain:  epoch  0, batch   187 | loss: 3.0825601MixupTrain:  epoch  0, batch   188 | loss: 2.9405763MixupTrain:  epoch  0, batch   189 | loss: 3.0849338MixupTrain:  epoch  0, batch   190 | loss: 3.0257578MixupTrain:  epoch  0, batch   191 | loss: 3.1617794MixupTrain:  epoch  0, batch   192 | loss: 2.9592624MixupTrain:  epoch  0, batch   193 | loss: 3.0756712MixupTrain:  epoch  0, batch   194 | loss: 3.1107936MixupTrain:  epoch  0, batch   195 | loss: 2.8491406MixupTrain:  epoch  0, batch   196 | loss: 2.9616704MixupTrain:  epoch  0, batch   197 | loss: 2.9654570MixupTrain:  epoch  0, batch   198 | loss: 3.0528934MixupTrain:  epoch  0, batch   199 | loss: 3.0111797MixupTrain:  epoch  0, batch   200 | loss: 2.8944917MixupTrain:  epoch  0, batch   201 | loss: 3.0169759MixupTrain:  epoch  0, batch   202 | loss: 3.0082912MixupTrain:  epoch  0, batch   203 | loss: 3.0062747MixupTrain:  epoch  0, batch   204 | loss: 2.9524465MixupTrain:  epoch  0, batch   205 | loss: 2.9355683MixupTrain:  epoch  0, batch   206 | loss: 2.8684108MixupTrain:  epoch  0, batch   207 | loss: 3.0595090MixupTrain:  epoch  0, batch   208 | loss: 3.0004930MixupTrain:  epoch  0, batch   209 | loss: 3.0334654MixupTrain:  epoch  0, batch   210 | loss: 2.9304948MixupTrain:  epoch  0, batch   211 | loss: 2.9926898MixupTrain:  epoch  0, batch   212 | loss: 2.9398382MixupTrain:  epoch  0, batch   213 | loss: 3.0130258MixupTrain:  epoch  0, batch   214 | loss: 2.9791241MixupTrain:  epoch  0, batch   215 | loss: 3.0373950MixupTrain:  epoch  0, batch   216 | loss: 3.0033789MixupTrain:  epoch  0, batch   217 | loss: 3.1748354MixupTrain:  epoch  0, batch   218 | loss: 3.2477827MixupTrain:  epoch  0, batch   219 | loss: 3.0562749MixupTrain:  epoch  0, batch   220 | loss: 2.8272958MixupTrain:  epoch  0, batch   221 | loss: 2.9911666MixupTrain:  epoch  0, batch   222 | loss: 2.8915761MixupTrain:  epoch  0, batch   223 | loss: 2.9747570MixupTrain:  epoch  0, batch   224 | loss: 2.9609408MixupTrain:  epoch  0, batch   225 | loss: 3.0488181MixupTrain:  epoch  0, batch   226 | loss: 3.1758137MixupTrain:  epoch  0, batch   227 | loss: 3.1431384MixupTrain:  epoch  0, batch   228 | loss: 3.0588040MixupTrain:  epoch  0, batch   229 | loss: 3.1414437MixupTrain:  epoch  0, batch   230 | loss: 3.0824018MixupTrain:  epoch  0, batch   231 | loss: 3.0144873MixupTrain:  epoch  0, batch   232 | loss: 3.1807020MixupTrain:  epoch  0, batch   233 | loss: 3.0593343MixupTrain:  epoch  0, batch   234 | loss: 2.8936234MixupTrain:  epoch  0, batch   235 | loss: 2.9226699MixupTrain:  epoch  0, batch   236 | loss: 3.0625834MixupTrain:  epoch  0, batch   237 | loss: 3.0160823MixupTrain:  epoch  0, batch   238 | loss: 3.0363164MixupTrain:  epoch  0, batch   239 | loss: 2.9401183MixupTrain:  epoch  0, batch   240 | loss: 2.9792147MixupTrain:  epoch  0, batch   241 | loss: 3.0884035MixupTrain:  epoch  0, batch   242 | loss: 2.9634635MixupTrain:  epoch  0, batch   243 | loss: 2.9101503MixupTrain:  epoch  0, batch   244 | loss: 2.8744011MixupTrain:  epoch  0, batch   245 | loss: 3.0610878MixupTrain:  epoch  0, batch   246 | loss: 2.9938488MixupTrain:  epoch  0, batch   247 | loss: 3.0086460MixupTrain:  epoch  0, batch   248 | loss: 2.9319611MixupTrain:  epoch  0, batch   249 | loss: 2.8947144MixupTrain:  epoch  0, batch   250 | loss: 2.9119406MixupTrain:  epoch  0, batch   251 | loss: 2.8375189MixupTrain:  epoch  0, batch   252 | loss: 3.0267239MixupTrain:  epoch  0, batch   253 | loss: 3.0783706MixupTrain:  epoch  0, batch   254 | loss: 2.8202486MixupTrain:  epoch  0, batch   255 | loss: 2.8289318MixupTrain:  epoch  0, batch   256 | loss: 3.2154417MixupTrain:  epoch  0, batch   257 | loss: 2.9208815MixupTrain:  epoch  0, batch   258 | loss: 3.0437460MixupTrain:  epoch  0, batch   259 | loss: 3.0279899MixupTrain:  epoch  0, batch   260 | loss: 3.0076079MixupTrain:  epoch  0, batch   261 | loss: 3.0650880MixupTrain:  epoch  0, batch   262 | loss: 2.9481664MixupTrain:  epoch  0, batch   263 | loss: 2.9880261MixupTrain:  epoch  0, batch   264 | loss: 2.9653854MixupTrain:  epoch  0, batch   265 | loss: 3.0068998MixupTrain:  epoch  0, batch   266 | loss: 3.1531572MixupTrain:  epoch  0, batch   267 | loss: 2.8771958MixupTrain:  epoch  0, batch   268 | loss: 2.8714674MixupTrain:  epoch  0, batch   269 | loss: 2.9766648MixupTrain:  epoch  0, batch   270 | loss: 2.9210043MixupTrain:  epoch  0, batch   271 | loss: 3.0527315MixupTrain:  epoch  0, batch   272 | loss: 2.9755607MixupTrain:  epoch  0, batch   273 | loss: 2.9725795MixupTrain:  epoch  0, batch   274 | loss: 3.0119584MixupTrain:  epoch  0, batch   275 | loss: 3.0194941MixupTrain:  epoch  0, batch   276 | loss: 3.0135059MixupTrain:  epoch  0, batch   277 | loss: 2.9890699MixupTrain:  epoch  0, batch   278 | loss: 3.0795417MixupTrain:  epoch  0, batch   279 | loss: 3.0037177MixupTrain:  epoch  0, batch   280 | loss: 2.8731384MixupTrain:  epoch  0, batch   281 | loss: 2.9015675MixupTrain:  epoch  0, batch   282 | loss: 2.9744287MixupTrain:  epoch  0, batch   283 | loss: 3.0904846MixupTrain:  epoch  0, batch   284 | loss: 2.8808553MixupTrain:  epoch  0, batch   285 | loss: 2.9050488MixupTrain:  epoch  0, batch   286 | loss: 2.9711306MixupTrain:  epoch  0, batch   287 | loss: 2.8703299MixupTrain:  epoch  0, batch   288 | loss: 3.1354976MixupTrain:  epoch  0, batch   289 | loss: 3.0208280MixupTrain:  epoch  0, batch   290 | loss: 2.8983357MixupTrain:  epoch  0, batch   291 | loss: 2.9890804MixupTrain:  epoch  0, batch   292 | loss: 3.1470189MixupTrain:  epoch  0, batch   293 | loss: 2.9979966MixupTrain:  epoch  0, batch   294 | loss: 2.9547782MixupTrain:  epoch  0, batch   295 | loss: 3.0388269MixupTrain:  epoch  0, batch   296 | loss: 2.9859688MixupTrain:  epoch  0, batch   297 | loss: 2.9540505MixupTrain:  epoch  0, batch   298 | loss: 2.8472180MixupTrain:  epoch  0, batch   299 | loss: 2.9781907MixupTrain:  epoch  0, batch   300 | loss: 2.9129970MixupTrain:  epoch  0, batch   301 | loss: 2.8658104MixupTrain:  epoch  0, batch   302 | loss: 3.0300229MixupTrain:  epoch  0, batch   303 | loss: 2.9542403MixupTrain:  epoch  0, batch   304 | loss: 2.9244218MixupTrain:  epoch  0, batch   305 | loss: 3.0612640MixupTrain:  epoch  0, batch   306 | loss: 2.8713129MixupTrain:  epoch  0, batch   307 | loss: 2.8687067MixupTrain:  epoch  0, batch   308 | loss: 3.0154743MixupTrain:  epoch  0, batch   309 | loss: 2.9336638MixupTrain:  epoch  0, batch   310 | loss: 3.0557947MixupTrain:  epoch  0, batch   311 | loss: 3.0284233MixupTrain:  epoch  0, batch   312 | loss: 3.0057731MixupTrain:  epoch  0, batch   313 | loss: 3.0479012MixupTrain:  epoch  0, batch   314 | loss: 3.0167103MixupTrain:  epoch  0, batch   315 | loss: 3.0006537MixupTrain:  epoch  0, batch   316 | loss: 3.0118237MixupTrain:  epoch  0, batch   317 | loss: 2.9606476MixupTrain:  epoch  0, batch   318 | loss: 2.9106755MixupTrain:  epoch  0, batch   319 | loss: 3.0386755MixupTrain:  epoch  0, batch   320 | loss: 2.9589682MixupTrain:  epoch  0, batch   321 | loss: 3.1820917MixupTrain:  epoch  0, batch   322 | loss: 3.0884180MixupTrain:  epoch  0, batch   323 | loss: 2.9738970MixupTrain:  epoch  0, batch   324 | loss: 2.9686990MixupTrain:  epoch  0, batch   325 | loss: 2.9197249MixupTrain:  epoch  0, batch   326 | loss: 2.7334688MixupTrain:  epoch  0, batch   327 | loss: 2.9388847MixupTrain:  epoch  0, batch   328 | loss: 2.9982440MixupTrain:  epoch  0, batch   329 | loss: 3.0279608MixupTrain:  epoch  0, batch   330 | loss: 2.9697161MixupTrain:  epoch  0, batch   331 | loss: 2.7904267MixupTrain:  epoch  0, batch   332 | loss: 2.9159656MixupTrain:  epoch  0, batch   333 | loss: 2.8094268MixupTrain:  epoch  0, batch   334 | loss: 2.8414407MixupTrain:  epoch  0, batch   335 | loss: 2.9910364MixupTrain:  epoch  0, batch   336 | loss: 3.1540995MixupTrain:  epoch  0, batch   337 | loss: 2.8664432MixupTrain:  epoch  0, batch   338 | loss: 2.9279037MixupTrain:  epoch  0, batch   339 | loss: 2.8745818MixupTrain:  epoch  0, batch   340 | loss: 3.0312712MixupTrain:  epoch  0, batch   341 | loss: 2.9102628MixupTrain:  epoch  0, batch   342 | loss: 2.8364100MixupTrain:  epoch  0, batch   343 | loss: 3.0019507MixupTrain:  epoch  0, batch   344 | loss: 3.0267248MixupTrain:  epoch  0, batch   345 | loss: 2.8931937MixupTrain:  epoch  0, batch   346 | loss: 3.0908365MixupTrain:  epoch  0, batch   347 | loss: 3.0192349MixupTrain:  epoch  0, batch   348 | loss: 2.9359021MixupTrain:  epoch  0, batch   349 | loss: 2.8494976MixupTrain:  epoch  0, batch   350 | loss: 2.9514785MixupTrain:  epoch  0, batch   351 | loss: 3.0119200MixupTrain:  epoch  0, batch   352 | loss: 2.8762100MixupTrain:  epoch  0, batch   353 | loss: 3.1115267MixupTrain:  epoch  0, batch   354 | loss: 3.0535500MixupTrain:  epoch  0, batch   355 | loss: 2.9126649MixupTrain:  epoch  0, batch   356 | loss: 3.0465238MixupTrain:  epoch  0, batch   357 | loss: 3.0027971MixupTrain:  epoch  0, batch   358 | loss: 3.0032296MixupTrain:  epoch  0, batch   359 | loss: 3.0106735MixupTrain:  epoch  0, batch   360 | loss: 2.9564853MixupTrain:  epoch  0, batch   361 | loss: 3.1290584MixupTrain:  epoch  0, batch   362 | loss: 2.7730470MixupTrain:  epoch  0, batch   363 | loss: 2.9489758MixupTrain:  epoch  0, batch   364 | loss: 3.0232325MixupTrain:  epoch  0, batch   365 | loss: 2.7954175MixupTrain:  epoch  0, batch   366 | loss: 2.9860830MixupTrain:  epoch  0, batch   367 | loss: 3.0516949MixupTrain:  epoch  0, batch   368 | loss: 2.9406645MixupTrain:  epoch  0, batch   369 | loss: 3.0216641MixupTrain:  epoch  0, batch   370 | loss: 3.0472405MixupTrain:  epoch  0, batch   371 | loss: 2.9179213MixupTrain:  epoch  0, batch   372 | loss: 2.9595342MixupTrain:  epoch  0, batch   373 | loss: 2.9765108MixupTrain:  epoch  0, batch   374 | loss: 3.0381074MixupTrain:  epoch  0, batch   375 | loss: 2.9247057MixupTrain:  epoch  0, batch   376 | loss: 2.9916391MixupTrain:  epoch  0, batch   377 | loss: 2.9320855MixupTrain:  epoch  0, batch   378 | loss: 2.9952931MixupTrain:  epoch  0, batch   379 | loss: 3.1328120MixupTrain:  epoch  0, batch   380 | loss: 2.8309884MixupTrain:  epoch  0, batch   381 | loss: 2.8328228MixupTrain:  epoch  0, batch   382 | loss: 2.9157214MixupTrain:  epoch  0, batch   383 | loss: 2.7790341MixupTrain:  epoch  0, batch   384 | loss: 3.1497049MixupTrain:  epoch  0, batch   385 | loss: 2.9994090MixupTrain:  epoch  0, batch   386 | loss: 2.9948931MixupTrain:  epoch  0, batch   387 | loss: 2.9576759MixupTrain:  epoch  0, batch   388 | loss: 3.0434694MixupTrain:  epoch  0, batch   389 | loss: 2.8784127MixupTrain:  epoch  0, batch   390 | loss: 2.9874048MixupTrain:  epoch  0, batch   391 | loss: 3.1559761MixupTrain:  epoch  0, batch   392 | loss: 3.0849710MixupTrain:  epoch  0, batch   393 | loss: 3.0642426MixupTrain:  epoch  0, batch   394 | loss: 2.9717686MixupTrain:  epoch  0, batch   395 | loss: 2.8947506MixupTrain:  epoch  0, batch   396 | loss: 2.8668451MixupTrain:  epoch  0, batch   397 | loss: 2.9053593MixupTrain:  epoch  0, batch   398 | loss: 2.8736472MixupTrain:  epoch  0, batch   399 | loss: 2.9789660MixupTrain:  epoch  0, batch   400 | loss: 3.0408778MixupTrain:  epoch  0, batch   401 | loss: 2.9425588MixupTrain:  epoch  0, batch   402 | loss: 3.0872471MixupTrain:  epoch  0, batch   403 | loss: 3.0929313MixupTrain:  epoch  0, batch   404 | loss: 3.0012107MixupTrain:  epoch  0, batch   405 | loss: 3.0931406MixupTrain:  epoch  0, batch   406 | loss: 2.9368067MixupTrain:  epoch  0, batch   407 | loss: 2.9217076MixupTrain:  epoch  0, batch   408 | loss: 2.8948369MixupTrain:  epoch  0, batch   409 | loss: 3.1059942MixupTrain:  epoch  0, batch   410 | loss: 3.0422881MixupTrain:  epoch  0, batch   411 | loss: 2.9454694MixupTrain:  epoch  0, batch   412 | loss: 3.0475280MixupTrain:  epoch  0, batch   413 | loss: 3.0762401MixupTrain:  epoch  0, batch   414 | loss: 3.0758691MixupTrain:  epoch  0, batch   415 | loss: 2.8531778MixupTrain:  epoch  0, batch   416 | loss: 2.8631904MixupTrain:  epoch  0, batch   417 | loss: 2.9507117MixupTrain:  epoch  0, batch   418 | loss: 3.0397010MixupTrain:  epoch  0, batch   419 | loss: 2.9199829MixupTrain:  epoch  0, batch   420 | loss: 2.9881465MixupTrain:  epoch  0, batch   421 | loss: 3.0343556MixupTrain:  epoch  0, batch   422 | loss: 2.8352289MixupTrain:  epoch  0, batch   423 | loss: 3.1424303MixupTrain:  epoch  0, batch   424 | loss: 2.9927416MixupTrain:  epoch  0, batch   425 | loss: 2.8751559MixupTrain:  epoch  0, batch   426 | loss: 2.9178684MixupTrain:  epoch  0, batch   427 | loss: 2.9135370MixupTrain:  epoch  0, batch   428 | loss: 2.8066826MixupTrain:  epoch  0, batch   429 | loss: 3.0162497MixupTrain:  epoch  0, batch   430 | loss: 3.0487270MixupTrain:  epoch  0, batch   431 | loss: 3.0214415MixupTrain:  epoch  0, batch   432 | loss: 2.8364038MixupTrain:  epoch  0, batch   433 | loss: 3.0228050MixupTrain:  epoch  0, batch   434 | loss: 2.9855306MixupTrain:  epoch  0, batch   435 | loss: 2.8538527MixupTrain:  epoch  0, batch   436 | loss: 2.8466234MixupTrain:  epoch  0, batch   437 | loss: 2.9007964MixupTrain:  epoch  0, batch   438 | loss: 3.0619798MixupTrain:  epoch  0, batch   439 | loss: 3.0162826MixupTrain:  epoch  0, batch   440 | loss: 3.1325698MixupTrain:  epoch  0, batch   441 | loss: 2.8435841MixupTrain:  epoch  0, batch   442 | loss: 2.9618061MixupTrain:  epoch  0, batch   443 | loss: 3.1530919MixupTrain:  epoch  0, batch   444 | loss: 2.9804103MixupTrain:  epoch  0, batch   445 | loss: 2.9536562MixupTrain:  epoch  0, batch   446 | loss: 2.9516454MixupTrain:  epoch  0, batch   447 | loss: 3.0576053MixupTrain:  epoch  0, batch   448 | loss: 2.9116077MixupTrain:  epoch  0, batch   449 | loss: 2.9802670MixupTrain:  epoch  0, batch   450 | loss: 3.1655049MixupTrain:  epoch  0, batch   451 | loss: 2.9905663MixupTrain:  epoch  0, batch   452 | loss: 3.0500352MixupTrain:  epoch  0, batch   453 | loss: 2.9051862MixupTrain:  epoch  0, batch   454 | loss: 2.9374449MixupTrain:  epoch  0, batch   455 | loss: 2.8808408MixupTrain:  epoch  0, batch   456 | loss: 3.0119658MixupTrain:  epoch  0, batch   457 | loss: 2.9764473MixupTrain:  epoch  0, batch   458 | loss: 2.9195466MixupTrain:  epoch  0, batch   459 | loss: 2.9930182MixupTrain:  epoch  0, batch   460 | loss: 2.9398146MixupTrain:  epoch  0, batch   461 | loss: 2.9945726MixupTrain:  epoch  0, batch   462 | loss: 2.7982218MixupTrain:  epoch  0, batch   463 | loss: 2.9917448MixupTrain:  epoch  0, batch   464 | loss: 2.9425755MixupTrain:  epoch  0, batch   465 | loss: 3.0499215MixupTrain:  epoch  0, batch   466 | loss: 2.9161725MixupTrain:  epoch  0, batch   467 | loss: 2.9582579MixupTrain:  epoch  0, batch   468 | loss: 3.1038601MixupTrain:  epoch  0, batch   469 | loss: 2.9756567MixupTrain:  epoch  0, batch   470 | loss: 2.9356897MixupTrain:  epoch  0, batch   471 | loss: 2.9629757MixupTrain:  epoch  0, batch   472 | loss: 2.9941661MixupTrain:  epoch  0, batch   473 | loss: 2.9528723MixupTrain:  epoch  0, batch   474 | loss: 2.9183264MixupTrain:  epoch  0, batch   475 | loss: 2.8481169MixupTrain:  epoch  0, batch   476 | loss: 2.9352763MixupTrain:  epoch  0, batch   477 | loss: 2.9905350MixupTrain:  epoch  0, batch   478 | loss: 2.8564444MixupTrain:  epoch  0, batch   479 | loss: 2.8184290MixupTrain:  epoch  0, batch   480 | loss: 2.9181218MixupTrain:  epoch  0, batch   481 | loss: 3.0619221MixupTrain:  epoch  0, batch   482 | loss: 2.9000530MixupTrain:  epoch  0, batch   483 | loss: 2.9317765MixupTrain:  epoch  0, batch   484 | loss: 2.9127355MixupTrain:  epoch  0, batch   485 | loss: 2.9878330MixupTrain:  epoch  0, batch   486 | loss: 2.9185233MixupTrain:  epoch  0, batch   487 | loss: 2.9255021MixupTrain:  epoch  0, batch   488 | loss: 3.0352345MixupTrain:  epoch  0, batch   489 | loss: 3.0637522MixupTrain:  epoch  0, batch   490 | loss: 2.8950438MixupTrain:  epoch  0, batch   491 | loss: 2.9300218MixupTrain:  epoch  0, batch   492 | loss: 2.9566407MixupTrain:  epoch  0, batch   493 | loss: 2.9631593MixupTrain:  epoch  0, batch   494 | loss: 2.9751809MixupTrain:  epoch  0, batch   495 | loss: 2.8769784MixupTrain:  epoch  0, batch   496 | loss: 2.8939977MixupTrain:  epoch  0, batch   497 | loss: 2.8687229MixupTrain:  epoch  0, batch   498 | loss: 2.9062285MixupTrain:  epoch  0, batch   499 | loss: 2.7662270MixupTrain:  epoch  0, batch   500 | loss: 2.9799252MixupTrain:  epoch  0, batch   501 | loss: 2.9042127MixupTrain:  epoch  0, batch   502 | loss: 3.0365312MixupTrain:  epoch  0, batch   503 | loss: 3.0646186MixupTrain:  epoch  0, batch   504 | loss: 2.9815855MixupTrain:  epoch  0, batch   505 | loss: 2.8656042MixupTrain:  epoch  0, batch   506 | loss: 2.9488850MixupTrain:  epoch  0, batch   507 | loss: 3.1366649MixupTrain:  epoch  0, batch   508 | loss: 2.8952830MixupTrain:  epoch  0, batch   509 | loss: 2.8886645MixupTrain:  epoch  0, batch   510 | loss: 2.9626086MixupTrain:  epoch  0, batch   511 | loss: 2.8208370MixupTrain:  epoch  0, batch   512 | loss: 2.9624076MixupTrain:  epoch  0, batch   513 | loss: 2.9831047MixupTrain:  epoch  0, batch   514 | loss: 2.8700190MixupTrain:  epoch  0, batch   515 | loss: 2.9961586MixupTrain:  epoch  0, batch   516 | loss: 2.8143604MixupTrain:  epoch  0, batch   517 | loss: 2.9025097MixupTrain:  epoch  0, batch   518 | loss: 2.9633155MixupTrain:  epoch  0, batch   519 | loss: 3.0248137MixupTrain:  epoch  0, batch   520 | loss: 3.0094988MixupTrain:  epoch  0, batch   521 | loss: 2.9866548MixupTrain:  epoch  0, batch   522 | loss: 2.9359355MixupTrain:  epoch  0, batch   523 | loss: 2.9443185MixupTrain:  epoch  0, batch   524 | loss: 2.8666992MixupTrain:  epoch  0, batch   525 | loss: 3.1004529MixupTrain:  epoch  0, batch   526 | loss: 3.0587268MixupTrain:  epoch  0, batch   527 | loss: 2.8985515MixupTrain:  epoch  0, batch   528 | loss: 2.9325814MixupTrain:  epoch  0, batch   529 | loss: 3.0251505MixupTrain:  epoch  0, batch   530 | loss: 3.1010437MixupTrain:  epoch  0, batch   531 | loss: 2.9164181MixupTrain:  epoch  0, batch   532 | loss: 2.9104469MixupTrain:  epoch  0, batch   533 | loss: 3.1227179MixupTrain:  epoch  0, batch   534 | loss: 2.9918468MixupTrain:  epoch  0, batch   535 | loss: 2.8660188MixupTrain:  epoch  0, batch   536 | loss: 2.8178921MixupTrain:  epoch  0, batch   537 | loss: 2.9294474MixupTrain:  epoch  0, batch   538 | loss: 3.0340359MixupTrain:  epoch  0, batch   539 | loss: 3.0275993MixupTrain:  epoch  0, batch   540 | loss: 3.0231624MixupTrain:  epoch  0, batch   541 | loss: 2.9176393MixupTrain:  epoch  0, batch   542 | loss: 2.9674621MixupTrain:  epoch  0, batch   543 | loss: 3.0547104MixupTrain:  epoch  0, batch   544 | loss: 2.9375997MixupTrain:  epoch  0, batch   545 | loss: 2.9885190MixupTrain:  epoch  0, batch   546 | loss: 2.9497981MixupTrain:  epoch  0, batch   547 | loss: 2.7719941MixupTrain:  epoch  0, batch   548 | loss: 2.9323258MixupTrain:  epoch  0, batch   549 | loss: 3.0125060MixupTrain:  epoch  0, batch   550 | loss: 2.8361123MixupTrain:  epoch  0, batch   551 | loss: 3.0504818MixupTrain:  epoch  0, batch   552 | loss: 2.9253521MixupTrain:  epoch  0, batch   553 | loss: 2.8258057MixupTrain:  epoch  0, batch   554 | loss: 2.9107666MixupTrain:  epoch  0, batch   555 | loss: 3.0118098MixupTrain:  epoch  0, batch   556 | loss: 2.9139490MixupTrain:  epoch  0, batch   557 | loss: 2.9753754MixupTrain:  epoch  0, batch   558 | loss: 2.9588504MixupTrain:  epoch  0, batch   559 | loss: 2.9980860MixupTrain:  epoch  0, batch   560 | loss: 2.9786277MixupTrain:  epoch  0, batch   561 | loss: 3.0237207MixupTrain:  epoch  0, batch   562 | loss: 2.8722956MixupTrain:  epoch  0, batch   563 | loss: 3.0000184MixupTrain:  epoch  0, batch   564 | loss: 2.9755738MixupTrain:  epoch  0, batch   565 | loss: 3.0791962MixupTrain:  epoch  0, batch   566 | loss: 3.0183687MixupTrain:  epoch  0, batch   567 | loss: 2.9522176MixupTrain:  epoch  0, batch   568 | loss: 2.9938886MixupTrain:  epoch  0, batch   569 | loss: 2.8545923MixupTrain:  epoch  0, batch   570 | loss: 2.8280013MixupTrain:  epoch  0, batch   571 | loss: 2.9170732MixupTrain:  epoch  0, batch   572 | loss: 2.9853551MixupTrain:  epoch  0, batch   573 | loss: 2.9167919MixupTrain:  epoch  0, batch   574 | loss: 2.8909349MixupTrain:  epoch  0, batch   575 | loss: 2.8740864MixupTrain:  epoch  0, batch   576 | loss: 3.0628469MixupTrain:  epoch  0, batch   577 | loss: 2.9742739MixupTrain:  epoch  0, batch   578 | loss: 3.0360403MixupTrain:  epoch  0, batch   579 | loss: 2.9591551MixupTrain:  epoch  0, batch   580 | loss: 2.8661520MixupTrain:  epoch  0, batch   581 | loss: 2.9207699MixupTrain:  epoch  0, batch   582 | loss: 2.9611235MixupTrain:  epoch  0, batch   583 | loss: 2.9865842MixupTrain:  epoch  0, batch   584 | loss: 2.9732895MixupTrain:  epoch  0, batch   585 | loss: 2.9079947MixupTrain:  epoch  0, batch   586 | loss: 2.9452240MixupTrain:  epoch  0, batch   587 | loss: 2.9410119MixupTrain:  epoch  0, batch   588 | loss: 2.8381755MixupTrain:  epoch  0, batch   589 | loss: 2.8747759MixupTrain:  epoch  0, batch   590 | loss: 2.8981881MixupTrain:  epoch  0, batch   591 | loss: 2.9254208MixupTrain:  epoch  0, batch   592 | loss: 2.8858571MixupTrain:  epoch  0, batch   593 | loss: 2.9431057MixupTrain:  epoch  0, batch   594 | loss: 3.0445113MixupTrain:  epoch  0, batch   595 | loss: 2.9912744MixupTrain:  epoch  0, batch   596 | loss: 2.9059196MixupTrain:  epoch  0, batch   597 | loss: 2.9509058MixupTrain:  epoch  0, batch   598 | loss: 3.0126433MixupTrain:  epoch  0, batch   599 | loss: 2.9833312MixupTrain:  epoch  0, batch   600 | loss: 2.9632273MixupTrain:  epoch  0, batch   601 | loss: 2.9253092MixupTrain:  epoch  0, batch   602 | loss: 2.8853185MixupTrain:  epoch  0, batch   603 | loss: 2.8183665MixupTrain:  epoch  0, batch   604 | loss: 3.0151951MixupTrain:  epoch  0, batch   605 | loss: 3.0451975MixupTrain:  epoch  0, batch   606 | loss: 3.0005779MixupTrain:  epoch  0, batch   607 | loss: 2.9048800MixupTrain:  epoch  0, batch   608 | loss: 2.9678860MixupTrain:  epoch  0, batch   609 | loss: 2.8613572MixupTrain:  epoch  0, batch   610 | loss: 2.9031126MixupTrain:  epoch  0, batch   611 | loss: 2.9240928MixupTrain:  epoch  0, batch   612 | loss: 2.9950211MixupTrain:  epoch  0, batch   613 | loss: 2.9229884MixupTrain:  epoch  0, batch   614 | loss: 2.8819232MixupTrain:  epoch  0, batch   615 | loss: 2.9818957MixupTrain:  epoch  0, batch   616 | loss: 3.0187764MixupTrain:  epoch  0, batch   617 | loss: 3.0239420MixupTrain:  epoch  0, batch   618 | loss: 2.9522414MixupTrain:  epoch  0, batch   619 | loss: 3.0623388MixupTrain:  epoch  0, batch   620 | loss: 2.9702668MixupTrain:  epoch  0, batch   621 | loss: 3.0849547MixupTrain:  epoch  0, batch   622 | loss: 2.7752573MixupTrain:  epoch  0, batch   623 | loss: 2.9809003MixupTrain:  epoch  0, batch   624 | loss: 2.9647677MixupTrain:  epoch  0, batch   625 | loss: 2.9313724MixupTrain:  epoch  0, batch   626 | loss: 3.0559554MixupTrain:  epoch  0, batch   627 | loss: 2.9313142MixupTrain:  epoch  0, batch   628 | loss: 2.7810111MixupTrain:  epoch  0, batch   629 | loss: 2.8115621MixupTrain:  epoch  0, batch   630 | loss: 3.1105785MixupTrain:  epoch  0, batch   631 | loss: 2.9716926MixupTrain:  epoch  0, batch   632 | loss: 2.9254186MixupTrain:  epoch  0, batch   633 | loss: 2.8680782MixupTrain:  epoch  0, batch   634 | loss: 2.9309707MixupTrain:  epoch  0, batch   635 | loss: 2.9628892MixupTrain:  epoch  0, batch   636 | loss: 2.9241168MixupTrain:  epoch  0, batch   637 | loss: 2.8833666MixupTrain:  epoch  0, batch   638 | loss: 2.8808560MixupTrain:  epoch  0, batch   639 | loss: 2.8036418MixupTrain:  epoch  0, batch   640 | loss: 2.8358207MixupTrain:  epoch  0, batch   641 | loss: 2.9524674MixupTrain:  epoch  0, batch   642 | loss: 2.8815956MixupTrain:  epoch  0, batch   643 | loss: 2.9058485MixupTrain:  epoch  0, batch   644 | loss: 2.9701471MixupTrain:  epoch  0, batch   645 | loss: 2.9886599MixupTrain:  epoch  0, batch   646 | loss: 2.9132550MixupTrain:  epoch  0, batch   647 | loss: 2.8537865MixupTrain:  epoch  0, batch   648 | loss: 2.8808997MixupTrain:  epoch  0, batch   649 | loss: 2.9866128MixupTrain:  epoch  0, batch   650 | loss: 3.0075827MixupTrain:  epoch  0, batch   651 | loss: 2.9641633MixupTrain:  epoch  0, batch   652 | loss: 2.9187856MixupTrain:  epoch  0, batch   653 | loss: 2.8655801MixupTrain:  epoch  0, batch   654 | loss: 3.1025419MixupTrain:  epoch  0, batch   655 | loss: 3.0227008MixupTrain:  epoch  0, batch   656 | loss: 3.0037973MixupTrain:  epoch  0, batch   657 | loss: 2.9688892MixupTrain:  epoch  0, batch   658 | loss: 2.9547200MixupTrain:  epoch  0, batch   659 | loss: 2.9181156MixupTrain:  epoch  0, batch   660 | loss: 2.8899417MixupTrain:  epoch  0, batch   661 | loss: 2.9235258MixupTrain:  epoch  0, batch   662 | loss: 2.8919554MixupTrain:  epoch  0, batch   663 | loss: 2.8614426MixupTrain:  epoch  0, batch   664 | loss: 2.9429176
MemoryTrain:  epoch  0, batch     0 | loss: 1.1939385MemoryTrain:  epoch  0, batch     1 | loss: 1.4315833MemoryTrain:  epoch  0, batch     2 | loss: 1.4117444MemoryTrain:  epoch  0, batch     3 | loss: 1.5589383MemoryTrain:  epoch  0, batch     4 | loss: 1.5933756MemoryTrain:  epoch  0, batch     5 | loss: 1.5463865MemoryTrain:  epoch  0, batch     6 | loss: 1.9797013MemoryTrain:  epoch  0, batch     7 | loss: 1.7636030MemoryTrain:  epoch  1, batch     0 | loss: 1.2914255MemoryTrain:  epoch  1, batch     1 | loss: 1.3214314MemoryTrain:  epoch  1, batch     2 | loss: 1.1727576MemoryTrain:  epoch  1, batch     3 | loss: 1.3826053MemoryTrain:  epoch  1, batch     4 | loss: 1.1973296MemoryTrain:  epoch  1, batch     5 | loss: 1.2196432MemoryTrain:  epoch  1, batch     6 | loss: 1.3009474MemoryTrain:  epoch  1, batch     7 | loss: 1.1541775MemoryTrain:  epoch  2, batch     0 | loss: 1.2861974MemoryTrain:  epoch  2, batch     1 | loss: 1.1634148MemoryTrain:  epoch  2, batch     2 | loss: 1.2366148MemoryTrain:  epoch  2, batch     3 | loss: 1.3633788MemoryTrain:  epoch  2, batch     4 | loss: 1.2909566MemoryTrain:  epoch  2, batch     5 | loss: 1.2690051MemoryTrain:  epoch  2, batch     6 | loss: 1.1698946MemoryTrain:  epoch  2, batch     7 | loss: 1.1785352MemoryTrain:  epoch  3, batch     0 | loss: 1.2032585MemoryTrain:  epoch  3, batch     1 | loss: 1.2261186MemoryTrain:  epoch  3, batch     2 | loss: 1.1796863MemoryTrain:  epoch  3, batch     3 | loss: 1.1593580MemoryTrain:  epoch  3, batch     4 | loss: 1.1659181MemoryTrain:  epoch  3, batch     5 | loss: 1.1868883MemoryTrain:  epoch  3, batch     6 | loss: 1.1650927MemoryTrain:  epoch  3, batch     7 | loss: 1.1551538MemoryTrain:  epoch  4, batch     0 | loss: 1.1723139MemoryTrain:  epoch  4, batch     1 | loss: 1.1973556MemoryTrain:  epoch  4, batch     2 | loss: 1.1989825MemoryTrain:  epoch  4, batch     3 | loss: 1.1902355MemoryTrain:  epoch  4, batch     4 | loss: 1.1623833MemoryTrain:  epoch  4, batch     5 | loss: 1.1570870MemoryTrain:  epoch  4, batch     6 | loss: 1.1655911MemoryTrain:  epoch  4, batch     7 | loss: 1.1971346MemoryTrain:  epoch  5, batch     0 | loss: 1.1422037MemoryTrain:  epoch  5, batch     1 | loss: 1.1795175MemoryTrain:  epoch  5, batch     2 | loss: 1.1743550MemoryTrain:  epoch  5, batch     3 | loss: 1.1591014MemoryTrain:  epoch  5, batch     4 | loss: 1.1865400MemoryTrain:  epoch  5, batch     5 | loss: 1.1587746MemoryTrain:  epoch  5, batch     6 | loss: 1.1817462MemoryTrain:  epoch  5, batch     7 | loss: 1.1915401MemoryTrain:  epoch  6, batch     0 | loss: 1.1446368MemoryTrain:  epoch  6, batch     1 | loss: 1.1745026MemoryTrain:  epoch  6, batch     2 | loss: 1.1674212MemoryTrain:  epoch  6, batch     3 | loss: 1.1663671MemoryTrain:  epoch  6, batch     4 | loss: 1.1805849MemoryTrain:  epoch  6, batch     5 | loss: 1.1820363MemoryTrain:  epoch  6, batch     6 | loss: 1.1614670MemoryTrain:  epoch  6, batch     7 | loss: 1.1581850MemoryTrain:  epoch  7, batch     0 | loss: 1.1843948MemoryTrain:  epoch  7, batch     1 | loss: 1.1775930MemoryTrain:  epoch  7, batch     2 | loss: 1.1539800MemoryTrain:  epoch  7, batch     3 | loss: 1.1812676MemoryTrain:  epoch  7, batch     4 | loss: 1.1595626MemoryTrain:  epoch  7, batch     5 | loss: 1.1793592MemoryTrain:  epoch  7, batch     6 | loss: 1.1485028MemoryTrain:  epoch  7, batch     7 | loss: 1.1344032MemoryTrain:  epoch  8, batch     0 | loss: 1.1326699MemoryTrain:  epoch  8, batch     1 | loss: 1.1421704MemoryTrain:  epoch  8, batch     2 | loss: 1.1578816MemoryTrain:  epoch  8, batch     3 | loss: 1.1469326MemoryTrain:  epoch  8, batch     4 | loss: 1.2136811MemoryTrain:  epoch  8, batch     5 | loss: 1.1980464MemoryTrain:  epoch  8, batch     6 | loss: 1.1521389MemoryTrain:  epoch  8, batch     7 | loss: 1.1572341MemoryTrain:  epoch  9, batch     0 | loss: 1.1521621MemoryTrain:  epoch  9, batch     1 | loss: 1.1663303MemoryTrain:  epoch  9, batch     2 | loss: 1.1671383MemoryTrain:  epoch  9, batch     3 | loss: 1.1495411MemoryTrain:  epoch  9, batch     4 | loss: 1.1666024MemoryTrain:  epoch  9, batch     5 | loss: 1.1599381MemoryTrain:  epoch  9, batch     6 | loss: 1.1600351MemoryTrain:  epoch  9, batch     7 | loss: 1.1312838
[EVAL] batch:    0 | acc: 31.25%,  total acc: 31.25%   [EVAL] batch:    1 | acc: 56.25%,  total acc: 43.75%   [EVAL] batch:    2 | acc: 56.25%,  total acc: 47.92%   [EVAL] batch:    3 | acc: 62.50%,  total acc: 51.56%   [EVAL] batch:    4 | acc: 100.00%,  total acc: 61.25%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 67.71%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 71.43%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 73.44%   [EVAL] batch:    8 | acc: 62.50%,  total acc: 72.22%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 73.12%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 74.43%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 75.52%   [EVAL] batch:   12 | acc: 50.00%,  total acc: 73.56%   
[EVAL] batch:    0 | acc: 31.25%,  total acc: 31.25%   [EVAL] batch:    1 | acc: 31.25%,  total acc: 31.25%   [EVAL] batch:    2 | acc: 25.00%,  total acc: 29.17%   [EVAL] batch:    3 | acc: 25.00%,  total acc: 28.12%   [EVAL] batch:    4 | acc: 6.25%,  total acc: 23.75%   [EVAL] batch:    5 | acc: 18.75%,  total acc: 22.92%   [EVAL] batch:    6 | acc: 18.75%,  total acc: 22.32%   [EVAL] batch:    7 | acc: 18.75%,  total acc: 21.88%   [EVAL] batch:    8 | acc: 18.75%,  total acc: 21.53%   [EVAL] batch:    9 | acc: 25.00%,  total acc: 21.88%   [EVAL] batch:   10 | acc: 6.25%,  total acc: 20.45%   [EVAL] batch:   11 | acc: 18.75%,  total acc: 20.31%   [EVAL] batch:   12 | acc: 6.25%,  total acc: 19.23%   [EVAL] batch:   13 | acc: 31.25%,  total acc: 20.09%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 23.75%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 25.78%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 28.68%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 30.56%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 32.24%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 35.00%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 38.10%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 40.62%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 42.93%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 45.05%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 47.25%   [EVAL] batch:   25 | acc: 93.75%,  total acc: 49.04%   [EVAL] batch:   26 | acc: 81.25%,  total acc: 50.23%   [EVAL] batch:   27 | acc: 50.00%,  total acc: 50.22%   [EVAL] batch:   28 | acc: 68.75%,  total acc: 50.86%   [EVAL] batch:   29 | acc: 50.00%,  total acc: 50.83%   [EVAL] batch:   30 | acc: 56.25%,  total acc: 51.01%   [EVAL] batch:   31 | acc: 50.00%,  total acc: 50.98%   [EVAL] batch:   32 | acc: 31.25%,  total acc: 50.38%   [EVAL] batch:   33 | acc: 0.00%,  total acc: 48.90%   [EVAL] batch:   34 | acc: 0.00%,  total acc: 47.50%   [EVAL] batch:   35 | acc: 0.00%,  total acc: 46.18%   [EVAL] batch:   36 | acc: 0.00%,  total acc: 44.93%   [EVAL] batch:   37 | acc: 0.00%,  total acc: 43.75%   [EVAL] batch:   38 | acc: 6.25%,  total acc: 42.79%   [EVAL] batch:   39 | acc: 62.50%,  total acc: 43.28%   [EVAL] batch:   40 | acc: 12.50%,  total acc: 42.53%   [EVAL] batch:   41 | acc: 31.25%,  total acc: 42.26%   [EVAL] batch:   42 | acc: 37.50%,  total acc: 42.15%   [EVAL] batch:   43 | acc: 68.75%,  total acc: 42.76%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 44.03%   [EVAL] batch:   45 | acc: 93.75%,  total acc: 45.11%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 46.28%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 47.40%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 48.47%   [EVAL] batch:   49 | acc: 81.25%,  total acc: 49.12%   [EVAL] batch:   50 | acc: 87.50%,  total acc: 49.88%   [EVAL] batch:   51 | acc: 62.50%,  total acc: 50.12%   [EVAL] batch:   52 | acc: 56.25%,  total acc: 50.24%   [EVAL] batch:   53 | acc: 87.50%,  total acc: 50.93%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 51.70%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 52.57%   [EVAL] batch:   56 | acc: 87.50%,  total acc: 53.18%   [EVAL] batch:   57 | acc: 100.00%,  total acc: 53.99%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 54.66%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 55.42%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 56.15%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 56.85%   [EVAL] batch:   62 | acc: 100.00%,  total acc: 57.54%   [EVAL] batch:   63 | acc: 100.00%,  total acc: 58.20%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 58.85%   [EVAL] batch:   65 | acc: 31.25%,  total acc: 58.43%   [EVAL] batch:   66 | acc: 6.25%,  total acc: 57.65%   [EVAL] batch:   67 | acc: 31.25%,  total acc: 57.26%   [EVAL] batch:   68 | acc: 93.75%,  total acc: 57.79%   [EVAL] batch:   69 | acc: 81.25%,  total acc: 58.13%   [EVAL] batch:   70 | acc: 68.75%,  total acc: 58.27%   [EVAL] batch:   71 | acc: 37.50%,  total acc: 57.99%   [EVAL] batch:   72 | acc: 68.75%,  total acc: 58.13%   [EVAL] batch:   73 | acc: 68.75%,  total acc: 58.28%   [EVAL] batch:   74 | acc: 62.50%,  total acc: 58.33%   [EVAL] batch:   75 | acc: 87.50%,  total acc: 58.72%   [EVAL] batch:   76 | acc: 81.25%,  total acc: 59.01%   [EVAL] batch:   77 | acc: 93.75%,  total acc: 59.46%   [EVAL] batch:   78 | acc: 68.75%,  total acc: 59.57%   [EVAL] batch:   79 | acc: 100.00%,  total acc: 60.08%   [EVAL] batch:   80 | acc: 12.50%,  total acc: 59.49%   [EVAL] batch:   81 | acc: 0.00%,  total acc: 58.77%   [EVAL] batch:   82 | acc: 0.00%,  total acc: 58.06%   [EVAL] batch:   83 | acc: 0.00%,  total acc: 57.37%   [EVAL] batch:   84 | acc: 6.25%,  total acc: 56.76%   [EVAL] batch:   85 | acc: 81.25%,  total acc: 57.05%   [EVAL] batch:   86 | acc: 25.00%,  total acc: 56.68%   [EVAL] batch:   87 | acc: 6.25%,  total acc: 56.11%   [EVAL] batch:   88 | acc: 6.25%,  total acc: 55.55%   [EVAL] batch:   89 | acc: 0.00%,  total acc: 54.93%   [EVAL] batch:   90 | acc: 25.00%,  total acc: 54.60%   [EVAL] batch:   91 | acc: 37.50%,  total acc: 54.42%   [EVAL] batch:   92 | acc: 93.75%,  total acc: 54.84%   [EVAL] batch:   93 | acc: 93.75%,  total acc: 55.25%   [EVAL] batch:   94 | acc: 87.50%,  total acc: 55.59%   [EVAL] batch:   95 | acc: 87.50%,  total acc: 55.92%   [EVAL] batch:   96 | acc: 87.50%,  total acc: 56.25%   [EVAL] batch:   97 | acc: 87.50%,  total acc: 56.57%   [EVAL] batch:   98 | acc: 31.25%,  total acc: 56.31%   [EVAL] batch:   99 | acc: 50.00%,  total acc: 56.25%   [EVAL] batch:  100 | acc: 56.25%,  total acc: 56.25%   [EVAL] batch:  101 | acc: 12.50%,  total acc: 55.82%   [EVAL] batch:  102 | acc: 25.00%,  total acc: 55.52%   [EVAL] batch:  103 | acc: 31.25%,  total acc: 55.29%   [EVAL] batch:  104 | acc: 12.50%,  total acc: 54.88%   [EVAL] batch:  105 | acc: 37.50%,  total acc: 54.72%   [EVAL] batch:  106 | acc: 37.50%,  total acc: 54.56%   [EVAL] batch:  107 | acc: 62.50%,  total acc: 54.63%   [EVAL] batch:  108 | acc: 62.50%,  total acc: 54.70%   [EVAL] batch:  109 | acc: 62.50%,  total acc: 54.77%   [EVAL] batch:  110 | acc: 62.50%,  total acc: 54.84%   [EVAL] batch:  111 | acc: 50.00%,  total acc: 54.80%   [EVAL] batch:  112 | acc: 37.50%,  total acc: 54.65%   [EVAL] batch:  113 | acc: 50.00%,  total acc: 54.61%   [EVAL] batch:  114 | acc: 56.25%,  total acc: 54.62%   [EVAL] batch:  115 | acc: 62.50%,  total acc: 54.69%   [EVAL] batch:  116 | acc: 31.25%,  total acc: 54.49%   [EVAL] batch:  117 | acc: 12.50%,  total acc: 54.13%   [EVAL] batch:  118 | acc: 12.50%,  total acc: 53.78%   [EVAL] batch:  119 | acc: 0.00%,  total acc: 53.33%   [EVAL] batch:  120 | acc: 25.00%,  total acc: 53.10%   [EVAL] batch:  121 | acc: 56.25%,  total acc: 53.12%   [EVAL] batch:  122 | acc: 56.25%,  total acc: 53.15%   [EVAL] batch:  123 | acc: 62.50%,  total acc: 53.23%   [EVAL] batch:  124 | acc: 100.00%,  total acc: 53.60%   [EVAL] batch:  125 | acc: 100.00%,  total acc: 53.97%   [EVAL] batch:  126 | acc: 93.75%,  total acc: 54.28%   [EVAL] batch:  127 | acc: 87.50%,  total acc: 54.54%   [EVAL] batch:  128 | acc: 68.75%,  total acc: 54.65%   [EVAL] batch:  129 | acc: 75.00%,  total acc: 54.81%   [EVAL] batch:  130 | acc: 87.50%,  total acc: 55.06%   [EVAL] batch:  131 | acc: 87.50%,  total acc: 55.30%   [EVAL] batch:  132 | acc: 56.25%,  total acc: 55.31%   
cur_acc:  ['0.8617', '0.9236', '0.8125', '0.9196', '0.8708', '0.8393', '0.5597', '0.7356']
his_acc:  ['0.8617', '0.8725', '0.7215', '0.6849', '0.6882', '0.6774', '0.5795', '0.5531']
----------END
his_acc mean:  [0.8627 0.8263 0.7567 0.7085 0.6545 0.634  0.593  0.5568]
